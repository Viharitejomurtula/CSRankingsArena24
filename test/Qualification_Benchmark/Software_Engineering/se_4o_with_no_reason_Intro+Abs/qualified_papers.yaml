papers:
- title: Automated Invariant Generation for Solidity Smart Contracts
  abstract: 'Smart contracts are computer programs running on blockchains to automate
    the

    transaction execution between users. The absence of contract specifications

    poses a real challenge to the correctness verification of smart contracts.

    Program invariants are properties that are always preserved throughout the

    execution, which characterize an important aspect of the program behaviors. In

    this paper, we propose a novel invariant generation framework, INVCON+, for

    Solidity smart contracts. INVCON+ extends the existing invariant detector,

    InvCon, to automatically produce verified contract invariants based on both

    dynamic inference and static verification. Unlike INVCON+, InvCon only produces

    likely invariants, which have a high probability to hold, yet are still not

    verified against the contract code. Particularly, INVCON+ is able to infer more

    expressive invariants that capture richer semantic relations of contract code.

    We evaluate INVCON+ on 361 ERC20 and 10 ERC721 real-world contracts, as well as

    common ERC20 vulnerability benchmarks. The experimental results indicate that

    INVCON+ efficiently produces high-quality invariant specifications, which can

    be used to secure smart contracts from common vulnerabilities.'
  url: http://arxiv.org/abs/2401.00650v1
  keywords: Smart contract, invariant detection.
  document: "# Automated Invariant Generation for Solidity Smart Contracts\n\nYe Liu,\
    \ Chengxuan Zhang, Yi Li Nanyang Technological University, Singapore {ye.liu,\
    \ chengxua001, yi li}@ntu.edu.sg\n\n*Abstract*—Smart contracts are computer programs\
    \ running on blockchains to automate the transaction execution between users.\
    \ The absence of contract specifications poses a real challenge to the correctness\
    \ verification of smart contracts. Program invariants are properties that are\
    \ always preserved throughout the execution, which characterize an important aspect\
    \ of the program behaviors. In this paper, we propose a novel invariant generation\
    \ framework, INVCON+, for Solidity smart contracts. INVCON+ extends the existing\
    \ invariant detector, InvCon, to automatically produce *verified* contract invariants\
    \ based on both dynamic inference and static verification. Unlike INVCON+, InvCon\
    \ only produces *likely* invariants, which have a high probability to hold, yet\
    \ are still not verified against the contract code. Particularly, INVCON+ is able\
    \ to infer more expressive invariants that capture richer semantic relations of\
    \ contract code. We evaluate INVCON+ on 361 ERC20 and 10 ERC721 real-world contracts,\
    \ as well as common ERC20 vulnerability benchmarks. The experimental results indicate\
    \ that INVCON+ efficiently produces high-quality invariant specifications, which\
    \ can be used to secure smart contracts from common vulnerabilities.\n\n*Index\
    \ Terms*—Smart contract, invariant detection.\n\n# I. INTRODUCTION\n\nSmart contracts\
    \ are computer programs that operate on blockchain networks. They are used to\
    \ facilitate the management of substantial financial assets and the automated\
    \ execution of agreements among multiple parties who lack inherent trust. Notably,\
    \ blockchain networks such as Ethereum [\\[1\\]](#page-13-0) and BSC [\\[2\\]](#page-13-1)\
    \ are widely recognized as leading platforms supporting smart contracts, with\
    \ applications spanning diverse domains such as supply-chain management, finance,\
    \ energy, games, and digital artworks. While smart contracts hold promise for\
    \ facilitating value transfer among users, those that deviate from their specifications\
    \ may harbor bugs or vulnerabilities. Numerous implementations of ERC20 contracts\
    \ diverge from common expectations, as exemplified by standard non-compliance\
    \ of ERC20 [\\[3\\]](#page-13-2), particularly concerning event emission, balance\
    \ updates, and the transaction fee mechanisms.\n\nEven well-established standard\
    \ ERC20 implementations exhibit inconsistencies [\\[4\\]](#page-13-3). The root\
    \ cause lies in the limited semantic specifications outlined in the ERC20 standard\
    \ proposal document [\\[5\\]](#page-13-4). Take the transfer function as an illustration\
    \ it is designed to move a specified amount of tokens from the sender to the recipient\
    \ while triggering the Transfer event and should throw an error if the sender\
    \ lacks adequate tokens for the transfer. Nevertheless, the ERC20 proposal provides\
    \ only simple textual descriptions of the function, leading to semantic disparities\
    \ across various ERC implementations and even\n\ndifferent versions of the same\
    \ implementation. For instance, the widely used ERC20 implementation from OpenZeppelin\
    \ initially did not permit a return value for the transfer function until a later\
    \ commit,[1](#page-0-0) causing incompatibility issues with renowned tokens like\
    \ BNB, as reported by the reputable security company SECBIT [\\[4\\]](#page-13-3).\
    \ In cases where a contract necessitates checking the return value of an external\
    \ call to a transfer function of ERC20 contracts, even if the transfer is successful,\
    \ it may revert due to the absence of a return value, resulting in compatibility\
    \ problems [\\[6\\]](#page-13-5). However, removing the return value check exposes\
    \ contracts to a potential vulnerability known as the *fake deposit attack* [\\\
    [7\\]](#page-13-6).\n\nEnsuring the correctness of smart contracts poses a significant\
    \ challenge, especially in the absence of contract specifications. On the one\
    \ hand, the documentation for most smart contracts is scant, with even widely\
    \ recognized smart contract libraries like OpenZeppelin [\\[8\\]](#page-13-7),\
    \ [\\[9\\]](#page-13-8) found to have errors and deficiencies in their documentation\
    \ [\\[10\\]](#page-13-9). On the other hand, the absence of contract specifications\
    \ hampers the widespread adoption of formal verification tools in the realm of\
    \ smart contracts. To address this issue, the commercial formal verification company\
    \ Certora[2](#page-0-1) has adopted a crowd sourcing approach—they hosted numerous\
    \ competitions on well-known bug bounty platforms, such as Code4Rena,[3](#page-0-2)\
    \ to engage third-party security experts in the formulation of contract specifications.\
    \ Yet, manual creation of formal specifications for smart contracts remains costly\
    \ and error-prune.\n\nMany automated techniques [\\[11\\]](#page-13-10), [\\[12\\\
    ]](#page-13-11) have been proposed to generate formal specifications in various\
    \ forms to support the testing, verification, and validation of software programs.\
    \ Among them, program invariants, which are enduring properties maintained throughout\
    \ program execution, inherently serve as excellent candidates for enhancing and\
    \ reinforcing program specifications. Program invariants have been used for vulnerability\
    \ detection [\\[13\\]](#page-13-12), conformance checking [\\[3\\]](#page-13-2),\
    \ runtime protection [\\[14\\]](#page-13-13), type checking [\\[15\\]](#page-13-14),\
    \ and formal verification [\\[16\\]](#page-13-15), [\\[17\\]](#page-13-16) for\
    \ smart contracts. Established tools, such as Daikon [\\[11\\]](#page-13-10),\
    \ can identify *likely* program invariants for Java programs through the execution\
    \ of their test cases. The process involves statistically inferring the invariants\
    \ that hold based on predefined templates, while discarding those refuted by the\
    \ data trace records. The complete historical transaction\n\n<span id=\"page-0-0\"\
    ></span><sup>1</sup>https://github.[com/OpenZeppelin/openzeppelin-solidity/commit/](https://github.com/OpenZeppelin/openzeppelin-solidity/commit/6331dd125d8e8429480b2630f49781f3e1ed49cd)\
    \ [6331dd125d8e8429480b2630f49781f3e1ed49cd](https://github.com/OpenZeppelin/openzeppelin-solidity/commit/6331dd125d8e8429480b2630f49781f3e1ed49cd)\n\
    \n<span id=\"page-0-1\"></span><sup>2</sup>https://www.certora.com/\n\n<span id=\"\
    page-0-2\"></span><sup>3</sup>https://code4rena.com/\n\ndata of smart contracts\
    \ is consistently stored on blockchains, encapsulating all execution data since\
    \ contract deployment, serving as a valuable data source for mining invariants.\n\
    \nIn our prior work, INVCON [\\[18\\]](#page-13-17) utilized Daikon to identify\
    \ *likely* invariants for smart contracts, all of which are primitive predicates\
    \ hold throughout the existing transaction histories. Moreover, Liu et al. [\\\
    [19\\]](#page-13-18) employed reinforcement learning to learn contract invariants\
    \ critical to safely performing arithmetic operations, with focus on preventing\
    \ integer overflow and underflow. Despite their usefulness, the correctness of\
    \ such inferred invariants remains unverified. In particular, an invariant which\
    \ holds in past transactions may not always hold in the future—this may be due\
    \ to the limited contract interactions observed in the transaction histories so\
    \ far.\n\nIn this paper, we expand upon INVCON to generate *verified* contract\
    \ invariants utilizing both dynamic inference and static verification. We introduce\
    \ a specialized invariant specification language tailored for Solidity smart contracts\
    \ and propose a novel approach for inferring high-quality verified invariants.\
    \ Specifically, we design a Houdini-like [\\[12\\]](#page-13-11) algorithm to\
    \ generate verified invariants for smart contracts. To address the explosion problem\
    \ in searching for richer invariant candidates, such as implications that prevail\
    \ in ERC20 and ERC721 [\\[20\\]](#page-13-19), [\\[21\\]](#page-13-20), [\\[22\\\
    ]](#page-13-21) specifications, we introduce an iterative and incremental process\
    \ for exploring these candidates on demand. We also apply control- and data-flow\
    \ analyses to eliminate meaningless candidates and further improve the invariant\
    \ generation efficiency. Our approach is implemented as an automated tool called\
    \ INVCON+. Through evaluation on 361 ERC20 contracts and 10 ERC721 real-world\
    \ Solidity contracts, we demonstrate that INVCON+ produces comprehensive contract\
    \ invariant specifications with no false positives. Furthermore, our analysis\
    \ of real-world vulnerable ERC20 contracts underscores the potential of INVCON+\
    \ in safeguarding these contracts through the application of mined invariant specifications.\n\
    \nIn summary, we make the following contributions:\n\n- We introduce a comprehensive\
    \ invariant specification language designed for expressing operational semantics\
    \ in Solidity smart contracts. This language enables logical operations on variables\
    \ of primitive types and commonly used data structures like structs, arrays, and\
    \ mappings in Solidity.\n- We present a unified framework for generating *verified*\
    \ invariants in Solidity smart contracts, combining dynamic invariant detection\
    \ and static invariant verification. Specifically, we develop a custom algorithm\
    \ inspired by the Houdini algorithm to verify invariants for smart contracts and\
    \ introduce an iterative process to derive a richer class of invariants.\n- Our\
    \ proposed approach is implemented in INVCON+, and its effectiveness is evaluated\
    \ on 361 ERC20 contracts and 10 ERC721 contracts, along with vulnerable ERC20\
    \ contracts involving 25 types of vulnerabilities. The results demonstrate that\
    \ INVCON+ can generate high-quality and comprehensive invariant specifications\
    \ for smart contracts. The dataset, raw results, and the prototype used in our\
    \ experiments are available online at: https://sites.google.[com/view/invconplus/.](https://sites.google.com/view/invconplus/)\n\
    \n<span id=\"page-1-1\"></span>a, v ∈ V ariable ::= address | uint | int | string\
    \ | bytes | byte | bool | array | mapping | struct{⃗v} f ∈ F unction ::= func(⃗a)\
    \ {⃗s} s ∈ Statement ::= v | v := e | if (e) {⃗s} else {⃗s}| call(⃗e) | return\
    \ e | require(e) | assert(e) | revert e ∈ Expr ::= v | *const* | e[e] | e.v |\
    \ e ▷◁ e\n\nFig. 1: The core grammar of the Solidity language.\n\nOrganizations.\
    \ The rest of the paper is organized as follows. Section [II](#page-1-0) provides\
    \ the background about smart contracts and invariant inference. Section [III](#page-2-0)\
    \ defines the invariant specification language. Then, Sect. [IV](#page-3-0) introduces\
    \ our invariant generation approach. Section [V](#page-5-0) describes our implementation\
    \ framework, INVCON+, and Sect. [VI](#page-6-0) demonstrates our evaluation results.\
    \ The related work is discussed in Sect. [VII](#page-11-0) and we conclude the\
    \ paper in Sect. [VIII.](#page-12-0)\n\n# II. BACKGROUND\n\n# <span id=\"page-1-0\"\
    ></span>*A. Solidity Smart Contracts*\n\nFigure [1](#page-1-1) presents the foundational\
    \ grammar of the Solidity language, with certain features, such as event emission,\
    \ intentionally excluded for the sake of clarity. Solidity encompasses various\
    \ primitive data types, including integer, string, and Boolean. Distinguishing\
    \ itself from other programming languages like Java, Solidity does not permit\
    \ floating-point numbers and incorporates a distinctive address type. This design\
    \ choice is rooted in the interaction pattern between contracts and blockchain\
    \ users, each possessing a unique address. Moreover, the majority of contracts\
    \ are developed with the primary goal of tokenizing digital assets.\n\nA Solidity\
    \ smart contract comprises a collection of state variables and a set of functions.\
    \ Statements within each function can take the form of variable assignments, conditional\
    \ statements, internal or external function calls, requirement or assertion statements,\
    \ and reversion or return statements. Notably, the *require* and *assert* statements\
    \ can be employed to enforce program invariants at runtime. In the realm of expressions,\
    \ ▷◁ denotes a binary operator encompassing {+, −, ∗, /, >, <, ≥, ≤, =, ̸=, ∧,\
    \ ∨}.\n\nSmart Contract Execution. The execution of a smart contract function\
    \ can be triggered by sending a blockchain transaction to the contract address.\
    \ Typically, each transaction incorporates one or more contract calls, potentially\
    \ leading to alterations in contract state variables unless the transaction undergoes\
    \ a reversion. To ease the discussion in this paper, we model a smart contract\
    \ SC as a tuple (⃗v, ⃗f), where ⃗v is a vector of state variables and ⃗f is a\
    \ list of public functions.\n\nDefinition II.1 (Contract Execution). Let Dom(v)\
    \ be the domain of a variable v and Dom(⃗v) = Q v∈⃗v Dom(v). Then, δ, δ′ ∈ Dom(⃗v)\
    \ represent two reachable contract states. For a function invocation f(⃗a), calling\
    \ function f with parameters values ⃗a, we define its high-level execution semantics\
    \ as a state transition δ f(⃗a) −−−→ δ ′ .\n\n<span id=\"page-2-1\"></span>const\
    \ ∈ Int, Bool, Addr, Str x ∈ FreeVar v ∈ Var e ∈ Expr ::= const | v | *old*(v)\
    \ | len(v) | SumMap(v) | e.x | e[x] | e ▷◁ e p ∈ Predicate ::= ⊥ | e | e =⇒ e\
    \ Statement ::= Requires p | Ensures p | ContractInv p\n\nFig. 2: The invariant\
    \ specification language.\n\nNote that since a contract execution is triggered\
    \ by a transaction recorded into a specific block of the blokchain, the parameter\
    \ values ⃗a also includes implicit transaction and block parameters, e.g., msg.sender\
    \ and block.number.\n\nTransaction Histories. The execution of a smart contract\
    \ is intricately linked to its transaction histories on the blockchain. The transaction\
    \ histories record every contract execution, capturing function calls, state transitions,\
    \ and modification to state variables from the contract deployment onward. It\
    \ encapsulates the evolution of the contract state, reflecting the cumulative\
    \ effect of all transactions. This historical traceability is fundamental for\
    \ auditing, debugging, and understanding the operational dynamics of smart contracts\
    \ on the blockchain.\n\n# *B. Invariant Inference*\n\nIn this paper, we aim to\
    \ mine *contract-level* and *functionlevel* invariant specifications.\n\nDefinition\
    \ II.2 (Function Pre/Post-conditions). Let f be a contract function, and predicates\
    \ p and q be the pre/postconditions of f, respectively, which can be represented\
    \ as a Hoare triple {p}f{q}. Then the following condition should be satisfied.\n\
    \n$$\\forall \\delta, \\forall \\vec{a} \\cdot \\delta \\vdash p \\land \\delta\
    \ \\xrightarrow{f(\\vec{a})} \\delta' \\implies \\delta' \\vdash q \\tag{1}$$\n\
    \nDefinition II.3 (Contract Invariant). Given a smart contract *SC*, its contract\
    \ invariant I is a predicate that must hold for any contract function execution.\
    \ More formally, we have ∀f ∈ SC · {I}f{I}.\n\nInvariant inference techniques\
    \ can be broadly categorized as static and dynamic. Static invariant inference\
    \ (e.g., Houdini [\\[12\\]](#page-13-11)) identifies function pre/post-conditions\
    \ and contract invariants that hold for any program execution. On the other hand,\
    \ dynamic invariant inference (e.g., Daikon [\\[11\\]](#page-13-10)) identifies\
    \ *likely* invariants that hold for specific contract executions (e.g., executions\
    \ of a test case).\n\nLet ∆ denotes a set of program executions {(δ, f(⃗a), δ′\
    \ )}, which bring the contract state from δ to δ ′ . The *likely* function pre/post-conditions\
    \ of f, i.e., {pˆ}f{qˆ}, hold for ∆ if ∀(δ, f(⃗a), δ′ ) ∈ ∆, δ |= ˆp ∧ δ f(⃗a)\
    \ −−−→ δ ′ =⇒ δ ′ |= ˆq. The *likely* contract invariants of a smart contract\
    \ is defined in a similar way, which is omitted here for brevity.\n\n# III. INVARIANT\
    \ SPECIFICATION LANGUAGE\n\n<span id=\"page-2-0\"></span>Figure [2](#page-2-1)\
    \ introduces our invariant specification language designed for Solidity smart\
    \ contracts. The language accommodates variables of four types: integer, Boolean,\
    \ address, and string, encompassing all primitive Solidity types illustrated\n\
    \nin Fig. [1.](#page-1-1) We facilitate two types of variables. The first, denoted\
    \ as v, pertains to function input parameters or contract state variables maintained\
    \ in the persistent storage of the blockchain. The second, denoted as x, is reserved\
    \ for free variables exclusively utilized to index structure members or items\
    \ within arrays and mappings. Each invariant predicate is expressed as either\
    \ a primitive logical expression or an implication expression. Furthermore, valid\
    \ specification statements encompass function-level precondition invariant predicates\
    \ (Requires) and postcondition invariant predicates (Ensures), and contract-level\
    \ invariant predicates (ContractInv).\n\nThe expressions within the language may\
    \ take the form of constants, variables, structure members, array items, and binary\
    \ expressions. The old(·) notation is employed to differentiate between the value\
    \ of a variable before entering the function and its value upon exiting the function,\
    \ while len(·) refers to the array length or mapping size. Additionally, the language\
    \ incorporates the widely used SumMap(·) operator for computing the arithmetic\
    \ sum over mapping items. The notation \"e ▷◁ e\" represents arithmetic or logical\
    \ binary operations, where the operator \"▷◁\" corresponds to the set defined\
    \ in Solidity as shown in Fig. [1.](#page-1-1)\n\nUtilizing this invariant language,\
    \ we can articulate a diverse range of function\n\nand contract invariants. To\
    \ exemplify its application, we present a simple illustration. In Fig. [3,](#page-3-1)\
    \ a basic ERC20 contract is depicted, featuring three state variables—totalSupply,\
    \ balances, allows (standing for allowances)—and a function, transferFrom. The\
    \ purpose of the transferFrom function is to transfer a specified amount of tokens\
    \ from the account addressed at from to another account at to. An extensively\
    \ studied ERC20 contract invariant of this example can be succinctly expressed\
    \ as: \"SumM ap(balances) = totalSupply\". This assertion signifies that the total\
    \ sum of items within the mapping variable balances must be equal to the value\
    \ of totalSupply. Additionally, the function pre/post-conditions can be articulated\
    \ as follows.\n\nRequires ⊥\n\n```\n⃝1 Ensures to ̸= 0 =⇒ allows[from][msg.sender]\
    \ =\n           old(allows[from][msg.sender]) − tokens\n```\n⃝2 Ensures to ̸=\
    \ 0 ∧ from ̸= to =⇒ balance[from] = old(balance[ from]) − tokens ∧ balance[to]\
    \ = old(balance[to]) + tokens\n\n$$\\begin{aligned} \\text{(\\text{\\textquotedblleft}\
    \ measures\\text{ }to} & to \\neq 0 \\land from = to \\implies balance[from] =\
    \ \\\\ & old(balance[from]) \\land balance[to] = old(balance[to]) \\end{aligned}$$\n\
    \nIn this instance, it is straightforward to ascertain that there are no preconditions\
    \ for the transferFrom function, assuming that all function preconditions are\
    \ primitive predicates. The function is characterized by three postconditions.\
    \ The first postcondition ⃝1 specifies that allows will undergo an update (Line\
    \ [11\\)](#page-3-1) when to is a non-zero address. Additionally, in cases where\
    \ from and to represent distinct addresses, the second postcondition ⃝2 dictates\
    \ that the balances should be adjusted accordingly (Lines [12–13\\)](#page-3-1).\
    \ Conversely, when from and to are identical, the last postcondition ⃝3 emphasizes\
    \ that the net effect on balance changes should be nullified. A detailed exploration\
    \ of how these invariants are mined will be provided in Sect. [IV-E.](#page-4-0)\n\
    \n```\n1 contract ERC20 {\n2 // state variables\n3 uint totalSupply;\n4 mapping(address=>uint)\
    \ balances;\n5 mapping(address=>mapping(address=>uint)) allows;\n6 ...\n7 function\
    \ transferFrom(address from, address to,\n   ,→ uint tokens) public returns (bool)\
    \ {\n8 if (to == address(0)){\n9 return false;\n10 }\n11 allows[from][msg.sender]\
    \ =\n     ,→ allows[from][msg.sender].sub(tokens);\n12 balances[from] = balances[from].sub(tokens);\n\
    13 balances[to] = balances[to].add(tokens);\n14 return true;\n15 }\n16 }\n```\n\
    ![](_page_3_Figure_2.jpeg)\n\n# IV. INVARIANT GENERATION APPROACH\n\n<span id=\"\
    page-3-0\"></span>In this section, we present our algorithm for generating verified\
    \ invariants in smart contracts and elaborate on the techniques employed to infer\
    \ implication invariants. For simplicity in presentation, we use the term \"invariants\"\
    \ to collectively denote both function pre/post-conditions and contract invariants\
    \ when explicit characterization is unnecessary.\n\n# *A. Algorithm*\n\nAlgorithm\
    \ [1](#page-3-2) outlines our approach to invariant generation. The algorithm\
    \ takes a smart contract SC , a sequence of contract transactions T, and a set\
    \ of invariant templates Q as input. The output, denoted as Invs, comprises a\
    \ set of *verified* invariants, encompassing both primitive and implication invariants.\n\
    \nIn this algorithm, Invs is initialized as an empty set (Line [1\\)](#page-3-3).\
    \ Subsequently, we initialize a set C that encompasses all potential invariant\
    \ candidates under the given input (Line [2\\)](#page-3-4), similar to Daikon's\
    \ initialization process [\\[11\\]](#page-13-10), which instantiates all the parameterized\
    \ invariant templates with concrete contract state variables and function input\
    \ variables. For example, \"X = Y \" is a binary equation template where X and\
    \ Y are placeholders that can be filled by two concrete variables: v<sup>x</sup>\
    \ and v<sup>y</sup> whenever Dom(vx) ≡ Dom(vy). It is important to note that here\
    \ C excludes implication invariant candidates due to the exponential complexity\
    \ of traversing all implication candidates. Instead, implication invariants will\
    \ be generated on demand. Moreover, the execution trace set ∆ is initialized as\
    \ an empty set (Line [3\\)](#page-3-5).\n\nThe algorithm processes the transaction\
    \ histories to extract corresponding execution traces. For each transaction t<sup>i</sup>\
    \ , the algorithm parses it to extract the invoked function f and parameters values\
    \ ⃗a (Line [5\\)](#page-3-6). Additionally, the old and present contract states\
    \ (i.e., values of the contract state variables), denoted as δ and δ ′ , respectively,\
    \ are recorded. The tuple (δ, f(⃗a), δ ′ ) is added to the execution trace set\
    \ ∆ (Line [6\\)](#page-3-7).\n\nNext, the algorithm executes the dynamic invariant\
    \ detection procedure INVDETECT (Line [8\\)](#page-3-8) to obtain two classes\
    \ of invariant candidates:\n\n- Clikely, likely invariant candidates that hold\
    \ for the entire transaction histories.\n- Cpartial, partially supported invariant\
    \ candidates that hold for a subset of transaction histories.\n\n# Algorithm 1:\
    \ Contract Invariant Inference\n\n```\nInputs : SC = {⃗v, ⃗f}, where each element\
    \ vi ∈ ⃗v is a\n            contract state variable and each element fi ∈ ⃗f is\
    \ a\n            public contract function;\n           : T = {ti|1 ≤ i ≤ n}, where\
    \ each element ti is a\n            contract transaction;\n           : Q, a set\
    \ of invariant templates.\n  Outputs :Invs, a set of verified invariants.\n1 Invs\
    \ := ∅;\n2 C := INITIALIZECANDIDATES(⃗v, ⃗f, Q) ; //primitive\n    candidates\n\
    3 ∆ := ∅ ; //execution trace set\n4 foreach ti ∈ T do\n5 (δ, f(⃗a), δ′\n     \
    \           ) ← PARSE(ti) ;\n6 ∆ ← ∆ ∪ (δ, f(\n                     −→a ), δ′\n\
    \                           );\n7 end foreach\n8 Clikely, Cpartial ←INVDETECT\
    \ (∆, C);\n9 Invs ← STATICINFER(Clikely) ;\n10 Cimp ← FINDIMPLICATIONS(Clikely\
    \ \\ Invs, Cpartial) ;\n   //implication candidates\n11 while Cimp ̸= ∅ do\n12\
    \ Invs ← Invs ∪ STATICINFER(Cimp) ;\n13 Cimp ← WEAKENIMPLICATIONS(Cimp \\ Invs);\n\
    14 end while\n15 return Invs\n```\n<span id=\"page-3-17\"></span><span id=\"page-3-16\"\
    ></span><span id=\"page-3-15\"></span><span id=\"page-3-14\"></span><span id=\"\
    page-3-13\"></span><span id=\"page-3-12\"></span><span id=\"page-3-11\"></span><span\
    \ id=\"page-3-10\"></span><span id=\"page-3-8\"></span><span id=\"page-3-7\"></span>\n\
    $$\\begin{array}{c} \\bot\\\\ C\\_{imp} := \\{ (\\eta \\implies \\tau) \\mid \\\
    eta, \\tau \\in C\\_{likelihood} \\mid Invs \\cup C\\_{partial}, \\eta \\neq \\\
    tau \\} \\end{array} \\text{Int}$$\n\n$$(\\eta \\implies \\tau) \\in C\\_{imp}\
    \ \\qquad \\begin{array}{c} \\forall a \\in vars(\\eta), \\forall b \\in vars(\\\
    tau). \\\\ \\neg dep(a, b) \\end{array} \\qquad \\begin{array}{c} \\text{Delete}\
    \ \\\\ \\neg dep(a, b) \\end{array} \\qquad \\begin{array}{c} \\text{Delete} \\\
    end{array}$$\n\n$$\\begin{array}{c} C\\_{imp} \\leftarrow C\\_{imp} \\mid (\\\
    eta \\implies \\tau) \\end{array} \\qquad \\begin{array}{c} \\text{Delete} \\\
    end{array}$$\n\n$$\\text{Fig. 4: FINDIMPLICATION}$$\n\nSubsequently, a primitive\
    \ invariant inference technique, detailed in Sect. [IV-B,](#page-3-9) is applied\
    \ to infer the standing invariants out of Clikely, and all the verified invariants\
    \ are included in Invs (Line [9\\)](#page-3-10). The unverified likely invariant\
    \ candidates, Clikely \\ Invs, and Cpartial are used to derive implication candidates\
    \ assigned to Cimp (Line [10\\)](#page-3-11) via FINDIMPLICATIONS, which will\
    \ be detailed in Sect. [IV-C.](#page-4-1) Additionally, it is important to note\
    \ that the found implications may not always hold. An iterative process is in\
    \ place to validate these implications (Line [12\\)](#page-3-12) or weaken these\
    \ implications via WEAKENIMPLICATIONS (Line [13\\)](#page-3-13) to identify new\
    \ ones. This iterative process continues until all valid candidates are examined\
    \ (Line [11\\)](#page-3-14). Finally, the algorithm returns Invs, which includes\
    \ all the correctly mined invariants from transaction histories (Line [15\\)](#page-3-15).\n\
    \n# <span id=\"page-3-9\"></span>*B. Primitive Invariant Inference*\n\nAlgorithm\
    \ [2](#page-4-2) illustrates our Houdini-like algorithm to infer verified primitive\
    \ invariants from the candidates mined from contract transaction histories. First,\
    \ we enable all the candidates in SC via contract instrumentation (Line [1\\)](#page-4-3);\
    \ each candidate is explicitly labeled by the added keywords, e.g., ContractInv\
    \ for contract invariant, Requires for function precondition, and Ensures for\
    \ function postcondition. Next, we invoke a modular verifier to statically verify\
    \ these enabled candidates (Line [3\\)](#page-4-4), i.e., verifying each function\
    \ in isolation where all the corresponding candidates are examined against the\
    \ function\n\n$$\\frac{\\bot}{C\\_{imp}^{\\circ} := \\emptyset} \\text{ limit}$$\n\
    \n<span id=\"page-4-9\"></span>(η<sup>1</sup> =⇒ τ ),(η<sup>2</sup> =⇒ τ ) ∈ Cimp\
    \ \\ Invs η<sup>1</sup> ∧ η<sup>2</sup> ̸≡ f alse Append-1 Cˆimp ← Cˆimp ∪ (η<sup>1</sup>\
    \ ∧ η<sup>2</sup> =⇒ τ )\n\n(η =⇒ τ1),(η =⇒ τ2) ∈ Cimp \\ Invs τ<sup>1</sup> ∨\
    \ τ<sup>2</sup> ̸≡ true Append-2 Cˆimp ← Cˆimp ∪ (η =⇒ τ<sup>1</sup> ∨ τ2)\n\n\
    Fig. 5: WEAKENIMPLICATIONS.\n\n# Algorithm 2: STATICINFER(Candidates)\n\n<span\
    \ id=\"page-4-7\"></span><span id=\"page-4-6\"></span><span id=\"page-4-4\"></span><span\
    \ id=\"page-4-3\"></span><span id=\"page-4-2\"></span>\n\n| 1 Instrument SC to\
    \ enable each candidate from Candidates; |  |  |  |  |\n|-----------------------------------------------------------|--|--|--|--|\n\
    | 2 while true do                                           |  |  |  |  |\n| result\
    \ = MODULARVERIFY(SC) ;                              |  |  |  |  |\n| if result\
    \ = CORRECT then<br>4                             |  |  |  |  |\n| return enabled\
    \ candidates ; //verified<br>5               |  |  |  |  |\n| invariants     \
    \                                           |  |  |  |  |\n| else if result =\
    \ INCORRECT due to failed candidate c<br>6 |  |  |  |  |\n| then             \
    \                                         |  |  |  |  |\n| disable c in SC;<br>7\
    \                                     |  |  |  |  |\n| else<br>8             \
    \                                    |  |  |  |  |\n| raise Error ; //INCORRECT\
    \ due to failed<br>9              |  |  |  |  |\n| assertion in SC           \
    \                                |  |  |  |  |\n| end if<br>10               \
    \                               |  |  |  |  |\n| 11 end while                \
    \                              |  |  |  |  |\n|                              \
    \                             |  |  |  |  |\n\n<span id=\"page-4-8\"></span><span\
    \ id=\"page-4-5\"></span>implementation. When there is a failed invariant candidate\
    \ c violating the verification condition, c will be disabled in SC (Line [7\\\
    )](#page-4-5). This process will continue until all the enabled candidates are\
    \ verified successfully (Line [4\\)](#page-4-6) and then returned (Line [5\\)](#page-4-7).\
    \ Particularly, whenever there is a failed assertion in SC, i.e., a violated condition\
    \ e in the assert(e) statement, the algorithm terminates with an error raised\
    \ (Line [9\\)](#page-4-8). This happens in Solidity contracts, because assert(e)\
    \ is often misused to replace require(e) that enforces program requirements due\
    \ to their similar effects on transaction reversion. For smart contracts without\
    \ failed assertions, the verified invariants is a maximal subset of the candidates\
    \ whose conjunction is an inductive invariant.\n\n# <span id=\"page-4-1\"></span>*C.\
    \ Implication Invariant Inference*\n\nFigures [4](#page-3-16) and [5](#page-4-9)\
    \ illustrate the two procedures for identifying implication candidates, respectively.\
    \ In Fig. [4,](#page-3-16) FINDIMPLICA-TIONS employs two straightforward inference\
    \ rules. The first rule explores all the potential implication candidates from\
    \ the unverified likely invariants Clikely \\ Invs and partial invariant candidates\
    \ Cpartial, including them in Cimp. An implication invariant takes the form of\
    \ η =⇒ τ , where η and τ comes from the existing the unverified and partial invariant\
    \ candidates.\n\nHowever, not all of the implication candidates constructed this\
    \ way are relevant in terms of the contract semantics. An implication can possibly\
    \ hold (i.e., relevant) if its precondition and postcondition align with the data/control-flow\
    \ of the contracts, and irrelevant implications should be discarded. The notation\
    \ vars(p) represents variables appearing in an invariant predicate p; for instance,\
    \ vars(p) = {from, to} when p is \"from ̸= to\". Additionally, dep(a, b) denotes\
    \ whether variable a depends on variable b in terms of control-flow or data-flow\
    \ in smart contract functions. To determine the valid implications, we leverage\
    \ the well-known static analysis tool Slither [\\[23\\]](#page-13-22) to trace\
    \ data-flow and control-flow in smart contract functions. Therefore, in Fig. [4,](#page-3-16)\
    \ a *Delete* rule is applied to eliminate implications that do not adhere to the\
    \ data-flow and controlflow relationship. This rule is iteratively applied until\
    \ no further implications can be eliminated.\n\nSome implication candidates may\
    \ be too strong and cannot be proved. Figure [5](#page-4-9) illustrates how we\
    \ derive a weaker set of implication candidates Cˆ imp from those unverified implication\
    \ candidates denoted as Cimp \\ Invs. In Fig. [5,](#page-4-9) WEAKENIMPLICATIONS\
    \ comprises three inference rules. It initially sets Cˆ imp to an empty set. Then\
    \ the rules *Append-1* and *Append-2* generate weaker implications by combing\
    \ two unverified implication candidates. In essence, η<sup>1</sup> ∧ η<sup>2</sup>\
    \ =⇒ τ is weaker than either η<sup>1</sup> =⇒ τ or η<sup>2</sup> =⇒ τ . Similarly,\
    \ η =⇒ τ<sup>1</sup> ∨ τ<sup>2</sup> is weaker than both η =⇒ τ<sup>1</sup> and\
    \ η =⇒ τ2. To eliminate useless implications that are tautologies, we impose restrictions\
    \ on the original implications, such as η<sup>1</sup> ∧ η<sup>2</sup> ̸≡ false\
    \ and τ<sup>1</sup> ∨ τ<sup>2</sup> ̸≡ true. It is evident that the weaker implications\
    \ are also relevant as they satisfy the same control/data-flow dependencies as\
    \ the original ones.\n\n# *D. Termination*\n\nThe termination of Alg. [1](#page-3-2)\
    \ can be ensured by the fact that INVCON+ can only produce a finite set of primitive\
    \ invariant predicates. The conclusion regarding the termination of Alg. [1](#page-3-2)\
    \ hinges on whether the loop (Lines [11](#page-3-14)[-14\\)](#page-3-17) comes\
    \ to an end. In each iteration of the loop, we possess at least one implication\
    \ candidate, constructed by WEAKENIMPLICATIONS (refer to Sect. [IV-C\\)](#page-4-1).\
    \ Regarding WEAKENIMPLICATIONS, it consistently generates weaker implication candidates\
    \ than the previous ones, utilizing conjunctions over premises or disjunctions\
    \ over consequences. Assuming INVDETECT yields n primitive invariant predicates\
    \ Clikely ∪ Cpartial = {p1, . . . , pn}, then the weakest implication will be\
    \ at least as strong as p<sup>1</sup> ∧ · · · ∧ p<sup>n</sup> =⇒ p<sup>1</sup>\
    \ ∨ · · · ∨ pn. Consequently, the loop will finish in no more than 2 × n iterations,\
    \ establishing the termination of this algorithm.\n\n# <span id=\"page-4-0\"></span>*E.\
    \ Running Example*\n\nWe illustrate our algorithm using the example presented\
    \ in Fig. [3.](#page-3-1) The details regarding our transaction parsing and invariant\
    \ detection will be elaborated in Sect. [V.](#page-5-0) For the sake of simplicity\
    \ in the illustration, assume that we have already acquired a set of likely and\
    \ partially supported invariants through invariant detection on the transaction\
    \ histories. In Table [I,](#page-6-1) the invariants labeled with ✓ are successfully\
    \ verified by the static verifier, while the ones with ✗ are unverified. In Step\
    \ ⃝1 , we perform a Houdini-like static inference on these detected invariant\
    \ candidates. Consequently, three likely invariants are verified, excluding to\
    \ ̸= 0. In the subsequent step (Step ⃝2 ), nine additional implication invariant\
    \ candidates are generated from the previously unverified likely invariants and\
    \ partially supported invariants, according to the rules in FINDIMPLICATIONS (see\
    \ Fig. [4\\)](#page-3-16). However, after the modular verification, only one implication\
    \ is confirmed. Furthermore, we weaken these unverified implication invariants\
    \ in Step ⃝3\n\n<span id=\"page-5-1\"></span>![](_page_5_Figure_0.jpeg)\n\nFig.\
    \ 6: The architecture overview of INVCON+.\n\nusing WEAKENIMPLICATIONS (see Fig.\
    \ [5\\)](#page-4-9) to derive four new implication candidates for further validation.\
    \ Eventually, all the invariants listed in Sect. [III](#page-2-0) are successfully\
    \ recovered (in a logically equivalent form). Moreover, two other invariants,\
    \ balances[to] ≥ old(balances[to]) and balances[from] ≤ old(balances[from]), are\
    \ verified, which provide additional insights on how the balances of the sender\
    \ and the receiver should change when transferFrom is called, beyond the standard\
    \ specifications.\n\n# V. IMPLEMENTATION\n\n# <span id=\"page-5-0\"></span>*A.\
    \ Overview*\n\nFigure [6](#page-5-1) demonstrates the high-level architecture\
    \ of IN-VCON+, our automated invariant detection tool for Solidity smart contracts.\
    \ The inputs to INVCON+ include a set of historical transactions and the corresponding\
    \ contract source code, while its output is a collection of smart contract invariant\
    \ specifications or the accordingly annotated contract code. INVCON+ comprises\
    \ four modules: (1) a *data parser* that decodes contract code and transaction\
    \ histories to extract concrete execution trace set; (2) a *dynamic invariant\
    \ detector* that generates a set of likely and partially supported invariants;\
    \ (3) a *modular invariant verifier* and an *implication learner* that verify\
    \ and learn contract invariants, respectively; and (4) a *suppressor* that simplifies\
    \ the results by removing redundant invariants. Notably, the implication learner\
    \ has already been detailed in Sect. [IV-C.](#page-4-1)\n\n# *B. Data Parser*\n\
    \nGiven a contract, we first collect all of its historical transactions. For each\
    \ transaction, we decode the specific function input based on the contract's Application\
    \ Binary Interface (ABI), and we interpret the transaction output in accordance\
    \ with the contract's storage layout specifications. This layout dictates where\
    \ each state variable is stored in the blockchain database. For instance, as shown\
    \ in Fig. [1,](#page-1-1) the first declared state variable totalSupply is stored\
    \ at the first slot (0x0) in the contract's blockchain database.\n\nThe input\
    \ of a contract transaction is represented as a tuple *(sender, function, parameters)*,\
    \ which encapsulates the transaction's sender, the invoked function's name, and\
    \ the corresponding input parameters. Conversely, the transaction's output\n\n\
    is denoted as *(status, storageChanges)*. Here, *status* signifies the transaction's\
    \ success or failure, while *storageChanges* details the alterations in the contract's\
    \ storage across various slots. By aligning storage slots with the contract's\
    \ storage layout, one can effectively interpret these storage modifications as\
    \ changes in the values of the contract's state variables. Employing the previously\
    \ described preprocessing technique enables the extraction of a sequence of data\
    \ triples (i.e., execution traces). These triples consist of the actual values\
    \ of state variables and function input variables at the point of function entry,\
    \ as well as the most recent values of state variables at the point of function\
    \ exit. It is important to note that any misrecognition of variables can lead\
    \ to incorrect invariant results. We have implemented measures to ensure the accuracy\
    \ of variable recognition. For state variables of primitive types, we directly\
    \ ascertain their values, as the storage layout for these variables remains constant\
    \ during runtime. In the case of non-primitive, dynamic state variables, to reduce\
    \ computational cost, we initially utilize the known variable values to hypothesize\
    \ a correlation between the altered storage slots and the dynamic state variables.\
    \ However, if this approach fails to produce an accurate mapping, it becomes necessary\
    \ to replay the entire transaction. This replay process enables us to track the\
    \ comprehensive execution information, including storage modifications, thus allowing\
    \ for the accurate determination of the correct mapping.\n\n# *C. Dynamic Invariant\
    \ Detector*\n\nThe effectiveness of dynamic invariant detection largely depends\
    \ on the diversity and scale of the customized invariant templates used. In our\
    \ methodology, these invariant templates are required to conform to the invariant\
    \ specification language outlined in Fig. [2.](#page-2-1) However, it is both\
    \ impossible and impractical to cover every conceivable invariant template. Our\
    \ approach, akin to that of INVCON [\\[18\\]](#page-13-17), limits the scope to\
    \ unary, binary, and ternary invariant templates. Unlike INVCON, our templates\
    \ are specifically designed for Solidity smart contracts, which are predominantly\
    \ used for financial applications. These contracts often entail intricate scientific\
    \ computations on scalar variables. Furthermore, Solidity features an array of\
    \ complex data structures, such as *mapping* and *struct*. To effectively infer\
    \ invariants related to these structures, we have incorporated several derivation\
    \ templates, such as *MemberItem* and *MappingItem*, which facilitate access to\
    \ elements within these data structures. Additionally, drawing inspiration from\
    \ the significance of balance invariants as highlighted by Wang et al. [\\[24\\\
    ]](#page-13-23), we have introduced a *SumMap* derivation template. This template\
    \ is specifically designed to aggregate the values contained within a mapping\
    \ variable.\n\nDynamic invariant detection employs a statistical methodology to\
    \ generate *likely* primitive invariants with a certain degree of statistical\
    \ confidence. Contrasting with the approach of IN-VCON, our method retains invariants\
    \ that are refuted by certain transactions in the final results. This is because\
    \ less stringent forms of these invariants, expressed as implications, may still\
    \ hold true for certain contracts. Both the likely invariants and the falsified\
    \ ones constitute a high-quality set of primitive\n\n<span id=\"page-6-1\"></span>\n\
    \n| Step | Invariants                                                        \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                       |                                    \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                     |  |  |\n|------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--|--|\n\
    | ⃝1   | Likely Contract Invariants:<br>totalSupply = SumMap(balances) ✓<br>Likely\
    \ Function Pre/post-conditions:<br>to ̸=0 ✗<br>balances[to] ≥ old(balances[to])\
    \ ✓<br>balances[from] ≤ old(balances[from]) ✓                                \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                              | Partially Supported Function Pre/post-conditions:<br>from\
    \ ̸= to<br>from = to<br>balances[from] = old(balances[from]) - tokens<br>balances[to]\
    \ = old(balances[to]) + tokens<br>allows[from][msg.sender] = old(allows[from][msg.sender])\
    \ - tokens<br>balances[from] = old(balances[from])<br>balances[to] = old(balances[to])\
    \ |  |  |\n| ⃝2   | Implication Invariant Candidates:<br>to ̸=0 =⇒ allows[from][msg.sender]\
    \ = old(allows[from][msg.sender]) - tokens ✓<br>to ̸=0 =⇒ balances[from] = old(balances[from])\
    \ - tokens ✗<br>to ̸=0 =⇒ balances[to] = old(balances[to]) + tokens ✗<br>to ̸=0\
    \ =⇒ balances[from] = old(balances[from] ✗<br>to ̸=0 =⇒ balances[to] = old(balances[to])\
    \ ✗<br>from ̸=to =⇒ balances[from] = old(balances[from]) - tokens ✗<br>from ̸=to\
    \ =⇒ balances[to] = old(balances[to]) + tokens ✗<br>from = to =⇒ balances[from]\
    \ = old(balances[from] ✗<br>from = to =⇒ balances[to] = old(balances[to]) ✗ |\
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \             |  |  |\n| ⃝3   | Weakened Implication Invariant Candidates:<br>to\
    \ ̸=0 ∧ from ̸=to =⇒ balances[from] = old(balances[from]) - tokens ✓<br>to ̸=0\
    \ ∧ from ̸=to =⇒ balances[to] = old(balances[to]) + tokens ✓<br>to ̸=0 ∧ from\
    \ = to =⇒ balances[from] = old(balances[from]) ✓<br>to ̸=0 ∧ from = to =⇒ balances[to]\
    \ = old(balances[to]) ✓                                                      \
    \                                                                            \
    \                                                                            \
    \                                               |                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                             |  |  |\n\nTABLE\
    \ I: Illustration example of invariant verification.\n\npredicates. Each of these\
    \ predicates has been empirically verified through historical transaction data\
    \ of smart contracts. In our evaluation setting, each valid primitive invariant\
    \ must be supported by at least three historical transactions.\n\n# *D. Modular\
    \ Invariant Verifier*\n\nThe Houdini algorithm [\\[12\\]](#page-13-11) is a widely\
    \ recognized technique commonly used in program annotation and validation processes.\
    \ Its primary objective is to automatically generate invariant annotations from\
    \ a group of candidates. To adapt Houdini algorithm for Solidity contracts, we\
    \ initially instrument the contracts with the mined invariants. This entails converting\
    \ the invariants into a compatible format and then embedding them into the contract.\
    \ The annotations are strategically placed at the beginning of functions to align\
    \ with their specific names and arguments. Subsequently, we transform the instrumented\
    \ contracts into *Boogie* [\\[25\\]](#page-13-24) programs, leveraging the existing\
    \ formal verification tool VeriSol [\\[26\\]](#page-13-25) for Solidity smart\
    \ contracts. We have refined the Boogie translator in VeriSol to better accommodate\
    \ contracts with inheritance and polymorphism features. For instance, the original\
    \ translator lacked support for unnamed parent contract calls using the \"super\"\
    \ keyword in Solidity, and it did not handle function overloading where a contract\
    \ includes multiple functions with the same name. We have enhanced its translation\
    \ rules to effectively translate these complex contracts into Boogie programs.\
    \ Finally, we utilize Boogie's own Houdini modular verifier to infer among the\
    \ aforementioned invariant annotations, resulting in a set of verified invariants.\n\
    \n# *E. Suppressor*\n\nAn invariant is deemed redundant if it can be derived from\
    \ another invariant. The invariants verified by INVCON+ may contain such redundancies.\
    \ Instead of eliminating redundancies in the dynamically detected invariants (as\
    \ what Daikon [\\[11\\]](#page-13-10) does), we only remove redundancies from\
    \ the invariants that are successfully verified. This design leaves more choices\
    \ to the implication learner, when synthesizing implication invariants. Among\
    \ the verified invariants, we utilize the Z3 solver [\\[27\\]](#page-13-26) to\
    \ determine if one invariant predicate can be deduced from another. Following\
    \ this analysis, we retain only the strongest invariant predicates in our final\
    \ results.\n\n# VI. EVALUATION\n\n<span id=\"page-6-0\"></span>In this section,\
    \ we evaluate INVCON+ to answer the following research questions:\n\n- 1) RQ1:\
    \ How effectively does INVCON+ generate invariants for smart contracts?\n- 2)\
    \ RQ2: How does the length of transaction histories used affect the performance\
    \ of INVCON+?\n- 3) RQ3: How effective are the invariants detected by IN-VCON+\
    \ in preventing real-world security attacks?\n\n# *A. Methodology*\n\nBenchmark.\
    \ To answer RQ1 and RQ2, we collected real-world smart contracts implementing\
    \ the most popular ERC20 and ERC721 standards, which have been studied extensively\
    \ in previous works [\\[3\\]](#page-13-2), [\\[4\\]](#page-13-3), [\\[14\\]](#page-13-13),\
    \ [\\[28\\]](#page-13-27), [\\[29\\]](#page-13-28). The most important reason\
    \ of choosing smart contracts implementing common standards is that their invariant\
    \ specifications are better understood, making it easier to obtain the ground\
    \ truth. First, we queried the public Ethereum ETL dataset hosted on the Google\
    \ BigQuery platform [\\[30\\]](#page-13-29) and then identified 13,116 contract\
    \ addresses flagged as ERC20 deployed between 2021 and 2022. Then, we identified\
    \ 2,689 ERC721 contract addresses deployed between 2020 and 2022. To facilitate\
    \ our analysis, we kept only open-source contracts written in Solidity versions\
    \ ranging between 0.5.0 and 0.5.17, which are currently supported by VERISOL.\
    \ Finally, we obtained 361 ERC20 contracts and 10\n\nERC721 contracts for the\
    \ experimental evaluation, where each contract has at least 50 historical transactions\
    \ as of June 2023.\n\nTo establish the ground truth for ERC20 and ERC721 contract\
    \ specifications and ensure the included invariants are comprehensive, we investigated\
    \ multiple external sources. These include the formal specifications referenced\
    \ in the existing literature [\\[4\\]](#page-13-3), [\\[14\\]](#page-13-13), popular\
    \ smart contract libraries, such as OpenZeppelin [\\[8\\]](#page-13-7), and online\
    \ documentations provided by smart contract formal verification companies. We\
    \ list the collected ERC20 and ERC721 invariant specifications in Table [II](#page-8-0)\
    \ and Table [III,](#page-8-1) respectively. These specifications are mainly based\
    \ on Certora [\\[20\\]](#page-13-19), [\\[21\\]](#page-13-20), [\\[31\\]](#page-13-30),\
    \ KEVM [\\[22\\]](#page-13-21), [\\[28\\]](#page-13-27), and OpenZeppelin API\
    \ documentations [\\[32\\]](#page-13-31), [\\[33\\]](#page-13-32). We analyzed\
    \ each of the collected invariants and manually translated it into our own specification\
    \ language (C.f. Fig. [2\\)](#page-2-1), which is a straightforward exercise in\
    \ most cases. We categorized these invariant specification into contract invariants,\
    \ function preconditions and postconditions in Tables [II](#page-8-0) and [III.](#page-8-1)\
    \ The functions listed in each table are the most commonly used standard functions\
    \ for ERC20 and ERC721 contracts. Some specifications documented in external sources\
    \ were omitted, e.g., \"Emitting a Transfer event\" for the transfer function,\
    \ because the particular language features are not supported by our specification\
    \ language.\n\nEvaluation Metrics. We use two evaluation metrics to evaluate INVCON+\
    \ on the ERC20 and ERC721 contracts. Particularly, we use Precision and RecallERC20\
    \ (RecallERC721) to measure the effectiveness of the generated invariants, denoted\
    \ as Xproved. We denote the ground truth invariants (e.g., ERC20) as Y . Formally,\n\
    \n$$\\text{Precision} = \\frac{|X\\_{\\text{provided}}|}{|X|},\\tag{2}$$\n\n$$\\\
    mathbf{Recall}\\_{\\text{ERC20}}(\\mathbf{Recall}\\_{\\text{ERC21}}) = \\frac{|X\\\
    _{\\text{provided}} \\cap Y|}{|Y|}, \\qquad \\text{(3)}$$\n\nwhere *precision*\
    \ refers to the proportion of the generated invariants which are correct and *recall*\
    \ is the proportion of the ground truth invariants which can be successfully generated.\
    \ Since the contract execution trace set ∆ from transaction histories may only\
    \ contain a subset of functions invocations, i.e., some functions are never invoked.\
    \ For a fair comparison, let Y ⇂ ∆ represent the ground truth invariants for the\
    \ functions appeared in the histories, and we use the adjusted recall in our experiments:\
    \ <sup>|</sup>Xproved∩<sup>Y</sup> <sup>|</sup> |Y ⇂∆| .\n\nNote that although\
    \ the ground truth invariants are derived based on multiple external sources and\
    \ widely deemed to be standard, they may still be incomplete, as there are infinitely\
    \ many correct invariants in theory. The purpose of collecting the ground truth\
    \ invariants is to include the list of common expectations that are needed for\
    \ contract safety and reliability. On the other hand, certain smart contracts\
    \ may not faithfully implement the ERC standards, and as a result, either some\
    \ ground-truth invariants may not hold for them or they satisfy additional invariants\
    \ not included in the ground truth. Nevertheless, an ideal invariant generation\
    \ tool should be able to recover as many ground-truth invariants as possible,\
    \ and meanwhile, recover other relevant invariants that are correct\n\nand useful\
    \ in describing unique smart contract behaviors.\n\n# *B. Experiment Setup*\n\n\
    All the experiments were conducted on a desktop computer with the Ubuntu 20.04\
    \ OS, an Intel Core Xeon 3.50GHx processor, and 32GB of RAM. To facilitate the\
    \ evaluation, we have crawled and cached all transaction histories in advance\
    \ for the contracts used in our experiments.\n\n# *C. RQ1: Effectiveness of Invariant\
    \ Generation*\n\nBaseline. To evaluate the performance of INVCON+, we used INVCON\
    \ as our baseline. INVCON uses Daikon as the back-end invariant detection engine\
    \ and more implementation details can be found in the previous work [\\[18\\]](#page-13-17).\
    \ To the best of our knowledge, Cider [\\[19\\]](#page-13-18) is the only automated\
    \ invariant generation tool for smart contracts besides INVCON. We have contacted\
    \ the authors of Cider to obtain a copy of the tool,[4](#page-7-0) but failed\
    \ to set it up. We will discuss and compare with this work in Sect. [VII.](#page-11-0)\n\
    \nAdditionally, we compared INVCON+ with its two variants: INVCON+ *Naive*, which\
    \ performs only dynamic invariant detection tailored to Solidity contracts, and\
    \ INVCON+ *Primitive*, which employs the Houdini algorithm to generate verified\
    \ invariants based only on dynamically detected invariant candidates.\n\nResults.\
    \ Table [IV](#page-8-2) presents the comparison results for 361 ERC20 contracts,\
    \ with a constraint of utilizing a maximum of 200 transactions per contract. The\
    \ first column displays the names of the tools, while the second column enumerates\
    \ the averaged number of invariants generated by each respective tool per contract.\
    \ The middle two columns showcase the overall Precision and RecallERC20 scores,\
    \ and the last column provides the averaged time usage for each tool.\n\nINVCON+\
    \ achieves the highest recall score, reaching 0.80, and generates approximately\
    \ 46 invariants per contract, all of which are successfully verified by VeriSol.\
    \ Notably, INVCON performs the least favorably in terms of the invariants generated,\
    \ even when compared with INVCON+ *Naive*. Specifically, INVCON produces the second-highest\
    \ number of invariants, yet its recall score is significantly lower than that\
    \ of INVCON+ *Naive*, while maintaining a similar precision score of less than\
    \ 0.1. The poor performance is primarily attributed to the fact that INVCON's\
    \ underlying invariant detection engine, Daikon, supports only Boolean, integer/float,\
    \ and string types native to Java. Consequently, the *address* type (20 bytes\
    \ long) in the Solidity language cannot be seamlessly converted into a Java integer\
    \ (8 bytes long). Its conversion to the Java string type discards semantic information,\
    \ rendering the straightforward production of common invariants (e.g., a1, a2\
    \ in Table [II\\)](#page-8-0) unattainable for INVCON. Additionally, Daikon employs\
    \ floating-point operations in arithmetic invariant templates (e.g., linear equation\
    \ templates), which is not allowed in the Solidity semantics, leading to incorrect\
    \ invariants for b6, b10 in Table [II.](#page-8-0)\n\nINVCON+ *Primitive* exhibits\
    \ a slightly lower recall score than INVCON+ *Naive*, because some ground truth\
    \ invariants\n\n<span id=\"page-7-0\"></span><sup>4</sup>https://github.[com/UCSB-PLSE/Cider](https://github.com/UCSB-PLSE/Cider)\n\
    \n| TABLE II: Common ERC20 Invariants. |\n|------------------------------------|\n\
    |------------------------------------|\n\n<span id=\"page-8-0\"></span>\n\n| Categories\
    \                             | Preconditions                                \
    \                                                                            \
    \                                                              | Postconditions\
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                              |  |\n|----------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--|\n\
    | transfer(to, amount)                   | [a1] msg.sender ̸= 0<br>[a2] to ̸=\
    \ address(0)<br>[a3] amount ≥ 0<br>[a4] amount ≤ balances[msg.sender]<br>[a5]\
    \ balances[to] + amount ≤ MAXVALUE                                        | [b1]\
    \ to ̸= msg.sender<br>=⇒<br>balances[msg.sender] =<br>old(balances[msg.sender])\
    \ - amount<br>[b2] to ̸= msg.sender =⇒ balances[to] = old(balances[to]) + amount<br>[b3]\
    \ to = msg.sender =⇒ balances[to] = old(balances[to])<br>[b4] to = msg.sender\
    \ =⇒ balances[msg.sender] = old(balances[msg.sender])<br>[b5] totalSupply = old(totalSupply)\
    \                                            |  |\n| transferFrom (from, to,<br>amount)\
    \     | [a6] from ̸= address(0)<br>[a7] to ̸= address(0)<br>[a8] amount ≥ 0<br>[a9]\
    \ amount ≤ balances[from]<br>[a10] amt ≤ allowed[from][msg.sender]<br>[a11] balances[to]\
    \ + amount ≤ MAXVALUE | [b6] allowed[from][msg.sender] = old(allowed[from][msg.sender])\
    \ - amount<br>[b7] from ̸= to =⇒ balances[from] = old(balances[from]) - amount<br>[b8]\
    \ from ̸= to =⇒ balances[to] = old(balances[to]) + amount<br>[b9] from = to =⇒\
    \ balances[from] = old(balances[from])<br>[b10] allowed[from][msg.sender] = old(allowed[from][msg.sender])\
    \ - amount<br>[b11] totalSupply = old(totalSupply) |  |\n| approve(spender,<br>amount)\
    \            | [a12] amount ≥ 0<br>[a13] spender ̸= address(0)               \
    \                                                                            \
    \                                             | [b12] allowed[msg.sender][spender]\
    \ = amount<br>[b13] totalSupply = old(totalSupply)                           \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                          |  |\n| increaseAllowance(<br>spender,\
    \ amount) | [a14] spender ̸= address(0)<br>[a15] amount ≥ 0<br>[a16] allowed[msg.sender][spender]\
    \ + amount ≤<br>MAXVALUE                                                     \
    \                      | [b14] allowed[msg.sender][spender] = old(allowed[msg.sender][spender])\
    \ +<br>amount<br>[b15] totalSupply = old(totalSupply)                        \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \      |  |\n| decreaseAllowance(<br>spender, amount) | [a17] spender ̸= address(0)<br>[a18]\
    \ amount ≥ 0<br>[a19] allowed[msg.sender][spender] ≥ amount                  \
    \                                                                       | [b16]\
    \ allowed[msg.sender][spender] = old(allowed[msg.sender][spender]) -<br>amount<br>[b17]\
    \ totalSupply = old(totalSupply)                                             \
    \                                                                            \
    \                                                                            \
    \                                                             |  |\n| mint(account,\
    \ amount)                  | [a20] account ̸= address(0)<br>[a21] amount ≥ 0<br>[a22]\
    \ balances[account] + amount ≤ MAXVALUE                                      \
    \                                                   | [b18] balances[account]\
    \ = old(balances[account]) + amount<br>[b19] totalSupply = old(totalSupply) +\
    \ amount                                                                     \
    \                                                                            \
    \                                                                            \
    \                                                     |  |\n| burn(from, amount)\
    \                     | [a23] from ̸= address(0)<br>[a24] amount ≥ 0<br>[a25]\
    \ balances[from] ≥ amount                                                    \
    \                                                      | [b20] balances[from]\
    \ = old(balances[from]) - amount<br>[b21] totalSupply = old(totalSupply) + amount\
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                    |  |\n| pause()         \
    \                       | [a26] paused = false                               \
    \                                                                            \
    \                                                        | [b22] paused = true\
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                         |  |\n| unpause()  \
    \                            | [a27] paused = true                           \
    \                                                                            \
    \                                                             | [b23] paused =\
    \ false                                                                      \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                              |  |\n| Contract\
    \ Invariant                     | [c1] totalSupply = SumMap(balances)        \
    \                                                                            \
    \                                                                |           \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                  |  |\n\n# TABLE\
    \ III: Common ERC721 Invariants.\n\n<span id=\"page-8-1\"></span>\n\n| Categories\
    \                                    | Preconditions                         \
    \                                                                            \
    \                                                                            \
    \                   | Postconditions                                         \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            |\
    \  |\n|-----------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--|\n\
    | (safe)-<br>transferFrom(from,<br>to, tokenId) | [a28] from =<br>tokenOwner[tokenId]<br>[a29]\
    \ from ̸= address(0)<br>[a30] to ̸= address(0)<br>[a31] (msg.sender = from ∨ msg.sender\
    \ =<br>tokenApprovals[tokenId] ∨<br>operatorApprovals[from][msg.sender] = true)\
    \ | [b24] from ̸= to =⇒<br>ownedTokensCount[from] = old(<br>ownedToken<br>sCount[from])\
    \ - 1<br>[b25] from ̸= to<br>=⇒<br>ownedTokensCount[to] = old(<br>ownedToken<br>sCount[to])\
    \ + 1<br>[b26] from = to =⇒<br>ownedTokensCount[from] = old(<br>ownedToken<br>sCount[from])<br>[b27]\
    \ from = to<br>=⇒<br>ownedTokensCount[to] = old(<br>ownedToken<br>sCount[to])<br>[b28]<br>tokenOwner[tokenId]\
    \ = to<br>[b29]<br>tokenApprovals[tokenId] = address(0) |  |\n| approve(to, tokenId)\
    \                          | tokenOwner[tokenId] ̸= address(0)<br>[a32]<br>tokenOwner[tokenId]\
    \ ∨<br>[a33] (msg.sender =<br>operatorApprovals[ tokenOwner[tokenId] ][msg.sender]<br>=\
    \ true)                                                    | [b30]<br>tokenApprovals[tokenId]\
    \ = to                                                                       \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                       |  |\n| setApproveForAll(<br>operator,<br>approved)  \
    \ | [a34] operator ̸= msg.sender                                             \
    \                                                                            \
    \                                                            | [b31]<br>operatorApprovals[msg.sender][operator]\
    \ =<br>approved                                                              \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \       |  |\n| Contract Invariant                            | [c2] len( tokenOwner)\
    \ = SumMap( ownerTokenCount)                                                 \
    \                                                                            \
    \                                    |                                       \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                 |  |\n\n<span id=\"page-8-2\"></span>\n\n| TABLE IV: The comparison\
    \ results on ERC20 contracts. |  |  |  |  |\n|------------------------------------------------------|--|--|--|--|\n\
    |------------------------------------------------------|--|--|--|--|\n\n| Tool\
    \              | #Inv   | Prec. | RecERC20 | Avg.time (s) |\n|-------------------|--------|-------|----------|--------------|\n\
    | INVCON            | 413.23 | 0.095 | 0.19     | 13.99        |\n| INVCON+ Naive\
    \     | 480.89 | 0.094 | 0.63     | 15.25        |\n| INVCON+ Primitive | 22.49\
    \  | 1.000 | 0.61     | 20.57        |\n| INVCON+           | 46.12  | 1.000 |\
    \ 0.80     | 250.25       |\n\nthat are inferred as likely invariants by INVCON+\
    \ *Naive* may not be verified by INVCON+ *Primitive*. This may be due to contract\
    \ implementations slightly deviating from the standard. For example, many ERC20\
    \ tokens do not enforce the precondition a2 of the transfer function in Table\
    \ [II,](#page-8-0) because transferring token to zero address could be used to\
    \ implement the token burning functionality. The verified invariants are a more\
    \ accurate reflection of the actual contract implementations, compared with the\
    \ likely invariants. Leveraging an algorithm capable of producing implications\
    \ that widely exist in ERC20 invariants (C.f. Table [II\\)](#page-8-0), INVCON+\
    \ outperforms all the baseline tools, yielding 100% precise invariant results.\n\
    \nRegarding the time usage, it is unsurprising that INVCON+ takes the most time,\
    \ whereas INVCON and INVCON+ *Naive* finish the fastest. In our experiments, we\
    \ observed that the static inference process consumes the majority of the time,\
    \ constituting nearly 53% of the overall time usage, as depicted in Fig. [9.](#page-10-0)\
    \ This is primarily due to the iterative application of static inference until\
    \ no more implication candidates are\n\n```\n1 /** @dev Creates `amount` tokens\
    \ and assigns them to\n  ,→ `account`,\n2 * increasing the total supply.\n3 *\
    \ Requirements\n4 * - `to` cannot be the zero address.*/\n5 function _mint(address\
    \ account, uint256 amount) internal\n  ,→ {\n6 require(account != address(0),\
    \ \"ERC20: mint to the zero\n   ,→ address\");\n7 _totalSupply = _totalSupply.add(amount);\n\
    8 _balances[account] = _balances[account].add(amount);\n9 _balances[Account] =\
    \ _totalSupply/100;\n10 }\n```\nFig. 7: TokenMintERC20Token contract violating\
    \ c1.\n\n<span id=\"page-9-2\"></span>TABLE V: The mutation testing results on\
    \ ERC20 contracts against the verified invariants by INVCON+.\n\n| Categories\
    \              | approve      | transfer     | transferFrom |\n|-------------------------|--------------|--------------|--------------|\n\
    | No. total mutants       | 1,539        | 1,141        | 297          |\n| No.\
    \ killed mutants      | 998 (64.8 %) | 624 (54.6 %) | 101 (34.0 %) |\n| P1. Contract\
    \ invariants | 245 (24.5 %) | 344 (55.1 %) | 55 (54.4 %)  |\n| P2. Function pre/post\
    \   | 763 (76.4 %) | 465 (74.5 %) | 61 (60.4 %)  |\n| P3. ERC20 standard     \
    \ | 751 (75.2 %) | 266 (42.6 %) | 43 (42.5 %)  |\n| P4. Non-ERC20 standard  |\
    \ 995 (99.7 %) | 601 (96.3 %) | 98 (97.0 %)  |\n\nprovided. Moreover, implication\
    \ invariants generated in later iterations tend to be more intricate, resulting\
    \ in more complicated SMT formulas which take more time to solve. To enhance the\
    \ efficiency of INVCON+, we recommend capping the iterations used in the verification\
    \ process to four; under such a setting, INVCON+ demonstrates an averaged time\
    \ savings of one minute in the entire verification process without compromising\
    \ the quality of resulting invariants.\n\nAdditionally, we investigated further\
    \ on the contracts for which INVCON+ generated additional invariants deviating\
    \ from the ground-truth ones. Many of these contracts are found to be non-compliant\
    \ with ERC20 specifications. As illustrated in Fig. [7,](#page-9-0) we examined\
    \ a real-world contract, TokenMintERC20Token[5](#page-9-1) , where the mint function\
    \ deviates from the contract invariant c1—the sum of account balances always equals\
    \ to the total supply—indicating noncompliance with the standard. This discrepancy\
    \ arises because only 1% of the total supply tokens have been distributed to the\
    \ Account (Line [9\\)](#page-9-0).\n\nInvariant Quality. Furthermore, to assess\
    \ the significance of the invariants generated by INVCON+, we conducted mutation\
    \ testing on the same benchmark and computed the corresponding mutation scores\
    \ against these invariants. Table [V](#page-9-2) presents the mutation testing\
    \ results on ERC20 contracts, specifically focusing on the three most important\
    \ functions: *approve*, *transfer*, and *transferFrom*. We introduced six mutation\
    \ operators, such as *binary/unary-op-mutation* and *require-mutation*, along\
    \ with the others, based on the mutation generator Gambit [\\[34\\]](#page-13-33)\
    \ developed by Certora.[6](#page-9-3) This mutation-based approach was also adopted\
    \ by Certora to evaluate the quality of smart contract specifications.[7](#page-9-4)\
    \ In total, we generated 1, 539, 1, 141, and 297 mutants for *approve*, *transfer*,\
    \ and *transferFrom*, respectively. Table [V](#page-9-2) shows that 64.8 %, 54.6\
    \ %, and 34.0 % mutants of\n\n```\n1 function _transfer(address sender, address\
    \ recipient,\n  ,→ uint256 amount) internal {\n2 require(sender!=address(0), \"\
    zero address\");\n3 require(recipient!=address(0), \"zero address\");\n4\n5 _balances[sender]=_balances[sender].sub(amount);\n\
    6 _balances[recipient]=_balances[recipient].add(amount);\n7 emit Transfer(sender,\
    \ recipient, amount);\n8 }\n9 function _approve(address owner, address spender,\n\
    \  ,→ uint256 value) internal {\n10 require(owner!=address(0), \"zero address\"\
    );\n11 require(spender!=address(0), \"zero address\");\n12\n13 _allowances[owner][spender]\
    \ = value;\n14 emit Approval(owner, spender, value);\n15 }\n16\n17 function transferFrom(address\
    \ sender, address recipient,\n  ,→ uint256 amount) public returns (bool) {\n18\
    \ _transfer(sender, recipient, amount);\n19 _approve(sender, msg.sender,\n   ,→\
    \ _allowances[sender][msg.sender].sub(amount));\n20 return true;\n21 }\n```\n\
    Fig. 8: Illustration of the overprotected *transferFrom* function.\n\napprove,\
    \ transfer, and transferFrom are successfully killed, respectively.\n\nTo delve\
    \ into those killed mutants, in Table [V,](#page-9-2) we use P1, P2, P3, and P4\
    \ to denote different types of invariants and the corresponding rows show the\
    \ number of killed mutants by these invariants. Although the contract invariants\
    \ (P1) accounts for 24.5 % to 54,4 % of the killed mutants, function pre/post-conditions\
    \ (P2) demonstrate a more substantial impact occupying at most 76.4 % of the killed\
    \ mutants. Moreover, non-ERC20 standard invariants successfully eliminate nearly\
    \ the entire set (96 % more) of the total killed mutants. In contrast, ERC20 standard\
    \ invariants eliminate a smaller set of mutants. This suggests that the invariants\
    \ generated by INVCON+ capture richer program semantics, contributing to a more\
    \ comprehensive set of invariant specifications for smart contracts.\n\nInterestingly,\
    \ Table [V](#page-9-2) reveals that only 34% of mutants related to the *transferFrom*\
    \ function are successfully eliminated. Upon investigation, we discovered that\
    \ *transferFrom* is overprotected, where one of its function-level preconditions\
    \ is redundant. Figure [8](#page-9-5) depicts a common implementation of *transferFrom*,\
    \ facilitating token transfer on behalf of the token owner through two internal\
    \ functions, *transfer* and *approve*. This design rationale primarily aims at\
    \ direct code reuse for the other two public functions, *transfer* and *approve*.\
    \ However, in the *transferFrom* function, both requirements (Line [2](#page-9-5)\
    \ and Line [10\\)](#page-9-5) check if the *sender* parameter is a zero address.\
    \ Consequently, mutations on either Line [2](#page-9-5) or Line [10](#page-9-5)\
    \ do not diminish the requirements that *transferFrom* should adhere to, resulting\
    \ in a low mutation score for *transferFrom*. It is noteworthy that redundant\
    \ requirements in smart contracts lead to higher gas consumption during transaction\
    \ execution and should be minimized whenever possible.\n\nInvariant Crowdsourcing.\
    \ Less popular smart contracts may have scarce transaction histories. For example,\
    \ many ERC721 contract instances may not have enough transactions to infer high-quality\
    \ invariants. Each contract instance can slightly deviate from the standard specifications,\
    \ therefore, we\n\n<span id=\"page-9-3\"></span><span id=\"page-9-1\"></span><sup>5</sup>https://etherscan.[io/address/0x62c23c5f75940c2275dd3cb9300289dd30992e59](https://etherscan.io/address/0x62c23c5f75940c2275dd3cb9300289dd30992e59)\
    \ <sup>6</sup>https://www.certora.com/\n\n<span id=\"page-9-4\"></span><sup>7</sup>https://docs.certora.com/en/latest/docs/gambit/index.html\n\
    \n<span id=\"page-10-1\"></span>TABLE VI: ERC721 invariants generated by INVCON+.\n\
    \n| Category                | Preconditions   | Postconditions               \
    \     |\n|-------------------------|-----------------|-----------------------------------|\n\
    | (safe)-<br>transferFrom | [a28, a29, a30] | [b24, b25, b26, b27, b28,<br>b29]\
    \ |\n| approve                 | [a32]           | [b30]                     \
    \        |\n| setApproveForAll        | [a34]           | [b31]              \
    \               |\n| Contract Invariant      |                 | [c2]        \
    \                      |\n|                         |                 |      \
    \                             |\n\n<span id=\"page-10-0\"></span>![](_page_10_Figure_3.jpeg)\n\
    \nFig. 9: The time usage by different components of INVCON+.\n\nhypothesize that\
    \ reverse engineering invariants from a single contract and its limited transaction\
    \ histories is inferior to that from multiple contracts. We validate this hypothesize\
    \ on a set of 10 ERC721 contracts, restricting the evaluation to at most 200 transactions\
    \ per contract. The objective is to examine IN-VCON+'s effectiveness in recovering\
    \ the ground truth invariants listed in Table [III](#page-8-1) by combining invariant\
    \ results from multiple contracts. Notably, to achieve meaningful combination,\
    \ every invariant result will be normalized according to a universal ERC721 definition\
    \ on the name of state variables and the name of function input variables.\n\n\
    Table [VI](#page-10-1) presents the combined invariant results from all ERC721\
    \ contracts. It demonstrates that INVCON+ successfully recovers the contract invariants,\
    \ all postconditions, and nearly all preconditions except a31 and a33, which contain\
    \ disjunctions over predicates. Consequently, the combination of invariant results\
    \ from multiple contracts significantly improves the overall recall rate (14/16).\n\
    \nAnswer to RQ1: INVCON+ is able to reverse engineer standard invariant specifications\
    \ from contract transaction histories and takes no more than five minutes per\
    \ contract. Additionally, the uncommon invariants generated for ERC20 contracts\
    \ capture important program semantics beyond the established standards. Moreover,\
    \ the evaluation on ERC721 contracts demonstrates the advantage to mine common\
    \ invariants from multiple contracts and their transaction histories.\n\n# *D.\
    \ RQ2: Impact of Transaction Histories*\n\nThe length of the transaction histories\
    \ used can influence the effectiveness of INVCON+. To investigate this impact,\
    \ we selected the top 10 ERC20 contracts with the longest transaction histories,\
    \ ensuring that all the chosen contracts have a history of at least 10,000 transactions.\
    \ In evaluating the influence of transaction history length, we employed the earliest\
    \ 4,000\n\n<span id=\"page-10-2\"></span>![](_page_10_Figure_10.jpeg)\n\nFig.\
    \ 10: The averaged number of invariants generated with different number of transactions.\n\
    \n<span id=\"page-10-3\"></span>![](_page_10_Figure_12.jpeg)\n\nFig. 11: The averaged\
    \ ERC20 recall score of the invariant results generated with different number\
    \ of transactions.\n\ntransactions and divided them into 20 groups, each subsequent\
    \ group having 200 more transactions than the previous one.\n\nWe utilized INVCON+\
    \ *Primitive* as the baseline and compared with it on the number of verified invariants\
    \ and the corresponding recall score. Additionally, to explore the effect of applying\
    \ the detected partially supported invariant candidates, which hold for a subset\
    \ of the transaction histories, we compared INVCON+ with a variant, INVCON+ *w/o\
    \ Partial*, that does not use these partial candidates. In this experiment, we\
    \ considered the ground truth invariants from the functions which are observed\
    \ in the earliest 4,000 transactions, when computing the recall score, i.e., RecallERC20.\n\
    \nFigure [10](#page-10-2) illustrates the number of verified invariants per contract\
    \ corresponding to the use of different transaction history lengths. The impact\
    \ of transaction history size on the number of verified invariants is evident,\
    \ with INVCON+ generating the most invariants, followed by INVCON+ INVCON+ *w/o\
    \ Partial*. This demonstrates that the partially supported invariant candidates,\
    \ although do not hold on their own, may be useful in constructing richer implication\
    \ invariants. By incorporating partial invariant candidates, INVCON+ captures\
    \ subtle contract behaviors more effectively, resulting in more comprehensive\
    \ invariant specifications—approximately two times and one time more than INVCON+\
    \ *w/o Partial* and INVCON+ *Primitive*, respectively.\n\nIn Fig. [11,](#page-10-3)\
    \ the recall score of invariant results is presented for varying transaction history\
    \ lengths. Clearly, all recall scores increase with longer transaction histories,\
    \ as more function invocations are observed. Notably, INVCON+ achieves a higher\
    \ recall score compared to the baselines. The figure also indicates a more significant\
    \ gain in recall score from 200 to 400 transactions, with negligible gains after\
    \ 400, 1,000, and 2,200 transactions for INVCON+ *Primitive*, INVCON+ *w/o Partial*,\
    \ and INVCON+, respectively. This observed difference suggests that INVCON+ has\
    \ a higher chance of capturing more comprehensive invariant specifications with\
    \ increased transaction histories. Additionally, to effectively apply INVCON+,\
    \ it is recommended to use around 2,000 transactions for invariant detection.\n\
    \nAnswer to RQ2: The scale of transaction histories affect the invariant results\
    \ of INVCON+, while longer histories empower INVCON+ to generate more comprehensive\
    \ invariant specifications.\n\n# *E. RQ3: Application in Securing Smart Contracts*\n\
    \nThe invariants generated by INVCON+ capture the key semantics of smart contracts\
    \ under normal executions, which may serve as a basis for formal contract specifications.\
    \ Highquality contract specifications have been shown to be effective in securing\
    \ smart contracts through runtime validation [\\[14\\]](#page-13-13) and static\
    \ verification [\\[26\\]](#page-13-25). To answer RQ3, we evaluated INVCON+ on\
    \ a set of benchmark contracts from SECBIT [\\[35\\]](#page-13-34), which contains\
    \ 25 types of vulnerabilities in real-world ERC20 contracts exposed to security\
    \ attacks that have resulted in significant financial losses.\n\nTable [VII](#page-12-1)\
    \ provides an overview of the verification results for the evaluated ERC20 contracts,\
    \ categorized by vulnerability types. It contains information about the overall\
    \ count of vulnerabilities and the effectiveness of our generated invariants in\
    \ detecting them. The benchmark contracts used in our evaluation encompass 9 instances\
    \ of integer overflow vulnerabilities and 16 other vulnerability types. However,\
    \ certain vulnerabilities are beyond the scope of formal specifications, such\
    \ as v14, v21, and v24 which are related to constructor naming, v15 and v16 which\
    \ are associated with different Solidity versions, and v23 which pertains to function\
    \ visibility. We focused on the remaining 18 types of vulnerabilities. Note that\
    \ some of the vulnerabilities identified are beyond the specifications outlined\
    \ in the ERC20 standard (see Table [II\\)](#page-8-0) and they can only be detected\
    \ using richer customized specifications.\n\nFor each of vulnerability types,\
    \ we evaluated the verification results of the invariants generated by INVCON+\
    \ on the corresponding benchmark contracts. We selected the top three contracts\
    \ with the highest occurrence of each vulnerability type and assessed whether\
    \ the invariants detected by INVCON+ could prevent the corresponding attacks on\
    \ these contracts. The results are shown in Table [VII.](#page-12-1) We found\
    \ that INVCON+ was able to detect all overflow vulnerabilities in the benchmark\
    \ contracts. For instance, Fig. [12](#page-11-1) demonstrated that INVCON+ detected\
    \ the integer overflow vulnerability (CVE-2018-10299) in the *batchTransfer* function\
    \ of the BEC contract. This\n\n```\n1 function batchTransfer(address[] _receivers,\n\
    \      uint256 _value) public whenNotPaused returns\n      (bool) {\n  ,→\n  ,→\n\
    2 uint cnt = _receivers.length;\n3 uint256 amount = uint256(cnt) * _value;\n4\
    \ require(cnt > 0 && cnt <= 20);\n5 require(_value > 0 && balances[msg.sender]\
    \ >=\n  ,→ amount);\n6\n7 [msg.sender] = balances[msg.sender].sub(amount);\n8\
    \ for (uint i = 0; i < cnt; i++) {\n9 balances[_receivers[i]] =\n  ,→ balances[_receivers[i]].add(_value);\n\
    10 Transfer(msg.sender, _receivers[i], _value);\n11 }\n12 return true;\n13 }\n\
    ```\nFig. 12: batchTransfer function in BEC contract.\n\nvulnerability is caused\
    \ by the unchecked multiplication of cnt and value in Line [3.](#page-11-1) If\
    \ an attacker calls batchTransfer with a large cnt value, the unsigned integer\
    \ amount will overflow, potentially allowing the attacker to receive more tokens\
    \ than intended. However, such a transaction would violate invariant c1 in Table\
    \ [II,](#page-8-0) as the totalSupply would no longer equal to the sum of all\
    \ balances. Thus, such an attack can be effectively prevented, if the generated\
    \ invariants are enforced for each function execution.\n\nINVCON+ is unable to\
    \ detect some remarkable mistakes that totally deviate from programmer expectations.\
    \ For example, v11 is a vulnerability that allows any party to halt the token\
    \ transfer process. This issue arises from the modification of the onlyFromWallet\
    \ modifier, wherein \"==\" was mistakenly replaced with \"!=\". Consequently,\
    \ anyone other than walletAddress can arbitrarily invoke the two permissioned\
    \ functions: enableTokenTransfer and disableTokenTransfer. INVCON+ failed to detect\
    \ this vulnerability for two primary reasons. First, the onlyFromWallet function\
    \ is not specified in the ERC20 standard, preventing the application of the existing\
    \ invariant templates. Second, the contract histories contain many irregular behaviors\
    \ exploiting these functions, hindering INVCON+ from inferring correct invariants\
    \ related to onlyFromWallet.\n\nAnswer to RQ3: INVCON+ is able to detect invariants\
    \ that are useful for preventing real-world smart contract vulnerabilities. Enforcing\
    \ invariants in contract executions may ensure the security and reliability of\
    \ smart contracts.\n\n# VII. RELATED WORK\n\n<span id=\"page-11-0\"></span>The\
    \ related works can be broadly categorized into smart contract security analysis\
    \ and invariant inference.\n\n# *A. Smart Contract Security Analysis*\n\nThe security\
    \ analysis primarily focuses on detecting smart contract vulnerabilities. Common\
    \ vulnerabilities in smart contracts include integer overflow/underflow [\\[36\\\
    ]](#page-13-35), reentrancy [\\[37\\]](#page-13-36), and dangerous delegatecall\
    \ operations [\\[38\\]](#page-13-37). For instance, in 2017, the Parity wallet\
    \ contract was hacked due to missing protection for the delegatecall operation,\
    \ a feature that allows\n\n<span id=\"page-12-1\"></span>\n\n| ID  | Vulnerability\
    \ Types                           | Total | Detected |\n|-----|-----------------------------------------------|-------|----------|\n\
    | v1  | batchTransfer-overflow                        | 13    | Yes      |\n|\
    \ v2  | totalsupply-overflow                          | 521   | Yes      |\n|\
    \ v3  | verify-invalid-by-overflow                    | 2     | Yes      |\n|\
    \ v4  | owner-control-sell-price-for<br>overflow      | 1     | Yes      |\n|\
    \ v5  | owner-overweight-token-by<br>overflow         | 9     | Yes      |\n|\
    \ v6  | owner-decrease-balance-by-mint<br>by-overflow | 487   | Yes      |\n|\
    \ v7  | excess-allocation-by-overflow                 | 1     | Yes      |\n|\
    \ v8  | excess-mint-token-by-overflow                 | 9     | Yes      |\n|\
    \ v9  | excess-buy-token-by-overflow                  | 4     | Yes      |\n|\
    \ v10 | verify-reverse-in-transferFrom                | 79    | Yes      |\n|\
    \ v11 | pauseTransfer-anyone                          | 1     | No       |\n|\
    \ v12 | transferProxy-keccak256                       | 10    | Yes      |\n|\
    \ v13 | approveProxy-keccak256                        | 10    | Yes      |\n|\
    \ v14 | constructor-case-insensitive                  | 4     | N/A      |\n|\
    \ v15 | custom-fallback-bypass-ds-auth                | 1     | N/A      |\n|\
    \ v16 | custom-call-abuse                             | 144   | N/A      |\n|\
    \ v17 | setowner-anyone                               | 3     | Yes      |\n|\
    \ v18 | allowAnyone                                   | 4     | Yes      |\n|\
    \ v19 | approve-with-balance-verify                   | 18    | Yes      |\n|\
    \ v20 | check-effect-inconsistency                    | 1     | Yes      |\n|\
    \ v21 | constructor-mistyping                         | 4     | N/A      |\n|\
    \ v22 | fake-burn                                     | 2     | Yes      |\n|\
    \ v23 | getToken-anyone                               | 3     | N/A      |\n|\
    \ v24 | constructor-naming-error                      | 1     | N/A      |\n\n\
    one contract to securely delegate part of its functionality to another contract.\
    \ As a result, the attacker gained control of the wallet and stole 150,000 ETH,\
    \ valued at approximately \\$30 million USD at the time.\n\nThese common vulnerabilities\
    \ have been extensively studied in [\\[13\\]](#page-13-12), [\\[39\\]](#page-13-38),\
    \ [\\[40\\]](#page-13-39), [\\[41\\]](#page-13-40), [\\[42\\]](#page-13-41), [\\\
    [43\\]](#page-13-42), [\\[44\\]](#page-13-43), [\\[45\\]](#page-14-0), [\\[46\\\
    ]](#page-14-1), [\\[47\\]](#page-14-2), [\\[48\\]](#page-14-3), [\\[49\\]](#page-14-4),\
    \ [\\[50\\]](#page-14-5). Most static analysis tools, such as Slither [\\[23\\\
    ]](#page-13-22), Securify [\\[42\\]](#page-13-41), Zeus [\\[47\\]](#page-14-2),\
    \ and Ethainter [\\[40\\]](#page-13-39), utilize controlflow, data-flow, or taint-flow\
    \ analysis for vulnerability detection, usually achieving a high recall but low\
    \ precision rate. In contrast, the others [\\[45\\]](#page-14-0), [\\[46\\]](#page-14-1),\
    \ [\\[51\\]](#page-14-6) use symbolic execution for program path exploration to\
    \ identify contract vulnerabilities, along with a higher precision but lower recall\
    \ rate. There are also formal verification tools for ensuring the correctness\
    \ of functional properties [\\[16\\]](#page-13-15), [\\[17\\]](#page-13-16), [\\\
    [52\\]](#page-14-7), and workflow policy [\\[26\\]](#page-13-25) in smart contracts.\
    \ The dynamic analyses [\\[48\\]](#page-14-3), [\\[50\\]](#page-14-5), [\\[53\\\
    ]](#page-14-8), [\\[54\\]](#page-14-9), [\\[55\\]](#page-14-10) perform random\
    \ or model-based testing on smart contracts and then check execution result against\
    \ predefined oracles for finding a wide range of vulnerabilities. Although these\
    \ tools have been proven effective in detecting common vulnerabilities, unfortunately,\
    \ Zhang et al. [\\[56\\]](#page-14-11) found that only 20.5% of realworld smart\
    \ contract bugs can be successfully detected by state-of-the-art tools. This is\
    \ because the existing tools use simple, generic, and hard-coded security patterns\
    \ or oracles, which are ineffective to recognize subtle logic bugs on specific\
    \ contracts.\n\nBecause there is no one-for-all patterns or oracles for identifying\
    \ contract logic bugs, most valued Web3 projects hire third-party security auditing\
    \ companies to manually review their contracts. Despite undergoing costly code\
    \ auditing, numerous projects still fall victim to security breaches [\\[57\\\
    ]](#page-14-12). In our opinion, one root cause is that contract developers and\
    \ the corresponding auditors may have divergent expectations on smart contracts,\
    \ which are not easy to pinpoint without sufficient contract specifications. Therefore,\
    \ apart from enhancing existing security tools, the invariants generated by INVCON+\
    \ can reinforce contract specifications to mitigate the incompleteness and inaccuracy\
    \ issues of automated verification and contract auditing.\n\n# *B. Invariant Inference*\n\
    \nThe static and dynamic invariant inference have been wellstudied for traditional\
    \ programs. ESC/Java [\\[58\\]](#page-14-13) is a wellknown static checking tool\
    \ for Java programs. It leverages invariant annotations to define properties in\
    \ the code, improving the precision of static checking. ESC/Java's emphasis on\
    \ invariants helps developers express expectations precisely, allowing potential\
    \ issues to be detected early in development. Daikon [\\[11\\]](#page-13-10) is\
    \ a well-known dynamic invariant detection tool to automatically infer likely\
    \ invariants from program executions. Daikon takes program execution traces as\
    \ input, which are typically obtained through testing. These execution traces\
    \ consist of sequences of program states and variable values observed during the\
    \ program's runtime. InvCon [\\[18\\]](#page-13-17) was the first tool that generates\
    \ *likely* invariants for smart contracts. With Daikon as the back-end invariant\
    \ detection engine, InvCon implemented an intermediary input transformer that\
    \ coverts historic contract transactions to the compatible data trace files accepted\
    \ by Daikon. In addition, some invariant templates are customized to support unique\
    \ Solidity features, e.g., MappingItem.\n\nThere also exist other works related\
    \ to invariant generation for smart contracts. SolType [\\[15\\]](#page-13-14)\
    \ is a type checking tool for Solidity smart contracts. It enables developers\
    \ to add refinement type annotations to smart contracts, incorporating static\
    \ analysis to prove that arithmetic operations are safe from integer overflows\
    \ or underflows. SolType can infer useful type annotations, but they are limited\
    \ to only contract-level invariants related to arithmetic operation. Using SolType\
    \ as a verifier to learn a policy, Cider [\\[19\\]](#page-13-18) applys deep reinforcement\
    \ learning to automatically learn contract invariants. The learned contract invariants\
    \ are mainly used to guard arithmetic operations in smart contracts to avoid integer\
    \ overflows and underflows. However, the correctness of the learned contract invariants\
    \ is still not formally verified.\n\nDistinguished from the aforementioned works,\
    \ INVCON+ is the first to implement a unified invariant generation framework for\
    \ Solidity contracts encompassing techniques from both dynamic detection and static\
    \ inference, where the the generated invariants are verified against the contract\
    \ code.\n\n# VIII. CONCLUSION\n\n<span id=\"page-12-0\"></span>We have presented\
    \ INVCON+, a novel invariant generation framework for Solidity smart contracts\
    \ where the invariants result from the integration between dynamic invariant detection\
    \ and static inference. Because implication invariants are important to capture\
    \ more fine-grained program semantics of smart contracts, INVCON+ devises an iterative\
    \ process to\n\nrepeat the generation and verification of implications to overcome\
    \ its combination explosion problem. We have evaluated INVCON+ on real-world ERC20\
    \ and ERC721 contracts and demonstrated that INVCON+ is able to achieve good recall\
    \ to recover common specifications. In addition, the experiments on mutation testing\
    \ and vulnerable benchmark contracts have shown that the invariant specifications\
    \ generated are effective to exclude program mistakes and make contracts secure\
    \ from vulnerabilities.\n\n# REFERENCES\n\n- <span id=\"page-13-0\"></span>[1]\
    \ G. Wood, \"Ethereum: A secure decentralised generalised transaction ledger,\"\
    \ *Ethereum project yellow paper*, vol. 151, pp. 1–32, 2014.\n- <span id=\"page-13-1\"\
    ></span>[2] \"Binance Smart Chain,\" https://docs.binance.[org/smart-chain/guides/bsc](https://docs.binance.org/smart-chain/guides/bsc-intro.html)intro.[html,](https://docs.binance.org/smart-chain/guides/bsc-intro.html)\
    \ 2020, introduction of Binance Smart Chain.\n- <span id=\"page-13-2\"></span>[3]\
    \ T. Chen, Y. Zhang, Z. Li, X. Luo, T. Wang, R. Cao, X. Xiao, and X. Zhang, \"\
    Tokenscope: Automatically detecting inconsistent behaviors of cryptocurrency tokens\
    \ in Ethereum,\" in *Proceedings of the 2019 ACM SIGSAC conference on computer\
    \ and communications security*, 2019, pp. 1503–1520.\n- <span id=\"page-13-3\"\
    ></span>[4] H.-A. Moon and S. Park, \"Conformance evaluation of the top-100 Ethereum\
    \ token smart contracts with Ethereum Request for Comment-20 functional specifications,\"\
    \ *IET Software*, vol. 16, no. 2, pp. 233–249, 2022.\n- <span id=\"page-13-4\"\
    ></span>[5] \"EIP-20: A standard interface for tokens,\" [https://eips](https://eips.ethereum.org/EIPS/eip-20).ethereum.org/EIPS/\
    \ [eip-20,](https://eips.ethereum.org/EIPS/eip-20) 2015.\n- <span id=\"page-13-5\"\
    ></span>[6] Y. Guo, *An Incompatibility in Ethereum Smart Contract Threatening\
    \ dApp Ecosystem*, 2018. [Online]. Available: https://medium.loopring.[io/an-incompatibility-in-smart-contract](https://medium.loopring.io/an-incompatibility-in-smart-contract-threatening-dapp-ecosystem-72b8ca5db4da)[threatening-dapp-ecosystem-72b8ca5db4da](https://medium.loopring.io/an-incompatibility-in-smart-contract-threatening-dapp-ecosystem-72b8ca5db4da)\n\
    - <span id=\"page-13-6\"></span>[7] A. Hui, \"Ethereum tokens worth \\$1b vulnerable\
    \ to 'Fake Deposit Attack',\" 2020. [Online]. Available: https://www.coindesk.[com/tech/2020/08/25/](https://www.coindesk.com/tech/2020/08/25/ethereum-tokens-worth-1b-vulnerable-to-fake-deposit-attack/)\
    \ [ethereum-tokens-worth-1b-vulnerable-to-fake-deposit-attack/](https://www.coindesk.com/tech/2020/08/25/ethereum-tokens-worth-1b-vulnerable-to-fake-deposit-attack/)\n\
    - <span id=\"page-13-7\"></span>[8] \"OpenZeppelin,\" https://github.[com/OpenZeppelin/openzeppelin](https://github.com/OpenZeppelin/openzeppelin-contracts)[contracts,](https://github.com/OpenZeppelin/openzeppelin-contracts)\
    \ 2022, openZeppelin contracts.\n- <span id=\"page-13-8\"></span>[9] \"Inconsistency\
    \ between the code and the doc of VestingWallet.release,\" https://github.[com/OpenZeppelin/openzeppelin-contracts/](https://github.com/OpenZeppelin/openzeppelin-contracts/issues/3368)\
    \ [issues/3368,](https://github.com/OpenZeppelin/openzeppelin-contracts/issues/3368)\
    \ 2022.\n- <span id=\"page-13-9\"></span>[10] C. Zhu, Y. Liu, X. Wu, and Y. Li,\
    \ \"Identifying Solidity smart contract API documentation errors,\" in *Proceedings\
    \ of the 37th IEEE/ACM International Conference on Automated Software Engineering\
    \ (ASE)*, Oct. 2022.\n- <span id=\"page-13-10\"></span>[11] \"Daikon,\" http://plse.cs.washington.[edu/daikon/,](http://plse.cs.washington.edu/daikon/)\
    \ 2021, the Daikon invariant detector.\n- <span id=\"page-13-11\"></span>[12]\
    \ C. Flanagan and K. R. M. Leino, \"Houdini, an annotation assistant for esc/java,\"\
    \ in *International Symposium of Formal Methods Europe*. Springer, 2001, pp. 500–517.\n\
    - <span id=\"page-13-12\"></span>[13] H. Wang, Y. Liu, Y. Li, S.-W. Lin, C. Artho,\
    \ L. Ma, and Y. Liu, \"Oracle-supported dynamic exploit generation for smart contracts,\"\
    \ *IEEE Transactions on Dependable and Secure Computing*, 2020.\n- <span id=\"\
    page-13-13\"></span>[14] A. Li, J. A. Choi, and F. Long, \"Securing smart contract\
    \ with runtime validation,\" in *Proceedings of the 41st ACM SIGPLAN Conference\
    \ on Programming Language Design and Implementation*, 2020, pp. 438–453.\n- <span\
    \ id=\"page-13-14\"></span>[15] B. Tan, B. Mariano, S. K. Lahiri, I. Dillig, and\
    \ Y. Feng, \"Soltype: refinement types for arithmetic overflow in solidity,\"\
    \ *Proceedings of the ACM on Programming Languages*, vol. 6, no. POPL, pp. 1–29,\
    \ 2022.\n- <span id=\"page-13-15\"></span>[16] S. So, M. Lee, J. Park, H. Lee,\
    \ and H. Oh, \"VeriSmart: A highly precise safety verifier for Ethereum smart\
    \ contracts,\" in *2020 IEEE Symposium on Security and Privacy (SP)*. IEEE, 2020,\
    \ pp. 1678–1694.\n- <span id=\"page-13-16\"></span>[17] A. Hajdu and D. Jovanovi\
    \ ´ c, \"solc-verify: A modular verifier for solidity ´ smart contracts,\" in\
    \ *Verified Software. Theories, Tools, and Experiments: 11th International Conference,\
    \ VSTTE 2019, New York City, NY, USA, July 13–14, 2019, Revised Selected Papers\
    \ 11*. Springer, 2020, pp. 161–179.\n- <span id=\"page-13-17\"></span>[18] Y.\
    \ Liu and Y. Li, \"Invcon: A dynamic invariant detector for ethereum smart contracts,\"\
    \ in *Proceedings of the 37th IEEE/ACM International Conference on Automated Software\
    \ Engineering*, 2022, pp. 1–4.\n- <span id=\"page-13-18\"></span>[19] J. Liu,\
    \ Y. Chen, B. Tan, I. Dillig, and Y. Feng, \"Learning contract invariants using\
    \ reinforcement learning,\" in *Proceedings of the 37th IEEE/ACM International\
    \ Conference on Automated Software Engineering*, 2022, pp. 1–11.\n- <span id=\"\
    page-13-19\"></span>[20] \"Openzeppelin erc20 contract specifications.\" [Online].\
    \ Available: https://github.[com/OpenZeppelin/openzeppelin-contracts/blob/](https://github.com/OpenZeppelin/openzeppelin-contracts/blob/master/certora/specs/ERC20.spec)\
    \ [master/certora/specs/ERC20](https://github.com/OpenZeppelin/openzeppelin-contracts/blob/master/certora/specs/ERC20.spec).spec\n\
    - <span id=\"page-13-20\"></span>[21] \"Openzeppelin erc721 contract specifications.\"\
    \ [Online]. Available: https://github.[com/OpenZeppelin/openzeppelin-contracts/blob/](https://github.com/OpenZeppelin/openzeppelin-contracts/blob/master/certora/specs/ERC721.spec)\
    \ [master/certora/specs/ERC721](https://github.com/OpenZeppelin/openzeppelin-contracts/blob/master/certora/specs/ERC721.spec).spec\n\
    - <span id=\"page-13-21\"></span>[22] G. Ros,u, \"ERC20-K: Formal Executable Specification\
    \ of ERC20,\" Mar. 2023, original-date: 2017-11-20T22:28:46Z. [Online]. Available:\
    \ https://github.[com/runtimeverification/erc20-semantics](https://github.com/runtimeverification/erc20-semantics)\n\
    - <span id=\"page-13-22\"></span>[23] \"Slither,\" https://github.[com/crytic/slither,](https://github.com/crytic/slither)\
    \ 2021, the Solidity Source Analyzer.\n- <span id=\"page-13-23\"></span>[24] H.\
    \ Wang, Y. Li, S.-W. Lin, L. Ma, and Y. Liu, \"VULTRON: Catching vulnerable smart\
    \ contracts once and for all,\" in *Proceedings of the 41st International Conference\
    \ on Software Engineering: New Ideas and Emerging Results (ICSE-NIER)*. IEEE Press,\
    \ 5 2019, pp. 1–4.\n- <span id=\"page-13-24\"></span>[25] M. Barnett, B.-Y. E.\
    \ Chang, R. DeLine, B. Jacobs, and K. R. M. Leino, \"Boogie: A modular reusable\
    \ verifier for object-oriented programs,\" in *Formal Methods for Components and\
    \ Objects: 4th International Symposium, FMCO 2005, Amsterdam, The Netherlands,\
    \ November 1-4, 2005, Revised Lectures 4*. Springer, 2006, pp. 364–387.\n- <span\
    \ id=\"page-13-25\"></span>[26] Y. Wang, S. K. Lahiri, S. Chen, R. Pan, I. Dillig,\
    \ C. Born, and I. Naseer, \"Formal specification and verification of smart contracts\
    \ for Azure blockchain,\" *arXiv preprint arXiv:1812.08829*, 2018.\n- <span id=\"\
    page-13-26\"></span>[27] M. Research, \"Z3,\" https://github.[com/Z3Prover/z3,](https://github.com/Z3Prover/z3)\
    \ 2022, accessed: December 15, 2023.\n- <span id=\"page-13-27\"></span>[28] E.\
    \ Hildenbrandt, M. Saxena, N. Rodrigues, X. Zhu, P. Daian, D. Guth, B. Moore,\
    \ D. Park, Y. Zhang, A. Stefanescu *et al.*, \"KEVM: A complete formal semantics\
    \ of the Ethereum virtual machine,\" in *2018 IEEE 31st Computer Security Foundations\
    \ Symposium (CSF)*. IEEE, 2018, pp. 204–217.\n- <span id=\"page-13-28\"></span>[29]\
    \ X. Li, C. Su, Y. Xiong, W. Huang, and W. Wang, \"Formal verification of bnb\
    \ smart contract,\" in *2019 5th International Conference on Big Data Computing\
    \ and Communications (BIGCOM)*. IEEE, 2019, pp. 74–78.\n- <span id=\"page-13-29\"\
    ></span>[30] E. ETL, \"Ethereum in BigQuery: a public dataset for smart contract\
    \ analytics,\" https://cloud.google.[com/blog/products/data-analytics/ethereum](https://cloud.google.com/blog/products/data-analytics/ethereum-bigquery-public-dataset-smart-contract-analytics)[bigquery-public-dataset-smart-contract-analytics,](https://cloud.google.com/blog/products/data-analytics/ethereum-bigquery-public-dataset-smart-contract-analytics)\
    \ 2017.\n- <span id=\"page-13-30\"></span>[31] \"Openzeppelin pausable contract\
    \ specifications.\" [Online]. Available: https://github.[com/OpenZeppelin/openzeppelin-contracts/blob/](https://github.com/OpenZeppelin/openzeppelin-contracts/blob/master/certora/specs/Pausable.spec)\
    \ [master/certora/specs/Pausable](https://github.com/OpenZeppelin/openzeppelin-contracts/blob/master/certora/specs/Pausable.spec).spec\n\
    - <span id=\"page-13-31\"></span>[32] OpenZeppelin, \"ERC 20 - OpenZeppelin Docs.\"\
    \ [Online]. Available: https://docs.openzeppelin.com/contracts/3.[x/api/token/ERC20](https://docs.openzeppelin.com/contracts/3.x/api/token/ERC20)\n\
    - <span id=\"page-13-32\"></span>[33] \"ERC 721 - OpenZeppelin Docs.\" [Online].\
    \ Available: [https:](https://docs.openzeppelin.com/contracts/2.x/api/token/ERC721)\
    \ //docs.openzeppelin.com/contracts/2.[x/api/token/ERC721](https://docs.openzeppelin.com/contracts/2.x/api/token/ERC721)\n\
    - <span id=\"page-13-33\"></span>[34] Certora, \"Gambit: Mutant generation for\
    \ Solidity,\" [https://github](https://github.com/Certora/gambit).com/ [Certora/gambit,](https://github.com/Certora/gambit)\
    \ 2022, accessed: December 9, 2023.\n- <span id=\"page-13-34\"></span>[35] S.\
    \ Labs, \"sec-bit/awesome-buggy-erc20-tokens: A Collection of Vulnerabilities\
    \ in ERC20 Smart Contracts With Tokens Affected,\" Aug. 2018. [Online]. Available:\
    \ https://github.[com/sec-bit/awesome-buggy](https://github.com/sec-bit/awesome-buggy-erc20-tokens)[erc20-tokens](https://github.com/sec-bit/awesome-buggy-erc20-tokens)\n\
    - <span id=\"page-13-35\"></span>[36] Blockchain-Projects, \"Overflow attack in\
    \ Ethereum smart contracts,\" https://blockchain-projects.readthedocs.io/overflow.html,\
    \ 2020.\n- <span id=\"page-13-36\"></span>[37] D. Siegel, *Understanding The DAO\
    \ Attack*, 2016. [Online]. Available: https://www.coindesk.[com/understanding-dao-hack-journalists](https://www.coindesk.com/understanding-dao-hack-journalists)\n\
    - <span id=\"page-13-37\"></span>[38] P. Santiago, *The Parity Wallet Hack Explained*,\
    \ 2017. [Online]. Available: https://blog.openzeppelin.[com/on-the-parity-wallet-multisig](https://blog.openzeppelin.com/on-the-parity-wallet-multisig-hack-405a8c12e8f7/)[hack-405a8c12e8f7/](https://blog.openzeppelin.com/on-the-parity-wallet-multisig-hack-405a8c12e8f7/)\n\
    - <span id=\"page-13-38\"></span>[39] J. Feist, G. Grieco, and A. Groce, \"Slither:\
    \ A static analysis framework for smart contracts,\" in *2019 IEEE/ACM 2nd International\
    \ Workshop on Emerging Trends in Software Engineering for Blockchain (WETSEB)*.\
    \ IEEE, 2019, pp. 8–15.\n- <span id=\"page-13-39\"></span>[40] L. Brent, N. Grech,\
    \ S. Lagouvardos, B. Scholz, and Y. Smaragdakis, \"Ethainter: a smart contract\
    \ security analyzer for composite vulnerabilities,\" in *Proceedings of the 41st\
    \ ACM SIGPLAN Conference on Programming Language Design and Implementation*, 2020,\
    \ pp. 454–469.\n- <span id=\"page-13-40\"></span>[41] S. Tikhomirov, E. Voskresenskaya,\
    \ I. Ivanitskiy, R. Takhaviev, E. Marchenko, and Y. Alexandrov, \"Smartcheck:\
    \ Static analysis of Ethereum smart contracts,\" in *Proceedings of the 1st International\
    \ Workshop on Emerging Trends in Software Engineering for Blockchain*, 2018, pp.\
    \ 9–16.\n- <span id=\"page-13-41\"></span>[42] *Securify*, Sofware Reliability\
    \ Lab, 2019. [Online]. Available: [https:](https://securify.ch/) [//securify](https://securify.ch/).ch/\n\
    - <span id=\"page-13-42\"></span>[43] Y. Feng, E. Torlak, and R. Bodik, \"Precise\
    \ Attack Synthesis for Smart Contracts,\" *arXiv preprint arXiv:1902.06067*, 2019.\n\
    - <span id=\"page-13-43\"></span>[44] L. Luu, D.-H. Chu, H. Olickel, P. Saxena,\
    \ and A. Hobor, \"Making smart contracts smarter,\" in *Proceedings of the 2016\
    \ ACM SIGSAC conference on computer and communications security*. ACM, 2016, pp.\
    \ 254–269.\n- <span id=\"page-14-0\"></span>[45] \"Manticore,\" https://github\
    \ .[com/trailofbits/manticore,](https://github.com/trailofbits/manticore) 2019,\
    \ symbolic Execution Tool for Smart Contracts.\n- <span id=\"page-14-1\"></span>[46]\
    \ \"Mythril,\" https://github .[com/ConsenSys/mythril,](https://github.com/ConsenSys/mythril)\
    \ 2019, a Security Analysis Tool for EVM Bytecode.\n- <span id=\"page-14-2\"></span>[47]\
    \ S. Kalra, S. Goel, M. Dhawan, and S. Sharma, \"Zeus: Analyzing safety of smart\
    \ contracts,\" in *Ndss*, 2018, pp. 1–12.\n- <span id=\"page-14-3\"></span>[48]\
    \ B. Jiang, Y. Liu, and W. Chan, \"Contractfuzzer: Fuzzing smart contracts for\
    \ vulnerability detection,\" in *Proceedings of the 33rd ACM/IEEE International\
    \ Conference on Automated Software Engineering*. ACM, 2018, pp. 259–269.\n- <span\
    \ id=\"page-14-4\"></span>[49] V. Wustholz and M. Christakis, \"Harvey: A greybox\
    \ fuzzer for smart ¨ contracts,\" in *Proceedings of the 28th ACM Joint Meeting\
    \ on European Software Engineering Conference and Symposium on the Foundations\
    \ of Software Engineering*, 2020, pp. 1398–1409.\n- <span id=\"page-14-5\"></span>[50]\
    \ *Echidna*, Trail of Bits, 2019. [Online]. Available: [https://github](https://github.com/trailofbits/echidna)\
    \ .com/ [trailofbits/echidna](https://github.com/trailofbits/echidna)\n- <span\
    \ id=\"page-14-6\"></span>[51] \"Oyente,\" https://github .[com/melonproject/oyente,](https://github.com/melonproject/oyente)\
    \ 2019, an Analysis Tool for Smart Contracts.\n- <span id=\"page-14-7\"></span>[52]\
    \ A. Permenev, D. Dimitrov, P. Tsankov, D. Drachsler-Cohen, and M. Vechev, \"\
    Verx: Safety verification of smart contracts,\" in *2020 IEEE symposium on security\
    \ and privacy (SP)*. IEEE, 2020, pp. 1661–1677.\n- <span id=\"page-14-8\"></span>[53]\
    \ H. Wang, Y. Liu, Y. Li, S.-W. Lin, C. Artho, L. Ma, and Y. Liu, \"Oracle-supported\
    \ dynamic exploit generation for smart contracts,\" *IEEE Transactions on Dependable\
    \ and Secure Computing*, 2020.\n- <span id=\"page-14-9\"></span>[54] T. D. Nguyen,\
    \ L. H. Pham, J. Sun, Y. Lin, and Q. T. Minh, \"sfuzz: An efficient adaptive fuzzer\
    \ for solidity smart contracts,\" in *Proceedings of the ACM/IEEE 42nd International\
    \ Conference on Software Engineering* , 2020, pp. 778–788.\n- <span id=\"page-14-10\"\
    ></span>[55] Y. Liu, Y. Li, S.-W. Lin, and C. Artho, \"Finding permission bugs\
    \ in smart contracts with role mining,\" in *Proceedings of the 31st ACM SIGSOFT\
    \ International Symposium on Software Testing and Analysis (ISSTA)*. New York,\
    \ NY, USA: ACM, Jul. 2022, pp. 716–727.\n- <span id=\"page-14-11\"></span>[56]\
    \ Z. Zhang, B. Zhang, W. Xu, and Z. Lin, \"Demystifying exploitable bugs in smart\
    \ contracts.\" ICSE, 2023.\n- <span id=\"page-14-12\"></span>[57] Sayfer, \"3\
    \ Hacks an Audit Could Not Find,\" [https://sayfer](https://sayfer.io/blog/3-hacks-an-audit-could-not-find/)\
    \ .io/blog/3 [hacks-an-audit-could-not-find/,](https://sayfer.io/blog/3-hacks-an-audit-could-not-find/)\
    \ 2023, accessed: December 18, 2023.\n- <span id=\"page-14-13\"></span>[58] C.\
    \ Flanagan, K. R. M. Leino, M. Lillibridge, G. Nelson, J. B. Saxe, and R. Stata,\
    \ \"Extended static checking for java,\" in *Proceedings of the ACM SIGPLAN 2002\
    \ Conference on Programming language design and implementation*, 2002, pp. 234–245."
  decisions:
    language: '- Qualified. Reason: English Paper'
    evaluation_prompt: Qualified.
    related_work_prompt: Qualified
    novelty_prompt: Qualified
    review_only_prompt: Qualified
  llm_input_used: '## Abstract

    Smart contracts are computer programs running on blockchains to automate the

    transaction execution between users. The absence of contract specifications

    poses a real challenge to the correctness verification of smart contracts.

    Program invariants are properties that are always preserved throughout the

    execution, which characterize an important aspect of the program behaviors. In

    this paper, we propose a novel invariant generation framework, INVCON+, for

    Solidity smart contracts. INVCON+ extends the existing invariant detector,

    InvCon, to automatically produce verified contract invariants based on both

    dynamic inference and static verification. Unlike INVCON+, InvCon only produces

    likely invariants, which have a high probability to hold, yet are still not

    verified against the contract code. Particularly, INVCON+ is able to infer more

    expressive invariants that capture richer semantic relations of contract code.

    We evaluate INVCON+ on 361 ERC20 and 10 ERC721 real-world contracts, as well as

    common ERC20 vulnerability benchmarks. The experimental results indicate that

    INVCON+ efficiently produces high-quality invariant specifications, which can

    be used to secure smart contracts from common vulnerabilities.


    ## Introduction

    Smart contracts are computer programs that operate on blockchain networks. They
    are used to facilitate the management of substantial financial assets and the
    automated execution of agreements among multiple parties who lack inherent trust.
    Notably, blockchain networks such as Ethereum [\[1\]](#page-13-0) and BSC [\[2\]](#page-13-1)
    are widely recognized as leading platforms supporting smart contracts, with applications
    spanning diverse domains such as supply-chain management, finance, energy, games,
    and digital artworks. While smart contracts hold promise for facilitating value
    transfer among users, those that deviate from their specifications may harbor
    bugs or vulnerabilities. Numerous implementations of ERC20 contracts diverge from
    common expectations, as exemplified by standard non-compliance of ERC20 [\[3\]](#page-13-2),
    particularly concerning event emission, balance updates, and the transaction fee
    mechanisms.


    Even well-established standard ERC20 implementations exhibit inconsistencies [\[4\]](#page-13-3).
    The root cause lies in the limited semantic specifications outlined in the ERC20
    standard proposal document [\[5\]](#page-13-4). Take the transfer function as
    an illustration it is designed to move a specified amount of tokens from the sender
    to the recipient while triggering the Transfer event and should throw an error
    if the sender lacks adequate tokens for the transfer. Nevertheless, the ERC20
    proposal provides only simple textual descriptions of the function, leading to
    semantic disparities across various ERC implementations and even


    different versions of the same implementation. For instance, the widely used ERC20
    implementation from OpenZeppelin initially did not permit a return value for the
    transfer function until a later commit,[1](#page-0-0) causing incompatibility
    issues with renowned tokens like BNB, as reported by the reputable security company
    SECBIT [\[4\]](#page-13-3). In cases where a contract necessitates checking the
    return value of an external call to a transfer function of ERC20 contracts, even
    if the transfer is successful, it may revert due to the absence of a return value,
    resulting in compatibility problems [\[6\]](#page-13-5). However, removing the
    return value check exposes contracts to a potential vulnerability known as the
    *fake deposit attack* [\[7\]](#page-13-6).


    Ensuring the correctness of smart contracts poses a significant challenge, especially
    in the absence of contract specifications. On the one hand, the documentation
    for most smart contracts is scant, with even widely recognized smart contract
    libraries like OpenZeppelin [\[8\]](#page-13-7), [\[9\]](#page-13-8) found to
    have errors and deficiencies in their documentation [\[10\]](#page-13-9). On the
    other hand, the absence of contract specifications hampers the widespread adoption
    of formal verification tools in the realm of smart contracts. To address this
    issue, the commercial formal verification company Certora[2](#page-0-1) has adopted
    a crowd sourcing approach—they hosted numerous competitions on well-known bug
    bounty platforms, such as Code4Rena,[3](#page-0-2) to engage third-party security
    experts in the formulation of contract specifications. Yet, manual creation of
    formal specifications for smart contracts remains costly and error-prune.


    Many automated techniques [\[11\]](#page-13-10), [\[12\]](#page-13-11) have been
    proposed to generate formal specifications in various forms to support the testing,
    verification, and validation of software programs. Among them, program invariants,
    which are enduring properties maintained throughout program execution, inherently
    serve as excellent candidates for enhancing and reinforcing program specifications.
    Program invariants have been used for vulnerability detection [\[13\]](#page-13-12),
    conformance checking [\[3\]](#page-13-2), runtime protection [\[14\]](#page-13-13),
    type checking [\[15\]](#page-13-14), and formal verification [\[16\]](#page-13-15),
    [\[17\]](#page-13-16) for smart contracts. Established tools, such as Daikon [\[11\]](#page-13-10),
    can identify *likely* program invariants for Java programs through the execution
    of their test cases. The process involves statistically inferring the invariants
    that hold based on predefined templates, while discarding those refuted by the
    data trace records. The complete historical transaction


    <span id="page-0-0"></span><sup>1</sup>https://github.[com/OpenZeppelin/openzeppelin-solidity/commit/](https://github.com/OpenZeppelin/openzeppelin-solidity/commit/6331dd125d8e8429480b2630f49781f3e1ed49cd)
    [6331dd125d8e8429480b2630f49781f3e1ed49cd](https://github.com/OpenZeppelin/openzeppelin-solidity/commit/6331dd125d8e8429480b2630f49781f3e1ed49cd)


    <span id="page-0-1"></span><sup>2</sup>https://www.certora.com/


    <span id="page-0-2"></span><sup>3</sup>https://code4rena.com/


    data of smart contracts is consistently stored on blockchains, encapsulating all
    execution data since contract deployment, serving as a valuable data source for
    mining invariants.


    In our prior work, INVCON [\[18\]](#page-13-17) utilized Daikon to identify *likely*
    invariants for smart contracts, all of which are primitive predicates hold throughout
    the existing transaction histories. Moreover, Liu et al. [\[19\]](#page-13-18)
    employed reinforcement learning to learn contract invariants critical to safely
    performing arithmetic operations, with focus on preventing integer overflow and
    underflow. Despite their usefulness, the correctness of such inferred invariants
    remains unverified. In particular, an invariant which holds in past transactions
    may not always hold in the future—this may be due to the limited contract interactions
    observed in the transaction histories so far.


    In this paper, we expand upon INVCON to generate *verified* contract invariants
    utilizing both dynamic inference and static verification. We introduce a specialized
    invariant specification language tailored for Solidity smart contracts and propose
    a novel approach for inferring high-quality verified invariants. Specifically,
    we design a Houdini-like [\[12\]](#page-13-11) algorithm to generate verified
    invariants for smart contracts. To address the explosion problem in searching
    for richer invariant candidates, such as implications that prevail in ERC20 and
    ERC721 [\[20\]](#page-13-19), [\[21\]](#page-13-20), [\[22\]](#page-13-21) specifications,
    we introduce an iterative and incremental process for exploring these candidates
    on demand. We also apply control- and data-flow analyses to eliminate meaningless
    candidates and further improve the invariant generation efficiency. Our approach
    is implemented as an automated tool called INVCON+. Through evaluation on 361
    ERC20 contracts and 10 ERC721 real-world Solidity contracts, we demonstrate that
    INVCON+ produces comprehensive contract invariant specifications with no false
    positives. Furthermore, our analysis of real-world vulnerable ERC20 contracts
    underscores the potential of INVCON+ in safeguarding these contracts through the
    application of mined invariant specifications.


    In summary, we make the following contributions:


    - We introduce a comprehensive invariant specification language designed for expressing
    operational semantics in Solidity smart contracts. This language enables logical
    operations on variables of primitive types and commonly used data structures like
    structs, arrays, and mappings in Solidity.

    - We present a unified framework for generating *verified* invariants in Solidity
    smart contracts, combining dynamic invariant detection and static invariant verification.
    Specifically, we develop a custom algorithm inspired by the Houdini algorithm
    to verify invariants for smart contracts and introduce an iterative process to
    derive a richer class of invariants.

    - Our proposed approach is implemented in INVCON+, and its effectiveness is evaluated
    on 361 ERC20 contracts and 10 ERC721 contracts, along with vulnerable ERC20 contracts
    involving 25 types of vulnerabilities. The results demonstrate that INVCON+ can
    generate high-quality and comprehensive invariant specifications for smart contracts.
    The dataset, raw results, and the prototype used in our experiments are available
    online at: https://sites.google.[com/view/invconplus/.](https://sites.google.com/view/invconplus/)


    <span id="page-1-1"></span>a, v ∈ V ariable ::= address | uint | int | string
    | bytes | byte | bool | array | mapping | struct{⃗v} f ∈ F unction ::= func(⃗a)
    {⃗s} s ∈ Statement ::= v | v := e | if (e) {⃗s} else {⃗s}| call(⃗e) | return e
    | require(e) | assert(e) | revert e ∈ Expr ::= v | *const* | e[e] | e.v | e ▷◁
    e


    Fig. 1: The core grammar of the Solidity language.


    Organizations. The rest of the paper is organized as follows. Section [II](#page-1-0)
    provides the background about smart contracts and invariant inference. Section
    [III](#page-2-0) defines the invariant specification language. Then, Sect. [IV](#page-3-0)
    introduces our invariant generation approach. Section [V](#page-5-0) describes
    our implementation framework, INVCON+, and Sect. [VI](#page-6-0) demonstrates
    our evaluation results. The related work is discussed in Sect. [VII](#page-11-0)
    and we conclude the paper in Sect. [VIII.](#page-12-0)'
  token_usage: 9846
  time_usage: 3.2916617393493652
- title: "Fixing Your Own Smells: Adding a Mistake-Based Familiarisation Step When\n\
    \  Teaching Code Refactoring"
  abstract: 'Programming problems can be solved in a multitude of functionally correct

    ways, but the quality of these solutions (e.g. readability, maintainability)

    can vary immensely. When code quality is poor, symptoms emerge in the form of

    ''code smells'', which are specific negative characteristics (e.g. duplicate

    code) that can be resolved by applying refactoring patterns. Many undergraduate

    computing curricula train students on this software engineering practice, often

    doing so via exercises on unfamiliar instructor-provided code. Our observation,

    however, is that this makes it harder for novices to internalise refactoring as

    part of their own development practices. In this paper, we propose a new

    approach to teaching refactoring, in which students must first complete a

    programming exercise constrained to ensure they will produce a code smell. This

    simple intervention is based on the idea that learning refactoring is easier if

    students are familiar with the code (having built it), that it brings

    refactoring closer to their regular development practice, and that it presents

    a powerful opportunity to learn from a ''mistake''. We designed and conducted
    a

    study with 35 novice undergraduates in which they completed various refactoring

    exercises alternately taught using a traditional and our ''mistake-based''

    approach, finding that students were significantly more effective and confident

    at completing exercises using the latter.'
  url: http://arxiv.org/abs/2401.01011v1
  keywords: ''
  document: '# Fixing Your Own Smells: Adding a Mistake-Based Familiarisation Step
    When Teaching Code Refactoring


    [Ivan Tan](https://orcid.org/0009-0001-6300-5445) Singapore Management University
    Singapore ivantan@smu.edu.sg


    ABSTRACT


    Programming problems can be solved in a multitude of functionally correct ways,
    but the quality of these solutions (e.g. readability, maintainability) can vary
    immensely. When code quality is poor, symptoms emerge in the form of ''code smells'',
    which are specific negative characteristics (e.g. duplicate code) that can be
    resolved by applying refactoring patterns. Many undergraduate computing curricula
    train students on this software engineering practice, often doing so via exercises
    on unfamiliar instructor-provided code. Our observation, however, is that this
    makes it harder for novices to internalise refactoring as part of their own development
    practices. In this paper, we propose a new approach to teaching refactoring, in
    which students must first complete a programming exercise constrained to ensure
    they will produce a code smell. This simple intervention is based on the idea
    that learning refactoring is easier if students are familiar with the code (having
    built it), that it brings refactoring closer to their regular development practice,
    and that it presents a powerful opportunity to learn from a ''mistake''. We designed
    and conducted a study with 35 novice undergraduates in which they completed various
    refactoring exercises alternately taught using a traditional and our ''mistake-based''
    approach, finding that students were significantly more effective and confident
    at completing exercises using the latter.


    # CCS CONCEPTS


    • Social and professional topics → Computing education; • Software and its engineering
    → Maintaining software.


    ## KEYWORDS


    Refactoring, code smells, code quality, software maintenance, software engineering,
    mistake-based learning, undergraduate course


    #### ACM Reference Format:


    Ivan Tan and Christopher M. Poskitt. 2024. Fixing Your Own Smells: Adding a Mistake-Based
    Familiarisation Step When Teaching Code Refactoring. In Proceedings of the 55th
    ACM Technical Symposium on Computer Science Education V. 1 (SIGCSE 2024), March
    20–23, 2024, Portland, OR, USA. ACM, New York, NY, USA, [7](#page-6-0) pages.<https://doi.org/10.1145/3626252.3630856>


    ![](_page_0_Picture_10.jpeg)


    [This work is licensed under a Creative Commons Attribution](https://creativecommons.org/licenses/by/4.0/)
    [International 4.0 License.](https://creativecommons.org/licenses/by/4.0/)


    SIGCSE 2024, March 20–23, 2024, Portland, OR, USA © 2024 Copyright held by the
    owner/author(s). ACM ISBN 979-8-4007-0423-9/24/03. <https://doi.org/10.1145/3626252.3630856>


    [Christopher M. Poskitt](https://orcid.org/0000-0002-9376-2471) Singapore Management
    University Singapore cposkitt@smu.edu.sg


    #### 1 INTRODUCTION


    Given any reasonably complex programming problem, there will be a multitude of
    functionally correct implementations that solve it. Two solutions that always
    generate the intended outputs, however, are not always equally good. Students
    are exposed to this fact early when they compare the performance of alternative
    solutions using recursion vs. iteration, or quicksort vs. bogosort [\[13\]](#page-6-1).
    Runtime performance, however, is not the only way that two solutions can differ:
    their code quality can vary immensely too. ''Code quality'' encompasses non-functional
    structural properties that arise from good engineering practices, e.g. readability
    and maintainability [\[7,](#page-6-2) [26\]](#page-6-3). In particular, a high-quality
    codebase is said to exhibit high cohesion and low coupling [\[20\]](#page-6-4):
    functionally related elements are grouped together in modules, and those modules
    are sufficiently independent such that implementation changes in one should not
    cause another to inexplicably break. When these principles are violated, concrete
    symptoms can emerge in the form of code smells [\[28\]](#page-6-5), which are
    characteristics (e.g. the presence of duplicate code) that may indicate deeper
    problems. In order to remove smells and improve code quality, software engineers
    apply refactoring patterns that produce functionally equivalent but ''odourless''
    code [\[10\]](#page-6-6).


    In undergraduate computing curricula, refactoring is typically introduced in software
    engineering modules taken after learning the fundamentals of programming. A traditional
    delivery of the topic might teach a few examples of code smells, some corresponding
    refactoring patterns, and then challenge the students to apply them to some functionally
    correct (but smelly) instructor-provided code. These exercises can be facilitated
    in a classroom or as part of an interactive online tutoring system [\[19\]](#page-6-7).
    While this mode of delivery has many advantages—the provided code is already working
    and simply needs refactoring—our own experiences have suggested that using instructor-provided
    code can make it harder for novices to internalise the concept into their own
    development practices. This is because refactoring is introduced as a standalone
    exercise on someone else''s code, rather than introducing it as a regular activity
    to be undertaken in any project they are developing.


    In this paper, we propose a new approach to teaching code refactoring that embeds
    the concept as part of a multi-step exercise. Students are first tasked to complete
    a programming exercise that is designed to ensure they will unwittingly produce
    smelly (but functionally correct) code. The goal of this step is not to ''bait''
    students, but to ensure that they are familiar with the code to be refactored.
    Following this, they are taught to identify the smell that is present, and how
    to refactor it towards an odourless solution. This simple intervention is based
    on three key ideas: (1) that learning refactoring is simpler if students are already
    familiar with the targeted code, having written it themselves; (2) that the approach


    aligns and embeds refactoring as part of their own coding practice; and (3) that
    ''mistakes''—in our case, induced code smells—present effective learning opportunities
    [\[6\]](#page-6-8).


    To assess the efficacy of this intervention, we conducted a study with 35 novice
    undergradate students. We asked them to complete two groups of refactoring exercises
    for which the code smells were alternately taught using our ''mistake-based''
    familiarisation approach or a traditional one. We found that our approach led
    to significantly higher code smell identification and refactoring success rates,
    suggesting that students are able to apply the concepts more effectively. The
    results encourage us to further explore mistake-based teaching approaches in other
    computing courses, e.g. improving the learning opportunities in security courses
    by demonstrating the presence of security flaws in student code [\[25\]](#page-6-9).


    #### 2 RELATED WORK


    In an ITiCSE''17 working group report, Börstler et al. [\[7\]](#page-6-2) analysed
    interviews with 34 students, educators, and developers on their perceptions of
    code quality. They found that code quality was mostly understood in terms of indicators
    such as ''readability'', which are measures that code smells would score poorly
    against. Notably, their interviewees ranked ''education'' lowest as the source
    they used most for learning about code quality, suggesting that undergraduate
    programmes can discuss the topic more thoroughly. Effenberger and Pelánek [\[8\]](#page-6-10)
    buttress this point through their analysis of 114,000 functionally correct solutions
    in their introductory programming class, finding most of them to contain quality
    defects.


    Bezerra et al. [\[4\]](#page-6-11) conducted a study on the perceptions and challenges
    of undergraduate students when teaching code quality through code smell refactoring.
    They highlighted a number of benefits, such as an improvement in problem solving
    and interpersonal skills, as well as a number of difficulties such as the fact
    that refactoring code can lead to new smells to further refactor. They also observed
    that students found it more complicated to refactor smells when they struggled
    to interpret the source code—a problem our approach attempts to address by having
    students construct the code first in our familiarisation step.


    Various techniques have been proposed for teaching code refactoring to undergraduates.
    For instance, Haendler et al. [\[16\]](#page-6-12) developed an interactive web-based
    tutor in which smelly code is visualised in UML (''as-is'') and students are challenged
    to refactor it towards a targeted design (''to-be''). The web-based tutor of Keuning
    et al. [\[18,](#page-6-13) [19\]](#page-6-7) allows students to request feedback
    which is generated based on some predefined rules provided by teachers, e.g. rewrite
    steps. In contrast, Haendler and Neumann [\[15\]](#page-6-14) proposed designing
    ''serious games'' for teaching refactoring. Students are presented with larger
    real-world code artefacts that are functionally correct but smelly, and are challenged
    to refactor them competitively. Izu et al. [\[17\]](#page-6-15) provide rules
    for simplifying conditional statements and some practice tasks to help students
    understand how to apply them. In all these examples, smelly code is provided to
    students: our approach differs in that students build the smelly code, and thus
    can learn about refactoring using code that they are more familiar with.


    Several existing tools can help to automatically identify smells [\[9\]](#page-6-16)
    and assess the quality of code submitted in programming assignments. Hyperstyle
    [\[5\]](#page-6-17), for example, integrates code analysis tools


    into online educational platforms to provide feedback on readability, complexity,
    and patterns of repeated mistakes. Prokic et al. [\[24\]](#page-6-18) integrate
    AI-based code quality assessment algorithms to identify issues as the code is
    written. Our approach is similar in that we focus on the code the student is writing,
    but differs in that we induce a code smell intentionally to create a learning
    opportunity.


    Many studies have demonstrated the effectiveness of learning from mistakes. Borasi
    [\[6\]](#page-6-8) suggests that mistakes can be capitalised as a learning opportunity
    (or as ''springboards'' for inquiry). Papert [\[23\]](#page-6-19) views code debugging
    as such a learning opportunity, and our hypothesis is simply that these opportunities
    can be extended to mistakes in code quality too. Ginat [\[11\]](#page-6-20) used
    erroneous solutions as a means to teach algorithm design: students would be introduced
    to an algorithm containing a common error, and would falsify inputs to trigger
    creative reasoning. Ouh and Irawan [\[22\]](#page-6-21) propose an experiential
    model for teaching software architecture, in which students undertake activities
    that simulate practical risks, helping them to learn how to identify, analyse,
    and resolve such risks in their own architectural solutions. Shar et al. [\[25\]](#page-6-9)
    demonstrated the value of security to web development students by introducing
    them to security scanners, and using them to uncover exploitable code in their
    own projects. Griffin [\[12\]](#page-6-22) highlights the controversy of intentionally
    incorporating errors, but argues that cognitive psychology theories support the
    idea that intentional errors can promote learning. Our work differs in that we
    focus on refactoring, and that a specific code quality ''mistake'' is induced
    in the familiarisation exercise (rather than provided directly by the instructor).


    #### 3 KEY PROBLEMS & RESEARCH QUESTIONS


    Our context is Singapore Management University, where code smells and refactoring
    have been taught in a software engineering module taken by all undergraduate Information
    Systems students. The delivery of this content has previously been ''traditional'',
    in that students were introduced to some key code smells [\[10\]](#page-6-6) and
    refactoring patterns [\[1\]](#page-6-23), then were challenged to address the
    former using the latter in some simple instructor-crafted exercises.


    We observed two key problems in this style of teaching. First, especially for
    novice students, using instructor-provided code for refactoring exercises posed
    a familiarity barrier (KP1). The issue we found was that for code beyond the very
    simplest, some students would struggle at the very first hurdle—familiarisation—and
    this would distract them from the primary learning objectives concerning smells
    and refactoring. Second, we observed that students treated these as isolated exercises
    and were not gaining the confidence to apply refactoring to their own code (KP2).


    Our proposed ''mistake-based'' intervention is inspired by the aforementioned
    works on learning from mistakes [\[6,](#page-6-8) [11,](#page-6-20) [12,](#page-6-22)
    [22,](#page-6-21) [23,](#page-6-19) [25\]](#page-6-9), as well as Abid et al.
    [\[3\]](#page-6-24), who observed benefits from asking students to ''enhance''
    (add features to) existing code before asking them to refactor it. In particular,
    rather than teach refactoring using instructor-provided code, we propose to first
    task students with completing a programming exercise that is designed to induce
    code that contains specific smells. The idea is to ensure that students fully
    understand the code to be refactored (having written it themselves) and thus can
    separate out code familiarisation from code smell analysis in their learning.
    Our intention is not to ''bait'' students: we


    Adding a Mistake-Based Familiarisation Step When Teaching Code Refactoring SIGCSE
    2024, March 20–23, 2024, Portland, OR, USA


    <span id="page-2-0"></span>![](_page_2_Figure_1.jpeg)


    Figure 1: High-level overview of the experiment protocol: participants are randomly
    allocated into two flows (A or B), and apply the two approaches to different code
    smell groups


    see this step as analogous to, for example, a programming exercise that first
    solves for specific inputs before guiding students to solve for all inputs, e.g.
    by replacing conditionals with a loop.


    The overall goal of this paper is to experimentally establish whether our ''mistake-based''
    approach to teaching refactoring solves our key problems (KP1, KP2). To guide
    our experiment design, we refined our goal to three research questions (RQs):


    - RQ1: Which method results in a higher success rate for refactoring exercises?

    - RQ2: Which method did students prefer and find more effective?

    - RQ3: How confident are students at being able to identify code smells in the
    future?


    RQ1 considers whether the introduction of a mistake-based familiarisation step
    helps students to complete refactoring exercises, thus addressing KP1. RQ2 considers
    the students'' subjective views, i.e. which of the methods do they prefer and
    find more effective. Finally, RQ3 addresses KP2, and considers whether students
    are confident that they can apply their refactoring skills in the future.


    #### 4 METHODOLOGY


    In this section, we describe the experiment design, pre-/post-surveys, and participants
    of our study.


    # 4.1 Experiment Design


    Figure [1](#page-2-0) presents a high-level overview of our experimental protocol,
    the detailed steps of which we describe in the following. (The full set of exercises
    is also provided in our supplementary material [\[2\]](#page-6-25).) First, we
    defined two groups of code smells (three smells per group). Group 1 contained
    Long Method, Long Parameter List, and Duplicate Code, whereas Group 2 contained
    Data Clumps, Large Class, and Primitive Obsession [\[10\]](#page-6-6). These were
    selected based on our expertise to ensure a roughly similar balance of difficulty
    and technical complexity between the two groups.


    For each group, we prepared: (1) a Python programming exercise with a template
    designed to induce some smelly code; and (2) the smelly code the exercise is designed
    to induce. Each exercise was set at a novice difficulty, given that the focus
    of the study was on code smells and not coding competency. Group 1''s exercise,
    for example, involved designing an object-oriented class for a sandwich shop (including,
    for example, a method for computing profit). Listing [1](#page-2-1) shows a snippet
    of the provided template (note that Lines [11–17](#page-2-0)


    <span id="page-2-1"></span>


    |    | 1 class Sandwich: |                                                            |  |

    |----|-------------------|------------------------------------------------------------|--|

    | 2  |                   | def __init__(self):                                        |  |

    | 3  |                   | pass                                                       |  |

    | 4  |                   |                                                            |  |

    | 5  |                   | def calculate_profit(self, name, num_sold, recipe,
    price): |  |

    | 6  |                   | profit = 0 # compute this below                            |  |

    | 7  |                   | cost = 0 # add ingredient costs to this                    |  |

    | 8  |                   | discount = 1 # change to 0.8 if num_sold >= 10             |  |

    | 9  |                   |                                                            |  |

    | 10 |                   | # ENTER YOUR CODE BELOW                                    |  |

    | 11 |                   | for ingredient in recipe:                                  |  |

    | 12 |                   | cost += ingredient_cost[ingredient]                        |  |

    | 13 |                   | if num_sold >= 10:                                         |  |

    | 14 |                   | discount = 0.8                                             |  |

    | 15 |                   | price_per_sandwich = price * discount                      |  |

    | 16 |                   | profit_per_sandwich = price_per_sandwich - cost            |  |

    | 17 |                   | profit = num_sold * profit_per_sandwich                    |  |

    | 18 |                   | # ENTER YOUR CODE ABOVE                                    |  |

    | 19 |                   |                                                            |  |

    | 20 |                   | return round(profit, 2)                                    |  |


    Listing 1: Coding exercise snippet. Lines [11–17](#page-2-0) show an example of
    the ''smelly'' code (long method) we expect the exercise to induce, and are blank
    in the mistake-based approach


    are blank in the mistake-based approach; this is the ''smelly'' code we expect
    to induce). The template''s pre-defined methods and parameter lists are designed
    to ensure solutions are largely similar (and in fact, the Long Parameter List
    code smell is guaranteed).


    The study was conducted on a one-to-one basis with a research assistant: apart
    from conducting a briefing and pre-study survey (Section [4.2\)](#page-3-0), the
    research assistant also provided oral instructions at all times. Each student
    was randomly allocated into one of two flows: A or B. Students in flow A were
    asked to apply the traditional approach to the Group 1 smells, followed by the
    mistake-based approach to the Group 2 smells. Students in flow B, however, applied
    the mistake-based approach to Group 1 and the traditional approach to Group 2.
    The specific steps of the approaches are described below.


    For the traditional approach, we asked students to watch some short videos we
    produced [\[2\]](#page-6-25) that explain the code smells relevant to the Group
    using (different) instructor-provided code. Each video introduces a specific code
    smell, explains why it occurs, why it should be refactored, how to refactor it,
    and then provides an example using instructor-provided code snippets. This content
    is conveyed in under four minutes so that the videos remain bite-sized and engaging
    for the participants [\[14\]](#page-6-26). Afterwards, the research assistant
    provided the students with smelly code for the Group directly (e.g. Listing [1](#page-2-1)
    including Lines [11–17\)](#page-2-0). In other words, the participants were asked
    to refactor instructor-provided smelly code based on what they learnt from the
    videos.


    For the mistake-based approach, students were first asked to complete the (smell-inducing)
    programming exercise for Group 1 or 2 based on the template we provided (e.g.
    Listing [1](#page-2-1) excluding Lines [11–17\)](#page-2-0). Again, the exercise
    and template were constrained to ensure that specific code smells from the given
    Group would emerge in the students'' solutions. In the event that a student struggled,
    the research assistant would provide hints to guide them towards the ''smelly''
    solution, so as to ensure that the next part of the experiment would be able to
    carry on. (We remark that hints were only provided in this familiarisation step,
    and not the refactoring exercise, which was the focus of the experiment.) Following
    the coding exercise, the research assistant would provide a brief explanation
    of the code


    <span id="page-3-1"></span>


    | Q1. Do you think your knowledge of code refactoring has improved? (Y/N)                 |

    |-----------------------------------------------------------------------------------------|

    | Q2. How familiar are you with the concept of code refactoring? (Likert)                 |

    | Q3a. How confident are you in identifying code smells from Group 1? (Likert)            |

    | Q3b. How confident are you in identifying code smells from Group 2? (Likert)            |

    | Q4a. How confident are you in refactoring identified code smells from Group
    1? (Likert) |

    | Q4b. How confident are you in refactoring identified code smells from Group
    2? (Likert) |

    | Q5a. Do you understand the videos for the traditional method? (Y/N)                     |

    | Q5b. Did you understand the content covered in the mistake-based approach? (Y/N)        |

    | Q6. How effective was the traditional method? (Likert)                                  |

    | Q7. How effective was the mistake-based approach? (Likert)                              |

    | Q8. Which method did you prefer? (Traditional/Mistake-based/No preference)              |

    | Q9. What did you like/dislike about the traditional method? (Open)                      |

    | Q10. What did you like/dislike about the mistake-based approach? (Open)                 |


    Figure 2: Post-experiment survey (Likert scales are 7-point)


    smells relevant to the Group (following the script used in the videos from the
    traditional approach), before asking the student to identify the smells in their
    own code. For any smells the student failed to identify, the research assistant
    would record this before showing the student what they missed. Finally, the research
    assistant gave another a brief oral explanation on the relevant refactoring patterns
    (again, following the video script), before asking the student to apply them where
    relevant to their own smelly code.


    In both approaches, the research assistant recorded the number of smells identified
    and resolved by the participants. They did not provide any help in identifying
    or resolving the smells until the participant indicated that they were finished.
    To earn a point for code smell identification, they had to name the correct smell
    and locate where it was occurring. For unidentified smells, these were highlighted
    by the research assistant to the students (with no point awarded), so that the
    refactoring part of the experiment could carry on. To earn a point for resolving
    a smell, the refactored code had to remain correct (buggy misconceptions [\[21\]](#page-6-27)
    did not count) and the smell had to be removed to be considered successful. Partial
    marks were not given for partial fixes. At the end of the experiment, any unresolved
    smells were explained to the students by the research assistant for the participant''s
    learning.


    #### <span id="page-3-0"></span>4.2 Pre- and Post-Surveys


    Prior to the experiment, we used a pre-study survey to collect some basic demographic
    information (e.g. gender, year of study), as well as the pre-university institution
    they studied at (as some involve significant practical programming lessons). We
    also asked them to rate their proficiency in Python, code smells, and code refactoring
    using Likert scales of 1–7 (based on the suggestion of [\[27\]](#page-6-28)).


    After the experiment, participants were asked to complete a poststudy survey (Figure
    [2\)](#page-3-1) to determine whether they perceived an improvement in their confidence
    to identify and resolve code smells. The final questions involved free text responses
    to collect some qualitative assessments from the participants. All Likert scales
    consisted of 7 points, where 1 indicates least confidence/effectiveness, 4 is
    neutral, and 7 indicates most confidence/effectiveness.


    <span id="page-3-2"></span>![](_page_3_Figure_8.jpeg)


    Figure 3: Number of code smells identified (out of 3)


    ## 4.3 Participants


    We recruited 35 undergraduate Information Systems students from our institution.
    Among these students, 30 were in the first or second year of their Bachelor''s
    degree, whereas the others were in their third or final years; 17 reported their
    gender as female with the remaining 18 reporting as male. Given that refactoring
    is only taught towards the end of their degrees, the vast majority of the participants
    can be considered novices in this topic. This was further confirmed by the pre-study
    survey, in which the majority of them rated their familiarity with refactoring
    as either 1 or 2 (out of 7), while proficiency in Python varied from 1 to 5 (out
    of 7), with most falling in the lower range (1 to 3).


    #### 5 RESULTS & ANALYSIS


    In this section, we will analyse the results gathered from the experiments in
    accordance to the research questions defined.


    In conducting our analysis, we primarily employed the Wilcoxon signed-rank test.
    The test was used as the key point was the difference between the two methods
    for each paired measurement (one participant), so that we could obtain a p-value
    to interpret against our null and alternative hypotheses. Whether it was comparing
    success rates, confidence, or effectiveness, they were all based on comparing
    the two different methods: traditional or mistake-based.


    #### 5.1 RQ1: Refactoring Success Rates


    For RQ1, we evaluated the success rates for the refactoring exercises, based on
    the methods that were used to teach the student. For each participant, four values
    were collected: (1) number of code smells identified when taught with the traditional
    method; (2) number of code smells refactored when taught with the traditional
    method; (3) number of code smells identified when taught with the mistake-based
    method; and (4) number of code smells refactored when taught with the mistake-based
    method. Wilcoxon signedranked tests were then conducted to compare the means of
    the variables: one for identification across the two methods, and one for refactoring
    across the two methods. The null hypothesis is that there is no significant difference
    in the success rates between the two methods, with the alternative hypothesis
    being that there is.


    We begin with the code smell identification rate between the two methods. Students
    were evaluated against a possible three code smells to be identified for each
    method, and only received the point if they were able to correctly identify what
    the code smell was and where it was located. Figure [3](#page-3-2) shows a comparison
    of the code smell identification success rates between the two methods. We can
    see that for the mistake-based method, most students were able to identify all
    three code smells, whereas only a few were able


    #### Adding a Mistake-Based Familiarisation Step When Teaching Code Refactoring
    SIGCSE 2024, March 20–23, 2024, Portland, OR, USA


    <span id="page-4-0"></span>![](_page_4_Figure_1.jpeg)


    Figure 4: Number of code smells successfully refactored (out of a possible 3)


    to do so for the traditional method. The average number of code smells identified
    for the mistake-based method was 2.93 (out of 3), whereas it was 1.9 for the traditional
    method. The median for the mistake-based method was 3 (out of 3), whereas it was
    2 for the traditional method. The test returned a p-value of 3.2017e-06, and thus
    we accept the alternative hypothesis that there is a significant difference between
    the identification rates for the two methods.


    Next, we look at the refactoring success rates. Similar to identification, students
    were evaluated out of a possible three code smells to be refactored for each method.
    They would only get the point if they were able to refactor the code smell and
    still maintain the function''s logic. For refactoring, the average number of code
    smells for the mistake-based method was 2.23 (out of 3), whereas it was 1 for
    the traditional method. The median for the mistake-based method was 2 (out of
    3), whereas it was 1 for the traditional method. Figure [4](#page-4-0) shows that
    for the mistake-based method, participants could refactor more code smells in
    general. The test returned a pvalue of 1.880441e-05, and thus we accept that there
    is a significant difference between the refactoring rates for the two methods.


    To conclude the statistical analysis for RQ1, we can say that the mistake-based
    method was able to generate a higher success rate for both code smell identification
    and refactoring. For code smells learnt with the mistake-based method, students
    were able to achieve a higher success rate than those learnt with the traditional
    method. We believe that this is because the mistake-based approach removed the
    familiarity barrier for the exercises.


    #### 5.2 RQ2: Preferred Method


    For RQ2, we used the post-study survey to establish which of the methods the students
    preferred learning with, and which they perceived to be more effective. First,
    out of the 35 participants, 34 said that they preferred the mistake-based method,
    with the remaining participant having no preference.


    Next, we want to find out which method the students perceived to be more effective.
    Using Q6 and Q7 (Figure [2\)](#page-3-1), we were able to gather quantitative
    data on a scale of 1–7 for the effectiveness ratings for both methods. Again,
    our null hypothesis is that there is no significant difference between the effectiveness
    ratings for the two methods, whereas our alternative hypothesis is that there
    is.


    From the histograms (Figure [5\)](#page-4-1), we can see that the effectiveness
    rating for the traditional method is generally on the lower side, with its peak
    being a 4 (out of 7). On the other hand, the mistake-based methods scored better
    in general. The mistake-based method was able to get an average of 5.5 (out of
    7), whereas the traditional approach obtained an average of 3.17. The median for


    <span id="page-4-1"></span>![](_page_4_Figure_10.jpeg)


    Figure 5: How effective the students found the traditional vs. mistake-based approaches
    (7-point Likert)


    <span id="page-4-2"></span>![](_page_4_Figure_12.jpeg)


    Figure 6: Confidence identifying code smells before and after the study (7-point
    Likert)


    the mistake-based method was 5.5 (out of 7), whereas the traditional method obtained
    a median of 3. With a p-value of 2.188529e-06, we thus accept the alternative
    hypothesis that there is a significant difference between the effectiveness ratings.


    For RQ2, it appears that the preferred method in terms of effectiveness was the
    mistake-based method. Based on our qualitative feedback from students (Q9 and
    Q10), we found out that they liked it more as it was a step-by-step approach,
    and they could digest the code as they built it. Highlighting their mistakes was
    also crucial: students shared that it helped them understand the concept more.


    #### 5.3 RQ3: Confidence at Refactoring


    For RQ3, we are interested in two things. Firstly, whether the students were more
    confident in identifying code smells after the experiment. Secondly, and most
    importantly, we want to establish whether they are more confident at identifying/resolving
    code smells learnt using the mistake-based or traditional method.


    First, we look at the change in students'' confidence before and after the experiment.
    Data was obtained using pre- and post-study survey questions which required students
    to rate their confidence on a scale of 1–7. Our null hypothesis was that there
    is no significant difference between the confidence levels before and after the
    experiment, with the alternative hypothesis being that there is one.


    We can see on the histograms (see Figure [6\)](#page-4-2) that there is a general
    increase in confidence after the experiment as compared to before. The average
    rating before was 1.6, which increased to an average of 3.6 after the experiment
    was conducted. Before the experiment, the median confidence was 1, and after the
    test, it increased to 3.5. The test returned a p-value of 1.597915e-06, allowing
    us to accept the alternative hypothesis, concluding that there is a difference
    between the confidence levels before and after the experiment.


    Next, we want to evaluate whether students are more confident in identifying smells
    that they learnt using the mistake-based method versus those learnt using the
    traditional method. Similar to


    #### SIGCSE 2024, March 20–23, 2024, Portland, OR, USA Ivan Tan and Christopher
    M. Poskitt


    <span id="page-5-0"></span>![](_page_5_Figure_1.jpeg)


    Figure 7: Confidence identifying code smells learnt using the traditional vs.
    mistake-based approach (7-point Likert)


    RQ2, we will be using the identification confidence ratings that the students
    provided in our surveys. Our null hypothesis is that there is no significant difference
    between the confidence levels between the two methods, with the alternative being
    that there is.


    From the histograms (see Figure [7\)](#page-5-0), we can observe that for the
    smells covered by the traditional method, the identification confidence level
    was lower than for the mistake-based method. The average identification confidence
    level was 3.83 (out of 7) for the mistake-based method, but only 2.63 for the
    traditional method. The median for the mistake-based method was 4 (out of 7),
    whereas the median for the traditional method was 2. With a p-value of 2.161186e-06,
    and we thus accept our alternative hypothesis that there is a significant difference
    between the identification confidence levels for code smells learnt for each method.


    We note an increase in the students'' confidence in identifying code smells, particularly
    for those that they learnt using the mistakebased method. We believe, again, that
    this is due to removing the familiarity barrier, allowing them to focus their
    learning entirely on refactoring instead of code comprehension.


    #### 5.4 Threats to Validity


    Finally, we remark on some threats to the validity of our results. First, the
    study was limited to undergraduates from a single institution. It is possible
    that the results may not generalise due to differences in curricula, student profiles,
    and pedagogy.


    Second, the exercises were designed around specific groupings of code smells and
    refactorings. It is possible that the results will not apply to refactorings beyond
    those covered, or to groupings of smells/refactorings that do not maintain our
    difficulty balance.


    Finally, to strive for objectivity in our evaluation, we used absolute values
    (0 or 1) to denote whether a student was successful in identifying or refactoring
    a certain code smell. This might not be fully accurate, especially for refactoring,
    as there could be partially acceptable answers that we simply counted as 0. For
    example, credit was not awarded for being able to correctly identify a code smell''s
    location if the participant could not also name it.


    #### 6 REFLECTIONS


    Some of the participants provided additional feedback at the end of the study.
    A participant shared that the mistake-based approach was very similar to their
    experience of learning mathematics, where they would learn certain concepts better
    after getting the questions wrong first. Another participant shared that they
    were inspired by the mistake-based approach, and would use it in their community


    service project involving teaching coding to secondary school children: they joked
    about asking the students to manually print "Hello World!" 20 times before introducing
    for-loops to them. One participant also likened the experience to their internship
    at a startup, where one of their first code commits went through heavy code refactoring
    by their colleague, teaching them a "lesson they would never forget". It was encouraging
    to hear these anecdotes, and it helped to validate our approach in ways we did
    not expect.


    While this study has conveyed the potential value of a mistakebased methodology,
    challenges remain for practitioners to apply it in a classroom setting. When setting
    the coding exercises in our study, we found it difficult to create questions where
    there was a balance between right and wrong, with just enough space and opportunity
    for a student to commit a smell that we were expecting. If the question''s design
    was too narrow, it would have been too obvious. On the other hand, if the question''s
    design was too broad, we would be getting mistakes that are irrelevant to the
    learning objectives. Through a lot of iterations and trials, we were able to achieve
    a balance for this study, but in terms of using this approach in the classroom,
    this would be an important point for educators to consider. Further research could
    potentially try to find ways to automate or use AI in generating these exercises.


    #### 7 CONCLUSION & FUTURE WORK


    In this paper, we proposed an approach to teaching refactoring that incorporates
    a ''mistake-based'' familiarisation step. In other words, rather than refactor
    unfamiliar instructor-provided code, students complete a programming exercise
    that leads to smelly (but familiar) code for them to refactor instead. This simple
    intervention is based on the idea that: (1) students will learn refactoring more
    effectively if they are already familiar with the targeted code, having built
    it; (2) it shows them refactoring isn''t just about fixing other people''s code,
    but can be incorporated into their own development practice; and (3) it aligns
    with the well-understood notion that ''mistakes'' provide a strong opportunity
    for learning. We presented a study comparing our mistake-based teaching approach
    with a traditional one, finding that students were significantly more effective
    and confident at completing exercises.


    This teaching methodology could potentially be used for other software engineering
    courses, and this is something that we are eager to test as well. For instance,
    in a basic SQL / database management course, it might be useful for students to
    see the wrong results returned or erroneous merged tables created, allowing them
    to understand what was wrong with their query from the mistakes they made. Similarly,
    in a web development course, the importance of securing web applications could
    be conveyed to students by subjecting their code to various security scanners
    and highlighting any vulnerabilities [\[25\]](#page-6-9).


    #### ACKNOWLEDGEMENTS


    We are grateful to the anonymous referees for their helpful feedback on drafts
    of this paper. We are also grateful to Sun Jun and Ouh Eng Lieh for their helpful
    comments during the ''UResearch'' programme at SMU. Thanks, finally, to the many
    students who kindly spent some time to participate in this study.


    <span id="page-6-0"></span>Adding a Mistake-Based Familiarisation Step When Teaching
    Code Refactoring SIGCSE 2024, March 20–23, 2024, Portland, OR, USA


    #### REFERENCES


    - <span id="page-6-23"></span>[1] 2023. Refactoring: clean your code. [https://refactoring.guru/refactoring.](https://refactoring.guru/refactoring)
    Accessed: December 2023.

    - <span id="page-6-25"></span>[2] 2023. Supplementary Materials: Exercises and
    Videos. [https://sites.google.com/](https://sites.google.com/view/fixingyourownsmells/home)
    [view/fixingyourownsmells/home.](https://sites.google.com/view/fixingyourownsmells/home)

    - <span id="page-6-24"></span>[3] Shamsa Abid, Hamid Abdul Basit, and Naveed Arshad.
    2015. Reflections on Teaching Refactoring: A Tale of Two Projects. In ITiCSE.
    ACM, 225–230.

    - <span id="page-6-11"></span>[4] Carla Bezerra, Humberto Damasceno, and João
    Teixeira. 2022. Perceptions and Difficulties of Software Engineering Students
    in Code Smells Refactoring. In VEM. SBC, 41–45.

    - <span id="page-6-17"></span>[5] Anastasiia Birillo, Ilya Vlasov, Artyom Burylov,
    Vitalii Selishchev, Artyom Goncharov, Elena Tikhomirova, Nikolay Vyahhi, and Timofey
    Bryksin. 2022. Hyperstyle: A Tool for Assessing the Code Quality of Solutions
    to Programming Assignments. In SIGCSE (1). ACM, 307–313.

    - <span id="page-6-8"></span>[6] Raffaella Borasi. 1994. Capitalizing on Errors
    as "Springboards for Inquiry": A Teaching Experiment. J. Res. Math. Educ. 25,
    2 (1994), 166 – 208.

    - <span id="page-6-2"></span>[7] Jürgen Börstler, Harald Störrle, Daniel Toll,
    Jelle van Assema, Rodrigo Duran, Sara Hooshangi, Johan Jeuring, Hieke Keuning,
    Carsten Kleiner, and Bonnie K. MacKellar. 2017. "I know it when I see it": Perceptions
    of Code Quality. In ITiCSE. ACM, 389.

    - <span id="page-6-10"></span>[8] Tomás Effenberger and Radek Pelánek. 2022. Code
    Quality Defects across Introductory Programming Topics. In SIGCSE (1). ACM, 941–947.

    - <span id="page-6-16"></span>[9] Eduardo Fernandes, Johnatan Oliveira, Gustavo
    Vale, Thanis Paiva, and Eduardo Figueiredo. 2016. A review-based comparative study
    of bad smell detection tools. In EASE. ACM, 18:1–18:12.

    - <span id="page-6-6"></span>[10] Martin Fowler. 2018. Refactoring - Improving
    the Design of Existing Code (2nd ed.). Addison-Wesley.

    - <span id="page-6-20"></span>[11] David Ginat. 2008. Learning from wrong and
    creative algorithm design. In SIGCSE. ACM, 26–30.

    - <span id="page-6-22"></span>[12] Jean M. Griffin. 2019. Designing Intentional
    Bugs for Learning. In UKICER. ACM, 5:1–5:7.

    - <span id="page-6-1"></span>[13] Hermann Gruber, Markus Holzer, and Oliver Ruepp.
    2007. Sorting the Slow Way: An Analysis of Perversely Awful Randomized Sorting
    Algorithms. In FUN (Lecture Notes in Computer Science, Vol. 4475). Springer, 183–197.

    - <span id="page-6-26"></span>[14] Philip J. Guo, Juho Kim, and Rob Rubin. 2014.
    How video production affects student engagement: an empirical study of MOOC videos.
    In L@S. ACM, 41–50.

    - <span id="page-6-14"></span>[15] Thorsten Haendler and Gustaf Neumann. 2019.
    Serious Refactoring Games. In HICSS. ScholarSpace, 1–10.

    - <span id="page-6-12"></span>[16] Thorsten Haendler, Gustaf Neumann, and Fiodor
    Smirnov. 2019. RefacTutor: An Interactive Tutoring System for Software Refactoring.
    In CSEDU (Selected Papers) (Communications in Computer and Information Science,
    Vol. 1220). Springer, 236–261.

    - <span id="page-6-15"></span>[17] Cruz Izu, Paul Denny, and Sayoni Roy. 2022.
    A Resource to Support Novices Refactoring Conditional Statements. In ITiCSE (1).
    ACM, 344–350.

    - <span id="page-6-13"></span>[18] Hieke Keuning, Bastiaan Heeren, and Johan Jeuring.
    2020. Student Refactoring Behaviour in a Programming Tutor. In Koli Calling. ACM,
    4:1–4:10.

    - <span id="page-6-7"></span>[19] Hieke Keuning, Bastiaan Heeren, and Johan Jeuring.
    2021. A Tutoring System to Learn Code Refactoring. In SIGCSE. ACM, 562–568.

    - <span id="page-6-4"></span>[20] Bertrand Meyer. 1997. Object-Oriented Software
    Construction (2nd ed.). Prentice-Hall.

    - <span id="page-6-27"></span>[21] Eduardo Oliveira, Hieke Keuning, and Johan
    Jeuring. 2023. Student Code Refactoring Misconceptions. In ITiCSE (1). ACM, 19–25.

    - <span id="page-6-21"></span>[22] Eng Lieh Ouh and Yunghans Irawan. 2018. Exploring
    Experiential Learning Model and Risk Management Process for an Undergraduate Software
    Architecture Course. In FIE. IEEE, 1–9.

    - <span id="page-6-19"></span>[23] Seymour Papert. 1980. Mindstorms: Children,
    computers and powerful ideas. Harvester.

    - <span id="page-6-18"></span>[24] Simona Prokic, Katarina-Glorija Grujic, Nikola
    Luburic, Jelena Slivka, Aleksandar Kovacevic, Dragan Vidakovic, and Goran Sladic.
    2021. Clean Code and Design Educational Tool. In MIPRO. IEEE, 1601–1606.

    - <span id="page-6-9"></span>[25] Lwin Khin Shar, Christopher M. Poskitt, Kyong
    Jin Shim, and Li Ying Leonard Wong. 2022. XSS for the Masses: Integrating Security
    in a Web Programming Course using a Security Scanner. In ITiCSE (1). ACM, 463–469.

    - <span id="page-6-3"></span>[26] Ian Sommerville. 2015. Software Engineering
    (10th ed.). Pearson.

    - <span id="page-6-28"></span>[27] Hamed Taherdoost. 2019. What is the best response
    scale for survey and questionnaire design; review of different lengths of rating
    scale/attitude scale/Likert scale. International Journal of Academic Research
    in Management (2019), 1–10.

    - <span id="page-6-5"></span>[28] Michele Tufano, Fabio Palomba, Gabriele Bavota,
    Rocco Oliveto, Massimiliano Di Penta, Andrea De Lucia, and Denys Poshyvanyk. 2015.
    When and Why Your Code Starts to Smell Bad. In ICSE (1). IEEE Computer Society,
    403–414.'
  decisions:
    language: '- Qualified. Reason: English Paper'
    evaluation_prompt: Qualified.
    related_work_prompt: Qualified
    novelty_prompt: Qualified
    review_only_prompt: Qualified
  llm_input_used: '## Abstract

    Programming problems can be solved in a multitude of functionally correct

    ways, but the quality of these solutions (e.g. readability, maintainability)

    can vary immensely. When code quality is poor, symptoms emerge in the form of

    ''code smells'', which are specific negative characteristics (e.g. duplicate

    code) that can be resolved by applying refactoring patterns. Many undergraduate

    computing curricula train students on this software engineering practice, often

    doing so via exercises on unfamiliar instructor-provided code. Our observation,

    however, is that this makes it harder for novices to internalise refactoring as

    part of their own development practices. In this paper, we propose a new

    approach to teaching refactoring, in which students must first complete a

    programming exercise constrained to ensure they will produce a code smell. This

    simple intervention is based on the idea that learning refactoring is easier if

    students are familiar with the code (having built it), that it brings

    refactoring closer to their regular development practice, and that it presents

    a powerful opportunity to learn from a ''mistake''. We designed and conducted
    a

    study with 35 novice undergraduates in which they completed various refactoring

    exercises alternately taught using a traditional and our ''mistake-based''

    approach, finding that students were significantly more effective and confident

    at completing exercises using the latter.


    ## Introduction

    Given any reasonably complex programming problem, there will be a multitude of
    functionally correct implementations that solve it. Two solutions that always
    generate the intended outputs, however, are not always equally good. Students
    are exposed to this fact early when they compare the performance of alternative
    solutions using recursion vs. iteration, or quicksort vs. bogosort [\[13\]](#page-6-1).
    Runtime performance, however, is not the only way that two solutions can differ:
    their code quality can vary immensely too. ''Code quality'' encompasses non-functional
    structural properties that arise from good engineering practices, e.g. readability
    and maintainability [\[7,](#page-6-2) [26\]](#page-6-3). In particular, a high-quality
    codebase is said to exhibit high cohesion and low coupling [\[20\]](#page-6-4):
    functionally related elements are grouped together in modules, and those modules
    are sufficiently independent such that implementation changes in one should not
    cause another to inexplicably break. When these principles are violated, concrete
    symptoms can emerge in the form of code smells [\[28\]](#page-6-5), which are
    characteristics (e.g. the presence of duplicate code) that may indicate deeper
    problems. In order to remove smells and improve code quality, software engineers
    apply refactoring patterns that produce functionally equivalent but ''odourless''
    code [\[10\]](#page-6-6).


    In undergraduate computing curricula, refactoring is typically introduced in software
    engineering modules taken after learning the fundamentals of programming. A traditional
    delivery of the topic might teach a few examples of code smells, some corresponding
    refactoring patterns, and then challenge the students to apply them to some functionally
    correct (but smelly) instructor-provided code. These exercises can be facilitated
    in a classroom or as part of an interactive online tutoring system [\[19\]](#page-6-7).
    While this mode of delivery has many advantages—the provided code is already working
    and simply needs refactoring—our own experiences have suggested that using instructor-provided
    code can make it harder for novices to internalise the concept into their own
    development practices. This is because refactoring is introduced as a standalone
    exercise on someone else''s code, rather than introducing it as a regular activity
    to be undertaken in any project they are developing.


    In this paper, we propose a new approach to teaching code refactoring that embeds
    the concept as part of a multi-step exercise. Students are first tasked to complete
    a programming exercise that is designed to ensure they will unwittingly produce
    smelly (but functionally correct) code. The goal of this step is not to ''bait''
    students, but to ensure that they are familiar with the code to be refactored.
    Following this, they are taught to identify the smell that is present, and how
    to refactor it towards an odourless solution. This simple intervention is based
    on three key ideas: (1) that learning refactoring is simpler if students are already
    familiar with the targeted code, having written it themselves; (2) that the approach


    aligns and embeds refactoring as part of their own coding practice; and (3) that
    ''mistakes''—in our case, induced code smells—present effective learning opportunities
    [\[6\]](#page-6-8).


    To assess the efficacy of this intervention, we conducted a study with 35 novice
    undergradate students. We asked them to complete two groups of refactoring exercises
    for which the code smells were alternately taught using our ''mistake-based''
    familiarisation approach or a traditional one. We found that our approach led
    to significantly higher code smell identification and refactoring success rates,
    suggesting that students are able to apply the concepts more effectively. The
    results encourage us to further explore mistake-based teaching approaches in other
    computing courses, e.g. improving the learning opportunities in security courses
    by demonstrating the presence of security flaws in student code [\[25\]](#page-6-9).'
  token_usage: 5210
  time_usage: 2.840696096420288
- title: 'PTE: Axiomatic Semantics based Compiler Testing'
  abstract: 'The correctness of a compiler affects the correctness of every program

    written in the language, and thus must be thoroughly evaluated. Existing

    automatic compiler testing methods however either rely on weak oracles (e.g.,
    a

    program behaves the same if only dead code is modified), or require substantial

    initial effort (e.g., having a complete operational language semantics). While

    the former prevents a comprehensive correctness evaluation, the latter makes

    those methods irrelevant in practice. In this work, we propose an axiomatic

    semantics based approach for testing compilers, called PTE. The idea is to

    incrementally develop a set of ``axioms'''' capturing anecdotes of the language

    semantics in the form of \emph{(\textbf{p}recondition, \textbf{t}ransformation,

    \textbf{e}xpectation) triples, which allows us to test the compiler

    automatically.} Such axioms are written in the same language whose compiler is

    under test, and can be developed either based on the language specification, or

    by generalizing the bug reports. PTE has been applied to a newly developed

    compiler (i.e., Cangjie) and a mature compiler (i.e., Java), and successfully

    identified 42 implementation bugs and 9 potential language design issues.'
  url: http://arxiv.org/abs/2401.01036v1
  keywords: ''
  document: "# PTE: Axiomatic Semantics based Compiler Testing\n\nGuoliang Dong gldong@smu.edu.sg\
    \ Singapore Management University Singapore, Singapore\n\nJun Sun junsun@smu.edu.sg\
    \ Singapore Management University Singapore, Singapore\n\nRichard Schumi rschumi@smu.edu.sg\
    \ Singapore Management University Singapore, Singapore\n\nBased on the testing\
    \ oracle, existing compiler testing approaches can be roughly categorized into\
    \ three groups. The first group contains approaches that aim to maintain a comprehensive\
    \ test suite [\\[9,](#page-10-4) [18,](#page-10-5) [34\\]](#page-10-3). This is\
    \ certainly necessary, although it is hardly sufficient since covering every aspect\
    \ of the compiler correctness (e.g., the language semantics) would require a huge\
    \ number of test cases. Creating and maintaining such a test suite is challenging,\
    \ especially for evolving languages. The second group contains approaches that\
    \ aim to automatically test the compiler based on certain properties of the language\
    \ semantics [\\[4\\]](#page-10-6). One noticeable example is EMI [\\[14\\]](#page-10-1),\
    \ which is based on the property that altering 'dead code' (i.e., part of the\
    \ program that is not reachable given certain program input) should not alter\
    \ program behaviors. While these approaches are shown to be effective in discovering\
    \ compiler bugs [\\[5\\]](#page-10-7), they are often limited to simple algebraic\
    \ properties. This means that they are far from comprehensively evaluating the\
    \ correct implementation of the language semantics. The third group contains those\
    \ heroic efforts which aim to fully specify the (operational) semantics of a language\
    \ and then evaluate the compiler's correctness accordingly [\\[2,](#page-10-8)\
    \ [13,](#page-10-9) [16\\]](#page-10-10). While such approaches offer a way of\
    \ systematically and comprehensively evaluating the correctness of the compiler,\
    \ their adaptation in practice is hindered by the massive initial effort required\
    \ to formalize the language semantics and the daunting task of maintaining the\
    \ semantics subsequently. In this work, we aim to develop a compiler testing approach\
    \ with the following objectives. First, it must be able to evaluate the compiler's\
    \ correctness comprehensively. Second, it must not require a huge initial effort,\
    \ and it must be relatively easy to maintain so that it can keep up with language\
    \ updates. We thus propose an axiomatic semantics based approach called PTE (that\
    \ stands for precondition, transformation, and expectation) for testing compilers.\
    \ The key idea behind PTE is to incrementally develop a set of axiomatic semantic\
    \ rules which are used to systematically test the target compiler. Each axiomatic\
    \ semantic rule is in the form of a triple (precondition, transformation, expectation),\
    \ where precondition specifies when the rule applies, transformation describes\
    \ a transformation of a program, and expectation specifies the expected outcome\
    \ of executing the transformed program. For example, the PTE rule in Table [1](#page-1-0)\
    \ captures the Liskov Substitution Principle [\\[17\\]](#page-10-11) for object-orientation,\
    \ which is often adopted as a language design principle. Each rule is implemented\
    \ as a program in the language whose compiler is being tested. It is thus not\
    \ necessary to learn a new language or notation. The actual implementation of\
    \ a rule\n\nBo Wang wangbo\\_cs@bjtu.edu.cn Beijing Jiaotong University Beijing,\
    \ China\n\nXinyu Wang wangxinyu@zju.edu.cn Shanghai Institute for Advanced Study\
    \ of Zhejiang University Shanghai, China\n\n## ABSTRACT\n\nThe correctness of\
    \ a compiler affects the correctness of every program written in the language,\
    \ and thus must be thoroughly evaluated. Existing automatic compiler testing methods\
    \ however either rely on weak oracles (e.g., a program behaves the same if only\
    \ dead code is modified), or require substantial initial effort (e.g., having\
    \ a complete operational language semantics). While the former prevents a comprehensive\
    \ correctness evaluation, the latter makes those methods irrelevant in practice.\
    \ In this work, we propose an axiomatic semantics based approach for testing compilers,\
    \ called PTE. The idea is to incrementally develop a set of \"axioms\" capturing\
    \ anecdotes of the language semantics in the form of (precondition, transformation,\
    \ expectation) triples, which allows us to test the compiler automatically. Such\
    \ axioms are written in the same language whose compiler is under test, and can\
    \ be developed either based on the language specification, or by generalizing\
    \ the bug reports. PTE has been applied to a newly developed compiler (i.e., Cangjie)\
    \ and a mature compiler (i.e., Java), and successfully identified 42 implementation\
    \ bugs and 9 potential language design issues.\n\n### KEYWORDS\n\nCompiler testing,\
    \ language semantics, automated testing\n\n#### ACM Reference Format:\n\nGuoliang\
    \ Dong, Jun Sun, Richard Schumi, Bo Wang, and Xinyu Wang. 2024. PTE: Axiomatic\
    \ Semantics based Compiler Testing. In Proceedings of ACM Conference (Conference'17).\
    \ ACM, New York, NY, USA, [12](#page-11-0) pages. <https://doi.org/10.1145/nnnnnnn.nnnnnnn>\n\
    \n### 1 INTRODUCTION\n\nA bug in a compiler potentially renders all programs written\
    \ in the language problematic. It is thus highly desirable that we have a systematic\
    \ and scalable way of evaluating the correctness of compilers. The challenge of\
    \ effective compiler testing [\\[3,](#page-10-0) [14,](#page-10-1) [32,](#page-10-2)\
    \ [34\\]](#page-10-3) is however enormous. A modern programming language often\
    \ has many features which may evolve through time. That is, the compiler not only\
    \ has to handle the complicated language semantics correctly, but also must keep\
    \ up with the constant language updates. In addition, for efficiency reasons,\
    \ a compiler is often teeming with various optimization options, which must be\
    \ kept consistent with the evolving language semantics. All of this makes the\
    \ task of evaluating the correctness of a compiler highly nontrivial.\n\n####\
    \ <span id=\"page-1-0\"></span>Table 1: PTE rule 1: 'Liskov Substitution Principle'\n\
    \n| of type \U0001D446\U0001D462\U0001D45D<br>and |\n|--------------------|\n\
    |                    |\n\n| a class \U0001D446\U0001D462\U0001D44F |  | that is\
    \ a subclass of \U0001D446\U0001D462\U0001D45D. |  |\n|-------------|--|----------------------------|--|\n\
    |             |  |                            |  |\n\n- T Replace <sup>1</sup>\
    \ with an object <sup>2</sup> of type .\n- E The program behavior remains the\
    \ same.\n\nmay have to resolve multiple details. For instance, for the high-level\
    \ rule shown in Table [1,](#page-1-0) <sup>2</sup> and <sup>1</sup> must have\
    \ the same values for the shared (static and instance) variables. Furthermore,\
    \ we must define the meaning of \"the program behavior is the same\".\n\nGiven\
    \ the implementation of a rule as well as a test suite for the compiler, PTE automatically\
    \ identifies test cases which satisfy the precondition; transforms the program\
    \ according to the transformation, and checks whether the expectation is met by\
    \ compiling and executing the transformed test cases with the target compiler.\n\
    \nWhile it is not hard to imagine that many such rules can be developed, in practice\
    \ it is important to answer the question of how we can systematically develop,\
    \ as well as maintain, a repository of such rules. We propose two ways of developing\
    \ the rules. First, they can be built based on the language specification (written\
    \ in natural languages). It is our observation that existing language specifications\
    \ often contain 'PTE rules' informally. For instance, here are some examples from\
    \ the Java specification.\n\n- \"The enhanced for statement is equivalent to a\
    \ basic for statement of the form: ...\"\n- \"It is a compile-time error if final\
    \ appears more than once as a modifier for each variable declared in a resource\
    \ specification.\"\n\nThe former can be understood as a PTE rule whose transformation\
    \ is to de-sugar an enhanced for-statement to a basic one with the expectation\
    \ of program equivalence. The latter can be turned into a PTE rule whose transformation\
    \ is to introduce an extra final modifier with the expectation of a compile-time\
    \ error.\n\nSecond, we propose to incrementally build up the rule repository based\
    \ on compiler bug fixes. We observed that it is often possible to develop PTE\
    \ rules that generalize compiler bug fixes (see Section [2\\)](#page-1-1). Having\
    \ such a way of developing and maintaining a rule repository is extremely important\
    \ in practice, as it is more than a one-off effort with diminishing effect, but\
    \ rather a way of systematically and incrementally approximating a language's\
    \ semantics.\n\nTo evaluate the feasibility and effectiveness of our approach,\
    \ we have applied PTE to two programming languages, i.e., Cangjie and Java. These\
    \ languages are chosen as representatives of a newly developed and a mature language.\
    \ The experimental results show that PTE finds not only compiler bugs, but also\
    \ potential issues regarding language design. In a short period of five months,\
    \ we have defined and implemented dozens of PTE rules, and identified 40 unique\
    \ bugs/issues in Cangjie and 11 bugs/issues in Java. Most of them (2 issues in\
    \ Cangjie and 3 issues in Java to be concluded) have been confirmed by the respective\
    \ compiler team and 23 bugs in Cangjie have been fixed. While more PTE semantics\
    \ rules are being developed continuously, we believe this result has successfully\
    \ demonstrated the effectiveness of our approach. In fact, PTE has now been adopted\
    \ by the Cangjie team who are working with us on building and maintaining the\
    \ PTE rules repository.\n\nTo sum up, we make the following technical contributions.\
    \ First, we propose a novel, and importantly, practical compiler testing approach\
    \ based on axiomatic semantics rules. Secondly, we define a set of PTE rules for\
    \ Cangjie and Java and demonstrate their effectiveness by identifying previously\
    \ unknown compiler bugs.\n\n### <span id=\"page-1-1\"></span>2 ILLUSTRATIVE EXAMPLES\n\
    \nBackground. Cangjie is a programming language that was newly developed by a\
    \ leading global IT company, and it is currently undergoing rapid development.\
    \ While the compiler team maintains a manually-written test suite (with about\
    \ 20000 test cases), there is still a demand to comprehensively evaluate the correctness\
    \ of the compiler. The idea of developing a formal language semantics (similarly\
    \ as for the Solidity and the Java compiler [\\[2,](#page-10-8) [13\\]](#page-10-9))\
    \ and a corresponding testing engine was proposed, but rejected due to the huge\
    \ initial effort as well as the fact that it is almost impossible for the semantics\
    \ to keep up with the rapid language development. While multiple existing compiler\
    \ testing techniques from the literature have been applied, such as EMI [\\[14\\\
    ]](#page-10-1) and SynFuzz [\\[37\\]](#page-10-12), it is observed that such techniques\
    \ often have a diminishing effect, i.e., they are able to find some bugs initially\
    \ but soon become ineffective. More importantly, it is generally agreed upon by\
    \ the testing team that many aspects of the language semantics are yet to be tested.\n\
    \nWe propose PTE as a complementary way of testing the compiler. A set of rules\
    \ in the form of (precondition, transformation, expectation) triples are gradually\
    \ developed, where precondition is the condition under which the rule applies,\
    \ transformation specifies how a program is transformed, and expectation is the\
    \ expected outcome (e.g., the program finishes in the same program state or the\
    \ same error is produced). These rules are gradually developed in two ways. One\
    \ is based on the language specification (in both Chinese and English). The other\
    \ is based on bugs that were raised and fixed, which are documented in the compiler\
    \ project repository. That is, we examine each reported bug and see whether a\
    \ rule can be developed to prevent such or similar issues in a generalized way.\n\
    \nThese rules are implemented in the form of a Cangjie program, utilizing existing\
    \ facilities such as macros and a library for abstract syntax tree (AST) analysis\
    \ (see Section [3\\)](#page-2-0). We show how PTE works using two example rules,\
    \ one derived from the language specification and one from a bug report. We remark\
    \ that all the bugs discussed below were discovered by our approach, i.e., they\
    \ were not detected by existing approaches including manual testing and multiple\
    \ automatic compiler testing methods.\n\nPTE rules from language specification.\
    \ The Cangjie language specification [\\[26\\]](#page-10-13) includes a program\
    \ (see below) with a variable initialization using a conditional expression.\n\
    \nlet num = 8; let r = if (num > 0) { 1 } else { 0 }\n\nThis example gave us the\
    \ idea that we can always replace a literal value with a conditional expression\
    \ that is equivalent to the literal. We thus formulate a PTE rule shown in Table\
    \ [2,](#page-2-1) which states that, for an assignment, the value of the variable\
    \ should remain unchanged if we rewrite the expression on the right side of '='\
    \ as a constant conditional expression whose result is always .\n\nThis PTE rule\
    \ is implemented using existing Cangjie facilities (i.e., macros) and systematically\
    \ applied to a suite of 5641 test cases for the Cangjie compiler. Out of which,\
    \ 190 fail. After examining\n\n#### <span id=\"page-2-1\"></span>Table 2: PTE\
    \ rule 2: the conditional expression identity rule\n\n| P | The program has a\
    \ statement which is either an assign         |\n|---|---------------------------------------------------------------|\n\
    |   | ment or a variable declaration with an initialization.        |\n| T | Replace\
    \ the \U0001D463<br>on the right of '=' with if(true){\U0001D463}else{\U0001D463\
    }, |\n|   | where \U0001D463<br>can be a variable, literal, or expression.   \
    \      |\n| E | The program remains compilable and behaves the same.         \
    \ |\n\n```\n1 // var b =\"\" is Bool\n2 var b = if(true){\"\" is Bool}else{\"\"\
    \ is Bool}\n3 main(): Int64 {\n4 println(b)\n5 return 0\n6 }\n```\n#### <span\
    \ id=\"page-2-3\"></span>Figure 1: The test case to which the rule in Table [2](#page-2-1)\
    \ is applied\n\nTable 3: A Cangjie bug report [\\[27\\]](#page-10-14)\n\n| Title\
    \       | Unexpected \"{}\" when using \"toTokens()\"      |\n|-------------|----------------------------------------------|\n\
    | Description | Unexpected \"{}\" when using \"toTokens()\" func |\n|        \
    \     | tion to obtain tokens from an AST node of an |\n|             | interface\
    \ which has an abstract property.    |\n| Steps of Re | 1 main():Unit{       \
    \                        |\n| production  | let input = quote(interface A{ prop<br>2\
    \     |\n|             | let a:Int64})                                |\n|   \
    \          | let nodeA = parseInterfaceDecl(input)<br>3   |\n|             | println(nodeA.toTokens())<br>4\
    \               |\n|             |                                           \
    \   |\n| Expected    | interface A { prop let a: Int64 }            |\n| Actual\
    \      | interface A { prop let a: Int64{} }          |\n\nthe failures manually,\
    \ multiple bugs are identified. Figure [1](#page-2-2) shows a simplified Cangjie\
    \ program where the original code snippet is highlighted in red and the corresponding\
    \ transformed code (after applying the rule) is marked in green. The transformed\
    \ program is expected to be equivalent to the original one. However, compiling\
    \ it triggers a crash. The error message says \"Internal Compiler Error: Semantic\
    \ error(s) in IR.\", indicating that the compiler fails to generate a valid intermediate\
    \ representation. This issue has been reported, confirmed and fixed in the Cangjie\
    \ version 0.37.2.\n\nWhile it is not hard to find anecdotes from the language\
    \ specification to develop PTE rules, in practice, the testing team is constantly\
    \ worried about whether sufficiently many PTE rules have been developed, which\
    \ is a relevant but challenging question. To answer it, we would have to check\
    \ if the axiomatic semantics is equivalent to the operational language semantics,\
    \ but this cannot be done without first developing the complete operational semantics.\
    \ A practical answer is that we can often develop new PTE rules based on bug reports,\
    \ as we illustrate below.\n\nPTE rules from bug reports. Table [3](#page-2-3)\
    \ summarizes a bug reported from the Cangjie's forum. The bug affects multiple\
    \ functions in the Cangjie core library which are designed to construct an AST\
    \ from a Cangjie program and transform it into tokens. The bug report shows a\
    \ failed test case, where an interface A is declared with an abstract property\
    \ a. Two built-in functions quote and parseInterfaceDecl are then used to construct\
    \ the AST node, which is transformed\n\n#### <span id=\"page-2-4\"></span>Table\
    \ 4: PTE rule 3: the toTokens-parse inverse rule\n\n- P The program has a statement\
    \ node.toTokens() where node is a variable or literal.\n- T Replace the statement\
    \ with parse(node.toTokens()).toTokens() where parse is a parsing function in\
    \ Cangjie.\n- E The program remains compilable and behaves the same.\n\ninto tokens\
    \ using the toTokens() function. The output is different from what is expected,\
    \ and importantly, not a compilable Cangjie program. The cause of the bug is that\
    \ a property defined in the above syntax is de-sugared during parsing to have\
    \ an empty getter and setter declaration, represented as \"{}\" internally.\n\n\
    This bug is fixed in Cangjie version 0.37.2. Based on this bug, we formulate a\
    \ generalized PTE rule (shown in Table [4\\)](#page-2-4), which intuitively says\
    \ that parsing a Cangjie program (in the form of tokens) and then generating the\
    \ tokens would give us back a compilable program which is equivalent to the original\
    \ tokens. We remark that this rule is not mentioned in the language specification\
    \ or the documentation. Nonetheless, such a rule is expected to be satisfied in\
    \ many scenarios. For instance, the toTokens() function might be used for implementing\
    \ code instrumentation (for various static or dynamic code analysis tasks), in\
    \ which case, the un-instrumented part of the program is expected to be unchanged.\n\
    \nThis PTE rule is applied to 5641 test cases for Cangjie. Out of which, 186 fail.\
    \ After examining the failures manually, multiple bugs are identified. Figure\
    \ [2](#page-3-0) shows one of the programs that produced a fault. The transformation\
    \ takes place at Lines 8 and 13, where the variable n is replaced with parseExpr(n.toTokens()).\
    \ Executing the transformed program results in a compile-time error, i.e., a violation\
    \ of the expectation. The AST of an if-expression consists of two parts, i.e.,\
    \ one or more if-blocks and one optional else-block. Our PTE rule is applied to\
    \ both Line 8 (the if-block) and Line 13 (the else-block). An error occurs at\
    \ Line 13 for the else-block when the program is compiled. The error message,\
    \ \"error: [libast]: Parsing Error in ParseExpr\", implies that the type of the\
    \ else-block node (obtained by expr.getElseBranch() at Line 10) is different from\
    \ the type of the if-block node though they should be the same. This bug is fixed\
    \ in version 0.37.2. In fact, this one PTE rule allowed us to identify 8 bugs\
    \ in total. Furthermore, it is agreed upon that this rule should be applied for\
    \ future versions of the compiler since de-sugaring may be applied to newly designed\
    \ language features.\n\n### <span id=\"page-2-0\"></span>3 OUR APPROACH\n\nIn\
    \ this section, we introduce how PTE is designed and realized. All the examples\
    \ and discussions in this section are based on our implementation in Cangjie,\
    \ although it should be clear that the approach naturally extends to other languages\
    \ such as Java.\n\n### 3.1 Overall Design\n\nThe overarching design principle\
    \ of PTE is that it should be easy to apply. Having witnessed the difficulties\
    \ of promoting (any kind of) formal notations and modeling in practice time and\
    \ time again, we decide to design PTE such that a user does not need to learn\
    \ any new language or notation to get started. Rather it should be possible to\
    \ entirely rely on existing facilities provided by the language and\n\nConference'17,\
    \ July 2017, Washington, DC, USA Guoliang Dong, Jun Sun, Richard Schumi, Bo Wang,\
    \ and Xinyu Wang\n\n```\n1 main(){\n2 let input=quote(\n3 if ( x == \"e\" ) {\
    \ return true } else { return false }\n4 )\n5 let expr=parseIfExpr(input)\n6 for\
    \ (n in expr.getIfBody()){\n7 //n.toTokens().dump()\n8 parseExpr(n.toTokens()).toTokens().dump()\n\
    9 }\n10 match (expr.getElseBranch()) {\n11 case Some(n) =>\n12 //n.toTokens().dump()\n\
    13 parseExpr(n.toTokens()).toTokens().dump()\n14 case None => ()\n15 }\n16 return\
    \ 0\n17 }\n```\nFigure 2: The test case to which the rule is applied\n\nthe compiler.\
    \ Furthermore, it should be possible to design and implement the rules incrementally\
    \ and independently, i.e., a rule does not need to rely on other rules to work\
    \ and it is not necessary to consider whether rules are overlapping or are complete.\
    \ The reason is that we would like to accumulate a large number of PTE rules over\
    \ time (so that PTE has a lasting effect) and it would be impossible to keep track\
    \ of all the rules. Lastly, it should be easy to design and implement new rules\
    \ or modify existing rules whenever a new language feature is introduced or the\
    \ semantics of an existing feature is modified. An overview of PTE's simple design\
    \ is shown in Figure [3,](#page-3-1) which consists of three main components,\
    \ i.e., an existing test suite for the compiler, a set of PTE rules and a test\
    \ engine that takes and . In practice, such a test suite is often available, e.g.,\
    \ the OpenJDK test suite for Java contains over 10000 test cases [\\[22\\]](#page-10-15)\
    \ and the test suite for Cangjie contains about 20000 test cases. In the following,\
    \ we describe the other two components in detail.\n\n### 3.2 PTE Rules\n\nThe\
    \ core part of PTE is a repository of PTE rules, each of which implements the\
    \ following interface.\n\n```\ninterface PTERule {\n   func precondition (program:\
    \ Tokens) : Bool\n   func transformation (program: Tokens) : Tokens\n   prop let\
    \ expectations: ArrayList<Expectation>\n}\n```\nA PTE rule has three parts, i.e.,\
    \ a precondition, a transformation and an expectation, which we describe in detail\
    \ in the following.\n\nPrecondition. The precondition of a PTE rule determines\
    \ if a rule is applicable to a program. It takes the form of a program written\
    \ in the same language whose compiler is under test. The input of precondition\
    \ is a program (which has the type of Tokens in Cangjie) and the output is either\
    \ true or false. The precondition is usually implemented using existing meta-programming\
    \ facilities. Note that different programming languages have varying degrees of\
    \ support for meta-programming. Some programming languages provide built-in features\
    \ specifically designed for meta-programming,\n\n<span id=\"page-3-1\"></span>![](_page_3_Figure_10.jpeg)\n\
    \nFigure 3: Overview of PTE\n\nwhereas others require more advanced techniques\
    \ to achieve similar results. For example, C++ uses templates to generate code\
    \ at compile time, while Java relies on reflection to manipulate objects at runtime.\
    \ Fortunately, Cangjie is designed explicitly with meta-programming in mind and\
    \ offer macros that allow developers to manipulate code at compile time. Often,\
    \ the precondition of a PTE rule determines whether a PTE rule should be applied\
    \ on the input program by checking whether the input program contains certain\
    \ programming features. We refer readers to the Appendix for a detailed example\
    \ on the implementation of one PTE rule.\n\nTransformation. Similarly, the transformation\
    \ of a PTE rule takes the form of a program written in the language whose compiler\
    \ is to be tested, and it is also based on existing facilities for metaprogramming.\
    \ The input of the program is a test program and the output is a transformed program.\
    \ How the transformation is done is always specific to each PTE rule.\n\nWe take\
    \ the implementation of the transformation of the PTE rule shown in Table [2](#page-2-1)\
    \ as an example. Given a program, we first convert it to an AST node, and then\
    \ enumerate every one of its child AST nodes and replace the right side part of\
    \ '=' in the assignment expression or variable declaration with the conditional\
    \ expression. For a more complicated example, the transformation function of the\
    \ PTE rule shown in Table [1](#page-1-0) would depend on additional information,\
    \ e.g., which classes are defined in the program and the inheritance relationship\
    \ among these classes.\n\nExpectation. The expectation of a PTE rule describes\
    \ the expected behavior of the transformed program. Note that since a PTE rule\
    \ is supposed to be applicable to any program satisfying the precondition, the\
    \ expectation thus is mostly program-agnostic. In our work, we define different\
    \ expectations for different stages of program compilation and execution, as shown\
    \ in Figure [4.](#page-4-0)\n\nDuring compilation, we distinguish two kinds of\
    \ expectations, i.e., and . The former means that the program successfully compiles\
    \ without any compile-time error, whereas the latter can be further detailed with\
    \ specific compiletime errors. For instance, because Cangjie is designed to be\
    \ strongtyped, assigning a String expression to a Boolean-typed variable is expected\
    \ to cause a . Note that a compiler crash is never an expectation and thus would\
    \ fail any expectation. During execution, we distinguish two expectations, i.e.,\n\
    \n<span id=\"page-4-0\"></span>\n\n![](_page_4_Figure_1.jpeg)\n\nFigure 4: Different\
    \ types of expectations\n\n<span id=\"page-4-1\"></span>\n\n| Algorithm 1: PTE\
    \ testing                |                                            |  |  |\
    \  |\n|-----------------------------------------|--------------------------------------------|--|--|--|\n\
    | 1 for each test program \U0001D4610<br>∈ \U0001D447<br>do |                \
    \                            |  |  |  |\n| 2                                 \
    \      | for each rule \U0001D45F<br>∈ \U0001D445<br>do               |  |  |\
    \  |\n| 3                                       | if \U0001D45F .\U0001D45D\U0001D45F\
    \U0001D452\U0001D450\U0001D45C\U0001D45B\U0001D451\U0001D456\U0001D461\U0001D456\
    \U0001D45C\U0001D45B(\U0001D4610)<br>then             |  |  |  |\n| 4        \
    \                               | let \U0001D4611<br>be \U0001D45F .\U0001D461\
    \U0001D45F\U0001D44E\U0001D45B\U0001D460 \U0001D453 \U0001D45C\U0001D45F\U0001D45A\
    \U0001D44E\U0001D461\U0001D456\U0001D45C\U0001D45B(\U0001D4610);      |  |  |\
    \  |\n| 5                                       | \U0001D450ℎ\U0001D452\U0001D450\
    \U0001D458\U0001D438\U0001D465\U0001D45D\U0001D452\U0001D450\U0001D461\U0001D44E\
    \U0001D461\U0001D456\U0001D45C\U0001D45B(\U0001D45F .\U0001D452\U0001D465\U0001D45D\
    \U0001D452\U0001D450\U0001D461\U0001D44E\U0001D461\U0001D456\U0001D45C\U0001D45B\
    \U0001D460, \U0001D4610, \U0001D4611); |  |  |  |\n|                         \
    \                |                                            |  |  |  |\n\nand\
    \ . The former means that the program successfully executes without any error,\
    \ whereas the latter captures the expected runtime errors. For instance, we expect\
    \ a whenever something is divided by zero. Exactly what kinds of compile-time\
    \ and runtime errors are supported depends on the standardized error handling\
    \ mechanism of the compiler. Lastly, we allow expectations that capture the relationship\
    \ between the program before and after the transformation. For instance, means\
    \ that the program before and after the transformation has the same behavior,\
    \ i.e., either both compilable and executable and having the same variable valuations\
    \ (as well as user-output) at the end of the program execution, or resulting in\
    \ the same compile-time error or runtime error.\n\nThe expectations are implemented\
    \ in Cangjie as an enum type. The default expectation is . All three PTE rules\
    \ shown in the Tables [1,](#page-1-0) [2](#page-2-1) and [4](#page-2-4) have the\
    \ expectation of . Generally, each PTE rule is allowed to declare an array of\
    \ expectations, which offers some flexibility in defining 'imprecise' PTE rules.\
    \ That is, ideally, a compiler tester should know precisely what the expected\
    \ behavior of the compiler is when given a program. However, in practice, it might\
    \ not be easy and it might simply be convenient to write PTE rules that have multiple\
    \ expectations. For instance, a rule may state that for any program containing\
    \ an Int64-typed variable , introducing the code x-=1; x++ somewhere in the program\
    \ after is initialized should either result in an equivalent program or an ArithmeticOverflowError\
    \ (since Cangjie is designed to conduct runtime overflow check for safety).\n\n\
    ### <span id=\"page-4-3\"></span>3.3 Testing with PTE Rules\n\nGiven a test suite\
    \ and a set of PTE rules , a test engine decides how to apply the rules to the\
    \ test suite. Algorithm [1](#page-4-1) shows a simple\n\n<span id=\"page-4-2\"\
    ></span>\n\n|   | 1 open class Super { public var s1: UInt32 = 1 } |  |  |  |\
    \  |  |\n|---|--------------------------------------------------|--|--|--|--|--|\n\
    |   | 2 class Base <: Super {                          |  |  |  |  |  |\n| 3 |\
    \ public var b1: UInt32 = 2                        |  |  |  |  |  |\n| 4 | //\
    \ public var obj: Super = Super()               |  |  |  |  |  |\n| 5 | public\
    \ var obj: Super                            |  |  |  |  |  |\n| 6 | init(){<br>obj\
    \ = Base()<br>}                     |  |  |  |  |  |\n\n7 }\n\n# Figure 5: An\
    \ example of applying multiple rules\n\ntesting algorithm which applies each PTE\
    \ rule to every test case in one-by-one. The function ℎ takes the program before\
    \ and after the transformation and checks whether the expectation is met according\
    \ to the value of the property expectations. For instance, if the expectation\
    \ of the PTE rule is Equiv, the test engine compiles and executes both the original\
    \ program and the transformed program independently, records the information such\
    \ as the output message and exit code for both programs, and compares them to\
    \ determine if they are equivalent. If there are multiple expectations, we check\
    \ each expectation one by one. As long as one of them is satisfied, it is considered\
    \ that the expectation is satisfied.\n\nIt is also possible and sometimes necessary\
    \ to apply multiple PTE rules on the same test case. A practical example for this\
    \ is that a compiler often applies multiple optimizations to generate more efficient\
    \ executable code (e.g., the O3 option of GCC combines many optimizations). Given\
    \ that each such optimization can be naturally encoded as a PTE rule (with the\
    \ expectation Equiv), our approach thus offers a way of checking whether applying\
    \ multiple optimizations is safe or not. To test multiple PTE rules, we simply\
    \ take a program and then apply the PTE rules one by one. That is, we first check\
    \ whether the precondition of the first rule is satisfied, apply the transformation\
    \ if it is, and then check whether the expectation is met. Afterwards, we apply\
    \ the second rule based on the transformed program, and so on. We remark that\
    \ this may also provide us with insights such as whether certain PTE rules are\
    \ exclusive, i.e., applying a certain sequence of rules always renders certain\
    \ rules inapplicable.\n\nFigure [5](#page-4-2) shows an example which reveals\
    \ a bug in the Cangjie compiler by applying two PTE rules. One is the rule shown\
    \ in Table [1,](#page-1-0) i.e., whose transformation replaces an object of class\
    \ Super with an object of its subclass Base. The other is a rule which says that\
    \ having a member variable initialized in the constructor (i.e., init()) or in\
    \ the containing class should be the same. As shown in Figure [5,](#page-4-2)\
    \ after applying both rules, the program is transformed such that Line 4 becomes\
    \ Line 5 to 6. It turns out that the program before the transformation results\
    \ in a StackOverflowError whereas the transformed program results in a compile-time\
    \ error (i.e., circular dependency), which violates the Equiv expectation.\n\n\
    ### <span id=\"page-4-4\"></span>3.4 Developing and Maintaining PTE Rules\n\n\
    So far, we aim to convince readers that all it takes to apply PTE is to develop\
    \ one PTE rule at a time, which is true except that developing and maintaining\
    \ PTE rules could be non-trivial in practice. Such difficulties can however often\
    \ be overcome by \"compromising\" the preciseness of the rule. In the following,\
    \ we illustrate the complications in developing and maintaining PTE rules through\
    \ examples.\n\nConference'17, July 2017, Washington, DC, USA Guoliang Dong, Jun\
    \ Sun, Richard Schumi, Bo Wang, and Xinyu Wang\n\n<span id=\"page-5-0\"></span>![](_page_5_Figure_2.jpeg)\n\
    \nFigure 6: Violations of the Liskov Substitution Principle\n\nDeveloping PTE\
    \ rules could be non-trivial. For instance, the rule shown in Table [1](#page-1-0)\
    \ is one of the guiding principles of Cangjie, which says that objects of a superclass\
    \ should be replaceable with objects of its subclasses. Implementing such a rule\
    \ for automatic compiler testing is however non-trivial as multiple critical details\
    \ are missing from such a general description. For instance, how do we construct\
    \ a corresponding instance of the subclass if additional state variables are needed?\
    \ In our work, we develop an imperfect implementation of the rule (see Figure\
    \ [13](#page-11-1) in Appendix for details). Intuitively, if there is an expression\
    \ of the form obj. () where obj matches a class which has at least one qualified\
    \ subclass, we then replace the base function name with the name of the subclass.\
    \ A subclass is considered qualified if its constructor's signature matches the\
    \ signature of the superclass. Note that a variety of additional information is\
    \ required for the rule's implementation, i.e., we first traverse the program\
    \ to obtain all the classes and their inheritance relationships, and then try\
    \ to infer the type of the arguments in all constructor calls. For simplicity,\
    \ we do not consider the scenario where additional state variables are required\
    \ to instantiate the subclass. We assume that only the subclass whose constructor\
    \ matches the superclass's constructor call can substitute the superclass. The\
    \ expectation is set to be Equiv.\n\nThe rule is then systematically and automatically\
    \ applied to a suite of 5641 test programs for the Cangjie compiler, 26 of which\
    \ fail. After examining the failures manually, they are classified into three\
    \ groups based on the underlying reasons for the failure. Figure [6](#page-5-0)\
    \ shows one example from each group. In particular, Figure [6a](#page-5-0) shows\
    \ a representative case (among 11 cases) where the failure is caused by polymorphism.\
    \ The subclass C2 overrides the method f1 of superclass C1 and because f1 returns\
    \ a different value in the two classes, a failure of the equivalence check occurs.\
    \ Such failures are not considered bugs (but rather a design choice of many languages).\
    \ Figure [6b](#page-5-0) shows one of the two failures that are due to the fact\
    \ that Cangjie does not support the array covariance. In particular, array covariance\
    \ allows assigning an array of a subtype to a variable of an array type of its\
    \ supertype, which is supported by many popular object-oriented languages, such\
    \ as Java, C++ and C#. After reporting this failure to the Cangjie team, we were\
    \ informed that it is a\n\n<span id=\"page-5-1\"></span> public override prop\
    \ let expectations:Array<Expectation>{ 2 get(){ //let equiv = Expectation(Equiv)\
    \ let executable = Expectation(Executable) let circurlarE =Expectation(CircularDependencyError)\
    \ let mismatchedE =Expectation(IncompatiableTypeError) return [executable,circurlarE,mismatchedE]\
    \ 8 } 9 }\n\nFigure 7: The refined expectations for the rule in Table [1](#page-1-0)\n\
    \ndesign choice due to potential type safety concerns. The remaining 13 failures\
    \ are due to a circular dependency, as exemplified in Figure [6c.](#page-5-0)\
    \ Originally, variable obj is initialized to be an instance of the superclass.\
    \ After applying the PTE rule and transforming the program, the variable is initialized\
    \ to be an instance of the subclass, resulting in a circular dependency. The circular\
    \ dependency in this case occurs because the initialization of class Base depends\
    \ on itself. This circular dependency escapes the compiler check somehow and incurs\
    \ a StackOverflowError during runtime. As discussed in Section [3.3,](#page-4-3)\
    \ combining this rule with another rule allows us to reveal a bug in the compiler.\n\
    \nGiven the above 'false alarms', the test engineer must refine the PTE rule to\
    \ avoid generating the same failures, e.g., by strengthening the precondition\
    \ and adjusting how the transformation is done or weakening the expectation to\
    \ cover such false alarms. For instance, to avoid the first group of failures,\
    \ we need to check if an invoked called function (e.g, f1()) is overwritten by\
    \ the subclass using the precondition. Instead of strengthening the precondition\
    \ using three conditions (i.e., one for each group of the failures), we can alternatively\
    \ weaken the expectation, which is often easier in practice. Figure [7](#page-5-1)\
    \ shows the amended expectations. That is, we replace the Equiv with a relaxed\
    \ one, i.e., Executable, to eliminate the false alarms caused by the polymorphism.\
    \ Furthermore, we add two new expectations CircularDependencyError and IncompatiableTypeError\
    \ to eliminate the false alarms caused by the circular dependency and the lack\
    \ of support for array covariance.\n\nWe remark that developing PTE rules is an\
    \ iterative process in general. An initial implementation of a PTE rule would\
    \ often be imperfect, e.g., it would result in multiple false alarms due to corner\
    \ cases that are overlooked. These false alarms then serve as valuable references\
    \ for refining the PTE rule and the implementation. Afterwards, we often re-run\
    \ the PTE rule and analyze the reported failures, and refine the PTE rule further\
    \ if necessary.\n\nMaintaining PTE rules could be nontrivial. Ideally, PTE should\
    \ be applied systematically such that over time a comprehensive set of rules are\
    \ developed. To make it happen, we must be able to systematically maintain the\
    \ rules so that they remain correct and applicable through language evolution.\
    \ This is particularly relevant for newly developed languages such as Cangjie\
    \ which constantly introduces new features or updates existing features, and not\
    \ unimportant for mature languages such as Java. If a new language feature is\
    \ introduced, our experience is that typically we can develop multiple new PTE\
    \ rules to capture the axiomatic\n\nsemantics of the new language feature. If\
    \ however an existing feature is amended, identifying and updating those relevant\
    \ rules that have become invalid may not be easy, as going through all the rules\
    \ each time is impractical. In practice, however, this problem is manageable.\
    \ Typically, whenever an existing language feature is amended, new test cases\
    \ which reflect the correct concrete program behavior are introduced into the\
    \ test suite, or some existing test cases are adjusted. Running those rules that\
    \ have become invalid on these test cases will most likely result in failures,\
    \ which makes it easy to identify and update such rules. For instance, PTE was\
    \ initially implemented for Cangjie 0.34.3. Since then, Cangjie has evolved to\
    \ 0.39.4 (i.e., the latest version at the time of writing), and many changes have\
    \ been introduced. When we run PTE developed for Cangjie 0.34.3 on Cangjie 0.39.4,\
    \ about 55 failures occur due to language changes. The causes of these errors\
    \ are quickly identified (e.g., the 'let' keyword is not needed when declaring\
    \ a property, shadowing a function parameter with a local variable in the function\
    \ is no longer allowed, and some built-in functions are no longer available) and\
    \ the relevant rules are fixed within an hour.\n\n### 4 IMPLEMENTATION AND EVALUATION\n\
    \nIn this section, we present details of our experiments on applying PTE in practice.\
    \ We have evaluated PTE for two compilers, one for Cangjie and one for Java (version\
    \ 20). Based on the support for macros and the built-in AST library of Cangjie\
    \ (and with the help of the Eclipse Java development tools (JDT) core library\
    \ [\\[8\\]](#page-10-16)), PTE is implemented with approximately 7500 lines of\
    \ code for Cangjie and about 3000 lines for Java. Note that the Cangjie implementation\
    \ additionally provides a range of APIs to facilitate developing PTE rules, and\
    \ streamlines the testing.\n\nIn the following, we report our experiences on applying\
    \ PTE to Cangjie and Java, by focusing on the following research questions: RQ1)\
    \ How effective is PTE in finding compiler bugs? RQ2) Which types of bugs can\
    \ PTE find? RQ3) How much effort is needed to apply PTE in practice? All the experiments\
    \ are conducted on a laptop equipped with 11th Gen Intel(R) Core(TM) i7-1165G7@2.80GHz\
    \ and 32 GB RAM, running Ubuntu 22.04.2 LTS (64 bit).\n\n### 4.1 Applying PTE\
    \ for Cangjie\n\nFor Cangjie, a set of 40 PTE rules are defined and then applied\
    \ to test the compiler. In the following, we present the experimental results\
    \ and answer the above-mentioned research questions. For a baseline analysis,\
    \ we compare PTE with three approaches that have been recently developed for Cangjie,\
    \ i.e., CJSmith, SynFuzz [\\[37\\]](#page-10-12) and MetaFuzz [\\[37\\]](#page-10-12),\
    \ which are shown to be more effective than previous approaches such as AFL [\\\
    [35\\]](#page-10-17). These three approaches are based on different techniques.\
    \ In particular, CJSmith, which follows the idea of Csmith [\\[34\\]](#page-10-3),\
    \ is provided by the Cangjie team, and it automatically generates random test\
    \ cases according to the Cangjie grammar. SynFuzz is a test case synthesis technique\
    \ which synthesizes test cases by inserting code snippets into a randomly selected\
    \ seed program. MetaFuzz is inspired by the approach known as EMI (i.e., equivalence\
    \ modulo inputs [\\[14\\]](#page-10-1)) and generates test cases by performing\
    \ semantic-preserving transformations on existing test cases. We remark that for\
    \ CJSmith and SynFuzz, the test oracle is whether a generated test case causes\
    \ a compiler crash,\n\n| Table 5: Bug finding statistics |  |\n|---------------------------------|--|\n\
    |---------------------------------|--|\n\n<span id=\"page-6-0\"></span>\n\n| Approach\
    \ | #Test Cases | #Bugs | Time (Hours) |  |  |\n|----------|-------------|-------|--------------|--|--|\n\
    | PTE      | 225640      | 34∗   | 59.3         |  |  |\n| CJSmith  | 905949 \
    \     | 0     | 72           |  |  |\n| SynFuzz  | 30525       | 0     | 72  \
    \         |  |  |\n| MetaFuzz | 41120       | 3     | 72           |  |  |\n|\
    \          |             |       |              |  |  |\n\n∗ potential design\
    \ issues are not included, which are shown in Table [6](#page-7-0)\n\nwhereas\
    \ the test oracle of MetaFuzz is whether a mutated test case produces the same\
    \ output as its original counterpart. Note that except CJSmith (which generates\
    \ test cases from scratch), SynFuzz, MetaFuzz and PTE require a suite of seed\
    \ programs. We first run all the manually crafted test cases developed by the\
    \ Cangjie team, and then keep those which pass the tests as the seed programs.\
    \ There are a total of 5641 seed programs. The toolkit of both SynFuzz and MetaFuzz\
    \ requires the seed programs to be in a specific format, and it can randomly generate\
    \ such seed programs. For a fair comparison, we use this toolkit to generate an\
    \ equal number of seed programs, i.e., 5641, in the required format, and then\
    \ take them as the seed programs for SynFuzz and MetaFuzz.\n\nRQ1: How effective\
    \ is PTE in finding bugs in Cangjie? To answer this question, we run the test\
    \ engine using the approach depicted in Section [3.3](#page-4-3) systematically\
    \ with the 40 PTE rules and with a timeout of 72 hours (the maximum testing time\
    \ used in work [\\[37\\]](#page-10-12)) on Cangjie 0.34.3 which is the latest\
    \ version at the time. The same timeout and version of Cangjie are used for both\
    \ PTE and the baseline approaches.\n\nThe results are summarized in Table [5.](#page-6-0)\
    \ Our approach identified 34 (confirmed) unique bugs, all of which were previously\
    \ unknown (we also found 6 potential design issues, which will be discussed in\
    \ RQ2). At the time of writing, 23 of them have been fixed by the Cangjie team.\
    \ The remaining bugs will be fixed in a future version. In contrast, MetaFuzz\
    \ only identified three bugs, and the remaining two baselines, i.e, CJSmith and\
    \ SynFuzz fail to identify any bugs. Although MetaFuzz identified three bugs,\
    \ we were unable to reproduce them in the later version (Cangjie 0.35.6). It is\
    \ possible that these bugs were fixed as a side-effect of the version iteration.\n\
    \nThere are multiple reasons why these approaches fail to effectively identify\
    \ bugs. First, the biggest reason is perhaps that CJSmith, SynFuzz and MetaFuzz\
    \ had been previously applied and those bugs had already been fixed. While it\
    \ is true that these approaches may have been successful in the beginning, it\
    \ certainly shows their diminishing effect, i.e., these approaches have a one-time\
    \ benefit. Furthermore, if we refer to the effectiveness of these approaches when\
    \ they were applied for the first time (i.e., reported in [\\[37\\]](#page-10-12)),\
    \ we observed that when applying CJSmith, SynFuzz, and MetaFuzz to the Cangjie\
    \ compiler of version 0.24.5, a total of 15, 53, and 39 crashes/inconsistencies\
    \ were identified, respectively. However, when these approaches were applied to\
    \ the Cangjie compiler of version 0.26.1, the numbers of crashes/inconsistencies\
    \ dropped significantly to 9, 35 and 24, respectively. Note that not every crash/inconsistency\
    \ is a unique bug, i.e., a total of 11 confirmed and unique bugs were found as\
    \ reported in [\\[37\\]](#page-10-12). Second, we observe that many test cases\
    \ generated by SynFuzz and MetaFuzz are invalid. A close investigation shows that\
    \ it is because these two approaches are implemented for an early version of Cangjie,\
    \ i.e., version 0.26.1.\n\n<span id=\"page-7-0\"></span>Table 6: Different types\
    \ of bugs found in Cangjie and Java\n\n| Type                         | #Cangjie\
    \ | #Java |\n|------------------------------|----------|-------|\n| Compiler crash\
    \               | 2        | 0     |\n| Miscompilation               | 1     \
    \   | 0     |\n| Problematic error messages   | 4        | 7     |\n| Inconsistent\
    \ error detection | 1        | 1     |\n| Bugs in core libraries       | 26  \
    \     | 0     |\n| Potential design issues      | 6        | 3     |\n\nSince\
    \ the Cangjie grammar undergoes substantial changes from version 0.26.1 to version\
    \ 0.34.3, and more importantly, SynFuzz and MetaFuzz are highly coupled with Cangjie\
    \ syntax, maintaining these approaches (i.e., so that they can be applied for\
    \ newer versions) is highly nontrivial. In comparison, PTE is more decoupled from\
    \ the Cangjie syntax (i.e., it relies on the PTE rules, the tests and the AST\
    \ library) and is easier to maintain. As long as the AST library is updated, the\
    \ changes required to update PTE are minimal. For example, from version 0.34.3\
    \ to the latest version 0.39.4, only a few minor modifications are required, as\
    \ discussed in Section [3.4.](#page-4-4)\n\nTable [5](#page-6-0) also shows the\
    \ number of test cases generated by CJSmith, SynFuzz and MetaFuzz. We can observe\
    \ that CJSmith generated the highest number of test cases, reaching 905949, but\
    \ found no bugs. On the other hand, SynFuzz and MetaFuzz were less efficient,\
    \ i.e., only generating 30525 and 41129 test cases, respectively, within 72 hours.\
    \ In contrast, PTE generated 225640 (i.e., 5641 test cases × 40 rules) test cases\
    \ within about 59 hours.\n\nTo have a fair comparison with CJSmith, SynFuzz and\
    \ MetaFuzz, we conduct an additional experiment on applying PTE to Cangjie 0.24.5.\
    \ Due to the significant syntax and semantics change between version 0.24.5 and\
    \ 0.34.3, we are only able to apply 10 of the PTE rules on Cangjie 0.24.5. With\
    \ only 10 PTE rules, we identified 63 bugs, including 10 compiler crashes (reporting\
    \ 'Segmentation fault' or 'CodeGen Error') and 3 miscompilations. We further analyze\
    \ whether the bugs that we found in Cangjie 0.24.5 and that in 0.34.3 overlap.\
    \ The results show that nearly 53% (18 out of 34) bugs are newly discovered in\
    \ Cangjie 0.34.3. In contrast, the baseline approaches only identified three bugs\
    \ in Cangjie 0.34.3. Our experience is that we are typically able to find new\
    \ bugs once we introduce new PTE rules. This result shows that PTE will have a\
    \ lasting effect if we continue to introduce new PTE rules, e.g., based on bug\
    \ reports.\n\nRQ2: Which types of bugs can PTE find in Cangjie? To answer this\
    \ question, we categorize our findings into different categories in Table [6.](#page-7-0)\
    \ A compiler crash happens during compilation and produces an error message such\
    \ as \"Internal Compiler Error\". A miscompilation occurs if the compiler generates\
    \ incorrect machine code, which either produces an incorrect execution result\
    \ or causes a program crash with an error which is not related to the program\
    \ itself. Figure [8](#page-7-1) shows such an example where the resulting machine\
    \ code crashes with a segmentation fault. The original code at Line 2 is transformed\
    \ to the code at Line 3 by the PTE rule shown in Table [2.](#page-2-1) Note that\
    \ this bug and the bug shown in Figure [1](#page-2-2) are different as they have\
    \ different causes and fixes, even though they were detected by the same PTE rule.\n\
    \nConference'17, July 2017, Washington, DC, USA Guoliang Dong, Jun Sun, Richard\
    \ Schumi, Bo Wang, and Xinyu Wang\n\n<span id=\"page-7-1\"></span>\n\n|      |\
    \ 1 public struct Test {                                            | F unhandled\
    \ signal. sig: 11, siginfo:                     |\n|------|-------------------------------------------------------------------|-----------------------------------------------------------|\n\
    | 2    | //static var x=(1,0.1,true,'a',\"a\")                               |\
    \ 0x5620cff428b0, context:                                  |\n| 3    | static\
    \ var x=if(true)(1,0.1,true,'a',\"a\")else(1,0.1,true,'a',\"a\") | 0x5620cff42780,\
    \ mutator's<br>mutatorPage: 0x7fa9a9f47000, |\n| 4    | public func getX() { Test.x\
    \ }                                     | si_addr: 0x8!                      \
    \                       |\n| 5 }  |                                          \
    \                         | F unhandled signal. sig: 6, siginfo:             \
    \         |\n|      | 6 main(){                                              \
    \           | 0x5620cff41330, context:                                  |\n| 7\
    \    | var (a_,b_,c_,d_,e_) = Test().getX()                              | 0x5620cff41200,\
    \ mutator's                                 |\n|      | 8 println(\"after:\\${a_},\\\
    ${b_},\\${c_},\\${d_},\\${e_}\")             | mutatorPage: 0x7fa9a9f47000,  \
    \                            |\n| 9    | return 0                            \
    \                              | si_addr: 0x3ea000c8b53!                     \
    \              |\n| 10 } |                                                   \
    \                | Aborted                                                   |\n\
    |      | (a) The test case                                                 | (b)\
    \ The error message                                     |\n\nFigure 8: An example\
    \ revealing the bug of miscompilation\n\n<span id=\"page-7-2\"></span>1 main()\
    \ { 2 // var ipmask=Array<Array<UInt8»([Array<UInt8>([255, 0, 0, 0]), Array<UInt8>([255,\
    \ 255, 0, 0])]) 3 var ipmask = Array<Array<UInt64»([Array<UInt64>([255, 0, 0,\
    \ 0]), Array<UInt64>([255, 255, 0, 0])]) 4 for( i in 0 .. ipmask.size ){ var ipmask\
    \ = IPMask(ipmask[i])} 5 }\n\nerror: invalid subscript operator [] on type 'Struct-Array<Struct-Array<UInt64»'\n\
    \n#### Figure 9: An example showing problematic error message\n\nCompiler error\
    \ messages are intended to help developers identify and fix issues in their code\
    \ by providing meaningful information about the whereabouts and nature of errors.\
    \ A problematic error message is one that is misleading or excessive, which makes\
    \ it difficult for developers to identify and resolve the issue in the program.\
    \ Figure [9](#page-7-2) shows such an example. In this example, the array ipmask\
    \ initially holds elements of type UInt8 (Line 2) and is transformed to the array\
    \ which holds elements of type UInt64 by the rule (Line 3). Error messages such\
    \ as mismatched types are expected. However, the actual error message points the\
    \ finger wrongly. It accuses an invalid subscript of the array ipmask, whereas\
    \ the subscript itself is valid and not the cause of the problem. This issue has\
    \ been reported to the Cangjie team, and has been confirmed and fixed. The next\
    \ category 'inconsistent error detection' refers to a situation when a compiler\
    \ behaves differently on equivalent programs in terms of bug detection. One such\
    \ example has been shown in Figure [5.](#page-4-2) The original test case has\
    \ a circular dependency problem since the initialization of the class Base depends\
    \ on itself. This circular dependency escapes the check of the compiler and incurs\
    \ a StackOverflowError during runtime. However, after moving the initialization\
    \ into the constructor, the compiler successfully detects the issue and generates\
    \ a compile-time error.\n\nMany of the detected bugs are in the Cangjie core library.\
    \ These bugs range from minor issues that cause unexpected behavior to critical\
    \ bugs that may lead to crashes or vulnerabilities. The following code reveals\
    \ one such bug in the library for type conversion.\n\n#### let count=Int64(Float64.tryParse(0.toString()).getOrThrow())\n\
    \nThe test case containing the above statement can be successfully compiled but\
    \ encounters a NoneValueException during runtime. The cause is that the function\
    \ Float64.tryParse() returns None when converting string 0 to a number of Float64\
    \ type. The PTE rule involved in discovering the bug transforms an Integer to\
    \ a String, then converts the String to a floating-point number, and finally converts\
    \ the floating-point number back to an Integer. The precondition of this rule\
    \ is that there exists an integer literal value which is assigned\n\nPTE: Axiomatic\
    \ Semantics based Compiler Testing Conference'17, July 2017, Washington, DC, USA\n\
    \n<span id=\"page-8-0\"></span>![](_page_8_Figure_1.jpeg)\n\nFigure 10: A potential\
    \ design issue\n\nto a variable of type Int64, and the expectation is that the\
    \ resulting integer value remains unchanged.\n\nBesides implementation bugs, PTE\
    \ also allows us to discover a number of potential design issues regarding the\
    \ core libraries or the language itself. These flaws can result in inconvenience\
    \ and confusion for Cangjie programmers, even though they may not be regarded\
    \ as bugs. One example is that the exponentiation operation in Cangjie does not\
    \ support negative exponent, which is an issue identified by the following transformed\
    \ test program.\n\n#### let p=Int64.parse(2.toString())\\*\\*Int64.parse(18.toString())\n\
    \nWhile there are reasons why it is designed so, negative exponents are a fundamental\
    \ mathematical concept and are useful in various applications. Furthermore, it\
    \ is supported by many programming languages such as Java and C++. Another example,\
    \ as shown in Figure [10,](#page-8-0) is that Cangjie lacks the ability to trace\
    \ where a default implementation of a function in an interface is from. That is,\
    \ if a class implements two interfaces, both of which inherits the same default\
    \ function implementation from some common ancestor (i.e., another interface),\
    \ a compile-time error occurs. The response from the Cangjie is that it is a design\
    \ choice for now, i.e., Cangjie does not maintain the information on where a default\
    \ implementation is from and has no way to check whether two implementations are\
    \ in fact from the same source. The downside of such a design choice is that the\
    \ programmers are forced to supply a new implementation which is likely a redundant\
    \ copy of the same implementation. We thus consider such a design choice questionable\
    \ and should be amended in the future.\n\nRQ3: How much effort is needed to apply\
    \ PTE for Cangjie? The effort required to apply PTE consists of multiple parts,\
    \ e.g., manual efforts for implementing PTE rules and analyzing the testing results,\
    \ and the computational resources for executing PTE.\n\nThe time of developing\
    \ PTE rules depends on the complexity of the rules and the facilities available\
    \ for meta-programming. Empirically, the time for developing a PTE rule varies\
    \ from 30 minutes to several hours using our framework. Most of the manual effort\
    \ comes from implementing and refining the transformation function of the PTE\
    \ rule. For Cangjie, the transformations could be developed with build-in macros,\
    \ and most of the rules (38 out of 40) are implemented with fewer than 100 lines\
    \ of code. Unfortunately, Java does not support such macros, and hence an external\
    \ AST processing library is required as we explain in Section [4.2.](#page-8-1)\n\
    \nIn total, it took 5 months to develop PTE for Cangjie, which includes time spent\
    \ on learning Cangjie, developing the framework, designing/implementing rules\
    \ based on the Cangjie language specification and its bug repository, and analyzing\
    \ the results. Note that once the framework is developed, adding new rules can\
    \ be done efficiently. The Cangjie testing team managed to develop 30 rules in\
    \ 3 days after PTE was adopted. The time to analyze the testing results varies\
    \ depending on the nature and complexity of the issue. In our experience, most\
    \ of the bugs are quickly identified and confirmed, whereas library/language design\
    \ issues typically take much more time (sometimes multiple meetings spanning over\
    \ weeks) to conclude.\n\nRegarding the computational resources for executing PTE,\
    \ PTE can be run with a standard laptop or desktop computer. In our experiments,\
    \ the average time is about 1.48 hours for each PTE rule (given the 5641 test\
    \ programs). The total testing time for all PTE rules is about 59.3 hours. In\
    \ contrast to alternative approaches such as CJSmith and SynFuzz which require\
    \ a manually-specified timeout, PTE finishes when the test engine finishes all\
    \ the test programs. Furthermore, we can systematically measure the coverage of\
    \ the PTE rules, which provides some test adequacy measure.\n\n### <span id=\"\
    page-8-1\"></span>4.2 Applying PTE for Java\n\nIn the following, we discuss our\
    \ (brief) experiment on testing the Java 20 compiler with PTE.\n\nRQ1: How effective\
    \ is PTE in finding bugs in Java? Since Java is a mature language, most of its\
    \ core language features have been intensively tested with various testing techniques\
    \ and also manually by the large user base. We thus focus on the newly introduced\
    \ features (i.e., preview or incubator features) in the latest Java version and\
    \ develop 20 PTE rules based on the documentation. One such example is pattern\
    \ matching for a switch or record patterns [\\[24\\]](#page-10-18). We take those\
    \ relevant tests in the openJDK test suite [\\[22\\]](#page-10-15) as seed programs.\
    \ Note that although openJDK consists of thousands of tests, most of them are\
    \ considered irrelevant since our PTE rules are concerned with the newly introduced\
    \ features. In the end, the PTE rules are applied to a range between 5 and 484\
    \ seed programs, depending on the type of the PTE rule. We run the 20 PTE rules\
    \ with the relevant seed programs as illustrated in Algorithm [1.](#page-4-1)\n\
    \nWe implement the PTE rules for Java by using an external library for AST parsing\
    \ and modifying, i.e., the Eclipse Java development tools (JDT) core library which\
    \ supports the latest Java version. Most of the effort in implementing PTE for\
    \ Java is on realizing the AST modification with the JDT library, which takes\
    \ hours to days depending on the complexity of the PTE rule. The average time\
    \ on testing all the test cases is about 32min for each PTE rule.\n\nIn total,\
    \ we found 11 issues and bugs that were previously unknown. We reported the bugs\
    \ to the Java bug database and eight of them have been confirmed (and will be\
    \ fixed).\n\nRQ2: What types of bugs can PTE find in Java? The types of bugs that\
    \ we found for Java are summarized in the last of column of Table [6.](#page-7-0)\
    \ Seven bugs (bug ID: 8311136, 8308642, 8311135, 8308638, 8313437, 8313543, and\
    \ 8313622) are related to problematic or misleading error messages. For example\
    \ (bug ID: 8308642), for the new switch feature with pattern matching, there occurs\
    \ an unhelpful error message when a break statement is forgotten (see Figure [14](#page-11-2)\
    \ in\n\n```\n1 Integer i = 5;\n2 switch (i) {\n3 case 1 −> System.out.println(\"\
    Case 1!\");\n4 case Integer i1 when i1 < 0 −> System.out.println(\"Case 2!\");\n\
    5 case 42 −> System.out.println(\"Case 3!\");\n6 default −> System.out.println(\"\
    Default Case\");\n7 }\n```\n#### Figure 11: An example of wrong Java error message\n\
    \nAppendix). The error states \"illegal fall-through to a pattern\", which is\
    \ unhelpful considering that most developers are not familiar with the fall-through\
    \ behavior of a switch statement. Moreover, they may not know that the fall-through\
    \ behavior is not allowed in the new pattern matching feature of Java 20. A more\
    \ helpful error message should rather state that there is a missing break statement.\
    \ The PTE rule that helped to find this bug is the one that introduces new redundant\
    \ switch cases. Even though such error message related bugs might seem trivial,\
    \ we believe it is important to fix them since they can cause a huge waste of\
    \ debugging effort.\n\nWe also found a case (bug ID: 8308636) of supposedly equivalent\
    \ programs resulting in different compilation results. This bug is shown in Figure\
    \ [11](#page-9-0) which includes a switch statement with the new guarded pattern\
    \ feature [\\[23\\]](#page-10-19). The switch statement contains normal static\
    \ cases, like case 1, and guarded pattern cases, like the second case. For this\
    \ example, the Java compiler produces an error with the message \"case label is\
    \ dominated by a preceding case label\" for the third case, even though it is\
    \ not the case. This error occurs due to an arbitrary design choice that requires\
    \ static cases before pattern cases. Even if we agree with this design choice,\
    \ the error message is unhelpful (and wrong) as a developer would not know about\
    \ the required order of cases.\n\nWe also found potential design issues with Java\
    \ (bug ID: 8311134, 8308640, and 8311132). For example, for record classes, which\
    \ are simple lightweight classes for storing data, we discover that a final modifier\
    \ can be added, although it is redundant. This is because records are final by\
    \ default, i.e., a record class cannot be extended. Allowing a final modifier\
    \ may confuse developers, who might think that records without this modifier are\
    \ extensible.\n\n### 5 RELATED WORK\n\nCompiler testing is a research topic that\
    \ has attracted considerable interests. Broadly speaking, all approaches can be\
    \ understood based on three key questions, i.e., the test case generation problem\
    \ [\\[11\\]](#page-10-20), the oracle problem [\\[1\\]](#page-10-21), and the\
    \ test adequacy problem [\\[40\\]](#page-10-22). For test generation, one common\
    \ approach is to manually build and maintain a test suite. Major compiler projects\
    \ like GCC, LLVM, and OpenJDK have their own manually created test suites [\\\
    [9,](#page-10-4) [22\\]](#page-10-15). These suites are designed to validate the\
    \ compilers' functionality, correctness, and performance. However, relying solely\
    \ on manually built test suites is insufficient. Achieving comprehensive coverage\
    \ would require an overwhelming number of test cases, posing significant challenges\
    \ in terms of creating and maintaining such test suites.\n\nResearchers have explored\
    \ the automatic generation of compiler test cases. Two popular categories of approaches\
    \ are grammarbased approaches [\\[10,](#page-10-23) [28\\]](#page-10-24) and mutation-based\
    \ approaches [\\[25,](#page-10-25) [31\\]](#page-10-26). Grammar-based approaches\
    \ typically start with a fixed code fragment, acting as a template or placeholder,\
    \ and then use grammar rules to generate the remaining parts of the program. Csmith\
    \ [\\[34\\]](#page-10-3) is\n\na well-known grammar-based tool based on a subset\
    \ of the C grammar. It generates programs that cover a significant portion of\
    \ C language features while avoiding undefined and unspecified behaviors. Sirer\
    \ et al. [\\[30\\]](#page-10-27) propose test generation for JVM using production\
    \ grammars. Some grammar-based approaches generate test cases solely based on\
    \ the language grammar. For example, Purdom [\\[28\\]](#page-10-24) proposed a\
    \ sentence generator that traverses the productions of a set of context-free grammar\
    \ rules. Zelenov and Zelenova [\\[36\\]](#page-10-28) present a method for producing\
    \ test cases using a BNF grammar and a coverage criteria based on the target compiler's\
    \ syntax analyzer.\n\nMutation-based approaches generate programs by modifying\
    \ existing test cases. For instance, LangFuzz [\\[12\\]](#page-10-29) takes a\
    \ seed program and randomly replaces some non-terminals in the program with code\
    \ fragments of the same type from a large code fragments pool. Another approach\
    \ named JavaTailor [\\[38\\]](#page-10-30) mutates existing test cases to increase\
    \ JVM code coverage. JavaTailor extracts various code fragments from historical\
    \ bug-revealing test programs and randomly inserts them into the seed programs.\
    \ EMI [\\[14\\]](#page-10-1) focuses on the 'dead code' of the seed program, mutating\
    \ an input program by deleting, inserting, or modifying unexecuted code.\n\nWith\
    \ a collection of automatically generated test cases, the test oracle in compiler\
    \ testing varies depending on the test case generation approach used. One popular\
    \ approach is differential testing [\\[15,](#page-10-31) [20,](#page-10-32) [21\\\
    ]](#page-10-33), which aims to identify potential bugs by comparing the results\
    \ of compiling and executing the same test case with different compilers or different\
    \ optimization levels within a single compiler. For example, in the case of Csmith,\
    \ each generated test case computes and prints a checksum of the non-pointer global\
    \ variables, allowing the detection of discrepancies among different compilers.\
    \ Note that differential testing typically requires at least one mature and well-established\
    \ compiler for the same language as a reference. However, this approach may not\
    \ be directly applicable to newly developed programming languages, like Cangjie,\
    \ where a mature compiler may not yet exist.\n\nAnother approach to address the\
    \ test oracle problem is metamorphic testing [\\[4,](#page-10-6) [29,](#page-10-34)\
    \ [39\\]](#page-10-35). Metamorphic testing focuses on defining metamorphic relations\
    \ that specify how changes in the input program should affect the output. EMI\
    \ serves as a typical example of metamorphic testing. The mutated programs generated\
    \ by EMI are expected to produce equivalent results to their original counterparts\
    \ when executed with a given set of test inputs. Bugs can be detected by comparing\
    \ the results of the mutated program and its original counterpart. Donaldson et\
    \ al. [\\[6,](#page-10-36) [7\\]](#page-10-37) introduced a metamorphic testing\
    \ approach for graphics shader compilers that includes semantics-preserving mutations\
    \ (e.g., like variable type replacement, expression reordering, or control flow\
    \ wrapping) that affects code executed during testing. There are also similar\
    \ metamorphic testing approaches [\\[19,](#page-10-38) [33\\]](#page-10-39) for\
    \ deep learning compilers.\n\nOur approach can certainly benefit from existing\
    \ approaches on compiler test generation such as grammar-based ones. The primary\
    \ aim of our approach is to solve the test oracle problem. In particular, our\
    \ approach falls under the category of metamorphic testing. However, unlike existing\
    \ metamorphic testing approaches that typically focus on a single metamorphic\
    \ relation, our approach provides a practical way for users to define metamorphic\
    \ relations in the form of PTE rules. The diversity of these metamorphic relations\
    \ enables us to uncover a broader range of language semantics and potential\n\n\
    bugs. Our approach can be argued to be more general, i.e., it is designed to work\
    \ with all types of compilers (not just domain specific ones), it supports transformations\
    \ that are not semantic-preserving, and it decouples the oracle (i.e., the PTE\
    \ rules) from the testing algorithm. Lastly, all the existing approaches mentioned\
    \ earlier do not specifically address the test adequacy problem. In contrast,\
    \ our approach has the potential capability to address this problem. Theoretically,\
    \ if our PTE rules cover all the semantics of the target language, we can claim\
    \ that our approach is testing sufficiently. In the future, we aim to explore\
    \ ways of measuring and approximating the coverage of language semantics based\
    \ on the test cases and PTE rules, and further develop ways of improving such\
    \ coverage through test generation.\n\n### 6 CONCLUSION\n\nIn this work, we propose\
    \ a new approach for compiler testing called PTE. The idea is to incrementally\
    \ develop a set of axiomatic semantic rules in the form of (precondition, transformation,\
    \ expectation), which subsequently are used to systematically test the target\
    \ compiler. We applied our approach to two compilers, i.e., the Cangjie compiler\
    \ and the Java compiler. Within a few months, we have identified 42 confirmed,\
    \ unique bugs as well as 9 potential design issues in Cangjie and Java.\n\n###\
    \ REFERENCES\n\n- <span id=\"page-10-21\"></span>[1] Earl T Barr, Mark Harman,\
    \ Phil McMinn, Muzammil Shahbaz, and Shin Yoo. 2014. The oracle problem in software\
    \ testing: A survey. IEEE transactions on software engineering 41, 5 (2014), 507–525.\n\
    - <span id=\"page-10-8\"></span>[2] Denis Bogdanas and Grigore Rosu. 2015. K-Java:\
    \ A Complete Semantics of Java. In Proceedings of the 42nd Annual ACM SIGPLAN-SIGACT\
    \ Symposium on Principles of Programming Languages, POPL 2015, Mumbai, India,\
    \ January 15-17, 2015, Sriram K. Rajamani and David Walker (Eds.). ACM, 445–456.\n\
    - <span id=\"page-10-0\"></span>[3] Junjie Chen, Jibesh Patra, Michael Pradel,\
    \ Yingfei Xiong, Hongyu Zhang, Dan Hao, and Lu Zhang. 2021. A Survey of Compiler\
    \ Testing. ACM Comput. Surv. 53, 1 (2021), 4:1–4:36.<https://doi.org/10.1145/3363562>\n\
    - <span id=\"page-10-6\"></span>[4] Tsong Yueh Chen, S. C. Cheung, and Siu-Ming\
    \ Yiu. 2020. Metamorphic Testing: A New Approach for Generating Next Test Cases.\
    \ CoRR abs/2002.12543 (2020).\n- <span id=\"page-10-7\"></span>[5] Tsong Yueh\
    \ Chen, Fei-Ching Kuo, Huai Liu, Pak-Lok Poon, Dave Towey, TH Tse, and Zhi Quan\
    \ Zhou. 2018. Metamorphic testing: A review of challenges and opportunities. ACM\
    \ Computing Surveys (CSUR) 51, 1 (2018), 1–27.\n- <span id=\"page-10-36\"></span>[6]\
    \ Alastair F. Donaldson, Hugues Evrard, Andrei Lascu, and Paul Thomson. 2017.\
    \ Automated testing of graphics shader compilers. Proc. ACM Program. Lang. 1,\
    \ OOPSLA (2017), 93:1–93:29.<https://doi.org/10.1145/3133917>\n- <span id=\"page-10-37\"\
    ></span>[7] Alastair F. Donaldson, Hugues Evrard, and Paul Thomson. 2020. Putting\
    \ Randomized Compiler Testing into Production (Experience Report). In 34th European\
    \ Conference on Object-Oriented Programming, ECOOP 2020, November 15-17, 2020,\
    \ Berlin, Germany (Virtual Conference) (LIPIcs, Vol. 166), Robert Hirschfeld and\
    \ Tobias Pape (Eds.). Schloss Dagstuhl - Leibniz-Zentrum für Informatik, 22:1–22:29.\
    \ <https://doi.org/10.4230/LIPICS.ECOOP.2020.22>\n- <span id=\"page-10-16\"></span>[8]\
    \ Eclipse. 2023. [https://github.com/eclipse-jdt/eclipse.jdt.core.](https://github.com/eclipse-jdt/eclipse.jdt.core)\
    \ Accessed Jun 26, 2023.\n- <span id=\"page-10-4\"></span>[9] GCC. 2023. 7 Testsuites.\
    \ [https://gcc.gnu.org/onlinedocs/gccint/Testsuites.html.](https://gcc.gnu.org/onlinedocs/gccint/Testsuites.html)\
    \ Accessed Jun 26, 2023.\n- <span id=\"page-10-23\"></span>[10] Hai-Feng Guo and\
    \ Zongyan Qiu. 2013. Automatic grammar-based test generation. In IFIP International\
    \ Conference on Testing Software and Systems. Springer, 17–32.\n- <span id=\"\
    page-10-20\"></span>[11] Kenneth V. Hanford. 1970. Automatic generation of test\
    \ cases. IBM Systems Journal 9, 4 (1970), 242–257.\n- <span id=\"page-10-29\"\
    ></span>[12] Christian Holler, Kim Herzig, Andreas Zeller, et al. 2012. Fuzzing\
    \ with Code Fragments.. In USENIX Security Symposium. 445–458.\n- <span id=\"\
    page-10-9\"></span>[13] Jiao Jiao, Shuanglong Kan, Shang-Wei Lin, David Sanán,\
    \ Yang Liu, and Jun Sun. 2020. Semantic Understanding of Smart Contracts: Executable\
    \ Operational Semantics of Solidity. In 2020 IEEE Symposium on Security and Privacy,\
    \ SP 2020, San Francisco, CA, USA, May 18-21, 2020. IEEE, 1695–1712.\n- <span\
    \ id=\"page-10-1\"></span>[14] Vu Le, Mehrdad Afshari, and Zhendong Su. 2014.\
    \ Compiler validation via equivalence modulo inputs. In ACM SIGPLAN Conference\
    \ on Programming Language Design and Implementation, PLDI '14, Edinburgh, United\
    \ Kingdom - June 09 - 11, 2014, Michael F. P. O'Boyle and Keshav Pingali (Eds.).\
    \ ACM, 216–226. <https://doi.org/10.1145/2594291.2594334>\n- <span id=\"page-10-31\"\
    ></span>[15] Vu Le, Chengnian Sun, and Zhendong Su. 2015. Randomized stress-testing\
    \ of linktime optimizers. In Proceedings of the 2015 international symposium on\
    \ software testing and analysis. 327–337.\n- <span id=\"page-10-10\"></span>[16]\
    \ Xavier Leroy, Sandrine Blazy, Daniel Kästner, Bernhard Schommer, Markus Pister,\
    \ and Christian Ferdinand. 2016. CompCert-a formally verified optimizing compiler.\
    \ In ERTS 2016: Embedded Real Time Software and Systems, 8th European Congress.\n\
    - <span id=\"page-10-11\"></span>[17] Barbara Liskov. 1987. Keynote address-data\
    \ abstraction and hierarchy. In Addendum to the proceedings on Object-oriented\
    \ programming systems, languages and applications (Addendum). 17–34.\n- <span\
    \ id=\"page-10-5\"></span>[18] LLVM. 2023. test-suite. [https://github.com/llvm/llvm-test-suite.git.](https://github.com/llvm/llvm-test-suite.git)\
    \ Accessed Jun 26, 2023.\n- <span id=\"page-10-38\"></span>[19] Haoyang Ma, Qingchao\
    \ Shen, Yongqiang Tian, Junjie Chen, and Shing-Chi Cheung. 2023. Fuzzing Deep\
    \ Learning Compilers with HirGen. In Proceedings of the 32nd ACM SIGSOFT International\
    \ Symposium on Software Testing and Analysis, ISSTA 2023, Seattle, WA, USA, July\
    \ 17-21, 2023, René Just and Gordon Fraser (Eds.). ACM, 248–260.<https://doi.org/10.1145/3597926.3598053>\n\
    - <span id=\"page-10-32\"></span>[20] William M McKeeman. 1998. Differential testing\
    \ for software. Digital Technical Journal 10, 1 (1998), 100–107.\n- <span id=\"\
    page-10-33\"></span>[21] Georg Ofenbeck, Tiark Rompf, and Markus Püschel. 2016.\
    \ RandIR: differential testing for embedded compilers. In Proceedings of the 2016\
    \ 7th ACM SIGPLAN Symposium on Scala. 21–30.\n- <span id=\"page-10-15\"></span>[22]\
    \ OpenJDK. 2023. test-suite. [https://openjdk.org/projects/code-tools/jtreg/intro.](https://openjdk.org/projects/code-tools/jtreg/intro.html)\
    \ [html.](https://openjdk.org/projects/code-tools/jtreg/intro.html) Accessed Jun\
    \ 26, 2023.\n- <span id=\"page-10-19\"></span><span id=\"page-10-18\"></span>[23]\
    \ OpenJDK. 2023. test-suite. [https://openjdk.org/jeps/406.](https://openjdk.org/jeps/406)\
    \ Accessed Jun 26, 2023. [24] Oracle. 2023. [https://www.oracle.com/java/technologies/javase/20-relnote-](https://www.oracle.com/java/technologies/javase/20-relnote-issues.html)\n\
    - <span id=\"page-10-25\"></span>[issues.html.](https://www.oracle.com/java/technologies/javase/20-relnote-issues.html)\
    \ Accessed Jun 26, 2023. [25] Mike Papadakis, Marinos Kintis, Jie Zhang, Yue Jia,\
    \ Yves Le Traon, and Mark Harman. 2019. Mutation testing advances: an analysis\
    \ and survey. In Advances in Computers. Vol. 112. Elsevier, 275–378.\n- <span\
    \ id=\"page-10-13\"></span>[26] PLLab. 2023. [https://gitee.com/HW-PLLab/cangjie/tree/master/docs/llvm.](https://gitee.com/HW-PLLab/cangjie/tree/master/docs/llvm)\
    \ Accessed Jun 26, 2023.\n- <span id=\"page-10-14\"></span>[27] PLLab. 2023. [https://e.gitee.com/HW-PLLab-pro/issues/list?issue=I6F8S6.](https://e.gitee.com/HW-PLLab-pro/issues/list?issue=I6F8S6)\
    \ Accessed Jun 26, 2023.\n- <span id=\"page-10-24\"></span>[28] Paul Purdom. 1972.\
    \ A sentence generator for testing parsers. BIT Numerical Mathematics 12 (1972),\
    \ 366–375.\n- <span id=\"page-10-34\"></span>[29] Sergio Segura, Gordon Fraser,\
    \ Ana B Sanchez, and Antonio Ruiz-Cortés. 2016. A survey on metamorphic testing.\
    \ IEEE Transactions on software engineering 42, 9 (2016), 805–824.\n- <span id=\"\
    page-10-27\"></span>[30] Emin Gün Sirer and Brian N Bershad. 1999. Using production\
    \ grammars in software testing. ACM SIGPLAN Notices 35, 1 (1999), 1–13.\n- <span\
    \ id=\"page-10-26\"></span>[31] Chengnian Sun, Vu Le, and Zhendong Su. 2016. Finding\
    \ compiler bugs via live code mutation. In Proceedings of the 2016 ACM SIGPLAN\
    \ International Conference on Object-Oriented Programming, Systems, Languages,\
    \ and Applications, OOPSLA 2016, part of SPLASH 2016, Amsterdam, The Netherlands,\
    \ October 30 - November 4, 2016, Eelco Visser and Yannis Smaragdakis (Eds.). ACM,\
    \ 849–863.\n- <span id=\"page-10-2\"></span>[32] Chengnian Sun, Vu Le, Qirun Zhang,\
    \ and Zhendong Su. 2016. Toward understanding compiler bugs in GCC and LLVM. In\
    \ Proceedings of the 25th International Symposium on Software Testing and Analysis,\
    \ ISSTA 2016, Saarbrücken, Germany, July 18-20, 2016, Andreas Zeller and Abhik\
    \ Roychoudhury (Eds.). ACM, 294–305.\n- <span id=\"page-10-39\"></span>[33] Dongwei\
    \ Xiao, Zhibo Liu, Yuanyuan Yuan, Qi Pang, and Shuai Wang. 2022. Metamorphic Testing\
    \ of Deep Learning Compilers. Proc. ACM Meas. Anal. Comput. Syst. 6, 1 (2022),\
    \ 15:1–15:28.<https://doi.org/10.1145/3508035>\n- <span id=\"page-10-3\"></span>[34]\
    \ Xuejun Yang, Yang Chen, Eric Eide, and John Regehr. 2011. Finding and understanding\
    \ bugs in C compilers. In Proceedings of the 32nd ACM SIGPLAN conference on Programming\
    \ language design and implementation. 283–294.\n- <span id=\"page-10-17\"></span>[35]\
    \ Michal Zalewski. 2017. American fuzzy lop.\n- <span id=\"page-10-28\"></span>[36]\
    \ Sergey Zelenov and Sophia Zelenova. 2006. Automated generation of positive and\
    \ negative tests for parsers. In Formal Approaches to Software Testing: 5th International\
    \ Workshop, FATES 2005, Edinburgh, UK, July 11, 2005, Revised Selected Papers\
    \ 5. Springer, 187–202.\n- <span id=\"page-10-12\"></span>[37] Yingquan Zhao,\
    \ Junjie Chen, Ruifeng Fu, Haojie Ye, and Zan Wang. 2023. Testing the Compiler\
    \ for a New-born Programming Language: An Industrial Case Study (Experience Paper).\
    \ In The 32nd International Symposium on Software Testing and Analysis.\n- <span\
    \ id=\"page-10-30\"></span>[38] Yingquan Zhao, Zan Wang, Junjie Chen, Mengdi Liu,\
    \ Mingyuan Wu, Yuqun Zhang, and Lingming Zhang. 2022. History-driven test program\
    \ synthesis for JVM testing. In Proceedings of the 44th International Conference\
    \ on Software Engineering. 1133–1144.\n- <span id=\"page-10-35\"></span>[39] Zhi\
    \ Quan Zhou, DH Huang, TH Tse, Zongyuan Yang, Haitao Huang, and TY Chen. 2004.\
    \ Metamorphic testing and its applications. In Proceedings of the 8th International\
    \ Symposium on Future Software Technology (ISFST 2004). Software Engineers Association\
    \ Xian, China, 346–351.\n- <span id=\"page-10-22\"></span>[40] Hong Zhu, Patrick\
    \ AV Hall, and John HR May. 1997. Software unit test coverage and adequacy. Acm\
    \ computing surveys (csur) 29, 4 (1997), 366–427.\n\n<span id=\"page-11-0\"></span>\n\
    \n<span id=\"page-11-1\"></span> class LiskovPTE <: PTERule{ public override func\
    \ precondition(tokens:Tokens): Bool {...} public override prop let expectations:\
    \ Array<Expectation> { ... } func replaceBaseName(srcExpr: CallExpr, oldBaseName:\
    \ String, newBaseName: String) { let baseName = Token(TokenKind.IDENTIFIER, newBaseName)\
    \ var newExpr = Tokens() for (tk in srcExpr.toTokens()) { if (tk.value == oldBaseName)\
    \ { newExpr += baseName } else { newExpr += tk } } return parseCallExpr(newExpr)\
    \ } public override func transformation(tokens:Tokens): Tokens{ let inheritRelations:\
    \ HashMap<String, HashSet<String>> = getInheritRelations(tokens) let classesInfoList:\
    \ HashMap<String, ClassInfo> = getClassInfoList(tokens) let argTypeMap: HashMap<String,\
    \ String> = getArgTypeMap(tokens) let topNode = parseFile(tokens) let nodes =\
    \ getAllNodesInTree(topNode) for (i in 0..nodes.size) { let node = nodes[i] if\
    \ (node is CallExpr) { let expr = node.asCallExpr() let callName = expr.getBaseFunc().toTokens()[0].value\
    \ if (inheritRelations.contains(callName)) { let currentCallArgs = parseArgs(expr,\
    \ argTypeMap) let children = tinheritRelations[callName] let candidateSubClassList\
    \ = ArrayList<ClassInfo>() for (childName in children) { let child = this.classesInfoList[childName]\
    \ if (isQualifiedSubClass(child, currentCallArgs)) { candidateSubClassList.append(child)\
    \ } } if (candidateSubClassList.size != 0){ let subClass = shuffle(candidateSubClassList)[0]\
    \ let newCallExpr=this.replaceBaseName(expr, callName, subClass.name) nodes[i]\
    \ = newCallExpr } } } } return toTokens(nodes) } }\n\n#### Figure 13: The implementation\
    \ of the Liskov Substitution Principle rule's transformation\n\n<span id=\"page-11-2\"\
    ></span> Object o = null; switch(o){ case null: System.out.println(\"null\");\
    \ case Integer i: System.out.println(\"int: \"+i); break; case Short s: System.out.println(\"\
    short: \"+s); break; default: System.out.println(\"default\"); }\n\nFigure 14:\
    \ Example Java program that produces an unhelpful error message.\n\n### APPENDIX\n\
    \n### 1) Additional Examples for Cangjie\n\nFigure [12](#page-11-3) shows the\
    \ implementation of the PTE rule shown in Table [2,](#page-2-1) based on the built-in\
    \ AST library of Cangjie. To determine if there exists an assignment expression\
    \ or a variable declaration with initialization, we enumerate each AST node of\
    \ the program one by one. \"getAllNodesInTree\" is a function used to extract\
    \ all the sub-nodes. If an AST node is either 'AssignExpr' (i.e., the AST node\
    \ type of assignment expression) or 'VarDecl' (i.e., the AST node type of variable\
    \ declaration), we return 'true'. Note that in the latter case, we further check\
    \ whether the declared variable is initialized. Given that there could be dozens\
    \ of thousands of test cases in and thousands of PTE rules, it is important that\
    \ the precondition is designed such that it is fail-fast. Furthermore, some preconditions\
    \ could be much more complicated than the example shown above. For instance, the\
    \ implementation of the 'Liskov Substitution Principle' rule requires extracting\
    \ multiple nodes from the AST to implement the precondition. Concretely, we extract\
    \ the name of constructors (as well as their signatures) of each class and the\
    \ inheritance relation between different classes.\n\n<span id=\"page-11-3\"></span>\
    \ public class EqConditioanlExprStd <: PTERule{ public override func precondition(tokens:Tokens):\
    \ Bool { let topNode = parseFile(tokens) let nodes = getAllNodesInTree(topNode)\
    \ for (node in nodes) { if (node is AssignExpr) { return true } if (node is VarDecl)\
    \ { let ve = decomposeVarDecl(decl) if (ve.contains(\"initlzr\")){ return true\
    \ } } } return false } public override func transformation(tokens:Tokens): Tokens\
    \ { let topNode = parseFile(tokens) let nodes = getAllNodesInTree(topNode) for\
    \ (i in 0..nodes.size) { let node = nodes[i] if (node is AssignExpr) { let leftRef\
    \ = expr.getLeftValue().toTokens() let rV = expr.getRightExpr().toTokens() nodes[i]\
    \ = parseAssignExpr(quote(\\$leftRef=if(true){\\$(rV)}else{\\$(rV)})) } if (node\
    \ is VarDecl) { ... } } return toTokens(nodes) } public override prop let expectations:\
    \ Array<Expectation> { get(){ return [Expectation(Equiv)] } } }\n\nFigure 12:\
    \ The implementation of the rule shown in Table [2](#page-2-1)\n\n### 2) Additional\
    \ Examples for Java\n\nFigure [14](#page-11-2) shows an example of Java program\
    \ that produces an unhelpful error message."
  decisions:
    language: '- Qualified. Reason: English Paper'
    evaluation_prompt: Qualified.
    related_work_prompt: Qualified
    novelty_prompt: Qualified
    review_only_prompt: Qualified
  llm_input_used: '## Abstract

    The correctness of a compiler affects the correctness of every program

    written in the language, and thus must be thoroughly evaluated. Existing

    automatic compiler testing methods however either rely on weak oracles (e.g.,
    a

    program behaves the same if only dead code is modified), or require substantial

    initial effort (e.g., having a complete operational language semantics). While

    the former prevents a comprehensive correctness evaluation, the latter makes

    those methods irrelevant in practice. In this work, we propose an axiomatic

    semantics based approach for testing compilers, called PTE. The idea is to

    incrementally develop a set of ``axioms'''' capturing anecdotes of the language

    semantics in the form of \emph{(\textbf{p}recondition, \textbf{t}ransformation,

    \textbf{e}xpectation) triples, which allows us to test the compiler

    automatically.} Such axioms are written in the same language whose compiler is

    under test, and can be developed either based on the language specification, or

    by generalizing the bug reports. PTE has been applied to a newly developed

    compiler (i.e., Cangjie) and a mature compiler (i.e., Java), and successfully

    identified 42 implementation bugs and 9 potential language design issues.


    ## Introduction

    A bug in a compiler potentially renders all programs written in the language problematic.
    It is thus highly desirable that we have a systematic and scalable way of evaluating
    the correctness of compilers. The challenge of effective compiler testing [\[3,](#page-10-0)
    [14,](#page-10-1) [32,](#page-10-2) [34\]](#page-10-3) is however enormous. A
    modern programming language often has many features which may evolve through time.
    That is, the compiler not only has to handle the complicated language semantics
    correctly, but also must keep up with the constant language updates. In addition,
    for efficiency reasons, a compiler is often teeming with various optimization
    options, which must be kept consistent with the evolving language semantics. All
    of this makes the task of evaluating the correctness of a compiler highly nontrivial.'
  token_usage: 2646
  time_usage: 2.4233226776123047
- title: "Learning in the Wild: Towards Leveraging Unlabeled Data for Effectively\n\
    \  Tuning Pre-trained Code Models"
  abstract: 'Pre-trained code models have recently achieved substantial improvements
    in

    many code intelligence tasks. These models are first pre-trained on large-scale

    unlabeled datasets in a task-agnostic manner using self-supervised learning,

    and then fine-tuned on labeled datasets in downstream tasks. However, the

    labeled datasets are usually limited in size (i.e., human intensive efforts),

    which may hinder the performance of pre-trained code models in specific tasks.

    To mitigate this, one possible solution is to leverage the large-scale

    unlabeled data in the tuning stage by pseudo-labeling. However, directly

    employing the pseudo-labeled data can bring a large amount of noise, i.e.,

    incorrect labels, leading to suboptimal performance. How to effectively

    leverage the noisy pseudo-labeled data is a challenging yet under-explored

    problem.In this paper, we propose a novel approach named HINT to improve

    pre-trained code models with large-scale unlabeled datasets by better utilizing

    the pseudo-labeled data. HINT includes two main modules: HybrId pseudo-labeled

    data selection and Noise-tolerant Training. In the hybrid pseudo-data selection

    module, considering the robustness issue, apart from directly measuring the

    quality of pseudo labels through training loss, we further propose to employ a

    retrieval-based method to filter low-quality pseudo-labeled data. The

    noise-tolerant training module aims to further mitigate the influence of errors

    in pseudo labels by training the model with a noise-tolerant loss function and

    by regularizing the consistency of model predictions.The experimental results

    show that HINT can better leverage those unlabeled data in a task-specific way

    and provide complementary benefits for pre-trained models, e.g., improving the

    best baseline model by 15.33%, 16.50%, and 8.98% on code summarization, defect

    detection, and assertion generation, respectively.'
  url: http://arxiv.org/abs/2401.01060v1
  keywords: ''
  document: "# Learning in the Wild: Towards Leveraging Unlabeled Data for Effectively\
    \ Tuning Pre-trained Code Models\n\nShuzheng Gao The Chinese University of Hong\
    \ Kong Hong Kong, China szgao23@cse.cuhk.edu.hk\n\n> Li Li Beihang university\
    \ Beijing, China lilicoding@ieee.org\n\nWenxin Mao Harbin Institute of Technology\
    \ Shenzhen, China maowx5519@mails.jlu.edu.cn\n\nXing Hu, Xin Xia Zhejiang university\
    \ Zhejiang, China xinghu@zju.edu.cn,xin.xia@acm.org\n\nCuiyun Gao<sup>∗</sup>\
    \ Harbin Institute of Technology Shenzhen, China gaocuiyun@hit.edu.cn\n\nMichael\
    \ R. Lyu The Chinese University of Hong Kong Hong Kong, China lyu@cse.cuhk.edu.hk\n\
    \n## ABSTRACT\n\nPre-trained code models have recently achieved substantial improvements\
    \ in many code intelligence tasks. These models are first pre-trained on large-scale\
    \ unlabeled datasets in a task-agnostic manner using self-supervised learning,\
    \ and then fine-tuned on labeled datasets in downstream tasks. However, the labeled\
    \ datasets are usually limited in size (i.e., human intensive efforts), which\
    \ may hinder the performance of pre-trained code models in specific tasks. To\
    \ mitigate this, one possible solution is to leverage the large-scale unlabeled\
    \ data in the tuning stage by pseudo-labeling, i.e., generating pseudo labels\
    \ for unlabeled data and further training the pre-trained code models with the\
    \ pseudo-labeled data. However, directly employing the pseudo-labeled data can\
    \ bring a large amount of noise, i.e., incorrect labels, leading to suboptimal\
    \ performance. How to effectively leverage the noisy pseudo-labeled data is a\
    \ challenging yet under-explored problem.\n\nIn this paper, we propose a novel\
    \ approach named HINT to improve pre-trained code models with large-scale unlabeled\
    \ datasets by better utilizing the pseudo-labeled data. HINT includes two main\
    \ modules: HybrId pseudo-labeled data selection and Noise-tolerant Training. In\
    \ the hybrid pseudo-data selection module, considering the robustness issue, apart\
    \ from directly measuring the quality of pseudo labels through training loss,\
    \ we propose to further employ a retrieval-based method to filter low-quality\
    \ pseudo-labeled data. The noise-tolerant training module aims to further mitigate\
    \ the influence of errors in pseudo labels by training the model with a noise-tolerant\
    \ loss function and by regularizing the consistency of model predictions. We evaluate\
    \ the effectiveness of HINT on three popular code intelligence tasks, including\
    \ code summarization, defect detection, and assertion generation. We build our\
    \ method on top of three popular open-source pre-trained code models. The\n\n\
    ICSE '24, April 14–20, 2024, Singapore, Singapore\n\n© 2023 Association for Computing\
    \ Machinery.\n\nACM ISBN 978-x-xxxx-xxxx-x/YY/MM. . . \\$15.00\n\n<https://doi.org/10.1145/nnnnnnn.nnnnnnn>\n\
    \nexperimental results show that HINT can better leverage those unlabeled data\
    \ in a task-specific way and provide complementary benefits for pre-trained models,\
    \ e.g., improving the best baseline model by 15.33%, 16.50%, and 8.98% on code\
    \ summarization, defect detection, and assertion generation, respectively.\n\n\
    ### CCS CONCEPTS\n\n#### • Software and its engineering → Software development\
    \ techniques;\n\n#### ACM Reference Format:\n\nShuzheng Gao, Wenxin Mao, Cuiyun\
    \ Gao, Li Li, Xing Hu, Xin Xia, and Michael R. Lyu. 2023. Learning in the Wild:\
    \ Towards Leveraging Unlabeled Data for Effectively Tuning Pre-trained Code Models.\
    \ In Proceedings of the 46th International Conference on Software Engineering\
    \ (ICSE '24), April 14–20, 2024, Lisbon, Portugal. ACM, New York, NY, USA, [13](#page-12-0)\
    \ pages. [https://doi.org/](https://doi.org/10.1145/nnnnnnn.nnnnnnn) [10.1145/nnnnnnn.nnnnnnn](https://doi.org/10.1145/nnnnnnn.nnnnnnn)\n\
    \n### 1 INTRODUCTION\n\nRecently, code intelligence has become a popular research\
    \ field in software engineering. It aims at improving developers' productivity\
    \ by providing real-time coding assistance and suggestions for them [\\[9,](#page-11-0)\
    \ [28\\]](#page-11-1). The advent of deep learning techniques, especially pretraining\
    \ techniques [\\[12,](#page-11-2) [54\\]](#page-12-1), has significantly advanced\
    \ progress in this area. Different from previous supervised learning methods that\
    \ train the model from scratch [\\[1,](#page-10-0) [72\\]](#page-12-2), these\
    \ pre-trained code models are first pre-trained on large-scale unlabeled datasets\
    \ using selfsupervised learning tasks and then fine-tuned on labeled datasets\
    \ in downstream tasks. For example, Masked Language Modeling (MLM) is one of the\
    \ most popular self-supervised pre-training tasks and is used in many pre-trained\
    \ code models such as CodeBERT [\\[14\\]](#page-11-3) and GraphCodeBERT [\\[21\\\
    ]](#page-11-4). It works by training the models to predict the masked tokens based\
    \ on the context of surrounding words. Since this process does not require human\
    \ annotation, it can be applied on large-scale unlabeled datasets, enabling the\
    \ models to acquire a vast amount of general programming knowledge. Equipped with\
    \ this ability, these pre-trained code models achieve state-of-the-art performance\
    \ on a variety of code intelligence tasks, such as code summarization and defect\
    \ detection [\\[14,](#page-11-3) [17,](#page-11-5) [20,](#page-11-6) [21\\]](#page-11-4).\n\
    \nDespite the promising results, deep learning models are known to be data-hungry\
    \ and the size of labeled datasets in downstream tasks is important for the performance\
    \ of pre-trained models [\\[23,](#page-11-7) [66\\]](#page-12-3). However, the\
    \ sizes of labeled datasets in downstream tasks are\n\n<sup>∗</sup>Corresponding\
    \ author. The author is also affiliated with Peng Cheng Laboratory and Guangdong\
    \ Provincial Key Laboratory of Novel Security Intelligence Technologies.\n\nPermission\
    \ to make digital or hard copies of all or part of this work for personal or classroom\
    \ use is granted without fee provided that copies are not made or distributed\
    \ for profit or commercial advantage and that copies bear this notice and the\
    \ full citation on the first page. Copyrights for components of this work owned\
    \ by others than ACM must be honored. Abstracting with credit is permitted. To\
    \ copy otherwise, or republish, to post on servers or to redistribute to lists,\
    \ requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.\n\
    \nusually limited due to two main reasons. On one hand, the datasets crawled from\
    \ open-source websites like Github or Stackoverflow are small in size and of low\
    \ quality. For example, as mentioned in the literature [\\[31\\]](#page-11-8),\
    \ only 6.8% JavaScript code snippets from popular GitHub repositories contain\
    \ corresponding comments, making only a few of them usable for tasks like code\
    \ summarization. Furthermore, recent studies have revealed that the quality of\
    \ existing crawled datasets is also quite poor [\\[11,](#page-11-9) [57,](#page-12-4)\
    \ [58\\]](#page-12-5). For example, as indicated in a recent work [\\[57\\]](#page-12-4),\
    \ over 40% of data in the widely-used code summarization datasets contain various\
    \ types of noise. On the other hand, due to the requirement of domain expert knowledge,\
    \ the annotation cost of code intelligence tasks is higher than other tasks in\
    \ natural language processing or computer vision, such as sentiment analysis and\
    \ image classification [\\[60\\]](#page-12-6). With insufficient annotated data\
    \ in downstream tasks, the performance of pre-trained code models is limited.\n\
    \nOne possible solution to this problem is to leverage the largescale unlabeled\
    \ data in the tuning stage by pseudo-labeling. Pseudolabeling first trains a base\
    \ model on the limited labeled dataset, which subsequently serves as a teacher\
    \ model to annotate the unlabeled dataset [\\[34,](#page-11-10) [40,](#page-11-11)\
    \ [48\\]](#page-12-7). The pseudo-labeled dataset is then merged with the original\
    \ labeled dataset to help improve the training of a new student model. By replacing\
    \ the teacher model with the stronger student model, the above process can be\
    \ iterated multiple times, aiming at improving the models themselves. This technique\
    \ leverages the unlabeled data in a task-specific way and has shown promising\
    \ results in tasks such as image classification [\\[40\\]](#page-11-11) and dialog\
    \ systems [\\[48\\]](#page-12-7). Although pseudo-labeling can enrich the labeled\
    \ dataset, directly employing the pseudo-labeled data can bring a large amount\
    \ of noise [\\[48\\]](#page-12-7). For example, as shown in Figure [1](#page-1-0)\
    \ (a), the pseudo-labeled summary of the top code snippet is not a meaningful\
    \ sentence and contains redundant tokens. Training with such noisy pseudo labels\
    \ may amplify the incorrect knowledge in the teacher model and ultimately degrades\
    \ the model's performance. However, identifying and removing noisy pseudo labels\
    \ is non-trivial due to the complex semantic of source code. Besides, it is difficult\
    \ and impractical to ensure that the filtered dataset is noise-free [\\[64,](#page-12-8)\
    \ [74\\]](#page-12-9). Therefore, how to effectively leverage the noisy pseudo-labeled\
    \ data and enable the model to be noise-tolerant for code intelligence tasks is\
    \ of vital importance, yet under-explored.\n\nIn this paper, we propose HINT with\
    \ two main components, i.e., the HybrId pseudo-labeled data selection module and\
    \ the Noisetolerant Training module. First, in the hybrid pseudo-labeled data\
    \ selection module, we propose to combine the training loss of the teacher model\
    \ and a retrieval-based method for removing the lowquality data. Specifically,\
    \ we filter out pseudo-labeled samples that present high training loss or low\
    \ label similarity with the retrieved similar training sample. To further mitigate\
    \ the influence of data noise on model performance, we propose a noise-tolerant\
    \ training objective that includes a noise-tolerant symmetric loss function and\
    \ a consistency regularization of model predictions. To evaluate the performance\
    \ of HINT, we conduct experiments on three popular code intelligence tasks including\
    \ code summarization, defect detection, and assertion generation. Following previous\
    \ work [\\[18,](#page-11-12) [51,](#page-12-10) [63\\]](#page-12-11), we build\
    \ our method on top of three popular open-source pretrained models: CodeBERT [\\\
    [14\\]](#page-11-3), CodeT5 [\\[65\\]](#page-12-12), and UniXcoder [\\[20\\]](#page-11-6).\n\
    \n<span id=\"page-1-0\"></span>![](_page_1_Figure_3.jpeg)\n\n(a) A Python example\
    \ of low-quality pseudo-labeled data (top).\n\n![](_page_1_Figure_5.jpeg)\n\n\
    (b) A Java example of high-quality pseudo-labeled data (top).\n\nFigure 1: Examples\
    \ in the code summarization task for illustrating the motivation of the hybrid\
    \ pseudo-labeled data selection method, which indicates the loss-based data selection\
    \ strategy alone may incorrectly measure the quality of pseudo labels.\n\nExtensive\
    \ experiments demonstrate that HINT can consistently improve the performance of\
    \ pre-trained code models on these code intelligence tasks. For example, HINT\
    \ improves UniXcoder by 15.33%, 16.50%, and 8.98% in terms of BLEU-4, F1, and\
    \ EM on code summarization, defect detection, and assertion generation, respectively,\
    \ indicating that our proposed HINT method can provide complementary benefits\
    \ for the pre-trained code models.\n\nIn summary, the main contributions of this\
    \ work are as follows:\n\n- (1) To the best of our knowledge, we are the first\
    \ to leverage the large-scale unlabeled data in a task-specific way in the turning\
    \ phase for code intelligence tasks.\n- (2) We propose HINT, a novel framework\
    \ to leverage large-scale unlabeled data for effectively tuning pre-trained code\
    \ models. It first selects high-quality pseudo-labeled data in a hybrid way and\
    \ then improves the model's tolerance to noisy data in the training process.\n\
    - (3) Extensive experiments on three tasks demonstrate that our method can be\
    \ built on top of a range of existing strong pretrained models and consistently\
    \ improve their performance on many downstream tasks.\n\n<span id=\"page-2-0\"\
    ></span>![](_page_2_Figure_0.jpeg)\n\nFigure 2: The overview of HINT.\n\n# 2 PROPOSED\
    \ APPROACH\n\n### 2.1 Problem Setup and Overview\n\nIn this section, we explicate\
    \ the detailed design of HINT. Formally, in code intelligence tasks such as code\
    \ summarization, we have a set of source codes and summaries . Let = {( , )} =1\
    \ denotes the labeled training dataset, where ∈ , ∈ and denotes the size of .\
    \ Let = { } =1 denote the large unlabeled dataset, where denotes the size of and\
    \ > in general. Our goal is to learn a model : ↦→ from both and that can well\
    \ predict the label of input in the test set.\n\nThe overall framework of HINT\
    \ is shown in Figure [2.](#page-2-0) We first train a teacher model on the original\
    \ labeled dataset and 1 use the teacher model to generate pseudo labels for the\
    \ unlabeled dataset. Then, 2 a hybrid pseudo-labeled data selection method that\
    \ contains loss-based selection and retrieval-based selection is proposed to filter\
    \ the code with low-quality pseudo labels (introduced in Section [2.2\\)](#page-2-1).\
    \ For further mitigating the influence of noise in pseudo labels during model\
    \ training, we propose 3 a noise-tolerant training strategy that trains the student\
    \ model with noise-tolerant symmetric cross entropy loss and consistency regularization\
    \ (introduced in Section [2.3\\)](#page-3-0). The above procedure can be iterated\
    \ multiple times, enabling the models to be self-improved (introduced in Section\
    \ [2.4\\)](#page-4-0). The algorithm is shown in Algorithm [1.](#page-3-1)\n\n\
    ## <span id=\"page-2-1\"></span>2.2 Hybrid Pseudo-labeled Data Selection\n\nOnce\
    \ we get a trained teacher model F , we use it to generate pseudo labels for unlabeled\
    \ dataset , producing a pseudo labeled dataset = {( ,ˆ )} =1 . The pseudo-labeled\
    \ data cannot be employed directly, since they may contain substantial noise and\
    \ impact the model performance. Previous studies in machine learning [\\[22,](#page-11-13)\
    \ [30\\]](#page-11-14) mainly employ loss-based selection by filtering the data\
    \ with high training loss based on the insight that neural models can well distinguish\
    \ the quality of each sample (i.e., noisy data are generally associated with higher\
    \ training loss). However, code intelligence\n\nmodels are known to suffer from\
    \ the robustness issue [\\[25\\]](#page-11-15), so solely relying on the model\
    \ training loss for noise filtering is ineffective. For the example in Figure\
    \ [1](#page-1-0) (a), we can observe that although the quality of this generated\
    \ summary is pretty poor, its loss is low in value. Specifically, when comparing\
    \ the loss of all the pseudolabeled data, it exhibits a lower loss than 83% of\
    \ the pseudo-labeled data. Besides, in Figure [1](#page-1-0) (b), the generated\
    \ pseudo summary can well describe the meaning of checking the equivalence of\
    \ two objects in the Java code snippet but its training loss value is relatively\
    \ high, i.e., surpassing 52% of the pseudo-labeled data.\n\nConsidering that code\
    \ reuse is widespread in software development [\\[35,](#page-11-16) [37\\]](#page-11-17),\
    \ apart from the loss-based selection, we propose to further select high-quality\
    \ data through a retrieval-based method. As shown in Figure [1,](#page-1-0) by\
    \ comparing the pseudo-labeled summaries and retrieved summaries, we can systematically\
    \ identify the pseudolabeled data in Figure [1](#page-1-0) (b) as a high-quality\
    \ sample and filter the low-quality pseudo-labeled data in Figure [1](#page-1-0)\
    \ (a). Specifically, in the retrieval-based selection, for each unlabeled data\
    \ , we first use the widely-used BM-25 method [\\[44\\]](#page-11-18) to retrieve\
    \ the most similar code in the labeled training set. Then we propose to compare\
    \ the similarities of and and their corresponding pseudo label ˆ and groud truth\
    \ label through normalized edit distance:\n\n$$NED(\\mathbf{x}, y) = \\begin{cases}\
    \ \\begin{array}{c} \\frac{edit\\\\_distance(\\mathbf{x}, y)}{||\\mathbf{x}||}\
    \ \\\\ \\end{array} & \\text{if } \\mathbf{x}, y \\in sequence \\end{array} \\\
    qquad (1)$$\n\nwhere ||.|| denotes the length of the sequence and I{.} is an indicator\
    \ function that returns 1 if the condition is true and 0 otherwise. Specifically,\
    \ if both ( , ) and (ˆ , ) are not higher than the threshold , we consider this\
    \ sample ( ,ˆ ) as a correctly predicted sample and add it to the selected dataset\
    \ . On the contrary, if ( , ) is lower than while (ˆ , ) is above 1 − , we choose\
    \ to filter it as it has a higher probability of being a noisy data (Line 9-11\
    \ in Algorithm [1\\)](#page-3-1). Here is a hyperparameter\n\n<span id=\"page-3-1\"\
    ></span>Algorithm 1 Algorithm of HINT\n\nInput: labeled dataset , unlabeled dataset\
    \ , threshold of edit distance , the threshold in loss-based selection , code\
    \ transformation function , iteration number Output: neural model F 1: Train the\
    \ teacher model F on 2: for each in do 3: Generate pseudo data for using F 4:\
    \ Calculate the loss of samples in using F 5: TK ← samples with the least top\
    \ % loss value in 6: ← ∅ 7: for each sample { , ′ } in do 8: Retrieve the most\
    \ similar sample ( , ) from 9: if ( , ) ≤ ∧ ( , ′ ) ≤ then 10: .insert({ , ′ })\
    \ 11: else if ( , ) ≤ ∧ ( , ′ ) ≥ 1 − then 12: continue 13: else if F ( , ′ )\
    \ ∈ TK then 14: .insert({ , ′ }) 15: end if 16: end for 17: ← ∪ 18: Train the\
    \ student model F with dataset and transformation function through Equation [4](#page-4-1)\
    \ 19: F ← F 20: end for\n\n```\n21: return model F\n```\nto control the filtering\
    \ threshold. For samples that cannot be decided by the retrieval-based selection,\
    \ we first calculate the training loss of each pseudo-labeled data by re-feeding\
    \ each code into the teacher model F and using the generated pseudo label ˆ as\
    \ the ground-truth label. We then select the top % data with the lowest loss values\
    \ among all pseudo-labeled data and add them to (Line 4-5, 13-14 in Algorithm\
    \ [1\\)](#page-3-1). Finally, we obtain a dataset = ( ,ˆ ) ′ =1 containing high-quality\
    \ pseudo-labeled data. The dataset is employed to train the student model, together\
    \ with the labeled dataset .\n\n### <span id=\"page-3-0\"></span>2.3 Noise-tolerant\
    \ Training\n\nDespite the dedicated data selection effort, it is still difficult\
    \ and impractical to ensure that the selected samples are noise-free. Besides,\
    \ the pseudo-labeled samples with minor noise are not completely harmful and can\
    \ also provide rich information for model training. For example, as shown in Figure\
    \ [3,](#page-3-2) the assertion statement generated by the teacher model mistakenly\
    \ predicts the assertion type as \"assertionEquals\". If we directly use it as\
    \ ground truth to train the model, the model may be misled. Nevertheless, this\
    \ sample still contains much valuable information since the predictions on other\
    \ positions such as the parameters are correct. Therefore, instead of directly\
    \ discarding these samples with a more strict filtering process, we propose to\
    \ leverage the pseudo-labeled data with noise-tolerant loss function and consistency\
    \ regularization during model training.\n\n<span id=\"page-3-2\"></span>![](_page_3_Figure_6.jpeg)\n\
    \norg**.**junit**.**Assert**.assertEquals**(value,adapter.getPreserved(\"prop\"\
    ))\n\n#### Figure 3: An example of a pseudo label with a minor error.\n\nNoise-tolerant\
    \ Loss Function: Previous studies [\\[64,](#page-12-8) [74\\]](#page-12-9) have\
    \ found that the widely-used cross entropy (CE) loss function is sensitive to\
    \ noisy training data. Specifically, the coefficient in the gradient of the CE\
    \ loss function − 1 ( ; ) ∇ ( ; ) assigns larger weights to samples with higher\
    \ loss and smaller weights to samples with lower loss. Since noisy data often\
    \ obtain a high training loss in the training process [\\[22,](#page-11-13) [30\\\
    ]](#page-11-14), models trained with CE loss easily focus on those noisy data\
    \ and tend to be misled. However, the reweighting coefficient in the gradient\
    \ of CE is also beneficial for model training. Directly removing it as −∇ ( ;\
    \ ) might bring the slow convergence problem [\\[74\\]](#page-12-9). To deal with\
    \ this problem, we propose to employ the Symmetric Cross Entropy loss (SCE) [\\\
    [64\\]](#page-12-8):\n\n<span id=\"page-3-3\"></span>\n$$l\\_{\\text{score}}(\\\
    mathbf{x}, \\mathbf{y}) = -\\sum\\_{c=1}^{C} (p(c|\\mathbf{x}\\_{l}) \\log(q(c|\\\
    mathbf{x})) + q(c|\\mathbf{x}\\_{l}) \\log(p(c|\\mathbf{x}))), \\tag{2}$$\n\n\
    where (|) is the prediction of the model and (|) is the corresponding ground truth\
    \ label in the dataset. denotes the number of classes in the classification task\
    \ or the size of vocabulary in the generation task. The former item represents\
    \ the CE loss, while the latter item corresponds to the Reverse Cross Entropy\
    \ (RCE) [\\[64\\]](#page-12-8) which assigns the same weights for all samples,\
    \ i.e., −∇ ( ; ). Note that log (|) is 0 for the ground truth class and the same\
    \ negative value for other classes. Based on the relation ( ∗ |) = 1 − Í =1,≠\
    \ <sup>∗</sup> (|) where <sup>∗</sup> denotes the ground class, we can achieve\
    \ the above result. In this way, the student model is less likely to be influenced\
    \ by minor errors in pseudo labels.\n\nConsistency Regularization: According to\
    \ Equation [2,](#page-3-3) the noise-tolerant loss relies on the quality of pseudo\
    \ labels. Considering that such supervision signals from pseudo labels might be\
    \ noisy and unreliable, we further propose to add consistency regularization between\
    \ predictions of the original code and the transformed code. It could enrich the\
    \ supervision signals and provide the student model with a more reliable objective\
    \ function without manual labels. Specifically, given a code snippet , we first\
    \ apply a code transformation function (·) and obtain the transformed input ().\
    \ Then, the consistency regularization is applied to align the distributions of\
    \ the predictions F () and F (()):\n\n$$dl\\_{cr} = KL(\\mathcal{F}\\_{\\mathfrak{s}}(\\\
    mathbf{x}) || \\mathcal{F}\\_{\\mathfrak{s}}(\\operatorname{ct}(\\mathbf{x})))\
    \ + KL(\\mathcal{F}\\_{\\mathfrak{s}}(\\operatorname{ct}(\\mathbf{x})) || \\mathcal{F}\\\
    _{\\mathfrak{s}}(\\mathbf{x})),\\tag{3}$$\n\nwhere (·||·) denotes the Kullback-Leibler\
    \ Divergence [\\[38\\]](#page-11-19). For generation tasks, we approximate it\
    \ by the average value per token. For code transformation methods, we follow previous\
    \ work [\\[56\\]](#page-12-13)\n\nand employ four effective transformation methods,\
    \ including Dynamic Masking, Dynamic Replacement, Dynamic Replacement of Specified\
    \ Type, and Dynamic Masking of Specified Type. For each sample, we randomly select\
    \ one transformation function in each epoch. Different from previous task-agnostic\
    \ contrastive learning pre-training methods that only focus on aligning the representation\
    \ of code and transformed code [\\[20,](#page-11-6) [33\\]](#page-11-20), our\
    \ method directly constrains the prediction F () and F (()) of code and transformed\
    \ code on downstream tasks and regularizes them in a task-specific way.\n\nFinally,\
    \ HINT trains the student model by combining the noisetolerant loss function and\
    \ consistency regularization as follows:\n\n$$I = \\sum\\_{(\\mathbf{x}, \\mathbf{y})\
    \ \\in D \\cup S} \\{ \\left. l\\_{\\text{sec}}(\\mathbf{x}, \\mathbf{y}) + l\\\
    _{\\text{sec}}(\\text{ct}(\\mathbf{x}), \\mathbf{y}) + \\mu \\cdot l\\_{\\text{cr}}\
    \ \\right\\}, \\tag{4}$$\n\n<span id=\"page-4-1\"></span>where is a hyperparameter\
    \ to balance the training signals from pseudo labels and the consistency regularization.\n\
    \n### <span id=\"page-4-0\"></span>2.4 Iterative Training\n\nBased on the aforementioned\
    \ process, we can obtain a student model that has better performance than the\
    \ teacher model. Then, we can build upon this student model and repeat the process\
    \ described above to further boost the models themselves ( 1 → 2 → 3 → 1 ). Specifically,\
    \ at the end of each iteration, the student model substitutes the teacher model,\
    \ which is then employed to generate pseudo labels for the unlabeled dataset in\
    \ the subsequent iteration. In general, the better the base model, the higher\
    \ the quality of the pseudo labels. In this way, the student model in the next\
    \ iteration is more likely to be trained on pseudo-labeled data with higher quality\
    \ and thus achieves better performance. We follow previous work [\\[47,](#page-12-14)\
    \ [48\\]](#page-12-7) and reinitialize the new student model from the pre-trained\
    \ code models in every iteration. After all iterations, the student model in the\
    \ last iteration will be used as the final model for predictions on the test set.\n\
    \n### 3 EXPERIMENTAL SETUP\n\n### 3.1 Research Questions\n\nAs we claimed above,\
    \ HINT is a generic framework that works without imposing specific assumptions\
    \ regarding data distribution, underlying models, or task characteristics, except\
    \ for the requirement that the input needs to be in the form of code. To validate\
    \ the generalizability of HINT, we propose to evaluate the performance of HINT\
    \ on a variety of pre-trained code models and three semi-supervised code intelligence\
    \ tasks with code as input. As for the data distribution, we also evaluate the\
    \ performance of HINT on cross-domain scenarios that do not have sufficient training\
    \ data and may present data distribution gap between training and unlabeled data.\
    \ Furthermore, we also explore the effectiveness of each component in HINT and\
    \ the influence of hyperparameters on its performance. In summary, we evaluate\
    \ HINT by addressing the following four research questions:\n\n- RQ1: How much\
    \ improvement can HINT provide to existing pre-trained code models?\n- RQ2: What\
    \ is the impact of each component on the performance of HINT?\n\nRQ3: How well\
    \ does HINT perform in cross-domain scenarios? RQ4: How does HINT's performance\
    \ vary under different parameter settings?\n\n### 3.2 Evaluation Tasks\n\nWe conduct\
    \ experiments on three representative code intelligence tasks: code summarization,\
    \ defect detection, and assertion generation, for covering different task types,\
    \ i.e., Code → Text, Code → Label, and Code → Code. Due to the space limitation,\
    \ we provide a more detailed description of evaluation metrics and statistics\
    \ of the benchmark datasets in our replication packages [\\[26\\]](#page-11-21).\n\
    \n3.2.1 Code Summarization. Code Summarization aims to generate useful comments\
    \ for a given code snippet. It can help alleviate the developers' cognitive efforts\
    \ in comprehending programs [\\[7,](#page-11-22) [19\\]](#page-11-23).\n\nDatasets.\
    \ In this study, we conduct experiments on two popular benchmark datasets JCSD\
    \ and PCSD, which contain Java and Python source code, respectively. The JCSD\
    \ dataset we used is publicly released by Hu et al. [\\[27\\]](#page-11-24), which\
    \ contains 87,136 pairs of Java methods and comments collected from 9,714 GitHub\
    \ repositories. The PCSD dataset comprises 92,545 functions with their respective\
    \ documentation, which is originally collected by Barone et al. [\\[3\\]](#page-10-1)\
    \ and later processed by Wei et al. [\\[69\\]](#page-12-15). For our experiments,\
    \ we directly used the benchmark datasets released by previous studies [\\[1,](#page-10-0)\
    \ [27\\]](#page-11-24), in which the datasets are divided into training, validation,\
    \ and test sets in a ratio of 8 : 1 : 1 and 6 : 2 : 2 for Java and Python, respectively.\
    \ As reported in previous work [\\[49,](#page-12-16) [55\\]](#page-12-17), there\
    \ are duplicated data in the training and test set of the JCSD dataset. Therefore,\
    \ following them, we remove the test samples that also appear in the training\
    \ or validation set and finally get a deduplicated test set with 6,489 samples.\
    \ Since there has been no dataset for the evaluation of code intelligence tasks\
    \ in a semi-supervised setting, we propose to simulate it by extending existing\
    \ datasets. Specifically, following previous studies [\\[36,](#page-11-25) [48\\\
    ]](#page-12-7), we randomly dividing the initial training data into two subsets:\
    \ labeled training data and an unlabeled dataset, with the ratio of 9:1.\n\nMetrics.\
    \ For code summarization, we follow previous work [\\[1,](#page-10-0) [16,](#page-11-26)\
    \ [49\\]](#page-12-16) and use four popular metrics BLEU-4 [\\[53\\]](#page-12-18),\
    \ ROUGE-L [\\[43\\]](#page-11-27), METEOR [\\[2\\]](#page-10-2), and CIDEr [\\\
    [61\\]](#page-12-19) for evaluation.\n\n3.2.2 Defect Detection. Defect detection\
    \ aims at identifying the vulnerabilities in the given program, which is crucial\
    \ to defend a software system from cyberattack [\\[13,](#page-11-28) [75\\]](#page-12-20).\n\
    \nDatasets. In our experiments, we utilize the widely-used Big-Vul dataset created\
    \ by Fan et al. [\\[13\\]](#page-11-28). This dataset contains 188,636 C/C++ code\
    \ snippets sourced from more than 300 GitHub projects dating from 2002 to 2019\
    \ in Common Vulnerabilities and Exposures (CVE) database. Following previous studies\
    \ [\\[13\\]](#page-11-28), we partition the dataset into training, validation,\
    \ and test sets with a ratio of 8:1:1. Same with code summarization, we also further\
    \ construct the labeled training data and unlabeled data by dividing the original\
    \ training set of Big-Vul with a ratio of 1:9.\n\nMetrics. We follow previous\
    \ work [\\[41,](#page-11-29) [75\\]](#page-12-20) and evaluate the results by\
    \ Precision (P), Recall (R), and F1.\n\n3.2.3 Assertion Generation. Assertion\
    \ Generation is the task of automatically generating meaningful assert statements\
    \ for unit\n\n| Approach  |            | JCSD   |         |        |       | PCSD\
    \   |         |        |       |\n|-----------|------------|--------|---------|--------|-------|--------|---------|--------|-------|\n\
    |           |            | BLEU-4 | ROUGE-L | METEOR | CIDEr | BLEU-4 | ROUGE-L\
    \ | METEOR | CIDEr |\n|           | Base model | 13.30  | 26.75   | 8.10   | 0.58\
    \  | 17.94  | 32.35   | 9.79   | 0.59  |\n| CodeBERT  | +HINT(1)   | 14.58* |\
    \ 29.06*  | 8.74*  | 0.69* | 18.81* | 34.18*  | 10.52* | 0.69* |\n|          \
    \ | +HINT(5)   | 14.64* | 29.00*  | 8.87*  | 0.71* | 18.86* | 34.25*  | 10.87*\
    \ | 0.72* |\n| CodeT5    | Base model | 16.67  | 34.28   | 11.39  | 1.05  | 21.13\
    \  | 40.27   | 15.69  | 1.22  |\n|           | +HINT(1)   | 18.32* | 35.49*  |\
    \ 12.36* | 1.22* | 22.33* | 41.42*  | 16.31* | 1.35* |\n|           | +HINT(5)\
    \   | 18.48* | 35.63*  | 12.29* | 1.24* | 22.55* | 41.67*  | 16.21* | 1.36* |\n\
    |           | Base model | 17.16  | 32.56   | 11.05  | 1.11  | 22.42  | 35.84\
    \   | 15.38  | 1.31  |\n| UniXcoder | +HINT(1)   | 18.90* | 35.16*  | 12.38* |\
    \ 1.28* | 23.77* | 41.67*  | 16.64* | 1.48* |\n|           | +HINT(5)   | 19.79*\
    \ | 35.83*  | 13.12* | 1.36* | 23.98* | 41.93*  | 16.83* | 1.50* |\n\n<span id=\"\
    page-5-0\"></span>Table 1: Experimental results on code summarization. \"\\*\"\
    \ denotes statistical significance in comparison to the base models (i.e., two-sided\
    \ -test with -value< 0.01).\n\ntests. It can reduce the manual efforts in writing\
    \ test cases and facilitate faster detection and diagnosis of software failures\
    \ [\\[46,](#page-12-21) [73\\]](#page-12-22).\n\nDatasets. For assertion generation,\
    \ we follow previous work [\\[46,](#page-12-21) [73\\]](#page-12-22) and use the\
    \ ATLAS dataset [\\[68\\]](#page-12-23). It contains 188,154 real-world test assertions\
    \ obtained from open-source projects in GitHub. The dataset is composed of eight\
    \ categories of assertions, and each sample in ATLAS is comprised of a focal method\
    \ and a test method which serve as the context for generating a single assertion\
    \ for the given test method. We use the original partition of ATLAS and split\
    \ it into three subsets: training, validation, and test, in an 8:1:1 ratio. The\
    \ construction of an unlabeled dataset for assertion generation is also the same\
    \ as the above two tasks. We randomly extract 90% of the training data for the\
    \ construction of the unlabeled dataset and use the remaining data as the labeled\
    \ dataset.\n\nMetrics. We follow previous work [\\[50,](#page-12-24) [73\\]](#page-12-22)\
    \ in this field and use Exact Match (EM), Longest Common Subsequence (LCS), and\
    \ Edit Distance (ED) as evaluation metrics.\n\n### 3.3 Baselines\n\nWe evaluate\
    \ the performance of HINT by building it on the top of three popular open-source\
    \ pre-trained code models, namely CodeBERT [\\[14\\]](#page-11-3), CodeT5 [\\\
    [65\\]](#page-12-12), and UniXcoder [\\[20\\]](#page-11-6). CodeBERT is a representative\
    \ pre-trained code model that is pre-trained with six programming languages and\
    \ uses Masked Language Modeling and Replace Token Detection as pre-trained tasks.\
    \ CodeT5 is a sequence-to-sequence pre-trained model which involves two coderelated\
    \ pre-training objectives: identifier tagging and masked identifier prediction.\
    \ It achieves state-of-the-art performance in many sequence generation tasks.\
    \ UniXcoder is a unified cross-modal pre-trained model which incorporates code\
    \ semantic and syntax information from AST. It is pre-trained with two new pre-training\
    \ tasks multi-modal contrastive learning and cross-modal generation to learn code\
    \ fragment representation. These models are all pretrained on CodeSearchNet [\\\
    [31\\]](#page-11-8) and CodeT5 is also pre-trained with C/CSharp code snippets\
    \ from BigQuery [\\[4\\]](#page-11-30).\n\n### 3.4 Implementation Details\n\n\
    We reproduce the results of all pre-trained models based on the official repositories\
    \ released by the model authors. In order to facilitate a fair comparison, we\
    \ ensure that the hyperparameters\n\nsuch as training epochs and learning rates\
    \ for the models with and without HINT are exactly the same. In our experiments,\
    \ we set and to 0.5 and 0.4, respectively. The maximum iteration is set to five.\
    \ To determine the percentage of selected samples, we tune the threshold in 10,\
    \ 15, 20, 25, 30, or 35 and select the best results for different datasets. Our\
    \ rationale for hyperparameter selection is discussed in Section [4.3.](#page-7-0)\
    \ When applying our pseudo-labeled data selection methods to the classification\
    \ task, we conduct Algorithm [1](#page-3-1) for each class respectively and balance\
    \ the class distribution by random down-sampling [\\[5\\]](#page-11-31). All the\
    \ experiments are conducted on an Ubuntu 20.04 server with an Intel Xeon Platinum\
    \ 8276 CPU, and 4 Nvidia Tesla A100 GPUs which have 40 GB graphic memory.\n\n\
    ### 4 EXPERIMENTAL RESULTS\n\n### 4.1 RQ1: Performance Evaluation\n\nIn this section,\
    \ we evaluate the effectiveness of HINT on three code intelligence tasks including\
    \ code summarization, defect detection, and assertion generation. We present the\
    \ results of HINT on the first iteration and the best results of HINT on all the\
    \ five iterations, namely HINT(1) and HINT(5). The results are displayed in Table\
    \ [1-](#page-5-0)[3.](#page-6-0) HINT consistently improves three pre-train code\
    \ models on all the tasks and metrics. In particular, HINT achieves 15.33%, 16.50%,\
    \ and 8.98% improvements in BLEU-4, F1, and EM over the best pretrained model\
    \ UniXcoder on the three datasets, respectively. We detail the results on each\
    \ task respectively as below.\n\nCode Summarization. As shown in Table [1,](#page-5-0)\
    \ HINT can significantly improve the performance of different existing pre-trained\
    \ code models on all datasets and metrics even with only one iteration. For example,\
    \ HINT(1) improves the BLEU-4 score of CodeT5 by 9.90% and 5.68% on two datasets,\
    \ respectively. Meanwhile, compared with the most powerful pre-trained model UniXcoder,\
    \ HINT(1) can still achieve consistent improvement, e.g., improving UniXcoder\
    \ on JCSD dataset by 10.14%, 12.04%, 7.99%, and 15.32% with respect to BLEU-4,\
    \ METEOR, ROUGE-L, and CIDEr, respectively. This indicates that HINT is effective\
    \ in leveraging the unlabeled data and benefits the strong task-agnostic pre-trained\
    \ code models in the downstream tasks.\n\nDefect Detection. Table [2](#page-6-1)\
    \ presents the results of defect detection. We can observe consistent improvement\
    \ on overall performance as in the defect detection task: HINT(5) improves the\
    \ F1 of\n\n<span id=\"page-6-1\"></span>Table 2: Experimental results on defect\
    \ detection. Statistical significance is not applicable to these metrics [\\[10\\\
    ]](#page-11-32).\n\n| Approach  |            | Precision | Recall | F1    |  |\n\
    |-----------|------------|-----------|--------|-------|--|\n|           | Base\
    \ model | 29.64     | 17.63  | 22.11 |  |\n| CodeBERT  | +HINT(1)   | 30.81  \
    \   | 21.52  | 25.34 |  |\n|           | +HINT(5)   | 32.09     | 22.36  | 26.35\
    \ |  |\n|           | Base model | 31.38     | 20.32  | 24.66 |  |\n| CodeT5 \
    \   | +HINT(1)   | 36.79     | 22.36  | 27.81 |  |\n|           | +HINT(5)   |\
    \ 37.66     | 22.36  | 28.06 |  |\n|           | Base model | 31.30     | 17.63\
    \  | 22.55 |  |\n| UniXcoder | +HINT(1)   | 33.28     | 20.96  | 25.73 |  |\n\
    |           | +HINT(5)   | 32.04     | 22.26  | 26.27 |  |\n\n<span id=\"page-6-0\"\
    ></span>Table 3: Experimental results on assertion generation. \"\\*\" denotes\
    \ statistical significance in comparison to the base models (i.e., two-sided -test\
    \ with -value< 0.01).\n\n| Approach  |            | EM     | LCS    | ED     |\n\
    |-----------|------------|--------|--------|--------|\n|           | Base model\
    \ | 31.82  | 65.99  | 21.68  |\n| CodeBERT  | +HINT(1)   | 37.75* | 69.46* | 19.05*\
    \ |\n|           | +HINT(5)   | 38.58* | 69.48* | 19.20* |\n|           | Base\
    \ model | 43.64  | 72.56  | 20.30  |\n| CodeT5    | +HINT(1)   | 46.53* | 74.32*\
    \ | 18.47* |\n|           | +HINT(5)   | 47.66* | 75.22* | 18.17* |\n|       \
    \    | Base model | 43.64  | 72.67  | 17.82  |\n| UniXcoder | +HINT(1)   | 47.13*\
    \ | 74.72* | 16.61* |\n|           | +HINT(5)   | 47.56* | 74.76* | 16.21* |\n\
    \nthree pre-trained models by 19.18%, 13.79%, and 16.50%, respectively. This indicates\
    \ that HINT can help pre-trained models to capture the patterns of vulnerable\
    \ code snippets. Besides, by comparing the results of HINT(1) and HINT(5), we\
    \ can also observe that after multiple iterations, HINT can achieve better performance,\
    \ e.g., improving HINT(1) by 2.10% F1 in average on UniXcoder.\n\nAssertion Generation.\
    \ For assertion generation, as shown in Table [3,](#page-6-0) we can observe that\
    \ HINT can improve all baseline pretrained models by a large margin. On average,\
    \ HINT(1) and HINT(5) improve the EM of these models by 11.09% and 13.14%, respectively.\
    \ Specifically, on CodeBERT, HINT(5) improves its baseline by 21.24%, 5.29%, and\
    \ 11.44% in terms of EM, LCS, and ED, respectively. This indicates that the ability\
    \ to better utilize the unlabeled data of HINT is also beneficial to generate\
    \ accurate assertion statements.\n\nAnswer to RQ1: HINT consistently improves\
    \ three pre-trained code models on all tasks and metrics, indicating its effectiveness\
    \ in leveraging unlabeled data for the pre-trained code models.\n\n### 4.2 RQ2:\
    \ Ablation Study\n\nIn this section, we explore the contribution of the hybrid\
    \ pseudolabeled data selection and the noise-tolerant training modules proposed\
    \ in HINT. We use UniXcoder as the base model since it shows the best performance\
    \ in the first research question. Besides, considering the time and resource limitation\
    \ of multiple iterations, we use HINT with one iteration for the following experiments.\
    \ Due to the\n\npage limit, we only present the results on Java in this paper\
    \ for code summarization, with results for other languages and pre-trained models\
    \ presented on our GitHub repository [\\[26\\]](#page-11-21).\n\n4.2.1 Impact\
    \ of hybrid pseudo-labeled data selection. We compare HINT with four other data\
    \ selection methods including Random selection, HINT w/o retrieval-based selection,\
    \ HINT w/o loss-based selection, and HINT w/o data selection. In HINT w/o loss-based\
    \ selection and HINT w/o retrieval-based selection, we validate the effectiveness\
    \ of two methods in hybrid pseudo-labeled data selection respectively. In HINT\
    \ w/o data selection, we remove the whole data selection process and directly\
    \ use all the generated pseudolabeled data, which aims at verifying the benefit\
    \ of data selection. In Random selection, we randomly select a subset from pseudo-labeled\
    \ data that has the same size as the subset selected by HINT. This is usually\
    \ used in controlled experiments to eliminate the potential confounding effect\
    \ of dataset size [\\[57\\]](#page-12-4). The experimental results are presented\
    \ in Table [4.](#page-7-1)\n\nLoss-based selection. We conduct this experiment\
    \ by removing the loss-based selection (Line 4-5, 13-14 in Algorithm [1\\)](#page-3-1).\
    \ From Table [4,](#page-7-1) we can observe that, without loss-based selection,\
    \ the performance of HINT decreases consistently on all the tasks. Specifically,\
    \ removing this component leads to an obvious decrease in defect detection, with\
    \ the decrease at 19.26%, 11.02%, and 14.42% regarding Precision, Recall, and\
    \ F1, respectively. This demonstrates the benefits of removing the noisy data\
    \ by the training loss.\n\nRetrieval-based selection. We conduct this experiment\
    \ by removing the retrieval-based selection (Line 9-11 in Algorithm [1\\)](#page-3-1).\
    \ As can be seen in Table [4,](#page-7-1) excluding the retrieval-based selection\
    \ process leads to a consistent drop in all tasks and metrics. The results demonstrate\
    \ the effectiveness of involving the retrievalbased strategy for data selection.\n\
    \nData selection and Random selection. As shown in Table [4,](#page-7-1) the model\
    \ suffers from a large degradation after removing the data selection procedure.\
    \ Specifically, on defect detection, the F1 of using all pseudo-labeled data is\
    \ only 22.29, much lower than the results of our method, i.e., 25.73. The performance\
    \ of random selection is even worse. For example, on assertion generation, random\
    \ selection has a decrease of 3.95%, 1.43%, and 2.95% with respect to EM, LCS,\
    \ and ED, respectively, indicating the importance of the data selection process\
    \ in HINT. This also demonstrates that directly using pseudo-labeling cannot achieve\
    \ promising results on code intelligence tasks.\n\n4.2.2 Impact of noise-tolerant\
    \ training. In this section, we validate the effectiveness of two components of\
    \ the noise-tolerant training module, i.e., noise-tolerant loss function and consistency\
    \ regularization.\n\nNoise-tolerant loss function. We conduct this experiment\
    \ by removing the noise tolerant loss in Equation [4,](#page-4-1) i.e., directly\
    \ using the cross entropy loss. From Table [4,](#page-7-1) we can observe that\
    \ removing the noise-tolerant loss results in a performance decrease in the vast\
    \ majority of cases. For example, on defect detection, HINT without the noise-tolerant\
    \ loss suffers from a decrease of 4.47% in terms of F1. This shows the importance\
    \ of using noise-tolerant loss to mitigate the negative impact of errors in pseudo\
    \ labels on the model performance.\n\nConsistency regularization. We conduct this\
    \ experiment by removing the noise tolerant loss in Equation [4,](#page-4-1) i.e.,\
    \ only use the first\n\n<span id=\"page-7-1\"></span>\n\n| Approach          \
    \              | Code Summarization |         |        |       | Defect Detection\
    \ |        |       | Assertion Generation |       |       |\n|---------------------------------|--------------------|---------|--------|-------|------------------|--------|-------|----------------------|-------|-------|\n\
    |                                 | BLEU-4             | ROUGE-L | METEOR | CIDEr\
    \ | Precision        | Recall | F1    | EM                   | LCS   | ED    |\n\
    | UniXcoder+HINT                  | 18.90              | 35.16   | 12.38  | 1.28\
    \  | 33.28            | 20.96  | 25.73 | 47.13                | 74.72 | 16.61\
    \ |\n| Random selection                | 18.34              | 34.11   | 11.70\
    \  | 1.23  | 34.39            | 16.14  | 21.97 | 45.27                | 73.65\
    \ | 17.10 |\n| -w/o loss-based selection       | 18.54              | 34.09  \
    \ | 12.34  | 1.24  | 26.87            | 18.65  | 22.02 | 45.55               \
    \ | 73.96 | 17.10 |\n| -w/o retrieval-based selection  | 18.71              |\
    \ 34.91   | 12.21  | 1.26  | 32.33            | 19.94  | 24.67 | 45.03       \
    \         | 73.50 | 17.16 |\n| -w/o data selection             | 18.27       \
    \       | 34.44   | 12.03  | 1.21  | 34.71            | 16.42  | 22.29 | 47.03\
    \                | 74.64 | 16.65 |\n| -w/o noise tolerant loss        | 18.93\
    \              | 34.96   | 12.26  | 1.29  | 33.02            | 19.57  | 24.58\
    \ | 46.64                | 74.46 | 16.56 |\n| -w/o consistency regularization\
    \ | 18.78              | 35.03   | 12.40  | 1.27  | 33.82            | 19.29 \
    \ | 24.57 | 46.81                | 74.55 | 16.70 |\n\nTable 4: Ablation study\
    \ of HINT. Best and second best results are marked in bold and underline respectively.\n\
    \n<span id=\"page-7-2\"></span>Table 5: Experimental results on cross-domain scenario.\
    \ \"\\*\" denotes statistical significance in comparison to the base models (i.e.,\
    \ two-sided -test with -value< 0.01).\n\n|                |        | Python →\
    \ Java |        |       | Java → Python |         |        |       |\n|----------------|--------|---------------|--------|-------|---------------|---------|--------|-------|\n\
    | Approach       | BLEU-4 | ROUGE-L       | METEOR | CIDEr | BLEU-4        | ROUGE-L\
    \ | METEOR | CIDEr |\n| CodeBERT       | 9.98   | 20.01         | 4.99   | 0.21\
    \  | 12.65         | 22.35   | 7.02   | 0.25  |\n| CodeBERT+HINT  | 13.82* | 27.71*\
    \        | 8.10*  | 0.60* | 16.14*        | 30.33*  | 9.89*  | 0.58* |\n| CodeT5\
    \         | 7.75   | 12.55         | 7.12   | 0.29  | 14.81         | 30.59  \
    \ | 9.66   | 0.84  |\n| CodeT5+HINT    | 14.09* | 22.51*        | 9.28*  | 0.88*\
    \ | 16.85*        | 33.70*  | 10.77* | 1.07* |\n| UniXcoder      | 12.68  | 26.03\
    \         | 8.33   | 0.66  | 13.18         | 20.94   | 10.70  | 0.64  |\n| UniXcoder+HINT\
    \ | 16.33* | 32.22*        | 10.54* | 1.03* | 17.26*        | 28.28*  | 13.14*\
    \ | 0.97* |\n\nterm in Equation [4.](#page-4-1) As can be seen in Table [4,](#page-7-1)\
    \ removing the adaptive regularization also leads to a drop in most tasks and\
    \ metrics. Specifically, removing consistency regularization leads to a decrease\
    \ of 0.63%, 4.51%, and 0.68% on three tasks regarding BLEU-4, F1, and EM, respectively,\
    \ indicating the effectiveness of providing reliable training objectives for leveraging\
    \ the pseudo-labeled data.\n\nAnswer to RQ2: All components in hybrid pseudo-labeled\
    \ data selection module and noise-tolerant training module demonstrate a positive\
    \ effect on the performance of HINT.\n\n### <span id=\"page-7-0\"></span>4.3 RQ3:\
    \ Evaluation on Cross-domain Scenario\n\nIn some programming languages, there\
    \ is often a shortage of training data. For the data-limited scenarios, transfer\
    \ learning is a popular solution which transfers the knowledge of similar domains\
    \ with sufficient data to the target domains [\\[42,](#page-11-33) [62\\]](#page-12-25).\
    \ In this section, we conduct experiments to study the effectiveness of HINT in\
    \ crossdomain scenarios, in which the model is trained on the source domain and\
    \ tested on the target domain with a different programming language. We use the\
    \ code summarization task for evaluation as it contains two kinds of programming\
    \ language. Specifically, we first train a model on the Java/Python dataset as\
    \ the source domain and then evaluate its performance on the test set of the other\
    \ (Python/Java) dataset as the target domain. As shown in Table [5,](#page-7-2)\
    \ HINT can improve the performance of pre-trained code models in the cross-domain\
    \ scenario by a large margin. Specifically, HINT improves the BLEU-4 score of\
    \ UniXcoder by 28.79% and 30.96% on Java to Python and Python to Java, respectively,\
    \ indicating that HINT can effectively utilize the knowledge in those unlabeled\
    \ data\n\n<span id=\"page-7-3\"></span>![](_page_7_Figure_8.jpeg)\n\nFigure 4:\
    \ Parameter analysis on threshold .\n\nby pseudo-labeling. This also shows HINT's\
    \ ability to enhance pretrained code models in new programming languages, regardless\
    \ of any disparities in their data distributions.\n\nAnswer to RQ3: HINT can substantially\
    \ boost the performance of pre-trained code models in cross-domain scenarios where\
    \ no annotated data exist in the target domain.\n\ncoder).\n\n<span id=\"page-8-0\"\
    ></span>![](_page_8_Figure_0.jpeg)\n\nFigure 6: Performance on each Iteration.\n\
    \n### 4.4 RQ4: Parameter Analysis\n\nIn this section, we study the impact of four\
    \ parameters on the performance of HINT, including the threshold in loss-based\
    \ data selection, the edit distance threshold in retrieval-based data selection,\
    \ the weight of consistency regularization , and the iteration number . Due to\
    \ the page limitation, we only present the results of UniXcoder and JCSD dataset\
    \ for and on code summarization, with results for other languages and pre-trained\
    \ models presented on our GitHub repository [\\[26\\]](#page-11-21).\n\nThe threshold\
    \ in loss-based data selection. We conduct experiments to evaluate how HINT performs\
    \ under different thresholds, i.e., 10%, 15%, 20%, 25%, 30%, and 35%. The larger\
    \ the is set to, the more the pseudo labeled samples will be selected. As shown\
    \ in Figure [4,](#page-7-3) the model performance shows a similar trend along\
    \ with the increase of on all pre-trained code models and tasks. HINT first increases\
    \ and achieves its peak, and then sharply descends with a larger . Larger has\
    \ the risk of involving more noisy data while smaller might be too strict and\
    \ filter many high-quality samples. Besides, we can find that the optimal value\
    \ of for different models and tasks varies a lot. For example, on code summarization,\n\
    \n0 0.25 0.5 0.75 1 0 0.25 0.5 0.75 1 UniXcoder achieves the best performance\
    \ when is set to 25%, while the optimal value for CodeBERT is 10%. We suggest\
    \ that it is because the capability of the base model on each task is different.\
    \ Specifically, the performance of UniXcoder on code summarization is very strong,\
    \ i.e. achieving 17.16 BLEU-4 on the Java dataset, while for CodeBERT, its performance\
    \ on the Java dataset is only 13.30. The poorer the performance of the based model\
    \ is, the lower the quality of pseudo-labeled data is. Therefore, when applying\
    \ HINT on different pre-trained code models, a relatively larger can be used on\
    \ a stronger base model and vice versa.\n\n<span id=\"page-8-1\"></span>0 0.25\
    \ 0.5 0.75 1 (a) Analysis of the parameter t(b) Analysis of the parameter μ The\
    \ edit distance threshold . We study the effect of , as introduced in Section\
    \ [2.3,](#page-3-0) by varying it from 0.2 to 0.5. As shown in Figure [5](#page-8-0)\
    \ (a), for both defect detection and assertion generation, HINT achieves the best\
    \ performance when is set to 0.4. Larger or lower values do not give better results.\
    \ On code summarization, setting to 0.5 only performs slightly better than 0.4.\
    \ This indicates that setting to 0.4 is more appropriate for HINT. Thus, we set\
    \ to 0.4 in this work.\n\n 0 1 2 3 4 5 6 The consistency regularization weight\
    \ . To study the impact of in HINT, we vary it from 0 to 1 and show the results\
    \ in Figure [5](#page-8-0) (b). Larger tends to give a stronger regularization\
    \ to the model. For both defect detection and assertion generation, HINT achieves\
    \ the best performance when is set to 0.5. However, on code summarization, increasing\
    \ leads to a decrease in performance. Therefore, we set to 0.5 to enable HINT\
    \ to produce relatively better results on different tasks.\n\n0 1 2 3 4 5 6 The\
    \ iteration number . We evaluate the performance of HINT on different iterations\
    \ by setting the maximum iteration to six, and present the results in Figure [6.](#page-8-1)\
    \ Iteration 0 represents the baseline results that do not use HINT. From the results,\
    \ we can observe that HINT can get better results with the growth of iterations\
    \ and achieves the peak at around the fifth iteration, indicating that HINT can\
    \ achieve self-improvement by leveraging the unlabeled data.\n\n> Answer to RQ4:\
    \ Different settings of hyperparameters can influence the performance of HINT\
    \ on different tasks. Our hyperparameter settings achieve relatively better results.\n\
    \n### 5 DISCUSSION\n\n### 5.1 What Makes HINT Work?\n\n5.1.1 HINT can better utilize\
    \ the unlabeled data for downstream tasks. To better understand how pseudo-labeling\
    \ benefits pre-trained code models, we give two examples in Figure [7](#page-9-0)\
    \ and Figure [8.](#page-9-1) The case in Figure [7](#page-9-0) shows a Java code\
    \ snippet with summaries generated by UniXcoder and UniXcoder+HINT. From the example,\
    \ we can see that the summary generated by UniXcoder only contains a simple description\
    \ without a detailed introduction to the parameters. HINT can avoid this problem\
    \ and give a more precise prediction since it can learn from more ⟨unlabeled data,\
    \ pseudo label⟩ pairs that have a similar summary pattern. We also present another\
    \ case in the assertion generation task in Figure [8.](#page-9-1) The assertion\
    \ statement generated by UniXcoder mistakenly predicts the assertion type as \"\
    assertionTrue\" since it does not learn the meaning of \"empty\" well. However,\
    \ since UniXcoder+HINT uses the code snippet in Figure [8](#page-9-1) as training\
    \ data which has the same assertion\n\ntypes as this test sample, it can correctly\
    \ predict the assertion type in Figure [8.](#page-9-1)\n\n5.1.2 HINT can select\
    \ pseudo-labeled data with higher quality. Another advantage of HINT comes from\
    \ our data selection process. HINT can select high-quality pseudo labels for model\
    \ training. As shown in Figure [1](#page-1-0) and [2,](#page-2-0) HINT identifies\
    \ low-quality pseudo-labeled data by employing both the implicit loss-based selection\
    \ and explicit retrieval-based selection. To further validate this, we calculate\
    \ the edit distance of the pseudo labels generated by UniXcoder to the ground\
    \ truth labels of the unlabeled dataset, and use the average distance on the whole\
    \ selected dataset to measure the quality of our selected dataset. Specifically,\
    \ on JCSD the average edit distance of all pseudo labels without filtering is\
    \ 53.70, which is much higher than the dataset selected by HINT, i.e., 37.16.\
    \ The results on assertion generation are the same. HINT achieves an average edit\
    \ distance of 5.34 while the average edit distance of all pseudo labels is 17.86.\
    \ This further shows that HINT can filter noisy data and select pseudo-labeled\
    \ data with higher quality for model training.\n\n### 5.2 Limitation of HINT\n\
    \nTo gain a deeper understanding of HINT's behavior and limitations, we further\
    \ investigate cases where HINT fails to make accurate predictions and conclude\
    \ two possible limitations of HINT.\n\nThe first limitation pertains to HINT's\
    \ inability to introduce additional knowledge and rectify factual knowledge errors.\
    \ From the example in the above Figure [9,](#page-10-3) UniXcoder misinterprets\
    \ the term \"bucket\\_acl\" as the name of a bucket and fails to rectify this\
    \ misunderstanding even after additional training on pseudo-labeled data. This\
    \ shows that without external feedback HINT is hard to identify and rectify the\
    \ problem on factual knowledge, which also aligns with recent findings on the\
    \ limited self-correction ability of large language models [\\[29\\]](#page-11-34).\
    \ To potentially alleviate this limitation, integrating factual knowledge into\
    \ pre-trained code models via the interaction with a knowledge base or search\
    \ engine could be further studied.\n\nThe second limitation of HINT is the reliance\
    \ on the capacity of the base model. HINT aims at autonomously synthesizing more\
    \ labeled data for model training. However, when the base model lacks sufficient\
    \ capacity, the benefits of additional training data are diminished. As depicted\
    \ in the Figure [10,](#page-10-4) despite the presence of training sample in the\
    \ pseudo-labeled data illustrating the usage of \"assertEquals\", UniXcoder still\
    \ fails to learn this and erroneously generates \"assertThat\" for the given function.\
    \ We attribute this limitation to the inherent constraints of the model's capacity\
    \ and believe that it could be mitigated by using more advanced pretrained code\
    \ models.\n\n# 5.3 Threats to Validity\n\nWe identify four main threats to validity\
    \ of our study:\n\n- (1) The selection of code intelligence tasks. We evaluate\
    \ HINT on three commonly-used code intelligence tasks: code summarization, defect\
    \ detection, and assertion generation. We aim to expand the validation of HINT\
    \ in the future by testing it on more code intelligence tasks.\n- (2) The selection\
    \ of pre-trained code models. In this paper, we select three popular open-source\
    \ pre-trained code\n\n<span id=\"page-9-0\"></span>![](_page_9_Figure_10.jpeg)\n\
    \nFigure 7: Case study on the code summarization task. The green texts highlight\
    \ the similar part between the prediction of UniXcoder+HINT and pseudo-labeled\
    \ summary.\n\n<span id=\"page-9-1\"></span>![](_page_9_Figure_12.jpeg)\n\nFigure\
    \ 8: Case study on the assertion generation task. The red and green texts highlight\
    \ the difference in predictions made by UniXcoder and UniXcoder+HINT.\n\nmodels\
    \ CodeBERT, CodeT5, and UniXcoder for evaluation. These models are all representative\
    \ and have shown state-ofthe-art performance on benchmarks [\\[20,](#page-11-6)\
    \ [65\\]](#page-12-12). Recent studies propose pre-trained models with much larger\
    \ sizes such as ChatGPT [\\[6\\]](#page-11-35) and GPT-4 [\\[52\\]](#page-12-26)\
    \ which also show impressive programming ability. However, since the weight of\
    \ these models is not publicly available, we cannot evaluate our framework on\
    \ those large language models. Besides, our framework is flexible and easy to\
    \ be applied to different pre-trained code models.\n\n(3) The selection of languages.\
    \ The datasets that we choose in experiments only contain two kinds of languages,\
    \ i.e., Java and Python. They are both popular languages. Additionally, our method\
    \ is language-agnostic and can be easily adapted to other programming languages.\n\
    \n```\n# Code from the test set:\ndef print_bucket_acl_for_user(bucket_name, user_email):\n\
    \ storage_client = storage.Client()\n bucket = storage_client.bucket(bucket_name)\n\
    \ bucket.acl.reload()\n roles = bucket.acl.user(user_email).get_roles()\n print\
    \ roles\nSummary generated by Unixcoder:\nprint the current users name for the\
    \ specified user.\nSummary generated by Unixcoder+HINT:\nprints the name for the\
    \ specified bucket and user.\nGround truth summary:\n```\nprints out a buckets\
    \ access control list for a given user.\n\nFigure 9: Error case on the code summarization\
    \ task.\n\n<span id=\"page-10-4\"></span>\n\n| Focal-test<br>from<br>test<br>set:<br>testIdAccessor(){<br>java.lang.Long<br>id<br>=<br>3L;<br>instance.setId(id);<br>\"\
    <AssertPlaceHolder>\";}<br>getId(){<br>return<br>id;<br>}                    \
    \  |  |\n|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--|\n\
    | Assertion<br>generated<br>by<br>Unixcoder+HINT:<br>org.junit.Assert.assertThat(id,<br>instance.getId());<br>Ground<br>truth<br>assertion:<br>org.junit.Assert.assertEquals(id,<br>instance.getId());\
    \ |  |\n| Focal-test<br>from<br>Unlable<br>dataset:<br>testGetName(){<br>java.lang.String<br>id<br>=<br>\"\
    id\";<br>togglePanelItem.setId(id);<br>\"<AssertPlaceHolder>\";}<br>getId(){<br>return<br>id;<br>}\
    \       |  |\n| Generated<br>pseudo<br>label:<br>org.junit.Assert.assertEquals(id,<br>togglePanelItem.getId());\
    \                                                                            \
    \                          |  |\n\n#### Figure 10: Error case on the assertion\
    \ generation task.\n\n(4) The limitation of selected metrics. We evaluate HINT\
    \ using a variety of commonly used metrics for different tasks. However, these\
    \ metrics are mainly used for evaluating accuracy and may not reflect other evaluation\
    \ aspects such as the diversity of generated code summaries. In the future, we\
    \ plan to conduct human studies to provide a more comprehensive evaluation.\n\n\
    ### 6 RELATED WORK\n\n### 6.1 Code Intelligence\n\nIn this section, we introduce\
    \ related neural code models in three tasks that are covered in our work, including\
    \ both non-pre-trained code models and pre-trained code models,\n\n6.1.1 Non-pre-trained\
    \ code models. Iyer et al. [\\[32\\]](#page-11-36) formulate code summarization\
    \ as a neural machine translation (NMT) problem and propose CODE-NN to translate\
    \ code snippets to code summaries. For better utilizing code structure information,\
    \ many works [\\[15,](#page-11-37) [39\\]](#page-11-38) in code summarization\
    \ also incorporate code-related graphs and GNN to boost performance. Recent studies\
    \ [\\[59,](#page-12-27) [70\\]](#page-12-28) further incorporate various code\
    \ structure information into the Transformer model and achieve promising performance.\
    \ As for vulnerability detection, many deep learning-based methods [\\[41,](#page-11-29)\
    \ [75\\]](#page-12-20) are proposed. For example, Devign [\\[75\\]](#page-12-20)\
    \ is proposed to learn the various vulnerability characteristics with a composite\
    \ code property graph and graph neural network. IVDetect [\\[41\\]](#page-11-29)\
    \ uses the program dependency graph and feature attention GCN to detect vulnerabilities\
    \ in the code. In assertion generation, recent studies adopt the T5 transformer\n\
    \nmodel and achieve promising results [\\[45,](#page-12-29) [46\\]](#page-12-21).\
    \ Yu et al. [\\[73\\]](#page-12-22) further involve information retrieval to generate\
    \ more accurate assertion statements.\n\n6.1.2 Pre-trained code models. Recently,\
    \ a series of pre-trained code models [\\[14,](#page-11-3) [21,](#page-11-4) [65\\\
    ]](#page-12-12) are proposed and achieve state-of-theart performance on various\
    \ code intelligence tasks such as code summarization and defect detection. CodeBERT\
    \ [\\[14\\]](#page-11-3) is a pioneer work that is pre-trained with six programming\
    \ languages and uses Masked Language Modeling and Replace Token Detection as pretrained\
    \ tasks. CodeT5 [\\[65\\]](#page-12-12) is a sequence-to-sequence pre-trained\
    \ model which involves two code-related pre-training objectives: identifier tagging\
    \ and masked identifier prediction. UniXcoder [\\[20\\]](#page-11-6) is a unified\
    \ cross-modal pre-trained model which incorporates code semantic and syntax information\
    \ from AST.\n\n### 6.2 Pseudo-labeling\n\nPseudo-labeling is one of the most widely-used\
    \ semi-supervised learning methods. It has been applied to different kinds of\
    \ tasks such as image classification [\\[40,](#page-11-11) [71\\]](#page-12-30),\
    \ machine translation [\\[24,](#page-11-39) [34\\]](#page-11-10), and dialog systems\
    \ [\\[48\\]](#page-12-7). To further boost the performance of selftraining in\
    \ sequence generation tasks, He et al. [\\[24\\]](#page-11-39) and Mi et al. [\\\
    [48\\]](#page-12-7) explore the data augmentation technique and use random noise\
    \ or gradient-based data augmentation to improve the generalization of the student\
    \ model. Another line of work [\\[8,](#page-11-40) [34,](#page-11-10) [67\\]](#page-12-31)\
    \ focus on the data selection procedure and propose to select highquality pseudo\
    \ labeled data based on the uncertainty or the model confidence, respectively.\
    \ However, these methods mainly filter the pseudo-labeled data only with training\
    \ loss and do not take the noisy data problem into consideration. Different from\
    \ them, we propose a hybrid data selection method with the training loss and a\
    \ retrieval-based method based on the code reuse. Additionally, we also propose\
    \ a noise-tolerant training module to further mitigate the influence of noise\
    \ on model performance.\n\n### 7 CONCLUSION\n\nIn this paper, we investigate leveraging\
    \ large-scale unlabeled datasets for effectively tuning pre-trained code models\
    \ by pseudo-labeling. We propose a method called HINT which consists of two main\
    \ components, the hybrid pseudo-labeled data selection module and the noise-tolerant\
    \ training module. Extensive experiments on three code intelligence tasks show\
    \ that HINT can be built on a variety of pre-trained models and provide complementary\
    \ benefits for them. Our replication package including our source code, experimental\
    \ data, and detailed experiment results is at [\\[26\\]](#page-11-21).\n\n###\
    \ REFERENCES\n\n- <span id=\"page-10-0\"></span>[1] Wasi Uddin Ahmad, Saikat Chakraborty,\
    \ Baishakhi Ray, and Kai-Wei Chang. 2020. A Transformer-based Approach for Source\
    \ Code Summarization. In Proceedings of the 58th Annual Meeting of the Association\
    \ for Computational Linguistics, ACL 2020. Association for Computational Linguistics,\
    \ 4998–5007.\n- <span id=\"page-10-2\"></span>[2] Satanjeev Banerjee and Alon\
    \ Lavie. 2005. METEOR: An Automatic Metric for MT Evaluation with Improved Correlation\
    \ with Human Judgments. In Proceedings of the Workshop on Intrinsic and Extrinsic\
    \ Evaluation Measures for Machine Translation and/or Summarization@ACL 2005, Ann\
    \ Arbor, Michigan, USA, June 29, 2005. Association for Computational Linguistics,\
    \ 65–72.\n- <span id=\"page-10-1\"></span>[3] Antonio Valerio Miceli Barone and\
    \ Rico Sennrich. 2017. A parallel corpus of Python functions and documentation\
    \ strings for automated code documentation and code generation. arXiv preprint\
    \ arXiv:1707.02275 (2017).\n- <span id=\"page-11-30\"></span>[4] BigQuery. 2022.\
    \ BigQuery. [https://console.cloud.google.com/marketplace/details/](https://console.cloud.google.com/marketplace/details/github/github-repos)\
    \ [github/github-repos.](https://console.cloud.google.com/marketplace/details/github/github-repos)\n\
    - <span id=\"page-11-31\"></span>[5] Christopher M Bishop and Nasser M Nasrabadi.\
    \ 2006. Pattern recognition and machine learning. Vol. 4. Springer.\n- <span id=\"\
    page-11-35\"></span>[6] ChatGPT. 2022. ChatGPT. [https://openai.com/blog/chatgpt.](https://openai.com/blog/chatgpt)\n\
    - <span id=\"page-11-22\"></span>[7] Jie-Cherng Chen and Sun-Jen Huang. 2009.\
    \ An empirical analysis of the impact of software development problem factors\
    \ on software maintainability. J. Syst. Softw. 82, 6 (2009), 981–992.\n- <span\
    \ id=\"page-11-40\"></span>[8] Yiming Chen, Yan Zhang, Chen Zhang, Grandee Lee,\
    \ Ran Cheng, and Haizhou Li. 2021. Revisiting Self-training for Few-shot Learning\
    \ of Language Model. In Proceedings of the 2021 Conference on Empirical Methods\
    \ in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican\
    \ Republic, 7-11 November, 2021. Association for Computational Linguistics, 9125–9135.\n\
    - <span id=\"page-11-0\"></span>[9] Matteo Ciniselli, Luca Pascarella, Emad Aghajani,\
    \ Simone Scalabrino, Rocco Oliveto, and Gabriele Bavota. 2023. Source Code Recommender\
    \ Systems: The Practitioners' Perspective. In 45th IEEE/ACM International Conference\
    \ on Software Engineering, ICSE 2023, Melbourne, Australia, May 14-20, 2023. IEEE,\
    \ 2161–2172.\n- <span id=\"page-11-32\"></span>[10] William Jay Conover. 1999.\
    \ Practical nonparametric statistics. Vol. 350. john wiley & sons.\n- <span id=\"\
    page-11-9\"></span>[11] Roland Croft, Muhammad Ali Babar, and M. Mehdi Kholoosi.\
    \ 2023. Data Quality for Software Vulnerability Datasets. In 45th IEEE/ACM International\
    \ Conference on Software Engineering, ICSE 2023, Melbourne, Australia, May 14-20,\
    \ 2023. IEEE, 121–133.\n- <span id=\"page-11-2\"></span>[12] Jacob Devlin, Ming-Wei\
    \ Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep\
    \ Bidirectional Transformers for Language Understanding. In Proceedings of the\
    \ 2019 Conference of the North American Chapter of the Association for Computational\
    \ Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA,\
    \ June 2-7, 2019, Volume 1 (Long and Short Papers). Association for Computational\
    \ Linguistics, 4171–4186.\n- <span id=\"page-11-28\"></span>[13] Jiahao Fan, Yi\
    \ Li, Shaohua Wang, and Tien N. Nguyen. 2020. A C/C++ Code Vulnerability Dataset\
    \ with Code Changes and CVE Summaries. In MSR '20: 17th International Conference\
    \ on Mining Software Repositories, Seoul, Republic of Korea, 29-30 June, 2020.\
    \ ACM, 508–512.\n- <span id=\"page-11-3\"></span>[14] Zhangyin Feng, Daya Guo,\
    \ Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting\
    \ Liu, Daxin Jiang, and Ming Zhou. 2020. CodeBERT: A Pre-Trained Model for Programming\
    \ and Natural Languages. In Findings of the Association for Computational Linguistics:\
    \ EMNLP 2020 (Findings of ACL, Vol. EMNLP 2020). Association for Computational\
    \ Linguistics, 1536–1547.\n- <span id=\"page-11-37\"></span>[15] Patrick Fernandes,\
    \ Miltiadis Allamanis, and Marc Brockschmidt. 2019. Structured Neural Summarization.\
    \ In 7th International Conference on Learning Representations, ICLR 2019, New\
    \ Orleans, LA, USA, May 6-9, 2019. OpenReview.net.\n- <span id=\"page-11-26\"\
    ></span>[16] Shuzheng Gao, Cuiyun Gao, Yulan He, Jichuan Zeng, Lunyiu Nie, Xin\
    \ Xia, and Michael R. Lyu. 2023. Code Structure-Guided Transformer for Source\
    \ Code Summarization. ACM Trans. Softw. Eng. Methodol. 32, 1 (2023), 23:1–23:32.\n\
    - <span id=\"page-11-5\"></span>[17] Shuzheng Gao, Xin-Cheng Wen, Cuiyun Gao,\
    \ Wenxuan Wang, Hongyu Zhang, and Michael R. Lyu. 2023. What Makes Good In-Context\
    \ Demonstrations for Code Intelligence Tasks with LLMs?. In 38th IEEE/ACM International\
    \ Conference on Automated Software Engineering, ASE 2023, Luxembourg, September\
    \ 11-15, 2023. IEEE, 761–773.\n- <span id=\"page-11-12\"></span>[18] Shuzheng\
    \ Gao, Hongyu Zhang, Cuiyun Gao, and Chaozheng Wang. 2023. Keeping Pace with Ever-Increasing\
    \ Data: Towards Continual Learning of Code Intelligence Models. In 45th IEEE/ACM\
    \ International Conference on Software Engineering, ICSE 2023, Melbourne, Australia,\
    \ May 14-20, 2023. IEEE, 30–42.\n- <span id=\"page-11-23\"></span>[19] Golara\
    \ Garousi, Vahid Garousi-Yusifoglu, Günther Ruhe, Junji Zhi, Mahmood Moussavi,\
    \ and Brian Smith. 2015. Usage and usefulness of technical software documentation:\
    \ An industrial case study. Inf. Softw. Technol. 57 (2015), 664–682.\n- <span\
    \ id=\"page-11-6\"></span>[20] Daya Guo, Shuai Lu, Nan Duan, Yanlin Wang, Ming\
    \ Zhou, and Jian Yin. 2022. UniXcoder: Unified Cross-Modal Pre-training for Code\
    \ Representation. In Proceedings of the 60th Annual Meeting of the Association\
    \ for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland,\
    \ May 22-27, 2022. Association for Computational Linguistics, 7212–7225.\n- <span\
    \ id=\"page-11-4\"></span>[21] Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu\
    \ Tang, Shujie Liu, Long Zhou, Nan Duan, Alexey Svyatkovskiy, Shengyu Fu, Michele\
    \ Tufano, Shao Kun Deng, Colin B. Clement, Dawn Drain, Neel Sundaresan, Jian Yin,\
    \ Daxin Jiang, and Ming Zhou. 2021. GraphCodeBERT: Pre-training Code Representations\
    \ with Data Flow. In 9th International Conference on Learning Representations,\
    \ ICLR 2021. OpenReview.net.\n- <span id=\"page-11-13\"></span>[22] Bo Han, Quanming\
    \ Yao, Xingrui Yu, Gang Niu, Miao Xu, Weihua Hu, Ivor W. Tsang, and Masashi Sugiyama.\
    \ 2018. Co-teaching: Robust training of deep neural networks with extremely noisy\
    \ labels. In Advances in Neural Information Processing Systems 31: Annual Conference\
    \ on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018,\
    \ Montréal, Canada. 8536–8546.\n- <span id=\"page-11-7\"></span>[23] Xu Han, Zhengyan\
    \ Zhang, Ning Ding, Yuxian Gu, Xiao Liu, Yuqi Huo, Jiezhong Qiu, Yuan Yao, Ao\
    \ Zhang, Liang Zhang, Wentao Han, Minlie Huang, Qin Jin, Yanyan Lan, Yang Liu,\
    \ Zhiyuan Liu, Zhiwu Lu, Xipeng Qiu, Ruihua Song, Jie Tang, Ji-Rong Wen, Jinhui\
    \ Yuan, Wayne Xin Zhao, and Jun Zhu. 2021. Pre-trained models: Past, present and\
    \ future. AI Open 2 (2021), 225–250.\n- <span id=\"page-11-39\"></span>[24] Junxian\
    \ He, Jiatao Gu, Jiajun Shen, and Marc'Aurelio Ranzato. 2020. Revisiting Self-Training\
    \ for Neural Sequence Generation. In 8th International Conference on Learning\
    \ Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.\n\
    - <span id=\"page-11-15\"></span>[25] Jordan Henkel, Goutham Ramakrishnan, Zi\
    \ Wang, Aws Albarghouthi, Somesh Jha, and Thomas W. Reps. 2022. Semantic Robustness\
    \ of Models of Source Code. In IEEE International Conference on Software Analysis,\
    \ Evolution and Reengineering, SANER 2022, Honolulu, HI, USA, March 15-18, 2022.\
    \ IEEE, 526–537.\n- <span id=\"page-11-21\"></span>[26] HINT. 2023. Replication\
    \ package of HINT. [https://github.com/shuzhenggao/](https://github.com/shuzhenggao/HINT)\
    \ [HINT.](https://github.com/shuzhenggao/HINT)\n- <span id=\"page-11-24\"></span>[27]\
    \ Xing Hu, Ge Li, Xin Xia, David Lo, Shuai Lu, and Zhi Jin. 2018. Summarizing\
    \ Source Code with Transferred API Knowledge. In Proceedings of the Twenty-Seventh\
    \ International Joint Conference on Artificial Intelligence, IJCAI 2018, July\
    \ 13-19, 2018, Stockholm, Sweden. ijcai.org, 2269–2275.\n- <span id=\"page-11-1\"\
    ></span>[28] Xing Hu, Xin Xia, David Lo, Zhiyuan Wan, Qiuyuan Chen, and Thomas\
    \ Zimmermann. 2022. Practitioners' Expectations on Automated Code Comment Generation.\
    \ In 44th IEEE/ACM 44th International Conference on Software Engineering, ICSE\
    \ 2022, Pittsburgh, PA, USA, May 25-27, 2022. ACM, 1693–1705.\n- <span id=\"page-11-34\"\
    ></span>[29] Jie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven Zheng, Adams\
    \ Wei Yu, Xinying Song, and Denny Zhou. 2023. Large Language Models Cannot Self-Correct\
    \ Reasoning Yet. CoRR abs/2310.01798 (2023).\n- <span id=\"page-11-14\"></span>[30]\
    \ Jinchi Huang, Lie Qu, Rongfei Jia, and Binqiang Zhao. 2019. O2U-Net: A Simple\
    \ Noisy Label Detection Approach for Deep Neural Networks. In 2019 IEEE/CVF International\
    \ Conference on Computer Vision, ICCV 2019, Seoul, Korea (South), October 27 -\
    \ November 2, 2019. IEEE, 3325–3333.\n- <span id=\"page-11-8\"></span>[31] Hamel\
    \ Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc Brockschmidt.\
    \ 2019. CodeSearchNet Challenge: Evaluating the State of Semantic Code Search.\
    \ CoRR abs/1909.09436 (2019).\n- <span id=\"page-11-36\"></span>[32] Srinivasan\
    \ Iyer, Ioannis Konstas, Alvin Cheung, and Luke Zettlemoyer. 2016. Summarizing\
    \ Source Code using a Neural Attention Model. In Proceedings of the 54th Annual\
    \ Meeting of the Association for Computational Linguistics, ACL 2016. The Association\
    \ for Computer Linguistics.\n- <span id=\"page-11-20\"></span>[33] Paras Jain,\
    \ Ajay Jain, Tianjun Zhang, Pieter Abbeel, Joseph Gonzalez, and Ion Stoica. 2021.\
    \ Contrastive Code Representation Learning. In Proceedings of the 2021 Conference\
    \ on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event\
    \ / Punta Cana, Dominican Republic, 7-11 November, 2021. Association for Computational\
    \ Linguistics, 5954–5971.\n- <span id=\"page-11-10\"></span>[34] Wenxiang Jiao,\
    \ Xing Wang, Zhaopeng Tu, Shuming Shi, Michael R. Lyu, and Irwin King. 2021. Self-Training\
    \ Sampling with Monolingual Data Uncertainty for Neural Machine Translation. In\
    \ Proceedings of the 59th Annual Meeting of the Association for Computational\
    \ Linguistics and the 11th International Joint Conference on Natural Language\
    \ Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August\
    \ 1-6, 2021. Association for Computational Linguistics, 2840–2850.\n- <span id=\"\
    page-11-16\"></span>[35] Toshihiro Kamiya, Shinji Kusumoto, and Katsuro Inoue.\
    \ 2002. CCFinder: A Multilinguistic Token-Based Code Clone Detection System for\
    \ Large Scale Source Code. IEEE Trans. Software Eng. 28, 7 (2002), 654–670.\n\
    - <span id=\"page-11-25\"></span>[36] Pei Ke, Haozhe Ji, Zhenyu Yang, Yi Huang,\
    \ Junlan Feng, Xiaoyan Zhu, and Minlie Huang. 2022. Curriculum-Based Self-Training\
    \ Makes Better Few-Shot Learners for Data-to-Text Generation. In Proceedings of\
    \ the Thirty-First International Joint Conference on Artificial Intelligence,\
    \ IJCAI 2022, Vienna, Austria, 23-29 July 2022. ijcai.org, 4178–4184.\n- <span\
    \ id=\"page-11-17\"></span>[37] Miryung Kim, Vibha Sazawal, David Notkin, and\
    \ Gail C. Murphy. 2005. An empirical study of code clone genealogies. In Proceedings\
    \ of the 10th European Software Engineering Conference held jointly with 13th\
    \ ACM SIGSOFT International Symposium on Foundations of Software Engineering,\
    \ 2005, Lisbon, Portugal, September 5-9, 2005. ACM, 187–196.\n- <span id=\"page-11-19\"\
    ></span>[38] Solomon Kullback. 1997. Information theory and statistics. Courier\
    \ Corporation.\n- <span id=\"page-11-38\"></span>[39] Alexander LeClair, Sakib\
    \ Haque, Lingfei Wu, and Collin McMillan. 2020. Improved Code Summarization via\
    \ a Graph Neural Network. In ICPC '20: 28th International Conference on Program\
    \ Comprehension, Seoul, Republic of Korea, July 13-15, 2020. ACM, 184–195.\n-\
    \ <span id=\"page-11-11\"></span>[40] Dong-Hyun Lee et al. 2013. Pseudo-label:\
    \ The simple and efficient semisupervised learning method for deep neural networks.\
    \ In Workshop on challenges in representation learning, ICML, Vol. 3. 896.\n-\
    \ <span id=\"page-11-29\"></span>[41] Yi Li, Shaohua Wang, and Tien N. Nguyen.\
    \ 2021. Vulnerability detection with fine-grained interpretations. In ESEC/FSE\
    \ '21: 29th ACM Joint European Software Engineering Conference and Symposium on\
    \ the Foundations of Software Engineering, Athens, Greece, August 23-28, 2021.\
    \ ACM, 292–303.\n- <span id=\"page-11-33\"></span>[42] Zhiming Li, Xiaofei Xie,\
    \ Haoliang Li, Zhengzi Xu, Yi Li, and Yang Liu. 2022. Cross-lingual transfer learning\
    \ for statistical type inference. In ISSTA '22: 31st ACM SIGSOFT International\
    \ Symposium on Software Testing and Analysis, Virtual Event, South Korea, July\
    \ 18 - 22, 2022. ACM, 239–250.\n- <span id=\"page-11-27\"></span>[43] Chin-Yew\
    \ Lin. 2004. ROUGE: A Package for Automatic Evaluation of Summaries. In Text Summarization\
    \ Branches Out. Association for Computational Linguistics, Barcelona, Spain, 74–81.\n\
    - <span id=\"page-11-18\"></span>[44] Christopher D Manning. 2009. An introduction\
    \ to information retrieval. Cambridge university press.\n- <span id=\"page-12-29\"\
    ></span><span id=\"page-12-0\"></span>[45] Antonio Mastropaolo, Nathan Cooper,\
    \ David Nader Palacio, Simone Scalabrino, Denys Poshyvanyk, Rocco Oliveto, and\
    \ Gabriele Bavota. 2022. Using Transfer Learning for Code-Related Tasks. IEEE\
    \ Transactions on Software Engineering (2022).\n- <span id=\"page-12-21\"></span>[46]\
    \ Antonio Mastropaolo, Simone Scalabrino, Nathan Cooper, David Nader Palacio,\
    \ Denys Poshyvanyk, Rocco Oliveto, and Gabriele Bavota. 2021. Studying the usage\
    \ of text-to-text transfer transformer to support code-related tasks. In 2021\
    \ IEEE/ACM 43rd International Conference on Software Engineering (ICSE). IEEE,\
    \ 336–347.\n- <span id=\"page-12-14\"></span>[47] Fei Mi, Liangwei Chen, Mengjie\
    \ Zhao, Minlie Huang, and Boi Faltings. 2020. Continual Learning for Natural Language\
    \ Generation in Task-oriented Dialog Systems. In Findings of the Association for\
    \ Computational Linguistics: EMNLP 2020, Online Event, 16-20 November 2020 (Findings\
    \ of ACL, Vol. EMNLP 2020). Association for Computational Linguistics, 3461–3474.\n\
    - <span id=\"page-12-7\"></span>[48] Fei Mi, Wanhao Zhou, Lingjing Kong, Fengyu\
    \ Cai, Minlie Huang, and Boi Faltings. 2021. Self-training Improves Pre-training\
    \ for Few-shot Learning in Task-oriented Dialog Systems. In Proceedings of the\
    \ 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021,\
    \ Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021. Association\
    \ for Computational Linguistics, 1887– 1898.\n- <span id=\"page-12-16\"></span>[49]\
    \ Fangwen Mu, Xiao Chen, Lin Shi, Song Wang, and Qing Wang. 2022. Automatic Comment\
    \ Generation via Multi-Pass Deliberation. In 37th IEEE/ACM International Conference\
    \ on Automated Software Engineering, ASE 2022, Rochester, MI, USA, October 10-14,\
    \ 2022. ACM, 14:1–14:12.\n- <span id=\"page-12-24\"></span>[50] Noor Nashid, Mifta\
    \ Sintaha, and Ali Mesbah. 2023. Retrieval-Based Prompt Selection for Code-Related\
    \ Few-Shot Learning. (2023).\n- <span id=\"page-12-10\"></span>[51] Changan Niu,\
    \ Chuanyi Li, Vincent Ng, Dongxiao Chen, Jidong Ge, and Bin Luo. 2023. An Empirical\
    \ Comparison of Pre-Trained Models of Source Code. In 45th IEEE/ACM International\
    \ Conference on Software Engineering, ICSE 2023, Melbourne, Australia, May 14-20,\
    \ 2023. IEEE, 2136–2148.\n- <span id=\"page-12-26\"></span>[52] OpenAI. 2023.\
    \ GPT-4 Technical Report. CoRR abs/2303.08774 (2023).\n- <span id=\"page-12-18\"\
    ></span>[53] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002.\
    \ Bleu: a Method for Automatic Evaluation of Machine Translation. In Proceedings\
    \ of the 40th Annual Meeting of the Association for Computational Linguistics,\
    \ July 6-12, 2002, Philadelphia, PA, USA. ACL, 311–318.\n- <span id=\"page-12-1\"\
    ></span>[54] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et\
    \ al. 2018. Improving language understanding by generative pre-training. (2018).\n\
    - <span id=\"page-12-17\"></span>[55] Ensheng Shi, Yanlin Wang, Lun Du, Junjie\
    \ Chen, Shi Han, Hongyu Zhang, Dongmei Zhang, and Hongbin Sun. 2022. On the Evaluation\
    \ of Neural Code Summarization. In 44th IEEE/ACM 44th International Conference\
    \ on Software Engineering, ICSE 2022, Pittsburgh, PA, USA, May 25-27, 2022. ACM,\
    \ 1597–1608.\n- <span id=\"page-12-13\"></span>[56] Ensheng Shi, Yanlin Wang,\
    \ Wenchao Gu, Lun Du, Hongyu Zhang, Shi Han, Dongmei Zhang, and Hongbin Sun. 2023.\
    \ CoCoSoDa: Effective Contrastive Learning for Code Search. In 45th IEEE/ACM International\
    \ Conference on Software Engineering, ICSE 2023, Melbourne, Australia, May 14-20,\
    \ 2023. IEEE, 2198–2210.\n- <span id=\"page-12-4\"></span>[57] Lin Shi, Fangwen\
    \ Mu, Xiao Chen, Song Wang, Junjie Wang, Ye Yang, Ge Li, Xin Xia, and Qing Wang.\
    \ 2022. Are we building on the rock? on the importance of data preprocessing for\
    \ code summarization. In Proceedings of the 30th ACM Joint European Software Engineering\
    \ Conference and Symposium on the Foundations of Software Engineering, ESEC/FSE\
    \ 2022, Singapore, Singapore, November 14-18, 2022. ACM, 107–119.\n- <span id=\"\
    page-12-5\"></span>[58] Zhensu Sun, Li Li, Yan Liu, Xiaoning Du, and Li Li. 2022.\
    \ On the Importance of Building High-quality Training Datasets for Neural Code\
    \ Search. In 44th IEEE/ACM 44th International Conference on Software Engineering,\
    \ ICSE 2022, Pittsburgh, PA, USA, May 25-27, 2022. ACM, 1609–1620.\n- <span id=\"\
    page-12-27\"></span>[59] Ze Tang, Xiaoyu Shen, Chuanyi Li, Jidong Ge, Liguo Huang,\
    \ Zheling Zhu, and Bin Luo. 2022. AST-Trans: Code Summarization with Efficient\
    \ Tree-Structured Attention. In 44th IEEE/ACM 44th International Conference on\
    \ Software Engineering, ICSE 2022, Pittsburgh, PA, USA, May 25-27, 2022. IEEE,\
    \ 150–162.\n- <span id=\"page-12-6\"></span>[60] Amazon Mechanical Turk. 2023.\
    \ Amazon Mechanical Turk. [https://www.mturk.](https://www.mturk.com/) [com/.](https://www.mturk.com/)\n\
    - <span id=\"page-12-19\"></span>[61] Ramakrishna Vedantam, C. Lawrence Zitnick,\
    \ and Devi Parikh. 2015. CIDEr: Consensus-based image description evaluation.\
    \ In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015, Boston,\
    \ MA, USA, June 7-12, 2015. IEEE Computer Society, 4566–4575.\n- <span id=\"page-12-25\"\
    ></span>[62] Chaozheng Wang, Yuanhang Yang, Cuiyun Gao, Yun Peng, Hongyu Zhang,\
    \ and Michael R. Lyu. 2022. No more fine-tuning? an experimental evaluation of\
    \ prompt tuning in code intelligence. In Proceedings of the 30th ACM Joint European\
    \ Software Engineering Conference and Symposium on the Foundations of Software\
    \ Engineering, ESEC/FSE 2022, Singapore, Singapore, November 14-18, 2022. ACM,\
    \ 382–394.\n- <span id=\"page-12-11\"></span>[63] Deze Wang, Boxing Chen, Shanshan\
    \ Li, Wei Luo, Shaoliang Peng, Wei Dong, and Xiangke Liao. 2023. One Adapter for\
    \ All Programming Languages? Adapter Tuning for Code Search and Summarization.\
    \ In 45th IEEE/ACM International Conference on Software Engineering, ICSE 2023,\
    \ Melbourne, Australia, May 14-20, 2023. IEEE, 5–16.\n- <span id=\"page-12-8\"\
    ></span>[64] Yisen Wang, Xingjun Ma, Zaiyi Chen, Yuan Luo, Jinfeng Yi, and James\
    \ Bailey. 2019. Symmetric Cross Entropy for Robust Learning With Noisy Labels.\
    \ In 2019\n\nIEEE/CVF International Conference on Computer Vision, ICCV 2019,\
    \ Seoul, Korea (South), October 27 - November 2, 2019. IEEE, 322–330.\n\n- <span\
    \ id=\"page-12-12\"></span>[65] Yue Wang, Weishi Wang, Shafiq R. Joty, and Steven\
    \ C. H. Hoi. 2021. CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder\
    \ Models for Code Understanding and Generation. In Proceedings of the 2021 Conference\
    \ on Empirical Methods in Natural Language Processing, EMNLP 2021. Association\
    \ for Computational Linguistics, 8696–8708.\n- <span id=\"page-12-3\"></span>[66]\
    \ Yaqing Wang, Quanming Yao, James T. Kwok, and Lionel M. Ni. 2021. Generalizing\
    \ from a Few Examples: A Survey on Few-shot Learning. ACM Comput. Surv. 53, 3\
    \ (2021), 63:1–63:34.\n- <span id=\"page-12-31\"></span>[67] Zhongyuan Wang, Yixuan\
    \ Wang, Shaolei Wang, and Wanxiang Che. 2022. Adaptive Unsupervised Self-training\
    \ for Disfluency Detection. In Proceedings of the 29th International Conference\
    \ on Computational Linguistics, COLING 2022, Gyeongju, Republic of Korea, October\
    \ 12-17, 2022. International Committee on Computational Linguistics, 7209–7218.\n\
    - <span id=\"page-12-23\"></span>[68] Cody Watson, Michele Tufano, Kevin Moran,\
    \ Gabriele Bavota, and Denys Poshyvanyk. 2020. On learning meaningful assert statements\
    \ for unit test cases. In Proceedings of the ACM/IEEE 42nd International Conference\
    \ on Software Engineering. 1398–1409.\n- <span id=\"page-12-15\"></span>[69] Bolin\
    \ Wei, Yongmin Li, Ge Li, Xin Xia, and Zhi Jin. 2020. Retrieve and Refine: Exemplar-based\
    \ Neural Comment Generation. In 35th IEEE/ACM International Conference on Automated\
    \ Software Engineering, ASE 2020, Melbourne, Australia, September 21-25, 2020.\
    \ IEEE, 349–360.\n- <span id=\"page-12-28\"></span>[70] Hongqiu Wu, Hai Zhao,\
    \ and Min Zhang. 2021. Code Summarization with Structure-induced Transformer.\
    \ In Findings of the Association for Computational Linguistics: ACL/IJCNLP 2021,\
    \ Online Event, August 1-6, 2021 (Findings of ACL, Vol. ACL/IJCNLP 2021). Association\
    \ for Computational Linguistics, 1078–1090.\n- <span id=\"page-12-30\"></span>[71]\
    \ Qizhe Xie, Minh-Thang Luong, Eduard H. Hovy, and Quoc V. Le. 2020. Self-Training\
    \ With Noisy Student Improves ImageNet Classification. In 2020 IEEE/CVF Conference\
    \ on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June\
    \ 13-19, 2020. Computer Vision Foundation / IEEE, 10684–10695.\n- <span id=\"\
    page-12-2\"></span>[72] Pengcheng Yin and Graham Neubig. 2017. A Syntactic Neural\
    \ Model for General-Purpose Code Generation. In Proceedings of the 55th Annual\
    \ Meeting of the Association for Computational Linguistics, ACL 2017, Vancouver,\
    \ Canada, July 30 - August 4, Volume 1: Long Papers. Association for Computational\
    \ Linguistics, 440–450.\n- <span id=\"page-12-22\"></span>[73] Hao Yu, Yiling\
    \ Lou, Ke Sun, Dezhi Ran, Tao Xie, Dan Hao, Ying Li, Ge Li, and Qianxiang Wang.\
    \ 2022. Automated Assertion Generation via Information Retrieval and Its Integration\
    \ with Deep learning. In 44th IEEE/ACM 44th International Conference on Software\
    \ Engineering, ICSE 2022, Pittsburgh, PA, USA, May 25-27, 2022. ACM, 163–174.\n\
    - <span id=\"page-12-9\"></span>[74] Zhilu Zhang and Mert R. Sabuncu. 2018. Generalized\
    \ Cross Entropy Loss for Training Deep Neural Networks with Noisy Labels. In Advances\
    \ in Neural Information Processing Systems 31: Annual Conference on Neural Information\
    \ Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montréal, Canada.\
    \ 8792–8802.\n- <span id=\"page-12-20\"></span>[75] Yaqin Zhou, Shangqing Liu,\
    \ Jing Kai Siow, Xiaoning Du, and Yang Liu. 2019. Devign: Effective Vulnerability\
    \ Identification by Learning Comprehensive Program Semantics via Graph Neural\
    \ Networks. In Advances in Neural Information Processing Systems 32: Annual Conference\
    \ on Neural Information Processing Systems 2019, NeurIPS 2019. 10197–10207."
  decisions:
    language: '- Qualified. Reason: English Paper'
    evaluation_prompt: Qualified.
    related_work_prompt: Qualified
    novelty_prompt: Qualified
    review_only_prompt: Qualified
  llm_input_used: '## Abstract

    Pre-trained code models have recently achieved substantial improvements in

    many code intelligence tasks. These models are first pre-trained on large-scale

    unlabeled datasets in a task-agnostic manner using self-supervised learning,

    and then fine-tuned on labeled datasets in downstream tasks. However, the

    labeled datasets are usually limited in size (i.e., human intensive efforts),

    which may hinder the performance of pre-trained code models in specific tasks.

    To mitigate this, one possible solution is to leverage the large-scale

    unlabeled data in the tuning stage by pseudo-labeling. However, directly

    employing the pseudo-labeled data can bring a large amount of noise, i.e.,

    incorrect labels, leading to suboptimal performance. How to effectively

    leverage the noisy pseudo-labeled data is a challenging yet under-explored

    problem.In this paper, we propose a novel approach named HINT to improve

    pre-trained code models with large-scale unlabeled datasets by better utilizing

    the pseudo-labeled data. HINT includes two main modules: HybrId pseudo-labeled

    data selection and Noise-tolerant Training. In the hybrid pseudo-data selection

    module, considering the robustness issue, apart from directly measuring the

    quality of pseudo labels through training loss, we further propose to employ a

    retrieval-based method to filter low-quality pseudo-labeled data. The

    noise-tolerant training module aims to further mitigate the influence of errors

    in pseudo labels by training the model with a noise-tolerant loss function and

    by regularizing the consistency of model predictions.The experimental results

    show that HINT can better leverage those unlabeled data in a task-specific way

    and provide complementary benefits for pre-trained models, e.g., improving the

    best baseline model by 15.33%, 16.50%, and 8.98% on code summarization, defect

    detection, and assertion generation, respectively.


    ## Introduction

    Recently, code intelligence has become a popular research field in software engineering.
    It aims at improving developers'' productivity by providing real-time coding assistance
    and suggestions for them [\[9,](#page-11-0) [28\]](#page-11-1). The advent of
    deep learning techniques, especially pretraining techniques [\[12,](#page-11-2)
    [54\]](#page-12-1), has significantly advanced progress in this area. Different
    from previous supervised learning methods that train the model from scratch [\[1,](#page-10-0)
    [72\]](#page-12-2), these pre-trained code models are first pre-trained on large-scale
    unlabeled datasets using selfsupervised learning tasks and then fine-tuned on
    labeled datasets in downstream tasks. For example, Masked Language Modeling (MLM)
    is one of the most popular self-supervised pre-training tasks and is used in many
    pre-trained code models such as CodeBERT [\[14\]](#page-11-3) and GraphCodeBERT
    [\[21\]](#page-11-4). It works by training the models to predict the masked tokens
    based on the context of surrounding words. Since this process does not require
    human annotation, it can be applied on large-scale unlabeled datasets, enabling
    the models to acquire a vast amount of general programming knowledge. Equipped
    with this ability, these pre-trained code models achieve state-of-the-art performance
    on a variety of code intelligence tasks, such as code summarization and defect
    detection [\[14,](#page-11-3) [17,](#page-11-5) [20,](#page-11-6) [21\]](#page-11-4).


    Despite the promising results, deep learning models are known to be data-hungry
    and the size of labeled datasets in downstream tasks is important for the performance
    of pre-trained models [\[23,](#page-11-7) [66\]](#page-12-3). However, the sizes
    of labeled datasets in downstream tasks are


    <sup>∗</sup>Corresponding author. The author is also affiliated with Peng Cheng
    Laboratory and Guangdong Provincial Key Laboratory of Novel Security Intelligence
    Technologies.


    Permission to make digital or hard copies of all or part of this work for personal
    or classroom use is granted without fee provided that copies are not made or distributed
    for profit or commercial advantage and that copies bear this notice and the full
    citation on the first page. Copyrights for components of this work owned by others
    than ACM must be honored. Abstracting with credit is permitted. To copy otherwise,
    or republish, to post on servers or to redistribute to lists, requires prior specific
    permission and/or a fee. Request permissions from permissions@acm.org.


    usually limited due to two main reasons. On one hand, the datasets crawled from
    open-source websites like Github or Stackoverflow are small in size and of low
    quality. For example, as mentioned in the literature [\[31\]](#page-11-8), only
    6.8% JavaScript code snippets from popular GitHub repositories contain corresponding
    comments, making only a few of them usable for tasks like code summarization.
    Furthermore, recent studies have revealed that the quality of existing crawled
    datasets is also quite poor [\[11,](#page-11-9) [57,](#page-12-4) [58\]](#page-12-5).
    For example, as indicated in a recent work [\[57\]](#page-12-4), over 40% of data
    in the widely-used code summarization datasets contain various types of noise.
    On the other hand, due to the requirement of domain expert knowledge, the annotation
    cost of code intelligence tasks is higher than other tasks in natural language
    processing or computer vision, such as sentiment analysis and image classification
    [\[60\]](#page-12-6). With insufficient annotated data in downstream tasks, the
    performance of pre-trained code models is limited.


    One possible solution to this problem is to leverage the largescale unlabeled
    data in the tuning stage by pseudo-labeling. Pseudolabeling first trains a base
    model on the limited labeled dataset, which subsequently serves as a teacher model
    to annotate the unlabeled dataset [\[34,](#page-11-10) [40,](#page-11-11) [48\]](#page-12-7).
    The pseudo-labeled dataset is then merged with the original labeled dataset to
    help improve the training of a new student model. By replacing the teacher model
    with the stronger student model, the above process can be iterated multiple times,
    aiming at improving the models themselves. This technique leverages the unlabeled
    data in a task-specific way and has shown promising results in tasks such as image
    classification [\[40\]](#page-11-11) and dialog systems [\[48\]](#page-12-7).
    Although pseudo-labeling can enrich the labeled dataset, directly employing the
    pseudo-labeled data can bring a large amount of noise [\[48\]](#page-12-7). For
    example, as shown in Figure [1](#page-1-0) (a), the pseudo-labeled summary of
    the top code snippet is not a meaningful sentence and contains redundant tokens.
    Training with such noisy pseudo labels may amplify the incorrect knowledge in
    the teacher model and ultimately degrades the model''s performance. However, identifying
    and removing noisy pseudo labels is non-trivial due to the complex semantic of
    source code. Besides, it is difficult and impractical to ensure that the filtered
    dataset is noise-free [\[64,](#page-12-8) [74\]](#page-12-9). Therefore, how to
    effectively leverage the noisy pseudo-labeled data and enable the model to be
    noise-tolerant for code intelligence tasks is of vital importance, yet under-explored.


    In this paper, we propose HINT with two main components, i.e., the HybrId pseudo-labeled
    data selection module and the Noisetolerant Training module. First, in the hybrid
    pseudo-labeled data selection module, we propose to combine the training loss
    of the teacher model and a retrieval-based method for removing the lowquality
    data. Specifically, we filter out pseudo-labeled samples that present high training
    loss or low label similarity with the retrieved similar training sample. To further
    mitigate the influence of data noise on model performance, we propose a noise-tolerant
    training objective that includes a noise-tolerant symmetric loss function and
    a consistency regularization of model predictions. To evaluate the performance
    of HINT, we conduct experiments on three popular code intelligence tasks including
    code summarization, defect detection, and assertion generation. Following previous
    work [\[18,](#page-11-12) [51,](#page-12-10) [63\]](#page-12-11), we build our
    method on top of three popular open-source pretrained models: CodeBERT [\[14\]](#page-11-3),
    CodeT5 [\[65\]](#page-12-12), and UniXcoder [\[20\]](#page-11-6).


    <span id="page-1-0"></span>![](_page_1_Figure_3.jpeg)


    (a) A Python example of low-quality pseudo-labeled data (top).


    ![](_page_1_Figure_5.jpeg)


    (b) A Java example of high-quality pseudo-labeled data (top).


    Figure 1: Examples in the code summarization task for illustrating the motivation
    of the hybrid pseudo-labeled data selection method, which indicates the loss-based
    data selection strategy alone may incorrectly measure the quality of pseudo labels.


    Extensive experiments demonstrate that HINT can consistently improve the performance
    of pre-trained code models on these code intelligence tasks. For example, HINT
    improves UniXcoder by 15.33%, 16.50%, and 8.98% in terms of BLEU-4, F1, and EM
    on code summarization, defect detection, and assertion generation, respectively,
    indicating that our proposed HINT method can provide complementary benefits for
    the pre-trained code models.


    In summary, the main contributions of this work are as follows:


    - (1) To the best of our knowledge, we are the first to leverage the large-scale
    unlabeled data in a task-specific way in the turning phase for code intelligence
    tasks.

    - (2) We propose HINT, a novel framework to leverage large-scale unlabeled data
    for effectively tuning pre-trained code models. It first selects high-quality
    pseudo-labeled data in a hybrid way and then improves the model''s tolerance to
    noisy data in the training process.

    - (3) Extensive experiments on three tasks demonstrate that our method can be
    built on top of a range of existing strong pretrained models and consistently
    improve their performance on many downstream tasks.


    <span id="page-2-0"></span>![](_page_2_Figure_0.jpeg)


    Figure 2: The overview of HINT.'
  token_usage: 9806
  time_usage: 2.640411138534546
- title: Experimenting a New Programming Practice with LLMs
  abstract: 'The recent development on large language models makes automatically

    constructing small programs possible. It thus has the potential to free

    software engineers from low-level coding and allow us to focus on the perhaps

    more interesting parts of software development, such as requirement engineering

    and system testing. In this project, we develop a prototype named AISD

    (AI-aided Software Development), which is capable of taking high-level

    (potentially vague) user requirements as inputs, generates detailed use cases,

    prototype system designs, and subsequently system implementation. Different

    from existing attempts, AISD is designed to keep the user in the loop, i.e., by

    repeatedly taking user feedback on use cases, high-level system designs, and

    prototype implementations through system testing. AISD has been evaluated with

    a novel benchmark of non-trivial software projects. The experimental results

    suggest that it might be possible to imagine a future where software

    engineering is reduced to requirement engineering and system testing only.'
  url: http://arxiv.org/abs/2401.01062v1
  keywords: ''
  document: '# Experimenting a New Programming Practice with LLMs


    Simiao Zhang<sup>∗</sup> smzhang@stu.ecnu.edu.cn East China Normal University
    Shanghai, China


    Jun Sun junsun@smu.edu.sg Singapore Management University Singapore, Singapore


    Jiaping Wang<sup>∗</sup> 51265902031@stu.ecnu.edu.cn East China Normal University
    Shanghai, China


    Yueling Zhang ylzhang@sei.ecnu.edu.cn East China Normal University Shanghai, China


    Guoliang Dong gldong@smu.edu.sg Singapore Management University Singapore, Singapore


    Geguang Pu ggpu@sei.ecnu.edu.cn East China Normal University Shanghai, China


    # ABSTRACT


    The recent development on large language models makes automatically constructing
    small programs possible. It thus has the potential to free software engineers
    from low-level coding and allow us to focus on the perhaps more interesting parts
    of software development, such as requirement engineering and system testing. In
    this project, we develop a prototype named AISD (AI-aided Software Development),
    which is capable of taking high-level (potentially vague) user requirements as
    inputs, generates detailed use cases, prototype system designs, and subsequently
    system implementation. Different from existing attempts, AISD is designed to keep
    the user in the loop, i.e., by repeatedly taking user feedback on use cases, high-level
    system designs, and prototype implementations through system testing. AISD has
    been evaluated with a novel benchmark of non-trivial software projects. The experimental
    results suggest that it might be possible to imagine a future where software engineering
    is reduced to requirement engineering and system testing only.


    ## KEYWORDS


    Requirement engineering, system testing, large language model, code generation


    ## 1 INTRODUCTION


    Large language models (LLMs), i.e., transformer-based language models with a huge
    number of parameters, have shown remarkable performance in natural language understanding
    as well as solving complex problems thanks to their emergent abilities [\[38\]](#page-10-0).
    In particular, their abilities of instruction following, step-by-step reasoning,
    and in-context learning have led to many applications in a variety of domains,
    including code generation [\[37\]](#page-10-1). That is, given a description of
    a low-level simple coding task, LLMs such as GPT are capable of synthesizing programs
    automatically, often correctly too [\[5,](#page-10-2) [23,](#page-10-3) [29\]](#page-10-4).
    Thus, it gives us a glint of hope that one day LLMs might free us from manually
    low-level coding.


    In fact, a few recent projects attempted the ambitious goal of replacing programmers
    with LLMs. Li et al. [\[24\]](#page-10-5) propose an end-toend software development
    framework known as ChatDev, which mimics the classical waterfall model and breaks
    down the software development process into four stages, i.e., designing, coding,
    testing, and documenting. That is, given a high-level rather vague requirement,
    ChatDev leverages multiple LLM-based virtual roles


    to generate detailed requirements, design and implementation in


    that sequence through rounds of conversions. MetaGPT [\[13\]](#page-10-6) adopts
    a similar idea and further standardizes each LLM-based agent''s output to guide
    the other agents in the subsequent tasks. For instance, during the design phase,
    MetaGPT generates Product Requirements Documents (PRDs) with a standardized structure
    to coordinate the subsequent development process. While these attempts are shown
    to improve the underlying LLM''s performance to certain extent, they often fail
    when the software project is non-trivial [\[31\]](#page-10-7).


    This is hardly surprising as, even for experienced human programmers, it is infeasible
    to complete a complex software based on vague high-level requirements. This is
    precisely why requirement engineering and system testing play vital roles in the
    software development process, and software development processes such as rapid
    prototyping and agile methods value user feedback during system development highly.
    Requirement engineering and system testing are essential for eliciting the expectations
    of endusers and stakeholders, ensuring that the delivered software aligns with
    their expectations. Requirement engineering is a complex and multifaceted process,
    and no human beings can produce flawless requirement specifications in a single
    attempt. However, the abovementioned approaches conduct requirement analysis only
    lightly and the identified requirements are directly passed on to the coding phase,
    depriving users of the opportunity to validate and modify the automatically generated
    requirements and implementation.


    At the same time, it is perhaps fair to say that requirement engineering in the
    traditional programming paradigm failed to achieve its promises to some extent
    as well as it may often be disconnected from system implementation as the project
    development progresses, e.g., the requirement and corresponding system design
    documents often are not properly maintained along with the system implementation.
    One fundamental reason is that there is limited ways of obtaining timely feedback
    from system designers and programmers (since a software project often lasts months
    or even years). With the help of LLMs, we can potentially shorten the development
    time significantly, allow users to test/validate prototype implementations "instantaneously",
    collect user feedback (e.g., in the form of failed test cases or updated requirements)
    timely and refine the implementation accordingly. In other words, we can make
    requirement engineering more relevant and effective by timely testing the implementation.


    In this work, we experiment with a novel AI-powered software development framework
    called AISD. AISD distinguishes itself from existing approaches in two aspects.
    Firstly, AISD is designed to


    <sup>\*</sup>These authors contributed equally to this work


    Corresponding author


    engage users throughout the software development process, especially during the
    requirement analysis, high-level system design and system validation phase. Secondly,
    AISD adheres to the philosophy that less is more when engaging the human developers.
    Specifically, when presented with a vague requirement, AISD generates a requirement
    document (e.g., use cases) capturing only the core functions required by the system
    and one system design document describing which source files should be built,
    and seeks user-feedback. With the user-feedback, these documents are updated accordingly.
    Note that due to the limited attention span of the LLMs [\[32\]](#page-10-8) (and
    humans too), both documents are designed to be simple but friendly for humans
    and LLMs, which we will show in Section [3.](#page-2-0) Subsequently, AISD decomposes
    the system design into low-level coding tasks and completes them systematically
    and automatically. Once a prototype is implemented (i.e., the resultant system
    passes the unit testing and basic system testing), users are engaged to validate
    the system to check whether their requirements are met. If any failures are identified,
    the implementation, the design and/or the requirements are updated accordingly
    to construct another prototype. This iterative process continues until the users
    accept the product.


    AISD has been implemented as a self-contained toolkit. Considering that existing
    benchmarks, e.g., HumanEval [\[5\]](#page-10-2), MBPP [\[3\]](#page-10-9) and
    CAMEL [\[18\]](#page-10-10) are not suitable for evaluating the capability of
    the LLM-based software development frameworks (all of them either are limited
    to simple function-level implementation tasks or lack detailed requirements specifications
    for system-level implementation tasks), we have developed a novel benchmark named
    CAASD (Capability Assessment of Automatic Software Development). Each task of
    CAASD is equipped with a list of reference use cases depicting the system requirements.
    The reference use cases are used to evaluate the quality and completeness of a
    system implementation. We have compared AISD with two state-of-the-art baselines
    Chat-Dev [\[24\]](#page-10-5) and MetaGPT [\[13\]](#page-10-6) on CAASD. The experimental
    results demonstrate that AISD achieves the highest pass rate while using the fewest
    tokens. On average, AISD achieves an impressive pass rate of 75.2%. Relative to
    these two baselines, there are improvements of 133.5% and 501.6% respectively.
    Moreover, it reduces the number of tokens consumed from at least 28734 to 21993.
    These experimental results provide compelling evidence for the importance of user
    engagement in AI-aided software development.


    In summary, we make the following contributions.


    - We introduce AISD, an AI-aided software development framework that is designed
    to keep users engaged through requirement engineering and system testing.

    - We build a novel benchmark named CAASD for objectively assessing the capabilities
    of various AI-aided software development systems. To the best of our knowledge,
    this is the first benchmark that offers criteria for assessing how well a software
    development task is completed.

    - We conduct a comprehensive evaluation of AISD using the benchmark. The results
    underscore the critical role of human engagement during AI-powered software development.
    These results suggest a potential future where software engineering may be streamlined
    to focus primarily on requirement engineering.


    The remainders of the paper are organized as follows. Section [2](#page-1-0) reviews
    some essential backgrounds. Section [3](#page-2-0) presents the detailed design
    of AISD. Section [4](#page-6-0) presents details on our experiments of applying
    AISD as well as two baselines to the CAASD benchmark. Section [5](#page-9-0) reviews
    related work and Section [6](#page-10-11) concludes.


    ## <span id="page-1-0"></span>2 PRELIMINARIES


    In this section, we review relevant backgrounds on LLMs and prompt engineering.


    ## 2.1 Large Language Models


    LLMs to pre-trained language models [\[26\]](#page-10-12) that have a heightened
    number of parameters, often in the range of tens or even thousands of billions
    [\[37\]](#page-10-1). LLMs, exemplified by models like GPT-3 [\[9\]](#page-10-13)
    and PaLM [\[6\]](#page-10-14), outperform smaller-scale counterparts like BERT
    [\[15\]](#page-10-15) and GPT-2 [\[27\]](#page-10-16) by not only achieving substantial
    performance improvements but also demonstrating emergent abilities. These emergent
    abilities, including in-context learning [\[7\]](#page-10-17), instruction following
    [\[36\]](#page-10-18), and step-by-step reasoning [\[16\]](#page-10-19), endow
    LLMs with the capability to tackle complex tasks.


    A prominent application showcasing the prowess of LLMs is ChatGPT, a significant
    chatbot that can complete various tasks from emulating human-like responses to
    aiding in debugging and writing programs. Such capabilities allow LLMs to potentially
    revolutionize many domains, with potential applications in software development
    and beyond. In this work, we utilize LLMs to experiment a new programming practice.


    ## 2.2 Prompt Engineering


    Prompt engineering is the practice of effectively exploiting and harnessing abilities
    of LLMs by optimizing prompts in a manner that LLMs can comprehend and interpret
    [\[22,](#page-10-20) [25\]](#page-10-21). A prompt is typically natural language
    text instructing how LLMs perform a task and specifying the desired output format.
    Prompt engineering is often based on the emergent abilities. That is, users can
    adopt certain prompt templates to have LLMs tackle complex and interesting tasks.
    In the following, we briefly introduce two prompting engineering techniques used
    in this work.


    Few-shot prompting. Few-shot prompting serves as a method to facilitate in-context
    learning by incorporating demonstrations within the prompt [\[4\]](#page-10-22).
    These demonstrations guide the model towards improved performance, acting as a
    form of conditioning for generating responses in subsequent examples. Figure [1a](#page-2-1)
    showcases an example of few-shot prompting. This example is from the work of Brown
    et al. [\[4\]](#page-10-22), the goal of which is to create a sentence using the
    given word. We can observe that the model completes this task successfully based
    on a single example. In this work, we use few-shot prompting to steer LLMs to
    outputs with certain format.


    Chain-of-thought. Chain-of-thought (CoT) prompting [\[35\]](#page-10-23) is mainly
    based on the "step-by-step reasoning" ability (one of the emergent abilities)
    of LLMs. It is often combined with few-shot prompting to perform intricate tasks
    that require reasoning. In the prompt, users often need to demonstrate how they
    approach a similar task step by step. Figure [1b](#page-2-1) is an example adapted
    from work [\[16\]](#page-10-19). In this example, the prompt exemplifies the calculation
    process, and the output indeed correctly follows it. Recently, Kojima


    <span id="page-2-1"></span>A "whatpu" is a small, furry animal native to Tanzania.
    An example of a sentence that uses the word whatpu is: We were traveling in Africa
    and we saw these very cute whatpus. To do a "farduddle" means to jump up and down
    really


    fast. An example of a sentence that uses the word farduddle is:


    (Output) When we won the game, we all started to farduddle in celebration.


    Q: Roger has 5 balls. He buys 2 more cans of balls. Each can has 3 balls. How
    many balls does he have now? A: Roger started with 5 balls. 2 cans of 3 tennis
    balls each is 6 tennis balls. 5 + 6 = 11. The answer is 11 Q: I had 23 apples.
    If I used 20 to make lunch and bought 6 more, how many apples do I have?


    (Output) You started with 23 apples. You used 20 to make lunch. So you had 23
    - 20 = 3. They bought 6 more apples, so you have 3 + 6 = 9. The answer is 9.


    #### (a) Few-shot prompting


    #### (b) Few-shot CoT prompting


    <span id="page-2-2"></span>![](_page_2_Figure_9.jpeg)


    ### Figure 1: Examples of different prompting techniques


    Figure 2: Overview of AISD


    et al. proposed zero-shot CoT prompting. Instead of exemplifying how to approach
    a task through examples, zero-shot CoT prompting simply adds "Let''s think step
    by step" after the question. In this work, we adopt the zero-shot CoT prompting
    in our prompts to improve outputs of LLMs.


    ## 2.3 LLM-based Autonomous Agent


    An LLM-based autonomous agent is a system that employs an LLM as its core controller
    to automatically plan tasks, make decisions, adopt actions, and reflect and update
    results. Technically, an LLMbased autonomous agent consists of four modules [\[34\]](#page-10-24):
    profiling module, memory module, planning module and action module.


    The profiling module specifies the role of the agent by writing the related profile
    in the prompt, potentially inducing LLMs to produce response with the role''s
    expertise. Different roles indicate different responsibilities for agents. For
    example, in the software development scenario, we may designate an agent as a
    programmer, focusing primarily on coding. The memory module is used to store past
    observations, decisions and actions, facilitating future actions. The planning
    module simulates the process that humans follow to handle a complex task. That
    is, this module focuses on breaking down a complex task to a series of simple
    and manageable subtasks. The action module is responsible for executing the agent''s
    decisions (i.e., the decomposed subtasks). Note that the action module


    directly interacts with the environment, i.e., using external tools. LLM agents
    provide a flexible way to accomplish intricate tasks. Note that LLM agents have
    varying levels of autonomy, and some LLM agents may only have some of the four
    modules. Depending on the capabilities granted during the design phase, agents
    can exhibit self-directed behaviors ranging from purely reactive to highly proactive.
    In our work, AISD is mainly built with multiple communicative agents [\[18\]](#page-10-10),
    which complete their work through conversation with each other and human developers.


    ## <span id="page-2-0"></span>3 OUR APPROACH


    In this section, we present the design of AISD and its interaction with users.
    Figure [2](#page-2-2) provides an overview of the overall workflow, where the
    tasks highlighted in green are the ones that rely on human-interaction. Starting
    with an initial idea, AISD firstly refines the task and generates a set of use
    cases to capture the core functions required by the desired system. Instead of
    immediately proceeding to the follow-up procedure, AISD interacts with the user
    to review and modify the generated use cases to ensure they properly convey the
    user''s requirements. Once the user agrees to proceed, AISD produces a simple
    but LLM-and-human friendly system design. Note that humans can also revise the
    high-level system design as they do for use cases, but in practice, the revision
    of system design requires some expertise, which may be not


    - <span id="page-3-0"></span>1. User can input the characteristics of iris flowers.

    - 2. User can submit the input data to the neural network classifier

    - 3. User can obtain the classification results.

    - 4. User can view the classification results in JSON format.

    - 1. User can input the characteristics of iris flowers.

    - 2. User can submit the input data to the neural network classifier

    - 3. User can obtain the classification results.

    - 4. User can view the classification results on a board.

    - 1. User can input the characteristics of iris flowers. The input includes four
    characteristics: "Sepal-LengthCm", "SepalWidthCm", "PetalLengthCm", and "PetalWidthCm".

    - 2. User can submit the input data to the neural network classifier

    - 3. User can obtain the classification result.

    - 4. User can view the classification name of the iris flower on the board. The
    result should be the species name.


    (a) Use cases directly generated by ChatGPT


    (b) The revised use cases based on human review


    (c) The revised use cases based on manual testing


    ### Figure 3: Use cases evolution of the task "A tool for classifying iris flowers"


    suitable for ordinary users. We thus highlight this phase with light green indicating
    that human engagement in this phase is optional. After that, AISD implements the
    system automatically according to the design. Following implementation, AISD iteratively
    tests and refines the system. There are two types of testing. One is the automation
    testing and the other is the manual testing. When errors occur during automation
    testing, the error messages are used as feedback to guide the bug fixing automatically.
    Dashed lines in Figure [2](#page-2-2) indicate that an iterative process is involved,
    and the red dashed lines indicate that humans are involved in the respective iteration.
    That is, AISD iteratively performs automation testing and bug fixing. To prevent
    AISD from getting stuck in this iteration (e.g., a bug that cannot be fixed by
    LLMs), AISD allows the user to set a maximum number of iteration times. After
    automation testing, unlike existing approaches that often terminate after code
    generation [\[18\]](#page-10-10) or automation testing [\[13,](#page-10-6) [24\]](#page-10-5),
    in AISD, the user is asked to test the resultant system manually. If a test failure
    occurs, different actions are taken according to the nature of the failure. Specifically,
    if there are any error messages reported during testing, these error messages
    are provided as input to the bug fixing module. Otherwise, the user may add or
    refine the use cases and instruct AISD to repeat the subsequent procedures (e.g.,
    in the case of that some system requirement was missing or is revised (as often
    is the case in practice)). In the following, we introduce the details of each
    step.


    ## 3.1 Use Cases Generation


    Use cases play a crucial role in the software development. They are helpful in
    identifying, clarifying, and understanding the functional requirements of a system
    from a user''s perspective. When implementing a desired system with LLMs, use
    cases can be integrated into specific prompts to articulate the concrete functions
    that should be implemented. Once a system is delivered, use cases can also serve
    as a means to validate whether the system meets the stack-holders'' requirements.


    In the traditional software development process, use cases are typically derived
    from a general idea by requirements engineers. While some works, such as MetaGPT,
    show that LLMs can generate well-documented use cases in certain scenarios, we
    argue that


    involving stakeholders in the derivation of use cases is crucial due to the vagueness
    and incompleteness of requirements in the initial stage, particularly when tackling
    complex tasks. That is, automatically generated use cases may fail to clearly
    and correctly convey the functional requirements of the system. However, on the
    other hand, directly writing out the detailed use cases manually from scratch
    is usually challenging and burdensome for users. Therefore, to alleviate the burden
    the user bears and avoid fully relying on outputs of LLMs, AISD initially generates
    a "draft" of use cases using LLMs with the prompt shown in Figure [4,](#page-4-0)
    and then asks the user to review (and revises if necessary) the generated use
    cases. Note that each prompt used in AISD consists of two parts: the system message
    and the user message. The system message is used for role assignment and thus
    induces LLMs to produce response with the role''s expertise. The user message
    is used for task specification. Once the use cases are deemed acceptable, the
    user then instructs AISD to proceed with these use cases.


    For example, a user intends to develop a tool for identifying various species
    of iris. The user inputs the task "develop a neural network classifier tool that
    allows users to input the characteristics of iris flowers and obtain classification
    results" to AISD. AISD then generates an initial version of use cases as shown
    in Figure [3a.](#page-3-0) Although most of the use cases are acceptable, the
    last use case fails to capture the correct requirement as the user would like
    to view the result on a graphic interface, instead of in a JSON file. Consequently,
    the user revises the use case to align it with the desired requirement, as shown
    in Figure [3b](#page-3-0) where the revision is highlighted with red color. This
    use case review and refinement process may repeat multiple times if necessary
    until the user is happy with the resultant use cases. After that, the user instructs
    AISD to proceed.


    ## <span id="page-3-1"></span>3.2 System Designing


    System design serves as a blueprint for implementing a system. The standard system
    design process is complex, involving the design of architecture, components, modules,
    interfaces, and data to meet specified requirements [\[8\]](#page-10-25). Typically,
    this process generates numerous documents at different levels of abstraction.
    While these


    #### <span id="page-4-0"></span>System Message:


    You are a Product Manager. You have extensive experience in designing products
    and translating complex technical requirements into clear, user-centric scenarios.


    #### User Message:


    According to the user''s task listed below: Task: "{task}". You should write down
    the use cases required by this task. Output in JSON format. The format is:


    { "1": "User can view the GUI." }


    #### <span id="page-4-1"></span>Figure 4: Prompts used in use cases generation
    phase


    #### System Message:


    You are a software architect. According to the user''s task and use cases, you
    will write the system design. List only key code files (no more than 6 files),
    ALWAYS start with the "main" file. Don''t list multi-level files. Output in JSON
    format. The format is: {"main.py": "This is the main file of ...", } User Message:
    Task: {task} Use Cases: {use\_cases}


    #### Figure 5: Prompts used for system designing


    documents are beneficial for developers to understand and implement the desired
    system, our experience suggests that they are unsuitable for instructing LLMs
    to code accordingly due to two challenges. Firstly, LLMs struggle when processing
    comprehensive system design documents that exceed their token limit. Secondly,
    the intricate and overwhelming information in these documents often distracts
    LLMs from focusing on the core task—implementing functional requirements. To address
    these challenges, we propose a simplified system design approach that produces
    documents more suitable for LLM-based software development. Specifically, we generate
    only one document based on use cases. This document outlines the source files
    the desired system should have and the functions each file should implement.


    Concretely, in the designing phase, AISD receives the initial task and the use
    cases accepted by the user. It utilizes the prompts shown in Figure [5](#page-4-1)
    to instruct LLMs in generating a list of source file names along with their corresponding
    descriptions. The system message in this prompt is designed by incorporating the
    ideas of role-playing [\[17\]](#page-10-26) and few-shot learning [\[4\]](#page-10-22)
    which we believe can enhance the quality of the outputs. Specifically, we have
    LLMs simulate the role of a software architect, tasking them with generating responses
    that are not only specific but also aligned with the given context. To facilitate
    the follow-up procedures, the outputs are expected to be in JSON format. To this
    end, we provide an output example in the prompt to demonstrate the expected outputs
    following the idea of few-shot learning.


    <span id="page-4-2"></span>"main.py": "This is the main file of the neural network
    classifier tool.",


    "classifier.py": "This file contains the implementation of the machine learning
    classification algorithm.", "gui.py": "This file provides the graphical user interface


    for users to enter iris characteristics and view classification results.",


    "utils.py": "This file contains utility functions used in the system."


    #### Figure 6: Example of the system design in AISD


    Figure [6](#page-4-2) presents an example of the system design based on the revised
    use cases shown in Figure [3b.](#page-3-0) The system design is articulated through
    four Python source files, each accompanied by a concise explanation detailing
    its functions. We remark that this form of system design is simple yet LLMs-friendly,
    enabling LLMs to focus more on tractable sub-tasks by reducing irrelevant information.


    We remark that the system design involves a certain level of expertise, which
    poses challenges for ordinary users to review and modify it. Specifically, users
    often need to possess a deep understanding of programming languages and have some
    development experience to assess the reasonability of each module in the presented
    system design, as well as to ensure that all modules can form a complete system.
    Nevertheless, we enable users to determine if the generated system design is acceptable
    and opt to revise it, particularly when interacting with professional users.


    ## 3.3 Coding and Automatic Testing


    The code generation in AISD is an automatic and iterative process. With the system
    design generated previously, AISD prompts LLMs to produce all the required source
    files at once, tests the resultant system, and refines the source code according
    to the testing outcome.


    Prompts used to generate source code are shown in Figure [7.](#page-5-0) In the
    system prompt, we set the LLM to be a skilled programmer with experience in various
    programming languages. In the user prompt, we provide the initial task along with
    the outputs of each previous phase, i.e., use cases and system design. We then
    explain what LLMs should complete. Note that involving the task description and
    the corresponding use cases in the prompt is necessary, as these two types of
    information help LLMs retain the user''s requirements, guiding them to code accordingly.


    Instead of generating each code file separately (i.e., one code file in one chat
    session), the LLM is asked to complete all the coding tasks in one chat session.
    Typically, different source files collaborate to form the complete system, indicating
    that these files are interdependent. In practice, when generating code for interconnected
    modules in separate chat sessions, the system might encounter the challenge of
    "robbing Peter to pay Paul" as generating one file without considering the entire
    system can inadvertently result in a failure. This is because the generated code
    files may not


    #### <span id="page-5-0"></span>System Message:


    You are a Programmer. You have extensive computing and coding experience in many
    programming languages and platforms, such as Python.


    #### User Message:


    The user''s task, use cases and original system designs are listed below:


    Task: "{task}".


    Use Cases: "{use\_cases}".


    System Design: "{system\_design}".


    You have to complete the task through an executable software with multiple files
    implemented via Python.


    To satisfy the new user''s demands, you should write files and make sure that
    every detail of the architecture is, in the end, implemented as code. The software
    should be equipped with graphical user interface (GUI) so that user can visually
    and graphically use it; so you must choose a GUI framework (e.g., in Python, you
    can implement GUI via tkinter, Pygame, Flexx, PyGUI, etc,). Think step by step
    and reason yourself to the right decisions to make sure we get it right. You will
    output the content of each file including complete code. Each file must strictly
    follow a markdown code block format, where the following tokens must be replaced
    such that "FILENAME" is the lowercase file name including the file extension,
    "LANGUAGE" in the programming language, "DOCSTRING" is a string literal specified
    in source code that is used to document a specific segment of code, and "CODE"
    is


    the original code: FILENAME LANGUAGE ''''''DOCSTRING'''''' CODE You will start
    with the "main" file, then go to the ones that are imported by that file, and
    so on. Please note that the code should be fully functional. Ensure to implement
    all functions. No placeholders (such as ''pass'' in Python).


    ### Figure 7: Prompts used for code generation


    coordinate with each other. Thus, in AISD, we instruct LLMs to generate all code
    files in one chat session, ensuring that the resultant files can work closely
    together, thereby reducing potential bugs in the system. Note that due to the
    hallucination of LLMs, there may exist some unimplemented functions (e.g., the
    function body just has a ''pass'' keyword in Python) and missing import packages
    in the resultant implementation. To remedy this, once the source code is generated,
    AISD employs LLMs to refine the source code, identifying and addressing these
    potential issues.


    Limited by the capability of LLMs, the resultant implementation may still contain
    bugs even after automatic refinement with LLMs. Therefore, we introduce an automatic
    testing phase before the system is presented for human testing. During the automatic
    testing, AISD sequentially performs two different types of testing: unit testing,
    and system testing. For the unit testing, AISD generates a set of unit tests for
    each function, automatically runs them and records the results. Failure tests
    and the corresponding code are provided to LLMs for code refinement. Once all
    unit tests pass, AISD proceeds to the system testing, checking if the system can
    correctly start execution as a complete system. If errors occur during startup,
    AISD instructs LLMs to refine the code according to the error messages. Note that
    each testing is iterative, and the whole automation testing is implemented as
    an agent capable of automatically performing two kinds of testing and deciding
    when each testing stops. Additionally, we allow users to set a maximum number
    of iterations for each testing to prevent endless refinement.


    We take an example to showcase how AISD automatically refines the generated system
    through basic system testing. This example is observed when performing the development
    task "Airplane War Game". Specifically, when AISD executes "python main.py" for
    the system testing, an error occurs. The error message is shown in


    Figure [8a](#page-6-1) which suggests that a function is used wrongly. Subsequently,
    AISD employs the prompt shown in Figure [8b](#page-6-1) to instruct the LLM to
    fix the error accordingly. Note that the two placeholders "code" and "message"
    in that prompt are automatically replaced with the source code of the entire system
    implementation and the error message obtained at runtime.


    ## 3.4 System Validation


    Passing the unit testing and being executable do not necessarily mean that the
    generated system is acceptable from the perspective of stakeholders. The main
    reason is that the automatic testing, as depicted earlier, is unable to verify
    if the functional requirements are satisfied, especially when the initial set
    of use cases may be still incomplete or wrong according to the actual user expectation,
    or the system is built with a graphic user interface (for which there will be
    typically a lot of details that may not be what the user wants). For example,
    the use case "User can view the classification results on a board" in Figure [3b](#page-3-0)
    directly requires the user''s feedback. In this case, it is challenging to assess
    whether this requirement is satisfied without human engagement.


    To ensure that the generated system aligns with the user''s requirements, it is
    essential to involve the user in validating the system. Specifically, when the
    generated system passes the automatic testing, the user is required to manually
    test the system according to the use cases generated previously. If the requirement
    depicted by a use case is not satisfied, the user is then required to revise the
    description of the use case or instruct AISD to revise the source code. Concretely,
    if the user fails to validate a use case but no errors occur when validating it,
    the user then revises the description of the use case to make it more detailed
    and instructs AISD to start from the system designing phase again. In case of
    errors reported, the user then provides the error message and instructs AISD to
    fix


    <span id="page-6-1"></span>Experimenting a New Programming Practice with LLMs
    Conference''17, July 2017, Washington, DC, USA


    Traceback (most recent call last): File "main.py", line 3, in <module> game.start\_game()
    File "game.py", line 21, in start\_game player.handle\_input(player\_airplane,
    bullets) TypeError: handle\_input() missing 1 required positional argument: ''canvas''


    #### (a) Error message when running ''python main.py''


    ### System Message:


    Please review the source code. Identify and fix the issues listed below. Think
    step by step. First, analyze the reason why errors are increasing. Second, write
    the entire code files, ensuring that the format matches that of the Source Code.
    Each file must strictly follow a markdown code block format, where the following
    tokens must be replaced such that "FILENAME" is the lowercase file name including
    the file extension, "LAN-GUAGE" in the programming language, "DOCSTRING" is a
    string literal specified in source code that is used to document a specific segment
    of code, and "CODE" is the original code: FILENAME LANGUAGE ''''''DOC-STRING''''''
    CODE You will start with the "main" file, then go to the ones that are imported
    by that file, and so on. Please note that the code should be fully functional.
    Ensure to implement all functions. No placeholders (such as "pass" in Python).


    User Message: Source Code: {code} Problem: {message}


    #### (b) Prompts used for fixing runtime bugs


    #### Figure 8: Example of automatic testing


    the bug accordingly. Lastly, the user can also choose to introduce new use cases
    (i.e., new requirements), which is often the case in practice. The user can repeat
    this process until all the use cases pass the testing.


    We present an example showcasing how the user collaborates with AISD in this phase.
    Figure [9a](#page-6-2) shows the resultant system based on the use cases presented
    in Figure [3b.](#page-3-0) However, this system does not meet the user''s expectations.
    Firstly, there is only one input box with no prompts on how to enter the iris''s
    characteristics. Secondly, the classification result is presented as a number,
    which is hard for users to understand. The second issue is interesting which exposes
    that humans and LLMs may have different understandings about the same expression,
    e.g., "Use can view the classification result". While users indeed can "view"
    the classification result, they generally expect the tool to directly present
    what species the input iris is, instead of a cryptic number (though it may correspond
    to a specific iris species). Therefore, we revise the use cases to add some details.
    The revised version is shown in Figure [3c.](#page-3-0) Subsequently, we instruct
    AISD to proceed with the updated use cases. After a series of steps as described
    previously, the resultant system aligns perfectly with our requirements, as shown
    in Figure [9b.](#page-6-2)


    <span id="page-6-2"></span>![](_page_6_Figure_11.jpeg)


    (b) The resultant system after manual testing


    Figure 9: Comparison of the resultant system before and after manual testing


    # <span id="page-6-0"></span>4 EXPERIMENTS


    We have implemented AISD as a self-contained toolkit based on LLMs, comprising
    approximately 2058 lines of source code written in Python 3.7. Our implementation
    as well as the benchmark used in this work are available at [\[1\]](#page-10-27).
    In the following, we conduct multiple experiments to address the following research
    questions:


    - RQ1: How effective is AISD in developing software?

    - RQ2: Does the human engagement matter?

    - RQ3: How much user involvement is needed for AISD to work effectively?


    RQ1 aims to evaluate the effectiveness of AISD in completing development tasks.
    RQ2 further conducts an ablation study to verify if human engagement does contribute
    to the effectiveness of AISD. Finally, RQ3 analyzes the number of user interactions
    required by AISD when performing a software development task.


    # 4.1 Experimental setup


    Assessment benchmark. We have manually constructed a benchmark named CAASD (Capability
    Assessment of Automatic Software Development) to assess the capabilities of various
    AIaided software development systems, which we consider to be another contribution
    of this work.


    Note that existing benchmarks are not suitable for evaluating the capability of
    the AI-aided software development system. This is because benchmarks like HumanEval
    [\[5\]](#page-10-2), MBPP [\[3\]](#page-10-9) and CAMEL [\[18\]](#page-10-10)
    either consist of simple function-level coding tasks or lack relatively objective
    criteria to assess the level of task completion. The former essentially reflects
    the capability of the LLM itself (since LLMs are effective for solving such tasks),
    while the latter hinders an objective assessment of task


    completion. To address the problem of lacking a benchmark for evaluating LLM-based
    automatic software development systems, we propose the CAASD benchmark which contains
    72 software development tasks collected from multiple sources and from multiple
    domains such as small games, personal websites, and various other applications.
    On average, the implementation of each task in CAASD requires approximately 240
    lines of code, based on the analysis of open-source implementations of similar
    tasks [\[10](#page-10-28)[–12\]](#page-10-29). Each test case in CASSD consists
    of four fields, i.e., a task ID, a task name, a task prompt and a list of reference
    use cases. Particularly, the task prompt describes the task to complete (i.e.,
    a high-level often vague system requirement) and is provided as input to various
    AI-aided software development systems. The reference use cases of each task depict
    the essential functional requirements of a system. We remark that these reference
    use cases serve as a means to objectively assess the task completion, details
    of which will be elaborated in Section [4.2.](#page-7-0)


    Baselines. We compare AISD with two state-of-the-art baselines, namely ChatDev
    [\[24\]](#page-10-5) and MetaGPT [\[13\]](#page-10-6). ChatDev divides the development
    process into four phases (i.e., designing, coding, testing, and documenting),
    and assigns two virtual roles to solve the corresponding subtask at each phase
    by multi-turn discussions. The two roles stop discussing only when they reach
    a consensus that the subtask has been successfully accomplished. MetaGPT also
    decomposes a general development task into several subtasks, but differs from
    ChatDev in two aspects. First, when solving each subtask, MetaGPT adopts only
    one virtual role (e.g., a product manager) to address the subtask. Second, MetaGPT
    incorporates the idea of human Standardized Operating Procedures (SOPs) and mandates
    each role to produce standardized outputs, facilitating knowledge sharing across
    different modules.


    We conducted all experiments based on ChatGPT [1](#page-7-1) with version "gpt3.5-turbo-16k".
    For ChatDev, we utilize the settings outlined in [\[24\]](#page-10-5), whereas
    for the MetaGPT, we adopt the default settings of the open-sourced implementation[2](#page-7-2)
    as the paper [\[13\]](#page-10-6) contains insufficient details of the experimental
    settings. Following the settings in ChatDev, we allowed up to 5 turns for both
    automation testing and manual testing in AISD, and selected the best as the final
    outcome among the systems generated from all turns.


    ## <span id="page-7-0"></span>4.2 Research Questions


    RQ1: How effective is AISD in developing software? To answer this question, we
    applied AISD and the two baselines to solve the tasks of the CAASD benchmark systematically,
    and then reported the pass rate of the tasks and costs for each approach. The
    "costs" here refers to the overall count of tokens consumed by LLMs to accomplish
    a task, encompassing both input tokens and output tokens. The pass rate for each
    task is calculated using the following formula:


    Pass rate = #Passed Use Cases #Total Use Cases


    where #Total Use Cases is the total number of the reference use cases of a task
    in CAASD, and #Passed Use Cases denotes the number of reference use cases which
    pass the manual testing. Specifically,


    <span id="page-7-2"></span>


    <span id="page-7-3"></span>![](_page_7_Figure_11.jpeg)


    Figure 10: The performance of three AI-aided software development systems on CAASD
    benchmark


    we manually inspect reference use cases of the corresponding task one by one by
    running the generated system. The value of #Passed Use Cases is then obtained
    by counting the number of successfully passed use cases.


    2https://github.com/geekan/MetaGPT(v0.3.0) **AISD ChatDev MetaGPT 0.0** The results
    are summarized in Figure [10.](#page-7-3) We observe that our approach, i.e.,
    AISD achieved an impressive 75.2% pass rate while maintaining an average token
    consumption of just 21993 tokens per task. In contrast, ChatDev has a pass rate
    of 32.2% and consumes an average of 28734 tokens per task, and MetaGPT performs
    even worse with a pass rate of only 12.5% and an average token cost of 37136 per
    task. It is worth noting that both MetaGPT and ChatDev have a cost that is over
    1.3 times higher than that of AISD. The significant improvement of AISD over ChatDev
    may be attributed to the human engagement in its design (which we will systematically
    analyze in RQ2). MetaGPT is dramatically less effective compared to AISD and ChatDev,
    although it is not surprising. The reason is that MetaGPT feeds too much complex
    information to LLMs, leading to difficulties for LLMs to understand the tasks.
    For example, before the coding phase, MetaGPT generates a standard PRD (Product
    Requirements Document), including "Product Goals", "User Stories", "Competitive
    Analysis", "Competitive Quadrant Chart" (in the form of mermaid[3](#page-7-4)
    code), "Requirement Analysis", "Requirement Pool". In the coding phase, the PRD
    with this rich information is directly used as a part of prompt to instruct LLMs,
    which may result in cognitive overload for LLMs. The results from MetaGPT further
    reinforce the point that we highlighted in Section [3.2](#page-3-1) (i.e., intricate
    and overwhelming information potentially distracts LLMs), ultimately undermining
    their ability to comprehend and complete core tasks. Additionally, among the three
    frameworks, AISD consumed the fewest tokens, while MetaGPT consumed the most,
    indicating that AISD is not only effective but also efficient.


    Answer to RQ1: AISD significantly improves the use cases pass rate with lower
    costs compared to existing baselines.


    ### RQ2: Does the human engagement matter?


    <span id="page-7-1"></span><sup>1</sup>https://openai.com/blog/introducing-chatgpt-and-whisper-apis


    <span id="page-7-4"></span><sup>3</sup>https://github.com/mermaid-js/mermaid


    <span id="page-8-0"></span>![](_page_8_Figure_1.jpeg)


    Figure 11: Pass rate of AISD with and without human engagement over CAASD benchmark.


    To address this question, we applied AISD to the CAASD benchmark again but omitted
    the human engagement in all phases. That is, we skipped the manual revision in
    the use cases generation and the entire manual testing phase. Note that we conducted
    5 trials for each task, and selected the best pass rate among the 5 trials as
    the pass rate of the task. We compared the results obtained without human engagement
    to those with human engagement.


    Figure [11](#page-8-0) illustrates the pass rate of AISD with and without human
    engagement over CAASD benchmark. The x-axis represents the ID of each task, ranging
    from 1 to 72. For each task, the pass rate with human engagement is denoted by
    a red point, and the pass rate without human engagement is denoted by a blue point.
    The green point indicates that there is an overlap of a blue point and a red point.
    The green horizontal line and the red horizontal line represent the average pass
    rate of each group, respectively. Observing the data, the average pass rate of
    AISD with human engagement (i.e., about 75.2%) is approximately 51.1 percentage
    higher than that of the AISD without human engagement (i.e., about 24.1%). It
    is worth noting that a concentration of 47.2% orange points is evident at the
    bottom (i.e., 0% pass rate), contrasting with only 5.6% of blue points at the
    same level. These findings suggest that human engagement not only enhances the
    overall use case pass rate of the generated system implementation, but also underscores
    the significance of human involvement in handling tasks that prove challenging
    when relying solely on the power of LLMs. In the next research question, we will
    conduct a more in-depth analysis of the influence of human involvement on the
    use case pass rate.


    Answer to RQ2: Engaging users during requirement analysis and system testing effectively
    bridges the gap between the generated system and users'' expectation, especially
    in handling challenging tasks.


    RQ3: How many times of user involvement are needed in AISD? In AISD, users mainly
    engage in two distinct phases. The first involves use case generation, and the
    second entails manual testing. In the former phase, users are required to review
    the generated use cases and revise them if necessary until they are deemed to


    <span id="page-8-1"></span>Table 1: The instances of revisions achieving the final
    pass rate using AISD.


    | ℎ1                               | ℎ2 | #Task | Avg. Pass Rate (%) | Total Revisions
    |

    |----------------------------------|----|-------|--------------------|-----------------|

    | 0                                | 0  | 1     | 100%               | 0               |

    | 0                                | 1  | 1     | 100%               | 1               |

    | 0                                | 2  | 0     | -                  | -               |

    | 0                                | 3  | 0     | -                  | -               |

    | 0                                | 4  | 0     | -                  | -               |

    | 0                                | 5  | 0     | -                  | -               |

    | 1                                | 0  | 14    | 68.57%             | 1               |

    | 1                                | 1  | 8     | 81.44%             | 2               |

    | 1                                | 2  | 12    | 85.42%             | 3               |

    | 1                                | 3  | 11    | 86.68%             | 4               |

    | 1                                | 4  | 9     | 86.89%             | 5               |

    | 1                                | 5  | 16    | 52.59%             | 6               |

    | Average times of manual revision |    |       |                    | 3.5             |


    <span id="page-8-2"></span>![](_page_8_Figure_10.jpeg)


    Figure 12: The pass rate with different numbers of human interactions (ℎ2)


    be acceptable. In the latter phase, users participate in an iterative process
    wherein they assess the use cases by executing the resultant system, and they
    may subsequently revise the use cases or prompt LLMs to address bugs. Consequently,
    users are involved in the entire development process at least twice, even if they
    choose not to revise the use cases or perform bug fixing based on error messages.
    To show the required manual efforts using AISD more specifically, we counted the
    times of manual revision in both use cases generation and the manual testing phases
    for each development task. In this context, "manual revision" refers to the manual
    adjustment of use cases or the prompt for fixing bugs.


    Table [1](#page-8-1) shows the manual revisions involved in completing development
    tasks with AISD. In this table, ℎ<sup>1</sup> denotes the times that a human revises
    use cases during the use case generation phase, ℎ<sup>2</sup> denotes the times
    that a human revises use cases or prompts LLMs to fix bugs during the manual testing
    phase, ''#Task'' is the number of tasks each of which undergoes ℎ<sup>1</sup>
    revisions during the use case generation phase and ℎ<sup>2</sup> revisions during
    the manual testing phase, ''Avg. Pass Rate'' indicates the average pass rate achieved
    by tasks of each group, and ''Total Revisions'' denotes the total times of manual
    revisions of each task, which is the sum of ℎ<sup>1</sup> and ℎ2. Note that ℎ<sup>1</sup>
    is either 0, indicating no modifications, or 1, indicating there


    are manual revisions during the use case generation phase. Note that all modifications
    are treated as one revision since there is no iteration during this phase. The
    value of ℎ<sup>2</sup> ranges from 0 to 5, as we allow a maximum of 5 iterations
    during the manual testing phase in this work.


    We can observe that AISD requires approximately four revisions on average. For
    over half of the tasks (47 out of 72), AISD achieves the best pass rate within
    four rounds of manual revision. Note that only one task out of the total 72 tasks
    attains a 100% pass rate without any manual revisions, and 70 tasks (where ℎ<sup>1</sup>
    equals 1) demonstrate the need for user involvement in revising the generated
    use cases (i.e., the generated use cases look implausible and thus need manual
    modifications). These findings underscore a substantial disparity between use
    cases generated by LLMs and user expectations, emphasizing the essential role
    of human engagement in successfully completing development tasks.


    We further analyze how the pass rate varies with the increasing number of human
    revisions. Figure [12](#page-8-2) displays changes in key statistical measures
    of pass rate as ℎ<sup>2</sup> increments through a boxand-whisker plot with observations.
    The numbers in red are the counts of minimum and maximum points. Note that the
    pass rate in this figure is obtained based on the resultant system after ℎ2-th
    revision. In general, with the rise in ℎ2, the median pass rate and the percentiles
    (i.e., 25th and 75th) gradually rise, and tasks attaining a 100% pass rate (depicted
    by blue points at the top) become more concentrated. The median pass rate even
    reaches 100% when ℎ<sup>2</sup> > 3. These findings, to some extent, rule out
    the possibility that the pass rate improvement is solely a result of the uncertainty
    of LLMs. We also observe that when increasing ℎ2, there are still some tasks for
    which the pass rate remains 0%. This is not surprising. First, the quality of
    the revision depends on both the user''s proficiency and the task''s complexity.
    When handling particularly complex tasks, it becomes challenging for users to
    discern the required functions of the system, hindering their ability to revise
    use cases effectively. Additionally, some use cases surpass the capabilities of
    LLMs. For instance, in the development task "Voice Assistant", users expected
    that the desired system can understand what they said, and respond correctly.
    The system delivered by AISD however can only record the voice of users, even
    after rounds of prompting.


    Answer to RQ3: In general, increasing user interactions leads to a higher pass
    rate of use cases. However, for some complex tasks, additional interactions may
    yield limited results due to LLMs'' constraints. Our practical guideline is thus
    to continue interacting with AISD until users struggle to improve the generated
    use cases, and no runtime error occurs.


    ## <span id="page-9-0"></span>5 RELATED WORK


    This work is closely related to existing approaches on automatic code generation.
    Automatic code generation is a hot topic in natural language processing community.
    Before the emergence of large neural network models, the works on this topic can
    be categorized into two groups. One primarily focuses on traditional techniques
    from both rule-based and statistical natural language processing, while the other
    is related to neural networks.


    In the early stages, researchers primarily employ heuristic rules or expert systems
    to synthesize program. Jha et al. [\[14\]](#page-10-30) propose an approach to
    automatically synthesize loop-free programs based on a combination of oracle-guided
    learning from examples and constraint-based synthesis from components using satisfiability
    modulo theories (SMT) solvers. Allamanis and Sutton [\[2\]](#page-10-31) propose
    to extract code idioms from a corpus of idiomatic software projects by nonparametric
    Bayesian probabilistic tree substitution grammars. Raychev et al. [\[28\]](#page-10-32)
    synthesize code completions by identifying the highest-ranked sentences with a
    statistical language model. However, these approaches have limitations in scalability
    and are confined to simple scenarios. For example, they struggle to generate a
    complete program from scratch according to a natural language description.


    Subsequently, with the rise of deep learning, neural networks including convolutional
    neural networks (CNNs) [\[19\]](#page-10-33) and various recurrent neural networks
    (RNNs) [\[21\]](#page-10-34) are widely adopted to approach the natural-language-to-code
    task. Ling et al. [\[20\]](#page-10-35) treat the code generation task as a sequence-to-sequence
    problem, and then propose an LSTM-based neural network architecture to generate
    code from natural language. Sun et al. [\[30\]](#page-10-36) design a grammar-based
    structural CNN to generate a program by predicting the grammar rules of the programming
    language. Although these works achieve significant improvement in flexibility
    and scalability, they still exhibit poor generalization ability due to limited
    language understanding.


    After the introduction of the Transformer [\[33\]](#page-10-37), numerous large
    language models (LLMs) have been proposed and have demonstrated impressive results
    in automatic code generation. For example, Codex [\[5\]](#page-10-2) can solve
    72.31% of challenging Python programming problems created by humans. We remark
    that while LLMs exhibit remarkable proficiency in programming, instructing them
    directly to complete complex software development tasks is challenging. In response,
    researchers have put forth a series of LLM-based multiagent frameworks to automatically
    generate software with an initial idea.


    Li et al. [\[18\]](#page-10-10) propose role-playing, a communicative agent framework,
    to complete a task (not limited to software development tasks) by having three
    agents communicate with each other. Specifically, when the human user specifies
    a task to implement, a task specifier agent specifies a role to play for the assistant
    agent and user agent respectively, and then the two roles engage in conversation
    to complete the task. ChatDev [\[24\]](#page-10-5) divides the development process
    into four stages: designing, coding, testing, and documenting. Each stage involves
    a group of agents to solve a distinct subtask, such as deciding the software modality
    and programming language at the designing phase. Note that the subtask of each
    stage is further decomposed into atomic subtasks, each of which is addressed by
    two agents engaging in conversation. Inspired by the human workflows, MetaGPT
    incorporates Standardized Operating Procedures (SOPs) into their framework to
    coordinate agents. MetaGPT also mirrors the waterfall model and breaks down the
    development process into several phases. The main difference is that each agent
    in MetaGPT solves a subtask independently and produces standardized action outputs
    for knowledge sharing. We remark that these existing frameworks exclude humans
    from the development


    Experimenting a New Programming Practice with LLMs Conference''17, July 2017,
    Washington, DC, USA


    process, and relying solely on the abilities of LLMs may undermine their effectiveness
    when facing complex tasks.


    ## <span id="page-10-11"></span>6 CONCLUSION


    In this work, we present an AI-powered software development framework. Different
    from existing approaches, our framework is designed to keep users lightly engaged
    when solving a complex task, emphasizing the importance of human-engaged requirement
    analysis and system validation. To objectively assess the capabilities of completing
    software development tasks, we have built a novel benchmark, each task of which
    is equipped with a list of use cases for reference during assessment. The evaluation
    results demonstrate that our framework significantly improves the task pass rate
    while consuming fewer tokens.


    ## REFERENCES


    - <span id="page-10-27"></span>[1] AISD. 2023. [https://drive.google.com/drive/folders/](
    https://drive.google.com/drive/folders/1i0UWqy1K4WwaCLnb7yhyQfV8UqjdXSkl?usp=sharing)
    [1i0UWqy1K4WwaCLnb7yhyQfV8UqjdXSkl?usp=sharing.]( https://drive.google.com/drive/folders/1i0UWqy1K4WwaCLnb7yhyQfV8UqjdXSkl?usp=sharing)
    Accessed Dec 15, 2023.

    - <span id="page-10-31"></span>[2] Miltiadis Allamanis and Charles Sutton. 2014.
    Mining idioms from source code. In Proceedings of the 22nd acm sigsoft international
    symposium on foundations of software engineering. 472–483.

    - <span id="page-10-9"></span>[3] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten
    Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry,
    Quoc Le, et al. 2021. Program synthesis with large language models. arXiv preprint
    arXiv:2108.07732 (2021).

    - <span id="page-10-22"></span>[4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie
    Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam,
    Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners.
    Advances in neural information processing systems 33 (2020), 1877–1901.

    - <span id="page-10-2"></span>[5] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming
    Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda,
    Nicholas Joseph, Greg Brockman, et al. 2021. Evaluating large language models
    trained on code. arXiv preprint arXiv:2107.03374 (2021).

    - <span id="page-10-14"></span>[6] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
    Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles
    Sutton, Sebastian Gehrmann, et al. 2023. Palm: Scaling language modeling with
    pathways. Journal of Machine Learning Research 24, 240 (2023), 1–113.

    - <span id="page-10-17"></span>[7] Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng,
    Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. 2022. A survey
    for in-context learning. arXiv preprint arXiv:2301.00234 (2022).

    - <span id="page-10-25"></span>[8] A. Ferrari and A. Sangiovanni-Vincentelli.
    1999. System design: traditional concepts and new paradigms. In Proceedings 1999
    IEEE International Conference on Computer Design: VLSI in Computers and Processors
    (Cat. No.99CB37040). 2–12. <https://doi.org/10.1109/ICCD.1999.808256>

    - <span id="page-10-13"></span>[9] Luciano Floridi and Massimo Chiriatti. 2020.
    GPT-3: Its nature, scope, limits, and consequences. Minds and Machines 30 (2020),
    681–694.

    - <span id="page-10-28"></span>[10] Geeksforgeeks. 2023. [https://www.geeksforgeeks.org/track-objects-with](https://www.geeksforgeeks.org/track-objects-with-camshift-using-opencv/?ref=lbp
    )[camshift-using-opencv/?ref=lbp.](https://www.geeksforgeeks.org/track-objects-with-camshift-using-opencv/?ref=lbp
    ) Accessed Dec 15, 2023.

    - [11] Github. 2022. [https://https://github.com/CharlesPikachu/Games/tree/master.](https://https://github.com/CharlesPikachu/Games/tree/master
    ) Accessed Dec 15, 2023.

    - <span id="page-10-29"></span>[12] Github. 2023. [https://github.com/OpenBMB/ChatDev/blob/main/Contribution.](https://github.com/OpenBMB/ChatDev/blob/main/Contribution.md
    ) [md.](https://github.com/OpenBMB/ChatDev/blob/main/Contribution.md ) Accessed
    Dec 15, 2023.

    - <span id="page-10-6"></span>[13] Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng
    Cheng, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu
    Ran, et al. 2023. Metagpt: Meta programming for multi-agent collaborative framework.
    arXiv preprint arXiv:2308.00352 (2023).

    - <span id="page-10-30"></span>[14] Susmit Jha, Sumit Gulwani, Sanjit A Seshia,
    and Ashish Tiwari. 2010. Oracleguided component-based program synthesis. In Proceedings
    of the 32nd ACM/IEEE International Conference on Software Engineering-Volume 1.
    215–224.

    - <span id="page-10-15"></span>[15] Jacob Devlin Ming-Wei Chang Kenton and Lee
    Kristina Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers
    for language understanding. In Proceedings of naacL-HLT, Vol. 1. 2.

    - <span id="page-10-19"></span>[16] Takeshi Kojima, Shixiang Shane Gu, Machel
    Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot
    reasoners. Advances in neural information processing systems 35 (2022), 22199–22213.

    - <span id="page-10-26"></span>[17] Aobo Kong, Shiwan Zhao, Hao Chen, Qicheng
    Li, Yong Qin, Ruiqi Sun, and Xin Zhou. 2023. Better zero-shot reasoning with role-play
    prompting. arXiv preprint arXiv:2308.07702 (2023).

    - <span id="page-10-10"></span>[18] Guohao Li, Hasan Abed Al Kader Hammoud, Hani
    Itani, Dmitrii Khizbullin, and Bernard Ghanem. 2023. Camel: Communicative agents
    for" mind" exploration of


    large scale language model society. arXiv preprint arXiv:2303.17760 (2023).


    - <span id="page-10-33"></span>[19] Zewen Li, Fan Liu, Wenjie Yang, Shouheng Peng,
    and Jun Zhou. 2021. A survey of convolutional neural networks: analysis, applications,
    and prospects. IEEE transactions on neural networks and learning systems (2021).

    - <span id="page-10-35"></span>[20] Wang Ling, Phil Blunsom, Edward Grefenstette,
    Karl Moritz Hermann, Tomáš Kočiský, Fumin Wang, and Andrew Senior. 2016. Latent
    Predictor Networks for Code Generation. In Proceedings of the 54th Annual Meeting
    of the Association for Computational Linguistics (Volume 1: Long Papers), Katrin
    Erk and Noah A. Smith (Eds.). Association for Computational Linguistics, Berlin,
    Germany, 599–609. <https://doi.org/10.18653/v1/P16-1057>

    - <span id="page-10-34"></span>[21] Zachary C Lipton, John Berkowitz, and Charles
    Elkan. 2015. A critical review of recurrent neural networks for sequence learning.
    arXiv preprint arXiv:1506.00019 (2015).

    - <span id="page-10-20"></span>[22] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao
    Jiang, Hiroaki Hayashi, and Graham Neubig. 2023. Pre-train, prompt, and predict:
    A systematic survey of prompting methods in natural language processing. Comput.
    Surveys 55, 9 (2023), 1–35.

    - <span id="page-10-3"></span>[23] Ansong Ni, Pengcheng Yin, Yilun Zhao, Martin
    Riddell, Troy Feng, Rui Shen, Stephen Yin, Ye Liu, Semih Yavuz, Caiming Xiong,
    et al. 2023. L2CEval: Evaluating Language-to-Code Generation Capabilities of Large
    Language Models. arXiv preprint arXiv:2309.17446 (2023).

    - <span id="page-10-5"></span>[24] Chen Qian, Xin Cong, Cheng Yang, Weize Chen,
    Yusheng Su, Juyuan Xu, Zhiyuan Liu, and Maosong Sun. 2023. Communicative agents
    for software development. arXiv preprint arXiv:2307.07924 (2023).

    - <span id="page-10-21"></span>[25] Shuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang
    Chen, Yunzhi Yao, Shumin Deng, Chuanqi Tan, Fei Huang, and Huajun Chen. 2022.
    Reasoning with language model prompting: A survey. arXiv preprint arXiv:2212.09597
    (2022).

    - <span id="page-10-12"></span>[26] Xipeng Qiu, Tianxiang Sun, Yige Xu, Yunfan
    Shao, Ning Dai, and Xuanjing Huang. 2020. Pre-trained models for natural language
    processing: A survey. Science China Technological Sciences 63, 10 (2020), 1872–1897.

    - <span id="page-10-16"></span>[27] Alec Radford, Jeffrey Wu, Rewon Child, David
    Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised
    multitask learners. OpenAI blog 1, 8 (2019), 9.

    - <span id="page-10-32"></span>[28] Veselin Raychev, Martin Vechev, and Eran Yahav.
    2014. Code completion with statistical language models. In Proceedings of the
    35th ACM SIGPLAN conference on programming language design and implementation.
    419–428.

    - <span id="page-10-4"></span>[29] Baptiste Roziere, Jonas Gehring, Fabian Gloeckle,
    Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy
    Rapin, et al. 2023. Code llama: Open foundation models for code. arXiv preprint
    arXiv:2308.12950 (2023).

    - <span id="page-10-36"></span>[30] Zeyu Sun, Qihao Zhu, Lili Mou, Yingfei Xiong,
    Ge Li, and Lu Zhang. 2019. A grammar-based structural cnn decoder for code generation.
    In Proceedings of the AAAI conference on artificial intelligence, Vol. 33. 7055–7062.

    - <span id="page-10-7"></span>[31] Samdyuti Suri, Sankar Narayan Das, Kapil Singi,
    Kuntal Dey, Vibhu Saujanya Sharma, and Vikrant Kaulgud. 2023. Software Engineering
    Using Autonomous Agents: Are We There Yet?. In 2023 38th IEEE/ACM International
    Conference on Automated Software Engineering (ASE). IEEE Computer Society, 1855–1857.

    - <span id="page-10-8"></span>[32] Haoye Tian, Weiqi Lu, Tsz On Li, Xunzhu Tang,
    Shing-Chi Cheung, Jacques Klein, and Tegawendé F Bissyandé. 2023. Is ChatGPT the
    Ultimate Programming Assistant–How far is it? arXiv preprint arXiv:2304.11938
    (2023).

    - <span id="page-10-37"></span>[33] Ashish Vaswani, Noam Shazeer, Niki Parmar,
    Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin.
    2017. Attention is all you need. Advances in neural information processing systems
    30 (2017).

    - <span id="page-10-24"></span>[34] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang,
    Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al.
    2023. A survey on large language model based autonomous agents. arXiv preprint
    arXiv:2308.11432 (2023).

    - <span id="page-10-23"></span>[35] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten
    Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting
    elicits reasoning in large language models. Advances in Neural Information Processing
    Systems 35 (2022), 24824–24837.

    - <span id="page-10-18"></span>[36] Zhiyuan Zeng, Jiatong Yu, Tianyu Gao, Yu Meng,
    Tanya Goyal, and Danqi Chen. 2023. Evaluating large language models at evaluating
    instruction following. arXiv preprint arXiv:2310.07641 (2023).

    - <span id="page-10-1"></span>[37] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi
    Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican
    Dong, et al. 2023. A survey of large language models. arXiv preprint arXiv:2303.18223
    (2023).

    - <span id="page-10-0"></span>[38] Barret Zoph, Colin Raffel, Dale Schuurmans,
    Dani Yogatama, Denny Zhou, Don Metzler, Ed H. Chi, Jason Wei, Jeff Dean, Liam
    B. Fedus, Maarten Paul Bosma, Oriol Vinyals, Percy Liang, Sebastian Borgeaud,
    Tatsunori B. Hashimoto, and Yi Tay. 2022. Emergent abilities of large language
    models. TMLR (2022).'
  decisions:
    language: '- Qualified. Reason: English Paper'
    evaluation_prompt: Qualified.
    related_work_prompt: Qualified
    novelty_prompt: Qualified
    review_only_prompt: Qualified
  llm_input_used: '## Abstract

    The recent development on large language models makes automatically

    constructing small programs possible. It thus has the potential to free

    software engineers from low-level coding and allow us to focus on the perhaps

    more interesting parts of software development, such as requirement engineering

    and system testing. In this project, we develop a prototype named AISD

    (AI-aided Software Development), which is capable of taking high-level

    (potentially vague) user requirements as inputs, generates detailed use cases,

    prototype system designs, and subsequently system implementation. Different

    from existing attempts, AISD is designed to keep the user in the loop, i.e., by

    repeatedly taking user feedback on use cases, high-level system designs, and

    prototype implementations through system testing. AISD has been evaluated with

    a novel benchmark of non-trivial software projects. The experimental results

    suggest that it might be possible to imagine a future where software

    engineering is reduced to requirement engineering and system testing only.


    ## Introduction

    Large language models (LLMs), i.e., transformer-based language models with a huge
    number of parameters, have shown remarkable performance in natural language understanding
    as well as solving complex problems thanks to their emergent abilities [\[38\]](#page-10-0).
    In particular, their abilities of instruction following, step-by-step reasoning,
    and in-context learning have led to many applications in a variety of domains,
    including code generation [\[37\]](#page-10-1). That is, given a description of
    a low-level simple coding task, LLMs such as GPT are capable of synthesizing programs
    automatically, often correctly too [\[5,](#page-10-2) [23,](#page-10-3) [29\]](#page-10-4).
    Thus, it gives us a glint of hope that one day LLMs might free us from manually
    low-level coding.


    In fact, a few recent projects attempted the ambitious goal of replacing programmers
    with LLMs. Li et al. [\[24\]](#page-10-5) propose an end-toend software development
    framework known as ChatDev, which mimics the classical waterfall model and breaks
    down the software development process into four stages, i.e., designing, coding,
    testing, and documenting. That is, given a high-level rather vague requirement,
    ChatDev leverages multiple LLM-based virtual roles


    to generate detailed requirements, design and implementation in


    that sequence through rounds of conversions. MetaGPT [\[13\]](#page-10-6) adopts
    a similar idea and further standardizes each LLM-based agent''s output to guide
    the other agents in the subsequent tasks. For instance, during the design phase,
    MetaGPT generates Product Requirements Documents (PRDs) with a standardized structure
    to coordinate the subsequent development process. While these attempts are shown
    to improve the underlying LLM''s performance to certain extent, they often fail
    when the software project is non-trivial [\[31\]](#page-10-7).


    This is hardly surprising as, even for experienced human programmers, it is infeasible
    to complete a complex software based on vague high-level requirements. This is
    precisely why requirement engineering and system testing play vital roles in the
    software development process, and software development processes such as rapid
    prototyping and agile methods value user feedback during system development highly.
    Requirement engineering and system testing are essential for eliciting the expectations
    of endusers and stakeholders, ensuring that the delivered software aligns with
    their expectations. Requirement engineering is a complex and multifaceted process,
    and no human beings can produce flawless requirement specifications in a single
    attempt. However, the abovementioned approaches conduct requirement analysis only
    lightly and the identified requirements are directly passed on to the coding phase,
    depriving users of the opportunity to validate and modify the automatically generated
    requirements and implementation.


    At the same time, it is perhaps fair to say that requirement engineering in the
    traditional programming paradigm failed to achieve its promises to some extent
    as well as it may often be disconnected from system implementation as the project
    development progresses, e.g., the requirement and corresponding system design
    documents often are not properly maintained along with the system implementation.
    One fundamental reason is that there is limited ways of obtaining timely feedback
    from system designers and programmers (since a software project often lasts months
    or even years). With the help of LLMs, we can potentially shorten the development
    time significantly, allow users to test/validate prototype implementations "instantaneously",
    collect user feedback (e.g., in the form of failed test cases or updated requirements)
    timely and refine the implementation accordingly. In other words, we can make
    requirement engineering more relevant and effective by timely testing the implementation.


    In this work, we experiment with a novel AI-powered software development framework
    called AISD. AISD distinguishes itself from existing approaches in two aspects.
    Firstly, AISD is designed to


    <sup>\*</sup>These authors contributed equally to this work


    Corresponding author


    engage users throughout the software development process, especially during the
    requirement analysis, high-level system design and system validation phase. Secondly,
    AISD adheres to the philosophy that less is more when engaging the human developers.
    Specifically, when presented with a vague requirement, AISD generates a requirement
    document (e.g., use cases) capturing only the core functions required by the system
    and one system design document describing which source files should be built,
    and seeks user-feedback. With the user-feedback, these documents are updated accordingly.
    Note that due to the limited attention span of the LLMs [\[32\]](#page-10-8) (and
    humans too), both documents are designed to be simple but friendly for humans
    and LLMs, which we will show in Section [3.](#page-2-0) Subsequently, AISD decomposes
    the system design into low-level coding tasks and completes them systematically
    and automatically. Once a prototype is implemented (i.e., the resultant system
    passes the unit testing and basic system testing), users are engaged to validate
    the system to check whether their requirements are met. If any failures are identified,
    the implementation, the design and/or the requirements are updated accordingly
    to construct another prototype. This iterative process continues until the users
    accept the product.


    AISD has been implemented as a self-contained toolkit. Considering that existing
    benchmarks, e.g., HumanEval [\[5\]](#page-10-2), MBPP [\[3\]](#page-10-9) and
    CAMEL [\[18\]](#page-10-10) are not suitable for evaluating the capability of
    the LLM-based software development frameworks (all of them either are limited
    to simple function-level implementation tasks or lack detailed requirements specifications
    for system-level implementation tasks), we have developed a novel benchmark named
    CAASD (Capability Assessment of Automatic Software Development). Each task of
    CAASD is equipped with a list of reference use cases depicting the system requirements.
    The reference use cases are used to evaluate the quality and completeness of a
    system implementation. We have compared AISD with two state-of-the-art baselines
    Chat-Dev [\[24\]](#page-10-5) and MetaGPT [\[13\]](#page-10-6) on CAASD. The experimental
    results demonstrate that AISD achieves the highest pass rate while using the fewest
    tokens. On average, AISD achieves an impressive pass rate of 75.2%. Relative to
    these two baselines, there are improvements of 133.5% and 501.6% respectively.
    Moreover, it reduces the number of tokens consumed from at least 28734 to 21993.
    These experimental results provide compelling evidence for the importance of user
    engagement in AI-aided software development.


    In summary, we make the following contributions.


    - We introduce AISD, an AI-aided software development framework that is designed
    to keep users engaged through requirement engineering and system testing.

    - We build a novel benchmark named CAASD for objectively assessing the capabilities
    of various AI-aided software development systems. To the best of our knowledge,
    this is the first benchmark that offers criteria for assessing how well a software
    development task is completed.

    - We conduct a comprehensive evaluation of AISD using the benchmark. The results
    underscore the critical role of human engagement during AI-powered software development.
    These results suggest a potential future where software engineering may be streamlined
    to focus primarily on requirement engineering.


    The remainders of the paper are organized as follows. Section [2](#page-1-0) reviews
    some essential backgrounds. Section [3](#page-2-0) presents the detailed design
    of AISD. Section [4](#page-6-0) presents details on our experiments of applying
    AISD as well as two baselines to the CAASD benchmark. Section [5](#page-9-0) reviews
    related work and Section [6](#page-10-11) concludes.'
  token_usage: 8026
  time_usage: 2.132810354232788
- title: "Applying Bayesian Data Analysis for Causal Inference about Requirements\n\
    \  Quality: A Controlled Experiment"
  abstract: 'It is commonly accepted that the quality of requirements specifications

    impacts subsequent software engineering activities. However, we still lack

    empirical evidence to support organizations in deciding whether their

    requirements are good enough or impede subsequent activities. We aim to

    contribute empirical evidence to the effect that requirements quality defects

    have on a software engineering activity that depends on this requirement. We

    conduct a controlled experiment in which 25 participants from industry and

    university generate domain models from four natural language requirements

    containing different quality defects. We evaluate the resulting models using

    both frequentist and Bayesian data analysis. Contrary to our expectations, our

    results show that the use of passive voice only has a minor impact on the

    resulting domain models. The use of ambiguous pronouns, however, shows a strong

    effect on various properties of the resulting domain models. Most notably,

    ambiguous pronouns lead to incorrect associations in domain models. Despite

    being equally advised against by literature and frequentist methods, the

    Bayesian data analysis shows that the two investigated quality defects have

    vastly different impacts on software engineering activities and, hence, deserve

    different levels of attention. Our employed method can be further utilized by

    researchers to improve reliable, detailed empirical evidence on requirements

    quality.'
  url: http://arxiv.org/abs/2401.01154v4
  keywords: ''
  document: "# Applying Bayesian Data Analysis for Causal Inference about Requirements\
    \ Quality: A Controlled Experiment\n\nJulian Frattini · Davide Fucci · Richard\
    \ Torkar · Lloyd Montgomery · Michael Unterkalmsteiner · Jannik Fischbach · Daniel\
    \ Mendez\n\nReceived: date / Accepted: date\n\nAbstract It is commonly accepted\
    \ that the quality of requirements specifications impacts subsequent software\
    \ engineering activities. However, we still lack empirical evidence to support\
    \ organizations in deciding whether their requirements are good enough or impede\
    \ subsequent activities. We aim to contribute empirical evidence to the effect\
    \ that requirements quality defects have on a software engineering activity that\
    \ depends on this requirement. We conduct a controlled experiment in which 25\
    \ participants from industry and university generate domain models from four natural\
    \ language requirements containing different quality defects. We evaluate the\
    \ resulting models using both frequentist and Bayesian data analysis. Contrary\
    \ to our expectations, our results show that the use of passive voice only has\
    \ a minor impact on the resulting domain models. The use of ambiguous pronouns,\
    \ however, shows a strong effect on various properties of the resulting domain\
    \ models. Most no-\n\nR. Torkar\n\nChalmers and University of Gothenburg, 41756\
    \ G¨oteborg, Sweden Stellenbosch Institute for Advanced Study (STIAS), Stellenbosch,\
    \ South Africa E-mail: richard.torkar@gu.se\n\nL. Montgomery University of Hamburg,\
    \ Mittelweg 177, 20148 Hamburg, Germany E-mail: lloyd.montgomery@uni-hamburg.de\n\
    \nJ. Fischbach Netlight Consulting GmbH, Prannerstraße 4, 80333 M¨unchen, Germany\
    \ E-mail: jannik.fischbach@netlight.com\n\nJ. Fischbach, D. Mendez fortiss GmbH,\
    \ Guerickestraße 25, 80805 M¨unchen, Germany E-mail: {lastname}@fortiss.org\n\n\
    - This version of the article has been accepted for publication, after peer review\
    \ (when applicable) but is not the Version of Record and does not reflect post-acceptance\
    \ improvements, or any corrections. The Version of Record is available online\
    \ at: <http://dx.doi.org/10.1007/s10664-024-10582-1>\n\nJ. Frattini, D. Fucci,\
    \ M. Unterkalmsteiner, and D. Mendez\n\nBlekinge Institute of Technology, Valhallav¨agen\
    \ 1, 37140 Karlskrona, Sweden\n\nE-mail: {firstname}.{lastname}@bth.se\n\ntably,\
    \ ambiguous pronouns lead to incorrect associations in domain models. Despite\
    \ being equally advised against by literature and frequentist methods, the Bayesian\
    \ data analysis shows that the two investigated quality defects have vastly different\
    \ impacts on software engineering activities and, hence, deserve different levels\
    \ of attention. Our employed method can be further utilized by researchers to\
    \ improve reliable, detailed empirical evidence on requirements quality.\n\nKeywords\
    \ Requirements Engineering · Requirements Quality · Experiment · Replication ·\
    \ Bayesian Data Analysis\n\n# 1 Introduction\n\nSoftware requirements specify\
    \ the needs and constraints that stakeholders impose on a desired system. Software\
    \ requirements specifications (SRS), the explicit manifestation of requirements\
    \ as an artifact [\\[78\\]](#page-47-0), serve as input for various subsequent\
    \ software engineering (SE) activities, such as deriving a software architecture,\
    \ implementing features, or generating test cases [\\[79\\]](#page-47-1). As a\
    \ consequence, the quality of an SRS impacts the quality of requirements-dependent\
    \ activities [\\[37,](#page-45-0) [38,](#page-45-1) [48\\]](#page-46-0). A quality\
    \ defect in an SRS—for example, an ambiguous formulation—can cause differing interpretations\
    \ and result in the design and implementation of a solution that does not meet\
    \ the stakeholders' needs [\\[77\\]](#page-47-2). The inherent complexity of natural\
    \ language (NL), which is most commonly used for specifying requirements [\\[43\\\
    ]](#page-46-1), aggravates this challenge further. Since quality defects are understood\
    \ to scale in cost for removal [\\[9\\]](#page-44-0), organizations are interested\
    \ in identifying and removing these defects as early as possible [\\[80\\]](#page-48-0).\n\
    \nWithin the requirements engineering (RE) research domain, the field of requirements\
    \ quality research aims to meet this challenge [\\[80\\]](#page-48-0). Requirements\
    \ quality research has already identified several attributes of requirements quality\
    \ [\\[80\\]](#page-48-0) (e.g., unambiguity, completeness, consistency) and proposes\
    \ quality factors, i.e., requirements writing rules (e.g., the use of passive\
    \ voice being associated with bad quality [\\[36\\]](#page-45-2)) as well as tools\
    \ that automatically detect alleged quality defects [\\[35\\]](#page-45-3). However,\
    \ existing approaches fall short in at least three regards [\\[48\\]](#page-46-0):\
    \ i) only a fraction of publications provide empirical evidence that would demonstrate\
    \ the impact of quality defects [\\[80\\]](#page-48-0), ii) the few empirical\
    \ studies that do so largely ignore potentially confounding context factors [\\\
    [65,](#page-47-3) [81\\]](#page-48-1), and iii) the analyses conducted in existing\
    \ publications do not go beyond binary insights (i.e., a quality factor does have\
    \ an impact or it does not) [\\[25,](#page-45-4) [36\\]](#page-45-2). These gaps\
    \ have impeded the adoption of requirements quality research in practice [\\[43\\\
    ]](#page-46-1).\n\nIn this article, we aim to address the above-mentioned shortcomings\
    \ by i) conducting a controlled experiment with 25 participants simulating a requirementsdependent\
    \ activity (i.e., domain modeling) using four natural-language requirements as\
    \ input. The experiment contributes empirical evidence on the impact of two commonly\
    \ researched quality factors passive voice [\\[36\\]](#page-45-2) and ambiguous\
    \ pronouns [\\[31\\]](#page-45-5). The investigation of the impact of passive\
    \ voice is\n\na conceptual replication [\\[2\\]](#page-44-1) of the only controlled\
    \ experiment studying the impact of passive voice on domain modeling [\\[36\\\
    ]](#page-45-2) known to us. Therefore, our experiment also strengthens the robustness\
    \ of their conclusions by providing diagnostic evidence [\\[83\\]](#page-48-2).\
    \ Further, we ii) collect data about relevant context factors such as experience\
    \ in software engineering (SE) and RE, domain knowledge, and task experience,\
    \ and integrate these data in our data analysis. Finally, we iii) contrast the\
    \ state-of-the-art frequentist data analysis (FDA) with Bayesian data analysis\
    \ (BDA), which entails both a causal framework and Bayesian modeling for statistical\
    \ causal inference [\\[76\\]](#page-47-4). The latter has recently been popularized\
    \ in SE research [\\[52\\]](#page-46-2) since it generates more nuanced empirical\
    \ insights. Our study is categorized as a laboratory experiment in a contrived\
    \ setting [\\[104\\]](#page-49-0), isolating the effect of the selected quality\
    \ factors of interest. The causal inference of their impact contributes to our\
    \ long-term goal of providing an empirically grounded understanding of the impact\
    \ of requirements quality. This will support organizations in assessing their\
    \ requirements and detecting relevant quality defects early.\n\nThis paper makes\
    \ the following contributions:\n\n- 1. a controlled experiment investigating the\
    \ impact of requirements quality;\n- 2. a conceptual replication of the only controlled\
    \ experiment investigating the impact of passive voice [\\[36\\]](#page-45-2);\n\
    - 3. the application of BDA to requirements quality research, which is among the\
    \ first of its kind in RE; and\n- 4. an archived replication package containing\
    \ all supplementary material, including protocols and guidelines for data collection\
    \ and extraction, the raw data, analysis scripts, figures, and results [\\[46\\\
    ]](#page-46-3).\n\nThe remainder of this manuscript is organized as follows. Section\
    \ [2](#page-2-0) introduces relevant related work. We present our research method\
    \ in Section [3](#page-9-0) and the results in Section [4.](#page-28-0) We discuss\
    \ these results in Section [5](#page-36-0) before concluding our manuscript in\
    \ Section [6.](#page-43-0)\n\n# <span id=\"page-2-0\"></span>2 Background\n\n\
    Section [2.1](#page-2-1) introduces the research domain of this work by summarizing\
    \ existing research on requirements quality. Section [2.2](#page-8-0) motivates\
    \ BDA—the statistical tool employed in this work—by explaining its adoption in\
    \ SE research.\n\n# <span id=\"page-2-1\"></span>2.1 Requirements Quality\n\n\
    Section [2.1.1](#page-3-0) introduces the general area of requirements quality\
    \ research and Section [2.1.2](#page-3-1) presents two research directions within.\
    \ Section [2.1.3](#page-6-0) summarizes the three major shortcomings that currently\
    \ challenge requirements quality research.\n\n# <span id=\"page-3-0\"></span>2.1.1\
    \ Requirements Quality Research\n\nIt is commonly accepted that the quality of\
    \ requirements specifications impacts subsequent SE activities, which depend on\
    \ these specifications [\\[38,](#page-45-1) [48\\]](#page-46-0). Quality defects\
    \ in requirements specifications may, therefore, ultimately cause budget overrun\
    \ [\\[91\\]](#page-48-3) or even project failure [\\[77\\]](#page-47-2). Two further\
    \ factors aggravate the effect. Firstly, natural language (NL), which is inherently\
    \ ambiguous and, hence, prone to quality defects, remains the most commonly used\
    \ syntax to specify requirements [\\[45,](#page-46-4) [109\\]](#page-49-1). Secondly,\
    \ the cost of removing quality defects scales the longer they remain undetected\
    \ [\\[9\\]](#page-44-0). For example, clarifying an ambiguous requirements specification\
    \ takes comparatively less effort than reimplementing a faulty implementation\
    \ based on the ambiguous specification. However, it requires detecting the ambiguity\
    \ and predicting that the ambiguity potentially causes the implementation to become\
    \ faulty before it happens. These circumstances necessitate managing the quality\
    \ of requirements specifications to detect and remove requirements quality defects\
    \ preemptively.\n\nRequirements quality research seeks answers to this need [\\\
    [80\\]](#page-48-0). One main driver of this research is requirements quality\
    \ factors [\\[49\\]](#page-46-5), i.e., metrics that can be evaluated on NL requirements\
    \ specifications to determine quality defects. For example, the voice of an NL\
    \ sentence (active or passive) is considered a quality factor, as the use of passive\
    \ voice is associated with bad requirements quality due to potential omission\
    \ of information [\\[36\\]](#page-45-2). Automatic detection techniques using\
    \ natural language processing (NLP) [\\[116\\]](#page-49-2) can automatically\
    \ evaluate quality factors to detect defects in NL requirements specifications\
    \ [\\[35\\]](#page-45-3).\n\n# <span id=\"page-3-1\"></span>2.1.2 Existing Research\
    \ on Passive Voice and Ambiguous Pronouns\n\nWe present two examples of commonly\
    \ researched requirements quality factors in the following sections.\n\nPassive\
    \ Voice One commonly researched requirements quality factor is using passive voice\
    \ in natural language requirements specifications. A sentence in passive voice\
    \ elevates the semantic patient rather than the semantic agent of the main verb\
    \ to the grammatical subject [\\[85\\]](#page-48-4). For example, in the passive\
    \ voice sentence \"Web-based displays of the most current ASPERA-3 data shall\
    \ be provided for public view.\", the patient of the providing process the \"\
    web-based displays\"—becomes the grammatical subject of the sentence. Even though\
    \ passive voice sentences may still contain the semantic agent (e.g., \"Web-based\
    \ displays of the most current ASPERA-3 data shall be provided for public view\
    \ by a front-end.\"), writers often omit it intentionally or unintentionally [\\\
    [73\\]](#page-47-5). Figure [1](#page-4-0) visualizes the omission of the semantic\
    \ agent in this exemplary requirement specification.\n\nOmitting the semantic\
    \ agent of a sentence in a passive voice formulation obscures critical information\
    \ in a requirements specification. Hence, requirements quality guidelines advise\
    \ against using passive voice [\\[94\\]](#page-48-5). However, while\n\n<span\
    \ id=\"page-4-0\"></span>![](_page_4_Figure_1.jpeg)\n\nFig. 1: Formalization of\
    \ a requirements specification R2 using passive voice\n\nseveral guidelines advise\
    \ against the use of passive voice based on the theoretical argument of information\
    \ omission presented above [\\[57,](#page-46-6) [58,](#page-46-7) [61,](#page-47-6)\
    \ [72,](#page-47-7) [94\\]](#page-48-5), only two papers investigate whether passive\
    \ voice has an actual impact on requirements quality: Krisch et al. let domain\
    \ experts rate active and passive voice requirements as either problematic or\
    \ unproblematic. They concluded that most passive voice requirements were unproblematic\
    \ as the surrounding context information compensated the omission of the semantic\
    \ agent of the sentence [\\[73\\]](#page-47-5). Femmer et al. conducted an empirical\
    \ investigation of the impact of the use of passive voice in requirements specification\
    \ on the domain modeling activity in a controlled experiment. They concluded that\
    \ passive voice only causes missing relationships from the domain model, but not\
    \ missing actors or entities as initially assumed [\\[36\\]](#page-45-2). The\
    \ limited evidence for the harmfulness of using passive voice in requirements\
    \ specifications [\\[36,](#page-45-2)[73\\]](#page-47-5) stands in stark contrast\
    \ to the amount of tools and approaches proposed to automatically detect quality\
    \ defects by identifying the use of passive voice [\\[27,](#page-45-6)[35,](#page-45-3)[38,](#page-45-1)[40,](#page-45-7)[57,](#page-46-6)[61,](#page-47-6)[71,](#page-47-8)[72,](#page-47-7)[86,](#page-48-6)[95,](#page-48-7)[103\\\
    ]](#page-49-3).\n\nAmbiguous Pronouns The inherent ambiguity of natural language\
    \ [\\[93\\]](#page-48-8) poses several challenges for requirements specifications\
    \ using natural language [\\[4,](#page-44-2)[84\\]](#page-48-9). One commonly\
    \ researched requirements quality factor related to ambiguity is the use of ambiguous\
    \ pronouns, which is a type of referential ambiguity [\\[8\\]](#page-44-3). An\
    \ ambiguous pronoun exhibits anaphoric ambiguity, that \"occurs when a pronoun\
    \ can plausibly refer to different entities and thus be interpreted differently\
    \ by different readers\" [\\[31\\]](#page-45-5). For example, in the requirements\
    \ specification \"The data processing unit stores telemetric data for scientific\
    \ evaluation; therefore, it needs to comply with the FAIR principles of data storage.\"\
    , the pronoun it could syntactically refer to the \"data processing unit\", the\
    \ \"telemetric data\",\n\n<span id=\"page-5-0\"></span>![](_page_5_Figure_1.jpeg)\n\
    \nFig. 2: Formalization of a requirements specification R3 using an ambiguous\
    \ pronoun\n\nor the \"scientific evaluation.\" Figure [2](#page-5-0) visualizes\
    \ how a reader can resolve the reference.\n\nTo avoid deviating interpretations\
    \ of a requirements specification, established requirements quality guidelines\
    \ advise against the use of ambiguous pronouns [\\[94\\]](#page-48-5) at the expense\
    \ of conciseness. However, the number of publications proposing tools and algorithms\
    \ to automatically identify and resolve ambiguous pronouns [\\[20,](#page-45-8)[31](#page-45-5)[–33,](#page-45-9)[68,](#page-47-9)[98,](#page-48-10)[100,](#page-48-11)[114,](#page-49-4)[115\\\
    ]](#page-49-5) significantly outweighs the singular publication that actually\
    \ has empirically investigated the effect of ambiguous pronouns. Kamsties et al.\
    \ investigated the effects of formalizing requirements, which included evaluating\
    \ the propagation of ambiguous pronouns from NL into more formal specifications\
    \ [\\[67\\]](#page-47-10). Their experiment involving students revealed that 20-37%\
    \ of all ambiguous pronouns were incorrectly resolved while formalizing NL requirements\
    \ specifications. While Kamsties et al. concluded that requirements formalization\
    \ does not sufficiently resolve ambiguities, these results also support the assumption\
    \ that ambiguous pronouns propagate into subsequent artifacts depending on the\
    \ requirements specifications. On the contrary, the scarce empirical work on the\
    \ effect of ambiguity in general (not specifically ambiguous pronouns) agrees\
    \ that ambiguity has a negligible effect on downstream software engineering activities\
    \ [\\[15,](#page-44-4) [91\\]](#page-48-3). Other than these empirical contributions,\
    \ the aforementioned publications proposing solutions rather than investigating\
    \ the relevance of the problem refer to deontic guidelines [\\[5,](#page-44-5)[94\\\
    ]](#page-48-5), anecdotal evidence about ambiguity in general [\\[11,](#page-44-6)[21,](#page-45-10)[39,](#page-45-11)[42\\\
    ]](#page-46-8), or—in very rare cases—cognitive science theory [\\[93\\]](#page-48-8).\n\
    \n# <span id=\"page-6-0\"></span>2.1.3 Shortcomings in Requirements Quality Research\n\
    \nThe previous examples highlight at least three shortcomings from which requirements\
    \ quality research suffers.\n\nLack of empirical evidence First, the relevance\
    \ of quality factors like passive voice or ambiguous pronouns is rarely determined\
    \ empirically [\\[48\\]](#page-46-0). Scientific contributions proposing solutions\
    \ (i.e., detecting or removing quality defects) outweigh those investigating the\
    \ actual extent of the assumed problem. Without knowledge about this extent, it\
    \ remains unclear whether a proposed solution addresses a problem that is actually\
    \ relevant to practice.\n\nPrevious systematic research has come to the same conclusion.\
    \ For example, in a previous systematic study, we determined that the effect of\
    \ quality defects is determined empirically in only 18% of the publications included\
    \ in our sample [\\[48\\]](#page-46-0). Bano et al. found only two publications\
    \ within their sample of 28 studies that empirically investigated the importance\
    \ of ambiguity detection [\\[4\\]](#page-44-2). Montgomery et al. systematically\
    \ investigated empirical research on requirements quality research and also concluded\
    \ that most studies focus on improving requirements quality (i.e., detecting and\
    \ removing defects) rather than defining or evaluating it (i.e., understanding\
    \ the actual effect) [\\[80\\]](#page-48-0). Instead, most requirements quality\
    \ publications draw on anecdotal evidence and unproven hypotheses [\\[48\\]](#page-46-0).\
    \ This lack of empirical evidence undermines the trust in requirements quality\
    \ research and hinders its adoption in practice [\\[34,](#page-45-12)[43,](#page-46-1)[90\\\
    ]](#page-48-12).\n\nLack of context Second, existing research mostly ignores the\
    \ influence of context factors on the effect of quality defects [\\[48\\]](#page-46-0).\
    \ Context factors encompass all human and organizational factors influencing the\
    \ downstream SE activities involving requirements specification [\\[89\\]](#page-48-13).\
    \ For example, the domain experience of a stakeholder or the process model used\
    \ during development may mediate the effect of ambiguity in requirements specifications\
    \ [\\[91\\]](#page-48-3).\n\nRequirements quality research has acknowledged the\
    \ relevance of context factors to requirements quality [\\[65,](#page-47-3)[81\\\
    ]](#page-48-1). Recent propositions have advocated for a shift away from the unrealistic\
    \ goal of developing a one-size-fits-all solution to requirements quality and,\
    \ instead, moving towards more context-sensitive research [\\[12,](#page-44-7)[77\\\
    ]](#page-47-2). However, this initiative has shown little effect in requirements\
    \ quality research so far [\\[48\\]](#page-46-0).\n\nLack of detailed projections\
    \ Third, the few empirical contributions to requirements quality research limit\
    \ their insights to categorical projections, i.e., the evaluation of a quality\
    \ factor on a requirements specification (e.g., using passive voice or not using\
    \ passive voice) are projected on a categorical scale (e.g., good quality or bad\
    \ quality). Most commonly, the categorical output space consists of two [\\[25\\\
    ]](#page-45-4) (impact or no impact) or three [\\[37\\]](#page-45-0) (positive\
    \ impact, no impact, or negative impact) categories. This simplification inhibits\
    \ a nuanced comparison of different quality factors. On an absolute scale, a quality\
    \ factor\n\n<span id=\"page-7-0\"></span>![](_page_7_Figure_1.jpeg)\n\nFig. 3:\
    \ Reduced version of the activity-based Requirements Quality Theory [\\[48\\]](#page-46-0)\n\
    \nhaving an impact does not automatically entail that this impact is significant\
    \ and warrants resources for detection and mitigation. On a relative scale, two\
    \ quality factors that have an impact are impossible to compare to allocate resources\
    \ towards the more significant one. Consequently, even empirical contributions\
    \ to the field of requirements quality lack sophisticated insights that would\
    \ support organizations in determining and dealing with relevant quality factors\
    \ to control during the RE phase.\n\n# Requirements Quality Research Gaps\n\n\
    Requirements quality research suffers from (1) a lack of empirical evidence about\
    \ the relevance of quality factors, (2) a lack of contextsensitivity, and (3)\
    \ evaluations of impact that are more fine-grained than categorical.\n\n# 2.1.4\
    \ Requirements Quality Theory\n\nBased on the identification of the above-mentioned\
    \ shortcomings, we have developed a requirements quality theory in previous research\
    \ [\\[48\\]](#page-46-0). This theory frames requirements quality as the impact\
    \ that properties of requirements specifications (called the quality factors)\
    \ in combination with context factors have on the properties (called attributes)\
    \ of activities that use these specifications as input. Figure [3](#page-7-0)\
    \ visualizes the requirements quality theory.\n\nThe requirements quality theory\
    \ facilitates overcoming the aforementioned shortcomings. Because the RQT makes\
    \ the quality of a requirements specification dependent on its impact on subsequent\
    \ activities, it demands empirical evidence about this impact before claiming\
    \ that a quality factor reflects actual requirements quality. The inclusion of\
    \ context factors in the definition of requirements quality mandates context-sensitivity.\
    \ The abstraction of the impact concept allows for more advanced relationships\
    \ between specifications and impacted activities than just the categorical type.\n\
    \nHowever, while the requirements quality research draws on mature software quality\
    \ research [\\[25,](#page-45-4) [110\\]](#page-49-6), it has not been actively\
    \ used yet. Even the predecessor of the theory [\\[37\\]](#page-45-0) was explicitly\
    \ ignored in follow-up research by its authors due to the complexity of its implementation\
    \ [\\[35\\]](#page-45-3). The work presented in this manuscript constitutes the\
    \ first application of the theory known to the authors.\n\n# <span id=\"page-8-0\"\
    ></span>2.2 Bayesian data analysis in software engineering\n\nIn recent years,\
    \ SE research has adopted Bayesian data analysis (BDA) for statistical causal\
    \ inference. BDA signifies a departure from frequentist methods like null-hypothesis\
    \ significance testing (NHST), the previous state-of-the-art in terms of inferential\
    \ statistics in SE research. NHST determines whether there is a \"statistically\
    \ significant\" difference between two or more distributions. Observations of\
    \ a dependent variable are stratified by an independent variable to obtain a binary\
    \ answer of whether or not different values of the independent variable correlate\
    \ with different distributions of the dependent variable.\n\nOpposed to that,\
    \ BDA encourages the use of causal frameworks [\\[76\\]](#page-47-4). These frameworks\
    \ make causal assumptions explicit [\\[29\\]](#page-45-13) and allow reasoning\
    \ about causally relevant variables [\\[87,](#page-48-14) [88\\]](#page-48-15).\
    \ Furthermore, BDA abstains from reducing complex variable distributions to binary\
    \ inference [\\[76\\]](#page-47-4). Instead, dependent variables are expressed\
    \ as a probability distribution, which preserves the natural uncertainty with\
    \ which any variable is determined. Similarly, the impact of any independent variable\
    \ on the dependent variable is expressed in terms of a probability distribution.\
    \ Using Bayes' Theorem, these assigned prior probability distributions are updated\
    \ with observed data to obtain a posterior probability distribution [\\[52\\]](#page-46-2).\
    \ Given the observed data, these posterior probability distributions model the\
    \ most likely impact of variable values. BDA methods are becoming widely adopted\
    \ also due to the modern computational power enabling Markov Chain Monte Carlo\
    \ (MCMC) randomized algorithms [\\[13\\]](#page-44-8), tools like Stan [\\[17\\\
    ]](#page-44-9), and libraries like rethinking [\\[76\\]](#page-47-4) and brms\
    \ [\\[16\\]](#page-44-10).\n\nWhile BDA is associated with a much steeper learning\
    \ curve than frequentist methods, it offers several advantages.\n\n- 1. BDA is\
    \ not based on the unsound probabilistic extension of the modus tollens like frequentist\
    \ hypothesis testing. The modus tollens (P → Q, ¬Q ∴ ¬P, or if P implies Q and\
    \ Q is false, then P must also be false) applies to propositional, Boolean logic,\
    \ but not when inferring from probabilities [\\[52\\]](#page-46-2).\n- 2. BDA\
    \ provides more complex insights than point-wise comparisons. Although BDA lacks\
    \ out-of-the-box statistical methods like frequentists' ttests that are simple\
    \ to apply, its results reflect the uncertainty of the data, the influence of\
    \ context, and they can be interpreted more intuitively.\n\n3. The causal framework\
    \ entailed by BDA makes causal assumptions explicit. The Bayesian workflow [\\\
    [56\\]](#page-46-9) makes any hypothesis of causal relations explicit. Analyses\
    \ become more transparent, and competing causal assumptions are easier to assess.\n\
    \nFuria et al. [\\[52\\]](#page-46-2), and Torkar et al. [\\[106\\]](#page-49-7)\
    \ advocate for the adoption of BDA in software engineering research by discussing\
    \ its advantages over the frequentist counterpart and mitigating its steep learning\
    \ curve with extensive demonstrations [\\[53\\]](#page-46-10). SE researchers\
    \ have begun to apply BDA in various evaluations. Previous studies have used BDA\
    \ to model bug-fixing time in open source software projects [\\[108\\]](#page-49-8),\
    \ to confirm the broken window theory in SE [\\[74\\]](#page-47-11), to investigate\
    \ gender differences in personality traits of software engineers [\\[96\\]](#page-48-16),\
    \ and to understand data-driven decision making practices [\\[105\\]](#page-49-9).\
    \ In the area of requirements engineering, BDA has been used to evaluate the effect\
    \ of obsolete requirements on software estimation [\\[60\\]](#page-47-12) and\
    \ to compare requirements prioritization criteria [\\[7\\]](#page-44-11).\n\n\
    # <span id=\"page-9-0\"></span>3 Method\n\nWe conducted a controlled experiment\
    \ that investigates the impact of requirements quality on a software engineering\
    \ activity. Our goal is both to (1) contribute empirical evidence to the effect\
    \ of quality defects and (2) compare the inferential capabilities of frequentist\
    \ (FDA) with Bayesian (BDA) statistics. Part of our experiment contributes a conceptual\
    \ replication [\\[2\\]](#page-44-1) of the study conducted and reported by Femmer\
    \ et al. [\\[36\\]](#page-45-2) and re-analyzed by us [\\[47\\]](#page-46-11),\
    \ as a subset of our hypotheses overlaps with theirs and our study contributes\
    \ diagnostic evidence for their claims [\\[83\\]](#page-48-2). Therefore, we report\
    \ the design of the experiment with emphasis on the replication following the\
    \ guidelines by Carver [\\[19\\]](#page-45-14).\n\n# 3.1 Goals\n\nWe formulate\
    \ our goal using the goal-question metric approach [\\[113\\]](#page-49-10). We\
    \ aim to characterize the impact of passive sentences and sentences using ambiguous\
    \ pronouns in requirements on domain modeling with respect to the quality of the\
    \ created domain model artifacts from the point of view of software engineers\
    \ in the context of an analysis of requirements from an industrial project. In\
    \ this definition, software engineer includes all roles that work with requirements\
    \ specifications, including software developers, requirements engineers, business\
    \ analysts, managers, and more. We derive the following research questions from\
    \ our goal:\n\n– RQ1: Do quality defects in NL requirements specifications harm\
    \ the domain modeling activity?\n\n- RQ1.1: Does the use of passive voice in NL\
    \ requirements specifications harm the duration, completeness, conciseness, and\
    \ correctness of the domain modeling activity?\n- RQ1.2: Does the use of ambiguous\
    \ pronouns in NL requirements specifications harm the duration, completeness,\
    \ conciseness, and correctness of the domain modeling activity?\n- RQ1.3: Does\
    \ the combined use of passive voice and ambiguous pronouns in NL requirements\
    \ specifications harm the duration, completeness, conciseness, and correctness\
    \ of the domain modeling activity?\n- RQ2: Do context factors influence the domain\
    \ modeling activity?\n\t- RQ2.1: Do context factors harm the domain modeling activity?\n\
    \t- RQ2.2: Do context factors mediate the impact of quality defects on the domain\
    \ modeling activity?\n\nRQ1 is dedicated to the main relationship of interest\
    \ between quality defects and an affected activity. RQ1.1 aligns with the research\
    \ question driving the original study [\\[36\\]](#page-45-2), which makes this\
    \ part of our study a conceptual replication. RQ1.2 extends the scope of the investigation\
    \ of quality factors with ambiguous pronouns. RQ1.3 investigates the interaction\
    \ between the two quality factors. RQ2 adds a context-sensitive perspective to\
    \ the relationship. RQ2.1 focuses on the direct effect that context factors have\
    \ on the affected activity. RQ2.2 investigates whether context factors mediate\
    \ the effect of quality defects on the activity.\n\n# 3.2 Original Experiment\n\
    \nThe original study [\\[36\\]](#page-45-2) addresses the research question \"\
    Is the use of passive sentences in requirements harmful for domain modeling?\"\
    \ The authors involved 15 university students from different study programs (2\
    \ B.Sc., 8 M.Sc., 4 Ph.D., one unknown) in a controlled randomized experiment\
    \ with a parallel design [\\[113\\]](#page-49-10). Each participant was assigned\
    \ to one of two groups and received seven requirements that were formulated either\
    \ using active or passive voice. The experimental task was to derive a domain\
    \ model from each requirement that contains all relevant actors, domain objects,\
    \ and associations between them. The study material and results are available\
    \ online.[1](#page-10-0)\n\nFor the dependent variable, the authors calculated\
    \ the number of missing domain model elements (i.e., actors, objects, and associations).\
    \ Although the authors also recorded context variables such as a categorical assessment\
    \ of general knowledge in SE and RE, these were not used in the analysis. The\
    \ analysis followed a frequentist approach performing a null-hypothesis significance\
    \ test for each of the three domain model elements to determine whether a statistically\
    \ significant difference between the experimental groups exists. The study shows\
    \ a statistically significant difference in the number of identified associations\
    \ but not in the number of actors or objects. The authors conclude\n\n<span id=\"\
    page-10-0\"></span><sup>1</sup> <https://doi.org/10.5281/zenodo.7499290>\n\nthat\
    \ the commonly assumed impact of passive voice on missing domain model actors\
    \ is actually negligible, but passive voice impedes the understanding of the relationships\
    \ between entities in the requirements specification.\n\n# 3.3 Reanalysis\n\n\
    The original study by Femmer et al. analyzed its data under simplified assumptions.\
    \ Among these is the assumption that the three dependent variables (number of\
    \ missing actors, objects, and associations) only depend on the main factor (use\
    \ of active or passive voice). We challenged this assumption in a reanalysis of\
    \ the original data [\\[47\\]](#page-46-11) for the following reasons:\n\n- 1.\
    \ In a small-scale experiment employing a parallel design, there is no measure\
    \ to control subject variability [\\[107\\]](#page-49-11), such that context factors\
    \ like experience or skill might affect the dependent variables.\n- 2. Missing\
    \ an actor or object in the domain model (i.e., a node) necessarily causes an\
    \ association to be missed (i.e., an edge that would have connected these nodes).\n\
    \nFigure [4a](#page-12-0) visualizes the causal assumptions of the original experiment\
    \ [\\[36\\]](#page-45-2) as a directed acyclic graph [\\[29\\]](#page-45-13) (the\
    \ syntax of which is further explained in Section [3.4.10\\)](#page-22-0) and\
    \ Figure [4b](#page-12-0) shows the revision in scope of the reanalysis [\\[47\\\
    ]](#page-46-11). The revision includes (1) two context factors that were already\
    \ recorded but not used in the original experiment, and (2) two causal relations\
    \ between the response variables.\n\nWe performed a re-analysis, i.e., an independent\
    \ analysis of the same data using a different statistical model [\\[59\\]](#page-47-13),\
    \ which is sometimes referred to as a test of robustness [\\[83\\]](#page-48-2).\
    \ During this re-analysis, we replaced the NHSTs with regression models that include\
    \ context factors and the affecting response variables in the case of missing\
    \ associations.\n\nThe results of this re-analysis agree with the original study\
    \ in that the effect of passive voice on the number of missing actors and objects\
    \ is negligible. However, the re-analysis disagrees with the original study regarding\
    \ the effect of passive voice on the number of missing associations. The re-analysis\
    \ determined that passive voice slightly increases the number of missing associations\
    \ (βpv = 0.7). Still, the confidence interval of this effect (CIpv = (−0.56, 1.90))\
    \ intersects 0 and is, therefore, not significant. On the other hand, the effect\
    \ of missing objects on missing associations was significant (βact = 1.12, CIact\
    \ = (0.32, 1.96)). The re-analysis did not find a significant effect of the available\
    \ context variables on the response variables. Our re-analysis concludes that\
    \ the effect of passive voice on the domain modeling activity is less significant\
    \ than originally assumed [\\[47\\]](#page-46-11).\n\n<span id=\"page-12-0\"></span>![](_page_12_Figure_1.jpeg)\n\
    \nFig. 4: Causal assumptions about the impact of passive voice\n\n# 3.4 Our Experiment\n\
    \nThe reanalysis [\\[47\\]](#page-46-11) of the study by Femmer et al. [\\[36\\\
    ]](#page-45-2) did improve the conclusion validity of the results but failed to\
    \ address other shortcomings. For example, the subject variability still threatened\
    \ the internal validity of the results due to the parallel design of the experiment\
    \ [\\[107\\]](#page-49-11), and the context factors were limited to those recorded\
    \ during the original study. Hence, we used their study as inspiration for our\
    \ own presented in this paper and aimed to improve upon the research design. During\
    \ the preparation of our study, we conferred with the authors of the original\
    \ study and made the following changes to the original study.\n\n- Experimental\
    \ design: We employ a factorial crossover instead of a parallel design, which\
    \ minimizes the risk of confounding (i.e., each participant acts as their own\
    \ control) while requiring a smaller sample [\\[107\\]](#page-49-11).\n- Independent\
    \ variables: This study investigates—in addition to using passive voice—the impact\
    \ of ambiguous pronouns in requirements specifications and their combined usage\
    \ to extend the range of requirements quality defects.\n- Dependent variables:\
    \ We merged two types of elements in the domain model (the nodes of the model,\
    \ i.e., actors and objects) into a single type entity because they represent the\
    \ same concept in the domain model (nodes) [\\[36\\]](#page-45-2) and the distribution\
    \ in our experimental objects is heavily skewed (16/17 entities are objects).\
    \ Furthermore, we increased the dependent variables by additionally evaluating\
    \ the number of superfluous enti-\n\nties, the number of wrong associations, and\
    \ the duration for creating the domain model.\n\n- Sampling strategy: We sample\
    \ from both students and practitioners of software engineering to more accurately\
    \ represent the target population of software engineers. This change aims at increasing\
    \ the external validity of our results [\\[3\\]](#page-44-12).\n- Instrumentation:\
    \ The participants performed the experimental task online using a web-based application\
    \ rather than offline using pen and paper. This allows for more flexibility in\
    \ reaching industry participants [\\[3\\]](#page-44-12).\n- Context factors: To\
    \ obtain a richer understanding of the impact of quality defects, we included\
    \ seven additional context factors. This change made it necessary to extend the\
    \ questionnaire used in the original study to collect demographic information\
    \ from the participants.\n- Experimental object: We sampled the objects from a\
    \ data set of industrial requirements specifications [\\[41\\]](#page-46-12) rather\
    \ than from a requirements specification written in a student project [\\[36\\\
    ]](#page-45-2) to increase the realism of the experimental task [\\[102\\]](#page-49-12).\n\
    - Analysis: The crossover design produces paired data as opposed to the unpaired\
    \ data of the original experiment, which changes the appropriate hypothesis test\
    \ [\\[107\\]](#page-49-11) (Mann-Whitney U test in the original study vs. Wilcoxon\
    \ signed-rank test in this study). In addition, we extend the original FDA by\
    \ performing a Bonferroni correction to deal with family-wise error rate when\
    \ testing multiple hypotheses [\\[6\\]](#page-44-13). Furthermore, we additionally\
    \ analyze the data using BDA.\n\nOur experiment differs from the original experiment\
    \ [\\[36\\]](#page-45-2) in all elements [\\[59\\]](#page-47-13). However, because\
    \ a subset of our hypotheses aligns with their hypotheses, part of our study counts\
    \ as a conceptual replication [\\[2\\]](#page-44-1) since it contributes diagnostic\
    \ evidence for the original claims [\\[83\\]](#page-48-2). In the rest of this\
    \ subsection, we report the design of our experiment following the guidelines\
    \ by Jedlitschka et al. [\\[64\\]](#page-47-14).\n\n# 3.4.1 Experimental Task\n\
    \nWe simulate the use of a requirements specification by subjecting participants\
    \ to a requirements processing activity, i.e., a common task representing the\
    \ use of requirements [\\[36\\]](#page-45-2). In particular, we present four single-sentence,\
    \ natural language requirements to the participants and request them to derive\
    \ a domain model for each of them. Figure [5](#page-14-0) visualizes the expected\
    \ domain model for the requirement \"Every research object is represented in a\
    \ JSON-LD format and stored in a document database if it contains a CC license.\"\
    \ which contains both of the two seeded quality defects. These defects result\
    \ in the following challenges according to literature [\\[94\\]](#page-48-5):\n\
    \n1. The verb in passive voice omits an important entity of the requirement; i.e.,\
    \ that the data processing unit stores the research object in a document database\
    \ (Label 1 in Figure [5\\)](#page-14-0).\n\n<span id=\"page-14-0\"></span>![](_page_14_Figure_1.jpeg)\n\
    \nFig. 5: Domain modeling task example for requirement 4.\n\n2. The ambiguous\
    \ pronoun \"it\" can syntactically be connected to several preceding noun phrases\
    \ (\"Every research object\", \"JSON-LD format\", and \"a document database\"\
    \ or the implicit \"Data Processing Unit\") by a reader but semantically only\
    \ applies to the research object (Label 2 in Figure [5\\)](#page-14-0).\n\nThe\
    \ goal of the experimental task is to derive a semantically correct domain model\
    \ from the natural language requirement which includes identifying all entities\
    \ (including the implicit ones) and connecting these entities correctly (including\
    \ those derived from syntactically vague associations).\n\nThe selection of dependent\
    \ variables was driven by the activity-based requirements quality theory [\\[37,](#page-45-0)\
    \ [48\\]](#page-46-0). Accordingly, requirements quality is measured by the effect\
    \ that quality factors have on the relevant attributes of requirements-dependent\
    \ activity. We selected the following dependent variables representing the relevant\
    \ attributes of the domain-modeling activity with the given motivation:\n\n- Duration:\
    \ the longer the domain modeling task takes, the more expensive it is.\n- Number\
    \ of missing entities: entities missing from the domain model produce potential\
    \ cost for failing to involve the respective actor or object.\n- Number of superfluous\
    \ entities: entities added to the domain model but not implied by the requirement\
    \ unnecessarily constrain the solution space.\n- Number of missing associations:\
    \ associations missing from the domain model produce a potential cost for failing\
    \ to identify a dependency between two entities.\n\n– Number of wrong associations:\
    \ associations connecting two entities that establish an unnecessary dependency\
    \ between them while neglecting an actual dependency.\n\nWe characterize the domain\
    \ modeling activity in terms of immediacy (duration), completeness (missing entities\
    \ and associations), conciseness (superfluous entities), and correctness (wrong\
    \ associations).\n\n# 3.4.2 Hypotheses\n\nThe three independent variables ind\
    \ ∈ {P V, AP,PVAP} (passive voice, ambiguous pronoun, and the coexistence of passive\
    \ voice and ambiguous pronoun) and the five dependent variables dep ∈ {D, E−,\
    \ E+, A−, A<sup>×</sup>} (duration, missing entities, superfluous entities, missing\
    \ associations, wrong associations) define our 15 null hypotheses as follows.\n\
    \n$$\\sum\\_{i \\text{ind} \\in \\{PV, AP, PVAP\\}} \\sum\\_{dep \\in \\{D, E^-,\
    \ E^+, A^-, A^\\times\\}} H\\_0^{ind \\to dep}$$\n\n\"There is no difference in\
    \ {dep} of the domain models based on requirements specifications containing no\
    \ quality defect and requirements specifications containing {ind}.\"\n\nTo capture\
    \ the context of the experiment, we collected factors based on related work [\\\
    [65,](#page-47-3) [81,](#page-48-1) [89\\]](#page-48-13), including the experience\
    \ of a practitioner regarding software and requirements engineering, but also\
    \ in SE roles and in the modeling task itself. Additionally, we assume that the\
    \ practitioners' education and domain knowledge influence the dependent variables.\
    \ Table [1](#page-16-0) summarizes the variables involved in this study.\n\nThe\
    \ variables in Table [1](#page-16-0) do not include a participant type that distinguishes\
    \ students from practitioners. While including such a variable is common practice\
    \ in SE research [\\[10\\]](#page-44-14), meta-research on the eligibility of\
    \ students as experiment participants suggests that the labels student or practitioner\
    \ are merely a proxy for levels of more meaningful factors like domain knowledge\
    \ and experience [\\[97\\]](#page-48-17). Additionally, the line between students\
    \ and practitioners becomes increasingly blurred as students more commonly gather\
    \ industrial experience before or during their studies [\\[18\\]](#page-45-15).\
    \ Consequently, we subsume the participant type variable by the causally more\
    \ meaningful and fine-grained variables of experience, education, domain knowledge,\
    \ and formal modeling training. We compared two models—one using the binary distinction\
    \ and one using the more fine-grained variables—and determined that the latter\
    \ outperforms the former in predictive power, even though only slightly. This\
    \ confirms to us that the variables we used are at least as expressive as the\
    \ binary participant type variable.\n\n# 3.4.3 Experimental Design\n\nOur experimental\
    \ design includes one factor (RQD) representing the alleged quality defect seeded\
    \ in a requirements specification. This main factor con-\n\n| dependent).   |\n\
    |---------------|\n| and           |\n| context,      |\n| (independent, |\n|\
    \               |\n| study<br>the  |\n| of            |\n|               |\n|\
    \ Variables     |\n| 1:            |\n| Table         |\n\n<span id=\"page-16-0\"\
    ></span>\n\n| Variable                              | Name             | Type\
    \ | Description                                                              \
    \                                                                            \
    \                                  | type<br>Data | Range                    \
    \                                                                            \
    \                            |\n|---------------------------------------|------------------|------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------|----------------------------------------------------------------------------------------------------------------------------------|\n\
    | Defect<br>Requirements<br>Quality     | RQD              | ind  | ambiguous<br>an<br>voice,<br>passive<br>in<br>verb<br>both<br>a<br>or<br>of<br>pronoun,<br>use<br>The\
    \                                                                            \
    \      | categorical  | PVAP}<br>AP,<br>PV,<br>{none,                        \
    \                                                                            |\n\
    | in<br>Experience<br>SE                | exp.se           | con  | engineering<br>software<br>in<br>experience<br>of<br>Years\
    \                                                                            \
    \                                                 | count        | N         \
    \                                                                            \
    \                                           |\n| in<br>Experience<br>RE      \
    \          | exp.re           | con  | engineering<br>requirements<br>in<br>experience<br>of<br>Years\
    \                                                                            \
    \                                             | count        | N             \
    \                                                                            \
    \                                       |\n| Education                       \
    \      | edu              | con  | degree<br>acquired<br>Highest             \
    \                                                                            \
    \                                                                 | ordinal  \
    \    | product<br>Ph.D.}<br>M.Sc.,<br>engineer,<br>B.Sc.,<br>{requirements<br>School,<br>{High\
    \                                          |\n| role<br>Primary              \
    \         | role             | con  | profes<br>of<br>years<br>most<br>the<br>with<br>experience<br>Role<br>SE-related<br>sional\
    \                                                                            \
    \                 | categorical  | developer,<br>man<br>trainer,<br>architect,<br>engineer,<br>none}<br>software<br>quality<br>other,<br>owner,<br>tester,<br>ager,\
    \ |\n| experi<br>Task<br>ence                | exp.task         | con  | modeling<br>domain<br>of<br>task<br>the<br>with<br>Experience\
    \                                                                            \
    \                                              | ordinal      | of<br>time,<br>to<br>time<br>from<br>rarely,<br>{never,<br>ten}\
    \                                                                  |\n| model<br>training<br>Formal<br>ing\
    \    | formal           | con  | modeling<br>domain<br>in<br>training<br>Formal\
    \                                                                            \
    \                                                             | categorical  |\
    \ {true,false}                                                               \
    \                                                      |\n| {domain}<br>knowledge<br>Domain<br>in\
    \ | {domain}<br>dom. | con  | aeronau-<br>{telemetry,<br>science}<br>∈<br>domain<br>open<br>the<br>databases,<br>of<br>Knowledge<br>tics,\
    \                                                                            |\
    \ ordinal      | {1,2,3,4,5}                                                 \
    \                                                                     |\n| experi<br>Tool<br>ence\
    \                | tool             | con  | modeling<br>for<br>Docs<br>Google<br>using<br>with<br>Experience\
    \                                                                            \
    \                                           | ordinal      | often}<br>time,<br>to<br>time<br>from<br>rarely,<br>{none,\
    \                                                                       |\n| Duration\
    \                              | D                | dep  | com<br>requirement<br>to<br>participant<br>one<br>on<br>the<br>task<br>took<br>experimental<br>it<br>minutes<br>of<br>the<br>Number<br>plete\
    \                                           | count        | N               \
    \                                                                            \
    \                                     |\n| entities<br>Missing               \
    \    | E−               | dep  | sub<br>the<br>from<br>missing<br>entities<br>model<br>relevant<br>domain<br>of<br>Number<br>mitted\
    \                                                                            \
    \         | count        | Eexpected]<br>[0,                                 \
    \                                                                            \
    \   |\n| en<br>Superfluous<br>tities           | +<br>E           | dep  | the<br>in<br>included<br>entities<br>model<br>relevant<br>domain<br>not<br>of<br>submitted<br>Number\
    \                                                                            \
    \       | count        | N                                                   \
    \                                                                            \
    \ |\n| associ<br>Missing<br>ations           | A−               | dep  | submit<br>the<br>from<br>missing<br>associations<br>model<br>domain<br>of<br>Number<br>ted\
    \                                                                            \
    \                 | count        | Aexpected]<br>[0,                         \
    \                                                                            \
    \           |\n| associa<br>Wrong<br>tions             | A×               | dep\
    \  | or<br>implied<br>source<br>than<br>the<br>entity<br>either<br>different<br>where<br>associations<br>a<br>is<br>requirement<br>edge<br>the<br>of<br>of<br>Number<br>the<br>target<br>by\
    \ | count        | Afound]<br>[0,                                            \
    \                                                                       |\n\n\
    <span id=\"page-17-0\"></span>\n\n| Variable    | Description                \
    \    | Data type   | Range                  |\n|-------------|--------------------------------|-------------|------------------------|\n\
    | Period      | Index of the experimental pe   | ordinal     | [1; 4]        \
    \         |\n|             | riod in which the data was ob  |             |  \
    \                      |\n|             | tained                         |   \
    \          |                        |\n| Sequence    | Order in which a participants\
    \  | categorical | {1234, 1243, , 4321}   |\n|             | received the treatments\
    \        |             |                        |\n| Carryover   | Interaction\
    \ between the period | categorical | {1 × none, 1 × P V , , |\n| effect      |\
    \ and the treatment              |             | 4 × P V AP }           |\n| Subject\
    \     | Index of a participant         | categorical | {1, 2, , 25}          \
    \ |\n| variability |                                |             |          \
    \              |\n\nTable 2: Variables of the study (experimental design factors)\n\
    \ntains four treatments: a control one (no defects) and three experimental ones\
    \ (passive voice (PV), ambiguous pronoun (AP), and both (PVAP)).\n\nGiven our\
    \ sampling strategy involving industry practitioners, which are difficult to recruit\
    \ for controlled experiments [\\[102\\]](#page-49-12), we anticipated a moderate\
    \ sample size of participants. Consequently, we opted for a crossover design [\\\
    [107\\]](#page-49-11) instead of a parallel design, i.e., we apply every treatment\
    \ to all subjects instead of distributing the subjects among the treatments. Previously,\
    \ Kitchenham et al. advised against the use of crossover designs [\\[70\\]](#page-47-15).\
    \ Mainly, the validity of crossover design experiments is challenged by the following\
    \ confounding factors:\n\n- 1. the period in which a treatment is applied to a\
    \ subject, as certain periods may influence the dependent variables (e.g., participants\
    \ may mature and perform increasingly better the more often they perform the experimental\
    \ task subsequently);\n- 2. the sequence in which the treatments are applied to\
    \ a subject, as certain sequences may have a beneficial effect on a dependent\
    \ variable (e.g., there might be an optimal sequence to apply the treatments in);\n\
    - 3. the effect from a previous treatment may carry over to the period when applying\
    \ a subsequent treatment [\\[107\\]](#page-49-11); and\n- 4. the subject variability,\
    \ as software engineering tasks are highly dependent on the skill of involved\
    \ individuals [\\[92\\]](#page-48-18).\n\nHowever, recent adoptions of best practices\
    \ from other disciplines made this design applicable to SE research without compromising\
    \ the validity of the results [\\[51,](#page-46-13) [107\\]](#page-49-11). The\
    \ threats to validity can be mitigated during design and analysis [\\[107\\]](#page-49-11)\
    \ by (1) randomizing the order of treatments and (2) including the independent\
    \ variables' period, sequence, the interaction between them (representing the\
    \ carryover effect), and subject variability in the analysis. Consequently, we\
    \ consider the experimental design variables listed in Table [2](#page-17-0) in\
    \ addition to the variables listed in Table [1.](#page-16-0)\n\nWhen controlling\
    \ the threats to validity, the crossover design provides two benefits. Firstly,\
    \ it requires fewer participants, as an experiment with n<sup>p</sup> participants\
    \ and n<sup>t</sup> treatments yields np×n<sup>t</sup> observations instead of\
    \ only n<sup>p</sup> [\\[113\\]](#page-49-10). Secondly, it accounts for subject\
    \ variability, as the dependent variables can be measured in relation to the average\
    \ response of each subject instead of the average response of each treatment group\
    \ [\\[70\\]](#page-47-15). Therefore, each subject acts as its own control and\
    \ mitigates within-subject variability.\n\nEach experimental session contained\
    \ four main periods in which we applied one treatment to the subject. We randomized\
    \ the order of treatment application to disperse the confounding sequence and\
    \ carryover effect [\\[14,](#page-44-15) [107\\]](#page-49-11). This resulted\
    \ in 24 unique sequences of treatment application (nt! = 24) and, consequently,\
    \ 24 experimental groups. The experiment was single-blinded—i.e., the participants\
    \ did not know the requirements' sequence, but the researchers did.\n\n# <span\
    \ id=\"page-18-0\"></span>3.4.4 Objects\n\nThe experimental object consisted of\
    \ four English, single-sentence NL requirements specifications R1-R4. An additional\
    \ warm-up object (R0) preceded the actual experimental objects, adding a fifth\
    \ experimental period to each session. It was only used to familiarize the participants\
    \ with the experimental task and tool and was not considered in the data analysis.\
    \ The four experimental objects were manually seeded with defects corresponding\
    \ to our four treatments: one requirement containing none of the two faults, one\
    \ containing a verb in passive voice, one containing an ambiguous pronoun, and\
    \ one containing both a verb in passive voice and an ambiguous pronoun. The requirements'\
    \ mean length is 17.8 words (sd=4).\n\nThe first author derived the experimental\
    \ objects from the requirements specification of the Mars Express ASPERA-3 Processing\
    \ and Archiving Facility (APAF), a real-world specification from the PuRE data\
    \ set [\\[41\\]](#page-46-12). From this requirements specification, the first\
    \ author selected five single-sentence natural language requirements and modified\
    \ them to ensure two defect-free requirements (one warm-up object R<sup>0</sup>\
    \ and one for the defect-free baseline R1) and three objects with the respective\
    \ defects (R2-R4). The second author reviewed and adjusted the selected objects.\n\
    \n# <span id=\"page-18-1\"></span>3.4.5 Subjects\n\nThe target population of interest\
    \ consists of people involved in software engineering who work with requirements\
    \ specifications. We used a non-probability sampling approach based on a mix of\
    \ purposive and convenience sampling [\\[3\\]](#page-44-12). In particular, we\
    \ wanted to select participants who are diverse in terms of the context variables\
    \ as determined in Table [1,](#page-16-0) including their experience, education,\
    \ and software engineering roles. We approached both students participating in\
    \ RE courses at our respective institutions and practitioners in our collaborators'\
    \ network to purposefully diversify the experience and education of our sample.\
    \ For all other demographic context factors (e.g., SE roles) we had to rely on\
    \ convenience sampling.\n\nWe approached 52 potential candidates (32 practitioners\
    \ and 20 students), and 27 candidates (19 & 8) agreed to participate in the experiment\
    \ (response\n\n<span id=\"page-19-0\"></span>![](_page_19_Figure_1.jpeg)\n\nFig.\
    \ 6: Distribution of SE and RE experience.\n\n<span id=\"page-19-1\"></span>![](_page_19_Figure_3.jpeg)\n\
    \nFig. 7: Distribution of knowledge in the four relevant domains.\n\nrate of 52%).\
    \ Two students did not show up to the agreed time slot. Our final sample includes\
    \ 19 practitioners from six different companies and six students from two universities.\
    \ Participation in the experiment was entirely voluntary and not compensated.\
    \ The number of participants (n<sup>p</sup> = 25) exceeded the number of experimental\
    \ sequences (nt! = 24) such that we had at least one subject per experimental\
    \ group and evenly dispersed any confounding sequence or carryover effect [\\\
    [107\\]](#page-49-11).\n\nFigure [6](#page-19-0) shows the distribution of experiment\
    \ subjects' experience in SE and RE in years. which is widespread in our sample.\
    \ Among the 25 participants, the reported primary roles are developer (10), architect\
    \ (6), requirements engineer (5), manager (1), and no prior professional role\
    \ (3). Students who have not yet had a professional software engineering role\
    \ constitute this last group. The distribution covers many SE-relevant roles but\
    \ excludes others, such as testers or product owners.\n\nAmong the 25 participants,\
    \ 5 reported a high school degree as their level of education, 8 a Bachelor's\
    \ degree, and 12 a Master's degree. No participant reported a Ph.D. degree as\
    \ their highest degree of education. Figure [7](#page-19-1) visualizes the distribution\
    \ of the participant's experience in the four domains[2](#page-19-2) that contribute\
    \ semantic knowledge to understanding the requirements: aeronautics, telemetry,\
    \ databases, and open source. For the latter two, results are balanced across\
    \ the different knowledge levels, while the former two confirm our assumption\
    \ that all participants had a low-level knowledge of aeronautics and telemetry\
    \ systems.\n\nThe experiment tool—the Google Draw plugin within the Google Document\
    \ was unknown to most (18 never used it, 6 rarely, and 1 from time to time).\n\
    \n<span id=\"page-19-2\"></span><sup>2</sup> Domain does not exclusively mean\
    \ application domain, but rather any coherent ontology related to a specific topic.\n\
    \nA total of 16 participants (64%) report having received a form of training in\
    \ the modeling activity. The modeling experience (never: 1, rarely: 11, from time\
    \ to time: 10, often: 3) resembles a normal distribution. We did not discard data\
    \ from participants who reported having no modeling experience or formal training\
    \ in modeling given that our experiment included both comprehensive instructions\
    \ and a warm-up phase as described in Section [3.4.4.](#page-18-0)\n\nGiven the\
    \ distribution of responses in these context variables, we disqualified the following\
    \ predictors: aeronautics domain knowledge, telemetry domain knowledge, and experience\
    \ with the experiment tool. These variables are not sufficiently distributed in\
    \ our sample of study participants, i.e., several categories of these variables\
    \ are underrepresented. Consequently, they are unable to effectively block the\
    \ influence of that variable on the dependent variable [\\[113\\]](#page-49-10).\n\
    \n# 3.4.6 Instrumentation\n\nWe used a Google Docs document[3](#page-20-0) for\
    \ the task and a Google Form questionnaire[4](#page-20-1) to collect demographic\
    \ information. The Google Docs document lent itself to the task due to its accessibility\
    \ and its simple modeling tool with the embedded Google Drawings. The modeling\
    \ tool represented the optimal trade-off between complexity—as neither previous\
    \ knowledge nor additional software was necessary to conduct the experimental\
    \ task—and suitability—as it contains all elements relevant to the domain modeling\
    \ task (i.e., nodes for entities and edges for associations). This main study\
    \ document explained the experimental task, an example of the domain modeling\
    \ task, and a short context description of the system from its original requirements\
    \ specification [\\[41\\]](#page-46-12).\n\nWe created a survey questionnaire\
    \ to collect demographic information relevant to the experiment using Google Forms.\
    \ All participants could answer the survey only after completing the task to avoid\
    \ fatiguing effects. At the beginning of the questionnaire, participants entered\
    \ their assigned participant ID (PID), such that we could connect their response\
    \ to the experimental task to their response to the questionnaire without storing\
    \ any personal data. The questions were designed to collect all relevant independent\
    \ variables listed in Table [1.](#page-16-0)\n\nWe piloted the experiment in a\
    \ session with two Ph.D. students in SE. We clarified the instruction text and\
    \ task descriptions based on the collected feedback.\n\n# 3.4.7 Data Collection\
    \ Procedure\n\nWe scheduled a one-hour session according to the availability of\
    \ the participants. Because of differing schedules and time zones, we scheduled\
    \ 16 sessions with up to three participants simultaneously. We conducted the sessions\
    \ between 2023-04-03 and 2023-04-17.\n\n<span id=\"page-20-0\"></span><sup>3</sup>\
    \ <https://www.google.de/intl/en/docs/about/>\n\n<span id=\"page-20-1\"></span><sup>4</sup>\
    \ <https://www.google.com/forms/about/>\n\nEach session started with the first\
    \ author explaining the general procedure of the experiment and obtaining consent\
    \ to evaluate and disclose the anonymized data. No participant refused this consent\
    \ and all data points could be included in the data evaluation procedure. Then,\
    \ participants were instructed to read the prepared document in order and complete\
    \ the contained tasks. The document contained all descriptions of the task such\
    \ that all participants received the same instructions. The first author oversaw\
    \ all sessions to address technical difficulties and recorded the minutes each\
    \ participant spent per period. Ten minutes were estimated per period, but participants\
    \ were free to allocate their time. In case participants took longer than the\
    \ scheduled one hour, they completed the task in as much time as they required.\
    \ Once the task was complete, participants also filled in the questionnaire to\
    \ provide demographic information on context variables.\n\n# 3.4.8 Data Preparation\n\
    \nTo evaluate the collected data, we created a code book that characterizes issues\
    \ in domain models. We developed detection rules for each dependent variable of\
    \ the resulting product—i.e., missing entity, superfluous entity, missing association,\
    \ and wrong association—and summarized them in a guideline (available in our replication\
    \ package [\\[46\\]](#page-46-3)). Then, the first author manually evaluated the\
    \ resulting domain models of each participant using this guideline and recorded\
    \ all detected issues.\n\nThe result of the coding process was a table where one\
    \ row represents the evaluation of one domain model. Given n<sup>p</sup> = 25\
    \ participants and n<sup>r</sup> = 4 requirements, we ended up with n<sup>p</sup>\
    \ × n<sup>r</sup> = 100 data points. Each data point contained the number of issues\
    \ of each of the four types that occurred in the respective domain model. Finally,\
    \ we standardized numerical variables in the demographic data for easier processing.\n\
    \nTo assess the reliability of the rating, the fourth author of the paper independently\
    \ recorded issues of three randomly selected participant responses, yielding an\
    \ overlap of twelve ratings. Since each domain model can contain an arbitrary\
    \ number of issues of each type, but our dependent variables only model the number\
    \ of times that an issue type occurred, we consider each rating of a domain model\
    \ as a vector of dimensions equal to the number of issue types. We then calculated\
    \ the inter-rater agreement of the same domain model using the Spearman rank correlation\
    \ between the vectors. The average cosine similarity is 77.0% and represents substantial\
    \ agreement. The two raters discussed the remaining disagreement and concluded\
    \ that they represented acceptable variance in the interpretation of participants'\
    \ responses.\n\n# 3.4.9 Frequentist Data Analysis\n\nWe performed a frequentist\
    \ data analysis of the experimental data as in the original experiment [\\[36\\\
    ]](#page-45-2). Since the factor is categorical, all dependent variables are continuous,\
    \ and our samples are dependent, our statistical method of choice falls between\
    \ the parametric paired t-test [\\[62\\]](#page-47-16) or the non-parametric Wilcoxon\
    \ signed-rank test [\\[112\\]](#page-49-13) based on the distribution of the variables,\
    \ which we evaluated using the Shapiro-Wilk test results [\\[99\\]](#page-48-19).\n\
    \nWe reject a null hypothesis if the resulting p-value of a two-tailed statistical\
    \ test is lower than the significance level α. To account for type I errors when\
    \ performing multiple hypotheses tests targeting the same independent variable,\
    \ we apply the Bonferroni correction [\\[6\\]](#page-44-13): we considered α ′\
    \ = α <sup>m</sup> where α = 0.05 and m is the number of hypotheses tested for\
    \ each value of the independent variable. For our five families of hypotheses\
    \ α ′ = 0.05 <sup>5</sup> = 0.01.\n\nAdditionally, we report the effect size [\\\
    [28\\]](#page-45-16) using Cohen's D for the paired student t-test [\\[24\\]](#page-45-17)\
    \ and the matched-pairs rank biserial correlation coefficient for Wilcoxon signed-rank\
    \ test [\\[69\\]](#page-47-17).\n\n# <span id=\"page-22-0\"></span>3.4.10 Bayesian\
    \ Data Analysis\n\nWe apply Bayesian data analysis with Pearl's framework for\
    \ causal inference [\\[88\\]](#page-48-15) to complement the frequentist data\
    \ analysis [\\[52\\]](#page-46-2). Given the limited adoption of Bayesian data\
    \ analysis in software engineering research [\\[52](#page-46-2)[–54\\]](#page-46-14),\
    \ we complement this method section with a running example for understandability.\
    \ In this running example, we illustrate the methodological steps of Bayesian\
    \ data analysis for the hypothesis that requirements quality defects influence\
    \ the number of wrong associations in a resulting domain model.\n\nThree steps\
    \ [\\[101\\]](#page-49-14), which are the major steps of Pearl's original model\
    \ of causal statistical inference [\\[88\\]](#page-48-15), comprise the analysis.\
    \ We explain each step in the following paragraphs.\n\nModeling In the modeling\
    \ step, we make our causal assumptions about the underlying effect explicit in\
    \ a graphical causal model. The graphical causal model takes the form of a directed\
    \ acyclic graph (DAG) in which nodes represent variables and directed edges represent\
    \ causal effects [\\[29\\]](#page-45-13). Our DAG contains four groups of variables:\n\
    \n- 1. Treatment: The independent variable that represents the requirements quality\
    \ defect present in the requirement.\n- 2. Context factors: The independent variables\
    \ that represent the properties of the participants.\n- 3. Experimental design\
    \ factors: The independent variables that represent all factors of the crossover\
    \ experiment design influencing the response variables [\\[107\\]](#page-49-11).\n\
    - 4. Response variables: The dependent variables.\n\nThe effect of the treatments\
    \ on the response variables is the subject of the analysis. By including both\
    \ context and confounding factors, their influence is factored out from the treatments'\
    \ causal effect on the response variables. Consequently, the effect of interest\
    \ can be isolated from any confounding factor included in the DAG. We assume causal\
    \ relations—represented by edges in the DAG—between every independent (treatment,\
    \ context, and confounding) and\n\n<span id=\"page-23-0\"></span>![](_page_23_Figure_1.jpeg)\n\
    \nFig. 8: DAG for the analysis of wrong associations\n\nthe dependent variables.\
    \ Additionally, independent variables may influence other independent variables.\n\
    \nFigure [8](#page-23-0) shows the DAG of the running example. The treatment,\
    \ response variable, and context factors correspond to the study variables as\
    \ outlined in Table [1.](#page-16-0) The experimental design variables correspond\
    \ to the factors listed in Table [2.](#page-17-0) The experimental period blocks\
    \ the learning effect, i.e., the influence on the response variable caused by\
    \ repeatedly performing the task. The duration blocks the time effect, i.e., the\
    \ influence on the response variable caused by the amount of time that a participant\
    \ took for each instance of the task. Note that the factor tool experience listed\
    \ in Table [1](#page-16-0) is missing from the DAG since we excluded it as explained\
    \ in Section [3.4.5.](#page-18-1) Note that the factor sequence listed in Table\
    \ [2](#page-17-0) is missing from the DAG since it is confounded with subject\
    \ variability (further explained in Section [5.4.1\\)](#page-40-0).\n\nThe DAG\
    \ does not visualize the interaction effects we assume between two independent\
    \ variables. An interaction effect occurs when the influence of one independent\
    \ variable on the dependent variable depends on the value of another independent\
    \ variable [\\[76\\]](#page-47-4). Visualizations of interaction effects in DAGs\
    \ have been proposed [\\[82\\]](#page-48-20) but are not common practice. In the\
    \ running example, we assume two interaction effects via the following hypotheses:\n\
    \n- 1. requirements quality \\* domain knowledge: domain knowledge can compensate\
    \ the effect of requirements quality defects [\\[93\\]](#page-48-8)\n- 2. requirements\
    \ quality \\* period carryover effect [\\[107\\]](#page-49-11): the effect of\
    \ a treatment may be influenced by the treatments applied in previous periods\n\
    \nIdentification Including an independent variable Z that has an assumed causal\
    \ effect on both the treatment X (i.e., Z → X) and the outcome Y (i.e., Z → Y\
    \ ) opens a non-causal path (i.e., X ← Z → Y ) from the treatment to the outcome\
    \ [\\[87\\]](#page-48-14). This so-called backdoor path introduces spurious associations.\
    \ Consequently, blindly moving forward with all variables may harm the causal\
    \ analysis. Instead, the so-called adjustment set of variables needs to be selected\
    \ [\\[76\\]](#page-47-4) in the identification step. A series of four criteria\
    \ [\\[76\\]](#page-47-4) allows to make an informed selection of variables to\
    \ include in the final estimation step. This way, we avoid variable bias like\
    \ colliders which confound the causal effect between the treatment and the response\
    \ variable.\n\nIn the running example, we assume the following causal relation\
    \ between independent variables. The more experience a participant has in SE or\
    \ RE, the more likely it is that they have acquired respective domain knowledge\
    \ (experience in SE/RE → domain knowledge). We need to consider this relationship\
    \ in the next step to avoid attributing impact to the wrong independent variable.\
    \ For instance, in the running example, we need to distinguish whether experience\
    \ in SE/RE has a direct influence on wrong association or whether it just influences\
    \ domain knowledge, which influences the response variable.\n\nBecause we employ\
    \ an experiment as our research method and fully control the treatment, there\
    \ is no influence of any other independent variable on the treatment variable.\n\
    \nEstimation In the estimation step, we perform a regression analysis. The regression\
    \ analysis results in estimates of the response variable depending on the values\
    \ of the independent variables. The result of the regression analysis is a Bayesian\
    \ model trained with empirical data. The model provides the magnitude and sign\
    \ of the effect that each independent variable has on the dependent response variable.\n\
    \nThe estimation step begins by selecting a distribution type (likelihood) that\
    \ represents the dependent response variable [\\[56\\]](#page-46-9). We select\
    \ the distribution type based on the maximum entropy criterion [\\[63\\]](#page-47-18)\
    \ and ontological assumptions. This means we select the least restrictive distribution\
    \ that fulfills all ontological assumptions about the variables' properties.\n\
    \nIn our running example, the response variable is a count of wrong associations\
    \ in a domain model. Consequently, the distribution must be discrete and only\
    \ allow positive numbers or zero. Additionally, the response variable is bounded\
    \ by the number of expected associations of the domain model, i.e., the number\
    \ of associations in the sample solution, since a participant can only connect\
    \ as many associations wrongly in the model as there were associations expected.\
    \ Any associations added beyond the expected associations count as superfluous\
    \ associations, a different response variable. Consequently, we represent the\
    \ response variable with a Binomial distribution. The following formula encodes\
    \ that the number of wrong associations in one domain model i (E × i ) is distributed\
    \ as a Binomial distribution with the number of trials equal to the number of\
    \ expected associations (E) and a probability p<sup>i</sup> ∈ [0, 1] of getting\
    \ one association wrong.\n\n#### E × <sup>i</sup> ∼ Binomial(E, pi)\n\nThis formula\
    \ assumes that the event—connecting one association wrong is independent, i.e.,\
    \ one wrong association does not influence the success of any other association.\n\
    \nIn the next step, we define the parameter that determines the response variable\
    \ distribution (in the running example: pi) in relation to the predictors selected\
    \ in the identification step. The following formula shows a simplified version\
    \ of this parameter definition (excluding most of the previously mentioned predictors\
    \ in Table [1](#page-16-0) for brevity).\n\n$$logit(p\\_i) = \\alpha + \\alpha\\\
    _{PID} + \\beta\\_{RQD}^T \\times RQD\\_i + \\beta\\_{SE} \\times exp.se\\_i$$\n\
    \nThe logit operator scales the parameter p<sup>i</sup> to a range of [0, 1] since\
    \ the probability parameter of the Binomial distribution only accepts this range\
    \ of values [\\[26,](#page-45-18) [76\\]](#page-47-4). The parameter p<sup>i</sup>\
    \ is, in this example, determined by the following predictors:\n\n- 1. Intercept\
    \ (α): the grand mean of connecting an association wrongly, i.e., the baseline\
    \ challenge of getting an association wrong.\n- 2. Group-level intercept (αP ID,\
    \ where the results of one participant represent one group): the participant-specific\
    \ mean of connecting an association wrongly, i.e., the within-subject variability\
    \ of response variables [\\[107\\]](#page-49-11) modeled via partial pooling [\\\
    [30\\]](#page-45-19)\n- 3. Treatment (RQDi): the influence of a requirements quality\
    \ defect on the probability of connecting an association wrong (as an offset from\
    \ the grand mean).\n- 4. Software Engineering Experience (exp.sei): the influence\
    \ of the subject's software engineering experience on the probability of connecting\
    \ an association wrong (as an offset from the grand mean).\n\nThe variables RQD<sup>i</sup>\
    \ and exp.se<sup>i</sup> contain the values recorded during instance i of conducting\
    \ the experimental task and are each prefixed with coefficients β T RQD and βSE.\
    \ These coefficients are Gaussian probability distributions that represent the\
    \ magnitude and direction of the influence that the variable values have on the\
    \ parameter p<sup>i</sup> and, therefore, on the distribution of the response\
    \ variable. The mean µ of the coefficient represents the average effect of the\
    \ variable on the parameter p<sup>i</sup> , and the standard deviation σ encodes\
    \ the variation around this average effect. A standard deviation of σ = 0 would\
    \ mean that the variable has a deterministic effect of strength µ on p<sup>i</sup>\
    \ and, therefore, the distribution of the response variable. In reality, this\
    \ is highly unrealistic. Hence, the standard deviation captures the uncertainty\
    \ of the effect of a variable on p<sup>i</sup> .\n\nSpecial cases of variables\
    \ are the intercepts α and αP ID, which are probability distributions without\
    \ any variable and, hence, represent the predictorindependent general and participant-specific\
    \ probability of connecting an association wrong. In the beginning, we assign\
    \ probability distributions spread\n\n<span id=\"page-26-0\"></span>![](_page_26_Figure_1.jpeg)\n\
    \nFig. 9: Predictive checks with prior and updated posterior coefficient distributions\n\
    \naround µ = 0 (β T RQD ∼ Normal(0, 0.5)), so-called uninformative priors, to\
    \ these coefficients. These distributions encode our prior beliefs about the influence\
    \ of the respective predictor, i.e., that it is yet unknown whether the predictor\
    \ has a positive (µ > 0) or negative (µ < 0) influence on p<sup>i</sup> . Only\
    \ where previous evidence for the impact of a predictor on the response variable\
    \ exists, we select more informative priors. For example, the experiment by Femmer\
    \ et al. [\\[36\\]](#page-45-2) indicates that missing entities and missing associations\
    \ are in general rare, which we represent in our priors by selecting α ∼ Normal(−1,\
    \ 0.5) for the intercept.\n\nWe assess the feasibility of the selected prior distributions\
    \ via prior predictive checks [\\[111\\]](#page-49-15). During this check, we\
    \ sample only from the priors, i.e., we predict the response variable given the\
    \ recorded data of independent variables and the prior probability distributions\
    \ of the predictor coefficients. Figure [9](#page-26-0) visualizes the result\
    \ of the prior predictive check. The grey bars represent the actual observed distribution\
    \ of the response variable. For example, 75 domain models contained zero wrong\
    \ associations. The distribution of predicted values for the response variable\
    \ (cyan whisker plots) encompasses the actual observed distribution of the response\
    \ variable. This confirms that the actually observed distribution is approximately\
    \ determined by the uninformative prior distributions.\n\nUpon confirmation of\
    \ the priors' feasibility, we train the Bayesian models with the data recorded\
    \ during the experiment. We conducted the analysis using the brms library [\\\
    [16\\]](#page-44-10) in R. Hamiltonian Monte Carlo Markov Chains (MCMC) [\\[13\\\
    ]](#page-44-8) update the coefficient distributions based on the empirical data.\
    \ During this process, the parameters of the coefficient distributions are adjusted\
    \ to better reflect the response variable based on the predictor variable values.\n\
    \nAfter the training process, we perform posterior predictive checks, which work\
    \ similarly to the prior predictive check but use the updated posterior coefficient\
    \ distributions instead of the prior distributions. Figure [9](#page-26-0) also\
    \ visualizes the posterior predictive check for the running example. The distribution\
    \ of the predicted values (red whisker plots) still encompasses the actually observed\
    \ distribution of the response variable but has narrowed around these values.\
    \ This indicates that the posterior distributions encode the influence of the\
    \ predictor variables more accurately than the prior distributions, i.e., that\
    \ the model has successfully gained predictive power during the training process.\n\
    \nTo overcome the problem mentioned in the identification step, i.e., attributing\
    \ impact to the wrong predictor, we train additional models per response variable\
    \ to test for conditional independence [\\[76\\]](#page-47-4). For example, to\
    \ determine the correct causal relationship between the two independent variables\
    \ experience in SE, domain knowledge, and the dependent variable, we train two\
    \ additional models where each one misses one of the two variables [\\[54\\]](#page-46-14).\
    \ After training, we compare the posterior distributions of the remaining parameter\
    \ coefficients. If a posterior distribution significantly moves from |µ| > 0 towards\
    \ µ = 0 when including a variable, then the response variable is independent of\
    \ that variable when conditioning on the included variable. The model does not\
    \ gain any further information from the variable with µ ≃ 0, and its causal relation\
    \ is disputed. If the posterior distribution does not deviate significantly when\
    \ including another variable, its causal impact is confirmed.\n\nFinally, we perform\
    \ a stratified posterior prediction to answer our research questions. To this\
    \ end, we construct a synthetic data set with four data points, one for each value\
    \ of the main factor variable (i.e., baseline, PV, AP, PVAP). We fix all other\
    \ independent variables at representative values—i.e., the mean for continuous\
    \ and the mode for discrete variables. Then, we sampled 6, 000 predictions for\
    \ each of the four data points. This isolates the effect of the treatment but\
    \ maintains the uncertainty of the influence of every independent variable encoded\
    \ in the standard deviation of every predictor coefficient and, hence, more accurately\
    \ describes the causal relationship between the treatment and the outcome. We\
    \ compare the 6, 000 predictions of each of the three treatments (PV, AP, PVAP)\
    \ with the 6, 000 predictions from the baseline (no defect) and count how often\
    \ the treatment causes a higher, equal, or lower outcome variable. We scale these\
    \ values to percentages to summarize the effect of the treatment on the outcome\
    \ variable. This evaluation avoids a point-wise reduction of the results and comparison\
    \ to an arbitrary significance level as customary in frequentist analyses [\\\
    [60\\]](#page-47-12). Rather than providing a binary answer to the hypotheses,\
    \ we present the more informative distribution of results. However, for the sake\
    \ of reporting, we consider the distribution of the duration variable skewed if\
    \ the two percentages differ from the mean (50%) by 10% each and consider the\
    \ other distributions skewed if the two percentages differ by 10% from each other.\n\
    \nAdditionally, we plot the marginal effect of selected independent variables\
    \ to visualize their isolated impact on the response variable. The isolated impact\
    \ reveals how context and confounding factors influence the response variable.\
    \ This includes visualizing the carryover effect, i.e., the interaction between\
    \ the treatment and the period.\n\n# <span id=\"page-28-0\"></span>4 Results\n\
    \nSection [4.1](#page-28-1) shows the results of the frequentist and Section [4.2](#page-29-0)\
    \ the results of the Bayesian data analysis. Section [4.3](#page-34-0) compares\
    \ the part of our results that contributes a conceptual replication to the original\
    \ study. Section [4.4](#page-35-0) compares the results from our FDA to the results\
    \ from our BDA.\n\n# <span id=\"page-28-1\"></span>4.1 Frequentist Data Analysis\n\
    \nTable [3](#page-28-2) shows the mean and median values of the response variables\
    \ similar to how they are reported by Femmer et al. [\\[36\\]](#page-45-2). Note\
    \ that the results are not directly comparable to those in the original study\
    \ as both our experimental objects and treatments varied.\n\n<span id=\"page-28-2\"\
    ></span>Table 3: Mean and median response variable values (reported as mean/median\
    \ in each cell)\n\n| Defect | Duration<br>D | Missing<br>Entities<br>E− | Superfluous<br>Entities<br>E+\
    \ | Missing<br>Associa<br>tions A− | Wrong As<br>sociations<br>A× |\n|--------|---------------|---------------------------|-------------------------------|--------------------------------|------------------------------|\n\
    | none   | 7.38/6.5      | 0.23/0                    | 0.5/0                 \
    \        | 0.38/0                         | 0.08/0                       |\n|\
    \ PV     | 6.88/7        | 0.81/1                    | 0.42/0                \
    \        | 0.81/1                         | 0.62/0                       |\n|\
    \ AP     | 7.12/6        | 1.23/1                    | 0.96/0.5              \
    \        | 1.12/1                         | 0.54/0.5                     |\n|\
    \ PVAP   | 7.96/7        | 1.27/1                    | 0.46/0                \
    \        | 1.38/1                         | 0.38/0                       |\n\n\
    Table [4](#page-29-1) lists the results of our frequentist data analysis and relates\
    \ them to the results from the original study [\\[36\\]](#page-45-2).\n\nThe frequentist\
    \ data analysis suggests rejecting the following hypotheses and, therefore, proposes\
    \ the following effects as statistically significant (with α ′ = 0.01):\n\n- 1.\
    \ HP V <sup>→</sup><sup>E</sup> − 0 : passive voice impacts the number of missing\
    \ entities\n- 2. HAP→<sup>E</sup> − 0 : ambiguous pronouns impacts the number\
    \ of missing entities\n- 3. HPVAP→<sup>E</sup> − 0 : the co-occurrence of passive\
    \ voice and ambiguous pronouns impacts the number of missing entities\n\n| Outcome\
    \               | Treatment |      | Original [36] |      |        |         \
    \          |       |\n|-----------------------|-----------|------|---------------|------|--------|-------------------|-------|\n\
    |                       |           | p    | CI            | ES   | p      | Replication<br>CI\
    \ | ES    |\n|                       | PV        |      |               |    \
    \  | 0.67   | (−0.4, 0.7)       | -0.13 |\n| Duration              | AP      \
    \  |      |               |      | 0.86   | (−0.7, 0.86)      | 0.01  |\n|   \
    \                    | PVAP      |      |               |      | 0.49   | (−0.5,\
    \ 0.25)      | 0.14  |\n| Missing               | PV        | 0.10 | (0, ∞)  \
    \      | 0.39 |        |                   |       |\n| Actors               \
    \ |           |      |               |      |        |                   |   \
    \    |\n| Missing               | PV        | 0.25 | (−1, ∞)       | 0.25 |  \
    \      |                   |       |\n| Objects               |           |  \
    \    |               |      |        |                   |       |\n| Missing\
    \               | PV        |      |               |      | ≪ 0.01 | (−1, 0) \
    \          | -0.79 |\n| Entities              | AP        |      |           \
    \    |      | ≪ 0.01 | (−1.5, 0)         | -0.93 |\n|                       |\
    \ PVAP      |      |               |      | ≪ 0.01 | (−2, 0)           | -0.81\
    \ |\n| Superfluous           | PV        |      |               |      | 0.64\
    \   | (−0.5, 0)         | 0.14  |\n| Entities              | AP        |     \
    \ |               |      | 0.19   | (−2.5, 0)         | -0.41 |\n|           \
    \            | PVAP      |      |               |      | 0.62   | (−1, 1)    \
    \       | 0.15  |\n| Missing               | PV        | 0.02 | (1, ∞)       \
    \ | 0.75 | 0.025  | (−1, 0)           | -0.58 |\n| Associations          | AP\
    \        |      |               |      | ≪ 0.01 | (−2, −1.5)        | -0.87 |\n\
    |                       | PVAP      |      |               |      | ≪ 0.01 | (−2,\
    \ −1)          | -0.84 |\n| Wrong<br>Associations | PV        |      |       \
    \        |      | 1.0    | (0, 0)            | 0.0   |\n|                    \
    \   | AP        |      |               |      | ≪ 0.01 | (−1, 0)           | -0.85\
    \ |\n|                       | PVAP      |      |               |      | 0.052\
    \  | (−1.5, 0)         | -0.67 |\n\n<span id=\"page-29-1\"></span>Table 4: Results\
    \ of frequentist analysis including the p-value of the hypothesis test (p), confidence\
    \ interval (CI), and effect size (ES). Statistically significant results in bold\
    \ (original study α = 0.05, this experiment α ′ = 0.01).\n\n- 4. HAP→<sup>A</sup>\
    \ − 0 : ambiguous pronouns impact the number of missing associations\n- 5. HPVAP→<sup>A</sup>\
    \ − 0 : the co-occurrence of passive voice and ambiguous pronouns impacts the\
    \ number of missing associations\n\n6. HAP→<sup>A</sup> × 0 : ambiguous pronouns\
    \ impact the number of wrong associations The associated effect size is considered\
    \ large [\\[23\\]](#page-45-20) in all cases.\n\n# <span id=\"page-29-0\"></span>4.2\
    \ Bayesian Data Analysis\n\nThis section follows the methodology described in\
    \ Section [3.4.10](#page-22-0) by presenting the DAG in Section [4.2.1,](#page-29-2)\
    \ posterior predictions in Section [4.2.2,](#page-31-0) and marginal plots in\
    \ Section [4.2.3.](#page-32-0)\n\n# <span id=\"page-29-2\"></span>4.2.1 Causal\
    \ Model and Adjustment Set\n\nFigure [10](#page-30-0) visualizes the DAG that\
    \ graphically models our causal assumptions. It is an extension of Figure [8,](#page-23-0)\
    \ the running example, including all five dependent variables. To preserve the\
    \ readability of the DAG, we introduce a distributor node. This node substitutes\
    \ the connections from the source of every incoming edge to the target of every\
    \ outgoing edge.\n\nThe edges represent the same causal reasoning as presented\
    \ in the running example in Section [3.4.10.](#page-22-0) We assume that all independent\
    \ variables (treatments, context factors, and experimental design factors) have\
    \ an impact on\n\n<span id=\"page-30-0\"></span>![](_page_30_Figure_1.jpeg)\n\n\
    Fig. 10: Directed acyclic graph visualizing all causal assumptions\n\nall dependent\
    \ variables. The impact of the requirements quality defect (the three treatments)\
    \ is the relationship of interest in our analyses. In addition to the already\
    \ described impact of experience in SE on domain knowledge, we assume the following\
    \ causal relationships:\n\n- 1. Duration impacts all other dependent variables:\
    \ Since we did not constrain the time for each period, different amounts of minutes\
    \ taken for each object may influence the results. Taking a longer time for one\
    \ domain model may reduce the amount of defects in the final model.\n- 2. Missing\
    \ entities impact missing associations: If an entity is missing from the domain\
    \ model, any association involving that entity will also be missing (as already\
    \ supported by our re-analysis [\\[47\\]](#page-46-11)).\n\nEqually notable are\
    \ the non-existing associations between nodes, especially between context factors.\
    \ In our DAG, we only assume an impact of experience in SE/RE on domain knowledge\
    \ (as explained in Section [3.4.10\\)](#page-22-0). We do not assume, for example,\
    \ a causal relation between education and experience in SE/RE as higher levels\
    \ of education do not entail more industrial experience or vice versa. Similarly,\
    \ we assume education to be independent of domain knowledge as most educational\
    \ programs known to us are domain-independent. The resulting set of associations\
    \ visualized in Figure [10](#page-30-0) corresponds to the authors' shared beliefs\
    \ that warrant assuming causal relationships between two variables. While we do\
    \ not expect every reader to share these beliefs, we hope that the explicit and\
    \ transparent documentation of our assumptions invites constructive, iterative\
    \ improvements by challenging them via empirical investigations.\n\nThe DAG from\
    \ the running example in Figure [8](#page-23-0) lists the variable duration as\
    \ an independent variable, while the final DAG in Figure [10](#page-30-0) lists\
    \ it as a response variable. The variable duration takes on two distinct roles\
    \ depending on the current analysis. In the case where the analysis targets the\
    \ effects on the duration, it is the sole response variable. In all other cases,\
    \ it is an independent variable.\n\nIn the second step in the statistical causal\
    \ inference [\\[101\\]](#page-49-14), the identification step, we found the adjustment\
    \ set to include all variables for prediction. To discern the impact of independent\
    \ variables with causal relations among them, we developed comparison models in\
    \ which certain variables were excluded. The comparison showed that the exclusion\
    \ did not change the estimations of the coefficient distributions. Hence, we assume\
    \ all causal relationships are feasible and evaluate the full model, including\
    \ all eligible variables as predictors.\n\n# <span id=\"page-31-0\"></span>4.2.2\
    \ Posterior Predictions\n\nBased on maximum entropy [\\[63\\]](#page-47-18), we\
    \ model the five response variables using the following probability distributions.\
    \ The duration is centered around the global mean, therefore, we model it with\
    \ a Gaussian distribution around µ = 0. The number of superfluous entities is\
    \ an unbounded count with an index of dispersion of about 1.5, hence, we model\
    \ it as a negative binomial distribution. Missing entities, missing associations,\
    \ and wrong associations are bounded counts and, hence, modeled as Binomial distributions.\
    \ Table [5](#page-31-1) contains the result of the predictions from the posterior\
    \ distributions. Each cell contains the resulting likelihood that the occurrence\
    \ of a factor causes fewer or more issues of the respective outcome compared to\
    \ the baseline of no quality defects[5](#page-31-2) . The larger the difference\
    \ between the likelihood of more (+) than fewer (−) issues, the stronger the effect\
    \ of that factor on the outcome. If the likelihood of more (+) issues outweighs\
    \ the likelihood of fewer (−) issues, the factor has a negative effect. If the\
    \ two values are similar, then the factor has no clear effect on the outcome.\n\
    \n<span id=\"page-31-1\"></span>Table 5: Likelihood that a treatment produces\
    \ fewer (−) or more (+) occurrences of the respective outcome variable.\n\n| Outcome\
    \              | PV    |       | AP    |       | PVAP  |       |\n|----------------------|-------|-------|-------|-------|-------|-------|\n\
    |                      | -     | +     | -     | +     | -     | +     |\n| Duration\
    \             | 57.2% | 42.8% | 51.7% | 48.3% | 44.8% | 55.2% |\n| Missing entities\
    \     | 29.6% | 32.5% | 24.7% | 40.1% | 27.4% | 35.6% |\n| Superfluous entities\
    \ | 26.6% | 22.5% | 22.5% | 33.3% | 26.4% | 24.6% |\n| Missing associations |\
    \ 25.0% | 45.0% | 20.2% | 51.2% | 22.2% | 49.4% |\n| Wrong associations   | 10.5%\
    \ | 11.5% | 5.2%  | 44.6% | 6.9%  | 31.5% |\n\n<span id=\"page-31-2\"></span><sup>5</sup>\
    \ The remaining cases (100% − less − more) are omitted from the table\n\nFor example,\
    \ the first two cells in the second row of Table [5](#page-31-1) state that using\
    \ passive voice causes fewer missing entities in 29.6% and more missing entities\
    \ in 32.5% of all cases. In the remaining 37.9% of all cases, passive voice causes\
    \ neither more nor fewer missing entities. Given this balance, the effect of passive\
    \ voice on missing entities is unclear, and there is not enough evidence to reject\
    \ HP V <sup>→</sup><sup>E</sup> − 0 .\n\n# Result of Posterior Predictions\n\n\
    The following effects are likely given the skewed distribution of posterior predictions:\
    \ passive voice, ambiguous pronouns, and their cooccurrence cause an increasing\
    \ number of missing associations. Ambiguous pronouns cause an increasing number\
    \ of wrong associations. Ambiguous pronouns cause an increasing number of missing\
    \ and superfluous.\n\nWe use an arbitrary threshold of 10% to report notable results\
    \ in textual form. Refer to Table [5](#page-31-1) for the actual, more fine-grained\
    \ results.\n\n# <span id=\"page-32-0\"></span>4.2.3 Marginal and Conditional Effects\n\
    \nMarginal plots visualize the isolated effect of specific predictors when fixing\
    \ all other predictors to representative values. In the following, we present\
    \ selected marginal plots that show the effects of interest. The remaining plots\
    \ can be found in our replication package.\n\nMissing entities impact missing\
    \ associations Figure [11](#page-33-0) visualizes the effect of the number of\
    \ missing entities on the number of missing associations. The y-axis represents\
    \ the expected value of missing entities over multiple attempts with a trial size\
    \ of one. Hence, it corresponds to the likelihood of missing one entity.\n\nThe\
    \ plot supports the assumption that missing an entity promotes missing an association,\
    \ which the original experiment did not consider and instead attributed the missing\
    \ associations fully to the use of passive voice [\\[36\\]](#page-45-2). In fact,\
    \ the strength of the effect of passive voice on missing associations (µ P V RQT\
    \ = 0.38) is similar to the strength of the effect of missing entities on missing\
    \ associations (µE<sup>−</sup> = 0.40). However, the uncertainty of the impact\
    \ of passive voice (σ P V RQT = 0.32) is higher than that of missing entities\
    \ (σE<sup>−</sup> = 0.09). This means that the effect of missing entities on missing\
    \ associations is much more reliable than the effect of passive voice on missing\
    \ associations.\n\nImpact of duration Figure [12](#page-33-1) visualizes the impact\
    \ of relative duration (i.e., deviation in duration from the overall average time\
    \ of creating a domain model in minutes) on the two response variables superfluous\
    \ entities and wrong associations. The red estimate shows that the longer a participant\
    \ took to generate a domain model (relative duration > 0), the more likely they\
    \ were to introduce\n\n<span id=\"page-33-0\"></span>![](_page_33_Figure_1.jpeg)\n\
    \nFig. 11: Impact of missing entities on missing associations.\n\n<span id=\"\
    page-33-1\"></span>![](_page_33_Figure_3.jpeg)\n\nFig. 12: Impact of relative\
    \ duration on the number of superfluous entities and wrong associations\n\nsuperfluous\
    \ entities. The cyan estimate shows that the shorter time a participant took to\
    \ generate a domain model (relative duration < 0), the more likely they were to\
    \ connect an association wrongly.\n\nImpact of previous training in modeling Figure\
    \ [13](#page-34-1) visualizes the impact of prior formal training in modeling\
    \ on the number of wrong associations in a domain model. A participant with prior\
    \ formal training (formal = T RUE) shows a slightly lower likelihood of connecting\
    \ associations wrongly. The overlapping confidence intervals do, however, indicate\
    \ a strong variance of the effect.\n\nImpact of remaining context factors None\
    \ of the remaining context factors has a stronger effect on any of the response\
    \ variables than the previously\n\n<span id=\"page-34-1\"></span>![](_page_34_Figure_1.jpeg)\n\
    \nFig. 13: Marginal effect of prior formal training in modeling on the number\
    \ of wrong associations\n\nmentioned impact visualized in Figure [13.](#page-34-1)\
    \ This means that the other context factors are neither notable (µ > 0.4) nor\
    \ significant (σ < µ). The replication package contains a detailed summary of\
    \ all coefficients.\n\nInteraction between domain knowledge and the treatment\
    \ Conditional plots visualize interaction effects between two predictors. Figure\
    \ [14](#page-35-1) visualizes the interaction effect between domain knowledge\
    \ about open source and the treatment on the number of wrong associations. The\
    \ figure shows that the impact of ambiguous pronouns (cyan whisker plots) on the\
    \ response variable number of wrong associations diminishes the greater the domain\
    \ knowledge about open source. For the co-occurrence of ambiguous pronouns and\
    \ passive voice (purple whisker plots), the effect is less pronounced but symmetrical,\
    \ i.e., the factor has the strongest impact on the response variable when the\
    \ domain knowledge is medium. However, the effect contains high uncertainty when\
    \ the treatment involves ambiguous pronouns, represented by the large and overlapping\
    \ confidence intervals (cyan and purple whiskers in Figure [14\\)](#page-35-1).\
    \ The collected data does not suffice to support the significance of this effect.\n\
    \n# Result of Marginal and Conditional Effects\n\nMost context factors do not\
    \ show a significant impact on either the response variables directly or mediate\
    \ the effect of quality defects. The few context factors that do show an impact\
    \ are not significant.\n\n# <span id=\"page-34-0\"></span>4.3 Comparison of original\
    \ with our Study Results\n\nFor the part of our study that serves as a conceptual\
    \ replication, we compare the results of the original study [\\[36\\]](#page-45-2)\
    \ and its re-analysis [\\[47\\]](#page-46-11) with our results [\\[19\\]](#page-45-14).\
    \ Regarding HP V <sup>→</sup><sup>E</sup> − 0 , we obtain conflicting results\
    \ from the FDA\n\n<span id=\"page-35-1\"></span>![](_page_35_Figure_1.jpeg)\n\n\
    ![](_page_35_Figure_2.jpeg)\n\nas we reject the null hypothesis while the original\
    \ study does not, but consistent results from the BDA, as the distribution of\
    \ the posterior prediction of missing entities is balanced. We obtain conflicting\
    \ results for HP V <sup>→</sup><sup>A</sup> − 0 from the FDA as we cannot reject\
    \ it as in the original study. Our BDA suggests that passive voice has a slight\
    \ impact on the number of missing associations (25.5% less and 45.0% more likely\
    \ to miss an association). The result of the BDA does not suppose an effect as\
    \ strong as the original study, but it does agree with the re-analysis of the\
    \ original study [\\[47\\]](#page-46-11) on a slight impact. Overall, the results\
    \ of our BDA agree with the reanalyzed results of the original study. The variation\
    \ of study elements (e.g., experimental subjects and objects) [\\[59\\]](#page-47-13)\
    \ increases the replicability space within the generalizability space [\\[83\\\
    ]](#page-48-2) and identifies those elements as non-influential [\\[66\\]](#page-47-19)\
    \ to the original claim.\n\n# Comparison of Studies\n\nThe results of the frequentist\
    \ analyses of the original and our study differ. However, the more thorough Bayesian\
    \ data analysis agrees with the properly re-analyzed original results. Due to\
    \ the variation of several elements of our study from the original study, the\
    \ conceptual replication extends the external validity of the original claim that\
    \ passive voice has only a slight impact on the domain modeling activity.\n\n\
    # <span id=\"page-35-0\"></span>4.4 Comparison of FDA with BDA Results\n\nSecondly,\
    \ we compare the results of our frequentist data analysis with the results of\
    \ our Bayesian data analysis. We obtain consistent results [\\[19\\]](#page-45-14)\
    \ for H RQD∈{P V,AP,P V AP }→D 0 . Neither the frequentist nor Bayesian analysis\
    \ suggests an impact of the treatment on the relative duration to create a domain\
    \ model. −\n\nThe frequentist analysis rejects H RQD∈{P V,AP,PVAP}→E 0 , while\
    \ the Bayesian analysis remains more cautious. The posterior predictions in Table\
    \ [5](#page-31-1) show a tendency towards the treatment having an impact, but\
    \ with large uncertainty. Additionally, marginal plots of the Bayesian analysis\
    \ reveal that the primary role, experience with domain modeling, and education\
    \ impact the dependent variable. Both analyses agree that H RQD∈{P V,AP,PVAP}→E\
    \ + 0 cannot be rejected, though the Bayesian analysis attributes a tendency of\
    \ causing superfluous entities to ambiguous pronouns.\n\nThe frequentist analysis\
    \ suggests to reject H RQD∈{AP,PVAP}→A − 0 , i.e., ambiguous pronouns and their\
    \ coexistence with passive voice influence the number of missing associations.\
    \ The Bayesian analysis again shows a tendency towards an impact but retains its\
    \ uncertainty about the effect. Marginal plots instead emphasize the influence\
    \ of missing entities on the response variable.\n\nThe analyses agree on the impact\
    \ of ambiguous pronouns on the number of wrong associations and suggest to reject\
    \ H RQD∈{AP,PVAP}→A × 0 . Both the large effect size and the skewed distribution\
    \ of posterior predictions support the existence of a causal effect of ambiguous\
    \ pronouns on wrong associations in a domain model.\n\n# Comparison of Analysis\
    \ Methods\n\nThe results of our frequentist analysis differ from our Bayesian\
    \ analysis: the Bayesian data analysis remains more cautious about several effects\
    \ suggested by the frequentist analysis. The extended casual model attributes\
    \ part of the effect on the response variable on other independent variables than\
    \ the treatment.\n\n# <span id=\"page-36-0\"></span>5 Discussion\n\nSection [5.1](#page-36-1)\
    \ answers the research questions. Section [5.2](#page-37-0) discusses implications\
    \ for requirements quality practice and Section [5.3](#page-38-0) for requirements\
    \ quality research. Section [5.4](#page-40-1) presents the threats to validity.\n\
    \n# <span id=\"page-36-1\"></span>5.1 Answers to research questions\n\n# 5.1.1\
    \ Answer to RQ1\n\nRQ1.1: Impact of passive voice. Using passive voice in natural\
    \ language requirements specifications has a slightly negative effect on the domain\
    \ modeling activity regarding missing associations. This finding aligns with the\
    \ conclusions drawn by the original study by Femmer et al. [\\[36,](#page-45-2)\
    \ [47\\]](#page-46-11). However, the Bayesian data analysis emphasizes that both\
    \ context factors, and especially the number of missing entities, have a significant\
    \ impact on the number of missing associations as well. Overall, these results\
    \ support the claim that passive voice can have a negative impact in specific\
    \ cases but is overall not a significant factor in subsequent activities depending\
    \ on the requirement [\\[36,](#page-45-2) [73\\]](#page-47-5).\n\nRQ1.2: Impact\
    \ of ambiguous pronouns. The use of ambiguous pronouns has a strong effect on\
    \ the number of wrong associations in the resulting domain model. Additionally,\
    \ using ambiguous pronouns has a slight negative effect on the number of missing\
    \ and superfluous entities and missing associations. This confirms the risk of\
    \ using ambiguous pronouns that have been mainly hypothesized in previous research\
    \ [\\[31\\]](#page-45-5) and explains the focus on ambiguity in requirements quality\
    \ research [\\[80\\]](#page-48-0). An ambiguous pronoun in a requirements specification\
    \ has a 44.6% chance of causing a wrongly connected association in the domain\
    \ model, limiting the model's correctness and propagating risk to further activities.\n\
    \nRQ1.3: Combined impact. The co-occurrence of passive voice and ambiguous pronouns\
    \ has a strong effect on the number of wrong associations. Additionally, it has\
    \ a slight effect on the number of missing entities and associations. The impact\
    \ correlates with but never exceeds the effect of pure, ambiguous pronouns. This\
    \ supports the assumption that passive voice does not create any further impact\
    \ in addition to the effect of ambiguous pronouns.\n\n# 5.1.2 Answer to RQ2\n\n\
    RQ2.1: Impact of context factors. Only a small number of context factors included\
    \ in the study show a notable effect on the response variables. The duration of\
    \ the domain modeling activity confirms assumed patterns: taking shorter than\
    \ average increases the chance of missing elements or connecting associations\
    \ wrongly, taking longer time than average increases the chance of adding superfluous\
    \ entities. Prior formal training in modeling shows a slight yet not significant\
    \ positive effect.\n\nRQ2.2: Mediation of context factors. The interaction effect\
    \ between domain knowledge and the treatment shows that higher domain knowledge\
    \ can mitigate the negative effect of quality defects on response variables. In\
    \ particular: higher domain knowledge reduces the chance of connecting associations\
    \ incorrectly. While the effect still exhibits a large variance, this hints at\
    \ the possibility of compensating quality defects with domain knowledge.\n\n#\
    \ <span id=\"page-37-0\"></span>5.2 Implications for Requirements Quality Practice\n\
    \nThe presented results indicate that the negative impact of two requirements\
    \ quality defects can differ significantly. When allocating resources toward detecting\
    \ and removing specific quality defects from requirements specifications, organizations\
    \ can make informed decisions based on the calculated impact of the respective\
    \ quality factor. In our case, we recommend explicitly detecting and resolving\
    \ ambiguous pronouns, while passive voice is not critical enough to deserve dedicated\
    \ attention. This aligns with the common perception in requirements quality research\
    \ that ambiguity receives the most attention [\\[80\\]](#page-48-0) while using\
    \ passive voice rarely has a tangible impact [\\[73\\]](#page-47-5). By filtering\
    \ requirements writing guidelines for quality factors that have a measurable effect,\
    \ we expect greater acceptance of requirements quality assessment tools in practice\
    \ [\\[44,](#page-46-15) [90\\]](#page-48-12).\n\nAdditionally, measuring the effect\
    \ of a quality defect on the relevant attributes of activities that use these\
    \ requirements allows quantifying it economically [\\[48\\]](#page-46-0). While\
    \ the cost of a quality defect is hard to determine, a company can quantify the\
    \ cost of activities' attributes like increased duration. This economic perspective\
    \ provides additional decision support for companies when assessing whether it\
    \ is worth detecting and removing a specific quality defect [\\[65\\]](#page-47-3).\n\
    \nFinally, the potential influence of context factors on the impact of quality\
    \ defects on affected activities may incentivize organizations to invest in developing\
    \ these factors. For example, improving domain knowledge and providing formal\
    \ modeling training may compensate for quality defects.\n\n# <span id=\"page-38-0\"\
    ></span>5.3 Implications for Requirements Quality Research\n\nEmploying Bayesian\
    \ data analysis to investigate the impact of requirements quality defects provides\
    \ sophisticated and sensitive insights necessary to propel requirements quality\
    \ research [\\[48\\]](#page-46-0). The result of the analysis models both the\
    \ direction and strength of an impacting factor while retaining information about\
    \ its certainty. These insights go beyond the point-wise comparison and binary\
    \ result of frequentist analyses [\\[52\\]](#page-46-2). The frequentist analysis\
    \ fails to compare the impact of quality defects on the response variables, as\
    \ even the calculated effect sizes are similar (0.79 < |ES| < 0.93). The Bayesian\
    \ data analysis, on the other hand, clearly shows that some effects are much stronger\
    \ (e.g., HAP→<sup>A</sup> × 0 ) than others (e.g., HP V <sup>→</sup><sup>A</sup>\
    \ − 0 ). Still, the BDA relies on the causal model expressed in a DAG, statistical\
    \ assumptions about variable types and their independence, and the validity of\
    \ constructs. Therefore, the results obtained via BDA cannot be seen as more valid\
    \ by design. However, the BDA is more transparent and allows critical debate—e.g.,\
    \ about the causal assumptions underlying our analysis in Figure [10—](#page-30-0)which\
    \ facilitates the incremental improvement of empirical studies.\n\nIncluding context\
    \ factors in the prediction allows comparing the impact of requirements quality\
    \ with the impact of human and process factors, revealing which causes changes\
    \ in the response variable. These context factors can also represent the properties\
    \ of non-human agents like generative artificial intelligence (GenAI) models which\
    \ are increasingly employed for RE tasks. Involving context factors like the version\
    \ number of a GenAI model, its parameters, its context window, and other factors\
    \ in empirical studies resembling our approach will allow to investigate which\
    \ configurations of these models excel at performing their RE task.\n\nAbandoning\
    \ simple NHSTs for identifying relevant factors of requirements quality and instead\
    \ opting for a proper framework for causal inference like BDA will increase the\
    \ likelihood of solving problems with practical relevance [\\[44\\]](#page-46-15)\
    \ that justify subsequent tool development [\\[50\\]](#page-46-16). Empirical\
    \ studies with explicit causal assumptions (e.g., visualized as DAGs) and sophisticated\
    \ analyses will produce context-sensitive evidence that can be synthesized in\
    \ the common framework of the requirements quality theory [\\[48\\]](#page-46-0).\
    \ The continuous synthesis of evidence from individual studies in this common\
    \ framework will produce more reliable and generalizable conclusions [\\[1,](#page-44-16)\
    \ [22\\]](#page-45-21) and effectively address the lack of empirical insights\
    \ in requirements quality research [\\[80\\]](#page-48-0).\n\nUsing a controlled\
    \ experiment benefits the investigation of the quality factor [\\[36\\]](#page-45-2).\
    \ The DAG shown in Figure [10](#page-30-0) visualizes this control, as no other\
    \ factor influences the treatment in question. This eliminates spurious associations\
    \ that could confuse the results [\\[76\\]](#page-47-4). On the other hand, the\
    \ cost of conducting a controlled experiment—especially with participants from\
    \ industry—cannot be neglected [\\[102\\]](#page-49-12). Luckily, statistical\
    \ causal inference via Bayesian data analysis works equally well with observational\
    \ data, as shown by Furia et al. [\\[54\\]](#page-46-14).\n\nFinally, Bayesian\
    \ data analysis allows for incremental improvement of empirical inquiry regarding\
    \ requirements quality. The causal assumptions that the DAG makes explicit can\
    \ be reviewed, discussed, and updated to inform future empirical methods. Insights\
    \ derived from Bayesian data analysis can be used as prior knowledge in subsequent\
    \ analyses, just as we sensibly used previous results [\\[36\\]](#page-45-2) to\
    \ inform our priors.\n\nWorth noting is that our comparison between FDA and BDA\
    \ conflates the use of causal frameworks with advanced Bayesian statistics. An\
    \ FDA can also employ causal frameworks that mitigate parts of the shortcomings\
    \ mentioned in Section [2.2,](#page-8-0) as previously shown by Furia et al. [\\\
    [54\\]](#page-46-14). However, frequentist approaches tend to limit their analyses\
    \ to the treatment and the response variable, disregarding potential context [\\\
    [81\\]](#page-48-1) or experimental design factors [\\[107\\]](#page-49-11). BDA,\
    \ on the other hand, entails the use of an explicit causal framework [\\[76,](#page-47-4)\
    \ [87\\]](#page-48-14), which is why we support the recommendation of abandoning\
    \ FDA for BDA in SE research [\\[52,](#page-46-2) [106\\]](#page-49-7).\n\n# Implications\n\
    \nQuality defects in requirements specifications have a varying impact on affected\
    \ activities that depend on them. Context factors may compensate for this impact\
    \ but require better metrics to quantify them. Bayesian data analysis provides\
    \ more fine-grained insights into these effects than frequentist methods.\n\n\
    # <span id=\"page-40-1\"></span>5.4 Threats to Validity\n\nWe present and discuss\
    \ threats that could affect our study based on the guidelines by Wohlin et al.\
    \ [\\[113\\]](#page-49-10) and extended by the guideline by Vegas et al. [\\[107\\\
    ]](#page-49-11) for the specific threats caused by the use of a crossover design.\
    \ The threats to validity are prioritized, considering our work focuses on replicating\
    \ the first study testing a causal theory predicting the impact of requirements\
    \ quality factors on downstream development tasks.\n\n# <span id=\"page-40-0\"\
    ></span>5.4.1 Internal validity\n\nOur design and the blind nature of the experiment\
    \ avoid the threat to selectionmaturation interaction. Nevertheless, the new settings\
    \ (i.e., online asynchronous experiment) may have caused a diffusion or imitation\
    \ of treatments—i.e., information may have been exchanged among the participants.\
    \ The experiment supervisor monitored the participants to prevent their communication\
    \ with each other and asked them not to distribute the experimental task and materials.\
    \ We acknowledge that selection can bias our sample as volunteers are generally\
    \ more motivated to perform in an experimental task [\\[3\\]](#page-44-12).\n\n\
    The crossover design emits additional threats to validity [\\[107\\]](#page-49-11).\
    \ We mitigate the learning by practice effect—i.e., participants getting better\
    \ when repeating the experimental task—in three ways: Firstly, we disperse the\
    \ learning effect evenly at design time by randomizing the sequences of treatments.\
    \ Secondly, we include a warm-up object to get participants used to the task and\
    \ tool but exclude that data from the analysis. Thirdly, we include the period\
    \ variable as a predictor to factor out the learning effect during the analysis.\
    \ We avoid the threat of copying by prohibiting communication among participants\
    \ and using experimental objects where solutions cannot be copied from one task\
    \ to another.\n\nThe threat of optimal sequence describes the risk that there\
    \ is a sequence in which the treatment is applied, which optimizes the participants'\
    \ performance in deriving domain models. We cannot block this threat at analysis\
    \ time as the sequence and participant IDs are highly correlated. This is because\
    \ we could in all but one case—assign only one participant (n<sup>p</sup> = 25)\
    \ to each sequence (n<sup>s</sup> = nr! = 4! = 24). Because of this strong correlation,\
    \ the Bayesian model is incapable of distinguishing between the impact of the\
    \ sequence (βseq) from the within-participant variance (αP ID) [\\[107\\]](#page-49-11).\
    \ More participants per sequence would have been necessary to block the threat\
    \ of an optimal sequence, but these were unavailable to us.\n\nFinally, we address\
    \ the threat of carryover—i.e., the change of the impact caused by the period\
    \ in which the treatment was applied—at analysis time by including the term period∗treatment\
    \ in the predictors. This way, the carryover effect is factored out from the impact\
    \ of the treatment and analyzable from the posterior distributions.\n\n# 5.4.2\
    \ Conclusion validity\n\nWe addressed the reliability of measures threat by creating\
    \ and disclosing evaluation guidelines and peer-reviewing the extraction of the\
    \ dependent variables from the collected domain models. Despite the acceptable\
    \ inter-rater agreement score, an in-depth qualitative evaluation of the remaining\
    \ disagreements may be beneficial to further improve the evaluation instrument\
    \ and, therefore, the reliability of the results. We addressed the random heterogeneity\
    \ of the subjects by a design in which each participant acts as their own control\
    \ group.\n\nMoreover, we focused on including and analyzing context variables\
    \ related to the participants' experience. Our sample of participants is not representative\
    \ of all context factors. Consequently, our Bayesian data analysis cannot identify\
    \ all causal effects of some context factors. However, by including them in the\
    \ causal considerations, the effect of the factors is isolated from the potential\
    \ confounding variables [\\[74\\]](#page-47-11).\n\nThe conclusion validity of\
    \ our study is strengthened by applying two different data analysis approaches\
    \ and comparing their results. The data analysis suffers from the threat of low\
    \ statistical power when it comes to evaluating interaction effects, as reliably\
    \ identifying them requires a larger sample size [\\[55\\]](#page-46-17). We limit\
    \ the number of interaction effects considered in our models and discuss the uncertainty\
    \ around the coefficient estimates to minimize this threat.\n\nThe analysis can\
    \ suffer from violated assumptions of statistical tests. Modeling the number of\
    \ missing entities and associations as binomial distributions implies the independence\
    \ of each event, i.e., that each missing entity and association is independent\
    \ of all other missing entities and associations. While we did not observe any\
    \ cascading, i.e., dependent, defects, their independence remains only assumed.\n\
    \n# 5.4.3 Construct validity\n\nOur study can suffer from mono-operation bias\
    \ as we focus only on a subset of quality factors that can potentially exist [\\\
    [49,](#page-46-5) [80\\]](#page-48-0). Nevertheless, our goal with this replication\
    \ is to extend the initial quality factor of passive voice reported by Femmer\
    \ et al. [\\[36\\]](#page-45-2) to a second one—ambiguous pronouns—which is widespread\
    \ as indicated by the literature [\\[49,](#page-46-5) [80\\]](#page-48-0).\n\n\
    Similarly, a confounding of constructs and level of constructs could influence\
    \ the outcomes of our study, for example, the presence of several ambiguous pronouns\
    \ or passive voice sentences rather than their binary presence or absence from\
    \ a specification. Further replications, focusing on improving construct validity,\
    \ should include several levels of each treatment.\n\nMono-method bias is a potential\
    \ threat to construct validity—i.e., we measured the dependent variables using\
    \ a single type of measurement, inspired by the original study. However, the measurements\
    \ were based on a pre-defined protocol and peer-reviewed. Our study may result\
    \ in a restricted generalizability across constructs since the presence or absence\
    \ of the different quality factors could result in side effects for other interesting\
    \ outcomes we did not measure (e.g., comprehensibility or maintainability of the\
    \ specification).\n\nAmong the social threats to construct validity, we acknowledge\
    \ that hypothesis guessing may have taken place since the participants could try\
    \ to guess the concrete goal of the experimental task based on the invitation\
    \ text and material provided during the sessions. Nevertheless, we used the same\
    \ text and phrasing to invite all participants and the same material during the\
    \ experimental task. Evaluation apprehension could have played a role since some\
    \ participants are students at the authors' institution. However, students did\
    \ not receive rewards (e.g., extra grade points) for participating in the experiment,\
    \ and they received their course grades before the start of the experiment.\n\n\
    Moreover, our study can suffer from an inadequate preoperational explication of\
    \ constructs as we did not validate our context factors. For example, we are unable\
    \ to provide any proof that the self-reported number of years spent in RE adequately\
    \ represents the latent variable of experience in RE beyond educated guesses and\
    \ relying on comparable practices in our scientific community [\\[10\\]](#page-44-14).\
    \ To improve the construct validity, separate studies investigating the adequacy\
    \ of these measurements in representing their constructs are necessary. This particularly\
    \ impacts our decision to replace the binary distinction of participants by type\
    \ (students versus practitioners) with more fine-grained variables like experience\
    \ and domain knowledge. While our study supports the feasibility of this step\
    \ on an analytical level, we cannot prove its validity on a conceptual level.\
    \ We encourage investigating the feasibility of variables to represent individual\
    \ skills to improve the construct validity of studies considering this impact\
    \ [\\[97\\]](#page-48-17).\n\nFinally, a variable of the selected population that\
    \ may interact with the treatment that we did not analyze is the language skill\
    \ of participants. Arguably, skills in the English language influence the ability\
    \ to comprehend and process the experimental objects and, therefore, may impact\
    \ the response variables. We were unable to measure this variable properly given\
    \ that all participants scored the same on the Common European Framework of Reference\
    \ for Languages (CEFR) [\\[75\\]](#page-47-20) (i.e., non-native, fluent English\
    \ speakers). While the threat is minimized in our study due to the comparable\
    \ language level of participants, future studies should develop measurement instruments\
    \ for this construct and involve this variable in such causal queries.\n\n# 5.4.4\
    \ External validity\n\nThe main threat to the external validity of this study\
    \ is the interaction of setting and treatment as the size and the complexity of\
    \ the selected specifications, despite being sampled from a real-world data set,\
    \ might not be representative of the industrial practice. Using Google Docs as\
    \ the modeling tool is not fully representative of real-world practices. Given\
    \ that it was appropriate and sufficient for the experimental task, however, renders\
    \ this as an opportunity for improving the realism of the experiment in future\
    \ studies rather than a threat to validity.\n\nThere may be the threat of interaction\
    \ of selection and treatment, as some participants reported no modeling experience\
    \ or training. These deficiencies might influence the results and render a subset\
    \ of the participants as nonrepresentative of our target population. We attempted\
    \ to mitigate this threat via comprehensive instructions and including a warm-up\
    \ phase in the experiment.\n\n# <span id=\"page-43-0\"></span>6 Conclusion\n\n\
    Requirements quality research lacks empirical evidence and research strategies\
    \ to advance beyond proposing and following normative rules with unclear impact\
    \ [\\[49\\]](#page-46-5) to better understanding and solving problems relevant\
    \ to practice [\\[43,](#page-46-1) [44\\]](#page-46-15). In the scope of our study,\
    \ we conducted a controlled experiment on the impact of requirements quality defects\
    \ on subsequent activities. We demonstrated a method of evaluating data collected\
    \ through a controlled experiment using a crossover design with Bayesian data\
    \ analysis. We showed the impact (1) of requirements quality defects varies and\
    \ (2) may be mediated by context and confounding factors. The part of our study\
    \ that serves as a conceptual replication strengthens the claims of the re-analyzed\
    \ original study [\\[36,](#page-45-2) [47\\]](#page-46-11) that passive voice\
    \ only has a slight impact on missing associations from domain models.\n\nWe can\
    \ confidently support the recommendation of SE researchers to adopt Bayesian data\
    \ analysis to improve causal reasoning and inference [\\[52,](#page-46-2) [53,](#page-46-10)\
    \ [106\\]](#page-49-7), which will propel requirements quality research. This\
    \ shift requires focusing on problems such as scrutinizing the explicit causal\
    \ assumptions of a DAG, visualizing requirements quality impact, evolving prior\
    \ knowledge about their impact, and comparing models concerning their predictive\
    \ power.\n\nWe envision that adopting sophisticated statistical tools like Bayesian\
    \ data analysis and the focus of empirical studies on investigating the impact\
    \ of requirements quality defects will steer requirements quality research in\
    \ a relevant and effective direction. Explicit causal assumptions and sophisticated\
    \ data analyses will produce empirical evidence which can be more easily synthesized\
    \ to more reliable and generalizable conclusions [\\[22\\]](#page-45-21) in a\
    \ common framework [\\[48\\]](#page-46-0). We hope that the documentation of this\
    \ study inspires fellow researchers to adopt our method and tools for replication.\n\
    \nAcknowledgements This work was supported by the KKS foundation through the S.E.R.T.\
    \ Research Profile project at Blekinge Institute of Technology. We are deeply\
    \ grateful to Parisa Yousefi from Ericsson AB for recruiting practitioners to\
    \ the experiment. We further thank the reviewers for their tremendous effort that\
    \ significantly improved this manuscript.\n\n# Conflict of interest\n\nThe authors\
    \ declare that they have no conflict of interest.\n\n# Data Availability Statement\n\
    \nAll supplementary material, including protocols and guidelines for data collection\
    \ and extraction, the raw data, analysis scripts, figures, and results, are available\
    \ in our replication package [\\[46\\]](#page-46-3). The replication package is\
    \ available on GitHub at <https://github.com/JulianFrattini/rqi-proto> and archived\
    \ on Zenodo at <https://doi.org/10.5281/zenodo.10423666>.\n\n# References\n\n\
    - <span id=\"page-44-16\"></span>1. Badampudi, D., Wohlin, C., Gorschek, T.: Contextualizing\
    \ research evidence through knowledge translation in software engineering. In:\
    \ Proceedings of the 23rd International Conference on Evaluation and Assessment\
    \ in Software Engineering, pp. 306–311 (2019)\n- <span id=\"page-44-1\"></span>2.\
    \ Baldassarre, M.T., Carver, J., Dieste, O., Juristo, N.: Replication types: Towards\
    \ a shared taxonomy. In: Proceedings of the 18th International Conference on Evaluation\
    \ and Assessment in Software Engineering, pp. 1–4 (2014)\n- <span id=\"page-44-12\"\
    ></span>3. Baltes, S., Ralph, P.: Sampling in software engineering research: A\
    \ critical review and guidelines. Empirical Software Engineering 27(4), 94 (2022)\n\
    - <span id=\"page-44-2\"></span>4. Bano, M.: Addressing the challenges of requirements\
    \ ambiguity: A review of empirical literature. In: 2015 IEEE Fifth International\
    \ Workshop on Empirical Requirements Engineering (EmpiRE), pp. 21–24. IEEE (2015)\n\
    - <span id=\"page-44-5\"></span>5. Belev, G.: Guidelines for specification development.\
    \ In: Proceedings., Annual Reliability and Maintainability Symposium, pp. 15–21.\
    \ IEEE (1989)\n- <span id=\"page-44-13\"></span>6. Benjamini, Y., Hochberg, Y.:\
    \ Controlling the false discovery rate: A practical and powerful approach to multiple\
    \ testing. Journal of the Royal statistical society: series B (Methodological)\
    \ 57(1), 289–300 (1995)\n- <span id=\"page-44-11\"></span>7. Berntsson Svensson,\
    \ R., Torkar, R.: Not all requirements prioritization criteria are equal at all\
    \ times: A quantitative analysis. Journal of Systems and Software 209, 111909\
    \ (2024). DOI 10.1016/j.jss.2023.111909\n- <span id=\"page-44-3\"></span>8. Berry,\
    \ D.M., Kamsties, E.: Ambiguity in requirements specification. In: Perspectives\
    \ on software requirements, pp. 7–44. Springer (2004)\n- <span id=\"page-44-0\"\
    ></span>9. Boehm, B.W.: Software engineering economics. IEEE transactions on Software\
    \ Engineering SE-10(1), 4–21 (1984)\n- <span id=\"page-44-14\"></span>10. Bogner,\
    \ J., Kotstein, S., Pfaff, T.: Do restful api design rules have an impact on the\
    \ understandability of web apis? Empirical software engineering 28(6), 132 (2023)\n\
    - <span id=\"page-44-6\"></span>11. Boyd, S., Zowghi, D., Farroukh, A.: Measuring\
    \ the expressiveness of a constrained natural language: An empirical study. In:\
    \ 13th IEEE International Conference on Requirements Engineering (RE'05), pp.\
    \ 339–349. IEEE (2005)\n- <span id=\"page-44-7\"></span>12. Briand, L., Bianculli,\
    \ D., Nejati, S., Pastore, F., Sabetzadeh, M.: The case for contextdriven software\
    \ engineering research: Generalizability is overrated. IEEE Software 34(5), 72–75\
    \ (2017)\n- <span id=\"page-44-8\"></span>13. Brooks, S., Gelman, A., Jones, G.,\
    \ Meng, X.L.: Handbook of Markov Chain Monte Carlo. CRC press (2011)\n- <span\
    \ id=\"page-44-15\"></span>14. Brown Jr, B.W.: The crossover experiment for clinical\
    \ trials. Biometrics pp. 69–79 (1980)\n- <span id=\"page-44-4\"></span>15. de\
    \ Bruijn, F., Dekkers, H.L.: Ambiguity in natural language software requirements:\
    \ A case study. In: Requirements Engineering: Foundation for Software Quality:\
    \ 16th International Working Conference, REFSQ 2010, Essen, Germany, June 30–July\
    \ 2, 2010. Proceedings 16, pp. 233–247. Springer (2010)\n- <span id=\"page-44-10\"\
    ></span>16. B¨urkner, P.C.: brms: An R package for Bayesian multilevel models\
    \ using Stan. Journal of statistical software 80, 1–28 (2017)\n- <span id=\"page-44-9\"\
    ></span>17. Carpenter, B., Gelman, A., Hoffman, M.D., Lee, D., Goodrich, B., Betancourt,\
    \ M., Brubaker, M., Guo, J., Li, P., Riddell, A.: Stan: A probabilistic programming\
    \ language. Journal of statistical software 76(1) (2017)\n- <span id=\"page-45-15\"\
    ></span>18. Carver, J., Jaccheri, L., Morasca, S., Shull, F.: Issues in using\
    \ students in empirical studies in software engineering education. In: Proceedings.\
    \ 5th international workshop on enterprise networking and computing in healthcare\
    \ industry (IEEE Cat. No. 03EX717), pp. 239–249. IEEE (2004)\n- <span id=\"page-45-14\"\
    ></span>19. Carver, J.C.: Towards reporting guidelines for experimental replications:\
    \ A proposal. In: 1st international workshop on replication in empirical software\
    \ engineering, vol. 1, pp. 1–4 (2010)\n- <span id=\"page-45-8\"></span>20. Chantree,\
    \ F., Nuseibeh, B., De Roeck, A., Willis, A.: Identifying nocuous ambiguities\
    \ in natural language requirements. In: 14th IEEE International Requirements Engineering\
    \ Conference (RE'06), pp. 59–68. IEEE (2006)\n- <span id=\"page-45-10\"></span>21.\
    \ Christel, M.G., Kang, K.C.: Issues in requirements elicitation (1992)\n- <span\
    \ id=\"page-45-21\"></span>22. Ciolkowski, M., M¨unch, J.: Accumulation and presentation\
    \ of empirical evidence: problems and challenges. ACM SIGSOFT Software Engineering\
    \ Notes 30(4), 1–3 (2005)\n- <span id=\"page-45-20\"></span>23. Cohen, B.H.: Explaining\
    \ psychological statistics. John Wiley & Sons (2008)\n- <span id=\"page-45-17\"\
    ></span>24. Cohen, J.: Statistical power analysis for the behavioral sciences.\
    \ Academic press (1969)\n- <span id=\"page-45-4\"></span>25. Deissenboeck, F.,\
    \ Wagner, S., Pizka, M., Teuchert, S., Girard, J.F.: An activity-based quality\
    \ model for maintainability. In: 2007 IEEE International Conference on Software\
    \ Maintenance, pp. 184–193. IEEE (2007)\n- <span id=\"page-45-18\"></span>26.\
    \ Demaris, A.: Logit modeling: Practical applications. 86. Sage (1992)\n- <span\
    \ id=\"page-45-6\"></span>27. Drechsler, R., Soeken, M., Wille, R.: Automated\
    \ and quality-driven requirements engineering. In: 2014 IEEE/ACM International\
    \ Conference on Computer-Aided Design (ICCAD), pp. 586–590. IEEE (2014)\n- <span\
    \ id=\"page-45-16\"></span>28. Dyb˚a, T., Kampenes, V.B., Sjøberg, D.I.: A systematic\
    \ review of statistical power in software engineering experiments. Information\
    \ and Software Technology 48(8), 745– 755 (2006)\n- <span id=\"page-45-13\"></span>29.\
    \ Elwert, F.: Graphical causal models. In: Handbook of causal analysis for social\
    \ research, pp. 245–273. Springer (2013)\n- <span id=\"page-45-19\"></span>30.\
    \ Ernst, N.A.: Bayesian hierarchical modelling for tailoring metric thresholds.\
    \ In: Proceedings of the 15th international conference on mining software repositories,\
    \ pp. 587– 591 (2018). DOI 10.1145/3196398.3196443\n- <span id=\"page-45-5\"></span>31.\
    \ Ezzini, S., Abualhaija, S., Arora, C., Sabetzadeh, M.: Automated handling of\
    \ anaphoric ambiguity in requirements: A multi-solution study. In: Proceedings\
    \ of the 44th International Conference on Software Engineering, pp. 187–199 (2022)\n\
    - 32. Ezzini, S., Abualhaija, S., Arora, C., Sabetzadeh, M.: TAPHSIR: towards\
    \ AnaPHoric ambiguity detection and ReSolution in requirements. In: Proceedings\
    \ of the 30th ACM Joint European Software Engineering Conference and Symposium\
    \ on the Foundations of Software Engineering, pp. 1677–1681 (2022)\n- <span id=\"\
    page-45-9\"></span>33. Ezzini, S., Abualhaija, S., Arora, C., Sabetzadeh, M.,\
    \ Briand, L.C.: Using domainspecific corpora for improved handling of ambiguity\
    \ in requirements. In: 2021 IEEE/ACM 43rd International Conference on Software\
    \ Engineering (ICSE), pp. 1485– 1497. IEEE (2021)\n- <span id=\"page-45-12\"></span>34.\
    \ Femmer, H.: Requirements quality defect detection with the qualicen requirements\
    \ scout. In: REFSQ Workshops (2018)\n- <span id=\"page-45-3\"></span>35. Femmer,\
    \ H., Fern´andez, D.M., Wagner, S., Eder, S.: Rapid quality assurance with requirements\
    \ smells. Journal of Systems and Software 123, 190–213 (2017)\n- <span id=\"page-45-2\"\
    ></span>36. Femmer, H., Kuˇcera, J., Vetr`o, A.: On the impact of passive voice\
    \ requirements on domain modelling. In: Proceedings of the 8th ACM/IEEE International\
    \ Symposium on Empirical Software Engineering and Measurement, pp. 1–4 (2014)\n\
    - <span id=\"page-45-0\"></span>37. Femmer, H., Mund, J., Fern´andez, D.M.: It's\
    \ the activities, stupid! A new perspective on re quality. In: 2015 IEEE/ACM 2nd\
    \ International Workshop on Requirements Engineering and Testing, pp. 13–19. IEEE\
    \ (2015)\n- <span id=\"page-45-1\"></span>38. Femmer, H., Vogelsang, A.: Requirements\
    \ quality is quality in use. IEEE Software 36(3), 83–91 (2018)\n- <span id=\"\
    page-45-11\"></span>39. Ferrari, A., Esuli, A.: An NLP approach for cross-domain\
    \ ambiguity detection in requirements engineering. Automated Software Engineering\
    \ 26(3), 559–598 (2019)\n- <span id=\"page-45-7\"></span>40. Ferrari, A., Gori,\
    \ G., Rosadini, B., Trotta, I., Bacherini, S., Fantechi, A., Gnesi, S.: Detecting\
    \ requirements defects with NLP patterns: An industrial experience in the railway\
    \ domain. Empirical Software Engineering 23, 3684–3733 (2018)\n- <span id=\"page-46-12\"\
    ></span>41. Ferrari, A., Spagnolo, G.O., Gnesi, S.: Pure: A dataset of public\
    \ requirements documents. In: 2017 IEEE 25th International Requirements Engineering\
    \ Conference (RE), pp. 502–505. IEEE (2017)\n- <span id=\"page-46-8\"></span>42.\
    \ Firesmith, D.: Common requirements problems, their negative consequences, and\
    \ the industry best practices to help solve them. J. Object Technol. 6(1), 17–33\
    \ (2007)\n- <span id=\"page-46-1\"></span>43. Franch, X., Fern´andez, D.M., Oriol,\
    \ M., Vogelsang, A., Heldal, R., Knauss, E., Travassos, G.H., Carver, J.C., Dieste,\
    \ O., Zimmermann, T.: How do practitioners perceive the relevance of requirements\
    \ engineering research? An ongoing study. In: 2017 IEEE 25th International Requirements\
    \ Engineering Conference (RE), pp. 382–387. IEEE (2017)\n- <span id=\"page-46-15\"\
    ></span>44. Franch, X., Mendez, D., Vogelsang, A., Heldal, R., Knauss, E., Oriol,\
    \ M., Travassos, G., Carver, J.C., Zimmermann, T.: How do practitioners perceive\
    \ the relevance of requirements engineering research? IEEE Transactions on Software\
    \ Engineering (2020)\n- <span id=\"page-46-4\"></span>45. Franch, X., Palomares,\
    \ C., Quer, C., Chatzipetrou, P., Gorschek, T.: The state-ofpractice in requirements\
    \ specification: an extended interview study at 12 companies. Requirements Engineering\
    \ pp. 1–33 (2023). DOI 10.1007/s00766-023-00399-7\n- <span id=\"page-46-3\"></span>46.\
    \ Frattini, J.: Replication package for the \"applying bayesian data analysis\
    \ for causal inference about requirements quality: a controlled experiment\".\
    \ [https://zenodo.org/](https://zenodo.org/doi/10.5281/zenodo.10423665) [doi/10.5281/zenodo.10423665](https://zenodo.org/doi/10.5281/zenodo.10423665)\
    \ (2024). Accessed: 2024-06-21\n- <span id=\"page-46-11\"></span>47. Frattini,\
    \ J., Fucci, D., Torkar, R., Mendez, D.: A second look at the impact of passive\
    \ voice requirements on domain modeling: Bayesian reanalysis of an experiment.\
    \ In: International Workshop on Methodological Issues with Empirical Studies in\
    \ Software Engineering (WSESE'24) (2024)\n- <span id=\"page-46-0\"></span>48.\
    \ Frattini, J., Montgomery, L., Fischbach, J., Mendez, D., Fucci, D., Unterkalmsteiner,\
    \ M.: Requirements quality research: A harmonized theory, evaluation, and roadmap.\
    \ Requirements Engineering pp. 1–14 (2023)\n- <span id=\"page-46-5\"></span>49.\
    \ Frattini, J., Montgomery, L., Fischbach, J., Unterkalmsteiner, M., Mendez, D.,\
    \ Fucci, D.: A live extensible ontology of quality factors for textual requirements.\
    \ In: 2022 IEEE 30th International Requirements Engineering Conference (RE), pp.\
    \ 274–280. IEEE (2022)\n- <span id=\"page-46-16\"></span>50. Frattini, J., Unterkalmsteiner,\
    \ M., Fucci, D., Mendez, D.: NLP4RE Tools: Classification, Overview, and Management.\
    \ Springer International Publishing (2024)\n- <span id=\"page-46-13\"></span>51.\
    \ Fucci, D., Scanniello, G., Romano, S., Shepperd, M., Sigweni, B., Uyaguari,\
    \ F., Turhan, B., Juristo, N., Oivo, M.: An external replication on the effects\
    \ of test-driven development using a multi-site blind analysis approach. In: Proceedings\
    \ of the 10th ACM/IEEE International Symposium on Empirical Software Engineering\
    \ and Measurement, ESEM '16. Association for Computing Machinery, New York, NY,\
    \ USA (2016). DOI 10.1145/2961111.2962592. URL [https://doi.org/10.1145/2961111.](https://doi.org/10.1145/2961111.2962592)\
    \ [2962592](https://doi.org/10.1145/2961111.2962592)\n- <span id=\"page-46-2\"\
    ></span>52. Furia, C.A., Feldt, R., Torkar, R.: Bayesian data analysis in empirical\
    \ software engineering research. IEEE Transactions on Software Engineering 47(9),\
    \ 1786–1810 (2019)\n- <span id=\"page-46-10\"></span>53. Furia, C.A., Torkar,\
    \ R., Feldt, R.: Applying Bayesian analysis guidelines to empirical software engineering\
    \ data: The case of programming languages and code quality. ACM Transactions on\
    \ Software Engineering and Methodology (TOSEM) 31(3), 1–38 (2022)\n- <span id=\"\
    page-46-14\"></span>54. Furia, C.A., Torkar, R., Feldt, R.: Towards causal analysis\
    \ of empirical software engineering data: The impact of programming languages\
    \ on coding competitions. ACM Transactions on Software Engineering and Methodology\
    \ 33(1) (2023). DOI 10.1145/3611667\n- <span id=\"page-46-17\"></span>55. Gelman,\
    \ A.: You need 16 times the sample size to estimate an interaction than to estimate\
    \ a main effect. <https://statmodeling.stat.columbia.edu/2018/03/15/need16/>.\
    \ Accessed: 2023-11-24\n- <span id=\"page-46-9\"></span>56. Gelman, A., Vehtari,\
    \ A., Simpson, D., Margossian, C.C., Carpenter, B., Yao, Y., Kennedy, L., Gabry,\
    \ J., B¨urkner, P.C., Modr´ak, M.: Bayesian workflow. arXiv preprint arXiv:2011.01808\
    \ (2020)\n- <span id=\"page-46-6\"></span>57. G´enova, G., Fuentes, J.M., Llorens,\
    \ J., Hurtado, O., Moreno, V.: A framework to measure and improve the quality\
    \ of textual requirements. Requirements engineering 18, 25–41 (2013)\n- <span\
    \ id=\"page-46-7\"></span>58. Gleich, B., Creighton, O., Kof, L.: Ambiguity detection:\
    \ Towards a tool explaining ambiguity sources. In: Requirements Engineering: Foundation\
    \ for Software Quality:\n\n16th International Working Conference, REFSQ 2010,\
    \ Essen, Germany, June 30–July 2, 2010. Proceedings 16, pp. 218–232. Springer\
    \ (2010)\n\n- <span id=\"page-47-13\"></span>59. G´omez, O.S., Juristo, N., Vegas,\
    \ S.: Replications types in experimental disciplines. In: Proceedings of the 2010\
    \ ACM-IEEE international symposium on empirical software engineering and measurement,\
    \ pp. 1–10 (2010)\n- <span id=\"page-47-12\"></span>60. Gren, L., Berntsson Svensson,\
    \ R.: Is it possible to disregard obsolete requirements? a family of experiments\
    \ in software effort estimation. Requirements Engineering 26(3), 459–480 (2021)\n\
    - <span id=\"page-47-6\"></span>61. Hasso, H., Dembach, M., Geppert, H., Toews,\
    \ D.: Detection of defective requirements using rule-based scripts. In: REFSQ\
    \ Workshops (2019)\n- <span id=\"page-47-16\"></span>62. Hsu, H., Lachenbruch,\
    \ P.A.: Paired t test. Wiley StatsRef: statistics reference online (2014)\n- <span\
    \ id=\"page-47-18\"></span>63. Jaynes, E.T.: Probability theory: The logic of\
    \ science. Cambridge University Press, Cambridge (2003)\n- <span id=\"page-47-14\"\
    ></span>64. Jedlitschka, A., Ciolkowski, M., Pfahl, D.: Reporting experiments\
    \ in software engineering. Guide to advanced empirical software engineering pp.\
    \ 201–228 (2008)\n- <span id=\"page-47-3\"></span>65. Juergens, E., Deissenboeck,\
    \ F.: How much is a clone. In: Proceedings of the 4th International Workshop on\
    \ Software Quality and Maintainability, pp. 79–88 (2010)\n- <span id=\"page-47-19\"\
    ></span>66. Juristo, N., Vegas, S.: The role of non-exact replications in software\
    \ engineering experiments. Empirical Software Engineering 16, 295–324 (2011)\n\
    - <span id=\"page-47-10\"></span>67. Kamsties, E., von Knethen, A., Philipps,\
    \ J.: An empirical investigation of requirements specification languages: Detecting\
    \ defects while formalizing requirements. In: Information Modeling Methods and\
    \ Methodologies: Advanced Topics in Database Research, pp. 125–147. IGI Global\
    \ (2005)\n- <span id=\"page-47-9\"></span>68. Kamsties, E., Peach, B.: Taming\
    \ ambiguity in natural language requirements. In: Proceedings of the Thirteenth\
    \ international conference on Software and Systems Engineering and Applications,\
    \ vol. 1315 (2000)\n- <span id=\"page-47-17\"></span>69. King, B.M., Rosopa, P.J.,\
    \ Minium, E.W.: Statistical reasoning in the behavioral sciences. John Wiley &\
    \ Sons (2018)\n- <span id=\"page-47-15\"></span>70. Kitchenham, B., Fry, J., Linkman,\
    \ S.: The case against cross-over designs in software engineering. In: Eleventh\
    \ annual international workshop on software technology and engineering practice,\
    \ pp. 65–67. IEEE (2003)\n- <span id=\"page-47-8\"></span>71. Knauss, E., Schneider,\
    \ K., Stapel, K.: Learning to write better requirements through heuristic critiques.\
    \ In: 2009 17th IEEE International Requirements Engineering Conference, pp. 387–388.\
    \ IEEE (2009)\n- <span id=\"page-47-7\"></span>72. Kof, L.: Treatment of passive\
    \ voice and conjunctions in use case documents. In: Natural Language Processing\
    \ and Information Systems: 12th International Conference on Applications of Natural\
    \ Language to Information Systems, NLDB 2007, Paris, France, June 27-29, 2007.\
    \ Proceedings 12, pp. 181–192. Springer (2007)\n- <span id=\"page-47-5\"></span>73.\
    \ Krisch, J., Houdek, F.: The myth of bad passive voice and weak words an empirical\
    \ investigation in the automotive industry. In: 2015 IEEE 23rd International Requirements\
    \ Engineering Conference (RE), pp. 344–351. IEEE (2015)\n- <span id=\"page-47-11\"\
    ></span>74. Lev´en, W., Broman, H., Besker, T., Torkar, R.: The Broken Windows\
    \ Theory applies to technical debt. arXiv preprint arXiv:2209.01549 (2022)\n-\
    \ <span id=\"page-47-20\"></span>75. Martyniuk, W.: Common european framework\
    \ of reference for languages: Learning, teaching, assessment (cefr)–a synopsis.\
    \ In: Annual meeting of the consortium for language teaching and learning cornell\
    \ university. Concil of Europe, Language policy division. https://rm. coe. int/16802fc1bf\
    \ (2006)\n- <span id=\"page-47-4\"></span>76. McElreath, R.: Statistical rethinking:\
    \ A Bayesian course with examples in R and Stan. CRC press (2020)\n- <span id=\"\
    page-47-2\"></span>77. M´endez, D., Wagner, S., Kalinowski, M., Felderer, M.,\
    \ Mafra, P., Vetr`o, A., Conte, T., Christiansson, M.T., Greer, D., Lassenius,\
    \ C., et al.: Naming the pain in requirements engineering: Contemporary problems,\
    \ causes, and effects in practice. Empirical software engineering 22, 2298–2338\
    \ (2017)\n- <span id=\"page-47-0\"></span>78. M´endez Fern´andez, D., B¨ohm, W.,\
    \ Vogelsang, A., Mund, J., Broy, M., Kuhrmann, M., Weyer, T.: Artefacts in software\
    \ engineering: A fundamental positioning. Software & Systems Modeling 18, 2777–2786\
    \ (2019)\n- <span id=\"page-47-1\"></span>79. M´endez Fern´andez, D., Penzenstadler,\
    \ B.: Artefact-based requirements engineering: the AMDiRE approach. Requirements\
    \ Engineering 20, 405–434 (2015)\n- <span id=\"page-48-0\"></span>80. Montgomery,\
    \ L., Fucci, D., Bouraffa, A., Scholz, L., Maalej, W.: Empirical research on requirements\
    \ quality: A systematic mapping study. Requirements Engineering 27(2), 183–209\
    \ (2022)\n- <span id=\"page-48-1\"></span>81. Mund, J., Fernandez, D.M., Femmer,\
    \ H., Eckhardt, J.: Does quality of requirements specifications matter? Combined\
    \ results of two empirical studies. In: 2015 ACM/IEEE International Symposium\
    \ on Empirical Software Engineering and Measurement (ESEM), pp. 1–10. IEEE (2015)\n\
    - <span id=\"page-48-20\"></span>82. Nilsson, A., Bonander, C., Str¨omberg, U.,\
    \ Bj¨ork, J.: A directed acyclic graph for interactions. International Journal\
    \ of Epidemiology 50(2), 613–619 (2021)\n- <span id=\"page-48-2\"></span>83. Nosek,\
    \ B.A., Errington, T.M.: What is replication? PLoS biology 18(3), e3000691 (2020)\n\
    - <span id=\"page-48-9\"></span>84. Nuseibeh, B., Easterbrook, S.: Requirements\
    \ engineering: A roadmap. In: Proceedings of the Conference on the Future of Software\
    \ Engineering, pp. 35–46 (2000)\n- <span id=\"page-48-4\"></span>85. O'Grady,\
    \ W., Archibald, J., Aronoff, M., Rees-Miller, J.: Contemporary Linguistics: An\
    \ Introduction. Bedford/St. Martin's, Boston (2001)\n- <span id=\"page-48-6\"\
    ></span>86. Parra, E., Dimou, C., Llorens, J., Moreno, V., Fraga, A.: A methodology\
    \ for the classification of quality of requirements using machine learning techniques.\
    \ Information and Software Technology 67, 180–195 (2015)\n- <span id=\"page-48-14\"\
    ></span>87. Pearl, J.: From Bayesian networks to causal networks. In: Mathematical\
    \ models for handling partial knowledge in artificial intelligence, pp. 157–182.\
    \ Springer (1995)\n- <span id=\"page-48-15\"></span>88. Pearl, J., Glymour, M.,\
    \ Jewell, N.P.: Causal inference in statistics: A primer. John Wiley & Sons (2016)\n\
    - <span id=\"page-48-13\"></span>89. Petersen, K., Wohlin, C.: Context in industrial\
    \ software engineering research. In: 2009 3rd International Symposium on Empirical\
    \ Software Engineering and Measurement, pp. 401–404. IEEE (2009)\n- <span id=\"\
    page-48-12\"></span>90. Phalp, K.T., Vincent, J., Cox, K.: Assessing the quality\
    \ of use case descriptions. Software Quality Journal 15(1), 69–97 (2007)\n- <span\
    \ id=\"page-48-3\"></span>91. Philippo, E.J., Heijstek, W., Kruiswijk, B., Chaudron,\
    \ M.R., Berry, D.M.: Requirement ambiguity not as important as expected—results\
    \ of an empirical evaluation. In: Requirements Engineering: Foundation for Software\
    \ Quality: 19th International Working Conference, REFSQ 2013, Essen, Germany,\
    \ April 8-11, 2013. Proceedings 19, pp. 65–79. Springer (2013)\n- <span id=\"\
    page-48-18\"></span>92. Pickard, L.M., Kitchenham, B.A., Jones, P.W.: Combining\
    \ empirical results in software engineering. Information and software technology\
    \ 40(14), 811–821 (1998)\n- <span id=\"page-48-8\"></span>93. Poesio, M.: Semantic\
    \ ambiguity and perceived ambiguity. In: K. van Deemter, S. Peters (eds.) Semantic\
    \ Ambiguity and Underspecification. Center for the Study of Language and Inf,\
    \ United Kingdom (1996). DOI 10.48550/arXiv.cmp-lg/9505034\n- <span id=\"page-48-5\"\
    ></span>94. Pohl, K.: Requirements engineering fundamentals: A study guide for\
    \ the certified professional for requirements engineering exam-foundation level-IREB\
    \ compliant. Rocky Nook, Inc. (2016)\n- <span id=\"page-48-7\"></span>95. Rosadini,\
    \ B., Ferrari, A., Gori, G., Fantechi, A., Gnesi, S., Trotta, I., Bacherini, S.:\
    \ Using NLP to detect requirements defects: An industrial experience in the railway\
    \ domain. In: Requirements Engineering: Foundation for Software Quality: 23rd\
    \ International Working Conference, REFSQ 2017, Essen, Germany, February 27–March\
    \ 2, 2017, Proceedings 23, pp. 344–360. Springer (2017)\n- <span id=\"page-48-16\"\
    ></span>96. Russo, D., Stol, K.J.: Gender differences in personality traits of\
    \ software engineers. IEEE Transactions on Software Engineering 48(3), 819–834\
    \ (2020)\n- <span id=\"page-48-17\"></span>97. Salman, I., Misirli, A.T., Juristo,\
    \ N.: Are students representatives of professionals in software engineering experiments?\
    \ In: 2015 IEEE/ACM 37th IEEE international conference on software engineering,\
    \ vol. 1, pp. 666–676. IEEE (2015)\n- <span id=\"page-48-10\"></span>98. Shah,\
    \ U.S., Jinwala, D.C.: Resolving ambiguity in natural language specification to\
    \ generate UML diagrams for requirements specification. International Journal\
    \ of Software Engineering, Technology and Applications 1(2-4), 308–334 (2015)\n\
    - <span id=\"page-48-19\"></span>99. Shapiro, S.S., Wilk, M.B.: An analysis of\
    \ variance test for normality (complete samples). Biometrika 52(3/4), 591–611\
    \ (1965)\n- <span id=\"page-48-11\"></span>100. Sharma, R., Sharma, N., Biswas,\
    \ K.: Machine learning for detecting pronominal anaphora ambiguity in NL requirements.\
    \ In: 2016 4th Intl Conf on Applied Computing and Information Technology/3rd Intl\
    \ Conf on Computational Science/Intelligence and\n\nApplied Informatics/1st Intl\
    \ Conf on Big Data, Cloud Computing, Data Science & Engineering (ACIT-CSII-BCD),\
    \ pp. 177–182. IEEE (2016)\n\n- <span id=\"page-49-14\"></span>101. Siebert, J.:\
    \ Applications of statistical causal inference in software engineering. Information\
    \ and Software Technology p. 107198 (2023)\n- <span id=\"page-49-12\"></span>102.\
    \ Sjøberg, D.I., Anda, B., Arisholm, E., Dyb˚a, T., Jørgensen, M., Karahasanovi´c,\
    \ A., Vok´aˇc, M.: Challenges and recommendations when increasing the realism\
    \ of controlled software engineering experiments. In: Empirical Methods and Studies\
    \ in Software Engineering: Experiences from ESERNET, pp. 24–38. Springer (2003).\
    \ DOI 10.1007/ 978-3-540-45143-3 3\n- <span id=\"page-49-3\"></span>103. Soeken,\
    \ M., Abdessaied, N., Allahyari-Abhari, A., Buzo, A., Musat, L., Pelz, G., Drechsler,\
    \ R.: Quality assessment for requirements based on natural language processing.\
    \ In: Forum on Specification and Design Languages. Proceedings. Citeseer (2014)\n\
    - <span id=\"page-49-0\"></span>104. Stol, K.J., Fitzgerald, B.: The ABC of software\
    \ engineering research. ACM Transactions on Software Engineering and Methodology\
    \ (TOSEM) 27(3), 1–51 (2018)\n- <span id=\"page-49-9\"></span>105. Svensson, R.B.,\
    \ Feldt, R., Torkar, R.: The unfulfilled potential of data-driven decision making\
    \ in agile software development. In: Agile Processes in Software Engineering and\
    \ Extreme Programming: 20th International Conference, XP 2019, Montr´eal, QC,\
    \ Canada, May 21–25, 2019, Proceedings 20, pp. 69–85. Springer (2019)\n- <span\
    \ id=\"page-49-7\"></span>106. Torkar, R., Feldt, R., Furia, C.A.: Bayesian data\
    \ analysis in empirical software engineering: The case of missing data, pp. 289–324.\
    \ Springer International Publishing, Cham (2020). DOI 10.1007/978-3-030-32489-6\
    \ 11\n- <span id=\"page-49-11\"></span>107. Vegas, S., Apa, C., Juristo, N.: Crossover\
    \ designs in software engineering experiments: Benefits and perils. IEEE Transactions\
    \ on Software Engineering 42(2), 120–135 (2015). DOI 10.1109/TSE.2015.2467378\n\
    - <span id=\"page-49-8\"></span>108. Vieira, R., Mesquita, D., Mattos, C.L., Britto,\
    \ R., Rocha, L., Gomes, J.: Bayesian analysis of bug-fixing time using report\
    \ data. In: Proceedings of the 16th ACM/IEEE International Symposium on Empirical\
    \ Software Engineering and Measurement, pp. 57–68 (2022)\n- <span id=\"page-49-1\"\
    ></span>109. Wagner, S., Fern´andez, D.M., Felderer, M., Vetr`o, A., Kalinowski,\
    \ M., Wieringa, R., Pfahl, D., Conte, T., Christiansson, M.T., Greer, D., et al.:\
    \ Status quo in requirements engineering: A theory and a global family of surveys.\
    \ ACM Transactions on Software Engineering and Methodology (TOSEM) 28(2), 1–48\
    \ (2019)\n- <span id=\"page-49-6\"></span>110. Wagner, S., Lochmann, K., Heinemann,\
    \ L., Kl¨as, M., Trendowicz, A., Pl¨osch, R., Seidi, A., Goeb, A., Streit, J.:\
    \ The Quamoco product quality modelling and assessment approach. In: 2012 34th\
    \ International Conference on Software Engineering (ICSE), pp. 1133–1142. IEEE\
    \ (2012)\n- <span id=\"page-49-15\"></span>111. Wesner, J.S., Pomeranz, J.P.:\
    \ Choosing priors in Bayesian ecological models by simulating from the prior predictive\
    \ distribution. Ecosphere 12(9), e03739 (2021)\n- <span id=\"page-49-13\"></span>112.\
    \ Wilcoxon, F.: Individual comparisons by ranking methods. biom bull 1 (6): 80–83\
    \ (1945)\n- <span id=\"page-49-10\"></span>113. Wohlin, C., Runeson, P., H¨ost,\
    \ M., Ohlsson, M.C., Regnell, B., Wessl´en, A.: Experimentation in software engineering.\
    \ Springer Science & Business Media (2012)\n- <span id=\"page-49-4\"></span>114.\
    \ Yang, H., De Roeck, A., Gervasi, V., Willis, A., Nuseibeh, B.: Extending nocuous\
    \ ambiguity analysis for anaphora in natural language requirements. In: 2010 18th\
    \ IEEE International Requirements Engineering Conference, pp. 25–34. IEEE (2010)\n\
    - <span id=\"page-49-5\"></span>115. Yang, H., De Roeck, A., Gervasi, V., Willis,\
    \ A., Nuseibeh, B.: Analysing anaphoric ambiguity in natural language requirements.\
    \ Requirements engineering 16, 163–189 (2011)\n- <span id=\"page-49-2\"></span>116.\
    \ Zhao, L., Alhoshan, W., Ferrari, A., Letsholo, K.J., Ajagbe, M.A., Chioasca,\
    \ E.V., Batista-Navarro, R.T.: Natural language processing for requirements engineering:\
    \ A systematic mapping study. ACM Computing Surveys (CSUR) 54(3), 1–41 (2021)"
  decisions:
    language: '- Qualified. Reason: English Paper'
    evaluation_prompt: Qualified.
    related_work_prompt: Qualified
    novelty_prompt: Qualified.
    review_only_prompt: Qualified.
  llm_input_used: '## Abstract

    It is commonly accepted that the quality of requirements specifications

    impacts subsequent software engineering activities. However, we still lack

    empirical evidence to support organizations in deciding whether their

    requirements are good enough or impede subsequent activities. We aim to

    contribute empirical evidence to the effect that requirements quality defects

    have on a software engineering activity that depends on this requirement. We

    conduct a controlled experiment in which 25 participants from industry and

    university generate domain models from four natural language requirements

    containing different quality defects. We evaluate the resulting models using

    both frequentist and Bayesian data analysis. Contrary to our expectations, our

    results show that the use of passive voice only has a minor impact on the

    resulting domain models. The use of ambiguous pronouns, however, shows a strong

    effect on various properties of the resulting domain models. Most notably,

    ambiguous pronouns lead to incorrect associations in domain models. Despite

    being equally advised against by literature and frequentist methods, the

    Bayesian data analysis shows that the two investigated quality defects have

    vastly different impacts on software engineering activities and, hence, deserve

    different levels of attention. Our employed method can be further utilized by

    researchers to improve reliable, detailed empirical evidence on requirements

    quality.


    ## Introduction

    Software requirements specify the needs and constraints that stakeholders impose
    on a desired system. Software requirements specifications (SRS), the explicit
    manifestation of requirements as an artifact [\[78\]](#page-47-0), serve as input
    for various subsequent software engineering (SE) activities, such as deriving
    a software architecture, implementing features, or generating test cases [\[79\]](#page-47-1).
    As a consequence, the quality of an SRS impacts the quality of requirements-dependent
    activities [\[37,](#page-45-0) [38,](#page-45-1) [48\]](#page-46-0). A quality
    defect in an SRS—for example, an ambiguous formulation—can cause differing interpretations
    and result in the design and implementation of a solution that does not meet the
    stakeholders'' needs [\[77\]](#page-47-2). The inherent complexity of natural
    language (NL), which is most commonly used for specifying requirements [\[43\]](#page-46-1),
    aggravates this challenge further. Since quality defects are understood to scale
    in cost for removal [\[9\]](#page-44-0), organizations are interested in identifying
    and removing these defects as early as possible [\[80\]](#page-48-0).


    Within the requirements engineering (RE) research domain, the field of requirements
    quality research aims to meet this challenge [\[80\]](#page-48-0). Requirements
    quality research has already identified several attributes of requirements quality
    [\[80\]](#page-48-0) (e.g., unambiguity, completeness, consistency) and proposes
    quality factors, i.e., requirements writing rules (e.g., the use of passive voice
    being associated with bad quality [\[36\]](#page-45-2)) as well as tools that
    automatically detect alleged quality defects [\[35\]](#page-45-3). However, existing
    approaches fall short in at least three regards [\[48\]](#page-46-0): i) only
    a fraction of publications provide empirical evidence that would demonstrate the
    impact of quality defects [\[80\]](#page-48-0), ii) the few empirical studies
    that do so largely ignore potentially confounding context factors [\[65,](#page-47-3)
    [81\]](#page-48-1), and iii) the analyses conducted in existing publications do
    not go beyond binary insights (i.e., a quality factor does have an impact or it
    does not) [\[25,](#page-45-4) [36\]](#page-45-2). These gaps have impeded the
    adoption of requirements quality research in practice [\[43\]](#page-46-1).


    In this article, we aim to address the above-mentioned shortcomings by i) conducting
    a controlled experiment with 25 participants simulating a requirementsdependent
    activity (i.e., domain modeling) using four natural-language requirements as input.
    The experiment contributes empirical evidence on the impact of two commonly researched
    quality factors passive voice [\[36\]](#page-45-2) and ambiguous pronouns [\[31\]](#page-45-5).
    The investigation of the impact of passive voice is


    a conceptual replication [\[2\]](#page-44-1) of the only controlled experiment
    studying the impact of passive voice on domain modeling [\[36\]](#page-45-2) known
    to us. Therefore, our experiment also strengthens the robustness of their conclusions
    by providing diagnostic evidence [\[83\]](#page-48-2). Further, we ii) collect
    data about relevant context factors such as experience in software engineering
    (SE) and RE, domain knowledge, and task experience, and integrate these data in
    our data analysis. Finally, we iii) contrast the state-of-the-art frequentist
    data analysis (FDA) with Bayesian data analysis (BDA), which entails both a causal
    framework and Bayesian modeling for statistical causal inference [\[76\]](#page-47-4).
    The latter has recently been popularized in SE research [\[52\]](#page-46-2) since
    it generates more nuanced empirical insights. Our study is categorized as a laboratory
    experiment in a contrived setting [\[104\]](#page-49-0), isolating the effect
    of the selected quality factors of interest. The causal inference of their impact
    contributes to our long-term goal of providing an empirically grounded understanding
    of the impact of requirements quality. This will support organizations in assessing
    their requirements and detecting relevant quality defects early.


    This paper makes the following contributions:


    - 1. a controlled experiment investigating the impact of requirements quality;

    - 2. a conceptual replication of the only controlled experiment investigating
    the impact of passive voice [\[36\]](#page-45-2);

    - 3. the application of BDA to requirements quality research, which is among the
    first of its kind in RE; and

    - 4. an archived replication package containing all supplementary material, including
    protocols and guidelines for data collection and extraction, the raw data, analysis
    scripts, figures, and results [\[46\]](#page-46-3).


    The remainder of this manuscript is organized as follows. Section [2](#page-2-0)
    introduces relevant related work. We present our research method in Section [3](#page-9-0)
    and the results in Section [4.](#page-28-0) We discuss these results in Section
    [5](#page-36-0) before concluding our manuscript in Section [6.](#page-43-0)'
  token_usage: 6464
  time_usage: 2.4181718826293945
- title: "Flexible Control Flow Graph Alignment for Delivering Data-Driven\n  Feedback\
    \ to Novice Programming Learners"
  abstract: 'Supporting learners in introductory programming assignments at scale
    is a

    necessity. This support includes automated feedback on what learners did

    incorrectly. Existing approaches cast the problem as automatically repairing

    learners'' incorrect programs extrapolating the data from an existing correct

    program from other learners. However, such approaches are limited because they

    only compare programs with similar control flow and order of statements. A

    potentially valuable set of repair feedback from flexible comparisons is thus

    missing. In this paper, we present several modifications to CLARA, a

    data-driven automated repair approach that is open source, to deal with

    real-world introductory programs. We extend CLARA''s abstract syntax tree

    processor to handle common introductory programming constructs. Additionally,

    we propose a flexible alignment algorithm over control flow graphs where we

    enrich nodes with semantic annotations extracted from programs using operations

    and calls. Using this alignment, we modify an incorrect program''s control flow

    graph to match the correct programs to apply CLARA''s original repair process.

    We evaluate our approach against a baseline on the twenty most popular

    programming problems in Codeforces. Our results indicate that flexible

    alignment has a significantly higher percentage of successful repairs at 46%

    compared to 5% for baseline CLARA. Our implementation is available at

    https://github.com/towhidabsar/clara.'
  url: http://arxiv.org/abs/2401.01416v1
  keywords: ''
  document: "# Flexible Control Flow Graph Alignment for Delivering Data-Driven Feedback\
    \ to Novice Programming Learners\n\nMd Towhidul Absar Chowdhury<sup>a</sup> ,\
    \ Maheen Riaz Contractor<sup>a</sup> , Carlos R. Rivero<sup>a</sup>\n\n> <sup>a</sup>Rochester\
    \ Institute of Technology, One Lomb Memorial Dr., Rochester, 14623, NY, USA\n\n\
    ### Abstract\n\nSupporting learners in introductory programming assignments at\
    \ scale is a necessity. This support includes automated feedback on what learners\
    \ did incorrectly. Existing approaches cast the problem as automatically repairing\
    \ learners' incorrect programs extrapolating the data from an existing correct\
    \ program from other learners. However, such approaches are limited because they\
    \ only compare programs with similar control flow and order of statements. A potentially\
    \ valuable set of repair feedback from flexible comparisons is thus missing. In\
    \ this paper, we present several modifications to CLARA, a data-driven automated\
    \ repair approach that is open source, to deal with real-world introductory programs.\
    \ We extend CLARA's abstract syntax tree processor to handle common introductory\
    \ programming constructs. Additionally, we propose a flexible alignment algorithm\
    \ over control flow graphs where we enrich nodes with semantic annotations extracted\
    \ from programs using operations and calls. Using this alignment, we modify an\
    \ incorrect program's control flow graph to match correct programs to apply CLARA's\
    \ original repair process. We evaluate our approach against a baseline on the\
    \ twenty most popular programming problems in Codeforces. Our results indicate\
    \ that flexible alignment has a significantly higher percentage of successful\
    \ repairs at 46% compared to 5% for baseline CLARA. Our implementation is available\
    \ at <https://github.com/towhidabsar/clara>.\n\nKeywords: Automated Program Repair,\
    \ Control Flow Graph,\n\nEmail addresses: mac9908@rit.edu (Md Towhidul Absar Chowdhury),\
    \ mc1927@rit.edu (Maheen Riaz Contractor), crr@cs.rit.edu (Carlos R. Rivero)\n\
    \n### 1. Introduction\n\nThe worldwide interest in computer science has originated\
    \ an unprecedented growth in the number of novice programming learners in both\
    \ traditional and online settings [\\[1–](#page-44-0)[4\\]](#page-45-0). In the\
    \ latter case, the number of novices taking programming Massive Open Online Courses\
    \ and/or practicing using programming online judges has scaled to millions [\\\
    [5,](#page-45-1) [6\\]](#page-45-2). One of the main challenges in the aforementioned\
    \ context is supporting novice programming learners at scale [\\[7\\]](#page-45-3),\
    \ which typically consists of delivering feedback explaining what and why they\
    \ did incorrectly in their programs [\\[8\\]](#page-45-4). Note that, different\
    \ than traditional settings, online programming settings often have a large proportion\
    \ of novice learners with a variety of backgrounds, who usually tend to need a\
    \ more direct level of feedback and assistance [\\[9\\]](#page-45-5). A common\
    \ practice to address such a challenge is to rely on functional tests; however,\
    \ feedback generated based solely on test cases does not sufficiently support\
    \ novice learners [\\[7,](#page-45-3) [10\\]](#page-45-6).\n\nCurrent approaches\
    \ cast the problem of delivering feedback to novices at scale as automatically\
    \ repairing their incorrect programs [\\[3,](#page-44-1) [7,](#page-45-3) [10–](#page-45-6)[13\\\
    ]](#page-45-7). Note that, similar to existing approaches, we consider a program\
    \ to be correct if it passes a number of predefined test cases [\\[3,](#page-44-1)\
    \ [10\\]](#page-45-6); otherwise, it is incorrect. Once a repair is found, it\
    \ can be used to determine pieces of feedback to deliver to learners [\\[7\\]](#page-45-3).\
    \ Non-data-driven approaches aim to find repairs by mutating incorrect programs\
    \ until they are correct, i.e., they pass all test cases [\\[14\\]](#page-45-8).\
    \ Data-driven approaches exploit the fact that repairs can be found in existing\
    \ correct programs and extrapolated to a given incorrect program [\\[3\\]](#page-44-1).\
    \ This paper focuses on the latter since, in a given programming assignment, there\
    \ is usually a variety of correct programs provided by other learners that can\
    \ be exploited to repair incorrect programs [\\[3,](#page-44-1) [10,](#page-45-6)\
    \ [12,](#page-45-9) [13\\]](#page-45-7).\n\nThe \"search, align and repair\" [\\\
    [3\\]](#page-44-1) framework consists of the following steps: 1) Given an incorrect\
    \ program p<sup>i</sup> , search for a correct program p<sup>c</sup> that may\
    \ be useful to repair p<sup>i</sup> ; 2) Align p<sup>i</sup> with respect to p<sup>c</sup>\
    \ to identify discrepancies and potential modifications in order to repair p<sup>i</sup>\
    \ ; and 3) Apply those modifications to p<sup>i</sup> until the resulting program\
    \ p ′ <sup>i</sup> passes all test cases. Current approaches instantiating the\
    \ \"search, align and repair\" framework use rigid comparisons to align incorrect\
    \ and correct programs, i.e., they require the programs to have the same or very\
    \ similar control flows (conditions and loops), and they are affected by the order\
    \ of program statements [\\[3,](#page-44-1) [10,](#page-45-6) [12,](#page-45-9)\
    \ [13\\]](#page-45-7). As a result, such approaches may miss a potentially valuable\
    \ set of correct programs that can repair incorrect programs using flexible program\
    \ comparisons.\n\nIn this paper, we focus on CLARA [\\[10\\]](#page-45-6), a \"\
    search, align and repair\" approach that is open source. We first adapt the original\
    \ implementation of CLARA to support introductory programming assignments. This\
    \ adaption involves non-trivial modifications to the parser and interpreter to\
    \ support various constructs, such as print to and read from the console, import\
    \ statements, built-in Python functions, and more. After these modifications,\
    \ we also need to adapt the alignment and repair processes. Based on these foundations,\
    \ we propose a flexible alignment algorithm that relies on control flow graphs.\
    \ It exploits the semantic information (operations and calls) to annotate the\
    \ graphs, and their topology information (edges, i.e., True and False transitions).\
    \ In order to evaluate the proposed algorithm, we create a dataset of incorrect\
    \ and correct programs for the twenty most popular programming problems in the\
    \ Codeforces online platform. Then, using the dataset, we execute CLARA's baseline\
    \ repair process and our flexible alignment repair to compare both the quantitative\
    \ and qualitative performance of the proposed technique. Furthermore, we include\
    \ another \"search, align and repair\" approach, Sarfgen [\\[3\\]](#page-44-1),\
    \ by utilizing a similar process as our flexible alignment, but enforcing a high\
    \ similarity between compared programs. This simulates the rigidity in program\
    \ comparisons applied by Sarfgen. Note that Sarfgen is not publicly available;\
    \ therefore, we needed to simulate it.\n\nTwo short versions of this paper have\
    \ been published elsewhere [\\[15,](#page-46-0) [16\\]](#page-46-1). In this paper,\
    \ we describe in detail all the modifications that we made to CLARA, and how the\
    \ parser and interpreter were updated. We also present our flexible alignment\
    \ algorithm as well as the changes made to the programs at hand after an alignment\
    \ is computed. These changes are necessary in order to apply CLARA's repair process.\
    \ Finally, we have significantly expanded our experiments to show the performance\
    \ of our modifications over twenty real-world introductory programming assignments.\
    \ We have made the implementation of this version of CLARA and our experimental\
    \ dataset publicly available.[1](#page-2-0)\n\n<span id=\"page-2-0\"></span><sup>1</sup><https://github.com/towhidabsar/clara>\
    \ The fundamental contributions of this pa-\n\nThe paper is organized as follows:\
    \ Section [2](#page-3-0) summarizes previous approaches and ours; Section [3](#page-6-0)\
    \ introduces CLARA's parser, interpreter, aligner and repairer; Sections [4](#page-14-0)\
    \ and [5](#page-19-0) describe our modifications to the parser and interpreter,\
    \ and aligner and repairer of CLARA's original implementation, respectively; Section\
    \ [6](#page-21-0) presents our flexible alignment approach and the necessary modifications\
    \ to CLARA's repairer; Section [7](#page-30-0) discusses our experimental results;\
    \ Section [8](#page-41-0) presents the related work; and Section [9](#page-43-0)\
    \ presents our conclusions and future work.\n\n### <span id=\"page-3-0\"></span>2.\
    \ Overview\n\nWe consider CLARA [\\[10\\]](#page-45-6), Refazer [\\[13\\]](#page-45-7),\
    \ Sarfgen [\\[3\\]](#page-44-1), and sk p [\\[12\\]](#page-45-9) the state of\
    \ the art in searching, aligning and repairing programs. CLARA and Sarfgen compare\
    \ variable traces between an incorrect and a correct programs that share the same\
    \ control statements like if or while. Refazer uses pairs of incorrect/correct\
    \ program samples to learn transformation rules, which aid a program synthesizer\
    \ to transform incorrect into correct programs. Finally, sk p uses partial fragments\
    \ of contiguous statements to train a neural network to predict possible repairs.\n\
    \nIn the alignment step, these approaches compare an incorrect program with respect\
    \ to a correct program based on rigid schemes, which limits their repair potential.\
    \ To illustrate our claim, we use the Python programs presented in Figure [1,](#page-4-0)\
    \ which aim to compute the minimum value in an array and the sum of all its elements,\
    \ and print both minimum and sum values to console. Note that the values of the\
    \ input array are assumed to be always less or equal than 100. In Sarfgen, an\
    \ incorrect program will be only repaired if its control statements match with\
    \ the control statements of an existing correct program. This is a hard constraint\
    \ since: a) It requires a correct program with the same control statements to\
    \ exist, and b) Such a correct program may not \"naturally\" exist. For instance,\
    \ the control statements of\n\nper are as follows: 1) Many of the modifications\
    \ we made to CLARA's parser and interpreter for Python programs also apply to\
    \ other programming languages like C and Java; 2) Both our flexible alignment\
    \ and model recreation algorithms can be used by any \"search, align and repair\"\
    \ approach based on control flow graphs and program expressions; 3) Our dataset\
    \ is one of the very few publicly-available datasets in the context of real-world\
    \ introductory programming assignments; 4) Our threshold-based solution to simulate\
    \ other alignment approaches is useful when existing approaches are not publicly\
    \ available.\n\n<span id=\"page-4-8\"></span><span id=\"page-4-7\"></span><span\
    \ id=\"page-4-6\"></span><span id=\"page-4-5\"></span><span id=\"page-4-4\"></span><span\
    \ id=\"page-4-3\"></span><span id=\"page-4-2\"></span><span id=\"page-4-1\"></span><span\
    \ id=\"page-4-0\"></span>![](_page_4_Figure_0.jpeg)\n\n<span id=\"page-4-10\"\
    ></span><span id=\"page-4-9\"></span>Figure 1: Correct and incorrect programs,\
    \ edits of abstract syntax trees derived from the programs and code fragments\n\
    \nthe correct program in Figure [1a](#page-4-3) do not match with the incorrect\
    \ program in Figure [1b;](#page-4-4) in order to match, the correct program should\
    \ \"artificially\" contain an if statement before or after line [8,](#page-4-5)\
    \ and such a statement should not modify the final output of the program. CLARA\
    \ relaxes these constraints such that, outside loop statements, both programs\
    \ can have different control statements, but they need to have the same inside\
    \ loops. This relaxation still forces a correct program with the same loop signature\
    \ to exist.\n\nRefazer exploits the tree edit distance between two programs to\
    \ find discrepancies between them; however, the tree edit distance between two\
    \ equivalent abstract syntax trees with different order of statements implies\
    \ multiple edits. For example, Figure [1c](#page-4-6) shows an excerpt of the\
    \ edits to transform the abstract syntax tree of the correct into the incorrect\
    \ program in our example, which implies removing and adding full subtrees; however,\
    \ only two edits would be necessary, i.e., changing \"<\" by \">\" and removing\
    \ the subtree formed by lines [9–](#page-4-7)[10](#page-4-8) in Figure [1b.](#page-4-4)\
    \ In sk p, different order of statements result in different partial fragments,\
    \ so additional correct programs will be required to train the program repairer.\
    \ For instance, Figure [1d](#page-4-9) shows a fragment extracted from the correct\
    \ program; however, the incorrect program will only be fixed by a fragment like\
    \ the one in Figure [1e.](#page-4-10)\n\nWe propose an alignment step based on\
    \ flexible alignment of control flow graphs. The first step consists of transforming\
    \ programs into control flow graphs that encode the True and False transitions\
    \ of the program at hand. For instance, the while loop in Figure [1a](#page-4-3)\
    \ (line [4\\)](#page-4-1) is encoded by three nodes in the graph: the guard, the\
    \ body and the end of the loop. There are transitions (edges) between these nodes.\
    \ For example, a True transition between the guard and the body encodes that the\
    \ guard is fulfilled, so the body is executed. These nodes are further annotated\
    \ with semantic labels. For example, the guard of the loop contains the following\
    \ labels: cond, indicating it is a Boolean condition, Lt, because there is a less\
    \ than operator, and len, which corresponds to the len call. We apply flexible\
    \ graph alignment over two (correct and incorrect) control flow graphs G<sup>C</sup>\
    \ and G<sup>I</sup> . Assume a given permutation of nodes ϕ such that every node\
    \ u<sup>i</sup> ∈ G<sup>C</sup> corresponds to a node v<sup>j</sup> ∈ G<sup>I</sup>\
    \ , i.e., ϕ(ui) = v<sup>j</sup> . We compute the similarity of ϕ as the similarity\
    \ between the labels of u<sup>i</sup> and v<sup>j</sup> (semantic similarity),\
    \ and the transitions (edges) outgoing from u<sup>i</sup> and v<sup>j</sup> (topology\
    \ similarity). We select the permutation with lowest similarity as the best alignment.\n\
    \n<span id=\"page-6-1\"></span>![](_page_6_Figure_0.jpeg)\n\nFigure 2: CLARA's\
    \ workflow: each program is translated into a model. Both models are aligned.\
    \ If they match, the repairer and the interpreter exchange model and trace information\
    \ using a test case until the incorrect program's model passes such test case.\n\
    \n### <span id=\"page-6-0\"></span>3. Introduction to CLARA [\\[10\\]](#page-45-6)\n\
    \nThe automated CLustering And program RepAir tool, CLARA [\\[10\\]](#page-45-6),\
    \ was first introduced in 2016. CLARA helps provide feedback to students in introductory\
    \ programming assignments. Even though CLARA supports C++, Java and Python, we\
    \ focus on the latter in this paper. Figure [2](#page-6-1) presents CLARA's workflow.\
    \ The abstract syntax tree processor receives each correct and incorrect programs\
    \ as input, parses them, and creates two models, one for each program. These models\
    \ are aligned: if a match is found, the repairer uses it as well as a test case\
    \ to find potential errors and fix them. Error detection is achieved by comparing\
    \ variable traces between the correct and incorrect programs.\n\n### 3.1. Models,\
    \ processing and interpreting\n\nBefore performing any repairs, CLARA creates\
    \ models for every input program. CLARA exploits Python's ast module, which is\
    \ a standard library that helps generate abstract syntax trees from Python source\
    \ code and manipulate them. An abstract syntax tree is a graphical representation\
    \ of a piece of source code containing nodes, where every node represents a language\
    \ construct or operation like If, Return and Import [\\[17\\]](#page-46-2). CLARA\
    \ traverses the returned abstract syntax tree, node by node, and creates a model.\
    \ For every part of the abstract syntax tree, such as FunctionDef (function definition),\
    \ Expr (expression) or Call (function call), there is a different processing function\
    \ that has a different representation in the model.\n\nFunctions and locations.\
    \ The entire model consists of one program containing multiple functions. Each\
    \ function is partitioned based on its control flow information. Control flow\
    \ information captures the order in which statements are evaluated. An example\
    \ of a control flow statement is an If statement, as it adds a new possible path\
    \ for the program to take. Locations, a construct in CLARA's model, represent\
    \ control flow information. Each function thus contains multiple locations. Locations\
    \ contain expressions and are created based on branching control flow statements\
    \ like If, While or For statements. Other statements like function calls or sequencing\
    \ of statements are not involved in location creation. As a result, if a function\
    \ contains no branching control flow statements, it will only contain a single\
    \ location. Locations also contain information about which location to go to next,\
    \ called transitions. There are two types of transitions, True and False. A True\
    \ transition contains the following location to go to if the conditional expression\
    \ inside the location evaluates to true. A False transition contains the location\
    \ to visit next if the expression inside the location evaluates to false.\n\n\
    However, it is possible to have expressions inside a location that do not evaluate\
    \ to a Boolean value, in which case, they will always go to a specific location.\
    \ For example, transitioning back to the program body after executing a Then or\
    \ an Else branch within an If statement. In this case, these are always True transitions.\
    \ To maintain consistency, these locations also have True and False transitions,\
    \ but the False transition always points to None, and the True transition always\
    \ points to the next location.\n\nExpressions. CLARA contains three types of expressions\
    \ as follows, where the names between parentheses refer to CLARA's naming convention:\
    \ variables (Var), operators (Op), and constants (Const). A Const can be a string,\
    \ byte, number, or a name constant. A Var represents variables and, therefore,\
    \ only comprises strings. An Op is the most complex type of expression as it encompasses\
    \ all computations involving any type of operation, such as creating a list, set\
    \ or tuple, and computations involving comparisons, if conditions, or binary operations.\
    \ Every Op comprises two components, the name of the operation and the arguments\
    \ it has to operate on. Depending on the type of operation, Op can contain a different\
    \ number of arguments. For example, the GetElement operator entails getting an\
    \ element from a list, dictionary or set, and contains two arguments: the object\
    \ it needs to get the element from and the element index. SetInit, which creates\
    \ a set, has multiple arguments as each argument is an element inside the set.\n\
    \nAll types of control flow statements are also operators. However, while processing\
    \ those statements, CLARA makes changes to the model. As men-\n\n```\n1 a = [5\
    \ , 6]\n2 b , c = a\n3 b += 1\n4 b += c\n5\n6 for i in a:\n7 c += i\n```\nFigure\
    \ 3: Sample Python source code\n\ntioned earlier, processing control flow statements\
    \ results in the addition of new locations to the model. The number of locations\
    \ is different for each control flow statement. If the program contains an If\
    \ statement, three or four locations will be added: one for the condition of the\
    \ statement, another for the expressions inside the Then branch, one for the expressions\
    \ after the statement, and, finally, another for the expressions inside the Else\
    \ branch. The latter location is optional as we do not always have an Else branch\
    \ accompanying the Then branch. However, CLARA recursively applies the following\
    \ optimization to improve program comparison: an If statement that has no loops\
    \ within is translated into a ternary operator. As a result, this statement is\
    \ embedded in its parent and does not add any new nodes to the control flow graph.\
    \ On the other hand, loops always result in the addition of three locations, corresponding\
    \ to the guard, body, and the statements after the loop.\n\nCLARA restricts its\
    \ models by requiring a variable to appear only once on the left side of an expression\
    \ per location. This restriction entails inspecting all the declarations of a\
    \ particular variable and nesting the declarations in the last use of the variable.\
    \ Hence, during the repair step, where variable traces are compared between models,\
    \ there is only one value per location, making the comparison deterministic.\n\
    \nExample of a model. Figures [3](#page-8-0) and [4](#page-9-0) present an example\
    \ of Python source code and its corresponding model. This model is the pretty-printed\
    \ version created by CLARA to help improve readability. Since the source code\
    \ contains a For loop, the model contains four locations. The True and False transitions\
    \ are shown at the bottom of each location and indicate how to traverse the model.\
    \ For example, if \\$cond in location 2 evaluates to true, we transition to location\
    \ 4. If it evaluates to false, we transition to location 3. ind#0 corresponds\
    \ to the index of the loop, i, and iter#0 corresponds to\n\n<span id=\"page-9-0\"\
    ></span>Loc 1 (around the beginning of function main)\n\n```\n−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\
    \ a := ListInit(5, 6)\n c := GetElement(a', 1)\n b := AssAdd(AssAdd(GetElement(a',\
    \ 0), 1), c')\n iter#0 := a'\n ind#0 := 0\n−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\
    \ True −> 2, False −> None\nLoc 2 (the condition of the 'for' loop at line 6)\n\
    −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n $cond := Lt(ind#0, len(iter#0))\n−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\
    \ True −> 4, False −> 3\nLoc 3 (∗after∗ the 'for' loop starting at line 6)\n−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\
    −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n True −> None, False −> None\nLoc 4 (inside\
    \ the body of the 'for' loop beginning at line 7)\n−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\
    \ i := GetElement(iter#0, ind#0)\n ind#0 := Add(ind#0, 1)\n```\n\n```\nc := AssAdd(c,\
    \ i')\n```\n−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nTrue −> 2, False −> None\n\n\
    Figure 4: Model generated by CLARA for the program in Figure [3](#page-8-0)\n\n\
    the value we are iterating over, a. CLARA internally creates both variables for\
    \ every loop. As it can be seen in the source code, b appears on the left side\
    \ of an expression three times (lines [2,](#page-8-1) [3](#page-8-2) and [4\\\
    )](#page-8-3), but in the model, it only appears once in location 1. All three\
    \ uses are nested inside one expression: b := AssAdd(AssAdd(GetElement(a ′ , 0),\
    \ 1), c ′ ). The other expressions correspond to the other operations before the\
    \ loop. Whenever CLARA uses a variable defined earlier in an expression, it converts\
    \ it to a different variable rather than using the original variable. For example,\
    \ a becomes a ′ . CLARA uses this notation (prime) to determine whether a variable\
    \ is being defined or used.\n\nSince CLARA relies on program execution and variable\
    \ traces, it exploits a Python interpreter that helps execute models. The interpreter\
    \ visits each expression in the model and recursively executes it, since an expression\
    \ can contain other nested expressions. For example, the expression b := AssAdd(AssAdd(GetElement(a\
    \ ′ , 0), 1), c ′ ) evaluates from the innermost expression, GetElement(a ′ ,\
    \ 0), until the most external expression. The interpreter contains functions for\
    \ every type of operator, in this case, AssAdd and GetElement, annotated with\
    \ the word execute as a prefix for each function. Therefore, while evaluating\
    \ the previous expression, CLARA first executes GetElement(a ′ , 0), which translates\
    \ to getting the element of a at index 0, invoking the function execute GetElement().\
    \ It then evaluates AssAdd(..., 1) that increments the result by 1 using execute\
    \ AssAdd(), and so on.\n\n### 3.2. Single function alignment and program repair\n\
    \nTo find an alignment between two programs, CLARA creates models for each of\
    \ them. If the control flow of both models is the same, for every variable in\
    \ one model, it finds a matching with a corresponding variable in the other model\
    \ based on variable tracing. The process consists of comparing values of variables\
    \ at each location of the program trace. If the matching variables hold the same\
    \ values at every point in the trace, the two programs are aligned. Single program\
    \ repair is performed between two programs, where one is a correct program and\
    \ the other is incorrect. The repair process starts by creating models for each\
    \ program and aligning them as described above. Additionally, CLARA requires the\
    \ programs to have at least one function and the same number of functions overall\
    \ in the programs being compared. These functions must also have the same names\
    \ and cannot be nested. These features determine the structure of the programs.\
    \ If there is a difference in the structure of the programs, CLARA throws a structure\
    \ mismatch error and does not run.\n\n### Algorithm 1: Align\n\n<span id=\"page-11-2\"\
    ></span><span id=\"page-11-1\"></span>in : G<sup>C</sup> = (U , E) control flow\
    \ graph (correct program); u ∈ U ; G<sup>I</sup> = (V , F) control flow graph\
    \ (incorrect program); v ∈ V in/out: ϕ : U → V program alignment out : Whether\
    \ there is an alignment <sup>1</sup> // If u and/or v are in ϕ, they must be mapped\
    \ to each other. <sup>2</sup> if u ∈ dom ϕ ∨ v ∈ ran ϕ then <sup>3</sup> return\
    \ ϕ(u) = v <sup>4</sup> end <sup>5</sup> // Add u and v to ϕ. <sup>6</sup> ϕ(u)\
    \ ← v <sup>7</sup> // u ′ and u ′′ (v ′ and v ′′) are the neighbors of u (v).\
    \ <sup>8</sup> Let {u True −−→ u ′ , u False −−−→ u ′′} ⊆ E, {v True −−→ v ′ ,\
    \ v False −−−→ v ′′} ⊆ F <sup>9</sup> // Align u and v neighbors. <sup>10</sup>\
    \ return Align(G<sup>C</sup> , u ′ , G<sup>I</sup> , v ′ , ϕ) ∧ Align(G<sup>C</sup>\
    \ , u ′′ , G<sup>I</sup> , v ′′, ϕ)\n\n<span id=\"page-11-4\"></span><span id=\"\
    page-11-3\"></span><span id=\"page-11-0\"></span>Algorithm [1](#page-11-0) corresponds\
    \ to CLARA's program alignment based on the control flow graphs (locations) of\
    \ the programs at hand. The algorithm receives two control flow graphs, G<sup>C</sup>\
    \ = (U , E) and G<sup>I</sup> = (V , F), where U and V are sets of locations,\
    \ and E and F are sets of transitions, and two locations u ∈ U and v ∈ V , respectively.\
    \ In the initial call, u and v are the entry points of both programs. The algorithm\
    \ receives ϕ, which determines the current mapping of U locations into V locations,\
    \ i.e., the alignment between the graphs. It outputs whether or not there is an\
    \ alignment between the graphs. If any of the locations is present in ϕ, it returns\
    \ whether they are both mapped (lines [2–](#page-11-1)[3\\)](#page-11-2). Note\
    \ that, if they are mapped, there is a match; otherwise, either u or v are mapped\
    \ to a different location and, therefore, there is a mismatch. If u and v are\
    \ not in ϕ, they are added to ϕ (line [6\\)](#page-11-3), and their neighbors\
    \ are recursively inspected (line [10\\)](#page-11-4).\n\nCLARA exploits ϕ during\
    \ the repair process. For every variable in one location of the correct program,\
    \ CLARA aims to match such variable in the corresponding location of the incorrect\
    \ program. Due to CLARA's modeling, a variable can only have one expression per\
    \ location, ensuring it can only hold one value. This makes the comparison with\
    \ other variables possible. During the repair process, a mapping of a variable\
    \ only deals with its specific expression and the expression it is mapped to.\
    \ While mapping a variable from the correct program, every variable in the incorrect\
    \ program is compared, whether or not it has already been mapped. Two variables\
    \ are declared a match if their corresponding expressions evaluate to the same\
    \ values using the same inputs, where the inputs denote the variables both the\
    \ expressions depend on. This match is evaluated based on cost. Since both expressions\
    \ depend on different variables, the cost of a match denotes the number of changes/steps\
    \ it takes to transform the incorrect program's expression into the correct program's\
    \ expression, where a variable from the incorrect program replaces each variable\
    \ in the correct program's expression.\n\nIn the repair process, CLARA assumes\
    \ that the number of variables in the correct program is the minimum number of\
    \ variables needed, so the number of variables in the incorrect program needs\
    \ to match the number of variables in the correct program precisely. Therefore,\
    \ if the incorrect program contains extra variables, CLARA suggests deleting the\
    \ variables it cannot match. If the incorrect program contains fewer variables\
    \ than the correct program, CLARA suggests creating new variables. During cost\
    \ calculation, all the variables in the correct program's expression are substituted\
    \ by variables from the incorrect program as follows: Assume s = a + 2 is a statement\
    \ in the correct program. CLARA replaces a by all other variables present in the\
    \ corresponding location in the incorrect program and evaluates the associated\
    \ cost. If the incorrect program contains three variables x, y and z in the corresponding\
    \ location, a in s = a + 2 is replaced by x, y and z, respectively. Since it is\
    \ possible that the correct program has extra variables, CLARA also calculates\
    \ the cost of the substitution using a fresh variable that does not exist in the\
    \ incorrect program. This value is initialized to the value of a with a cost of\
    \ 1. CLARA finally saves all the costs for every location and provides the cost\
    \ array to a linear programming solver, which minimizes the overall cost and suggests\
    \ matches for all the variables involved. Based on these matches and costs, the\
    \ final set of repairs are suggested, which can be of three types: variable additions,\
    \ variable deletions, or variable changes.\n\nWe use the models shown in Figure\
    \ [5](#page-13-0) to illustrate how the variable matching process works. Both\
    \ models contain a single location that are trivially mapped to each other. Table\
    \ [1](#page-13-1) presents the variable matching of a in the correct program.\
    \ The table contains five columns as follows: The first column represents the\
    \ variable comparison number. The second column is the variable in the correct\
    \ program we aim to match. The third column rep-\n\n<span id=\"page-13-0\"></span>\n\
    \n| Correct Model:  | Incorrect Model: |\n|-----------------|------------------|\n\
    | a := 1          | x := 1           |\n| b := 2          | y := 2           |\n\
    | c := Add(a', 1) | z := Add(y', 1)  |\n\nFigure 5: Sample correct and incorrect\
    \ program models\n\n<span id=\"page-13-1\"></span>\n\n| # | Variable (C) | Variable\
    \ (IC) | Dependency | Cost |\n|---|--------------|---------------|------------|------|\n\
    | 1 | a            | *             | None       | 2    |\n| 2 | a            |\
    \ x             | None       | 0    |\n| 3 | a            | y             | None\
    \       | 1    |\n| 4 | a            | z             | None       | 2    |\n\n\
    Table 1: Repair cost table for variable a in Figure [5](#page-13-0)\n\nresents\
    \ the possible variable match in the incorrect program. The fourth column contains\
    \ the possible substitution for the dependent variables as a tuple (i, j), where\
    \ i is the dependent variable from the correct program, and j is its possible\
    \ substitution in the incorrect program. The fifth column is the cost for each\
    \ variable match. In the first row, ∗ entails that a fresh variable is used; the\
    \ cost is 2 because we need to create a new variable and assign its value to 1\
    \ since a's value is one. The cost of the second row is 0 because no changes are\
    \ needed. Note that there are no variable dependencies in this example.\n\nTable\
    \ [2](#page-13-2) shows the cost computation for variable b. Note that the fourth\
    \ row consists of replacing b by z, which depends on y; therefore, additional\
    \ combinations of variables in the correct program are used, e.g., y and a are\
    \ matched. Finally, Table [3](#page-14-1) presents the cost computation for variable\
    \ c in which fresh variables are also used in the dependencies. The optimal\n\n\
    <span id=\"page-13-2\"></span>\n\n| # | Variable (C) | Variable (IC) | Dependency\
    \ | Cost |\n|---|--------------|---------------|------------|------|\n| 1 | b\
    \            | *             | None       | 2    |\n| 2 | b            | x   \
    \          | None       | 1    |\n| 3 | b            | y             | None  \
    \     | 0    |\n| 4 | b            | z             | (a, y)     | 0    |\n| 5\
    \ | b            | z             | None       | 3    |\n\nTable 2: Repair cost\
    \ table for variable b in Figure [5](#page-13-0)\n\n<span id=\"page-14-1\"></span>\n\
    \n| #  | Variable (C) | Variable (IC) | Dependency | Cost |\n|----|--------------|---------------|------------|------|\n\
    | 1  | c            | *             | (a, *)     | 4    |\n| 2  | c          \
    \  | *             | (a, x)     | 4    |\n| 3  | c            | *            \
    \ | (a, y)     | 4    |\n| 4  | c            | *             | (a, z)     | 4\
    \    |\n| 5  | c            | x             | (a, *)     | 2    |\n| 6  | c  \
    \          | x             | (a, y)     | 2    |\n| 7  | c            | x    \
    \         | (a, z)     | 2    |\n| 8  | c            | y             | (a, *)\
    \     | 3    |\n| 9  | c            | y             | (a, x)     | 3    |\n| 10\
    \ | c            | y             | None       | 0    |\n| 11 | c            |\
    \ y             | (a, z)     | 3    |\n| 12 | c            | z             | (a,\
    \ *)     | 1    |\n| 13 | c            | z             | (a, x)     | 1    |\n\
    | 14 | c            | z             | (a, y)     | 0    |\n\nTable 3: Repair cost\
    \ table for variable c in Figure [5](#page-13-0)\n\nsolution is a matching that\
    \ minimizes the overall cost for all variables. All of these possible matches\
    \ for every variable are the input provided to the linear programming solver.\
    \ The best variable matching is as follows: {ϕ(a) = x , ϕ(b) = y, ϕ(c) = z}, which\
    \ corresponds to rows 2, 3 and 14 in Tables [1,](#page-13-1) [2](#page-13-2) and\
    \ [3,](#page-14-1) respectively. The overall cost is 1 and the suggested repair\
    \ is as follows: Replace z := Add(y ′ , 1) by z := Add(x ′ , 1) with cost = 1.0.\n\
    \n### <span id=\"page-14-0\"></span>4. Parser and interpreter modifications\n\n\
    We analyzed introductory programming assignments to identify language constructs\
    \ commonly used like print statements, input functions and import statements.\
    \ Some of these statements were not supported by CLARA's original implementation.\
    \ In this section, we report the language constructs we added and the changes\
    \ we performed to support them. Both abstract syntax tree processor and interpreter\
    \ were updated to include these language constructs, since the former builds models\
    \ and the latter executes the constructs.\n\nPrint statements. Many introductory\
    \ programming assignments use printing to console to verify whether a program\
    \ is correct or incorrect. The verification of correctness determines whether\
    \ a program should be repaired or not. Similar to other programming languages,\
    \ computations in Python can be performed using variables or inside the parentheses\
    \ of a print statement, removing the need for variables altogether. Hence, while\
    \ evaluating the similarity between two programs or performing a repair, it is\
    \ crucial to match the contents inside these print statements. CLARA's authors\
    \ did incorporate the parsing of print statements. However, when Python 3.x was\
    \ introduced, the print operation switched from a statement to a function call.\
    \ Furthermore, when CLARA's authors updated the implementation to work with Python\
    \ 3.x, not all of the code was updated, leading to the loss of functionality of\
    \ the print operation. We updated the section of the abstract syntax tree processor\
    \ that checks for function calls by checking if the print function was called,\
    \ and updated the model accordingly. Once the print function was correctly processed,\
    \ the comparison of the print operation during the matching and repair processes\
    \ was handled automatically.\n\nImport statements. Certain introductory programming\
    \ assignments require the use of external libraries, such as math, re or string.\
    \ These libraries provide access to functions like sqrt, ceil, search (regular\
    \ expressions), or format (a string). Hence, for CLARA to execute these functions\
    \ while performing a repair or a match, it is essential to have the functionality\
    \ to parse and record the data within these import statements. The original implementation\
    \ of CLARA ignored import statements, causing the program to crash during the\
    \ repair or alignment processes, as their corresponding functions cannot be found\
    \ while executing the function trace.\n\nSince CLARA has its own version of an\
    \ abstract syntax tree processor and interpreter for Python, after parsing and\
    \ processing import statements, we represent and store them in a way such that\
    \ a function call is successfully recognized during execution by the interpreter.\
    \ We created a section in the abstract syntax tree processor to deal with import\
    \ statements and stored them in a nested global dictionary, which is provided\
    \ to the interpreter to be accessed during execution.\n\nVariable assignment.\
    \ In Python 3.x, a programmer can use a variable assignment based on list deconstruction\
    \ or unpacking. For instance, the statement a, b, c = [1, 2, 3] is convenient\
    \ to assign the values 1, 2, and 3 to variables a, b and c, respectively. These\
    \ types of assignments are commonly used in introductory programming assignments.\
    \ We updated the abstract syntax tree processor to recognize and process multiple\
    \ assignments from a single statement. We separated the assignments with their\
    \ corresponding expressions and added each assignment as an individual expression\
    \ to the model. Without these changes, CLARA's original implementation produces\
    \ an error stating that multiple assignments within a single line are not supported,\
    \ halting the repair and alignment processes.\n\nBuilt-in Python functions. CLARA's\
    \ interpreter helps recognize functions in the model and execute them in Python.\
    \ Therefore, common built-in functions like max, sum or len are individually defined\
    \ in the interpreter using auxiliary functions like execute max, execute sum or\
    \ execute len, respectively. These auxiliary functions implement the expected\
    \ functionality. Since Python has a substantial collection of built-in functions,\
    \ manual addition of every function was not included in the interpreter, causing\
    \ CLARA to fail during the alignment and repair processes. Our approach to circumvent\
    \ the need for manual addition of every function is to use Python's internal dictionary\
    \ named builtins. Every time a function is called, we verify whether it is a built-in\
    \ Python function using such dictionary. If this is the case, we proceed to execute\
    \ the function. As a result, all auxiliary functions of the type execute XYZ are\
    \ not needed anymore.\n\nVariable additions and deletions. CLARA expects all variable\
    \ declarations to have a definition in the first location of the program. In other\
    \ words, the beginning of the program contains assignments for every variable,\
    \ and the rest of the program makes use of those variables. Hence, a new variable\
    \ cannot be declared later in the program. If this happens, CLARA outputs unnecessary\
    \ repairs and the final mapping of variables may be inaccurate. We modified the\
    \ abstract syntax tree processor by removing the restriction of requiring variables\
    \ to be declared in the first location, that is, new variables can be declared\
    \ at any point in the program. Additionally, we modified the list of repairs generated\
    \ by CLARA such that repairs suggesting to create and assign the same variable\
    \ are no longer output.\n\nDuring the repair process, CLARA creates a mapping\
    \ of variables from the correct program to an incorrect program. This mapping\
    \ is based on the variable tracing performed throughout the process. A dictionary\
    \ is used to store the mapping. It uses variables from the correct program as\
    \ keys and incorrect program variables as values. While updating the source code\
    \ to include our modifications, we noticed that, if more than one extra variable\
    \ is declared, CLARA does not suggest deleting more than one variable, resulting\
    \ in an incorrect final variable mapping. Figure [6](#page-17-0) illustrates this\
    \ issue with two programs such that the incorrect program contains two extra variables,\
    \ g and\n\n<span id=\"page-17-0\"></span>![](_page_17_Figure_0.jpeg)\n\n1) Delete\
    \ 'g := 3' ∗after∗ the 'for' loop (cost=1.0)\n\n### (c) Suggested repair\n\nFigure\
    \ 6: Correct and incorrect programs and the corresponding suggested repair that\
    \ indicates to delete a single variable rather than two variables (g and f)\n\n\
    f, that must be deleted; however, the suggested repair does not mention f. The\
    \ internal dictionary is as follows: {a : a, s : 0, x : x, − : g}, where variable\
    \ f has been omitted. In this dictionary, variable addition and deletion are represented\
    \ by the ∗ and − keys, respectively. Since it is a dictionary, one of the deletions\
    \ is overwritten as the key is the same. Note that, if the variables f and g were\
    \ in the correct program, CLARA would suggest to add two new variables. This never\
    \ results in an incorrect mapping problem because, in the dictionary, they are\
    \ represented as {f : ∗, g : ∗}. On the contrary, deletions do not work as expected\
    \ since {− : g, − : f} is not allowed and, therefore, we only get the suggestion\
    \ to remove one variable. We thus adjusted the internal dictionary to save an\
    \ array of values in the case of deletions, i.e., {− : ⟨f, g⟩} in our example.\n\
    \nInput statements. One of the commonalities of introductory programming assignments\
    \ is that they evaluate the correctness of a program based on test cases using\
    \ console input and output. The original implementation of CLARA, however, does\
    \ not support standard input. All inputs have to be provided via the command line\
    \ as function arguments. This is not always possible since input arguments can\
    \ be multiple lines long and do not have the same length. Therefore, we updated\
    \ CLARA to read all of the inputs using an argument file and store them in an\
    \ internal list accessible by the interpreter. We updated the interpreter to handle\
    \ calls to the input function separately. So a call like x = input() is handled\
    \ as follows: we extract the first element from the internal list and assign it\
    \ to variable x. If there are subsequent calls to input, we keep extracting elements\
    \ from the internal list. As a result, this modification allows us to repair programs\
    \ that had inputs of different length.\n\nHowever, while adding this feature,\
    \ we encountered another problem. Since CLARA nests the expressions of variables\
    \ while creating its model, it creates copies of the input function when there\
    \ should only be a single call. For example, consider the following Python statement:\n\
    \n<sup>1</sup> a , b , c = input () . split ()\n\nIt becomes the following statements\
    \ in the model:\n\na = GetElement(split(input()), 0) b = GetElement(split(input()),\
    \ 1) c = GetElement(split(input()), 2)\n\nThis change results in input being called\
    \ three times, where it should have been called just once. Since it is possible\
    \ for this problem to occur with other function calls as well, we updated the\
    \ abstract syntax tree processor to create a new variable to store the result\
    \ of calling the input function. Additionally, the expression referencing the\
    \ function references the new variable instead. Therefore, using the above example,\
    \ the statements in the model are as follows:\n\n> input val = input() a = GetElement(split(input\
    \ val), 0) b = GetElement(split(input val), 1) c = GetElement(split(input val),\
    \ 2)\n\nRepetition of expressions is expected if we have multiple variable declarations\
    \ in a single line during model creation. If these expressions include side-effecting\
    \ functions, we can have a similar problem as we had with the input function.\
    \ It is challenging to automatically detect whether a function is side-effecting,\
    \ and creating new variables for every single function call in a program is also\
    \ challenging to handle due to the addition of multiple variables. Furthermore,\
    \ it can cause a mismatch in the number of variables between the correct program\
    \ and the incorrect program, resulting in unexpected repairs. However, this is\
    \ not a problem for the input function, as we expect both the programs to have\
    \ the same number of calls to the input function. We adjusted the source code\
    \ to address this issue to receive an optional list of the side-effecting functions\
    \ via command line. New variables will be created for each of these functions\
    \ similar to input, aiding us in solving the problem and limiting the addition\
    \ of extra variables. In practice, one can apply a preprocessing step detecting\
    \ side-effecting functions and add them to this command-line list.\n\n### <span\
    \ id=\"page-19-0\"></span>5. Alignment and repair modifications\n\nBefore performing\
    \ the modifications described above, CLARA's original implementation did not output\
    \ any model when processing a program containing any unsupported statements. Since\
    \ both the alignment and repair processes depend on models, both processes were\
    \ thus not executed. After performing the modifications described above, the alignment\
    \ and repair processes worked properly for many programs. However, some programs\
    \ still failed. In this section, we report our modifications to the alignment\
    \ and repair processes of CLARA's original implementation. Note that these modifications\
    \ were necessary because of the modifications made to the parser and interpreter\
    \ presented above.\n\nNested functions. The original implementation of CLARA is\
    \ not able to parse nested functions as functions cannot store other functions\
    \ in the model. A function is thus only allowed to store expressions. We updated\
    \ the model to support nested functions by creating a link between two or more\
    \ functions. After adding it to the model, we updated the processes to align and\
    \ repair these functions successfully. Both processes involve creating a one-to-one\
    \ mapping between variables. Therefore, we need to ensure that the variables inside\
    \ the nested functions are not involved in the mapping as those exist in a different\
    \ environment. We treat each nested function as a variable, which is evaluated\
    \ during trace execution, and its return value is substituted by the variable\
    \ calling the function. If the function is called on its own and does not have\
    \ a return value, we check if it is printing to standard output. If that is the\
    \ case, the expression being printed is added to the standard output of the outer\
    \ function. Since CLARA performs variable tracing and tracks the values a variable\
    \ holds throughout the program, CLARA places more importance on the variable's\
    \ values than its expression. This helps eliminate the need for function inlining.\
    \ Our aim while updating the alignment and repair processes was to avoid the suggestion\
    \ of creating/deleting nested functions if the other program is performing the\
    \ exact computation without using nested functions.\n\nApplying repairs. CLARA's\
    \ repair process is sound and complete for the test case provided as input [\\\
    [10\\]](#page-45-6). However, it is typically the case that introductory programming\
    \ assignments are evaluated with a variety of test cases. Therefore, we aim to\
    \ determine whether the incorrect program provided as input is repaired for only\
    \ that particular test case or for all test cases available. To accomplish this,\
    \ we need to convert CLARA's output into actual repairs and, then, apply these\
    \ repairs to the model of the program. Note that the repair process focuses solely\
    \ on models and not the original source code; therefore, we decided to repair\
    \ the program's model rather than the source code.\n\nEvery statement output by\
    \ CLARA contains the variables involved in the repair for both the correct and\
    \ incorrect programs, the associated location in the correct program, and the\
    \ associated expression from the correct program. Hence, we must extract the rest\
    \ of the necessary information, i.e., the location in the incorrect program and\
    \ the expression. Every function in a program groups and stores its variables\
    \ and their associated expressions by location. There are three types of repairs:\
    \ variable deletion, variable addition, and changing the variable definition.\
    \ In the case of variable deletion, we access its corresponding location expressions\
    \ and remove the variable from the list. Similarly, we add the variable and the\
    \ new expression to its corresponding location expressions for variable additions.\
    \ Finally, we substitute the variable's expression in its corresponding location\
    \ for variable changes.\n\nHowever, CLARA's output is not sorted; therefore, for\
    \ variable additions and changes, we must ensure expressions are added in such\
    \ an order that the variables being used exist. For example, if we have the following\
    \ output:\n\n- 1) Change 'a = x + 5' to 'a = m + 5'\n- 2) Add 'm = 3'\n\nWe must\
    \ ensure that m is defined before a; otherwise, an error is thrown during program\
    \ execution. As mentioned earlier, CLARA differentiates between variable definition\
    \ and variable usage. An example of this can be seen\n\n```\n1 a = 3\n2 b = a\
    \ + 1\n3 for x in range (0 , 2 ):\n4 b += x\n5 b += a\n```\n<span id=\"page-21-1\"\
    ></span>Figure 7: Sample program to illustrate how CLARA differentiates between\
    \ variable definition and usage\n\nin Figures [7](#page-21-1) and [8,](#page-22-0)\
    \ where location 1 defines variable a and uses it, denoted as a ′ . Note that\
    \ location 3 uses the same variable; however, in this case, location 3 does not\
    \ (re)define variable a; therefore, it does not use a ′ . As a result, we need\
    \ to deal with an additional problem involving variable definition. For variable\
    \ usage, we must ensure that variables exist and are defined earlier in the location.\
    \ If we add a new variable definition within a location, we need to make sure\
    \ that usages of that variable are updated. For instance, if we add a definition\
    \ of a into location 3, we must change a to a ′ when it is used to update b. Therefore,\
    \ every time a new variable is added or deleted, we execute a trace of the function\
    \ to help us determine which variables we have access to before adding a repair.\
    \ If all of the variables used in that expression are defined, we add the repair.\
    \ Otherwise, we continue adding the rest of the repairs and come back to the ones\
    \ we skipped earlier until no more repairs are available.\n\nOnce all the repairs\
    \ have been applied, we rerun the repair process to determine if any additional\
    \ repairs are suggested for the same test case. If this happens, we halt and conclude\
    \ that the program is not successfully repaired. Otherwise, we conclude that the\
    \ incorrect program is repaired for that particular test case, run the repair\
    \ process for all other available test cases, and record if any repairs are suggested.\
    \ If no other repairs are suggested overall, we can successfully conclude that\
    \ the incorrect program has been fully repaired for all test cases.\n\n## <span\
    \ id=\"page-21-0\"></span>6. Flexible program alignment\n\nOne of CLARA's main\
    \ limitations is that it requires both the correct and the incorrect programs\
    \ to have similar control flows in order to proceed with the repair process. It\
    \ is uncommon to find many programs containing the same control flow, which reduces\
    \ the number of programs CLARA can\n\n<span id=\"page-22-0\"></span>Loc 1 (around\
    \ the beginning of function)\n\n−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− a := 3 b :=\
    \ Add(a', 1) iter#0 := range(0, 2) ind#0 := 0 −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\
    \ True −> 2 False −> None Loc 2 (the condition of the 'for' loop at line 3) −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\
    \ \\$cond := Lt(ind#0, len(iter#0)) −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− True −>\
    \ 4 False −> 3 Loc 3 (∗after∗ the 'for' loop starting at line 3) −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\
    \ b := AssAdd(b, a) −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− True −> None False −>\
    \ None Loc 4 (inside the body of the 'for' loop beginning at line 4) −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\
    \ x := GetElement(iter#0, ind#0)\n\nind#0 := Add(ind#0, 1) b := AssAdd(b, x')\
    \ −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nTrue −> 2 False −> None\n\nFigure 8:\
    \ Model of the program in Figure [7](#page-21-1) that differentiates between variable\
    \ definition and usage, e.g., a and a ′\n\n<span id=\"page-23-0\"></span>![](_page_23_Figure_0.jpeg)\n\
    \nFigure 9: Correct and incorrect programs used to illustrate flexible alignment\n\
    \nrepair in practice. Hu et al. [\\[18\\]](#page-46-3) reported that CLARA's repair\
    \ process did not work for 35.5% of incorrect programs using several introductory\
    \ programming assignments. Our experiments below also confirm these findings.\n\
    \nWe propose an algorithm that creates a flexible alignment between the control\
    \ flow graphs of the correct and incorrect programs. This flexible alignment takes\
    \ into consideration both semantic and topological information of the graphs.\
    \ Our main goal is to reduce mismatches in the alignment process; however, this\
    \ comes with a penalty: since it is approximate, it is possible to obtain an alignment\
    \ that makes the repair process fail. We discuss these issues in this section.\n\
    \nFigure [9](#page-23-0) presents a simplified version of the programs presented\
    \ in Section [2.](#page-3-0) We use these two programs to illustrate our discussions\
    \ in this section. Note that, in this section, we assume CLARA's optimization\
    \ of replacing if statements containing no loops by ternary operators is disabled.\n\
    \n### 6.1. Creation of the control flow graph\n\nEach model created by CLARA is\
    \ based on the program's control flow. Hence, we decided to utilize the information\
    \ available to us in the model to create control flow graphs for both the correct\
    \ and incorrect programs. Each location is a node in the control flow graph, and\
    \ the location's transitions are the edges in the graph. Since many transitions\
    \ pointed to None, indicating that no transition exists, we decided to create\
    \ a special node for None.\n\nEach node in the control flow graph contains a location\
    \ number, the corresponding location's expressions, the starting line number of\
    \ that location, a description, and a multiset of labels. This multiset contains\
    \ all the semantic information we can extract from the (nested) expressions of\
    \ the location at hand. Each expression contains semantic information in terms\
    \ of constants and operation semantics. Operation semantics allow us to identify\
    \ the type of statement, e.g., addition or subtraction. Unlike expressions, which\
    \ can hold different values due to variables, we want the elements of a label\
    \ to always remain constant for a particular expression. Therefore, variable names\
    \ are not added to the multiset of labels. For example, consider the following\
    \ Python statements:\n\n$$\\begin{array}{rcl} \\mathbf{a} & \\mathbf{a} & = &\
    \ \\begin{bmatrix} \\mathbf{1} & \\mathbf{5} \\end{bmatrix}, & \\mathbf{6} \\\
    end{array}$$\n\n$$\\begin{array}{rcl} \\mathbf{a} & \\mathbf{b} & = & \\mathbf{a}\
    \ \\begin{bmatrix} \\mathbf{0} \\end{bmatrix} \\end{array}$$\n\n$$\\begin{array}{rcl}\
    \ \\mathbf{a} & \\mathbf{c} & = & \\mathbf{b} \\end{array} + \\begin{array}{rcl}\
    \ \\mathbf{1} \\end{array}$$\n\nWe extract a multiset of labels for each statement.\
    \ Note that these labels correspond to expressions in the model derived from the\
    \ Python statements. Since a is a list formed by the elements 1, 5 and 6, it contains\
    \ the labels {ListInit, 1, 5, 6}. Also, b is initialized with the first element\
    \ of a, so it has the labels {GetElement, 0}; similarly, c is initialized with\
    \ b + 1, so it has the labels, {Add, 1}. Therefore, the labels assigned to the\
    \ node is the union of all of these multisets, i.e, {ListInit, 1, 5, 6, GetElement,\
    \ 0, Add, 1} (note that 1 appears twice). Recall that we exclude the variable\
    \ names a, b and c from the labels of the node as the variables do not necessarily\
    \ evaluate to the same value at every point in the program.\n\nWe iterate through\
    \ the model of a function to create the nodes of the control flow graph with the\
    \ information mentioned above. After the nodes have been created, we connect the\
    \ nodes with edges representing the transitions between the model locations. Each\
    \ edge is annotated with a True or False label based on the type of the transition.\
    \ Figure [10](#page-26-0) shows the control flow graphs obtained from the programs\
    \ in Figure [9.](#page-23-0) Each node in the figure contains the location number\
    \ and its corresponding multiset of labels. The expressions, line numbers, and\
    \ descriptions have been omitted from the figure to improve readability. The dotted\
    \ arrows represent False transitions, and the solid arrows represent True transitions.\
    \ The node with the label Empty represents a location in the program model that\
    \ does not contain any expressions, which corresponds to the end of the loop.\n\
    \nAlgorithm 2: FlexAlign\n\n<span id=\"page-25-9\"></span><span id=\"page-25-8\"\
    ></span><span id=\"page-25-7\"></span><span id=\"page-25-6\"></span><span id=\"\
    page-25-5\"></span><span id=\"page-25-4\"></span><span id=\"page-25-3\"></span><span\
    \ id=\"page-25-2\"></span><span id=\"page-25-1\"></span><span id=\"page-25-0\"\
    ></span>input : G<sup>C</sup> = (U , E) control flow graph of the correct program;\
    \ G<sup>I</sup> = (V , F) control flow graph of the incorrect program input/output:\
    \ ϕbest : U → V <sup>1</sup> // Initialize best alignment and similarity. <sup>2</sup>\
    \ ϕbest ← {}, sbest ← 0 <sup>3</sup> // Check every permutation. <sup>4</sup>\
    \ for ϕ ∈ Permutations(U , V ) do <sup>5</sup> // s is the similarity of ϕ. <sup>6</sup>\
    \ s ← 0 <sup>7</sup> for u ∈ dom ϕ do <sup>8</sup> v ← ϕ(u) <sup>9</sup> // Jaccard\
    \ similarity between the labels of u and v. <sup>10</sup> slabel ← Jaccard(L(u),\
    \ L(v)) <sup>11</sup> // u ′ and u ′′ (v ′ and v ′′) are the neighbors of u (v).\
    \ <sup>12</sup> Let {u True −−→ u ′ , u False −−−→ u ′′} ⊆ E, {v True −−→ v ′\
    \ , v False −−−→ v ′′} ⊆ F <sup>13</sup> // Edge similarity is 0.5 by default.\
    \ <sup>14</sup> sedge ← 0.5 <sup>15</sup> // Check if the neighbors match. <sup>16</sup>\
    \ if v ′ = ϕ(u ′ ) ∧ v ′′ = ϕ(u ′′) then <sup>17</sup> sedge ← 1 <sup>18</sup>\
    \ else if v ′ ̸= ϕ(u ′ ) ∧ v ′′ ̸= ϕ(u ′′) then <sup>19</sup> sedge ← 0 <sup>20</sup>\
    \ end <sup>21</sup> // Aggregate both similarities (same importance). <sup>22</sup>\
    \ s ← s + (slabel + sedge )/2 <sup>23</sup> end <sup>24</sup> // Update the best\
    \ alignment found. <sup>25</sup> if s > sbest then <sup>26</sup> sbest ← s, ϕbest\
    \ ← ϕ <sup>27</sup> end <sup>28</sup> end\n\n<span id=\"page-26-0\"></span>![](_page_26_Figure_0.jpeg)\n\
    \nv7\n\nv8\n\nu2 v1 u2 Figure 10: Control flow graphs derived from the programs\
    \ in Figure [9](#page-23-0)\n\n0,101,0\n\nv1 0,101,0\n\nv2 cond,Lt,len\n\nv2 cond,Lt,len\n\
    \n#### u3 u4 6.2. Alignment algorithm\n\ncond,Lt,len\n\ncond,Lt,len\n\nu1 0,101,0\n\
    \nu1 0,101,0\n\nu3 Empty\n\n> u0 None\n\nu1 0,101,0\n\nu3\n\nu0\n\nv3 Empty v0\
    \ None v11 out,StrAppend,Add,Add,\",\" v4 AssAdd,GetElement v5 cond,Lt,GetElement\
    \ v6 GetElement v7 AssAdd,1 Empty u0 None u8 out,StrAppend,Add,Add,\",\" AssAdd,GetElement\
    \ u5 cond,Gt,GetElement u6 GetElement u7 AssAdd,1 v8 cond,Eq,0 v9 Sub,1 v10 Empty\
    \ v3 Empty v0 None v11 out,StrAppend,Add,Add,\",\" v4 AssAdd,GetElement v5 cond,Lt,GetElement\
    \ v6 GetElement v7 AssAdd,1 Empty None u8 out,StrAppend,Add,Add,\",\" u4 AssAdd,GetElement\
    \ u5 cond,Gt,GetElement u6 GetElement u7 AssAdd,1 v8 cond,Eq,0 v9 Sub,1 v10 Empty\
    \ Our graph alignment process aims to find a mapping between the nodes of the\
    \ control flow graphs of the correct and incorrect programs. Algorithm [2](#page-25-0)\
    \ aims to compute such mapping taking into consideration both the semantic and\
    \ topological information of the graph, i.e., labels and edges, respectively.\
    \ The algorithms takes the two control flow graphs of the incorrect and correct\
    \ programs as input, denoted as G<sup>C</sup> = (U , E) and G<sup>I</sup> = (V\
    \ , F), respectively, where U and V are sets of nodes, and E and F are sets of\
    \ edges. The algorithm aims to find a mapping ϕbest from U to V . To accomplish\
    \ this, it explores all possible permutations of these mappings, that is, all\
    \ combinations of nodes in U mapped to nodes in V , which is performed by the\
    \ Permutations function (line [4\\)](#page-25-1). Let ϕ be one of these permutations.\
    \ The similarity of ϕ, denoted by s, is computed by considering label and edge\
    \ similarities. For each node u present in ϕ (line [7\\)](#page-25-2) and its\
    \ corresponding v (line [8\\)](#page-25-3), the algorithm computes the Jaccard\
    \ similarity between the multisets of labels of both nodes (line [10\\)](#page-25-4).\
    \ The formula for the multisets L(u) and L(v) is as follows [\\[19\\]](#page-46-4):\n\
    \n$$Jaccard(L(u), L(v)) = \\frac{\\sum\\_{x \\in L(u) \\cap L(v)} \\min(W(x, L(u)),\
    \ W(x, L(v)))}{\\sum\\_{x \\in L(u) \\cup L(v)} \\max(W(x, L(u)), W(x, L(v)))}$$\n\
    \n<span id=\"page-27-0\"></span>\n\n| Multiset<br>L(u)             | Multiset<br>L(v)\
    \             | Jaccard |\n|------------------------------|------------------------------|---------|\n\
    | GetElement, 0, 0             | GetElement, 0                | 0.667   |\n| GetElement,\
    \ 0                | GetElement, 0                | 1       |\n| cond, Lt, ind#0,\
    \ len, iter#0 | cond, Gt, ind#1, len, iter#1 | 0.25    |\n| ListInit, 5, 6   \
    \            | ListInit, 8, 9               | 0.2     |\n\nTable 4: Multisets\
    \ of labels and their corresponding Jaccard similarities\n\nwhere W (x , S) is\
    \ the number of times x appears in the multiset S. Note that 0 ≤ Jaccard(L(u),\
    \ L(v)) ≤ 1, where zero indicates no similarity and one indicates both multisets\
    \ are equal. Table [4](#page-27-0) presents several examples of multisets of labels\
    \ and their corresponding Jaccard similarities. We rely on Jaccard similarity\
    \ because it penalizes dissimilarities between the multisets compared, intersection\
    \ divided by union, more than others like Sørensen–Dice.\n\nOnce Jaccard similarity\
    \ is computed, the algorithm computes edge similarity (lines [12](#page-25-5)[–19\\\
    )](#page-25-6). It considers both neighbors of u and v that correspond to the\
    \ False and True transitions in each control flow graph. If both sets of neighbors\
    \ are mapped in ϕ, the edge similarity is one; if none of them are mapped in ϕ,\
    \ the edge similarity is zero; otherwise, the edge similarity is 0.5 (one but\
    \ not the other is mapped).\n\nBoth similarities are combined using the same importance\
    \ and added to the current similarity of ϕ (line [22\\)](#page-25-7). Finally,\
    \ the graph alignment that is output is the one with highest similarity (lines\
    \ [25–](#page-25-8)[26\\)](#page-25-9).\n\nNote that, even though we expect to\
    \ deal with small programs, the number of nodes in a program in an introductory\
    \ programming assignment typically ranges between 10 and 20; therefore, it is\
    \ possible to have more than 3 million permutations. In practice, we aim to find\
    \ permutations with high similarities first. To accomplish this, we use a heuristic\
    \ in which we explore the permutations in ascending order by semantic similarity.\
    \ Furthermore, we only explore the top-k permutations found using this approach.\n\
    \nFigure [11](#page-28-0) presents an alignment between the control flow graphs\
    \ discussed above. The control flow graph of the correct program is on the left\
    \ side, and the graph of the incorrect program is on the right side. The solid\
    \ red lines indicate the mapping ϕ between the nodes. As it can be observed, nodes\
    \ v8, v<sup>9</sup> and v<sup>10</sup> are not mapped to any nodes in the correct\
    \ program. These locations correspond to the second if statement in the incorrect\
    \ program.\n\nAlgorithm [2](#page-25-0) always produces an alignment between the\
    \ input programs.\n\n<span id=\"page-28-0\"></span>![](_page_28_Figure_0.jpeg)\n\
    \nv1 0,101,0\n\n> v3 Empty v0 None\n\nv11 out,StrAppend,Add,Add,\",\"\n\nv10 Empty\n\
    \nv6 GetElement\n\nv2 cond,Lt,len\n\nv4 AssAdd,GetElement\n\nv5 cond,Lt,GetElement\n\
    \n> v9 Sub,1\n\nv7 AssAdd,1\n\n> v8 cond,Eq,0\n\nFigure 11: Alignment between\
    \ the control flow graphs from Figure [10](#page-26-0)\n\nOne can use s, the similarity\
    \ of an alignment, to decide whether or not to proceed with the repair process.\
    \ In our experiments below, we use a threshold over s to proceed to repair the\
    \ programs.\n\n### 6.3. Recreating the model\n\nu1 0,101,0\n\nu3 Empty\n\n> u0\
    \ None\n\nu8 out,StrAppend,Add,Add,\",\"\n\nu6 GetElement\n\nu2 cond,Lt,len\n\n\
    u4 AssAdd,GetElement\n\nu5 cond,Gt,GetElement\n\n> u7 AssAdd,1\n\nOnce we have\
    \ computed an alignment ϕ between the control flow graphs of the correct and the\
    \ incorrect programs, we have to recreate the input that CLARA's repair process\
    \ requires. Since the process receives models for each program, we rely on ϕ and\
    \ the model of the correct program to recreate the new model. There are three\
    \ possibilities. First, the control flow graph of the correct program has less\
    \ nodes than the graph of the incorrect program. In this case, nodes need to be\
    \ removed from the new model. Second, the control flow graph of the correct program\
    \ has more nodes, which implies that new nodes need to be added to the new model.\
    \ Third, both graphs have the same size, so no nodes need to be added or removed.\
    \ In all of the three cases, edges or nodes might need rearrangement in the new\
    \ model after an alignment has been determined.\n\nAlgorithm [3](#page-29-0) takes\
    \ as input both control flow graphs (G<sup>C</sup> and G<sup>I</sup> ) as well\
    \ as the alignment between them (ϕ). It outputs M<sup>I</sup> , the recreated\
    \ model of the incorrect program. If there are more nodes in the incorrect program\n\
    \n### <span id=\"page-29-0\"></span>Algorithm 3: RecreateModel\n\n<span id=\"\
    page-29-6\"></span><span id=\"page-29-5\"></span><span id=\"page-29-4\"></span><span\
    \ id=\"page-29-3\"></span><span id=\"page-29-2\"></span><span id=\"page-29-1\"\
    ></span>input : G<sup>C</sup> = (U , E) control flow graph of the correct program;\
    \ G<sup>I</sup> = (V , F) control flow graph of the incorrect program; ϕ : U →\
    \ V alignment output: M<sup>I</sup> = (W , J ) recreated model of the incorrect\
    \ program <sup>1</sup> // Initialize M<sup>I</sup> with the nodes in G<sup>I</sup>\
    \ and empty edges. <sup>2</sup> W ← V , J ← ∅ <sup>3</sup> // Correct is smaller\
    \ than incorrect; remove extra nodes. <sup>4</sup> if |U | < |V | then <sup>5</sup>\
    \ // If v is not in ϕ, remove from W . <sup>6</sup> for v ∈ V such that v ∈/ ran\
    \ ϕ do <sup>7</sup> W ← W \\ {v} <sup>8</sup> end <sup>9</sup> end <sup>10</sup>\
    \ // Incorrect is smaller than correct; add extra nodes. <sup>11</sup> if |U |\
    \ > |V | then <sup>12</sup> // If u is not in ϕ, add new node to W and ϕ. <sup>13</sup>\
    \ for u ∈ U such that u ∈/ dom ϕ do <sup>14</sup> v ′ ← CreateNode() <sup>15</sup>\
    \ W ← W ∪ {v ′} <sup>16</sup> ϕ(u) ← v ′ <sup>17</sup> end <sup>18</sup> end <sup>19</sup>\
    \ // Update J to reflect the edges in G<sup>C</sup> . <sup>20</sup> for u ∈ dom\
    \ ϕ do <sup>21</sup> v ← ϕ(u) <sup>22</sup> Let {u True −−→ u ′ , u False −−−→\
    \ u ′′} ⊆ E <sup>23</sup> J ← J ∪ {v True −−→ ϕ(u ′ ), v False −−−→ ϕ(u ′′)} <sup>24</sup>\
    \ end\n\nthan in the correct program, the algorithm removes the extra nodes from\
    \ the new model (lines [4](#page-29-1)[–7\\)](#page-29-2). Note that, for every\
    \ node that is deleted, its corresponding expressions and edges (transitions)\
    \ are deleted too. If there are less nodes, it generates new, fresh nodes using\
    \ the CreateNode function, which are added to both the new model and ϕ (lines\
    \ [11](#page-29-3)[–16\\)](#page-29-4). Note that these new nodes do not contain\
    \ any expressions because we do not know yet the corresponding variables associated\
    \ with those expressions. During the repair process, additions of new expressions\
    \ will be suggested for these new nodes. Finally, the algorithm updates the edges\
    \ of the new model based on the edges of the correct program (lines [20–](#page-29-5)[23\\\
    )](#page-29-6).\n\n### <span id=\"page-30-0\"></span>7. Evaluation\n\nTo evaluate\
    \ the performance of CLARA and the improvements of our flexible alignment scheme,\
    \ we built a dataset of correct and incorrect programs from the online programming\
    \ website Codeforces ([https://codeforces.](https://codeforces.com) [com](https://codeforces.com)).\
    \ It is an online platform that hosts competitive programming contests and programming\
    \ problems divided into multiple difficulty ratings. Submissions by users, both\
    \ correct and incorrect, are publicly available. To evaluate the correctness of\
    \ a program, the platform executes it on several test cases. Codeforces programming\
    \ problems follow the same structure: each test case must be read from the console\
    \ by a given program, and such a test case consists of line-delimited parameters.\
    \ The first line indicates the number of arguments, and the following lines include\
    \ arguments as a single block of text that requires parsing before performing\
    \ any computations to solve the problem at hand. A variety of methods can be used\
    \ for such parsing, but the most common is Python's input function for reading\
    \ console input.\n\nIf a program does not pass a particular test case, the testing\
    \ stops and the program is declared incorrect. The test case on which the program\
    \ failed is provided by the platform. Therefore, we only have access to the first\
    \ test case that is not passed for every incorrect program. Furthermore, because\
    \ CLARA uses variable tracing to find repairs in the program, we only need the\
    \ test case input, not the output it is supposed to produce. The platform does\
    \ not display test cases that are longer than 50 lines; instead, it displays the\
    \ initial 50 lines followed by \". . . \" characters. As a result, these test\
    \ cases are incomplete, inaccessible, and therefore, invalid for our purposes.\n\
    \nTaking these factors into account, in our experiments, we utilized valid submissions\
    \ for twenty programming problems with the highest number of Python submissions,\
    \ and fulfilled the condition of having at least one of the following: (1) a loop,\
    \ (2) an if statement, (3) a call to read from standard input, and (4) a call\
    \ to print to standard output.\n\n### 7.1. Dataset and Experimental Setup\n\n\
    Since CLARA analyzes an incorrect program based on correct programs, we selected\
    \ thirty correct programs to be compared with the pool of all incorrect programs\
    \ for each of the twenty programming problems. The subset of thirty correct programs\
    \ for each problem was selected as follows: select the top-10 programs when sorted\
    \ by date and they are from different users. Then, select the top-10 programs\
    \ when sorted by size ascending and they are from different users. Repeat the\
    \ same operation using descending order. All of the incorrect programs taken into\
    \ account failed valid test cases as recorded by the platform. These programs\
    \ were all unique and made by different users. We evaluated five techniques as\
    \ follows:\n\n- 1. Baseline CLARA with No Alignment (CNA): The original implementation\
    \ of CLARA with no modifications and no flexible alignment.\n- 2. SARFGEN: Sarfgen\
    \ [\\[3\\]](#page-44-1) is not publicly available; therefore, we simulated its\
    \ alignment step. We used our proposed flexible alignment (see Algorithm [2\\\
    )](#page-25-0), considering both semantic and topological similarities (label\
    \ and edge), and model recreation (see Algorithm [3\\)](#page-29-0). However,\
    \ we set a threshold: only similarities greater or equal than 0.95 are kept. This\
    \ simulates Sarfgen's rigid program comparison in which both control flow graphs\
    \ must perfectly match.\n- 3. Baseline CLARA: The original implementation with\
    \ the modifications described in Section [4.](#page-14-0) The alignment process\
    \ is the one originally implemented (see Algorithm [1\\)](#page-11-0).\n- 4. FA(L):\
    \ It uses our proposed flexible alignment (FA) (see Algorithm [2\\)](#page-25-0)\
    \ and model recreation (see Algorithm [3\\)](#page-29-0). However, it only exploits\
    \ semantic information (label) for alignments, i.e., edge similarity is always\
    \ equal to one (sedge = 1).\n- 5. FA(L+E): Similar to FA(L) but it considers both\
    \ semantic and topological similarities (label and edge) as described in Algorithm\
    \ [2.](#page-25-0)\n\nNote that the five techniques rely on the same repair process,\
    \ i.e., the process of CLARA's original implementation. Furthermore, we noted\
    \ that the number of nodes in each program's control flow graph ranges between\
    \ 1 and\n\n| Reasons                      | CNA    | SGEN   | CLARA  | FA(L) \
    \ | FA(L+E) |\n|------------------------------|--------|--------|--------|--------|---------|\n\
    | Unavailable Test Cases       | 25,485 | 25,498 | 25,485 | 25,498 | 25,498  |\n\
    | Unsupported Constructs       | 26,152 | 26,152 | 26,152 | 26,152 | 26,152  |\n\
    | Unsupported Class Attributes | 7,868  | 7,868  | 7,868  | 7,868  | 7,868   |\n\
    | Keyword Arguments            | 5,148  | 5,148  | 5,148  | 5,148  | 5,148   |\n\
    | Timeout                      | 9      | 172    | 9      | 191    | 172     |\n\
    | Total Invalid                | 64,838 | 64,838 | 64,662 | 64,857 | 64,838  |\n\
    | Total Valid                  | 15,925 | 15,749 | 15,925 | 15,730 | 15,749  |\n\
    | Total Comparisons            | 80,587 | 80,587 | 80,587 | 80,587 | 80,587  |\n\
    \n<span id=\"page-32-0\"></span>Table 5: Program comparisons available in our\
    \ dataset and summary of invalidity reasons. SGEN refers to SARFGEN while CNA\
    \ refers to CLARA with no flexible alignment.\n\n70. This implies that, in many\
    \ cases, we can have millions of permutations to be evaluated to compute an alignment.\
    \ Therefore, for SARFGEN, FA(L) and FA(L+E), we set a limit of 1,000 permutations.\
    \ The best alignment is thus chosen from these permutations. Recall that we sort\
    \ the node candidates by semantic similarity (labels) with the expectation that\
    \ an alignment with a high similarity will be computed. We also set a time limit\
    \ of one minute for SARFGEN, FA(L), and FA(L+E), and an overall time limit of\
    \ five minutes. If any of these limits is reached, we report a timeout error.\n\
    \nTable [5](#page-32-0) presents a summary of the program comparisons available\
    \ in our dataset. The total number of program comparisons is 80,587, which corresponds\
    \ to the Cartesian product between the total number of incorrect programs and\
    \ the subset of correct programs selected as explained above. A significant portion\
    \ of the comparisons was filtered out because of the following reasons: (1) The\
    \ test cases were not available, (2) Contained unsupported language constructs\
    \ like lambda expressions or try-catch blocks, (3) Contained class attributes,\
    \ (4) Contained functions with keyword arguments, and (5) There were timeout errors.\
    \ Note that there were only 191 timeout errors in the worst case among the comparisons,\
    \ which amounts to less than 1.25% of the total valid comparisons. Therefore,\
    \ timeout errors were significantly mitigated thanks to the time thresholds we\
    \ established. Due to the difference in the number of timeouts for each technique,\
    \ the total valid comparisons are different between them. As a result, we further\
    \ filter the remaining valid comparisons to only pick the comparisons with common\
    \ permutations of valid and invalid programs. This results in 15,688 common valid\
    \ program comparisons available for all the techniques.\n\n| Problem | Total \
    \ | Correct | Incorrect | LOC               | Exprs.             | Diff. |\n|---------|--------|---------|-----------|-------------------|--------------------|-------|\n\
    | 4A      | 1,059  | 10      | 43        | 6.94<br>±<br>0.43 | 5.02<br>±<br>0.14\
    \  | 800   |\n| 50A     | 567    | 5       | 60        | ±<br>2.38<br>1.31 | ±<br>4.90<br>1.26\
    \  | 800   |\n| 214A    | 717    | 7       | 66        | ±<br>11.8<br>5.21 | ±<br>21.0<br>7.87\
    \  | 800   |\n| 255A    | 1,856  | 12      | 96        | 12.5<br>±<br>7.77 | 13.1<br>±<br>7.14\
    \  | 800   |\n| 265A    | 300    | 6       | 25        | 4.95<br>±<br>0.22 | 11.6<br>±<br>1.75\
    \  | 800   |\n| 510A    | 1,001  | 7       | 78        | 17.7<br>±<br>5.14 | 18.4<br>±<br>8.68\
    \  | 800   |\n| 1097A   | 1,329  | 10      | 72        | 16.1<br>±<br>15.6 | 17.6<br>±<br>13.13\
    \ | 800   |\n| 1360B   | 544    | 4       | 85        | ±<br>8.61<br>1.20 | ±<br>18.1<br>2.71\
    \  | 800   |\n| 1370A   | 3,012  | 6       | 249       | ±<br>4.53<br>1.09 | ±<br>10.0<br>3.06\
    \  | 800   |\n| 1385A   | 4,779  | 6       | 428       | 26.8<br>±<br>3.45 | 24.9<br>±<br>4.52\
    \  | 800   |\n| 1391A   | 9,033  | 25      | 148       | 5.83<br>±<br>2.27 | 9.23<br>±<br>4.35\
    \  | 800   |\n| 1391B   | 318    | 6       | 51        | 9.91<br>±<br>2.80 | 23.1<br>±<br>5.19\
    \  | 800   |\n| 208A    | 447    | 5       | 65        | ±<br>8.64<br>10.9 | ±<br>8.44<br>10.16\
    \ | 900   |\n| 1A      | 877    | 5       | 125       | ±<br>2.00<br>0.00 | ±<br>5.73<br>1.03\
    \  | 1000  |\n| 1382B   | 1,532  | 5       | 145       | 15.5<br>±<br>4.31 | 21.2<br>±<br>2.59\
    \  | 1100  |\n| 492B    | 1,147  | 9       | 90        | 9.03<br>±<br>8.15 | 18.7<br>±<br>11.41\
    \ | 1200  |\n| 1363A   | 2,788  | 7       | 295       | 17.0<br>±<br>9.96 | 27.5<br>±<br>5.96\
    \  | 1200  |\n| 1364A   | 12,581 | 11      | 679       | 8.63<br>±<br>5.74 | 14.5<br>±<br>4.65\
    \  | 1200  |\n| 1369B   | 2,410  | 7       | 211       | ±<br>8.00<br>5.72 | ±<br>14.4<br>5.63\
    \  | 1200  |\n| 4C      | 1,107  | 10      | 66        | ±<br>10.3<br>1.57 | ±<br>16.9<br>2.53\
    \  | 1300  |\n\n<span id=\"page-33-0\"></span>Table 6: Dataset statistics for\
    \ valid programs, where LOC and Exprs. respectively indicate total lines of code\
    \ and number of expressions, and Diff. is the problem's difficulty\n\nThe summary\
    \ statistics of the dataset are shown in Table [6.](#page-33-0) The Total column\
    \ indicates the number of valid comparisons each programming problem comprises,\
    \ while the Correct and Incorrect columns display the number of unique programs\
    \ that are part of the total valid comparisons. The LOC and Exprs. columns highlight\
    \ the mean and standard deviation of the number of lines of code and expressions\
    \ (nodes in the control flow graph) respectively across all programs, both correct\
    \ and incorrect. The Diff. column displays the number assigned by Codeforces to\
    \ indicate the difficulty of the problem. However, the ranking and reasoning behind\
    \ the assignment of the number are not officially documented. The closest explanation\
    \ we found was through a Codeforces blogpost [\\[20\\]](#page-46-5), where difficulty\
    \ is assigned such that the expected probability of solving the problem is 0.5\
    \ for coders of that rating. Since we are looking at introductory assignments,\
    \ we limit our ratings from 800 to 1300. We consider the difficulty rating of\
    \ 800 to be low difficulty (it is the lowest available in Codeforces). We selected\
    \ the range of 900-1300 as hard difficulty problem as there are 11,841 data points\
    \ for 800, and 9,518 for 900-1300, allowing for as close to equal binning between\
    \ the two as possible. Analyzing the table, we observe lower lines of code with\
    \ greater variance for low-difficulty problems while higher-difficulty problems\
    \ on average have more lines of code but less variance. This is indicative that\
    \ low-difficulty problems comprise a combination of one-line and longer solutions,\
    \ depending on the capability of the programmers. In contrast, the number of expressions\
    \ at higher difficulty is higher with higher variance, indicating the increase\
    \ in complexity of the solution to the problems. This is also confirmed by Figure\
    \ [12,](#page-35-0) which displays boxplots of the number of lines of code and\
    \ expressions in the programs grouped by difficulty.\n\n### 7.2. Quantitative\
    \ Analysis\n\nSuccessful repairs. In Figure [13,](#page-35-1) we present the percentage\
    \ of successful repairs, which was computed as follows: the number of unique incorrect\
    \ programs fully repaired divided by the total number of incorrect programs. In\
    \ Figure [13a,](#page-35-2) we present the macro success rate for the five techniques\
    \ under evaluation. It can be observed that flexible alignment in both flavors\
    \ significantly outperforms CLARA (5%) and CNA (0.3%) with FA(L) at 45% and FA(L+E)\
    \ at 46% success rates, respectively. We can also observe that baseline CLARA\
    \ performs slightly better than SARFGEN and significantly better than CNA. This\
    \ macro result highlights the major improvement in repair capability that flexible\
    \ alignment can achieve. Figure [13b](#page-35-3) aggregates\n\n<span id=\"page-35-0\"\
    ></span>![](_page_35_Figure_0.jpeg)\n\nFigure 12: Number of lines of code and\
    \ expressions of programs grouped by difficulty\n\n<span id=\"page-35-2\"></span><span\
    \ id=\"page-35-1\"></span>![](_page_35_Figure_2.jpeg)\n\n<span id=\"page-35-3\"\
    ></span>Figure 13: Percentage of incorrect programs fully repaired grouped by\
    \ technique and difficulty\n\n<span id=\"page-36-2\"></span><span id=\"page-36-0\"\
    ></span>![](_page_36_Figure_0.jpeg)\n\nFigure 14: Percentage of incorrect programs\
    \ fully repaired grouped by lines of code and expressions. Cases that failed before\
    \ model creation are ignored.\n\n<span id=\"page-36-1\"></span>Table 7: Number\
    \ of samples in each bin when grouped by LOC (lines of code) and Exprs. (expressions).\
    \ Failed indicates failures before model creation.\n\n<span id=\"page-36-3\"></span>\n\
    \n| LOC    | #     | Exprs. | #     |\n|--------|-------|--------|-------|\n|\
    \ 0–4    | 1,072 | 0–4    | 464   |\n| 5      | 2,301 | 5–10   | 1,597 |\n| 6–15\
    \   | 2,661 | 11–20  | 3,398 |\n| 16–40  | 1,712 | 21–40  | 2,230 |\n| 41–80 \
    \ | 209   | 41–80  | 266   |\n| Failed | 7,733 | Failed | 7,733 |\n\nresults by\
    \ problem difficulty. We observe that the five techniques achieve better success\
    \ rates in low-difficulty problems. We also observe that success rates decrease\
    \ in high-difficulty problems compared to low-difficulty ones. In low-difficulty\
    \ problems, FA(L+E) at 48.2% outperforms FA(L) at 47.5% by 0.7%. However, this\
    \ gain is not as evident in the other problems. Our hypothesis to explain this\
    \ behavior is that, on one hand, low-difficulty problems contain similar statements\
    \ that cannot be easily differentiated based solely on labels. On the other hand,\
    \ high-difficulty problems contain many specialized statements that are almost\
    \ unique, so labels are helpful to align statements without the use of edges.\
    \ We can conclude that using both label and edge similarities in the alignment\
    \ process is beneficial.\n\nAs lines of code increase for programs as shown in\
    \ Figure [14a,](#page-36-0) the performance of all the techniques decreases, but\
    \ baseline CLARA fails to find repairs for programs with 41 lines of code or higher.\
    \ Note that these bins\n\n<span id=\"page-37-0\"></span>![](_page_37_Figure_0.jpeg)\n\
    \n<span id=\"page-37-1\"></span>Figure 15: Number of repairs necessary to repair\
    \ incorrect programs and change percentage (the proportion of the incorrect program\
    \ that was changed) grouped by difficulty\n\nare not balanced as presented in\
    \ Table [7.](#page-36-1) In 7,733 of these comparisons, the techniques failed\
    \ to produce a model to compute lines of code and expressions, which are reported\
    \ in the table. In Figure [14,](#page-36-2) once model creation has been completed,\
    \ flexible alignment has a success percentage over 80% for all LOC and Exprs.\
    \ except for those above 41. In Figure [14a,](#page-36-0) FA(L+E) with its additional\
    \ topological information and flexibility performs better compared to more rigid\
    \ alignment schemes. In addition to lines of code, if we analyze successful repairs\
    \ in terms of expressions in programs, FA(L+E) with both semantic and topological\
    \ information outperforms FA(L), even as the number of expressions – by proxy\
    \ complexity – increases, as seen in Figure [14b.](#page-36-3)\n\nNumber of repairs\
    \ and percentage changes. We also evaluate the performance considering the number\
    \ of repairs. Note that, for a given incorrect program, there is typically the\
    \ case that several correct programs can be used to repair it. We measure, for\
    \ each incorrect program, the minimum number of repairs among all correct programs,\
    \ and the change percentage, that is, the percentage of the incorrect program\
    \ that was altered in order to repair it. As shown in Figure [15a,](#page-37-0)\
    \ flexible alignment has a higher minimum number of repairs than baseline CLARA,\
    \ and the range of repairs is significantly higher. This happens in both bins\
    \ of low- and high-difficulty problems. We observe that the number of repairs\
    \ in the high-difficulty problems is similar for both FA(L) and FA(L+E); however,\
    \ for low-difficulty problems, FA(L) has a slightly reduced number of repairs\
    \ compared to FA(L+E) when they both achieve very similar performance. CNA and\
    \ SARFGEN achieve less number of repairs than any of the other techniques, and\
    \ their performances\n\n<span id=\"page-38-0\"></span>![](_page_38_Figure_0.jpeg)\n\
    \nFigure 16: Average number of repairs necessary to repair incorrect programs\
    \ and change percentage (the proportion of the incorrect program that was changed)\
    \ grouped by lines of code\n\nare quite poor for high-difficulty problems.\n\n\
    Figure [15b](#page-37-1) presents the change percentage, i.e., the percentage\
    \ of the incorrect program that was replaced with the correct program. As expected,\
    \ for low-difficulty problems, our flexible alignment techniques change a higher\
    \ percentage of the incorrect programs than baseline CLARA. Surprisingly, this\
    \ is not the case in the high-difficulty problems, in which we observe a higher\
    \ mean of change percentages for baseline CLARA. This implies that our flexible\
    \ scheme finds smaller repairs than baseline CLARA. The behavior of both FA(L)\
    \ and FA(L+E) are very similar. This highlights the benefit of using both label\
    \ and edge flexible alignments for high-difficulty problems. In contrast, SARFGEN\
    \ significantly reduces the number of changes, but this comes with the penalty\
    \ of very low repair rates. For CNA, without any flexible alignment or parser\
    \ modifications, the median number of changes is lower than SARFGEN but at a penalty\
    \ of even lower repair rates than SARFGEN due to the rigidity of the approach\
    \ i.e. it will fix only similar programs.\n\nFigure [16](#page-38-0) presents\
    \ the average number of repairs and change percentage achieved by each technique\
    \ when grouped by lines of code. We observe that, as lines of code increase, the\
    \ average change percentage of baseline CLARA decreases, while the same average\
    \ for our flexible schemes increases. It is surprising though that, for the small\
    \ bin (0–4), baseline CLARA's and SAR-FGEN's means are higher than those of our\
    \ flexible schemes while CNA has the highest mean. The presence of labels and\
    \ edges allows our flexible approach to reduce the change percentage by identifying\
    \ key labels and edges,\n\n<span id=\"page-39-0\"></span>![](_page_39_Figure_0.jpeg)\n\
    \nFigure 17: Percentage of programs fully repaired grouped by programming problem\n\
    \nwhile also maintaining a high degree of repairs at higher lines of code. This\
    \ suggests that by using flexible alignment, one can find a correct–incorrect\
    \ program comparison that is more efficient than using a rigid program comparison\
    \ scheme.\n\n### 7.3. Qualitative Analysis\n\nWe conducted a fine-grained analysis\
    \ of the repairs within the programming problems selected in our dataset. Figure\
    \ [17](#page-39-0) shows the percentage of programs successfully repaired for\
    \ each technique grouped by problem. The results achieved by baseline CLARA, CNA,\
    \ SARFGEN, FA(L), and FA(L+E) across the twenty problems are consistent with the\
    \ macro results in the previous section. Flexible alignment consistently outperforms\
    \ baseline CLARA and CNA: in four problems (50A, 1360B, 1A, 1363A), FA(L) and\
    \ FA(L+E) achieve more than 80% of successful repairs, while baseline CLARA achieves\
    \ less than 20% success rate. CLARA outperforms SARFGEN except in problems 1385A\
    \ and 208A. In these two problems, FA(L), and FA(L+E) are far superior. CNA fails\
    \ to find solutions in most problems with no flexibility or alignment except for\
    \ 4A and 1A.\n\nWe observe several problems in which baseline CLARA achieves poor\
    \ performance of 5% or less success rate. In the worst cases, FA(L) and FA(L+E)\
    \ achieve approximately 20% success rate (problems 214A, 1097A, and 1364A), still\
    \ outperforming the rest. Comparing the performance of FA(L) vs. FA(L+E), we observe\
    \ that, in problem 510A, FA(L+E) significantly outperforms FA(L). In the rest\
    \ of the problems, both techniques perform similarly.\n\n### 7.4. Threats to validity\n\
    \nWe built our dataset of correct and incorrect programs from Codeforces and utilized\
    \ their assignment of difficulty as a metric for our comparisons. However, the\
    \ reasoning and ranking behind the difficulty assignment from Codeforces for each\
    \ of the problems are not well documented (see discussion above). We present an\
    \ analysis to check correlations between difficulty, lines of code, and expressions\
    \ (code complexity) in Figure [12](#page-35-0) to get a better understanding of\
    \ the dataset. Due to the addition of the graph alignment step, we need to process\
    \ the feedback returned by CLARA before it is provided to students. Based on the\
    \ locations added or deleted and the edges modified, we need to be able to inform\
    \ the learner to add/delete the corresponding expressions.\n\nAnother limitation\
    \ of the flexible alignment step is that, while removing locations from the incorrect\
    \ program model, it is possible to remove a variable with no other definition\
    \ in the rest of the program. Therefore, causing the program to crash if it is\
    \ used. We hope to address this issue in the future by checking if deleting a\
    \ variable can cause the program to crash and, if so, adding that variable and\
    \ its expression to a different location.\n\nRecent advances in large language\
    \ models like ChatGPT[2](#page-40-0) and Codex [\\[21\\]](#page-46-6) have raised\
    \ the question of whether they can be used for program repairs such as the one\
    \ we discuss in this paper. While such language models are good at providing suggestions\
    \ during coding or to generate introductory code from scratch, they are not yet\
    \ developed enough to identify and repair incorrect programs given a correct program\
    \ and test cases as a reference. For example, we prompted ChatGPT with the following\
    \ request: Fix the issues in the incorrect code to match the correct code. The\
    \ result of ChatGPT was to replace the entire incorrect program with the correct\
    \ program instead of\n\n<span id=\"page-40-0\"></span><sup>2</sup>https://chat.openai.com/\n\
    \nidentifying exactly the issues of the incorrect program. Our future work will\
    \ focus on adapting our flexible alignment scheme utilizing the semantic strength\
    \ of such large language models.\n\n### <span id=\"page-41-0\"></span>8. Related\
    \ work\n\nThere are many approaches to automatically repair programs in different\
    \ areas [\\[14\\]](#page-45-8). We categorize these into data- and non-data-driven.\
    \ We also discuss program comparison approaches.\n\nData-driven feedback. CLARA\
    \ [\\[10\\]](#page-45-6) clusters correct programs based on test cases and variable\
    \ traces. Each incorrect program is compared to the representative of each cluster\
    \ to find minimal repairs. The repairs consist of adding new variables and modifying\
    \ existing statements without changing the control flow of the incorrect program.\
    \ Sarfgen [\\[3\\]](#page-44-1) searches for correct programs that share the same\
    \ control flow structure as the incorrect program. Incorrect and correct programs\
    \ are fragmented based on their control flows, and, for each fragment pair that\
    \ is matched, potential repairs are computed using abstract syntax tree edits.\
    \ CLARA and Sarfgen only consider pairs of programs whose control flow match,\
    \ which is a hard constraint since such a pair may not currently be present in\
    \ the set of correct programs or, when the incorrect program significantly deviates\
    \ from a correct program, a correct program with such a control flow may not even\
    \ be possible. Hu et al. [\\[18\\]](#page-46-3) addressed CLARA's drawback of\
    \ rigid program comparisons by refactoring the correct program at hand using a\
    \ set of predefined transformations, such that its control flow matches the incorrect\
    \ program at hand. Using program refactoring, it is possible to modify a program\
    \ so thoroughly that it no longer resembles the original version, and can potentially\
    \ cause a correct program to become incorrect. Furthermore, the repairs suggested\
    \ to change the incorrect program into a correct one need to be backtraced to\
    \ the original program before refactoring.\n\nRefazer [\\[13\\]](#page-45-7) proposes\
    \ \"if-then\" rules to match and transform abstract syntax subtrees of a program.\
    \ Such rules are synthesized from sample pairs of correct/incorrect programs,\
    \ in which tree edit distance comparisons between correct and incorrect programs\
    \ help identify individual transformations. Refazer has been extended to propagate\
    \ feedback based on learned transformations [\\[22\\]](#page-46-7). sk p [\\[12\\\
    ]](#page-45-9) relies on neural networks to repair incorrect programs. It constructs\
    \ partial fragments of three consecutive statements using these renamed tokens.\
    \ The middle statements are removed and fed to the repairer for training. The\
    \ order of statements is one of the main drawbacks of Refazer, Sarfgen, and sk\
    \ p: Refazer and Sarfgen rely on edit distances of abstract syntax trees, while\
    \ sk p treats programs as documents. Our flexible alignment allows to account\
    \ for more implementation variability and increases the number of valid program\
    \ comparisons. Piech et al. [\\[11\\]](#page-45-10) select a subset of existing\
    \ programs to annotate with feedback. Each annotation is used individually to\
    \ learn a binary classifier to propagate feedback to unseen programs. These binary\
    \ classifiers are applied to each incorrect program to decide whether it should\
    \ be annotated with a piece of feedback. This approach requires the number of\
    \ variables in programs to be fixed beforehand and a large number of existing\
    \ programs [\\[11\\]](#page-45-10).\n\nNon-data-driven feedback. AutoGrader [\\\
    [7\\]](#page-45-3) allows to define rules using an error model language to describe\
    \ potential repairs to be applied to incorrect programs, e.g., a condition x <\
    \ y can be mistaken by x ≤ y. Based on these rules, AutoGrader generates a \"\
    sketch\" of the program, i.e., a program that contains multiple choices for those\
    \ statements that matched the given rules [\\[23\\]](#page-46-8). A correct program\
    \ is then assembled by ensuring functional equivalence with respect to a single,\
    \ reference program. Repairs are computed as the changes to transform from an\
    \ incorrect to a correct program. There are several approaches that rely on program\
    \ sketching to compute repairs [\\[24](#page-46-9)[–26\\]](#page-47-0). Codewebs\
    \ [\\[27\\]](#page-47-1) allows to search for code snippets by exploiting probabilistic\
    \ semantic equivalence between abstract syntax trees to perform the matching,\
    \ which is based on functional tests over the trees. Feedback can be propagated\
    \ to identified code snippets that are similar. Marin et al. [\\[28\\]](#page-47-2)\
    \ encode correct and incorrect feedback in subgraph patterns over program dependence\
    \ graphs. Feedback is propagated based on exact subgraph matching with approximations\
    \ at the statement level defined by regular expressions. Verifix [\\[29\\]](#page-47-3)\
    \ uses satisfiability modulo theories solvers to find verified repairs between\
    \ incorrect and correct programs. Edmison and Edwards [\\[30\\]](#page-47-4),\
    \ Nguyen et al. [\\[31\\]](#page-47-5) and Li et al. [\\[32\\]](#page-47-6) applied\
    \ fault localization techniques to detect defects in student programs and suggest\
    \ repairs.\n\nMany approaches have focused on discovering repairs by mutating\
    \ programs until repairing them [\\[14,](#page-45-8) [33\\]](#page-47-7). These\
    \ mutations can be predefined and explored using genetic algorithms [\\[34\\]](#page-47-8).\
    \ Yi et al. [\\[35\\]](#page-47-9) analyzed the usage of some of these approaches\
    \ to repair student programs, and concluded that they are better suited for programs\
    \ that fail a small number of tests, while student programs are typically significantly\
    \ incorrect. Mutations can also be retrieved from existing software repositories\
    \ [\\[36–](#page-48-0)[38\\]](#page-48-1). While these approaches can be seen\
    \ as data-driven, they aim to find repairs based on programs that are generally\
    \ not related to the incorrect program at hand to be repaired; therefore, this\
    \ is a more difficult problem than the one we aim to tackle.\n\nProgram comparison.\
    \ There is a large body of knowledge of program comparison in the context of code\
    \ clones and code plagiarism, i.e., copied-andpasted pieces of code with some\
    \ possible modifications. Successful code clone detectors have focused on comparing\
    \ program tokens and (features of) abstract syntax trees [\\[39\\]](#page-48-2).\
    \ These detectors find blocks of lines of code that are similar, but they usually\
    \ fail to detect correspondences between statements. Program dependence graphs\
    \ are believed to achieve the best accuracy when detecting Type 4 clones, i.e.,\
    \ two pieces of code that perform the same computation but are implemented by\
    \ different syntactic variants [\\[39\\]](#page-48-2). Existing approaches have\
    \ mainly focused on comparing programs based on subgraph isomorphism [\\[40–](#page-48-3)[43\\\
    ]](#page-48-4); however, they are generally not flexible enough to cope with implementation\
    \ variability, and they only provide binary comparisons (Are graphs isomorphic?\
    \ Is a graph contained in the other graph?).\n\nThe approach by Li et al. [\\\
    [44\\]](#page-48-5), the most related to our approximate alignment, compares the\
    \ kernel representations of data-flow and API-call graphs. In this case, a kernel\
    \ is the histogram of node colors that result after the Weisfeiler-Leman algorithm\
    \ is applied for several rounds. This algorithm computes an initial coloring for\
    \ each node based on its immediate neighbors, which is later refined in subsequent\
    \ rounds. It only computes graph topological similarity while our approach aims\
    \ to combine both topological and semantic similarities of nodes. Also, this approach\
    \ does not compute similarities between mapped nodes that can be later exploited.\n\
    \n### <span id=\"page-43-0\"></span>9. Conclusions\n\nNowadays, programming is\
    \ perceived as a must-have skill. It is thus not surprising that the number of\
    \ learners have scaled to millions, especially in online settings. Delivering\
    \ feedback is addressed by repairing learners' incorrect programs. The trend in\
    \ data-driven approaches is to perform a rigid matching between correct and incorrect\
    \ programs to discover snippets of code with mending capabilities. The downside\
    \ is that potential repairs that could be captured by looser alignments may be\
    \ missed.\n\nThis paper explores using a flexible alignment between statements\
    \ in pairs of programs to discover potential repairs. We extend an existing data-driven\
    \ automated repair approach that is open source, CLARA, with our flexible alignment\
    \ approach to deal with such real-world problems. We utilize the abstract syntax\
    \ tree parser in Python to build control flow graphs, and assign a similarity\
    \ to aligning a node in a correct program to a node in an incorrect program. In\
    \ our evaluation, we compare flexible alignment with respect to rigid program\
    \ comparisons. The former is capable of repairing more programs than rigid schemes,\
    \ which supports our hypothesis that rigid approaches might be missing valuable\
    \ code snippets for repairs that could be discovered by an approximate method\
    \ otherwise. Furthermore, our analysis reveals that flexible alignment also decreases\
    \ the changes required to fix more difficult problems. For shorter programs, less\
    \ number of changes are necessary than when using rigid schemes. As a result,\
    \ we claim that \"search, align, and repair\" approaches should rely on flexible\
    \ alignments to improve their repair capabilities. Our analysis comparing both\
    \ semantic and topological (labels and edges) similarities of our flexible alignment\
    \ approach indicates that using both types of similarities is beneficial compared\
    \ to only using labels.\n\nIn future work, we plan to integrate our flexible alignment\
    \ schemes with repairs based on variable traces or program sketches. We will use\
    \ other node semantic similarities rather than the Jaccard distance between multisets\
    \ of labels, such as graph representation of statements. We also plan to study\
    \ how large language models can be leveraged to improve our flexible alignment\
    \ approach.\n\n## References\n\n- <span id=\"page-44-0\"></span>[1] D. D. Garcia,\
    \ J. Campbell, J. DeNero, M. L. Dorf, S. Reges, CS10K teachers by 2017?: Try CS1K+\
    \ students now! coping with the largest CS1 courses in history, in: SIGCSE, 2016,\
    \ pp. 396–397.\n- [2] C. O. Rodriguez, MOOCs and the AI-Stanford like courses:\
    \ Two successful and distinct course formats for massive open online courses,\
    \ EU-RODL 15 (2012).\n- <span id=\"page-44-1\"></span>[3] K. Wang, R. Singh, Z.\
    \ Su, Search, align, and repair: data-driven feedback generation for introductory\
    \ programming exercises, in: PLDI, 2018, pp. 481–495.\n- <span id=\"page-45-0\"\
    ></span>[4] S. Zweben, B. Bizot, 2015 Taulbee Survey, Technical Report, Computing\
    \ Research Association, 2016.\n- <span id=\"page-45-1\"></span>[5] J. Huang, C.\
    \ Piech, A. Nguyen, L. J. Guibas, Syntactic and functional variability of a million\
    \ code submissions in a machine learning MOOC, in: AIED Workshops, 2013.\n- <span\
    \ id=\"page-45-2\"></span>[6] R. Y. Toledo, L. Mart´ınez-L´opez, A recommendation\
    \ approach for programming online judges supported by data preprocessing techniques,\
    \ AI 47 (2017) 277–290.\n- <span id=\"page-45-3\"></span>[7] R. Singh, S. Gulwani,\
    \ A. Solar-Lezama, Automated feedback generation for introductory programming\
    \ assignments, in: PLDI, 2013, pp. 15–26.\n- <span id=\"page-45-4\"></span>[8]\
    \ P. A. Kirschner, J. Sweller, R. E. Clark, Why minimal guidance during instruction\
    \ does not work: An analysis of the failure of constructivist, discovery, problem-based,\
    \ experiential, and inquiry-based teaching, Educational Psychologist 41 (2006)\
    \ 75–86.\n- <span id=\"page-45-5\"></span>[9] D. Coetzee, A. Fox, M. A. Hearst,\
    \ B. Hartmann, Should your MOOC forum use a reputation system?, in: CSCW, 2014,\
    \ pp. 1176–1187.\n- <span id=\"page-45-6\"></span>[10] S. Gulwani, I. Radicek,\
    \ F. Zuleger, Automated clustering and program repair for introductory programming\
    \ assignments, in: PLDI, 2018, pp. 465–480.\n- <span id=\"page-45-10\"></span>[11]\
    \ C. Piech, J. Huang, A. Nguyen, M. Phulsuksombati, M. Sahami, L. J. Guibas, Learning\
    \ program embeddings to propagate feedback on student code, in: ICML, 2015, pp.\
    \ 1093–1102.\n- <span id=\"page-45-9\"></span>[12] Y. Pu, K. Narasimhan, A. Solar-Lezama,\
    \ R. Barzilay, sk p: A neural program corrector for MOOCs, in: SPLASH, 2016, pp.\
    \ 39–40.\n- <span id=\"page-45-7\"></span>[13] R. Rolim, G. Soares, L. D'Antoni,\
    \ O. Polozov, S. Gulwani, R. Gheyi, R. Suzuki, B. Hartmann, Learning syntactic\
    \ program transformations from examples, in: ICSE, 2017, pp. 404–415.\n- <span\
    \ id=\"page-45-8\"></span>[14] M. Monperrus, Automatic software repair: A bibliography,\
    \ CSUR 51 (2018) 17:1–17:24.\n- <span id=\"page-46-0\"></span>[15] V. J. Marin,\
    \ M. R. Contractor, C. R. Rivero, Flexible program alignment to deliver data-driven\
    \ feedback to novice programmers, in: ITS, volume 12677, 2021, pp. 247–258.\n\
    - <span id=\"page-46-1\"></span>[16] M. R. Contractor, C. R. Rivero, Improving\
    \ program matching to automatically repair introductory programs, in: ITS, 2022,\
    \ pp. 323–335.\n- <span id=\"page-46-2\"></span>[17] R. Harper, Practical Foundations\
    \ for Programming Languages (2nd. Ed.), Cambridge University Press, 2016.\n- <span\
    \ id=\"page-46-3\"></span>[18] Y. Hu, U. Z. Ahmed, S. Mechtaev, B. Leong, A. Roychoudhury,\
    \ Refactoring based program repair applied to programming assignments, in: ASE,\
    \ 2019, pp. 388–398.\n- <span id=\"page-46-4\"></span>[19] L. da F. Costa, Further\
    \ generalizations of the jaccard index, CoRR abs/2110.09619 (2021).\n- <span id=\"\
    page-46-5\"></span>[20] M. Mirzayanov, Codeforces: Problem Difficulties, 2018.\
    \ URL: [https:](https://codeforces.com/blog/entry/62865?#comment-468443) [//codeforces.com/blog/entry/62865?#comment-468443](https://codeforces.com/blog/entry/62865?#comment-468443).\n\
    - <span id=\"page-46-6\"></span>[21] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P.\
    \ de Oliveira Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman,\
    \ A. Ray, R. Puri, G. Krueger, M. Petrov, H. Khlaaf, G. Sastry, P. Mishkin, B.\
    \ Chan, S. Gray, N. Ryder, M. Pavlov, A. Power, L. Kaiser, M. Bavarian, C. Winter,\
    \ P. Tillet, F. P. Such, D. Cummings, M. Plappert, F. Chantzis, E. Barnes, A.\
    \ Herbert-Voss, W. H. Guss, A. Nichol, A. Paino, N. Tezak, J. Tang, I. Babuschkin,\
    \ S. Balaji, S. Jain, W. Saunders, C. Hesse, A. N. Carr, J. Leike, J. Achiam,\
    \ V. Misra, E. Morikawa, A. Radford, M. Knight, M. Brundage, M. Murati, K. Mayer,\
    \ P. Welinder, B. Mc-Grew, D. Amodei, S. McCandlish, I. Sutskever, W. Zaremba,\
    \ Evaluating large language models trained on code, CoRR (2021).\n- <span id=\"\
    page-46-7\"></span>[22] A. Head, E. Glassman, G. Soares, R. Suzuki, L. Figueredo,\
    \ L. D'Antoni, B. Hartmann, Writing reusable code feedback at scale with mixedinitiative\
    \ program synthesis, in: L@S, 2017, pp. 89–98.\n- <span id=\"page-46-8\"></span>[23]\
    \ A. Solar-Lezama, Program sketching, STTT 15 (2013) 475–495.\n- <span id=\"page-46-9\"\
    ></span>[24] L. D'Antoni, R. Samanta, R. Singh, Qlose: Program repair with quantitative\
    \ objectives, in: CAV, 2016, pp. 383–401.\n- [25] J. Hua, M. Zhang, K. Wang, S.\
    \ Khurshid, Towards practical program repair with on-demand candidate generation,\
    \ in: ICSE, 2018, pp. 12–23.\n- <span id=\"page-47-0\"></span>[26] X. Liu, S.\
    \ Wang, P. Wang, D. Wu, Automatic grading of programming assignments: an approach\
    \ based on formal semantics, in: ICSE, 2019, pp. 126–137.\n- <span id=\"page-47-1\"\
    ></span>[27] A. Nguyen, C. Piech, J. Huang, L. J. Guibas, Codewebs: Scalable homework\
    \ search for massive open online programming courses, in: WWW, 2014, pp. 491–502.\n\
    - <span id=\"page-47-2\"></span>[28] V. J. Marin, T. Pereira, S. Sridharan, C.\
    \ R. Rivero, Automated personalized feedback in introductory Java programming\
    \ MOOCs, in: ICDE, 2017, pp. 1259–1270.\n- <span id=\"page-47-3\"></span>[29]\
    \ U. Z. Ahmed, Z. Fan, J. Yi, O. I. Al-Bataineh, A. Roychoudhury, Verifix: Verified\
    \ repair of programming assignments, ACM Trans. Softw. Eng. Methodol. 31 (2022)\
    \ 74:1–74:31.\n- <span id=\"page-47-4\"></span>[30] B. Edmison, S. H. Edwards,\
    \ Turn up the heat!: using heat maps to visualize suspicious code to help students\
    \ successfully complete programming problems faster, in: ICSE-SEET, 2020, pp.\
    \ 34–44.\n- <span id=\"page-47-5\"></span>[31] T. Nguyen, T. Le-Cong, D. Luong,\
    \ V. Duong, X. D. Le, D. Lo, Q. Huynh, FFL: fine-grained fault localization for\
    \ student programs via syntactic and semantic reasoning, in: ICSME, 2022, pp.\
    \ 151–162.\n- <span id=\"page-47-6\"></span>[32] Z. Li, S. Wu, Y. Liu, J. Shen,\
    \ Y. Wu, Z. Zhang, X. Chen, VsusFL: Variable-suspiciousness-based fault localization\
    \ for novice programs, J. Syst. Softw. 205 (2023) 111822.\n- <span id=\"page-47-7\"\
    ></span>[33] Y. Hu, U. Z. Ahmed, S. Mechtaev, B. Leong, A. Roychoudhury, Refactoring\
    \ based program repair applied to programming assignments, in: ASE, 2019, pp.\
    \ 388–398.\n- <span id=\"page-47-8\"></span>[34] C. Le Goues, T. Nguyen, S. Forrest,\
    \ W. Weimer, GenProg: A generic method for automatic software repair, TSE 38 (2012)\
    \ 54–72.\n- <span id=\"page-47-9\"></span>[35] J. Yi, U. Z. Ahmed, A. Karkare,\
    \ S. H. Tan, A. Roychoudhury, A feasibility study of using automated program repair\
    \ for introductory programming assignments, in: ESEC/FSE, 2017, pp. 740–751.\n\
    - <span id=\"page-48-0\"></span>[36] F. Long, M. Rinard, Automatic patch generation\
    \ by learning correct code, in: POPL, 2016, pp. 298–312.\n- [37] S. Sidiroglou-Douskos,\
    \ E. Lahtinen, F. Long, M. Rinard, Automatic error elimination by horizontal code\
    \ transfer across multiple applications, in: PLDI, 2015, pp. 43–54.\n- <span id=\"\
    page-48-1\"></span>[38] Q. Xin, S. P. Reiss, Leveraging syntax-related code for\
    \ automated program repair, in: ASE, 2017, pp. 660–670.\n- <span id=\"page-48-2\"\
    ></span>[39] C. K. Roy, J. R. Cordy, R. Koschke, Comparison and evaluation of\
    \ code clone detection techniques and tools: A qualitative approach, SCP 74 (2009)\
    \ 470–495.\n- <span id=\"page-48-3\"></span>[40] C. Liu, C. Chen, J. Han, P. S.\
    \ Yu, GPLAG: Detection of software plagiarism by program dependence graph analysis,\
    \ in: KDD, 2006, pp. 872–881.\n- [41] J. Li, M. D. Ernst, CBCD: Cloned buggy code\
    \ detector, in: ICSE, 2012, pp. 310–320.\n- [42] B. Sun, G. Shu, A. Podgurski,\
    \ S. Li, S. Zhang, J. Yang, Propagating bug fixes with fast subgraph matching,\
    \ in: ISSRE, 2010, pp. 21–30.\n- <span id=\"page-48-4\"></span>[43] S. Xu, Y.\
    \ S. Chee, Transformation-based diagnosis of student programs for programming\
    \ tutoring systems, TSE 29 (2003) 360–384.\n- <span id=\"page-48-5\"></span>[44]\
    \ W. Li, H. Saidi, H. Sanchez, M. Sch¨af, P. Schweitzer, Detecting similar programs\
    \ via the Weisfeiler-Leman graph kernel, in: ICSR, 2016, pp. 315–330."
  decisions:
    language: '- Qualified. Reason: English Paper'
    evaluation_prompt: Qualified.
    related_work_prompt: Qualified
    novelty_prompt: Qualified
    review_only_prompt: Qualified
  llm_input_used: '## Abstract

    Supporting learners in introductory programming assignments at scale is a

    necessity. This support includes automated feedback on what learners did

    incorrectly. Existing approaches cast the problem as automatically repairing

    learners'' incorrect programs extrapolating the data from an existing correct

    program from other learners. However, such approaches are limited because they

    only compare programs with similar control flow and order of statements. A

    potentially valuable set of repair feedback from flexible comparisons is thus

    missing. In this paper, we present several modifications to CLARA, a

    data-driven automated repair approach that is open source, to deal with

    real-world introductory programs. We extend CLARA''s abstract syntax tree

    processor to handle common introductory programming constructs. Additionally,

    we propose a flexible alignment algorithm over control flow graphs where we

    enrich nodes with semantic annotations extracted from programs using operations

    and calls. Using this alignment, we modify an incorrect program''s control flow

    graph to match the correct programs to apply CLARA''s original repair process.

    We evaluate our approach against a baseline on the twenty most popular

    programming problems in Codeforces. Our results indicate that flexible

    alignment has a significantly higher percentage of successful repairs at 46%

    compared to 5% for baseline CLARA. Our implementation is available at

    https://github.com/towhidabsar/clara.


    ## Introduction

    The worldwide interest in computer science has originated an unprecedented growth
    in the number of novice programming learners in both traditional and online settings
    [\[1–](#page-44-0)[4\]](#page-45-0). In the latter case, the number of novices
    taking programming Massive Open Online Courses and/or practicing using programming
    online judges has scaled to millions [\[5,](#page-45-1) [6\]](#page-45-2). One
    of the main challenges in the aforementioned context is supporting novice programming
    learners at scale [\[7\]](#page-45-3), which typically consists of delivering
    feedback explaining what and why they did incorrectly in their programs [\[8\]](#page-45-4).
    Note that, different than traditional settings, online programming settings often
    have a large proportion of novice learners with a variety of backgrounds, who
    usually tend to need a more direct level of feedback and assistance [\[9\]](#page-45-5).
    A common practice to address such a challenge is to rely on functional tests;
    however, feedback generated based solely on test cases does not sufficiently support
    novice learners [\[7,](#page-45-3) [10\]](#page-45-6).


    Current approaches cast the problem of delivering feedback to novices at scale
    as automatically repairing their incorrect programs [\[3,](#page-44-1) [7,](#page-45-3)
    [10–](#page-45-6)[13\]](#page-45-7). Note that, similar to existing approaches,
    we consider a program to be correct if it passes a number of predefined test cases
    [\[3,](#page-44-1) [10\]](#page-45-6); otherwise, it is incorrect. Once a repair
    is found, it can be used to determine pieces of feedback to deliver to learners
    [\[7\]](#page-45-3). Non-data-driven approaches aim to find repairs by mutating
    incorrect programs until they are correct, i.e., they pass all test cases [\[14\]](#page-45-8).
    Data-driven approaches exploit the fact that repairs can be found in existing
    correct programs and extrapolated to a given incorrect program [\[3\]](#page-44-1).
    This paper focuses on the latter since, in a given programming assignment, there
    is usually a variety of correct programs provided by other learners that can be
    exploited to repair incorrect programs [\[3,](#page-44-1) [10,](#page-45-6) [12,](#page-45-9)
    [13\]](#page-45-7).


    The "search, align and repair" [\[3\]](#page-44-1) framework consists of the following
    steps: 1) Given an incorrect program p<sup>i</sup> , search for a correct program
    p<sup>c</sup> that may be useful to repair p<sup>i</sup> ; 2) Align p<sup>i</sup>
    with respect to p<sup>c</sup> to identify discrepancies and potential modifications
    in order to repair p<sup>i</sup> ; and 3) Apply those modifications to p<sup>i</sup>
    until the resulting program p ′ <sup>i</sup> passes all test cases. Current approaches
    instantiating the "search, align and repair" framework use rigid comparisons to
    align incorrect and correct programs, i.e., they require the programs to have
    the same or very similar control flows (conditions and loops), and they are affected
    by the order of program statements [\[3,](#page-44-1) [10,](#page-45-6) [12,](#page-45-9)
    [13\]](#page-45-7). As a result, such approaches may miss a potentially valuable
    set of correct programs that can repair incorrect programs using flexible program
    comparisons.


    In this paper, we focus on CLARA [\[10\]](#page-45-6), a "search, align and repair"
    approach that is open source. We first adapt the original implementation of CLARA
    to support introductory programming assignments. This adaption involves non-trivial
    modifications to the parser and interpreter to support various constructs, such
    as print to and read from the console, import statements, built-in Python functions,
    and more. After these modifications, we also need to adapt the alignment and repair
    processes. Based on these foundations, we propose a flexible alignment algorithm
    that relies on control flow graphs. It exploits the semantic information (operations
    and calls) to annotate the graphs, and their topology information (edges, i.e.,
    True and False transitions). In order to evaluate the proposed algorithm, we create
    a dataset of incorrect and correct programs for the twenty most popular programming
    problems in the Codeforces online platform. Then, using the dataset, we execute
    CLARA''s baseline repair process and our flexible alignment repair to compare
    both the quantitative and qualitative performance of the proposed technique. Furthermore,
    we include another "search, align and repair" approach, Sarfgen [\[3\]](#page-44-1),
    by utilizing a similar process as our flexible alignment, but enforcing a high
    similarity between compared programs. This simulates the rigidity in program comparisons
    applied by Sarfgen. Note that Sarfgen is not publicly available; therefore, we
    needed to simulate it.


    Two short versions of this paper have been published elsewhere [\[15,](#page-46-0)
    [16\]](#page-46-1). In this paper, we describe in detail all the modifications
    that we made to CLARA, and how the parser and interpreter were updated. We also
    present our flexible alignment algorithm as well as the changes made to the programs
    at hand after an alignment is computed. These changes are necessary in order to
    apply CLARA''s repair process. Finally, we have significantly expanded our experiments
    to show the performance of our modifications over twenty real-world introductory
    programming assignments. We have made the implementation of this version of CLARA
    and our experimental dataset publicly available.[1](#page-2-0)


    <span id="page-2-0"></span><sup>1</sup><https://github.com/towhidabsar/clara>
    The fundamental contributions of this pa-


    The paper is organized as follows: Section [2](#page-3-0) summarizes previous
    approaches and ours; Section [3](#page-6-0) introduces CLARA''s parser, interpreter,
    aligner and repairer; Sections [4](#page-14-0) and [5](#page-19-0) describe our
    modifications to the parser and interpreter, and aligner and repairer of CLARA''s
    original implementation, respectively; Section [6](#page-21-0) presents our flexible
    alignment approach and the necessary modifications to CLARA''s repairer; Section
    [7](#page-30-0) discusses our experimental results; Section [8](#page-41-0) presents
    the related work; and Section [9](#page-43-0) presents our conclusions and future
    work.'
  token_usage: 7846
  time_usage: 2.2257983684539795
- title: "Which Syntactic Capabilities Are Statistically Learned by Masked\n  Language\
    \ Models for Code?"
  abstract: 'This paper discusses the limitations of evaluating Masked Language Models

    (MLMs) in code completion tasks. We highlight that relying on accuracy-based

    measurements may lead to an overestimation of models'' capabilities by

    neglecting the syntax rules of programming languages. To address these issues,

    we introduce a technique called SyntaxEval in which Syntactic Capabilities are

    used to enhance the evaluation of MLMs. SyntaxEval automates the process of

    masking elements in the model input based on their Abstract Syntax Trees

    (ASTs). We conducted a case study on two popular MLMs using data from GitHub

    repositories. Our results showed negative causal effects between the node types

    and MLMs'' accuracy. We conclude that MLMs under study fail to predict some

    syntactic capabilities.'
  url: http://arxiv.org/abs/2401.01512v2
  keywords: ''
  document: '# Which Syntactic Capabilities Are Statistically Learned by Masked Language
    Models for Code?


    Alejandro Velasco, David N. Palacio, Daniel Rodriguez-Cardenas and Denys Poshyvanyk


    {svelascodimate,danaderpalacio,dhrodriguezcar,dposhyvanyk}@wm.edu


    William & Mary


    Williamsburg, Virginia, USA


    #### ABSTRACT


    This paper discusses the limitations of evaluating Masked Language Models (MLMs)
    in code completion tasks. We highlight that relying on accuracy-based measurements
    may lead to an overestimation of models'' capabilities by neglecting the syntax
    rules of programming languages. To address these issues, we introduce a technique
    called SyntaxEval in which Syntactic Capabilities are used to enhance the evaluation
    of MLMs. SyntaxEval automates the process of masking elements in the model input
    based on their Abstract Syntax Trees (ASTs). We conducted a case study on two
    popular MLMs using data from GitHub repositories. Our results showed negative
    causal effects between the node types and MLMs'' accuracy. We conclude that MLMs
    under study fail to predict some syntactic capabilities.


    #### CCS CONCEPTS


    #### • Software and its engineering → Software maintenance tools.


    #### KEYWORDS


    deep learning, code generation, interpretability, transformers, dl4se


    #### ACM Reference Format:


    Alejandro Velasco, David N. Palacio, Daniel Rodriguez-Cardenas and Denys Poshyvanyk.
    2024. Which Syntactic Capabilities Are Statistically Learned by Masked Language
    Models for Code? . In New Ideas and Emerging Results (ICSE-NIER''24), April 14–20,
    2024, Lisbon, Portugal. ACM, New York, NY, USA, [5](#page-4-0) pages.<https://doi.org/10.1145/3639476.3639768>


    #### 1 INTRODUCTION


    Large language models have illustrated convincing performance across a range of
    different software engineering (SE) tasks [\[5,](#page-4-1) [7,](#page-4-2) [23,](#page-4-3)
    [35,](#page-4-4) [36,](#page-4-5) [39](#page-4-6)[–41\]](#page-4-7). In particular,
    code generation has been an important area of research for SE tasks such as code
    completion [\[8\]](#page-4-8). Code completion is a disciplined technique for
    generating missing syntactic features of an incomplete snippet based on its semantic
    and structural context [\[4\]](#page-4-9). These syntactic features usually adopt
    the form of identifiers, function names, conditionals, or parameters depending
    on the granularity of the snippet. Software researchers are particularly interested
    in improving code completion to optimize time spent during the development and
    maintenance cycles [\[12,](#page-4-10) [13\]](#page-4-11). Numerous studies have
    investigated code completion automation


    ICSE-NIER''24, April 14–20, 2024, Lisbon, Portugal


    © 2024 Copyright held by the owner/author(s).


    ACM ISBN 979-8-4007-0500-7/24/04.


    <https://doi.org/10.1145/3639476.3639768>


    using machine learning [\[4,](#page-4-9) [15,](#page-4-12) [28,](#page-4-13) [42\]](#page-4-14).
    Current research has focused on exploiting deep learning representations using
    LSTMs [\[33\]](#page-4-15), GPT [\[32\]](#page-4-16), RoBERTa [\[20\]](#page-4-17),
    and T5 [\[6,](#page-4-18) [9\]](#page-4-19).


    Masked Language Models (MLMs) have been recently used for code completion tasks
    demonstrating promising results (an avg. accuracy of 38.7% in perfect predictions)
    at different masking levels (i.e., Token, Construct, and Block) [\[6\]](#page-4-18).
    Some studies suggest that MLMs statistically learn the underlying structure of
    Abstract Syntax Trees (ASTs) at certain degree [\[19,](#page-4-20) [24,](#page-4-21)
    [37\]](#page-4-22). Yet, given the high accuracy achieved by MLMs [\[6\]](#page-4-18),
    few attempts have been made to investigate the role of Syntactic Capabilities
    for evaluating code completion. Syntactic Capabilities are interpretable prediction
    estimates for a terminal () and non-terminal (Σ) nodes of ASTs that are ruled
    by a Context Free Grammar (CFG) of Programming Languages (PLs) [\[31\]](#page-4-23).


    To date, the primary focus on evaluating MLMs has been on the role of accuracy
    as the principal metric, which may lead to erroneous and/or incomplete interpretation
    of the syntactic features embedded in neural architectures [\[25,](#page-4-24)
    [27,](#page-4-25) [37\]](#page-4-22). Relatively little is understood about incorporating
    these interpretable prediction estimates into the evaluation of MLMs, hence current
    evaluation methods do not help practitioners to decide whether MLMs are confidently
    generating code at AST node granularity and to what extent these syntactic features
    affect general prediction performance. That is, these methods do not reveal information
    about syntactic capabilities and their causal effects on the overall MLMs performance.


    Our study attempts to establish the causal connection between syntactic features
    in the form of AST node types and MLMs'' performance. Under this premise, we introduce
    SyntaxEval, an approach that leverages syntactic capabilities to evaluate how
    good MLMs infer and Σ AST nodes of a given PL. When evaluating the performance
    of an MLM, SyntaxEval selectively masks tokens according to the AST Node types
    defined by the CFG. Subsequently, an MLM predicts the masked tokens. Finally,
    SyntaxEval measures the causal effect of AST node types on code completion performance.


    Our results suggest that although MLMs are homogeneously predicting individual
    AST node types with high accuracy, we observed no evidence of effects from syntactic
    features on MLMs'' prediction after controlling for confounding factors. Hence,
    no causal evidence supports the fact that MLMs are statistically learning syntactic
    structures with acceptable confidence, contradicting recent studies in the explainability
    field [\[24,](#page-4-21) [37\]](#page-4-22). We hope that the results of our
    work will shed more light on the syntactic capabilities of current MLMs to enable
    a more systematic and rigorous evaluation of code completion tasks. The contributions
    of this paper are as follows: 1) a technique for evaluating the extent to which
    MLMs


    Permission to make digital or hard copies of part or all of this work for personal
    or classroom use is granted without fee provided that copies are not made or distributed
    for profit or commercial advantage and that copies bear this notice and the full
    citation on the first page. Copyrights for third-party components of this work
    must be honored. For all other uses, contact the owner/author(s).


    predict AST structures; 2) a case study that leverages causal analysis to understand
    how different AST node types influence code completion; 3) experimental data,
    curated datasets, source code, and complementary statistical analysis used in
    this research are published in an open-source repository [\[1\]](#page-4-26).


    #### 2 BACKGROUND & RELATED WORK


    The accurate identification and generation of code tokens is a widely studied
    field at the intersection of SE and DL [\[38\]](#page-4-27). State-of-the-art
    code generators estimate the token prediction using probabilistic distribution
    (i.e., a Large Language Model (LLMs)) obtained by training on large amounts of
    code corpora. Put simply, code completion models should statistically approximate
    the production rules defined by the CFG. These production rules are recursively
    applied to terminal and non-terminal Σ nodes to formally define the structure
    of a PL. For instance, recent explainability studies have claimed that the syntactic
    structures of code are encapsulated in the internal layers of LLMs across software
    tasks, implying a foundational statistical comprehension of code semantics [\[14,](#page-4-28)
    [37\]](#page-4-22). In this section, we introduce the concept of MLMs and their
    current evaluation methods.


    Masked Language Models for Code. Considerable research attention has been directed
    toward the usage of Bidirectional Encoder Representation from Transformers (BERT)
    on code completion as an attempt to push the predictability boundaries beyond
    the next token prediction. BERT allows higher granularity syntax structures (i.e.,
    entire code statement) to be generated using self-attention layers trained to
    restore a masked subset of tokens in the input [\[6,](#page-4-18) [10\]](#page-4-29).
    This peculiar form of training the architecture is known as denoising autoencoding,
    or Masked Language Models (MLM), which we formalize as () = E∈E <sup>⊂</sup> hÍ
    <sup>∈</sup> log ( |˜) i , || = |()|, where a masking rate (usually 15%) is applied
    on the original sequence of a training corpus . The model attempts to predict
    the set of masked tokens given the corrupted context ˜ (the masked version of
    ) [\[18\]](#page-4-30). MLMs for code completion are mostly evaluated using metrics
    such as CodeBleu, EM, F1, and Pass@k [\[16\]](#page-4-31).


    Syntax-Based Evaluation of MLMs. Due to the unpredictable behavior of MLMs while
    generating tokens, explainability techniques are complementary evaluative methods
    for understanding the decision-making process by reducing the uncertainty of the
    models. Such uncertainty can be controlled by exploring the inner layers of the
    neural net or performing guided perturbations on models'' input [\[3\]](#page-4-32).
    Recent studies have explored the use of structural information as an interpretability
    tool for pre-trained models for code [\[25\]](#page-4-24). For instance, Wan et
    al. [\[37\]](#page-4-22) conducted an explainability analysis focusing on three
    aspects: 1) how the self-attention weights align with the syntax structure, 2)
    whether the syntax structure is encoded in the hidden layers, and 3) how pre-trained
    models induce syntax structures. Similarly, Mohammadkhani et al. [\[24\]](#page-4-21)
    propose an eXplainable AI method (attention mechanism) on three downstream code
    tasks: 1) code generation, 2) refinement, and 3) translation. Previous findings
    imply that Encoder-based models can effectively extract detailed syntactic information
    using selfattention mechanisms. We used prior observations about encoded information
    of ASTs to formulate an evaluative approach based


    <span id="page-1-0"></span>![](_page_1_Figure_7.jpeg)


    Figure 1: SyntaxEval Process for the identifier AST Node.


    on measuring the prediction performance of syntactic capabilities directly from
    (non)terminal nodes.


    #### 3 SYNTACTIC CAUSAL EVALUATION


    SyntaxEval is an evaluative approach organized into two distinct parts. The first
    part estimates a fine-grained performance, grouped by AST node types, for a given
    MLM ([RQ](#page-2-0)1). The second part adopts causal interpretability theory
    to quantify the influence of previously estimated AST node types on the accuracy
    of the model ([RQ](#page-2-1)2).


    Evaluating Syntactic Capabilities. Fig. [1](#page-1-0) depicts the process of
    evaluating syntactic capabilities for code completion using MLMs. This evaluative
    process is comprised of five steps. Firstly, we must define a set of AST node
    types to be analyzed. This set of AST node types is ruled by Python CFG adopting
    the form of = ∪ Σ. Then we search for the positions of these node types in the
    code sequence after iterating for each sample (i.e., snippet) of a given ground
    truth (Fig. [1-](#page-1-0) 1 ). Secondly, detected tokens, which correspond to
    the previously defined set , are masked with the label <mask> (Fig. [1-](#page-1-0)
    2 ). Thirdly, we use an MLM to infer the masked tokens for each sample obtaining
    a set ¯ of predicted samples¯ . Fourthly, we parse the AST of and ¯ to generate
    a list of extracted nodes for the ground truth and predicted samples using the
    in-order traversal algorithm (Fig. [1-](#page-1-0) 4 ). Finally, we compare both
    ground truth and predicted ¯ lists of extracted nodes by computing three similarity
    metrics for each sample (i.e., Jaccard, Levenshtein & Sorensen-Dice) (Fig. [1-](#page-1-0)
    5 ).


    Computing Causal Interpretability. Causal Inference has been adopted to complement
    the assessment of LLMs by controlling for confounding factors in code data. Palacio
    et al. [\[25\]](#page-4-24) introduce , a post hoc interpretability methodology
    that explains model predictions by providing causal explanations. These explanations
    are generated by estimating the effect of binary interventions , such as masking
    random tokens <sup>0</sup> versus masking AST node types 1, on MLMs'' performance.
    Specifically, in SyntaxEval, the treatment <sup>1</sup> refers to samples that
    are masked on AST node tokens , while the control <sup>0</sup> refers to samples
    that are randomly masked on any position. The control <sup>0</sup> preserves the
    same number of masked tokens as in 1.


    SyntaxEval formulates a Structural Causal Model (SCM), which is a graphical model
    composed of outcomes, treatments, and confounders [\[26\]](#page-4-33), to explain
    a set of potential outcomes (e.g., Jaccard, Levenshtein, Sorensen-Dice) in terms
    of treatments (i.e., <span id="page-2-2"></span>Which Syntactic Capabilities Are
    Statistically Learned by Masked Language Models for Code? ICSE-NIER''24, April
    14–20, 2024, Lisbon, Portugal


    Table 1: Evaluated Encoder-Based Transformers.


    | Id | MLM                     | Size | Layers | Vocab. |

    |----|-------------------------|------|--------|--------|

    | 𝑀1 | CodeBERTa-small-v1 [21] | 84M  | 6      | 52,000 |

    | 𝑀2 | codebert-base-mlm [11]  | 125M | 12     | 50,265 |


    masked AST node types) by controlling for a set of code confounders to avoid spurious
    correlations. These code confounders consist of seven variables, which include
    the # of parsing errors, the height of the AST, the # of nodes, the # of whitespaces,
    the # of lines of codes, the cyclo complexity, and the token counts. Finally,
    SyntaxEval computes the Average Treatment Effect () of a treatment has on the
    outcomes after controlling for confounders . In other words, we want to estimate
    the expected value = E[ ()] = E[ | (1)] − E[ | (0)] = E[<sup>1</sup> − 0]. The
    variables 1, <sup>0</sup> refer to potential outcomes observed under the treatments
    1,0. For the sake of brevity, we do not discuss the details of treatment effects
    computations. However, these effects are approximated using propensity score methods
    after applying the the back-door criterion [\[30\]](#page-4-36).


    #### 4 CASE STUDY DESIGN


    This section outlines the methodology employed to consider the potential influence
    of syntactic capabilities on the evaluation of MLMs, we conducted a case study
    on two popular architectures to explore the following RQs:


    - <span id="page-2-0"></span>RQ1 [Performance] How good are MLMs at predicting
    AST nodes?

    - <span id="page-2-1"></span>RQ2 [Causality] How do node types impact MLMs'' performance?


    Data Collection: To mitigate the risk of data snooping, we curated our testbed
    with 50 Python snippets. This testbed exclusively comprises commits executed between
    January 01, 2022 and January 01, 2023. We collected the snippets from newly added
    or updated Python Github repositories with over 1k stars scoring. Additionally,
    we discarded duplicated samples by referring to the history of the commits. The
    testbed also contains complementary code features (e.g., LoC, CYCLO, and # of
    nodes), these features were extracted using Galeras pipeline [\[29\]](#page-4-37).
    Masked Language Models: We evaluated two encoder-based transformers trained on
    CodeSearchNet [\[17\]](#page-4-38) with different hyperparameters (see Tab. [1\)](#page-2-2).
    These encoders have been assessed in prior studies in which they were found to
    capture structural information [\[37\]](#page-4-22), [\[37\]](#page-4-22), and
    [\[24\]](#page-4-21). Node Types: Tree-sitter CFG defines 196 AST node types for
    Python. For the sake of simplicity, we selected a subset of terminal and non-terminal
    nodes defined in Python''s CFG as depicted in the first column of Tab. [2.](#page-3-0)
    The subset entails the most basic syntactic structures for control, iteration,
    operators, and functional programming. This study showcases the nodes that exhibited
    the most interesting behavior. We chose Python for code completion experiments
    due to its extended use in recent studies.


    Evaluation Methodology. To address [RQ](#page-2-0)1, we estimated syntactic capabilities
    of <sup>1</sup> and <sup>2</sup> encoders using 8 randomly selected samples from
    the collected testbed. SyntaxEval masks the associated tokens for each chosen
    node type (1) and subsequently uses the MLM to infer the missing elements. Then,
    we compute normalized similarity distances (i.e., Jaccard, Levenshtain, and Sorence-Dice)
    between the AST in-order traversal of both the predictions and the ground truth.
    Global results indicate the average prediction


    accuracy (i.e., normalized distance) for all node types within . In contrast,
    local results detail the prediction accuracy for individual node types.


    To address [RQ](#page-2-1)2, SyntaxEval computes the Average Causal Effect between
    syntactic capabilities and MLMs'' performance. This method consists of estimating
    using treatments <sup>1</sup> and <sup>0</sup> (i.e., tokens randomly masked)
    while controlling for confounders in (code features in Data Collection), to mitigate
    the presence of spurious correlations. The removal of confounding bias can be
    formally achieved using both an SCM and the -operator introduced by Pearl et al.
    [\[26\]](#page-4-33). To verify the robustness of our SCM, we computed placebo
    refutations, which is a method that fakes an unrelated treatment by re-estimating
    the causal effects. That is, we assessed that the causal effects of the fake treatment
    on the outcome were close to zero. Moreover, to ensure a balanced distribution
    of randomly masking tokens within 0, we created 20 distinct variations for each
    sample. Afterward, we computed the average of the resulting similarity scores.
    Finally, to ensure statistical significance, we bootstrapped the similarity scores
    using the for 500 samples per node type.


    #### 5 RESULTS & DISCUSSION


    The aim of this study is to determine the effect of Syntactic Capabilities, in
    the form of interpretable prediction estimates for node types, on the prediction
    performance of MLMs. We concentrated on evaluating Encoder-based Transformers
    beyond accuracy.


    ## 5.1 [RQ](#page-2-0)<sup>1</sup> Syntactic Capabilities Performance


    Global Results. A cursory glance at Tab. [2](#page-3-0) reveals that control groups
    <sup>0</sup> of each performance metric are not significantly different from treatments
    <sup>1</sup> for both encoders. For example, the control median values greater
    than 0.8 are within the interquartile range () 0.78 ± 0.22 of the corresponding
    treatment. Furthermore, the standard deviation () values of the performance are
    predominantly more dispersed in the treatments than in the control. For example,
    the <sup>1</sup> of <sup>1</sup> Jaccard is 0.21, while the <sup>0</sup> is 0.17.
    Appealingly, all average values of performance are above 0.5, this indicates that
    <sup>1</sup> and <sup>2</sup> models are predicting masking tokens with high confidence
    despite the group treatments . Although the median global performance has consistently
    high accuracy among the metrics (> 0.8), the average separation values between
    the groups are not significant with an average median distance of 0.096 and 0.06
    for <sup>1</sup> and <sup>2</sup> respectively. However, a preliminary analysis
    for node types estimations suggests that <sup>1</sup> and <sup>2</sup> have a
    tendency to not statistically learn syntactic-oriented masked tokens 1. Our findings
    reveal a subtle inclination towards predicting random masked tokens over syntactic-oriented
    ones.


    <span id="page-2-3"></span>![](_page_2_Figure_17.jpeg)


    Figure 2: <sup>0</sup> vs. <sup>1</sup> Local Jaccard for Nodes using 1.


    <span id="page-3-0"></span>Table 2: Global Perf. and Causal Effects for <sup>1</sup>
    and 2.


    | Performance                                                                     |
    Jaccard                                                                                                     |                                                                         |
    Levenshtein |        | Sorensen-Dice |        |  |  |  |

    |---------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------|-------------|--------|---------------|--------|--|--|--|

    | MLMs                                                                            |
    𝑀1                                                                                                          |
    𝑀2                                                                      | 𝑀1          |
    𝑀2     | 𝑀1            | 𝑀2     |  |  |  |

    | Treatments                                                                      |
    Performance Metric 𝑌 [avg ± std]*<br>0.88 ± 0.17 0.84 ± 0.16 0.87 ± 0.16 0.83
    ± 0.17 0.92 ± 0.1 0.89 ± 0.12 |                                                                         |             |        |               |        |  |  |  |

    | 𝑇0                                                                              |                                                                                                             |                                                                         |             |        |               |        |  |  |  |

    | 𝑇1                                                                              |                                                                                                             |
    0.78 ± 0.21 0.76 ± 0.21 0.78 ± 0.22 0.76 ± 0.21 0.85 ± 0.17 0.84 ± 0.17 |             |        |               |        |  |  |  |

    | AST Node Type 𝐶                                                                 |
    CausalEffect 𝜏                                                                                              |                                                                         |             |        |               |        |  |  |  |

    | boolean_operator                                                                |
    -0.083                                                                                                      |
    -0.150                                                                  | -0.069      |
    -0.136 | -0.048        | -0.095 |  |  |  |

    | comparison_operator                                                             |
    -0.186                                                                                                      |
    -0.027                                                                  | -0.179      |
    -0.018 | -0.126        | -0.015 |  |  |  |

    | for_in_clause                                                                   |
    -0.059                                                                                                      |
    -0.053                                                                  | -0.050      |
    -0.045 | -0.034        | -0.029 |  |  |  |

    | for_statement                                                                   |
    -0.269                                                                                                      |
    -0.101                                                                  | -0.193      |
    -0.041 | -0.243        | -0.083 |  |  |  |

    | identifier                                                                      |
    0.016                                                                                                       |
    -0.075                                                                  | 0.001       |
    -0.073 | 0.010         | -0.039 |  |  |  |

    | if_clause                                                                       |
    -0.070                                                                                                      |
    -0.040                                                                  | -0.058      |
    -0.036 | -0.037        | -0.022 |  |  |  |

    | if_statement                                                                    |
    -0.163                                                                                                      |
    -0.118                                                                  | -0.140      |
    -0.093 | -0.116        | -0.095 |  |  |  |

    | parameters                                                                      |
    -0.140                                                                                                      |
    -0.048                                                                  | -0.127      |
    -0.046 | -0.087        | -0.029 |  |  |  |

    | return_statement                                                                |
    -0.144                                                                                                      |
    -0.121                                                                  | -0.118      |
    -0.113 | -0.087        | -0.075 |  |  |  |

    | string                                                                          |
    -0.156                                                                                                      |
    -0.168                                                                  | -0.102      |
    -0.145 | -0.118        | -0.116 |  |  |  |

    | while_statement                                                                 |
    -0.200                                                                                                      |
    -0.096                                                                  | -0.139      |
    -0.009 | -0.186        | -0.077 |  |  |  |

    | * Medians are > 0.8. The biggest causal effect 𝜏 for each node type is in gray.
    |                                                                                                             |                                                                         |             |        |               |        |  |  |  |


    Local Results. Fig. [2](#page-2-3) shows the Jaccard performance statistical behavior
    across some selected node types for 1. Due to the nonoverlapping between the <sup>0</sup>
    and 1, we observed a significant difference between treatment groups in the performance
    distribution for the nodes comparison\_operator and string, revealing that <sup>1</sup>
    struggles at predicting tokens associated with such types in contrast to random
    masked tokens. We found that identifier was the only node type that performed
    better in the treatment than the control group. Fig. [3](#page-3-1) presents the
    Empirical Cumulative Distribution (ECD) plots of <sup>1</sup> Jaccard distance
    across selected node types. We observed that if\_clause was remarkably achieved
    with the highest score prediction (0.9) at the lowest percentage of the population
    (42% of the samples in the testbed). Conversely, for\_statement was the most difficult
    node to predict across the population. We believe that MLMs struggle to predict
    these previous nodes due to their complexity. A node is complex when its block
    has incorporated other node types.


    <span id="page-3-1"></span>![](_page_3_Figure_4.jpeg)


    Figure 3: Syntactic Capabilities Statistically Learned by 1.


    [RQ](#page-2-0)1: MLMs tend to complete missing AST-masked tokens with acceptable
    accuracy (> 0.5). However, the reported performance suffers from high variability
    (±0.21) making the prediction process less confident compared to completing randomly
    masking tokens.


    ## 5.2 [RQ](#page-2-1)<sup>2</sup> Causal Evaluation Effect


    This study used a quantitative causal technique to analyze the influence of masking
    binary treatments (i.e., AST and random) on the performance of both <sup>1</sup>
    and 2 transformers after defining the


    Structural Causal Model of the problem. To draw a causal link between syntactic
    features (i.e., AST nodes) and performance metrics (i.e., Jaccard, Levenshtain,
    and SD), we expect to observe a positive causal effect. A positive effect would
    indicate that syntactic features are affecting models'' performance and AST nodes
    would be statistically learned by MLMs. On the other hand, a negative causal effect
    would imply that randomly masked tokens have more influence on the performance.
    That is, tokens without any particular syntactic order are being predicted accurately.


    Unlike previous assumptions, it can be inferred from Tab. [2](#page-3-0) that
    the control group (i.e., masking random treatment) is having more impact on MLMs''
    performance than the actual syntactic features. For instance, a set of samples
    masked for for\_statement tokens are underperforming (a.k.a. negative effects)
    compared to the same set but randomly masked tokens. This suggests that although
    transformers are predicting AST node types with confidence (see Fig. [3,](#page-3-1)
    these syntactic features are not particularly relevant compared to predicting
    any other set of unstructured tokens in the snippet (see gray areas in Tab. [2\)](#page-3-0).
    These findings tend to corroborate Karmakar et al. research [\[19\]](#page-4-20)
    in which MLMs do not fully grasp the syntax and structural aspects of code. Our
    findings offer an alternative perspective compared to claims made by other probing
    approaches [\[22,](#page-4-39) [34\]](#page-4-40). For example, Hernandez Lopez
    et al. [\[14\]](#page-4-28) argue for the presence of a syntax subspace within
    the hidden layers that encode structures of PLs. Similarly, Toufique et al. [\[2\]](#page-4-41)
    outline that pre-trained language models learn robust representations of code
    semantics, which implies a deep understanding of syntax elements from the source
    code.


    [RQ](#page-2-1)2: The performance of MLMs is negatively impacted by ASTmasked
    tokens ( < −0.1). Our causal analysis yielded no signs of Transformers'' performance
    being affected or guided by syntactic features, contradicting SOTA explainability
    findings.


    #### 6 CONCLUSION & FUTURE PLANS


    Our negative causal effect results corroborate recent findings that show flaws
    when claiming that MLMs are understanding syntax rules of PLs. Such effects amplify
    the disparities between Natural and Programming languages, underscoring the need
    for tailored representations in deep learning architectures. These findings pave
    the way for future research to evaluate semantic capabilities in the form of recursions,
    dead code, or code smells. We also highlight the necessity to delve deeper into
    understanding why MLMs are more adept at predicting random masked tokens than
    syntax-based tokens. This tendency may be linked to the models'' pre-training
    objectives, which frequently involve masking random tokens at a certain rate [\[10\]](#page-4-29).


    #### 7 ACKNOWLEDGEMENTS


    This research has been supported in part by the NSF CCF-2311469, CNS-2132281,
    CCF-2007246, and CCF-1955853. We also acknowledge support from Cisco Systems.
    Any opinions, findings, and conclusions expressed herein are the authors'' and
    do not necessarily reflect those of the sponsors.


    <span id="page-4-0"></span>Which Syntactic Capabilities Are Statistically Learned
    by Masked Language Models for Code? ICSE-NIER''24, April 14–20, 2024, Lisbon,
    Portugal


    #### REFERENCES


    - <span id="page-4-26"></span>[1] 2023. WM-SEMERU/SyntaxEval.<https://github.com/WM-SEMERU/SyntaxEval>
    original-date: 2022-09-09T20:53:59Z.

    - <span id="page-4-41"></span>[2] Toufique Ahmed, Dian Yu, Chengxuan Huang, Cathy
    Wang, et al. 2023. Towards Understanding What Code Language Models Learned. [https://doi.org/10.48550/](https://doi.org/10.48550/arXiv.2306.11943)
    [arXiv.2306.11943](https://doi.org/10.48550/arXiv.2306.11943) arXiv:2306.11943
    [cs].

    - <span id="page-4-32"></span>[3] Vaishak Belle and Ioannis Papantonis. 2020.
    Principles and Practice of Explainable Machine Learning. CoRR abs/2009.11698 (2020).
    arXiv:2009.11698 [https://arxiv.](https://arxiv.org/abs/2009.11698) [org/abs/2009.11698](https://arxiv.org/abs/2009.11698)

    - <span id="page-4-9"></span>[4] Marcel Bruch, Martin Monperrus, and Mira Mezini.
    [n. d.]. Learning from examples to improve code completion systems. In Proceedings
    of the 7th joint meeting of the European software engineering conference and the
    ACM SIGSOFT symposium on The foundations of software engineering (2009-08-24).
    ACM, 213– 222.<https://doi.org/10.1145/1595696.1595728>

    - <span id="page-4-1"></span>[5] Zimin Chen, Steve James Kommrusch, Michele Tufano,
    Louis-Noël Pouchet, et al. 2019. SEQUENCER: Sequence-to-Sequence Learning for
    End-to-End Program Repair. IEEE Transactions on Software Engineering (2019), 1–1.
    [https://doi.org/10.](https://doi.org/10.1109/TSE.2019.2940179) [1109/TSE.2019.2940179](https://doi.org/10.1109/TSE.2019.2940179)

    - <span id="page-4-18"></span>[6] Matteo Ciniselli, Nathan Cooper, Luca Pascarella,
    Antonio Mastropaolo, et al. [n. d.]. An Empirical Study on the Usage of Transformer
    Models for Code Completion. ([n. d.]), 1–1.<https://doi.org/10.1109/TSE.2021.3128234>

    - <span id="page-4-2"></span>[7] Matteo Ciniselli, Nathan Cooper, Luca Pascarella,
    Antonio Mastropaolo, et al. 2021. An Empirical Study on the Usage of Transformer
    Models for Code Completion. arXiv[:cs.SE/2108.01585](https://arxiv.org/abs/cs.SE/2108.01585)

    - <span id="page-4-8"></span>[8] Matteo Ciniselli, Nathan Cooper, Luca Pascarella,
    Denys Poshyvanyk, et al. 2021. An Empirical Study on the Usage of BERT Models
    for Code Completion. CoRR abs/2103.07115 (2021). arXiv:2103.07115<https://arxiv.org/abs/2103.07115>

    - <span id="page-4-19"></span>[9] Colin Clement, Dawn Drain, Jonathan Timcheck,
    Alexey Svyatkovskiy, et al. [n. d.]. PyMT5: multi-mode translation of natural
    language and Python code with transformers. In Proceedings of the 2020 Conference
    on Empirical Methods in Natural Language Processing (EMNLP) (2020). Association
    for Computational Linguistics, 9052–9065.<https://doi.org/10.18653/v1/2020.emnlp-main.728>

    - <span id="page-4-29"></span>[10] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
    Kristina Toutanova. [n. d.]. BERT: Pre-training of Deep Bidirectional Transformers
    for Language Understanding. <https://doi.org/10.48550/arXiv.1810.04805> arXiv[:1810.04805
    \[cs\]](https://arxiv.org/abs/1810.04805 [cs])

    - <span id="page-4-35"></span>[11] Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan,
    et al. 2020. CodeBERT: A Pre-Trained Model for Programming and Natural Languages.
    arXiv[:cs.CL/2002.08155](https://arxiv.org/abs/cs.CL/2002.08155)

    - <span id="page-4-10"></span>[12] Sangmok Han, David R. Wallace, and Robert C.
    Miller. [n. d.]. Code Completion from Abbreviated Input. In 2009 IEEE/ACM International
    Conference on Automated Software Engineering (2009-11). IEEE, 332–343. [https://doi.org/10.1109/ASE.2009.](https://doi.org/10.1109/ASE.2009.64)
    [64](https://doi.org/10.1109/ASE.2009.64)

    - <span id="page-4-11"></span>[13] Sangmok Han, David R. Wallace, and Robert C.
    Miller. [n. d.]. Code completion of multiple keywords from abbreviated input.
    18, 3 ([n. d.]), 363–398. [https:](https://doi.org/10.1007/s10515-011-0083-2)
    [//doi.org/10.1007/s10515-011-0083-2](https://doi.org/10.1007/s10515-011-0083-2)

    - <span id="page-4-28"></span>[14] José Antonio Hernández López, Martin Weyssow,
    Jesús Sánchez Cuadrado, and Houari Sahraoui. 2022. AST-Probe: Recovering abstract
    syntax trees from hidden representations of pre-trained language models. Proceedings
    of the 37th IEEE/ACM International Conference on Automated Software Engineering
    (Oct. 2022), 1–11.<https://doi.org/10.1145/3551349.3556900> Conference Name: ASE
    ''22: 37th IEEE/ACM International Conference on Automated Software Engineering
    ISBN: 9781450394758 Place: Rochester MI USA Publisher: ACM.

    - <span id="page-4-12"></span>[15] Abram Hindle, Earl T. Barr, Zhendong Su, Mark
    Gabel, et al. 2012. On the naturalness of software. In 2012 34th International
    Conference on Software Engineering (ICSE). 837–847.<https://doi.org/10.1109/ICSE.2012.6227135>

    - <span id="page-4-31"></span>[16] Xinyi Hou, Yanjie Zhao, Yue Liu, Zhou Yang,
    et al. 2023. Large Language Models for Software Engineering: A Systematic Literature
    Review. [http://arxiv.org/abs/](http://arxiv.org/abs/2308.10620) [2308.10620](http://arxiv.org/abs/2308.10620)
    arXiv:2308.10620 [cs].

    - <span id="page-4-38"></span>[17] Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit,
    Miltiadis Allamanis, et al. 2019. CodeSearchNet Challenge: Evaluating the State
    of Semantic Code Search. arXiv:1909.09436 [cs, stat] (Sept. 2019).<http://arxiv.org/abs/1909.09436>
    arXiv: 1909.09436.

    - <span id="page-4-30"></span>[18] Masahiro Kaneko, Masato Mita, Shun Kiyono,
    Jun Suzuki, et al. 2020. Encoder-Decoder Models Can Benefit from Pre-trained Masked
    Language Models in Grammatical Error Correction. arXiv[:cs.CL/2005.00987](https://arxiv.org/abs/cs.CL/2005.00987)

    - <span id="page-4-20"></span>[19] Anjan Karmakar and Romain Robbes. 2023. INSPECT:
    Intrinsic and Systematic Probing Evaluation for Code Transformers. IEEE Transactions
    on Software Engineering (2023), 1–19.<https://doi.org/10.1109/TSE.2023.3341624>
    Conference Name: IEEE Transactions on Software Engineering.

    - <span id="page-4-17"></span>[20] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei
    Du, et al. 2019. RoBERTa: A Robustly Optimized BERT Pretraining Approach. arXiv[:cs.CL/1907.11692](https://arxiv.org/abs/cs.CL/1907.11692)

    - <span id="page-4-34"></span>[21] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei
    Du, et al. 2019. RoBERTa: A Robustly Optimized BERT Pretraining Approach. [https://doi.org/10.48550/arXiv.1907.](https://doi.org/10.48550/arXiv.1907.11692)
    [11692](https://doi.org/10.48550/arXiv.1907.11692) arXiv:1907.11692 [cs].

    - <span id="page-4-39"></span>[22] Wei Ma, Mengjie Zhao, Xiaofei Xie, Qiang Hu,
    et al. 2023. Are Code Pre-trained Models Powerful to Learn Code Syntax and Semantics?
    [https://doi.org/10.48550/](https://doi.org/10.48550/arXiv.2212.10017) [arXiv.2212.10017](https://doi.org/10.48550/arXiv.2212.10017)
    arXiv:2212.10017 [cs].

    - <span id="page-4-3"></span>[23] Antonio Mastropaolo, Simone Scalabrino, Nathan
    Cooper, David Nader Palacio, et al. 2021. Studying the Usage of Text-To-Text Transfer
    Transformer to Support Code-Related Tasks. (2021), 336–347. [https://doi.org/10.1109/icse43902.2021.](https://doi.org/10.1109/icse43902.2021.00041)
    [00041](https://doi.org/10.1109/icse43902.2021.00041)

    - <span id="page-4-21"></span>[24] Ahmad Haji Mohammadkhani and Hadi Hemmati.
    [n. d.]. Explainable AI for Pre-Trained Code Models: What Do They Learn? When
    They Do Not Work? ([n. d.]).

    - <span id="page-4-24"></span>[25] David N. Palacio, Nathan Cooper, Alvaro Rodriguez,
    Kevin Moran, et al. [n. d.]. Toward a Theory of Causation for Interpreting Neural
    Code Models. [https:](https://doi.org/10.48550/arXiv.2302.03788) [//doi.org/10.48550/arXiv.2302.03788](https://doi.org/10.48550/arXiv.2302.03788)
    arXiv[:2302.03788 \[cs, stat\]](https://arxiv.org/abs/2302.03788 [cs, stat])

    - <span id="page-4-33"></span>[26] Judea Pearl. 2009. Causality: models, reasoning,
    and inference.

    - <span id="page-4-25"></span>[27] Rafiqul Islam Rabin, Arjun Mukherjee, Omprakash
    Gnawali, and Mohammad Amin Alipour. [n. d.]. Towards Demystifying Dimensions of
    Source Code Embeddings. ([n. d.]), 29–38. ISBN: 9781450381253.

    - <span id="page-4-13"></span>[28] Veselin Raychev, Martin Vechev, and Eran Yahav.
    [n. d.]. Code completion with statistical language models. In Proceedings of the
    35th ACM SIGPLAN Conference on Programming Language Design and Implementation
    (2014-06-09). ACM, 419– 428.<https://doi.org/10.1145/2594291.2594321>

    - <span id="page-4-37"></span>[29] Daniel Rodriguez-Cardenas, David N. Palacio,
    Dipin Khati, Henry Burke, et al. 2023. Benchmarking Causal Study to Interpret
    Large Language Models for Source Code. In 2023 IEEE International Conference on
    Software Maintenance and Evolution (ICSME). 329–334.<https://doi.org/10.1109/ICSME58846.2023.00040>

    - <span id="page-4-36"></span>[30] Amit Sharma, Vasilis Syrgkanis, Cheng Zhang,
    and Emre Kıcıman. 2021. DoWhy : Addressing Challenges in Expressing and Validating
    Causal Assumptions. (2021).

    - <span id="page-4-23"></span>[31] P. K. Srimani and S. F. B. Nasir. 2007. A Textbook
    on Automata Theory. Foundation Books.<https://doi.org/10.1017/UPO9788175968363>

    - <span id="page-4-16"></span>[32] Alexey Svyatkovskiy, Shao Kun Deng, Shengyu
    Fu, and Neel Sundaresan. [n. d.]. IntelliCode compose: code generation using transformer.
    In Proceedings of the 28th ACM Joint Meeting on European Software Engineering
    Conference and Symposium on the Foundations of Software Engineering (2020-11-08).
    ACM, 1433–1443.<https://doi.org/10.1145/3368089.3417058>

    - <span id="page-4-15"></span>[33] Alexey Svyatkovskiy, Ying Zhao, Shengyu Fu,
    and Neel Sundaresan. [n. d.]. Pythia: AI-assisted Code Completion System. In Proceedings
    of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data
    Mining (2019- 07-25). 2727–2735.<https://doi.org/10.1145/3292500.3330699> arXiv[:1912.00742](https://arxiv.org/abs/1912.00742
    [cs]) [\[cs\]](https://arxiv.org/abs/1912.00742 [cs])

    - <span id="page-4-40"></span>[34] Sergey Troshin and Nadezhda Chirkova. 2022.
    Probing Pretrained Models of Source Codes. Proceedings of the Fifth BlackboxNLP
    Workshop on Analyzing and Interpreting Neural Networks for NLP (2022), 371–383.
    [https://doi.org/10.18653/v1/](https://doi.org/10.18653/v1/2022.blackboxnlp-1.31)
    [2022.blackboxnlp-1.31](https://doi.org/10.18653/v1/2022.blackboxnlp-1.31) Conference
    Name: Proceedings of the Fifth BlackboxNLP Workshop on Analyzing and Interpreting
    Neural Networks for NLP Place: Abu Dhabi, United Arab Emirates (Hybrid) Publisher:
    Association for Computational Linguistics.

    - <span id="page-4-4"></span>[35] Michele Tufano, Cody Watson, Gabriele Bavota,
    Massimiliano Di Penta, et al. 2018. Deep Learning Similarities from Different
    Representations of Source Code. In 2018 IEEE/ACM 15th International Conference
    on Mining Software Repositories (MSR). 542–553.

    - <span id="page-4-5"></span>[36] Rosalia Tufano, Luca Pascarella, Michele Tufano,
    Denys Poshyvanyk, et al. 2021. Towards Automating Code Review Activities. In 43rd
    International Conference on Software Engineering, ICSE''21.<https://arxiv.org/abs/2101.02518>

    - <span id="page-4-22"></span>[37] Yao Wan, Wei Zhao, Hongyu Zhang, Yulei Sui,
    et al. [n. d.]. What Do They Capture? – A Structural Analysis of Pre-Trained Language
    Models for Source Code.<https://doi.org/10.48550/arXiv.2202.06840> arXiv[:2202.06840
    \[cs\]](https://arxiv.org/abs/2202.06840 [cs])

    - <span id="page-4-27"></span>[38] Cody Watson, Nathan Cooper, David Nader Palacio,
    Kevin Moran, et al. 2022. A Systematic Literature Review on the Use of Deep Learning
    in Software Engineering Research. ACM Transactions on Software Engineering and
    Methodology 31, 2 (March 2022), 32:1–32:58.<https://doi.org/10.1145/3485275>

    - <span id="page-4-6"></span>[39] Cody Watson, Michele Tufano, Kevin Moran, Gabriele
    Bavota, et al. 2020. On Learning Meaningful Assert Statements for Unit Test Cases.
    In Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering
    (ICSE ''20). Association for Computing Machinery, New York, NY, USA, 1398–1409.
    [https:](https://doi.org/10.1145/3377811.3380429) [//doi.org/10.1145/3377811.3380429](https://doi.org/10.1145/3377811.3380429)

    - [40] Martin White, Michele Tufano, Matías Martínez, Martin Monperrus, et al.
    2019. Sorting and Transforming Program Repair Ingredients via Deep Learning Code
    Similarities. In 2019 IEEE 26th International Conference on Software Analysis,
    Evolution and Reengineering (SANER). 479–490. [https://doi.org/10.1109/SANER.](https://doi.org/10.1109/SANER.2019.8668043)
    [2019.8668043](https://doi.org/10.1109/SANER.2019.8668043)

    - <span id="page-4-7"></span>[41] Martin White, Michele Tufano, Christopher Vendome,
    and Denys Poshyvanyk. 2016. Deep learning code fragments for code clone detection.
    In 2016 31st IEEE/ACM International Conference on Automated Software Engineering
    (ASE). 87–98.

    - <span id="page-4-14"></span>[42] Martin White, Christopher Vendome, Mario Linares-Vasquez,
    and Denys Poshyvanyk. [n. d.]. Toward Deep Learning Software Repositories. In
    2015 IEEE/ACM 12th Working Conference on Mining Software Repositories (2015-05).
    IEEE, 334–345. <https://doi.org/10.1109/MSR.2015.38>'
  decisions:
    language: '- Qualified. Reason: English Paper'
    evaluation_prompt: Qualified.
    related_work_prompt: Qualified
    novelty_prompt: Qualified
    review_only_prompt: Qualified
  llm_input_used: '## Abstract

    This paper discusses the limitations of evaluating Masked Language Models

    (MLMs) in code completion tasks. We highlight that relying on accuracy-based

    measurements may lead to an overestimation of models'' capabilities by

    neglecting the syntax rules of programming languages. To address these issues,

    we introduce a technique called SyntaxEval in which Syntactic Capabilities are

    used to enhance the evaluation of MLMs. SyntaxEval automates the process of

    masking elements in the model input based on their Abstract Syntax Trees

    (ASTs). We conducted a case study on two popular MLMs using data from GitHub

    repositories. Our results showed negative causal effects between the node types

    and MLMs'' accuracy. We conclude that MLMs under study fail to predict some

    syntactic capabilities.


    ## Introduction

    Large language models have illustrated convincing performance across a range of
    different software engineering (SE) tasks [\[5,](#page-4-1) [7,](#page-4-2) [23,](#page-4-3)
    [35,](#page-4-4) [36,](#page-4-5) [39](#page-4-6)[–41\]](#page-4-7). In particular,
    code generation has been an important area of research for SE tasks such as code
    completion [\[8\]](#page-4-8). Code completion is a disciplined technique for
    generating missing syntactic features of an incomplete snippet based on its semantic
    and structural context [\[4\]](#page-4-9). These syntactic features usually adopt
    the form of identifiers, function names, conditionals, or parameters depending
    on the granularity of the snippet. Software researchers are particularly interested
    in improving code completion to optimize time spent during the development and
    maintenance cycles [\[12,](#page-4-10) [13\]](#page-4-11). Numerous studies have
    investigated code completion automation


    ICSE-NIER''24, April 14–20, 2024, Lisbon, Portugal


    © 2024 Copyright held by the owner/author(s).


    ACM ISBN 979-8-4007-0500-7/24/04.


    <https://doi.org/10.1145/3639476.3639768>


    using machine learning [\[4,](#page-4-9) [15,](#page-4-12) [28,](#page-4-13) [42\]](#page-4-14).
    Current research has focused on exploiting deep learning representations using
    LSTMs [\[33\]](#page-4-15), GPT [\[32\]](#page-4-16), RoBERTa [\[20\]](#page-4-17),
    and T5 [\[6,](#page-4-18) [9\]](#page-4-19).


    Masked Language Models (MLMs) have been recently used for code completion tasks
    demonstrating promising results (an avg. accuracy of 38.7% in perfect predictions)
    at different masking levels (i.e., Token, Construct, and Block) [\[6\]](#page-4-18).
    Some studies suggest that MLMs statistically learn the underlying structure of
    Abstract Syntax Trees (ASTs) at certain degree [\[19,](#page-4-20) [24,](#page-4-21)
    [37\]](#page-4-22). Yet, given the high accuracy achieved by MLMs [\[6\]](#page-4-18),
    few attempts have been made to investigate the role of Syntactic Capabilities
    for evaluating code completion. Syntactic Capabilities are interpretable prediction
    estimates for a terminal () and non-terminal (Σ) nodes of ASTs that are ruled
    by a Context Free Grammar (CFG) of Programming Languages (PLs) [\[31\]](#page-4-23).


    To date, the primary focus on evaluating MLMs has been on the role of accuracy
    as the principal metric, which may lead to erroneous and/or incomplete interpretation
    of the syntactic features embedded in neural architectures [\[25,](#page-4-24)
    [27,](#page-4-25) [37\]](#page-4-22). Relatively little is understood about incorporating
    these interpretable prediction estimates into the evaluation of MLMs, hence current
    evaluation methods do not help practitioners to decide whether MLMs are confidently
    generating code at AST node granularity and to what extent these syntactic features
    affect general prediction performance. That is, these methods do not reveal information
    about syntactic capabilities and their causal effects on the overall MLMs performance.


    Our study attempts to establish the causal connection between syntactic features
    in the form of AST node types and MLMs'' performance. Under this premise, we introduce
    SyntaxEval, an approach that leverages syntactic capabilities to evaluate how
    good MLMs infer and Σ AST nodes of a given PL. When evaluating the performance
    of an MLM, SyntaxEval selectively masks tokens according to the AST Node types
    defined by the CFG. Subsequently, an MLM predicts the masked tokens. Finally,
    SyntaxEval measures the causal effect of AST node types on code completion performance.


    Our results suggest that although MLMs are homogeneously predicting individual
    AST node types with high accuracy, we observed no evidence of effects from syntactic
    features on MLMs'' prediction after controlling for confounding factors. Hence,
    no causal evidence supports the fact that MLMs are statistically learning syntactic
    structures with acceptable confidence, contradicting recent studies in the explainability
    field [\[24,](#page-4-21) [37\]](#page-4-22). We hope that the results of our
    work will shed more light on the syntactic capabilities of current MLMs to enable
    a more systematic and rigorous evaluation of code completion tasks. The contributions
    of this paper are as follows: 1) a technique for evaluating the extent to which
    MLMs


    Permission to make digital or hard copies of part or all of this work for personal
    or classroom use is granted without fee provided that copies are not made or distributed
    for profit or commercial advantage and that copies bear this notice and the full
    citation on the first page. Copyrights for third-party components of this work
    must be honored. For all other uses, contact the owner/author(s).


    predict AST structures; 2) a case study that leverages causal analysis to understand
    how different AST node types influence code completion; 3) experimental data,
    curated datasets, source code, and complementary statistical analysis used in
    this research are published in an open-source repository [\[1\]](#page-4-26).'
  token_usage: 6322
  time_usage: 2.0296099185943604
- title: "De-Hallucinator: Mitigating LLM Hallucinations in Code Generation Tasks\n\
    \  via Iterative Grounding"
  abstract: 'Large language models (LLMs) trained on datasets of publicly available
    source

    code have established a new state of the art in code generation tasks. However,

    these models are mostly unaware of the code that exists within a specific

    project, preventing the models from making good use of existing APIs. Instead,

    LLMs often invent, or "hallucinate", non-existent APIs or produce variants of

    already existing code. This paper presents De-Hallucinator, a technique that

    grounds the predictions of an LLM through a novel combination of retrieving

    suitable API references and iteratively querying the model with increasingly

    suitable context information in the prompt. The approach exploits the

    observation that predictions by LLMs often resemble the desired code, but they

    fail to correctly refer to already existing APIs. De-Hallucinator automatically

    identifies project-specific API references related to the model''s initial

    predictions and adds these references into the prompt. Unlike

    retrieval-augmented generation (RAG), our approach uses the initial

    prediction(s) by the model to iteratively retrieve increasingly suitable API

    references. Our evaluation applies the approach to two tasks: predicting API

    usages in Python and generating tests in JavaScript. We show that

    De-Hallucinator consistently improves the generated code across five LLMs. In

    particular, the approach improves the edit distance by 23.3-50.6% and the

    recall of correctly predicted API usages by 23.9-61.0% for code completion, and

    improves the number of fixed tests that initially failed because of

    hallucinations by 63.2%, resulting in a 15.5% increase in statement coverage

    for test generation.'
  url: http://arxiv.org/abs/2401.01701v3
  keywords: ''
  document: "# De-Hallucinator: Mitigating LLM Hallucinations in Code Generation Tasks\
    \ via Iterative Grounding\n\nAryaz Eghbali Software Lab University of Stuttgart\
    \ Stuttgart, Germany aryaz.eghbali@iste.uni-stuttgart.de\n\n#### ABSTRACT\n\n\
    Large language models (LLMs) trained on datasets of publicly available source\
    \ code have established a new state of the art in code generation tasks. However,\
    \ these models are mostly unaware of the code that exists within a specific project,\
    \ preventing the models from making good use of existing APIs. Instead, LLMs often\
    \ invent, or \"hallucinate\", non-existent APIs or produce variants of already\
    \ existing code. This paper presents De-Hallucinator, a technique that grounds\
    \ the predictions of an LLM through a novel combination of retrieving suitable\
    \ API references and iteratively querying the model with increasingly suitable\
    \ context information in the prompt. The approach exploits the observation that\
    \ predictions by LLMs often resemble the desired code, but they fail to correctly\
    \ refer to already existing APIs. De-Hallucinator automatically identifies project-specific\
    \ API references related to the model's initial predictions and adds these references\
    \ into the prompt. Unlike retrieval-augmented generation (RAG), our approach uses\
    \ the initial prediction(s) by the model to iteratively retrieve increasingly\
    \ suitable API references. Our evaluation applies the approach to two tasks: predicting\
    \ API usages in Python and generating tests in JavaScript. We show that De-Hallucinator\
    \ consistently improves the generated code across five LLMs. In particular, the\
    \ approach improves the edit distance by 23.3–50.6% and the recall of correctly\
    \ predicted API usages by 23.9–61.0% for code completion, and improves the number\
    \ of fixed tests that initially failed because of hallucinations by 63.2%, resulting\
    \ in a 15.5% increase in statement coverage for test generation.\n\n#### 1 INTRODUCTION\n\
    \nLarge language models (LLMs) have proven effective in many natural language\
    \ [\\[7\\]](#page-10-0) and programming tasks [\\[4,](#page-10-1) [9,](#page-10-2)\
    \ [23,](#page-10-3) [46,](#page-11-0) [54,](#page-11-1) [61,](#page-11-2) [63\\\
    ]](#page-11-3). Rapid adoption of LLM-based tools, such as Copilot[1](#page-0-0)\
    \ and Tabnine[2](#page-0-1) , shows practical productivity benefits [\\[33,](#page-10-4)\
    \ [66\\]](#page-11-4). State-of-the-art LLMs build on transformers [\\[58\\]](#page-11-5),\
    \ which use self-attention to generate sequences of tokens in an auto-regressive\
    \ process. That is, the model decides what token to predict next based on the\
    \ tokens in the prompt and any already generated tokens. Hence, designing effective\
    \ prompts, sometimes called prompt engineering, is a crucial part of developing\
    \ a practical LLM-based technique [\\[35,](#page-11-6) [39,](#page-11-7) [56\\\
    ]](#page-11-8).\n\nDespite the impressive success of LLM-based code generation,\
    \ these techniques are still at an early stage. In particular, we identify two\
    \ key challenges faced by current approaches:\n\nChallenge 1: Project-specific\
    \ APIs. As LLMs are trained on huge code bases, they effectively capture typical\
    \ language idioms and\n\nMichael Pradel Software Lab University of Stuttgart Stuttgart,\
    \ Germany michael@binaervarianz.de\n\n<span id=\"page-0-2\"></span>\n\n| DataStore.py<br>class\
    \ DataStore():<br>def __init__(self, file: str):                             \
    \                   |\n|---------------------------------------------------------------------------------------------------------------------|\n\
    |                                                                            \
    \                                         |\n|                               \
    \                                                                            \
    \          |\n| with open(file, 'r') as f:                                   \
    \                                                       |\n| self.documents =\
    \ f.read().split('-----')                                                    \
    \                        |\n| <br>def find_by_keyword(self, keyword: str) -> List[str]:<br>return\
    \ [d for d in self.documents if keyword in d]<br> |\n| utils.py<br>          \
    \                                                                            \
    \                  |\n| def relevance(document: str, keyword: str) -> float:<br>return\
    \ document.count(keyword) / len(document)<br>          |\n| UI.py            \
    \                                                                            \
    \                       |\n| <br>def search(ds: DataStore, keyword: str, top_k:\
    \ int) -> List[str]:<br>docs = ds.find_by_keyword(keyword)         |\n| return\
    \ sorted(docs, key=lambda d: relevance(d, keyword),                          \
    \                                  |\n| reverse=True)[:top_k]                \
    \                                                                            \
    \   |\n|                                                                     \
    \                                                |\n|                        \
    \                                                                            \
    \                 |\n\nFigure 1: The desired completion of **search** is highlighted\
    \ in gray .\n\n<span id=\"page-0-3\"></span>\n\n|  | def search(ds: DataStore,\
    \ keyword: str, top_k: int) -> List[str]: |  |  |  |  |\n|--|-------------------------------------------------------------------|--|--|--|--|\n\
    |  | docs = ds.find_by_keyword(keyword)                                |  |  |\
    \  |  |\n|  | return sorted(docs, key=lambda x: x.score , reverse=True)[:top_k]\
    \ |  |  |  |  |\n\nFigure 2: The completion of **search** by CodeGen-2B-mono highlighted\
    \ in gray , and the wrong API usage highlighted in red .\n\ncommonly used libraries.\
    \ In contrast, a general-purpose model lacks knowledge of project-specific APIs,\
    \ and may fail to correctly use existing functions and classes. In particular,\
    \ this lack of knowledge may cause the model to \"hallucinate\" APIs that actually\
    \ do not exist in the current code base [\\[40\\]](#page-11-9), or perhaps even\
    \ worse, it may reimplement some functionality that is already present in the\
    \ code base. Developers perceive this lack of knowledge about projectspecific\
    \ APIs as an obstacle to using AI programming assistants [\\[33\\]](#page-10-4).\n\
    \nAs a running example, consider three files in a large project dealing with text\
    \ documents, shown in Fig. [1.](#page-0-2) One file, DataStore.py, contains a\
    \ class implementing a data structure that stores documents and provides a keyword-based\
    \ search over the documents. Another file, utils.py, provides helper functions,\
    \ one of which allows for measuring the relevance of a document to a keyword.\
    \ In a third file, UI.py, the developer is working on a function, search, to search\
    \ for the top\\_k documents that are most relevant to a keyword.\n\nRequesting\
    \ an LLM, e.g., CodeGen [\\[43\\]](#page-11-10), to complete the search function\
    \ given a prompt that contains all existing code in UI.py results in Fig. [2.](#page-0-3)\
    \ The code is partially correct, but refers to a nonexisting API (an attribute\
    \ x.score). The underlying problem is that the models are not aware of the project-specific\
    \ APIs that should be used to complete the code, and hence, the LLM simply hallucinates\
    \ some plausible but ultimately wrong APIs.\n\n<span id=\"page-0-0\"></span><sup>1</sup>https://github.com/features/copilot\n\
    \n<span id=\"page-0-1\"></span><sup>2</sup>https://www.tabnine.com/\n\nChallenge\
    \ 2: Prioritizing context. A naive solution to address Challenge 1 would be to\
    \ simply add all of the code in the project into the prompt. However, LLMs have\
    \ a fixed maximum sequence length, which restricts how many tokens one can provide\
    \ to the model. Even with the recent increases in sequence length of LLMs, providing\
    \ the most useful context can improve the output and reduce the costs. Choosing\
    \ the most helpful context for a given completion task is crucial, but an inherently\
    \ difficult problem, because the optimal context depends on the desired code,\
    \ which is not known a-priori. While traditional code completion approaches typically\
    \ have access to various kinds of information available in IDEs, such as the names\
    \ and types of program elements, providing all this information, or even all the\
    \ code of the project, to an LLM is impossible due to the limited prompt size.\n\
    \nThis paper presents De-Hallucinator, which addresses the above challenges through\
    \ a novel combination of retrieval-augmented generation (RAG) and an iterative\
    \ form of LLM-based code generation. Our approach uses three types of prompts,\
    \ which provide increasingly suitable context information. The initial prompt\
    \ type is querying the LLM with the conventional prompt, i.e., without any retrieval.\
    \ Retrieval-augmented generation (RAG) [\\[28\\]](#page-10-5) proposes to retrieve\
    \ relevant context based on the initial prompt to address both Challenges 1 and\
    \ 2, which we refer to as the RAG prompt type. The idea of augmenting an LLM with\
    \ well-grounded facts relates to work on grounding of language models for natural\
    \ languages [\\[2,](#page-10-6) [17,](#page-10-7) [52\\]](#page-11-11). However,\
    \ also this prompt may fail to generate factually correct code (i.e., without\
    \ hallucinations), because the initial prompt might not have any similar code\
    \ to the desired API, or there are other APIs more similar to the initial prompt\
    \ than the correct one. We make the improtant observation that the generated code\
    \ from the previous prompt types often resembles the desired API. Hence, we construct\
    \ a new type of prompt called the iterative prompt. De-Hallucinator leverages\
    \ the code that the model predicts to retrieve suitable project-specific APIs,\
    \ which are then added to the iterative prompt for the next round of model prediction.\
    \ The iterative prompt type complements prior work that tries to guess the most\
    \ suitable context from the incomplete code alone [\\[12,](#page-10-8) [56\\]](#page-11-8).\n\
    \nThe presented approach offers several benefits. First, De-Hallucinator works\
    \ with any off-the-shelf LLM trained on code because the approach treats the model\
    \ as a black box. In particular, we do not require to train or fine-tune the model\
    \ in any way, but simply exploit the fact that its predictions contain implicit\
    \ hints about additional context the model would benefit from. Second, because\
    \ APIs usually evolve only slowly, De-Hallucinator can precompute, and occasionally\
    \ update in the background, the set of project-specific API references. As a result,\
    \ the latency of code generation is not impacted by any expensive program analysis,\
    \ which is important for practical adoption. Finally, the approach is fully transparent\
    \ to developers, because the approach hides the iterative interaction with the\
    \ LLM from the user and simply returns a ranked list of predictions.\n\nWe evaluate\
    \ De-Hallucinator by applying the approach to code completion with four state-of-the-art\
    \ LLMs for code, namely Code-Gen [\\[43\\]](#page-11-10), CodeGen 2.5 [\\[42\\\
    ]](#page-11-12), UniXcoder [\\[18\\]](#page-10-9), and StarCoder+ [\\[30\\]](#page-10-10),\
    \ and to test generation with GPT-3.5-turbo. Conceptually, the approach can be\
    \ applied to any programming language, and our evaluation focuses on two popular\
    \ languages, Python and JavaScript\n\nas they are among the most popular languages\
    \ [3](#page-1-0) and common targets of prior work on code completion [\\[9,](#page-10-2)\
    \ [18,](#page-10-9) [29,](#page-10-11) [30,](#page-10-10) [42,](#page-11-12) [43,](#page-11-10)\
    \ [57,](#page-11-13) [65\\]](#page-11-14) and test generation [\\[3,](#page-10-12)\
    \ [54\\]](#page-11-1). Compared to conventional prompts, we find that De-Hallucinator\
    \ enables the models to provide more accurate predictions. In particular, we show\
    \ a relative improvement of 23.3–50.6% in edit distance, and of 23.9–61.0% in\
    \ recall of correctly predicted API usages for code completion. Moreover, we show\
    \ relative improvement of 17.9% in number of passing tests, of 15.5% in coverage,\
    \ and of 63.2% in the number of mitigated hallucinations. In summary, this paper\
    \ contributes the following:\n\n• Empirical motivation showing that API hallucinations\
    \ affect a large portion of failed code completion and test generation tasks.\n\
    \n- A technique for addressing this problem using off-the-shelf, unmodified LLMs.\n\
    - A novel algorithm that combines retrieval-augmented generation with an iterative\
    \ method for constructing increasingly suitable prompts by using the hallucinations\
    \ produced in earlier iterations to augment the context information provided in\
    \ the prompts of future iterations.\n- Empirical evidence that, across two code\
    \ generation tasks, two programming languages, and five state-of-the-art LLMs,\
    \ De-Hallucinator offers more accurate generations than conventional prompts.\n\
    \n## 2 PRELIMINARY STUDY\n\nBefore delving into our approach, we validate the\
    \ motivation for this work by performing a preliminary study, which assesses the\
    \ importance of the two challenges described in the introduction.\n\n#### 2.1\
    \ Project-Specific APIs\n\nThe main motivation for this work is our observation\
    \ that LLMs often hallucinate code that resembles the desired code, but that fail\
    \ to correctly refer to an API. To assess the importance of this limitation, we\
    \ investigate the prevalence and causes of hallucinated APIs in the two code generation\
    \ tasks focused in this paper. For code completion, we manually investigate and\
    \ classify the reasons why an LLM fails to predict the desired completion. We\
    \ perform this preliminary study on 50 function-level code completion tasks, which\
    \ we collect by (i) randomly selecting ten Python projects from a curated list\
    \ of open-source projects [4](#page-1-1) and (ii) by then randomly selecting five\
    \ functions from each project. The only filtering we perform is to ignore functions\
    \ with more than 25 lines, as these are likely out of reach for today's LLMs.\
    \ For each of the 50 functions, we query an LLM (CodeGen 2.5 with 7B parameters\
    \ and 4-bit quantization) with the code before the beginning of the function body,\
    \ including the function signature and any docstring, in the prompt.\n\nGiven\
    \ the 50 pairs of an LLM-predicted function body and the ground-truth function\
    \ body, we manually classify them based on two questions. First, is the prediction\
    \ correct w.r.t. the ground truth, where \"correct\" includes exact matches and\
    \ semantically equivalent code? Second, does the ground truth contain an API usage,\
    \ e.g., a function call, that is missing in the prediction? Initially, two\n\n\
    <span id=\"page-1-0\"></span><sup>3</sup><https://octoverse.github.com/2022/top-programming-languages>\n\
    \n<span id=\"page-1-1\"></span><sup>4</sup>[https://github.com/vinta/awesome-python.](https://github.com/vinta/awesome-python)\
    \ We randomly sample ten application domains and then sample one project from\
    \ each domain.\n\nof the authors independently classify the 50 pairs, with an\
    \ interrater agreement (Cohen's kappa) of 0.76, which is considered excellent\
    \ [\\[14\\]](#page-10-13), and after discussing the discrepencies reach a consensus\
    \ about all 50 pairs.\n\nThe final inspection results show that in 13 out of the\
    \ 50 cases, the LLM either predicts exactly the expected function body or a function\
    \ body that is semantically equivalent to the expected one. For 22 out of the\
    \ 37 remaining cases, there is at least one API usage that the LLM fails to correctly\
    \ predict, similar to the example in Fig. [2.](#page-0-3) In other words, the\
    \ problem identified and addressed in this work affects 44% of all studied function-level\
    \ code completion tasks, and even 59% of all tasks where the LLM alone fails to\
    \ predict the expected code.\n\nFor test generation, we use error messages of\
    \ crashing tests generated by TestPilot [\\[54\\]](#page-11-1) on a diverse set\
    \ of 12 JavaScript projects. We automatically count the number of generated tests\
    \ that result in \"\\* is not a function\", or \"Cannot read properties of undefined\"\
    \ errors, which typically indicates hallucinations of non-existing APIs. We find\
    \ that, on average, 16.4% of all generated tests fail because of the aforementioned\
    \ errors, which indicates that hallucinations of non-existing APIs are a common\
    \ problem in test generation as well.\n\n## 2.2 Prioritizing Context\n\nTo validate\
    \ the importance of the second challenge, we compare the amount of code in a single\
    \ project to the prompt sizes of highend LLMs. The models in the popular GPT series\
    \ by OpenAI have prompt sizes between 4k (GPT-3.5 models) and 128k (GPT-4) tokens.\
    \ In contrast, in a sample dataset of 50 Python projects, which are randomly selected\
    \ from the same curated list of projects as above, there are 488,635 tokens per\
    \ project, on average. Furthermore, the average project has around 13 files longer\
    \ than 8,192 tokens, and 22 projects in our sample have at least one file longer\
    \ than 32,768 tokens. This means that even knowing the exact file that contains\
    \ the relevant context (e.g., based on heuristics, such as recently used files\
    \ or similar file names) leaves us with more tokens than one could fit into the\
    \ prompt of some models. Even for models with longer context window, considering\
    \ a cost of 0.5\\$ for 100k tokens means that the cost of long prompts would be\
    \ impractical for regular use. In other words, simply adding all potentially relevant\
    \ code to the prompt is not a viable solution, but we need to prioritize the context\
    \ information.\n\n#### 3 APPROACH\n\nThis section describes our approach for iteratively\
    \ retrieving relevant APIs to improve the prompts for code generation tasks. We\
    \ call the approach De-Hallucinator, as it reduces the hallucinations of the LLM\
    \ by providing relevant API references to ground the model. First, we provide\
    \ an overview of the approach (Section [3.1\\)](#page-2-0), and then present each\
    \ of the components of De-Hallucinator in detail (Sections [3.2](#page-2-1) to\
    \ [3.5\\)](#page-4-0).\n\n#### <span id=\"page-2-0\"></span>3.1 Overview\n\n3.1.1\
    \ Main Algorithm. Figure [3](#page-2-2) gives an overview of the approach, which\
    \ we use to illustrate the main algorithm. The top of the figure shows the traditional\
    \ code generation process, where an LLM receives a prompt and generates code.\
    \ We call this prompt the initial\n\n<span id=\"page-2-2\"></span>![](_page_2_Figure_10.jpeg)\n\
    \nFigure 3: Overview of De-Hallucinator.\n\nprompt. Because the model may not\
    \ be aware of project-specific APIs, the output is likely to refer to some hallucinated\
    \ APIs.\n\nTo help the model predict better code, De-Hallucinator refines the\
    \ initial prompt using two techniques. Both of them add API references to the\
    \ prompt, but they do so in different ways. First, as shown by the dotted lines,\
    \ De-Hallucinator uses the initial prompt to retrieve related API references from\
    \ the project. This approach is similar to retrieval-augmented generation (RAG)\
    \ [\\[28\\]](#page-10-5), and we refer to the resulting prompt as the RAG prompt.\n\
    \nSecond, as shown by the dashed lines, De-Hallucinator uses the output of the\
    \ model to retrieve related API references. This technique is unique to our approach,\
    \ and it is based on the observation that the model's output often resembles the\
    \ desired code, but fails to refer to the correct APIs. Retrieving suitable API\
    \ references based on the model's output can be done multiple times, and hence,\
    \ we refer to the resulting prompt as the iterative prompt. In general, De-Hallucinator\
    \ repeats the iterative prompt refinement, i.e., the dashed loop in the figure,\
    \ until exhausting a configurable maximum number of queries to the model.\n\n\
    For efficient retrieval of APIs, De-Hallucinator analyzes the project in advance,\
    \ as shown in the \"Pre-analysis\" component, and indexes all APIs of the project.\n\
    \n3.1.2 Example. Fig. [4](#page-3-0) shows each step of the approach on our running\
    \ example from Fig. [1.](#page-0-2) Given the initial prompt, the initial completion\
    \ by the model refers to a non-existing API x.score. Then, for the RAG prompt\
    \ the initial prompt is used for retrieval. In this step, as shown in Fig. [4,](#page-3-0)\
    \ the reference to an already used function, find\\_by\\_keyword is retrieved.\
    \ Consequently, the completion uses the wrong API, as the model still does not\
    \ have knowledge of the relevance function. Next, using the initial completion\
    \ by the model, De-Hallucinator retrieves a reference to the relevance function\
    \ defined in utils.py. In the example, the iterative prompt results in the correct\
    \ completion, as shown at the bottom of Fig. [4.](#page-3-0)\n\nThe following\
    \ presents each component of De-Hallucinator in more detail, as well as how we\
    \ instantiate the components for our two target tasks, code completion and test\
    \ generation.\n\n#### <span id=\"page-2-1\"></span>3.2 Pre-Analysis\n\n3.2.1 General\
    \ Idea. To ensure that the retrieval of API references does not unnecessarily\
    \ slow down the code generation, De-Hallucinator has a preprocessing phase that\
    \ indexes the current project for fast retrieval. We use API reference throughout\
    \ this paper to refer to a piece of text extracted from the project's code, which\
    \ can be added to the prompt to provide further information about a project-specific\
    \ API.\n\n<span id=\"page-3-0\"></span>\n\n| Initial prompt                  \
    \                                                                            \
    \                                                                            \
    \                                                      |\n|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n\
    | <br>def search(ds: DataStore, keyword: str, top_k: int) -> List[str]:<br>docs\
    \ = ds.find_by_keyword(keyword)                                              \
    \                                                                            \
    \         |\n| Initial completion<br>def search(ds: DataStore, keyword: str, top_k:\
    \ int) -> List[str]:<br>docs = ds.find_by_keyword(keyword)<br>return sorted(docs,\
    \ key=lambda x: x.score , reverse=True)[:top\\_k]                            \
    \               |\n| Most relevant API reference<br>DataStore.find_by_keyword(self,\
    \ keyword: str) -> List[str]                                                 \
    \                                                                            \
    \                        |\n| RAG prompt<br># API Reference:<br># DataStore.find_by_keyword(self,\
    \ keyword: str) -> List[str]<br>def search(ds: DataStore, keyword: str, top_k:\
    \ int) -> List[str]:<br>docs = ds.find_by_keyword(keyword)                   \
    \                  |\n| RAG prompt completion<br>def search(ds: DataStore, keyword:\
    \ str, top_k: int) -> List[str]:<br>docs = ds.find_by_keyword(keyword)<br>return\
    \ sorted(docs, key=lambda x: x.score , reverse=True)[:top\\_k]               \
    \                         |\n| Most relevant API reference<br>relevance(document:\
    \ str, keyword: str) -> float                                                \
    \                                                                            \
    \                                    |\n| Iterative prompt<br># API Reference:<br>#\
    \ relevance(document: str, keyword: str) -> float<br>def search(ds: DataStore,\
    \ keyword: str, top_k: int) -> List[str]:<br>docs = ds.find_by_keyword(keyword)\
    \                                          |\n| Iterative prompt completion<br>def\
    \ search(ds: DataStore, keyword: str, top_k: int) -> List[str]:<br>docs = ds.find_by_keyword(keyword)<br>return\
    \ sorted(docs, key=lambda doc: relevance(keyword, doc),<br>reverse=True)[:top\\\
    _k] # <- Correct |\n\n<span id=\"page-3-1\"></span>Figure 4: Step-by-step progression\
    \ of De-Hallucinator on the example in Fig. [1.](#page-0-2) Table 1: Examples\
    \ of API references extracted from the project in Fig. [1.](#page-0-2)\n\n| Source\
    \       | API reference                                                 | Type<br>of<br>API<br>reference\
    \ |\n|--------------|---------------------------------------------------------------|--------------------------------|\n\
    | DataStore.py | DataStore.find_by_keyword(self,<br>keyword: str) -> List[str]\
    \ | Function refer<br>ence         |\n| utils.py     | relevance(document:<br>str,<br>keyword:\
    \ str) -> float         | Function refer<br>ence         |\n| DataStore.py | class\
    \ DataStore()                                             | Class reference  \
    \              |\n| DataStore.py | DataStore.documents                       \
    \                    | Attribute refer<br>ence        |\n\nDefinition 3.1 (API\
    \ reference). An API reference is one of the following:\n\n- A function reference,\
    \ which consists of\n\t- the qualified name of the function,\n\t- the parameter\
    \ names,\n\t- any available default values for arguments,\n\t- any available type\
    \ annotations, and\n\t- any available function-level docstring.\n- A class reference,\
    \ which consists of\n\t- the qualified name of the class,\n\t- the parent class(es),\
    \ and\n\t- any available class-level docstring.\n- An attribute reference, which\
    \ is the qualified name of a self attribute assigned to in the constructor.\n\n\
    3.2.2 Example. Table [1](#page-3-1) shows some of the API references extracted\
    \ from our example project. Note that the API references resemble real code to\
    \ maintain compatibility with any model that is trained on source code.\n\n3.2.3\
    \ Application to Code Completion. For the code completion task, we use CodeQL[5](#page-3-2)\
    \ to statically extract APIs from the project source. The Python language support\
    \ of CodeQL offers easy access to the classes, functions, etc. in a code base.\
    \ Another benefit of CodeQL is that we can utilize databases created by GitHub\
    \ for open-source projects.\n\n3.2.4 Application to Test Generation. Since the\
    \ APIs in JavaScript are in many cases only available once a module is instantiated,\
    \ De-Hallucinator dynamically loads the modules and traverses them to extract\
    \ API references.\n\nAlternatively to our approaches for gathering API references,\
    \ an IDE-based implementation could reuse information about the current project\
    \ that is anyway computed by the static code indexing in an IDE.\n\n#### 3.3 Retrieval\
    \ of Related APIs\n\n3.3.1 General Idea. The retrieval module takes an input code\
    \ piece and returns a ranked list of project-specific API references that are\
    \ most similar to the input. To enable similarity-based search, De-Hallucinator\
    \ embeds the extracted API references into a vector space. Formally, we need an\
    \ embedding function, , for which () = ∈ R , such that, for two code pieces <sup>1</sup>\
    \ and 2, the cosine similarity of their embeddings, <sup>1</sup> · <sup>2</sup>\
    \ /(|<sup>1</sup> ||<sup>2</sup> |), approximates the semantic similarity of <sup>1</sup>\
    \ and 2. Recently, many models have been trained for this task [\\[1\\]](#page-10-14).\
    \ Because our approach uses the embedding function as a black-box, any embedding\
    \ model or similarity-preserving vector representation [\\[59\\]](#page-11-15)\
    \ can be used with De-Hallucinator, e.g., GloVe [\\[45\\]](#page-11-16), BERT\
    \ [\\[11\\]](#page-10-15), or FastText [\\[5\\]](#page-10-16).\n\nGiven the embeddings\
    \ of the API references, De-Hallucinator retrieves API references that are most\
    \ similar to the input code piece. To this end, the approach embeds the input\
    \ code piece and searches for the API references that minimize the cosine distance\
    \ to the input code piece. The parameter specifies the number of API references\
    \ to include in the prompt.\n\n3.3.2 Example. Getting back to our running example,\
    \ consider the third section in Fig. [4.](#page-3-0) It shows the API reference\
    \ from our example project that the retrieval component finds to be the topmost\
    \ relevant to the incomplete code (initial prompt). Even though our implementation\
    \ retrieves relevant API references, we show only one in the example for brevity.\n\
    \n3.3.3 Application to Code Completion. For code completion, we split the given\
    \ input code piece into lines, then retrieve the most relevant APIs for each line,\
    \ and finally merge the most similar API references into single sorted list. The\
    \ reason for retrieving the API references similar to full lines of code, as opposed\
    \ to only API usages, is that we want the completion to avoid re-implementing\
    \ existing code. Therefore, if there exists some API similar to a line of code\
    \ that does not use any APIs, we want the approach to be able to retrieve suitable\
    \ API references to generate the correct completion. We embed the API references\
    \ into a vector space using Sentence-BERT [\\[51\\]](#page-11-17), a BERT-based\
    \ model designed for measuring the semantic similarity between sentences. We use\
    \ a variant of this\n\n<span id=\"page-3-2\"></span><sup>5</sup>https://codeql.github.com/\n\
    \nmodel that is pre-trained on code.[6](#page-4-1) The model maps sentences, or\
    \ in our case lines of code, into a dense vector representation of size 768.\n\
    \nAfter embedding the API references, the approach indexes the normalized vectors\
    \ (/| | for all ∈ API references) in a Ball Tree.[7](#page-4-2) This index allows\
    \ for fast retrieval of nearest neighbors. During the retrieval, we embed each\
    \ line in the input using the same pre-trained SentenceBERT model used for indexing\
    \ the API references, and then normalize the vectors. The normalization is done\
    \ to turn the Euclidean distance used by the Ball Tree into cosine similarity,\
    \ which is commonly used. Next, we find the closest API reference of each line\
    \ by querying the Ball Tree constructed in the pre-analysis step. The result is\
    \ a list of API references, sorted by their similarity to the line in the input.\
    \ To obtain a single ranked list of API references, we merge the lists across\
    \ all ∈ completion based on their similarity scores. This finally yields a single\
    \ list of API references, of which we use the top- as additional context to add\
    \ into the prompt.\n\n3.3.4 Application to Test Generation. For test generation,\
    \ we extract all API usages in a previously generated test using regular expression\
    \ matching and embed them. Next, we retrieve the most relevant APIs for every\
    \ API usage and return a single list. Here we retrieve API references based on\
    \ API usages, and not based on lines. The reason is that as there are no ground\
    \ truths for the test generation task, and the main goal is to generate passing\
    \ tests. Therefore, fixing a wrong API usage is more important than avoiding re-implementation\
    \ of existing code. The embedding used for indexing the API references and during\
    \ retrieval is a BERT-based model available on HuggingFace.[8](#page-4-3) The\
    \ reason for using a different model than the one used for code completion is\
    \ that not all models on HuggingFace are compatible with TypeScript, so we choose\
    \ a compatible model. Using mean pooling, we embed the code pieces into a vector\
    \ of size 768. We use the same model for both indexing and retrieval, where during\
    \ indexing each API signature is embedded and stored in a list, and during retrieval\
    \ the API usage is embedded. During retrieval, we perform a linear search for\
    \ the most relevant API references. Since the size of JavaScript projects are\
    \ much smaller than the Python projects, there is not much benefit in using the\
    \ Ball Tree data structure.\n\n#### 3.4 Prompt Construction\n\n3.4.1 General Idea.\
    \ Given the input and a list of at most API references that may enable the LLM\
    \ to accurately generate code, De-Hallucinator constructs an augmented prompt\
    \ for querying the model. The prompt is designed in a way that resembles \"normal\"\
    \ code, i.e., the kind of data that the LLM has been trained on. The API references\
    \ are augmented as a block of commented lines to the prompt. These lines start\
    \ with API Reference:, and the following lines contain the relevant API references\
    \ in decreasing order of similarity to the lines in the input of the retrieval\
    \ module.\n\n<span id=\"page-4-2\"></span><span id=\"page-4-1\"></span><sup>6</sup>https://huggingface.co/flax-sentence-embeddings/st-codesearch-distilroberta-base\
    \ <sup>7</sup>https://scikit-learn.org/stable/modules/neighbors.html#ball-tree\n\
    \n<span id=\"page-4-3\"></span><sup>8</sup><https://huggingface.co/jinaai/jina-embeddings-v2-base-code>\n\
    \n3.4.2 Example. For our running example, the \"Iterative prompt\" section in\
    \ Fig. [4](#page-3-0) shows the prompt for function search in our example, augmented\
    \ with the API reference. Given the iterative prompt, the same LLM that predicted\
    \ the code in Fig. [2,](#page-0-3) completes this function correctly in the last\
    \ section of Fig. [4.](#page-3-0)\n\n3.4.3 Application to Code Completion. In\
    \ the code completion task, we prepend the prompt with the API references, as\
    \ it minimally disrupts the structure of the existing code. See our running example\
    \ for illustration.\n\n3.4.4 Application to Test Generation. On the other hand,\
    \ for test generation, as TestPilot already includes additional information into\
    \ the prompt, we append the API references to the end of the additional context\
    \ section. Figure [6](#page-7-0) shows how the prompt is augmented with the API\
    \ references in JavaScript, and the model's success in generating a correct test\
    \ based on that prompt.\n\n#### <span id=\"page-4-0\"></span>3.5 Integration with\
    \ the LLM\n\nDe-Hallucinator is designed with minimal assumptions about the underlying\
    \ LLM, and the tool that solves the code generation task. The approach considers\
    \ the LLM to be a black box that we query with a string, which then returns one\
    \ or multiple strings with suggested code pieces. Hence, we do not fine-tune the\
    \ LLM, or train a model to preform retrievals, which makes De-Hallucinator applicable\
    \ to more scenarios. To that end, the instantiation of De-Hallucinator in both\
    \ code completion and test generation tasks are compatible with any LLM, and our\
    \ experiments with five LLMs show the flexibility of the approach.\n\n#### <span\
    \ id=\"page-4-4\"></span>4 IMPLEMENTATION\n\nThe general ideas behind De-Hallucinator\
    \ are language-agnostic, and the approach can be applied to different programming\
    \ languages and to different code prediction tasks. We present two implementations,\
    \ one for code completion in Python and one for test generation in JavaScript.\
    \ The code completion implementation is in Python and builds on top of the HuggingFace\
    \ transformers library. Adapting our implementation to other models requires only\
    \ to adjust the prompt size of the model and to select other parameters passed\
    \ to its API. The test generation implementation is in TypeScript and builds on\
    \ top of the state-of-the-art LLM-based test generator TestPilot [\\[54\\]](#page-11-1).\
    \ Like TestPilot, we use GPT-3.5-turbo as the LLM. TestPilot generates tests by\
    \ going through the functions in the package under test, and for each function\
    \ tries to generate a test with a simple input, which consists of a test header\
    \ and the signature of the function under test. Then, a set of \"prompt refiners\"\
    \ modify the prompt to include usage snippets, error messages, and the body of\
    \ the function under test. We implement two new prompt refiners, one for the RAG\
    \ prompts, and one for iterative prompts. These two refiners are only activated\
    \ when a test fails with an error that is likely caused by a hallucinated API,\
    \ which we detect by checking if the error message contains \"is not a function\"\
    \ or \"of undefined\".\n\n#### 5 EVALUATION\n\nTo evaluate the effectiveness and\
    \ efficiency of our approach, we perform experiments that answer the following\
    \ research questions:\n\n- Table 2: List of projects used for evaluation.\n- RQ1:\
    \ How much does De-Hallucinator improve the generated code compared to the baseline\
    \ approachs?\n- RQ2: How effective is De-Hallucinator at adding the correct API\
    \ references to the prompt?\n- RQ3: How do the hyperparameters of De-Hallucinator\
    \ affect the results?\n- RQ4: How efficient is De-Hallucinator, and how much do\
    \ the different steps of the approach contribute to the running time?\n\n####\
    \ 5.1 Experimental Setup\n\n5.1.1 Tasks. Our evaluation targets two tasks, code\
    \ completion and test generation. For code completion, we define the task as completing\
    \ an incomplete function at the beginning of a line with an API usage, given the\
    \ preceding code and the existing code in the project. This problem definition\
    \ matches the common scenario of a developer implementing a function in an existing\
    \ project, where the code to be written should use a project-specific API. For\
    \ example, suppose that the cursor in Fig. [1](#page-0-2) is at the beginning\
    \ of the code marked with gray background. Everything above the cursor is our\
    \ incomplete code , and the problem is to predict the marked code ′ , which refers,\
    \ e.g., to the project-specific relevance API. For test generation, the task is\
    \ to generate tests for a given JavaScript package. This problem setting has been\
    \ well established by previous work [\\[3,](#page-10-12) [54\\]](#page-11-1).\n\
    \n#### 5.1.2 LLMs and Baseline.\n\nCode Completion. We evaluate the code completion\
    \ task on four state-of-the-art LLMs: CodeGen [\\[43\\]](#page-11-10) with 2.7B\
    \ parameters (Salesforce/codegen-2B-mono), CodeGen 2.5 [\\[42\\]](#page-11-12)\
    \ with 7B parameters (Salesforce/codegen25-7b-mono), UniXCoder [\\[18\\]](#page-10-9)\
    \ with 125M parameters (microsoft/unixcoder-base), and StarCoder+ [\\[31\\]](#page-10-17)\
    \ with 15.5B parameters (bigcode/starcoderplus). The reason for selecting these\
    \ models is that they cover a variety of parameter sizes, model architectures,\
    \ and pre-training processes. We leave all parameters of the models at their defaults,\
    \ except for the maximum new tokens parameter, which we set to 256 to allow for\
    \ longer completions. As a baseline, we query the models with a prompt that contains\
    \ all the code preceding the cursor. In case this prompt exceeds the maximum prompt\
    \ size of 2,048 tokens, we truncate the prompt from the beginning.\n\nTest Generation.\
    \ For the test generation task, we build upon Test-Pilot and use GPT-3.5-turbo-0125\
    \ for the LLM as the OpenAI GPT models are already integrated into TestPilot and\
    \ require minimal effort to run. As a baseline, we use the original TestPilot\
    \ implementation with a one-hour time limit per package, 130k token limit per\
    \ package, and four completions per prompt with temperature 0.1. To make the comparison\
    \ fair, we set the token limit of De-Hallucinator to the amount of tokens used\
    \ by the baseline, and the number of completions to four with temperature 0.1.\
    \ This prevents De-Hallucinator being advantaged with generating more tokens.\
    \ Moreover, we cache the model outputs so that the same prompts return the same\
    \ completion for De-Hallucinator and the baseline.\n\n#### 5.1.3 Datasets.\n\n\
    Code Completion. With the goal of having a diverse set of projects in terms of\
    \ size, domain, and popularity, we gather a dataset of\n\n<span id=\"page-5-0\"\
    ></span>\n\n| Project (owner/name)                         | Commit  | LoC   \
    \  | Stars∗ |  |  |  |  |\n|----------------------------------------------|---------|---------|--------|--|--|--|--|\n\
    | Python projects used for code completion     |         |         |        |\
    \  |  |  |  |\n| graphql-python/graphene                      | 57cbef6 | 9,484\
    \   | 7.8k   |  |  |  |  |\n| geopy/geopy                                  | ef48a8c\
    \ | 10,000  | 4.3k   |  |  |  |  |\n| nvbn/thefuck                           \
    \      | ceeaeab | 12,181  | 83.3k  |  |  |  |  |\n| aaugustin/websockets∗∗  \
    \                     | ba1ed7a | 14,186  | 5k     |  |  |  |  |\n| arrow-py/arrow\
    \                               | 74a759b | 14,402  | 8.6k   |  |  |  |  |\n|\
    \ lektor/lektor                                | be3c8cb | 16,852  | 3.8k   |\
    \  |  |  |  |\n| Parsely/streamparse                          | aabd9d0 | 26,214\
    \  | 1.5k   |  |  |  |  |\n| Supervisor/supervisor                        | ca54549\
    \ | 29,860  | 8.3k   |  |  |  |  |\n| mwaskom/seaborn                        \
    \      | f9827a3 | 37,367  | 12.1k  |  |  |  |  |\n| psf/black               \
    \                     | ef6e079 | 106,005 | 37.6k  |  |  |  |  |\n| scikit-learn/scikit-learn\
    \                    | f3c6fd6 | 193,863 | 58.5k  |  |  |  |  |\n| JavaScript\
    \ projects used for test generation |         |         |        |  |  |  |  |\n\
    | node-red/node-red                            | 29ed5b2 | 60      | 18.8k  |\
    \  |  |  |  |\n| winstonjs/winston                            | c63a5ad | 496\
    \     | 22.2k  |  |  |  |  |\n| prettier/prettier                            |\
    \ 7142cf3 | 916     | 48.5k  |  |  |  |  |\n| tj/commander.js                \
    \              | 83c3f4e | 1,134   | 26.3k  |  |  |  |  |\n| js-sdsl/js-sdsl \
    \                             | 055866a | 1,198   | 0.7k   |  |  |  |  |\n| goldfire/howler.js\
    \                           | 003b917 | 1,319   | 23.1k  |  |  |  |  |\n| websockets/ws\
    \                                | b73b118 | 1,546   | 21.2k  |  |  |  |  |\n\
    | handlebars-lang/handlebars.js                | 8dc3d25 | 2,117   | 17.8k  |\
    \  |  |  |  |\n| petkaantonov/bluebird                        | df70847 | 3,105\
    \   | 20.4k  |  |  |  |  |\n| hapijs/joi                                   | 5b96852\
    \ | 4,149   | 20.7k  |  |  |  |  |\n| Unitech/pm2                            \
    \      | a092db2 | 5,048   | 40.9k  |  |  |  |  |\n| 11ty/eleventy           \
    \                     | e71cb94 | 5,772   | 16.4k  |  |  |  |  |\n\n<sup>∗</sup>\
    \ As of June 5, 2024\n\n∗∗ Has been moved to python-websockets/websockets\n\n\
    eleven public Python projects from GitHub, shown in Table [2](#page-5-0) for the\
    \ code completion task. We construct a dataset of API-related code completion\
    \ tasks by removing API usages from the benchmark projects and by considering\
    \ the removed code as the ground truth to be predicted by a model. For each such\
    \ API call, we remove the lines containing the call. If a call spans multiple\
    \ lines, we remove all of them. To prevent data leakage from imports of the API\
    \ in the ground truth, we also remove API-related imports. Next, we check if the\
    \ off-the-shelf LLMs can predict the exact code as in the original file using\
    \ the code preceding the cursor as the prompt. If an LLM predicts exactly the\
    \ original code, we ignore this API usage for the evaluation, as there is no need\
    \ to further improve the prediction and to avoid any potential memorizations.\
    \ We continue with this process for each of the four models, until we have ten\
    \ code completion tasks for each of the eleven projects. During this process,\
    \ we ignore 18, 51, 76, and 31 completions for UniXcoder, CodeGen, CodeGen v2.5,\
    \ and StarCoder+, respectively. Overall, the code completion evaluation dataset\
    \ consists of 11 projects × 10 × 4 models = 440 code completion tasks.\n\nTest\
    \ Generation. For the test generation task, we also gather a diverse set of 12\
    \ JavaScript projects from GitHub, shown in Table [2.](#page-5-0) These projects\
    \ cover a variety of domains, such as website generation, code formatting, and\
    \ process management. Since TestPilot cannot generate tests for ES modules, we\
    \ only consider JavaScript\n\nDe-Hallucinator: Mitigating LLM Hallucinations in\
    \ Code Generation Tasks via Iterative Grounding\n\nprojects that are CommonJS\
    \ packages. The rest of the setup is the same as in the TestPilot paper [\\[54\\\
    ]](#page-11-1).\n\n5.1.4 Metrics.\n\nCode Completion. We evaluate the code completions\
    \ in three ways:\n\n- Edit distance. To quantify the number of edits a developer\
    \ needs to apply after receiving a code completion, we measure the edit distance\
    \ between the predicted code and the ground truth. This metric provides a sense\
    \ for how many token edits are saved when using De-Hallucinator. We compute edit\
    \ distance using the Levenshtein distance at the subtoken level. For each pair\
    \ of completion and ground truth, we tokenize the code pieces with a GPT-2 fast\
    \ tokenizer,[9](#page-6-0) and then calculate the edit distance using NLTK's edit\\\
    _distance. [10](#page-6-1)\n- Normalized edit similarity. Similar to previous\
    \ work [\\[36\\]](#page-11-18) we also compute the normalized edit similarity.\
    \ To this end, we normalize the absolute edit distance (computed as above) to\
    \ the length of the longer of the two token sequences, and then turn the result\
    \ into a similarity metric.\n- Exact API match. Since the goal of De-Hallucinator\
    \ is to predict better API usages, we measure how many of all desired API usages\
    \ are predicted exactly as in the ground truth. To identify the API usages in\
    \ the lines of code to complete, we extract function calls, including the access\
    \ path to the function, and the parameters.\n\nFor all the above metrics, we report\
    \ the best completion obtained among completions of De-Hallucinator. Measuring\
    \ the @ matches a common usage scenario where a developer inspects a ranked list\
    \ of code completion suggestions, and picks the first that matches the developer's\
    \ expectations.\n\nTest Generation. For the test generation task, we use the following\
    \ three metrics:\n\n- Number of passing tests. We count the number of passing\
    \ tests as a proxy for code without any hallucinations.\n- Coverage. We measure\
    \ the statement coverage in the passing tests.\n- Number of fixed hallucinated\
    \ tests. We measure the number of tests that initially crash with \"∗ is not a\
    \ function\" or \"Reading property of undefined\" errors and that subsequently\
    \ De-Hallucinator turns into passing tests.\n\n5.1.5 Hardware. We perform the\
    \ experiments with the CodeGen v2.5 model and the GPT-3.5-turbo-0125 model on\
    \ a machine equipped with two Nvidia T4 GPUs, each having 16GB of memory. The\
    \ experiments with the UniXcoder, CodeGen, and the StarCoder+ models are performed\
    \ on a machine with a single Nvidia Tesla V100 with 32GB of memory. Each machine\
    \ has a 48-core Intel Xeon CPU clocked at 2.20GHz.\n\n#### <span id=\"page-6-4\"\
    ></span>5.2 RQ1: Effectiveness of De-Hallucinator\n\nCode Completion. In the first\
    \ set of experiments, we investigate to what extent De-Hallucinator improves code\
    \ completions compared to the baseline. By default, we run De-Hallucinator with\
    \ = 3\n\n<span id=\"page-6-1\"></span><span id=\"page-6-0\"></span>![](_page_6_Figure_15.jpeg)\n\
    \n<span id=\"page-6-2\"></span>Table 3: Effectiveness of De-Hallucinator in code\
    \ completion compared to the baseline on four off-the-shelf LLMs. The bold numbers\
    \ show statistically significant improvement over the initial prompt. The numbers\
    \ in parentheses show the relative improvement over the baseline.\n\n| Prompt<br>type\
    \              | UniXCoder<br>125M                    | CodeGen v1<br>2B     \
    \                | CodeGen<br>v2.5 7B                   | StarCoder+<br>15B  \
    \                  |  |\n|-----------------------------|--------------------------------------|--------------------------------------|--------------------------------------|--------------------------------------|--|\n\
    |                             | Edit distance (lower is better):     |       \
    \                               |                                      |     \
    \                                 |  |\n| Initial<br>RAG<br>Iterative | 52.4<br>46.8\
    \ (10.7%)<br>25.9 (50.6%) | 40.0<br>33.4 (16.5%)<br>30.7 (23.3%) | 47.2<br>31.6\
    \ (33.0%)<br>30.1 (36.3%) | 44.6<br>35.9 (19.4%)<br>33.5 (24.9%) |  |\n|     \
    \                        | Edit similarity (lower is better):   |            \
    \                          |                                      |          \
    \                            |  |\n| Initial<br>RAG<br>Iterative | 33.4<br>37.3\
    \ (11.7%)<br>42.6 (27.5%) | 43.6<br>48.0 (10.0%)<br>48.9 (12.1%) | 43.9<br>49.4\
    \ (12.5%)<br>50.2 (14.2%) | 33.2<br>38.0 (14.3%)<br>39.7 (19.3%) |  |\n|     \
    \                        | Exact API match (higher is better):  |            \
    \                          |                                      |          \
    \                            |  |\n| Initial<br>RAG<br>Iterative | 4.8<br>4.8\
    \ (0.0%)<br>5.9 (23.9%)     | 7.1<br>10.2 (42.6%)<br>11.1 (55.3%)  | 8.3<br>11.6\
    \ (39.1%)<br>13.4 (61.0%)  | 5.7<br>7.5 (32.0%)<br>7.5 (32.0%)    |  |\n|    \
    \                         |                                      |           \
    \                           |                                      |         \
    \                             |  |\n\n<span id=\"page-6-3\"></span>Table 4: Effectiveness\
    \ of De-Hallucinator in test generation compared to Test-Pilot. The bold numbers\
    \ show statistically significant improvement over the baseline. The numbers in\
    \ parentheses show the relative improvement over the baseline.\n\n| Prompt type\
    \         | Passing      | Coverage     | Fixed hallu  |  |\n|---------------------|--------------|--------------|--------------|--|\n\
    |                     | tests        |              | cinations    |  |\n| Initial\
    \ (TestPilot) | 64.8         | 32.1         | 19.3         |  |\n| RAG & iterative\
    \     | 66.3 (3.1%)  | 33.7 (3.6%)  | 43.2 (94.0%) |  |\n| Iterative         \
    \  | 76.4 (17.9%) | 37.0 (15.5%) | 31.4 (63.2%) |  |\n\niterations and add = 20\
    \ API references into the prompt. As shown in Table [3,](#page-6-2) De-Hallucinator\
    \ reduces the edit distance by 9.3 to 26.5 tokens, on average over the completions\
    \ by the initial prompt, which is a relative improvement between 23.3% and 50.6%\
    \ . This in turn translates to normalized edit similarity improvements of 12.1%\
    \ to 27.5% relative to the baseline. Moreover, the approach relatively improves\
    \ the exact API matches by 23.9% to 61.0%. For example, for the CodeGen v2.5 model,\
    \ De-Hallucinator is able to predict 1.5 times more APIs correctly than the baseline.\
    \ The approach shows statistically significant (using the Wilcoxon test and Pratt\
    \ method) improvements over the baseline consistently for all metrics and all\
    \ models. For example, Figure [5](#page-7-1) shows a scenario where De-Hallucinator\
    \ improves the completion. In this case the correct function is used by the model\
    \ in the first try, but the order of parameters is wrong. By providing the API\
    \ reference in the prompt, De-Hallucinator predicts the correct API usage, as\
    \ shown in Fig. [5.](#page-7-1)\n\nTest Generation. In the second set of experiments,\
    \ we evaluate the effectiveness of De-Hallucinator in test generation. We use\
    \ = 3 and = 3 as default parameters for these experiments. Table [4](#page-6-3)\
    \ shows statistically significant (using Wilcoxon test and Pratt method) improvements\
    \ for code coverage and fixed hallucinations by De-Hallucinator. Since RAG prompts\
    \ use the initial prompt for retrieval, and because the initial prompt just contains\
    \ a single signature, it is not as useful as iterative prompts. This is also reflected\
    \ in Table [4](#page-6-3) as lower coverage compared to only using iterative prompts.\n\
    \n<span id=\"page-7-1\"></span>**async def** schedule\\_formatting(sources: Set[Path],\
    \ fast: bool, write\\_back: WriteBack, mode: Mode, report: \"Report\", loop: asyncio.AbstractEventLoop,\
    \ executor: \"Executor\") -> **None**: \"\"\"Run formatting of `sources` in parallel\
    \ using the provided `executor`. (Use ProcessPoolExecutors for actual parallelism.)\
    \ `write\\_back`, `fast`, and `mode` options are passed to :func:`format\\_file\\\
    _in\\_place`. \"\"\" cache: Cache = {} **if** write\\_back **not in** (WriteBack.DIFF,\
    \ WriteBack.COLOR\\_DIFF): cache = read\\_cache(mode) sources, cached = filter\\\
    _cached(sources, cache) # <- baseline sources, cached = filter\\_cached(cache,\
    \ sources) # <- ground truth # API Reference: # filter\\_cached(cache: Cache,\
    \ sources: Iterable[Path]) -> Tuple[Set[Path], Set[Path]] # Split an iterable\
    \ of paths in `sources` into two sets. The first contains paths of files that\
    \ modifi ... **async def** schedule\\_formatting(sources: Set[Path], fast: bool,\
    \ write\\_back: WriteBack, mode: Mode, report: \"Report\", loop: asyncio.AbstractEventLoop,\
    \ executor: \"Executor\") -> **None**: \"\"\"Run formatting of `sources` in parallel\
    \ using the provided `executor`. (Use ProcessPoolExecutors for actual parallelism.)\
    \ `write\\_back`, `fast`, and `mode` options are passed to :func:`format\\_file\\\
    _in\\_place`. \"\"\" cache: Cache = {} **if** write\\_back **not in** (WriteBack.DIFF,\
    \ WriteBack.COLOR\\_DIFF): cache = read\\_cache(mode) sources, cached = filter\\\
    _cached(cache, sources) #<-De-Hallucinator\n\nFigure 5: Completion by CodeGen\
    \ highlighted in red , the ground truth, highlighted in green, and the completion\
    \ by De-Hallucinator after augmenting the prompt with relevant APIs highlighted\
    \ in blue .\n\nNote that TestPilot uses some prompt refiners, such as retrying\
    \ with error message, and including usage snippets and function bodies. These\
    \ refiners can mitigate some hallucinations, but not as much as De-Hallucinator,\
    \ as shown in Section [5.2.](#page-6-4) For example, Figure [6](#page-7-0) shows\
    \ a test generated by TestPilot, which uses non-existing APIs, but after providing\
    \ the API reference in the prompt De-Hallucinator generates a test using the correct\
    \ API.\n\n#### 5.3 RQ2: Correct Retrieval of API References\n\nCode Completion.\
    \ To better understand the effectiveness of De-Hallucinator, we investigate how\
    \ often the approach successfully augments the prompt with the correct API references.\
    \ Answering this question for all code completion tasks and all LLMs is difficult,\
    \ because comparing the API references to the API usages is non-trivial due to\
    \ different ways of importing APIs and passing arguments. Instead, for code completion,\
    \ we manually inspect a sample of 20 completion tasks per LLM and count the number\
    \ of times an API used in the ground truth is successfully added to the prompt\
    \ by De-Hallucinator.\n\nIn our inspected samples for code completion, there are\
    \ in total between 22 and 25 API usages (a completion can contain multiple API\
    \ usages). The new prompt generated by our approach contains the correct API between\
    \ two and six times, as shown in Table [5.](#page-7-2) Moreover, for CodeGen and\
    \ CodeGen v2.5 there are five completion tasks where the completion from the initial\
    \ prompt either misses the APIs or uses them incorrectly, but in the completions\
    \ from RAG or iterative prompts, the API reference section of the prompt contains\
    \ the correct API. The same happens for UniXcoder in four\n\n<span id=\"page-7-0\"\
    ></span>\n\n| let mocha = require('mocha');                   |  |  |  |  |  |\
    \  |\n|-------------------------------------------------|--|--|--|--|--|--|\n\
    | let assert = require('assert');                 |  |  |  |  |  |  |\n| let js_sdsl\
    \ = require('js-sdsl');               |  |  |  |  |  |  |\n| // class js-sdsl.Deque(t\
    \ = [], i = 1 << 12)     |  |  |  |  |  |  |\n| describe('test js_sdsl', function()\
    \ {           |  |  |  |  |  |  |\n| it('test js-sdsl.Deque', function(done) {\
    \       |  |  |  |  |  |  |\n| let deque = new js_sdsl.Deque([1, 2, 3, 4, 5]);\
    \ |  |  |  |  |  |  |\n| assert.equal(deque.size(), 5);                  |  |\
    \  |  |  |  |  |\n| assert.equal(deque.front(), 1);                 |  |  |  |\
    \  |  |  |\n| assert.equal(deque.back(), 5);                  |  |  |  |  |  |\
    \  |\n| deque.push_back(6);                             |  |  |  |  |  |  |\n\
    | assert.equal(deque.size(), 6);                  |  |  |  |  |  |  |\n| assert.equal(deque.back(),\
    \ 6);                  |  |  |  |  |  |  |\n| deque.push_front(0);           \
    \                 |  |  |  |  |  |  |\n| assert.equal(deque.size(), 7);      \
    \            |  |  |  |  |  |  |\n| assert.equal(deque.front(), 0);          \
    \       |  |  |  |  |  |  |\n| assert.equal(deque.pop_back(), 6);            \
    \  |  |  |  |  |  |  |\n| assert.equal(deque.size(), 6);                  |  |\
    \  |  |  |  |  |\n| assert.equal(deque.pop_front(), 0);             |  |  |  |\
    \  |  |  |\n| assert.equal(deque.size(), 5);                  |  |  |  |  |  |\
    \  |\n| done();                                         |  |  |  |  |  |  |\n\
    | })                                              |  |  |  |  |  |  |\n| })  \
    \                                            |  |  |  |  |  |  |\n|          \
    \                                       |  |  |  |  |  |  |\n| let mocha = require('mocha');\
    \                   |  |  |  |  |  |  |\n| let assert = require('assert');   \
    \              |  |  |  |  |  |  |\n| let js_sdsl = require('js-sdsl');      \
    \         |  |  |  |  |  |  |\n| // class js-sdsl.Deque(t = [], i = 1 << 12) \
    \    |  |  |  |  |  |  |\n| // API Reference:                               |\
    \  |  |  |  |  |  |\n\n| // js-sdsl.Deque.prototype.pushFront(t)   |  |\n|-------------------------------------------|--|\n\
    | // js-sdsl.Deque.prototype.front()        |  |\n| // js-sdsl.Queue.prototype.front()\
    \        |  |\n| describe('test js_sdsl', function() {     |  |\n| it('test js-sdsl.Deque',\
    \ function(done) { |  |\n| let deque = new js_sdsl.Deque([1, 2, 3]); |  |\n| assert.equal(deque.front(),\
    \ 1);           |  |\n| deque.pushFront(0);                       |  |\n| assert.equal(deque.front(),\
    \ 0);           |  |\n| done();                                   |  |\n| }) \
    \                                       |  |\n\n<span id=\"page-7-2\"></span>Figure\
    \ 6: Test generated by TestPilot using GPT-3.5-turbo (top), and the iterative\
    \ prompt by De-Hallucinator resulting in correct usage of APIs (bottom).\n\n})\n\
    \n|              |       | API usages |                         |         |  \
    \       |         |\n|--------------|-------|------------|-------------------------|---------|---------|---------|\n\
    | Model        | Tasks | Missing    | Expected API ref. added |         |    \
    \     |         |\n|              |       | /wrong     | RAG                 \
    \    | Iter. 1 | Iter. 2 | Iter. 3 |\n| UniXcoder    | 20    | 18         | 4\
    \                       | 5       | 5       | 5       |\n| CodeGen v1   | 20 \
    \   | 17         | 5                       | 6       | 6       | 6       |\n|\
    \ CodeGen v2.5 | 20    | 15         | 5                       | 5       | 6  \
    \     | 6       |\n| StarCoder+   | 20    | 17         | 2                   \
    \    | 3       | 3       | 3       |\n\ntasks and for StarCoder+ in two tasks.\
    \ For cases where the approach fails to add the correct API reference into the\
    \ prompt, the main reason is that the initial completion has low relevance w.r.t.\
    \ the ground truth.\n\nTest Generation. Since for the test generation task there\
    \ are no ground truths for the generated code pieces, manual inspection is infeasible.\
    \ Instead, we measure the number of passing tests and the number of non-crashing\
    \ failing tests from an iterative prompt, as a proxy of success for the retrieval.\
    \ Note that an iterative prompt is only created when an initial prompt causes\
    \ a crash with one of the specified errors in Section [4.](#page-4-4) We observe\
    \ that from the 622 instances where an iterative prompt is generated, 16.7% of\
    \ iterative prompts result in a passing test, and 26.8% of iterative prompts result\
    \ in a\n\n<span id=\"page-8-0\"></span>![](_page_8_Figure_1.jpeg)\n\nFigure 7:\
    \ Effects of and for code completion.\n\ntest without hallucinations. These results\
    \ are also in line with the manual inspections of the code completion task above.\n\
    \nOverall, the results show that De-Hallucinator is able to successfully augment\
    \ the prompt with the correct API references in many cases.\n\n#### 5.4 RQ3: Impact\
    \ of the Hyperparameters\n\nThis research question evaluates the impact of the\
    \ two main parameters of our approach. First, we consider the number of iterations\
    \ of iterative prompting. For both code completion and test generation, we run\
    \ the approach with ∈ {1, 2, 3}. As shown in Fig. [7,](#page-8-0) the first iteration\
    \ provides significant improvement over the baseline, but the gain is reduced\
    \ with further iterations. Higher values of are beneficial when the model cannot\
    \ immediately predict a relevant code, but upon presenting the first round of\
    \ API references, the model responds with more relevant outputs. At the same time,\
    \ even = 1 provides clear improvements over the baseline, which makes De-Hallucinator\
    \ useful even in scenarios where the cost of querying the model is high. We choose\
    \ = 3 for all other experiments in this paper.\n\nSecond, we study the impact\
    \ of the maximum number of API references that we add to the prompt. Depending\
    \ on the task, the optimal number of API references in the prompt varies. Setting\
    \ low values for can result in missing relevant context, whereas adding many API\
    \ references can confuse the model, while also costing context space. We perform\
    \ experiments with ∈ {2, 10, 20, 40} for code completion, and ∈ {3, 5, 10} for\
    \ test generation. Figure [7](#page-8-0) shows the results of code completion,\
    \ with exact API match peaking between = 10 and = 20. As a default in the rest\
    \ of the paper, we use = 20 for code completion. Figure [8](#page-8-1) shows the\
    \ effects of on coverage of the generated tests. The peak in this case is at =\
    \ 3, which we use as the parameter for other experiments in this paper.\n\n####\
    \ 5.5 RQ4: Efficiency\n\nThe following evaluates the efficiency of the approach\
    \ and how much each of De-Hallucinator's components contributes to its running\
    \ time. The pre-analysis step of code completion using CodeQL\n\n<span id=\"page-8-1\"\
    ></span>![](_page_8_Figure_10.jpeg)\n\nFigure 8: Effects of and for test generation.\n\
    \ntakes, on average, under one second per 1,000 lines of code in a project. For\
    \ the projects in our dataset, it takes at most 80 seconds, and most projects\
    \ need at most 26 seconds for the whole preprocessing phase. In a production-level\
    \ implementation, our CodeQL-based approach could be replaced by using static\
    \ information that is available in an IDE anyway, which is likely to further reduce\
    \ the computational effort. Moreover, updating the indexed API references, e.g.,\
    \ when the code base evolves, can be done at low frequency in the background.\
    \ For test generation, the pre-analysis takes between 0.6 seconds and 20 seconds\
    \ with an average of 3.5 seconds. Because the JavaScript projects in our dataset\
    \ are smaller than the Python projects, the pre-analysis is faster for test generation.\n\
    \nRetrieving relevant APIs and constructing the augmented prompt for one iteration\
    \ takes from 21 to 227 milliseconds for code completion, and from 0.1 to 17 milliseconds\
    \ for test generation. The time to query the LLMs ranges between 1.3 seconds (for\
    \ the remotely deployed GPT-3.5) and 66.7 seconds (for CodeGen v2.5 running on\
    \ our local Nvidia T4 GPU), on average per query. These numbers are roughly the\
    \ same for the baseline and for querying the model with De-Hallucinator-augmented\
    \ prompts. It is important to note that a production-level deployment would run\
    \ the LLM on a GPU cluster, which typically answers queries within tens to hundreds\
    \ of milliseconds, as evidenced by tools like Copilot and Tabnine.\n\n#### 6 LIMITATIONS\
    \ AND THREATS TO VALIDITY\n\nWe assume an API to be available when generating\
    \ the code that uses it. However, in some cases, a developer may first write an\
    \ API usage and then implement the API. In such cases, De-Hallucinator would be\
    \ unable to retrieve the API reference, and hence, could not provide any benefits.\
    \ To address this limitation, one could configure De-Hallucinator to abstain from\
    \ repeatedly querying the LLM if the similarity between the initial completion\
    \ and the retrieved API references is below a threshold. We implement and evaluate\
    \ De-Hallucinator for Python and JavaScript, and although our general approach\
    \ could be applied to any language, our conclusions are valid only for these languages.\
    \ The set of projects we use might\n\nnot be representative of all projects, which\
    \ we try to mitigate by selecting a diverse set of popular projects.\n\n#### 7\
    \ RELATED WORK\n\nData-Driven Code Completion. The idea to augment traditional\
    \ type-based code completion in a data-driven manner was introduced by Bruch et\
    \ al.[\\[8\\]](#page-10-18). More recently, statistical models are used, such\
    \ as a pre-trained BERT model [\\[10\\]](#page-10-19) applied to code completion\
    \ [\\[34\\]](#page-11-19), and models trained for specific kinds of completions,\
    \ e.g., API usages [\\[50\\]](#page-11-20) and test methods [\\[41\\]](#page-11-21).\
    \ Grammars can improve statistical code completions, either by restricting the\
    \ tokens to predict [\\[46\\]](#page-11-0) or by generating code that leaves some\
    \ syntax subtrees undefined [\\[19\\]](#page-10-20). Hellendoorn et al. [\\[21\\\
    ]](#page-10-21) study data-driven code completion based on recorded real-world\
    \ completion requests, with a focus on completing single identifiers. Our work\
    \ differs from all the above by providing project-specific API references based\
    \ on previous completions as an input to a code completion model.\n\nCode Completion\
    \ with LLMs and Local Context. Motivated by the observation that LLMs lack project-specific\
    \ information, Shrivastava et al. [\\[56\\]](#page-11-8) propose a repository-level\
    \ prompt generation technique to select the best context from a set of predefined\
    \ contexts to solve the task of line completion. Their method relies on training\
    \ a separate model that takes a context window around the incomplete line as input,\
    \ and outputs a ranking for additional contexts. The training routine uses the\
    \ LLM (in their case Codex) to calculate the loss function. Ding et al.[\\[12\\\
    ]](#page-10-8) describe a similar method, called CoCoMIC, to address the challenge\
    \ of project-specific APIs. They utilize a custom static analyzer, CCFinder, that\
    \ initially creates a context graph of program components in the project, and\
    \ allows retrieval of relevant contexts to complete a statement. They then fine-tuned\
    \ CodeGen-2B-mono by adding the cross-file contexts to the input. Both of the\
    \ above are tightly coupled with the underlying LLM: The first approach [\\[56\\\
    ]](#page-11-8) uses the LLM to calculate the loss function for training a new\
    \ model, and the second approach [\\[12\\]](#page-10-8) changes the model's weights\
    \ during fine-tuning. In contrast, De-Hallucinator queries the LLM as a black-box,\
    \ and hence, can be easily applied to other models.\n\nAn approach developed concurrently\
    \ with ours [\\[64\\]](#page-11-22) includes fragments of project-specific code\
    \ in the prompt to improve the LLM's predictions. Similar to our work, they also\
    \ query the model iteratively. Unlike De-Hallucinator, their approach retrieves\
    \ existing code fragments, and not API signatures. Since their approach relies\
    \ on existing code fragments, it can only improve predictions when a project-specific\
    \ API has already been used before and when this existing usage resembles the\
    \ desired prediction, whereas our approach applies to all usages of project-specific\
    \ APIs.\n\nCombining LLMs and Retrieval. Lu et al.[\\[36\\]](#page-11-18) use\
    \ conventional retrieval methods to find similar code pieces in a pre-defined\
    \ code database and add them to the prompt as dead code. Although this approach\
    \ improves the quality of code completions by the LLMs, it does not address the\
    \ challenge of project-specific APIs. Nashid et al.[\\[39\\]](#page-11-7) propose\
    \ a retrieval technique to find suitable examples for few-shot learning, but do\
    \ not apply the idea to code completion. HyDE [\\[15\\]](#page-10-22) prompts\
    \ an LLM to generate hypothetical textual documents for a given query, and then\
    \ retrieves real documents that\n\nhave an embedding similar to the hypothetical\
    \ documents. Their work shares the observation that LLM predictions may be factually\
    \ inaccurate, e.g., in our case by referring to non-existing APIs, while being\
    \ similar to a factually correct document. By addressing this problem via retrieval,\
    \ their approach is limited to producing already existing documents, whereas De-Hallucinator\
    \ generates new code using an augmented prompt.\n\nAutomated Test Generation.\
    \ Before the era of LLMs, random feedback-directed test generation became practical\
    \ through Randoop [\\[44\\]](#page-11-23). LambdaTester [\\[55\\]](#page-11-24)\
    \ integrates higher order functions into test generation, and Nessie [\\[3\\]](#page-10-12)\
    \ targets asynchronous callbacks. TestPilot [\\[54\\]](#page-11-1) uses LLMs and\
    \ prompt refinement to generate humanreadable regression tests. CodaMosa [\\[27\\\
    ]](#page-10-23) uses LLMs to increase the coverage of automatic test generators\
    \ when stuck in a plateau.\n\nImproving LLM-Suggested Code. To improve code suggested\
    \ by LLMs, existing techniques for automated program repair [\\[26\\]](#page-10-24)\
    \ can be applied in a post-processing step [\\[13\\]](#page-10-25). Alternatively,\
    \ the code predicted by a model can serve as input for initializing and guiding\
    \ component-based code synthesis [\\[49\\]](#page-11-25). The above work and ours\
    \ shares the observation that completions from LLMs often share code elements\
    \ with the desired code. Instead of improving code in a post-processing step,\
    \ De-Hallucinator nudges an LLM toward producing better completions by improving\
    \ the prompt.\n\nQuerying LLMs Multiple Times. Work on program repair queries\
    \ a model multiple times until finding a suitable repair [\\[37\\]](#page-11-26).\
    \ They repeatedly query the model with the same prompt and may trigger thousands\
    \ of queries, whereas De-Hallucinator continuously augments the prompt and queries\
    \ the model only a few times. Li et al.[\\[32\\]](#page-10-26) propose querying\
    \ a model with multiple mutations of the given code, and to then use the completion\
    \ that is closest to the \"average\" completion. De-Hallucinator instead uses\
    \ the initial prediction to construct an improved prompt. Xia et al.[\\[62\\]](#page-11-27)\
    \ introduce conversational program repair, which iteratively improves a prompt\
    \ by adding test failures observed when executing the predicted code. In contrast,\
    \ we do not require tests or executions, but only information that is statically\
    \ available in a typical IDE.\n\nOther Work on Models of Code. The impressive\
    \ abilities of neural models of code [\\[47\\]](#page-11-28) has lead to various\
    \ other applications beyond code completion. For example, neural models provide\
    \ type predictions [\\[20,](#page-10-27) [38,](#page-11-29) [48,](#page-11-30)\
    \ [60\\]](#page-11-31), make predictions about code changes [\\[6,](#page-10-28)\
    \ [22\\]](#page-10-29), and enable code search [\\[16,](#page-10-30) [53\\]](#page-11-32).\
    \ LLMs are shown to be useful, e.g., for code mutation, test oracle generation,\
    \ and test case generation [\\[4,](#page-10-1) [25,](#page-10-31) [54\\]](#page-11-1),\
    \ and for automated program repair [\\[24\\]](#page-10-32).\n\n#### 8 CONCLUSION\n\
    \nMotivated by the inability of current LLM-based code generation approaches to\
    \ correctly predict project-specific APIs, we present De-Hallucinator. The approach\
    \ exploits the observation that LLMs often predict code that is similar to the\
    \ desired code, but factually incorrect. We address the hallucination problem\
    \ by iteratively augmenting the prompt with increasingly relevant API references.\
    \ Our evaluation on two task, code completion and test generation, shows that\
    \ De-Hallucinator significantly improves the quality of generations over the state-of-the-art\
    \ baselines.\n\nDe-Hallucinator: Mitigating LLM Hallucinations in Code Generation\
    \ Tasks via Iterative Grounding\n\n## DATA-AVAILABILITY STATEMENT\n\nOur implementation,\
    \ datasets, and evaluation scripts are publicly available at<https://github.com/AryazE/dehallucinator>\
    \ and [https:](https://github.com/AryazE/testpilot) [//github.com/AryazE/testpilot.](https://github.com/AryazE/testpilot)\n\
    \n## REFERENCES\n\n- <span id=\"page-10-14\"></span>[1] [n. d.]. HuggingFace code\
    \ embedding models. [https://huggingface.co/models?](https://huggingface.co/models?pipeline_tag=feature-extraction&sort=trending&search=code)\
    \ [pipeline\\\\_tag=feature-extraction&sort=trending&search=code.](https://huggingface.co/models?pipeline_tag=feature-extraction&sort=trending&search=code)\
    \ Accessed: 2024- 06-05.\n- <span id=\"page-10-6\"></span>[2] Michael Ahn, Anthony\
    \ Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn,\
    \ Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, et al. 2022. Do as i can,\
    \ not as i say: Grounding language in robotic affordances. arXiv preprint arXiv:2204.01691\
    \ (2022).\n- <span id=\"page-10-12\"></span>[3] Ellen Arteca, Sebastian Harner,\
    \ Michael Pradel, and Frank Tip. 2022. Nessie: Automatically Testing JavaScript\
    \ APIs with Asynchronous Callbacks. In ICSE.\n- <span id=\"page-10-1\"></span>[4]\
    \ Patrick Bareiß, Beatriz Souza, Marcelo d'Amorim, and Michael Pradel. 2022. Code\
    \ Generation Tools (Almost) for Free? A Study of Few-Shot, Pre-Trained Language\
    \ Models on Code. CoRR abs/2206.01335 (2022). [https://doi.org/10.48550/arXiv.](https://doi.org/10.48550/arXiv.2206.01335)\
    \ [2206.01335](https://doi.org/10.48550/arXiv.2206.01335) arXiv[:2206.01335](https://arxiv.org/abs/2206.01335)\n\
    - <span id=\"page-10-16\"></span>[5] Piotr Bojanowski, Edouard Grave, Armand Joulin,\
    \ and Tomas Mikolov. 2017. Enriching Word Vectors with Subword Information. TACL\
    \ 5 (2017), 135–146. <https://transacl.org/ojs/index.php/tacl/article/view/999>\n\
    - <span id=\"page-10-28\"></span>[6] Shaked Brody, Uri Alon, and Eran Yahav. 2020.\
    \ A Structural Model for Contextual Code Changes. In OOPSLA.\n- <span id=\"page-10-0\"\
    ></span>[7] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,\
    \ Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,\
    \ Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon\
    \ Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher\
    \ Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack\
    \ Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and\
    \ Dario Amodei. 2020. Language Models are Few-Shot Learners. In Advances in Neural\
    \ Information Processing Systems 33: Annual Conference on Neural Information Processing\
    \ Systems 2020, NeurIPS. [https://proceedings.neurips.](https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html)\
    \ [cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html](https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html)\n\
    - <span id=\"page-10-18\"></span>[8] Marcel Bruch, Martin Monperrus, and Mira\
    \ Mezini. 2009. Learning from examples to improve code completion systems. In\
    \ European Software Engineering Conference and International Symposium on Foundations\
    \ of Software Engineering (ESEC/FSE). ACM, 213–222.\n- <span id=\"page-10-2\"\
    ></span>[9] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de\
    \ Oliveira Pinto, Jared Kaplan, Harrison Edwards, Yuri Burda, Nicholas Joseph,\
    \ Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy\
    \ Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder,\
    \ Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter,\
    \ Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios\
    \ Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol,\
    \ Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu\
    \ Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Joshua\
    \ Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage,\
    \ Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish,\
    \ Ilya Sutskever, and Wojciech Zaremba. 2021. Evaluating Large Language Models\
    \ Trained on Code. CoRR abs/2107.03374 (2021). arXiv[:2107.03374 https://arxiv.org/abs/2107.03374](https://arxiv.org/abs/2107.03374)\n\
    - <span id=\"page-10-19\"></span>[10] Jacob Devlin, Ming-Wei Chang, Kenton Lee,\
    \ and Kristina Toutanova. 2018. BERT: Pre-training of Deep Bidirectional Transformers\
    \ for Language Understanding. CoRR abs/1810.04805 (2018). arXiv[:1810.04805](https://arxiv.org/abs/1810.04805)<http://arxiv.org/abs/1810.04805>\n\
    - <span id=\"page-10-15\"></span>[11] Jacob Devlin, Jonathan Uesato, Rishabh Singh,\
    \ and Pushmeet Kohli. 2017. Semantic Code Repair using Neuro-Symbolic Transformation\
    \ Networks. CoRR abs/1710.11054 (2017). arXiv[:1710.11054](https://arxiv.org/abs/1710.11054)<http://arxiv.org/abs/1710.11054>\n\
    - <span id=\"page-10-8\"></span>[12] Yangruibo Ding, Zijian Wang, Wasi Uddin Ahmad,\
    \ Murali Krishna Ramanathan, Ramesh Nallapati, Parminder Bhatia, Dan Roth, and\
    \ Bing Xiang. 2022. CoCoMIC: Code Completion By Jointly Modeling In-file and Cross-file\
    \ Context. arXiv preprint arXiv:2212.10007 (2022).\n- <span id=\"page-10-25\"\
    ></span>[13] Zhiyu Fan, Xiang Gao, Abhik Roychoudhury, and Shin Hwei Tan. 2022.\
    \ Improving automatically generated code from Codex via Automated Program Repair.\
    \ Technical Report.\n- <span id=\"page-10-13\"></span>[14] Joseph L Fleiss. 1981.\
    \ Statistical methods for rates and proportions. John Wiley.\n- <span id=\"page-10-22\"\
    ></span>[15] Luyu Gao, Xueguang Ma, Jimmy Lin, and Jamie Callan. 2023. Precise\
    \ Zero-Shot Dense Retrieval without Relevance Labels. In Proceedings of the 61st\
    \ Annual Meeting of the Association for Computational Linguistics (Volume 1: Long\
    \ Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, Anna Rogers, Jordan L.\
    \ Boyd-Graber, and Naoaki Okazaki (Eds.). Association for Computational Linguistics,\
    \ 1762–1777. <https://aclanthology.org/2023.acl-long.99>\n- <span id=\"page-10-30\"\
    ></span>[16] Xiaodong Gu, Hongyu Zhang, and Sunghun Kim. 2018. Deep code search.\
    \ In Proceedings of the 40th International Conference on Software Engineering,\
    \ ICSE. 933–944.<https://doi.org/10.1145/3180155.3180167>\n- <span id=\"page-10-7\"\
    ></span>[17] Yu Gu, Xiang Deng, and Yu Su. 2022. Don't Generate, Discriminate:\
    \ A Proposal for Grounding Language Models to Real-World Environments. arXiv preprint\
    \ arXiv:2212.09736 (2022).\n- <span id=\"page-10-9\"></span>[18] Daya Guo, Shuai\
    \ Lu, Nan Duan, Yanlin Wang, Ming Zhou, and Jian Yin. 2022. Unixcoder: Unified\
    \ cross-modal pre-training for code representation. arXiv preprint arXiv:2203.03850\
    \ (2022).\n- <span id=\"page-10-20\"></span>[19] Daya Guo, Alexey Svyatkovskiy,\
    \ Jian Yin, Nan Duan, Marc Brockschmidt, and Miltiadis Allamanis. 2022. Learning\
    \ to Complete Code with Sketches. In ICLR. <https://arxiv.org/abs/2106.10158>\n\
    - <span id=\"page-10-27\"></span>[20] Vincent J. Hellendoorn, Christian Bird,\
    \ Earl T. Barr, and Miltiadis Allamanis. 2018. Deep learning type inference. In\
    \ Proceedings of the 2018 ACM Joint Meeting on European Software Engineering Conference\
    \ and Symposium on the Foundations of Software Engineering, ESEC/SIGSOFT. 152–162.\
    \ [https://doi.org/10.1145/3236024.](https://doi.org/10.1145/3236024.3236051)\
    \ [3236051](https://doi.org/10.1145/3236024.3236051)\n- <span id=\"page-10-21\"\
    ></span>[21] Vincent J. Hellendoorn, Sebastian Proksch, Harald C. Gall, and Alberto\
    \ Bacchelli. 2019. When Code Completion Fails: a Case Study on Real-World Completions.\
    \ In ICSE.\n- <span id=\"page-10-29\"></span>[22] Thong Hoang, Hong Jin Kang,\
    \ David Lo, and Julia Lawall. 2020. Cc2vec: Distributed representations of code\
    \ changes. In Proceedings of the ACM/IEEE 42nd International Conference on Software\
    \ Engineering. 518–529.\n- <span id=\"page-10-3\"></span>[23] Naman Jain, Skanda\
    \ Vaidyanath, Arun Shankar Iyer, Nagarajan Natarajan, Suresh Parthasarathy, Sriram\
    \ K. Rajamani, and Rahul Sharma. 2022. Jigsaw: Large Language Models meet Program\
    \ Synthesis. In 44th IEEE/ACM 44th International Conference on Software Engineering,\
    \ ICSE. 1219–1231. [https://doi.org/10.1145/](https://doi.org/10.1145/3510003.3510203)\
    \ [3510003.3510203](https://doi.org/10.1145/3510003.3510203)\n- <span id=\"page-10-32\"\
    ></span>[24] Nan Jiang, Kevin Liu, Thibaud Lutellier, and Lin Tan. 2023. Impact\
    \ of Code Language Models on Automated Program Repair. In 45th IEEE/ACM International\
    \ Conference on Software Engineering, ICSE. IEEE, 1430–1442. [https://doi.org/10.](https://doi.org/10.1109/ICSE48619.2023.00125)\
    \ [1109/ICSE48619.2023.00125](https://doi.org/10.1109/ICSE48619.2023.00125)\n\
    - <span id=\"page-10-31\"></span>[25] Sungmin Kang, Juyeon Yoon, and Shin Yoo.\
    \ 2023. Large Language Models are Few-shot Testers: Exploring LLM-based General\
    \ Bug Reproduction. In 45th IEEE/ACM International Conference on Software Engineering,\
    \ ICSE. 2312–2323. <https://doi.org/10.1109/ICSE48619.2023.00194>\n- <span id=\"\
    page-10-24\"></span>[26] Claire Le Goues, Michael Pradel, and Abhik Roychoudhury.\
    \ 2019. Automated program repair. Commun. ACM 62, 12 (2019), 56–65. [https://doi.org/10.1145/](https://doi.org/10.1145/3318162)\
    \ [3318162](https://doi.org/10.1145/3318162)\n- <span id=\"page-10-23\"></span>[27]\
    \ Caroline Lemieux, Jeevana Priya Inala, Shuvendu K Lahiri, and Siddhartha Sen.\
    \ 2023. CODAMOSA: Escaping Coverage Plateaus in Test Generation with Pretrained\
    \ Large Language Models. In 45th International Conference on Software Engineering,\
    \ ser. ICSE.\n- <span id=\"page-10-5\"></span>[28] Patrick Lewis, Ethan Perez,\
    \ Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich\
    \ Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. 2020. Retrieval-augmented\
    \ generation for knowledge-intensive nlp tasks. Advances in Neural Information\
    \ Processing Systems 33 (2020), 9459–9474.\n- <span id=\"page-10-11\"></span>[29]\
    \ Jian Li, Yue Wang, Michael R. Lyu, and Irwin King. 2018. Code Completion with\
    \ Neural Attention and Pointer Networks. In Proceedings of the Twenty-Seventh\
    \ International Joint Conference on Artificial Intelligence, IJCAI 2018, July\
    \ 13-19, 2018, Stockholm, Sweden, Jérôme Lang (Ed.). ijcai.org, 4159–4165. [https:](https://doi.org/10.24963/ijcai.2018/578)\
    \ [//doi.org/10.24963/ijcai.2018/578](https://doi.org/10.24963/ijcai.2018/578)\n\
    - <span id=\"page-10-10\"></span>[30] Raymond Li, Loubna Ben Allal, Yangtian Zi,\
    \ Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki,\
    \ Jia Li, Jenny Chim, et al. 2023. StarCoder: may the source be with you! arXiv\
    \ preprint arXiv:2305.06161 (2023).\n- <span id=\"page-10-17\"></span>[31] Raymond\
    \ Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao\
    \ Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii,\
    \ Terry Yue Zhuo, Thomas Wang, Olivier Dehaene, Mishig Davaadorj, Joel Lamy-Poirier,\
    \ João Monteiro, Oleh Shliazhko, Nicolas Gontier, Nicholas Meade, Armel Zebaze,\
    \ Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muhtasham Oblokulov,\
    \ Zhiruo Wang, Rudra Murthy V, Jason Stillerman, Siva Sankalp Patel, Dmitry Abulkhanov,\
    \ Marco Zocca, Manan Dey, Zhihan Zhang, Nour Moustafa-Fahmy, Urvashi Bhattacharyya,\
    \ Wenhao Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas, Maxim Kunakov, Fedor\
    \ Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire Schlesinger,\
    \ Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Jennifer Robinson,\
    \ Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy,\
    \ Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Muñoz Ferrandis, Sean\
    \ Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, and Harm de Vries. 2023.\
    \ StarCoder: may the source be with you! CoRR abs/2305.06161 (2023). <https://doi.org/10.48550/arXiv.2305.06161>\
    \ arXiv[:2305.06161](https://arxiv.org/abs/2305.06161)\n- <span id=\"page-10-26\"\
    ></span>[32] Zongjie Li, Chaozheng Wang, Zhibo Liu, Haoxuan Wang, Shuai Wang,\
    \ and Cuiyun Gao. 2022. CCTEST: Testing and Repairing Code Completion Systems.\
    \ arXiv preprint arXiv:2208.08289 (2022).\n- <span id=\"page-10-4\"></span>[33]\
    \ Jenny T. Liang, Chenyang Yang, and Brad A. Myers. 2024. A Large-Scale Survey\
    \ on the Usability of AI Programming Assistants: Successes and Challenges. In\
    \ Proceedings of the IEEE/ACM 46th International Conference on Software Engineering\
    \ (<conf-loc>, <city>Lisbon</city>, <country>Portugal</country>, </conf-loc>)\
    \ (ICSE '24). Association for Computing Machinery, New York, NY, USA, Article\
    \ 52, 13 pages.<https://doi.org/10.1145/3597503.3608128>\n- <span id=\"page-11-19\"\
    ></span>[34] Fang Liu, Ge Li, Yunfei Zhao, and Zhi Jin. 2020. Multi-Task Learning\
    \ based Pre-Trained Language Model for Code Completion. In ASE.\n- <span id=\"\
    page-11-6\"></span>[35] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki\
    \ Hayashi, and Graham Neubig. 2021. Pre-train, Prompt, and Predict: A Systematic\
    \ Survey of Prompting Methods in Natural Language Processing. CoRR abs/2107.13586\
    \ (2021). arXiv[:2107.13586 https://arxiv.org/abs/2107.13586](https://arxiv.org/abs/2107.13586)\n\
    - <span id=\"page-11-18\"></span>[36] Shuai Lu, Nan Duan, Hojae Han, Daya Guo,\
    \ Seung won Hwang, and Alexey Svyatkovskiy. 2022. ReACC: A Retrieval-Augmented\
    \ Code Completion Framework. arXiv[:2203.07722](https://arxiv.org/abs/2203.07722)\
    \ [cs.SE]\n- <span id=\"page-11-26\"></span>[37] Thibaud Lutellier, Hung Viet\
    \ Pham, Lawrence Pang, Yitong Li, Moshi Wei, and Lin Tan. 2020. CoCoNuT: combining\
    \ context-aware neural translation models using ensemble for program repair. In\
    \ ISSTA '20: 29th ACM SIGSOFT International Symposium on Software Testing and\
    \ Analysis, Virtual Event, USA, July 18-22, 2020, Sarfraz Khurshid and Corina\
    \ S. Pasareanu (Eds.). ACM, 101–114. [https:](https://doi.org/10.1145/3395363.3397369)\
    \ [//doi.org/10.1145/3395363.3397369](https://doi.org/10.1145/3395363.3397369)\n\
    - <span id=\"page-11-29\"></span>[38] Rabee Sohail Malik, Jibesh Patra, and Michael\
    \ Pradel. 2019. NL2Type: Inferring JavaScript function types from natural language\
    \ information. In Proceedings of the 41st International Conference on Software\
    \ Engineering, ICSE 2019, Montreal, QC, Canada, May 25-31, 2019. 304–315.<https://doi.org/10.1109/ICSE.2019.00045>\n\
    - <span id=\"page-11-7\"></span>[39] Noor Nashid, Mifta Sintaha, and Ali Mesbah.\
    \ 2023. Retrieval-Based Prompt Selection for Code-Related Few-Shot Learning. In\
    \ ICSE.\n- <span id=\"page-11-9\"></span>[40] Nhan Nguyen and Sarah Nadi. 2022.\
    \ An Empirical Evaluation of GitHub Copilot's Code Suggestions. In 2022 IEEE/ACM\
    \ 19th International Conference on Mining Software Repositories (MSR). 1–5.<https://doi.org/10.1145/3524842.3528470>\n\
    - <span id=\"page-11-21\"></span>[41] Pengyu Nie, Rahul Banerjee, Junyi Jessy\
    \ Li, Raymond J Mooney, and Milos Gligoric. 2023. Learning Deep Semantics for\
    \ Test Completion, In ICSE. arXiv preprint arXiv:2302.10166.\n- <span id=\"page-11-12\"\
    ></span>[42] Erik Nijkamp, Hiroaki Hayashi, Caiming Xiong, Silvio Savarese, and\
    \ Yingbo Zhou. 2023. Codegen2: Lessons for training llms on programming and natural\
    \ languages. arXiv preprint arXiv:2305.02309 (2023).\n- <span id=\"page-11-10\"\
    ></span>[43] Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo\
    \ Zhou, Silvio Savarese, and Caiming Xiong. 2022. CodeGen: An Open Large Language\
    \ Model for Code with Multi-Turn Program Synthesis. arXiv preprint (2022).\n-\
    \ <span id=\"page-11-23\"></span>[44] Carlos Pacheco, Shuvendu K. Lahiri, Michael\
    \ D. Ernst, and Thomas Ball. 2007. Feedback-Directed Random Test Generation. In\
    \ International Conference on Software Engineering (ICSE). IEEE, 75–84.\n- <span\
    \ id=\"page-11-16\"></span>[45] Jeffrey Pennington, Richard Socher, and Christopher\
    \ Manning. 2014. GloVe: Global Vectors for Word Representation. In Proceedings\
    \ of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP).\
    \ Association for Computational Linguistics, Doha, Qatar, 1532–1543. [https://doi.org/10.3115/v1/](https://doi.org/10.3115/v1/D14-1162)\
    \ [D14-1162](https://doi.org/10.3115/v1/D14-1162)\n- <span id=\"page-11-0\"></span>[46]\
    \ Gabriel Poesia, Alex Polozov, Vu Le, Ashish Tiwari, Gustavo Soares, Christopher\
    \ Meek, and Sumit Gulwani. 2022. Synchromesh: Reliable Code Generation from Pre-trained\
    \ Language Models. In The Tenth International Conference on Learning Representations,\
    \ ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net. <https://openreview.net/forum?id=KmtVD97J43e>\n\
    - <span id=\"page-11-28\"></span>[47] Michael Pradel and Satish Chandra. 2022.\
    \ Neural software analysis. Commun. ACM 65, 1 (2022), 86–96.<https://doi.org/10.1145/3460348>\n\
    - <span id=\"page-11-30\"></span>[48] Michael Pradel, Georgios Gousios, Jason\
    \ Liu, and Satish Chandra. 2020. Type-Writer: Neural Type Prediction with Search-based\
    \ Validation. In ESEC/FSE '20: 28th ACM Joint European Software Engineering Conference\
    \ and Symposium on the Foundations of Software Engineering, Virtual Event, USA,\
    \ November 8-13, 2020. 209–220.<https://doi.org/10.1145/3368089.3409715>\n- <span\
    \ id=\"page-11-25\"></span>[49] Kia Rahmani, Mohammad Raza, Sumit Gulwani, Vu\
    \ Le, Daniel Morris, Arjun Radhakrishna, Gustavo Soares, and Ashish Tiwari. 2021.\
    \ Multi-modal program inference: a marriage of pre-trained language models and\
    \ component-based synthesis. Proc. ACM Program. Lang. 5, OOPSLA (2021), 1–29.\
    \ [https://doi.org/](https://doi.org/10.1145/3485535) [10.1145/3485535](https://doi.org/10.1145/3485535)\n\
    - <span id=\"page-11-20\"></span>[50] Veselin Raychev, Martin T. Vechev, and Eran\
    \ Yahav. 2014. Code completion with statistical language models. In ACM SIGPLAN\
    \ Conference on Programming Language Design and Implementation, PLDI '14, Edinburgh,\
    \ United Kingdom - June 09 - 11, 2014. 44.\n- <span id=\"page-11-17\"></span>[51]\
    \ Nils Reimers and Iryna Gurevych. 2019. Sentence-BERT: Sentence Embeddings using\
    \ Siamese BERT-Networks.<https://doi.org/10.48550/ARXIV.1908.10084>\n- <span id=\"\
    page-11-11\"></span>[52] Deb Roy. 2005. Semiotic schemas: A framework for grounding\
    \ language in action and perception. Artificial Intelligence 167, 1-2 (2005),\
    \ 170–205.\n- <span id=\"page-11-32\"></span>[53] Saksham Sachdev, Hongyu Li,\
    \ Sifei Luan, Seohyun Kim, Koushik Sen, and Satish Chandra. 2018. Retrieval on\
    \ source code: a neural code search. In Proceedings of the 2nd ACM SIGPLAN International\
    \ Workshop on Machine Learning and Programming Languages. ACM, 31–41.\n- <span\
    \ id=\"page-11-1\"></span>[54] Max Schäfer, Sarah Nadi, Aryaz Eghbali, and Frank\
    \ Tip. 2024. An Empirical Evaluation of Using Large Language Models for Automated\
    \ Unit Test Generation. IEEE Transactions on Software Engineering 50, 1 (2024),\
    \ 85–105. [https://doi.org/](https://doi.org/10.1109/TSE.2023.3334955) [10.1109/TSE.2023.3334955](https://doi.org/10.1109/TSE.2023.3334955)\n\
    - <span id=\"page-11-24\"></span>[55] Marija Selakovic, Michael Pradel, Rezwana\
    \ Karim Nawrin, and Frank Tip. 2018. Test Generation for Higher-Order Functions\
    \ in Dynamic Languages. In OOPSLA.\n- <span id=\"page-11-8\"></span>[56] Disha\
    \ Shrivastava, Hugo Larochelle, and Daniel Tarlow. 2023. Repository-level prompt\
    \ generation for large language models of code. In International Conference\n\n\
    on Machine Learning. PMLR, 31693–31715.\n\n- <span id=\"page-11-13\"></span>[57]\
    \ Alexey Svyatkovskiy, Ying Zhao, Shengyu Fu, and Neel Sundaresan. 2019. Pythia:\
    \ Ai-assisted code completion system. In Proceedings of the 25th ACM SIGKDD international\
    \ conference on knowledge discovery & data mining. 2727–2735.\n- <span id=\"page-11-5\"\
    ></span>[58] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion\
    \ Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention\
    \ is All you Need. In NIPS. 6000–6010. [http://papers.nips.cc/paper/7181-attention-is](http://papers.nips.cc/paper/7181-attention-is-all-you-need)[all-you-need](http://papers.nips.cc/paper/7181-attention-is-all-you-need)\n\
    - <span id=\"page-11-15\"></span>[59] Yaza Wainakh, Moiz Rauf, and Michael Pradel.\
    \ 2021. IdBench: Evaluating Semantic Representations of Identifier Names in Source\
    \ Code. In 43rd IEEE/ACM International Conference on Software Engineering, ICSE\
    \ 2021, Madrid, Spain, 22-30 May 2021. IEEE, 562–573.<https://doi.org/10.1109/ICSE43902.2021.00059>\n\
    - <span id=\"page-11-31\"></span>[60] Jiayi Wei, Maruth Goyal, Greg Durrett, and\
    \ Isil Dillig. 2020. LambdaNet: Probabilistic Type Inference using Graph Neural\
    \ Networks. In 8th International Conference on Learning Representations, ICLR\
    \ 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.<https://openreview.net/forum?id=Hkx6hANtwH>\n\
    - <span id=\"page-11-2\"></span>[61] Chunqiu Steven Xia and Lingming Zhang. 2022.\
    \ Less training, more repairing please: revisiting automated program repair via\
    \ zero-shot learning. In Proceedings of the 30th ACM Joint European Software Engineering\
    \ Conference and Symposium on the Foundations of Software Engineering, ESEC/FSE\
    \ 2022, Singapore, Singapore, November 14-18, 2022, Abhik Roychoudhury, Cristian\
    \ Cadar, and Miryung Kim (Eds.). ACM, 959–971.<https://doi.org/10.1145/3540250.3549101>\n\
    - <span id=\"page-11-27\"></span>[62] Chunqiu Steven Xia and Lingming Zhang. 2023.\
    \ Conversational Automated Program Repair. CoRR abs/2301.13246 (2023). [https://doi.org/10.48550/arXiv.](https://doi.org/10.48550/arXiv.2301.13246)\
    \ [2301.13246](https://doi.org/10.48550/arXiv.2301.13246) arXiv[:2301.13246](https://arxiv.org/abs/2301.13246)\n\
    - <span id=\"page-11-3\"></span>[63] Frank F. Xu, Uri Alon, Graham Neubig, and\
    \ Vincent J. Hellendoorn. 2022. A Systematic Evaluation of Large Language Models\
    \ of Code. CoRR abs/2202.13169 (2022). arXiv[:2202.13169 https://arxiv.org/abs/2202.13169](https://arxiv.org/abs/2202.13169)\n\
    - <span id=\"page-11-22\"></span>[64] Fengji Zhang, Bei Chen, Yue Zhang, Jin Liu,\
    \ Daoguang Zan, Yi Mao, Jian-Guang Lou, and Weizhu Chen. 2023. RepoCoder: Repository-Level\
    \ Code Completion Through Iterative Retrieval and Generation. arXiv[:2303.12570](https://arxiv.org/abs/2303.12570)\
    \ [cs.CL]\n- <span id=\"page-11-14\"></span>[65] Tianyi Zhang, Tao Yu, Tatsunori\
    \ B. Hashimoto, Mike Lewis, Wen-tau Yih, Daniel Fried, and Sida I. Wang. 2023.\
    \ Coder Reviewer Reranking for Code Generation. In ICML.\n- <span id=\"page-11-4\"\
    ></span>[66] Albert Ziegler, Eirini Kalliamvakou, X. Alice Li, Andrew Rice, Devon\
    \ Rifkin, Shawn Simister, Ganesh Sittampalam, and Edward Aftandilian. 2024. Measuring\
    \ GitHub Copilot's Impact on Productivity. Commun. ACM 67, 3 (feb 2024), 54–63.\
    \ <https://doi.org/10.1145/3633453>"
  decisions:
    language: '- Qualified. Reason: English Paper'
    evaluation_prompt: Qualified.
    related_work_prompt: Qualified.
    novelty_prompt: Qualified
    review_only_prompt: Qualified.
  llm_input_used: '## Abstract

    Large language models (LLMs) trained on datasets of publicly available source

    code have established a new state of the art in code generation tasks. However,

    these models are mostly unaware of the code that exists within a specific

    project, preventing the models from making good use of existing APIs. Instead,

    LLMs often invent, or "hallucinate", non-existent APIs or produce variants of

    already existing code. This paper presents De-Hallucinator, a technique that

    grounds the predictions of an LLM through a novel combination of retrieving

    suitable API references and iteratively querying the model with increasingly

    suitable context information in the prompt. The approach exploits the

    observation that predictions by LLMs often resemble the desired code, but they

    fail to correctly refer to already existing APIs. De-Hallucinator automatically

    identifies project-specific API references related to the model''s initial

    predictions and adds these references into the prompt. Unlike

    retrieval-augmented generation (RAG), our approach uses the initial

    prediction(s) by the model to iteratively retrieve increasingly suitable API

    references. Our evaluation applies the approach to two tasks: predicting API

    usages in Python and generating tests in JavaScript. We show that

    De-Hallucinator consistently improves the generated code across five LLMs. In

    particular, the approach improves the edit distance by 23.3-50.6% and the

    recall of correctly predicted API usages by 23.9-61.0% for code completion, and

    improves the number of fixed tests that initially failed because of

    hallucinations by 63.2%, resulting in a 15.5% increase in statement coverage

    for test generation.


    ## Introduction

    Large language models (LLMs) have proven effective in many natural language [\[7\]](#page-10-0)
    and programming tasks [\[4,](#page-10-1) [9,](#page-10-2) [23,](#page-10-3) [46,](#page-11-0)
    [54,](#page-11-1) [61,](#page-11-2) [63\]](#page-11-3). Rapid adoption of LLM-based
    tools, such as Copilot[1](#page-0-0) and Tabnine[2](#page-0-1) , shows practical
    productivity benefits [\[33,](#page-10-4) [66\]](#page-11-4). State-of-the-art
    LLMs build on transformers [\[58\]](#page-11-5), which use self-attention to generate
    sequences of tokens in an auto-regressive process. That is, the model decides
    what token to predict next based on the tokens in the prompt and any already generated
    tokens. Hence, designing effective prompts, sometimes called prompt engineering,
    is a crucial part of developing a practical LLM-based technique [\[35,](#page-11-6)
    [39,](#page-11-7) [56\]](#page-11-8).


    Despite the impressive success of LLM-based code generation, these techniques
    are still at an early stage. In particular, we identify two key challenges faced
    by current approaches:


    Challenge 1: Project-specific APIs. As LLMs are trained on huge code bases, they
    effectively capture typical language idioms and


    Michael Pradel Software Lab University of Stuttgart Stuttgart, Germany michael@binaervarianz.de


    <span id="page-0-2"></span>


    | DataStore.py<br>class DataStore():<br>def __init__(self, file: str):                                                |

    |---------------------------------------------------------------------------------------------------------------------|

    |                                                                                                                     |

    |                                                                                                                     |

    | with open(file, ''r'') as f:                                                                                          |

    | self.documents = f.read().split(''-----'')                                                                            |

    | <br>def find_by_keyword(self, keyword: str) -> List[str]:<br>return [d for d
    in self.documents if keyword in d]<br> |

    | utils.py<br>                                                                                                        |

    | def relevance(document: str, keyword: str) -> float:<br>return document.count(keyword)
    / len(document)<br>          |

    | UI.py                                                                                                               |

    | <br>def search(ds: DataStore, keyword: str, top_k: int) -> List[str]:<br>docs
    = ds.find_by_keyword(keyword)         |

    | return sorted(docs, key=lambda d: relevance(d, keyword),                                                            |

    | reverse=True)[:top_k]                                                                                               |

    |                                                                                                                     |

    |                                                                                                                     |


    Figure 1: The desired completion of **search** is highlighted in gray .


    <span id="page-0-3"></span>


    |  | def search(ds: DataStore, keyword: str, top_k: int) -> List[str]: |  |  |  |  |

    |--|-------------------------------------------------------------------|--|--|--|--|

    |  | docs = ds.find_by_keyword(keyword)                                |  |  |  |  |

    |  | return sorted(docs, key=lambda x: x.score , reverse=True)[:top_k] |  |  |  |  |


    Figure 2: The completion of **search** by CodeGen-2B-mono highlighted in gray
    , and the wrong API usage highlighted in red .


    commonly used libraries. In contrast, a general-purpose model lacks knowledge
    of project-specific APIs, and may fail to correctly use existing functions and
    classes. In particular, this lack of knowledge may cause the model to "hallucinate"
    APIs that actually do not exist in the current code base [\[40\]](#page-11-9),
    or perhaps even worse, it may reimplement some functionality that is already present
    in the code base. Developers perceive this lack of knowledge about projectspecific
    APIs as an obstacle to using AI programming assistants [\[33\]](#page-10-4).


    As a running example, consider three files in a large project dealing with text
    documents, shown in Fig. [1.](#page-0-2) One file, DataStore.py, contains a class
    implementing a data structure that stores documents and provides a keyword-based
    search over the documents. Another file, utils.py, provides helper functions,
    one of which allows for measuring the relevance of a document to a keyword. In
    a third file, UI.py, the developer is working on a function, search, to search
    for the top\_k documents that are most relevant to a keyword.


    Requesting an LLM, e.g., CodeGen [\[43\]](#page-11-10), to complete the search
    function given a prompt that contains all existing code in UI.py results in Fig.
    [2.](#page-0-3) The code is partially correct, but refers to a nonexisting API
    (an attribute x.score). The underlying problem is that the models are not aware
    of the project-specific APIs that should be used to complete the code, and hence,
    the LLM simply hallucinates some plausible but ultimately wrong APIs.


    <span id="page-0-0"></span><sup>1</sup>https://github.com/features/copilot


    <span id="page-0-1"></span><sup>2</sup>https://www.tabnine.com/


    Challenge 2: Prioritizing context. A naive solution to address Challenge 1 would
    be to simply add all of the code in the project into the prompt. However, LLMs
    have a fixed maximum sequence length, which restricts how many tokens one can
    provide to the model. Even with the recent increases in sequence length of LLMs,
    providing the most useful context can improve the output and reduce the costs.
    Choosing the most helpful context for a given completion task is crucial, but
    an inherently difficult problem, because the optimal context depends on the desired
    code, which is not known a-priori. While traditional code completion approaches
    typically have access to various kinds of information available in IDEs, such
    as the names and types of program elements, providing all this information, or
    even all the code of the project, to an LLM is impossible due to the limited prompt
    size.


    This paper presents De-Hallucinator, which addresses the above challenges through
    a novel combination of retrieval-augmented generation (RAG) and an iterative form
    of LLM-based code generation. Our approach uses three types of prompts, which
    provide increasingly suitable context information. The initial prompt type is
    querying the LLM with the conventional prompt, i.e., without any retrieval. Retrieval-augmented
    generation (RAG) [\[28\]](#page-10-5) proposes to retrieve relevant context based
    on the initial prompt to address both Challenges 1 and 2, which we refer to as
    the RAG prompt type. The idea of augmenting an LLM with well-grounded facts relates
    to work on grounding of language models for natural languages [\[2,](#page-10-6)
    [17,](#page-10-7) [52\]](#page-11-11). However, also this prompt may fail to generate
    factually correct code (i.e., without hallucinations), because the initial prompt
    might not have any similar code to the desired API, or there are other APIs more
    similar to the initial prompt than the correct one. We make the improtant observation
    that the generated code from the previous prompt types often resembles the desired
    API. Hence, we construct a new type of prompt called the iterative prompt. De-Hallucinator
    leverages the code that the model predicts to retrieve suitable project-specific
    APIs, which are then added to the iterative prompt for the next round of model
    prediction. The iterative prompt type complements prior work that tries to guess
    the most suitable context from the incomplete code alone [\[12,](#page-10-8) [56\]](#page-11-8).


    The presented approach offers several benefits. First, De-Hallucinator works with
    any off-the-shelf LLM trained on code because the approach treats the model as
    a black box. In particular, we do not require to train or fine-tune the model
    in any way, but simply exploit the fact that its predictions contain implicit
    hints about additional context the model would benefit from. Second, because APIs
    usually evolve only slowly, De-Hallucinator can precompute, and occasionally update
    in the background, the set of project-specific API references. As a result, the
    latency of code generation is not impacted by any expensive program analysis,
    which is important for practical adoption. Finally, the approach is fully transparent
    to developers, because the approach hides the iterative interaction with the LLM
    from the user and simply returns a ranked list of predictions.


    We evaluate De-Hallucinator by applying the approach to code completion with four
    state-of-the-art LLMs for code, namely Code-Gen [\[43\]](#page-11-10), CodeGen
    2.5 [\[42\]](#page-11-12), UniXcoder [\[18\]](#page-10-9), and StarCoder+ [\[30\]](#page-10-10),
    and to test generation with GPT-3.5-turbo. Conceptually, the approach can be applied
    to any programming language, and our evaluation focuses on two popular languages,
    Python and JavaScript


    as they are among the most popular languages [3](#page-1-0) and common targets
    of prior work on code completion [\[9,](#page-10-2) [18,](#page-10-9) [29,](#page-10-11)
    [30,](#page-10-10) [42,](#page-11-12) [43,](#page-11-10) [57,](#page-11-13) [65\]](#page-11-14)
    and test generation [\[3,](#page-10-12) [54\]](#page-11-1). Compared to conventional
    prompts, we find that De-Hallucinator enables the models to provide more accurate
    predictions. In particular, we show a relative improvement of 23.3–50.6% in edit
    distance, and of 23.9–61.0% in recall of correctly predicted API usages for code
    completion. Moreover, we show relative improvement of 17.9% in number of passing
    tests, of 15.5% in coverage, and of 63.2% in the number of mitigated hallucinations.
    In summary, this paper contributes the following:


    • Empirical motivation showing that API hallucinations affect a large portion
    of failed code completion and test generation tasks.


    - A technique for addressing this problem using off-the-shelf, unmodified LLMs.

    - A novel algorithm that combines retrieval-augmented generation with an iterative
    method for constructing increasingly suitable prompts by using the hallucinations
    produced in earlier iterations to augment the context information provided in
    the prompts of future iterations.

    - Empirical evidence that, across two code generation tasks, two programming languages,
    and five state-of-the-art LLMs, De-Hallucinator offers more accurate generations
    than conventional prompts.'
  token_usage: 11744
  time_usage: 2.6385042667388916
- title: Profiling the carbon footprint of performance bugs
  abstract: 'Much debate nowadays is devoted to the impacts of modern information
    and

    communication technology on global carbon emissions. Green information and

    communication technology is a paradigm creating a sustainable and

    environmentally friendly computing field that tries to minimize the adverse

    effects on the environment. Green information and communication technology are

    under constant development nowadays. Thus, in this paper, we undertake the

    problem of performance bugs that, until recently, have never been studied so

    profoundly. We assume that inappropriate software implementations can have a

    crucial influence on global carbon emissions. Here, we classify those

    performance bugs and develop inappropriate implementations of four programs

    written in C++. To mitigate these simulated performance bugs, measuring

    software and hardware methods that can estimate the increased carbon footprint

    properly were proposed.'
  url: http://arxiv.org/abs/2401.01782v1
  keywords: carbon footprint, green computing, performance bugs, software engineering
  document: '# Profiling the carbon footprint of performance bugs


    1 st Iztok Fister Jr. *University of Maribor* Maribor, Slovenia iztok.fister1@um.si


    2 nd Dusan Fister ˇ *University of Maribor* Maribor, Slovenia dusan.fister@um.si


    3 th Vili Podgorelec *University of Maribor* Maribor, Slovenia vili.podgorelec@um.si


    4 th Iztok Fister *University of Maribor* Maribor, Slovenia iztok.fister@um.si


    *Abstract*—Much debate nowadays is devoted to the impacts of modern information
    and communication technology on global carbon emissions. Green information and
    communication technology is a paradigm creating a sustainable and environmentally
    friendly computing field that tries to minimize the adverse effects on the environment.
    Green information and communication technology are under constant development
    nowadays. Thus, in this paper, we undertake the problem of performance bugs that,
    until recently, have never been studied so profoundly. We assume that inappropriate
    software implementations can have a crucial influence on global carbon emissions.
    Here, we classify those performance bugs and develop inappropriate implementations
    of four programs written in C++. To mitigate these simulated performance bugs,
    measuring software and hardware methods that can estimate the increased carbon
    footprint properly were proposed.


    *Index Terms*—carbon footprint, green computing, performance bugs, software engineering


    ## I. INTRODUCTION


    We find ourselves in an era marked by turbulence, where the ominous specters of
    global warming and excessive carbon emissions loom large in our collective consciousness
    [\[8\]](#page-6-0). Many scientists have long sounded the alarm about the urgent
    need to curb these emissions to avert catastrophic consequences. However, despite
    the multitude of dire projections, political movements worldwide often fall short
    in their efforts to mitigate the impact of global warming effectively [\[9\]](#page-6-1).


    Modern humans and novel economic systems play a pivotal role in exacerbating the
    carbon footprint. On one hand, we engage in activities such as deforestation,
    construction on valuable land, the proliferation of supermarkets, while, on the
    other, in the extensive use of automobiles and airplanes [\[1\]](#page-6-2), [\[10\]](#page-6-3).
    Moreover, it has become increasingly evident that Information and Communication
    Technology (ICT), which we rely upon heavily, contributes significantly in increasing
    the carbon emissions. Considering the vast number of data centers scattered across
    the globe [\[7\]](#page-6-4), as well as the multitude of machine learning models
    running incessantly [\[5\]](#page-6-5), it becomes clear just how profound our
    environmental impact can be. In addition to all these examples, the personal computers
    are using electricity, and potentially contribute in increasing the carbon emissions
    [\[11\]](#page-6-6).


    The task of Green ICT is to mitigate the carbon emissions of ICT production, applications,
    and services [\[12\]](#page-6-7). It holds that the ICT systems nowadays produce
    even 2 % of global emissions [\[6\]](#page-6-8). The reduction of carbon emissions
    comes out either directly from the hardware, or directly and indirectly from the
    software. Indeed, the carbon emission is measured as a Carbon Footprint (CF) that
    is proportional to the amount of trees needed to absorb the emitted carbon dioxide
    in a year [\[12\]](#page-6-7).


    Several publications have tackled the challenge of enhancing the sustainability
    of ICT and its associated processes. In a paper authored by Taina [\[12\]](#page-6-7),
    a comprehensive approach was presented for analyzing the carbon footprint associated
    with software. This study examined each phase of a typical software lifecycle
    meticulously, quantifying the carbon emissions generated at each step. Additionally,
    the author shared insightful strategies aimed at minimizing this carbon footprint.
    Conversely, in paper [\[6\]](#page-6-8), the authors introduced a systematic methodology
    for quantifying the carbon footprint of a software product throughout its entire
    lifecycle. Furthermore, they proposed a method for incorporating certain facets
    of carbon footprint assessment seamlessly into the software development process.
    The paper also delves into the implications and tools associated with this innovative
    calculation approach. Thus, this work underscores the significance of energy metrics,
    and the consideration of carbon footprint implications within the realm of Green
    Software Engineering.


    Performance bugs are unnecessarily inefficient code chunks in software that can
    cause prolonged execution times and degraded resource utilization [\[3\]](#page-6-9).
    For instance, an execution time of a program calling a function each time it needs
    in place of referencing the variable storing the result of the calling function
    can increase its execution time substantially. The execution time is increased
    proportionally to the number of function calls.


    The impact of the unnecessary inefficient code chunks has rarely been taken into
    consideration, especially in the sense of Green ICT. The purpose of the study
    is to observe the well known performance bug in C++ referring to a vector class,
    and to show how its inefficient usage can increase the energy consumption (indirectly
    also carbon emission) of the corresponding algorithm. Vector in C++ reallocates
    memory when the new elements are added, and no memory is avail-


    Corresponding author: Iztok Fister Jr. (e-mail: iztok.fister1@um.si).


    able to hold them. The reallocation would cause significant performance degradation
    if this occurs too often [\[3\]](#page-6-9).


    To simulate performance bug, four different versions of an algorithm were developed
    that manipulate the big number of elements, either in the vector class or the
    double linked list. All the algorithms were executed on three different platforms
    (i.e., a laptop, a Raspberry Pi 3 microcomputer, and an iPad table computer),
    and compared with each other according to the increased energy consumption measured,
    depending on the platform either using the Linux system software tool or a power
    meter capable of measuring the energy consumption on the hardware level.


    The motivation of the study was three-fold:


    - Identifying the performance bugs in software or inappropriate implementation
    that can have a great influence toward the carbon emission.

    - Investigating the influence of the iteration performance bugs on the increased
    carbon emission by results of four simulation programs developed in C++ programming
    language.

    - Searching for methods of how to measure the influence of performance bugs.


    Although it can be found three performance bugs in the literature [\[3\]](#page-6-9)
    (i.e., iteration, enumeration, and deadlock performance bugs), here, we are focused
    on the first kind only. As a result, the main contributions of the study are as
    follows:


    - Simulating the performance bugs by four different algorithms written in C++.

    - Measuring an increasing carbon footprint caused by the simulation.

    - Showing that the proposed measuring methods can be applied to estimate the increased
    carbon footprint due to the simulated performance bugs.


    In general, the main novelty of the proposed method is to link the identification
    of performance bugs with measuring the increased carbon footprint they cause.
    Thus, the domain of software engineering is integrated with the domain of green
    computing.


    The structure of the paper is as follows: Section [II](#page-1-0) discusses potential
    ways in which to measure the carbon footprint on various digital computers. In
    Section [III,](#page-2-0) the concept of performance bugs is explained briefly.
    The performed experiments and the obtained results are the subjects of Section
    [IV.](#page-3-0) Section [V](#page-5-0) concludes the paper and outlines the plausible
    directions for the future work.


    ## <span id="page-1-0"></span>II. MEASURING THE CARBON FOOTPRINT OF COMPUTERS


    A carbon footprint is the amount of greenhouse gases (e.g., carbon dioxide and
    methane) that are generated by our actions [\[4\]](#page-6-10). The carbon footprint
    is measured in units of CO<sup>2</sup> per unit (CO2e). Actually, most things
    in the world contribute to carbon emissions, i.e., has its carbon footprint [\[2\]](#page-6-11).
    Electricity belongs today to the primary source of energy. Moreover, energy consumption
    is increasing crucially. As the production of electricity increases, the carbon
    emissions rise simultaneously. However, electricity can be produced based on different
    sources (e.g., coal, oil, gas, nuclear and renewable) that contribute a different
    level of carbon emissions. For instance, the low-carbon sources, like nuclear
    and renewable, are more environmentally friendly than the coal that belongs to
    the high-carbon sources.


    Typically, each country produces electricity from sources of different levels
    of carbon emissions. Therefore, the carbon footprint of electricity varies from
    country to country. The carbon footprint for electricity in the United States
    is estimated to be 0.65 kg CO2e.


    The energy consumption needs to be estimated in order to determine the carbon
    footprint of computers. Although the electricity is not the only source of carbon
    emissions by computers [\[2\]](#page-6-11), it is a good approximation for calculation
    of the carbon footprint. Indeed, there are two ways to determine the energy consumption
    of computers, i.e., software and hardware tools. However, the known software tools
    only measure power consumption on laptops when running on a battery. On the other
    hand, for desktop or server machines the only current solution is an electronic
    power-meter that plugs into the mains socket. In our study, the *powertop* utility
    was examined among the software tools, and the AVHzY ct-3 power-meter among the
    hardware tools. The characteristics of both tools are discussed in the remainder
    of the paper.


    ## *A. Powertop utility on Linux*


    The purpose of the *powertop* utility is to analyze and manage power consumption
    on laptops using battery power. The tool is able to display and export reports
    about the estimated discharge rate, and statics about processors, devices, kernel,
    timer, and interrupt handler behavior. It also lets us tune some kernel parameters
    easily on the fly, in order to maximize the battery life.


    Before operating the tools needs to be calibrated, where, during the process,
    the power engine adjusts to the specific computer environment in order to take
    the accurate power measures. In operation mode, the laptop must be on battery
    power only. The *powertop* takes measurements at 20 seconds intervals by default.
    Obviously, the interval can be set arbitrarily by the user.


    Indeed, the process of discharging the battery is strongly non-linear. Consequently,
    measurements taken at full battery capacity under a similar strain can differ
    from those taken when the capacity of the battery is not full. As a result, we
    need to ensure that the battery is at full capacity at the beginning of each experiment.


    ## *B. Power meter AVHzY CT-3*


    A power-meter measures electrical power in Watts. For measuring computer power,
    electronic Watt-meters are used that are capable of small power measurements,
    or of power measurements at frequencies beyond the range of dynamometertype instruments.


    The AVHzY CT-3 power-meter (Fig. [1\)](#page-2-1) measures active electrical power
    while connected directly to the electricity network, and works in 0 − 26 V of
    voltage and 0 − 6 A of current ranges. The device is connected to the electricity


    ![](_page_2_Picture_1.jpeg)


    Fig. 1. Power meter AVHzY ct-3.


    <span id="page-2-1"></span>network via a 230 V plug adapter with a USB port. It
    measures the consumer that is connected to it via an output USB port. Furthermore,
    the power-meter also enables connecting the external power via a USB-C input port.


    The main advantage of the power-meter is represented by the USB-C output port,
    to which the personal computer can be connected, on which the powerful PC software
    can be installed for monitoring power consumption simultaneously. The software
    is dedicated for data logging up to 1000sps, viewing the VBUS ripple, and diagnosing
    the devices. Using this PC software, the power consumption can be monitored online
    at a high level of accuracy.


    ## III. PERFORMANCE BUGS


    <span id="page-2-0"></span>Performance bugs typically increase the time complexity
    of the algorithm due to coding the inefficient code chunks. Obviously, this inefficient
    coding is programmed by the programmer unintentionally, but has a crucial impact
    on the performance behavior of the algorithm. For instance, allocations of heap
    memory are performed with a *malloc* function in standard C and with *new* function
    in C++. If low-level system allocations are needed, the custom allocators, e.g.,
    kalloc in Linux kernel, or ALLOC and xmalloc in gnulib, are available for vector
    reallocation. Consequently, performance improvements are clearly observable by
    calling the low-level allocation function.


    In order to show, how the performance bugs affect the performance of the algorithm,
    different applications of the vector class in C++ were taken into consideration.
    Indeed, the memory is allocated on demand by the class, and is deallocated at
    the very least when the vector''s destructor is called. When no memory is available
    to hold the new allocated elements in a heap, the vector in C++ needs to reallocate
    memory. These reallocations can be simulated as a performance bug by mass insertion
    of the new head vector''s elements by its member function *push back*, and mass
    deletion of the last vector''s element by the member function *pop back*.


    In line with this, the following four different algorithms were developed:


    • Vector (i.e., the simulation of the performance bug),


    - Raw (i.e., avoiding the performance bug version I),

    - Array (i.e., avoiding the performance bug version II),

    - double linked List (i.e., avoiding the performance bug version III).


    obtaining the same results in different ways, of course. The task of each algorithm
    is simply to initialize the array of long integer elements with their sequence
    numbers in the interval [0, 99], and, then, iterate deleting the last element
    of the array and adding the next element in the sequence of numbers to the head
    of the array. Finally, the elements of the array are ordered in descend order
    starting with the the first number that is equal to the maximum number of iterations,
    until the number that is for 100 elements lower than the maximum. Thus, two representations
    of the array are applied, i.e., vector class and array data structure in C++.
    Thus, it is expected that algorithms using the elementary data structures (e.g.,
    Array and Link) would be more efficient than those using the more abstract Vector
    class (e.g., Vector and Raw).


    The pseudo-code of the algorithm Vector is illustrated in Algorithm [1,](#page-2-2)
    from which it can be seen that this uses the vector


    <span id="page-2-2"></span>


    | Algorithm 1 Algorithm Vector              |

    |-------------------------------------------|

    | ▷ vector class C++                        |

    |                                           |

    | ▷ Initialization of vector                |

    |                                           |

    | ▷ Program loop<br>for i=0L to MAX ITER do |

    | ▷ Delete the last element                 |

    | ▷ Insert new as the first element         |

    |                                           |

    |                                           |


    class functions *push back*, *pop back*, and *insert* for adding the last, removing
    the last and inserting the first element into/from the vector variable *vec*.


    Algorithm Raw is implemented according to the pseudocode depicted in Algorithm
    [2.](#page-2-3) As evident from the pseudo-


    <span id="page-2-3"></span>


    | Algorithm 2 Algorithm Raw |                            |

    |---------------------------|----------------------------|

    | vector < long int > vec   | ▷ vector class C++         |

    | for i=0L to 100L do       |                            |

    | vec.push back(i)          | ▷ Initialization of vector |

    | end for                   |                            |

    | for i=0L to MAX ITER do   | ▷ Program loop             |

    | for j=99 to 1 step -1 do  | ▷ Exchange elements        |

    | vec[j] = vec[j-1]         |                            |

    | end for                   |                            |

    | vec[0] = i+100L           |                            |

    | end for                   |                            |


    code, the array of integer elements is defined as a class of long integer, where
    the initialization is performed in the same way as by the algorithm Vector (i.e.,
    using the *push back* function call). However, manipulation of the vector elements
    is developed in a more elementary way: At first, the whole array are reassigned
    sequentially element by element backward, while the first element is adopted with
    the number proportional to the current iteration (i.e., variable i).


    Algorithm Array, presented in pseudo-code Algorithm [3,](#page-3-1) applies the
    array C++ data structure for representation of elements of type long integer.
    As is evident from the pseudo-code,


    <span id="page-3-1"></span>


    | Algorithm 3 Algorithm Array |                            |

    |-----------------------------|----------------------------|

    | long int vec[100]           | ▷ array C++ data structure |

    | for i=0L to 100L do         |                            |

    | vec[i] = i                  | ▷ Initialization of vector |

    | end for                     |                            |

    | for i=0L to MAX ITER do     | ▷ Program loop             |

    | for j=99 to 1 step -1 do    |                            |

    | vec[j] = vec[j-1]           |                            |

    | end for                     |                            |

    | vec[0] = i+100L             |                            |

    | end for                     |                            |


    the implementation is similar to the implementation of the Raw algorithm, except
    in the declaration of the array variable vec. Here, the array data structure in
    C++ is employed in place of using vector class. The motivation behind implementing
    the algorithm is to show, which potential overhead brings the introduction of
    the more abstract class vector over the more elementary data structure array in
    C++.


    Finally, the algorithm List implements the double linked list data structure in
    C++. The advantage of this algorithm is that the functions of the high-level vector
    class are replaced with the low-level functions implemented by its own, which
    are able to optimize the algorithm''s behavior in the sense of speed and space.
    The double linked list uses the data structure as illustrated in Algorithm [4,](#page-3-2)
    from which it can be seen that this


    <span id="page-3-2"></span>


    | Algorithm 4 Double linked List data structure |                      |  |

    |-----------------------------------------------|----------------------|--|

    | struct str list {                             |                      |  |

    | long int num;                                 |                      |  |

    | str list* prev;                               |                      |  |

    | str list* next;                               |                      |  |

    | } *list;                                      | ▷ double linked list |  |


    consists of two pointers prev and next, and the long integer variable num.


    The pseudo-code of the algorithm List is presented in Algorithm [5,](#page-3-3)
    from which it can be considered that the elements are entered into the double
    linked list by calling the function *add new*. The implementation of the function
    is straightforward, and demands only to allocate the new item in the heap, initialize
    it, and ensure the proper forward and backward linking. Therefore, the pseudo-code
    of the function is not presented in the paper. Interestingly, the effect of element
    temptation is achieved by putting the current iteration number into the first
    element and tying the other 99 elements for one. This whole task can be performed
    simply by modifying the value of the pointers first and last (i.e., no reallocation
    is needed).


    <span id="page-3-3"></span>


    | Algorithm 5 Algorithm double linked List |                            |

    |------------------------------------------|----------------------------|

    | struct str list* first = NULL            |                            |

    | struct str list* last = NULL             |                            |

    | for i=99L to 1L step -1L do              |                            |

    | first = add new(first, last, i)          | ▷ Initialization of vector |

    | if i == 99 then                          |                            |

    | last = first                             |                            |

    | end if                                   |                            |

    | end for                                  |                            |

    | for i=0L to MAX ITER do                  | ▷ Program loop             |

    | last− >num = i + 100L                    |                            |

    | first = last                             |                            |

    | last = last− >prev                       |                            |

    | end for                                  |                            |


    ## IV. EXPERIMENTS AND RESULTS


    <span id="page-3-0"></span>The purpose of our experimental work was to show what
    amount of carbon footprint can be expected due to performance bugs. In line with
    this, the four implemented algorithms (i.e., Vector, Raw, Array and List) [1](#page-3-4)
    were compared according to their carbon footprint, measured on three different
    computer platforms, i.e.,:


    - with software tools on a laptop,

    - with a power-meter on a Raspberry Pi 3,

    - with a power-meter on an Apple iPad.


    In the remainder of the paper, the results of all three measurements are discussed
    in detail.


    ## *A. Measuring the carbon footprint on a laptop*


    The effective electrical energy consumption was measured on a laptop using battery
    power with *powertop* software utility under the Linux operating system. The four
    implemented test algorithms were running on the mentioned computer platforms with
    the maximum number of iterations set to MAX ITER = 10<sup>10</sup>. All the programs
    were performed independently over 10 runs and the average measures were taken
    into consideration.


    <span id="page-3-5"></span>The characteristics of the laptop battery are illustrated
    in Table [I.](#page-3-5) Let us mention that the laptop battery was at full


    TABLE I LAPTOP BATTERY CHARACTERISTICS.


    | Specification      | Description        |

    |--------------------|--------------------|

    | Type               | HP ProBook 470 G3  |

    | Amp-hour capacity  | 3 Ah               |

    | Watt-hour capacity | 44 Wh              |

    | Voltage            | 14.8 V             |

    | Cell type          | 4 cell Lithium-Ion |


    capacity before each start of the particular experiment.


    The results of the experiments are depicted in Table [II](#page-5-1) that is divided
    into the following columns: the "Init DR" represents the average initial Discharging
    Rate (DR) before and after running the algorithm, the "Running DR" is the DR under
    the strain, the "Alg. DR" refers to the DR caused


    <span id="page-3-4"></span><sup>1</sup><https://codeberg.org/firefly-cpp/green-ict-benchmarks>


    by the running algorithm, the "Time" measures the average execution time of the
    algorithm, the "Energy" denotes the energy used as a product of algorithm DR by
    the time, and the "Carbon footprint" estimates the carbon footprint as a product
    of CO<sup>2</sup> per unit emitted by one kWh (i.e., 0.65 CO2e/kWh). As indicated
    from the table, the Raw algorithm produced the higher carbon footprint among the
    four algorithms. As expected, the List algorithm was the most green in the sense
    of Green ITC.


    # *B. Measuring the carbon footprint with a power-meter on a Raspberry Pi 3*


    Raspberry Pi is a series of small single-board computers (SBCs) based on an ARM
    processor that represents an all in one computer. Due to its affordability, it
    is suitable for using in teaching basic computer science in schools. In our study,
    the Raspberry Pi was used that was equipped with a 32-bit ARM processor, 1 GB
    memory, 256 GB SSD, DVI video port, Ethernet and Wireless LAN. The Raspbian OS
    was installed on the device. The main advantage of this all in one computer represents
    its power supply over a USB-C port. This means that it is able to be monitored
    using the already mentioned AVHzY CT-3 power-meter. Let us mention that the maximum
    number of iterations was set to MAX ITER = 10<sup>8</sup> .


    An example of the measuring protocol obtained by measuring the power consumption
    by executing the Array algorithm by the PC is illustrated in Fig. [2.](#page-5-2)
    The curve in the figure illustrates the VBUS ripple that reflects a reaction of
    the Raspberry Pi in the strain caused by executing the Array algorithm. As can
    be observed, the strain is demonstrated as a step function denoting the increasing
    of the voltage. Interestingly, the sudden withdrawal of voltage is a typical consequence
    of some interrupts caused by the algorithm itself (e.g., by the output of control
    messages), different I/O actions (e.g., moving the mouse) or system actions. Let
    us mention that no other programs were active during the experimental work.


    The results of measuring the carbon footprint are depicted in Table [III.](#page-5-3)
    Actually, all the experiments consisted of three phases: (1) measuring the inactivity
    before the strain, (2) measuring the strain activity, and (3) measuring the inactivity
    after the strain. Typically, the duration of both inactivity phases was approximately
    one minute, while the strain activity phase denotes the effective power obtained
    during executing the particular algorithm. In the table, the row "Average inactivity"
    denotes the average carbon footprint in CO2/Wh measured during both inactivity
    phases, the row "Total strain" is the total carbon footprint measured during the
    strain phase, while the row "Algorithm strain" refers to the foot print caused
    by executing the algorithm (i.e., simply the difference between the total and
    the average strain). As evident from the table, the double linked list implementation
    is the most environmentally sustainable in the sense of emitting the lowest carbon
    footprint. The Vector implementation of the algorithm was better than the rest
    of the algorithms in the tests.


    # *C. Measuring the carbon footprint with a power meter on an Apple iPad*


    The Apple iPad prefers the foreground applications in order to ensure users the
    online response. Therefore, terminations are part of the application life-cycle,
    when the system terminates the long-term running process. In the case of running
    the C++ application, this termination is followed with issuing the message "Too
    much resources..." by the iOS after approximately 20 sec. Although the iPad allows
    running a long-term application in the background asynchronously, our goal was
    to indicate the carbon footprint of the foreground processes running on the iPad.
    In line with this, the maximum number of iterations need to be reduced significantly
    to MAX ITER = 10<sup>7</sup> . The specifications of the iPad were as follow:
    model number MYLA2HC/A, iPad OS version 16.7, processor 2 × Vortex, 3 GB operating
    memory, and 32 GB built-in memory.


    The measuring protocol obtained by the Array algorithm strain on the iPad using
    the power-meter AVHzY CT-3 was presented in Fig. [3,](#page-6-12) from which it
    can be concluded that the supplement of the Array algorithm to the total power
    consumption (i.e., the VBUS ripple) was not easy to observe. Therefore, this supplement
    of 6.135 seconds is denoted in the figure by arrows. When one compares the curve
    with those presented in Fig. [2](#page-5-2) obtained on the Raspberry Pi, he/she
    can conclude that the iPad system is very agile when handling a lot of active
    processes simultaneously. Although frequent withdrawal of voltage can be indicated
    in the figure, the running application left its carbon footprint clearly.


    The measured carbon footprint is depicted in Table [IV,](#page-5-4) from which
    it is evident that the carbon footprint by the algorithms in test was lower due
    to the lower execution time, but the relations between the algorithms remained
    the same as in the last experiment, i.e., the double linked list left the lesser
    carbon footprint, while the Raw algorithm was distinguished as the worst environment
    pollutant.


    ## *D. Discussion*


    As evident from the performed experimental work, measuring the power consumption
    is more accurate using the hardware device (i.e., the power-meter) than with software
    tools (i.e., the *powertop*). Although the measurements using *powertop* were
    taken at 1 second intervals, less than one-third could be captured in one minute
    (i.e., 16/60). The reason behind the behavior needs to be searched for in the
    slow communication of the tool with the power engine. On the other hand, the communication
    of the control PC with the power-meter was very fast and accurate. The hardware
    device is able to transmit up to 100 measurements in one second. This means that
    even small changes of power consumption can be sensed by the power-meter. However,
    the basic problem which arose by using the AVHzY CT-3 device with the laptop was
    that the power-meter device was able to support only devices connected to the
    power source with the USB port. This means that for measuring power consumption
    on laptops the more professional equipment is necessary.


    TABLE II CARBON FOOTPRINT OBTAINED ON THE LAPTOP BY DIFFERENT C++ ALGORITHMS.


    | Algorithm | Init DR<br>[W] | Running DR<br>[W] | Alg. DR<br>[W] | Time<br>[sec]
    | Energy<br>[Wh] | Carbon footprint<br>[CO2e/Wh] |

    |-----------|----------------|-------------------|----------------|---------------|----------------|-------------------------------|

    | Vector    | 9.288          | 15.953            | 6.665          | 198.496       |
    22.051         | 14.333                        |

    | Raw       | 9.010          | 16.142            | 7.132          | 195.606       |
    23.250         | 15.113                        |

    | Array     | 9.532          | 16.221            | 6.689          | 177.381       |
    19.776         | 12.854                        |

    | List      | 7.850          | 15.138            | 7.288          | 16.081        |
    1.953          | 1.270                         |


    <span id="page-5-1"></span>![](_page_5_Figure_2.jpeg)


    <span id="page-5-2"></span>Fig. 2. Measuring protocol on AVHzY CT-3 by Array algorithm
    strain.


    TABLE III RESULTS OF MEASURING THE CARBON FOOTPRINT.


    <span id="page-5-3"></span>


    | Carbon footprint   | Vector | Raw    | Array  | List  |  |  |  |  |  |

    |--------------------|--------|--------|--------|-------|--|--|--|--|--|

    | Average inactivity | 3.299  | 3.242  | 3.273  | 3.637 |  |  |  |  |  |

    | Total strain       | 12.860 | 63.659 | 27.496 | 8.211 |  |  |  |  |  |

    | Algorithm strain   | 9.561  | 60.417 | 24.223 | 4.574 |  |  |  |  |  |

    |                    |        |        |        |       |  |  |  |  |  |

    |                    |        |        |        |       |  |  |  |  |  |

    | TABLE IV           |        |        |        |       |  |  |  |  |  |


    APPLE IPAD


    <span id="page-5-4"></span>


    | Carbon footprint   | Vector | Raw   | Array | List  |

    |--------------------|--------|-------|-------|-------|

    | Average inactivity | 6.087  | 6.121 | 5.808 | 5.975 |

    | Total strain       | 0.855  | 1.289 | 0.637 | 0.824 |

    | Algorithm strain   | 0.123  | 0.516 | 0.317 | 0.075 |


    In first sight, it seems that measuring the carbon footprint was measured incorrectly,
    because the measured algorithm was executed on an operating system that handles
    a lot of the other programs simultaneously and also affected the increased power
    consumption. However, the number of these processes (e.g., internet explorer,
    e-mail client, etc.) was minimized during the measuring, while the average of
    both the so-called inactivity phases (i.e., before and after executing the algorithm)
    were measured explicitly. Obviously, the best solution is to put the computer
    in single-user mode, but this option is unfortunately not available to all computers
    (e.g., an iPad).


    It turns out, that the vector in C++ is sensitive on performance problem only,
    when the data class is used a lot of the time. Interestingly, sequential usage
    of the array data structure by the algorithm Array was even more efficient than
    using the built-in functions by the algorithm Vector, when the enormous actions
    were applied on the class. As expected, the algorithm List, using the classical
    implementation of double linked list, outperformed all the other algorithms.


    ## V. CONCLUSION


    <span id="page-5-0"></span>The paper tries to achieve three goals: (1) to estimate
    how the performance bugs in software influence increasing the carbon footprint,
    (2) to focus primarily on the iteration performance bugs, and (3) to find methods
    for measuring the carbon footprint caused by the performance bugs properly. In
    line with this, four algorithms simulating the iterative performance bugs were
    considered and run on three platforms (i.e., a laptop, a micro-computer Raspberry
    Pi, and an Apple iPad). Two methods for measuring the carbon footprint were


    ![](_page_6_Figure_0.jpeg)


    <span id="page-6-12"></span>Fig. 3. Measuring protocol on AVHzY CT-3 by Array
    algorithm strain on iPad.


    examined, i.e., using the software tool *powertop* on Linux and the power meter
    AVHzY CT-3. The experiments revealed that the hardware measurement using the power
    meter was more accurate than those using the software tools. However, both can
    adequately identify the increased carbon footprint caused by the simulation.


    In summary, the study integrates two domains, i.e., software engineering and green
    computing. The former can identify the software bugs, while the latter deals with
    the effects of computing on global warming. In line with this, this study warns
    of harmful effects caused by performance bugs in the sense of the increased carbon
    footprint.


    As potential directions for the future, searching for cheaper and more accurate
    solutions should be made for measuring power consumption on computers connected
    to the 230 V electricity network(e.g., AVHzY AC WiFi Watt Meter). Also, widening
    the study to include all performance bugs would be welcome.


    ## REFERENCES


    - <span id="page-6-2"></span>[1] Clare Balboni, Aaron Berman, Robin Burgess, and
    Benjamin A Olken. The economics of tropical deforestation. *Annual Review of Economics*,
    15:723–754, 2023.

    - <span id="page-6-11"></span>[2] M. Berners-Lee. *The Carbon Footprint of Everything*.
    Greystone Books, 2022.

    - <span id="page-6-9"></span>[3] Yiqun Chen, Oliver Schwahn, Roberto Natella,
    Matthew Bradbury, and Neeraj Suri. Slowcoach: Mutating code to simulate performance
    bugs. In *2022 IEEE 33rd International Symposium on Software Reliability Engineering
    (ISSRE)*, pages 274–285, 2022.

    - <span id="page-6-10"></span>[4] The Nature Conservancy. Calculate your carbon
    footprint, 2023. Last accessed 25 Octobre 2023.

    - <span id="page-6-5"></span>[5] Payal Dhar. The carbon impact of artificial intelligence.
    *Nat. Mach. Intell.*, 2(8):423–425, 2020.

    - <span id="page-6-8"></span>[6] Eva Kern, Markus Dick, Stefan Naumann, and Tim
    Hiller. Impacts of software and its engineering on the carbon footprint of ict.
    *Environmental Impact Assessment Review*, 52:53–61, 2015.

    - <span id="page-6-4"></span>[7] Jonathan Koomey et al. Growth in data center
    electricity use 2005 to 2010. *A report by Analytical Press, completed at the
    request of The New York Times*, 9(2011):161, 2011.

    - <span id="page-6-0"></span>[8] Zhu Liu, Zhu Deng, Steven J Davis, Clement Giron,
    and Philippe Ciais. Monitoring global carbon emissions in 2021. *Nature Reviews
    Earth & Environment*, 3(4):217–219, 2022.

    - <span id="page-6-1"></span>[9] Leonardo Nascimento and Niklas Hohne. Expanding
    climate policy ¨ adoption improves national mitigation efforts. *npj Climate Action*,
    2(1):12, 2023.

    - <span id="page-6-3"></span>[10] Ambuj D Sagar. Automobiles and global warming:
    Alternative fuels and other options for carbon dioxide emissions reduction. *Environmental
    Impact Assessment Review*, 15(3):241–274, 1995.

    - <span id="page-6-6"></span>[11] Pavel Somavat, Vinod Namboodiri, et al. Energy
    consumption of personal computing including portable communication devices. *Journal
    of Green Engineering*, 1(4):447–475, 2011.

    - <span id="page-6-7"></span>[12] Juha Taina. How green is your software? In *International
    Conference of Software Business*, pages 151–162. Springer, 2010.'
  decisions:
    language: '- Qualified. Reason: English Paper'
    evaluation_prompt: Qualified.
    related_work_prompt: Qualified
    novelty_prompt: Qualified
    review_only_prompt: Qualified
  llm_input_used: '## Abstract

    Much debate nowadays is devoted to the impacts of modern information and

    communication technology on global carbon emissions. Green information and

    communication technology is a paradigm creating a sustainable and

    environmentally friendly computing field that tries to minimize the adverse

    effects on the environment. Green information and communication technology are

    under constant development nowadays. Thus, in this paper, we undertake the

    problem of performance bugs that, until recently, have never been studied so

    profoundly. We assume that inappropriate software implementations can have a

    crucial influence on global carbon emissions. Here, we classify those

    performance bugs and develop inappropriate implementations of four programs

    written in C++. To mitigate these simulated performance bugs, measuring

    software and hardware methods that can estimate the increased carbon footprint

    properly were proposed.


    ## Introduction

    We find ourselves in an era marked by turbulence, where the ominous specters of
    global warming and excessive carbon emissions loom large in our collective consciousness
    [\[8\]](#page-6-0). Many scientists have long sounded the alarm about the urgent
    need to curb these emissions to avert catastrophic consequences. However, despite
    the multitude of dire projections, political movements worldwide often fall short
    in their efforts to mitigate the impact of global warming effectively [\[9\]](#page-6-1).


    Modern humans and novel economic systems play a pivotal role in exacerbating the
    carbon footprint. On one hand, we engage in activities such as deforestation,
    construction on valuable land, the proliferation of supermarkets, while, on the
    other, in the extensive use of automobiles and airplanes [\[1\]](#page-6-2), [\[10\]](#page-6-3).
    Moreover, it has become increasingly evident that Information and Communication
    Technology (ICT), which we rely upon heavily, contributes significantly in increasing
    the carbon emissions. Considering the vast number of data centers scattered across
    the globe [\[7\]](#page-6-4), as well as the multitude of machine learning models
    running incessantly [\[5\]](#page-6-5), it becomes clear just how profound our
    environmental impact can be. In addition to all these examples, the personal computers
    are using electricity, and potentially contribute in increasing the carbon emissions
    [\[11\]](#page-6-6).


    The task of Green ICT is to mitigate the carbon emissions of ICT production, applications,
    and services [\[12\]](#page-6-7). It holds that the ICT systems nowadays produce
    even 2 % of global emissions [\[6\]](#page-6-8). The reduction of carbon emissions
    comes out either directly from the hardware, or directly and indirectly from the
    software. Indeed, the carbon emission is measured as a Carbon Footprint (CF) that
    is proportional to the amount of trees needed to absorb the emitted carbon dioxide
    in a year [\[12\]](#page-6-7).


    Several publications have tackled the challenge of enhancing the sustainability
    of ICT and its associated processes. In a paper authored by Taina [\[12\]](#page-6-7),
    a comprehensive approach was presented for analyzing the carbon footprint associated
    with software. This study examined each phase of a typical software lifecycle
    meticulously, quantifying the carbon emissions generated at each step. Additionally,
    the author shared insightful strategies aimed at minimizing this carbon footprint.
    Conversely, in paper [\[6\]](#page-6-8), the authors introduced a systematic methodology
    for quantifying the carbon footprint of a software product throughout its entire
    lifecycle. Furthermore, they proposed a method for incorporating certain facets
    of carbon footprint assessment seamlessly into the software development process.
    The paper also delves into the implications and tools associated with this innovative
    calculation approach. Thus, this work underscores the significance of energy metrics,
    and the consideration of carbon footprint implications within the realm of Green
    Software Engineering.


    Performance bugs are unnecessarily inefficient code chunks in software that can
    cause prolonged execution times and degraded resource utilization [\[3\]](#page-6-9).
    For instance, an execution time of a program calling a function each time it needs
    in place of referencing the variable storing the result of the calling function
    can increase its execution time substantially. The execution time is increased
    proportionally to the number of function calls.


    The impact of the unnecessary inefficient code chunks has rarely been taken into
    consideration, especially in the sense of Green ICT. The purpose of the study
    is to observe the well known performance bug in C++ referring to a vector class,
    and to show how its inefficient usage can increase the energy consumption (indirectly
    also carbon emission) of the corresponding algorithm. Vector in C++ reallocates
    memory when the new elements are added, and no memory is avail-


    Corresponding author: Iztok Fister Jr. (e-mail: iztok.fister1@um.si).


    able to hold them. The reallocation would cause significant performance degradation
    if this occurs too often [\[3\]](#page-6-9).


    To simulate performance bug, four different versions of an algorithm were developed
    that manipulate the big number of elements, either in the vector class or the
    double linked list. All the algorithms were executed on three different platforms
    (i.e., a laptop, a Raspberry Pi 3 microcomputer, and an iPad table computer),
    and compared with each other according to the increased energy consumption measured,
    depending on the platform either using the Linux system software tool or a power
    meter capable of measuring the energy consumption on the hardware level.


    The motivation of the study was three-fold:


    - Identifying the performance bugs in software or inappropriate implementation
    that can have a great influence toward the carbon emission.

    - Investigating the influence of the iteration performance bugs on the increased
    carbon emission by results of four simulation programs developed in C++ programming
    language.

    - Searching for methods of how to measure the influence of performance bugs.


    Although it can be found three performance bugs in the literature [\[3\]](#page-6-9)
    (i.e., iteration, enumeration, and deadlock performance bugs), here, we are focused
    on the first kind only. As a result, the main contributions of the study are as
    follows:


    - Simulating the performance bugs by four different algorithms written in C++.

    - Measuring an increasing carbon footprint caused by the simulation.

    - Showing that the proposed measuring methods can be applied to estimate the increased
    carbon footprint due to the simulated performance bugs.


    In general, the main novelty of the proposed method is to link the identification
    of performance bugs with measuring the increased carbon footprint they cause.
    Thus, the domain of software engineering is integrated with the domain of green
    computing.


    The structure of the paper is as follows: Section [II](#page-1-0) discusses potential
    ways in which to measure the carbon footprint on various digital computers. In
    Section [III,](#page-2-0) the concept of performance bugs is explained briefly.
    The performed experiments and the obtained results are the subjects of Section
    [IV.](#page-3-0) Section [V](#page-5-0) concludes the paper and outlines the plausible
    directions for the future work.'
  token_usage: 6558
  time_usage: 2.001105785369873
- title: "ModuleGuard:Understanding and Detecting Module Conflicts in Python\n  Ecosystem"
  abstract: 'Python has become one of the most popular programming languages for software

    development due to its simplicity, readability, and versatility. As the Python

    ecosystem grows, developers face increasing challenges in avoiding module

    conflicts, which occur when different packages have the same namespace modules.

    Unfortunately, existing work has neither investigated the module conflict

    comprehensively nor provided tools to detect the conflict. Therefore, this

    paper systematically investigates the module conflict problem and its impact on

    the Python ecosystem. We propose a novel technique called InstSimulator, which

    leverages semantics and installation simulation to achieve accurate and

    efficient module extraction. Based on this, we implement a tool called

    ModuleGuard to detect module conflicts for the Python ecosystem. For the study,

    we first collect 97 MC issues, classify the characteristics and causes of these

    MC issues, summarize three different conflict patterns, and analyze their

    potential threats. Then, we conducted a large-scale analysis of the whole PyPI

    ecosystem (4.2 million packages) and GitHub popular projects (3,711 projects)

    to detect each MC pattern and analyze their potential impact. We discovered

    that module conflicts still impact numerous TPLs and GitHub projects. This is

    primarily due to developers'' lack of understanding of the modules within their

    direct dependencies, not to mention the modules of the transitive dependencies.

    Our work reveals Python''s shortcomings in handling naming conflicts and

    provides a tool and guidelines for developers to detect conflicts.'
  url: http://arxiv.org/abs/2401.02090v1
  keywords: ''
  document: "# ModuleGuard: Understanding and Detecting Module Conflicts in Python\
    \ Ecosystem\n\n[Ruofan Zhu](https://orcid.org/0009-0005-5181-4797) zhuruofan@zju.edu.cn\
    \ Zhejiang University Hangzhou, China\n\n[Zhengzi Xu](https://orcid.org/0000-0002-8390-7518)\
    \ zhengzi.xu@ntu.edu.sg Nanyang Technological University Singapore\n\n[Xingyu\
    \ Wang](https://orcid.org/0009-0009-9988-8065) wangxingyu@zju.edu.cn Zhejiang\
    \ University Hangzhou, China\n\n[Wenbo Shen](https://orcid.org/0000-0003-2899-6121)<sup>∗</sup>\
    \ shenwenbo@zju.edu.cn Zhejiang University Hangzhou, China\n\n[Liu Yang](https://orcid.org/0000-0001-7300-9215)\
    \ yangliu@ntu.edu.sg Nanyang Technological University Singapore\n\n[Chengwei Liu](https://orcid.org/0000-0003-1175-2753)\
    \ chengwei001@e.ntu.edu.sg Nanyang Technological University Singapore\n\n> [Rui\
    \ Chang](https://orcid.org/0000-0002-0178-0171) crix1021@zju.edu.cn Zhejiang University\
    \ Hangzhou, China\n\n### ABSTRACT\n\nPython has become one of the most popular\
    \ programming languages for software development due to its simplicity, readability,\
    \ and versatility. As the Python ecosystem grows, developers face increasing challenges\
    \ in avoiding module conflicts, which occur when different packages have the same\
    \ namespace modules. Unfortunately, existing work has neither investigated the\
    \ module conflict comprehensively nor provided tools to detect the conflict. Therefore,\
    \ this paper systematically investigates the module conflict problem and its impact\
    \ on the Python ecosystem. We propose a novel technique called InstSimulator,\
    \ which leverages semantics and installation simulation to achieve accurate and\
    \ efficient module extraction. Based on this, we implement a tool called ModuleGuard\
    \ to detect module conflicts for the Python ecosystem.\n\nFor the study, we first\
    \ collect 97 MC issues, classify the characteristics and causes of these MC issues,\
    \ summarize three different conflict patterns, and analyze their potential threats.\
    \ Then, we conducted a large-scale analysis of the whole PyPI ecosystem (4.2 million\
    \ packages) and GitHub popular projects (3,711 projects) to detect each MC pattern\
    \ and analyze their potential impact. We discovered that module conflicts still\
    \ impact numerous TPLs and GitHub projects. This is primarily due to developers'\
    \ lack of understanding of the modules within their direct dependencies, not to\
    \ mention the modules of the transitive dependencies. Our work reveals Python's\
    \ shortcomings in handling naming conflicts and provides a tool and guidelines\
    \ for developers to detect conflicts.\n\nICSE '24, April 14–20, 2024, Lisbon,\
    \ Portugal\n\n© 2024 Copyright held by the owner/author(s). Publication rights\
    \ licensed to ACM. ACM ISBN 979-8-4007-0217-4/24/04. . . \\$15.00 <https://doi.org/10.1145/3597503.3639221>\n\
    \n#### CCS CONCEPTS\n\n• Software and its engineering → Software safety; Software\
    \ reliability.\n\n#### KEYWORDS\n\nModule Conflict, PyPI Ecosystem, Dependency\
    \ Graphs, Namespace Conflict, Dependency Resolution\n\n#### ACM Reference Format:\n\
    \nRuofan Zhu, Xingyu Wang, Chengwei Liu, Zhengzi Xu, Wenbo Shen, Rui Chang, and\
    \ Liu Yang. 2024. ModuleGuard: Understanding and Detecting Module Conflicts in\
    \ Python Ecosystem. In 2024 IEEE/ACM 46th International Conference on Software\
    \ Engineering (ICSE '24), April 14–20, 2024, Lisbon, Portugal. ACM, Lisbon, Portugal,\
    \ [12](#page-11-0) pages. [https://doi.org/10.1145/3597503.](https://doi.org/10.1145/3597503.3639221)\
    \ [3639221](https://doi.org/10.1145/3597503.3639221)\n\n#### 1 INTRODUCTION\n\n\
    Namespace conflicts are a ubiquitous challenge in the field of computer science.\
    \ However, with the thriving development of the software supply chain in recent\
    \ years, there has been explosive growth in the number of open-source software,\
    \ making the issue of namespace conflicts more pressing [\\[47\\]](#page-11-1).\
    \ According to the Sonatype report [\\[39\\]](#page-11-2), the number of open-source\
    \ software represents a 20% year-over-year growth globally. As software systems\
    \ become larger and more diverse, the likelihood of encountering namespace conflicts\
    \ increases significantly.\n\nWhile namespace conflicts have long been a topic\
    \ of discussion, different language ecosystems have taken different approaches\
    \ to address namespace conflict issues. For instance, the Java ecosystem uses\
    \ groupId, artifactId, and version to name an open-source package uniquely and\
    \ extracts the package's modules to a unique path formed by this triplet [\\[26\\\
    ]](#page-11-3). Rust isolates different open-source packages by placing them in\
    \ separate folders [\\[9\\]](#page-11-4). Similarly, Python Package Index (PyPI)\
    \ uses the project name as a unique identifier for an open-source package, but\
    \ the module names within the package are not unique [\\[29\\]](#page-11-5). As\
    \ a result, we have found that it poses some threats in handling naming conflicts.\
    \ First, unlike other languages,\n\n<sup>∗</sup>Wenbo Shen is the corresponding\
    \ author.\n\nPermission to make digital or hard copies of all or part of this\
    \ work for personal or classroom use is granted without fee provided that copies\
    \ are not made or distributed for profit or commercial advantage and that copies\
    \ bear this notice and the full citation on the first page. Copyrights for components\
    \ of this work owned by others than the author(s) must be honored. Abstracting\
    \ with credit is permitted. To copy otherwise, or republish, to post on servers\
    \ or to redistribute to lists, requires prior specific permission and/or a fee.\
    \ Request permissions from permissions@acm.org.\n\nPython does not isolate each\
    \ package that has been downloaded locally but instead installs them together\
    \ by default. This results in an impact of module overwriting when two packages\
    \ with conflicting module namespaces are installed simultaneously.\n\nSecond,\
    \ as an interpreted language, Python determines which specific module to import\
    \ at runtime, unlike compiled languages that can report errors in advance. Hence,\
    \ when importing packages at runtime, the existence of modules in different paths\
    \ may lead to conflicts, posing a threat of import confusion. We refer to the\
    \ conflicts caused by module namespaces as Module Conflicts (MC). These two threats\
    \ of module conflicts are unique to the Python ecosystem since they are caused\
    \ by Python's specific mechanisms for handling namespace conflicts. These threats\
    \ can cause the program to break locally existing third-party packages when installed,\
    \ causing environmental damage that is difficult to repair. They can cause function\
    \ errors in program execution, leading to bugs that are hard to trace and fix.\
    \ Moreover, the MC issues can inhibit the normal update of packages, breaking\
    \ their integrity.\n\nExisting work provides in-depth research and analysis on\
    \ a number of issues specific to the Python ecosystem. Cheng et al. [\\[7\\]](#page-10-0)\
    \ propose PyCRE, a new approach to automatically infer Pythoncompatible runtime\
    \ environments with domain knowledge graphs. Ye et al. [\\[49\\]](#page-11-6)\
    \ propose PyEGo, a knowledge-based technique that can automatically infer dependencies'\
    \ compatible versions for Python programs. Unfortunately, when dealing with module\
    \ conflicts, they both simply consider the conflicting modules belonging to the\
    \ most popular packages. Wang et al. [\\[45\\]](#page-11-7) present SnifferDog,\
    \ an approach that can restore the execution environments of Jupyter notebooks.\
    \ Garcia et al. [\\[20\\]](#page-11-8) present DockerizeMe, a tool that can generate\
    \ Dockerfiles for Python projects. However, when dealing with module conflicts,\
    \ they all use a limited number of module-to-package mappings, which do not realize\
    \ conflicts. Other works primarily focus on dependency conflicts [\\[7,](#page-10-0)\
    \ [24,](#page-11-9) [44,](#page-11-10) [46\\]](#page-11-11) that resolve different\
    \ software components require incompatible versions of the same dependency, dependency\
    \ diagnosis [\\[6\\]](#page-10-1) that fix incorrect dependency configurations,\
    \ and detecting malicious packages [\\[3,](#page-10-2) [10,](#page-11-12) [19,](#page-11-13)\
    \ [36,](#page-11-14) [42\\]](#page-11-15) for PyPI packages and dependencies.\
    \ These works do not systematically address the MC problem or detect MC. They\
    \ either ignore the existence of multiple packages that provide the same module\
    \ or rely on incomplete module-to-package mappings that can not cover all possible\
    \ scenarios.\n\nTo fill this gap, this paper conducts a systematic study to investigate\
    \ the module conflict problem and its impact on the Python ecosystem. To achieve\
    \ the large-scale study, we first propose a novel technique named installation-free\
    \ module extraction (InstSimulator in short). It extracts the semantics of different\
    \ configuration files from multiple dimensions and then simulates the installation\
    \ process according to the semantics to extract exact modules. Our evaluation\
    \ shows that InstSimulator achieves over 95% accuracy and can complete the module\
    \ extraction of all four million packages in 10 hours with 40 threads in parallel.\
    \ Then, based on InstSimulator we implement a novel tool named ModuleGuard for\
    \ the study.\n\nTo conduct the study, we first collect 97 MC issues on GitHub\
    \ and StackOverflow, classify the characteristics and causes of these MC issues,\
    \ summarize three different conflict patterns, and analyze their potential threats.\
    \ Next, using ModuleGuard, we conduct a largescale analysis of the whole PyPI\
    \ ecosystem (4.2 million packages)\n\nand GitHub popular projects (3,711 projects)\
    \ to detect each MC pattern and analyze their potential impact. We have discovered\
    \ that there are still numerous open-source software packages in PyPI that are\
    \ impacted by module conflicts among their dependencies. This is primarily due\
    \ to developers lacking understanding of the modules within their direct and indirect\
    \ dependencies. Our work not only reveals Python's shortcomings in handling naming\
    \ conflicts but also provides tools and guidelines for developers to detect conflicts\
    \ when developing and debugging.\n\nTo summarize, this paper makes the following\
    \ contributions.\n\n- New study. We conduct a systematic study on module conflicts\
    \ (MC) in the Python ecosystem. We conducted an issue study from GitHub and StackOverflow\
    \ and summarized three MC patternsmodule-to-TPL, module-to-Lib, and module-in-Dep\
    \ conflicts and their two potential threats.\n- New technique. We propose InstSimulator,\
    \ which leverages the semantics and installation simulation to achieve accurate\
    \ and efficient module extraction. Based on this, we implement a tool ModuleGuard\
    \ to detect MCs for the Python ecosystem. We construct benchmarks for evaluating\
    \ the capabilities of module information extraction and dependency graph resolution.\n\
    - Ecosystem-scale analysis. Utilizing ModuleGuard, we conduct a large-scale study\
    \ and analyze 4.2 million packages on PyPI (434,823 latest version packages as\
    \ of April 2023). We get a lot of interesting findings, shed some light on the\
    \ nature of module conflicts, and provide some guidance for developers.\n- Issue\
    \ reporting. We examine 93,487 tags of 3,711 popular GitHub projects, of which\
    \ 108 are now or ever affected by MC. We reported issues, and a lot of issues\
    \ have been confirmed and fixed. This proves that our work can help developers\
    \ understand previously unrealized errors and help them fix potential threats.\n\
    \nThis paper is organized as follows. We introduce background knowledge in [§2.](#page-1-0)\
    \ In [§3,](#page-2-0) we propose our ModuleGuard tool and introduce it in detail.\
    \ In [§4](#page-4-0) we evaluate ModuleGuard in different metrics. In [§5](#page-5-0)\
    \ we conduct an ecosystem-scale study on MC, including issues study, GitHub projects,\
    \ and PyPI packages. [§6](#page-9-0) and [§7](#page-9-1) present some limitations\
    \ and discussion. Related work is described in [§8.](#page-10-3) Finally, we conclude\
    \ the whole paper in [§9.](#page-10-4)\n\n#### <span id=\"page-1-0\"></span>2\
    \ BACKGROUND\n\nIn this section, we provide the necessary background for our study.\
    \ We first explain how Python code is managed and how Python modules are shared\
    \ among developers. Then, we describe how PyPI handles third-party libraries (TPLs)\
    \ and their dependencies. Finally, we illustrate two examples of module conflicts\
    \ (MCs), which are the main problem we address in this paper.\n\nPython code management.\
    \ Python uses pip [\\[29\\]](#page-11-5) as its official package manager for downloading\
    \ and installing TPLs from PyPI. When pip receives a package request, it first\
    \ resolves the constraint and selects a suitable version of the package. Then,\
    \ it downloads the package to a local temporary folder. If the package is a source\
    \ distribution package, pip extracts all the files, compiles the package, generates\
    \ metadata files, and installs the package based on the metadata files. If the\
    \ package is a binary distribution package, pip installs it directly based on\
    \ the metadata files embedded in it. By default, pip installs third-party packages\
    \ into the site-packages\n\n<span id=\"page-2-1\"></span>![](_page_2_Figure_1.jpeg)\n\
    \nFigure 1: Module conflict example. Example (a) illustrates the overwriting module\
    \ when downloading the package. Example (b) illustrates importing confusion when\
    \ running the code.\n\nfolder unless the user specifies a different folder using\
    \ the target (-t) argument.\n\nAfter installing packages, users can import modules\
    \ from them in their Python codes. The import process consists of two steps: first,\
    \ the user declares the name of the module to be imported (e.g., import bs4).\
    \ Second, the Python interpreter searches for the module when it executes the\
    \ import statement. The Python interpreter first looks in the module cache that\
    \ records the modules already imported. If not found in the cache, the interpreter\
    \ searches in the sys.path order, which is a list of directories where Python\
    \ looks for modules. Once the interpreter finds a module with the same name as\
    \ the import statement, it stops the search and returns the module.\n\nPython\
    \ dependency management. Python developers can specify the dependencies of their\
    \ projects in requirement.txt and use the command pip install -r requirement.txt\
    \ to install them. Alternatively, developers can package and upload their projects\
    \ to PyPI after declaring the dependencies in a configuration file, such as setup.py\
    \ or pyproject.toml. The configuration file is compiled to generate metadata files,\
    \ which contain the dependency information. The dependency information is stored\
    \ in the requires.txt file of the source distribution package or in the METADATA\
    \ file of the binary distribution package.\n\nPython supports various types of\
    \ dependencies, such as build, install, extra, and test dependencies. Each type\
    \ can also be conditional on the local environment so that pip will install them\
    \ selectively. Build dependencies and test dependencies are typically used only\
    \ during development and testing, and they do not affect the installation process.\
    \ Therefore, in this paper, we only consider install dependencies and extra dependencies,\
    \ which are the dependencies required for a successful installation of a package.\n\
    \nModule conflict examples. Here we give two examples of how module conflicts\
    \ can affect Python projects. Figure [1\\(](#page-2-1)a) shows a module conflict\
    \ example that causes the module overwriting\n\nthreat. More specifically, the\
    \ project ysr-monitor@0.1.5 [\\[50\\]](#page-11-16) has two dependencies board@1.0\
    \ [\\[5\\]](#page-10-5) and adafruit-blinka@8.15.2 [\\[1\\]](#page-10-6), which\
    \ both contain a module named board.py. When pip installs these packages, it will\
    \ overwrite the board.py module from the first package with the one from the second\
    \ package. This can lead to errors that are hard to debug, especially in CI/CD\
    \ environments. To fix this issue, the developer needs to manually find the overwritten\
    \ module and reinstall the corresponding package. However, this process is complicated\
    \ and may corrupt the local environment further. Figure [1\\(](#page-2-1)b) shows\
    \ another example of importing a wrong module. The packages jwt@1.3.1 and pyjwt@2.6.0\
    \ are installed in the two different targets (e.g., one in the system site-packages\
    \ folder and one in the virtualenv [\\[41\\]](#page-11-17) site-packages folder).\
    \ However, they both have the jwt/exceptions.py module, which will conflict when\
    \ imported. The Python interpreter will import the module based on the order of\
    \ the folders in the sys.path list, which may not be the intended one. To fix\
    \ this issue, the developer needs to either change the order of the folders in\
    \ the sys.path list or use absolute paths to import the modules, which is inconvenient\
    \ and error-prone.\n\n#### <span id=\"page-2-0\"></span>3 MODULEGUARD DESIGN\n\
    \nWe present ModuleGuard, a tool that helps us to investigate the impacts of module\
    \ conflicts in the Python ecosystem. Figure [2](#page-3-0) illustrates the framework\
    \ of our work, which comprises two main components. The first component is InstSimulator,\
    \ which extracts module information from various configuration files and simulates\
    \ the installation process without actually installing the packages. The second\
    \ component is EnvResolution, which resolves more accurate dependency graphs of\
    \ Python packages by considering the local environment and the extra dependencies.\n\
    \n# 3.1 InstSimulator: Installation-free Module Extraction\n\nChallenges. To conduct\
    \ a large-scale study of the Python ecosystem, we need to obtain the module information\
    \ of Python packages. However, this is not a trivial task, as it faces several\
    \ challenges. First, PyPI has several types of packages, and each type has its\
    \ own configuration files and formats. However, there is no comprehensive documentation\
    \ that specifies which files and parameters are related to module information\
    \ and how to parse them. Second, the module information can change before and\
    \ after the package installation. As shown in Figure [3,](#page-3-1) the package\
    \ pugs@0.0.1 has different modules before and after the installation. This is\
    \ because the installation process is controlled by configuration files (e.g.,\
    \ setup.py), which can add, remove, or rename modules. For example, if namespace\\\
    _packages = ['namespace\\_pugs'] is defined in the setup.py, it will remove the\
    \ namespace\\_pugs/\\_\\_init\\_\\_.py module and add a new nspkg.pth module.\n\
    \nTo address these challenges, we propose InstSimulator, which has two main functionalities:\
    \ (1) To solve the first challenge, we systematically study all module-related\
    \ files and parse the raw module data from different types of configuration and\
    \ metadata files, and (2) to solve the second challenge, we leverage a novel approach\
    \ to simulate the installation process to obtain the accurate module information\
    \ without installing the packages.\n\n<span id=\"page-3-0\"></span>![](_page_3_Figure_2.jpeg)\n\
    \nFigure 2: Overview of our work.\n\nTable 1: Module and dependency-related data.\n\
    \n<span id=\"page-3-2\"></span>\n\n|                        | File           |\
    \ Module-Related Data    | Dependency-Related Data |  |\n|------------------------|----------------|------------------------|-------------------------|--|\n\
    | Files<br>Metadata      | EGG-INFO*      | top_level.txt          | requires.txt\
    \            |  |\n|                        |                | SOURCES.txt   \
    \         |                         |  |\n|                        |         \
    \       | top_level.txt          |                         |  |\n|           \
    \             | egg-info*      | namespace_packages.txt | requires.txt       \
    \     |  |\n|                        |                | SOURCES.txt          \
    \  |                         |  |\n|                        |                |\
    \ top_level.txt          |                         |  |\n|                   \
    \     | dist-info*     | namespace_packages.txt | METADATA                |  |\n\
    |                        |                | RECORD                 |         \
    \                |  |\n| Files<br>Configuration | setup.py       | py_modules\
    \             |                         |  |\n|                        |     \
    \           | packages               | install_requires        |  |\n|       \
    \                 |                | package_dir            | extras_require \
    \         |  |\n|                        |                | namespace_packages\
    \     |                         |  |\n|                        |             \
    \   | py_modules             |                         |  |\n|               \
    \         | setup.cfg      | packages               | install_requires       \
    \ |  |\n|                        |                | package_dir            | extras_require\
    \          |  |\n|                        |                | namespace_packages\
    \     |                         |  |\n|                        |             \
    \   | py-modules             |                         |  |\n|               \
    \         | pyproject.toml | packages               | dependencies           \
    \ |  |\n|                        |                | package-dir            | optional-dependencies\
    \   |  |\n|                        |                |                        |\
    \                         |  |\n\n\\* The EGG-INFO is in \"egg\" type packages.\
    \ The egg-info is in \"tar.gz\" type packages. The dist-info is in \"whl\" type\
    \ packages.\n\n<span id=\"page-3-1\"></span>![](_page_3_Figure_7.jpeg)\n\n####\
    \ Figure 3: Module paths change after installation. Specific parameters in the\
    \ configuration file control these behaviors.\n\nRaw module data extraction. To\
    \ extract the raw module data from Python packages, we first identify the types\
    \ and formats of files that contain module information. Then, we implement different\
    \ parsers for each type of file.\n\nWe conduct a systematic study of the packaging,\
    \ compilation, and installation process of Python packages. We use differential\
    \ testing to modify different parameters in the configuration files and observe\
    \ the changes in the metadata files and the module information. Based on these\
    \ results, we combine the official Python [\\[15\\]](#page-11-18) and pip [\\\
    [29\\]](#page-11-5) documentation to infer the function of different parameters.\
    \ Table [1](#page-3-2) shows the result of our study. Python packages have three\
    \ kinds of metadata files: egg-info, EGG-INFO, and dist-info. They correspond\
    \ to the .tar.gz, .egg, and .whl distribution packages, respectively. Moreover,\
    \ Python packages have three types of configuration files: setup.py, setup.cfg,\
    \ and pyproject.toml. The setup.py is an executable script file, while the other\
    \ two are formatted configuration files. They can all define module information\
    \ but use different formats and parameters.\n\nThe InstSimulator implements different\
    \ parsers for each type of configuration and metadata file. Specifically, InstSimulator\
    \ converts text-type files into a list, parses formatted configuration files with\
    \ a format-specific parser, and for setup.py, the executable script file, InstSimulator\
    \ uses AST and data flow analysis to extract module configuration parameters based\
    \ on a PyCD tool proposed in [\\[6\\]](#page-10-1). These parsers can extract\
    \ the relevant information from the files and store them in a structured way.\
    \ In this way, InstSimulator can avoid decompressing the package locally by reading\
    \ only a few configuration files and metadata files from memory, which saves time\
    \ and space.\n\nInstallation-free module simulation. We propose a novel technique\
    \ named installation-free module simulation to convert raw module data into module\
    \ information after installation. This technique consists of three steps: (1)\
    \ InstSimulator first takes the file structure in the compressed package and turns\
    \ it into a virtual file tree. (2) It then translates each raw data semantics\
    \ into operations that add, delete, and search for nodes in the virtual file tree.\
    \ (3) It employs the DFS algorithm to traverse the file tree and obtain all module\
    \ paths.\n\nFor example, suppose a package has the raw data packages = [pugs\\\
    _lib, namespace\\_pugs] and package\\_dir = pugs\\_lib : pugs. The packages data\
    \ is a list of folder names after installation, and the package\\_dir data is\
    \ a mapping of folder names before and after installation (pugs\\_lib is the folder\
    \ name of the pugs after installation). If there is no mapping in package\\_dir,\
    \ the folder name will not change before and after installation. Based on their\
    \ semantics, InstSimulator uses the BFS algorithm to search for the folder names\
    \ (e.g., pugs) before installation recorded in package\\_dir (searching root node\
    \ is the parent node of the setup.py or other configuration files), and changes\
    \ their node names to the names after installation (i.e., pugs\\_lib). Then, it\
    \ deletes all the subtrees except the nodes\n\nrecorded in the packages data.\
    \ After simulating all the raw module data semantics, we can obtain a file tree\
    \ that contains the module information without the installation process. Finally,\
    \ InstSimulator uses the DFS algorithm to traverse the file tree and obtain the\
    \ module paths. Note that each path from the root node to a leaf node within this\
    \ tree corresponds to a module path. We have described the semantics of all the\
    \ parameters and a more detailed code demo in the artifact we release.\n\n# 3.2\
    \ EnvResolution: Environment-aware Dependency Resolution\n\nChallenges. To investigate\
    \ the module conflicts in the Python ecosystem, we need to resolve the dependency\
    \ graphs of over 4.2 million packages. However, it faces the following challenges.\
    \ First, using common static resolution methods to obtain the dependency graphs\
    \ without installation is not very accurate. This is because the Python dependency\
    \ graphs depend on the local environment information, and the dependencies between\
    \ packages are complex due to the extra dependencies. Moreover, previous work\
    \ [\\[17,](#page-11-19) [40,](#page-11-20) [44,](#page-11-10) [46\\]](#page-11-11)\
    \ either lacks up-to-date information or has lower accuracy. They often parse\
    \ dependencies from only a single dimension, such as setup.py or requires.txt,\
    \ resulting in incomplete information. Furthermore, they do not consider the local\
    \ environment-related and extra dependencies, which are common in Python packages\
    \ and can affect the dependency graphs. This leads to inaccurate dependency graphs.\
    \ On the other hand, obtaining accurate dependency graphs using pip requires the\
    \ installation process, which is time-consuming and may fail due to local environment\
    \ incompatibilities.\n\nTo solve these, we present EnvResolution, a local environmentaware\
    \ dependency resolution that supports the local environment and extra dependencies.\
    \ It consists of three steps: (1) multidimensional dependency information extraction,\
    \ (2) local environment information collection, and (3) dependency graph resolution.\n\
    \nMultidimensional dependency information extraction. EnvResolution adopts a multi-dimensional\
    \ approach for extracting direct dependencies from three dimensions: PyPI API\
    \ [\\[12\\]](#page-11-21), dependencies in metadata files, and dependencies in\
    \ configuration files. Similar to the approach in [§3.1,](#page-3-1) we first\
    \ classify the dependencyrelated information from the metadata and configuration\
    \ files, as shown in Table [1.](#page-3-2) In this step, we will save both the\
    \ direct dependencies with their environmental conditions and the optional extra\
    \ dependencies of the Python project. Then we convert the dependency information\
    \ parsed from different files into a unified format for subsequent parsing.\n\n\
    Local environment information collection. For the local environment, EnvResolution\
    \ collects 11 types of environmental information [\\[14\\]](#page-11-22) that\
    \ may affect the dependency graphs, such as python\\_version, os\\_name, and so\
    \ on. These environment variables and their values are stored in a global dictionary\
    \ when resolving dependencies (e.g., {(python\\_version, 3.10), (os\\_name, posix),\
    \ (sys\\_platform, linux), ...}).\n\nDependency resolution. EnvResolution uses\
    \ the resolvelib [\\[37\\]](#page-11-23) framework as the core backtracking resolution\
    \ algorithm. To improve the efficiency and accuracy of dependency graph resolution,\
    \ EnvResolution implements the following optimizations. First, like\n\nTable 2:\
    \ Evaluation of ModuleGuard.\n\n<span id=\"page-4-1\"></span>\n\n| Benchmark \
    \    | Dataset | Correct | Miss | Excess | Error | Accuracy |\n|---------------|---------|---------|------|--------|-------|----------|\n\
    |               | Data1   | 4,045   | 152  | 28     | 7     | 95.58%   |\n| InstSimulator\
    \ | Data2   | 3,834   | 116  | 37     | 2     | 96.11%   |\n| EnvResolution |\
    \ Data1   | 4,177   | 41   | 8      | 13    | 98.70%   |\n| (Node)        | Data2\
    \   | 3,795   | 93   | 30     | 20    | 96.37%   |\n| EnvResolution | Data1  \
    \ | 4,133   | 46   | 11     | 47    | 97.66%   |\n| (Edge)        | Data2   |\
    \ 3,748   | 107  | 40     | 33    | 95.18%   |\n\n\\* Node: evaluate nodes in\
    \ dependency graph only.\n\n\\* Edge: evaluate nodes and edges in the dependency\
    \ graph.\n\n\\* Accuracy: correct/total\n\nprevious work [\\[6,](#page-10-1) [44,](#page-11-10)\
    \ [46\\]](#page-11-11), EnvResolution also employs a local knowledge base. However,\
    \ EnvResolution adopts multi-dimensional dependency information extraction, which\
    \ can obtain more comprehensive dependency information than previous work, including\
    \ local environmental conditions and extra dependencies.\n\nSecond, EnvResolution\
    \ supports resolving extra dependencies and local environment dependencies. More\
    \ specifically, during dependency resolution, an extra dependency will add an\
    \ entry to the environment variable dictionary. For example, dependency pandas[compression]\
    \ will add compression to the environmental variable dictionary. After that, we\
    \ can treat an extra dependency as a special local environmental dependency. When\
    \ resolving each direct dependency of packages, EnvResolution checks whether the\
    \ dependency's environmental conditions match the values in the environmental\
    \ variable dictionary. A dependency will be dropped if the value in the dictionary\
    \ does not meet the condition. For instance, dependency numpy>= 1.21.0; (python\\\
    _version, >= 3.11) will be dropped if the python version is 3.10 and does not\
    \ meet the condition >=3.11.\n\nThird, EnvResolution adopts a priority policy\
    \ when resolving dependencies for efficiency. This is based on our observation\
    \ that the order of dependencies does not affect the result of the resolution,\
    \ but it does affect the time to backtrack the resolution algorithm. A good order\
    \ can reduce the number of backtrack times, thus improving the parsing efficiency.\
    \ Therefore, EnvResolution sorts the dependencies according to the following rules\
    \ in each recursion: resolving the pinned version dependencies first, then the\
    \ dependencies with a scope constraint, and finally the dependencies with no constraint.\
    \ Moreover, dependencies that are close to the root node are always resolved before\
    \ dependencies that are far from the root node. In this way, EnvResolution can\
    \ significantly reduce the number of backtracking times and thus improve the resolution\
    \ efficiency.\n\n#### <span id=\"page-4-0\"></span>4 MODULEGUARD EVALUATION\n\n\
    Due to the lack of established ground truth for comparison, we carefully construct\
    \ a benchmark to evaluate the two aspects of ModuleGuard, respectively. We collect\
    \ datasets from the two following sources:\n\n• Dataset 1. We select the top 3,000\
    \ projects from Libraries.io [\\[25\\]](#page-11-24) and PyPI Downloads Table\
    \ [\\[18\\]](#page-11-25) for six months from August 2022 to February 2023, respectively\
    \ and we apply a de-weighting process to the two sets to reduce the bias.\n\n\
    • Dataset 2. We randomly select 5000 projects from the total list of the PyPI\
    \ package.\n\nFor each dataset, we select the latest version and install it using\
    \ pip for each project. We use the module relative paths and the dependency graphs\
    \ obtained after the installation as the ground truth for comparison. There are\
    \ two main reasons for installation failure: First, the local environment is not\
    \ compatible with the package, e.g. Python2 package cannot be installed in the\
    \ Python3 environment; Second, an error occurred while running the installation\
    \ script, causing the installation process to exit. Finally, we get 4,232 and\
    \ 3,989 projects in the two datasets.\n\nEvaluate metrics. We define four metrics\
    \ to evaluate the accuracy of ModuleGuard. (1) Correct. The modules or dependency\
    \ graphs resolved by ModuleGuard are totally consistent with the ground truth.\
    \ (2) Miss. Some modules or some elements in dependency graphs of the ground truth\
    \ do not exist in our results. (3) Excess. Some modules or some elements in dependency\
    \ graphs resolved by ModuleGuard do not exist in the ground truth. (4) Error.\
    \ Other cases.\n\nExperimental setup. To obtain the module paths after installation,\
    \ we use pip install XXX -t target –no-dependencies to install packages. This\
    \ command implies that the latest version package will be installed in the target\
    \ folder and no dependencies will be installed. Moreover, we only considered modules\
    \ with .py extensions, so the data files (e.g. pictures, tables) included in the\
    \ packages will be ignored.\n\nTo obtain the dependency graphs, we also use pip\
    \ installation to get the ground truth. We add the following settings to obtain\
    \ the exact dependency graph. First, pip installation process depends on the repository\
    \ status of the remote, which is updated in real-time. In contrast, our local\
    \ knowledge base is updated on a daily basis. To address this gap, we mirrored\
    \ approximately 13TB of PyPI packages locally with bandersnatch [\\[4\\]](#page-10-7)\
    \ tool and we use this local mirror during pip installations (i.e. pip install\
    \ -i localhost/simple). Second, in order to obtain pip's dependency resolution\
    \ results, we hook pip's dependency resolve function and write the dependency\
    \ graph to files.\n\nInstSimulator result. Table [2](#page-4-1) shows the results\
    \ of InstSimulator evaluation benchmark. The results show that InstSimulator has\
    \ the ability to extract module information with 95.58% and 96.11% accuracy on\
    \ different datasets. In addition, the table shows that InstSimulator technique\
    \ has 152 (3.59%) and 116 (2.91%) Misses on the two datasets, respectively. This\
    \ is mainly because InstSimulator uses AST static analysis method parsing the\
    \ setup.py install script, which has limitations on parsing syntactically complex\
    \ install scripts correctly. Moreover, a given version of a Python project has\
    \ multiple packages with slightly different modules. However, the InstSimulator\
    \ selects only one of these packages to parse, which might differ from the package\
    \ actually installed. For instance, the 0.4.4 version of the jaxlib project has\
    \ 12 packages. Overall, ModuleGuard is able to achieve over 95% accuracy in module\
    \ extraction. It meets our requirement of extracting the whole ecosystem packages'\
    \ modules.\n\nEnvResolution result. We considered both node-level and graph-level\
    \ benchmarks to evaluate the accuracy of the EnvResolution technique in ModuleGuard.\
    \ The node-level benchmark focuses\n\nsolely on the individual nodes within the\
    \ graph, without considering the edges between them. It evaluates the ability\
    \ to extract direct dependencies. In contrast, the graph-level benchmark takes\
    \ into account both the nodes and edges present in the graph. It evaluates the\
    \ ability to resolve dependency graphs.\n\nTable [2](#page-4-1) shows the results\
    \ of our evaluation. The results show that the accuracy of EnvResolution ranges\
    \ from 95.18% to 98.70% where the edge accuracy is lower than that of node. What's\
    \ more, we manually reviewed projects that were unable to resolve and identified\
    \ two primary causes for their failure. First, we are unable to process complex\
    \ setup.py configuration files. For example, the setup.py file of the project\
    \ ta retrieves dependencies from files within the package, however, ModuleGuard\
    \ does not unpack the package to optimize extraction speed. Second, we read dependencies\
    \ from metadata files generated by maintainers' local environments, which may\
    \ deviate from our experimental environment. For instance, project fortnitepy\
    \ has three direct dependencies in its metadata, whereas pip only extracts two\
    \ during our local installation.\n\nOverall, the results demonstrate that although\
    \ EnvResolution has some limitations in extracting dependencies information, it\
    \ is capable of handling large-scale packages quickly with an acceptable error.\n\
    \n#### <span id=\"page-5-0\"></span>5 LARGE-SCALE STUDY\n\nSince the MC problem\
    \ has not been studied systematically in Python language in existing work, we\
    \ empirically study the MC issues from GitHub and Stack Overflow and classify\
    \ them into three patterns. We then used ModuleGuard to evaluate all 4.2 million\
    \ PyPI packages and 3,711 high-star projects collected from GitHub for the presence\
    \ of MCs and their potential impacts. In summary, we propose the following research\
    \ questions:\n\n- RQ1 (Issue Study). What are the common types of module conflict\
    \ issues? What potential threats might they have?\n- RQ2 (PyPI Packages). How\
    \ many of all PyPI packages have MC effects?\n- RQ3 (GitHub Projects). How many\
    \ popular projects on GitHub are affected by MC, and what are their characteristics?\n\
    \n#### 5.1 RQ1: Issue Study\n\n5.1.1 Data Collection. We collect 97 MC issues\
    \ in total and we search them in two steps. First, we combined two sets of keywords—\
    \ (module OR name) AND (clash OR conflict) to search for MC issues on GitHub and\
    \ added is:issue and language:python options. Since Github can only show the first\
    \ 100 pages of search results for each combination search result, we obtained\
    \ the search results in order of best match and collected 4,000 issues. Second,\
    \ for the 4,000 issues, the three co-authors manually reviewed the descriptions\
    \ and bug reports in the issues and finally filtered out 55 issues that were strongly\
    \ related to MC issues. We also notice that some maintainers or reporters would\
    \ cite related issues in their comments. As a result, we searched for other issues\
    \ mentioned in these 55 issues using the snowballing technique [\\[21\\]](#page-11-26)\
    \ and checked them manually. Finally, we collected 78 MC issues from GitHub. The\
    \ keyword \"Python module name (clash OR conflict)\" was used to search on StackOverflow.\
    \ We manually review the top 200 most relevant issues that include\n\nanswers.\
    \ Ultimately, a total of 19 issues related to MC are collected from Stackoverflow.\n\
    \n5.1.2 Module Conflict Types. After studying the collected 97 MC issues, we observe\
    \ that module conflicts can occur in three situations after packaging the project\
    \ and uploading it to the PyPI or Github. First, modules of the project may conflict\
    \ with the built-in standard library modules, causing module-to-Lib conflict.\
    \ Second, As a TPL, its modules can conflict with the other TPLs (not relevant\
    \ to this project), leading to module-to-TPL conflict. In addition, projects that\
    \ declare direct dependencies in their configuration files may have module conflicts\
    \ within the dependency graph (those TPLs are relevant), resulting in module-in-Dep\
    \ conflicts.\n\nIn the following, we give each type of conflict a formal definition\
    \ and discuss them in detail with illustrative issue examples. To ease the discussion,\
    \ we first give some grammar below:\n\n: the set of all packages on PyPI\n\n-\
    \ : ∈ ,representing a specific package\n- : { | is a module after installing package\
    \ }\n\n : { | is a standard library module}\n\n() : the dependency graph of the\
    \ package\n\n1 conflict ←−−−−→ <sup>2</sup> : <sup>1</sup> and <sup>2</sup> have\
    \ the same name or same path\n\nModule-to-Lib conflict. Conflicts can occur between\
    \ the project's modules and standard library (Lib) modules (21/97=21.65%). Suppose\
    \ there are two modules. One module belongs to package and the other is a library\
    \ module and they have conflicts. In that case, we consider the packages to have\
    \ a module-to-Lib conflict. We formulate it as the following:\n\n∃ ∈ ,\n\n$$m\
    \ \\in M\\_{\\mathfrak{p}} \\land m\\_l \\in M\\_{Lib} \\land m \\xleftarrow{\\\
    text{conflict}} m\\_l$$\n\nFor example, the #14 issue [\\[48\\]](#page-11-27)\
    \ of the python-hgijson. The package python-hgijson@1.5.0 has a json module, and\
    \ it conflicts with the standard library json module, resulting in module-to-Lib\
    \ conflict.\n\nModule-to-TPL conflict. Modules will also conflict with the modules\
    \ of the other unrelated third-party packages (64/97=65.98%). Suppose there are\
    \ two different modules. One module belongs to package and the other module ′\
    \ belongs to another unrelated package ′ such that the two module conflicts with\
    \ each other, we consider the package and ′ both have a module-to-TPL conflict.\
    \ We formulate module-to-TPL conflict as the following:\n\n$$\n\\exists \\text{\
    \ } p, p' \\in P \\land p \\neq p',\n$$\n\n$$m \\in M\\_{\\mathfrak{b}} \\land\
    \ m' \\in M\\_{\\mathfrak{b}'} \\land m \\xleftarrow{\\text{conflect}} m'$$\n\n\
    For example, the #3 issue [\\[28\\]](#page-11-28) shows that the python-slugify@8.0.0\
    \ and the awesome-slugify@1.6.5 packages both have a slugify conflicting module\
    \ and they have a conflict with each other if they installed together, so the\
    \ two packages have a module-to-TPL conflict.\n\nModule-in-Dep conflict. Conflicts\
    \ can occur within the project's dependency graphs (12/97=12.37%). If there are\
    \ two packages and ′ within a Dependency Graph of the root package ( can be one\n\
    \n<span id=\"page-6-0\"></span>Table 3: Statistics of MC types and their potential\
    \ threats.\n\n| MC types           | Modules overwriting | Import confusion |\n\
    |--------------------|---------------------|------------------|\n| Module-to-Lib\
    \ (21) | -                   | ✓                |\n| Module-to-TPL (64) | ✓  \
    \                 | ✓                |\n| Module-in-Dep (12) | ✓             \
    \      | -                |\n\nof or ′ , or it can be another package), a module\
    \ within package and a module ′ within package ′ such that they conflict with\
    \ each other, we denote the packages has a module-in-Dep conflict. We formulate\
    \ it as the following:\n\n$$\\begin{aligned} \\Xi \\left( \\not p, \\not p' \\\
    right) &\\subseteq DG(r) \\land p \\neq p',\\\\ m &\\in M\\_{\\mathfrak{d}} \\\
    land m' \\in M\\_{\\mathfrak{d}'} \\land m \\xleftarrow{\\text{conflict}} m' \\\
    end{aligned}$$\n\nFor example, the #44 issue [\\[35\\]](#page-11-29) of theemoca.\
    \ The package emoca@1.0 has opencv-python-headless@4.5.5 and opencv-python@4.5.5\
    \ in its dependency graph, and they both have a cv2 module, so the root package\
    \ emoca@1.0 has a module-in-Dep conflict.\n\n5.1.3 Module Conflict Threats. Based\
    \ on the description of issues, we summarize that the Python ecosystem suffers\
    \ from two shortcomings in code management. First, pip installs TPL modules into\
    \ the site-packages folder by default and does not isolate packages from each\
    \ other. This means that different packages, including their direct and indirect\
    \ dependencies, are mixed in the same directory. As a result, different modules\
    \ from different TPLs would conflict with each other and will cause modules overwriting\
    \ threats. Second, Python provides a more flexible code management mechanism,\
    \ allowing the import of modules (with access rights) from anywhere in the system,\
    \ including standard libraries, TPL modules, and the project's own modules. However,\
    \ they use the same statement for importing and do not differentiate between them.\
    \ The Python interpreter searches for modules using the first-match principle.\
    \ Therefore, conflicts can occur between modules that have the same module path\
    \ but are installed in different locations and will lead to importing confusion\
    \ threats. Furthermore, these two flaws pose threats that will have potential\
    \ impacts on the installation, upgrade, and importing of software packages. Table\
    \ [3](#page-6-0) illustrates statistics on the different types of module conflicts\
    \ that may have threats on the corresponding stages. In the following, we will\
    \ introduce them in detail with examples.\n\nThreat 1 (Modules overwriting). Module\
    \ overwriting is a serious threat to the integrity and functionality of Python\
    \ projects, as it may cause unexpected errors or behaviors. Module overwriting\
    \ occurs when two modules with the same relative module path are installed into\
    \ the same directory, resulting in one module being overwritten by another. It\
    \ can be triggered by two types of module conflicts: module-to-TPL conflict and\
    \ module-in-Dep conflict.\n\nModule-to-TPL conflict refers to the situation where\
    \ two packages have conflicting modules. This can happen when pip installs TPL\
    \ modules into the site-packages folder by default and it does not isolate packages\
    \ from each other. For instance, in issue #4625 [\\[32\\]](#page-11-30) of the\
    \ project pypa/pip, the developer installs both pyjwt@1.5 and jwt@0.5.2, which\
    \ have conflicts on the module jwt/exceptions.py. The later installed module will\
    \ overwrite the first installed module when two conflicting modules are installed\
    \ simultaneously.\n\n′\n\nMoreover, module-to-TPL conflict can also occur in Windows\
    \ systems due to case insensitivity of paths. For example, in issue #156 [\\[31\\\
    ]](#page-11-31) of the project pycrypto, the developer installs the crypto (lowercase)\
    \ module first, and then installs pycrypto, which has the Crypto (upper case)\
    \ module name. However, since the crypto folder already exists locally, pip cannot\
    \ create the Crypto (upper case) folder and instead installs all modules under\
    \ the pre-existing crypto (lowercase) folder. Consequently, this process overwrites\
    \ crypto's modules and breaks the project's functionality.\n\nModule-in-Dep conflict\
    \ refers to the situation where a project and its dependencies have conflicting\
    \ modules. The conflict can cause module overwriting during the installation.\
    \ For example, in issue #841[\\[2\\]](#page-10-8) of the project Albumentations,\
    \ the project installs both opencv-python (indirect dependency) and opencv-python-headless\
    \ (direct dependency). They have conflicts on the cv2 module and its sub-modules.\
    \ Although the official documentation [\\[13\\]](#page-11-32) states that they\
    \ cannot be installed simultaneously, developers are not aware of this restriction\
    \ as indirect dependencies are a black box for them.\n\nFurthermore, module overwriting\
    \ can also affect package upgrades, as pip's update process may cause module conflicts\
    \ during installation or uninstallation. For example, in issue #8509 [\\[33\\\
    ]](#page-11-33), the package ansible@2.9.10 misses some files after upgrading\
    \ due to modules being overwritten during the update process. The update process\
    \ is as follows: first, pip installs the package's dependencies (where module\
    \ overwrites may occur), then it uninstalls the old version of the package (possibly\
    \ uninstalling newly installed modules that have already been overwritten), and\
    \ finally it installs the new version of the package (package integrity is compromised).\n\
    \nThreat 2 (Importing confusion). Conflicts between modules and the standard library\
    \ (module-to-Lib) or third-party libraries (module-to-TPL) pose a potential threat\
    \ of importing confusion. For module-to-Lib conflicts, the standard library modules\
    \ are stored separately and cannot be overwritten when downloading a package.\
    \ However, if a module has the same name as a standard library module, it can\
    \ confuse the interpreter and cause it to import the wrong module. For example,\
    \ the project FibexConverter [\\[23\\]](#page-11-34) has a module named parser.py,\
    \ which conflicts with the standard library module parser. This leads to an issue\
    \ where the program imports the parser module from the standard library instead\
    \ of the parser.py module from the project. This is because Python's import mechanism\
    \ first searches for modules already imported in sys.modules, which contains a\
    \ cache of modules pre-recorded when the Python interpreter starts up, such as\
    \ os, abc, etc. As a result, modules with names identical to these standard library\
    \ modules are not properly imported.\n\nFor module-to-TPL conflicts, Python searches\
    \ for modules in the order of the paths in sys.path and stops at the first match\
    \ it finds. When conflicting modules are downloaded and located in different locations,\
    \ this requires the developer to be very experienced and carefully set the order\
    \ of sys.path to handle conflicts. Note that while the namespace package handles\
    \ modules overwriting, it does not address the importing confusion threats caused\
    \ by the import prioritization. Moreover, Python also supports various ways to\
    \ install packages, which often have different default paths. For example, one\
    \ can use apt-get to install packages in /usr/, pip to install packages in site-packages/,\
    \ or other tools such as conda [\\[8\\]](#page-11-35)\n\nTable 4: Top 10 conflict\
    \ modules in packages.\n\n<span id=\"page-7-0\"></span>\n\n| Module paths    \
    \                      | # of latest pkgs | # of all pkgs |\n|---------------------------------------|------------------|---------------|\n\
    | src/__init__.py                       | 1,157            | 8,777         |\n\
    | __init__.py                           | 1,083            | 4,421         |\n\
    | utils/__init__.py                     | 410              | 3,899         |\n\
    | distributions/__init__.py             | 404              | 448           |\n\
    | distributions/Generaldistribution.py  | 394              | 431           |\n\
    | distributions/Gaussiandistribution.py | 394              | 431           |\n\
    | distributions/Binomialdistribution.py | 393              | 428           |\n\
    | client/__init__.py                    | 367              | 1,142         |\n\
    | scripts/__init__.py                   | 363              | 5,336         |\n\
    | server/__init__.py                    | 360              | 796           |\n\
    \nand poetry [\\[30\\]](#page-11-36) that have their own default installation\
    \ paths. This greatly increases the threat of importing confusion.\n\n#### 5.2\
    \ RQ2: PyPI Packages Study\n\nWe use ModuleGuard to conduct a large-scale study\
    \ of module conflicts (MCs) in the PyPI ecosystem. We study three types of MC\
    \ patterns for all 4.2 million PyPI packages as of March 2023. For module-to-TPL\
    \ conflict, we only consider the latest version packages for each project as of\
    \ March 2023, since different versions of the same project cannot coexist in a\
    \ local environment and pip will install the latest version by default unless\
    \ specified constraints otherwise. We identify packages that have module conflicts,\
    \ and make the assumption that these packages will be installed at the same time.\
    \ For module-to-Lib conflict, we first collect 199 standard library module names\
    \ from the Python official documentation [\\[16\\]](#page-11-37). Then we analyze\
    \ the module names used by all the packages in the ecosystem. It's worth noting\
    \ that we cannot know the order of sys.path or the standard library in the users'\
    \ environment. Therefore, we also assume that the users have 199 standard libraries\
    \ available locally, all of which are loaded into the cache. For modulein-Dep\
    \ conflict, we consider all version packages for each project and resolve their\
    \ dependency graphs with EnvResolution. For the nodes in the resolved dependency\
    \ graphs, we check whether their modules have conflicts.\n\nWe extract 177,216,363\
    \ modules and 27,678,668 direct dependencies for 4,223,950 packages from PyPI\
    \ as of March 2023 and resolve 4,223,950 dependency graphs. This includes 424,823\
    \ latest version packages with 5,419,306 modules.\n\nModule-to-TPL conflict. We\
    \ use the latest version packages of 424,408 projects as of 2023 March to study\
    \ module-to-TPL conflicts. We find that 91,134 (21.45%) packages have module-to-TPL\
    \ conflicts, affecting 386,595 (7.13% out of 5,419,306) module paths. These packages\
    \ may have module overwriting or importing confusion threats depending on whether\
    \ they are installed in the same or different locations. Moreover, 27,851 (6.56%)\
    \ packages may have an overwriting impact in a Windows environment, involving\
    \ 3,517 module paths.\n\nFindings. We observe that developers often package redundant\
    \ modules that are not needed for runtime, such as testing modules (e.g., 41,095\
    \ packages have test(s)/\\_\\_init\\_\\_.py,) and example modules (e.g., 14,877\
    \ packages have example(s)/\\_\\_init\\_\\_.py). These modules are only for the\
    \ development process and are more\n\n<span id=\"page-8-0\"></span>![](_page_8_Figure_1.jpeg)\n\
    \nFigure 4: Statistics of the number of packages released and the number of conflict\
    \ packages in each year.\n\nerror-prone and confused [\\[34\\]](#page-11-38).\
    \ They not only increase the storage pressure on the PyPI server, but also slow\
    \ down the efficiency of pip resolution due to the backtracking algorithm.\n\n\
    Furthermore, we identify the top 10 most common module paths in software packages\
    \ as shown in Table [4.](#page-7-0) There are over 1000 packages that include\
    \ src/\\_\\_init\\_\\_.py and \\_\\_init\\_\\_.py, which are the result of the\
    \ misconfiguration of the src-layout and flat-layout format packages [\\[34\\\
    ]](#page-11-38), respectively. These two modules are stored in the project root\
    \ directory without any meaning or functionality.\n\nAdditionally, we find that\
    \ packages with conflicting modules often have similar names, which reflect their\
    \ functionality. For example, out of the 404 packages that have the distributions/\\\
    _\\_init\\_\\_.py module, 290 contain the substring 'distribution' in their project\
    \ name. This means that conflicting packages are more likely to be installed together,\
    \ because they most likely belong to the same domain or have the same functionality.\n\
    \nIn addition, through exploring related GitHub Issues, we find that project maintainers\
    \ who have the same module name are often reluctant to change their own module\
    \ name. Changing the module name will not only break forward compatibility, but\
    \ also the workload is very large, and increase the learning cost of users when\
    \ used.\n\nModule-to-Lib conflict. We analyzed the entire ecosystem of 4.2 million\
    \ packages and found that 345,068 (8.17%) packages have module-to-Lib conflicts,\
    \ which may cause import errors at runtime. Moreover, we discovered that 182 (91.96%)\
    \ out of 199 standard library modules are affected by these conflicts. The most\
    \ frequently used standard library module names that conflict with third-party\
    \ packages are types, io, and logging, which are used by 69,940, 47,214, and 35,694\
    \ packages, respectively. These results suggest that developers should be careful\
    \ when choosing module names for their packages and avoid using names that already\
    \ exist in the standard library.\n\nFindings. We also observed a gap between the\
    \ local development and the deployment environments that can lead to import confusion\
    \ issues. When a program is developed locally, the current working directory has\
    \ a higher priority than the standard library modules in sys.path. However, when\
    \ the program is packaged and\n\ninstalled by others from PyPI, the site-packages\
    \ directory has a lower priority than the standard library modules in sys.path.\
    \ This gap can result in unexpected runtime errors due to importing wrong modules.\
    \ To address this problem, developers have two options: changing the module name\
    \ or using relative path import. However, the two solutions may break backward\
    \ compatibility and reduce the readability or portability of the code.\n\nIn addition,\
    \ we notice that the number of Module-to-Lib conflict packages increased each\
    \ year. To further illustrate this trend, we plot the number and percentage of\
    \ Module-to-Lib conflict packages for each year from 2005 to 2022. As shown in\
    \ the Figure [4,](#page-8-0) both the number and percentage increased steadily\
    \ over the years, indicating that this threat became more prevalent and severe\
    \ as the Python ecosystem grew and the extensions to the standard library. This\
    \ suggests that developers do not pay enough attention to the potential conflicts\
    \ with the standard library modules when naming their modules, or they are unaware\
    \ of the existing or newly added standard library modules that might conflict\
    \ with their modules.\n\nModule-in-Dep conflict. We conducted an empirical study\
    \ on the entire ecosystem of 4.2 million packages and detected 129,840 (3.07%)\
    \ packages with module-in-Dep conflicts, involving 11,666 projects. we also find\
    \ 38,371 packages involving 4,516 projects that exhibit different module file\
    \ contents but the same paths, which may cause functionality errors. Moreover,\
    \ we noticed that some conflicting modules may change their contents after package\
    \ updates, which could introduce new problems in the future. Although these conflicting\
    \ modules may not be invoked at runtime, they do have the effect of module overwriting,\
    \ which compromises the integrity of packages. There is also no guarantee that\
    \ these modules will not be called and used in a future version.\n\nFindings.\
    \ We further analyzed the characteristics of the conflicting packages. First,\
    \ two packages from different maintainers that provide similar functionalities\
    \ often use the same or similar module names. This is because they tend to copy\
    \ from each other, thus avoiding unnecessary duplication of the wheel or convenient\
    \ naming. Second, two packages related by migration often result in a conflict,\
    \ where one package is deprecated and replaced by another package. Third, two\
    \ packages that are different incompatible versions or variants of the same project.\
    \ For example, in the dependency graph of saleor, python-magic-bin is a fork of\
    \ python-magic with a different maintainer; in the dependency graph of riffusion,\
    \ soundfile is a migrated version of pysoundfile; and opencv-python and opencv-python-headless\
    \ are two distributions of opencv for different environments.\n\nWe also observed\
    \ that these conflicts often occurred either in older continuous versions (2,342\
    \ out of 4,516) or in all versions (1,819 out of 4,516) of a project. This indicates\
    \ that some developers or users discovered and resolved some conflicts when they\
    \ encountered functionality issues, while others did not notice or update their\
    \ dependencies. This implies that module-in-Dep conflicts have a certain persistence\
    \ and concealment, which may affect the reliability of Python applications. For\
    \ example, the project aniposelib used opencv-python and opencv-contrib-python\
    \ dependencies prior to version 0.3.7, which was fixed by maintainers in a later\
    \ version due to bugs raised in issue [\\[22\\]](#page-11-39) caused by Module-in-Dep\
    \ conflicts. What's more, such conflicts exist in an average of 6.5 versions.\
    \ Such a large time gap can affect the functionality and maintainability\n\nof\
    \ the project. Therefore, we argue that it is important to detect and prevent\
    \ module-in-Dep conflicts in Python packages to ensure correct functionality.\n\
    \nFrom the time dimension, the number of packages with Modulein-Dep conflicts\
    \ also gradually increases over time, as shown in Figure [4.](#page-8-0) Many\
    \ older packages that didn't have conflicts before are coming back into conflict\
    \ as dependencies are migrated.\n\n#### 5.3 RQ3: GitHub Projects Study\n\nWe select\
    \ popular Python projects from GitHub. We collect the top 3,000 most-starred Python\
    \ projects and 1,187 popular projects from awesome-Python[\\[11\\]](#page-11-40).\
    \ We merge and deduplicate the two datasets and obtain a total of 3,711 projects\
    \ with 93,487 tags. We analyze their dependencies, resolve dependency graphs with\
    \ EnvResolution, and detect module-in-Dep conflicts for them.\n\nWe detect 519\
    \ (13.93%) projects with 10,850 (11.61%) tags that have module overwriting threats.\
    \ The results show that modulein-Dep conflicts are more prevalent in GitHub projects,\
    \ as these projects tend to declare more dependencies than packages on PyPI. Although\
    \ these conflicting modules may not affect the functionality of the program if\
    \ the file contents are unchanged before and after overwriting, they can break\
    \ the integrity of the package in the local environment and cause errors that\
    \ are hard to debug. Moreover, there are 2,569 tags for 108 projects that may\
    \ have functional errors, due to the difference in file contents before and after\
    \ overwriting. Of the 108 projects, 65 are the latest version, while 43 projects\
    \ have fixed module-in-Dep issues in later versions. This means that the module\
    \ conflict problem is latent, with an average of 23 historical versions affected.\
    \ It is often only when a user encounters an error that the maintainer becomes\
    \ aware of the problem and fixes it. We manually analyze the conflicting modules\
    \ in these 65 projects and report 35 issues to the project developers, of which\
    \ 11 projects replied and 12 fixed the MC problems. The others do not respond,\
    \ but since they have the same conflicting modules as the confirmed issues, we\
    \ can assume that they have a real impact.\n\nFindings. We find that module conflicts\
    \ occur more often in the AI field. This is because developers need to introduce\
    \ one of the four opencv-python base packages when adding dependencies, along\
    \ with other related AI projects. However, other related AI projects may also\
    \ introduce other incompatible versions of the base package. These base packages\
    \ are stated in the official documentation [\\[13\\]](#page-11-32) that they cannot\
    \ coexist because they all use the module name of cv2. This behavior is beyond\
    \ the developer's control because they can only control direct dependencies, and\
    \ the indirect dependencies are a black box. Such conflicts result in incompatibility\
    \ between different AI projects when they are used together.\n\nIn addition, we\
    \ find that some developers even include the same functional dependency in the\
    \ direct dependency, and the two dependencies have module conflicts. Talking to\
    \ the developers, they say that adding a dependency when they encountered an error\
    \ could fix a strange error (which was actually caused by module overwriting).\
    \ This means that developers tend to focus more on whether the program can run\
    \ properly, and introduce functionally redundant dependencies, which not only\
    \ increase the complexity of the project, but also increase the difficulty of\
    \ building the project environment. To make matters worse, project issues reveal\
    \ that\n\nmany developers are not aware of module conflicts. They often add or\
    \ remove dependencies after getting an error report to keep the program working.\
    \ Of the 12 latest tags that were fixed, 10 were fixed by removing redundant dependencies.\
    \ Therefore, our work reveals the nature and potential impact of module conflicts,\
    \ and helps them to recognize and correctly declare dependencies to mitigate conflicts\
    \ during the debugging phase.\n\n#### <span id=\"page-9-0\"></span>6 LIMITATIONS\n\
    \nModuleGuard can help developers detect potential module conflict threats during\
    \ the development and packaging phase, thus helping them properly configure packaging\
    \ scripts, but it faces the following threats.\n\nFirst, in terms of detecting\
    \ module-in-Dep conflicts, Module-Guard is responsible for detecting whether there\
    \ are conflict modules in the dependencies declared by the project, which will\
    \ definitely cause module overwriting and destroy the integrity of packages installed\
    \ in the local environment. However, environmental damage does not necessarily\
    \ affect the running of the program, because they may not have been imported or\
    \ the files are empty (e.g., empty \\_\\_init\\_\\_.py file). For the sake of\
    \ efficiency, our work did not take into account the call graph, which is time-consuming.\
    \ But we will analyze it as our future work to optimize ModuleGuard.\n\nSecond,\
    \ when it comes to detecting module-to-Lib conflicts, ModuleGuard doesn't know\
    \ which standard library packages are cached by the user's native runtime. Because\
    \ this involves the Python interpreter running time, Python version updates, etc.,\
    \ we have no way to collect relevant data from users due to privacy reasons. As\
    \ a result, ModuleGuard only collects and detects if the module names declared\
    \ in packages conflict with the names in the standard library as a reminder, and\
    \ we recommend that developers rename these modules that may conflict to prevent\
    \ potential problems.\n\n#### <span id=\"page-9-1\"></span>7 DISCUSSION\n\nWe\
    \ discuss from different perspectives: developers, PyPI maintainers, and future\
    \ works.\n\nDevelopers. For developers, when declaring dependencies, it is important\
    \ to be aware of the potential module conflicts between dependencies, especially\
    \ those that provide similar functionality. In addition, redundant dependencies\
    \ should be avoided to prevent some errors caused by module overwriting during\
    \ the installation process, which are often difficult to detect and debug. Furthermore,\
    \ when developers declare a dependency, they should pay attention to whether the\
    \ dependency is deprecated or migrated. Because these dependencies mean that there\
    \ is no subsequent maintenance. If a vulnerability is disclosed, there is no maintainer\
    \ to release a patch to fix it. We believe effective dependency management is\
    \ very important, as it not only reduces the likelihood of module conflicts, but\
    \ also ensures compatibility and functional integrity.\n\nPyPI maintainers. Although\
    \ Python provides the concept of namespace packages, the installation process\
    \ for namespace packages is similar to that of regular packages and can still\
    \ result in overwriting behavior. While the presence of \\_\\_init\\_\\_.py files\
    \ in the same namespace does not cause impacts, because there is no real reason\
    \ to call them, if there are other module conflicts between\n\ntwo packages within\
    \ the same namespace, it can still lead to problems. In addition, we think pip\
    \ should isolate third-party packages from each other when installing them, like\
    \ other languages do, so that different third-party packages cannot interact with\
    \ each other. Moreover, pip should warn about overwriting when appropriate, rather\
    \ than overwriting by default, which can cause developers to destroy the local\
    \ development environment without their notice. Python should provide a mechanism\
    \ to select modules from third-party packages to be imported, instead of importing\
    \ an entire module or importing some features from a module while missing the\
    \ concept of packages.\n\nFuture work. We see two main directions for future work.\
    \ First, existing developers lack analysis tools for their own dependencies and\
    \ project profiles. This is mainly because we found that developers pack redundant\
    \ files into packages and upload them to PyPI, or project developers declare redundant\
    \ dependencies. Second, because the current tools of ModuleGuard do not take into\
    \ account the actual order of installation and whether the overwritten modules\
    \ are actually imported, there is a certain false positive. However, these false\
    \ positive examples do have overwriting problems, which we summarized in the issues\
    \ study. Moreover, considering the efficiency of large-scale analysis, ModuleGuard\
    \ does not use the project's call graph to analyze whether the conflicting modules\
    \ in Python projects have real import behavior. Therefore, we will make up for\
    \ this deficiency in future work, so as to improve the accuracy of the tool.\n\
    \n# <span id=\"page-10-3\"></span>8 RELATED WORK\n\nThe most relevant works to\
    \ this paper are PyCRE [\\[7\\]](#page-10-0), Pipreq [\\[38\\]](#page-11-41),\
    \ PyEGo [\\[49\\]](#page-11-6), SnifferDog [\\[45\\]](#page-11-7), and PyDFix\
    \ [\\[27\\]](#page-11-42). PyCRE [\\[7\\]](#page-10-0) and PyD-Fix [\\[27\\]](#page-11-42)\
    \ aim to solve the problem that some projects on PyPI cannot be installed properly\
    \ due to environmental compatibility issues. PyCRE generates project dependencies\
    \ through static analysis of calls between modules to form a domain knowledge\
    \ graph, while PyDFix dynamically obtains the log content of installation failures\
    \ to judge dependency errors and fixes them through continuous patching. However,\
    \ our work focuses on the module conflicts that occur during and after the installation\
    \ of PyPI projects, assuming that the installation process is successful. In addition,\
    \ in terms of module extraction, PyCRE not only does not notice that the module\
    \ path will change before and after package installation, but also does not notice\
    \ that modules with the same import statement may belong to different packages.\n\
    \nPipreq is a tool that generates the requirement.txt from the project source\
    \ code. It treats modules and packages as one-to-one mappings, and doesn't deal\
    \ with the case where different packages can contain the same module path. PyEGo\
    \ [\\[49\\]](#page-11-6) is a tool that automatically infers the dependencies\
    \ of third-party packages, the Python interpreter, and system libraries at compatible\
    \ versions for Python programs. It only extracts module paths from the metadata\
    \ file. For cases where different packages can contain the same module path, it\
    \ considers the conflicting module to belong to the most popular package. Vu et\
    \ al. [\\[43\\]](#page-11-43) study the possible attack vectors in the Python\
    \ ecosystem, but do not delve into the MC problem. SnifferDog [\\[45\\]](#page-11-7)\
    \ fixes the environment of the Jupyter Notebook project using module information\
    \ and dependency information. It\n\nalso uses a one-to-one mapping, so it doesn't\
    \ notice the module conflict problem either.\n\nOn the contrary, our work mainly\
    \ analyzes the impact of MC problem in the ecosystem on a large scale, which is\
    \ not involved in the above work. They either did not cover the module extraction\
    \ process, or simply took the module information from the file (low accuracy),\
    \ and did not pay attention to the complex problem of mapping the module before\
    \ and after installation. For the dependency resolution part, they use their own\
    \ specific algorithm for resolution, but the resolution rules are different from\
    \ the resolution rules used in the actual pip installation. In general, we noticed\
    \ some aspects in the module aspect that others had not noticed before, and used\
    \ pip-compatible algorithms in dependency resolution with higher accuracy.\n\n\
    #### <span id=\"page-10-4\"></span>9 CONCLUSION\n\nThis paper makes a systematic\
    \ empirical study of module conflicts in Python. We implemented a tool called\
    \ ModuleGuard. It parses module information and dependency information from the\
    \ PyPI project and, in turn, detects three types of module conflicts. We used\
    \ ModuleGuard to detect 4.2 million version packages and 3,711 popular GitHub\
    \ projects. We identified 108 GitHub projects with module-in-Dep conflicts and\
    \ reported issues to them and we get 12 fixed and good feedback. All experimental\
    \ data in this paper are available at [https://sites.google.com/view/moduleguard.](https://sites.google.com/view/moduleguard)\n\
    \n#### ACKNOWLEDGMENTS\n\nThe authors would like to thank all reviewers sincerely\
    \ for their valuable comments. This work is partially supported by the National\
    \ Key R&D Program of China (2022YFB3103900). It is also supported by the National\
    \ Research Foundation, Singapore, and DSO National Laboratories under the AI Singapore\
    \ Programme (AISG Award No: AISG2-GC-2023-008). It is also supported by the National\
    \ Research Foundation, Singapore, and the Cyber Security Agency under its National\
    \ Cybersecurity R&D Programme (NCRP25-P04-TAICeN) and the NRF Investigatorship\
    \ NRF-NRFI06-2020-0001. Any opinions, findings, conclusions, or recommendations\
    \ expressed in this material are those of the author(s) and do not reflect the\
    \ views of the National Research Foundation, Singapore, and the Cyber Security\
    \ Agency of Singapore.\n\n#### REFERENCES\n\n- <span id=\"page-10-6\"></span>[1]\
    \ Adafruit-Blinka. 2023. Retrieved March 10, 2023 from [https://pypi.org/project/](https://pypi.org/project/Adafruit-Blinka/)\
    \ [Adafruit-Blinka/](https://pypi.org/project/Adafruit-Blinka/)\n- <span id=\"\
    page-10-8\"></span>[2] albumentations team. 2022. Retrieved March 10, 2023 from\
    \ [https://github.com/](https://github.com/albumentations-team/albumentations/issues/841)\
    \ [albumentations-team/albumentations/issues/841](https://github.com/albumentations-team/albumentations/issues/841)\n\
    - <span id=\"page-10-2\"></span>[3] Mahmoud Alfadel, Diego Elias Costa, and Emad\
    \ Shihab. 2021. Empirical Analysis of Security Vulnerabilities in Python Packages.\
    \ In 2021 IEEE International Conference on Software Analysis, Evolution and Reengineering\
    \ (SANER). 446–457. <https://doi.org/10.1109/SANER50967.2021.00048>\n- <span id=\"\
    page-10-7\"></span>[4] bandersnatch developers. 2022. bandersnatch. Retrieved\
    \ March 10, 2023 from <https://bandersnatch.readthedocs.io/en/latest/>\n- <span\
    \ id=\"page-10-5\"></span>[5] board. 2023. Retrieved March 10, 2023 from<https://pypi.org/project/board/>\n\
    - <span id=\"page-10-1\"></span>[6] Yulu Cao, Lin Chen, Wanwangying Ma, Yanhui\
    \ Li, Yuming Zhou, and Linzhang Wang. 2022. Towards Better Dependency Management:\
    \ A First Look At Dependency Smells in Python Projects. IEEE Transactions on Software\
    \ Engineering (2022), 1–26.<https://doi.org/10.1109/TSE.2022.3191353>\n- <span\
    \ id=\"page-10-0\"></span>[7] Wei Cheng, Xiangrong Zhu, and Wei Hu. 2022. Conflict-Aware\
    \ Inference of Python Compatible Runtime Environments with Domain Knowledge Graph.\
    \ In Proceedings of the 44th International Conference on Software Engineering\
    \ (Pittsburgh, Pennsylvania) (ICSE '22). Association for Computing Machinery,\
    \ New\n\n<span id=\"page-11-0\"></span>York, NY, USA, 451–461.<https://doi.org/10.1145/3510003.3510078>\n\
    \n- <span id=\"page-11-35\"></span>[8] conda. 2023. . Retrieved January 10, 2023\
    \ from<https://conda.io/>\n- <span id=\"page-11-4\"></span>[9] crates.io. 2022.\
    \ cargo. Retrieved March 10, 2023 from<https://crates.io/>\n- <span id=\"page-11-12\"\
    ></span>[10] Ruian Duan, Omar Alrawi, Ranjita Pai Kasturi, Ryan Elder, Brendan\
    \ Saltaformaggio, and Wenke Lee. 2020. Towards measuring supply chain attacks\
    \ on package managers for interpreted languages. arXiv preprint arXiv:2002.01139\
    \ (2020).\n- <span id=\"page-11-40\"></span>[11] dylanhogg. 2022. Python Awesome\
    \ Project. Retrieved March 10, 2023 from <https://awesomepython.org/>\n- <span\
    \ id=\"page-11-21\"></span>[12] Python Software Foundation. 2022. api-reference.\
    \ Retrieved March 10, 2023 from<https://warehouse.pypa.io/api-reference/xml-rpc.html>\n\
    - <span id=\"page-11-32\"></span>[13] Python Software Foundation. 2022. opencv-python.\
    \ Retrieved March 10, 2023 from<https://pypi.org/project/opencv-python/>\n- <span\
    \ id=\"page-11-22\"></span>[14] Python Software Foundation. 2022. Python Dependency\
    \ Specifiers. Retrieved March 10, 2023 from [https://packaging.python.org/en/latest/specifications/](https://packaging.python.org/en/latest/specifications/dependency-specifiers/)\
    \ [dependency-specifiers/](https://packaging.python.org/en/latest/specifications/dependency-specifiers/)\n\
    - <span id=\"page-11-18\"></span>[15] Python Software Foundation. 2022. Python\
    \ Documentation. Retrieved March 10, 2023 from<https://docs.python.org/3/>\n-\
    \ <span id=\"page-11-37\"></span>[16] Python Software Foundation. 2022. Python\
    \ Standard Libraries Documentation. Retrieved March 10, 2023 from<https://docs.python.org/3.10/library/index.html>\n\
    - <span id=\"page-11-19\"></span>[17] Google. 2022. deps.dev. Retrieved March\
    \ 10, 2023 from<https://deps.dev/>\n- <span id=\"page-11-25\"></span>[18] Google.\
    \ 2022. PyPI downloads table. Retrieved March 10, 2023 from [https:](https://bigquery.cloud.google.com/table/bigquery-public-data:pypi.downloads)\
    \ [//bigquery.cloud.google.com/table/bigquery-public-data:pypi.downloads](https://bigquery.cloud.google.com/table/bigquery-public-data:pypi.downloads)\n\
    - <span id=\"page-11-13\"></span>[19] Wenbo Guo, Zhengzi Xu, Chengwei Liu, Cheng\
    \ Huang, Yong Fang, and Yang Liu. 2023. An Empirical Study of Malicious Code In\
    \ PyPI Ecosystem. In 2023 38th IEEE/ACM International Conference on Automated\
    \ Software Engineering (ASE). IEEE, 166–177.\n- <span id=\"page-11-8\"></span>[20]\
    \ Eric Horton and Chris Parnin. 2019. Dockerizeme: Automatic inference of environment\
    \ dependencies for python code snippets. In 2019 IEEE/ACM 41st International Conference\
    \ on Software Engineering (ICSE). IEEE, 328–338.\n- <span id=\"page-11-26\"></span>[21]\
    \ Piergiorgio Ladisa, Henrik Plate, Matias Martinez, and Olivier Barais. 2022.\
    \ Taxonomy of attacks on open-source software supply chains. arXiv preprint arXiv:2204.04008\
    \ (2022).\n- <span id=\"page-11-39\"></span>[22] lambdaloop. 2022. aniposelib.\
    \ Retrieved March 10, 2023 from [https://github.](https://github.com/lambdaloop/anipose/issues/22)\
    \ [com/lambdaloop/anipose/issues/22](https://github.com/lambdaloop/anipose/issues/22)\n\
    - <span id=\"page-11-34\"></span>[23] LarsVoelker. 2022. Retrieved March 10, 2023\
    \ from [https://github.com/](https://github.com/LarsVoelker/FibexConverter/issues/7)\
    \ [LarsVoelker/FibexConverter/issues/7](https://github.com/LarsVoelker/FibexConverter/issues/7)\n\
    - <span id=\"page-11-9\"></span>[24] Shuo Li. [n. d.]. EasyPip: Detect and Fix\
    \ Dependency Problems in Python Dependency Declaration Files. ([n. d.]).\n- <span\
    \ id=\"page-11-24\"></span>[25] Libraries.io. 2022. Libraries.io Query. Retrieved\
    \ March 10, 2023 from [https:](https://libraries.io/search?order=desc&platforms=PyPI)\
    \ [//libraries.io/search?order=desc&platforms=PyPI](https://libraries.io/search?order=desc&platforms=PyPI)\n\
    - <span id=\"page-11-3\"></span>[26] Maven. 2021. Maven – Guide to Naming Conventions.\
    \ [https://maven.apache.](https://maven.apache.org/guides/mini/guide-naming-conventions.html)\
    \ [org/guides/mini/guide-naming-conventions.html](https://maven.apache.org/guides/mini/guide-naming-conventions.html)\
    \ Accessed: 2022-01-19.\n- <span id=\"page-11-42\"></span>[27] Suchita Mukherjee,\
    \ Abigail Almanza, and Cindy Rubio-González. 2021. Fixing dependency errors for\
    \ Python build reproducibility. In Proceedings of the 30th ACM SIGSOFT international\
    \ symposium on software testing and analysis. 439–451.\n- <span id=\"page-11-28\"\
    ></span>[28] opendatateam. 2022. Cookiecutter-udata-plugin issue. Retrieved March\
    \ 10, 2023 from<https://github.com/opendatateam/cookiecutter-udata-plugin/issues/3>\n\
    - <span id=\"page-11-5\"></span>[29] pip. 2023. pip documentation v22.3.1. Retrieved\
    \ March 10, 2023 from [https:](https://pip.pypa.io/) [//pip.pypa.io/](https://pip.pypa.io/)\n\
    - <span id=\"page-11-36\"></span>[30] poetry. 2023. . Retrieved January 10, 2023\
    \ from [https://python-poetry.org/docs/](https://python-poetry.org/docs/repositories/)\
    \ [repositories/](https://python-poetry.org/docs/repositories/)\n- <span id=\"\
    page-11-31\"></span>[31] pycrypto. 2022. Pycrypto issue. Retrieved March 10, 2023\
    \ from [https://github.](https://github.com/pycrypto/pycrypto/issues/156) [com/pycrypto/pycrypto/issues/156](https://github.com/pycrypto/pycrypto/issues/156)\n\
    - <span id=\"page-11-30\"></span>[32] pypa. 2022. Retrieved March 10, 2023 from\
    \ [https://github.com/pypa/pip/issues/](https://github.com/pypa/pip/issues/4625)\
    \ [4625](https://github.com/pypa/pip/issues/4625)\n- <span id=\"page-11-33\"></span>[33]\
    \ pypa. 2022. Retrieved March 10, 2023 from [https://github.com/pypa/pip/issues/](https://github.com/pypa/pip/issues/8509)\
    \ [8509](https://github.com/pypa/pip/issues/8509)\n- <span id=\"page-11-38\"></span>[34]\
    \ pypa. 2023. Package Discovery and Namespace Packages. Retrieved January 10,\
    \ 2023 fro[m https://setuptools.pypa.io/en/latest/userguide/package\\\\_discovery.html](https://setuptools.pypa.io/en/latest/userguide/package_discovery.html)\n\
    - <span id=\"page-11-29\"></span>[35] radekd91. 2022. Emoca issue. Retrieved March\
    \ 10, 2023 from [https://github.com/](https://github.com/radekd91/emoca/issues/44)\
    \ [radekd91/emoca/issues/44](https://github.com/radekd91/emoca/issues/44)\n- <span\
    \ id=\"page-11-14\"></span>[36] Jukka Ruohonen, Kalle Hjerppe, and Kalle Rindell.\
    \ 2021. A Large-Scale Security-Oriented Static Analysis of Python Packages in\
    \ PyPI. In 2021 18th International Conference on Privacy, Security and Trust (PST).\
    \ IEEE, 1–10.\n- <span id=\"page-11-23\"></span>[37] sarugaku. 2022. resolvelib.\
    \ Retrieved March 10, 2023 from [https://github.com/](https://github.com/sarugaku/resolvelib)\
    \ [sarugaku/resolvelib](https://github.com/sarugaku/resolvelib)\n- <span id=\"\
    page-11-41\"></span>[38] Jessamyn Smith. 2023. pipreq. Retrieved March 10, 2023\
    \ from [https://github.](https://github.com/bndr/pipreqs/) [com/bndr/pipreqs/](https://github.com/bndr/pipreqs/)\n\
    - <span id=\"page-11-2\"></span>[39] Sonatype. 2021. State of the 2021 Software\
    \ Supply Chain. Sonatype Blog (2021). <https://www.sonatype.com/blog/software-supply-chain-2021>\n\
    - <span id=\"page-11-20\"></span>[40] TIDELIFT. 2022. Libraries.io. Retrieved\
    \ March 10, 2023 from<https://libraries.io/>\n- <span id=\"page-11-17\"></span>[41]\
    \ virtualenv [n. d.]. Retrieved March 10, 2023 from<https://virtualenv.pypa.io/>\n\
    - <span id=\"page-11-15\"></span>[42] Duc-Ly Vu, Ivan Pashchenko, Fabio Massacci,\
    \ Henrik Plate, and Antonino Sabetta. 2020. Typosquatting and combosquatting attacks\
    \ on the python ecosystem. In 2020 IEEE European Symposium on Security and Privacy\
    \ Workshops. IEEE, 509–514.\n- <span id=\"page-11-43\"></span>[43] Duc-Ly Vu,\
    \ Ivan Pashchenko, Fabio Massacci, Henrik Plate, and Antonino Sabetta. 2020. Typosquatting\
    \ and Combosquatting Attacks on the Python Ecosystem. In 2020 IEEE European Symposium\
    \ on Security and Privacy Workshops. IEEE. <https://doi.org/10.1109/eurospw51379.2020.00074>\n\
    - <span id=\"page-11-10\"></span>[44] Chao Wang, Rongxin Wu, Haohao Song, Jiwu\
    \ Shu, and Guoqing Li. 2022. Smart-Pip: A Smart Approach to Resolving Python Dependency\
    \ Conflict Issues. IEEE Transactions on Software Engineering (2022).\n- <span\
    \ id=\"page-11-7\"></span>[45] Jiawei Wang, Li Li, and Andreas Zeller. 2021. Restoring\
    \ execution environments of Jupyter notebooks. In 2021 IEEE/ACM 43rd International\
    \ Conference on Software Engineering (ICSE). IEEE, 1622–1633.\n- <span id=\"page-11-11\"\
    ></span>[46] Ying Wang, Ming Wen, Yepang Liu, Yibo Wang, Zhenming Li, Chao Wang,\
    \ Hai Yu, Shing-Chi Cheung, Chang Xu, and Zhiliang Zhu. 2020. Watchman: Monitoring\
    \ Dependency Conflicts for Python Library Ecosystem. In 2020 IEEE/ACM 42nd International\
    \ Conference on Software Engineering (ICSE). 125–135. [https://doi.](https://doi.org/10.1145/3377811.3380426)\
    \ [org/10.1145/3377811.3380426](https://doi.org/10.1145/3377811.3380426)\n- <span\
    \ id=\"page-11-1\"></span>[47] Wikipedia. 2023. Identifier. Retrieved March 10,\
    \ 2023 from [https://en.wikipedia.](https://en.wikipedia.org/wiki/Identifier#Implicit_context_and_namespace_conflicts)\
    \ [org/wiki/Identifier#Implicit\\\\_context\\\\_and\\\\_namespace\\\\_conflicts](https://en.wikipedia.org/wiki/Identifier#Implicit_context_and_namespace_conflicts)\n\
    - <span id=\"page-11-27\"></span>[48] wtsi hgi. 2022. Python-hgijson issue. Retrieved\
    \ March 10, 2023 from [https:](https://github.com/wtsi-hgi/python-hgijson/issues/14)\
    \ [//github.com/wtsi-hgi/python-hgijson/issues/14](https://github.com/wtsi-hgi/python-hgijson/issues/14)\n\
    - <span id=\"page-11-6\"></span>[49] Hongjie Ye, Wei Chen, Wensheng Dou, Guoquan\
    \ Wu, and Jun Wei. 2022. Knowledge-based environment dependency inference for\
    \ Python programs. In Proceedings of the 44th International Conference on Software\
    \ Engineering. 1245– 1256.\n- <span id=\"page-11-16\"></span>[50] ysr monitor.\
    \ 2023. Retrieved March 10, 2023 from [https://pypi.org/project/ysr](https://pypi.org/project/ysr-monitor/)[monitor/](https://pypi.org/project/ysr-monitor/)"
  decisions:
    language: '- Qualified. Reason: English Paper'
    evaluation_prompt: Qualified.
    related_work_prompt: Qualified
    novelty_prompt: Qualified
    review_only_prompt: Qualified
  llm_input_used: '## Abstract

    Python has become one of the most popular programming languages for software

    development due to its simplicity, readability, and versatility. As the Python

    ecosystem grows, developers face increasing challenges in avoiding module

    conflicts, which occur when different packages have the same namespace modules.

    Unfortunately, existing work has neither investigated the module conflict

    comprehensively nor provided tools to detect the conflict. Therefore, this

    paper systematically investigates the module conflict problem and its impact on

    the Python ecosystem. We propose a novel technique called InstSimulator, which

    leverages semantics and installation simulation to achieve accurate and

    efficient module extraction. Based on this, we implement a tool called

    ModuleGuard to detect module conflicts for the Python ecosystem. For the study,

    we first collect 97 MC issues, classify the characteristics and causes of these

    MC issues, summarize three different conflict patterns, and analyze their

    potential threats. Then, we conducted a large-scale analysis of the whole PyPI

    ecosystem (4.2 million packages) and GitHub popular projects (3,711 projects)

    to detect each MC pattern and analyze their potential impact. We discovered

    that module conflicts still impact numerous TPLs and GitHub projects. This is

    primarily due to developers'' lack of understanding of the modules within their

    direct dependencies, not to mention the modules of the transitive dependencies.

    Our work reveals Python''s shortcomings in handling naming conflicts and

    provides a tool and guidelines for developers to detect conflicts.


    ## Introduction

    Namespace conflicts are a ubiquitous challenge in the field of computer science.
    However, with the thriving development of the software supply chain in recent
    years, there has been explosive growth in the number of open-source software,
    making the issue of namespace conflicts more pressing [\[47\]](#page-11-1). According
    to the Sonatype report [\[39\]](#page-11-2), the number of open-source software
    represents a 20% year-over-year growth globally. As software systems become larger
    and more diverse, the likelihood of encountering namespace conflicts increases
    significantly.


    While namespace conflicts have long been a topic of discussion, different language
    ecosystems have taken different approaches to address namespace conflict issues.
    For instance, the Java ecosystem uses groupId, artifactId, and version to name
    an open-source package uniquely and extracts the package''s modules to a unique
    path formed by this triplet [\[26\]](#page-11-3). Rust isolates different open-source
    packages by placing them in separate folders [\[9\]](#page-11-4). Similarly, Python
    Package Index (PyPI) uses the project name as a unique identifier for an open-source
    package, but the module names within the package are not unique [\[29\]](#page-11-5).
    As a result, we have found that it poses some threats in handling naming conflicts.
    First, unlike other languages,


    <sup>∗</sup>Wenbo Shen is the corresponding author.


    Permission to make digital or hard copies of all or part of this work for personal
    or classroom use is granted without fee provided that copies are not made or distributed
    for profit or commercial advantage and that copies bear this notice and the full
    citation on the first page. Copyrights for components of this work owned by others
    than the author(s) must be honored. Abstracting with credit is permitted. To copy
    otherwise, or republish, to post on servers or to redistribute to lists, requires
    prior specific permission and/or a fee. Request permissions from permissions@acm.org.


    Python does not isolate each package that has been downloaded locally but instead
    installs them together by default. This results in an impact of module overwriting
    when two packages with conflicting module namespaces are installed simultaneously.


    Second, as an interpreted language, Python determines which specific module to
    import at runtime, unlike compiled languages that can report errors in advance.
    Hence, when importing packages at runtime, the existence of modules in different
    paths may lead to conflicts, posing a threat of import confusion. We refer to
    the conflicts caused by module namespaces as Module Conflicts (MC). These two
    threats of module conflicts are unique to the Python ecosystem since they are
    caused by Python''s specific mechanisms for handling namespace conflicts. These
    threats can cause the program to break locally existing third-party packages when
    installed, causing environmental damage that is difficult to repair. They can
    cause function errors in program execution, leading to bugs that are hard to trace
    and fix. Moreover, the MC issues can inhibit the normal update of packages, breaking
    their integrity.


    Existing work provides in-depth research and analysis on a number of issues specific
    to the Python ecosystem. Cheng et al. [\[7\]](#page-10-0) propose PyCRE, a new
    approach to automatically infer Pythoncompatible runtime environments with domain
    knowledge graphs. Ye et al. [\[49\]](#page-11-6) propose PyEGo, a knowledge-based
    technique that can automatically infer dependencies'' compatible versions for
    Python programs. Unfortunately, when dealing with module conflicts, they both
    simply consider the conflicting modules belonging to the most popular packages.
    Wang et al. [\[45\]](#page-11-7) present SnifferDog, an approach that can restore
    the execution environments of Jupyter notebooks. Garcia et al. [\[20\]](#page-11-8)
    present DockerizeMe, a tool that can generate Dockerfiles for Python projects.
    However, when dealing with module conflicts, they all use a limited number of
    module-to-package mappings, which do not realize conflicts. Other works primarily
    focus on dependency conflicts [\[7,](#page-10-0) [24,](#page-11-9) [44,](#page-11-10)
    [46\]](#page-11-11) that resolve different software components require incompatible
    versions of the same dependency, dependency diagnosis [\[6\]](#page-10-1) that
    fix incorrect dependency configurations, and detecting malicious packages [\[3,](#page-10-2)
    [10,](#page-11-12) [19,](#page-11-13) [36,](#page-11-14) [42\]](#page-11-15) for
    PyPI packages and dependencies. These works do not systematically address the
    MC problem or detect MC. They either ignore the existence of multiple packages
    that provide the same module or rely on incomplete module-to-package mappings
    that can not cover all possible scenarios.


    To fill this gap, this paper conducts a systematic study to investigate the module
    conflict problem and its impact on the Python ecosystem. To achieve the large-scale
    study, we first propose a novel technique named installation-free module extraction
    (InstSimulator in short). It extracts the semantics of different configuration
    files from multiple dimensions and then simulates the installation process according
    to the semantics to extract exact modules. Our evaluation shows that InstSimulator
    achieves over 95% accuracy and can complete the module extraction of all four
    million packages in 10 hours with 40 threads in parallel. Then, based on InstSimulator
    we implement a novel tool named ModuleGuard for the study.


    To conduct the study, we first collect 97 MC issues on GitHub and StackOverflow,
    classify the characteristics and causes of these MC issues, summarize three different
    conflict patterns, and analyze their potential threats. Next, using ModuleGuard,
    we conduct a largescale analysis of the whole PyPI ecosystem (4.2 million packages)


    and GitHub popular projects (3,711 projects) to detect each MC pattern and analyze
    their potential impact. We have discovered that there are still numerous open-source
    software packages in PyPI that are impacted by module conflicts among their dependencies.
    This is primarily due to developers lacking understanding of the modules within
    their direct and indirect dependencies. Our work not only reveals Python''s shortcomings
    in handling naming conflicts but also provides tools and guidelines for developers
    to detect conflicts when developing and debugging.


    To summarize, this paper makes the following contributions.


    - New study. We conduct a systematic study on module conflicts (MC) in the Python
    ecosystem. We conducted an issue study from GitHub and StackOverflow and summarized
    three MC patternsmodule-to-TPL, module-to-Lib, and module-in-Dep conflicts and
    their two potential threats.

    - New technique. We propose InstSimulator, which leverages the semantics and installation
    simulation to achieve accurate and efficient module extraction. Based on this,
    we implement a tool ModuleGuard to detect MCs for the Python ecosystem. We construct
    benchmarks for evaluating the capabilities of module information extraction and
    dependency graph resolution.

    - Ecosystem-scale analysis. Utilizing ModuleGuard, we conduct a large-scale study
    and analyze 4.2 million packages on PyPI (434,823 latest version packages as of
    April 2023). We get a lot of interesting findings, shed some light on the nature
    of module conflicts, and provide some guidance for developers.

    - Issue reporting. We examine 93,487 tags of 3,711 popular GitHub projects, of
    which 108 are now or ever affected by MC. We reported issues, and a lot of issues
    have been confirmed and fixed. This proves that our work can help developers understand
    previously unrealized errors and help them fix potential threats.


    This paper is organized as follows. We introduce background knowledge in [§2.](#page-1-0)
    In [§3,](#page-2-0) we propose our ModuleGuard tool and introduce it in detail.
    In [§4](#page-4-0) we evaluate ModuleGuard in different metrics. In [§5](#page-5-0)
    we conduct an ecosystem-scale study on MC, including issues study, GitHub projects,
    and PyPI packages. [§6](#page-9-0) and [§7](#page-9-1) present some limitations
    and discussion. Related work is described in [§8.](#page-10-3) Finally, we conclude
    the whole paper in [§9.](#page-10-4)'
  token_usage: 8950
  time_usage: 2.0916409492492676
- title: 'REDriver: Runtime Enforcement for Autonomous Vehicles'
  abstract: 'Autonomous driving systems (ADSs) integrate sensing, perception, drive

    control, and several other critical tasks in autonomous vehicles, motivating

    research into techniques for assessing their safety. While there are several

    approaches for testing and analysing them in high-fidelity simulators, ADSs may

    still encounter additional critical scenarios beyond those covered once they

    are deployed on real roads. An additional level of confidence can be

    established by monitoring and enforcing critical properties when the ADS is

    running. Existing work, however, is only able to monitor simple safety

    properties (e.g., avoidance of collisions) and is limited to blunt enforcement

    mechanisms such as hitting the emergency brakes. In this work, we propose

    REDriver, a general and modular approach to runtime enforcement, in which users

    can specify a broad range of properties (e.g., national traffic laws) in a

    specification language based on signal temporal logic (STL). REDriver monitors

    the planned trajectory of the ADS based on a quantitative semantics of STL, and

    uses a gradient-driven algorithm to repair the trajectory when a violation of

    the specification is likely. We implemented REDriver for two versions of Apollo

    (i.e., a popular ADS), and subjected it to a benchmark of violations of Chinese

    traffic laws. The results show that REDriver significantly improves Apollo''s

    conformance to the specification with minimal overhead.'
  url: http://arxiv.org/abs/2401.02253v1
  keywords: ''
  document: "# REDriver: Runtime Enforcement for Autonomous Vehicles\n\n[Yang Sun](https://orcid.org/0000-0002-2409-2160)\
    \ Singapore Management University Singapore yangsun.2020@phdcs.smu.edu.sg\n\n\
    [Xiaodong Zhang](https://orcid.org/0000-0002-8380-1019) Xidian University China\
    \ zhangxiaodong@xidian.edu.cn\n\n# ABSTRACT\n\nAutonomous driving systems (ADSs)\
    \ integrate sensing, perception, drive control, and several other critical tasks\
    \ in autonomous vehicles, motivating research into techniques for assessing their\
    \ safety. While there are several approaches for testing and analysing them in\
    \ high-fidelity simulators, ADSs may still encounter additional critical scenarios\
    \ beyond those covered once they are deployed on real roads. An additional level\
    \ of confidence can be established by monitoring and enforcing critical properties\
    \ when the ADS is running. Existing work, however, is only able to monitor simple\
    \ safety properties (e.g., avoidance of collisions) and is limited to blunt enforcement\
    \ mechanisms such as hitting the emergency brakes. In this work, we propose REDriver,\
    \ a general and modular approach to runtime enforcement, in which users can specify\
    \ a broad range of properties (e.g., national traffic laws) in a specification\
    \ language based on signal temporal logic (STL). REDriver monitors the planned\
    \ trajectory of the ADS based on a quantitative semantics of STL, and uses a gradient-driven\
    \ algorithm to repair the trajectory when a violation of the specification is\
    \ likely. We implemented REDriver for two versions of Apollo (i.e., a popular\
    \ ADS), and subjected it to a benchmark of violations of Chinese traffic laws.\
    \ The results show that REDriver significantly improves Apollo's conformance to\
    \ the specification with minimal overhead.\n\n#### ACM Reference Format:\n\nYang\
    \ Sun, Christopher M. Poskitt, Xiaodong Zhang, and Jun Sun. 2024. REDriver: Runtime\
    \ Enforcement for Autonomous Vehicles. In 2024 IEEE/ACM 46th International Conference\
    \ on Software Engineering (ICSE '24), April 14–20, 2024, Lisbon, Portugal. ACM,\
    \ New York, NY, USA, [12](#page-11-0) pages. [https://doi.org/](https://doi.org/10.1145/3597503.3639151)\
    \ [10.1145/3597503.3639151](https://doi.org/10.1145/3597503.3639151)\n\n# 1 INTRODUCTION\n\
    \nAutonomous driving systems (ADSs) are the core of autonomous vehicles (AVs),\
    \ integrating sensing, perception, drive control, and several other tasks that\
    \ are necessary for automating their journeys. Given the safety-critical nature\
    \ of ADSs [\\[14,](#page-10-0) [18\\]](#page-10-1), it is imperative that they\
    \ operate safely at all times, including in rare or unexpected scenarios that\
    \ may not have been explicitly considered when the\n\nICSE '24, April 14–20, 2024,\
    \ Lisbon, Portugal\n\n© 2024 Copyright held by the owner/author(s).\n\nACM ISBN\
    \ 979-8-4007-0217-4/24/04.\n\n<https://doi.org/10.1145/3597503.3639151>\n\n[Christopher\
    \ M. Poskitt](https://orcid.org/0000-0002-9376-2471) Singapore Management University\
    \ Singapore cposkitt@smu.edu.sg\n\n[Jun Sun](https://orcid.org/0000-0002-3545-1392)\
    \ Singapore Management University Singapore junsun@smu.edu.sg\n\nsystem was designed.\
    \ This has spurred a multitude of research into techniques for establishing confidence\
    \ in an ADS, e.g., by modelling and verifying aspects of its design [\\[23\\]](#page-10-2),\
    \ by subjecting it to reconstructions of real-world accidents [\\[6\\]](#page-10-3),\
    \ or by testing it against automatically generated critical scenarios [\\[27,](#page-10-4)\
    \ [44,](#page-11-1) [50\\]](#page-11-2) in a high-fidelity simulator such as CARLA\
    \ [\\[15\\]](#page-10-5) or LGSVL [\\[38\\]](#page-11-3).\n\nThese approaches\
    \ all analyse an ADS before it is deployed on real roads, where it may still encounter\
    \ additional scenarios beyond those that were covered. In fact, an analysis of\
    \ accidents involving autonomous vehicles [\\[31\\]](#page-11-4) suggests that\
    \ the broader implementation of current AV technologies may not lead to a reduction\
    \ in vehicle crash frequency. An additional level of confidence can thus be established\
    \ if desirable properties are also monitored—even enforced—while the ADS is running.\
    \ This is the idea of runtime enforcement, a technique that observes the execution\
    \ of a system and then modifies it in a minimal way to ensure certain properties\
    \ are satisfied. In AVs, runtime enforcement has been applied, for example, to\
    \ monitor basic safety properties such as the avoidance of collisions, applying\
    \ the emergency brake before they are violated [\\[21\\]](#page-10-6). Avoiding\
    \ collisions, however, is not enough in general. ADSs are expected to satisfy\
    \ a broader range of complicated properties concerning the overall traffic systems\
    \ they operate in, such as national traffic laws that describe how vehicles should\
    \ behave with respect to various junctions, signals, and (most precariously) other\
    \ vehicles or pedestrians. Currently, no existing approach supports runtime enforcement\
    \ of properties in this direction.\n\nIn this work, we aim to provide a general\
    \ solution to the runtime enforcement problem for AVs. In particular, we propose\
    \ REDriver, a general framework for runtime enforcement that can be integrated\
    \ into ADSs with state-of-the-art modular designs, as exhibited by Apollo [\\\
    [4\\]](#page-10-7) and Autoware [\\[2\\]](#page-10-8). REDriver allows users to\
    \ specify desirable properties of AVs using an existing and powerful domainspecific\
    \ language (DSL) based on signal temporal logic (STL). This language supports\
    \ properties ranging from the simplest, concerning collision avoidance, through\
    \ to entire formalisations of national traffic laws [\\[44\\]](#page-11-1). REDriver\
    \ monitors the planned trajectories and command sequences of the ADS at runtime\
    \ and assesses them against the user's specifications. If the AV is predicted\
    \ to potentially violate them in the near future (based on a quantitative semantics\
    \ of STL), REDriver repairs the trajectories using a gradient-driven algorithm.\
    \ Furthermore, it does so while minimising the \"overhead\" (or change) to the\
    \ original journey. That is, by efficiently computing the gradient of each signal\
    \ (with respect to the robustness degree\n\nPermission to make digital or hard\
    \ copies of part or all of this work for personal or classroom use is granted\
    \ without fee provided that copies are not made or distributed for profit or commercial\
    \ advantage and that copies bear this notice and the full citation on the first\
    \ page. Copyrights for third-party components of this work must be honored. For\
    \ all other uses, contact the owner/author(s).\n\nICSE '24, April 14–20, 2024,\
    \ Lisbon, Portugal Yang Sun, Christopher M. Poskitt, Xiaodong Zhang, and Jun Sun\n\
    \n<span id=\"page-1-0\"></span>![](_page_1_Figure_2.jpeg)\n\nFigure 1: The architecture\
    \ of an ADS with REDriver\n\n<span id=\"page-1-1\"></span>\n\n| Time | Position\
    \         | Speed | Acc   | Steer | Gear  |\n|------|------------------|-------|-------|-------|-------|\n\
    | 0    | (x: 0, y: 0)     | 7.01  | -0.05 | 0     | DRIVE |\n| 2    | (x: 0, y:\
    \ 13.34) | 6.13  | -0.48 | 0     | DRIVE |\n| 4    | (x: 0, y: 24.83) | 5.44 \
    \ | -0.24 | 0     | DRIVE |\n| 6    | (x: 0, y: 35.85) | 5.09  | -0.18 | 0   \
    \  | DRIVE |\n| 8    | (x: 0, y: 44.75) | 3.89  | -1.44 | 0     | DRIVE |\n\n\
    Table 1: An example planned trajectory\n\nof the STL formula), we identify and\
    \ modify the signal that is most likely to repair the trajectories.\n\nREDriver\
    \ has been implemented for two versions of Apollo (i.e., versions 6.0 and 7.0,\
    \ the latest at the time of experimentation). The implementation consists of a\
    \ plan validation algorithm and a control validation algorithm that respectively\
    \ observe and modify (if necessary) the outputs of the ADSs' motion planning and\
    \ control modules. Note that the motion planning and control modules are black\
    \ boxes to us. In particular, we enforce that these outputs (i.e., planned trajectories\
    \ and command sequences) do not lead to violations—whenever possible—of a comprehensive\
    \ formalisation of the Chinese traffic laws. This goes far beyond existing runtime\
    \ enforcement approaches, which focus on simple safety properties (e.g., collision\
    \ avoidance) and blunt enforcement mechanisms (e.g., hitting the emergency brakes).\
    \ Figure [1](#page-1-0) depicts how REDriver is integrated into the modular design\
    \ of Apollo. In particular, we have added two new modules while ensuring that\
    \ the existing modules and their inner logic remain unchanged. In the diagram,\
    \ the perception, motion planning, and control boxes represent the existing Apollo\
    \ modules, while the green plan validation and control validation boxes represent\
    \ the new modules from REDriver. The arrow denotes the flow of signal transmission.\
    \ We evaluated our implementation of REDriver against a benchmark of violation-inducing\
    \ scenarios for Chinese traffic laws [\\[44\\]](#page-11-1), finding that our\
    \ runtime enforcement approach significantly reduces the likelihood of those violations\
    \ occurring. Furthermore, REDriver's overhead in terms of time and how often it\
    \ intervenes is negligible.\n\n# 2 BACKGROUND AND PROBLEM\n\nIn this section,\
    \ we review the architecture of ADSs, the DSL for specifying safety properties,\
    \ and then define our problem.\n\n# 2.1 Overview of Autonomous Driving Systems\n\
    \nState-of-the-art open-source ADSs such as Apollo [\\[3\\]](#page-10-9) and Autoware\
    \ [\\[2\\]](#page-10-8) have similar architectures. They are typically organised\
    \ into loosely coupled modules that communicate via messagepassing. Three of these\
    \ modules are particularly relevant to our context, i.e., perception, motion planning,\
    \ and control.\n\nFirst, the perception module receives sensor readings (e.g.,\
    \ from a camera or LIDAR), processes them, and then feeds them to the\n\n<span\
    \ id=\"page-1-2\"></span>\n\n| Type | Time | Position            | Speed | Acc\
    \   | Steer  |\n|------|------|---------------------|-------|-------|--------|\n\
    |      | 0    | (x: 2.5, y: 5)      | 7.42  | -0.05 | -7.25  |\n| Car1 | 2   \
    \ | (x: 1.67, y: 18.34) | 6.37  | -0.48 | -11.10 |\n|      | 4    | (x: 0, y:\
    \ 29.88)    | 5.44  | -0.24 | 0      |\n|      | 6    | (x: 0, y: 40.87)    |\
    \ 5.09  | -0.18 | 0      |\n|      | 8    | (x: 0, y: 49.76)    | 3.89  | -1.44\
    \ | 0      |\n|      | 0    | (x: -2.5, y: 15)    | 0     | 0     | 0      |\n\
    | Car2 | 2    | (x: -2.5, y: 15)    | 0     | 0     | 0      |\n|      | 4   \
    \ | (x: -2.5, y: 15)    | 0     | 0     | 0      |\n|      | 6    | (x: -2.5,\
    \ y: 15)    | 0     | 0     | 0      |\n|      | 8    | (x: -2.5, y: 15)    |\
    \ 0     | 0     | 0      |\n\n (x: 0.23, y: 48) 0 0 0 (x: 0.23, y: 48) 0 0 0 (x:\
    \ 0.23, y: 48) 0 0 0 (x: 0.23, y: 48) 0 0 0 (x: 0.23, y: 48) 0 0 0\n\n GREEN False\
    \ – – YELLOW False – – YELLOW False – – YELLOW False – – RED False – –\n\nTL-ID\
    \ Time Color Blink – –\n\nPed1\n\nTL-0\n\nTable 2: An example predicted environment\n\
    \nmotion planning module. Second, the motion planning module generates a planned\
    \ trajectory based on the map, the destination, the sensor inputs, and the state\
    \ of the ego vehicle, i.e., the one under the control of the ADS. Intuitively,\
    \ the planned trajectory describes where the vehicle will be at future time points,\
    \ and is computed based on a predicted environment that includes, for example,\
    \ the predicted trajectories of other vehicles (NPCs, non-player characters),\
    \ pedestrians, and traffic lights. For instance, Table [1](#page-1-1) shows a\
    \ planned trajectory for an ego vehicle with respect to the predicted environment\
    \ shown in Table [2.](#page-1-2) Here, the ego vehicle slows down before approaching\
    \ an intersection as the traffic light is changing to red. Every line in Table\
    \ [1](#page-1-1) represents a planned waypoint, i.e., the planned position, speed,\
    \ acceleration, steer, and gear of the ego vehicle at a series of future time\
    \ points. Note that an actual planned trajectory typically contains hundreds of\
    \ waypoints. Similarly, every line in Table [2](#page-1-2) corresponds to the\
    \ predicted states of NPCs such as vehicles and pedestrians, as well as environmental\
    \ parameters like traffic lights. Here, Car2 and Ped1 are predicted to be stationary,\
    \ Car1 is predicated to change lanes, and the color of the traffic light ahead\
    \ is predicted to change from green to yellow and eventually to red. Furthermore,\
    \ in general there may be multiple planned trajectories for a given destination,\
    \ and the planning module attempts to find the \"best\" one. Finally, the control\
    \ module translates the planned trajectory into control commands (e.g., 'brake',\
    \ and 'signal') so that the ego vehicle is likely to follow the planned trajectory,\
    \ i.e., passing through the waypoints with the planned speed, acceleration, steering\
    \ angle, and gear position. We refer to [\\[2,](#page-10-8) [3\\]](#page-10-9)\
    \ for details on how commands are generated.\n\nThere may be other modules in\
    \ an ADS (e.g., the map module in Apollo) or the above-mentioned modules may be\
    \ further divided into sub-modules (e.g., motion planning in Apollo is divided\
    \ into routing, prediction, and planning). Nonetheless, the similar highlevel\
    \ design of existing ADSs implies we could potentially introduce\n\n<span id=\"\
    page-2-0\"></span>\n$$\\begin{aligned} \\varphi &:= \\mu \\mid \\neg \\varphi\
    \ \\mid \\varphi\\_1 \\lor \\varphi\\_2 \\mid \\varphi\\_1 \\land \\varphi\\_2\
    \ \\mid \\varphi\\_1 \\Downarrow \\Downarrow \\varphi\\_2 \\\\ \\mu &:= f(x\\\
    _0, x\\_1, \\dots, x\\_k) \\sim 0 \\quad \\sim := > \\mid \\ge \\mid < \\mid \\\
    le \\mid \\ne \\mid =; \\end{aligned}$$\n\nFigure 2: Specification language syntax,\
    \ where , <sup>1</sup> and <sup>2</sup> are STL formulas, is an interval, and\
    \ is a multivariate linear continuous function over language variables\n\na module\
    \ for runtime monitoring and enforcement which sits inbetween existing modules,\
    \ i.e., to intercept, analyse, and alter (if necessary) the inter-module messages.\
    \ This way, runtime monitoring and enforcement can be introduced without changing\
    \ the inner logic of existing modules. For instance, given the planned trajectory\
    \ generated by the planning module shown in Table [1,](#page-1-1) if we decide\
    \ that the trajectory could potentially lead to the violation of a certain property,\
    \ we can simply modify the planned trajectory before forwarding it to the control\
    \ module (to trigger a different control command generation).\n\n# 2.2 Property\
    \ Specification\n\nTo go beyond the simplest safety requirements (e.g., 'the ego\
    \ vehicle does not collide'), we require a specification language that is able\
    \ to express a rich set of properties that are relevant to autonomous vehicles\
    \ and driving in general. In this work, we adopt the driveroriented specification\
    \ language of LawBreaker [\\[44\\]](#page-11-1), which is based on signal temporal\
    \ logic (STL), and has been demonstrated to be expressive enough to specify the\
    \ traffic laws of China and Singapore. We highlight its key features, referring\
    \ readers to [\\[44\\]](#page-11-1) for details.\n\nThe high-level syntax of the\
    \ language is shown in Figure [2.](#page-2-0) A time interval is of the form [,\
    \ ] where and are respectively the lower and upper bounds of the interval. Following\
    \ convention, we write ^ to denote U ; and □ to denote ¬ ^ ¬. Intuitively, U,\
    \ □, and ^ are modal operators that are respectively interpreted as 'until', 'always',\
    \ and 'eventually'. Note that the time interval is omitted when it is [0, ∞].\
    \ The propositions in this language are constructed using 17 variables and 16\
    \ functions that are relevant to AVs, some of which are shown in Tables [3.](#page-2-1)\
    \ In general, can be regarded as a proposition of the form (0, 1, · · · , ) ∼\
    \ 0 where is a multivariate linear continuous function and for all in [0, ] is\
    \ a variable supported in the language.\n\n<span id=\"page-2-2\"></span>Example\
    \ 2.1. Consider the following two (English translations of) traffic rules from\
    \ the Regulations for Road Traffic Safety of the People's Republic of China [\\\
    [9\\]](#page-10-10).\n\n- (1) Article #38-(3): When a red light is on, vehicles\
    \ are prohibited from passing. However, vehicles turning right can pass without\
    \ hindering the passage of vehicles or pedestrians.\n- (2) Article #58-(3): When\
    \ a vehicle is driving on a foggy day, the fog lights and hazard warning flashing\
    \ should be on.\n\nTable 3: Car and environment related variables\n\n<span id=\"\
    page-2-1\"></span>\n\n| Signal                                          | Type\
    \ | Remarks                                  |  |\n|-------------------------------------------------|------|------------------------------------------|--|\n\
    | speed                                           | Num  | Speed of ego vehicle\
    \ (\U0001D45A/\U0001D460).              |  |\n| acc                          \
    \                   | Num  | 2<br>Acceleration of ego veh (\U0001D45A/\U0001D460\
    <br>).  |  |\n| direction                                       | Enum | forward,\
    \ left, right                     |  |\n| D(stopline)                        \
    \             | Num  | distance to the stopline ahead           |  |\n| D(junction)\
    \                                     | Num  | distance to the junction ahead\
    \           |  |\n| Bool<br>whether the fog light is on<br>fogLight |      | \
    \                                         |  |\n| warningFlash               \
    \                     | Bool | whether the warning flash light is on    |  |\n\
    | PriorityV(n)                                    | Bool | Whether there are vehicles\
    \ with priority |  |\n|                                                 |    \
    \  | within n meters                          |  |\n| PriorityP(n)           \
    \                         | Bool | Whether there are pedestrians with pri   |\
    \  |\n|                                                 |      | ority within\
    \ n meters                    |  |\n| TL(color)                              \
    \         | Enum | YELLOW, GREEN, RED, or BLACK             |  |\n| TL(blink)\
    \                                       | Bool | if the traffic light ahead is\
    \ blinking   |  |\n| fog                                             | Num  |\
    \ degree of fog ranging from 0 to 1        |  |\n| snow                      \
    \                      | Num  | degree of snow ranging from 0 to 1       |  |\n\
    \nThe above can be formalised as follows.\n\n38<sup>3</sup> ≡ □( ( () = ∧ (()\
    \ < 2 ∨ () < 2) ∧ ¬ = ℎ) → (^[0,3] ( < 0.5)) ∧ ( () = ∧ (() < 2 ∨ () < 2) ∧ =\
    \ ℎ ∧ ¬ (20) ∧ ¬ (20)) → (^[0,2] ( > 0.5))) 58<sup>3</sup> ≡ □( ≥ 0.5 → ( ℎ ∧\
    \ ℎ))\n\nwhere speed, direction, fogLight, and warningFlash represent the speed,\
    \ direction, fog light status, and warning flash light status of the vehicle;\
    \ TL() returns the status of traffic light ahead; D(object) calculates the distance\
    \ from the vehicle to the object ahead; and PriorityV(n), PriorityP(n) check whether\
    \ there is a priority vehicle or pedestrian within n meters ahead. Note that several\
    \ configurable constants (e.g., the distance 2 and the time interval [0, 3]) are\
    \ introduced to reduce the vagueness of the law in practice [\\[44\\]](#page-11-1).\
    \ □\n\nA specification is evaluated with respect to a trace of scenes, denoted\
    \ as = ⟨0, 1, <sup>2</sup> . . . , ⟩, where each scene is a valuation of the propositions\
    \ at time step and <sup>0</sup> reflects the state at the start of a simulation.\
    \ These traces can be constructed from the planned trajectory generated by the\
    \ ADS (Section [3.1\\)](#page-3-0). We follow the standard semantics of STL (see\
    \ e.g., [\\[29\\]](#page-10-11)).\n\n# 2.3 The Runtime Enforcement Problem for\
    \ AVs\n\nGiven an ADS and a user-specified property , our goal is to solve the\
    \ runtime enforcement problem for AVs by monitoring traces of the ADS against\
    \ at runtime, and altering its behavior when a violation is likely in the near\
    \ future. Here, altering the ADS's behavior means adjusting its planned trajectory\
    \ and consequently the control commands. Solving this problem could systematically\
    \ improve the safety of ADSs when encountering unusual situations on the road.\
    \ We formulate our problem as follows:\n\nDefinition 1 (Problem Definition). Given\
    \ a runtime planned trajectory , runtime control commands , a specification of\
    \ ADS behavior , and a trace of the AV in a scenario. Let ′ , ′ , and ′ denote\
    \ the adjusted planned trajectory, modified control commands, and resulting trace\
    \ of the AV after these adjustments. Our problem is:\n\n$$Maximise: \\frac{\\\
    rho(\\wp, \\pi') - \\rho(\\wp, \\pi)}{|\\chi' - \\chi| + |\\zeta' - \\zeta|}.$$\n\
    \n□\n\nIntuitively, we seek to maximize the improvement in adhering to the desired\
    \ behavior, while considering the magnitude of changes made to the planned trajectory\
    \ and control commands. Here, the function serves as the quantitative semantics\
    \ of a trace concerning the specification. Its purpose is to provide a numerical\
    \ assessment that calculates the distance to a violation of the specification.\n\
    \n# <span id=\"page-3-4\"></span>3 OUR APPROACH\n\nREDriver, our runtime enforcement\
    \ approach, consists of three broad steps. First, plan validation, in which it\
    \ evaluates the planned trajectory against the specification to determine if there\
    \ is a risk of violation. Second, trajectory repair, in which the planned trajectory\
    \ is modified so as to avoid the violation. Finally, control validation, in which\
    \ the commands generated by the control module are further evaluated to ensure\
    \ the specification is satisfied. As shown in Figure [1,](#page-1-0) these steps\
    \ seamlessly integrate into the modular design of ADSs: REDriver sits between\
    \ the modules, intercepting and altering the messages they exchange. Note that\
    \ we assume that the sensor data received by the ADS is accurate.\n\n# <span id=\"\
    page-3-0\"></span>3.1 Plan Validation\n\nGiven a specification and a planned trajectory\
    \ from the motion planning module of the ADS, REDriver first determines whether\
    \ the trajectory is likely to violate . To achieve this, REDriver first constructs\
    \ a trace from the planned trajectory, i.e., by evaluating all variables and functions\
    \ relevant to at every time point with respect to the planned trajectory and the\
    \ predicted environment. For instance, given the planned trajectory in Table [1](#page-1-1)\
    \ (and the predicted environment in Table [2\\)](#page-1-2), Table [4](#page-3-1)\
    \ shows the constructed trace.\n\nOne practical complication is that some variables\
    \ relevant to cannot be obtained from the planned trajectory as they are only\
    \ known after command generation (see Section [3.3\\)](#page-6-0). For example,\
    \ the values of fogLight (i.e., whether the fog light is on) and warningFlash\
    \ can only be determined once the respective commands are generated. For such\
    \ situations, we use typed 'placeholder' variables x, in the scenes of the trace\
    \ for each time step and position . We define an assignment to be a function mapping\
    \ the typed variables x, to the value domains. Then, for traces containing those\
    \ variables, satisfies if and only if there exists an assignment such that [ (x,)/x,]\
    \ satisfies for every variable x, in . Practically, finding a suitable assignment\
    \ is straightforward: all variables for assignment have only a few possible discrete\
    \ values (e.g., the light is on or off), and thus brute force search is sufficient\
    \ and inexpensive.\n\nNext, REDriver computes how 'close' the ego vehicle will\
    \ come to violating . Note that our goal is to proactively react when a violation\
    \ is likely in the near future. This is because the ego vehicle operates in an\
    \ open environment (e.g., with other vehicles and pedestrians) and thus reacting\
    \ too late may be too risky if the predicted environment turns out to be wrong\
    \ (e.g., a sudden move of a pedestrian). To measure how close a trace is to violating\n\
    \nICSE '24, April 14–20, 2024, Lisbon, Portugal Yang Sun, Christopher M. Poskitt,\
    \ Xiaodong Zhang, and Jun Sun\n\nTable 4: Trace obtained from the trajectory in\
    \ Table [1](#page-1-1)\n\n<span id=\"page-3-1\"></span>\n\n| planning signals\
    \   | 0     | 2     | 4     | 6     | 8     |\n|--------------------|-------|-------|-------|-------|-------|\n\
    | speed              | 7.01  | 6.13  | 5.44  | 5.09  | 3.89  |\n| direction  \
    \        | 0     | 0     | 0     | 0     | 0     |\n| D(stopline)        | 44\
    \    | 30.66 | 19.17 | 8.15  | -0.75 |\n| D(junction)        | 44    | 30.66 |\
    \ 19.17 | 8.15  | -0.75 |\n| fogLight           | x0,0  | x2,0  | x4,0  | x6,0\
    \  | x8,0  |\n| warningFlash       | x0,1  | x2,1  | x4,1  | x6,1  | x8,1  |\n\
    | Prediction Signals | 0     | 2     | 4     | 6     | 8     |\n| TL(color)  \
    \        | 1     | 0     | 0     | 0     | 2     |\n| fog                | 0.6\
    \   | 0.6   | 0.6   | 0.6   | 0.6   |\n| PriorityV(20)      | false | false |\
    \ false | false | false |\n| PriorityP(10)      | false | false | false | true\
    \  | true  |\n\n, we adopt a quantitative semantics [\\[13,](#page-10-12) [29,](#page-10-11)\
    \ [34\\]](#page-11-5) that produces a numerical robustness degree.\n\n<span id=\"\
    page-3-3\"></span>Definition 2 (Quantitative Semantics). Given a trace and a formula\
    \ , the quantitative semantics is defined as the robustness degree (, , ), computed\
    \ as follows. Recall that propositions are of the form (0, 1, · · · , ) ∼ 0.\n\
    \n$$\\rho(\\mu,\\pi,t) = \\begin{cases} -\\pi\\_{\\ell}(f(\\mathbf{x}\\_{0},\\\
    mathbf{x}\\_{1},\\cdots,\\mathbf{x}\\_{k})) & \\text{if } \\sim \\text{ is } \\\
    llcorner \\alpha\\\\ \\pi\\_{\\ell}(f(\\mathbf{x}\\_{0},\\mathbf{x}\\_{1},\\cdots,\\\
    mathbf{x}\\_{k})) & \\text{if } \\sim \\text{ is } \\gg \\text{ or } > \\text{\
    \ and } \\gg \\text{ is } \\llcorner \\alpha\\\\ |\\;\\pi\\_{\\ell}(f(\\mathbf{x}\\\
    _{0},\\mathbf{x}\\_{1},\\cdots,\\mathbf{x}\\_{k}))| & \\text{if } \\sim \\text{\
    \ is } \\neq \\text{ and } \\gg \\text{ is } \\llcorner \\alpha\\\\ -|\\;\\pi\\\
    _{\\ell}(f(\\mathbf{x}\\_{0},\\mathbf{x}\\_{1},\\cdots,\\mathbf{x}\\_{k}))| &\
    \ \\text{if } \\sim \\text{ is } = \\text{ or } \\llcorner \\alpha\\\\ \\text{and\
    \ } \\sim \\text{ is } \\llcorner \\alpha\\\\ \\end{cases}$$\n\nwhere is the time\
    \ step and () is the valuation of expression at time in .\n\n$$\\begin{aligned}\
    \ \\rho(\\neg\\varphi,\\pi,t) &= -\\rho(\\varphi,\\pi,t) \\\\ \\rho(\\varphi\\\
    _1 \\wedge \\varphi\\_2,\\pi,t) &= \\min\\{\\rho(\\varphi\\_1,\\pi,t), \\rho(\\\
    varphi\\_2,\\pi,t)\\} \\\\ \\rho(\\varphi\\_1 \\vee \\varphi\\_2,\\pi,t) &= \\\
    max\\{\\rho(\\varphi\\_1,\\pi,t), \\rho(\\varphi\\_2,\\pi,t)\\} \\\\ \\rho(\\\
    varphi\\_1 \\cup \\varphi\\_2,\\pi,t) &= \\sup\\_{t\\_1 \\in t+\\mathbb{T}} \\\
    min\\{\\rho(\\varphi\\_2,\\pi,t\\_1), \\inf\\_{t\\_2 \\in \\left[t,t\\_1\\right]}\
    \ \\rho(\\varphi\\_1,\\pi,t\\_2)\\} \\end{aligned}$$\n\nwhere + is the interval\
    \ [ + , + ] given = [, ]. □\n\nNote that the smaller (, , ) is, the closer is\
    \ to violating . If (, , ) ≤ 0, is violated. We write (, ) to denote (, , 0);\
    \ ⊨ to denote (, , ) > 0; and ⊭ to denote (, , ) ≤ 0. Note that time is discrete\
    \ in our setting.\n\n<span id=\"page-3-2\"></span>Example 3.1. Let = □( < 90),\
    \ i.e., the speed limit is 90km/h. Suppose is ⟨( ↦→ 0, . . . ), ( ↦→ 0.5, . .\
    \ . ), · · · ( ↦→ 85, . . . )⟩ where the ego vehicle's max is 85km/h at the last\
    \ time step. We have (, ) = (, , 0) = ∈ [0,| | ] (90 − ()) = 5. Suppose instead\
    \ that is the specification from Example [2.1](#page-2-2) and is the trace from\
    \ Table [4.](#page-3-1) The robustness value is (, ) = 0, i.e., is violated as\
    \ the ego vehicle fails to stop before the stop line when the traffic light turns\
    \ red. □\n\n# 3.2 Trajectory Repair\n\nIf the robustness value of with respect\
    \ to a trace is below a certain threshold , there is a risk of violating in the\
    \ future, even if 0 < (, ) ≤ (given that there is uncertainty in the predicted\
    \ environment). This threshold is determined experimentally in our work (Section\
    \ [4\\)](#page-6-1): intuitively, it characterises how 'cautious' the ADS is.\
    \ In order to enforce , i.e., proactively prevent its possible violation, REDriver\
    \ repairs the planned trajectory before sending\n\nit to the control module of\
    \ the ADS so as to change the commands that will be generated.\n\nOur trajectory\
    \ repair method consists of three steps. First, we identify the earliest time\
    \ step when the robustness value falls below the threshold. Second, we compute\
    \ the gradient (through autodifferentiation [\\[22\\]](#page-10-13)) of each variable\
    \ at the identified time step with respect to the robustness degree. Based on\
    \ the result, we then modify the variable to increase the robustness degree. Finally,\
    \ we modify the planned trajectory accordingly and feed it into the control module.\
    \ In the following, we present each step in detail.\n\nDetermine the time step.\
    \ Given a trace = ⟨(0, 0), · · · , (, )⟩, we write to denote the prefix ⟨(0, 0),\
    \ (1, 1), · · · , ( , )⟩. Given such that (, ) < , we aim to identify a time step\
    \ such that: (1) (, ) < ; and (2) there does not exist a time step such that <\
    \ and (, ) < . Intuitively, is the earliest time step when the robustness value\
    \ falls below the threshold. We identify the time step using a sequential search,\
    \ i.e., we start from = 0 and keep increasing until we find a such that (, ) <\
    \ .\n\n<span id=\"page-4-0\"></span>Example 3.2. Let = 38<sup>3</sup> from Example\
    \ [2.1](#page-2-2) and denote the trace from Table [4.](#page-3-1) Suppose the\
    \ threshold is 10. Then, as shown in Example [3.1,](#page-3-2) (383, ) = 0 and\
    \ is thus below the threshold. Then, we apply the above-mentioned algorithm to\
    \ identify the time step. The following are computed in sequence.\n\n$$\\rho(\\\
    wp, \\pi^0) = 42, \\dots, \\rho(\\wp, \\pi^2) = 28.66, \\dots, \\text{ },$$\n\n\
    $$\\rho(\\wp, \\pi^4) = 17.17, \\dots, \\rho(\\wp, \\pi^6) = 6.15$$\n\nThus, the\
    \ time step that we are looking for is 6 (as 6 < ). □\n\nCalculate the gradient.\
    \ Next, we find out how the variables at time step should be modified so that\
    \ the robustness degree of the resulting trace can be improved. We thus define\
    \ a differentiation function that calculates the gradient of each relevant variable\
    \ with respect to the robustness degree. Intuitively, when the gradient of a variable\
    \ at time is positive (resp. negative), we can increase the robustness degree\
    \ by increasing (resp. decreasing) the value of .\n\nRecall that the robustness\
    \ degree of is computed using discrete functions and (Definition [2\\)](#page-3-3)\
    \ that are hard to differentiate [\\[46\\]](#page-11-6). Hence, we adopt a continuous\
    \ robustness measure as defined in [\\[20,](#page-10-14) [35\\]](#page-11-7) which\
    \ replaces and in Definition [2](#page-3-3) with continuous functions <sup>g</sup>\
    \ and <sup>g</sup> as follows:\n\n$$\\begin{aligned} \\widetilde{max}\\{\\mathbf{x}\\\
    _0, \\mathbf{x}\\_1, \\dots, \\mathbf{x}\\_m\\} &= \\frac{1}{a} \\ln(\\sum\\_{i=1}^m\
    \ e^{\\alpha \\mathbf{x}\\_i}) \\\\\\widetilde{min}\\{\\mathbf{x}\\_0, \\mathbf{x}\\\
    _1, \\dots, \\mathbf{x}\\_m\\} &= -\\widetilde{max}(-\\mathbf{x}\\_0, -\\mathbf{x}\\\
    _1, \\dots, -\\mathbf{x}\\_m) \\end{aligned}$$\n\nwhere is a constant that controls\
    \ the accuracy of <sup>g</sup> and <sup>g</sup>. The larger is, the closer <sup>g</sup>\
    \ (resp. <sup>g</sup>) is to (resp. ). We set to be 10, following [\\[20,](#page-10-14)\
    \ [35\\]](#page-11-7). We denote the continuous robustness degree as ˜(, ). The\
    \ following proposition from [\\[20,](#page-10-14) [35\\]](#page-11-7) establishes\
    \ the soundness of approximating (, ) with ˜(, ).\n\nProposition 3.3. Let be an\
    \ STL formula, be a trace, and be a real value larger than 0. Then, there exists\
    \ a value <sup>1</sup> such that |˜(, ,) − (, ,)| < holds for all > 1. □\n\nNext,\
    \ we define a differentiation function(, , ) that returns a given variable 's\
    \ gradient with respect to (, ) at time .\n\n$$D(\\boldsymbol{\\varphi}, \\boldsymbol{\\\
    pi}, \\mathbf{x}^k) = \\frac{\\partial \\bar{\\rho}(\\boldsymbol{\\varphi}, \\\
    boldsymbol{\\pi}, \\mathbf{0})}{\\partial \\mathbf{x}^k}$$\n\nThe following shows\
    \ how (, , ) is computed.\n\n<span id=\"page-4-1\"></span>Definition 3. Given\
    \ an STL formula and trace , function (, , ) is defined as follows:\n\n$$\\frac{\\\
    partial \\bar{\\rho}\\,(\\mu,\\pi,t)}{\\partial \\mathbf{x}^k} = \\begin{cases}\
    \ 0 & \\text{if } k \\neq t \\\\ \\frac{df'(\\varkappa\\_0, \\varkappa\\_1, \\\
    cdots, \\varkappa\\_n)}{dx^k} & \\text{otherwise} \\end{cases}$$\n\nwhere ′ (0,1,···\
    \ , ) is the derivative of function ′ with respect to . Furthermore, let ( {0,1,...,\
    \ } ) be defined as Í =1 , and let <sup>g</sup> ( {0,1,..., } ) be defined as\
    \ − Í =1 − . ˜ (¬, , ) = − ˜ (, , ) ˜ (<sup>1</sup> ∧ 2, , ) = <sup>g</sup> {˜\
    \ (1, , ) , ˜ (2, , ) } ˜ (1, , ) · ˜ (1, , ) + <sup>g</sup> {˜ (1, , ) , ˜ (2,\
    \ , ) } ˜ (2, , ) · ˜ (2, , ) ˜ (<sup>1</sup> ∨ 2, , ) = {˜ (1, , ) , ˜ (2, ,\
    \ ) } ˜ (1, , ) · ˜ (1, , ) + {˜ (1, , ) , ˜ (2, , ) } ˜ (2, , ) · ˜ (2, , ) ˜\
    \ (<sup>1</sup> U<sup>I</sup> 2, , ) = ∑︁ ′ ∈+I ˜ (<sup>1</sup> U<sup>I</sup>\
    \ 2, , ) ˜ (1, , ′ ) · ˜ (1, , ′ ) + ˜ (<sup>1</sup> U<sup>I</sup> 2, , ) ˜ (2,\
    \ , ′ ) · ˜ (2, , ′ ) \n\nwhere ˜(<sup>1</sup> <sup>U</sup><sup>I</sup> 2,,) ˜(1,,′\
    \ ) is the derivative of ˜ (1UI2, , ) with respect to ˜ (1, , ′ ), and is defined\
    \ as:\n\n∑︁ 1 ∈+I∧1≥ ′ © « { {˜ (<sup>2</sup> , , 1 ), inf<sup>2</sup> ∈ [,<sup>1</sup>\
    \ ] ˜ (1 , , 2 ) } |1 ∈ + I} {˜ (<sup>2</sup> , , 1 ), inf<sup>2</sup> ∈ [,<sup>1</sup>\
    \ ] ˜ (1 , , 2 ) } · {˜ (<sup>2</sup> , , 1 ), inf<sup>2</sup> ∈ [,<sup>1</sup>\
    \ ] ˜ (1 , , 2 ) } inf<sup>2</sup> ∈ [,<sup>1</sup> ] ˜ (1 , , 2 ) · {˜ (<sup>1</sup>\
    \ , , 2 ) | 2 ∈ [, 1 ]} ˜ (1 , , ′ ) ª ® where ˜(<sup>1</sup> <sup>U</sup><sup>I</sup>\
    \ 2,,) ˜(2,,′ ) is defined as:\n\n { <sup>g</sup> {˜(2, , <sup>1</sup> ), inf<sup>2</sup>\
    \ ∈ [,<sup>1</sup> ] ˜(1, , <sup>2</sup> ) } | <sup>1</sup> <sup>∈</sup> <sup>+</sup>\
    \ <sup>I</sup>} <sup>g</sup> {˜(2, , ′ ), inf<sup>2</sup> ∈ [, ′ ] ˜(1, , <sup>2</sup>\
    \ ) } · <sup>g</sup> {˜(2, , ′ ), inf<sup>2</sup> ∈ [, ′ ] ˜(1, , <sup>2</sup>\
    \ ) } ˜(2, , ′ )\n\n□\n\nGiven the time step previously identified, we apply the\
    \ above definition to compute(, , ) for every variable . The purpose of the differentiation\
    \ function D is to determine the \"responsibility\" of each signal in violating\
    \ the specification. In other words, consider the computation of robustness as\
    \ a function of multiple variables where D determines the gradient of each variable.\
    \ We remark that our implementation of (, , ) is based on automatic differentiation\
    \ techniques [\\[22\\]](#page-10-13). Intuitively, we store the intermediate values\
    \ while computing the robustness degree, and then compute the gradients based\
    \ on reverse accumulation.\n\n<span id=\"page-5-1\"></span>Example 3.4. Given\
    \ the trace of Table [4,](#page-3-1) the following shows how to calculate the\
    \ gradient of with respect to <sup>0</sup> = □( > 5) at time step 6:\n\n$$D\\\
    left(\\varphi\\_0, \\pi^6, speed^6\\right) = \\frac{\\partial \\rho\\left(\\varphi,\
    \ \\pi^6, 0\\right)}{\\partial \\rho\\left(speed^2 > 5, \\pi^6, 6\\right)} \\\
    cdot \\frac{\\partial \\rho\\left(speed > 5, \\pi^6, 6\\right)}{\\partial speed^6}$$\n\
    \n$$= \\frac{e^{-10 \\times 0.09}}{e^{-10 \\times 2.01} + e^{-10 \\times 1.13}\
    \ + e^{-10 \\times 0.44} + e^{-10 \\times 0.09}} \\cdot 1 = 0.97$$\n\n$$Similarly,\
    \ continuity \\, Example 3.2, the gradients \\, are \\, computed \\, as \\, follows:$$\n\
    \n$$D\\left(llaw \\, 38\\_3, \\pi^6, speed^6\\right) = 8.39 \\times 10^{-08}$$\n\
    \n$$D\\left(llaw \\, 38\\_3, \\pi^6, D\\left(stophine\\right)^6\\right) = 0.5$$\n\
    \n$$D\\left(llaw \\, 38\\_3, \\pi^6, D\\left(j\\omega cten\\right)^6\\right) =\
    \ 0.5$$\n\n$$D\\left(llaw \\, 38\\_3, \\pi^6, drection^6\\right) = -4.74 \\times\
    \ 10^{-19}$$\n\n$$D\\left(llaw \\, 38\\_3, \\pi^6, TL\\left(color\\,\\big)^6\\\
    right) = -9.48 \\times 10^{-19}$$\n\n$$D\\left(llaw \\, 38\\_3, \\pi^6, Plot\\\
    right) V\\left(20\\right)^6 = -9.77 \\times 10^{-28}$$\n\n$$D(la \\bowtie 38\\\
    _3, \\pi^6, Prior \\wr light yN(20)^6) = 2.15 \\times 10^{-23}$$\n\nThe gradients\
    \ for variable ( ) and () at time step 6 are positive, which means that we can\
    \ effectively increase the robustness value (383, <sup>6</sup> ) by increasing\
    \ ( ) 6 or () 6 . □\n\n<span id=\"page-5-0\"></span>Proposition 3.5. Let (, ,\
    \ ) be the result of gradient calculation as shown in Definition [3.](#page-4-1)\
    \ When (, , ) is positive (or negative), there exists an interval (0, Δ) such\
    \ that increasing (or decreasing) within this interval increases in the value\
    \ of ˜(, ).\n\nProof. First, if is a Boolean Expression , ˜(, ) can be represented\
    \ by a continuous function ′ (0, · · · , ) as shown in Definition [2.](#page-3-3)\
    \ Given that ′ (0, · · · , ) is confined to linear or absolute value functions,\
    \ the proposition holds for .\n\nThen, assuming the proposition holds, the proposition\
    \ holds if we can prove the proposition holds for each and every way can be constructed,\
    \ i.e., ¬1, <sup>1</sup> ∧ 2, <sup>1</sup> ∨ 2, and <sup>1</sup> U<sup>I</sup>\
    \ 2.\n\nIf is in the format of ¬1, we have ˜(¬1, ) = −˜(1, ), and (¬1, , ) = −(1,\
    \ , ). Thus, by negating the modification, we can ensure that the proposition\
    \ holds for ¬1.\n\nIf is in the format of <sup>1</sup> ∨ 2, ˜(<sup>1</sup> ∨ 2,\
    \ ) = 1 ln( <sup>1</sup> + <sup>2</sup> ), and (<sup>1</sup> ∨2, , ) = 1 1+ 2\
    \ · (1, , ) + 2 1+ 2 · (2, , ). Here, <sup>1</sup> = ˜(1, ), <sup>2</sup> = ˜(2,\
    \ ), → ∞. Suppose the proposition holds for ˜(1, ) within interval (0, Δ1), and\
    \ holds for ˜(2, ) within interval (0, Δ2). If <sup>1</sup> ≠ 2, suppose <sup>1</sup>\
    \ > 2, then we have 1 1+ <sup>2</sup> → 1, 2 1+ <sup>2</sup> → 0, and (<sup>1</sup>\
    \ ∨ 2, , ) → (1, , ). The proposition holds for interval (0, Δ1). If <sup>1</sup>\
    \ = 2, then (<sup>1</sup> ∨ 2, , ) > 0 indicates (1, , ) + (2, , ) > 0. Even if\
    \ one of (1, , ) and (2, , ) is negative, the value of <sup>1</sup> + <sup>2</sup>\
    \ still increases, leading to the increase of ˜(<sup>1</sup> ∨2, ). Let Δ ′ be\
    \ a number larger than 0 and smaller than {Δ1, Δ2}. The proposition holds for\
    \ (0, Δ ′ ).\n\nIf is in the format of <sup>1</sup> ∧ 2, since <sup>1</sup> ∧\
    \ <sup>2</sup> = ¬(¬<sup>1</sup> ∨ ¬2), we can deduce that the proposition always\
    \ holds for <sup>1</sup> ∧ 2.\n\nIf is in the format of <sup>1</sup> U<sup>I</sup>\
    \ 2. Since ˜(<sup>1</sup> U<sup>I</sup> 2, ) is a combination of the function\
    \ <sup>g</sup> and <sup>g</sup>, we can deduce that the proposition always holds\
    \ for <sup>1</sup> U<sup>I</sup> 2.\n\nTherefore, we can conclude that the proposition\
    \ holds. □\n\nIntuitively, Proposition [3.5](#page-5-0) clarifies that the gradient\
    \ calculation function (, , ) reflects the changing trend of the robustness\n\n\
    <span id=\"page-5-2\"></span>\n\n| Algorithm 1: Trajectory repair algorithm |\
    \                                                                            \
    \   |  |  |\n|------------------------------------------|-------------------------------------------------------------------------------|--|--|\n\
    |                                          | Input: variable/function<br>\U0001D465\
    , time step<br>\U0001D458, magnitude<br>\U0001D6FF                 |  |  |\n|\
    \                                          | 1 case \U0001D465 is \U0001D460\U0001D45D\
    \U0001D452\U0001D452\U0001D451 do                                            \
    \              |  |  |\n| 2                                        | \U0001D460\
    \U0001D45D\U0001D452\U0001D452\U0001D451\U0001D458<br>\U0001D460\U0001D45D\U0001D452\
    \U0001D452\U0001D451\U0001D458 +<br>Set<br>to be<br>\U0001D6FF;              \
    \                        |  |  |\n|                                          |\
    \ 3 case \U0001D465 is \U0001D451\U0001D456\U0001D45F\U0001D452\U0001D450\U0001D461\
    \U0001D456\U0001D45C\U0001D45B do                                            \
    \          |  |  |\n| 4                                        | \U0001D451\U0001D456\
    \U0001D45F\U0001D452\U0001D450\U0001D461\U0001D456\U0001D45C\U0001D45B\U0001D458\
    \ +<br>Choose a value<br>(0, 1, or 2) that is closest to<br>\U0001D6FF;<br>\U0001D451\
    0 |  |  |\n| 5                                        | Set the<br>\U0001D460\U0001D461\
    \U0001D452\U0001D452\U0001D45F at time<br>\U0001D458 to 0 if<br>\U0001D4510<br>=\
    \ 0;                           |  |  |\n| 6                                  \
    \      | Otherwise set the<br>\U0001D460\U0001D461\U0001D452\U0001D452\U0001D45F\
    \ at time<br>\U0001D458 to 0.1 if<br>= 1;<br>\U0001D4510               |  |  |\n\
    | 7                                        | Otherwise set the<br>\U0001D460\U0001D461\
    \U0001D452\U0001D452\U0001D45F at time<br>\U0001D458 to -0.1 if<br>\U0001D451\
    0<br>= 2;              |  |  |\n|                                          | 8\
    \ case \U0001D465 is of the form \U0001D437 (_) or \U0001D43F\U0001D44E\U0001D45B\
    \U0001D452 (_) do                                  |  |  |\n| 9              \
    \                          | Search for a coordinate<br>(\U0001D44E, \U0001D44F\
    ) (i.e., new position for the ego             |  |  |\n|                     \
    \                     | vehicle) such that<br>\U0001D437 (_)<br>becomes<br>\U0001D437\
    \ (_) +<br>\U0001D6FF or<br>\U0001D43F\U0001D44E\U0001D45B\U0001D452 (_)     \
    \    |  |  |\n|                                          | becomes<br>\U0001D43F\
    \U0001D44E\U0001D45B\U0001D452 (_) +<br>\U0001D6FF;                          \
    \                         |  |  |\n| 10                                      \
    \ | Set the position of the ego vehicle at time<br>\U0001D458 to<br>(\U0001D44E\
    , \U0001D44F);                |  |  |\n| 11 end                              \
    \     |                                                                      \
    \         |  |  |\n\nfunction ˜(, ) in terms of variable . However, the changing\
    \ trend is sensitive to the variable's current value. If we increase the variable\
    \ by too much, it may lead to a decrease in robustness. For instance, consider\
    \ the specification: = 10 < < 100. Suppose the current speed is 8, then the robustness\
    \ is −2, and the gradient for speed (, , ) is 1, indicating that we should increase\
    \ the value of speed. If we increase the speed within the interval (0, 94), the\
    \ robustness will always be larger than −2. However, if we increase the speed\
    \ to 103, the robustness will become −3, resulting in a decrease. Therefore, to\
    \ guarantee an increase in robustness, it is necessary to limit the modification\
    \ within an interval of (0, Δ). Repair the trajectory. The gradients calculated\
    \ above allow us to determine how to effectively increase the robustness degree.\
    \ We can proceed to repair the trace by modifying the variable with the maximal\
    \ absolute gradient at time step . The magnitude of the modification is calculated\
    \ as follows:\n\n$$\\delta = \\frac{\\theta - \\bar{\\rho}(\\varphi, \\pi^k)}{D(\\\
    varphi, \\pi^k, \\pi^k)}; \\text{ while } \\bar{\\rho}(\\varphi, \\pi') < \\bar{\\\
    rho}(\\varphi, \\pi) \\text{ Do} : \\ \\{\\delta \\gets \\delta/2\\}.$$\n\nwhere\
    \ ′ is the trace after the modification. This magnitude of the modification indicates\
    \ that we try to increase the robustness value to . However, this adjustment might\
    \ sometimes lead to overreactions, causing a decrease in the robustness value.\
    \ In such cases, we reduce until we observe an increase in the robustness value,\
    \ and the descent rate during this process follows a scale of 2 , enabling us\
    \ to efficiently determine the magnitude. For instance, according to Example [3.4,](#page-5-1)\
    \ we should modify () or () at time step 6 with a magnitude of − (383,<sup>6</sup>\
    \ ) 0.5 = 7.7. This modification results in (383, <sup>6</sup> ) increasing from\
    \ 6.15 to 13.85.\n\nProposition 3.6. Let be a variable, and be the magnitude of\
    \ the modification on . The robustness value ˜(, ) always increases after the\
    \ modification.\n\nProof. The modification is triggered only when − ˜(, ) > 0,\
    \ which implies that and (, , ) share the same sign. As shown in Proposition [3.5,](#page-5-0)\
    \ there exists an interval (0, Δ) in which the gradient value is effective. If\
    \ the previous modification results in a decrease of ˜(, ), we can ensure an increase\
    \ in ˜(, ) by decreasing | | to a value smaller than Δ. The proposition holds.\
    \ □\n\n<span id=\"page-6-2\"></span>\n\n| Input: specification<br>\U0001D711,\
    \ trajectory<br>Γ, the threshold<br>\U0001D703             |\n|----------------------------------------------------------------------------|\n\
    | 1 Generate trace<br>\U0001D70B based on<br>Γ;                              \
    \         |\n| 2 if \U0001D70C (\U0001D711, \U0001D70B ) ≤ \U0001D703 then   \
    \                                                 |\n| \U0001D70C (\U0001D711\
    , \U0001D70B\U0001D458<br>Identify the smallest<br>\U0001D458 such that<br>) ≤\
    \ \U0001D703;<br>3            |\n| \U0001D437 (\U0001D711, \U0001D70B\U0001D458\
    <br>, \U0001D465\U0001D458<br>Compute<br>) for every controllable variable<br>\U0001D465\
    <br>4 |\n| \U0001D458 with the maximal absolute gradient;<br>Identify variable<br>\U0001D465\
    <br>5       |\n| Invoke Algorithm 1 to fix the trajectory;<br>6              \
    \               |\n| 7 end                                                   \
    \                   |\n|                                                     \
    \                       |\n\n ;\n\nRecall that our goal is to modify the planned\
    \ trajectory so as to trigger different control commands. While we may modify\
    \ the trace arbitrarily, we cannot do the same for the planned trajectory. First,\
    \ some of the variables may not be controllable, e.g., the color of the traffic\
    \ light is beyond the control of the ADS. Second, a variable may have a specific\
    \ domain of discrete values in the ADS (e.g., has the value of 0, 1, or 2) and\
    \ thus we can only choose one of those valid values. Finally, the value of a variable\
    \ may be the result of a function which depends on the current and future scenes.\
    \ For instance, () measures the distance from the ego vehicle (according to the\
    \ planned trajectory) to the stop line ahead (according to the map). In these\
    \ situations, it is very difficult to translate the modification to the planned\
    \ trajectory. Thus, we focus on modifying those signals that the ADS has control\
    \ over and modify the planned trajectory accordingly, which are , , , (\\_) (i.e.,\
    \ which lane the ego vehicle should be in), and (\\_) (i.e., how far the ego vehicle\
    \ is from a certain artifact). These naturally correspond to what human drivers\
    \ focus on. Algorithm [1](#page-5-2) describes how the planned trajectory is repaired\
    \ with respect to a specific variable/function , time step , and magnitude . Note\
    \ that the fixes are specific to certain variables since they may have specific\
    \ domains. In the case of , we are constrained to choose a value from 0, 1, 2\
    \ and set the value in the trajectory accordingly. In the case of functions based\
    \ on the ego vehicle's position (e.g., ()), we search for nearby coordinates that\
    \ are close to the desired value while still remaining on the road. Note that\
    \ the ADS's planning module and the control module are entirely black boxes to\
    \ us. Therefore, we do not take into account the correlations between variables\
    \ when modifying the planned trajectory. To do so would require the construction\
    \ of an exhaustive physical model, essentially equivalent to rebuilding the planning\
    \ module of the ADS.\n\nExample 3.7. Given the planned trajectory in Table [1,](#page-1-1)\
    \ and the repair computed for () and () in Example [3.4.](#page-5-1) We modify\
    \ the planned car position at time step 6 from (0, 35.85) to (0, 28.15) (so the\
    \ vehicle should be positioned further from the junction), leaving the remaining\
    \ planned trajectory unchanged. Note that changing the value of can rectify the\
    \ trajectory as well, however, the gradient values strongly suggest that optimizing\
    \ the position of the waypoint is a more efficient approach. □\n\n# <span id=\"\
    page-6-0\"></span>3.3 Runtime Enforcement\n\nWe are now ready to present our runtime\
    \ enforcement algorithm, as shown in Algorithm [2.](#page-6-2) First, we generate\
    \ a trace based on the planned trajectory Γ and check whether (, ) ≤ . If so,\
    \ we\n\nproceed to identify the time step when the robustness degree falls below\
    \ the threshold. Then we compute gradients for the variables at time , identify\
    \ the controllable one with the maximal absolute gradient (w.r.t. the robustness\
    \ degree) and repair the trajectory accordingly. The repaired trajectory is then\
    \ sent to the control module, which generates the commands accordingly (e.g.,\
    \ turn on/off beam, and apply brake).\n\nRecall that the specification may also\
    \ constrain the generated commands, e.g., the need to signal before turning. To\
    \ make sure the commands generated do not violate , we introduce a control validation\
    \ module (refer to Figure [1\\)](#page-1-0) that intercepts and checks the generated\
    \ commands, modifying them if necessary. Recall that commands related to motion\
    \ (e.g., brake, accelerate, steer, and gear) are generated according to the (repaired)\
    \ trajectory and thus do not require modification. We remark that these commands\
    \ are mostly simple in nature (i.e., with Boolean values) and thus we can easily\
    \ modify them according to the specification. For instance, consider the beam-related\
    \ signals, namely highBeam and lowBeam, which have on and off states. We can easily\
    \ modify these states by switching the values in the control commands sent to\
    \ the AV's chassis control.\n\nExample 3.8. Consider the trace shown in Table\
    \ [4.](#page-3-1) Recall that the signals fogLight and warningFlash are not part\
    \ of the planned trajectory: in fact, the ADS turns these off by default, i.e.,\
    \ we initially have (x,) = false (x, is the placeholder variable as discussed\
    \ in Section [3.1\\)](#page-3-0). To satisfy the specification in Example [2.1,](#page-2-2)\
    \ REDriver sets (x,) = true for each , . To realize this, we activate the fogLight\
    \ and warningFlash in the control commands. □\n\n# <span id=\"page-6-1\"></span>4\
    \ IMPLEMENTATION AND EVALUATION\n\nWe implemented REDriver for Apollo 6.0 and\
    \ 7.0 [\\[3,](#page-10-9) [4\\]](#page-10-7). The code is on our website [\\[1\\\
    ]](#page-10-15). In particular, we built a bridge program that interprets Apollo's\
    \ messages (in JSON format) and obtains the values of variables and functions\
    \ used by the specification language. Some of these values are obtained directly\
    \ (e.g., and ), but some require complex processing. For example, to get the value\
    \ of variable ℎ. at time , we obtain the planned position of the ego vehicle at\
    \ time from the planning module, and check every NPC vehicle's predicted trajectory\
    \ from the prediction module to identify the one that is ahead of the ego vehicle\
    \ at time . Our implementation relies on a third party component provided by LawBreaker\
    \ [\\[44\\]](#page-11-1). In particular, we utilise the tool's specification language\
    \ and the corresponding verification algorithm.\n\nWe conducted experiments to\
    \ answer the following Research Questions (RQs):\n\nRQ1: Can REDriver be used\
    \ to enforce non-trivial specifications? RQ2: How much overhead is there for runtime\
    \ enforcement? RQ3: Does REDriver minimise the enforcement?\n\nRQ1 considers whether\
    \ REDriver achieves its primary goal of being able to enforce complex specifications\
    \ (i.e., beyond collision avoidance). RQ2 and RQ3 consider whether REDriver implements\
    \ its enforcement in a way that is practically reasonable. The former focuses\
    \ on the overhead of runtime enforcement, since AVs are expected to react quickly\
    \ on the road. The latter focuses on the\n\n<span id=\"page-7-0\"></span>\n\n\
    | traffic laws   |      | enforced? |          | improve | fail reason  | content\
    \                             |  |\n|----------------|------|-----------|----------|---------|--------------|-------------------------------------|--|\n\
    |                |      | 6.0<br>√  | 7.0<br>√ |         |              |    \
    \                                 |  |\n|                | sub1 | √         |\
    \ √        |         | -            | green light                         |  |\n\
    | Law38<br>Law46 | sub2 | √         | √        | +55.83% | -            | yellow\
    \ light                        |  |\n|                | sub3 | √         | √ \
    \       |         | -            | red light                           |  |\n\
    | Law44          |      | √         | √        | +30.00% | -            | lane\
    \ change                         |  |\n|                | sub2 |           | \
    \         | +44.00% | -            | speed limit                         |  |\n\
    |                | sub3 | ×         | ×        | -       | Lack support | speed\
    \ limit                         |  |\n| Law47          |      | ×         | ×\
    \        | -       | Lack support | overtake                            |  |\n\
    | Law51          | sub3 | ×<br>√    | ×<br>√   | -       | Lack support | traffic\
    \ light                       |  |\n|                | sub4 | √         | √  \
    \      | +35.00% | -            | traffic light                       |  |\n|\
    \                | sub5 |           |          |         | -            | traffic\
    \ light                       |  |\n| Law57          | sub1 | ×         | ×  \
    \      | -       | Lack support | left turn signal                    |  |\n|\
    \                | sub2 | ×         | ×        | -       | Lack support | right\
    \ turn signal<br>warning signal |  |\n| Law58          |      | ×         | ×\
    \        | -       | Lack support |                                     |  |\n\
    | Law59          |      | ×         | ×        | -       | Lack support | signals\
    \                             |  |\n\nTable 5: Violations of Chinese traffic laws\n\
    \nmagnitude of the repair to the original trajectory, i.e., the enforcement should\
    \ take place only if necessary and should minimally alter the behaviour of ADS.\n\
    \nOur experiments were run in the high-fidelity LGSVL simulator [\\[38\\]](#page-11-3).\
    \ Due to randomness in the simulator (mostly due to concurrency), each experiment\
    \ was executed 100 times and we report the averages. The threshold was determined\
    \ in a preliminary experiment in which we ran Apollo multiple times for each scenario\
    \ to get the range of the possible robustness values. All experiments were obtained\
    \ using two machines with 32GB of memory, an Intel i7-10700k CPU, and an RTX 2080Ti\
    \ graphics card. The machines respectively use Linux (Ubuntu 20.04.5 LTS) and\
    \ Windows (10 Pro).\n\nRQ1: Can REDriver be used to enforce non-trivial specifications?\
    \ To answer this question, we adopted the formalisation of traffic laws reported\
    \ in [\\[44\\]](#page-11-1) as our specification and evaluated whether REDriver\
    \ can be applied so that the ADS follows them. We remark the traffic laws are\
    \ rather complicated as they model 13 testable traffic laws with many sub-clauses.\
    \ Furthermore, we use the benchmark of scenarios provided by [\\[44\\]](#page-11-1)\
    \ in which Apollo is known to violate the specification. We replay these violation-inducing\
    \ scenarios with REDriver enabled, and report in Table [5](#page-7-0) whether\
    \ our approach is able to prevent the violations from occurring. Each 'subX' in\
    \ Table [5](#page-7-0) represents sub-rules of a traffic laws. For example, Law38\
    \ pertains to traffic light regulations and has three sub-rules, each covering\
    \ the yellow, green, and red lights, respectively. The 'enforced?' column indicates\
    \ whether the enforcement is successful. The enforcement was considered to be\
    \ successful if the average passing rate of the specification after the enforcement\
    \ is more than 50% across all the repetitions. Note that Apollo's passing rate\
    \ is always below 50% for the selected scenarios. The improve column in Table\
    \ [5](#page-7-0) reports the average improvement of REDriver over Apollo, i.e.,\
    \ the maximum enhancement achieved by REDriver across a threshold value spectrum\
    \ ranging from 0.0 to 1.2, with intervals of 0.1. The improvement is calculated\
    \ by subtracting the pass rate of Apollo from the pass rate of REDriver. Note\
    \ that since the sub laws of law38 and law51 are closely related, they are evaluated\
    \ together for avg improve. The only reason that we cannot enforce some of the\
    \ failed laws is that some simulator support is currently lacking. For example,\
    \ we generate a command to turn on fogLight to satisfy the law58 as shown in Example\
    \ [2.1](#page-2-2) and LGSVL's car model ignores the command since it currently\
    \ does not support fog lights. We mark Lack support in the table to illustrate\
    \ this. The detailed improvement for all thresholds is shown in Figure [3.](#page-7-1)\
    \ In this figure,\n\n<span id=\"page-7-1\"></span>![](_page_7_Figure_7.jpeg)\n\
    \nFigure 3: Improvement of performance across thresholds\n\nthe x-axis denotes\
    \ the threshold value (), while the y-axis signifies the average percentage improvement\
    \ of REDriver over Apollo. As shown in Figure [3,](#page-7-1) REDriver successfully\
    \ enforced all cases where an enforcement is feasible.\n\nTo explore RQ1 in more\
    \ detail, we designed a second experiment that focused on law38—one of the most\
    \ complicated formulae in [\\[44\\]](#page-11-1)—which specifies how a vehicle\
    \ should behave at a traffic light junction (i.e., the constraints on movements\
    \ due to green/yellow/red lights). We then selected three scenarios highly relevant\
    \ to this traffic law: Double Lined Junction, Single Direction Junction, and T\
    \ Junction. The detailed specification 38 is given in our website [\\[1\\]](#page-10-15).\
    \ Note that these scenarios were generated by Law-Breaker [\\[44\\]](#page-11-1)\
    \ to reliably induce traffic law violations in Apollo. The seeds for the fuzzing\
    \ algorithm are given on the website [\\[1\\]](#page-10-15).\n\nWe tested Apollo\
    \ with and without REDriver on each violationinducing scenario 100 times and recorded\
    \ the pass rate and average robustness with respect to law38. Table [6](#page-8-0)\
    \ presents the result of our evaluation using Apollo 7.0 (the results for version\
    \ 6.0 are on our website [\\[1\\]](#page-10-15)), where indicates threshold values.\
    \ Recall that the smaller the robustness value, the 'closer' is a violation.\n\
    \nAs can be seen from Table [6,](#page-8-0) REDriver significantly outperforms\
    \ the original Apollo in terms of respecting the specification. In scenarios \"\
    Double-Lined Junction\" and \"Single-Direction Junction\", the original Apollo\
    \ failed to pass at the green light because it is too conservative at the junction.\
    \ For instance, Apollo sometimes decides to stop before an intersection when there\
    \ is enough space for the vehicle to pass safely. REDriver avoided the violations\
    \ by enforcing the vehicle to drive within the junction first or pass the junction\
    \ directly. As a consequence, the average improvement to the pass rate is more\
    \ than 50% for scenario \"Double-Lined Junction\" and \"Single-Direction Junction\"\
    . For scenario \"T-Junction\", the improvement of REDriver is relatively small\
    \ since there is heavy traffic in this scenario and Apollo sometimes produces\
    \ the stop command. By design, the stop command has higher priority than the planned\
    \ trajectory since it prevents crashing in urgent situations. Therefore, the enforcement\
    \ did not take effect in some cases.\n\nFurthermore, the performance of REDriver\
    \ varies with threshold , i.e., being too small or too large both lead to degraded\
    \ performance. If is too small, for example 0 (which is equivalent to the driver\
    \ being ignorant of what is going to happen), sometimes the ADS cannot enforce\
    \ in time. But if is too large (e.g., 1.0-1.2 which is equivalent to the driver\
    \ being too scared of what might happen), REDriver is overcompensating and fixing\
    \ things it should\n\n<span id=\"page-8-0\"></span>Table 6: Performance comparison\
    \ of REDriver and Apollo\n\n| Scenario   | Driver    | \U0001D703   | pass/total\
    \ | robustness | avg time |\n|------------|-----------|-----|------------|------------|----------|\n\
    |            | Apollo7.0 | -   | 40/100     | 0.27       | 71.11s   |\n|     \
    \       | REDriver  | 0.0 | 73/100     | 0.67       | 55.18s   |\n|          \
    \  | REDriver  | 0.1 | 78/100     | 0.75       | 51.11s   |\n|            | REDriver\
    \  | 0.2 | 85/100     | 0.96       | 50.56s   |\n|            | REDriver  | 0.3\
    \ | 93/100     | 1.05       | 45.67s   |\n| Double     | REDriver  | 0.4 | 95/100\
    \     | 1.07       | 45.71s   |\n| Lined      | REDriver  | 0.5 | 93/100     |\
    \ 1.10       | 45.32s   |\n| Junction   | REDriver  | 0.6 | 98/100     | 1.19\
    \       | 44.92s   |\n|            | REDriver  | 0.7 | 99/100     | 1.21     \
    \  | 44.90s   |\n|            | REDriver  | 0.8 | 96/100     | 1.15       | 45.13s\
    \   |\n|            | REDriver  | 0.9 | 99/100     | 1.20       | 45.07s   |\n\
    |            | REDriver  | 1.0 | 80/100     | 0.78       | 51.90s   |\n|     \
    \       | REDriver  | 1.1 | 75/100     | 0.71       | 52.71s   |\n|          \
    \  | REDriver  | 1.2 | 81/100     | 0.80       | 53.10s   |\n|            | Apollo7.0\
    \ | -   | 18/100     | 0.18       | 57.60s   |\n|            | REDriver  | 0.0\
    \ | 26/100     | 0.38       | 56.38s   |\n|            | REDriver  | 0.1 | 24/100\
    \     | 0.37       | 55.93s   |\n|            | REDriver  | 0.2 | 85/100     |\
    \ 0.84       | 55.95s   |\n|            | REDriver  | 0.3 | 89/100     | 0.86\
    \       | 55.75s   |\n| Single     | REDriver  | 0.4 | 89/100     | 0.89     \
    \  | 55.69s   |\n| Direction  | REDriver  | 0.5 | 88/100     | 0.87       | 56.71s\
    \   |\n| Junction   | REDriver  | 0.6 | 91/100     | 0.92       | 56.18s   |\n\
    |            | REDriver  | 0.7 | 95/100     | 0.99       | 56.13s   |\n|     \
    \       | REDriver  | 0.8 | 93/100     | 0.96       | 57.09s   |\n|          \
    \  | REDriver  | 0.9 | 96/100     | 1.02       | 57.55s   |\n|            | REDriver\
    \  | 1.0 | 35/100     | 0.56       | 57.10s   |\n|            | REDriver  | 1.1\
    \ | 31/100     | 0.45       | 56.79s   |\n|            | REDriver  | 1.2 | 24/100\
    \     | 0.33       | 56.61s   |\n|            | Apollo7.0 | -   | 45/100     |\
    \ 0.29       | 64.66s   |\n|            | REDriver  | 0.0 | 41/100     | 0.32\
    \       | 64.10s   |\n|            | REDriver  | 0.1 | 45/100     | 0.43     \
    \  | 63.93s   |\n|            | REDriver  | 0.2 | 46/100     | 0.59       | 60.95s\
    \   |\n|            | REDriver  | 0.3 | 50/100     | 0.77       | 61.82s   |\n\
    |            | REDriver  | 0.4 | 49/100     | 0.79       | 60.76s   |\n| T-Junction\
    \ | REDriver  | 0.5 | 53/100     | 0.45       | 62.30s   |\n|            | REDriver\
    \  | 0.6 | 55/100     | 0.39       | 61.92s   |\n|            | REDriver  | 0.7\
    \ | 50/100     | 0.41       | 61.70s   |\n|            | REDriver  | 0.8 | 51/100\
    \     | 0.35       | 62.89s   |\n|            | REDriver  | 0.9 | 42/100     |\
    \ 0.33       | 63.23s   |\n|            | REDriver  | 1.0 | 43/100     | 0.37\
    \       | 63.45s   |\n|            | REDriver  | 1.1 | 40/100     | 0.40     \
    \  | 63.70s   |\n|            | REDriver  | 1.2 | 39/100     | 0.41       | 64.07s\
    \   |\n\nnot, which can lead to unexpected ADS behaviour. Note that a significant\
    \ number of valid trajectories have a robustness of 1, and such an overreaction\
    \ is more likely to occur for ≥ 1.\n\nRQ2: How much overhead does the runtime\
    \ enforcement impose? To answer this question, we collect information on the running\
    \ time of the plan validation module of REDriver for different scenarios. The\
    \ overhead for control validation is very small (less than 0.01% of the the overhead\
    \ of the plan validation module), and we ignore it in the later experiment. The\
    \ detailed data for REDriver based on Apollo 7.0 is shown in Table [7](#page-8-1)\
    \ (the results for version 6.0 are shown on our website [\\[1\\]](#page-10-15)).\
    \ Here, S1-S3 corresponds to the Double Lined Junction, Single-Direction Junction,\
    \ and T-Junction as in Table [6,](#page-8-0) avg fix represents the average number\
    \ of fixes during a test that successfully enables the ADS to follow the specification,\
    \ max fix represents the maximum number of fixes detected across the test cases,\
    \ avg(ms) means the average time consumption of the\n\nTable 7: Overhead of REDriver\n\
    \n<span id=\"page-8-1\"></span>\n\n|    | \U0001D703   | avg fix | max fix | fix\
    \ (%) | avg(ms) | max(ms) | time (%) |  |\n|----|-----|---------|---------|---------|---------|---------|----------|--|\n\
    | S1 | 0.0 | 26.08   | 35      | 5.09%   | 1.92    | 6.82    | 4.88%    |  |\n\
    |    | 0.1 | 24.19   | 35      | 4.76%   | 1.88    | 9.11    | 4.81%    |  |\n\
    |    | 0.2 | 19.20   | 35      | 4.15%   | 1.90    | 9.10    | 4.72%    |  |\n\
    |    | 0.3 | 18.33   | 32      | 3.57%   | 1.87    | 9.23    | 5.05%    |  |\n\
    |    | 0.4 | 22.33   | 35      | 3.89%   | 1.85    | 9.86    | 4.98%    |  |\n\
    |    | 0.5 | 33.12   | 45      | 6.19%   | 1.91    | 8.81    | 4.90%    |  |\n\
    |    | 0.6 | 32.06   | 45      | 6.07%   | 1.72    | 9.08    | 4.89%    |  |\n\
    |    | 0.7 | 33.20   | 45      | 6.19%   | 1.88    | 9.12    | 5.04%    |  |\n\
    |    | 0.8 | 39.75   | 93      | 7.31%   | 1.95    | 9.10    | 4.92%    |  |\n\
    |    | 0.9 | 40.18   | 102     | 7.45%   | 1.96    | 10.53   | 4.85%    |  |\n\
    |    | 1.0 | 87.20   | 230     | 17.29%  | 2.13    | 11.15   | 6.35%    |  |\n\
    |    | 1.1 | 86.02   | 231     | 17.14%  | 2.05    | 10.55   | 6.01%    |  |\n\
    |    | 1.2 | 86.05   | 230     | 16.79%  | 2.11    | 11.13   | 6.26%    |  |\n\
    |    | 0.0 | 10.22   | 17      | 2.51%   | 1.67    | 7.01    | 4.52%    |  |\n\
    |    | 0.1 | 10.71   | 17      | 2.55%   | 1.53    | 6.97    | 4.49%    |  |\n\
    |    | 0.2 | 10.56   | 20      | 2.78%   | 1.55    | 6.74    | 4.51%    |  |\n\
    |    | 0.3 | 12.05   | 22      | 2.93%   | 1.56    | 6.72    | 4.22%    |  |\n\
    |    | 0.4 | 12.10   | 22      | 2.90%   | 1.55    | 7.15    | 4.21%    |  |\n\
    |    | 0.5 | 12.21   | 20      | 2.95%   | 1.66    | 6.97    | 4.38%    |  |\n\
    | S2 | 0.6 | 12.23   | 20      | 2.98%   | 1.54    | 7.53    | 4.31%    |  |\n\
    |    | 0.7 | 12.48   | 22      | 2.73%   | 1.52    | 7.19    | 4.47%    |  |\n\
    |    | 0.8 | 14.35   | 22      | 3.11%   | 1.54    | 6.80    | 4.19%    |  |\n\
    |    | 0.9 | 14.75   | 20      | 3.60%   | 1.49    | 6.78    | 4.28%    |  |\n\
    |    | 1.0 | 170.15  | 177     | 42.57%  | 1.88    | 9.15    | 5.89%    |  |\n\
    |    | 1.1 | 169.12  | 185     | 40.82%  | 1.75    | 9.23    | 5.30%    |  |\n\
    |    | 1.2 | 169.20  | 177     | 41.17%  | 2.09    | 9.09    | 5.61%    |  |\n\
    |    | 0.0 | 38.92   | 51      | 9.02%   | 1.63    | 9.14    | 4.18%    |  |\n\
    |    | 0.1 | 37.03   | 49      | 8.87%   | 1.67    | 9.21    | 4.50%    |  |\n\
    | S3 | 0.2 | 37.71   | 49      | 8.89%   | 1.70    | 9.45    | 4.39%    |  |\n\
    |    | 0.3 | 37.28   | 49      | 8.96%   | 1.84    | 9.21    | 4.54%    |  |\n\
    |    | 0.4 | 35.22   | 52      | 8.75%   | 1.82    | 9.50    | 4.43%    |  |\n\
    |    | 0.5 | 37.33   | 49      | 9.01%   | 1.71    | 9.34    | 4.70%    |  |\n\
    |    | 0.6 | 34.70   | 52      | 8.05%   | 1.69    | 10.13   | 4.90%    |  |\n\
    |    | 0.7 | 33.13   | 52      | 7.80%   | 1.75    | 9.12    | 4.82%    |  |\n\
    |    | 0.8 | 35.56   | 40      | 8.35%   | 1.70    | 9.29    | 4.88%    |  |\n\
    |    | 0.9 | 32.46   | 40      | 7.76%   | 1.77    | 9.17    | 4.91%    |  |\n\
    |    | 1.0 | 233.73  | 298     | 52.05%  | 2.19    | 11.21   | 5.83%    |  |\n\
    |    | 1.1 | 232.76  | 298     | 52.18%  | 2.27    | 11.20   | 5.79%    |  |\n\
    |    | 1.2 | 234.46  | 298     | 51.99%  | 2.22    | 11.34   | 5.85%    |  |\n\
    \nplan validation module in one run, max(ms) indicates the maximum time consumption\
    \ detected, fix (%) is calculated by dividing the average fixes by the average\
    \ updates of the planned trajectory during a run, and time (%) is calculated by\
    \ dividing the average time consumption of the plan validation module by the average\
    \ time consumption of the production of a planned trajectory. The time units in\
    \ Table [7](#page-8-1) are all milliseconds.\n\nAs can be seen from Table [7,](#page-8-1)\
    \ the time consumption of the plan validation module is practical, i.e., the average\
    \ time consumption is always smaller than 2.5 milliseconds, the max time consumption\
    \ is always smaller than 12 milliseconds, and the time percent is always within\
    \ 6%. Furthermore, the number of fixes is related to the value of as expected.\
    \ There is a large increase in the number of fixes and fix percent for a large\
    \ ℎℎ ≥ 1.0 across all three scenarios. This is consistent with the performance\
    \ degradation at ℎℎ ≥ 1.0 shown in Table [6](#page-8-0) since unnecessary fixes\
    \ cause problems.\n\nRQ3: Does REDriver minimise the enforcement? To answer this\
    \ question, first, recall our approach as described in Section [3.](#page-3-4)\
    \ We identify the smallest k with (, ) < . Hence, our fix applies\n\n<span id=\"\
    page-9-0\"></span>![](_page_9_Figure_2.jpeg)\n\nFigure 4: Magnitude of modifications\
    \ to planned trajectories\n\nonly to the earliest part of the planned trajectory\
    \ that leads to the near-violation of the specification. In most cases, we only\
    \ modify one variable at one time step, such as the \"speed\" at some time step.\
    \ For some rare cases, we may modify multiple variables at one time step, such\
    \ as the \"speed\" and \"position\", if modifying one single variable is not sufficient.\
    \ Note that the ADS updates the planned trajectory based on current perceptions\
    \ and predictions and the impact of our change does not accumulate.\n\nHere, we\
    \ require a method to assess the variance between the modified planned trajectory\
    \ and the original trajectory. This quantification is calculated by assessing\
    \ the positional variance between these trajectories. When alterations are made\
    \ to the speed or acceleration, we translate these changes into positional differences.\
    \ To be precise, the conversion for speed discrepancies is determined as follows:\
    \ (′ − ) · , and for acceleration discrepancies: (′ −) · 2 , where signifies the\
    \ time interval between the current planned waypoint and the subsequent waypoint.\
    \ For instance, if a speed adjustment of magnitude 2/ is applied to a planned\
    \ waypoint, and the time interval is 0.2, then the positional difference is calculated\
    \ as 2/ · 0.1 = 0.2. The magnitude of modifications is shown in Figure [4.](#page-9-0)\
    \ In this figure, the x-axis represents the threshold value , and the y-axis represents\
    \ the magnitude of the modification of REDriver in meters. The graph presented\
    \ in the figure denotes the average/max modification value of REDriver across\
    \ thousands of fixes. Notably, for thresholds ranging from 0.0 to 1.2, the average\
    \ difference consistently remains below 1 meter. This observation suggests that\
    \ REDriver's modification on the planned trajectory is small. Note that there\
    \ is a significant increase in max difference for threshold 1.0. This phenomenon\
    \ is attributed to an excessive number of unnecessary fixes, as explained in RQ1.\n\
    \nIn addition, the average running time for the test cases is listed in the last\
    \ column of Table [6.](#page-8-0) Here, avg time represents the average time spent\
    \ by the ADS to travel from the start point to the destination. As can be seen\
    \ from the last column of Table [6,](#page-8-0) the running time did not increase\
    \ across all these test cases. This indicates that REDriver did not, in practice,\
    \ force the ADS to produce a substantially different trajectory to follow (e.g.,\
    \ halting the car). Note in addition that the time consumption of REDriver has\
    \ dropped substantially compared to the original Apollo in the scenario \"Double-Lined\
    \ Junction\". This is because Apollo hesitated at the green light, while REDriver\
    \ successfully passed through.\n\n# 5 RELATED WORK\n\nRuntime verification approaches\
    \ monitor messages obtained from ADSs and evaluate them against a specification\
    \ using a number of different techniques. For instance, Kane et al. [\\[26\\]](#page-10-16)\
    \ generate a system trace from the observed network state, and Heffernan et al.\
    \ [\\[24\\]](#page-10-17) use system-on-a-chip based monitors as sources of information.\
    \ Watanabe et al. [\\[45\\]](#page-11-8) focus on runtime monitoring of the controller\
    \ safety properties of advanced driver-assistance systems (ADASs). Mauritz et\
    \ al. [\\[30\\]](#page-10-18) generate monitors for ADAS features from safety\
    \ requirements and by training on simulators. D'Angelo et al. [\\[10\\]](#page-10-19)\
    \ present Lola, a simple and expressive specification language to describe both\
    \ correctness/failure assertions, which has been successfully deployed on autonomous\
    \ vehicles in addition to many successful flight deployments. Note that there\
    \ is no enforcement of specifications in the works mentioned above.\n\nRuntime\
    \ enforcement goes beyond monitoring and attempts to enforce certain safety properties.\
    \ Existing works [\\[8,](#page-10-20) [21,](#page-10-6) [25,](#page-10-21) [43\\\
    ]](#page-11-9) already propose a few methods for runtime enforcement of ADSs.\
    \ AVGuardian [\\[25\\]](#page-10-21) performs static analysis of the communication\
    \ messages between the ADS modules to generate control policies and enforce them\
    \ during runtime. Guardauto et al. [\\[8\\]](#page-10-20) divide the ADS into\
    \ a few partitions for the detection of rogue behaviours and restart the partition\
    \ in order to clear them. Shankaro et al. [\\[43\\]](#page-11-9) define a policy\
    \ using an automaton and enforce the car to stop when the policy is violated.\
    \ Grieser et al. [\\[21\\]](#page-10-6) build an end-to-end neural network (from\
    \ LIDAR to torques/steering) that implicitly picks up safety rules. Simultaneously,\
    \ the distance to obstacles on the current trajectory is monitored and emergency\
    \ brakes are applied if a collision is likely. Generally, when enforcement for\
    \ an ADS takes place in these works, it tends to be quite 'weak', (e.g. emergency\
    \ brake). REDriver, on the other hand, provides runtime enforcement for a rich\
    \ specification in ways that are less intrusive.\n\nIn addition, there are existing\
    \ works for cyber-physical systems [\\[37,](#page-11-10) [48,](#page-11-11) [49\\\
    ]](#page-11-12). Pinisetty et al. [\\[37\\]](#page-11-10) formalise the runtime\
    \ enforcement problem for CPSs, where policies depend not only on a controller\
    \ but also an environment. Another approach, Safety Guard [\\[49\\]](#page-11-12),\
    \ adds automata-based reactive components to the original system, which react\
    \ to ensure a predefined set of safety properties, while also keeping the deviation\
    \ from the original system to a minimum. ModelPlex [\\[32\\]](#page-11-13) checks\
    \ for model compliance of cyber-physical systems and includes a fail-safe action\
    \ to avoid violations of safety properties. CBSA [\\[36\\]](#page-11-14) proposes\
    \ the idea of integrating assume-guarantee reasoning to allow runtime assurance\
    \ of cyber-physical systems. These works are relevant to the runtime enforcement\
    \ of ADSs since ADSs are cyber-physical systems as well. However, we can not directly\
    \ apply these methods to ADSs and customization of the enforcement techniques\
    \ is necessary due to the unique requirements and challenges posed by ADSs. For\
    \ instance, the enforcement of ADSs requires consideration of not only the current\
    \ control commands but also the planned trajectory.\n\nRuntime enforcement is\
    \ not limited to ADSs, i.e., there are works providing runtime enforcement/verification\
    \ for general systems (e.g., [\\[5,](#page-10-22) [7,](#page-10-23) [11,](#page-10-24)\
    \ [12,](#page-10-25) [16,](#page-10-26) [17,](#page-10-27) [19,](#page-10-28)\
    \ [28,](#page-10-29) [33,](#page-11-15) [39](#page-11-16)[–42,](#page-11-17) [47\\\
    ]](#page-11-18)). The Simplex architecture [\\[5,](#page-10-22) [42\\]](#page-11-17)\
    \ introduces the idea of \"runtime enforcement\" to enhance the reliability of\
    \ complex software, and has been widely adopted in both academia and industry.\
    \ Shield synthesis [\\[7\\]](#page-10-23) proposes a method\n\nof runtime enforcement\
    \ for reactive systems while also minimising interference to the original behaviour.\
    \ Schneider [\\[40\\]](#page-11-19) looks at runtime enforcement of security policies\
    \ and stops the program when they are violated. Falcone et al. [\\[16\\]](#page-10-26)\
    \ propose enforcement by buffering actions and dumping them only when deemed safe.\
    \ Ligatti et al. [\\[28\\]](#page-10-29) use 'edit automata' to respond to dangerous\
    \ actions by suppressing them or inserting other actions. Desai et al. [\\[11\\\
    ]](#page-10-24) enforce the plan trajectory of mobile robots so as to follow STL\
    \ specifications. Expanding upon this idea, Soter [\\[12\\]](#page-10-25) allows\
    \ for safety properties to be specified and enforced in robotic systems. Tools\
    \ such as TuLip [\\[47\\]](#page-11-18) and LTLMoP [\\[19\\]](#page-10-28) synthesize\
    \ trajectories to assist evaluation of the control system under linear temporal\
    \ logic (LTL) specifications. Barron Associates provide a comprehensive study\
    \ of runtime enforcement architecture for highly adaptive flight ontrol systems\
    \ [\\[39\\]](#page-11-16). The Copilot tool [\\[33\\]](#page-11-15) offers a comprehensive\
    \ runtime enforcement environment that incorporates numerous operating-system-like\
    \ functionalities. The R2U2 [\\[41\\]](#page-11-20) monitors the security properties\
    \ of on-board Unmanned Aerial Systems (UAS) and is implemented in FPGA hardware.\
    \ Unfortunately, many existing general runtime enforcement/verification methods\
    \ are not suitable for ADSs due to their safety-critical and highly interactive\
    \ nature. The survey paper by Falcone et al. [\\[17\\]](#page-10-27) on existing\
    \ runtime enforcement/verification tools clarifies that many 'reactions' provided\
    \ by general runtime verification tools are weak, which is not acceptable for\
    \ our situation. In this paper, we propose a runtime enforcement method applicable\
    \ to any given specification with acceptable overhead for ADSs, and our method\
    \ concerns not only the current driving conditions but also the ADS's future plans.\n\
    \n# 6 CONCLUSION\n\nWe proposed, REDriver, a solution to the runtime enforcement\
    \ problem for ADSs. REDriver supports the enforcement of complex user-provided\
    \ specifications such as national traffic laws in a way which is similar to experienced\
    \ human drivers, i.e., based on nearfuture predictions and proactively correcting\
    \ the vehicle's trajectory accordingly with minimal adjustment.\n\n# ACKNOWLEDGMENT\n\
    \nWe are grateful to the anonymous ICSE referees for their insights and feedback,\
    \ which have helped to improve this paper. This research is supported by the Ministry\
    \ of Education, Singapore under its Academic Research Fund Tier 3 (Award ID: MOET32020-0004).\
    \ Any opinions, findings and conclusions or recommendations expressed in this\
    \ material are those of the author(s) and do not reflect the views of the Ministry\
    \ of Education, Singapore.\n\n# REFERENCES\n\n- <span id=\"page-10-15\"></span>[1]\
    \ 2023. REDriver Source Codes. [https://redriver2023.github.io/.](https://redriver2023.github.io/)\
    \ Online; accessed Jan 2024.\n- <span id=\"page-10-9\"></span><span id=\"page-10-8\"\
    ></span>[2] Autoware.AI. 2022. Autoware.AI. [www.autoware.ai/.](www.autoware.ai/)\
    \ Online; accessed Jan 2024. [3] Baidu. 2019. APOLLO 6.0. [https://github.com/ApolloAuto/apollo/releases/tag/v6.](https://github.com/ApolloAuto/apollo/releases/tag/v6.0.0)\
    \ [0.0.](https://github.com/ApolloAuto/apollo/releases/tag/v6.0.0) Online; accessed\
    \ Jan 2024.\n- <span id=\"page-10-7\"></span>[4] Baidu. 2022. APOLLO 7.0. [https://github.com/ApolloAuto/apollo/releases/tag/v7.](https://github.com/ApolloAuto/apollo/releases/tag/v7.0.0)\
    \ [0.0.](https://github.com/ApolloAuto/apollo/releases/tag/v7.0.0) Online; accessed\
    \ Jan 2024.\n- <span id=\"page-10-22\"></span>[5] Stanley Bak, Deepti K Chivukula,\
    \ Olugbemiga Adekunle, Mu Sun, Marco Caccamo, and Lui Sha. 2009. The system-level\
    \ simplex architecture for improved real-time embedded system safety. In 2009\
    \ 15th IEEE Real-Time and Embedded Technology and Applications Symposium. IEEE,\
    \ 99–107.\n- <span id=\"page-10-3\"></span>[6] Sai Krishna Bashetty, Heni Ben\
    \ Amor, and Georgios Fainekos. 2020. DeepCrashTest: Turning Dashcam Videos into\
    \ Virtual Crash Tests for Automated Driving\n\nSystems. In 2020 IEEE International\
    \ Conference on Robotics and Automation, ICRA. Paris, France, 11353–11360.\n\n\
    - <span id=\"page-10-23\"></span>[7] Roderick Bloem, Bettina Könighofer, Robert\
    \ Könighofer, and Chao Wang. 2015. Shield Synthesis: - Runtime Enforcement for\
    \ Reactive Systems. In TACAS'15 (Lecture Notes in Computer Science, Vol. 9035).\
    \ Springer, 533–548.\n- <span id=\"page-10-20\"></span>[8] Kun Cheng, Yuan Zhou,\
    \ Bihuan Chen, Rui Wang, Yuebin Bai, and Yang Liu. 2021. Guardauto: A Decentralized\
    \ Runtime Protection System for Autonomous Driving. IEEE Trans. Computers 70,\
    \ 10 (2021), 1569–1581.\n- <span id=\"page-10-10\"></span>[9] Chinese Government.\
    \ 2021. Regulations for the Implementation of the Road Traffic Safety Law of the\
    \ People's Republic of China. [http://www.gov.cn/gongbao/](http://www.gov.cn/gongbao/content/2004/content_62772.htm)\
    \ [content/2004/content\\\\_62772.htm.](http://www.gov.cn/gongbao/content/2004/content_62772.htm)\
    \ Online; accessed Jan 2024.\n- <span id=\"page-10-19\"></span>[10] Ben d'Angelo,\
    \ Sriram Sankaranarayanan, César Sánchez, Will Robinson, Bernd Finkbeiner, Henny\
    \ B Sipma, Sandeep Mehrotra, and Zohar Manna. 2005. LOLA: runtime monitoring of\
    \ synchronous systems. In 12th International Symposium on Temporal Representation\
    \ and Reasoning (TIME'05). IEEE, 166–174.\n- <span id=\"page-10-24\"></span>[11]\
    \ Ankush Desai, Tommaso Dreossi, and Sanjit A Seshia. 2017. Combining model checking\
    \ and runtime verification for safe robotics. In Runtime Verification: 17th International\
    \ Conference, RV 2017, Seattle, WA, USA, September 13-16, 2017, Proceedings. Springer,\
    \ 172–189.\n- <span id=\"page-10-25\"></span>[12] Ankush Desai, Shromona Ghosh,\
    \ Sanjit A Seshia, Natarajan Shankar, and Ashish Tiwari. 2019. SOTER: a runtime\
    \ assurance framework for programming safe robotics systems. In 2019 49th Annual\
    \ IEEE/IFIP International Conference on Dependable Systems and Networks (DSN).\
    \ IEEE, 138–150.\n- <span id=\"page-10-12\"></span>[13] Jyotirmoy V Deshmukh,\
    \ Alexandre Donzé, Shromona Ghosh, Xiaoqing Jin, Garvit Juniwal, and Sanjit A\
    \ Seshia. 2017. Robust online monitoring of signal temporal logic. Formal Methods\
    \ in System Design 51, 1 (2017), 5–30.\n- <span id=\"page-10-0\"></span>[14] Vinayak\
    \ V Dixit, Sai Chand, and Divya J Nair. 2016. Autonomous vehicles: disengagements,\
    \ accidents and reaction times. PLoS one 11, 12 (2016), e0168054.\n- <span id=\"\
    page-10-5\"></span>[15] Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio\
    \ Lopez, and Vladlen Koltun. 2017. CARLA: An open urban driving simulator. In\
    \ Conference on Robot Learning. 1–16.\n- <span id=\"page-10-26\"></span>[16] Yliès\
    \ Falcone, Jean-Claude Fernandez, and Laurent Mounier. 2012. What can you verify\
    \ and enforce at runtime? Int. J. Softw. Tools Technol. Transf. 14, 3 (2012),\
    \ 349–382.\n- <span id=\"page-10-27\"></span>[17] Yliès Falcone, Srdan Krstic,\
    \ Giles Reger, and Dmitriy Traytel. 2021. A taxonomy for classifying runtime verification\
    \ tools. Int. J. Softw. Tools Technol. Transf. 23, 2 (2021), 255–284.\n- <span\
    \ id=\"page-10-1\"></span>[18] Francesca M Favarò, Nazanin Nader, Sky O Eurich,\
    \ Michelle Tripp, and Naresh Varadaraju. 2017. Examining accident reports involving\
    \ autonomous vehicles in California. PLoS one 12, 9 (2017), e0184952.\n- <span\
    \ id=\"page-10-28\"></span>[19] Cameron Finucane, Gangyuan Jing, and Hadas Kress-Gazit.\
    \ 2010. LTLMoP: Experimenting with language, temporal logic and robot control.\
    \ In 2010 IEEE/RSJ International Conference on Intelligent Robots and Systems.\
    \ IEEE, 1988–1993.\n- <span id=\"page-10-14\"></span>[20] Yann Gilpin, Vince Kurtz,\
    \ and Hai Lin. 2020. A smooth robustness measure of signal temporal logic for\
    \ symbolic control. IEEE Control Systems Letters 5, 1 (2020), 241–246.\n- <span\
    \ id=\"page-10-6\"></span>[21] Jörg Grieser, Meng Zhang, Tim Warnecke, and Andreas\
    \ Rausch. 2020. Assuring the Safety of End-to-End Learning-Based Autonomous Driving\
    \ through Runtime Monitoring. In DSD. IEEE, 476–483.\n- <span id=\"page-10-13\"\
    ></span>[22] Andreas Griewank et al. 1989. On automatic differentiation. Mathematical\
    \ Programming: recent developments and applications 6, 6 (1989), 83–107.\n- <span\
    \ id=\"page-10-2\"></span>[23] Rong Gu, Raluca Marinescu, Cristina Seceleanu,\
    \ and Kristina Lundqvist. 2019. Towards a Two-Layer Framework for Verifying Autonomous\
    \ Vehicles. In NFM (Lecture Notes in Computer Science, Vol. 11460). Springer,\
    \ 186–203.\n- <span id=\"page-10-17\"></span>[24] Donal Heffernan, Ciaran MacNamee,\
    \ and Padraig Fogarty. 2014. Runtime verification monitoring for automotive embedded\
    \ systems using the ISO 26262 functional safety standard as a guide for the definition\
    \ of the monitored properties. IET Softw. 8, 5 (2014), 193–203.\n- <span id=\"\
    page-10-21\"></span>[25] David Ke Hong, John Kloosterman, Yuqi Jin, Yulong Cao,\
    \ Qi Alfred Chen, Scott Mahlke, and Z Morley Mao. 2020. AVGuardian: Detecting\
    \ and mitigating publishsubscribe overprivilege for autonomous vehicle systems.\
    \ In 2020 IEEE European Symposium on Security and Privacy (EuroS&P). IEEE, 445–459.\n\
    - <span id=\"page-10-16\"></span>[26] Aaron Kane, Omar Chowdhury, Anupam Datta,\
    \ and Philip Koopman. 2015. A Case Study on Runtime Monitoring of an Autonomous\
    \ Research Vehicle (ARV) System. In RV'15 (Lecture Notes in Computer Science,\
    \ Vol. 9333). Springer, 102–117.\n- <span id=\"page-10-4\"></span>[27] Guanpeng\
    \ Li, Yiran Li, Saurabh Jha, Timothy Tsai, Michael Sullivan, Siva Kumar Sastry\
    \ Hari, Zbigniew Kalbarczyk, and Ravishankar Iyer. 2020. AV-FUZZER: Finding safety\
    \ violations in autonomous driving systems. In 2020 IEEE 31st International Symposium\
    \ on Software Reliability Engineering (ISSRE). IEEE, 25–36.\n- <span id=\"page-10-29\"\
    ></span>[28] Jay Ligatti, Lujo Bauer, and David Walker. 2009. Run-Time Enforcement\
    \ of Nonsafety Policies. ACM Trans. Inf. Syst. Secur. 12, 3 (2009), 19:1–19:41.\n\
    - <span id=\"page-10-11\"></span>[29] Oded Maler and Dejan Nickovic. 2004. Monitoring\
    \ temporal properties of continuous signals. In Formal Techniques, Modelling and\
    \ Analysis of Timed and Fault-Tolerant Systems. 152–166.\n- <span id=\"page-10-18\"\
    ></span>[30] Malte Mauritz, Falk Howar, and Andreas Rausch. 2016. Assuring the\
    \ Safety of Advanced Driver Assistance Systems Through a Combination of Simulation\
    \ and Runtime Monitoring. In ISoLA (2) (Lecture Notes in Computer Science, Vol.\
    \ 9953). 672–687.\n\n<span id=\"page-11-0\"></span>\n\n- <span id=\"page-11-4\"\
    ></span>[31] Roger L McCarthy. 2022. Autonomous vehicle accident data analysis:\
    \ California OL 316 reports: 2015–2020. ASCE-ASME Journal of Risk and Uncertainty\
    \ in Engineering Systems, Part B: Mechanical Engineering 8, 3 (2022), 034502.\n\
    - <span id=\"page-11-13\"></span>[32] Stefan Mitsch and André Platzer. 2016. ModelPlex:\
    \ Verified runtime validation of verified cyber-physical system models. Formal\
    \ Methods in System Design 49 (2016), 33–74.\n- <span id=\"page-11-15\"></span>[33]\
    \ NASA. 2023. Copilot. [https://nari.arc.nasa.gov/sws-tc3-diagram/capability/](https://nari.arc.nasa.gov/sws-tc3-diagram/capability/copilot/)\
    \ [copilot/.](https://nari.arc.nasa.gov/sws-tc3-diagram/capability/copilot/) Online;\
    \ accessed Jan 2024.\n- <span id=\"page-11-5\"></span>[34] Dejan Ničković and\
    \ Tomoya Yamaguchi. 2020. RTAMT: Online robustness monitors from STL. In International\
    \ Symposium on Automated Technology for Verification and Analysis. 564–571.\n\
    - <span id=\"page-11-7\"></span>[35] Yash Vardhan Pant, Houssam Abbas, and Rahul\
    \ Mangharam. 2017. Smooth operator: Control using the smooth robustness of temporal\
    \ logic. In 2017 IEEE Conference on Control Technology and Applications (CCTA).\
    \ IEEE, 1235–1240.\n- <span id=\"page-11-14\"></span>[36] Dung Phan, Junxing Yang,\
    \ Matthew Clark, Radu Grosu, John Schierman, Scott Smolka, and Scott Stoller.\
    \ 2017. A component-based simplex architecture for high-assurance cyber-physical\
    \ systems. In 2017 17th International Conference on Application of Concurrency\
    \ to System Design (ACSD). IEEE, 49–58.\n- <span id=\"page-11-10\"></span>[37]\
    \ Srinivas Pinisetty, Partha S. Roop, Steven Smyth, Nathan Allen, Stavros Tripakis,\
    \ and Reinhard von Hanxleden. 2017. Runtime Enforcement of Cyber-Physical Systems.\
    \ ACM Trans. Embed. Comput. Syst. 16, 5s (2017), 178:1–178:25.\n- <span id=\"\
    page-11-3\"></span>[38] Guodong Rong, Byung Hyun Shin, Hadi Tabatabaee, Qiang\
    \ Lu, Steve Lemke, Marti ¯ n,š Možeiko, Eric Boise, Geehoon Uhm, Mark Gerow, Shalin\
    \ Mehta, et al. 2020. LGSVL simulator: A high fidelity simulator for autonomous\
    \ driving. In 2020 IEEE 23rd International Conference on Intelligent Transportation\
    \ Systems (ITSC). 1–6.\n- <span id=\"page-11-16\"></span>[39] John D Schierman,\
    \ Michael D DeVore, Nathan D Richards, Neha Gandhi, Jared K Cooper, Kenneth R\
    \ Horneman, Scott Stoller, and Scott Smolka. 2015. Runtime assurance framework\
    \ development for highly adaptive flight control systems. Technical Report. Barron\
    \ Associates, Inc. Charlottesville.\n- <span id=\"page-11-19\"></span>[40] Fred\
    \ B. Schneider. 2000. Enforceable security policies. ACM Trans. Inf. Syst. Secur.\
    \ 3, 1 (2000), 30–50.\n- <span id=\"page-11-20\"></span>[41] Johann Schumann,\
    \ Patrick Moosbrugger, and Kristin Y Rozier. 2015. R2U2: monitoring and diagnosis\
    \ of security threats for unmanned aerial systems. In Runtime Verification: 6th\
    \ International Conference, RV 2015, Vienna, Austria, September 22-25, 2015. Proceedings.\
    \ Springer, 233–249.\n- <span id=\"page-11-17\"></span>[42] Lui Sha et al. 2001.\
    \ Using simplicity to control complexity. IEEE Software 18, 4 (2001), 20–28.\n\
    - <span id=\"page-11-9\"></span>[43] Saumya Shankar, Ujwal V. R, Srinivas Pinisetty,\
    \ and Partha S. Roop. 2020. Formal Runtime Monitoring Approaches for Autonomous\
    \ Vehicles. In OVERLAY'20 (CEUR Workshop Proceedings, Vol. 2785). CEUR-WS.org,\
    \ 89–94.\n- <span id=\"page-11-1\"></span>[44] Yang Sun, Christopher M. Poskitt,\
    \ Jun Sun, Yuqi Chen, and Zijiang Yang. 2022. LawBreaker: An Approach for Specifying\
    \ Traffic Laws and Fuzzing Autonomous Vehicles. In ASE. ACM, 62:1–62:12.\n- <span\
    \ id=\"page-11-8\"></span>[45] Kosuke Watanabe, Eunsuk Kang, Chung-Wei Lin, and\
    \ Shinichi Shiraishi. 2018. Runtime monitoring for safety of intelligent vehicles.\
    \ In DAC. ACM, 31:1–31:6.\n- <span id=\"page-11-6\"></span>[46] Ching-Feng Wen\
    \ and Hsien-Chung Wu. 2012. Using the parametric approach to solve the continuous-time\
    \ linear fractional max–min problems. Journal of Global Optimization 54, 1 (2012),\
    \ 129–153.\n- <span id=\"page-11-18\"></span>[47] Tichakorn Wongpiromsarn, Ufuk\
    \ Topcu, Necmiye Ozay, Huan Xu, and Richard M Murray. 2011. TuLiP: a software\
    \ toolbox for receding horizon temporal logic planning. In Proceedings of the\
    \ 14th international conference on Hybrid systems: computation and control. 313–314.\n\
    - <span id=\"page-11-11\"></span>[48] Meng Wu, Jingbo Wang, Jyotirmoy Deshmukh,\
    \ and Chao Wang. 2019. Shield Synthesis for Real: Enforcing Safety in Cyber-Physical\
    \ Systems. In FMCAD. IEEE, 129–137.\n- <span id=\"page-11-12\"></span>[49] Meng\
    \ Wu, Haibo Zeng, Chao Wang, and Huafeng Yu. 2017. Safety Guard: Runtime Enforcement\
    \ for Safety-Critical Cyber-Physical Systems: Invited. In DAC. ACM, 84:1–84:6.\n\
    - <span id=\"page-11-2\"></span>[50] Yuan Zhou, Yang Sun, Yun Tang, Yuqi Chen,\
    \ Jun Sun, Christopher M. Poskitt, Yang Liu, and Zijiang Yang. 2023. Specification-Based\
    \ Autonomous Driving System Testing. IEEE Trans. Software Eng. 49, 6 (2023), 3391–3410."
  decisions:
    language: '- Qualified. Reason: English Paper'
    evaluation_prompt: Qualified.
    related_work_prompt: Qualified
    novelty_prompt: Qualified
    review_only_prompt: Qualified
  llm_input_used: '## Abstract

    Autonomous driving systems (ADSs) integrate sensing, perception, drive

    control, and several other critical tasks in autonomous vehicles, motivating

    research into techniques for assessing their safety. While there are several

    approaches for testing and analysing them in high-fidelity simulators, ADSs may

    still encounter additional critical scenarios beyond those covered once they

    are deployed on real roads. An additional level of confidence can be

    established by monitoring and enforcing critical properties when the ADS is

    running. Existing work, however, is only able to monitor simple safety

    properties (e.g., avoidance of collisions) and is limited to blunt enforcement

    mechanisms such as hitting the emergency brakes. In this work, we propose

    REDriver, a general and modular approach to runtime enforcement, in which users

    can specify a broad range of properties (e.g., national traffic laws) in a

    specification language based on signal temporal logic (STL). REDriver monitors

    the planned trajectory of the ADS based on a quantitative semantics of STL, and

    uses a gradient-driven algorithm to repair the trajectory when a violation of

    the specification is likely. We implemented REDriver for two versions of Apollo

    (i.e., a popular ADS), and subjected it to a benchmark of violations of Chinese

    traffic laws. The results show that REDriver significantly improves Apollo''s

    conformance to the specification with minimal overhead.


    ## Introduction

    Autonomous driving systems (ADSs) are the core of autonomous vehicles (AVs), integrating
    sensing, perception, drive control, and several other tasks that are necessary
    for automating their journeys. Given the safety-critical nature of ADSs [\[14,](#page-10-0)
    [18\]](#page-10-1), it is imperative that they operate safely at all times, including
    in rare or unexpected scenarios that may not have been explicitly considered when
    the


    ICSE ''24, April 14–20, 2024, Lisbon, Portugal


    © 2024 Copyright held by the owner/author(s).


    ACM ISBN 979-8-4007-0217-4/24/04.


    <https://doi.org/10.1145/3597503.3639151>


    [Christopher M. Poskitt](https://orcid.org/0000-0002-9376-2471) Singapore Management
    University Singapore cposkitt@smu.edu.sg


    [Jun Sun](https://orcid.org/0000-0002-3545-1392) Singapore Management University
    Singapore junsun@smu.edu.sg


    system was designed. This has spurred a multitude of research into techniques
    for establishing confidence in an ADS, e.g., by modelling and verifying aspects
    of its design [\[23\]](#page-10-2), by subjecting it to reconstructions of real-world
    accidents [\[6\]](#page-10-3), or by testing it against automatically generated
    critical scenarios [\[27,](#page-10-4) [44,](#page-11-1) [50\]](#page-11-2) in
    a high-fidelity simulator such as CARLA [\[15\]](#page-10-5) or LGSVL [\[38\]](#page-11-3).


    These approaches all analyse an ADS before it is deployed on real roads, where
    it may still encounter additional scenarios beyond those that were covered. In
    fact, an analysis of accidents involving autonomous vehicles [\[31\]](#page-11-4)
    suggests that the broader implementation of current AV technologies may not lead
    to a reduction in vehicle crash frequency. An additional level of confidence can
    thus be established if desirable properties are also monitored—even enforced—while
    the ADS is running. This is the idea of runtime enforcement, a technique that
    observes the execution of a system and then modifies it in a minimal way to ensure
    certain properties are satisfied. In AVs, runtime enforcement has been applied,
    for example, to monitor basic safety properties such as the avoidance of collisions,
    applying the emergency brake before they are violated [\[21\]](#page-10-6). Avoiding
    collisions, however, is not enough in general. ADSs are expected to satisfy a
    broader range of complicated properties concerning the overall traffic systems
    they operate in, such as national traffic laws that describe how vehicles should
    behave with respect to various junctions, signals, and (most precariously) other
    vehicles or pedestrians. Currently, no existing approach supports runtime enforcement
    of properties in this direction.


    In this work, we aim to provide a general solution to the runtime enforcement
    problem for AVs. In particular, we propose REDriver, a general framework for runtime
    enforcement that can be integrated into ADSs with state-of-the-art modular designs,
    as exhibited by Apollo [\[4\]](#page-10-7) and Autoware [\[2\]](#page-10-8). REDriver
    allows users to specify desirable properties of AVs using an existing and powerful
    domainspecific language (DSL) based on signal temporal logic (STL). This language
    supports properties ranging from the simplest, concerning collision avoidance,
    through to entire formalisations of national traffic laws [\[44\]](#page-11-1).
    REDriver monitors the planned trajectories and command sequences of the ADS at
    runtime and assesses them against the user''s specifications. If the AV is predicted
    to potentially violate them in the near future (based on a quantitative semantics
    of STL), REDriver repairs the trajectories using a gradient-driven algorithm.
    Furthermore, it does so while minimising the "overhead" (or change) to the original
    journey. That is, by efficiently computing the gradient of each signal (with respect
    to the robustness degree


    Permission to make digital or hard copies of part or all of this work for personal
    or classroom use is granted without fee provided that copies are not made or distributed
    for profit or commercial advantage and that copies bear this notice and the full
    citation on the first page. Copyrights for third-party components of this work
    must be honored. For all other uses, contact the owner/author(s).


    ICSE ''24, April 14–20, 2024, Lisbon, Portugal Yang Sun, Christopher M. Poskitt,
    Xiaodong Zhang, and Jun Sun


    <span id="page-1-0"></span>![](_page_1_Figure_2.jpeg)


    Figure 1: The architecture of an ADS with REDriver


    <span id="page-1-1"></span>


    | Time | Position         | Speed | Acc   | Steer | Gear  |

    |------|------------------|-------|-------|-------|-------|

    | 0    | (x: 0, y: 0)     | 7.01  | -0.05 | 0     | DRIVE |

    | 2    | (x: 0, y: 13.34) | 6.13  | -0.48 | 0     | DRIVE |

    | 4    | (x: 0, y: 24.83) | 5.44  | -0.24 | 0     | DRIVE |

    | 6    | (x: 0, y: 35.85) | 5.09  | -0.18 | 0     | DRIVE |

    | 8    | (x: 0, y: 44.75) | 3.89  | -1.44 | 0     | DRIVE |


    Table 1: An example planned trajectory


    of the STL formula), we identify and modify the signal that is most likely to
    repair the trajectories.


    REDriver has been implemented for two versions of Apollo (i.e., versions 6.0 and
    7.0, the latest at the time of experimentation). The implementation consists of
    a plan validation algorithm and a control validation algorithm that respectively
    observe and modify (if necessary) the outputs of the ADSs'' motion planning and
    control modules. Note that the motion planning and control modules are black boxes
    to us. In particular, we enforce that these outputs (i.e., planned trajectories
    and command sequences) do not lead to violations—whenever possible—of a comprehensive
    formalisation of the Chinese traffic laws. This goes far beyond existing runtime
    enforcement approaches, which focus on simple safety properties (e.g., collision
    avoidance) and blunt enforcement mechanisms (e.g., hitting the emergency brakes).
    Figure [1](#page-1-0) depicts how REDriver is integrated into the modular design
    of Apollo. In particular, we have added two new modules while ensuring that the
    existing modules and their inner logic remain unchanged. In the diagram, the perception,
    motion planning, and control boxes represent the existing Apollo modules, while
    the green plan validation and control validation boxes represent the new modules
    from REDriver. The arrow denotes the flow of signal transmission. We evaluated
    our implementation of REDriver against a benchmark of violation-inducing scenarios
    for Chinese traffic laws [\[44\]](#page-11-1), finding that our runtime enforcement
    approach significantly reduces the likelihood of those violations occurring. Furthermore,
    REDriver''s overhead in terms of time and how often it intervenes is negligible.'
  token_usage: 8350
  time_usage: 2.1300196647644043
- title: 'SliceLocator: Locating Vulnerable Statements with Graph-based Detectors'
  abstract: 'Vulnerability detection is a crucial component in the software development

    lifecycle. Existing vulnerability detectors, especially those based on deep

    learning (DL) models, have achieved high effectiveness. Despite their

    capability of detecting vulnerable code snippets from given code fragments, the

    detectors are typically unable to further locate the fine-grained information

    pertaining to the vulnerability, such as the precise vulnerability triggering

    locations. Although explanation methods can filter important statements based

    on the predictions of code fragments, their effectiveness is limited by the

    fact that the model primarily learns the difference between vulnerable and

    non-vulnerable samples. In this paper, we propose SliceLocator, which, unlike

    previous approaches, leverages the detector''s understanding of the differences

    between vulnerable and non-vulnerable samples, essentially,

    vulnerability-fixing statements. SliceLocator identifies the most relevant

    taint flow by selecting the highest-weighted flow path from all potential

    vulnerability-triggering statements in the program, in conjunction with the

    detector. We demonstrate that SliceLocator consistently performs well on four

    state-of-the-art GNN-based vulnerability detectors, achieving an accuracy of

    around 87% in flagging vulnerability-triggering statements across six common

    C/C++ vulnerabilities. It outperforms five widely used GNN-based explanation

    methods and two statement-level detectors.'
  url: http://arxiv.org/abs/2401.02737v4
  keywords: ''
  document: '# <span id="page-0-0"></span>SliceLocator: Locating Vulnerable Statements
    with Graph-based Detectors


    Baijun Cheng 1 , Kailong Wang2\*, Cuiyun Gao 3 , Xiapu Luo 4 , Li Li 5 , Yao Guo1\*,
    Xiangqun Chen 1 , Haoyu Wang2\*


    1\*School of Computer Science, Peking University, Beijing, China.


    2\*School of Cyber Science and Engineering, Huazhong University of Science and
    Technology, Wuhan, Hubei, China.


    <sup>3</sup>School of Computer Science and Technology, Harbin Institute of Technolgy,
    Shenzhen, Guangdong, China.


    <sup>4</sup>Department of Computing, The Hong Kong Polytechnic University, Hong
    Kong, China.


    <sup>5</sup>School of Software, Beihang University, Beijing, China.


    \*Corresponding author(s). E-mail(s): wangkl@hust.edu.cn ; yaoguo@pku.edu.cn ;
    haoyuwang@hust.edu.cn ; Contributing authors: prophecheng@stu.pku.edu.cn ;


    #### Abstract


    Vulnerability detection is a crucial component in the software development lifecycle.
    Existing vulnerability detectors, especially those based on deep learning (DL)
    models, have achieved high effectiveness. Despite their capability of detecting
    vulnerable code snippets from given code fragments, the detectors are typically
    unable to further locate the fine-grained information pertaining to the vulnerability,
    such as the precise vulnerability triggering locations. Although explanation methods
    can filter important statements based on the predictions of code fragments, their
    effectiveness is limited by the fact that the model primarily learns the difference
    between vulnerable and non-vulnerable samples. In this paper, we propose SliceLocator,
    which, unlike previous approaches, leverages the detector''s understanding of
    the differences between vulnerable and non-vulnerable samples—essentially, vulnerability-fixing
    statements. SliceLocator identifies the most relevant taint flow by selecting
    the highest-weighted flow path from all potential vulnerability-triggering statements
    in the program, in conjunction with the detector. We demonstrate that SliceLocator
    consistently performs well on four state-of-the-art GNN-based vulnerability detectors,
    achieving an accuracy


    of around 87% in flagging vulnerability-triggering statements across six common
    C/C++ vulnerabilities. It outperforms five widely used GNN-based explanation methods
    and two statement-level detectors.


    Keywords: vulnerability detection, deep learning, graph representation, vulnerability
    localization


    # 1 introduction


    The proliferation of modern software programs developed for diverse purposes and
    usage scenarios is inevitably and persistently coupled with intensified security
    threats from vulnerabilities, evidenced by the substantial surge in the volume
    of reported vulnerabilities via the Common Vulnerabilities and Exposures (CVE)
    [\[1\]](#page-19-0). To counteract the potential exploitation, both academia and
    industrial communities have proposed numerous techniques for identifying and locating
    those vulnerabilities.


    Traditional approaches, such as the rule-based analysis techniques (e.g., SVF
    [\[2\]](#page-19-1), Checkmarx [\[3\]](#page-19-2), Infer [\[4\]](#page-20-0),
    and clang static analyzer [\[5\]](#page-20-1)), leverage predefined signatures
    or rules to identify vulnerabilities. Unfortunately, similar to other static analysis
    techniques, they typically suffer from high false positive and negative rates
    [\[6\]](#page-20-2). More recently, DL-based detection techniques [\[6–](#page-20-2)[9\]](#page-20-3),
    which generally operate on extracted code feature representations, have shown
    great effectiveness in flagging vulnerabilitycontaining code fragments (i.e.,
    functions or slices). However, the coarse granularity and the black-box nature
    of the analysis renders poor interpretability in the detection results. For example,
    a function or a code snippet could contain over a dozen code lines, which remains
    challenging for the developers to understandthe root cause of the vulnerabilities
    and further take action to fix them. A recent work [\[10\]](#page-20-4) suggests
    that the bug trigger path is the key to locating and fixing a vulnerability.


    One promising way to tackle this problem is leveraging explanation approaches
    to select important features for the DL-based detectors, and then mapping them
    to the corresponding code lines. Recent rapid advances in graph-based explainability
    technology show great potential for this solution. In particular, the existing
    explanation methods commonly facilitate model interpretability from three angles:
    assigning numeric values to graph edges [\[11,](#page-20-5) [12\]](#page-20-6),
    computing importance scores for nodes [\[13\]](#page-20-7), and calculating scores
    for graph walks while traversing through GNNs [\[14\]](#page-20-8). Despite their
    success in tasks such as molecular graph classifications, current GNN-based explanation
    methodologies exhibit inherent limitations that impede their direct applicability
    in extracting fine-grained vulnerability-related information, particularly in
    identifying relevant statements.


    Limitations. The first limitation of GNN explanation methods lies in their reliance
    on selecting the most influential parts of the graph for model inference. However,
    they may not always accurately identify these critical components [\[15\]](#page-20-9).
    A thorough analysis of the GNN inference process and the explanation methods is
    required to further validate this point, which, however, falls beyond the scope
    of our work. More importantly, according to previous studies [\[16\]](#page-21-0),
    GNN-based detectors


    are likely to focus primarily on learning the differences between vulnerable and
    nonvulnerable samples for inference, rather than relying on domain-specific knowledge
    such as taint flow. Moreover, the differences between vulnerable and non-vulnerable
    samples may not be limited to the location of code fixes, but could also involve
    other structural changes in the graph, such as alterations in topology due to
    added conditional statements (e.g., if clauses). Explanation methods tend to amplify
    these differences.


    Insights. Graph-based detectors, while potentially learning irrelevant features,
    show high sensitivity to vulnerability fixes. For example, in Devign, masking
    vulnerable fixing statements (VFS) reduces the vulnerability probability by 0.32
    on average, while masking other locations causes a loss of no more than 0.15.
    Since VFS and vulnerable triggering statements (VTS) are strongly data-dependent,
    and data flow graphs are sparse [\[2\]](#page-19-1), this suggests that vulnerabilities
    are likely triggered at the VTS. The detector often assigns the flow linking VFS
    and VTS high weight, and due to the sparsity of data flows, the highest-weighted
    flow traced back from VTS is likely the vulnerable flow.


    Solution. In this work, we propose SliceLocator, a novel approach to identify
    fine-grained information from vulnerable code reported by GNN-based vulnerability
    detectors. Given a detected vulnerable code fragment, the key idea behind SliceLocator
    is to leverage GNN-based detectors to identify the highest-weighted taint flow
    from the VFS to the VTS. The core step of SliceLocator involves performing backward
    program slicing based on potential sink points (PSPs). First, a set of flow paths
    is extracted, and then, using GNN-based detectors, the weight of each path is
    predicted. The path with the highest weight is selected as the most relevant flow
    for vulnerability localization. Compared with prior works (e.g., DeepWukong [\[6\]](#page-20-2)),
    SliceLocator only preserves vulnerability-triggering and vulnerability-dependent
    program path-level information, rather than that of the full program. This significantly
    improves the analysis efficiency as program paths contain a smaller number of
    code lines. Leveraging the program slicing method, SliceLocator captures more
    semantic information encompassed in code lines. As a result, it can provide more
    accurate localization results than the approaches only focusing on topological
    features.


    Evaluation. We follow previous study [\[16\]](#page-21-0) to use vulnerability-triggering
    code line coverage (TLC) and vulnerability-fixing code line coverage (FLC) to
    evaluate the effectiveness of SliceLocator. We apply SliceLocator to four detectors,
    including DeepWukong [\[6\]](#page-20-2), Reveal [\[7\]](#page-20-10), IVDetect
    [\[8\]](#page-20-11), Devign [\[9\]](#page-20-3) and perform multi-dimensional
    evaluations. In the first phase, we assess the performance of SliceLocator by
    comparing it to the other five explanation methods, including PGExplainer [\[12\]](#page-20-6),
    GNNExplainer [\[11\]](#page-20-5), GNN-LRP [\[14\]](#page-20-8), GradCAM [\[13\]](#page-20-7),
    and DeepLift [\[17\]](#page-21-1). The experimental data indicate that the SliceLocator,
    combined with four detectors, achieves a TLC score ranging from 0.83 to 0.93 and
    an FLC score of at least around 0.7. This performance is not only superior to
    the other five explanation methods but also demonstrates minimal deviation across
    different detectors. In the second phase, we compare SliceLocator''s performance
    combined with four graph-based detectors against two deep learningbased statement-level
    detectors, LineVul [\[18\]](#page-21-2) and LineVD [\[19\]](#page-21-3). Experimental
    data demonstrate that SliceLocator consistently outperforms both LineVul and LineVD.


    <span id="page-3-0"></span>![](_page_3_Figure_0.jpeg)


    Fig. 1: General detection phase of deep-learning-based vulnerability detectors
    with graph representations


    This higher performance can be attributed to SliceLocator''s ability to effectively
    leverage both the detector''s sensitivity to VFS and heuristic taint flow knowledge.
    Unlike other explanation methods and statement-level detectors, which fail to
    fully exploit this critical information, SliceLocator combines these factors to
    enhance its vulnerability localization accuracy. The data supporting the paper
    can be accessed at [\[20\]](#page-21-4).


    In summary, we make the following main contributions:


    - A novel vulnerability statement locating technique via GNN-based vulnerability
    detectors. Given the inadequate explainability of the existing GNNbased vulnerability
    detectors, we propose the framework SliceLocator as a solution. It can identify
    important flow paths in a program that contain vulnerabilitytriggering statements,
    providing finer-grained semantics contexts for the identified vulnerabilities.

    - Approach effectiveness. Through a multi-dimensional evaluation of a comprehensive
    benchmark dataset, we demonstrate that SliceLocator outperforms existing explanation
    methods in terms of both TLC and FLC, which are crucial factors influencing vulnerability
    localization and fixing. On average, SliceLocator achieves a TLC of 0.89 and an
    FLC of 0.85 across all vulnerability detectors used in this study, highlighting
    its strong generalization ability across different GNN-based vulnerability detectors.


    # 2 Background


    ### 2.1 GNN-based Vulnerability Detectors


    Recently, GNNs have been utilized by security analysts and researchers in vulnerability
    detection tasks [\[6–](#page-20-2)[9,](#page-20-3) [21,](#page-21-5) [22\]](#page-21-6).
    They presume the graph representation of codes could better preserve critical
    semantic information of vulnerability-related programs, compared with traditional
    sequence-based representation. Typically, the most frequently used graph representation
    is code property graph (CPG) [\[23\]](#page-21-7), which is combined with abstract
    syntax tree (AST), control flow graph (CFG), control dependence graph (CDG), and
    data dependence graph (DDG). Generally, the detection phase of a GNN-based detector
    usually consists of three steps, as shown in Figure [1:](#page-3-0)


    (a) Parsing source code into a graph representation. Source code is typically
    in plain text, and must first be parsed into an AST, which can then be further


    transformed into other graph representations. This process can be accomplished
    using tools like Joern [\[23\]](#page-21-7).


    (b) Embedding code graph into vectorized representation. In a code graph, a node
    typically represents a program statement, while an edge indicates a relationship
    (such as execution order or def-use) between two statements. To generate the initial
    embeddings for the graph, each node needs to be vectorized. Following previous
    studies [\[7,](#page-20-10) [19,](#page-21-3) [22\]](#page-21-6), this can be
    achieved using techniques such as Word2Vec [\[24\]](#page-21-8), Doc2Vec [\[25\]](#page-21-9),
    or CodeBert [\[26\]](#page-21-10). The graph''s vectorized representation is then
    created by sequentially embedding all the nodes.


    (c) Using a well-trained GNN model to classify vectorized code graph. With vectorized
    graphs of code fragments and their labels, a GNN model, such as Graph Convolutional
    Networks (GCN) and Gated Graph Neural Networks (GGNN), could be trained to detect
    vectorized graph data from target programs.


    ### 2.2 Explanations approaches for vulnerability detection


    GNN-based detectors are capable of identifying vulnerable code snippets but fail
    to pinpoint the exact vulnerable statements. A direct solution to this issue is
    to employ instance-level explanation methods [\[15\]](#page-20-9). The basic principle
    of these methods is to identify and highlight the parts of a sample that are most
    critical to the model''s prediction, and then map them to the corresponding vulnerable
    statements for localization.


    Currently, instance-level explanation methods can be categorized into three main
    types: Gradients-based, Perturbation-based, and Decomposition-based, with prominent
    examples including GNNExplainer [\[11\]](#page-20-5), PGExplainer [\[12\]](#page-20-6),
    GradCAM [\[13\]](#page-20-7), DeepLIFT [\[17\]](#page-21-1), and GNNLRP [\[14\]](#page-20-8).
    While these methods appear promising, previous studies [\[16,](#page-21-0) [27,](#page-22-0)
    [28\]](#page-22-1) have shown that their effectiveness, stability, and robustness
    are often suboptimal. This can be attributed to the sometimes imperfect performance
    of GNN-based detectors, which, despite their high detection efficiency, may still
    overfit the differences between vulnerable and non-vulnerable samples, leading
    to poor explanation results. Overall, the poor performance of existing explanation
    methods can be attributed to their over-reliance on the model itself, without
    adequately considering the underlying semantics of vulnerabilities.


    # <span id="page-4-0"></span>3 Problem Formulation and Challenge


    ### 3.1 Problem Overview


    Considering the points raised in the previous section, one might wonder whether
    a more direct approach could be employed for vulnerability localization. The most
    straightforward method would involve slicing the program at all potential vulnerability
    trigger points. Approaches such as VulDeePecker [\[29\]](#page-22-2), SySeVR [\[30\]](#page-22-3),
    and DeepWuKong [\[6\]](#page-20-2) follow this strategy, training detectors after
    performing program slicing. However, because these methods include all statements
    with potential data dependencies in a saturated manner, the resulting slices remain
    quite large. Therefore, a more fine-grained approach is necessary. Specifically,
    one could perform a finer backward slicing within a given slice or function, focusing
    on all Vulnerability Triggering Statements (VTS). This process could yield multiple
    smaller slices, each containing only a subset of statements. Specifically, each
    slice could be a unique data-flow path. The critical challenge then becomes identifying
    the slice most likely to trigger the vulnerability.


    The idea can be illustrated with the following example, as shown in Figure [3.](#page-7-0)
    It involves a buffer overflow vulnerability triggered by copying more data (i.e.,
    100 bytes, defined on line 11 of the code fragment) than the maximum capacity
    of an array (i.e., 50 bytes, defined on line 2). A GNN-based vulnerability detector
    would simply output a binary detection result (1 indicating the code fragment
    is vulnerable, or 0 indicating it is not). The objective of the vulnerability
    localization task is to identify the VTS line 11 along with its related data dependencies.
    Since the vulnerability is triggered by array access or a copy function call,
    lines 5, 9, 10, 11, and 13 should be conservatively flagged as potential slicing
    starting points, with multiple slicing paths generated from these points. From
    the generated paths, we select those that align with our vulnerability localization
    objective. In the example shown in Figure [3,](#page-7-0) several flow paths can
    be extracted from the original code fragment, such as 8 --> 11, 2 --> 6 --> 7
    --> 13, and so on. Among these, the path 2 --> 6 --> 7 --> 11 is considered the
    most critical, as it includes both line 2 (where a critical variable is assigned)
    and line 11 (where the vulnerability is triggered).


    ### 3.2 Challenge


    The ideas outlined below are straightforward. However, the challenge lies in how
    to select the most important paths. Theoretically, more important paths should
    be assigned higher weights. Achieving this goal is difficult with current explanation
    approaches. However, a previous study [\[16\]](#page-21-0) has found that models
    tend to be more sensitive to vulnerability-fixing statements (VFS) than to VTS.
    One possible reason for this is that the code at VFS locations differs between
    vulnerable and non-vulnerable samples, making it easier for the model to determine
    whether the code is vulnerable based on the VFS. In contrast, VTS appear in both
    vulnerable and non-vulnerable samples, meaning that their presence does not necessarily
    have as strong an influence on the model''s decision-making process.


    To further investigate the significance of VFS in model predictions, we apply
    a method similar to that used in a previous study, utilizing the full dataset
    they employed [\[16\]](#page-21-0). Specifically, we mask individual code lines
    and calculate the change in vulnerability prediction probabilities, which serves
    as the importance score for each line with respect to the model. We then calculate
    the importance scores for VFS, along with the maximum importance scores for the
    non-VFS code lines. The experimental results for DeepWuKong, Devign, IVDetect,
    and Reveal are presented in Table [1.](#page-6-0) The results indicate that VFS
    tends to be more important to the model than other code lines. We conduct a manual
    analysis of several cases and found that there is typically a strong program dependency
    between VFS and VTS in many vulnerability samples. Moreover, the potential VTS
    within a sample rarely appears across multiple data


    <span id="page-6-0"></span>


    | Detector   | VFS  | max non-VFS |

    |------------|------|-------------|

    | DeepWuKong | 0.44 | 0.31        |

    | Reveal     | 0.2  | 0.09        |

    | IVDetect   | 0.13 | 0.1         |

    | Devign     | 0.32 | 0.14        |

    |            |      |             |


    Table 1: Importance Score of VFS and the maximum of non-VFS


    dependencies leading to the VFS. For example, in CVE-2013-2174[1](#page-0-0) which
    is shown in Figure [2,](#page-6-1) insufficient validation of the alloc variable
    leads to a potential buffer overflow at string[2] on line 7. To address this,
    developers add the condition alloc > 2 in line 5 to ensure proper validation.
    The VTS involves access to string[2], which is control-dependent on the condition
    in the VFS, specifically the if condition. Therefore, we propose using a trained
    detection model to predict the importance of each path and introduce SliceLocator
    as a solution for this task.


    <span id="page-6-1"></span>![](_page_6_Figure_3.jpeg)


    Fig. 2: Simplified code from the fix commit for CVE-2013-2174


    # 4 Approach


    The overall framework of SliceLocator is shown in Figure [4,](#page-7-1) which
    consists of two modules: flow path generation from the original code graph and
    critical path selection.


    Flow Path Generation. Given a vulnerable code fragment represented as a graph
    with its control- and data-dependence computed (in Figure [4\(](#page-7-1)a)),
    SliceLocator first identifies statements (i.e., nodes) in the program that might
    trigger the vulnerability, denoted as potential sink points (PSPs). Next, SliceLocator
    iteratively traverses backward from a PSP along a data- & control-dependence path
    (denoted as flow path


    <sup>1</sup><https://github.com/curl/curl/commit/192c4f788d48f82c03e9cef40013f34370e90737>


    <span id="page-7-0"></span>![](_page_7_Figure_0.jpeg)


    Fig. 3: A example extracted from SARD.


    <span id="page-7-1"></span>![](_page_7_Figure_2.jpeg)


    Fig. 4: Overview of SliceLocator.


    hereafter) in the program graph, until the source of the PSP (e.g., a node representing
    critical variable assignment) is reached. Similarly, SliceLocator generates all
    the qualified flow paths from the graph, each ending with a PSP.


    Flow Path Selection. SliceLocator first vectorizes each flow path and computes
    an importance score correlated to the vulnerability probability (in Figure [4\(](#page-7-1)b)).
    Next, SliceLocator selects the flow path with the highest importance score as
    the vulnerability data flow. Note that we do not directly train a classifier for
    the path selection as each path is regarded as a data flow rather than a code
    fragment.


    ### 4.1 Flow Path Generation


    To generate flow paths from the original code graph (i.e., PDG), we utilize the
    program slicing [\[31\]](#page-22-4) technique, which has been widely adopted
    in previous works such as DeepWukong and VulDeePecker. Unlike previous approaches,
    which focus on generating complete code fragments, our slicing technique emphasizes
    selecting a fine-grained set of flow paths. The slicing principle is based on
    both control and data dependence within the PDG, enabling more precise vulnerability
    localization. More specifically, the detailed flow path generation approach is
    outlined in the GENERATESLICE function in Algorithm [1.](#page-9-0)


    Algorithm [1](#page-9-0) Details. In line 2, the path set S for the current program
    is initialized as an empty set. In line 3, the algorithm extracts PSPs with the
    given code graph (Section [4.1.1\)](#page-8-0). Then the algorithm generates flow
    paths for each PSP with the following steps. In line 5, we initialize the current
    traversed path p with the corresponding PSP. Then in line 6, the current PSP''s
    flow-path set is initialized as an empty set. In line 7, flow paths are generated
    with a DFS algorithm (to be explained next). In line 8, we include all flow paths
    of the current PSP to the path set S.


    The function DFS describes the process of the backward traversing algorithm when
    generating flow paths. In lines 14-16, if the length of the current flow path
    p reaches the upper limit, then p will be appended to the path set S and the function
    will return. In lines 18-19, the algorithm extracts nodes on which the last node
    n of p is dependent. In lines 20-22, if p cannot continue to extend, then p will
    be appended to S. Otherwise, in lines 24-27, we repeat this DFS process for each
    node that n is dependent on.


    #### <span id="page-8-0"></span>4.1.1 Potential Sink Points (PSPs)


    PSPs are statements that are critically related to vulnerabilities. In Algorithm
    [1,](#page-9-0) they are extracted by the function ExtractSinkNode (line 3) which
    considers the following four types of PSPs in our program slicing. We adopt the
    same definition proposed by Li et al [\[30\]](#page-22-3).


    - Library/API Function Call (FC). This kind of PSP covers almost all vulnerability
    types except for integer overflow. Different types of vulnerabilities are triggered
    by various types of API calls. For example, OS command injection is usually triggered
    by APIs such as system and execl, while buffer overflow is normally triggered
    by data copy functions like memcpy.

    - Array Usage (AU). This kind of PSP usually appears in memory errors. In this
    study, AU only covers the buffer overflow vulnerability. For example, data[i]
    = 1; might cause a buffer overflow. Note that we do not consider trivial cases
    such as array accesses with constant indexes in this work.

    - Pointer Usage (PU). Similar to AU, PU usually appears in memory errors. This
    study only covers buffer overflow vulnerability.

    - Arithmetic Expression (AE). This type of PSP is usually an arithmetic expression
    like a + 1 or a++. AE is usually related to integer overflow and division-by-zero
    vulnerabilities. Here we mainly focus on the former. Note that we do not consider
    trivial cases such as self-increment and self-decrement operations with conditional
    checks in this work.


    <span id="page-9-0"></span>Algorithm 1 Slice Generation Algorithm.


    Input: code graph G,max length of path k Output: path set S 1: function GenerateSlice(G,
    k) 2: S ← ∅ 3: sink nodes ← ExtractSinkNodes(G) 4: for sink node ∈ sink nodes
    do 5: p ← {sink node} 6: S ′ ← ∅ 7: DFS(p, G, 1, k, S ′ ) 8: Append all slice
    in S ′ to S 9: end for 10: return S 11: end function 12: 13: function DFS(p, G,
    l, k, S) 14: if l = k then 15: Append p to S 16: return 17: end if 18: n ← last
    node in p 19: prec nodes ← ExtractPrecNodes(n, G) 20: if prec nodes is ∅ then
    21: Append p to S 22: return 23: end if 24: for prec node ∈ prec nodes do 25:
    Append prec node to p 26: DFS(p, G, 1 + 1, k, S) 27: pop the last node in p 28:
    end for 29: end function


    #### 4.1.2 Dependent Statements


    The function ExtractPrecNodes (line 19) in Algorithm [1](#page-9-0) establishes
    the dependence relation for the node n (i.e., identify nodes that the node n is
    dependent on). We find that not every dependence relation for node n is related
    to the vulnerability, as a source code statement might contain multiple expressions
    among which only one could trigger the vulnerability. Therefore, we only focus
    on the control and data dependence involving key variables related to each PSP
    when extracting dependent nodes. For illustration in Figure [5,](#page-10-0) our
    tool identified arithmetic operation CHAR ARRAY SIZE - 1 which might trigger integer
    underflow in statement S3. Although S3 is data-dependent with S1 via the variable
    connectSocket, they do not appear in arithmetic operations. We thus do not consider
    the data-dependence


    <span id="page-10-0"></span>![](_page_10_Figure_0.jpeg)


    Fig. 5: An example of ignored data-dependence edges.


    ![](_page_10_Figure_2.jpeg)


    Fig. 6: An example to demonstrate how slicing works by revisiting the code in
    Figure [3.](#page-7-0)


    edge S1 --> S3 when performing slicing. For other nodes, we consider all dependent
    statements of the current node.


    ### 4.2 Flow Path Selection


    Among the flow paths, we aim to select one that can best locate the vulnerabilitytriggering
    statements based on the prediction results. The key intuition is that if a path
    contains both the PSP and its source node, the path should be selected. For example,
    the path 2 --> 6 --> 7 --> 11 in the example in Section [3.](#page-4-0) If there
    is more than one qualified path, we further rank them based on the path importance
    (to be detailed below) and select the one with the highest importance score.


    More formally, given a code graph G, we extract flow paths from it and vectorize
    each path. The process of vectorizing a flow path is consistent with how detectors
    vectorize the corresponding code graph. We then compute the importance score for


    each flow path by treating each vectorized flow path as a subgraph of the original
    code graph and inputting it into a well-trained GNN-based vulnerability detector.
    This process could be formally described as:


    $$p\_g = \Phi(\text{vec}(g))\tag{1}$$


    where g is a flow path extracted from G, Φ is one of the GNN-based vulnerability
    detectors.Finally, we compute the importance score IS<sup>g</sup> for each path,
    measuring their contribution to the detector predicting the corresponding code
    fragment.


    $$\text{IS}\_g = 1 - \left(\Phi(\text{vec}(G)) - p\_g\right) \tag{2}$$


    Suppose there are n flow paths after slicing G and denoted as {g1, ..., g<sup>i</sup>
    , ...gn}. The vulnerability data flow g<sup>∗</sup> is denoted as:


    $$g^\* = \text{argmax } (\text{IS}\_{g\_i}) \tag{3}$$


    # 5 Study Design


    ### 5.1 Evaluation Methodology


    We evaluate the effectiveness of SliceLocator in locating vulnerability statements
    based on the prediction results from DeepWukong, Reveal, IVDetect, and Devign.
    Our evaluation addresses the following research questions:


    - RQ1 Can SliceLocator outperform existing instance-level explanation methods
    in vulnerability localization when combined with GNN-based detectors? We compare
    SliceLocator with five other instance-level explanation methods [\[11–](#page-20-5)
    [14,](#page-20-8) [17\]](#page-21-1) in terms of vulnerability localization performance
    on GNN-based detectors.

    - RQ2 Can SliceLocator outperform deep learning-based statement-level detectors
    in vulnerability localization? We compare SliceLocator, combined with four GNN-based
    detectors, to two deep learning-based statement-level detectors [\[18,](#page-21-2)
    [19\]](#page-21-3) for vulnerability localization performance.

    - RQ3 Is the trained detector crucial for path selection? In some of the experiments,
    we investigate this by replacing the path selection strategy that assigns the
    highest weight to the path identified by the detector with a random selection
    of paths. This allows us to examine the importance of the detector in the path
    selection process.


    For evaluating the explanation methods, we follow the approach in the previous
    study [\[16\]](#page-21-0), using two metrics: Vulnerability-Triggering Line Coverage
    (TLC) and Vulnerability-Fixing Line Coverage (FLC). Since the study also highlights
    that fidelity is not a reliable measure for assessing the effectiveness of explanation
    methods in vulnerability detection, we exclude fidelity from our evaluation. The
    line coverage (LC) can be calculated using the following equation, where s <sup>v</sup>
    denotes the set of labeled triggering statements and s e represents the set of
    statements predicted by statement-localization methods.


    $$\text{LC} = \frac{|s^e \cap s^v|}{|s^v|} \tag{4}$$


    $$^{12}$$


    For the vulnerability localization results, we present the top-k TLC or FLC score.
    For explanation methods, the top-k results refer to the k highest-weighted statements
    after the explanation method assigns weights to each statement. For both SliceLocator
    and random path selection, top-k represents selecting the highest-weighted path
    from those sliced paths with lengths less than k. Here, k can take values of 3,
    5, and 7.


    ### <span id="page-12-0"></span>5.2 Dataset Construction


    The dataset used for evaluation must support fine-grained vulnerability detection,
    which requires explicit annotations of vulnerable code lines. Many real-world
    datasets, such as Devign [\[9\]](#page-20-3), Reveal, and Big-Vul [\[32\]](#page-22-5),
    label flaw lines based on code change information extracted from committed version
    patches. While D2A [\[33\]](#page-22-6) constructs the dataset by comparing the
    vulnerability reports produced by Infer [\[4\]](#page-20-0) with GitHub commit
    information. Roland Croft et al. [\[34\]](#page-22-7) have reported that real-world
    datasets contain between 20-71% false positive samples, where code marked as vulnerable
    is actually safe. This might be because the fixing commits contain changes unrelated
    to vulnerability fixes. More importantly, datasets like Big-Vul and Devign, which
    annotate vulnerable functions based on fixing commits, only include information
    about modified code lines without indicating the locations where vulnerabilities
    are triggered. Additionally, even vulnerability-related code changes can include
    non-vulnerabilityrelated changes, leading to potential mislabeling of code lines.
    Due to these challenges, previous studies on vulnerability detection [\[7,](#page-20-10)
    [8,](#page-20-11) [19,](#page-21-3) [28\]](#page-22-1) have faced difficulties
    in training well-performing detectors on these datasets. Consequently, following
    prior research [\[16,](#page-21-0) [35\]](#page-22-8), we adopt the SARD dataset
    [\[36\]](#page-22-9), which offers more accurate vulnerability annotations and
    facilitates the training of effective detectors. Our focus is on six of the top
    30 most critical C/C++ software weaknesses identified in 2021, specifically CWE20,
    CWE119, CWE125, CWE190, CWE400, and CWE787 following those studies.


    We use the same crawler employed in DeepWuKong to download the SARD dataset. Following
    previous studies [\[6,](#page-20-2) [16,](#page-21-0) [35\]](#page-22-8), we use
    tools such as SVF [\[2\]](#page-19-1) and Joern [\[23\]](#page-21-7) to split
    the code into fragments, such as slices or functions, and then parse them into
    graph representations. The SARD dataset annotates certain VTS, and we match the
    parsed code fragments with these annotations to identify vulnerable code fragments
    and VTS. Next, we apply a heuristic automated labeling mechanism, as done in prior
    work [\[16\]](#page-21-0), to annotate the VFS. Finally, we remove duplicate code
    fragments by following the method outlined in previous studies [\[6\]](#page-20-2),
    which utilizes MD5 value comparison to identify and exclude duplicates. After
    the processing stage, we collect 73,750 vulnerable functions, 152,771 non-vulnerable
    functions, 138,360 slices, and 364,177 non-vulnerable slices from the SARD dataset,
    as listed in Table [2.](#page-13-0)


    # 6 Experiment


    ### 6.1 Experimental Setup


    The experiments are conducted on a machine with two NVIDIA GeForce GTX TitanX
    GPUs and an Intel Xeon E5-2603 CPU. Graph neural networks are implemented using


    <span id="page-13-0"></span>


    | Vulnerability Category | Code Fragment | # Vulnerable Samples | # Safe Samples
    | # Total |

    |------------------------|---------------|----------------------|----------------|---------|

    | CWE20                  | slice         | 58,350               | 174,250        |
    232,600 |

    |                        | function      | 25,829               | 54,842         |
    80,671  |

    | CWE119                 | slice         | 34,901               | 80,155         |
    115,056 |

    |                        | function      | 21,662               | 40,466         |
    62,128  |

    | CWE125                 | slice         | 6,147                | 12,469         |
    18,616  |

    |                        | function      | 4,315                | 7,907          |
    12,222  |

    | CWE190                 | slice         | 4,173                | 10,168         |
    14,341  |

    |                        | function      | 3,948                | 11,347         |
    15,295  |

    | CWE400                 | slice         | 11,296               | 37,417         |
    48,713  |

    |                        | function      | 2,199                | 10,831         |
    13,030  |

    | CWE787                 | slice         | 23,493               | 49,718         |
    73,211  |

    |                        | function      | 15,977               | 27,378         |
    43,355  |

    | Total                  | slice         | 138,360              | 364,177        |
    502,537 |

    |                        | function      | 73,750               | 152,771        |
    226,521 |


    Table 2: Distribution of labeled samples from SARD.


    PyTorch Geometric [\[37\]](#page-23-0). We train separate models for each of the
    six vulnerability categories, using 80% of the data for training, 10% for validation,
    and 10% for testing. The model implementation follows DeepWuKong [\[38\]](#page-23-1),
    IVDetect [\[39\]](#page-23-2), Devign [\[40\]](#page-23-3), and Reveal [\[41\]](#page-23-4),
    with hyperparameters consistent with the original works. Neural networks are trained
    in batches (batch size = 64) using Adam [\[42\]](#page-23-5) with a learning rate
    of 0.001. All models are initialized randomly via Torch initialization. For the
    result explanation, we implement five state-of-the-art methods—PGExplainer, GNNExplainer,
    GradCAM, DeepLift, and GNNLRP—following DIG [\[43\]](#page-23-6).


    Before presenting the experimental results for our three research questions, we
    first provide an overview of the average detection performance of the four detectors
    on the SARD dataset. We evaluate the performance of the detectors using four metrics:
    accuracy, recall, precision, and F1 score. The average results are summarized
    in Table [3.](#page-13-1) We observe that the performance of all four detectors
    is generally satisfactory, although Devign exhibits slightly lower performance
    compared to the other three detectors.


    | Detector   | Accuracy | Precision | Recall | F1   |

    |------------|----------|-----------|--------|------|

    | DeepWuKong | 0.97     | 0.95      | 0.98   | 0.95 |

    | Reveal     | 0.96     | 0.91      | 0.99   | 0.95 |

    | IVDetect   | 0.98     | 0.95      | 0.99   | 0.97 |

    | Devign     | 0.95     | 0.9       | 0.94   | 0.92 |


    <span id="page-13-1"></span>Table 3: Detection performance of four detectors.


    ### 6.2 RQ1: SliceLocator VS Explanation Approaches


    The comparison of vulnerability localization performance (TLC and FLC scores)
    between SliceLocator and the other five explanation approaches is shown in Figure
    [7](#page-14-0) and Figure [8,](#page-15-0) respectively. Overall, SliceLocator
    achieves average top-3 to top-7 TLC scores ranging from 0.87 to 0.89 and FLC scores
    ranging from 0.78 to 0.87 across the four detectors. In contrast, among the five
    instance-level explanation approaches, GradCAM achieves the highest TLC scores,
    with average top-3 to top-7 scores ranging from 0.55 to 0.76, while DeepLift achieves
    the highest FLC scores, with top-3 to top-7 scores ranging from 0.49 to 0.64.
    These results demonstrate that SliceLocator outperforms the explanation approaches
    by at least 0.22 in TLC scores and by at least 0.33 in FLC scores.


    To further evaluate the performance of SliceLocator and the explanation methods,
    we conduct a case study based on the example presented in Section [3,](#page-4-0)
    with the results shown in Figure [9.](#page-15-1) Here, SL, PE, GE, GR, DL, and
    GL represent SliceLocator, PGExplainer, GNNExplainer, GradCAM, DeepLift, and GNN-LRP,
    respectively. From the results, we observe that PGExplainer, GradCAM, and GNN-LRP
    fail to identify the statement triggering the vulnerability. Moreover, while GNNExplainer
    and DeepLift acknowledge that statement 11 is relevant to the vulnerability, they
    struggle to capture the connections between this triggering statement and other
    vulnerability-relevant statements. The limitations of these explanation methods
    stem from two key factors. First, their ability to explain deep learning models
    is inherently constrained. Second, their focus is on imitating the inference process
    of the model, which primarily learns the distinctions between vulnerable and normal
    samples. This approach hinders their capacity to derive the underlying semantics
    of vulnerabilities. In contrast, SliceLocator leverages the learned distinctions
    between different sample types, combined with relevant taint flow knowledge, to
    predict the most vulnerability-related taint flows, effectively identifying the
    taint flows linked to vulnerabilities.


    <span id="page-14-0"></span>


    |              | (a).DWK-TLC |                |      |      | (b).ReV-TLC    |      |      |
    (c).IVD-TLC    |      |      | (d).Dev-TLC    |      |

    |--------------|-------------|----------------|------|------|----------------|------|------|----------------|------|------|----------------|------|

    | SliceLocator | 0.91        | 0.93           | 0.93 | 0.83 | 0.86           |
    0.86 | 0.9  | 0.9            | 0.9  | 0.85 | 0.88           | 0.88 |

    | Random       | 0.73        | 0.73           | 0.73 | 0.72 | 0.72           |
    0.73 | 0.73 | 0.73           | 0.74 | 0.72 | 0.74           | 0.74 |

    | GNN-LRP      | 0.35        | 0.54           | 0.61 | 0.33 | 0.45           |
    0.5  | 0.17 | 0.33           | 0.4  | 0.4  | 0.56           | 0.61 |

    | GradCAM      | 0.6         | 0.74           | 0.81 | 0.51 | 0.65           |
    0.68 | 0.49 | 0.69           | 0.76 | 0.61 | 0.74           | 0.79 |

    | DeepLift     | 0.23        | 0.37           | 0.43 | 0.39 | 0.52           |
    0.56 | 0.08 | 0.15           | 0.18 | 0.24 | 0.34           | 0.41 |

    | PGExplainer  | 0.23        | 0.36           | 0.4  | 0.47 | 0.58           |
    0.63 | 0.39 | 0.55           | 0.6  | 0.33 | 0.49           | 0.54 |

    | GNNExplainer | 0.25        | 0.41           | 0.46 | 0.35 | 0.5            |
    0.56 | 0.4  | 0.58           | 0.64 | 0.35 | 0.51           | 0.57 |

    |              |             | Top3 Top5 Top7 |      |      | Top3 Top5 Top7 |      |      |
    Top3 Top5 Top7 |      |      | Top3 Top5 Top7 |      |


    Fig. 7: Comparsion between SliceLocator with explanation approaches and random
    path selection in TLC.


    <span id="page-15-0"></span>


    |                    |      | Top3 Top5 Top7 |      |                    | Top3
    Top5 Top7 |      |               | Top3 Top5 Top7 |      |               | Top3
    Top5 Top7 |      |

    |--------------------|------|----------------|------|--------------------|----------------|------|---------------|----------------|------|---------------|----------------|------|

    | GNNExplainer       | 0.26 | 0.4            | 0.47 | 0.33               | 0.48           |
    0.55 | 0.45          | 0.6            | 0.66 | 0.33          | 0.45           |
    0.51 |

    | PGExplainer        | 0.25 | 0.39           | 0.45 | 0.32               | 0.47           |
    0.53 | 0.56          | 0.67           | 0.72 | 0.43          | 0.59           |
    0.63 |

    | DeepLift           | 0.42 | 0.57           | 0.61 | 0.48               | 0.58           |
    0.62 | 0.45          | 0.6            | 0.66 | 0.6           | 0.65           |
    0.67 |

    | GradCAM            | 0.2  | 0.39           | 0.49 | 0.39               | 0.52           |
    0.57 | 0.6           | 0.68           | 0.72 | 0.19          | 0.38           |
    0.47 |

    | GNN-LRP            | 0.3  | 0.47           | 0.54 | 0.38               | 0.5            |
    0.56 | 0.57          | 0.78           | 0.82 | 0.43          | 0.58           |
    0.62 |

    | Random             | 0.34 | 0.37           | 0.38 | 0.38               | 0.38           |
    0.39 | 0.37          | 0.39           | 0.4  | 0.38          | 0.38           |
    0.4  |

    | SliceLocator       | 0.68 | 0.79           | 0.8  | 0.74               | 0.88           |
    0.89 | 0.85          | 0.85           | 0.87 | 0.86          | 0.87           |
    0.92 |

    | (a.1).DWK-Sard-FLC |      |                |      | (a.2).ReV-Sard-FLC |                |      |
    (a.3).IVD-FLC |                |      | (a.4).Dev-FLC |                |      |


    <span id="page-15-1"></span>Fig. 8: Comparsion between SliceLocator with explanation
    approaches and random path selection in FLC.


    ![](_page_15_Figure_2.jpeg)


    Fig. 9: Vulnerability locating results by different explainers for the prediction
    of Reveal in the motivating example.


    ANSWER: SliceLocator outperforms other explanation methods in vulnerability localization
    because it more effectively integrates the model''s understanding of the differences
    between vulnerable and non-vulnerable code, along with predefined taint flow knowledge.


    ### 6.3 RQ2: SliceLocator VS Statement-level Detectors


    Prior research has explored both the use of explanation methods for locating vulnerability
    code lines based on binary classification detectors [\[8,](#page-20-11) [44\]](#page-23-7)
    and the direct training of statement-level detectors. In this section, we select
    two representative detectors, LineVul [\[18\]](#page-21-2) and LineVD [\[19\]](#page-21-3),
    as baselines for comparison.


    • LineVul employs a straightforward vulnerability detection approach. It fine-tunes
    a pre-trained CodeBERT [\[26\]](#page-21-10) model on a vulnerability dataset
    to directly train a function-level binary classifier. For functions predicted
    as vulnerable, LineVul


    leverages CodeBERT''s attention mechanism to compute the weight of each statement,
    then selects the top-k statements based on their weights as the vulnerability
    localization results.


    • LineVD directly predicts vulnerability at the statement level. It leverages
    a pretrained CodeBERT model to generate embeddings for functions and statements,
    which are then processed using a Graph Attention Network (GAT) [\[45\]](#page-23-8).
    A classifier is trained to predict the vulnerability of both function and statement
    embeddings, with predictions of 1 indicating vulnerability.


    <span id="page-16-0"></span>We first present the function-level vulnerability
    detection results of LineVD and LineVul in Table [4.](#page-16-0) LineVul performs
    excellently, outperforming IVDetect and Reveal, while LineVD shows considerably
    lower effectiveness. One possible explanation for this difference is that LineVD
    uses a shared classifier for both function and statement-level predictions.


    Table 4: Detection performance of linelevel detectors.


    | Detector | Accuracy | Precision | Recall | F1   |

    |----------|----------|-----------|--------|------|

    | LineVul  | 0.99     | 0.99      | 0.99   | 0.99 |

    | LineVD   | 0.78     | 0.61      | 0.87   | 0.72 |


    The vulnerability localization results of SliceLocator combined with four detectors,
    compared to LineVul and LineVD, are shown in Figure [10.](#page-17-0) It can be
    observed that, regardless of the detector used, SliceLocator consistently outperforms
    both LineVul and LineVD. LineVul locates vulnerability statements by computing
    the statement weights using the attention mechanism. However, like other instance-level
    explanation approaches, LineVul is constrained by two factors: (1) the model learns
    only the differences between vulnerable and non-vulnerable samples without taint
    inference capabilities, and (2) the attention mechanism may not serve as a perfect
    explanation method [\[8\]](#page-20-11). On the other hand, LineVD combines CodeBert
    and GAT to train a statement-level classifier. However, at the statement level,
    the issue of dataset imbalance is more pronounced than at the function level,
    and statements inherently contain more complex features, making it difficult to
    improve the training process using dataset balancing strategies. As a result,
    the classifier trained by LineVD struggles to detect vulnerability statements,
    yielding a TLC of only 0.01 and an FLC close to zero.


    ANSWER: SliceLocator generally outperforms other statement-level detectors. Existing
    statement-level detectors are similarly constrained by two key factors: (1) the
    detector has not learned predefined taint flow knowledge, and (2) the dataset
    exhibits a significant imbalance between vulnerability and non-vulnerability samples
    at the statement level.


    <span id="page-17-0"></span>


    |               | Top3      | Top5 | Top7 |  | Top3       | Top5 | Top7 |  |

    |---------------|-----------|------|------|--|------------|------|------|--|

    | SL + DWK      | 0.91      | 0.93 | 0.93 |  | 0.68       | 0.79 | 0.8  |  |

    | SL + Reveal   | 0.83      | 0.86 | 0.86 |  | 0.74       | 0.88 | 0.89 |  |

    | SL + IVDetect | 0.9       | 0.9  | 0.9  |  | 0.85       | 0.85 | 0.87 |  |

    | SL + Devign   | 0.85      | 0.88 | 0.88 |  | 0.86       | 0.87 | 0.92 |  |

    | LineVul       | 0.52      | 0.7  | 0.81 |  | 0.33       | 0.47 | 0.58 |  |

    | lineVD        | 0.05      | 0.05 | 0.05 |  | 0.01       | 0.01 | 0.01 |  |

    |               | (a.1).TLC |      |      |  | (a.2). FLC |      |      |  |


    Fig. 10: Comparsion between SliceLocator with statement-level detectors.


    ### 6.4 RQ3: SliceLocator VS Random Path Selection


    To further investigate the role of GNN-based detectors in flow path selection,
    we implemented a random path selection as a baseline. In this approach, instead
    of selecting the flow path with the highest weight based on the detector''s prediction,
    we randomly select a flow path. A comparison between random path selection and
    SliceLocator is shown in Figure [7](#page-14-0) and Figure [8,](#page-15-0) where
    Random denotes random path selection. We observe that after replacing the path
    selection strategy with random path selection, the TLC scores decrease by 0.11
    to 0.2, while the drop in FLC scores is even more significant. This further emphasizes
    that a well-trained GNN detector can effectively assist in selecting the most
    vulnerability-relevant taint flows for vulnerability localization.


    ANSWER: GNN-based detectors are crucial for SliceLocator, as they assist in selecting
    the optimal flow path for vulnerability localization.


    # 7 Threats to Validity


    First, we only conduct experiments on the SARD dataset, which contains synthetic
    and academic programs, but it may not be representative of real-world software
    products. We have discussed the problems in existing real-world datasets in section
    [5.2.](#page-12-0) It remains an open problem to generate reliable datasets on
    a fine-grained granularity and train a high-performing detector.


    Second, our experiments are limited to six vulnerability types in C/C++ programs.
    Nonetheless, our methodology can be effortlessly expanded to encompass additional
    source-sink vulnerabilities and other programming languages.


    Third, our approach only considers locating vulnerable statements based on four
    graph-based vulnerability detectors. However, our approach is easily applicable
    to other detectors, and potentially to other program analysis tasks.


    # 8 related work


    Conventional static analysis tools. Several conventional static program analysis
    frameworks(e.g. clang static analyzer [\[5\]](#page-20-1), Infer [\[4\]](#page-20-0),
    SVF [\[2\]](#page-19-1), MalWuKong [\[46\]](#page-23-9)) have


    been designed to detect vulnerabilities or identify malicious behaviors in software
    systems. clang static analyzer [\[5\]](#page-20-1) is a constraint-based static
    analysis tool that performs symbolic execution to explore paths in the program''s
    control-flow graph and detect potential bugs. While Infer [\[4\]](#page-20-0)
    is a static program analysis tool for detecting security issues such as null-pointer
    dereference and memory leaks based on abstract interpretation. SVF [\[2\]](#page-19-1)
    first parses a program into a sparse value-flow graph (SVFG) and then conducts
    path-sensitive source-sink analysis by traversing SVFG. The effect of conventional
    approaches depends on two factors: static analysis theories and security rules.
    static analysis theories include but are not limited to, parsing code into abstract
    structures (such as SVFG), where a better abstract structure facilitates the development
    of more sophisticated rules for detecting vulnerabilities. The effectiveness of
    detection rules depends on the expertise of the person who writes the rules. The
    quantity of rules is restricted, and it is impossible to encompass all of the
    vulnerability patterns, which frequently results in high rates of false positives
    and false negatives when analyzing intricate programs [\[6,](#page-20-2) [30\]](#page-22-3).


    Deep learning based vulnerability detection. Compared to conventional static analysis,
    another field is machine/deep-learning-based analysis [\[47–](#page-23-10)[49\]](#page-24-0).
    DeepBugs [\[50\]](#page-24-1) represents code via text vector for detecting name-based
    bugs. VGDetector [\[51\]](#page-24-2) uses a control flow graph and graph convolutional
    network to detect control-flow-related vulnerabilities. In this field, Devign
    [\[9\]](#page-20-3) and Reveal [\[7\]](#page-20-10) utilize graph representations
    to represent source code to detect vulnerabilities. They aim to pinpoint bugs
    at the function level. VulDeePecker [\[29\]](#page-22-2) applies code embedding
    using the data-flow information of a program for detecting resource management
    errors and buffer overflows. SySeVR [\[30\]](#page-22-3) and µVulDeePecker [\[52\]](#page-24-3)
    extend VulDeePecker by combining both control and data flow and different Recurrent
    neural networks(RNN) to detect various types of vulnerability. DeepWuKong [\[6\]](#page-20-2)
    utilizes program slicing methods to generate code fragments that are vectorized
    to apply the GNN model for classification. Hao et al. [\[53\]](#page-24-4) extend
    CFG in the domain of exception handling, subsequently leveraging this extension
    to enhance the detection capability of existing DL-based detectors for exception-handling
    bugs. W Zheng et al. [\[21\]](#page-21-5) combine DDG, CDG, and function call
    dependency graph (FCDG) into slice property graph (SPG), which is materialized
    into the implementation of the detection tool vulspg. Bin Yuan et al. [\[54\]](#page-24-5)
    construct a behavior graph for each function and implement VulBG to enhance the
    performance of DL-based detectors by behavior graphs. All these solutions can
    only detect vulnerabilities on coarse granularity, and they can only tell whether
    a given code fragment is vulnerable.


    Statement-level vulnerability detection. On the basis of deep learning vulnerability
    detection, fine-grained vulnerability detection has received increasing attention
    in recent years. More recently, Zou et al. [\[44\]](#page-23-7) propose an explanation
    framework to select key tokens in code gadgets generated by VulDeePecker and SeVCs
    generated by SySeVR to locate the vulnerable lines. VulDeeLocator [\[55\]](#page-24-6)
    compiles source codes into LLVM IRs, performs program slicing, and uses a customized
    neural network to predict relevance to vulnerabilities. LineVul [\[18\]](#page-21-2)
    analyses each function with fine-tuned CodeBert and ranks each statement based
    on attention scores, a higher attention score


    implies a stronger relation with vulnerability. IVDetect [\[8\]](#page-20-11)
    attain this goal by first identifying vulnerabilities at the source code level
    and utilizing the existing explanation approach GNNExplainer to generate a subgraph
    of the PDG to locate vulnerabilities in the function subsequently. However, several
    recent studies [\[16,](#page-21-0) [27,](#page-22-0) [28\]](#page-22-1) have substantiated
    the inefficiency of current explanation approaches in vulnerability detection.
    proved the inefficiency of current explanation approaches in vulnerability detection.
    LineVD [\[19\]](#page-21-3) leverages CodeBert and GAT to directly train a statement-level
    classifier, aiming to simultaneously predict both vulnerable functions and statements.
    However, this approach is constrained by the significant imbalance between vulnerable
    and non-vulnerable statements in the dataset.


    Machine-learning for software engineering. In addition to vulnerability detection,
    deep learning has made significant progress in recent years in software engineering
    tasks such as code clone detection and code understanding, The main difference
    between these methods lies in the different vectorization processes proposed for
    their specific tasks. The vectorizing pipelines can be categorized into tokens-based
    [\[56–](#page-24-7)[59\]](#page-25-0), ASTs-based [\[60–](#page-25-1)[63\]](#page-25-2)
    and graphs-based [\[64–](#page-25-3)[69\]](#page-26-0). Complex vectorizing pipelines
    often yield better results on specific tasks, but also rely on more precise program
    analysis theories.


    # 9 Conclusion


    In this paper, we present SliceLocatr. A tool that leverages the insights of GNNbased
    vulnerability detectors, which capture the differences between vulnerable and
    non-vulnerable samples—essentially vulnerability-fixing statements. Additionally,
    it incorporates taint flow knowledge related to vulnerabilities. By directly utilizing
    the predictions from detectors, SliceLocator selects the most relevant taint flow
    paths by assigning weights to these paths. The method begins with program slicing
    to extract flow paths of a code fragment, where each flow path concludes at a
    potential sink point (PSP). Afterward, SliceLocator applies a scoring function
    to assign importance scores to each path and selects the highest-weighted path
    as the most relevant explanation for the vulnerability data flow. We demonstrate
    the effectiveness of SliceLocator across six of the 30 most critical C/C++ vulnerabilities,
    showing that it outperforms several state-of-the-art GNN-based explainers and
    statement-level detectors in vulnerability detection tasks.


    # References


    - <span id="page-19-0"></span>[1] American Information Technology Laboratory:
    NATIONAL VULNERABILITY DATABASE. <https://nvd.nist.gov/> (2020)

    - <span id="page-19-1"></span>[2] Sui, Y., Xue, J.: Svf: interprocedural static
    value-flow analysis in llvm. In: Proceedings of the 25th International Conference
    on Compiler Construction, pp. 265–266 (2016)

    - <span id="page-19-2"></span>[3] Checkmarx. <https://www.checkmarx.com/> (2020)


    - <span id="page-20-0"></span>[4] Infer. <https://fbinfer.com/> (2020)

    - <span id="page-20-1"></span>[5] Clang static analyzer. <https://clang-analyzer.llvm.org/scan-build.html>
    (2020)

    - <span id="page-20-2"></span>[6] Cheng, X., Wang, H., Hua, J., Xu, G., Sui, Y.:
    Deepwukong: Statically detecting software vulnerabilities using deep graph neural
    network. ACM Trans. Softw. Eng. Methodol. 30(3) (2021) <https://doi.org/10.1145/3436877>

    - <span id="page-20-10"></span>[7] Chakraborty, S., Krishna, R., Ding, Y., Ray,
    B.: Deep learning based vulnerability detection: Are we there yet? IEEE Transactions
    on Software Engineering 48(9), 3280–3296 (2021)

    - <span id="page-20-11"></span>[8] Li, Y., Wang, S., Nguyen, T.N.: Vulnerability
    detection with fine-grained interpretations. (2021)

    - <span id="page-20-3"></span>[9] Zhou, Y., Liu, S., Siow, J.K., Du, X., Liu,
    Y.: Devign: Effective vulnerability identification by learning comprehensive program
    semantics via graph neural networks. In: Wallach, H.M., Larochelle, H., Beygelzimer,
    A., d''Alch´e-Buc, F., Fox, E.B., Garnett, R. (eds.) Advances in Neural Information
    Processing Systems 32: Annual Conference on Neural Information Processing Systems
    2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp. 10197–10207
    (2019). [https://proceedings.neurips.cc/paper/2019/](https://proceedings.neurips.cc/paper/2019/hash/49265d2447bc3bbfe9e76306ce40a31f-Abstract.html)
    [hash/49265d2447bc3bbfe9e76306ce40a31f-Abstract.html](https://proceedings.neurips.cc/paper/2019/hash/49265d2447bc3bbfe9e76306ce40a31f-Abstract.html)

    - <span id="page-20-4"></span>[10] Cheng, X., Nie, X., Li, N., Wang, H., Zheng,
    Z., Sui, Y.: How about bug-triggering paths? - understanding and characterizing
    learning-based vulnerability detectors. IEEE Transactions on Dependable and Secure
    Computing, 1–18 (2022) [https:](https://doi.org/10.1109/TDSC.2022.3192419) [//doi.org/10.1109/TDSC.2022.3192419](https://doi.org/10.1109/TDSC.2022.3192419)

    - <span id="page-20-5"></span>[11] Ying, R., Bourgeois, D., You, J., Zitnik, M.,
    Leskovec, J.: Gnnexplainer: Generating explanations for graph neural networks.
    Advances in neural information processing systems 32, 9240–9251 (2019)

    - <span id="page-20-6"></span>[12] Luo, D., Cheng, W., Xu, D., Yu, W., Zhang,
    .X.: Parameterized explainer for graph neural network (2020)

    - <span id="page-20-7"></span>[13] Pope, P.E., Kolouri, S., Rostami, M., Martin,
    C.E., Hoffmann, H.: Explainability methods for graph convolutional neural networks.
    In: 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
    (2020)

    - <span id="page-20-8"></span>[14] Schnake, T., Eberle, O., Lederer, J., Nakajima,
    S., Sch¨utt, K., M¨uller, K., Montavon, G.: Higher-order explanations of graph
    neural networks via relevant walks (2020)

    - <span id="page-20-9"></span>[15] Yuan, H., Yu, H., Gui, S., Ji, S.: Explainability
    in graph neural networks: A taxonomic survey. IEEE transactions on pattern analysis
    and machine intelligence 45(5), 5782–5799 (2022)

    - <span id="page-21-0"></span>[16] Cheng, B., Zhao, S., Wang, K., Wang, M., Bai,
    G., Feng, R., Guo, Y., Ma, L., Wang, H.: Beyond fidelity: Explaining vulnerability
    localization of learning-based detectors 33(5) (2024) <https://doi.org/10.1145/3641543>

    - <span id="page-21-1"></span>[17] Shrikumar, A., Greenside, P., Kundaje, A.:
    Learning important features through propagating activation differences. In: Precup,
    D., Teh, Y.W. (eds.) Proceedings of the 34th International Conference on Machine
    Learning. Proceedings of Machine Learning Research, vol. 70, pp. 3145–3153. PMLR,
    ??? (2017). [https:](https://proceedings.mlr.press/v70/shrikumar17a.html) [//proceedings.mlr.press/v70/shrikumar17a.html](https://proceedings.mlr.press/v70/shrikumar17a.html)

    - <span id="page-21-2"></span>[18] Fu, M., Tantithamthavorn, C.: Linevul: a transformer-based
    line-level vulnerability prediction. In: Proceedings of the 19th International
    Conference on Mining Software Repositories, pp. 608–620 (2022)

    - <span id="page-21-3"></span>[19] Hin, D., Kan, A., Chen, H., Babar, M.A.: Linevd:
    statement-level vulnerability detection using graph neural networks. In: Proceedings
    of the 19th International Conference on Mining Software Repositories, pp. 596–607
    (2022)

    - <span id="page-21-4"></span>[20] Dataset Repository <https://github.com/for-just-we/VulExplainerExp/>
    (2023)

    - <span id="page-21-5"></span>[21] Zheng, W., Jiang, Y., Su, X.: Vu1spg: Vulnerability
    detection based on slice property graph representation learning. In: 2021 IEEE
    32nd International Symposium on Software Reliability Engineering (ISSRE), pp.
    457–467 (2021). IEEE

    - <span id="page-21-6"></span>[22] Cheng, X., Wang, H., Hua, J., Zhang, M., Sui,
    Y.: Static detection of controlflow-related vulnerabilities using graph embedding.
    In: 2019 24th International Conference on Engineering of Complex Computer Systems
    (ICECCS) (2019)

    - <span id="page-21-7"></span>[23] Yamaguchi, F., Golde, N., Arp, D., Rieck, K.:
    Modeling and discovering vulnerabilities with code property graphs. In: 2014 IEEE
    Symposium on Security and Privacy (SP), pp. 590–604. IEEE Computer Society, Los
    Alamitos, CA, USA (2014). <https://doi.org/10.1109/SP.2014.44> . [https://doi.ieeecomputersociety.](https://doi.ieeecomputersociety.org/10.1109/SP.2014.44)
    [org/10.1109/SP.2014.44](https://doi.ieeecomputersociety.org/10.1109/SP.2014.44)

    - <span id="page-21-8"></span>[24] Mikolov, T., Sutskever, I., Chen, K., Corrado,
    G., Dean, J.: Distributed representations of words and phrases and their compositionality.
    In: Proceedings of the 26th International Conference on Neural Information Processing
    Systems - Volume 2. NIPS''13, pp. 3111–3119. Curran Associates Inc., USA (2013).
    <http://dl.acm.org/citation.cfm?id=2999792.2999959>

    - <span id="page-21-9"></span>[25] Le, Q.V., Mikolov, T.: Distributed representations
    of sentences and documents. In: ICML. JMLR Workshop and Conference Proceedings,
    vol. 32, pp. 1188– 1196. JMLR.org, ??? (2014). [http://dblp.uni-trier.de/db/conf/icml/icml2014.](http://dblp.uni-trier.de/db/conf/icml/icml2014.html#LeM14)
    [html#LeM14](http://dblp.uni-trier.de/db/conf/icml/icml2014.html#LeM14)

    - <span id="page-21-10"></span>[26] Feng, Z., Guo, D., Tang, D., Duan, N., Feng,
    X., Gong, M., Shou, L., Qin, B., Liu, T., Jiang, D., et al.: Codebert: A pre-trained
    model for programming and


    natural languages. arXiv preprint arXiv:2002.08155 (2020)


    - <span id="page-22-0"></span>[27] Ganz, T., H¨arterich, M., Warnecke, A., Rieck,
    K.: Explaining graph neural networks for vulnerability discovery. In: Proceedings
    of the 14th ACM Workshop on Artificial Intelligence and Security, pp. 145–156
    (2021)

    - <span id="page-22-1"></span>[28] Hu, Y., Wang, S., Li, W., Peng, J., Wu, Y.,
    Zou, D., Jin, H.: Interpreters for gnn-based vulnerability detection: Are we there
    yet? In: Proceedings of the 32nd ACM SIGSOFT International Symposium on Software
    Testing and Analysis, pp. 1407–1419 (2023)

    - <span id="page-22-2"></span>[29] Li, Z., Zou, D., Xu, S., Ou, X., Jin, H., Wang,
    S., Deng, Z., Zhong, Y.: Vuldeepecker: A deep learning-based system for vulnerability
    detection. The Network and Distributed System Security Symposium (NDSS) (2018)

    - <span id="page-22-3"></span>[30] Li, Z., Zou, D., Xu, S., Jin, H., Zhu, Y.,
    Chen, Z.: Sysevr: A framework for using deep learning to detect software vulnerabilities.
    IEEE Transactions on Dependable and Secure Computing, 1–1 (2021) [https://doi.org/10.1109/TDSC.](https://doi.org/10.1109/TDSC.2021.3051525)
    [2021.3051525](https://doi.org/10.1109/TDSC.2021.3051525)

    - <span id="page-22-4"></span>[31] Weiser, M.: Program slicing. In: Proceedings
    of the 5th International Conference on Software Engineering. ICSE ''81, pp. 439–449.
    IEEE Press, ??? (1981)

    - <span id="page-22-5"></span>[32] Fan, J., Li, Y., Wang, S., Nguyen, T.N.: A
    c/c++ code vulnerability dataset with code changes and cve summaries. In: MSR
    ''20: 17th International Conference on Mining Software Repositories (2020)

    - <span id="page-22-6"></span>[33] Zheng, Y., Pujar, S., Lewis, B., Buratti, L.,
    Epstein, E., Yang, B., Laredo, J., Morari, A., Su, Z.: D2a: A dataset built for
    ai-based vulnerability detection methods using differential analysis. In: Proceedings
    of the ACM/IEEE 43rd International Conference on Software Engineering: Software
    Engineering in Practice. ICSE-SEIP ''21. Association for Computing Machinery,
    New York, NY, USA (2021)

    - <span id="page-22-7"></span>[34] Croft, R., Babar, M.A., Kholoosi, M.M.: Data
    quality for software vulnerability datasets. In: 2023 IEEE/ACM 45th International
    Conference on Software Engineering (ICSE), pp. 121–133 (2023). IEEE

    - <span id="page-22-8"></span>[35] Nie, X., Li, N., Wang, K., Wang, S., Luo, X.,
    Wang, H.: Understanding and tackling label errors in deep learning-based vulnerability
    detection (experience paper). In: Proceedings of the 32nd ACM SIGSOFT International
    Symposium on Software Testing and Analysis, pp. 52–63 (2023)

    - <span id="page-22-9"></span>[36] Software Assurance Reference Dataset. [https://samate.nist.gov/SARD/index.](https://samate.nist.gov/SARD/index.php)
    [php](https://samate.nist.gov/SARD/index.php) (2017)

    - <span id="page-23-0"></span>[37] Fey, M., Lenssen, J.E.: Fast graph representation
    learning with PyTorch Geometric. In: ICLR Workshop on Representation Learning
    on Graphs and Manifolds (2019)

    - <span id="page-23-1"></span>[38] Cheng, Xiao and Wang, Haoyu and Hua, Jiayi
    and Xu, Guoai and Sui, Yulei <https://github.com/jumormt/DeepWukong> (2021)

    - <span id="page-23-2"></span>[39] Yi Li, Shaohua Wang, Tien N. Nguyen [https://github.com/](https://github.com/vulnerabilitydetection/VulnerabilityDetectionResearch)
    [vulnerabilitydetection/VulnerabilityDetectionResearch](https://github.com/vulnerabilitydetection/VulnerabilityDetectionResearch)
    (2021)

    - <span id="page-23-3"></span>[40] Yaqin Zhou and Shangqing Liu and Jing Kai Siow
    and Xiaoning Du and Yang Liu <https://github.com/vulnerabilitydetection/VulnerabilityDetectionResearch>
    (2019)

    - <span id="page-23-4"></span>[41] Chakraborty, Saikat and Krishna, Rahul and
    Ding, Yangruibo and Ray, Baishakhi <https://github.com/VulDetProject/ReVeal> (2020)

    - <span id="page-23-5"></span>[42] Kingma, D.P., Ba, J.: Adam: A method for stochastic
    optimization. In: Bengio, Y., LeCun, Y. (eds.) 3rd International Conference on
    Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference
    Track Proceedings (2015). <http://arxiv.org/abs/1412.6980>

    - <span id="page-23-6"></span>[43] Liu, M., Luo, Y., Wang, L., Xie, Y., Yuan,
    H., Gui, S., Yu, H., Xu, Z., Zhang, J., Liu, Y., Yan, K., Liu, H., Fu, C., Oztekin,
    B.M., Zhang, X., Ji, S.: DIG: A turnkey library for diving into graph deep learning
    research. Journal of Machine Learning Research 22(240), 1–9 (2021)

    - <span id="page-23-7"></span>[44] Zou, D., Zhu, Y., Jin, H., Ye, H., Zhu, Y.,
    Ye, H., Xu, S., Li, Z.: Interpreting deep learning-based vulnerability detector
    predictions based on heuristic searching. ACM Transactions on Software Engineering
    and Methodology 30 (2020) [https:](https://doi.org/10.1145/3429444) [//doi.org/10.1145/3429444](https://doi.org/10.1145/3429444)

    - <span id="page-23-8"></span>[45] Velikovi, P., Cucurull, G., Casanova, A., Romero,
    A., Lio, P., Bengio, Y.: Graph attention networks. arXiv preprint arXiv:1710.10903
    (2017)

    - <span id="page-23-9"></span>[46] Li, N., Wang, S., Feng, M., Wang, K., Wang,
    M., Wang, H.: Malwukong: Towards fast, accurate, and multilingual detection of
    malicious code poisoning in oss supply chains. In: 2023 38th IEEE/ACM International
    Conference on Automated Software Engineering (ASE), pp. 1993–2005 (2023). IEEE

    - <span id="page-23-10"></span>[47] Neuhaus, S., Zimmermann, T.: The beauty and
    the beast: Vulnerabilities in red hat''s packages. In: In Proceedings of the 2009
    USENIX Annual Technical Conference (USENIX ATC (2009)

    - [48] Grieco, G., Grinblat, G.L., Uzal, L., Rawat, S., Feist, J., Mounier, L.:
    Toward large-scale vulnerability discovery using machine learning. In: Proceedings
    of the Sixth ACM Conference on Data and Application Security and Privacy.


    CODASPY ''16, pp. 85–96. ACM, New York, NY, USA (2016). [https://doi.org/](https://doi.org/10.1145/2857705.2857720)
    [10.1145/2857705.2857720](https://doi.org/10.1145/2857705.2857720) . <http://doi.acm.org/10.1145/2857705.2857720>


    - <span id="page-24-0"></span>[49] Yan, H., Sui, Y., Chen, S., Xue, J.: Machine-learning-guided
    typestate analysis for static use-after-free detection. In: Proceedings of the
    33rd Annual Computer Security Applications Conference. ACSAC 2017, pp. 42–54.
    Association for Computing Machinery, New York, NY, USA (2017). [https://doi.org/10.1145/3134600.](https://doi.org/10.1145/3134600.3134620)
    [3134620](https://doi.org/10.1145/3134600.3134620) . <https://doi.org/10.1145/3134600.3134620>

    - <span id="page-24-1"></span>[50] Pradel, M., Sen, K.: Deepbugs: A learning approach
    to name-based bug detection. Proc. ACM Program. Lang. 2(OOPSLA), 147–114725 (2018)
    [https://doi.org/10.](https://doi.org/10.1145/3276517) [1145/3276517](https://doi.org/10.1145/3276517)

    - <span id="page-24-2"></span>[51] Cheng, X., Wang, H., Hua, J., Zhang, M., Xu,
    G., Yi, L., Sui, Y.: Static detection of control-flow-related vulnerabilities
    using graph embedding. In: 2019 24th International Conference on Engineering of
    Complex Computer Systems (ICECCS), pp. 41–50 (2019). <https://doi.org/10.1109/ICECCS.2019.00012>

    - <span id="page-24-3"></span>[52] Zou, D., Wang, S., Xu, S., Li, Z., Jin, H.:
    muvuldeepecker: A deep learning-based system for multiclass vulnerability detection.
    IEEE Transactions on Dependable and Secure Computing 18(5), 2224–2236 (2021) <https://doi.org/10.1109/TDSC.2019.2942930>

    - <span id="page-24-4"></span>[53] Zhang, H., Luo, J., Hu, M., Yan, J., Zhang,
    J., Qiu, Z.: Detecting exception handling bugs in c++ programs. In: 2023 IEEE/ACM
    45th International Conference on Software Engineering (ICSE), pp. 1084–1095 (2023).
    IEEE

    - <span id="page-24-5"></span>[54] Yuan, B., Lu, Y., Fang, Y., Wu, Y., Zou, D.,
    Li, Z., Li, Z., Jin, H.: Enhancing deep learning-based vulnerability detection
    by building behavior graph model. In: 2023 IEEE/ACM 45th International Conference
    on Software Engineering (ICSE), pp. 2262–2274 (2023). IEEE

    - <span id="page-24-6"></span>[55] Li, Z., Zou, D., Xu, S., Chen, Z., Zhu, Y.,
    Jin, H.: Vuldeelocator: A deep learningbased fine-grained vulnerability detector.
    IEEE Transactions on Dependable and Secure Computing PP, 1–1 (2021) <https://doi.org/10.1109/TDSC.2021.3076142>

    - <span id="page-24-7"></span>[56] Kamiya, T., Kusumoto, S., Inoue, K.: Ccfinder:
    a multilinguistic token-based code clone detection system for large scale source
    code. IEEE Transactions on Software Engineering 28(7), 654–670 (2002) <https://doi.org/10.1109/tse.2002.1019480>

    - [57] Li, Z., Lu, S., Myagmar, S., Zhou, Y.: Cp-miner: finding copy-paste and
    related bugs in large-scale software code. IEEE Transactions on Software Engineering
    32(3), 176–192 (2006) <https://doi.org/10.1109/TSE.2006.28>

    - [58] Sajnani, H., Lopes, C.: A parallel and efficient approach to large scale
    clone detection. In: 2013 7th International Workshop on Software Clones (IWSC),
    pp. 46–52 (2013). <https://doi.org/10.1109/IWSC.2013.6613042>


    - <span id="page-25-0"></span>[59] Sajnani, H., Saini, V., Svajlenko, J., Roy,
    C.K., Lopes, C.V.: Sourcerercc: Scaling code clone detection to big-code. In:
    Proceedings of the 38th International Conference on Software Engineering. ICSE
    ''16, pp. 1157–1168. ACM, New York, NY, USA (2016). <https://doi.org/10.1145/2884781.2884877>
    . [http://doi.acm.org/10.](http://doi.acm.org/10.1145/2884781.2884877) [1145/2884781.2884877](http://doi.acm.org/10.1145/2884781.2884877)

    - <span id="page-25-1"></span>[60] Zhang, J., Wang, X., Zhang, H., Sun, H., Wang,
    K., Liu, X.: A novel neural source code representation based on abstract syntax
    tree. In: Proceedings of the 41st International Conference on Software Engineering.
    ICSE ''19, pp. 783–794. IEEE Press, Piscataway, NJ, USA (2019). <https://doi.org/10.1109/ICSE.2019.00086>
    . <https://doi.org/10.1109/ICSE.2019.00086>

    - [61] Wang, S., Liu, T., Tan, L.: Automatically learning semantic features for
    defect prediction. In: Proceedings of the 38th International Conference on Software
    Engineering. ICSE ''16, pp. 297–308. ACM, New York, NY, USA (2016). [https://doi.](https://doi.org/10.1145/2884781.2884804)
    [org/10.1145/2884781.2884804](https://doi.org/10.1145/2884781.2884804) . <http://doi.acm.org/10.1145/2884781.2884804>

    - [62] Alon, U., Zilberstein, M., Levy, O., Yahav, E.: Code2vec: Learning distributed
    representations of code. Proc. ACM Program. Lang. 3(POPL), 40–14029 (2019) <https://doi.org/10.1145/3290353>

    - <span id="page-25-2"></span>[63] Allamanis, M., Brockschmidt, M., Khademi, M.:
    Learning to represent programs with graphs. CoRR abs/1711.00740 (2017) [arXiv:1711.00740](https://arxiv.org/abs/1711.00740)

    - <span id="page-25-3"></span>[64] Chen, K., Liu, P., Zhang, Y.: Achieving accuracy
    and scalability simultaneously in detecting application clones on android markets.
    In: Proceedings of the 36th International Conference on Software Engineering.
    ICSE 2014, pp. 175–186. ACM, New York, NY, USA (2014). <https://doi.org/10.1145/2568225.2568286>
    . <http://doi.acm.org/10.1145/2568225.2568286>

    - [65] Gabel, M., Jiang, L., Su, Z.: Scalable detection of semantic clones. In:
    Proceedings of the 30th International Conference on Software Engineering. ICSE
    ''08, pp. 321–330. ACM, New York, NY, USA (2008). [https://doi.org/10.1145/1368088.](https://doi.org/10.1145/1368088.1368132)
    [1368132](https://doi.org/10.1145/1368088.1368132) . <http://doi.acm.org/10.1145/1368088.1368132>

    - [66] Komondoor, R., Horwitz, S.: Using slicing to identify duplication in source
    code. In: Cousot, P. (ed.) Static Analysis, pp. 40–56. Springer, Berlin, Heidelberg
    (2001)

    - [67] Krinke, J.: Identifying similar code with program dependence graphs. In:
    Proceedings of the Eighth Working Conference on Reverse Engineering (WCRE''01).
    WCRE ''01, p. 301. IEEE Computer Society, Washington, DC, USA (2001). <http://dl.acm.org/citation.cfm?id=832308.837142>

    - [68] Liu, C., Chen, F., Han, J., Yu, P.: Gplag: Detection of software plagiarism
    by program dependence graph analysis. In: Proceedings of the 12th ACM SIGKDD International
    Conference on Knowledge Discovery and Data Mining, vol. 2006, pp. 872–881 (2006).
    <https://doi.org/10.1145/1150402.1150522>


    <span id="page-26-0"></span>[69] Sui, Y., Cheng, X., Zhang, G., Wang, H.: Flow2vec:
    Value-flow-based precise code embedding. Proc. ACM Program. Lang. 4(OOPSLA) (2020)
    [https://doi.org/10.](https://doi.org/10.1145/3428301) [1145/3428301](https://doi.org/10.1145/3428301)'
  decisions:
    language: '- Qualified. Reason: English Paper'
    evaluation_prompt: Qualified.
    related_work_prompt: Qualified
    novelty_prompt: Qualified
    review_only_prompt: Qualified
  llm_input_used: '## Abstract

    Vulnerability detection is a crucial component in the software development

    lifecycle. Existing vulnerability detectors, especially those based on deep

    learning (DL) models, have achieved high effectiveness. Despite their

    capability of detecting vulnerable code snippets from given code fragments, the

    detectors are typically unable to further locate the fine-grained information

    pertaining to the vulnerability, such as the precise vulnerability triggering

    locations. Although explanation methods can filter important statements based

    on the predictions of code fragments, their effectiveness is limited by the

    fact that the model primarily learns the difference between vulnerable and

    non-vulnerable samples. In this paper, we propose SliceLocator, which, unlike

    previous approaches, leverages the detector''s understanding of the differences

    between vulnerable and non-vulnerable samples, essentially,

    vulnerability-fixing statements. SliceLocator identifies the most relevant

    taint flow by selecting the highest-weighted flow path from all potential

    vulnerability-triggering statements in the program, in conjunction with the

    detector. We demonstrate that SliceLocator consistently performs well on four

    state-of-the-art GNN-based vulnerability detectors, achieving an accuracy of

    around 87% in flagging vulnerability-triggering statements across six common

    C/C++ vulnerabilities. It outperforms five widely used GNN-based explanation

    methods and two statement-level detectors.


    ## Introduction

    The proliferation of modern software programs developed for diverse purposes and
    usage scenarios is inevitably and persistently coupled with intensified security
    threats from vulnerabilities, evidenced by the substantial surge in the volume
    of reported vulnerabilities via the Common Vulnerabilities and Exposures (CVE)
    [\[1\]](#page-19-0). To counteract the potential exploitation, both academia and
    industrial communities have proposed numerous techniques for identifying and locating
    those vulnerabilities.


    Traditional approaches, such as the rule-based analysis techniques (e.g., SVF
    [\[2\]](#page-19-1), Checkmarx [\[3\]](#page-19-2), Infer [\[4\]](#page-20-0),
    and clang static analyzer [\[5\]](#page-20-1)), leverage predefined signatures
    or rules to identify vulnerabilities. Unfortunately, similar to other static analysis
    techniques, they typically suffer from high false positive and negative rates
    [\[6\]](#page-20-2). More recently, DL-based detection techniques [\[6–](#page-20-2)[9\]](#page-20-3),
    which generally operate on extracted code feature representations, have shown
    great effectiveness in flagging vulnerabilitycontaining code fragments (i.e.,
    functions or slices). However, the coarse granularity and the black-box nature
    of the analysis renders poor interpretability in the detection results. For example,
    a function or a code snippet could contain over a dozen code lines, which remains
    challenging for the developers to understandthe root cause of the vulnerabilities
    and further take action to fix them. A recent work [\[10\]](#page-20-4) suggests
    that the bug trigger path is the key to locating and fixing a vulnerability.


    One promising way to tackle this problem is leveraging explanation approaches
    to select important features for the DL-based detectors, and then mapping them
    to the corresponding code lines. Recent rapid advances in graph-based explainability
    technology show great potential for this solution. In particular, the existing
    explanation methods commonly facilitate model interpretability from three angles:
    assigning numeric values to graph edges [\[11,](#page-20-5) [12\]](#page-20-6),
    computing importance scores for nodes [\[13\]](#page-20-7), and calculating scores
    for graph walks while traversing through GNNs [\[14\]](#page-20-8). Despite their
    success in tasks such as molecular graph classifications, current GNN-based explanation
    methodologies exhibit inherent limitations that impede their direct applicability
    in extracting fine-grained vulnerability-related information, particularly in
    identifying relevant statements.


    Limitations. The first limitation of GNN explanation methods lies in their reliance
    on selecting the most influential parts of the graph for model inference. However,
    they may not always accurately identify these critical components [\[15\]](#page-20-9).
    A thorough analysis of the GNN inference process and the explanation methods is
    required to further validate this point, which, however, falls beyond the scope
    of our work. More importantly, according to previous studies [\[16\]](#page-21-0),
    GNN-based detectors


    are likely to focus primarily on learning the differences between vulnerable and
    nonvulnerable samples for inference, rather than relying on domain-specific knowledge
    such as taint flow. Moreover, the differences between vulnerable and non-vulnerable
    samples may not be limited to the location of code fixes, but could also involve
    other structural changes in the graph, such as alterations in topology due to
    added conditional statements (e.g., if clauses). Explanation methods tend to amplify
    these differences.


    Insights. Graph-based detectors, while potentially learning irrelevant features,
    show high sensitivity to vulnerability fixes. For example, in Devign, masking
    vulnerable fixing statements (VFS) reduces the vulnerability probability by 0.32
    on average, while masking other locations causes a loss of no more than 0.15.
    Since VFS and vulnerable triggering statements (VTS) are strongly data-dependent,
    and data flow graphs are sparse [\[2\]](#page-19-1), this suggests that vulnerabilities
    are likely triggered at the VTS. The detector often assigns the flow linking VFS
    and VTS high weight, and due to the sparsity of data flows, the highest-weighted
    flow traced back from VTS is likely the vulnerable flow.


    Solution. In this work, we propose SliceLocator, a novel approach to identify
    fine-grained information from vulnerable code reported by GNN-based vulnerability
    detectors. Given a detected vulnerable code fragment, the key idea behind SliceLocator
    is to leverage GNN-based detectors to identify the highest-weighted taint flow
    from the VFS to the VTS. The core step of SliceLocator involves performing backward
    program slicing based on potential sink points (PSPs). First, a set of flow paths
    is extracted, and then, using GNN-based detectors, the weight of each path is
    predicted. The path with the highest weight is selected as the most relevant flow
    for vulnerability localization. Compared with prior works (e.g., DeepWukong [\[6\]](#page-20-2)),
    SliceLocator only preserves vulnerability-triggering and vulnerability-dependent
    program path-level information, rather than that of the full program. This significantly
    improves the analysis efficiency as program paths contain a smaller number of
    code lines. Leveraging the program slicing method, SliceLocator captures more
    semantic information encompassed in code lines. As a result, it can provide more
    accurate localization results than the approaches only focusing on topological
    features.


    Evaluation. We follow previous study [\[16\]](#page-21-0) to use vulnerability-triggering
    code line coverage (TLC) and vulnerability-fixing code line coverage (FLC) to
    evaluate the effectiveness of SliceLocator. We apply SliceLocator to four detectors,
    including DeepWukong [\[6\]](#page-20-2), Reveal [\[7\]](#page-20-10), IVDetect
    [\[8\]](#page-20-11), Devign [\[9\]](#page-20-3) and perform multi-dimensional
    evaluations. In the first phase, we assess the performance of SliceLocator by
    comparing it to the other five explanation methods, including PGExplainer [\[12\]](#page-20-6),
    GNNExplainer [\[11\]](#page-20-5), GNN-LRP [\[14\]](#page-20-8), GradCAM [\[13\]](#page-20-7),
    and DeepLift [\[17\]](#page-21-1). The experimental data indicate that the SliceLocator,
    combined with four detectors, achieves a TLC score ranging from 0.83 to 0.93 and
    an FLC score of at least around 0.7. This performance is not only superior to
    the other five explanation methods but also demonstrates minimal deviation across
    different detectors. In the second phase, we compare SliceLocator''s performance
    combined with four graph-based detectors against two deep learningbased statement-level
    detectors, LineVul [\[18\]](#page-21-2) and LineVD [\[19\]](#page-21-3). Experimental
    data demonstrate that SliceLocator consistently outperforms both LineVul and LineVD.


    <span id="page-3-0"></span>![](_page_3_Figure_0.jpeg)


    Fig. 1: General detection phase of deep-learning-based vulnerability detectors
    with graph representations


    This higher performance can be attributed to SliceLocator''s ability to effectively
    leverage both the detector''s sensitivity to VFS and heuristic taint flow knowledge.
    Unlike other explanation methods and statement-level detectors, which fail to
    fully exploit this critical information, SliceLocator combines these factors to
    enhance its vulnerability localization accuracy. The data supporting the paper
    can be accessed at [\[20\]](#page-21-4).


    In summary, we make the following main contributions:


    - A novel vulnerability statement locating technique via GNN-based vulnerability
    detectors. Given the inadequate explainability of the existing GNNbased vulnerability
    detectors, we propose the framework SliceLocator as a solution. It can identify
    important flow paths in a program that contain vulnerabilitytriggering statements,
    providing finer-grained semantics contexts for the identified vulnerabilities.

    - Approach effectiveness. Through a multi-dimensional evaluation of a comprehensive
    benchmark dataset, we demonstrate that SliceLocator outperforms existing explanation
    methods in terms of both TLC and FLC, which are crucial factors influencing vulnerability
    localization and fixing. On average, SliceLocator achieves a TLC of 0.89 and an
    FLC of 0.85 across all vulnerability detectors used in this study, highlighting
    its strong generalization ability across different GNN-based vulnerability detectors.'
  token_usage: 8946
  time_usage: 6.4184300899505615
- title: "\"My GitHub Sponsors profile is live!\" Investigating the Impact of\n  Twitter/X\
    \ Mentions on GitHub Sponsors"
  abstract: 'GitHub Sponsors was launched in 2019, enabling donations to open-source

    software developers to provide financial support, as per GitHub''s slogan:

    "Invest in the projects you depend on". However, a 2022 study on GitHub

    Sponsors found that only two-fifths of developers who were seeking sponsorship

    received a donation. The study found that, other than internal actions (such as

    offering perks to sponsors), developers had advertised their GitHub Sponsors

    profiles on social media, such as Twitter (also known as X). Therefore, in this

    work, we investigate the impact of tweets that contain links to GitHub Sponsors

    profiles on sponsorship, as well as their reception on Twitter/X. We further

    characterize these tweets to understand their context and find that (1) such

    tweets have the impact of increasing the number of sponsors acquired, (2)

    compared to other donation platforms such as Open Collective and Patreon,

    GitHub Sponsors has significantly fewer interactions but is more visible on

    Twitter/X, and (3) developers tend to contribute more to open-source software

    during the week of posting such tweets. Our findings are the first step toward

    investigating the impact of social media on obtaining funding to sustain

    open-source software.'
  url: http://arxiv.org/abs/2401.02755v1
  keywords: ''
  document: '# "My GitHub Sponsors profile is live!" Investigating the Impact of Twitter/X
    Mentions on GitHub Sponsors


    Youmei Fan Nara Institute of Science and Technology, Japan fan.youmei.fs2@is.naist.jp


    Tao Xiao Nara Institute of Science and Technology, Japan tao.xiao.ts2@is.naist.jp


    Christoph Treude University of Melbourne Australia christoph.treude@unimelb.edu.au


    Kenichi Matsumoto Nara Institute of Science and Technology, Japan matumoto@is.naist.jp


    Lisbon, Portugal. ACM, New York, NY, USA, [12](#page-11-0) pages. [https://doi.org/10.](https://doi.org/10.1145/3597503.3639127)
    [1145/3597503.3639127](https://doi.org/10.1145/3597503.3639127)


    Hideaki Hata Shinshu University Japan hata@shinshu-u.ac.jp


    # ABSTRACT


    GitHub Sponsors was launched in 2019, enabling donations to opensource software
    developers to provide financial support, as per GitHub''s slogan: "Invest in the
    projects you depend on". However, a 2022 study on GitHub Sponsors found that only
    two-fifths of developers who were seeking sponsorship received a donation. The
    study found that, other than internal actions (such as offering perks to sponsors),
    developers had advertised their GitHub Sponsors profiles on social media, such
    as Twitter (also known as X). Therefore, in this work, we investigate the impact
    of tweets that contain links to GitHub Sponsors profiles on sponsorship, as well
    as their reception on Twitter/X. We further characterize these tweets to understand
    their context and find that (1) such tweets have the impact of increasing the
    number of sponsors acquired, (2) compared to other donation platforms such as
    Open Collective and Patreon, GitHub Sponsors has significantly fewer interactions
    but is more visible on Twitter/X, and (3) developers tend to contribute more to
    open-source software during the week of posting such tweets. Our findings are
    the first step toward investigating the impact of social media on obtaining funding
    to sustain open-source software.


    ### CCS CONCEPTS


    • Social and professional topics → Sustainability; • Software and its engineering
    → Open source model.


    ### KEYWORDS


    Open-source Software, Sponsorship, Social Media


    #### ACM Reference Format:


    Youmei Fan, Tao Xiao, Hideaki Hata, Christoph Treude, and Kenichi Matsumoto. 2024.
    "My GitHub Sponsors profile is live!" Investigating the Impact of Twitter/X Mentions
    on GitHub Sponsors. In 2024 IEEE/ACM 46th International Conference on Software
    Engineering (ICSE ''24), April 14–20, 2024,


    ICSE ''24, April 14–20, 2024, Lisbon, Portugal


    © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
    ACM ISBN 979-8-4007-0217-4/24/04. . . \$15.00 <https://doi.org/10.1145/3597503.3639127>


    ### 1 INTRODUCTION


    Open-source software (OSS) is ubiquitous, but sustaining it is a challenge [\[28\]](#page-11-1).
    Maintaining an OSS project requires not only intrinsic motivation (e.g., joy of
    participation) but also extrinsic motivation (e.g., financial incentives) [\[49\]](#page-11-2).
    The last few years have seen the emergence of many platforms that allow open-source
    developers to receive donations for their work, such as PayPal [\[30\]](#page-11-3),
    Open Collective [\[11\]](#page-11-4), Patreon [\[29\]](#page-11-5), and GitHub
    Sponsors [\[37\]](#page-11-6). Several platforms support sponsoring OSS projects
    in cryptocurrencies, with the rise in popularity of cryptocurrencies today, e.g.,
    Gitcoin Grants [\[16\]](#page-11-7) and Giveth [\[20\]](#page-11-8). However,
    as Overney et al.''s paper title "How to not get rich: an empirical study of donations
    in open source" [\[28\]](#page-11-1) suggests, simply having a platform for donations
    is not enough. In a 2022 study on GitHub Sponsors, Shimada et al. [\[35\]](#page-11-9)
    found that out of approximately 9,000 developers who had activated their GitHub
    Sponsors profile, less than 40% had received a donation.


    If simply creating a sponsorship profile is not enough, what else can open-source
    software developers do to attract donations? Following the long line of work on
    studying the intersection between social media and software development [\[15,](#page-11-10)
    [38,](#page-11-11) [39\]](#page-11-12), in this paper, we investigate the impact
    of tweeting about a GitHub Sponsors profile on sponsorship. To make it easy for
    its users to reach a large audience, GitHub provides tweet templates that users
    can use to advertise a new GitHub Sponsors profile ("My GitHub Sponsors profile
    is live! You can sponsor me to support my open source work ") or to broadcast
    that they made a donation (" I''m sponsoring [username] because..."). The impact
    of tweets on open-source software development has been investigated before. Fang
    et al. [\[15\]](#page-11-10) found that tweets have a significant effect on obtaining
    new stars and new contributors for an open-source project and that the formation
    of an active Twitter/X community plays an important role in attracting new contributors.
    The role of tweets has also been studied in the context of bug fixing [\[24\]](#page-11-13)
    and trend awareness [\[36\]](#page-11-14). To the best of our knowledge, the role
    of Twitter/X in obtaining funding for open-source development has not yet been
    studied.


    We first characterize the state of the practice by quantitatively and qualitatively
    analyzing more than 10,000 tweets linking to GitHub Sponsors profiles to understand
    the context of such tweets.


    Permission to make digital or hard copies of all or part of this work for personal
    or classroom use is granted without fee provided that copies are not made or distributed
    for profit or commercial advantage and that copies bear this notice and the full
    citation on the first page. Copyrights for components of this work owned by others
    than the author(s) must be honored. Abstracting with credit is permitted. To copy
    otherwise, or republish, to post on servers or to redistribute to lists, requires
    prior specific permission and/or a fee. Request permissions from permissions@acm.org.


    We then measure the impact of the tweets in terms of their reception on Twitter/X
    and their effect on sponsorship. We found links to GitHub Sponsors profiles on
    Twitter/X are common and the majority of such tweets are written by users other
    than the profile owner, such as a sponsor. We identified a significant positive
    effect of GitHub Sponsor profile mentions in tweets on the number of sponsors
    acquired. Tweet mentions have the impact of increasing the number of sponsors
    by 1.22. Although GitHub Sponsors has surpassed other platforms such as Open Collective
    and Patreon in terms of visibility on Twitter/X, tweets about GitHub Sponsors
    received significantly fewer likes, retweets, and replies compared to other platforms.
    Developers tended to be more active during the week of a tweet, in particular,
    in terms of the number of commits.


    Significance of research contribution. The findings of our study have significant
    implications, indicating a strong interconnection between social media channels
    and donation pathways within the social programmer ecosystem [\[42\]](#page-11-15).
    Our research demonstrates that actively engaging on social media platforms to
    promote sponsorship opportunities for open-source development can yield fruitful
    outcomes. This suggests that open-source developers stand to benefit from expanding
    their presence and networking efforts beyond the GitHub platform. Furthermore,
    our study highlights the notion that publicity and visibility in the realm of
    open-source sponsorship need not be limited to a unidirectional flow. Rather,
    sponsors themselves have the potential to enhance the exposure and reach of open-source
    projects by publicizing their donations. In doing so, they serve as exemplars,
    setting a positive precedent and inspiring others to follow suit. By emphasizing
    these key findings, we provide compelling evidence to support the notion that
    using social media channels, diversifying online networks, and fostering mutual
    publicity between sponsors and developers can yield substantial advantages within
    the open-source community. These insights encourage open-source developers and
    sponsors alike to consider the broader potential of social media engagement and
    collaborative promotion to achieve their goals.


    #### 2 RESEARCH QUESTIONS


    The main objective of our study is to understand the state of practice and the
    impact of GitHub Sponsors profile mentions on Twitter/X. The insights drawn from
    this study will not only contribute to the academic understanding but also have
    practical implications for developers, sponsors, and platforms like GitHub. Furthermore,
    our findings can shed light on the relationship between social activities and
    monetary contributions, ultimately serving to augment the appeal of developers.
    To guide our investigation, we present main questions and sub-questions, along
    with motivations and relevance. RQ1: How are GitHub Sponsors profiles discussed
    on Twitter/X? The motivation behind RQ1 is to provide insights into the dynamics
    of GitHub Sponsors profile mentions, ultimately informing better strategies for
    developers seeking sponsorship.


    RQ1.1 What are the characteristics of tweets mentioning GitHub Sponsors profiles
    from organizational and personal accounts? Understanding the language, account
    types, and programming languages in these tweets will enable developers to craft
    more appealing content for potential sponsors, ultimately enhancing engagement.


    RQ1.2 Who mentions GitHub Sponsors profiles on Twitter/X? By


    identifying who engages with these profiles, sponsorship acquisition strategies
    can be tailored to target specific demographics.


    RQ1.3 What is the context of GitHub Sponsors profile mentions on Twitter/X? Investigating
    the context in which profiles are mentioned will shed light on why tweets are
    used in sponsorship communication, potentially informing strategies for developers
    seeking sponsorship.


    RQ1.4 When are GitHub Sponsors profiles mentioned on Twitter/X? Analyzing the
    timing of mentions can lead to the discovery of optimal moments for posting, which
    could help in securing sponsorship.


    RQ2: What is the impact of GitHub Sponsors profile mentions on Twitter/X? Building
    on RQ1, RQ2 explores the effects of the dynamics uncovered, allowing us to measure
    and interpret their impact.


    RQ2.1 How are GitHub Sponsors profile mentions received on Twitter/X? Understanding
    the reception will aid platforms like GitHub in providing targeted support and
    tools to developers, such as social media templates and guidelines.


    RQ2.2 How are GitHub Sponsors profile mentions discussed on Twitter/X? By examining
    engagement metrics and replies, we will gain a deeper understanding of the conversations,
    ultimately helping in crafting more effective strategies for community engagement.


    RQ2.3 How do GitHub Sponsors profile mentions impact sponsorship? Through a quasi-experimental
    approach, our goal is to provide quantitative evidence of the causal impact, which
    can guide both developers in improving their social media strategies and platforms
    in enhancing features that facilitate sponsorship acquisition.


    By addressing these research questions, we aim to provide insights into the dynamics
    and consequences of GitHub Sponsors mentions on Twitter/X. This exploration contributes
    to theoretical understanding and practical strategies, offering value to the broader
    Open Source community.


    #### 3 RESEARCH METHODS


    This section describes our methods for data collection and our quantitative and
    qualitative analyses.


    #### 3.1 Data Collection


    In this study, we examine tweets containing links to GitHub Sponsors profiles.
    We successfully applied for Twitter/X''s Academic Research Access [\[45\]](#page-11-16),
    which offers a higher limit on the number of tweets that can be retrieved per
    month, and we analyzed tweets from May 2019, when GitHub Sponsors was launched,
    through April 2022, using Twitter/X API v2 [\[44\]](#page-11-17) in May 2022.
    The Twitter/X API provides a search function that allows for a set of query mechanisms
    against tweets. We use the "url" query to retrieve tweets that contain links with
    the specific substring "github.com/ sponsors/" so that we ensure all the tweets
    are developer-related. We obtained 11,582 tweets that contain GitHub Sponsors
    profile links. Among these tweets, the majority (91%) were written in English,
    accounting for 10,531 tweets. We only use English tweets for the following quantitative
    and qualitative analyses, except RQ1.1.


    #### <span id="page-1-0"></span>3.2 Quantitative Analysis


    To understand the characteristics of tweets mentioning GitHub Sponsors profiles
    (RQ1.1), we investigate written languages, types of GitHub accounts, and primary
    programming languages of developers mentioned in the tweets. For written languages,
    we calculate the distribution of languages in tweets. In cases where Twitter/X
    cannot determine the language of a tweet (e.g., the tweet only contains hashtags,
    emojis, or links), Undetermined is used.


    Since one GitHub Sponsors profile may appear in different tweets, we obtained
    distinct GitHub Sponsors profiles in tweets to collect the types of GitHub accounts
    and the primary programming languages of the corresponding developers. We obtained
    3,766 distinct GitHub Sponsors profiles from the 11,582 tweets. For the types
    of GitHub accounts, we calculate the distribution of the types of GitHub accounts
    (i.e., personal or organizational) across all distinct GitHub Sponsors profiles
    in tweets. Since the URL of a GitHub Sponsors profile is organized as https://github.com/sponsors/
    [username], we can retrieve the corresponding GitHub account using username in
    the GitHub GraphQL API [\[19\]](#page-11-18).


    The primary programming languages of the repositories can also be retrieved using
    the GitHub GraphQL API. Same to the previous work [\[35\]](#page-11-9), we take
    the most common primary language of the repositories to which each developer contributed
    as the primary language of that developer. This is an approximation because we
    did not analyze whether the developer actually committed in that language. If
    the occurrences of each programming language per repository are the same, we consider
    the primary programming language to be Undetermined. The primary languages of
    developers identified in this way can be interpreted as the programming languages
    of the ecosystems to which the developers mainly contributed.


    To attract potential sponsors, developers might be particularly active on GitHub
    around the time they advertise their GitHub Sponsors profile on Twitter/X. To
    investigate whether such correlations exist, we considered three time periods
    related to a "My GitHub Sponsors profile is live!" tweet, i.e., a week before
    posting this tweet, the week in which the tweet was posted, and a week after posting
    this tweet (RQ1.4). For example, if a tweet has been posted on 15 June 2020, these
    three periods will be from 2020-06-05 to 2020-06-11, 2020-06-12 to 2020-06-18,
    and 2020-06-19 to 2020-06-25, respectively. Following the approach of related
    work [\[10\]](#page-11-19), which used a time frame of one week before and after,
    our decision to adopt a one-week duration allows us to quickly assess immediate
    changes in productivity, engagement, and quality. This analysis involves scrutinizing
    short-term developer activities before and after sponsorship, facilitating a timely
    evaluation. We obtained 810 distinct GitHub Sponsors profiles that were posted
    using that template from our data set. Then we investigate different categories
    of contribution activities in each period, as shown below. To collect these contribution
    activities in a week, we retrieve them from the profile pages of the GitHub accounts
    as https://github.com/[username] ?tab=overview& from=[time period]&to=[time period].


    - opening pull request: The profile page indicates that the GitHub account has
    opened pull requests, including substrings such as "Created a pull request", "Opened
    1 other pull request", "Opened [number] pull requests", and "Opened their first
    pull request".

    - submitting pull request review: The profile page indicates that the GitHub account
    has reviewed pull requests, including a substring such as "Reviewed [number] pull
    requests".

    - opening issue: The profile page indicates that the GitHub account has opened
    issues, including substrings such as "Created an issue", "Opened [number] other
    issues", "Opened their first issue", and "Opened [number] issues".

    - opening discussion: The profile page indicates that the GitHub account started
    a GitHub Discussion, including a substring such as "Started [number] discussions".

    - answering discussion: The profile page indicates that the GitHub account answered
    a GitHub Discussion, including a substring such as "Answered [number] discussions".

    - committing: The profile page indicates that the GitHub account has authored
    commits, including a substring such as "Created [number] commits".

    - contributing in private repository: The profile page indicates that the GitHub
    account contributed to private repositories, including a substring such as "[number]
    contributions in private repositories".

    - creating repository: The profile page indicates that the GitHub account created
    private repositories, including substrings such as "Created [number] other repositories",
    "Created [number] repositories", and "Created their first repository".

    - joining organization: The profile page indicates that the GitHub account joined
    an organization, including a substring such as "Joined the [name] organization".


    We conduct Mann-Whitney U tests to compare activities in these three periods,
    i.e., between a week before posting the tweet and the week when the tweet was
    posted, and between the week when the tweet was posted and a week after posting
    this tweet. To estimate the effect size of significant differences, we use Cliff''s
    delta with the following thresholds [\[34\]](#page-11-20): negligible for 0 ≤
    |delta| < 0.147, small for 0.147 ≤ |delta| < 0.33, medium for 0.33 ≤ |delta| <
    0.474, and large otherwise.


    To investigate the reception of tweets mentioning GitHub Sponsors profiles (RQ2.1),
    we analyze the popularity of tweets that mentioned GitHub Sponsors profiles on
    Twitter/X (number of likes, number of retweets, and number of replies). Then,
    we compare these interactions to tweets that contain links to other donation and
    crowd-funding platforms that are often used to obtain financial support for OSS
    development [\[28\]](#page-11-1): PayPal, Open Collective, and Patreon. To ensure
    that the tweets obtained are related to OSS, we collect tweets that contain links
    to at least one of these three platforms and GitHub (i.e., "github.com", except
    links to GitHub Sponsors) using Twitter/X API v2 in the same time period for which
    we collected GitHub Sponsors profile tweets. We consider a link to point to a
    PayPal profile when it contains "paypal.com/paypalme/", Open Collective when it
    contains "opencollective.com/", and Patreon when it contains "patreon.com/", except
    Patreon posts (i.e., "patreon.com/posts/"). We exclude tweets that contain links
    to "github.com/sponsors/" and at least one of these three platforms from the 10,531
    English tweets obtained. In the end, we obtained 10,440 tweets for GitHub Sponsors,
    four tweets for PayPal, 88 tweets for Open Collective, and 228 tweets for Patreon.
    Since only four tweets contain links to PayPal, we focus on comparisons between
    GitHub Sponsors, Open Collective, and Patreon.


    We also conduct Mann-Whitney U tests to compare Twitter/X interactions between
    GitHub Sponsors and Open Collective, and ICSE ''24, April 14–20, 2024, Lisbon,
    Portugal Youmei Fan, Tao Xiao, Hideaki Hata, Christoph Treude, and Kenichi Matsumoto


    between GitHub Sponsors and Patreon. We use Cliff''s delta with the same thresholds
    to estimate the effect size of significant differences.


    #### <span id="page-3-0"></span>3.3 Qualitative Analysis


    For our qualitative analyses, we randomly selected a statistically representative
    number of tweets with a confidence level of 95% and a confidence interval of 5
    to obtain 371 tweets from the initial population of 10,531 English-language tweets.


    Unsurprisingly, our initial analysis revealed differences between the dynamics
    around tweets from users looking for sponsorship and those from users who made
    donations. Therefore, we categorized the 371 tweets into three different behavioral
    groups based on the purpose of the tweet, so we could see what kind of tweets
    a user would make based on their behavior:


    - looking for sponsors: This tweet is posted by developers who publicized their
    profiles to look for sponsors, e.g.,"My GitHub Sponsors profile is live! You can
    sponsor me to support my open source work ".

    - sponsors: This tweet is posted by developers who sponsored others, e.g., " I''m
    sponsoring [username] because...".

    - no purpose: This tweet does not have sufficient information to decide, e.g.,
    "You guys make magic.".


    In the end, we identified 183 tweets from developers that were looking for sponsors,
    168 tweets from sponsors, and 20 tweets from no purpose. These 351 tweets (20
    tweets from no purpose are excluded) are used for answering RQ1.2–RQ1.4, and RQ2.2.


    Four of the authors collaboratively took an initial look at a randomly selected
    subset of 30 tweets from the sample of 351 tweets, discussed which themes were
    present in the data and how these themes related to the research questions, and
    then formalized this discussion into coding schemata. For each aspect that entailed
    manual coding, a total of 30 tweets were independently labeled by four annotators,
    resulting in Cohen''s kappa exceeding 0.6 for all parts and even reaching 0.94
    for RQ2.2.


    Encouraged by the initial kappa agreements, the first two authors independently
    coded the remaining sample of 321 tweets. Then, they recalculated kappa agreements
    to assess the improvement in understanding of the coding schemata after labeling
    the first 30 tweets. Finally, four authors engaged in collaborative discussions
    to attain a consensus in cases of disagreement. We attribute this stability to
    the fact that we had an initial discussion about all data, that tweets are relatively
    short, and that this particular team of authors has experience working together
    on qualitative data analysis from previous research projects. We describe the
    coding schemata related to each research question in the following paragraphs.


    To investigate who mentions GitHub Sponsors profiles on Twitter/X (RQ1.2), we
    analyze the relationship between the authors of the tweets and the GitHub accounts
    that are linked in the tweets. Furthermore, the rationale behind having a "user"
    category in the aforementioned code is rooted in the goal of acquiring insights
    into what extent users benefit from a developer''s project and are willing to
    voluntarily advertise the developer, thereby enabling the developer to obtain
    more sponsorship. The prevalence of the code "non-specific" in the results indicates
    that some users advertise for others without a specific purpose. Since the names
    of accounts on Twitter/X and GitHub do not necessarily have to be the same, we


    employed qualitative analysis for this investigation. It is important to highlight
    that our decision not to employ automated techniques for verifying the association
    between Twitter/X and GitHub accounts was motivated by the realization that these
    techniques often fail to account for certain scenarios. For example, when a GitHub
    account is classified as an organization type and one of its members posts a tweet,
    it should be regarded as emanating from the same user. Consequently, we opt for
    a cautious approach, acknowledging the limitations of automated techniques and
    acknowledging the need for context-sensitive judgment in determining the correspondence
    between Twitter/X and GitHub accounts. The four annotators independently labeled
    30 tweets. Then, we calculate the kappa agreement of our coding schemata from
    four annotators. The initial Cohen''s kappa for this qualitative analysis is 0.75,
    which indicates ''substantial'' agreement [\[46\]](#page-11-21). For the remaining
    sample of 321 tweets, Cohen''s kappa is 0.78, which also indicates ''substantial''
    agreement, from the first two authors. Examples of the following codes are covered
    in our replication package, aiming to facilitate the reader''s comprehension of
    this taxonomy.


    - same: The author of this tweet is the same as the GitHub account that is shown
    on the GitHub Sponsors profile linked in the tweet, or the content of the tweet
    implies that they are the same developer or the author belongs to the GitHub organizational
    account on that GitHub Sponsors profile.

    - user: The tweet explicitly indicates that the author of this tweet is a user
    of an open-source project that belongs to the GitHub account on the GitHub Sponsors
    profile.

    - non-specific: There is not sufficient information to determine the relationship
    between the tweet author and the GitHub account.


    To understand the context of tweets mentioning GitHub Sponsors profiles (RQ1.3),
    we analyze why GitHub Sponsors profiles were mentioned in tweets. Additionally,
    the reason to distinguish between the "advertisement with new information" and
    "advertisement with new functionality" categories in the aforementioned coding
    schemata is to enable a more nuanced analysis: the former encompasses a range
    of updates, including changes to users'' profile descriptions and tier information
    whereas the latter is related to functionality in the projects they are dedicated
    to. The four annotators independently coded 30 tweets, achieving the initial Cohen''s
    kappa of 0.66 or ''substantial'' agreement [\[46\]](#page-11-21). The first two
    authors then independently labeled the remaining sample of 321 tweets, finally
    reaching Cohen''s kappa of 0.84 or ''almost perfect'' agreement. The following
    list shows the coding schema that emerged from the data. Examples of the following
    codes are described in detail in our replication package to help the reader understand
    this taxonomy.


    - generic advertisement: This tweet advertises the tweet author''s own GitHub
    Sponsors profile (use this code if the tweet does not fit the other advertisement
    categories).

    - donation appreciation: This tweet explicitly expresses appreciation of a donation.

    - sponsor template: This tweet contains GitHub''s template for advertising one''s
    own GitHub Sponsors profile: "My GitHub Sponsors profile is live! You can sponsor
    me to support my open source work " with no or minor changes.

    - advertisement of developer: This tweet advertises the GitHub Sponsors profile
    of another personal GitHub account.

    - advertisement with new functionality: This tweet explicitly advertises the author''s
    own GitHub Sponsors profile while mentioning new functionality of an open-source
    project.

    - advertisement with new information: This tweet explicitly advertises the author''s
    own GitHub Sponsors profile with an update.

    - sustainability: This tweet explicitly indicates an appreciation or need for
    a donation for the sustainability of an open-source project, often associated
    with terms such as "sustainable".

    - advertisement with early access: This tweet explicitly advertises the author''s
    own GitHub Sponsors profile with early access to features (usually accompanied
    by a phrase such as "early access" and "insider")

    - income: This tweet explicitly indicates the need for income to support one''s
    daily life.

    - advertisement with event: This tweet explicitly advertises a GitHub Sponsors
    profile with an event.

    - set example / peer pressure: This tweet explicitly motivates others in either
    a positive or negative way.

    - advertisement of organization: This tweet advertises the GitHub Sponsors profile
    of another organizational GitHub account.

    - donation to developer announcement: This tweet explicitly indicates that the
    author of this tweet donated to the personal GitHub account in the GitHub Sponsors
    profile.

    - donation to organization announcement: This tweet explicitly indicates that
    the author of this tweet donated to the organizational GitHub account in the GitHub
    Sponsors profile.

    - donation to developer template: This tweet contains GitHub''s template that
    indicates donation to a personal GitHub account: " I''m sponsoring [username]
    because..." with no or minor changes.

    - donation to organization template: This tweet contains GitHub''s template that
    indicates donation to an organizational GitHub account: " I''m sponsoring [username]
    because..." with no or minor changes.


    To study when tweets related to GitHub Sponsors profiles occur in relation to
    other activities on GitHub (RQ1.4), we analyze the timing of such tweets. The
    four annotators independently coded 30 tweets, achieving the initial Cohen''s
    kappa of 0.62 or ''substantial'' agreement [\[46\]](#page-11-21). For the remaining
    321 tweets, the first two authors reached Cohen''s kappa of 0.85 or ''almost perfect''
    agreement. The following list shows the coding schemata that emerged from the
    data.


    - start: This tweet was posted when the GitHub Sponsors profile is activated (usually
    accompanied by a phrase like "profile is live").

    - no specific timing: This tweet was posted with no particular timing.

    - donation: This tweet was posted when the author of the tweet received a donation.

    - update: This tweet was posted when there was an update to a GitHub project or
    GitHub Sponsors profile.

    - reach goal: This tweet was posted in relation to reaching a goal.

    - release: This tweet explicitly indicates that a release of the software project
    has been delivered.

    - event: This tweet was posted when an event has been announced.

    - resignation / paycut: This tweet was posted during a change in the author''s
    work professional situation.

    - benefit: This tweet explicitly mentions a particular benefit.

    - activity spike: This tweet was posted to indicate the GitHub developer was particularly
    active and explicitly mentions the activity spike.


    To investigate the responses to tweets mentioning GitHub Sponsors profiles (RQ2.2),
    we analyze the replies to tweets mentioning GitHub Sponsors profiles. Four annotators
    independently annotated 30 tweets. The initial kappa agreement is 0.94, interpreted
    as ''almost perfect'' agreement [\[46\]](#page-11-21). The first two authors independently
    annotated the remaining 321 tweets, reaching the same kappa agreement. Our coding
    schemata emerged from the data and is as follows. Note that examples for these
    codes are described in detail in our replication package to help the reader understand
    this taxonomy.


    - support: The response to this tweet demonstrates endorsement or encouragement
    for the author, often extending beyond appreciation and indicating a willingness
    to assist or advocate for the author''s cause.

    - appreciation of work: The respondent acknowledges and values the author''s open-source
    contributions and their impact, without necessarily conveying explicit support
    or a commitment to assist in further efforts.

    - appreciation of donation: The respondent to this tweet appreciates the donation.

    - emoji only: The response to this tweet only contains emoji.

    - other: The response to this tweet does not fit into the categories above, or
    there is no response to this tweet.


    #### 3.4 Causal Inference


    We conduct a quasi-experiment to estimate the causal impact of GitHub Sponsors
    profile mentions in tweets on the number of sponsors acquired (RQ2.3). Unlike
    prior studies that have conducted quasi-experiments for causal inference in software
    engineering by employing difference-in-differences [\[15,](#page-11-10) [26,](#page-11-22)
    [47\]](#page-11-23) or CausalImpact [\[25\]](#page-11-24), we are unable to employ
    these methods. This is because these methods require the values of the outcome
    variables in the periods before and after the treatment, but data on the number
    of sponsors at a given point in time were not available at the time we conducted
    our analysis.[1](#page-4-0) Therefore, in this analysis, we apply a statistical
    matching method called propensity score matching (PSM), which attempts to estimate
    the effect of treatment by constructing a control group by matching each treated
    unit with a non-treated unit with similar characteristics [\[22\]](#page-11-25).
    PSM predicts the probability of belonging to


    <span id="page-4-0"></span><sup>1</sup>We contacted the GitHub team in a public
    forum and they responded that they would consider making the sponsor count data
    publicly available; we do not provide a link to that form because of the double-anonymous
    submission.


    ICSE ''24, April 14–20, 2024, Lisbon, Portugal Youmei Fan, Tao Xiao, Hideaki Hata,
    Christoph Treude, and Kenichi Matsumoto


    the treatment and control groups based on observed predictors. Some of the studies
    mentioned above used PSM to prepare data for the treatment and control groups
    [\[15,](#page-11-10) [26\]](#page-11-22).


    To collect developers as potential members of a control group, we contacted the
    authors of previous work [\[35\]](#page-11-9) to obtain the list of GitHub users
    who had participated in GitHub Sponsors. From 3,697 sponsored and 5,666 non-sponsored
    developers collected in July 2021 for the previous study, we identified 1,930
    and 4,913 developers who had not deleted their GitHub accounts and whose GitHub
    Sponsors profiles do not appear in our tweet data (neither in their own tweets
    nor in tweets from others). Potential members of the treatment group are developers
    whose GitHub Sponsors profiles appear in the "sponsor template" tweets, that is,
    "My GitHub Sponsors profile is live!". By targeting only "sponsor templates",
    the influence of wording differences can be eliminated. We observed that "sponsor
    template" appears most often after "sponsor template", which are free-text tweets
    (see Section [4.1.3\)](#page-6-0). To limit developers to those who started using
    GitHub Sponsors at the same period as control group developers, we collected only
    those developers whose GitHub Sponsors profiles appeared in such tweets by July
    2021 and identified 568 developers.


    The following are variables of developers used in the logistic regression to estimate
    the propensity score for PSM.


    These variables have been used in previous related studies: for example, sponsored
    developers sponsor more than non-sponsored developers [\[35\]](#page-11-9), sponsored
    developers form language-specific clusters that sponsor each other [\[35\]](#page-11-9),
    and the number of followers is the most important feature for predicting long-term
    contributors [\[3\]](#page-11-26). All values were measured in August 2022.


    - repositories: The number of public repositories created.

    - sponsoring: The number of developers sponsoring.

    - openedPRs: The number of opened pull requests.

    - reviewedPRs: The number of reviewed pull requests.

    - followers: The number of followers.

    - organizations: The number of joined organizations.

    - language: Categorical variable for the primary programming language determined
    by the method described in Section [3.2.](#page-1-0) The values are the top 10
    languages (JavaScript, Python, PHP, C#, Go, Java, TypeScript, C++, Ruby, and C)
    and others (including undetermined) seen in Table [2.](#page-6-1) In regression
    model building, dummy variables are prepared that take a value of 0 or 1 indicating
    the absence or presence of a particular language.


    We obtained 1,094 matched developers out of 7,411 (1, 930 + 4, 913 + 568) developers
    from the PSM. Figure [1](#page-5-0) shows how the absolute mean differences have
    decreased as a result of the matching, from unadjusted to adjusted (unadjusted
    indicates all developers before matching, and adjusted indicates matched developers).
    None of the absolute mean differences of adjusted exceeds 0.10, which means that
    we obtained developer matches for the treatment and control groups with a balanced
    distribution of covariates [\[22\]](#page-11-25). This balance is a measure of
    the quality of the propensity score matching and we achieved the well-established
    and well-cited threshold [\[2,](#page-11-27) [27\]](#page-11-28).


    To estimate the impact of GitHub Sponsors profile mentions in tweets, a linear
    regression is performed using the above variables


    <span id="page-5-0"></span>![](_page_5_Figure_15.jpeg)


    Figure 1: Covariate balance before (unadjusted) and after (adjusted) propensity
    score matching.


    <span id="page-5-1"></span>Table 1: Frequency of written languages of tweets that
    contain links to GitHub Sponsors profiles.


    | written languages | Person       | Organization |

    |-------------------|--------------|--------------|

    | English           | 3,074 (94%)  | 479 (97%)    |

    | Japanese          | 151 (5%)     | 5 (1%)       |

    | Undetermined      | 18 (0%)      | 1 (0%)       |

    | Spanish           | 8 (0%)       | 1 (0%)       |

    | Other             | 19 (0%)      | 4 (0%)       |

    | sum               | 3,270 (100%) | 496 (100%)   |


    and the variable treatment, which takes a value of 0 or 1 that indicates the presence
    or absence of general template tweets. The outcome variable is the number of sponsors
    obtained by each developer, measured in August 2022. Therefore, this analysis
    estimates the impact of tweeting "My GitHub Sponsors profile is live!" on the
    number of sponsors as of August 2022, for early adopters starting GitHub Sponsors
    and tweeting from May 2019 (GitHub Sponsors launched) through July 2021.


    #### 4 RESULTS


    This section presents answers to our research questions.


    ## 4.1 RQ1: How are GitHub Sponsors profiles discussed on Twitter/X?


    The results of the analysis of the characteristics, participants, context, and
    timing of tweets mentioning GitHub Sponsors profiles are presented in this section.


    4.1.1 RQ1.1: What are the characteristics of tweets mentioning GitHub Sponsors
    profiles from organizational and personal accounts? We investigated the written
    languages, GitHub account types, and the primary programming languages of the
    developers mentioned in the tweets. These elements were categorized based on whether
    they originated from personal or organizational accounts. This initial analysis
    serves as a foundation for our subsequent in-depth investigation, offering an
    initial understanding of the nature of these tweets.


    Written languages. Table [1](#page-5-1) presents the frequency of written languages
    in tweets that contain links to GitHub Sponsors profiles. <span id="page-6-1"></span>Table
    2: Frequency of GitHub account types and primary programming languages of distinct
    GitHub Sponsors profiles.


    | programming languages | Person       | Organization |

    |-----------------------|--------------|--------------|

    | JavaScript            | 816 (25%)    | 89 (18%)     |

    | Python                | 333 (10%)    | 47 (9%)      |

    | PHP                   | 309 (9%)     | 44 (9%)      |

    | C#                    | 228 (7%)     | 22 (4%)      |

    | Go                    | 180 (6%)     | 21 (4%)      |

    | Java                  | 153 (5%)     | 21 (4%)      |

    | Other                 | 1251 (38%)   | 252 (51%)    |

    | sum                   | 3,270 (100%) | 496 (100%)   |


    <span id="page-6-3"></span>Table 3: Frequency of relationships between tweet authors
    and linked GitHub Sponsors profiles.


    |                      | looking for sponsors | sponsors              |

    |----------------------|----------------------|-----------------------|

    | same                 | 169 (48%)            | -                     |

    | user<br>non-specific | -<br>14 (4%)         | 55 (16%)<br>113 (32%) |

    | sum                  | 183 (52%)            | 168 (48%)             |


    <span id="page-6-4"></span>


    | Table 4: Frequency of context of the GitHub Sponsors profile |  |

    |--------------------------------------------------------------|--|

    | mentions on Twitter/X.                                       |  |


    Compared to the ranks and portions of the written languages of general tweets
    [\[21\]](#page-11-29), in English GitHub Sponsors profile tweets, both personal
    and organizational accounts make up a significantly larger portion than general
    English tweets (51%). Japanese tweets rank second in general tweets, comprising
    5% of personal accounts and 1% of organizational accounts. Spanish also stands
    out as a top contributor among the top ten languages used frequently in general
    tweets.


    GitHub account types. As seen in Table [1](#page-5-1) and Table [2,](#page-6-1)
    most GitHub Sponsors profiles mentioned in tweets are associated with personal
    accounts, accounting for 87% of 3,766 distinct GitHub Sponsors profiles. Approximately
    a fifth of GitHub Sponsors profiles in tweets are associated with organizational
    accounts, representing 13% of distinct GitHub Sponsors profiles in the obtained
    tweets. According to GitHub''s advanced search engine [\[18\]](#page-11-30) in
    August 2022, 18,129 personal GitHub accounts had activated GitHub Sponsors, accounting
    for 91%. Furthermore, only 9% of all GitHub accounts (1,889) that activated GitHub
    Sponsors are organizational accounts. Comparing GitHub Sponsors profiles that
    were posted on Twitter/X and all GitHub Sponsors profiles on GitHub, they tend
    to share a similar trend for GitHub account types.


    Programming languages. Among the 3,766 distinct GitHub Sponsors profiles mentioned
    in the collected tweets, JavaScript stands out as the most prominent language,
    with 25% of personal accounts, suggesting its popularity among individual users.
    Conversely, its relatively lower representation in organizational accounts (18%)
    may indicate a preference for other languages in professional settings. Python,
    with 10% of usage among personal accounts, appears to be a language of choice
    for individual developers, potentially due to its versatility and readability.
    The prevalence of Python and PHP, both of them at 9%, among organizational accounts
    hints at their significance in enterprise-level development projects, as seen
    in Table [2.](#page-6-1) In the "other" category of coding repositories, where
    many instances are labeled as "None", there are organizations like PJSoftCo.[2](#page-6-2)
    They are a prime example of how GitHub organizations are using sponsorship funds
    to invest in their organization-wide documentation. Comparing these results with
    previous work [\[35\]](#page-11-9), we find that, except for Undetermined, the
    top four programming languages are exactly the same. The top ten primary programming
    languages are the same on individual GitHub Sponsors and GitHub Sponsors profiles
    that were posted on Twitter/X.


    |                                       | looking for sponsors | sponsors  |

    |---------------------------------------|----------------------|-----------|

    | generic advertisement                 | 72 (21%)             | -         |

    | donation appreciation                 | 34 (10%)             | -         |

    | sponsor template                      | 33 (9%)              | -         |

    | advertisement of developer            | 9 (3%)               | 7 (2%)    |

    | advertisement with new functionality  | 9 (3%)               | -         |

    | advertisement with new information    | 8 (2%)               | -         |

    | sustainability                        | 5 (1%)               | -         |

    | advertisement with early access       | 5 (1%)               | -         |

    | income                                | 2 (1%)               | -         |

    | advertisement with event              | 3 (1%)               | -         |

    | set example / peer pressure           | 2 (1%)               | 4 (1%)    |

    | advertisement of organization         | 1 (0%)               | 2 (1%)    |

    | donation to developer announcement    | -                    | 101 (29%) |

    | donation to organization announcement | -                    | 33 (9%)   |

    | donation to developer template        | -                    | 16 (5%)   |

    | donation to organization template     | -                    | 5 (1%)    |


    sum 183 (52%) 168 (48%)


    4.1.2 RQ1.2: Who mentions GitHub Sponsors profiles on Twitter/X? Table [3](#page-6-3)
    shows the results of the coding for RQ1.2. As mentioned in Section [3.3,](#page-3-0)
    we separated tweets by purpose, distinguishing developers who mention their GitHub
    Sponsors profiles to look for sponsors from those who are sponsors. Developers
    that were looking for sponsors mentioning their own GitHub Sponsors profiles in
    tweets is the most frequently occurring case, accounting for 48% of the sample.
    However, it is also common that sponsors mention GitHub Sponsors profiles of other
    GitHub accounts, accounting for 32% of the sample. We observe that sponsors also
    explicitly mentioned GitHub Sponsors profiles of others due to dependencies or
    other benefits, accounting for 16% of the sample. In previous work [\[35\]](#page-11-9),
    Shimada et al. showed that developers sponsoring others via GitHub Sponsors due
    to dependencies is the most frequent reason for sponsoring. In the context of
    Twitter/X, our result partially agrees with their observations.


    <span id="page-6-0"></span>4.1.3 RQ1.3: What is the context of GitHub Sponsors
    profile mentions on Twitter/X? In Table [4,](#page-6-4) the frequency of various
    GitHub Sponsors profile mentions on Twitter/X is presented. Most sponsors mentioned
    GitHub Sponsors profiles on Twitter/X in the context of donating to personal or
    organizational GitHub accounts, accounting for 29% and 9%, respectively. Sponsors
    also mentioned their donation to personal or organizational GitHub accounts using
    GitHub''s tweet templates, accounting for 5% and 1%, respectively.


    In addition to donations, developers looking for sponsors mentioned GitHub Sponsors
    profiles on Twitter/X to advertise their own profile (21%) or to advertise profiles
    of other personal GitHub accounts (3%). Specifically, developers looking for sponsors
    advertise


    <span id="page-6-2"></span><sup>2</sup>https://github.com/PJSoftCo


    <span id="page-7-0"></span>Table 5: Frequency of timing of tweets that contain
    links to GitHub Sponsors profiles.


    |                      | looking for sponsors | sponsors  |

    |----------------------|----------------------|-----------|

    | start                | 62 (18%)             | -         |

    | no specific timing   | 46 (13%)             | 111 (32%) |

    | donation             | 35 (10%)             | -         |

    | update               | 20 (6%)              | -         |

    | reach goal           | 8 (2%)               | -         |

    | release              | 6 (2%)               | -         |

    | event                | 3 (1%)               | -         |

    | resignation / paycut | 3 (1%)               | -         |

    | benefit              | -                    | 55 (16%)  |

    | activity spike       | -                    | 2 (1%)    |

    | sum                  | 182 (52%)            | 169 (48%) |


    their own profile with updates on the functionality of the project (3%), updates
    on the profile (2%), early access features (1%), and events (1%). As with the
    donation, developers looking for sponsors advertised their own profiles using
    GitHub''s tweet templates, accounting for 9%. Furthermore, a few developers looking
    for sponsors also posted tweets to encourage others to donate by setting an example
    or applying peer pressure, accounting for 1%.


    Some developers looking for sponsors use Twitter/X as a channel to express appreciation
    to sponsors (10%). Furthermore, a few tweets from developers looking for sponsors
    mention GitHub Sponsors profiles in the context of sustainability of the project
    or the financial income of developers. In particular, we see that several tweets
    are posted to share GitHub Sponsors updates in the context of the Log4j vulnerability
    [\[12\]](#page-11-31) that was exploited in December 2021. For example, "It''s
    nice to see that a month after the Log4Shell vulnerability Log4j''s maintainer
    has 101 GitHub Sponsors instead of 3, including corporate accounts such as Amazon
    Web Services".


    <span id="page-7-2"></span>4.1.4 RQ1.4: When are GitHub Sponsors profiles mentioned
    on Twitter/X? Table [5](#page-7-0) presents the frequency of different types of
    timing when different types of developers mentioned GitHub Sponsors profiles on
    Twitter/X. Regardless of the different types of developers, we find that most
    of the tweets (45%) do not specify an explicit explanation of the reason for the
    tweet''s posting at that particular time. However, we can see that some tweets
    (16%) were posted at a time when sponsors benefited from a project. Developers
    that were looking for sponsors posted those tweets during the initiation of GitHub
    Sponsors profiles (18%), at the time of donation (10%), or when updating projects
    or profiles (6%). Furthermore, some developers that were looking for sponsors
    posted tweets with GitHub Sponsors profile mentions when they need financial resources
    due to changes in their work arrangements.


    As seen in Table [5,](#page-7-0) we found an interesting type of timing with regard
    to when GitHub Sponsors profiles were mentioned in tweets: activity spikes, i.e.,
    a sponsor donated due to an activity spike of a developer. Inspired by this code,
    we conducted a quantitative study to analyze the correlations between contributions
    of developers and GitHub Sponsors profile mentions on Twitter/X. Table [6](#page-7-1)
    presents comparisons among three periods across a set of GitHub contribution types.
    Since the GitHub organization account lacks information on activity, we excluded
    tweets that contain GitHub Sponsors from organizations. Then, we focus on tweets
    from distinct developers that tweeted with "My GitHub Sponsors profile


    ICSE ''24, April 14–20, 2024, Lisbon, Portugal Youmei Fan, Tao Xiao, Hideaki Hata,
    Christoph Treude, and Kenichi Matsumoto


    is live!". Only 810 tweets were included out of the 10,531 English tweets. The
    rationale behind this choice was to specifically analyze the initial reactions
    and sentiments expressed by users who had just enabled their GitHub Sponsors account.
    Our primary goal was to capture the immediate activity of individuals in this
    specific context. Our sample is representative of GitHub Sponsors users'' initial
    tweets about their GitHub Sponsors account, but not of all tweets in our dataset.
    We observe that most of the mean values for the week in which a tweet was posted
    are higher than the corresponding values in the week before or after. For the
    contribution activities of Opening discussion, Committing, and Creating repository,
    there are significant differences between a week before posting this tweet and
    the week when the tweet was posted, and between the week when the tweet was posted
    and a week after posting this tweet, with at least negligible effect sizes (Committing
    shows small effect size). In addition to these activities, comparing a week before
    posting the tweet and the week when the tweet was posted, we find that developers
    proposed significantly more pull requests, with negligible effect size. These
    results indicate that when developers posted their GitHub Sponsors profile on
    Twitter/X, they generally contribute more actively to OSS projects.


    Summary: Of the GitHub Sponsors profiles mentioned in the tweets, 87% belong to
    individual developers, whose top primary languages were JavaScript, Python, and
    PHP. Such tweets were posted by the owners of the profiles or by others who depended
    on the work of the developer they sponsored. Developers looking for sponsors were
    more active on GitHub during the week in which tweets linking to their GitHub
    Sponsors profile were posted.


    <span id="page-7-1"></span>Table 6: Comparisons among three periods of GitHub
    contributions


    |                            | Before |   |                                                             |
    During |   |   | After |           |                     |

    |----------------------------|--------|---|-------------------------------------------------------------|--------|---|---|-------|-----------|---------------------|

    |                            |        |   | mean Q3 effect size mean Q3 effect
    size mean Q3 effect size |        |   |   |       |           |                     |

    | Opening PR                 | 1.12   |   | 1 0.0929*** 1.75 2                                          |        |   |
    - | 1.5   | 1         | -                   |

    | Subm. PR review            | 1.2    | 0 | -                                                           |
    1.28   | 0 | - | 1.19  | 0         | -                   |

    | Opening issue              | 0.61   | 0 | -                                                           |
    1.01   | 1 | - | 0.84  | 1         | -                   |

    | Opening disc.              | 0.03   |   | 0 0.0419*** 0.12                                            |        |
    0 | - | 0.06  | 0         | 0.0274**            |

    | Answering disc.            | 0.02   | 0 | -                                                           |
    0.03   | 0 | - | 0.04  | 0         | -                   |

    | Committing                 |        |   | 13.49 15 0.181*** 18.31 21                                  |        |   |
    - |       |           | 14.4 16.75 0.153*** |

    | Contr. to priv. repo. 7.69 |        | 6 | -                                                           |
    8.39 7 |   | - |       | 7.42 5.75 | -                   |

    | Creating repo.             | 0.52   |   | 0 0.0938*** 0.71 1                                          |        |   |
    - | 0.5   | 0         | 0.103***            |

    | Joining org.               | 0.01   | 0 | -                                                           |
    0.01   | 0 | - | 0.01  | 0         | -                   |


    \* p-value < 0.05; \*\* p-value < 0.01; and \*\*\* p-value < 0.001. The Cliff''s
    delta effect size with thresholds [\[34\]](#page-11-20) are highlighted in Negligible
    Small Medium Large. The hyphen (-) is used as a placeholder for cases where p-value
    ≥ 0.05 indicates there is no significant difference in the comparison or when
    comparing to itself.


    # 4.2 RQ2: What is the impact of GitHub Sponsors profile mentions on Twitter/X?


    4.2.1 RQ2.1: How are GitHub Sponsors profile mentions received on Twitter/X? Table
    [8](#page-9-0) presents comparisons between donation and crowd-funding platforms
    in Twitter interactions. For median values, Patreon tweets received the highest
    number of likes,


    Table 7: Frequency of response to tweets.


    |                          | looking for sponsors | sponsors  |

    |--------------------------|----------------------|-----------|

    | other                    | 165 (47%)            | 138 (39%) |

    | appreciation of work     | 9 (3%)               | -         |

    | support                  | 7 (2%)               | 3 (1%)    |

    | appreciation of donation | 2 (1%)               | 25 (7%)   |

    | emoji only               | -                    | 2 (1%)    |

    | sum                      | 183 (52%)            | 168 (48%) |


    accounting for six. Additionally, Patreon tweets were retweeted twice, which is
    the highest number of retweets in terms of median values. The median values of
    replies for the three platforms are zero. In terms of likes, the p-value of Patreon
    vs. GitHub Sponsors is less than 0.05 and Cliff''s delta is 0.265, indicating
    Patreon and GitHub Sponsors have a significant difference with a small effect
    size. In terms of retweets, Open Collective and GitHub Sponsors have a significant
    difference (i.e., p-value < 0.05) with a small effect size (i.e., 0.147 ≤ |delta|
    < 0.33). Comparing Patreon and GitHub Sponsors in terms of retweets, there is
    a significant difference with a small effect size. In terms of replies, we find
    that Patreon and GitHub Sponsors have a significant difference with a negligible
    effect size (i.e., Cliff''s delta is 0.132). In conclusion, while the number of
    tweets containing GitHub Sponsors profile mentions is much larger, tweets that
    link to Patreon or Open Collective in the context of OSS receive more likes and
    retweets.


    4.2.2 RQ2.2: How are GitHub Sponsors profile mentions discussed on Twitter/X?
    Table [9](#page-9-1) shows the results of our coding of replies to tweets that
    mentioned GitHub Sponsors profiles. We can see that most tweets (86%) do not receive
    a response on Twitter/X. For the remaining 14%, the majority consists of expressions
    of appreciation for donations (8%). Since Twitter/X is an informal communication
    channel, we observe that some responses consist only of one or more emoji.


    <span id="page-8-0"></span>4.2.3 RQ2.3: How do GitHub Sponsors profile mentions
    impact sponsorship? Table [10](#page-9-2) summarizes the regression result. As
    seen in the coefficient estimate of treatment, there is a statistically significant
    positive effect of GitHub Sponsors profile mentions in tweets on the number of
    sponsors acquired. As the average of the expected causal effect of treatment on
    individuals in the treatment group, called Average Treatment Effects on the Treated
    (ATT), we find that tweet mentions have an impact of increasing the number of
    sponsors by 1.22. However, note that the medians, Q3, and means for the matched
    treatment and control groups are {0, 2.00, 2.56} and {0, 1.00, 1.30}, respectively,
    indicating a skewness in the developers who obtained sponsorship, that is, the
    effects are not uniform.


    Summary: GitHub Sponsors profile mentions have a positive impact on the number
    of sponsors acquired, increasing the number of sponsors by 1.22. On Twitter/X,
    tweets mentioning GitHub Sponsors receive fewer interactions than those mentioning
    Patreon or Open Collective, and most tweets do not attract replies.


    #### 5 THREATS TO VALIDITY


    Subjective nature of coding. We conducted qualitative analyses of a statistically
    representative sample of tweets. The codes we assigned to different tweets may
    be inadequate due to the subjective nature of understanding the various coding
    schemata. To migrate this threat, we require kappa agreements of at least "substantial
    agreement" to ensure a common understanding of the coding schemata among all four
    annotators. Then, we initiated another round of coding between the first two authors
    for the remaining sample. By recalculating kappa agreements, we can see the improvement
    in understanding the various coding schemata. For example, Cohen''s kappa increased
    from 0.62 for the first 30 tweets to 0.85 for the reaming 321 tweets in the coding
    of the timing of tweets. The final results are based on the codes on which the
    authors, after discussion, reached a consensus and collectively agreed.


    Limitations in causal inference result. We compared developers with and without
    tweets who started GitHub Sponsors in the same period and engaged in similar activities,
    but we may have missed important developer characteristics other than the metrics
    we measured. The result is best interpreted as an increase in sponsors acquired
    through social activities on Twitter/X, rather than simply tweeting "My GitHub
    Sponsors profile is live!". In this study, we only analyzed the impact of tweets
    using such a template, so the impact of free-text tweets is unknown. In addition,
    since this analysis was conducted on early adopters, it is not possible to generalize
    whether similar effects will be seen in the future, so a continued analysis is
    needed.


    Multiple GitHub Sponsors profiles in the same tweet. There is a small number of
    cases where the same tweet contains multiple GitHub Sponsors profile links to
    different GitHub accounts. Since these cases are rare (i.e., only five tweets)
    and to avoid confusion in our analyses, we exclude these tweets from our analyses.


    Only tweets with GitHub Sponsors profiles links. Simple keyword searches would
    have introduced too much noise to our large-scale analysis. To avoid false positives,
    we only recovered tweets with GitHub Sponsors profile links, but we acknowledge
    that other relevant tweets without links may have been omitted.


    Primary programming languages of the developers. We considered the most common
    primary language of the repositories to which each developer contributed as the
    primary language of that developer. This means it could happen for some users,
    for example, that the most common language of a developer''s contributed repositories
    is Java, but the developer may only contribute the documentation of these Java
    repositories, whereas committing Python code to another project. Therefore, it
    is important to acknowledge this potential limitation in accurately capturing
    a developer''s primary programming language through this methodology.


    The number of tweets mentioning GitHub Sponsors has a different scale of data
    compared with other sponsorship platforms. We collected 10,440 tweets for GitHub
    Sponsors compared to other platforms: 4 for PayPal, 88 for Open Collective, and
    228 for Patreon. It is important to recognize that this difference in data size
    may affect the robustness and generalizability of our conclusions.


    External validity is concerned with our ability to generalize based on our results.
    In Section [4.1.4,](#page-7-2) we used a subset of 810 tweets from a pool of 10,531
    English tweets. It is crucial to acknowledge that the


    <span id="page-9-0"></span>Table 8: Comparisons among donation and crowd-funding
    platforms in Twitter interactions.


    |                 | like   |             |        | retweet     | reply  |             |        |

    |-----------------|--------|-------------|--------|-------------|--------|-------------|--------|

    |                 | median | effect size | median | effect size | median | effect
    size | #      |

    | Open Collective | 4      | -           | 1      | 0.216***    | 0      | -           |
    88     |

    | Patreon         | 6      | 0.265***    | 2      | 0.278***    | 0      | 0.132***    |
    228    |

    | GitHub Sponsors | 3      | -           | 0      | -           | 0      | -           |
    10,440 |


    \* p-value < 0.05; \*\* p-value < 0.01; and \*\*\* p-value < 0.001. The Cliff''s
    delta effect size with thresholds [\[34\]](#page-11-20) are highlighted in Negligible
    Small Medium Large. The hyphen (-) is used as a placeholder for cases where p-value
    ≥ 0.05 indicates there is no significant difference in the comparison or when
    comparing to itself (GitHub Sponsors).


    <span id="page-9-1"></span>Table 9: Frequency of response to tweets.


    | looking for sponsors | sponsors                                           |

    |----------------------|----------------------------------------------------|

    |                      | 130 (37%)                                          |

    |                      | 9 (3%)                                             |

    |                      | 2 (1%)                                             |

    |                      | -                                                  |

    |                      | 25 (7%)                                            |

    | -                    | 2 (1%)                                             |

    | 182 (52%)            | 169 (48%)                                          |

    |                      | 140 (40%)<br>27 (8%)<br>6 (2%)<br>6 (2%)<br>2 (1%) |


    <span id="page-9-2"></span>Table 10: Causal inference impact of GitHub Sponsors
    Profile Mentions in "My GitHub Sponsors Profile is Live!" Tweets.


    |                     | estimate  | std. error | p        |

    |---------------------|-----------|------------|----------|

    | treatment           | 1.22      | 0.452      | 0.00681  |

    | repositories        | -0.000818 | 0.00209    | 0.696    |

    | sponsoring          | 1.12      | 0.194      | 1.01e-8  |

    | openedPRs           | 0.000432  | 0.000457   | 0.345    |

    | reviewedPRs         | 0.00301   | 0.00140    | 0.0325   |

    | followers           | 0.00271   | 0.000345   | 8.42e-15 |

    | organizations       | -0.0637   | 0.0870     | 0.465    |

    | language_JavaScript | -1.45     | 0.657      | 0.0279   |

    | language_Python     | -1.18     | 0.856      | 0.168    |

    | language_PHP        | -0.343    | 0.906      | 0.705    |

    | language_C#         | 0.559     | 0.973      | 0.566    |

    | language_Go         | -0.933    | 1.02       | 0.360    |

    | language_Java       | -1.66     | 1.15       | 0.149    |

    | language_TypeScript | -0.692    | 1.04       | 0.505    |

    | language_C++        | -1.07     | 1.44       | 0.458    |

    | language_Ruby       | -1.01     | 1.50       | 0.500    |

    | language_C          | -0.659    | 1.21       | 0.586    |


    chosen subset may not fully represent the broader spectrum of reactions across
    all types of tweets related to GitHub Sponsors. Users who express their thoughts
    in different formats or use alternative phrases may not be fully captured in our
    analysis.


    #### 6 DISCUSSION


    This section presents implications and future work from this study.


    Implications. We categorized the practical implications for diverse groups of
    individuals by offering tailored guidance and recommendations that align with
    the specific concerns and interests of each stakeholder group.


    • Developers seeking sponsorship: our study shows that mentioning GitHub Sponsors
    profiles in tweets has a positive impact on the number of sponsors acquired. The
    finding that the number of sponsors acquired increased depending on whether they
    tweeted, is evidence of the importance of social


    media and should encourage developers to go beyond the GitHub platform in order
    to attract sponsorship. Additionally, our research reveals many different types
    of messages surrounding GitHub Sponsors in various contexts, providing insights
    that might assist others in crafting their own effective social media strategies
    for sponsorship engagement.


    - Developers interested in sponsoring: Within our sample, approximately half of
    the participants are sponsors. This finding underscores the importance of encouraging
    users who depend on OSS projects to actively promote the developers they rely
    on, even if the sponsorship amount is not substantial. Engaging in social media
    promotion can significantly enhance the visibility of these developers, allowing
    their exceptional work to reach a wider audience and garner increased recognition.

    - Companies: The relationship between companies and OSS projects is undergoing
    a pivotal change, largely driven by an expanding recognition of sustainability
    issues inherent in OSS. Instead of merely expressing dissatisfaction with the
    lack of sustainability in these projects, our study offers evidence that a two-pronged
    approach of corporate sponsorship and active social media engagement could be
    an effective strategy for businesses. This strategy allows them to constructively
    engage with OSS projects they rely upon, particularly those struggling with sustainability,
    thereby addressing their concerns and contributing to potential solutions.


    Future Work. As our study is positioned as an early adopter study, we have not
    yet obtained conclusive evidence of a significant impact of financial support
    on OSS sustainability at this stage. Therefore, further investigation of the potential
    impact of financial support on sustainability is needed.


    We have focused on Twitter/X as the starting point for our exploration. For future
    research, there is significant value in extending our analysis to encompass posts
    from multiple social media platforms (e.g., Facebook, Reddit) to gain a more comprehensive
    understanding of these dynamics.


    As part of our investigation into RQ2.1, we found that tweets linking exclusively
    to GitHub Sponsors were more common, and among those platforms, GitHub is the
    only one that provides Twitter/X templates for developers looking for sponsors
    and Twitter/X templates for sponsors. However, GitHub Sponsors received fewer
    responses compared to tweets promoting alternative sponsorship platforms. Since
    GitHub Sponsors launched 4–6 years later than Open Collective and Patreon, so
    it had less time to solidify its position and gain widespread awareness. Further
    research is needed


    to determine the importance of the template if such a sponsorship platform provides
    a template when users are trying to advertise on social media, and strategies
    for increasing the response and engagement of tweets containing only links to
    GitHub Sponsors.


    In light of our findings regarding the skewness in the developers who obtained
    sponsorship (Section [4.2.3\)](#page-8-0), factors other than tweets may play
    an important role in sponsor acquisition. Thus, further research is needed to
    explore and identify these additional factors that contribute to the sponsor acquisition
    process.


    In subsequent studies, it would be valuable to investigate the specific strategies
    and practices employed by organizations when leveraging social media platforms
    to disseminate project updates. By examining the relationship between these practices
    and the resulting engagement levels within the community, we can gain insights
    into the effectiveness of such approaches and their potential for enhancing community
    involvement. Moreover, it is worth considering a more in-depth investigation into
    how the domain and functionalities of open-source projects can impact and guide
    the dynamics of sponsorship, such as evaluations of a project''s sustainability
    and its sponsorship status [\[32\]](#page-11-32).


    #### 7 RELATED WORK


    In this section, we situate our work with respect to the literature on donations
    and one potential advertising channel, Social Media.


    Donation. OSS development heavily relies on volunteer contributions, as highlighted
    in a recent GitHub survey [\[17\]](#page-11-33), revealing that just 23% of respondents
    contribute to open source as part of their job description. Despite more employees
    being paid for contributing to OSS projects during work hours [\[33\]](#page-11-34),
    developers still perceive compensation asymmetry in OSS projects [\[1\]](#page-11-35).
    OSS projects that are distributed unequally may fail if they are mismanaged and
    financial benefits are a factor in the sustainability of OSS projects [\[1\]](#page-11-35).
    Donation is one of the common ways to obtain these financial benefits [\[13\]](#page-11-36)
    to support OSS projects, in addition to Bounty [\[13\]](#page-11-36). In a mixed-method
    empirical study, Overney et al. [\[28\]](#page-11-1) found that only a few projects
    (0.04–0.2%) ask for donations, primarily using platforms like PayPal and Patreon.
    These projects tend to be more active, more mature, and more popular.


    Recently, Zhou et al. [\[49\]](#page-11-2) explored donations on the Open Collective
    platform that support open-source projects. They indicated the influence of individual
    donors; although corporate donors tend to donate more money than individual donors
    for an individual donation, the total donation amount from individual donors is
    greater than corporate donors. However, corporate collectives are more likely
    to receive a larger total donation amount than individual collectives. Regarding
    the study on GitHub Sponsors, Shimada et al. [\[35\]](#page-11-9) revealed that
    developers typically do not have channels at their disposal to attract sponsors
    and communicate with those who might be interested in donating. Zhang et al. [\[48\]](#page-11-37)
    discovered that sponsorship through GitHub Sponsors has a short-term impact on
    developers'' activities. Their survey highlighted key challenges, including the
    difficulty of attracting sponsorship and the absence of corporate support.


    Social Media. Social media channels are one way of communicating and advertising
    in the world of developers. Different social media channels play different roles
    and have different impact on


    OSS projects, e.g., facilitating communication [\[5,](#page-11-38) [38\]](#page-11-11),
    awareness of the status of other developers [\[4,](#page-11-39) [6\]](#page-11-40),
    gaining attention [\[8,](#page-11-41) [23,](#page-11-42) [43\]](#page-11-43),
    and attracting new contributors [\[7,](#page-11-44) [23,](#page-11-42) [31\]](#page-11-45).


    Researchers studied the use of microblog services such as Twitter/X in software
    development [\[8\]](#page-11-41). The tweets of developers differ from those of
    the general public in terms of length, use of URLs, and @-mentions, and software
    microbloggers are more tightly knit than general microbloggers [\[9,](#page-11-46)
    [41\]](#page-11-47). Twitter/X is widely adopted in the software engineering community
    [\[5,](#page-11-38) [39\]](#page-11-12). Tian et al. [\[40\]](#page-11-48) found
    that knowledge sharing, technical discussion, and software product updates are
    the most frequent categories of developers'' tweets. Fang et al. [\[14\]](#page-11-49)
    proposed an approach to cross-link users on Twitter/X and GitHub; they observed
    that tweeting patterns appear in tweets from different developer roles when including
    GitHub links in their tweets (e.g., repository owners prompt their projects instead
    of discussing specific software artifacts).


    For the impact of Twitter/X, Singer et al. [\[36\]](#page-11-14) indicated that
    Twitter/X can help developers become aware of industry changes, learn, and build
    work relationships in communities. Mezouar et al. [\[24\]](#page-11-13) found
    that tweets from end users can lead to early discovery of bugs in web browsers.
    Fang et al. [\[15\]](#page-11-10) explored the causal effects of Twitter/X on
    the attraction of stars and contributors by analyzing tweets that contain links
    to GitHub repositories. They found that Twitter/X has a statistically significant
    and sizeable effect to help make projects popular (i.e., stars) but only a small
    effect to attract new contributors (i.e., commits). Moreover, these newly attracted
    contributors showed to be more active in OSS projects when they had prior Twitter/X
    interactions with the tweet authors.


    #### 8 CONCLUSION


    There are several platforms that enable contributions to opensource software developers,
    but attracting sponsors in order to ensure project sustainability remains a challenge.
    To understand the impact of Twitter/X on helping OSS developers attract sponsors,
    we conducted quantitative and qualitative analyses of more than 10,000 tweets
    containing links to GitHub Sponsors profiles. We find that such tweets have a
    significant positive effect on the acquisition of sponsors, and that developers
    contribute more OSS work than usual to attract potential sponsors during the week
    in which they posted tweets that link to their own GitHub Sponsors profile.


    Open-source developers who maintain an active presence on social media can attract
    donations that help sustain their projects. Our findings suggest that social media
    channels and donation channels are linked in the social programmer ecosystem and
    will continue to grow in importance for the sustainability of open source software.


    #### DATA AVAILABILITY


    The replication package includes scripts and data set, which is available at<https://doi.org/10.5281/zenodo.10461383>
    and [https://](https://github.com/NAIST-SE/GHSponsorsX) [github.com/NAIST-SE/GHSponsorsX](https://github.com/NAIST-SE/GHSponsorsX)


    #### ACKNOWLEDGEMENTS


    This work was supported by JSPS KAKENHI Grant Numbers JP20H00587 and JP20H05706,
    JSPS Grant-in-Aid for JSPS Fellows JP23KJ1589, and JST PRESTO Grant Number JPMJPR22P6.


    <span id="page-11-0"></span>ICSE ''24, April 14–20, 2024, Lisbon, Portugal Youmei
    Fan, Tao Xiao, Hideaki Hata, Christoph Treude, and Kenichi Matsumoto


    #### REFERENCES


    - <span id="page-11-35"></span>[1] Arzoo Atiq and Arvind Tripathi. 2016. Impact
    of financial benefits on open source software sustainability. In Proceedings of
    37th International Conference on Information Systems (ICIS ''16). 10.

    - <span id="page-11-27"></span>[2] Peter C. Austin. 2009. Balance diagnostics
    for comparing the distribution of baseline covariates between treatment groups
    in propensity-score matched samples. Statistics in Medicine 28, 25 (2009), 3083–3107.<https://doi.org/10.1002/sim.3697>
    arXiv[:https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.3697](https://arxiv.org/abs/https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.3697)

    - <span id="page-11-26"></span>[3] Lingfeng Bao, Xin Xia, David Lo, and Gail C.
    Murphy. 2021. A Large Scale Study of Long-Time Contributor Prediction for GitHub
    Projects. IEEE Transactions on Software Engineering 47, 6 (2021), 1277–1298. [https://doi.org/10.1109/TSE.2019.](https://doi.org/10.1109/TSE.2019.2918536)
    [2918536](https://doi.org/10.1109/TSE.2019.2918536)

    - <span id="page-11-39"></span>[4] Andrew Begel, Robert DeLine, and Thomas Zimmermann.
    2010. Social media for software engineering. In Proceedings of the FSE/SDP workshop
    on Future of software engineering research. 33–38.

    - <span id="page-11-38"></span>[5] Sue Black, Rachel Harrison, and Mark Baldwin.
    2010. A survey of social media use in software systems development. In Proceedings
    of the 1st Workshop on Web 2.0 for Software Engineering. 1–5.

    - <span id="page-11-40"></span>[6] Sue Black and Joanne Jacobs. 2010. Using Web
    2.0 to improve software quality. In Proceedings of the 1st Workshop on Web 2.0
    for Software Engineering. 6–11.

    - <span id="page-11-44"></span>[7] Kelly Blincoe, Jyoti Sheoran, Sean Goggins,
    Eva Petakovic, and Daniela Damian. 2016. Understanding the popular users: Following,
    affiliation influence and leadership on GitHub. Information and Software Technology
    70 (2016), 30–39.

    - <span id="page-11-41"></span>[8] Hudson Silva Borges and Marco Tulio Valente.
    2019. How do developers promote open source projects? Computer 52, 8 (2019), 27–33.

    - <span id="page-11-46"></span>[9] Gargi Bougie, Jamie Starke, Margaret-Anne Storey,
    and Daniel M German. 2011. Towards understanding twitter use in software engineering:
    preliminary findings, ongoing challenges and future questions. In Proceedings
    of the 2nd international workshop on Web 2.0 for software engineering. 31–36.

    - <span id="page-11-19"></span>[10] Andrea Capiluppi and Daniel Izquierdo-Cortázar.
    2013. Effort estimation of FLOSS projects: a study of the Linux kernel. Empirical
    Software Engineering 18 (2013), 60–88.

    - <span id="page-11-4"></span>[11] Open Collective. 2023. Raise and spend money
    with full transparency. — opencollective.com. [https://opencollective.com/.](https://opencollective.com/)
    [Accessed 28-Jun-2023].

    - <span id="page-11-31"></span>[12] CVE. 2021. CVE - CVE-2021-44228 — cve.mitre.org.
    [https://cve.mitre.org/cgi](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-44228)[bin/cvename.cgi?name=CVE-2021-44228.](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-44228)
    [Accessed 28-Jun-2023].

    - <span id="page-11-36"></span>[13] Nadia Eghbal. 2019. A handy guide to financial
    support for open source. [https://github.com/nayafia/lemonade-stand.](https://github.com/nayafia/lemonade-stand)
    Accessed: https://opensourcesurvey.org/2017/.

    - <span id="page-11-49"></span>[14] Hongbo Fang, Daniel Klug, Hemank Lamba, James
    Herbsleb, and Bogdan Vasilescu. 2020. Need for tweet: How open source developers
    talk about their github work on twitter. In Proceedings of the 17th International
    Conference on Mining Software Repositories. 322–326.

    - <span id="page-11-10"></span>[15] Hongbo Fang, Hemank Lamba, James Herbsleb,
    and Bogdan Vasilescu. 2022. "This is Damn Slick!": Estimating the Impact of Tweets
    on Open Source Project Popularity and New Contributors. In Proceedings of the
    44th International Conference on Software Engineering (ICSE ''22). 2116–2129.
    [https://doi.org/10.1145/](https://doi.org/10.1145/3510003.3510121) [3510003.3510121](https://doi.org/10.1145/3510003.3510121)

    - <span id="page-11-7"></span>[16] Gitcoin. 2023. Grants — gitcoin.co. [https://gitcoin.co/grants/.](https://gitcoin.co/grants/)
    [Accessed 28-Jun-2023].

    - <span id="page-11-33"></span>[17] GitHub. 2017. Open Source Survey 2017. [https://opensourcesurvey.org/2017/.](https://opensourcesurvey.org/2017/)
    Accessed: 2022-08-18.

    - <span id="page-11-30"></span>[18] GitHub. 2023. Build software better, together
    — github.com. [https://github.com/](https://github.com/search/) [search/.](https://github.com/search/)
    [Accessed 28-Jun-2023].

    - <span id="page-11-18"></span>[19] GitHub. 2023. GitHub GraphQL API documentation
    - GitHub Docs docs.github.com. [https://docs.github.com/en/graphql.](https://docs.github.com/en/graphql)
    [Accessed 28-Jun-2023].

    - <span id="page-11-8"></span>[20] Giveth. 2023. Giveth: Welcome to the Future
    of Giving — giveth.io. [https:](https://giveth.io) [//giveth.io.](https://giveth.io)
    [Accessed 28-Jun-2023].

    - <span id="page-11-29"></span>[21] Lichan Hong, Gregorio Convertino, and Ed Chi.
    2011. Language matters in twitter: A large scale study. In Proceedings of the
    international AAAI conference on web and social media, Vol. 5. 518–521.

    - <span id="page-11-25"></span>[22] Guido W. Imbens and Donald B. Rubin. 2015.
    Causal Inference for Statistics, Social, and Biomedical Sciences: An Introduction.
    Cambridge University Press. <https://doi.org/10.1017/CBO9781139025751>

    - <span id="page-11-42"></span>[23] Danaja Maldeniya, Ceren Budak, Lionel P Robert
    Jr, and Daniel M Romero. 2020. Herding a Deluge of Good Samaritans: How GitHub
    Projects Respond to Increased Attention. In Proceedings of The Web Conference
    2020. 2055–2065.

    - <span id="page-11-13"></span>[24] Mariam El Mezouar, Feng Zhang, and Ying Zou.
    2018. Are tweets useful in the bug fixing process? an empirical study on firefox
    and chrome. Empirical Software Engineering 23, 3 (2018), 1704–1742.

    - <span id="page-11-24"></span>[25] Emerson Murphy-Hill, Edward K. Smith, Caitlin
    Sadowski, Ciera Jaspan, Collin Winter, Matthew Jorde, Andrea Knight, Andrew Trenk,
    and Steve Gross. 2019. Do Developers Discover New Tools on the Toilet?. In Proceedings
    of the 41st International Conference on Software Engineering (Montreal, Quebec,
    Canada) (ICSE ''19). IEEE Press, 465–475.<https://doi.org/10.1109/ICSE.2019.00059>

    - <span id="page-11-22"></span>[26] Keitaro Nakasai, Hideaki Hata, and Kenichi
    Matsumoto. 2019. Are Donation Badges Appealing?: A Case Study of Developer Responses
    to Eclipse Bug Reports.


    IEEE Software 36, 03 (2019), 22–27.


    - <span id="page-11-28"></span>[27] Sharon-Lise T. Normand, Mary Beth Landrum,
    Edward Guadagnoli, John Z. Ayanian, Thomas J. Ryan, Paul D. Cleary, and Barbara
    J. McNeil. 2001. Validating recommendations for coronary angiography following
    acute myocardial infarction in the elderly: A matched analysis using propensity
    scores. Journal of Clinical Epidemiology 54, 4 (2001), 387–398. [https://doi.org/10.1016/S0895-4356\(00\)00321-8](https://doi.org/10.1016/S0895-4356(00)00321-8)

    - <span id="page-11-1"></span>[28] Cassandra Overney, Jens Meinicke, Christian
    Kästner, and Bogdan Vasilescu. 2020. How to Not Get Rich: An Empirical Study of
    Donations in Open Source. In Proceedings of ACM/IEEE 42nd International Conference
    on Software Engineering (ICSE ''20). 1209–1221.

    - <span id="page-11-5"></span>[29] Patreon. 2023. Creativity powered by membership
    | Patreon — patreon.com. [https://www.patreon.com/.](https://www.patreon.com/)
    [Accessed 28-Jun-2023].

    - <span id="page-11-3"></span>[30] PayPal. 2023. Digital Wallets, Money Management,
    and More — paypal.com. [https://www.paypal.com/.](https://www.paypal.com/) [Accessed
    29-Jun-2023].

    - <span id="page-11-45"></span>[31] Huilian Sophie Qiu, Yucen Lily Li, Susmita
    Padala, Anita Sarma, and Bogdan Vasilescu. 2019. The signals that potential contributors
    look for when choosing open-source projects. Proceedings of the ACM on Human-Computer
    Interaction 3, CSCW (2019), 1–29.

    - <span id="page-11-32"></span>[32] Uzma Raja and Marietta J. Tretter. 2012. Defining
    and Evaluating a Measure of Open Source Project Survivability. IEEE Transactions
    on Software Engineering 38, 1 (2012), 163–174.<https://doi.org/10.1109/TSE.2011.39>

    - <span id="page-11-34"></span>[33] Dirk Riehle, Philipp Riemer, Carsten Kolassa,
    and Michael Schmidt. 2014. Paid vs. volunteer work in open source. In Proceedings
    of 47th Hawaii International Conference on System Sciences (HICSS ''14). 3286–3295.

    - <span id="page-11-20"></span>[34] Jeanine Romano, Jeffrey D Kromrey, Jesse Coraggio,
    and Jeff Skowronek. 2006. Appropriate statistics for ordinal level data: Should
    we really be using t-test and Cohen''sd for evaluating group differences on the
    NSSE and other surveys. In annual meeting of the Florida Association of Institutional
    Research, Vol. 177. 34.

    - <span id="page-11-9"></span>[35] Naomichi Shimada, Tao Xiao, Hideaki Hata, Christoph
    Treude, and Kenichi Matsumoto. 2022. GitHub Sponsors: Exploring a New Way to Contribute
    to Open Source. In Proceedings of the 44th International Conference on Software
    Engineering (ICSE ''22). 1058–1069.<https://doi.org/10.1145/3510003.3510116>

    - <span id="page-11-14"></span>[36] Leif Singer, Fernando Figueira Filho, and
    Margaret-Anne Storey. 2014. Software engineering at the speed of light: how developers
    stay current using twitter. In Proceedings of the 36th International Conference
    on Software Engineering. 211–221.

    - <span id="page-11-6"></span>[37] GitHub Sponsors. 2023. GitHub Sponsors — github.com.
    [https://github.com/](https://github.com/sponsors) [sponsors.](https://github.com/sponsors)
    [Accessed 28-Jun-2023].

    - <span id="page-11-11"></span>[38] Margaret-Anne Storey, Christoph Treude, Arie
    van Deursen, and Li-Te Cheng. 2010. The impact of social media on software engineering
    practices and tools. In Proceedings of the FSE/SDP workshop on Future of software
    engineering research. 359–364.

    - <span id="page-11-12"></span>[39] Margaret-Anne Storey, Alexey Zagalsky, Fernando
    Figueira Filho, Leif Singer, and Daniel M German. 2016. How social and communication
    channels shape and challenge a participatory culture in software development.
    IEEE Transactions on Software Engineering 43, 2 (2016), 185–204.

    - <span id="page-11-48"></span>[40] Yuan Tian, Palakorn Achananuparp, Ibrahim
    Nelman Lubis, David Lo, and Ee-Peng Lim. 2012. What does software engineering
    community microblog about?. In 2012 9th IEEE Working Conference on Mining Software
    Repositories (MSR). IEEE, 247–250.

    - <span id="page-11-47"></span>[41] Yuan Tian and David Lo. 2014. An exploratory
    study on software microblogger behaviors. In 2014 IEEE 4th Workshop on Mining
    Unstructured Data. IEEE, 1–5.

    - <span id="page-11-15"></span>[42] Christoph Treude, Fernando Figueira Filho,
    Brendan Cleary, and Margaret-Anne Storey. 2012. Programming in a socially networked
    world: the evolution of the social programmer. In The Future of Collaborative
    Software Development. 1–3.

    - <span id="page-11-43"></span>[43] Asher Trockman, Shurui Zhou, Christian Kästner,
    and Bogdan Vasilescu. 2018. Adding sparkle to social coding: an empirical study
    of repository badges in the npm ecosystem. In Proceedings of the 40th international
    conference on software engineering. 511–522.

    - <span id="page-11-17"></span>[44] Twitter. 2022. Twitter API Documentation —
    developer.twitter.com. [https:](https://developer.twitter.com/en/docs/twitter-api)
    [//developer.twitter.com/en/docs/twitter-api.](https://developer.twitter.com/en/docs/twitter-api)
    [Accessed 28-Jun-2023].

    - <span id="page-11-16"></span>[45] Twitter. 2022. Twitter API for Academic Research
    | Products — developer.twitter.com. [https://developer.twitter.com/en/products/twitter-api/](https://developer.twitter.com/en/products/twitter-api/academic-research)
    [academic-research.](https://developer.twitter.com/en/products/twitter-api/academic-research)
    [Accessed 28-Jun-2023].

    - <span id="page-11-21"></span>[46] Anthony Viera and Joanne Garrett. 2005. Understanding
    Interobserver Agreement: The Kappa Statistic. Family medicine (2005).

    - <span id="page-11-23"></span>[47] Longqi Yang, David Holtz, Sonia Jaffe, Siddharth
    Suri, Shilpi Sinha, Jeffrey Weston, Connor Joyce, Neha Shah, Kevin Sherman, Brent
    Hecht, and Jaime Teevan. 2022. The effects of remote work on collaboration among
    information workers. Nature Human Behaviour 6, 1 (01 Jan 2022), 43–54. [https://doi.org/10.1038/s41562-021-](https://doi.org/10.1038/s41562-021-01196-4)
    [01196-4](https://doi.org/10.1038/s41562-021-01196-4)

    - <span id="page-11-37"></span>[48] Xunhui Zhang, Tao Wang, Yue Yu, Qiubing Zeng,
    Zhixing Li, and Huaimin Wang. 2022. Who, What, Why and How? Towards the Monetary
    Incentive in Crowd Collaboration: A Case Study of Github''s Sponsor Mechanism.
    In CHI Conference on Human Factors in Computing Systems. 1–18.

    - <span id="page-11-2"></span>[49] Jiayuan Zhou, Shaowei Wang, Yasutaka Kamei,
    Ahmed E Hassan, and Naoyasu Ubayashi. 2022. Studying donations and their expenses
    in open source projects: a case study of GitHub projects collecting donations
    through open collectives. Empirical Software Engineering 27, 1 (2022), 1–38.'
  decisions:
    language: '- Qualified. Reason: English Paper'
    evaluation_prompt: Qualified.
    related_work_prompt: Qualified
    novelty_prompt: Qualified
    review_only_prompt: Qualified.
  llm_input_used: '## Abstract

    GitHub Sponsors was launched in 2019, enabling donations to open-source

    software developers to provide financial support, as per GitHub''s slogan:

    "Invest in the projects you depend on". However, a 2022 study on GitHub

    Sponsors found that only two-fifths of developers who were seeking sponsorship

    received a donation. The study found that, other than internal actions (such as

    offering perks to sponsors), developers had advertised their GitHub Sponsors

    profiles on social media, such as Twitter (also known as X). Therefore, in this

    work, we investigate the impact of tweets that contain links to GitHub Sponsors

    profiles on sponsorship, as well as their reception on Twitter/X. We further

    characterize these tweets to understand their context and find that (1) such

    tweets have the impact of increasing the number of sponsors acquired, (2)

    compared to other donation platforms such as Open Collective and Patreon,

    GitHub Sponsors has significantly fewer interactions but is more visible on

    Twitter/X, and (3) developers tend to contribute more to open-source software

    during the week of posting such tweets. Our findings are the first step toward

    investigating the impact of social media on obtaining funding to sustain

    open-source software.


    ## Introduction

    Open-source software (OSS) is ubiquitous, but sustaining it is a challenge [\[28\]](#page-11-1).
    Maintaining an OSS project requires not only intrinsic motivation (e.g., joy of
    participation) but also extrinsic motivation (e.g., financial incentives) [\[49\]](#page-11-2).
    The last few years have seen the emergence of many platforms that allow open-source
    developers to receive donations for their work, such as PayPal [\[30\]](#page-11-3),
    Open Collective [\[11\]](#page-11-4), Patreon [\[29\]](#page-11-5), and GitHub
    Sponsors [\[37\]](#page-11-6). Several platforms support sponsoring OSS projects
    in cryptocurrencies, with the rise in popularity of cryptocurrencies today, e.g.,
    Gitcoin Grants [\[16\]](#page-11-7) and Giveth [\[20\]](#page-11-8). However,
    as Overney et al.''s paper title "How to not get rich: an empirical study of donations
    in open source" [\[28\]](#page-11-1) suggests, simply having a platform for donations
    is not enough. In a 2022 study on GitHub Sponsors, Shimada et al. [\[35\]](#page-11-9)
    found that out of approximately 9,000 developers who had activated their GitHub
    Sponsors profile, less than 40% had received a donation.


    If simply creating a sponsorship profile is not enough, what else can open-source
    software developers do to attract donations? Following the long line of work on
    studying the intersection between social media and software development [\[15,](#page-11-10)
    [38,](#page-11-11) [39\]](#page-11-12), in this paper, we investigate the impact
    of tweeting about a GitHub Sponsors profile on sponsorship. To make it easy for
    its users to reach a large audience, GitHub provides tweet templates that users
    can use to advertise a new GitHub Sponsors profile ("My GitHub Sponsors profile
    is live! You can sponsor me to support my open source work ") or to broadcast
    that they made a donation (" I''m sponsoring [username] because..."). The impact
    of tweets on open-source software development has been investigated before. Fang
    et al. [\[15\]](#page-11-10) found that tweets have a significant effect on obtaining
    new stars and new contributors for an open-source project and that the formation
    of an active Twitter/X community plays an important role in attracting new contributors.
    The role of tweets has also been studied in the context of bug fixing [\[24\]](#page-11-13)
    and trend awareness [\[36\]](#page-11-14). To the best of our knowledge, the role
    of Twitter/X in obtaining funding for open-source development has not yet been
    studied.


    We first characterize the state of the practice by quantitatively and qualitatively
    analyzing more than 10,000 tweets linking to GitHub Sponsors profiles to understand
    the context of such tweets.


    Permission to make digital or hard copies of all or part of this work for personal
    or classroom use is granted without fee provided that copies are not made or distributed
    for profit or commercial advantage and that copies bear this notice and the full
    citation on the first page. Copyrights for components of this work owned by others
    than the author(s) must be honored. Abstracting with credit is permitted. To copy
    otherwise, or republish, to post on servers or to redistribute to lists, requires
    prior specific permission and/or a fee. Request permissions from permissions@acm.org.


    We then measure the impact of the tweets in terms of their reception on Twitter/X
    and their effect on sponsorship. We found links to GitHub Sponsors profiles on
    Twitter/X are common and the majority of such tweets are written by users other
    than the profile owner, such as a sponsor. We identified a significant positive
    effect of GitHub Sponsor profile mentions in tweets on the number of sponsors
    acquired. Tweet mentions have the impact of increasing the number of sponsors
    by 1.22. Although GitHub Sponsors has surpassed other platforms such as Open Collective
    and Patreon in terms of visibility on Twitter/X, tweets about GitHub Sponsors
    received significantly fewer likes, retweets, and replies compared to other platforms.
    Developers tended to be more active during the week of a tweet, in particular,
    in terms of the number of commits.


    Significance of research contribution. The findings of our study have significant
    implications, indicating a strong interconnection between social media channels
    and donation pathways within the social programmer ecosystem [\[42\]](#page-11-15).
    Our research demonstrates that actively engaging on social media platforms to
    promote sponsorship opportunities for open-source development can yield fruitful
    outcomes. This suggests that open-source developers stand to benefit from expanding
    their presence and networking efforts beyond the GitHub platform. Furthermore,
    our study highlights the notion that publicity and visibility in the realm of
    open-source sponsorship need not be limited to a unidirectional flow. Rather,
    sponsors themselves have the potential to enhance the exposure and reach of open-source
    projects by publicizing their donations. In doing so, they serve as exemplars,
    setting a positive precedent and inspiring others to follow suit. By emphasizing
    these key findings, we provide compelling evidence to support the notion that
    using social media channels, diversifying online networks, and fostering mutual
    publicity between sponsors and developers can yield substantial advantages within
    the open-source community. These insights encourage open-source developers and
    sponsors alike to consider the broader potential of social media engagement and
    collaborative promotion to achieve their goals.'
  token_usage: 6447
  time_usage: 2.436267852783203
- title: "Guiding Effort Allocation in Open-Source Software Projects Using Bus\n \
    \ Factor Analysis"
  abstract: 'A critical issue faced by open-source software projects is the risk of
    key

    personnel leaving the project. This risk is exacerbated in large projects that

    have been under development for a long time and experienced growth in their

    development teams. One way to quantify this risk is to measure the

    concentration of knowledge about the project among its developers. Formally

    known as the Bus Factor (BF) of a project and defined as ''the number of key

    developers who would need to be incapacitated to make a project unable to

    proceed''. Most of the proposed algorithms for BF calculation measure a

    developer''s knowledge of a file based on the number of commits. In this work,

    we propose using other metrics like lines of code changes (LOCC) and cosine

    difference of lines of code (change-size-cos) to calculate the BF. We use these

    metrics for BF calculation for five open-source GitHub projects using the CST

    algorithm and the RIG algorithm, which is git-blame-based. Moreover, we

    calculate the BF on project sub-directories that have seen the most active

    development recently. Lastly, we compare the results of the two algorithms in

    accuracy, similarity in results, execution time, and trends in BF values over

    time.'
  url: http://arxiv.org/abs/2401.03303v1
  keywords: ''
  document: '# Guiding Effort Allocation in Open-Source Software Projects Using Bus
    Factor Analysis


    [Aliza Lisan](https://orcid.org/0002-1544-1493) Boyana Norris alisan@uoregon.edu
    norris@cs.uoregon.edu University of Oregon Eugene, Oregon, USA


    #### ABSTRACT


    A critical issue faced by open-source software projects is the risk of key personnel
    leaving the project. This risk is exacerbated in large projects that have been
    under development for a long time and experienced growth in their development
    teams. One way to quantify this risk is to measure the concentration of knowledge
    about the project among its developers. Formally known as the Bus Factor (BF)
    of a project and defined as "the number of key developers who would need to be
    incapacitated to make a project unable to proceed" [\[1\]](#page-7-0). Most of
    the proposed algorithms for BF calculation measure a developer''s knowledge of
    a file based on the number of commits. In this work, we propose using other metrics
    like lines of code changes (LOCC) and cosine difference of lines of code (change-size-cos)
    to calculate the BF. We use these metrics for BF calculation for five open-source
    GitHub projects using the CST algorithm and the RIG algorithm, which is git-blame-based.
    Moreover, we calculate the BF on project sub-directories that have seen the most
    active development recently. Lastly, we compare the results of the two algorithms
    in accuracy, similarity in results, execution time, and trends in BF values over
    time.


    ### CCS CONCEPTS


    • Software and its engineering → Risk management; Maintaining software; Software
    version control; Open source model; Programming teams.


    #### KEYWORDS


    Bus factor, Open-Source, Code ownership, Risk management, Mining GitHub repositories


    #### 1 INTRODUCTION


    If we look at our daily use of the Internet, we realize that we are consumed with
    the use of software and applications. The development and maintenance of all these
    software projects are based on the knowledge held by its developers. This makes
    the software development process the most dependent upon its developers. Given
    that, developers become an important asset for project organizations and open-source
    project teams. This also makes the rate at which developers leave a software project
    a critical matter and risk. To mitigate this risk, it is important that project
    managers or principal developers monitor and quantify the concentration of knowledge
    of the project among its developers [\[2\]](#page-7-1) An interesting measurement
    used in this regard is known as the Bus Factor (BF), which is defined as the minimum
    number of key developers whose departure


    would make a project unable or difficult to proceed [\[2\]](#page-7-1) A smaller
    bus factor value would mean that the maximum knowledge of the project is concentrated
    among them and the project is at a higher risk if some or all of these developers
    leave the project, company or go for a vacation, etc. A bus factor of one would
    be the worst-case scenario. Conversely, a high bus factor means a lesser risk
    is posed to the project in case some of the developers end up leaving.


    When it comes to open-source software projects, the bus factor is of much more
    importance. Most of the people working on these projects are making contributions
    voluntarily. Volunteering means that the developers or contributors do not have
    financial benefits associated with these projects. This puts open-source software
    projects at a higher risk of developer turnover [\[2\]](#page-7-1). Given the
    importance of bus factor measurement in open-source software development projects,
    algorithms have been proposed to calculate it using data gathered from version
    control systems such as GitHub [\[1,](#page-7-0) [3,](#page-7-2) [4\]](#page-7-3).
    To the best of our knowledge, most of these algorithms are commit-based, i.e.,
    the algorithms look at the commit data from version control repositories. Moreover,
    existing work mostly proposes bus factor calculation algorithms and presents tools
    for the same. Few studies attempt to validate the results of these algorithms.
    [\[2\]](#page-7-1) is an empirical and comparative study where three Bus Factor
    algorithms are validated, but they only used the tools and metrics provided by
    the original authors.


    Therefore, in this paper, we look at two algorithms (a) first one was proposed
    by Cosentino et al. [\[2\]](#page-7-1), which calculates the bus factor of each
    file and aggregates it up to branch, directory, or project level; (b) second one
    proposed by Rigby et al. [\[4\]](#page-7-3), which uses a git-blame-based approach
    to calculate the bus factor. The tool based on the first algorithm is publicly
    available [\[1\]](#page-7-0); however, the authors have only provided the pseudo-code
    of the second. Using the convention followed in [\[2\]](#page-7-1), we refer to
    the first algorithm as CST and the second one as RIG. Among the two algorithms,
    CST is a commit-based algorithm, and we wanted to use it with other metrics like
    lines of code changes (LOCC) and cosine difference of lines of code (change-size-cos).
    For that, we implemented it by understanding the algorithm mentioned in [\[1\]](#page-7-0).
    RIG algorithm had to be implemented as its code, or any related tool is not publicly
    available. We implemented and tested both algorithms on five opensource projects
    and compared their results. We also got feedback from the principal developers
    of these projects to validate the results. Lastly, we used the tool provided in
    [\[1\]](#page-7-0) to obtain bus factor results for the selected projects.


    To summarize, we seek to answer the following research questions:


    - (1) How do different metrics such as lines of changed code and cosine difference
    affect the bus factor computation?

    - (2) How can the bus factor guide principal developers or managers to allocate
    effort for hiring or knowledge transfer among the existing team of developers?

    - (3) How do bus factor algorithms differ in terms of accuracy and performance?


    As such, our contributions can be summarized as follows:


    - Implemented the CST algorithm (with lines of code changes and cosine difference
    of lines of code) and the RIG algorithm and compared their results.

    - Got feedback from the principal developers of the five selected projects to
    validate our results.

    - Calculated the BF of the selected projects using the commitbased CST algorithm
    tool provided in [\[1\]](#page-7-0) to compare with our results.

    - Calculated the bus factor for the selected projects for the last five years
    to see the trend in BF values.


    In Section [2,](#page-1-0) we review the existing literature for bus factor calculation.
    Section [3](#page-2-0) explains the design and steps involved in our study. We
    present our results in Section [4](#page-4-0) and conclude the paper in Section
    [6.](#page-7-4)


    #### <span id="page-1-0"></span>2 EXISTING LITERATURE


    In this section, first, we describe the two algorithms for measuring bus factors
    that we will be using as part of our study in this paper. Towards the end, we
    describe other algorithms that are part of the existing literature but not implemented
    in our study.


    #### <span id="page-1-2"></span>2.1 CST Algorithm


    The CST algorithm is proposed as part of a tool paper [\[1\]](#page-7-0) by Cosentino
    et al. The algorithm calculates the knowledge of developers on each file and determines
    the bus factor for each file based on that. Furthermore, aggregation is performed
    from file to directory, branch, or project level to get developer knowledge. The
    aggregation is done by simply adding the knowledge on each file and scaling with
    the number of files in the directory, branch, or project. The authors proposed
    four metrics to calculate a developer''s knowledge of a file. last change takes
    it all assigns 100% knowledge of a file to the last developer who modified it,
    and in multiple changes equally considered, the knowledge of a developer is calculated
    by dividing the number of commits made by the developer on a file by the total
    number of commits ever made on that file. The third metric is non-consecutive
    changes, which considers the developer''s non-consecutive commits on a file only
    and merges the consecutive commits into a single one. The last metric is an extension
    of the third metric, called the weighted non-consecutive changes, and assigns
    incremental weight to the later commits on the file.


    Once the developer''s knowledge of the artifacts, i.e., file, directory, branch,
    or project, has been calculated, the bus factor for each of these artifacts is
    measured by using two sets of developers. The first set is called the primary
    developers who have minimum knowledge of the artifact. The second set is called
    the secondary developers who at least have some knowledge of the artifact, where
    is less than . The threshold for the selection of primary developers is set to
    1/ where is the total number of developers that have made changes to the artifact
    till date. is set of half of for the selection of secondary developers. The number
    of developers in the union of both these sets gives us the bus factor for the
    particular artifact.


    Since, CST is a commit-based algorithm, we use the same algorithm with LOCC and
    change-size-cos. The details of how we collect and use this data for open-source
    projects on GitHub are provided in later sections.


    <span id="page-1-1"></span>


    | Algorithm 1 RIG Algorithm                        |

    |--------------------------------------------------|

    | Data git-blame data for each file in the project |

    | Result BFset (BF developers), g (Bus Factor)     |

    | 1: for 𝑔<br>← 1 𝑡𝑜<br>200 do                     |

    | for 𝑖<br>← 1 𝑡𝑜<br>1000 do<br>2:                 |

    | 𝐵𝐹𝑠𝑒𝑡<br>← random sample of g devs;<br>3:        |

    | remove-authors (BFset);<br>4:                    |

    | if abandoned-files() ≥ 50% then<br>5:            |

    | return g, BFset;<br>6:                           |

    | end if<br>7:                                     |

    | end for<br>8:                                    |

    | 9: end for                                       |

    | 10: return null, null;                           |

    |                                                  |

    |                                                  |


    #### <span id="page-1-3"></span>2.2 RIG Algorithm


    RIG algorithm was proposed by Rigby et al. [\[4\]](#page-7-3), in which they adapt
    the financial risk management measures to a developer turnover context to measure
    the risk posed to a project from developer turnover. Unlike the CST algorithm,
    RIG uses a blame-based approach to calculate code ownership of developers. The
    blame feature is implemented in version control systems such that a git-blame
    command assigns each line in a file to a developer who changed it last. The algorithm''s
    authors claim that this approach allows them to follow code ownership at a finer
    granularity. As per the algorithm, a line is considered abandoned if it is attributed
    to a developer who is no longer part of the project, and a file is considered
    abandoned when 90% of its files are abandoned. This high threshold makes sure
    that developers with trivial contributions are excluded. Algorithm [1](#page-1-1)
    shows the pseudo-code for the RIG algorithm. The algorithm starts by varying the
    size of developers from 1 to 200 who leave the project, as shown in line 1. Line
    3 shows the random sampling of developers of size who leave the project, and this
    random sampling is repeated 1,000 times for every value of . We limit these iterations
    to 10 instead of 1,000 because the algorithm takes a long time on relatively larger
    projects we chose. Moreover, following [\[2\]](#page-7-1), we return the lowest
    value of that resulted in the abandoned status of more than 50% of the files,
    as shown from lines 5-7. The authors in the original paper [\[4\]](#page-7-3)
    also calculate the likelihood of each group of developers leaving the project
    between lines 3-4, but we omit it since we are A noticeable characteristic of
    this algorithm is its non-deterministic nature, i.e., different results are produced
    for each execution of the algorithm because of the random sampling of the developers.
    Also, the algorithm may not


    return a valid result, as can be seen from line 10, which happens when no BFset
    results in more than 50% of abandon files.


    #### 2.3 Other Bus Factor Algorithms


    Another commit-based algorithm was proposed by Avelino et al. and computes the
    Degree of Authorship (DOA) of each file to identify the key developers [\[3\]](#page-7-2).
    The DOA value for a file is initialized when it is created by a certain developer
    . The DOA of increases when makes more commits on while it decreases if other
    developers commit to . Lastly, DOA values are normalized for each file with the
    developer with the highest DOA equal to 1. Developers with a DOA greater than
    0.75 are considered the authors of that file. We have not implemented this algorithm
    as part of our study and chose one commit-based algorithm only i.e., the CST algorithm
    explained in Section [2.1.](#page-1-2)


    The first algorithm for the automated calculation of BF from version control systems
    was proposed by Zazworka et al. [\[10\]](#page-7-5) in which each developer who
    made changes to a file, regardless of the number of commits, is considered the
    author of that file. Moreover, to find the BF, they look at each combination of
    developers ranging from 1 to (total developers). The BF is the largest combination
    of developers that a project may lose while the remaining developers still have
    knowledge of at least a part of the project''s files. Since the algorithm looks
    at each combination, it is shown in [\[11\]](#page-7-6) that the algorithm scales
    to a maximum of 30 developers only. It is also mentioned in [\[2\]](#page-7-1)
    that this algorithm did not terminate on projects even after running for more
    than three days. Hence, we did not include this algorithm in our study since all
    our projects'' total number of developers exceeds 30.


    #### <span id="page-2-0"></span>3 STUDY DESIGN


    In the following subsections, we will discuss in detail the database for GitHub
    projects that we use in this study, the bus factor algorithms that we implement,
    and how we validate the bus factor results. For the RIG algorithm, we executed
    our experimental runs in Google Colaboratory and on a server.


    ## 3.1 Database of GitHub data for HPC projects and other Metrics


    In this work, we used GitHub data from five open-source highperformance computing
    (HPC) software projects. Table [1](#page-3-0) shows some details about these projects.
    The GitHub data for a number of open-source projects, including these, has been
    stored, organized and parsed by us in a SQL database to be used for different
    research projects. The database contains computations for the lines of code changes
    (LOCC) and the cosine difference of code changes (changesize-cos), which are used
    for the bus factor calculation in our study for the CST algorithm.


    As mentioned in Section [2,](#page-1-0) AVL and CST algorithms are commitbased.
    However, commits are not a great metric to consider to measure code ownership.
    One reason is the difference in coding styles of different individuals, where
    some frequently commit after every small change while others prefer to commit
    once after completing the task. Moreover, a commit-based approach would consider
    a commit consisting of deletions equal to that of additions


    regardless of the actual contribution made [\[4\]](#page-7-3). Also, deletion
    removes the authorship of a certain piece of code from a developer and, hence,
    decreases the developer''s knowledge.


    With LOCC, each file is analyzed based on the number of lines the author has contributed
    with a commit. However, it will also consider the addition and deletion of blank
    spaces or comments as meaningful contributions. For the cosine difference of the
    changes, textdistance Python package is used to examine differences in text. It
    adds all the words from the changed lines to a word bank, places the added words
    in a column and deleted words in a row. Then it compares the columns and rows
    and assigns a numeric value of how much the line was changed. Given that, a change
    of variable name will be considered as a very small change under the cosine metric
    but may be a larger change under LOCC. Hence, using the cosine difference between
    the changed lines of code only considers the contributions with actual impact
    on the files, giving a comprehensive understanding of a developer''s contribution.


    The authors of the RIG algorithm give a similar reasoning for not using a commit-based
    approach and instead mention that a blame-based approach allows them to study
    code ownership at a finer granularity [\[4\]](#page-7-3).


    #### <span id="page-2-1"></span>Algorithm 2 CST Algorithm


    | Input cMetric, cstMetric, timeRange (opt.), directory (opt.), |

    |---------------------------------------------------------------|

    | branch(es) (opt.)                                             |

    | Result primaryDev, secondaryDev, busFactor                    |


    - 1: ← ;

    - 2: ← 1/;

    - 3: ← /2;

    - 4: Assign dataFrame for sum of cMetric values against each file to ;

    - 5: if == ℎ then

    - 6: for file in do

    - 7: ← () for ;

    - 8: ← () for on ;

    - 9: ← ;

    - 10: end for

    - 11: end if

    - 12: Assign a dataFrame for aggregated to directory/project level to ;

    - 13: for in do

    - 14: if ≥ then

    - 15: = + 1;

    - 16: else if ≥ then

    - 17: = + 1;

    - 18: end if

    - 19: end for

    - 20: ← +;


    #### 3.2 Implementation of Bus Factor Algorithms


    After reviewing the literature for the CST algorithm [\[1\]](#page-7-0), we aimed
    to use this algorithm on two new metrics. Before doing that, we decided to calculate
    BF on our selected projects using the tools provided by the authors of the CST
    algorithm. Two tools are needed to calculate the BF of a project, as provided
    by the authors. The


    <span id="page-3-0"></span>


    | Project   | Release Year | Contributors | Language | Description                                                |  |

    |-----------|--------------|--------------|----------|------------------------------------------------------------|--|

    | PETSc[5]  | 1994         | 207          | C        | Portable, Extensible Toolkit
    for Scientific Computation.   |  |

    | Spack[6]  | 2014         | 1,164        | Python   | Multi-platform package
    manager.                            |  |

    | Hypre[7]  | 2004         | 34           | C        | Library of high performance
    pre-conditioners and solvers.  |  |

    | Lammps[8] | 2016         | 224          | C++      | Large-scale Atomic/Molecular
    Massively Parallel Simulator. |  |

    | NWChem[9] | 1994         | 44           | Fortran  | Open Source High-Performance
    Computational Chemistry.      |  |


    ![](_page_3_Figure_1.jpeg)


    <span id="page-3-4"></span>![](_page_3_Figure_2.jpeg)


    <span id="page-3-6"></span>Table 2: Bus factors provided by the principal developers
    of the projects and the loss tolerance of each calculated using Equation [1.](#page-5-0)


    | Project | Key developers | Loss tolerance |  |  |

    |---------|----------------|----------------|--|--|

    | PETSc   | 7              | 4              |  |  |

    | Hypre   | 17             | 3              |  |  |

    | Lammps  | 4              | -              |  |  |

    | NWChem  | 4              | 2              |  |  |


    Figure 1: GReMCat Software Framework.


    first is called Gitana[1](#page-3-1) , which imports all the data from a GitHub
    repository to an SQL database and then exports it to a JSON file [\[12\]](#page-7-12).
    Second, the web-based bus factor calculating tool[2](#page-3-2) takes the JSON
    file as input and calculates BF based on the selected CST metric and thresholds.
    It is worth noting that exporting data from the database using Gitana takes a
    lot longer than importing the data from GitHub. For instance, running Gitana for
    the smallest project, Hypre took more than 8 hours.


    Moreover, we implemented the CST algorithm as part of the Git repository mining
    and analysis software (GReMCat)[3](#page-3-3) . The implementation design of the
    complete software framework can be seen in Figure [1.](#page-3-4) The CST algorithm
    is implemented in the patterns package and can be used from a Jupiter Notebook[4](#page-3-5)
    by calling the visualizer object. The authors of CST do not provide a pseudocode
    for their proposed algorithm in their paper [\[1\]](#page-7-0). We implemented
    it based on the explanation in [\[1\]](#page-7-0) and [\[2\]](#page-7-1) and its
    pseudocode for our implementation of multiple changes equally considered metric
    is presented in Algorithm [2.](#page-2-1) Instead of using the number of commits,
    we used the LOCC or change-size-cos values against each commit that is pre-computed
    in our database. The algorithm allows the user to input the branch, directory,
    time period, CST metric and the proposed data metrics they want to be used for
    the BF calculation. The time period can be year-year, month-month, a specific
    year or month.


    As mentioned in Section [2.2,](#page-1-3) the RIG algorithm is not commitbased,
    and due to the size and continuously changing nature of the git blame data, we
    do not store it in our database. In fact, we implemented RIG presented in Algorithm
    [1](#page-1-1) outside of GReMCat,


    <span id="page-3-5"></span><sup>4</sup>https://tinyurl.com/CSTnotebook


    and executed it on our selected projects in Google Colaboratory and on a server.
    The authors of the RIG algorithm do not provide a public tool or its code but
    it was fairly easy to understand and implement from their explanation in [\[4\]](#page-7-3)
    and also using the pseudocode provided in [\[2\]](#page-7-1).


    #### 3.3 Handling External Code and Aliases


    GReMCat already had its implementation of removing external code and libraries
    from the commits data of the projects in the database based on the names of directories.
    We used it for the CST algorithm and applied the same methodology to the RIG algorithm.
    Moreover, GReMCat uses Python fuzzywuzzy package [\[13\]](#page-7-13) for identifying
    multiple different names and emails associated with the same author. We used this
    implementation for both CST and RIG algorithms to handle aliases.


    #### <span id="page-3-7"></span>3.4 Validating Bus Factor Results


    In order to validate the Bus Factor results, we reached out to the principal developers
    of the five projects via email and asked them three questions: (a) Can you estimate
    the total number of key developers in the project? If your estimate is for a specific
    time period, please indicate the years. (b) Who do you consider your top project
    contributors overall or during a specific time period (please indicate which years)?
    (c) How many key developers could you lose (in a worst-case scenario) and still
    continue successfully with your project? We received answers from four out of
    five projects, while some chose not to name the developers. The answers we received
    are reported in Table [2,](#page-3-6) where the key developers correspond to answers
    to (a) while loss tolerance corresponds to answers to (c).


    Additionally, we also used the tool provided by the authors of the CST algorithm
    to get BF results for the projects. We were able to export GitHub data to JSON
    files for three of the projects while getting the BF results for only two of them.
    The tools continued


    <span id="page-3-1"></span><sup>1</sup>https://github.com/valeriocos/Gitana


    <span id="page-3-2"></span><sup>2</sup>https://github.com/SOM-Research/busfactor


    <span id="page-3-3"></span><sup>3</sup>https://github.com/HPCL/ideas-uo


    <span id="page-4-1"></span>Table 3: Bus Factor values calculated using change-size-cos
    and LOCC in CST, RIG, and the tools by the authors of commits-based CST.


    | Project | CST (cos) | CST (LOCC) | RIG | CST (commits) |

    |---------|-----------|------------|-----|---------------|

    | PETSc   | 32        | 28         | -   | -             |

    | Hypre   | 12        | 10         | 29  | 5             |

    | Lammps  | 24        | 23         | -   | -             |

    | NWChem  | 13        | 13         | -   | 7             |

    | Spack   | 211       | 172        | -   | -             |


    executing for larger projects like PETSc[\[5\]](#page-7-7) and Lammps[\[8\]](#page-7-10)
    for more than 48 hours without producing results.


    <span id="page-4-2"></span>![](_page_4_Figure_3.jpeg)


    ![](_page_4_Figure_4.jpeg)


    #### <span id="page-4-0"></span>4 RESULTS


    In this section, first we compare the project and directory-level BF results of
    the two algorithms with the feedback received from the principal developers and
    compare results from different CST metrics. Secondly, we look at the trend in
    bus factor values for the past five years. Lastly, we look into the performance
    of RIG algorithm by comparing its directory-level results.


    <span id="page-4-3"></span>![](_page_4_Figure_7.jpeg)


    ![](_page_4_Figure_8.jpeg)


    ![](_page_4_Figure_9.jpeg)


    Figure 3: Comparison between bus factor values for the combinations of the four
    CST metrics and the two data metrics.


    #### <span id="page-4-4"></span>4.1 Comparison for Accuracy of Results


    In this subsection, we answer the first of the research questions, which focuses
    on the impact of different metrics on the bus factor computation. The bus factors
    calculated by the CST algorithm with our proposed data metrics, the CST tool provided
    by the authors (commits-based), and our implementation of the RIG algorithm are
    reported in Table [3.](#page-4-1) The values reported are based on the mulchanges-equal
    CST metric. The tool for the CST algorithm took more than 8 hours to produce results
    for each of the two projects Hypre and NWChem. The calls to the SQL database containing
    the commits data timed out for the other three projects, given their large sizes.
    Moreover, the RIG algorithm only returned a bus factor for the Hypre project,
    which is the smallest in terms of the number of files and subsequently the git-blame
    data. For projects with many files and authors, the algorithm continued execution
    for more than 24 hours. Thus, the missing results in the Table [3](#page-4-1)
    highlight the limitations of the RIG algorithm and the tools of the CST algorithm
    given the large size of the projects.


    As mentioned in Section [3.4,](#page-3-7) for the accuracy of results and validation
    of the bus factor values in Table [3,](#page-4-1) we got in touch with the principal
    developers of each of the projects and recorded their responses in Table [2.](#page-3-6)
    By comparing the bus factor values in Table [2](#page-3-6) and [3,](#page-4-1)
    it can be seen that the values given by the developers do not exactly match the
    values by any of the algorithms. However, the error for the Hypre project, which
    is calculated as follows:


    <span id="page-5-0"></span>

    $$error = |BF\_{algorithm} - BF\_{prricipal\\_dev}|\tag{1}$$


    is the smallest for change-size-cos ( = 5) and LOCC ( = 7) based CST algorithm
    implemented as part of this study. = 12 for the commit-based CST [\[1\]](#page-7-0)
    and the RIG algorithm [\[4\]](#page-7-3). Lastly, we looked at the results from
    the commit-based CST algorithm and observed that the results were not in sync
    with the cut-offs mentioned in [\[1\]](#page-7-0) for primary and secondary developers,
    hence, the high error value.


    <span id="page-5-1"></span>![](_page_5_Figure_3.jpeg)


    ![](_page_5_Figure_4.jpeg)


    Figure 4: Trend in project level bus factors for last five years for LOCC and
    change-size-cos based CST. The yellow bars represent the total number of developers.
    The dotted lines represent the percentage of BF developers from the total corresponding
    to the right y-axis.


    Along with comparing the bus factor values, we also looked at their accuracy in
    identifying the developers who are part of the bus factor result by each algorithm.
    Only the principal developers of the Lammps and NWChem project shared the names
    of the developers constituting the bus factor. For privacy reasons, we won''t
    be sharing the names in this paper. However, from our comparative study, we can
    state that the developers identified by the principal developers for both Lammps
    and NWChem are at the top of the sorted list for change-size-cos and LOCC-based
    CST algorithm. The top developer identified by the tool in [\[1\]](#page-7-0)
    for NWChem is not even part of the developers named by the NWChem''s principal
    developer. This shows that the threshold for primary and secondary developers
    in [\[1\]](#page-7-0) is not the best case and also that change-size-cos and LOCCbased
    CST algorithm is more accurate. RIG performed the worst by identifying only 5
    out of 12 (change-size-cos) or 5 out of 10 (LOCC) developers correctly for the
    Hypre project. It is important to note that the five projects chosen for this
    study are large in terms of files, number of developers, commits, and git-blame
    data. This highlighted the limitation of [\[12\]](#page-7-12), [\[14\]](#page-7-14),
    and [\[15\]](#page-7-15) when it comes to large GitHub projects, as results were
    not returned for the three larger projects.


    In Figure [2,](#page-4-2) we look into the bus factors for the most recently updated
    directories of PETSc and NWChem projects. We compare the bus factor values of
    the change-size-cos and LOCC-based CST algorithm with the RIG algorithm. It can
    be clearly seen that the results from the RIG algorithm are significantly different
    and higher than not only the CST algorithm but also the data provided by the principal
    developers of the projects. Hence, we found the RIG algorithm to be the worst
    performer.


    Moreover, we present comparative plots for the four CST algorithm metrics explained
    in Section [2.1](#page-1-2) and also the two data metrics change-size-cos and
    LOCC in Figure [3](#page-4-3) for Hypre and PETSc. It can be seen from the graphs
    that there is not a significant variation in the results for each combination.
    Given that information, we focused our study on the mul-changes-equal metric.
    However, as mentioned in [\[1\]](#page-7-0), a CST metric can be chosen as per
    the requirements and processes followed by an organization for version control.


    #### 4.2 Trend in bus factors over time


    In this subsection, we seek the answer to the second research question, i.e.,
    the role of the bus factor in guiding effort allocation towards hiring and knowledge
    transfer. The intention is to study the applicability of the bus factor and the
    guidance these results provide to principal developers and managers. Intuitively,
    the bus factor of a project or directory will indicate the concentration of knowledge
    within a certain number of people. If that number is small, then principal developers
    or project managers can work on effort allocation for hiring new people to work
    on that project. Another thing that can be done is to ensure knowledge transfer
    to other developers within the organization. For our study, we looked at the trend
    in bus factor over the past five years for the selected projects. Figure [4](#page-5-1)
    shows the trend in bus factor since 2018 for PETSc and Spack. The yellow bars
    represent the total number of developers; the left y-axis, along with solid lines,
    represents the bus factor value, while the right y-axis and the dashed lines are
    for the percentage of developers that are part of the bus factor. For both projects,
    we can see that the developers with higher knowledge of the projects are decreasing
    in later years, reaching the minimum value in the year 2022. Knowledge of this
    trend can help the principal developers or managers hire new people or initiate
    the knowledge transfer process to other existing developers for these projects.


    Sometimes, looking at the trend in bus factor for the whole project does not provide
    the best view of the knowledge concentration and the risk of the project in terms
    of employee turnover.


    <span id="page-6-0"></span>![](_page_6_Figure_0.jpeg)


    ![](_page_6_Figure_1.jpeg)


    ![](_page_6_Figure_2.jpeg)


    (c) Spack: **var/spack/**


    Figure 5: Trend in directory level bus factors for last five years for LOCC and
    change-size-cos based CST. The yellow bars represent the total number of developers.
    The dotted lines represent the percentage of BF developers from the total corresponding
    to the right y-axis.


    Given that, looking at individual directories or important parts of the projects
    is more helpful. Figure [5](#page-6-0) shows the trend in bus factor values for
    major directories of PETSc and Spack. With this narrowed-down view, a more informed
    decision can be taken for the important parts of the projects. Figure [5](#page-6-0)
    (c) particularly shows a steep drop in the bus factor value for var/spack/ directory
    of the project, which cannot be picked up from the holistic view shown in Figure
    [4](#page-5-1) (b).


    #### 4.3 Performance comparison of BF Algorithms


    This subsection focuses on answering the research question about the accuracy
    of results and the performance of the bus factor algorithms. For that, we wanted
    to look at each algorithm to compare and contrast results. Most of this question
    is answered for the CST algorithm in Section [4.1,](#page-4-4) where we compared
    the results from the CST algorithm with RIG using Figure [2,](#page-4-2) so here
    we focused on the RIG algorithm and its non-deterministic nature.


    <span id="page-6-1"></span>![](_page_6_Figure_9.jpeg)


    ![](_page_6_Figure_10.jpeg)


    As mentioned in Section [2.2,](#page-1-3) the RIG algorithm is non-deterministic,
    i.e., it does not produce the same results for different executions. We collected
    data from two runs for different directories of the five projects to see the variation
    in results by the RIG algorithm. In Figure [6,](#page-6-1) we show results for
    two of the projects, PETSc and NWChem. It can be seen that none of the executions
    resulted in the same result for any of the directories. The difference is significant
    for some, including petsc/src/vec in (a) and nwchem/src/ccd


    in (b). We also looked at the developer names resulting from the algorithm for
    each execution and did not find a common pattern of similarity among results,
    which further confirms the random nature of the RIG algorithm.


    The non-deterministic and random nature of the RIG algorithm automatically makes
    the CST algorithm a better choice. This is further corroborated by the responses
    of the principal developers, as shown in Table [3](#page-4-1) and the lower errors
    for the CST algorithm. Moreover, Rigby et al. themselves claim in [\[4\]](#page-7-3)
    that bus factor scenarios computed using loss percentages are unrealistic.


    #### 5 FUTURE WORK


    There are many future dimensions to this work. One way to extend this work is
    to look at the impact of different threshold values for the algorithms with the
    proposed data metrics. Since authors of both CST and RIG algorithm do not give
    reasoning behind the choice of all the chosen threshold and cutoff values, this
    can be an interesting study. The results can help in making a more informed choice
    behind these values with reasoning provided. Moreover, using a more data-driven
    approach for the validation of results with trained machine learning models can
    help in eliminating the threats to correctness of the validation process. The
    large amount of mined GitHub data can be used for training purposes.


    #### <span id="page-7-4"></span>6 CONCLUSION


    It is common for open-source projects to face the risk of key developers leaving
    the project. This risk is even higher with long-term and large projects. To judge
    the risk posed to the projects or the severity of it, the concentration of knowledge
    among its developers can be measured. This measurement is known as the bus factor
    of the project and there are several algorithms proposed for its calculation.
    Most of these algorithms use the commits data from version control systems. We
    proposed the use of two other metrics i.e. lines of code changes (LOCC) and cosine
    difference of lines of code (change-size-cos) and tested them with the CST algorithm.
    We also used a git-blame based RIG algorithm on our use-case of five High Performance
    Computing projects on GitHub. Along with complete projects, we also looked at
    the bus factor values of the major directories in each. We did a comparative study
    for the accuracy, similarity in results and trend in BF values on the two algorithms
    and also the metrics proposed by us. We validated our results from the principal
    developers of the selected projects which showed LOCC and change-size-cos to be
    more accurate than commits. Our implementation of the CST algorithm is scalable
    when compared to the online available tools by Cosentino at el. Lastly, we demonstrated
    with examples how looking at the trend in bus factor values over time can guide
    principal developers in effort allocation.


    #### REFERENCES


    - <span id="page-7-0"></span>[1] Valerio Cosentino, Javier Canovas Izquierdo,
    and Jordi Cabot. Assessing the bus factor of git repositories. 03 2015.

    - <span id="page-7-1"></span>[2] Mívian Ferreira, Marco Tulio Valente, and Kecia
    Ferreira. A comparison of three algorithms for computing truck factors. In 2017
    IEEE/ACM 25th International Conference on Program Comprehension (ICPC), pages
    207–217, 2017.

    - <span id="page-7-2"></span>[3] G. Avelino, L. Passos, A. Hora, and M. Valente.
    A novel approach for estimating truck factors. In 2016 IEEE 24th International
    Conference on Program Comprehension (ICPC), pages 1–10, Los Alamitos, CA, USA,
    may 2016. IEEE Computer Society.

    - <span id="page-7-3"></span>[4] Peter C. Rigby, Yue Cai Zhu, Samuel M. Donadelli,
    and Audris Mockus. Quantifying and mitigating turnover-induced knowledge loss:
    Case studies of chrome and a project at avaya. In Proceedings of the 38th International
    Conference on Software Engineering, ICSE ''16, page 1006–1016, New York, NY, USA,
    2016. Association for Computing Machinery.

    - <span id="page-7-8"></span><span id="page-7-7"></span>[5] Petsc project. Available
    online at: [https://github.com/petsc/petsc.](https://github.com/petsc/petsc)

    - <span id="page-7-9"></span>[6] Spack project. Available online at: [https://github.com/spack/spack.](https://github.com/spack/spack)

    - <span id="page-7-10"></span>[7] Hypre project. Available online at: [https://github.com/hypre-space/hypre.](https://github.com/hypre-space/hypre)

    - <span id="page-7-11"></span>[8] Lammps project. Available online at: [https://github.com/lammps/lammps.](https://github.com/lammps/lammps)
    [9] Nwchem project. Available online at: [https://github.com/nwchemgit/nwchem.](https://github.com/nwchemgit/nwchem)

    - <span id="page-7-5"></span>[10] Nico Zazworka, Kai Stapel, Eric Knauss, Forrest
    Shull, Victor R. Basili, and Kurt Schneider. Are developers complying with the
    process: An xp study. In Proceedings of the 2010 ACM-IEEE International Symposium
    on Empirical Software Engineering and Measurement, ESEM ''10, New York, NY, USA,
    2010. Association for Computing Machinery.

    - <span id="page-7-6"></span>[11] Filippo Ricca, Alessandro Marchetto, and Marco
    Torchiano. On the difficulty of computing the truck factor. volume 6759, pages
    337–351, 06 2011.

    - <span id="page-7-12"></span>[12] Valerio Cosentino, Javier Canovas Izquierdo,
    and Jordi Cabot. Gitana: A software project inspector. Science of Computer Programming,
    153, 12 2017.

    - <span id="page-7-14"></span><span id="page-7-13"></span>[13] SeatGeek Inc. fuzzywuzzy:
    Fuzzy String Matching in Python, 2014. [14] Valerio Cosentino, Javier Canovas
    Izquierdo, and Jordi Cabot. Gitana: a sqlbased project activity inspector. Available
    online at: [https://github.com/SOM-](https://github.com/SOM-Research/Gitana)[Research/Gitana.](https://github.com/SOM-Research/Gitana)

    - <span id="page-7-15"></span>[15] Valerio Cosentino. Bus factor analyzer. Available
    online at: [https://github.com/](https://github.com/SOM-Research/busfactor) [SOM-Research/busfactor.](https://github.com/SOM-Research/busfactor)'
  decisions:
    language: '- Qualified. Reason: English Paper'
    evaluation_prompt: Qualified.
    related_work_prompt: Qualified
    novelty_prompt: Qualified
    review_only_prompt: Qualified.
  llm_input_used: '## Abstract

    A critical issue faced by open-source software projects is the risk of key

    personnel leaving the project. This risk is exacerbated in large projects that

    have been under development for a long time and experienced growth in their

    development teams. One way to quantify this risk is to measure the

    concentration of knowledge about the project among its developers. Formally

    known as the Bus Factor (BF) of a project and defined as ''the number of key

    developers who would need to be incapacitated to make a project unable to

    proceed''. Most of the proposed algorithms for BF calculation measure a

    developer''s knowledge of a file based on the number of commits. In this work,

    we propose using other metrics like lines of code changes (LOCC) and cosine

    difference of lines of code (change-size-cos) to calculate the BF. We use these

    metrics for BF calculation for five open-source GitHub projects using the CST

    algorithm and the RIG algorithm, which is git-blame-based. Moreover, we

    calculate the BF on project sub-directories that have seen the most active

    development recently. Lastly, we compare the results of the two algorithms in

    accuracy, similarity in results, execution time, and trends in BF values over

    time.


    ## Introduction

    If we look at our daily use of the Internet, we realize that we are consumed with
    the use of software and applications. The development and maintenance of all these
    software projects are based on the knowledge held by its developers. This makes
    the software development process the most dependent upon its developers. Given
    that, developers become an important asset for project organizations and open-source
    project teams. This also makes the rate at which developers leave a software project
    a critical matter and risk. To mitigate this risk, it is important that project
    managers or principal developers monitor and quantify the concentration of knowledge
    of the project among its developers [\[2\]](#page-7-1) An interesting measurement
    used in this regard is known as the Bus Factor (BF), which is defined as the minimum
    number of key developers whose departure


    would make a project unable or difficult to proceed [\[2\]](#page-7-1) A smaller
    bus factor value would mean that the maximum knowledge of the project is concentrated
    among them and the project is at a higher risk if some or all of these developers
    leave the project, company or go for a vacation, etc. A bus factor of one would
    be the worst-case scenario. Conversely, a high bus factor means a lesser risk
    is posed to the project in case some of the developers end up leaving.


    When it comes to open-source software projects, the bus factor is of much more
    importance. Most of the people working on these projects are making contributions
    voluntarily. Volunteering means that the developers or contributors do not have
    financial benefits associated with these projects. This puts open-source software
    projects at a higher risk of developer turnover [\[2\]](#page-7-1). Given the
    importance of bus factor measurement in open-source software development projects,
    algorithms have been proposed to calculate it using data gathered from version
    control systems such as GitHub [\[1,](#page-7-0) [3,](#page-7-2) [4\]](#page-7-3).
    To the best of our knowledge, most of these algorithms are commit-based, i.e.,
    the algorithms look at the commit data from version control repositories. Moreover,
    existing work mostly proposes bus factor calculation algorithms and presents tools
    for the same. Few studies attempt to validate the results of these algorithms.
    [\[2\]](#page-7-1) is an empirical and comparative study where three Bus Factor
    algorithms are validated, but they only used the tools and metrics provided by
    the original authors.


    Therefore, in this paper, we look at two algorithms (a) first one was proposed
    by Cosentino et al. [\[2\]](#page-7-1), which calculates the bus factor of each
    file and aggregates it up to branch, directory, or project level; (b) second one
    proposed by Rigby et al. [\[4\]](#page-7-3), which uses a git-blame-based approach
    to calculate the bus factor. The tool based on the first algorithm is publicly
    available [\[1\]](#page-7-0); however, the authors have only provided the pseudo-code
    of the second. Using the convention followed in [\[2\]](#page-7-1), we refer to
    the first algorithm as CST and the second one as RIG. Among the two algorithms,
    CST is a commit-based algorithm, and we wanted to use it with other metrics like
    lines of code changes (LOCC) and cosine difference of lines of code (change-size-cos).
    For that, we implemented it by understanding the algorithm mentioned in [\[1\]](#page-7-0).
    RIG algorithm had to be implemented as its code, or any related tool is not publicly
    available. We implemented and tested both algorithms on five opensource projects
    and compared their results. We also got feedback from the principal developers
    of these projects to validate the results. Lastly, we used the tool provided in
    [\[1\]](#page-7-0) to obtain bus factor results for the selected projects.


    To summarize, we seek to answer the following research questions:


    - (1) How do different metrics such as lines of changed code and cosine difference
    affect the bus factor computation?

    - (2) How can the bus factor guide principal developers or managers to allocate
    effort for hiring or knowledge transfer among the existing team of developers?

    - (3) How do bus factor algorithms differ in terms of accuracy and performance?


    As such, our contributions can be summarized as follows:


    - Implemented the CST algorithm (with lines of code changes and cosine difference
    of lines of code) and the RIG algorithm and compared their results.

    - Got feedback from the principal developers of the five selected projects to
    validate our results.

    - Calculated the BF of the selected projects using the commitbased CST algorithm
    tool provided in [\[1\]](#page-7-0) to compare with our results.

    - Calculated the bus factor for the selected projects for the last five years
    to see the trend in BF values.


    In Section [2,](#page-1-0) we review the existing literature for bus factor calculation.
    Section [3](#page-2-0) explains the design and steps involved in our study. We
    present our results in Section [4](#page-4-0) and conclude the paper in Section
    [6.](#page-7-4)'
  token_usage: 6255
  time_usage: 1.8193295001983643
- title: "T-FREX: A Transformer-based Feature Extraction Method from Mobile App\n\
    \  Reviews"
  abstract: 'Mobile app reviews are a large-scale data source for software-related

    knowledge generation activities, including software maintenance, evolution and

    feedback analysis. Effective extraction of features (i.e., functionalities or

    characteristics) from these reviews is key to support analysis on the

    acceptance of these features, identification of relevant new feature requests

    and prioritization of feature development, among others. Traditional methods

    focus on syntactic pattern-based approaches, typically context-agnostic,

    evaluated on a closed set of apps, difficult to replicate and limited to a

    reduced set and domain of apps. Meanwhile, the pervasiveness of Large Language

    Models (LLMs) based on the Transformer architecture in software engineering

    tasks lays the groundwork for empirical evaluation of the performance of these

    models to support feature extraction. In this study, we present T-FREX, a

    Transformer-based, fully automatic approach for mobile app review feature

    extraction. First, we collect a set of ground truth features from users in a

    real crowdsourced software recommendation platform and transfer them

    automatically into a dataset of app reviews. Then, we use this newly created

    dataset to fine-tune multiple LLMs on a named entity recognition task under

    different data configurations. We assess the performance of T-FREX with respect

    to this ground truth, and we complement our analysis by comparing T-FREX with
    a

    baseline method from the field. Finally, we assess the quality of new features

    predicted by T-FREX through an external human evaluation. Results show that

    T-FREX outperforms on average the traditional syntactic-based method,

    especially when discovering new features from a domain for which the model has

    been fine-tuned.'
  url: http://arxiv.org/abs/2401.03833v1
  keywords: feature extraction, mobile apps, reviews, token classification, named
    entity recognition, large language models
  document: '# T-FREX: A Transformer-based Feature Extraction Method from Mobile App
    Reviews


    Quim Motger<sup>1</sup> , Alessio Miaschi<sup>2</sup> , Felice Dell''Orletta<sup>2</sup>
    , Xavier Franch<sup>1</sup> , Jordi Marco<sup>1</sup>


    <sup>1</sup>Universitat Politècnica de Catalunya, Barcelona


    {quim.motger,xavier.franch,jordi.marco}@upc.edu


    2 Institute for Computational Linguistics "A. Zampolli" (CNR-ILC), ItaliaNLP Lab,
    Pisa


    {alessio.miaschi,felice.dellorletta}@ilc.cnr.it


    *Abstract*—Mobile app reviews are a large-scale data source for software-related
    knowledge generation activities, including software maintenance, evolution and
    feedback analysis. Effective extraction of features (i.e., functionalities or
    characteristics) from these reviews is key to support analysis on the acceptance
    of these features, identification of relevant new feature requests and prioritization
    of feature development, among others. Traditional methods focus on syntactic pattern-based
    approaches, typically context-agnostic, evaluated on a closed set of apps, difficult
    to replicate and limited to a reduced set and domain of apps. Meanwhile, the pervasiveness
    of Large Language Models (LLMs) based on the Transformer architecture in software
    engineering tasks lays the groundwork for empirical evaluation of the performance
    of these models to support feature extraction. In this study, we present T-FREX,
    a Transformer-based, fully automatic approach for mobile app review feature extraction.
    First, we collect a set of ground truth features from users in a real crowdsourced
    software recommendation platform and transfer them automatically into a dataset
    of app reviews. Then, we use this newly created dataset to fine-tune multiple
    LLMs on a named entity recognition task under different data configurations. We
    assess the performance of T-FREX with respect to this ground truth, and we complement
    our analysis by comparing T-FREX with a baseline method from the field. Finally,
    we assess the quality of new features predicted by T-FREX through an external
    human evaluation. Results show that T-FREX outperforms on average the traditional
    syntacticbased method, especially when discovering new features from a domain
    for which the model has been fine-tuned.


    *Index Terms*—feature extraction, mobile apps, reviews, token classification,
    named entity recognition, large language models


    ## I. INTRODUCTION


    Mobile app repositories provide valuable access to timely large-scale datasets
    of software-related information [\[1\]](#page-10-0). These repositories include
    heterogeneous, multiple-purpose platforms, from app stores to sideloading repositories
    and search engines [\[2\]](#page-10-1). One of the most popular contributions
    across these platforms is the publication of app reviews, in which users express
    multiple facets such as personal opinions or experiences, bug reports, inquiries
    or requests [\[3\]](#page-10-2). This information is relevant to multiple software
    engineering processes, including requirements elicitation and prioritization,
    release planning, validation analysis and software evolution [\[3\]](#page-10-2)–[\[7\]](#page-10-3).


    App features are considered a core descriptor for understanding and categorizing
    app reviews [\[8\]](#page-10-4)–[\[10\]](#page-10-5). In this context, a feature
    is considered as a distinct function or capability within a mobile application
    serving a particular purpose or need [\[11\]](#page-10-6). App feature extraction
    supports featurerelated knowledge generation, in which mobile app developers can
    potentially rely on to improve user experience, enhance app functionality, identify
    user preferences, and make datadriven decisions for app development strategies
    [\[3\]](#page-10-2), [\[12\]](#page-10-7), [\[13\]](#page-10-8). Consequently,
    mining large amounts of app reviews to extract app features has become a relevant
    task. Nevertheless, mining features from app reviews presents particular challenges.
    It requires the daily analysis of thousands of short documents, each with a limited
    length, composed of an average of a few dozen words per review [\[14\]](#page-10-9).
    Beyond measurable characteristics, user-generated documents tend to present multiple
    informal writing styles and vocabulary, including misspelt words, repetitions
    or cross-language terminology [\[15\]](#page-10-10), polarized or biased information,
    or even noisy and spam content [\[16\]](#page-10-11).


    Consolidated approaches rely on syntactic pattern-matching techniques to retrieve
    features from app descriptions and reviews [\[11\]](#page-10-6). Nevertheless,
    several challenges emerge from their applicability, including limited replicability,
    unavailability of data and a lack of user evaluation [\[11\]](#page-10-6). Furthermore,
    rule-based strategies for knowledge generation can be brittle to identify complex
    patterns, domain-specific terminology, unexpected contents and contextual knowledge,
    which affects the generalization of these techniques [\[17\]](#page-10-12). To
    overcome these challenges, deep learning strategies, and in particular Large Language
    Models (LLMs) based on the Transformer architecture [\[18\]](#page-10-13), have
    shown promising results in multiple software-related data mining tasks. These
    approaches leverage the knowledge embedded in these pre-trained models by extending
    their capabilities through task-specific supervised fine-tuning tasks such as
    sentiment analysis, text classification or named-entity recognition (NER) [\[19\]](#page-10-14)–[\[22\]](#page-10-15).


    In this paper, we present T-FREX (*Transformer-based FeatuRe EXtraction*), a novel
    approach to support feature extraction from app reviews using LLMs. Our proposal
    redefines the app feature extraction problem as a NER task, a specific type of
    token classification in which tokens referring to a particular entity type (e.g.
    dates, geopolitical entities, features, etc.) are labelled as such. Our main contributions
    are[1](#page-0-0) : (1) a


    <span id="page-0-0"></span><sup>1</sup>GitHub repository:<https://github.com/gessi-chatbots/t-frex/>


    | Telegram Features                                                                       |
    Suggest and vote on featu                      |

    |-----------------------------------------------------------------------------------------|------------------------------------------------|

    | Lightweight<br>Telegram consumes less device resources compared to similar apps.        |                                                |

    | Ad-free<br>0<br>Telegram doesn''t contain any form of external advertising.              |                                                |

    | · End-to-End Encryption<br>Telegram has E2E Encryption, for entire or parts
    of the app. |                                                |

    | Dark Mode<br>Telegram supports dark mode for comfortable usage in low light
    conditions. |                                                |

    | Portable<br>Support for Themes<br>IFTTT Integration                                     |
    Self Destructing Messages<br>Works Offline     |

    | Cloud Sync   Multiple Account support   VoiP Calls<br>Two-factor Authentication         |
    Encrypted Chat                                 |

    | Stickers Chat Bot<br>Large File Transfer<br>Multi Device Support                        |
    Instant Messaging<br>Bots                      |

    | Video Calling<br>Channels<br>Share Videos<br>Secret chats                               |
    Cloud based<br>Animated stickers               |

    | Persistent History<br>Integrated File Sharing                                           |
    Folders Video Conferencing<br>Security focused |


    <span id="page-1-0"></span>Fig. 1. Sample of crowdsourced user annotated features
    in a software recommendation platform (https://alternativeto.net/) for the Telegram
    app.


    Transformer-based, fully automatic approach for the extraction of mobile app features
    from user reviews; (2) an extensive evaluation of the performance of multiple
    Transformer-based LLMs [\[18\]](#page-10-13) in different classification scenarios;
    (3) a reusable fine-tuned model and a ground-truth dataset of annotated app reviews,
    based on crowdsourced annotated app features extracted from a popular software
    recommendation platform and automatically transferred into the corpus of app reviews.


    ## II. BACKGROUND


    ## <span id="page-1-4"></span><span id="page-1-3"></span>*A. Mobile app features*


    There are multiple definitions of the term *feature* within related literature
    according to different dimensions: *(i)* scope: definitions refer to functionalities
    [\[11\]](#page-10-6) (e.g., *send message*), quality aspects [\[23\]](#page-10-16)
    (e.g., *lightweight*) or both [\[24\]](#page-10-17); *(ii)* abstraction: feature
    expressions vary from generic, high-level categories (e.g., *communication*) to
    specific, actionable aspects (e.g., *integrated file sharing*) [\[13\]](#page-10-8);
    *(iii)* formalization: definitions range from a particular focus on the requirements
    engineering field, using terms like *logically related system capabilities* and
    *set of functional requirements* [\[25\]](#page-10-18), to a more user-oriented
    perspective, referring to a *property* [\[26\]](#page-10-19) or *characteristic*
    [\[27\]](#page-10-20) of a mobile app. In the context of grey literature and industrial
    applications, both functional and quality *features* with different levels of
    abstraction and formalization are often presented as descriptors at the same hierarchical
    level. Figure [1](#page-1-0) illustrates the set of crowdsourced user annotated
    features (i.e., collaboratively labelled by multiple users) for a given mobile
    app in a software recommendation platform, for which we find different examples
    in terms of scope (e.g., *portable* vs. *instant messaging*), abstraction (e.g.,
    *channels* vs. *share videos*) and formalization (e.g., *file sharing* vs. *send
    file*).


    To accommodate our research to both scientific and industrial applications, in
    this research, we define a feature as a distinct functionality or capability within
    a mobile application that serves a particular purpose or provides a specific benefit
    to the user. It is a functional component or attribute of the software designed
    to perform a well-defined task or address a particular user need, enhancing the
    utility of the app.


    ## <span id="page-1-2"></span>*B. NER using LLMs*


    Our proposal is based on redefining feature extraction as a NER task, one of the
    most common token classification tasks in the context of natural language understanding
    (NLU) and for which LLMs have been largely used in the past few years [\[28\]](#page-10-21).
    Given a set of app reviews R of size n, the tokenized representation of a given
    r<sup>i</sup> ∈ R is expressed as T(ri) = [ti1, ti2, ..., tim], where each tij
    ∈ T(ri) represents a token from the original review r<sup>i</sup> . The *feature
    extraction* task consists in identifying sequences of contiguous tokens Tif ⊆
    T(ri) composing the written expression of a feature. Consequently, features are
    extracted within the context of a particular app review. This context dimension
    is key, as a particular sequence of tokens T<sup>f</sup> might refer to a feature
    within a given review for a given app, but the same sequence T<sup>f</sup> might
    not refer to a feature in another context. To this end, the dynamic attention
    mechanism enables LLMs to attend to crucial contextual information within the
    context of a review from a mobile app of a specific category. For example, in
    the review sentence "*I find that managing my channels is quite frustrating*",
    the word *channels* refers to an actual feature of a communication mobile app.
    Contrarily, in the review sentence "*This art app offers diverse channels for
    unleashing creativity*", which belongs to a mobile app from the arts and design
    category, *channels* is not an actual feature, despite having the same Part-of-Speech
    (PoS) tag (NOUN) and syntactic dependency role (direct object). Traditional syntactic-based
    approaches lack the potential to determine how contextual information suits a
    more fine-grained selection of features.


    In token-based classification tasks, the input can range from a sentence to a
    paragraph or even an entire document. Our approach is defined at the sentence
    level, allowing for more granular analysis and facilitating efficient processing.
    Therefore, tokenization is refined by splitting the reviews into sentences, T(ri)
    = [si1, si2, ..., sip], where each sentence is a subsequence of the original tokenization,
    sik ⊆ T(ri). Given a sentence input sik, a token classification model assigns
    one unique label to each token, indicating whether the token is the beginning
    of a feature named entity (*B-feature*), an internally contained element of a
    feature entity (*I-feature*), or none (*O*). Figure [2](#page-1-1) illustrates
    the NER output on a mobile app review. For simplicity, this example is architecture-agnostic,
    meaning that we consider each word from the original review individually, ignoring
    special tokens or multiple tokens referring to the same word. While numerous models
    are publicly available for generic types of NER (e.g., dates, locations, persons,
    e-mail addresses...) and for some specific domains (e.g. medical and legal domains)
    [\[29\]](#page-10-22)–[\[31\]](#page-10-23), to the best of our knowledge there
    are no proposals for the identification of app feature entities exploiting fine-tuned
    LLMs.


    <span id="page-1-1"></span>Fig. 2. Example of NER task on a mobile app review.


    ## III. RESEARCH METHOD


    <span id="page-2-2"></span>Our research aims to demonstrate the hypothesis that
    Transformer-based LLMs can significantly enhance mobile app feature extraction
    tasks by redefining this process as a token classification problem. Consequently,
    we conducted a sample study as defined by Stol and Fitzgerald [\[32\]](#page-10-24),
    which aims at maximizing the generalization of the feature extraction task over
    the population of mobile apps publicly available in mobile app repositories. To
    this end, we leverage crowdsourced annotated data from actual mobile app users,
    using AlternativeTo[2](#page-2-0) as the main source for ground-truth knowledge
    generation, as a relevant representative of search engines in the context of mobile
    app repositories [\[33\]](#page-10-25). This platform provides, for each mobile
    app, a list of features which have been suggested and ranked by real users, while
    they are being used for navigating the catalogue of mobile applications (as illustrated
    previously in Figure [1\)](#page-1-0). More details on the data collection and
    annotation processes are depicted in Section [IV-A.](#page-2-1)


    To assess the validity of T-FREX, we guide our research through the following
    research questions:


    RQ1) What is the effectiveness of T-FREX using different LLMs with respect to
    crowdsourced user-annotated features? RQ2) What is the effectiveness of T-FREX
    compared to traditional feature extraction methods (i.e., SAFE approach)? RQ3)
    What is the effectiveness of T-FREX with respect to new, undocumented features?


    RQ1 is defined to assess the effectiveness of our approach in terms of functional
    suitability. We present the design and development results of an end-to-end pipeline
    for finetuning and using different types of LLMs under different data configurations.
    This analysis will allow us to gain a deeper understanding of how different LLM
    architectures behave under different training datasets. Moreover, RQ1 provides
    token-level empirical evaluation results for the overall quality of the NER (i.e.
    token classification) task.


    RQ2 is intended to compare T-FREX performance at feature level with respect to
    a standard baseline method in the field of feature extraction. We selected the
    SAFE approach as a baseline for a comparative analysis [\[34\]](#page-10-26),
    as it is considered the most consolidated approach for mobile app feature extraction
    in the software engineering field (see Section [VII](#page-9-0) for more details
    on related work and comparison with other approaches). Given that the original
    study does not provide a publicly available replication package, we built on the
    work of Shahe et al. [\[17\]](#page-10-12), who conducted a replication study
    and distributed a replicated development of the SAFE approach.


    Finally, RQ3 is designed to analyse how T-FREX generalizes and overcomes the constraints
    of limited, domain-specific datasets. While the use of data generated and consumed
    by real users offers multiple advantages, we have no control in the extent and
    representativeness with respect to the complete set of features exposed by mobile
    apps. This entails that our model might predict features that are not included
    in the ground truth (i.e., *false positive*), while this might simply relate to
    incompleteness of user annotated data. Hence, we propose to overcome this limitation
    while also gaining deeper insights on generalization of our model by conducting
    a human evaluation on new features predicted by our model (i.e., features not
    included in the ground truth dataset).


    ## IV. DESIGN


    Figure [3](#page-3-0) shows an overview of our research. We elaborate the details
    in the upcoming subsections.


    ## <span id="page-2-1"></span>*A. Data Collection and Annotation*


    While there is related work publishing gold datasets with expert feature annotations
    [\[11\]](#page-10-6), our approach is intended to leverage real user crowdsourced
    annotation, as well as to assess its generalization and applicability in real,
    practical uses cases. However, published datasets are typically internally annotated
    by human coders, very limited in terms of number of applications (e.g., between
    8-10 mobile apps) and domains, and focused on productivity and communication apps,
    excluding more expert, domain-specific categories like navigation, sports or weather
    [\[11\]](#page-10-6), [\[13\]](#page-10-8), [\[17\]](#page-10-12), [\[34\]](#page-10-26),
    [\[35\]](#page-10-27). Therefore, we opted to build our own dataset of mobile
    app reviews with annotated features generated by real users. We built on the work
    of Motger et al. [\[33\]](#page-10-25), who collected and published a sample dataset
    of 639 mobile apps with 622,370 reviews from multiple categories, to which we
    applied the following extensions:


    - We extended the mobile app metadata with the official category from Google Play
    as gold knowledge for the category-based analysis of the feature extraction task.
    The original dataset included custom defined categories based on a keyword-based
    search of domain-related terms. Instead, we propose to use the taxonomy of categories
    defined by Google Play [\[36\]](#page-10-28), which is considered as the largest,
    most relevant mobile app store worldwide [\[2\]](#page-10-1).

    - We limited the mobile apps included in our study to the 10 most frequent Google
    Play categories in the original dataset in number of mobile apps, excluding those
    categories with a minimal representation (≤ 5 apps) to ensure relevant statistical
    inference. Exceptionally, we excluded "GAME" related categories, considered as
    a special kind of mobile apps with a different *feature* conception [\[11\]](#page-10-6).

    - We extended the annotated features for a given mobile app using AlternativeTo
    features as ground truth. These features are voted by logged-in users of the software
    recommendation platform, and they are ranked and sorted by absolute number of
    votes. To obtain this data, we reused the web scraping data collection mechanisms
    originally developed by Motger et al. [\[33\]](#page-10-25).


    Table [I](#page-4-0) summarizes the resulting dataset distributed according to
    the Google Play category to which the apps belong to. It is important to highlight
    that multiple distinct features might belong to multiple categories (e.g., *video
    call* is a feature for both SOCIAL and COMMUNICATION mobile apps). Notice that
    the last row refers to the total number


    <span id="page-2-0"></span><sup>2</sup>https://alternativeto.net/


    ![](_page_3_Figure_0.jpeg)


    <span id="page-3-0"></span>Fig. 3. Research design overview.


    of feature annotations in the complete corpus of annotated reviews, which we explain
    in more detail in Section [IV-B.](#page-3-1)


    ## <span id="page-3-1"></span>*B. Data Pre-processing and Feature Transfer*


    Let F = {f1, f2, ..., fq} be the set of crowdsourced feature annotations defined
    at app entity level. To train and evaluate our model, these features are transferred
    into the corpus of app reviews R. This results in the annotation of all tokens
    t ∈ T(r) for each review r ∈ R with the corresponding name entity label L = {*O*,
    *B-feature*, *I-feature*}. To this end, we used Stanza''s neural pipeline [\[37\]](#page-10-29)
    to pre-process and transform both corpus F and R into their correspondent CoNLL-U
    format representation [\[38\]](#page-10-30), which includes for each token t a
    list of syntactic and morphological features. We use this format to facilitate
    replicability of our approach and reusability of the resulting dataset. Specifically,
    the pre-processing pipeline included: (*i*) tokenization, (*ii*) multi-word token
    expansion, (*iii*) PoS tagging, (*iv*) morphological feature extraction, and (*v*)
    lemmatization. Feature transfer is then applied to exact matches between the CoNLL
    representation of a given feature f and a contiguous sequence of tokens of a given
    review r so that f ⊆ T(r) after pre-processing r and f. Given that not all features
    f ∈ F relate to actual features in different contexts, we scoped this label transfer
    to features originally extracted from the same application to which the review
    belonged to. This means that, for ground truth generation, the example used in
    Section [II-B](#page-1-2) for context-dependent features (i.e., use of *channels*
    as a feature from communication apps) is not considered as an actual feature in
    the context of a different app (i.e., the arts and design app). As a result, the
    CoNLL-U representation of R is extended with an additional annotation for each
    token t, represented by one of the labels in L = {*O*, *B-feature*, *I-feature*}.


    Table [I](#page-4-0) reports the total amount of feature annotations transferred
    into the corpus of reviews R (last row).


    ## *C. Model Fine-tuning*


    The pre-processed and annotated corpus R serves as the primary input for training
    various LLMs under diverse data configurations. In this section, we elaborate
    on the reasoning behind our choices regarding the selection, preparation, and
    training of these models.


    <span id="page-3-2"></span>*1) Model Selection:* State-of-the-art LLMs encompass
    multiple architectures (e.g., encoder-only, decoder-only, encoderdecoder), modelling
    paradigms (e.g., discriminative, generative), pre-training tasks (e.g., masked
    language modelling or MLM, permutation language modelling), size and scale, among
    other descriptors [\[39\]](#page-11-0). Appropriate model selection is not trivial
    and is often neglected or undermined. For our experiments, we opted for decoder-only
    models, due to their better suitability for handling classification tasks. Moreover,
    we avoided testing large-scale generative models due to their considerable dimensions
    and, therefore, their practical limitations in terms of memory and time constraints.
    Below we provide the selection of LLM for our research, including those features
    suited for our task and their role in the evaluation.


    - BERT, one of the first groundbreaking LLMs, is celebrated for its bidirectional
    nuanced contextual understanding [\[40\]](#page-11-1). Trained on a vast corpus
    using MLM as pre-training objective, it excels in capturing context from both
    left and right, empowering it for diverse token-level tasks [\[41\]](#page-11-2).
    Consequently, we select BERT as a baseline for the use of LLMs in the context
    of feature extraction.

    - RoBERTa is considered a refinement on BERT''s architecture and training process
    [\[42\]](#page-11-3). It achieves heightened performance through extended pre-training
    on a larger dataset and augmented data, resulting in more robust language representations.
    It also uses MLM for pretraining and outperforms BERT in various scenarios [\[42\]](#page-11-3),
    making it a valuable addition to our model evaluation.

    - XLNet combines autoregressive and bidirectional training by considering all
    possible permutations of a sentence''s words during pre-training [\[43\]](#page-11-4).
    This methodology fosters improved contextual understanding and dependency modelling
    among tokens, surpassing the conventional models. Unlike BERT and RoBERTa, XLNet
    uses a permutation-based training objective, allowing it to model token dependencies
    differently.


    For each of these models, we consider both base and large versions (i.e., in terms
    of number of model parameters).


    <span id="page-3-3"></span>*2) Data Preparation:* We split the dataset of annotated
    reviews (reported in Table [I\)](#page-4-0) under different data configurations
    to support different analytical perspectives.


    • Out-of-domain. The original dataset is split according to the category to which
    the app review belongs to. We then use these data partitions to run 10 different
    fine-tuning


    <span id="page-4-0"></span>TABLE I DISTRIBUTION OF MOBILE APPS, REVIEWS AND FEATURES
    IN THE DATASET, SORTED BY DECREASING ORDER BY NUMBER OF DISTINCT FEATURES. CATEGORY
    ABBREVIATIONS REFER TO: PRODUCTIVITY (PROD.), COMMUNICATION (COMM.), PERSONALIZATION
    (PERS.).


    | Metric                | PROD.   | COMM.   | TOOLS  | SOCIAL | HEALTH | PERS.
    | TRAVEL | MAPS  | LIFESTYLE | WEATHER | ALL     |

    |-----------------------|---------|---------|--------|--------|--------|-------|--------|-------|-----------|---------|---------|

    | #apps                 | 137     | 51      | 58     | 14     | 75     | 6     |
    19     | 31    | 12        | 65      | 468     |

    | #reviews              | 7,348   | 7,003   | 4,321  | 819    | 2,154  | 112   |
    530    | 284   | 344       | 901     | 23,816  |

    | #sentences            | 8,604   | 8,135   | 5,402  | 899    | 2,330  | 118   |
    602    | 315   | 391       | 984     | 27,780  |

    | #tokens               | 148,172 | 134,833 | 93,395 | 15,597 | 40,907 | 2,022
    | 11,105 | 5,868 | 8,044     | 15,439  | 475,382 |

    | #features (distinct)  | 77      | 54      | 50     | 26     | 23     | 19    |
    17     | 12    | 10        | 7       | 198     |

    | #features (annotated) | 9,866   | 9,800   | 6,626  | 1,049  | 2,524  | 127   |
    662    | 333   | 419       | 1,037   | 32,443  |


    processes, using 9 out of 10 categories for training the model and using the remaining
    category for testing. This setup evaluates the model''s capacity to generalize
    feature extraction to unfamiliar, new app domains.


    • In-domain. The original dataset is split under a 10-fold cross-validation setup
    with a balanced distribution of app reviews from each category, focusing on evaluating
    the model''s performance when predicting features within its domain of expertise.
    This setup assesses the model''s proficiency in feature extraction for categories
    closely aligned with its training data.


    We exclude from all training sets all references to features included in its corresponding
    testing set. This allows evaluation of the model''s performance to recognize tokens
    (extract features) for which it was not specifically fine-tuned.


    *3) Training Configuration:* For each model (Section [IV-C1\)](#page-3-2) and
    data setting (Section [IV-C2\)](#page-3-3), we configure and run a token classification
    fine-tuning process. First, we implement the preprocessing stage, which includes
    using a proper tokenizer according to the model architecture. BERT uses WordPiece
    tokenization and introduces [CLS] and [SEP] tokens for classification and separation.
    In contrast, RoBERTa and XLNet utilize SentencePiece tokenization, and they use
    only [SEP] tokens for separation while omitting the [CLS] token. Additionally,
    RoBERTa and XLNet employ a more aggressive subword tokenization approach, capturing
    finer linguistic details by breaking words into smaller subword units. This implies
    that a single word in the original review might be transformed into multiple tokens,
    which also affects the performance analysis and accuracy evaluation of the token
    classification (and ultimately, feature extraction) method. Second, we define
    the evaluation method for reporting and computing quality metrics (see Section
    [IV-D\)](#page-4-1). Third, in order to adjust the experiments to the available
    computational resources and model characteristics, we define the training parameters
    for each fine-tuning process (available in the replication package). Finally,
    the output of each fine-tuning process (including checkpoints, predictions and
    quality metrics) for the best performing checkpoint (i.e., with the lowest evaluation
    loss) are saved and reported.


    ## <span id="page-4-1"></span>*D. Evaluation design*


    We structure evaluation results in alignment with the formulation of research
    questions (Section [III\)](#page-2-2). In this section, we focus on the design
    of the evaluation plan.


    *1) Token-based ground-truth (RQ1):* Each fine-tuning process depicted in Section
    [IV-C2](#page-3-3) uses a token-level evaluation method for computing quality
    metrics for token prediction. This implies that results evaluate the model quality
    to predict whether a specific token is the beginning of a feature expression (*B-feature*),
    the inner part of a feature expression (*I-feature*) or none of the above (*O*).


    *2) Baseline feature extraction (RQ2):* Each fine-tuning process depicted in Section
    [IV-C2](#page-3-3) uses a feature-level evaluation method for computing quality
    metrics for feature extraction. Consequently, instead of computing prediction
    quality at token level, in this stage we evaluate the quality prediction of complete
    sequences of tokens T<sup>f</sup> composing a whole feature according to the ground-truth
    data set. We compare the performance of our approach with respect to the baseline
    method selected for feature extraction (i.e., SAFE [\[34\]](#page-10-26)).


    *3) New features (RQ3):* We select the best performing model to collect all new
    features predicted by our model. These features are then submitted to a human
    evaluation process to measure the prediction quality of new features. The human
    evaluation is composed of three main stages:


    - Data preparation. We collect all features predicted by the best-performing model
    (based on RQ1 and RQ2) for each test set under each data configuration scenario,
    as depicted in Section [IV-C2.](#page-3-3) We then remove all features included
    in the complete ground-truth annotated dataset, keeping exclusively newly reported
    features.

    - Set up. We iteratively elaborate and refine the guidelines, the selection of
    examples and the definition of feature annotation tasks. A task is defined as
    a sub set of review sentences, each one of them with a potential feature candidate
    which the annotator can either confirm (*Yes*), reject (*No*), or mark as not
    clear (*I don''t know*). Figure [4](#page-4-2) shows an example of a feature annotation
    question.


    | App name: Boosted Time Tracker<br>App category: PRODUCTIVITY                                                           |

    |------------------------------------------------------------------------------------------------------------------------|

    | Review: Timer is not included as a free feature but the app is very helpful
    .                                          |

    | Is the following expression mentioned as a reference to a feature of the<br>mobile
    application in the previous review? |

    | Feature: timer                                                                                                         |

    | O<br>Yes                                                                                                               |

    | O<br>No                                                                                                                |

    | I don''t know                                                                                                           |


    <span id="page-4-2"></span>Fig. 4. Example of a feature annotation question for
    human evaluation.


    <span id="page-5-0"></span>


    | Analysis      | Category  | Metric    | BERTbase | BERTlarge | RoBERTabase |
    RoBERTalarge | XLNetbase | XLNetlarge |

    |---------------|-----------|-----------|----------|-----------|-------------|--------------|-----------|------------|

    |               |           | precision | 0.799    | 0.734     | 0.539       |
    0.287        | 0.582     | 0.687      |

    |               | PROD.     | recall    | 0.343    | 0.320     | 0.244       |
    0.062        | 0.330     | 0.331      |

    |               |           | F1        | 0.480    | 0.445     | 0.335       |
    0.102        | 0.421     | 0.447      |

    |               |           | precision | 0.407    | 0.502     | 0.455       |
    0.384        | 0.438     | 0.412      |

    |               | COMM.     | recall    | 0.156    | 0.202     | 0.173       |
    0.276        | 0.261     | 0.317      |

    |               |           | F1        | 0.225    | 0.288     | 0.251       |
    0.321        | 0.327     | 0.358      |

    |               |           | precision | 0.513    | 0.570     | 0.462       |
    0.221        | 0.423     | 0.214      |

    |               | TOOLS     | recall    | 0.085    | 0.138     | 0.102       |
    0.065        | 0.204     | 0.026      |

    |               |           | F1        | 0.145    | 0.222     | 0.167       |
    0.100        | 0.275     | 0.046      |

    |               |           | precision | 0.606    | 0.696     | 0.621       |
    0.610        | 0.734     | 0.688      |

    |               | SOCIAL    | recall    | 0.513    | 0.410     | 0.462       |
    0.462        | 0.603     | 0.679      |

    |               |           | F1        | 0.556    | 0.516     | 0.529       |
    0.526        | 0.662     | 0.684      |

    |               |           | precision | 0.482    | 0.503     | 0.658       |
    0.584        | 0.710     | 0.663      |

    |               | HEALTH    | recall    | 0.179    | 0.240     | 0.127       |
    0.224        | 0.373     | 0.384      |

    | Out-of-domain |           | F1        | 0.261    | 0.325     | 0.213       |
    0.323        | 0.489     | 0.486      |

    |               | PERS.     | precision | 0.731    | 0.955     | 0.933       |
    0.973        | 0.972     | 1.000      |

    |               |           | recall    | 0.500    | 0.553     | 0.737       |
    0.947        | 0.921     | 0.684      |

    |               |           | F1        | 0.594    | 0.700     | 0.824       |
    0.960        | 0.946     | 0.813      |

    |               |           | precision | 0.773    | 0.647     | 0.720       |
    0.682        | 0.481     | 0.613      |

    |               | TRAVEL    | recall    | 0.708    | 0.458     | 0.750       |
    0.625        | 0.542     | 0.792      |

    |               |           | F1        | 0.739    | 0.537     | 0.735       |
    0.652        | 0.510     | 0.691      |

    |               |           | precision | 0.029    | 0.120     | 0.045       |
    0.077        | 0.560     | 0.467      |

    |               | MAPS      | recall    | 0.021    | 0.063     | 0.063       |
    0.063        | 0.292     | 0.146      |

    |               |           | F1        | 0.024    | 0.082     | 0.053       |
    0.069        | 0.384     | 0.222      |

    |               |           | precision | 0.500    | 0.400     | 0.600       |
    0.600        | 0.800     | 1.000      |

    |               | LIFESTYLE | recall    | 0.400    | 0.400     | 0.600       |
    0.600        | 0.800     | 0.200      |

    |               |           | F1        | 0.444    | 0.400     | 0.600       |
    0.600        | 0.800     | 0.333      |

    |               |           | precision | 0.619    | 0.642     | 0.273       |
    0.129        | 0.571     | 0.769      |

    |               | WEATHER   | recall    | 0.232    | 0.607     | 0.107       |
    0.071        | 0.500     | 0.179      |

    |               |           | F1        | 0.338    | 0.624     | 0.154       |
    0.092        | 0.533     | 0.290      |

    |               |           | precision | 0.546    | 0.577     | 0.531       |
    0.455        | 0.627     | 0.651      |

    |               | Average   | recall    | 0.314    | 0.339     | 0.336       |
    0.339        | 0.482     | 0.374      |

    |               |           | F1        | 0.381    | 0.414     | 0.386       |
    0.374        | 0.535     | 0.437      |

    |               |           | precision | 0.596    | 0.719     | 0.668       |
    0.688        | 0.679     | 0.761      |

    | In-domain     | Average   | recall    | 0.488    | 0.582     | 0.569       |
    0.509        | 0.519     | 0.573      |

    |               |           | F1        | 0.532    | 0.637     | 0.611       |
    0.571        | 0.582     | 0.646      |


    TABLE II TOKEN CLASSIFICATION EVALUATION RESULTS.


    This includes: app name, link to Google Play (for app context), category, review
    sentence, question and feature candidate. In this stage, we used a test task which
    is sequentially annotated by internal members of this research study, until an
    acceptable agreement is reached. After each annotation process, the collected
    feedback is used for refining the guidelines and list of examples used for designing
    the evaluation task.


    • Evaluation. The full data set of new features is submitted for human evaluation
    through sequential iterations in different batches. We used Prolific [\[44\]](#page-11-5)
    as a crowdsourced annotation platform to reach users worldwide and Quest-Base
    [\[45\]](#page-11-6) for the creation of the tasks. Each annotator is limited
    to participate in a single task. For each task, we include a subset of 5 control
    questions using ground-truth annotated features to reject low-confidence annotators.
    On each task, we measure the proportion of features confirmed (*Yes*), which relates
    to the precision of new features. Additionally, for inter-rater reliability, we
    report (1) the average pairwise agreement, and (2) F1, which has been used in
    related work as an appropriate and effective inter-rater agreement measure for
    the evaluation of text annotations such as features in app reviews [\[11\]](#page-10-6).


    ## V. EVALUATION


    ## *A. Token-based ground-truth*


    Table [II](#page-5-0) reports the precision, recall and F1 metrics for all data
    configurations and all selected models. Given that we do not have ground-truth
    data for non-feature entities (*true negatives*), we exclude accuracy from the
    results.


    *1) Out-of-domain Feature Extraction:* In this configuration, each block in Table
    [II](#page-5-0) for a given category C refers to the quality metrics reported
    when fine-tuning the specified model with the set of reviews from all categories
    from Table [I](#page-4-0) except C. Metrics refer then to the test set of reviews
    belonging to C. For example, the first 3 rows in Table [II](#page-5-0) report
    the performance of each model for predicting features included in app reviews
    from the PRODUCTIVITY when training the model with app reviews from all categories
    except PRODUCTIVITY. In this scenario, the best precision is reported by BERTbase
    (0.799), while the lowest is reported by RoBERTalarge (0.287). The last 3 rows
    in the *out-of-domain* block are the average value for each metric and each model
    configuration. For example, the highest average recall among all categories is
    reported by XLNetbase (0.482), followed by XLNetlarge (0.374).


    Complementarily, we extend the visualization of the results in two dimensions.
    A vertical analysis illustrates the comparison between different categories for
    a given model configuration. We use a colour-code pattern to highlight the best
    (green) and worst (red) performing category for each model. For example, for the
    baseline model (BERTbase), prediction of PRODUCTIVITY features reports the highest
    precision (0.799), while TRAVEL reports the highest recall (0.708), F1 (0.739)
    and accuracy (0.966). On the other hand, predicting new features from the MAPS
    domain reports the lowest overall accuracy for BERTbase. If we focus on F1, predicting
    PERSONALIZATION features under a set-up where the model was not trained under
    features of this domain reports the best results for 5 out of 6 model configurations.
    Contrarily, predicting MAPS features under the same set-up reports the lowest
    overall token-level prediction quality for 4 out of 6 model configurations. For
    a better understanding of this phenomenon, Figure [5](#page-6-0) showcases the
    degree (expressed in % of tokens) of lexical overlapping of the set of reviews
    of a given category (Y axis) with respect to the set of reviews from another category
    (X axis). We exclusively considered verbs, nouns and adjectives. For example,
    the cell on (0,0) coordinates illustrates the proportion of tokens from WEATHER
    apps that are also present in reviews from the COMMUNICATION app (around 15%).
    Consistently with our previous results, PERSONAL-IZATION presents a high overlap
    with all other categories (30- 35%), which showcases that the training set of
    reviews used for this configuration includes a high proportion of lexicon that
    the model has also been trained with. PERSONALIZATION apps often expose extended
    functions and customization capabilities to other apps, including widgets, wallpapers,
    stickers, themes and optimization tools. On the other hand, MAPS apps report a
    very low lexical overlap with respect to all categories (< 5%). This implies that
    the training set of reviews used for this setup did not include any of the category-specific
    lexicon from the navigational domain (e.g., GPS, GPX, POI...). Consequently, out-of-domain
    prediction of MAPS features becomes a challenging task. Similar conclusions can
    be reached by observing other categories. For example, PRODUCTIVITY apps include
    a large sub set of apps with features present in multiple categories (e.g., calling,
    note-taking, file-sharing).


    In addition, a horizontal analysis in Table [II](#page-5-0) illustrates the best-performing
    model for a given category. We use bold-face style and a special icon to highlight
    the best model for each category according to each metric. Additionally, we use
    the same strategy to report best average metrics for out-of-domain and in-domain
    analysis. Overall, RoBERTa models report the worst results for all metrics in
    almost each category, except for PERSONALIZATION apps when focusing on recall
    (0.947) and F1 (0.960). For BERT checkpoints, both base (PRODUC-TIVITY, TRAVEL)
    and large (COMMUNICATION, TOOLS) report the best metrics for precision. Nevertheless,
    XLNet variants excel in the majority of categories, especially if we focus on
    recall. Specifically, on average, XLNetbase reports the best recall (0.482) and
    F1 (0.535), while XLNetlarge reports the highest precision (0.652), but only by
    a small difference with respect to XLNetbase (+0.024).


    Given the limitations of the ground-truth generation, preci-


    ![](_page_6_Figure_3.jpeg)


    <span id="page-6-0"></span>Fig. 5. Lexical overlap between reviews from different
    categories.


    sion results must be interpreted under certain constraints. The lack of a guarantee
    of the exhaustivity of the crowdsourced features annotated by users implies that
    there might be tokens predicted by our model that are rejected as tokens from
    a feature (*false positives*), while they might be part of an actual feature (*true
    positives*) that was not indexed in the original set of features. Given that precision
    is the proportion of correct feature tokens with respect to all reported named
    entities, we argue that precision values reported above can be interpreted as
    the lower threshold of the minimum precision raised by our approach. On the other
    hand, recall (i.e., the proportion of retrieved named entities with respect to
    all ground-truth named entities) can be considered as a gold metric for quality
    analysis. All in all, for out-of-domain feature extraction, we argue that the
    best performing model is XLNetbase.


    *2) In-domain Feature Extraction:* The last rows in Table [II](#page-5-0) report
    average results for the 10-fold cross-validation analysis using the complete dataset
    in Table [I.](#page-4-0) While features in the test set for each data partition
    are not present in the training set (as explained in Section [IV-C2\)](#page-3-3),
    domain-related features from the same category are distributed in balance across
    all data splits. As expected, average results for all metrics are significantly
    higher in the in-domain analysis with respect to the out-ofdomain analysis. This
    result indicates that language models enhance their feature extraction capabilities
    for a specific category C when their training dataset includes reviews from that
    category, even if the predicted features are absent from the original training
    dataset. This underscores the relevance of domain-specific training data for the
    improvement of model performance in feature extraction tasks.


    XLNetlarge reports the best results for precision (0.761) and F1 (0.646). In addition,
    XLNetlarge recall (0.573) is only slightly below (−0.009) with respect to BERTlarge
    recall (0.582). Consequently, we argue that the best performing model for in-domain
    feature extraction is XLNetlarge.


    ## *B. Baseline feature extraction*


    Table [III](#page-7-0) reports out-of-domain and in-domain results for the selected
    feature extraction baseline method (i.e., SAFE),


    |                                                                            |
    TABLE III |  |  |

    |----------------------------------------------------------------------------|-----------|--|--|

    | FEATURE EXTRACTION EVALUATION RESULTS AND COMPARISON WITH BASELINE METHOD. |           |  |  |


    <span id="page-7-0"></span>


    |           |           | Out-of-domain |       |       |        |        |       |        |       |           |
    In-domain |         |         |

    |-----------|-----------|---------------|-------|-------|--------|--------|-------|--------|-------|-----------|-----------|---------|---------|

    | Method    | Metric    | PROD          | COMM  | TOOLS | SOCIAL | HEALTH | PERS  |
    TRAVEL | MAPS  | LIFESTYLE | WEATHER   | Average | Average |

    |           | precision | 0.309         | 0.235 | 0.299 | 0.286  | 0.298  | 0.195
    | 0.300  | 0.074 | 0.500     | 0.413     | 0.301   | 0.193   |

    | SAFE      | recall    | 0.300         | 0.229 | 0.292 | 0.329  | 0.274  | 0.229
    | 0.375  | 0.106 | 0.500     | 0.481     | 0.321   | 0.215   |

    |           | F1        | 0.304         | 0.232 | 0.295 | 0.306  | 0.285  | 0.211
    | 0.333  | 0.087 | 0.500     | 0.444     | 0.310   | 0.199   |

    |           | precision | 0.667         | 0.278 | 0.392 | 0.361  | 0.335  | 0.895
    | 0.591  | 0.000 | 0.500     | 0.690     | 0.471   | 0.575   |

    | BERTbase  | recall    | 0.128         | 0.131 | 0.222 | 0.385  | 0.186  | 0.447
    | 0.542  | 0.000 | 0.600     | 0.357     | 0.300   | 0.419   |

    |           | F1        | 0.215         | 0.178 | 0.284 | 0.373  | 0.240  | 0.596
    | 0.565  | 0.000 | 0.545     | 0.471     | 0.347   | 0.485   |

    |           | precision | 0.399         | 0.479 | 0.347 | 0.561  | 0.494  | 0.912
    | 0.538  | 0.200 | 0.600     | 0.502     | 0.503   | 0.631   |

    | XLNetbase | recall    | 0.244         | 0.145 | 0.168 | 0.590  | 0.402  | 0.816
    | 0.583  | 0.188 | 0.600     | 0.432     | 0.417   | 0.572   |

    |           | F1        | 0.303         | 0.222 | 0.226 | 0.575  | 0.443  | 0.861
    | 0.560  | 0.194 | 0.600     | 0.464     | 0.445   | 0.600   |


    our baseline model (i.e., BERTbase) and best-performing model (i.e., XLNetbase).
    Due to space constraints, we exclude results from other models (available in the
    replication package). Metrics in Table [III](#page-7-0) are computed using exact
    matches with the whole feature (i.e., *B-feature* tokens followed by none or any
    sequence of *I-feature* tokens). For presentation purposes, we invert the dimensions
    of Table [III](#page-7-0) with respect to Table [II,](#page-5-0) meaning that
    the horizontal dimensions relate to the comparison of categories (green/red),
    and the vertical dimension relates to different methods (bold and icon).


    On average, both the baseline model (BERTbase) and the best-performing model (XLNetbase)
    surpass SAFE''s quality metrics for out-of-domain and in-domain analyses. The
    only exception is the BERTbase recall (0.300), which is slightly below (−0.021)
    from SAFE. XLNetbase improves significantly the performance with respect to BERTbase,
    especially for recall (+0.117 for out-of-domain, +0.153 for in-domain). On a category-level,
    for the out-of-domain analysis, LLM-based approaches report a higher precision
    in all categories, and a higher recall for 6 out of 10 categories. On a horizontal
    analysis, similarly to results in Table [II,](#page-5-0) predicting out-of-domain
    features from MAPS reviews reports the worst results in all scenarios, while the
    best results are provided by LIFESTYLE and PERSONALIZATION. If we focus on the
    categories for which SAFE outperforms T-FREX on the out-of-domain analysis, we
    realize that the original SAFE approach was designed using feature syntactic patterns
    from apps belonging to these domains (PRODUCTIVITY, COMMUNICATION, TOOLS). Nevertheless,
    as the out-of-domain configuration implies that the model was not trained using
    any reviews from these categories, certain limitations are expected. However,
    the in-domain analysis, which illustrates the performance of T-FREX discovering
    new features from a domain for which it was fine-tuned, reports a substantial
    improvement for precision (+0.438), recall (+0.357) and F1 (+0.401).


    ## *C. New Features*


    Table [IV](#page-8-0) reports data and metrics resulted from the evaluation of
    new features. We discuss this results in alignment with the evaluation process
    depicted in Section [IV-D.](#page-4-1)


    *1) Data preparation:* We select XLNetbase as the best model for out-of-domain
    feature extraction (focusing on recall as the most reliable quality metric). After
    processing all reviews under each out-of-domain data configuration, we collected
    a total amount of 1,067 unique new features (1,956 annotations in total). We excluded
    3 out of 10 categories from this analysis (PERSONALIZATION, LIFESTYLE, WEATHER)
    where either 1 or even no new features were extracted. The whole set of 1,956
    review sentences was split into 21 annotation tasks. Each task included 95 new
    feature annotations (as in Figure [4\)](#page-4-2) plus 5 control questions, with
    100 annotations in total per task. Additionally, we prepared 2 ground-truth tasks
    of the same size containing at least 1 instance of each of the distinct features
    available in the crowdsourced data set in Table [I.](#page-4-0) The purpose of
    these tasks is to compare the overall precision of undocumented features with
    respect to the overall, perceived precision from users with respect to the ground-truth.


    *2) Evaluation set up:* We conducted up to 3 iterations of internal (i.e., expert)
    annotations for the test task. Each iteration was performed by a different annotator,
    providing feedback about the guidelines, the examples, the required time to conduct
    the task, and the difficulty. At the end of each iteration, we refined the guidelines
    (focusing on reducing ambiguity), extended the examples (focusing on covering
    exhaustively the different kinds of features) and adjusted the expected resolution
    time. As a result, we decided to keep task size to 100 features and an estimated
    average time of 15'' per task. On the test task, we report an average pairwise
    agreement between internal annotators of 73.3%, and an average F1 of 0.719. Complementarily,
    for internal evaluation, given that guidelines and annotators were selected and
    instructed under a test set-up, and all of them annotated the same features, we
    measured the Fleiss kappa agreement between all annotators, reporting a substantial
    degree of agreement (0.718).


    *3) Evaluation:* We set as acceptance criteria for annotators to reply correctly
    to 4 out of 5 control questions, and we ran multiple iterations until reaching
    5 accepted annotators for each task. We recruit participants by taking into consideration
    only fluent English speakers and without any language-related disorders. Each
    annotator is paid \$2 per task (around 15'') for their participation. Category-level
    and aggregated results are reported in Table [IV.](#page-8-0) The gold label for
    each feature was assigned using a voting-based approach between all annotators
    for each review sentence and feature (any ties were resolved as ''*I don''t know*''
    to reduce any biases of results). For the whole dataset, 61.2% of new features
    were confirmed as true features, which leads to a precision of 0.612 for new features.
    Results are generally balanced across categories with minor deviations, being
    TOOLS the category with less accepted features (58.4%)


    <span id="page-8-0"></span>


    | EVALUATION OF NEW FEATURES |       |                       |       |        |        |        |       |       |                      |                 |

    |----------------------------|-------|-----------------------|-------|--------|--------|--------|-------|-------|----------------------|-----------------|

    |                            |       | Evaluation (external) |       |        |        |        |       |       |
    Gr. truth (external) | Test (internal) |

    |                            | PROD. | COMM.                 | TOOLS | SOCIAL
    | HEALTH | TRAVEL | MAPS  | TOTAL | TOTAL                | TOTAL           |

    | #features (annotations)    | 459   | 643                   | 560   | 44     |
    218    | 8      | 29    | 1956  | 190                  | 95              |

    | #features (distinct)       | 294   | 383                   | 363   | 36     |
    155    | 8      | 19    | 1067  | 144                  | 17              |

    | % Yes                      | 68.6% | 62.3%                 | 58.4% | 63.6%  |
    59.4%  | 66.7%  | 58.6% | 61.2% | 77.0%                | 70.0%           |

    | % No                       | 28.8% | 35.0%                 | 41.7% | 34.1%  |
    39.3%  | 33.3%  | 41.4% | 37.0% | 23.0%                | 28.0%           |

    | % I don''t know             | 1.6%  | 2.7%                  | 1.8%  | 2.2%   |
    0.6%   | 0.0%   | 0.0%  | 1.9%  | 0.0%                 | 2.0%            |

    | Pairwise agreement         | 58.5% | 58.0%                 | 62.1% | 60.2%  |
    62.4%  | 57.7%  | 69.7% | 58.5% | 58.0%                | 73.3%           |

    | F1                         | 0.585 | 0.566                 | 0.622 | 0.594  |
    0.631  | 0.639  | 0.717 | 0.613 | 0.546                | 0.719           |


    TABLE IV EVALUATION OF NEW FEATURES


    and PRODUCTIVITY the category with more accepted features (68.6%). For the ground-truth
    validation with external annotators (including only actual features), 77.0% of
    feature annotations were accepted as true features, while 23.0% were rejected.
    While there is a difference of +15.8% with respect to the new features, this confirms
    the cognitive difficulty for actual users on the formalization of a *feature*.


    ## VI. DISCUSSION


    ## *A. Research Questions*


    Based on the evaluation results, we consolidate and report the response to each
    research question defined in Section [III.](#page-2-2)


    RQ1) Table [II](#page-5-0) provides a comprehensive empirical evaluation of the
    effectiveness of the T-FREX approach at tokenlevel. Among the different model
    configurations, XLNet approaches (benefitting from autoregressive methods to learn
    bidirectional contexts) seem to provide better results on average, especially
    for the out-of-domain analysis. Concerning data configurations, T-FREX proves
    to be significantly effective for an in-domain setting. Since mobile app markets
    are generally stable and the emergence of new domains is rare, the in-domain configuration
    is the most practical and common application for feature extraction tasks. Nevertheless,
    out-of-domain extraction also proves effective for certain domains and model configurations,
    particularly when the domain lexicon is not highly specialized. These results
    underscore T-FREX''s ability to discover new features even from an unknown domain.
    Consequently, the results for both in-domain and out-of-domain analyses highlight
    the adaptability and potential generalization of T-FREX across various settings.


    RQ2) Table [III](#page-7-0) presents a comprehensive empirical evaluation of the
    feature extraction method compared to the SAFE approach. On average, LLM-based
    token classification consistently outperforms SAFE across all metrics. Notably,
    there is a significant performance improvement when transitioning from the baseline
    model BERTbase to XLNetbase. The limitations of a deterministic approach, such
    as a nonspecialized vocabulary and context-agnostic behaviour, are particularly
    pronounced in specific domains or categories. This holds true even when the LLMs
    are evaluated in an out-ofdomain setting. Moreover, in an in-domain analysis,
    LLMs quickly overcome these limitations and significantly enhance their performance.
    They become adept at accurately predicting new features within domains they have
    been fine-tuned for. In addition to leveraging pre-trained LLMs, our supervised
    approach can be iteratively refined and tailored to specific domains, serving
    specialized markets and application categories. T-FREX enables the collection
    of recent app reviews and the integration of up-to-date crowdsourced features,
    continually enhancing feature extraction through subsequent fine-tuning iterations.
    This approach facilitates effective context integration, adaptation to new domains,
    and responsiveness to users'' vocabulary, syntax, and colloquial language—capabilities
    notably limited in SAFE and other feature extraction techniques.


    RQ3) Table [IV](#page-8-0) illustrates in detail the human evaluation process
    of new features. Results support the original hypothesis that the ground-truth
    dataset of features is limited. Despite these constraints, the human evaluation
    confirms the effectiveness of the model in the retrieval of new features. Furthermore,
    it is noteworthy that the precision of newly reported features surpasses the average
    precision of groundtruth features in the out-of-domain analysis. As knowledge
    from crowdsourced repositories is not typically exhaustive, results underscore
    T-FREX''s ability to automatically supplement feature annotations. Instead of
    relying on manual user input, user feedback (i.e., reviews) can serve as a valuable
    resource for suggesting features in a streamlined manner, employing a voting-based
    mechanism for automatically extracted features. This setting can potentially address
    the limitations of manual annotations and reduce the imbalance in feature representation
    across apps with similar user interaction levels.


    ## *B. Threats to Validity*


    We assess the constraints of our study by considering the validity threats as
    outlined by Wohlin et al. [\[46\]](#page-11-7).


    Concerning construct and internal validity, we mainly relate to the formalization
    of *features*, including its definition (used for the human evaluation in RQ3),
    exemplification and analysis. Related work illustrates cognitive differences in
    formalizing the limits of a natural language expression for a given feature (see
    Section [II-A\)](#page-1-3). This includes the generation of the ground-truth
    dataset by transferring app annotations to reviews. To mitigate internal bias,
    we use a reliable crowdsourced software recommendation platform with real user-annotated
    features that are used in practice for navigation, indexing, and software comparison.
    Moreover, we provide a detailed analysis (see Section [II\)](#page-1-4) to consolidate
    accepted criteria and descriptors for the formalization of a feature in the context
    of mobile apps.


    Concerning external validity, delegating the assignment of ground-truth feature
    annotations to an external entity leads to a lack of control of the annotation
    process and the annotators (i.e, the users). While this entails some risks, we
    argue that using annotations from real users in a practical environment provides
    significant benefits to an LLM-based feature extraction approach. This becomes
    especially relevant in the context of processing reviews generated by users themselves.
    Additionally, RQ3 is also designed to overcome and measure the impact of missing
    features in the ground truth. Concerning the latter, the human evaluation process
    also entails external validity concerns, especially for the lack of control of
    the human annotators and their potential bias. To reduce this risk, we included
    control questions to assess the reliability of each annotation task, and we used
    a voting-based approach to consider the most common prediction among all annotators.


    Concerning conclusion validity, the main concern is derived from the interpretation
    and generalization of the performance of each evaluation setting. For this reason,
    we included in this study two different analytical perspectives (i.e., out-of-domain
    and in-domain), each including a detailed perspective on the performance of all
    metrics for all selected models. For the out-of-domain analysis, we also include
    a detailed perspective on the performance of each category. Rather than providing
    a gold method in a one-fits-all fashion, we aim to provide researchers with enough
    information to interpret the strengths and limitations of each method under each
    configuration.


    ## VII. RELATED WORK


    <span id="page-9-0"></span>Dabrowski et al. recently conducted an evaluation and
    replication study focusing on mining techniques of app reviews for multiple tasks,
    including feature extraction [\[11\]](#page-10-6). Related work mainly refers
    to the SAFE approach as the most consolidated technique for feature extraction
    [\[34\]](#page-10-26). They identified and formalized a set of 18 common Part-of-Speech
    patterns from app descriptions and app reviews used to express app features. Through
    a pattern-matching approach, complemented by semantic similarity and synonymity
    resolution, they identify potential feature expressions. Nevertheless, reported
    performance in the original study is limited, especially when applying the technique
    to reviews, where a lot of noise features (i.e., false positives) are reported,
    leading to low precision. Furthermore, the original code and dataset are not publicly
    available. Consequently, replication studies have reimplemented and built new
    annotated data sets using groundtruth from instructed coders [\[11\]](#page-10-6),
    [\[47\]](#page-11-8), reporting lower quality than originally reported, especially
    for direct feature match.


    Similar conclusions apply to other related works applying the same syntactic-based
    strategy, either for early work [\[13\]](#page-10-8) or more up-to-date solutions
    like the ReUS approach [\[35\]](#page-10-27). In addition to previous limitations,
    evaluation strategies (including replication studies [\[11\]](#page-10-6)) are
    limited to instructed internal coders. The lack of the user perspective is key,
    especially when analysing user-generated documents (i.e., reviews). Moreover,
    they all focus on a reduced set of apps (8-10) and even fewer domains, and there
    is no formal evaluation of a categoryoriented analysis for their ability to generalize
    to new domains. Nevertheless, they are still used in practice for feature-based
    knowledge generation from mobile app repositories [\[48\]](#page-11-9), [\[49\]](#page-11-10).


    Few works can be found on the application of LLMs for the task of feature extraction.
    Similarly to previous studies [\[13\]](#page-10-8), [\[34\]](#page-10-26), [\[35\]](#page-10-27),
    the TransFeatEx tool applies PoS patterns by leveraging the knowledge embedded
    in a RoBERTa model to extract syntactic and semantic annotations [\[50\]](#page-11-11).
    Nevertheless, their contribution is presented as a tool without further evaluation
    or a concrete proposal for configuring the pattern template or the sentiment analysis
    thresholds. KEFE [\[51\]](#page-11-12) uses features extracted using PoS patterns
    as input to a BERT model for text classification of correct and incorrect features.
    However, they focus on the application of this technique for app descriptions,
    using the extracted features to transfer potential feature matches with user reviews.
    Consequently, feature knowledge is limited to developer-generated documentation.


    ## VIII. CONCLUSIONS AND FUTURE WORK


    In this research, we conducted an empirical evaluation of a token classification-based
    approach using LLMs to support feature extraction in the context of mobile app
    reviews. We explored and discussed in detail the performance of multiple models
    (BERT, RoBERTa, XLNet) under different data configurations (out-of-domain vs.
    in-domain) from multiple app categories. The evaluation provides a comprehensive
    perspective of the performance of each approach under each data configuration.
    Furthermore, ground-truth feature annotations by real users and external human
    evaluation contribute to extending the scope and body of knowledge of the feature
    landscape. All in all, our proposal leverages the potential of LLMs to benefit
    from contextualized knowledge and to overcome the limitations of syntactic-based
    approaches. Research and industrial applications focusing on software evolution
    analysis can benefit from the outcomes of this study, either by replicating T-FREX
    as a fully automatic process for feature extraction (either with different data
    sets or different models), by using the ground-truth data set of annotated reviews,
    or by using any of the fine-tuned models distributed for replication.


    As future work, we are currently working on the potential of extending the pre-training
    of the LLMs used for evaluation with a large data set of app reviews. Evaluation
    will focus on token classification and feature extraction metrics with respect
    to the original models. Furthermore, we plan to gain a better understanding of
    the inner workings of these models by analysing the embedded knowledge across
    multiple layers. To this end, hidden layers can be used as input for probing classifiers
    to determine to what extent the given layer embeds relevant knowledge to support
    feature extraction.


    ## ACKNOWLEDGMENTS


    With the support from the Secretariat for Universities and Research of the Ministry
    of Business and Knowledge of the Government of Catalonia and the European Social
    Fund. This paper has been funded by the Spanish Ministerio de Ciencia e Innovación
    under project / funding scheme PID2020-117191RB-I00 / AEI/10.13039/501100011033.
    Alessio Miaschi and Felice Dell''Orletta have also been supported by the PNRR
    project FAIR - Future AI Research (PE00000013), under the NRRP MUR program funded
    by the NextGenerationEU.


    ## REFERENCES


    - <span id="page-10-0"></span>[1] W. Martin, F. Sarro, Y. Jia, Y. Zhang, and M.
    Harman, "A survey of app store analysis for software engineering," *IEEE Transactions
    on Software Engineering*, vol. 43, no. 9, pp. 817–847, 2017.

    - <span id="page-10-1"></span>[2] Authority for Consumers & Markets, "Market study
    into mobile app stores (Report ACM/18/032693)," april 2019. [Online]. Available:
    [https://www.acm.nl/sites/default/files/documents/](https://www.acm.nl/sites/default/files/documents/market-study-into-mobile-app-stores.pdf)
    [market-study-into-mobile-app-stores.pdf](https://www.acm.nl/sites/default/files/documents/market-study-into-mobile-app-stores.pdf)

    - <span id="page-10-2"></span>[3] W. Maalej and H. Nabil, "Bug report, feature
    request, or simply praise? on automatically classifying app reviews," in *2015
    IEEE 23rd International Requirements Engineering Conference (RE)*, 2015, pp. 116–125.

    - [4] S. Hassan, H. Li, and A. E. Hassan, "On the importance of performing app
    analysis within peer groups," in *2022 IEEE International Conference on Software
    Analysis, Evolution and Reengineering (SANER)*, 2022, pp. 890–901.

    - [5] A. Yadav, R. Sharma, and F. H. Fard, "A semantic-based framework for analyzing
    app users'' feedback," in *2020 IEEE 27th International Conference on Software
    Analysis, Evolution and Reengineering (SANER)*, 2020, pp. 572–576.

    - [6] L. Guerrouj, S. Azad, and P. C. Rigby, "The influence of app churn on app
    success and stackoverflow discussions," in *2015 IEEE 22nd International Conference
    on Software Analysis, Evolution, and Reengineering (SANER)*, 2015, pp. 321–330.

    - <span id="page-10-3"></span>[7] S. Panichella, A. Di Sorbo, E. Guzman, C. A.
    Visaggio, G. Canfora, and H. C. Gall, "How can i improve my app? classifying user
    reviews for software maintenance and evolution," in *2015 IEEE International Conference
    on Software Maintenance and Evolution (ICSME)*, 2015, pp. 281–290.

    - <span id="page-10-4"></span>[8] J. D ˛abrowski, E. Letier, A. Perini, and A.
    Susi, "Analysing app reviews for software engineering: A systematic literature
    review," *Empirical Softw. Engg.*, vol. 27, no. 2, mar 2022.

    - [9] A. Begel and T. Zimmermann, "Analyze this! 145 questions for data scientists
    in software engineering," in *Proceedings of the 36th International Conference
    on Software Engineering*, 2014, p. 12–23.

    - <span id="page-10-5"></span>[10] R. P. L. Buse and T. Zimmermann, "Information
    needs for software development analytics," in *2012 34th International Conference
    on Software Engineering (ICSE)*, 2012, pp. 987–996.

    - <span id="page-10-6"></span>[11] J. Da¸browski, E. Letier, A. Perini, and A.
    Susi, "Mining and searching app reviews for requirements engineering: Evaluation
    and replication studies," *Information Systems*, vol. 114, p. 102181, 2023. [Online].
    Available:<https://doi.org/10.1016/j.is.2023.102181>

    - <span id="page-10-7"></span>[12] F. Palomba, M. Linares-Vásquez, G. Bavota,
    R. Oliveto, M. Di Penta, D. Poshyvanyk, and A. De Lucia, "User reviews matter!
    tracking crowdsourced reviews to support evolution of successful apps," in *2015
    IEEE International Conference on Software Maintenance and Evolution (ICSME)*,
    2015, pp. 291–300.

    - <span id="page-10-8"></span>[13] E. Guzman and W. Maalej, "How do users like
    this feature? A fine grained sentiment analysis of App reviews," *2014 IEEE 22nd
    International Requirements Engineering Conference, RE 2014 - Proceedings*, pp.
    153–162, 2014.

    - <span id="page-10-9"></span>[14] D. Pagano and W. Maalej, "User feedback in
    the appstore: An empirical study," in *2013 21st IEEE International Requirements
    Engineering Conference (RE)*, 2013, pp. 125–134.

    - <span id="page-10-10"></span>[15] C. Gao, J. Zeng, M. R. Lyu, and I. King, "Online
    app review analysis for identifying emerging issues," in *Proceedings of the 40th
    International Conference on Software Engineering*, 2018, p. 48–58.

    - <span id="page-10-11"></span>[16] N. Chen, J. Lin, S. C. H. Hoi, X. Xiao, and
    B. Zhang, "Ar-miner: Mining informative reviews for developers from mobile app
    marketplace," in *Proceedings of the 36th International Conference on Software
    Engineering*. New York, NY, USA: Association for Computing Machinery, 2014, p.
    767–778.

    - <span id="page-10-12"></span>[17] Shah, Faiz Ali and Sirts, Kairit and Pfahl,
    Dietmar, "Is the SAFE Approach Too Simple for App Feature Extraction? A Replication
    Study," in *Requirements Engineering: Foundation for Software Quality*, Knauss,
    Eric and Goedicke, Michael, Ed. Cham: Springer International Publishing, 2019,
    pp. 21–36.

    - <span id="page-10-13"></span>[18] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit,
    L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin, "Attention is all you need,"
    *Advances in neural information processing systems*, vol. 30, 2017.

    - <span id="page-10-14"></span>[19] T. Zhang, B. Xu, F. Thung, S. A. Haryono,
    D. Lo, and L. Jiang, "Sentiment analysis for software engineering: How far can
    pre-trained


    transformer models go?" in *2020 IEEE International Conference on Software Maintenance
    and Evolution (ICSME)*, 2020, pp. 70–80.


    - [20] A. Ciborowska and K. Damevski, "Fast changeset-based bug localization with
    bert," in *Proceedings of the 44th International Conference on Software Engineering*.
    New York, NY, USA: Association for Computing Machinery, 2022, p. 946–957. [Online].
    Available: <https://doi-org.recursos.biblioteca.upc.edu/10.1145/3510003.3510042>

    - [21] C. Yang, B. Xu, J. Khan, G. Uddin, D. Han, Z. Yang, and D. Lo, "Aspect-based
    api review classification: How far can pre-trained transformer model go?" in *2022
    IEEE International Conference on Software Analysis, Evolution and Reengineering
    (SANER)*, 2022, pp. 385–395.

    - <span id="page-10-15"></span>[22] J. Tabassum, M. Maddela, W. Xu, and A. Ritter,
    "Code and named entity recognition in StackOverflow," in *Proceedings of the 58th
    Annual Meeting of the Association for Computational Linguistics*. Online: Association
    for Computational Linguistics, jul 2020, pp. 4913–4926. [Online]. Available:<https://aclanthology.org/2020.acl-main.443>

    - <span id="page-10-16"></span>[23] N. Jha and A. Mahmoud, "Mining non-functional
    requirements from app store reviews," *Empirical Software Engineering*, pp. 1–
    37, 2019. [Online]. Available: [https://api.semanticscholar.org/CorpusID:](https://api.semanticscholar.org/CorpusID:174802984)
    [174802984](https://api.semanticscholar.org/CorpusID:174802984)

    - <span id="page-10-17"></span>[24] K. C. Kang, S. G. Cohen, J. A. Hess, W. E.
    Novak, and A. S. Peterson, "Feature-oriented domain analysis feasibility study,"
    SEI Technical Report CMU/SEI-90-TR-21, Tech. Rep., 1990.

    - <span id="page-10-18"></span>[25] K. E. Wiegers and J. Beatty, *Software Requirements
    3*. USA: Microsoft Press, 2013.

    - <span id="page-10-19"></span>[26] M. Harman, Y. Jia, and Y. Zhang, "App store
    mining and analysis: Msr for app stores," in *2012 9th IEEE Working Conference
    on Mining Software Repositories (MSR)*, 2012, pp. 108–111.

    - <span id="page-10-20"></span>[27] K. Kang, S. Cohen, J. Hess, W. Novak, and
    A. Peterson, "Feature-oriented domain analysis (foda) feasibility study," Software
    Engineering Institute, Carnegie Mellon University, Pittsburgh, PA, Tech. Rep.
    CMU/SEI-90-TR-021, 1990. [Online]. Available: [http:](http://resources.sei.cmu.edu/library/asset-view.cfm?AssetID=11231)
    [//resources.sei.cmu.edu/library/asset-view.cfm?AssetID=11231](http://resources.sei.cmu.edu/library/asset-view.cfm?AssetID=11231)

    - <span id="page-10-21"></span>[28] B. Jehangir, S. Radhakrishnan, and R. Agarwal,
    "A survey on named entity recognition—datasets, tools, and methodologies," *Natural
    Language Processing Journal*, vol. 3, p. 100017, 2023.

    - <span id="page-10-22"></span>[29] K. Hakala and S. Pyysalo, "Biomedical named
    entity recognition with multilingual BERT," in *Proceedings of the 5th Workshop
    on BioNLP Open Shared Tasks*. Hong Kong, China: Association for Computational
    Linguistics, Nov. 2019, pp. 56–61. [Online]. Available: <https://aclanthology.org/D19-5709>

    - [30] A. K. Tarcar, A. Tiwari, D. Rao, V. N. Dhaimodker, P. Rebelo, and R. Desai,
    "Healthcare ner models using language model pretraining," in *HSDM@WSDM*, 2019.
    [Online]. Available: [https:](https://api.semanticscholar.org/CorpusID:210943047)
    [//api.semanticscholar.org/CorpusID:210943047](https://api.semanticscholar.org/CorpusID:210943047)

    - <span id="page-10-23"></span>[31] L. Gu, W. Zhang, Y. Wang, B. Li, and S. Mao,
    "Named entity recognition in judicial field based on bert-bilstm-crf model," in
    *2020 International Workshop on Electronic Communication and Artificial Intelligence
    (IWECAI)*. IEEE, 2020, pp. 170–174.

    - <span id="page-10-24"></span>[32] K.-J. Stol and B. Fitzgerald, "The abc of
    software engineering research," *ACM Trans. Softw. Eng. Methodol.*, vol. 27, no.
    3, sep 2018. [Online]. Available:<https://doi.org/10.1145/3241743>

    - <span id="page-10-25"></span>[33] Q. Motger, X. Franch, and J. Marco, "Mobile
    feature-oriented knowledge base generation using knowledge graphs," in *New Trends
    in Database and Information Systems - ADBIS 2023 Short Papers, Doctoral Consortium
    and Workshops: AIDMA, DOING, K-Gals, MADEISD, PeRS, Barcelona, Spain, September
    4-7, 2023, Proceedings*, ser. Communications in Computer and Information Science,
    vol. 1850. Springer, 2023, pp. 269–279. [Online]. Available: [https://doi.org/10.1007/978-3-031-42941-5\\_24](https://doi.org/10.1007/978-3-031-42941-5_24)

    - <span id="page-10-26"></span>[34] T. Johann, C. Stanik, A. M. Alizadeh, and
    W. Maalej, "SAFE: A Simple Approach for Feature Extraction from App Descriptions
    and App Reviews," *Proceedings - 2017 IEEE 25th International Requirements Engineering
    Conference, RE 2017*, pp. 21–30, 2017.

    - <span id="page-10-27"></span>[35] M. Dragoni, M. Federici, and A. Rexha, "An
    unsupervised aspect extraction strategy for monitoring real-time reviews stream,"
    *Information Processing & Management*, vol. 56, no. 3, pp. 1103–1118, 2019.

    - <span id="page-10-28"></span>[36] AppTweak, "Google Play Store Categories,"
    2022, Accessed 5th October, 2023. [Online]. Available: [https://developers.apptweak.com/](https://developers.apptweak.com/reference/google-play-store-categories)
    [reference/google-play-store-categories](https://developers.apptweak.com/reference/google-play-store-categories)

    - <span id="page-10-29"></span>[37] Stanford NLP Group, "Neural pipeline." [Online].
    Available: [https:](https://stanfordnlp.github.io/stanza/neural_pipeline.html)
    [//stanfordnlp.github.io/stanza/neural\\_pipeline.html](https://stanfordnlp.github.io/stanza/neural_pipeline.html)

    - <span id="page-10-30"></span>[38] Universal Dependencies, "CoNLL-U Format."
    [Online]. Available: <https://universaldependencies.org/format.html>

    - <span id="page-11-0"></span>[39] W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang,
    Y. Hou, Y. Min, B. Zhang, J. Zhang, Z. Dong *et al.*, "A survey of large language
    models," *arXiv preprint arXiv:2303.18223*, 2023.

    - <span id="page-11-1"></span>[40] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova,
    "BERT: Pre-training of deep bidirectional transformers for language understanding,"
    in *Proceedings of the 2019 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies, Volume 1 (Long and
    Short Papers)*. Minneapolis, Minnesota: Association for Computational Linguistics,
    Jun. 2019, pp. 4171–4186. [Online]. Available:<https://aclanthology.org/N19-1423>

    - <span id="page-11-2"></span>[41] S. Broscheit, "Investigating entity knowledge
    in BERT with simple neural end-to-end entity linking," in *Proceedings of the
    23rd Conference on Computational Natural Language Learning (CoNLL)*. Hong Kong,
    China: Association for Computational Linguistics, Nov. 2019, pp. 677–685. [Online].
    Available:<https://aclanthology.org/K19-1063>

    - <span id="page-11-3"></span>[42] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi,
    D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov, "Roberta: A robustly
    optimized bert pretraining approach," 2019.

    - <span id="page-11-4"></span>[43] Z. Yang, Z. Dai, Y. Yang, J. Carbonell, R.
    R. Salakhutdinov, and Q. V. Le, "Xlnet: Generalized autoregressive pretraining
    for language understanding," *Advances in neural information processing systems*,
    vol. 32, 2019.

    - <span id="page-11-5"></span>[44] "Prolific · Quickly find research participants
    you can trust." [Online]. Available:<https://www.prolific.com/>

    - <span id="page-11-6"></span>[45] "QuestBase." [Online]. Available: [https://questbase.com/en/](https://questbase.com/en/home-questbase/)
    [home-questbase/](https://questbase.com/en/home-questbase/)

    - <span id="page-11-7"></span>[46] C. Wohlin, "Guidelines for snowballing in systematic
    literature studies and a replication in software engineering," in *Proceedings
    of the 18th International Conference on Evaluation and Assessment in Software
    Engineering*, 2014.

    - <span id="page-11-8"></span>[47] F. A. Shah, K. Sirts, and D. Pfahl, "Using
    app reviews for competitive analysis: Tool support," in *MAWA 2019*, 2019, pp.
    40–46.

    - <span id="page-11-9"></span>[48] S. Kumari and Z. A. Memon, "Extracting feature
    requests from online reviews of travel industry," *Acta Scientiarum - Technology*,
    vol. 44, 2022.

    - <span id="page-11-10"></span>[49] M. Kasri *et al.*, "A Comparison of Features
    Extraction Methods for Arabic Sentiment Analysis," in *4th International Conference
    on Big Data and Internet of Things*, 2020.

    - <span id="page-11-11"></span>[50] A. Gállego, J. Motger, X. Franch, and J. Marco,
    "TransFeatEx: a NLP pipeline for feature extraction," in *Joint proceedings of
    REFSQ-2023 Workshops, Doctoral Symposium, Posters & Tools Track and Journal Early
    Feedback: co-located with the 28th International Conference on Requirements Engineering:
    Foundation for Software Quality (REFSQ 2023): Barcelona, Catalunya, Spain, April
    17-20, 2023*. CEUR-WS.org, 2023. [Online]. Available:<https://ceur-ws.org/Vol-3378/PT-paper1.pdf>

    - <span id="page-11-12"></span>[51] H. Wu *et al.*, "Identifying key features
    from app user reviews," in *International Conference on Software Engineering*,
    2021.'
  decisions:
    language: '- Qualified. Reason: English Paper'
    evaluation_prompt: Qualified.
    related_work_prompt: Qualified
    novelty_prompt: Qualified
    review_only_prompt: Qualified
  llm_input_used: '## Abstract

    Mobile app reviews are a large-scale data source for software-related

    knowledge generation activities, including software maintenance, evolution and

    feedback analysis. Effective extraction of features (i.e., functionalities or

    characteristics) from these reviews is key to support analysis on the

    acceptance of these features, identification of relevant new feature requests

    and prioritization of feature development, among others. Traditional methods

    focus on syntactic pattern-based approaches, typically context-agnostic,

    evaluated on a closed set of apps, difficult to replicate and limited to a

    reduced set and domain of apps. Meanwhile, the pervasiveness of Large Language

    Models (LLMs) based on the Transformer architecture in software engineering

    tasks lays the groundwork for empirical evaluation of the performance of these

    models to support feature extraction. In this study, we present T-FREX, a

    Transformer-based, fully automatic approach for mobile app review feature

    extraction. First, we collect a set of ground truth features from users in a

    real crowdsourced software recommendation platform and transfer them

    automatically into a dataset of app reviews. Then, we use this newly created

    dataset to fine-tune multiple LLMs on a named entity recognition task under

    different data configurations. We assess the performance of T-FREX with respect

    to this ground truth, and we complement our analysis by comparing T-FREX with
    a

    baseline method from the field. Finally, we assess the quality of new features

    predicted by T-FREX through an external human evaluation. Results show that

    T-FREX outperforms on average the traditional syntactic-based method,

    especially when discovering new features from a domain for which the model has

    been fine-tuned.


    ## Introduction

    Mobile app repositories provide valuable access to timely large-scale datasets
    of software-related information [\[1\]](#page-10-0). These repositories include
    heterogeneous, multiple-purpose platforms, from app stores to sideloading repositories
    and search engines [\[2\]](#page-10-1). One of the most popular contributions
    across these platforms is the publication of app reviews, in which users express
    multiple facets such as personal opinions or experiences, bug reports, inquiries
    or requests [\[3\]](#page-10-2). This information is relevant to multiple software
    engineering processes, including requirements elicitation and prioritization,
    release planning, validation analysis and software evolution [\[3\]](#page-10-2)–[\[7\]](#page-10-3).


    App features are considered a core descriptor for understanding and categorizing
    app reviews [\[8\]](#page-10-4)–[\[10\]](#page-10-5). In this context, a feature
    is considered as a distinct function or capability within a mobile application
    serving a particular purpose or need [\[11\]](#page-10-6). App feature extraction
    supports featurerelated knowledge generation, in which mobile app developers can
    potentially rely on to improve user experience, enhance app functionality, identify
    user preferences, and make datadriven decisions for app development strategies
    [\[3\]](#page-10-2), [\[12\]](#page-10-7), [\[13\]](#page-10-8). Consequently,
    mining large amounts of app reviews to extract app features has become a relevant
    task. Nevertheless, mining features from app reviews presents particular challenges.
    It requires the daily analysis of thousands of short documents, each with a limited
    length, composed of an average of a few dozen words per review [\[14\]](#page-10-9).
    Beyond measurable characteristics, user-generated documents tend to present multiple
    informal writing styles and vocabulary, including misspelt words, repetitions
    or cross-language terminology [\[15\]](#page-10-10), polarized or biased information,
    or even noisy and spam content [\[16\]](#page-10-11).


    Consolidated approaches rely on syntactic pattern-matching techniques to retrieve
    features from app descriptions and reviews [\[11\]](#page-10-6). Nevertheless,
    several challenges emerge from their applicability, including limited replicability,
    unavailability of data and a lack of user evaluation [\[11\]](#page-10-6). Furthermore,
    rule-based strategies for knowledge generation can be brittle to identify complex
    patterns, domain-specific terminology, unexpected contents and contextual knowledge,
    which affects the generalization of these techniques [\[17\]](#page-10-12). To
    overcome these challenges, deep learning strategies, and in particular Large Language
    Models (LLMs) based on the Transformer architecture [\[18\]](#page-10-13), have
    shown promising results in multiple software-related data mining tasks. These
    approaches leverage the knowledge embedded in these pre-trained models by extending
    their capabilities through task-specific supervised fine-tuning tasks such as
    sentiment analysis, text classification or named-entity recognition (NER) [\[19\]](#page-10-14)–[\[22\]](#page-10-15).


    In this paper, we present T-FREX (*Transformer-based FeatuRe EXtraction*), a novel
    approach to support feature extraction from app reviews using LLMs. Our proposal
    redefines the app feature extraction problem as a NER task, a specific type of
    token classification in which tokens referring to a particular entity type (e.g.
    dates, geopolitical entities, features, etc.) are labelled as such. Our main contributions
    are[1](#page-0-0) : (1) a


    <span id="page-0-0"></span><sup>1</sup>GitHub repository:<https://github.com/gessi-chatbots/t-frex/>


    | Telegram Features                                                                       |
    Suggest and vote on featu                      |

    |-----------------------------------------------------------------------------------------|------------------------------------------------|

    | Lightweight<br>Telegram consumes less device resources compared to similar apps.        |                                                |

    | Ad-free<br>0<br>Telegram doesn''t contain any form of external advertising.              |                                                |

    | · End-to-End Encryption<br>Telegram has E2E Encryption, for entire or parts
    of the app. |                                                |

    | Dark Mode<br>Telegram supports dark mode for comfortable usage in low light
    conditions. |                                                |

    | Portable<br>Support for Themes<br>IFTTT Integration                                     |
    Self Destructing Messages<br>Works Offline     |

    | Cloud Sync   Multiple Account support   VoiP Calls<br>Two-factor Authentication         |
    Encrypted Chat                                 |

    | Stickers Chat Bot<br>Large File Transfer<br>Multi Device Support                        |
    Instant Messaging<br>Bots                      |

    | Video Calling<br>Channels<br>Share Videos<br>Secret chats                               |
    Cloud based<br>Animated stickers               |

    | Persistent History<br>Integrated File Sharing                                           |
    Folders Video Conferencing<br>Security focused |


    <span id="page-1-0"></span>Fig. 1. Sample of crowdsourced user annotated features
    in a software recommendation platform (https://alternativeto.net/) for the Telegram
    app.


    Transformer-based, fully automatic approach for the extraction of mobile app features
    from user reviews; (2) an extensive evaluation of the performance of multiple
    Transformer-based LLMs [\[18\]](#page-10-13) in different classification scenarios;
    (3) a reusable fine-tuned model and a ground-truth dataset of annotated app reviews,
    based on crowdsourced annotated app features extracted from a popular software
    recommendation platform and automatically transferred into the corpus of app reviews.'
  token_usage: 6906
  time_usage: 2.033235788345337
- title: "How Dataflow Diagrams Impact Software Security Analysis: an Empirical\n\
    \  Experiment"
  abstract: 'Models of software systems are used throughout the software development

    lifecycle. Dataflow diagrams (DFDs), in particular, are well-established

    resources for security analysis. Many techniques, such as threat modelling, are

    based on DFDs of the analysed application. However, their impact on the

    performance of analysts in a security analysis setting has not been explored

    before. In this paper, we present the findings of an empirical experiment

    conducted to investigate this effect. Following a within-groups design,

    participants were asked to solve security-relevant tasks for a given

    microservice application. In the control condition, the participants had to

    examine the source code manually. In the model-supported condition, they were

    additionally provided a DFD of the analysed application and traceability

    information linking model items to artefacts in source code. We found that the

    participants (n = 24) performed significantly better in answering the analysis

    tasks correctly in the model-supported condition (41% increase in analysis

    correctness). Further, participants who reported using the provided

    traceability information performed better in giving evidence for their answers

    (315% increase in correctness of evidence). Finally, we identified three open

    challenges of using DFDs for security analysis based on the insights gained in

    the experiment.'
  url: http://arxiv.org/abs/2401.04446v1
  keywords: security, analysis, dataflow diagrams, microservices, model-based, empirical
    experiment
  document: '# How Dataflow Diagrams Impact Software Security Analysis: an Empirical
    Experiment


    Simon Schneider<sup>∗</sup> , Nicolas E. D ´ ´ıaz Ferreyra<sup>∗</sup> , Pierre-Jean
    Queval ´ † , Georg Simhandl† ,


    Uwe Zdun† , Riccardo Scandariato<sup>∗</sup>


    <sup>∗</sup>Hamburg University of Technology, *firstname.lastname@tuhh.de* †University
    of Vienna, *firstname.lastname@univie.ac.at*


    *Abstract*—Models of software systems are used throughout the software development
    lifecycle. Dataflow diagrams (DFDs), in particular, are well-established resources
    for security analysis. Many techniques, such as threat modelling, are based on
    DFDs of the analysed application. However, their impact on the performance of
    analysts in a security analysis setting has not been explored before. In this
    paper, we present the findings of an empirical experiment conducted to investigate
    this effect. Following a within-groups design, participants were asked to solve
    security-relevant tasks for a given microservice application. In the control condition,
    the participants had to examine the source code manually. In the model-supported
    condition, they were additionally provided a DFD of the analysed application and
    traceability information linking model items to artefacts in source code. We found
    that the participants (n = 24) performed significantly better in answering the
    analysis tasks correctly in the model-supported condition (41% increase in analysis
    correctness). Further, participants who reported using the provided traceability
    information performed better in giving evidence for their answers (315% increase
    in correctness of evidence). Finally, we identified three open challenges of using
    DFDs for security analysis based on the insights gained in the experiment.


    *Index Terms*—security, analysis, dataflow diagrams, microservices, model-based,
    empirical experiment


    #### I. INTRODUCTION


    Dataflow Diagrams (DFDs) are integral to many software security analysis techniques.
    For instance, they are required by prominent security assessment techniques [\[1\]](#page-10-0)–[\[4\]](#page-10-1).
    They are also the program representation chosen in many model-based security approaches
    [\[5\]](#page-10-2)–[\[12\]](#page-10-3). As such, they can help software engineers
    build more secure software systems. However, their impact on security analysis
    by their mere provision has not been investigated before to the best of our knowledge.
    DFDs offer a high-level yet detailed representation of applications'' architecture.
    Enriched with annotations representing, e.g., employed security mechanisms, they
    offer easy accessibility of the architectural security. We hypothesize that providing
    users with a DFD of an application enables them to analyse the application''s
    security properties with higher correctness.


    To investigate this hypothesis, we conducted an empirical experiment. This paper
    reports on our findings. The experiment was performed with master students who
    solved tasks related to software security analysis activities. For this, they
    received the source code and a textual description of an open-source microservice
    application, and six tasks to answer. We chose microservice applications as the
    target of analysis because the distributed nature of this architectural style
    poses additional challenges in terms of cognitive load to security analysts. Systems
    following the microservice architecture split their codebase into multiple independent
    microservices, where each fulfils a part of the business functionality and can
    be independently developed and deployed [\[13\]](#page-10-4), [\[14\]](#page-10-5).
    The resulting codebase can be challenging to oversee in analysis scenarios.


    The experiment followed a within-groups design. In the model-supported condition,
    participants received a DFD and traceability information of the analysed application
    in addition to the source code and textual description provided in the control
    condition. We chose DFDs created by *Code2DFD*, a tool for the automatic extraction
    of DFDs from source code [\[15\]](#page-10-6), as these contain extensive security
    annotations. We infer insights on the impact of DFDs on security analysis by comparing
    the participants'' performance in analysis correctness, correctness of evidence,
    and time in the two conditions. Specifically, we answer the following research
    questions:


    ® RQ 1: Do security-annotated architectural models of applications, specifically
    DFDs, support developers in the security analysis of the applications?


    A crucial part of security analysis is identifying and localizing security (and
    other) features in source code. To assess whether models with extensive security
    annotations can support developers and security experts in this activity, the
    participants in our experiment solved tasks that require the identification of
    implemented security mechanisms and other relevant system properties. We quantified
    their answers and compared the scores between the two conditions. Additionally,
    we asked them about the perceived usefulness and analysed the answers.


    ® RQ 2: Does access to and use of traceability information improve the ability
    of the security analysts to provide correct evidence for their analysis findings?


    Traceability information establishes the validity of model items by referencing
    corresponding artefacts in the source code. It can provide value for security
    analysis, since in scenarios such as software security assessment or certification,
    evidence has to be given for the reached findings. The participants provided evidence
    for their answers in the form of locations in the source code. We examined its
    correctness and compared the scores of those participants who reported using the
    traceability information frequently and those who did not. ® RQ 3: What is the
    experience in using security-annotated DFDs for security analysis, specifically
    concerning the usefulness and accessibility?


    Users'' acceptance of offered tools and techniques is crucial for using resources
    such as DFDs for security analysis. Thus, the participants'' perceived usefulness
    of the DFDs is essential to judge their suitability for real-world application.
    Further, the information presented by DFDs has to be conveyed to the users efficiently
    and accessible. To judge this aspect, we asked the participants about their experience
    using the DFDs in the model-supported condition and analysed their responses.


    ® RQ 4: What are the open challenges of using securityannotated DFDs in the context
    of security analysis?


    Based on the insights gained during the empirical experiment and the analysis
    of the results, we identified and formulated open challenges that should be addressed
    in future work.


    The rest of this paper is structured as follows: Section [II](#page-1-0) introduces
    the used DFDs; Section [III](#page-1-1) describes the experiment''s design, i.e.,
    the methodology; the results are presented in Section [IV](#page-4-0) and discussed
    in Section [V;](#page-7-0) Section [VI](#page-8-0) describes limitations of this
    work; Section [VII](#page-9-0) presents related work; and Section [VIII](#page-9-1)
    concludes the paper.


    #### II. CHARACTERISTICS OF THE USED DFDS


    <span id="page-1-0"></span>Since no standard specification for DFDs exists, the
    various styles of DFDs found in the model-based literature differ in their characteristics.
    Here, *style* refers to the types of model items, their presentation and richness
    of detail, and the scope of considered system components. All DFD styles share
    the four base item groups *external entities*, *data flows*, *processes*, and
    *data stores* [\[16\]](#page-10-7). Many approaches include additional model items
    to increase the models'' expressivity [\[1\]](#page-10-0), [\[9\]](#page-10-8),
    [\[17\]](#page-10-9). In our experiment, we used DFDs from a dataset published
    by Schneider et al. [\[18\]](#page-10-10). An example is shown in Figure [1.](#page-1-2)
    The style is the same as those generated by an automatic extraction approach by
    Schneider and Scandariato [\[15\]](#page-10-6). Two of the DFDs'' properties stand
    out compared to other styles, the included *annotations* and the *traceability
    information* for model items. Annotations in the DFDs provide information about
    the corresponding system''s security and other properties. The annotations represent
    implemented security mechanisms (e.g., encryption or authorization mechanisms),
    deployment information (e.g., ports or addresses), or other system properties
    (e.g., used technologies and frameworks). Annotations are associated with model
    items from the four base item groups mentioned above. That means that base model
    items (such as, e.g., a service) are augmented with the annotations. Figure [1](#page-1-2)
    shows examples of this (see the *--annotation-* and *Key: Value* annotations).
    Traceability information links model items to source code by pointing to code
    snippets that prove the existence of the model item. Figure [2](#page-1-3) shows
    as an example the traceability information for the annotation *authorization server*
    as well as a screenshot of the target of the contained URL, i.e., the line of
    code on GitHub.


    ![](_page_1_Figure_7.jpeg)


    Fig. 1. Example DFD provided to participants in model-supported condition.


    <span id="page-1-2"></span>![](_page_1_Figure_9.jpeg)


    <span id="page-1-3"></span><span id="page-1-1"></span>Fig. 2. Screenshots of an
    example traceability information for the annotation *authorization server* (top)
    and the target of the URL (bottom).


    #### III. STUDY DESIGN


    We designed and conducted an empirical experiment to answer the formulated research
    questions. We consulted established sources of guidelines for empirical research
    in the design of the experiment ( [\[19\]](#page-10-11)–[\[21\]](#page-10-12)).
    All materials as well as the results are available in the replication package
    [\[22\]](#page-10-13).


    #### *A. Setup*


    The experiment followed a within-groups design, where participants participated
    in both conditions. The study was performed in two 90-minutes lab sessions of
    a master''s course at a university in subsequent weeks. The participants were
    randomly assigned to one of the two groups, G1 and G2. A different application
    was given as the target of analysis in the two weeks to mitigate learning effects,
    *App 1* in week 1 and *App 2* in week 2. The two sessions were structured as follows:


    |    | Week 1                    | Week 2                    |

    |----|---------------------------|---------------------------|

    |    | App 1                     | App 2                     |

    | G1 | control condition         | model-supported condition |

    | G2 | model-supported condition | control condition         |


    In the *model-supported condition*, the participants performed tasks with access
    to a DFD and corresponding traceability information, whereas in the *control condition*,
    they performed a similar set of tasks without this additional support. They were
    supervised during the sessions and were discouraged from talking to each other
    about the experiment. Google Forms surveys were used to provide the tasks and
    gather the answers.


    # *B. Tasks*


    Table [I](#page-3-0) lists the analysis tasks given to the participants. They
    were chosen such that they resemble common security analysis activities. The first
    two tasks are not specific to security but are relevant nevertheless since they
    foster a required comprehension of the analysed system. The tasks cover three
    different kinds of questions, the participants had to find:


    - general information about single services (tasks 1 & 2)

    - information about security mechanisms of single services (tasks 3 & 4)

    - information about system-wide security mechanisms (tasks 5 & 6)


    For all tasks, the participants were asked to provide evidence for their answers
    via a reference to the code.


    After completion of the technical tasks, the participants were also posed an open
    question about their experience with using the resources they had been provided.
    In the first week, they were further asked questions concerning their expertise.


    As the target of evaluation for the security analysis, we chose two open-source
    applications from a list published by Schneider et al. [\[18\]](#page-10-10).
    The applications are referred to as App1 [1](#page-2-0) and App2 [2](#page-2-1)
    . These applications were selected based on two properties: (i) high architectural
    similarity between the two in order to enable an accurate comparison of participants''
    performance between the two sessions, and (ii) sufficient architectural complexity
    to allow insightful and relevant tasks. The two applications exhibit a high degree
    of similarity concerning their architecture, size, and used technologies. They
    incorporate some of the most prevalent microservice patterns and employ widely
    adopted technology solutions for Java-based microservice development. For instance,
    they both realize an API Gateway with Zuul, authentication with OAuth 2.0, a load
    balancer with Ribbon, and service discovery with Eureka. Consequently, the applications
    are a suitable representation of open-source microservice applications developed
    in Java.


    #### *C. Provided Application Artefacts*


    All participants were provided the source code and a basic textual description
    of the analysed application. The textual description is an explanatory document
    that was created based on the code and information provided by the developers
    of the applications. To mitigate a potential influence on the experiment''s outcome
    based on different qualities of the textual descriptions of the two applications,
    they were created in identical structure and contain the same basic information
    about the corresponding application. They illustrate the basic architectural design
    of the applications. We remark that some tasks could be answered based on these
    documents. The code was provided via GitHub. Specifically, the applications''
    repositories were forked to remove the original documentation, which could otherwise
    influence the outcome.


    In addition to the code and textual description, the participants in the model-supported
    condition received the DFD and the traceability information of the application
    to be analysed.


    #### <span id="page-2-2"></span>*D. Participants*


    The experiment''s participants comprised 24 students enrolled in a master''s level
    software security course at *Hamburg University of Technology*. They originate
    from various disciplines, all incorporating computer science to a large degree.
    The students were informed about the empirical experiment two months in advance
    and were invited to participate. Figure [3](#page-3-1) shows the participants''
    programming knowledge (a), experience with reading Java code (b), and work experience
    (c). This information was reported in a self-assessment at the end of the first
    week''s session. Based on the results ("intermediate level" being the answer most
    often given to the question of programming skills; little experience with reading
    Java code; and an average work experience of 1.1 years), we deduced that the participants
    were on average advanced beginners in software development and had moderate experience
    in analysing Java code. Accordingly, they represent well the target population
    of the experiment. The goal of the experiment was to investigate the effect of
    architectural models on the performance of users with low expertise in software
    security analysis, for example novice developers. The metrics in Figure [3](#page-3-1)
    fit well to such users. Furthermore, using students as proxies for the target
    population is a common practice in empirical software engineering and has been
    shown to be a suitable method [\[19\]](#page-10-11), [\[23\]](#page-10-14)–[\[25\]](#page-10-15).


    *1) Incentives:* The students were generally incentivized to participate in the
    course''s lab sessions, independent of the empirical experiment. For this, they
    were rewarded with a 5% bonus on their final exam, granted upon attending all
    of the lab sessions in the semester except for one (a common practice at the university).
    Further, they were encouraged to participate because the sessions were relevant
    for the final exam, and the participants could hone the required skills there.
    The lab sessions where the empirical experiment was conducted were akin to all
    other lab sessions in these regards. Consequently, the experiment''s impact on
    students'' grades was consistent


    <span id="page-2-0"></span><sup>1</sup>github.com/anilallewar/microservices-basics-spring-boot


    <span id="page-2-1"></span><sup>2</sup>github.com/piomin/sample-spring-oauth2-microservices/tree/with
    database


    TABLE I TASKS FOR APP 2. SERVICE NAMES AND NUMBER OF CONNECTIONS SLIGHTLY DIFFER
    FOR APP 1.


    <span id="page-3-0"></span>![](_page_3_Figure_1.jpeg)


    <span id="page-3-1"></span>Fig. 3. Participants'' (a) programming skills, (b)
    experience in reading Java code, and (c) work experience as developers. All self-reported.


    with that of the other lab sessions in this course. No other incentives were pledged
    or given.


    *2) Preparation:* To prepare the participants, a 90-minute lecture before the
    lab sessions was dedicated to introducing them to the topic (available in this
    paper''s replication package [\[22\]](#page-10-13)). The lecture covered key concepts
    relevant to the experiment. The primary focus was on the origin of software vulnerabilities
    and methods for detecting them. The lecture also encompassed topics such as DFDs,
    microservice architectures, and security considerations in microservice applications.
    Following this lecture, the students were expected to possess the required knowledge
    to undertake the experiment. Their attendance at the lecture was recorded and
    was a prerequisite for participating in the experiment.


    *3) Informed consent and ethical assessment:* All participants read and signed
    an informed consent form before the experiment, informing them that they are the
    subjects of an empirical experiment, that they participate voluntarily, that they
    do not have to expect any negative consequences whatsoever if they do not participate,
    and that they can retract their consent at any time. To ensure the experiment''s
    ethical innocuity, it was assessed by the *German Association for Experimental
    Economic Research e.V.* before execution. A description of the planned experiment
    and its design was approved under grant nr. *2pxo1bap*. The certificate can be
    accessed via [https://gfew.de/ethik/2pxo1bap.](https://gfew.de/ethik/2pxo1bap)


    #### *E. Measurement*


    To evaluate the participants'' performance, three metrics were introduced. The
    *analysis correctness* represents the ability to provide correct answers to the
    tasks. The *correctness of evidence* measures whether the evidence that the participants
    provided as support for their answers points to a code snippet that justifies
    their answer. Both are numerical scores derived from the participants'' responses.
    Additionally, we measured the time spent on solving the tasks. The three metrics
    (analysis correctness, correctness of evidence, and time) were calculated for
    each participant in both conditions separately.


    *1) Analysis Correctness:* We quantified the given answers concerning the analysis
    correctness by manually checking the participants'' responses. To remove subjectivity,
    we created a reference solution that was used to check the answers. It was created
    prior to the execution of the experiment. The DFDs and source code of the applications
    have been analysed to create the reference answers, which were afterwards confirmed
    by conducting technical documentation of the code libraries used in the applications,
    information provided by the developers of the applications, and other typical
    online resources. This process was performed by the first author and validated
    afterwards by two additional authors. After the experiment, the participants''
    responses were mapped via the reference solution to a table indicating correct
    and incorrect answers. Each response was examined manually, compared against the
    reference solution, and true answers were marked in the table. We further reviewed
    answers that did not match the reference solution to check whether they were correct.
    For this, various typical online resources were conducted to verify whether the
    specific answer applies to the task. Each correctly given answer gives a score
    of 1. There is a peculiarity for some tasks. Task 3 asks for a list of connections
    between services, and tasks 4 and 5 ask whether a property applies to each item
    on a given list. Consequently, these tasks each required multiple distinct responses.
    All responses were checked individually. Then, to allow a more detailed and nuanced
    evaluation, we converted the results to scores. A score of 0 was assigned for
    no correct responses, a score of 1 was given for partially correct responses (meaning
    that some but not all responses of a task were correct), and a score of 2 was
    awarded when all responses were correct. With three tasks giving a maximum of
    one point and three tasks giving a maximum of two points, the overall highest
    achievable score in analysis correctness is 9.


    *2) Correctness of Evidence:* The traceability information that is contained in
    the used DFDs constitutes a reference solution for quantifying the correctness
    of the evidence given by the participants. Each given evidence was checked manually
    for matches to this reference solution. Here, we employed some tolerance in accepting
    evidence as correct. For example, when participants referred to a block of code
    slightly larger than the lines of code needed to prove an answer, we still accepted
    this as correct (e.g., referring to a method consisting of some lines of code
    instead of referring to a single line of code in that method). We carried out
    a further validation check, similar to the quantification of the analysis correctness.
    Each provided evidence that differed from the reference solution was checked manually
    whether it supported the given answer or not. The first author carried out the
    above steps. As for the analysis correctness, each correct evidence gives a score
    of 1. Again, for tasks 3, 4, and 5, the multiple distinct responses were converted
    into a score of 0, 1, or 2 for no correct, partially correct, and all correct
    responses, respectively.


    *3) Time:* To measure the time spent on solving the tasks, the participants were
    asked to record the current time when starting and finishing to work on the tasks.
    We calculated the time metric based on these answers (i.e., the period of time
    between the start and finish of solving the tasks).


    *4) Reported Usefulness:* The DFDs'' usefulness as reported by the participants
    was assessed via the open question about the participants'' experience in using
    the DFDs that was posed after the technical tasks. We qualitatively analysed all
    answers'' general intents (positive/negative feedback) and identified recurring
    topics manually. This analysis was performed by the first author and verified
    by two further authors.


    #### <span id="page-4-2"></span>*F. Statistical Tests*


    Throughout the analysis, the difference of scores between two groups was checked
    for statistical significance with a Wilcoxon-Mann-Whitney test. Before, a Shapiro-Wilke
    test was used to verify that the data does not follow a normal distribution. Hence,
    no parametric tests could be used. The assumptions for the Wilcoxon-Mann-Whitney
    test (the samples are independent, random, and continuous and the sample size
    is sufficiently large) are met in our experiment.


    #### IV. RESULTS


    #### <span id="page-4-0"></span>*A. Analysis Correctness*


    Figure [4](#page-4-1) presents the average score in analysis correctness that
    the participants achieved in the two conditions. The figure shows that the participants
    performed better in the modelsupported condition, both overall and in every individual
    task. For all tasks, the average score is 6.75 out of a possible 9 in the model-supported
    condition compared to 4.79 in the control condition, a 41% higher average. The
    applied statistical test (compare Section [III-F\)](#page-4-2) indicates a statistically
    significant difference between the two conditions'' average scores in analysis
    correctness overall (p = 0.0025). These results provide the following answer to
    RQ1:


    ![](_page_4_Figure_8.jpeg)


    <span id="page-4-1"></span>Fig. 4. Comparison across the two treatments of the
    participants'' average score in analysis correctness. Per task and overall.


    ![](_page_4_Figure_10.jpeg)


    <span id="page-4-3"></span>Fig. 5. Comparison across the two treatments of the
    participants'' average score in correctness of evidence. Per task and overall.


    - RQ 1: In the context of our experiment, providing a security-annotated DFD of
    the system to be analysed improved participants'' analysis correctness in solving
    security analysis tasks. We observed a statistically significant (p = 0.0025)
    improvement of 41% on average.


    For some individual tasks, the difference in the average scores is only marginal
    (task 2: 0.42 vs. 0.46 <sup>∧</sup>= 10% improvement in model-supported condition;
    task 6: 0.75 vs. 0.79 <sup>∧</sup>= 5.6% improvement). In Section [V,](#page-7-0)
    we discuss whether the nature of the tasks might be an indication of the extent
    to which a DFD improves the score in analysis correctness. However, even though
    the improvement is not statistically significant for all individual tasks (statistical
    significance only for task 3 with a p-value of 0.0003), Figure [4](#page-4-1)
    clearly shows a trend of improved performance in the model-supported condition.


    #### *B. Correctness of Evidence*


    Figure [5](#page-4-3) presents the average score in correctness of evidence achieved
    by the participants. There are only small differences in the average scores for
    the model-supported and control condition. With an average of 3.08 out of a possible
    9 in the control condition compared to an average of 2.71 in the modelsupported
    condition (-12%), the participants performed better in the control condition,
    albeit without statistical significance (p = 0.52). Task 4 has the lowest average
    correctness of evidence of the individual tasks (average of 0.29 out of 2 in the
    control condition and 0.13 in the model-supported condition),


    ![](_page_5_Figure_0.jpeg)


    <span id="page-5-0"></span>Fig. 6. Reported usage of provided artefacts per task
    (in the model-supported condition, where all artefacts were available).


    while task 1 has the highest average (0.58 out of 1 in the control condition and
    0.63 in the model-supported condition).


    #### *C. Use of Provided Artefacts*


    After each task, the participants were asked to name all provided artefacts that
    they used to answer the task. Figure [6](#page-5-0) shows the reported usage per
    task in the model-supported condition. We focus on this condition because, there,
    the participants had access to all artefacts. Note that the participants had the
    possibility to name multiple artefacts per task. Overall, the DFD was used the
    most, with a total of 88 answers. The source code was used 55 times, the traceability
    information 43 times, and the textual description 21 times. The numbers show,
    that the participants did not solely rely on the provided DFD, but instead also
    referred to the source code in many cases and the textual descriptions in some
    cases. This could indicate, that the participants verified information they found
    in the DFD by checking the corresponding part of the code.


    #### *D. Influence of Use of Artefacts on Scores*


    We investigated whether the participants'' usage of the provided artefacts had
    an influence on their performance. Although they were provided more artefacts
    in the modelsupported condition, this does not necessarily mean that they used
    them all. The answers to the tasks could be found with more than one of the artefacts.
    Thus, the influence of single artefacts on the performance is not necessarily
    reflected in the comparison of outcomes between the two conditions. For example,
    participants in the model-supported condition could have not used the provided
    DFD to answer the tasks. Consequently, we compared the average scores in analysis
    correctness and in the correctness of evidence between two groups of participants
    for each artefact. To the group *Using Artefact* we assigned all participants
    that reported using the artefact in more than 50% of the tasks (4 or more). The
    group *Not Using Artefact* contains those participants who reported using it less
    (3 or fewer). We considered only the outcomes from the participants in the model-supported
    condition, since only here they had access to all artefacts. The grouping and
    analysis were done separately for each artefact, thus, the cardinality and members
    of the groups differ between artefacts. Not Using Using


    ![](_page_5_Figure_7.jpeg)


    <span id="page-5-1"></span>Fig. 7. Average scores in analysis correctness of those
    participants that reported using an artefact in more than 50% of the tasks (Using
    Artefact) and those that reported using it less (Not Using Artefact).


    ![](_page_5_Figure_9.jpeg)


    <span id="page-5-2"></span>Fig. 8. Average scores in the correctness of evidence
    of those participants that reported using an artefact in more than 50% of the
    tasks (Using Artefact) and those that reported using it less (Not Using Artefact).


    Figure [7](#page-5-1) presents the average scores in analysis correctness of the
    two groups per application artefact. The figure shows that the Using Artefact
    group performed better compared to the Not Using artefact group for the artefacts
    DFD (+23% in score) and traceability information (+12%), while they performed
    worse for the source code (-17%). Figure [8](#page-5-2) presents the results of
    this analysis for the correctness of evidence. For the artefact source code, the
    Using Artefact group achieved a 15% higher score in correctness of evidence than
    the Not Using Artefact group. For the use of DFD, they performed worse than the
    Not Using Artefact group (-20%). The highest difference, however, is seen in the
    traceability information. The Using Artefact group achieved a 315% higher average
    score in correctness of evidence than the Not Using Artefact group. We answer
    RQ2 based on these results since they distinguish between the use of the DFD and
    traceability information. In the results above, this distinction could not be
    made because the traceability information is an integral part of the DFDs and
    its isolated impact on the analysis could not be measured.


    - RQ 2: Using traceability information significantly improved the correctness
    of evidence given for answers. On average, participants that used this artefact
    in more than half of the tasks achieved a 315% higher correctness of evidence
    compared to participants that used it less than that.


    # *E. Time*


    All participants were able to complete the tasks in the allotted 90 minutes. Their
    average time to complete all tasks was 34 minutes in the control condition and
    35 minutes in the modelsupported condition. No notable difference was observed.
    To examine a possible correlation between performance and time spent to finish
    the tasks, we also created a scatter plot visualizing their scores against their
    time. No correlation between scores and time could be visually identified.


    # *F. Perceived Usefulness and Usability of DFDs*


    The answers given to the open question at the end of the analysis sessions provide
    insights into the participants'' perceived usefulness of the DFDs. The question
    asked about positive or negative observations during the experiment. For participants
    in the model-supported condition, it explicitly mentioned the usefulness of the
    DFDs and traceability information.


    Out of 23 answers given by participants in the modelsupported condition, two were
    negative, stating that thorough documentation would be preferred and that the
    DFD was "*a little bit hard to understand at first*". Three answers listed both
    positive and negative experiences, where the negative points were two mentions
    that finding implementation details was hard (both participants reported using
    the traceability only in one task) and one that the participant lacked domain
    knowledge. A further 14 answers were predominantly positive.


    # *"Dataflow Diagrams were incredibly helpful, and all questions were answered
    almost completely from it."*


    Of the 23 answers, 9 mentioned specific beneficial scenarios for the use of DFDs.
    The ability to provide an overview of the system was mentioned 8 times, the benefit
    of referring to the important places in source code and use of the models as interface
    to the code was mentioned 3 times, and the reduction of the required domain knowledge
    was mentioned once.


    Mild critique about the accessibility of the DFDs or traceability information
    was raised in 4 responses, for example:


    *"[...] the transfer from the DFD to the traceability information could be made
    easier by clickable links in the DFD [...]"*


    In summary, the statements made by participants in the model-supported condition
    include descriptions of the general usefulness of the DFDs, of benefits in finding
    implementation details via the model items and traceability information, and of
    their usefulness for architectural details and providing an overview. The positive
    feedback outweighed the few negative comments. Most participants reported the
    DFDs to be of help in the analysis and to be accessible to use.


    Of the answers given after the control condition, only one was positive, stating
    that the textual description was helpful. Four others referred to the DFDs (these
    answers were given in the second week, the participants had thus already performed
    the session with the DFD), stating that, in comparison, the lack of a DFD was
    an obstacle during the analysis. Specifically, they raised concerns about the
    correctness of their given answers and stated that finding the required features
    directly in source code was challenging.


    *"The traceability file and the DFD were a big help last time, this time I wasn''t
    really sure if I even answered correctly and didn''t really know if the evidence
    I gave was correct. [...]"*


    Six further answers of participants in the control condition in the first week
    (and thus without the comparison to the model-supported condition) reported negatively
    about their experience in the experiment. Specifically, they mentioned a lack
    of expertise, uncertainty about the given answers, and general difficulties in
    answering the tasks. Interestingly, two participants criticized the lack of a
    "*CFG*" or "*some kind of map of the architecture*". This could have been sparked
    by the introductory lecture where DFDs were addressed but is still seen as an
    interesting comment. The obstacles reported by the participants in the control
    condition give further weight to the positive feedback of those in the model-supported
    condition.


    Based on a qualitative analysis of the participants'' statements, we can cautiously
    judge the perceived usefulness and accessibility of the DFDs to answer RQ3:


    - RQ 3: In our experiment, the perceived usefulness and accessibility reported
    by the participants varied from very positive feedback to mild critiques reporting
    some confusion. Overall, the statements focussed on usefulness and were predominantly
    positive.


    #### *G. Open Challenges of DFDs*


    The above observations of the quantitative and qualitative results allowed us
    to distill a number of open challenges of DFDs, i.e., current obstacles that would
    increase the DFDs'' positive impact further if solved. Although these challenges
    were not explicitly investigated as independent variables in our experiment, they
    became evident from the results of the experiment, explicit answers given by participants,
    and observations made during the analysis of the tasks.


    *Open Challenge 1: Understandability of Models* The participants in our experiment
    performed with statistical significance better in the model-supported condition
    and they reported a generally good accessibility. Nevertheless, concerns were
    raised about the understandability of the models. Some participants commented,
    that they did not understand the model initially or that they did not know what
    some annotations mean. A more usable model representation of software systems
    should consider the accessibility for human users, especially those with lower
    domain knowledge.


    *Open Challenge 2: Presenting Missing Features* The DFDs in their current form
    do not support the explicit presentation of the absence of features or properties.
    In the context of security analysis, these could be security mechanisms that are
    not implemented by a given application. To enable more comprehensive analysis
    and increase users'' trust, it is important to show that such mechanisms were
    investigated and are not implemented in the analysed application. In this context,
    the challenges are to prove the absence, to decide what features to consider,
    and how to convey this information to the user. We see this open challenge as
    the hardest one to solve, both conceptually and practically.


    *Open Challenge 3: Accessibility of Traceability Information* The quantitative
    results of our experiment show, that the traceability information has a positive
    impact on the correctness of evidence provided for answers to the tasks. While
    this is an expected observation, multiple participants also mentioned the usefulness
    of the traceability information for navigating the source code. However, it was
    also mentioned in some answers that the connection to the source code was difficult
    to follow. Also, the traceability information was not used by everyone even when
    it was provided. We conclude that the ease of use can be improved and that navigating
    the links to source code should be simplified. This challenge is of more practical
    nature and can likely be solved with some clever engineering.


    - RQ 4: We identified three open challenges of DFDs (understandability of models,
    presenting missing features, and accessibility of traceability information). If
    any of these are solved, the positive impact of DFDs on security analysis can
    be expected to further increase.


    # V. DISCUSSION


    <span id="page-7-0"></span>At the heart of the conducted experiment lay the question
    of the impact of providing DFDs and traceability information on the participants''
    performance. The results presented in Section [IV](#page-4-0) indicate an overall
    positive impact on the analysis correctness. The scores improved with statistical
    significance in the model-supported condition. Figure [7](#page-5-1) emphasizes
    this finding. Participants in the model-supported condition who reported using
    the provided DFD in more than half of the tasks had a 23% higher score on average
    than those who reported using it less. A 12% higher average score for participants
    using the traceability information is further proof of the usefulness of the DFDs,
    since the traceability information is one of their core features. The observed
    17% lower score in analysis correctness for participants who reported using the
    source code in more than half of the tasks was an unexpected outcome at first
    sight. A closer look at the usage of the source code as artefact revealed, that
    out of 55 responses that mentioned using the source code, 34 (this corresponds
    to 62%) did not use the DFD or traceability information in conjunction. In other
    words, the source code was predominantly not used alongside the models, but instead
    as the only artefact to answer a task. Consequently, in our experiment, many participants
    who reported using the source code could also be described as not using the provided
    models. With this re-phrasing, the results are another indication of the models''
    positive impact.


    Looking at the individual tasks, the increase in scores differed between them.
    This suggests the question of which type of tasks the DFDs have the most impact
    on, and how exactly they impact different types of tasks. We investigated whether
    the nature of the tasks could be an explanation for the observations, i.e. whether
    the type of task can indicate how the score is influenced. We found that the DFDs
    impacted the analysis tasks in our experiment in different ways. They are described
    in the following. Please refer to Table [I](#page-3-0) for the tasks.


    *Providing an Overview:* Tasks 1, 2, and 6 have fairly simple answers in comparison
    to the other tasks. The answer for task 1 (in which the analysis correctness improved
    by 33% in the model-supported condition) could be found at two places in the code,
    either a deployment file indicating the container''s port, or a configuration
    file indicating the service''s port. Both answers were accepted as correct. In
    the DFD, the port is shown as annotation to the corresponding node. Interestingly,
    the wrong answers given are one of two options. One is the port number of a different
    microservice, which likely showed up when searching for "port" with GitHub''s
    search function. The other is the port of a database that is only visible in the
    code as part of the database''s URL. How this answer was reached by participants
    is puzzling. For task 6, the improvement of the average score was the lowest of
    all tasks (0.75 in the control condition and 0.79 in the modelsupported condition;
    5.6% increase). The task has the overall best average scores, likely, because
    the authorization service''s name ("auth server") hints towards the answer of
    which of the services handles the authorization. Task 2 could be answered based
    on the textual description, on the Java annotation that implements the API gateway
    in code, or on an annotation in the DFD. A 10% improvement in average score in
    analysis correctness was observed from the control condition (0.42) to the model-supported
    condition (0.46). The answers lead us to believe that the question might have
    been formulated such that participants did not fully understand it. Many of the
    wrong answers in both conditions stated the used framework (Spring) instead of
    the library that was asked for (Zuul). Further, this task had the lowest reported
    number of usages of the DFD as well as traceability information (compare Figure
    [6\)](#page-5-0).


    The answers and evidences indicate, that DFDs are helpful in providing an overview
    and presenting the answers to simple questions such as the port number of a microservice.
    Evidently, finding *any* port in the code is a simple task in many systems'' codebases,
    however, the answers suggest that finding the *correct* one can be challenging.
    Likely, this is heightened by the complexity that the microservice architecture
    adds to an application''s codebase due to its decoupling. The answers given by
    participants in the model-supported condition further emphasize this quality of
    DFDs to provide an overview of the important system components (compare Section
    [IV,](#page-4-0) where this was the benefit most often mentioned by participants).
    Simultaneously, for simple tasks with a fairly easy answer, good coding practice
    such as choosing descriptive identifiers seems to support the analysts well and
    there is no pressing need to provide a DFD. Whether this holds true in the analysis
    of larger applications should be investigated in future work. The results of task
    2 indicate problems in the DFDs'' accessibility. The presented information seems
    to not be selfexplanatory enough for the participants to answer this task reliably,
    even when the information is contained in the DFDs.


    \$ The results indicate, that DFDs serve as a means to "navigate the jungle" that
    is the application''s codebase. They provide an overview of the application''s
    architecture and (security and other) features. At the same time, wellchosen identifiers
    in code can support the solving of simple analysis tasks and the DFDs add less
    value in this scenario.


    *Reducing Required Domain Knowledge:* To answer tasks 3 and 5 in the control condition,
    some domain knowledge was needed to correctly grasp the functionality of the relevant
    code. Task 3 required the participants to identify three outgoing connections
    (for App 1, two for App 2) of a microservice. One is a direct API call implemented
    with Spring Boot''s RestTemplate, another a registration with a service discovery
    service, and the third a registration with a tracing server (similar for App 2).
    Some domain knowledge about these technologies or Java was required to identify
    them. With the DFD at hand, answer the task came down to identifying the correct
    node in the diagram and noting the three nodes to which there was an information
    flow. To answer task 5 without the DFD, participants had to check whether three
    services (for App 1, two for App 2) refer to the authorization service in a configuration
    file under an authorization section. In the DFD, a connection to the authorization
    server indicated this. Again, knowledge about Spring or Java made it easier to
    find the correct answers without the support of the DFDs.


    Task 3 showed the biggest impact of the models, with a doubled average score in
    analysis correctness (0.875 in control condition and 1.75 in model-supported condition;
    100% increase). While this task was more difficult to answer than the others without
    a DFD and the required domain knowledge, the magnitude of the difference is still
    substantial. For task 5, the average score in analysis correctness was 1.29 in
    the control condition and 1.58 in the model-supported condition, a 23% increase.
    The differences show how the DFDs reduce the domain knowledge required for analysis
    activities. However, we hypothesize, that the participants without the DFD could
    answer the task simply by identifying the keyword "authorization" in the configuration
    files without checking if the implementation is correct and behaves in the way
    that is asked for. We believe, that this led them to achieve an average score
    without the DFDs that is still high. Given the scenario in which they solved the
    tasks (empirical experiment, where answers are expected), this was likely sufficient
    evidence for them to answer, independent of whether their domain knowledge was
    profound enough to fully understand the workings.


    \$ Our interpretation of the results is that DFDs are especially helpful in scenarios
    where a lack of domain knowledge about the analysed application''s framework,
    libraries, etc. hinders the identification of features and system components.
    The DFDs'' ability to shed light on properties shaded by a curtain of domain knowledge
    seems to be one of their core virtues.


    *Indicating Absence of Features:* Despite the open challenge 2 (presenting missing
    features in the DFDs, see Section [IV\)](#page-4-0), the results also indicate
    that the DFDs in their current form already support users in answering tasks concerning
    the absence of features in the code. Task 4 was different from the other ones
    in that the challenge lay not in finding an artefact in the code but instead the
    absence of it. The task asked for the presence of encryption in two connections
    (for App 1, three for App 2) between services. The correct answer to all of them
    was "No". The average score in analysis correctness was 0.83 in the control condition
    and 1.33 in the model-supported condition out of a possible score of 2 (60% increase).
    The difficulty in this task also became apparent when looking at the results for
    the evidence. The participants achieved an average score in correctness of evidence
    of 0.042 in both conditions.


    \$ Although the DFDs still face the open challenge of presenting missing features,
    their current form already supports users in answering tasks that require identifying
    the absence of features in code.


    In summary of the discussion of the results, we see that the DFDs had a positive
    impact on the scores in different types of tasks. Specifically, they provide an
    overview of the analysed application, they reduce the required domain knowledge,
    and they can indicate the absence of features in the application. The highest
    increase in scores is seen for tasks where some domain knowledge was needed to
    answer them without the DFDs. The only task where the improvement of the analysis
    correctness in the model-supported condition was neglectable was a simple task
    where descriptive identifiers in code indicated the answer.


    # VI. THREATS TO VALIDITY


    <span id="page-8-0"></span>*Internal validity:* With a large group of university
    students as participants, collaborations during or between the sessions and resulting
    cross-contamination cannot be ruled out completely. As mitigation, we strictly
    discouraged collaborations and conversations about the study and supervised the
    analysis sessions. Learning effects or the possibility of preparing for the tasks
    were mitigated with the employed within-groups design where the scenarios switched
    over the two sessions and with the use of different applications. With 90-minute
    long sessions, experimental fatigue is limited. The random assignment to the groups
    G1 and G2 limits selection bias. Some of the analysed data (timestamps, experience,
    resource usage) is self-reported, and we have to rely on its correctness. The
    encouragement of positive as well as negative feedback and the often-repeated
    reassurance of full anonymity of the answers were used to increase the reliability
    of the data. By making participation voluntary and using only the standard incentive
    for attending the lab sessions, it is possible that we have attracted mainly students
    who show high motivation and are at the top of their class. This could have had
    distorting effects on the results and could not be reasonably mitigated.


    *External validity:* The conclusions drawn in this paper might not entirely map
    to other scenarios or populations. The tasks used as examples of security analysis
    activities might differ from real-world use cases and thus influence the shown
    effects. Further, the experiment focused on microservice applications written
    in Java. We chose Java applications because it is the most used programming language
    for open-source microservice applications. The analysis of systems that follow
    a different architectural style or are written in another programming language
    could show other outcomes. The number of participants (24) is relatively small.
    We chose robust statistical methods that are suitable for the sample size and
    discussed the impact of the participants'' experience and the choice of tasks.
    The participants'' expertise in security analysis is rather low. Thus, the effects
    described in this paper might not be observed for other users, e.g., with more
    experience. However, the use of DFDs is not confined to security experts, hence
    rendering the participants a suited population for the experiment. Finally, experiments
    with practitioners instead of students could lead to different results, however,
    it is a common practice and has been shown to produce valid results as well (see
    Section [III-D\)](#page-2-2).


    *Construct validity:* We measured the participants'' performance in terms of correctness
    and time, which are common and objective metrics for such experiments. They relate
    to the practical use-case of the investigated effects directly. The analysis correctness
    is crucial in security analysis to ensure accurate security evaluations and, consequently,
    secure systems. The time serves as a measure of productivity and efficiency. Other
    constructs were disregarded but could be suited as well.


    *Content validity:* The tasks concerned the key security mechanisms implemented
    in the analysed applications. These or similar tasks would be part of a real-world
    security analysis. However, other tasks might also be important in this context.


    *Conclusion validity:* The responses to the tasks were given in free-text fields.
    Although we did not identify such ambiguities in quantifying the responses, it
    is possible that some answers were phrased in a way that was interpreted incorrectly.
    A more restrictive way of collecting the answers could have increased the conclusion''s
    validity.


    #### VII. RELATED WORK


    <span id="page-9-0"></span>Although DFDs are used for different aspects of security
    analysis, no related work could be found that investigates their direct impact
    on the correctness of the analysis. Publications for other model types exist.
    For example, a considerable body of empirical research on Unified Modeling Language
    (UML) diagrams has been published [\[26\]](#page-10-16). A number of experiments
    have been conducted to investigate whether users'' comprehension of the modelled
    systems increases with UML diagrams. Gravino et al. [\[27\]](#page-10-17), [\[28\]](#page-10-18)
    observed a positive impact of the models, while experiments by Scanniello et al.
    [\[29\]](#page-10-19) did not show such an improvement (the authors attribute
    this to the type of UML diagrams, which had little connection to the code since
    they had been created in the initial requirements elicitation phase of the development
    process). In an experiment by Arisholm et al. [\[30\]](#page-10-20), code changes
    performed by participants with access to UML documentations showed significantly
    improved functional correctness. Other researchers investigated the impact of
    specific properties of UML diagrams on users'' comprehension. For example, Cruz-Lemus
    et al. [\[31\]](#page-10-21), [\[32\]](#page-10-22), Ricca et al. [\[33\]](#page-10-23),
    and Staron et al. [\[34\]](#page-10-24), [\[35\]](#page-10-25) found that stereotypes
    (which are similar to annotations in DFDs in our experiment) increased users''
    efficiency and effectiveness in code comprehension. Some publications found alternative
    model representations to yield better comprehension among participants in empirical
    experiments: Otero and Dolado [\[36\]](#page-10-26) reported that OPEN Modelling
    Language (OML) models led to faster and easier comprehension than UML diagrams,
    while Reinhartz-Berger and Dori [\[37\]](#page-11-0) reported Object-Process Methodology
    (OPM) models to be better suited than UML diagrams for modelling dynamic aspects
    of applications.


    Bernsmed et al. [\[38\]](#page-11-1) presented insights into the use of DFDs in
    agile teams by triangulating four studies on the adoption of DFDs. In the studies,
    software engineers were confused about the structure, granularity, and what to
    include in the models, because no formal specification of DFDs exists. The participants
    in our experiment also showed some difficulties that could be resolved by a clear
    definition and well-established documentation of DFDs. Regarding DFDs'' structure,
    Faily et al. [\[39\]](#page-11-2) argued that they should not be enriched with
    additional groups of model items, since their simplicity and accessibility for
    human users might suffer. Instead, they proposed to use them together with other
    system representations. In contrast, Sion et al. argued in a position paper [\[40\]](#page-11-3)
    that using DFDs in their basic form is insufficient for threat modelling. Based
    on our findings, we argue that adding annotations to DFDs does not impede their
    accessibility and that security-enriched DFDs are well suited to support security
    analysis activities.


    In conclusion, no publications were found that empirically investigate the impact
    of DFDs (or other model representations) on the security analysis (or related
    activities) of microservice applications.


    # VIII. CONCLUSION


    <span id="page-9-1"></span>This paper presents the results of an empirical experiment
    conducted to investigate the impact of DFDs on software security analysis tasks.
    DFDs are widely used for security analysis and their varied adoption indicates
    a high confidence in their usefulness. To the best of our knowledge, the presented
    results are the first to investigate these assumptions and can confirm a positive
    impact of DFDs in the given context. We found, that participants performed significantly
    better concerning the analysis correctness of security analysis tasks when they
    were provided a DFD of the analysed application. Additionally, traceability information
    that links model items to artefacts in source code significantly improved their
    ability to provide correct evidence for their answers. Consequently, this paper
    serves as a basis for future research on specific applicabilities and properties
    of DFDs. Further, it can provide guidance in decisions on the adoption of model-based
    practices.


    #### ACKNOWLEDGEMENT


    This work was partly funded by the European Union''s Horizon 2020 programme under
    grant agreement No. 952647 (Assure-MOSS).


    #### REFERENCES


    - <span id="page-10-0"></span>[1] L. Sion, K. Yskout, D. Van Landuyt, W. Joosen,
    Solution-aware data flow diagrams for security threat modeling, in: Proceedings
    of the 33rd Annual ACM Symposium on Applied Computing, SAC ''18, Association for
    Computing Machinery, New York, NY, USA, 2018, p. 1425–1432. [doi:10.1145/3167132.3167285](https://doi.org/10.1145/3167132.3167285).

    - [2] S. Hernan, S. Lambert, T. Ostwald, A. Shostack, Threat modelinguncover security
    design flaws using the stride approach, MSDN Magazine (2006) 68–75.

    - [3] Microsoft Corporation, [Microsoft threat modeling tool 2016](https://www.microsoft.com/en-us/download/details.aspx?id=49168)
    (2016). URL<https://www.microsoft.com/en-us/download/details.aspx?id=49168>

    - <span id="page-10-1"></span>[4] P. Torr, Demystifying the threat modeling process,
    IEEE Security & Privacy 3 (5) (2005) 66–70. [doi:10.1109/MSP.2005.119](https://doi.org/10.1109/MSP.2005.119).

    - <span id="page-10-2"></span>[5] M. Abi-Antoun, D. Wang, P. Torr, Checking threat
    modeling data flow diagrams for implementation conformance and security, in: Proceedings
    of the 22nd IEEE/ACM International Conference on Automated Software Engineering,
    ASE ''07, Association for Computing Machinery, New York, NY, USA, 2007, p. 393–396.
    [doi:10.1145/1321631.](https://doi.org/10.1145/1321631.1321692) [1321692](https://doi.org/10.1145/1321631.1321692).

    - [6] M. Abi-Antoun, J. M. Barnes, Analyzing security architectures, in: Proceedings
    of the IEEE/ACM International Conference on Automated Software Engineering, ASE
    ''10, Association for Computing Machinery, New York, NY, USA, 2010, p. 3–12. [doi:10.1145/1858996.](https://doi.org/10.1145/1858996.1859001)
    [1859001](https://doi.org/10.1145/1858996.1859001).

    - [7] B. Berger, K. Sohr, R. Koschke, Automatically Extracting Threats from Extended
    Data Flow Diagrams, in: Engineering Secure Software and Systems, Vol. 9639, 2016,
    pp. 56–71. [doi:10.1007/](https://doi.org/10.1007/978-3-319-30806-7_4) [978-3-319-30806-7\\\_4](https://doi.org/10.1007/978-3-319-30806-7_4).

    - [8] C. Cao, S. Schneider, N. Diaz Ferreyra, S. Verweer, A. Panichella, R. Scandariato,
    CATMA: Conformance Analysis Tool For Microservice Applications, in: 2024 IEEE/ACM
    46th International Conference on Software Engineering: Companion Proceedings (ICSE-Companion),
    2024.

    - <span id="page-10-8"></span>[9] K. Tuma, R. Scandariato, M. Balliu, Flaws in
    Flows: Unveiling Design Flaws via Information Flow Analysis, in: 2019 IEEE International
    Conference on Software Architecture (ICSA), 2019, pp. 191–200. [doi:](https://doi.org/10.1109/ICSA.2019.00028)
    [10.1109/ICSA.2019.00028](https://doi.org/10.1109/ICSA.2019.00028).

    - [10] R. Chen, S. Li, Z. E. Li, From monolith to microservices: A dataflowdriven
    approach, in: 2017 24th Asia-Pacific Software Engineering Conference (APSEC),
    2017, pp. 466–475. [doi:10.1109/APSEC.](https://doi.org/10.1109/APSEC.2017.53)
    [2017.53](https://doi.org/10.1109/APSEC.2017.53).

    - [11] T. D. Stojanovic, S. D. Lazarevic, M. Milic, I. Antovic, Identifying microservices
    using structured system analysis, in: 2020 24th International Conference on Information
    Technology (IT), 2020, pp. 1–4. [doi:10.1109/IT48810.2020.9070652](https://doi.org/10.1109/IT48810.2020.9070652).

    - <span id="page-10-3"></span>[12] S. Li, H. Zhang, Z. Jia, Z. Li, C. Zhang, J.
    Li, Q. Gao, J. Ge, Z. Shan, A dataflow-driven approach to identifying microservices
    from monolithic applications, Journal of Systems and Software 157 (2019) 110380.
    [doi:](https://doi.org/10.1016/j.jss.2019.07.008) [10.1016/j.jss.2019.07.008](https://doi.org/10.1016/j.jss.2019.07.008).

    - <span id="page-10-4"></span>[13] N. Dragoni, S. Giallorenzo, A. Lluch-Lafuente,
    M. Mazzara, F. Montesi, R. Mustafin, L. Safina, Microservices: yesterday, today,
    and tomorrow, Springer International Publishing, 2016, Ch. 12, pp. 195–216. [doi:](https://doi.org/10.1007/978-3-319-67425-4_12)
    [10.1007/978-3-319-67425-4\\\_12](https://doi.org/10.1007/978-3-319-67425-4_12).

    - <span id="page-10-5"></span>[14] J. Lewis, M. Fowler, Microservices: a definition
    of this new architectural term, MartinFowler.com 25 (14-26) (2014) 12.

    - <span id="page-10-6"></span>[15] S. Schneider, R. Scandariato, Automatic extraction
    of security-rich dataflow diagrams for microservice applications written in java,
    Journal of Systems and Software 202 (2023) 111722. [doi:10.1016/j.jss.](https://doi.org/10.1016/j.jss.2023.111722)
    [2023.111722](https://doi.org/10.1016/j.jss.2023.111722).

    - <span id="page-10-7"></span>[16] T. DeMarco, Structure Analysis and System Specification,
    Springer Berlin Heidelberg, 1979. [doi:10.1007/978-3-642-48354-7\](https://doi.org/10.1007/978-3-642-48354-7_9)
    [\\_9](https://doi.org/10.1007/978-3-642-48354-7_9).

    - <span id="page-10-9"></span>[17] K. Tuma, R. Scandariato, M. Widman, C. Sandberg,
    Towards security threats that matter, in: S. K. Katsikas, F. Cuppens, N. Cuppens,
    C. Lambrinoudakis, C. Kalloniatis, J. Mylopoulos, A. Anton, S. Gritzalis (Eds.),
    ´ Computer Security, Springer International Publishing, Cham, 2018, pp. 47–62.
    [doi:10.1007/978-3-319-72817-9\\_4](https://doi.org/10.1007/978-3-319-72817-9_4).

    - <span id="page-10-10"></span>[18] S. Schneider, T. Ozen, M. Chen, R. Scandariato,
    microsecend: A dataset ¨ of security-enriched dataflow diagrams for microservice
    applications, in: 2023 IEEE/ACM 20th International Conference on Mining Software
    Repositories (MSR), 2023, pp. 125–129. [doi:10.1109/MSR59073.](https://doi.org/10.1109/MSR59073.2023.00030)
    [2023.00030](https://doi.org/10.1109/MSR59073.2023.00030).

    - <span id="page-10-11"></span>[19] B. Kitchenham, S. Pfleeger, L. Pickard, P.
    Jones, D. Hoaglin, K. El Emam, J. Rosenberg, Preliminary guidelines for empirical
    research in software engineering, IEEE Transactions on Software Engineering 28
    (8) (2002) 721–734. [doi:10.1109/TSE.2002.1027796](https://doi.org/10.1109/TSE.2002.1027796).

    - [20] N. Juristo, A. Moreno, Basics of Software Engineering Experimentation,
    2001. [doi:10.1007/978-1-4757-3304-4](https://doi.org/10.1007/978-1-4757-3304-4).

    - <span id="page-10-12"></span>[21] C. Wohlin, P. Runeson, M. Host, M. Ohlsson,
    B. Regnell, A. Wessl ¨ en, ´ Experimentation in Software Engineering, Springer,
    Germany, 2012. [doi:10.1007/978-3-642-29044-2](https://doi.org/10.1007/978-3-642-29044-2).

    - <span id="page-10-13"></span>[22] S. Schneider, N. E. Diaz Ferreyra, P.-J. Queval,
    G. Simhandl, U. Zdun, R. Scandariato, [Replication package,](https://github.com/tuhh-softsec/SANER2024_empirical_experiment_DFDs)
    2024. URL [https://github.com/tuhh-softsec/SANER2024](https://github.com/tuhh-softsec/SANER2024_empirical_experiment_DFDs)
    empirical [experiment](https://github.com/tuhh-softsec/SANER2024_empirical_experiment_DFDs)
    DFDs

    - <span id="page-10-14"></span>[23] I. Salman, A. T. Misirli, N. Juristo, Are
    students representatives of professionals in software engineering experiments?,
    in: Proceedings of the 37th International Conference on Software Engineering -
    Volume 1, ICSE ''15, IEEE Press, 2015, p. 666–676.

    - [24] M. Svahnberg, A. Aurum, C. Wohlin, Using students as subjects an empirical
    evaluation, in: Proceedings of the Second ACM-IEEE International Symposium on
    Empirical Software Engineering and Measurement, ESEM ''08, Association for Computing
    Machinery, New York, NY, USA, 2008, p. 288–290. [doi:10.1145/1414004.1414055](https://doi.org/10.1145/1414004.1414055).

    - <span id="page-10-15"></span>[25] D. Falessi, N. Juristo, C. Wohlin, B. Turhan,
    J. Munch, A. Jedlitschka, ¨ M. Oivo, Empirical software engineering experts on
    the use of students and professionals in experiments, Empirical Softw. Engg. 23
    (1) (2018) 452–489. [doi:10.1007/s10664-017-9523-3](https://doi.org/10.1007/s10664-017-9523-3).

    - <span id="page-10-16"></span>[26] D. Budgen, A. J. Burn, O. P. Brereton, B.
    A. Kitchenham, R. Pretorius, Empirical evidence about the uml: a systematic literature
    review, Software: Practice and Experience 41 (4) (2011) 363–392. [doi:https:](https://doi.org/https://doi.org/10.1002/spe.1009)
    [//doi.org/10.1002/spe.1009](https://doi.org/https://doi.org/10.1002/spe.1009).

    - <span id="page-10-17"></span>[27] C. Gravino, G. Scanniello, G. Tortora, Source-code
    comprehension tasks supported by uml design models: Results from a controlled
    experiment and a differentiated replication, Journal of Visual Languages & Computing
    28 (2015) 23–38. [doi:https://doi.org/10.1016/j.](https://doi.org/https://doi.org/10.1016/j.jvlc.2014.12.004)
    [jvlc.2014.12.004](https://doi.org/https://doi.org/10.1016/j.jvlc.2014.12.004).

    - <span id="page-10-18"></span>[28] C. Gravino, G. Tortora, G. Scanniello, An
    empirical investigation on the relation between analysis models and source code
    comprehension, in: Proceedings of the 2010 ACM Symposium on Applied Computing,
    SAC ''10, Association for Computing Machinery, New York, NY, USA, 2010, p. 2365–2366.
    [doi:10.1145/1774088.1774576](https://doi.org/10.1145/1774088.1774576).

    - <span id="page-10-19"></span>[29] G. Scanniello, C. Gravino, M. Risi, G. Tortora,
    G. Dodero, Documenting design-pattern instances: A family of experiments on source-code
    comprehensibility, ACM Trans. Softw. Eng. Methodol. 24 (3) (may 2015). [doi:10.1145/2699696](https://doi.org/10.1145/2699696).

    - <span id="page-10-20"></span>[30] E. Arisholm, L. Briand, S. Hove, Y. Labiche,
    The impact of uml documentation on software maintenance: an experimental evaluation,
    IEEE Transactions on Software Engineering 32 (6) (2006) 365–381. [doi:10.1109/TSE.2006.59](https://doi.org/10.1109/TSE.2006.59).

    - <span id="page-10-21"></span>[31] J. A. Cruz-Lemus, M. Genero, D. Caivano, S.
    Abrahao, E. Insfr ˜ an, ´ J. A. Cars´ı, Assessing the influence of stereotypes
    on the comprehension of uml sequence diagrams: A family of experiments, Information
    and Software Technology 53 (12) (2011) 1391–1403. [doi:10.1016/j.](https://doi.org/10.1016/j.infsof.2011.07.002)
    [infsof.2011.07.002](https://doi.org/10.1016/j.infsof.2011.07.002).

    - <span id="page-10-22"></span>[32] M. Genero, J. A. Cruz-Lemus, D. Caivano, S.
    Abrahao, E. Insfran, ˜ J. A. Cars´ı, Assessing the influence of stereotypes on
    the comprehension of uml sequence diagrams: A controlled experiment, in: K. Czarnecki,
    I. Ober, J.-M. Bruel, A. Uhl, M. Volter (Eds.), Model Driven Engi- ¨ neering Languages
    and Systems, Springer Berlin Heidelberg, Berlin, Heidelberg, 2008, pp. 280–294.

    - <span id="page-10-23"></span>[33] F. Ricca, M. Di Penta, M. Torchiano, P. Tonella,
    M. Ceccato, How developers'' experience and ability influence web application
    comprehension tasks supported by uml stereotypes: A series of four experiments,
    IEEE Transactions on Software Engineering 36 (1) (2010) 96–118. [doi:10.1109/TSE.2009.69](https://doi.org/10.1109/TSE.2009.69).

    - <span id="page-10-24"></span>[34] M. Staron, L. Kuzniarz, C. Wohlin, Empirical
    assessment of using stereotypes to improve comprehension of uml models: A set
    of experiments, Journal of Systems and Software 79 (5) (2006) 727–742, quality
    Software. [doi:10.1016/j.jss.2005.09.014](https://doi.org/10.1016/j.jss.2005.09.014).

    - <span id="page-10-25"></span>[35] M. Staron, L. Kuzniarz, C. Thurn, An empirical
    assessment of using stereotypes to improve reading techniques in software inspections,
    SIGSOFT Softw. Eng. Notes 30 (4) (2005) 1–7. [doi:10.1145/](https://doi.org/10.1145/1082983.1083308)
    [1082983.1083308](https://doi.org/10.1145/1082983.1083308).

    - <span id="page-10-26"></span>[36] M. C. Otero, J. J. Dolado, An empirical comparison
    of the dynamic


    modeling in oml and uml, Journal of Systems and Software 77 (2) (2005) 91–102.
    [doi:10.1016/j.jss.2004.11.022](https://doi.org/10.1016/j.jss.2004.11.022).


    - <span id="page-11-0"></span>[37] I. Reinhartz-berger, D. Dori, Opm vs. uml—experimenting
    with comprehension and construction of web application models, Empirical Software
    Engineering 10 (2005) 57–80. [doi:10.1023/B:EMSE.](https://doi.org/10.1023/B:EMSE.0000048323.40484.e0)
    [0000048323.40484.e0](https://doi.org/10.1023/B:EMSE.0000048323.40484.e0).

    - <span id="page-11-1"></span>[38] K. Bernsmed, D. Cruzes, M. Jaatun, M. Iovan,
    Adopting threat modelling in agile software development projects, Journal of Systems
    and Software 183 (2021) 111090. [doi:10.1016/j.jss.2021.](https://doi.org/10.1016/j.jss.2021.111090)
    [111090](https://doi.org/10.1016/j.jss.2021.111090).

    - <span id="page-11-2"></span>[39] S. Faily, R. Scandariato, A. Shostack, L. Sion,
    D. Ki-Aries, Contextualisation of data flow diagrams for security analysis, in:
    H. Eades III, O. Gadyatskaya (Eds.), Graphical Models for Security, Springer International
    Publishing, Cham, 2020, pp. 186–197.

    - <span id="page-11-3"></span>[40] L. Sion, K. Yskout, D. Van Landuyt, A. van
    den Berghe, W. Joosen, Security threat modeling: Are data flow diagrams enough?,
    in: Proceedings of the IEEE/ACM 42nd International Conference on Software Engineering
    Workshops, ICSEW''20, Association for Computing Machinery, New York, NY, USA,
    2020, p. 254–257. [doi:10.1145/3387940.](https://doi.org/10.1145/3387940.3392221)
    [3392221](https://doi.org/10.1145/3387940.3392221).'
  decisions:
    language: '- Qualified. Reason: English Paper'
    evaluation_prompt: Qualified.
    related_work_prompt: Qualified
    novelty_prompt: Qualified.
    review_only_prompt: Qualified
  llm_input_used: '## Abstract

    Models of software systems are used throughout the software development

    lifecycle. Dataflow diagrams (DFDs), in particular, are well-established

    resources for security analysis. Many techniques, such as threat modelling, are

    based on DFDs of the analysed application. However, their impact on the

    performance of analysts in a security analysis setting has not been explored

    before. In this paper, we present the findings of an empirical experiment

    conducted to investigate this effect. Following a within-groups design,

    participants were asked to solve security-relevant tasks for a given

    microservice application. In the control condition, the participants had to

    examine the source code manually. In the model-supported condition, they were

    additionally provided a DFD of the analysed application and traceability

    information linking model items to artefacts in source code. We found that the

    participants (n = 24) performed significantly better in answering the analysis

    tasks correctly in the model-supported condition (41% increase in analysis

    correctness). Further, participants who reported using the provided

    traceability information performed better in giving evidence for their answers

    (315% increase in correctness of evidence). Finally, we identified three open

    challenges of using DFDs for security analysis based on the insights gained in

    the experiment.


    ## Introduction

    Dataflow Diagrams (DFDs) are integral to many software security analysis techniques.
    For instance, they are required by prominent security assessment techniques [\[1\]](#page-10-0)–[\[4\]](#page-10-1).
    They are also the program representation chosen in many model-based security approaches
    [\[5\]](#page-10-2)–[\[12\]](#page-10-3). As such, they can help software engineers
    build more secure software systems. However, their impact on security analysis
    by their mere provision has not been investigated before to the best of our knowledge.
    DFDs offer a high-level yet detailed representation of applications'' architecture.
    Enriched with annotations representing, e.g., employed security mechanisms, they
    offer easy accessibility of the architectural security. We hypothesize that providing
    users with a DFD of an application enables them to analyse the application''s
    security properties with higher correctness.


    To investigate this hypothesis, we conducted an empirical experiment. This paper
    reports on our findings. The experiment was performed with master students who
    solved tasks related to software security analysis activities. For this, they
    received the source code and a textual description of an open-source microservice
    application, and six tasks to answer. We chose microservice applications as the
    target of analysis because the distributed nature of this architectural style
    poses additional challenges in terms of cognitive load to security analysts. Systems
    following the microservice architecture split their codebase into multiple independent
    microservices, where each fulfils a part of the business functionality and can
    be independently developed and deployed [\[13\]](#page-10-4), [\[14\]](#page-10-5).
    The resulting codebase can be challenging to oversee in analysis scenarios.


    The experiment followed a within-groups design. In the model-supported condition,
    participants received a DFD and traceability information of the analysed application
    in addition to the source code and textual description provided in the control
    condition. We chose DFDs created by *Code2DFD*, a tool for the automatic extraction
    of DFDs from source code [\[15\]](#page-10-6), as these contain extensive security
    annotations. We infer insights on the impact of DFDs on security analysis by comparing
    the participants'' performance in analysis correctness, correctness of evidence,
    and time in the two conditions. Specifically, we answer the following research
    questions:


    ® RQ 1: Do security-annotated architectural models of applications, specifically
    DFDs, support developers in the security analysis of the applications?


    A crucial part of security analysis is identifying and localizing security (and
    other) features in source code. To assess whether models with extensive security
    annotations can support developers and security experts in this activity, the
    participants in our experiment solved tasks that require the identification of
    implemented security mechanisms and other relevant system properties. We quantified
    their answers and compared the scores between the two conditions. Additionally,
    we asked them about the perceived usefulness and analysed the answers.


    ® RQ 2: Does access to and use of traceability information improve the ability
    of the security analysts to provide correct evidence for their analysis findings?


    Traceability information establishes the validity of model items by referencing
    corresponding artefacts in the source code. It can provide value for security
    analysis, since in scenarios such as software security assessment or certification,
    evidence has to be given for the reached findings. The participants provided evidence
    for their answers in the form of locations in the source code. We examined its
    correctness and compared the scores of those participants who reported using the
    traceability information frequently and those who did not. ® RQ 3: What is the
    experience in using security-annotated DFDs for security analysis, specifically
    concerning the usefulness and accessibility?


    Users'' acceptance of offered tools and techniques is crucial for using resources
    such as DFDs for security analysis. Thus, the participants'' perceived usefulness
    of the DFDs is essential to judge their suitability for real-world application.
    Further, the information presented by DFDs has to be conveyed to the users efficiently
    and accessible. To judge this aspect, we asked the participants about their experience
    using the DFDs in the model-supported condition and analysed their responses.


    ® RQ 4: What are the open challenges of using securityannotated DFDs in the context
    of security analysis?


    Based on the insights gained during the empirical experiment and the analysis
    of the results, we identified and formulated open challenges that should be addressed
    in future work.


    The rest of this paper is structured as follows: Section [II](#page-1-0) introduces
    the used DFDs; Section [III](#page-1-1) describes the experiment''s design, i.e.,
    the methodology; the results are presented in Section [IV](#page-4-0) and discussed
    in Section [V;](#page-7-0) Section [VI](#page-8-0) describes limitations of this
    work; Section [VII](#page-9-0) presents related work; and Section [VIII](#page-9-1)
    concludes the paper.'
  token_usage: 6031
  time_usage: 2.449629068374634
- title: Finding XPath Bugs in XML Document Processors via Differential Testing
  abstract: 'Extensible Markup Language (XML) is a widely used file format for data

    storage and transmission. Many XML processors support XPath, a query language

    that enables the extraction of elements from XML documents. These systems can

    be affected by logic bugs, which are bugs that cause the processor to return

    incorrect results. In order to tackle such bugs, we propose a new approach,

    which we realized as a system called XPress. As a test oracle, XPress relies on

    differential testing, which compares the results of multiple systems on the

    same test input, and identifies bugs through discrepancies in their outputs. As

    test inputs, XPress generates both XML documents and XPath queries. Aiming to

    generate meaningful queries that compute non-empty results, XPress selects a

    so-called targeted node to guide the XPath expression generation process. Using

    the targeted node, XPress generates XPath expressions that reference existing

    context related to the targeted node, such as its tag name and attributes,

    while also guaranteeing that a predicate evaluates to true before further

    expanding the query. We tested our approach on six mature XML processors,

    BaseX, eXist-DB, Saxon, PostgreSQL, libXML2, and a commercial database system.

    In total, we have found 20 unique bugs in these systems, of which 25 have been

    verified by the developers, and 12 of which have been fixed. XPress is

    efficient, as it finds 12 unique bugs in BaseX in 24 hours, which is 2x as fast

    as naive random generation. We expect that the effectiveness and simplicity of

    our approach will help to improve the robustness of many XML processors.'
  url: http://arxiv.org/abs/2401.05112v1
  keywords: ''
  document: "# Finding XPath Bugs in XML Document Processors via Differential Testing\n\
    \n[Shuxin Li](https://orcid.org/0009-0003-0468-2029)<sup>∗</sup>\n\nshuxin.li.lv@gmail.com\
    \ Southern University of Science and Technology China\n\nABSTRACT\n\nExtensible\
    \ Markup Language (XML) is a widely used file format for data storage and transmission.\
    \ Many XML processors support XPath, a query language that enables the extraction\
    \ of elements from XML documents. These systems can be affected by logic bugs,\
    \ which are bugs that cause the processor to return incorrect results. In order\
    \ to tackle such bugs, we propose a new approach, which we realized as a system\
    \ called XPress. As a test oracle, XPress relies on differential testing, which\
    \ compares the results of multiple systems on the same test input, and identifies\
    \ bugs through discrepancies in their outputs. As test inputs, XPress generates\
    \ both XML documents and XPath queries. Aiming to generate meaningful queries\
    \ that compute non-empty results, XPress selects a so-called targeted node to\
    \ guide the XPath expression generation process. Using the targeted node, XPress\
    \ generates XPath expressions that reference existing context related to the targeted\
    \ node, such as its tag name and attributes, while also guaranteeing that a predicate\
    \ evaluates to true before further expanding the query. We tested our approach\
    \ on six mature XML processors, BaseX, eXist-DB, Saxon, PostgreSQL, libXML2, and\
    \ a commercial database system. In total, we have found 27 unique bugs in these\
    \ systems, of which 25 have been verified by the developers, and 20 of which have\
    \ been fixed. XPress is efficient, as it finds 12 unique bugs in BaseX in 24 hours,\
    \ which is 2× as fast as naive random generation. We expect that the effectiveness\
    \ and simplicity of our approach will help to improve the robustness of many XML\
    \ processors.\n\n## CCS CONCEPTS\n\n• Software and its engineering → Software\
    \ testing and debugging.\n\n### KEYWORDS\n\nXML processors, XPath generation,\
    \ differential testing\n\n#### ACM Reference Format:\n\nShuxin Li and Manuel Rigger.\
    \ 2024. Finding XPath Bugs in XML Document Processors via Differential Testing.\
    \ In 2024 IEEE/ACM 46th International Conference on Software Engineering (ICSE\
    \ '24), April 14–20, 2024, Lisbon, Portugal. ACM, New York, NY, USA, [12](#page-11-0)\
    \ pages. [https://doi.org/10.1145/3597503.3639208](https://doi.org/10.1145/3597503.3639208\
    \ )\n\n© 2024 Copyright held by the owner/author(s).\n\n[Manuel Rigger](https://orcid.org/0000-0001-8303-2099)\
    \ rigger@nus.edu.sg National University of Singapore Singapore\n\n### 1 INTRODUCTION\n\
    \nExtensible Markup Language (XML) is a widely used file format for data storage\
    \ and transmission. XPath is an expression language, which provides the ability\
    \ to navigate through XML documents to select wanted nodes. XPath is also at the\
    \ core of other XML query language standards such as XSLT [\\[7\\]](#page-10-0)\
    \ and XQuery [\\[6\\]](#page-10-1), making it a fundamental XML query language.\n\
    \nVarious XML document processors have been developed for extracting information\
    \ from XML documents efficiently. We loosely categorize them depending on whether\
    \ they can store XML documents in addition to processing them—that is, whether\
    \ they are Database Management Systems (DBMSs), or provide only processing functionality.\
    \ In terms of DBMSs specialized for XML, popular examples include BaseX [\\[8\\\
    ]](#page-10-2) and eXist-DB [\\[10\\]](#page-10-3). Many generalpurpose DBMSs\
    \ such as Oracle Database [\\[14\\]](#page-10-4), MySQL [\\[13\\]](#page-10-5),\
    \ and PostgreSQL [\\[15\\]](#page-10-6) have adopted support for processing XML\
    \ documents. In fact, out of the 10 most popular DBMSs according to the DB-engines\
    \ ranking [\\[9\\]](#page-10-7), 6 support at least partial XML document parsing.\
    \ A popular example of a processor without storage functionality is Saxon. Saxon\
    \ [\\[18\\]](#page-11-1) is an in-memory XML processor that can be either used\
    \ in a standalone way or embedded as a library. Finally, libxml2 [\\[12\\]](#page-10-8)\
    \ is a popular XML processing library written in C. XPath is supported by all\
    \ of these processors.\n\nXML document processors can be affected by logic bugs.\
    \ Logic bugs are bugs that cause the XML processor to produce incorrect results\
    \ without crashing the system, meaning that they can often go unnoticed. In order\
    \ to find such bugs, developers mainly rely on test suites such as the XPathMark\
    \ test suite for XPath [\\[25\\]](#page-11-2), the W3C qt3 test suite [\\[19\\\
    ]](#page-11-3), or hand-written unit tests. Manually writing tests requires much\
    \ effort, and it is challenging to comprehensively cover the XML processors' functionality.\
    \ To find logic bugs automatically, a so-called test oracle is required that can\
    \ determine whether the system's output is correct in order to find logic bugs.\
    \ Todic and Uzelac have proposed an automated testing technique for SQLServer's\
    \ index support; their test oracle compared the results of a given query with\
    \ and without index definition [\\[41\\]](#page-11-4). A limitation of this technique\
    \ is that it is applicable only to finding index-related bugs in DBMSs. To the\
    \ best of our knowledge, no other test oracles have been proposed in this context.\n\
    \nIn order to detect XPath-related bugs in XML processors, we propose differential\
    \ testing as an oracle. The core idea of differential testing is to use one input\
    \ that is executed using multiple systems; any discrepancy in the results indicates\
    \ a potential bug in the system. For testing XML processors, the input for the\
    \ XML processors under test is an XML document and XPath expression, and the results\
    \ are a sequence of XML nodes or values. Differential testing has been successfully\
    \ applied in various related domains,\n\n<sup>∗</sup>Work done during an internship\
    \ at the National University of Singapore.\n\nICSE '24, April 14–20, 2024, Lisbon,\
    \ Portugal\n\nThis is the author's version of the work. It is posted here for\
    \ your personal use. Not for redistribution. The definitive Version of Record\
    \ was published in 2024 IEEE/ACM 46th International Conference on Software Engineering\
    \ (ICSE '24), April 14–20, 2024, Lisbon, Portugal, [https://doi.org/10.1145/3597503.3639208.](https://doi.org/10.1145/3597503.3639208\
    \ )\n\nsuch as relational DBMSs [\\[38\\]](#page-11-5), compilers [\\[43,](#page-11-6)\
    \ [45\\]](#page-11-7), JVM implementations [\\[23\\]](#page-11-8), ORM systems\
    \ [\\[39\\]](#page-11-9), and graph DBMSs [\\[44,](#page-11-10) [46\\]](#page-11-11).\
    \ Its effectiveness hinges on two main requirements. First, multiple systems to\
    \ be compared must be available. As discussed above, various XML processors with\
    \ XPath support exist. Second, for any valid input, the systems should produce\
    \ the same result, since otherwise, a differential-testing approach raises many\
    \ false alarms. This requirement is not always met, for example, when applying\
    \ differential testing to relational DBMSs, where the \"common SQL subset is relatively\
    \ small and changes with each release\" and NULL handling differs between DBMSs\
    \ [\\[38\\]](#page-11-5). As we found, XPath is a well-defined language by the\
    \ W3C standard, and XPath implementations of the same standard follow the same\
    \ language rules, making differential testing highly applicable.\n\nTo generate\
    \ test cases, we propose an approach that selects a so-called targeted node from\
    \ the XML document, based on which we generate a query that is guaranteed to fetch\
    \ at least that node. As such, it tackles two challenges that might prevent testing\
    \ from exercising interesting behaviors. First, by generating the query based\
    \ on the targeted node, we can guarantee that we access a tag name, attributes,\
    \ and relative paths that exist with respect to at least the targeted node. Second,\
    \ by rectifying predicates so that they evaluate to true for the targeted node,\
    \ we can ensure that the result set is non-empty even for complex queries. A similar\
    \ highlevel idea has been proposed in the context of testing relational DBMSs,\
    \ called Pivoted Query Synthesis (PQS) [\\[36\\]](#page-11-12), where a pivot\
    \ row was selected, based on which predicates were rectified to return true. Apart\
    \ from applying that idea in a different context, we also propose a different\
    \ rectification strategy that eschews mirroring the predicate's execution logic\
    \ in the testing tool, which was required for realizing PQS.\n\nWe implemented\
    \ our approach as a tool named XPress,[1](#page-1-0) which, to the best of our\
    \ knowledge, is the first general automated testing tool for XML processors, and\
    \ tested our method on six mature and widely-used XML processors BaseX, eXist-DB,\
    \ Saxon, PostgreSQL, libXML2 and a commercial DBMS. The experimental results show\
    \ that our approach is effective in detecting XPath-related logic bugs in XML\
    \ processors. We found 27 previously unknown unique bugs, not covered by existing\
    \ test suites, of which 19 were logic bugs. 25 of them have been confirmed, and\
    \ 20 of them have been fixed. Furthermore, these test cases have been integrated\
    \ into the aforementioned qt3 test suite, so that they can detect potential bugs\
    \ in XML processors that we have not tested. Our experiments demonstrate that\
    \ our proposed guided query generation process improves testing efficiency by\
    \ finding 2× more unique bugs within 24 hours in BaseX as compared to random generation.\
    \ Given the high effectiveness and efficiency of the approach, we believe it will\
    \ likely be adopted by developers of XML processors to improve their systems.\n\
    \nTo summarize, we make the following contributions:\n\n- We propose the first\
    \ general approach for automatically testing XML processors in order to find logic\
    \ bugs.\n- We implemented and evaluated our approach on six widelyused XML process\
    \ systems, which successfully found 27 previously-unknown unique bugs.\n\n```\n\
    <Books>\n   ① <Book id=\"1\" year=\"2020\">\n       <Author name=\"Sam\"/>\n \
    \      <Author name=\"Bob\"/>\n       A fairy tale.\n     </Book>\n   ② <Book\
    \ id=\"2\">\n       <Author name=\"Alice\"/>\n       Fiction.\n     </Book>\n\
    \   ③ <Book id=\"3\" year=\"2023\">\n       History of fairy kingdom.\n     </Book>\n\
    </Books>\n                                 XPath:\n                          \
    \        //*[@id*(-1)<2]\n                                   BaseX: {}\n     \
    \                              Saxon: ①②③\n                                  \
    \ eXist: ①②③\n```\nFigure 1: Example XML and motivating example.\n\n### <span\
    \ id=\"page-1-3\"></span>2 BACKGROUND\n\nRunning example. Figure [1](#page-1-1)\
    \ shows a running example that we will subsequently use to explain basic XML and\
    \ XPath concepts and outline the challenges of automated testing as applied in\
    \ this context. The left shows an XML document with the root node Books, while\
    \ the right shows an XPath expression //\\*[@id \\* -1 < 2]. We adapted this example\
    \ from a bug-inducing test case that XPress discovered.[2](#page-1-2) As shown,\
    \ for the query on the document, BaseX returned an empty result, while both Saxon\
    \ and eXist returned all three Book nodes.\n\nXML. Extensible Markup Language\
    \ (XML) is a text format for describing structured data. XML documents are trees\
    \ that consist of nodes, as illustrated in Figure [1.](#page-1-1) An XML document\
    \ has a single root element node (see <Books>). Each element node has a tag name\
    \ (see Books, Book, and Author). Element nodes can include attribute nodes. For\
    \ example, two of the <Book> nodes have both attribute nodes id and year. An element\
    \ node can also include child element nodes; in the example, the <Books> node\
    \ contains three child element nodes <Book>. Element nodes can hold text contents,\
    \ which can be of any defined data type. For the <Book> node with attribute id\
    \ = 1, the text content it holds is \"A fairy tale\". Attribute nodes are disallowed\
    \ from holding child nodes. In the example, id and year are integer-typed attribute\
    \ nodes and the name attributes are string-typed attribute nodes.\n\nXPath. The\
    \ XPath language is an expression language that allows navigating the XML tree\
    \ and hierarchic addressing of the element nodes. XPath is at the core of both\
    \ eXtensible Stylesheet Language - Transformation (XSLT) [\\[7\\]](#page-10-0)\
    \ and XQuery, a more expressive query language for XML [\\[6\\]](#page-10-1).\
    \ XSLT transforms XML documents into other formats and the XQuery language is\
    \ a super-set of XPath expressions. XQuery extends XPath to provide functionalities\
    \ such as node constructors and SQL-like clauses.\n\nXPath structure. An XPath\
    \ expression describes the selection and transformation of nodes of the XML tree.\
    \ Figure [2](#page-2-0) shows a simplified XPath 3.0 [\\[4\\]](#page-10-9) grammar\
    \ using EBNF notation from the W3C XML 1.0 standard [\\[3\\]](#page-10-10). We\
    \ introduce the non-established terms Section and Section Prefix to describe our\
    \ generation approach in\n\n<span id=\"page-1-0\"></span><sup>1</sup>Our artifact\
    \ is publicly available at<https://zenodo.org/records/10473926>\n\n<span id=\"\
    page-1-2\"></span><sup>2</sup><https://github.com/BaseXdb/basex/issues/2188>\n\
    \n<span id=\"page-2-0\"></span>**XPathExpression** ::= Section+ **SectionPrefix**\
    \ ::= (\"/\" | \"//\") AxisStep **AxisStep** ::= Axis\\* NameTest **Section**\
    \ ::= SectionPrefix Predicate\\*\n\nFigure 2: Simplified structure of XPath expressions.\n\
    \nSection [3.2.](#page-3-0) XPath expressions consist of one or more sections,\
    \ and a section contains one section prefix followed by zero or more predicates.\
    \ In Figure [1,](#page-1-1) the XPath expression //\\*[@id\\*(-1)<2] consists\
    \ of a single section with section prefix //\\* and a single predicate [@id\\\
    *(-1)<2]. Each section prefix starts with either / or //. / is called the path\
    \ operator, which accepts a node sequence as the left-hand operand and orders\
    \ it in document order while eliminating duplicate nodes. // represents the abbreviated\
    \ relative path /descendant-or-self::node()/, which matches the current context\
    \ and all descendant nodes of the current context, regardless of the intermediate\
    \ path. An axis step consists of an optional axis and a name test.\n\nXPath axes.\
    \ Axes define the relationship between selected nodes and current context nodes.\
    \ For example, the axis parent:: selects all parent nodes of current context nodes.\
    \ If omitted, it is equivalent to child::, which selects all direct children nodes\
    \ of current context nodes. A name test is a string literal to fetch only nodes\
    \ with the same tag name. It could also be a wildcard \\*, which matches all nodes\
    \ without applying filters. The section prefix //\\* in the example selects all\
    \ descendant nodes of the document node, which is all element nodes in the document.\n\
    \nXPath predicates. Predicates in XPath include positional predicates and boolean\
    \ predicates. Positional predicates contain an expression that evaluates to a\
    \ single integer and select only values whose position in the context matches\
    \ the integer value. In the XPath expression /Books/Book[1], [1] is a positional\
    \ predicate and selects only the first child of <Books>, which is the <Book> node\
    \ with @id=1. Boolean predicates evaluate current context nodes to a boolean value\
    \ according to a given expression and only nodes for which the predicate evaluates\
    \ to true are selected. In Figure [1,](#page-1-1) [@id \\* -1 < 2] is a boolean\
    \ predicate. The query //\\*[@id \\* -1 < 2] selects all nodes in the XML document\
    \ with attribute id that satisfy id \\* -1 < 2. The three nodes with tag name\
    \ Book in the document have attribute id, and all satisfy the condition. Therefore,\
    \ if correctly evaluated, this query should return all three Book nodes.\n\nLogic\
    \ bug. For the test input in Figure [1,](#page-1-1) systems like Saxon and eXist-DB\
    \ both returned a result set with three Book nodes, while BaseX returned an empty\
    \ result set. The difference between the processors indicates a potential bug.\
    \ Based on our manual analysis, we suspected that BaseX computed an incorrect\
    \ result, which is why we reported it to the BaseX developers. They fixed the\
    \ bug quickly. The reason for this bug was an incorrect simplification of the\
    \ arithmetic expression x \\* a > b to x > b / a. When the divisor is a negative\
    \ number, the original operator > should be reversed to <.\n\nXPath standard.\
    \ There are majorly two different standards of XPath implementations in use today,\
    \ which we need to consider in our work. The XPath 1.0 standard was the first\
    \ version. As a superset of XPath 1.0, the XPath 3.0 standard is the latest standard\
    \ of the XPath language and provides more functionalities such as advanced data\
    \ types and functions [\\[5\\]](#page-10-11). Most multi-model DBMSs, which support\
    \ XPath queries, support only XPath 1.0 [\\[1\\]](#page-10-12) (e.g., Oracle,\
    \ MySQL, and PostgreSQL). While some specialized XML processors support also only\
    \ XPath 1.0 (e.g., libXML2), others support the more recent XPath 3.0 standard\
    \ (e.g., BaseX, eXist-DB, and Saxon).\n\nXPath versions and differential testing.\
    \ The same queries might produce different results under different standards.\
    \ For example, for the XPath expression Book/@name = false(), under the XPath\
    \ 1.0 standard, the expression is expected to return true. @name is first cast\
    \ into its equivalent boolean value. In the current case <Book> has no name attribute,\
    \ therefore, an empty node set is returned. The equivalent boolean value evaluates\
    \ to false for empty nodes. Comparing false to false is equal, therefore true\
    \ is returned. Under the XPath 3.0 standard, however, the result is expected to\
    \ be false. @name returns an empty sequence and equality comparison between an\
    \ empty sequence and a boolean value false would evaluate to false. Thus, applying\
    \ differential testing to XML processors supporting different standards is infeasible.\n\
    \n### 3 APPROACH\n\nFigure [3](#page-3-1) shows an overview of the approach using\
    \ the same example as in Figure [1.](#page-1-1) At a high level, our approach\
    \ consists of three main steps. First, we randomly generate an XML document as\
    \ the context for the following queries (step ○1 ). We then generate an XPath\
    \ expression that we will subsequently validate (step ○2 to step ○5 ). Finally,\
    \ we execute the XPath expression on the XML document using all engines under\
    \ test and compare the resulting outputs to detect potential bugs (step ○6 ).\
    \ In the subsequent subsections, we explain these steps in reverse order to reflect\
    \ their importance.\n\nWe guide the XPath expression generation towards queries\
    \ that reference nodes and attributes present in the XML document and result in\
    \ non-empty result sets based on the intuition that they are more likely to stress\
    \ the underlying logic of the tested systems. To generate XPath expressions with\
    \ non-empty result sets, we construct the query section-by-section and ensure\
    \ that a non-empty result set is produced before proceeding with the next section.\
    \ Each section consists of a section prefix and predicates, and we first generate\
    \ the prefix (step ○2 ) and then the predicate. By restricting the section prefix,\
    \ we guarantee that the result contains at least one node. From the nodes selected\
    \ by the section prefix, we randomly select a node as a target (step ○3 ). We\
    \ then generate a predicate aiming to select the targeted node using a bottom-up\
    \ tree construction method (step ○4 ). We rectify the predicate to ensure that\
    \ the result set contains the targeted node (step ○5 ). We repeat this process\
    \ until the XPath query reaches the desired length.\n\n### <span id=\"page-2-1\"\
    ></span>3.1 Differential Testing for XML Processors\n\nAs detailed subsequently,\
    \ differential testing enables us to find both logic bugs as well as internal\
    \ errors when comparing the results of XML processors implementing the same XPath\
    \ standard.\n\nQuery execution. When passing XML documents and XPath queries to\
    \ different systems, we must account for the different\n\n<span id=\"page-3-1\"\
    ></span>![](_page_3_Figure_2.jpeg)\n\n**Repeat process until generated XPath reaches\
    \ specified length**\n\nFigure 3: Overview of the approach implemented in XPress.\n\
    \nListing 1: Execution of XPath using Oracle Database\n\n```\nCREATE TABLE t (a\
    \ XMLType);\nINSERT INTO t VALUES (XMLType(XML)));\nSELECT XMLQuery(XPATH PASSING\
    \ a RETURNING CONTENT) FROM t;\n```\nListing 2: Execution of XPath using Saxon\
    \ in Java\n\n```\nXQueryExecutable exec = compiler.compile(XPATH);\nXQueryEvaluator\
    \ query = exec.load();\nquery.setContextItem(XML);\nXdmValue result = query.evaluate();\n\
    ```\ninput interfaces. For example, DBMSs use database connection interfaces to\
    \ store and query data, while Saxon can be used as a library. To abstract this,\
    \ we treat every XML processor implementation as a function that returns a result\
    \ set and expects two string values, namely an XML document XML and an XPath query\
    \ XPATH. Listing [1](#page-3-2) shows an implementation of this interface for\
    \ Oracle Database using SQL statements. It creates a table t, inserts the XML\
    \ document—the XMLType constructor is used to convert the string to an XML data\
    \ type—and uses an XMLQuery function call in a SELECT statement to compute the\
    \ result set. For BaseX and eXist-db, similar to the commands shown for Oracle\
    \ Database, we also start with an empty database and subsequently insert an XML\
    \ document. Listing [2](#page-3-3) shows an excerpt of the Java code for Saxon.\
    \ First, the call to compile converts the textual XPath query to an executable\
    \ object, which is then loaded. Unlike for the DBMSs, which require inserting\
    \ data into a database, for Saxon, the XML document is simply associated with\
    \ the query using the setContextItem call. The evaluate call computes the result,\
    \ which is returned for comparison.\n\nBug identification. We identify both logic\
    \ bugs and internal errors by comparing the returned results of different processors\
    \ on the same XML document and query. We identify logic bugs when the tested systems\
    \ return different node-set outputs for the same test cases. To parse and track\
    \ the results easily under different output formats, we use unique node ids to\
    \ identify element nodes. We detect internal errors as discrepancies with respect\
    \ to errors. Rather than checking for an exact error message match, we validate\
    \ whether all the systems produce an error, or all execute the XPath query successfully.\
    \ If only a subset of the systems report an error for the same XPath query, we\
    \ found a potential bug.\n\nDifferent XPath standards. Our approach and tool are\
    \ applicable to both XPath 1.0 and XPath 3.0. However, due to the differences\
    \ in the formats, only processors using the same standard can be tested. Functionality\
    \ that is supported only in XPath 3.0, can be disabled while generating test cases\
    \ for processors that implement XPath 1.0. For example, sequence functions, such\
    \ as subsequence, are defined only for the XPath 3.0 standard. When generating\
    \ test cases for XPath 1.0 processors, we omit to generate subsequence function\
    \ nodes for predicates. We did not encounter any functions or operators that were\
    \ removed in the XPath 3.0 standard, so all expressions that we generate when\
    \ testing XPath 1.0 processors can be used also when testing XPath 3.0 processors.\
    \ By comparing only processors with the same XPath standard against each other,\
    \ the difference in the results between different XPath standards (see Section\
    \ [2\\)](#page-1-3) has no influence on the testing process.\n\n### <span id=\"\
    page-3-0\"></span>3.2 XPath Expression Generation\n\nIn this section, we introduce\
    \ how we generate XPath queries. We encountered two main challenges that we had\
    \ to tackle when generating XPath expressions.\n\nNon-existent elements. Randomly\
    \ generated queries could be semantically correct, but reference non-existent\
    \ nodes or attributes. For the document in Figure [1,](#page-1-1) //Author[@id\
    \ < 1] is a valid XPath expression. However, none of the Author nodes contain\
    \ an id attribute. Thus, XPath returns an empty sequence for each node, causing\
    \ the predicate @id < 1 to evaluate to false. We believe that queries, where only\
    \ non-existent attributes or nodes are referenced, are less likely to exercise\
    \ the logic of the processors under test, as subsequent operations are likely\
    \ to evaluate to an empty sequence as well. Thus, we aim to avoid generating such\
    \ queries.\n\nEmpty results. Randomly generated predicates might likely evaluate\
    \ to false and cause queries to generate empty result sets. For the document in\
    \ Figure [1,](#page-1-1) the XPath predicate starts-with(text(), x) identifies\
    \ nodes whose text starts with x. If x is a randomly generated string, the possibility\
    \ is high that no nodes in the current result set match the condition. Consequently,\
    \ any use of the predicate would yield an empty result. Any subsequently added\
    \ section would yield an empty result as well, meaning that such queries would\
    \ be less likely to exercise the processor under test. Consequently, we want to\
    \ avoid generating such predicates, in particular, when\n\nthey involve multiple\
    \ sections. This relates to the first problem, as non-existent nodes or attributes\
    \ can also introduce empty results.\n\nApproach overview. We designed the XPath\
    \ generation process of XPress tackling the two aforementioned issues. To create\
    \ XPath expressions that refer to valid nodes and attributes to trigger deeper\
    \ logic of the system under test, we generate queries that reference existent\
    \ context relative to the so-called targeted node, such as its tag name and attributes\
    \ (steps ○3 and ○4 ). Since randomly generated predicates might miss the targeted\
    \ node from the result set, we rectify the generated expressions to ensure the\
    \ inclusion of the targeted node (step ○5 ).\n\nIterative section generation.\
    \ We create XPath expressions sectionby-section by executing step ○2 to step ○5\
    \ for each section, which allows us to ensure non-empty results after generating\
    \ each section. In the example, we first generate section /Books by selecting\
    \ <Books> as the targeted node, and after executing steps ○2 to step ○5 , the\
    \ result of /Books is non-empty—containing the node <Books>. Based on this, we\
    \ further proceed to generate the next section /Book[count(Author) > 1] starting\
    \ at step ○2 .\n\nSection prefix. We randomly generate one of the applicable section\
    \ prefixes. First, we randomly select the start of the section to be / or //.\
    \ We then retrieve the current context node sequence by executing the expression—/Books\
    \ in the example—on a processor. Based on the result, we include all possible\
    \ axes that would not lead to an empty result set by simple conditional checks.\
    \ We support all 11 axes described in the XPath 3.0 standard [\\[4\\]](#page-10-9).\
    \ For example, applying the axis /descendants:: will lead to a non-empty result,\
    \ if at least one non-leaf node exists in the current selection. From the possible\
    \ axes, we select a random one and apply it. When generating the section prefix\
    \ /Book, the axis step is implicit. It is equivalent to /Books/child::Book, which\
    \ selects all child nodes of the previously selected nodes. We again execute the\
    \ query and retrieve the result node-set. We use the result for the name test,\
    \ for which we either select a tag name from the result node-set, or use the wildcard\
    \ \\*. By doing so, we are again guaranteed a non-empty result set. In the example,\
    \ the tag name Book is selected and applied, resulting in the selection of all\
    \ three Book nodes. In our artifact, we include a table that details the conditional\
    \ checks for all 11 axes.\n\nTarget node selection. To generate targeted queries\
    \ that fetch at least one node, we select a so-called targeted node to guide the\
    \ predicate generation process. We use information about the target node, such\
    \ as its text content, the attributes it holds, and its relationship to other\
    \ nodes during the predicate generation. This is similar to the concept of the\
    \ pivot row in PQS [\\[36\\]](#page-11-12), which is a technique that has been\
    \ proposed to test relational DBMSs. After the generated predicate is applied,\
    \ we expect the target node to be included in the result node-set. In step ○3\
    \ , we select node 1 as the target node for the predicate generation process.\
    \ Constraining the context to exist for the targeted node does not affect the\
    \ evaluation of the expression on other candidate nodes and, therefore, still\
    \ allows finding bugs that are triggered only when referring to nodes' non-existing\
    \ attributes or child nodes.\n\nPredicate generation. We use a tree structure\
    \ to represent the predicate and take a bottom-up construction approach to enable\
    \ tracking of expression results along tree construction. We start generating\
    \ the predicate from a specific subject, which is either the targeted node or\
    \ a node sequence derived from the targeted node with equal probability. In the\
    \ example, we select the <Author> child node sequence from the targeted node as\
    \ the subject. We then iteratively apply random function nodes and supply function\
    \ parameters to construct the predicate, until the predicate reaches a desired\
    \ length. We keep track of the data type and value of the current subexpression\
    \ when constructing the predicate, by executing the subexpression on one randomly\
    \ chosen XML processor—we use this XML processor also for predicate rectification\
    \ and we subsequently refer to this XML processor as the designated XML processor.\
    \ We use the value and data type of the current sub-expression in the following\
    \ two ways: by (1) selecting function nodes of according data types and (2) supplying\
    \ arguments to reference existent context and triggering corner cases. Specifically,\
    \ we select a random function node from functions that could accept the value\
    \ of the current data type as input. A function node can either represent a function\
    \ or an operation. In the example, Author is a node sequence and count is a randomly\
    \ selected function from functions that accept node sequence as input. For function\
    \ nodes that require additional arguments, we supply arguments while taking the\
    \ current result value into consideration. As an example, when selecting attribute\
    \ values from node sequences, we use name tests referencing existent attributes.\
    \ For the = operator, we choose an operand that is equal to the current value\
    \ with a high probability of triggering the equal case which is of low probability\
    \ under random generation. Aside from constants, we also set the possibility for\
    \ operands to be other predicate trees. Through this, we support the generation\
    \ of expressions with multiple subject occurrences. Besides boolean predicates,\
    \ we also apply positional predicates to the XPath expression randomly.\n\nPredicate\
    \ rectification. Lastly, we rectify the generated predicate to guarantee that\
    \ the targeted node is contained in the final result set. We first execute the\
    \ generated predicate on the designated XML processor. If the result set misses\
    \ the targeted node, we rectify the predicate. To negate the predicate's result,\
    \ we can always apply a not operator. However, as shown in Algorithm [1,](#page-5-0)\
    \ we probabilistically apply more specific rectification for certain operators\
    \ to uncover additional potential bugs. For logical operators such as and, both\
    \ child expressions need to be modified to evaluate to true to contain the targeted\
    \ node, while or needs only modification of one random child expression. For comparison\
    \ operators, such as <=, we replace them with their opposite operators, which,\
    \ in the example, is >. Thus, the targeted node is guaranteed to be contained\
    \ in the result set.\n\n### 3.3 XML Generation\n\nIn this section, we outline\
    \ how we generate XML documents (step ○1 ), which we do not consider part of our\
    \ core contribution.\n\nTree creation. We use a bottom-up approach to generate\
    \ XML documents. We first generate a number of node templates, which we use to\
    \ generate XML nodes that have overlaps in terms of structure, as detailed below.\
    \ We select one of these nodes as a root element. For the remaining nodes, we\
    \ randomly assign each node to a parent. As XML documents support recursive structure,\
    \ we allow cyclic\n\n<span id=\"page-5-0\"></span>\n\n|  |  | Algorithm 1 Predicate\
    \ Rectification |\n|--|--|-------------------------------------|\n|--|--|-------------------------------------|\n\
    \n|     | 1: function RectifyPredicate(\U0001D45D\U0001D45F\U0001D452\U0001D451\
    \U0001D456\U0001D450\U0001D44E\U0001D461\U0001D452_\U0001D45B\U0001D45C\U0001D451\
    \U0001D452,<br>\U0001D461\U0001D44E\U0001D45F\U0001D454\U0001D452\U0001D461\U0001D452\
    \U0001D451_\U0001D45B\U0001D45C\U0001D451\U0001D452) |\n|-----|----------------------------------------------------------------|\n\
    | 2:  | \U0001D4501<br>← \U0001D45D\U0001D45F\U0001D452\U0001D451\U0001D456\U0001D450\
    \U0001D44E\U0001D461\U0001D452_\U0001D45B\U0001D45C\U0001D451\U0001D452.\U0001D459\
    \U0001D452 \U0001D453 \U0001D461\U0001D436ℎ\U0001D456\U0001D459\U0001D451    \
    \                         |\n| 3:  | \U0001D4502<br>← \U0001D45D\U0001D45F\U0001D452\
    \U0001D451\U0001D456\U0001D450\U0001D44E\U0001D461\U0001D452_\U0001D45B\U0001D45C\
    \U0001D451\U0001D452.\U0001D45F\U0001D456\U0001D454ℎ\U0001D461\U0001D436ℎ\U0001D456\
    \U0001D459\U0001D451                              |\n| 4:  | if targeted_node\
    \ in GetResult(\U0001D45D\U0001D45F\U0001D452\U0001D451\U0001D456\U0001D450\U0001D44E\
    \U0001D461\U0001D452_\U0001D45B\U0001D45C\U0001D451\U0001D452)<br>then       \
    \   |\n| 5:  | return                                                        \
    \ |\n| 6:  | if RandomProb() < 0.5 then                                     |\n\
    | 7:  | AddNot(\U0001D45D\U0001D45F\U0001D452\U0001D451\U0001D456\U0001D450\U0001D44E\
    \U0001D461\U0001D452_\U0001D45B\U0001D45C\U0001D451\U0001D452)               \
    \                          |\n| 8:  | return                                 \
    \                        |\n| 9:  | switch \U0001D45D\U0001D45F\U0001D452\U0001D451\
    \U0001D456\U0001D450\U0001D44E\U0001D461\U0001D452_\U0001D45B\U0001D45C\U0001D451\
    \U0001D452<br>do                                    |\n| 10: | case \U0001D45C\
    \U0001D45F \U0001D45C\U0001D45D\U0001D452\U0001D45F\U0001D44E\U0001D461\U0001D45C\
    \U0001D45F                                               |\n| 11: | if RandomProb()\
    \ < 0.5 then                                     |\n| 12: | RectifyPredicate(\U0001D450\
    1)                                           |\n| 13: | else                 \
    \                                          |\n| 14: | RectifyPredicate(\U0001D450\
    2)                                           |\n| 15: | end case             \
    \                                          |\n| 16: | case \U0001D44E\U0001D45B\
    \U0001D451 \U0001D45C\U0001D45D\U0001D452\U0001D45F\U0001D44E\U0001D461\U0001D45C\
    \U0001D45F                                              |\n| 17: | RectifyPredicate(\U0001D450\
    1)                                           |\n| 18: | RectifyPredicate(\U0001D450\
    2)                                           |\n| 19: | end case             \
    \                                          |\n| 20: | case \U0001D450\U0001D45C\
    \U0001D45A\U0001D45D\U0001D44E\U0001D45F\U0001D456\U0001D460\U0001D45C\U0001D45B\
    \ \U0001D45C\U0001D45D\U0001D452\U0001D45F\U0001D44E\U0001D461\U0001D45C\U0001D45F\
    \                                       |\n| 21: | ChangeToOpposite(\U0001D45D\
    \U0001D45F\U0001D452\U0001D451\U0001D456\U0001D450\U0001D44E\U0001D461\U0001D452\
    _\U0001D45B\U0001D45C\U0001D451\U0001D452)                               |\n|\
    \ 22: | end case                                                       |\n| 23:\
    \ | default:                                                       |\n| 24: |\
    \ AddNot(\U0001D45D\U0001D45F\U0001D452\U0001D451\U0001D456\U0001D450\U0001D44E\
    \U0001D461\U0001D452_\U0001D45B\U0001D45C\U0001D451\U0001D452)               \
    \                          |\n| 25: | end switch                             \
    \                        |\n| 26: | return                                   \
    \                      |\n\nrelationships. In Section 4, we provide details on\
    \ how we configured the number of nodes in a document.\n\nNode generation. We\
    \ introduce how each element node is instantiated. By default, XML documents do\
    \ not have to adhere to a specific schema, which is unlike, for example, relational\
    \ DBMSs. Nevertheless, we want to generate element nodes that have overlaps in\
    \ terms of structure, to test for more interesting behaviors. To that end, we\
    \ generate element nodes based on so-called node templates that we randomly generate.\
    \ A node template represents a type of node. For example, in Figure [1,](#page-1-1)\
    \ Book is a node template whose tag name is Book, has attributes id and year,\
    \ and has text content of string data type. To instantiate the template, we fill\
    \ in values for the attributes and text contents. For each node we created in\
    \ the aforementioned XML tree, we instantiate it with a randomly assigned template.\
    \ In the example of Figure [1,](#page-1-1) we generated three nodes using the\
    \ Book template. We assign random values for element nodes and their attributes\
    \ according to the associated data types except id, to which we assign a unique\
    \ identifier, which we use to unambiguously identify the processors' outputs (see\
    \ Section [3.1\\)](#page-2-1). For the <Book> node with id = 1, we assign the\
    \ random integer value 2020 to year and the random string value \"A fairy tale\"\
    \ as its text content. Similar strategies have been applied also to other schema-less\
    \ systems such as graph DBMSs [\\[26,](#page-11-13) [29\\]](#page-11-14).\n\n\
    ### 4 EVALUATION\n\nIn the evaluation, we sought to investigate whether our technique\
    \ is effective and efficient in finding bugs for XPath expression processors.\
    \ Specifically, we were interested in the following questions:\n\n- Q1. Is XPress\
    \ effective in finding new XPath-related bugs in established XML processors (see\
    \ Section [4.1\\)](#page-5-1)?\n- Q2. Does the query generation approach described\
    \ in Section [3.2](#page-3-0) improve the bug-finding efficiency of XPress with\
    \ respect to real-world baselines and a random generation approach (see Section\
    \ [4.2\\)](#page-7-0)?\n- Q3. How does the differential testing test oracle compare\
    \ to the state-of-the-art oracle (see Section [4.3\\)](#page-8-0)?\n- Q4. What\
    \ kind of XPath-related bugs might be overlooked by XPress (see Section [4.4\\\
    )](#page-9-0)?\n\nTested XML Processors. We tested our method on six mature, wellknown,\
    \ and actively maintained XPath processors: BaseX, exist-DB, Saxon-HE, PostgreSQL,\
    \ libXML2, and a commercial DBMS, whose name we have omitted due to its \"DeWitt\
    \ clause\" [\\[24\\]](#page-11-15). We started testing on BaseX 10.4, eXist-DB\
    \ 6.2.0, Saxon Home Edition 12.2, PostgreSQL version 15, and libXML2 commit version\
    \ 106153. As bugs were resolved, we constantly updated to the latest available\
    \ version. We selected BaseX, eXist-DB, and Saxon to be our main testing targets,\
    \ because they all implement the more recent XPath 3.0 standard. BaseX ranks as\
    \ the most popular Native XML DBMS on the DB-Engines Ranking [\\[9\\]](#page-10-7).\
    \ eXist-DB is widely applied in data centers, systems, and platforms, as referenced\
    \ on the eXist-DB reference page [\\[11\\]](#page-10-13). Saxon is an in-memory\
    \ processor and therefore is not included in the DB-Engines rankings. However,\
    \ the official website of Saxon [\\[16\\]](#page-10-14) states: \"More than 170\
    \ software vendors have built Saxon into their own applications\" and \"6 of the\
    \ world's top 10 software vendors are Saxonica clients\", demonstrating that Saxon\
    \ is a widely-used and popular XML processor. For XPath 1.0 standard implementations,\
    \ we tested PostgreSQL, libXML2, and the commercial DBMS. PostgreSQL is a popular\
    \ open-source DBMS, which ranks 4 on the DB-Engines ranking and has 12.8k stars\
    \ on GitHub. libXML2 is a software library developed for the GNOME project. The\
    \ commercial DBMS is often considered the most popular and important DBMS overall,\
    \ as also reflected in various rankings. All XML processors have been actively\
    \ maintained for over 15 years.\n\nExperimental setup. We implemented the tool,\
    \ XPress, in around 8,000 LOC in Java. In our experiments, we configured it to\
    \ generate XML documents that contain 1 to 50 nodes. We create half as many node\
    \ templates as element nodes. For each XML document, we generated 200 XPath expressions.\
    \ Each XPath expression had an equal possibility to hold 1 to 7 sections. We set\
    \ one predicate to hold at most 10 subjects (see Section [3.2\\)](#page-3-0) and\
    \ the depth of the predicate tree to be at most 10. We used the default settings\
    \ of each XML processor. We conducted all our experiments using a personal computer\
    \ with a 64-Core AMD EPYC 7763 CPU at 2.45GHz and 512GB memory running Ubuntu\
    \ 22.04.\n\n### <span id=\"page-5-1\"></span>4.1 Effectiveness\n\nIn this section,\
    \ we show XPress' effectiveness through the number of bugs found, developer feedback,\
    \ and illustrative examples.\n\n<span id=\"page-6-0\"></span>Finding XPath Bugs\
    \ in XML Document Processors via Differential Testing ICSE '24, April 14–20, 2024,\
    \ Lisbon, Portugal\n\nTable 1: Bugs found by XPress\n\n| XML Processor   | Fixed\
    \ | Confirmed | Reported | Total |\n|-----------------|-------|-----------|----------|-------|\n\
    | BaseX           | 15    | 0         | 0        | 15    |\n| eXist-DB       \
    \ | 1     | 5         | 0        | 6     |\n| Saxon           | 4     | 0    \
    \     | 0        | 4     |\n| Commercial DBMS | 0     | 0         | 2        |\
    \ 2     |\n\nTable 2: Category of Bugs found by XPress\n\n| XML Processor   |\
    \ Logic Bugs | Internal Errors |\n|-----------------|------------|-----------------|\n\
    | BaseX           | 10         | 5               |\n| eXist-DB        | 5    \
    \      | 1               |\n| Saxon           | 2          | 2               |\n\
    | Commercial DBMS | 2          | 0               |\n\nMethodology. We implemented\
    \ the tool while intermittently testing the systems over a period of 3 months.\
    \ For every found discrepancy, we reduced the test case. If the test case exhibited\
    \ an unreported pattern, we considered it likely to be an unknown bug and reported\
    \ it to the developers. Note that this was a best-effort approach, and that it\
    \ is an unsolved problem of how to identify duplicate bugs effectively. Whether\
    \ we considered a bug as unique was based on the developers' verdict; we considered\
    \ a bug only as unique if an issue was addressed through an independent bug fix.\
    \ Unfixed bugs hinder testing, as the duplicates tend to be repeatedly triggered.\
    \ To tackle this, we attempted to disable the construction of bug-inducing elements,\
    \ and also ignored known discrepancy patterns before the reported bug was resolved.\n\
    \nFound bugs overview. As shown in Table [1,](#page-6-0) we successfully found\
    \ 27 unique bugs in total, 15 in BaseX, 6 in eXist-DB, 4 in Saxon, and 2 in the\
    \ commercial DBMS. As detailed subsequently, we could have reported additional\
    \ bugs for eXist-DB and the commercial DBMS, but refrained from doing so due to\
    \ the high number of unfixed bugs for eXist-DB, and lack of developer feedback\
    \ for the commercial DBMS. The bug-inducing test cases we found were not covered\
    \ by the W3C qt3 test suite [\\[19\\]](#page-11-3), which contains around 30,000\
    \ tests for XPath and XQuery—Saxon 11.1 passes all applicable tests in the W3C\
    \ qt3 test suite [\\[17\\]](#page-11-16). Out of the 27 bugs found, the majority,\
    \ 19 bugs, were logic bugs. Based on developer feedback, we learned that among\
    \ the 20 fixed bugs, at least 8 bugs were due to incorrect optimizations. We detected\
    \ the remaining bugs through unexpected errors. All systems we tested were implemented\
    \ in Java, so we did not observe any crash bugs. We did not find any bugs in PostgreSQL\
    \ and libXML2, both of which are known to be robust systems. For example, previous\
    \ bug-finding efforts on testing DBMSs using SQL queries also found no logic bug\
    \ in PostgreSQL [\\[34,](#page-11-17) [35\\]](#page-11-18).\n\nSmall-scope hypothesis.\
    \ We observed that the reported bugs are mainly reproducible by short test cases.\
    \ 70% of all the reported cases can be reproduced with an XML document that consists\
    \ of only one node and 91% of XPath expression consists of only one section. The\
    \ average length of the XML documents in the reported test cases was 12 characters\
    \ and XPath expressions 30 characters. This\n\n<span id=\"page-6-4\"></span>\n\
    \n| XML:       | XPath:                      |  |\n|------------|-----------------------------|--|\n\
    | <T>1</T>   | //T[(@t >= 0) or (@t <= 1)] |  |\n| Result: {} | <T>1</T>     \
    \               |  |\n\nFigure 4: Incorrect optimization of comparison conditions.\n\
    \n<span id=\"page-6-6\"></span>\n\n| XML:         | XPath: |    |  |  |      \
    \                              |  |\n|--------------|--------|----|--|--|------------------------------------|--|\n\
    | <S/>         |        |    |  |  | //S[last() * 150000 >= position()] |  |\n\
    | Result: <S/> |        | {} |  |  |                                    |  |\n\
    \nFigure 5: Arithmetic overflow in pre-check conditions.\n\nphenomenon is known\
    \ as the small-scope hypothesis [\\[21\\]](#page-11-19), and this observation\
    \ has been exploited in testing work that systematically generates small test\
    \ inputs [\\[33\\]](#page-11-20).\n\nDeveloper reception. Developer feedback is\
    \ an important indicator of the bugs' importance. A core developer of BaseX stated\
    \ \"Thanks for sharing the bug reports with us. I appreciate that, they're definitely\
    \ helpful.\"[3](#page-6-1) All 15 bugs reported to BaseX were resolved within\
    \ one month—10 bugs were resolved even within 24 hours. This indicates not only\
    \ that the team was fast in resolving bugs, but also that the bug reports were\
    \ considered valuable. Due to the timely fixes of the BaseX team, we invested\
    \ most time and effort in testing BaseX. After encouragement from the developers\
    \ of BaseX, we contributed the bug-inducing test cases to the W3C XQuery and XPath\
    \ test suite [\\[19\\]](#page-11-3). Most bugs submitted to eXist-DB have not\
    \ yet been fixed, which is likely the result of the many open issues (over 400).\
    \ Nevertheless, the developers from eXist-DB confirmed the bugs quickly and also\
    \ expressed appreciation towards the bug reports \"thank you for finding and reporting.\"\
    [4](#page-6-2)[5](#page-6-3) Because the reported bugs remained unfixed for over\
    \ two months, we stopped testing and reporting to eXist-DB after reporting the\
    \ first few found inconsistencies due to the difficulties of filtering out duplicate\
    \ bugs. We believe that XPress has the ability to find more bugs in eXist-DB after\
    \ the known bugs are resolved. Similarly, for the commercial DBMS, since the developers\
    \ did not follow up on the bugs that we reported, we stopped testing this DBMS.\
    \ For Saxon, all four bugs reported were resolved quickly within one week's time.\n\
    \nSelected bugs. Below, we give a few selected examples of bugs found by XPress\
    \ to illustrate its bug-finding capability.\n\nIncorrect optimization of comparison\
    \ conditions. Figure [4](#page-6-4) shows a fixed bug that we reported to BaseX.[6](#page-6-5)\
    \ The XPath expression selects all T nodes with attribute @t that satisfies @t\
    \ >= 0 or @t <= 1. When @t exists and is a numeric value, this is a condition\
    \ that always evaluates to true. Therefore, an optimization in BaseX rewrote the\
    \ predicate to true. However, when @t does not exist for node T, @t evaluates\
    \ to an empty sequence and returns false for both @t >= 0 and @t <= 1. Before\
    \ we reported this bug, this case was overlooked and resulted in an incorrect\
    \ optimization.\n\n<span id=\"page-6-1\"></span><sup>3</sup>[https://www.mail-archive.com/basex-talk@mailman.uni-konstanz.de/msg15173.](https://www.mail-archive.com/basex-talk@mailman.uni-konstanz.de/msg15173.html)\
    \ [html](https://www.mail-archive.com/basex-talk@mailman.uni-konstanz.de/msg15173.html)\n\
    \n<span id=\"page-6-2\"></span><sup>4</sup>[https://www.mail-archive.com/basex-talk@mailman.uni-konstanz.de/msg15204.](https://www.mail-archive.com/basex-talk@mailman.uni-konstanz.de/msg15204.html)\
    \ [html](https://www.mail-archive.com/basex-talk@mailman.uni-konstanz.de/msg15204.html)\n\
    \n<span id=\"page-6-3\"></span><sup>5</sup><https://github.com/eXist-db/exist/issues/4830>\n\
    \n<span id=\"page-6-5\"></span><sup>6</sup><https://github.com/BaseXdb/basex/issues/2190>\n\
    \n<span id=\"page-7-2\"></span>**Result**: \"2\" | {} **XPath:** tail(subsequence(**(1\
    \ to 2)**,1,2))\n\nFigure 6: Result of tail after subsequence off by one.\n\n\
    <span id=\"page-7-4\"></span>\n\n| XML:        | XPath:                      \
    \                     |  |  |  |\n|-------------|--------------------------------------------------|--|--|--|\n\
    | <A><B/></A> | //*[((.,.)/parent::*/last() ! (. > 1)) = true()] |  |  |  |\n\
    | Result: {}  | <B/>                                             |  |  |  |\n\n\
    Figure 7: Incorrect reduce in positional expressions.\n\nArithmetic overflow in\
    \ pre-check conditions. Figure [5](#page-6-6) shows a fixed bug that we reported\
    \ to BaseX.[7](#page-7-1) last() and position() returns the context size and the\
    \ context position from the dynamic context respectively. In the context XML document,\
    \ the prefix //S selects only one node, and therefore both last() and position()\
    \ return 1. Therefore, the condition is true and node S should be selected. In\
    \ BaseX, an empty result set was returned. The problem was related to optimization\
    \ for positional arguments in conditional comparisons. BaseX substituted last()\
    \ with the greatest theoretical last() value and checked if the condition could\
    \ evaluate to true. If not, the condition could not be satisfied regardless of\
    \ the actual context and could be rewritten to false to reduce context analysis.\
    \ When calculating the multiplication, as the theoretical maximum value for last()\
    \ is a big integer, calculating the expression with long instead of double caused\
    \ an overflow and produced the incorrect result.\n\nResult of tail after subsequence\
    \ off by one. Figure [6](#page-7-2) shows a fixed bug that we reported to eXist-DB.[8](#page-7-3)\
    \ 1 to 2 creates an integer sequence consisting of 1 and 2. The subsequence()\
    \ function in this example selects two elements starting from index 1, and the\
    \ tail() function returns a new sequence excluding the first element of the input\
    \ sequence. The correct result is to return 2. Unexpectedly, eXist-DB returned\
    \ an empty result set. This was caused by a mistake when processing a call to\
    \ tail that has a call to subsequence as an argument, which incorrectly reduced\
    \ the ending index by 1.\n\nIncorrect reduce in positional expressions. Figure\
    \ [7](#page-7-4) shows a fixed bug that we reported to Saxon.[9](#page-7-5) The\
    \ dot (.) stands for the current context in XPath expressions. For node B, (.,\
    \ .)/parent::\\* selects the single node A as the parent. Therefore, last() =\
    \ 1 and the condition evaluates to false. Saxon unexpectedly returned the node\
    \ B. The = operator is considered to be an unordered operator, which does not\
    \ require operands to be sorted. In Saxon, an optimization was applied to eschew\
    \ removing duplicate nodes when evaluating the sub-expression, which resulted\
    \ in A being selected twice and last() evaluated to 2. After we found and reported\
    \ the bug, a patch was applied by the developers to remove the duplicates, when\
    \ the left operand of = is positional sensitive.\n\n### <span id=\"page-7-0\"\
    ></span>4.2 Efficiency\n\nExisting-generator baselines. We considered the only\
    \ two—to the best of our knowledge—approaches to generate XPath expressions. Neither\
    \ of them was specifically designed to be combined with a\n\nXPath test oracle.\
    \ XQgen [\\[42\\]](#page-11-21) generates XPath queries for microbenchmarking.\
    \ Its generated predicates only check for sub-element existence. The XQuery generator\
    \ designed by Todic and Uzelac [\\[41\\]](#page-11-4) generates XPath queries\
    \ for automatically testing index support in DBMSs. Given that indexes apply only\
    \ to sargable queries (i.e., simple comparisons), the expressions it generates\
    \ are simple. Both approaches generate XPath expressions based on an XML schema,\
    \ while XPress generates XPath expressions based on the actual XML document. Based\
    \ on this, we expect both of them to have low applicability for our differential-testing\
    \ approach. Given that neither implementations are publicly available, we re-implemented\
    \ them based on the description in the papers.\n\nSelf-constructed baselines.\
    \ We also constructed our own baselines to investigate the efficiency of the separate\
    \ components of XPress. XPress has two main components, namely (1) the targeted\
    \ predicate generation by using the targeted node to refer to existing nodes and\
    \ attributes and (2) the predicate rectification to avoid empty result sets. To\
    \ evaluate the effect of the components individually, we enabled them individually\
    \ to test whether they improve XPress's bug detection efficiency.\n\nConfigurations.\
    \ We considered four configurations for our selfconstructed baselines. Apart from\
    \ our proposed approach introduced in Section [3.2](#page-3-0) as (1) Targeted,\
    \ we derive configuration (2) Targeted without Rectification, (3) Untargeted with\
    \ Rectification, and (4) Untargeted without Rectification. In (2) Targeted without\
    \ Rectification, we disable the rectification process, which would otherwise ensure\
    \ targeted node selection. Since selecting a targeted node for predicate generation\
    \ guidance always requires at least one node in the result set, we stop generating\
    \ new sections after an empty result set is produced. In (3) Untargeted with Rectification,\
    \ we generate predicates without using targeted node information to supply parameters\
    \ that reference existent context and trigger corner cases for function nodes,\
    \ while keeping the rectification to ensure that at least one node from the candidate\
    \ set is included in the result set. In (4) Untargeted without rectification,\
    \ we remove both components to generate predicates randomly, while omitting rectification.\n\
    \nMethodology. We set each baseline to run for 24 hours [\\[30\\]](#page-11-22).\
    \ We repeated each experiment 10 times to account for potential performance deviations,\
    \ and report the arithmetic mean for all metrics. As our testing target, we selected\
    \ BaseX 10.4, which is the BaseX version that we first started testing. The reason\
    \ for selecting BaseX as a representative is that we found most bugs in BaseX\
    \ and all bugs were fixed, allowing us to determine the number of unique bugs\
    \ we found in a testing campaign by deduplicating bug-inducing test cases automatically.\
    \ Specifically, given two bug-inducing test cases, we could determine whether\
    \ they trigger the same underlying bug by identifying their fix commits; only\
    \ if their associated fix commit are different, do we consider the bugs unique.\
    \ This is a best-effort technique, as, for example, one fix commit might address\
    \ multiple bugs. We disabled the generation of the has-children functions as well\
    \ as using relative XPath expressions in predicates, as they consistently lead\
    \ to crashes, triggering known bugs.\n\nResults of existing generators. Neither\
    \ XQGen nor the Combined XML/XQuery generator found bugs in our experiment. This\
    \ is expected, as previously proposed approaches were not designed for\n\n<span\
    \ id=\"page-7-1\"></span><sup>7</sup><https://github.com/BaseXdb/basex/issues/2220>\n\
    \n<span id=\"page-7-3\"></span><sup>8</sup><https://github.com/eXist-db/exist/issues/4830>\n\
    \n<span id=\"page-7-5\"></span><sup>9</sup><https://saxonica.plan.io/issues/6093?pn=1#change-24136>\n\
    \n<span id=\"page-8-1\"></span>![](_page_8_Figure_1.jpeg)\n\nFigure 8: Average\
    \ number of unique bugs found under different configurations in 24 hours across\
    \ 10 runs.\n\n<span id=\"page-8-2\"></span>Table 3: Average bug report collection\
    \ under different configurations in 24 hours across 10 runs.\n\n| Config     \
    \         | Total<br>cases | Differences<br>detected | Unique<br>bugs | Non-empty<br>result\
    \ |\n|---------------------|----------------|-------------------------|----------------|---------------------|\n\
    | Targeted            | 6.6M           | 11.8K                   | 12.5      \
    \     | 100%                |\n| Targeted w/o Rect   | 9.4M           | 10.2K\
    \                   | 12             | 66%                 |\n| Untargeted w/\
    \ Rect  | 8.8M           | 1.4K                    | 6.5            | 100%   \
    \             |\n| Untargeted w/o Rect | 13.5M          | 0.6K               \
    \     | 6.1            | 44%                 |\n\nautomated testing. As mentioned\
    \ above, XQGen generates predicates that only check for element existence. The\
    \ XQuery generator designed by Todic and Uzelac generates simple predicates that\
    \ include at most one comparison operator.\n\nResults of different configurations.\
    \ As Figure [8](#page-8-1) shows, our proposed approach, Targeted outperforms\
    \ the other configurations. Within 24 hours, it found the most number of unique\
    \ bugs (namely 12.5). Both configurations with targeted generation clearly outperformed\
    \ the untargeted approaches, while rectification shows a similar performance in\
    \ the speed of bug detection. As shown in Table [3,](#page-8-2) both targeted\
    \ generation and rectification reduce the testing throughput, as they obtain intermediate\
    \ results using the XML processor under test. Despite generating only 50% of the\
    \ number of test cases as compared to (4) Untargeted without Rectification, (1)\
    \ Targeted detected 20× more bug-inducing test cases and 2× more unique bugs.\
    \ The results show that selecting a target node to guide the XPath generation\
    \ process improves testing efficiency significantly. As observed above when discussing\
    \ the small-scope hypothesis, most of the bugs that we found can be reproduced\
    \ using a single section, explaining the limited effectiveness of rectification.\
    \ However, we still believe that rectification is an important component, since\
    \ without it, bugs requiring multiple sections with non-empty results could hardly\
    \ be found.\n\nCode coverage. We collected code coverage for three processors'\
    \ core modules for XPress for 24 hours [\\[30\\]](#page-11-22) of execution. The\
    \ result is shown in Table [4.](#page-8-3) To put the numbers in relation, we\
    \ collected coverage also for the projects' test suites; Saxon has no publicly\
    \ available test suites and is therefore excluded. For the three XML processors,\
    \ the line coverage ranged from 15% to 20%, and the\n\n<span id=\"page-8-3\"></span>Table\
    \ 4: Code coverage of tested systems in 24 Hours\n\n| Approach   |      | BaseX\
    \  |      | eXist  | Saxon |        |  |\n|------------|------|--------|------|--------|-------|--------|--|\n\
    |            | Line | Branch | Line | Branch | Line  | Branch |  |\n| XPress \
    \    | 20%  | 16%    | 18%  | 10%    | 15%   | 10%    |  |\n| Unit Tests | 67%\
    \  | 58%    | 52%  | 47%    | -     | -      |  |\n\nbranch coverage ranged from\
    \ 10% to 16%. The coverage percentages are low, which is expected. The main reason\
    \ for low code coverage is that XML processors typically also have other components\
    \ than XPath processing. Taking BaseX as an example, around 21% of uncovered code\
    \ was GUI-related, 10% was due to lack of full-text functionality support, and\
    \ 5% were database commands. In Saxon, as another example, XSLT modules have not\
    \ been covered. A further 18% uncovered code in BaseX involved unimplemented functions;\
    \ it would be straightforward to implement many additional ones, such as math\
    \ functions, but the many functions available would make this a tedious task.\
    \ In Section [4.4,](#page-9-0) we detail unsupported XPath features, implementing\
    \ which might allow us to find more bugs. XPress's test-case generation process\
    \ primarily aims at generating semantically valid expressions, which results in\
    \ low error-checking branch coverage, quantifying which is difficult, as the relevant\
    \ code is spread throughout the code base.\n\n### <span id=\"page-8-0\"></span>4.3\
    \ Comparison to the State of the Art\n\nWe are aware of only one automated testing\
    \ approach that has been proposed to test XML processors [\\[41\\]](#page-11-4).\
    \ It tackled the test oracle problem by using differential testing by comparing\
    \ the results of Microsoft's SQLServer with and without using indexes. Their approach\
    \ was specifically designed to test SQLServer's index support and is not publicly\
    \ available. Due to the narrow testing scope, and since the tool is not publicly\
    \ available, we could not conduct experiments to directly compare the approaches.\
    \ However, we further extended our tool to support differential testing with index\
    \ configurations. Both approaches are complementary, as XPress could not only\
    \ use differential testing among various XML processors, but also create or omit\
    \ indexes to find additional bugs.\n\nIndex support in BaseX, eXist-DB, Saxon,\
    \ and libxml2. Database indexes are data structures built to speed up data retrieval\
    \ [\\[31\\]](#page-11-23) and are DBMS-specific. Not all XML processors are DBMSs—as\
    \ in-memory processors, Saxon and libxml2 lack support for indexes. BaseX and\
    \ eXist-DB both enable structural indexes, such as storing all distinct paths\
    \ of nodes by default. For value indexes to optimize querying on content values,\
    \ BaseX creates text index and attribute index automatically. Users can further\
    \ define additional indexes. Additionally, BaseX provides token indexes, which\
    \ apply to specific functions, such as contains-token. eXist supports range indexes,\
    \ which could be defined for specific nodes or attributes to speed up related\
    \ comparison searches on their contents.\n\nMethodology. We tested eXist's range\
    \ index and BaseX's token index using the XPath expression generation approach\
    \ as described in Section [3.2.](#page-3-0) Due to the found unfixed bugs in eXist,\
    \ we conducted differential testing within eXist by checking the results with\
    \ and\n\n|  |  |  |  |  |  | Shuxin Li and Manuel Rigger |\n|--|--|--|--|--|--|-----------------------------|\n\
    |--|--|--|--|--|--|-----------------------------|\n\n<span id=\"page-9-2\"></span>\n\
    \n| XML:         | XPath:                                             |  |\n|--------------|----------------------------------------------------|--|\n\
    | <M v=\"a\"/>   | //M/descendant-or-self::M[contains-token(@v, \"a\")] |  |\n\
    | Result: <M/> | {} (create index token)                            |  |\n\nFigure\
    \ 9: Found bug with token index in BaseX.\n\nwithout range index definition. For\
    \ BaseX, we defined a token index and compared its results directly with the results\
    \ of Saxon.\n\nResults. Throughout the testing method, we detected one additional\
    \ bug for BaseX[10](#page-9-1) and found no additional bugs in eXist. We reported\
    \ the found bug shown in Figure [9](#page-9-2) to the BaseX developers, who quickly\
    \ fixed it. The query selects all nodes with tag name M in the document which\
    \ holds attribute v that contains token \"a\". BaseX returned node M without token\
    \ index, as expected, while unexpectedly returning an empty result set when not\
    \ using an index. Overall, while the results suggest that using or removing indexes\
    \ might find additional bugs, doing so had low effectiveness. A potential explanation\
    \ could be that our test-case generation approach does not consider when indexes\
    \ could be applied, which might result in low testing efficiency.\n\n### <span\
    \ id=\"page-9-0\"></span>4.4 Analysis of BaseX Historical Bug Reports\n\nUnlike\
    \ formal verification approaches, automatic testing approaches might miss bugs\
    \ in the system tested. Due to the lack of ground truth, we cannot generally determine\
    \ which bugs are overlooked by our approach. However, as a best-effort approach,\
    \ we studied historical bug reports in order to determine whether XPress could\
    \ have found them.\n\nBug reports. We analyzed all historical BaseX bug reports\
    \ in its GitHub bug tracker. We selected BaseX, because the majority of issues\
    \ are closed (1618 out of 1640). The issue tracker of BaseX is used for confirmed\
    \ bug reports filtered from reports from the mailing list, and the BaseX maintainers\
    \ carefully label and document them. For these reasons, it was easy to identify\
    \ and classify the underlying problem of each bug report.\n\nMethodology. We manually\
    \ analyzed all historical bug issues until 2023 Apr 17 in BaseX, which were 1597\
    \ issues, after excluding the issues we reported. To confine the study of bug\
    \ reports within the scope of XPath, we selected bug reports triggered by only\
    \ XPath expressions. To determine whether a bug could be theoretically found by\
    \ XPress, we mainly checked three aspects of the reports. For XPress to cover\
    \ the test case, both the XML document and the XPath expression in the test case\
    \ should not include any unimplemented functions or language features. Second,\
    \ we could construct the sections and the predicate tree structure of XPress for\
    \ involved predicates to form the pattern of the bug-inducing XPath expression.\
    \ Third, XML processors should disagree on the result set. Note that this is a\
    \ best-effort approach, because we might both incorrectly conclude that XPress\
    \ might find a bug (e.g., it might be unlikely that the test case would be generated\
    \ in practice) or incorrectly conclude that a bug cannot be found even when a\
    \ different test-case within the reach of XPress would trigger the same underlying\
    \ bug.\n\nResults. Out of the total 78 bugs that we collected, we identified 20\
    \ bugs that could have been detected by XPress. For the other 58 bugs, we identified\
    \ 4 kinds of bugs that XPress would have failed to find, namely due to (1) unimplemented\
    \ functionalities (51 cases), (2) invalid inputs where the expected result would\
    \ be an error (6 cases), (3) processors producing different results (2 cases),\
    \ and (4) miscellaneous other issues (6 cases). Bugs belonging to more than one\
    \ group are included in all involved groups. The differential testing oracle fails\
    \ to detect the bugs with processors producing different results, while we consider\
    \ the other categories mostly as implementation limitations in test-case generation.\
    \ Therefore, out of all 78 bugs, 76 bugs (97%) could be detected through differential\
    \ testing. This further demonstrates the effectiveness of employing a differential\
    \ testing oracle for XPath-related testing.\n\nUnimplemented functionalities.\
    \ Most uncovered bug reports are due to unimplemented functionalities. Unsupported\
    \ functions include constructors defined by the XML or XPath language standards,\
    \ array and map functions, and also constructors of derived datatypes [\\[2\\\
    ]](#page-10-15), such as xs:NMtokens. Given enough time, it would be straightforward\
    \ to implement them in XPress. For/while loops, variable declaration, if-else\
    \ conditional expressions, and self-defined functions are also unimplemented.\
    \ These could be supported based on approaches that have been proposed in the\
    \ context of compiler testing [\\[32,](#page-11-24) [43\\]](#page-11-6). Neither\
    \ the XML documents nor XPath expressions that XPress constructs involve namespaces,\
    \ which allow distinguishing items with the same tag name. They could be integrated\
    \ into the XPress test-case generator. By implementing all these features, an\
    \ additional 38 bugs (48%) could have been found.\n\nExpected errors. Bug reports\
    \ grouped into expected is error refers to invalid test cases, which are successfully\
    \ executed instead of throwing an error. XPress constructs both syntactically\
    \ and semantically valid expressions and therefore could not detect bugs within\
    \ this category. However, the differential testing oracle could detect these bugs\
    \ by comparing the errors of the different XML processors.\n\nDifferent results.\
    \ The different result category contains queries for which different processors\
    \ intentionally produce different results, which shows the limitation of the differential\
    \ testing oracle. One example is the function id, which selects nodes with xml:id\
    \ attributes. BaseX takes attributes named as id as xml:id attributes, while Saxon\
    \ and eXist-DB require an explicit declaration.\n\n### 5 RELATED WORK\n\nWhile\
    \ various related approaches to our work exist, to the best of our knowledge,\
    \ we propose the first general approach to testing XML processors to find logic\
    \ bugs. As discussed above, the most closely related work proposed testing the\
    \ index support of SQLServer in the context of XPath and XQuery [\\[41\\]](#page-11-4),\
    \ which, to the best of our knowledge, is the only work that has tackled the test-oracle\
    \ problem for XML processors, but is limited in scope.\n\nTesting XPath functionality.\
    \ Various approaches to benchmarking XPath implementations or test suites for\
    \ them have been proposed, the most representative being XPathMark and the W3C\
    \ qt3 test suite. XPathMark [\\[25\\]](#page-11-2) is a benchmark for testing\
    \ XML processors' XPath standard 1.0 functionality, containing both correctness\n\
    \n<span id=\"page-9-1\"></span><sup>10</sup><https://github.com/BaseXdb/basex/issues/2222>\n\
    \nas well as performance tests. The W3C qt3 test suite developed by the W3C XQuery\
    \ and XSLT Working Groups [\\[19\\]](#page-11-3) contains around 30,000 tests\
    \ for XPath and XQuery targeting XPath 3.0 and later versions, which cover a broad\
    \ range of functions and expressions.\n\nXML-related automated synthetic data\
    \ generation. Previous works have proposed approaches for automatically generating\
    \ XML-related data, such as XML documents, XPath, and XQuery expressions. Aboulnaga\
    \ et al. proposed an XML document generator to generate synthetic, but complex,\
    \ structured XML data by introducing recursion and repetition on tag name assignment\
    \ and controlling the element frequency distribution [\\[20\\]](#page-11-25).\
    \ Rychnovský and Holubová proposed an approach to generate XML documents related\
    \ to given XPath queries from a specific XML schema to improve query efficiency\
    \ [\\[37\\]](#page-11-26), which is useful for developers to create micro-benchmarks\
    \ for testing performance over certain XPath expressions. XQGen [\\[42\\]](#page-11-21)\
    \ is a tool for generating XPath queries that conform to a given XML schema, allowing\
    \ users to specify multiple parameters, such as the percentage of empty queries\
    \ desired and the percentage of queries with predicates. XPath generated by XQ-Gen\
    \ includes only direct node tests without introducing complex expressions, such\
    \ as axes or function transformations. Similarly, the XQuery generator designed\
    \ by Todic and Uzelac [\\[41\\]](#page-11-4) includes XQuery FLWOR expressions,\
    \ but the logic predicate consists only of simple operations, such as value comparisons.\
    \ Neither of these works tackled the test oracle problem, and, as indicated by\
    \ the results in Section [4.3,](#page-8-0) given their different focus, they cannot\
    \ be effectively combined with a differential testing oracle.\n\nTargeted test\
    \ case generation. Many testing tools guide their test case generation process\
    \ to improve testing efficiency, for random approaches such as random byte mutation\
    \ used in fuzzing approaches generate a large proportion of invalid queries [\\\
    [47\\]](#page-11-27). DynSQL [\\[27\\]](#page-11-28) guides the fuzzing process\
    \ of DBMSs towards increased code coverage and high statement validity. APOLLO\
    \ [\\[28\\]](#page-11-29) is a system for detecting performance regression bugs\
    \ in DBMSs. It increases the probability of including components from previously\
    \ encountered performance issues. Cynthia [\\[39\\]](#page-11-9) was proposed\
    \ to test Object Relational Mappers (ORMs) and generates targeted databases dependent\
    \ on generated abstract SQL queries, which are likely to return non-empty results.\
    \ Query Plan Guidance (QPG) [\\[22\\]](#page-11-30) guides testing towards exploring\
    \ more unique query plans.\n\nPivoted Query Synthesis. The targeted node in XPress\
    \ was inspired by the pivot row in Pivoted Query Synthesis (PQS) [\\[36\\]](#page-11-12),\
    \ which was originally proposed to test relational DBMSs. PQS' and XPress' commonality\
    \ is that they select a random element, in PQS, a row in the database, while for\
    \ XPress, a node in an XML document, based on which they generate a query that\
    \ is guaranteed to fetch the element. However, both the purpose and use of the\
    \ targeted node and pivot row differ. In PQS, the pivot row is used both for test-case\
    \ generation and to construct the test oracle, by evaluating an expression and\
    \ ensuring that it evaluates to true for the pivot row so that it can be used\
    \ in a query that is guaranteed to fetch the row. Doing so requires a naive reimplementation\
    \ of all the DBMSs' operators that should be tested, which incurs a high implementation\
    \ effort, as highlighted in follow-up work [? ]. In XPress, the targeted node\
    \ is used only for test-case generation, to improve\n\ntesting efficiency and\
    \ to ensure non-empty intermediate results; to this end, XPress uses the XML processor\
    \ to determine the result of the expression, rather than requiring the reimplementation\
    \ of operators. In addition, for predicate rectification, XPress provides operator-specific\
    \ rules, rather than relying on a generic one, aiming to generate more interesting\
    \ test cases. The high-level idea of a pivot element also inspired other works;\
    \ for example, recent work on Android testing introduced the concept of a pivot\
    \ layout [\\[40\\]](#page-11-31).\n\n### 6 CONCLUSION\n\nThis paper has presented\
    \ a general automated testing approach for detecting XPath-related logic bugs\
    \ in XML processors. We demonstrate that differential testing is applicable in\
    \ this domain, since XML processors widely adhere to the XPath standards. To generate\
    \ interesting XPath queries, our approach selects a so-called targeted node to\
    \ guide predicate generation and predicate rectification to ensure the inclusion\
    \ of that node. Our evaluation shows that this improves the number of bugs detected\
    \ in 24 hours to 2× as compared to random generation. More importantly, we have\
    \ successfully detected 27 previously unknown, unique bugs in six mature XML processing\
    \ systems. We believe that this high number is surprising, given that XML processors\
    \ are an essential part of our computing infrastructure, with the first XPath\
    \ standard having been proposed more than 20 years ago, and the systems that we\
    \ have tested having been maintained for at least 15 years. We believe that XPress,\
    \ given its simplicity and generality, has a high chance of being integrated into\
    \ the toolbox of XML processor developers. Furthermore, we believe that our work\
    \ might inspire testing approaches for other XML standards, such as XQuery or\
    \ XSLT.\n\n### ACKNOWLEDGMENTS\n\nThis research was supported by a Ministry of\
    \ Education (MOE) Academic Research Fund (AcRF) Tier 1 grant.\n\n### REFERENCES\n\
    \n- <span id=\"page-10-12\"></span>[1] 1999. XML Path Language (XPath) Version\
    \ 1.0 W3C Recommendation. Retrieved July 17, 2023 from<https://www.w3.org/TR/1999/REC-xpath-19991116/>\n\
    - <span id=\"page-10-15\"></span>[2] 2004. XML Schema Part 2: Datatypes Second\
    \ Edition - Built-in datatypes. Retrieved July 17, 2023 from<https://www.w3.org/TR/xmlschema-2/#built-in-datatypes>\n\
    - <span id=\"page-10-10\"></span>[3] 2008. EBNF notation from the W3C Extensible\
    \ Markup Language (XML) 1.0 (Fifth Edition). Retrieved July 17, 2023 from<https://www.w3.org/TR/REC-xml/>\n\
    - <span id=\"page-10-9\"></span>[4] 2014. XML Path Language (XPath) 3.0 W3C Recommendation.\
    \ Retrieved July 17, 2023 from<https://www.w3.org/TR/xpath-30/>\n- <span id=\"\
    page-10-11\"></span>[5] 2014. XPath and XQuery Functions and Operators 3.0 W3C\
    \ Recommendation. Retrieved July 17, 2023 from<https://www.w3.org/TR/xpath-functions-30/>\n\
    - <span id=\"page-10-1\"></span>[6] 2017. XQuery 3.1: An XML Query Language W3C\
    \ Recommendation. Retrieved July 17, 2023 from<https://www.w3.org/TR/xquery-31/>\n\
    - <span id=\"page-10-0\"></span>[7] 2017. XSL Transformations (XSLT) Version 3.0\
    \ W3C Recommendation. Retrieved July 17, 2023 from<https://www.w3.org/TR/xslt-30/>\n\
    - <span id=\"page-10-2\"></span>[8] 2023. BaseX. Retrieved July 31, 2023 from<https://basex.org/>\n\
    - <span id=\"page-10-7\"></span>[9] 2023. DB-Engines Ranking. Retrieved July 6,\
    \ 2023 from [https://db-engines.com/](https://db-engines.com/en/ranking) [en/ranking](https://db-engines.com/en/ranking)\n\
    - <span id=\"page-10-3\"></span>[10] 2023. eXist-DB. Retrieved July 31, 2023 from\
    \ [http://exist-db.org/exist/apps/](http://exist-db.org/exist/apps/homepage/index.html)\
    \ [homepage/index.html](http://exist-db.org/exist/apps/homepage/index.html)\n\
    - <span id=\"page-10-13\"></span>[11] 2023. eXist DB reference page. Retrieved\
    \ July 6, 2023 from [http://exist-db.org/](http://exist-db.org/exist/apps/homepage/references.html)\
    \ [exist/apps/homepage/references.html](http://exist-db.org/exist/apps/homepage/references.html)\n\
    - <span id=\"page-10-8\"></span>[12] 2023. libXML2. Retrieved July 31, 2023 from\
    \ [https://gitlab.gnome.org/GNOME/](https://gitlab.gnome.org/GNOME/libxml2) [libxml2](https://gitlab.gnome.org/GNOME/libxml2)\n\
    - <span id=\"page-10-5\"></span>[13] 2023. MySQL. Retrieved July 31, 2023 from<https://www.mysql.com/>\n\
    - <span id=\"page-10-4\"></span>[14] 2023. Oracle Database. Retrieved July 31,\
    \ 2023 from [https://www.oracle.com/](https://www.oracle.com/database/) [database/](https://www.oracle.com/database/)\n\
    - <span id=\"page-10-14\"></span><span id=\"page-10-6\"></span>[15] 2023. PostgreSQL.\
    \ Retrieved July 31, 2023 from<https://www.postgresql.org/>\n- [16] 2023. Saxon\
    \ home page. Retrieved July 6, 2023 from [https://saxonica.com/html/](https://saxonica.com/html/welcome/welcome.html)\
    \ [welcome/welcome.html](https://saxonica.com/html/welcome/welcome.html)\n- <span\
    \ id=\"page-11-16\"></span><span id=\"page-11-0\"></span>[17] 2023. Saxon XQuery\
    \ 3.1 conformance page. Retrieved July 13, 2023 from [https:](https://www.saxonica.com/documentation12/#!conformance/xquery31)\
    \ [//www.saxonica.com/documentation12/#!conformance/xquery31](https://www.saxonica.com/documentation12/#!conformance/xquery31)\n\
    - <span id=\"page-11-1\"></span>[18] 2023. Saxonica. Retrieved July 31, 2023 from<https://saxonica.com/>\n\
    - <span id=\"page-11-3\"></span>[19] 2023. W3C qt3 test suite github repository.\
    \ Retrieved July 11, 2023 from [https:](https://github.com/w3c/qt3tests) [//github.com/w3c/qt3tests](https://github.com/w3c/qt3tests)\n\
    - <span id=\"page-11-25\"></span>[20] Jeffrey F. Naughton Aboulnaga, Ashraf and\
    \ Chun Zhang. 2001. Generating Synthetic Complex-Structured XML Data. WebDB. 1\
    \ (2001), 79–84.\n- <span id=\"page-11-19\"></span>[21] Alexandr Andoni, Dumitru\
    \ Daniliuc, Sarfraz Khurshid, and Darko Marinov. 2003. Evaluating the \"small\
    \ scope hypothesis\".\n- <span id=\"page-11-30\"></span>[22] Jinsheng Ba and Manuel\
    \ Rigger. 2023. Testing Database Engines via Query Plan Guidance. In 2023 IEEE/ACM\
    \ 45th International Conference on Software Engineering (ICSE). 2060–2071.<https://doi.org/10.1109/ICSE48619.2023.00174>\n\
    - <span id=\"page-11-8\"></span>[23] Yuting Chen, Ting Su, Chengnian Sun, Zhendong\
    \ Su, and Jianjun Zhao. 2016. Coverage-Directed Differential Testing of JVM Implementations.\
    \ In Proceedings of the 37th ACM SIGPLAN Conference on Programming Language Design\
    \ and Implementation (Santa Barbara, CA, USA) (PLDI '16). Association for Computing\
    \ Machinery, New York, NY, USA, 85–99.<https://doi.org/10.1145/2908080.2908095>\n\
    - <span id=\"page-11-15\"></span>[24] Timothy Dyck. 2002. DB Test Pioneer Makes\
    \ History. Retrieved July 31, 2023 from<https://www.eweek.com/development/db-test-pioneer-makes-history/>\n\
    - <span id=\"page-11-2\"></span>[25] Massimo Franceschet. 2005. XPathMark: An\
    \ XPath Benchmark for the XMark Generated Data. In Database and XML Technologies,\
    \ Stéphane Bressan, Stefano Ceri, Ela Hunt, Zachary G. Ives, Zohra Bellahsène,\
    \ Michael Rys, and Rainer Unland (Eds.). Springer Berlin Heidelberg, Berlin, Heidelberg,\
    \ 129–143.\n- <span id=\"page-11-13\"></span>[26] Ziyue Hua, Wei Lin, Luyao Ren,\
    \ Zongyang Li, Lu Zhang, Wenpin Jiao, and Tao Xie. 2023. GDsmith: Detecting Bugs\
    \ in Cypher Graph Database Engines. Association for Computing Machinery, New York,\
    \ NY, USA. [https://doi.org/10.1145/3597926.](https://doi.org/10.1145/3597926.3598046)\
    \ [3598046](https://doi.org/10.1145/3597926.3598046)\n- <span id=\"page-11-28\"\
    ></span>[27] Zu-Ming Jiang, Jia-Ju Bai, and Zhendong Su. 2023. DynSQL: Stateful\
    \ Fuzzing for Database Management Systems with Complex and Valid SQL Query Generation.\
    \ In Proceedings of the 32nd USENIX Conference on Security Symposium (Anaheim,\
    \ CA, USA) (SEC '23). USENIX Association, USA, Article 277, 17 pages.\n- <span\
    \ id=\"page-11-29\"></span>[28] Jinho Jung, Hong Hu, Joy Arulraj, Taesoo Kim,\
    \ and Woonhak Kang. 2019. APOLLO: Automatic Detection and Diagnosis of Performance\
    \ Regressions in Database Systems. Proc. VLDB Endow. 13, 1 (sep 2019), 57–70.\
    \ [https:](https://doi.org/10.14778/3357377.3357382) [//doi.org/10.14778/3357377.3357382](https://doi.org/10.14778/3357377.3357382)\n\
    - <span id=\"page-11-14\"></span>[29] Matteo Kamm, Manuel Rigger, Chengyu Zhang,\
    \ and Zhendong Su. 2023. Testing Graph Database Engines via Query Partitioning.\
    \ Association for Computing Machinery, New York, NY, USA.<https://doi.org/10.1145/3597926.3598044>\n\
    - <span id=\"page-11-22\"></span>[30] George Klees, Andrew Ruef, Benji Cooper,\
    \ Shiyi Wei, and Michael Hicks. 2018. Evaluating Fuzz Testing. Proceedings of\
    \ the 2018 ACM SIGSAC conference on computer and communications security (2018).\
    \ [https://doi.org/10.1145/3243734.](https://doi.org/10.1145/3243734.3243804)\
    \ [3243804](https://doi.org/10.1145/3243734.3243804)\n- <span id=\"page-11-23\"\
    ></span>[31] Quanzhong Li and Bongki Moon. 2001. Indexing and Querying XML Data\
    \ for Regular Path Expressions. In Proceedings of the 27th International Conference\
    \ on Very Large Data Bases (VLDB '01). Morgan Kaufmann Publishers Inc., San Francisco,\
    \ CA, USA, 361–370.\n- <span id=\"page-11-24\"></span>[32] Vsevolod Livinskii,\
    \ Dmitry Babokin, and John Regehr. 2023. Fuzzing Loop Optimizations in Compilers\
    \ for C++ and Data-Parallel Languages. Proc. ACM Program. Lang. 7, PLDI, Article\
    \ 181 (jun 2023), 22 pages.<https://doi.org/10.1145/3591295>\n- <span id=\"page-11-20\"\
    ></span>[33] Jayashree Mohan, Ashlie Martinez, Soujanya Ponnapalli, Pandian Raju,\
    \ and Vijay Chidambaram. 2018. Finding Crash-Consistency Bugs with Bounded Black-Box\
    \ Crash Testing. In 13th USENIX Symposium on Operating Systems Design and Implementation\
    \ (OSDI 18). 33–50.\n- <span id=\"page-11-17\"></span>[34] Manuel Rigger and Zhendong\
    \ Su. 2020. Detecting Optimization Bugs in Database Engines via Non-Optimizing\
    \ Reference Engine Construction. In Proceedings of the 28th ACM Joint Meeting\
    \ on European Software Engineering Conference and\n\nSymposium on the Foundations\
    \ of Software Engineering (Virtual Event, USA) (ESEC/FSE 2020). Association for\
    \ Computing Machinery, New York, NY, USA, 1140–1152.<https://doi.org/10.1145/3368089.3409710>\n\
    \n- <span id=\"page-11-18\"></span>[35] Manuel Rigger and Zhendong Su. 2020. Finding\
    \ Bugs in Database Systems via Query Partitioning. Proc. ACM Program. Lang. 4,\
    \ OOPSLA, Article 211 (nov 2020), 30 pages.<https://doi.org/10.1145/3428279>\n\
    - <span id=\"page-11-12\"></span>[36] Manuel Rigger and Zhendong Su. 2020. Testing\
    \ Database Engines via Pivoted Query Synthesis. In Proceedings of the 14th USENIX\
    \ Conference on Operating Systems Design and Implementation (OSDI'20). USENIX\
    \ Association, USA, Article 38, 16 pages.\n- <span id=\"page-11-26\"></span>[37]\
    \ Dušan Rychnovský and Holubová. 2015. Generating XML Data for XPath Queries.\
    \ Association for Computing Machinery. (2015). [https://doi.org/10.1145/2695664.](https://doi.org/10.1145/2695664.2695691)\
    \ [2695691](https://doi.org/10.1145/2695664.2695691)\n- <span id=\"page-11-5\"\
    ></span>[38] Donald R. Slutz. 1998. Massive Stochastic Testing of SQL. In Proceedings\
    \ of the 24rd International Conference on Very Large Data Bases (VLDB '98). Morgan\
    \ Kaufmann Publishers Inc., San Francisco, CA, USA, 618–622.\n- <span id=\"page-11-9\"\
    ></span>[39] Thodoris Sotiropoulos, Stefanos Chaliasos, Vaggelis Atlidakis, Dimitris\
    \ Mitropoulos, and Diomidis Spinellis. 2021. Data-Oriented Differential Testing\
    \ of Object-Relational Mapping Systems. In 2021 IEEE/ACM 43rd International Conference\
    \ on Software Engineering (ICSE). 1535–1547. [https://doi.org/10.1109/ICSE43902.2021.](https://doi.org/10.1109/ICSE43902.2021.00137)\
    \ [00137](https://doi.org/10.1109/ICSE43902.2021.00137)\n- <span id=\"page-11-31\"\
    ></span>[40] Ting Su, Yichen Yan, Jue Wang, Jingling Sun, Yiheng Xiong, Geguang\
    \ Pu, Ke Wang, and Zhendong Su. 2021. Fully Automated Functional Fuzzing of Android\
    \ Apps for Detecting Non-Crashing Logic Bugs. Proc. ACM Program. Lang. 5, OOPSLA,\
    \ Article 156 (oct 2021), 31 pages.<https://doi.org/10.1145/3485533>\n- <span\
    \ id=\"page-11-4\"></span>[41] Milos Todic and Branislav Uzelac. 2012. Combined\
    \ XML/XQuery generator. Proceedings of the Fifth International Workshop on Testing\
    \ Database Systems (2012). <https://doi.org/10.1145/2304510.2304519>\n- <span\
    \ id=\"page-11-21\"></span>[42] Yuqing Wu, Namrata Lele, Rashmi Aroskar, Sharanya\
    \ Chinnusamy, and Sofia Brenes. 2009. XQGen: An Algebra-Based XPath Query Generator\
    \ for Micro-Benchmarking. In Proceedings of the 18th ACM Conference on Information\
    \ and Knowledge Management (Hong Kong, China) (CIKM '09). Association for Computing\
    \ Machinery, New York, NY, USA, 2109–2110. [https://doi.org/10.1145/1645953.](https://doi.org/10.1145/1645953.1646328)\
    \ [1646328](https://doi.org/10.1145/1645953.1646328)\n- <span id=\"page-11-6\"\
    ></span>[43] Xuejun Yang, Yang Chen, Eric Eide, and John Regehr. 2011. Finding\
    \ and Understanding Bugs in C Compilers. Association for Computing Machinery,\
    \ New York, NY, USA.<https://doi.org/10.1145/1993498.1993532>\n- <span id=\"page-11-10\"\
    ></span>[44] Hua Z, Lin W, Ren L, Li Z, Zhang L, Jiao W, and Xie T. 2023. GDsmith:\
    \ Detecting bugs in Cypher graph database engines. Proceedings of ACM SIG-SOFT\
    \ International Symposium on Software Testing and Analysis (2023). [https:](https://doi.org/10.48550/arXiv.2206.08530)\
    \ [//doi.org/10.48550/arXiv.2206.08530](https://doi.org/10.48550/arXiv.2206.08530)\n\
    - <span id=\"page-11-7\"></span>[45] Qirun Zhang, Chengnian Sun, and Zhendong\
    \ Su. 2017. Skeletal Program Enumeration for Rigorous Compiler Testing. In Proceedings\
    \ of the 38th ACM SIGPLAN Conference on Programming Language Design and Implementation\
    \ (Barcelona, Spain) (PLDI 2017). Association for Computing Machinery, New York,\
    \ NY, USA, 347–361.<https://doi.org/10.1145/3062341.3062379>\n- <span id=\"page-11-11\"\
    ></span>[46] Yingying Zheng, Wensheng Dou, Yicheng Wang, Zheng Qin, Lei Tang,\
    \ Yu Gao, Dong Wang, Wei Wang, and Jun Wei. 2022. Finding bugs in Gremlin-based\
    \ graph database systems via randomized differential testing. Proceedings of the\
    \ 31st ACM SIGSOFT International Symposium on Software Testing and Analysis (2022).\
    \ <https://doi.org/10.1145/3533767.3534409>\n- <span id=\"page-11-27\"></span>[47]\
    \ Rui Zhong, Yongheng Chen, Hong Hu, Hangfan Zhang, Wenke Lee, and Dinghao Wu.\
    \ 2020. Squirrel: Testing database management systems with language validity and\
    \ coverage feedback. In Proceedings of the 2020 ACM SIGSAC Conference on Computer\
    \ and Communications Security. 955–970."
  decisions:
    language: '- Qualified. Reason: English Paper'
    evaluation_prompt: Qualified.
    related_work_prompt: Qualified
    novelty_prompt: Qualified
    review_only_prompt: Qualified
  llm_input_used: "## Abstract\nExtensible Markup Language (XML) is a widely used\
    \ file format for data\nstorage and transmission. Many XML processors support\
    \ XPath, a query language\nthat enables the extraction of elements from XML documents.\
    \ These systems can\nbe affected by logic bugs, which are bugs that cause the\
    \ processor to return\nincorrect results. In order to tackle such bugs, we propose\
    \ a new approach,\nwhich we realized as a system called XPress. As a test oracle,\
    \ XPress relies on\ndifferential testing, which compares the results of multiple\
    \ systems on the\nsame test input, and identifies bugs through discrepancies in\
    \ their outputs. As\ntest inputs, XPress generates both XML documents and XPath\
    \ queries. Aiming to\ngenerate meaningful queries that compute non-empty results,\
    \ XPress selects a\nso-called targeted node to guide the XPath expression generation\
    \ process. Using\nthe targeted node, XPress generates XPath expressions that reference\
    \ existing\ncontext related to the targeted node, such as its tag name and attributes,\n\
    while also guaranteeing that a predicate evaluates to true before further\nexpanding\
    \ the query. We tested our approach on six mature XML processors,\nBaseX, eXist-DB,\
    \ Saxon, PostgreSQL, libXML2, and a commercial database system.\nIn total, we\
    \ have found 20 unique bugs in these systems, of which 25 have been\nverified\
    \ by the developers, and 12 of which have been fixed. XPress is\nefficient, as\
    \ it finds 12 unique bugs in BaseX in 24 hours, which is 2x as fast\nas naive\
    \ random generation. We expect that the effectiveness and simplicity of\nour approach\
    \ will help to improve the robustness of many XML processors.\n\n## Introduction\n\
    Extensible Markup Language (XML) is a widely used file format for data storage\
    \ and transmission. XPath is an expression language, which provides the ability\
    \ to navigate through XML documents to select wanted nodes. XPath is also at the\
    \ core of other XML query language standards such as XSLT [\\[7\\]](#page-10-0)\
    \ and XQuery [\\[6\\]](#page-10-1), making it a fundamental XML query language.\n\
    \nVarious XML document processors have been developed for extracting information\
    \ from XML documents efficiently. We loosely categorize them depending on whether\
    \ they can store XML documents in addition to processing them—that is, whether\
    \ they are Database Management Systems (DBMSs), or provide only processing functionality.\
    \ In terms of DBMSs specialized for XML, popular examples include BaseX [\\[8\\\
    ]](#page-10-2) and eXist-DB [\\[10\\]](#page-10-3). Many generalpurpose DBMSs\
    \ such as Oracle Database [\\[14\\]](#page-10-4), MySQL [\\[13\\]](#page-10-5),\
    \ and PostgreSQL [\\[15\\]](#page-10-6) have adopted support for processing XML\
    \ documents. In fact, out of the 10 most popular DBMSs according to the DB-engines\
    \ ranking [\\[9\\]](#page-10-7), 6 support at least partial XML document parsing.\
    \ A popular example of a processor without storage functionality is Saxon. Saxon\
    \ [\\[18\\]](#page-11-1) is an in-memory XML processor that can be either used\
    \ in a standalone way or embedded as a library. Finally, libxml2 [\\[12\\]](#page-10-8)\
    \ is a popular XML processing library written in C. XPath is supported by all\
    \ of these processors.\n\nXML document processors can be affected by logic bugs.\
    \ Logic bugs are bugs that cause the XML processor to produce incorrect results\
    \ without crashing the system, meaning that they can often go unnoticed. In order\
    \ to find such bugs, developers mainly rely on test suites such as the XPathMark\
    \ test suite for XPath [\\[25\\]](#page-11-2), the W3C qt3 test suite [\\[19\\\
    ]](#page-11-3), or hand-written unit tests. Manually writing tests requires much\
    \ effort, and it is challenging to comprehensively cover the XML processors' functionality.\
    \ To find logic bugs automatically, a so-called test oracle is required that can\
    \ determine whether the system's output is correct in order to find logic bugs.\
    \ Todic and Uzelac have proposed an automated testing technique for SQLServer's\
    \ index support; their test oracle compared the results of a given query with\
    \ and without index definition [\\[41\\]](#page-11-4). A limitation of this technique\
    \ is that it is applicable only to finding index-related bugs in DBMSs. To the\
    \ best of our knowledge, no other test oracles have been proposed in this context.\n\
    \nIn order to detect XPath-related bugs in XML processors, we propose differential\
    \ testing as an oracle. The core idea of differential testing is to use one input\
    \ that is executed using multiple systems; any discrepancy in the results indicates\
    \ a potential bug in the system. For testing XML processors, the input for the\
    \ XML processors under test is an XML document and XPath expression, and the results\
    \ are a sequence of XML nodes or values. Differential testing has been successfully\
    \ applied in various related domains,\n\n<sup>∗</sup>Work done during an internship\
    \ at the National University of Singapore.\n\nICSE '24, April 14–20, 2024, Lisbon,\
    \ Portugal\n\nThis is the author's version of the work. It is posted here for\
    \ your personal use. Not for redistribution. The definitive Version of Record\
    \ was published in 2024 IEEE/ACM 46th International Conference on Software Engineering\
    \ (ICSE '24), April 14–20, 2024, Lisbon, Portugal, [https://doi.org/10.1145/3597503.3639208.](https://doi.org/10.1145/3597503.3639208\
    \ )\n\nsuch as relational DBMSs [\\[38\\]](#page-11-5), compilers [\\[43,](#page-11-6)\
    \ [45\\]](#page-11-7), JVM implementations [\\[23\\]](#page-11-8), ORM systems\
    \ [\\[39\\]](#page-11-9), and graph DBMSs [\\[44,](#page-11-10) [46\\]](#page-11-11).\
    \ Its effectiveness hinges on two main requirements. First, multiple systems to\
    \ be compared must be available. As discussed above, various XML processors with\
    \ XPath support exist. Second, for any valid input, the systems should produce\
    \ the same result, since otherwise, a differential-testing approach raises many\
    \ false alarms. This requirement is not always met, for example, when applying\
    \ differential testing to relational DBMSs, where the \"common SQL subset is relatively\
    \ small and changes with each release\" and NULL handling differs between DBMSs\
    \ [\\[38\\]](#page-11-5). As we found, XPath is a well-defined language by the\
    \ W3C standard, and XPath implementations of the same standard follow the same\
    \ language rules, making differential testing highly applicable.\n\nTo generate\
    \ test cases, we propose an approach that selects a so-called targeted node from\
    \ the XML document, based on which we generate a query that is guaranteed to fetch\
    \ at least that node. As such, it tackles two challenges that might prevent testing\
    \ from exercising interesting behaviors. First, by generating the query based\
    \ on the targeted node, we can guarantee that we access a tag name, attributes,\
    \ and relative paths that exist with respect to at least the targeted node. Second,\
    \ by rectifying predicates so that they evaluate to true for the targeted node,\
    \ we can ensure that the result set is non-empty even for complex queries. A similar\
    \ highlevel idea has been proposed in the context of testing relational DBMSs,\
    \ called Pivoted Query Synthesis (PQS) [\\[36\\]](#page-11-12), where a pivot\
    \ row was selected, based on which predicates were rectified to return true. Apart\
    \ from applying that idea in a different context, we also propose a different\
    \ rectification strategy that eschews mirroring the predicate's execution logic\
    \ in the testing tool, which was required for realizing PQS.\n\nWe implemented\
    \ our approach as a tool named XPress,[1](#page-1-0) which, to the best of our\
    \ knowledge, is the first general automated testing tool for XML processors, and\
    \ tested our method on six mature and widely-used XML processors BaseX, eXist-DB,\
    \ Saxon, PostgreSQL, libXML2 and a commercial DBMS. The experimental results show\
    \ that our approach is effective in detecting XPath-related logic bugs in XML\
    \ processors. We found 27 previously unknown unique bugs, not covered by existing\
    \ test suites, of which 19 were logic bugs. 25 of them have been confirmed, and\
    \ 20 of them have been fixed. Furthermore, these test cases have been integrated\
    \ into the aforementioned qt3 test suite, so that they can detect potential bugs\
    \ in XML processors that we have not tested. Our experiments demonstrate that\
    \ our proposed guided query generation process improves testing efficiency by\
    \ finding 2× more unique bugs within 24 hours in BaseX as compared to random generation.\
    \ Given the high effectiveness and efficiency of the approach, we believe it will\
    \ likely be adopted by developers of XML processors to improve their systems.\n\
    \nTo summarize, we make the following contributions:\n\n- We propose the first\
    \ general approach for automatically testing XML processors in order to find logic\
    \ bugs.\n- We implemented and evaluated our approach on six widelyused XML process\
    \ systems, which successfully found 27 previously-unknown unique bugs.\n\n```\n\
    <Books>\n   ① <Book id=\"1\" year=\"2020\">\n       <Author name=\"Sam\"/>\n \
    \      <Author name=\"Bob\"/>\n       A fairy tale.\n     </Book>\n   ② <Book\
    \ id=\"2\">\n       <Author name=\"Alice\"/>\n       Fiction.\n     </Book>\n\
    \   ③ <Book id=\"3\" year=\"2023\">\n       History of fairy kingdom.\n     </Book>\n\
    </Books>\n                                 XPath:\n                          \
    \        //*[@id*(-1)<2]\n                                   BaseX: {}\n     \
    \                              Saxon: ①②③\n                                  \
    \ eXist: ①②③\n```\nFigure 1: Example XML and motivating example."
  token_usage: 9186
  time_usage: 1.9669079780578613
- title: 'MicroFuzz: An Efficient Fuzzing Framework for Microservices'
  abstract: 'This paper presents a novel fuzzing framework, called MicroFuzz, specifically

    designed for Microservices. Mocking-Assisted Seed Execution, Distributed

    Tracing, Seed Refresh and Pipeline Parallelism approaches are adopted to

    address the environmental complexities and dynamics of Microservices and

    improve the efficiency of fuzzing. MicroFuzz has been successfully implemented

    and deployed in Ant Group, a prominent FinTech company. Its performance has

    been evaluated in three distinct industrial scenarios: normalized fuzzing,

    iteration testing, and taint verification.Throughout five months of operation,

    MicroFuzz has diligently analyzed a substantial codebase, consisting of 261

    Apps with over 74.6 million lines of code (LOC). The framework''s effectiveness

    is evident in its detection of 5,718 potential quality or security risks, with

    1,764 of them confirmed and fixed as actual security threats by software

    specialists. Moreover, MicroFuzz significantly increased program coverage by

    12.24% and detected program behavior by 38.42% in the iteration testing.'
  url: http://arxiv.org/abs/2401.05529v1
  keywords: ''
  document: '# MicroFuzz: An Efficient Fuzzing Framework for Microservices


    Peng Di Ant Group Hangzhou, China dipeng.dp@antgroup.com


    Bingchang Liu Ant Group Beijing, China bingchang.lbc@antgroup.com


    Yiyi Gao Ant Group Hangzhou, China gaoyiyi.gyy@antgroup.com


    ## ABSTRACT


    Fuzzing is a widely adopted technique in the software industry to enhance security
    and software quality. However, most existing fuzzers are specifically designed
    for monolithic software architectures and face significant limitations when it
    comes to serving distributed Microservices applications (Apps). These limitations
    primarily revolve around issues of inconsistency, communication, and applicability
    which arise due to the differences in monolithic and distributed software architecture.


    This paper presents a novel fuzzing framework, called Micro-Fuzz, specifically
    designed for Microservices. Mocking-Assisted Seed Execution, Distributed Tracing,
    Seed Refresh and Pipeline Parallelism approaches are adopted to address the environmental
    complexities and dynamics of Microservices and improve the efficiency of fuzzing.
    MicroFuzz has been successfully implemented and deployed in AntGroup [1](#page-0-0)
    , a prominent FinTech company. Its performance has been evaluated in three distinct
    industrial scenarios: normalized fuzzing, iteration testing, and taint verification.


    Throughout five months of operation, MicroFuzz has diligently analyzed a substantial
    codebase, consisting of 261 Apps with over 74.6 million lines of code (LOC). The
    framework''s effectiveness is evident in its detection of 5,718 potential quality
    or security risks, with 1,764 of them confirmed and fixed as actual security threats
    by software specialists. Moreover, MicroFuzz significantly increased line coverage
    by 12.24% and detected new paths by 38.42% in the iteration testing.


    #### ACM Reference Format:


    Peng Di, Bingchang Liu, and Yiyi Gao. 2024. MicroFuzz: An Efficient Fuzzing Framework
    for Microservices. In 46th International Conference on Software Engineering: Software
    Engineering in Practice (ICSE-SEIP ''24), April 14–20, 2024, Lisbon, Portugal.
    ACM, New York, NY, USA, [12](#page-11-0) pages. <https://doi.org/10.1145/3639477.3639723>


    ### <span id="page-0-0"></span>1 INTRODUCTION


    Fuzzing is a widely adopted technique in the software industry to enhance security
    and ensure software quality. Several fuzzers such as AFL [\[49\]](#page-11-1),
    libFuzzer [\[31\]](#page-10-0), honggfuzz [\[18\]](#page-10-1) and their extensions
    [\[7,](#page-10-2) [8,](#page-10-3) [16,](#page-10-4) [27,](#page-10-5) [30,](#page-10-6)
    [38\]](#page-10-7) have successfully uncovered numerous bugs in both open-source
    and commercial programs [\[19\]](#page-10-8). However, the existing fuzzers primarily
    focus on monolithic software


    ICSE-SEIP ''24, April 14–20, 2024, Lisbon, Portugal


    © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
    ACM ISBN 979-8-4007-0501-4/24/04. . . \$15.00 <https://doi.org/10.1145/3639477.3639723>


    wherein the program''s components or functions are tightly coupled. They are not
    readily applicable to fuzzing Microservices software, which has emerged as one
    of the most popular software architectures in the industry [\[35,](#page-10-9)
    [46\]](#page-11-2). The adaptation of fuzzing techniques for microservices faces
    obstacles due to the differences between monolithic and distributed software architectures
    [\[10,](#page-10-10) [43,](#page-11-3) [51,](#page-11-4) [52\]](#page-11-5).


    Inconsistency. Existing fuzzers struggle to adapt to the intricate Microservices
    in the industry, primarily due to their inability to address Microservices'' inconsistency
    issue that refers to the execution paths that are not repeated or predictable.
    The inconsistency arises from the unpredictable runtime logic of Microservice
    frameworks [\[3\]](#page-10-11). Additionally, the independent development and
    redeployment of Microservice Applications (abbr. Apps) can result in temporary
    failures in cross-App invocations [\[55\]](#page-11-6). These factors contribute
    to the inconsistency observed in Microservices, posing a challenge for enabling
    existing fuzzers in Microservices.


    Communication. Given that Microservice Apps are often deployed separately in different
    containers, the communication overhead between the target Apps and the fuzzer
    is essential for efficiency [\[35,](#page-10-9) [46\]](#page-11-2). Traditional
    fuzzers often overlook the network consumption considerations specific to Microservices.
    Coordinating different fuzzing components and hiding the substantial time delay
    caused by such architectural disparities remains a daunting and challenge task
    for Microservice fuzzers.


    Applicability. Existing fuzzers fail to meet the cost requirements associated
    with the extensive code bases prevalent in the industry. For instance, the Microservices
    software in AntGroup <sup>1</sup> comprises more than 3,000 Microservices Apps
    and encompasses hundreds of millions of lines of code. To minimize the cost of
    fuzzing, it is crucial to determine when to terminate the fuzzing process across
    thousands of Apps and assess the impact of termination on fuzzing effectiveness.


    In this paper, we introduce MicroFuzz, a novel fuzzing framework specifically
    designed and implemented to tackle the challenges associated with Microservices
    architecture. For Inconsistency, we overcome the oracles of environmental complexities
    and dynamics using Mocking-Assisted Seed Execution and Seed Refresh techniques,
    and propose a Microservice testing harness by virtue of the above two and Distributed
    Tracing techniques. To address Communication, we decouple different fuzzing components,
    making each of them parallel work in pipelines, to mitigate the low efficiency
    caused by the across-App network communication in Microservice software. Besides,
    for Applicability, we utilized the ecological idea of the study [\[6\]](#page-10-12)
    to fuzz the industrial-level Microservice software, greatly saving the CPU consumption,
    and make our Microservice fuzzing framework cut out for large-scale industrial
    scenarios: we take Iteration Testing and Taint Verification, two important software


    Permission to make digital or hard copies of all or part of this work for personal
    or classroom use is granted without fee provided that copies are not made or distributed
    for profit or commercial advantage and that copies bear this notice and the full
    citation on the first page. Copyrights for components of this work owned by others
    than the author(s) must be honored. Abstracting with credit is permitted. To copy
    otherwise, or republish, to post on servers or to redistribute to lists, requires
    prior specific permission and/or a fee. Request permissions from permissions@acm.org.


    <sup>1</sup>AntGroup is a prominent FinTech company that serves billions of users
    worldwide and processes 1 million user requests per minute on average.


    quality assurance processes used in enterprises, as examples to demonstrate MicroFuzz''s
    applicability in industrial fields.


    We successfully deployed MicroFuzz in AntGroup to evaluate a total of 261 representative
    Apps from a pool of over 3,000 Microservice Apps. These Apps spanned various business
    domains, including e-commerce, insurance, and investments, and encompassed commonly
    used Apps such as account, trade, and payment. These Apps generated significant
    traffic and had substantial representations within the FinTech Microservices.
    Over five months, MicroFuzz uncovered 5,718 potential security risks, out of which
    1,764 risks were confirmed through internal cybersecurity exercises, highlighting
    the effectiveness of the tool in identifying genuine vulnerabilities. Furthermore,
    MicroFuzz significantly increased line coverage by an average of 12.24% across
    the tested applications. MicroFuzz also enhanced the richness of testing in the
    Iteration Testing scenario by 38.42% more program paths.


    In summary, we make the following contributions:


    - We have developed an innovative fuzzing framework called MicroFuzz specifically
    designed for Microservices Software. Our framework, MicroFuzz, is tailored to
    effectively and efficiently fuzz Microservice software deployed in cloud environments.

    - We have employed the Mocking-Assisted Seed Execution, Distributed Tracing and
    Seed Refresh techniques to tackle the complexities and dynamics of Microservice
    software and facilitate Microservice fuzzing. Furthermore, we have leveraged Pipeline
    Parallelism to decouple fuzzer components and improve the overall efficiency of
    Microservice fuzzing.

    - MicroFuzz has been successfully deployed at AntGroup and utilized for evaluating
    hundreds of Microservices Apps. Over five months, it has effectively identified
    and confirmed 1,764 quality issues and security threats, resulting in significant
    improvements to program coverage with an average increase of 12.24%. Moreover,
    MicroFuzz provides support for various program assurance processes, as it exhibits
    38.42% more program paths in the iteration testing scenario.


    ### 2 BACKGROUND


    In this section, we describe some key features of Microservices and some challenges
    posed by these features to fuzzing.


    ### 2.1 Microservices


    Microservices [\[35,](#page-10-9) [46\]](#page-11-2) is an architecture pattern
    that arranges an application as a collection of loosely coupled, fine-grained
    services, communicating through network protocols. In this paper, a system implemented
    with this architecture is called Microservices Software, while each service working
    as a component is called Microservices Applications (abbr. App). Figure [1](#page-2-0)
    depicts an example of Microservices Software for processing goods purchases. The
    software consists of six Microservices Apps, each with a specific and bounded
    functionality. For instance, service C handles account-related transactions, while
    service B manages order-related transactions. This architecture promotes modularity
    and specialization within the system.


    Microservices has several key features:


    - Independence. Each App usually is developed and maintained by an independent
    team and is independently deployed. This makes the development of a Microservice
    software more flexible than a monolithic software.

    - Interactivity. While Microservices Apps can be developed and deployed independently,
    they still require communication with each other through network protocols like
    HTTP and RPC (Remote Procedure Call) to collaborate on user requests. In other
    words, from the perspective of effectively handling user requests, there is interdependence
    and interactivity among the Microservices Apps.

    - Relative Complexity. In contrast to a monolithic software with equivalent functionality,
    each individual Microservices App has a smaller size and lower complexity. However,
    when considering the Microservices Software as a whole, it is different. Microservices
    in AntGroup typically comprise thousands of Apps, encompassing hundreds of millions
    of lines of code. This indicates that the overall complexity is considerably high.

    - Rapid Evolution. Development-independence of each App makes Microservice Software
    (as a whole) always be in a stage of evolution in industrial scenes, even if each
    App is relatively stable. In AntGroup, for instance, there are thousands of merge
    requests (MR) being committed daily, and an average of one new version of an App
    is released every 4.3 days.


    ## <span id="page-1-0"></span>2.2 Challenges


    The above features of Microservices bring the convenience of deployment and development,
    but pose some challenges to fuzzing:


    • Challenge 1. How to enable a fuzzer for Microservices? Problem 1 (Inconsistency)
    remains a significant obstacle for existing fuzzing techniques when it comes to
    testing complex Microservice software. The fuzzing technique was originally designed
    for testing deterministic programs, where multiple runs of an input consistently
    follow the same program path. For programs with uncertain attributes (e.g., decision
    conditions determined by random numbers), traditional fuzzers are not suitable
    because the inconsistent program behaviors are hugely disruptive to the functions
    of seed selection and trimming, two important phases of the fuzzing workflow.
    To solve this issue, we adopt the Mocking-assisted seed execution approach to
    approximately guarantee a consistent behavior (more detailed in [§3.2\)](#page-2-1).


    Problem 2 (Incomplete Program Coverage) Fuzzing technique requires a total program
    coverage as the feedback of each fuzzing iteration. In monolithic software, coverage
    information is typically collected by directly reading the local memory. However,
    due to the distributed nature of Microservices, the coverage collection can only
    achieve multiple partial and disconnected coverages instead of obtaining a complete
    coverage view across all Microservices. (more detailed in [§3.3\)](#page-3-0).
    In a word, the lack of microservice testing harness disables traditional fuzzing
    techniques to test Microservice software.


    • Challenge 2. How to make Microservice fuzzing efficient? Taking each App of
    an industrial-level Microservice software as a target, fuzzing faces two efficiency
    problems, caused by environmental complexities, dynamics, and architectural differences.


    <span id="page-2-0"></span>![](_page_2_Figure_1.jpeg)


    Figure 1: An example of Microservices and its Apps.


    Problem 3 (Inconsistency-derived Inefficiency) The adoption of the mocking-assisted
    technique helps address Problem 1, but it is not a perfect solution. Striving
    for strict consistency in fuzzing requires frequent replay of existing seeds,
    which can impact performance. To balance consistency and performance in an efficient
    manner, we propose Seed Refresh & Life-cycle Management that involves periodically
    refreshing seed inputs during the fuzzing process (more detailed in [§3.4\)](#page-4-0).


    Problem 4 (High Network Cost) Due to inherent architectural differences, fuzzing
    an App deployed in a cloud environment poses distinct challenges compared to fuzzing
    desktop software or client-server-style protocol applications, e.g. AFL [\[49\]](#page-11-1).
    The fuzzer and the target App need to be deployed in separate containers, often
    residing on different hosts. This implies that communication between the fuzzer
    and the target App occurs through network protocols rather than the low-cost Inter-Process
    Communication (IPC) mechanism used by AFL and similar tools.


    An App typically exposes interfaces used for transmitting requests to other Apps
    and receiving their corresponding responses. To ensure efficiency, it is imperative
    to design a novel fuzzing framework that addresses the inefficiencies arising
    from network communication between Apps. To solve it, we redesign the fuzzing
    workflow by decoupling various fuzzing phases (more detailed in [§3.5\)](#page-4-1).


    • Challenge 3. How to scale Microservice fuzzing to real industrial scenarios?
    Fuzzing is costly due to the continuous seed generation and execution. When to
    terminate it and whether the termination impacts the fuzzing effectiveness or
    not are important applicable metrics in the enterprise. For applicability, our
    approach is inspired by the concept of ecology-based fuzzing [\[6\]](#page-10-12)
    and implemented for the normalized fuzzing (more detailed in [§4.1\)](#page-4-2).
    Besides, we explore the combination of our MicroFuzz with other techniques to
    assure program quality and make two real practices in AntGroup (more detailed
    in [§4\)](#page-4-3).


    ### 3 APPROACHES AND DESIGNS


    We propose and develop a fuzzing framework targeting at Microservices Software,
    named MicroFuzz. In this section, we will describe the overall architecture of
    MicroFuzz and detail our solutions to address the identified challenges.


    ## 3.1 MicroFuzz''s Architecture


    Figure [2](#page-3-1) shows the overall architecture of MicroFuzz. It comprises
    an intelligent switch and five core modules: seed selection, seed mutation, trace
    analysis, mocking-assisted seed execution, and distributed tracing. While the
    first three modules are traditional components, the latter two are newly designed
    modules serving as microservice testing harnesses, specifically enabling microservice
    fuzzing.


    ○1 Seed selection module is responsible for selecting the optimal initial seeds
    from a large set. These seeds are obtained from real Internet traffic or generated
    by our fuzzing tool in previous iterations.


    ○2 Seed mutation module is responsible for generating new test cases using mutation
    strategies commonly employed in traditional fuzzing techniques.


    - Bit/Byte Mutation: bit or byte-level flip

    - Arithmetic Mutation: add/subtract one on original integer

    - Interesting Replacement: replace some bytes with the interesting integer value
    belonging to (0, INT\_MAX)

    - Havoc Mutation: set random bytes to random values; delete or duplicate some
    byte sequences

    - Splice Mutation: splice two seeds at an arbitrary midpoint


    ○3 Mocking-assisted seed execution module is responsible for executing a seed
    by invoking either real or mocked services (more detailed in [§3.2\)](#page-2-1).


    ○4 Distributed tracing module is responsible for collecting comprehensive coverage
    information during the execution of a seed in a cluster environment (more detailed
    in [§4\)](#page-4-4).


    ○5 Trace analysis module is responsible for analyzing the execution trace of a
    seed and deciding whether to store it in databases or not. Similar to traditional
    fuzzing techniques, only seeds that trigger new crashes or cover unique program
    locations are typically stored for further investigation and utilization.


    ### <span id="page-2-1"></span>3.2 Mocking-Assisted Seed Execution


    Microservice software is highly complex and undergoes rapid evaluation due to
    frequent releases and interactivity-dependency among its component Apps. This
    inconsistency poses a challenge for traditional fuzzing techniques, making them
    unsuitable for testing Microservice software. To tackle this challenge, we utilize
    the mocking method [\[29\]](#page-10-13) to ensure approximate seed consistency,
    which allows multiple runs of a seed to follow the same program path within a
    short time period. The consistency is maintained by replacing the values of each
    mocking point with the recorded values from the previous version. The process
    begins with static analysis, where three types of mocking points are identified
    and collected:


    • System Dependencies. An App usually calls some library methods closely related
    to the operating system. Outputs of these methods are actually decided by the
    underlying OS, such


    #### <span id="page-3-1"></span>ICSE-SEIP ''24, April 14–20, 2024, Lisbon, Portugal
    Peng Di, Bingchang Liu, and Yiyi Gao


    ![](_page_3_Figure_2.jpeg)


    Figure 2: MicroFuzz''s architecture and application scenarios.


    as Math.random() method and System.currentTimeMillis() method. We call them system
    dependencies.


    - Internal Dependencies. In an App, some static fields and fields of some singleton
    beans are usually used to maintain some global (internal) states. For instance,
    the counter variable in Figure [3](#page-3-2) will be increased by one every time
    when getValue() method is called, making any seed that reaches this location be
    inconsistent due to the different counter of each run.

    - External Dependencies. One App usually refers to the exposed interfaces (e.g.
    RPC Reference of Spring) of other Apps to transfer some sub-requests to them and
    cooperate to finish user requests. Because the dependent Apps may release new
    versions during fuzzing, improper processing on these external dependencies also
    easily causes inconsistency issues.


    Corresponding to the above dependencies, we refer to the program locations involved
    as mocking points, denoted as , , and for brevity. Using the static analysis approach
    presented in [\[29\]](#page-10-13), we identify these mocking points and establish
    a consistent execution of seeds by following the steps outlined below.


    Mocking Points Collection. Given a Microservices App, all its mocking points =
    {1,2, · · · } are recursively gathered from its bytecode, using [\[29\]](#page-10-13)''s
    method: (1) are collected if the called functions are from or tainted from the
    third libraries; (2) are collected if they are static/Spring bean fields or tainted
    by any static/Spring bean fields; (3) are collected if they are the database access
    objects (DAO) invoking the Mybatis mapper interfaces, or the RPC references configured
    in the XML files via the tags of sofa:reference or sofa:binding.tr.RPC or tainted
    by DAOs and RPC references.


    Seed Record. JVM injector is utilized to record a seed''s execution. For any seed
    , the input and output at each mocking point are recorded in format of = ( , ),
    and its execution can be described using = { 1 , 2 · · · }. Note that if a mocking
    point is not covered in that run, both its input and output will be set to NULL
    as = (NULL, NULL).


    <span id="page-3-2"></span>


    | public class InternalExample {                               |  |  |  |  |  |  |

    |--------------------------------------------------------------|--|--|--|--|--|--|

    | private<br>static<br>Integer counter = 0;                    |  |  |  |  |  |  |

    | private<br>static Map<String, String> kvmap = new HashMap(); |  |  |  |  |  |  |

    | public<br>static<br>Integer getCounter() {                   |  |  |  |  |  |  |

    | return counter;                                              |  |  |  |  |  |  |

    | }                                                            |  |  |  |  |  |  |

    | public<br>static<br>String getValue( String key) {           |  |  |  |  |  |  |

    | counter++;                                                   |  |  |  |  |  |  |

    | return kvmap.getOrDefault(key, "") ;                         |  |  |  |  |  |  |

    | }                                                            |  |  |  |  |  |  |

    | public<br>static void putValue( String key, String value) {  |  |  |  |  |  |  |

    | kvmap.put(key, value) ;                                      |  |  |  |  |  |  |

    | }                                                            |  |  |  |  |  |  |

    | }                                                            |  |  |  |  |  |  |


    #### Figure 3: An example of static fields influencing the internal state.


    Seed Replay. Based on the latest = { 1 , 2 · · · }, a JVM injector is used to
    perform the value replacement in seed ''s replay: At each mocking point , the
    JVM injector captures the current input value in real-time, and then will be directly
    returned, rather than be really executed, if and only if a same input is observed
    in this execution (i.e. = ); Otherwise, a real execution will be performed, without
    any value replaced. We have established rules for assessing equality. If the features
    extracted from two inputs are categorized as similar (not necessarily identical),
    then we define them as equal.


    ## <span id="page-3-0"></span>3.3 Distributed Tracing


    Problem 2 highlights the difficulty of achieving comprehensive coverage information
    in Microservice software, which is distributedly deployed in a cluster. Traditional
    fuzzing techniques struggle to


    <span id="page-4-4"></span>MicroFuzz: An Efficient Fuzzing Framework for Microservices
    ICSE-SEIP ''24, April 14–20, 2024, Lisbon, Portugal


    ![](_page_4_Figure_1.jpeg)


    Figure 4: Distributed tracing architecture.


    accomplish their testing goals under such circumstances. To overcome this challenge,
    we propose the utilization of the distributed tracing technique.


    Our approach involves deploying agents throughout the cluster to collect coverage
    information during seed execution in Microservice software. Agents record covered
    probes and assign unique traceId to request contexts. The recorded coverage is
    hashed into coverDigest and stored locally. Unique coverDigest is transferred
    to the central Trace Collector, signaling the discovery of new program behavior
    (i.e., program path). Users can query the complete program coverage using traceId.
    The Trace Collector retrieves relevant covered probes from agents, splices them
    based on timing sequences, and generates an entire coverage description following
    the OpenTracing specification [\[36\]](#page-10-14). Figure [4](#page-4-4) illustrates
    this process.


    ### <span id="page-4-0"></span>3.4 Seed Refresh & Life-cycle Management


    In an industrial scene, Microservices exhibit high complexity and dynamics, where
    a single request to an App can trigger a sequence of service invocations. When
    focusing on a specific App, modifications to its upstream services can potentially
    impact its behavior. This is because the App relies on the output of upstream
    Apps as input to execute its own business logic. This situation is illustrated
    as Problem 1. In an enterprise setting, it is common to see hundreds to thousands
    of merge requests (MRs) being committed on a daily basis, further contributing
    to the dynamic nature of Microservices. Consider a target App with upstream services,
    where each service undergoes an average of changes per day. As a result, the program
    behaviors of the target App can potentially change up to times daily. To accurately
    capture and describe the program behaviors of the latest version of each App,
    it becomes necessary to refresh all the App''s seeds in the seed database whenever
    the target App or its upstream Apps release new versions. This process may involve
    performing seed replay up to a maximum of times per day.


    Considering the frequent software version evolution and huge amount of seeds,
    -times seed re-execution is definitely unacceptable in practice, as Problem 3
    shows. Instead of this accurate but expensive method, we propose an industrially
    degraded approach.


    <span id="page-4-5"></span>![](_page_4_Figure_9.jpeg)


    Figure 5: Pipeline parallelism.


    We make use of an event listener and two crontabs to approximately enable the
    Seed Refresh & Life-cycle Management. As Figure [2,](#page-3-1) triggered by Seed
    Refresh or Software Version Evolution, any seed in the seed database will be replayed
    by Seed Execution module every 12 hours or once the given App releases a new version.
    During such replay, those seeds will be run on the latest version of the target
    App, and invocations to other Apps will be concretely transferred to these dependent
    Apps to get real responses instead of using the mocking value stored in the database
    (i.e. Mocking-Assisted Seed Execution). The real responses will then be used to
    update the mocking database. Besides, Seed Life-cycle Management module cleanups
    all the seeds, created three days ago, together with their outdated mocking values.


    ### <span id="page-4-1"></span>3.5 Pipeline Parallelism


    Fuzz testing typically involves several steps, such as seed mutation, seed execution,
    and coverage collection. In traditional monolithic fuzzing techniques, these steps
    are performed sequentially. This means that the next fuzzing iteration can only
    begin once the current iteration is complete and coverage has been obtained. However,
    Problem 4 highlights the presence of high network costs between Microservice Apps.
    The sequential nature of traditional fuzzing significantly hampers the fuzzing
    process and reduces its efficiency. To address this issue, we have decoupled all
    the fuzzing phases, as depicted in Figure [5.](#page-4-5) In this decoupled work
    mode, the fuzzing process no longer needs to wait for the return of distributed
    tracing and analysis results. This greatly improves the efficiency of the fuzzing
    process, as it can continue without delay.


    ### <span id="page-4-3"></span>4 APPLICATIONS


    We have applied MicroFuzz in practical industrial scenarios, including normal
    fuzzing scenario to find bugs and improve coverage ([§4.1\)](#page-4-2), iteration
    testing scenario to find more behaviors manifested in an iteration ([§4.2\)](#page-5-0),
    and taint verification scenario to confirm data leakage risks found by dynamic
    taint analysis ([§4.3\)](#page-6-0). In this section, we show the details of these
    three application scenarios.


    ### <span id="page-4-2"></span>4.1 S1: Normal Fuzzing Scenario


    Fuzz testing often wastes computational power on ineffective exploration, where
    no new coverage or crashes are found. Based on the


    insights gained from AFL''s fuzzing exploration [\[49\]](#page-11-1), it has been
    observed that most program locations are covered in the early stages, with only
    a few new discoveries made later on. Therefore, an Intelligent Switch is introduced
    into MicroFuzz to control core modules (e.g. Seed Scheduler and others) and make
    power-aware decisions. Once this switch identifies the fuzzing progress trapped
    and stuck in the above inefficient loops, it will terminate the working of each
    fuzzing module immediately. The extended Fuzzing Monitor collects and analyzes
    exploration information, while the switch is activated during software updates,
    inconsistent behavior, or user commands. It can be turned off by user commands
    or the Fuzzing Monitor to optimize computational resource usage.


    Böhme et al. [\[6\]](#page-10-12) draw a strong similarity between fuzzing exploration
    and species discovery in ecology. Both involve mutation strategies, spatial distribution,
    and temporal dynamics. Statistical methods used in species discovery can be applied
    to answer key questions in fuzzing campaigns, such as the presence of undetected
    bugs, maximum program coverage achievable, and the number of additional test cases
    required to reach desired coverage.


    In the normalized fuzzing scenario, the Fuzzing Monitor is specifically designed
    to collect the information, shown in Table [1,](#page-5-1) during the whole fuzzing
    exploration.


    ### Table 1: Tracking factors and estimation.


    <span id="page-5-1"></span>


    | Tracking factors |                                                                                |  |  |  |  |  |

    |------------------|--------------------------------------------------------------------------------|--|--|--|--|--|

    | 𝑛                | the number of seeds replayed                                                   |  |  |  |  |  |

    | ˆ𝑆               | the number of all statements in target App                                     |  |  |  |  |  |

    | 𝑆<br>(𝑛)         | the number of statements covered by 𝑛<br>seeds                                 |  |  |  |  |  |

    | 𝑓1               | the number of singletons* detected                                             |  |  |  |  |  |

    | 𝑓2               | the number of doubletons* detected                                             |  |  |  |  |  |

    | 𝑄0               | the number of undiscovered statements,𝑄0<br>= ˆ𝑆<br>− 𝑆<br>(𝑛)                 |  |  |  |  |  |

    | 𝑄1               | the number of statements only executed by singleton                            |  |  |  |  |  |

    | 𝐶                | the number of stored seeds with different coverDigest                          |  |  |  |  |  |

    |                  | * singleton/doubleton mean the seed whose coverDigest has
    occurred once/twice. |  |  |  |  |  |


    We utilize the method proposed in [\[6\]](#page-10-12) to estimate our framework.


    - 1/ represents the ratio or proportion of new crashes or coverage discovered.

    - () = ()/(() + (−1) 2 1 /(2 2)) represents the upper bound of potential program
    coverage or vulnerabilities that can be discovered by continuous fuzzing exploration.

    - ( <sup>+</sup> ) <sup>=</sup> () + <sup>0</sup> [<sup>1</sup> − (<sup>1</sup>
    <sup>−</sup> 1/(<sup>0</sup> <sup>+</sup> 1))]) is the expected program coverage
    or vulnerability count after executing more seeds.


    The Fuzzing Monitor utilizes statistical estimations to make decisions regarding
    the Intelligent Switch behavior. If no new crashes or coverage are expected, indicated
    by either a low value of 1/ or () − ˆ ( + ) compared to a threshold , the monitor
    turns the switch off. Conversely, if inconsistencies are detected during seed
    replay or software version evolution, the monitor turns the switch back on. The
    intelligent on-off control optimizes computing resources without compromising
    fuzzing effectiveness. Figure [7](#page-7-0) demonstrates the successful balance
    achieved, where resources are efficiently allocated while maintaining the ability
    to discover crashes and coverage during fuzzing.


    ## <span id="page-5-0"></span>4.2 S2: Iteration Testing Scenario


    Software evolves by new feature import or logic update in each iteration. To plan
    tests for an iteration, two aspects should be considered. One is regression test
    which reruns the prior test cases to ensure the previous functions performed as
    expected after a change [\[26\]](#page-10-15). Another is to test the given software
    as thoroughly as possible to ensure that the new features are free from any residual
    vulnerabilities before release. From the above aspects, this section will detail
    the extensions of the proposed microservice fuzzing technology and introduce its
    application in iteration testing scenario.


    #### Aspect of Regression Testing.


    Due to shorter software delivery life cycles and the presence of large test suites,
    re-running all test cases for a given software under test (SUT) after each change
    can be costly [\[48\]](#page-11-7). To address this, a technique called dynamic
    regression test selection (RTS) is employed to select a subset of test cases from
    the test suite. These selected test cases are then executed on the latest version
    of the SUT to verify different program behaviors by comparing the new code changes
    with per-test execution traces [\[32,](#page-10-16) [50,](#page-11-8) [54\]](#page-11-9).
    MicroFuzz is extended with Change Impact Analysis, as depicted in Figure [2,](#page-3-1)
    to associate each execution trace with line-, block-, or method-level coverage.
    These traces are indexed based on commit information, such as the commitId or
    the name and branch of the Microservice App. This indexing allows for quick retrieval
    of relevant traces from a large set, enabling efficient test selection and execution.
    Further implementation details will be discussed in [§5.](#page-6-1)


    For an iteration where a merge request (MR) is committed from branch A to branch
    B, regression testing will be planned on test suite ∪, which is accomplished by
    following the steps:


    - Change Impact Analysis collects each different block between branch and branch
    , and then puts it into .

    - From traces database, select each trace covering any different block ∈ , and
    put it into set or if it was previously executed on branch A or branch B.

    - Based on the mapping between traces and seeds database, the corresponding seed
    for each selected trace ∈ ∪ is retrieved and stored in sets or accordingly, if
    the trace is derived from or .


    ### Aspect of New Features Testing.


    In an iteration, testing efforts are focused on exploring new basic block ∈ that
    have not been associated with any traces yet. MicroFuzz utilizes directed fuzzing
    techniques and incorporates Call Graph Analysis to obtain the call graph of the
    latest Microservice Software after each software version evolution. This is an
    incremental analysis process [\[53\]](#page-11-10). It also includes a Seed Scheduler
    and Trace Analysis, which select seeds based on the closest prior execution traces
    and calculate the shortest distances between methods using Dijkstra''s algorithm
    [\[45\]](#page-11-11). These techniques optimize seed selection for effective
    testing and exploration of new features.


    Figure [6](#page-6-2) illustrates the target method C1 for directed fuzzing exploration.
    The Seed Scheduler module assigns decreasing priority to seed3, seed1, and seed2.
    Preference is given to seed3 and seed1 as they provide better coverage or proximity
    to the target method. This preference is also extended to the Trace Analysis module
    during


    MicroFuzz: An Efficient Fuzzing Framework for Microservices ICSE-SEIP ''24, April
    14–20, 2024, Lisbon, Portugal


    <span id="page-6-2"></span>![](_page_6_Figure_2.jpeg)


    Figure 6: Shortest distance calculation in directed fuzzing technique.


    iteration testing, where seeds that can adequately cover or closely match the
    target are prioritized.


    ### <span id="page-6-0"></span>4.3 S3: Taint Verification Scenario


    Taint information plays a critical role in ensuring program quality and safeguarding
    data privacy within the industry. To capture taint information, dataflow and taint
    analysis techniques are commonly employed, allowing for the monitoring and tracing
    of data propagation from specific sources. However, existing traffic volumes often
    fall short in providing sufficient data for accurate verification. To overcome
    this limitation, our MicroFuzz bolsters verification capabilities by significantly
    increasing the volume of traffic available for analysis. In comparison to static
    taint analyzers with lower precision [\[4,](#page-10-17) [42\]](#page-11-12) and
    dynamic taint analyzers with higher costs [\[5,](#page-10-18) [11\]](#page-10-19),
    the combination of our MicroFuzz with static analyzers has demonstrated its effectiveness
    and practicality. We have successfully validated this approach through two real
    industrial practices, and further results can be found in Table [2.](#page-7-1)


    To establish the taint verification, MicroFuzz works as follows:


    - from seeds database, Seed Selection picks a seed whose prior execution trace
    reaches the source''s method entry.

    - Seed Mutation generates new seeds = {1, 2, · · · } by only performing mutations
    on the source parameter of .

    - in each ''s execution, Trace Analysis observes the sink''s value and denotes
    it as . Using the variable-controlling approach, <source,sink> is verified if
    ≠ , ≠ . Otherwise, the taint-relation is uncertain (e.g., neither existence nor
    inexistence can be decided).


    ### <span id="page-6-1"></span>5 IMPLEMENTATION


    We have successfully developed and deployed our proposed microservice fuzzing
    framework as a SOFABoot [\[23\]](#page-10-20) microservice App, named MicroFuzz.
    It comprises primarily of 263K lines of Java code. MicroFuzz can receive requests
    from either human users or CI/CD platforms, and it will commence fuzzing the designated
    Microservice App once receives a request. In the following section, we will provide
    a thorough exposition of several pivotal components.


    Seeds & Mocks Storage. In MicroFuzz,seeds and mocks databases serve as repositories
    for the respective seed and concomitant mock


    data. Similar to the fuzzing queue, the seeds database facilitates frequent CRUD
    (Create, Read, Update, and Delete) operations throughout the fuzz exploration
    process, including the data access of Seed Mutation in each fuzzing iteration
    and the bulk updating/deleting of data in regards to seed refresh and seed life-cycle
    management.


    To ensure storage durability and efficiency, our MicroFuzz utilizes the Cache-Aside
    pattern [\[34\]](#page-10-21) in conjunction with Ocean-Base [\[2\]](#page-10-22)
    and Redis [\[33\]](#page-10-23). Seeds and mocks are stored using Ocean-Base as
    the main store and Redis as the cache. Read-through and write-through strategies
    are implemented to load data from Ocean-Base into Redis on demand. Seeds are initially
    grouped by the application information, such as name, branch, or commitId, of
    the corresponding Microservice Software in OceanBase. In Redis, each seed is stored
    with its associated mocks to optimize Distributed Tracing. The framework uses
    the <coverDigest, seed> pair to associate coverage information with each seed
    in Redis, enabling faster Trace Analysis by querying coverDigest for new program
    coverage.


    ### Seed Management and Software Version Evolution.


    In MicroFuzz, the functionalities of seed refresh and seed lifecycle management
    are facilitated by an event listener and two crontabs, respectively. The periodic
    tasks of seed replay and cleanup are carried out with the aid of AntScheduler
    [\[21\]](#page-10-24). Furthermore, the feature of software version evolution
    is implemented through a callback handler instance that is embedded in the CI/CD
    pipeline and will trigger upon the merging of a new commit.


    #### Intelligent Switch.


    In MicroFuzz, the Intelligent Switch serves as the primary controller for fuzzing
    exploration. It receives signals, such as on/off commands, from various sources
    including users, software version evolution, and the Fuzzing Monitor. This functionality
    is implemented using Distributed Resource Management (DRM) [\[20\]](#page-10-25),
    which enables dynamic modification of the entrance state of each core module while
    the Microservice Software is running. By setting the state of an entrance to off,
    the corresponding fuzzing module becomes inaccessible unless it is reset to on
    again. This capability allows the Intelligent Switch to initiate or terminate
    a fuzzing exploration in real time based on the received signals.


    Trace-Block Association.


    <span id="page-7-1"></span>


    | Microservice | Effectiveness |         |      |            | Efficiency |          |          |          |          |         |

    |--------------|---------------|---------|------|------------|------------|----------|----------|----------|----------|---------|

    |              | #LOC          | COV (%) | VUL  | New Traces | Taints     | #TT
    (ms) | #TS (ms) | #TO (ms) | #TD (ms) | SAV (X) |

    | M1           | 306K          | +23.88% | 3    | #          | 9          | 55.26    |
    3.06     | 172.32   | 117.32   | 0.46    |

    | M2           | 704K          | +19.96% | 2    | #          | 90         | 66.64    |
    2.74     | 139.64   | 73.62    | 0.89    |

    | M3           | 31K           | +8.54%  | 0    | #          | 28         | 54.35    |
    3.18     | 93.26    | 39.18    | 1.38    |

    | M4           | 94K           | +17.83% | 0    | #          | 33         | 57.13    |
    2.23     | 89.4     | 32.19    | 1.78    |

    | M5           | 818K          | +21.05% | 3    | #          | 56         | 49.7     |
    3.02     | 190.28   | 140.9    | 0.35    |

    | M6           | 156K          | +17.48% | 0    | 46.42%     | 8          | 51.86    |
    2.32     | 63.64    | 12.17    | 4.22    |

    | M7           | 273K          | +18.29% | 1    | 22.99%     | 6          | 53.43    |
    2.14     | 89.67    | 35.89    | 1.5     |

    | M8           | 504K          | +13.03% | 3    | 307.59%    | 6          | 52.72    |
    2.29     | 100.04   | 47.92    | 1.09    |

    | M9           | 75K           | +16.12% | 0    | 176.72%    | 467        | 49.52    |
    2.79     | 97.38    | 47.14    | 1.07    |

    | M10          | 120K          | +19.67% | 0    | 109.09%    | 15         | 62.2     |
    2.22     | 71.2     | 9.02     | 6.89    |

    | 10 Aggs      | 24.96M        | +17.59% | 12   | 135.54%    | 718        | 55.28    |
    2.60     | #        | #        | 1.96    |

    | 261 Aggs     | 74.6M         | +12.24% | 3831 | 38.42%     | 5718       | 52.72    |
    2.89     | #        | #        | 2.67    |


    Table 2: Experimental results on 10 example Microservice Apps.


    To establish associations between traces and blocks, which is crucial for both
    regression testing and directed fuzzing, we employ Geabase [\[15\]](#page-10-26),
    a graph database, in MicroFuzz. When each software version evolves, a new call
    graph will be stored in Geabase and named using its git commit information. This
    allows for the identification and differentiation of different versions of the
    Microservice App. Each seed is associated with node of this graph once it reaches
    the corresponding basic block of during execution on the commitId version.


    #### Data Flow Monitor.


    To implement the lightweight dynamic taint verification approach discussed in
    [§4.3,](#page-6-0) a JVM injector is utilized as part of the Distributed Tracing
    process. This injector is responsible for gathering the values of service inputs
    and data access objects (DAOs) during the execution of the system. The purpose
    of this is to verify the taint relations between services or between services
    and the database. These taint relations are represented as <source, sink>, where
    the source indicates the taint position of the inputs (such as the parameters
    of a service), and the sink represents either the same position or a database
    operation. SOFAMQ [\[22\]](#page-10-27), a distributed message middleware that
    builds upon RocketMQ [\[1\]](#page-10-28), is employed to decouple the key fuzzing
    modules, allowing the seamless and concurrent operations of the key fuzzing modules.


    ### 6 EVALUATION


    In this section, we present our evaluation setup and performance of our tool,
    MicroFuzz, corresponding to the 3 key challenges defined in § [2.2.](#page-1-0)
    That''s to say, we try to answer three questions:


    - Q1: Can MicroFuzz effectively fuzz Microservice Apps deployed in an industrial
    environment, and what is its performance?

    - Q2: How efficient MicroFuzz is at fuzzing Microservice Apps?

    - Q3: How does MicroFuzz perform when applied in other scenarios except normal
    fuzzing?


    It is worth noting that, to the best of our knowledge, MicroFuzz is likely the
    first practical fuzzer for industrial-level Microservice software. As a result,
    when establishing baseline comparisons, we rely on user-provided data and feedback.
    Indeed, checking all the


    #### **Power-Saving of Intelligent Switch**


    <span id="page-7-0"></span>![](_page_7_Figure_14.jpeg)


    Figure 7: Experimental results of Intelligent Switch.


    results from MicroFuzz can be a challenging task, even for developers. This is
    primarily due to the large volume of data generated by the fuzzing process, often
    reaching thousands of entries for each Microservice App. Reviewing and analyzing
    such a vast amount of data can be time-consuming and resource-intensive.


    ## 6.1 Setup


    We have deployed MicroFuzz on a cluster of elastic cloud instances, each equipped
    with eight 2.5GHz cores and 32GB of RAM. Within the same cloud environment, we
    have deployed over 3,000 targeted Microservice Apps. MicroFuzz has been running
    continuously for more than five months under this specific setup.


    ### 6.2 Q1 - Effectiveness


    ### Vulnerability Discovery Scenario.


    In order to assess the efficacy of MicroFuzz in fuzzing Microservice Apps within
    an industrial cloud environment, we conducted


    <span id="page-8-0"></span>Table 3: Vulnerability types on 261 target Microservice
    Apps.


    | Vul Type | Vul Detail                 | Count |

    |----------|----------------------------|-------|

    |          | Biz_Exception              | 254   |

    | Biz_Vul  | Biz_Error                  | 3548  |

    |          | Null Pointer Exception     | 3     |

    |          | SQL Exception              | 22    |

    | Sys_Vul  | NumberFormatException      | 1     |

    |          | UncleardThrowableException | 1     |

    |          | IOException                | 2     |


    a random selection process. Out of the over 3,000 Apps available, we chose 261
    Apps to serve as our fuzzing targets. These selected Apps hold significant importance
    as they are fundamental, hightraffic, and heavily relied upon by other Apps in
    the ecosystem. Compared with others, they are more mature and fully tested in
    various business domains.


    After five months of intensive fuzzing, 3,831 exception reports were uncovered.


    Table [3](#page-8-0) presents a breakdown of the reported exception types, revealing
    Biz\_Error (a type of business error) as the most prevalent. Additionally, critical
    exceptions such as Null Pointer Exception have been identified during the evaluation
    process. Most of Sys\_Vul exceptions have been patched by Apps'' developers.


    Furthermore, we specifically chose 10 sample Apps to present a more comprehensive
    overview of the effectiveness of MicroFuzz, as shown in Table [2.](#page-7-1)
    These Apps have a line of code (LoC) ranging from 31,000 to 818,000. The added
    code coverage achieved by MicroFuzz ranges from 8.54% to 23.88%, with an average
    of 17.59%, and 12 exceptions were found among these 10 Apps. In total, the collective
    code coverage of the 261 Apps is 12.24% on average, and 3,831 exception reports
    have been submitted.


    Overall, we conclude that MicroFuzz is effective in improving code coverage and
    discovering potential vulnerabilities.


    ## 6.3 Q2 - Efficiency


    In addition to effectiveness, we also evaluated the efficiency of MicroFuzz. For
    illustration purposes, we still take the selected 10 Apps as examples and present
    the results in Table [2.](#page-7-1) The #TT column denotes the average time taken
    for Distributed Tracing, which is responsible for collecting runtime information
    during the fuzzing process for each App. The average time duration for Distributed
    Tracing across the 10 Apps is 52.72ms, ranging from 49.7ms to 66.64ms. This duration
    includes both the coverDigest query and crash capture procedures. #TS denotes
    the time consumption of Distributed Tracing spent for data collection at each
    service entry or data access point. Each time takes around 2~3 ms on average,
    which proves our Distributed Tracing technique is efficient in the industrial
    scene. The results demonstrate that our Distributed Tracing technique efficiently
    collects runtime information while maintaining the fuzzing performance at a satisfactory
    level.


    The columns labeled #TO and #TD represent the total time required for a single
    fuzzing iteration with and without Pipeline Parallelism respectively. The SAV
    column denotes the improved efficiency achieved through Pipeline Parallelism,
    calculated as SAV = #TD/#TO - 1. The efficiency improvement achieved through the


    use of Pipeline Parallelism varies. For example, App M6 and M10 experienced a
    4.22x and 6.89x improvement. Conversely, the improvement for App M1 was relatively
    modest, with only a 0.46x increase. The difference comes from each App''s characteristics.
    On average, however, the efficiency improvement across all Apps was notable, with
    a factor of 2.67x improvement achieved when employing the Pipeline Parallelism
    mechanism.


    We evaluated the resource efficiency of the Intelligent Switch mechanism, which
    optimizes computing resources by assessing the need for further fuzzing based
    on monitoring results. To demonstrate its effectiveness, we conducted a comparative
    study using M1 fuzzing as a case study, as depicted in Figure [7.](#page-7-0)
    In this study, we performed two experiments: one with the Intelligent Switch enabled
    and another with it disabled. We plotted the changes in the proportion of discovered
    vulnerabilities over time (represented by the red line) and the code coverage
    ratio (represented by the blue line). With the Intelligent Switch enabled, the
    fuzzer terminated after 4.6 hours as the mechanism determined that no new paths
    could be discovered, thus conserving computational resources. Conversely, in the
    control experiment without the Intelligent Switch, the fuzzer continued running,
    but no new paths were found, resulting in wasted resources. On average, enabling
    the Intelligent Switch allowed for a 61.7% reduction in computing resources while
    maintaining virtually no loss in path coverage. We have observed similar phenomena
    across nine other Apps as well. This showcases the significant resource savings
    achieved by leveraging the effectiveness of the Intelligent Switch mechanism.


    In summary, our adapted mechanism incorporating Distributed Tracing, Pipeline
    Parallelism, and Intelligent Switch demonstrates high efficiency in terms of both
    time and computing resources. This outstanding performance establishes MicroFuzz
    as an efficient tool for fuzzing Microservice Apps within an industrial cloud
    environment.


    ### 6.4 Q3 - Applicability


    We assessed the applicability of MicroFuzz in two additional scenarios: iteration
    testing and taint verification.


    ### Iteration Testing Scenario.


    When a code change is made to a Microservice App, it is crucial to generate relevant
    test cases that cover the modified code blocks to uncover any potential bugs,
    called iteration testing. To address this, MicroFuzz leverages directed fuzzing
    techniques. In this specific use case, we evaluate MicroFuzz''s ability to generate
    new traces specifically targeting the modified code segments. To achieve this,
    we select traffic data that includes the code blocks affected by the changes as
    initial fuzzing seeds. We then analyze the number of newly discovered traces produced
    by MicroFuzz as a measure of its effectiveness in capturing the behavior of the
    modified code segments.


    Figure [8](#page-9-0) presents the average count of newly generated traces during
    the fuzzing process of the 10 selected Apps using Micro-Fuzz, as depicted in Table
    [2.](#page-7-1) #RI represents the count of initial seeds, while #RT indicates
    the number of traces discovered during the fuzzing process, including the initial
    seeds. Taking the example of the M6 App, there were initially 280 seeds, and after
    the fuzzing process, a total of 410 traces were discovered. This indicates that


    <span id="page-9-0"></span>![](_page_9_Figure_1.jpeg)


    **Fuzzing Effectiveness**


    Figure 8: Experimental results in iteration testing scenario.


    MicroFuzz uncovered 130 new paths, resulting in an effectiveness rate of 46.24%.
    The average effectiveness of the M6-M10 Apps, as depicted in the Traces column
    of Table [2,](#page-7-1) was computed. Overall, MicroFuzz demonstrated the capability
    to discover 38.42% of new paths during each App''s iteration. It is important
    to note that the M1-M5 Apps do not have historical traffic data covering the changed
    code blocks. Therefore, in cases where a new functional module is introduced in
    a single iteration, the initial seed numbers for these modules in Figure [8](#page-9-0)
    would be 0, and there would be no corresponding effectiveness measure data in
    Table [2.](#page-7-1)


    In summary, MicroFuzz demonstrates its effectiveness in iteration testing by successfully
    identifying new coverage paths.


    Taint Verification Scenario.


    In this particular scenario, we utilized MicroFuzz to validate the findings of
    static taint analysis, a technique prone to false positives. The testing results
    are presented in Table [2,](#page-7-1) where the Taints column represents the
    cumulative number of taint-relations confirmed by MicroFuzz. Additionally, the
    TS column indicates the time required for Distributed Tracing to collect data
    at each service entry or data access point.


    Overall, MicroFuzz identified a total of 5,718 unique possible instances of quality
    issues and security risks, of which 1,764 were confirmed by software specialists.
    These results demonstrate the effectiveness of MicroFuzz in taint verification
    scenarios, enhancing the overall accuracy of the analysis process.


    ### 7 RELATED WORK


    Fuzzers for Monolithic Software. Representative fuzzers such as AFL [\[49\]](#page-11-1),
    libFuzzer [\[31\]](#page-10-0), and honggfuzz [\[18\]](#page-10-1) provide guidance
    to other fuzzers. The typical fuzzing workflow consists of four phases: seed scheduling,
    seed mutation, seed execution, and seed selection. Various designs have been incorporated
    into these phases. Böhme et al. [\[8\]](#page-10-3) treat fuzzing exploration
    as a Markov process and propose a novel seed scheduling strategy that prioritizes
    seeds exploring lowfrequency paths for further mutations. FairFuzz [\[25\]](#page-10-29)
    introduces new seed mutation operations, such as overwriting, deleting, and


    inserting, to enhance testing. AFLGo [\[7\]](#page-10-2) devises a seed selection
    strategy that favors seeds with execution paths closer to the target locations
    in each fuzzing iteration. Similar ideas are also explored in [\[27\]](#page-10-5).
    Gan et al. [\[16\]](#page-10-4) address path collision issues in AFL by correcting
    path coverage calculations. Peng et al. [\[37\]](#page-10-30) combined directed
    fuzzing with symbolic execution to reproduce 1-day vulnerabilities. In recent
    years, many approaches have incorporated AI techniques into the seed mutation
    phase. For example, Zong et al. [\[57\]](#page-11-13) improve directed fuzzing
    efficiency by filtering out inputs predicted to be unreachable to targets. In
    [\[40\]](#page-11-14), an LSTM model is used to learn the mutable positions of
    inputs. Godefroid et al. [\[17\]](#page-10-31) employ an RNN to learn the grammar
    of program inputs using numerous test cases and use the learned grammar to generate
    new inputs. NEUZZ [\[41\]](#page-11-15) applies the concept of gradient descent
    to smooth the neural network model and significantly enhances program coverage
    by learning program branches. With the recent enormous advances in Large Language
    Models (LLMs), TitanFuzz [\[12\]](#page-10-32) and FuzzGPT [\[13\]](#page-10-33)
    have been proposed to directly leverage LLMs for fuzzing DL libraries.


    Unlike other fuzzing frameworks, MicroFuzz does not aim to improve existing fuzzing
    strategies. Instead, its main objective is to provide support and enable the effective
    implementation of these strategies specifically for Microservice software.


    Parallel Fuzzing. Existing approaches have made advancements in enhancing the
    performance of parallel fuzzing by focusing on improving the fuzzing strategy
    [\[9,](#page-10-34) [28,](#page-10-35) [39,](#page-10-36) [44,](#page-11-16) [56\]](#page-11-17)
    or increasing the fuzzing speed [\[14,](#page-10-37) [47\]](#page-11-18). One
    common approach to enhance the fuzzing strategy is task partitioning. PAFL [\[28\]](#page-10-35)introduces
    an effective method to synchronize guiding information and statically divide fuzzing
    tasks based on branching information to minimize overlap between instances. AFLEdge
    [\[44\]](#page-11-16) utilizes static analysis to dynamically create exclusive
    and evenly weighted fuzzing tasks. Another strategy to improve fuzzing is through
    ensemble fuzzing [\[9\]](#page-10-34) or collaborative fuzzing [\[24\]](#page-10-38),
    where the strengths of different fuzzers are combined. By fuzzing the same target
    with multiple fuzzers and sharing their progress, overall performance can be improved.
    EnFuzz [\[9\]](#page-10-34) designs heuristics to evaluate fuzzer diversity and
    selects the most diverse subset for ensemble fuzzing through efficient seed synchronization.
    Cupid [\[24\]](#page-10-38) proposes a collaborative fuzzing framework that automatically
    discovers the optimal combination of fuzzers for a target. One challenge in parallel
    fuzzing is the operating system bottleneck. Xu et al. [\[47\]](#page-11-18) address
    this by introducing new operating primitives that enhance scalability and performance
    in parallel fuzzing, mitigating file system contention and scalability issues.


    A recent endeavor in parallelizing fuzzing involved the redesign of parallel fuzzing
    using a microservice architecture, as demonstrated in µFUZZ [\[10\]](#page-10-10).
    It aimed to use the CPU power in the distributed cloud for fuzzing monolithic
    software by enhancing I/O operations and eliminating synchronization.


    Nevertheless, our MicroFuzz is uniquely designed to address the specific challenges
    of identifying potential issues in Microservice software. As a result, the challenges
    we encounter differ from those encountered in other fuzzing frameworks.


    MicroFuzz: An Efficient Fuzzing Framework for Microservices ICSE-SEIP ''24, April
    14–20, 2024, Lisbon, Portugal


    ### 8 CONCLUSION


    This paper presents the first comprehensive study on Microservice fuzzing, which
    differs significantly from state-of-the-art fuzzing techniques for monolithic
    software. To facilitate Microservice fuzzing, we propose an efficient fuzzing
    framework named MicroFuzz incorporating innovative approaches such as Mocking-Assisted
    Seed Execution, Distributed Tracing, Seed Refresh, and Pipeline Parallelism to
    enhance fuzzing efficiency. After running on thousands of Apps in AntGroup, MicroFuzz
    identified 5,718 potential quality or security risks, out of which 1,764 are confirmed.
    Furthermore, Micro-Fuzz significantly enhances line coverage by 12.24% and detects
    new paths by 38.42% in the iteration testing.


    ### ACKNOWLEDGMENTS


    This work is supported by Ant Group.


    ### REFERENCES


    - <span id="page-10-28"></span>[1] 2021. Apache RocketMQ.<https://rocketmq.apache.org/>

    - <span id="page-10-22"></span>[2] Alibaba. 2021. OceanBase.<https://dbdb.io/db/oceanbase>

    - <span id="page-10-11"></span>[3] Anastasios Antoniadis, Nikos Filippakis, Paddy
    Krishnan, Raghavendra Ramesh, Nicholas Allen, and Yannis Smaragdakis. 2020. Static
    Analysis of Java Enterprise Applications: Frameworks and Caches, the Elephants
    in the Room. In Proceedings of the 41st ACM SIGPLAN Conference on Programming
    Language Design and Implementation (London, UK) (PLDI 2020). Association for Computing
    Machinery, New York, NY, USA, 794–807.<https://doi.org/10.1145/3385412.3386026>

    - <span id="page-10-17"></span>[4] Steven Arzt, Siegfried Rasthofer, Christian
    Fritz, Eric Bodden, Alexandre Bartel, Jacques Klein, Yves Le Traon, Damien Octeau,
    and Patrick McDaniel. 2014. Flow-Droid: Precise Context, Flow, Field, Object-Sensitive
    and Lifecycle-Aware Taint Analysis for Android Apps. In Proceedings of the 35th
    ACM SIGPLAN Conference on Programming Language Design and Implementation (Edinburgh,
    United Kingdom) (PLDI ''14). Association for Computing Machinery, New York, NY,
    USA, 259–269.<https://doi.org/10.1145/2594291.2594299>

    - <span id="page-10-18"></span>[5] Jonathan Bell and Gail Kaiser. 2014. Phosphor:
    Illuminating Dynamic Data Flow in Commodity Jvms. In Proceedings of the 2014 ACM
    International Conference on Object Oriented Programming Systems Languages & Applications
    (Portland, Oregon, USA) (OOPSLA ''14). Association for Computing Machinery, New
    York, NY, USA, 83–101.<https://doi.org/10.1145/2660193.2660212>

    - <span id="page-10-12"></span>[6] Marcel Böhme. 2018. STADS: Software Testing
    as Species Discovery. ACM Trans. Softw. Eng. Methodol. 27, 2, Article 7 (jun 2018),
    52 pages. [https://doi.org/10.1145/](https://doi.org/10.1145/3210309) [3210309](https://doi.org/10.1145/3210309)

    - <span id="page-10-2"></span>[7] Marcel Böhme, Van-Thuan Pham, Manh-Dung Nguyen,
    and Abhik Roychoudhury. 2017. Directed Greybox Fuzzing. In Proceedings of the
    2017 ACM SIGSAC Conference on Computer and Communications Security (Dallas, Texas,
    USA) (CCS ''17). Association for Computing Machinery, New York, NY, USA, 2329–2344.
    <https://doi.org/10.1145/3133956.3134020>

    - <span id="page-10-3"></span>[8] Marcel Böhme, Van-Thuan Pham, and Abhik Roychoudhury.
    2016. Coverage-Based Greybox Fuzzing as Markov Chain. In Proceedings of the 2016
    ACM SIGSAC Conference on Computer and Communications Security (Vienna, Austria)
    (CCS ''16). Association for Computing Machinery, New York, NY, USA, 1032–1043.
    <https://doi.org/10.1145/2976749.2978428>

    - <span id="page-10-34"></span>[9] Yuanliang Chen, Yu Jiang, Fuchen Ma, Jie Liang,
    Mingzhe Wang, Chijin Zhou, Xun Jiao, and Zhuo Su. 2019. EnFuzz: Ensemble Fuzzing
    with Seed Synchronization among Diverse Fuzzers. In 28th USENIX Security Symposium
    (USENIX Security 19). USENIX Association, Santa Clara, CA, 1967–1983. [https://www.usenix.org/](https://www.usenix.org/conference/usenixsecurity19/presentation/chen-yuanliang)
    [conference/usenixsecurity19/presentation/chen-yuanliang](https://www.usenix.org/conference/usenixsecurity19/presentation/chen-yuanliang)

    - <span id="page-10-10"></span>[10] Yongheng Chen, Rui Zhong, Yupeng Yang, Hong
    Hu, Dinghao Wu, and Wenke Lee. 2023. µFUZZ: Redesign of Parallel Fuzzing using
    Microservice Architecture. In 32nd USENIX Security Symposium (USENIX Security
    23). USENIX Association, Anaheim, CA, 1325–1342. [https://www.usenix.org/conference/usenixsecurity23/](https://www.usenix.org/conference/usenixsecurity23/presentation/chen-yongheng)
    [presentation/chen-yongheng](https://www.usenix.org/conference/usenixsecurity23/presentation/chen-yongheng)

    - <span id="page-10-19"></span>[11] James Clause, Wanchun Li, and Alessandro Orso.
    2007. Dytan: A Generic Dynamic Taint Analysis Framework. In Proceedings of the
    2007 International Symposium on Software Testing and Analysis (London, United
    Kingdom) (IS-STA ''07). Association for Computing Machinery, New York, NY, USA,
    196–206. <https://doi.org/10.1145/1273463.1273490>

    - <span id="page-10-32"></span>[12] Yinlin Deng, Chunqiu Steven Xia, Haoran Peng,
    Chenyuan Yang, and Lingming Zhang. 2023. Large Language Models are Zero-Shot Fuzzers:
    Fuzzing Deep-Learning Libraries via Large Language Models. arXiv[:2212.14834](https://arxiv.org/abs/2212.14834)
    [cs.SE]

    - <span id="page-10-33"></span>[13] Yinlin Deng, Chunqiu Steven Xia, Chenyuan
    Yang, Shizhuo Dylan Zhang, Shujing Yang, and Lingming Zhang. 2023. Large Language
    Models are Edge-Case Fuzzers: Testing Deep Learning Libraries via FuzzGPT. arXiv[:2304.02014](https://arxiv.org/abs/2304.02014)
    [cs.SE]

    - <span id="page-10-37"></span>[14] Andrea Fioraldi, Dominik Maier, Heiko Eißfeldt,
    and Marc Heuse. 2020. AFL++: Combining Incremental Steps of Fuzzing Research.
    In 14th USENIX Workshop on Offensive Technologies (WOOT 20). USENIX Association.

    - <span id="page-10-26"></span>[15] Zhisong Fu, Zhengwei Wu, Houyi Li, Yize Li,
    Min Wu, Xiaojie Chen, Xiaomeng Ye, Benquan Yu, and Xi Hu. 2017. GeaBase: A High-Performance
    Distributed Graph Database for Industry-Scale Applications. In 2017 Fifth International
    Conference on Advanced Cloud and Big Data (CBD). 170–175. [https://doi.org/10.1109/CBD.](https://doi.org/10.1109/CBD.2017.37)
    [2017.37](https://doi.org/10.1109/CBD.2017.37)

    - <span id="page-10-4"></span>[16] Shuitao Gan, Chao Zhang, Xiaojun Qin, Xuwen
    Tu, Kang Li, Zhongyu Pei, and Zuoning Chen. 2018. CollAFL: Path Sensitive Fuzzing.
    In 2018 IEEE Symposium on Security and Privacy (SP). 679–696.<https://doi.org/10.1109/SP.2018.00040>

    - <span id="page-10-31"></span>[17] Patrice Godefroid, Hila Peleg, and Rishabh
    Singh. 2017. Learn&Fuzz: Machine Learning for Input Fuzzing. CoRR abs/1701.07232
    (2017). arXiv[:1701.07232](https://arxiv.org/abs/1701.07232) <http://arxiv.org/abs/1701.07232>

    - <span id="page-10-1"></span>[18] Google. 2018. honggfuzz.<hhttps://github.com/google/honggfuzz>

    - <span id="page-10-8"></span>[19] Google. 2022. ClusterFuzz Trophies.<https://google.github.io/clusterfuzz#trophies>

    - <span id="page-10-25"></span>[20] Ant Group. 2020. Introduction to SOFAStack
    Microservices).

    - <span id="page-10-24"></span>[21] Ant Group. 2021. AntScheduler.<https://github.com/mcalus3/AntScheduler>

    - <span id="page-10-27"></span>[22] Ant Group. 2021. SOFAMQ.<https://github.com/sofastack-guides/sofamq-demo>

    - <span id="page-10-38"></span><span id="page-10-20"></span>[23] Ant Group. 2021.
    SOFASTACK.<https://github.com/sofastack> [24] Emre Güler, Philipp Görz, Elia Geretto,
    Andrea Jemmett, Sebastian Österlund, Herbert Bos, Cristiano Giuffrida, and Thorsten
    Holz. 2020. Cupid: Automatic Fuzzer

    - Selection for Collaborative Fuzzing. In Annual Computer Security Applications
    Conference (Austin, USA) (ACSAC ''20). Association for Computing Machinery, New
    York, NY, USA, 360–372.<https://doi.org/10.1145/3427228.3427266>

    - <span id="page-10-29"></span>[25] Caroline Lemieux and Koushik Sen. 2018. FairFuzz:
    A Targeted Mutation Strategy for Increasing Greybox Fuzz Testing Coverage. In
    2018 33rd IEEE/ACM International Conference on Automated Software Engineering
    (ASE). 475–485. <https://doi.org/10.1145/3238147.3238176>

    - <span id="page-10-15"></span>[26] Hareton K. N. Leung and Lee J. White. 1989.
    Insights into regression testing (software testing). Proceedings. Conference on
    Software Maintenance - 1989 (1989), 60–69.

    - <span id="page-10-5"></span>[27] Yuekang Li, Bihuan Chen, Mahinthan Chandramohan,
    Shang-Wei Lin, Yang Liu, and Alwen Tiu. 2017. Steelix: Program-State Based Binary
    Fuzzing. In Proceedings of the 2017 11th Joint Meeting on Foundations of Software
    Engineering (Paderborn, Germany) (ESEC/FSE 2017). Association for Computing Machinery,
    New York, NY, USA, 627–637.<https://doi.org/10.1145/3106237.3106295>

    - <span id="page-10-35"></span>[28] Jie Liang, Yu Jiang, Yuanliang Chen, Mingzhe
    Wang, Chijin Zhou, and Jiaguang Sun. 2018. PAFL: Extend Fuzzing Optimizations
    of Single Mode to Industrial Parallel Mode. In Proceedings of the 2018 26th ACM
    Joint Meeting on European Software Engineering Conference and Symposium on the
    Foundations of Software Engineering (Lake Buena Vista, FL, USA) (ESEC/FSE 2018).
    Association for Computing Machinery, New York, NY, USA, 809–814.<https://doi.org/10.1145/3236024.3275525>

    - <span id="page-10-13"></span>[29] Jiangchao Liu, Jierui Liu, Peng Di, Alex X.
    Liu, and Zexin Zhong. 2022. Record and Replay of Online Traffic for Microservices
    with Automatic Mocking Point Identification. In Proceedings of the 44th International
    Conference on Software Engineering: Software Engineering in Practice (Pittsburgh,
    Pennsylvania) (ICSE-SEIP ''22). Association for Computing Machinery, New York,
    NY, USA, 221–230. <https://doi.org/10.1145/3510457.3513029>

    - <span id="page-10-6"></span>[30] Zixi Liu, Yang Feng, Yining Yin, Jingyu Sun,
    Zhenyu Chen, and Baowen Xu. 2023. QATest: A Uniform Fuzzing Framework for Question
    Answering Systems. In Proceedings of the 37th IEEE/ACM International Conference
    on Automated Software Engineering (Rochester, MI, USA) (ASE ''22). Association
    for Computing Machinery, New York, NY, USA, Article 81, 12 pages.<https://doi.org/10.1145/3551349.3556929>

    - <span id="page-10-16"></span><span id="page-10-0"></span>[31] LLVM. 2018. libFuzzer.<https://llvm.org/docs/LibFuzzer.html>
    [32] Zhenyue Long, Zeliu Ao, Guoquan Wu, Wei Chen, and Jun Wei. 2020. WebRTS:
    A Dynamic Regression Test Selection Tool for Java Web Applications. In 2020 IEEE
    International Conference on Software Maintenance and Evolution (ICSME).

    - 822–825.<https://doi.org/10.1109/ICSME46990.2020.00102>

    - <span id="page-10-23"></span>[33] Redis Ltd. [n. d.]. Redis.<https://redis.io/>

    - <span id="page-10-21"></span>[34] Microsoft. 2022. Cache-Aside pattern. [https://learn.microsoft.com/en-us/azure/](https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside)
    [architecture/patterns/cache-aside](https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside)

    - <span id="page-10-9"></span>[35] Sam Newman. 2021. Building microservices. "
    O''Reilly Media, Inc.".

    - <span id="page-10-14"></span>[36] OpenTracing. [n. d.]. .<https://opentracing.io/>

    - <span id="page-10-30"></span>[37] Jiaqi Peng, Feng Li, Bingchang Liu, Lili Xu,
    Binghong Liu, Kai Chen, and Wei Huo. 2019. 1dvul: Discovering 1-day vulnerabilities
    through binary patches. In 2019 49th Annual IEEE/IFIP International Conference
    on Dependable Systems and Networks (DSN). IEEE, 605–616.

    - <span id="page-10-7"></span>[38] Theofilos Petsios, Adrian Tang, Salvatore Stolfo,
    Angelos D. Keromytis, and Suman Jana. 2017. NEZHA: Efficient Domain-Independent
    Differential Testing. In 2017 IEEE Symposium on Security and Privacy (SP). 615–632.
    [https://doi.org/](https://doi.org/10.1109/SP.2017.27) [10.1109/SP.2017.27](https://doi.org/10.1109/SP.2017.27)

    - <span id="page-10-36"></span>[39] Van-Thuan Pham, Manh-Dung Nguyen, Quang-Trung
    Ta, Toby Murray, and Benjamin I.P. Rubinstein. 2021. Towards Systematic and Dynamic
    Task Allocation for Collaborative Parallel Fuzzing. In 2021 36th IEEE/ACM International
    Conference on Automated Software Engineering (ASE). 1337–1341. [https://doi.org/10.1109/](https://doi.org/10.1109/ASE51524.2021.9678810)
    [ASE51524.2021.9678810](https://doi.org/10.1109/ASE51524.2021.9678810)


    <span id="page-11-0"></span>ICSE-SEIP ''24, April 14–20, 2024, Lisbon, Portugal
    Peng Di, Bingchang Liu, and Yiyi Gao


    - <span id="page-11-14"></span>[40] Mohit Rajpal, William Blum, and Rishabh Singh.
    2017. Not all bytes are equal: Neural byte sieve for fuzzing. CoRR abs/1711.04596
    (2017). arXiv[:1711.04596](https://arxiv.org/abs/1711.04596) <http://arxiv.org/abs/1711.04596>

    - <span id="page-11-15"></span>[41] Dongdong She, Kexin Pei, Dave Epstein, Junfeng
    Yang, Baishakhi Ray, and Suman Jana. 2019. NEUZZ: Efficient Fuzzing with Neural
    Program Smoothing. In 2019 IEEE Symposium on Security and Privacy (SP). 803–817.
    [https://doi.org/10.1109/](https://doi.org/10.1109/SP.2019.00052) [SP.2019.00052](https://doi.org/10.1109/SP.2019.00052)

    - <span id="page-11-12"></span>[42] Manu Sridharan, Shay Artzi, Marco Pistoia,
    Salvatore Guarnieri, Omer Tripp, and Ryan Berg. 2011. F4F: Taint Analysis of Framework-Based
    Web Applications. In Proceedings of the 2011 ACM International Conference on Object
    Oriented Programming Systems Languages and Applications (Portland, Oregon, USA)
    (OOPSLA ''11). Association for Computing Machinery, New York, NY, USA, 1053–1068.
    <https://doi.org/10.1145/2048066.2048145>

    - <span id="page-11-3"></span>[43] W. Wang, A. Benea, and F. Ivancic. 2023. Zero-Config
    Fuzzing for Microservices. In 2023 38th IEEE/ACM International Conference on Automated
    Software Engineering (ASE). IEEE Computer Society, Los Alamitos, CA, USA, 1840–1845.
    [https://doi.](https://doi.org/10.1109/ASE56229.2023.00036) [org/10.1109/ASE56229.2023.00036](https://doi.org/10.1109/ASE56229.2023.00036)

    - <span id="page-11-16"></span>[44] Yifan Wang, Yuchen Zhang, Chenbin Pang, Peng
    Li, Nikolaos Triandopoulos, and Jun Xu. 2021. Facilitating Parallel Fuzzing with
    Mutually-Exclusive Task Distribution. In Secur. Priv. Commun. Networks, Joaquin
    Garcia-Alfaro, Shujun Li, Radha Poovendran, Hervé Debar, and Moti Yung (Eds.).
    Springer International Publishing, Cham, 185–206.

    - <span id="page-11-11"></span>[45] Wiki. [n. d.]. Dijkstra''s Algorithm. [https://en.wikipedia.org/wiki/Dijkstra%27s\\_](https://en.wikipedia.org/wiki/Dijkstra%27s_algorithm)
    [algorithm](https://en.wikipedia.org/wiki/Dijkstra%27s_algorithm)

    - <span id="page-11-2"></span>[46] Wikipedia. [n. d.]. Microservices. Retrieved
    Feb 14, 2023 from [https://en.wikipedia.](https://en.wikipedia.org/wiki/Microservices)
    [org/wiki/Microservices](https://en.wikipedia.org/wiki/Microservices)

    - <span id="page-11-18"></span>[47] Wen Xu, Sanidhya Kashyap, Changwoo Min, and
    Taesoo Kim. 2017. Designing New Operating Primitives to Improve Fuzzing Performance.
    In Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications
    Security (Dallas, Texas, USA) (CCS ''17). Association for Computing Machinery,
    New York, NY, USA, 2313–2328.<https://doi.org/10.1145/3133956.3134046>

    - <span id="page-11-7"></span>[48] S. Yoo and M. Harman. 2012. Regression Testing
    Minimization, Selection and Prioritization: A Survey. Softw. Test. Verif. Reliab.
    22, 2 (mar 2012), 67–120. [https:](https://doi.org/10.1002/stv.430) [//doi.org/10.1002/stv.430](https://doi.org/10.1002/stv.430)

    - <span id="page-11-1"></span>[49] Michal Zalewski. 2014. American Fuzzing Loop.<https://lcamtuf.coredump.cx/afl/>

    - <span id="page-11-8"></span>[50] Lingming Zhang. 2018. Hybrid Regression Test
    Selection. In Proceedings of the 40th International Conference on Software Engineering
    (Gothenburg, Sweden) (ICSE ''18). Association for Computing Machinery, New York,
    NY, USA, 199–209. <https://doi.org/10.1145/3180155.3180198>

    - <span id="page-11-4"></span>[51] Man Zhang, Andrea Arcuri, Yonggang Li, Yang
    Liu, and Kaiming Xue. 2023. White-Box Fuzzing RPC-Based APIs with EvoMaster: An
    Industrial Case Study. ACM Trans. Softw. Eng. Methodol. 32, 5, Article 122 (jul
    2023), 38 pages. [https:](https://doi.org/10.1145/3585009) [//doi.org/10.1145/3585009](https://doi.org/10.1145/3585009)

    - <span id="page-11-5"></span>[52] Man Zhang, Andrea Arcuri, Yonggang Li, Kaiming
    Xue, Zhao Wang, Jian Huo, and Weiwei Huang. 2022. Fuzzing Microservices In Industry:
    Experience of Applying EvoMaster at Meituan. arXiv[:2208.03988](https://arxiv.org/abs/2208.03988)
    [cs.SE]

    - <span id="page-11-10"></span>[53] Zelin Zhao, Xizao Wang, Zhaogui Xu, Zhenhao
    Tang, Yongchao Li, and Peng Di. 2023. Incremental Call Graph Construction in Industrial
    Practice. In 2023 IEEE/ACM 45th International Conference on Software Engineering:
    Software Engineering in Practice (ICSE-SEIP). IEEE, 471–482.

    - <span id="page-11-9"></span>[54] Hua Zhong, Lingming Zhang, and Sarfraz Khurshid.
    2019. TestSage: Regression Test Selection for Large-Scale Web Service Testing.
    In 2019 12th IEEE Conference on Software Testing, Validation and Verification
    (ICST). 430–440. [https://doi.org/](https://doi.org/10.1109/ICST.2019.00052) [10.1109/ICST.2019.00052](https://doi.org/10.1109/ICST.2019.00052)

    - <span id="page-11-6"></span>[55] Zexin Zhong, Jiangchao Liu, Diyu Wu, Peng Di,
    Yulei Sui, Alex X. Liu, and John C. S. Lui. 2023. Scalable Compositional Static
    Taint Analysis for Sensitive Data Tracing on Industrial Micro-Services. In 2023
    IEEE/ACM 45th International Conference on Software Engineering: Software Engineering
    in Practice (ICSE-SEIP). 110–121.<https://doi.org/10.1109/ICSE-SEIP58684.2023.00015>

    - <span id="page-11-17"></span>[56] Xu Zhou, Pengfei Wang, Chenyifan Liu, Tai
    Yue, Yingying Liu, Congxi Song, Kai Lu, and Qidi Yin. 2020. UniFuzz: Optimizing
    Distributed Fuzzing via Dynamic Centralized Task Scheduling. ArXiv abs/2009.06124
    (2020). [https:](https://api.semanticscholar.org/CorpusID:221655823) [//api.semanticscholar.org/CorpusID:221655823](https://api.semanticscholar.org/CorpusID:221655823)

    - <span id="page-11-13"></span>[57] Peiyuan Zong, Tao Lv, Dawei Wang, Zizhuang
    Deng, Ruigang Liang, and Kai Chen. 2020. FuzzGuard: Filtering out Unreachable
    Inputs in Directed Grey-box Fuzzing through Deep Learning. In 29th USENIX Security
    Symposium (USENIX Security 20). USENIX Association, 2255–2269. [https://www.usenix.org/conference/](https://www.usenix.org/conference/usenixsecurity20/presentation/zong)
    [usenixsecurity20/presentation/zong](https://www.usenix.org/conference/usenixsecurity20/presentation/zong)'
  decisions:
    language: '- Qualified. Reason: English Paper'
    evaluation_prompt: Qualified.
    related_work_prompt: Qualified
    novelty_prompt: Qualified
    review_only_prompt: Qualified.
  llm_input_used: '## Abstract

    This paper presents a novel fuzzing framework, called MicroFuzz, specifically

    designed for Microservices. Mocking-Assisted Seed Execution, Distributed

    Tracing, Seed Refresh and Pipeline Parallelism approaches are adopted to

    address the environmental complexities and dynamics of Microservices and

    improve the efficiency of fuzzing. MicroFuzz has been successfully implemented

    and deployed in Ant Group, a prominent FinTech company. Its performance has

    been evaluated in three distinct industrial scenarios: normalized fuzzing,

    iteration testing, and taint verification.Throughout five months of operation,

    MicroFuzz has diligently analyzed a substantial codebase, consisting of 261

    Apps with over 74.6 million lines of code (LOC). The framework''s effectiveness

    is evident in its detection of 5,718 potential quality or security risks, with

    1,764 of them confirmed and fixed as actual security threats by software

    specialists. Moreover, MicroFuzz significantly increased program coverage by

    12.24% and detected program behavior by 38.42% in the iteration testing.


    ## Introduction

    Fuzzing is a widely adopted technique in the software industry to enhance security
    and ensure software quality. Several fuzzers such as AFL [\[49\]](#page-11-1),
    libFuzzer [\[31\]](#page-10-0), honggfuzz [\[18\]](#page-10-1) and their extensions
    [\[7,](#page-10-2) [8,](#page-10-3) [16,](#page-10-4) [27,](#page-10-5) [30,](#page-10-6)
    [38\]](#page-10-7) have successfully uncovered numerous bugs in both open-source
    and commercial programs [\[19\]](#page-10-8). However, the existing fuzzers primarily
    focus on monolithic software


    ICSE-SEIP ''24, April 14–20, 2024, Lisbon, Portugal


    © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
    ACM ISBN 979-8-4007-0501-4/24/04. . . \$15.00 <https://doi.org/10.1145/3639477.3639723>


    wherein the program''s components or functions are tightly coupled. They are not
    readily applicable to fuzzing Microservices software, which has emerged as one
    of the most popular software architectures in the industry [\[35,](#page-10-9)
    [46\]](#page-11-2). The adaptation of fuzzing techniques for microservices faces
    obstacles due to the differences between monolithic and distributed software architectures
    [\[10,](#page-10-10) [43,](#page-11-3) [51,](#page-11-4) [52\]](#page-11-5).


    Inconsistency. Existing fuzzers struggle to adapt to the intricate Microservices
    in the industry, primarily due to their inability to address Microservices'' inconsistency
    issue that refers to the execution paths that are not repeated or predictable.
    The inconsistency arises from the unpredictable runtime logic of Microservice
    frameworks [\[3\]](#page-10-11). Additionally, the independent development and
    redeployment of Microservice Applications (abbr. Apps) can result in temporary
    failures in cross-App invocations [\[55\]](#page-11-6). These factors contribute
    to the inconsistency observed in Microservices, posing a challenge for enabling
    existing fuzzers in Microservices.


    Communication. Given that Microservice Apps are often deployed separately in different
    containers, the communication overhead between the target Apps and the fuzzer
    is essential for efficiency [\[35,](#page-10-9) [46\]](#page-11-2). Traditional
    fuzzers often overlook the network consumption considerations specific to Microservices.
    Coordinating different fuzzing components and hiding the substantial time delay
    caused by such architectural disparities remains a daunting and challenge task
    for Microservice fuzzers.


    Applicability. Existing fuzzers fail to meet the cost requirements associated
    with the extensive code bases prevalent in the industry. For instance, the Microservices
    software in AntGroup <sup>1</sup> comprises more than 3,000 Microservices Apps
    and encompasses hundreds of millions of lines of code. To minimize the cost of
    fuzzing, it is crucial to determine when to terminate the fuzzing process across
    thousands of Apps and assess the impact of termination on fuzzing effectiveness.


    In this paper, we introduce MicroFuzz, a novel fuzzing framework specifically
    designed and implemented to tackle the challenges associated with Microservices
    architecture. For Inconsistency, we overcome the oracles of environmental complexities
    and dynamics using Mocking-Assisted Seed Execution and Seed Refresh techniques,
    and propose a Microservice testing harness by virtue of the above two and Distributed
    Tracing techniques. To address Communication, we decouple different fuzzing components,
    making each of them parallel work in pipelines, to mitigate the low efficiency
    caused by the across-App network communication in Microservice software. Besides,
    for Applicability, we utilized the ecological idea of the study [\[6\]](#page-10-12)
    to fuzz the industrial-level Microservice software, greatly saving the CPU consumption,
    and make our Microservice fuzzing framework cut out for large-scale industrial
    scenarios: we take Iteration Testing and Taint Verification, two important software


    Permission to make digital or hard copies of all or part of this work for personal
    or classroom use is granted without fee provided that copies are not made or distributed
    for profit or commercial advantage and that copies bear this notice and the full
    citation on the first page. Copyrights for components of this work owned by others
    than the author(s) must be honored. Abstracting with credit is permitted. To copy
    otherwise, or republish, to post on servers or to redistribute to lists, requires
    prior specific permission and/or a fee. Request permissions from permissions@acm.org.


    <sup>1</sup>AntGroup is a prominent FinTech company that serves billions of users
    worldwide and processes 1 million user requests per minute on average.


    quality assurance processes used in enterprises, as examples to demonstrate MicroFuzz''s
    applicability in industrial fields.


    We successfully deployed MicroFuzz in AntGroup to evaluate a total of 261 representative
    Apps from a pool of over 3,000 Microservice Apps. These Apps spanned various business
    domains, including e-commerce, insurance, and investments, and encompassed commonly
    used Apps such as account, trade, and payment. These Apps generated significant
    traffic and had substantial representations within the FinTech Microservices.
    Over five months, MicroFuzz uncovered 5,718 potential security risks, out of which
    1,764 risks were confirmed through internal cybersecurity exercises, highlighting
    the effectiveness of the tool in identifying genuine vulnerabilities. Furthermore,
    MicroFuzz significantly increased line coverage by an average of 12.24% across
    the tested applications. MicroFuzz also enhanced the richness of testing in the
    Iteration Testing scenario by 38.42% more program paths.


    In summary, we make the following contributions:


    - We have developed an innovative fuzzing framework called MicroFuzz specifically
    designed for Microservices Software. Our framework, MicroFuzz, is tailored to
    effectively and efficiently fuzz Microservice software deployed in cloud environments.

    - We have employed the Mocking-Assisted Seed Execution, Distributed Tracing and
    Seed Refresh techniques to tackle the complexities and dynamics of Microservice
    software and facilitate Microservice fuzzing. Furthermore, we have leveraged Pipeline
    Parallelism to decouple fuzzer components and improve the overall efficiency of
    Microservice fuzzing.

    - MicroFuzz has been successfully deployed at AntGroup and utilized for evaluating
    hundreds of Microservices Apps. Over five months, it has effectively identified
    and confirmed 1,764 quality issues and security threats, resulting in significant
    improvements to program coverage with an average increase of 12.24%. Moreover,
    MicroFuzz provides support for various program assurance processes, as it exhibits
    38.42% more program paths in the iteration testing scenario.'
  token_usage: 7435
  time_usage: 1.9199323654174805
- title: Optimistic Prediction of Synchronization-Reversal Data Races
  abstract: "Dynamic data race detection has emerged as a key technique for ensuring\n\
    reliability of concurrent software in practice. However, dynamic approaches can\n\
    often miss data races owing to nondeterminism in the thread scheduler.\nPredictive\
    \ race detection techniques cater to this shortcoming by inferring\nalternate\
    \ executions that may expose data races without re-executing the\nunderlying program.\
    \ More formally, the dynamic data race prediction problem\nasks, given a trace\
    \ \\sigma of an execution of a concurrent program, can \\sigma\nbe correctly reordered\
    \ to expose a data race? Existing state-of-the art\ntechniques for data race prediction\
    \ either do not scale to executions arising\nfrom real world concurrent software,\
    \ or only expose a limited class of data\nraces, such as those that can be exposed\
    \ without reversing the order of\nsynchronization operations.\n  In general, exposing\
    \ data races by reasoning about synchronization reversals\nis an intractable problem.\
    \ In this work, we identify a class of data races,\ncalled Optimistic Sync(hronization)-Reversal\
    \ races that can be detected in a\ntractable manner and often include non-trivial\
    \ data races that cannot be\nexposed by prior tractable techniques. We also propose\
    \ a sound algorithm OSR\nfor detecting all optimistic sync-reversal data races\
    \ in overall quadratic\ntime, and show that the algorithm is optimal by establishing\
    \ a matching lower\nbound. Our experiments demonstrate the effectiveness of OSR\
    \ on our extensive\nsuite of benchmarks, OSR reports the largest number of data\
    \ races, and scales\nwell to large execution traces."
  url: http://arxiv.org/abs/2401.05642v1
  keywords: ''
  document: "# Optimistic Prediction of Synchronization-Reversal Data Races\n\n[Zheng\
    \ Shi](https://orcid.org/0000-0001-5021-7134) National University of Singapore\
    \ Singapore, Singapore shizheng@u.nus.edu\n\n[Umang Mathur](https://orcid.org/0000-0002-7610-0660)\
    \ National University of Singapore Singapore, Singapore umathur@comp.nus.edu.sg\n\
    \n[Andreas Pavlogiannis](https://orcid.org/0000-0002-8943-0722) Aarhus University\
    \ Aarhus, Denmark pavlogiannis@cs.au.dk\n\n# ABSTRACT\n\nDynamic data race detection\
    \ has emerged as a key technique for ensuring reliability of concurrent software\
    \ in practice. However, dynamic approaches can often miss data races owing to\
    \ nondeterminism in the thread scheduler. Predictive race detection techniques\
    \ cater to this shortcoming by inferring alternate executions that may expose\
    \ data races without re-executing the underlying program. More formally, the dynamic\
    \ data race prediction problem asks, given a trace of an execution of a concurrent\
    \ program, can be correctly reordered to expose a data race? Existing state-of-the\
    \ art techniques for data race prediction either do not scale to executions arising\
    \ from real world concurrent software, or only expose a limited class of data\
    \ races, such as those that can be exposed without reversing the order of synchronization\
    \ operations.\n\nIn general, exposing data races by reasoning about synchronization\
    \ reversals is an intractable problem. In this work, we identify a class of data\
    \ races, called Optimistic Sync(hronization)-Reversal races that can be detected\
    \ in a tractable manner and often include non-trivial data races that cannot be\
    \ exposed by prior tractable techniques. We also propose a sound algorithm OSR\
    \ for detecting all optimistic sync-reversal data races in overall quadratic time,\
    \ and show that the algorithm is optimal by establishing a matching lower bound.\
    \ Our experiments demonstrate the effectiveness of OSR— on our extensive suite\
    \ of benchmarks, OSR reports the largest number of data races, and scales well\
    \ to large execution traces.\n\n#### ACM Reference Format:\n\nZheng Shi, Umang\
    \ Mathur, and Andreas Pavlogiannis. 2024. Optimistic Prediction of Synchronization-Reversal\
    \ Data Races. In Proceedings of International Conference on Software Engineering\
    \ (ICSE '24). ACM, New York, NY, USA, [28](#page-27-0) pages.<https://doi.org/10.1145/nnnnnnn.nnnnnnn>\n\
    \n#### <span id=\"page-0-1\"></span>1 INTRODUCTION\n\nConcurrency bugs such as\
    \ data races and deadlocks often escape in-house testing and manifest only in\
    \ production [\\[19,](#page-11-0) [59\\]](#page-12-0), making the development\
    \ of reliable concurrent software a challenging task. Automated data race detection\
    \ has emerged as a first line of defense against undesired behaviors caused by\
    \ data races, has been actively studied over multiple decades, and is also the\
    \ subject of this paper. In particular, our focus is on dynamic analyses, which,\
    \ unlike static\n\nICSE '24, April 12–21, 2024, Lisbon, Portugal\n\n© 2024 Association\
    \ for Computing Machinery.\n\nACM ISBN 978-x-xxxx-xxxx-x/YY/MM. . . \\$15.00\n\
    \n<https://doi.org/10.1145/nnnnnnn.nnnnnnn>\n\n<span id=\"page-0-0\"></span>![](_page_0_Figure_16.jpeg)\n\
    \nFigure 1: The two conflicting events <sup>1</sup> = ⟨1, w()⟩ and <sup>12</sup>\
    \ = ⟨4, w()⟩ is a predictable data race of <sup>1</sup> which is also an optimistic\
    \ sync-reversal race, witnessed by the correct reordering <sup>1</sup> that reverses\
    \ critical sections.\n\ntechniques, are the preferred class of techniques for\
    \ detecting data races for industrial scale software applications [\\[59\\]](#page-12-0).\n\
    \nA dynamic data race detector observes an execution of a concurrent program and\
    \ infers the presence of a data race by analysing the trace of the observed execution.\
    \ A key challenge in the design of such a technique is sensitivity to non-deterministic\
    \ thread schedules — even for a fixed program input, a data race may be observed\
    \ under a very specific thread schedule, but not under other thread schedules.\
    \ This means that a simplistic race detector that, say, only checks for two conflicting\
    \ events appearing simultaneously in the execution trace, is likely going to miss\
    \ many bugs. This is where predictive analysis techniques shine — instead of looking\
    \ for bugs only in the execution that was observed, they additionally also detect\
    \ bugs in executions that, while not explicitly observed during testing, can nevertheless\
    \ be inferred from the observed execution, without rerunning the underlying program\
    \ [\\[31,](#page-11-1) [34,](#page-11-2) [45,](#page-11-3) [57,](#page-12-1) [60,](#page-12-2)\
    \ [65\\]](#page-12-3). Predictive techniques identify the space of executions\
    \ or reorderings that can provably be inferred from a given observed execution\
    \ , and then look for a reordering in this space, that can serve as a witness\
    \ to a bug such as a data race. Consider the execution 1 in Figure [1a](#page-0-0)\
    \ consisting of events 1, 2, . . . , <sup>12</sup> where denotes the th event\
    \ from the top. The two write events on variable , 1 and 12, are far apart and\
    \ not witnessed as a data race in 1. However, the correct reordering 1 of 1, in\
    \ which the two write events appear consecutively, shows that it is nevertheless,\
    \ a predictable data race of 1. Indeed any program that generates <sup>1</sup>\
    \ will also generate 1 albeit with a different thread interleaving.\n\nIn general,\
    \ sound (no false positives) and complete (no false negatives) data race prediction\
    \ is known to be an intractable problem [\\[44\\]](#page-11-4). Soundness is a\
    \ key desired property, since false positives need to be otherwise vetted manually,\
    \ a task which is particularly challenging in the case of concurrent programs.\
    \ Consequently,\n\nPermission to make digital or hard copies of all or part of\
    \ this work for personal or classroom use is granted without fee provided that\
    \ copies are not made or distributed for profit or commercial advantage and that\
    \ copies bear this notice and the full citation on the first page. Copyrights\
    \ for components of this work owned by others than ACM must be honored. Abstracting\
    \ with credit is permitted. To copy otherwise, or republish, to post on servers\
    \ or to redistribute to lists, requires prior specific permission and/or a fee.\
    \ Request permissions from permissions@acm.org.\n\nmany recent works counter the\
    \ intractability by proposing incomplete (but nevertheless sound) predictive race\
    \ detection algorithms that work in polynomial time and have high precision in\
    \ practice. The main contribution of this paper is a new race prediction algorithm\
    \ OSR that is sound, has higher prediction power than prior algorithms and achieves\
    \ high scalability in practice.\n\nThe design of our algorithm OSR stems from\
    \ the observation that often, data races can be exposed only by inverting the\
    \ relative order of (some pairs of) critical sections, or synchronizations. The\
    \ data race (1, 12) in Figure [1a,](#page-0-0) for instance, can in fact only\
    \ be observed in correct reorderings that invert the order of the two critical\
    \ sections on lock ℓ. However, reversing synchronization (lock/unlock) operations\
    \ in the reordering can further force a reversal in the order in which memory\
    \ access events must appear in the reordering, and can be intractable to reason\
    \ about [\\[44,](#page-11-4) [45\\]](#page-11-3). This strong tradeoff between\
    \ precision (obtained by virtue of reversing the order of many synchronization\
    \ operations) and performance has materialized on both the extremes. Algorithms\
    \ such as those based on the happens-before partial order [\\[42,](#page-11-5)\
    \ [55\\]](#page-12-4) or the recently proposed SyncP [\\[45\\]](#page-11-3) run\
    \ in linear time but fail to expose races that mandate reasoning about synchronization\
    \ reversals. On the other extreme, methods that exhaustively search for reversals,\
    \ either resort to expensive constraint solving [\\[31,](#page-11-1) [60\\]](#page-12-2)\
    \ or saturation style reasoning [\\[18,](#page-11-6) [54\\]](#page-12-5), and\
    \ do not scale to long execution traces observed in real world concurrent applications.\
    \ Our proposed algorithm OSR aims to strike a balance — it is designed to optimistically\
    \ reason about synchronization reversals, and identifies those reversals that\
    \ do not lead to the reversal of memory operations. The pair (1, 12) in Figure\
    \ [1](#page-0-0) is an example of a race that OSR reports.\n\nOSR reports all\
    \ optimistic synchronization-reversal races in overall time e(N<sup>2</sup> ),\
    \ spending e(N ) time for processing each event in the given execution trace .\
    \ Here, N is the number of events in and <sup>e</sup> hides polynomial multiplicative\
    \ factors due to number of locks and threads which are typically considered constants.\
    \ In order to check for the absence of memory reversals, OSR constructs a graph\
    \ (optimistic reordering graph) of events and checks if it is acyclic. Naively,\
    \ such an acyclicity check would take e(N ) time for every pair of conflicting\
    \ events, resulting in a total cubic running time. A key technical contribution\
    \ of our work is to perform this check in amortized constant time by constructing\
    \ a succinct representation of this graph, called abstract optimistic reordering\
    \ graph, of constant size. We show that this abstract graph preserves acyclicity,\
    \ and can be constructed in an incremental manner in amortized constant time,\
    \ allowing us to perform race prediction for the entire input execution in overall\
    \ quadratic (instead of cubic) time. Finally, we show that the problem of checking\
    \ the existence of an optimistic sync-reversal race also admits a matching quadratic\
    \ time lower bound, thereby implying that our algorithm is optimal.\n\nWe implemented\
    \ OSR and evaluate its performance thoroughly. Our evaluation demonstrates the\
    \ effectiveness of our algorithm on a comprehensive suite of 153 Java and C/C++\
    \ benchmarks derived from real-world programs. Our results show OSR has comparable\
    \ scalability as linear time algorithms SyncP and WCP, while it reports significantly\
    \ more races than the second most predictive one on many benchmarks, confirming\
    \ our hypothesis that going beyond the principle of synchronisation preservation\
    \ allows us\n\nto discover significantly more races and with better performance.\
    \ OSR, thus, advances the state-of-the-art in sound predictive race detection.\n\
    \nThe rest of the paper is organized as follows. In Section [2,](#page-1-0) we\
    \ discuss relevant background. In Section [3,](#page-3-0) we formally define the\
    \ notion of optimistic sync-reversal races, and present our algorithm OSR for\
    \ detecting all optimistic sync-reversal races in Section [4.](#page-4-0) Our\
    \ evaluation of OSR and its comparison with other race prediction algorithms is\
    \ presented in Section [5.](#page-7-0) In Section [6](#page-8-0) we discuss related\
    \ work and conclude in Section [7.](#page-10-0)\n\n#### <span id=\"page-1-0\"\
    ></span>2 PRELIMINARIES\n\nIn this section, we discuss preliminary notation and\
    \ the formal definition of the problem of dynamic data race prediction. Next,\
    \ we briefly recall the notion of sync-preserving data races [\\[45\\]](#page-11-3)\
    \ and discuss some of the limitations of this notion, paving the way to our algorithm\
    \ OSR.\n\nTrace and events. An execution trace (or simply trace) of a concurrent\
    \ program is a sequence of events = 1<sup>2</sup> . . . N. An event is a tuple\
    \ = ⟨, , ⟩, where is a unique identifier for , is the thread that performs and\
    \ is the operation corresponding to ; often the identifier will be clear from\
    \ context and we will drop it. We use th() and op() to denote the thread and operation\
    \ of . Operations are r(), w() (read or write access of memory location or variable\
    \ ) or acq(ℓ), rel(ℓ) (acquire or release of lock ℓ); fork and join operations\
    \ are omitted from presentation but not from our implementation. For a trace ,\
    \ we will use Events(), Threads(), Vars(), Locks() to denote respectively the\
    \ set of all events, threads, variables and locks appearing in .\n\nWell-formedness.\
    \ We assume that traces are well-formed, in that they do not violate lock semantics.\
    \ In particular, for a well formed trace , we require that for each lock ℓ ∈ Locks(),\
    \ the sequence of operations on ℓ alternate between acquires and releases, where\
    \ each release event is preceded by a matching acquire event of the same thread.\
    \ For an acquire (resp. release) event , we use the notation match () to denote\
    \ the matching release (resp. acquire) event of in if one exists; otherwise we\
    \ say match () = ⊥.\n\nTrace order, thread order and reads-from. The trace order\
    \ ≤ tr of a trace is the total order induced by the sequence of events in , i.e.,\
    \ <sup>1</sup> ≤ tr <sup>2</sup> iff either <sup>1</sup> <sup>=</sup> <sup>2</sup>\
    \ or <sup>1</sup> appears earlier than <sup>2</sup> in . The thread order ≤ TO\
    \ is a partial order on Events() such that for any two events 1, 2, we have <sup>1</sup>\
    \ ≤ TO <sup>2</sup> iff th(1) <sup>=</sup> th(2) and <sup>1</sup> ≤ tr 2. When\
    \ looking for predictable data races, we often look for reorderings of a given\
    \ trace that preserve its control flow, and determine this using the reads-from\
    \ function. For a read event ∈ Events() with op() = r() for some variable , the\
    \ writer of , denoted = rf () is the last write event on before , i.e., op() =\
    \ w(), ≤ tr and ¬(∃ ′ ≠ , op( ′ ) = w() ∧ ≤ tr ′ ≤ tr ). Without loss of generality,\
    \ we will assume that rf () is always defined for each read event . Given a set\
    \ ⊆ Events(), we say that is (≤ TO,rf )-closed if (a) for all events 1, <sup>2</sup>\
    \ ∈ Events() if (<sup>1</sup> ≤ TO 2∧<sup>2</sup> <sup>∈</sup> ), then <sup>1</sup>\
    \ <sup>∈</sup> , and (b) for all events ∀1, <sup>2</sup> ∈ Events(), if (<sup>1</sup>\
    \ = rf (2) ∧ <sup>2</sup> ∈ ), then <sup>1</sup> ∈ . We use TRClosure(S) to denote\
    \ the smallest set ′ such that ⊆ ′ and ′ is (≤ TO,rf )-closed.\n\n<span id=\"\
    page-2-0\"></span>![](_page_2_Figure_1.jpeg)\n\n2 Figure 2: The two write events\
    \ <sup>1</sup> = ⟨1, w()⟩ and <sup>5</sup> = ⟨2, w()⟩ in <sup>2</sup> are conflicting.\
    \ (1, 5) is not a data race but a predictable data race of 2, witnessed by correct\
    \ reorderings <sup>2</sup> and ′ 2 .\n\nCorrect reordering. Predictive race detection,\
    \ given a trace , asks if an alternate execution trace witnesses a data race,\
    \ and more importantly, can be inferred from . The notion of correct reorderings\
    \ precisely formalizes this. Given well-formed traces and , with Events() ⊆ Events(),\
    \ we say that is a correct reordering of if respects the thread order and reads-from\
    \ relations of . This means that (1) Events() is (≤ TO,rf )-closed, (2) for any\
    \ two events 1, <sup>2</sup> ∈ Events(), if <sup>1</sup> ≤ TO 2, then <sup>1</sup>\
    \ <sup>≤</sup> TO 2, and (3) for any two events 1, <sup>2</sup> ∈ Events(), if\
    \ <sup>1</sup> = rf (2), then <sup>1</sup> = rf (2).\n\nData races and predictable\
    \ data races. A pair of events (, ′ ) in is said to be a conflicting pair, denoted\
    \ ⊲⊳ ′ , if both are access events to the same variable, and at least one of them\
    \ is a write event, i.e., (op(), op( ′ )) ∈ {(w(), w()), (w(), r()), (r(), w())}\
    \ for some ∈ Vars(). For a trace with Events() ⊆ Events(), we say that event is\
    \ -enabled in if ∉ Events() but all threadpredecessors of are in , i.e., { ′ ∈\
    \ Events() | ′ ≠ , ′ ≤ TO } ⊆ Events(). A conflicting pair (, ′ ) is said to be\
    \ a data race of if there is a prefix of such that both and ′ are -enabled in\
    \ . Finally, a conflicting pair (, ′ ) is a predictable data race of if there\
    \ is a correct reordering of such that both and ′ are -enabled in some prefix\
    \ of . In this case, we say that witnesses the data race (, ′ ).\n\n<span id=\"\
    page-2-1\"></span>Example 1. Consider trace <sup>2</sup> in Figure [2a](#page-2-0)\
    \ containing 6 events performed by two threads <sup>1</sup> and 2. As before,\
    \ we use to denote the th event of 2. The two events <sup>1</sup> <sup>=</sup>\
    \ ⟨1, w()⟩ and <sup>5</sup> <sup>=</sup> ⟨2, w()⟩ are conflicting (i.e., <sup>1</sup>\
    \ ⊲⊳ 5). The pair (1, 5) is not a data race in 2 as no prefix of 2 has both these\
    \ events simultaneously enabled. Consider the trace <sup>2</sup> in Figure [2b;](#page-2-0)\
    \ it is a correct reordering of <sup>2</sup> because it preserves both the thread\
    \ order and reads-from relation of 2. For the same reason, ′ 2 is a correct reordering\
    \ of 2 (and also of 2). Now, observe that (1, 5) is a data race in <sup>2</sup>\
    \ (and also in ′ 2 ) because in the prefix = ⟨2, acq(ℓ)⟩, both <sup>1</sup> and\
    \ <sup>5</sup> are 2-enabled (resp. ′ 2 -enabled) and thus 2-enabled. Thus, while\
    \ (1, 5) is not a data race in 2, it is a predictable data race of 2.\n\nThe problem\
    \ of predicting data races — given an execution trace , determine if there is\
    \ a predictable data race of — has been studied before [\\[31,](#page-11-1) [34,](#page-11-2)\
    \ [54,](#page-12-5) [57,](#page-12-1) [60,](#page-12-2) [65\\]](#page-12-3) and\
    \ is known to be an intractable problem [\\[44\\]](#page-11-4). This means that\
    \ any sound and complete algorithm for predicting data races is unlikely to scale\
    \ to real world software applications whose execution traces can have billions\
    \ of events. To cater to this, practical data race predictors resort to incomplete\
    \ but\n\nsound algorithms that run in polynomial time. In the next section, we\
    \ discuss the recently proposed SyncP algorithm that employs the principle of\
    \ synchronization preservation for predicting data races whose theoretical complexity\
    \ is linear.\n\n#### 2.1 Sync-Preserving Data Races\n\nOur work is closer in spirit\
    \ to the work of [\\[45\\]](#page-11-3) which presents the SyncP algorithm that\
    \ works in linear time and is the current state-of-the-art race prediction algorithm.\
    \ The principle employed by SyncP is to focus on a special class of reorderings\
    \ and the data races witnessed by such reorderings; we discuss these next.\n\n\
    Sync-preserving reorderings and data races. A correct reordering of a trace is\
    \ said to be sync(hronization)-preserving if for any two critical sections of\
    \ (on the same lock) that are both present in , their relative order is the same,\
    \ That is, for every lock ℓ ∈ Locks() and for any two acquire events 1, <sup>2</sup>\
    \ ∈ Events() such that op(1) = op(2) = acq(ℓ), if 1, <sup>2</sup> ∈ Events(),\
    \ then we have: <sup>1</sup> ≤ tr <sup>2</sup> iff <sup>1</sup> <sup>≤</sup> tr\
    \ 2. A pair of conflicting events (, ′ ) in Events() is said to be a sync-preserving\
    \ data race of if there is a sync-preserving correct reordering of that witnesses\
    \ this race.\n\nExample 2. Consider again, the trace <sup>2</sup> and recall from\
    \ Example [1](#page-2-1) that the pair (1, 5) is not a data race of <sup>2</sup>\
    \ but a predictable race witnessed by the correct reordering 2. Observe however\
    \ that 2 is not a sync-preserving reordering of 2 because it flips the order of\
    \ the two critical sections on lock ℓ. Nevertheless, (1, 5) is a syncpreserving\
    \ race of 2. This is because the reordering ′ 2 is, in fact, a sync-preserving\
    \ reordering of <sup>2</sup> (even though it is a prefix of the non-sync-preserving\
    \ reordering 2); there is only one critical section in ′ 2 and thus vacuously,\
    \ the relative order on critical sections is the same as in 2.\n\nLimited predictive\
    \ power of SyncP. While the SyncP algorithm runs in overall linear time, it can\
    \ miss data races which are not synchronization-preserving. These are precisely\
    \ those conflicting pairs (, ′ ) such that any correct reordering that witnesses\
    \ a race on and ′ necessarily reverses the relative order of two critical sections\
    \ on a common lock. We illustrate this next, and remark that, in general, reasoning\
    \ about even a single reversal is intractable [\\[45\\]](#page-11-3).\n\n<span\
    \ id=\"page-2-2\"></span>Example 3. Let us again consider the trace in Figure\
    \ [1a](#page-0-0) (Section [1\\)](#page-0-1). The two conflicting events <sup>1</sup>\
    \ = ⟨1, w()⟩ and <sup>12</sup> = ⟨4, w()⟩, are a predictable data race of <sup>1</sup>\
    \ as witnessed by the correct reordering 1 in Figure [1b,](#page-0-0) which is\
    \ not a sync-preserving correct reordering of 1. In fact, consider any correct\
    \ reordering of <sup>1</sup> that witnesses the race (1, 12). Then must include\
    \ the events 10 and 11, and thus the corresponding write events 4 and 8, together\
    \ with the thread predecessors <sup>3</sup> = ⟨2, acq(ℓ)⟩ and <sup>7</sup> = ⟨3,\
    \ acq(ℓ)⟩. Next, for well-formedness, at least one of the matching releases <sup>6</sup>\
    \ = ⟨2,rel(ℓ)⟩ as well as <sup>9</sup> = ⟨3,rel(ℓ)⟩ must also be present in .\
    \ However, including <sup>6</sup> in would enforce that <sup>5</sup> = ⟨2,r()⟩,\
    \ and its write event <sup>2</sup> = ⟨1, w()⟩ are present in , and then, the event\
    \ <sup>1</sup> must also be present in the reordering making it no longer enabled\
    \ in . This, therefore, means that <sup>6</sup> ∉ Events(), and thus, the only\
    \ other available release event <sup>9</sup> must be present in (for well-formedness).\
    \ Further, to ensure well-formedness, 3 must appear after 9 in . Thus, any reordering\n\
    \n witnessing the race between 1and 12 must reverse the order of the critical\
    \ sections.\n\n#### <span id=\"page-3-0\"></span>3 OPTIMISTIC REASONING FOR REVERSALS\n\
    \nGiven that reasoning about synchronization reversals is computationally hard,\
    \ how do we identify such races efficiently? At a high level, the intractability\
    \ in data race prediction arises because a search for a correct reordering entails\
    \ (1) a search for an appropriate set of events (amongst exponentially many sets)\
    \ and further, (2) given an appropriate set of events, a search for a linear order\
    \ (amongst exponentially many linear orders) on this set which is well-formed,\
    \ is a correct reordering and witnesses the race. We propose (1) a new notion\
    \ of data races called optimistic sync(hronization) reversal races which can be\
    \ predicted by opting for an optimistic approach to resolve both these steps,\
    \ and (2) an algorithm OSR to detect all such data races in e(N<sup>2</sup> )\
    \ time. In this section, we discuss this notion of data races and discuss our\
    \ algorithm in Section [4.](#page-4-0)\n\n#### 3.1 Optimistic Sync-Reversal Races\n\
    \nA crucial aspect of choosing the correct set of events is to ensure that multiple\
    \ acquire events on the same lock do not stay unmatched; otherwise, the set cannot\
    \ be linearized to a well-formed trace. In general, adding a matching release\
    \ event may lead to recursive addition of further events. Some choices may (recursively)\
    \ at times lead to the addition of one of the two focal events , ′ (candidate\
    \ data race), leading to them being no longer enabled. We define a simple and\
    \ tractable notion of optimistic lock-closure, which, instead of considering all\
    \ choices, simply includes all matching release events as long as the two focal\
    \ events are not included. In the following, we fix a trace .\n\nOptimistic lock-closure.\
    \ Let 1, <sup>2</sup> ∈ Events(). We say that a set ⊆ Events() is optimistically\
    \ lock-closed with respect to (1, 2) if (a) 1, <sup>2</sup> ∉ and prev (1), prev\
    \ (2) ∈ , (b) is (≤ TO,rf )-closed, and (c) for every acquire event <sup>∈</sup>\
    \ , if 1, <sup>2</sup> <sup>∉</sup> TRClosure(match (a)), then match () ∈ . We\
    \ denote the smallest set that contains and is optimistically lock-closed set,\
    \ as OLClosure(S, e1, e2)\n\n<span id=\"page-3-2\"></span>Example 4. Let us recall\
    \ trace <sup>1</sup> from Figure [1](#page-0-0) and consider the set <sup>1</sup>\
    \ = {3, 4, 7, 8, 9, 10, 11}. Observe that <sup>1</sup> is optimistically lock-closed\
    \ with respect to (1, 12), because (1) <sup>1</sup> doesn't include either of\
    \ 1, 12, (2) <sup>1</sup> is (≤ TO,rf )-closed, and finally, (3) 1, <sup>12</sup>\
    \ ∉ TRClosure(e9). Note that <sup>1</sup> ∈ TRClosure(e6) but <sup>6</sup> ∉ 1.\n\
    \nEven though the notion of optimistically lock-closed set is simple, in general,\
    \ checking if such a set can be linearized into a correct reordering that witnesses\
    \ a data race, is an intractable problem, as we show next (Theorem [3.1\\)](#page-3-1).\n\
    \n<span id=\"page-3-1\"></span>Theorem 3.1. Let be a trace, let 1, <sup>2</sup>\
    \ be conflicting events and let ⊆ Events() be an optimistically lock-closed set\
    \ with respect to (1, 2). The problem of determining whether there is a correct\
    \ reordering such that Events() = is NP-hard.\n\nThe proof of Theorem [3.1](#page-3-1)\
    \ is presented in appendix [A.1.](#page-13-0) Given the above result, we also\
    \ define the following more tractable notion of optimistic reordering that ensures\
    \ that there are no memory reversals, and moreover, critical sections are reversed\
    \ only when absolutely required, i.e., that unmatched critical sections appear\
    \ later than matched ones.\n\nOptimistic correct reordering. A trace is said to\
    \ be an optimistic correct reordering of if (a) is a correct reordering of , (b)\
    \ for all pairs of conflicting memory access events <sup>1</sup> ⊲⊳ <sup>2</sup>\
    \ in Events(), 1 ≤ tr <sup>2</sup> iff <sup>1</sup> <sup>≤</sup> tr 2, and (c)\
    \ for any lock <sup>ℓ</sup> and for any two acquire events <sup>1</sup> ≠ <sup>2</sup>\
    \ (with op(1) = op(2) = acq(ℓ)), if <sup>1</sup> and <sup>2</sup> are both matched\
    \ in (i.e., match () ∈ Events() for both ∈ {1, 2}), then we must have <sup>1</sup>\
    \ ≤ tr <sup>2</sup> iff <sup>1</sup> <sup>≤</sup> tr 2.\n\nWe now formalize optimistic\
    \ sync-reversal data races.\n\n<span id=\"page-3-4\"></span>Definition 1 (Optimistic\
    \ Sync-Reversal Race). Let be a trace and let (1, 2) be a pair of conflicting\
    \ events in . We say that (1, 2) is an optimistic sync-reversal data race if there\
    \ is an optimistic correct reordering of such that Events() is optimistically\
    \ lock-closed with respect to (1, 2) and both <sup>1</sup> and <sup>2</sup> are\
    \ -enabled in .\n\n<span id=\"page-3-3\"></span>Example 5. In Figure [1,](#page-0-0)\
    \ the pair (1, 12) is an optimistic syncreversal race, because the prefix ′ <sup>1</sup>\
    \ with first 7 events of <sup>1</sup> is an optimistic reordering of the optimistically\
    \ lock closed set 1, outlined in Example [4,](#page-3-2) (in which 1 and 12 are\
    \ 1-enabled). This is because, all conflicting accesses of ′ 1 have the same relative\
    \ order as in 1, and further, the unmatched acquire event is positioned after\
    \ all closed critical sections. Similarly, for the trace 2 of Figure [2,](#page-2-0)\
    \ the linearization ′ 2 = ⟨2, acq(ℓ)⟩ of the set <sup>2</sup> (outlined in Example\
    \ [4\\)](#page-3-2) is trivially an optimistic correct reordering.\n\n#### 3.2\
    \ Comparison with other techniques\n\nHere, we qualitatively compare our proposed\
    \ class of races with those reported by other sound predictive race detection\
    \ techniques proposed in the literature, namely SyncP [\\[45\\]](#page-11-3) and\
    \ M2 [\\[54\\]](#page-12-5) and illustrate how the set of races reported by OSR\
    \ is neither a strict subset, nor a strict super set of those detected by each.\n\
    \nExample 6. Recall again the execution trace <sup>1</sup> in Figure [1.](#page-0-0)\
    \ In Example [5](#page-3-3) we established that the pair (1, 12) is an optimistic\
    \ sync-reversal race, while in Example [3,](#page-2-2) we showed that it is not\
    \ a sync-preserving data race. When determining if (1, 12) can be declared a predictive\
    \ data race, the M2 algorithm computes the set = {1, 2, 3, 4, 5, 6, 7, 8, 10,\
    \ 11} to be the candidate set that witnesses the race. Observe however, this set\
    \ contains the event 1 and thus cannot witness the race (1, 12) since one of these\
    \ events is not enabled in . Thus, some optimistic sync-reversal races are neither\
    \ sync-preserving races, nor can be detected by M2.\n\nExample 7. Consider the\
    \ trace in Figure [3a.](#page-4-1) The pair (1, 21) is a sync-preserving data\
    \ race as witnessed by the correct reordering shown in Figure [3b.](#page-4-1)\
    \ This pair, however is not an optimistic syncreversal data race since the smallest\
    \ optimistically lock-closed set capable of witnessing the race is the set OSR\
    \ = {[3,9] , [12,14] , 17,20}, where , is shorthand for , +1, . . . , −1, . Observe\
    \ that OSRcontains two unmatched acquire events of lock ℓ2, and adding either\
    \ matching release will bring 1 in the set. Likewise, M2 computes the set containing\
    \ all events but 21, and thus contains 1. Thus, there are sync-preserving races\
    \ which are neither optimistic sync-reversal races, nor can be detected by M2.\n\
    \nExample 8. Finally, consider the trace in Figure [3c,](#page-4-1) derived from\
    \ [\\[54\\]](#page-12-5). Here, the pair (10, 19) is a data race that M2 can predict\n\
    \n<span id=\"page-4-1\"></span>![](_page_4_Figure_2.jpeg)\n\nFigure 3: Two traces\
    \ containing two predictable races. One of them (a) can be detected by SyncP,\
    \ but not M2 nor OSR. (b) is the witness of race in (a). The other one (c) can\
    \ be detected by M2, but not SyncP nor OSR. Trace in (c) is directly cited from\
    \ M2 paper [\\[54\\]](#page-12-5) without modification. (d) is the witness of\
    \ race in (c).\n\n(also see Figure [3d](#page-4-1) for the witnessing execution).\
    \ We remark that any correct reordering witnessing this race must reverse the\
    \ order of the two acquire events <sup>8</sup> and 13, as well as the order of\
    \ conflicting memory access events 9 and 14. Consequently, this is an example\
    \ of a race reported by M2 that is neither a sync-preserving race, nor an optimistic\
    \ sync-reversal race.\n\n### <span id=\"page-4-0\"></span>4 THE OSR ALGORITHM\n\
    \nWe now describe our algorithm OSR that detects optimistic syncreversal data\
    \ races. For ease of presentation, we will first discuss how to check if a given\
    \ pair (1, 2) of conflicting events is an optimistic sync-reversal data race (Section\
    \ [4.1\\)](#page-4-2), ine(N ) time, where N is the number of events in the given\
    \ trace. Naively, it can be used to report all optimistic sync-reversal data races\
    \ in e(N<sup>3</sup> ) time, by enumerating all (N<sup>2</sup> ) pairs of conflicting\
    \ events and checking each of them in e(N ) time. Instead, OSR runs in overall\
    \ e(N<sup>2</sup> ) time and is based on interesting insights that enable it to\
    \ perform incremental computation over the entire trace (Section [4.2\\)](#page-5-0).\
    \ We present our overall algorithm and its optimality in Section [4.3.](#page-7-1)\n\
    \n#### <span id=\"page-4-2\"></span>4.1 Checking Race On A Given Pair Of Events\n\
    \nBased on Definition [1,](#page-3-4) the task of checking if a given pair (1,\
    \ 2) of conflicting events is an optimistic sync-reversal data race entails examining\
    \ all optimistic lock-closed sets and checking if any of these can be linearized.\n\
    \nConstructing optimistically lock-closed set. Our algorithm, however, exploits\
    \ the following observation (Lemma [4.1\\)](#page-4-3), and focuses on only a\
    \ single set, namely the smallest such set. In the following, we will abuse the\
    \ notation and use OLClosure(e1, e2) to denote the set OLClosure(Se1,e<sup>2</sup>\
    \ , e1, e2), where 1,<sup>2</sup> = {prev (1)} ∪ {prev (2)}. Here, prev () is\
    \ the last event such that ≤ TO ; if no such event exists, we say prev () = ⊥,\
    \ in which case {prev ()} = ∅.\n\n<span id=\"page-4-3\"></span>Lemma 4.1. Let\
    \ 1, <sup>2</sup> be conflicting events in trace . If (1, 2) is an optimistic\
    \ sync-reversal race, then it can be witnessed in an optimistic correct reordering\
    \ such that Events() = OLClosure(e1, e2).\n\n<span id=\"page-4-4\"></span>\n\n\
    | Algorithm 1: Computing optimistic lock closure |                           \
    \                                  |  |  |  |  |\n|------------------------------------------------|-------------------------------------------------------------|--|--|--|--|\n\
    | 1 procedure ComputeOLClosure(\U0001D4460,<br>\U0001D4521,\U0001D4522)     |\
    \                                                             |  |  |  |  |\n\
    | 2                                              | \U0001D446<br>← \U0001D446\
    0∪TRClosure(prev\U0001D70E<br>(e1))∪TRClosure(prev\U0001D70E<br>(e2)) |  |  |\
    \  |  |\n| 3                                              | while \U0001D446<br>changes\
    \ do                                       |  |  |  |  |\n| 4                \
    \                              | ∃\U0001D44E<br>∈ Acqs(S),<br>(\U0001D44E)<br>∉\
    \ \U0001D446<br>if<br>∧<br>match\U0001D70E         |  |  |  |  |\n|          \
    \                                      | \U0001D4521, \U0001D4522<br>∉ TRClosure(match\U0001D70E\
    <br>(a))<br>then                |  |  |  |  |\n| 5                           \
    \                   | \U0001D446<br>← \U0001D446<br>∪ TRClosure(match\U0001D70E\
    <br>(a))                      |  |  |  |  |\n| 6                             \
    \                 | return \U0001D446                                        \
    \            |  |  |  |  |\n\nIn Algorithm [1,](#page-4-4) we outline our algorithm\
    \ to compute the smallest set that we identified in Lemma [4.1.](#page-4-3) It\
    \ takes 3 arguments — the two events 1, <sup>2</sup> and a set 0; for computing\
    \ OLClosure(e1, e2), we must set <sup>0</sup> = ∅; later in Section [4.2](#page-5-0)\
    \ this set will be used to enable incremental computation. This algorithm performs\
    \ a fixpoint computation starting from the set <sup>0</sup> ∪ TRClosure(prev (e1))\
    \ ∪ TRClosure(prev (e2)), and identifies an unmatched acquire event and checks\
    \ if its matching release can be added without adding <sup>1</sup> or 2; if so,\
    \ is added; Acqs(S) denotes the set of acquire events in the set . The algorithm\
    \ ensures that the set is (≤ TO,rf )-closed at each step, and runs in (T2N ) <sup>=</sup>\
    \ e(N ) time.\n\nChecking optimistic reordering. First, we check if the set constructed\
    \ by Algorithm [1](#page-4-4) islock-feasible, i.e., the set of unmatched acquires\
    \ OAcqs(S, ℓ) = { ∈ Acqs(S) | match () ∉ } for each lock ℓ is either singleton\
    \ or empty:\n\nlockFeasible() ≡ ∀ℓ ∈ Locks(), |OAcqs(S, ℓ)| ≤ 1\n\n<span id=\"\
    page-5-1\"></span>![](_page_5_Figure_1.jpeg)\n\n(a) Opt. reord. graph of <sup>1</sup>\
    \ (b) Abst. opt. reord. graph of <sup>1</sup> Figure 4: Optimistic and abstract\
    \ optimistic reordering graphs of <sup>1</sup> from Figure [1a](#page-0-0) are\
    \ acyclic\n\nObserve that if lockFeasible() does not hold, then every linearization\
    \ of will have more than one critical sections (on some lock) that overlap, making\
    \ it a non-well-formed trace. Next, inspired from the notion of optimistic reordering,\
    \ we construct the optimisticreordering-graph Opt = ( Opt , Opt ), where Opt =\
    \ , and Opt = Opt ,≤ TO ∪ Opt ,⊲⊳ ∪ Opt ,match∪ Opt ,unmatch. Here, Opt ,≤ TO\
    \ is the set of edges (, ′ ), where = prev ( ′ ). The set Opt ,⊲⊳ consists of\
    \ all immediate conflict edges, i.e., all pairs (, ′ ) in such that ⊲⊳ ′ , ≤ tr\
    \ ′ and there is no intermediate event in that conflicts with both. The set Opt\
    \ ,match consists of all pairs (, ′ ) such that ≤ tr ′ and there is a common lock\
    \ ℓ for which op() = rel(ℓ), op( ′ ) = acq(ℓ), both and ′ are matched in , and\
    \ there is no intermediate critical section on ℓ. Finally, the remaining set of\
    \ edges order matched critical sections before unmatched ones, i.e., Opt ,unmatch\
    \ <sup>=</sup> {(, ′ ) | ∃ℓ, op() = rel(ℓ), op( ′ ) = acq(ℓ), match ( ′ ) ∉ }.\
    \ Since optimistic reorderings forbid reversal in the order of conflicting memory\
    \ accesses, as well as in the order of same-lock critical sections that are completely\
    \ matched, it suffices to check the acycliclity of Opt, so that the existence\
    \ of witness is guaranteed.\n\n<span id=\"page-5-2\"></span>Lemma 4.2. Let be\
    \ a trace and let ⊆ Events() such that is (≤ TO,rf )-closed and also lock-feasible.\
    \ Then, there is an optimistic reordering of on the set iff the graph Opt is acyclic.\n\
    \nExample 9. For trace <sup>1</sup> in Figure [1a,](#page-0-0) we have OLClosure(e1,\
    \ e12) = {3, 4, 7, 8, 9, 10, 11}. The optimisticreordering-graph over <sup>1</sup>\
    \ = OLClosure(e1, e12) is shown in Figure [4a;](#page-5-1) Observe that there\
    \ is no cycle. Indeed, as guaranteed by Lemma [4.2,](#page-5-2) there is an optimistic\
    \ reordering, namely the 7 length prefix of <sup>1</sup> from Figure [1b](#page-0-0)\
    \ that witnesses the race (1, 12). Let us now consider 3, Figure [5a.](#page-6-0)\
    \ The optimistic lock-closure with respect to (4, 9) is <sup>3</sup> = OLClosure(e4,\
    \ e9) = {1, 2, 3, 6, 7, 8}. The optimistic reordering graph over 3, shown in Figure\
    \ [5b,](#page-6-0) contains a cycle. Indeed, (4, 9) is not a predictable race.\n\
    \nWe remark that Opt can be constructed and checked for cycles in time (T N )\
    \ <sup>=</sup> e(N ). Thus the overall algorithm for checking if given (1, 2)\
    \ is an optimistic sync-reversal race is — first compute OLClosure(e1, <sup>e</sup>2)\
    \ in e(N ) time, check lock-feasibility in\n\n(LT ) <sup>=</sup> e(1) time and\
    \ perform graph construction and cycle detection in e(N ) time. We thus have the\
    \ following theorem.\n\n<span id=\"page-5-4\"></span>Theorem 4.1. Let be a trace\
    \ and let 1, <sup>2</sup> be conflicting events in . The problem of determining\
    \ if (1, 2) is an optimistic syncreversal race can be solved in time T (T N +\
    \ L) <sup>=</sup> e(N ) time.\n\n#### <span id=\"page-5-0\"></span>4.2 Incremental\
    \ Race Detection\n\nOverview. Recall that there are (N<sup>2</sup> ) pairs of\
    \ conflicting events, and instead of naively examining each of them, we develop\
    \ an incremental algorithm that determines the existence of an optimistic sync-reversal\
    \ race in totale(N<sup>2</sup> ) time. We achieve this by spending e(N ) time\
    \ per (read/write) event <sup>∈</sup> Events(), and determine in overall e(N )\
    \ time if there is some event ′ such that ( ′ , ) is a race, by scanning the trace\
    \ from earliest to latest events. To do so, our algorithm exploits several novel\
    \ insights. Let us fix one of the events . First, we show that the optimistic\
    \ lock closure can be computed incrementally from previously computed sets, instead\
    \ of computing it from scratch for each ′ . Even though the closure sets can be\
    \ computed incrementally, the optimistic-reordering-graph Opt (Section [4.1\\\
    )](#page-4-2) cannot be computed in an incremental fashion, because the edges\
    \ in this graph depend upon precisely which events are present in the set. In\
    \ particular, a previously unmatched acquire event may become matched in a larger\
    \ set, and thus, we may have fewer edges in the larger graph. Our second insight\
    \ caters to this — we represent the graph succinctly as an abstract optimisticreordering-graph\
    \ which has e(1) (instead of e(N )) nodes, and moreover, can be computed by pre-populating\
    \ an appropriate data structure and performing range minima queries over it, to\
    \ determine reachability information in the abstract graph in e(1) time.\n\nIncrementally\
    \ constructing optimistic lock closure. The incremental closure computation relies\
    \ on the observation that the closure is monotonic with respect to thread-order\
    \ (Lemma [4.3\\)](#page-5-3). Thus, if we fix a thread , and scan the events of\
    \ from earliest to latest events, then we can reuse prior computations. In fact,\
    \ Algorithm [1](#page-4-4) already works in this fashion — it builds on top of\
    \ the given input set . Lemma [4.3](#page-5-3) establishes the correctness and\
    \ time complexity of closure computation.\n\n<span id=\"page-5-3\"></span>Lemma\
    \ 4.3. Let 1, 2, ′ 2 ∈ Events() be events in trace with <sup>2</sup> ≤ TO ′ 2\
    \ . Let = OLClosure(e1, e2) and let ′ = OLClosure(e1, e ′ 2 ). We have the following:\
    \ (1) ⊆ ′ . (2) = ComputeOLClosure(1, 2, ∅), and further this call (in Algorithm\
    \ [1\\)](#page-4-4) takes e(| |) time. (3) ′ = ComputeOLClosure(1, ′ 2 , ), and\
    \ further this call (in Algorithm [1\\)](#page-4-4) takes e(| ′ | − | |) time.\n\
    \nAbstract optimistic-reordering-graph. For a set ⊆ Events(), the abstract optimistic-reordering-graph\
    \ is a tuple Abs = ( Abs , Abs ), where the vertices and edges are defined as\
    \ follows. (1) Abs = Ð ℓ ∈Locks() {lastRel(S, ℓ)} ∪ OAcqs(S, ℓ), where lastRel(S,\
    \ ℓ) is the last release event on lock ℓ (according to ≤ tr) which is present\
    \ in . (2) (, ′ ) ∈ Abs if there is a path from to ′ in the graph Opt . In other\
    \ words, Abs only contains (L) vertices, corresponding to the last release events,\
    \ and acquire events that are unmatched in , and preserves the reachability information\n\
    \n<span id=\"page-6-4\"></span><span id=\"page-6-0\"></span>![](_page_6_Figure_1.jpeg)\n\
    \n(a) Trace <sup>3</sup> (b) Reordering graph (c) Abstract graph Figure 5: In\
    \ 3, (4, 9) is not a predictable race. The optimistic reordering graph and the\
    \ abstract optimistic reordering graph are cyclic.\n\nbetween these events. Lemma\
    \ [4.4](#page-6-1) formalizes the intuition behind this graph — it preserves the\
    \ cyclicity information of the larger graph Opt , because any cycle in Opt must\
    \ involve a 'backward' edge from a matched release and an unmatched acquire event.\
    \ Abs can thus be used to check for the existence of an optimistic reordering\
    \ using an e(1) check instead of an e(N ) check based on Lemma [4.2.](#page-5-2)\n\
    \n<span id=\"page-6-1\"></span>Lemma 4.4. Let be a trace and let ⊆ Events() be\
    \ a (≤ TO,rf ) closed set. Opt has a cycle iff Abs has a cycle.\n\nExample 10.\
    \ Figure [4b](#page-5-1) shows the abstract optimistic reordering graph for trace\
    \ <sup>1</sup> in Figure [1a,](#page-0-0) corresponding to the set <sup>1</sup>\
    \ = OLClosure(e1, e12), and contains the last release of lock ℓ in <sup>1</sup>\
    \ as well as the only open acquire in 1. This graph, like the graph in Figure\
    \ [4a](#page-5-1) is acyclic. In Figure [5,](#page-6-0) the abstract graph (Figure\
    \ [5c\\)](#page-6-0) captures the path <sup>2</sup> → <sup>3</sup> → <sup>7</sup>\
    \ → <sup>8</sup> of Figure [5b](#page-6-0) with a direct edge <sup>2</sup> → 8,\
    \ thereby preserving the cycle.\n\nConstructing vertices and backward edges of\
    \ Abs . Recall that is a (≤ TO,rf )-closed subset of Events(). The set of vertices\
    \ of this graph can be determined in (L) time by maintaining the last event of\
    \ every thread present in . This information can be inductively maintained as\
    \ is being computed incrementally. The 'backward' edges — namely those pairs (,\
    \ ) where ∈ is an unmatched acquire on some lock ℓ, and = lastRel(S, ℓ) but ≤\
    \ tr — can be computed in (L) time.\n\nPre-computing earliest immediate successor.\
    \ For constructing forward edges, we first pre-compute a map (for each pair of\
    \ threads 1, 2), EIS1,<sup>2</sup> such that, for every <sup>1</sup> ∈ Events()|<sup>1</sup>\
    \ = { ∈ Events() | th() = }, the event EIS1,<sup>2</sup> (1) is the earliest immediate\
    \ successor of 1 in thread 2, in the full graph Opt Events() ; observe the subscript\
    \ Events() instead of an arbitrary set . EIS1,<sup>2</sup> can be computed as\
    \ a pre-processing step in (T N ) <sup>=</sup> e(N ) time and stored as an array,\
    \ indexed by the events of thread 1.\n\nDetermining forward edges of Abs . The\
    \ forward edges of Abs summarize paths in Opt and are computed as follows. Recall\
    \ that we are given a (≤ TO,rf )-closed subset of Events(), and the path between\
    \ two events must only be contained with the events of , thus the arrays {EIS1,<sup>2</sup>\
    \ }1,2∈Threads() cannot be used as is\n\n<span id=\"page-6-2\"></span>\n\n| Algorithm\
    \ 2: Earliest successors of event<br>\U0001D452<br>within set<br>\U0001D446  \
    \              |  |  |  |  |  |  |\n|----------------------------------------------------------------------------------|--|--|--|--|--|--|\n\
    | 1 procedure getSuccessors(\U0001D452,<br>\U0001D446)                       \
    \                        |  |  |  |  |  |  |\n| = th(\U0001D452),<br>let \U0001D461\
    \U0001D452<br>visitedThr ← ∅                                             |  |\
    \  |  |  |  |  |\n| let last\U0001D446<br>be the last event by<br>\U0001D461<br>in\
    \ S, for<br>\U0001D461<br>∈ Threads(\U0001D70E)<br>\U0001D461    |  |  |  |  |\
    \  |  |\n| for \U0001D461<br>∈ Threads(\U0001D70E)<br>do                     \
    \                                 |  |  |  |  |  |  |\n| succ\U0001D446<br>last\U0001D446\
    <br>,\U0001D461) [\U0001D452,<br>\U0001D452,\U0001D461 ← rangeMin(EIS\U0001D461\
    \U0001D452<br>]<br>5<br>\U0001D461\U0001D452                |  |  |  |  |  | \
    \ |\n| while visitedThr ≠ Threads(\U0001D70E)<br>do<br>6                     \
    \                    |  |  |  |  |  |  |\n| succ\U0001D446<br>let \U0001D4611<br>be\
    \ s.t.<br>\U0001D4611<br>∉ visitedThr and<br>is the<br>7<br>\U0001D452,\U0001D461\
    1      |  |  |  |  |  |  |\n| \U0001D70E<br>{succ\U0001D446<br>earliest in<br>tr\
    \ from<br>≤<br>\U0001D452,\U0001D461 }\U0001D461∉visitedThr                  |\
    \  |  |  |  |  |  |\n| for \U0001D4612<br>∈ Threads(\U0001D70E)<br>do<br>8   \
    \                                             |  |  |  |  |  |  |\n| ) [succ\U0001D446\
    <br>last\U0001D446<br>\U0001D45B\U0001D452\U0001D464\U0001D446\U0001D462\U0001D450\
    \U0001D450<br>,<br>← rangeMin(EIS\U0001D4611,\U0001D4612<br>]<br>9<br>\U0001D452\
    ,\U0001D4611<br>\U0001D4611 |  |  |  |  |  |  |\n| TO succ\U0001D446<br>\U0001D70E\
    <br>if \U0001D45B\U0001D452\U0001D464\U0001D446\U0001D462\U0001D450\U0001D450\
    <br>≤<br>then<br>10<br>\U0001D452,\U0001D4612                           |  | \
    \ |  |  |  |  |\n| succ\U0001D446<br>← \U0001D45B\U0001D452\U0001D464\U0001D446\
    \U0001D462\U0001D450\U0001D450<br>11<br>\U0001D452,\U0001D4612               \
    \                                  |  |  |  |  |  |  |\n| visitedThr ← visitedThr\
    \ ∪ {\U0001D4611}<br>12                                             |  |  |  |\
    \  |  |  |\n| return {succ\U0001D446<br>\U0001D452,\U0001D461 }\U0001D461<br>∈Threads(\U0001D70E\
    )<br>13                                     |  |  |  |  |  |  |\n\n| Algorithm\
    \ 3: Detecting races between<br>\U0001D452<br>and thread |\n|---------------------------------------------------------|\n\
    |---------------------------------------------------------|\n\n<span id=\"page-6-3\"\
    ></span>\n\n| 1 procedure incrementalRaceDetection(\U0001D452,<br>\U0001D461)\
    \ |                                                                          \
    \                |  |  |  |  |  |  |  |\n|-----------------------------------------------|------------------------------------------------------------------------------------------|--|--|--|--|--|--|--|\n\
    | 2                                             | \U0001D446<br>← ∅          \
    \                                                                       |  | \
    \ |  |  |  |  |  |\n| 3                                             | \U0001D70E\
    <br>′ ∈<br>′ and<br>′ ≤<br>for \U0001D452<br>Events(\U0001D70E) \U0001D461<br>s.t.\
    \ \U0001D452<br>⊲⊳<br>\U0001D452<br>\U0001D452<br>tr \U0001D452<br>do |  |  |\
    \  |  |  |  |  |\n| 4                                             | \U0001D446\
    <br>← ComputeOLClosure(e, e', S)                                             \
    \           |  |  |  |  |  |  |  |\n| 5                                      \
    \       | if lockFeasible(\U0001D446)<br>and \U0001D43A<br>Abs<br>is acyclic then<br>\U0001D446\
    \                               |  |  |  |  |  |  |  |\n| 6                  \
    \                           | ′<br>declare (\U0001D452<br>,<br>\U0001D452) as\
    \ race.                                                      |  |  |  |  |  |\
    \  |  |\n\nto efficiently determine paths. However, a combination of range minima\
    \ queries [\\[7\\]](#page-11-7) and shortest path computation can nevertheless\
    \ still be used to determine path information efficiently. Let us use succ , to\
    \ denote the earliest event in thread that has a path from event , using only\
    \ forward edges of Opt . The event succ , can be computed using a Bellman-Ford-Moore\
    \ [\\[12,](#page-11-8) [27,](#page-11-9) [48\\]](#page-11-10) style shortest path\
    \ computation, as shown in Algorithm [2.](#page-6-2) This algorithm performsrangeMin()\
    \ [, ] queries which return the earliest event (according to ≤ TO) in the segment\
    \ of the array starting at index and ending at index . With e(N ) time and space\
    \ pre-processing, each range minimum query takes (1) time [\\[7,](#page-11-7)\
    \ [28\\]](#page-11-11), Thus, the task of determining {succ , } <sup>∈</sup>Threads()\
    \ takes (T<sup>2</sup> ) time. Now, in the graph Abs , we add an edge from to\
    \ ′ if succ ,th( ′ ) ≤ TO ′ . Thus, we add all forward edges of the graph in overall\
    \ (T2L) time.\n\nChecking if a given event is in race with some event. We now\
    \ have all the ingredients to describe our overall incremental algorithm to check\
    \ if event is in optimistic-sync-reversal race with some event of a given thread\
    \ (Algorithm [3\\)](#page-6-3). For this, we first initialize all the arrays {EIS1,<sup>2</sup>\
    \ }1,2∈Threads() using a linear scan of the trace , and also do pre-processing\
    \ for fast performing range minima queries, spending overall time (T N ). Then,\
    \ we iterate over each event ′ of thread that conflict with , starting from the\
    \ earliest to the latest. For each event, we incrementally update the optimistic\
    \ lock-closure set and check if it is lock-feasible. If so, we construct the abstract\
    \ optimistic-reordering-graph Abs and check if it is acyclic, and report a race\
    \ if so.\n\n<span id=\"page-7-2\"></span>\n\n| Algorithm 4: Detecting optimistic\
    \ sync-reversal races in<br>\U0001D70E |                                     \
    \                          |  |                                 |  |  |  |  |\n\
    |---------------------------------------------------------------|---------------------------------------------------------------|--|---------------------------------|--|--|--|--|\n\
    | 1 procedure OSR(\U0001D70E)                                            |   \
    \                                                            |  |            \
    \                     |  |  |  |  |\n| 2                                     \
    \                        | for \U0001D452<br>∈ Events(\U0001D70E)<br>s.t. \U0001D452\
    <br>is a memory access event do |  |                                 |  |  | \
    \ |  |\n| 3                                                             | ′ ∈<br>for\
    \ \U0001D461<br>Threads(\U0001D70E)<br>do                              |  |  \
    \                               |  |  |  |  |\n| 4                           \
    \                                  |                                         \
    \                      |  | incrementalRaceDetection(e, t') |  |  |  |  |\n\n\
    Theorem 4.2. Let be an execution, ∈ Events() be a read or write event and let\
    \ ∈ Threads(). The problem of checking if there is an event ′ with th( ′ ) = such\
    \ that (, ′ ) is an optimisticsync-reversal race, can be solved in time (T<sup>2</sup>\
    \ + L)LN .\n\n# <span id=\"page-7-1\"></span>4.3 Detecting All Optimistic Sync-Reversal\
    \ Races\n\nGiven a trace , all the optimistic sync-reversal races in can now be\
    \ detected by enumerating all events and threads and checking if incrementalRaceDetection(,\
    \ ) reports a race. Our resulting algorithm OSR (Algorithm [4\\)](#page-7-2) runs\
    \ in time T L (T<sup>2</sup> + L)N<sup>2</sup> .\n\n<span id=\"page-7-4\"></span>Theorem\
    \ 4.3. Given a trace , the problem of checking if has an optimistic sync-reversal\
    \ data race, can be solved in time T L (T<sup>2</sup> + L)N<sup>2</sup> <sup>=</sup>\
    \ e(N<sup>2</sup> ) time.\n\nHardness of detecting optimistic sync-reversal races.\
    \ We have, thus far, established that the problem of checking the existence of\
    \ optimistic sync-reversal data races can be solved in quadratic time. In the\
    \ following, we also show a matching quadratic time lower bound, thus establishing\
    \ that our algorithm OSR is indeed optimal. The lower bound is conditioned on\
    \ the Strong Exponential Time Hypothesis (SETH), which is a widely believed conjecture.\
    \ We use fine-grained reductions to establish a reduction from the orthogonal\
    \ vectors problem which holds true under SETH [\\[70\\]](#page-12-6). The full\
    \ proof of the following result is presented in Appendix [B.8.](#page-16-0)\n\n\
    <span id=\"page-7-5\"></span>Theorem 4.4. Assume SETH holds. Given an arbitrary\
    \ trace , the problem of determining if has an OSR race cannot be solved in time\
    \ (N2− ) (where N = |Events()|) for every > 0.\n\n#### <span id=\"page-7-0\"></span>5\
    \ EVALUATION\n\nWe implemented our algorithm OSR in Java, using the Rapid dynamic\
    \ analysis framework [\\[4\\]](#page-11-12). We evaluate the performance and precision\
    \ of OSR, on 153 benchmarks and compare it with prior state-of-the-art sound predictive\
    \ race detection algorithms. We discuss our experimental set up in Section [5.1](#page-7-3)\
    \ and our evaluation results in Section [5.2,](#page-8-1) Section [5.3](#page-8-2)\
    \ and Section [5.4.](#page-8-3)\n\n#### <span id=\"page-7-3\"></span>5.1 Experimental\
    \ Setup\n\nBenchmarks. Our evaluation subjects are both Java (Category-1) as well\
    \ as C/C++/OpenMP (Category-2) benchmarks. Category-1, derived from [\\[45\\]](#page-11-3),\
    \ contains 30 Java programs from the IBM Contest benchmark suite [\\[24\\]](#page-11-13),\
    \ the Java Grande forum benchmark suite [\\[63\\]](#page-12-7), DaCapo [\\[13\\\
    ]](#page-11-14), SIR [\\[22\\]](#page-11-15) and other standalone benchmarks.\
    \ Category-2 contains 123 benchmarks from OmpSCR [\\[23\\]](#page-11-16), DataRaceBench\
    \ [\\[39\\]](#page-11-17) DataRaceOnAccelerator [\\[62\\]](#page-12-8), NAS parallel\
    \ benchmarks [\\[11\\]](#page-11-18), CORAL [\\[5,](#page-11-19) [6\\]](#page-11-20),\
    \ ECP proxy applications [\\[1\\]](#page-11-21) and the Mantevo project [\\[2\\\
    ]](#page-11-22). For an apples-to-apples comparison, we\n\nevaluate all compared\
    \ techniques on the same execution trace to remove bias due to thread-scheduler.\
    \ For this, we generate traces out of these programs using ThreadSanitizer [\\\
    [64\\]](#page-12-9) (for Category-2) and using RVPredict [\\[47\\]](#page-11-23)\
    \ (for Category-1). For Java programs, we generate one trace per program and for\
    \ C/C++ programs, we generate multiple traces of the same program with different\
    \ thread number and input parameters. All compared methods then evaluate each\
    \ generated trace 3 times. We did not exclude any traces from the benchmarks,\
    \ except one corrupted trace.\n\nAs part of our evaluation, we also explored synthetically\
    \ created benchmark traces from RaceInjector [\\[3,](#page-11-24) [69\\]](#page-12-10),\
    \ that uses SMT solving to inject data races into existing traces. However, the\
    \ traces in [\\[3\\]](#page-11-24) are short, could not be used to distinguish\
    \ most compared methods and were not useful for a conclusive evaluation. Our evaluation\
    \ on these traces is deferred to Appendix [C](#page-17-0) (Table [4\\)](#page-19-0).\
    \ As observed in prior works [\\[18,](#page-11-6) [26,](#page-11-25) [45,](#page-11-3)\
    \ [54\\]](#page-12-5), a large fraction of events in traces are thread-local,\
    \ and do not affect the precision or soundness of race detection algorithms, but\
    \ can significantly slow down race detection. Therefore, we filter out these thread-local\
    \ events, as with prior work [\\[34,](#page-11-2) [45,](#page-11-3) [54\\]](#page-12-5).\n\
    \nCompared methods. We compare OSR with state-of-the-art sound predictive algorithms:\
    \ WCP [\\[34\\]](#page-11-2), SHB [\\[42\\]](#page-11-5), M2 [\\[54\\]](#page-12-5)\
    \ and SyncP [\\[45\\]](#page-11-3). Amongst these, SHB and WCP are partial order\
    \ based methods and run in linear time. M2 and SyncP are closer in spirit to ours\
    \ — they first identify a set of events and then a linearization of this set that\
    \ can witness a data race. SyncP works in linear time while M2 has higher polynomial\
    \ complexity of e(N<sup>4</sup> log(N )) [\\[54\\]](#page-12-5). For all these\
    \ algorithms, we use the publicly available source codes [\\[34,](#page-11-2)\
    \ [42,](#page-11-5) [45,](#page-11-3) [54\\]](#page-12-5). To achieve fair comparison,\
    \ we modify each of them, so that (1) each algorithm reports on the same criteria\
    \ (events v/s memory locations v/s program locations) (2) any redundant operations\
    \ not relevant to the reporting criteria are removed. A comparison with recent\
    \ work SeqC [\\[18\\]](#page-11-6) was not possible because the implementation\
    \ of SeqC is neither publicly available nor could be obtained even after contacting\
    \ the authors. Our evaluation didn't include comparison with solver-aided race\
    \ predictors, such as RVPredict [\\[31\\]](#page-11-1). Based on prior work [\\\
    [34\\]](#page-11-2), such predictors are known to not scale, have unpredictable\
    \ race reports and typically have lower predictive power than the simplest of\
    \ race prediction algorithms, thanks to the windowing strategy they implement.\n\
    \nMachine configuration and evaluation settings. The experiments are conducted\
    \ on a 2.0GHz 64-bit Linux machine. For Category-1 (Java) benchmarks, we set the\
    \ heap size of JVM to be 60GB and timeout to be 2 hours; this set up is similar\
    \ to previous works [\\[34,](#page-11-2) [45\\]](#page-11-3), except for the larger\
    \ heap space, mandated by the larger memory requirement of M2. For Category-2\
    \ (C/C++) benchmarks, we set the heap size to be 400GB and timeout to be 3 hours,\
    \ since these are much more challenging — the number of events, locks and variables\
    \ in these are typically 10 − 100× more than traces in Category-1. All experiments\
    \ are repeated 3 times and the times reported are averaged over these 3 runs.\n\
    \nReported metrics. Our evaluation aims to understand the prediction power (precision)\
    \ as well as the scalability of OSR and assess how it compares against existing\
    \ state-of-the-art race prediction techniques. For each execution trace , we report\
    \ key characteristics\n\n(number of events, threads, locks, read events, write\
    \ events, acquire events and release events) to estimate how challenging each\
    \ benchmark is. Next, we measure and report the following :\n\n- Running time.\
    \ For each algorithm, we report the average running time (over 3 trials) for processing\
    \ the entire execution. This is aimed to understand if the worst case quadratic\
    \ complexity of OSR affects its performance in practice, or it is on par with\
    \ other linear time methods such as WCP, SHB and SyncP.\n- Race reports in Category-1.\
    \ For benchmarks in Category-1, we report the number of racy events reported;\
    \ an event <sup>2</sup> is racy if there is a conflicting event 1 earlier in the\
    \ trace, such that (1, 2) is a race. We also report the number of distinct source\
    \ code lines for these racy events. We note here one racy source code line could\
    \ correspond to many racy events.\n- Race reports in Category-2. For benchmarks\
    \ in Category-2, we report the number of variables (memory locations) that are\
    \ racy. A variable is racy if there is a racy event that accesses . The number\
    \ of racy events in the C/C++ benchmarks is typically very large, and reporting\
    \ each racy event throttles nearly all algorithms. If a compared method times\
    \ out, we report the number of racy variables found before timing out. This enables\
    \ us to better evaluate their ability to find races in a more reasonable setting.\
    \ Besides, most algorithms report many races before they timeout.\n- Scaling behavior\
    \ of OSR. OSR runs in worst case quadratic time. We empirically evaluate how OSR\
    \ scales with trace length, for a small set of benchmarks to gauge its in-practice\
    \ behavior.\n\n#### <span id=\"page-8-1\"></span>5.2 Evaluation Results For Java\
    \ Benchmarks\n\nTable [1](#page-9-0) summarizes the results for Category-1.\n\n\
    Prediction power. OSR reports the largest number of races on each trace; it reports\
    \ about 200 more racy events and 3 extra racy locations over the second most predictive\
    \ method (SyncP); we remark that any extra data race can be an insidious bug [\\\
    [15\\]](#page-11-26) and deserves rigorous attention by developers. Although WCP\
    \ can detect syncreversal races in principle, and reports much fewer races than\
    \ OSR (and also misses races reported by SyncP). M2 takes much more memory and\
    \ time than OSR, and times out on two benchmarks (linkedlist and lufact), while\
    \ runs out of memory on the benchmark tsp. On other benchmarks, OSR demonstrates\
    \ the same prediction power as M2. Overall M2 detects 29.2k less races. In terms\
    \ of racy source code locations, OSR also reports 24, 47, 3, 13 more than SHB,\
    \ WCP, SyncP and M2, respectively. We remark that this class of benchmarks does\
    \ not bring out the full potential of OSR— even if OSR reports the highest number\
    \ of races individually for each benchmark, at least one other method also reports\
    \ this number of races. Category-2 though does better justice to OSR.\n\nRunning\
    \ time. SHB and WCP are lightweight partial order-based linear time algorithms\
    \ and finish fastest. On the other hand, M2 performs an expensive computation,\
    \ times out on some large traces and takes more than 6 hours to finish. SyncP\
    \ runs in linear time, but our algorithm OSR outperforms it by about 1.5×. We\
    \ note that the linkedlist benchmark is especially challenging, with large number\
    \ of variables, as a result of which SyncP allocates a large memory to account\
    \ for its heavy data structure usage.\n\nThus, for Category-1 benchmarks, OSR\
    \ demonstrates highest race coverage, and runs faster than the state-of-the-art\
    \ SyncP.\n\n### <span id=\"page-8-2\"></span>5.3 Evaluation Results For C/C++\
    \ Benchmarks\n\nTable [2](#page-10-1) summarizes our evaluation over Category-2\
    \ (C/C++) benchmarks. In Appendix [C,](#page-17-0) we present detailed statistics\
    \ of these benchmarks (see Table [6](#page-25-0) and Table [5\\)](#page-22-0).\n\
    \nPrediction power. OSR displays high race coverage on this set of traces. Overall,\
    \ OSR reports 2.5× more races than the second most predictive method (SHB). On\
    \ all, except 5, of the 118 benchmarks, OSR reports the highest number of racy\
    \ variables. Each of the remaining 5 benchmark traces have a large number of events,\
    \ and only the lightweight algorithms (SHB and WCP) finish within the 3 hour time\
    \ limit. In terms of total races found, OSR reports 2.5× and 2.7× more races than\
    \ SHB (2 nd highest) and WCP (3 rd highest). SyncP and M2 time out on most benchmarks.\
    \ We speculate that this is because both these methods have high memory requirement\
    \ and result in large time spent in garbage collection. OSR, therefore, has the\
    \ highest race coverage even for the C/C++ benchmarks.\n\nWe remark that the number\
    \ of racy variables in this class of benchmarks is very high. We speculate this\
    \ is because our instrumentation using ThreadSanitizer does not explicitly tag\
    \ atomic operations. Further many benchmarks perform matrix operations, giving\
    \ rise to many distinct memory locations. Nevertheless, we choose to report all\
    \ races because data races can render these programs potentially non-robust, and\
    \ under weak memory consistency, data races can lead to undefined semantics.\n\
    \nRunning time. Overall, SHB runs the fastest. SyncP and M2, on the other hand,\
    \ frequently time out. The difference in the performance between SyncP, M2 and\
    \ OSR gets exacerbated on the C/C++ benchmarks because these contain much larger\
    \ execution traces than Java benchmarks. The performance of OSR (total running\
    \ time of 42 hours) is close to WCP (30 hours). OSR, therefore, achieves an optimal\
    \ balance between predictive power and scalability — OSR has the highest predictive\
    \ power and outperforms SHB, WCP, SyncP, M2, and often runs faster than more exhaustive\
    \ techniques.\n\n#### <span id=\"page-8-3\"></span>5.4 Scalability\n\nIn this\
    \ section, we take a closer look at the run-time behavior of OSR to understand\
    \ its unexpected high scalability on some benchmarks. We select the most challenging\
    \ benchmarks from each of the following groups: HPCBench, CoMD, DataRaceBench,\
    \ OMPRacer in Category-2. For these benchmarks, we measure the time to process\
    \ every million events and report it in Figure [6.](#page-10-2) We observe that\
    \ on these four benchmarks, OSR scales linearly for a large prefix, while gradually\
    \ slows down on two of them. The nearlinear behavior of OSR is likely an artefact\
    \ of the fact that, many of these benchmarks traces have large number of data\
    \ races, thus the race check for a single event succeeds quickly instead of the\
    \ worst case linear time requirement. Therefore, instead of spending overall quadratic\
    \ time, OSR spends linear time on average.\n\n#### <span id=\"page-8-0\"></span>6\
    \ RELATED WORK\n\nDynamic predictive analysis. Happens-before (HB) [\\[37\\]](#page-11-27)\
    \ based race detection [\\[26,](#page-11-25) [55\\]](#page-12-4) has been adopted\
    \ by mature tools [\\[49,](#page-12-11) [64\\]](#page-12-9),\n\n<span id=\"page-9-0\"\
    ></span>Table 1: Evaluation on **Category-1** (Java benchmarks). Columns 1-3 denote\
    \ the name, number of events and number of threads for each benchmark. Columns\
    \ 4-13 are the number of racy events (and racy program locations) reported and\
    \ average running time of each algorithm.\n\n| 1            | 2      | 3 | 4 \
    \              | 5              | 6                | 7              | 8      \
    \          | 9              | 10             | 11             | 12           \
    \    | 13       |\n|--------------|--------|---|-----------------|----------------|------------------|----------------|------------------|----------------|----------------|----------------|------------------|----------|\n\
    | Benchmarks N |        | T | SHB             |                | WCP         \
    \     |                | SyncP            |                | M2             |\
    \                | OSR              |          |\n|              |        |  \
    \ | Races           | Time (s) Races |                  | Time (s) Races |   \
    \               | Time (s) Races |                | Time (s) Races |         \
    \         | Time (s) |\n| array        | 11     | 3 | 0(0)            | 0.05 \
    \          | 0(0)             | 0.08           | 0(0)             | 0.06     \
    \      | 0(0)           | 0.03           | 0(0)             | 0.09     |\n| critical\
    \     | 11     | 4 | 3(3)            | 0.04           | 1(1)             | 0.05\
    \           | 3(3)             | 0.07           | 3(3)           | 0.02      \
    \     | 3(3)             | 0.07     |\n| account      | 15     | 4 | 3(1)    \
    \        | 0.04           | 3(1)             | 0.06           | 3(1)         \
    \    | 0.06           | 3(1)           | 0.02           | 3(1)             | 0.08\
    \     |\n| airtickets   | 18     | 5 | 8(3)            | 0.05           | 5(2)\
    \             | 0.08           | 8(3)             | 0.06           | 8(3)    \
    \       | 0.03           | 8(3)             | 0.08     |\n| pingpong     | 24\
    \     | 7 | 8(3)            | 0.04           | 8(3)             | 0.07       \
    \    | 8(3)             | 0.06           | 8(3)           | 0.03           | 8(3)\
    \             | 0.08     |\n| twostage     | 83     |   | 12 4(1)         | 0.06\
    \           | 4(1)             | 0.10           | 4(1)             | 0.14    \
    \       | 8(2)           | 0.05           | 8(2)             | 0.10     |\n| wronglock\
    \    | 122    |   | 22 12(2)        | 0.07           | 3(2)             | 0.11\
    \           | 25(2)            | 0.22           | 25(2)          | 0.18      \
    \     | 25(2)            | 0.13     |\n| bbuffer      | 9      | 3 | 3(1)    \
    \        | 0.05           | 1(1)             | 0.06           | 3(1)         \
    \    | 0.05           | 3(1)           | 0.02           | 3(1)             | 0.10\
    \     |\n| prodcons     | 246    | 8 | 1(1)            | 0.07           | 1(1)\
    \             | 0.13           | 1(1)             | 0.16           | 1(1)    \
    \       | 0.06           | 1(1)             | 0.12     |\n| clean        | 867\
    \    | 8 | 59(4)           | 0.11           | 82(4)            | 0.23        \
    \   | 60(4)            | 0.26           | 110(4)         | 0.65           | 110(4)\
    \           | 0.20     |\n| mergesort    | 167    | 5 | 1(1)            | 0.89\
    \           | 1(1)             | 0.13           | 3(1)             | 0.10    \
    \       | 5(2)           | 0.04           | 5(2)             | 0.12     |\n| bubblesort\
    \   | 1.7K   |   | 13 269(5)       | 0.15           | 100(5)           | 0.30\
    \           | 269(5)           | 2.29           | 374(5)         | 8.40      \
    \     | 374(5)           | 0.28     |\n| lang         | 1.8K   | 7 | 400(1)  \
    \        | 0.17           | 400(1)           | 0.26           | 400(1)       \
    \    | 0.33           | 400(1)         | 0.54           | 400(1)           | 0.22\
    \     |\n| readwrite    | 9.8K   | 5 | 92(4)           | 0.27           | 92(4)\
    \            | 0.63           | 199(4)           | 0.81           | 228(4)   \
    \      | 9.00           | 228(4)           | 0.69     |\n| raytracer    | 526\
    \    | 3 | 8(4)            | 0.10           | 8(4)             | 0.17        \
    \   | 8(4)             | 0.15           | 8(4)           | 0.09           | 8(4)\
    \             | 0.15     |\n| bufwriter    | 10K    | 6 | 8(4)            | 0.29\
    \           | 8(4)             | 0.77           | 8(4)             | 0.75    \
    \       | 8(4)           | 0.52           | 8(4)             | 0.49     |\n| ftpserver\
    \    | 17K    |   | 11 69(21)       | 1.18           | 70(21)           | 0.99\
    \           | 85(21)           | 6.01           | 85(21)         | 2.43      \
    \     | 85(21)           | 0.79     |\n| moldyn       | 21K    | 3 | 103(3)  \
    \        | 1.03           | 103(3)           | 0.73           | 103(3)       \
    \    | 0.79           | 103(3)         | 31.43          | 103(3)           | 0.46\
    \     |\n| linkedlist   |        |   | 910K 12 6.0K(4) | 3.77           | 6.0K(3)\
    \          | 6.80           | 7.1K(4)          | 378.25         | 0(0)       \
    \    | 7200           | 7.1K(4)          | 6.56     |\n| derby        | 75K  \
    \  | 4 | 29(10)          | 0.94           | 28(10)           | 2.30          \
    \ | 29(10)           | 19.08          | 30(11)         | 5.66           | 30(11)\
    \           | 3.67     |\n| jigsaw       | 3.2K   | 8 | 4(4)            | 0.17\
    \           | 4(4)             | 0.39           | 6(6)             | 2.90    \
    \       | 6(6)           | 0.23           | 6(6)             | 0.35     |\n| sunflow\
    \      | 3.3K   |   | 17 84(6)        | 0.17           | 69(6)            | 0.39\
    \           | 119(7)           | 2.53           | 130(7)         | 1.10      \
    \     | 130(7)           | 0.35     |\n| cryptorsa    | 1.3M   | 7 | 11(5)   \
    \        | 5.95           | 11(5)            | 10.87          | 35(7)        \
    \    | 156.19         | 35(7)          | 20.39          | 35(7)            | 173.74\
    \   |\n| xalan        | 672K 7 |   | 31(10)          | 3.22           | 21(7)\
    \            | 12.07          | 37(12)           | 160.62         | 37(12)   \
    \      | 6.56           | 37(12)           | 230.03   |\n| lufact       | 892K\
    \ 5 |   | 22.0K(3)        | 3.39           | 22.0K(3)         | 7.16         \
    \  | 22.0K(3)         | 62.10          | 0(0)           | 7200           | 22.0K(3)\
    \         | 4.15     |\n| batik        | 131    | 7 | 10(2)           | 0.09 \
    \          | 10(2)            | 0.11           | 10(2)            | 0.12     \
    \      | 10(2)          | 0.04           | 10(2)            | 0.12     |\n| lusearch\
    \     | 751K 8 |   | 232(44)         | 2.86           | 119(27)          | 7.94\
    \           | 232(44)          | 9.26           | 232(44)        | 50.4      \
    \     | 232(44)          | 3.65     |\n| tsp          | 15M    |   | 10 143(6)\
    \       | 33.63          | 140(6)           | 66.07          | 143(6)        \
    \   | 146.24         | 0(0)           | 7200           | 143(6)           | 160.39\
    \   |\n| luindex      | 16K    | 3 | 1(1)            | 0.38           | 2(2) \
    \            | 0.68           | 15(15)           | 0.71           | 15(15)   \
    \      | 0.53           | 15(15)           | 0.49     |\n| sor          | 1.9M\
    \   | 5 | 0(0)            | 4.79           | 0(0)             | 9.92         \
    \  | 0(0)             | 13.16          | 0(0)           | 10.61          | 0(0)\
    \             | 38.0     |\n| Sum          |        |   | 29.5K(157) 64.0 |  \
    \              | 29.2K(134) 129.7 |                | 30.9K(178) 961.9 |      \
    \          | 1.9K(168) 6.0h |                | 31.1K(181) 625.7 |          |\n\
    \nand has subsequently been strengthened to SHB [\\[42\\]](#page-11-5) so that\
    \ all races reported are sound. Causal Precedence (CP) [\\[65\\]](#page-12-3)\
    \ and Weak Causal Precedence (WCP) [\\[34\\]](#page-11-2) weaken HB in favor of\
    \ predictive power, and run in polynomial and linear time, respectively. Other\
    \ works such as DC [\\[56,](#page-12-12) [58\\]](#page-12-13) and SDP [\\[29\\\
    ]](#page-11-28) are also partial order based methods that are either sound by\
    \ design or perform graph-based analysis to regain soundness. SyncP [\\[45\\]](#page-11-3),\
    \ M2 [\\[54\\]](#page-12-5), SeqCheck [\\[18\\]](#page-11-6) work similar to OSR,\
    \ by constructing an appropriate set of events and appropriate linearization over\
    \ this set. SMT solver backed approaches [\\[31,](#page-11-1) [60\\]](#page-12-2)\
    \ aim for sound and complete race prediction but do not scale to moderately large\
    \ execution traces. The complexity of data race prediction was extensively studied\
    \ in [\\[44\\]](#page-11-4) and was shown to be NP-hard and also W[1]-hard, implying\
    \ that an FPT algorithm (parameterized by the number of threads) for race prediction\
    \ is unlikely. The fine-grained complexity of HB and SyncP\n\nwas studied in [\\\
    [36\\]](#page-11-29); in practice, HB can be sped up using the tree clock data\
    \ structure [\\[43\\]](#page-11-30). Predictive analyses have also been developed\
    \ for deadlocks [\\[33,](#page-11-31) [68\\]](#page-12-14), atomicity violations\
    \ [\\[46,](#page-11-32) [66\\]](#page-12-15), for more general temporal specifications\
    \ [\\[10\\]](#page-11-33) and more recently has been investigated from the lens\
    \ of generalizing trace equivalence [\\[25\\]](#page-11-34).\n\nOther concurrency\
    \ testing approaches. Static analysis techniques employ forms of lockset style\
    \ reasoning [\\[61\\]](#page-12-16) to detect data races [\\[14,](#page-11-35)\
    \ [38,](#page-11-36) [51,](#page-12-17) [73\\]](#page-12-18) to report data races,\
    \ but are known to report false positives. Model checking techniques for concurrent\
    \ software [\\[8,](#page-11-37) [35,](#page-11-38) [52\\]](#page-12-19) have been\
    \ employed to detect concurrency bugs [\\[30,](#page-11-39) [53\\]](#page-12-20).\
    \ Another class of systematic exploration techniques include controlled concurrency\
    \ testing [\\[9,](#page-11-40) [21\\]](#page-11-41), including those that employ\
    \ randomization [\\[17,](#page-11-42) [40,](#page-11-43) [72\\]](#page-12-21)\
    \ and state-based learning [\\[50\\]](#page-12-22). More recently, feedback driven\
    \ randomized techniques have been\n\n<span id=\"page-10-1\"></span>Table 2: Evaluation\
    \ summary on **Category-2** (C/C++ benchmarks). Benchmarks are grouped based on\
    \ their source, and each row corresponds to one group. Column 1 denotes the source\
    \ and size of each group. Columns 2 and 3 respectively denote the range and the\
    \ total number of events in each group. Column 4 denotes the range of number of\
    \ threads in the benchmarks. Column 5-14 denote the total number of racy memory\
    \ locations, and average running time (in minutes) reported by each algorithm.\n\
    \n| 1               | 2            | 3     | 4        | 5     | 6     | 7    \
    \ | 8     | 9     | 10     | 11    | 12     | 13    | 14    |\n|-----------------|--------------|-------|----------|-------|-------|-------|-------|-------|--------|-------|--------|-------|-------|\n\
    | Benchmark Group | N            |       | T        |       | SHB   |       |\
    \ WCP   |       | SyncP  |       | M2     |       | OSR   |\n|               \
    \  | Range        | Total |          | Races | Time  | Races | Time  | Races |\
    \ Time   | Races | Time   | Races | Time  |\n| CoMD (8)        | [2.5M, 117M]\
    \ | 707M  | [16, 56] | 41.6k | 29.1  | 247k  | 72.3  | 32    | 1440   | 672  \
    \ | 1440   | 441k  | 32.4  |\n| SimpleMOC (1)   | [19M, 19M]   | 19M   | [16,\
    \ 16] | 380   | 0.1   | 388   | 23.8  | 32    | 180    | 32    | 180    | 32 \
    \   | 180   |\n| OMPRacer (15)   | [0.7M, 157M] | 625M  | [16, 58] | 1.2M  | 17.4\
    \  | 0.7M  | 84.1  | 3.3k  | 2.4k   | 1.9k  | 2.1k   | 1.3M  | 35.8  |\n| DRACC\
    \ (13)      | [0.5k, 104M] | 694M  | [16, 16] | 2247  | 8.4   | 2247  | 105.7\
    \ | 2442  | 1440.5 | 361   | 990.1  | 2450  | 66.6  |\n| DRB (33)        | [0.5k,\
    \ 900M] | 5.7B  | [16, 56] | 50.4k | 169.5 | 54.5k | 0.6k  | 1.5k  | 5.4k   |\
    \ 0.9k  | 4.9k   | 47.4k | 1.4k  |\n| HPC (46)        | [1k, 335M]   | 3.8B  |\
    \ [16, 56] | 6.5M  | 174.2 | 6.4M  | 775.4 | 102k  | 7.7k   | 3305  | 6.8k   |\
    \ 18.3M | 574.8 |\n| misc (7)        | [1k, 29M]    | 49M   | [4, 219] | 8548\
    \  | 0.9   | 8481  | 182.2 | 479   | 900.9  | 895   | 444.6  | 4289  | 183.4 |\n\
    | Total (123)     |              | 11.6B |          | 7.9M  | 6.7h  | 7.4M  |\
    \ 30.3h | 109k  | 324.4h | 8.1k  | 280.5h | 20.1M | 41.2h |\n\n<span id=\"page-10-2\"\
    ></span>![](_page_10_Figure_4.jpeg)\n\nemployed for testing concurrent programs\
    \ [\\[32,](#page-11-44) [71\\]](#page-12-23) Randomization has also been shown\
    \ to reduce time overhead of dynamic data race detection [\\[16,](#page-11-45)\
    \ [41,](#page-11-46) [67\\]](#page-12-24).\n\n#### <span id=\"page-10-0\"></span>7\
    \ CONCLUSIONS AND FUTURE WORK\n\nWe propose OSR, a sound polynomial time race\
    \ prediction algorithm that identifies data races that can be witnessed by optimistically\
    \ reversing synchronization operations. OSR significantly advances the state-of-the-art\
    \ in sound dynamic data race prediction. OSR-style reasoning can be helpful for\
    \ exposing other concurrency bugs such as deadlocks [\\[33,](#page-11-31) [68\\\
    ]](#page-12-14) and atomicity violations.\n\n### ACKNOWLEDGMENTS\n\nThis work\
    \ is partially supported by the National Research Foundation, Singapore, and Cyber\
    \ Security Agency of Singapore under its National Cybersecurity R&D Programme\
    \ (Fuzz Testing <NRF-NCR25-Fuzz-0001>) and by a research grant (VIL42117) from\
    \ VIL-LUM FONDEN. Any opinions, findings and conclusions, or recommendations expressed\
    \ in this material are those of the author(s) and do not reflect the views of\
    \ National Research Foundation, Singapore, and Cyber Security Agency of Singapore.\n\
    \nICSE '24, April 12–21, 2024, Lisbon, Portugal Zheng Shi, Umang Mathur, and Andreas\
    \ Pavlogiannis\n\n#### REFERENCES\n\n- <span id=\"page-11-21\"></span>[1] [n.\
    \ d.]. ECP Proxy Applications. [https://proxyapps.exascaleproject.org/.](https://proxyapps.exascaleproject.org/)\
    \ Accessed: 2021-08-01.\n- <span id=\"page-11-22\"></span>[2] [n. d.]. Mantevo\
    \ Project. [https://mantevo.org/.](https://mantevo.org/) Accessed: 2021-08-01.\n\
    - <span id=\"page-11-24\"></span>[3] [n. d.]. RaceInjector traces. [https://github.com/ALFA-group/RaceInjector](https://github.com/ALFA-group/RaceInjector-counterexamples/tree/main)[counterexamples/tree/main.](https://github.com/ALFA-group/RaceInjector-counterexamples/tree/main)\
    \ Accessed: 2023-07-14.\n- <span id=\"page-11-12\"></span>[4] [n. d.]. RAPID.\
    \ [https://github.com/umangm/rapid.](https://github.com/umangm/rapid) Accessed:\
    \ 2023-07-06.\n- <span id=\"page-11-19\"></span>[5] 2014. CORAL Benchmarks. Accessed:\
    \ 2021-08-01.\n- <span id=\"page-11-20\"></span>[6] 2014. CORAL2 Benchmarks. Accessed:\
    \ 2021-08-01.\n- <span id=\"page-11-7\"></span>[7] 2023. Range Minima Query Solutions.\
    \ [https://en.wikipedia.org/wiki/Range\\\\_](https://en.wikipedia.org/wiki/Range_minimum_query)\
    \ [minimum\\\\_query.](https://en.wikipedia.org/wiki/Range_minimum_query) Accessed:\
    \ 2023-07-18.\n- <span id=\"page-11-37\"></span>[8] Parosh Abdulla, Stavros Aronis,\
    \ Bengt Jonsson, and Konstantinos Sagonas. 2014. Optimal Dynamic Partial Order\
    \ Reduction. In Proceedings of the 41st ACM SIGPLAN-SIGACT Symposium on Principles\
    \ of Programming Languages (San Diego, California, USA) (POPL '14). Association\
    \ for Computing Machinery, New York, NY, USA, 373–384.<https://doi.org/10.1145/2535838.2535845>\n\
    - <span id=\"page-11-40\"></span>[9] Udit Agarwal, Pantazis Deligiannis, Cheng\
    \ Huang, Kumseok Jung, Akash Lal, Immad Naseer, Matthew Parkinson, Arun Thangamani,\
    \ Jyothi Vedurada, and Yunpeng Xiao. 2021. Nekara: Generalized Concurrency Testing.\
    \ In 2021 36th IEEE/ACM International Conference on Automated Software Engineering\
    \ (ASE). 679–691.<https://doi.org/10.1109/ASE51524.2021.9678838>\n- <span id=\"\
    page-11-33\"></span>[10] Zhendong Ang and Umang Mathur. 2024. Predictive Monitoring\
    \ against Pattern Regular Languages. Proc. ACM Program. Lang. 8, POPL, Article\
    \ 73 (jan 2024). <https://doi.org/10.1145/3632915>\n- <span id=\"page-11-18\"\
    ></span>[11] David H Bailey, Eric Barszcz, John T Barton, David S Browning, Robert\
    \ L Carter, Leonardo Dagum, Rod A Fatoohi, Paul O Frederickson, Thomas A Lasinski,\
    \ Rob S Schreiber, et al. 1991. The NAS parallel benchmarks—summary and preliminary\
    \ results. In Proceedings of the 1991 ACM/IEEE Conference on Supercomputing. 158–\
    \ 165.\n- <span id=\"page-11-8\"></span>[12] RICHARD BELLMAN. 1958. ON A ROUTING\
    \ PROBLEM. Quart. Appl. Math. 16, 1 (1958), 87–90.<http://www.jstor.org/stable/43634538>\n\
    - <span id=\"page-11-14\"></span>[13] Stephen M Blackburn, Robin Garner, Chris\
    \ Hoffmann, Asjad M Khang, Kathryn S McKinley, Rotem Bentzur, Amer Diwan, Daniel\
    \ Feinberg, Daniel Frampton, Samuel Z Guyer, et al. 2006. The DaCapo benchmarks:\
    \ Java benchmarking development and analysis. In Proceedings of the 21st annual\
    \ ACM SIGPLAN conference on Object-oriented programming systems, languages, and\
    \ applications. 169–190.\n- <span id=\"page-11-35\"></span>[14] Sam Blackshear,\
    \ Nikos Gorogiannis, Peter W O'Hearn, and Ilya Sergey. 2018. RacerD: compositional\
    \ static race detection. Proceedings of the ACM on Programming Languages 2, OOPSLA\
    \ (2018), 1–28.\n- <span id=\"page-11-26\"></span>[15] Hans-J Boehm. 2012. Position\
    \ paper: Nondeterminism is unavoidable, but data races are pure evil. In Proceedings\
    \ of the 2012 ACM workshop on Relaxing synchronization for multicore and manycore\
    \ scalability. 9–14.\n- <span id=\"page-11-45\"></span>[16] Michael D. Bond, Katherine\
    \ E. Coons, and Kathryn S. McKinley. 2010. PACER: Proportional Detection of Data\
    \ Races. In Proceedings of the 31st ACM SIGPLAN Conference on Programming Language\
    \ Design and Implementation (Toronto, Ontario, Canada) (PLDI '10). Association\
    \ for Computing Machinery, New York, NY, USA, 255–268.<https://doi.org/10.1145/1806596.1806626>\n\
    - <span id=\"page-11-42\"></span>[17] Sebastian Burckhardt, Pravesh Kothari, Madanlal\
    \ Musuvathi, and Santosh Nagarakatte. 2010. A randomized scheduler with probabilistic\
    \ guarantees of finding bugs. ACM SIGARCH Computer Architecture News 38, 1 (2010),\
    \ 167–178.\n- <span id=\"page-11-6\"></span>[18] Yan Cai, Hao Yun, Jinqiu Wang,\
    \ Lei Qiao, and Jens Palsberg. 2021. Sound and efficient concurrency bug prediction.\
    \ In Proceedings of the 29th ACM Joint Meeting on European Software Engineering\
    \ Conference and Symposium on the Foundations of Software Engineering. 255–267.\n\
    - <span id=\"page-11-0\"></span>[19] Milind Chabbi and Murali Krishna Ramanathan.\
    \ 2022. A Study of Real-World Data Races in Golang. In Proceedings of the 43rd\
    \ ACM SIGPLAN International Conference on Programming Language Design and Implementation\
    \ (San Diego, CA, USA) (PLDI 2022). Association for Computing Machinery, New York,\
    \ NY, USA, 474–489.<https://doi.org/10.1145/3519939.3523720>\n- <span id=\"page-11-47\"\
    ></span>[20] Lijie Chen and Ryan Williams. 2019. An equivalence class for orthogonal\
    \ vectors. In Proceedings of the Thirtieth Annual ACM-SIAM Symposium on Discrete\
    \ Algorithms. SIAM, 21–40.\n- <span id=\"page-11-41\"></span>[21] Pantazis Deligiannis,\
    \ Aditya Senthilnathan, Fahad Nayyar, Chris Lovett, and Akash Lal. 2023. Industrial-Strength\
    \ Controlled Concurrency Testing for C# Programs with COYOTE. In International\
    \ Conference on Tools and Algorithms for the Construction and Analysis of Systems.\
    \ Springer, 433–452.\n- <span id=\"page-11-15\"></span>[22] Hyunsook Do, Sebastian\
    \ Elbaum, and Gregg Rothermel. 2005. Supporting controlled experimentation with\
    \ testing techniques: An infrastructure and its potential impact. Empirical Software\
    \ Engineering 10 (2005), 405–435.\n- <span id=\"page-11-16\"></span>[23] Antonio\
    \ J Dorta, Casiano Rodriguez, and Francisco de Sande. 2005. The OpenMP source\
    \ code repository. In 13th Euromicro Conference on Parallel, Distributed and Network-Based\
    \ Processing. IEEE, 244–250.\n- <span id=\"page-11-13\"></span>[24] Eitan Farchi,\
    \ Yarden Nir, and Shmuel Ur. 2003. Concurrent bug patterns and how to test them.\
    \ In Proceedings international parallel and distributed processing symposium.\
    \ IEEE, 7–pp.\n- <span id=\"page-11-34\"></span>[25] Azadeh Farzan and Umang Mathur.\
    \ 2024. Coarser Equivalences for Causal Concurrency. Proc. ACM Program. Lang.\
    \ 8, POPL, Article 31 (jan 2024). [https:](https://doi.org/10.1145/3632873)\n\n\
    [//doi.org/10.1145/3632873](https://doi.org/10.1145/3632873)\n\n- <span id=\"\
    page-11-25\"></span>[26] Cormac Flanagan and Stephen N. Freund. 2009. FastTrack:\
    \ Efficient and Precise Dynamic Race Detection. In Proceedings of the 30th ACM\
    \ SIGPLAN Conference on Programming Language Design and Implementation (Dublin,\
    \ Ireland) (PLDI '09). Association for Computing Machinery, New York, NY, USA,\
    \ 121–133. [https:](https://doi.org/10.1145/1542476.1542490) [//doi.org/10.1145/1542476.1542490](https://doi.org/10.1145/1542476.1542490)\n\
    - <span id=\"page-11-9\"></span>[27] Lester Randolph Ford. 1956. Network flow\
    \ theory. (1956).\n- <span id=\"page-11-11\"></span>[28] Harold N Gabow, Jon Louis\
    \ Bentley, and Robert E Tarjan. 1984. Scaling and related techniques for geometry\
    \ problems. In Proceedings of the sixteenth annual ACM symposium on Theory of\
    \ computing. 135–143.\n- <span id=\"page-11-28\"></span>[29] Kaan Genç, Jake Roemer,\
    \ Yufan Xu, and Michael D Bond. 2019. Dependenceaware, unbounded sound predictive\
    \ race detection. Proceedings of the ACM on Programming Languages 3, OOPSLA (2019),\
    \ 1–30.\n- <span id=\"page-11-39\"></span>[30] Patrice Godefroid. 2005. Software\
    \ model checking: The VeriSoft approach. Formal Methods in System Design 26 (2005),\
    \ 77–101.\n- <span id=\"page-11-1\"></span>[31] Jeff Huang, Patrick O'Neil Meredith,\
    \ and Grigore Rosu. 2014. Maximal sound predictive race detection with control\
    \ flow abstraction. In Proceedings of the 35th ACM SIGPLAN conference on programming\
    \ language design and implementation. 337–348.\n- <span id=\"page-11-44\"></span>[32]\
    \ Dae R Jeong, Kyungtae Kim, Basavesh Shivakumar, Byoungyoung Lee, and Insik Shin.\
    \ 2019. Razzer: Finding kernel race bugs through fuzzing. In 2019 IEEE Symposium\
    \ on Security and Privacy (SP). IEEE, 754–768.\n- <span id=\"page-11-31\"></span>[33]\
    \ Christian Gram Kalhauge and Jens Palsberg. 2018. Sound Deadlock Prediction.\
    \ Proc. ACM Program. Lang. 2, OOPSLA, Article 146 (oct 2018), 29 pages. [https:](https://doi.org/10.1145/3276516)\
    \ [//doi.org/10.1145/3276516](https://doi.org/10.1145/3276516)\n- <span id=\"\
    page-11-2\"></span>[34] Dileep Kini, Umang Mathur, and Mahesh Viswanathan. 2017.\
    \ Dynamic Race Prediction in Linear Time. In Proceedings of the 38th ACM SIGPLAN\
    \ Conference on Programming Language Design and Implementation (Barcelona, Spain)\
    \ (PLDI 2017). Association for Computing Machinery, New York, NY, USA, 157–170.\
    \ <https://doi.org/10.1145/3062341.3062374>\n- <span id=\"page-11-38\"></span>[35]\
    \ Michalis Kokologiannakis and Viktor Vafeiadis. 2021. GenMC: A model checker\
    \ for weak memory models. In International Conference on Computer Aided Verification.\
    \ Springer, 427–440.\n- <span id=\"page-11-29\"></span>[36] Rucha Kulkarni, Umang\
    \ Mathur, and Andreas Pavlogiannis. 2021. Dynamic Data-Race Detection Through\
    \ the Fine-Grained Lens. In 32nd International Conference on Concurrency Theory.\n\
    - <span id=\"page-11-27\"></span>[37] Leslie Lamport. 1978. Time, Clocks, and\
    \ the Ordering of Events in a Distributed System. Commun. ACM 21, 7 (jul 1978),\
    \ 558–565. [https://doi.org/10.1145/359545.](https://doi.org/10.1145/359545.359563)\
    \ [359563](https://doi.org/10.1145/359545.359563)\n- <span id=\"page-11-36\"></span>[38]\
    \ Yanze Li, Bozhen Liu, and Jeff Huang. 2019. Sword: A scalable whole program\
    \ race detector for java. In 2019 IEEE/ACM 41st International Conference on Software\
    \ Engineering: Companion Proceedings (ICSE-Companion). IEEE, 75–78.\n- <span id=\"\
    page-11-17\"></span>[39] Chunhua Liao, Pei-Hung Lin, Joshua Asplund, Markus Schordan,\
    \ and Ian Karlin. 2017. DataRaceBench: a benchmark suite for systematic evaluation\
    \ of data race detection tools. In Proceedings of the International Conference\
    \ for High Performance Computing, Networking, Storage and Analysis. 1–14.\n- <span\
    \ id=\"page-11-43\"></span>[40] Weiyu Luo and Brian Demsky. 2021. C11Tester: a\
    \ race detector for C/C++ atomics. In Proceedings of the 26th ACM International\
    \ Conference on Architectural Support for Programming Languages and Operating\
    \ Systems. 630–646.\n- <span id=\"page-11-46\"></span>[41] Daniel Marino, Madanlal\
    \ Musuvathi, and Satish Narayanasamy. 2009. LiteRace: Effective sampling for lightweight\
    \ data-race detection. In Proceedings of the 30th ACM SIGPLAN Conference on Programming\
    \ Language Design and Implementation. 134–143.\n- <span id=\"page-11-5\"></span>[42]\
    \ Umang Mathur, Dileep Kini, and Mahesh Viswanathan. 2018. What happens-after\
    \ the first race? enhancing the predictive power of happens-before based dynamic\
    \ race detection. Proceedings of the ACM on Programming Languages 2, OOPSLA (2018),\
    \ 1–29.\n- <span id=\"page-11-30\"></span>[43] Umang Mathur, Andreas Pavlogiannis,\
    \ Hünkar Can Tunç, and Mahesh Viswanathan. 2022. A Tree Clock Data Structure for\
    \ Causal Orderings in Concurrent Executions. In Proceedings of the 27th ACM International\
    \ Conference on Architectural Support for Programming Languages and Operating\
    \ Systems. ACM, Lausanne Switzerland, 710–725.<https://doi.org/10.1145/3503222.3507734>\n\
    - <span id=\"page-11-4\"></span>[44] Umang Mathur, Andreas Pavlogiannis, and Mahesh\
    \ Viswanathan. 2020. The complexity of dynamic data race prediction. In Proceedings\
    \ of the 35th Annual ACM/IEEE Symposium on Logic in Computer Science. 713–727.\n\
    - <span id=\"page-11-3\"></span>[45] Umang Mathur, Andreas Pavlogiannis, and Mahesh\
    \ Viswanathan. 2021. Optimal prediction of synchronization-preserving races. Proceedings\
    \ of the ACM on Programming Languages 5, POPL (2021), 1–29.\n- <span id=\"page-11-32\"\
    ></span>[46] Umang Mathur and Mahesh Viswanathan. 2020. Atomicity Checking in\
    \ Linear Time Using Vector Clocks. In Proceedings of the Twenty-Fifth International\
    \ Conference on Architectural Support for Programming Languages and Operating\
    \ Systems (Lausanne, Switzerland) (ASPLOS '20). Association for Computing Machinery,\
    \ New York, NY, USA, 183–199.<https://doi.org/10.1145/3373376.3378475>\n- <span\
    \ id=\"page-11-23\"></span>[47] Patrick Meredith and Grigore Roşu. 2010. Runtime\
    \ verification with the RV system. In International Conference on Runtime Verification.\
    \ Springer, 136–152.\n- <span id=\"page-11-10\"></span>[48] Edward F. Moore. 1959.\
    \ The shortest path through a maze. In Proc. Internat. Sympos. Switching Theory\
    \ 1957, Part II. Harvard Univ. Press, Cambridge, Mass., 285–292.\n\n- <span id=\"\
    page-12-11\"></span>[49] Arndt Müehlenfeld and Franz Wotawa. 2007. Fault Detection\
    \ in Multi-threaded C++ Server Applications. In Proceedings of the 12th ACM SIGPLAN\
    \ Symposium on Principles and Practice of Parallel Programming (San Jose, California,\
    \ USA) (PPoPP '07). ACM, New York, NY, USA, 142–143.<https://doi.org/10.1145/1229428.1229457>\n\
    - <span id=\"page-12-22\"></span>[50] Suvam Mukherjee, Pantazis Deligiannis, Arpita\
    \ Biswas, and Akash Lal. 2020. Learning-based controlled concurrency testing.\
    \ Proceedings of the ACM on Programming Languages 4, OOPSLA (2020), 1–31.\n- <span\
    \ id=\"page-12-17\"></span>[51] Mayur Naik, Alex Aiken, and John Whaley. 2006.\
    \ Effective static race detection for Java. In Proceedings of the 27th ACM SIGPLAN\
    \ Conference on Programming Language Design and Implementation. 308–319.\n- <span\
    \ id=\"page-12-19\"></span>[52] Brian Norris and Brian Demsky. 2013. CDSchecker:\
    \ Checking Concurrent Data Structures Written with C/C++ Atomics. In Proceedings\
    \ of the 2013 ACM SIGPLAN International Conference on Object Oriented Programming\
    \ Systems Languages & Applications (Indianapolis, Indiana, USA) (OOPSLA '13).\
    \ Association for Computing Machinery, New York, NY, USA, 131–150. [https://doi.org/10.1145/2509136.](https://doi.org/10.1145/2509136.2509514)\
    \ [2509514](https://doi.org/10.1145/2509136.2509514)\n- <span id=\"page-12-20\"\
    ></span>[53] Jonas Oberhauser, Rafael Lourenco de Lima Chehab, Diogo Behrens,\
    \ Ming Fu, Antonio Paolillo, Lilith Oberhauser, Koustubha Bhat, Yuzhong Wen, Haibo\
    \ Chen, Jaeho Kim, et al. 2021. VSync: push-button verification and optimization\
    \ for synchronization primitives on weak memory models. In Proceedings of the\
    \ 26th ACM International Conference on Architectural Support for Programming Languages\
    \ and Operating Systems. 530–545.\n- <span id=\"page-12-5\"></span>[54] Andreas\
    \ Pavlogiannis. 2019. Fast, sound, and effectively complete dynamic race prediction.\
    \ Proceedings of the ACM on Programming Languages 4, POPL (2019), 1–29.\n- <span\
    \ id=\"page-12-4\"></span>[55] Eli Pozniansky and Assaf Schuster. 2003. Efficient\
    \ on-the-fly data race detection in multithreaded C++ programs. In Proceedings\
    \ of the ninth ACM SIGPLAN symposium on Principles and practice of parallel programming.\
    \ 179–190.\n- <span id=\"page-12-12\"></span>[56] Jake Roemer, Kaan Genç, and\
    \ Michael D Bond. 2020. SmartTrack: efficient predictive race detection. In Proceedings\
    \ of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation.\
    \ 747–762.\n- <span id=\"page-12-1\"></span>[57] Jake Roemer, Kaan Genç, and Michael\
    \ D. Bond. 2018. High-Coverage, Unbounded Sound Predictive Race Detection. In\
    \ Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design\
    \ and Implementation (Philadelphia, PA, USA) (PLDI 2018). Association for Computing\
    \ Machinery, New York, NY, USA, 374–389.<https://doi.org/10.1145/3192366.3192385>\n\
    - <span id=\"page-12-13\"></span>[58] Jake Roemer, Kaan Genç, and Michael D. Bond.\
    \ 2018. High-Coverage, Unbounded Sound Predictive Race Detection. In Proceedings\
    \ of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation\
    \ (Philadelphia, PA, USA) (PLDI 2018). Association for Computing Machinery, New\
    \ York, NY, USA, 374–389.<https://doi.org/10.1145/3192366.3192385>\n- <span id=\"\
    page-12-0\"></span>[59] Caitlin Sadowski and Jaeheon Yi. 2014. How Developers\
    \ Use Data Race Detection Tools. In Proceedings of the 5th Workshop on Evaluation\
    \ and Usability of Programming Languages and Tools (Portland, Oregon, USA) (PLATEAU\
    \ '14). Association for Computing Machinery, New York, NY, USA, 43–51. [https:](https://doi.org/10.1145/2688204.2688205)\
    \ [//doi.org/10.1145/2688204.2688205](https://doi.org/10.1145/2688204.2688205)\n\
    - <span id=\"page-12-2\"></span>[60] Mahmoud Said, Chao Wang, Zijiang Yang, and\
    \ Karem Sakallah. 2011. Generating data race witnesses by an SMT-based analysis.\
    \ In NASA Formal Methods Symposium. Springer, 313–327.\n- <span id=\"page-12-16\"\
    ></span>[61] Stefan Savage, Michael Burrows, Greg Nelson, Patrick Sobalvarro,\
    \ and Thomas Anderson. 1997. Eraser: A dynamic data race detector for multithreaded\
    \ programs. ACM Transactions on Computer Systems (TOCS) 15, 4 (1997), 391–411.\n\
    - <span id=\"page-12-8\"></span>[62] Adrian Schmitz, Joachim Protze, Lechen Yu,\
    \ Simon Schwitanski, and Matthias S Müller. 2019. DataRaceOnAccelerator–a micro-benchmark\
    \ suite for evaluating correctness tools targeting accelerators. In European Conference\
    \ on Parallel Processing. Springer, 245–257.\n- <span id=\"page-12-7\"></span>[63]\
    \ Koushik Sen, Grigore Roşu, and Gul Agha. 2005. Detecting errors in multithreaded\
    \ programs by generalized predictive analysis of executions. In Formal Methods\
    \ for Open Object-Based Distributed Systems: 7th IFIP WG 6.1 International Conference,\
    \ FMOODS 2005, Athens, Greece, June 15-17, 2005. Proceedings 7. Springer, 211–226.\n\
    - <span id=\"page-12-9\"></span>[64] Konstantin Serebryany and Timur Iskhodzhanov.\
    \ 2009. ThreadSanitizer: data race detection in practice. In Proceedings of the\
    \ workshop on binary instrumentation and applications. 62–71.\n- <span id=\"page-12-3\"\
    ></span>[65] Yannis Smaragdakis, Jacob Evans, Caitlin Sadowski, Jaeheon Yi, and\
    \ Cormac Flanagan. 2012. Sound Predictive Race Detection in Polynomial Time. In\
    \ Proceedings of the 39th Annual ACM SIGPLAN-SIGACT Symposium on Principles of\
    \ Programming Languages (Philadelphia, PA, USA) (POPL '12). Association for Computing\
    \ Machinery, New York, NY, USA, 387–400. [https://doi.org/10.1145/](https://doi.org/10.1145/2103656.2103702)\
    \ [2103656.2103702](https://doi.org/10.1145/2103656.2103702)\n- <span id=\"page-12-15\"\
    ></span>[66] Francesco Sorrentino, Azadeh Farzan, and P. Madhusudan. 2010. PENELOPE:\
    \ Weaving Threads to Expose Atomicity Violations. In Proceedings of the Eighteenth\
    \ ACM SIGSOFT International Symposium on Foundations of Software Engineering (Santa\
    \ Fe, New Mexico, USA) (FSE '10). Association for Computing Machinery, New York,\
    \ NY, USA, 37–46.<https://doi.org/10.1145/1882291.1882300>\n- <span id=\"page-12-24\"\
    ></span>[67] Mosaad Al Thokair, Minjian Zhang, Umang Mathur, and Mahesh Viswanathan.\
    \ 2023. Dynamic Race Detection with O (1) Samples. Proceedings of the ACM on Programming\
    \ Languages 7, POPL (2023), 1308–1337.\n- <span id=\"page-12-14\"></span>[68]\
    \ Hünkar Can Tunç, Umang Mathur, Andreas Pavlogiannis, and Mahesh Viswanathan.\
    \ 2023. Sound Dynamic Deadlock Prediction in Linear Time. Proc. ACM Program. Lang.\
    \ 7, PLDI, Article 177 (jun 2023), 26 pages. [https:](https://doi.org/10.1145/3591291)\
    \ [//doi.org/10.1145/3591291](https://doi.org/10.1145/3591291)\n- <span id=\"\
    page-12-10\"></span>[69] Michael Wang, Shashank Srikant, Malavika Samak, and Una-May\
    \ O'Reilly. 2023. RaceInjector: Injecting Races to Evaluate and Learn Dynamic\
    \ Race Detection Algorithms. In Proceedings of the 12th ACM SIGPLAN International\
    \ Workshop on the State Of the Art in Program Analysis. 63–70.\n- <span id=\"\
    page-12-6\"></span>[70] Ryan Williams. 2005. A new algorithm for optimal 2-constraint\
    \ satisfaction and its implications. Theoretical Computer Science 348, 2 (2005),\
    \ 357–365. [https://doi.org/](https://doi.org/10.1016/j.tcs.2005.09.023) [10.1016/j.tcs.2005.09.023](https://doi.org/10.1016/j.tcs.2005.09.023)\
    \ Automata, Languages and Programming: Algorithms and Complexity (ICALP-A 2004).\n\
    - <span id=\"page-12-23\"></span>[71] Meng Xu, Sanidhya Kashyap, Hanqing Zhao,\
    \ and Taesoo Kim. 2020. Krace: Data race fuzzing for kernel file systems. In 2020\
    \ IEEE Symposium on Security and Privacy (SP). IEEE, 1643–1660.\n- <span id=\"\
    page-12-21\"></span>[72] Xinhao Yuan, Junfeng Yang, and Ronghui Gu. 2018. Partial\
    \ order aware concurrency sampling. In Computer Aided Verification: 30th International\
    \ Conference, CAV 2018, Held as Part of the Federated Logic Conference, FloC 2018,\
    \ Oxford, UK, July 14-17, 2018, Proceedings, Part II 30. Springer, 317–335.\n\
    - <span id=\"page-12-18\"></span>[73] Sheng Zhan and Jeff Huang. 2016. ECHO: instantaneous\
    \ in situ race detection in the IDE. In Proceedings of the 2016 24th ACM SIGSOFT\
    \ International Symposium on Foundations of Software Engineering. 775–786.\n\n\
    #### A PROOFS FROM SECTION 3\n\n#### <span id=\"page-13-0\"></span>A.1 Proof of\
    \ Theorem [3.1](#page-3-1)\n\nTheorem [3.1.](#page-3-1) Let be a trace, let 1,\
    \ <sup>2</sup> be conflicting events and let ⊆ Events() be an optimistically lock-closed\
    \ set. The problem of determining whether there is a correct reordering such that\
    \ Events() = and both <sup>1</sup> and <sup>2</sup> are -enabled in is NP-hard.\n\
    \nWe prove this theorem by instead establishing the following stronger Theorem\
    \ [A.1;](#page-13-1) it claims that, the problem of determining if the smallest\
    \ optimistically lock-closed set can be linearized, is NP-hard problem.\n\n<span\
    \ id=\"page-13-1\"></span>Theorem A.1. Let be a trace, let 1, <sup>2</sup> be\
    \ conflicting events, and let = OLClosure(e1, e2). The problem of determining\
    \ whether there is a correct reordering s.t. Events() = is NP-hard.\n\nThe high\
    \ level idea behind our proof of is inspired from [\\[44\\]](#page-11-4), which\
    \ shows that the problem of checking if a given pair of conflicting events is\
    \ a predictable data race, is NP-hard. In [\\[44\\]](#page-11-4), the proof proceeds\
    \ by first showing that an intermediate problem, namely RF-poset realizability,\
    \ is NP-hard. An instance of this problem is a triple P = (, , ), where is some\
    \ set of read, write, acquire and release events, ⊆ × is a partial order on and\
    \ is a function that maps every read event ∈ to a unique write event ∈ on the\
    \ same memory location. P is a positive intance of RF-poset-realixability if there\
    \ is a linearization of that respects ∪ {(, ) | = ()} and also ensures that between\
    \ any read (resp. release) event and its corresponding write (resp. matching acquire)\
    \ event = (), there is no other write event of the same memory location (resp.\
    \ lock) as . In [\\[44\\]](#page-11-4), the NP-hardness of RF-poset realizability\
    \ is established via a reduction from INDEPENDENT-SET(c), which is the problem\
    \ of checking if for an input graph , there is an independent set of of size at\
    \ least . Following this, [\\[44\\]](#page-11-4) establishes a reduction from\
    \ RF-poset realizability to the race prediction problem.\n\nOur proof is inspired\
    \ from this, but is a direct reduction from INDEPENDENT-SET(c) to our problem\
    \ — given a trace and a pair (1, 2), determine if there is a correct reordering\
    \ containing exactly the events OLClosure(e1, e2). Given an input graph (instance\
    \ of RF-poset realizability problem), we construct trace with two events 1 and\
    \ 2 in as follows. We first construct an intermediate RF-poset instace P by slightly\
    \ modifying the RF-poset instance constructed by [\\[44\\]](#page-11-4), ensuring\
    \ that P is realizable iff the graph has an independent set of size ≥ . Starting\
    \ with P, we can then construct a trace with two specific events 1, <sup>2</sup>\
    \ such that P can be realized iff there is a correct reordering of for which OLClosure(e1,\
    \ e2) can be linearized.\n\nProof. Given an INDEPENDENT-SET(c) problem on graph\
    \ , we encode a RF-poset realizability instance P as following. The set of events\
    \ belong to 2 + 2 threads 1, <sup>2</sup> . . . 2+2, and we describe the total\
    \ order of events in each thread next.\n\n$$\\begin{array}{l} \\text{(1) For }\
    \ i = 2 \\cdot c + 1, \\,\\tau\\_{\\bar{t}} = \\mathbf{w}(q), \\mathbf{w}(v\\\
    _{2c+1}) \\\\ \\text{(2) For } i = 2 \\cdot c + 2, \\,\\tau\\_{\\bar{t}} = \\\
    tau\\_{\\bar{t}}^1 \\circ \\tau\\_{\\bar{t}}^2, \\text{ where } \\bar{t} \\end{array}$$\n\
    \n$$\\tau\\_{\\mathfrak{t}}^{1} = \\mathbf{r}(\\mathsf{s}\\_{1}), \\dots, \\mathbf{r}(\\\
    mathsf{s}\\_{\\mathcal{C}}), \\mathbf{acq}(\\ell\\_{\\mathfrak{l}}), \\dots, \\\
    mathbf{acq}(\\ell\\_{\\mathfrak{c}}), \\mathbf{r}(q), \\mathbf{w}\\_{\\mathfrak{l}}(\\\
    mathbf{x}), \\mathbf{rel}(\\ell\\_{\\mathfrak{c}}), \\dots, \\mathbf{rel}(\\ell\\\
    _{\\mathfrak{l}}).$$\n\n$$\\text{Let } C\\_{\\mathfrak{l}} = \\mathbf{r}(v\\_{\\\
    mathfrak{l} + \\mathbf{c}}) \\cdot \\mathbf{r}(v\\_{\\mathfrak{l}}) \\text{ and\
    \ } \\tau\\_{\\mathfrak{l}}^{2} = \\mathbf{r}(v\\_{2\\mathfrak{c} + \\mathbf{1}})\
    \ \\diamond C\\_{\\mathfrak{c}} \\diamond \\cdots \\diamond C\\_{\\mathfrak{l}}.$$\n\
    \n(3) For each integer 1 ≤ ≤ , = 1 ◦ 2 ◦. . . ◦w(). For 2 ≤ ≤ − 1, we have = acq\
    \ (ℓ,<sup>1</sup> ), . . . , acq (ℓ, ), w( ), r( ),rel(ℓ, ), . . . ,rel(ℓ,<sup>1</sup>\
    \ ), where 1, . . . , denotes the neighbors of node in graph . Let 1 = acq (ℓ1,<sup>1</sup>\
    \ ), . . . , acq (ℓ1, ), w(),r( 1 ) rel(ℓ1, ), . . . ,rel(ℓ1,<sup>1</sup> ), and\
    \ = acq (ℓ,<sup>1</sup> ), . . . , acq (ℓ, ), w( ),r(),rel(ℓ, ), . . . rel(ℓ,<sup>1</sup>\
    \ ). 1 2\n\n(4) For each integer + 1 ≤ ≤ 2, = ◦ ◦ . . . ◦ w(), where = acq (ℓ−\
    \ ), w( − ),r( +1 − ),rel (ℓ− ).\n\nSee an example of the reduction outlined above\
    \ in Figure [7.](#page-14-0) Each variable in P is written once, so the read-from\
    \ relation is clear. The partial order is the thread order induced by the threads\
    \ 1, . . . 2+2.\n\nWe remark that the poset P is almost identical to the one in\
    \ [\\[44\\]](#page-11-4) (let's call it P ′ ), except for the extra read and write\
    \ events on variables {1, . . . , 2+1} at the end of each thread in P. We will\
    \ use P′ to denote the subset of events that belong to P ′ .\n\nLet us first argue\
    \ that P is realizable iff P ′ is realizable. If P ′ is realizable, then there\
    \ is a linearization ′ of P′ that preserves thread order and the reads-from of\
    \ events in P′ . Consider the the trace = ′ , w(1) . . . w(2 ),r(1) . . . r(2\
    \ ), where we omit the obvious thread identifiers of events on {1, . . . , 2+1}.\
    \ Clearly, ′ witnesses the realizability of P. Now, if P ′ is realizable using\
    \ a linearization , it is easy to argue that the linearization ′ obtained by removing\
    \ events of memory locations {1, . . . , 2+1} witnesses the realizability of P\
    \ ′ . It thus also follows that is a positive instance of INDEPENDENT-SET(c) iff\
    \ P is realizable.\n\nLet us now construct the trace and complete our reduction.\
    \ The set of events of trace will be Events() = ⊎ {w<sup>1</sup> (), w<sup>2</sup>\
    \ ()}, where is a fresh memory location and w<sup>1</sup> () writes to as the\
    \ new last event of 2+1, while w<sup>2</sup> () writes to as the new last event\
    \ of 2+2. Observe that in P, every write event in thread is read by events in\
    \ + for 1 ≤ ≤ . We let be an arbitrary interleaving of and + that respects the\
    \ thread order and read-from relation. We construct the trace to be the following.\n\
    \n$$\\sigma = \\tau\\_{2c+1} \\circ \\text{w}\\_1(a) \\circ \\mathfrak{t}\\_1\
    \ \\circ \\cdots \\circ \\mathfrak{t}\\_c \\circ \\tau\\_{2c+2} \\circ \\text{w}\\\
    _2(a)$$\n\nFirst, it is clear that OLClosure(w<sup>1</sup> (a), w<sup>2</sup>\
    \ (a)) = . Thus, OLClosure(w<sup>1</sup> (a), w<sup>2</sup> (a)) can be linearized\
    \ iff P can be realized. Consequently, the input graph has an independent set\
    \ of size ≥ iff (w<sup>1</sup> (), w<sup>2</sup> ()) is witnessed as a race of\
    \ using OLClosure(w<sup>1</sup> (a), w<sup>2</sup> (a)). Finally, it is clear\
    \ that the construction takes polynomial time in the size of the graph . Thus,\
    \ it follows that the problem of checking if, for a given trace and a pair of\
    \ conflicting events (1, 2) in , whether there is a correct reordering of with\
    \ Events() = OLClosure(e1, e2), is also NP-hard. □\n\n#### B PROOFS FROM SECTION\
    \ 4\n\n#### B.1 Proof of Lemma [4.1](#page-4-3)\n\nLemma [4.1.](#page-4-3) Let\
    \ (1, 2) be a conflicting pair of events in trace . If (1, 2) is an optimistic\
    \ sync-reversal race, then it can be witnessed in an optimistic correct reordering\
    \ with Events() = OLClosure(e1, e2).\n\nProof Sketch. Given and two conflicting\
    \ events e1, e2, let = OLClosure(e1, e2) and be an arbitrary optimistically lock-closed\n\
    \n<span id=\"page-14-0\"></span>![](_page_14_Figure_2.jpeg)\n\nFigure 7: Given\
    \ a graph and independent set size of 2, our construction to show NP-Hardness\
    \ of linearizing OLClosure(e1, e2)\n\nevent set. By definition, is the smallest\
    \ optimistically lock-closed set and thus ⊆ . Let Opt ,Opt be the optimistic-reorderinggraph\
    \ of and , respectively, We now show if there is a cycle in Opt , then Opt also\
    \ has a cycle.\n\nFirst we consider the nodes in Opt and we have for any node\
    \ in Opt , must be in Opt , because is a superset over . Second we show for all\
    \ forward edges → in Opt , this edge also exists in Opt . This is because , are\
    \ also nodes in , and the edge → exists, if both , are in .\n\nNext, we show that\
    \ for each backward edges (, ) in Opt , there is a path from to in Opt ; here,\
    \ by backward edge we mean that ≤ tr . Notice that since (, ) is a backward edge,\
    \ it must be that = last(rel(ℓ)) be the last release of lock ℓ and = acqO(ℓ) is\
    \ an unmatched acquire in , for some ℓ. We first establish that indeed must also\
    \ be unmatched in . Since is optimistically lock closed, it must be that {1, 2}\
    \ ∩ TRClosure(match (a)) ≠ ∅. If, on the contrary, match () ∈ , the fact that\
    \ is (≤ TO,rf )-closed, we must have {1, 2} ∩ ≠ ∅. Clearly this would contradicts\
    \ the fact that is optimistically lock-closed. Thus, is unmatched even in\n\n\
    . Further, from the definition of Opt , it follows that ( ′ , ) is an edge of\
    \ Opt .\n\nNow let ′ = last(rel(ℓ)) We have ≤ tr ′ , because ⊆ . This means that\
    \ there is a path in Opt of the form → ′ → in Opt . In other words, all paths\
    \ of Opt are preserved in Opt and thus, so are the cycles of Opt .\n\nBy definition,\
    \ if (e1, e2) is an optimistic sync-reversal race, there is a optimistic correct\
    \ reordering , s.t. the optimistic-reorderinggraph of Events() has no cycle. Then\
    \ we can conclude Opt also has no cycle, thus the linearization of Opt can also\
    \ witness this race (thanks to Lemma [4.2\\)](#page-5-2). □\n\n#### B.2 Proof\
    \ of Lemma [4.2](#page-5-2)\n\nLemma [4.2.](#page-5-2) Let be a trace and let\
    \ ⊆ Events() such that is (≤ TO,rf )-closed and also lock-feasible. Then, there\
    \ is an optimistic reordering of on the set iff the graph Opt is acyclic.\n\n\
    Proof Sketch. Let us first assume that Opt is acyclic. Consider a linearization\
    \ of Opt . We argue that is an optimistic reordering of . First, is (≤ TO,rf )-closed\
    \ and Opt orders all events of the same thread as in . Hence respects ≤ TO. Second,\
    \ for every read event, its corresponding writerrf () is in , and further is ordered\
    \ before in the graph Opt , and every other conflicting write ′ is either after\
    \ or before in and thus in Opt and thus in . Finally, lock semantics are preserved\
    \ since is lock-feasible, the matched critical sections are totally ordered and\
    \ further the unmatched acquire is ordered after every other release of the same\
    \ lock. Finally, is an optimistic reordering because the order of matched critical\
    \ sections and the order of conflicting events is preserved because they are explicit\
    \ edges in Opt .\n\nNow assume that there is an optimistic reordering of with\
    \ Events() = . We will argue that for every edge (1, 2) of Opt , we have <sup>1</sup>\
    \ ≤ tr 2. This would imply that Opt is acyclic, since is acylic. First, consider\
    \ the case when (1, 2) is such that <sup>1</sup> ≤ TO 2. Since is a correct reordering\
    \ of , we must also have <sup>1</sup> ≤ TO 2. Second, consider the case when <sup>1</sup>\
    \ = rel(ℓ) and <sup>2</sup> = acq(ℓ). If <sup>2</sup> is matched in , then we\
    \ have <sup>1</sup> ≤ tr <sup>2</sup> since is an optimistic reordering. Otherwise,\
    \ <sup>2</sup> is an unmatched acquire and must be placed last in anyway. Finally,\
    \ if <sup>1</sup> ⊲⊳ 2, then the fact that 1 ≤ tr <sup>2</sup> and that orders\
    \ conflicting events the same way as implies that <sup>1</sup> ≤ tr 2. □\n\n####\
    \ B.3 Proof of Lemma [4.3](#page-5-3)\n\nLemma [4.3.](#page-5-3) Let 1, 2, ′ 2\
    \ ∈ Events() be events in trace with <sup>2</sup> ≤ TO ′ 2 . Let = OLClosure(e1,\
    \ e2) and let ′ = OLClosure(e1, e ′ 2 ). We have the following: (1) ⊆ ′ . (2)\
    \ = ComputeOLClosure(1, 2, ∅), and further this call (in Algorithm [1\\)](#page-4-4)\
    \ takes e(| |) time. (3) ′ = ComputeOLClosure(1, ′ 2 , ), and further this call\
    \ (in Algorithm [1\\)](#page-4-4) takes e(| ′ | − | |) time.\n\nProof Sketch.\
    \ We first show ComputeOLClosure(1, 2, ∅) = . For convenience, we denote as after\
    \ i-th iteration in Algorithm [1.](#page-4-4) Now we prove ComputeOLClosure(1,\
    \ 2, ∅) ⊆ by induction.\n\n- Firstly, the initial set <sup>0</sup> ⊆\n- Assuming\
    \ after iterations, ⊆ , we show +<sup>1</sup> ⊆ . By definition of ComputeOLClosure,\
    \ +<sup>1</sup> = ∪TRClosure(rel), where match (rel) ∈ and {1, 2} ∩ TRClosure(rel)\
    \ = ∅. Since ⊆ , by definition of OLClosure(e1, e2), we have rel ∈ and therefore\
    \ +<sup>1</sup> ⊆\n\nSo far we proved ComputeOLClosure(1, 2, ∅) ⊆ . Further we\
    \ claim ComputeOLClosure(1, 2, ∅) is optimistic lock-closed. Otherwise, the while\
    \ loop in Algorithm [1](#page-4-4) will not terminate. This proves = ComputeOLClosure(1,\
    \ 2, ∅).\n\nNow we prove ⊆ ′ . We consider an arbitrary run of Algorithm [1](#page-4-4)\
    \ on computing = ComputeOLClosure(1, 2, ∅), and construct another valid run of\
    \ Algorithm [1](#page-4-4) on computing ′ = ComputeOLClosure(1, ′ 2 , ∅). During\
    \ this process, we prove after iterations, ⊆ ′ for all .\n\n(1) We observe <sup>0</sup>\
    \ ⊆ ′ 0 , as prev (2) ∈ TRClosure(prev (e ′ 2 )).\n\n(2) Assuming ⊆ ′ , we prove\
    \ +<sup>1</sup> ⊆ ′ +1 .\n\nFor all release event rel(ℓ), if rel(ℓ) ∉ and {1,\
    \ 2} ∩ TRClosure(rel(ℓ)) = ∅, we have {1, ′ 2 } ∩ TRClosure(rel(ℓ)) = ∅, because\
    \ <sup>2</sup> ≤ TO ′ 2 . Then there are two possibilities. The first being that\
    \ rel(ℓ) ∈ ′ , which means ′ is already a superset of +1, so that +<sup>1</sup>\
    \ ⊆ ′ ⊆ ′ +1 . Alternatively, if rel(ℓ) ∉ ′ , for any update we do for , we can\
    \ also do the same update for ′ . Therefore, after one more iteration, the observation\
    \ of +<sup>1</sup> ⊆ ′ +1 still holds.\n\nThe observation above proves ComputeOLClosure(1,\
    \ 2, ∅) ⊆ ComputeOLClosure(1, ′ 2 , ∅). Since ComputeOLClosure(1, 2, ∅) = and\
    \ ComputeOLClosure(1, ′ 2 , ∅) = ′ , we have ⊆ ′ .\n\nNow we show can be computed\
    \ by ComputeOLClosure in ˜ (| |) time. In each iteration of ComputeOLClosure,\
    \ we need (T L) time to check if any updates can be done, and there are at most\
    \ | | iterations. Therefore, each event is visited at most ˜ (T L) times.\n\n\
    For the third conclusion, we can take as a start point and call ComputeOLClosure(1,\
    \ ′ 2 , ) to compute ′ , as ⊆ ′ . Following the proof of the second conclusion,\
    \ this takes ˜ (| ′ | − | |) time. It remains to show ComputeOLClosure(1, ′ 2\
    \ , ) returns ′ . We show this by induction. We denote ′ as ′ after iterations\
    \ in Algorithm [1.](#page-4-4)\n\n- It's obvious that ′ 0 ⊆ ′\n- Assuming ′ ⊆\
    \ ′ , we show ′ +1 ⊆ ′ . By definition of ComputeOLClosure, ′ +1 = ′ ∪ TRClosure(rel),\
    \ where match (rel) ∈ ′ and {1, ′ 2 } ∩ TRClosure(rel) = ∅. Since ′ ⊆ ′ , by definition\
    \ of OLClosure(e1, e ′ 2 ), we have rel ∈ ′ and therefore ′ +1 ⊆ ′\n\nThis proves\
    \ ComputeOLClosure(1, ′ 2 , ) indeed returns ′ .\n\n□\n\n#### B.4 Proof of Theorem\
    \ [4.1](#page-5-4)\n\nTheorem [4.1.](#page-5-4) Let be a trace and let 1, <sup>2</sup>\
    \ be conflicting events in . The problem of determining if (1, 2) is an optimistic\
    \ syncreversal race can be solved in time T (T N + L) <sup>=</sup> e(N ) time.\n\
    \nProof. For given 1, 2, to determine if they are OSR race, we firstly compute\
    \ their optimistic lock closure, check for lockfeasibility and then build the\
    \ abstract graph to check for cycles. We have shown in Section [4.1](#page-4-2)\
    \ that for any given 1, 2, OLClosure(e1, <sup>e</sup>2) can be computed in (T2N\
    \ ). Lock-feasibility can be checked in (T L) time.\n\nTo build the graph, we\
    \ firstly add all vertices and backward edges. Later, we compute earliest successors\
    \ for each vertex in the graph and add forward edges correspondingly. The abstract\
    \ graph contains at most 2L nodes by definition. Also in Section [4.2,](#page-5-0)\
    \ we have shown that it takes (L) time to add all backward edges and (T2L) time\
    \ to add all forward edges. Checking cycles in the graph takes (L + L<sup>2</sup>\
    \ ) time, as there are at most (L) vertices and (L<sup>2</sup> ) edges. Therefore,\
    \ building the graph and checking for cycle take (L + L + T2L + L<sup>2</sup>\
    \ ), i.e. (L (T<sup>2</sup> + L)) in total.\n\nTo do race detection on given 1,\
    \ 2, it takes (NT<sup>2</sup> + LT + L (T<sup>2</sup> + L)), i.e. (T2N + L<sup>2</sup>\
    \ ) □\n\n#### B.5 Proof of Theorem [4.2](#page-6-4)\n\nTheorem [4.2.](#page-6-4)\
    \ Let be an execution, ∈ Events() be a read or write event and let ∈ Threads().\
    \ The problem of checking if there is an event ′ with th( ′ ) = such that (, ′\
    \ ) is an optimisticsync-reversal race, can be solved in time (T<sup>2</sup> +\
    \ L)LN .\n\nProof. Following Algorithm [3,](#page-6-3) the computation ComputeOLClosure\
    \ for each (, ′ ), s.t. th( ′ ) = is equivalent to compute the ComputeOLClosure\
    \ for and the last ′ in thread , which can be done in (T2N ) time.\n\nWe also\
    \ need to check lock-feasibility, build graph and check cycles for each ′ in .\
    \ There are at most N such ′ . The total time complexity to do so is (N (LT +\
    \ L (T<sup>2</sup> + L))), i.e. (NL (T<sup>2</sup> + L)).\n\nIn total, we need\
    \ (T2N + NL (T<sup>2</sup> + L)), i.e. (T<sup>2</sup> + L)LN time to check for\
    \ all races between (, ′ ), s.t. th( ′ ) = □\n\n#### B.6 Proof of Theorem [4.3](#page-7-4)\n\
    \nTheorem [4.3.](#page-7-4) Given a trace , the problem of checking if has an\
    \ optimistic sync-reversal data race, can be solved in time T L (T<sup>2</sup>\
    \ + L)N<sup>2</sup> <sup>=</sup> e(N<sup>2</sup> ) time.\n\nProof. Following Algorithm\
    \ [4,](#page-7-2) we iterate over all events and for a fixed event , we iterate\
    \ over all threads. Therefore, Algorithm [3](#page-6-3) is called at most(T N\
    \ ) times. Then the total complexity to check for races is bound by (T N · (T<sup>2</sup>\
    \ + L)LN ), i.e. (T L (T<sup>2</sup> + L)N<sup>2</sup> time. □\n\n#### B.7 Proof\
    \ of Lemma [4.4](#page-6-1)\n\nLemma [4.4.](#page-6-1) Let be a trace and let\
    \ ⊆ Events() be a (≤ TO,rf ) closed set. Opt has a cycle iff Abs has a cycle.\n\
    \nProof Sketch. Firstly, we show if there is a cycle in Abs , then there is a\
    \ cycle ′ in Opt . For every edge → in , if it is a forward edge, then we replace\
    \ it with the corresponding forward path from to . If → is a backward edge, then\
    \ we keep as it is. After this substitution, we get the replaced as a cycle ′\
    \ in Opt .\n\nIf there is a cycle ′ in Opt , then there is a cycle in Abs . Considering\
    \ the edges in ′ , must contain backward edges, otherwise ′ cannot be a cycle.\
    \ Let be the set of backward edges in and be the set of nodes in . We observe\
    \ that is a subset of the vertices in Opt , because the vertex set of Opt is a\
    \ super set over Abs .\n\nTherefore, can be constructed as following. First we\
    \ keep all last release and open acquire event as nodes in ′ . Second we add all\
    \ backward edges in to ′ . Lastly, we replace all forward paths (paths don't contain\
    \ backward edges) in with a direct edge and add them into ′ . We now have successfully\
    \ constructed cycle ′ in Abs . □\n\n#### <span id=\"page-16-0\"></span>B.8 Proof\
    \ of Theorem [4.4](#page-7-5)\n\nTheorem [4.4.](#page-7-5) Assume SETH holds.\
    \ Given an arbitrary trace , the problem of determining if has an OSR race cannot\
    \ be solved in time (N2− ) (where N = |Events()|) for every > 0.\n\nOrthogonal\
    \ Vector Hypothesis (OV). The Orthogonal Vectors problem is defined as following.\
    \ Given two sets, each containing <sup>N</sup> -dimensional 0-1 vectors, where\
    \ <sup>=</sup> e((N )), determine if there exists two vectors <sup>1</sup> ∈ ,\
    \ <sup>2</sup> ∈ , s.t. (1, 2) has an inner product of zero. OV Hypothesis is\
    \ a well-known conjecture and it has been widely accepted that it's not likely\
    \ to give a sub-quadratic\n\n<span id=\"page-16-1\"></span>![](_page_16_Figure_16.jpeg)\n\
    \nFigure 8: Given two sets , of vectors of length 2, our construction to show\
    \ quadratic hardness of OSR race detection\n\nalgorithm to solve the Orthogonal\
    \ Vector problem [\\[20,](#page-11-47) [36\\]](#page-11-29), i.e. OV Hypothesis\
    \ states OV Problem has a lower bound of e(N )<sup>2</sup> .\n\nWe now reduce\
    \ the existence problem of OSR race to the OV problem and show that the problem\
    \ of determining if there is a OSR race in also has a lower bound of e(N )<sup>2</sup>\
    \ , unless OV Hypothesis fails.\n\nProof. Given two sets , of N -dimensional 0-1\
    \ vectors, we construct a trace as following (shown in Figure [8\\)](#page-16-1).\
    \ contains two threads , . As , are finite sets, we enumerate elements from ,\
    \ as 1, 2, . . . , 1, 2, .... For an arbitrary vector = (1, ..., ), assuming it\
    \ contains non-zero bits, we use a list (1, ..., ) to denote the index of non-zero\
    \ elements in . For example, vector (0, 1, 0, 1) has non-zero bits [2, 4], as\
    \ its 2nd and 4th bits are 1. We define an event clause associated with vector\
    \ as = acq(ℓ<sup>1</sup> ) ◦ · · ·◦acq(ℓ ) ◦w() ◦rel(ℓ ) ◦· · ·◦rel(ℓ<sup>1</sup>\
    \ ). Let = <sup>1</sup> ◦<sup>2</sup> ◦· · ·◦ <sup>N</sup> and = <sup>1</sup>\
    \ ◦<sup>2</sup> ◦ · · · ◦<sup>N</sup> . And we require ∀ ∈ , ′ ∈ , ≤ tr ′ . Then\
    \ we observe a total order ≤ tr on .\n\nNow we show there is a pair of orthogonal\
    \ vectors in , , iff there is a OSR race in . If there is a OSR race (), () in\
    \ , they must correspond to vector ∈ , ∈ . Since ( (), ()) is a data race, they\
    \ must be from different threads and their lock set must be disjoint. Therefore\
    \ ∀ 1 ≤ ≤ , either [] = 0 or [] = 0, thus , are orthogonal.\n\nIf there is a pair\
    \ of orthogonal vector , , then we consider their clause, . Let (), () be the\
    \ two write operations in , and now we show , is a OSR race. For convenience,\
    \ let = OLClosure(w<sup>a</sup> (x), w<sup>b</sup> (x)). The following observations\
    \ hold.\n\n- (1) () ∈ Events(tA) and () ∈ Events(tB), so that ∈ and ∈ .\n- (2)\
    \ = { | ≤ TO prev ( ())} ∪ { | ≤ TO prev ( ())} and ∀ lock ℓ ∈ Locks(S), there\
    \ is at most one open acquire on ℓ, because , are orthogonal, so the\n\nclause\
    \ , don't hold the same lock. This proves is potentially feasible\n\n(3) We guarantee\
    \ has no cycles, as no direct edge is from an acquire event to other events except\
    \ thread order.\n\nFollowing the definition, it's obvious to see () and () is\
    \ a OSR race, and thus we have proved there is a pair of orthogonal vectors in\
    \ , , iff there is a OSR race in . If OV Hypothesis holds, then the problem of\
    \ checking existence of OSR race has a lower bound of e(N )<sup>2</sup> .\n\n\
    □\n\n#### <span id=\"page-17-0\"></span>C EXTRA TABLES FOR SECTION 5\n\nTable\
    \ 3: Statistics of the Java benchmarks. N, T, V, L, Reads, Writes, Acq are the\
    \ number of events, threads, variables, locks, read events, write events and acquire\
    \ events after filtering, respectively.\n\n| Benchmark  | N    | T  | V   | L\
    \   | Reads | Writes | Acq  | Benchmark   | N    | T  | V   | L   | Reads | Writes\
    \ | Acq  |\n|------------|------|----|-----|-----|-------|--------|------|-------------|------|----|-----|-----|-------|--------|------|\n\
    | array      | 11   | 3  | 2   | 1   | 1     | 4      | 2    | critical    | 11\
    \   | 4  | 1   | 0   | 2     | 4      | 0    |\n| account    | 15   | 4  | 1 \
    \  | 0   | 6     | 5      | 0    | airtickets  | 18   | 5  | 1   | 0   | 9   \
    \  | 5      | 0    |\n| pingpong   | 24   | 7  | 2   | 0   | 10    | 8      |\
    \ 0    | twostage    | 83   | 12 | 2   | 2   | 20    | 12     | 20   |\n| wronglock\
    \  | 122  | 22 | 1   | 2   | 40    | 21     | 20   | bbuffer     | 9    | 3  |\
    \ 1   | 0   | 2     | 5      | 0    |\n| prodcons   | 246  | 8  | 3   | 1   |\
    \ 125   | 41     | 34   | clean       | 867  | 8  | 2   | 2   | 286   | 96   \
    \  | 239  |\n| mergesort  | 167  | 5  | 1   | 1   | 55    | 7      | 49   | bubblesort\
    \  | 1.6K | 13 | 25  | 1   | 1.1K  | 263    | 119  |\n| lang       | 1.8K | 7\
    \  | 100 | 0   | 1.3K  | 500    | 0    | readswrites | 10K  | 5  | 6   | 1   |\
    \ 4.2K  | 2.2K   | 1.7K |\n| raytracer  | 526  | 3  | 3   | 0   | 514   | 9  \
    \    | 0    | bufwriter   | 10K  | 6  | 6   | 1   | 5.3K  | 2.2K   | 1.4K |\n\
    | ftpserver  | 17K  | 11 | 135 | 143 | 7.9K  | 0.8K   | 4.2K | moldyn      | 21K\
    \  | 3  | 2   | 0   | 21K   | 68     | 0    |\n| linkedlist | 0.9M | 12 | 932\
    \ | 1   | 0.9M  | 1.9K   | 1.0K | derby       | 75K  | 4  | 190 | 133 | 19K  \
    \ | 12K    | 22K  |\n| jigsaw     | 3.2K | 8  | 51  | 45  | 551   | 498    | 1.1K\
    \ | sunflow     | 3.3K | 17 | 20  | 7   | 2.0K  | 125    | 585  |\n| cryptorsa\
    \  | 1.3M | 7  | 18  | 27  | 709K  | 287K   | 156K | xalan       | 671K | 7  |\
    \ 72  | 138 | 205K  | 99K    | 184K |\n| lufact     | 891K | 5  | 6   | 1   |\
    \ 5.3K  | 2.2K   | 1.4K | batik       | 131  | 7  | 5   | 0   | 115   | 10   \
    \  | 0    |\n| lusearch   | 751K | 8  | 77  | 4   | 751K  | 172    | 53   | tsp\
    \         | 15M  | 10 | 189 | 2   | 15M   | 30K    | 91   |\n| luindex    | 16K\
    \  | 3  | 9   | 4   | 2.6K  | 66     | 6.6K | sor         | 1.9M | 5  | 4   |\
    \ 1   | 633K  | 804    | 633K |\n\n| Table 4: Summarized races and running time\
    \ (in seconds) for RaceInjector traces |  |  |  |  |\n|---------------------------------------------------------------------------------|--|--|--|--|\n\
    |---------------------------------------------------------------------------------|--|--|--|--|\n\
    \n<span id=\"page-19-0\"></span>\n\n| 1                         | 2     | 3  \
    \   | 4     | 5    | 6     | 7    | 8     | 9    | 10    | 11    | 12    | 13\
    \   |\n|---------------------------|-------|-------|-------|------|-------|------|-------|------|-------|-------|-------|------|\n\
    | Benchmark                 | Trace | N     | SHB   |      | WCP   |      | SyncP\
    \ |      | M2    |       | OSR   |      |\n|                           |     \
    \  |       | Races | Time | Races | Time | Races | Time | Races | Time  | Races\
    \ | Time |\n|                           | 43    | 494   | 37    | 0.12 | 28  \
    \  | 0.18 | 37    | 0.21 | 37    | 0.36  | 37    | 0.16 |\n|                 \
    \          | 45    | 494   | 37    | 0.11 | 28    | 0.16 | 37    | 0.22 | 37 \
    \   | 0.34  | 37    | 0.17 |\n|                           | 47    | 494   | 37\
    \    | 0.12 | 28    | 0.18 | 37    | 0.21 | 37    | 0.33  | 37    | 0.15 |\n|\
    \                           | 49    | 494   | 37    | 0.12 | 28    | 0.17 | 37\
    \    | 0.22 | 37    | 0.35  | 37    | 0.16 |\n|                           | 51\
    \    | 494   | 37    | 0.11 | 28    | 0.17 | 37    | 0.19 | 37    | 0.34  | 37\
    \    | 0.16 |\n|                           | 54    | 494   | 37    | 0.12 | 28\
    \    | 0.17 | 37    | 0.2  | 37    | 0.36  | 37    | 0.17 |\n|               \
    \            | 66    | 494   | 37    | 0.12 | 28    | 0.21 | 37    | 0.2  | 37\
    \    | 0.37  | 37    | 0.16 |\n| SHB-missed/ArrayList-27th | 91    | 494   | 37\
    \    | 0.11 | 28    | 0.16 | 37    | 0.21 | 37    | 0.37  | 37    | 0.17 |\n|\
    \                           | 108   | 494   | 44    | 0.11 | 39    | 0.17 | 44\
    \    | 0.21 | 44    | 0.36  | 44    | 0.16 |\n|                           | 109\
    \   | 494   | 44    | 0.11 | 40    | 0.19 | 44    | 0.2  | 44    | 0.36  | 44\
    \    | 0.16 |\n|                           | 115   | 494   | 44    | 0.1  | 40\
    \    | 0.17 | 44    | 0.18 | 44    | 0.36  | 44    | 0.15 |\n|               \
    \            | 118   | 494   | 44    | 0.1  | 40    | 0.16 | 44    | 0.2  | 44\
    \    | 0.36  | 44    | 0.15 |\n|                           | 120   | 494   | 44\
    \    | 0.11 | 40    | 0.18 | 44    | 0.21 | 44    | 0.36  | 44    | 0.15 |\n|\
    \                           | 122   | 494   | 44    | 0.1  | 40    | 0.18 | 44\
    \    | 0.18 | 44    | 0.35  | 44    | 0.15 |\n|                           | 124\
    \   | 494   | 37    | 0.12 | 28    | 0.17 | 37    | 0.21 | 37    | 0.34  | 37\
    \    | 0.16 |\n|                           | 158   | 494   | 37    | 0.11 | 28\
    \    | 0.17 | 37    | 0.22 | 37    | 0.37  | 37    | 0.15 |\n|               \
    \            | 184   | 42461 | 1129  | 0.79 | 739   | 1.09 | 1129  | 1.54 | 1129\
    \  | 50.47 | 1129  | 1.29 |\n|                           | 319   | 42461 | 1129\
    \  | 0.83 | 739   | 1.05 | 1129  | 1.59 | 1129  | 45.16 | 1129  | 1.21 |\n| SHB-missed/Jigsaw-35th\
    \    | 414   | 42461 | 1129  | 0.87 | 739   | 1.08 | 1129  | 1.52 | 1129  | 49.41\
    \ | 1129  | 1.22 |\n|                           | 468   | 42461 | 1129  | 0.8\
    \  | 739   | 1.05 | 1129  | 1.53 | 1129  | 48.49 | 1129  | 1.2  |\n|         \
    \                  | 475   | 42461 | 1129  | 0.87 | 739   | 1.07 | 1129  | 1.57\
    \ | 1129  | 48.01 | 1129  | 1.2  |\n|                           | 484   | 42461\
    \ | 1129  | 0.85 | 739   | 1.05 | 1129  | 1.6  | 1129  | 47.91 | 1129  | 1.23\
    \ |\n|                           | 97    | 635   | 42    | 0.11 | 35    | 0.17\
    \ | 42    | 0.19 | 42    | 0.3   | 42    | 0.14 |\n|                         \
    \  | 98    | 635   | 42    | 0.12 | 34    | 0.17 | 42    | 0.19 | 42    | 0.31\
    \  | 42    | 0.14 |\n|                           | 99    | 635   | 42    | 0.12\
    \ | 35    | 0.16 | 42    | 0.19 | 42    | 0.33  | 42    | 0.14 |\n|          \
    \                 | 100   | 635   | 42    | 0.13 | 34    | 0.17 | 42    | 0.21\
    \ | 42    | 0.33  | 42    | 0.14 |\n|                           | 101   | 635\
    \   | 42    | 0.12 | 35    | 0.17 | 42    | 0.22 | 42    | 0.32  | 42    | 0.14\
    \ |\n|                           | 102   | 635   | 42    | 0.12 | 34    | 0.18\
    \ | 42    | 0.2  | 42    | 0.31  | 42    | 0.14 |\n|                         \
    \  | 105   | 635   | 42    | 0.11 | 33    | 0.17 | 42    | 0.21 | 42    | 0.31\
    \  | 42    | 0.14 |\n|                           | 107   | 635   | 42    | 0.12\
    \ | 33    | 0.18 | 42    | 0.2  | 42    | 0.32  | 42    | 0.15 |\n|          \
    \                 | 109   | 635   | 42    | 0.12 | 33    | 0.18 | 42    | 0.19\
    \ | 42    | 0.33  | 42    | 0.14 |\n|                           | 111   | 635\
    \   | 42    | 0.12 | 34    | 0.17 | 42    | 0.19 | 42    | 0.31  | 42    | 0.14\
    \ |\n|                           | 113   | 635   | 42    | 0.12 | 34    | 0.18\
    \ | 42    | 0.21 | 42    | 0.31  | 42    | 0.14 |\n|                         \
    \  | 115   | 635   | 42    | 0.12 | 34    | 0.19 | 42    | 0.2  | 42    | 0.31\
    \  | 42    | 0.19 |\n|                           | 117   | 635   | 42    | 0.11\
    \ | 34    | 0.18 | 42    | 0.18 | 42    | 0.37  | 42    | 0.15 |\n|          \
    \                 | 119   | 635   | 42    | 0.12 | 34    | 0.17 | 42    | 0.21\
    \ | 42    | 0.36  | 42    | 0.14 |\n|                           | 120   | 635\
    \   | 42    | 0.12 | 35    | 0.17 | 42    | 0.19 | 42    | 0.31  | 42    | 0.14\
    \ |\n|                           | 121   | 635   | 42    | 0.11 | 34    | 0.18\
    \ | 42    | 0.21 | 42    | 0.31  | 42    | 0.14 |\n|                         \
    \  | 122   | 635   | 42    | 0.11 | 35    | 0.16 | 42    | 0.2  | 42    | 0.32\
    \  | 42    | 0.14 |\n|                           | 123   | 635   | 42    | 0.11\
    \ | 35    | 0.18 | 42    | 0.2  | 42    | 0.3   | 42    | 0.15 |\n|          \
    \                 | 126   | 635   | 42    | 0.12 | 35    | 0.17 | 42    | 0.2\
    \  | 42    | 0.31  | 42    | 0.14 |\n|                           | 127   | 635\
    \   | 42    | 0.11 | 34    | 0.17 | 42    | 0.18 | 42    | 0.32  | 42    | 0.14\
    \ |\n| SHB-missed/TreeSet-22th   | 128   | 635   | 42    | 0.12 | 35    | 0.18\
    \ | 42    | 0.2  | 42    | 0.32  | 42    | 0.15 |\n|                         \
    \  | 129   | 635   | 42    | 0.13 | 34    | 0.17 | 42    | 0.2  | 42    | 0.32\
    \  | 42    | 0.14 |\n|                           | 130   | 635   | 42    | 0.12\
    \ | 35    | 0.18 | 42    | 0.21 | 42    | 0.3   | 42    | 0.15 |\n|          \
    \                 | 131   | 635   | 42    | 0.12 | 34    | 0.18 | 42    | 0.19\
    \ | 42    | 0.32  | 42    | 0.14 |\n|                           | 132   | 635\
    \   | 42    | 0.13 | 35    | 0.17 | 42    | 0.2  | 42    | 0.31  | 42    | 0.14\
    \ |\n\n| 1                           | 2     | 3     | 4     | 5    | 6     |\
    \ 7    | 8     | 9    | 10    | 11    | 12    | 13   |\n|-----------------------------|-------|-------|-------|------|-------|------|-------|------|-------|-------|-------|------|\n\
    | Benchmark                   | Trace | N     | SHB   |      | WCP   |      |\
    \ SyncP |      | M2    |       | OSR   |      |\n|                           \
    \  |       |       | Races | Time | Races | Time | Races | Time | Races | Time\
    \  | Races | Time |\n|                             | 133   | 635   | 42    | 0.13\
    \ | 35    | 0.17 | 42    | 0.2  | 42    | 0.32  | 42    | 0.14 |\n|          \
    \                   | 134   | 635   | 42    | 0.12 | 35    | 0.18 | 42    | 0.21\
    \ | 42    | 0.31  | 42    | 0.15 |\n|                             | 135   | 635\
    \   | 42    | 0.11 | 35    | 0.18 | 42    | 0.19 | 42    | 0.32  | 42    | 0.15\
    \ |\n|                             | 136   | 635   | 42    | 0.12 | 35    | 0.19\
    \ | 42    | 0.2  | 42    | 0.33  | 42    | 0.14 |\n|                         \
    \    | 137   | 635   | 42    | 0.11 | 35    | 0.17 | 42    | 0.19 | 42    | 0.33\
    \  | 42    | 0.14 |\n|                             | 138   | 635   | 42    | 0.12\
    \ | 35    | 0.17 | 42    | 0.21 | 42    | 0.31  | 42    | 0.15 |\n|          \
    \                   | 139   | 635   | 42    | 0.12 | 35    | 0.18 | 42    | 0.17\
    \ | 42    | 0.3   | 42    | 0.15 |\n|                             | 140   | 635\
    \   | 42    | 0.12 | 35    | 0.19 | 42    | 0.2  | 42    | 0.32  | 42    | 0.14\
    \ |\n|                             | 141   | 635   | 42    | 0.13 | 35    | 0.19\
    \ | 42    | 0.19 | 42    | 0.3   | 42    | 0.14 |\n|                         \
    \    | 142   | 635   | 42    | 0.12 | 35    | 0.18 | 42    | 0.19 | 42    | 0.32\
    \  | 42    | 0.15 |\n|                             | 143   | 635   | 42    | 0.12\
    \ | 35    | 0.17 | 42    | 0.19 | 42    | 0.32  | 42    | 0.15 |\n|          \
    \                   | 144   | 635   | 42    | 0.11 | 35    | 0.16 | 42    | 0.2\
    \  | 42    | 0.31  | 42    | 0.14 |\n|                             | 145   | 635\
    \   | 42    | 0.12 | 35    | 0.17 | 42    | 0.21 | 42    | 0.32  | 42    | 0.15\
    \ |\n|                             | 149   | 635   | 42    | 0.11 | 35    | 0.17\
    \ | 42    | 0.19 | 42    | 0.32  | 42    | 0.15 |\n|                         \
    \    | 150   | 635   | 42    | 0.12 | 34    | 0.17 | 42    | 0.22 | 42    | 0.31\
    \  | 42    | 0.14 |\n|                             | 151   | 635   | 42    | 0.13\
    \ | 35    | 0.2  | 42    | 0.19 | 42    | 0.32  | 42    | 0.14 |\n|          \
    \                   | 98    | 635   | 42    | 0.12 | 34    | 0.17 | 42    | 0.19\
    \ | 42    | 0.33  | 42    | 0.16 |\n|                             | 100   | 635\
    \   | 42    | 0.12 | 34    | 0.17 | 42    | 0.2  | 42    | 0.31  | 42    | 0.16\
    \ |\n|                             | 102   | 635   | 42    | 0.12 | 34    | 0.17\
    \ | 42    | 0.2  | 42    | 0.32  | 42    | 0.15 |\n|                         \
    \    | 109   | 635   | 42    | 0.11 | 33    | 0.18 | 42    | 0.19 | 42    | 0.33\
    \  | 42    | 0.16 |\n|                             | 111   | 635   | 42    | 0.12\
    \ | 34    | 0.18 | 42    | 0.2  | 42    | 0.3   | 42    | 0.16 |\n|          \
    \                   | 113   | 635   | 42    | 0.12 | 34    | 0.17 | 42    | 0.2\
    \  | 42    | 0.32  | 42    | 0.16 |\n|                             | 115   | 635\
    \   | 42    | 0.12 | 34    | 0.17 | 42    | 0.2  | 42    | 0.32  | 42    | 0.16\
    \ |\n|                             | 117   | 635   | 42    | 0.12 | 34    | 0.17\
    \ | 42    | 0.2  | 42    | 0.34  | 42    | 0.16 |\n|                         \
    \    | 119   | 635   | 42    | 0.12 | 34    | 0.16 | 42    | 0.19 | 42    | 0.33\
    \  | 42    | 0.16 |\n|                             | 121   | 635   | 42    | 0.12\
    \ | 34    | 0.17 | 42    | 0.19 | 42    | 0.32  | 42    | 0.16 |\n| WCP-missed/TreeSet-22th\
    \     | 123   | 635   | 42    | 0.11 | 35    | 0.18 | 42    | 0.2  | 42    | 0.31\
    \  | 42    | 0.16 |\n|                             | 127   | 635   | 42    | 0.12\
    \ | 34    | 0.18 | 42    | 0.21 | 42    | 0.32  | 42    | 0.16 |\n|          \
    \                   | 129   | 635   | 42    | 0.11 | 34    | 0.17 | 42    | 0.2\
    \  | 42    | 0.32  | 42    | 0.15 |\n|                             | 131   | 635\
    \   | 42    | 0.12 | 34    | 0.18 | 42    | 0.18 | 42    | 0.34  | 42    | 0.15\
    \ |\n|                             | 133   | 635   | 42    | 0.12 | 35    | 0.17\
    \ | 42    | 0.2  | 42    | 0.33  | 42    | 0.16 |\n|                         \
    \    | 135   | 635   | 42    | 0.12 | 35    | 0.18 | 42    | 0.19 | 42    | 0.31\
    \  | 42    | 0.17 |\n|                             | 137   | 635   | 42    | 0.12\
    \ | 35    | 0.16 | 42    | 0.2  | 42    | 0.32  | 42    | 0.16 |\n|          \
    \                   | 139   | 635   | 42    | 0.11 | 35    | 0.19 | 42    | 0.19\
    \ | 42    | 0.3   | 42    | 0.15 |\n|                             | 141   | 635\
    \   | 42    | 0.11 | 35    | 0.2  | 42    | 0.21 | 42    | 0.31  | 42    | 0.15\
    \ |\n|                             | 143   | 635   | 42    | 0.12 | 35    | 0.19\
    \ | 42    | 0.18 | 42    | 0.31  | 42    | 0.15 |\n|                         \
    \    | 145   | 635   | 42    | 0.12 | 35    | 0.17 | 42    | 0.19 | 42    | 0.33\
    \  | 42    | 0.16 |\n|                             | 109   | 494   | 44    | 0.11\
    \ | 40    | 0.16 | 44    | 0.2  | 44    | 0.39  | 44    | 0.16 |\n|          \
    \                   | 118   | 494   | 44    | 0.11 | 40    | 0.16 | 44    | 0.19\
    \ | 44    | 0.37  | 44    | 0.16 |\n| SyncP-missed/ArrayList-27th | 120   | 494\
    \   | 44    | 0.1  | 40    | 0.16 | 44    | 0.2  | 44    | 1.21  | 44    | 0.14\
    \ |\n|                             | 122   | 494   | 44    | 0.12 | 40    | 0.17\
    \ | 44    | 0.19 | 44    | 0.38  | 44    | 0.14 |\n|                         \
    \    | 219   | 42461 | 1129  | 0.85 | 739   | 1.08 | 1129  | 1.47 | 1129  | 48.19\
    \ | 1129  | 1.51 |\n| SyncP-missed/Jigsaw-35th    | 475   | 42461 | 1129  | 0.81\
    \ | 739   | 1.07 | 1129  | 1.57 | 1129  | 48.48 | 1129  | 1.44 |\n|          \
    \                   |       |       |       |      |       |      |       |  \
    \    |       |       |       |      |\n|                             | 484   |\
    \ 42461 | 1129  | 0.8  | 739   | 1.06 | 1129  | 1.41 | 1129  | 47.2  | 1129  |\
    \ 1.43 |\n|                             | 97    | 635   | 42    | 0.12 | 35  \
    \  | 0.17 | 42    | 0.2  | 42    | 0.36  | 42    | 0.15 |\n|                 \
    \            | 99    | 635   | 42    | 0.11 | 35    | 0.16 | 42    | 0.19 | 42\
    \    | 0.36  | 42    | 0.15 |\n|                             | 101   | 635   |\
    \ 42    | 0.12 | 35    | 0.18 | 42    | 0.2  | 42    | 0.36  | 42    | 0.16 |\n\
    |                             | 120   | 635   | 42    | 0.11 | 35    | 0.17 |\
    \ 42    | 0.19 | 42    | 0.36  | 42    | 0.17 |\n\nICSE '24, April 12–21, 2024,\
    \ Lisbon, Portugal Zheng Shi, Umang Mathur, and Andreas Pavlogiannis\n\n| 1  \
    \       | 2     | 3   | 4     | 5    | 6     | 7    | 8     | 9    | 10    | 11\
    \   | 12    | 13   |\n|-----------|-------|-----|-------|------|-------|------|-------|------|-------|------|-------|------|\n\
    | Benchmark | Trace | N   | SHB   |      | WCP   |      | SyncP |      | M2  \
    \  |      | OSR   |      |\n|           |       |     | Races | Time | Races |\
    \ Time | Races | Time | Races | Time | Races | Time |\n|           | 122   | 635\
    \ | 42    | 0.12 | 35    | 0.17 | 42    | 0.2  | 42    | 0.35 | 42    | 0.14 |\n\
    |           | 126   | 635 | 42    | 0.13 | 35    | 0.17 | 42    | 0.2  | 42  \
    \  | 0.37 | 42    | 0.16 |\n|           | 128   | 635 | 42    | 0.13 | 35    |\
    \ 0.19 | 42    | 0.2  | 42    | 0.37 | 42    | 0.16 |\n|           | 130   | 635\
    \ | 42    | 0.13 | 35    | 0.18 | 42    | 0.18 | 42    | 0.38 | 42    | 0.15 |\n\
    |           | 132   | 635 | 42    | 0.12 | 35    | 0.19 | 42    | 0.2  | 42  \
    \  | 0.37 | 42    | 0.15 |\n|           | 134   | 635 | 42    | 0.11 | 35    |\
    \ 0.17 | 42    | 0.2  | 42    | 0.36 | 42    | 0.14 |\n|           | 136   | 635\
    \ | 42    | 0.12 | 35    | 0.17 | 42    | 0.19 | 42    | 0.38 | 42    | 0.16 |\n\
    |           | 138   | 635 | 42    | 0.11 | 35    | 0.16 | 42    | 0.2  | 42  \
    \  | 0.36 | 42    | 0.14 |\n|           | 140   | 635 | 42    | 0.11 | 35    |\
    \ 0.17 | 42    | 0.21 | 42    | 0.36 | 42    | 0.15 |\n|           | 142   | 635\
    \ | 42    | 0.12 | 35    | 0.16 | 42    | 0.19 | 42    | 0.35 | 42    | 0.16 |\n\
    |           | 144   | 635 | 42    | 0.11 | 35    | 0.18 | 42    | 0.19 | 42  \
    \  | 0.35 | 42    | 0.14 |\n\n<span id=\"page-22-0\"></span>Table 5: Details on\
    \ reported races and running time (in minute) by each algorithm on C/C++ benchmarks.\
    \ Column 1-3 states the source of these benchmarks, trace name with number of\
    \ threads and events number after filtering. Columns 4-13 are reported races and\
    \ average running time by each algorithm.\n\n| 1             | 2             \
    \   | 3      | 4      | 5    | 6      | 7    | 8     | 9     | 10    | 11    |\
    \ 12     | 13    |\n|---------------|------------------|--------|--------|------|--------|------|-------|-------|-------|-------|--------|-------|\n\
    |               | Benchmark        | N      | SHB    |      | WCP    |      |\
    \ SyncP |       | M2    |       | OSR    |       |\n| Benchmark Set |        \
    \          |        | Races  | Time | Races  | Time | Races | Time  | Races |\
    \ Time  | Races  | Time  |\n|               | task-16th        | 117M   | 6267\
    \   | 5.5  | 5669   | 14.6 | 1     | 180.0 | 0     | 180.0 | 11757  | 6.5   |\n\
    |               | task-56th        | 117M   | 6267   | 6.1  | 5669   | 14.5 |\
    \ 1     | 180.0 | 0     | 180.0 | 11757  | 5.9   |\n|               | taskdeps-16th\
    \    | 115M   | 5627   | 2.2  | 5190   | 6.5  | 14    | 180.0 | 13    | 180.0\
    \ | 10915  | 3.4   |\n| CoMD          | taskdeps-56th    | 117M   | 6267   | 5.0\
    \  | 5669   | 13.9 | 1     | 180.0 | 0     | 180.0 | 11757  | 5.5   |\n|     \
    \          | taskloop-16th    | 2M     | 257    | 0.1  | 168766 | 0.1  | 0   \
    \  | 180.0 | 474   | 180.0 | 177566 | 0.1   |\n|               | taskloop-56th\
    \    | 4M     | 4982   | 0.1  | 44977  | 0.2  | 0     | 180.0 | 186   | 180.0\
    \ | 194160 | 1.0   |\n|               | openmp-16th      | 115M   | 5627   | 3.3\
    \  | 5190   | 6.8  | 14    | 180.0 | 13    | 180.0 | 10915  | 3.2   |\n|     \
    \          | openmp-56th      | 117M   | 6267   | 5.6  | 5669   | 14.3 | 1   \
    \  | 180.0 | 0     | 180.0 | 11757  | 6.9   |\n| SimpleMOC     | trace-16th  \
    \     | 19M    | 380    | 0.2  | 388    | 23.1 | 32    | 180.0 | 32    | 180.0\
    \ | 32     | 180.0 |\n|               | Amg2013-18th     | 39M    | 140541 | 0.4\
    \  | 107793 | 3.0  | 103   | 180.0 | 102   | 180.0 | 145485 | 0.7   |\n|     \
    \          | Amg2013-58th     | 52M    | 181018 | 1.9  | 133280 | 7.4  | 70  \
    \  | 180.0 | 0     | 180.0 | 190994 | 4.4   |\n|               | Kripke-16th \
    \     | 20M    | 14162  | 0.1  | 44     | 1.4  | 46    | 180.0 | 259   | 180.0\
    \ | 22481  | 0.3   |\n|               | Kripke-56th      | 34M    | 20824  | 1.2\
    \  | 128    | 5.5  | 39    | 180.0 | 12    | 180.0 | 34,155 | 9.8   |\n|     \
    \          | Lulesh-16th      | 10M    | 51621  | 0.3  | 27472  | 0.4  | 1312\
    \  | 180.0 | 620   | 180.0 | 52940  | 0.1   |\n|               | Lulesh-16th \
    \     | 130M   | 158081 | 3.0  | 103306 | 8.8  | 5     | 180.0 | 13    | 180.0\
    \ | 167595 | 4.5   |\n|               | Lulesh-56th      | 14M    | 71676  | 0.3\
    \  | 40322  | 2.7  | 1     | 180.0 | 70    | 180.0 | 73432  | 0.1   |\n| OMPRacer\
    \      | Lulesh-56th      | 156M   | 250954 | 7.8  | 157756 | 27.6 | 1     | 180.0\
    \ | 0     | 180.0 | 261857 | 10.7  |\n|               | miniFE-18th      | 44M\
    \    | 148645 | 0.6  | 51478  | 2.5  | 121   | 180.0 | 77    | 180.0 | 159052\
    \ | 0.3   |\n|               | miniFE-58th      | 63M    | 171460 | 2.7  | 74252\
    \  | 10.6 | 77    | 180.0 | 0     | 180.0 | 191862 | 4.0   |\n|              \
    \ | QuickSilver-56th | 1M     | 20753  | 0.1  | 7288   | 0.1  | 8     | 180.0\
    \ | 610   | 180.0 | 21132  | 0.2   |\n|               | XSBench-16th     | 693.9K\
    \ | 27     | 0.1  | 30     | 0.1  | 221   | 0.9   | 222   | 3.6   | 225    | 0.1\
    \   |\n|               | XSBench-56th     | 710.9K | 89     | 0.1  | 117    |\
    \ 0.1  | 15    | 180.0 | 361   | 17.6  | 370    | 0.1   |\n|               | RSBench-16th\
    \     | 27M    | 22     | 0.3  | 35     | 0.6  | 1271  | 43.7  | 199   | 180.0\
    \ | 1278   | 0.2   |\n|               | RSBench-56th     | 27M    | 95     | 0.1\
    \  | 114    | 1.8  | 0     | 180.0 | 30    | 180.0 | 1405   | 0.3   |\n|     \
    \          | DRACC-009        | 70M    | 16     | 0.2  | 16     | 4.1  | 31  \
    \  | 180.0 | 31    | 142.1 | 32     | 3.3   |\n|               | DRACC-010   \
    \     | 70M    | 16     | 0.4  | 16     | 4.0  | 31    | 180.0 | 31    | 144.4\
    \ | 32     | 2.7   |\n|               | DRACC-011        | 0.5K   | 15     | 0.1\
    \  | 15     | 0.1  | 30    | 0.1   | 30    | 0.1   | 30     | 0.1   |\n|     \
    \          | DRACC-012        | 103M   | 527    | 1.7  | 527    | 16.0 | 542 \
    \  | 180.0 | 22    | 180.0 | 543    | 13.5  |\n|               | DRACC-013   \
    \     | 103M   | 527    | 1.7  | 527    | 22.7 | 542   | 180.0 | 22    | 180.0\
    \ | 543    | 13.7  |\n|               | DRACC-014        | 0.5K   | 15     | 0.1\
    \  | 15     | 0.1  | 30    | 0.1   | 30    | 0.1   | 30     | 0.1   |\n|     \
    \          | DRACC-015        | 70M    | 16     | 0.6  | 16     | 3.7  | 31  \
    \  | 180.0 | 31    | 145.3 | 32     | 3.8   |\n| DRACC-16th    | DRACC-016   \
    \     | 70M    | 16     | 0.7  | 16     | 4.5  | 31    | 180.0 | 31    | 144.9\
    \ | 32     | 4.2   |\n|               | DRACC-017        | 0.5K   | 15     | 0.1\
    \  | 15     | 0.1  | 30    | 0.1   | 30    | 0.1   | 30     | 0.1   |\n|     \
    \          |                  | 0.5K   | 15     | 0.1  | 15     | 0.1  | 30  \
    \  | 0.1   | 30    | 0.1   | 30     | 0.1   |\n|               | DRACC-018   \
    \     | 103M   | 527    | 1.5  | 527    | 19.3 | 542   | 180.0 | 21    | 180.0\
    \ | 543    | 12.4  |\n|               | DRACC-019        | 103M   | 527    | 1.4\
    \  | 527    | 15.8 | 542   | 180.0 | 22    | 180.0 | 543    | 12.5  |\n|     \
    \          | DRACC-020        | 0.5K   | 15     | 0.1  | 15     | 0.1  | 30  \
    \  | 0.1   | 30    | 0.1   | 30     | 0.1   |\n|               | DRB-062     \
    \     | 70M    | 31     | 0.8  | 31     | 4.4  | 46    | 4.7   | 45    | 180.0\
    \ | 46     | 1.4   |\n|               | DRB-105          | 44M    | 866    | 0.4\
    \  | 874    | 2.6  | 46    | 180.0 | 101   | 180.0 | 889    | 7.0   |\n|     \
    \          | DRB-106          | 70M    | 709    | 0.7  | 732    | 4.9  | 46  \
    \  | 180.0 | 46    | 180.0 | 789    | 6.8   |\n|               | DRB-110     \
    \     | 35M    | 15     | 0.3  | 16     | 1.1  | 32    | 180.0 | 32    | 47.8\
    \  | 32     | 6.9   |\n|               | DRB-122          | 0.5K   | 15     |\
    \ 0.1  | 15     | 0.1  | 30    | 0.1   | 30    | 0.1   | 30     | 0.1   |\n| \
    \              | DRB-123          | 77M    | 227    | 0.5  | 713    | 4.8  | 231\
    \   | 180.0 | 46    | 180.0 | 243    | 3.7   |\n|               | DRB-144    \
    \      | 70M    | 16     | 0.7  | 16     | 4.1  | 30    | 180.0 | 30    | 180.0\
    \ | 30     | 180.0 |\n|               | DRB-148          | 70M    | 16     | 0.4\
    \  | 16     | 5.9  | 31    | 180.0 | 31    | 125.9 | 32     | 3.7   |\n\n| 1 \
    \            | 2                                    | 3            | 4       \
    \  | 5           | 6         | 7           | 8         | 9              | 10 \
    \     | 11             | 12              | 13          |\n|---------------|--------------------------------------|--------------|-----------|-------------|-----------|-------------|-----------|----------------|---------|----------------|-----------------|-------------|\n\
    |               | Benchmark<br>N                       |              | SHB  \
    \     |             | WCP       |             | SyncP     |                | M2\
    \      |                | OSR             |             |\n| Benchmark Set | \
    \                                     |              | Races     | Time      \
    \  | Races     | Time        | Races     | Time           | Races   | Time   \
    \        | Races           | Time        |\n|               | DRB-150        \
    \                      | 56M          | 16        | 0.4         | 16        |\
    \ 2.8         | 31        | 180.0          | 31      | 104.4          | 32   \
    \           | 3.3         |\n|               | DRB-152                       \
    \       | 56M          | 16        | 0.3         | 16        | 4.3         | 31\
    \        | 180.0          | 31      | 105.5          | 32              | 3.1 \
    \        |\n|               | DRB-154                              | 0.5K    \
    \     | 15        | 0.1         | 15        | 0.1         | 30        | 0.1  \
    \          | 30      | 0.1            | 30              | 0.1         |\n|   \
    \            | DRB-155                              | 12M          | 17      \
    \  | 0.1         | 18        | 0.6         | 32        | 132.1          | 36 \
    \     | 13.6           | 36              | 0.1         |\n|               | DRB-176\
    \                              | 47M          | 1697      | 0.5         | 1899\
    \      | 2.1         | 51        | 180.0          | 77      | 180.0          |\
    \ 2079            | 7.0         |\n|               | DRB-176                 \
    \             | 272M         | 1835      | 7.2         | 2005      | 20.7    \
    \    | 51        | 180.0          | 0       | 180.0          | 2209          \
    \  | 61.8        |\n|               | DRB-176                              | 782M\
    \         | 2154      | 19.7        | 2385      | 61.9        | 51        | 180.0\
    \          | 0       | 180.0          | 2611            | 175.4       |\n|   \
    \            | DRB-177                              | 45M          | 1204    \
    \  | 0.5         | 1238      | 2.6         | 54        | 180.0          | 78 \
    \     | 180.0          | 1315            | 6.0         |\n|               | DRB-177\
    \                              | 191M         | 1395      | 4.5         | 1424\
    \      | 18.5        | 55        | 180.0          | 0       | 180.0          |\
    \ 1522            | 30.3        |\n|               | DRB-177                 \
    \             | 106M         | 982       | 1.9         | 999       | 7.4     \
    \    | 53        | 180.0          | 13      | 180.0          | 1080          \
    \  | 13.5        |\n|               | DRB-177                              | 519M\
    \         | 1634      | 13.6        | 1685      | 42.0        | 54        | 180.0\
    \          | 0       | 180.0          | 1799            | 80.5        |\n|   \
    \            | DRB-177                              | 333M         | 1704    \
    \  | 8.4         | 1771      | 26.4        | 54        | 180.0          | 0  \
    \     | 180.0          | 1881            | 52.9        |\n|               | DRB-177\
    \                              | 836M         | 1791      | 22.8        | 1857\
    \      | 71.7        | 53        | 180.0          | 0       | 180.0          |\
    \ 1963            | 160.9       |\n|               | DRB-062                 \
    \             | 72M          | 111       | 3.0         | 111       | 10.8    \
    \    | 42        | 180.0          | 0       | 180.0          | 166           \
    \  | 1.8         |\n|               | DRB-105                              | 46M\
    \          | 2778      | 1.4         | 2793      | 6.0         | 42        | 180.0\
    \          | 0       | 180.0          | 2849            | 19.2        |\n|   \
    \            | DRB-106                              | 68M          | 2042    \
    \  | 2.2         | 2086      | 9.4         | 42        | 180.0          | 0  \
    \     | 180.0          | 2284            | 11.7        |\n|               | DRB-110\
    \                              | 35M          | 55        | 0.5         | 56 \
    \       | 5.4         | 42        | 180.0          | 15      | 180.0         \
    \ | 112             | 1.3         |\n|               | DRB-122               \
    \               | 1.8K         | 55        | 0.1         | 55        | 0.1   \
    \      | 43        | 180.0          | 110     | 0.1            | 110         \
    \    | 0.1         |\n| DRB-56th      | DRB-123                              |\
    \ 77M          | 712       | 2.5         | 228       | 10.4        | 42      \
    \  | 180.0          | 0       | 180.0          | 778             | 9.5       \
    \  |\n|               | DRB-155                              | 12M          |\
    \ 58        | 0.2         | 59        | 0.7         | 38        | 180.0      \
    \    | 94      | 180.0          | 126             | 1.1         |\n|         \
    \      | DRB-176                              | 49M          | 5216      | 1.6\
    \         | 5989      | 7.6         | 25        | 180.0          | 0       | 180.0\
    \          | 6802            | 15.1        |\n|               | DRB-176      \
    \                        | 348M         | 6911      | 16.4        | 7985     \
    \ | 70.3        | 24        | 180.0          | 0       | 180.0          | 8174\
    \            | 180.0       |\n|               | DRB-176                      \
    \        | 900M         | 7854      | 43.2        | 9110      | 169.8       |\
    \ 25        | 180.0          | 0       | 180.0          | 5026            | 180.0\
    \       |\n|               | DRB-177                              | 43M      \
    \    | 3138      | 1.2         | 3115      | 5.9         | 4         | 180.0 \
    \         | 0       | 180.0          | 3421            | 13.3        |\n|    \
    \           | DRB-177                              | 326M         | 5131     \
    \ | 15.0        | 5197      | 57.3        | 3         | 180.0          | 0   \
    \    | 180.0          | 5541            | 166.8       |\n|               | graph500-16th\
    \                        | 81M          | 23732     | 1.1         | 14764    \
    \ | 4.5         | 40        | 180.0          | 30      | 180.0          | 113732\
    \          | 3.2         |\n|               | graph500-56th                  \
    \      | 82M          | 38416     | 3.5         | 17050     | 12.0        | 38\
    \        | 180.0          | 0       | 180.0          | 119601          | 7.8 \
    \        |\n|               | HPCCG-16th                           | 55M     \
    \     | 9531      | 0.9         | 4480      | 3.7         | 34        | 180.0\
    \          | 60      | 180.0          | 9547            | 2.2         |\n|   \
    \            | HPCCG-56th                           | 79M          | 15027   \
    \  | 3.4         | 228       | 11.9        | 40        | 180.0          | 0  \
    \     | 180.0          | 15083           | 3.5         |\n|               | DC.S-16th\
    \                            | 1.0K         | 78        | 0.1         | 34   \
    \     | 0.1         | 97        | 0.2            | 98      | 0.1            |\
    \ 99              | 0.1         |\n|               | DC.S-56th               \
    \             | 19.8K        | 570       | 0.1         | 153       | 0.1     \
    \    | 60        | 180.0          | 629     | 0.2            | 629           \
    \  | 0.1         |\n|               | IS.W-16th                            | 48M\
    \          | 64155     | 0.8         | 30642     | 2.3         | 31        | 180.0\
    \          | 75      | 180.0          | 64597           | 2.3         |\n|   \
    \            | IS.W-56th                            | 140M         | 193236  \
    \  | 9.4         | 128441    | 28.8        | 38        | 180.0          | 0  \
    \     | 180.0          | 202142          | 23.7        |\n|               | loopA.bad-16th\
    \                       | 93M          | 30        | 2.0         | 30        |\
    \ 7.5         | 1         | 180.0          | 28      | 180.0          | 150033\
    \          | 2.4         |\n|               | loopA.bad-56th                 \
    \      | 334M         | 118       | 26.4        | 119       | 87.1        | 0\
    \         | 180.0          | 0       | 180.0          | 550125          | 21.0\
    \        |\n|               | loopA.solu1-16th                     | 93M     \
    \     | 31        | 1.9         | 31        | 7.5         | 1         | 180.0\
    \          | 28      | 180.0          | 150049          | 2.0         |\n|   \
    \            | loopA.solu1-56th                     | 334M         | 116     \
    \  | 23.5        | 117       | 94.5        | 0         | 180.0          | 40 \
    \     | 180.0          | 550181          | 23.0        |\n|               | loopA.solu2-16th\
    \                     | 51M          | 31        | 0.4         | 31        | 2.2\
    \         | 15        | 180.0          | 0       | 180.0          | 10049    \
    \       | 0.6         |\n|               | loopA.solu2-56th                  \
    \   | 171M         | 118       | 7.9         | 117       | 27.9        | 1   \
    \      | 180.0          | 0       | 180.0          | 10180           | 10.3  \
    \      |\n|               | loopA.solu3-16th                     | 51M       \
    \   | 31        | 1.0         | 30        | 3.0         | 16        | 180.0  \
    \        | 80      | 180.0          | 10048           | 0.6         |\n|     \
    \          | loopA.solu3-56th<br>loopB.solu1-16th | 171M<br>93M  | 116<br>30 |\
    \ 8.7<br>1.9  | 114<br>30 | 24.6<br>7.1 | 1<br>1    | 180.0<br>180.0 | 0<br>28\
    \ | 180.0<br>180.0 | 10177<br>150037 | 8.8<br>2.1  |\n|               |      \
    \                                |              |           |             |  \
    \         |             |           |                |         |             \
    \   |                 |             |\n|               | loopB.solu1-56th<br>Mandelbrot-16th\
    \  | 334M<br>112M | 113<br>26 | 21.8<br>2.0 | 113<br>33 | 91.8<br>5.9 | 0<br>1968\
    \ | 180.0<br>180.0 | 0<br>13 | 180.0<br>180.0 | 550127<br>1973  | 21.5<br>2.5\
    \ |\n|               | Mandelbrot-56th                      | 114M         | 87\
    \        | 4.6         | 113       | 18.0        | 2         | 180.0         \
    \ | 0       | 180.0          | 2196            | 4.6         |\n|            \
    \   | Pi-16th                              | 96M          | 27        | 1.4  \
    \       | 35        | 5.3         | 48        | 7.5            | 28      | 180.0\
    \          | 53              | 2.5         |\n|               | Pi-56th      \
    \                        | 99M          | 91        | 4.2         | 115      \
    \ | 14.9        | 37        | 180.0          | 0       | 180.0          | 184\
    \             | 6.1         |\n|               | QuickSort-16th              \
    \         | 41M          | 31752     | 0.4         | 31758     | 1.5         |\
    \ 2         | 180.0          | 115     | 180.0          | 91092           | 2.4\
    \         |\n| HPCBench      |                                      |        \
    \      |           |             |           |             |           |     \
    \           |         |                |                 |             |\n\n|\
    \ 1             | 2                | 3      | 4       | 5    | 6       | 7   \
    \  | 8     | 9     | 10    | 11    | 12       | 13    |\n|---------------|------------------|--------|---------|------|---------|-------|-------|-------|-------|-------|----------|-------|\n\
    |               | Benchmark        |        | SHB     |      | WCP     |     \
    \  | SyncP |       | M2    |       | OSR      |       |\n| Benchmark Set |   \
    \               | N      | Races   | Time | Races   | Time  | Races | Time  |\
    \ Races | Time  | Races    | Time  |\n|               | QuickSort-56th   | 41M\
    \    | 31792   | 1.0  | 31798   | 6.8   | 0     | 180.0 | 2     | 180.0 | 91172\
    \    | 3.7   |\n|               | fft6-16th        | 0.9K   | 30      | 0.1  |\
    \ 31      | 0.1   | 78    | 0.2   | 81    | 0.1   | 81       | 0.1   |\n|    \
    \           | fft6-56th        | 2.4K   | 74      | 0.1  | 74      | 0.1   | 27\
    \    | 180.0 | 172   | 0.1   | 172      | 0.1   |\n|               | LUReduction-16th\
    \ | 45M    | 89100   | 0.9  | 32      | 2.1   | 2     | 180.0 | 32    | 180.0\
    \ | 89116    | 3.5   |\n|               | LUReduction-56th | 45M    | 88766  \
    \ | 1.5  | 112     | 7.9   | 0     | 180.0 | 0     | 180.0 | 89209    | 11.2 \
    \ |\n|               | MD-16th          | 118M   | 1499    | 2.9  | 59      |\
    \ 7.9   | 1512  | 180.0 | 13    | 180.0 | 1515     | 4.5   |\n|              \
    \ | MD-56th          | 120M   | 1683    | 5.9  | 178     | 15.5  | 3     | 180.0\
    \ | 0     | 180.0 | 1747     | 8.2   |\n|               | testPath-16th    | 7M\
    \     | 16      | 0.1  | 16      | 0.2   | 124   | 180.0 | 154   | 81.7  | 154\
    \      | 49.0  |\n|               | testPath-56th    | 10M    | 57      | 0.2\
    \  | 57      | 0.3   | 1     | 180.0 | 44    | 180.0 | 544      | 180.0 |\n| \
    \              | fft-16th         | 78M    | 983086  | 2.0  | 983086  | 5.7  \
    \ | 98599 | 180.0 | 30    | 180.0 | 2424886  | 4.1   |\n|               | fft-56th\
    \         | 83M    | 1030016 | 4.5  | 1030016 | 14.3  | 37    | 180.0 | 0    \
    \ | 180.0 | 2565429  | 13.7  |\n|               | fft-56th         | 363M   |\
    \ 3894868 | 24.2 | 4119572 | 91.0  | 37    | 180.0 | 0     | 180.0 | 10261235\
    \ | 98.5  |\n|               | qsomp1-16th      | 674.6K | 16      | 0.1  | 16\
    \      | 0.1   | 282   | 180.0 | 282   | 37.7  | 282      | 0.1   |\n|       \
    \        | qsomp1-56th      | 540.1K | 56      | 0.1  | 56      | 0.1   | 27 \
    \   | 180.0 | 156   | 79.5  | 156      | 0.3   |\n|               | qsomp2-16th\
    \      | 870.5K | 16      | 0.1  | 16      | 0.1   | 375   | 74.1  | 375   | 41.9\
    \  | 375      | 0.2   |\n|               | qsomp2-56th      | 629.6K | 56    \
    \  | 0.1  | 56      | 0.1   | 15    | 180.0 | 208   | 180.0 | 253      | 0.5 \
    \  |\n|               | qsomp3-16th      | 15M    | 16      | 0.1  | 17      |\
    \ 0.4   | 32    | 123.4 | 32    | 15.3  | 32       | 0.3   |\n|              \
    \ | qsomp3-56th      | 4M     | 56      | 0.1  | 57      | 0.3   | 40    | 180.0\
    \ | 112   | 46.4  | 112      | 0.1   |\n|               | qsomp4-16th      | 19M\
    \    | 17      | 0.1  | 17      | 4.3   | 37    | 160.0 | 33    | 180.0 | 37 \
    \      | 0.5   |\n|               | qsomp4-56th      | 6M     | 5269    | 0.1\
    \  | 5044    | 0.2   | 0     | 180.0 | 113   | 180.0 | 5349     | 19.7  |\n| \
    \              | qsomp6-56th      | 507.2K | 56      | 0.1  | 56      | 0.1  \
    \ | 8     | 180.0 | 139   | 180.0 | 536      | 1.8   |\n|               | qsomp7-16th\
    \      | 44M    | 8016    | 0.6  | 8001    | 2.7   | 6787  | 180.0 | 96    | 180.0\
    \ | 8035     | 0.9   |\n|               | qsomp7-56th      | 147M   | 6053   \
    \ | 6.4  | 6052    | 22.1  | 0     | 180.0 | 0     | 180.0 | 6119     | 20.5 \
    \ |\n|               | biojava-4th      | 0.9K   | 2       | 0.1  | 3       |\
    \ 0.1   | 6     | 0.1   | 6     | 0.1   | 6        | 0.1   |\n|              \
    \ | cassandra-132th  | 28M    | 5053    | 0.2  | 5026    | 180.0 | 0     | 180.0\
    \ | 0     | 180.0 | 514      | 180.0 |\n|               | graphchi-20th    | 206.3K\
    \ | 21      | 0.1  | 21      | 0.1   | 137   | 0.1   | 138   | 0.1   | 138   \
    \   | 0.1   |\n| misc          | hsqldb-44th      | 647.5K | 5       | 0.1  |\
    \ 5       | 0.1   | 3     | 180.0 | 5     | 0.1   | 5        | 2.4   |\n|    \
    \           | tradebeans-222th | 218.9K | 170     | 0.1  | 157     | 0.1   | 0\
    \     | 180.0 | 203   | 40.9  | 205      | 0.1   |\n|               | tradesoap-221th\
    \  | 218.6K | 169     | 0.1  | 155     | 0.1   | 0     | 180.0 | 203   | 44.0\
    \  | 205      | 0.4   |\n|               | zxing-15th       | 18M    | 3128  \
    \  | 0.1  | 3114    | 0.9   | 333   | 180.0 | 340   | 180.0 | 3216     | 0.2 \
    \  |\n\n<span id=\"page-25-0\"></span>Table 6: Details of C/C++ benchmarks. Columns\
    \ 1-2 states the source of these benchmarks and trace name with number of threads.\
    \ Columns 3-6 are number of events before filtering, number of events after filtering,\
    \ number of variables, number of locks. Columns 7-10 are number of read, write,\
    \ acquire and release events after filtering.\n\n| 1             | 2         \
    \       | 3    | 4      | 5       | 6     | 7                     | 8        \
    \   | 9             | 10         |\n|---------------|------------------|------|--------|---------|-------|-----------------------|-------------|---------------|------------|\n\
    | Benchmark Set | Benchmark        | N′   | N      | V       | L     | Reads \
    \                | Writes      | Acq           | Rel        |\n|             \
    \  | task-16th        | 174M | 117M   | 11,757  | 56    | 107,864,829        \
    \   | 9,345,091   | 31,555        | 31,555     |\n|               | task-56th\
    \        | 175M | 117M   | 11,757  | 56    | 107,864,829           | 9,345,091\
    \   | 31,555        | 31,555     |\n|               | taskdeps-16th    | 174M\
    \ | 115M   | 10,915  | 16    | 107,576,209           | 7,749,193   | 7,065   \
    \      | 7,065      |\n| CoMD          | taskdeps-56th    | 175M | 117M   | 11,757\
    \  | 56    | 107,864,889           | 9,345,091   | 31,585        | 31,585    \
    \ |\n|               | taskloop-16th    | 251M | 2M     | 177,566 | 16    | 1,132,063\
    \             | 1,364,476   | 1,241         | 1,241      |\n|               |\
    \ taskloop-56th    | 251M | 4M     | 194,160 | 56    | 3,058,872             |\
    \ 1,930,831   | 4,044         | 4,044      |\n|               | openmp-16th  \
    \    | 174M | 115M   | 10,915  | 16    | 107,576,257           | 7,749,193   |\
    \ 7,089         | 7,089      |\n|               | openmp-56th      | 175M | 117M\
    \   | 11,757  | 56    | 107,864,811           | 9,345,091   | 31,546        |\
    \ 31,546     |\n| SimpleMOC     | trace-16th       | 170M | 19M    | 60,029  |\
    \ 5,017 | 7,833,334             | 7,641,040   | 1,771,680     | 1,771,680  |\n\
    |               | Amg2013-18th     | 170M | 39M    | 145,485 | 36    | 3,332,723\
    \             | 36,405,204  | 1,657         | 1,657      |\n|               |\
    \ Amg2013-58th     | 190M | 52M    | 190,994 | 76    | 4,306,355             |\
    \ 48,531,303  | 14,039        | 14,039     |\n|               | Kripke-16th  \
    \    | 117M | 20M    | 22,482  | 17    | 12,509,801            | 8,366,557   |\
    \ 9,447         | 9,447      |\n|               | Kripke-56th      | 119M | 34M\
    \    | 34,156  | 58    | 18,994,381            | 15,904,315  | 44,773        |\
    \ 44,773     |\n|               | Lulesh-16th      | 35M  | 10M    | 52,940  |\
    \ 15    | 8,987,707             | 1,431,554   | 72            | 72         |\n\
    |               | Lulesh-16th      | 543M | 130M   | 167,595 | 16    | 123,136,205\
    \           | 7,407,451   | 2,230         | 2,230      |\n|               | Lulesh-56th\
    \      | 52M  | 14M    | 73,432  | 56    | 12,065,387            | 1,999,030 \
    \  | 1,707         | 1,707      |\n| OMPRacer      | Lulesh-56th      | 569M |\
    \ 156M   | 261,857 | 56    | 143,236,930           | 13,541,685  | 32,046    \
    \    | 32,046     |\n|               | miniFE-18th      | 208M | 44M    | 159,052\
    \ | 36    | 6,323,648             | 37,932,412  | 1,379         | 1,379      |\n\
    |               | miniFE-58th      | 207M | 63M    | 191,862 | 76    | 55,826,793\
    \            | 7,658,888   | 8,823         | 8,823      |\n|               | QuickSilver-56th\
    \ | 133M | 1M     | 21,132  | 56    | 890,633               | 650,042     | 17,387<br>114\
    \ | 17,387     |\n|               | XSBench-16th     | 97M  | 693.9K | 225   \
    \  | 15    | 691,187               | 2,467       |               | 114       \
    \ |\n|               | XSBench-56th     | 97M  | 710.9K | 370     | 56    | 707,006\
    \               | 2,948       | 442           | 442        |\n|              \
    \ | RSBench-16th     | 1.2B | 27M    | 1,278   | 16    | 27,005,898<br>109,606\
    \ |             | 123           | 123        |\n|               | RSBench-56th\
    \     | 1.2B | 27M    | 1,405   | 56    | 27,006,574            | 121,858    \
    \ | 421           | 421        |\n|               | DRACC-009        | 135M |\
    \ 70M    | 32      | 18    | 122                   | 10,000,063  | 30,000,060\
    \    | 30,000,060 |\n|               | DRACC-010        | 135M | 70M    | 32 \
    \     | 18    | 122                   | 10,000,063  | 30,000,060    | 30,000,060\
    \ |\n|               | DRACC-011        | 135M | 0.5K   | 30      | 15    | 204\
    \                   | 60          | 102           | 102        |\n|          \
    \     | DRACC-012        | 105M | 103M   | 543     | 18    | 632             \
    \      | 102,400,575 | 600,060       | 600,060    |\n|               | DRACC-013\
    \        | 105M | 103M   | 543     | 18    | 632                   | 102,400,575\
    \ | 600,060       | 600,060    |\n|               | DRACC-014        | 105M |\
    \ 0.5K   | 30      | 15    | 204                   | 60          | 102       \
    \    | 102        |\n| DRACC-16th    | DRACC-015        | 135M | 70M    | 32 \
    \     | 18    | 121                   | 10,000,063  | 30,000,060    | 30,000,060\
    \ |\n|               | DRACC-016        | 135M | 70M    | 32      | 18    | 121\
    \                   | 10,000,063  | 30,000,060    | 30,000,060 |\n|          \
    \     | DRACC-017        | 27M  | 0.5K   | 30      | 15    | 204             \
    \      | 60          | 102           | 102        |\n|               |       \
    \           | 135M | 0.5K   | 30      | 15    | 204                   | 60   \
    \       | 102           | 102        |\n|               | DRACC-018        | 105M\
    \ | 103M   | 543     | 18    | 632                   | 102,400,575 | 600,060 \
    \      | 600,060    |\n|               | DRACC-019        | 105M | 103M   | 543\
    \     | 18    | 632                   | 102,400,575 | 600,060       | 600,060\
    \    |\n|               | DRACC-020        | 105M | 0.5K   | 30      | 15    |\
    \ 204                   | 60          | 102           | 102        |\n|      \
    \         | DRB-062          | 184M | 70M    | 46      | 15    | 36,102,121  \
    \          | 33,912,061  | 60            | 60         |\n|               | DRB-105\
    \          | 134M | 44M    | 889     | 31    | 8,339,388             | 8,339,291\
    \   | 14,098,397    | 14,098,397 |\n|               | DRB-106          | 134M\
    \ | 70M    | 789     | 31    | 35,341,444            | 7,144,665   | 14,098,428\
    \    | 14,098,428 |\n|               | DRB-110          | 120M | 35M    | 32 \
    \     | 18    | 207                   | 5,000,047   | 15,000,103    | 15,000,103\
    \ |\n|               | DRB-122          | 112M | 0.5K   | 30      | 15    | 210\
    \                   | 60          | 105           | 105        |\n|          \
    \     | DRB-123          | 112M | 77M    | 243     | 16    | 35,000,220      \
    \      | 14,000,062  | 14,000,109    | 14,000,109 |\n|               | DRB-144\
    \          | 140M | 70M    | 32      | 17    | 121                   | 10,000,063\
    \  | 30,000,060    | 30,000,060 |\n|               | DRB-148          | 135M |\
    \ 70M    | 32      | 18    | 121                   | 10,000,063  | 30,000,060\
    \    | 30,000,060 |\n|               | DRB-150          | 112M | 56M    | 32 \
    \     | 17    | 121                   | 8,000,064   | 24,000,060    | 24,000,060\
    \ |\n\n| 1             | 2                | 3    | 4     | 5       | 6   | 7 \
    \          | 8           | 9           | 10          |\n|---------------|------------------|------|-------|---------|-----|-------------|-------------|-------------|-------------|\n\
    | Benchmark Set | Benchmark        | N    | N     | L       | V   | Reads    \
    \   | Writes      | Acq         | Rel         |\n|               | DRB-152   \
    \       | 112M | 56M   | 32      | 17  | 121         | 8,000,064   | 24,000,060\
    \  | 24,000,060  |\n|               | DRB-154          | 112M | 0.5K  | 30   \
    \   | 15  | 204         | 60          | 102         | 102         |\n|       \
    \        | DRB-155          | 50M  | 12M   | 51      | 18  | 344         | 87\
    \          | 6,000,141   | 6,000,141   |\n|               | DRB-176          |\
    \ 90M  | 47M   | 2079    | 31  | 20,649,583  | 10,224,212  | 8,077,747   | 8,077,747\
    \   |\n|               | DRB-176          | 341M | 272M  | 2209    | 31  | 109,382,709\
    \ | 52,675,980  | 55,364,923  | 55,364,923  |\n|               | DRB-176     \
    \     | 1.6B | 782M  | 2611    | 31  | 337,612,032 | 155,172,752 | 144,947,035\
    \ | 144,947,035 |\n|               | DRB-177          | 90M  | 45M   | 1315  \
    \  | 31  | 20,300,303  | 9,112,538   | 8,077,747   | 8,077,747   |\n|        \
    \       | DRB-177          | 211M | 191M  | 1522    | 31  | 82,046,982  | 40,599,487\
    \  | 34,217,455  | 34,217,455  |\n|               | DRB-177          | 382M |\
    \ 106M  | 1080    | 31  | 47,124,525  | 16,760,905  | 21,147,601  | 21,147,601\
    \  |\n|               | DRB-177          | 552M | 519M  | 1799    | 31  | 220,163,418\
    \ | 119,866,791 | 89,582,245  | 89,582,245  |\n|               | DRB-177     \
    \     | 618M | 333M  | 1881    | 31  | 142,820,683 | 80,362,747  | 55,364,923\
    \  | 55,364,923  |\n|               | DRB-177          | 1.6B | 836M  | 1963 \
    \   | 31  | 347,911,303 | 198,868,896 | 144,947,035 | 144,947,035 |\n|       \
    \        | DRB-062          | 193M | 72M   | 166     | 55  | 36,343,511  | 35,982,221\
    \  | 755         | 755         |\n|               | DRB-105          | 134M |\
    \ 46M   | 2,849   | 111 | 9,375,994   | 9,375,741   | 14,098,575  | 14,098,575\
    \  |\n|               | DRB-106          | 134M | 68M   | 2,284   | 111 | 34,064,016\
    \  | 6,297,656   | 14,098,676  | 14,098,676  |\n|               | DRB-110    \
    \      | 120M | 35M   | 112     | 58  | 765         | 5,000,167   | 15,000,382\
    \  | 15,000,382  |\n|               | DRB-122          | 112M | 1.8K  | 110  \
    \   | 55  | 770         | 220         | 385         | 385         |\n| DRB-56th\
    \      | DRB-123          | 112M | 77M   | 778     | 56  | 35,000,781  | 14,000,276\
    \  | 14,000,389  | 14,000,389  |\n|               | DRB-155          | 50M  |\
    \ 12M   | 181     | 58  | 1296        | 307         | 6,000,538   | 6,000,538\
    \   |\n|               | DRB-176          | 90M  | 49M   | 6802    | 111 | 22,550,856\
    \  | 10,755,451  | 8,078,107   | 8,078,107   |\n|               | DRB-176    \
    \      | 341M | 348M  | 8946    | 111 | 160,758,955 | 76,630,057  | 55,365,283\
    \  | 55,365,283  |\n|               | DRB-176          | 1.6B | 900M  | 10164\
    \   | 111 | 406,628,634 | 204,003,136 | 144,947,395 | 144,947,395 |\n|       \
    \        | DRB-177          | 90M  | 43M   | 3421    | 111 | 19,743,274  | 7,490,436\
    \   | 8,078,107   | 8,078,107   |\n|               | DRB-177          | 618M |\
    \ 326M  | 5541    | 111 | 139,688,872 | 75,806,547  | 55,365,283  | 55,365,283\
    \  |\n|               | graph500-16th    | 171M | 81M   | 113,732 | 16  | 76,413,805\
    \  | 4,799,533   | 2,526       | 2,526       |\n|               | graph500-56th\
    \    | 172M | 82M   | 119,601 | 56  | 77,444,355  | 5,086,862   | 26,472     \
    \ | 26,472      |\n|               | HPCCG-16th       | 228M | 55M   | 9,547 \
    \  | 16  | 50,028,952  | 5,798,966   | 2,199       | 2,199       |\n|        \
    \       | HPCCG-56th       | 230M | 79M   | 15,083  | 56  | 72,511,642  | 6,836,516\
    \   | 17,722      | 17,722      |\n|               | DC.S-16th        | 12M  |\
    \ 1.0K  | 102     | 18  | 338         | 409         | 132         | 132      \
    \   |\n|               | DC.S-56th        | 12M  | 19.8K | 633     | 57  | 13,917\
    \      | 4,731       | 524         | 524         |\n|               | IS.W-16th\
    \        | 153M | 48M   | 64,597  | 16  | 31,324,449  | 17,038,722  | 384    \
    \     | 384         |\n|               | IS.W-56th        | 300M | 140M  | 202,142\
    \ | 56  | 119,664,913 | 20,347,145  | 4,400       | 4,400       |\n|         \
    \      | loopA.bad-16th   | 113M | 93M   | 150,033 | 16  | 78,322,852  | 15,149,966\
    \  | 825         | 825         |\n|               | loopA.bad-56th   | 394M |\
    \ 334M  | 550,125 | 56  | 279,145,300 | 55,550,150  | 9,436       | 9,436    \
    \   |\n|               | loopA.solu1-16th | 193M | 93M   | 150,049 | 16  | 78,327,081\
    \  | 15,151,468  | 1,325       | 1,325       |\n|               | loopA.solu1-56th\
    \ | 674M | 334M  | 550,181 | 56  | 279,184,949 | 55,555,652  | 22,485      | 22,485\
    \      |\n|               | loopA.solu2-16th | 96M  | 51M   | 10,049  | 16  |\
    \ 50,165,397  | 1,011,384   | 618         | 618         |\n|               | loopA.solu2-56th\
    \ | 337M | 171M  | 10,180  | 56  | 170,601,797 | 1,015,606   | 11,858      | 11,858\
    \      |\n|               | loopA.solu3-16th | 96M  | 51M   | 10,048  | 16  |\
    \ 50,167,481  | 1,011,381   | 882         | 882         |\n|               | loopA.solu3-56th\
    \ | 337M | 171M  | 10,177  | 56  | 170,604,653 | 1,015,599   | 10,840      | 10,840\
    \      |\n|               | loopB.solu1-16th | 113M | 93M   | 150,037 | 16  |\
    \ 78,322,094  | 15,150,074  | 433         | 433         |\n|               | loopB.solu1-56th\
    \ | 394M | 334M  | 550,127 | 56  | 279,149,802 | 55,550,254  | 11,324      | 11,324\
    \      |\n|               | Mandelbrot-16th  | 116M | 112M  | 1,973   | 16  |\
    \ 112,231,695 | 2,708       | 117         | 117         |\n|               | Mandelbrot-56th\
    \  | 116M | 114M  | 2,196   | 56  | 114,434,245 | 3,096       | 448         |\
    \ 448         |\n|               | Pi-16th          | 150M | 96M   | 53      |\
    \ 16  | 50,000,252  | 46,875,100  | 99          | 99          |\n|           \
    \    | Pi-56th          | 150M | 99M   | 184     | 56  | 50,001,048  | 49,107,502\
    \  | 420         | 420         |\n| HPCBench      | QuickSort-16th   | 134M |\
    \ 41M   | 91,092  | 16  | 32,609,684  | 8,417,535   | 522         | 522      \
    \   |\n|               | QuickSort-56th   | 134M | 41M   | 91,172  | 56  | 32,612,404\
    \  | 8,417,695   | 1,882       | 1,882       |\n|               | fft6-16th  \
    \      | 146M | 0.9K  | 81      | 16  | 561         | 145         | 108      \
    \   | 108         |\n\n<span id=\"page-27-0\"></span>ICSE '24, April 12–21, 2024,\
    \ Lisbon, Portugal Zheng Shi, Umang Mathur, and Andreas Pavlogiannis\n\n| 1  \
    \           | 2                | 3    | 4      | 5          | 6      | 7     \
    \      | 8           | 9          | 10         |\n|---------------|------------------|------|--------|------------|--------|-------------|-------------|------------|------------|\n\
    | Benchmark Set | Benchmark        | N    | N      | L          | V      | Reads\
    \       | Writes      | Acq        | Rel        |\n|               | fft6-56th\
    \        | 146M | 2.4K   | 172        | 55     | 1,234       | 323         | 394\
    \        | 394        |\n|               | LUReduction-16th | 136M | 45M    |\
    \ 89,116     | 16     | 35,960,758  | 9,044,179   | 480        | 480        |\n\
    |               | LUReduction-56th | 137M | 45M    | 89,209     | 56     | 36,002,402\
    \  | 9,044,365   | 15,309     | 15,309     |\n|               | MD-16th      \
    \    | 204M | 118M   | 1,515      | 16     | 113,558,997 | 5,224,843   | 522 \
    \       | 522        |\n|               | MD-56th          | 204M | 120M   | 1,747\
    \      | 56     | 115,223,309 | 5,466,221   | 3,629      | 3,629      |\n|   \
    \            | testPath-16th    | 30M  | 7M     | 25,048     | 17     | 2,647,876\
    \   | 3,556,911   | 631,221    | 631,221    |\n|               | testPath-56th\
    \    | 37M  | 10M    | 69,308     | 57     | 4,219,733   | 3,671,124   | 1,303,636\
    \  | 1,303,636  |\n|               | fft-16th         | 496M | 78M    | 2,424,886\
    \  | 17     | 53,084,901  | 25,690,379  | 159        | 159        |\n|       \
    \        | fft-56th         | 496M | 83M    | 2,565,429  | 56     | 55,988,645\
    \  | 27,188,775  | 589        | 589        |\n|               | fft-56th     \
    \    | 2.1B | 363M   | 10,261,235 | 56     | 244,470,405 | 119,014,191 | 586 \
    \       | 586        |\n|               | qsomp1-16th      | 107M | 674.6K | 283\
    \        | 17     | 203,340     | 26,123      | 222,546    | 222,546    |\n| \
    \              | qsomp1-56th      | 107M | 540.1K | 157        | 57     | 143,062\
    \     | 6,387       | 195,286    | 195,286    |\n|               | qsomp2-16th\
    \      | 108M | 870.5K | 376        | 17     | 261,250     | 32,622      | 288,285\
    \    | 288,285    |\n|               | qsomp2-56th      | 107M | 629.6K | 254\
    \        | 57     | 177,428     | 14,801      | 218,671    | 218,671    |\n| \
    \              | qsomp3-16th      | 142M | 15M    | 33         | 17     | 3,929,700\
    \   | 51          | 5,894,435  | 5,894,434  |\n|               | qsomp3-56th \
    \     | 115M | 4M     | 113        | 57     | 1,009,842   | 171         | 1,514,348\
    \  | 1,514,347  |\n|               | qsomp4-16th      | 164M | 19M    | 37   \
    \      | 17     | 4,782,343   | 14,827      | 7,137,390  | 7,137,390  |\n|   \
    \            | qsomp4-56th      | 114M | 6M     | 5,350      | 57     | 2,707,676\
    \   | 518,055     | 1,863,535  | 1,863,535  |\n|               | qsomp6-56th \
    \     | 107M | 507.2K | 537        | 57     | 186,521     | 44,137      | 138,229\
    \    | 138,229    |\n|               | qsomp7-16th      | 89M  | 44M    | 8,035\
    \      | 16     | 44,130,252  | 304,813     | 123        | 123        |\n|   \
    \            | qsomp7-56th      | 296M | 147M   | 6,119      | 56     | 146,867,929\
    \ | 619,075     | 439        | 439        |\n|               | biojava-4th   \
    \   | 221M | 0.9K   | 9          | 12     | 59          | 24          | 383  \
    \      | 383        |\n|               | cassandra-132th  | 259M | 28M    | 9,839\
    \      | 12,211 | 2,686,843   | 1,672,488   | 12,246,313 | 12,246,313 |\n|   \
    \            | graphchi-20th    | 216M | 206.3K | 144        | 15     | 204,879\
    \     | 719         | 344        | 344        |\n| misc          | hsqldb-44th\
    \      | 19M  | 647.5K | 318        | 51     | 260,630     | 52,096      | 167,362\
    \    | 167,362    |\n|               | tradebeans-222th | 39M  | 218.9K | 778\
    \        | 674    | 71,344      | 30,605      | 58,338     | 58,338     |\n| \
    \              | tradesoap-221th  | 39M  | 218.6K | 775        | 672    | 71,264\
    \      | 30,541      | 58,266     | 58,266     |\n|               | zxing-15th\
    \       | 547M | 18M    | 3,310      | 359    | 18,389,530  | 9,748       | 3,624\
    \      | 3,624      |"
  decisions:
    language: '- Qualified. Reason: English Paper'
    evaluation_prompt: Qualified.
    related_work_prompt: Qualified
    novelty_prompt: Qualified
    review_only_prompt: Qualified
  llm_input_used: "## Abstract\nDynamic data race detection has emerged as a key technique\
    \ for ensuring\nreliability of concurrent software in practice. However, dynamic\
    \ approaches can\noften miss data races owing to nondeterminism in the thread\
    \ scheduler.\nPredictive race detection techniques cater to this shortcoming by\
    \ inferring\nalternate executions that may expose data races without re-executing\
    \ the\nunderlying program. More formally, the dynamic data race prediction problem\n\
    asks, given a trace \\sigma of an execution of a concurrent program, can \\sigma\n\
    be correctly reordered to expose a data race? Existing state-of-the art\ntechniques\
    \ for data race prediction either do not scale to executions arising\nfrom real\
    \ world concurrent software, or only expose a limited class of data\nraces, such\
    \ as those that can be exposed without reversing the order of\nsynchronization\
    \ operations.\n  In general, exposing data races by reasoning about synchronization\
    \ reversals\nis an intractable problem. In this work, we identify a class of data\
    \ races,\ncalled Optimistic Sync(hronization)-Reversal races that can be detected\
    \ in a\ntractable manner and often include non-trivial data races that cannot\
    \ be\nexposed by prior tractable techniques. We also propose a sound algorithm\
    \ OSR\nfor detecting all optimistic sync-reversal data races in overall quadratic\n\
    time, and show that the algorithm is optimal by establishing a matching lower\n\
    bound. Our experiments demonstrate the effectiveness of OSR on our extensive\n\
    suite of benchmarks, OSR reports the largest number of data races, and scales\n\
    well to large execution traces.\n\n## Introduction\nConcurrency bugs such as data\
    \ races and deadlocks often escape in-house testing and manifest only in production\
    \ [\\[19,](#page-11-0) [59\\]](#page-12-0), making the development of reliable\
    \ concurrent software a challenging task. Automated data race detection has emerged\
    \ as a first line of defense against undesired behaviors caused by data races,\
    \ has been actively studied over multiple decades, and is also the subject of\
    \ this paper. In particular, our focus is on dynamic analyses, which, unlike static\n\
    \nICSE '24, April 12–21, 2024, Lisbon, Portugal\n\n© 2024 Association for Computing\
    \ Machinery.\n\nACM ISBN 978-x-xxxx-xxxx-x/YY/MM. . . \\$15.00\n\n<https://doi.org/10.1145/nnnnnnn.nnnnnnn>\n\
    \n<span id=\"page-0-0\"></span>![](_page_0_Figure_16.jpeg)\n\nFigure 1: The two\
    \ conflicting events <sup>1</sup> = ⟨1, w()⟩ and <sup>12</sup> = ⟨4, w()⟩ is a\
    \ predictable data race of <sup>1</sup> which is also an optimistic sync-reversal\
    \ race, witnessed by the correct reordering <sup>1</sup> that reverses critical\
    \ sections.\n\ntechniques, are the preferred class of techniques for detecting\
    \ data races for industrial scale software applications [\\[59\\]](#page-12-0).\n\
    \nA dynamic data race detector observes an execution of a concurrent program and\
    \ infers the presence of a data race by analysing the trace of the observed execution.\
    \ A key challenge in the design of such a technique is sensitivity to non-deterministic\
    \ thread schedules — even for a fixed program input, a data race may be observed\
    \ under a very specific thread schedule, but not under other thread schedules.\
    \ This means that a simplistic race detector that, say, only checks for two conflicting\
    \ events appearing simultaneously in the execution trace, is likely going to miss\
    \ many bugs. This is where predictive analysis techniques shine — instead of looking\
    \ for bugs only in the execution that was observed, they additionally also detect\
    \ bugs in executions that, while not explicitly observed during testing, can nevertheless\
    \ be inferred from the observed execution, without rerunning the underlying program\
    \ [\\[31,](#page-11-1) [34,](#page-11-2) [45,](#page-11-3) [57,](#page-12-1) [60,](#page-12-2)\
    \ [65\\]](#page-12-3). Predictive techniques identify the space of executions\
    \ or reorderings that can provably be inferred from a given observed execution\
    \ , and then look for a reordering in this space, that can serve as a witness\
    \ to a bug such as a data race. Consider the execution 1 in Figure [1a](#page-0-0)\
    \ consisting of events 1, 2, . . . , <sup>12</sup> where denotes the th event\
    \ from the top. The two write events on variable , 1 and 12, are far apart and\
    \ not witnessed as a data race in 1. However, the correct reordering 1 of 1, in\
    \ which the two write events appear consecutively, shows that it is nevertheless,\
    \ a predictable data race of 1. Indeed any program that generates <sup>1</sup>\
    \ will also generate 1 albeit with a different thread interleaving.\n\nIn general,\
    \ sound (no false positives) and complete (no false negatives) data race prediction\
    \ is known to be an intractable problem [\\[44\\]](#page-11-4). Soundness is a\
    \ key desired property, since false positives need to be otherwise vetted manually,\
    \ a task which is particularly challenging in the case of concurrent programs.\
    \ Consequently,\n\nPermission to make digital or hard copies of all or part of\
    \ this work for personal or classroom use is granted without fee provided that\
    \ copies are not made or distributed for profit or commercial advantage and that\
    \ copies bear this notice and the full citation on the first page. Copyrights\
    \ for components of this work owned by others than ACM must be honored. Abstracting\
    \ with credit is permitted. To copy otherwise, or republish, to post on servers\
    \ or to redistribute to lists, requires prior specific permission and/or a fee.\
    \ Request permissions from permissions@acm.org.\n\nmany recent works counter the\
    \ intractability by proposing incomplete (but nevertheless sound) predictive race\
    \ detection algorithms that work in polynomial time and have high precision in\
    \ practice. The main contribution of this paper is a new race prediction algorithm\
    \ OSR that is sound, has higher prediction power than prior algorithms and achieves\
    \ high scalability in practice.\n\nThe design of our algorithm OSR stems from\
    \ the observation that often, data races can be exposed only by inverting the\
    \ relative order of (some pairs of) critical sections, or synchronizations. The\
    \ data race (1, 12) in Figure [1a,](#page-0-0) for instance, can in fact only\
    \ be observed in correct reorderings that invert the order of the two critical\
    \ sections on lock ℓ. However, reversing synchronization (lock/unlock) operations\
    \ in the reordering can further force a reversal in the order in which memory\
    \ access events must appear in the reordering, and can be intractable to reason\
    \ about [\\[44,](#page-11-4) [45\\]](#page-11-3). This strong tradeoff between\
    \ precision (obtained by virtue of reversing the order of many synchronization\
    \ operations) and performance has materialized on both the extremes. Algorithms\
    \ such as those based on the happens-before partial order [\\[42,](#page-11-5)\
    \ [55\\]](#page-12-4) or the recently proposed SyncP [\\[45\\]](#page-11-3) run\
    \ in linear time but fail to expose races that mandate reasoning about synchronization\
    \ reversals. On the other extreme, methods that exhaustively search for reversals,\
    \ either resort to expensive constraint solving [\\[31,](#page-11-1) [60\\]](#page-12-2)\
    \ or saturation style reasoning [\\[18,](#page-11-6) [54\\]](#page-12-5), and\
    \ do not scale to long execution traces observed in real world concurrent applications.\
    \ Our proposed algorithm OSR aims to strike a balance — it is designed to optimistically\
    \ reason about synchronization reversals, and identifies those reversals that\
    \ do not lead to the reversal of memory operations. The pair (1, 12) in Figure\
    \ [1](#page-0-0) is an example of a race that OSR reports.\n\nOSR reports all\
    \ optimistic synchronization-reversal races in overall time e(N<sup>2</sup> ),\
    \ spending e(N ) time for processing each event in the given execution trace .\
    \ Here, N is the number of events in and <sup>e</sup> hides polynomial multiplicative\
    \ factors due to number of locks and threads which are typically considered constants.\
    \ In order to check for the absence of memory reversals, OSR constructs a graph\
    \ (optimistic reordering graph) of events and checks if it is acyclic. Naively,\
    \ such an acyclicity check would take e(N ) time for every pair of conflicting\
    \ events, resulting in a total cubic running time. A key technical contribution\
    \ of our work is to perform this check in amortized constant time by constructing\
    \ a succinct representation of this graph, called abstract optimistic reordering\
    \ graph, of constant size. We show that this abstract graph preserves acyclicity,\
    \ and can be constructed in an incremental manner in amortized constant time,\
    \ allowing us to perform race prediction for the entire input execution in overall\
    \ quadratic (instead of cubic) time. Finally, we show that the problem of checking\
    \ the existence of an optimistic sync-reversal race also admits a matching quadratic\
    \ time lower bound, thereby implying that our algorithm is optimal.\n\nWe implemented\
    \ OSR and evaluate its performance thoroughly. Our evaluation demonstrates the\
    \ effectiveness of our algorithm on a comprehensive suite of 153 Java and C/C++\
    \ benchmarks derived from real-world programs. Our results show OSR has comparable\
    \ scalability as linear time algorithms SyncP and WCP, while it reports significantly\
    \ more races than the second most predictive one on many benchmarks, confirming\
    \ our hypothesis that going beyond the principle of synchronisation preservation\
    \ allows us\n\nto discover significantly more races and with better performance.\
    \ OSR, thus, advances the state-of-the-art in sound predictive race detection.\n\
    \nThe rest of the paper is organized as follows. In Section [2,](#page-1-0) we\
    \ discuss relevant background. In Section [3,](#page-3-0) we formally define the\
    \ notion of optimistic sync-reversal races, and present our algorithm OSR for\
    \ detecting all optimistic sync-reversal races in Section [4.](#page-4-0) Our\
    \ evaluation of OSR and its comparison with other race prediction algorithms is\
    \ presented in Section [5.](#page-7-0) In Section [6](#page-8-0) we discuss related\
    \ work and conclude in Section [7.](#page-10-0)"
  token_usage: 9690
  time_usage: 2.0325369834899902
