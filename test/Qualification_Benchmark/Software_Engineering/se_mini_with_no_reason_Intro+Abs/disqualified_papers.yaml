papers:
- title: Automated Test Production -- Systematic Literature Mapping
  abstract: 'The broader goal of this research, on the one hand, is to obtain the
    State of

    the Art in Automated Test Production (ATP), to find the open questions and

    related problems and to track the progress of researchers in the field, and on

    the other hand is to list and categorize the methods, techniques and tools of

    ATP that meet the needs of practitioners who produce computerized business

    applications for internal use in their corporations - eventually it can be

    extended to the needs of practitioners in companies that specialize in

    producing computer applications for generic use.'
  url: http://arxiv.org/abs/2401.01430v1
  keywords: ''
  document: "# Automated Test Production Systematic Literature Mapping\n\nGomes, J.M.\
    \ <sup>1</sup> and Dias, L.A.V. 1\n\n1 Instituto Tecnol´ogico de Aeron´autica\
    \ - ITA\n\nJanuary 4, 2024\n\n# 1 Objectives\n\nThe broader goal of this research,\
    \ on the one hand, is to obtain the State of the Art in ATP (Automated Test Production),\
    \ to find the open questions and related problems and to track the progress of\
    \ researchers in the field, and on the other hand is to list and categorize the\
    \ methods, techniques and tools of ATP that meet the needs of practitioners who\
    \ produce computerized business applications for internal use in their corporations\
    \ - eventually it can be extended to the needs of practitioners in companies that\
    \ specialize in producing computer applications for generic use.\n\n# 2 Literature\
    \ Systematic Mapping\n\n### 2.1 Planning\n\nIn order to obtain an overview of\
    \ the research on ATP, an SLM (Systematic Literature Mapping) is conducted here\
    \ so that from this study we can perform an SLR (Systematic Literature Review)\
    \ in order to investigate it further. We apply the method proposed by Petersen\
    \ et al. and which we present in the Figure [1](#page-1-0) to conduct this SLM[\\\
    [1\\]](#page-16-0).\n\nWe sought with this study to identify the amount and types\
    \ of research and its results under the topic ATP. As important secondary results,\
    \ we also sought to identify the discussion forums on the subject.\n\nThe question\
    \ QP1 is a filter for narrowing the scope of the research. The question QP2 aims\
    \ to identify the main discussion forums where researchers related to ATP publish\
    \ their work or meet to present advances and update their knowledge in the area.\
    \ In the question QP3 we propose to classify the search results and identify the\
    \ main types of studies related to ATP and categorize their contributions.\n\n\
    <span id=\"page-1-0\"></span>![](_page_1_Figure_0.jpeg)\n\nFigure 1: Steps do\
    \ execute the research for an SLM (adapted from [\\[1\\]](#page-16-0))\n\nThe\
    \ search term was generated from keywords and their synonyms as presented in the\
    \ Table [2.](#page-1-1) In Keele et al. it is pointed out that Petticrew and Roberts\
    \ suggest the use of PICOC (Population, Intervention, Comparison, Outcome, Context)\
    \ to formulate the search term in scientific publication databases [\\[2,](#page-16-1)\
    \ [3\\]](#page-16-2).\n\nTwo of the main considerations in designing the search\
    \ protocol are that, first, we are neglecting \"comparison\", and are therefore\
    \ using the PIOC (Population, Intervention, Outcome, Context) variation, because\
    \ it is not part of our goal to compare different solutions to the same problem,\
    \ and second, we avoid considering specific results, i.e., studies that are not\
    \ aimed at the production of general-purpose computer applications, or use within\
    \ an enterprise and business world context.\n\n<span id=\"page-1-2\"></span>\n\
    \n| #   | Question                                         |\n|-----|--------------------------------------------------|\n\
    | QP1 | Is the study?                                    |\n|     | Recently published\
    \ (within the last five years)? |\n| QP2 | Which \"journals\" include studies\
    \ in \"ATP?        |\n|     | Or Annals of Congresses, Events, Authors, etc. \
    \  |\n| QP3 | What kinds of studies are published in ATP?      |\n|     | Categorized\
    \ as listed in Figure 5                |\n\n<span id=\"page-1-1\"></span>\n\n\
    | Term       | Synonyms                                        | Related to  \
    \ |\n|------------|-------------------------------------------------|--------------|\n\
    | software   | program                                         | Population  \
    \ |\n| test       | check<br>checking<br>validation<br>verification | Population\
    \   |\n| generation | creation<br>inception<br>production             | Intervention\
    \ |\n| method     | methodology<br>model<br>process<br>standard     | Outcome\
    \      |\n| tool       | environment<br>framework<br>software<br>suite   | Outcome\
    \      |\n\nTable 1: Research questions for SLM\n\nTable 2: Search terms for the\
    \ SLM\n\nThe Context is an extended view of the population, where we say whether\
    \ the study is conducted in Academy or Industry, in which Industry segment [\\\
    [4\\]](#page-16-3). In our case we were indifferent with regard to this aspect.\n\
    \nFinally we applied the search criteria of the Section [2.1](#page-2-0) to the\
    \ scientific publication databases listed in the Table [3.](#page-2-1)\n\n```\n\
    (\"software\" OR \"program\" OR \"test\" OR \"check\" OR \"checking\" OR \"\n\
    \   ,→ validation\" OR \"verification\") AND (\"generation\" OR \"\n   ,→ creation\"\
    \ OR \"inception\" OR \"production\") AND (\"method\" OR\n   ,→ \"methodology\"\
    \ OR \"model\" OR \"process\" OR \"standard\" OR \"\n   ,→ tool\" OR \"environment\"\
    \ OR \"framework\" OR \"software\" OR \"\n   ,→ suite\")\n```\nListing 1: Search\
    \ criteria for the SLM\n\n### 2.2 Conduction\n\n<span id=\"page-2-1\"></span>The\
    \ scientific publication sources, listed in Table [3,](#page-2-1) are, according\
    \ to Brereton et al., the most relevant for Software Engineering [\\[5\\]](#page-16-4).\n\
    \n| Name                |  |\n|---------------------|--|\n| IEEE Xplore      \
    \   |  |\n| ACM Digital Library |  |\n| Google Scholar      |  |\n| CiteSeerX\
    \           |  |\n| Inspec              |  |\n| ScienceDirect       |  |\n| EI\
    \ Compendex        |  |\n| Springer Link       |  |\n\nTable 3: Scientific publishing\
    \ databases\n\nThe choice of primary research sources was based on simple and\
    \ practical premises:\n\n- use of structured search terms (using \"AND\", \"OR\"\
    , \"NOT\" and parentheses);\n- filters to search for more recent documents; and\n\
    - filters to list relevant documents in the desired area of expertise.\n\nBased\
    \ on these criteria, the ScienceDirect and ISI Web of Science publication bases\
    \ were discarded for not presenting satisfactory results for this research [1](#page-2-2)\
    \ and both Google Scholar and CiteSeerX by not providing additional\n\n<span id=\"\
    page-2-2\"></span><sup>1</sup>The ScienceDirect of Elsevier for example restricts\
    \ the search to only 8 terms, and in one exercise returned only 11 studies. Of\
    \ these only one could be read in full and was still not relevant to the present\
    \ study.\n\nfilters by area or subarea of knowledge and finally EI Compendex and\
    \ Inspec also did not allow access. As an alternative we used the Periodical Portal\
    \ of CAPES (Coordena¸c˜ao de Aperfei¸coamento de Pessoal de N´ıvel Superior do\
    \ Brasil) [2](#page-3-0) with the exception that the search criteria had to be\
    \ adapted because of restrictions in the platform as can be seen in the Section\
    \ [2.2](#page-3-1) (as in the other platforms, additional filters were applied\
    \ - see Table [7\\)](#page-4-0).\n\n```\n(software OR program OR test OR check\
    \ OR checking OR\n   ,→ validation OR verification) AND\n(generation OR creation\
    \ OR inception OR production)\n```\nListing 2: Search criteria for the SLM used\
    \ at CAPES\n\n<span id=\"page-3-2\"></span>Then, within each publication base,\
    \ filters were applied to improve the quality of the results obtained. In general,\
    \ we selected only recent documents (2015 to 2020 - up to the date of the survey:\
    \ April 2020). The particular criteria for each base are listed in the Tables\
    \ [4](#page-3-2) to [7.](#page-4-0)\n\n| Filter            | Value           \
    \                                                                            \
    \                                                                            \
    \                              |\n|-------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n\
    | Publication title | IEEE Access<br>IEEE Systems Journal<br>IEEE Latin America<br>Transactions\
    \                                                                            \
    \                                                 |\n|                   | IEEE\
    \ Transactions on<br>Software Engineering                                    \
    \                                                                            \
    \                                          |\n| Indexation terms  | learning (artificial<br>intelligence)<br>optimisation<br>neural\
    \ nets<br>cloud computing<br>genetic algorithms<br>probability<br>program testing<br>search\
    \ problems<br>Internet<br>resource allocation |\n\nTable 4: IEEE Xplore filters\n\
    \n| Filter                   | Value                   |\n|--------------------------|-------------------------|\n\
    | ACM Full-Text Collection | All journals collection |\n| Publication Title  \
    \      | Search title only       |\n\nTable 5: ACM Digital Library filters\n\n\
    <span id=\"page-3-0\"></span><sup>2</sup>See <http://www.periodicos.capes.gov.br>.\n\
    \n| Filter        | Value                                                    \
    \                                                                            \
    \                                                                            \
    \                                                        |\n|---------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n\
    | Content Type  | Article<br>Chapter<br>Conference Paper                     \
    \                                                                            \
    \                                                                            \
    \                                                      |\n| Discipline    | Computer\
    \ Science                                                                    \
    \                                                                            \
    \                                                                            \
    \                             |\n| Subdiscipline | Computer Science,<br>general<br>Computer\
    \ Systems<br>Organizations and<br>Communications<br>Networks<br>Data Structures\
    \ and<br>Information Theory<br>Information Systems<br>and Communication<br>Service<br>Software\
    \ Engineer<br>ing/Programming and<br>Operating Systems |\n\n<span id=\"page-4-0\"\
    ></span>\n\n| Filter                          | Value                        \
    \                        |\n|---------------------------------|------------------------------------------------------|\n\
    | Type of<br>source               | Studies                                  \
    \            |\n| Language<br>Refinement<br>Topic | English<br>Pair revised journals<br>Computer\
    \ Science |\n\n| Table 7: \"Peri´odicos da CAPES\" filters |\n|-----------------------------------------|\n\
    |-----------------------------------------|\n\n<span id=\"page-4-1\"></span>After\
    \ running the search using the Section [2.1](#page-2-0) criteria and applying\
    \ the filters listed in the Tables [4](#page-3-2) to [7](#page-4-0) we obtained\
    \ the results listed in the Table [8](#page-4-1) (in number of documents).\n\n\
    | Name                 | Qty. | %     |\n|----------------------|------|-------|\n\
    | IEEE Xplore          | 715  | 33.10 |\n| ACM Digital Library  | 308  | 14.26\
    \ |\n| Springer Link        | 709  | 32.82 |\n| Peri´odicos da CAPES | 428  |\
    \ 19.81 |\n| TOTAL                | 2160 |       |\n\nTable 8: Total documents\
    \ retrieved from each publishing databases for the SLM\n\nThe selection of documents\
    \ was based on Inclusion and Exclusion criteria defined iteratively during the\
    \ reading of the documents found and served to determine the suitability of each\
    \ one to the objectives of this work. The Inclusion Criteria are those presented\
    \ in the Table [9,](#page-5-0) and in the Table [10](#page-5-1) we have the Exclusion\
    \ Criteria.\n\n<span id=\"page-5-0\"></span>\n\n| #   | Description          \
    \                                                                        |\n|-----|----------------------------------------------------------------------------------------------|\n\
    | CI1 | Document types:<br>books (book excerpts), technical re<br>ports;     \
    \                        |\n| CI2 | If several have reported the same study, only\
    \ the most<br>recent one will be considered; and |\n| CI3 | From the abstract\
    \ the researcher can deduce that the ar<br>ticle is about ATP.               |\n\
    \nTable 9: Inclusion criteria for the SLM\n\n<span id=\"page-5-1\"></span>\n\n\
    | #   | Description                                                |\n|-----|------------------------------------------------------------|\n\
    | CE1 | The article strays from the main topic of this study which |\n|     |\
    \ deals with ATP for general applications;                   |\n| CE2 | The topic\
    \ ATP is not part of the article's contribution or |\n|     | the topic is only\
    \ mentioned; and                           |\n| CE3 | No empirical findings or\
    \ current available literature are  |\n|     | reported.                     \
    \                             |\n\nTable 10: Exclusion criteria for the SLM\n\n\
    Given the large number of studies (see Table [8\\)](#page-4-1) found we organized\
    \ our work into iterative steps:\n\n- 1. Reading the summary and conclusion; and\n\
    - 2. Selection and classification by reading the entire document.\n\nA pre-selection\
    \ was based only on the title of the document found because, as already noted\
    \ by Keele et al., searches of electronic databases bring a very large number\
    \ of irrelevant results. During the review, other studies were rejected as being\
    \ outside the scope of this study [\\[2\\]](#page-16-1).\n\n| Name           \
    \      | Qty. | %     |\n|----------------------|------|-------|\n| IEEE Xplore\
    \          | 33   | 14.73 |\n| ACM Digital Library  | 66   | 29.46 |\n| Springer\
    \ Link        | 11   | 4.91  |\n| Peri´odicos da CAPES | 114  | 50.89 |\n| Sub-total\
    \            | 224  |       |\n| Duplicate studies    | 7    | 4.24  |\n| Rejected\
    \ studies     | 52   | 31.52 |\n| TOTAL                | 165  |       |\n|   \
    \                   |      |       |\n\nTable 11: Primary studies selction result\
    \ for the SLM\n\n<span id=\"page-6-0\"></span>\n\n| #     | Description      \
    \                                  |\n|-------|----------------------------------------------------|\n\
    | CE1.1 | Applied to hardware;                               |\n| CE1.2 | Applied\
    \ to embedded software;                      |\n| CE1.3 | Language-specific; \
    \                                |\n| CE1.4 | Does not deal with tests for general\
    \ applications; |\n| CE1.5 | Not intended for general applications;          \
    \   |\n| CE2.1 | Does not deal with test generation;                |\n| CE3.1\
    \ | No contribution to this study;                     |\n| CE3.2 | This is not\
    \ scientific research; and               |\n| CE3.3 | Survey with old data.  \
    \                            |\n\nTable 12: Refining exclusion criteria for the\
    \ SLM\n\n<span id=\"page-6-1\"></span>The studies were rejected in the second\
    \ selection based on a refinement of the exclusion criteria listed in the Table\
    \ [10](#page-5-1) and that we list in the Table [12.](#page-6-0) Our motivation\
    \ behind these criteria, as explained in the \"Objectives\", is to find solutions\
    \ that meet the needs of professionals who produce computerized business applications\
    \ for internal use in their corporations - eventually extending to the needs of\
    \ professionals in companies specializing in the production of generic computer\
    \ applications.\n\n![](_page_6_Figure_3.jpeg)\n\nFigure 2: Classification scheme\
    \ (adapted from [\\[1\\]](#page-16-0))\n\nFor documents classification we adopted\
    \ the Petersen et al. as can be seen in the Figure [2,](#page-6-1) and in the\
    \ same way we adopted facets analyzing the abstracts of the studies found. We\
    \ started by analyzing two main facets to classify the documents. The research\
    \ type facet was based on the classification proposed by Wieringa et al. and summarized\
    \ in the Table [13.](#page-7-0) The type of contribution is based on the interpretation\
    \ of the abstracts and listed in the Table [14](#page-8-0) [\\[1,](#page-16-0)\
    \ [6\\]](#page-16-5).\n\n<span id=\"page-7-0\"></span>\n\n| Category      | Description\
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                             |\n|---------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n\
    | Validation    | The techniques investigated are new and have not<br>been implemented\
    \ in practice. Techniques used<br>are from experimental examples and done in the<br>laboratory.\
    \                                                                            \
    \                                              |\n| Avaliation    | The technique\
    \ was implemented in practice and<br>the evaluation was conducted.<br>Demonstrated<br>how\
    \ the technique was implemented in practice<br>and what the consequences were\
    \ in terms of ben<br>efits and disadvantages. Also includes identifica<br>tion\
    \ of problems in the industry. |\n| Proposal      | A solution to a problem has\
    \ been proposed, ei<br>ther new or a significant extension of an existing<br>technique.\
    \ The potential benefits and applicabil<br>ity of the solution is shown by a short\
    \ example or<br>a good line of argument.                                     \
    \              |\n| Philosophical | These studies outlined a new approach to pre<br>existing\
    \ knowledge and structured the field in the<br>form of a taxonomy or conceptual\
    \ work.                                                                      \
    \                                                                          |\n\
    | Opinion       | These studies express someone's personal opinion<br>about a\
    \ particular technique, whether it is good<br>or not, or how things are done.<br>They\
    \ are not<br>supported by related work or research methods.                  \
    \                                                                 |\n| Practice\
    \      | They explain in what and how something was<br>done in practice. It has\
    \ to be the author's per<br>sonal experience.                                \
    \                                                                            \
    \                                                              |\n\nTable 13:\
    \ Researches types\n\n<span id=\"page-8-0\"></span>\n\n| Category     | Description\
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \   |\n|--------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n\
    | Metric       | A system or standard of measures or measure<br>ments taken using\
    \ an existing standard.                                                      \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                          |\n| Tool         | A device or implementation, used\
    \ to perform a<br>certain function.                                          \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                          |\n| Model        |\
    \ A comprehensive and systematic approach that<br>includes theoretical principles,\
    \ benefits and draw<br>backs, objectives, methodological guidelines and<br>specifications,\
    \ and the characteristic use of cer<br>tain sets of methods and techniques.  \
    \                                                                            \
    \                                                                         |\n\
    | Method       | No particular theoretical orientation is inferred<br>in a method.\
    \ Researchers impose their own par<br>ticular theoretical beliefs on an experiment\
    \ when<br>they design and implement it by applying one or<br>more techniques.\
    \                                                                            \
    \                                                                            \
    \                    |\n| Technique    | A single operation or interaction in\
    \ which a re<br>searcher uses one or more procedures to elicit an<br>immediate\
    \ reaction from the object of study or to<br>shape the experiment and obtain results.\
    \                                                                            \
    \                                                                            \
    \                                             |\n| Procedure    | An organized\
    \ sequence of operations and interac<br>tions that a researcher uses to conduct\
    \ an exper<br>iment.                                                         \
    \                                                                            \
    \                                                                            \
    \                                                                            |\n\
    | Intervention | Purposefully interferes with or mitigates various<br>aspects\
    \ of the object of study and that affect the<br>outcome by applying procedure,\
    \ technique. The<br>elements acting on the industry during a partic<br>ular intervention\
    \ are most often computer appli<br>cations, the researcher, or both.         \
    \                                                                            \
    \                  |\n| Approach     | A broad way of addressing an industry concern\
    \ or<br>problem. A specific methods is not implied, but<br>a specific set of techniques\
    \ will likely come into<br>play when trying to intervene in the industry and<br>the\
    \ problem that is the subject of the research.<br>The procedures to be used will\
    \ be determined by<br>the delimitations of the methodological variant in<br>which\
    \ we design the study. |\n| Strategy     | An action plan designed to achieve\
    \ an overall<br>goal.                                                        \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                        |\n\nTable 14: Contributions\
    \ types\n\n### 2.3 Analysis of Results\n\n<span id=\"page-8-1\"></span>Based on\
    \ the criteria listed in the Table [10](#page-5-1) and expanded in the Table [12](#page-6-0)\
    \ we selected 165 and rejected 52 documents categorized according to the Table\
    \ [15.](#page-8-1)\n\n| Motive                                            | Qty.\
    \ |\n|---------------------------------------------------|------|\n| Applied to\
    \ hardware                               | 9    |\n| Applied to embedded software\
    \                      | 2    |\n| Language-specific                         \
    \        | 2    |\n| No contribution to this study                     | 5   \
    \ |\n| This is not scientific research                   | 1    |\n| Does not\
    \ deal with test generation                | 22   |\n| Does not deal with tests\
    \ for general applications | 3    |\n| Not intended for general applications \
    \            | 7    |\n| Survey with old data                              | 1\
    \    |\n| TOTAL                                             | 52   |\n\nTable\
    \ 15: Rejection motives\n\nThe research questions listed in Table [1](#page-1-2)\
    \ were applied to the selected studies and we obtained the results that we list\
    \ below.\n\n#### QP1 - Is the study current?\n\n<span id=\"page-9-1\"></span>Recently\
    \ published (within the last five years)?\n\n| Tipo  | criteria |\n|-------|----------|\n\
    | 2015  | 34       |\n| 2016  | 32       |\n| 2017  | 20       |\n| 2018  | 26\
    \       |\n| 2019  | 38       |\n| 2020  | 15       |\n| TOTAL | 165      |\n\n\
    Table 16: Publications / Year\n\n| Tipo        | Qty. | %     |\n|-------------|------|-------|\n\
    | Books       | 3    | 1.82  |\n| Conferences | 51   | 30.91 |\n| Journals   \
    \ | 111  | 67.27 |\n| TOTAL       | 165  |       |\n\nTable 17: Studies / Channel\n\
    \n<span id=\"page-9-0\"></span>We can observe a relative constancy of studies\
    \ published on the topic in recent years, indicating that the topic is of interest\
    \ and there is potential progress to be explored.\n\n#### QP2 - Which \"journals\"\
    \ include studies in \"ATP\"?\n\nOr Annals of Congresses, Events, Authors, etc.\n\
    \nWe classified how the studies were published (see Table [17\\)](#page-9-0) and\
    \ then sought to identify where they were published to get an idea of the best\
    \ Journals and Events where to look for information on the topic. Unfortunately,\
    \ as can be seen in the Table [18](#page-10-0) there is no specific event for\
    \ this topic. In the Figure [3](#page-10-1) and Table [19](#page-11-0) we obtained\
    \ a more satisfactory result in identifying the most relevant publications for\
    \ our research.\n\n<span id=\"page-10-1\"></span>![](_page_10_Figure_0.jpeg)\n\
    \nFigure 3: Participation of ATP Studies in Congresses\n\n<span id=\"page-10-0\"\
    ></span>\n\n| Conference                                                     \
    \                                                         | Qty. |\n|-------------------------------------------------------------------------------------------------------------------------|------|\n\
    | The Search-Based Software Testing (SBST) Workshop (co-located<br>in ICSE)  \
    \                                             | 7    |\n| ACM ESEC/FSE Joint European\
    \ Software Engineering Conference<br>and Symposium on the Foundations of Software\
    \ Engineering | 6    |\n| Internation Conference on Software Engineering     \
    \                                                                     | 6    |\n\
    | IEEE/ACM International Conference on Automated Software En<br>gineering    \
    \                                             | 5    |\n| ACM ICSCA Software and\
    \ Computer Applications                                                      \
    \                      | 3    |\n| ACM SIGSOFT ISSTA International Symposium on\
    \ Software<br>Testing and Analysis                                           |\
    \ 3    |\n| ACM/IEEE International Conference on Automation of Software<br>Test\
    \                                                     | 2    |\n| ACM ESEC/FSE\
    \ Joint European Software Engineering Conference                             \
    \                                | 2    |\n| and Symposium on the Foundations\
    \ of Software Engineering                                                    \
    \            |      |\n| ACM SBES Brazilian Symposium on Software Engineering\
    \                                                                    | 2    |\n\
    | Symposium on Information and Communication Technology                      \
    \                                             | 2    |\n\nTable 18: Major conferences\
    \ addressing ATP\n\nIn the graph from Figure [3](#page-10-1) we have a look at\
    \ the interest in the topic over the course of the surveyed period in the main\
    \ congresses listed.\n\nIn Journals we identify in the Table [19](#page-11-0)\
    \ where the topic is most frequently addressed.\n\n![](_page_11_Figure_0.jpeg)\n\
    \nParticipation of ATP Studies in Publications\n\n<span id=\"page-11-0\"></span>Figure\
    \ 4: Participation of ATP Studies in Publications\n\n| Publication           \
    \                                         | Qty. |\n|----------------------------------------------------------------|------|\n\
    | IEEE Trans. Software Eng.                                      | 20   |\n| IEEE\
    \ Access                                                    | 9    |\n| The Journal\
    \ of Systems & Software                              | 6    |\n| Science of Computer\
    \ Programming                                | 5    |\n| IET Software        \
    \                                           | 4    |\n| International Journal\
    \ of Software Engineering and Knowledge En | 4    |\n| gineering             \
    \                                         |      |\n| Journal of King Saud University\
    \ - Computer and Information Sci | 4    |\n| ences                           \
    \                               |      |\n| ACM Trans. Softw. Eng. Methodol. \
    \                              | 3    |\n| Applied Soft Computing            \
    \                             | 3    |\n| Autom Softw Eng                    \
    \                            | 3    |\n| Empir Software Eng                  \
    \                           | 3    |\n| J Softw Eng Res Dev                  \
    \                          | 3    |\n| Software Testing, Verification and Reliability\
    \                 | 3    |\n| The Computer Journal                           \
    \                | 3    |\n\nTable 19: Major Publications Addressing ATP\n\nOnce\
    \ again we present in a graph at Figure [3](#page-10-1) a view of the interest\
    \ in the topic over the surveyed period in the main Journals listed.\n\nWe also\
    \ sought to identify the main authors on the subject, regardless of the medium\
    \ of publication. In the Table [20](#page-12-1) we list those who presented the\
    \ highest production within this research and their H-Index [\\[7\\]](#page-16-6).\n\
    \n<span id=\"page-12-1\"></span>\n\n| Author                    | Institution\
    \                                | Qty. | h-idx3 |\n|---------------------------|--------------------------------------------|------|--------|\n\
    | Harman, Mark              | University College Lon<br>don              | 3 \
    \   | 68     |\n| Arcuri, Andrea            | Kristiania<br>University<br>College\
    \        | 9    | 39     |\n| Fraser, Gordon            | University of Passau\
    \                       | 8    | 39     |\n| McMinn, Phil              | University\
    \ of Sheffield                    | 4    | 27     |\n| Zamli,<br>Kamal<br>Z. \
    \    | University<br>Malaysia<br>Pahang           | 8    | 23     |\n| Panichella,\
    \ An<br>nibale  | Delft<br>University<br>of<br>Technology    | 3    | 27     |\n\
    | Gargantini,<br>Angelo     | University of Bergamo                      | 3 \
    \   | 18     |\n| Vergilio,<br>Silvia<br>R. | Federal<br>University<br>of<br>Paran´a\
    \     | 4    | 17     |\n| Riccobene,<br>Elvinia     | Universit`a di Milano \
    \                     | 3    | 16     |\n| Arcaini, Paolo            | National<br>Institute<br>of<br>Informatics\
    \ | 3    | 15     |\n| Staats, Matt4             | University<br>of<br>Luxem<br>bourg\
    \         | 3    | 15     |\n| Gay, Gregory              | Chalmers, University\
    \ of<br>Gothenburg      | 6    | 12     |\n| Rojas,<br>Jos´e<br>Miguel | University\
    \ of Leicester                    | 3    | 13     |\n\nTable 20: Main Authors\
    \ in ATP of this Study\n\nThe Table [20](#page-12-1) seeks to order the authors\
    \ by weighting their H-Index and the number of publications found within the search.\n\
    \n### QP3 - What kinds of studies are published in ATP?\n\nCategorized as listed\
    \ in Figure [5](#page-12-0)\n\nWe first build on the classification proposed by\
    \ Wieringa et al. and quantify the Facet of Study Types in Figure [5](#page-12-0)\
    \ [\\[6\\]](#page-16-5). This categorization will be useful in performing a SLR\
    \ as we qualify the studies with the desired bias for this research.\n\n<span\
    \ id=\"page-12-0\"></span>![](_page_12_Figure_6.jpeg)\n\nFigure 5: Types of Artifacts\
    \ Generated\n\n<span id=\"page-12-3\"></span><span id=\"page-12-2\"></span><sup>3</sup>Data\
    \ obtained via Google Scholar and calculated since 2015.\n\n<sup>4</sup>See Scopus\
    \ [https://www.scopus.com/results/authorNamesList.uri?sort=count-f&](https://www.scopus.com/results/authorNamesList.uri?sort=count-f&src=al&affilName=University+of+Luxembourg&s=AUTHLASTNAME%28Staats%29+AND+AUTHFIRST%28Matt%29+AND+AFFIL%28University+of+Luxembourg%29&st1=Staats&st2=Matt)\
    \ [src=al&affilName=University+of+Luxembourg&s=AUTHLASTNAME%28Staats%29+AND+](https://www.scopus.com/results/authorNamesList.uri?sort=count-f&src=al&affilName=University+of+Luxembourg&s=AUTHLASTNAME%28Staats%29+AND+AUTHFIRST%28Matt%29+AND+AFFIL%28University+of+Luxembourg%29&st1=Staats&st2=Matt)\n\
    \n[AUTHFIRST%28Matt%29+AND+AFFIL%28University+of+Luxembourg%29&st1=Staats&st2=Matt](https://www.scopus.com/results/authorNamesList.uri?sort=count-f&src=al&affilName=University+of+Luxembourg&s=AUTHLASTNAME%28Staats%29+AND+AUTHFIRST%28Matt%29+AND+AFFIL%28University+of+Luxembourg%29&st1=Staats&st2=Matt).\n\
    \n<span id=\"page-13-0\"></span>![](_page_13_Figure_0.jpeg)\n\nFigure 6: Types\
    \ of Test Generators\n\nIn Figure [6](#page-13-0) we categorize studies also with\
    \ respect to the level of access to the source code of the program object of testing\
    \ in the Verification and Validation language, where white-box [5](#page-13-1)\
    \ tests are applied in verification (i.e., was the correct program built?) and\
    \ black-box [6](#page-13-2) tests are applied in validation (i.e., was the program\
    \ built correctly?). For the purposes of this research, we consider any access\
    \ to the source code to mean white-box testing and neglect so-called graybox [7](#page-13-3)\
    \ testing. We classify as white-box or black-box tests those tests that apply\
    \ verification techniques through observation or direct experience.\n\nWe consider\
    \ as Formal tests verifiable by theoretical means or pure logic, whose specifications\
    \ may include expressions in various logical forms, used to write pre and post\
    \ conditions, axioms of data types, constraints, temporal properties. They can\
    \ represent definitions of process states, and there is a formal deduction system,\
    \ enabling proofs, or other verifications (such as model checking), or both. Thus,\
    \ formal specifications can be analyzed to guide the identification of appropriate\
    \ test cases. According to Gaudel, these are black-box type tests, where the internal\
    \ organization of the program under test is ignored and the strategy is based\
    \ on a description of the desired properties and program behavior, grouped here\
    \ by those that meet these [\\[8\\]](#page-16-7) characteristics.\n\nAlso following\
    \ the methodology proposed by Wieringa et al., we quantified the documents by\
    \ the Facet of the Artifact Type generated by the study, taking as a basis the\
    \ approach presented by each and which we list in the Figure [7](#page-14-0) [\\\
    [6\\]](#page-16-5). In a future SLR we can apply qualitative aspects that will\
    \ determine whether for the purposes of this research specialized (generate only\
    \ the code or the test data) or general (generate both the code and the test data)\
    \ approaches are the most relevant.\n\n<span id=\"page-13-2\"></span><span id=\"\
    page-13-1\"></span><sup>5</sup>Validating non-functional, internal aspects of\
    \ a computer application.\n\n<sup>6</sup>Validating functional and external aspects\
    \ of a computer application.\n\n<span id=\"page-13-3\"></span><sup>7</sup>The\
    \ combination of white-box and black-box testing methods.\n\n<span id=\"page-14-0\"\
    ></span>![](_page_14_Figure_0.jpeg)\n\nFigure 7: Types of Artifacts Generated\n\
    \n<span id=\"page-14-1\"></span>![](_page_14_Figure_2.jpeg)\n\nFigure 8: Contributions\
    \ of the Studies\n\nThe quantification of the studies by the Contribution Type\
    \ Facet (see Figure [7\\)](#page-14-0) was important for the qualification and\
    \ selection of the most relevant studies to meet the objectives of the present\
    \ research and the quantified can be observed in the Figure [8.](#page-14-1)\n\
    \n# 3 Results\n\nWas presented the elementary results of a SLM applied to finding\
    \ relevant studies in ATP. This review applied the methodology of Petersen et\
    \ al. with elements of Keele et al., Brereton et al. [\\[1,](#page-16-0) [2,](#page-16-1)\
    \ [5\\]](#page-16-4)\n\n### 3.1 Conclusions\n\nBased on the research questions\
    \ developed in the Table [1](#page-1-2) the conclusions is:\n\n#### 3.1.1 Is the\
    \ study current?\n\nWas ensured that the studies were recent by restricting our\
    \ search to the last 5 years and we can observe in the Table [16](#page-9-1) an\
    \ even distribution of studies across the surveyed period.\n\n#### 3.1.2 Which\
    \ \"journals\" include studies in ATP?\n\nIn the Table [17](#page-9-0) it can\
    \ be seen the large concentration of studies published in conferences and \"Journals\"\
    \ and this led us to list the main conferences (see Table [18\\)](#page-10-0)\
    \ and the main publications (see Table [19\\)](#page-11-0). The relatively small\
    \ number of books on the subject, in our view, is due to the innovative characteristics\
    \ under which the fields of engineering and computer science live today.\n\n####\
    \ 3.1.3 What categories of studies are published in ATP?\n\nOf particular interest\
    \ to our research on ATP, the types of studies that stood out the most can be\
    \ seen in Figure [8.](#page-14-1) The concentration in practical aspects, as tools,\
    \ methods, models and metrics leads us to conclude that the maturity the subject\
    \ is now in Academy. The generation of both code and data is addressed by the\
    \ studies, and this is a guarantee that we're covering all aspects of the subject.\n\
    \n### 3.2 Future Work\n\nThis work aims to prepare ground for a SLR where it will\
    \ determine the challenges in applying generative testing techniques and evaluate\
    \ the solutions intended to be applied in future work.\n\n# Acronyms\n\nATP Automated\
    \ Test Production - pages: 1, 2, 6, 10, 13, 15, 16\n\nCAPES Coordena¸c˜ao de Aperfei¸coamento\
    \ de Pessoal de N´ıvel Superior do Brasil - page: 4\n\nPICOC Population, Intervention,\
    \ Comparison, Outcome, Context - page: 2\n\nPIOC Population, Intervention, Outcome,\
    \ Context - page: 2\n\nSLM Systematic Literature Mapping - pages: 1, 15\n\nSLR\
    \ Systematic Literature Review - pages: 1, 13, 14, 16\n\n# References\n\n- <span\
    \ id=\"page-16-0\"></span>[1] K. Petersen, R. Feldt, S. Mujtaba, and M. Mattsson,\
    \ \"Systematic mapping studies in software engineering,\" in 12th International\
    \ Conference on Evaluation and Assessment in Software Engineering (EASE) 12, 2008,\
    \ pp. 1– 10.\n- <span id=\"page-16-1\"></span>[2] S. Keele et al., \"Guidelines\
    \ for performing systematic literature reviews in software engineering,\" Technical\
    \ report, Ver. 2.3 EBSE Technical Report. EBSE, Tech. Rep., 2007.\n- <span id=\"\
    page-16-2\"></span>[3] M. Petticrew and H. Roberts, Systematic reviews in the\
    \ social sciences: A practical guide. John Wiley & Sons, 2008.\n- <span id=\"\
    page-16-3\"></span>[4] C. Wohlin, P. Runeson, M. H¨ost, M. C. Ohlsson, B. Regnell,\
    \ and A. Wessl´en, Experimentation in software engineering. Springer Science &\
    \ Business Media, 2012.\n- <span id=\"page-16-4\"></span>[5] P. Brereton, B. A.\
    \ Kitchenham, D. Budgen, M. Turner, and M. Khalil, \"Lessons from applying the\
    \ systematic literature review process within the software engineering domain,\"\
    \ Journal of systems and software, vol. 80, no. 4, pp. 571–583, 2007.\n- <span\
    \ id=\"page-16-5\"></span>[6] R. Wieringa, N. Maiden, N. Mead, and C. Rolland,\
    \ \"Requirements engineering paper classification and evaluation criteria: A proposal\
    \ and a discussion,\" Requirements engineering, vol. 11, no. 1, pp. 102–107, 2006.\n\
    - <span id=\"page-16-6\"></span>[7] J. E. Hirsch, \"An index to quantify an individual's\
    \ scientific research output,\" Proceedings of the National academy of Sciences,\
    \ vol. 102, no. 46, pp. 16 569–16 572, 2005.\n- <span id=\"page-16-7\"></span>[8]\
    \ M.-C. Gaudel, \"Formal methods for software testing,\" in 2017 International\
    \ Symposium on Theoretical Aspects of Software Engineering (TASE), IEEE, 2017,\
    \ pp. 1–3."
  decisions:
    language: '- Qualified. Reason: English Paper'
    evaluation_prompt: Disqualified.
    related_work_prompt: Disqualified
    novelty_prompt: Disqualified
    review_only_prompt: Disqualified
  llm_input_used: '## Abstract

    The broader goal of this research, on the one hand, is to obtain the State of

    the Art in Automated Test Production (ATP), to find the open questions and

    related problems and to track the progress of researchers in the field, and on

    the other hand is to list and categorize the methods, techniques and tools of

    ATP that meet the needs of practitioners who produce computerized business

    applications for internal use in their corporations - eventually it can be

    extended to the needs of practitioners in companies that specialize in

    producing computer applications for generic use.


    '
  token_usage: 1302
  time_usage: 1.6255710124969482
- title: "Practical Guidelines for the Selection and Evaluation of Natural\n  Language\
    \ Processing Techniques in Requirements Engineering"
  abstract: 'Natural Language Processing (NLP) is now a cornerstone of requirements

    automation. One compelling factor behind the growing adoption of NLP in

    Requirements Engineering (RE) is the prevalent use of natural language (NL) for

    specifying requirements in industry. NLP techniques are commonly used for

    automatically classifying requirements, extracting important information, e.g.,

    domain models and glossary terms, and performing quality assurance tasks, such

    as ambiguity handling and completeness checking. With so many different NLP

    solution strategies available and the possibility of applying machine learning

    alongside, it can be challenging to choose the right strategy for a specific RE

    task and to evaluate the resulting solution in an empirically rigorous manner.

    In this chapter, we present guidelines for the selection of NLP techniques as

    well as for their evaluation in the context of RE. In particular, we discuss

    how to choose among different strategies such as traditional NLP, feature-based

    machine learning, and language-model-based methods. Our ultimate hope for this

    chapter is to serve as a stepping stone, assisting newcomers to NLP4RE in

    quickly initiating themselves into the NLP technologies most pertinent to the

    RE field.'
  url: http://arxiv.org/abs/2401.01508v3
  keywords: ''
  document: '# 15. Practical Guidelines for the Selection and Evaluation of Natural
    Language Processing Techniques in Requirements Engineering


    Mehrdad Sabetzade[h](https://orcid.org/0000-0002-4711-8319) <sup>1</sup> and Chetan
    Aror[a](https://orcid.org/0000-0003-1466-7386) <sup>2</sup>


    <sup>1</sup> University of Ottawa, ON, Canada.


    <sup>2</sup> Monash University, Vic, Australia.


    Contributing authors: m.sabetzadeh@uottawa.ca; chetan.arora@monash.edu;


    #### Abstract


    Natural Language Processing (NLP) is now a cornerstone of requirements automation.
    One compelling factor behind the growing adoption of NLP in Requirements Engineering
    (RE) is the prevalent use of natural language (NL) for specifying requirements
    in industry. NLP techniques are commonly used for automatically classifying requirements,
    extracting important information, e.g., domain models and glossary terms, and
    performing quality assurance tasks, such as ambiguity handling and completeness
    checking. With so many different NLP solution strategies available and the possibility
    of applying machine learning alongside, it can be challenging to choose the right
    strategy for a specific RE task and to evaluate the resulting solution in an empirically
    rigorous manner. In this chapter, we present guidelines for the selection of NLP
    techniques as well as for their evaluation in the context of RE. In particular,
    we discuss how to choose among different strategies such as traditional NLP, feature-based
    machine learning, and language-model-based methods. Our ultimate hope for this
    chapter is to serve as a stepping stone, assisting newcomers to NLP4RE in quickly
    initiating themselves into the NLP technologies most pertinent to the RE field.


    # 1 Introduction


    NLP''s role in requirements automation is pivotal, due to the widespread use of
    natural language (NL) in industrial requirements specifications. Historically,
    NL has posed challenges for requirements analysis because of its inherent proneness
    to defects such as incompleteness and ambiguity. Recent breakthroughs in NLP,
    e.g., the emergence of large language models, have nonetheless drastically enhanced
    our ability to automatically analyze textual information. This development is
    poised to even further amplify the adoption and influence of NL in requirements
    engineering.


    Due to the rapid advancement of NLP, newcomers to NLP4RE may feel overwhelmed
    by the numerous potentially applicable technologies. Another challenge is the
    necessity to empirically assess a proposed automation solution, ensuring proper
    optimization, and, where applicable, improve performance over existing solutions.


    Over the past several years, we have studied various requirements automation problems,
    including checking compliance with requirements templates [\[5\]](#page-20-0),
    glossary construction [\[7\]](#page-20-1), model extraction [\[6\]](#page-20-2),
    requirements demarcation [\[2\]](#page-20-3), ambiguity handling [\[19\]](#page-21-0),
    and question answering [\[20\]](#page-21-1). With the benefit of hindsight, this
    chapter aims to reflect on our research process and offer our collective insights
    into how we approach NLP4RE problems. Before we start, we need to emphasize that
    our perspective is retrospective. Given the fast pace of progress in NLP technologies,
    new considerations may surface, and existing technologies could become outdated.
    Therefore, it is important for readers to consider the time this chapter was written
    (2023) when dealing with new technologies. This advice applies to most works in
    the fast-changing field of Applied AI.


    Structure. Section [2](#page-1-0) outlines the steps for automating pre-processing,
    analysis, and post-processing in NLP4RE. Section [3](#page-6-0) describes various
    NLP techniques and discusses their key considerations for automation in RE. Finally,
    Section [4](#page-20-4) summarizes the chapter and presents conclusions.


    # <span id="page-1-0"></span>2 Automation Steps in NLP4RE


    The automation process for NL requirements and related textual artifacts can be
    structured into three sequential steps. These steps, shown in Figure [1](#page-2-0)
    are: (1) Preprocessing, (2) Analysis, and (3) Post-processing. We outline these
    steps next.


    ### 2.1 Pre-processing


    The goal of pre-processing is to automatically examine the NL content of requirements
    or requirements-related artifacts (e.g., design documents and code) and generate
    structured information for use by the Analysis step (Section [2.2\)](#page-2-1).


    The specific information targeted by pre-processing depends on the needs of the
    subsequent Analysis step. Importantly, the information to obtain through preprocessing
    relies on the selection of the units of analysis for the Analysis step. In the
    context of NLP, "unit of analysis" refers to the particular textual components
    that are intended for processing and interpretation. Some common units of analysis
    are: words, phrases, sentences, and paragraphs. For instance, when the objective
    of Analysis is identifying statements that contain requirements, sentences are
    frequently adopted as the units of analysis [\[2\]](#page-20-3). We note that
    there is a composition relationship between different unit types in NLP. For example,
    a sentence is made up of phrases, and a phrase


    ![](_page_2_Figure_0.jpeg)


    #### <span id="page-2-0"></span>Fig. 1 NLP4RE Steps


    is made up of words. Because of this characteristic, it is possible to use a combination
    of units of analysis simultaneously, e.g., phrases alongside sentences.


    Typically, during the pre-processing phase, a set of "features" are calculated
    for the units of analysis (or the relationships between the units). A feature
    refers to a distinct attribute or characteristic of a unit of analysis. Features
    can be numeric (e.g., the number of tokens appearing in a sentence) or categorical
    (e.g., part-of-speech tags).


    The most common enabling technologies for computing features are presented in
    Figure [1](#page-2-0) under the Pre-processing activity. These technologies include
    the NLP Pipeline, Relevance Measures and Embeddings, which are discussed further
    in Section [3.](#page-6-0)


    #### <span id="page-2-1"></span>2.2 Analysis


    The core of the process shown in Figure [1](#page-2-0) is the Analysis step. In
    NLP4RE, this step typically manifests as one of the following three alternative
    activities: Classification, Clustering, and Text Generation.


    #### 2.2.1 Analysis Activities


    Classification involves the assignment of labels or categories to the units of
    analysis. There are numerous NLP4RE use cases for classification. An example use
    case would be differentiating between functional (F) and non-functional requirements
    (NF) [\[27\]](#page-22-0). This task can be framed as the assignment of F and
    NF labels to the units of analysis, which in this context, are typically sentences
    within a requirements document.


    Classification further extends to encompass verbatim information extraction. Verbatim
    information extraction involves directly extracting exact segments from source
    documents without abstraction, inference, or interpretation. This is done by marking
    off the text segments of interest and assigning labels to them. A typical use
    case is identifying requirements-related text segments in legal documents and
    annotating them with labels such as "permission", "obligation", "condition", and
    "exception" [\[43\]](#page-23-0).


    Clustering leverages inherent similarities among the units of analysis to organize
    them into groups or themes. Unlike classification, which requires predetermined
    labels, clustering emphasizes the intrinsic structure and similarities within
    the content. The goal is to bring together similar content based on shared features,
    thereby avoiding the need for explicit predefined categories. This makes clustering
    particularly useful when dealing with problems where the labels are not well-defined
    or when exploring content where the underlying patterns might not be immediately
    evident. For instance, clustering can be used to identify groups of closely related
    requirements phrases during glossary construction [\[7\]](#page-20-1); here there
    is no predetermined set of classes for the groups of related phrases that will
    emerge.


    Text Generation involves the automated creation of human-readable text based on
    either structured or unstructured inputs to aid the derivation, completion, understanding
    and communication of requirements. NLP4RE solutions based on text generation are
    a relatively recent development but are rapidly gaining momentum thanks to advances
    in generative language models like GPT [\[9\]](#page-21-2). Example use cases
    for text generation include constructing requirements models based on prompts
    and early-stage descriptions [\[16\]](#page-21-3), summarizing requirements-related
    documents [\[26\]](#page-22-1), and providing predictive assistance for requirements
    completion [\[31\]](#page-22-2).


    #### 2.2.2 Technique Selection


    Selecting suitable enabling technique(s) for the Analysis step is crucial. To
    make this selection easier, we have developed a decision process, shown in Figure
    [2.](#page-4-0) This simple process, which is based on our past experience, aims
    to facilitate narrowing the options for the Analysis step.


    The first and most important criterion in this process is decision node (a), as
    depicted in Figure [2.](#page-4-0) This decision concerns whether we have a well-established
    and pre-existing set of conceptual categories relevant to automation. For instance,
    consider the task of classifying functional and non-functional requirements. In
    this task, the categories would be functional and non-functional, and this understanding
    exists before classification. For another example, consider the task of domain
    model extraction. Here, all pertinent categories are identified and can be listed,
    such as class, attribute, association, cardinality constraint, and so on. In contrast,
    consider a problem like identifying similar and potentially redundant requirements.
    When presented with a requirements document, predicting the number of equivalence
    classes (clusters) is impossible. As a result, there is no predefined set of conceptual
    categories for this problem that can be known beforehand.


    ![](_page_4_Figure_0.jpeg)


    <span id="page-4-0"></span>Fig. 2 Identifying Suitable Enabling Technique(s) for
    a Specific Analysis Task


    Without established conceptual categories, dataset size is the next crucial criterion
    to consider. Historically, when predefined categories are not possible, clustering
    algorithms have been the preferred enabling technique for analysis. Recent advances
    in generative language models like ChatGPT and Llama nonetheless offer an alternative.
    For instance, when dealing with data sources such as requirements documents, one
    can use exploratory prompts like the following to uncover the main themes: "Cluster
    the primary concepts in the following: [contents of the data source]".


    As of writing, using a generative language model as an alternative to clustering
    algorithms is effective only for relatively small content volumes. Existing language
    models set a limit on total input and output tokens during interactive dialogs,
    known as "token limit" or "session size." This limit defines how much context
    the model is capable of taking into account for response generation. Currently,
    the largest token limit we are aware of is 32,768 tokens (GPT-4-32k); this would
    be insufficient for many NLP4RE problems, such as traceability retrieval, which
    can potentially involve millions of tokens.


    When an a-priori-known set of conceptual categories exists, the next question
    is whether a labelled dataset is available. This is captured by decision node
    (c) in Figure [2.](#page-4-0) When a labelled dataset is lacking, the most common
    enabling technique is query-based pattern matching. Although pattern matching
    does not require the explicit creation of labelled data, formulating the queries
    often entails some form of qualitative analysis, e.g., grounded theory [\[41\]](#page-23-1).
    Alternatively, language models can be employed when labelled data is unavailable.
    In such a scenario, one has to rely exclusively on the language model''s pre-training,
    without further specific training for the task at hand. For example, a language
    model could be asked the following: "Is this requirements statement functional
    or non-functional? [an individual requirements statement]".


    If labelled data is available, the next factor to consider is the volume of such
    data, as captured by decision node (d) in Figure [2.](#page-4-0) When a considerable
    amount of labelled data is available, there are two options: applying feature-based
    learning or utilizing the labelled data to "fine-tune" a language model. Fine-tuning
    involves adapting a pre-trained model to a specific task through targeted training.
    In cases of limited labelled data, achieving task adaptation for a language model
    with minimal examples – a technique known as few-shot learning – is likely to
    yield better results.


    It is important to note that there is no general rule as to what constitutes "considerable"
    or "limited" labelled data. Several parameters such as the quality of labels,
    the complexity of the relationships to be learned, the number of classes (in classification)
    and the range of values (in regression), the amount of noise in the data, the
    dimensionality of the feature space (in case of feature-based learning), and the
    desired level of accuracy to achieve can influence data needs. As such, experimentation
    on a case-by-case basis is crucial to determine whether the labelled data at hand
    should be regarded as considerable or limited. In our experience, and for the
    sake of offering ballpark figures, having fewer than 100 data points tends to
    constitute a limited amount. A considerable amount of data, on the other hand,
    is likely to materialize within the range of 500 to 5000 labelled data points.


    We need to highlight three important aspects related to the process in Figure
    [2.](#page-4-0) First, no individual decision model can encompass the full spectrum
    of techniques employed in NLP4RE. Our model aims to offer a simplified representation
    of common scenarios, rather than imposing constraints or promoting a lack of flexibility
    in technique selection. Second, the process is likely to evolve in the future
    to stay in step with NLP4RE research. In particular, capitalizing on the interactive
    capabilities of large language models, there is potential to further elaborate
    the process by considering prompting strategies and providing additional guidelines.
    However, due to the scarcity of NLP4RE approaches built on large language models,
    we have to defer doing so to the future. Finally, when selecting enabling techniques
    for analysis, the cost and environmental impact must be considered. Most notably,
    the resource-intensive nature of large language models requires justification,
    especially when alternatives cannot be dismissed due to compelling reasons such
    as lack of accuracy.


    #### 2.3 Post-processing


    Post-processing – the third step in the process of Figure [1](#page-2-0) – aims
    to enhance the results of the Analysis step or to adapt these results for human
    analysts'' better understanding. Post-processing is not needed in all NLP4RE solutions
    and is thus an optional step.


    To illustrate a simple scenario where post-processing is required, let us consider
    the task of requirements identification. For this task, one may apply the following
    heuristic as a post-processing step: if all but one sentence in a passage are
    categorized as requirements (during the Analysis step), that lone sentence should
    be reclassified from a non-requirement to a requirement. This adjustment will
    likely increase the accuracy of requirements identification [\[2\]](#page-20-3).


    For a more advanced example of post-processing, let us consider requirements completion
    based on predictions by a language model. In this context, the language model
    is likely to generate a non-negligible number of predictions that are not useful
    (false positives) alongside the useful ones. To reduce the incidence of unuseful
    predictions in the final results, the development of a post-processing filter
    becomes essential [\[31\]](#page-22-2).


    In its simplest form, post-processing can be light, e.g., in the case of the heuristic
    mentioned earlier for requirements identification. In more complex scenarios,
    like the one mentioned above where predictions need to be filtered, additional
    enabling techniques might be needed to carry out post-processing.


    # <span id="page-6-0"></span>3 Enabling Techniques: Overview and Guidelines


    In this section, we outline the various enabling techniques shown in Figure [1](#page-2-0)
    and provide practical guidelines for applying and evaluating them.


    #### <span id="page-6-1"></span>3.1 NLP Pipeline


    The NLP pipeline is a sequence of modules that incrementally add linguistic annotations
    to an input text. The pipeline typically begins with tasks like tokenization (breaking
    text into words or sub-words), sentence splitting (segmenting a passage into sentences),
    and lemmatization (reducing words to their base forms). Next and depending on
    the annotations required, the pipeline performs tasks such as part-ofspeech (POS)
    tagging (labelling words with their grammatical roles), named-entity recognition
    (identifying entities like names, dates, and locations), and syntactic parsing
    (analyzing sentence structure). Syntactic parsing includes two main techniques:
    constituency parsing (deconstructing sentences into grammatical constituents)
    and dependency parsing (determining grammatical relationships between words).


    Figures [3](#page-7-0) (a) and (b) respectively exemplify the annotations generated
    by the constituency parsing and dependency parsing modules for the following requirements
    sentence: R = "The flight simulator shall store log messages in the database.".
    The annotations generated by the constituency parser for this sentence are: NP
    (noun phrase), VP (verb phrase), and PP (prepositional phrase). These annotations
    capture the hierarchical constituents of the sentence. The relationships between
    the words in the sentence are given by the pairwise links generated through dependency
    parsing. For instance, from the dependency parse graph of Figure [3](#page-7-0)
    (b), one can determine that "simulator" functions as the subject (nsubj) for "store,"
    and that "messages" serves as the object (obj) for this transitive verb. Crucially,
    both parsing methods require sentence splitting and POS tagging. Figure [3\(](#page-7-0)a)
    illustrates sentence annotation (S),


    ![](_page_7_Figure_0.jpeg)


    This is an implementation of a minimal neural model for constituency parsing based
    on an independent scoring of labels and spans. This SpanConstituencyParser simply
    encodes a sequence of text with a stacked Seq2SeqEncoder , extracts span representations
    using a SpanExtractor , and then predicts a label for each span in the sequence.
    These labels are non-terminal nodes in a constituency parse tree, which we then
    greedily reconstruct. The model uses ELMo embeddings, which are completely character-based
    and improves single model performance from 92.6 F1 to 94.11 F1 on the


    The flight simulator shall store log messages in the database .


    **Constituency Parsing**


    **Constituency Parser with ELMo embeddings**


    Penn Treebank, a 20% relative error reduction.


    The flight simulator shall store log messages in the database.


    Demo Model Card Model Usage


    Model


    Example Inputs Select a Sentence Sentence


    Answer a question Reading Comprehension Visual Question Answering Annotate a sentence
    Named Entity Recognition Open Information Extraction Sentiment Analysis Dependency
    Parsing Constituency Parsing Semantic Role Labeling Annotate a passage Coreference
    Resolution Generate a passage Language Modeling Masked Language Modeling Compare
    two sentences Textual Entailment Evaluate Reading Comprehension


    **Model Output**


    Run Model


    **CLI Output** *What is this?*


    Input Model Output


    Constituency parsing is the task of breaking a text into sub-phrases, or constituents.
    Non-terminals in the parse tree are types of phrases, the terminals are the words
    in the sentence.


    © The Allen Institute for Artificial Intelligence - All Rights Reserved | Privacy
    Policy | Terms of Use | Business Code of Conduct (b) Dependency Parse Graph


    The flight simulator shall store log messages in the database . DT NN NN MD VB
    NN NNS IN DT NN . compound aux compound det det nsubj obj case 1 Fig. 3 Illustration
    of (a) Constituency Parsing and (b) Dependency Parsing. Both Parsing Methods Require
    Sentence Detection and POS Tagging.


    <span id="page-7-0"></span>obl:in punct **— Text to annotate —**


    version 4.5.4


    **— Language —**


    Share


    English Submit


    CoreNLP Tools: while both figures include POS tags (DT: determiner, NN: noun,
    NNS: plural noun, MD: modifier, VB: verb, IN: preposition).


    **Enter a TokensRegex expression to run against the above sentence:** TokensRegex
    Semgrex Tregex e.g., (?\$foxtype [{pos:JJ}]+ ) fox Match When using the NLP pipeline,
    it is important to consider the specific natural language(s) that require support.
    While most NLP modules have good performance over well-written English, their
    effectiveness can vary significantly when dealing with other languages or when
    processing text that deviates from grammatical norms, such as user feedback (e.g.,
    app reviews) or developer commit messages. Experimentation is therefore typically
    necessary for constructing an accurate NLP pipeline. NLP workbenches such as GATE
    [\[23\]](#page-22-3) and DKPro [\[14\]](#page-21-4) facilitate the integration
    of NLP modules from different NLP libraries. For instance, these workbenches enable
    the use of the POS tagger provided by one library, say, Apache OpenNLP [\[4\]](#page-20-5),
    alongside the constituency and dependency parsers from another library, say, Stanford
    CoreNLP [\[32\]](#page-22-4). This flexibility to combine NLP modules from different
    libraries enables systematic experimentation with various pipeline configurations
    to determine the optimal configuration for the task at hand. An example of such
    experimentation can be found in our work on checking conformance with requirements
    templates [\[5\]](#page-20-0).


    Visualisation provided using the brat visualisation/annotation software. If systematic
    experimentation with alternative NLP modules is not feasible or if there are constraints
    on using a single library, e.g., to minimize complexity, it would be important
    to conduct an error analysis on the annotations generated by the NLP pipeline.
    This analysis helps with ensuring the absence of systemic issues. Common


    8


    systemic issues include recurring errors in sentence detection, repeated inaccuracies
    in tokenization and POS tags, and incorrect parsing results.


    Takeaway: NLP Pipeline


    Prioritize pipeline implementations that are known to work well for the language(s)
    you need to support. Experimentation with different NLP modules is often useful
    for improving accuracy. If extensive experimentation with the NLP pipeline is
    not possible or you are limited to a single library, conduct an error analysis
    on the annotations to identify and address any systemic issues such as recurring
    errors in sentence detection, POS tags, and parsing.


    #### <span id="page-8-0"></span>3.2 Relevance Measures


    In the context of RE, relevance denotes the extent to which various requirement
    segments are related or how closely a specific requirements segment aligns with
    a particular query, topic, or artifact (e.g., a design document or a portion thereof).
    The concept of relevance is typically quantified using one or a combination of
    three distinct categories of metrics: syntactic, semantic, and statistical.


    - Syntactic Relevance is the string-based or structural relatedness between two
    or more text segments. There are numerous syntactic relevance measures. For a
    fairly comprehensive list of syntactic measures, consult [\[24\]](#page-22-5).
    An example metric in the syntactic category is Levenshtein [\[24\]](#page-22-5).
    This metric calculates the minimum number of single-character edits (insertions,
    deletions, or substitutions) required to transform one string into another. Levenshtein
    distance is usually normalized (scaled from 0 to 1) by dividing it by the maximum
    possible edit distance between the two strings. For instance, the (normalized)
    Levenshtein distance between the phrases "software system" and "software systems"
    is 0.9375, indicating that the two strings have considerable lexical overlap.

    - Semantic Relevance goes beyond string matching to measure similarity between
    word meanings. Semantic relevance is typically quantified using metrics like PATH
    within a semantic network such as WordNet [\[37\]](#page-23-2). WordNet captures
    various relationships, such as hypernymy and hyponymy, which denote an "is-a"
    connection, and meronymy and holonymy, representing a "part-of" association. To
    illustrate, consider the relationship between "vehicle" and "car", where "vehicle"
    serves as the hypernym of "car", and the relationship between "wheel" and "car",
    with "wheel" acting as the meronym of "car". PATH similarity calculates the shortest
    path between two concepts on the "is-a" hierarchy. For instance, the PATH score
    between the terms "cat" and "mammal" is higher than between "cat" and "vehicle",
    indicating that "cat" is semantically more related to "mammal" than "vehicle".

    - Statistical Relevance assesses relevance by analyzing the frequency and distribution
    of terms within a document, often employing algorithms such as TF-IDF and BM25.
    These algorithms are frequently normalized on a scale from 0 to 1 [\[12\]](#page-21-5).
    Statistical methods typically operate at the level of term frequency and do not
    necessitate


    a pre-constructed semantic network. For instance, TF-IDF evaluates a term''s importance
    within a document based on its frequency of occurrence in that document, normalized
    by its frequency across the entire corpus. BM25, which extends TF-IDF, takes into
    account further factors like term saturation and document length. To illustrate,
    consider a corpus of requirements documents. A term like "user authentication"
    might appear infrequently but could be highly significant. BM25 can rank a document
    that extensively discusses this term higher than a document that only mentions
    it in passing, thereby indicating its greater relevance in the context of security
    requirements. Statistical measures are more commonly employed in information retrieval
    (IR) problems within NLP4RE, such as querying requirements [\[3\]](#page-20-6).


    Applications and Other Considerations. Relevance measures serve two main use cases
    in NLP4RE. The first is to calculate similarity either within a set of requirements
    or between requirements and textual segments found in other development artifacts.
    The second use case is to assess the relative significance of terms in documents.
    Frequently, relevance measures are employed as features in both supervised and
    unsupervised learning.


    Different relevance measures can be combined to provide a more holistic characterization
    of relevance. For example, when tasked with constructing a requirements glossary,
    the combination of syntactic and semantic measures can help identify a wider range
    of variations among related domain terms. To illustrate, if our objective is to
    cluster terms related to a flight simulation system, we anticipate that "flight
    coordinates", "aircraft position", and all concepts associated with "flight positioning"
    should land in the same cluster [\[7\]](#page-20-1). Simultaneously applying both
    syntactic and semantic measures facilitates the determination of these terms being
    highly similar.


    In relation to syntactic measures, it is important to note that, because requirements
    frequently manifest variability in phrasing, vocabulary selection and syntactic
    structure, techniques like lemmatization and tokenization are often required prior
    to computing syntactic measures. This preprocessing helps mitigate variability
    and enhances the accuracy of comparisons [\[7\]](#page-20-1).


    Finally, and in relation to semantic measures, we note that these measures are
    quickly being replaced by more advanced techniques, notably embeddings. Embeddings
    not only capture semantic similarity but also contextual similarity, as we discuss
    in Section [3.3.](#page-10-0) Furthermore, while lexical resources like WordNet
    offer the advantage of establishing human-interpretable connections between words,
    techniques such as embeddings provide a more nuanced characterization of meaning,
    although they may not be entirely interpretable by humans. Consequently, when
    the primary goal is the application of similarity metrics, rather than explaining
    relationships, there is often limited justification for employing semantic measures
    like PATH in future research.


    #### Takeaway: Relevance Measures


    Combining different relevance measures often results in more accurate analytical
    outcomes. Relevance measures can be applied at different levels of granularity,
    ranging from individual tokens (e.g., Levenshtein distance) to entire documents
    (e.g., TF-IDF). Empirical evaluations of relevance measures can centre around
    identifying the most effective combination of these measures or benchmarking advanced
    solutions against relevance measures considered as baseline methods.


    #### <span id="page-10-0"></span>3.3 Embeddings


    Embeddings enable the conversion of words into numerical vectors. These vectors
    encapsulate the semantic connections among words, in turn supporting a more meaningful
    manipulation of language. Word embeddings are typically derived through self-supervised
    approaches such as Word2Vec [\[34\]](#page-22-6), GloVe [\[36\]](#page-23-3),
    and the pre-training of language models such as BERT [\[18\]](#page-21-6) and
    GPT [\[38\]](#page-23-4). For example, using the 300 dimensional variant of GloVe
    embedding vectors, the word "requirements" would be represented as a 300-dimensional
    vector: [-1.3598e-01, -1.8174e-01, · · · , -6.2015e-02].


    While the primary goal of embeddings is to represent individual words, methods
    also exist for generating embeddings for phrases and sentences. For example, a
    simple approach for obtaining sentence embeddings is to compute the weighted average
    of the word embeddings in a given sentence [\[10\]](#page-21-7).


    There are two main use cases for embeddings in the existing NLP4RE literature:
    (1) computing semantic similarity, typically through the cosine measure, and (2)
    using embeddings as features for learning. To illustrate, suppose that we are
    interested in identifying most similar requirements, e.g., as a way to find overlapping
    or redundant requirements. Consider the following three statements: R1 = "The
    system shall react to user input within one second."; R2 = "The system shall respond
    within one second."; and R3 = "The system shall encrypt sensitive data.". For
    a requirement sentence R, let emb(R) denote the sentence''s embeddings. By utilizing
    the 300 dimensional variant of GloVe and employing averaging to derive sentence
    embeddings from word embeddings, we would obtain the following: cosine(emb(R1),
    emb(R2)) ≈ 0.95 > cosine(emb(R1), emb(R3)) ≈ 0.83 > cosine(emb(R2), emb(R3)) ≈
    0.78. Now, the requirements analyst can, for example, sort the requirements pairs
    in descending order of similarity and inspect the most similar pairs to determine
    if there are overlaps or redundancies. Alternatively, when it is feasible to create
    a labelled dataset for training, the embeddings can be used as features – either
    on their own or alongside other features – for building a feature-based classifier
    that distinguishes similar and non-similar requirements pairs. In our illustrative
    example, and assuming that only R1 and R2 are deemed similar, one could infer
    (among other labelled data points) the following for training: emb(R1)|emb(R2)|SIMILAR
    and emb(R1)|emb(R3)|NOT-SIMILAR. Here, "|" denotes vector concatenation, and "SIMILAR"
    and "NOT-SIMILAR" denote labels for pairs of requirements.


    There are some important considerations to note when working with embeddings:


    Dimensionality of Embeddings. The dimensionality of embeddings, which refers to
    the number of dimensions (or features) used to represent each word as a vector,
    determines the richness of information in word vectors. While some dimensions
    may have clear interpretations, e.g., gender or sentiment, others can be complex,
    capturing subtle and abstract aspects of word semantics that would be difficult
    for humans to interpret directly. There is no universally suitable choice for
    the dimensionality of word embeddings. Although a larger dimensionality can provide
    increased semantic nuance and potentially improved accuracy, one must consider
    the curse of dimensionality [\[34\]](#page-22-6) – the inherent challenges that
    arise when dealing with high-dimensional data. We recommend experimentation with
    alternative embedding methods and dimensionality options to find an acceptable
    trade-off between accuracy for the analytical task at hand and the challenges
    posed by high-dimensional data. Note that the techniques discussed in Section
    [3.5](#page-14-0) for feature selection and reduction can also be applied to embeddings
    to reduce dimensionality.


    Non-contextual vs. Contextual Embeddings. Embeddings can be either noncontextual
    or contextual. Non-contextual embeddings, such as those produced by GloVe, create
    fixed word vectors that remain the same regardless of context. In contrast, contextual
    word embeddings, as produced by models like BERT and GPT, consider the surrounding
    words to generate word vectors that adapt to the context. To illustrate, consider
    the following two sentences: S = "Meeting privacy requirements is essential.";
    and S ′ = "The system shall meet all the privacy requirements stipulated by the
    GDPR." Using GloVe to obtain embeddings for the word "requirements" yields the
    same vector for both sentences. In contrast, if one uses models such as BERT or
    GPT, the embeddings obtained for the same word in different sentences would differ.
    For instance, the bert-base-uncased variant of BERT yields the following vectors
    for "requirements" in S and S ′ , respectively: [2.7466e-01, 4.8193e-01, · · ·
    , 4.1681e-01] and [-1.6852e-01, 3.5143e-01, · · · , 3.5699e-01]. This difference
    is due to the contextually different meaning of "requirements" in these two sentences,
    where the word conveys the sense of a prerequisite condition in S and the sense
    of a specification in S ′ .


    Contextual embeddings offer greater accuracy but come at a higher computational
    cost. It is therefore important not to dismiss non-contextual embeddings outright
    but to weigh their potential alongside contextual embeddings to determine whether
    the added accuracy of contextual embeddings is worth the extra cost.


    Domain-specific Embeddings. Domain-specific embeddings capture contextspecific
    word meanings that generic embeddings may miss. For instance, specialized BERT
    variants, such as BioBERT [\[28\]](#page-22-7) for biomedical texts and LegalBERT
    [\[15\]](#page-21-8) for legal documents, provide domain-specific embeddings for
    their respective domains. When domain-specific embeddings exist, it is worthwhile
    to compare them with generic embeddings for potential improvements. Further, in
    cases where domain-specific embeddings are unavailable, but a suitable domain-specific
    corpus is accessible, one can


    attempt to construct domain-specific embeddings from scratch. Recent efforts in
    software engineering, such as building Word2Vec and GloVe embeddings for model-driven
    engineering [\[30\]](#page-22-8), provide useful guidance in this regard.


    ### Takeaway: Embeddings


    Experiment with different embedding technologies and dimensionality options to
    strike a balance between accuracy and the challenges of high-dimensional data.
    Assess whether the added accuracy of contextual embeddings justifies their higher
    computational cost, or if non-contextual embeddings suffice for your specific
    task. Explore domain-specific embeddings when available, as they may be superior
    at capturing context-specific meanings.


    #### 3.4 Query-based Pattern Matching


    The annotations produced by the NLP pipeline provide a rich basis for defining
    queries that detect patterns of interest within an input text. Typically, pattern-matching
    queries work with "spans". Each span represents a distinct sequence of consecutive
    words or tokens within the given text.


    An important technical aspect in pattern matching is the choice of the query language.
    In NLP, span information can be flat, focusing on individual words and their properties
    (like POS tags), hierarchical, revealing how words combine into larger structures
    (as in constituency parsing), or graph-based, capturing relationships between
    words (as in dependency parsing). Different NLP toolkits offer different query
    languages; CoreNLP [\[32\]](#page-22-4), for instance, provides TokensRegex for
    token-based regular expressions, Tregex for tree-based linguistic structures,
    and Semgrex for syntactic dependency patterns. To illustrate, we exemplify these
    query languages:


    - (a) The TokensRegex query [{tag:/VB.\*/}] extracts all spans tagged as verbs.

    - (b) The Tregex expression NP[<NN | <NNS] extracts all spans that are Noun Phrases
    (NPs) and immediately dominate a singular noun (NN) or a plural noun (NNS).

    - (c) The Semgrex query {pos:/VB.\*/} >nsubj {}=subject >obj {}=object extracts
    verbs that both have a subject and an object, alongside the subject and the object.


    Figure [4](#page-13-0) shows the results of the above queries as applied to the
    annotations in Fig. [3.](#page-7-0) All three queries serve a purpose in NLP4RE.
    Query (a) produces results that can be used as a feature for identifying requirements,
    grounded in the hypothesis that a higher verb count signifies a higher likelihood
    of requirements being present. Query (b) identifies constituent noun phrases and
    verb phrases within a requirements statement. This information is valuable for
    various purposes, one of which is to validate whether the statement conforms to
    a specific template, such as EARS [\[33\]](#page-22-9). Query (c) is useful for
    constructing a domain model [\[6\]](#page-20-2); the query extracts a probable
    association. In the case of our example, the association would be "[flight] simulator
    stores [log] messages".


    Query-based pattern matching techniques often face criticism related to both the
    scope of the study that informs query development and the applicability of the
    resulting queries beyond their initial purpose. To address these concerns, we
    recommend a


    ![](_page_13_Figure_0.jpeg)


    © The Allen Institute for Artificial Intelligence - All Rights Reserved | Privacy
    Policy | Terms of Use | Business Code of Conduct


    **Constituency Parsing**


    1


    1


    Penn Treebank, a 20% relative error reduction.


    1


    1


    1


    Demo Model Card Model Usage Example Inputs


    Model **Constituency Parser with ELMo embeddings**


    Answer a question Reading Comprehension Visual Question Answering Annotate a sentence
    Named Entity Recognition Open Information Extraction Sentiment Analysis Dependency
    Parsing


    Constituency parsing is the task of breaking a text into sub-phrases, or constituents.
    Non-terminals in the parse tree are types of phrases, the terminals are the words
    in the sentence.


    TokensRegex Semgrex Tregex


    **DT**


    **DT**


    **The**


    **The**


    This is an implementation of a minimal neural model for constituency parsing based
    on an independent scoring of labels and spans. This SpanConstituencyParser simply
    encodes a sequence of text with a stacked Seq2SeqEncoder , extracts span representations
    using a SpanExtractor , and then predicts a label for each span in the sequence.
    These labels are non-terminal nodes in a constituency parse tree, which we then
    greedily reconstruct. The model uses ELMo embeddings, which are completely character-based
    and improves single model performance from 92.6 F1 to 94.11 F1 on the


    The flight simulator shall store log messages in the database . DT NN NN MD VB
    NN NNS IN DT NN . compound aux compound det det nsubj obj case


    The flight simulator shall store log messages in the database . DT NN NN MD VB
    NN NNS IN DT NN . compound aux compound det det nsubj obj case


    The flight simulator shall store log messages in the database . DT NN NN MD VB
    NN NNS IN DT NN . compound aux compound det det nsubj obj case


    > obl punct


    obl punct


    obl:in punct


    The flight simulator shall store log messages in the database . DT NN NN MD VB
    NN NNS IN DT NN .


    **NP**


    **NN**


    **flight**


    The flight simulator shall store log messages in the database.


    The flight simulator shall store log messages in the database . DT NN NN MD VB
    NN NNS IN DT NN .


    The flight simulator shall store log messages in the database.


    parts-of-speech dependency parse constituency parse


    parts-of-speech dependency parse constituency parse


    **NP**


    **NN**


    **NN**


    **NN**


    **simulator**


    **MD**


    **MD**


    **shall**


    **— Annotations —**


    **— Annotations —**


    **shall**


    **simulator**


    **flight**


    Share


    Part-of-Speech:


    **VP**


    **— Text to annotate —**


    **ROOT**


    **S**


    Constituency Parse:


    Part-of-Speech:


    **— Text to annotate —**


    Constituency Parse:


    **ROOT**


    **S**


    **VP**


    **VB**


    **VB**


    **store**


    **store**


    **VP**


    **VP**


    **NNS**


    **.**


    **.**


    **.**


    **NNS**


    **messages**


    **.**


    **PP**


    **PP**


    **NP**


    **NP**


    **DT**


    **the**


    **DT**


    **NN**


    **NN**


    **database**


    **database**


    **the**


    **IN**


    **IN**


    **in**


    **— Language —**


    **— Language —**


    English Submit


    English Submit


    **in**


    **messages**


    **NP**


    **NP**


    **NN**


    **log**


    **NN**


    **log**


    Basic Dependencies:


    CoreNLP Tools:


    Basic Dependencies:


    Enhanced++ Dependencies:


    Enhanced++ Dependencies:


    **Enter a TokensRegex (http://nlp.stanford.edu/software/tokensregex.shtml) expression
    to run against the above sentence:**


    CoreNLP Tools:


    **dependencies" above:**


    Visualisation provided using the brat visualisation/annotation software (http://brat.nlplab.org/).
    Fig. 4 Illustration of Query-based Pattern Matching over the Annotations of Fig.
    [3.](#page-7-0)


    <span id="page-13-0"></span>Output


    two-fold approach. First, it is important for query designers to carefully document
    the query specification process, providing a clear explanation of the considered
    documents and domains. The primary objective should be to enhance empirical reliability,
    ensuring that the process is as reproducible as possible by others. Second and
    concerning generalizability, one should avoid using all the text under analysis
    for query development. This approach allows for the assessment of query generalizability
    on unseen text, resulting in a more credible evaluation. An even more robust strategy
    involves expanding the analysis beyond the immediate domain under study. For example,
    if the focus is on regulatory documents, it is common to derive queries in one
    legal jurisdiction, such as Europe, and subsequently evaluate query accuracy in
    texts from another jurisdiction, like Canada [\[43\]](#page-23-0). By adhering
    to these basic principles — detailed documentation of the query derivation process
    and assessments of generalizability — one can enhance the utility of query-based
    pattern matching techniques.


    #### Takeaway: Query-based Pattern Matching


    (1) When working with NLP annotations, select query language(s) that align with
    your specific analysis needs. (2) To ensure the reliability of query derivation,
    carefully document the steps you take to create the queries. This documentation
    should include the criteria, context, and content used for query design, making
    it easier for others to reproduce your work. (3) Refrain from using all the text
    you have for query development. To improve the applicability of your queries,
    evaluate their performance on unseen (or withheld) text or in different domains.


    ### <span id="page-14-0"></span>3.5 Feature-based Machine Learning


    Feature-based machine learning (ML) is concerned with using labelled data to train
    algorithms to make predictions based on predefined features. Feature-based ML
    has numerous applications in NLP4RE. For instance, it can be used for (1) categorizing
    input requirements as either functional or non-functional [\[27\]](#page-22-0),
    or (2) predicting the number of story points for user stories based on historical
    data [\[17\]](#page-21-9). The former application is a classification problem,
    entailing the assignment of discrete labels to data points ("functional" or "non-functional");
    whereas the latter scenario constitutes a regression problem as it involves estimating
    numeric values (story points).


    Building a feature-based ML model has five main steps. Below, we discuss these
    steps and outline some important practical considerations related to each step.


    - 1. Data collection involves gathering relevant and sufficient data, including
    the labelling process. This phase can be very time- and effort-intensive, particularly
    when manual labelling is required. To ensure consistency, it is crucial to define
    a clear and agreed-upon labelling task with protocols that annotators can consistently
    follow. To enhance internal validity, authors should avoid annotating their test
    data. When feasible, it is further important to employ multiple annotators with
    some overlap to enable the calculation of inter-annotator agreement. Detailed
    documentation of the data-collection protocol is essential for reliability [\[2,](#page-20-3)
    [3\]](#page-20-6).

    - 2. Data wrangling is concerned with handling missing data, noise removal (e.g.,
    stop words or redundant information), deciding about how to manage structured
    and unstructured data, and transforming results into the desired format. It is
    crucial to thoroughly document all techniques and decisions made in this step,
    as even minor changes can significantly impact the accuracy of the ML model [\[21\]](#page-21-10).

    - 3. Feature engineering is the process of defining relevant features for learning
    from the input data. In NLP4RE, features are usually defined over the outputs
    of the preprocessing techniques discussed in Sections [3.1–](#page-6-1)[3.3.](#page-10-0)
    For instance, a feature based on the NLP pipeline could be the number of verbs
    in a sentence. A feature using relevance measures might be the highest TF-IDF
    rank of a specific term across all articles in a corpus. Finally and in relation
    to embeddings, the components within an embedding vector can be regarded as features,
    as elaborated in Section [3.3.](#page-10-0)


    Features should be treated as hypotheses for addressing the problem at hand, and
    their usefulness must be evaluated. Techniques for computing feature importance,
    e.g., information gain [\[11\]](#page-21-11), and PCA analysis [\[1\]](#page-20-7),
    may be used for eliminating redundant or less significant features, thereby preventing
    overfitting. Feature engineering must also prioritize computational efficiency,
    particularly in real-time tasks such as live editing of requirements [\[2\]](#page-20-3),
    where almost instant feature computation is necessary. In such scenarios, certain
    computationally expensive features may need to be sacrificed, even if they offer
    marginal improvements. For example, the resource-intensive process of constituent
    parsing, despite its potential value, may prove impractical. Conducting a cost-effectiveness
    analysis, weighing the cost of computing each feature against its impact on the
    resulting ML model''s accuracy is therefore recommended.


    4. Model Selection, Tuning, and Training is concerned with choosing an appropriate
    ML algorithm, fine-tuning its hyperparameters, and training it over featureengineered
    data [\[22\]](#page-22-10). Initially, the computed feature data should be split
    into three sets: the training set (typically 70% of all the data), the validation
    set (usually 10% of the data) for model selection and hyperparameter optimization,
    and the test set (typically 20% of the data) to evaluate the trained ML model
    [\[39\]](#page-23-5). During dataset partitioning, it is paramount to avoid data
    leakage, which means avoiding any exposure of test set data during training and
    validation [\[42\]](#page-23-6). If the dataset covers multiple projects, it is
    advisable to ensure that the test set includes data from ''hold-out'' projects
    not involved in training or validation at all [\[39\]](#page-23-5).


    NLP4RE frequently encounters a scarcity of the kind of "abundant" data necessary
    for training complex ML algorithms, such as deep learning. In such instances,
    ML algorithms like Random Forest can provide scalable solutions, even when dealing
    with smaller datasets, noting of course that this advantage comes at the cost
    of requiring manual feature engineering.


    Model tuning is another vital step in the process. While hyperparameter optimization
    should theoretically be integrated with model selection, the number of combinations
    to evaluate when combining model selection and hyperparameter optimization can
    be exceedingly large [\[22\]](#page-22-10). For example, systematically exploring
    discrete values across key hyperparameters in the Random Forest algorithm can
    result in over 500 combinations. Given this challenge, a more practical approach
    would be to initially select a suitable ML algorithm by experimenting with multiple
    algorithms at their default settings, and then proceed to fine-tune the hyperparameters
    for the chosen algorithm. The choice of hyperparameter tuning strategy depends
    on available resources, such as an extensive grid search or a lightweight stochastic
    random search, for example.


    5. Evaluation involves assessing a trained ML model''s performance on a test set
    using metrics such as accuracy, precision, recall, F-score, and mean absolute
    error (MAE). Depending on the context, one can opt for "hold-out" test sets or
    k-fold crossvalidation. The selection and prioritization of metrics require special
    attention. For instance, in a binary classification task where 95% of samples
    belong to Class A and only 5% to Class B, a classifier exclusively predicting
    Class A would achieve 95% accuracy. Consequently, accuracy may not be a suitable
    metric when the dataset is imbalanced. Another notable point related to metrics
    is that, in NLP4RE, recall often outweighs precision in terms of importance, typically
    making recall a priority in solution development [\[13\]](#page-21-12). In relation
    to reporting, care needs to be taken when reporting aggregate metrics like F-score:
    such metrics should be reported alongside the source metrics (in this case, precision
    and recall) rather than as substitutes.


    Beyond reporting metrics, it is important to reflect on how these metrics translate
    into either benefits or drawbacks for users. For example, it is always valuable
    to contemplate the significance of various types of classification errors and
    determine their impact on users. When the cost of prediction errors is deemed
    too high, e.g., when the ML model''s predictions serve as recommendations requiring
    meticulous manual follow-up, one may consider implementing a human feedback loop
    to continuously retrain the model and reduce errors [\[8\]](#page-20-8).


    #### Takeaway: Feature-based ML


    Develop explicit procedures for manual data labelling during data collection and
    document these procedures. Keep track of data-wrangling decisions, as they can
    have a substantial impact on the outcomes. Approach features as hypotheses and
    confirm their significance before incorporating them into the final solution.
    Given the numerous combinations available for hyperparameter tuning during model
    selection and tuning, you may want to begin with experimentation using the default
    settings of machine learning algorithms. Ensure there is no data leakage, which
    means avoiding the inadvertent exposure of parts of test documents during the
    training and validation processes. The choice of evaluation metrics is of great
    importance and should be aligned with the specific NLP4RE task at hand.


    #### 3.6 Clustering Algorithms


    Clustering algorithms group similar data points into subsets or clusters to reveal
    patterns and structures within the data. This is achieved using a quantitative
    measure of similarity and ensuring that points in the same cluster are more similar
    to each other than to those in different clusters. In NLP4RE, relevance measures
    (Section [3.2\)](#page-8-0) and embeddings (Section [3.3\)](#page-10-0) are commonly
    used to compute similarity for clustering purposes. For instance, using GloVe
    embeddings, requirements statements can be transformed into vectors and then clustered
    using clustering algorithms such as Kmeans, agglomerative clustring or expectation
    maximization (EM) [\[46\]](#page-23-7). To illustrate, consider a system with
    six requirements: R1-R3 from Section [3.3](#page-10-0) and R4-R6 defined as follows:
    R4 = "The system shall allow users to customize the UI theme, available as ''dark''
    and ''light'' versions."; R5 = "The system shall allow users to sync data across
    multiple devices."; and R6 = "The system shall allow users to reset passwords
    only after email authentication.". Once sentence embeddings have been generated,
    cosine similarity between requirements pairs can be used as the basis for clustering.
    An illustrative clustering of these requirements, aimed at determining implementation
    responsibilities, might consist of [R1, R2] (system responsiveness), [R3, R6]
    (system protection), [R4] (system UI customization), and [R5] (system data management).
    Clustering has numerous applications in NLP4RE, including tasks such as traceability
    link retrieval, identifying overlapping or redundant requirements, and categorizing
    app reviews to pinpoint new feature requests. Below, we present some important
    practical considerations for an efficient utilization of clustering algorithms
    in NLP4RE.


    Determining the Number of Clusters (k). Choosing an appropriate number of clusters
    (k) is an important prerequisite for effectively applying many common clustering
    algorithms such as K-means and EM. Techniques like the Elbow method, Silhouette
    analysis, and Bayesian Information Criterion (BIC) can help estimate an appropriate
    k [\[46\]](#page-23-7). Another alternative is a recent summarization metric proposed
    in clustering app reviews [\[35\]](#page-23-8). Domain knowledge remains a key
    factor alongside these methods in deciding the number of clusters. The appropriate
    value of k is influenced by the task


    at hand and how users plan to utilize the resulting clusters. For instance, when
    clustering requirements terms, one may emphasize creating numerous small clusters,
    e.g., to simplify glossary construction [\[7\]](#page-20-1). On the other hand,
    when clustering core requirements within software product lines, it may be preferable
    to have a smaller number of clusters, as this streamlines the reviewing of common
    functions [\[40\]](#page-23-9).


    Hierarchical vs. Partitional vs. Soft Clustering. In partitional clustering algorithms,
    such as K-means, data is divided into non-overlapping clusters without any inherent
    structure. In contrast, hierarchical clustering creates a tree-like structure.
    Partitional methods are more suitable when the primary focus of the analysis is
    to rapidly identify overarching themes. For instance, Di Sorbo et al. [\[44\]](#page-23-10)
    adopt a partitional approach to categorize app reviews into pre-defined topics,
    determining the necessary changes in the apps according to user feedback. Hierarchical
    clustering, on the other hand, is better suited when there is a need to comprehend
    and visualize the relationships and nested structures within the dataset. For
    instance, Reinhartz-Berger and Kemelman [\[40\]](#page-23-9) use hierarchical
    clustering to explore the relationships between software product line requirements
    from different products by clustering the core requirements. It is worth noting
    that hierarchical clustering algorithms also provide mechanisms to create partitions.
    In soft clustering (also know as fuzzy clustering), each data point can belong
    to multiple clusters with varying degrees of membership. Soft clustering is ideal
    for situations where the boundaries between clusters are not rigidly defined,
    and instances may possess properties of multiple clusters. An example of this
    approach is the EM algorithm. An RE-related use case is clustering of requirements
    terms. In this context, it is beneficial to allow each term to have membership
    in different clusters [\[7\]](#page-20-1). For instance, the term "flight simulator"
    may find membership in both the "flight"-related and "simulator"-related clusters
    within a requirements document. Ultimately, the choice between partitional, hierarchical,
    or soft clustering depends on the specific task and analysis objectives at hand.
    Analysts should therefore understand the distinctions and potential applications
    of different types of clustering algorithms to ensure that their chosen algorithm
    aligns with the goals of their analysis.


    Evaluation. Cluster evaluation can be either internal or external. Internal evaluation
    assesses the quality of clusters without relying on external labels, focusing
    on the principles of cohesion and separation. Cohesion measures how closely related
    the members of the same cluster are, reflecting the compactness of the clusters,
    while separation assesses how distinct or well-separated a cluster is from others.
    A higher separation implies that clustering can more effectively distinguish between
    clusters. External evaluation, on the other hand, measures the quality of clusters
    by comparing them to a ground truth. This comparison can involve evaluating the
    overlap or mutual information between the original clusters and the generated
    ones. Developing a ground truth in clustering is challenging due to the inherent
    subjectivity of this task and its context dependence. Requirements and related
    artifacts can have multifaceted interpretations, leading to multiple valid ways
    of clustering based on different perspectives or objectives. Creating a good ground
    truth necessitates: (i) a clear understanding of the concept of clustering as
    well as the end goal to achieve, (ii) substantial time and effort, and (iii) vigilance
    against bias and inconsistency, as different experts may have


    contrasting opinions based on their experiences, potentially resulting in inconsistencies
    and posing threats to both internal and construct validity.


    #### Takeaway: Clustering Algorithms


    Important decisions in clustering include determining a suitable number of clusters
    (e.g., the Elbow method or Silhouette analysis), and opting between clustering
    types (partitional, hierarchical, or soft). The choice of clustering algorithm
    depends on the goal of the analysis and requires domain understanding. Evaluating
    the effectiveness of clustering is critical, which can be approached internally
    by assessing the cohesion and separation of clusters or externally by comparing
    against a ground truth. An external evaluation often poses challenges, as creating
    a ground truth for clustering is subjective and often requires substantial effort.


    ### 3.7 Large Language Models


    Language models (LMs) are statistical models that use neural networks to predict
    the next word in a sequence based on preceding words. For example, given the text
    "Paris is the capital of", a langauge model may predict the next word as "France".
    Language models increasingly serve as the foundation for various downstream NLP
    tasks such as text completion, translation, and summarization. Large language
    models (LLMs) like GPT-4 and BERT are scaled-up versions of the LM concept, with
    billions and sometimes even trillions of parameters. The descriptor "large" in
    LLM pertains to the model''s size in terms of the number of parameters. These
    parameters represent the neural network layers'' weights that the model acquires
    through training. Generally, larger models tend to perform better at comprehending
    context, drawing inferences and producing answers that resemble human responses.


    Hyperparameters. LLMs have a range of hyperparameters that can be adjusted during
    both training and fine-tuning. The hyperparameter profiles can vary among different
    LLMs, and the extent to which individual LLMs allow end-users to modify these
    hyperparameters also varies. Some common LLM hyperparameters include: (i) learning
    rate, determining how quickly the model adapts its weights during training; (ii)
    number of epochs, referring to how many times the model will go through the entire
    training dataset; (iii) batch size, denoting the number of training examples utilized
    in one iteration; (iv) sequence length, referring to the number of tokens that
    the model reads in one go; (v) dropout rate, referring to the fraction of input
    units to drop during training to mitigate overfitting; and (vi) temperature, a
    parameter that modulates the probability distribution over the predicted words,
    making the outputs more focused (lower values) or more random (higher values).


    Extensive experimentation across a broad range of LLM hyperparameter combinations
    currently presents challenges due to constraints in both time and budget. Nevertheless,
    it remains essential to select a practical subset of hyperparameter combinations
    that can be explored within the available resources. This approach enables a more
    informed understanding of how hyperparameters influence the performance of an
    LLM in conducting a specific task.


    Prompting. Prompting refers to providing a specific input or query to guide a
    generative AI model in producing the desired output, such as text or images [\[25\]](#page-22-11).
    Prompts, which are concise textual inputs given to generative AI models, including
    LLMs, convey information about the specific task that the model is expected to
    execute.


    Creating prompts that are effective in eliciting the desired output from an LLM
    requires good prompt engineering. Prompt engineering involves the selection of
    appropriate prompt patterns and prompting techniques [\[45\]](#page-23-11). Prompt
    patterns encompass various templates tailored for specific objectives. For example,
    the output customization pattern focuses on refining the format or structure of
    LLM-generated output, with the LLM often adopting a particular persona (role)
    while generating the output. Prompting techniques, on the other hand, are strategies
    to extract the best possible output from LLMs. Example prompting techniques include
    zero-shot prompting, few-shot prompting, chain-of-thought prompting, and tree-of-thought
    prompting [\[29\]](#page-22-12).


    When conducting empirical examinations of LLM-based solutions, it is important
    to take into account the alternative choices that one can make during prompt engineering.
    These alternatives should ideally be compared through empirical means. Nonetheless,
    much like the challenges encountered when exploring hyperparameters, the vast
    array of possible combinations of prompt patterns and prompting strategies may
    be too numerous to thoroughly investigate. Another important aspect to consider
    is that even minor alterations in prompts can lead to considerable variation in
    the outputs generated by LLMs. In view of this, empirical studies should also
    assess prompt robustness by examining multiple variants of the same prompt.


    Fine-tuning. While LLMs come pre-trained on very large corpora, they often require
    some level of fine-tuning to specialize them for particular tasks or domains.
    When the requirements automation task at hand aligns closely with common knowledge,
    such as ambiguity handling, one might achieve good results with little or no fine-tuning.
    However, if the task is specialized and involves RE-specific semantics, such as
    requirements classifications, fine-tuning often becomes necessary to ensure accurate
    results. When fine-tuning an LLM, one needs to be cognizant of the non-deterministic
    nature of the process, which is influenced by factors such as random initialization
    and regularization. Depending on the extent and diversity of the fine-tuning data,
    significant variations may be observed in results across different fine-tuning
    attempts. To account for this randomness, we recommend fine-tuning LLMs multiple
    times and reporting average results rather than relying on the outcome of a single
    fine-tuned model.


    #### Takeaway: Large Language Models


    When working with LLMs, explore hyperparameters within resource constraints. Invest
    in prompt engineering, using suitable prompt patterns and techniques, and empirically
    compare different alternatives. Take note of the non-deterministic nature of fine-tuning,
    and present results averaged over multiple fine-tuning runs.


    # <span id="page-20-4"></span>4 Summary and Conclusion


    The main goal of this chapter was to aid newcomers in the selection of appropriate
    NLP4RE techniques and the application of some essential principles for their evaluation.
    We acknowledge the wide array of NLP technologies employed in RE. Not all NLP4RE
    approaches may neatly align with the general, and sometimes simplified, framework
    we have outlined in this chapter.


    This chapter was written during a transformative period in the NLP field, spurred
    by the emergence of generative AI and large language models. We hope that this
    chapter can serve as a stepping stone for quickly grasping the NLP technologies
    most relevant to RE. With both the NLP and RE landscapes constantly evolving,
    our hope is to maintain this chapter as a living document, continuously integrating
    emerging trends and pertinent techniques for automated requirements analysis.


    Acknowledgements.The first author is grateful for the financial support provided
    by the Natural Sciences and Engineering Research Council of Canada (NSERC) through
    the Discovery and Discovery Accelerator programs.


    # References


    - <span id="page-20-7"></span>[1] Abdi H, Williams LJ (2010) Principal component
    analysis. Wiley interdisciplinary reviews: computational statistics 2(4):433–459.
    URL [https://api.semanticscholar.](https://api.semanticscholar.org/CorpusID:122379222)
    [org/CorpusID:122379222](https://api.semanticscholar.org/CorpusID:122379222)

    - <span id="page-20-3"></span>[2] Abualhaija S, Arora C, Sabetzadeh M, et al (2020)
    Automated demarcation of requirements in textual specifications: a machine learning-based
    approach. Empirical Software Engineering 25(6):5454–5497. [https://doi.org/10.](https://doi.org/10.1007/S10664-020-09864-1)
    [1007/S10664-020-09864-1](https://doi.org/10.1007/S10664-020-09864-1)

    - <span id="page-20-6"></span>[3] Abualhaija S, Arora C, Sleimi A, et al (2022)
    Automated question answering for improved understanding of compliance requirements:
    A multi-document study. In: 30th IEEE International Requirements Engineering Conference,
    RE, pp 39–50, <https://doi.org/10.1109/RE54965.2022.00011>

    - <span id="page-20-5"></span>[4] Apache OpenNLP (2006) Apache. URL [http://opennlp.apache.org,](http://opennlp.apache.org)
    last accessed: October 2023

    - <span id="page-20-0"></span>[5] Arora C, Sabetzadeh M, Briand L, et al (2015)
    Automated checking of conformance to requirements templates using natural language
    processing. IEEE Trans Software Eng 41(10):944–968. <https://doi.org/10.1109/tse.2015.2428709>

    - <span id="page-20-2"></span>[6] Arora C, Sabetzadeh M, Briand L, et al (2016)
    Extracting domain models from natural-language requirements: approach and industrial
    evaluation. In: Baudry B, Combemale B (eds) Proceedings of the ACM/IEEE 19th International
    Conference on Model Driven Engineering Languages and Systems, MODELS, pp 250–260,
    <https://doi.org/10.1145/2976767.2976769>

    - <span id="page-20-1"></span>[7] Arora C, Sabetzadeh M, Briand L, et al (2017)
    Automated extraction and clustering of requirements glossary terms. IEEE Trans
    Software Eng 43(10):918–945. <https://doi.org/10.1109/tse.2016.2635134>

    - <span id="page-20-8"></span>[8] Arora C, Sabetzadeh M, Briand LC (2019) An empirical
    study on the potential usefulness of domain models for completeness checking of
    requirements. Empir Softw Eng 24(4):2509–2539. <https://doi.org/10.1007/S10664-019-09693-X>

    - <span id="page-21-2"></span>[9] Arora C, Grundy J, Abdelrazek M (2023) Advancing
    requirements engineering through generative ai: Assessing the role of LLMs. CoRR
    abs/2310.13976. [https:](https://doi.org/10.48550/ARXIV.2310.13976) [//doi.org/10.48550/ARXIV.2310.13976](https://doi.org/10.48550/ARXIV.2310.13976)

    - <span id="page-21-7"></span>[10] Arora S, Liang Y, Ma T (2017) A simple but
    tough-to-beat baseline for sentence embeddings. In: 5th International Conference
    on Learning Representations, ICLR, URL <https://openreview.net/forum?id=SyK00v5xx>

    - <span id="page-21-11"></span>[11] Azhagusundari B, Thanamani AS, et al (2013)
    Feature selection based on information gain. International Journal of Innovative
    Technology and Exploring Engineering 2(2):18–21. URL <https://api.semanticscholar.org/CorpusID:212611078>

    - <span id="page-21-5"></span>[12] Baeza-Yates RA, Ribeiro-Neto B (1999) Modern
    information retrieval. Addison-Wesley Longman Publishing Co., Inc., USA, <https://doi.org/10.5555/553876>

    - <span id="page-21-12"></span>[13] Berry DM (2021) Empirical evaluation of tools
    for hairy requirements engineering tasks. Empir Softw Eng 26(5):111. <https://doi.org/10.1007/S10664-021-09986-0>

    - <span id="page-21-4"></span>[14] de Castilho RE, Gurevych I (2014) A broad-coverage
    collection of portable NLP components for building shareable analysis pipelines.
    In: Ide N, Grivolla J (eds) Proceedings of the Workshop on Open Infrastructures
    and Analysis Frameworks for HLT, OIAF4HLT@COLING, pp 1–11, <https://doi.org/10.3115/V1/W14-5201>

    - <span id="page-21-8"></span>[15] Chalkidis I, Fergadiotis M, Malakasiotis P,
    et al (2020) LEGAL-BERT: the muppets straight out of law school. CoRR abs/2010.02559.
    URL [https://arxiv.org/](https://arxiv.org/abs/2010.02559) [abs/2010.02559](https://arxiv.org/abs/2010.02559)

    - <span id="page-21-3"></span>[16] Chen B, Chen K, Hassani S, et al (2023) On
    the use of GPT-4 for creating goal models: An exploratory study. In: Schneider
    K, Dalpiaz F, Horkoff J (eds) 31st IEEE International Requirements Engineering
    Conference, RE, Workshops, pp 262–271, <https://doi.org/10.1109/REW57809.2023.00052>

    - <span id="page-21-9"></span>[17] Choetkiertikul M, Dam HK, Tran T, et al (2019)
    A deep learning model for estimating story points. IEEE Trans Software Eng 45(7):637–656.
    [https://doi.](https://doi.org/10.1109/TSE.2018.2792473) [org/10.1109/TSE.2018.2792473](https://doi.org/10.1109/TSE.2018.2792473)

    - <span id="page-21-6"></span>[18] Devlin J, Chang M, Lee K, et al (2019) BERT:
    pre-training of deep bidirectional transformers for language understanding. In:
    Burstein J, Doran C, Solorio T (eds) Proceedings of the 2019 Conference of the
    North American Chapter of the Association for Computational Linguistics: Human
    Language Technologies, NAACL-HLT, pp 4171–4186, <https://doi.org/10.18653/V1/N19-1423>

    - <span id="page-21-0"></span>[19] Ezzini S, Abualhaija S, Arora C, et al (2022)
    Automated handling of anaphoric ambiguity in requirements: A multi-solution study.
    In: XXXX (ed) 44th IEEE/ACM 44th International Conference on Software Engineering,
    ICSE, pp 187–199, <https://doi.org/10.1145/3510003.3510157>

    - <span id="page-21-1"></span>[20] Ezzini S, Abualhaija S, Arora C, et al (2023)
    AI-based question answering assistance for analyzing natural-language requirements.
    In: XXXX (ed) 45th IEEE/ACM International Conference on Software Engineering,
    ICSE, pp 1277–1289, <https://doi.org/10.1109/ICSE48619.2023.00113>

    - <span id="page-21-10"></span>[21] Fan Y, Arora C, Treude C (2023) Stop words
    for processing software engineering documents: Do they matter? In: 2nd IEEE/ACM
    International Workshop on Natural Language-Based Software Engineering, NLBSE@ICSE.
    IEEE, pp 40–47, <https://doi.org/10.1109/NLBSE59153.2023.00016>


    - <span id="page-22-10"></span>[22] Feurer M, Hutter F (2019) Hyperparameter Optimization,
    Springer International Publishing, Cham, pp 3–33. [https://doi.org/10.1007/978-3-030-05318-5](https://doi.org/10.1007/978-3-030-05318-5_1)
    1

    - <span id="page-22-3"></span>[23] GATE NLP Workbench (2020) GATE. URL [http://gate.ac.uk/,](http://gate.ac.uk/)
    last accessed: October 2023

    - <span id="page-22-5"></span>[24] Gomaa WH, Fahmy AA (2013) A survey of text
    similarity approaches. International Journal of Computer Applications 68(13):13–18.
    [https://doi.org/10.5120/](https://doi.org/10.5120/11638-7118) [11638-7118](https://doi.org/10.5120/11638-7118)

    - <span id="page-22-11"></span>[25] Hariri W (2023) Unlocking the potential of
    chatgpt: A comprehensive exploration of its applications, advantages, limitations,
    and future directions in natural language processing. CoRR abs/2304.02017. [https://doi.org/10.48550/ARXIV.2304.](https://doi.org/10.48550/ARXIV.2304.02017)
    [02017](https://doi.org/10.48550/ARXIV.2304.02017)

    - <span id="page-22-1"></span>[26] Jain C, Anish PR, Singh A, et al (2023) A transformer-based
    approach for abstractive summarization of requirements from obligations in software
    engineering contracts. In: Schneider K, Dalpiaz F, Horkoff J (eds) Proceedings
    of the 31st IEEE International Requirements Engineering Conference, (RE''23),
    pp 169–179, <https://doi.org/10.1109/RE57278.2023.00025>

    - <span id="page-22-0"></span>[27] Kurtanovic Z, Maalej W (2017) Automatically
    classifying functional and nonfunctional requirements using supervised machine
    learning. In: Moreira A, Ara´ujo J, Hayes J, et al (eds) 25th IEEE International
    Requirements Engineering Conference, RE, pp 490–495, <https://doi.org/10.1109/RE.2017.82>

    - <span id="page-22-7"></span>[28] Lee J, Yoon W, Kim S, et al (2020) Biobert:
    a pre-trained biomedical language representation model for biomedical text mining.
    Bioinform 36(4):1234–1240. <https://doi.org/10.1093/BIOINFORMATICS/BTZ682>

    - <span id="page-22-12"></span>[29] Liu P, Yuan W, Fu J, et al (2023) Pre-train,
    prompt, and predict: A systematic survey of prompting methods in natural language
    processing. ACM Comput Surv 55(9):195:1–195:35. <https://doi.org/10.1145/3560815>

    - <span id="page-22-8"></span>[30] L´opez JAH, Dur´a C, Cuadrado JS (2023) Word
    embeddings for model-driven engineering. In: 2023 ACM/IEEE 26th International
    Conference on Model Driven Engineering Languages and Systems (MODELS), pp 151–161,
    [https://doi.org/](https://doi.org/10.1109/MODELS58315.2023.00036) [10.1109/MODELS58315.2023.00036](https://doi.org/10.1109/MODELS58315.2023.00036)

    - <span id="page-22-2"></span>[31] Luitel D, Hassani S, Sabetzadeh M (2023) Using
    language models for enhancing the completeness of natural-language requirements.
    In: Ferrari A, Penzenstadler B (eds) Requirements Engineering: Foundation for
    Software Quality - 29th International Working Conference, REFSQ, pp 87–104, [https://doi.org/10.1007/](https://doi.org/10.1007/978-3-031-29786-1_7)
    [978-3-031-29786-1](https://doi.org/10.1007/978-3-031-29786-1_7) 7

    - <span id="page-22-4"></span>[32] Manning CD, Surdeanu M, Bauer J, et al (2014)
    The stanford corenlp natural language processing toolkit. In: Proceedings of the
    52nd Annual Meeting of the Association for Computational Linguistics, ACL, pp
    55–60, [https://doi.org/10.](https://doi.org/10.3115/V1/P14-5010) [3115/V1/P14-5010](https://doi.org/10.3115/V1/P14-5010)

    - <span id="page-22-9"></span>[33] Mavin A (2012) Listen, then use EARS. IEEE
    Softw 29(2):17–18. [https://doi.](https://doi.org/10.1109/MS.2012.36) [org/10.1109/MS.2012.36](https://doi.org/10.1109/MS.2012.36)

    - <span id="page-22-6"></span>[34] Mikolov T, Chen K, Corrado G, et al (2013)
    Efficient estimation of word representations in vector space. In: Bengio Y, LeCun
    Y (eds) 1st International Conference on Learning Representations, ICLR, URL <http://arxiv.org/abs/1301.3781>

    - <span id="page-23-8"></span>[35] Nema P, Anthonysamy P, Taft N, et al (2022)
    Analyzing user perspectives on mobile app privacy at scale. In: 44th IEEE/ACM
    44th International Conference on Software Engineering, ICSE, pp 112–124, [https://doi.org/10.1145/3510003.](https://doi.org/10.1145/3510003.3510079)
    [3510079](https://doi.org/10.1145/3510003.3510079)

    - <span id="page-23-3"></span>[36] Pennington J, Socher R, Manning CD (2014) Glove:
    Global vectors for word representation. In: Moschitti A, Pang B, Daelemans W (eds)
    Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing,
    EMNLP, pp 1532–1543, <https://doi.org/10.3115/V1/D14-1162>

    - <span id="page-23-2"></span>[37] Princeton University (2010) About WordNet.
    URL [https://wordnet.princeton.](https://wordnet.princeton.edu/) [edu/,](https://wordnet.princeton.edu/)
    last accessed: March 2019

    - <span id="page-23-4"></span>[38] Radford A, Narasimhan K, Salimans T, et al
    (2018) Improving language understanding by generative pre-training. OpenAI URL
    [https://api.semanticscholar.](https://api.semanticscholar.org/CorpusID:49313245)
    [org/CorpusID:49313245](https://api.semanticscholar.org/CorpusID:49313245)

    - <span id="page-23-5"></span>[39] Raschka S (2018) Model evaluation, model selection,
    and algorithm selection in machine learning. CoRR abs/1811.12808. URL <http://arxiv.org/abs/1811.12808>

    - <span id="page-23-9"></span>[40] Reinhartz-Berger I, Kemelman M (2020) Extracting
    core requirements for software product lines. Requir Eng 25(1):47–65. [https://doi.org/10.1007/](https://doi.org/10.1007/S00766-018-0307-0)
    [S00766-018-0307-0](https://doi.org/10.1007/S00766-018-0307-0)

    - <span id="page-23-1"></span>[41] Salda˜na J (2015) The coding manual for qualitative
    researchers. Sage Publications, Thousand Oaks, CA, USA, URL [https://us.sagepub.com/en-us/nam/](https://us.sagepub.com/en-us/nam/the-coding-manual-for-qualitative-researchers/book273583)
    [the-coding-manual-for-qualitative-researchers/book273583](https://us.sagepub.com/en-us/nam/the-coding-manual-for-qualitative-researchers/book273583)

    - <span id="page-23-6"></span>[42] Shabtai A, Elovici Y, Rokach L (2012) A Survey
    of Data Leakage Detection and Prevention Solutions. Springer, <https://doi.org/10.1007/978-1-4614-2053-8>

    - <span id="page-23-0"></span>[43] Sleimi A, Sannier N, Sabetzadeh M, et al (2021)
    An automated framework for the extraction of semantic legal metadata from legal
    texts. Empir Softw Eng 26(3):43. <https://doi.org/10.1007/S10664-020-09933-5>

    - <span id="page-23-10"></span>[44] Sorbo AD, Panichella S, Alexandru CV, et al
    (2016) What would users change in my app? summarizing app reviews for recommending
    software changes. In: Zimmermann T, Cleland-Huang J, Su Z (eds) Proceedings of
    the 24th ACM SIG-SOFT International Symposium on Foundations of Software Engineering,
    FSE, pp 499–510, <https://doi.org/10.1145/2950290.2950299>

    - <span id="page-23-11"></span>[45] White J, Fu Q, Hays S, et al (2023) A prompt
    pattern catalog to enhance prompt engineering with chatgpt. CoRR abs/2302.11382.
    [https://doi.org/10.](https://doi.org/10.48550/ARXIV.2302.11382) [48550/ARXIV.2302.11382](https://doi.org/10.48550/ARXIV.2302.11382)

    - <span id="page-23-7"></span>[46] Witten IH, Frank E, Hall MA (2011) Data mining:
    practical machine learning tools and techniques, 3rd Edition. Morgan Kaufmann,
    Elsevier, URL [https://](https://www.worldcat.org/oclc/262433473) [www.worldcat.org/oclc/262433473](https://www.worldcat.org/oclc/262433473)'
  decisions:
    language: '- Qualified. Reason: English Paper'
    evaluation_prompt: Disqualified
    related_work_prompt: Qualified
    novelty_prompt: Disqualified
    review_only_prompt: Qualified
  llm_input_used: '## Abstract

    Natural Language Processing (NLP) is now a cornerstone of requirements

    automation. One compelling factor behind the growing adoption of NLP in

    Requirements Engineering (RE) is the prevalent use of natural language (NL) for

    specifying requirements in industry. NLP techniques are commonly used for

    automatically classifying requirements, extracting important information, e.g.,

    domain models and glossary terms, and performing quality assurance tasks, such

    as ambiguity handling and completeness checking. With so many different NLP

    solution strategies available and the possibility of applying machine learning

    alongside, it can be challenging to choose the right strategy for a specific RE

    task and to evaluate the resulting solution in an empirically rigorous manner.

    In this chapter, we present guidelines for the selection of NLP techniques as

    well as for their evaluation in the context of RE. In particular, we discuss

    how to choose among different strategies such as traditional NLP, feature-based

    machine learning, and language-model-based methods. Our ultimate hope for this

    chapter is to serve as a stepping stone, assisting newcomers to NLP4RE in

    quickly initiating themselves into the NLP technologies most pertinent to the

    RE field.


    ## Introduction

    NLP''s role in requirements automation is pivotal, due to the widespread use of
    natural language (NL) in industrial requirements specifications. Historically,
    NL has posed challenges for requirements analysis because of its inherent proneness
    to defects such as incompleteness and ambiguity. Recent breakthroughs in NLP,
    e.g., the emergence of large language models, have nonetheless drastically enhanced
    our ability to automatically analyze textual information. This development is
    poised to even further amplify the adoption and influence of NL in requirements
    engineering.


    Due to the rapid advancement of NLP, newcomers to NLP4RE may feel overwhelmed
    by the numerous potentially applicable technologies. Another challenge is the
    necessity to empirically assess a proposed automation solution, ensuring proper
    optimization, and, where applicable, improve performance over existing solutions.


    Over the past several years, we have studied various requirements automation problems,
    including checking compliance with requirements templates [\[5\]](#page-20-0),
    glossary construction [\[7\]](#page-20-1), model extraction [\[6\]](#page-20-2),
    requirements demarcation [\[2\]](#page-20-3), ambiguity handling [\[19\]](#page-21-0),
    and question answering [\[20\]](#page-21-1). With the benefit of hindsight, this
    chapter aims to reflect on our research process and offer our collective insights
    into how we approach NLP4RE problems. Before we start, we need to emphasize that
    our perspective is retrospective. Given the fast pace of progress in NLP technologies,
    new considerations may surface, and existing technologies could become outdated.
    Therefore, it is important for readers to consider the time this chapter was written
    (2023) when dealing with new technologies. This advice applies to most works in
    the fast-changing field of Applied AI.


    Structure. Section [2](#page-1-0) outlines the steps for automating pre-processing,
    analysis, and post-processing in NLP4RE. Section [3](#page-6-0) describes various
    NLP techniques and discusses their key considerations for automation in RE. Finally,
    Section [4](#page-20-4) summarizes the chapter and presents conclusions.'
  token_usage: 3531
  time_usage: 2.123293399810791
- title: Automated Test Production -- Systematic Literature Review
  abstract: 'Identifying the main contributions related to the Automated Test Production

    (ATP) of Computer Programs and providing an overview about models,

    methodologies and tools used for this purpose is the aim of this Systematic

    Literature Review (SLR). The results will enable a comprehensive analysis and

    insight to evaluate their applicability. A previously produced Systematic

    Literature Mapping (SLM) contributed to the formulation of the ``Research

    Questions'''' and parameters for the definition of the qualitative analysis

    protocol of this review.'
  url: http://arxiv.org/abs/2401.02033v1
  keywords: ''
  document: "## Automated Test Production Systematic Literature Review\n\nGomes, J.M.\
    \ <sup>1</sup> and Dias, L.A.V. 1\n\n1 Instituto Tecnol´ogico de Aeron´autica\
    \ - ITA\n\nJanuary 5, 2024\n\n#### Abstract\n\nIdentifying the main contributions\
    \ related to the Automated Test Production of Computer Programs and providing\
    \ an overview about models, methodologies and tools used for this purpose is the\
    \ aim of this Systematic Literature Review. The results will enable a comprehensive\
    \ analysis and insight to evaluate their applicability. A previously produced\
    \ SLM (Systematic Literature Mapping) contributed to the formulation of the \"\
    Research Questions\" and parameters for the definition of the qualitative analysis\
    \ protocol of this review.\n\n## 1 Objectives\n\nThe broader goal of this research,\
    \ while on the one hand is to obtain the State of the Art in ATP (Automated Test\
    \ Production), find the problems faced and track the progress of researchers in\
    \ the field, on the other hand we also intend to list and categorize the ATP methods,\
    \ techniques and tools that meet the needs of professionals producing specialized\
    \ business applications for internal use within their corporations - eventually\
    \ extending to the needs of professionals in companies specializing in the production\
    \ of generic or even academic computer applications.\n\nProviding the scientific\
    \ and technological community with an overview of models, methodologies and tools\
    \ used for this purpose is the goal of this SLR (Systematic Literature Review).\
    \ The results will allow a comprehensive analysis and insight to evaluate their\
    \ applicability.\n\n## <span id=\"page-0-0\"></span>2 Systematic Literature Review\n\
    \n### 2.1 Planning\n\nIn order to analyze, evaluate and interpret the available\
    \ research for the matter of ATP of Computer Programs, we applied the method proposed\
    \ by Brereton et\n\n![](_page_1_Figure_0.jpeg)\n\n<span id=\"page-1-0\"></span>Figure\
    \ 1: Steps to perform an SLR (adaptedo from [\\[1\\]](#page-13-0))\n\nal. and\
    \ presented in the Figure [1.](#page-1-0) Whereas the Systematic Literature Mapping[\\\
    [2\\]](#page-13-1) survey gave us an overview and insight into what represents\
    \ current research in ATP, with the present SLR we aim to identify the effort\
    \ needed to answer our research questions[\\[1\\]](#page-13-0).\n\nSeveral adjustments\
    \ suggested by Kitchenham et al. led us to consider the methodologies of Basili\
    \ and Weiss for formulating the research questions, of Chen and Babar for classifying\
    \ the type of study, of Keele et al. for conducting a quality assessment of each\
    \ proposal, and of Kitchenham et al., Dyba et al. for gauging the scientific rigor\
    \ of the studies obtained and in accordance with criteria of interest to our research\
    \ [\\[3,](#page-13-2) [4,](#page-13-3) [5,](#page-13-4) [6,](#page-14-0) [7,](#page-14-1)\
    \ [8\\]](#page-14-2).\n\nWe defined the questions that this research intends to\
    \ answer by following the Basili and Weiss Basili and Weiss - a systematic method\
    \ for organizing measurements. The method begins by specifying an objective (purpose,\
    \ object, problem, point of view). The goal is refined into several questions,\
    \ each of which in turn is refined into Basili and Weiss metrics. Providing the\
    \ answers to the questions, the data can be analyzed to identify whether or not\
    \ the objectives were met [\\[4\\]](#page-13-3).\n\nThus, the goal of this SLR\
    \ is:\n\n- Purpose Understand and characterize ...\n\t- Problem ... the solutions\
    \ in ATP ...\n\t\t- ∗ Object ... of Computer Program Testing ...\n\t\t\t- · Point\
    \ of view ... used by researchers and informatics professionals.\n\nBased on the\
    \ above stated goal, we derive the questions to be answered (see Table [1\\)](#page-2-0).\n\
    \n| #     | Question                                           |  |\n|-------|----------------------------------------------------|--|\n\
    | QP1   | What is the model, process, framework or tool      |  |\n|       | used\
    \ for test production?                          |  |\n| QP1.1 | What type of test\
    \ is generated?                    |  |\n| QP1.2 | What test production techniques\
    \ were applied?      |  |\n| QP1.3 | What tools does the study employ?       \
    \           |  |\n| QP1.4 | What are the prerequisites for its application?  \
    \  |  |\n| QP1.5 | What types of studies or evaluations have been con |  |\n|\
    \       | ducted?                                            |  |\n\n<span id=\"\
    page-2-0\"></span>Table 1: Research questions from SLR\n\nThe question QP1 allows\
    \ us to categorize the approaches and learn about the applicability of the study\
    \ proposal and meets the requirement of identifying solutions to the problem.\n\
    \n| Name                                                                     \
    \                                    |  |  |  |\n|--------------------------------------------------------------------------------------------------------------|--|--|--|\n\
    | IEEE Xplore<br>ACM Digital Library<br>Google Scholar<br>CiteSeerX<br>Inspec<br>ScienceDirect<br>EI\
    \ Compendex |  |  |  |\n| Springer Link                                      \
    \                                                          |  |  |  |\n\n<span\
    \ id=\"page-2-1\"></span>Table 2: Scientific publication bases\n\n### 2.2 Conduction\n\
    \nThe studies obtained for conducting this SLR were obtained from the scientific\
    \ publication sources listed in the Table [2](#page-2-1) and selected in our SLM[\\\
    [2\\]](#page-13-1).\n\nThe selection of documents was based on Inclusion and Exclusion\
    \ Criteria defined iteratively during the reading of the documents found and provided\
    \ to determine the suitability of each to the objectives of this work. The Inclusion\
    \ Criteria are those presented in the Table [3,](#page-2-2) and in the Table [4](#page-3-0)\
    \ we have the Exclusion Criteria.\n\n| #   | Descri¸c˜ao                     \
    \                                                      |\n|-----|---------------------------------------------------------------------------------------|\n\
    | CI1 | The Study is a review of current<br>models/processes/frameworks      \
    \                 |\n| CI2 | The study is useful for a better understanding of\
    \ the<br>problem                      |\n| CI3 | The study discusses one or more\
    \ tools within a process<br>applicable to this research |\n| CI4 | The study proposes\
    \ a<br>model/process/framework/methodology                           |\n\n<span\
    \ id=\"page-2-2\"></span>Table 3: Inclusion criteria for the SLR\n\n| CE1<br>The\
    \ study presents some<br>model/process/framework/methodology but does not<br>provide\
    \ enough information about its use<br>CE2<br>The study does not contain any kind\
    \ of evaluation for<br>the demonstration of results, such as case studies,<br>experiments,\
    \ or examples of use<br>CE3<br>The study is not directly related to this research\
    \ | # | Descri¸c˜ao |\n|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---|-------------|\n\
    |                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                       |   |             |\n|               \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                        |   |             |\n|                              \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \         |   |             |\n|                                             \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                      |   | \
    \            |\n|                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                       |   |             |\n\
    |                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                       |   |             |\n|               \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                        |   |             |\n\n<span id=\"page-3-0\"></span>Table\
    \ 4: Exclusion criteria from SLR\n\nGiven the large number of studies[\\[2\\]](#page-13-1)\
    \ found we organized our work into iterative steps:\n\n- 1. Reading the summary\
    \ and conclusion; and\n- 2. Selection by reading the entire document.\n\n| Name\
    \                 | Qty. | %     |\n|----------------------|------|-------|\n\
    | IEEE Xplore          | 33   | 14.73 |\n| ACM Digital Library  | 66   | 29.46\
    \ |\n| Springer Link        | 11   | 4.91  |\n| Peri´odicos da CAPES | 114  |\
    \ 50.89 |\n| Sub-total            | 224  |       |\n| Duplicate studies    | 7\
    \    | 4.24  |\n| Rejected studies     | 52   | 31.52 |\n| TOTAL             \
    \   | 165  |       |\n\n<span id=\"page-3-1\"></span>Table 5: Result of the selection\
    \ of primary studies for the SLR\n\nAfter reading the abstract of the pre-selected\
    \ publications, if they are interesting, we also read the conclusion, and if the\
    \ paper remains promising to our expectation it is then selected (see Figure [1](#page-1-0)\
    \ and Table [5\\[](#page-3-1)[2\\]](#page-13-1)). The selection criteria at this\
    \ stage become slightly stricter and we are now looking for model-oriented papers,\
    \ methods, techniques and tools that generate tests for general-purpose computer\
    \ applications and we avoid for example:\n\n- Texts that deal with test generation\
    \ but never in an automated or semiautomated way;\n- Tests applied to computing\
    \ devices or equipment of specific use such as those of industrial, embedded and\
    \ specialized use such as automotive and so on;\n- Tests related to a specific\
    \ or particular application such as a database manager or a commercial application,\
    \ etc;\n- Tests aimed at a specific use of a technology or facility, such as geo-location\
    \ or the use of gestures on touch screens, etc;\n- Testing targeted at a particular\
    \ industry or segment of the economy or society, such as healthcare, banking and\
    \ finance, trade, etc.; and\n\n• Tests oriented to the detection of non-functional\
    \ flaws, such as security, performance, etc.\n\nOf course, despite the rigor we\
    \ seek to conduct this research, exceptions will be made for the sake of completeness\
    \ and common sense, and we may include studies, which applied to the technologies\
    \ or segments listed above, are part of a case study within a larger context.\n\
    \nA difficult decision to make at this point is whether or not to discriminate\
    \ the application of tests on a given platform. Nowadays we see the proliferation\
    \ of the use of computing resources in a pervasive way, and we have computing\
    \ devices on our desks, in our pockets, like jewelry on our wrists, and even in\
    \ our home appliances [\\[9\\]](#page-14-3). And this is just one of the aspects\
    \ we observe directly, since the computing itself can take place inside the device\
    \ locally, or remotely on servers connected to the Internet, or even in hybrid\
    \ mode. Following the criteria listed above, we will discriminate particular or\
    \ niche or proprietary uses of certain manufacturers and not consider them, but\
    \ we must include the cases where these platforms are extremely widespread and\
    \ have a wide range of applications developed on them, such as those used in cell\
    \ phone handsets for example.\n\nAfter this preliminary selection we will proceed\
    \ with a more careful reading of the complete document that will generate notes\
    \ that will be useful in this work.\n\nIn the full reading of the documents obtained\
    \ after the first and second search criteria, the selection becomes a qualitative\
    \ analysis where each document is evaluated according to quality criteria and\
    \ scored, a method proposed by Keele et al. and which we used to evaluate quality\
    \ of studies for this review [\\[6\\]](#page-14-0). These new criteria, which\
    \ we call Quality Criteria, are based on the research questions listed in the\
    \ Table [1](#page-2-0) and whose answers will be mapped into points. The sum of\
    \ the points obtained by each document will give us a score, within which we define\
    \ a cut-off value that will select or eliminate the document.\n\nThe questions\
    \ defined for this selection phase are those listed in the Table [6.](#page-4-0)\n\
    \n| #     | Description                  | Questions    |\n|-------|------------------------------|--------------|\n\
    | CQ1   | Describe the model, process, |              |\n|       | framework or\
    \ tool used or    |              |\n|       | created?                     | \
    \             |\n| CQ1.1 | Does it show the types       | QP1.1        |\n|  \
    \     | of tests generated?          |              |\n| CQ1.2 | Does it present\
    \ the test     | QP1.2        |\n|       | generation techniques used?  |    \
    \          |\n| CQ2   | List the prerequisites?      | QP1.3, QP1.4 |\n| CQ2.1\
    \ | Does it list technologies    |              |\n|       | and knowledge required\
    \ for   |              |\n|       | proper use?                  |           \
    \   |\n| CQ2.2 | List situations for which    |              |\n|       | its\
    \ use is recommended?      |              |\n| CQ2.3 | List situations for which\
    \    |              |\n|       | the use is not recommended?  |              |\n\
    \n<span id=\"page-4-0\"></span>Table 6: Quality criteria of the SLR\n\nThe possible\
    \ answers to the questions presented in the Table [6](#page-4-0) are listed in\
    \ the Table [7.](#page-5-0) With the quality assessment we answered most of the\
    \ research questions proposed in the Table [1](#page-2-0) with the exception of\
    \ the question \"QP1.4 - What types of studies or evaluations have been conducted\"\
    , for which we adopted the classification into six categories proposed by Chen\
    \ and Babar and listed in the Table [9](#page-6-0) [\\[5\\]](#page-13-4).\n\n\
    | Answer    | Value |\n|-----------|-------|\n| Yes       | 1.0   |\n| Partially\
    \ | 0.5   |\n| No        | 0.0   |\n\n<span id=\"page-5-0\"></span>Table 7: Quality\
    \ criteria values\n\nBased on the questions in the Table [6](#page-4-0) and the\
    \ answer values listed in the Table [7](#page-5-0) we have a Max Score of 10 points\
    \ (only elementary questions count towards the maximum score). We set the cutoff\
    \ score at cutoffScore points, i.e. documents that do not get at least this score\
    \ in the Qualitative Analysis phase will be discarded.\n\n| Criteria | Weight\
    \ |  |\n|----------|--------|--|\n| CQ1      | 7      |  |\n| CQ2      | 3   \
    \   |  |\n\n<span id=\"page-5-1\"></span>Table 8: Quality criteria weights\n\n\
    We apply the weights listed in the \"Table [8](#page-5-1) and thus a description\
    \ of the requirements in the studies (\"CQ1\") of the models or tools presented\
    \ have higher objective value than the description (\"CQ2\") or the listing of\
    \ their prerequisites.\n\n| Description                                      \
    \                                 |\n|-----------------------------------------------------------------------------------|\n\
    | Rigorous Analysis                                                          \
    \       |\n| Rigorous derivation and proof suitable for formal model         \
    \                  |\n| Case Study                                           \
    \                             |\n| An empirical investigation of a current phenomenon\
    \ in its actual context;         |\n| when the boundaries between phenomenon and\
    \ context are not clearly evident;       |\n| and in which multiple sources of\
    \ evidence are used                                |\n| Discussion           \
    \                                                             |\n| Qualitative\
    \ and textual opinion                                                   |\n| Example\
    \                                                                           |\n\
    | The authors describe an application and provide an example to help in the  \
    \       |\n| description, but the example is \"used to validate\" or \"evaluate\"\
    \ to the extent    |\n| that the authors suggest                             \
    \                             |\n| Experience Report                         \
    \                                        |\n| The result has been used in real\
    \ examples, but not in the form of case studies or |\n| controlled experiments,\
    \ evidence of its use is collected informally or formally   |\n| Field Study \
    \                                                                      |\n| Controlled\
    \ experiment conducted in industry settings                              |\n|\
    \ Laboratory Experiment with Human Subjects                                  \
    \       |\n| Identifying precise relationships between variables in a controlled\
    \ environment   |\n| using humans and applying quantitative techniques       \
    \                          |\n| Laboratory Experiment with Software Subjects \
    \                                     |\n| A laboratory experiment to compare\
    \ the performance of the proposed model or       |\n| tool with existing ones\
    \                                                           |\n| Simulation  \
    \                                                                      |\n| Running\
    \ with stochastic data and using a real model or tool                       |\n\
    |                                                                            \
    \       |\n\n<span id=\"page-6-0\"></span>Table 9: Categories of studies evaluated\
    \ in the SLR\n\nWith the adoption of the categorization of studies that we have\
    \ listed in Table [9,](#page-6-0) it only remains to quantify these categories\
    \ so that we can properly score the studies and select those of interest to us.\
    \ For this Galster et al. suggests an analysis in three dimensions[\\[10\\]](#page-14-4).\n\
    \n- C Context, which refers to the environment in which conducted, includes the\
    \ experience of the personnel involved, processes used, and illustrates the feasibility\
    \ of applying a particular technology\n- D Design, which describes the products,\
    \ resources and processes used in the study\n- V Validity, that is, a discussion\
    \ of the validity of the results obtained, their limitations, and what threatens\
    \ this validity\n\nWe capture the value of each dimension as proposed in Ivarsson\
    \ and Gorschek in three levels [\\[11\\]](#page-14-5):\n\n- 1 weak;\n- 2 regular;\
    \ and\n- 3 strong.\n\nThis rigor is appropriate for empirical studies and meets\
    \ the ideas presented by Kitchenham et al. and elaborated on in Dyba et al. Cross-referencing\
    \ the\n\n|                                                              | Value\
    \                                                                            \
    \                      |                                                     \
    \                                                                            \
    \                                                                            \
    \         |                                                                  \
    \                                                                            \
    \                                                                            \
    \                                        |\n|--------------------------------------------------------------|--------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n\
    | Dimension                                                    | Weak (1)    \
    \                                                                            \
    \               | Regular (2)                                                \
    \                                                                            \
    \                                                                            \
    \  | Strong (3)                                                              \
    \                                                                            \
    \                                                                            \
    \                                 |\n| Context<br>of<br>the<br>study<br>(C)  \
    \                       | There seems to<br>be no<br>description of<br>the context\
    \ in<br>which the<br>evaluation is<br>conducted | The context in<br>which the\
    \ study<br>is conducted is<br>mentioned or<br>briefly<br>presented, but is<br>not\
    \ described in<br>a way that the<br>reader can<br>understand it<br>and compare\
    \ it<br>to another<br>context | The context is<br>described in a<br>way that the<br>reader\
    \ can<br>understand it<br>and compare it<br>to another<br>context            \
    \                                                                            \
    \                                               |\n| Study design<br>(D)     \
    \                                     | There seems to<br>be no<br>description\
    \ of<br>the evaluation<br>design presented                        | The study\
    \ design<br>is described<br>briefly, e.g., \"10<br>students turned<br>in assignments<br>1,\
    \ 2, and 3 on<br>time\"                                                      \
    \                                         | The study design<br>is described in\
    \ a<br>way that a<br>reader can<br>understand,<br>among other<br>things, the<br>variables<br>measured,\
    \ the<br>control used, the<br>treatments, the<br>selection and<br>sampling used,<br>and\
    \ other<br>pertinent<br>information |\n| Validity<br>of<br>the<br>dis<br>cussion<br>of<br>results\
    \ (V) | There seems to<br>be no<br>description of<br>any threats to<br>the validity\
    \ of<br>the evaluation       | The validity of<br>the study is<br>mentioned, but<br>not\
    \ described in<br>detail                                                     \
    \                                                                            \
    \     | The validity of<br>the evaluation is<br>discussed in<br>detail where<br>threats\
    \ are<br>described and<br>measures to<br>limit them are<br>detailed          \
    \                                                                            \
    \                          |\n\ndimensions with the values we arrive at the Table\
    \ [10](#page-7-0) with the classification of rigor [\\[7,](#page-14-1) [8\\]](#page-14-2).\n\
    \n<span id=\"page-7-0\"></span>Table 10: Classification of rigor applied to studies\
    \ in the SLR\n\n## 2.3 Qualitative Analysis\n\nBased on the parameters set in\
    \ the Tables [6,](#page-4-0) [7](#page-5-0) and [10](#page-7-0) the selected studies\
    \ (see Table [5\\)](#page-3-1) were qualified and the result can be seen in the\
    \ \"Selected Primary Studies\".\n\n![](_page_7_Figure_5.jpeg)\n\n<span id=\"page-7-2\"\
    ></span><span id=\"page-7-1\"></span>Figure 2: Qualified studies\n\nIn the Figure\
    \ [2](#page-7-1) and Table [11](#page-7-2) we list the totals of studies that\
    \ have qualified based on the cutoff score (7 points).\n\n## 2.4 Analysis of Results\n\
    \nBased on the criteria listed in the Table [3](#page-2-2) 165 were selected and\
    \ 52[\\[2\\]](#page-13-1) rejected.\n\nThe research questions listed in Table\
    \ [1](#page-2-0) were applied to the selected studies and we obtained the results\
    \ that we list further below.\n\n#### 2.4.1 Models, processes, frameworks or tools\
    \ used for test production (QP1)\n\nIn the SLM[\\[2\\]](#page-13-1) we applied\
    \ the Petersen et al. systematics to classify the obtained documents as seen in\
    \ the Figure [3](#page-8-0) and evaluated the type of contribution based on interpretation\
    \ of the abstracts and listed in the Figure [4](#page-8-1) and Table [17](#page-11-0)\
    \ [\\[12,](#page-14-6) [13\\]](#page-14-7).\n\n![](_page_8_Figure_6.jpeg)\n\n\
    <span id=\"page-8-0\"></span>Figure 3: Classification Scheme (adapted from [\\\
    [12\\]](#page-14-6))\n\nWhat types of tests are produced (QP1.1)? In the Figure\
    \ [4](#page-8-1) we list the types of generators addressed by the studies, and\
    \ in the Figure [5](#page-8-2) the types of tests produced.\n\n![](_page_8_Figure_9.jpeg)\n\
    \n<span id=\"page-8-2\"></span><span id=\"page-8-1\"></span>Figure 4: Artifact\
    \ Types\n\n| Artifact Type | %   | Qty.  |\n|---------------|-----|-------|\n\
    | Both          | 32  | 6.67  |\n| Code          | 5   | 57.58 |\n| Data     \
    \     | 124 | 32.73 |\n| N/A           | 4   | 3.03  |\n\n| Generator Type | %\
    \   | Qty.  |\n|----------------|-----|-------|\n| Formal         | 33  | 20.00\
    \ |\n| White-box      | 130 | 78.79 |\n| Black-box      | 2   | 1.21  |\n\nTable\
    \ 12: Types of Artifacts Produced\n\nTable 13: Types of Generators\n\nWhat test\
    \ generation techniques were applied (QP1.2)? The most used techniques found in\
    \ the studies can be seen in Figure [6.](#page-9-0)\n\n![](_page_9_Figure_5.jpeg)\n\
    \n<span id=\"page-9-0\"></span>Figure 6: Models, methods, processes and techniques\
    \ applied in ATP\n\nWhat tools does the study employ (QP1.3)? In Figure [7](#page-9-1)\
    \ we list the tools employed by the studies.\n\n![](_page_9_Figure_8.jpeg)\n\n\
    <span id=\"page-9-1\"></span>Figure 7: Tools used in the studies\n\n<span id=\"\
    page-9-2\"></span>Table 14: Tools used in the studies\n\nWe classified as \"Proof\
    \ of concept\" the studies that used some program to present their approach but\
    \ did not use or produce a tool that can be considered ready for use by industry\
    \ and in some even for other studies.\n\nWhat are the prerequisites for its application\
    \ (QP1.4)? What artificats we observed that are required to perform the analysis\
    \ according to the study.\n\n![](_page_10_Figure_0.jpeg)\n\n<span id=\"page-10-0\"\
    ></span>Figure 8: Study Requirements\n\n| Study type          | %     | Qty. |\n\
    |---------------------|-------|------|\n| Source code         | 52.12 | 86   |\n\
    | Model               | 9.09  | 15   |\n| Model + Source code | 0.61  | 1    |\n\
    | Tool + Source code  | 30.30 | 50   |\n| Tool + Model        | 7.88  | 13   |\n\
    \n<span id=\"page-10-1\"></span>Table 15: Study Requirements\n\nWhat types of\
    \ studies or evaluations have been conducted (QP1.5)? The types of studies were\
    \ classified according to Table [16](#page-11-1) and the results listed in Figure\
    \ [9](#page-11-2) and Table [17.](#page-11-0)\n\n| Category     | Description\
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \   |\n|--------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n\
    | Metric       | A system or standard of measures or measure<br>ments taken using\
    \ an existing standard.                                                      \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                          |\n| Tool         | A device or implementation, used\
    \ to perform a<br>certain function.                                          \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                          |\n| Model        |\
    \ A comprehensive and systematic approach that<br>includes theoretical principles,\
    \ benefits and draw<br>backs, objectives, methodological guidelines and<br>specifications,\
    \ and the characteristic use of cer<br>tain sets of methods and techniques.  \
    \                                                                            \
    \                                                                         |\n\
    | Method       | No particular theoretical orientation is inferred<br>in a method.\
    \ Researchers impose their own par<br>ticular theoretical beliefs on an experiment\
    \ when<br>they design and implement it by applying one or<br>more techniques.\
    \                                                                            \
    \                                                                            \
    \                    |\n| Technique    | A single operation or interaction in\
    \ which a re<br>searcher uses one or more procedures to elicit an<br>immediate\
    \ reaction from the object of study or to<br>shape the experiment and obtain results.\
    \                                                                            \
    \                                                                            \
    \                                             |\n| Procedure    | An organized\
    \ sequence of operations and interac<br>tions that a researcher uses to conduct\
    \ an exper<br>iment.                                                         \
    \                                                                            \
    \                                                                            \
    \                                                                            |\n\
    | Intervention | Purposefully interferes with or mitigates various<br>aspects\
    \ of the object of study and that affect the<br>outcome by applying procedure,\
    \ technique. The<br>elements acting on the industry during a partic<br>ular intervention\
    \ are most often computer appli<br>cations, the researcher, or both.         \
    \                                                                            \
    \                  |\n| Approach     | A broad way of addressing an industry concern\
    \ or<br>problem. A specific methods is not implied, but<br>a specific set of techniques\
    \ will likely come into<br>play when trying to intervene in the industry and<br>the\
    \ problem that is the subject of the research.<br>The procedures to be used will\
    \ be determined by<br>the delimitations of the methodological variant in<br>which\
    \ we design the study. |\n| Strategy     | An action plan designed to achieve\
    \ an overall<br>goal.                                                        \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                        |\n\n<span id=\"page-11-1\"\
    ></span>Table 16: Type of contribution\n\n![](_page_11_Figure_2.jpeg)\n\n<span\
    \ id=\"page-11-0\"></span>Table 17: Contributions of the Studies\n\n<span id=\"\
    page-11-2\"></span>Figure 9: Contributions of the Studies\n\n## 3 Results\n\n\
    We present the results of a SLR employed to find relevant studies on ATP. This\
    \ review applied the methodology of Petersen et al. with elements of Basili and\
    \ Weiss, Keele et al., Brereton et al. The results obtained list different models,\
    \ processes, frameworks and tools used with diverse approaches and results.\n\n\
    ### 3.1 Conclusions\n\nBased on the research questions developed in Section [2](#page-0-0)\
    \ we conclude that:\n\n#### 3.1.1 Models, processes, frameworks or tools applied\
    \ to test production\n\nTypes of tests produced As we can see from the Figures\
    \ [4](#page-8-1) and [5](#page-8-2) much of the studies have been devoted to producing\
    \ test verification of white-box [1](#page-12-0) . We also observe significant\
    \ concern with the Oracle problem (determining the correct outputs for given inputs)\
    \ and there are a significant number of approaches that produce data for program\
    \ verification.\n\nGiven the aspect of this research and the questions we address,\
    \ black-box tests[2](#page-12-1) were not representative in our results, however\
    \ according to Gaudel formal methods were a highlight and these can be considered\
    \ to be black-box tests.\n\nTechniques applyed to test production Several techniques\
    \ were employed in test production, and among them we highlight Search Methods\
    \ (see Figure [6\\)](#page-9-0). Secondly we note the application of Specification\
    \ Extraction[3](#page-12-2) as an important approach employed by the studies.\n\
    \nHighlighted we can also observe: Symbolic Execution, Combinatorial Methods and\
    \ Model Notation.\n\nTools applyed by the studies In the Figure [7](#page-9-1)\
    \ and Table [14](#page-9-2) we observed a great diversity of tools used and none\
    \ in particular stood out in our study, highlighting the use of UML (Unified Modelling\
    \ Language) by a significant percentage (if compared to other approaches) of studies.\
    \ This fragmentation demonstrates, on the one hand, a great wealth of available\
    \ solutions and approaches, but it may also be indicative of a lack of maturity\
    \ of the solutions presented, which can be observed by the opinion of some authors\
    \ [\\[15,](#page-14-8) [16\\]](#page-14-9).\n\nPre-requisites to the proposed\
    \ approaches For even obvious reasons (related to white-box testing - see Figure\
    \ [5\\)](#page-8-2), the vast majority of studies require access to the source\
    \ code of the applications (see Figure [8](#page-10-0) and Table [15\\)](#page-10-1).\
    \ Many\n\n<span id=\"page-12-0\"></span><sup>1</sup>Method of validating non-functional,\
    \ internal aspects of a computer application.\n\n<span id=\"page-12-1\"></span><sup>2</sup>Method\
    \ of validating functional and external aspects of a computer application\n\n\
    <span id=\"page-12-2\"></span><sup>3</sup>Comparative studies do not present particular\
    \ techniques or methods\n\nstudies are conducted using tools (none in particular\
    \ as we have noted - see Figure [7\\)](#page-9-1), but an important number have\
    \ been conducted from models (previously produced by a development process or\
    \ ad hoc).\n\nTypes of studies or evaluations conducted In the Figure [9](#page-11-2)\
    \ and Table [17](#page-11-0) we list the main contributions of the studies, and\
    \ note a rather encouraging number of concrete contributions (in the form of methods,\
    \ tools, metrics, and models) that can be leveraged and extended by the industry.\n\
    \n### 3.2 Future work\n\nThis work aimed to prepare the ground for further research\
    \ on ATP where we will determine the challenges in applying generative testing\
    \ techniques and evaluate the solutions we intend to address.\n\n## Acronyms\n\
    \n| ATP        | Automated Test Production<br>- pages: 1, 2, 13, 14          \
    \                                          |\n|------------|-------------------------------------------------------------------------------------------------------|\n\
    | SLM<br>SLR | Systematic Literature Mapping<br>- pages: 1, 3, 9<br>Systematic\
    \ Literature Review<br>- pages: 1–3, 13 |\n\nUML Unified Modelling Language -\
    \ page: 13\n\n## References\n\n- <span id=\"page-13-0\"></span>[1] P. Brereton,\
    \ B. A. Kitchenham, D. Budgen, M. Turner, and M. Khalil, \"Lessons from applying\
    \ the systematic literature review process within the software engineering domain,\"\
    \ Journal of systems and software, vol. 80, no. 4, pp. 571–583, 2007.\n- <span\
    \ id=\"page-13-1\"></span>[2] J. M. Gomes and L. A. V. Dias, \"Automated test\
    \ production – systematic literature mapping,\" 2024. arXiv: [2401.01430 \\[cs.SE\\\
    ]](https://arxiv.org/abs/2401.01430).\n- <span id=\"page-13-2\"></span>[3] B.\
    \ Kitchenham, O. P. Brereton, D. Budgen, M. Turner, J. Bailey, and S. Linkman,\
    \ \"Systematic literature reviews in software engineering–a systematic literature\
    \ review,\" Information and software technology, vol. 51, no. 1, pp. 7–15, 2009.\n\
    - <span id=\"page-13-3\"></span>[4] V. R. Basili and D. M. Weiss, \"A methodology\
    \ for collecting valid software engineering data,\" IEEE Transactions on software\
    \ engineering, no. 6, pp. 728–738, 1984.\n- <span id=\"page-13-4\"></span>[5]\
    \ L. Chen and M. A. Babar, \"A systematic review of evaluation of variability\
    \ management approaches in software product lines,\" Information and Software\
    \ Technology, vol. 53, no. 4, pp. 344–362, 2011.\n- <span id=\"page-14-0\"></span>[6]\
    \ S. Keele et al., \"Guidelines for performing systematic literature reviews in\
    \ software engineering,\" Technical report, Ver. 2.3 EBSE Technical Report. EBSE,\
    \ Tech. Rep., 2007.\n- <span id=\"page-14-1\"></span>[7] B. A. Kitchenham, T.\
    \ Dyba, and M. Jorgensen, \"Evidence-based software engineering,\" in Proceedings.\
    \ 26th International Conference on Software Engineering, IEEE, 2004, pp. 273–281.\n\
    - <span id=\"page-14-2\"></span>[8] T. Dyba, B. A. Kitchenham, and M. Jorgensen,\
    \ \"Evidence-based software engineering for practitioners,\" IEEE software, vol.\
    \ 22, no. 1, pp. 58–65, 2005.\n- <span id=\"page-14-3\"></span>[9] R. M. Davis,\
    \ \"Evolution of computers and computing,\" Science, vol. 195, no. 4283, pp. 1096–1102,\
    \ 1977.\n- <span id=\"page-14-4\"></span>[10] M. Galster, D. Weyns, D. Tofan,\
    \ B. Michalik, and P. Avgeriou, \"Variability in software systems—a systematic\
    \ literature review,\" IEEE Transactions on Software Engineering, vol. 40, no.\
    \ 3, pp. 282–306, 2013.\n- <span id=\"page-14-5\"></span>[11] M. Ivarsson and\
    \ T. Gorschek, \"A method for evaluating rigor and industrial relevance of technology\
    \ evaluations,\" Empirical Software Engineering, vol. 16, no. 3, pp. 365–395,\
    \ 2011.\n- <span id=\"page-14-6\"></span>[12] K. Petersen, R. Feldt, S. Mujtaba,\
    \ and M. Mattsson, \"Systematic mapping studies in software engineering,\" in\
    \ 12th International Conference on Evaluation and Assessment in Software Engineering\
    \ (EASE) 12, 2008, pp. 1–10.\n- <span id=\"page-14-7\"></span>[13] R. Wieringa,\
    \ N. Maiden, N. Mead, and C. Rolland, \"Requirements engineering paper classification\
    \ and evaluation criteria: A proposal and a discussion,\" Requirements engineering,\
    \ vol. 11, no. 1, pp. 102–107, 2006.\n- [14] M.-C. Gaudel, \"Formal methods for\
    \ software testing,\" in 2017 International Symposium on Theoretical Aspects of\
    \ Software Engineering (TASE), IEEE, 2017, pp. 1–3.\n- <span id=\"page-14-8\"\
    ></span>[15] U. Rueda, F. Kifetew, and X. Devroey, \"Towards automated test case\
    \ generation maturity,\" in Proceedings of the 12th International Workshop on\
    \ Search-Based Software Testing, ser. SBST '19, ZSCC: 0000000, IEEE Press, 2019,\
    \ pp. 9–10. doi: [10.1109/SBST.2019.00011](https://doi.org/10.1109/SBST.2019.00011).\
    \ [Online]. Available: <https://doi.org/10.1109/SBST.2019.00011>.\n- <span id=\"\
    page-14-9\"></span>[16] A. Arcuri, \"An experience report on applying software\
    \ testing academic results in industry: We need usable automated test generation,\"\
    \ Empir Software Eng, vol. 23, no. 4, pp. 1959–1981, 2018, issn: 1382-3256. doi:\
    \ [10.1007/s10664-017-9570-9](https://doi.org/10.1007/s10664-017-9570-9).\n\n\
    | #        | Title                                                           \
    \                                                                            \
    \                         | Author                                           \
    \                                                                            \
    \ |\n|----------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------|\n\
    | 1<br>2   | Support<br>Software<br>and<br>Primary<br>Evolving<br>Using<br>for<br>Generation<br>Generation<br>Suite<br>Test<br>Test<br>Unit<br>Multifaceted<br>Automated\
    \          | Sina<br>Gregory<br>Shamshiri,<br>Gay,                           \
    \                                                              |\n|          |\
    \ Functions<br>Fitness<br>ing                                                \
    \                                                                            \
    \              |                                                             \
    \                                                                  |\n| 3    \
    \    | Decomposition<br>Model<br>by<br>Generation<br>Test<br>Model-Based<br>Improving\
    \                                                                            \
    \           | Riccobene,<br>Angelo;<br>Gargantini,<br>Paolo;<br>Arcaini,<br>Elvinia\
    \                                                         |\n| 4        | the<br>to<br>Applied<br>Algorithms<br>Search-Based<br>of<br>Generation<br>Automatic\
    \                                                                            \
    \      | A.<br>Jackson<br>Lima,<br>Jakubovski;<br>L.<br>Helson<br>Filho,     \
    \                                                          |\n| 5        | Strategy<br>Optimization<br>Lines<br>Learning-Based<br>Product<br>Software<br>Teaching<br>of<br>Adaptive<br>Testing<br>Feature<br>Fuzzy\
    \                              | Z.<br>Kamal<br>R.<br>Silvia<br>Zamli,<br>Vergilio,<br>Fakhrud;<br>Prado;<br>Din,\
    \                                              |\n|          | Generation<br>Cases<br>Test<br>Functional<br>GUI<br>for\
    \                                                                            \
    \                                  |                                         \
    \                                                                            \
    \          |\n| 6        | Context<br>Using<br>Generation<br>Test<br>Improving<br>Functions<br>One-Size-Fits-None?<br>Fitness<br>Optimized\
    \                                                      | Gregory<br>Gay,     \
    \                                                                            \
    \                              |\n| 7        | Optimisa<br>Many-Objective<br>a<br>as<br>Generation<br>Case<br>Test<br>Automated\
    \                                                                            \
    \         | Meshesha;<br>Fitsum<br>Kifetew,<br>Annibale;<br>Panichella,      \
    \                                                             |\n| 8        |\
    \ Generation<br>Test<br>Targets<br>Model-Based<br>the<br>of<br>Selection<br>for<br>Approach<br>Dynamic<br>Decomposition-Based<br>with<br>Problem<br>tion\
    \               | Riccobene,<br>Angelo;<br>Gargantini,<br>Paolo;<br>Paolo<br>Tonella,<br>Arcaini,\
    \                                               |\n| 9        | Distributed<br>for<br>Suites<br>Test<br>Controllable<br>Complete<br>Generating\
    \                                                                            \
    \           | M.<br>Robert<br>Hierons,<br>Elvinia                            \
    \                                                               |\n|         \
    \ | Testing                                                                  \
    \                                                                            \
    \                |                                                           \
    \                                                                    |\n| 10<br>11\
    \ | Mo<br>for<br>Strategy<br>Testing<br>Generation<br>Event-Based<br>Model<br>for<br>Morphology<br>Static-Dynamic<br>Model<br>A<br>Exploiting<br>AMOGA:\
    \                  | Ahmed,<br>Rosziati;<br>Ibrahim,<br>Mutlu<br>Ibrahim-Anka;<br>Beyazit,<br>Fevzi;<br>Salihu,<br>Belli,\
    \                          |\n|          | Testing<br>Apps<br>bile           \
    \                                                                            \
    \                                                       | Asmau<br>Usman,<br>Z.;<br>Kamal<br>Zamli,<br>S.;<br>Bestoun\
    \                                                                   |\n| 12  \
    \     | Optimisa<br>Compiler<br>via<br>Equivalences<br>Mutant<br>Trivial<br>Detecting<br>tions\
    \                                                                            \
    \   | Yue;<br>Mark<br>Jia,<br>Harman,<br>Mike;<br>Yves;<br>Papadakis,<br>Traon,<br>Le<br>Marinos;<br>Nicos;<br>Malevris,<br>Kintis,\
    \ |\n| 13       | to<br>Proofs<br>Pen-and-Paper<br>From<br>Verification:<br>Software<br>Tools<br>Deductive<br>Industrial\
    \                                                               | Marieke<br>Huisman,<br>Reiner;<br>H¨ahnle,\
    \                                                                            \
    \        |\n| 14       | test<br>mutation<br>for<br>approach<br>generation<br>data<br>test<br>multi-objective<br>A\
    \                                                                            |\
    \ R.<br>Silvia<br>Vergilio,<br>A.;<br>Rui<br>Filho,<br>Matnei                \
    \                                                   |\n| 15       | val<br>for<br>data<br>invalid<br>and<br>valid<br>of<br>models<br>feature<br>Automatic<br>of<br>ing\
    \                                                                   |        \
    \                                                                            \
    \                                           |\n|          | string<br>expressions<br>regular<br>test<br>and<br>searches<br>web<br>using<br>generation<br>routines<br>idation\
    \                                                     | Stevenson,<br>Phil;<br>McMinn,<br>Muzammil;<br>Shahbaz,<br>Mark\
    \                                                               |\n| 16      \
    \ | suites<br>test<br>whole<br>of<br>generation<br>mutation-based<br>scalable<br>Achieving\
    \                                                                            \
    \   | Andrea<br>Arcuri,<br>Gordon;<br>Fraser,                                \
    \                                                       |\n| 17       | T<br>AC<br>CONSTRAINTS<br>LATE<br>ON<br>BASED<br>OF<br>STRATEGY<br>BENCHMARKING<br>GENERATION<br>COMPARATIVE<br>TEST<br>WAY\
    \                                          | Alsewari;<br>Rahman<br>Abdul<br>Z.;<br>Al-Kazemi<br>Kamal<br>Basem<br>Zamli,\
    \                                                  |\n| 18       | and<br>selection<br>ALGORITHM<br>CLIMBING<br>of<br>HILL<br>CEPTANCE<br>An\
    \                                                                            \
    \                | Gra<br>Kamal                                              \
    \                                                                    |\n|    \
    \      | acceptance<br>generation<br>suite<br>hyper-heuristic<br>test<br>t-way<br>combinatorial<br>study<br>experimental<br>for<br>mechanism\
    \                                  | Kendall,<br>Fakhrud;<br>S.<br>Din,<br>Bestoun<br>Z.;<br>Ahmed,<br>Zamli,<br>ham;\
    \                                              |\n| 19       | for<br>graph<br>flow<br>control<br>condition<br>multiple<br>MOF-based<br>an<br>MCCFG:\
    \                                                                            \
    \    | R.<br>Kim,<br>Young;<br>Park,<br>Hyun;<br>Son,                        \
    \                                                        |\n| 20       | gen<br>suite<br>test<br>t-way<br>for<br>strategy<br>hyper-heuristic<br>generation<br>case<br>Search<br>test<br>automatic<br>Tabu<br>A\
    \                                | Kendall,<br>Y;<br>Basem<br>Alkazemi,<br>Z.;<br>Kamal<br>Zamli,\
    \                                                                |\n|        \
    \  | eration                                                                 \
    \                                                                            \
    \                 | Graham                                                   \
    \                                                                     |\n| 21\
    \       | specification<br>evolved<br>for<br>generation<br>case<br>test<br>based<br>net<br>Petri\
    \                                                                            \
    \   | Jin,<br>Haibo;<br>Chen,<br>Mingyue;<br>Jiang,<br>Mengchu<br>Zuohua;<br>Zhou,<br>Ding,<br>Zhi;\
    \                                 |\n| 22       | Reduc<br>Mutant<br>of<br>Generation<br>the<br>for<br>Hyper-Heuristic<br>A<br>Sentinel:\
    \                                                                            \
    \   | Jens;<br>Krinke,<br>Federica;<br>Sarro,<br>Giovani;<br>Guizzo,         \
    \                                                       |\n| 23       | debug<br>and<br>for<br>oracles<br>(F)LTL<br>of<br>Strategies<br>Automated<br>tion\
    \                                                                            \
    \        | Franz<br>Wotawa,<br>R.<br>Silvia<br>Vergilio,<br>Pill,            \
    \                                                            |\n|          | testing<br>generation<br>ging\
    \                                                                            \
    \                                                            | Ingo;         \
    \                                                                            \
    \                                    |\n| 24       | Gen<br>Test<br>for<br>Solvers<br>SMT<br>and<br>SAT<br>Expressions<br>of<br>Use<br>the<br>Boolean<br>Optimize<br>of<br>to<br>eration<br>How\
    \                           | Riccobene,<br>Angelo;<br>Gargantini,<br>Paolo;<br>Arcaini,<br>Elvinia\
    \                                                         |\n| 25       | generation<br>Automated<br>job:<br>the<br>for<br>function<br>fitness<br>the<br>Choosing\
    \                                                                            \
    \  | Gre<br>Gay,<br>Hussein;<br>Almulla,<br>Alireza;<br>Salahirad,           \
    \                                                      |\n| 26       | Gen<br>Case<br>Test<br>Automatic<br>for<br>faults<br>Tool<br>and<br>real<br>Process<br>detect<br>that<br>TDD<br>suites<br>A<br>MoFQA:<br>test<br>of\
    \                  | Nathalie<br>Gonz´alez;<br>Magal´ı<br>Riquelme;<br>Linda<br>gory\
    \                                                               |\n|         \
    \ | Models<br>MDD<br>from<br>eration                                         \
    \                                                                            \
    \                | Cernuzzi<br>Luca<br>Aquino;                               \
    \                                                                    |\n| 27 \
    \      | models<br>process<br>business<br>from<br>generation<br>cases<br>test<br>Automatic\
    \                                                                            \
    \        | Mohammad;<br>Amiri,<br>Mahnaz<br>Arezoo;<br>Koupaee,<br>Seqerloo,<br>Saeed;<br>Yazdani<br>Parsa,\
    \                              |\n| 28<br>29 | chart<br>Covering<br>state<br>Cell<br>from<br>by<br>generation<br>Expressions<br>case<br>Boolean<br>test<br>based<br>for<br>Generation<br>Transition<br>Case<br>Test\
    \ | S.K.<br>Swain,<br>Mitrabinda;<br>Tsai<br>Wei-Tek<br>Ray,<br>S.;<br>Pradhan,<br>Yu;<br>Lian\
    \                                    |\n|          | coverage<br>diagram     \
    \                                                                            \
    \                                                                 |          \
    \                                                                            \
    \                                         |\n| 30<br>31 | algo<br>pollination<br>generation<br>flower<br>suite<br>on<br>based<br>test<br>whole<br>generation<br>for<br>Algorithm<br>data<br>test<br>Memetic<br>Pairwise<br>A\
    \   | Tairan,<br>Phil<br>McMinn,<br>A.A.;<br>Alsewari,<br>Andrea;<br>Arcuri,<br>B.;<br>Abdullah<br>Gordon;<br>Nasser,<br>Fraser,\
    \    |\n| 32       | Isabelle<br>in<br>diagrams<br>invariant<br>for<br>generation<br>code<br>and<br>Verification<br>rithm\
    \                                                                 | J<br>Eriksson,<br>Rj;<br>Z.<br>Kamal<br>Back,<br>V;<br>Zamli,<br>Preoteasa,<br>N.M.;\
    \                                          |\n| 33       | generation<br>test<br>for<br>under-approximation<br>Tri-modal\
    \                                                                            \
    \                            | Masson,<br>Jacques;<br>Julliand,<br>Hadrien;<br>Pierre-Alain<br>Bride,\
    \                                                        |\n| 34       | strength<br>variable<br>for<br>algorithm<br>greedy<br>on<br>elitism<br>the<br>Adapting\
    \                                                                            \
    \   | Kamal<br>Zamli,<br>A.A.;<br>Alsewari,<br>A.A.B.A.;<br>Homaid,          \
    \                                                       |\n\n# Appendices\n\n\
    ## Appendix A Selected Primary Studies\n\ncombinatorial\n\n35 Model Learning and\
    \ Test Generation\n\n test cases generation\n\n Using Cover Automata\n\nZ.; Alsariera,\n\
    \n Ipate, Florentin;\n\n Y.A.\n\n Stefanescu,\n\n Alin; Dinca, Ionut"
  decisions:
    language: '- Qualified. Reason: English Paper'
    evaluation_prompt: Disqualified.
    related_work_prompt: Disqualified
    novelty_prompt: Disqualified
    review_only_prompt: Disqualified
  llm_input_used: '## Abstract

    Identifying the main contributions related to the Automated Test Production

    (ATP) of Computer Programs and providing an overview about models,

    methodologies and tools used for this purpose is the aim of this Systematic

    Literature Review (SLR). The results will enable a comprehensive analysis and

    insight to evaluate their applicability. A previously produced Systematic

    Literature Mapping (SLM) contributed to the formulation of the ``Research

    Questions'''' and parameters for the definition of the qualitative analysis

    protocol of this review.


    '
  token_usage: 1266
  time_usage: 2.2501189708709717
- title: Automated Test Production -- Complement to "Ad-hoc" Testing
  abstract: 'A view on software testing, taken in a broad sense and considered a important

    activity is presented. We discuss the methods and techniques for applying tests

    and the reasons we recognize make it difficult for industry to adopt the

    advances observed in academia. We discuss some advances in the area and briefly

    point out the approach we intend to follow in the search for a solution.'
  url: http://arxiv.org/abs/2401.02230v1
  keywords: ''
  document: '# Automated Test Production Complement to "Ad-hoc" Testing


    Gomes, J.M.<sup>1</sup> and Dias, L.A.V.<sup>1</sup>


    1 Instituto Tecnol´ogico de Aeron´autica - ITA


    January 5, 2024


    #### Abstract


    A view on software testing, taken in a broad sense and considered a important
    activity is presented. We discuss the methods and techniques for applying tests
    and the reasons we recognize make it difficult for industry to adopt the advances
    observed in academia. We discuss some advances in the area and briefly point out
    the approach we intend to follow in the search for a solution.


    # 1 Motivation


    Accepting that tests are important, but are not always implemented or kept up
    to date during the lifetime of a program, we conclude that nothing has changed
    since the introduction of the Agile Manifesto earlier this century [\[1\]](#page-13-0)
    which we reproduce below and from which we highlight the passage "Software that
    works rather than complete documentation"[\[1\]](#page-13-0).


    - "Individuals and interactions over processes and tools"

    - "Working software over comprehensive documentation"

    - "Customer collaboration over contract negotiation"

    - "Responding to change over following a plan"


    This view has come to become an important industry trend [\[2\]](#page-13-1)[1](#page-0-0)
    , where face-to-face interactions are preferable to formal communication processes
    and working programs are preferable to comprehensive documentation, leaving the
    interpretation of the term "comprehensive" to each agile development team to decide
    [\[4](#page-13-2)]. In fact the agile method suggests that all documentation can
    be replaced by informal communication with an emphasis on tacit rather than explicit
    knowledge [\[5\]](#page-14-0).


    <span id="page-0-0"></span><sup>1</sup>The 14<sup>o</sup> annual report STATEofAGILE
    from 2020 points out that 95% of organizations practice agile software development
    methods. [\[3](#page-13-3)].


    On the other hand, the adoption of continuous integration and continuous delivery
    processes and tools has been steadily and unequivocally growing in both industry
    [\[6](#page-14-1), [7](#page-14-2)] and open source projects [\[8\]](#page-14-3),
    which can to some extent be interpreted as a denial of one of the principles of
    the Agile Manifesto: "Individuals and interactions over processes and tools",
    yet this does not come as a relief to the fact that many see benefits in building
    and maintaining formal models, but are not content to build them as they believe
    they consume too much time and resources, even believing in the slim chances of
    success of projects that do not use some modeling [\[9\]](#page-14-4).


    The implications of this view for the construction and maintenance of programs
    and the use and application of development methods and tools are discussed.


    # 2 Test Production Methods


    The present discussion is a contribution to the understanding of how software
    testing fits into the present realities perceived by both industry and academia,
    even if these realities, as we shall see, do not correspond and will not converge.
    The TDD (Test Driven Development) technique is widely cited and recommended by
    the signers of the Agile Manifesto [\[10\]](#page-14-5), even though it is not
    part of the manifesto or its twelve principles [\[1\]](#page-13-0), so we can
    conclude that the IT (Information Technology) industry at least recognizes the
    importance of testing programs. The academia, on the other hand, perceives program
    testing based on formal specifications as inevitable in pioneering studies since
    the 1970s [\[11,](#page-14-6) [12\]](#page-14-7), the foundations for combining
    formal methods and program testing being established and accepted, and it is up
    to the community to put them into practice, optimize and extend them.


    In general, we classify the tests in Formal: verifiable by theoretical means or
    pure logic; and Empirical: verifiable through observation or direct experience[2](#page-1-0)
    .


    ### 2.1 Formal Testing


    Hoare and Floyd introduced formal methods by introducing the "Hoare calculus"
    for proving the correctness of a program as well as the notions of pre and postconditions,
    invariants and assertions. His ideas were gradually developed into the current
    formal software engineering tools and techniques, such as the OCL (Object Constraint
    Language) [\[15](#page-14-8)] used to specify constraints in UML (Unified Modelling
    Language) diagrams.


    According to Gaudel, for each and every specification method, there is a notation
    [\[16](#page-14-9)]. Depending on the method, specifications can include expressions
    in various logical forms, used to write pre and postconditions, axioms of data
    types, constraints, temporal properties. They can represent definitions of process
    states, such as:


    <span id="page-1-0"></span><sup>2</sup>We take into account the formality of the
    test and not the conduct of the test, as it is perfectly possible to conduct empirical
    tests by adopting formal practices in their execution.


    - CSP (Communicating Sequential Processes) [\[17\]](#page-14-10)

    - CCS (Calculus of Communicating Systems) [\[18](#page-14-11)]

    - LOTOS (Language Of Temporal Ordering Specification) [\[19](#page-14-12)]

    - Circus [\[20\]](#page-14-13)


    Or they can have annotated diagrams, such as:


    - FSM (Finite State Machine) [\[21\]](#page-15-0)

    - LTS (Labelled Transition Systems) [\[22](#page-15-1)]

    - Petri Networks [\[23\]](#page-15-2)

    - etc.


    But there is more than a syntax. First, there is a formal semantics, in terms
    of mathematical notions such as:


    - Predicate transformers for pre and post conditions

    - Classified sets and algebras for axiomatic definitions

    - Various types of automata, traces, faults, divergences, for process algebras


    Second, there is a formal deduction system, making it possible to perform proofs,
    or other checks (such as model checking), or both. Thus, formal specifications
    can be analyzed to guide the identification of appropriate test cases.


    In addition to syntax, semantics, and the deduction system, formal methods come
    with some relations between specifications that formalize equivalence or correct
    step-by-step development. Depending on the context, such relations are called:
    refinement, conformance, or, in the case of formulas, satisfiability, and are
    fundamental to test methods [\[16\]](#page-14-9).


    Gaudel concludes that model-based tests are tests of the black-box [3](#page-2-0)
    type, where the internal organization of the program under test is ignored and
    the strategy is based on a description of the desired properties and behavior
    of the program[4](#page-2-1) , which may be formal or not, or in other words,
    these methods target certain classes of faults and assume that the program is
    exempt from other types and classes of faults [\[16\]](#page-14-9).


    <span id="page-2-0"></span><sup>3</sup>Method of validating functional and external
    aspects of a computer application.


    <span id="page-2-1"></span><sup>4</sup>We separate these tests into a category
    - that of Formal Tests - that is, tests with a formal basis and that originate
    from models.


    ### 2.2 Empirical Tests


    Without formal defined specifications a priori, which as we have seen in "Motivation"
    is a trend in the industry, we are left only with informal and empirical practice[5](#page-3-0)
    for the verification and validation of the correctness of computer program implementation[6](#page-3-1)
    . One of the practices advocated by supporters of agile methods is TDD, where
    tests are written even before the program itself, but it does not show clear benefits[7](#page-3-2)
    compared to the option of implementing the tests after the program is ready [\[25](#page-15-3),
    [26](#page-15-4), [27](#page-15-5)], or it may be linked to the fact that processes
    like TDD encourage stable and refined steps of continuous improvement [\[28\]](#page-15-6).


    In the informal test, we have a relation of the hypothesis to an observation statement,
    which is nothing more than a proposition about the perceptible properties of some
    entity, set of entities, or system, followed by a rule transmission where, if
    the observation statement directly confirms the hypothesis, then indirectly it
    confirms any of its logical consequences [\[29](#page-15-7)].


    We can state that formal tests are cases of inductive inference[8](#page-3-3)
    , and that in empirical tests we have a direct confirmation of the hypothesis,
    but without the soundness and precision that formal methods[9](#page-3-4) guarantee
    [\[30\]](#page-15-8) as a consequence of the ad hoc attitude with which the informality
    of design[10](#page-3-5) of empirical testing is practiced.


    Just as using only Formal Methods we are unable to judge all the possibilities
    of flaws that a program may present[\[31\]](#page-15-9), we can state that Empirical
    Methods are also so, and for the same reasons, with the aggravating factor of
    introducing a certain randomness[11](#page-3-6) to the process.


    ### 2.3 Static and Dynamic Analysis


    This is a case where the test can either be defined a priori (as in TDD or modelbased)
    or a posteriori (as most informal tests are done), and which according to Gaudel,
    would be the answer to the lack of coverage of Formal Tests, but which as we will
    see below, also present problems of application in practice.


    Static analysis was introduced in 1980 with the work "Methods to ensure the standardization
    of FORTRAN software. [PFORT, DAVE, POLISH, and BRNANL, for analysis and editing
    of codes, in FORTRAN for PDP-10 and IBM 360 and 370]" by Gaffney and Wooten [\[32](#page-15-10)].
    The nature of verification


    <span id="page-3-0"></span><sup>5</sup>Which generally means: verifiable by direct
    observation or experience rather than by theory or pure logic, even though it
    is possible to adopt formal practices during an empirical procedure.


    <sup>6</sup>Nothing prevents that, even starting from a basis of formal specifications,
    empirical tests be adopted in the verification of the implementation.


    <span id="page-3-1"></span><sup>7</sup>The practice of TDD is advocated mainly
    because the alternative is to have no tests at all after the program is ready[\[24\]](#page-15-11).


    <span id="page-3-3"></span><span id="page-3-2"></span><sup>8</sup>We cannot call
    "formal tests" a case of "indirect confirmation".


    <span id="page-3-4"></span><sup>9</sup>Formal methods pursue qualitative and quantitative
    metrics of the soundness and precision of the method itself.


    <sup>10</sup>And as we said earlier, not necessarily of the actual conduct, which
    can be perfectly formal.


    <span id="page-3-6"></span><span id="page-3-5"></span><sup>11</sup>The observer''s
    objectivity and his judgment.


    performed by static parsers include [\[33,](#page-15-12) [34\]](#page-15-13) (but
    not limited to only these) the following analyses:


    - Layout and source code formatting

    - Identifying language constructs known to be non-portable

    - Identifying algorithm constructs known to be unsafe

    - Use of variables or constants with suspect names and contents (for example:
    PASSWORD = ''SECRET'')

    - Detection of faults not considered by compilers

    - Control flow analysis (detection of loops)

    - Detect data usage in variables before a value has been entered

    - Detect value overloading in variables (assign a very large value to a variable
    that only supports small values - in some languages assign a DOUBLE value to a
    simple INT variable)

    - Detect memory overflow (leak) or the non-validation of may memory overflow (assigning
    a very long constant to a variable that supports a small memory size)

    - Detect leakage of handles (the reference to the control structure) of files
    and accesses to communication resources

    - Check permission to perform certain operations

    - Ensuring the termination of a processing (or ensuring indications that it will
    not terminate)

    - Ensure the order in which processing is performed and terminated in a way that
    maintains the integrity of the information (or ensure that it gives indications
    that the information is not intact)

    - Ensure that the process can be observed as deterministic[12](#page-4-0) (or
    ensure that there are indications that the process cannot be observed as deterministic)


    Many of these validations can be (and most often are) done by compilers (when
    the language is compiled)[\[35\]](#page-15-14). Since the purpose of the compiler
    is to generate executable code and not to check for programming faults, and other
    classes of faults can only be determined at runtime, such as memory overflow,
    which only occurs if a very long constant is supplied during program use,[13](#page-4-1)
    ,


    <span id="page-4-0"></span><sup>12</sup>If an action is visible to the environment
    (i.e. if it performs data retrieval or changes data), then we say it is observable.
    The order of execution of non-priority rules will make a difference in the order
    of appearance of observable actions.


    <span id="page-4-1"></span><sup>13</sup>Although it is possible, as we can see
    later, to predict overflow using one of the many static analysis methods available.


    | Classe                      | Descri¸c˜ao                                                                                                                                                         |

    |-----------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------|

    | Lexical Analysis            | Lexical analysis is based on the grammatical structure
    of the language. It divides the                                                                              |

    |                             | program into small parts that are compared to
    known fault libraries. Disregarding                                                                                   |

    |                             | syntax, semantics and interaction between subroutines,
    the incidence of false posi                                                                                  |

    |                             | tives is high [42].                                                                                                                                                 |

    | Type Inference              | It infers the type of variables and functions
    by the compiler or interpreter, and checks                                                                            |

    |                             | that accesses to these variables and functions
    conform to predefined rules for the type                                                                             |

    | Data Flow                   | [43].<br>Refers to collecting semantic information
    from source code, and using algebraic                                                                            |

    | Analysis                    | method determines the definition and use of variables
    at compile time.<br>Starting                                                                                  |

    |                             | from the execution flow graph, a data flow analysis
    determines whether values in a                                                                                  |

    |                             | program are flagged as potentially vulnerable
    variables [44].                                                                                                       |

    | Rule Checking               | Checks the security of a program using pre-set
    rules [45]. Some rules, such as re                                                                                   |

    |                             | quiring execution under elevated privilege, carry
    security implications [42] and are                                                                                |

    |                             | detected.                                                                                                                                                           |

    | Constraints                 | Divided between constraint generation and constraint
    resolution during the analysis                                                                                 |

    | Analysis                    | process. Constraint generation sets variable types
    or analyzes the constraint system                                                                                |

    |                             | between different states of execution using predetermined
    rules; constraint resolution                                                                              |

    |                             | applies and resolves the generated constraints
    [42].                                                                                                                |

    | Comparison of<br>Correction | Comparison of source or binary code snippets changed
    during the process of fixing                                                                                   |

    | Snippets                    | flaws is used to find known implementation gaps.
    After patches have been applied<br>to a program, the comparison serves to determine
    the location and causes of the |

    |                             | vulnerability to which they apply [42].                                                                                                                             |

    | Symbolic                    | It represents program inputs as symbols instead
    of the actual data, and produces al                                                                                 |

    | Execution                   | gebraic expressions over the symbol in the implementation
    process. By the constraint                                                                                |

    |                             | solving method symbolic execution can detect possible
    failures [46, 47, 48, 49].                                                                                    |

    | Abstract<br>Interpretation  | It is a formal description of program analysis,
    which maps the program to abstract                                                                                  |

    |                             | domains. The technique requires completeness,
    which makes it impractical for very                                                                                   |

    |                             | large programs, but proves correct for all possible
    inputs [50, 51].                                                                                                |

    | Proof of<br>Theorems        | Semantic analysis of the program, which can solve
    infinite state system problems [52,                                                                               |

    |                             | 53]. First convert the program into a logical
    formula, then prove that the program is                                                                               |

    |                             | a valid theorem using axioms and rules [42].                                                                                                                        |

    | Model<br>Verification       | Starting from formal models, such as state machines
    or directed graphs, it runs<br>through them and compares the model with the implementation
    to see if it matches |

    |                             | the characteristics predefined by the first [54].                                                                                                                   |

    |                             |                                                                                                                                                                     |


    <span id="page-5-0"></span>Table 1: Classification of Static Analyzers


    then specialized checkers such as Linters[\[36\]](#page-16-9) are adopted. Capable
    of detecting a wide range of faults, including style (layout of source code),
    some use source code annotations to achieve better problem detection, at the expense
    of extra developer work [\[37,](#page-16-10) [38,](#page-16-11) [39,](#page-16-12)
    [40,](#page-16-13) [41\]](#page-16-14).


    Static analyzers can then be classified (see Table [1\)](#page-5-0) into various
    types and capabilities, covering the detection of several possible fault categories,
    from implementation to vulnerability and security related.


    The problem with static analyzers is the high false positive rate (alerts that
    are not real problems), low understandability of alerts and lack of automation
    in quick fixes for the large number of identified problems [\[55\]](#page-17-4),
    such as: [\[56](#page-17-5)] code structure and [\[57](#page-17-6)] coding patterns,
    which could easily be fixed using automatic refactoring techniques [\[58](#page-17-7)],
    but as we will see below, the available tools are not in line with the latest
    advances made by the scientific community.


    Dynamic analysis, on the other hand, is in contrast to static analysis and contemplates
    the forms best known and adopted by the industry in the appli-


    | Classe           | Descri¸c˜ao                                                                            |

    |------------------|----------------------------------------------------------------------------------------|

    | Unit Test        | The process of testing subprograms, subroutines, classes,
    or functional units within   |

    |                  | a program to verify that there are no programming flaws [60,
    p. 486].                  |

    | Integration Test | Testing phase where the functional units are combined and
    tested as a group to assess  |

    |                  | whether they worked properly in the complete system [60,
    p. 235].                      |

    | System Testing   | Test conducted on multiple integrated systems to evaluate
    their ability to communi     |

    |                  | cate with each other and achieve general and specific integration
    requirements [60,    |

    |                  | p. 545].                                                                               |

    | Acceptance Test  | Testing of a system or functional unit generally performed
    by the buyer or user on     |

    |                  | site after installation of the software to make sure that
    the contractual requirements |

    |                  | have been met [60, p. 5].                                                              |


    <span id="page-6-0"></span>Table 2: Classification of Dynamic Tests


    cation of software testing [\[59\]](#page-17-9) (see Table [2\)](#page-6-0).


    # 3 The Challenge of Testing


    ### 3.1 Software Quality


    C. A. R. Hoare in the research "How did software get so reliable without proof?"
    conducted in 1996 states that it was reasonable to predict that the size and ambition
    of software products would be severely limited by the lack of reliability in their
    components. Estimates suggested, in its study, that professionally written programs
    may contain between one and ten correctable faults for every thousand lines of
    code; and any one software fault, in principle, can have a spectacular effect
    (or worse: a subtly misleading effect) on the behavior of the entire system [\[61\]](#page-17-10).


    Hoare found at the time that the software patch problem turned out to be far less
    serious than anticipated. An analysis by Mackenzie [\[62\]](#page-17-11) showed
    that of several thousand deaths attributed to computer applications, only ten
    or so could be explained by software crashes: most due to a few cases of incorrect
    dosage calculations in radiation cancer treatment. Similarly, predictions of collapse
    due to the size of computer programs have been falsified by the continuous operation
    of real-time software systems now measured in tens of millions of lines of code
    and subject to thousands of updates per year.


    In his review Hoare concludes that, despite appearances, modern software engineering
    practice owes much to the theoretical concepts and ideals of early research in
    this field; and that formalization and proof techniques have played an essential
    role in the validation and progress of research.


    Hoare concludes that the main factors for the apparent success of the software
    are:


    • Management - The most dramatic advances in the delivery of reliable software
    are directly attributable to a wider recognition of the fact that the process
    of program development can be predicted, planned, managed, and controlled just
    as in any other branch of engineering.


    - Test Thorough testing is the cornerstone of reliability in quality assurance
    and control in modern production engineering. Tests are applied as early as possible
    throughout the production line. They are rigorously designed to maximize the probability
    of detecting failures and as quickly as possible.

    - Debugging The secret of successful testing is that it checks the quality of
    the process and methods by which the code was produced. But there is an entirely
    different and very common response to the discovery of a flaw by testing: simply
    fix it and get on with the job. This is known as debugging, by analogy with trying
    to get rid of a mosquito infestation by killing the ones that bite - much faster,
    cheaper and more satisfying than draining the swamps in which they breed.

    - Excess Engineering The concept of safety factor is very widespread in engineering.
    After calculating the worst case load on a beam, the civil engineer will try to
    build it at least twice as strong. In computing, a continuous drop in the price
    of storage and increased processing power has made it acceptable to add redundancies
    to reduce the risk of software failures and a smaller scale of damage. This leads
    to the same kind of over-engineering required by law for bridge construction;
    and it is extremely effective, although there is no clear way to measure it by
    a numerical factor.

    - Programming Methodologies Most of the measures described so far for achieving
    reliability in software are the same ones that have been proven equally effective
    in all engineering disciplines. But the best general techniques for management,
    quality control, and safety would be totally useless by themselves; they are only
    effective when there is a general understanding, a common conceptual framework
    and terminology for discussing the relationship between cause and effect, between
    action and consequence. Research in programming methodology has this goal: to
    establish a conceptual framework and a theoretical basis to assist in the systematic
    derivation and justification of each design decision by a rational and explicable
    line of reasoning.


    ### 3.2 Perceived Quality when Using Software


    According to the NIST (National Institute of Standards and Technology) report,
    the estimated impact (in the United States) of inadequate software testing infrastructure
    is 859 billions dollars and the potential cost savings from feasible improvements
    is 822 billions dollars. Software users account for a larger share of the total
    costs of inadequate infrastructure (64 percent) compared to "viable" cost reductions
    (52 percent) because a large share of user costs are due to prevention activities.
    Whereas mitigation activities decrease proportionally to the decrease in the number
    of failures, prevention costs (such as redundant systems and investigating purchasing
    decisions) are likely to persist, even if only a few errors are expected. For
    software developers, the feasible cost savings are


    |            | Testers /<br>Employees<br>(millions) | Cost of inadequate<br>testing
    infrastructure |                              | Potential cost reduction<br>with
    feasible improvements |                              |

    |------------|--------------------------------------|----------------------------------------------|------------------------------|--------------------------------------------------------|------------------------------|

    |            |                                      | Unit cost                                    |
    Total cost<br>(million US\$) | Unit cost<br>(million US\$)                            |
    Total Cost<br>(million US\$) |

    | Developers | 0,302                                | 69.945                                       |
    21.155                       | 34.964                                                 |
    10.575                       |

    | Users      |                                      |                                              |                              |                                                        |                              |

    | Industry   | 25,0                                 | 459                                          |
    11.463                       | 135                                                    |
    3.375                        |

    | Services   | 74,1                                 | 362                                          |
    26.858                       | 112                                                    |
    8.299                        |

    | Total      |                                      |                                              |
    59.477                       |                                                        |
    22.249                       |


    Table 3: Estimated national impact in the US (adapted from [\[63\]](#page-17-12))


    approximately 50 percent of the total costs of inadequate infrastructure. This
    reflects a more proportional decrease in testing effort as testing resources and
    tools improve [\[63\]](#page-17-12).


    If we add up everything from minor inconveniences in our daily lives to incalculable
    human and social damage from software failures, the perception we have may be
    quite different from that of Hoare in his study. This is because today the penetration
    of computerized systems in our lives, with its own challenges and opportunities
    due to the great convergence of connected systems, interoperability and massive
    distribution of information, can make the most insignificant failure from a mere
    annoyance (such as losing access to your favorite music playlist) to a catastrophe
    of global proportions (such as a widespread failure in a worldwide satellite communications
    system).


    ### 3.3 The Gap Between Industry and Scientific Advances


    In 1996 Hoare noted that academic research gains in programming methodologies
    took up to 20 years to be adopted by industry as a sign of maturity and sanity
    - only in very specific areas and for a brief period would it be justified to
    apply the latest pure research advances to people''s everyday lives [\[61\]](#page-17-10).
    This mismatch also has the benefit of providing adequate planning of research
    and education as well as adequacy of the installed park in the industry. The result
    of not following this step is to adopt immature technologies and practices, with
    unpredictable and undesirable results, with no skilled labor available to apply
    it and make the necessary corrections when failures occur[14](#page-8-0) .


    Another consequence of not observing the maturity of cutting-edge research before
    its adoption in practice is the fact that, paradoxically, mature and effective
    technologies have not yet been adopted by industry, or when they are, they are
    isolated cases that cause astonishment when they present better results than those
    obtained with "state-of-the-art technologies". As an example we cite the adoption
    of the pairwise technique for test generation. The mathematical theory behind
    this technique has been around since the 1960s (see DESIGN, TESTING AND ESTIMATION
    IN COMPLEX EXPERIMENTATION. I. EXPANSIBLE AND CONTRACTIBLE FACTORIAL DESIGNS AND
    THE APPLICATION


    <span id="page-8-0"></span><sup>14</sup>If this scenario sounds like something
    that is happening in your industry, then maybe this is the reason.


    | Research                                               | Type      | Tests  |
    Time | Defects |

    |--------------------------------------------------------|-----------|--------|------|---------|

    | "A case study on pairwise testing application"<br>[66] | Ad<br>hoc | 14,041
    | 20h  | 10      |

    |                                                        | Pairwise  | 68     |
    4h   | 10      |

    | "A case study using testing technique for soft         | Manual    | 159    |
    6h   | 3       |

    | ware as a service (SaaS)" [67]                         | Pairwise  | 17     |
    1h   | 3       |


    <span id="page-9-0"></span>Table 4: Pairwise Application Research Results


    OF LINEAR PROGRAMMING TO COMBINATORIAL PROBLEMS publishied in 1965 [\[64](#page-17-13)]),
    the application in software testing using pairwise was presented earlier this
    century (see Combinatorial group testing and its applications published in 2000
    [\[65](#page-17-14)]). Recent research using these techniques (see Table [4\)](#page-9-0)
    shows promising numbers[15](#page-9-1):


    With results like this, it was expected that the adoption of the Pairwise technique
    to tests production in a cost-effective way would be more welcomed by the industry[16](#page-9-2)
    .


    # 4 Promises of Formal Development


    ### 4.1 Model Driven Development


    One of the most promising approaches to computer program development was MDD (Model
    Drive Development) and MDA (Model Drive Architecture), where models are the primary
    artifacts and the others, such as code, are generated from them [\[68\]](#page-18-2).
    The goal is to raise the level of abstraction, making software development closer
    to solving the requirements and problems outlined by its future users and making
    the developer''s life simpler and easier [\[69](#page-18-3)] and providing mainly
    automation of the process [\[70](#page-18-4)]. According to Yusuf et al. and Swithinbank
    et al., the advantages of using MDD are:


    - Increased developer productivity because of automation and focus on requirements
    analysis

    - Ease of maintenance many software was developed by specialists who left the
    organization at some point, and the technique would facilitate the evolution by
    retaining the knowledge of these specialists

    - Legacy reuse can make it easy and feasible to migrate old applications to new
    systems by applying the technique


    <span id="page-9-1"></span><sup>15</sup>We are aware that this sampling is neither
    meaningful nor representative, but only illustrative from our point of view.


    <span id="page-9-2"></span><sup>16</sup>Informally, in our contacts with software
    development practitioners and testing experts and discussions about the practice
    of Pairwise have ranged from ignorance of its existence to negative concepts and
    objections to its use as ineffective.


    - Adaptability adding or modifying is made easy given the automation already in
    place

    - Consistency every application will strictly follow the pattern established by
    the tools

    - Repetition great return on investment if applied throughout an organization

    - Improved communication with sponsors models are easier to interpret than code

    - Improved project communication templates help to understand the system design
    and assist in the discussion about the system itself

    - Domain knowledge capture if there is sufficient documentation of the system,
    the organization''s knowledge is maintained

    - Long-term asset high-level models and abstractions of business solutions are
    immune to technological change

    - Ability to postpone technology decisions focus on solving business problems
    allows decisions on non-functional problems to be left for a more opportune time


    #### 4.1.1 Problems with models


    The biggest problem with using models as the only source for software production
    is that trying to solve an organizational problem from conceptual abstractions
    larger than the machine languages used by computers to run programs implies a
    reduction of information [\[72](#page-18-5), p. 90]. This information has to be
    supplanted by the MDD tool itself by means of ready-made patterns, or from the
    developer by means of extensions, and that leads, according to Hailpern and Tarr
    [\[69\]](#page-18-3) to other problems:


    - Redundancy because of the widespread use of ready-made code examples

    - Unbridled back and forth problems to adjust the model to conform to another
    system or module

    - Moving complexity elsewhere rather than reducing it, requiring even more specialization


    #### 4.1.2 Future of MDD


    Standardization around UML and tool interoperability around the XMI (XML Metadata
    Interchange)[\[73\]](#page-18-6) standard can lead the open source community to
    produce products that can leverage development using MDD. Tools such as the Eclipse
    Modeling Framework (see <https://www.eclipse.org/modeling/emf/>) is an example
    of technology with this kind of potential, however this leads us to another conclusion.


    #### 4.1.3 Prospects


    Our view is that, the main barrier to the adoption of technologies like MDD, is
    how quickly this kind of solution becomes irrelevant.


    This irrelevance happens as the application and use of information technologies
    and platforms evolve.


    In the 1970s and 1980s, the adoption of CASE (Computer Aided Software Engineering)
    tools, which we can say were the precursors of MDD and MDA, was seen as a solution
    to the same problems we have listed above. At that time software development took
    place mainly on large computers, the Mainframes. But at the same time personal
    computers emerged, which at first were not seen as business tools, this soon became
    an untruth with the release of the IBM PC in 1981[\[74\]](#page-18-7) and since
    then software development has moved from the older and more expensive platform
    (Mainframes) to the more modern and cheaper (PCs (Personal Computers)), and this
    became increasingly true with the adoption of local networks like Novell in 1979
    [\[75](#page-18-8)] with over 500.000 computers installed in the world [\[76\]](#page-18-9)
    at the time. This movement continued, but once again changed focus. In 1989 Tim
    Berners-Lee invented the World Wide Web, in 1993 we had the release of the Mosaic
    browser by NCSA (National Center for Supercomputing Applications), and in 1994
    we had Netscape Navigator created by the same developers, now in a private company
    of the same name. Since then the development has been turning to applications
    presented by the browsers but running on corporate servers on the Internet. In
    early 2007 Apple introduces the iPhone, and at the end of the following year Google
    introduces Android. Still supported by the basic Internet infrastructure, application
    development shifts focus once again to the new mobile platform. And these days,
    some technologies are on the threshold, or at least promise to be, of creating
    new platforms, and among them we can mention Bitcoin (announced in 2009), virtual
    reality (as used in airplane pilot training and introduced as a consumer product
    in the 1990s by computer game companies like Sega in 1991) and augmented reality
    (made popular in games like Pok´emon Go in 2016) and finally the renaissance of
    Artificial Intelligence with the adoption of Machine Learning techniques.


    This rapid evolution and shift of focus to different platforms, with different
    approaches that decisively impact the architecture of the systems, databases,
    operating systems, programming languages, forms of presentation, number of application
    layers, and different APIs (Application Programming Interfaces) employed to mediate
    an increasingly large and complex network of interconnected products and services
    makes it practically impossible to develop, train personnel, and make them productive
    in the employment of any technology with the nature of the MDDs tools, which end
    up being relegated only to the role of modeling, right at the initial requirements
    gathering phase, within a longer development life cycle and without fulfilling
    the promise of covering it completely that has been made since the 1970s and 1980s
    by the CASE[\[77](#page-18-10)] tools, and which, as we saw earlier in this introduction,
    often does not motivate software development professionals and decision makers
    to bear the cost and time required in their absortion and deployment.


    # 5 Conclusions and Future Work


    If in one hand we have the promise of great advances and improvements in the quality
    of software products by applying techniques and tools developed by both academia
    and industry, despite the expected (and even desirable) delay between the development
    and adoption of these new technologies, we also have on the other hand the adoption
    of practices by the industry that make it difficult to incorporate certain mature
    technologies, or even to put them to the test, due to the lack of formalization
    that these practices prescribe in the name of agility in producing products quickly
    and meeting the desires of their customers.


    Without the adoption of formal software development methods, it is not possible
    to continue and progress with the advanced quality methods and methodologies developed
    in academia.


    The solution to this would be a back-and-forth approach, whereby by reverse engineering
    and starting from the source code of the computer programs, formal models are
    deduced and then complemented by the developers in order to produce the artifacts
    and inputs necessary for formal methods of quality verification and validation.
    Automation and adoption of standards are key to keeping costs within acceptable
    parameters for the industry.


    This approach has its pros and cons. Using reverse engineering to produce formal
    models will cause loss of information [17](#page-12-0), and this and other problems
    to come are what we set out to address.


    We intend to continue these studies with an analysis of the State of the Art in
    the conception and production of computer program tests, followed by ways of bringing
    together the methods and practices adopted by industry and the techniques developed
    by academia.


    <span id="page-12-0"></span><sup>17</sup>In general, models should have less information
    than the finished products that originated from them


    # Acronyms


    | API                | Application Programming Interface 12                                                                                  |  |  |

    |--------------------|-----------------------------------------------------------------------------------------------------------------------|--|--|

    | CASE<br>CCS<br>CSP | Computer Aided Software Engineering 12<br>Calculus of Communicating
    Systems 3<br>Communicating Sequential Processes 3 |  |  |

    | FSM                | Finite State Machine 3                                                                                                |  |  |

    | IT                 | Information Technology 2                                                                                              |  |  |

    | LOTOS<br>LTS       | Language Of Temporal Ordering Specification 3<br>Labelled
    Transition Systems 3                                        |  |  |

    | MDA<br>MDD         | Model Drive Architecture 10, 12<br>Model Drive Development
    10–12                                                      |  |  |

    | NCSA<br>NIST       | National Center for Supercomputing Applications 12<br>National
    Institute of Standards and Technology 8                |  |  |

    | OCL                | Object Constraint Language 2                                                                                          |  |  |

    | PC                 | Personal Computer 12                                                                                                  |  |  |

    | TDD                | Test Driven Development 2, 4                                                                                          |  |  |

    | UML                | Unified Modelling Language 2, 11                                                                                      |  |  |

    | XMI<br>XML         | XML Metadata Interchange 11<br>Extensible Markup Language
    11                                                          |  |  |


    # <span id="page-13-0"></span>References


    - <span id="page-13-1"></span>[1] K. Beck et al., "Manifesto for agile software
    development," 2001.

    - [2] B. Ramesh, L. Cao, K. Mohan, and P. Xu, "Can distributed software development
    be agile?" Communications of the ACM, vol. 49, no. 10, pp. 41–46, 2006.

    - <span id="page-13-3"></span>[3] V. One, "14th annual state of agile report,"
    Online: https://stateofagile.com, 2020.

    - <span id="page-13-2"></span>[4] R. Hoda, J. Noble, and S. Marshall, "How much
    is just enough? some documentation patterns on agile projects," in Proceedings
    of the 15th European Conference on Pattern Languages of Programs, 2010, pp. 1–13.

    - <span id="page-14-0"></span>[5] A. Cockburn and J. Highsmith, "Agile software
    development, the people factor," Computer, vol. 34, no. 11, pp. 131–133, 2001.

    - <span id="page-14-1"></span>[6] D. G. Feitelson, E. Frachtenberg, and K. L.
    Beck, "Development and deployment at facebook," IEEE Internet Computing, vol.
    17, no. 4, pp. 8–17, 2013.

    - <span id="page-14-2"></span>[7] G. G. Claps, R. B. Svensson, and A. Aurum, "On
    the journey to continuous deployment: Technical and social challenges along the
    way," Information and Software technology, vol. 57, pp. 21–31, 2015.

    - <span id="page-14-3"></span>[8] M. Hilton, T. Tunnell, K. Huang, D. Marinov,
    and D. Dig, "Usage, costs, and benefits of continuous integration in open-source
    projects," in 2016 31st IEEE/ACM International Conference on Automated Software
    Engineering (ASE), IEEE, 2016, pp. 426–437.

    - <span id="page-14-4"></span>[9] M. Canat, N. P. Catal`a, A. Jourkovski, S. Petrov,
    M. Wellme, and R. Lagerstr¨om, "Enterprise architecture and agile development:
    Friends or foes?" In 2018 IEEE 22nd International Enterprise Distributed Object
    Computing Workshop (EDOCW), IEEE, 2018, pp. 176–183.

    - <span id="page-14-5"></span>[10] K. Beck, "Aim, fire [test-first coding]," IEEE
    Software, vol. 18, no. 5, pp. 87–89, 2001.

    - <span id="page-14-6"></span>[11] J. B. Goodenough and S. L. Gerhart, "Toward
    a theory of test data selection," IEEE Transactions on software Engineering, no.
    2, pp. 156–173, 1975.

    - <span id="page-14-7"></span>[12] T. S. Chow, "Testing software design modeled
    by finite-state machines," IEEE transactions on software engineering, no. 3, pp.
    178–187, 1978.

    - [13] C. A. R. Hoare, "An axiomatic basis for computer programming," Communications
    of the ACM, vol. 12, no. 10, pp. 576–580, 1969.

    - [14] R. W. Floyd, "Toward interactive design of correct programs," in Readings
    in artificial intelligence and software engineering, Elsevier, 1986, pp. 331–
    334.

    - <span id="page-14-8"></span>[15] J. B. Warmer and A. G. Kleppe, The Object Constraint
    Language: Precise Modeling with UML. Addison Wesley, 1999.

    - <span id="page-14-9"></span>[16] M.-C. Gaudel, "Formal methods for software
    testing," in 2017 International Symposium on Theoretical Aspects of Software Engineering
    (TASE), IEEE, 2017, pp. 1–3.

    - <span id="page-14-11"></span><span id="page-14-10"></span>[17] B. Roscoe, "The
    theory and practice of concurrency," 1998.

    - [18] R. Milner, "Lectures on a calculus for communicating systems," in International
    Conference on Concurrency, Springer, 1984, pp. 197–220.

    - <span id="page-14-12"></span>[19] E. Brinksma, "An algebraic language for the
    specification of the temporal order of events in services and protocols," in Proc.
    of the European Teleinformatics Conference, Varese, Italy, 1983, pp. 533–542.

    - <span id="page-14-13"></span>[20] M. de Almeida Xavier, "Defini¸c˜ao e implementa¸c˜ao
    do sistema de tipos da linguagem circus," M.S. thesis, Universidade Federal de
    Pernambuco, 2006.

    - <span id="page-15-1"></span><span id="page-15-0"></span>[21] M. L. Minsky, Computation.
    Prentice-Hall Englewood Cliffs, 1967.

    - [22] E. M. Clarke, E. A. Emerson, and A. P. Sistla, "Automatic verification
    of finite-state concurrent systems using temporal logic specifications," ACM Transactions
    on Programming Languages and Systems (TOPLAS), vol. 8, no. 2, pp. 244–263, 1986.

    - <span id="page-15-11"></span><span id="page-15-2"></span>[23] C. A. Petri and
    W. Reisig, "Petri net," Scholarpedia, vol. 3, no. 4, p. 6477, 2008.

    - [24] B. George and L. Williams, "A structured experiment of test-driven development,"
    Information and software Technology, vol. 46, no. 5, pp. 337– 342, 2004.

    - <span id="page-15-3"></span>[25] F. Shull, G. Melnik, B. Turhan, L. Layman,
    M. Diep, and H. Erdogmus, "What do we know about test-driven development?" IEEE
    software, vol. 27, no. 6, pp. 16–19, 2010.

    - <span id="page-15-4"></span>[26] M. Josefsson, "Making architectural design
    phase obsolete-tdd as a design method," in Seminar course on SQA in Agile Software
    Development Helsinki University of Technology, 2004.

    - <span id="page-15-5"></span>[27] L. Madeyski, "The impact of test-first programming
    on branch coverage and mutation score indicator of unit tests: An experiment,"
    Information and Software Technology, vol. 52, no. 2, pp. 169–184, 2010.

    - <span id="page-15-6"></span>[28] D. Fucci, H. Erdogmus, B. Turhan, M. Oivo,
    and N. Juristo, "A dissection of the test-driven development process: Does it
    really matter to test-first or to test-last?" IEEE Transactions on Software Engineering,
    vol. 43, no. 7, pp. 597–614, 2016.

    - <span id="page-15-7"></span>[29] C. G. Hempel, "Studies in the logic of confirmation
    (i.)," Mind, vol. 54, no. 213, pp. 1–26, 1945.

    - <span id="page-15-8"></span>[30] G. ISO, "Information technology, open systems
    interconnection, conformance testing methodology and framework," International
    Standard IS, vol. 9646, 1991.

    - <span id="page-15-9"></span>[31] E. Dijkstra, "Structured programming," in Classics
    in software engineering, 1979, pp. 41–48.

    - <span id="page-15-10"></span>[32] P. W. Gaffney and J. W. Wooten, "Methods to
    ensure the standardization of fortran software. [pfort, dave, polish, and brnanl,
    for analysis and editing of codes, in fortran for pdp-10 and ibm 360 and 370],"
    May 1980.

    - <span id="page-15-12"></span>[33] A. Aiken, J. M. Hellerstein, and J. Widom,
    "Static analysis techniques for predicting the behavior of active database rules,"
    ACM Transactions on Database Systems (TODS), vol. 20, no. 1, pp. 3–41, 1995.

    - <span id="page-15-13"></span>[34] N. Ayewah, W. Pugh, D. Hovemeyer, J. D. Morgenthaler,
    and J. Penix, "Using static analysis to find bugs," IEEE software, vol. 25, no.
    5, pp. 22– 29, 2008.

    - <span id="page-15-14"></span>[35] R. P. Wilson and M. S. Lam, "Efficient context-sensitive
    pointer analysis for c programs," ACM Sigplan Notices, vol. 30, no. 6, pp. 1–12,
    1995.

    - <span id="page-16-10"></span><span id="page-16-9"></span>[36] I. F. Darwin,
    Checking C Programs with lint. " O''Reilly Media, Inc.", 1988.

    - [37] D. Evans, "Static detection of dynamic memory errors," ACM SIGPLAN Notices,
    vol. 31, no. 5, pp. 44–53, 1996.

    - <span id="page-16-11"></span>[38] D. Jackson, "Aspect: Detecting bugs with abstract
    dependences," ACM Transactions on Software Engineering and Methodology (TOSEM),
    vol. 4, no. 2, pp. 109–145, 1995.

    - <span id="page-16-12"></span>[39] D. L. Detlefs, "An overview of the extended
    static checking system," in Proceedings of The First Workshop on Formal Methods
    in Software Practice, Citeseer, 1996, pp. 1–9.

    - <span id="page-16-13"></span>[40] D. L. Detlefs, K. R. M. Leino, G. Nelson,
    and J. B. Saxe, "Extended static checking," 1998.

    - <span id="page-16-14"></span>[41] J. L. Jensen, M. E. Jørgensen, M. I. Schwartzbach,
    and N. Klarlund, "Automatic verification of pointer programs using monadic second-order
    logic," in Proceedings of the ACM SIGPLAN 1997 conference on Programming language
    design and implementation, 1997, pp. 226–234.

    - <span id="page-16-0"></span>[42] P. Li and B. Cui, "A comparative study on software
    vulnerability static analysis techniques and tools," in 2010 IEEE international
    conference on information theory and information security, IEEE, 2010, pp. 521–524.

    - <span id="page-16-1"></span>[43] C. Hankin and D. Le M´etayer, "Deriving algorithms
    from type inference systems: Application to strictness analysis," in Proceedings
    of the 21st ACM SIGPLAN-SIGACT symposium on Principles of programming languages,
    1994, pp. 202–212.

    - <span id="page-16-2"></span>[44] L. D. Fosdick and L. J. Osterweil, "Data flow
    analysis in software reliability," ACM Computing Surveys (CSUR), vol. 8, no. 3,
    pp. 305–330, 1976.

    - <span id="page-16-3"></span>[45] F. Hayes-Roth, "Rule-based systems," Communications
    of the ACM, vol. 28, no. 9, pp. 921–932, 1985.

    - <span id="page-16-4"></span>[46] R. S. Boyer, B. Elspas, and K. N. Levitt, "Select—a
    formal system for testing and debugging programs by symbolic execution," ACM SigPlan
    Notices, vol. 10, no. 6, pp. 234–245, 1975.

    - <span id="page-16-5"></span>[47] J. C. King, "Symbolic execution and program
    testing," Communications of the ACM, vol. 19, no. 7, pp. 385–394, 1976.

    - <span id="page-16-6"></span>[48] W. E. Howden, "Experiments with a symbolic
    evaluation system," in Proceedings of the June 7-10, 1976, national computer conference
    and exposition, 1976, pp. 899–908.

    - <span id="page-16-7"></span>[49] L. A. Clarke, "A program testing system," in
    Proceedings of the 1976 annual conference, 1976, pp. 488–491.

    - <span id="page-16-8"></span>[50] S. Abramsky and C. Hankin, Abstract interpretation
    of declarative languages. Prentice Hall Professional Technical Reference, 1987.

    - <span id="page-17-0"></span>[51] F. Nielson and N. Jones, "Abstract interpretation:
    A semantics-based tool for program analysis," Handbook of logic in computer science,
    vol. 4, pp. 527–636, 1994.

    - <span id="page-17-1"></span>[52] M. Davis, "The early history of automated deduction:
    Dedicated to the memory of hao wang," in Handbook of Automated Reasoning, Elsevier,
    2001, pp. 3–15.

    - <span id="page-17-2"></span>[53] W. Bibel, "Early history and perspectives of
    automated deduction," in Annual Conference on Artificial Intelligence, Springer,
    2007, pp. 2–18.

    - <span id="page-17-3"></span>[54] E. M. Clarke Jr, O. Grumberg, D. Kroening,
    D. Peled, and H. Veith, Model checking. MIT press, 2018.

    - <span id="page-17-4"></span>[55] B. Johnson, Y. Song, E. Murphy-Hill, and R.
    Bowdidge, "Why don''t software developers use static analysis tools to find bugs?"
    In 2013 35th International Conference on Software Engineering (ICSE), IEEE, 2013,
    pp. 672–681.

    - <span id="page-17-5"></span>[56] S. Panichella, V. Arnaoudova, M. Di Penta,
    and G. Antoniol, "Would static analysis tools help developers with code reviews?"
    In 2015 IEEE 22nd International Conference on Software Analysis, Evolution, and
    Reengineering (SANER), IEEE, 2015, pp. 161–170.

    - <span id="page-17-6"></span>[57] F. Zampetti, S. Scalabrino, R. Oliveto, G.
    Canfora, and M. Di Penta, "How open source projects use static code analysis tools
    in continuous integration pipelines," in 2017 IEEE/ACM 14th International Conference
    on Mining Software Repositories (MSR), IEEE, 2017, pp. 334–344.

    - <span id="page-17-7"></span>[58] M. Agnihotri and A. Chug, "A systematic literature
    survey of software metrics, code smells and refactoring techniques," Journal of
    Information Processing Systems, vol. 16, no. 4, pp. 915–934, 2020.

    - <span id="page-17-9"></span>[59] G. J. Myers, T. Badgett, T. M. Thomas, and
    C. Sandler, The art of software testing. Wiley Online Library, 2004, vol. 2.

    - <span id="page-17-8"></span>[60] I. ( O. for Standardization), Iso/iec/ieee
    24765: 2017 systems and software engineering-vocabulary, 2017.

    - <span id="page-17-10"></span>[61] C. A. R. Hoare, "How did software get so reliable
    without proof?" In International Symposium of Formal Methods Europe, Springer,
    1996, pp. 1– 17.

    - <span id="page-17-11"></span>[62] D. MacKenzie, "Computer-related accidental
    death: An empirical exploration," Science and Public Policy, vol. 21, no. 4, pp.
    233–248, 1994.

    - <span id="page-17-12"></span>[63] S. Planning, "The economic impacts of inadequate
    infrastructure for software testing," National Institute of Standards and Technology,
    2002.

    - <span id="page-17-13"></span>[64] S. R. Webb, "Design, testing and estimation
    in complex experimentation. i. expansible and contractible factorial designs and
    the application of linear programming to combinatorial problems," ROCKETDYNE CANOGA
    PARK CA, Tech. Rep., 1965.

    - <span id="page-17-14"></span>[65] D. Du, F. K. Hwang, and F. Hwang, Combinatorial
    group testing and its applications. World Scientific, 2000, vol. 12.

    - <span id="page-18-0"></span>[66] C. B. Monteiro, L. A. V. Dias, and A. M. da
    Cunha, "A case study on pairwise testing application," in 2014 11th International
    Conference on Information Technology: New Generations, IEEE, 2014, pp. 639–640.

    - <span id="page-18-1"></span>[67] A. C. da Silva, L. R. Correa, L. A. V. Dias,
    and A. M. da Cunha, "A case study using testing technique for software as a service
    (saas)," in 2015 12th International Conference on Information Technology-New Generations,
    IEEE, 2015, pp. 761–762.

    - <span id="page-18-2"></span>[68] L. Yusuf, M. Chessel, and T. Gardner, "Implement
    model-driven development to increase the business value of your it system," Retrieved
    January, vol. 29, p. 2008, 2006.

    - <span id="page-18-3"></span>[69] B. Hailpern and P. Tarr, "Model-driven development:
    The good, the bad, and the ugly," IBM systems journal, vol. 45, no. 3, pp. 451–461,
    2006.

    - <span id="page-18-4"></span>[70] R. Jacobs, ARCast with Ron Jacobs, English.
    [Online]. Available: [https://channel9.msdn.com/Shows/AR](https://channel9.msdn.com/Shows/ARCast+with+Ron+Jacobs/ARCast-5)
    (visited on 11/19/2020).

    - [71] P. Swithinbank et al., Patterns: Model-Driven Development Using IBM Rational
    Software Architect. IBM, International Technical Support Organization, 2005.

    - <span id="page-18-6"></span><span id="page-18-5"></span>[72] S. K. Langer, Feeling
    and form. Routledge and Kegan Paul London, 1953, vol. 3.

    - [73] O. M. Group, XML Metadata Interchange, English, Technology Standards Consortium,
    Jun. 2015. [Online]. Available: <https://www.omg.org/spec/XMI/About-XMI/>.

    - <span id="page-18-7"></span>[74] M. J. Miller, Why the IBM PC had an Open Architecture,
    English, News Site, publisher: Ziff Davis, Aug. 2011. [Online]. Available: [https://www.pcmag.com/archive/why-the-ib](https://www.pcmag.com/archive/why-the-ibm-pc-had-an-open-architecture-286065)

    - <span id="page-18-8"></span>[75] L. Proven, How the clammy claws of Novell NetWare
    were torn from today''s networks, English, News Site, publisher: Situation Publishing,
    Jul. 2013. [Online]. Available: [https://www.theregister.com/2013/07/16/netware\\_4\\_anniversary/](https://www.theregister.com/2013/07/16/netware_4_anniversary/).

    - <span id="page-18-9"></span>[76] R. Payne and K. Manweiler, CCIE: Cisco Certified
    Internetwork Expert Study Guide: Routing and Switching. John Wiley & Sons, 2006.

    - <span id="page-18-10"></span>[77] V. J. Mercurio, B. F. Meyers, A. M. Nisbet,
    and G. Radin, "Ad/cycle strategy and architecture," IBM Systems Journal, vol.
    29, no. 2, pp. 170– 188, 1990.'
  decisions:
    language: '- Qualified. Reason: English Paper'
    evaluation_prompt: Disqualified.
    related_work_prompt: Qualified
    novelty_prompt: Disqualified
    review_only_prompt: Qualified
  llm_input_used: '## Abstract

    A view on software testing, taken in a broad sense and considered a important

    activity is presented. We discuss the methods and techniques for applying tests

    and the reasons we recognize make it difficult for industry to adopt the

    advances observed in academia. We discuss some advances in the area and briefly

    point out the approach we intend to follow in the search for a solution.


    ## Introduction

    Accepting that tests are important, but are not always implemented or kept up
    to date during the lifetime of a program, we conclude that nothing has changed
    since the introduction of the Agile Manifesto earlier this century [\[1\]](#page-13-0)
    which we reproduce below and from which we highlight the passage "Software that
    works rather than complete documentation"[\[1\]](#page-13-0).


    - "Individuals and interactions over processes and tools"

    - "Working software over comprehensive documentation"

    - "Customer collaboration over contract negotiation"

    - "Responding to change over following a plan"


    This view has come to become an important industry trend [\[2\]](#page-13-1)[1](#page-0-0)
    , where face-to-face interactions are preferable to formal communication processes
    and working programs are preferable to comprehensive documentation, leaving the
    interpretation of the term "comprehensive" to each agile development team to decide
    [\[4](#page-13-2)]. In fact the agile method suggests that all documentation can
    be replaced by informal communication with an emphasis on tacit rather than explicit
    knowledge [\[5\]](#page-14-0).


    <span id="page-0-0"></span><sup>1</sup>The 14<sup>o</sup> annual report STATEofAGILE
    from 2020 points out that 95% of organizations practice agile software development
    methods. [\[3](#page-13-3)].


    On the other hand, the adoption of continuous integration and continuous delivery
    processes and tools has been steadily and unequivocally growing in both industry
    [\[6](#page-14-1), [7](#page-14-2)] and open source projects [\[8\]](#page-14-3),
    which can to some extent be interpreted as a denial of one of the principles of
    the Agile Manifesto: "Individuals and interactions over processes and tools",
    yet this does not come as a relief to the fact that many see benefits in building
    and maintaining formal models, but are not content to build them as they believe
    they consume too much time and resources, even believing in the slim chances of
    success of projects that do not use some modeling [\[9\]](#page-14-4).


    The implications of this view for the construction and maintenance of programs
    and the use and application of development methods and tools are discussed.'
  token_usage: 3112
  time_usage: 1.9213578701019287
- title: On Augmenting Scenario-Based Modeling with Generative AI
  abstract: 'The manual modeling of complex systems is a daunting task; and although
    a

    plethora of methods exist that mitigate this issue, the problem remains very

    difficult. Recent advances in generative AI have allowed the creation of

    general-purpose chatbots, capable of assisting software engineers in various

    modeling tasks. However, these chatbots are often inaccurate, and an

    unstructured use thereof could result in erroneous system models. In this

    paper, we outline a method for the safer and more structured use of chatbots as

    part of the modeling process. To streamline this integration, we propose

    leveraging scenario-based modeling techniques, which are known to facilitate

    the automated analysis of models. We argue that through iterative invocations

    of the chatbot and the manual and automatic inspection of the resulting models,

    a more accurate system model can eventually be obtained. We describe favorable

    preliminary results, which highlight the potential of this approach.'
  url: http://arxiv.org/abs/2401.02245v1
  keywords: ''
  document: '# On Augmenting Scenario-Based Modeling with Generative AI


    David Harel<sup>1</sup> , Guy Katz<sup>2</sup> , Assaf Marron<sup>1</sup> , and
    Smadar Szekely<sup>1</sup>


    <sup>1</sup>*Weizmann Institute of Science, Rehovot, Israel*


    <sup>2</sup>*The Hebrew University of Jerusalem, Jerusalem, Israel*


    *dharel@weizmann.ac.il, guykatz@cs.huji.ac.il, assaf.marron@weizmann.ac.il, smadarsz@gmail.com*


    Keywords: Generative AI, chatbots, scenario-based modeling, rule-based specifications


    Abstract: The manual modeling of complex systems is a daunting task; and although
    a plethora of methods exist that mitigate this issue, the problem remains very
    difficult. Recent advances in generative AI have allowed the creation of general-purpose
    chatbots, capable of assisting software engineers in various modeling tasks. However,
    these chatbots are often inaccurate, and an unstructured use thereof could result
    in erroneous system models. In this paper, we outline a method for the safer and
    more structured use of chatbots as part of the modeling process. To streamline
    this integration, we propose leveraging scenario-based modeling techniques, which
    are known to facilitate the automated analysis of models. We argue that through
    iterative invocations of the chatbot and the manual and automatic inspection of
    the resulting models, a more accurate system model can eventually be obtained.
    We describe favorable preliminary results, which highlight the potential of this
    approach.


    ## 1 INTRODUCTION


    Manually modeling complex systems is a daunting and error-prone endeavor. Furthermore,
    even after the system is modeled, ongoing tasks, such as modification and repair,
    continue to tax human engineers. Creating tools and methodologies for streamlining
    and facilitating this process has been the topic of extensive work, but many aspects
    of the problem remain unsolved [\(Pettersson and Andersson, 2016;](#page-11-0)
    [Biolchini](#page-10-0) [et al., 2005\)](#page-10-0).


    In recent years, the deep learning revolution has been causing dramatic changes
    in many areas, including computer science; and this revolution has recently taken
    yet another step towards general-purpose AI, with the release of ChatGPT, the
    learning-based chatbot [\(OpenAI, 2022\)](#page-11-1). ChatGPT, and other, similar
    tools [\(Google, 2023;](#page-10-1) [MetaAI, 2023\)](#page-11-2), can be used
    for countless kinds of tasks — including the modeling and coding of complex systems
    [\(Surameery and](#page-11-3) [Shakor, 2023\)](#page-11-3). An engineer might
    provide ChatGPT with a natural-language description of the system at hand, and
    receive in return a model of the system, or even computer code that implements
    it; and through iterative querying of ChatGPT, the system can later be modified
    or enhanced. This approach has already been used in several application domains
    [\(Surameery](#page-11-3) [and Shakor, 2023;](#page-11-3) [Burak et al., 2023;](#page-10-2)
    [Liu et al., 2023\)](#page-11-4).


    Although the ability to integrate ChatGPT[1](#page-0-0) into the software development
    cycle will undoubtedly empower engineers, there are also potential pitfalls that
    need to be taken into account. One drawback of Chat-GPT and similar tools is that
    the answers they provide are often inaccurate, and might overlook important aspects
    of the input query [\(Liu et al., 2023\)](#page-11-4). Moreover, the input query
    itself might be imperfect, and the engineer might not realize this until the system
    is deployed. Thus, if we make the reasonable assumption that human engineers will
    gradually become dependent on chatbots for various tasks, the risk increases that
    these inaccuracies will find their way into the final models of the system at
    hand and the code that ensues. We are thus faced with the following challenge:
    how can we harness ChatGPT in a way that lifts a significant load of work off
    the shoulders of the engineers, but which still results in sound and accurate
    models?


    Here, we advocate the creation of an encompassing modeling scheme that will combine
    ChatGPT with more traditional techniques for manual modeling of systems [\(Biolchini
    et al., 2005;](#page-10-0) [Pettersson and](#page-11-0) [Andersson, 2016\)](#page-11-0),
    in a way that will achieve this goal. Our core idea is to use ChatGPT in a controlled
    way; i.e., to repeatedly invoke it for various tasks, but to


    <span id="page-0-0"></span><sup>1</sup>We will often use the term *ChatGPT* somewhat
    generically, to represent an arbitrary, modern chatbot.


    then thoroughly inspect and analyze its results, to ensure their soundness and
    accuracy. We argue that such a scheme, if designed properly, would allow software
    and system engineers to benefit from the capabilities of modern chatbots, but
    without jeopardizing the quality of the resulting products. In the long run, we
    regard such a scheme as a step towards the *Wise Computing* vision [\(Harel et
    al., 2018\)](#page-10-3), which calls for turning the computer into a proactive
    member of the software development team — one which can propose courses of action,
    detect under-specified portions of the model, and assist in the various routine
    actions that naturally arise as part of the software development cycle.


    In order to design such a modeling scheme, we propose to leverage the extensive
    work carried out in the modeling community over the years. Specifically, we propose
    to focus on modeling frameworks that afford two benefits that complement the capabilities
    of ChatGPT: (i) the models produced by the framework are naturally well-aligned
    with how humans perceive systems; this, we believe, will make it easier for the
    human engineer to inspect ChatGPT''s output; and (ii) the resulting models are
    amenable to automated analysis tasks, such as model checking, which will support
    the automated detection of bugs and inconsistencies in the automatically generated
    models.


    Several modeling approaches fit this description, and many of them can probably
    be used, but for the initial evaluation presented here, we focus on *scenario-based
    modeling* (*SBM*) — a technique that generates models comprised of simple *scenarios*,
    each of which describes a single aspect of the system at hand [\(Harel et al.,
    2012b;](#page-11-5) [Damm and Harel,](#page-10-4) [2001\)](#page-10-4). As we
    later discuss, this can facilitate the smooth collaboration between ChatGPT and
    the human engineers.


    To demonstrate the potential of this combined framework, we focus on a few tasks
    that arise naturally as part of a system''s life cycle. Specifically, we discuss
    the initial design of the model, its testing and the verification of its properties,
    its later enhancement or repair due to the discovery of inconsistencies, and also
    a search for under-specified portions of the model. Our results, although preliminary,
    are very promising, and we hope this paper will form a basis for further research
    in this direction.


    In the remainder of the paper, we present the key concepts of our approach, and
    discuss a high-level plan for the next steps. We begin by introducing the concepts
    of SBM and language model-based chatbots in Section [2.](#page-1-0) Next, we present
    the proposed integration of SBM and ChatGPT in Section [3,](#page-3-0) followed
    by a discussion of some of the more advanced aspects of this integration in Section
    [4.](#page-5-0) We discuss related work in Section [5](#page-9-0) and conclude
    in Section [6.](#page-9-1)


    # <span id="page-1-0"></span>2 BACKGROUND


    # 2.1 Large Language Model-Based Chatbots


    ChatGPT (Chat Generative Pre-trained Transformer) is a large language model (LLM)
    based chatbot, developed by OpenAI [\(OpenAI, 2022;](#page-11-1) [Chang et al.,](#page-10-5)
    [2023\)](#page-10-5). The chatbot is able to conduct an iterative conversation
    of variable length, format, style, level of detail, and language. At each stage,
    the user presents a new prompt to ChatGPT, which then replies, based on all previous
    prompts in that conversation (the context). Following its debut in 2022, ChatGPT
    quickly became highly successful, and inspired multiple other companies to release
    their own chatbots [\(Google,](#page-10-1) [2023;](#page-10-1) [MetaAI, 2023\)](#page-11-2).


    Internally, ChatGPT is implemented using a proprietary series of generative pre-trained
    transformer (GPT) models, which in turn are based on Google''s transformer architecture
    [\(Vaswani et al., 2017\)](#page-11-6). Chat-GPT is fine-tuned for conversational
    applications, through a combination of supervised and reinforcement learning techniques,
    as well as manual adjustment by human engineers. ChatGPT''s training, as well
    as its inference, are considered very costly in terms of power consumption and
    processing resources.


    Functionality-wise, ChatGPT is highly versatile. Some of its many uses include
    generating student essays [\(AlAfnan et al., 2023\)](#page-10-6), writing and
    debugging computer programs [\(Surameery and Shakor, 2023\)](#page-11-3), and
    composing music [\(Lu et al., 2023\)](#page-11-7). However, it will sometimes
    produce plausible-sounding but incorrect or nonsensical answers — a common limitation
    for large language models [\(Gregorcic and Pendrill,](#page-10-7) [2023\)](#page-10-7).


    #### 2.2 Scenario-Based Modeling


    Scenario-based modeling [\(Harel et al., 2012b\)](#page-11-5) (SBM) is a modeling
    approach aimed at modeling complex, reactive systems. The main component in a
    scenario-based (SB) model is the *scenario object*, which describes a single behavior
    of the system at hand, whether desirable or undesirable, so that one can specify
    it as necessary, allowed or forbidden. Each scenario object does not directly
    interact with its counterparts, and can be created in isolation. Crossscenario
    interaction is allowed only through a global execution mechanism, which can execute
    a collection of scenarios in a manner that produces cohesive, global behavior.


    There exist several flavors of SBM, employing slightly different mechanisms for
    cross-scenario interactions. We focus here on a particular set of idioms, which
    has become quite popular: the *requesting*, *waiting-for* and *blocking* of discrete
    events [\(Harel](#page-11-5) [et al., 2012b\)](#page-11-5). During execution,
    each scenario object repeatedly visits designated *synchronization points*, and
    in each of these the global execution mechanism selects one event for triggering.
    A scenario object may declare events that it wishes to be triggered (*requested*
    events), events that it wishes to avoid (*blocked* events), and also events it
    does not request itself but would like to monitor (*waited-for events*). The execution
    mechanism collects these declarations from each of the scenario objects (or a
    subset thereof [\(Harel et al., 2013a\)](#page-10-8)), selects one event that
    is requested and not blocked, and then informs all relevant scenario objects of
    this selection.


    In a given synchronization point, multiple events may be requested and not blocked,
    and several strategies have been proposed for selecting one of them. These include
    an arbitrary selection, a random selection, a round-robin mechanism, and look-ahead
    that simulates possible progression of the execution and selects events with an
    attempt to achieve a desirable objective specified a-priori (e.g., the avoidance
    of deadlocks). Executing a scenario-based program in this manner is termed play-out
    [\(Harel and Marelly,](#page-11-8) [2003\)](#page-11-8)).


    Fig. [1](#page-2-0) depicts a simple example of an SB model. The system at hand
    controls the water level in a water tank, which is equipped with hot and cold
    water taps. Each scenario object appears as a transition system, in which nodes
    corresponds to the predetermined synchronization points. Scenario object ADDHOT-WATER
    repeatedly waits for WATERLOW events, and when such an event is triggered, it
    requests three times the event ADDHOT. Similarly, scenario object AD-DCOLDWATER
    requests the addition of cold water. When the model includes only objects ADDHOTWA-TER
    and ADDCOLDWATER, three ADDHOT events and three ADDCOLD events may be triggered
    in any order during execution. If we wish to maintain a more stable water temperature
    within the tank, we might add the scenario object STABILITY, to enforce the interleaving
    of ADDHOT and ADDCOLD — through the use of event blocking. An execution trace
    of the model containing all three objects appears in the event log.


    The SBM framework has been implemented on top of multiple high-level languages,
    including


    <span id="page-2-0"></span>


    | ADDHOTWATER          | ADDCOLDWATER         | STABILITY | EVENT LOG |

    |----------------------|----------------------|-----------|-----------|

    |                      |                      | wait for  |           |

    | wait for<br>WATERLOW | wait for<br>WATERLOW | ADDHOT    | ···       |

    |                      |                      | while     | WATERLOW  |

    | request              | request              | blocking  | ADDHOT    |

    | ADDHOT               | ADDCOLD              | ADDCOLD   | ADDCOLD   |

    |                      |                      |           | ADDHOT    |

    | request              | request              | wait for  | ADDCOLD   |

    | ADDHOT               | ADDCOLD              | ADDCOLD   | ADDHOT    |

    |                      |                      | while     | ADDCOLD   |

    | request              | request              | blocking  | ···       |

    | ADDHOT               | ADDCOLD              | ADDHOT    |           |


    Figure 1: A scenario-based model for a system that controls the water level in
    a tank with hot and cold water taps (taken from [\(Harel et al., 2014\)](#page-11-9)).


    Java [\(Harel et al., 2010\)](#page-11-10), C++ [\(Harel and Katz, 2014\)](#page-10-9),
    Python [\(Yaacov, 2023\)](#page-11-11), JavsScript [\(Bar-Sinai et al.,](#page-10-10)
    [2018\)](#page-10-10) and ScenarioTools [\(Greenyer et al., 2017\)](#page-10-11).
    Furthermore, SBM has been applied in modeling various complex systems, such as
    web-servers [\(Harel](#page-10-9) [and Katz, 2014\)](#page-10-9), cache coherence
    protocols [\(Harel](#page-10-12) [et al., 2016a\)](#page-10-12) and robotic controllers
    [\(Gritzner and](#page-10-13) [Greenyer, 2018\)](#page-10-13). In order to simplify
    the presentation in the following sections, we mostly describe SB models as transitions
    systems.


    In formally defining SBM, we follow the definitions of [\(Katz, 2013\)](#page-11-12).
    A scenario object *O* over event set *E* is a tuple *O* = ⟨*Q*,δ,*q*0,*R*,*B*⟩,
    where the components are interpreted as follows:


    - *Q* is the set of states. Each state represents a single, predetermined synchronization
    point;

    - *q*<sup>0</sup> ∈ *Q* is the initial state;

    - *R* : *Q* → 2 *E* and *B* : *Q* → 2 *<sup>E</sup>* map states to the sets of
    events requested and blocked at these states, respectively; and

    - δ : *Q*×*E* → 2 *<sup>Q</sup>* is the transition function, which indicates how
    the object switches states in response to the triggering of events.


    Once the individual scenario objects are created, they can be composed in a pairwise
    fashion. Two scenario objects *O* <sup>1</sup> = ⟨*Q* 1 ,δ 1 ,*q* 1 0 ,*R* 1 ,*B*
    1 ⟩ and *O* <sup>2</sup> = ⟨*Q* 2 ,δ 2 ,*q* 2 0 ,*R* 2 ,*B* 2 ⟩, specified over
    a common set of events *E*, can be composed into a single scenario object *O*
    <sup>1</sup> ∥ *O* <sup>2</sup> = ⟨*Q* <sup>1</sup> ×*Q* 2 ,δ,⟨*q* 1 0 ,*q* 2
    0 ⟩,*R* <sup>1</sup> ∪*R* 2 ,*B* <sup>1</sup> ∪*B* 2 ⟩, where:


    - ⟨*q*˜ 1 ,*q*˜ 2 ⟩ ∈ δ(⟨*q* 1 ,*q* 2 ⟩, *e*) if and only if ˜*q* <sup>1</sup>
    ∈ δ 1 (*q* 1 , *e*) and ˜*q* <sup>2</sup> ∈ δ 2 (*q* 2 , *e*); and

    - the union of the labeling functions is defined in the natural way; i.e., *e*
    ∈ (*R* <sup>1</sup> ∪ *R* 2 )(⟨*q* 1 ,*q* 2 ⟩) if and only if *e* ∈ *R* 1 (*q*
    1 ) ∪ *R* 2 (*q* 2 ), and *e* ∈ (*B* <sup>1</sup> ∪ *B* 2 )(⟨*q* 1 ,*q* 2 ⟩) if
    and only if *e* ∈ *B* 1 (*q* 1 )∪*B* 2 (*q* 2 ).


    Using the composition operator ∥, we can define a *behavioral model M* as a collection
    of scenario objects, *M* = {*O* 1 ,*O* 2 ,...,*O <sup>n</sup>*}. The executions
    of *M* are defined to be the executions of the single, composite object *O* =
    *O* <sup>1</sup> ∥ *O* <sup>2</sup> ∥ ... ∥ *O n* . Thus, each execution starts
    from the initial state of *O*, which is the *n*tuple of the initial states of
    its constituent objects, and throughout the run, in each state *q* an enabled
    event *e* ∈ *R*(*q*)−*B*(*q*) is chosen for triggering, if one exists. The execution
    then moves to a state ˜*q* ∈ δ(*q*, *e*), and the process repeats.


    # <span id="page-3-0"></span>3 INTEGRATING CHATGPT AND SBM


    ### 3.1 Basic Integration


    As a first step to integrating ChatGPT and SBM, we present a simple methodology
    for creating scenario objects from free-text, using ChatGPT. In order to get ChatGPT
    to present its output as a scenario object, we propose to include in each query
    a *preamble* that describes the SBM semantics, as described in Section [2.](#page-1-0)
    This preamble instructs ChatGPT to present its output as a transition system,
    in which each state is labeled with requested and blocked events, and has a set
    of transition rules indicating its successor state, based on the event later triggered.
    Note that this preamble needs only be crafted once, and can then be attached to
    any subsequent query. If a specific implementation of SBM is used (e.g., C++,
    Java), the output can be crafted to match the required syntax of that implementation.


    The simple preamble that we used in our initial experiments is as follows:


    I would like you to help me create a scenariobased model. A scenario-based model
    is comprised of a finite set of events, and scenarios. Each scenario is a sequence
    of states. In each state the scenario can request events, wait-for events, and
    block events. Afterwards, one event that is requested by at least one scenario,
    but is blocked by no scenario, is selected for triggering. Scenarios that requested
    or waited-for this scenario may then change states, and the process repeats itself.
    Scenarios cannot have internal loops or variables; these must be expressed using
    additional states.


    To display scenarios, please use the following format. For each state, mention
    the state''s name, and the sets of requested and blocked events. Also, mention
    the transitions to other states. For example:


    - s1: request X, block Y. If X is triggered, go to state s2.

    - s2: request Y, block X. If Y is triggered, go to state s2.

    - Now, here are the details of the specific scenario-based model I''d like you
    to create. . .


    Given this preamble, we can use ChatGPT to produce the model we described in Section
    [2.](#page-1-0) To produce the ADDHOTWATER scenario, we use the following prompt:


    Consider the event set WaterLow, AddHot, AddCold.


    Please suggest a scenario that waits for event WaterLow (without requesting it).
    When Water-Low is triggered, the scenario performs the following, 3 times in a
    row: request the event AddHot, and wait for it to be triggered (both in a single
    state). When the 3rd AddHot event occurs, the scenario transitions back to its
    initial state.


    In response to this prompt, the chatbot replies:


    - s1: wait for WaterLow. If WaterLow is triggered, go to state s2.

    - s2: request AddHot, wait for AddHot. If AddHot is triggered, go to state s3.

    - s3: request AddHot, wait for AddHot. If AddHot is triggered, go to state s4.

    - s4: request AddHot, wait for AddHot. If AddHot is triggered, go to state s1.


    Which is equivalent to the transition system we presented in Fig. [1.](#page-2-0)
    The ADDCOLDWATER scenario can be generated symmetrically. Now, if we wish to incrementally
    modify the system by introducing the STA-BILITY scenario, we can issue the following
    prompt:


    Please suggests a scenario that ensures that uses blocking to ensure that no two
    consecutive AddHot events can be triggered, and that no two consecutive AddCold
    events can be triggered; that is, once AddHot is triggered, AddCold must be triggered
    before AddHot can be triggered again, and vice versa. This scenario should not
    request any events, and should work regardless of any WaterLow events.


    And in response, the chatbot produces the STABILITY scenario, as described in
    Fig. [1.](#page-2-0)


    We note a subtle difference between the way we prompted ChatGPT for the first
    two scenarios, ADDHOTWATER and ADDCOLDWATER, and our prompting for STABILITY.
    In the former two cases, our prompt contained information that roughly described
    the transition system itself, whereas in the third case our description was more
    high-level, and did not mention the word "state". Still, in both cases, ChatGPT
    produced the desired result. This demonstrates the wide range of specifications
    with which the chatbot can successfully deal, and suggests that it can be used
    even when the engineers are themselves not entirely certain of the structure of
    the scenario they require. While it stands to reason that more accurate descriptions
    would lead to more accurate results, it appears that even high-level descriptions
    can be very useful, especially when combined with the automated analysis techniques
    that we discuss next.


    ### 3.2 The Proposed Methodology


    Building upon this basic integration of ChatGPT and SBM, we now outline a structured
    LLM-agnostic and language-agnostic methodology for creating complex *reactive
    models*, i.e., models of systems that interact with their environment repeatedly
    over time, and receive external inputs [\(Harel and Pnueli, 1985\)](#page-11-13).
    Numerous modern, critical systems can be regarded as reactive [\(Aceto et al.,
    2007\)](#page-10-14), and consequently there has been extensive research on developing
    tools and methods for modeling these systems. Despite this tremendous effort,
    there still remain significant gaps, which could result in models that are inaccurate
    or that are difficult to maintain. The present work, which can be seen as an element
    of the Wise Computing vision [\(Harel et al., 2018\)](#page-10-3), seeks to mitigate
    these gaps, through the creation of advanced, intelligent tools that will begin
    to undertake the software and system development tasks that are normally assigned
    to humans. The core of the approach is to have system components be generated,
    iteratively and incrementally, with the help of an LLM; and to have the LLM''s
    outputs checked systematically, and perhaps automatically, using various tools
    and methods.


    - 1. Describe the problem and the environment textually, in natural language.

    - 2. Choose a compositional, scenario-based modeling language, which has well-defined
    execution semantics and is suitable for the incremental development of the system.

    - 3. Obtain an LLM that is familiar with the application domain in general, or
    can readily gain extensive knowledge about that domain, and which can (or can
    be taught to) generate code in the chosen scenario-based language.

    - 4. Describe, perhaps iteratively, the semantics of the scenario-based language
    to the LLM as a preamble. Confirm that the LLM indeed internalizes the details
    of the language semantics by teaching it to execute (i.e., play out [\(Harel and
    Marelly, 2003\)](#page-11-8)) systems described as scenarios or rules in the chosen
    language, where the LLM outputs logs of triggered events, scenario states, composite
    system states, values of environment variables and changes thereto, etc.

    - 5. Iteratively add scenarios and refine existing ones, as follows:

    - (a) Describe in a prompt one or more scenarios for certain not-yet-specified
    requirements or aspects of the system.

    - (b) Have the LLM generate actual scenarios for the prompt, in the chosen language.

    - (c) Have the LLM generate natural language description of properties to be verified,
    executable test cases, and assertions for formal verification tools, per the original
    requirements. This constitutes stating the requirement at hand from different
    perspectives.

    - (d) Carry out initial testing and validation within the LLM, challenging the
    LLM to find gaps and incorrect execution paths on its own. Correct the natural
    language specification and prompts as needed.

    - (e) Systematically check the LLM output outside of the LLM, using any or all
    of the following: code reviews by human engineers, unit testing of individual
    scenarios, subsystem testing with some or all of the already-developed scenarios,
    model checking of the new scenarios, as well as those of the composite system,
    etc. The testing is to be carried out in the execution environment of the language,
    and model checking is to be carried out using a suitable formal verification tool.
    Both should be independent of the LLM environment. Possibly automate the subjecting
    of generated scenarios to testing and model checking.

    - (f) When errors are found, do not modify the generated code. Instead, revise
    the LLM prompts until correct system scenarios and verification and testing properties
    are generated. This step is critical for aligning the stakeholder (i.e., customer)
    view of the requirements, the developer''s understanding, and the actual code.

    - (g) Once the set of generated scenarios seems ready, repeat the likes of step
    (d), asking the LLM to find gaps or potential failures in this set of scenarios;
    specifically look for LLM


    suggestions of new environment considerations that prevent the system from working
    correctly. This step simulates the common system engineering task of having external
    experts or potential customers review advanced prototypes of systems. Repeat earlier
    steps as needed.


    Next, we elaborate on some of these steps, and provide simple, illustrative examples.


    # <span id="page-5-0"></span>4 USING THE METHOD IN THE DEVELOPMENT CYCLE


    #### 4.1 Code Generation


    Code generation is probably the most straightforward chatbot capability that we
    propose be integrated into the development cycle. In Section [3](#page-3-0) we
    showed that ChatGPT can generate an (executable) SB model — a capability that
    has also been demonstrated with other languages [\(Surameery and Shakor, 2023;](#page-11-3)
    [Burak et al.,](#page-10-2) [2023;](#page-10-2) [Liu et al., 2023\)](#page-11-4).
    A unique advantage in the context of SB systems is the ability to generate standalone
    scenarios, which can be reviewed and tested separately, and then be incrementally
    added to the system under development. In our preliminary testing for this paper,
    we experimented with code generation for requirements in the realms of autonomous
    vehicles, algorithms on data structures, simulating natural phenomena, and control
    systems. In all of these, the ChatGPT/SBM integration proved useful.


    #### 4.2 Modeling


    Once ChatGPT understood the principles underlying scenario-based models, it was
    able to combine its knowledge of the problem domain, the world at large, and logic,
    in order to develop or enhance a model. It was able to introduce new environment
    events, describe the sensor scenarios that are required for triggering these events,
    and then add the necessary application scenarios that react to these events. For
    example, when we asked ChatGPT to generate scenariobased code for a bubble-sort
    algorithm to be used by a robot moving numbered tiles on a sequence of cells,
    it was able to introduce the events of detecting the arrival of a tile at the
    tail-end of the array, as well as scenarios for reacting to such events.


    #### 4.3 Play Out & Simulation


    After a few iterations, we were able to teach ChatGPT to produce an execution
    log of an arbitrary scenariobased specification. At first we observed "wishful
    thinking", where ChatGPT described the run as it should be per the problem description.
    However, as illustrated in Fig. [2,](#page-6-0) ChatGPT was then able to follow
    the execution steps correctly, displaying at each synchronization point the event
    that triggered the state transition that led to this synchronization point, and
    a table of all scenarios, indicating for each one whether or not it was put into
    action by the triggered event, and providing its declaration of requested, blocked
    and waited-for events.


    #### 4.4 SMT-Like Model Analysis


    One of the advantages of scenario-based modeling is its amenability to formal
    verification with appropriate tools, both by exhaustive model checking traversing
    all paths, and by using domain knowledge for Satisfiability Modulo Theory (SMT)
    verification [\(Harel et al.,](#page-11-14) [2011;](#page-11-14) [Harel et al.,
    2013b\)](#page-10-15). This is accomplished by virtue of the abstraction and encapsulation
    of domainspecific processes, actions and conditions as events and states. System
    complexity thus emerges from the composition of relatively small intuitive scenarios
    reflecting individual requirements, and not from the intricate conditional flow
    of delicate and sensitive processes with numerous steps.


    Our experiments have shown that ChatGPT is able to leverage this kind of abstraction
    and encapsulation to identify cases that a specification either omitted or handles
    incorrectly. For example, we presented ChatGPT with the following three requirements
    for an autonomous vehicle: (i) always stop at a red light; (ii) always obey a
    police person''s instructions; (iii) never injure a person. ChatGPT readily recognized
    that these requirements may be in conflict with each other. Given the safety property
    of not entering an intersection when the traffic light is red, it pointed out
    that the AV may be unable to stop if the road is icy, that it may enter the intersection
    involuntarily if after stopping it is hit from behind by a car that did not stop,
    and, furthermore, that it may injure a person without moving, if a person walks
    behind the AV, and another vehicle hits the person who is then thrust against
    the AV.


    In a mathematical, SMT-like analysis, ChatGPT was able to identify a particularly
    interesting execution path. We presented it with a system consisting of the following
    four scenarios: the first requests flashing a red light briefly in response to
    an external time tick event, and then waits for any event; the second does the
    same with a green light; the third scenario blocks the event of flashing the red
    light every third tick, and another blocks flashing the green light every


    <span id="page-6-0"></span>


    | Scenario Short name   Wake-up   Requested |   |   |                   | Blocked   Waited
    for |  |

    |-------------------------------------------|---|---|-------------------|----------------------|--|

    |                                           |   |   |                   |                      |  |

    | GI                                        |   | Y | Flash Green Light |                      |  |

    | RT.                                       | Y |   | Flash Red Light   |                      |  |

    | ALTG                                      |   | Y |                   | Flash
    Green Light    |  |

    | ALTR                                      |   | Y |                   | Flash
    Red Light      |  |


    | Scenario Short name   Wake-up   Requested |             |                                      |
    Blocked | Waited for                                            |

    |-------------------------------------------|-------------|--------------------------------------|---------|-------------------------------------------------------|

    | GL<br>RL<br>ALTG<br>ALTR                  | N<br>Y<br>N | Flash Green Light<br>Flash
    Red Light |         | Flash Green Light  Flash Red Light<br>Flash Red Light |


    Figure 2: *A simulation of scenario execution with ChatGPT.* Top: textual description
    of scenarios in user prompts. Middle and bottom: The first two entries in ChatGPT''s
    output when requested to play out the execution, showing triggered events, the
    scenarios that reacted to the triggered events, and the declarations of each scenario
    at the next synchronization point.


    five ticks. We then added the requirement that at least one light must be flashed
    between any two consecutive time ticks. ChatGPT was able to respond that the requirement
    is not satisfied, and that every 15 time ticks (3 · 5) both events will be blocked.
    ChatGPT apparently recognized this by arithmetic calculation and not by step-by-step
    simulation, as it was able to quickly reach the same conclusion for arbitrarily
    large numbers.


    Here is an excerpt from this conversation, after describing the scenarios. When
    prompted with


    now a new requirement came in: always, after every tick, *at least one light*
    must be flashed. Does the combined system of red flashing and green flashing with
    the 3 and 5 restrictions satisfy the requirement?


    ChatGPT replies:


    To analyze whether the combined system satisfies the new requirement that at least
    one light must flash after every tick, we must consider the synchronization points
    of both the red and green systems with their respective restrictions:


    The red system flashes on every tick, except every third one. The green system
    flashes on every tick, except every fifth one. If we consider the sequence of
    ticks and apply both restrictions, the combined system would not flash a light
    on a tick that is both a multiple of three and five specifically, on ticks that
    are multiples of fifteen, since 15 is the least common multiple of 3 and 5. At
    such ticks, both systems would be blocking their respective lights from flashing
    due to their individual restrictions.


    Therefore, on ticks that are multiples of fifteen (like the 15th, 30th, 45th tick,
    and so on), neither light would flash, violating the new requirement that at least
    one light must flash after every tick.


    In conclusion, the combined system of red flashing and green flashing with the
    restrictions of not flashing on ticks divisible by three and five, respectively,
    does not satisfy the requirement that at least one light must be flashed after
    every tick.


    ChatGPT also successfully discussed generaliza-


    tions of this computation, including calculating the number of composite system
    states that would have to be traversed in exhaustive model checking.


    It is interesting to note though, that while the LLM was able to apply its logic
    to find system execution paths with special properties, it did not do well when
    asked to list *all* the execution paths possessing a certain property. For example,
    we presented it with a 4-by-4 grid of cells, and asked for the steps that a robot
    may take from (0,0) at the bottom left to (3,3) at the top right, using the events
    for right, left, up, and down motions. We also required that the path remains
    within the grid. ChatGPT was able to list paths like *right,right,right,up,up,up*
    and *right,up,right,up,right,up*, but when asked to list all the paths that go
    through (1,3) it failed, listing several paths that do not reach the desired target,
    including *up,up,up,right,down,down,right,up,right*. Chat-GPT is thus not yet
    ready to carry out exhaustive model checking on scenario-based systems. This is
    not really an issue, however, since with the proposed methodology verification
    of the LLM''s outputs will be carried out independently of the LLM environment.


    #### 4.5 Identifying Under-Specification


    Even when the created model addresses the stated requirements perfectly, as may
    be possible with synthetic or reduced problem descriptions, there may be additional
    areas of the broader problem domain that can or must be handled. In response to
    appropriate prompts ChatGPT was able to significantly enrich a developed model,
    or helped restate the explicit boundaries and expected scope of the intended system.


    For example, in [\(Harel et al., 2023\)](#page-10-16) we presented ChatGPT with
    a scenario-based model that was developed for science teaching purposes. The model
    describes how bees depend on the ambient temperature for switching between hibernation
    and being active, flower plants depend on hours of daylight in order to start
    blooming, and active bees depend on blooming flowers for survival. In the original
    model, average temperatures and number of daylight hours were parameters that
    students could adjust to see how certain combinations of settings might be sustaining
    while others can cause the bees to die out due to hunger.


    When we asked ChatGPT to offer enhancements to the education model, it suggested
    to consider weather-related variations that modify the prespecified fixed average
    temperature and the number of daylight hours for a given time of year.


    ### 4.6 Explainability


    Given a certain observed or expected behavior of the system, ChatGPT is able to
    produce a natural language description of the cause of that behavior, finding
    a reasonable balance between the arcane details of the complete execution trace,
    and the overall requirement that "this is what the system was supposed to do".


    For example, following a paragraph on the semantics of SB modeling we presented
    the following prompt:


    In a system with two scenarios, one waits for time tick events and after each
    time tick requests a short beep (100 msec). The other scenario waits for time
    ticks, and after each such event requests a long beep (500 msec). Each of these
    scenarios waits for both the short and long beep events, so that after each time
    tick, exactly one beep occurs. Actuator scenarios actually cause the respective
    beeping action. Time tick events are injected by the environment every one second.
    A user reported that it watched the system for 20 seconds, and never heard a long
    beep. Can you explain why this may have happened?


    ChatGPT response was:


    The user''s report of not hearing a long beep in a system with two scenarios—one
    requesting a short beep after each time tick, and the other requesting a long
    beep after each time tick—can be explained through the principles of Scenariobased
    Modeling (SBM).


    In SBM, when multiple scenarios request different events after a shared trigger
    event, the execution environment selects one of these requested events to occur.
    The selection can be arbitrary or follow a specific strategy, such as alphabetic
    or lexical order, priority, random, round-robin, etc. In this case, if the event
    selection strategy prioritizes the short beep over the long beep (for example,
    due to alphabetic order, where "short beep" comes before "long beep"), then the
    short beep will always be selected to occur after each time tick, and the long
    beep will never be executed. Since time tick events are injected by the environment
    every second, and both scenarios are waiting for this event, each time tick leads
    to both scenarios being activated. However, if the system consistently selects
    the short beep event over the long beep event after each time tick, the scenario
    requesting the long beep will never progress to the state where it actually triggers
    the long beep. As a result, only short beeps will be heard.


    In summary, the user''s observation of not hearing any long beeps can be attributed
    to the event selection mechanism within the SBM framework, which in this case
    consistently favors the short beep event over the long beep event. This could
    be due to the selection strategy used by the system (like alphabetic order) or
    other operational logic specifics.


    We then challenged ChatGPT again:


    Thank you. Assume that the developers changed the selection algorithm and they
    now select events randomly. The user tested the system many times, for hours,
    and never heard a long beep. Can you try to explain?


    To this, ChatGPT produced a very comprehensive answer, covering possible issues
    with application scenarios, the execution infrastructure, and humanmachine interaction.
    Here are some excerpts:


    If the event selection algorithm was changed to randomly select between the short
    beep and long beep events, but the user still never hears a long beep, there are
    several potential explanations:


    Implementation Error in Random Selection: The most straightforward explanation
    could be a bug or implementation error in the random selection algorithm. It might
    not be truly random, or it could be incorrectly favoring the short beep event
    due to a coding mistake.


    Issue with the Long Beep Scenario: There might be a problem with the scenario
    that requests the long beep. This could be a coding error or a logical flaw in
    the scenario itself, preventing it from properly requesting the long beep event.


    User Perception or Reporting Error: Finally, there could be an issue with how
    the user is perceiving or reporting the beeps. They might be missing the long
    beep due to distraction, ambient noise, or misunderstanding what constitutes a
    long beep in the context of this system.


    To diagnose the issue accurately, developers would need to conduct thorough testing
    and debugging, looking into each of these potential areas to find where the problem
    lies.


    While such explanations are impressive, they were not perfect. We had cases where
    we presented Chat-GPT with well specified scenarios and a description of an observed
    execution log, i.e., an event sequence, but ChatGPT was not able to retrospectively
    produce a step by step play-out sequence that would generate this execution log.
    Instead, it provided incorrect explanations for that event sequence. While frequently
    providing ChatGPT with better reminders and interactive practice of its knowledge
    of semantics and play-out may remedy such issues, this caveat is a reminder that
    all outputs produced by the LLM must be formally checked.


    # 4.7 Accommodating Semantic Flexibility


    Most software development studios are tied to specific languages and their associated
    semantics. In our experiments, ChatGPT was able to accommodate, and discuss, alternative
    semantics.


    For example, in the water tap example in Section [2,](#page-1-0) when the scenario
    ADDHOTWATER is in any of the states where it requests ADDHOT, it cannot react
    to WATERLOW, since it is not waiting for that event. By contrast, in the semantics
    of the LSC scenario-based language, the infrastructure constantly listens for
    all events that are waited for in the first state of all scenarios. When such
    an event occurs, the infrastructure instantiates another copy of the scenario.
    In fact, from our first textual descriptions of SBP, ChatGPT understood this semantics
    to be the default.


    In another example, we asked ChatGPT to generate scenarios for Quicksort. Before
    starting, it commented that it will be hard, as classical solutions are recursive.
    We then pointed out to ChatGPT that there is a published implementation that is
    iterative, not recursive [\(Harel et al., 2021\)](#page-11-15), that is structured
    as instructions to human workers arranging cars in an automobile dealership parking
    lot according to, say, window-sticker price, where each employee had one narrow
    role. ChatGPT readily accommodated the new mindset and produced the desired scenario-based
    specification.


    #### 4.8 Interactive Mutual Learning


    In our experiments, we noticed that ChatGPT learns from multiple prompts, discussions
    and exploration better than from concise or detailed descriptions. We believe
    that the same may hold for software and system developers. Interactive, agile
    development processes may not be just trial and error, or spiral convergence to
    and discovery of a predefined but poorly specified goal. Rather, they are often
    constructive processes, where stakeholders and developers build their wishes and
    plans, as they refine their own understanding of the environment, the systems,
    their needs, and their future interactions with the system.


    An important part of this refinement is producing more explicit definitions of
    elements that are outside the scope of the system. In contrast, such definitions
    are often totally absent from classical system specifications.


    # <span id="page-9-0"></span>5 RELATED WORK


    Recent advances in LLM-based chatbots have made a considerable impact on numerous
    domains. Researchers and engineers are now examining the potential applications
    of this technology in education [\(AlAfnan et al., 2023\)](#page-10-6), music
    [\(Lu et al., 2023\)](#page-11-7), academia and libraries [\(Lund and Wang, 2023\)](#page-11-16),
    healthcare [\(Li et al., 2023\)](#page-11-17), and many other areas.


    Within the field of software engineering, which is our subject matter here, attempts
    have been made to apply chatbots to evaluate the quality of code [\(Burak](#page-10-2)
    [et al., 2023\)](#page-10-2), to correct bugs [\(Surameery and Shakor,](#page-11-3)
    [2023\)](#page-11-3), and to generate code automatically or semiautomatically
    [\(Feng et al., 2023;](#page-10-17) [Dong et al., 2023\)](#page-10-18). The general
    consensus appears to be that chatbots will play a key role in code generation
    in years to come. Our work here outlines a possible path towards allowing this
    integration in a safe and controlled manner.


    Our proposed methodology for integrating Chat-GPT into the software engineering
    process leverages the large body of existing work on scenariobased modeling [\(Harel
    et al., 2012b;](#page-11-5) [Damm and](#page-10-4) [Harel, 2001\)](#page-10-4).
    Specifically, we propose to make use of the amenability of SBM to formal analysis
    techniques [\(Harel et al., 2015a;](#page-10-19) [Harel et al., 2015b\)](#page-11-18),
    such as verification [\(Harel et al., 2011;](#page-11-14) [Harel et al., 2013b\)](#page-10-15),
    automatic repair [\(Harel et al., 2012a\)](#page-11-19), and synthesis [\(Greenyer
    et al., 2016\)](#page-10-20). Despite our focus on SBM, other modeling approaches,
    with similar traits, could be used in a similar manner.


    Finally, our work here can be regarded as another step towards the *Wise Computing*
    vision [\(Harel et al.,](#page-10-12) [2016a;](#page-10-12) [Harel et al., 2016b;](#page-10-21)
    [Harel et al., 2018\)](#page-10-3), which seeks to transform the computer into
    an active member of the software engineering team — raising questions, making
    suggestions and observations, and carrying out verification-like processes, even
    without explicitly being asked to do so.


    # <span id="page-9-1"></span>6 CONCLUSION


    The appearance of large language models, and the subsequent release of advanced
    chatbots, is a major development, and it is likely to revolutionize the domain
    of software engineering in coming years. However, because of inaccuracies and
    errors that are inherent to the outputs of these chatbots, such an integration
    must be performed with care. We outline here a possible method for such an integration,
    which makes use of the advanced features of chatbots, but which also puts an emphasis
    on inspecting and analyzing the results of the integration. We are hopeful that
    our work will trigger additional research in this important direction.


    Moving forward, we plan to continue this work along several axes. First and foremost,
    we intend to implement the necessary tools and environments needed to fully integrate
    ChatGPT with SBM, and then use these tools and environments in large, realworld
    case studies that will demonstrate the usefulness of the approach as a whole.


    In addition, we expect that this line of work will require us to enhance and modify
    existing tools, both on the SBM said and on the chatbot one. For instance, with
    the current version of ChatGPT, every conversation starts from a blank slate,
    whereas for the ongoing development of a system, as part of the Wise Computing
    vision, it would be more useful to have the chatbot remember and use previous
    conversations. This could be achieved, for instance, by summarizing each conversation
    as it ends, and then feeding these summaries back to the chatbot when a new conversation
    starts. In fact, with the newly announced GPTs feature introduced in ChatGPT one
    can build a chatbot that is customized specifically for developing SB models and
    programs.


    Ideally, LLMs will be able learn immediately from ongoing conversations, yet they
    will do so selectively, learning over time, to select what should be retained
    in each conversation and for how long.


    These developments can also be beneficial in a broader perspective: prompt engineering
    methods and practices that would be developed along the way for such interactive,
    incremental development may prove useful not only in teaching computers, but in
    enhancing the training and everyday communications of human engineers.


    ## ACKNOWLEDGEMENTS


    The work of Harel, Marron and Szekely was funded in part by an NSFC-ISF grant
    to DH, issued jointly by the National Natural Science Foundation of China (NSFC)
    and the Israel Science Foundation (ISF grant 3698/21). Additional support was
    provided by a research grant to DH from the Estate of Harry Levine, the Estate
    of Avraham Rothstein, Brenda Gruss, and Daniel Hirsch, the One8 Foundation, Rina
    Mayer, Maurice Levy, and the Estate of Bernice Bernath.


    The work of Katz was partially funded by the European Union (ERC, VeriDeL, 101112713).
    Views and opinions expressed are however those of the author(s) only and do not
    necessarily reflect those of the European Union or the European Research Council
    Executive Agency. Neither the European Union nor the granting authority can be
    held responsible for them.


    ## REFERENCES


    - <span id="page-10-14"></span>Aceto, L., Ingolfsd ´ ottir, A., Larsen, K., and
    Srba, J. (2007). ´ *Reactive Systems: Modelling, Specification and Verification*.
    Cambridge University Press.

    - <span id="page-10-6"></span>AlAfnan, M., Dishari, S., Jovic, M., and Lomidze,
    K. (2023). ChatGPT as an Educational Tool: Opportunities, Challenges, and Recommendations
    for Communication, Business Writing, and Composition Courses. *Journal of Artificial
    Intelligence and Technology*, 3(2):60–68.

    - <span id="page-10-10"></span>Bar-Sinai, M., Weiss, G., and Shmuel, R. (2018).
    BPjs: An Extensible, Open Infrastructure for Behavioral Programming Research.
    In *Proc. 21st ACM/IEEE Int. Conf. on Model Driven Engineering Languages and Systems
    (MODELS)*, pages 59–60.

    - <span id="page-10-0"></span>Biolchini, J., Mian, P., Natali, A., and Travassos,
    G. (2005). Systematic Review in Software Engineering. Technical Report. System
    Engineering and Computer Science Department COPPE/UFRJ, Report ES 679.

    - <span id="page-10-2"></span>Burak, Y., Ozsoy, I., Ayerdem, M., and Tuz¨ un,
    E. ¨ (2023). Evaluating the Code Quality of AI-Assisted Code Generation Tools:
    An Empirical Study on GitHub Copilot, Amazon CodeWhisperer, and Chat-GPT. Technical
    Report. [https://arxiv.org/abs/2304.](https://arxiv.org/abs/2304.10778/) [10778/.](https://arxiv.org/abs/2304.10778/)

    - <span id="page-10-5"></span>Chang, Y., Wang, X., Wang, J., Wu, Y., Zhu, K.,
    Chen, H., Yang, L., Yi, X., Wang, C., Wang, Y., and Ye, W. (2023). A Survey on
    Evaluation of Large Language Models. Technical Report. [https://arxiv.org/abs/2307.](https://arxiv.org/abs/2307.03109/)
    [03109/.](https://arxiv.org/abs/2307.03109/)

    - <span id="page-10-4"></span>Damm, W. and Harel, D. (2001). LSCs: Breathing Life
    into Message Sequence Charts. *J. on Formal Methods in System Design (FMSD)*,
    19(1):45–80.

    - <span id="page-10-18"></span>Dong, Y., Jiang, X., Jin, Z., and Li, G. (2023).
    Self-Collaboration Code Generation via ChatGPT. Technical Report. [https://arxiv.org/abs/2304.07590/.](https://arxiv.org/abs/2304.07590/)

    - <span id="page-10-17"></span>Feng, Y., Vanam, S., Cherukupally, M., Zheng, W.,
    Qiu, M., and Chen, H. (2023). Investigating Code Generation Performance of Chat-GPT
    with Crowdsourcing


    Social Data. In *Proc. 47th IEEE Computer Software and Applications Conf. (COMPSAC)*,
    pages 1–10.


    - <span id="page-10-1"></span>Google (2023). Bard. [https://bard.google.com/.](https://bard.google.com/)

    - <span id="page-10-11"></span>Greenyer, J., Gritzner, D., Gutjahr, T., Konig,
    F., Glade, ¨ N., Marron, A., and Katz, G. (2017). ScenarioTools — A Tool Suite
    for the Scenario-based Modeling and Analysis of Reactive Systems. *Journal of
    Science of Computer Programming (J. SCP)*, 149:15–27.

    - <span id="page-10-20"></span>Greenyer, J., Gritzner, D., Katz, G., and Marron,
    A. (2016). Scenario-Based Modeling and Synthesis for Reactive Systems with Dynamic
    System Structure in ScenarioTools. In *Proc. 19th ACM/IEEE Int. Conf. on Model
    Driven Engineering Languages and Systems (MOD-ELS)*, pages 16–23.

    - <span id="page-10-7"></span>Gregorcic, B. and Pendrill, A.-M. (2023). ChatGPT
    and the Frustrated Socrates. *Physics Education*, 58(2).

    - <span id="page-10-13"></span>Gritzner, D. and Greenyer, J. (2018). Synthesizing
    Executable PLC Code for Robots from Scenario-Based GR(1) Specifications. In *Proc.
    4th Workshop of Model-Driven Robot Software Engineering (MORSE)*, pages 247–262.

    - <span id="page-10-16"></span>Harel, D., Assmann, U., Fournier, F., Limonad,
    L., Marron, A., and Szekely, S. (2023). Toward Methodical Discovery and Handling
    of Hidden Assumptions in Complex Systems and Models. In *Engineering Safe and
    Trustworthy Cyber Physical Systems – Essays Dedicated to Werner Damm on the Occasion
    of His 71st Birthday. To Appear*. arXiV preprint. [https://arxiv.org/abs/2312.16507.](https://arxiv.org/abs/2312.16507)

    - <span id="page-10-8"></span>Harel, D., Kantor, A., and Katz, G. (2013a). Relaxing
    Synchronization Constraints in Behavioral Programs. In *Proc. 19th Int. Conf.
    on Logic for Programming, Artificial Intelligence and Reasoning (LPAR)*, pages
    355– 372.

    - <span id="page-10-15"></span>Harel, D., Kantor, A., Katz, G., Marron, A., Mizrahi,
    L., and Weiss, G. (2013b). On Composing and Proving the Correctness of Reactive
    Behavior. In *Proc. 13th Int. Conf. on Embedded Software (EMSOFT)*, pages 1–10.

    - <span id="page-10-9"></span>Harel, D. and Katz, G. (2014). Scaling-Up Behavioral
    Programming: Steps from Basic Principles to Application Architectures. In *Proc.
    4th SPLASH Workshop on Programming based on Actors, Agents and Decentralized Control
    (AGERE!)*, pages 95–108.

    - <span id="page-10-19"></span>Harel, D., Katz, G., Lampert, R., Marron, A., and
    Weiss, G. (2015a). On the Succinctness of Idioms for Concurrent Programming. In
    *Proc. 26th Int. Conf. on Concurrency Theory (CONCUR)*, pages 85–99.

    - <span id="page-10-12"></span>Harel, D., Katz, G., Marelly, R., and Marron, A.
    (2016a). An Initial Wise Development Environment for Behavioral Models. In *Proc.
    4th Int. Conf. on Model-Driven Engineering and Software Development (MODEL-SWARD)*,
    pages 600–612.

    - <span id="page-10-21"></span>Harel, D., Katz, G., Marelly, R., and Marron, A.
    (2016b). First Steps Towards a Wise Development Environment for Behavioral Models.
    *Int. Journal of Information System Modeling and Design (IJISMD)*, 7(3):1– 22.

    - <span id="page-10-3"></span>Harel, D., Katz, G., Marelly, R., and Marron, A.
    (2018). Wise Computing: Toward Endowing System Devel-


    opment with Proactive Wisdom. *IEEE Computer*, 51(2):14–26.


    - <span id="page-11-19"></span>Harel, D., Katz, G., Marron, A., and Weiss, G.
    (2012a). Non-Intrusive Repair of Reactive Programs. In *Proc. 17th IEEE Int. Conf.
    on Engineering of Complex Computer Systems (ICECCS)*, pages 3–12.

    - <span id="page-11-9"></span>Harel, D., Katz, G., Marron, A., and Weiss, G. (2014).
    Non-Intrusive Repair of Safety and Liveness Violations in Reactive Programs. *Transactions
    on Computational Collective Intelligence (TCCI)*, 16:1–33.

    - <span id="page-11-18"></span>Harel, D., Katz, G., Marron, A., and Weiss, G.
    (2015b). The Effect of Concurrent Programming Idioms on Verification. In *Proc.
    3rd Int. Conf. on Model-Driven Engineering and Software Development (MODEL-SWARD)*,
    pages 363–369.

    - <span id="page-11-14"></span>Harel, D., Lampert, R., Marron, A., and Weiss,
    G. (2011). Model-Checking Behavioral Programs. In *Proc. 9th ACM Int. Conf. on
    Embedded Software (EMSOFT)*, pages 279–288.

    - <span id="page-11-8"></span>Harel, D. and Marelly, R. (2003). Specifying and
    Executing Behavioral Requirements: The Play In/Play-Out Approach. *Software and
    System Modeling (SoSyM)*, 2:82–107.

    - <span id="page-11-10"></span>Harel, D., Marron, A., and Weiss, G. (2010). Programming
    Coordinated Scenarios in Java. In *Proc. 24th European Conf. on Object-Oriented
    Programming (ECOOP)*, pages 250–274.

    - <span id="page-11-5"></span>Harel, D., Marron, A., and Weiss, G. (2012b). Behavioral
    Programming. *Communications of the ACM (CACM)*, 55(7):90–100.

    - <span id="page-11-15"></span>Harel, D., Marron, A., and Yerushalmi, R. (2021).
    Scenario-Based Algorithmics: Coding Algorithms by Automatic Composition of Separate
    Concerns. *Computer*, 54(10):95–101.

    - <span id="page-11-13"></span>Harel, D. and Pnueli, A. (1985). On the Development
    of Reactive Systems. *Logics and Models of Concurrent Systems*, F-13:474–498.

    - <span id="page-11-12"></span>Katz, G. (2013). On Module-Based Abstraction and
    Repair of Behavioral Programs. In *Proc. 19th Int. Conf. on Logic for Programming,
    Artificial Intelligence and Reasoning (LPAR)*, pages 518–535.

    - <span id="page-11-17"></span>Li, J., Dada, A., Kleesiek, J., and Egger, J. (2023).
    Chat-GPT in Healthcare: A Taxonomy and Systematic Review. Technical Report. [https://www.medrxiv.org/](https://www.medrxiv.org/content/10.1101/2023.03.30.23287899v1)
    [content/10.1101/2023.03.30.23287899v1.](https://www.medrxiv.org/content/10.1101/2023.03.30.23287899v1)

    - <span id="page-11-4"></span>Liu, J., Xia, C., Wang, Y., and Zhang, L. (2023).
    Is your Code Generated by ChatGPT really Correct? Rigorous Evaluation of Large
    Language Models for Code Generation. Technical Report. [https://arxiv.org/abs/](https://arxiv.org/abs/2305.01210/)
    [2305.01210/.](https://arxiv.org/abs/2305.01210/)

    - <span id="page-11-7"></span>Lu, P., Xu, X., Kang, C., Yu, B., Xing, C., Tan,
    X., and Bian, J. (2023). MuseCoco: Generating Symbolic Music from Text. Technical
    Report. [https://arxiv.org/](https://arxiv.org/abs/2306.00110/) [abs/2306.00110/.](https://arxiv.org/abs/2306.00110/)

    - <span id="page-11-16"></span>Lund, B. and Wang, T. (2023). Chatting about Chat-GPT:
    how may AI and GPT Impact Academia and Libraries? *Library Hi Tech News*, 40(3):26–29.

    - <span id="page-11-2"></span>MetaAI (2023). LLaMa. [https://ai.meta.com/llama/.](https://ai.meta.com/llama/)

    - <span id="page-11-1"></span>OpenAI (2022). ChatGPT. [https://chat.openai.com/.](https://chat.openai.com/)

    - <span id="page-11-0"></span>Pettersson, O. and Andersson, J. (2016). A Survey
    of Modeling Approaches for Software Ecosystems. In *Proc. 7th Int. Conf. on Software
    Business (ICSOB)*, pages 79–93.

    - <span id="page-11-3"></span>Surameery, N. and Shakor, M. (2023). Use Chat GPT
    to Solve Programming Bugs. *Int. Journal of Information Technology & Computer
    Engineering (IJITC)*, 3(1):17–22.

    - <span id="page-11-6"></span>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit,
    J., Jones, L., Gomez, A., Kaiser, L., and Polosukhin, I. (2017). Attention is
    all you Need. In *Proc. 31st Conf. on Advances in Neural Information Processing
    Systems (NeurIPS)*.

    - <span id="page-11-11"></span>Yaacov, T. (2023). BPPy: Behavioral Programming
    in Python. *SoftwareX*, 24.'
  decisions:
    language: '- Qualified. Reason: English Paper'
    evaluation_prompt: Disqualified
    related_work_prompt: Qualified
    novelty_prompt: Qualified
    review_only_prompt: Qualified
  llm_input_used: '## Abstract

    The manual modeling of complex systems is a daunting task; and although a

    plethora of methods exist that mitigate this issue, the problem remains very

    difficult. Recent advances in generative AI have allowed the creation of

    general-purpose chatbots, capable of assisting software engineers in various

    modeling tasks. However, these chatbots are often inaccurate, and an

    unstructured use thereof could result in erroneous system models. In this

    paper, we outline a method for the safer and more structured use of chatbots as

    part of the modeling process. To streamline this integration, we propose

    leveraging scenario-based modeling techniques, which are known to facilitate

    the automated analysis of models. We argue that through iterative invocations

    of the chatbot and the manual and automatic inspection of the resulting models,

    a more accurate system model can eventually be obtained. We describe favorable

    preliminary results, which highlight the potential of this approach.


    ## Introduction

    Manually modeling complex systems is a daunting and error-prone endeavor. Furthermore,
    even after the system is modeled, ongoing tasks, such as modification and repair,
    continue to tax human engineers. Creating tools and methodologies for streamlining
    and facilitating this process has been the topic of extensive work, but many aspects
    of the problem remain unsolved [\(Pettersson and Andersson, 2016;](#page-11-0)
    [Biolchini](#page-10-0) [et al., 2005\)](#page-10-0).


    In recent years, the deep learning revolution has been causing dramatic changes
    in many areas, including computer science; and this revolution has recently taken
    yet another step towards general-purpose AI, with the release of ChatGPT, the
    learning-based chatbot [\(OpenAI, 2022\)](#page-11-1). ChatGPT, and other, similar
    tools [\(Google, 2023;](#page-10-1) [MetaAI, 2023\)](#page-11-2), can be used
    for countless kinds of tasks — including the modeling and coding of complex systems
    [\(Surameery and](#page-11-3) [Shakor, 2023\)](#page-11-3). An engineer might
    provide ChatGPT with a natural-language description of the system at hand, and
    receive in return a model of the system, or even computer code that implements
    it; and through iterative querying of ChatGPT, the system can later be modified
    or enhanced. This approach has already been used in several application domains
    [\(Surameery](#page-11-3) [and Shakor, 2023;](#page-11-3) [Burak et al., 2023;](#page-10-2)
    [Liu et al., 2023\)](#page-11-4).


    Although the ability to integrate ChatGPT[1](#page-0-0) into the software development
    cycle will undoubtedly empower engineers, there are also potential pitfalls that
    need to be taken into account. One drawback of Chat-GPT and similar tools is that
    the answers they provide are often inaccurate, and might overlook important aspects
    of the input query [\(Liu et al., 2023\)](#page-11-4). Moreover, the input query
    itself might be imperfect, and the engineer might not realize this until the system
    is deployed. Thus, if we make the reasonable assumption that human engineers will
    gradually become dependent on chatbots for various tasks, the risk increases that
    these inaccuracies will find their way into the final models of the system at
    hand and the code that ensues. We are thus faced with the following challenge:
    how can we harness ChatGPT in a way that lifts a significant load of work off
    the shoulders of the engineers, but which still results in sound and accurate
    models?


    Here, we advocate the creation of an encompassing modeling scheme that will combine
    ChatGPT with more traditional techniques for manual modeling of systems [\(Biolchini
    et al., 2005;](#page-10-0) [Pettersson and](#page-11-0) [Andersson, 2016\)](#page-11-0),
    in a way that will achieve this goal. Our core idea is to use ChatGPT in a controlled
    way; i.e., to repeatedly invoke it for various tasks, but to


    <span id="page-0-0"></span><sup>1</sup>We will often use the term *ChatGPT* somewhat
    generically, to represent an arbitrary, modern chatbot.


    then thoroughly inspect and analyze its results, to ensure their soundness and
    accuracy. We argue that such a scheme, if designed properly, would allow software
    and system engineers to benefit from the capabilities of modern chatbots, but
    without jeopardizing the quality of the resulting products. In the long run, we
    regard such a scheme as a step towards the *Wise Computing* vision [\(Harel et
    al., 2018\)](#page-10-3), which calls for turning the computer into a proactive
    member of the software development team — one which can propose courses of action,
    detect under-specified portions of the model, and assist in the various routine
    actions that naturally arise as part of the software development cycle.


    In order to design such a modeling scheme, we propose to leverage the extensive
    work carried out in the modeling community over the years. Specifically, we propose
    to focus on modeling frameworks that afford two benefits that complement the capabilities
    of ChatGPT: (i) the models produced by the framework are naturally well-aligned
    with how humans perceive systems; this, we believe, will make it easier for the
    human engineer to inspect ChatGPT''s output; and (ii) the resulting models are
    amenable to automated analysis tasks, such as model checking, which will support
    the automated detection of bugs and inconsistencies in the automatically generated
    models.


    Several modeling approaches fit this description, and many of them can probably
    be used, but for the initial evaluation presented here, we focus on *scenario-based
    modeling* (*SBM*) — a technique that generates models comprised of simple *scenarios*,
    each of which describes a single aspect of the system at hand [\(Harel et al.,
    2012b;](#page-11-5) [Damm and Harel,](#page-10-4) [2001\)](#page-10-4). As we
    later discuss, this can facilitate the smooth collaboration between ChatGPT and
    the human engineers.


    To demonstrate the potential of this combined framework, we focus on a few tasks
    that arise naturally as part of a system''s life cycle. Specifically, we discuss
    the initial design of the model, its testing and the verification of its properties,
    its later enhancement or repair due to the discovery of inconsistencies, and also
    a search for under-specified portions of the model. Our results, although preliminary,
    are very promising, and we hope this paper will form a basis for further research
    in this direction.


    In the remainder of the paper, we present the key concepts of our approach, and
    discuss a high-level plan for the next steps. We begin by introducing the concepts
    of SBM and language model-based chatbots in Section [2.](#page-1-0) Next, we present
    the proposed integration of SBM and ChatGPT in Section [3,](#page-3-0) followed
    by a discussion of some of the more advanced aspects of this integration in Section
    [4.](#page-5-0) We discuss related work in Section [5](#page-9-0) and conclude
    in Section [6.](#page-9-1)'
  token_usage: 7106
  time_usage: 1.788506031036377
- title: Exception-aware Lifecycle Model Construction for Framework APIs
  abstract: 'The implementation of complex software systems usually depends on low-level

    frameworks or third-party libraries. During their evolution, the APIs adding

    and removing behaviors may cause unexpected compatibility problems. So,

    precisely analyzing and constructing the framework/ library''s API lifecycle

    model is of great importance. Existing works have proposed the API

    existence-changing model for defect detection, while not considering the

    influence of semantic changes in APIs. In some cases, developers will not

    remove or deprecate APIs but modify their semantics by adding, removing, or

    modifying their exception-thrown code, which may bring potential defects to

    upper-level code. Therefore, besides the API existence model, it is also

    necessary for developers to be concerned with the exception-related code

    evolution in APIs, which requires the construction of exception-aware API

    lifecycle models for framework/library projects. To achieve automatic

    exception-aware API lifecycle model construction, this paper adopts a static

    analysis technique to extract exception summary information in the framework

    API code and adopts a multi-step matching strategy to obtain the changing

    process of exceptions. Then, it generates exception-aware API lifecycle models

    for the given framework/library project. With this approach, the API lifecycle

    extraction tool, JavaExP, is implemented, which is based on Java bytecode

    analysis. Compared to the state-of-the-art tool, JavaExP achieves both a higher

    F1 score (+60%) and efficiency (+7x), whose precision of exception matching and

    changing results is 98%. Compared to the exception-unaware API lifecycle

    modeling on 60 versions, JavaExp can identify 18% times more API changes. Among

    the 75,433 APIs under analysis, 20% of APIs have changed their

    exception-throwing behavior at least once after API introduction, which may

    bring many hidden compatibility issues.'
  url: http://arxiv.org/abs/2401.02660v1
  keywords: '* static analysis; program evolution; Java exception summary; API lifecycle'
  document: "# 异常信息敏感的框架 API 生命周期模型构造\\*\n\n燕季薇 1) ,2) 黄进豪 3) 杨恒钦 2),4) 严俊 1),2) \\\
    *\n\n1) (中国科学院软件研究所 软件工程技术研究开发中心 北京 100190)\n\n> 2) (中国科学院大学 北京 100049)\n\n(北京工业大学\
    \ 北京 100124)\n\n3)\n\n4) (国科大杭州高等研究院 杭州 310024)\n\n摘 要 大型软件系统的实现通常依赖于底层框架或第三方库。这些框架/库代码数量繁多、实现复杂,但它们的演化升级\
    \ 往往独立于其调用者,为上层软件的质量保障带来挑战。例如,框架/库代码升级时 API 的新增和删除行为可能引发上层软 件的兼容性问题。准确分析并提取框架/库代码\
    \ API 生命周期模型可以有效辅助对这类代码演化情况的理解,支撑上层的分 析与测试。现有工作中的 API 生命周期模型主要关注 API 的存在性变动,而未考虑特定代码语义变更对开发者的影响。在\
    \ 某些情况下,开发者没有删除或废弃特定的 API,而是更改其代码语义,例如增加、删除或修改用于外部数据校验的异常抛 出相关代码。如果新版本框架/库中的异常抛出行为发生变更而调用者不知情,可能给上层软件系统带来隐患。因此,除了\
    \ API 的存在性,开发者还应特别关注异常相关代码的变更情况,即为框架/库代码构建异常信息敏感的 API 生命周期模型。\n\n为实现异常信息敏感的 API\
    \ 生命周期模型构造,本文采用面向 Java 字节码的静态分析方法,首先提取框架 API 中的异 常抛出行为,生成异常摘要信息,然后通过多轮流式匹配策略获取异常信息的变更情况,构造异常信息敏感的\
    \ API 生命周期 模型。该方法:1)通过控制依赖语句切片提取异常抛出语句的关键触发条件,采用参数推断策略将局部变量的约束条件转 换为仅与外部输入参数相关的异常前断言,并基于自底向上的摘要传递实现跨过程异常摘要提取;2)通过关键信息精准匹\
    \ 配和自适应模糊匹配策略,分析异常摘要信息的新增、删除和修改情况,最终得到异常敏感的 API 生命周期模型,共包含七 种 API 变更形式。基于该方法,实现了基于\
    \ Java 字节码分析的 API 生命周期提取工具 JavaExP。与最新的 Java 异常分析工 具相比,在异常摘要信息提取方面,JavaExP 在大幅提高分析效率(+7x)的同时实现了更高的\
    \ F1 分数(+60%)。通过人工确 认 Apache common-io 项目在 19 个版本上的演化报告,发现异常级别的演化分析准确率达到 98%。对六个真实框架/库项目\
    \ 在 60 个版本上的 API 生命周期分析表明,与异常不敏感的 API 生命周期相比,采用异常敏感的分析时,API 发生变动的比 例提高了 18%。在\
    \ 75,433 个被分析的 API 中,约有 20% API 的异常抛出行为至少发生过一次改变,这些 API 共涉及超过七 千多处独立的异常变更。在多个项目上的分析结果表明,异常敏感的模型构造能够更加精准地描述\
    \ API 的演化过程。\n\n关键词 静态分析;代码演化;Java 异常摘要;API 生命周期 中图法分类号 TP311 DOI 号 xxx\n\n# **Exception-aware\
    \ Lifecycle Model Construction for Framework APIs**\n\nYAN Ji-Wei1),2) HUANG Jin-Hao3)\
    \ Yang Heng-Qin2),4) YAN Jun1),2)\\*\n\n1)(Technology Center of Software Engineering,\
    \ Institute of Software, Chinese Academy of Sciences Beijing, 100190)\n\n2)(University\
    \ of Chinese Academy of Sciences, Beijing, 100049)\n\n3)(Beijing University of\
    \ Technology, Beijing, 100124)\n\n4)(Hangzhou Institute for Advanced Study, Hangzhou,\
    \ 310024)\n\n#### **Abstract**\n\nThe implementation of complex software systems\
    \ usually depends on low-level frameworks or third-party libraries. However, the\
    \ evolution of these frameworks or libraries is independent of the upper-level\
    \ applications, which brings challenges in upper-level code quality assurance.\
    \ For example, during code evolution, the APIs adding and removing behaviors may\
    \ cause unexpected compatibility problems. Precisely analyzing and constructing\
    \ the framework/ library's API lifecycle model is of great importance, which could\
    \ help in understanding changes in APIs as well as supporting the analysis and\
    \ testing of upper-level code. Nowadays, existing works propose the API existencechanging\
    \ model for defect detection, while not considering the influence of semantic\
    \ changes in APIs. In some cases, developers will not remove or deprecate APIs\
    \ but modify their semantics by adding, removing, or modifying their exception-thrown\
    \ code, which is used to verify the users' inputs. It may bring potential defects\
    \ to upper-level code if the exception-related behaviors in newer versions are\
    \ changed silently. Therefore, besides the API existence model, it is also necessary\
    \ for developers to be concerned with the exception-related code evolution in\
    \ APIs, which requires the construction of exception-aware API lifecycle models\
    \ for framework/library projects.\n\nTo achieve automatic exception-aware API\
    \ lifecycle model construction, this paper adopts static analysis technique to\
    \ extract exception summary information in the framework API code and adopts a\
    \ multi-step matching strategy to obtain the changing process of exceptions. Then,\
    \ it generates exception-aware API lifecycle models for the given framework/library\
    \ project. Our approach: 1) adopts control-dependency slicing analysis to extract\
    \ the conditions of the exception-thrown statements; uses a parameter tracing\
    \ strategy to transform exception-throwing conditions into external-variable-related\
    \ preconditions; and performs inter-procedure precondition construction by a bottom-up\
    \ summary-based analysis. 2) proposes the exact-matching and adaptive-matching\
    \ strategies to analyze the addition, deletion, and modification changes based\
    \ on the summarized exception summaries; generates exception-aware API lifecycle\
    \ model which covers seven API changing types. With this approach, the API lifecycle\
    \ extraction tool, JavaExP, is implemented, which is based on Java bytecode analysis.\
    \ Compared to the state-of-the-art tool, JavaExP achieves both higher F1-score\
    \ (+60%) and efficiency (+7x). By manually confirming the exception changing reports\
    \ on 19 versions of Apache common-io project, we found that the precision of exception\
    \ matching and changing results is 98%. The evaluation of 60 versions of six projects\
    \ shows that, compared to the exception-unaware API lifecycle modeling, JavaExp\
    \ can identify 18% times more API changes. Among the 75,433 APIs under analysis,\
    \ 20% of APIs have changed their exception-throwing behavior at least once after\
    \ API introduction. These APIs involve a total of more than 7k independent exception\
    \ changes, which shows that the exception-aware lifecycle modeling can describe\
    \ the evolution process of APIs more accurately.\n\n**Key words** static analysis;\
    \ program evolution; Java exception summary; API lifecycle\n\n# 1. 引言\n\n具有复杂功能的大型软件系统往往由多个模\
    \ 块组成,其实现依赖于底层编程框架和种类繁多的 第三方库。这些框架/库代码通过持续的版本更新修 改代码缺陷或完善代码功能,其演化过程独立于调 用它们的上层软件系统。在上层应用的开发过程\
    \ 中,软件供应链安全分析中的依赖安全检测工具会 帮助开发者识别项目依赖中的漏洞,并提醒应用开 发者尽快更新版本以保障代码质量安全。例如,当 GitHub\
    \ 检测到项目代码中使用易受攻击的依赖项 或恶意软件时,会向开发者发送 Dependabot 警报 [\\[1\\]](#page-16-0)。如果上层应用开发者在不熟悉框架/库代码\
    \ API 演化过程的情况下变更版本,可能会引入其他问 题,如使用了过时/被移除的 API 或未及时捕获处理 新抛出的异常,进而导致程序错误或引发兼容性问\
    \ 题等。\n\n框架/库代码中 API 的变更导致用户在迁移上 层软件系统时花费较多精力,间接增加了使用特定 框架/库的开发难度[\\[2\\]\\[3\\\
    ]](#page-16-1)[\\[4\\]](#page-16-2)。为保障这些依赖于底 层框架/库函数的软件系统质量,一种解决方案是预 先分析不同框架/库代码版本下的\
    \ API 调用规约,并 检测调用代码的正确性。针对这一问题,现有工作 提出了 API 级别的生命周期模型[\\[18\\]](#page-16-3),并通过分析\
    \ 框架更新时提供的 API 变更文本文件[\\[5\\]](#page-16-4)或通过轻 量级框架代码分析扫描其 API 列表[\\[6\\]\\[7\\\
    ]](#page-16-5)等方法来 构建 API 级别的生命周期模型。虽然这些工作考虑 了 API 的存在性变更,但忽略了 API 中关键代码语 义变更的影响。在某些情况下,开发者没有删除或\
    \ 废弃特定的 API,而是更改其代码语义。对于框架 /库的使用人员,除了 API 的存在性变更外,语义信 息变更也是 API 调用时以及 API 调用合规性检测中\
    \ 需要考虑的一项关键信息。\n\n基于API完整代码的差异分析可以准确反映代 码的变更情况,但完整的代码变更结果数据量庞大 且复杂,对于上层应用的分析和测试,并非所有变\
    \ 更都会对用户的使用产生影响。我们发现,在 API 演化过程中,同一 API 的基本功能往往保持一致, 即代码升级不应影响现有 API 的基本功能实现,这\
    \ 类变化应是对用户透明的,但 API 对外部输入数据 的校验过程和校验结果的反馈方式是可变的,它们 会影响 API 的上层调用。在 Java 代码中,当\
    \ API 接 收到非预期的外部输入时,通常会抛出异常来应对 这一非预期行为,而上层用户应该及时捕获并处理\n\n这些异常行为[\\[41\\]](#page-17-0)。Mostafa\
    \ 等人对 Java 库代码兼 容性错误的统计[\\[55\\]](#page-17-1)表明,由异常导致的兼容性问 题占比超过 1/3(105/296)。葛等人在文献[\\\
    [58\\]](#page-17-2)中指 出,框架/库代码中存在的错误或漏洞可能会被攻击 者利用, 从而损害软件供应链安全,这些错误或漏 洞往往与框架/库代码中存在的异常有关。由此可\
    \ 见,API 异常抛出行为的变化会对用户调用方式产 生重要影响,在软件快速演化背景下,其变更行为 对于软件的健壮性与安全性息息相关。\n\n| 1. public\
    \ int getCount() {                                 |  |  |  |  |\n|------------------------------------------------------------|--|--|--|--|\n\
    | 2. -<br>return (int) getByteCount();                       |  |  |  |  |\n|\
    \ 3. +<br>long result = getByteCount();                      |  |  |  |  |\n|\
    \ 4. +<br>if (result > Integer.MAX_VALUE) {                  |  |  |  |  |\n|\
    \ 5. +<br>throw new ArithmeticException(\"The byte count      |  |  |  |  |\n\
    | \" + result + \" is too large to be converted to an int\"); } |  |  |  |  |\n\
    | 6. +<br>return (int) result;<br>//修改返回空值为抛出异常              |  |  |  |  |\n|\
    \ 7. }                                                       |  |  |  |  |\n|\
    \ (a)新增异常实例                                                  |  |  |  |  |\n\n\
    | 1. public static void moveFile(File srcFile, File destFile) |  |  |\n|-------------------------------------------------------------|--|--|\n\
    | throws IOException {                                        |  |  |\n| 2.<br>if\
    \ (destFile.exists())                                |  |  |\n| 3. -<br>throw\
    \ new IOException(\"Destination '\" + destFile +  |  |  |\n| \"' already exists\"\
    );                                        |  |  |\n| 8. +<br>throw new FileExistsException(\"\
    Destination '\" +     |  |  |\n| destFile + \"' already exists\"); //修改异常类型  \
    \                  |  |  |\n| 4. }                                           \
    \             |  |  |\n| (b)修改抛出异常类型                                         \
    \        |  |  |\n\n| 1. public void forceDelete(File file) throws IOException\
    \ { |  |  |  |\n|------------------------------------------------------------|--|--|--|\n\
    | 2. +<br>boolean filePresent = file.exists();               |  |  |  |\n| 3.\
    \ +<br>if (!file.delete()) { //增加文件删除判断条件                 |  |  |  |\n| 4. +<br>if\
    \ (!filePresent) {                                |  |  |  |\n| 5. -<br>if (!file.exists())\
    \ {                              |  |  |  |\n| throw new FileNotFoundException(\"\
    File does<br>6.           |  |  |  |\n| not exist: \" + file); }             \
    \                       |  |  |  |\n| 7. +<br>}                              \
    \                    |  |  |  |\n| 8. }                                      \
    \                 |  |  |  |\n| (c)修改异常抛出条件                                  \
    \              |  |  |  |\n\n#### 图 **1** 不同版本 **API** 中异常相关代码变更示例\n\n图 1 给出了真实项目中的异常相关变更代码片\
    \ 段示例,其变更形式多种多样,包括新增或删除异 常实例[\\[12\\]\\[13\\]](#page-16-6)、修改异常实例的类型、描述或抛出 条件[\\\
    [14\\]](#page-16-7) [\\[15\\]](#page-16-8) [\\[16\\]](#page-16-9)等。在代码演化过程中,同一异常\
    \ 可能发生多次不同类型的变更[\\[14\\]\\[17\\]](#page-16-7)(参见图 3)。 这些变更可能对 API 的外部使用产生影响,即改变\n\
    \nAPI 使用规约。因此,为了正确理解框架 API 的生 命周期行为,需在考虑 API 增删变化之外,结合 API 中异常的变更情况,为其构造异常信息敏感的生命\
    \ 周期模型。该模型的构建依赖于对异常摘要信息的 准确提取与匹配分析。其中的关键挑战是:1)应提 取哪些关键信息表征API中的异常实例并尽量减少 数据中影响匹配的噪音;2)当一个方法中存在多个\
    \ 同类型的异常实例时,如何准确地在多个版本中匹 配到同一实例并识别变更内容,从而准确构建 API 及其异常集合的生命周期。\n\n针对这些挑战,本文首先设计了一种面向演化\
    \ 分析的异常摘要形式,包括异常类型、描述文本、 前断言三类核心信息。为了减少匹配时的数据噪 音,本文在异常抛出条件分析中,通过控制依赖约 束分析去除了与异常抛出无关的条件约束;通过数\
    \ 据流分析将所有中间局部变量约束转换为外部输 入变量约束;通过跨过程异常传递分析避免函数级 代码重构导致的函数内异常变更。此外,为准确识 别不同版本代码中的异常实例,本文共设计了基于\
    \ 类型、描述、前断言、关键前断言四类信息的过滤 器,对于无法完全匹配的异常实例,采用多轮流式 匹配策略识别变更实例、分析变更过程,最终生成 涵盖七类变更行为的异常敏感\
    \ API 生命周期报告。\n\n基于该方法,本文实现了异常信息敏感的 Java API 分析工具 JavaExP (Java Exception-aware\
    \ API analyzer[\\)\\[46\\]](#page-17-3)。其框架图如图 2 所示,对于任意两个\n\n(或多个)版本的 Java 框架/库的\
    \ jar 包/class 文件, JavaExP 先通过异常摘要提取模块获取每个版本的 异常摘要报告。其次,将这些摘要被输入到生命周 期构造模块,对不同版本中\
    \ API 异常语义摘要执行 自适应匹配和异常变更分析。分析后,可以获取 API 的新增、删除情况和 API 中异常的新增、删除和修 改情况,从而得到目标版本区间上异常信息敏感的\
    \ API 生命周期模型。\n\n![](_page_3_Figure_5.jpeg)\n\n#### 图 **2 JavaExP** 方法框架图\n\n\
    多组对比实验验证了本文方法的有效性。对于 异常摘要提取模块,与最新的 Java 异常分析工具 WI[T \\[26\\]](#page-16-10)相比,JavaExP\
    \ 使用更短的时间 (-87%) 提 取到了大量 WIT 无法分析的异常信息,并显著提高 F1 分数(相对提升 60%)。应用 JavaExP 分析了六 个项目的\
    \ 60 个版本,并为其生成 API 生命周期报 告,演化分析的准确率达到 98%;找到了 API 中大 量的异常变更行为,在 75,433 个 API 中,在异常敏\
    \ 感的 API 生命周期模型中,约 20%的 API 在首次引 入后,异常信息发生过至少一次变动;与异常不敏 感的 API 生命周期相比,异常敏感的 API\
    \ 发生变动 的比例提高了 18%。本文的工具和实验数据均已开 源到 GitHub [\\[46\\]](#page-17-3)。\n\n本文的章节结构设计如下:第\
    \ 1 章概述异常信 息敏感的 API 生命周期模型构造方法;第 2 章介 绍本文所需的基础知识和概念定义;第 3 章介绍面 向 Java 程序的异常摘要提取方法;第\
    \ 4 章介绍基于 异常摘要分析 API 生命周期构造方法;第 5 章给出 实验设计和结果分析,评估所提方法的有效性;第 6 章介绍本文的相关工作;最后一章为总结与展望。\n\
    \n# 2. 基础知识与示例应用\n\n### **2.1. Java**异常\n\n异常是在程序执行过程中出现的问题或错误 的一种表示。在 Java 语言中,异常被定义为派生\
    \ 自 java.lang.Throwable 类的对象,在 Java 类库、用 户方法及运行时故障中都可能会抛出异常。Java 提 供了很多内置的异常类\
    \ [\\[42\\]](#page-17-4),如 IOException、 IllegalArgumentException 等,此外,开发人员还可 以自定义异常类以便更好地适应特定需求。一部分\
    \ 异常会被 Java 虚拟机自动的抛出,在运行时不需要 显式处理,但它们可能会导致程序的异常终止,这 类异常也被称为非受检异常或运行时异常;还有一 类异常需在编译时显式地处理,否则会导致编译错\
    \ 误,它们又叫受检异常或编译时异常[\\[43\\]](#page-17-5)。在方法内 部检测到不符合预期条件或无法处理的情况时,开 发者可以通过 throw\
    \ 语句声明主动抛出异常提供有 关特定问题或错误的信息,并将控制权交给调用者 或上层代码来处理(try-catch 行为)。对于框架/库 API 的调用者,API\
    \ 中抛出的异常类型、抛出条件 等与API调用过程的中数据输入规约和异常捕获方 式息息相关。因此,这类变更应被及时传递给开发 人员。\n\n### **2.2.\
    \ API** 演化和生命周期\n\n 应用程序接口(API)是框架/库代码对外提供服 务的调用接口。随着代码功能的演化升级,旧版本 API 在新版中可能被删除,新版代码中也会增加\
    \ API 以提供更丰富的功能。除了 API 的增加与删除, API 中代码的实现方式、数据校验方式等均可能发 生变化。API 的生命周期指 API 在不同的框架/库版\
    \ 本中的存在范围,如 Li 等人提取了安卓框架代码中 API 的生命周期模型[\\[5\\]](#page-16-4),虽然 API 的变动较为频 繁,但在\
    \ API 持续存在的生命周期中,其中包含的 异常实例可能是持续演化的,这类变化在异常不敏 感的 API 级别生命周期模型中无法体现。\n\n### **2.3.**\
    \ 概念定义\n\n针对 API、异常及生命周期等概念,我们分别 给出如下定义。\n\n定义 **1**(应用程序接口方法): API = (id, version,\
    \ class, method, S\\_Exp) 为一个应用程序接口方法,它 包含 API 的签名、版本号、所在的类名称、方法名 称以及包含的异常集合 S\\\
    _Exp。\n\n定义 **2**(异常摘要):Summay(exp) = (API.id, type, message, condition, precondition),\
    \ exp∈API.S\\_Exp 为一个异常摘要,它包含异常所在的 API、异常的 类型、异常抛出时的描述文本信息、异常抛出语句 的控制依赖条件以及与异常抛出相关的外部参数\
    \ 前断言。\n\n定义 **3**(**API/**异常存在性生命周期)API 的存在性 生命周期为该 API 从引入到删除的版本区间的并 集。对于其中包含的异常实例\
    \ Exp∈S\\_Exp,异常的 存在性生命周期为该异常在该API中从引入到删除 的版本区间的并集。\n\n定义 **4**(异常敏感的 **API**\
    \ 生命周期)API 中所有异 常对象 exp∈API.S\\_Exp 摘要信息 Summay(exp)的 集合为 S\\_Summay(S\\_Exp)\
    \ ,在不同的版本中,当 且仅当两个异常摘要信息集合中任何异常摘要信 息均相同时,可认为异常摘要信息的取值在不同版 本上保持不变。对于异常摘要信息集合的特定取\
    \ 值,如果其出现的最早版本为 Vi,最末版本为 V<sup>j</sup> (i<=j),则其生命周期为[Vi, Vj]。对于异常敏感的 API 生命周期,它给出了摘要信息集合\
    \ S\\_Summay 在目标版本中所有不同取值到其生命周期的映射 关系。导致 API 中异常摘要信息取值变更的操作被 称为异常敏感的 API 操作,包括:API\
    \ 新增、API 删除、API 修改-异常新增、API 修改-异常删除、API 修改-异常类型变更、API 修改-异常描述变更和 API 修改-异常断言变更七类。\n\
    \n### **2.4.** 示例代码\n\n图 3 为 Apache commons-io [\\[14\\]\\[17\\]](#page-16-7)项目中\
    \ API moveFile()的部分代码,给出了其异常 e 在不同版本 中的变更情况,+表示新增代码,-表示删除代码。 该异常为针对 API 第 2 个参数变量\
    \ destFile 的文件 存在性校验。在版本号 1.4 的代码中,类型为 IOException 的异常实例 e 被引入,而在 2.0 版本 中,异常实例\
    \ e 的类型被更改为 FileExistsException 类。随后,在版本 2.7 中,该方法被重构,但实际 异常 e 的抛出条件未发生变化。在版本\
    \ 2.9 中,该 方法被再次重构,异常 e 的抛出位置发生变化,其 描述文本和异常前断言也发生改变。因此,对于图 5 中的异常 e,其演化过程为一次新增,一次异常类\
    \ 型更改,一次异常描述文本和前断言更改,其中经 历了两次方法重构。在这种情况下,上层开发者很 难快速评估不同版本中API包含的异常是否变动以 及变动的过程。为提取目标\
    \ API 的生命周期,首先 需准确获取每一个异常实例的关键信息,判定不同 版本中的多个异常信息是否指向同一个异常实例, 如是同一实例,则记录其变更过程,在此基础上,\
    \ 生成完整的 API 生命周期报告。\n\n```\n1. public static void moveFile(File srcFile, File\
    \ destFile) ...{\n2. if (srcFile == null) {\n3. throw new NullPointerException(\"\
    Source must not \nbe null\");\n4. if (destFile == null) {\n5. throw new NullPointerException(\"\
    Destination \nmust not be null\");\n6. if (!srcFile.exists()) {\n7. throw new\
    \ FileNotFoundException(\"Source '\" + \nsrcFile + \"' does not exist\");\n8.\
    \ if (srcFile.isDirectory()) {\n9. throw new IOException(\"Source '\" + srcFile\
    \ + \"' is \na directory\");\n10. if (destFile.exists()) \n11. - throw new IOException(\"\
    Destination '\" + destFile \n+ \"' already exists\"); //在 1.4 版本引入\n12. + throw\
    \ new FileExistsException(\"Destination '\" + \ndestFile + \"' already exists\"\
    ); //在 2.0 版本变更类型\n13. }\n                (a) 版本变更 V1.4 V2.0\n```\n\n| 1. public\
    \ static void moveFile(File srcFile, File destFile) {   |\n|-----------------------------------------------------------------|\n\
    | 2. +<br>validateMoveParameters(srcFile, destFile);// //throw    |\n| other three\
    \ exceptions //在 2.7 版本移动其他异常的位置                      |\n| 3.<br>if (srcFile.isDirectory())\
    \ {                              |\n| 4.<br>throw new IOException(\"Source '\"\
    \ + srcFile + \"' is        |\n| a directory\");                             \
    \                     |\n| 5.<br>if (destFile.exists())                      \
    \              |\n| 6.<br>throw new FileExistsException(\"Destination '\" +  \
    \         |\n| destFile + \"' already exists\");                             \
    \    |\n| 7. }                                                            |\n\
    | (b)版本变更 V2.0  V2.7                                             |\n| 1. public\
    \ static void moveFile(File srcFile, File destFile) {   |\n| 2. -<br>validateMoveParameters(srcFile,\
    \ destFile);// //throw    |\n| other two exceptions                          \
    \                  |\n| 3. -<br>if (srcFile.isDirectory()) {                 \
    \           |\n| 4. -<br>throw new IOException(\"Source '\" + srcFile + \"'  \
    \       |\n| is a directory\");                                              \
    \ |\n| 5. -<br>if (destFile.exists())                                  |\n| 6.\
    \ -<br>throw new FileExistsException(\"Destination '\" +         |\n| destFile\
    \ + \"' already exists\"); //在 2.9 版本整体重构                  |\n| 7. + moveFile(srcFile,\
    \ destFile,                                |\n| StandardCopyOption.COPY_ ATTRIBUTES);\
    \                           |\n| 8. }                                        \
    \                    |\n| 9.                                                 \
    \             |\n| 10. + public static void moveFile(File srcFile, File destFile,\
    \  |\n| CopyOption copyOptions) throws IOException {                    |\n| 11.\
    \ +<br>validateMoveParameters(srcFile, destFile); //throw     |\n| other three\
    \ exceptions                                          |\n| 12. +<br>requireFile(srcFile,\
    \ \"srcFile\"); //throw other two     |\n| exceptions                        \
    \                              |\n| 13. +<br>requireAbsent(destFile, null);  \
    \                       |\n| 14. + }                                         \
    \                |\n| 15.                                                    \
    \         |\n| 16. + private static File requireFile(File file, String name) {\
    \ |\n| 17. +<br>Objects.requireNonNull(file, name);                    |\n| 18.\
    \ +<br>if (!file.isFile())                                    |\n| 19. +<br>throw\
    \ new                                              |\n| IllegalArgumentException(\"\
    Parameter '\" + name + \"' is not       |\n| a file: \" + file);             \
    \                                 |\n| 20. +<br>return file;                 \
    \                          |\n| 21. +<br>}                                   \
    \                   |\n| 22.                                                 \
    \            |\n| 23. + private static void requireAbsent(File file, String  \
    \     |\n| name) throws FileExistsException {                              |\n\
    \n24. + if (file.exists())\n\n25. + throw new FileExistsException(String.format\
    \ (\"File element in parameter '%s' already exists: '%s'\", name, file));\n\n\
    26. +}\n\n#### (**c**)版本变更 **V2.7 V2.9**\n\n#### 图 **3 Apache Commons-IO** 异常变更示例代码\n\
    \n# 3. Java 程序异常摘要提取方法\n\n本章介绍异常摘要提取模块的主要方法。\n\n## **3.1.** 异常摘要提取模块概览\n\nJavaExP\
    \ 的异常摘要提取模块主要包含基本信 息分析和异常前断言分析两个部分。如图 4 所示, 基本信息分析部分以 jar 包或 class 文件为输入,负 责构建程序的控制流图、函数调用图等数据结构,\
    \ 并提取每个方法中抛出异常的基本信息,获得方法 到异常的映射。异常前断言分析部分首先通过构建 方法的控制依赖图,去除判定结果与异常抛出行为 无关的非控制依赖条件,提高了断言分析结果的精\
    \ 准性。获取依赖条件后,再通过参数约束推断将异 常触发条件关联到外部输入参数,获取单个方法的 前断言。此外,通过函数调用关系和参数传递关系 追踪进一步构造了跨过程的异常抛出前断言。最\
    \ 后,形成异常信息摘要报告。\n\n![](_page_5_Figure_11.jpeg)\n\n### 图 4 Java 程序异常摘要提取模块流程图\n\
    \n# **3.2.** 基本信息分析\n\n基本信息提取部分基于静态分析框架 Soot [\\[44\\]](#page-17-6) 对输入代码进行预处理,为每个程序方法构建控制\
    \ 流图(Control Flow Graph,CFG),并生成全局的 函数调用图(Call Graph,CG)。接着,通过遍历 所有的语句,可以定位到显式抛出异常的\
    \ throw() 语句(称为异常抛出点),分析在每个异常抛出点 抛出的异常类型和描述文本等异常基本信息。对于 异常类型,我们分析异常变量的定义语句,并分析\
    \ 对应实例的类型;对于描述分析,我们从异常抛出\n\n点的描述文本变量反向追踪,通过字符串函数建模 还原完整的文本字符串,考虑到部分变量取值无法 直接获取,这里将异常拼接后的文本信息转换为正\
    \ 则表达式形式。\n\n获取基本信息后,JavaExP 记录方法名称和方 法中显示抛出异常的映射关系,并生成方法-异常映 射表,其中一个方法可以对应多个异常。对于图\
    \ 3(c) 的示例应用,可以得到一条映射边{ requireAbsent () FileExistsException @loc25}。由于方法 moveFile\
    \ 的参数会影响 requireAbsent 的异常抛出,因此对 moveFile 也应生成异常前断言。在后续分析中,该 断言可通过为requireAbsent方法构建异常行为摘要\
    \ 和追踪跨过程参数关系得到。\n\n# **3.3.** 前断言分析\n\n除了基本信息,异常摘要中的另一类重要元素 是前断言信息。前断言分析包括控制依赖条件分\
    \ 析、外部输入参数约束推断和跨过程参数约束推断 三个部分。\n\n### **3.3.1.** 控制依赖条件分析\n\n为了生成异常的前断言,首先需要准确提取异\
    \ 常的控制依赖条件。控制依赖条件一定在异常触发 的前置路径上,即在异常被触发时经过的程序路径 上。该路径可以从异常抛出点通过后向路径遍历得 到,而该路径上的全部条件被称为为异常触发的前\
    \ 置路径条件。\n\n定义 **5**(异常前置路径):PrePath (m,e) = (S0,…,Si, Si+1,..,Se) 为异常触发的一条前置路径,其\
    \ 中 S<sup>0</sup> 为方法 m 的入口语句,S<sup>e</sup> 为异常 e 的抛出语 句,程序语句 Si+1 是语句 Si的一个后继节点。异常\
    \ e 对应的多个异常前置路径形成了异常前置路径集 合 PrePathSet (m, e)。\n\n定义 **6**(异常前置路径条件): CondInPath\
    \ (m, e, prePath) 为异常前置路径上的条件语句的集合, 每个元素 cond ∈ CondInPath均是一个条件语句。\n\n对于图 3(a)中的\
    \ moveFile () 方法,第 2-9 行分 别抛出四个异常,如果他们的异常条件被满足,第 12 行异常 e 不会被抛出,因此,异常 e 依赖于这些\
    \ 控制条件。但如果将 2-9 行中的 throw 语句更改为 非终止语句,如输出、日志、数据处理等语句,则 其所属的条件将与第 12 行的异常无关。所有与异\
    \ 常抛出无关的路径前置条件语句无需被作为最终 异常断言的一部分。\n\n定义 **7**(异常控制依赖条件):ControlCond InPath (m,\
    \ e, prePath) 为异常前置路径上控制依赖 条 件 的 集 合 , 其 中 每 个 元 素 controlCond ∈ ControlCondInPath\
    \ 是一条和异常 e 之间存在控制依 赖关系的条件语句。即对于条件 controlCond 的多 个后继节点,存在至少一个后继节点不存在于异常 e 的任何异常前置路径中。\n\
    \n定义 **8**(异常控制依赖约束):ControlConstraint InPath (m, e, prePath)为异常前置路径上的异常控制 依 赖\
    \ 约 束 集 合 , 每 个 异 常 控 制 依 赖 约 束 (controlCond, isCondTrue) ∈ ControlConstraintInPath包\
    \ 括一个控制依赖条件语句及其条件判定结果。\n\n算法 1(extractConstraint)给出了异常控制依赖 条件的提取算法。第 2 行首先获取方法\
    \ m 的控制流 图 cfg,其中节点代表语句,边代表语句之间的控制 流向关系。第 3 行提取控制依赖图 cdg [\\[32\\]\\[33\\]](#page-16-11),其\
    \ 中节点代表语句,边代表语句之间的控制依赖关 系。通过搜索 cdg,可以得到异常抛出语句 Se对应 的异常控制依赖约束集合 controlConstraintSet。接着,\
    \ 第 5 行通过在 cfg 上后向路径遍历得到异常前置路 径集合 prePathSet。对于 prePathSet 中的每条异常前 置路径 prePath,第\
    \ 7-13 行负责构建仅包含异常控 制依赖条件切片的集合 controlConstraintInPath,并 在第 14 行将其加入输出集合 controlConstraintSet\
    \ 中。在这一过程中,第 8 行遍历 prePath 中的每一 个节点,如果一个节点是条件语句,且存在于 Se的 控制依赖节点集合 controlNodeSet\
    \ 中,将记录该节 点 node(第 9 行)。第 10 行分析 node 在 prePath 上的后继节点 node.succ,判定当前路径上 if 条件的\
    \ 判定结果为 true 或为 false(在字节码中,if 语句会 指明当 if 条件为真时的 goto 语句的位置,因此,可 通过下一语句 succ 是否为\
    \ goto 的目标语句判定条 件是否取值为真)。随后在第 11 行,该节点 node 与条件判定结果 isCondTrue 形成的约束条件 constraint\
    \ 会被加入当前路径的异常控制依赖约束 集合 controlConstraintInPath 中。最后,第 16 行将 返 回 方 法 m 的 异 常 控\
    \ 制 依 赖 约 束 集 合 controlConstraintSet,其中每个元素为一条路径上的 一组控制依赖约束。\n\n| 算法<br>1 | 控制依赖条件分析<br>extractConstraint\
    \                      |\n|---------|----------------------------------------------------|\n\
    | 输入:方法   | m, 异常<br>e(异常抛出语句为<br>Se)                          |\n|         |\
    \ 输出:异常控制依赖约束集合<br>controlConstraintSet              |\n|         | 1 Set <Set<Constraint>>\
    \ controlConstraintSet = new |\n\nHashSet (); //初始化\n\n- 2 Graph cfg = constructCFG\
    \ (m); //构建控制流图 cfg\n- 3 Graph cdg = constructCDG (cfg, m); //构建控制依赖图 cdg\n- 4\
    \ Set <Node> controlNodeSet= getControlNodesOfExp (cdg, Se); //在 cdg 中找到 S<sup>e</sup>\
    \ 的控制依赖节点\n- 5 Set <Path> prePathSet = backTraverseFromExp (cfg, Se); //从 S<sup>e</sup>\
    \ 后向路径遍历得到异常前置路径\n- 6 for (Path prePath : prePathSet){\n- 7 Set <Constraint> controlConstraintInPath=\
    \ new HashSet ();\n- 8 for (node : prePath) {·\n- 9 if (node.isCondition () and\
    \ controlNodeSet.contains (node)) {\n- 10 Boolean isCondTrue =getCondJudgeRes\
    \ (node, node.succ); //获取分支条件的判定结果\n- 11 controlConstraintInPath.add (new Constraint\
    \ (node, isCondTrue)); //新增控制依赖条件\n- 12 }\n- 13 }\n- 14 controlConstraintSet.add\
    \ (controlConstraintInPath) //增加一条路径上的一组控制依赖条件\n- 15 }\n\n16 Return controlConstraintSet\n\
    \n例如,对于图 3(a)中的 moveFile() 方法,第 2- 11 行的 5 个控制条件均存在一个后继节点不在任 何一条异常前置路径中,因此异常控制依赖条件为\
    \ ControlCondInPath ={2,4,6,8,10}。接着,通过分析异 常控制依赖条件在异常前置路径中的后继语句,即 条件为真或为假时的后继语句在异常前置路径中,\
    \ 可以得到每个异常控制依赖条件取值结果,即 controlConstraintInPath = {(srcFile == null, false), (destFile\
    \ == null, false), (srcFile.exists(), true), (srcFile.isDirectory(), false), (destFile.exists(),\
    \ true)}。 这里仅为方便展示,在实际的字节码分析过程中, 可被获取的是中间变量约束,如(\\$z0==0, false)。\n\n# **3.3.2.**\
    \ 外部输入参数约束推断\n\n经过上一节的分析,可以获取异常的控制依赖 约束。但字节码中没有变量名称信息,仅有按序编 号的内部变量,如 r0,z1,而这些变量在不同版本\
    \ 中不存在关联关系,难以被直接用于匹配和比较。 因此,应将约束的主体转换为语义固定的对象,如 API 的参数,提取异常前断言时通过数据流追踪获 取的内部变量约束归约为参数相关约束,从而准确\
    \ 分析异常前断言的变更情况。\n\n算法 2(refineAnalysis)对于异常控制依赖条件 集合中每条路径上的每个 控制依赖约束条件 controlConstraint\
    \ 进行分析,通过参数推断得到仅 与外部输入参数相关的 refinedConstraint。算法第 1 行提取语句中的所有变量并将它们放入集合 dataRelatedVars。第\
    \ 2-4 行中,对于变量集合中非输 入参数相关的内部变量,通过数据流分析获取其最 近的变量赋值语句,其中方法 getDefUse 通过数据 流分析获取方法\
    \ m 的定义-使用链 [\\[45\\]](#page-17-7)。第 5 行根 据数据分析结果更改原约束条件 controlConstraint, 即依次使用内部变量的赋值结果替换该变量,并将\
    \ 更新后的约束条件递归地传入 refineAnalysis 方法 继续分析,直至 refinedConstraint 中不再包含内部 变量时,迭代终止。这里,中间码预先被转换为\
    \ SSA 格式。此外,JavaExP 还通过启发式策略调整优化 了输出形式,增强断言的用户可读性。该算法最后 返回推断后的约束条件 refinedConstraint。\n\
    \n| 算法<br>参数约束推断分析<br>2<br>refineAnalysis          |\n|------------------------------------------------|\n\
    | 输入:方法 m, 异常 e(异常抛出语句为 Se),异常                   |\n| 控制依赖条件 controlConstraint\
    \                       |\n| 输出:推断后的约束条件 refinedConstraint                  |\n\
    | 1 Set <Value> dataRelatedVars= getVarFromStmts |\n| (constraint.getStmt ());\
    \ //提取语句中的变量            |\n|                                                |\n\
    \n- 2 For (Value: value dataRelatedVars){\n- 3 if(isOutsideValue(value)) continue;\n\
    - 4 Stmt assignStmt= getAssignStmtofValue (getDefUse(m), value); //后向数据流分析获取最近\
    \ 的变量赋值语句\n- 5 refinedConstraint = replaceValueInConstraint (constraint.getStmt\
    \ (), value, assignStmt. getRightValue()) //将约束语句中的 value 替换为 value 的赋值内容\n- 6\
    \ refineAnalysis(m, e, refinedConstraint)\n- 7 }\n- 8 Return refinedConstraint\n\
    \n据流追踪,得到该方法内的控制依赖条件为 (\\$z0==0, false)}。表 1 (a) 中给出了通过数据流分析 反向推断外部数据约束 parameter0.exists()的过程。\n\
    \n# 表 **1(a)** 过程内追踪约束条件 **(requireAbsent)**\n\n| 控制依赖      | 数据流追踪后推断的约束条件  \
    \                            |  |  |\n|-----------|--------------------------------------------|--|--|\n\
    |           | \\$z0 is true + \\$z0 is-invoke r0.exists()  |  |  |\n| (\\$z0==0,\
    \ |  r0.exists() is true                      |  |  |\n| false)    | r0.exists()\
    \ is true + r0 denote parameter0 |  |  |\n|           |  parameter0.exists()\
    \ is true              |  |  |\n\n### **3.3.3.** 跨过程参数约束推断\n\n在 API 的演化过程中,异常的抛出位置可能发\
    \ 生移动,如将异常抛出语句移动到另一方法并调用 它[\\[17\\]](#page-16-12)。如采用过程间分析,这类代码重构会被识别 为异常的删除,从而引发\
    \ API 演化分析中的误报。 如图 3 对应的异常实例,在版本 1.4 中,异常抛出 代码仅在 moveFile()本身中出现,但在最新的 2.13 版本中,该异常需要至少联合分析六个函数的才能\
    \ 被准确获取。为了增加异常匹配分析的准确度, JavaExP 在为过程内所有异常实例构造摘要信息的 基础上,通过函数调用关系分析和参数映射关系分 析,生成跨过程的异常摘要信息。\n\
    \n具体的跨过程参数约束推断的过程为:首先构 造应用程序的函数调用图,并对函数调用关系进行 拓扑排序。再按照拓扑序的逆序自底向上的分析每 个方法,其中被调用的方法\
    \ callee 一定会比调用它 的方法 caller 更早被分析。当定位到当前方法 caller 中的一个调用语句 stmt 时,可得到被调用的方法 callee。如果被调用方法\
    \ callee 的参数与异常的抛出 有关,则会根据调用方法 caller 和被调用方法 callee 中参数位置的映射关系,更新从被调用方法 callee\
    \ 中获取的参数相关约束 cons1。此外,还应提取 caller 方法中调用 callee 语句前的程序路径上需要满足的 参数相关约束,包含路径上的控制依赖约束\
    \ cons2, 和路径上被调用函数中的其他异常抛出约束的取 反 cons3。约束 cons1、cons2、cons<sup>3</sup> 均更新到调用方\
    \ 法 caller 的参数约束中后,可得到关于方法 caller 的 异常前断言。对于函数内直接抛出的异常,类似的, 也应使用异常抛出前置路径的函数调用中使得异\
    \ 常不被抛出的条约束 cons<sup>3</sup> 更新其直接约束。按照 拓扑逆序分析,从而可以依次更新每个方法的前断 言信息,由于底层的约束会向上传递更新,每个方\
    \ 法仅需被分析一次。\n\n对于图 3(c)中的 moveFile 方法,其第 1 个参数\n\n对应方法 requireAbsent 中的第 0 个参数,根据这一\
    \ 参数映射关系可以更新 requireAbsent 中的前断言 信息得到 moveFile 的约束。此外,在 moveFile 中 调用 requireAbsent\
    \ 方法时,执行到该方法调用点时 应满足来自前置方法 validateMoveParameters 和 requireFile 中的其他约束,最终得到由 5\
    \ 个约束组 成的完整前置条件。结果见表 1(b)。\n\n| 表<br>跨过程追踪约束条件<br>1 (b)<br>(moveFile) |      \
    \                                                                 |  |  |  | \
    \ |\n|---------------------------------------|-----------------------------------------------------------------------|--|--|--|--|\n\
    | 控制依赖                                  | 数据流追踪后推断的约束条件                      \
    \                                   |  |  |  |  |\n|                         \
    \              | requireAbsent@parameter0.exists() is true<br>+ requireAbsent(r1,null)\
    \ |  |  |  |  |\n|                                       |  r1.exists() is true\
    \                                                 |  |  |  |  |\n| constraint\
    \ in                         | r1.exists() is true + r1 denote parameter1    \
    \                        |  |  |  |  |\n| requireAbsent                      \
    \   |  parameter1.exists() is true                                         |\
    \  |  |  |  |\n| + constraints                         | merge constraints from\
    \ related methods                                |  |  |  |  |\n| in moveFile\
    \                           | parameter0 is not null + parameterl is not     \
    \                       |  |  |  |  |\n|                                     \
    \  | null + parameter0.exists() is true +                                  | \
    \ |  |  |  |\n|                                       | parameter0. isFile() is\
    \ true +                                        |  |  |  |  |\n|             \
    \                          | parameterl.exists() is true                     \
    \                      |  |  |  |  |\n\n# 4. 基于异常提取的 API 生命周期构造\n\n本章介绍 API 生命周期构造模块的主要方法。\n\
    \n# **4.1. API**生命周期构造模块概览\n\nJavaExP 的生命周期构造模块主要包含 API 匹 配与变更分析、生命周期模型构造两个部分。如图\
    \ 5 所示,异常匹配与变更分析部分以异常摘要文件 为输入,先采用完全匹配策略获取 API 异常实例的 匹配关系,再通过自适应匹配策略识别其他异常实 例的映射和局部变更情况,生成异常敏感的\
    \ API 变 更报告;生命周期模型构造部分则以多个版本的 API 变更分析报告为输入,分析同一 API 方法或同 一异常实例在不同版本中的变更过程,最后生成相\
    \ 应的生命周期模型。\n\n![](_page_8_Figure_14.jpeg)\n\n图 5 API 生命周期构造模块流程图\n\n# **4.2.**\
    \ 异常信息敏感的**API**匹配与变更分析\n\n基于提取的异常摘要信息,接下来,我们采用 完全匹配和自适应匹配相结合的异常匹配方式对 不同版本中的异常进行匹配。在异常匹配过程,API\
    \ 签名、异常类型、描述和断言信息是四个关键信息, 这里仅考虑签名相同的 API 中异常的匹配,如果 API 签名变更,则认为发生方法级改变。如表 2 所\
    \ 示,异常类型、描述和断言信息的不同组合共有 8 种。如果两个异常符合规则 R1,三个信息可完全匹 配,则表明该异常未发生改变。如不能完全匹配, 则需根据规则\
    \ R2-R8 进行自适应匹配。在这些规则 中,我们将根据对符合该类别特征代码实例的经验 分析和意图理解,判定不同变更特征下的异常代码 是否可匹配。对于 R2-R4,仅有单一信息变更,这\
    \ 类变化更有可能是同一异常的正常演化导致的,而 非两个高度相似的异常。而如果多个信息变更,其 匹配情况将相对复杂,如果两个异常摘要中有至少 两类信息不一致(R5-R8),通常可认定两个异常指\
    \ 向不同的实例。但考虑到异常类型这一信息指向性 较为稳定,在代码重构中有可能出现类型不变但是 异常描述和断言发生改变的情况,因此规则 R5 下 的匹配结果将视情况而定,具体参见以下异常匹配\
    \ 的详细流程。\n\n|    | API | 异常摘要 |             |    |         |  |\n|----|-----|------|-------------|----|---------|--|\n\
    | 规则 | 签名  | 类型   | 描述          | 断言 | 匹配结果    |  |\n| R1 | √   | √    | √   \
    \        | √  | 匹配-未变更  |  |\n| R2 | √   | X    | √           | √  | 匹配-类型变更 |\
    \  |\n| R3 | √   | √    | X           | √  | 匹配-描述变更 |  |\n| R4 | √   | √    |\
    \ √           | X  | 匹配-断言变更 |  |\n|    |     |      |             |    | 可能匹配-断言\
    \ |  |\n| R5 | √   |      | √<br>X<br>X |    | 和描述变更   |  |\n| R6 | √   | X  \
    \  | √           | x  | 不匹配     |  |\n| R7 | √   | X    | X           | √  | 不匹配\
    \     |  |\n| R8 | √   | X    | X           | X  | 不匹配     |  |\n\n表 **2** 异常实例匹配规则\n\
    \n- 1) 对于所有待分析 API,采用规则 R1 对 API 在相 邻版本中的异常进行完全匹配,如果匹配成功则 将匹配到的一对异常分别从各自版本下的匹配\
    \ 队列中移除。\n- 2) 采用异常类型变更 R2、异常描述变更 R3、和异 常断言变更 R4 规则对应的三个规则进行匹配。 这里将择依次使用各个信息过滤器,比较异常类\n\
    \n型、描述、前断言信息是否一致。过滤器接受一 组异常,根据过滤器类型返回过滤后的一组异 常。如旧版本中的异常 e<sup>1</sup> 经过第一个过滤器之\
    \ 后匹配到多个异常,那么所有被匹配的异常会被 作为候选对象传递给下一个过滤器;如 e<sup>1</sup> 未匹 配到任何异常,则将当前过滤器接受到的全部异\
    \ 常将被作为候选对象传递给下一个过滤器。如果 在过滤过程中,e<sup>1</sup> 在新版本中唯一匹配到异常 e2,则表明 e<sup>1</sup>\
    \ 和 e<sup>2</sup> 指向同一个异常,e<sup>1</sup> 和 e<sup>2</sup> 将 被记录并从各自的匹配队列中移除。\n\n\
    - 3) 经过步骤 2 后未找到唯一匹配的异常,将继续采 用规则 R5 进行匹配。满足这种情况的可匹配异 常往往是由于代码重构造成的,由于前置方法、 代码位置变更导致了异常前断言不完全相同,但\
    \ 这种情况下异常最近最直接的抛出条件通常不 会改变,因此,可提取距离异常抛出点最近的直 接条件对应的前断言(关键前断言)信息用于匹 配。对于异常 e1,如果能够唯一找到异常\
    \ e2,它 们的异常类型和异常关键前断言均相同,即使断 言和描述信息不同,e1和 e<sup>2</sup> 也会被匹配,记录后 从各自的匹配队列中移除。\n\
    - 4) 经过步骤 3 后,待匹配异常集合规模可能减小, 使得之前一对多匹配的异常变更为唯一匹配。因 此,可再次重复过程 2)和 3),直至匹配结果不 变,即达到不动点。\n\
    - 5) 如果多轮匹配结束后异常仍未被匹配,那么旧版 本中的未匹配异常将被标记为异常删除,新版本 中的未匹配异常将被标记为异常新增。\n\n在图 3 代码中,JavaExP\
    \ 根据规则 R2 匹配到 V1.4-V2.0 的变化,根据规则 R1 完全匹配了 V2.0- V2.7,最后根据规则 R5 匹配到 V2.7-V2.9 的变化,\
    \ 从而完整刻画了该 API 中异常的变化情况。\n\n### **4.3.** 异常敏感的**API**生命周期模型构造\n\n经过异常匹配与变更分析后,可得到\
    \ API 在相 邻版本中异常的变更结果。变更类型包含定义 **4** 中 声明的七类操作:API 新增、API 删除、异常新增、 异常删除、异常类型变更、异常描述变更和异常断\
    \ 言变更。为了生成完整的生命周期报告,我们从初 始版本开始,不断使用相邻版本的异常摘要构造更 完整的生命周期报告,直至最新版本。\n\n图 6 给出对图\
    \ 3 示例中异常实例的生命周期报 告。该报告包含异常所在的 API 的信息,如引入和 删除版本;包括异常的基本信息,如异常的引入、 删除版本;以及在不同版本区间上异常摘要信息的\n\
    \n变更情况,如异常类型变更。在这个实例中,异常 所在的 API moveFile(), 在 1.4 版本引入,在最新的 2.13 版本中依然存在。其中目标异常\
    \ e 共发生了三 次变更,分别是:2.0 版本变更了异常类型;2.9 版 本中更改了异常的描述文本;2.9 版本中更改了异 常前断言中涉及到的方法调用。值得注意的是,在\
    \ 2.7 版本中,虽然相关代码被开发者重构,但异常信 息分析表明,其异常抛出行为未发生任何改变,因 此,该版本未出现在变更报告中。\n\n![](_page_10_Figure_2.jpeg)\n\
    \n图 6 图 3 中 **API moveFile()**的生命周期模型\n\n# 5. 工具实现与实验分析\n\n基于本文提出的方法,我们实现了一种基于静\
    \ 态分析的 Java 程序 API 生命周期模型自动分析工 具 JavaExP [\\[46\\]](#page-17-3)。该工具包含约 10k 行 Java\
    \ 代码和 1.4k 行 Python 代码,依赖于底层分析框架 Soot [\\[44\\]](#page-17-6) 完成中间码提取、控制流图、函数调用图和控制依\
    \ 赖图[\\[32\\]](#page-16-11)等数据结构的构建。为了评估本文方法的 有效性和效率,本章在多个基准数据集上对工具 JavaExP 开展了一系列实验,主要研究问题如下:\n\
    \n- **RQ1**:在手工构造和真实 Java 项目上,JavaExP 能否准确高效地提取异常摘要信息?\n- **RQ2**:在真实框架/库项目上,JavaExP\
    \ 能否正 确构造异常信息敏感的API的生命周期模型?\n- **RQ3**:在真实框架/库项目上,异常信息敏感的 API 的生命周期模型有何特征?\n\n\
    ### **5.1.** 实验设置\n\n为回答 RQ1,我们手工构造了一个包含常见异常抛 出方式的基准测试集 ExceptionBench [\\[51\\\
    ]](#page-17-8),涉及到 多种常见的 Java 特性。数据集中的六个类分别为: 基本场景 Basic 类,其中包含无条件抛出、判空条 件抛出、字符串取值条件抛出、字符串操作条件抛\
    \ 出、逻辑与/或条件抛出等;跨函数调用场景 MultipleCall 类,包括多种跨函数调用场景;多路径 场景 MultiplePath 类,包含 if-else\
    \ 分支路径和 for 循 环路径;多个异常场景 MultipleThrow 类,包含同 一函数内多个异常和跨过程调用导致的多个异常; 类字段变量使用场景\
    \ FieldValue 类;和一个融合了 多种场景的综合场景 Motivation 类;共包含 40 个 异常。此外,本实验复用了 Nassif 等人构造的面向\
    \ Apache Commons IO [\\[49\\]](#page-17-9)的异常断言标注集合[\\[34\\]](#page-17-10)。 为保证对比的公平性,本实验仅将目标项目的源文\
    \ 件和对应 Jar 包作为输入,而未将任何项目外的第 三方库、JDK 等代码作为分析目标,排除未显式抛 出的异常和代码实现不在项目中的异常后,该数据 集共包含\
    \ 392 个独立异常。此外,本实验还选取了 六个广泛使用的 Java 项目来评估工具在这些真实 项目上的分析性能。评估标准包括异常摘要数量和 运行时间等。这里对每个项目的分析时间上限均被\
    \ 设置为 2 小时。为回答 RQ2 和 RQ3,我们以 RQ1 中六个真实项目为演化分析目标,根据这些项目在 Maven 仓库[\\[53\\]](#page-17-11)中的代码发布情况,收集了共计\
    \ 60 个历史版本 jar 包。随后,我们在这些版本上对真 实项目中API的演化情况进行分析。对于同一项目, 演化分析时仅考虑同级别变更版本。\n\n在实验有效性评估的对比工具选择方面,由于\
    \ 尚没有对于异常信息敏感的API演化分析的直接对 比工具,我们首先选择 Java 异常信息提取工具,对 异常分析这一重要模块的精度进行评估,再人工检 验\
    \ API 演化分析结果的有效性。经调研,工作 [\\[21\\]\\[22\\]](#page-16-13)可通过自然语言处理方式从 JavaDoc 文档\
    \ 或注释中提取异常信息,但由于文档/注释信息和代 码信息常存在偏差,不能将文档信息中提取的异常 信息作为演化分析的对象。工作[\\[34\\]](#page-17-10)\
    \ [\\[54\\]](#page-17-12)通过机器 翻译方法为异常抛出代码生成文档或通过自然语 言处理技术自动生成测试用例,但其异常信息分析 结果无法直接用于异常演化分析。基于此,本文选\
    \ 择了最新 SOTA 的 Java 异常前断言提取自动化工 具 WIT [\\[26\\]](#page-16-10),评估该工具与 JavaExP 在异常提取方\
    \ 面的能力差异。WIT 工具通过静态分析解析目标 Java 项目源代码,构造控制流图并提取跨过程路径 约束,结合约束求解技术获得异常相关的变量约束 信息,最终提取出\
    \ Java 异常的类型、前断言、描述 等信息,且其工具是公开可获取的。\n\n### **5.2.** 实验结果与分析\n\n### **5.2.1.**\
    \ 异常摘要报告有效性评估 **(RQ1)**\n\n对于异常分析提取,分析工具提取到的异常都 是真实存在的,其错误结果分为两种:未识别到真 实的异常,对应为漏报\
    \ FN;识别到真实异常,提取 了错误的异常摘要信息,而遗漏了正确的异常摘要 信息,可被认为既属于误报,也属于漏报信息。\n\n|  |  |  |  |\
    \  |  | 表 3 工具在 ExceptionBench 数据集上的有效性 |\n|--|--|--|--|--|--|---------------------------------|\n\
    |--|--|--|--|--|--|---------------------------------|\n\n| 工具      | TP | FP |\
    \ FN | Precision | Recall | F1-Score |\n|---------|----|----|----|-----------|--------|----------|\n\
    | JavaExP | 39 | 1  | 0  | 0.98      | 0.98   | 0.98     |\n| WIT     | 30 | 5\
    \  | 10 | 0.86      | 0.75   | 0.80     |\n\n表 3 分别给出本文 JavaExP 工具和异常提取工 具 WIT\
    \ 工具在 ExceptionBench 数据集上的有效性 评估结果,包括 TP、FP、FN 数值,和根据公式 Precision=TP/(TP+FP),\
    \ Recall=TP/(TP+FN), F1- Score=2×Precision×Recall/(Precision+Recall)计算 出的精确度、召回率和\
    \ F1 分数的结果。可以看到, JavaExP成功为其中的39个异常生成了正确的异常 摘要报告,其中 1 个误报是由于异常涉及复杂的数 据值变更,导致前断言分析不准确。在\
    \ WIT 不能处 理的 10(5+5)个异常中,5 个由于包含冗余且错误 的约束或涉及复杂的数据值变更导致异常前断言 提取结果有误,5 个涉及不支持的语法特性导致异\
    \ 常摘要未成功提取。在精确度、召回率和 F1 分数 三个度量指标上,JavaExP 均优于 WIT 工具。\n\n| 工具      | TP  | FP\
    \ | FN  | Precision | Recall | F1-Score |  |\n|---------|-----|----|-----|-----------|--------|----------|--|\n\
    | JavaExP | 300 | 56 | 36  | 0.84      | 0.77   | 0.80     |  |\n| WIT     | 137\
    \ | 17 | 255 | 0.89      | 0.35   | 0.50     |  |\n\n表 4 给出了工具 WIT 和 JavaExP 在公开基准\
    \ 测试集 DScrib[e\\[34\\]](#page-17-10)中异常上的有效性分析结果。在 该数据集上,WIT 正确生成的异常摘要数量为 137。\
    \ WIT 不 能 正 确 生 成 摘 要 的 异 常 数 量 为 255 (238+17),其中有 238 个异常的摘要为空,17 个 异常的摘要信息不准确。与之相比,JavaExP\
    \ 成功 生成了 300 个正确的异常摘要报告。JavaExP 不能 正确生成摘要的异常数量为 92(36+56),其中有 36 个异常的摘要为空,56 个异常的摘要信息不准\
    \ 确。虽然 WIT 的分析精确度略高于 JavaExP,但其\n\n召回率显著下降。与 WIT 相比,JavaExP 同时实现 了较高的精确度和召回率,F1\
    \ 分数与 WIT 比相对 提升了 60%。\n\n通过对 JavaExP 错误结果的分类分析,我们发 现对于 56 个摘要信息不准确的异常,其中的 22\
    \ 个 异常受限于循环条件展开次数,11 个异常存在无法 正确分析的复杂关系,10 个异常缺少部分正确路 径,8 个异常中被调用函数 callee 中的前断言约束\
    \ 无法正确映射到调用函数 caller 的参数,3 个异常 没有正确处理 try 语句中的异常抛出,2 个异常的前 断言条件自相冲突。对于另外 36 个摘要结果为空\
    \ 的异常,导致精度损失的一个原因是复杂跨函数参 数传递增加了分析难度。如图 3 所示,随着版本更 新,框架/库开发者在重构的过程中倾向于将异常抛 出代码进行包装以便复用,这间接增加了分析的复\
    \ 杂性。在 Apache Commons IO 项目 V2.13 版本中, 调用链长度不少于 5 的共有 578 处,调用链长度不 少于 10 的共有 50\
    \ 处,而调用链中任意一处异常摘 要信息误差均可能影响最终的匹配情况。此外, JavaExP 对字节码的静态分析能力也影响了分析结 果,如在处理位运算代码的断言条件时尚存在偏\
    \ 差、对静态变量的取值使用初始赋值,循环条件仅 展开 0 次和 1 次等,这些分析影响了前断言中部分 条件的准确性。\n\n|             |\
    \       |         |       | WIT          | JavaExP |        |       |  |\n|-------------|-------|---------|-------|--------------|---------|--------|-------|--|\n\
    | 项目名         | 版本    | LOC     | 总摘    | 时间           | 独立摘     | 总摘     | 时间\
    \    |  |\n|             |       |         | 要数    | /秒           | 要数      |\
    \ 要数     | /秒    |  |\n| Commons IO  | 2.6   | 9,984   | 297   | 3,903       \
    \ | 268     | 1,285  | 44    |  |\n| JGraphT     | 0.9.2 | 15,660  | 142   | 119\
    \          | 176     | 4,17   | 52    |  |\n| GraphStream | 1.3   | 48,535  |\
    \ 142   | 431          | 342     | 2,087  | 160   |  |\n| Guava       | 19.0 \
    \ | 70,250  | 2,347 | 6,133        | 221     | 3,891  | 112   |  |\n| Nashorn\
    \     | 1.8   | 83,728  | 177   | 1,759        | 949     | 3,446  | 355   |  |\n\
    | Android     | 10.0  | 546,655 | 524*  | 7,200        | 7,906   | 51,915 | 1743\
    \  |  |\n| 合计/平均       |       | 129,135 |       | 3,692 19,545 | 9,862   | 62,624\
    \ | 2,466 |  |\n\n表 **5** 工具在真实项目上的分析结果\n\n进一步,我们在六个真实项目上进行分析性能 的评估。表 5 给出了所选项目的名称、版本和大小,\
    \ 其中大小为不含注释的 Java 源码行数。后五列对比 了两个工具的结果中异常摘要数量和分析时间。其 中,对于大型的 Android 框架代码,WIT 超时导致\
    \ 未完成分析,因此,仅统计其在两小时内生成的结 果,并计算其中包含的异常摘要数量。对于六个被 测项目,WIT 用时约 5.5 小时,提取的总异常摘要\n\n\
    数量为 3,692。与之相比,JavaExP 用时仅 0.7 小时, 提取的独立摘要数量为 9,862,总摘要数量为 62,624,分析效率提高约 7 倍,提取数量显著增加。\
    \ 基于这一结果,我们进一步分析了异常数量的增加 是由于 JavaExP 分析到了更多的独立异常,还是由 于独立异常在跨函数调用过程中在不同调用路径 中重复出现导致的。我们根据异常抛出方法、异常\
    \ 抛出语句位置信息进行去重后的异常数量计为独 立异常的数量,独立摘要的数量少于总摘要数量。 据统计, WIT 的总摘要数量 (3,692)显著少于 JavaExP\
    \ 的独立摘要数量(9,862),由此可知,与 WIT 相比,JavaExP 不仅提取出更多的异常摘要结果, 且成功分析了更多的独立异常。\n\n实验表明,JavaExP\
    \ 提取的异常摘要数量显著 多于 WIT,且用时更短,带来这一优势的主要原因 有以下三个方面。首先,JavaExP 基于字节码分析, 不受限于新的 Java\
    \ 语法特性,分析范围更广,能够 正确处理 Java 的各种复杂语法特性支持,其分析能 力不受到代码形式的影响。其次,JavaExP 对分析 规模的限制更少,该方法并没有对每条路径上的节\
    \ 点数、函数内联后的节点数量等做严格限制[26],而 是在遍历控制流路径时先提取终止于异常抛出的 语句,仅分析异常抛出行为相关的代码切片,分析 范围更为聚焦。此外,JavaExP\
    \ 采用自底向上构建 函数摘要的方式,对每个函数不会被重复分析,带 来了明显的效率优势。除了效率优势,JavaExP 的 分析准确度也较高。JavaExP\
    \ 通过提取异常的类型、 描述文本、前断言三类核心信息对异常进行刻画, 在异常前断言分析时主动忽略了与当前异常抛出 无关的非控制依赖条件,但沿着函数调用链追踪异\
    \ 常抛出必要的关键前置条件,提高了异常分析的准 确性,这也为后续在不同版本中匹配异常的演化信 息打下了良好的基础。\n\n**RQ1** 结论:相比于现有的异常分析工具,JavaExP\
    \ 能够更加准确地提取异常的摘要信息,在现有数 据集上,将分析精度提高了约 60%;通过跨函数 摘要合并策略,将分析效率提高了 7 倍,并显著 增加了成功提取的异常摘要数量。\n\
    \n# **5.2.2. API** 生命周期模型构造正确性 **(RQ2)**\n\n在 RQ2 中,我们选取六个项目中发布历史版本 数量最多(19 个)的\
    \ Apache Commons IO 项目,人 工确认 JavaExP 在该项目上 API 演化分析结果的有 效性。对于 API 和异常增删修改的七种形式,为了\
    \ 保证公平性,本实验选取对象具体的方法为:对于 每个变更类别,首先根据变更实例数量对所在的 Java 类(class)文件进行排序。对于各个变更类型, 根据其变更总数,从所有变更实例中按照均匀分布\
    \ 采样间隔地选择实例。考虑到异常前断言的数量相 对较多,在选择时容易选择到因方法封装在不同上 层方法被重复调用的异常实例,为增加多样性,避 免确认相似的异常,该类别下会对异常调用链进行\
    \ 过滤,仅收集未包含相同异常抛出方法的实例。我 们对每种类别均收集 10 个实例(不足 10 时按实际 数量)。最终,对于 7 种变更类型,共收集了 63\
    \ 个 变更实例,结果见表 6,详细的人工确认报告见[\\[46\\]](#page-17-3)。\n\n对于过程内的分析,大量跨过程调用引入的异 常均无法被分析,占异常总数的\
    \ 94%;对于在当前 方法中抛出的可被分析的异常,异常抛出之前的跨 过程调用也会对前断言产生影响。在表 6 中,过程 内分析评估时仅选择在当前方法中抛出的异常,如\
    \ 果只考虑当前方法内出现的约束条件,分析结果 均正确,但如考虑其他方法调用带来的隐式约束, 有 5 个断言信息变更行为正确,其中 3 个异常摘要 前断言信息提取结果完全正确。\n\
    \n表 **6 API** 演化分析的正确性\n\n|        | 统计 API |    | API | 异常 | 异常 | 异常文 | 异常类 |\
    \ 异常条      |\n|--------|--------|----|-----|----|----|-----|-----|----------|\n\
    |        |        | 新增 | 删除  | 新增 | 删除 | 本修改 | 型修改 | 件修改      |\n| 过程内 数量 |  \
    \      | 10 | 10  | 10 | 10 | 10  | 3   | 10       |\n|        | 正确     | 10 |\
    \ 10  | 10 | 10 | 10  | 3   | 10[5/3]  |\n| 跨过程 数量 |        | 10 | 10  | 10 |\
    \ 10 | 10  | 10  | 10       |\n|        | 正确     | 10 | 10  | 10 | 10 | 9   |\
    \ 10  | 10[10/5] |\n\n与之相比,跨过程的分析则可以分析被调用函 数中抛出的深层异常和前置函数中的隐式约束。受 限于字节码静态分析,在异常条件修改变更结果\
    \ 中,5 个异常摘要前断言信息提取结果完全正确。5 个异常摘要提取结果不完全准确,但它们不影响对 断言变更检测结果的正确性,如循环展开有限次和 位运算约束结果不完全准确,但变更前后能够显著\
    \ 区分。在异常描述变更结果中,有一处错误的异常 匹配。这是因为版本 2.9 中的代码被大幅重构,原 异常的类型、消息、前断言均发生了改变,但函数 中刚好存在另一个与原异常类型和关键前断言均\
    \ 相同的异常,从而导致它们被错误匹配。总体来看, 跨过程策略下,演化分析的整体准确率达到 98%, 跨过程分析能够捕获到其他函数内存在的异常及 其断言条件,变更分析结果整体较为准确。后续\
    \ RQ3 中异常演化分析默认采用跨过程分析策略。\n\n经人工总结,影响 API 中异常匹配的可能因素 包括:1)在跨过程传递分析中,异常断言分析的精 度和过程间参数约束的分析可能传递影响最终的\
    \ 匹配结果;2)开发者可能同时修改同一个异常的多 个信息,导致难以通过单一变化严格限制匹配规 则,需在尽量避免错误匹配的前提下,尽可能识别 出存在差异的同一异常。3)目前仅能匹配文本相等\
    \ 和逻辑相等,如果开发者换用语义相同的不同 API, 如!isFile()和 isDirectory()语义相同,因无法自动判 断前后是否一致,会识别其为异常条件变更。如需\
    \ 判断语义一致性,需在后续研究中引入语义分析。\n\n**RQ2** 结论: JavaExP 能够基于提取的异常摘要, 准确构建 API 的生命周期模型,其中跨过程分析\
    \ 策略更为准确。部分异常的前断言信息存在精度 损失,但对 API 演化分析的影响不大。\n\n### **5.2.3. API** 生命周期变更结果分析\
    \ **(RQ3)**\n\n对于 RQ2 中的六个项目,我们在 Maven 仓库 中收集了各项目的共计 60 个历史发布版本,并在 表 7 中统计了各个项目的\
    \ API 变更情况。所有版本 的分析时间共计 30 分钟。对于 API 本身的变更, 新增 API 数量较多,随着版本演化,API 的数量整 体趋向于一直增加,但也有部分\
    \ API 会被删除。当 发生大版本重构时,API 变化较为明显。除了 API 的新增删除,JavaExP 还识别出了大量的异常变更 行为。在所有的 75,433\
    \ 个 API 中,约 14.3%的 API 新增过异常抛出行为,13.9% 删除过原有的异常, 6.5%更改过抛出条件,1.9%更改过异常描述文本, 0.1%变更过异常类型。在异常敏感的\
    \ API 生命周期 模型中,约 20%的 API 在被引入后,异常信息在后 续版本中发生过调整,这说明 API 中异常相关代码 的调整是十分常见的,异常敏感的\
    \ API 生命周期构 造能够更加精准的描述 API 的实际变更情况。\n\n| 项目名         | #分析 | #总     | #新增    |\
    \ #删除    | #异常摘要        |\n|-------------|-----|--------|--------|--------|--------------|\n\
    |             | 版本  | API    | API    | API    | 改变 API       |\n| Commons IO\
    \  | 19  | 1,880  | 1,665  | 36     | 548 (29%)    |\n| JGraphT     | 7   | 2,493\
    \  | 1,859  | 740    | 665 (27%)    |\n| GraphStream | 5   | 3,345  | 1,607  |\
    \ 1,460  | 194 (6%)     |\n| Guava       | 14  | 3,772  | 1,868  | 666    | 848\
    \ (22%)    |\n| Nashorn     | 5   | 4,340  | 8      | 11     | 739 (17%)    |\n\
    | Android     | 10  | 59,603 | 32,331 | 8,260  | 11,512 (19%) |\n| Total     \
    \  | /   | 75,433 | 39,338 | 11,173 | 14,506 (20%) |\n\n表 **7 API** 变更情况统计分析\n\
    \n进一步的,我们在图 7 和表 8 中展示了六个项 目中异常实例的变更情况。图 7(a)中统计了每个 API 中的发生变更的全部异常实例,当一个异常被 封装并多次调用时会多次统计。可以看到,当考虑\
    \ 重复异常时,新增异常的数量占比最高,其次删除 异常的数量。实际上部分新增异常是被重复调用 的。图 7(b)以独立异常为关注对象,异常多次调 用时仅统计一次,表\
    \ 8 给出了对应图 7(b)中独立 异常变更的数量统计,与图 7(a)相比,新增删除 异常的数量占比有所下降。在异常语义行为的不同 变更中,异常类型变更整体数量最少,主要包括子\
    \ 类到父类的变更,父类到子类变更,原生异常类到 自定义异常类的变更,代码重构复用功能相似代码 导致的类型变更等。描述信息变更其次,其原因包 括修正文字错误、增加描述信息、代码重构导致使\
    \ 用封装代码的描述文字等。而异常前断言更改的数 量相对较多,包括增删路径上的前置异常导致条件 变化,代码重构导致的条件变更等。\n\n![](_page_13_Figure_8.jpeg)\n\
    \n(**a**)异常实例变更类型统计\n\n![](_page_13_Figure_10.jpeg)\n\n# (**b**)独立异常实例变更类型统计 图\
    \ **7** 异常变更情况统计\n\n| 项目名         | #异常    | #异常<br>新增 | #异常<br>删除 | #异常<br>类型<br>修改\
    \ | #异常<br>描述<br>修改 | #异常<br>条件<br>修改 |\n|-------------|--------|-----------|-----------|-----------------|-----------------|-----------------|\n\
    | Commons IO  | 746    | 148       | 160       | 6               | 135       \
    \      | 200             |\n| JGraphT     | 979    | 87        | 51        | 1\
    \               | 55              | 130             |\n| GraphStream | 485   \
    \ | 58        | 55        | 1               | 31              | 39           \
    \   |\n| Guava       | 453    | 46        | 56        | 2               | 28 \
    \             | 140             |\n| Nashorn     | 1,332  | 72        | 32   \
    \     | 11              | 6               | 269             |\n| Android     |\
    \ 11,471 | 1,669     | 901       | 21              | 310             | 3,039 \
    \          |\n\n#### 表 **8** 不同项目中独立异常变更统计\n\n**RQ3** 结论:在代码演化过程中,不仅 API 的新 增、删除行为较为常见,异常的新增、删除和更\
    \ 改行为也十分频繁。在 75,433 个被分析的 API 中,约有 20% API 的异常抛出行为至少发生过 一次改变,这些 API 共涉及超过七千多处独立\
    \ 的异常变更。相比于 API 的存在性生命周期模 型,采用异常敏感的分析时,API 发生变动的比 例提高了 18%,该模型能够更加精准地描述 API 的实际变更情况,对框架/库代码的开发者和使\
    \ 用者都具有指导意义。\n\n# **5.2.4.** 对实验有效性的威胁\n\n在本文中,对实验有效性的威胁主要与数据集 的构建与选取有关。1)在手工基准测试集的构建阶\
    \ 段,设计思路的不同会在一定程度上影响在该数据 集上的评估结果,这一偏差难以避免。为了保障测 试集的公平性,本文在构建手工基准测试集时预先 对抛出异常的基本场景进行分类,再按照类别设计\
    \ 数据集代码。对于这些场景,该数据集仅考虑具有 指定特性的精简代码片段,用于测试异常分析工具 在具有不同特性代码上的基础分析能力。我们注意 到 WIT 不支持一些常见的语法特性,为保障公平\
    \ 性,我们仅根据数据集设计需要设计不同特性的代 码片段,而未在设计后主动添加移除特定工具(如 WIT)不支持的语法特性。2)在真实项目数据集的 选取上,真实项目的异常抛出代码风格、函数封装\
    \ 复杂度、相邻版本代码变更差异等均会影响评估结 果,为增强被测项目的代表性,本文复用已有的异 常断言标注集合[34]用于异常分析能力评估,并选 取了六个广泛使用且具有多个版本的\
    \ Java 开源项 目用于 API 变更分析评估,通过工具在不同被测项 目上的整体结果评估工具的综合分析能力。\n\n# 6. 相关工作\n\n## **6.1.**\
    \ 异常摘要提取\n\n异常(Exception)机制是 Java 中正式的错误报 告机制,为了能够及时有效地处理程序中的运行错 误,开发者需合理地抛出、捕获并处理异常。程序\
    \ 崩溃时,打印的异常堆栈信息是错误调试的一类重 要信息[\\[57\\]](#page-17-13)。由于异常机制的复杂性,研究人员围绕 着异常的使用[\\\
    [38\\]](#page-17-14)、异常抛出代码的编程指导 [\\[39\\]\\[40\\]](#page-17-15)、异常抛出行为的正确性测试[\\\
    [37\\]\\[56\\]](#page-17-16)、以及 基 于 程 序 异 常 抛 出 信 息 的 错 误 定 位 与 修 复\n\n[\\[47\\\
    ]\\[46\\]](#page-17-17)[\\[48\\]](#page-17-18)等方向开展了一系列工作。其中,为了 帮助开发人员了解代码中何时何处会抛出异常,理\
    \ 解程序的规范行为,异常的摘要信息,特别是其前 断言生成工作也受到了广泛的关注,主要类别包括 基于自然语言处理的方法和基于代码静态分析的 方法。\n\n\
    # **6.1.1** 基于自然语言处理的断言提取\n\n在开发过程中,断言信息可以帮助开发人员明 确方法的使用规范,避免API演化导致的代码缺陷; 在代码缺陷检测和定位时,前断言分析结果可以辅\
    \ 助测试人员构造高质量的测试用例,对满足/不满足 断言的行为进行系统地测试。\n\n基于自然语言处理(NLP)的断言生成方法被 广泛地应用,这类方法通过统计分析文档、注释等\
    \ 文 本 文 件 推 断 方 法 的 断 言 或 测 试 预 言 信 息 [\\[5\\]\\[20\\]](#page-16-4)[\\[21\\]\\\
    [22\\]](#page-16-13) [\\[23\\]\\[24\\]](#page-16-14)。Tan 等人在@Tcommen[t \\[1\\\
    ]](#page-16-0) 中通过定义自然语言模式和使用启发式方法来推 断程序的异常前断言,该方法仅关注空指针类型。 与之相比,Goff 等人提出的 ToraDoc[u\
    \ \\[21\\]](#page-16-13) 通过解 析 Javadoc 文档,自动为所有的异常行为构造测试 预言,工作 JDoctor [\\[22\\\
    ]](#page-16-15) 在此基础上扩展,实现了 面向更多程序行为的断言提取。此外,Zhai 等人提 出了从文档中自动生成 JML 规范的方法 C2[S\\\
    [20\\]](#page-16-16)。 由于大部分真实应用并不存在完整的 JML 规范,该 方法仅基于 JDK 的规范文档进行训练,其模型不一 定适用于其它的\
    \ Java 项目。基于自然语言处理的方 法可以有效基于文本分析实现断言提取,但无论是 方法文档或是代码注释,开发者对它们的编写情况 都是不确定的。代码中的文档、注释信息既可能缺\
    \ 失,也可能在代码演化过程中未被及时更新,因此, 这类方法适用于文档编写较为规范且被长期维护 的大型项目。但在大量真实项目中,存在着文档缺 失、不完整或未被及时维护的现象[\\\
    [25\\]\\[35\\]](#page-16-17)[\\[36\\]](#page-17-19),无 法准确体现 API 代码实现本身的演化情况。\n\
    \n# **6.1.2** 基于代码分析的断言提取\n\n另一类方法基于静态代码分析来提取断言信 息[\\[52\\]](#page-17-20)。Buse\
    \ 和 Weimer 基于 Java 异常分析工具 Jex [\\[30\\]](#page-16-18) 提出了一种自动推断 Java 方法异常抛出条件\
    \ 的方法 [\\[29\\]](#page-16-19)。该工作首先提取方法和异常的映射 表,然后采用符号执行和跨过程数据流分析技术提 取每个异常的抛出条件。由于该工作后向遍历了所\
    \ 有的控制流路径(control flow path),在单个方法 代码复杂、异常抛出前存在分支条件较多的情况下 会出现路径爆炸问题;收集到路径约束后,该方法\n\
    \n设计了一些约束处理规则以简化断言形式,但其生 成的结果均是围绕所有程序变量的,而不是只关注 异常和方法输入参数的关系。与之相似,Chandra 等 人也采用后向符号执行技术,提出了一种推断最弱\
    \ 前断言的技术 SnuggleBu[g \\[31\\]](#page-16-20),它将问题泛化为如 何找到从某入口点到达目标状态的前置条件,因此 该方法不限于异常分析。为了解决\
    \ Java 多态虚函数 调用关系分析带来的路径爆炸问题,该工作采用符 号执行和函数调用图交错的按需分析方法以提高 效率。但它们[\\[29\\]](#page-16-19)\
    \ [\\[31\\]](#page-16-20)均未提供可公开获取的工具。\n\n近期,Marcilio 等人[\\[26\\]](#page-16-10)\
    \ 提出了基于 Java 源码 分析的轻量级异常前断言分析方法 WIT。该方法可 以有效提取部分 Java 方法断言,但由于面向源代 码,其分析而受限于复杂的语法特性,如不能处理\
    \ 包含 for-each 循环语句, switch 语句, 和 try/catch 块的代码;此外,基于源码的分析依赖于变量名称 的匹配,在变量重新赋值时难以准确解析条件变量\
    \ 和输入参数的关系。WIT 项目的源码未公开,但工 具可公开获取。\n\n在基于代码分析的断言提取方法中,于源码的 分析可以有效提取部分方法的断言,但其受限于复\
    \ 杂且不断更新的 Java 语法特性、难以准确追踪内部 变量和外部参数之间的复杂关系[\\[26\\]](#page-16-10)。此外,对于上 层应用依赖的底层框架和第三方库,其源代码未必\
    \ 是可获取的。与之相比,基于字节码的分析不会受 限于高级语言的语法特性,并支持开展精确的控制 流和数据流追踪。为增加方法的普适性,适应不同 版本的 Java\
    \ 代码,并支撑框架/库源码不可获取的 分析场景,JavaExP 向 Java 字节码的静态分析技术, 通过追踪分析字节码中的异常抛出条件和变量取 值,实现异常相关的\
    \ API 语义变更分析。\n\n### **6.2. API**生命周期模型构建\n\nAPI 生命周期模型常被用于上层应用的 API 误 用检测或兼容性错误检测。Li\
    \ 等人在工作 CiD [\\[5\\]](#page-16-4) 中提出了安卓生命周期模型 (ALM),CiD 从安卓 开发框架中提取了完整的 API 方法列表,并给出不\
    \ 同 API 存在的版本范围。Huang 等人提出了 CIDE[R\\[8\\],](#page-16-21)该工作关注 API 回调函数变化导致的兼 容性问题,该工作依赖于手工构建的回调函数调用\
    \ 协议一致性图。工作 ACI[D\\[7\\]](#page-16-22)同时关注 API 调用问 题和 API 回调函数兼容性问题,该工作没有分析框 架代码,而是根据安卓框架官方提供的\
    \ API 差异列 表轻量级地获取其生命周期。本文也关注与框架 API 生命周期的提取,与这些工作相比,我们不仅 关注 API 的存在性问题,即在不同版本中\
    \ API 的新 增和删除情况,还重点分析了API中异常抛出情况, 特别是同一个 API 中异常抛出条件、描述、类型等 是否发生变化。\n\n除了 API\
    \ 方法的演化,工作[\\[11\\]](#page-16-23)还关注了框架 代码中字段(Field)信息的演化,并关注了字段变 化引发的上层代码缺陷。更多的工作[\\\
    [6\\]\\[9\\]](#page-16-5)[\\[10\\]](#page-16-24)关注 与在给定 API 生命周期模型的基础上,如何精确分 析上层应用代码,以找到\
    \ API 的误用问题,我们的 模型提取工作可以为这类研究提供支撑。\n\n# 7. 总结与展望\n\n针对框架/库项目和上层应用开发者在代码升 级演化过程中难以准确获取其开发或使用的API变\
    \ 更行为这一问题,本文基于静态分析方法,提出了 面向底层框架和第三方库的异常信息敏感的API生 命周期模型生成方法,形成原型工具 JavaExP。与 已有工作相比,JavaExP\
    \ 生成的异常摘要信息在准 确率和分析效率方面均有大幅提高。与异常不敏感 的 API 演化分析相比,异常敏感的 API 发生变动的 比例提高了 18%,在六个真实框架/库项目的\
    \ 60 个 版本中发现了超过七千多处独立的异常变更。\n\n这一工具可同时服务于框架/库的开发人员和 使用人员。一方面,对于框架/库的开发者,应在发 布新版本软件前,通过\
    \ API 生命周期分析工具精确 获取新版本代码中 API 中异常信息变更情况,确保 小版本升级时不产生 API 语义变更,大版本升级时 及时将语义变更情况更新在文档中。另一方面,对\
    \ 于应用开发者,在对所使用的框架/库代码进行版本 升级时,可通过分析工具查看当前版本到新版本中 API 的方法变更和异常变更情况,开展未捕获的异 常分析和异常传播分析等应用层检测,保障应用层\
    \ API 调用的正确性和鲁棒性,服务于软件供应链安 全分析。此外,对于基于大模型的自动代码生成, 框架/库 API 误用是生成代码中的一种典型错误模 式,API\
    \ 生命周期信息对于生成代码的版本一致性 检测和修复也有重要意义。\n\n考虑到真实的大规模框架/库中异常信息变动 非常频繁,在后续的研究工作中,我们将进一步探\
    \ 索如何从大量的异常信息变更中自动识别出可能 影响代码可靠性的语义变化、如何自动构造可触发 API 中的异常抛出行为的测试用例等研究问题,从 而精准定位上层软件系统中的\
    \ API 误用行为。\n\n# 致 谢 诚挚感谢评阅老师对论文提出的改进意见!\n\n### 参考文献:\n\n<span id=\"page-16-0\"\
    ></span>[1] Dependabot. https://docs.github.com/en/code-security/dependabot/ dependabot-alerts\n\
    \n<span id=\"page-16-1\"></span>[2] Hora, André, Romain Robbes, Marco Tulio Valente,\
    \ Nicolas Anquetil, Anne Etien, and Stéphane Ducasse. How do developers react\
    \ to API evolution? A large-scale empirical study. Software Quality Journal 26\
    \ (2018): 161-191.\n\n[3] Wu, Wei, Foutse Khomh, Bram Adams, Yann-Gaël Guéhéneuc,\
    \ and Giuliano Antoniol. An exploratory study of api changes and usages based\
    \ on apache and eclipse ecosystems. Empirical Software Engineering 21 (2016):\
    \ 2366-2412.\n\n<span id=\"page-16-2\"></span>[4] Bavota, Gabriele, Mario Linares-Vasquez,\
    \ Carlos Eduardo Bernal-Cardenas, Massimiliano Di Penta, Rocco Oliveto, and Denys\
    \ Poshyvanyk. The impact of api change-and fault-proneness on the user ratings\
    \ of android apps. IEEE Transactions on Software Engineering 41, no. 4 (2014):\
    \ 384-407.\n\n<span id=\"page-16-4\"></span>[5] Li Li, Tegawendé F Bissyandé,\
    \ Haoyu Wang, and Jacques Klein. 2018. Cid: Automating the detection of api-related\
    \ compatibility issues in android apps. https://github.com/lilicoding/CiD. In\
    \ Proceedings ofthe 27th ACM SIGSOFT Inter- national Symposium on Software Testing\
    \ and Analysis. 153–163.\n\n<span id=\"page-16-5\"></span>[6] Dongjie He, Lian\
    \ Li, Lei Wang, Hengjie Zheng, Guangwei Li, and Jingling Xue. 2018. Understanding\
    \ and detecting evolution-induced compatibility issues in android apps. In 2018\
    \ 33rd IEEE/ACM International Conference on Automated Software Engineering (ASE).\
    \ IEEE, 167–177.\n\n<span id=\"page-16-22\"></span>[7] Tarek Mahmud, Meiru Che,\
    \ and Guowei Yang. 2021. Android compatibility issue detection using api differences.\
    \ In 2021 IEEE International Conference on Software Analysis, Evolution and Reengineering\
    \ (SANER). IEEE, 480–490.\n\n<span id=\"page-16-21\"></span>[8] Huaxun Huang,\
    \ Lili Wei, Yepang Liu, and Shing-Chi Cheung. 2018. Under- standing and detecting\
    \ callback compatibility issues for android applications. In Proceedings ofthe\
    \ 33rdACM/IEEE International Conference on Automated Software Engineering. 532ś542.\n\
    \n[9] Patrick Mutchler, Yeganeh Safaei, Adam Doupé, and John Mitchell. 2016. Target\
    \ fragmentation in Android apps. In 2016 IEEE Security and Privacy Workshops (SPW).\
    \ IEEE, 204–213.\n\n<span id=\"page-16-24\"></span>[10] Yang Sen, Sen Chen, Lingling\
    \ Fan, Sihan Xu, Zhanwei Hui, and Song Huang. Compatibility Issue Detection for\
    \ Android Apps Based on Path-Sensitive Semantic Analysis. In 2023 IEEE/ACM 45th\
    \ International Conference on Software Engineering (ICSE), pp. 257-269. IEEE,\
    \ 2023.\n\n<span id=\"page-16-23\"></span>[11] Mahmud Tarek, Meiru Che, and Guowei\
    \ Yang. Android api field evolution and its induced compatibility issues. Proceedings\
    \ of the 16th ACM/IEEE International Symposium on Empirical Software Engineering\
    \ and Measurement. 2022.\n\n<span id=\"page-16-6\"></span>[12] 新增异常实例。[https://github.com/apache/commons-](https://github.com/apache/commons-io/commit/e03d721bb4dfd2e4ba7719f76ce17e860890dd2e)\n\
    \n[io/commit/e03d721bb4dfd2e4ba7719f76ce17e860890dd2e](https://github.com/apache/commons-io/commit/e03d721bb4dfd2e4ba7719f76ce17e860890dd2e)\n\
    \n[13] 删除异常实例。[https://github.com/aosp-](https://github.com/aosp-mirror/platform_frameworks_base/commit/8b73d86492c3bcd2fbca6545b89c159ace637b72)\n\
    \n[mirror/platform\\\\_frameworks\\\\_base/commit/8b73d86492c3bcd2fbca6545b89](https://github.com/aosp-mirror/platform_frameworks_base/commit/8b73d86492c3bcd2fbca6545b89c159ace637b72)\
    \ [c159ace637b72](https://github.com/aosp-mirror/platform_frameworks_base/commit/8b73d86492c3bcd2fbca6545b89c159ace637b72)\n\
    \n<span id=\"page-16-7\"></span>[14] 修改异常类型。[https://github.com/apache/commons](https://github.com/apache/commons-io/commit/af9bf289fbd7f6294ca399c54f4277e8631e903a)[io/commit/af9bf289fbd7f6294ca399c54f4277e8631e903a](https://github.com/apache/commons-io/commit/af9bf289fbd7f6294ca399c54f4277e8631e903a)\n\
    \n<span id=\"page-16-8\"></span>[15] 修改异常抛出条件。[https://github.com/apache/commons-](https://github.com/apache/commons-io/commit/50f9c9370c1286039fb6750e08e0fcbc20c6adc0)\n\
    \n[io/commit/50f9c9370c1286039fb6750e08e0fcbc20c6adc0](https://github.com/apache/commons-io/commit/50f9c9370c1286039fb6750e08e0fcbc20c6adc0)\n\
    \n<span id=\"page-16-9\"></span>[16] 修改异常描述文本。[https://github.com/apache/commons-](https://github.com/apache/commons-io/commit/8814b6d7efa235fc1410c9699eb136da780d70a2)\n\
    \n[io/commit/8814b6d7efa235fc1410c9699eb136da780d70a2](https://github.com/apache/commons-io/commit/8814b6d7efa235fc1410c9699eb136da780d70a2)\n\
    \n<span id=\"page-16-12\"></span>[17] 移动异常位置。[https://github.com/apache/commons-](https://github.com/apache/commons-io/commit/2d37ab6a0d53fce60095b352d36dddd9d26f5ab0)\n\
    \n[io/commit/2d37ab6a0d53fce60095b352d36dddd9d26f5ab0](https://github.com/apache/commons-io/commit/2d37ab6a0d53fce60095b352d36dddd9d26f5ab0)\n\
    \n<span id=\"page-16-3\"></span>[18] Yu, Dung-Feng, Cheng-Ying Chang, Hewijin\
    \ Christine Jiau, and Kuo-Feng Ssu. Which API Lifecycle Model is the Best for\
    \ API Removal Management?. ICSEA 2017: 230.\n\n[19] Shin Hwei Tan, Darko Marinov,\
    \ Lin Tan, and Gary T. Leavens. @tComment: Testing Javadoc Comments to Detect\
    \ Comment-Code Inconsistencies. ICST 2012: 260–269, 2012.\n\n<span id=\"page-16-16\"\
    ></span>[20] Juan Zhai, Yu Shi, Minxue Pan, Guian Zhou, Yongxiang Liu, Chunrong\
    \ Fang, Shiqing Ma, Lin Tan, and Xiangyu Zhang. 2020. C2S: translating natural\
    \ language comments to formal program specifications. ESEC/FSE 2020: 25-37, 2020.\n\
    \n<span id=\"page-16-13\"></span>[21] Alberto Goffi, Alessandra Gorla, Michael\
    \ D. Ernst, and Mauro Pezzè. 2016. Automatic generation of oracles for exceptional\
    \ behaviors. ISSTA 2016: 213–224, 2016.\n\n<span id=\"page-16-15\"></span>[22]\
    \ Arianna Blasi, Alberto Goffi, Konstantin Kuznetsov, Alessandra Gorla, Michael\
    \ D. Ernst, Mauro Pezzè, and Sergio Delgado Castellanos. Translating code comments\
    \ to procedure specifications. ISSTA 2018:242- 253, 2018.\n\n<span id=\"page-16-14\"\
    ></span>[23] Elizabeth Dinella, Gabriel Ryan, Todd Mytkowicz, and Shuvendu K.\
    \ Lahiri. 2022. TOGA: a neural method for test oracle generation. In Proceedings\
    \ of the 44th International Conference on Software Engineering (ICSE '22): 2130–2141,\
    \ 2022.\n\n[24] Rahul Pandita, Xusheng Xiao, Hao Zhong, Tao Xie, Stephen Oney,\
    \ and Amit Paradkar. Inferring method specifications from natural language api\
    \ descriptions. ICSE 2012: 815–825, 2012.\n\n<span id=\"page-16-17\"></span>[25]\
    \ Zhong, Hao, Na Meng, Zexuan Li, and Li Jia. An empirical study on API parameter\
    \ rules. ICSE 2020:899-911, 2020.\n\n<span id=\"page-16-10\"></span>[26] Diego\
    \ Marcilio and Carlo A. Furia, What Is Thrown? Lightweight Precise Automatic Extraction\
    \ of Exception Preconditions in Java Methods. ICSME 2022:340-35, 2022.\n\n[27]\
    \ Raymond P. L. Buse and Westley Weimer. Automatic documentation inference for\
    \ exceptions. ISSTA 2008: 273–282. 2008.\n\n[28] Yu Zhou, Changzhi Wang, Xin Yan,\
    \ Taolue Chen, Sebastiano Panichella, and Harald Gall. Automatic detection and\
    \ repair recommendation of directive defects in java api documentation. IEEE Transactions\
    \ on Software Engineering, 46(9):1004–1023, 2020.\n\n<span id=\"page-16-19\"></span>[29]\
    \ Raymond P. L. Buse and Westley Weimer. Automatic documentation inference for\
    \ exceptions. ISSTA 2008: 273–282. 2008.\n\n<span id=\"page-16-18\"></span>[30]\
    \ Martin P. Robillard and Gail C. Murphy. Static analysis to support the evolution\
    \ of exception structure in object-oriented systems. ACM Trans. Softw. Eng. Methodol.\
    \ 12(2):191–221, 2003.\n\n<span id=\"page-16-20\"></span>[31] Satish Chandra,\
    \ Stephen J. Fink, and Manu Sridharan. Snugglebug: a powerful approach to weakest\
    \ preconditions. PLDI 2009: 363–374. 2009.\n\n<span id=\"page-16-11\"></span>[32]\
    \ Jeanne Ferrante, Karl J. Ottenstein, and Joe D. Warren. The program dependence\
    \ graph and its use in optimization. ACM Trans. Program. Lang. Syst. 9, 3, 319–349,\
    \ 1987.\n\n[33] Cytron, Ron, Jeanne Ferrante, Barry K. Rosen, Mark N. Wegman,\
    \ and F. Kenneth Zadeck. Efficiently computing static single assignment form and\
    \ the control dependence graph. ACM Transactions on Programming Languages and\
    \ Systems (TOPLAS) 13, no. 4 (1991): 451-490.\n\n<span id=\"page-17-10\"></span>[34]\
    \ Nassif, Mathieu, Alexa Hernandez, Ashvitha Sridharan, and Martin P. Robillard.\
    \ Generating unit tests for documentation. IEEE Transactions on Software Engineering\
    \ 48, no. 9 (2021): 3268-3279.\n\n<span id=\"page-17-19\"></span>[35] Emad Aghajani,\
    \ Csaba Nagy, Olga Lucero Vega-Márquez, Mario Linares-Vásquez, Laura Moreno, Gabriele\
    \ Bavota, and Michele Lanza. Software documentation issues unveiled. In Proceedings\
    \ of the 41st International Conference on Software Engineering, ICSE, 1199–1210,\
    \ 2019. [36] Emad Aghajani, Csaba Nagy, Mario Linares-Vásquez, Laura Moreno, Gabriele\
    \ Bavota, Michele Lanza, and David C. Shepherd. Software documentation: the practitioners'\
    \ perspective. In Proceedings of the ACM/IEEE 42nd International Conference on\
    \ Software Engineering (ICSE '20): 590–601, 2020.\n\n<span id=\"page-17-16\"></span>[37]\
    \ Francisco Dalton, Márcio Ribeiro, Gustavo Pinto, Leo Fernandes, Rohit Gheyi,\
    \ and Baldoino Fonseca. 2020. Is Exceptional Behavior Testing an Exception? An\
    \ Empirical Assessment Using Java Automated Tests. In Proceedings of the 24th\
    \ International Conference on Evaluation and Assessment in Software Engineering\
    \ (EASE '20). 170–179, 2020.\n\n<span id=\"page-17-14\"></span>[38] Haidar Osman,\
    \ Andrei Chiş, Jakob Schaerer, Mohammad Ghafari, and Oscar Nierstrasz. On the\
    \ evolution of exception usage in Java projects, 2017 IEEE 24th International\
    \ Conference on Software Analysis, Evolution and Reengineering (SANER), pp. 422-426,\
    \ 2017.\n\n<span id=\"page-17-15\"></span>[39] Xiangyang Jia, Songqiang Chen,\
    \ Xingqi Zhou, Xintong Li, Run Yu, Xu Chen, Jifeng Xuan. Where to Handle an Exception?\
    \ Recommending Exception Handling Locations from a Global Perspective, 2021 IEEE/ACM\
    \ 29th International Conference on Program Comprehension (ICPC), pp. 369- 380,\
    \ 2021.\n\n[40] Hao Zhong. 2023. Which Exception Shall We Throw? In Proceedings\
    \ of the 37th IEEE/ACM International Conference on Automated Software Engineering\
    \ (ASE '22). Article 116, 1–12, 2022.\n\n<span id=\"page-17-0\"></span>[41] Exception\
    \ handling. Wikipedia. 2023.\n\n[https://en.wikipedia.org/wiki/Exception\\\\_handling](https://en.wikipedia.org/wiki/Exception_handling)\n\
    \n<span id=\"page-17-4\"></span>[42] Java Exception, Oracle.\n\n<https://docs.oracle.com/javase/8/docs/api/java/lang/Exception.html>\n\
    \n<span id=\"page-17-5\"></span>[43] Eckel, Bruce, and 侯捷. Java 编程思想. Vol. 2.\
    \ No. 02. 机械工业 出版社, 2002.\n\n<span id=\"page-17-6\"></span>[44] Soot. 2023[. https://github.com/soot-oss/soot](https://github.com/soot-oss/soot)\n\
    \n<span id=\"page-17-7\"></span>[45] Use-define chain. 2023[. https://en.wikipedia.org/wiki/Use](https://en.wikipedia.org/wiki/Use-define_chain)[define\\\
    \\_chain.](https://en.wikipedia.org/wiki/Use-define_chain)\n\n<span id=\"page-17-3\"\
    ></span>[46] JavaExP, 2023[. https://github.com/hanada31/JavaExP](https://github.com/hanada31/JavaExP)\n\
    \n<span id=\"page-17-17\"></span>[47] Jiwei Yan, Miaomiao Wang, Yepang Liu, Jun\
    \ Yan, Long Zhang. Locating Framework-specific Crashing Faults with Compact and\
    \ Explainable Candidate Set. The 45th IEEE/ACM International Conference on Software\
    \ Engineering, ICSE, 2023.\n\n<span id=\"page-17-18\"></span>[48] Rongxin Wu,\
    \ Hongyu Zhang, Shing-Chi Cheung, and Sunghun Kim. CrashLocator: locating crashing\
    \ faults based on crash stacks. In Proceedings of the 2014 International Symposium\
    \ on Software Testing and Analysis, ISSTA 2014. 204–214,2014.\n\n<span id=\"page-17-9\"\
    ></span>[49] Apache Commons IO, 2023. <https://github.com/apache/commons-io>\n\
    \n[50] Leonardo Mendonça de Moura and Nikolaj Bjørner. Z3: an efficient SMT solver.\
    \ In C. R. Ramakrishnan and Jakob Rehof, editors, Tools and Algorithms for the\
    \ Construction and Analysis of Systems, 14th International Conference, TACAS 2008\
    \ , pages 337–340. 2008.\n\n<span id=\"page-17-8\"></span>[51] ExceptionBench,\n\
    \n[https://github.com/hanada31/JavaExP/tree/master/Evaluation/RQ1/Exceptio](https://github.com/hanada31/JavaExP/tree/master/Evaluation/RQ1/ExceptionBench%20Code)\
    \ [nBench%20Code](https://github.com/hanada31/JavaExP/tree/master/Evaluation/RQ1/ExceptionBench%20Code)\n\
    \n<span id=\"page-17-20\"></span>[52] Rak-Amnouykit, Ingkarat, et al. The raise\
    \ of machine learning hyperparameter constraints in Python code. Proceedings of\
    \ the 31st ACM SIGSOFT International Symposium on Software Testing and Analysis.\
    \ 2022. [53] Maven 仓库。<https://mvnrepository.com/>\n\n<span id=\"page-17-12\"\
    ></span><span id=\"page-17-11\"></span>[54] Blasi, Arianna, Alessandra Gorla,\
    \ Michael D. Ernst, and Mauro Pezzè. \"Call Me Maybe: Using NLP to Automatically\
    \ Generate Unit Test Cases Respecting Temporal Constraints.\" In Proceedings of\
    \ the 37th IEEE/ACM International Conference on Automated Software Engineering,\
    \ pp. 1-11. 2022.\n\n<span id=\"page-17-1\"></span>[55] Shaikh Mostafa, Rodney\
    \ Rodriguez, and Xiaoyin Wang. 2017. Experience paper: a study on behavioral backward\
    \ incompatibilities of Java software libraries. In Proceedings of the 26th ACM\
    \ SIGSOFT International Symposium on Software Testing and Analysis (ISSTA 2017).\
    \ Association for Computing Machinery, New York, NY, USA, 215–225.\n\n### 附中文参考文献:\n\
    \n[56] 姜淑娟,闫大顺. 一种快速测试Java异常处理机制的方法[J]. 小型 微型计算机系统,2005,26(10):1854-1857.\n\n\
    <span id=\"page-17-13\"></span>[57] 顾咏丰, 马萍, 贾向阳, 等. 软件崩溃研究进展. 中国科学: 信息 科学, 2019,\
    \ 49: 1383–1398.\n\n<span id=\"page-17-2\"></span>[58] 葛丽丽, 帅东昕, 谢金言, 张迎周, 薛渝川,\
    \ 杨嘉毅, 密杰, 卢跃. 面向软件供应链的异常分析方法综述. 软 件学报, 2023, 34(6): 2606– 2627."
  decisions:
    language: 'Disqualified: Not English. Reason: Paper is not primarily written in
      English.'
- title: What Is an App Store? The Software Engineering Perspective
  abstract: "\"App stores\" are online software stores where end users may browse,\
    \ purchase,\ndownload, and install software applications. By far, the best known\
    \ app stores\nare associated with mobile platforms, such as Google Play for Android\
    \ and\nApple's App Store for iOS. The ubiquity of smartphones has led to mobile\
    \ app\nstores becoming a touchstone experience of modern living. However, most\
    \ of app\nstore research has concentrated on properties of the apps rather than\
    \ the\nstores themselves. Today, there is a rich diversity of app stores and these\n\
    stores have largely been overlooked by researchers: app stores exist on many\n\
    distinctive platforms, are aimed at different classes of users, and have\ndifferent\
    \ end-goals beyond simply selling a standalone app to a smartphone\nuser.\n  We\
    \ survey and characterize the broader dimensionality of app stores, and\nexplore\
    \ how and why they influence software development practices, such as\nsystem design\
    \ and release management. We begin by collecting a set of app store\nexamples\
    \ from web search queries. By analyzing and curating the results, we\nderive a\
    \ set of features common to app stores. We then build a dimensional\nmodel of\
    \ app stores based on these features, and we fit each app store from our\nweb\
    \ search result set into this model. Next, we performed unsupervised\nclustering\
    \ to the app stores to find their natural groupings. Our results\nsuggest that\
    \ app stores have become an essential stakeholder in modern software\ndevelopment.\
    \ They control the distribution channel to end users and ensure that\nthe applications\
    \ are of suitable quality; in turn, this leads to developers\nadhering to various\
    \ store guidelines when creating their applications. However,\nwe found the app\
    \ stores operational model could vary widely between stores, and\nthis variability\
    \ could in turn affect the generalizability of existing\nunderstanding of app\
    \ stores."
  url: http://arxiv.org/abs/2401.04287v1
  keywords: ''
  document: "# What is an App Store? The Software Engineering Perspective\n\nWenhan\
    \ Zhu · Sebastian Proksch · Daniel M. German · Michael W. Godfrey · Li Li · Shane\
    \ McIntosh\n\nAuthor pre-print copy. The final publication is available at Springer\
    \ via: https://doi.org/ 10.1007/s10664-023-10362-3\n\nAbstract \"App stores\"\
    \ are online software stores where end users may browse, purchase, download, and\
    \ install software applications. By far, the best known app stores are associated\
    \ with mobile platforms, such as Google Play for Android and Apple's App Store\
    \ for iOS. The ubiquity of smartphones has led to mobile app stores becoming a\
    \ touchstone experience of modern living. App stores have been the subject of\
    \ many empirical studies. However, most of this research has concentrated on properties\
    \ of the apps rather than the stores themselves. Today, there is a rich diversity\
    \ of app stores and these stores have largely been overlooked by researchers:\
    \ app stores exist on many distinctive platforms, are aimed at different classes\
    \ of users, and have different end-goals beyond simply selling a standalone app\
    \ to a smartphone user.\n\nThe goal of this paper is to survey and characterize\
    \ the broader dimensionality of app stores, and to explore how and why they influence\
    \ software development practices, such as system design and release management.\
    \ We begin by collecting a set of app store examples from web search queries.\
    \ By analyzing and curating the results, we derive a set of features common to\
    \ app stores. We then build a dimensional model of app stores based on these features,\
    \ and we fit each app store from our web search result set into this model.\n\n\
    Wenhan Zhu · Michael W. Godfrey · Shane McIntosh\n\nSebastian Proksch Delft University\
    \ of Technology, Delft, Netherlands E-mail: s.proksch@tudelft.nl\n\nSchool of\
    \ Software, Beihang University, Beijing, China\n\nDaniel M. German Department\
    \ of Computer Science, University of Victoria, Victoria, Canada E-mail: dmg@uvic.ca\n\
    \nLi Li\n\nE-mail: lilicoding@ieee.org\n\narXiv:2401.04287v1 [cs.SE] 8 Jan 2024\n\
    \nDavid R. Cheriton School of Computer Science, University of waterloo, Waterloo,\
    \ Canada E-mail: {w65zhu, migod, shane.mcintosh}@uwaterloo.ca\n\nNext, we performed\
    \ unsupervised clustering to the app stores to find their natural groupings. Our\
    \ results suggest that app stores have become an essential stakeholder in modern\
    \ software development. They control the distribution channel to end users and\
    \ ensure that the applications are of suitable quality; in turn, this leads to\
    \ developers adhering to various store guidelines when creating their applications.\
    \ However, we found the app stores' operational model could vary widely between\
    \ stores, and this variability could in turn affect the generalizability of existing\
    \ understanding of app stores.\n\nKeywords app store, software release, software\
    \ distribution, empirical software engineering\n\n# 1 Introduction\n\nThe widespread\
    \ proliferation of smartphones and other mobile devices in recent years has in\
    \ turn produced an immense demand for applications that run on these platforms.\
    \ In response, online \"app stores\" such as Google Play and Apple's App Store\
    \ have emerged to facilitate the discovery, purchasing, installation, and management\
    \ of apps by users on their mobile devices. The success of mobile app stores has\
    \ enabled a new and more direct relationship between app creators and users. The\
    \ app store serves as a conduit between software creators (often, developers)\
    \ and their users, with some mediation provided by the app store. The app store\
    \ provides a \"one-stop shopping\" experience for users, who can compare competing\
    \ products and read reviews of other users. The app store might also acts as a\
    \ quality gatekeeper for the platform, providing varying levels of guarantees\
    \ about the apps, such as easy installation and removal, expected functionality,\
    \ and malware protection. To the software creator, the app store provides a centralized\
    \ marketplace for their app, where potential users can find, purchase, and acquire\
    \ the app easily; the app store also relieves the developer from basic support\
    \ problems related to distribution and installation, since apps must be shown\
    \ to install easily during the required approval process. Indeed, one of the key\
    \ side effects of mobile app stores is that it has forced software developers\
    \ to streamline their release management practices and ensure hassle-free deployment\
    \ at the user's end.\n\nThe success of mobile app stores has also led to the establishment\
    \ of a plethora of other kinds of app store, often for non-mobile platforms, serving\
    \ diverse kinds of user communities, offering different kinds of services, and\
    \ using a variety of monetization strategies. Many technical platforms now operate\
    \ in a store-centric way: essential services and functionality are provided by\
    \ the platform while access to extensions/add-ons is offered only through interaction\
    \ with the app store. For instance, Google Play, the app store, operates on top\
    \ of the technical platform Android, which provides the runtime environment for\
    \ the applications. When new technical platforms are introduced, an app store\
    \ is often expected to serve as a means to host and deliver products to its users\
    \ [1]. Example technical platforms that use app store-like approaches\n\ninclude\
    \ Steam [2], GitHub Marketplace [3], the Chrome Web Store [4], Word-Press [5],\
    \ AutoDesk [6], DockerHub [7], Amazon Web Services (AWS) [8], Homebrew [9], or\
    \ Ubuntu Packages [10].\n\nFor platforms that operate in this way, the app store\
    \ is an essential part of the platform's design. For example, consider source\
    \ code editors, such as VSCode and IntelliJ. The tool itself — which we consider\
    \ to be a technical platform in this context — offers the essential functionality\
    \ of a modern source code editor; however, many additional services are available\
    \ through the associated app store that are not included by default. Thus, extensions\
    \ that allow for language-specific syntax highlighting or version control integration\
    \ must be added manually by the user through interaction with the tool's app store.\
    \ We conjecture that the app store has fundamentally changed how some classes\
    \ of software systems are designed, from the overall ecosystem architecture of\
    \ the technical platform to the way in which add-ons are engineered to fit within\
    \ its instances.\n\nIn this work, we will explore the general space of app stores,\
    \ and also consider how app store-centric design can affect software development\
    \ practices. Previous research involving app stores has focused mainly on mobile\
    \ app stores, often concentrating on properties of the apps rather than properties\
    \ of the stores. For example, Harman et al. performed one of the first major studies\
    \ of app stores in 2012, focusing on the BlackBerry App World [11]. However, concentrating\
    \ the investigative scope so narrowly may lead to claims that do not generalize\
    \ well across the space of all app stores. For example, Lin et al. found that\
    \ reviews of games that appeared in mobile app stores differed significantly from\
    \ the reviews of the same game that appeared within the Steam platform's own app\
    \ store [12]. In our work, we aim to take a more holistic approach to studying\
    \ app stores by considering both mobile and non-mobile variants. In so doing,\
    \ we hope to create a more general model of app stores that fits this broader\
    \ space.\n\nTo achieve a holistic view, we start from the definition of an app\
    \ store. A precise definition of the term \"app store\" has been omitted in much\
    \ of the previous research in this area. Currently, Google Play and Apple's App\
    \ Store dominate the market and are the main targets of research on app stores;\
    \ in the past, the BlackBerry App World and Microsoft's Windows Phone Store were\
    \ also important players, but these stores are now defunct.<sup>1</sup> Wikipedia\
    \ recognizes Electronic AppWrapper [13] as the first true platform-specific electronic\
    \ marketplace for software applications, but the term became popular when Apple\
    \ introduced its App Store along with the iPhone 3G in 2008. Since then, the term\
    \ has largely come to refer to any centralized store for mobile applications.\
    \ We present our own working definition of the term \"app store\" in Sec. 2.4.\n\
    \nThe goal of this work is to survey and characterize the broader dimensionality\
    \ of app stores, and also to explore how and why they may feed back into software\
    \ development practices, such as release management. As a step toward\n\n<sup>1</sup>\
    \ The Windows Phone Store was absorbed into the broader Windows Store in 2015.\n\
    \nthis goal, we focus on two research questions (RQs) that aim to explore the\
    \ space of app stores:\n\n# RQ1: What fundamental features describe the space\
    \ of app stores?\n\nTo understand app stores, we first need a way to describe\
    \ them. It would be especially useful if this description framework would highlight\
    \ the similarities and differences of app stores. We start by collecting a set\
    \ of app store examples, and then extract from them a set of features that illustrate\
    \ important differences between them. We then expand this list of app stores with\
    \ search queries to derive a larger set of example stores. We explicitly seek\
    \ generalized web queries to broaden our search space beyond the common two major\
    \ mobile app stores of Apple and Google. By combining the web queries and the\
    \ initial set of app stores, we selected a representative set of app stores and\
    \ extracted their features. In the end, we first surveyed app stores and derived\
    \ a feature-based model to describe them; we then expanded the set of app stores\
    \ through web queries; and finally, we extracted features based on the model for\
    \ a representative set of app stores.\n\n#### RQ2: Are there groups of stores\
    \ that share similar features?\n\nDespite the ability to describe individual stores,\
    \ it is also important to understand the relationships between different stores.\
    \ Having a understanding of the natural groupings can help us gain insights into\
    \ the understanding of the generalizability of results gathered for different\
    \ app stores. We perform a K-means [14] clustering based on the extracted features\
    \ of the expanded set of app stores collected previously. The optimal k value\
    \ is determined by the Silhouette method [15]. The clustering results suggest\
    \ that there are 8 groups in the expanded set of app stores. The differences can\
    \ be observed in the type of application offered, standalone or extension, and/or\
    \ type of operation, business or community-oriented.\n\nIn this study, we make\
    \ several contributions towards a better understanding of the app store ecosystem.\n\
    \n- We identified a set of descriptive features that can be used to characterize\
    \ app stores.\n- We identified a set of 291 app stores and mapped 53 of them into\
    \ the feature space.\n- We identified 8 coherent groups of app stores based on\
    \ the similarity of features.\n- We discuss our insights on how the features and\
    \ the diversity of app stores can impact software engineering practices.\n\nOverall,\
    \ our study contributes towards a holistic view of app stores within software\
    \ engineering, which can form the basis for subsequent study of app stores in\
    \ general.\n\n# 2 Background and Related Work\n\n#### 2.1 Early App Store Research\n\
    \nTo date, research in this area has concentrated on a narrow set of app stores\
    \ that primarily involves mobile platforms. Harman et al. [11] proposed app stores\
    \ as a valid kind of software repository worthy of formal study within the broader\
    \ research area of mining software repositories; while their work was not specific\
    \ to mobile app stores, they used BlackBerry App World as their canonical example.\
    \ Ruiz et al. [16] studied the topic of reuse within app stores, focusing their\
    \ work on Android Marketplace. 2 In both cases, these early works did not provide\
    \ a formal definition of \"app store\", and tacitly used only app stores for mobile\
    \ platforms in their studies.\n\nIn their 2016 survey on app store research, Martin\
    \ et al. [17] observed that studies have often focused on only a few specific\
    \ app stores, and have ignored comparisons between app stores. In a recent literature\
    \ survey, Dąbrowski et al. [18] found the median number of app stores studied\
    \ to be 1, with the maximum being 3. We also note that results from one app store\
    \ study may not generalize to another store since the two stores may differ in\
    \ significant ways; for example, if a store does not allow users to provide their\
    \ own reviews of the apps within the store, app creators will have to rely on\
    \ other means to gain popularity and trust from users, such as promotion outside\
    \ of the app store. The same trend can be observed in more specific app store\
    \ topics such as app reviews; for example, Lin et al. [12] found that reviews\
    \ of games within the Steam app store can be dramatically different from reviews\
    \ of the same game in mobile app stores.\n\nExisting work has yet to explore the\
    \ full diversity of app stores, concentrating on Google Play and Apple's App Store,\
    \ and largely ignoring those such as Steam, AWS, and GitHub Marketplace that are\
    \ not specific to mobile platforms. With the heterogeneity of app stores and their\
    \ typical uses, we believe that the research in this area can be strengthened\
    \ by expanding the breadth to encompass a more diverse perspective on app stores;\
    \ in turn, this breadth can help to validate the generalizability of the study\
    \ findings.\n\n#### 2.2 App Stores in Recent Software Engineering Research\n\n\
    To better understand the involvement of app stores in recent research, we reviewed\
    \ relevant recent papers from the two flagship software engineering research conferences:\
    \ the ACM/IEEE International Conference on Software Engineering (\"ICSE\") and\
    \ the ACM SIGSOFT International Symposium on the Foundations of Software Engineering\
    \ (\"FSE\") We used Google Scholar to find papers containing the keyword \"app\
    \ store\" between January 2020 and April 2022 for the two conferences. We found\
    \ a total of 34 such papers (listed in Table 2.1). After reading through all of\
    \ them, we found that each paper\n\n<sup>2</sup> Android Marketplace has since\
    \ been re-branded as Google Play.\n\nfit into one of two broad categories: mining\
    \ software applications (20/34) and mining app store artifacts (14/34). We note\
    \ that our efforts do not constitute a comprehensive literature survey; instead,\
    \ our goal was to gain an overview of how app stores are involved in recent research,\
    \ and why app stores matter in their context.\n\n| Loc                  | Paper\
    \                                                                            \
    \                                                                       | Store\
    \                                         |\n|----------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------|\n\
    |                      | Mining software applications                        \
    \                                                                            \
    \                        |                                               |\n|\
    \ ICSE '21             | Atvhunter: Reliable version detection of third-party\
    \ libraries for vulnerability<br>identification in android applications [19] \
    \                        | Google Play                                   |\n|\
    \ ICSE '20             | How does misconfiguration of analytic services compromise\
    \ mobile privacy? [20]                                                       \
    \                   | Google Play                                   |\n| FSE '21\
    \              | Algebraic-datatype taint tracking, with applications to understanding\
    \ Android<br>identifier leaks [21]                                           \
    \       | Google Play                                   |\n| FSE '20         \
    \     | Code recommendation for exception handling [22]                      \
    \                                                                            \
    \       | Google Play                                   |\n| FSE '20         \
    \     | Static asynchronous component misuse detection for Android applications\
    \ [23]                                                                       \
    \     | F-Droid, Google Play, Wan<br>doujia App Store |\n| ICSE '21          \
    \   | Sustainable Solving: Reducing The Memory Footprint of IFDS-Based Data Flow<br>Analyses\
    \ Using Intelligent Garbage Collection [24]                        | Google Play\
    \                                   |\n| ICSE '22             | DescribeCtx: Context-Aware\
    \ Description Synthesis for Sensitive Behaviors in Mo<br>bile Apps [25]      \
    \                                                  | Google Play             \
    \                      |\n| ICSE '20             | Time-travel testing of android\
    \ apps [26]                                                                  \
    \                                              | Google Play                 \
    \                  |\n| ICSE '20             | An empirical assessment of security\
    \ risks of global android banking apps [27]                                  \
    \                                         | Play, APKMonk,<br>Google<br>and others\
    \        |\n| ICSE '21             | Too Quiet in the Library: An Empirical Study\
    \ of Security Updates in Android<br>Apps' Native Code [28]                   \
    \                                | Google Play                               \
    \    |\n| ICSE '20             | Accessibility issues in android apps: state of\
    \ affairs, sentiments, and ways for<br>ward [29]                             \
    \                              | Google Play                                 \
    \  |\n| ICSE '21             | Don't do that! hunting down visual design smells\
    \ in complex uis against design<br>guidelines [30]                           \
    \                            | Android                                       |\n\
    | ICSE '21             | Identifying and characterizing silently-evolved methods\
    \ in the android API [31]                                                    \
    \                     | Google Play                                   |\n| ICSE\
    \ '21             | Layout and image recognition driving cross-platform automated\
    \ mobile test<br>ing [32]                                                    \
    \               | Apple's App Store, Google<br>Play             |\n| FSE '21 \
    \             | An empirical study of GUI widget detection for industrial mobile\
    \ games [33]                                                                 \
    \            | Android Games                                 |\n| ICSE '21   \
    \          | Fine with \"1234\"? An Analysis of SMS One-Time Password Randomness\
    \ in An<br>droid Apps [34]                                                   \
    \           | Play,<br>Tencent<br>Google<br>Myapp           |\n| ICSE '21    \
    \         | IMGDroid: Detecting Image Loading Defects in Android Applications\
    \ [35]                                                                       \
    \           | Android                                       |\n| ICSE '21    \
    \         | GUIGAN: Learning to Generate GUI Designs Using Generative Adversarial\
    \ Net<br>works [36]                                                          \
    \       | Android                                       |\n| ICSE '20        \
    \     | Unblind your apps: Predicting natural-language labels for mobile gui components<br>by\
    \ deep learning [37]                                                | Google Play\
    \                                   |\n| FSE '21              | Frontmatter: mining\
    \ Android user interfaces at scale [38]                                      \
    \                                                         | Google Play      \
    \                             |\n|                      | Mining app store non-technical\
    \ attributes                                                                 \
    \                                              |                             \
    \                  |\n| ICSE '20             | Schrödinger's security: Opening\
    \ the box on app developers' security rationale [39]                         \
    \                                             | Apple's App Store, Google<br>Play\
    \             |\n| ICSE '20             | Scalable statistical root cause analysis\
    \ on app telemetry [40]                                                      \
    \                                    | Facebook App                          \
    \        |\n| ICSE '21<br>ICSE '21 | An empirical assessment of global COVID-19\
    \ contact tracing applications [41]<br>We'll Fix It in Post: What Do Bug Fixes\
    \ in Video Game Update Notes Tell | Android<br>Steam                         \
    \     |\n| ICSE '21             | Us? [42]<br>Automatically matching bug reports\
    \ with related app reviews [43]                                              \
    \                              | Google Play                                 \
    \  |\n| ICSE '21             | Prioritize crowdsourced test reports via deep screenshot\
    \ understanding [44]                                                         \
    \                    | Android                                       |\n| ICSE\
    \ '21             | A first look at human values-violation in app reviews [45]\
    \                                                                            \
    \                  | Google Play                                   |\n| ICSE '21\
    \             | Does culture matter? impact of individualism and uncertainty avoidance\
    \ on app                                                                     \
    \      | Apple's App Store                             |\n| ICSE '21         \
    \    | reviews [46]<br>COVID-19 vs social media apps: does privacy really matter?\
    \ [47]                                                                       \
    \  | Google Play, Apple's App                      |\n| ICSE '20             |\
    \ Society-oriented applications development: Investigating users' values from\
    \                                                                            \
    \ | Store<br>Google Play                          |\n| FSE '21              |\
    \ bangladeshi agriculture mobile applications [48]<br>Checking conformance of\
    \ applications against GUI policies [49]                                     \
    \ | Android                                       |\n| ICSE '21             |\
    \ Identifying key features from app user reviews [50]                        \
    \                                                                            \
    \ | Apple's App Store                             |\n| ICSE '21             |\
    \ Champ: Characterizing undesired app behaviors from user comments based on<br>market\
    \ policies [51]                                                       | Google\
    \ Play, Chinese an<br>droid app stores   |\n| ICSE '20             | Caspar: extracting\
    \ and synthesizing user stories of problems from app reviews [52]            \
    \                                                          | Apple's App Store\
    \                             |\n|                      |                    \
    \                                                                            \
    \                                                         |                  \
    \                             |\n\n> Mining software applications — App stores\
    \ have been extensively used as a mining source of software applications. In these\
    \ papers, the major focus is often on another subject and app stores provide a\
    \ source where they can collect applications for either a data source or verification\
    \ dataset. For example, Zhan et al. [19] proposed an approach to detect software\
    \ vulnerabilities in third-party libraries of Android applications. They leveraged\
    \ the app store to collect a dataset to verify the effectiveness of their approach.\
    \ In these studies, the app store is both a convenient and practical source of\
    \ data collection. However, the involvement of app stores may not be necessary\
    \ since the purpose is to gather a dataset of application. In Yang et al.'s work\
    \ [30], they leveraged Android applications from an existing dataset without the\
    \ need to collect from an app store. We argue that the importance of app stores\
    \ in these types of studies is the selection criteria used by the researchers\
    \ to collect applications from app stores. These features can include star ratings,\
    \ total downloads, and app category.\n\n> Mining app store artifacts — In these\
    \ studies, researchers focused on unique software artifacts that come from the\
    \ operation of the app stores. App stores have a much heavier involvement in these\
    \ studies compared to the previous group. App reviews is the major software artifact\
    \ the researchers focused on, where they leverage the data to identify features\
    \ of applications [50], locating bug reports [43], and detect undesired app behaviors\
    \ [51]. One interesting research practice we observed is where van der Linden\
    \ et al. [39] leveraged the developer contact information shared on app stores\
    \ to send out surveys related to security practices.\n\n#### 2.3 Store-Focused\
    \ Research\n\nAs stated above, we found that most recent research involving app\
    \ stores focuses on the applications they offer rather than on studying the app\
    \ stores themselves; in particular, most research in the domain focuses on the\
    \ development of mobile applications. Meanwhile, a few papers have specifically\
    \ considered app stores and their effects on software engineering, but again these\
    \ works focus heavily on mobile app stores.\n\nIn a recent paper, Al-Subaihin\
    \ et al. [53] interviewed developers about how app stores affect their software\
    \ engineering tasks. They found that developers often leverage the review section\
    \ from similar applications to help with understanding the expected user experience\
    \ and anticipated features. App stores also provides a kind of playground for\
    \ releasing beta version of apps to receive feedback from users. The built-in\
    \ communication channels also play a large role in informing development. The\
    \ interviews suggest that developers pay attention to viewing user requests in\
    \ app store via channels such as reviews and forums. The approval period of app\
    \ stores affects how developers plan their release. App stores introduce non-technical\
    \ challenges in the development process. Given the app store model of release,\
    \ app store-specific metrics, such as total number of downloads, are considered\
    \ highly important to developers.\n\nRunning an app store presents both technical\
    \ and non-technical challenges to the store owner. Technical challenges include\
    \ verifying that each app will install correctly, while non-technical challenges\
    \ include ensuring that the promotional information in the app's product page\
    \ adheres to store guidelines. Wang et al. [54] investigated several Android app\
    \ stores in China and compared them to Google Play. Their study showed that these\
    \ stores were much less diligent in screening the apps they offered, with a significantly\
    \ higher presence of fake, cloned, and malicious apps than Google Play.\n\nJansen\
    \ and Bloemendal surveyed the landscape of app stores from the perspective of\
    \ the business domain [55]. They selected 6 app stores — 5 mobile stores and 1\
    \ Windows store — at the time of publication (2013), and investigated each store\
    \ manually to find features (i.e., those actors can interact with) and policies\
    \ (i.e., rules, regulations and governing processes that limit the functional\
    \ reach of the features) from each app store. The actors they define are the same\
    \ as the three major stakeholders of the app store model (i.e., the store owner,\
    \ users, and developers). Our study further contributes to the understanding of\
    \ app stores. First, we studied a significantly larger set of app stores: our\
    \ methodology was focused towards the identification of as many different types\
    \ of stores as possible. In total, we studied 53 stores in various domains including\
    \ mobile, embedded systems, computer games, application add-ons, and open source\
    \ distributions and packaging systems. Second, Jansen and Bloemendal studied app\
    \ stores from the perspective of a software business; for example, in their work\
    \ they would consider features and policies on whether users are able to generate\
    \ affiliate links to earn revenue through sharing applications. In contrast, our\
    \ work focuses on app stores in the perspective of their role in the software\
    \ engineering process.\n\nIn our study, we approach app stores from a broad landscape\
    \ not limited to mobile app stores. We focus on the similarity of features offered\
    \ between stores to understand their natural groupings and discuss the challenges\
    \ in the diversity of app stores.\n\n#### 2.4 Working Definition of an App Store\n\
    \nPrevious researchers have often taken a casual approach to defining the term\
    \ \"app store\", when a definition has been provided at all. For example, in their\
    \ survey paper, Martin et al. define an app store as \"A collection of apps that\
    \ provides, for each app, at least one non-technical attribute\", with an app\
    \ defined as \"An item of software that anyone with a suitable platform can install\
    \ without the need for technical expertise\" [17]. However, we feel that this\
    \ definition is too generous. For example, consider a static website called Pat's\
    \ Apps that lists of a few of someone's (Pat's) favourite applications together\
    \ with their personalized ratings and reviews; superficially, this would satisfy\
    \ Martin et al.'s requirements as it is a collection of apps together with Pat's\
    \ own reviews (which are non-technical attributes). We feel that this kind of\
    \ \"store\" is outside our scope of study for several reasons: Pat's software\
    \ collection is not comprehensive, it is unlikely that Pat provides any technical\
    \ guarantees about quality of the apps, and a passive list of apps on a web page\
    \ does not constitute an automated \"store\".\n\n![](_page_8_Figure_1.jpeg)\n\n\
    Fig. 2.1 Three major stakeholders of most app stores\n\nJansen and Bloemendal\
    \ [55] define app store as \"An online curated marketplace that allows developers\
    \ to sell and distribute their products to actors within one or more multi-sided\
    \ software platform ecosystems.\" We note that this definition ignores that app\
    \ stores are expected to provide infrastructure for the deployment, installation,\
    \ and maintenance of the apps, which impacts the software development process.\
    \ Their model also ignores marketplaces that do not have payment mechanisms, such\
    \ as the Google Chrome Extensions store and the various open source apps stores,\
    \ where all of the software products may be free to download and install.\n\n\
    In our work, we seek to define an idea of app store beyond the well-known mobile\
    \ ones and with an emphasis on how their existence may affect the software development\
    \ cycle. Because we are focused on exploring the notion of what app stores are,\
    \ we formulate a working definition of the term; we did so to provide clear inclusion/exclusion\
    \ criteria for the candidate app stores that we discover in Sec. 3.\n\nOur working\
    \ definition was influenced by considering the three major stakeholders of the\
    \ app store model: the app creators who create and submit applications to the\
    \ store; the app stores themselves, and the organizations behind their operation\
    \ who curate the app collection and coordinate both the store and installation\
    \ mechanisms; and the end users who browse, download, review, and update their\
    \ applications through the app store (see Figure 2.1).\n\nWe thus arrived at the\
    \ following working definition for app store as an online distribution mechanism\
    \ that:\n\n- 1. offers access to a comprehensive collection of software or software-based\
    \ services (henceforth, \"apps\") that augment an existing technical infrastructure\
    \ (i.e., the runtime environment),\n- 2. is curated, i.e., provides some level\
    \ of guarantees about the apps, such as ensuring basic functionality and freedom\
    \ from malware, and\n- 3. provides an end-to-end automated \"store\" experience\
    \ for end users, where\n\t- (a) the user can acquire the app directly through\
    \ the store,\n- (b) users trigger store events, such as browsing, ordering, selecting\
    \ options, arranging payment, etc., and\n- (c) the installation process is coordinated\
    \ automatically between the store and the user's own instance of the technical\
    \ platform.\n\nWe can see that using this working definition, our Pat's Apps example\
    \ fails to meet all three of our main criteria.\n\nWe note that our working definition\
    \ above evolved during our investigations; it represents our final group consensus\
    \ on what is or is not an app store for the purposes of doing the subsequent exploratory\
    \ study. The steps by which the representation is finalized are discussed in Sec.\
    \ 3.1.2. For example, our working definition implicitly includes package managers\
    \ such as the Debian-Linux apt tool and Javascript's NPM tool. It is true that\
    \ package managers are typically non-commercial, and so are \"stores\" only in\
    \ a loose sense of the term; furthermore, they usually lack a mechanism for easy\
    \ user browsing of apps and do not provide a facility for user reviews. However,\
    \ at the same time, they are a good fit conceptually: they tend to be comprehensive,\
    \ curated, and offer an automated user experience for selection and installation.\
    \ Furthermore, some package managers serve as the backend to a more traditional\
    \ store-like experience; for example, the Ubuntu Software Center builds on a tool\
    \ aptitude, which interacts with software repositories to provide a user experience\
    \ similar to that of Google Play.\n\n### 3 Research Methodology\n\nTo investigate\
    \ the research questions, we designed a three-stage methodology that is illustrated\
    \ in Figure 3.1. The goal of the first two stages is to answer RQ1, while the\
    \ third stage addresses RQ2.\n\nIn the first stage (Step ○1 and ○2 ) we identified\
    \ our initial list of features using a small set of well-known app stores (Apple's\
    \ App Store, Google Play, Steam etc.) In the second stage (Steps ○3 , ○4 , and\
    \ ○5 ) we methodically expanded our list to a conceptually wider ranging set of\
    \ 53 app stores. We then described these stores using the features identified\
    \ in the first stage. A major goal of this stage was to evaluate whether the set\
    \ of available features was sufficient to describe the characteristics of all\
    \ these stores. This set of features forms the answer to RQ1.\n\nIn the third\
    \ stage (Step ○6 ), we took advantage of the labeling of the 53 stores. We used\
    \ K-means clustering analysis to identify groups of stores that shared similar\
    \ features. These groupings form the answer to RQ2.\n\nWe now describe our methodology\
    \ in more detail.\n\n#### 3.1 Extracting Features Describing App Stores\n\nOur\
    \ basic assumption is that an app store can be categorized based on a finite set\
    \ of features. The features would correspond to traits of the app store where\n\
    \n![](_page_10_Figure_1.jpeg)\n\nFig. 3.1 Methodology overview: There are three\
    \ main stages, further broken down into six steps.\n\nthey describe the distinguishing\
    \ qualities or functional characteristics of the app store. We encode these features\
    \ as binary values, i.e., each store has or does not have a given feature.\n\n\
    In order to identify such features, we first created a seeding set of representative\
    \ app stores. We started by enumerating well-known app stores that we were aware\
    \ of (Step ○1 ). Once this set of representative app stores was created, we used\
    \ an iterative process to identify the features that we felt best characterized\
    \ these stores (Step ○2 ). We then used these features to describe each store.\n\
    \n#### 3.1.1 Stage 1: Identifying Features\n\nFirst, each of the six authors was\
    \ tasked with identifying representative characteristics of five stores and the\
    \ possible features for each. Each author worked alone in this step; however,\
    \ to seek better reliability as well as encourage diverse opinions, each store\
    \ was assigned to two authors. We list the 15 stores that were assigned in this\
    \ step with a short description in Table 3.1. After that, all of the authors met\
    \ as a group to discuss their findings and further refine the proposed feature\
    \ set.\n\nIn the subsequent iterations, the authors worked in pairs, and the pairings\
    \ were reassigned after each iteration (Step ○2 ). In these iterations, each authorpair\
    \ was assigned a set of 2–3 app stores and was asked to describe them using the\
    \ current set of features; a key concern was to evaluate whether the existing\
    \ features were sufficient or needed refinement. For each store, each author-pair\
    \ analyzed both its store-front and its documentation; in some cases, we could\
    \ navigate the store as users but not as developers, in these cases, we relied\
    \ on the store's supporting documentation.\n\nAfter this step, the six authors\
    \ discussed their findings as a group and updated the set of features. The features\
    \ were discussed in detail to ensure that they were conceptually independent from\
    \ each other. We also made sure that each feature applied to at least one store\
    \ to ensure that it was relevant.\n\nOur process leveraged ideas from the coding\
    \ process of Grounded theory [56] to extract the features of app stores, and followed\
    \ the practice of open card sorting [57] to create the categorized feature set.\
    \ Similar to prior work [58–60], we followed practices of Grounded theory's coding\
    \ process to extract the features— where we consider codes as a specific feature\
    \ of app store operation — and stopped when we reached saturation with no new\
    \ features added after a new round of describing app stores. Similar to prior\
    \ work [61–63], we applied card sorting to the collected features so inter-related\
    \ features are grouped together. The authors formed a group in this process and\
    \ discussed how different features belong to the same conceptual group and stopped\
    \ when consensus was reached.\n\n| Store                  | Description      \
    \                                                                            \
    \              |\n|------------------------|------------------------------------------------------------------------------------------------------------|\n\
    | Google Play Store      | Google's app store for Android                    \
    \                                                         |\n| Apple App Store\
    \        | Store for Apple devices                                           \
    \                                         |\n| Samsung GalaxyApps     | Store\
    \ specifically for Samsung devices                                           \
    \                          |\n| GitHub Marketplace     | Providing applications\
    \ and services to integrate with GitHub plat<br>form                         \
    \         |\n| Atlassian Marketplace  | Providing applications and services to\
    \ integrate with various At<br>lassian products                       |\n| Homebrew\
    \               | Package manager for MacOS                                  \
    \                                                |\n| MacPorts               |\
    \ A package manager for MacOS                                                \
    \                                |\n| Ubuntu Packages        | Software repository\
    \ for the Ubuntu Linux distribution, with a<br>official front end Ubuntu Software\
    \ Center |\n| Steam                  | Gaming focused app store running on multiple\
    \ operating systems<br>(e.g., Windows, Linux)                   |\n| Nintendo\
    \ EShop         | Provides applications for Nintendo devices (e.g., Nintendo Switch,<br>Nintendo\
    \ 3DS)                        |\n| GoG                    | Gaming focused store\
    \ focusing on providing DRM free games                                       \
    \           |\n| JetBrains Plugin Store | Provides plugins to enhance the behavior\
    \ of JetBrains IDEs                                                 |\n| VSCode\
    \ Marketplace     | Provides plugins to enhance the editor                   \
    \                                                  |\n| Chrome Web Store     \
    \  | Provides extensions to enhance Chromium based web browsers              \
    \                                   |\n| AWS Marketplace        | Provides servers\
    \ and cloud services                                                         \
    \               |\n\nTable 3.1 Investigated stores for feature extraction\n\n\
    3.1.2 Stage 2: Expanding Our Set of App Stores and Further Evaluation and Refinement\
    \ the Features\n\nOnce we had agreed on the features, our next goal was to verify\
    \ that these features were capable of describing other app stores that were not\
    \ part of the initial seed, or if features were missing or needed refinement.\
    \ We used a common search engine, Google, to expand our set of app stores in a\
    \ methodical manner (Step ○3 ). To achieve the goal of including a broad range\
    \ of yet undiscovered app stores, we first derived general search terms by combining\
    \ synonyms for \"app\" and \"store\". More specifically, we have built all possible\
    \ combinations of the following terms to construct our search queries:\n\n# First\
    \ half software, (extension -hair -lash), (addon OR add-on), solution, plugin\
    \ OR plug-in, install, app, package\n\nSecond half repository, shop, (\"app store\"\
    \ OR store), (\"market place\" OR marketplace), manager\n\nFor example, a concrete\
    \ query was created by combining app and (\"app store\" OR store). For some queries,\
    \ it was necessary to refine the term to avoid noise in the results; for example,\
    \ searching for the term extension would mainly return results related to hair\
    \ product or eye lashes. In total, with 8 synonyms for app and 5 synonyms for\
    \ store we were able to create 40 unique Google search queries. We felt confident\
    \ that these search terms were representative when we found that the initial seed\
    \ list had been exhaustively covered.\n\nOur Google search was performed in November\
    \ 2020. We queried and stored the search results for each search query. Two authors\
    \ classified each result as to whether or not it corresponded to an app store.\
    \ We devised two inclusion criteria for this decision: 1) the store in question\
    \ should offer software or software-based services, and 2) the store in question\
    \ should offer an end-toend experience for users (ordering, delivery, installation).\
    \ We considered only direct hits to the store (e.g., product page), and we explicitly\
    \ excluded results that contain only indirect references to a store, such as blog\
    \ posts, videos, or news. Any disagreements were resolved through discussion.\
    \ However, despite our initial effort of maintaining a clear set of inclusion\
    \ criteria for app stores, several corner cases became apparent during the labeling\
    \ process. The first two authors discussed these cases as they arose, and continually\
    \ updated the inclusion criteria throughout the labeling process. In a few special\
    \ cases no agreement could be reached, so another author acted as a moderator\
    \ and resolved the disagreement by a majority vote. Over time, the inclusion criteria\
    \ and features evolved and eventually reached a stable state (in Step ○3 ). Our\
    \ final state of the inclusion/exclusion criteria is presented as the working\
    \ definition for app stores defined in Sec. 2.4.\n\nThe classification of search\
    \ results was stopped when a new results page did not contain any new links to\
    \ app stores, or once all 10 retrieved pages were analyzed. Initially, 586 URLs\
    \ were examined by the first two authors until a saturation of agreement was reached\
    \ (90.7% agreement rate). The first author continued to label the rest. In the\
    \ end, a total of 1,600 URLs were labeled. Multiple search results can refer to\
    \ the same store; these duplicates were detected and eliminated by using the root\
    \ domain of the URL. The most common duplicate references were found for the domains\
    \ google. com (61), apple. com (22), and microsoft. com (18). In the end, we found\
    \ 291 stores. We note that the exact number of unique stores may differ since\
    \ two root domains can point to the same store, kodi. tv and kodi. wiki , or the\
    \ same root domain may contain multiple stores, chrome. google. com and play.\
    \ google. com .\n\nIn the next step (Step ○5 ), we constructed and labeled a set\
    \ of app stores based on our identified features from Step ○2 . We began from\
    \ the URLs labeled in the last step and selected the first three occurring stores\
    \ for each search term; this resulted in 104 URLs pointing to 48 unique stores.\
    \ Two of the stores were could not be accessed by the authors: ASRock App Shop\
    \ requires physical hardware to use it, and PLCnext Store's website was unresponsive\
    \ at the time of labeling. These stores were removed from the list. In addition,\
    \ we discussed several more stores that we felt deserved explicit investigation:\
    \ AWS, Flatpak, GoG, MacPorts, Nintendo eShop, Steam, and Samsung's Galaxy Store.\
    \ These are the stores that the authors investigated in Step ○2 but did not show\
    \ up in the first three occurring results from the search terms. Meanwhile, the\
    \ added stores all show up in the list of 291 stores identified by all labeled\
    \ URLs.\n\nWe thus selected and labeled a total of 53 app stores. This sample\
    \ is nonexhaustive, but we believe that our wide range of search queries has created\
    \ a representative sample of the population of app stores that enables our experiments.\n\
    \nThe first two authors proceeded to describe 12 app stores, selected as the first\
    \ from each search query, using the set of features. This was done to make sure\
    \ there was consistency in the interpretation and use of each feature. After that,\
    \ the first author labeled the remaining stores.\n\nTo check the applicability\
    \ of our dimensions and the labeling guidelines, we have measured the inter-rater\
    \ agreement between two authors on the 12 stores. We used the Cohen's Kappa [64]\
    \ as a measurement for our inter-rater agreement. The Cohen's Kappa is widely\
    \ used in software engineering research [65]. We have reached an agreement of\
    \ 86.3% with Cohen's Kappa [64] of 0.711). Our agreement based on the Cohen's\
    \ Kappa is considered as a substantial [66] inter-rater agreement suggesting a\
    \ high confidence of agreement between the two raters.\n\nThe outcomes of RQ1\
    \ were a list of features that describe the main characteristics of app stores\
    \ grouped by dimensions, and a set of 53 App Stores, each labeled using these\
    \ features.\n\n#### 3.2 Finding Natural Groupings of App Stores\n\nWith the outcomes\
    \ of RQ1, we next performed a K-means clustering analysis to identify groups of\
    \ similar stores. K-means is a well known clustering algorithm widely used in\
    \ software engineering research [67–70]. It groups vectorized data points iteratively\
    \ until k centroids are formed. We used the K-means++ implementation [71] to conduct\
    \ the clustering process.\n\n#### 3.2.1 Stage 3: Cluster Analysis\n\nTo identify\
    \ related app stores, we decided to cluster them using the K-means algorithm (Step\
    \ ○6 ).\n\nTo prepare our labels for the K-means clustering process, we converted\
    \ each label of the feature to a binary value: 1 if the store has the feature,\
    \ and 0 if it does not. Having binary-encoded data ensured that we do not suffer\
    \ from having categorical values that do not make sense in the scope of K-means.\
    \ However, performing K-means on binary data can also be problematic, since the\
    \ initial centroids selected will be binary. To mitigate this issue, we applied\
    \ Principal Component Analysis (PCA) [72] to both reduce the dimensional space\
    \ and to produce a mapping in the continuous range. We kept all principal components\
    \ that explained a variance of at least 0.05. Finally, we used the Silhouette\
    \ method [15] to determine the best number of clusters within a range of 1 to\
    \ 20. To identify the features that best characterize each cluster, we have calculated\
    \ the deviation of each cluster centroid (i.e, the center of the cluster) from\
    \ the centroid-of-centroids (C ) over all clusters.\n\nAs an unsupervised method,\
    \ the result of K-means provides only the clustering result with the stores in\
    \ each cluster. We then further discussed the results of the K-means process and\
    \ categorized the clusters by the properties of the contained stores. Following\
    \ our discussion and categorization, we assigned groupings and names to each of\
    \ the clusters.\n\n#### 4 Results\n\nIn this section, we present the results of\
    \ our investigations into each of the research questions. The results are organized\
    \ based on the three stages discussed in Sec. 3.\n\n### RQ1: What fundamental\
    \ features describe the space of app stores?\n\n#### Stage 1: Features characterizing\
    \ app stores\n\nAs discussed in Sec. 3.1, we derived a set of features and organizational\
    \ categories that describe the set of studied app stores; the results of these\
    \ efforts are summarized in Table 4.1. We have modelled the features as a binary\
    \ representation; thus, each store either has or does not have this feature. We\
    \ note that for some categories, the features are mutually exclusive; for example,\
    \ in the category Rights Management, a store can have either Creator managed DRM\
    \ or Store-enforced DRM, but not both. In other categories, an app store may have\
    \ several of the features within a given category; for example, there may be several\
    \ kinds of communication channels between users, app creators, and the store owner\
    \ for a given app store. We now describe each high-level category in detail.\n\
    \n> Monetization — describes what, if any, payment options are provided to the\
    \ user directly by the store. If a product is offered free within the store, but\
    \ requires an activation key obtained elsewhere, we consider that the product\
    \ is free. While most of the options are self-explanatory, some may be less obvious.\
    \ For example, GitHub Marketplace offers seat-based subscriptions where app pricing\
    \ is calculated by the number of installations made to individual machines; usually,\
    \ this occurs within the context of enterprise purchase. Also, AWS offers resource-based\
    \ subscription where the price charged is determined by the amount of resources\
    \ — such as cloud storage and CPU time — that are used during the execution of\
    \ the service.\n\n> Rights Management — describes the Digital Rights Management\
    \ (DRM) policy of the store; the values describe whether the store uses a store-wide\
    \ DRM feature. For example, for Steam, all games have DRM encryption, whereas\
    \ the F-Droid store contains only open source apps, so there is no need for DRM.\n\
    \n> Do I need an account? — describes whether a user can access and use the store\
    \ without being registered with the app store. We find that most stores are either\
    \ account required (e.g., Apple's App Store) or no registration possible (e.g.,\
    \ Snapcraft). However, we also found that some stores can be used without an account\
    \ for some purposes, with other features requiring explicit\n\n|  |  |  | Table\
    \ 4.1 Features for describing app stores |  |  |\n|--|--|--|----------------------------------------------|--|--|\n\
    |--|--|--|----------------------------------------------|--|--|\n\n| Feature \
    \                                               | Description                \
    \                                                                            \
    \                                                                   |\n|--------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n\
    | Monetization                                           | The type of payment\
    \ options directly offered by the app store.                                 \
    \                                                                           |\n\
    | Free                                                   | Free as in in the product\
    \ can be directly acquired                                                   \
    \                                                                     |\n| One-time\
    \ payment                                       | A single payment needed for\
    \ the product                                                                \
    \                                                                   |\n| Seat-based\
    \ subscription                                | The subscription is based on the\
    \ number of products provided                                                \
    \                                                              |\n| Time-based\
    \ subscription                                | A payment is needed by a set time\
    \ interval (e.g.\" monthy, yearly)                                           \
    \                                                              |\n| Resourced-based\
    \ subscription                           | A payment is needed by the amount of\
    \ resource used (e.g., API calls, CPU time)                                  \
    \                                                          |\n| Micro-transaction\
    \                                      | Additional payment can be collected based\
    \ on additional feature offered in a product                                 \
    \                                                     |\n| Custom pricing (i.e.,\
    \ \"Contact us for price\")          | The actual price is based on a per case\
    \ situation; this happens mostly in business-focused<br>app stores           \
    \                                                       |\n| Rights Management*\
    \                                     | How does the store take care of DRM on\
    \ the product provided.                                                      \
    \                                                        |\n| Creator-managed\
    \ DRM                                    | No DRM is offered by the store and\
    \ is taken care of by the creator                                            \
    \                                                            |\n| Store-enforced\
    \ DRM                                     | Store wide DRM for every product offered\
    \ in the store                                                               \
    \                                                      |\n| Do I need an account?*\
    \                                 | Whether it is possible to use the app store\
    \ without registration.                                                      \
    \                                                   |\n| Account required    \
    \                                   | An account is required to use the store\
    \                                                                            \
    \                                                       |\n| No registration possible\
    \                               | The store does not have an account system  \
    \                                                                            \
    \                                                   |\n| Some features requires\
    \ registration                    | Some content of the store is locked behind\
    \ an account, but the store can be used without<br>one.                      \
    \                                                    |\n| Product type       \
    \                                    | The type of product the store offers. \
    \                                                                            \
    \                                                        |\n| Standalone apps\
    \                                        | The product operates by itself    \
    \                                                                            \
    \                                                            |\n| Extension/add-ons\
    \ to apps/hardware                     | The product acts as a feature extension\
    \ to another application/hardware                                            \
    \                                                       |\n| Service/resources\
    \                                      | The software product is a service   \
    \                                                                            \
    \                                                          |\n| Package/library\
    \                                        | The product is not an end-user product,\
    \ but offers functionality to other products                                 \
    \                                                       |\n| Target audience*\
    \                                       | The intended users of the app store.\
    \                                                                            \
    \                                                          |\n| General purpose\
    \                                        | The app store is intended to be used\
    \ by everyone.                                                               \
    \                                                          |\n| Domain-specific<br>Type\
    \ of product creators            | The app store have a specific focus and is\
    \ very unlikely to be used by a normal person<br>The type of creators who submits\
    \ products to the app store.                     |\n| Business               \
    \                                | The creators mostly have a commercial or business\
    \ focus                                                                      \
    \                                             |\n| Community                 \
    \                             | The creators are from the community (e.g., open\
    \ source developers)                                                         \
    \                                               |\n| Intent of app store     \
    \                               | The reason why the app store exists from the\
    \ app stores' perspective.                                                   \
    \                                                  |\n| Community building/support\
    \                             | The app store aims to serve a technical community\
    \                                                                            \
    \                                             |\n| Profit                    \
    \                             | The app store aims to earn money             \
    \                                                                            \
    \                                                 |\n| Centralization of product\
    \ delivery                     | The app store aims to provide a way for customer\
    \ to gather apps in a centralized way                                        \
    \                                              |\n| Expanding a platform popularity/usefulness\
    \             | The app store aims to extend functionality from the platform it\
    \ is based on                                                                \
    \                               |\n| Role of intermediary                    \
    \               | The role app store play between the creator of products and\
    \ the customer of the app store.                                             \
    \                                   |\n| Embedded advertisement API          \
    \                   | Provides an advertisement method for creators to take advantage\
    \ of                                                                         \
    \                               |\n| CI/CD                                   \
    \               | Offers continuous integration/continuous deployment for creators\
    \                                                                            \
    \                              |\n| Checks at run time                       \
    \              | Provide checks when apps installed from the app store is ran\
    \                                                                            \
    \                                  |\n| Checks before making available to the\
    \ customer         | Provide checks when an app is submitted to the app store\
    \ for quality reasons                                                        \
    \                                      |\n| Composability*                   \
    \                      | The relationship between products provided in the app\
    \ store.                                                                     \
    \                                         |\n| Independent<br>Vendor internal\
    \ add-on/extension/unlock | The products in the app store are unrelated to each\
    \ other<br>Some products can be based on other products from the same creator\
    \ (e.g., game DLC,                          |\n|                             \
    \                           | app feature packs)                             \
    \                                                                            \
    \                                               |\n| Package manager type of app\
    \ relationship               | A dependency relationship exists between products\
    \ in the app store                                                           \
    \                                             |\n| Analytics                 \
    \                             | The type of analytical data provided by the app\
    \ store.                                                                     \
    \                                               |\n| Sentiment and popularity\
    \ ratings                       | Information related to the popularity of a product\
    \ (e.g., downloads, score ratings)                                           \
    \                                            |\n| Marketing feedback         \
    \                            | Information related to marketing for the creator\
    \ (e.g., sales, conversion, retention)                                       \
    \                                              |\n| Product usage data<br>Communication\
    \ channels           | Information related to the usage of the product. (e.g.,\
    \ logging, user profiling)<br>The methods where different parties of the app store\
    \ can communicate with each other. |\n| Documentation                        \
    \                  | Information related to the operation of the store (e.g.,\
    \ instructions to install applications)                                      \
    \                                      |\n| Product homepage                 \
    \                      | A homepage for a specific product in the app store  \
    \                                                                            \
    \                                          |\n| Ratings                      \
    \                          | Any form of rating customers can give to a product\
    \ (e.g., star, score, up/down vote)                                          \
    \                                            |\n| Written reviews (in text)  \
    \                            | A written viewer where customers can write their\
    \ experience to the product.                                                 \
    \                                              |\n| Community forum          \
    \                              | A forum like feature offered by the store where\
    \ people can discuss things related to the                                   \
    \                                               |\n|                         \
    \                               | store/product.                             \
    \                                                                            \
    \                                                   |\n| Support ticket      \
    \                                   | A system where customers can inquiry for\
    \ support questions related to the product offered<br>by the store.          \
    \                                                      |\n| Promotion/marketing\
    \                                    | The store offers a way to provide promotional/marketing\
    \ feature to the products in the<br>app store (e.g., featured apps, top downloads\
    \ of the month).                    |\n\n\\*: Categorical values are mutually\
    \ exclusive; one and only one categorical value in the dimension can apply to\
    \ a given store.\n\nregistration; for example, the Microsoft Store allows users\
    \ to download free applications without an account, but to purchase an app or\
    \ leave a review, an account is required.\n\n> Product type — describes the kinds\
    \ of applications that are offered by the store. For example, Google Play and\
    \ Steam focus on standalone apps, the VSCode Marketplace store offers add-ons\
    \ to an existing tool, and AWS allows users to \"rent\" web-based resources and\
    \ services.\n\n> Target audience — describes the intended user base of the store.\
    \ Generalpurpose stores offer products aimed at the broad general public of everyday\
    \ technology users; this includes stores such as Google Play, Steam, and the Chrome\
    \ Web Store. Domain-specific stores, on the other hand, have a dedicated focus\
    \ on a specialized field; for example, Adobe Magento focuses on building e-commerce\
    \ platforms.\n\n> Type of product creators — describes the typical focus of creators\
    \ submitting applications to the store. We distinguish between two groups of creators:\
    \ those with a commercial or business focus, and those with community focus such\
    \ as open source developers.\n\n> Intent of app store — describes the perceived\
    \ high-level goals of the app store. The values are derived from the app stores'\
    \ own descriptions of their goals, often found in \"About us\" web pages. For\
    \ example, both F-Droid and ApkPure are Android app stores; however, F-Droid's\
    \ focus is to provide a location to download and support FOSS software, while\
    \ ApkPure's goal is to provide a location for users to be able to download Android\
    \ apps when Google Play may be unavailable.\n\n> Role of intermediary — describes\
    \ the roles that the app store plays in mediating between the users and creators;\
    \ these are software engineering-related services that are mostly independent\
    \ of each other. For example, checks at run time tracks if the app store ensures\
    \ that its products function correctly (e.g., Steam tracking game stats). Also,\
    \ CI/CD indicates that the app store provides explicit support for continuous\
    \ integration and deployment of the apps, which may be linked to specific development\
    \ tools used by the creator.\n\n> Composability — describes the relationship between\
    \ products offered by the store. App stores of independent composability offer\
    \ products that have no relationship with each other, such as Firefox Add-ons.\
    \ Vendor internal add-on/extension/unlock means that the products within the app\
    \ store can be based on each other, but only when they are from the same vendor,\
    \ such as game DLC and micro-transaction unlocks. Package managers contain apps\
    \ that can have complicated dependency relationships regardless of the creator\
    \ of the products, such as the Ubuntu package management tool apt.\n\n> Analytics\
    \ — describes what kind of diagnostic information is provided by the store. We\
    \ distinguish between three kinds: Sentiment and popularity ratings offer user-based\
    \ information related to store products, such as number of installs in Home Assistant.\
    \ Marketing feedback tracks telemetry information for creators on the performance\
    \ of their product, such as GitHub Marketplace tracking retention rate for their\
    \ products for creators. Product usage data details the observed usage of the\
    \ products; for example, Steam tracks the average number of hours users spend\
    \ on each product.\n\n> Communication channels — tracks the types of methods the\
    \ store directly offers for communications between both users and creators. Since\
    \ most stores offer a product homepage for each of their products, the app creators\
    \ are largely free to put any information here. This means that if a creator wishes,\
    \ they can put links to other communication methods external to the store. We\
    \ do not track such information here since it is product dependent instead of\
    \ store dependent. While ratings and reviews/comments are often paired together,\
    \ during our exploration, we found cases where user ratings were permitted but\
    \ user reviews were not; thus, we have separate values for ratings and reviews.\
    \ Communication channels can take various forms with different variability, for\
    \ example, some stores allow responses for reviews. For this aspect, we stay at\
    \ a high level based on the functionality of the communication channels and consider\
    \ the variations as detailed implementation for each functionality.\n\nStage 2:\
    \ Expanded collection of app stores and labeled set of representative stores\n\
    \nIn stage 1, we identified 53 store candidates. To provide the required data\
    \ for our experiments, two authors explored these stores to identify which of\
    \ the fundamental features of the previous stage are true for each store. The\
    \ query results are summarized in Table 4.2, where we list the search term construction\
    \ keywords and the first 3 occurrence of stores by the search term. For example,\
    \ in search term constructed from (addon OR add-on) and (\"market place\") OR\
    \ marketplace, the first 3 occurrences are Google Play, PrestaShop, and CS-Cart.\n\
    \nThere are many app stores beyond Google Play and Apple's App Store. These app\
    \ stores exhibit a diverse set of features.\n\n#### RQ2: Are there groups of stores\
    \ that share similar features?\n\nUsing the labeled data of the 53 stores, we\
    \ were able to perform the K-means cluster analysis that we have introduced in\
    \ Sec. 3.2. With the number of clusters guided by the Silhouette method to choose\
    \ the best K value for Kmeans, our clustering resulted in eight clusters.\n\n\
    Due to the nature of unsupervised methods, K-means is able to identify only the\
    \ clusters and their members; no real-world meanings are extracted for why the\
    \ cluster members belong together. It is also important to note that the K-means\
    \ algorithm performs hard clustering; that is, it creates a partitioning of the\
    \ stores into mutually exclusive groups that together span the whole space. Thus\
    \ each store will be assigned to the unique cluster that the algorithm considers\
    \ to best represent it. For this reason, the raw results from K-means should not\
    \ be seen as authoritative, but rather as a vehicle for identifying groups of\
    \ stores with similar characteristics. Therefore, we leverage the K-means clustering\
    \ and further examine the clusters in detail to try to derive a human understandable\
    \ categorization of the stores.\n\nWe start by analyzing the differences between\
    \ clusters by analyzing the definitive characteristics of each cluster. In Sec.\
    \ 4, we show the details of the top 10 features that deviate the most from the\
    \ C. Column C contains\n\n|                                | store)<br>OR<br>store\"\
    <br>(\"app                                      | OR<br>place\"<br>marketplace)<br>(\"\
    market                                   | shop                              \
    \                                   | repository                             \
    \              | manager                                   |\n|--------------------------------|----------------------------------------------------------------------|----------------------------------------------------------------------------|----------------------------------------------------------------------|------------------------------------------------------|-------------------------------------------|\n\
    | app                            | Store,<br>App<br>Play<br>Google<br>Apple  \
    \                           | Google<br>BigCommerce,<br>HubSpot<br>Play,     \
    \                            | Store<br>App<br>Apple                         \
    \                       | Guardian<br>IzzyOn-<br>F-Droid,<br>Project,<br>Droid\
    \ | Play<br>Google                            |\n| software                  \
    \     | Store<br>App<br>Mac                                                  |\
    \ Sella-<br>MarketPlaceKit,<br>CS-Cart<br>cious,                             |\
    \ ϕ                                                                    | ϕ   \
    \                                                 | ϕ                        \
    \                 |\n| add-on)<br>OR<br>(addon        | Home<br>Add-<br>Firefox<br>Store,<br>App<br>Assistant,<br>Mac<br>ons\
    \ | Play,<br>CS-Cart<br>PrestaShop,<br>Google                                \
    \  | Chrome<br>PrestaShop,<br>Store<br>Web                                | Kodi\
    \                                                 | Ajour,<br>CurseForge,<br>Minion\
    \           |\n| plug-in)<br>OR<br>(plugin      | SketchUca-<br>THETA<br>RICOH<br>Play,<br>Google<br>tion,\
    \             | JetBrains<br>WordPress,                                      \
    \              | Bou-<br>Plugin<br>Bukkit,<br>tique                          \
    \         | Jet-<br>WordPress,<br>Brains                         | JMeter,<br>Autodesk<br>Jenkins,\
    \           |\n| -lash)<br>-hair<br>(extension  | Mi-<br>Store,<br>Web<br>Edge<br>Chrome<br>crosoft\
    \                    | Market-<br>Magento,<br>Store<br>Adobe<br>Web<br>VSCode<br>Chrome<br>place,\
    \ | Store<br>Web<br>Chrome                                               | GNOME<br>TYPO3,<br>SHELL\
    \                             | Store<br>Web<br>Chrome                    |\n\
    | install                        | App<br>Apple<br>Play,<br>Google<br>Store  \
    \                           | Eclipse<br>Play,<br>Google                     \
    \                            | Store,<br>Mi-<br>Play,<br>Store<br>App<br>Google<br>crosoft<br>Apple\
    \ | Assis-<br>DockerHub<br>Home<br>Kodi,<br>tant,        | AP-<br>Daz3D<br>Play,<br>Google<br>KPure,\
    \ |\n| solution                       | Mi-<br>Store,<br>Store<br>App<br>crosoft<br>Mac\
    \                      | CS-Cart                                             \
    \                       | ϕ                                                  \
    \                  | ϕ                                                    | ϕ\
    \                                         |\n| -book)<br>library<br>(software\
    \ | Store<br>Microsoft                                                   | GitHub<br>Marketplace,<br>Extensions,<br>Marketplace<br>VSCode<br>QT\
    \       | ϕ                                                                  \
    \  | ϕ                                                    | ϕ                \
    \                         |\n| package                        | Store,<br>Snapcraft<br>App<br>Play,<br>Google<br>Apple\
    \               | concrete5<br>CS-Cart,                                      \
    \                | Play<br>Google                                            \
    \           | PyPI,<br>Packages<br>Packagist,<br>Ubuntu            | NPM,<br>Chocolatey,<br>NuGet\
    \              |\n|                                |                         \
    \                                             |                              \
    \                                              |                             \
    \                                         |                                  \
    \                    |                                           |\n\nTable 4.2First\
    \ three identified stores for eachGooglequery Table 4.3 The 8 clusters found by\
    \ the K-means algorithm, with top deviated features from the centroid of centroids\
    \ (C). Each cell with a value represents one of the ten most influential features\
    \ of the corresponding cluster. The number indicates the percentage of stores\
    \ with the specific feature. The color encodes whether stores in that cluster\
    \ are less (magenta) or more (green) likely to have the feature, compared to the\
    \ centroid.\n\n|                                       |      |      |      |\
    \      |      | Cluster Index |      |      |      |\n|---------------------------------------|------|------|------|------|------|---------------|------|------|------|\n\
    | Features                              | C    | 1    | 2    | 3    | 4    | 5\
    \             | 6    | 7    | 8    |\n| Monetization                         \
    \ |      |      |      |      |      |               |      |      |      |\n\
    | Free                                  | 1.00 |      |      |      |      | \
    \              |      |      |      |\n| One-time payment                    \
    \  | 0.35 | 0.00 |      | 0.00 | 0.00 |               |      |      | 1.00 |\n\
    | Seat-based subscription               | 0.09 |      | 0.50 |      |      | \
    \              |      |      |      |\n| Time-based subscription             \
    \  | 0.30 |      | 0.75 | 0.00 |      |               |      |      | 0.86 |\n\
    | Resource-based subscription           | 0.05 |      |      |      |      | \
    \              |      |      |      |\n| Micro-transactions                  \
    \  | 0.11 |      |      |      |      |               |      |      | 0.86 |\n\
    | Custom Pricing                        | 0.01 |      |      |      |      | \
    \              |      |      |      |\n| Rights Management                   \
    \  |      |      |      |      |      |               |      |      |      |\n\
    | Creator managed DRM                   | 0.72 |      | 0.25 | 1.00 |      | \
    \              |      |      | 0.14 |\n| Store-enforced DRM                  \
    \  | 0.27 |      | 0.75 |      |      |               |      |      | 0.86 |\n\
    | Do I need an account to use the store |      |      |      |      |      | \
    \              |      |      |      |\n| Account Required                    \
    \  | 0.33 |      |      | 0.00 |      | 0.75          | 1.00 |      | 0.86 |\n\
    | No registration possible              | 0.35 | 1.00 | 0.00 |      | 0.00 | \
    \              | 0.00 | 1.00 |      |\n| Some features require registration  \
    \  | 0.30 |      | 1.00 |      | 1.00 |               |      |      |      |\n\
    | Product Type                          |      |      |      |      |      | \
    \              |      |      |      |\n| Standalone apps                     \
    \  | 0.42 |      | 0.00 |      |      |               |      | 1.00 |      |\n\
    | Extension/add-ons to apps/hardware    | 0.68 | 0.33 |      |      |      | \
    \              |      | 0.00 |      |\n| Service/Resources                   \
    \  | 0.08 |      |      |      |      |               |      |      |      |\n\
    | Package/Library                       | 0.17 | 0.89 |      |      |      | \
    \              |      |      |      |\n| Target audience                     \
    \  |      |      |      |      |      |               |      |      |      |\n\
    | General purpose                       | 0.33 |      |      | 0.00 | 0.83 | \
    \              | 0.00 | 1.00 |      |\n| Domain-specific                     \
    \  | 0.67 |      |      | 1.00 | 0.17 |               | 1.00 | 0.00 |      |\n\
    | Type of product creators              |      |      |      |      |      | \
    \              |      |      |      |\n| Business                            \
    \  | 0.67 | 0.22 |      | 0.00 |      |               | 1.00 |      |      |\n\
    | Community                             | 0.67 |      |      | 1.00 |      | \
    \              | 0.11 |      |      |\n| Intent of app store                 \
    \  |      |      |      |      |      |               |      |      |      |\n\
    | Community building / support          | 0.52 |      | 1.00 |      | 1.00 | 0.00\
    \          | 0.11 |      |      |\n| Profit                                | 0.38\
    \ | 0.00 |      | 0.00 | 0.00 |               | 0.78 | 0.00 | 1.00 |\n| Centralization\
    \ of product delivery    | 0.84 |      |      |      |      |               |\
    \      |      |      |\n| Expanding the platform                | 0.76 |     \
    \ |      |      |      |               |      | 0.17 |      |\n| Role of intermediary\
    \                  |      |      |      |      |      |               |      |\
    \      |      |\n| Embedded Advertisement API            | 0.16 |      |     \
    \ |      |      |               |      |      | 0.71 |\n| CI/CD              \
    \                   | 0.05 |      |      |      |      |               |     \
    \ |      |      |\n| Checks at run time                    | 0.14 |      | 0.50\
    \ |      |      |               |      |      |      |\n| Quality/security checks\
    \               | 0.74 |      |      |      |      | 0.25          |      |  \
    \    |      |\n| Composability                         |      |      |      |\
    \      |      |               |      |      |      |\n| Independent          \
    \                 | 0.56 | 0.00 |      |      | 1.00 | 1.00          |      |\
    \      | 0.00 |\n| Vendor internal                       | 0.15 |      |     \
    \ |      |      |               |      |      | 1.00 |\n| Package manager type\
    \                  | 0.19 | 1.00 |      |      |      |               |      |\
    \      |      |\n| Analytics                             |      |      |     \
    \ |      |      |               |      |      |      |\n| Sentiment and popularity\
    \ ratings      | 0.73 |      |      |      |      | 0.00          |      | 0.33\
    \ |      |\n| Marking feedback                      | 0.25 |      |      |   \
    \   |      |               |      |      |      |\n| Product Usage data      \
    \              | 0.33 |      |      |      |      |               |      |   \
    \   |      |\n| Communication channels                |      |      |      | \
    \     |      |               |      |      |      |\n| Documentation (wikis, FAQs)\
    \           | 0.81 |      |      |      |      | 0.25          |      |      |\
    \      |\n| Product homepage                      | 0.97 |      |      |     \
    \ |      |               |      |      |      |\n| Star/Score/Up/Downvote rating\
    \         | 0.57 | 0.11 | 1.00 |      | 1.00 | 0.00          | 1.00 | 0.00 | \
    \     |\n| Written reviews (in text)             | 0.47 | 0.00 |      |      |\
    \ 1.00 | 0.00          | 0.89 | 0.00 |      |\n| Community Forum             \
    \          | 0.45 |      |      | 0.75 |      | 0.00          |      |      |\
    \      |\n| Support Ticket                        | 0.35 |      |      |     \
    \ |      |               |      |      |      |\n| Promotion/Marketing       \
    \            | 0.71 |      |      |      |      | 0.25          |      |     \
    \ |      |\n\nthe centroid-of-centroids with values for each feature. The remaining\
    \ columns represent each cluster by an index from 1 to 8. The values in these\
    \ columns represent the proportion of app stores in the cluster with a specific\
    \ feature, the mean, and the background color of each cell represent the deviation\
    \ of the particular cluster centroid (i.e., difference between the centroid of\
    \ this cluster and the centroid-of-centroids for the feature). Each row corresponds\
    \ to a feature of the stores, which makes it easy to understand which features\
    \ are descriptive of a cluster.\n\nThe table only shows the top 10 deviations\
    \ per cluster (i.e., column) to focus on the most important contributors to each\
    \ cluster. Since all features are binary — each store has or does not have the\
    \ feature— all values of the centroid-of-centroids are between [0, 1]; thus, a\
    \ positive deviation (shown with a green background) implies that the stores in\
    \ the cluster are more likely to have the attribute, and a negative deviation\
    \ (shown with a magenta background) implies that the stores are less likely to\
    \ have the attribute.\n\nFor example, for cluster 8 the most important contributor\
    \ is [Composability] Vendor internal add-on/extension/unlock where the centroid\
    \ of the cluster is 1. When comparing against the centroid-of-centroids (at 0.15),\
    \ the deviation is at 0.85; this implies that all stores in this cluster have\
    \ this feature. On the other hand, an example of negative deviation for cluster\
    \ 1 is the feature [Composability] Independent with a centroid of 0 indicating\
    \ that no stores in this cluster have this feature. Since the centroid-of-centroids\
    \ for this features is at 0.56, this implies the deviation for stores in this\
    \ cluster is −0.56.\n\nAfter the top characteristics that make each cluster distinctive\
    \ had been identified, we leveraged this information to name and describe each\
    \ cluster accordingly. Using the information from Sec. 4 which shows the defining\
    \ features of each cluster, we derived an organization of the clusters based on\
    \ several dimensions. The results are described in Table 4.4.\n\nOne important\
    \ dimension focuses on the type of application served by stores in the cluster.\
    \ We identified three major types of applications that differentiate the clusters:\
    \ General, where the store offers stand-alone programs that run without the need\
    \ of specific software (aside from a specific operating system, e.g., Google Play,\
    \ AWS, Steam); Extensions, where the store offers extensions to a specific program\
    \ or platform e.g., VSCode Marketplace for VSCode, Chrome Web Store for Google\
    \ Chrome; and Package manager, where the store offers stand-alone programs, but\
    \ also manages dependencyrelationships and requirements between different applications\
    \ in the store e.g., NPM, MacPorts, Ubuntu Packages. Another dimension in which\
    \ these clusters can be organized is whether they are Commercial (business-oriented)\
    \ or Community-managed (no money is involved).\n\nApp stores are not all alike.\
    \ Intuitive groupings emerge naturally from the data. Their differences can be\
    \ due to the type of application they offer standalone or extensions — and their\
    \ operational model, either business- or community-oriented. We found that app\
    \ stores in different groups of our clustering have different properties, and\
    \ these properties may have bearing on empirical studies involving app stores.\n\
    \nTable 4.4List of stores and descriptions by cluster, with the example store\
    \ that is closest to cluster centroid\n\n| Type                              \
    \      | Cluster<br>in<br>Stores                                             \
    \                                                                            |\
    \ Store<br>Example                                                           \
    \                                                                            \
    \                 | Description<br>Cluster                                   \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                      | Index |\n|-----------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------|\n\
    | specialized<br>Commercial<br>Extensions | Hub-<br>AutoDesk,<br>Boutique,<br>SketchUcation,<br>GoG,<br>Magento,<br>Plugin<br>BigCommerce,<br>Shop,<br>CS-Cart<br>Adobe<br>Presta<br>Spot,\
    \  | addons<br>solution<br>offers<br>ecommerce<br>Shop<br>platform.<br>Presta<br>the<br>to\
    \                                                                            \
    \       | Creators<br>sys-<br>rating<br>specific.<br>offers<br>front<br>domain<br>store<br>very<br>their<br>are<br>reviews.<br>and<br>stores<br>business<br>written<br>the<br>in<br>mostly<br>and<br>Products<br>tems<br>are\
    \                                                                            \
    \                                     | 6     |\n| specialized<br>Community  \
    \              | Docker<br>Jme<br>VSCode<br>Assistant,<br>CurseForge,<br>Minion,<br>Marketplace<br>Home<br>Kodi,<br>Bukkit,<br>Hub,<br>ter,\
    \                      | center.<br>components<br>the<br>entertainment<br>to<br>extensions<br>add-on<br>offers<br>Kodi<br>Kodi\
    \                                                                   | prod<br>domain.<br>free<br>offers<br>specific<br>that<br>stores<br>a<br>to<br>tailor<br>focused<br>also<br>community<br>Stores<br>users.<br>are<br>to<br>These<br>ucts\
    \                                                                            \
    \                                                                            \
    \     | 3     |\n| non-specialized<br>Community            | Web<br>Marketplace,<br>Gnome,<br>Chrome<br>Add-ons,<br>Eclipse<br>Wordpress<br>Apkpure,<br>Firefox<br>Store,\
    \                                    | using<br>platform.<br>free<br>users<br>offers<br>for<br>wordpress<br>Wordpress<br>extensions<br>the\
    \                                                                     | in<br>generic<br>platform.<br>(e.g.,<br>a<br>face<br>the<br>registration<br>other.<br>to<br>stores<br>extensions<br>each<br>the<br>from<br>need<br>in<br>offered<br>independent<br>offers<br>not<br>do<br>stores<br>Products<br>operations<br>these<br>are<br>apps).<br>and<br>in<br>Products<br>Essential<br>audience<br>stalling\
    \ | 4     |\n| Commercial<br>General                   | Nintendo<br>Samsung<br>App<br>Store,<br>Apple<br>Play<br>Store,<br>Steam,<br>Store,<br>Google<br>Microsoft<br>Galaxy<br>eShop,<br>AWS,<br>Store\
    \ | offers<br>the<br>platform.<br>Store<br>for<br>applications<br>MicroSoft<br>windows\
    \                                                                            \
    \          | run<br>supporting<br>They<br>everyday.<br>products<br>encounter<br>internal<br>options.<br>people<br>vendor<br>many<br>offer<br>monetization<br>stores<br>and<br>profit<br>Typical<br>most<br>for\
    \                                                                            \
    \                                                      | 8     |\n| Community\
    \                               | Flat<br>Project<br>IzzyOnDroid,<br>F-Droid,<br>Guardian<br>Chocolatey,<br>Repository,<br>Snapcraft<br>pak,\
    \                                      | open<br>store.<br>and<br>only<br>application<br>free<br>software<br>a<br>is<br>Android<br>F-Droid<br>source\
    \                                                             | Cre<br>and<br>community<br>only.<br>products<br>the<br>source.<br>free<br>from<br>standalone<br>open<br>mostly<br>majority<br>are<br>contain<br>stores<br>are<br>the<br>products<br>stores<br>for<br>These<br>ators<br>the\
    \                                                                            \
    \                             | 7     |\n| Manager<br>Package                \
    \      | Packagist,<br>pack<br>MacPorts,<br>Ubuntu<br>Jenkins,<br>NuGet,<br>Typo3,<br>Ajour,<br>NPM,<br>PyPI,<br>ages\
    \                                    | main<br>PHP<br>the<br>for<br>is<br>repository<br>Packagist<br>packages.\
    \                                                                            \
    \                     | Products<br>inter-dependency<br>limited<br>also<br>stores.<br>stores.<br>are<br>most<br>these<br>channels<br>with<br>for<br>for<br>style<br>missing<br>involved<br>Communication<br>package<br>reviews<br>is<br>in<br>system<br>most<br>and<br>relationships.<br>ratings<br>and<br>account<br>free<br>with<br>are<br>No\
    \            | 1     |\n| oriented<br>Subscription                | Jet<br>con<br>Marketplace,<br>Marketplace,<br>marketplace<br>Qt<br>Github<br>Brains,<br>crete5\
    \                                                  | to<br>offers<br>repositories<br>actions<br>workflow<br>Marketplace<br>GitHub<br>and<br>git<br>applications<br>the<br>to<br>on<br>improve<br>Github<br>related<br>hosted\
    \ | DRM<br>standalone<br>plat<br>a<br>supports<br>extends<br>not<br>or<br>are<br>and<br>service<br>Products<br>services<br>provide<br>store.<br>subscription<br>either<br>the<br>and<br>by<br>management<br>offers<br>applications<br>Often<br>form.\
    \                                                                            \
    \       | 2     |\n| Other                                   | RICOH<br>daz3D<br>Sellacious,<br>MarketPlaceKit,<br>THETA,\
    \                                                                            \
    \          | platform.<br>ecommerce<br>provides<br>the<br>a<br>and<br>to<br>is<br>Sellacious<br>extensions<br>platform\
    \                                                               | stores<br>the<br>offered.<br>to<br>The<br>centrally<br>channels<br>stores.<br>communication<br>extensions<br>the<br>in<br>exist<br>distribute<br>on.<br>not<br>based<br>much<br>do<br>reviews<br>have<br>are<br>to<br>they<br>exists<br>not<br>and<br>platform<br>do<br>Rating<br>mostly<br>They\
    \                                   | 5     |\n\n#### What is an App Store? The\
    \ Software Engineering Perspective 23\n\n# 5 Discussion\n\nIn this section, we\
    \ discuss our findings regarding what we consider app stores to be based on our\
    \ clustering results, and we describe various research opportunities involving\
    \ the influence of app stores on software engineering practices.\n\n# 5.1 What\
    \ Is an App Store?\n\nThe term app store became popular largely through Apple's\
    \ App Store, which launched in 2008 along with the iPhone 3G [73]. Other online\
    \ software stores have also appeared and have had the term applied to them. Originally,\
    \ the term usually referred to stores of applications for mobile devices, but\
    \ we have found that today there is ample diversity of the type of applications\
    \ that app stores offer and in the features they provide to app developers and\
    \ users. App Stores are also dynamic: features are continually being added, removed,\
    \ and altered by store owners in response to changes in their goals and feedback\
    \ from their socio-technical environments. For example, the Chrome Web Store initially\
    \ introduced a built-in monetization option that provided a mechanism for applications\
    \ to receive payments from its users; however, the store later decided to deprecate\
    \ this monetization option [74] and suggested developers to switch to alternative\
    \ payment-handling options.\n\nIn our work, we have employed a working definition\
    \ through our inclusion/exclusion criteria for app stores to be included in our\
    \ research. However, due to the complexity, diversity, and constantly evolving\
    \ nature of app stores, we have decided not to attempt a firm, prescriptive definition\
    \ of the term. Instead, in the following paragraphs, we will discuss each of several\
    \ aspects of app stores in detail, and hope that in the future, a more robust\
    \ definition and operating model can emerge.\n\n#### 5.1.1 Common Features of\
    \ App Stores\n\nAlthough we found significant diversity among the example app\
    \ stores we studied, we were able to identify a set of three common features that\
    \ appear to span the space of app stores.\n\n> Simple installation and updates\
    \ of apps — An app store facilitates simple installation of a selected application,\
    \ and can also enable simple updating. For some stores, apps are expected to run\
    \ on the hardware of the client; in others, the app store provides and manages\
    \ the hardware where the app runs. In both cases, the app store frees the user\
    \ from worrying about the technical details of installation, including compatibility\
    \ with their specific hardware and software configuration, as well as the installation\
    \ of the app and its dependencies, if any. Typically, app stores will also automate\
    \ the installation of updates to the application, again freeing the user from\
    \ worrying about if they have the latest version of the app with the latest features\
    \ and bug fixes.\n\n> App exploration and discovery — App Stores provide mechanisms\
    \ that allow users to find apps they might want to use. In its simple form, this\
    \ mechanism might be a search engine that returns a list of apps that match a\
    \ given set of keywords (such as homebrew, PyPI). In the labeled app stores, 73%\
    \ of stores provide some kind of aggregated recommendations (e.g., advertisement\
    \ and trends in WordPress), up to personal recommendations that are based on other\
    \ apps the user has installed before (e.g., Apple's App Store). User feedback\
    \ via reviews (present in 47% of the labeled app stores) and forums (present in\
    \ 45% of the labeled app stores) can provide further information to aid other\
    \ users in identifying apps of possible interest to them.\n\n> The app store guarantees\
    \ the runtime environment — In practice, app stores often execute within a runtime\
    \ environment (RTE), such as an operating system (e.g., Google Play on Android)\
    \ or an extensible software application (e.g., Firefox Add-ons on Firefox ). Many\
    \ app stores simply sit on top of the RTE, acting primarily as a gatekeeper for\
    \ adding and deleting apps. However, some app stores are more tightly integrated\
    \ with the RTE; in extreme cases, the app store can extend the RTE with the app\
    \ store's own functionality and together provide an augmented RTE for the applications\
    \ managed through the app store. Steam is a good example for extending the RTE\
    \ with its own features; developers can integrate with many services offered by\
    \ Steam, such as an achievement system that offers players recognition when they\
    \ fulfill certain requirements in the game. Figure 5.1 illustrates the situation\
    \ where a product may integrate with additional store-added features to the RTE,\
    \ which in turn enriches the user experience of the store users. When Product\
    \ B is offered in App Store Y, it will not have the features provided by App Store\
    \ X.\n\nThe app store ensures that apps are installed only when their runtime\
    \ requirements are satisfied. The process is often done through running checks\
    \ on apps submitted to the app store, which 74% of the labeled app stores perform\
    \ specifically. By specifying the runtime requirements, the assumption for both\
    \ the developer and the user is that if the application is installed implying\
    \ that the requirements are satisfied — it is expected to run properly. This is\
    \ usually achieved by a software layer on top of the RTE, provided by either the\
    \ app store or the user. In its simplest form, this software layer is responsible\
    \ for installing and updating apps (see \"Simple installation and updates of apps\"\
    \ above). In some cases, this software layer might also include a set of libraries\
    \ that the apps can use to provide features specific to the app store thus forming\
    \ part of the RTE for the applications. These libraries might range in purpose\
    \ (domain specific, common GUI, resource management, etc.). In extreme cases,\
    \ this layer includes the operating system, as it is the case with Apple's App\
    \ Store. However, checks during runtime is a very rare feature, which only 14%\
    \ of the labeled app stores provides.\n\nSome hardware platforms have become so\
    \ tightly integrated to the software layer of the app store that they can be considered\
    \ monolithic: the hardware is rendered unusable without the app store. This is\
    \ exemplified by the Apple's\n\n![](_page_25_Figure_1.jpeg)\n\nFig. 5.1 Stores\
    \ may offer optional extensions to the runtime environment for applications\n\n\
    App Store, where one cannot use the hardware without first having an account in\
    \ the app store; even operating system upgrades are distributed via the store.\n\
    \nThis tight level of integration has clear benefits for all three stakeholders:\
    \ end users have fewer installation technical details to worry about; app developers\
    \ can be assured that users will be able to install their apps without the need\
    \ for technical support; and app store owners can strictly manage who has access\
    \ to the user's RTE and how. However, such tight integration is technically unnecessary\
    \ and may even be undesirable. From a software engineering perspective, such tight\
    \ coupling could be seen as a \"design smell\", since the operating system and\
    \ the app store layers address fundamentally different concerns. Also, tight integration\
    \ can create an artificial barrier to competition, effectively establishing a\
    \ quasi-monopoly for the store owner; the store owner may assume the role of gatekeeper\
    \ not only for streamlining technical issues, but also for business reasons, requiring\
    \ a kind of toll to be paid by app developers for access to the store. A recent\
    \ initiative in the European Union [75] aims to enable fair competition by enforcing\
    \ that ecosystems are opened up, which will likely also allow the installation\
    \ of alternative software layers for other app stores, a term called side-loading.\
    \ In contrast to the Apple's tight control of the operating system as part of\
    \ its app store, Android allows third-party app store software (e.g., F-Droid\
    \ [76]) to be installed in co-existence with the system default (often Google\
    \ Play).\n\nAs mentioned above, some stores distribute software that runs on hardware\
    \ owned by the App Store itself; in these cases, the RTE is fully managed and\
    \ controlled by the store. For example GitHub Marketplace and Atlassian Marketplace\
    \ offer applications that run on GitHub and Atlassian servers respectively. In\
    \ most cases, these applications are not deployed to the user's computers.\n\n\
    #### 5.1.2 Different Types of App Stores\n\nWhile some features are broadly shared\
    \ by all app stores, in Sec. 4, we identified different groups of app stores based\
    \ on their features. For stores within the same group, they often share common\
    \ features, whereas across different groups, the stores tend to have less in common.\
    \ We now discuss the differences across the groups in detail.\n\n> Diversity in\
    \ goals — As a platform focusing on delivering products to customers, the high-level\
    \ goal of one app store can be dramatically different from the other. Even app\
    \ stores providing software for the same underlying RTE can have radically different\
    \ purposes. For example, consider the app stores that run on Android. Google Play\
    \ is the de facto store for Android applications. F-Droid store, on the other\
    \ hand, offers only free and open source Android applications, and APKPure offers\
    \ multiple versions of the same software so the user can decide which version\
    \ they would like to install.\n\nApple's app store offers applications for all\
    \ its RTEs: MacOS (laptop and desktops), iOS (phones and tables), and the Safari\
    \ browser. In contrast, Google has different stores for AndroidOS and for its\
    \ web browser, Chrome. The Microsoft Store sells hardware and apps for Windows\
    \ and XBox. Alexa Skills offers skills that enhance the voice agent Alexa's capabilities.\n\
    \nIn many program language ecosystems, the core language development (focusing\
    \ on the language features) and packaging system (focusing on extending the functionality\
    \ of the language) are led by separate organizations (e.g., NPM [77] and JavaScript\
    \ [78]).\n\n> Diversity in business model — Another important difference we observed\
    \ is between business-managed and community-managed stores. In businessmanaged\
    \ stores (with few exceptions), a primary goal is to generate a profit. These\
    \ stores provide a payment mechanism between the app creator and the purchaser,\
    \ with the store keeping a percentage of any sales. These stores have to solve\
    \ three key concerns: first, implementing registration and authentication of users\
    \ and developers; second, some type of digital rights management, so only users\
    \ who have acquired the software can use it; and third, a payment mechanism e.g.,\
    \ subscription, one-time payment, and advertisement.\n\nCommunity-managed stores,\
    \ on the other hand, are often run by volunteers, and their features focus on\
    \ facilitating not-for-profit product delivery from developer to user. Many community\
    \ stores offer limited community interactions compared to business stores where\
    \ customer feedback is important. For example, in the Kodi store, add-ons have\
    \ a web page (e.g., The Movie Database Python [79]). This page provides information\
    \ regarding installation of the add-on, such as known compatibility concerns,\
    \ download links, and installation requirements. Meanwhile, most communication\
    \ channels about the add-on are hosted elsewhere; for example, installation and\
    \ usage instructions, extended descriptions, and screenshots can be found in the\
    \ community forum instead.\n\nIt is important to note that the products contained\
    \ in community-oriented stores are not limited to open source software; some community-managed\
    \ app store policies often permit the distribution of proprietary software. In\
    \ the natural groupings we observed, no rights management is enforced from the\
    \ store side for Cluster 3; at the same time, most stores in Cluster 8 have some\
    \ form of rights management built-in to the store. For example, Homebrew permits\
    \ apps that are not open source if the apps are free to use; these apps might\
    \ include in-app purchases — such as an upgrade to a full-feature app — that are\
    \ handled outside of Homebrew.\n\n5.2 Implications for the Main Participant Stakeholders\n\
    \nThe results of our study includes an evidence-based detailed view of the broad\
    \ landscape of app stores. This view can help us improve the understanding of\
    \ the realities and potentialities of app stores in general. Meanwhile, the results\
    \ of our work can also benefit the different stakeholders involved with app stores,\
    \ including app creators, app stores themselves, users, and researchers.\n\n>\
    \ Application creators — Those who create applications — including those who design,\
    \ develop, test, and market apps — benefit from a holistic view of other stores\
    \ that will allow them identify potential new markets (stores where they can offer\
    \ their software) and to understand changing and emerging features that could\
    \ eventually come to their app store of choice. For new creators, this research\
    \ emphasizes that a software store has both technical requirements — such as the\
    \ use of a specific software development kit — and non-technical ones — such as\
    \ restrictions on what applications can do, approval processes and timelines —\
    \ and that these requirements vary significantly from one store to another.\n\n\
    > App Stores — The overview presented herein provides a framework for comparison\
    \ between app stores, particularly those that operate on the same market, such\
    \ as Android application stores. It can also help promote wide adoption of features\
    \ that are not universal, such as communication channels between users and developers.\n\
    \n> Users — With the diversity in app stores, especially when multiple app stores\
    \ are competing in the same domain, it allows users the chose of where to acquire\
    \ their applications. This allows for more diversity for how the apps are distributed\
    \ and the user's choice also affect the competition.\n\n> Researchers — As discussed\
    \ in Sec. 2, most prior research has focused on the applications offered in app\
    \ stores, and there is a need for research that focuses on studying the store\
    \ themselves. This emphasis could aid researchers in considering different points\
    \ of view when conducting app store-centric studies, and also suggest avenues\
    \ of exploration concerning how the development process is affected by the existence\
    \ of app stores.\n\nWe describe this point in detail in the next sections.\n\n\
    # 5.3 App Store Features\n\nIn this section, we discuss how each of feature groups\
    \ from Table 4.1 has been addressed by current SE research and we suggest some\
    \ possible future directions.\n\n> Monetization — App development can be affected\
    \ by their pricing strategy. For example different software architecture to support\
    \ a different system of monetization (e.g., locking functionality behind microtransactions)\
    \ [80]. Studies have shown a correlation between app features and pricing [11,\
    \ 81, 82]. Moreover, in many studies on apps [29, 83], free apps and charged apps\
    \ are often considered as different types of applications. Future work could further\
    \ explore how different monetization options affect app development.\n\n> Rights\
    \ management — Digital rights management is still an ongoing challenge in software\
    \ engineering. Existing studies have explored the options of implementing different\
    \ DRM systems to support developers [84,85]. DRM can also add challenges in other\
    \ development activities such as complicating the testing procedure [86] and affect\
    \ performance [87]. Often we can observe the store offering means for providing\
    \ and enforcing DRM. Because DRM is still a nascent technology within software\
    \ engineering, it remains an open area to explore for future study and how app\
    \ stores can play a role.\n\n> Account requirement — User identity enables telemetry\
    \ of user behavior. An account system is also the prerequisite of a store-wide\
    \ DRM system as discussed in the previous paragraph. Existing research has focused\
    \ on how to leverage the user identity information to create targeted recommender\
    \ systems [88] and also investigated the concerns of privacy-related issues [89].\
    \ The interest of developers (detailed tracing data) and users (privacy) are in\
    \ conflict, app stores that require user identification could prove to be an excellent\
    \ study subject for future research in that area.\n\n> Product type — Existing\
    \ research has already shown different software engineering practices based on\
    \ the software product. For example, gaming development is very different from\
    \ traditional software development and open source development [90, 91]. Research\
    \ have shown that different types of software can introduce specific challenges\
    \ unique to them [92,93]. Future research should better understand how the product\
    \ type affects user expectations and development practices, for example, with\
    \ respect to the delivery of software or the way creators and users can interact.\n\
    \n> Target audience — When an app developer decides on a specific app store to\
    \ sell their app, they are also effectively selecting for a specific type of user\
    \ [94– 96]. Users of a general-purpose store such as Google Play are different\
    \ and much more diverse than the user population [97] in very specialized stores,\
    \ such as the add-on store for a particular game. Research needs to understand\
    \ better which features are relevant in each specific context [41], so the experience\
    \ can be tailored to the concrete situation.\n\n> Type of product creators — Existing\
    \ research has shown many differences between open source and industrial software\
    \ development [91,98]. Some studies have touched the aspect of release engineering\
    \ in open source development [99], where developers would strategically select\
    \ which versions to release on the app store. However, we believe that there is\
    \ still room for more understanding in how targeting releases towards app stores\
    \ affects software development.\n\n> Intent of app store — While in most domains,\
    \ there exists a dominant app store, we can also observe situations where multiple\
    \ app stores compete in the same domain (e.g., game stores on PC, mobile app stores\
    \ in China [54]). In these situations, users have a choice of which app store\
    \ to use when the same application is offered. In practice, some studies have\
    \ explored how the high level operation of app stores can affect the software\
    \ delivery process especially involving security concerns [41, 51]. Competition\
    \ between app stores within the same domain remains largely unstudied, as does\
    \ how their operations can affect both developers and users.\n\n> Role of intermediary\
    \ — App stores provide a platform for users and developers. Researchers have explored\
    \ how it affects software development processes such as testing and release management\
    \ [100, 101]. There are many opportunities for security [102] and quality assurance\
    \ [53, 103] to be ensured on the app store side. Future study can explore how\
    \ the differences between apps managed through an app store and apps that are\
    \ not. For example, studying the difference between open-source web extensions\
    \ that are in and not in app stores.\n\n> Composability — Existing research has\
    \ explored co-installability in the scope of package manager systems [104, 105].\
    \ However, we only have limited understanding of co-installability for standalone\
    \ applications in an extension system. For example, if two standalone extensions\
    \ were to modify the same component of the underlying software, a potential incompatibility\
    \ could occur. Future research can explore this area by performing empirical studies\
    \ on existing systems to understand the issue of conflicts.\n\n> Analytics — App\
    \ stores as the central hub between developers and users have access to rich information\
    \ useful for analytics. Previous studies have taken advantage of the app store\
    \ specific information to help software developers [106–108]. For example, Ullmann\
    \ et al. [109] leveraged records of rating statistics and downloaded information\
    \ to study the factors in developing successful video games. Another study leveraged\
    \ analytic information collected by the app store to identify incompatible builds\
    \ of application and physical devices [110]. Future work can explore what are\
    \ the possible data to collect and form analytics, and how can the analytic data\
    \ be leveraged to help developers and users.\n\n> Communication channels — Communication\
    \ channels are the most studied area of app store features. Specifically, there\
    \ has been a heavy focus on app reviews, where researchers have leveraged the\
    \ information in app reviews to aid software development in areas such as extracting/locating\
    \ bug reports [43], discover feature requests [50] and collect user feedback [52].\
    \ However, existing studies also suggest that the use of communication channels\
    \ in app stores are often multi-purpose [18]. Researchers also find that some\
    \ interaction requirements between interested parties are relegated to other platforms\
    \ such as Twitter [111]. Future work can explore different types of communication\
    \ channels in their functionality and how they can integrate with app stores.\
    \ The corpora from communication channels are also rich information sources where\
    \ researchers can leverage to extract information about developer-user interactions.\n\
    \n#### 5.4 Research Opportunities Involving App Stores\n\nApp Stores are becoming\
    \ the primary channel for software delivery and exert considerable influence in\
    \ many aspects of the software development process. A previous study by Rosen\
    \ and Shihab [112] on Stack Overflow questions by mobile developers has shown\
    \ that app delivery is one of the biggest challenges developers face. Our results\
    \ in Sec. 4 demonstrate that there is a wide variety of types of stores, each\
    \ with different features and goals. Today, app stores encompass many kinds of\
    \ applications, from games running on the hardware of the user to add-ons for\
    \ applications that run on corporate servers such as GitHub. However, existing\
    \ research often focuses heavily on the applications offered inside app stores,\
    \ especially those of the two major mobile app stores. In the following paragraphs,\
    \ we discuss several research opportunities to study how app stores can affect\
    \ software development.\n\n#### 5.4.1 App Stores as Actors in Software Development\n\
    \n> App Stores affect the software product cycle — Researchers need to consider\
    \ how and why app stores can affect the software development life cycle. For example,\
    \ we know that app stores can constrain and sometimes even dictate software release\
    \ processes. Some stores go beyond this and exert a kind of socio-technical environmental\
    \ pressure on other software development practices, becoming a de facto stakeholder\
    \ in app development. Sometimes these environmental pressures are technical in\
    \ nature, where the app store might dictate the programming language or deployment\
    \ platform/OS; some app stores go further and create RTEs, software development\
    \ kits (SDKs), and user interface (UI) libraries that must be used by all app\
    \ developers. Sometimes these environmental pressures are non-technical in nature,\
    \ such as when the app store prescribes the kinds of application that is allowed\
    \ in the store. For example, Microsoft recently announced that it will not permit\
    \ app developers to profit from open source applications.<sup>3</sup> When an\
    \ app store operates in a manner such that it has control over what kind of application\
    \ to include, it creates a software ecosystem and as such, it faces the same challenges\
    \ that any other ecosystem has: how to thrive. In particular, stores need to understand\
    \ the needs of their developers and users to retain existing ones and attract\
    \ new ones. However, suggested by what we have observed in Sec. 4, app stores\
    \ are diverse with a large number of features that characterize and differentiate\
    \ between them. While stores are experimenting and evolving, each action is likely\
    \ to have an effect on the ecosystems they formed, both positively and negatively.\
    \ Thus, the impact of app stores in the economy and their markets is worthy of\
    \ further study.\n\n> An app may be offered in several app stores — Developers\
    \ want to run their software on the platform that is provided or supported by\
    \ the store, and as such they must accept the requirements and limitations that\
    \ such a store may impose. This issue is compounded when the app is being offered\
    \ in more than one store, as the developers might have to adapt their processes\
    \ to different sets of requirements, some of which might be conflicting. For instance,\
    \ an app can be both available in F-Droid (in Cluster 7) and Google Play (in Cluster\
    \ 8). In Google Play, it is common for applications to collect telemetry data\
    \ to better understand typical user behaviour; however, in F-Droid — an open source\
    \ and privacy-oriented store — such data collection is highly discouraged. Furthermore,\
    \ developers must also adapt to the features and limitations that a store provides\
    \ regarding software deployment, communication with users and — when they exist\
    \ — the mechanism available to profit from their software and to use digital rights\
    \ management. This is particularly interesting if the targeted app stores are\
    \ in different natural groupings. This introduces new areas of studies such as\
    \ how store policies propagate to applications over time, and how violations of\
    \ store policies can be detected automatically. Researchers have already begun\
    \ to investigate this topic through qualitative approaches to identify how applications\
    \ comply with specific policies that concern accessibility [29] and human values\
    \ [45].\n\n> App stores strongly affect the release engineering process — App\
    \ Stores are especially important in release engineering. Specifically, the release\
    \ process needs to consider how the application is to be packaged, deployed, and\
    \ updated. The heterogeneity of the platform provided by RTEs might also affect\
    \ the number of versions of the application that need to be deployed, e.g., variety\
    \ of target CPUs, different screen sizes and orientations, and amount of available\
    \ memory.\n\nWhen an application is developed for multiple stores, it must effectively\
    \ be managed as a product line; this is because multiple deliverables must be\
    \ created, one for each platform-store combination [113]. Multiple deliverables\
    \ can also help for telemetry reasons such as tracking the installation source\
    \ of the application [114]. The differences between packaged versions might be\n\
    \n<sup>3</sup> See Update to 10.8.7 https://docs.microsoft.com/en-us/windows/uwp/publish/\
    \ store-policies-change-history\n\nas significant as requiring the source code\
    \ to be written in different programming languages, using different frameworks;\
    \ also, each store is likely to require different deployment processes.\n\nFor\
    \ example, when cross-releasing browser add-ons, developers may have to rewrite\
    \ part of the functionality in Swift/Objective-C for better integration with Safari\
    \ (in the Apple's App Store), while at the same time maintaining a fully JavaScript\
    \ version for Chrome Web Store. Also, the scheduling of release activities is\
    \ often dictated by the release processes of the stores. A previous study has\
    \ showed that taking into consideration of app review times is an important factor\
    \ when planning releases [53]. The app store standardizes, and often simplifies,\
    \ the release engineering processes for its store; but it also becomes a potential\
    \ roadblock that might delay or even reject a new release.\n\n#### 5.4.2 The Challenge\
    \ of Transferring Understanding Between Stores\n\nAs noted above, prior work has\
    \ examined many aspects of app stores, yet the app store itself has rarely been\
    \ the focus of the research. In many studies, the app store serves as a convenient\
    \ collection of apps, and the research focuses on mobile development concerns\
    \ such as testing and bug localization. Even when research focuses on the app\
    \ store itself, the scope rarely extends beyond Google Play and Apple's App Store.\
    \ Based on our observations, the diversity of app stores in their operational\
    \ goals, business models, delivery channels, and feature sets can affect the generalizability\
    \ of research outcomes. For example, there have recently been many studies [12,18,43,45,46,50,52]\
    \ that focus on app reviews. However, for an app store that does not have reviews\
    \ (e.g., Nintendo eShop) none of the findings and tools can be leveraged (e.g.,\
    \ stores in Cluster 1, 5, and 7).\n\n> App Stores that have the same features\
    \ may still differ significantly — Depending on the problem domain, the details\
    \ of software development practices can vary dramatically. For example, game development\
    \ has been compared to both more traditional industrial software development [90]\
    \ and to open source software development [91]; in both cases, the development\
    \ processes can differ greatly. We conjecture that the same may also occur across\
    \ app stores, where despite the same feature is being offered in the different\
    \ stores, the convention of using them could be different. As mentioned above,\
    \ one specific observation has been made between the gaming-focused store Steam\
    \ and mobile stores (e.g., Google Play) in Cluster 8, where Lin et al. [12] found\
    \ that reviews across the platforms for the same app were often quite different\
    \ in tone. Such uncertainly invites future research to validate their findings\
    \ in one store to another to improve the generalizability of the results, and\
    \ also encourages replication studies to verify existing results on other stores.\n\
    \n> A feature not in the app store does not mean the functionality is missing\
    \ — While some app stores aim to provide a complete experience, where all interactions\
    \ from the developers and users are expected to be performed within the store,\
    \ some app stores export part of the work to other platforms. This can even occur\
    \ for common features that one might find essential. For instance, starred reviews\
    \ are universal in Cluster 2, 4, and 6 where typical users leverage this information\
    \ to decide whether an application is good; starred reviews are uncommon for other\
    \ stores in Cluster 1, 5, 7. The specialized store may have some other metric\
    \ to indicate popularity or quality, such as total number of downloads, but the\
    \ focus of the store is often to offer a managed way of installation. Other features,\
    \ such as application support, are left to other platforms such as social media.\
    \ Research can further explore the integration between app stores and other platforms.\n\
    \n#### 6 Threats to Validity\n\n> Internal Validity — Our initial seeding of app\
    \ stores comes from personal experience of app stores by the authors of the paper.\
    \ Personal bias could cause us to miss other types of app stores. However, given\
    \ the number of authors on this paper and our initial effort to consider as many\
    \ stores as possible, we feel that have created a wide, deep, and collaborative\
    \ \"best effort\". When we labeled app stores by their dimensions, it is a qualitative\
    \ process. As with any qualitative process, the results could be biased by the\
    \ authors performing the task. We tackled this issue by first labeling a few stores\
    \ separately by all authors and discussing the results until a consensus was achieved;\
    \ thus, we started with a set of \"gold standard\" labels. Then the labeling task\
    \ was delegated to two authors who continued to label the stores separately with\
    \ a portion of the store overlapping. The overlapping labels are then verified\
    \ by the Cohen's Kappa between the two authors to measure the agreement.\n\nWe\
    \ leveraged the K-means algorithm for the clustering process. We first applied\
    \ PCA techniques to reduce the dimensions of the initial labeling and provide\
    \ an orthogonal basis to feed the K-means clustering. When using other clustering\
    \ algorithms (e.g., Mean-shift, DBSCAN ), the clustering result might change;\
    \ while K-means is widely adopted for clustering process in SE research, by nature,\
    \ determining the proper k value is still a challenge. We followed common best\
    \ practice to use metrics (i.e., the Silhouette method) to determine the best\
    \ value k. Despite our efforts, the output of the K-means clustering is not perfect.\
    \ We mainly leveraged the K-means clustering as the first step to illustrate that\
    \ app stores forms natural clusters which are different from each other. Based\
    \ on the K-means output, we further grouped the clusters into types based on our\
    \ qualitative understanding of the app store space.\n\n> External Validity — During\
    \ the process of expanding app stores, we relied on the Google Search Engine to\
    \ find web results based on keywords. The results of this step rely on the capability\
    \ of Google and are subject to change over time as Google updates its search algorithms.\
    \ The order may also be affected by SEO operations. Combining results from other\
    \ search engines (e.g., DuckDuckGo, Bing) can help to reduce the bias.\n\nWhen\
    \ we applied our inclusion criteria, 1) app stores must contain software products\
    \ and 2) should offer an end-to-end experience for users (ordering, delivery,\
    \ installation), we excluded stores that focus on digital assets that are not\
    \ software, such as a pure assets store that offers cosmetic enhancements to desktop\
    \ environments; we also excluded stores that offer software products but in a\
    \ way such that installation is completely managed by users. An extreme example,\
    \ would be the software section of Amazon where software is sold as an activation\
    \ key which users would input to activate the software that they need to install\
    \ themselves. A more general inspection of all means of distribution software\
    \ can be performed to gain a broader understanding of software distribution.\n\
    \nWe relied on only publicly available information to label each store. So if\
    \ some functionality (e.g., analytics information) is not documented publicly,\
    \ we were unable to confirm whether the store has such functionality. We also\
    \ set a time limit to label each store so in case we were unable to find information\
    \ about the store, with each store receives the same amount of attention.\n\n\
    One of the main challenges for reproducibility and replicability is that the Google\
    \ Search results and app stores can change overtime. New app stores are likely\
    \ to emerge and existing app stores may introduce and remove features. The focus\
    \ of our study is not to establish an exhaustive catalog of app stores, nor to\
    \ study the historic evolution of a store. Our goal is to establish a framework\
    \ that can describe app stores and to understand whether the operations of app\
    \ stores follow different patterns. Based on the granularity which we extracted\
    \ features from app stores, we expect the majority of the feature groups will\
    \ remain stable over time. In the future, if researchers would like to repeat\
    \ our study, the labeling results may differ due to updates in the app store.\
    \ To mitigate this issue, we have included a snapshot of all Google Search results,\
    \ and documented how we would perform the labeling. So while the final labels\
    \ may differ, by applying the same process, a replication study would be possible\
    \ with updated data.\n\n#### 7 Summary\n\nIn this paper, we have explored the\
    \ idea of what an app store is and what features make app stores unique from each\
    \ other. We labeled a set of representative stores, curated from web search queries,\
    \ by their features to study the natural groupings of the stores. Our analysis\
    \ suggests that app stores can differ in the type of product offered in the store,\
    \ and whether the store is business oriented or community oriented. These natural\
    \ groupings of the stores challenge the manner in which app store research has\
    \ largely been mobile focused. Previous studies have already shown empirical differences\
    \ in activities in mobile app stores and game stores [12]. Our study further suggests\
    \ that in the future, when we study app stores, we will need to consider the generalizability\
    \ of the results across app stores. Since one type of app store may operate under\
    \ different constraints than another kind, results observed in one app store setting\
    \ may not generalize to others.\n\n# Conflict of Interests\n\nThe authors declared\
    \ that they have no conflict of interest.\n\n# Data Availability Statement\n\n\
    A dataset consists of the Google query results and the app store labeling results\
    \ are available on Zenodo. 4\n\nAcknowledgements We would like to thank the attendees\
    \ of the Shonan meeting [115] on \"Release Engineering for Mobile Applications\"\
    , where the paper's idea was conceived.\n\nOne of the authors has received funding\
    \ from the European Union's Horizon 2020 research and innovation programme under\
    \ grant agreement number 825328 (FASTEN).\n\n#### References\n\n- 1. C. Dixon,\
    \ R. Mahajan, S. Agarwal, A. Brush, B. Lee, S. Saroiu, and V. Bahl, \"The home\
    \ needs an operating system (and an app store),\" in SIGCOMM Workshop on Hot Topics\
    \ in Networks, ACM, 2010.\n- 2. Valve, \"Welcome to Steam.\" https://store.steampowered.com/,\
    \ 2022. Accessed: Jun. 22 2022.\n- 3. GitHub, \"GitHub Marketplace · to improve\
    \ your workflow · GitHub.\" https://github. com/marketplace?type=, 2022. Accessed:\
    \ Jun. 06 2022.\n- 4. Google, \"Chrome Web Store Extensions.\" https://chrome.google.com/webstore/\
    \ category/extensions, 2022. Accessed: Jun. 22, 2022.\n- 5. WordPress, \"WordPress\
    \ Plugins | WordPress.org.\" https://wordpress.org/plugins/, 2022. Accessed: Jun.\
    \ 22, 2022.\n- 6. Autodesk, \"Autodesk App Store : Plugins, Add-ons for Autodesk\
    \ software, AutoCAD, Revit, Inventor, 3ds Max, Maya ....\" https://apps.autodesk.com/,\
    \ 2022. Accessed: Jun. 22, 2022.\n- 7. Docker, \"Explore Docker's Container Image\
    \ Repository | Docker Hub.\" https://hub. docker.com/search?q=, 2022. Accessed:\
    \ Jun. 22, 2022.\n- 8. Amazon, \"AWS Marketplace: Homepage.\" https://aws.amazon.com/marketplace/,\
    \ 2022. Accessed: Jun. 22, 2022.\n- 9. Rémi Prévost, Mike McQuaid, and Danielle\
    \ Lalonde, \"The Missing Package Manager for macOS (or Linux) — Homebrew.\" https://brew.sh/,\
    \ 2022. Accessed: Jun. 22, 2022.\n- 10. Canonical, \"Ubuntu Software Center in\
    \ Launchpad.\" https://launchpad.net/ software-center, 2009. Accessed: Jun. 22,\
    \ 2022.\n- 11. M. Harman, Y. Jia, and Y. Zhang, \"App store mining and analysis:\
    \ MSR for App Stores,\" in Int. Conf. on Mining Software Repositories, IEEE, 2012.\n\
    - 12. D. Lin, C.-P. Bezemer, Y. Zou, and A. E. Hassan, \"An empirical study of\
    \ game reviews on the steam platform,\" in Empirical Software Engineering, Springer,\
    \ 2019.\n- 13. Wikipedia, \"Electronic AppWrapper Wikipedia.\" https://en.wikipedia.org/wiki/\
    \ Electronic\\_AppWrapper, 2022. Accessed: Jun. 22, 2022.\n- 14. J. MacQueen et\
    \ al., \"Some methods for classification and analysis of multivariate observations,\"\
    \ in Proceedings of the fifth Berkeley symposium on mathematical statistics and\
    \ probability, Oakland, CA, USA, 1967.\n- 15. P. J. Rousseeuw, \"Silhouettes:\
    \ a graphical aid to the interpretation and validation of cluster analysis,\"\
    \ in Journal of computational and applied mathematics, Elsevier, 1987.\n\n<sup>4</sup>\
    \ https://zenodo.org/record/7968192\n\n- 16. I. J. M. Ruiz, M. Nagappan, B. Adams,\
    \ and A. E. Hassan, \"Understanding reuse in the android market,\" in Int. Conf.\
    \ on Program Comprehension, IEEE, 2012.\n- 17. W. Martin, F. Sarro, Y. Jia, Y.\
    \ Zhang, and M. Harman, \"A survey of app store analysis for software engineering,\"\
    \ in Transactions on Software Engineering, IEEE, 2016.\n- 18. J. Dąbrowski, E.\
    \ Letier, A. Perini, and A. Susi, \"Analysing app reviews for software engineering:\
    \ a systematic literature review,\" in Empirical Software Engineering, Springer,\
    \ 2022.\n- 19. X. Zhan, L. Fan, S. Chen, F. Wu, T. Liu, X. Luo, and Y. Liu, \"\
    Atvhunter: Reliable version detection of third-party libraries for vulnerability\
    \ identification in android applications,\" in Int. Conf. on Software Engineering,\
    \ IEEE, 2021.\n- 20. X. Zhang, X. Wang, R. Slavin, T. Breaux, and J. Niu, \"How\
    \ does misconfiguration of analytic services compromise mobile privacy?,\" in\
    \ Int. Conf. on Software Engineering, IEEE, 2020.\n- 21. S. Rahaman, I. Neamtiu,\
    \ and X. Yin, \"Algebraic-datatype taint tracking, with applications to understanding\
    \ Android identifier leaks,\" in Joint Meeting on European Software Engineering\
    \ Conference and Symposium on the Foundations of Software Engineering, ACM, 2021.\n\
    - 22. T. Nguyen, P. Vu, and T. Nguyen, \"Code recommendation for exception handling,\"\
    \ in Joint Meeting on European Software Engineering Conference and Symposium on\
    \ the Foundations of Software Engineering, ACM, 2020.\n- 23. L. Pan, B. Cui, H.\
    \ Liu, J. Yan, S. Wang, J. Yan, and J. Zhang, \"Static asynchronous component\
    \ misuse detection for Android applications,\" in Joint Meeting on European Software\
    \ Engineering Conference and Symposium on the Foundations of Software Engineering,\
    \ ACM, 2020.\n- 24. S. Arzt, \"Sustainable Solving: Reducing The Memory Footprint\
    \ of IFDS-Based Data Flow Analyses Using Intelligent Garbage Collection,\" in\
    \ Int. Conf. on Software Engineering, IEEE, 2021.\n- 25. S. Yang, Y. Wang, Y.\
    \ Yao, H. Wang, Y. F. Ye, and X. Xiao, \"DescribeCtx: Context-Aware Description\
    \ Synthesis for Sensitive Behaviors in Mobile Apps,\" in Int. Conf. on Software\
    \ Engineering, IEEE, 2022.\n- 26. Z. Dong, M. Böhme, L. Cojocaru, and A. Roychoudhury,\
    \ \"Time-travel testing of android apps,\" in Int. Conf. on Software Engineering,\
    \ IEEE, 2020.\n- 27. S. Chen, L. Fan, G. Meng, T. Su, M. Xue, Y. Xue, Y. Liu,\
    \ and L. Xu, \"An empirical assessment of security risks of global android banking\
    \ apps,\" in Int. Conf. on Software Engineering, IEEE, 2020.\n- 28. S. Almanee,\
    \ A. Ünal, M. Payer, and J. Garcia, \"Too Quiet in the Library: An Empirical Study\
    \ of Security Updates in Android Apps' Native Code,\" in Int. Conf. on Software\
    \ Engineering, IEEE, 2021.\n- 29. A. Alshayban, I. Ahmed, and S. Malek, \"Accessibility\
    \ issues in android apps: state of affairs, sentiments, and ways forward,\" in\
    \ Int. Conf. on Software Engineering, IEEE, 2020.\n- 30. B. Yang, Z. Xing, X.\
    \ Xia, C. Chen, D. Ye, and S. Li, \"Don't do that! hunting down visual design\
    \ smells in complex uis against design guidelines,\" in Int. Conf. on Software\
    \ Engineering, IEEE, 2021.\n- 31. P. Liu, L. Li, Y. Yan, M. Fazzini, and J. Grundy,\
    \ \"Identifying and characterizing silently-evolved methods in the android API,\"\
    \ in Int. Conf. on Software Engineering: Software Engineering in Practice, IEEE,\
    \ 2021.\n- 32. S. Yu, C. Fang, Y. Yun, and Y. Feng, \"Layout and image recognition\
    \ driving crossplatform automated mobile testing,\" in Int. Conf. on Software\
    \ Engineering, IEEE, 2021.\n- 33. J. Ye, K. Chen, X. Xie, L. Ma, R. Huang, Y.\
    \ Chen, Y. Xue, and J. Zhao, \"An empirical study of GUI widget detection for\
    \ industrial mobile games,\" in Joint Meeting on European Software Engineering\
    \ Conference and Symposium on the Foundations of Software Engineering, ACM, 2021.\n\
    - 34. S. Ma, J. Li, H. Kim, E. Bertino, S. Nepal, D. Ostry, and C. Sun, \"Fine\
    \ with \"1234\"? An Analysis of SMS One-Time Password Randomness in Android Apps,\"\
    \ in Int. Conf. on Software Engineering, IEEE, 2021.\n- 35. W. Song, M. Han, and\
    \ J. Huang, \"IMGDroid: Detecting Image Loading Defects in Android Applications,\"\
    \ in Int. Conf. on Software Engineering, IEEE, 2021.\n- 36. T. Zhao, C. Chen,\
    \ Y. Liu, and X. Zhu, \"GUIGAN: Learning to Generate GUI Designs Using Generative\
    \ Adversarial Networks,\" in Int. Conf. on Software Engineering, IEEE, 2021.\n\
    - 37. J. Chen, C. Chen, Z. Xing, X. Xu, L. Zhut, G. Li, and J. Wang, \"Unblind\
    \ your apps: Predicting natural-language labels for mobile gui components by deep\
    \ learning,\" in Int. Conf. on Software Engineering, IEEE, 2020.\n- 38. K. Kuznetsov,\
    \ C. Fu, S. Gao, D. N. Jansen, L. Zhang, and A. Zeller, \"Frontmatter: mining\
    \ Android user interfaces at scale,\" in Joint Meeting on European Software Engineering\
    \ Conference and Symposium on the Foundations of Software Engineering, ACM, 2021.\n\
    - 39. D. Van Der Linden, P. Anthonysamy, B. Nuseibeh, T. T. Tun, M. Petre, M.\
    \ Levine, J. Towse, and A. Rashid, \"Schrödinger's security: Opening the box on\
    \ app developers' security rationale,\" in Int. Conf. on Software Engineering,\
    \ IEEE, 2020.\n- 40. V. Murali, E. Yao, U. Mathur, and S. Chandra, \"Scalable\
    \ statistical root cause analysis on app telemetry,\" in Int. Conf. on Software\
    \ Engineering: Software Engineering in Practice, IEEE, 2021.\n- 41. R. Sun, W.\
    \ Wang, M. Xue, G. Tyson, S. Camtepe, and D. C. Ranasinghe, \"An empirical assessment\
    \ of global COVID-19 contact tracing applications,\" in Int. Conf. on Software\
    \ Engineering, IEEE, 2021.\n- 42. A. Truelove, E. S. de Almeida, and I. Ahmed,\
    \ \"We'll Fix It in Post: What Do Bug Fixes in Video Game Update Notes Tell Us?,\"\
    \ in Int. Conf. on Software Engineering, IEEE, 2021.\n- 43. M. Haering, C. Stanik,\
    \ and W. Maalej, \"Automatically matching bug reports with related app reviews,\"\
    \ in Int. Conf. on Software Engineering, IEEE, 2021.\n- 44. S. Yu, C. Fang, Z.\
    \ Cao, X. Wang, T. Li, and Z. Chen, \"Prioritize crowdsourced test reports via\
    \ deep screenshot understanding,\" in Int. Conf. on Software Engineering, IEEE,\
    \ 2021.\n- 45. H. O. Obie, W. Hussain, X. Xia, J. Grundy, L. Li, B. Turhan, J.\
    \ Whittle, and M. Shahin, \"A first look at human values-violation in app reviews,\"\
    \ in Int. Conf. on Software Engineering: Software Engineering in Society, IEEE,\
    \ 2021.\n- 46. R. A.-L. Fischer, R. Walczuch, and E. Guzman, \"Does culture matter?\
    \ impact of individualism and uncertainty avoidance on app reviews,\" in Int.\
    \ Conf. on Software Engineering: Software Engineering in Society, IEEE, 2021.\n\
    - 47. O. Haggag, S. Haggag, J. Grundy, and M. Abdelrazek, \"COVID-19 vs social\
    \ media apps: does privacy really matter?,\" in Int. Conf. on Software Engineering:\
    \ Software Engineering in Society, IEEE, 2021.\n- 48. R. A. Shams, W. Hussain,\
    \ G. Oliver, A. Nurwidyantoro, H. Perera, and J. Whittle, \"Society-oriented applications\
    \ development: Investigating users' values from bangladeshi agriculture mobile\
    \ applications,\" in Int. Conf. on Software Engineering: Software Engineering\
    \ in Society, IEEE, 2020.\n- 49. Z. Zhang, Y. Feng, M. D. Ernst, S. Porst, and\
    \ I. Dillig, \"Checking conformance of applications against GUI policies,\" in\
    \ Joint Meeting on European Software Engineering Conference and Symposium on the\
    \ Foundations of Software Engineering, ACM, 2021.\n- 50. H. Wu, W. Deng, X. Niu,\
    \ and C. Nie, \"Identifying key features from app user reviews,\" in Int. Conf.\
    \ on Software Engineering, IEEE, 2021.\n- 51. Y. Hu, H. Wang, T. Ji, X. Xiao,\
    \ X. Luo, P. Gao, and Y. Guo, \"Champ: Characterizing undesired app behaviors\
    \ from user comments based on market policies,\" in Int. Conf. on Software Engineering,\
    \ IEEE, 2021.\n- 52. H. Guo and M. P. Singh, \"Caspar: extracting and synthesizing\
    \ user stories of problems from app reviews,\" in Int. Conf. on Software Engineering,\
    \ IEEE, 2020.\n- 53. A. A. Al-Subaihin, F. Sarro, S. Black, L. Capra, and M. Harman,\
    \ \"App Store Effects on Software Engineering Practices,\" in Transactions on\
    \ Software Engineering, IEEE, 2021.\n- 54. H. Wang, Z. Liu, J. Liang, N. Vallina-Rodriguez,\
    \ Y. Guo, L. Li, J. Tapiador, J. Cao, and G. Xu, \"Beyond google play: A large-scale\
    \ comparative study of chinese android app markets,\" in Internet Measurement\
    \ Conference 2018, 2018.\n- 55. S. Jansen and E. Bloemendal, \"Defining app stores:\
    \ The role of curated marketplaces in software ecosystems,\" in ICSOB, Springer,\
    \ 2013.\n- 56. D. Walker and F. Myrick, \"Grounded theory: An exploration of process\
    \ and procedure,\" in Qualitative health research, Sage, 2006.\n- 57. A. P. M.\
    \ Coxon et al., Sorting data: Collection and analysis. Sage, 1999.\n- 58. S. Adolph,\
    \ W. Hall, and P. Kruchten, \"Using grounded theory to study the experience of\
    \ software development,\" in Empirical Software Engineering, Springer, 2011.\n\
    - 59. R. Hoda, J. Noble, and S. Marshall, \"Developing a grounded theory to explain\
    \ the practices of self-organizing Agile teams,\" in Empirical Software Engineering,\
    \ Springer, 2012.\n- 60. Z. Masood, R. Hoda, and K. Blincoe, \"How agile teams\
    \ make self-assignment work: a grounded theory study,\" in Empirical Software\
    \ Engineering, Springer, 2020.\n- 61. C. Vassallo, S. Panichella, F. Palomba,\
    \ S. Proksch, H. C. Gall, and A. Zaidman, \"How developers engage with static\
    \ analysis tools in different contexts,\" in Empirical Software Engineering, Springer,\
    \ 2020.\n- 62. J. Chen, X. Xia, D. Lo, J. Grundy, and X. Yang, \"Maintenance-related\
    \ concerns for post-deployed Ethereum smart contract development: issues, techniques,\
    \ and future challenges,\" in Empirical Software Engineering, Springer, 2021.\n\
    - 63. P. Wang, C. Brown, J. A. Jennings, and K. T. Stolee, \"Demystifying regular\
    \ expression bugs,\" in Empirical Software Engineering, Springer, 2022.\n- 64.\
    \ J. Cohen, \"A coefficient of agreement for nominal scales,\" in Educational\
    \ and psychological measurement, Sage, 1960.\n- 65. J. Pérez, J. Díaz, J. Garcia-Martin,\
    \ and B. Tabuenca, \"Systematic literature reviews in software engineering—Enhancement\
    \ of the study selection process using Cohen's kappa statistic,\" in Journal of\
    \ Systems and Software, Elsevier, 2020.\n- 66. C. A. Lantz and E. Nebenzahl, \"\
    Behavior and interpretation of the κ statistic: Resolution of the two paradoxes,\"\
    \ in Journal of clinical epidemiology, Elsevier, 1996.\n- 67. P. Pickerill, H.\
    \ J. Jungen, M. Ochodek, M. Maćkowiak, and M. Staron, \"Phantom: Curating github\
    \ for engineered software projects using time-series clustering,\" Empirical Software\
    \ Engineering, 2020.\n- 68. V. Khatibi Bardsiri, D. N. A. Jawawi, S. Z. M. Hashim,\
    \ and E. Khatibi, \"A flexible method to estimate the software development effort\
    \ based on the classification of projects and localization of comparisons,\" Empirical\
    \ Software Engineering, 2014.\n- 69. A. Al-Subaihin, F. Sarro, S. Black, and L.\
    \ Capra, \"Empirical comparison of textbased mobile apps similarity measurement\
    \ techniques,\" Empirical Software Engineering, 2019.\n- 70. T. Kuchta, T. Lutellier,\
    \ E. Wong, L. Tan, and C. Cadar, \"On the correctness of electronic documents:\
    \ studying, finding, and localizing inconsistency bugs in PDF readers and files,\"\
    \ Empirical Software Engineering, 2018.\n- 71. D. Arthur and S. Vassilvitskii,\
    \ \"k-means++: The advantages of careful seeding,\" tech. rep., Stanford, 2006.\n\
    - 72. S. Wold, K. Esbensen, and P. Geladi, \"Principal component analysis,\" in\
    \ Chemometrics and intelligent laboratory systems, Elsevier, 1987.\n- 73. Apple,\
    \ \"Apple Introduces the New iPhone 3G.\" https://www.apple.com/ca/newsroom/ 2008/06/09Apple-Introduces-the-New-iPhone-3G/,\
    \ 2008. Accessed: Jul. 17, 2022.\n- 74. Google, \"Chrome Web Store payments deprecation.\"\
    \ https://developer.chrome.com/ docs/webstore/cws-payments-deprecation/, 2022.\
    \ Accessed: Mar. 16, 2022.\n- 75. E. Commission, \"Digital Markets Act: Commission\
    \ welcomes political agreement on rules to ensure fair and open digital markets.\"\
    \ https://ec.europa.eu/commission/ presscorner/detail/en/IP\\_22\\_1978, 2022.\
    \ Accessed: Jul. 13, 2022.\n- 76. F-Droid, \"F-Droid Free and Open Source Android\
    \ App Repository.\" https:// f-droid.org/, 2022. Accessed: Oct. 02, 2022.\n- 77.\
    \ npm, \"npm About.\" https://www.npmjs.com/about, 2022. Accessed: Oct. 02, 2022.\n\
    - 78. E. International, \"TC39 Specifying JavaScript..\" https://tc39.es/, 2022.\
    \ Accessed: Oct. 02, 2022.\n- 79. T. Kodi, \"The Movie Database Python | Matrix\
    \ | Addons | Kodi.\" https://kodi.tv/ addons/matrix/metadata.themoviedb.org.python,\
    \ 2022. Accessed: Jul. 13, 2022.\n- 80. V. V. H. Pham, X. Liu, X. Zheng, M. Fu,\
    \ S. V. Deshpande, W. Xia, R. Zhou, and M. Abdelrazek, \"PaaS-black or white:\
    \ an investigation into software development model for building retail industry\
    \ SaaS,\" in Int. Conf. on Software Engineering Companion (ICSE-C), IEEE, 2017.\n\
    - 81. A. Finkelstein, M. Harman, Y. Jia, W. Martin, F. Sarro, and Y. Zhang, \"\
    Investigating the relationship between price, rating, and popularity in the Blackberry\
    \ World App Store,\" Information and Software Technology, 2017.\n- 82. F. Sarro,\
    \ A. A. Al-Subaihin, M. Harman, Y. Jia, W. Martin, and Y. Zhang, \"Feature lifecycles\
    \ as they spread, migrate, remain, and die in app stores,\" in Int. requirements\
    \ engineering conference (RE), IEEE, 2015.\n- 83. W. Aljedaani, M. Nagappan, B.\
    \ Adams, and M. Godfrey, \"A comparison of bugs across the ios and android platforms\
    \ of two open source cross platform browser apps,\" in Int. Conf. on Mobile Software\
    \ Engineering and Systems, IEEE, 2019.\n- 84. Z. Lu, Y. Shi, R. Tao, and Z. Zhang,\
    \ \"Blockchain for digital rights management of design works,\" in Int. Conf on\
    \ Software Engineering and Service Science (ICSESS), IEEE, 2019.\n- 85. T. Gaber,\
    \ A. Ahmed, and A. Mostafa, \"Privdrm: A privacy-preserving secure digital right\
    \ management system,\" in Evaluation and Assessment in Software Engineering, ACM,\
    \ 2020.\n- 86. A. Sung, S. Kim, Y. Kim, Y. Jang, and J. Kim, \"Test automation\
    \ and its limitations: a case study,\" in Int. Conf. on Automated Software Engineering\
    \ (ASE), IEEE, 2019.\n- 87. M. Lemon, \"Two Point Hospital no longer uses Denuvo\
    \ DRM.\" https://www.vg247. com/two-point-hospital-no-longer-uses-denuvo-drm,\
    \ 2018. Accessed: Mar. 31, 2023.\n- 88. X. He, W. Dai, G. Cao, R. Tang, M. Yuan,\
    \ and Q. Yang, \"Mining target users for online marketing based on app store data,\"\
    \ in Int. Conf. on Big Data (Big Data), IEEE, 2015.\n- 89. G. L. Scoccia, M. Autili,\
    \ G. Stilo, and P. Inverardi, \"An empirical study of privacy labels on the Apple\
    \ iOS mobile app store,\" in Int. Conf. on Mobile Software Engineering and Systems,\
    \ 2022.\n- 90. E. Murphy-Hill, T. Zimmermann, and N. Nagappan, \"Cowboys, ankle\
    \ sprains, and keepers of quality: How is video game development different from\
    \ software development?,\" in Int. Conf. on Software Engineering, 2014.\n- 91.\
    \ L. Pascarella, F. Palomba, M. Di Penta, and A. Bacchelli, \"How is video game\
    \ development different from software development in open source?,\" in Int. Conf.\
    \ on Mining Software Repositories, IEEE, 2018.\n- 92. D. Lee, G. K. Rajbahadur,\
    \ D. Lin, M. Sayagh, C.-P. Bezemer, and A. E. Hassan, \"An empirical study of\
    \ the characteristics of popular Minecraft mods,\" Empirical Software Engineering,\
    \ 2020.\n- 93. M. H. Ibrahim, M. Sayagh, and A. E. Hassan, \"Too many images on\
    \ dockerhub! how different are images for the same system?,\" Empirical Software\
    \ Engineering, 2020.\n- 94. G. H. Subramanian, P. C. Pendharkar, and M. Wallace,\
    \ \"An empirical study of the effect of complexity, platform, and program type\
    \ on software development effort of business applications,\" Empirical Software\
    \ Engineering, 2006.\n- 95. I. Manotas, C. Bird, R. Zhang, D. Shepherd, C. Jaspan,\
    \ C. Sadowski, L. Pollock, and J. Clause, \"An empirical study of practitioners'\
    \ perspectives on green software engineering,\" in Int. Conf. on Software Engineering,\
    \ 2016.\n- 96. S. Gholami, H. Khazaei, and C.-P. Bezemer, \"Should you upgrade\
    \ official docker hub images in production environments?,\" in Int. Conf. on Software\
    \ Engineering: New Ideas and Emerging Results (ICSE-NIER), IEEE, 2021.\n- 97.\
    \ E. Guzman, L. Oliveira, Y. Steiner, L. C. Wagner, and M. Glinz, \"User feedback\
    \ in the app store: a cross-cultural study,\" in Int. Conf. on Software Engineering:\
    \ Software Engineering in Society, 2018.\n- 98. D. Lee, D. Lin, C.-P. Bezemer,\
    \ and A. E. Hassan, \"Building the perfect game–an empirical study of game modifications,\"\
    \ Empirical Software Engineering, 2020.\n- 99. M. Nayebi, H. Farahi, and G. Ruhe,\
    \ \"Which version should be released to app store?,\" in Int. Symposium on Empirical\
    \ Software Engineering and Measurement (ESEM), IEEE, 2017.\n- 100. M. Nayebi,\
    \ B. Adams, and G. Ruhe, \"Release Practices for Mobile Apps–What do Users and\
    \ Developers Think?,\" in Int. Conf. on software analysis, evolution, and reengineering\
    \ (saner), IEEE, 2016.\n- 101. S. Shen, X. Lu, Z. Hu, and X. Liu, \"Towards release\
    \ strategy optimization for apps in Google Play,\" in Proceedings of the 9th Asia-Pacific\
    \ Symposium on Internetware, 2017.\n- 102. G. Ferreira, L. Jia, J. Sunshine, and\
    \ C. Kästner, \"Containing malicious package updates in npm with a lightweight\
    \ permission system,\" in Int. Conf. on Software Engineering (ICSE), IEEE, 2021.\n\
    - 103. C. Tang, S. Chen, L. Fan, L. Xu, Y. Liu, Z. Tang, and L. Dou, \"A large-scale\
    \ empirical study on industrial fake apps,\" in Int. Conf. on Software Engineering:\
    \ Software Engineering in Practice (ICSE-SEIP), IEEE, 2019.\n- 104. J. Vouillon\
    \ and R. D. Cosmo, \"On software component co-installability,\" Transactions on\
    \ Software Engineering and Methodology (TOSEM), 2013.\n- 105. M. Claes, T. Mens,\
    \ R. Di Cosmo, and J. Vouillon, \"A historical analysis of Debian package incompatibilities,\"\
    \ in Int. Conf. on Mining Software Repositories, IEEE, 2015.\n- 106. C. McMillan,\
    \ M. Grechanik, and D. Poshyvanyk, \"Detecting similar software applications,\"\
    \ in Int. Conf. on Software Engineering (ICSE), IEEE, 2012.\n- 107. W. Martin,\
    \ F. Sarro, and M. Harman, \"Causal impact analysis for app releases in google\
    \ play,\" in Int. Symposium on Foundations of software engineering, 2016.\n- 108.\
    \ W. Maalej, M. Nayebi, and G. Ruhe, \"Data-driven requirements engineering-an\
    \ update,\" in Int. Conf. on Software Engineering: Software Engineering in Practice\
    \ (ICSE-SEIP), IEEE, 2019.\n- 109. G. C. Ullmann, C. Politowski, Y.-G. Guéhéneuc,\
    \ and F. Petrillo, \"What makes a game high-rated? towards factors of video game\
    \ success,\" in Int. ICSE Workshop on Games and Software Engineering: Engineering\
    \ Fun, Inspiration, and Motivation, 2022.\n- 110. H. Khalid, M. Nagappan, E. Shihab,\
    \ and A. E. Hassan, \"Prioritizing the devices to test your app on: A case study\
    \ of android game apps,\" in Int. Symposium on Foundations of Software Engineering,\
    \ 2014.\n- 111. M. Nayebi, H. Cho, H. Farrahi, and G. Ruhe, \"App store mining\
    \ is not enough,\" in Int. Conf. on Software Engineering Companion (ICSE-C), IEEE,\
    \ 2017.\n- 112. C. Rosen and E. Shihab, \"What are mobile developers asking about?\
    \ a large scale study using stack overflow,\" in Empirical Software Engineering,\
    \ Springer, 2016.\n- 113. H. Wang, X. Wang, and Y. Guo, \"Characterizing the global\
    \ mobile app developers: a large-scale empirical study,\" in Int. Conf. on Mobile\
    \ Software Engineering and Systems, IEEE, 2019.\n- 114. Y. Y. Ng, H. Zhou, Z.\
    \ Ji, H. Luo, and Y. Dong, \"Which Android app store can be trusted in China?,\"\
    \ in Computer Software and Applications Conference, IEEE, 2014.\n- 115. S. McIntosh,\
    \ Y. Kamei, and M. Nagappan, Release Engineering for Mobile Applications — Communications\
    \ of NII Shonan Meetings. Springer, 2019."
  decisions:
    language: '- Qualified. Reason: English Paper'
    evaluation_prompt: Disqualified
    related_work_prompt: Qualified
    novelty_prompt: Qualified
    review_only_prompt: Disqualified
  llm_input_used: "## Abstract\n\"App stores\" are online software stores where end\
    \ users may browse, purchase,\ndownload, and install software applications. By\
    \ far, the best known app stores\nare associated with mobile platforms, such as\
    \ Google Play for Android and\nApple's App Store for iOS. The ubiquity of smartphones\
    \ has led to mobile app\nstores becoming a touchstone experience of modern living.\
    \ However, most of app\nstore research has concentrated on properties of the apps\
    \ rather than the\nstores themselves. Today, there is a rich diversity of app\
    \ stores and these\nstores have largely been overlooked by researchers: app stores\
    \ exist on many\ndistinctive platforms, are aimed at different classes of users,\
    \ and have\ndifferent end-goals beyond simply selling a standalone app to a smartphone\n\
    user.\n  We survey and characterize the broader dimensionality of app stores,\
    \ and\nexplore how and why they influence software development practices, such\
    \ as\nsystem design and release management. We begin by collecting a set of app\
    \ store\nexamples from web search queries. By analyzing and curating the results,\
    \ we\nderive a set of features common to app stores. We then build a dimensional\n\
    model of app stores based on these features, and we fit each app store from our\n\
    web search result set into this model. Next, we performed unsupervised\nclustering\
    \ to the app stores to find their natural groupings. Our results\nsuggest that\
    \ app stores have become an essential stakeholder in modern software\ndevelopment.\
    \ They control the distribution channel to end users and ensure that\nthe applications\
    \ are of suitable quality; in turn, this leads to developers\nadhering to various\
    \ store guidelines when creating their applications. However,\nwe found the app\
    \ stores operational model could vary widely between stores, and\nthis variability\
    \ could in turn affect the generalizability of existing\nunderstanding of app\
    \ stores.\n\n## Introduction\nThe widespread proliferation of smartphones and\
    \ other mobile devices in recent years has in turn produced an immense demand\
    \ for applications that run on these platforms. In response, online \"app stores\"\
    \ such as Google Play and Apple's App Store have emerged to facilitate the discovery,\
    \ purchasing, installation, and management of apps by users on their mobile devices.\
    \ The success of mobile app stores has enabled a new and more direct relationship\
    \ between app creators and users. The app store serves as a conduit between software\
    \ creators (often, developers) and their users, with some mediation provided by\
    \ the app store. The app store provides a \"one-stop shopping\" experience for\
    \ users, who can compare competing products and read reviews of other users. The\
    \ app store might also acts as a quality gatekeeper for the platform, providing\
    \ varying levels of guarantees about the apps, such as easy installation and removal,\
    \ expected functionality, and malware protection. To the software creator, the\
    \ app store provides a centralized marketplace for their app, where potential\
    \ users can find, purchase, and acquire the app easily; the app store also relieves\
    \ the developer from basic support problems related to distribution and installation,\
    \ since apps must be shown to install easily during the required approval process.\
    \ Indeed, one of the key side effects of mobile app stores is that it has forced\
    \ software developers to streamline their release management practices and ensure\
    \ hassle-free deployment at the user's end.\n\nThe success of mobile app stores\
    \ has also led to the establishment of a plethora of other kinds of app store,\
    \ often for non-mobile platforms, serving diverse kinds of user communities, offering\
    \ different kinds of services, and using a variety of monetization strategies.\
    \ Many technical platforms now operate in a store-centric way: essential services\
    \ and functionality are provided by the platform while access to extensions/add-ons\
    \ is offered only through interaction with the app store. For instance, Google\
    \ Play, the app store, operates on top of the technical platform Android, which\
    \ provides the runtime environment for the applications. When new technical platforms\
    \ are introduced, an app store is often expected to serve as a means to host and\
    \ deliver products to its users [1]. Example technical platforms that use app\
    \ store-like approaches\n\ninclude Steam [2], GitHub Marketplace [3], the Chrome\
    \ Web Store [4], Word-Press [5], AutoDesk [6], DockerHub [7], Amazon Web Services\
    \ (AWS) [8], Homebrew [9], or Ubuntu Packages [10].\n\nFor platforms that operate\
    \ in this way, the app store is an essential part of the platform's design. For\
    \ example, consider source code editors, such as VSCode and IntelliJ. The tool\
    \ itself — which we consider to be a technical platform in this context — offers\
    \ the essential functionality of a modern source code editor; however, many additional\
    \ services are available through the associated app store that are not included\
    \ by default. Thus, extensions that allow for language-specific syntax highlighting\
    \ or version control integration must be added manually by the user through interaction\
    \ with the tool's app store. We conjecture that the app store has fundamentally\
    \ changed how some classes of software systems are designed, from the overall\
    \ ecosystem architecture of the technical platform to the way in which add-ons\
    \ are engineered to fit within its instances.\n\nIn this work, we will explore\
    \ the general space of app stores, and also consider how app store-centric design\
    \ can affect software development practices. Previous research involving app stores\
    \ has focused mainly on mobile app stores, often concentrating on properties of\
    \ the apps rather than properties of the stores. For example, Harman et al. performed\
    \ one of the first major studies of app stores in 2012, focusing on the BlackBerry\
    \ App World [11]. However, concentrating the investigative scope so narrowly may\
    \ lead to claims that do not generalize well across the space of all app stores.\
    \ For example, Lin et al. found that reviews of games that appeared in mobile\
    \ app stores differed significantly from the reviews of the same game that appeared\
    \ within the Steam platform's own app store [12]. In our work, we aim to take\
    \ a more holistic approach to studying app stores by considering both mobile and\
    \ non-mobile variants. In so doing, we hope to create a more general model of\
    \ app stores that fits this broader space.\n\nTo achieve a holistic view, we start\
    \ from the definition of an app store. A precise definition of the term \"app\
    \ store\" has been omitted in much of the previous research in this area. Currently,\
    \ Google Play and Apple's App Store dominate the market and are the main targets\
    \ of research on app stores; in the past, the BlackBerry App World and Microsoft's\
    \ Windows Phone Store were also important players, but these stores are now defunct.<sup>1</sup>\
    \ Wikipedia recognizes Electronic AppWrapper [13] as the first true platform-specific\
    \ electronic marketplace for software applications, but the term became popular\
    \ when Apple introduced its App Store along with the iPhone 3G in 2008. Since\
    \ then, the term has largely come to refer to any centralized store for mobile\
    \ applications. We present our own working definition of the term \"app store\"\
    \ in Sec. 2.4.\n\nThe goal of this work is to survey and characterize the broader\
    \ dimensionality of app stores, and also to explore how and why they may feed\
    \ back into software development practices, such as release management. As a step\
    \ toward\n\n<sup>1</sup> The Windows Phone Store was absorbed into the broader\
    \ Windows Store in 2015.\n\nthis goal, we focus on two research questions (RQs)\
    \ that aim to explore the space of app stores:"
  token_usage: 6775
  time_usage: 1.6889889240264893
- title: "Formal Modelling of Safety Architecture for Responsibility-Aware\n  Autonomous\
    \ Vehicle via Event-B Refinement"
  abstract: 'Ensuring the safety of autonomous vehicles (AVs) is the key requisite
    for

    their acceptance in society. This complexity is the core challenge in formally

    proving their safety conditions with AI-based black-box controllers and

    surrounding objects under various traffic scenarios. This paper describes our

    strategy and experience in modelling, deriving, and proving the safety

    conditions of AVs with the Event-B refinement mechanism to reduce complexity.

    Our case study targets the state-of-the-art model of goal-aware

    responsibility-sensitive safety to argue over interactions with surrounding

    vehicles. We also employ the Simplex architecture to involve advanced black-box

    AI controllers. Our experience has demonstrated that the refinement mechanism

    can be effectively used to gradually develop the complex system over scenario

    variations.'
  url: http://arxiv.org/abs/2401.04875v1
  keywords: ''
  document: "# Formal Modelling of Safety Architecture for Responsibility-Aware Autonomous\
    \ Vehicle via Event-B Refinement<sup>⋆</sup>\n\nTsutomu Kobayashi1[0000−0002−8795−3183],\
    \ Martin Bondu<sup>2</sup> , and Fuyuki Ishikawa3[0000−0001−7725−2618]\n\n> 1\
    \ Japan Aerospace Exploration Agency, Tsukuba, Japan kobayashi.tsutomu@jaxa.jp\
    \ <sup>2</sup> Sorbonne University, Paris, France martin.bondu@etu.sorbonne-universite.fr\n\
    \n<sup>3</sup> National Institute of Informatics, Tokyo, Japan\n\nf-ishikawa@nii.ac.jp\n\
    \nAbstract. Ensuring the safety of autonomous vehicles (AVs) is the key requisite\
    \ for their acceptance in society. This complexity is the core challenge in formally\
    \ proving their safety conditions with AI-based black-box controllers and surrounding\
    \ objects under various traffic scenarios. This paper describes our strategy and\
    \ experience in modelling, deriving, and proving the safety conditions of AVs\
    \ with the Event-B refinement mechanism to reduce complexity. Our case study targets\
    \ the state-of-the-art model of goal-aware responsibility-sensitive safety to\
    \ argue over interactions with surrounding vehicles. We also employ the Simplex\
    \ architecture to involve advanced black-box AI controllers. Our experience has\
    \ demonstrated that the refinement mechanism can be effectively used to gradually\
    \ develop the complex system over scenario variations.\n\nKeywords: Autonomous\
    \ driving · AI safety · Responsibility-sensitive safety · Safety architecture\
    \ · Event-B · Refinement\n\n## 1 Introduction\n\nThe safety of automated vehicles\
    \ has been attracting increased interest in society. In addition to the intensive\
    \ effort of simulation-based testing, there is a key approach based on formal\
    \ reasoning called responsibility-sensitive safety (RSS) [\\[13\\]](#page-16-0).\
    \ RSS defines the minimum rules that traffic participants should comply with for\
    \ safety, i.e., no collisions. This rule-based approach has recently recently\
    \ been extended to goal-aware RSS (GA-RSS) to deal with the goal-achievement,\
    \ i.e., the driving goal of the ego-vehicle is eventually achieved such as pulling\
    \ over\n\n<sup>⋆</sup> The first author is supported by JSPS KAKENHI grant number\
    \ 19K20249 and JST ERATO-MMSD (JPMJER1603) project. The third author is supported\
    \ by JST MIRAI-eAI (JPMJMI20B8) project.\n\nupon emergency [\\[7\\]](#page-16-1).\
    \ GARSS is effective for formally limiting liabilities, which is vital for AV\
    \ manufacturers.\n\nThe challenge lies in deriving the necessary GARSS conditions\
    \ and formally checking the compliance of the design of the ego vehicle over various\
    \ scenarios under different environmental conditions. In addition, there is increasing\
    \ demand to consider complex behaviours of black-box AI-based advanced controllers\
    \ backed up with safety-ensured controllers, e.g., the Simplex architecture [\\\
    [10\\]](#page-16-2).\n\nExisting efforts have clarified the principles to derive\
    \ and argue conditions that ego-vehicles should comply with in example scenarios.\
    \ However, the engineering aspect has yet to be investigated. Specifically, we\
    \ need a systematic modelling design that accepts the flexibility to mitigate\
    \ the complexity in dealing with multiple aspects of scenario variations and architectural\
    \ design.\n\nTo this end, we report our experience in modelling, deriving, and\
    \ proving the safety conditions of autonomous vehicles (AVs). We follow the GA-RSS\
    \ approach to define and derive the safety conditions to be checked with architectural\
    \ design with black-box advanced controllers. We propose a strategy for using\
    \ the refinement mechanism of Event-B [\\[2\\]](#page-16-3) to gradually argue\
    \ the complex aspects including the scenario variations. Our experience has shown\
    \ the potential of the refinement mechanism for the flexible design of models\
    \ and proofs to mitigate the complexity in a gradual manner. To the best of our\
    \ knowledge, this is the first attempt to focus on the model engineering aspect\
    \ over scenario variations in the deductive approach for AV safety.\n\nThe rest\
    \ of this paper is structured as follows: In § [2,](#page-1-0) we describe the\
    \ safety architecture, RSS, and Event-B. § [3](#page-3-0) introduces GA-RSS and\
    \ a case study example. We elaborate on our approach and its application to the\
    \ case studies in § [4–](#page-6-0)[5.](#page-11-0) We discuss the approach in\
    \ § [6](#page-13-0) before concluding the paper in § [7.](#page-15-0)\n\n## <span\
    \ id=\"page-1-0\"></span>2 Preliminaries\n\n### 2.1 Safety Architecture\n\nContemporary\
    \ software systems often have black-box modules, such as machine learning modules,\
    \ in which their safety is essentially difficult to verify.\n\nA safety architecture,\
    \ such as Simplex architecture (Fig. [1\\)](#page-2-0) [\\[10\\]](#page-16-2),\
    \ is a fundamental approach to guaranteeing the safety of such systems while benefitting\
    \ from the high performance and functionality of black-box modules. It models\
    \ interactions between a controller and a plant. The controller part has two different\
    \ controllers: the baseline controller (BC), which is designed to force safe behaviour,\
    \ and the advanced controller (AC), which aims at satisfying various requirements\
    \ (e.g., comfort and progress) in addition to safety. The decision module (DM)\
    \ switches between the BC and AC in accordance with the state of the plant. BC\
    \ may fail to satisfy requirements other than safety, but it has a simple white-box\
    \ behaviour enabling the safety to be easily verified. In contrast, although AC\
    \ usually gives better user experiences, guaranteeing its safety is difficult\
    \ due to its complicated black-box behaviour. For example, a typical\n\n![](_page_2_Figure_1.jpeg)\n\
    \n<span id=\"page-2-0\"></span>Fig. 1. Component-based simplex architecture [\\\
    [10\\]](#page-16-2)\n\nBC for an AV may drive by following a predefined rule that\
    \ is guaranteed to be safe in certain situations. A typical AC, on the other hand,\
    \ would be one that uses machine learning for motion planning.\n\n#### 2.2 Responsibility-Sensitive\
    \ Safety (RSS)\n\nRSS is an approach to determining the safety of AVs by formal\
    \ proof. The core idea is to derive conditions that should be satisfied by the\
    \ current state of the traffic participants such that safety, or no collisions,\
    \ is ensured in the future.\n\nAn RSS rule consists of an assertion φ called an\
    \ RSS condition and a control strategy α called a proper response. They are defined\
    \ for particular traffic scenarios. For example, a subject vehicle (SV), i.e.,\
    \ the ego vehicle, is following a preceding vehicle on a one-way road. We consider\
    \ this preceding vehicle as the sole traffic participant called a principal other\
    \ vehicle (POV). The SV must satisfy the RSS condition φ regarding the minimum\
    \ relative distance from the POV. The distance is defined by considering the response\
    \ time for braking and the distance necessary for the maximum comfortable braking\
    \ to stop. The proper response α of the SV is to engage the maximum comfortable\
    \ braking when the distance condition φ is about to be violated. The proof should\
    \ show the RSS condition φ is preserved through the execution with the proper\
    \ response α.\n\nIn a general setting, RSS considers the SV and POV in the target\
    \ scenario and determines the RSS condition and proper response. To prove the\
    \ condition is preserved through the execution, a certain set of constraints must\
    \ be satisfied by not only the SV but also all traffic participants (POVs), called\
    \ RSS responsibility principles. Examples of the principles include \"do not cut\
    \ in recklessly\" and \"be cautious in areas with limited visibility\", intuitively.\n\
    \nOur focus is not on the core responsibility principles of RSS but on the RSSdriven\
    \ framework for proving safety of AVs. We are interested in the formal engineering\
    \ aspect to model and verify scenario variations.\n\n### 2.3 Modelling and Proving\
    \ in Event-B\n\nIn this section, we describe the concepts of modelling and theorem\
    \ proving in Event-B [\\[2\\]](#page-16-3) that are used in our case study [4](#page-2-1)\
    \ .\n\n<span id=\"page-2-1\"></span><sup>4</sup> For simplicity, we do not cover\
    \ the \"full\" Event-B (described in [\\[2\\]](#page-16-3)). For instance, our\
    \ concrete machines inherit all variables and parameters from abstract machines,\
    \ which is not necessary in general Event-B machines.\n\n![](_page_3_Figure_1.jpeg)\n\
    \n<span id=\"page-3-1\"></span>Fig. 2. Structure of Event-B model components\n\
    \nEvent-B Model Components. Event-B models are structured as shown in Fig. [2.](#page-3-1)\
    \ The static aspects of the target system are specified as contexts, which consist\
    \ of constants and their properties (axioms). The dynamic aspects are specified\
    \ as machines, which consist of variables, invariant predicates, and a set of\
    \ events. An event e has parameters pe, guard condition Ge, and before-after predicate\
    \ BA<sup>e</sup> that explains the assignment performed in e in terms of variables'\
    \ current values v and next values v ′ . A significant feature of Event-B is a\
    \ flexible refinement mechanism that enables declaring a machine M<sup>c</sup>\
    \ as a refinement of another machine Ma. Every event in M<sup>c</sup> should be\
    \ seen as a refinement of events in M<sup>a</sup> (including the implicit skip\
    \ event). M<sup>c</sup> does not need to inherit predicates of Ma, but those two\
    \ machines should be compatible as described in the following.\n\nProving Consistency\
    \ of Models. Constructed models should be verified by discharging proof obligations\
    \ (POs) generated with predicates in the models. Primary POs include the following:\n\
    \n- Invariant Preservation (for an abstract machine): Invariant predicates are\
    \ inductive ones, i.e., they must hold after every occurrence of events, given\
    \ that they hold beforehand. Formally, invariant preservation by an event e<sup>a</sup>\
    \ is: A(c) ∧ Ia(c, va) ∧ G<sup>e</sup><sup>a</sup> (c, va, p<sup>e</sup><sup>a</sup>\
    \ ) ∧ BA<sup>e</sup><sup>a</sup> (c, va, p<sup>e</sup><sup>a</sup> , v′ a ) ∧\
    \ . . . =⇒ Ia(c, v′ a ).\n- Invariant Preservation (for concrete machines): Formally,\
    \ invariant preservation by an event e<sup>c</sup> is: A(c) ∧ Ia(c, va) ∧ Ic(c,\
    \ va, vc) ∧ G<sup>e</sup><sup>c</sup> (c, vc, p<sup>e</sup><sup>c</sup> ) ∧ BA<sup>e</sup><sup>c</sup>\
    \ (c, vc, p<sup>e</sup><sup>c</sup> , v′ c ) ∧ . . . =⇒ Ic(c, v′ a , v′ c ).\n\
    - Guard Strengthening: For an event e<sup>c</sup> to be a refinement of an event\
    \ ea, the guard of e<sup>c</sup> must be stronger than that of ea's. Formally,\
    \ guard strengthening of e<sup>c</sup> is: A(c)∧Ic(c, va, vc)∧Ia(c, va)∧Ge<sup>c</sup>\
    \ (c, vc, pe<sup>c</sup> )∧. . . =⇒ G<sup>e</sup><sup>a</sup> (c, va, p<sup>e</sup><sup>a</sup>\
    \ ).\n\n## <span id=\"page-3-0\"></span>3 Example: Goal-Aware RSS for Pull Over\
    \ Scenario\n\nGoal-aware RSS (GA-RSS) [\\[7\\]](#page-16-1) is an extension of\
    \ RSS for dealing with complex scenarios that require planning over multiple manoeuvres\
    \ to achieve particular\n\n![](_page_4_Figure_1.jpeg)\n\n<span id=\"page-4-0\"\
    ></span>Fig. 3. Pull over scenario [\\[7\\]](#page-16-1)\n\ngoals. For instance,\
    \ consider the scenario shown in Fig. [3](#page-4-0) (pull over scenario) [\\\
    [7\\]](#page-16-1): the SV needs to stop at a designated location (xT gt) on the\
    \ shoulder lane while keeping safe distances from POVs as required by RSS. Following\
    \ only the original RSS rules for avoiding collisions is necessary but not enough\
    \ to achieve the goal. The goal should be decomposed into several subgoals, such\
    \ as (1) getting ready to merge between two POVs by changing the velocity, (2–3)\
    \ changing lanes, and (4) stopping at xT gt. Different proper responses are required\
    \ for different subgoals as well. However, for example, the SV can be trapped\
    \ in Lane 1 if it is concerned about only the distance from the car ahead.\n\n\
    The workflow of GA-RSS is based on their extension of Floyd-Hoare logic. Given\
    \ a driving scenario S composed of the goal condition Goal and safety condition\
    \ Safety, the workflow is first used to decompose S into subscenarios S1,...,n\
    \ and identify the proper response α<sup>i</sup> for each subscenario S<sup>i</sup>\
    \ . [5](#page-4-1)\n\nThen, the precondition φ<sup>i</sup> for each subscenario\
    \ is calculated as the precondition for establishing Goali∧φ<sup>i</sup>+1 while\
    \ satisfying Safety<sup>i</sup> , by performing α<sup>i</sup> . Here, by seeing\
    \ the (grand) goal of S as the postcondition of the final subscenario Sn, the\
    \ preconditions of all subscenarios are derived in a backward manner, à la Floyd-Hoare\
    \ logic, and then integrated into the precondition of S.\n\nFor instance, Fig.\
    \ [4](#page-5-0) shows the subgoals, safety conditions, proper responses, and\
    \ preconditions of a subscenario chain (defined and derived in [\\[7\\]](#page-16-1))\
    \ where the SV goes between POV1 and POV2 and changes lanes.\n\nVariables are\
    \ as follows: xSV and x1,2,<sup>3</sup> are the lateral positions of the SV and\
    \ the three POVs; vSV and v1,2,<sup>3</sup> are their lateral velocities; aSV\
    \ and a1,2,<sup>3</sup> are their lateral acceleration rates; L and L1,2,<sup>3</sup>\
    \ for set of lanes they are on. Constants are as follows: xT gt is the position\
    \ of the final goal position; vmin and vmax are the legal speed limits; bmin and\
    \ bmax are the minimum (comfortable) and maximum (emergency) braking deceleration\
    \ rates; amax is the maximum acceleration rate.\n\n<span id=\"page-4-1\"></span><sup>5</sup>\
    \ To be precise, with case distinctions, a tree of subscenarios is derived.\n\n\
    | Subscenario S4 (Stop at the target)                                        \
    \                                                                            \
    \                                                                            \
    \                      | Subgoal Goal4: xSV = xT gt ∧ vSV = 0                \
    \                       |\n|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------|\n\
    | Safety Safety4<br>: L = {3} ∧ 0 ≤ vSV ≤ vmax ∧ −bmin ≤ aSV ≤ amax<br>Proper\
    \ Response α4: Cruise for timeT oCruise4(xSV 0, vSV 0), then<br>brake with bmin\
    \ for timeT oBrake4(vSV 0)<br>2<br>Precondition φ4: Env ∧ L = {3} ∧ xT gt − xSV\
    \ ≥ v<br>SV /2bmin |                                                         \
    \                   |\n| Subscenario S3 (Change to Lane 3)                   \
    \                                                                            \
    \                                                                            \
    \                                             | Subgoal Goal3: L = {3}       \
    \                                              |\n| Safety Safety3<br>−bmin ≤\
    \ aSV ≤ amax                                                                 \
    \                                                                            \
    \                                                                        | : (L\
    \ = {3} ∨ L = {2, 3}) ∧ 0 ≤ vSV ≤ v2 ∧ x2 − xSV ≥ dRSS(v2, vSV ) ∧      |\n| Proper\
    \ Response α3: Cruise for timeT oCruise3(xSV 0, vSV 0,), then<br>brake with bmin\
    \ for timeT oBrake3(xSV 0, vSV 0,)                                           \
    \                                                                            \
    \            |                                                               \
    \             |\n| 2<br>xT gt − xSV ≥ v<br>SV /2bmin                         \
    \                                                                            \
    \                                                                            \
    \                                       | Precondition φ3: Env ∧ L = {2} ∧ 0 <\
    \ vSV ≤ v2 ∧ x2 − xSV ≥ dRSS(v2, vSV )∧ |\n| Subscenario S2 (Change to Lane 2)\
    \                                                                            \
    \                                                                            \
    \                                                                | Subgoal Goal2:\
    \ lanes = {2}                                                 |\n| Safety Safety2<br>x3\
    \ − xSV ≥ dRSS(v3, vSV ) ∧ −bmin ≤ aSV ≤ amax                                \
    \                                                                            \
    \                                                                            \
    \ | : (L = {2} ∨ L = {1, 2}) ∧ 0 ≤ vSV ≤ v2 ∧ x2 − xSV ≥ dRSS(v2, vSV ) ∧    \
    \  |\n| Proper Response α2: Cruise for timeT oCruise2(xSV 0, vSV 0,), then<br>brake\
    \ with bmin for timeT oBrake2(xSV 0, vSV 0,)                                 \
    \                                                                            \
    \                      |                                                     \
    \                       |\n| Precondition φ2:                                \
    \                                                                            \
    \                                                                            \
    \                                                 |                          \
    \                                                  |\n\nSubscenario S<sup>1</sup>\
    \ (Get ready to merge)\n\nSubgoal Goal1: x<sup>2</sup> − xSV ≥ dRSS(v2, vSV )\
    \ ∧ xSV − x<sup>1</sup> ≥ dRSS(vSV , v1) ∧ v<sup>2</sup> = vSV Safety Safety<sup>1</sup>\
    \ : L = {1} ∧ x<sup>3</sup> − xSV ≥ dRSS(v3, vSV ) ∧ 0 ≤ vSV ≤ vmax ∧ −bmin ≤\
    \ aSV ≤ amax\n\nProper Response α1: Combinations of acceleration, cruising, and\
    \ braking depending on the situation. See § [6.2](#page-13-1) for details.\n\n\
    Precondition φ1: . . .\n\n<span id=\"page-5-0\"></span>Fig. 4. Subscenarios of\
    \ pull over scenario with proper response and precondition\n\nThe condition of\
    \ environment Env is as follows:\n\n$$\\begin{aligned} \\mathsf{Env} &= \\bigwedge\\\
    _{i=1,2,3} (v\\_{min} \\le v\\_i \\le v\\_{max} \\land a\\_i = 0) \\\\ &\\wedge\
    \ L\\_1 = \\{2\\} \\land L\\_2 = \\{2\\} \\land L\\_3 = \\{1\\} \\land x\\_2 >\
    \ x\\_1. \\end{aligned}$$\n\nThis condition includes the assumption that POVs\
    \ are supposed to run at constant velocity.\n\nThe RSS safety distance that the\
    \ SV running at vSV should keep from the POVi ahead running at v<sup>i</sup> is\
    \ defined as follows:\n\n$$\\text{dPRS}(v\\_i, v\\_{SV}) = \\max\\left(0, \\frac{v\\\
    _{SV}^2}{2b\\_{min}} - \\frac{v\\_i^2}{2b\\_{max}}\\right). \\tag{1}$$\n\nThe\
    \ times the SV should cruise, brake, or accelerate in subscenario S<sup>i</sup>\
    \ for proper response α<sup>i</sup> are derived in the GA-RSS workflow [\\[7\\\
    ]](#page-16-1). For instance,\n\n$$timeToCruise\\_4(x\\_{SV0}, v\\_{SV0}) = \\\
    frac{x\\_{Tgt} - x\\_{SV0}}{v\\_{SV0}} - \\frac{v\\_{SV0}}{2b\\_{min}},\\tag{2}$$\n\
    \n$$timeToBrake\\_4(v\\_{SV0}) = \\frac{v\\_{SV0}}{b\\_{min}},\\tag{3}$$\n\nwhere\
    \ xSV <sup>0</sup> and vSV <sup>0</sup> are the position and velocity of the SV,\
    \ respectively, when the switching occurs.\n\nGA-RSS is designed to be integrated\
    \ with the Simplex architecture. The identified scenarios are used to construct\
    \ the BC that performs the derived proper response α in the situation compatible\
    \ with the scenario, and thus the BC is guaranteed to be safe and goal-achieving.\
    \ While the correctness of the DM is not covered with the method in [\\[7\\]](#page-16-1),\
    \ their experiment used their implementation of a Simplex-based controller, where\
    \ the AC is black-box.\n\nMotivation of our case study. Even with the BC specifications\
    \ identified with the GA-RSS workflow, a formal model of the whole Simplex architecture\
    \ closer to the implementation is desired to construct safe and goal-achieving\
    \ controllers of AVs. Such models should at least take into account the behaviour\
    \ of the DM and the monitor-decide-control loop (Fig. [1\\)](#page-2-0).\n\nThe\
    \ challenge here is the model's complexity; for example, in addition to DM-related\
    \ elements, we need to take switching time delays into consideration.\n\nTo overcome\
    \ this, we exploit the refinement mechanism of Event-B, which distributes the\
    \ complexity of modelling and verification over multiple steps.\n\nThe rest of\
    \ this paper discusses our case study, where we constructed and verified Event-B\
    \ models of Simplex-based controllers for pull over subscenarios.\n\n## <span\
    \ id=\"page-6-0\"></span>4 Case Study 1: Modelling Subscenario S<sup>4</sup>\n\
    \nIn this section, we introduce our modelling strategy, where elements of systems\
    \ should be specified in each refinement step by using our model for subscenario\
    \ S<sup>4</sup> of the pull over scenario as an example. We model the entire safety\
    \ architecture and verify its safety in three refinement steps as follows:\n\n\
    Machine M4,0: Whole controller-level. This is the most abstract machine. The properties\
    \ of the whole controller's\n\n(AC+BC+DM) behaviour at every cycle are modelled.\
    \ We focus on physical requirements that should be satisfied due to the controller's\
    \ behaviour.\n\n- Machine M4,1: Module-level. This machine refines M4,0. This\
    \ machine is aware of the safety architecture; behavioural properties of AC, BC,\
    \ and DM are specified separately. We checked that the switching by the DM satisfies\
    \ the requirements in M4,<sup>0</sup> by proving the correctness of M4,0–M4,<sup>1</sup>\
    \ refinement.\n- Machine M4,2: Manoeuvre-level. This machine refines M4,1. Details\
    \ of the BC's behaviour (proper responses) are specified. By checking the correctness\
    \ of M4,1–M4,<sup>2</sup> refinement, we check that the proper responses satisfy\
    \ the requirements.\n\n<span id=\"page-6-2\"></span><span id=\"page-6-1\"></span>\n\
    \n```\nvar iab les xSV , vSV\nEvent initialisation\n   any ( ) where ⊤ then\n\
    \     init_sv : (x\n                  ′\n                  SV , v′\n         \
    \              SV ) =\n          (xSV 0, vSV 0) end\n                        \
    \             invar iants\n                                    types : xSV ∈ R\
    \ ∧ vSV ∈ R\n                                    no_overrun : 0 ≤ xSV ≤ xT gt\n\
    \                                    v_regulated : 0 ≤ vSV ≤ vmax\n          \
    \                          precond : xT gt − xSV ≥ v\n                       \
    \                                       2\n                                  \
    \                            SV /2bmin\nEvent run\n  any px , pv where\n     preserve_no_overrun\
    \ : 0 ≤ px ≤ xT gt\n     preserve_v_regulated : 0 ≤ pv ≤ vmax\n     preserve_precond\
    \ : xT gt − px ≥ p\n                                       2\n               \
    \                        v/2bmin\n     x_physical_constr : xSV ≤ px ≤ xSV +\n\
    \                                             R 1\n                          \
    \                    t=0(vSV + amaxt)dt\n     v_physical_constr : vSV −\n    \
    \                             R 1\n                                  t=0 bmaxdt\
    \ ≤ pv ≤ vSV +\n                                                           R 1\n\
    \                                                            t=0 amaxdt\n  then\n\
    \     update_xv : (x\n                    ′\n                    SV , v′\n   \
    \                      SV ) = (px, pv) end\n```\n<span id=\"page-7-0\"></span>Fig.\
    \ 5. M<sup>4</sup>,<sup>0</sup>: Abstract, whole controller-level machine for\
    \ subscenario S<sup>4</sup>\n\n# 4.1 Machine <sup>M</sup>4,0: Whole Controller-Level\
    \ Behaviour\n\nMachine M4,<sup>0</sup> is shown in Fig. [5.](#page-7-0) In this\
    \ machine, we abstract away details of the controller and focus on the SV's position\
    \ (xSV ) and velocity (vSV ) as the result of the controller's behaviour.\n\n\
    Invariant predicates no\\_overrun and v\\_regulated express basic requirements.\n\
    \nThe precondition φ<sup>4</sup> derived from the GA-RSS workflow is designed\
    \ to be an invariant that the safety architecture should preserve; the DM enables\
    \ using the AC while φ<sup>4</sup> is robustly satisfied, but it switches to the\
    \ control using the BC once φ<sup>4</sup> is about to be violated. Therefore,\
    \ we specify φ<sup>4</sup> as an invariant predicate (precond).\n\nThere is only\
    \ a single non-initialisation event named run. It has parameters p<sup>x</sup>\
    \ and pv, which are specified as values of xSV and vSV at the next cycle (update\\\
    _xv). The parameters are constrained by the guard predicates preserve\\_\\* required\
    \ for the event's invariant preservation and those for the constraints related\
    \ to physics (\\*\\_physical\\_constr). With these constraints as guard predicates\
    \ of the event, we declare that every detailed behavioural description specified\
    \ as events in concrete machines (M4,<sup>1</sup> and M4,2) should satisfy the\
    \ constraints.\n\nThe guard predicate preserve\\_precond states that the controller\
    \ somehow produces the result (i.e., p<sup>x</sup> and pv) such that precond is\
    \ satisfied. Indeed, the preservation of the precondition φ<sup>4</sup> is trivial\
    \ because:\n\n$$\\begin{aligned} \\left(x\\_{Tgt} - p\\_x \\ge p\\_v^2 / 2b\\\
    _{min}\\right) \\land & \\dots \\land \\left(\\left(x\\_{SV}', v\\_{SV}'\\right)\
    \ = \\left(p\\_x, p\\_v\\right)\\right) \\\\ \\implies x\\_{Tgt} - x\\_{SV}' \\\
    ge v\\_{SV}'^2 / 2b\\_{min} .\\end{aligned}$$\n\nNote that how the controller\
    \ works to produce the invariant-satisfying result is not yet specified and deferred\
    \ to concrete machines; how the DM prevents the\n\n```\nvar iab les xSV , vSV\
    \ , ctrl , vBC0\ninvar iants\ntypes : ctrl ∈ {AC, BC} ∧ vBC0 ∈ R\nvsvbcinit_regulated\
    \ : 0 ≤ vBC0 ≤ vmax\nbc_no_accel : ctrl = BC =⇒ vSV ≤ vBC0\nswitching : ctrl =\
    \ AC =⇒ φ4(xSV +\n                                     R 1\n                 \
    \                     t=0(vSV + amaxt)dt, vSV +\n                            \
    \                                      R 1\n                                 \
    \                                  t=0 amaxdt)\nEvent AC → BC r e f in e s run\n\
    \  any px , pv where\n     . . . ( guard p r e d i c a t e s o f run e x c e p\
    \ t preserve_precond ) . . .\n     AC_operating : ctrl = AC\n     maybe_unsafe_next\
    \ : ¬φ4(xSV +\n                                       R 2\n                  \
    \                      t=0(vSV + amax)dt, vSV +\n                            \
    \                                      R 2\n                                 \
    \                                  t=0 amaxdt)\n  then\n     . . . ( a c t i o\
    \ n s o f run) . . .\n     switch_to_bc : ctrl′ = BC\n     vsvbcinit_update :\
    \ v\n                           ′\n                           BC0 = pv end\nEvent\
    \ BC → AC r e f in e s run\n  any px , pv where\n     . . . ( guard p r e d i\
    \ c a t e s o f run) . . .\n     BC_operating : ctrl = BC\n     no_acceleration\
    \ : pv ≤ vBC0\n     surely_safe_next : φ4(xSV +\n                            \
    \        R 2\n                                     t=0(vSV + amax)dt, vSV +\n\
    \                                                               R 2\n        \
    \                                                        t=0 amaxdt)\n  then\n\
    \     . . . ( a c t i o n s o f run) . . .\n     switch_to_ac : ctrl′ = AC end\n\
    ```\n<span id=\"page-8-0\"></span>Fig. 6. (A part of) M<sup>4</sup>,<sup>1</sup>:\
    \ Intermediate, module-level machine for subscenario S<sup>4</sup>\n\nAC from\
    \ violating it is specified in machine M4,1, and how the BC's behaviour (proper\
    \ responses) satisfies it is specified in machine M4,2.\n\n# <span id=\"page-8-1\"\
    ></span>4.2 Machine <sup>M</sup>4,1: Module-Level Behaviour\n\nIn machine M4,<sup>1</sup>\
    \ (Fig. [6\\)](#page-8-0), which refines M4,0, we focus on the requirements on\
    \ white-box modules of the architecture, namely the BC and DM, particularly the\
    \ condition for switching; through the proof attempt, we derived the switching\
    \ condition such that the precondition is always satisfied. Note that we assume\
    \ that the AC's behaviour is arbitrary as long as it satisfies run's guard. Details\
    \ of the BC's behaviour that should be specified using the time spent for each\
    \ manoeuvre are introduced in machine M4,2.\n\nThere are two new variables: ctrl,\
    \ for the currently active controller, and vBC0, which stores the velocity at\
    \ the time when switching to the BC occurs.\n\nInvariant predicates are in regard\
    \ to the requirements on the BC and DM. vsvbcinit\\_regulated requests that vBC<sup>0</sup>\
    \ should not exceed vmax like vSV , and bc\\_no\\_accel expresses that the BC\
    \ does not accelerate in the proper response.\n\nswitching states that if the\
    \ AC is active, then the SV will be goal-achieving and safe after a cycle even\
    \ if the SV accelerated with the maximum rate amax. The contraposition of switching\
    \ means that the BC is used if the precondition φ<sup>4</sup> may be violated\
    \ at the next cycle.\n\nThere are four events for cases of switching: AC → AC,\
    \ AC → BC, BC → BC, and BC → AC. They all refine the run event of the previous\
    \ machine M4,0. For instance, AC → BC is for the case where the current controller\
    \ is the AC (AC\\_operating) and switching can be violated after the event (maybe\\\
    _unsafe\\_next; note that the integrals are from t = 0 to 2 to look ahead for\
    \ two cycles). Note that, however, switching is guaranteed to hold before the\
    \ event since it is an invariant predicate. In addition to actions of run, the\
    \ controller is switched to the BC (switch\\_to\\_bc) and vBC<sup>0</sup> is updated\
    \ (vsvbcinit\\_update). On the other hand, BC → AC is the case where the controller\
    \ is switched from the BC to AC because the invariant switching will be satisfied\
    \ after the occurrence of the event (surely\\_safe\\_next).\n\nThe main POs are\
    \ as follows:\n\n1. Do the events AC → ∗ preserve the invariant precond? This\
    \ corresponds to the guard strengthening PO of AC → ∗. The intuition of the proof\
    \ is because the AC is operating only if the precondition is guaranteed to hold\
    \ after two cycles (surely\\_safe\\_next), and it is guaranteed to hold after\
    \ one cycle as well.\n\n2. Do events ∗ → AC preserve the invariant switching?\
    \ It is preserved because the AC will be used only if surely\\_safe\\_next holds\
    \ at the current state. In fact, we derived the switching condition surely\\_safe\\\
    _next through the attempt to discharge this PO.\n\n# 4.3 Machine <sup>M</sup>4,2:\
    \ Manoeuvre-Level Behaviour\n\nIn machine M4,<sup>2</sup> (Fig. [7\\)](#page-10-0),\
    \ which refines M4,1, we focus on the details of the behaviour with the notion\
    \ of time to spend on each manoeuvre to verify that the BC's behaviour satisfies\
    \ the requirements specified in machines M4,<sup>0</sup> and M4,1.\n\nTwo new\
    \ variables about the remaining time for cruising (tBCCruise) and braking (tBCBrake)\
    \ are introduced. The unit of time here is the cycle, e.g., the value of tBCCruise\
    \ is the number of the controller's cycles spent for cruising.\n\nInvariant predicates\
    \ are in regard to the detailed properties of the BC's behaviour: cruise\\_before\\\
    _brake expresses that the proper response α<sup>4</sup> is cruising and then braking,\
    \ and \\*\\_in\\_BC\\* states that the velocity and position should follow the\
    \ proper response α<sup>4</sup> as shown in Fig. [8.](#page-11-1)\n\nEvents of\
    \ M4,<sup>2</sup> refine those of M4,<sup>1</sup> as shown in Fig. [9.](#page-11-2)\n\
    \nThree events that refine AC → ∗ are mostly the same as M4,1, but events regarding\
    \ switching to the BC (such as AC\\_run → BC) are extended with actions of calculating\
    \ tBCCruise and tBCBrake as equations [2](#page-6-1) and [3](#page-6-2) (derived\
    \ in the GA-RSS workflow) because the BC should calculate them every time it get\
    \ activated.\n\nUnlike events that refine AC → ∗, six events that refine BC →\
    \ ∗ do not inherit all of the guard predicates and actions of corresponding events\
    \ in machine M4,1. For example, the differences between the event BC\\_cruise\
    \ → AC in M4,<sup>2</sup> and\n\n![](_page_10_Figure_1.jpeg)\n\n<span id=\"page-10-0\"\
    ></span>Fig. 7. (A part of) M<sup>4</sup>,<sup>2</sup>: Concrete, manoeuvre-level\
    \ machine for subscenario S<sup>4</sup>\n\nthe corresponding event BC → AC in\
    \ M4,<sup>1</sup> is as shown in Fig. [10.](#page-12-0) The removed guard predicates\
    \ (lines with red background) are requirements on the values of the SV's position\
    \ and velocity after the occurrence of the event (p<sup>x</sup> and pv), while\
    \ introduced guard predicates (lines with green background) include the concrete\
    \ behaviour of the BC (cruise\\_xv), namely running with the constant velocity.\
    \ By changing events in this way and checking that the guard of BC\\_cruise →\
    \ AC is stronger than that of BC → AC, we can verify that the BC's concrete behaviour\
    \ satisfies the requirements specified in machines M4,<sup>0</sup> and M4,1.\n\
    \nIn addition to the consistency between the BC's concrete behaviour specified\
    \ in M4,<sup>2</sup> and requirements on the BC specified in M4,1, we checked\
    \ that events ∗ → BC and BC → ∗ preserve the invariant.\n\n![](_page_11_Figure_1.jpeg)\n\
    \n<span id=\"page-11-2\"></span><span id=\"page-11-1\"></span>\n\nFig. 8. Proper\
    \ response α<sup>4</sup> Fig. 9. Event refinement relationship\n\n## <span id=\"\
    page-11-0\"></span>5 Case Study 2: Modelling Subscenario S<sup>3</sup>\n\nIn this\
    \ section, we use subscenario S<sup>3</sup> to demonstrate how our modelling strategy\
    \ (§ [4\\)](#page-6-0) is applicable to other subscenarios. subscenario S<sup>3</sup>\
    \ has new aspects; the SV is changing lanes and the leading vehicle POV2.\n\n\
    # 5.1 Machine <sup>M</sup>3,0: Whole Controller-Level Behaviour\n\nFollowing machine\
    \ M4,<sup>0</sup> of subscenario S4, we focus only on the physical results of\
    \ the controller behaviour.\n\nPOV2's variable position (x2) and constant velocity\
    \ (v2) are used in addition to SV's position and velocity.\n\nAs the SV is changing\
    \ lanes, we assume that this action will be done in an exact amount of time modelled\
    \ as a constant tLC (the time for lane changing), and therefore we introduce another\
    \ variable tLCe (the time for lane changing elapsed) so that when the time elapsed\
    \ reaches tLC , the SV should have finished switching lanes and the subscenario\
    \ is over. We modelled lanes in this style instead of introducing another physical\
    \ coordinate for simplicity.\n\nA new invariant predicate no\\_overtime regarding\
    \ the time limit of this subscenario is also introduced as a replacement for no\\\
    _overrun of subscenario S4. The corresponding guard predicates of the event run\
    \ are specified so that no event can occur once the lane switching is over.\n\n\
    $$\\text{no\\\\_overturetime: } t\\_{LCe} \\le t\\_{LC}$$\n\nThe precondition\
    \ for subscenario S<sup>3</sup> (φ<sup>3</sup> derived in [\\[7\\]](#page-16-1))\
    \ takes into consideration the RSS safety distance between the SV and the leading\
    \ vehicle POV2.\n\n$$\\begin{array}{c} \\hline \\textbf{precond:} \\ x\\_{Tgt}\
    \ - x\\_{SV} \\ge v\\_{SV}^2 / 2b\\_{min} \\land x\\_{SV} < x\\_2\\\\ \\land 2(x\\\
    _{SV} - x\\_2) + \\frac{v\\_{SV}^2}{b\\_{min}} \\le \\frac{v\\_2^2}{b\\_{max}}\
    \ \\end{array}$$\n\nAs in subscenario S4, the run event has guard predicates to\
    \ preserve invariant predicates. The event also has new actions for updating x<sup>2</sup>\
    \ and tLCe:\n\n$$\\begin{array}{|c|}\\hline \\text{update\\\\_xLead: } x\\_2'\
    \ = x\\_2 + \\int\\_{t=0}^1 (v\\_2 t) dt\\\\ \\text{update\\\\_xLС: } t\\_{LCe}'\
    \ = \\min(t\\_{LC}, t\\_{LCe} + 1) \\\\\\hline \\end{array}$$\n\n![](_page_12_Figure_1.jpeg)\n\
    \n<span id=\"page-12-0\"></span>Fig. 10. Differences between BC → AC (in M<sup>4</sup>,<sup>1</sup>)\
    \ and BC\\_cruise → AC (in M<sup>4</sup>,<sup>2</sup>)\n\n# 5.2 Machine <sup>M</sup>3,1:\
    \ Module-Level Behaviour\n\nThis machine is also similar to M4,1, but the invariant\
    \ switching and guard predicates surely\\_safe\\_next (and its negation maybe\\\
    _unsafe\\_next) take into account the distance between the SV and POV2.\n\n$$\\\
    begin{array}{c} \\text{switching:} \\; ctl = AC \\Longrightarrow \\phi\\_3(x\\\
    _{SV} + \\int\\_{t=0}^1 (v\\_{SV} + a\\_{max}t)dt, \\\\\\ v\\_{SV} + \\int\\_{t=0}^1\
    \ a\\_{max}dt, \\; x\\_2 + \\int\\_{t=0}^1 (v\\_2t)dt, \\; v\\_2) \\\\\\ \\hline\
    \ \\text{surely\\\\_safe\\\\_next:} \\; \\phi\\_3(x\\_{SV} + \\int\\_{t=0}^2 (v\\\
    _{SV} + a\\_{max}t)dt, \\; v\\_{SV} + \\int\\_{t=0}^2 a\\_{max}dt, \\\\\\ x\\\
    _2 + \\int\\_{t=0}^2 (v\\_2t)dt, \\; v\\_2) \\end{array}$$\n\nAs subscenario S<sup>4</sup>\
    \ (§ [4.2\\)](#page-8-1), the POs are in regard to the preservations of invariants\
    \ precond and switching.\n\n# 5.3 Machine <sup>M</sup>3,2: Manoeuvre-Level Behaviour\n\
    \nCompared with M4,<sup>2</sup> for subscenario S4, there are two major differences:\
    \ when switching to the BC, the calculation of tBCCruise and tBCBrake (derived\
    \ in [\\[7\\]](#page-16-1)) is different because the velocity of the SV should\
    \ not be zero by the end of the subscenario S<sup>3</sup> but only low enough\
    \ to satisfy the goal invariant.\n\n![](_page_12_Picture_9.jpeg)\n\nThe six events\
    \ that refine BC\\_∗ → ∗ have to satisfy machine M3,0's precond that now includes\
    \ the safety distance to the leading vehicle POV2.\n\nThe POs in regard to this\
    \ invariant were discharged in the following way:\n\n- 1. BC\\_∗ → BC The idea\
    \ behind this proof is that BC's proper response does not include accelerating\
    \ and the leading vehicle's velocity is constant, so the distance between these\
    \ two may only increase.\n- 2. BC\\_∗ → AC The guard predicate surely\\_safe\\\
    _next states that the invariant will be satisfied in two cycles without having\
    \ to break in the next cycle because the controller will be in the AC.\n\n## <span\
    \ id=\"page-13-0\"></span>6 Discussion\n\n### 6.1 Model Engineering\n\nIn the\
    \ case studies, we have used the refinement mechanism of Event-B to gradually\
    \ model and verify the different aspects. Specifically, we separated the argument\
    \ over the definition of safe and goal-achieving behaviour, architecture for switching\
    \ behaviours, and concrete behaviour design. The refinement mechanism limits the\
    \ complexity of modelling and proof in each step, which was essential in handling\
    \ the increasing complexity in proving continuous properties.\n\nWe did not directly\
    \ reuse the models between subscenarios, e.g., sharing the abstract steps between\
    \ subscenarios. This is our explicit choice as the key safety properties and involved\
    \ variables for the POVs are unique to each subscenario. We instead used the common\
    \ refinement strategy as well as the model representations. We believe this experience\
    \ enables us to demonstrate the know-how for scenarios other than the pull over\
    \ scenario. The generality of the approach is further discussed in the following.\n\
    \n#### <span id=\"page-13-1\"></span>6.2 Generality of Approach\n\nWe have described\
    \ how the same refinement strategy can deal with subscenarios S<sup>3</sup> and\
    \ S4. We describe how the other subscenarios can be modelled as well as the omitted\
    \ aspect of perception errors.\n\nSubscenario S2. The machines for subscenario\
    \ S<sup>2</sup> are similar to that for subscenario S3. The main difference between\
    \ them is the presence of a leading vehicle in the next lane in subscenario S<sup>2</sup>\
    \ while there is none in subscenario S3.\n\nSubscenario S1. In this subscenario,\
    \ the SV needs to prepare to switch lanes and merge into the next lane. There\
    \ are three POVs to take into account: one ahead of the SV in the current lane\
    \ (POV3) and two others in the next lane (POV 1 and 2). This subscenario thus\
    \ involves multiple (in this case, four) proper responses: an example is accelerating\
    \ to pass POV1 in the next lane, and another example is decelerating to match\
    \ the velocity of POV2 in the next lane. To handle multiple proper responses in\
    \ a unified manner, we modelled them as a sequence of proper responses with variable\
    \ durations as follows: (1) Accelerate for tBCAccel (2) Cruise for tBCCruise (3)\
    \ Brake for tBCBrake. Moreover, we needed to take into account different precondition\
    \ for each proper response. Therefore, we introduced a variable to record which\
    \ proper response was taken the last time the BC got activated.\n\nPerceptual\
    \ Uncertainty Another aspect not included in the case studies is perceptual uncertainty\
    \ or the possibility of errors in sensing. A basic approach to this issue would\
    \ be adding safety margins to the behaviour of the controller. For instance, introducing\
    \ a variable <sup>x</sup>dT gt for the perceived value of target location (xT\
    \ gt) and discussing assumptions on the difference between xT gt and <sup>x</sup>dT\
    \ gt enables us to derive the appropriate amount of the safety margin for this\
    \ uncertainty.\n\n#### 6.3 Using Event-B for Modelling and Proving\n\nFeatures\
    \ of Event-B and its modelling environment Rodin [\\[1\\]](#page-15-1) were useful\
    \ for modelling and proving the safety architecture for GA-RSS. Rodin generated\
    \ POs and helped interactive proof of them. The refinement mechanism of Event-B\
    \ was effective for distributing the complexity of modelling and proving over\
    \ multiple steps. In addition, as we discussed in [§4.2,](#page-8-0) we derived\
    \ the correct behaviour of DM from generated POs.\n\nOur contributions in this\
    \ paper, namely strategies of modelling and refinement, provide a guide to the\
    \ effective use of Event-B's features for the rigorous and systematic construction\
    \ of controllers for different subscenarios.\n\nOn the other hand, although Rodin\
    \ has proof tactics and provers for automatically discharging POs, we had to manually\
    \ discharge all POs. It is because we needed an extension of Event-B language\
    \ [\\[4\\]](#page-16-4) to use real numbers in models, and Rodin's current automatic\
    \ proof functionalities are not strong when the language is extended. However,\
    \ we expect that this problem will be solved; for instance, there are studies\
    \ aiming at assisting automatic proof of hybrid systems by bridging Rodin with\
    \ external solvers [\\[3\\]](#page-16-5).\n\n### 6.4 Related Work\n\nRSS was originally\
    \ proposed as the formal approach for AVs, but the paper did not include any machine-processible\
    \ models [\\[13\\]](#page-16-0). The work on GA-RSS extended the framework of\
    \ RSS with formal specifications and partial calculations supported by Mathematica\
    \ [\\[7\\]](#page-16-1). Other studies only used the resulting RSS conditions,\
    \ for example, encoding them in signal temporal logic for runtime verification\
    \ [\\[8\\]](#page-16-6). To the best of our knowledge, this is the first attempt\
    \ to make use of formal modelling for the RSS scheme. The study in [\\[12\\]](#page-16-7)\
    \ demonstrated the difficulty in checking RSS properties with automated \"one\
    \ button\" tools for reachability analysis and model checking.\n\nOther formal\
    \ attempts for AVs include proofs with the Isabelle/HOL prover [\\[11\\]](#page-16-8)\
    \ with support of MATLAB. The focus was on the detailed computation including\
    \ floating-point errors while the driving behaviour was rather simple; avoidance\
    \ of one static object with a white-box controller.\n\nVerification over RSS is\
    \ intrinsically hybrid, i.e., including continuous aspects such as velocity and\
    \ distance. Proofs over hybrid models have been actively investigated in the Hoare-style\
    \ reasoning, not only for Event-B but also in other formalisms such as KeYmaera\
    \ X [\\[6\\]](#page-16-9). Our case study did not focus on the continuous aspects\
    \ and used rather simple theories for handling real arithmetic. Our future work\
    \ includes the use of more sophisticated support for discharging the proof obligations.\
    \ It is notable that refining continuous models in the physics world into discrete\
    \ software controllers has been actively investigated for Event-B, e.g., [\\[5\\\
    ]](#page-16-10). Models obtained in our approach can be further refined with such\
    \ techniques into concrete designs of discrete software controllers.\n\nGuidelines\
    \ with a focus on refinement strategies have been considered useful for Event-B\
    \ as reusable know-how for specific types of systems [\\[14\\]](#page-17-0). Our\
    \ case study has the potential to be elaborated into such guidelines. Although\
    \ the effectiveness of refinement strategies has been discussed qualitatively\
    \ in most cases, there have been efforts on quantitative analysis [\\[9\\]](#page-16-11).\
    \ Our future work will include analysis of refinement strategies in this work\
    \ in a more systematic way.\n\n## <span id=\"page-15-0\"></span>7 Conclusion\n\
    \nIn this paper, we reported our case study to model, derive, and prove the safety\
    \ conditions of AVs in the RSS scheme. We target a state-of-the-art problem with\
    \ the goal-aware version of RSS as well as the Simplex architecture to consider\
    \ black-box AI controllers. We proposed a strategy for leveraging the refinement\
    \ mechanism of Event-B and demonstrated how it mitigates the complexity over scenario\
    \ variations. We will continue studying other scenarios to convert the obtained\
    \ lessons into more concrete and general guidelines for formal modelling and verification\
    \ of AVs.\n\n## Acknowledgements\n\nWe thank our industrial partner Mazda for\
    \ discussions of realistic problems in the safety assurance of autonomous driving.\
    \ We also thank members of JST ERATO HASUO Metamathematics for Systems Design\
    \ Project for discussions of Goal-Aware RSS and the safety architecture.\n\n##\
    \ References\n\n<span id=\"page-15-1\"></span>1. Abrial, J.R., Butler, M., Hallerstede,\
    \ S., Hoang, T.S., Mehta, F., Voisin, L.: Rodin: an open toolset for modelling\
    \ and reasoning in Event-B. International Journal on Software Tools for Technology\
    \ Transfer 12(6), 447–466 (2010). <https://doi.org/10.1007/s10009-010-0145-y>\n\
    \n- <span id=\"page-16-5\"></span><span id=\"page-16-3\"></span>2. Abrial, J.R.:\
    \ Modeling in Event-B: System and software engineering. Cambridge University Press\
    \ (2010)\n- 3. Afendi, M., Mammar, A., Laleau, R.: Building correct hybrid systems\
    \ using Event-B and Sagemath: Illustration by the hybrid smart heating system\
    \ case study. In: 26th International Conference on Engineering of Complex Computer\
    \ Systems (ICECCS). pp. 91–96. Hiroshima, Japan (2022). <https://doi.org/10.1109/ICECCS54210.2022.00019>\n\
    - <span id=\"page-16-4\"></span>4. Butler, M., Maamria, I.: Practical theory extension\
    \ in Event-B. In: Liu, Z., Woodcock, J., Zhu, H. (eds.) Theories of Programming\
    \ and Formal Methods. pp. 67–81. Springer, Berlin, Heidelberg (2013). [https://doi.org/10.1007/978-3-642-39698-4\\\
    \\_5](https://doi.org/10.1007/978-3-642-39698-4_5)\n- <span id=\"page-16-10\"\
    ></span>5. Dupont, G., Ait-Ameur, Y., Singh, N.K., Pantel, M.: Event-B hybridation:\
    \ A proof and refinement-based framework for modelling hybrid systems. ACM Transactions\
    \ on Embedded Computing Systems 20(4) (2021). <https://doi.org/10.1145/3448270>\n\
    - <span id=\"page-16-9\"></span>6. Fulton, N., Mitsch, S., Quesel, J.D., Völp,\
    \ M., Platzer, A.: KeYmaera X: An axiomatic tactical theorem prover for hybrid\
    \ systems. In: Felty, A.P., Middeldorp, A. (eds.) Automated Deduction - CADE-25.\
    \ pp. 527–538. Springer, Cham (2015). [https://doi.org/10.1007/978-3-319-21401-6\\\
    \\_36](https://doi.org/10.1007/978-3-319-21401-6_36)\n- <span id=\"page-16-1\"\
    ></span>7. Hasuo, I., Eberhart, C., Haydon, J., Dubut, J., Bohrer, R., Kobayashi,\
    \ T., Pruekprasert, S., Zhang, X.Y., Pallas, E.A., Yamada, A., Suenaga, K., Ishikawa,\
    \ F., Kamijo, K., Shinya, Y., Suetomi, T.: Goal-aware RSS for complex scenarios\
    \ via program logic. IEEE Transactions on Intelligent Vehicles pp. 1–33 (2022).\
    \ <https://doi.org/10.1109/TIV.2022.3169762>\n- <span id=\"page-16-6\"></span>8.\
    \ Hekmatnejad, M., Yaghoubi, S., Dokhanchi, A., Amor, H.B., Shrivastava, A., Karam,\
    \ L., Fainekos, G.: Encoding and monitoring responsibility sensitive safety rules\
    \ for automated vehicles in signal temporal logic. In: 17th ACM-IEEE International\
    \ Conference on Formal Methods and Models for System Design (MEMOCODE). ACM, New\
    \ York, NY, USA (2019). <https://doi.org/10.1145/3359986.3361203>\n- <span id=\"\
    page-16-11\"></span>9. Kobayashi, T., Ishikawa, F.: Analysis on strategies of\
    \ superposition refinement of Event-B specifications. In: Sun, J., Sun, M. (eds.)\
    \ Formal Methods and Software Engineering (ICFEM). pp. 357–372. Springer, Cham\
    \ (2018). [https://doi.org/10.1007/978-3-030-02450-5\\\\_21](https://doi.org/10.1007/978-3-030-02450-5_21)\n\
    - <span id=\"page-16-2\"></span>10. Phan, D., Yang, J., Clark, M., Grosu, R.,\
    \ Schierman, J., Smolka, S., Stoller, S.: A component-based simplex architecture\
    \ for high-assurance cyberphysical systems. In: 17th International Conference\
    \ on Application of Concurrency to System Design (ACSD). pp. 49–58. Zaragoza,\
    \ Spain (2017). <https://doi.org/10.1109/ACSD.2017.23>\n- <span id=\"page-16-8\"\
    ></span>11. Rizaldi, A., Immler, F., Schürmann, B., Althoff, M.: A formally verified\
    \ motion planner for autonomous vehicles. In: Lahiri, S.K., Wang, C. (eds.) Automated\
    \ Technology for Verification and Analysis (ATVA). pp. 75–90. Springer, Cham (2018).\
    \ [https://doi.org/10.1007/978-3-030-01090-4\\\\_5](https://doi.org/10.1007/978-3-030-01090-4_5)\n\
    - <span id=\"page-16-7\"></span>12. Roohi, N., Kaur, R., Weimer, J., Sokolsky,\
    \ O., Lee, I.: Self-driving vehicle verification towards a benchmark. CoRR abs/1806.08810\
    \ (2018), <http://arxiv.org/abs/1806.08810>\n- <span id=\"page-16-0\"></span>13.\
    \ Shalev-Shwartz, S., Shammah, S., Shashua, A.: On a formal model of safe and\
    \ scalable self-driving cars. CoRR abs/1708.06374 (2017), <http://arxiv.org/abs/1708.06374>\n\
    - <span id=\"page-17-0\"></span>18 Tsutomu Kobayashi, Martin Bondu, and Fuyuki\
    \ Ishikawa\n- 14. Yeganefard, S., Butler, M.J., Rezazadeh, A.: Evaluation of a\
    \ guideline by formal modelling of cruise control system in Event-B. In: Muñoz,\
    \ C.A. (ed.) the Second NASA Formal Methods Symposium (NFM). NASA Conference Proceedings,\
    \ vol. NASA/CP-2010-216215, pp. 182–191 (2010)"
  decisions:
    language: '- Qualified. Reason: English Paper'
    evaluation_prompt: Disqualified
    related_work_prompt: Qualified
    novelty_prompt: Qualified
    review_only_prompt: Qualified
  llm_input_used: '## Abstract

    Ensuring the safety of autonomous vehicles (AVs) is the key requisite for

    their acceptance in society. This complexity is the core challenge in formally

    proving their safety conditions with AI-based black-box controllers and

    surrounding objects under various traffic scenarios. This paper describes our

    strategy and experience in modelling, deriving, and proving the safety

    conditions of AVs with the Event-B refinement mechanism to reduce complexity.

    Our case study targets the state-of-the-art model of goal-aware

    responsibility-sensitive safety to argue over interactions with surrounding

    vehicles. We also employ the Simplex architecture to involve advanced black-box

    AI controllers. Our experience has demonstrated that the refinement mechanism

    can be effectively used to gradually develop the complex system over scenario

    variations.


    ## Introduction

    The safety of automated vehicles has been attracting increased interest in society.
    In addition to the intensive effort of simulation-based testing, there is a key
    approach based on formal reasoning called responsibility-sensitive safety (RSS)
    [\[13\]](#page-16-0). RSS defines the minimum rules that traffic participants
    should comply with for safety, i.e., no collisions. This rule-based approach has
    recently recently been extended to goal-aware RSS (GA-RSS) to deal with the goal-achievement,
    i.e., the driving goal of the ego-vehicle is eventually achieved such as pulling
    over


    <sup>⋆</sup> The first author is supported by JSPS KAKENHI grant number 19K20249
    and JST ERATO-MMSD (JPMJER1603) project. The third author is supported by JST
    MIRAI-eAI (JPMJMI20B8) project.


    upon emergency [\[7\]](#page-16-1). GARSS is effective for formally limiting liabilities,
    which is vital for AV manufacturers.


    The challenge lies in deriving the necessary GARSS conditions and formally checking
    the compliance of the design of the ego vehicle over various scenarios under different
    environmental conditions. In addition, there is increasing demand to consider
    complex behaviours of black-box AI-based advanced controllers backed up with safety-ensured
    controllers, e.g., the Simplex architecture [\[10\]](#page-16-2).


    Existing efforts have clarified the principles to derive and argue conditions
    that ego-vehicles should comply with in example scenarios. However, the engineering
    aspect has yet to be investigated. Specifically, we need a systematic modelling
    design that accepts the flexibility to mitigate the complexity in dealing with
    multiple aspects of scenario variations and architectural design.


    To this end, we report our experience in modelling, deriving, and proving the
    safety conditions of autonomous vehicles (AVs). We follow the GA-RSS approach
    to define and derive the safety conditions to be checked with architectural design
    with black-box advanced controllers. We propose a strategy for using the refinement
    mechanism of Event-B [\[2\]](#page-16-3) to gradually argue the complex aspects
    including the scenario variations. Our experience has shown the potential of the
    refinement mechanism for the flexible design of models and proofs to mitigate
    the complexity in a gradual manner. To the best of our knowledge, this is the
    first attempt to focus on the model engineering aspect over scenario variations
    in the deductive approach for AV safety.


    The rest of this paper is structured as follows: In § [2,](#page-1-0) we describe
    the safety architecture, RSS, and Event-B. § [3](#page-3-0) introduces GA-RSS
    and a case study example. We elaborate on our approach and its application to
    the case studies in § [4–](#page-6-0)[5.](#page-11-0) We discuss the approach
    in § [6](#page-13-0) before concluding the paper in § [7.](#page-15-0)'
  token_usage: 3946
  time_usage: 1.7178165912628174
