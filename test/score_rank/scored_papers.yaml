papers:
- title: "MX: Enhancing RISC-V's Vector ISA for Ultra-Low Overhead,\n  Energy-Efficient\
    \ Matrix Multiplication"
  abstract: 'Dense Matrix Multiplication (MatMul) is arguably one of the most ubiquitous

    compute-intensive kernels, spanning linear algebra, DSP, graphics, and machine

    learning applications. Thus, MatMul optimization is crucial not only in

    high-performance processors but also in embedded low-power platforms. Several

    Instruction Set Architectures (ISAs) have recently included matrix extensions

    to improve MatMul performance and efficiency at the cost of added matrix

    register files and units. In this paper, we propose Matrix eXtension (MX), a

    lightweight approach that builds upon the open-source RISC-V Vector (RVV) ISA

    to boost MatMul energy efficiency. Instead of adding expensive dedicated

    hardware, MX uses the pre-existing vector register file and functional units to

    create a hybrid vector/matrix engine at a negligible area cost (< 3%), which

    comes from a compact near-FPU tile buffer for higher data reuse, and no clock

    frequency overhead. We implement MX on a compact and highly energy-optimized

    RVV processor and evaluate it in both a Dual- and 64-Core cluster in a 12-nm

    technology node. MX boosts the Dual-Core''s energy efficiency by 10% for a

    double-precision 64x64x64 matrix multiplication with the same FPU utilization

    (~97%) and by 25% on the 64-Core cluster for the same benchmark on 32-bit data,

    with a 56% performance gain.'
  url: http://arxiv.org/abs/2401.04012v1
  keywords: RISC-V, Matrix, Vector, Efficiency
  document: "#### I. INTRODUCTION\n\nThe exponential growth of the computational requirements\
    \ in Machine Learning (ML) and Artificial Intelligence (AI) applications is a\
    \ major challenge for hardware architects. The rise of application-specific accelerators\
    \ [1] and single instruction, multiple data (SIMD) programmable systems [2] demonstrates\
    \ the need for novel architectures able to cope with the rising computational\
    \ demand. Furthermore, AI/ML applications also found their way into edge computing,\
    \ with benefits such as higher privacy, user personalization, and lower power\
    \ consumption. However, edge-AI/ML systems have the additional challenge of balancing\
    \ large computational demands against a very tight power envelope and minimal\
    \ area footprint. The quest for energy efficiency and cost (i.e., area) minimization\
    \ is even more pressing today since ML/AI computation at the edge does not only\
    \ involve inference but also training in the so-called AI on Edge [3].\n\nMatrix\
    \ Multiplication (MatMul) is a cornerstone in ML and AI, and essential in scientific\
    \ computing, graphics, and Digital Signal Processing (DSP). The importance of\
    \ MatMul is testified by market-leading companies, such as Google, which developed\
    \ the first Tensor-Processing Unit (TPU) in 2015 to accelerate matrix operations\
    \ [4] and updated it in 2018 with an edgeoriented version achieving 4 TOPS within\
    \ a 2 W power envelope [5]. As with other Domain-Specific Accelerators (DSAs)\
    \ for specific neural-network tasks [6], the TPU is an added resource to which\
    \ a general-purpose processor offloads the workload (for example, through a PCIe\
    \ interface). This brings an area and power overhead that is not affordable in\
    \ constrained systems at the edge, especially when they need to compute non-AI\
    \ tasks as well. Moreover, an excessively specialized accelerator risks becoming\
    \ useless when the ML/AI algorithm evolves.\n\nMost proprietary Instruction Set\
    \ Architectures (ISAs) offer dedicated matrix extensions, such as Arm's Scalable\
    \ Matrix Extension (SME), Intel Advanced Matrix Extension (AMX), and IBM Matrix-Multiply\
    \ Assist (MMA). Unluckily, the microarchitectural details of the implementations\
    \ remain company secrets. So far, the RISC-V open-source ISA features only a vector\
    \ extension (RISC-V Vector (RVV)), even though researchers developed multiple\
    \ unofficial AI/matrix extensions. Still, they add tightly coupled matrix units\
    \ [7] or a new matrix register file [8] used only during matrix operations, which\
    \ add area and power consumption.\n\nRVV recently showed to be a valid solution\
    \ to efficiently accelerate MatMul while keeping a well-known programming model\
    \ to handle diverse data-parallel workloads, also in the embedded domain [9].\
    \ Vector processors execute multiple operations with one instruction, amortizing\
    \ its fetch/decode cost. Moreover, they feature a Vector Register File (VRF) to\
    \ buffer the vector elements, decreasing the accesses to memory without changing\
    \ the computational balance for the architecture [10]. Even if the VRF helps decrease\
    \ the power associated with the memory accesses, it is an additional block at\
    \ the bottom of the memory hierarchy, one of the key drivers for performance and\
    \ energy efficiency [11]. Its size can be way larger than the one of a scalar\
    \ register file, and it is usually connected to multiple functional units in parallel,\
    \ which leads to energyhungry interconnects. Hence, the VRF access-related energy\
    \ is usually non-negligible [9], [12].\n\nWith this paper, we present Matrix eXtension\
    \ (MX), a non-intrusive ISA extension to RVV that creates a generalpurpose hybrid\
    \ matrix/vector architecture with minimal area impact and superior energy efficiency.\
    \ To cut the power consumption, we reduce the expensive accesses to/from the\n\
    \nThe first two authors contributed equally to this work.\n\n<sup>\xA9</sup> 2023\
    \ IEEE. Personal use of this material is permitted. Permission from IEEE must\
    \ be obtained for all other uses, in any current or future media, including reprinting/republishing\
    \ this material for advertising or promotional purposes, creating new collective\
    \ works, for resale or redistribution to servers or lists, or reuse of any copyrighted\
    \ component of this work in other works.\n\nVRF by featuring a software-transparent\
    \ lightweight accumulator close to the processing units. MX does not add a matrix\
    \ unit to the architecture but re-uses the already available processing resources\
    \ to keep the area and energy overhead at its minimum and exploit the energy efficiency\
    \ savings that come from the reduced VRF accesses.\n\nTo validate MX across multiple\
    \ domains, we add MX to a constrained embedded Dual-Core cluster built upon the\
    \ opensource energy-optimized RVV-based Spatz [9] vector processor and to a scaled-up\
    \ MemPool architecture [13] with 64 Spatz processors and implement both systems\
    \ in a competitive 12-nm technology. We provide a quantitative justification of\
    \ the energy savings and a detailed power, performance, and area (PPA) analysis\
    \ on matrix multiplications on different data precisions, finding that our matrix\
    \ extension can boost not only energy efficiency but also performance.\n\nWith\
    \ this paper, we present the following contributions:\n\n- We define MX, a lightweight\
    \ and non-intrusive ISA extension based on RVV 1.0 aimed at supporting memory\
    \ and computational operations directly on matrices. MX reduces the power consumption\
    \ of the architecture with similar or better performance by introducing a near-Floating\
    \ Point Unit (FPU) tile buffer, a per-vector-element broadcast system, and minimal\
    \ modifications to the Vector Load/Store Unit (VLSU).\n- We provide a theoretical\
    \ justification of the benefits that the ISA has on the power consumption when\
    \ executing a matrix multiplication kernel, effectively reducing the expensive\
    \ VRF accesses.\n- We implement MX on a constrained Dual-Core and a complex 64-core\
    \ clusters based on the energy-efficient RVV vector processor Spatz, and characterize\
    \ MX's impact on performance and PPA metrics in a 12-nm technology. For less than\
    \ 3% area overhead, we get a maximum of 56% and 25% performance and energy efficiency\
    \ gains, respectively.\n\n#### II. ANALYSIS\n\nIn the following, we discuss the\
    \ tiling of a General Matrix Multiply (GEMM) problem through a multi-level memory\
    \ hierarchy. When C is a zero matrix, GEMM becomes a MatMul.\n\n$$D\\_{M \\times\
    \ N} = A\\_{M \\times K} \\cdot B\\_{K \\times N} + C\\_{M \\times N} \\tag{l}$$\n\
    \nFor convenience, let us consider a memory hierarchy composed of a memory, a\
    \ VRF, and a near-FPU buffer. The memory connects to the VRF, which is connected\
    \ to a buffer that feeds the FPUs, as reported in Figure 1. The following analysis\
    \ can be easily extended to memory hierarchies with a different number of levels.\n\
    \n#### *A. The tiling problem*\n\nThe lower level of the hierarchy is usually\
    \ not large enough to keep the input and output matrices all at once. Therefore,\
    \ the matrices are divided into chunks (tiles), and the hardware works on one\
    \ output tile at a time, and the outer product algorithm is often used to maximize\
    \ parallelism. The number of elements\n\n![](_page_1_Figure_12.jpeg)\n\nFigure\
    \ 1. The tiling problem over a memory hierarchy composed of three levels, ending\
    \ with the processing elements (FPUs).\n\ntransferred between two consecutive\
    \ levels of the hierarchy impacts both performance and power consumption and depends\
    \ on how the matrices are tiled. Usually, the number of transfers is partially\
    \ encoded in the *arithmetic intensity*, i.e., the total number of operations\
    \ divided by the total number of Bytes transferred between the memory and the\
    \ core.\n\nIn the following, we provide equations to fine-grain count how many\
    \ memory accesses happen between each pair of consecutive levels of the hierarchy.\
    \ Each equation contains four terms, which correspond to 1) the elements of matrix\
    \ A, 2) the elements of matrix B, 3) the elements of matrix C (or D) from the\
    \ upper level to the lower one (load/fetch), and 4) the elements of the matrix\
    \ D from the lower level back to the upper one (store/write-back).\n\nIn the most\
    \ generic scenario, without buffering the output tile in the VRF for more than\
    \ updates, the number of elements moved between the memory and the VRF is:\n\n\
    $$\\#E\\,lm\\_{VRF}^{MEM} = \\frac{N}{n}MK + \\frac{M}{m}NK + \\frac{K}{k}MN +\
    \ \\frac{K}{k}MN \\qquad (2)$$\n\nWhere the A, B, and D (C) matrices stored in\
    \ memory have sizes , , , and we tile the problem between the memory and the VRF\
    \ with tiles of size , , .\n\nFor each matrix tile, the number of elements exchanged\
    \ between the VRF and the buffer is:\n\n$$\\#Elim\\_{BUF}^{VRF} = \\frac{n}{n'}mk\
    \ + \\frac{m}{m'}nk + \\frac{k}{k'}mn + \\frac{k}{k'}mn\\tag{3}$$\n\nWhere the\
    \ tiles stored in the VRF have sizes , , , and we sub-tile the problem between\
    \ the VRF and the buffer with sub-tiles of size \u2032 \u2032 , \u2032 \u2032\
    \ , \u2032 \u2032 .\n\nFor each matrix sub-tile, the number of elements exchanged\
    \ between the buffer and the FPUs is:\n\n$$\\#E\\,l m\\_{FPU}^{BUF} = \\frac{n'}{t\\\
    _B} m' k' + \\frac{m'}{t\\_A} n' k' + k' m' n' + k' m' n' \\qquad (4)$$\n\nWhere\
    \ the sub-tiles stored in the buffer have sizes \u2032 \u2032 , \u2032 \u2032\
    \ , \u2032 \u2032 , and we access and elements from tiles A and B, respectively.\n\
    \n## *B. Total number of transfers*\n\nTo get the total number of transfers between\
    \ each pair of hierarchy levels, we need to take into account how many output\
    \ tiles and sub-tiles we calculate throughout the program.\n\nTable I NUMBER OF\
    \ ACCESSES BETWEEN CONSECUTIVE LEVELS OF THE MEMORY HIERARCHY.\n\n| Ref.     \
    \      | Metric                                             | A (\u2193)     \
    \                                   | B (\u2193)                             \
    \          | C, D (\u2193)                                                   \
    \          | D (\u2191)                                                      \
    \          |\n|----------------|----------------------------------------------------|----------------------------------------------|---------------------------------------------|----------------------------------------------------------------------|----------------------------------------------------------------------|\n\
    | 1)<br>2)<br>3) | #ElmMEM<br>VRF<br>#ElmVRF<br>BUF<br>#ElmBUF<br>FPU | N<br>n\
    \ MK<br>N<br>\u2032 MK<br>n<br>N<br>MK<br>tB | M<br>NK<br>m<br>M<br>m\u2032 NK<br>M<br>NK<br>tA\
    \ | K<br>k MN<br>k<br>K<br>k MN<br>\u2032<br>k<br>k\u2032 k<br>K<br>k MN<br>\u2032\
    <br>k | K<br>k MN<br>k<br>K<br>k MN<br>\u2032<br>k<br>k\u2032 k<br>K<br>k MN<br>\u2032\
    <br>k |\n\n(a) \u2193 /\u2191 indicate transfers to a lower/higher level of the\
    \ memory.\n\n*a) Memory and VRF:* In the most generic case, we load the C (first\
    \ iteration) and D (from the second iteration on) tiles from memory before consuming\
    \ the input , -sized tiles, and we store the D tile back to memory after updates.\
    \ Without inter-k-tile buffering in the VRF, we load ( ) output tiles with size\
    \ from the memory to the VRF, and we store back the same amount. If we buffer\
    \ the output tiles until they are completely calculated over the whole K dimension,\
    \ the formula simplifies to . Instead, we load a total of input A and B tiles,\
    \ with sizes , .\n\n*b) VRF and buffer:* The \u2032 \u2032 output sub-tiles of\
    \ a tile are fetched for ( \u2032 ) \u2032 \u2032 times from the VRF before consuming\
    \ the input \u2032 \u2032 , \u2032 \u2032 -sized sub-tiles in the buffer, and\
    \ writtenback to the VRF for the same number of times if there is no inter-k-tile\
    \ buffering in the buffer. Instead, the input A and B sub-tiles, with sizes \u2032\
    \ \u2032 , \u2032 \u2032 , are loaded times. If we keep into account how many\
    \ times each tile is loaded/stored from/to memory, these formulas become ( ) (\
    \ \u2032 ) \u2032 \u2032 for the sub-tiles fetch, the same amount for the sub-tiles\
    \ writes-back, and \u2032 \u2032 \u2032 for the A and B sub-tiles fetch.\n\nWe\
    \ summarize all the transfers across the hierarchy in Table I.\n\n#### *C. Optimizations*\n\
    \n*a) Inter-k-buffering:* If the output tile (sub-tile) is buffered in the VRF\
    \ (buffer) until the whole K (k) dimension is traversed and the whole output tile\
    \ (sub-tile) is ready, we can simplify the equations above. If the buffering happens\
    \ in the VRF, = 1 in Table I Ref. 1), while if it happens in the buffer until\
    \ the whole dimension, \u2032 = 1, in Table I Ref. 2) (if the buffering only happens\
    \ until the whole dimension is traversed, \u2032 = 1).\n\nInter-k-buffering is\
    \ ultimately limited by the size of the lower memory level, which should be able\
    \ to host the whole output tile (sub-tile) for the whole computation on the ()\
    \ dimension. Therefore, keeping the output tile in the buffer for the whole dimension\
    \ requires that = \u2032 , = \u2032 . On the other hand, relaxing this constraint,\
    \ e.g. = \u2032 , = \xD7 \u2032 , allows for fewer overall transfers between the\
    \ memory and the VRF. In this case, the inter-k-buffering can be done only between\
    \ the memory and the VRF.\n\n*b) C-tile reset:* When C is a zero matrix, it's\
    \ possible to avoid loading it from memory and initialize the VRF with zeroes\
    \ or reset the buffer. If we use inter-k-buffering and the zero initialization\
    \ is applied to the VRF and the buffer, the third term of the equations related\
    \ to the load/fetch of matrix C, D, becomes zero in Table I Ref. 1) and 2), respectively.\n\
    \n![](_page_2_Figure_10.jpeg)\n\nFigure 2. Spatz's VLSU, VRF, and VFU with MX\
    \ architectural schematic.\n\n#### III. MX IMPLEMENTATION\n\n#### *A. ISA Extension*\n\
    \nWe implement MX in Spatz [9], an open-source, RVV-based, highly-optimized compact\
    \ vector processor, targeting minimal area and power overhead. Thus, we do not\
    \ add any dedicated matrix units or software-addressable registers, as shown in\
    \ Figure 2. MX adds three configure instructions (msettile[m,n,k]), three memory\
    \ instructions (mld.[a,b], mst.c), and two computation instructions (mx[f]macc).\
    \ The first three instructions set up the sub-tile sizes \u2032 , \u2032 , \u2032\
    \ on which the matrix instructions operate ( \u2032 \u2032 = , \u2032 \u2032 \u2264\
    \ , and is the vector length in elements). We enhance the VLSU to enable matrix\
    \ load and store operations, which are composed of multiple unitand non-unit-stride\
    \ memory operations already supported by Spatz. The new memory instructions are\
    \ introduced to load tiles from matrix *A* and *B* and to store the computed tile\
    \ back to memory, while the two computational instructions perform a MatMul between\
    \ the two \u2032 \u2032 and \u2032 \u2032 sub-tiles, storing the resulting \u2032\
    \ \u2032 sub-tile in the VRF. Close to the FPUs, we introduce a tiny broadcast\
    \ block consisting only of a register and some multiplexers to broadcast single\
    \ elements from the *A* tile across multiple elements of tile *B*. This block\
    \ increases the data reuse of tile *A* at a minimal cost. Finally, we implement\
    \ a latch-based result tile buffer in the vector functional unit (VFU) to reduce\
    \ energy consumption by minimizing VRF accesses by intermediate result accumulation,\
    \ limiting the buffer size to <sup>1</sup> 8 of the VRF size (i.e., = 256) to\
    \ maintain low energy and area overhead. Since Spatz's VLSU has four parallel\
    \ memory ports and the buffer is constrained in size, \u2032 , \u2032 , \u2032\
    \ \u2208 {4, 8}.\n\n#### *B. MX Benefits*\n\nTable II summarizes the number of\
    \ elements transferred between consecutive memory hierarchies for a baseline vectoronly\
    \ MatMul and a MX-ready MatMul. The baseline employs a traditional scalar-vector\
    \ algorithm to load *m* scalar elements from the input matrix *A* and an *n*-long\
    \ vector from matrix *B*. The MX-ready configuration loads A tiles with size \u2032\
    \ \u2032 and *B* tiles with size \u2032 \u2032 . While the MX algorithm does not\
    \ further sub-tile the tiles on or (*m'* = *m* and *k'* = *k*), it sub-tiles\n\
    \n| Config                     | Metric                                      \
    \       | A (\u2193)                                           | B (\u2193)  \
    \                                  | C, D (\u2193)                   | D (\u2191\
    )                       |\n|----------------------------|----------------------------------------------------|-------------------------------------------------|------------------------------------------|----------------------------|-----------------------------|\n\
    | Baseline(a)<br>Baseline(a) | #ElmMEM<br>VRF<br>#ElmVRF<br>FPU              \
    \     | N<br>n MK<br>N<br>F MK                          | M<br>NK<br>m<br>MNK\
    \                      | 0<br>KMN                   | MN<br>KMN              \
    \     |\n| MX<br>MX<br>MX             | #ElmMEM<br>VRF<br>#ElmVRF<br>BUF<br>#ElmBUF<br>FPU\
    \ | N<br>\u2032 MK<br>B\xD7n<br>N<br>\u2032 MK<br>n<br>N<br>F MK | M<br>m\u2032\
    \ NK<br>M<br>m\u2032 NK<br>M<br>NK<br>F | 0<br>K<br>\u2032 MN<br>k<br>KMN | MN<br>K<br>\u2032\
    \ MN<br>k<br>KMN |\n\nTable II DATA TRANSFERS: MX-READY VS BASELINE.\n\n(a) Elements\
    \ from A are loaded/fetched to/from the scalar register file;\n\n(b) *F* represents\
    \ the number of FPUs;\n\n(c) \u2193 /\u2191 indicate transfers to a lower/higher\
    \ level of the memory.\n\nalong *n* such that *n* = \xD7*n'*, where \u2208 {2,\
    \ 4}. In the following, we highlight the benefits brought by the MX algorithm.\n\
    \n*1) Matrix A operands:* In the baseline approach, operands from matrix *A* are\
    \ fetched as scalars from the scalar register file and individually forwarded\
    \ to the vector unit. In contrast, the matrix algorithm retrieves multiple elements\
    \ from *A* in a tiled-vector manner, improving the access pattern and enabling\
    \ the data reuse of the *A* tile by means of the broadcast engine.\n\n*2) Instruction\
    \ count:* In the baseline algorithm, each vector instruction is amortized over\
    \ operations. With MX, the total number of instructions fetched and decoded is\
    \ lower, as each mxfmacc instruction is amortized over \u2032 \u2032 \u2032 operations,\
    \ \u2032 \u2032 = , and \u2032 > 1. This boosts the SIMD ratio, i.e., the average\
    \ number of operations per instruction.\n\n*3) Tile window:* The matrix algorithm\
    \ exploits the *k* dimension to increase the size of the tile window when the\
    \ dimensions *M* and *N* are limited. This is especially beneficial as the SIMD\
    \ ratio is further improved by allowing each core to work on a larger output tile\
    \ window in a multi-core environment when processing matrices with a limited dimension.\n\
    \n*4) Scalar-vector interactions:* With the baseline algorithm, the scalar core\
    \ must remain active to compute operand addresses and forward scalar operands\
    \ to the Vector Processing Unit (VPU). In contrast, MX pushes the whole computation\
    \ to the vector unit, freeing up the scalar core.\n\n*5) Performance:* The computing\
    \ performance is significantly impacted by the number of data transfers between\
    \ the memory and the VRF and the related latency. The MX-ready VLSU regularizes\
    \ the memory accesses, which can reduce conflicts in both the interconnect and\
    \ memory banks.\n\n*6) Energy:* In the VPU, the power consumption of the VRF normally\
    \ constitutes a non-negligible portion of the overall energy usage. MX's inexpensive\
    \ broadcast engine and tile buffers enhance data reuse for the tiled matrix *A*\
    \ and reduce the VRF access by a \u2032 factor. Moreover, the reduced instruction\
    \ count and more regular memory access pattern alleviate the pressure on the instruction\
    \ and data memories, further improving the energy efficiency of the overall system.\n\
    \n#### IV. EXPERIMENT SETUP AND RESULTS\n\n# *A. Computing Clusters and Methodology*\n\
    \nWe integrate the baseline and the MX-ready versions of the Spatz VPU into two\
    \ floating-point-capable computing clusters: a 64-bit constrained Dual-Core cluster\
    \ for in-depth analysis of various tile and sub-tile configurations, and a 32-bit\
    \ large-scale 64-Core cluster for performance evaluation in a complex system.\n\
    \n*1) Dual-Core Cluster:* The Dual-Core cluster is a 64 bit shared-L1-memory cluster,\
    \ implemented with 128 KiB of Tightly Coupled Data Memory (TCDM) across 16 Static\
    \ Random-Access Memory (SRAM) banks. This cluster features 2 Snitch cores, each\
    \ controlling a Spatz instance equipped with 4 double-precision FPUs and 2 KiB\
    \ VRF each, supporting a vector length of 512 bits. The peak achievable performance\
    \ is 16 DP\u2212FLOP/cycle.\n\n*2) 64-Core MemPool Cluster:* MemPool, a large-scale\
    \ 32 bit shared-L1-memory cluster, scales up to 256 RISC-V cores and includes\
    \ 1 MiB of L1 TCDM [13]. The cluster is hierarchically organized into 4 groups,\
    \ each containing 16 tiles. A fully connected logarithmic crossbar is employed\
    \ between the cores and memories, achieving non-uniform memory access (NUMA) with\
    \ a maximum latency of 5 cycles. We equip each Spatz instance with 4 32-bit FPUs\
    \ and 2 KiB of VRFs each, supporting a vector length of 512 bits, and pair each\
    \ instance with a scalar Snitch core to form a Core Complex (CC). This cluster\
    \ configuration, labeled MemPool64Spatz4, consists of 64 CCs, one for each tile,\
    \ and achieves a peak performance of 512 SP\u2212FLOP/cycle, as detailed further\
    \ in [9].\n\nWe implement our designs in GlobalFoundries' 12 nm LP-PLUS FinFET\
    \ technology through Synopsys Fusion Compiler 2022.03 for synthesis and Place-and-Route\
    \ (PnR). We analyze the PPA metrics of the MX-ready clusters at the post-PnR implementation\
    \ stage and compare them to their respective non-MX baseline architectures. We\
    \ calculate power consumption using Synopsys' PrimeTime 2022.03 under typical\
    \ operating conditions (TT/0.80 V/25 \xB0C), with switching activities obtained\
    \ from QuestaSim 2021.3 post-layout gate-level simulations and back-annotated\
    \ parasitic information. In the used MatMul kernels, all the input and output\
    \ matrices are kept in the L1 memory and each core of the cluster calculates one\
    \ portion of the output matrix. The kernel executes in parallel across the entire\
    \ cluster, partitioning the matrix equally among multiple cores. At the end of\
    \ each parallel task, the cores are synchronized to ensure consistent write-back\
    \ of the results.\n\n#### *B. Implementation Area and Frequency*\n\nThe logic\
    \ area breakdown of the clusters is presented in Table III. For the MX-ready Dual-Core\
    \ cluster, the main area increase originates from the VFU (+5.3%) due to the near-FPU\
    \ tile buffer and is followed by a slight increase in the VLSU (+5.94 kGE), which\
    \ is related to supporting matrix loads/stores. The total area overhead of MX\
    \ is negligible, amounting to an increase of 2.5 %. The MemPool64Spatz<sup>4</sup>\
    \ cluster follows the same trend, resulting in a similar 2.89 % area overhead.\
    \ MX does not affect the critical path of the two systems in analysis, which runs\
    \ through Snitch to a TCDM bank. Thus, the MXready dual- and 64-core systems achieve\
    \ 920 MHz and 720 MHz in the (SS/0.72 V/125 \xB0C) corner, respectively, with\
    \ no frequency degradation with respect to the baseline clusters.\n\nTable III\
    \ LOGIC AREA BREAKDOWN IN 12-NM TECHNOLOGY.\n\n|         |          | Dual-Core\
    \ Cluster[kGE] |        | 64-Core Cluster[MGE] |       |          |  |  |\n|---------|----------|------------------------|--------|----------------------|-------|----------|--|--|\n\
    |         | Baseline | MX<br>Overhead         |        | Baseline<br>MX      \
    \ |       | Overhead |  |  |\n| Snitch  | 47.82    | 48.01                  |\
    \ +0.40% | 1.50                 | 1.47  | -2.04%   |  |  |\n| i-Cache | 149.67\
    \   | 149.56                 | -0.07% | 4.96                 | 4.95  | -0.20%\
    \   |  |  |\n| TCDM(a) | 1191.89  | 1192.03                | +0.01% | 20.46  \
    \              | 20.48 | +0.09%   |  |  |\n| VRF     | 345.04   | 348.87     \
    \            | +1.11% | 9.32                 | 9.32  | 0.0%     |  |  |\n| VFU\
    \     | 1532.11  | 1613.39                | +5.31% | 12.91                | 13.97\
    \ | +8.21%   |  |  |\n| VLSU    | 111.66   | 117.60                 | +5.32% |\
    \ 2.54                 | 3.07  | +20.87%  |  |  |\n| Other   | 570.63   | 575.97\
    \                 | +0.94% | 7.28                 | 7.39  | +1.51%   |  |  |\n\
    | Total   | 3948.82  | 4045.43                | +2.45% | 59.70               \
    \ | 61.43 | +2.89%   |  |  |\n\n(a) Including Memory Banks and Interconnect Logic.\n\
    \n![](_page_4_Figure_3.jpeg)\n\nFigure 3. Power breakdown for Dual-Core (Left)\
    \ and 64-Core clusters (Right) executing MatMul. Dual-Core: at *TT@1GHz*, executing\
    \ non-MX (4 vectors, length 32) and MX-ready algorithms (\u2032 = 8, \u2032 =\
    \ 4, \u2032 = 4, = 4). 64- Core: at *TT@910MHz*, executing non-MX (8 vectors,\
    \ length 32) and MX-ready algorithms (\u2032 = 8, \u2032 = 4, \u2032 = 8, = 8).\n\
    \n#### *C. Performance, Power and Energy Efficiency*\n\n*1) Dual-Core Cluster:*\
    \ The upper part of Table IV summarizes the kernel information, execution performance,\
    \ and energy efficiency for the Dual-Core cluster when executing a 64-bit MatMul\
    \ across various problem sizes and tile/sub-tile configurations, highlighting\
    \ the rows where the kernel's tile and sub-tile configurations achieve the best\
    \ energy efficiency. The MX-ready cluster with a sub-tile size of (8, 4, 4) achieves\
    \ performance similar to the best-performing execution on the baseline cluster\
    \ with efficiency gains by +10.9 % (16 \xD7 16 \xD7 16), +10.3 % (32 \xD7 32 \xD7\
    \ 32), and +5.2 % (64 \xD7 64 \xD7 64).\n\nWe evaluate the baseline algorithm\
    \ using two different output tile configurations with constant sizes. For small\
    \ problems (16 \xD7 16 \xD7 16), the output tile size of (8, 16, 1) yields higher\
    \ FPU utilization. As discussed in Section II, although (8, 16, 1) has higher\
    \ arithmetic intensity and fewer transfers between TCDM and VRF compared to (4,\
    \ 32, 1), the latter configuration benefits from a 2\xD7 increase in SIMD ratio,\
    \ leading to better performance for larger problem sizes. For the MX-ready algorithm,\
    \ the output tiles with larger and equal \u2032 sub-tile dimensions consistently\
    \ yield better performance and energy efficiency. This improvement is attributed\
    \ to their higher arithmetic intensity and average SIMD ratio. A similar trend\
    \ is observed for the energy efficiency when increasing the \u2032 dimension of\
    \ the sub-tile. Due to the higher arithmetic intensity, the power decreases when\
    \ the output tile size changes from (4, 16, 4) to (8, 8, 4). However, the (4,\
    \ 16, 4) configuration achieves higher performance thanks to more and shorter\
    \ matrix result stores, which can be interleaved with computational instructions\
    \ to hide latency.\n\nThe left part of Figure 3 presents the power breakdown of\
    \ the Dual-Core cluster's baseline and MX-ready execution of a 64 \xD7 64 \xD7\
    \ 64 MatMul, with the most energy-efficient tile and sub-tile size in our benchmarks.\
    \ MX reduces VRF access for the B tile and intermediate result storage, leading\
    \ to a 53.5 % reduction in VRF power consumption. Although the sub-tile buffer\
    \ integration results in a slight 9.4 % power increase in VFU, the overall VPU\
    \ power decreases by 4.1 %. We also observed a power decrease across the rest\
    \ of the cluster components, including the Snitch core, instruction caches, and\
    \ TCDM. This reduction is attributed to the higher SIMD ratio and tiled memory\
    \ request pattern in MX-ready execution, which eliminates the multiple requests\
    \ for scalar operands generated by the Snitch core in the baseline. As a result,\
    \ the total power savings for the Dual-Core cluster achieved through MX amounts\
    \ to 10.4 %.\n\n*2) 64-Core Cluster:* Our benchmark results for various problem\
    \ sizes on MemPool64Spatz<sup>4</sup> are presented in the bottom section of Table\
    \ IV. In such a large interconnected memory, contentions may occur when memory\
    \ requests in the same tile access the same local bank or the same remote group\
    \ in the same cycle. This generates stalls of the VLSU and increases the access\
    \ latency. Although such contentions could be mitigated by allocating data structures\
    \ in a local tile's memory [14], this approach is hard to implement for MatMul,\
    \ which inherently requires an extremely global data access pattern.\n\nMX regular\
    \ memory accesses alleviate contention and improve VLSU utilization by distributing\
    \ vector element loads/stores across different banks and groups in a strided fashion,\
    \ contrasting with the baseline where vector elements are fetched from continuous\
    \ addresses within the same group by both scalar and vector core. This is even\
    \ more evident with small matrices, where the initial vector load and final result\
    \ store constitute a significant portion of the total runtime due to the inability\
    \ to hide latency. FPU utilization increases from 50.4% to 78.7%, leading to a\
    \ 56% improvement in cluster performance.\n\nDespite a power consumption increase\
    \ due to the higher FPU utilization, the MX-ready cluster achieves 25 % better\
    \ energy efficiency. Even though the baseline kernels already achieve near-peak\
    \ utilization for matrix sizes of 128 \xD7 128 \xD7 128 and 256 \xD7 256 \xD7\
    \ 256, with the same arithmetic intensity, MX still improves performance by 5.6\
    \ % and 2.3 %, with energy efficiency gains by 13.4 % and 9.8 %, respectively.\
    \ The right side of Figure 3 presents the MemPool64Spatz4-related power breakdown\
    \ comparison for a 256 \xD7 256 \xD7 256 MatMul. MX reduces the VRF power consumption\
    \ by 60 %, thanks to fewer accesses achieved by buffering intermediate results.\
    \ The VFU power increases by only 6 %, which comes from the sub-tile buffer and\
    \ higher FPU utilization. Overall, MX leads to a 6.9 % cluster power reduction\
    \ with near-peak FPU utilization.\n\nThese analyses on both small- and large-scale\
    \ vector clusters demonstrate that MX significantly improves the energy efficiency\
    \ by reducing the power consumption related to the VRF accesses. MX also pushes\
    \ the FPU utilization closer to its peak with a negligible area overhead. A quantitative\
    \ comparison of MX against [7], [8] is hard since none of them presents area or\
    \ power results, and the effective MatMul speed-up is unclear [7].\n\nTable IV\
    \ THE SUMMARY OF KERNEL INFORMATION, EXECUTION PERFORMANCE AND ENERGY EFFICIENCY\n\
    \n| Config                   | Mtx Size<br>[M, N, K] | Tile Size<br>[m, n, k]\
    \ | Sub-Tile Size<br>[m', n', k'] | Mem-VRF<br>Transfers | Arithmetic<br>Intensity<br>[FLOP/B]\
    \ | SIMD Ratio<br>[FLOP/vinsn] | Utilization | Performance<br>@ss freq<br>[GFLOPS]\
    \ | Performance<br>@tt freq<br>[GFLOPS] | Power<br>@tt freq<br>[W] | En. Efficiency<br>@tt\
    \ freq<br>[GFLOPS/W] |\n|--------------------------|-----------------------|------------------------|-------------------------------|----------------------|-------------------------------------|----------------------------|-------------|-------------------------------------|-------------------------------------|--------------------------|------------------------------------------|\n\
    | Dual-Core Cluster(a) (b) |                       |                        |\
    \                               |                      |                     \
    \                |                            |             |                \
    \                     |                                     |                \
    \          |                                          |\n| Baseline          \
    \       | 64x64x64              | 8,16,1                 | -                 \
    \            | 53248                | 1.23                                | 16.00\
    \                      | 95.9%       | 14.13                               | 15.34\
    \                               | 0.21                     | 71.49           \
    \                         |\n| Baseline                 | 64x64x64           \
    \   | 4,32,1                 | -                             | 77824         \
    \       | 0.84                                | 32.00                      | 97.8%\
    \       | 14.41                               | 15.65                        \
    \       | 0.21                     | 73.48                                   \
    \ |\n| Baseline                 | 32x32x32              | 8,16,1             \
    \    | -                             | 7168                 | 1.14           \
    \                     | 16.00                      | 90.0%       | 13.26     \
    \                          | 14.40                               | 0.20      \
    \               | 70.95                                    |\n| Baseline     \
    \            | 32x32x32              | 4,32,1                 | -            \
    \                 | 10240                | 0.80                              \
    \  | 32.00                      | 93.3%       | 13.75                        \
    \       | 14.93                               | 0.20                     | 72.87\
    \                                    |\n| Baseline                 | 16x16x16\
    \              | 8,16,1                 | -                             | 1024\
    \                 | 1.00                                | 16.00              \
    \        | 70.1%       | 10.33                               | 11.22         \
    \                      | 0.16                     | 71.69                    \
    \                |\n| Baseline                 | 16x16x16              | 4,32,1\
    \                 | -                             | 1408                 | 0.73\
    \                                | 32.00                      | 64.7%       |\
    \ 9.53                                | 10.35                               |\
    \ 0.16                     | 66.70                                    |\n| MX-ready\
    \                 | 64x64x64              | 4,8,4                  | 4,4,4   \
    \                      | 102400               | 0.64                         \
    \       | 34.73                      | 94.1%       | 13.86                   \
    \            | 15.06                               | 0.21                    \
    \ | 72.91                                    |\n| MX-ready                 | 64x64x64\
    \              | 8,8,4                  | 8,4,4                         | 69632\
    \                | 0.94                                | 63.22               \
    \       | 95.6%       | 14.08                               | 15.30          \
    \                     | 0.19                     | 79.15                     \
    \               |\n| MX-ready                 | 64x64x64              | 4,16,4\
    \                 | 4,4,4                         | 86016                | 0.76\
    \                                | 36.76                      | 96.4%       |\
    \ 14.20                               | 15.42                               |\
    \ 0.21                     | 75.19                                    |\n| MX-ready\
    \                 | 64x64x64              | 8,16,4                 | 8,4,4   \
    \                      | 53248                | 1.23                         \
    \       | 66.59                      | 97.2%       | 14.32                   \
    \            | 15.55                               | 0.19                    \
    \ | 81.49                                    |\n| MX-ready                 | 32x32x32\
    \              | 4,8,4                  | 4,4,4                         | 13312\
    \                | 0.62                                | 34.29               \
    \       | 88.4%       | 13.02                               | 14.14          \
    \                     | 0.20                     | 71.90                     \
    \               |\n| MX-ready                 | 32x32x32              | 8,8,4\
    \                  | 8,4,4                         | 9216                 | 0.89\
    \                                | 62.48                      | 89.7%       |\
    \ 13.22                               | 14.35                               |\
    \ 0.18                     | 77.68                                    |\n| MX-ready\
    \                 | 32x32x32              | 4,16,4                 | 4,4,4   \
    \                      | 11264                | 0.73                         \
    \       | 36.21                      | 92.7%       | 13.66                   \
    \            | 14.83                               | 0.20                    \
    \ | 74.36                                    |\n| MX-ready                 | 32x32x32\
    \              | 8,16,4                 | 8,4,4                         | 7168\
    \                 | 1.14                                | 65.68              \
    \        | 93.5%       | 13.78                               | 14.96         \
    \                      | 0.19                     | 80.38                    \
    \                |\n| MX-ready                 | 16x16x16              | 4,8,4\
    \                  | 4,4,4                         | 1792                 | 0.57\
    \                                | 33.45                      | 63.1%       |\
    \ 9.30                                | 10.10                               |\
    \ 0.15                     | 67.45                                    |\n| MX-ready\
    \                 | 16x16x16              | 8,8,4                  | 8,4,4   \
    \                      | 1280                 | 0.80                         \
    \       | 61.09                      | 66.1%       | 9.74                    \
    \            | 10.58                               | 0.14                    \
    \ | 75.03                                    |\n| MX-ready                 | 16x16x16\
    \              | 4,16,4                 | 4,4,4                         | 1536\
    \                 | 0.67                                | 35.20              \
    \        | 71.6%       | 10.55                               | 11.46         \
    \                      | 0.16                     | 72.03                    \
    \                |\n| MX-ready                 | 16x16x16              | 8,16,4\
    \                 | 8,4,4                         | 1024                 | 1.00\
    \                                | 64.00                      | 70.3%       |\
    \ 10.36                               | 11.25                               |\
    \ 0.15                     | 75.41                                    |\n| 64-Core\
    \ Cluster(c)       |                       |                        |        \
    \                       |                      |                             \
    \        |                            |             |                        \
    \             |                                     |                        \
    \  |                                          |\n| Baseline                 |\
    \ 256x256x256           | 8,32,1                 | -                         \
    \    | 2686976              | 3.12                                | 32       \
    \                  | 94.5%       | 372.26                              | 439.94\
    \                              | 1.57                     | 279.86           \
    \                        |\n| Baseline                 | 128x128x128         \
    \  | 8,32,1                 | -                             | 344064         \
    \      | 3.05                                | 32                         | 90.7%\
    \       | 357.34                              | 422.31                       \
    \       | 1.57                     | 268.64                                  \
    \ |\n| Baseline                 | 64x64x64              | 8,8,1              \
    \    | -                             | 69632                | 1.88           \
    \                     | 8                          | 50.4%       | 198.57    \
    \                          | 234.68                              | 1.20      \
    \               | 194.91                                   |\n| MX-ready     \
    \            | 256x256x256           | 8,32,8                 | 8,4,8        \
    \                 | 2686976              | 3.12                              \
    \  | 137.74                     | 96.7%       | 380.74                       \
    \       | 449.97                              | 1.46                     | 307.35\
    \                                   |\n| MX-ready                 | 128x128x128\
    \           | 8,32,8                 | 8,4,8                         | 344064\
    \               | 3.05                                | 136.23               \
    \      | 95.8%       | 377.27                              | 445.86          \
    \                    | 1.46                     | 304.55                     \
    \              |\n| MX-ready                 | 64x64x64              | 8,8,8 \
    \                 | 8,4,8                         | 69632                | 1.88\
    \                                | 123.43                     | 78.7%       |\
    \ 309.99                              | 366.35                              |\
    \ 1.50                     | 244.24                                   |\n\n(a)\
    \ In bold, we highlight the best metrics for both the Baseline and MX-ready Dual-Core\
    \ Cluster execution across various matrix sizes.\n\n(b) Dual-Core Cluster: Double-Precision\
    \ operations; ss freq = 920MHz; tt freq = 1GHz.\n\n(c) 64-Core Cluster: Single-Precision\
    \ operations; ss freq = 770MHz; tt freq = 910MHz.\n\n## V. CONCLUSION\n\nIn this\
    \ paper, we presented MX, an RVV-based ISA extension to support tiled matrix operations\
    \ for energy-efficient MatMuls. With an embedded-device-friendly and extremely\
    \ low footprint overhead, MX enhances the energy efficiency of MatMul by means\
    \ of a small tile buffer near the FPUs, which minimizes the VRF accesses by storing\
    \ and reusing both input and output matrix tiles. Moreover, MX reduces the number\
    \ of instructions fetched by the scalar core, decreases the interaction between\
    \ the scalar and vector cores, and regularizes the memory access pattern, further\
    \ reducing power consumption. We characterized MX by implementing it on two multi-core\
    \ clusters in a modern 12-nm technology node. With less than 3 % area overhead\
    \ and no impact on the operating frequency, MX significantly boosts MatMul's energy\
    \ efficiency of a Dual-Core cluster by up to 10.9 %. In a 64-Core cluster and\
    \ 64 \xD7 64 matrices, performance and energy efficiency improve by 56 % and 25\
    \ %, respectively, further pushing the FPU utilization toward the theoretical\
    \ peak.\n\n### ACKNOWLEDGMENTS\n\nThis project has received funding from the ISOLDE\
    \ project, No. 101112274, supported by the Chips Joint Undertaking of the European\
    \ Union's Horizon Europe's research and innovation program and its members Austria,\
    \ Czechia, France, Germany, Italy, Romania, Spain, Sweden, Switzerland.\n\n####\
    \ REFERENCES\n\n[1] B. Peccerillo, M. Mannino, A. Mondelli, and S. Bartolini,\
    \ \"A survey on hardware accelerators: Taxonomy, trends, challenges, and perspectives,\"\
    \ *Journal of Systems Architecture*, vol. 129, p. 102561, 2022.\n\n- [2] H. Amiri\
    \ and A. Shahbahrami, \"Simd programming using Intel vector extensions,\" *J.\
    \ of Parallel and Distr. Comp.*, vol. 135, pp. 83\u2013100, 2020.\n- [3] S. Deng,\
    \ H. Zhao, W. Fang, J. Yin, S. Dustdar, and A. Y. Zomaya, \"Edge intelligence:\
    \ The confluence of edge computing and artificial intelligence,\" *IEEE Internet\
    \ of Things Journal*, vol. 7, no. 8, pp. 7457\u20137469, 2020.\n- [4] N. Jouppi,\
    \ C. Young, N. Patil, and D. Patterson, \"Motivation for and evaluation of the\
    \ first tensor processing unit,\" *IEEE Micro*, vol. 38, no. 3, pp. 10\u201319,\
    \ 2018.\n- [5] C. AI, \"Edge TPU performance benchmarks,\" 2020. [Online]. Available:\
    \ https://coral.ai/docs/edgetpu/benchmarks\n- [6] Y.-H. Chen, T. Krishna, J. S.\
    \ Emer, and V. Sze, \"Eyeriss: An energyefficient reconfigurable accelerator for\
    \ deep convolutional neural networks,\" *IEEE Journal of Solid-State Circuits*,\
    \ vol. 52, no. 1, pp. 127\u2013138, 2017.\n- [7] V. Verma, T. Tracy II, and M.\
    \ R. Stan, \"EXTREM-EDGE EXtensions To RISC-V for Energy-efficient ML inference\
    \ at the EDGE of IoT,\" *Sust. Comp.: Informatics and Systems*, vol. 35, p. 100742,\
    \ 2022.\n- [8] T-Head Semiconductor, *RISC-V Matrix Multiplication Extension Specification*,\
    \ T-Head Semiconductor, 2023. [Online]. Available: https:// github.com/T-head-Semi/riscv-matrix-extension-spec/releases/tag/v0.3.0\n\
    - [9] M. Cavalcante, D. Wuthrich, M. Perotti, S. Riedel, and L. Benini, \"Spatz:\
    \ A \xA8 compact vector processing unit for high-performance and energy-efficient\
    \ shared-L1 clusters,\" in *Proc. of the 41st ICCAD*. San Diego, CA, USA: IEEE/ACM,\
    \ Oct. 2022.\n- [10] H. T. Kung, \"Memory requirements for balanced computer architectures,\"\
    \ *SIGARCH Comp. Arch. News*, vol. 14, no. 2, p. 49\u201354, May 1986.\n- [11]\
    \ B. Dally, \"Hardware for deep learning,\" in *Hot Chips*, Stanford, CA, USA,\
    \ Aug. 2023.\n- [12] M. Perotti, M. Cavalcante, N. Wistoff, R. Andri, L. Cavigelli,\
    \ and L. Benini, \"A 'New Ara' for Vector Computing: an Open Source Highly Efficient\
    \ RISC-V V 1.0 Vector Processor Design,\" in *Proceedings of the 33rd IEEE Int.\
    \ Conf. on ASAP*. Gothenburg, Sweden: IEEE, Jul. 2022.\n- [13] S. Riedel, M. Cavalcante,\
    \ R. Andri, and L. Benini, \"MemPool: A scalable manycore architecture with a\
    \ low-latency shared L1 memory,\" *IEEE Transactions on Computers*, 2023, early\
    \ access.\n- [14] M. Bertuletti, Y. Zhang, A. Vanelli-Coralli, and L. Benini,\
    \ \"Efficient parallelization of 5G-PUSCH on a scalable RISC-V many-core processor,\"\
    \ in *Proc. of the 2023 DATE Conf.* Antwerp, Belgium: IEEE, Mar. 2023."
  paper_id: 1
  scores:
    originality:
      raw: 4.5
      reason: The paper presents a novel enhancement to the RISC-V architecture, which
        is a significant contribution to the field of hardware design for AI/ML applications.
    technical_soundness:
      raw: 4.0
      reason: The proposed methods are well-founded and technically sound, though
        some implementation details could be elaborated for clarity.
    clarity:
      raw: 4.2
      reason: The writing is generally clear and well-structured, but some sections
        could benefit from more concise explanations to enhance understanding.
    significance:
      raw: 4.5
      reason: The work addresses a critical need for energy-efficient computation
        in edge AI/ML applications, making it highly significant in the current research
        landscape.
    relevance:
      raw: 5.0
      reason: The topic is highly relevant to ongoing research in hardware architectures
        for machine learning, particularly in the context of RISC-V and energy efficiency.
  total_score: 4.34
  tier: A
- title: "X-HEEP: An Open-Source, Configurable and Extendible RISC-V\n  Microcontroller\
    \ for the Exploration of Ultra-Low-Power Edge Accelerators"
  abstract: "Current soft processor architectures for FPGAs do not utilize the potential\n\
    of the massive parallelism available. FPGAs now support many thousands of\nembedded\
    \ floating point operators, and have similar computational densities to\nGPGPUs.\
    \ Several soft GPGPU or SIMT processors have been published, but the\nreported\
    \ large areas and modest Fmax makes their widespread use unlikely for\ncommercial\
    \ designs. In this paper we take an alternative approach, building the\nsoft GPU\
    \ microarchitecture around the FPGA resource mix available. We\ndemonstrate a\
    \ statically scalable soft GPGPU processor (where both parameters\nand feature\
    \ set can be determined at configuration time) that always closes\ntiming at the\
    \ peak speed of the slowest embedded component in the FPGA (DSP or\nhard memory),\
    \ with a completely unconstrained compile into a current Intel\nAgilex FPGA. We\
    \ also show dynamic scalability, where a subset of the thread\nspace can be specified\
    \ on an instruction-by-instruction basis.\n  For one example core type, we show\
    \ a logic range -- depending on the\nconfiguration -- of 4k to 10k ALMs, along\
    \ with 24 to 32 DSP Blocks, and 50 to\n250 M20K memories. All of these instances\
    \ close timing at 771 MHz, a\nperformance level limited only by the DSP Blocks.\
    \ We describe our methodology\nfor reliably achieving this clock rate by matching\
    \ the processor pipeline\nstructure to the physical structure of the FPGA fabric.\
    \ We also benchmark\nseveral algorithms across a range of data sizes, and compare\
    \ to a commercial\nsoft RISC processor."
  url: http://arxiv.org/abs/2401.04261v1
  keywords: ''
  document: "# A Statically and Dynamically Scalable Soft GPGPU\n\nMartin Langhammer\
    \ Intel Corporation & Imperial College London London, UK martin.langhammer@intel.com\n\
    \n## ABSTRACT\n\nCurrent soft processor architectures for FPGAs do not utilize\
    \ the potential of the massive parallelism available. FPGAs now support many thousands\
    \ of embedded floating point operators, and have similar computational densities\
    \ to GPGPUs. Several soft GPGPU or SIMT processors have been published, but the\
    \ reported large areas and modest Fmax makes their widespread use unlikely for\
    \ commercial designs. In this paper we take an alternative approach, building\
    \ the soft GPU microarchitecture around the FPGA resource mix available. We demonstrate\
    \ a statically scalable soft GPGPU processor (where both parameters and feature\
    \ set can be determined at configuration time) that always closes timing at the\
    \ peak speed of the slowest embedded component in the FPGA (DSP or hard memory),\
    \ with a completely unconstrained compile into a current Intel Agilex FPGA. We\
    \ also show dynamic scalability, where a subset of the thread space can be specified\
    \ on an instruction-by-instruction basis.\n\nFor one example core type, we show\
    \ a logic range \u2013 depending on the configuration \u2013 of 4k to 10k ALMs,\
    \ along with 24 to 32 DSP Blocks, and 50 to 250 M20K memories. All of these instances\
    \ close timing at 771 MHz, a performance level limited only by the DSP Blocks.\
    \ We describe our methodology for reliably achieving this clock rate by matching\
    \ the processor pipeline structure to the physical structure of the FPGA fabric.\
    \ We also benchmark several algorithms across a range of data sizes, and compare\
    \ to a commercial soft RISC processor.\n\n## 1 INTRODUCTION\n\nFPGAs are capable\
    \ platforms, with multiple thousands of embedded memories as well as DSP Blocks,\
    \ many of which now support IEEE 754 floating point numerics. In addition, there\
    \ is a significant amount of high performance IP available for FPGAs, e.g. FFTs\
    \ [\\[2,](#page-10-0) [8\\]](#page-10-1) and error correction such as Reed-Solomon\
    \ codecs [\\[3\\]](#page-10-2). High performance systems can readily be assembled\
    \ using a combination of original design and these IP Blocks. The value of the\
    \ FPGA is integration: although each individual IP or function is lower performance\
    \ than ASIC, this is offset by the flexibility. However, modifying IP - even your\
    \ own - requires significant effort. FPGA hardware compile times (synthesis, place\
    \ and route) can take hours, and timing closure can be a significant unknown.\
    \ Implementing (and modifying) a complex subset of a system by a pure software\
    \ approach, where the result of the compile or assembly is essentially instantly\
    \ available, and loaded onto an already placed and routed processor, may be very\
    \ attractive.\n\nSoft RISC cores (Nios [\\[9\\]](#page-10-3) and MicroBlaze [\\\
    [4\\]](#page-10-4)) for FPGA have been used for over two decades, and allow the\
    \ inclusion of complex control flow, or the offload of ancillary functions. Although\
    \ these RISC processors are very flexible, they also have a rather low\n\nGeorge\
    \ A. Constantinides Imperial College London London, UK g.constantinides@imperial.ac.uk\n\
    \nperformance. Parallel processor architectures may offer better performance,\
    \ and SIMT (GPGPU) processors may be able to efficiently use the large number\
    \ of memory and DSP Blocks distributed across the FPGA device. There have been\
    \ a number of soft SIMT FPGA architectures published [\\[14,](#page-10-5) [15,](#page-10-6)\
    \ [17,](#page-10-7) [18,](#page-10-8) [24,](#page-10-9) [25,](#page-10-10) [29\\\
    ]](#page-10-11), but these are often very large (50K-300K LUTs), and typically\
    \ have a low clock frequency (30MHz-100MHz). Other types of parallel processors\
    \ are also known for FPGA [\\[22,](#page-10-12) [30,](#page-10-13) [31\\]](#page-10-14)\
    \ (and commercialized [\\[5\\]](#page-10-15)), but the Fmax is relatively low\
    \ at \u223C150MHz.\n\nA different approach has been taken by Xilinx (now AMD)\
    \ in the Versal devices, with arrays of AI Engines, a hardened VLIW processor.\
    \ This motivates us to consider whether we can combine the flexibility of a soft\
    \ processor (where any number can be instantiated into the soft fabric), but with\
    \ the performance of an ASIC implementation (in this case, running at the speed\
    \ of the embedded hardened features).\n\nOur design, which we call the eGPU (for\
    \ embeddedGPU), is both statically and dynamically scalable, features which make\
    \ it particularly useful and performant for FPGA applications. Static scalability\
    \ is the ability to parameterize the thread space, shared memory space, integer\
    \ ALU functions, as well as major processor features (such as predicates). Dynamic\
    \ scalability allows us to operate on a defined subset of the thread space, and\
    \ change this on an instruction by instruction basis, without any dead time. We\
    \ will see that this can greatly reduce the number of cycles required in some\
    \ portions of the program, such as during a vector reduction (which is a common\
    \ kernel of GPGPU applications).\n\nWe make the following contributions:\n\n-\
    \ Describe a novel parameterized SIMT processor for FPGA, with a wide range of\
    \ user defined instructions, as well as architectural trade-offs (such as predicates).\n\
    - Demonstrate that a soft processor can consistently close timing at a level limited\
    \ only by the embedded features such as DSP and memory, and do so with a completely\
    \ unconstrained compile.\n- Compare the absolute and normalized (by resource cost)\
    \ results of a soft GPGPU with a soft RISC processor, and show that the SIMT architecture\
    \ is better in the general case, and significantly better when using dedicated\
    \ hardware extensions.\n\n## 2 BACKGROUND\n\nOur goal for this project was to\
    \ architect and implement a compact, high performance SIMT processor, that can\
    \ be used for commercial FPGA system designs. We can use current and prior FPGA\
    \ processors both to understand the limitations of previous projects, and to validate\
    \ some of our design choices. The axes of comparison to other work include memory\
    \ systems, complexity (such as workload balancing), and trade-offs between hard\
    \ and soft implementation.\n\n#### Table 1: Resource Comparison\n\n<span id=\"\
    page-1-0\"></span>\n\n| Architecture<br>Config. |          | LUTs | DSP  | FMax\
    \ | PPA | Device     |  |\n|-------------------------|----------|------|------|------|-----|------------|--|\n\
    | FGPU [15]               | 2CUx8PE  | 57K  | 48   | 250  | 36  | Zynq-7000  |\
    \  |\n| DO-GPU [29]             | 4CUx8PE  | 360K | 1344 | 208  | 133 | Stratix\
    \ 10 |  |\n| FlexGrip [17]           | 1SMx16PE | 114K | 300  | 100  | 175 | Virtex-6\
    \   |  |\n| eGPU                    | 1SMx16SP | 5K   | 24   | 771  | 1   | Agilex\
    \     |  |\n\nMany of the previously published GPGPUs [\\[15,](#page-10-6) [17,](#page-10-7)\
    \ [24,](#page-10-9) [29\\]](#page-10-11) are SIMT processors which were compiled\
    \ to an FPGA, whereas eGPU was designed for FPGA. The eGPU has an power-performancearea\
    \ (PPA) metric which is one or two orders of magnitude (OOM) smaller than some\
    \ of the earlier soft GPGPUs. Comparisons between high-performance processor designs\
    \ are complex and multidimensional. For example, some existing soft GPUs have\
    \ more complex memory systems, including caches and dynamic workload balancing.\
    \ This does come with a cost, with a typical order of magnitude resource difference,\
    \ as can be seen in Table [1,](#page-1-0) where we compare configurations of the\
    \ other soft GPGPUs that are closest in computational structure to eGPU (PEs are\
    \ roughly the same as SPs). Despite the much deeper pipelines (e.g. FlexGrip [\\\
    [17\\]](#page-10-7) has a 21 deep pipeline, FGPU has a 18 deep pipeline [\\[15\\\
    ]](#page-10-6)), they also run at a considerably slower clock frequency. Although\
    \ they are implemented in older FPGA technology (FlexGrip is in Virtex-6 at 100MHz),\
    \ this does not fully explain the performance level, as there are soft processors\
    \ that run at 450MHz in those devices [\\[20\\]](#page-10-16) [\\[21\\]](#page-10-17).\
    \ In the benchmarking section we will also see that the benchmarks also run slower\
    \ than expected on the earlier GPGPUs based on the difference in clock frequency.\n\
    \nInstead, we validate eGPU against existing soft RISC processors [\\[9\\]](#page-10-3),\
    \ which are extensively used in real applications. We will normalize the benchmark\
    \ results based on cost i.e. FPGA resources consumed. The eGPU, being a parallel\
    \ processor (with essentially 16 smaller multi-threaded processors) will naturally\
    \ be larger; to be effective and usable, it must have a clear advantage in both\
    \ absolute performance and normalized efficiency over the RISC processors.\n\n\
    eGPU uses a single local data memory, which is configurable in size, and does\
    \ not support a cache. Larger datasets need to be externally managed. Like the\
    \ eGPU, the Xilinx AI Engines [\\[13\\]](#page-10-18), which are organized as\
    \ hard VLIW hard processor arrays, have only a single local data memory per CPU,\
    \ the loading and unloading of which has to be managed externally. Algorithms\
    \ with larger amounts of data (such as 4K FFTs) need to be split across multiple\
    \ AI Engines [\\[6\\]](#page-10-19). The eGPU has a greater memory flexibility,\
    \ as we are able to configure a larger shared memory instance (we show examples\
    \ with up to 128KB in this paper). The AI Engines give us an example of a commercial\
    \ FPGA parallel processor, where using multiple simpler processors have been found\
    \ to have an advantage over using complex memory systems.\n\n## 3 ARCHITECTURE\
    \ DESCRIPTION\n\nThe architecture of the eGPU is based on an earlier proof-of-concept\
    \ design [\\[28\\]](#page-10-20). Our new design adds significant scalability\
    \ - thread and register space, shared memory size, instruction set support, as\
    \ well as optional predicates for thread divergence. Figure [1](#page-2-0) shows\
    \ the top level architecture of the eGPU. The streaming multi-processor (SM) contains\
    \ 16 parallel scalar processors (SP), although only 8 are shown in the figure\
    \ for clarity. An optional dot-product core and special function unit (SFU) reciprocal\
    \ square root can be attached. We target the Intel Agilex [\\[23\\]](#page-10-21)\
    \ family of FPGAs in this work. The eGPU has a very short pipeline (8 stages)\
    \ compared to other GPUs; therefore, hazards are hidden for most programs. Consequently,\
    \ we do not provide hardware support for tracking hazards in the current version,\
    \ which in turn gives us an efficient and fast processor.\n\nTwo types of embedded\
    \ memories are now supported, simple dual port (DP) and the emulated quad port\
    \ (QP) blocks [\\[12\\]](#page-10-22). One of the largest performance limitations\
    \ of the earlier eGPU architecture was memory bandwidth. The QP memory will double\
    \ the write bandwidth, while at the same time reducing the number of embedded\
    \ memory blocks required (the 20K-bit M20K blocks) by half. The trade-off is that\
    \ in QP mode, the memory speed is reduced from 1 GHz to 600 MHz, which then becomes\
    \ the critical path in the processor. Resource, Fmax, and benchmark results are\
    \ all described later in this paper.\n\n## 3.1 Dynamic Scalability\n\nMost GPGPUs\
    \ support thread divergence by predicates (threadspecific conditionals) but these\
    \ have a potential significant performance impact, as all threads are run, whether\
    \ or not they are written back. In addition to predicates, the eGPU sequencer\
    \ supports an instruction by instruction specification of a subset of the thread\
    \ space, where only the indicated threads are run. If the program can be constructed\
    \ such that the data of interest can be written to the threads that can be isolated\
    \ by the dynamic thread allocation, then a large number of processing cycles can\
    \ be skipped. This is particularly noticeable in programs with many multi-cycle\
    \ instructions, such as reads and writes to shared memory. This will have a direct\
    \ impact on the benchmark performance (number of cycles).\n\nWe define a wavefront\
    \ as the maximum number of operations that can be run per clock cycle; with 16\
    \ SPs we have a wavefront width of 16. The thread block depth (alternately, the\
    \ wavefront depth) is the number of wavefronts per instruction, which is the initialized\
    \ thread size / 16. We feel these terms allow us to describe our dynamic thread\
    \ scalability more concisely.\n\nThe eGPU can be configured, on a cycle by cycle\
    \ basis, to act as a standard SIMT processor, a multi-threaded CPU, or a single\
    \ threaded MCU. While the number of clock cycles to execute all the threads for\
    \ an operation instruction (e.g. FP or INT) is dependent on the depth of the thread\
    \ block, loads and stores are multi-cycle (because of the limited number of ports\
    \ to shared memory). The impact of dynamically adjusting the width of certain\
    \ instructions (e.g. reduction, where the writeback data can be orders of magnitude\
    \ less than the read data) can be seen in the benchmark section later in this\
    \ paper.\n\nThe upper 4-bit field in the instruction word (IW) allows the wavefront\
    \ width and depth to be coded for that instruction. Perhaps the most common case\
    \ will be using only the first SP, or even the first thread in the first SP; many\
    \ GPU applications will have vector reduction kernels, where a reduction result(s)\
    \ may end up in the leftmost SP. If we can operate on this SP exclusively for\
    \ a certain subset of time during the execution of the program, we A Statically\
    \ and Dynamically Scalable Soft GPGPU\n\n<span id=\"page-2-0\"></span>![](_page_2_Figure_1.jpeg)\n\
    \nFigure 1: eGPU SM Top Level Architecture\n\ncan save significant processing\
    \ time, and power. The coding of the personality is described in the Instruction\
    \ Set section.\n\nHence, we can change the scale of the SIMT span by reducing\
    \ the wavefront width and/or depth. The eGPU can act as a multithreaded CPU if\
    \ we set the wavefront width to one, and if we also set the thread depth to one,\
    \ each instruction will only act on thread 0 of the first SP - this SP can then\
    \ be used like a MCU. We will use these modes to good effect in our benchmarks\
    \ later in this paper.\n\n<span id=\"page-2-1\"></span>![](_page_2_Figure_5.jpeg)\n\
    \n## 3.2 Predicates\n\nThe eGPU optionally - by user configuration - supports\
    \ predicates, which enable thread divergence. Conditionals can be applied to each\
    \ thread individually using a condition instruction (see Table [2\\)](#page-3-0).\n\
    \nAs with many aspects of eGPU, the number and type of conditions can be selected\
    \ at compile time. Although these will have only a minimal impact on area, the\
    \ additional wireload may impact performance because of the large number of individual\
    \ predicate stacks. There is one predicate stack per initialized thread, so there\
    \ may be thousands of stacks per eGPU instance.\n\nSome algorithms, such as the\
    \ bitonic sort benchmark in this paper, require predicates. On the other hand,\
    \ many of the signal processing applications that we expect that the eGPU will\
    \ be used for (such as FFTs and matrix decomposition) do not use data dependent\
    \ decisions. These do not need predicates, and can be programmed using only loop\
    \ constructs, which are supported in the eGPU sequencer. For this reason, the\
    \ presence and complexity of predication is a parameter of our design, especially\
    \ considering the large potential cost of the feature.\n\nFigure [2](#page-2-1)\
    \ shows the structure of a single predicate block. Each SP has a separate block,\
    \ which are comprised of multiple predicate stacks. Each thread has a unique predicate\
    \ stack. Multiple nested levels of conditional operations (IF/ELSE/END IF) are\
    \ supported\n\nFigure 2: One Predicate Block\n\nper stack, with the maximum supported\
    \ depth of nesting being parameterized.\n\nThe incremental cost of adding one\
    \ level of nesting is trivial, as the control logic of each predicate stack is\
    \ the dominant user of logic here. The wavefront value (for example, in our base\
    \ eGPU configuration of 512 threads with 16 SPs, there will be 32 wavefronts -\
    \ i.e. 32 threads per SP) enables the correct predicate stack for the current\
    \ thread. If the condition instruction (IF) condition is true for that thread,\
    \ a '1' will be set at the top of the predicate stack, and the rest of the stack\
    \ pushed down. An ELSE instruction will invert the top of the stack, and an END\
    \ IF will pop the stack and return to the previous nesting level.\n\nThe eGPU\
    \ is configured at compile time for a maximum number of threads; if the run time\
    \ configuration of threads is less than this, there is no issue as only the selected\
    \ threads will trigger the operation of the predicate block.\n\n<span id=\"page-3-1\"\
    ></span>\n\n| [43:40]  |        |      | [39:34] [33:32] [31:27] | [26:22] | [21:17]\
    \ | [16:1]    |\n|----------|--------|------|-------------------------|---------|---------|-----------|\n\
    | Variable | Opcode | Typ. | RD                      | RA      | RB      | Immediate\
    \ |\n\nFigure 3: Instruction Word\n\nThe conditional value will only be applied\
    \ to the current predicate block, and all others ignored in that clock cycle.\
    \ The current thread activation thread\\_active signal will be muxed from all\
    \ the predicate blocks, selected by the current wavefront. The thread\\_active\
    \ signal is used to pass or zero the write\\_enable signals to either the register\
    \ files or shared memory, whichever is the destination for that instruction.\n\
    \n## 4 INSTRUCTION SET\n\nTable [2](#page-3-0) shows most of the instruction set\
    \ for the eGPU. There are a total of 61 instructions, including 18 conditional\
    \ cases (we omit the FP conditional instructions here for brevity). Usually, only\
    \ a subset of instructions are included (by the user defined configuration of\
    \ the eGPU). The 18 conditional cases depend on predicates being included in the\
    \ parameters - as predicates typically increase soft logic cost by 50% they are\
    \ only used when the expected class of applications need them. Many of the intended\
    \ applications, such as FFT, matrix multiplication and decomposition, do not,\
    \ and the required loops can be handled with the dedicated loop instructions.\
    \ Some instructions can support multiple TYPES, such as signed (INT32) and unsigned\
    \ (UINT32) formats for integer instructions.\n\nThe integer ALU uses a large proportion\
    \ of the soft logic (\u2248100 ALMs to \u2248400 ALMs), so selecting only the\
    \ required precision (16 bit or 32-bit) and feature subset can reduce the cost\
    \ of the eGPU substantially. Extension instructions are also optional. We will\
    \ use the dot product instruction for some of the benchmarks in this paper; if\
    \ used, it can make significant difference to the performance of some functions.\
    \ We can also add elementary functions (currently we support only reciprocal square\
    \ root), which are required for algorithms such as matrix decomposition. In contrast,\
    \ the FP instructions are almost completely contained inside the DSP Block, with\
    \ only the FP Max() and Min() instructions having a potential impact on area or\
    \ performance.\n\nFigure [3](#page-3-1) shows an instruction word, here shown\
    \ in a 43-bit form. As the number of registers per thread changes, the three register\
    \ field widths also change; the displayed word is for a 32 registers per thread\
    \ configuration, which requires 5 bits to encode the register number. The 2-bit\
    \ representation field encodes whether the number is unsigned integer, signed\
    \ integer, or FP32. The four most significant bits encode the processing type,\
    \ which allow the wavefront depth and the width of the wavefront to be changed\
    \ on an instruction by instruction basis.\n\nWriting these results into shared\
    \ memory using subset write can be 16x faster than using the generic write. An\
    \ instruction, whether used for a full or a partial thread space, is almost identical,\
    \ with only the four instruction type bits used to control the subset of the thread\
    \ space. Table [3](#page-3-2) shows how the upper 4 bits of the IW control the\
    \ width and depth of the thread space.\n\n| Table 2: Instruction Set |  |\n|--------------------------|--|\n\
    |--------------------------|--|\n\n<span id=\"page-3-0\"></span>\n\n| Group  \
    \            | Instruction            |                         |  |  |  |  |\n\
    |--------------------|------------------------|-------------------------|--|--|--|--|\n\
    |                    | ADD.TYPE Rd,Ra,Rb      | Rd = Ra + Rb            |  | \
    \ |  |  |\n|                    | SUB.TYPE Rd,Ra,Rb      | Rd = Ra - Rb      \
    \      |  |  |  |  |\n| Integer Arithmetic | NEG.TYPE Rd,Ra         | Rd = -Ra\
    \                |  |  |  |  |\n|                    | ABS.TYPE Rd,Ra        \
    \ | Rd = absolute(Ra)       |  |  |  |  |\n|                    | MUL16LO.TYPE\
    \ Rd,Ra,Rb  | Rd = Ra * Rb            |  |  |  |  |\n|                    | MUL16HI.TYPE\
    \ Rd,Ra,Rb  | Rd = (Ra * Rb)\xBB16       |  |  |  |  |\n| Integer Multiply   |\
    \ MUL24.LO.TYPE Rd,Ra,Rb | Rd = Ra * Rb            |  |  |  |  |\n|          \
    \          | MUL24.HI.TYPE Rd,Ra,Rb | Rd = (Ra * Rb)\xBB24       |  |  |  |  |\n\
    |                    | AND Rd,Ra,Rb           | Rd = Ra & Rb            |  | \
    \ |  |  |\n|                    | OR Rd,Ra,Rb            | Rd = Ra \u2225 Rb \
    \           |  |  |  |  |\n|                    | XOR Rd,Ra,Rb           | Rd\
    \ = Ra \u2295 Rb            |  |  |  |  |\n| Integer Logic      | NOT Rd,Ra  \
    \            | Rd = !Ra                |  |  |  |  |\n|                    | cNOT\
    \ Rd,Ra             | Rd = (Ra == 0)?1:0      |  |  |  |  |\n|               \
    \     | BVS Rd,Ra              | Rd = bit_reverse(Ra)    |  |  |  |  |\n|    \
    \                | SHL.TYPE Rd,Ra,Rb      | Rd = Ra \u226A Rb            |  |\
    \  |  |  |\n| Integer Shift      | SHR.TYPE Rd,Ra,Rb      | Rd = Ra \u226B Rb\
    \            |  |  |  |  |\n|                    | POP Rd,Ra              | Rd\
    \ = unary(Ra)          |  |  |  |  |\n| Integer Other      | MAX.TYPE Rd,Ra,Rb\
    \      | Rd = (Ra>Rb)?Ra:Rb      |  |  |  |  |\n|                    | MIN.TYPE\
    \ Rd,Ra,Rb      | Rd = (Ra<Rb)?Ra:Rb      |  |  |  |  |\n|                   \
    \ | ADD.FP32 Rd,Ra,Rb      | Rd = Ra + Rb            |  |  |  |  |\n|        \
    \            | SUB.FP32 Rd,Ra,Rb      | Rd = Ra - Rb            |  |  |  |  |\n\
    |                    | NEG.FP32 Rd,Ra         | Rd = -Ra                |  | \
    \ |  |  |\n| FP ALU             | ABS.FP32 Rd,Ra         | Rd = absolute(Ra) \
    \      |  |  |  |  |\n|                    | MUL.FP32 Rd,Ra,Rb      | Rd = Ra*Rb\
    \              |  |  |  |  |\n|                    | MAX.FP32 Rd,Ra,Rb      |\
    \ Rd = (Ra>Rb)?Ra:Rb      |  |  |  |  |\n|                    | MIN.FP32 Rd,Ra,Rb\
    \      | Rd = (Ra<Rb)?Ra:Rb      |  |  |  |  |\n|                    | eq    \
    \                 | \U0001D445\U0001D44E == \U0001D445\U0001D44F             \
    \   |  |  |  |  |\n|                    | ne                     | \U0001D445\U0001D44E\
    \ \u2260 \U0001D445\U0001D44F                 |  |  |  |  |\n| Int Compare   \
    \     | lt (INT), lo (UINT)    | \U0001D445\U0001D44E < \U0001D445\U0001D44F \
    \                |  |  |  |  |\n|                    | le (INT), ls (UINT)   \
    \ | \U0001D445\U0001D44E \u2264 \U0001D445\U0001D44F                 |  |  | \
    \ |  |\n|                    | gt (INT), hi (UINT)    | \U0001D445\U0001D44E >\
    \ \U0001D445\U0001D44F                 |  |  |  |  |\n|                    | ge\
    \ (INT), hs (UINT)    | \U0001D445\U0001D44E \u2265 \U0001D445\U0001D44F     \
    \            |  |  |  |  |\n| Memory             | LOD Rd (Ra)+offset     | Read\
    \ from Shared        |  |  |  |  |\n|                    | STO Rd (Ra)+offset\
    \     | Write to Shared         |  |  |  |  |\n| Immediate          | LOD Rd #Imm\
    \            | Rd = Imm                |  |  |  |  |\n| Thread             | TDx\
    \ Rd                 | Rd = Thread IDx         |  |  |  |  |\n|              \
    \      | TDy Rd                 | Rd = Thread IDy         |  |  |  |  |\n|   \
    \                 | DOT Rd,Ra,Rb           | Dot Product \u27E8\U0001D445\U0001D44E\
    , \U0001D445\U0001D44F\u27E9    |  |  |  |  |\n| Extension          | SUM Rd,Ra,Rb\
    \           | Reduction \u27E8\U0001D445\U0001D44E, \U0001D445\U0001D44F\u27E9\
    <br>\u221A |  |  |  |  |\n|                    | INVSQR Rd,Ra           | \U0001D445\
    \U0001D451 = 1/<br>\U0001D445\U0001D44E           |  |  |  |  |\n|           \
    \         | JMP address            | Jump to Address         |  |  |  |  |\n|\
    \                    | JSR address            | Subroutine Address      |  | \
    \ |  |  |\n|                    | RTS                    | Return from Subroutine\
    \  |  |  |  |  |\n| Control            | LOOP address           | Jump and Dec\
    \ Loop Ctr   |  |  |  |  |\n|                    | INIT loops             | Set\
    \ Loop Ctr            |  |  |  |  |\n|                    | STOP             \
    \      | Stop and Set Flag       |  |  |  |  |\n|                    | IF.cc \
    \                 | if cc true              |  |  |  |  |\n| Conditional     \
    \   | ELSE                   | if cc false             |  |  |  |  |\n|      \
    \              | ENDIF                  | clear cc                |  |  |  | \
    \ |\n\nTable 3: Thread Space Control\n\n<span id=\"page-3-2\"></span>\n\n| Coding\
    \ | Width [4:3]             | Depth [2:1]          |  |  |  |\n|--------|-------------------------|----------------------|--|--|--|\n\
    | \"00\"   | All (16 SPs)            | Wavefront 0 only     |  |  |  |\n| \"01\"\
    \   | 1/4 width (first 4 SPs) | all wavefronts       |  |  |  |\n| \"10\"   |\
    \ SP0 only                | first 1/2 wavefronts |  |  |  |\n| \"11\"   | Undefined\
    \               | first 1/4 wavefronts |  |  |  |\n\n## 5 RESULTS\n\nWe compiled\
    \ a number of different eGPU instances, using both DP and QP memory versions.\
    \ We used Quartus Pro V22.4 and targeted an Intel Agilex AGIB027R29A1E1V device\
    \ [\\[7\\]](#page-10-23). All of our results are reported for a single compilation\
    \ attempt (we did not use seed sweeps).\n\nThe DP memory results are tabulated\
    \ in Table [4.](#page-6-0) We define three categories - small, medium, and large\
    \ - to show the effects of different thread space, shared memory, and ALU features,\
    \ as well as the impact of supporting predicates. The base eGPU architecture is\
    \ the same for all instances: one SM with 16 SPs, a two read port register memory,\
    \ and a four read and one write port shared memory. We configured all of these\
    \ cases to use 512 threads, but with varying numbers of registers per thread.\
    \ QP memory results are shown in Table [5,](#page-6-1) the main architectural\
    \ change being the two write port shared memory.\n\nThe 'small' category uses\
    \ a 16-bit ALU, which will likely only be used for address generation. The minimum\
    \ specification supports only a single bit shift, as well as a 16-bit adder/subtractor,\
    \ and arithmetic logic (AND/OR/XOR) operations. The memory requirements for the\
    \ SPs is reduced by providing 16 registers per thread. The 'large' category implements\
    \ 64 registers per thread, and larger shared memory sizes with up to 128KB. The\
    \ integer ALU supports the full set of integer instructions defined in the previous\
    \ section. We also include a 'medium' category for further examples. Many other\
    \ combinations of parameters and features sets are possible as well.\n\n## 5.1\
    \ Impact of Register and Shared Memory Size\n\nBoth the thread registers and the\
    \ shared memories are implemented using M20K memories, which can be configured\
    \ into either DP (one read port and one write port active simultaneously) or QP\
    \ memories (two read ports and two write ports active simultaneously). The natural\
    \ datapath size of the eGPU is 32-bits, defined by the native IEEE 754 single\
    \ precision (FP32) floating point DSPs which will be doing the majority of the\
    \ calculations. In DP mode, a M20K can be configured as a 512x32-bit memory. Port\
    \ restrictions mean that in QP mode the M20K is a 2048x8-bit block, which requires\
    \ a larger minimum thread register space to take advantage of the extra ports.\n\
    \nIn DP mode thread registers are implemented in two dual port memories, providing\
    \ two read ports and one write port per clock cycle. In our most common eGPU configuration\
    \ (with 16 registers per thread), a 512 thread machine will require two M20Ks\
    \ per SP, or 32 M20Ks for thread registers in total, which is also the minimum\
    \ size. Both the number of registers per thread and the number of total threads\
    \ are parameterized, but the number of M20Ks will increase accordingly. In QP\
    \ mode, the 8-bit data port width means that there is no point in using less than\
    \ 2 thread registers per SP, although we will use half the number of M20K blocks\
    \ compared to the DP version when we configure at least these number of registers.\n\
    \nThe shared memory is implemented as a four read port, one write port per memory\
    \ in DP mode. The smallest possible shared memory is 512 words (2KB), which would\
    \ require four M20Ks. This is very small, and unlikely to be useful, as the shared\
    \ memory size would only be as large as the register space in a single SP. A more\
    \ realistic shared memory size would be 2 words (8KB), which would require 16\
    \ M20Ks; the total memory usage for a small eGPU instance, including registers,\
    \ would therefore be 48 M20Ks. The shared memory is set by parameter, and significantly\
    \ larger sizes are possible without frequency impact. For example, a 64KB shared\
    \ memory needs 128 M20Ks, and a 128KB shared memory 256 M20Ks, which is a small\
    \ fraction of the memories on the device. In QP\n\nmode, the number of M20Ks is\
    \ halved, and the number of write ports doubled to two.\n\n## 5.2 Integer ALU\
    \ Architecture and Resources\n\nUnlike the floating point arithmetic, which can\
    \ be mapped directly to a DSP Block, the simpler integer operations need to be\
    \ implemented in soft logic. We will see that up to half of the soft logic and\
    \ registers in an eGPU is required for the integer ALU. Table [6](#page-6-2) shows\
    \ the resources, split by operation type, for a wide range of integer ALUs.\n\n\
    The smallest reasonable integer ALU is a 16 bit version with single bit shifts,\
    \ which consumes 90 ALMs and 136 registers, most of which are used for the 5 stage\
    \ pipeline. Here we have a signed adder/subtractor, as well as logic functions\
    \ (in this case, only AND, OR, and XOR are supported). The more typical full 16-bit\
    \ ALU implementation supports signed and unsigned arithmetic, a more complete\
    \ set of logic operations (AND/OR/XOR/NOT/cNOT/BVS), full 16-bit left and right\
    \ shifts, population count, as well as max/min functions. The resource cost is\
    \ approximately double that of the minimum ALU. The 5 stage pipeline 32-bit version\
    \ again doubles the logic, as might be expected, but the number of registers triples,\
    \ as individual functions (specifically the adder/subtractor and shifters) are\
    \ themselves pipelined to ensure that the ALU always exceeds 800MHz. This contrasts\
    \ with the 16-bit ALU, where the pipelining is used to improve the placement of\
    \ the entire ALU, rather than improving the performance of any individual function.\
    \ There is also a 4 stage pipeline version of the 32-bit integer ALU, which is\
    \ about the size of the 16-bit full function ALU. This returns a lower performance\
    \ (typically 700 MHz), and is used in order to save logic for the QP version of\
    \ the eGPU (which has a lower target speed of 600MHz). The individual resource\
    \ counts in Table [6](#page-6-2) may not accurately reflect the impact of each\
    \ function to the overall ALU size, as synthesis may combine aspects of some functions\
    \ together.\n\n## 5.3 Predicate Resources\n\nIn Table [4](#page-6-0) and [5](#page-6-1)\
    \ the area impact of predicate support is clearly visible, increasing the soft\
    \ logic resources by about 50%. While each predicate stack (including its control)\
    \ is very small, each thread has a unique stack. The base predicate area consists\
    \ of only a thread comparator (which checks that the SP currently executing the\
    \ thread that the predicate circuit is associated with), an instruction decode\
    \ (IF/ELSE/ENDIF), and the single bit-wide predicate stack. This may only be 5\
    \ ALMs per thread, but if a typical eGPU contains 1 threads, the predicate circuitry\
    \ can quickly grow to be as large as the rest of the soft logic. Increasing the\
    \ stack depth will have only a minimal impact on area, as each additional level\
    \ consists of only a two input mux and a register.\n\n## 5.4 Instruction Fetch,\
    \ Decode, and Control\n\nThis section will always have a modest footprint, requiring\
    \ 200 to 250 ALMs, and a handful of M20Ks to store the instruction words. The\
    \ instruction decoder takes about 40 ALMs, and the thread generator around 25\
    \ ALMs. A single M20K can store 512 40-bit instruction words; the benchmarks we\
    \ analyse later in this paper range from\n\n30 instructions (32 element reduction)\
    \ to 250 instructions (256 element bitonic sort), so a multi-tenancy of programs\
    \ would only need several M20Ks.\n\nIncreasing the IW to 43 or 46 bits (which\
    \ is required to support a 32 and 64 registers per thread, respectively), adds\
    \ only a single M20K per 2 instructions, as the M20K containing the upper bits\
    \ would be configured in 8 format. In any case, the number of M20Ks needed for\
    \ program storage is small compared to the thread registers and shared memory.\
    \ For example, a 1 word program space would require three M20Ks, and a 4 program\
    \ space nine M20Ks.\n\n## 5.5 Calculating Resources and Setting Performance\n\n\
    Although the eGPU has a parameterized pipeline depth between the SPs and shared\
    \ memory, it can achieve the target performance (771MHz and 600MHz respectively)\
    \ using the minimum depth of 8 stages. The parameterized pipelining can be used\
    \ for future applications with larger shared memories, or when the shared memories\
    \ are placed elsewhere on the device, and not located near the SP array. We also\
    \ report the slowest path outside the embedded (M20K and DSP) resources (see Table\
    \ [4](#page-6-0) and [5\\)](#page-6-1). If needed, there are also additional pipelining\
    \ parameters inside the SP for the paths both to and from the FP and Integer ALUs.\
    \ We will show in the next section how additional pipelining may not improve Fmax\
    \ as the eGPU has been designed to fit into an Agilex sector in the base configuration.\n\
    \nWe can see that the SP overhead (mux and control) is \u2248150 ALMs, the integer\
    \ ALU ranges from \u2248100 ALMs to \u2248400 ALMs, and the predicates, if used,\
    \ start from \u2248 150ALMs. A single SP will therefore be as small as 250 ALMs,\
    \ and can be as large as 650 ALMs; this translates into a small eGPU core (16\
    \ SPs) requiring 4 ALMs, and over 10 ALMs for fully featured example.\n\nThe number\
    \ of M20Ks for the register memory for the DP eGPU can be calculated as threads\xD7\
    registers/256; for the shared memory the number of blocks is 2 \xD7 size(). The\
    \ number of M20K blocks required for the QP eGPU are half of this, except that\
    \ there is a minimum size (threads \xD7 registers\\_per\\_thread/16 > 2047) for\
    \ the number of registers, in which case the QP eGPU will need the same number\
    \ of register blocks as the DP version.\n\n## 5.6 FPGA Sector Resources and Impact\n\
    \nIt is most beneficial to select eGPU parameters around the available FPGA resources\
    \ and their on-chip organization. The Intel Agilex devices are arranged in sectors,\
    \ the most common of which contains about 16400 ALMs, 240 M20K memories, and 160\
    \ DSP Blocks. Although we are not limited to a single sector (additional pipelining\
    \ may be required to maintain performance across sector boundaries), this ratio\
    \ of resources provides a good guide how to parameterize a eGPU instance. In particular,\
    \ creating too large a register or memory space will be inefficient, as the ALMs\
    \ between the M20K columns will likely be unreachable by other designs in the\
    \ FPGA. Likewise, there is no point in specifying a small register space or shared\
    \ memory, as the M20Ks between the logic structures may not be accessible by other\
    \ functions. Further analysis is provided in the following section where we demonstrate\
    \ that by selecting parameters in this way, the eGPU consistently achieves the\
    \ reported\n\nperformance levels by matching its architecture with the sector\
    \ structure.\n\n## 6 REPEATABLE HIGH PERFORMANCE\n\nThis section provides the\
    \ required information to make our design process repeatable for those wishing\
    \ to achieve high performance in their own designs. It is therefore necessarily\
    \ 'close to metal' in abstraction. Although the details are specific to Intel\
    \ FPGAs, we believe the same approaches are valid for all other FPGAs as well.\n\
    \nThe eGPU is designed to give consistent performance, which will always be limited\
    \ by the slowest embedded (DSP or M20K memory) resource in that configuration.\
    \ The clock network in Agilex is specified at 1GHz, which is the absolute limit\
    \ of performance for any design. The M20K memories in DP mode also achieve 1GHz,\
    \ but only 600 MHz in QP mode. The DSP Blocks can run at 771 MHz when implementing\
    \ a FP32 multiply-add datapath with a 4 stage pipeline [\\[11\\]](#page-10-24).\
    \ We are therefore limited to a maximum speed of 771MHz, unless we use QP memory,\
    \ in which case the maximum frequency drops to 600MHz. The lower performance of\
    \ the QP memory, however, will allow us to support a higher density storage, and\
    \ the doubled write bandwidth may offer an overall higher throughput for some\
    \ applications. We will examine some of these trade-offs in the benchmarking section.\n\
    \nUsing the sector architecture effectively enables the eGPU performance and efficiency.\
    \ Sector resources are arranged in columns, each approximately 41 rows high (several\
    \ columns are shorter because of non-user accessible device features). Achieving\
    \ a 1GHz speed for soft logic does not require logic to be immediately adjacent\
    \ to each other, as there are different horizontal and vertical wire lengths -\
    \ too much pipelining can negatively impact performance as much as too little.\
    \ More important is using the minimal number of wire resources per connection.\
    \ In the Agilex devices, there is a constant 4 columns of logic between each column\
    \ of either DSP or M20K. In a sector we will have 40 columns of logic, 4 columns\
    \ of DSP, and 6 columns of M20K. There is little point in saving logic or memory\
    \ if it is not accessible by other portions of design.\n\nAs we have shown in\
    \ the previous section, the results are deterministic and repeatable, in both\
    \ area and performance. Ideally, the resource use would be balanced to realize\
    \ the maximum efficiency from the device.\n\nTo map eGPU to the device, we first\
    \ sketched out a LUT level construct of an SP, and adjusted it so that the number\
    \ of logic levels would align with the sector column ratios described above. Paths\
    \ directly between M20K memories (which implement the thread registers in each\
    \ SP) and the DSP Blocks had to fit into a 4 column group of LABs, and longer\
    \ pure logic paths (e.g. the integer ALUs) were organized so that the total area\
    \ did not spill over into a M20K or DSP column that might be used by another SP.\n\
    \nWe can see from the results (Table [6\\)](#page-6-2) that a 16-bit Integer ALU\
    \ is in the range of 100-200 ALMs and the 32-bit version requires 200- 400 ALMs.\
    \ If predicates are used, they will cost an additional 125-250 ALMs per SP, depending\
    \ on the defined thread space. The remaining 150 ALMs per SP are used for the\
    \ data muxing and alignment shown in Figure [1.](#page-2-0) We were able to implement\
    \ a small eGPU (the first example in Table [4\\)](#page-6-0) that was able to\
    \ close timing over 771 MHz with no dedicated soft logic registers (i.e. registers\
    \ that were not\n\n<span id=\"page-6-0\"></span>\n\n| Scale  | ALU<br>Precision\
    \ | Shift<br>Precision | Threads | Reg.<br>Thread | Shared<br>Memory | Predicate<br>Levels\
    \ | ALM   | Registers | DSP | M20K | Freq<br>(MHz) | SP<br>(ALM/Reg.) |\n|--------|------------------|--------------------|---------|----------------|------------------|---------------------|-------|-----------|-----|------|---------------|------------------|\n\
    | Small  | 16               | 1                  | 512     | 16             |\
    \ 8KB              | 0                   | 4243  | 13635     | 24  | 50   | 1018/771\
    \      | 224/707          |\n| Small  | 16               | 16                \
    \ | 512     | 16             | 32KB             | 5                   | 7518 \
    \ | 18992     | 24  | 98   | 898/771       | 413/979          |\n| Medium | 16\
    \               | 16                 | 512     | 32             | 32KB       \
    \      | 5                   | 7579  | 19155     | 24  | 131  | 883/771      \
    \ | 426/1043         |\n| Medium | 32               | 16                 | 512\
    \     | 32             | 32KB             | 5                   | 9754  | 25425\
    \     | 24  | 131  | 902/771       | 461/1277         |\n| Large  | 32       \
    \        | 16                 | 512     | 64             | 32KB             |\
    \ 8                   | 10127 | 26040     | 32  | 195  | 860/771       | 575/1505\
    \         |\n| Large  | 32               | 32                 | 512     | 64 \
    \            | 64KB             | 16                  | 10697 | 26618     | 32\
    \  | 259  | 841/771       | 600/1476         |\n\n#### Table 4: Fitting Results\
    \ - DP Memory\n\n#### Table 5: Fitting Results - QP Memory\n\n<span id=\"page-6-1\"\
    ></span>\n\n| Scale  | ALU       | Shift     | Threads | Regs./ | Shared | Predicate\
    \ | ALM   | Registers | DSP | M20K | Freq    | SP         |\n|--------|-----------|-----------|---------|--------|--------|-----------|-------|-----------|-----|------|---------|------------|\n\
    |        | Precision | Precision |         | Thread | Memory | Levels    |   \
    \    |           |     |      | (MHz)   | (ALM/Reg.) |\n| Small  | 32        |\
    \ 1         | 512     | 64     | 32KB   | 0         | 5468  | 14487     | 24 \
    \ | 98   | 840/600 | 287/830    |\n| Medium | 32        | 32        | 1024   \
    \ | 32     | 64KB   | 0         | 7057  | 16722     | 32  | 131  | 763/600 | 396/1016\
    \   |\n| Large  | 32        | 32        | 1024    | 32     | 64KB   | 16     \
    \   | 11314 | 25050     | 32  | 131  | 763/600 | 685/1601   |\n| Large  | 32 \
    \       | 32        | 1024    | 32     | 128KB  | 10        | 10174 | 23094  \
    \   | 32  | 195  | 714/600 | 556/1391   |\n\nTable 6: Fitting Results - Integer\
    \ ALU\n\n<span id=\"page-6-2\"></span>\n\n| Prec. | Type  | ALM | Registers |\
    \ Add/<br>Sub | Logic | SHL | SHR | Pop |\n|-------|-------|-----|-----------|-------------|-------|-----|-----|-----|\n\
    | 16    | Min   | 90  | 136       | 3           | 9     | -   | -   | -   |\n\
    | 16    | Small | 134 | 207       | 9           | 10    | 20  | 23  | -   |\n\
    | 16    | Full  | 199 | 269       | 9           | 18    | 20  | 23  | 11  |\n\
    | 32    | Min   | 208 | 406       | 5           | 27    | 28  | 28  | -   |\n\
    | 32    | Full  | 394 | 704       | 27          | 36    | 50  | 53  | 27  |\n\n\
    directly connected with a logic function, such as a mux), but for the generic\
    \ parameterized case, we added a single additional pipeline stage register between\
    \ the thread registers and the functional units, and also one level in the write-back\
    \ path between the functional units and the thread registers. For all of the examples\
    \ in Table [4](#page-6-0) there are also single pipeline stages to and from the\
    \ shared memory. We parameterized the pipeline depth for all of these stages,\
    \ along with the appropriate balancing delays for the data and control paths into\
    \ the write paths of the thread registers, but found that these were not needed\
    \ to be increased beyond one pipeline stage for any of the reported examples.\n\
    \nFigure [4](#page-7-0) shows the unconstrained placement of the largest instance\
    \ of Table [4.](#page-6-0) The shared memory and 8 out of the 16 SPs have been\
    \ color coded for identification. The shared memory creates a spine in the middle\
    \ of the core, with 8 SPs placed on either side of it. For purposes of illustration\
    \ we have colored a subset of SPs: three random SPs and the left of the spine,\
    \ and five contiguous ones on the right. Three things are evident with all SPs:\
    \ (a) the majority of the logic is in one contiguous block, (b) there is a separate\
    \ contiguous structure (the predicate block) placed some distance away, and (c),\
    \ the SP straddles a columns of DSP Blocks. All of the instances of Table [4](#page-6-0)\
    \ and [5](#page-6-1) display this pattern, including the shared memory spine.\n\
    \nFigure [5](#page-7-1) shows one of the SPs in greater detail (this SP is the\
    \ one marked by the black boxes in Figure [4\\)](#page-7-0). The largest component\
    \ is the integer ALU. The operators (adder/subtractor, shifters, arithmetic logic,\
    \ etc.) are in the 4 columns to the right of the two DSP blocks (the DSP Block\
    \ for the floating point operators is adjacent to the integer multiplier). To\
    \ the left of the DSP Blocks is largely pipelining logic - of the 5 pipeline stages\
    \ in the ALU, only one is used for pipelining the operators - the rest is used\
    \ to break up the paths between the thread register memories and the ALUs. We\
    \ examined all of the SP placements, and the placement of the M20Ks for the register\
    \ memories (8 M20Ks for this instance) was in one of three layouts: (a) a contiguous\
    \ single column (b) most of the registers in one column, with a smaller number\
    \ in the next column further away from the integer ALU, and (c) equally split\
    \ between two columns on either side of the integer ALU. In all of these cases,\
    \ the pipeline wrapper around the ALU was usually grouped together, and essentially\
    \ separate from the actual operators. Rather than having to be in a specific location\
    \ relative to the M20Ks and operators, the ability to split up a bus so that it\
    \ can be mapped to the same number of wire hops is what was important. Fewer pipeline\
    \ stages would have introduced a two stage routing path, which would have likely\
    \ become the critical path in the eGPU. (In the QP memory version, we can remove\
    \ one of the pipeline stages as the M20K becomes the slowest component at 600\
    \ MHz, and we can see that the removal of some of the pipeline path reduce the\
    \ non-memory path performance to just over 700 MHz). On the other hand, more than\
    \ a 5 stage integer ALU could potentially decrease performance as it could spread\
    \ out the placement of the SP.\n\nThe predicate circuitry is placed in another\
    \ contiguous block, but well away from the SP core it is associated with. From\
    \ Figure [4](#page-7-0) we can see that the majority of the other predicates have\
    \ a similar relationship with their respective SP. All of these have been automatically\
    \ placed by Quartus. This is possible because the interface to and from the predicate\
    \ block is very narrow, with only a single bit (thread\\_active signal) returned.\
    \ The signals to the block are relatively few: a thread index (typically 5 to\
    \ 8 bits wide), a 3-bit decoded instruction signal (IF/ELSE/ENDIF), and a single\
    \ bit valid condition code. Although there are many possible conditions from many\
    \ different instructions, these can be decoded into a single\n\n<span id=\"page-7-0\"\
    ></span>![](_page_7_Figure_1.jpeg)\n\nFigure 4: eGPU Placement\n\nvalid condition\
    \ bit in the main SP body. These narrow busses give us flexibility to wrap multiple\
    \ pipes around the relatively simple (consisting largely of a chained registers\
    \ organized in individual stacks) predicate blocks, which makes it possible for\
    \ the tool to place them almost completely independently of the main datapaths.\n\
    \nTo create repeatable high performance designs, we need to understand both the\
    \ structure, and the position of embedded features to each other. Here we are\
    \ using integer ALUs which range in size by four times, our logic and memory density\
    \ is very high, but our performance always exceeds that of the slowest embedded\
    \ feature. It is possible to build a completely different type of CPU (or indeed\
    \ any other type of core) and achieve this type of performance via a push button\
    \ flow, but the architecture of the FPGA needs to be considered at every stage\
    \ of the IP architecture phase.\n\n## 7 BENCHMARKS\n\nWe ran a number of workloads\
    \ of different types to evaluate absolute and relative performance of the eGPU\
    \ for varying data sizes that we might expect for embedded applications. We also\
    \ profiled all the workloads to examine the efficiency of the eGPU. For continuity\
    \ we selected many of the same benchmarks as used by Flexgrip [\\[16\\]](#page-10-25).\
    \ We chose vector reduction, matrix transpose, and matrix-matrix multiply (MMM),\
    \ as these would be common building blocks for many GPGPU applications. Bitonic\
    \ sort [\\[19\\]](#page-10-26) is a sorting algorithm suited for parallel processing.\
    \ Instead of the simpler autocorrelation, we used the FFT, as we felt this would\
    \ be more representative of the\n\n<span id=\"page-7-1\"></span>![](_page_7_Figure_7.jpeg)\n\
    \nFigure 5: Single SP Placement\n\nworkloads expected for the eGPU. All benchmarks\
    \ were written in assembly code (we have not written our compiler yet).\n\nWe\
    \ report the comparison to FlexGrip only for the MMM, as the larger dataset size\
    \ would be less affected by any overheads for setup and data transfer. We see\
    \ that there is a significant performance advantage in favor of eGPU in cycle\
    \ time alone. We ran all benchmarks, except the FFT, for which there are no reported\
    \ FlexGrip results. FlexGrip underperforms eGPU by a factor of \u224831x, averaged\
    \ over all benchmarks. We did not compare against DO-GPU (which is the latest\
    \ iteration of FGPU), as DO-GPU normalized size is 50x-100x greater than eGPU.\n\
    \nOur reported measurements are all based on core performance: we start the clock\
    \ once the data has been loaded into the shared memory, and stop the clock once\
    \ the final result has been written back to the shared memory. The most likely\
    \ use of the eGPU is to apply multiple algorithms to the same data - .. there\
    \ is no loading and unloading of data between different algorithms. For completeness,\
    \ we also ran all of our benchmarks taking into account the time to load and unload\
    \ the data over the 32-bit wide data bus. The performance impact was only 4.7%,\
    \ averaged over all benchmarks.\n\nClock frequency was 771 MHz for eGPU (including\
    \ where the Dot Product operator is used), and 600 MHz for eGPU-QP variant. We\
    \ compare both cycle counts and elapsed time for the eGPUs with the two shared\
    \ memory architectures, and also the impact of the optional Dot Product core for\
    \ reduction and MMM benchmarks. We then normalize the performance (time), by the\
    \ resource cost, which we calculated on the basis of ALMs and DSP Blocks. We estimate\
    \ that the effective cost of a DSP block is 100 ALMs, which we calculate as follows:\
    \ we start with the ALM count of the pure soft logic implementation of a FP32\
    \ multiply and adder (approximately 650 ALMs [\\[10\\]](#page-10-27)), and add\
    \ 50% area to this number for DSP Block overhead (a DSP Block contains considerable\
    \ additional features). We then divide by 10 for an approximate soft logic to\
    \ hard logic\n\n<span id=\"page-8-1\"></span><span id=\"page-8-0\"></span>\n\n\
    |                                         |                                  \
    \           | Vector Reduction               |                |              \
    \    | Matrix Transpose   |                      |                      | Matrix\
    \ x Matrix          |                             |                          \
    \             |              |                            |                  \
    \                           |\n|-----------------------------------------|---------------------------------------------|--------------------------------|----------------|------------------|--------------------|----------------------|----------------------|--------------------------|-----------------------------|---------------------------------------|--------------|----------------------------|---------------------------------------------|\n\
    | Dimension                               | Metric                           \
    \           | Nios                           | eGPU           | eGPU         \
    \    | eGPU               | Nios                 | eGPU                 | eGPU\
    \                     | Nios                        | FlexGrip               \
    \               | eGPU         | eGPU                       | eGPU           \
    \                             |\n|                                         | \
    \                                            |                               \
    \ | DP             | QP               | Dot                |                 \
    \     | DP                   | QP                       |                    \
    \         |                                       | DP           | QP        \
    \                 | Dot                                         |\n|         \
    \                                | Cycles                                    \
    \  | 459                            | 168            | 160              | 62 \
    \                | 21809                | 1720                 | 1208        \
    \             | 1.45M                       | 2.14M                          \
    \       | 111546       | 103354                     | 19800                  \
    \                     |\n|                                         | Time(us)\
    \                                    | 1.32                           | 0.22 \
    \          | 0.27             | 0.08               | 62.85                | 2.23\
    \                 | 2.01                     | 4179                        | 21400\
    \                                 | 144.7        | 172.3                     \
    \ | 25.7                                        |\n| 32                      \
    \                | Ratio(cycles)                               | 2.73        \
    \                   | 1.0            | 0.95             | 0.37               |\
    \ 12.68                | 1.0                  | 0.7                      | 13.03\
    \                       | 19.2                                  | 1.0        \
    \  | 0.93                       | 0.18                                       \
    \ |\n|                                         | Ratio(time)                 \
    \                | 6.01                           | 1.0            | 1.23    \
    \         | 0.37               | 28.18                | 1.0                  |\
    \ 0.9                      | 28.97                       | 147.9             \
    \                    | 1.0          | 1.19                       | 0.18      \
    \                                  |\n|                                      \
    \   | Normalized                                  | 1.14                     \
    \      | 1.0            | 1.4              | 0.45               | 5.33       \
    \          | 1.0                  | 1.02                     | 5.48          \
    \              | -                                     | 1.0          | 1.35 \
    \                      | 0.21                                        |\n|    \
    \                                     | Cycles                               \
    \       | 1803                           | 202            | 194              |\
    \ 94                 | 86609                | 5529                 | 3481    \
    \                 | 11.6M                       | 16.6M                      \
    \           | 451066       | 418671                     | 84425              \
    \                         |\n|                                         | Time(us)\
    \                                    | 5.20                           | 0.26 \
    \          | 0.32             | 0.12               | 249.6                | 7.17\
    \                 | 5.80                     | 33383                       | 166000\
    \                                | 585.0        | 697.8                      |\
    \ 109.5                                       |\n| 64                        \
    \              | Ratio(cycles)                               | 8.93          \
    \                 | 1.0            | 0.96             | 0.47               | 15.66\
    \                | 1.0                  | 0.63                     | 25.7    \
    \                    | 36.8                                  | 1.0          |\
    \ 0.93                       | 0.19                                        |\n\
    |                                         | Ratio(time)                      \
    \           | 19.98                          | 1.0            | 1.23         \
    \    | 0.47               | 34.81                | 1.0                  | 0.81\
    \                     | 57.1                        | 284                    \
    \               | 1.0          | 1.19                       | 0.19           \
    \                             |\n|                                         | Normalized\
    \                                  | 3.78                           | 1.0    \
    \        | 1.4              | 0.60               | 6.59                 | 1.0\
    \                  | 0.92                     | 10.80                       |\
    \ -                                     | 1.0          | 1.35                \
    \       | 0.23                                        |\n|                   \
    \                      | Cycles                                      | 3595  \
    \                         | 216            | 208              | 101          \
    \      | 345233               | 20481                | 12649                 \
    \   | 92.5M                       | 441.2M                                | 2342356\
    \      | 2212136                    | 886452                                 \
    \     |\n|                                         | Time(us)                \
    \                    | 10.36                          | 0.28           | 0.35\
    \             | 0.13               | 994.91               | 26.56            \
    \    | 21.08                    | 266491                      | 4412.1       \
    \                         | 3038.1       | 3686.9                     | 1149.7\
    \                                      |\n| 128                              \
    \       | Ratio(cycles)                               | 16.64                \
    \          | 1.0            | 0.96             | 0.47               | 16.86  \
    \              | 1.0                  | 0.62                     | 39.47     \
    \                  | 188.3                                 | 1.0          | 0.94\
    \                       | 0.38                                        |\n|   \
    \                                      | Ratio(time)                         \
    \        | 37.00                          | 1.0            | 1.23            \
    \ | 0.47               | 37.45                | 1.0                  | 0.79  \
    \                   | 87.71                       | 1452                     \
    \             | 1.0          | 1.21                       | 0.38             \
    \                           |\n|                                         | Normalized\
    \                                  | 7.00                           | 1.0    \
    \        | 1.4              | 0.60               | 7.09                 | 1.0\
    \                  | 0.90                     | 1659                        |\
    \ -                                     | 1.0          | 1.37                \
    \       | 0.46                                        |\n|                   \
    \                      |                                             |       \
    \                         |                |                  |              \
    \      |                      |                      |                       \
    \   |                             |                                       |  \
    \            |                            |                                  \
    \           |\n| Bitonic 64<br>Bitonic 32<br>Bitonic 128 | Bitonic 256<br>Bitonic\
    \ 32QP<br>Bitonic 64QP | Bitonic 128QP<br>Bitonic 256QP | FFT32<br>FFT64 | FFT128<br>FFT256\
    \ | FFT32QP<br>FFT64QP | FFT128QP<br>FFT256QP | Reduce32<br>Reduce64 | Reduce32Dot<br>Reduce128\
    \ | Reduce64Dot<br>Reduce128Dot | MMM 64x64<br>MMM 32x32<br>MMM 128x128 | MMM32x32QP\
    \   | MMM64x64QP<br>MMM128x128QP | MMM32x32Dot<br>MMM64x64Dot<br>MMM128x128Dot\
    \ |\n| FP OP                                   | INT OP                      \
    \                |                                | IMM OP         |         \
    \         | Branch             | Load                 |                      |\
    \ Save                     | Predicate                   |                   \
    \                    | Thread Setup |                            | NOP       \
    \                                  |\n\nTable 7: Vector and Matrix Benchmarks\n\
    \nFigure 6: Benchmark Profiling (Y-Axis shows proportion of instructions executed\
    \ by type).\n\nscaling factor (earlier work [\\[26\\]](#page-10-28) suggested\
    \ a higher ratio in the general case, but recent work [\\[27\\]](#page-10-29)\
    \ described more efficient ways of mapping arithmetic, especially multipliers,\
    \ to FPGAs). We report normalized cost (considering both elapsed time and resources,\
    \ with eGPU-DP as the baseline).\n\nAs a comparison, we ran all of the benchmarks\
    \ on Nios IIe [\\[1\\]](#page-10-30), which is a mature RISC processor for Intel\
    \ FPGAs. The configuration we used consumed 1100 ALMs (plus 3 DSP Blocks, giving\
    \ a normalized cost of 1400), and closed timing at 347 MHz. We did not profile\
    \ the Nios code, but analyzed the efficiency of operation (CPI). Most of the benchmarks\
    \ retired an instruction every 1.7 clock cycles, except for the matrix-matrix\
    \ multiplies and FFT, which required about 3 clocks, because of the way that 32\xD7\
    32 multipliers\n\nwere implemented. (For simplicity, we replaced the FP32 arithmetic\
    \ with INT32 for the Nios examples).\n\nFor the vector and matrix benchmarks,\
    \ we chose an eGPU configuration with 32 registers per thread, with a 32 bit ALU,\
    \ and a 128KB shared memory. This configuration has an equivalent cost (see Table\
    \ [4](#page-6-0) and [5\\)](#page-6-1) of 7400, 8400, and 9000 ALMs for the eGPU-DP,\
    \ eGPU-QP, and eGPU-Dot variants respectively. Depending on the configuration,\
    \ eGPU is 5\xD7 to 6\xD7 larger than Nios (but also more than twice the operating\
    \ frequency). We would therefore expect (or at least hope) that eGPU would give\
    \ an OOM performance increase over Nios.\n\nWe can deduce the mechanism of the\
    \ matrix transpose benchmarks from Table [7](#page-8-0) directly. For a given\
    \ \xD7 matrix, we know that the eGPU will need 2 cycles to write the transposed\
    \ elements\n\nto shared memory and 1/4th of those cycles to initially read them\
    \ into the SP threads. We can see that the number of cycles clocked is marginally\
    \ larger than this; these are largely used for the integer instructions needed\
    \ to generate the transposed write addresses. We expect that the eGPU-QP will\
    \ require about 40% fewer cycles, being able to write two transposed elements\
    \ per clock, which indeed is the case.\n\nThe vector reduction needs inter-SP\
    \ communication, which go through the shared memory, which is the performance\
    \ bottleneck in the eGPU. Table [7](#page-8-0) shows the impact of memory accesses\
    \ on reduction performance. The actual floating point operations are a relatively\
    \ small (\u224810%) component of the reduction, with the majority of the cycles\
    \ used by the memory operations. If we are using the dot product operator, there\
    \ are even fewer FP operations required, and most of the time is spent waiting\
    \ (NOPs) for the dot product to write back to the SP. All final vector reductions\
    \ end up in the first SP, and we can use the multi-threaded CPU or MCU eGPU dynamic\
    \ scaling personalities to write these values to the shared memory.\n\nThe MMMs\
    \ are much more complex. Although the algorithm itself is very simple, consisting\
    \ only of a three level loop, the standard GPU implementation requires a vector\
    \ reduction. While the cycle count increases as expected (\u223C 4\xD7) from 32\xD7\
    32 to 64\xD764, there is an unexpected jump from 64\xD764 to 128\xD7128, which\
    \ is particularly evident in the eGPU-Dot case. Analysis of the code shows that\
    \ while we are able to store the entire matrix (or at least a majority of the\
    \ matrix) in the SP registers (there are 16384 total registers across the 16 SPs\
    \ in the configuration we have chosen here) for the 32\xD732 and 64\xD764 cases,\
    \ we need to keep reloading portions of the matrix in the 128\xD7128 case, which\
    \ can also be seen in the profile stack in Figure [6.](#page-8-1) Of course, there\
    \ is always the option of increasing the maximum thread space or registers per\
    \ thread (through parameterization) if the expected workloads were larger matrices.\
    \ Compared to the vector reduction (where we profile a single vector), the thread\
    \ initialization and integer operations are amortized away as we operate on many\
    \ vectors. The NOPs also disappear as the the thread depth increases here.\n\n\
    The bitonic sort benchmark requires a wider mix of instructions. Predicates are\
    \ required, which increases the effective cost of the eGPU core by about 50%.\
    \ The smaller sorts require many NOPs, which progressively reduce as the number\
    \ of wavefronts increase for the larger datasets. The nature of the bitonic sort\
    \ tends to use many subroutine calls, which we can see here in the relatively\
    \ large number of branch operations. Again, the memory operations take the majority\
    \ of all cycles, as each pass of the sort requires a redistribution of the data\
    \ among the SPs. While the eGPU-QP version requires fewer clock cycles because\
    \ of the increased write bandwidth, the normalized cost of the QP version is higher,\
    \ largely because of the lower clock frequency.\n\nA similar pattern of instruction\
    \ distribution is seen in the FFT. Increasing wavefront depth for larger datasets\
    \ reduces NOPs significantly. The number of FP instructions (which are doing the\
    \ actual FFT calculations) is relatively small, at about 10%. The largest proportion\
    \ of operations are once again the memory accesses, especially in the write to\
    \ shared memory; using the QP version of the eGPU results in a 20% to 30% decrease\
    \ in total cycles. The normalized cost of the two eGPU versions, however, is approximately\
    \ the\n\nTable 8: Bitonic Sort and FFT Benchmarks\n\n|     |               | \
    \       | Bitonic Sort |       | FFT    |      |            |  |  |\n|-----|---------------|--------|--------------|-------|--------|------|------------|--|--|\n\
    | Dim | Metric        |        | eGPU         | eGPU  |        | eGPU | eGPU<br>QP\
    \ |  |  |\n|     |               | Nios   | DP           | QP    | Nios   | DP\
    \   |            |  |  |\n|     | Cycles        | 8457   | 1742         | 1543\
    \  | 9165   | 876  | 714        |  |  |\n|     | Time(us)      | 24.37  | 2.25\
    \         | 2.51  | 26.41  | 1.14 | 1.19       |  |  |\n| 32  | Ratio(cycles)\
    \ | 4.89   | 1.0          | 0.86  | 10.46  | 1.0  | 0.82       |  |  |\n|    \
    \ | Ratio(time)   | 10.8   | 1.0          | 1.1   | 23.16  | 1.0  | 1.04     \
    \  |  |  |\n|     | Normalized    | 1.24   | 1.0          | 1.24  | 4.38   | 1.0\
    \  | 1.18       |  |  |\n|     | Cycles        | 20687  | 3728         | 3054\
    \  | 20848  | 1695 | 1312       |  |  |\n|     | Time(us)      | 59.6   | 4.83\
    \         | 5.09  | 60.08  | 2.20 | 2.19       |  |  |\n| 64  | Ratio(cycles)\
    \ | 5.54   | 1.0          | 0.82  | 12.30  | 1.0  | 0.82       |  |  |\n|    \
    \ | Ratio(time)   | 12.3   | 1.0          | 1.05  | 27.31  | 1.0  | 1.01     \
    \  |  |  |\n|     | Normalized    | 1.42   | 1.0          | 1.18  | 5.17   | 1.0\
    \  | 1.13       |  |  |\n|     | Cycles        | 49741  | 8326         | 6536\
    \  | 46667  | 3463 | 2558       |  |  |\n|     | Time(us)      | 143.3  | 10.8\
    \         | 10.9  | 134.49 | 4.29 | 4.26       |  |  |\n| 128 | Ratio(cycles)\
    \ | 5.97   | 1.0          | 0.79  | 13.48  | 1.0  | 0.74       |  |  |\n|    \
    \ | Ratio(time)   | 13.2   | 1.0          | 1.01  | 31.35  | 1.0  | 0.95     \
    \  |  |  |\n|     | Normalized    | 1.48   | 1.0          | 1.13  | 5.93   | 1.0\
    \  | 1.08       |  |  |\n|     | Cycles        | 149271 | 16578        | 11974\
    \ | 103636 | 6813 | 4736       |  |  |\n|     | Time(us)      | 430.2  | 21.5\
    \         | 19.9  | 298.66 | 8.84 | 7.89       |  |  |\n| 256 | Ratio(cycles)\
    \ | 9.0    | 1.0          | 0.72  | 15.21  | 1.0  | 0.70       |  |  |\n|    \
    \ | Ratio(time)   | 20.0   | 1.0          | 0.93  | 33.79  | 1.0  | 0.89     \
    \  |  |  |\n|     | Normalized    | 2.24   | 1.0          | 1.05  | 6.39   | 1.0\
    \  | 1.01       |  |  |\n\nsame, with the high clock frequency of the base version\
    \ offsetting the higher memory bandwidth of the QP version. These results also\
    \ point to a better optimization for the FFT: by using a higher radix FFT, there\
    \ will be correspondingly fewer passes through the shared memory. (We have a extensive\
    \ flexibility in specifying the register and thread parameters, we can easily\
    \ support much higher radices, which will require much larger register spaces).\n\
    \nComparing against Nios, we can see that the eGPU performs very well. We see\
    \ at least an OOM performance difference based on time, and in almost all cases\
    \ on a cycle basis as well. This tells us that eGPU is a more efficient architecture\
    \ than a RISC processor, and is a viable candidate for a soft accelerator core.\n\
    \n## 8 CONCLUSIONS\n\nWe have demonstrated a GPGPU that consistently beats 770\
    \ MHz for a wide range of parameters, and described the design approach required\
    \ to reach such frequencies. We are able to swap in and out features as well as\
    \ change the precision of the integer ALU to optimize for area and resource balancing\
    \ in the FPGA.\n\nFor the eGPU to be useful in an actual system design, it must\
    \ offer an improvement over known methods. We compare the eGPU to a mature commercial\
    \ soft CPU (Nios) over a number of benchmarks. The eGPU is much better on a cycle\
    \ by cycle or elasped time basis in all cases we tried (typically by one to two\
    \ OOM), and is still better on an area normalized basis. When we add the dot product\
    \ core which can be used directly by the eGPU in a regular GPGPU context - the\
    \ advantage can increase again by several times. A soft GPU therefore can offer\
    \ a valid implementation option for many types of algorithms. This does not mean\
    \ that a GPGPU will replace the RISC, anymore than a discrete GPGPU will replace\
    \ a discrete RISC, only that we have shown that the soft GPGPU can now be considered\
    \ for commercial designs, rather than just being of academic interest. The eGPU\
    \ only uses 1%-2% of a current mid-range device, making it a cost effective option\
    \ to implement complex algorithms in a larger FPGA system design, even if multiple\
    \ cores are required.\n\nA Statically and Dynamically Scalable Soft GPGPU\n\n\
    ## REFERENCES\n\n- <span id=\"page-10-30\"></span>[1] 2016. Nios II Classic Processor\
    \ Reference GuideNios II Classic Processor Reference Guide. [https://www.intel.com/content/www/us/en/docs/programmable/683620/](https://www.intel.com/content/www/us/en/docs/programmable/683620/current/overview-67435.html)\
    \ [current/overview-67435.html.](https://www.intel.com/content/www/us/en/docs/programmable/683620/current/overview-67435.html)\n\
    - <span id=\"page-10-0\"></span>[2] 2017. FFT IP Core: User Guide. [https://www.intel.co.uk/content/www/uk/en/](https://www.intel.co.uk/content/www/uk/en/products/details/fpga/intellectual-property/dsp/fft.html)\
    \ [products/details/fpga/intellectual-property/dsp/fft.html.](https://www.intel.co.uk/content/www/uk/en/products/details/fpga/intellectual-property/dsp/fft.html)\n\
    - <span id=\"page-10-2\"></span>[3] 2017. High-speed Reed-Solomon IP Core User\
    \ Guide. [https://www.intel.com/](https://www.intel.com/content/www/us/en/docs/programmable/683120/17-1/about-the-high-speed-reed-solomon-ip-core.html)\
    \ [content/www/us/en/docs/programmable/683120/17-1/about-the-high-speed](https://www.intel.com/content/www/us/en/docs/programmable/683120/17-1/about-the-high-speed-reed-solomon-ip-core.html)[reed-solomon-ip-core.html.](https://www.intel.com/content/www/us/en/docs/programmable/683120/17-1/about-the-high-speed-reed-solomon-ip-core.html)\n\
    - <span id=\"page-10-4\"></span>[4] 2018. Microblaze Processor Reference Guide.\
    \ [https://docs.xilinx.com/v/u/2018.2-](https://docs.xilinx.com/v/u/2018.2-English/ug984-vivado-microblaze-ref)\
    \ [English/ug984-vivado-microblaze-ref.](https://docs.xilinx.com/v/u/2018.2-English/ug984-vivado-microblaze-ref)\n\
    - <span id=\"page-10-15\"></span>[5] 2020. HB0919 Handbook CoreVectorBlox. [https://www.microsemi.com/existing](https://www.microsemi.com/existing-parts/parts/152678)[parts/parts/152678.](https://www.microsemi.com/existing-parts/parts/152678)\n\
    - <span id=\"page-10-19\"></span>[6] 2021. Block-by-Block Configurable Fast Fourier\
    \ Transform Implementation on AI Engine (XAPP1356). [https://docs.xilinx.com/r/en-US/xapp1356-fft-ai-engine/FFT](https://docs.xilinx.com/r/en-US/xapp1356-fft-ai-engine/FFT-on-Multiple-AI-Engines)[on-Multiple-AI-Engines.](https://docs.xilinx.com/r/en-US/xapp1356-fft-ai-engine/FFT-on-Multiple-AI-Engines)\n\
    - <span id=\"page-10-23\"></span>[7] 2021. Intel Agilex7 FPGAs and SoCs F-Series:\
    \ Product Table. [https://www.intel.](https://www.intel.com/content/dam/www/programmable/us/en/pdfs/literature/pt/intel-agilex-f-series-product-table.pdf)\
    \ [com/content/dam/www/programmable/us/en/pdfs/literature/pt/intel-agilex-f](https://www.intel.com/content/dam/www/programmable/us/en/pdfs/literature/pt/intel-agilex-f-series-product-table.pdf)[series-product-table.pdf.](https://www.intel.com/content/dam/www/programmable/us/en/pdfs/literature/pt/intel-agilex-f-series-product-table.pdf)\n\
    - <span id=\"page-10-1\"></span>[8] 2022. Fast Fourier Transform v9.1. [https://www.xilinx.com/content/dam/xilinx/](https://www.xilinx.com/content/dam/xilinx/support/documents/ip_documentation/xfft/v9_1/pg109-xfft.pdf)\
    \ [support/documents/ip\\\\_documentation/xfft/v9\\\\_1/pg109-xfft.pdf.](https://www.xilinx.com/content/dam/xilinx/support/documents/ip_documentation/xfft/v9_1/pg109-xfft.pdf)\n\
    - <span id=\"page-10-3\"></span>[9] 2022. Nios V Processor Reference Manual. [https://www.intel.com/content/www/](https://www.intel.com/content/www/us/en/products/details/fpga/nios-processor/v.html)\
    \ [us/en/products/details/fpga/nios-processor/v.html.](https://www.intel.com/content/www/us/en/products/details/fpga/nios-processor/v.html)\n\
    - <span id=\"page-10-27\"></span>[10] 2023. Floating-Point IP Cores User Guide.\
    \ [https://www.intel.com/content/www/](https://www.intel.com/content/www/us/en/docs/programmable/683750/23-1/about-floating-point-ip-cores.html)\
    \ [us/en/docs/programmable/683750/23-1/about-floating-point-ip-cores.html.](https://www.intel.com/content/www/us/en/docs/programmable/683750/23-1/about-floating-point-ip-cores.html)\n\
    - <span id=\"page-10-24\"></span>[11] 2023. Intel Agilex 7 Variable Precision\
    \ DSP Blocks. [https://www.intel.com/](https://www.intel.com/content/www/us/en/docs/programmable/683037/23-3/variable-precision-dsp-blocks-overview.html)\
    \ [content/www/us/en/docs/programmable/683037/23-3/variable-precision-dsp](https://www.intel.com/content/www/us/en/docs/programmable/683037/23-3/variable-precision-dsp-blocks-overview.html)[blocks-overview.html.](https://www.intel.com/content/www/us/en/docs/programmable/683037/23-3/variable-precision-dsp-blocks-overview.html)\n\
    - <span id=\"page-10-22\"></span>[12] 2023. Intel Agilex7 Embedded Memory User\
    \ Guide. [https://www.intel.com/](https://www.intel.com/content/www/us/en/docs/programmable/683241/23-2/embedded-memory-overview.html)\
    \ [content/www/us/en/docs/programmable/683241/23-2/embedded-memory](https://www.intel.com/content/www/us/en/docs/programmable/683241/23-2/embedded-memory-overview.html)[overview.html.](https://www.intel.com/content/www/us/en/docs/programmable/683241/23-2/embedded-memory-overview.html)\n\
    - <span id=\"page-10-18\"></span>[13] 2023. Versal Adaptive SoC AI Engine Architecture\
    \ Manual (AM009). [https:](https://docs.xilinx.com/v/u/en-US/wp506-ai-engine)\
    \ [//docs.xilinx.com/v/u/en-US/wp506-ai-engine.](https://docs.xilinx.com/v/u/en-US/wp506-ai-engine)\n\
    - <span id=\"page-10-5\"></span>[14] Abdullah Al-Dujaili, Florian Deragisch, Andrei\
    \ Hagiescu, and Weng-Fai Wong. 2012. Guppy: A GPU-like soft-core processor. In\
    \ 2012 International Conference on Field-Programmable Technology. 57\u201360.<https://doi.org/10.1109/FPT.2012.6412112>\n\
    - <span id=\"page-10-6\"></span>[15] Muhammed Al Kadi, Benedikt Janssen, and Michael\
    \ Huebner. 2016. FGPU: An SIMT-Architecture for FPGAs. In Proceedings of the 2016\
    \ ACM/SIGDA International Symposium on Field-Programmable Gate Arrays (Monterey,\
    \ California, USA) (FPGA '16). Association for Computing Machinery, New York,\
    \ NY, USA, 254\u2013263.<https://doi.org/10.1145/2847263.2847273>\n- <span id=\"\
    page-10-25\"></span>[16] Kevin Andryc. 2018. An Architecture Evaluation and Implementaiton\
    \ of a Soft GPGPU for FPGAs. (2018).<https://doi.org/10.7275/12722172>\n- <span\
    \ id=\"page-10-7\"></span>[17] Kevin Andryc, Murtaza Merchant, and Russell Tessier.\
    \ 2013. FlexGrip: A soft GPGPU for FPGAs. In 2013 International Conference on\
    \ Field-Programmable Technology (FPT). 230\u2013237.<https://doi.org/10.1109/FPT.2013.6718358>\n\
    - <span id=\"page-10-8\"></span>[18] Raghuraman Balasubramanian, Vinay Gangadhar,\
    \ Ziliang Guo, Chen-Han Ho, Cherin Joseph, Jaikrishnan Menon, Mario Paulo Drumond,\
    \ Robin Paul, Sharath Prasad, Pradip Valathol, and Karthikeyan Sankaralingam.\
    \ 2015. Enabling GPGPU Low-Level Hardware Explorations with MIAOW: An Open-Source\
    \ RTL Implementation of a GPGPU. ACM Trans. Archit. Code Optim. 12, 2, Article\
    \ 21 (jun 2015), 25 pages.<https://doi.org/10.1145/2764908>\n- <span id=\"page-10-26\"\
    ></span>[19] K. E. Batcher. 1968. Sorting Networks and Their Applications. In\
    \ Proceedings of the April 30\u2013May 2, 1968, Spring Joint Computer Conference\
    \ (Atlantic City, New Jersey) (AFIPS '68 (Spring)). Association for Computing\
    \ Machinery, New York,\n\nNY, USA, 307\u2013314.<https://doi.org/10.1145/1468075.1468121>\n\
    \n- <span id=\"page-10-16\"></span>[20] Hui Yan Cheah, Fredrik Brosser, Suhaib\
    \ A. Fahmy, and Douglas L. Maskell. 2014. The IDEA DSP Block-Based Soft Processor\
    \ for FPGAs. ACM Trans. Reconfigurable Technol. Syst. 7, 3, Article 19 (sep 2014),\
    \ 23 pages.<https://doi.org/10.1145/2629443>\n- <span id=\"page-10-17\"></span>[21]\
    \ Hui Yan Cheah, Suhaib A. Fahmy, and Nachiket Kapre. 2014. Analysis and optimization\
    \ of a deeply pipelined FPGA soft processor. In 2014 International Conference\
    \ on Field-Programmable Technology (FPT). 235\u2013238. [https://doi.org/10.](https://doi.org/10.1109/FPT.2014.7082783)\
    \ [1109/FPT.2014.7082783](https://doi.org/10.1109/FPT.2014.7082783)\n- <span id=\"\
    page-10-12\"></span>[22] Christopher Han-Yu Chou, Aaron Severance, Alex D. Brant,\
    \ Zhiduo Liu, Saurabh Sant, and Guy G. Lemieux. 2011. VEGAS: soft vector processor\
    \ with scratchpad memory. In Proceedings of the ACM/SIGDA 19th International Symposium\
    \ on Field Programmable Gate Arrays, FPGA 2011, Monterey, California, USA, February\
    \ 27, March 1, 2011, John Wawrzynek and Katherine Compton (Eds.). ACM, 15\u2013\
    24. <https://doi.org/10.1145/1950413.1950420>\n- <span id=\"page-10-21\"></span>[23]\
    \ Jeffrey Chromczak, Mark Wheeler, Charles Chiasson, Dana How, Martin Langhammer,\
    \ Tim Vanderhoek, Grace Zgheib, and Ilya Ganusov. 2020. Architectural Enhancements\
    \ in Intel\xAE Agilex\u2122 FPGAs. In FPGA '20: The 2020 ACM/SIGDA International\
    \ Symposium on Field-Programmable Gate Arrays, Seaside, CA, USA, February 23-25,\
    \ 2020, Stephen Neuendorffer and Lesley Shannon (Eds.). ACM, 140\u2013149.<https://doi.org/10.1145/3373087.3375308>\n\
    - <span id=\"page-10-9\"></span>[24] Pedro Duarte, Pedro Tomas, and Gabriel Falcao.\
    \ 2017. SCRATCH: An End-to-End Application-Aware Soft-GPGPU Architecture and Trimming\
    \ Tool. In Proceedings of the 50th Annual IEEE/ACM International Symposium on\
    \ Microarchitecture (Cambridge, Massachusetts) (MICRO-50 '17). Association for\
    \ Computing Machinery, New York, NY, USA, 165\u2013177.<https://doi.org/10.1145/3123939.3123953>\n\
    - <span id=\"page-10-10\"></span>[25] Jeffrey Kingyens and J. Gregory Steffan.\
    \ 2010. A GPU-inspired soft processor for high-throughput acceleration. In 2010\
    \ IEEE International Symposium on Parallel and Distributed Processing, Workshops\
    \ and Phd Forum (IPDPSW). 1\u20138. [https:](https://doi.org/10.1109/IPDPSW.2010.5470679)\
    \ [//doi.org/10.1109/IPDPSW.2010.5470679](https://doi.org/10.1109/IPDPSW.2010.5470679)\n\
    - <span id=\"page-10-28\"></span>[26] Ian Kuon and Jonathan Rose. 2006. Measuring\
    \ the gap between FPGAs and ASICs. In Proceedings of the ACM/SIGDA 14th International\
    \ Symposium on Field Programmable Gate Arrays, FPGA 2006, Monterey, California,\
    \ USA, February 22- 24, 2006, Steven J. E. Wilton and Andr\xE9 DeHon (Eds.). ACM,\
    \ 21\u201330. [https:](https://doi.org/10.1145/1117201.1117205) [//doi.org/10.1145/1117201.1117205](https://doi.org/10.1145/1117201.1117205)\n\
    - <span id=\"page-10-29\"></span>[27] Martin Langhammer and Gregg Baeckler. 2018.\
    \ High Density and Performance Multiplication for FPGA. In 25th IEEE Symposium\
    \ on Computer Arithmetic, ARITH 2018, Amherst, MA, USA, June 25-27, 2018. IEEE,\
    \ 5\u201312. [https://doi.org/10.1109/](https://doi.org/10.1109/ARITH.2018.8464695)\
    \ [ARITH.2018.8464695](https://doi.org/10.1109/ARITH.2018.8464695)\n- <span id=\"\
    page-10-20\"></span>[28] Martin Langhammer and George A. Constantinides. 2023.\
    \ eGPU: A 750 MHz Class Soft GPGPU for FPGA. In 2023 33rd International Conference\
    \ on Field-Programmable Logic and Applications (FPL). 277\u2013282. [https://doi.org/10.1109/](https://doi.org/10.1109/FPL60245.2023.00047)\
    \ [FPL60245.2023.00047](https://doi.org/10.1109/FPL60245.2023.00047)\n- <span\
    \ id=\"page-10-11\"></span>[29] Rui Ma, Jia-Ching Hsu, Tian Tan, Eriko Nurvitadhi,\
    \ Rajesh Vivekanandham, Aravind Dasu, Martin Langhammer, and Derek Chiou. 2021.\
    \ DO-GPU: Domain Optimizable Soft GPUs. In 2021 31st International Conference\
    \ on Field-Programmable Logic and Applications (FPL). 140\u2013144. [https://doi.org/10.1109/FPL53798.2021.](https://doi.org/10.1109/FPL53798.2021.00031)\
    \ [00031](https://doi.org/10.1109/FPL53798.2021.00031)\n- <span id=\"page-10-13\"\
    ></span>[30] Aaron Severance and Guy Lemieux. 2012. VENICE: A compact vector processor\
    \ for FPGA applications. In 2012 International Conference on Field-Programmable\
    \ Technology, FPT 2012, Seoul, Korea (South), December 10-12, 2012. IEEE, 261\u2013\
    268. <https://doi.org/10.1109/FPT.2012.6412146>\n- <span id=\"page-10-14\"></span>[31]\
    \ Aaron Severance and Guy G. F. Lemieux. 2013. Embedded supercomputing in FPGAs\
    \ with the VectorBlox MXP Matrix Processor. In Proceedings of the International\
    \ Conference on Hardware/Software Codesign and System Synthesis, CODES+ISSS 2013,\
    \ Montreal, QC, Canada, September 29 - October 4, 2013. IEEE, 6:1\u20136:10.<https://doi.org/10.1109/CODES-ISSS.2013.6658993>"
  paper_id: 3
  scores:
    originality:
      raw: 4.5
      reason: The introduction of X-HEEP as a configurable and extendible platform
        for ultra-low-power edge accelerators presents a novel approach in the field,
        showcasing significant innovation.
    technical_soundness:
      raw: 4.0
      reason: The paper demonstrates solid technical foundations with practical implementations,
        although some details on the integration process could be elaborated for clarity.
    clarity:
      raw: 4.2
      reason: The writing is generally clear and well-structured, but certain technical
        aspects could benefit from additional explanations to enhance understanding.
    significance:
      raw: 4.5
      reason: The work addresses critical challenges in edge computing, making a strong
        contribution to the field with potential wide-ranging applications, particularly
        in healthcare.
    relevance:
      raw: 5.0
      reason: The topic is highly relevant to current trends in edge computing and
        low-power design, appealing to a broad audience in embedded systems and microcontroller
        development.
  total_score: 4.34
  tier: A
- title: "Energy-adaptive Buffering for Efficient, Responsive, and Persistent\n  Batteryless\
    \ Systems"
  abstract: 'The field of edge computing has witnessed remarkable growth owing to
    the

    increasing demand for real-time processing of data in applications. However,

    challenges persist due to limitations in performance and power consumption. To

    overcome these challenges, heterogeneous architectures have emerged that

    combine host processors with specialized accelerators tailored to specific

    applications, leading to improved performance and reduced power consumption.

    However, most of the existing platforms lack the necessary configurability and

    extendability options for integrating custom accelerators. To overcome these

    limitations, we introduce in this paper the eXtendible Heterogeneous

    Energy-Efficient Platform (X-HEEP). X-HEEP is an open-source platform designed

    to natively support the integration of ultra-low-power edge accelerators. It

    provides customization options to match specific application requirements by

    exploring various core types, bus topologies, addressing modes, memory sizes,

    and peripherals. Moreover, the platform prioritizes energy efficiency by

    implementing low-power strategies, such as clock-gating and power-gating. We

    demonstrate the real-world applicability of X-HEEP by providing an integration

    example tailored for healthcare applications that includes a coarse-grained

    reconfigurable array (CGRA) and in-memory computing (IMC) accelerators. The

    resulting design, called HEEPocrates, has been implemented both in field

    programmable gate array (FPGA) on the Xilinx Zynq-7020 chip and in silicon with

    TSMC 65nm low-power CMOS technology. We run a set of healthcare applications

    and measure their energy consumption to demonstrate the alignment of our chip

    with other state-of-the-art microcontrollers commonly adopted in this domain.

    Moreover, we present the energy benefits of 4.9x and 4.8x gained by exploiting

    the integrated CGRA and IMC accelerators compared to running on the host CPU.'
  url: http://arxiv.org/abs/2401.05548v2
  keywords: ''
  document: "# X-HEEP: An Open-Source, Configurable and Extendible RISC-V Microcontroller\
    \ for the Exploration of Ultra-Low-Power Edge Accelerators\n\n[SIMONE MACHETTI,](HTTPS://ORCID.ORG/0000-0002-2887-5031)\
    \ Embedded Systems Laboratory (ESL), EPFL, Switzerland [PASQUALE DAVIDE SCHIAVONE,](HTTPS://ORCID.ORG/0000-0003-2931-0435)\
    \ Embedded Systems Laboratory (ESL), EPFL, Switzerland [THOMAS CHRISTOPH M\xDC\
    LLER,](HTTPS://ORCID.ORG/0009-0004-2805-6310) Embedded Systems Laboratory (ESL),\
    \ EPFL, Switzerland [MIGUEL PE\xD3N-QUIR\xD3S,](HTTPS://ORCID.ORG/0000-0002-5760-090X)\
    \ EcoCloud, EPFL, Switzerland [DAVID ATIENZA,](HTTPS://ORCID.ORG/0000-0001-9536-4947)\
    \ Embedded Systems Laboratory (ESL), EPFL, Switzerland\n\nThe field of edge computing\
    \ has witnessed remarkable growth owing to the increasing demand for real-time\
    \ processing of data in applications. However, challenges persist due to limitations\
    \ in the performance and power efficiency of edge-computing devices. To overcome\
    \ these challenges, heterogeneous architectures have emerged that combine host\
    \ processors with specialized accelerators tailored to specific applications,\
    \ leading to improved performance and reduced power consumption. However, most\
    \ of the existing platforms lack configurability and extendability options, necessitating\
    \ extensive modifications of the register transfer level (RTL) code for integrating\
    \ custom accelerators.\n\nTo overcome these limitations, we introduce in this\
    \ paper the eXtendible Heterogeneous Energy-Efficient Platform (X-HEEP). X-HEEP\
    \ is an open-source platform designed to natively support the integration of ultra-low-power\
    \ edge accelerators. It provides customization options to match specific application\
    \ requirements by exploring various core types, bus topologies, and memory addressing\
    \ modes. It also enables a fine-grained configuration of memory banks to match\
    \ the constraints of the integrated accelerators. The platform prioritizes energy\
    \ efficiency by implementing low-power strategies, such as clock-gating and power-gating,\
    \ and integrating these with connected accelerators through dedicated power control\
    \ interfaces.\n\nWe demonstrate the real-world applicability of X-HEEP by providing\
    \ an integration example tailored for healthcare applications that includes a\
    \ coarse-grained reconfigurable array (CGRA) and in-memory computing (IMC) accelerators.\
    \ The resulting design, called HEEPocrates, has been implemented both in field\
    \ programmable gate arrays (FPGAs) on multiple Xilinx chips, for prototyping and\
    \ exploration, and in silicon with TSMC 65 nm low-power CMOS technology. The fabricated\
    \ chip can operate from 0.8 V to 1.2 V, achieving a maximum frequency of 170 MHz\
    \ and 470 MHz, respectively. Its power consumption ranges from 270 \xB5W at 32\
    \ kHz and 0.8 V, to 48 mW at 470 MHz and 1.2 V.\n\nWe run a set of healthcare\
    \ applications and measure their energy consumption to demonstrate the alignment\
    \ of our chip with other state-of-the-art microcontrollers commonly adopted in\
    \ this domain, showing that HEEPocrates provides a good trade-off between acquisition-dominated\
    \ and processing-dominated applications for energy efficiency. Moreover, we present\
    \ the energy benefits of 4.9 \xD7 and 4.8 \xD7 gained by exploiting the integrated\
    \ CGRA accelerator and IMC accelerator, respectively, compared to running on the\
    \ host CPU.\n\nAdditional Key Words and Phrases: Ultra-Low Power, Energy Efficiency,\
    \ Microcontroller, Accelerator, Field Programmable Gate Array (FPGA), Course-Grained\
    \ Reconfigurable Array (CGRA), In-Memory Computing (IMC), Tapeout, Silicon Validation.\n\
    \nAuthors' addresses: [Simone Machetti,](https://orcid.org/0000-0002-2887-5031)\
    \ Embedded Systems Laboratory (ESL), EPFL, Lausanne, Switzerland; [Pasquale Davide\
    \ Schiavone,](https://orcid.org/0000-0003-2931-0435) Embedded Systems Laboratory\
    \ (ESL), EPFL, Lausanne, Switzerland; [Thomas Christoph M\xFCller,](https://orcid.org/0009-0004-2805-6310)\
    \ Embedded Systems Laboratory (ESL), EPFL, Lausanne, Switzerland; [Miguel Pe\xF3\
    n-Quir\xF3s,](https://orcid.org/0000-0002-5760-090X) EcoCloud, EPFL, Lausanne,\
    \ Switzerland; [David Atienza,](https://orcid.org/0000-0001-9536-4947) Embedded\
    \ Systems Laboratory (ESL), EPFL, Lausanne, Switzerland.\n\n\xA9 2024 Association\
    \ for Computing Machinery.\n\nManuscript submitted to ACM\n\nPermission to make\
    \ digital or hard copies of all or part of this work for personal or classroom\
    \ use is granted without fee provided that copies are not made or distributed\
    \ for profit or commercial advantage and that copies bear this notice and the\
    \ full citation on the first page. Copyrights for components of this work owned\
    \ by others than ACM must be honored. Abstracting with credit is permitted. To\
    \ copy otherwise, or republish, to post on servers or to redistribute to lists,\
    \ requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.\n\
    \n#### ACM Reference Format:\n\nSimone Machetti, Pasquale Davide Schiavone, Thomas\
    \ Christoph M\xFCller, Miguel Pe\xF3n-Quir\xF3s, and David Atienza. 2024. X-HEEP:\
    \ An Open-Source, Configurable and Extendible RISC-V Microcontroller for the Exploration\
    \ of Ultra-Low-Power Edge Accelerators. 1, 1 (March 2024), [21](#page-20-0) pages.\
    \ <https://doi.org/10.1145/nnnnnnn.nnnnnnn>\n\n#### 1 INTRODUCTION\n\nIn recent\
    \ years, the field of edge computing has witnessed remarkable growth and adoption\
    \ in commercial products. This process has been driven by the increasing demand\
    \ for real-time computing solutions, particularly Artificial Intelligence (AI)\
    \ and Machine Learning (ML) algorithms. As data processing at the edge for new\
    \ edge AI computing has become more prevalent, the performance and power consumption\
    \ limitations of edge-computing devices have become increasingly apparent, which\
    \ has posed significant challenges for researchers and engineers.\n\nHeterogeneous\
    \ architectures have emerged to overcome these challenges. These architectures\
    \ offer a promising path toward high energy efficiency while maintaining performance\
    \ constraints. Heterogeneous architectures rely on a combination of ultra-low-power\
    \ host processors to run control and communication tasks, and custom accelerators\
    \ tailored to specific application domains, such as artificial intelligence, image\
    \ processing, healthcare, and cryptography, to run computationally demanding tasks.\n\
    \nControl and communication tasks include accessing external memories, acquiring\
    \ data from analog-to-digital converters (ADCs) or sensors, preparing data for\
    \ computations, and running real-time operating system (RTOS) functions. Meanwhile,\
    \ computational demanding tasks focus on data processing, for example, convolutional\
    \ and fully connected layers in neural networks (NNs), fast-Fourier transforms\
    \ (FFTs) in temporal series, secure-hash algorithms (SHA) in cryptography, etc.\n\
    \nEach accelerator comes with unique requirements, such as memory size, area,\
    \ performance, and power, to meet the constraints of the target applications.\
    \ For this reason, proper customization of host platforms is imperative. This\
    \ may include exploring different CPUs to trade performance and power, bus topologies\
    \ and memory hierarchy, memory sizes to accommodate the required computational\
    \ data, peripherals to provide the necessary I/O connectivity, power domains and\
    \ strategies, etc. However, commercial platforms limit hardware exploration due\
    \ to their non-open-source nature. They often involve costly licensing models\
    \ and do not allow for customization. As a result, there is a growing preference\
    \ for open-source platforms as a more attractive solution that does not limit\
    \ exploration and customization, and that gives designers digital sovereignty\
    \ and control over IPs.\n\nToday, there are an increasing number of open-source\
    \ projects related to heterogeneous systems, thanks to the open RISC-V instruction\
    \ set architecture (ISA) revolution. However, many of such platforms focus only\
    \ on the CPU part, whereas microcontroller-based state-of-the-art projects lack\
    \ the flexibility and customization options needed to fulfill accelerator requirements\
    \ natively. These limitations include restricted configurability for the internal\
    \ platform's components (core, memory, bus, etc.) to adapt to the application\
    \ needs, limited support for external accelerator connectivity to communicate\
    \ with the host system, and inadequate built-in power management strategies to\
    \ optimize energy efficiency. Thus, hardware developers need to extensively modify\
    \ the platform to properly align with the target applications on their own copy\
    \ of the platform. This includes forking, modifying, and maintaining the forked\
    \ platform's repository, leading to high maintenance costs. Therefore, addressing\
    \ the configurability and extendability aspects of these platforms is crucial\
    \ to lowering the adoption barrier of open-source-based edge solutions.\n\nIn\
    \ this paper, we address the limitations mentioned above by introducing X-HEEP[1](#page-2-0)\
    \ [\\[30\\]](#page-20-1), an open-source configurable and extendable platform\
    \ designed to support the exploration of ultra-low power edge accelerators. X-HEEP\
    \ is a streamlined configurable host architecture based on RISC-V and built on\
    \ top of existing IPs from relevant open-source projects, such as the PULP project,\
    \ the OpenHW Group, and the OpenTitan project, as extensively verified, validated\
    \ in silicon, and adopted in commercial products. This allows extensive reuse\
    \ of third-party hardware and software extensions and inheriting verification\
    \ and silicon validation.\n\nTo allow users to explore their custom solutions,\
    \ X-HEEP can be natively extended via the proposed eXtendible Accelerator InterFace\
    \ (XAIF), which allows the integration of a wide range of accelerators with different\
    \ area, power, and performance constraints. Having a complete interface that covers\
    \ all the edge accelerator performance and power requirements will enable extensive\
    \ reuse of hardware and software IPs, reducing costs and mitigating fragmentation.\
    \ To explore the custom hardware design space, users will use X-HEEP as an IP\
    \ which exposes an interface capable of addressing all the edge-computing state-of-the-art\
    \ requirements for domain-specific applications. To enable a high degree of versatility,\
    \ such exploration can be performed both on FPGAs or RTL simulators, as well as\
    \ SystemC for a mixed high-level and RTL simulation environment. Additionally,\
    \ to offer high degree of optimizations, X-HEEP offers internal configurability\
    \ options through the selection of different (1) core types, depending on the\
    \ target workloads [\\[29\\]](#page-20-2); (2) bus topology and addressing mode,\
    \ ensuring a perfect match with the bandwidth requirements of the integrated accelerators;\
    \ (3) memory size, depending on the processing data and application complexity;\
    \ and finally (4) peripherals, to provide the needed I/O connectivity. This configurability\
    \ enables designers to tailor the platform to specific application requirements\
    \ and meet area, power, and performance constraints.\n\nAs energy efficiency is\
    \ a key figure in edge computing devices, X-HEEP implements state-of-the-art fine-grained\
    \ low-power strategies such as clock-gating, power-gating, and RAM retention,\
    \ which are integrated into the XAIF interface to be leveraged by the connected\
    \ accelerators and maximize overall energy efficiency.\n\nTo demonstrate the real-world\
    \ applicability of X-HEEP, we present an integration example specifically tailored\
    \ for ultra-low-power healthcare applications. These applications typically involve\
    \ long and slow acquisition periods, where data from external bio-sensors are\
    \ stored in memory while the rest of the system is in an idle state, followed\
    \ by intense processing periods to compute pattern extraction algorithms based\
    \ on digital signal processing algorithms, or machine learning (ML), and deep\
    \ learning. Therefore, we extended X-HEEP with a CGRA accelerator [\\[9\\]](#page-19-0)\
    \ and an IMC accelerator [\\[31\\]](#page-20-3), both of which have been shown\
    \ to efficiently reduce the overall energy consumption of healthcare applications\
    \ [\\[7,](#page-19-1) [24\\]](#page-20-4). We configured X-HEEP with the RISC-V\
    \ OpenHW Group CV32E20 core [\\[29\\]](#page-20-2), 8 banks of 32 KiB on-chip\
    \ SRAM organized in a contiguous addressing mode, and 11 different power domains\
    \ (including the external accelerators) that can be individually switched on and\
    \ off for fine-grained power control.\n\nThe resulting design, called HEEPocrates,\
    \ has been implemented both in FPGAs on the Zynq 7020, Zynq UltraScale+, and Artix\
    \ 7 chips by Xilinx, for early prototyping, verification, and system exploration,\
    \ as well as in silicon with TSMC 65 nm low-power CMOS technology, for silicon\
    \ validation and profiling performance, power, and area figures. The measured\
    \ performance of the fabricated chip shows that it can operate in a wide voltage\
    \ range, from 0.8 V to 1.2 V, achieving a maximum frequency of 170 MHz and 470\
    \ MHz, respectively. Its power consumption ranges from 270 \xB5W at 32 kHz and\
    \ 0.8 V, to 48 mW at 470 MHz and 1.2 V.\n\nTo validate our design and compare\
    \ it with state-of-the-art solutions, we measured the energy consumption of the\
    \ chip in a set of healthcare applications, showing that it offers a good trade-off\
    \ between the computationally hungry\n\n<span id=\"page-2-0\"></span><sup>1</sup>X-HEEP\
    \ is freely downloadable at https://github.com/esl-epfl/x-heep under a permissive\
    \ license.\n\nand acquisition-dominated state-of-the-art microcontrollers commonly\
    \ adopted in this domain. This demonstrates the flexibility of our host platform\
    \ in adapting to the specific needs of integrated accelerators and matching the\
    \ strict requirements of healthcare applications. Furthermore, we present the\
    \ energy benefits of 4.9 \xD7 and 4.8 \xD7 gained by exploiting the integrated\
    \ CGRA accelerator and the IMC accelerator, respectively, compared to running\
    \ on the host CPU.\n\nThroughout this work, we will dive deeper into the features\
    \ of the X-HEEP platform, describing its architecture, configurability, and extendability\
    \ options to build versatile and energy-efficient edge applications.\n\nThe following\
    \ contributions are presented:\n\n- X-HEEP: A configurable and extendible RISC-V\
    \ microcontroller to support the exploration of ultra-low-power edge accelerators.\n\
    - XAIF: A configurable interface that adapts to the different requirements of\
    \ accelerators in terms of programmability, bandwidth, interrupts, and power modes,\
    \ and that allows their seamless integration into the X-HEEP architecture.\n-\
    \ HEEPocrates: A real-world integration example, based on TSMC 65 nm low-power\
    \ CMOS technology, that includes a CGRA and an IMC accelerator.\n- An open-source\
    \ repository with a permissive license with the complete X-HEEP platform code\
    \ and documentation to allow researchers to explore new custom accelerators and\
    \ advance research in this field.\n\nThe remainder of this paper is structured\
    \ as follows. In Section 2, we conduct an in-depth analysis of the most relevant\
    \ state-of-the-art accelerators and host platforms. In Section 3, we provide a\
    \ qualitative and quantitative description of the configurability and extendability\
    \ features of X-HEEP. In Section 4, we present a real-world integration example,\
    \ called HEEPocrates. In Section 5 we describe our experimental setup, while in\
    \ Section 6 our experimental results. Lastly, in Section 7, we offer a comprehensive\
    \ summary of the main conclusions of our work.\n\n#### 2 STATE-OF-THE-ART\n\n\
    This section gives an overview of cutting-edge accelerators, analyzing the fundamental\
    \ requirements necessary for their integration into host platforms. Such requirements\
    \ are collected into the XAIF to accommodate all the state-of-the-art accelerators.\
    \ Subsequently, it focuses on host platforms from the state of the art, conducting\
    \ an evaluation of their strengths and limitations in terms of configurability\
    \ and extendability.\n\n#### 2.1 Edge-computing accelerators\n\nThe extensive\
    \ array of open-source accelerators includes a diverse range of requirements regarding\
    \ memory capacity, area, performance, and power efficiency. Therefore, an analysis\
    \ of their main features becomes imperative for the design of flexible and efficient\
    \ host platforms.\n\nWe can divide accelerators into three main categories: memories,\
    \ processors (and co-processors), and I/O peripherals. One accelerator can belong\
    \ to one or more categories.\n\n<span id=\"page-3-0\"></span>2.1.1 Memories. Memory\
    \ accelerators are a class of IPs that feature one or more slave ports to access\
    \ internal functionality. These IPs require the host CPU, or a DMA, to copy the\
    \ needed computational data from the main memory of the platform to their internal\
    \ data memory, before starting the operations. At the end of the computations,\
    \ an interrupt or status bit could be used to synchronize with the host CPU.\n\
    \nAn example is the Keccak accelerator presented in [\\[8\\]](#page-19-2). This\
    \ accelerator exposes two 32 bit slave ports, one to access the internal register\
    \ file used for control and status operations and one to access the private data\
    \ memory, which stores the processing data.\n\nOther examples are in-memory or\
    \ near-memory macros such as the C-SRAM [\\[15\\]](#page-19-3), where the IP is\
    \ connected via a 32 bit slave port to a host platform that sends commands/instructions\
    \ to the memory through write operations. C-SRAM decodes the memory instructions,\
    \ by concatenating the address and the write data transmitted by the CPU, and\
    \ performs the requested operation.\n\n<span id=\"page-4-0\"></span>2.1.2 Processors.\
    \ To improve performance, many accelerators feature one or multiple master ports\
    \ to independently read in parallel the processing data and write back the generated\
    \ results from/to the main memory.\n\nSome examples are domain-specific accelerators,\
    \ such as DSP engines [\\[16\\]](#page-19-4), CGRAs [\\[9\\]](#page-19-0), multi-CPU\
    \ clusters [\\[25\\]](#page-20-5), GPUs [\\[33\\]](#page-20-6), etc., and application-specific\
    \ accelerators for neural networks [\\[5\\]](#page-19-5), FFT [\\[32\\]](#page-20-7),\
    \ cryptography [\\[8\\]](#page-19-2), image processing [\\[20\\]](#page-20-8),\
    \ etc.\n\nAn example of a domain-specific accelerator is the CGRA presented in\
    \ [\\[9\\]](#page-19-0), which has two 32 bit slave ports for configuration registers\
    \ and private instruction memory and four 32 bit master ports for reading and\
    \ writing data from and to the main memory, reaching a maximum bandwidth of 128\
    \ bit per bus cycle.\n\nAnother example is the PULP cluster [\\[25\\]](#page-20-5),\
    \ which features four to eight CV32E40P cores [\\[12\\]](#page-19-6) connected\
    \ to a shared instruction cache and a multi-bank scratchpad memory. The cluster\
    \ exposes one 32 bit slave port for configuration and for pre-loading the memories,\
    \ and one 64 bit master port shared between the cluster DMA, to transfer data\
    \ in and out of the scratchpad memory, and the instruction cache, to fetch program\
    \ code.\n\nExamples of application-specific accelerators are Echoes [\\[32\\]](#page-20-7)\
    \ and Marsellus [\\[13\\]](#page-19-7). The former is used to speed up FFT execution\
    \ and has eight 32 bit master ports, four allocated for input and four for output,\
    \ reaching a maximum bandwidth of 256 bit per bus cycle. The latter accelerates\
    \ convolution layers and offers nine 32 bit master ports, with a maximum bandwidth\
    \ of 288 bit per bus cycle.\n\nCo-processors are a sub-category of processors\
    \ used to implement custom ISA extensions. Co-processors are either tightly coupled\
    \ in the processor pipeline, or integrated via a dedicated interface for reusability.\n\
    \nTheir wide application domains include floating-point operations [\\[3\\]](#page-19-8),\
    \ posit arithmetic [\\[18\\]](#page-20-9), post-quantum cryptography [\\[10\\\
    ]](#page-19-9), integer complex arithmetic [\\[34\\]](#page-20-10), etc. For example,\
    \ [\\[10\\]](#page-19-9) proposes a post-quantum cryptography ISA extension and\
    \ interacts with the coupled RISC-V CPU thanks to the CORE-V-XIF [\\[6\\]](#page-19-10)\
    \ interface of OpenHW Group.\n\nAll these previous examples illustrate that there\
    \ is a large choice of possible processors and co-processors for edge AI systems\
    \ today. Therefore, it is required to have a fast and scalable exploration and\
    \ prototyping framework to choose the right set of components, co-processors,\
    \ or domain-specific accelerators that a final implementation should have, and\
    \ then a well-tuned silicon design flow with a predefined set of open-source hardware\
    \ components and peripherals.\n\n2.1.3 I/O peripherals. These IPs are meant to\
    \ implement special interfaces to communicate with off-chip components or to pre/post-process\
    \ data during such communications.\n\nAn example can be found in Arnold [\\[27\\\
    ]](#page-20-11), where the embedded FPGA (eFPGA) can be used to control an off-chip\
    \ accelerator, which requires a custom interface, as well as to pre-process data\
    \ coming from peripherals before being stored in memory.\n\nIn this case, there\
    \ is a clear need to ideally have a framework that can target both on-chip and\
    \ off-chip accelerator concepts by enabling a flexible set of interconnect standards.\
    \ This set of standards should be extendable with minimum\n\neffort from the system\
    \ designer thanks to an interface that enables a superset of interconnection protocols,\
    \ as we propose in X-HEEP.\n\n### 2.2 Host platforms\n\nIn this subsection, we\
    \ present a comparison of relevant open-source platforms that can be used to host\
    \ edge-computing accelerators, focusing on configurability, extendability, and\
    \ other key features. The limitations of each platform are analyzed in detail\
    \ to motivate the need for a dedicated solution that can fulfill all the requirements.\n\
    \n2.2.1 PULPissimo [\\[28\\]](#page-20-12). A single-core platform within the\
    \ PULP family, designed to target ultra-low-power edgecomputing applications.\
    \ Depending on performance requirements, designers can configure the platform\
    \ with the CV32E20 [\\[29\\]](#page-20-2) or CV32E40P [\\[12\\]](#page-19-6) cores.\
    \ PULPissimo has been integrated with various accelerators, including the aforementioned\
    \ multi-CPU cluster [\\[25\\]](#page-20-5), neural network accelerators [\\[13\\\
    ]](#page-19-7), CGRAs [\\[9\\]](#page-19-0), eFPGAs [\\[27\\]](#page-20-11), etc.\
    \ Many silicon prototypes have been implemented, which demonstrate best-in-class\
    \ energy efficiency in a wide range of applications.\n\nHowever, PULPissimo provides\
    \ only a generic AXI 32 bit slave and a 64 bit master external interfaces that\
    \ are used to connect the multi-CPU cluster, while the other accelerators have\
    \ been integrated by forking and modifying the original RTL code. Such external\
    \ interfaces may limit accelerators' bandwidth. Moreover, the platform lacks native\
    \ support for external interrupts/events and power control, which is crucial for\
    \ efficient power management. Lastly, the platform does not offer configurability\
    \ options to select memory size, bus topology, and memory addressing mode, or\
    \ to change the included peripherals, which limit area, bandwidth, and power-space\
    \ exploration.\n\n2.2.2 Cheshire [\\[22\\]](#page-20-13). The limitations mentioned\
    \ above have been partially addressed by another PULP-based platform, called Cheshire.\
    \ Cheshire is based on the CVA6 core [\\[35\\]](#page-20-14) and allows designers\
    \ to choose the number of external slave and master ports to connect their custom\
    \ accelerators. Furthermore, the platform allows for the configuration of the\
    \ internal last-level cache (LLC) size and of the necessary peripherals, providing\
    \ the flexibility needed to target specific application requirements.\n\nHowever,\
    \ Cheshire has been designed for high-performance systems and consumes up to 300\
    \ mW, making it unsuitable for most ultra-low-power devices, which typically operate\
    \ in the range of tens of mW. Furthermore, Cheshire lacks support for external\
    \ interrupts and power control, which has implications for its overall energy\
    \ efficiency, as the accelerators are usually power-hungry. Lastly, designers\
    \ do not have the option to select the core type, bus topology, and memory addressing\
    \ mode.\n\n2.2.3 BlackParrot [\\[23\\]](#page-20-15). An open-source Linux-capable\
    \ platform designed to accommodate one or multiple customdesigned accelerators.\
    \ The platform showcases a mesh of heterogeneous tiles, offering the flexibility\
    \ to compose 64 bit BlackParrot cores, L2 cache slices, I/O, DRAM controllers,\
    \ and accelerators in various configurations.\n\nHowever, it does not allow for\
    \ selecting the core type, bus topology, and memory addressing mode. Additionally,\
    \ the absence of essential peripherals commonly used in edge devices, such as\
    \ I2Cs, GPIOs, timers, DMAs, interrupt controllers, and a power manager to implement\
    \ low-power strategies, restricts the usage of the platform for real applications\
    \ deployed on ultra-low-power edge applications. Moreover, the platform's internal\
    \ integration of accelerators, as opposed to external plug-ins, involves forking\
    \ and modifying the original RTL code, leading to greater effort and higher development\
    \ costs. Lastly, the 64 bit architecture of BlackParrot targets high-performance\
    \ systems and is unsuitable for ultra-low-power edge devices.\n\n2.2.4 OpenTitan\
    \ [\\[17\\]](#page-19-11). OpenTitan is designed for ultra-low-power edge-secure\
    \ applications. It offers a single-core architecture based on the CV32E20 [\\\
    [29\\]](#page-20-2) core and an extensive portfolio of peripherals.\n\nDespite\
    \ these strengths, OpenTitan does not offer external support for accelerator plug-ins,\
    \ requiring designers to manually modify the RTL code to integrate their custom\
    \ accelerators. Furthermore, the platform lacks configurability for core type,\
    \ bus topology, and memory addressing mode and size. Furthermore, OpenTitan does\
    \ not come equipped with built-in low-power strategies.\n\n2.2.5 Chipyard [\\\
    [1\\]](#page-19-12). On the contrary, the Rocket chip generator [\\[26\\]](#page-20-16),\
    \ which has been subsequently incorporated and expanded into the Chipyard platform,\
    \ offers extensive configuration options. Using the open-source Chisel hardware\
    \ description language, designers can craft their system, providing flexibility\
    \ and customization. The platform offers a wide range of core types, including\
    \ Ariane, CV32E20, Rocket, and BOOM, allowing designers to tailor the system's\
    \ performance to meet specific application requirements. Additionally, the memory\
    \ size and peripherals can be customized, further enhancing its adaptability.\n\
    \nHowever, even though Chipyard enables accelerators to be integrated into the\
    \ design using the Chisel language, the platform does not offer external master\
    \ and slave ports for the connectivity of accelerators. As a result, designers\
    \ need to invest time in becoming familiar with the Chisel language to successfully\
    \ configure the architecture and integrate custom accelerators. Furthermore, Chipyard\
    \ does not provide support for any specific power reduction strategies. Given\
    \ the critical importance of power efficiency in ultra-low-power applications,\
    \ designers are forced to implement power-saving techniques manually to achieve\
    \ the desired energy efficiency level.\n\n2.2.6 LiteX [\\[14\\]](#page-19-13)\
    \ and ESP [\\[19\\]](#page-20-17). Two other notable SoC generators are LiteX\
    \ and ESP. LiteX serves as a framework thought to explore various FPGA-based architectures.\
    \ On the other hand, ESP is an open-source platform designed for heterogeneous\
    \ SoC design and prototyping on FPGAs. Both platforms offer configurable options,\
    \ allowing designers to customize core type, memory size, peripherals, and the\
    \ number of external master and slave ports, making them adaptable to various\
    \ application requirements.\n\nHowever, LiteX and ESP focus on FPGA development\
    \ only and do not offer support for ASIC design flow. Such limitations hinder\
    \ their applicability in projects aimed at silicon implementations and present\
    \ difficulties in accurately estimating the platform energy consumption, crucial\
    \ when evaluating the impact of integrated accelerators. Moreover, they lack built-in\
    \ support for external interrupts and power control, essential for efficient power\
    \ management.\n\n2.2.7 X-HEEP. To overcome the limitations mentioned above and\
    \ cater to the unique needs of ultra-low-power edge designers, we present in this\
    \ paper the X-HEEP platform. The proposed platform features a streamlined architecture\
    \ that operates in conjunction with dedicated open-source tools. These tools enable\
    \ developers to easily customize and extend the architecture with their accelerators\
    \ and interconnection interfaces, thus eliminating the need for manual modification\
    \ of the RTL code. Using X-HEEP, designers can achieve the desired level of configurability,\
    \ extendability, and power efficiency, making it an ideal choice for a wide range\
    \ of ultra-low-power edge applications. In addition, it has been developed using\
    \ SystemVerilog, to offer high compatibility with most of the available electronic\
    \ design automation (EDA) tools.\n\n#### 3 X-HEEP\n\nIn this section, we present\
    \ a qualitative and quantitative analysis of the key features of X-HEEP regarding\
    \ configurability, extendability, and software support. We synthesized X-HEEP\
    \ with TSMC 65 nm low-power technology and performed our quantitative analysis\
    \ at the nominal voltage, 1.2 V.\n\n#### 3.1 Architecture\n\nFigure [1](#page-8-0)\
    \ shows the X-HEEP architecture and its essential components. These include a\
    \ configurable RISC-V CPU, a configurable bus, a configurable memory, two configurable\
    \ peripheral domains, and a debug unit.\n\nX-HEEP leverages existing widely adopted\
    \ open-source IPs to maintain compatibility with existing systems and reuse available\
    \ software routines and hardware extensions. Among the wide portfolio of open-source\
    \ IPs, we selected those that provide permissive licenses, to ease the X-HEEP\
    \ adoption to a wide range of users, and written in SystemVerilog, to make the\
    \ integration in existing systems and EDA tools compatible with industrial standards.\n\
    \nThe RISC-V cores have been selected from the OpenHW Group CORE-V family, as\
    \ extensively verified, mature, and implemented in silicon many times; the bus,\
    \ the memory models, the debug unit, and a plethora of IPs from the PULP project,\
    \ as again adopted by several stakeholders and validated in silicon multiple times;\
    \ and the peripherals from the OpenTitan project as documented, verified, and\
    \ inclusive of hardware-abstraction-layer (HAL) functions. Moreover, X-HEEP includes\
    \ home-made IPs such as a boot ROM, a power manager, a fast interrupt controller,\
    \ and a DMA.\n\n3.1.1 CPU. The user can choose among the CV32E20, CV32E40X, and\
    \ CV32E40P as core options [\\[29\\]](#page-20-2), to trade off power and performance.\
    \ In particular, the CV32E20 core is optimized for control-oriented tasks, while\
    \ the CV32E40P core is optimized for processing-oriented tasks. The CV32E40X core\
    \ offers power consumption and performance similar to the CV32E40P core, without\
    \ featuring the floating-point RVF and custom Xpulp ISA extensions. Moreover,\
    \ it provides an external interface, known as CORE-V-XIF [\\[6\\]](#page-19-10),\
    \ that allows for the plug-in of custom co-processors to extend the RISC-V ISA\
    \ without the need to modify the RTL code of the core.\n\n3.1.2 Memory. The user\
    \ can select the memory size and number of memory banks to trade off area, power,\
    \ and storage capacity. Each bank offers a retention state aimed at reducing leakage\
    \ power, of about 42.5 % compared to active leakage, when the bank is not accessed\
    \ for some time but the data needs to be preserved.\n\n3.1.3 Bus. To maximize\
    \ compatibility with the other IPs selected from the OpenHW Group, PULP platforms,\
    \ and OpenTitan project, the bus is based on the same open-bus interface (OBI)\
    \ [\\[21\\]](#page-20-18) protocol.\n\nThe user can choose either a one-at-a-time\
    \ topology, where only one master at a time can access the bus (one decoder),\
    \ or a fully connected topology (same number of decoders as simultaneous masters),\
    \ where multiple masters can access multiple slaves in parallel, to trade off\
    \ area and bandwidth. When the fully connected option is used, the user can further\
    \ configure the bus to access a variable number of banks in a contiguous or interleaved\
    \ addressing mode. The contiguous mode offers limited bandwidth to applications\
    \ that require multiple masters to access contiguous data stored in memory but\
    \ allows for power-gating or setting in retention mode the banks that are not\
    \ actively used. Vice versa, the interleaved mode offers higher bandwidth to applications\
    \ that access contiguous data in memory, at the cost of keeping all the banks\
    \ active all the time.\n\nIn addition, to connect additional components, the bus\
    \ also exposes a configurable number of slave and master ports to the external\
    \ XAIF interface to accommodate one or multiple accelerators with different bandwidth\
    \ constraints. Manuscript submitted to ACM\n\n<span id=\"page-8-0\"></span>![](_page_8_Figure_1.jpeg)\n\
    \nFigure 1. X-HEEP architecture. The various power domains are visually marked\
    \ using different colors. The components in grey are always on. The accelerator\
    \ and co-processor integration are highlighted in red.\n\nFigures [2](#page-9-0)\
    \ (a) and (b) show the variation in the area and the bandwidth of the X-HEEP bus\
    \ by adding slave/master ports to the basic bus configuration, which connects\
    \ the CV32E20 core, two memory banks, the debug unit, the two peripheral domains,\
    \ and no external connection. Ports are added in pairs, i.e., for each external\
    \ master port (M), we add an internal slave port (S) for a memory bank to avoid\
    \ limiting bandwidth during memory access.\n\nIncreasing the number of slave/master\
    \ ports does not lead to any performance improvement in the one-at-atime configuration,\
    \ limited to 32 bit per bus cycle according to its architecture. On the contrary,\
    \ the fully connected configuration maximizes bandwidth, which increases linearly\
    \ with the number of bus ports, at the cost of a higher area (and power consumption).\
    \ The bus in the one-at-a-time configuration occupies about 85 % less silicon\
    \ space compared to the fully connected configuration, considering the same number\
    \ of slave/master ports.\n\nIn overall performance, a 16 \xD7 16 matrix multiplication\
    \ algorithm on X-HEEP takes approximately 34 % fewer clock cycles in the highest\
    \ performance configuration with the CV32E40P core and fully connected bus compared\
    \ to the lowest power configuration with the CV32E40P core and one-at-a-time bus.\
    \ Furthermore, when using the Xpulp extensions and fully connected bus, the CV32E40P\
    \ can compute matrix multiplication algorithms 4 \xD7 faster with 32 bit data\
    \ or up to 16 \xD7 faster with 8 bit SIMD extensions for the same CPU without\
    \ extensions, as shown in [\\[12\\]](#page-19-6).\n\n3.1.4 Peripheral domain.\
    \ Figure [2](#page-9-0) (c) shows the area of the IPs located in the peripheral\
    \ domain. This domain includes peripherals that can be removed from the design\
    \ or powered off if not needed to trade off area or power and functionality. Manuscript\
    \ submitted to ACM\n\n<span id=\"page-9-0\"></span>![](_page_9_Figure_1.jpeg)\n\
    \nFigure 2. Exploration of different X-HEEP configurations. The used technology\
    \ is TSMC 65 nm low-power CMOS at 1.2 V.\n\nThese include a platform-level interrupt\
    \ controller (PLIC), a timer, and general-purpose I/O peripherals such as a GPIO,\
    \ I2C, and SPI.\n\n3.1.5 Always-on peripheral domain. This domain includes IPs\
    \ that are always powered on. To meet our specific needs and requirements, we\
    \ custom-designed key components such as an SoC controller, a boot ROM, a power\
    \ manager, a fast interrupt controller, and a DMA. The domain also includes other\
    \ peripherals such as a timer, a UART, an SPI, and a GPIO.\n\nThe power manager\
    \ is responsible for implementing low-power strategies, including clock-gating,\
    \ power-gating, and RAM retention. It features a set of configuration registers\
    \ that provide the user with real-time control over the available low-power techniques.\n\
    \nThe architecture is divided into several power domains, marked with different\
    \ colors in Figure [1.](#page-8-0) Clock-gating can be applied to the main CPU,\
    \ peripheral domain, and each memory bank, while retention can only be applied\
    \ to memory banks. Additionally, each power domain can be individually power-gated.\
    \ The leakage power consumption of each domain is reported in Figure [2](#page-9-0)\
    \ (d). The system bus, debug unit, and other essential IPs represent about 35\
    \ % of the Manuscript submitted to ACM\n\nleakage power of the always-on domain.\
    \ The remaining 65 % comes from other general-purpose peripherals added to enhance\
    \ versatility, such as a GPIO, SPI, UART, etc.\n\nThe platform can be extended\
    \ with additional power domains to include user external accelerators. This is\
    \ possible thanks to external power ports, part of the XAIF interface, directly\
    \ connected to the power manager, which can be used to clock-gate, power-gate,\
    \ or set in retention mode external accelerators.\n\n#### 3.2 Extendible Accelerator\
    \ InterFace (XAIF)\n\nThe extensive array of domain-specific hardware accelerators\
    \ encompasses a diverse range of requirements, including memory capacity, area,\
    \ performance, and power efficiency. These varied demands are aggregated into\
    \ the configurable XAIF interface, facilitating enhanced connectivity to state-of-the-art\
    \ accelerators, and agile integration into microcontrollers for real-life applications.\
    \ Such an interface gathers all the requirements to extend X-HEEP with domain-specific\
    \ customizations. To the best of the authors' knowledge, no other open-source\
    \ platform for edge-computing applications exists that provides such a complete\
    \ extension interface to fulfill the requirements of state-of-the-art solutions.\n\
    \n3.2.1 Memory mapped ports. A configurable number of slave and master ports,\
    \ utilizing the OBI protocol, can be harnessed to connect custom accelerators\
    \ to the X-HEEP bus. Slave ports provide easy access and configuration for memory-like\
    \ accelerators, exemplified in Subsection [2.1.1,](#page-3-0) such as the Keccak\
    \ [\\[8\\]](#page-19-2), which requires two 32 bit slave ports for control and\
    \ status operations and data memory. In addition, a further peripheral interface\
    \ connected to the X-HEEP peripheral bus is provided for external custom peripherals.\
    \ This peripheral interface is further extended by a FIFO interface to allow easy\
    \ DMA-peripheral connections. This allows the CPU to wait for peripheral transactions\
    \ to transfer all data to the main memory with the support of the system DMA,\
    \ as implemented in [\\[27\\]](#page-20-11). On the other hand, master ports accommodate\
    \ the bandwidth requirements of processor-like accelerators outlined in Subsection\
    \ [2.1.2.](#page-4-0) For example, a CGRA [\\[9\\]](#page-19-0) leverages the\
    \ four 32 bit master ports that are used to independently read and write data\
    \ to and from the main memory.\n\n3.2.2 Interrupt ports. A configurable number\
    \ of interrupt lines can be used by the custom hardware to rapidly synchronize\
    \ with the host CPU. Each line is connected to the X-HEEP PLIC interrupt controller,\
    \ which can be controlled via software. This functionality allows the host CPU\
    \ to enter a sleep state during active accelerator periods, significantly reducing\
    \ the overall energy consumption of the running application.\n\n3.2.3 Power control\
    \ ports. To provide low-power strategy capabilities to custom accelerators, a\
    \ configurable number of power control interfaces is provided. Each interface\
    \ is connected to the X-HEEP power manager to implement different power-saving\
    \ strategies. Each interface includes control signals for power-gating, clock-gating,\
    \ and RAM retention.\n\n#### 3.3 Tools and software\n\nWe present the tools and\
    \ software provided by X-HEEP to configure, program, and implement user designs.\n\
    \n3.3.1 Configuration. X-HEEP is configured through SystemVerilog templates, which\
    \ function as a dynamic tool that enables users to automatically customize the\
    \ RTL code of the platform thanks to customizable parameters. This makes the generated\
    \ code readable and easy to maintain and debug.\n\n3.3.2 Software. X-HEEP includes\
    \ a HAL to access peripheral functionalities and supports FreeRTOS for improved\
    \ development and efficient resource management.\n\n3.3.3 Simulation and implementation.\
    \ X-HEEP offers support for simulation and implementation, in FPGA and silicon,\
    \ based on the FuseSoc build system [\\[11\\]](#page-19-14). FuseSoC supports\
    \ several EDA tools, such as Verilator, Questasim, Design Compiler, Genus, and\
    \ Vivado, and automatically generates the scripts required to simulate or implement\
    \ user designs. Thanks to it, the user can explore the design both at a high level\
    \ by integrating accelerators described in SystemC, at the RTL level, and FPGA,\
    \ for early prototyping and exploration, as well as in silicon, for final validation.\n\
    \n#### 4 HEEPOCRATES\n\nIn this section, we present an integration example to\
    \ demonstrate the real-world applicability of X-HEEP. This integration effort\
    \ results in HEEPocrates, a heterogeneous architecture designed for ultra-low-power\
    \ healthcare applications. These applications typically involve extended data\
    \ acquisition periods during which data from external biosensors are stored in\
    \ memory, followed by intensive processing periods to classify such data. Therefore,\
    \ we exploited the XAIF interface to extend X-HEEP with a CGRA accelerator [\\\
    [9\\]](#page-19-0) and an IMC accelerator [\\[31\\]](#page-20-3), both of which\
    \ have been shown to efficiently reduce the overall energy consumption of healthcare\
    \ applications [\\[7,](#page-19-1) [24\\]](#page-20-4). Moreover, each accelerator\
    \ is located in a separate power domain that can be individually switched on and\
    \ off for fine-grained power control.\n\n### 4.1 Architecture\n\nFigure [3](#page-12-0)\
    \ shows the HEEPocrates architecture highlighting how the CGRA and IMC accelerators\
    \ are integrated to minimize power and maximize bandwidth.\n\n4.1.1 X-HEEP configuration.\
    \ We configured the X-HEEP host platform with (1) the CV32E20 core, which is optimal\
    \ for running control tasks and offloading performance-intensive computations\
    \ to the external accelerators while preserving low power consumption; (2) 8 SRAM\
    \ banks of 32 KiB in contiguous addressing mode to accommodate variable lengths\
    \ of data acquisitions while power-gating the unused banks on different applications;\
    \ (3) a fully connected bus to provide high-bandwidth capabilities to the integrated\
    \ accelerators; (4) all the available peripherals in place to deliver high flexibility;\
    \ (5) a CGRA and IMC accelerators connected to the external XAIF interface.\n\n\
    4.1.2 CGRA accelerator [\\[9\\]](#page-19-0). This accelerator offers two slave\
    \ ports, one to access the internal configuration registers, and one for the context\
    \ memory, plus four master ports used to load and store data. The context memory\
    \ stores the kernel's code executed by the four internal processing elements (PEs).\
    \ Each PE is connected to a dedicated master port to read and write data from/to\
    \ the X-HEEP main memory, independently. This allows a maximum bandwidth of 128\
    \ bit per bus cycle. To synchronize the CGRA and the CPU, the CGRA end-of-computation\
    \ event is connected to the X-HEEP interrupt controller (PLIC) via the XAIF interface.\n\
    \nThe CGRA is divided into two power domains: one for the control logic and the\
    \ datapaths; and one for the context memory. The control logic and datapaths can\
    \ be clock-gated or power-gated, while the context memory can be clock-gated,\
    \ power-gated, or set in retention mode. This dual power domain structure enables\
    \ clock-gating individual domains during short periods of inactivity and power-gating\
    \ during extended non-use periods. Additionally, it offers the flexibility of\
    \ independently setting the context memory in retention mode while clock-gating\
    \ or power-gating the datapaths and control logic to save CGRA configuration time.\
    \ The XAIF interface provides control over the various power modes, enabling the\
    \ system to dynamically adjust its power consumption based on the operational\
    \ requirements of the CGRA accelerator.\n\n<span id=\"page-12-0\"></span>![](_page_12_Figure_1.jpeg)\n\
    \nFigure 3. HEEPocrates architecture. Power domains are visually marked using\
    \ different colors. The components highlighted in grey are always on. The accelerator\
    \ integration is highlighted in red.\n\n4.1.3 IMC accelerator [\\[31\\]](#page-20-3).\
    \ This accelerator offers one slave port to access its memory array. An internal\
    \ controller decodes the memory requests and facilitates the transition of the\
    \ accelerator between two modes: memory mode and computation mode. In memory mode,\
    \ the memory space functions as a conventional memory bank. In contrast, the computation\
    \ mode enables the execution of in-memory computations, eliminating the need for\
    \ additional data transfers between the main memory and the accelerator.\n\nAs\
    \ for the CGRA, the IMC accelerator is placed in a separate power domain to save\
    \ power when not used.\n\n4.1.4 Frequency-locked loop [\\[2\\]](#page-19-15).\
    \ We utilized the XAIF interface to connect the frequency-locked loop (FLL) responsible\
    \ for generating the system clock from a 32 kHz external source. For real-time\
    \ configurability, the FLL exposes a set of memory-mapped registers that enable\
    \ the host CPU to adjust the system clock frequency during application execution,\
    \ dynamically. This feature is precious during extended data acquisition periods\
    \ in healthcare applications because it allows for reducing the system frequency\
    \ to the minimum value required for acquiring the necessary biosignals, thereby\
    \ minimizing dynamic power consumption. Lastly, the FLL can be also bypassed,\
    \ allowing the external source to serve as the system clock.\n\n## 4.2 FPGA implementation\n\
    \nWe implemented HEEPocrates in FPGAs on the Zynq 7020, Zynq UltraScale+, and\
    \ Artix 7 chips by Xilinx for early prototyping. This allows for the exploration\
    \ of different X-HEEP configurations and accelerators to optimally tune the architecture\
    \ for the healthcare domain.\n\n#### 4.3 Silicon implementation\n\nAfter FPGA\
    \ prototyping and exploration, we implemented HEEPocrates in silicon with TSMC\
    \ 65 nm low-power CMOS technology. Figure [4](#page-13-0) shows the 6 mm<sup>2</sup>\
    \ layout of HEEPocrates, with the power domains shown in different colors.\n\n\
    For conducting our measurements, we developed a board specifically designed to\
    \ accommodate our chip. HEEPocrates has been tested from 0.8 V to 1.2 V, achieving\
    \ a maximum frequency of 170 MHz and 470 MHz, respectively. Power Manuscript submitted\
    \ to ACM\n\n## 14 Machetti et al.\n\n<span id=\"page-13-0\"></span>![](_page_13_Figure_1.jpeg)\n\
    \nFigure 4. HEEPocrates layout, silicon photo, and physical chip (on a Swiss 5-cent\
    \ franc coin).\n\nconsumption ranges from 270 \xB5W at 32 kHz and 0.8 V, to 48\
    \ mW at 470 MHz and 1.2 V. Each phase of healthcare applications has been optimized\
    \ to minimize power consumption.\n\n4.3.1 Acquisition phase. Healthcare applications\
    \ commonly feature an extended acquisition phase due to the lowbandwidth nature\
    \ of biosignals and the typical lengthy data windows. During this phase, samples\
    \ are gathered from external ADCs via SPI, or other I/O peripherals, and stored\
    \ in memory by the main CPU or the DMA. We run this phase at 1 MHz, 0.8 V to minimize\
    \ power while offering enough performance for the acquisition of bio-signals in\
    \ the order of hundreds of Hertz. HEEPocrates consumes 384 \xB5W during acquisition\
    \ when the complete system is active, and the host CPU is clock-gated when not\
    \ used. However, power can be further optimized by switching off the unused memory\
    \ banks, the peripheral domain, and the external accelerators for the entire acquisition\
    \ period. This enables a reduction in power of 19 %, which leads to 310 \xB5W.\
    \ Furthermore, the CPU can be turned off during idle periods, i.e., when not used\
    \ actively to acquire ADC samples, reaching the lowest power level of the system\
    \ at 1 MHz of 286 \xB5W, with a further reduction of 8 %.\n\n4.3.2 Processing\
    \ phase. Upon completion of the acquisition phase, we run the processing phase\
    \ at the maximum speed of 170 MHz, 0.8 V to minimize processing time and race\
    \ to sleep. HEEPocrates consumes 8.17 mW during the processing phase when the\
    \ complete system is active and the CPU executes a matrix multiplication. Power\
    \ can be further optimized 6 % by turning off the unused memory banks, the peripheral\
    \ domain, and the external accelerators, with a consumption of about 7.68 mW.\
    \ During the processing phase, the external accelerators can be individually powered\
    \ on, and computationally intensive tasks can be offloaded by the main CPU to\
    \ reduce the system's overall energy consumption. HEEPocrates consumes 4.01 mW\
    \ and 1.65 mW when CNN algorithms are executed on the CGRA accelerator and IMC\
    \ accelerator, respectively, at their maximum frequency of 60 MHz. The host CPU,\
    \ the unused memory banks, and the peripheral domain are powered off during accelerator\
    \ activity.\n\n#### 5 EXPERIMENTAL SETUP\n\nThis section introduces a representative\
    \ set of different families of microcontrollers commonly used in healthcare applications.\
    \ Subsequently, it describes the biomedical applications that are included in\
    \ our benchmark.\n\n#### 5.1 Healthcare microcontrollers\n\nHealthcare applications\
    \ exhibit significant variability in acquisition and processing times, influenced\
    \ by factors such as the length of sampling windows and the complexity of adopted\
    \ algorithms. To address this variability, a diverse range of microcontrollers\
    \ have been designed, each optimized to minimize power consumption during specific\
    \ phases. The Apollo 3 Blue excels in acquisition phases, prioritizing power efficiency\
    \ through its deep sleep mode, which ensures remarkably low power consumption\
    \ when the system is inactive during idle periods. On the other hand, GAP9 takes\
    \ the lead in processing phases thanks to its higher-performance core, which guarantees\
    \ substantial reductions in processing time. The analysis of these two microcontrollers\
    \ enables covering the entire spectrum of ultra-low-power edge devices, ranging\
    \ from top-tier power efficiency, with Apollo 3 Blue, to top-tier performance,\
    \ with GAP9. Furthermore, the frequent use of both microcontrollers in this domain\
    \ demonstrates their capability to meet the rigorous demands of healthcare applications\
    \ in terms of performance, power, and area. Table [1](#page-15-0) reports the\
    \ features of the selected microcontrollers.\n\n5.1.1 Apollo 3 Blue. This MCU\
    \ is part of the Ambiq board and features an ARM Cortex-M4 core. The code is stored\
    \ in the on-chip flash memory with zero overhead in instruction fetching, while\
    \ the rest of the data resides either entirely in the SRAM when it fits or in\
    \ both the SRAM and the flash. Unnecessary SRAM banks are turned off for the entire\
    \ duration of the application. Its optimal processing configuration is 0.7 V,\
    \ 48 MHz. However, we exploited the TurboSPOT mode to increase the frequency to\
    \ 96 MHz when required to meet the timing constraints of the benchmark applications.\
    \ Moreover, during idle periods, the system enters its deep sleep mode, consuming\
    \ approximately 6 \xB5A/MHz, where most of the system components are power-gated,\
    \ with only a few power control modules active.\n\n5.1.2 GAP9. This MCU is part\
    \ of the GAP9EVK board and features one CV32E40P core, known as the fabric controller\
    \ (FC), and a cluster (CL) with nine CV32E40P cores, which can be switched on\
    \ and off. We execute the benchmark applications exclusively on the FC while power-gating\
    \ the CL and unnecessary SRAM banks for the entire duration of the application.\
    \ The application code and data are stored in the SRAM for maximum performance.\
    \ Its optimal processing configuration is 0.65 V, 240 MHz. Furthermore, during\
    \ idle periods, the system transitions into its sleep mode, where the majority\
    \ of components are power-gated, except for memory banks, which enter a retention\
    \ mode.\n\n5.1.3 HEEPocrates. The application code and data are completely stored\
    \ in the SRAM, when possible, or in a combination of the SRAM and the off-chip\
    \ flash, connected through the SPI interface. The peripheral domain and the unused\
    \ memory banks are also powered off throughout the entire duration of the application.\
    \ We execute all the benchmark applications on the host CPU while power-gating\
    \ the external accelerators. Moreover, we also accelerate CNN computations on\
    \ the CGRA and IMC accelerators and showcase the energy improvement compared to\
    \ running on the host CPU. We performed each measurement under the optimal operating\
    \ conditions: 170 MHz at 0.8 V, for the host CPU; 60 MHz at 0.8 V, for the CGRA\
    \ and IMC accelerators. During idle periods, the host CPU and the external accelerators\
    \ are power-gated, and the system frequency is lowered to 1 MHz to reduce power\
    \ consumption.\n\n<span id=\"page-15-0\"></span>\n\n| MCU           | Board  \
    \       | Processing element | Voltage | Maximum frequency |\n|---------------|---------------|--------------------|---------|-------------------|\n\
    | Apollo 3 Blue | Ambiq         | Cortex-M4          | 0.7 V   | 48 MHz      \
    \      |\n| GAP9          | Gapuino       | CV32E40P           | 0.65 V  | 240\
    \ MHz           |\n| HEEPocrates   | Testing board | CV32E20            | 0.8\
    \ V   | 170 MHz           |\n\nTable 1. Microcontrollers commonly adopted in healthcare\
    \ applications.\n\nTable 2. Healthcare applications included in our benchmark.\n\
    \n<span id=\"page-15-1\"></span>\n\n| Application           | Acquisition window\
    \ | Input leads | Sampling rate | Bits per sample |\n|-----------------------|--------------------|-------------|---------------|-----------------|\n\
    | Heartbeat classifier  | 15 s               | 3           | 256 Hz        | 16\
    \              |\n| Seizure detection CNN | 4 s                | 23          |\
    \ 256 Hz        | 16              |\n\n#### 5.2 Healthcare applications\n\nTable\
    \ [2](#page-15-1) reports the healthcare applications selected for our benchmark.\
    \ Our selection ensures that we cover the full spectrum of ultra-low-power healthcare\
    \ applications, ranging from acquisition-dominated, with the heartbeat classifier,\
    \ to processing-dominated, with the seizure detection CNN. Moreover, these applications\
    \ showcase computational algorithms of varying complexity, thereby enhancing the\
    \ comprehensiveness of our analysis.\n\n5.2.1 Heartbeat classifier [\\[4\\]](#page-19-16).\
    \ This application is used to detect irregular beat patterns for common heart\
    \ diseases through the analysis of electrocardiogram (ECG) signals. The most resource-intensive\
    \ part of this application lies in the initial computation phase, specifically\
    \ the morphological filtering, which consumes over 80 % of the total processing\
    \ time. Subsequently, the classification stage employs random projections. Initially,\
    \ the algorithm processes a single input channel. If an abnormal heartbeat is\
    \ detected, the analysis extends to the other leads for a more precise determination.\
    \ Our testing scenarios involve input signals that all contain abnormal beats\
    \ to evaluate the complete application pipeline. The input signal is derived from\
    \ three distinct ECG leads, each sampled at 256 Hz with an accuracy of 16 bit.\
    \ A 15 s acquisition window produces an input signal of 22.5 KiB.\n\n5.2.2 Seizure\
    \ detection CNN [\\[13\\]](#page-19-7). This application is used to detect seizures\
    \ in electroencephalography (EEG) signals. It features a CNN with three one-dimensional\
    \ convolutional layers, each incorporating pooling and ReLU layers. 90 % of the\
    \ processing time is spent in convolutional computations, which mainly involve\
    \ multiply and accumulate (MAC) and shift operations. Following each convolution,\
    \ there is an overflow check and a maximum test for the pooling layer. Two fully\
    \ connected layers end the network. The signal is sampled from 23 leads at a rate\
    \ of 256 Hz with 16 bit accuracy and the acquisition phase lasts 4 s, resulting\
    \ in an input signal size of 46 KiB.\n\n#### 6 EXPERIMENTAL RESULTS\n\nIn this\
    \ section, first, we analyze the energy consumption of the proposed host platform,\
    \ HEEPocrates (with the accelerators power-gated), in comparison with the selected\
    \ state-of-the-art microcontrollers that may serve as host platforms. Subsequently,\
    \ we assess the energy efficiency gained from leveraging the HEEPocrates' accelerators\
    \ in comparison to execution on the host CPU.\n\n<span id=\"page-16-0\"></span>![](_page_16_Figure_1.jpeg)\n\
    \nFigure 5. Energy consumption of our benchmark running on common healthcare microcontrollers\
    \ and on HEEPocrates at 0.8 V.\n\n#### 6.1 Host platforms\n\nFigure [5](#page-16-0)\
    \ illustrates the measured energy values for each healthcare application from\
    \ our benchmark.\n\nThe heartbeat classifier application exhibits an acquisition-driven\
    \ nature, characterized by extended acquisition windows and a low sampling rate\
    \ of 256 Hz. This forces microcontrollers to spend a significant amount of time\
    \ in idle states during acquisition. In particular, the Apollo 3 Blue stands out\
    \ for its energy efficiency, attributed to its remarkably low sleep mode of only\
    \ 6 \xB5A/MHz, where most of the system is power-gated, with only a few control\
    \ modules active. On the contrary, GAP9 lacks aggressive sleep modes and keeps\
    \ more modules always on, resulting in considerably higher energy consumption.\
    \ Even during the processing phase, Apollo 3 Blue maintains a slight energy advantage\
    \ over GAP9. This can be attributed to the optimized design of its CPU, the ARM\
    \ Cortex-M4, which is more efficient for the specific operations required by this\
    \ application, including logical and comparison operations, branches, as well\
    \ as load and store instructions [\\[4\\]](#page-19-16).\n\nHEEPocrates positions\
    \ itself in a middle ground during acquisition, offering a more robust sleep mode\
    \ compared to GAP9. However, it does not reach the exceptionally low power consumption\
    \ levels of Apollo 3 Blue due to the absence of aggressive sleep strategies for\
    \ faster wake-up times, which includes in the always-on IPs more peripherals as\
    \ an FLL, a pad controller, bus, a debug unit, and more general-purpose peripherals\
    \ added for enhanced versatility (e.g. SPI, UART, etc.). However, HEEPocrates'\
    \ energy efficiency can be improved by removing the general-purpose peripherals,\
    \ resulting in a 27 % reduction in overall energy consumption. During processing,\
    \ HEEPocrates consumes slightly higher energy compared to the other microcontrollers,\
    \ due to its ultra-low-power CV32E20 core [\\[29\\]](#page-20-2) that is not optimized\
    \ for performance like GAP9, and due to the higher-power consumption of the active\
    \ part of the chip compared to Apollo-3, sitting HEEPocrates in the middle between\
    \ the two.\n\nThe seizure detection CNN application is processing-dominated due\
    \ to its computationally intense convolutional network, leading microcontrollers\
    \ to spend the majority of their time in the processing phase. GAP9 emerges as\
    \ the dominant contender in this phase, leveraging its high-performance core to\
    \ achieve reduced processing times and efficient transitions to sleep. In contrast,\
    \ the core of Apollo 3 Blue lacks sufficient computational power, resulting in\
    \ an extended processing phase and increased energy consumption. However, during\
    \ the acquisition phase, Apollo 3 Blue maintains dominance over GAP9 due to its\
    \ more efficient sleep mode, resulting in lower energy consumption.\n\n## <span\
    \ id=\"page-17-0\"></span>18 Machetti et al.\n\n![](_page_17_Figure_1.jpeg)\n\n\
    Figure 6. Energy consumption of HEEPocrates at 0.8 V running a 16x16 convolution\
    \ (3x3 filter) on the host CPU (at 170 MHz) and the CGRA and IMC accelerators\
    \ (at 60 MHz).\n\nHEEPocrates finds itself positioned between Apollo 3 Blue and\
    \ GAP9 in both the processing and acquisition phases. During acquisition, it offers\
    \ a more efficient low-power mode than GAP9 but does not reach the efficiency\
    \ levels of Apollo 3 Blue, for the reasons explained earlier. In the processing\
    \ phase, the higher performance of HEEPocrates allows for faster entry into the\
    \ sleep state than Apollo 3 Blue but lags behind GAP9 due to its higher-frequency\
    \ core. Notably, similar to the previous application, HEEPocrates' energy efficiency\
    \ may be enhanced by removing general-purpose peripherals from the always-on domain,\
    \ resulting in an overall energy reduction of about 3 %.\n\nIn conclusion, our\
    \ analysis reveals the energy consumption alignment of HEEPocrates with state-of-the-art\
    \ microcontrollers commonly adopted in healthcare applications. The performance\
    \ and power efficiency of our platform falls between the top-tier power efficiency\
    \ of Apollo 3 Blue and the top-tier performance of GAP9. This underscores that\
    \ HEEPocrates achieves state-of-the-art energy efficiency figures across a wide\
    \ range of real-world application profiles typical of the healthcare domain, ranging\
    \ from acquisition-dominated to processing-dominated scenarios.\n\n#### 6.2 Accelerators\n\
    \nIn Figure [6,](#page-17-0) we compare the energy consumption of HEEPocrates\
    \ while running a 16\xD716 convolutional layer with a 3\xD73 filter on the host\
    \ CPU, the CGRA and IMC accelerators. Our results demonstrate an improvement in\
    \ energy efficiency of approximately 4.9 \xD7 and 4.8 \xD7 achieved by exploiting\
    \ the integrated CGRA accelerator and the IMC accelerator, respectively, compared\
    \ to running on the host CPU. This improvement is attributed to the higher parallelism\
    \ of the proposed accelerators, which compensates for the increased power consumption\
    \ resulting from the more intense computation.\n\n#### 7 CONCLUSIONS\n\nIn this\
    \ paper, we have explored the growth and increasing demand for efficient processing\
    \ solutions in the field of edge computing, particularly in the context of new\
    \ AI/ML applications. Persistent challenges arise from the limitations in performance\
    \ and power consumption of edge devices, which impact overall energy efficiency.\n\
    \nTo address these challenges, heterogeneous architectures have emerged, presenting\
    \ a promising solution by combining ultra-low-power host processors with specialized\
    \ accelerators tailored to specific applications or domains.\n\nHowever, we have\
    \ shown the limitations of existing host platforms in exploring the design space\
    \ of acceleratorbased ultra-low power edge AI platforms, as well as in providing\
    \ the configurability and extendability options needed Manuscript submitted to\
    \ ACM\n\nto integrate the large variety of custom accelerators and interfaces\
    \ that exist nowadays. Consequently, extensive modifications to the RTL code are\
    \ often required to integrate accelerators effectively, leading to high maintenance\
    \ costs.\n\nTo overcome these limitations, we introduced X-HEEP, an open-source\
    \ solution designed specifically to support the integration and exploration of\
    \ ultra-low-power edge AI/ML accelerators. The platform offers comprehensive customizability\
    \ and extendability options via the proposed XAIF, which gathers all the requirements\
    \ of state-of-the-art domain-specific solutions, as memory-mapped accelerators,\
    \ including memory, processors, and peripherals with DMAsupport, custom ISA co-processor,\
    \ interrupts, and power saving strategies interface, enabling designers to tailor\
    \ the platform to meet the unique requirements of the target applications in performance,\
    \ power, and area.\n\nX-HEEP provides configuration options to match specific\
    \ application requirements by exploring various core types, bus topologies, and\
    \ memory addressing modes. It also enables a fine-grained configuration of memory\
    \ banks to match the constraints of the integrated accelerators. The platform\
    \ prioritizes energy efficiency by implementing low-power strategies and integrating\
    \ them with accelerators through dedicated power control interfaces. This cohesive\
    \ integration ensures that all system components work together to maximize energy\
    \ savings.\n\nTo illustrate the practical benefits of X-HEEP, in this work, we\
    \ presented a real-world integration example tailored for healthcare applications,\
    \ which shows high variability among acquisition and processing-dominated application\
    \ profiles. This example featured a CGRA accelerator and an IMC accelerator, both\
    \ of which have proved to effectively reduce the overall energy consumption for\
    \ this application domain. The resulting design, called HEEPocrates, has been\
    \ implemented both in FPGAs on the Zynq 7020, Zynq UltraScale+, and Artix 7 chips\
    \ by Xilinx, for early prototyping and exploration, and in silicon with TSMC 65\
    \ nm low-power CMOS technology, for silicon validation. The fabricated chip can\
    \ operate from 0.8 V to 1.2 V, achieving a maximum frequency of 170 MHz and 470\
    \ MHz, respectively. Its power consumption ranges from 270 \xB5W at 32 kHz and\
    \ 0.8 V, to 48 mW at 470 MHz and 1.2 V.\n\nTo measure the performance and versatility\
    \ of the proposed design, we analyze the execution of an illustrative real-life\
    \ set of edge AI/ML benchmarks that combines ultra-low power healthcare applications\
    \ from the latest advances in the field, showing high variability in the execution\
    \ profile. Through the execution of our benchmark and the measurement of the energy\
    \ consumption of the chip, we demonstrated HEEPocrates' alignment with other stateof-the-art\
    \ microcontrollers that are frequently employed in healthcare applications. This\
    \ is achieved thanks to a balanced trade-off between fine-grain power domains,\
    \ to reduce power consumption during acquisition phases, and on-demand accelerator\
    \ capabilities, to speed up the execution of processing phases, resulting in a\
    \ good trade-off between acquisition-dominated and processing-dominated applications.\
    \ These results also showcase the representativeness of the experiments that other\
    \ researchers could perform after integrating their accelerators with X-HEEP.\
    \ Lastly, we proved the energy benefit of 4.9 \xD7 and 4.8 \xD7 gained by exploiting\
    \ the integrated CGRA accelerator and IMC accelerator, respectively, compared\
    \ to running on the host CPU.\n\nIn conclusion, the introduction of the X-HEEP\
    \ platform leads to a significant step forward in overcoming the challenges faced\
    \ in the field of edge computing. By providing extensive options for customizability\
    \ and extendability, prioritizing energy efficiency, and presenting a practical\
    \ real-world integration example, X-HEEP presents itself as an innovative platform,\
    \ empowering designers and researchers to create efficient heterogeneous edge\
    \ AI/ML computing systems.\n\n#### 8 ACKNOWLEDGEMENTS\n\nWe would like to thank\
    \ the entire X-HEEP team for their great contribution to the platform.\n\n####\
    \ REFERENCES\n\n- <span id=\"page-19-12\"></span>[1] Alon Amid et al. \"Chipyard:\
    \ Integrated Design, Simulation, and Implementation Framework for Custom SoCs\"\
    . In: IEEE Micro 40.4 (2020), pp. 10\u201321. doi: [10.1109/MM.2020.2996616.](https://doi.org/10.1109/MM.2020.2996616)\n\
    - <span id=\"page-19-15\"></span>[2] David E Bellasi and Luca Benini. \"Smart\
    \ energy-efficient clock synthesizer for duty-cycled sensor socs in 65 nm/28nm\
    \ cmos\". In: IEEE Transactions on Circuits and Systems I: Regular Papers 64.9\
    \ (2017), pp. 2322\u20132333.\n- <span id=\"page-19-8\"></span>[3] Andrea Bocco,\
    \ Yves Durand, and Florent De Dinechin. \"SMURF: Scalar Multiple-Precision Unum\
    \ Risc-V Floating-Point Accelerator for Scientific Computing\". In: Proc. of the\
    \ ACM Conference for Next Generation Arithmetic. CoNGA'19. 2019. isbn: 9781450371391.\
    \ doi: [10.1145/3316279.3316280.](https://doi.org/10.1145/3316279.3316280)\n-\
    \ <span id=\"page-19-16\"></span>[4] Rub\xE9n Braojos, Giovanni Ansaloni, and\
    \ David Atienza. \"A Methodology for Embedded Classification of Heartbeats Using\
    \ Random Projections\". In: DATE. IEEE, May 2013, pp. 899\u2013904. isbn: 9781467350716.\
    \ doi: [10.7873/DATE.](https://doi.org/10.7873/DATE.2013.189) [2013.189.](https://doi.org/10.7873/DATE.2013.189)\n\
    - <span id=\"page-19-5\"></span>[5] Francesco Conti et al. \"A 12.4TOPS/W @ 136GOPS\
    \ AI-IoT System-on-Chip with 16 RISC-V, 2-to-8b Precision-Scalable DNN Acceleration\
    \ and 30%-Boost Adaptive Body Biasing\". In: IEEE ISSCC. 2023, pp. 21\u201323.\
    \ doi: [10.1109/](https://doi.org/10.1109/ISSCC42615.2023.10067643) [ISSCC42615.2023.10067643.](https://doi.org/10.1109/ISSCC42615.2023.10067643)\n\
    - <span id=\"page-19-10\"></span>[6] CORE-V X-Interface. url: [https://github.com/openhwgroup/core-v-xif.](https://github.com/openhwgroup/core-v-xif)\n\
    - <span id=\"page-19-1\"></span>[7] Elisabetta De Giovanni et al. \"Modular Design\
    \ and Optimization of Biomedical Applications for Ultralow Power Heterogeneous\
    \ Platforms\". In: IEEE Transactions on Computer-Aided Design of Integrated Circuits\
    \ and Systems 39.11 (2020), pp. 3821\u20133832. doi: [10.1109/TCAD.2020.3012652.](https://doi.org/10.1109/TCAD.2020.3012652)\n\
    - <span id=\"page-19-2\"></span>[8] Alessandra Dolmeta et al. \"Implementation\
    \ and Integration of Keccak Accelerator on RISC-V for CRYSTALS-Kyber\". In: Proc.\
    \ of the 20th ACM Int. Conf. on Computing Frontiers. CF '23. Bologna, Italy, 2023,\
    \ pp. 381\u2013382. doi: [10.1145/3587135.3591432.](https://doi.org/10.1145/3587135.3591432)\n\
    - <span id=\"page-19-0\"></span>[9] Loris Duch et al. \"A multi-core reconfigurable\
    \ architecture for ultra-low power bio-signal analysis\". In: IEEE BioCAS. 2016,\
    \ pp. 416\u2013419. doi: [10.1109/BioCAS.2016.7833820.](https://doi.org/10.1109/BioCAS.2016.7833820)\n\
    - <span id=\"page-19-9\"></span>[10] Tim Fritzmann, Georg Sigl, and Johanna Sep\xFA\
    lveda. \"RISQ-V: Tightly Coupled RISC-V Accelerators for Post-Quantum Cryptography\"\
    . In: IACR Transactions on Cryptographic Hardware and Embedded Systems 2020.4\
    \ (Aug. 2020), pp. 239\u2013280. doi: [10.13154/tches.v2020.i4.239-280.](https://doi.org/10.13154/tches.v2020.i4.239-280)\n\
    - <span id=\"page-19-14\"></span>[11] FuseSoC. url: [https://github.com/olofk/fusesoc.](https://github.com/olofk/fusesoc)\n\
    - <span id=\"page-19-6\"></span>[12] Michael Gautschi et al. \"Near-threshold\
    \ RISC-V core with DSP extensions for scalable IoT endpoint devices\". In: IEEE\
    \ Transactions on Very Large Scale Integration (VLSI) Systems 25.10 (2017), pp.\
    \ 2700\u20132713.\n- <span id=\"page-19-7\"></span>[13] Catalina G\xF3mez et al.\
    \ \"Automatic seizure detection based on imaged-EEG signals through fully convolutional\
    \ networks\". In: Scientific reports 10.1 (2020), pp. 1\u201313.\n- <span id=\"\
    page-19-13\"></span>[14] Florent Kermarrec et al. LiteX: an open-source SoC builder\
    \ and library based on Migen Python DSL. 2020. arXiv: 2005.02506 [\\[cs.AR\\]](https://arxiv.org/abs/2005.02506).\n\
    - <span id=\"page-19-3\"></span>[15] Maha Kooli et al. \"Towards a Truly Integrated\
    \ Vector Processing Unit for Memory-Bound Applications Based on a Cost-Competitive\
    \ Computational SRAM Design Solution\". In: J. Emerg. Technol. Comput. Syst. 18.2\
    \ (2022). issn: 1550-4832. doi: [10.1145/3485823.](https://doi.org/10.1145/3485823)\n\
    - <span id=\"page-19-4\"></span>[16] Kai Li, Wei Yin, and Qiang Liu. \"A Portable\
    \ DSP Coprocessor Design Using RISC-V Packed-SIMD Instructions\". In: IEEE ISCAS.\
    \ 2023, pp. 1\u20135. doi: [10.1109/ISCAS46773.2023.10181681.](https://doi.org/10.1109/ISCAS46773.2023.10181681)\n\
    - <span id=\"page-19-11\"></span>[17] LowRISC. OpenTitan. url: [https://github.com/lowRISC/opentitan.](https://github.com/lowRISC/opentitan)\n\
    \n- <span id=\"page-20-9\"></span><span id=\"page-20-0\"></span>[18] David Mallas\xE9\
    n, Alberto A. del Barrio, and Manuel Prieto-Matias. Big-PERCIVAL: Exploring the\
    \ Native Use of 64-Bit Posit Arithmetic in Scientific Computing. 2023. arXiv:\
    \ [2305.06946.](https://arxiv.org/abs/2305.06946)\n- <span id=\"page-20-17\"></span>[19]\
    \ Paolo Mantovani et al. \"Agile SoC Development with Open ESP : Invited Paper\"\
    . In: 2020 IEEE/ACM International Conference On Computer Aided Design (ICCAD).\
    \ 2020, pp. 1\u20139.\n- <span id=\"page-20-8\"></span>[20] Katayoun Neshatpour\
    \ et al. \"Big biomedical image processing hardware acceleration: A case study\
    \ for K-means and image filtering\". In: IEEE ISCAS. 2016, pp. 1134\u20131137.\
    \ doi: [10.1109/ISCAS.2016.7527445.](https://doi.org/10.1109/ISCAS.2016.7527445)\n\
    - <span id=\"page-20-18\"></span>[21] Open Bus Interface Protocol. url: [https://github.com/openhwgroup/obi.](https://github.com/openhwgroup/obi)\n\
    - <span id=\"page-20-13\"></span>[22] Alessandro Ottaviano et al. \"Cheshire:\
    \ A Lightweight, Linux-Capable RISC-V Host Platform for Domain-Specific Accelerator\
    \ Plug-In\". In: IEEE Transactions on Circuits and Systems II: Express Briefs\
    \ (2023), pp. 1\u20131. doi: [10.1109/](https://doi.org/10.1109/TCSII.2023.3289186)\
    \ [TCSII.2023.3289186.](https://doi.org/10.1109/TCSII.2023.3289186)\n- <span id=\"\
    page-20-15\"></span>[23] Daniel Petrisko et al. \"BlackParrot: An Agile Open-Source\
    \ RISC-V Multicore for Accelerator SoCs\". In: IEEE Micro 40.4 (2020), pp. 93\u2013\
    102. doi: [10.1109/MM.2020.2996145.](https://doi.org/10.1109/MM.2020.2996145)\n\
    - <span id=\"page-20-4\"></span>[24] Flavio Ponzina et al. \"A Hardware/Software\
    \ Co-Design Vision for Deep Learning at the Edge\". In: IEEE Micro 42.6 (July\
    \ 2022), pp. 48\u201354. doi: [10.1109/MM.2022.3195617.](https://doi.org/10.1109/MM.2022.3195617)\n\
    - <span id=\"page-20-5\"></span>[25] Antonio Pullini et al. \"Mr. Wolf: An energy-precision\
    \ scalable parallel ultra low power SoC for IoT edge processing\". In: IEEE Journal\
    \ of Solid-State Circuits 54.7 (2019), pp. 1970\u20131981.\n- <span id=\"page-20-16\"\
    ></span>[26] Rocket. url: [https://github.com/chipsalliance/rocket-chip.](https://github.com/chipsalliance/rocket-chip)\n\
    - <span id=\"page-20-11\"></span>[27] Pasquale Davide Schiavone et al. \"Arnold:\
    \ An eFPGA-augmented RISC-V SoC for flexible and low-power IoT end nodes\". In:\
    \ IEEE Transactions on Very Large Scale Integration (VLSI) Systems 29.4 (2021),\
    \ pp. 677\u2013690.\n- <span id=\"page-20-12\"></span>[28] Pasquale Davide Schiavone\
    \ et al. \"Quentin: an Ultra-Low-Power PULPissimo SoC in 22nm FDX\". In: (2018),\
    \ pp. 1\u20133. doi: [10.1109/S3S.2018.8640145.](https://doi.org/10.1109/S3S.2018.8640145)\n\
    - <span id=\"page-20-2\"></span>[29] Pasquale Davide Schiavone et al. \"Slow and\
    \ steady wins the race? A comparison of ultra-low-power RISC-V cores for Internet-of-Things\
    \ applications\". In: Int. Symp. on Power and Timing Modeling, Optimization and\
    \ Simulation (PATMOS). IEEE. 2017, pp. 1\u20138.\n- <span id=\"page-20-1\"></span>[30]\
    \ Pasquale Davide Schiavone et al. \"X-HEEP: An Open-Source, Configurable and\
    \ Extendible RISC-V Microcontroller\". In: Proc. of Int. Conf. on Computing Frontiers.\
    \ CF '23. New York, NY, USA: ACM, 2023, pp. 379\u2013380. isbn: 9798400701405.\
    \ doi: [10.1145/3587135.3591431.](https://doi.org/10.1145/3587135.3591431)\n-\
    \ <span id=\"page-20-3\"></span>[31] William Andrew Simon et al. \"BLADE: An in-cache\
    \ computing architecture for edge devices\". In: IEEE Transactions on Computers\
    \ 69.9 (2020), pp. 1349\u20131363.\n- <span id=\"page-20-7\"></span>[32] Mattia\
    \ Sinigaglia et al. Echoes: a 200 GOPS/W Frequency Domain SoC with FFT Processor\
    \ and I2S DSP for Flexible Data Acquisition from Microphone Arrays. 2023. arXiv:\
    \ [2305.07325.](https://arxiv.org/abs/2305.07325)\n- <span id=\"page-20-6\"></span>[33]\
    \ Blaise Tine et al. \"Vortex: Extending the RISC-V ISA for GPGPU and 3D-Graphics\"\
    . In: IEEE/ACM Int. Symp. on Microarchitecture (MICRO). 2021, pp. 754\u2013766.\
    \ isbn: 9781450385572. doi: [10.1145/3466752.3480128.](https://doi.org/10.1145/3466752.3480128)\n\
    - <span id=\"page-20-10\"></span>[34] Y. Varma and M.P. Tull. \"Architectural\
    \ design of a complex arithmetic signal processor (CASP)\". In: Region 5 Conference:\
    \ Annual Technical and Leadership Workshop. 2004, pp. 69\u201376. doi: [10.1109/REG5.2004.1300163.](https://doi.org/10.1109/REG5.2004.1300163)\n\
    - <span id=\"page-20-14\"></span>[35] Florian Zaruba and Luca Benini. \"The cost\
    \ of application-class processing: Energy and performance analysis of a Linux-ready\
    \ 1.7-GHz 64-bit RISC-V core in 22-nm FDSOI technology\". In: IEEE Transactions\
    \ on Very Large Scale Integration (VLSI) Systems 27.11 (2019), pp. 2629\u2013\
    2640."
  paper_id: 7
  scores:
    originality:
      raw: 4.5
      reason: The proposed REACT system introduces a novel approach to energy buffering
        that adapts to fluctuating power sources, representing a significant advancement
        in batteryless system design.
    technical_soundness:
      raw: 4.0
      reason: The methodology appears robust and is supported by empirical evaluation,
        although further details on implementation challenges could enhance the technical
        rigor.
    clarity:
      raw: 4.2
      reason: The paper is well-structured and communicates complex ideas effectively,
        though some sections could benefit from additional explanations for broader
        accessibility.
    significance:
      raw: 4.5
      reason: The findings demonstrate a substantial improvement in energy efficiency
        and system responsiveness, highlighting the potential impact on the future
        of IoT applications.
    relevance:
      raw: 5.0
      reason: The topic is highly relevant to current trends in energy harvesting
        and IoT, addressing critical challenges in sustainable system design.
  total_score: 4.34
  tier: A
- title: "A Composable Dynamic Sparse Dataflow Architecture for Efficient\n  Event-based\
    \ Vision Processing on FPGA"
  abstract: 'Event-based vision represents a paradigm shift in how vision information
    is

    captured and processed. By only responding to dynamic intensity changes in the

    scene, event-based sensing produces far less data than conventional frame-based

    cameras, promising to springboard a new generation of high-speed, low-power

    machines for edge intelligence. However, processing such dynamically sparse

    input originated from event cameras efficiently in real time, particularly with

    complex deep neural networks (DNN), remains a formidable challenge. Existing

    solutions that employ GPUs and other frame-based DNN accelerators often

    struggle to efficiently process the dynamically sparse event data, missing the

    opportunities to improve processing efficiency with sparse data. To address

    this, we propose ESDA, a composable dynamic sparse dataflow architecture that

    allows customized DNN accelerators to be constructed rapidly on FPGAs for

    event-based vision tasks. ESDA is a modular system that is composed of a set of

    parametrizable modules for each network layer type. These modules share a

    uniform sparse token-feature interface and can be connected easily to compose

    an all-on-chip dataflow accelerator on FPGA for each network model. To fully

    exploit the intrinsic sparsity in event data, ESDA incorporates the use of

    submanifold sparse convolutions that largely enhance the activation sparsity

    throughout the layers while simplifying hardware implementation. Finally, a

    network architecture and hardware implementation co-optimizing framework that

    allows tradeoffs between accuracy and performance is also presented.

    Experimental results demonstrate that when compared with existing GPU and

    hardware-accelerated solutions, ESDA achieves substantial speedup and

    improvement in energy efficiency across different applications, and it allows

    much wider design space for real-world deployments.'
  url: http://arxiv.org/abs/2401.05626v1
  keywords: ''
  document: "# A Composable Dynamic Sparse Dataflow Architecture for Efficient Event-based\
    \ Vision Processing on FPGA\n\n[Yizhao Gao](https://orcid.org/0000-0001-5673-3746)\
    \ yzgao@eee.hku.hk University of Hong Kong Hong Kong\n\n[Yuhao Ding](https://orcid.org/0009-0009-4819-9251)\
    \ yhding@eee.hku.hk University of Hong Kong Hong Kong\n\n### ABSTRACT\n\nEvent-based\
    \ vision represents a paradigm shift in how vision information is captured and\
    \ processed. By only responding to dynamic intensity changes in the scene, event-based\
    \ sensing produces far less data than conventional frame-based cameras, promising\
    \ to springboard a new generation of high-speed, low-power machines for edge intelligence.\
    \ However, processing such dynamically sparse input originated from event cameras\
    \ efficiently in real time, particularly with complex deep neural networks (DNN),\
    \ remains a formidable challenge. Existing solutions that employ GPUs and other\
    \ frame-based DNN accelerators often struggle to efficiently process the dynamically\
    \ sparse event data, missing the opportunities to improve processing efficiency\
    \ with sparse data. To address this, we propose ESDA, a composable dynamic sparse\
    \ dataflow architecture that allows customized DNN accelerators to be constructed\
    \ rapidly on FPGAs for event-based vision tasks. ESDA is a modular system that\
    \ is composed of a set of parametrizable modules for each network layer type.\
    \ These modules share a uniform sparse token-feature interface and can be connected\
    \ easily to compose an all-on-chip dataflow accelerator on FPGA for each network\
    \ model. To fully exploit the intrinsic sparsity in event data, ESDA incorporates\
    \ the use of submanifold sparse convolutions that largely enhance the activation\
    \ sparsity throughout the layers while simplifying hardware implementation. Finally,\
    \ a network architecture and hardware implementation co-optimizing framework that\
    \ allows tradeoffs between accuracy and performance is also presented. Experimental\
    \ results demonstrate that when compared with existing GPU and hardware-accelerated\
    \ solutions, ESDA achieves substantial speedup and improvement in energy efficiency\
    \ across different applications, and it allows much wider design space for real-world\
    \ deployments.\n\nFPGA '24, March 3\u20135, 2024, Monterey, CA, USA\n\n\xA9 2024\
    \ Copyright held by the owner/author(s).\n\nACM ISBN 979-8-4007-0418-5/24/03.\n\
    \n<https://doi.org/10.1145/3626202.3637558>\n\n[Baoheng Zhang](https://orcid.org/0000-0003-1685-656X)\
    \ bhzhang@eee.hku.hk University of Hong Kong Hong Kong\n\n[Hayden Kwok-Hay So](https://orcid.org/0000-0002-6514-0237)\
    \ hso@eee.hku.hk University of Hong Kong Hong Kong\n\n### CCS CONCEPTS\n\n\u2022\
    \ Computer systems organization \u2192 Reconfigurable computing; Neural networks;\
    \ Data flow architectures; \u2022 Computing methodologies \u2192 Object recognition.\n\
    \n### KEYWORDS\n\nevent-based vision, event camera, sparse DNN accelerator, submanifold\
    \ sparse convolution, dataflow accelerator, FPGA\n\n#### ACM Reference Format:\n\
    \nYizhao Gao, Baoheng Zhang, Yuhao Ding, and Hayden Kwok-Hay So. 2024. A Composable\
    \ Dynamic Sparse Dataflow Architecture for Efficient Eventbased Vision Processing\
    \ on FPGA. In Proceedings of the 2024 ACM/SIGDA International Symposium on Field\
    \ Programmable Gate Arrays (FPGA '24), March 3\u20135, 2024, Monterey, CA, USA.\
    \ ACM, New York, NY, USA, [12](#page-11-0) pages. <https://doi.org/10.1145/3626202.3637558>\n\
    \n### 1 INTRODUCTION\n\nEvent-based vision systems are bio-inspired systems that\
    \ represent a paradigm shift from conventional frame-based vision systems [\\\
    [16\\]](#page-10-0). Instead of capturing the entire field of view as dense matrices\
    \ of pixel values like in a conventional camera, an event camera only produces\
    \ a spiking event when a substantial change in light intensity local to each pixel\
    \ is detected. Despite the fundamentally different way of sensing, substantial\
    \ progress has been made in recent years to perform a wide range of vision tasks\
    \ such as object classification, object detection, and tracking utilizing deep\
    \ neural network models [\\[49\\]](#page-11-1).\n\nHowever, deploying event-based\
    \ vision systems with deep learning models in near-sensor or edge-computing contexts\
    \ remains a great challenge. On the one hand, off-the-shelf GPU platforms and\
    \ dense DNN accelerators that are optimized to operate on dense input from conventional\
    \ cameras can rarely exploit the sparsity in event-based inputs. As a result,\
    \ if blindly employed to accelerate event camera output, these systems struggle\
    \ to deliver the promised energy efficiency and performance benefits due to a\
    \ large amount of wasted computation. On the other hand, some sparse DNN accelerators\
    \ have previously been proposed to address the need for event vision processing\
    \ [\\[1,](#page-10-1) [31\\]](#page-10-2). Yet, these designs primarily concentrate\
    \ on optimizing sparse computation at the matrix multiplication or single layer\
    \ level, often through a reusable processing engine for various layers. Such an\
    \ approach may lead to considerable latency and energy overhead, mainly due to\
    \ the recurrent\n\nPermission to make digital or hard copies of part or all of\
    \ this work for personal or classroom use is granted without fee provided that\
    \ copies are not made or distributed for profit or commercial advantage and that\
    \ copies bear this notice and the full citation on the first page. Copyrights\
    \ for third-party components of this work must be honored. For all other uses,\
    \ contact the owner/author(s).\n\ninput/output operations involving weights and\
    \ intermediate activations. Consequently, these designs are suboptimal for real-time\
    \ fast-reacting systems intended to exploit the low latency sensing and high temporal\
    \ resolution benefits of event cameras.\n\nTo address these challenges, we introduce\
    \ ESDA, a system based on FPGA that can efficiently accelerate Event-based vision\
    \ processing with a composable dynamic Sparse Dataflow Architecture. ESDA is a\
    \ modular system that allows parametrizable modules for each network layer to\
    \ be connected together to form a customized all-on-chip system on FPGA. Employing\
    \ the all-on-chip dataflow architecture is a key design choice towards low latency\
    \ and power consumption by minimizing the off-chip communication of weight and\
    \ activation. In contrast to prior dataflow architectures that handle dense frame-based\
    \ input, ESDA exclusively streams the spatially sparse features from the event-based\
    \ input across the different dataflow modules by using a unified sparse token-feature\
    \ interface. Through the straightforward cascading of dataflow modules corresponding\
    \ with the model definition, a sparse accelerator can be rapidly constructed to\
    \ speed up the application. Furthermore, to take full advantage of the sparse\
    \ dataflow approach, ESDA also integrates submanifold sparse convolution [\\[23\\\
    ]](#page-10-3) at the algorithmic level. Submanifold convolution can effectively\
    \ maintain high spatial sparsity of feature activations of the models by constraining\
    \ the spatial activation locations, which also helps to derive simpler hardware\
    \ implementation.\n\nTo facilitate design space exploration, we also develop a\
    \ network model and hardware architecture co-optimization framework that leverages\
    \ the sparsity statistics from a given event-based vision task or dataset to perform\
    \ accuracy and hardware efficiency tradeoffs. Given the capacity of today's FPGA\
    \ edge device, the search framework can identify many efficient models that fit\
    \ on-chip while achieving good performance and accuracy. To this end, the main\
    \ contribution of this work can be summarized as:\n\n- We proposed a novel composable\
    \ sparse dataflow architecture for DNN inference in event-based vision applications\
    \ that effectively harness the inherent sparsity in event camera data.\n- We implemented\
    \ ESDA on embedded FPGA platforms and established a set of sparsity-aware co-optimization\
    \ flow, enabling the customization of DNN accelerators for each specific application\
    \ or dataset.\n- We showcased a substantial end-to-end hardware speedup and energy-efficiency\
    \ improvement through extensive evaluation on multiple event-based datasets.\n\
    \nIn the next section, background and related work about neuromorphic vision systems\
    \ and processing will first be provided. The detailed design of ESDA will be discussed\
    \ in Section [3.](#page-2-0) Experiment results are shown in Section [4](#page-6-0)\
    \ and we will conclude in Section [5.](#page-9-0) The artifact associated with\
    \ this work is open-sourced and available at [\\[17\\]](#page-10-4).\n\n### 2\
    \ BACKGROUND AND RELATED WORK\n\n### 2.1 Event-based Vision\n\nEvent cameras,\
    \ also known as Neuromorphic Cameras or Dynamic Vision Sensors, are designed to\
    \ mimic the functioning of the human retina. Instead of capturing full frames,\
    \ they detect and report light intensity changes as events asynchronously with\
    \ high temporal resolution. Events are usually encoded in Address Event Representation\
    \ (AER) format. Typically each event can be written as [, , , ] where , points\
    \ to the location of the event, is the timestamp and is the polarity of changes.\
    \ Commercial event cameras nowadays can support 720 \xD7 1280 spatial resolution\
    \ with 1 \xB5s temporal resolution [\\[14\\]](#page-10-5). Compared with traditional\
    \ frame-based cameras, event cameras have several advantages, including: (1) high\
    \ temporal resolution allowing capturing fast events that conventional cameras\
    \ may miss; (2) high dynamic range enabling them to capture scenes with varying\
    \ lighting conditions; (3) high efficiency in terms of data processing and transmission.\
    \ As an emerging technology, event-based vision has shown great potential in areas\
    \ like robotics, autonomous vehicles, high-speed tracking, scientific imaging,\
    \ low-power embedded systems, etc [\\[16\\]](#page-10-0).\n\nFigure [1](#page-2-1)\
    \ shows a sample from event-based dataset DvsGesture [\\[2\\]](#page-10-6) to\
    \ illustrate the working principle of the event camera. In this sample, a man\
    \ is rotating his arm counter-clockwise. As the event camera exclusively responds\
    \ to changes in scene intensity, it captures only the movements of the person\
    \ as a sequence of events. The upper figure shows the scatter plots of events\
    \ in 3D spatial-temporal space. The below four images show the 2D feature representations\
    \ constructed from events in a short time interval. It's a common practice to\
    \ use these 2D representations to serve as the DNN input. Prior research works\
    \ have introduced many different 2D representation algorithms like event histogram\
    \ [\\[36\\]](#page-11-2), time-surface [\\[26,](#page-10-7) [44\\]](#page-11-3),\
    \ learnable MLP kernels [\\[21\\]](#page-10-8), and etc. It's worth noting that\
    \ ESDA can seamlessly integrate with different 2D representation algorithms. As\
    \ long as the resulting feature is spatially sparse, it can benefit from the sparse\
    \ architecture of ESDA.\n\n# 2.2 Hardware Acceleration of Event-based Vision\n\
    \nFrom a system perspective, fully unlocking the potential of the event-based\
    \ approaches cannot be achieved without considering efficient hardware processing.\
    \ Previous efforts have mainly been devoted to two directions. On the one hand,\
    \ due to the similar neuromorphic working principle, many research works have\
    \ integrated event cameras with bio-inspired neuromorphic computing devices. Typically,\
    \ these works seek to accelerate Spiking Neural Networks (SNNs) on customized\
    \ hardware such as TruthNorth [\\[38\\]](#page-11-4), Loihi [\\[10\\]](#page-10-9),\
    \ SpiNNaker [\\[15\\]](#page-10-10), and etc. However, training SNN for some complex\
    \ event-based vision tasks itself can also be challenging. Moreover, despite deployment\
    \ on neuromorphic hardware like TrueNorth, the latency for network inference remains\
    \ substantial, e.g. 105 ms on DVS Gesture Recognition.\n\nOn the other hand, some\
    \ domain-specific architectures for eventbased vision tasks have been proposed,\
    \ such as optical flow calculation [\\[3,](#page-10-11) [32,](#page-10-12) [33\\\
    ]](#page-10-13), object tracking [\\[18,](#page-10-14) [19,](#page-10-15) [29,](#page-10-16)\
    \ [30\\]](#page-10-17), feature recognition [\\[25,](#page-10-18) [34\\]](#page-11-5),\
    \ etc. In recent years, the event-based vision has seen significant advancements\
    \ through integrating deep neural networks (DNNs) in its processing pipeline [\\\
    [49\\]](#page-11-1). However, accelerators originally designed for frame-based\
    \ DNN models struggle to efficiently process event-based inputs with the required\
    \ efficiency. Many sparse DNN accelerators have primarily emphasized element-wise\
    \ weight sparsity or activation sparsity induced by the\n\n<span id=\"page-2-1\"\
    ></span>![](_page_2_Figure_2.jpeg)\n\nEvents in 3D spatial-temporal space\n\n\
    Figure 1: Working principle of event camera. The event camera only captures the\
    \ light intensity change as spiking events in AER format ([x, y, \xB11, timestamp]).\
    \ This figure shows a recording sample from DvsGesture dataset where a man is\
    \ rotating his left arm counter-clockwise. Due to the dynamic nature of event\
    \ cameras, only the motions of the man are captured. For vision tasks like object\
    \ recognition, a certain amount of events are usually grouped to form a 2D representation\
    \ as DNN input.\n\nReLU [\\[7,](#page-10-19) [22,](#page-10-20) [42,](#page-11-6)\
    \ [50\\]](#page-11-7). These approaches differ significantly from the intrinsic\
    \ spatial sparsity in event data. While some accelerators can leverage spatial\
    \ sparsity, they often target at point cloud [\\[13,](#page-10-21) [28\\]](#page-10-22)\
    \ or dynamic spatial pruning networks [\\[9,](#page-10-23) [20\\]](#page-10-24),\
    \ which are also different in terms of the source and pattern of sparsity.\n\n\
    Thus, there is a growing need for specialized accelerators to effectively leverage\
    \ the unique spatial sparsity characteristics of eventbased data to unlock its\
    \ advantages in real-time energy-efficient DNN inference. Prior work like NullHop\
    \ [\\[1,](#page-10-1) [31\\]](#page-10-2) has proposed an architecture that uses\
    \ a binary bitmap to represent layer-wise sparse activation to skip the computation\
    \ of zeros. Other solutions like PFF [\\[40\\]](#page-11-8) have also explored\
    \ treating the event-based input sparsity as a bit-level sparsity by using Binary\
    \ Neural Networks. In this work, we aim to achieve low-latency and energy-efficient\
    \ processing of event-based DNN models by proposing an all-on-chip sparse dataflow\
    \ architecture on FPGA. The framework is also highly composable, allowing the\
    \ customization of accelerators for different event-based models and datasets.\n\
    \n### <span id=\"page-2-0\"></span>3 METHODOLOGY\n\n### 3.1 Design Principle of\
    \ Sparse Dataflow\n\nESDA is built upon distinct sparse dataflow modules that\
    \ share a unified token-feature interface. The framework enables seamless customization\
    \ of a sparse accelerator by cascading various modules aligned with the model's\
    \ architecture. By definition, ESDA only streams spatially sparse features along\
    \ with their coordinate tokens through the dataflow modules. Eqn. [1](#page-2-2)\
    \ shows the design principle for an ESDA dataflow module using a unified sparse\
    \ token-feature interface:\n\n<span id=\"page-2-2\"></span>\n$$\\begin{aligned}\
    \ \\text{token\\_out}\\left[i\\right] &= f\\_{\\overline{T}}(\\text{token\\_in}\\\
    left[j\\right], \\dots, \\text{token\\_in}\\left[j-N\\right]) \\\\ \\text{feature}\\\
    _{\\text{out}}\\left[i\\right] &= f\\_{\\overline{F}}(\\text{feature}\\_{\\text{in}}\\\
    left[j\\right], \\dots, \\text{feature}\\_{\\text{in}}\\left[j-N\\right]) \\\\\
    \ \\text{s.t. } & \\quad \\text{rawel}(\\text{token\\_out}\\left[i\\right]) >\
    \ \\text{rawel}(\\text{token\\_out}\\left[i-1\\right]) \\end{aligned} \\quad (1)$$\n\
    \nwhere each [] (written in [., ., .]) marks the spatial location of the current\
    \ feature vector []. The flag . denotes whether it's the end of the stream. The\
    \ order, calculated as\n\n<span id=\"page-2-3\"></span>![](_page_2_Figure_12.jpeg)\n\
    \nFigure 2: Overall architecture of an ESDA accelerator.\n\n( \u2217 \u210E +\
    \ ), refers to the left-to-right, top-to-bottom streaming order of spatial locations,\
    \ which also corresponds to the 1D memory layout order of a 2D dense matrix. In\
    \ other words, the module itself should maintain the correct output order so that\
    \ different modules can be pipelined.\n\nThe functionality of each dataflow module\
    \ is separated into: (1) to resolve the new output token based on previous input\
    \ token information; (2) to compute the output features at new output token locations\
    \ by using some temporal neighboring features. Depending on the algorithm requirement,\
    \ different numbers of temporal neighboring data might be needed, specified as\
    \ in Eqn. [1.](#page-2-2) Each module in ESDA is designed as a flexible hardware\
    \ template, where many hardware design parameters like bitwidth, parallel factor,\
    \ buffer size, etc., can be specified at compile time.\n\nFigure [2](#page-2-3)\
    \ shows an illustrative accelerator diagram for the eventbased object classification\
    \ task. The processing system, or CPU, is responsible for event streaming and\
    \ 2D representation construction. While the accelerator on the programmable logic\
    \ reads the sparse input features following the left-to-right, top-to-bottom ordering\
    \ to align with our dataflow requirement. A binary bitmap is used to mark the\
    \ non-zero locations and generate the token stream. The tokenized sparse feature\
    \ then propagates through each convolution block. Finally, the accelerator returns\
    \ the classification outputs back to the processing system.\n\n<span id=\"page-3-1\"\
    ></span>![](_page_3_Figure_1.jpeg)\n\n![](_page_3_Figure_2.jpeg)\n\n(a) Standard\
    \ vs. Submanifold Sparse Convolution when stride is 1\n\n(b) Standard vs. Submanifold\
    \ Sparse Convolution when stride is 2\n\nFigure 3: Compare standard convolution\
    \ with submanifold sparse convolution. The gray/green locations in the figure\
    \ mean non-zero pixels. (a) When stride = 1, the input and output location of\
    \ submanifold convolution is restricted to be identical. In the \"x\" location,\
    \ standard convolution leads to a valid non-zero output while submanifold convolution\
    \ does not. (b) When stride > 1 (2 in the figure), an output location is non-zero\
    \ if the corresponding input \xD7 grid contains non-zeros.\n\nIn the following\
    \ section, we will first discuss how ESDA incorporates submanifold sparse convolution,\
    \ which is used to enhance the model's sparsity while simplifying the dataflow\
    \ design ( , and in Eqn. [1\\)](#page-2-2). In Section [3.3,](#page-3-0) we will\
    \ show the detailed microarchitecture of some commonly used dataflow modules.\n\
    \n# 3.2 Sparsify the Algorithm: Submanifold Sparse Convolution\n\nThe input of\
    \ a DNN model for event-based vision tasks is typically a spatially sparse 2D\
    \ feature/representation generated by a certain amount of events. Although input\
    \ features can be highly sparse (e.g. <%1 in ASL-DVS dataset), using standard\
    \ convolution can easily leads to much denser intermediate features because of\
    \ the \"dilation\" effect of convolution. Figure [3](#page-3-1) shows the example\
    \ of sparse input features convoluted with a 3 \xD7 3 kernel. The output feature\
    \ of standard convolution is much denser than the input feature, which can diminish\
    \ the opportunities brought by the sparsity.\n\nTo address this, ESDA incorporates\
    \ submanifold sparse convolution [\\[23\\]](#page-10-3) to better enhance the\
    \ spatial sparsity of a model. Figure [3](#page-3-1) illustrates the operation\
    \ of submanifold sparse convolution. When the convolution stride is 1, the submanifold\
    \ sparse convolution\n\nenforces input and output non-zero locations to be identical.\
    \ This means that for positions that are originally zeros, e.g. \"x\" in the Figure\
    \ [3.](#page-3-1)a, the output remains zeros. While for those non-zeros output\
    \ locations, e.g. \"y\" in the figure, the feature is calculated by using the\
    \ similar weighted sum algorithm as standard convolution. In this way, submanifold\
    \ convolution prevents the \"dilation\" effect of standard convolution by restricting\
    \ the possible non-zero output locations.\n\nWhen convolution stride is larger\
    \ than 1, an output location of sparse convolution is non-zero if the corresponding\
    \ input \xD7 stride grid contains any non-zeros. For instance, the \"x\" in Figure\
    \ [3.](#page-3-1)b remains zero because its corresponding 2\xD72 grid of the inputs\
    \ are all zeros. This is in contrast to standard convolution which generates non-zero\
    \ outputs whenever the convolution kernel meets any nonzeros. Thus, sparser output\
    \ features can be expected by using sparse convolution if the stride size is smaller\
    \ than the kernel size, which is a common configuration.\n\nSubmanifold sparse\
    \ convolution also comes with a simpler hardware implementation. For example,\
    \ in the stride-1 case where the input-output sparse locations are the same, the\
    \ input tokens can simply be queued in a FIFO and reused for the output. Consequently,\
    \ the abstract module function can be written as:\n\n$$\\begin{aligned} \\text{token}\\\
    _{\\text{out}}[i] &= \\text{token}\\_{\\text{in}}[i] \\\\ \\text{feature}\\_{\\\
    text{out}}[i] &= f\\_F(\\text{feature}\\_{\\text{in}}[i], \\dots, \\text{feature}\\\
    _{\\text{in}}[i-N]) \\end{aligned} \\tag{2}$$\n\nIn this way, the hardware can\
    \ co-iterate input/output tokens and features, while saving the additional logic\
    \ to generate potential output locations during runtime.\n\n# <span id=\"page-3-0\"\
    ></span>3.3 Sparsify the Architecture: Composable Sparse Dataflow Modules\n\n\
    <span id=\"page-3-2\"></span>![](_page_3_Figure_16.jpeg)\n\nFigure 4: Convolution\
    \ 1 \xD7 1 Module.\n\n3.3.1 1 \xD7 1 Convolution Module. 1 \xD7 1 convolution,\
    \ also known as point-wise convolution, only operates locally by iterating each\
    \ sparse location and updating its features. Benefiting from this property, the\
    \ input and output tokens can simply be relayed using a token register. Figure\
    \ [4](#page-3-2) shows the hardware diagram of the 1 \xD7 1 convolution module.\
    \ The feature vector at the current location [., .] is first read into an input\
    \ feature buffer and then multiplied with the weight matrix in the static buffer.\
    \ Since the convolution layers are spatially mapped on the accelerator, the weights\
    \ will be synthesized into Read-Only Memory using Block RAM (BRAM) and initialized\
    \ during bitstream download. The computation of the output feature vector can\
    \ be implemented as a Matrix-Vector multiplication unit as illustrated in Figure\
    \ [4.](#page-3-2) The size of the PE array, which can also be regarded as the\
    \ parallel factor (PF) on channel\n\ndimension, is exposed as a hardware template\
    \ parameter. These parallel factors can also affect the BRAM usage by partitioning\
    \ the weights buffer to match the bandwidth requirement for the MAC array. Note\
    \ that the batch normalization layer can be folded into the convolution layer,\
    \ and the activation function is omitted from the diagram for simplicity.\n\n\
    <span id=\"page-4-0\"></span>![](_page_4_Figure_3.jpeg)\n\nFigure 5: Convolution\
    \ 3 \xD7 3 Module.\n\n3.3.2 \xD7 Convolution Module ( > 1). Different from 1 \xD7\
    \ 1 convolution that directly operates on non-zero positions locally, when the\
    \ convolution kernel size is larger than 1, the computation is the weighted summation\
    \ of neighboring features. In ESDA, \xD7 convolution is implemented by cascading\
    \ a Sparse Line Buffer module and a \xD7 convolution computation module as shown\
    \ in Figure [5.](#page-4-0) The Sparse Line Buffer designs can be slightly different\
    \ for different convolution strides (usually 1 or 2), while the computation module\
    \ remains the same. As shown in Figure [3,](#page-3-1) the weighted sum of spatial\
    \ kernels can also be sparse (e.g., \"y\" only requires \u2217 4 + \u2217 7).\
    \ To leverage this type of sparsity, an additional kernel offset stream is used\
    \ to denote the sparse neighboring offset within the \xD7 kernel.\n\n<span id=\"\
    page-4-1\"></span>![](_page_4_Figure_6.jpeg)\n\nFigure 6: Depthwise Convolution\
    \ 3 \xD7 3 Computation Module.\n\n3.3.3 \xD7 Convolution Computation Module (\
    \ > 1). Figure [6](#page-4-1) shows the diagram of the 3 \xD7 3 depthwise convolution\
    \ module, which is widely used in many mobile networks [\\[43,](#page-11-9) [45,](#page-11-10)\
    \ [46\\]](#page-11-11). For each non-zero output location marked by the current\
    \ token register, the weighted sum of sparse neighboring features is controlled\
    \ by the additional kernel offset stream. The kernel offset marks the relative\
    \ spatial locations of the current input features in the computation window, which\
    \ also serves as the index of the weights buffer. After performing the weighted\
    \ sum for all non-zero kernel offsets, the final results will be streamed out\
    \ as the output features. Other types of convolution, such as full convolution,\
    \ only differ slightly in the PE organization, while having identical token interfaces\
    \ and sparse weighted sum operations. Like the 1 \xD7 1 convolution module, the\
    \ hardware template also incorporates the channel dimension parallel factor as\
    \ a configurable variable.\n\n<span id=\"page-4-2\"></span>![](_page_4_Figure_9.jpeg)\n\
    \nFigure 7: Sparse Line Buffer Module (stride=1).\n\n3.3.4 Sparse Line Buffer\
    \ with Stride Equal to 1. The Sparse Line Buffer (SLB) mainly comprises a rows\
    \ data buffer ( being the kernel size), a token FIFO, and a small bitmap to record\
    \ current nonzero spatial locations in the buffer. When stride = 1, submanifold\
    \ convolution enforces the input and output non-zero locations to be identical.\
    \ As a result, a simple token FIFO is used to set up a queue for the output tokens.\
    \ Due to the deterministic streaming order of the sparse sliding window algorithm\
    \ (left-to-right and top-to-bottom as discussed in Eqn. [1\\)](#page-2-2), we\
    \ can simply use the token FIFO's head and tail tokens to control the read and\
    \ write operations of SLB.\n\nSpecifically, the FIFO's head token should represent\
    \ the next spatial location for weighted sum computation, while the FIFO's tail\
    \ token indicates the most recently received input location. The offset between\
    \ the head and tail token can be used to determine whether the buffer already\
    \ contains all the data required for the next output (center at head token). If\
    \ the current head token is \u210E, the required input window would be [\u210E\
    . \u2212, \u210E. +] \xD7 [\u210E.\u2212, \u210E.+], where = ( \u22121)/2 is the\
    \ kernel radius. Since our dataflow adheres to the top-to-bottom, left-to-right\
    \ ordering, when the tail token points to a location with ravel order larger than\
    \ [\u210E. + , \u210E. + ], it signifies that all the required data are already\
    \ in the buffer. The head token is valid to be sent out, and the corresponding\
    \ features are streamed for computation.\n\nIn terms of the ready signal, new\
    \ tokens and features are only allowed to be written into the SLB if the input\
    \ still lies in the currently buffered rows/region. When the new input is beyond\
    \ the buffered rows, SLB will first output the buffered tokens before accepting\
    \ new inputs. This approach is inherently free from deadlock as the arrival of\
    \ a new token from subsequent rows indicates that tokens from preceding rows (\u2264\
    \ . \u2212 \u2212 1) are all valid for output. This is because the ravel order\
    \ of should be larger than [\u210E. + , \u210E. + ] when \u210E. \u2264 . \u2212\
    \ \u2212 1. Those valid tokens will be popped out one by one, and buffers will\
    \ be cleaned up to allow new input data to be buffered. In summary, the control\
    \ signals ready and valid can then be written as:\n\n<span id=\"page-4-3\"></span>\n\
    $$\\begin{aligned} \\sigma &= \\left( (t.x - h.x \\ge u) \\land (t.y - h.y \\\
    ge u) \\right) \\lor (t.y - h.y \\ge u + 1) \\\\ r &= (t.y - h.y \\le u) \\\\\
    \ u &= (k - 1) / 2 \\end{aligned} \\tag{3}$$\n\nwhere \u210E, stands for head\
    \ and tail tokens. is the radius of the kernel. refers to kernel size.\n\nAs mentioned\
    \ before, the kernel offset stream is used to denote each non-zero location within\
    \ the \xD7 window. This information is obtained by querying a bitmap that marks\
    \ whether a spatial location in the buffer is currently non-zero or not. Figure\
    \ [7](#page-4-2) shows an example diagram of an SLB for 3 \xD7 3 convolution with\
    \ stride equal to 1. The snapshot shows the kernel window center at the head token,\
    \ where the features on kernel offset 2, 4, 6 are non-zeros. In this case, the\
    \ features at 2, 4, 6 will be outputted along with the kernel offsets for downstream\
    \ weighted sum computation.\n\n<span id=\"page-5-1\"></span>![](_page_5_Figure_2.jpeg)\n\
    \nFigure 8: Sparse Line Buffer Module (stride=2).\n\n3.3.5 Sparse Line Buffer\
    \ with Stride Equal to 2. When using the stride = 2 in a convolution layer, the\
    \ output feature map will be downsampled. As illustrated in Figure [3b](#page-3-1),\
    \ the sparse convolution will recalculate the non-zero coordinates (tokens). For\
    \ each 2 \xD7 2 input stride grid, if it contains any non-zero pixels, it will\
    \ generate a valid non-zero output. In this case, the Sparse Line Buffer module\
    \ with stride equal to 2 (SLB s2) adopts two token FIFOs to buffer tokens from\
    \ even and odd rows, respectively. A Token Merge unit is positioned at the head\
    \ of the two FIFOs to calculate the new output tokens, which can be written as:\n\
    \n<span id=\"page-5-0\"></span>\n$$\\begin{aligned} h\\_e e2 &= \\{h\\_e.x/2,\
    \ \\ h\\_e.y/2, \\ h\\_e.e.end\\} \\\\ h\\_o o2 &= \\{h\\_o.x/2, \\ h\\_o.y/2,\
    \ \\ h\\_o.e.end\\} \\\\ \\text{token}\\_{\\text{out}} &= (ravel(h\\_e2) > ravel(h\\\
    _o2)) \\text{ ? } h\\_o o2 : h\\_e e2 \\end{aligned} \\tag{4}$$\n\nwhere the \u210E\
    \\_ and \u210E\\_ are the head tokens from odd and even token FIFOs, respectively.\
    \ When a new output token is generated, the tokens in the corresponding 2 \xD7\
    \ 2 grid will all be popped out from FIFOs.\n\nSimilar to the case with stride\
    \ being 1, the tail token can help determine whether the new output token already\
    \ sees sufficient data in the buffer. The control logic is similar to Eqn. [3,](#page-4-3)\
    \ with the head token \u210E being the tokenout in Eqn. [4.](#page-5-0) Figure\
    \ [8](#page-5-1) demonstrated an example diagram of an SLB s2 module for 3 \xD7\
    \ 3 convolution, where the dark blue and green positions within the 2 \xD7 2 stride\
    \ grid signify a valid output. The token merge units will pop the dark blue and\
    \ green tokens from both token FIFOs and stream out the new token. The SLB s2\
    \ module then outputs the corresponding features and kernel offsets (1, 4, 8)\
    \ for downstream computation.\n\nIn typical Deep Neural Network (DNN) model designs,\
    \ the kernel stride values are commonly set to either 1 or 2. If using an even\
    \ larger stride size ( > 2), the SLB can be designed similarly by splitting number\
    \ of token FIFO channels.\n\n3.3.6 Pooling and FC Module. For the object classification\
    \ task, a model usually uses a global pooling layer followed by a fully connected\
    \ layer to generate predictions. In the sparse dataflow of ESDA, the global pooling\
    \ layer will iterate through all the non-zero\n\n<span id=\"page-5-2\"></span>![](_page_5_Figure_11.jpeg)\n\
    \nFigure 9: Pooling and FC Module.\n\ntokens until it encounters a token with\
    \ the . = flag. The features are aggregated into a single vector based on the\
    \ pooling algorithm (maximum or average). Finally, the pooling results are fed\
    \ into a linear classifier layer to compute the final prediction. Figure [9](#page-5-2)\
    \ shows an example diagram of a global average pooling layer combined with a fully\
    \ connected classifier.\n\n<span id=\"page-5-3\"></span>![](_page_5_Figure_14.jpeg)\n\
    \nFigure 10: Chaining a Residual Block.\n\n3.3.7 Chaining Modules into a Block.\
    \ Many DNN models are built upon convolution blocks with residual connections\
    \ [\\[24\\]](#page-10-25). Thanks to the autonomous sparse iteration capabilities\
    \ of all the modules in ESDA, building convolution blocks by chaining the dataflow\
    \ modules is straightforward. Figure [10](#page-5-3) shows an example of building\
    \ an inverted residual block MBConv with stride being 1. MBConv is commonly employed\
    \ in efficient models like MobileNetV2 [\\[43\\]](#page-11-9), EfficientNet [\\\
    [46\\]](#page-11-11), MnasNet [\\[45\\]](#page-11-10), etc. Although ESDA mainly\
    \ uses MBConv blocks in the following experiments, similar approaches can be applied\
    \ to other models like ResNet [\\[24\\]](#page-10-25), ShuffleNet [\\[48\\]](#page-11-12),\
    \ etc. Figure [10](#page-5-3) illustrates a convolution block incorporating an\
    \ identity shortcut, with all layer strides set to 1. Leveraging the principles\
    \ of submanifold convolution, the input and output tokens remain identical across\
    \ all the layers within this block. Thus, the input feature stream can simply\
    \ be forked into two separate streams. One stream goes to the residual functional\
    \ layers, while the other is used for the identity shortcut. The features in the\
    \ shortcut branch are buffered in a FIFO and will be merged at the output of the\
    \ last 1 \xD7 1 convolution layer.\n\n# 3.4 Customize Accelerator with Algorithm/Hardware\
    \ Co-optimization\n\nThe highly parameterized and composable sparse dataflow modules\
    \ in ESDA empower us to customize all-on-chip accelerators\n\nfor different models\
    \ or applications by spatially mapping distinct network components on FPGA. Moreover,\
    \ this flexibility allows us to trade off between accuracy, resource consumption,\
    \ and performance. However, it's essential to recognize that the design space\
    \ can be vast for this problem. To address this challenge, ESDA incorporates an\
    \ algorithm/hardware co-optimization flow to automate this process.\n\n3.4.1 Sparsity-awared\
    \ Hardware Optimization. In a dataflow accelerator, the throughput is usually\
    \ bounded by the slowest module in the pipeline. Thus, finding a load-balanced\
    \ hardware resource configuration for different modules is critical. To approach\
    \ the optimal configuration, ESDA adopts a static optimization strategy that hinges\
    \ on application-wise sparsity statistics. More precisely, for each application/dataset,\
    \ we collect the spatial and kernel sparsity ratios ( and ) from all the samples\
    \ in the dataset. The degree of spatial sparsity within the feature activation\
    \ reflects the total number of iterations in each dataflow module, while the kernel\
    \ sparsity indicates the averaged number of weighted sum operations occurring\
    \ within convolution layers. Using the sparsity statistics, we can estimate the\
    \ average latency and resource utilization for each dataflow module. Take the\
    \ depthwise 3 \xD7 3 convolution as an example. The latency, the number of BRAM,\
    \ and the DSP consumption can be estimated as:\n\n$$\\begin{aligned} lat &= (H\
    \ \\times W \\times \\text{S}\\_{\\text{s}}) \\times (9 \\times \\text{S}\\_{\\\
    text{k}}) \\times (C/PF) \\\\ bram &= \\lceil (B \\times 9 \\times C) / 16 \\\
    text{K} / PF \\rceil \\times PF \\\\ ds \\, p &= PF \\end{aligned} \\tag{5}$$\n\
    \nwhere , is the input height and width in the current layer, is the number of\
    \ channels, and is the channel parallel factor. is the bitwidth of the quantized\
    \ weight and we assume one BRAM can store 16 Kb data. The ( \xD7 \xD7 ) term reflects\
    \ the averaged number of iterations on the spatial dimension of this layer and\
    \ (9 \xD7 ) \xD7 (/ ) estimates the averaged cycles needed for computing an output.\
    \ The constant weight buffer is also partitioned to satisfy the on-chip bandwidth\
    \ requirement determined by the parallelism factor .\n\nUsing the similar approaches,\
    \ we can formulate the latency and resource utilization for the remaining modules.\
    \ Since all the layers are pipelined, we can estimate the theoretical throughput\
    \ by determining the maximum latency across all layers. Thus, the overall hardware\
    \ optimization problem can be written as:\n\n<span id=\"page-6-2\"></span>\n$$\\\
    begin{aligned} \\text{min} & \\quad \\text{lat} \\\\ \\text{s.t.} & \\quad \\\
    text{lat}\\_{\\bar{t}} \\le \\text{lat}, \\quad \\text{i in all layers} \\\\ &\
    \ \\sum\\_{\\bar{t}} r\\_{\\bar{t}\\bar{f}} \\le R\\_{\\bar{f}}, \\quad \\text{i\
    \ in all layers, j in [BRAM, DSP]} \\end{aligned} \\tag{6}$$\n\nwhere means the\
    \ type resource consumed for layer . is the total resource of each kind available.\
    \ The current framework mainly concerns BRAM and DSP resources on FPGA, and the\
    \ proposed optimization program will be solved by an open-sourced mixed integer\
    \ programming solver.\n\n3.4.2 Searching for Efficient Models. For an all-on-chip\
    \ accelerator, the weights of all the layers should be placed on-chip using BRAM.\
    \ In application scenarios with tight hardware resource constraints, deploying\
    \ larger models like ResNet [\\[24\\]](#page-10-25) might be infeasible. Many\n\
    \n<span id=\"page-6-1\"></span>![](_page_6_Figure_10.jpeg)\n\nFigure 11: Optimization\
    \ flow of ESDA.\n\nprevious research works have explored integrating Neural Architecture\
    \ Search into the deployment process to facilitate the model performances while\
    \ satisfying the hardware constraints.\n\nInspired by these concepts, we develop\
    \ a simple two-step greedy search algorithm to identify efficient models for deployment.\
    \ Initially, our software randomly samples a pool of model architectures built\
    \ upon the MBConv convolution block. The search space encompasses variations in:\
    \ (1) the number of blocks; (2) the stride of the block; (3) the number of channels\
    \ in each layer. The models are sampled within a coarse-defined model size range\
    \ according to the available on-chip buffer size. Also, the total down-sampling\
    \ ratio of each model remains fixed. These sampled models, along with the sparsity\
    \ statistic, are fed into the hardware optimizer, which searches for the optimal\
    \ hardware mapping of each model and estimates its performance. Running the hardware\
    \ optimizer for each model typically takes only several seconds/minutes to complete.\
    \ This means that for each dataset, we can effortlessly sample hundreds of models.\n\
    \nOnce all the hardware performance profiles for all the models are collected,\
    \ the software selects the top-k highest throughput models for training and finally\
    \ chooses the model with the best accuracy. This approach allows us to identify\
    \ models that fit the application constraints while balancing between accuracy\
    \ and hardware performance. Note that the current simple model search algorithm\
    \ does not leverage any sophisticated reinforcement learning or other meta-learning\
    \ algorithms. Nevertheless, the possibility of integrating other Neural Architecture\
    \ Search algorithms presents an exciting direction for future exploration. Figure\
    \ [11](#page-6-1) provides a summary of the co-optimization process in ESDA.\n\
    \n### <span id=\"page-6-0\"></span>4 EVALUATIONS\n\n### 4.1 Experiment Setup\n\
    \nWe evaluated ESDA on five different event-based gestures or object recognition\
    \ datasets: DvsGesture [\\[2\\]](#page-10-6), RoShamBo17 [\\[35\\]](#page-11-13),\
    \ ASL-DVS [\\[5\\]](#page-10-26), N-MNIST and N-Caltech101 [\\[41\\]](#page-11-14).\
    \ DvsGesture contains 10 classes of human body gestures captured by DVS128 camera\
    \ and RoShamBo17 records hand gestures playing rock-scissors-paper games. ASL-DVS\
    \ is a large-scale event-based American sign language letters recognition dataset\
    \ recorded by DAVIS240C event camera. N-Caltech101 and N-MNIST datasets slightly\
    \ differ from previous datasets that were directly shot in real scenes. Instead,\n\
    \n<span id=\"page-7-0\"></span>![](_page_7_Figure_2.jpeg)\n\nFigure 12: Compare\
    \ the spatial sparsity between standard and submanifold convolution on different\
    \ event-based vision datasets. The x-axis of each subplot is the spatial resolution\
    \ of the feature activation. Accuracy results of two different models in floating\
    \ point precision are also written in the legends.\n\nboth are recaptured from\
    \ static image-based Caltech101 [\\[12\\]](#page-10-27) and MNIST [\\[27\\]](#page-10-28)\
    \ datasets by using an actuated pan-tilt event-camera platform. All the datasets\
    \ are preprocessed with a similar method that first clips event recordings with\
    \ a fixed time interval and then generates histogram representation with two channels\
    \ (positive and negative events). Except for the RoShamBo17 dataset, we directly\
    \ use their preprocessed histogram format.\n\nFor hardware experiments, we implement\
    \ ESDA on an embedded FPGA board ZCU102 with a Zynq UltraScale+ MPSoC device (XCZU9EG)\
    \ using Vitis HLS and Vivado Design Flow. End-to-end hardware performances are\
    \ measured with on-board testing after synthesis, place, and route. Models are\
    \ trained using PyTorch and MinkowskiEngine [\\[8\\]](#page-10-29) sparse DNN\
    \ library. Weights and activations are quantized into 8 bits by using HAWQ-V3\
    \ [\\[47\\]](#page-11-15) framework in our training pipeline. The hardware optimization\
    \ program in Eqn. [6](#page-6-2) is solved by a mixed-integer geometric programming\
    \ solver developed in AGNA [\\[11\\]](#page-10-30) with SCIP [\\[4\\]](#page-10-31)\
    \ and GPkit [\\[6\\]](#page-10-32) optimization frameworks.\n\n### 4.2 Performance\
    \ of Submanifold Convolution\n\nFigure [12](#page-7-0) shows the spatial statistics\
    \ from different datasets. The sparsity ratios are measured on different feature\
    \ resolutions throughout the network. For example, the N-MNIST dataset has the\
    \ input resolution being 34 \xD7 34. Through the downsample stages of the network,\
    \ the resolution will change to 17\xD717, 8\xD78, and finally 4\xD74. We can see\
    \ that submanifold sparse convolution can significantly improve sparsity compared\
    \ to standard convolution. Even though all the input data are very sparse (1.1%\
    \ to 23.1%), the dilation effect (shown in Figure [3\\)](#page-3-1) of standard\
    \ convolution can easily lead to very dense features throughout the network. In\
    \ ASL-DVS dataset, using submanifold sparse convolution can lead to as much as\
    \ 3.4\xD7 sparser features than standard convolution.\n\nTo compare the accuracy\
    \ performances of two different types of convolution, we trained MobileNetV2 models\
    \ with both convolution methods on N-Caltech101, DvsGesture, and ASL-DVS. Since\
    \ N-MNIST and RoShamBo17 have smaller input resolutions, we use a customized network\
    \ architecture instead. The accuracy results (written in the legends of Figure\
    \ [12\\)](#page-7-0) show that for most datasets, submanifold sparse convolution\
    \ achieves similar or higher accuracy compared with standard convolution. In summary,\
    \ submanifold sparse convolution showcases its ability to effectively leverage\n\
    \nthe intrinsic spatial sparsity present in event-based datasets while having\
    \ similar accuracy performances compared with standard convolution, making it\
    \ a highly promising candidate for hardware acceleration.\n\n### 4.3 Compare ESDA\
    \ with Dense Dataflow\n\nTo demonstrate the effectiveness of the sparse dataflow\
    \ modules, we individually synthesized all the MBConv convolution blocks of the\
    \ MobileNetV2 network and evaluated their performance across various input sparsity\
    \ ratios. The hardware configuration of each block aligns with the overall optimization\
    \ result of the entire MobileNetV2 architecture. To evaluate the overhead for\
    \ supporting dynamic sparse control logic, we implement a dense baseline with\
    \ a similar sliding-window-based dataflow that uses a standard line buffer design.\
    \ The dense baseline eliminates all token stream interfaces and dynamic logic\
    \ components, maintaining identical parallel factors, bitwidths, and other design\
    \ parameters in alignment with the sparse implementations.\n\nFigure [13](#page-8-0)\
    \ presents the speedup ratio of ESDA modules over the dense baseline using C/RTL\
    \ co-simulation latency. The inputs consist of randomly generated data with varying\
    \ spatial sparsity ratios, ranging from 10% to 90%. The results demonstrate that\
    \ dynamic sparse dataflow architecture can effectively leverage sparsity in general.\
    \ With an input non-zero ratio of 10%, the sparse designs achieve significant\
    \ speedups of 4.5 \u2212 11\xD7 compared to the dense baselines. Moreover, the\
    \ speedup shows a nearly linear increase as sparsity grows.\n\nHowever, it is\
    \ worth noting that the overhead of supporting dynamic sparse computation does\
    \ exist. Specifically, when the input is nearly dense (greater than 70% NZ), certain\
    \ blocks like blk\\_0 to blk\\_5 exhibit slower performance than the dense baseline.\
    \ These blocks typically have large spatial resolutions and smaller channels,\
    \ making them more susceptible to overhead. Fortunately, it turns out that these\
    \ larger resolution layers in the early stages of a network typically have higher\
    \ sparsity (usually below 40% NZ) as shown in Figure [12.](#page-7-0) As a result,\
    \ despite potential slowdowns with very dense input, these modules can still attain\
    \ notable speedup with highly sparse inputs.\n\n<span id=\"page-8-0\"></span>![](_page_8_Figure_2.jpeg)\n\
    \nFigure 13: Speedup of ESDA sparse dataflow architecture over sliding-window-based\
    \ dense dataflow architecture with different input sparsity ratios (randomly generated).\
    \ The sparse and dense designs use identical hardware configurations like Parallel\
    \ Factor, Bitwidth, etc. blk\\_x refers to different convolution blocks in MobileNetV2.\n\
    \n<span id=\"page-8-1\"></span>![](_page_8_Figure_4.jpeg)\n\nFigure 14: Compare\
    \ ESDA with an embedded GPU platform (NVIDIA Jetson Xavier NX). For GPU performances,\
    \ we tested both dense DNN implementation using PyTorch (baseline) and submanifold\
    \ sparse DNN implementation using MinkowskiEngine. And we compared both MobileNetV2\
    \ and the customized models searched by ESDA.\n\n### 4.4 Compare ESDA with Embedded\
    \ GPU\n\nFigure [14](#page-8-1) shows the comparison between ESDA and an embedded\
    \ GPU platform (NVIDIA Jetson Xavier NX) on three event-based datasets: N-Caltech101,\
    \ DvsGesture, and ASL-DVS. For GPU implementation, we tested the performance of\
    \ (1) dense DNN implementation using PyTorch and (2) submanifold sparse DNN implementation\
    \ using MinkowskiEngine [\\[8\\]](#page-10-29) library. In addition, we evaluated\
    \ the MobileNetV2 (with width-multiplier being 0.5) as well as the customized\
    \ ESDA-Net (the models searched by using ESDA optimization flow for each dataset).\
    \ The latency and throughput performances of GPU use batch size 1 and 128 respectively.\n\
    \nIn terms of latency performance, ESDA exhibits a substantial speedup over both\
    \ dense and sparse GPU implementations for all the model-dataset combinations.\
    \ In comparison to the dense GPU baseline, ESDA delivers 3.3 \u2212 23.0\xD7 speedup\
    \ on MobileNetV2 and 9.4 \u2212 54.8\xD7 speedup on customized models. An interesting\
    \ observation is that the latency performance of sparse GPU implementation lags\
    \ behind the dense GPU baseline. This discrepancy primarily comes from the way\
    \ that GPUs handle sparsity in submanifold convolutions. It stores the sparse\
    \ input coordinates in a hash table and breaks the original computation into a\
    \ fine-grained sparse gatherscatter flow for each spatial kernel (marked as k0-k8\
    \ in Figure [3\\)](#page-3-1). In scenarios with relatively small batch sizes,\
    \ this additional overhead can have an adverse impact on performance.\n\nAs for\
    \ throughput performances, ESDA surpasses the dense GPU baseline in most cases,\
    \ with the exception being MobileNetV2 on N-Caltech101. The smaller speedup ratio\
    \ is attributed to the fact that current ESDA is mainly optimized for batch-1\
    \ low-latency inference. This design choice is driven by its primary focus on\
    \ near-sensor or edge systems, where latency performance is vital. Nevertheless,\
    \ our customized models searched by the co-optimization flow also demonstrate\
    \ better speedup performances than MobileNetV2, which highlights the advantages\
    \ and effectiveness of the framework. Last but not least, ESDA demonstrates an\
    \ average of 5.8\xD7 and 3.3\xD7 improvement of energy efficiency (measured in\
    \ Joules per inference) when compared with GPU dense and sparse implementations,\
    \ respectively, across the benchmarks. This underscores the low-power capability\
    \ of our sparse architecture for event-based vision DNN inference.\n\n### 4.5\
    \ Compare ESDA with Other Related Works\n\nTable [1](#page-9-1) provides the detailed\
    \ hardware implementation results of ESDA and other related works on different\
    \ datasets. The hardware performances of ESDA were measured on a ZCU102 FPGA board\
    \ with the programmable logic (PL) running at 187 MHz. The power was measured\
    \ during runtime using the built-in power monitor controlled by the PMBus. For\
    \ N-Caltech101, DvsGesture, and ASL-DVS datasets, we implement two models: MobileNetV2\
    \ (with the width-multiplier being 0.5) and the customized models. Note that the\
    \ customized models are all named EDSA-Net in Table [1](#page-9-1) for brevity\
    \ but the model for each dataset is different.\n\nIn general, ESDA demonstrates\
    \ superior performance in terms of overall system efficiency and effectiveness.\
    \ Across all the datasets, ESDA achieves ultra-low latency (0.15 ms to 7.12 ms),\
    \ significantly boosting the real-time performance of event-based DNN inference.\n\
    \n<span id=\"page-9-1\"></span>\n\n|                | Dataset      | Resolution\
    \ | Modela      | Bit<br>width | Acc<br>(%) | Latency<br>(ms) | Throughput<br>(fps)\
    \ | Power<br>(W) | Energy<br>Efficiency<br>(mJ/inf) | Implementation<br>Detailsb\
    \     |      |      |      |\n|----------------|--------------|------------|-------------|--------------|------------|-----------------|---------------------|--------------|----------------------------------|--------------------------------|------|------|------|\n\
    |                |              |            |             |              |  \
    \          |                 |                     |              |          \
    \                        | DSP                            | BRAM | FF   | LUT\
    \  |\n| ESDA<br>(Ours) | N-Caltech101 | 180 \xD7 240  | ESDA-Net    | 8      \
    \      | 72.4       | 3.09            | 323                 | 1.81         | 5.61\
    \                             | 1792                           | 1278 | 115K |\
    \ 154K |\n|                |              |            | MobileNetV2 | 8     \
    \       | 71.6       | 7.12            | 140                 | 2.10         |\
    \ 14.96                            | 1992                           | 1600 | 198K\
    \ | 207K |\n|                | DvsGesture   | 128 \xD7 128  | ESDA-Net    | 8\
    \            | 92.5       | 0.66            | 1526                | 1.58     \
    \    | 1.03                             | 1532                           | 848\
    \  | 97K  | 128K |\n|                |              |            | MobileNetV2\
    \ | 8            | 93.9       | 1.19            | 839                 | 1.73 \
    \        | 2.06                             | 1636                           |\
    \ 1134 | 104K | 140K |\n|                | ASL-DVS      | 180 \xD7 240  | ESDA-Net\
    \    | 8            | 99.5       | 0.71            | 1406                | 1.60\
    \         | 1.14                             | 1494                          \
    \ | 917  | 97K  | 131K |\n|                |              |            | MobileNetV2\
    \ | 8            | 99.3       | 1.08            | 927                 | 1.75 \
    \        | 1.88                             | 1416                           |\
    \ 1069 | 108K | 144K |\n|                | N-MNIST      | 34 \xD7 34    | ESDA-Net\
    \    | 8            | 98.9       | 0.15            | 6657                | 1.55\
    \         | 0.23                             | 1525                          \
    \ | 978  | 93K  | 121K |\n|                | RoShamBo17   | 64 \xD7 64    | ESDA-Net\
    \    | 8            | 99.6       | 0.98            | 1016                | 1.40\
    \         | 1.38                             | 1282                          \
    \ | 765  | 72K  | 95K  |\n| NullHop        | RoShamBo17   | 64 \xD7 64    | RoshamboNet\
    \ | 16           | 99.3       | 10              | 160                 | 0.27 \
    \        | 1.69                             | 657                            |\
    \ 802  | 139K | 266K |\n| PPF            | -            | 60 \xD7 40    | PFF-BNN\
    \     | 1            | 87         | 7.71            | -                   | -\
    \            | -                                | 12                         \
    \    | 223  | 16K  | 42K  |\n| Asynet         | N-Caltech101 | 180 \xD7 240  |\
    \ VGG         | FP32         | 74.5       | 80.4            | -              \
    \     | -            | -                                | CPU                \
    \            |      |      |      |\n| TrueNorth      | DvsGesture   | 64 \xD7\
    \ 64    | SNN         | Tenary       | 94.6       | 105             | -      \
    \             | 0.18         | 18.7                             | Samsung 28 nm\
    \ LPP CMOS process |      |      |      |\n| Loihi          | DvsGesture   | 32\
    \ \xD7 32    | SNN         | 9            | 90.5       | 11.43           | - \
    \                  | -            | -                                | Intel 14\
    \ nm process            |      |      |      |\n\n#### Table 1: System Performance\n\
    \n<sup>a</sup> ESDA-Net refers to the customized network for each dataset. <sup>b</sup>\
    \ If written in 4 columns, the designs are implemented on FPGA.\n\nAs mentioned\
    \ before, the current implementation of ESDA only supports batch size 1 inference,\
    \ and the throughput is calculated using 1/latency. Nevertheless, ESDA still achieves\
    \ over 1000 fps throughput for most datasets except N-Caltech101. The performance\
    \ of N-Caltech101 dataset is slightly lower than the others because it has a larger\
    \ spatial resolution and denser input. On the other hand, ESDA also showcases\
    \ notable low power and high energy efficiency features. The power consumption\
    \ of the FPGA fabric ranges from 1.4 W to 2.1 W across different datasets, with\
    \ the average energy per inference spanning from 0.23 mJ to 14.96 mJ. In summary,\
    \ ESDA provides a promising solution for low-power DNN acceleration on edge event-based\
    \ vision systems.\n\nESDA also demonstrates better system performances compared\
    \ to other prior FPGA accelerators, NullHop [\\[1,](#page-10-1) [31\\]](#page-10-2)\
    \ and PPF [\\[40\\]](#page-11-8). Both are FPGA DNN accelerators that have been\
    \ applied to event-based vision tasks. On the RoShamBo17 dataset, ESDA achieves\
    \ 10.2\xD7 speedup in latency and 6.4\xD7 speedup in throughput while also having\
    \ better accuracy performance. In terms of power efficiency, although NullHop\
    \ exhibits lower power consumption, which may be attributed to its lower clock\
    \ frequency (60 MHz), ESDA outperforms it in terms of energy efficiency (1.38\
    \ mJ/inf vs 1.69 mJ/inf). PPF is a binary neural network FPGA accelerator for\
    \ event-based pedestrian detection. Since PPF did not release their datasets,\
    \ we can not compare with them directly. With similar input resolution on the\
    \ RoShamBo17 dataset, ESDA has significant latency speedup over PPF (0.98 ms vs\
    \ 7.71 ms).\n\nAsynet [\\[39\\]](#page-11-16) is an asynchronous event-based submanifold\
    \ sparse convolution network implemented on CPU. Despite the attractive asynchronous\
    \ feature of Asynet, the overhead of asynchronous computation can be high, which\
    \ involves updating the kernel maps and feature vectors incrementally for each\
    \ event. On the N-Caltech101 dataset, ESDA achieves 26\xD7 speedup compared with\
    \ Asynet with only around 2% lower accuracy. When implementing Asynet on customized\
    \ hardware, one can expect a significant amount of random memory access for fetching\
    \ and updating non-zero features. Otherwise, the accelerator shall buffer all\
    \ the intermediate features on-chip, which can easily overwhelm the available\
    \ resources.\n\nWe also compare ESDA with neuromorphic hardware solutions. TrueNorth\
    \ [\\[38\\]](#page-11-4) and Loihi [\\[10\\]](#page-10-9) are well-known SNN chips,\
    \ and they both have been used to accelerate event-based gesture recognition on\
    \ the DvsGesture dataset [\\[2,](#page-10-6) [37\\]](#page-11-17). In terms of\
    \ latency, ESDA achieves 160\xD7 and 17.4\xD7 speedup over TrueNorth and Loihi,\
    \ respectively. At the same time, ESDA also exhibits 18\xD7 better energy efficiency\
    \ compared with TrueNorth. In essence, ESDA attains rapid and energy-efficient\
    \ DNN acceleration for event-based vision without relying on traditional neuromorphic\
    \ techniques, which paves the way to future efficient event-based systems.\n\n\
    ### <span id=\"page-9-0\"></span>5 CONCLUSION\n\nIn this work, we have presented\
    \ ESDA, a novel composable dynamic sparse dataflow architecture for efficient\
    \ event-based vision DNN inference. ESDA confronts the challenge of low-latency\
    \ and low-power processing of event-based data by focusing on its inherent sparsity.\
    \ This is made possible by synergizing algorithmic advancements through submanifold\
    \ sparse convolution with novel architectural solutions realized through the sparse\
    \ dataflow design. The flexibility and programmability of the ESDA are also supercharged\
    \ by the highly modular dataflow design as well as the sparsity-aware design optimization\
    \ flow for FPGA platforms. Extensive experiments show the ESDA achieves up to\
    \ 54.8\xD7 speedup than the embedded GPU as well as 10.2\xD7 speedup compared\
    \ to NullHop. In addition, ESDA also shows 160\xD7 speedup and 17.4\xD7 better\
    \ energy efficiency compared to SNN hardware platform TrueNorth. The ability to\
    \ effectively exploit sparsity in ESDA opens up new possibilities for real-time,\
    \ energy-efficient DNN inference for event-based vision applications.\n\n### ACKNOWLEDGMENTS\n\
    \nThis work was supported in part by the Research Grants Council (RGC) of Hong\
    \ Kong under the Research Impact Fund project R7003- 21 and the Theme-based Research\
    \ Scheme (TRS) Project T45-701- 22-R. This work was supported by AI Chip Center\
    \ for Emerging Smart Systems (ACCESS), sponsored by InnoHK funding, Hong Kong\
    \ SAR.\n\n### REFERENCES\n\n- <span id=\"page-10-1\"></span>[1] Alessandro Aimar,\
    \ Hesham Mostafa, Enrico Calabrese, Antonio Rios-Navarro, Ricardo Tapiador-Morales,\
    \ Iulia-Alexandra Lungu, Moritz B. Milde, Federico Corradi, Alejandro Linares-Barranco,\
    \ Shih-Chii Liu, and Tobi Delbruck. 2019. NullHop: A Flexible Convolutional Neural\
    \ Network Accelerator Based on Sparse Representations of Feature Maps. IEEE Transactions\
    \ on Neural Networks and Learning Systems 30, 3 (2019), 644\u2013656. [https://doi.org/10.1109/TNNLS.2018.](https://doi.org/10.1109/TNNLS.2018.2852335)\
    \ [2852335](https://doi.org/10.1109/TNNLS.2018.2852335)\n- <span id=\"page-10-6\"\
    ></span>[2] Arnon Amir, Brian Taba, David Berg, Timothy Melano, Jeffrey McKinstry,\
    \ Carmelo Di Nolfo, Tapan Nayak, Alexander Andreopoulos, Guillaume Garreau, Marcela\
    \ Mendoza, et al. 2017. A low power, fully event-based gesture recognition system.\
    \ In Proceedings of the IEEE conference on computer vision and pattern recognition.\
    \ 7243\u20137252.\n- <span id=\"page-10-11\"></span>[3] Myo Tun Aung, Rodney Teo,\
    \ and Garrick Orchard. 2018. Event-based Plane-fitting Optical Flow for Dynamic\
    \ Vision Sensors in FPGA. In 2018 IEEE International Symposium on Circuits and\
    \ Systems (ISCAS). 1\u20135. [https://doi.org/10.1109/ISCAS.](https://doi.org/10.1109/ISCAS.2018.8351588)\
    \ [2018.8351588](https://doi.org/10.1109/ISCAS.2018.8351588)\n- <span id=\"page-10-31\"\
    ></span>[4] Ksenia Bestuzheva, Mathieu Besan\xE7on, Wei-Kun Chen, Antonia Chmiela,\
    \ Tim Donkiewicz, Jasper van Doornmalen, Leon Eifler, Oliver Gaul, Gerald Gamrath,\
    \ Ambros Gleixner, Leona Gottwald, Christoph Graczyk, Katrin Halbig, Alexander\
    \ Hoen, Christopher Hojny, Rolf van der Hulst, Thorsten Koch, Marco L\xFCbbecke,\
    \ Stephen J. Maher, Frederic Matter, Erik M\xFChmer, Benjamin M\xFCller, Marc\
    \ E. Pfetsch, Daniel Rehfeldt, Steffan Schlein, Franziska Schl\xF6sser, Felipe\
    \ Serrano, Yuji Shinano, Boro Sofranac, Mark Turner, Stefan Vigerske, Fabian Wegscheider,\
    \ Philipp Wellner, Dieter Weninger, and Jakob Witzig. 2021. The SCIP Optimization\
    \ Suite 8.0. Technical Report. Optimization Online. [http://www.optimization](http://www.optimization-online.org/DB_HTML/2021/12/8728.html)[online.org/DB\\\
    \\_HTML/2021/12/8728.html](http://www.optimization-online.org/DB_HTML/2021/12/8728.html)\n\
    - <span id=\"page-10-26\"></span>[5] Yin Bi, Aaron Chadha, Alhabib Abbas, Eirina\
    \ Bourtsoulatze, and Yiannis Andreopoulos. 2019. Graph-based object classification\
    \ for neuromorphic vision sensing. In Proceedings of the IEEE/CVF international\
    \ conference on computer vision. 491\u2013501.\n- <span id=\"page-10-32\"></span>[6]\
    \ Edward Burnell, Nicole B Damen, and Warren Hoburg. 2020. GPkit: A Human-Centered\
    \ Approach to Convex Optimization in Engineering Design. In Proceedings of the\
    \ 2020 CHI Conference on Human Factors in Computing Systems. <https://doi.org/10.1145/3313831.3376412>\n\
    - <span id=\"page-10-19\"></span>[7] Yu-Hsin Chen, Tien-Ju Yang, Joel Emer, and\
    \ Vivienne Sze. 2019. Eyeriss v2: A Flexible Accelerator for Emerging Deep Neural\
    \ Networks on Mobile Devices. IEEE Journal on Emerging and Selected Topics in\
    \ Circuits and Systems 9, 2 (2019), 292\u2013308.<https://doi.org/10.1109/JETCAS.2019.2910232>\n\
    - <span id=\"page-10-29\"></span>[8] Christopher Choy, JunYoung Gwak, and Silvio\
    \ Savarese. 2019. 4d spatio-temporal convnets: Minkowski convolutional neural\
    \ networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern\
    \ recognition. 3075\u20133084.\n- <span id=\"page-10-23\"></span>[9] Steven Colleman,\
    \ Thomas Verelst, Linyan Mei, Tinne Tuytelaars, and Marian Verhelst. 2021. Processor\
    \ Architecture Optimization for Spatially Dynamic Neural Networks. In 2021 IFIP/IEEE\
    \ 29th International Conference on Very Large Scale Integration (VLSI-SoC). 1\u2013\
    6.<https://doi.org/10.1109/VLSI-SoC53125.2021.9607013>\n- <span id=\"page-10-9\"\
    ></span>[10] Mike Davies, Narayan Srinivasa, Tsung-Han Lin, Gautham Chinya, Yongqiang\
    \ Cao, Sri Harsha Choday, Georgios Dimou, Prasad Joshi, Nabil Imam, Shweta Jain,\
    \ Yuyun Liao, Chit-Kwan Lin, Andrew Lines, Ruokun Liu, Deepak Mathaikutty, Steven\
    \ McCoy, Arnab Paul, Jonathan Tse, Guruguhanathan Venkataramanan, Yi-Hsin Weng,\
    \ Andreas Wild, Yoonseok Yang, and Hong Wang. 2018. Loihi: A Neuromorphic Manycore\
    \ Processor with On-Chip Learning. IEEE Micro 38, 1 (2018), 82\u201399.<https://doi.org/10.1109/MM.2018.112130359>\n\
    - <span id=\"page-10-30\"></span>[11] Yuhao Ding, Jiajun Wu, Yizhao Gao, Maolin\
    \ Wang, and Hayden Kwok-Hay So. 2023. Model-Platform Optimized Deep Neural Network\
    \ Accelerator Generation through Mixed-Integer Geometric Programming. In 2023\
    \ IEEE 31st Annual International Symposium on Field-Programmable Custom Computing\
    \ Machines (FCCM). 83\u201393.<https://doi.org/10.1109/FCCM57271.2023.00018>\n\
    - <span id=\"page-10-27\"></span>[12] Li Fei-Fei, R. Fergus, and P. Perona. 2004.\
    \ Learning Generative Visual Models from Few Training Examples: An Incremental\
    \ Bayesian Approach Tested on 101 Object Categories. In 2004 Conference on Computer\
    \ Vision and Pattern Recognition Workshop. 178\u2013178.<https://doi.org/10.1109/CVPR.2004.383>\n\
    - <span id=\"page-10-21\"></span>[13] Yu Feng, Gunnar Hammonds, Yiming Gan, and\
    \ Yuhao Zhu. 2022. Crescent: Taming Memory Irregularities for Accelerating Deep\
    \ Point Cloud Analytics. In Proceedings of the 49th Annual International Symposium\
    \ on Computer Architecture (New York, New York) (ISCA '22). Association for Computing\
    \ Machinery, New York, NY, USA, 962\u2013977.<https://doi.org/10.1145/3470496.3527395>\n\
    - <span id=\"page-10-5\"></span>[14] Thomas Finateu, Atsumi Niwa, Daniel Matolin,\
    \ Koya Tsuchimoto, Andrea Mascheroni, Etienne Reynaud, Pooria Mostafalu, Frederick\
    \ Brady, Ludovic Chotard, Florian LeGoff, Hirotsugu Takahashi, Hayato Wakabayashi,\
    \ Yusuke Oike, and Christoph Posch. 2020. 5.10 A 1280\xD7720 Back-Illuminated\
    \ Stacked Temporal Contrast Event-Based Vision Sensor with 4.86\xB5m Pixels, 1.066GEPS\
    \ Readout, Programmable Event-Rate Controller and Compressive Data-Formatting\
    \ Pipeline. In 2020 IEEE International Solid- State Circuits Conference - (ISSCC).\
    \ 112\u2013 114.<https://doi.org/10.1109/ISSCC19947.2020.9063149>\n- <span id=\"\
    page-10-10\"></span>[15] Steve B. Furber, Francesco Galluppi, Steve Temple, and\
    \ Luis A. Plana. 2014. The SpiNNaker Project. Proc. IEEE 102, 5 (2014), 652\u2013\
    665. [https://doi.org/10.1109/](https://doi.org/10.1109/JPROC.2014.2304638)\n\n\
    [JPROC.2014.2304638](https://doi.org/10.1109/JPROC.2014.2304638)\n\n- <span id=\"\
    page-10-0\"></span>[16] G. Gallego, T. Delbruck, G. Orchard, C. Bartolozzi, B.\
    \ Taba, A. Censi, S. Leutenegger, A. J. Davison, J. Conradt, K. Daniilidis, and\
    \ D. Scaramuzza. 2022. Event-Based Vision: A Survey. IEEE Transactions on Pattern\
    \ Analysis and Machine Intelligence 44, 01 (jan 2022), 154\u2013180.<https://doi.org/10.1109/TPAMI.2020.3008413>\n\
    - <span id=\"page-10-4\"></span>[17] Yizhao Gao, Yuhao Ding, Baoheng Zhang, and\
    \ Hayden Kwok Hay So. 2023. ESDA: A Composable Dynamic Sparse Dataflow Architecture\
    \ for Efficient Event-based Vision Processing on FPGA. (12 2023).<https://doi.org/10.25442/hku.24784221.v1>\n\
    - <span id=\"page-10-14\"></span>[18] Yizhao Gao, Song Wang, and Hayden Kwok-Hay\
    \ So. 2022. REMOT: A Hardware-Software Architecture for Attention-Guided Multi-Object\
    \ Tracking with Dynamic Vision Sensors on FPGAs. In Proceedings of the 2022 ACM/SIGDA\
    \ International Symposium on Field-Programmable Gate Arrays (Virtual Event, USA)\
    \ (FPGA '22). Association for Computing Machinery, New York, NY, USA, 158\u2013\
    168. [https:](https://doi.org/10.1145/3490422.3502365) [//doi.org/10.1145/3490422.3502365](https://doi.org/10.1145/3490422.3502365)\n\
    - <span id=\"page-10-15\"></span>[19] Yizhao Gao, Song Wang, and Hayden Kwok-Hay\
    \ So. 2023. A Reconfigurable Architecture for Real-Time Event-Based Multi-Object\
    \ Tracking. ACM Trans. Reconfigurable Technol. Syst. (apr 2023).<https://doi.org/10.1145/3593587>\
    \ Just Accepted.\n- <span id=\"page-10-24\"></span>[20] Yizhao Gao, Baoheng Zhang,\
    \ Xiaojuan Qi, and Hayden Kwok-Hay So. 2023. DPACS: Hardware Accelerated Dynamic\
    \ Neural Network Pruning through Algorithm-Architecture Co-Design. In Proceedings\
    \ of the 28th ACM International Conference on Architectural Support for Programming\
    \ Languages and Operating Systems, Volume 2 (Vancouver, BC, Canada) (ASPLOS 2023).\
    \ Association for Computing Machinery, New York, NY, USA, 237\u2013251. [https://doi.org/10.1145/3575693.](https://doi.org/10.1145/3575693.3575728)\
    \ [3575728](https://doi.org/10.1145/3575693.3575728)\n- <span id=\"page-10-8\"\
    ></span>[21] Daniel Gehrig, Antonio Loquercio, Konstantinos G Derpanis, and Davide\
    \ Scaramuzza. 2019. End-to-end learning of representations for asynchronous eventbased\
    \ data. In Proceedings of the IEEE/CVF International Conference on Computer Vision.\
    \ 5633\u20135643.\n- <span id=\"page-10-20\"></span>[22] Ashish Gondimalla, Noah\
    \ Chesnut, Mithuna Thottethodi, and T. N. Vijaykumar. 2019. SparTen: A Sparse\
    \ Tensor Accelerator for Convolutional Neural Networks. In Proceedings of the\
    \ 52nd Annual IEEE/ACM International Symposium on Microarchitecture (Columbus,\
    \ OH, USA) (MICRO '52). Association for Computing Machinery, New York, NY, USA,\
    \ 151\u2013165.<https://doi.org/10.1145/3352460.3358291>\n- <span id=\"page-10-3\"\
    ></span>[23] Benjamin Graham, Martin Engelcke, and Laurens Van Der Maaten. 2018.\
    \ 3d semantic segmentation with submanifold sparse convolutional networks. In\
    \ Proceedings of the IEEE conference on computer vision and pattern recognition.\
    \ 9224\u20139232.\n- <span id=\"page-10-25\"></span>[24] Kaiming He, Xiangyu Zhang,\
    \ Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recognition.\
    \ In Proceedings of the IEEE conference on computer vision and pattern recognition.\
    \ 770\u2013778.\n- <span id=\"page-10-18\"></span>[25] Michael Hofst\xE4tter,\
    \ Martin Litzenberger, Daniel Matolin, and Christoph Posch. 2011. Hardware-accelerated\
    \ address-event processing for high-speed visual object recognition. In 2011 18th\
    \ IEEE International Conference on Electronics, Circuits, and Systems. 89\u2013\
    92.<https://doi.org/10.1109/ICECS.2011.6122221>\n- <span id=\"page-10-7\"></span>[26]\
    \ Xavier Lagorce, Garrick Orchard, Francesco Galluppi, Bertram E. Shi, and Ryad\
    \ B. Benosman. 2017. HOTS: A Hierarchy of Event-Based Time-Surfaces for Pattern\
    \ Recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence\
    \ 39, 7 (2017), 1346\u20131359.<https://doi.org/10.1109/TPAMI.2016.2574707>\n\
    - <span id=\"page-10-28\"></span>[27] Yann LeCun, Corinna Cortes, and CJ Burges.\
    \ 2010. MNIST handwritten digit database. ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist\
    \ 2 (2010).\n- <span id=\"page-10-22\"></span>[28] Yujun Lin, Zhekai Zhang, Haotian\
    \ Tang, Hanrui Wang, and Song Han. 2021. PointAcc: Efficient Point Cloud Accelerator.\
    \ In MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture\
    \ (Virtual Event, Greece) (MICRO '21). Association for Computing Machinery, New\
    \ York, NY, USA, 449\u2013461. [https:](https://doi.org/10.1145/3466752.3480084)\
    \ [//doi.org/10.1145/3466752.3480084](https://doi.org/10.1145/3466752.3480084)\n\
    - <span id=\"page-10-16\"></span>[29] A. Linares-Barranco, F. G\xF3mez-Rodr\xED\
    guez, V. Villanueva, L. Longinotti, and T. Delbr\xFCck. 2015. A USB3.0 FPGA event-based\
    \ filtering and tracking framework for dynamic vision sensors. In 2015 IEEE International\
    \ Symposium on Circuits and Systems (ISCAS). 2417\u20132420.<https://doi.org/10.1109/ISCAS.2015.7169172>\n\
    - <span id=\"page-10-17\"></span>[30] Alejandro Linares-Barranco, Fernando Perez-Pe\xF1\
    a, Diederik Paul Moeys, Francisco Gomez-Rodriguez, Gabriel Jimenez-Moreno, Shih-Chii\
    \ Liu, and Tobi Delbruck. 2019. Low Latency Event-Based Filtering and Feature\
    \ Extraction for Dynamic Vision Sensors in Real-Time FPGA Applications. IEEE Access\
    \ 7 (2019), 134926\u2013134942.<https://doi.org/10.1109/ACCESS.2019.2941282>\n\
    - <span id=\"page-10-2\"></span>[31] Alejandro Linares-Barranco, Antonio Rios-Navarro,\
    \ Salvador Canas-Moreno, Enrique Pi\xF1ero Fuentes, Ricardo Tapiador-Morales,\
    \ and Tobi Delbruck. 2021. Dynamic Vision Sensor Integration on FPGA-Based CNN\
    \ Accelerators for High-Speed Visual Classification. In International Conference\
    \ on Neuromorphic Systems 2021 (Knoxville, TN, USA) (ICONS 2021). Association\
    \ for Computing Machinery, New York, NY, USA, Article 21, 7 pages.<https://doi.org/10.1145/3477145.3477167>\n\
    - <span id=\"page-10-12\"></span>[32] Min Liu and Tobi Delbruck. 2017. Block-matching\
    \ optical flow for dynamic vision sensors: Algorithm and FPGA implementation.\
    \ In 2017 IEEE International Symposium on Circuits and Systems (ISCAS). 1\u2013\
    4. [https://doi.org/10.1109/ISCAS.](https://doi.org/10.1109/ISCAS.2017.8050295)\
    \ [2017.8050295](https://doi.org/10.1109/ISCAS.2017.8050295)\n- <span id=\"page-10-13\"\
    ></span>[33] Min Liu and Tobi Delbruck. 2022. EDFLOW: Event Driven Optical Flow\
    \ Camera With Keypoint Detection and Adaptive Block Matching. IEEE Transactions\
    \ on Circuits and Systems for Video Technology 32, 9 (2022), 5776\u20135789. [https://doi.](https://doi.org/10.1109/TCSVT.2022.3156653)\n\
    \n<span id=\"page-11-0\"></span>[org/10.1109/TCSVT.2022.3156653](https://doi.org/10.1109/TCSVT.2022.3156653)\n\
    \n- <span id=\"page-11-5\"></span>[34] Min Liu, Wei-Tse Kao, and Tobi Delbruck.\
    \ 2019. Live demonstration: A realtime event-based fast corner detection demo\
    \ based on fpga. In Proceedings of the IEEE/CVF Conference on Computer Vision\
    \ and Pattern Recognition Workshops. 0\u20130.\n- <span id=\"page-11-13\"></span>[35]\
    \ Iulia-Alexandra Lungu, Federico Corradi, and Tobi Delbr\xFCck. 2017. Live demonstration:\
    \ Convolutional neural network driven by dynamic vision sensor playing RoShamBo.\
    \ In 2017 IEEE International Symposium on Circuits and Systems (ISCAS). 1\u2013\
    1.<https://doi.org/10.1109/ISCAS.2017.8050403>\n- <span id=\"page-11-2\"></span>[36]\
    \ Ana I Maqueda, Antonio Loquercio, Guillermo Gallego, Narciso Garc\xEDa, and\
    \ Davide Scaramuzza. 2018. Event-based vision meets deep learning on steering\
    \ prediction for self-driving cars. In Proceedings of the IEEE conference on computer\
    \ vision and pattern recognition. 5419\u20135427.\n- <span id=\"page-11-17\"></span>[37]\
    \ Riccardo Massa, Alberto Marchisio, Maurizio Martina, and Muhammad Shafique.\
    \ 2020. An efficient spiking neural network for recognizing gestures with a dvs\
    \ camera on the loihi neuromorphic processor. In 2020 International Joint Conference\
    \ on Neural Networks (IJCNN). IEEE, 1\u20139.\n- <span id=\"page-11-4\"></span>[38]\
    \ Paul A Merolla, John V Arthur, Rodrigo Alvarez-Icaza, Andrew S Cassidy, Jun\
    \ Sawada, Filipp Akopyan, Bryan L Jackson, Nabil Imam, Chen Guo, Yutaka Nakamura,\
    \ et al. 2014. A million spiking-neuron integrated circuit with a scalable communication\
    \ network and interface. Science 345, 6197 (2014), 668\u2013673.\n- <span id=\"\
    page-11-16\"></span>[39] Nico Messikommer, Daniel Gehrig, Antonio Loquercio, and\
    \ Davide Scaramuzza. 2020. Event-based Asynchronous Sparse Convolutional Networks.\
    \ European Conference on Computer Vision. (ECCV). [http://rpg.ifi.uzh.ch/docs/ECCV20\\\
    \\_](http://rpg.ifi.uzh.ch/docs/ECCV20_Messikommer.pdf) [Messikommer.pdf](http://rpg.ifi.uzh.ch/docs/ECCV20_Messikommer.pdf)\n\
    - <span id=\"page-11-8\"></span>[40] Fernando Cladera Ojeda, Anthony Bisulco,\
    \ Daniel Kepple, Volkan Isler, and Daniel D. Lee. 2020. On-Device Event Filtering\
    \ with Binary Neural Networks for Pedestrian Detection Using Neuromorphic Vision\
    \ Sensors. In 2020 IEEE International Conference on Image Processing (ICIP). 3084\u2013\
    3088. [https:](https://doi.org/10.1109/ICIP40778.2020.9191148) [//doi.org/10.1109/ICIP40778.2020.9191148](https://doi.org/10.1109/ICIP40778.2020.9191148)\n\
    - <span id=\"page-11-14\"></span>[41] Garrick Orchard, Ajinkya Jayawant, Gregory\
    \ K Cohen, and Nitish Thakor. 2015. Converting static image datasets to spiking\
    \ neuromorphic datasets using saccades. Frontiers in neuroscience 9 (2015), 437.\n\
    - <span id=\"page-11-6\"></span>[42] Angshuman Parashar, Minsoo Rhu, Anurag Mukkara,\
    \ Antonio Puglielli, Rangharajan Venkatesan, Brucek Khailany, Joel Emer, Stephen\
    \ W. Keckler, and\n\nWilliam J. Dally. 2017. SCNN: An Accelerator for Compressed-Sparse\
    \ Convolutional Neural Networks. In Proceedings of the 44th Annual International\
    \ Symposium on Computer Architecture (Toronto, ON, Canada) (ISCA '17). Association\
    \ for Computing Machinery, New York, NY, USA, 27\u201340. [https://doi.org/10.](https://doi.org/10.1145/3079856.3080254)\
    \ [1145/3079856.3080254](https://doi.org/10.1145/3079856.3080254)\n\n- <span id=\"\
    page-11-9\"></span>[43] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov,\
    \ and Liang-Chieh Chen. 2018. Mobilenetv2: Inverted residuals and linear bottlenecks.\
    \ In Proceedings of the IEEE conference on computer vision and pattern recognition.\
    \ 4510\u20134520.\n- <span id=\"page-11-3\"></span>[44] Amos Sironi, Manuele Brambilla,\
    \ Nicolas Bourdis, Xavier Lagorce, and Ryad Benosman. 2018. HATS: Histograms of\
    \ averaged time surfaces for robust eventbased object classification. In Proceedings\
    \ of the IEEE conference on computer vision and pattern recognition. 1731\u2013\
    1740.\n- <span id=\"page-11-10\"></span>[45] Mingxing Tan, Bo Chen, Ruoming Pang,\
    \ Vijay Vasudevan, Mark Sandler, Andrew Howard, and Quoc V Le. 2019. Mnasnet:\
    \ Platform-aware neural architecture search for mobile. In Proceedings of the\
    \ IEEE/CVF conference on computer vision and pattern recognition. 2820\u20132828.\n\
    - <span id=\"page-11-11\"></span>[46] Mingxing Tan and Quoc Le. 2019. Efficientnet:\
    \ Rethinking model scaling for convolutional neural networks. In International\
    \ conference on machine learning. PMLR, 6105\u20136114.\n- <span id=\"page-11-15\"\
    ></span>[47] Zhewei Yao, Zhen Dong, Zhangcheng Zheng, Amir Gholami, Jiali Yu,\
    \ Eric Tan, Leyuan Wang, Qijing Huang, Yida Wang, Michael Mahoney, et al. 2021.\
    \ Hawq-v3: Dyadic neural network quantization. In International Conference on\
    \ Machine Learning. PMLR, 11875\u201311886.\n- <span id=\"page-11-12\"></span>[48]\
    \ Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian Sun. 2018. Shufflenet: An\
    \ extremely efficient convolutional neural network for mobile devices. In Proceedings\
    \ of the IEEE conference on computer vision and pattern recognition. 6848\u2013\
    6856.\n- <span id=\"page-11-1\"></span>[49] Xu Zheng, Yexin Liu, Yunfan Lu, Tongyan\
    \ Hua, Tianbo Pan, Weiming Zhang, Dacheng Tao, and Lin Wang. 2023. Deep learning\
    \ for event-based vision: A comprehensive survey and benchmarks. arXiv preprint\
    \ arXiv:2302.08890 (2023).\n- <span id=\"page-11-7\"></span>[50] Xuda Zhou, Zidong\
    \ Du, Qi Guo, Shaoli Liu, Chengsi Liu, Chao Wang, Xuehai Zhou, Ling Li, Tianshi\
    \ Chen, and Yunji Chen. 2018. Cambricon-S: Addressing Irregularity in Sparse Neural\
    \ Networks through A Cooperative Software/Hardware Approach. In 2018 51st Annual\
    \ IEEE/ACM International Symposium on Microarchitecture (MICRO). 15\u201328.<https://doi.org/10.1109/MICRO.2018.00011>"
  paper_id: 4
  scores:
    originality:
      raw: 4.5
      reason: The proposed ESDA architecture introduces a novel approach to processing
        event-based vision data, showcasing significant innovation in FPGA design.
    technical_soundness:
      raw: 4.0
      reason: The methods presented are robust and well-supported by experimental
        results, although some implementation details could be further clarified.
    clarity:
      raw: 4.0
      reason: The paper is generally well-written, but certain sections could benefit
        from more detailed explanations to enhance understanding.
    significance:
      raw: 4.5
      reason: This work addresses a critical challenge in the field of event-based
        vision processing, making it highly significant for both academia and industry.
    relevance:
      raw: 5.0
      reason: The topic is highly relevant to current trends in machine learning and
        computer vision, particularly in the context of low-power and high-speed applications.
  total_score: 4.3
  tier: A
- title: "Accelerating Boolean Constraint Propagation for Efficient SAT-Solving on\n\
    \  FPGAs"
  abstract: 'We present a hardware-accelerated SAT solver targeting processor/Field

    Programmable Gate Arrays (FPGA) SoCs. Our solution accelerates the most

    expensive subroutine of the Davis-Putnam-Logemann-Loveland (DPLL) algorithm,

    Boolean Constraint Propagation (BCP) through fine-grained FPGA parallelism.

    Unlike prior state-of-the-art solutions, our solver eliminates costly clause

    look-up operations by assigning clauses directly to clause processors on the

    FPGA and dividing large formulas into smaller partitions manageable by FPGA.

    Partitions are hot-swapped during runtime as required and the supported formula

    size is limited only by available external memory, not on-chip FPGA memory. We

    evaluate our solver on a Xilinx Zynq platform with results showing quicker

    execution time across various formula sizes, subject to formula partitioning

    strategy. Compared to prior state-of-the-art, we achieve 1.7x and 1.1x speed up

    on BCP for 2 representative benchmarks and up to 6x total speedup over

    software-only implementation.'
  url: http://arxiv.org/abs/2401.07429v2
  keywords: ''
  document: "# Accelerating Boolean Constraint Propagation for Efficient SAT-Solving\
    \ on FPGAs\n\nHari Govindasamy Carleton University Ottawa, Canada hari@sce.carleton.ca\n\
    \nBabak Esfandiari Carleton University Ottawa, Canada babak@sce.carleton.ca\n\n\
    Paulo Garcia Chulalongkorn University Bangkok, Thailand paulo.g@chula.ac.th\n\n\
    #### ABSTRACT\n\nWe present a hardware-accelerated SAT solver targeting processor/Field\
    \ Programmable Gate Arrays (FPGA) SoCs. Our solution accelerates the most expensive\
    \ subroutine of the Davis-Putnam-Logemann-Loveland (DPLL) algorithm, Boolean Constraint\
    \ Propagation (BCP) through fine-grained FPGA parallelism. Unlike prior state-of-the-art\
    \ solutions, our solver eliminates costly clause lookup operations by assigning\
    \ clauses directly to clause processors on the FPGA and dividing large formulas\
    \ into smaller partitions manageable by FPGA. Partitions are hot-swapped during\
    \ runtime as required and the supported formula size is limited only by available\
    \ external memory, not on-chip FPGA memory.\n\nWe evaluate our solver on a Xilinx\
    \ Zynq platform with results showing quicker execution time across various formula\
    \ sizes, subject to formula partitioning strategy. Compared to prior state-of-theart,\
    \ we achieve 1.7x and 1.1x speed up on BCP for 2 representative benchmarks and\
    \ up to 6x total speedup over software-only implementation.\n\n## CCS CONCEPTS\n\
    \n\u2022 Computer systems organization \u2192 Robotic autonomy; \u2022 Networks\
    \ \u2192 Cyber-physical networks; \u2022 Applied computing \u2192 Industry and\
    \ manufacturing; \u2022 Hardware \u2192 Hardware accelerators; Application specific\
    \ processors; \u2022 Theory of computation \u2192 Equational logic and rewriting.\n\
    \n### KEYWORDS\n\nFPGA, SAT, acceleration, embedded, boolean, satisfiability\n\
    \n#### ACM Reference Format:\n\nHari Govindasamy, Babak Esfandiari, and Paulo\
    \ Garcia. 2024. Accelerating Boolean Constraint Propagation for Efficient SAT-Solving\
    \ on FPGAs. In Great Lakes Symposium on VLSI 2024 (GLSVLSI '24), June 12\u2013\
    14, 2024, Clearwater, FL, USA. ACM, New York, NY, USA, [5](#page-4-0) pages. [https://doi.org/10.1145/](https://doi.org/10.1145/3649476.3658808)\
    \ [3649476.3658808](https://doi.org/10.1145/3649476.3658808)\n\n#### 1 INTRODUCTION\n\
    \nThe Boolean Satisfiability problem (SAT) is a fundamental problem in computer\
    \ science, the first NP-Complete problem [\\[7\\]](#page-4-1). SAT solvers have\
    \ become the backbone of several engineering domains, as any\n\nGLSVLSI '24, June\
    \ 12\u201314, 2024, Clearwater, FL, USA\n\n\xA9 2024 Copyright held by the owner/author(s).\n\
    \nACM ISBN 979-8-4007-0605-9/24/06.\n\n<https://doi.org/10.1145/3649476.3658808>\n\
    \nNP-Complete problem can be encoded as instance of SAT [\\[2,](#page-4-2) [7\\\
    ]](#page-4-1). SAT solvers determine whether a given boolean formula is satisfiable\
    \ by identifying an assignment to the formulas' free variables that evaluate the\
    \ formula to true. The formula is unsatisfiable otherwise. Most SAT solvers target\
    \ CNF-SAT, a subset of SAT that determines the satisfiability of formulas encoded\
    \ in Conjunctive Normal Form (CNF) . Formulas in CNF are conjunctions of clauses,\
    \ where each clause is a disjunction of one or more literals (a variable or its\
    \ negation).\n\nWith the advent of modern Systems-on-Chip (SoC) comprised of both\
    \ hard embedded processors and configurable FPGA fabric offering myriad implementation\
    \ opportunities [\\[17\\]](#page-4-3), deployed from the embedded to the high\
    \ performance computing domain [\\[1\\]](#page-4-4), accelerating SAT-solving\
    \ through hardware is an attractive approach. We present a novel architecture\
    \ for hardware-accelerated SAT-solving that outperforms state of the art solutions,\
    \ released in open-source form for the Xilinx Zynq platform. Specifically, this\
    \ article offers the following contributions:\n\n- We describe a methodology to\
    \ map and runtime-manage clauses across a processor and connected FPGA, making\
    \ efficient use of FPGA resources and avoiding recurring performance pitfalls.\n\
    - We describe the implementation of an open-source prototype system, deployed\
    \ on a Xilinx Zynq chip, identifying how the hardware architecture effects the\
    \ aforementioned strategy.\n- We evaluate our design against the state of the\
    \ art using two representative benchmarks, showing speed-ups of 1.7x and 1.1x,\
    \ respective, and overall a 6x improvement over vanilla software execution.\n\n\
    Section [2](#page-0-0) describes necessary background knowledge on a particular\
    \ SAT-solving algorithm required to understand the remainder of this paper. Section\
    \ [3](#page-1-0) presents an overview of historical solutions and state of the\
    \ art, directly compared against in this paper. Section [4](#page-2-0) presents\
    \ our contribution, evaluated in Section [5,](#page-3-0) with concluding remarks\
    \ and suggestions for future work described in Section [6.](#page-4-5)\n\n###\
    \ <span id=\"page-0-0\"></span>2 BACKGROUND: DPLL AND BCP\n\nSAT solvers are categorized\
    \ into complete and incomplete solvers[\\[15\\]](#page-4-6). Complete solvers\
    \ evaluate every possible variable assignment, ending on the first satisfying\
    \ assignment or after exhausting the search space. A formula is unsatisfiable\
    \ if the complete solver concludes without finding a satisfying assignment. Most\
    \ incomplete solvers use Stochastic Local Search (SLS) to greedily search for\
    \ a satisfying assignment in the formula's variable assignment search space[\\\
    [16\\]](#page-4-7). While typically quicker than complete solvers, incomplete\
    \ solvers do not guarantee results as they tend to get stuck in local maxima or\
    \ skip satisfying assignments. Since they don't explore the solution\n\nPermission\
    \ to make digital or hard copies of part or all of this work for personal or classroom\
    \ use is granted without fee provided that copies are not made or distributed\
    \ for profit or commercial advantage and that copies bear this notice and the\
    \ full citation on the first page. Copyrights for third-party components of this\
    \ work must be honored. For all other uses, contact the owner/author(s).\n\nGLSVLSI\
    \ '24, June 12\u201314, 2024, Clearwater, FL, USA Govindasamy, Esfandiari, and\
    \ Garcia\n\n<span id=\"page-1-1\"></span>![](_page_1_Figure_2.jpeg)\n\nFigure\
    \ 1: Interface between processor and FPGA-based BCP Coprocessor. DPLL's BCP is\
    \ accelerated through fine-grained parallelization across Clause Processors.\n\
    \nspace exhaustively they can never conclude that a formula is unsatisfiable.\
    \ Davis-Putnam-Logemann-Loveland (DPLL) and DPLL-based algorithms are the most\
    \ predominant complete solvers [\\[16\\]](#page-4-7). DPLL performs two primary\
    \ operations: 1) decision and 2) Boolean Constraint Propagation (BCP). DPLL-based\
    \ algorithms follow DPLL's core structure, and propose improved decision heuristics,\
    \ learning and BCP mechanisms. During decision, DPLL heuristically picks and assigns\
    \ truth values to free variables. BCP subsequently propagates the effect of the\
    \ decision using the unit implication rule[\\[5\\]](#page-4-8). The unit implication\
    \ rule identifies unit clauses where all but one of its literals are false. Unit\
    \ clauses can only be satisfied by assigning the variable to true if the literal\
    \ is positive or to false on negative literals. The resulting assignment is known\
    \ as an implication. BCP repeatedly applies this rule until all clauses are satisfied\
    \ (formula is therefore satisfiable) or at least one clause evaluates false (conflict).\
    \ On conflicts, DPLL backtracks by retracting and/or inverting assignments from\
    \ earlier decisions. BCP is expensive, accounting for 80-90% of DPLL's CPU time,\
    \ rendering it a prime candidate for hardware acceleration [\\[5,](#page-4-8)\
    \ [18\\]](#page-4-9). BCP coprocessors accelerate DPLL by implementing specialized\
    \ BCP processing engines on FPGA. These run alongside a General Purpose Processor\
    \ (GPP) that performs the remaining DPLL operations: decision heuristics and backtracking.\
    \ Using this architecture, the BCP coprocessor is first configured with the clauses,\
    \ and then waits to evaluate decisions from the GPP. Any DPLL-based software solver\
    \ can integrate with a BCP-coprocessor by replacing software BCP with the hardware\
    \ accelerated BCP-coprocessor [\\[18,](#page-4-9) [19\\]](#page-4-10). FPGA-based\
    \ BCP coprocessors are either instance-specific or application-specific. Instancespecific\
    \ solver are built to solve a single SAT instance and designed by translating\
    \ an input formula into its equivalent logical circuit. However, to solve new\
    \ instances, the FPGA requires a complete rebuild (synthesis and FPGA programming\
    \ may take up to several hours). Although these solvers can be significantly quicker\
    \ than their software counterparts, their performance becomes notably slower when\
    \ build times are included. For instance, Ivan et al's best result against the\
    \ hole7 benchmark achieves a 6.66x speedup against MiniSAT[\\[11,](#page-4-11)\
    \ [12\\]](#page-4-12); however, when build times are included, compilation alone\
    \ takes 50 seconds, whereas MiniSAT finishes in under 0.064 seconds [\\[10\\]](#page-4-13).\
    \ Application-specific solvers eliminate the need to rebuild the FPGA by instantiating\
    \ general-purpose processing units capable of tackling any SAT instance (given\
    \ that it fits in hardware). The BCP coprocessor is configured with the target\
    \ problem by simply overwriting FPGA memory.\n\n#### <span id=\"page-1-0\"></span>3\
    \ STATE OF THE ART\n\nAlgorithmic techniques for efficient SAT solving have been\
    \ extensively researched, and the literature contains several surveys that describe\
    \ the history and state of the art of the problem ([\\[8\\]](#page-4-14), [\\\
    [13\\]](#page-4-15)). Techniques aimed at accelerating the execution of a particular\
    \ SAT solving algorithm include software parallelization [\\[9\\]](#page-4-16),\
    \ deployment on specialized GPUs [\\[14\\]](#page-4-17), and even acceleration\
    \ through machinelearning approaches [\\[20\\]](#page-4-18).\n\nOur approach sits\
    \ within FPGA-based acceleration, which began roughly 3 decades ago [\\[6\\]](#page-4-19),\
    \ with a few prominent results at the turn of the century ([\\[21\\]](#page-4-20),\
    \ [\\[3\\]](#page-4-21)). However, it was not until significant advances in FPGA\
    \ performance occurred in the last decade, and the rise of SoC platforms combining\
    \ FPGA fabric with hard processors, that FPGA-based SAT acceleration matured.\
    \ The most notable architectures were proposed by Davis et al [\\[5\\]](#page-4-8)\
    \ and Thong et al [\\[18,](#page-4-9) [19\\]](#page-4-10): both exploring the\
    \ use of FPGA to implement BCP coprocessors, keeping the remainder of DPLL in\
    \ software.\n\nDavis et al calculate implications in parallel by using several\
    \ inference engines (IE), each assigned a list of clauses (partitions) [\\[5\\\
    ]](#page-4-8). For every decision/implication, the clause containing the assignment\
    \ variable is first retrieved before calculating implications. Implications are\
    \ forwarded to a conflict detector that ensures that two or more IEs have not\
    \ implied opposing values for the same variable. Implications are then sent to\
    \ the processor and queued up for propagation.\n\nTo keep clause retrieval time\
    \ low, a variable only occurs once in each IEs partition (i.e clauses within the\
    \ same IE share no common variables). This limits the number of clauses affected\
    \ by a decision to one, thereby also limiting implications per IE to one, constraining\
    \ the effected performance. While some strategies to increase this limit have\
    \ been proposed [\\[4\\]](#page-4-22), they remain unexplored.\n\nThong et al.\
    \ propose a concurrent BCP coprocessor comprising multiple sequential processing\
    \ engines (PE) [\\[19\\]](#page-4-10). Identifying that Davis et al.'s clause\
    \ lookup is slower than direct access [\\[18\\]](#page-4-9), they develop a clause\
    \ storage and encoding scheme that efficiently links clauses with shared variables.\
    \ The processor sends decisions to the FPGA and starts BCP execution at a single\
    \ PE. Using the linked list, the PE traverses every clause containing the decision\
    \ variable and calculates implications, which are then added to a local queue\
    \ and propagated. The running PE triggers BCP execution in another PE when it\
    \ arrives at a link to a clause that is located elsewhere. The coprocessor supports\
    \ multithreaded software execution, hiding communication and software latency\
    \ by keeping the coprocessor busy while software threads make decisions when possible.\n\
    \nDavis et al. and Thong et al. have laid a strong foundation in developing application-specific\
    \ FPGA-based BCP coprocessors; we extend their work and propose a solution that\
    \ processes clauses in parallel without the need for clause lookup.\n\nAccelerating\
    \ Boolean Constraint Propagation for Efficient SAT-Solving on FPGAs GLSVLSI '24,\
    \ June 12\u201314, 2024, Clearwater, FL, USA\n\n#### <span id=\"page-2-0\"></span>4\
    \ THE SAT SOLVER ARCHITECTURE\n\nWe present a BCP coprocessor that works alongside\
    \ vanilla DPLL (and should, in theory, work seamlessly with any DPLL-based solver).\
    \ Like Thong et al., we forgo clause lookup and allow clauses to share variables\
    \ within the same partition. However, we still achieve Davis et al.'s high degree\
    \ of parallelism by placing clauses directly in clause processors (explained in\
    \ Section [4.1\\)](#page-2-1).\n\nSAT instances larger than the available number\
    \ of Clause Processors (CPs) are partitioned, stored in external memory (i.e.,\
    \ software) and hot-swapped into the BCP coprocessors as required during runtime.\
    \ Solvable instance size is limited only by the GPP's RAM, not on-chip FPGA memory.\
    \ We deploy our solution on the Zynq chip, and available here[1](#page-2-2) for\
    \ use. To our knowledge, this is the first open-source hardware-accelerated SAT\
    \ solver.\n\n#### <span id=\"page-2-1\"></span>4.1 The BCP accelerator architecture\n\
    \nFigure [1](#page-1-1) illustrates our approach, comprising a GPP and an FPGA\
    \ accelerated BCP coprocessor. The GPP executes DPLL's remaining elements (decisions,\
    \ backtrack, etc.), partitions large SAT instances (explained in Section [4.2\\\
    )](#page-2-3) and swaps partitions into hardware as required. Its default state\
    \ is idle, awaiting instructions to execute. Once a decision is received, the\
    \ systems loops until all unit clauses are exhausted. The BCP coprocessor, depicted\
    \ in Figure [1,](#page-1-1) comprises a control unit (1), an array of clause processors\
    \ (2) and an implication selector (3). The central control unit communicates directly\
    \ with the GPP and each CP. Based on the received GPP command, it loads clauses\
    \ into CPs, broadcasts decisions, or clears assignments during backtrack. At its\
    \ core, the BCP coprocessor consists of an array of CPs that calculate decision\
    \ and implication results in parallel. CPs store clauses as an array of literals\
    \ maintain a local copy of each literal's respective variable assignment. Partitions\
    \ are hot-swapped into FPGA by overwriting a CPs array of literals with the literals\
    \ of the new clause. Variable assignments are updated during decisions and BCP,\
    \ and cleared during backtrack. Finally, the implication selector chooses a single\
    \ implication to propagate when multiple implications arise as a result of BCP.\
    \ Rather than using an explicit implication conflict detector, as done by Davis\
    \ et al [\\[5\\]](#page-4-8), we propagate the chosen implication, and identify\
    \ conflicts during evaluation.\n\n#### <span id=\"page-2-3\"></span>4.2 Formulae\
    \ partitioning\n\nSAT instances contain an arbitrary number of variables and clauses.\
    \ The problem size solvable on FPGA is limited by its available Configurable Logic\
    \ Block (CLB) and memory, and requires large problems be partitioned into smaller\
    \ manageable sizes. Partitions are stored in the GPP, and swapped into FPGA during\
    \ run time by overwriting CPs clauses. BCP is performed individually on each partition,\
    \ and implications are relayed back to the GPP. Implications are subsequently\
    \ propagated to other partitions. We aim to make partitions as large as possible,\
    \ limited by the coprocessor's clause and variable threshold. Consider Equation\
    \ [1,](#page-2-4) composed of four clauses, and an instance of our coprocessor\
    \ that supports two clauses and three variables. Equation [2](#page-2-5) and [3](#page-2-6)\
    \ outline the two possible ways to partition Equation [1.](#page-2-4) Equation\
    \ [2](#page-2-5) describes a scenario where the partitions\n\n<span id=\"page-2-7\"\
    ></span>![](_page_2_Figure_10.jpeg)\n\nFigure 2: (a) Davis et al. store the formula\
    \ directly on FPGA. Clauses within partitions contain no shared variables, and\
    \ partitions are mapped directly to Implication Engines. (b) Thong et al. store\
    \ formula directly on FPGA. Clauses are linked to other clauses with shared variables\
    \ and are processed sequentially. (c) Formula stored in external memory (\"software\"\
    \ view). Clauses in partitions mapped directly to Clause Processors, and hot-swapped\
    \ as required.\n\nreach the clause limit, while the Equation [3](#page-2-6) reaches\
    \ the variable limit.\n\n<span id=\"page-2-4\"></span>\n$$f = (\\neg a \\lor b\
    \ \\lor \\neg c) \\land (a \\lor \\neg b \\lor \\neg c) \\land (\\neg d \\lor\
    \ e \\lor f) \\land (d \\lor e \\lor f) \\tag{1}$$\n\n<span id=\"page-2-5\"></span>\n\
    $$\\begin{aligned} \\{ \\begin{aligned} \\{ \\begin{aligned} \\{ \\begin{array}{l}\
    \ (\\neg a \\lor b \\lor \\neg c) \\land (a \\lor \\neg b \\lor \\neg c) \\end{array}\
    \ \\} \\end{aligned} \\} \\end{aligned} \\} $$\n\n<span id=\"page-2-6\"></span>\n\
    $$\n\\begin{aligned}\n\\{\\neg variation\\\\_2 &= \\{\\{ (\\neg a \\lor b \\lor\
    \ \\neg c) \\}, \\{ (a \\lor \\neg b \\lor \\neg c) \\} \\}, \\\\\n\\{ (\\neg\
    \ d \\lor e \\lor f) \\}, &\\{ (d \\lor e \\lor f) \\} \\end{aligned}\n\\begin{aligned}\n\
    \\{ \\neg variation\\\\_2 \\}, \\end{aligned}\n$$\n\nResults (refer to Section\
    \ [5\\)](#page-3-0) indicate that partitioning is a bottleneck in our approach.\
    \ Performance improvement is dictated by the amount of required partition swapping\
    \ and the number of unused CPs (occurs when the number of clauses in a partition\
    \ is less than the available number of CPs). Thus, performance improvement is\
    \ observed with certain partition assignments, while others lead to performance\
    \ degradation. System performance can be improved by developing a more effective\
    \ partitioning algorithm, but beyond the scope of this paper and reserved for\
    \ future work.\n\n#### 4.3 Execution\n\nEach clause processor is only associated\
    \ with a single clause; thus, no clause look-up or traversal is required to retrieve\
    \ the affected clause for processing. All clauses on the FPGA are processed in\
    \ parallel as soon as a decision is received. Consider Equation [1'](#page-2-4)s\n\
    \n<span id=\"page-2-2\"></span><sup>1</sup>[https://github.com/harigovind1998/FPGA\\\
    \\_BCP\\\\_acceleration](https://github.com/harigovind1998/FPGA_BCP_acceleration)\n\
    \nGLSVLSI '24, June 12\u201314, 2024, Clearwater, FL, USA Govindasamy, Esfandiari,\
    \ and Garcia\n\n<span id=\"page-3-1\"></span>\n\n| Step                 |    \
    \  | 0    | 1                              | 2                       | 3     \
    \              | 4    |\n|----------------------|------|------|--------------------------------|-------------------------|---------------------|------|\n\
    | Our<br>Approach      | CP   | Rx   | Process<br>Decision  Clause 1  | Done \
    \                   |                     |      |\n|                      | CP\
    \ 2 | Rx   | Process<br>Decision  Clause 2  | Done                    |      \
    \               |      |\n| Davis et<br>al.      | IE 1 | Rx   | Retrieve<br>Decision\
    \ Clause 1  | Process<br>Clause 1     | Done                |      |\n|      \
    \                | IE 2 | Rx   | Retrieve<br>Decision  Clause 2 | Process<br>Clause\
    \ 2     | Done                |      |\n| Thong et PE 1<br>al. |      | Rx   |\
    \ Process<br>Decision  Clause 1  | Traverse to<br>Clause 2 | Process<br>Clause\
    \ 2 | Done |\n|                      | PE 2 | ldle | ldle                    \
    \       | ldle                    | ldle                | ldle |\n\nFigure 3:\
    \ Execution steps of each described approach.\n\nmapping of partitions to hardware\
    \ as presented in Figure [2.](#page-2-7) Figure [3](#page-3-1) summarizes the\
    \ execution stages for Davis et al.'s, Thong et al.'s and our approach for the\
    \ theoretical execution for a decision of variable . In our approach, clauses\
    \ 1 and 2 are processed by Clause Processor 1 and 2 in parallel once the decision\
    \ is received. Since clauses 3 and 4 do not contain variable , Partition 2 remains\
    \ in external memory and is not processed. Though Davis et al. also process clause\
    \ 1 and 2 in parallel, each Implication Engine first performs a clause look-up\
    \ to retrieve the affected clause. Results of the decision on the affected clause\
    \ are then calculated. Thong et al.'s approach starts BCP on Processing Engine\
    \ 1. After clause 1 is processed Processing Engine 1 traverses to clause 2. In\
    \ the manner, clauses in a partition are processed sequentially. Execution concludes\
    \ after computing Partitions 1's final element, clause 2. Processing Engine 2\
    \ remains idle for the entire duration as clauses in partition 2 do not contain\
    \ variable .\n\n#### 4.4 Processor-FPGA interface\n\nThe BCP coprocessor implements\
    \ the Advanced eXtensible Interface 4-Lite (AXI-Lite) IP interface, acting as\
    \ a subordinate to a processor (AXI master). Using AXI, the processor writes directly\
    \ to the coprocessor's registers to send instructions and data, and continues\
    \ polling for status updates and new implication until the coprocessor completes.\n\
    \nStatus changes dictate DPLL's flow, either allowing the search to continue assigning\
    \ additional variables, or triggers backtracking on conflicts. A copy of all the\
    \ implications are saved on the processor to avoid re-assigning implied variables,\
    \ and further propagated to the remaining partitions.\n\n#### <span id=\"page-3-0\"\
    ></span>5 EXPERIMENTS AND RESULTS\n\nOn a Xilinx Zynq chip with total capacity\
    \ of 14400 LUTs and 28800 FF, our solution supports 224 parallel Clause Processors\
    \ and 63 variables. We achieve a clock frequency of 106.66 MHz, utilizing 647\
    \ LUTRAM of on-chip memory, 13151 LUTs, and 11059 FFs.\n\nRelated work calculates\
    \ throughput (in BPCs performed per second), assuming full data availability:\
    \ i.e., not taking into account software execution and communication/data transfer\
    \ latency. Whilst this is a useful metric to assess hardware performance in isolation\
    \ (and we report equivalent results in Table [1\\)](#page-3-2), it does not accurately\
    \ depict system performance; to do so, we break down\n\n<span id=\"page-3-2\"\
    ></span>\n\n|               | Millions of BCP/s |                     |      \
    \      |  |  |\n|---------------|-------------------|---------------------|------------|--|--|\n\
    | SAT Instance  | Davis et al [5]   | Thong et al<br>[19] | Our Design |  |  |\n\
    | bmc-galileo-8 | 40                | 102                 | 175        |  |  |\n\
    | bmc-ibm-12    | 33                | 150                 | 169        |  |  |\n\
    \nTable 1: Comparison of BCP engine throughput (BCPs/s) with related work. Results\
    \ reflect maximum theoretical throughput, achieved only data is fully available\
    \ to BCP engines.\n\n<span id=\"page-3-3\"></span>![](_page_3_Figure_13.jpeg)\n\
    \nFigure 4: Breakdown of the total execution time across constituent components.\n\
    \n<span id=\"page-3-4\"></span>\n\n|         |       | Variables  |          \
    \ |           |           |  |  |  |\n|---------|-------|------------|-----------|-----------|-----------|--|--|--|\n\
    |         |       | 63         | 126       | 252       | 630       |  |  |  |\n\
    | Clauses | 224   | 362M BCP/s | 17K BCP/s |           | NA        |  |  |  |\n\
    |         |       | 2.2x       | 0.17x     | NA        |           |  |  |  |\n\
    |         | 448   | 702K BCP/s | 21K BCP/s | 13K BCP/s | NA        |  |  |  |\n\
    |         |       | 1.6x       | 0.21x     | 0.08x     |           |  |  |  |\n\
    |         | 2240  | 441K BCP/s | 22K BCP/s | 16K BCP/s | 12K BCP/s |  |  |  |\n\
    |         |       | 1.91x      | 1.26x     | 0.61x     | 0.10x     |  |  |  |\n\
    |         | 22400 | 313K BCP/s | 20K BCP/s | 16K BCP/s | 14K BCP/s |  |  |  |\n\
    |         |       | 6.32x      | 5.04x     | 4.86x     | 3.31x     |  |  |  |\n\
    \nTable 2: Varied clause/variable sizes and their impact on the relative speedup\
    \ of hardware/software and the effective throughput of BCP engines.\n\nthe full\
    \ execution in Figure [4](#page-3-3) and evaluate speedup over vanilla software\
    \ implementation, evaluating combinations of clause and variable sizes, with speedup\
    \ depicted in Table [2](#page-3-4) for meaningful combinations. For each combination,\
    \ we also depict real throughput, in the form of BCPs/s averaged over total execution\
    \ time (63 variables and 224 clauses is the theoretical upper bound, without the\
    \ need for hot swapping). To evaluate the different effects of clause/variable\
    \ sizes on execution, we fix one and vary the other, measuring total execution\
    \ time: results are depicted in Figures [5](#page-4-23) and [6.](#page-4-24)\n\
    \n<span id=\"page-4-0\"></span>Accelerating Boolean Constraint Propagation for\
    \ Efficient SAT-Solving on FPGAs GLSVLSI '24, June 12\u201314, 2024, Clearwater,\
    \ FL, USA\n\n<span id=\"page-4-23\"></span>![](_page_4_Figure_1.jpeg)\n\n![](_page_4_Figure_2.jpeg)\n\
    \n<span id=\"page-4-24\"></span>![](_page_4_Figure_3.jpeg)\n\nFigure 6: Effect\
    \ of increasing variables size on total execution time, for 22400 clauses.\n\n\
    #### <span id=\"page-4-5\"></span>6 CONCLUSIONS\n\nWe described a SAT-solver hardware-accelerated\
    \ architecture that outperforms state of the art by hot-swapping clause assignment\
    \ at runtime, making efficient use of FPGA resources. Our solution prototype,\
    \ on a Xilinx Zynq chip, is available in open-source. Practitioners may use the\
    \ presented solution in their designs, whenever a problem is encoded in SAT form\
    \ and performance is critical.\n\nAn important open question remains: our performance\
    \ is constrained by how clauses are partitioned. A partitioning scheme that minimizes\
    \ the distribution of variables among clauses will minimize runtime swapping,\
    \ resulting in improved execution. However, how to best partition a formula to\
    \ achieve this is not yet known. Future work must formulate this challenge as\
    \ an optimization problem, and methods for its efficient solution must be devised.\
    \ Once that is achieved, they can be applied (offline) prior to deployment on\
    \ our architecture.\n\n#### ACKNOWLEDGMENTS\n\nWe acknowledge the support of the\
    \ Natural Sciences and Engineering Research Council of Canada (NSERC).\n\n####\
    \ REFERENCES\n\n- <span id=\"page-4-4\"></span>[1] Rabie Ben Atitallah and Karim\
    \ MA Ali. 2017. FPGA-Centric High Performance Embedded Computing: Challenges and\
    \ Trends. In 2017 Euromicro Conference on Digital System Design (DSD). IEEE, 390\u2013\
    395.\n- <span id=\"page-4-2\"></span>[2] Stephen A Cook. 2023. The complexity\
    \ of theorem-proving procedures. In Logic, Automata, and Computational Complexity:\
    \ The Works of Stephen A. Cook. 143\u2013152.\n- <span id=\"page-4-21\"></span>[3]\
    \ Andreas Dandalis and Viktor K Prasanna. 2002. Run-time performance optimization\
    \ of an FPGA-based deduction engine for SAT solvers. ACM Transactions on Design\
    \ Automation of Electronic Systems (TODAES) 7, 4 (2002), 547\u2013562.\n- <span\
    \ id=\"page-4-22\"></span>[4] John D Davis, Zhangxi Tan, Fang Yu, and Lintao Zhang.\
    \ 2008. Designing an efficient hardware implication accelerator for SAT solving.\
    \ In International Conference on Theory and Applications of Satisfiability Testing.\
    \ Springer, 48\u201362.\n- <span id=\"page-4-8\"></span>[5] John D. Davis, Zhangxi\
    \ Tan, Fang Yu, and Lintao Zhang. 2008. A practical reconfigurable hardware accelerator\
    \ for boolean satisfiability solvers. In 2008 45th ACM/IEEE Design Automation\
    \ Conference. 780\u2013785. [https://doi.org/10.1145/](https://doi.org/10.1145/1391469.1391669)\
    \ [1391469.1391669](https://doi.org/10.1145/1391469.1391669)\n- <span id=\"page-4-19\"\
    ></span>[6] Amir H Farrahi and Majid Sarrafzadeh. 1994. FPGA technology mapping\
    \ for power minimization. In International Workshop on Field Programmable Logic\
    \ and Applications. Springer, 66\u201377.\n- <span id=\"page-4-1\"></span>[7]\
    \ Michael R. Garey and David S. Johnson. 1990. Computers and Intractability; A\
    \ Guide to the Theory of NP-Completeness. W. H. Freeman & Co., USA.\n- <span id=\"\
    page-4-14\"></span>[8] Weiwei Gong and Xu Zhou. 2017. A survey of SAT solver.\
    \ In AIP Conference Proceedings, Vol. 1836. AIP Publishing.\n- <span id=\"page-4-16\"\
    ></span>[9] Youssef Hamadi, Said Jabbour, and Lakhdar Sais. 2010. ManySAT: a parallel\
    \ SAT solver. Journal on Satisfiability, Boolean Modeling and Computation 6, 4\
    \ (2010), 245\u2013262.\n- <span id=\"page-4-13\"></span>[10] Anping He, Lvying\
    \ Yu, Haitao Zhang, Lian Li, and Jinzhao Wu. 2018. A FPGA Based SAT Solver with\
    \ High Random and Concurrent Strategies. In 2018 IEEE International Conference\
    \ on Software Quality, Reliability and Security Companion (QRS-C). 221\u2013228.<https://doi.org/10.1109/QRS-C.2018.00049>\n\
    - <span id=\"page-4-11\"></span>[11] Teodor Ivan and El Mostapha Aboulhamid. 2013.\
    \ An Efficient Hardware Implementation of a SAT Problem Solver on FPGA. In 2013\
    \ Euromicro Conference on Digital System Design. 209\u2013216.<https://doi.org/10.1109/DSD.2013.31>\n\
    - <span id=\"page-4-12\"></span>[12] Teodor Ivan and El Mostapha Aboulhamid. 2013.\
    \ Exploring limits of parallelism in FPGA-based Boolean satisfiability. In 2013\
    \ 2nd Mediterranean Conference on Embedded Computing (MECO). 62\u201365.<https://doi.org/10.1109/MECO.2013.6601319>\n\
    - <span id=\"page-4-15\"></span>[13] Ruben Martins, Vasco Manquinho, and In\xEA\
    s Lynce. 2012. An overview of parallel SAT solving. Constraints 17 (2012), 304\u2013\
    347.\n- <span id=\"page-4-17\"></span>[14] Muhammad Osama, Anton Wijs, and Armin\
    \ Biere. 2021. SAT solving with GPU accelerated inprocessing. In International\
    \ Conference on Tools and Algorithms for the Construction and Analysis of Systems.\
    \ Springer, 133\u2013151.\n- <span id=\"page-4-6\"></span>[15] I. Skliarova and\
    \ A.B. Ferrari. 2004. A software/reconfigurable hardware SAT solver. IEEE Transactions\
    \ on Very Large Scale Integration (VLSI) Systems 12, 4 (2004), 408\u2013419.<https://doi.org/10.1109/TVLSI.2004.825859>\n\
    - <span id=\"page-4-7\"></span>[16] Ali Asgar Sohanghpurwala, Mohamed W. Hassan,\
    \ and Peter Athanas. 2017. Hardware accelerated SAT solvers: A survey. J. Parallel\
    \ and Distrib. Comput. 106 (2017), 170\u2013184.<https://doi.org/10.1016/j.jpdc.2016.12.014>\n\
    - <span id=\"page-4-3\"></span>[17] Robert Stewart, Bernard Berthomieu, Paulo\
    \ Garcia, Idris Ibrahim, Greg Michaelson, and Andrew Wallace. 2019. Verifying\
    \ parallel dataflow transformations with model checking and its application to\
    \ FPGAs. Journal of Systems Architecture 101 (2019), 101657.\n- <span id=\"page-4-9\"\
    ></span>[18] Jason Thong and Nicola Nicolici. 2013. FPGA acceleration of enhanced\
    \ boolean constraint propagation for SAT solvers. In 2013 IEEE/ACM International\
    \ Conference on Computer-Aided Design (ICCAD). 234\u2013241. [https://doi.org/10.1109/](https://doi.org/10.1109/ICCAD.2013.6691124)\
    \ [ICCAD.2013.6691124](https://doi.org/10.1109/ICCAD.2013.6691124)\n- <span id=\"\
    page-4-10\"></span>[19] Jason Thong and Nicola Nicolici. 2015. SAT solving using\
    \ FPGA-based heterogeneous computing. In 2015 IEEE/ACM International Conference\
    \ on Computer-Aided Design (ICCAD). 232\u2013239.<https://doi.org/10.1109/ICCAD.2015.7372575>\n\
    - <span id=\"page-4-18\"></span>[20] Haoze Wu. 2017. Improving SAT-solving with\
    \ machine learning. In Proceedings of the 2017 ACM SIGCSE Technical Symposium\
    \ on Computer Science Education. 787\u2013788.\n- <span id=\"page-4-20\"></span>[21]\
    \ Peixin Zhong, Margaret Martonosi, and Pranav Ashar. 2000. FPGA-based SAT solver\
    \ architecture with near-zero synthesis and layout overhead. IEE Proceedings-Computers\
    \ and Digital Techniques 147, 3 (2000), 135\u2013141."
  paper_id: 5
  scores:
    originality:
      raw: 4.5
      reason: The approach of using FPGA for accelerating BCP in SAT solvers presents
        a novel angle that builds on existing methods while introducing significant
        improvements.
    technical_soundness:
      raw: 4.0
      reason: The technical implementation appears robust, but further details on
        the methodology and evaluation metrics could enhance the assessment of its
        soundness.
    clarity:
      raw: 4.0
      reason: The paper is generally well-structured and communicates its ideas effectively,
        though some sections could benefit from clearer explanations of complex concepts.
    significance:
      raw: 4.5
      reason: The demonstrated speedups over existing solutions highlight the potential
        impact of this work in the field of SAT solving and hardware acceleration.
    relevance:
      raw: 5.0
      reason: The topic is highly relevant to current trends in computational efficiency
        and hardware utilization in solving NP-Complete problems.
  total_score: 4.3
  tier: A
- title: "LRSCwait: Enabling Scalable and Efficient Synchronization in Manycore\n\
    \  Systems through Polling-Free and Retry-Free Operation"
  abstract: 'Deep Learning (DL) has achieved unprecedented success in various application

    domains. Meanwhile, model pruning has emerged as a viable solution to reduce

    the footprint of DL models in mobile applications, without compromising their

    accuracy. To enable the matrix engines built for dense DL models to also handle

    their pruned counterparts, pruned DL models follow a fine-grained structured

    sparsity pattern of 1:4, or 2:4, whereby in each group of four contiguous

    values, at least one, or two, respectively, must be non-zero. Structured

    sparsity has recently also moved to coarser (relaxed) cases of N:128, or N:256,

    for small values of N, targeting a wider range of sparsity (10%-90%) for the DL

    models. In this work, we design an accelerator that operates, by construction,

    on wide blocks with relaxed structured sparsity. In contrast to the

    conventional systolic array archetype, the new engine decouples the memory part

    of the systolic array from the multiply-add units. The memory block comprises
    1

    write and N read ports, with the number of read ports being equal to the number

    of non-zero elements per row. The multiply-add units connect directly to each

    read port and complete the multiplication in a row-wise product-first order.

    More importantly, simple reconfiguration facilitates more dense patterns. The

    experimental evaluation demonstrates substantial latency improvements over

    current state-of-the-art systolic array engines built for fine-grained and

    relaxed structured sparsity.'
  url: http://arxiv.org/abs/2401.08179v1
  keywords: Structured sparsity, Matrix-multiplication engine, Machine learning accelerator,
    Systolic computation
  document: "## I. INTRODUCTION\n\nThe acceleration of DL models, for both training\
    \ and inference, relies primarily on equivalent matrix multiplications that inherently\
    \ map to systolic arrays. To reduce memory storage and computation cost, the weights\
    \ of DL models are pruned, thereby leading to sparse models [\\[1\\]](#page-3-0),\
    \ [\\[2\\]](#page-3-1). The derived zero weights are not stored and the corresponding\
    \ computation is skipped. When sparsification occurs during training, the possible\
    \ accuracy loss is ameliorated by allowing the model to adapt to the removal of\
    \ certain weights.\n\nThe achieved sparsity can either be *unstructured* [\\[3\\\
    ]](#page-3-2), or *structured* [\\[4\\]](#page-3-3), [\\[5\\]](#page-3-4). In\
    \ unstructured sparsity, there is no constraint on the locations of the zeros,\
    \ as shown in Fig. [1\\(](#page-0-0)a). In this case, together with the non-zero\
    \ elements, multiple metadata indexes are also required to identify the original\
    \ position of each non-zero element.\n\nOn the contrary, in structured sparsity,\
    \ there is an upper limit on the number of non-zero elements that may be present\
    \ within a block of consecutive elements (other forms of structured sparsity are\
    \ also possible). For instance, in Fig. [1\\(](#page-0-0)b), for every 4 elements\
    \ in each row, there is up to one non-zero element. Such structured sparsity simplifies\
    \ both the indexing required to identify the position of each non-zero element\
    \ inside each block, and the hardware needed to operate on such sparse data. This\
    \ simplicity is of paramount importance to *lightweight* engines (the focus of\
    \ this work) found in mobile and embedded applications.\n\nChristodoulos Peltekis,\
    \ Vasileios Titopoulos and Giorgos Dimitrakopoulos are with the Department of\
    \ Electrical and Computer Engineering, Democritus University of Thrace, Xanthi,\
    \ Greece, (e-mail: cpeltekis@ee.duth.gr, vtitopou@ee.duth.gr, dimitrak@ee.duth.gr)\n\
    \nChrysostomos Nicopoulos is with the Department of Electrical and Computer Engineering\
    \ at the University of Cyprus, Nicosia, Cyprus (e-mail: nicopoulos@ucy.ac.cy).\n\
    \n![](_page_0_Figure_13.jpeg)\n\n<span id=\"page-0-0\"></span>Fig. 1. Examples\
    \ of (a) unstructured sparsity; (b) structured block sparsity of 1:4 (i.e., up\
    \ to 1 non-zero element in every 4 consecutive elements); and (c) relaxed structured\
    \ sparsity 4:16, and the corresponding packed representation of the non-zero elements.\
    \ A blue square indicates a non-zero element.\n\nIn most practical applications\
    \ [\\[4\\]](#page-3-3), [\\[6\\]](#page-3-5), [\\[7\\]](#page-3-6), blocks are\
    \ small and *fine-grained* N : M sparsity patterns of 1:2, 1:4 or 2:4 are supported,\
    \ where each block of M elements may contain up to N non-zero elements. Nevertheless,\
    \ while fine-grained structured sparsity promises high performance and low storage\
    \ overhead, it may also lead to less accurate ML models [\\[2\\]](#page-3-1),\
    \ [\\[8\\]](#page-3-7). This possible weakness is attributed to the constraints\
    \ imposed during the fine-grained sparsification, where a fixed amount of non-zero\
    \ elements is required for all consecutive small blocks.\n\nTo increase the flexibility\
    \ during model training, sparsity could refer to much coarser blocks [\\[9\\]](#page-3-8),\
    \ [\\[10\\]](#page-3-9). For instance, a *relaxed* (coarser) structured sparsity\
    \ of 8:128 allows the presence of at most 8 non-zero elements in every 128 consecutive\
    \ elements. Fig. [1\\(](#page-0-0)c) shows an example of 4:16 relaxed structured\
    \ sparsity, together with the packed representation of each row, which contains\
    \ the non-zero element values and their corresponding column indexes. Moving to\
    \ coarser blocks complicates the operation of the corresponding hardware modules\
    \ \u2013 e.g., systolic arrays \u2013 that operate optimally on well-structured\
    \ data with small block sizes.\n\nTo effectively reconcile these two conflicting\
    \ attributes of relaxed (coarser) sparsity vs. hardware complexity, this work\
    \ proposes a novel matrix-multiplication engine that supports *relaxed* structured\
    \ sparsity patterns, while still employing a simple and *decoupled* hardware organization.\
    \ Unlike conventional systolic arrays that colocate the Multiply-Accumulate (MAC)\
    \ and storage units within each tile, the proposed Decoupled Matrix-Multiplication\
    \ (DeMM) engine decouples the two. It essentially re-organizes the (dispersed)\
    \ memory portion of a traditional systolic array into a regular standardcell memory\
    \ structure with multiple read ports. This transformation enables the support\
    \ of relaxed structured sparsity and maintains the required regularities in the\
    \ data flow and the physical layout.\n\nOverall, the proposed DeMM engine provides\
    \ a two-fold benefit.\n\nThis work was supported by a research grant from Codasip,\
    \ a provider of customizable RISC-V IP and Codasip Studio design toolset, to DUTh.\n\
    \n![](_page_1_Figure_1.jpeg)\n\n<span id=\"page-1-0\"></span>Fig. 2. Read and\
    \ multiply operations are sufficient to perform matrix multiplication when the\
    \ sparse matrix contains at most one non-zero element per row.\n\nFirst, it enables\
    \ support for relaxed structured sparsity patterns that combine hardware simplicity\
    \ (similar to handling fine-grained sparsity) with additional flexibility during\
    \ DL model pruning [\\[2\\]](#page-3-1), [\\[8\\]](#page-3-7). Secondly, through\
    \ appropriate reconfiguration, the DeMM engine can also support denser sparsity,\
    \ which allows for the tackling of more common fine-grained structured sparsity\
    \ patterns [\\[4\\]](#page-3-3).\n\nThe experimental results demonstrate substantial\
    \ improvements in overall execution latency over state-of-the-art matrix engines\
    \ built to support fine-grained [\\[7\\]](#page-3-6) and relaxed structured sparsity\
    \ [\\[9\\]](#page-3-8), [\\[11\\]](#page-3-10) when executing structured-sparse\
    \ CNN models. It should be noted that, even though said approaches \u2013 including\
    \ the proposed DeMM engine \u2013 are effective for the low-sparsity levels of\
    \ DNNs (i.e., 10%-90%), they are not as efficient in high-sparsity levels of above\
    \ 95%. At such high-sparsity levels, other accelerator architectures perform better.\
    \ Examples include architectures following a non-systolic dataflow-like organization\
    \ [\\[12\\]](#page-3-11), [\\[13\\]](#page-3-12) and ones that optimize memory\
    \ traffic by operating either near-memory [\\[14\\]](#page-3-13), [\\[15\\]](#page-3-14),\
    \ or in-memory [\\[16\\]](#page-3-15).\n\n## II. SIMPLIFYING SPARSE\xD7DENSE MATRIX\
    \ MULTIPLICATION\n\nThe proposed DeMM engine employs a row-wise approach [\\[17\\\
    ]](#page-3-16) in computing the matrix product A \xD7 B. Matrix A follows a relaxed\
    \ structured sparsity template and B is dense. The product of the multiplication\
    \ is produced row-by-row, as follows:\n\n$$C[i,:] = \\sum\\_{k} A[i,k]B[k,:] \\\
    tag{1}$$\n\nAll the non-zero elements in a single row of matrix A should be multiplied\
    \ in parallel with the corresponding rows of matrix B, where the row index of\
    \ matrix B is determined by the column index of the non-zero value in matrix A.\n\
    \n## *A. The proposed DeMM engine*\n\nTo achieve the desired parallelism, we *decouple*\
    \ the *storage* (as used in a systolic array) from the *multiply-add* units and\
    \ treat each portion separately. Matrix B is assumed to be pre-loaded in the storage\
    \ area of DeMM. The pre-loading resembles the pre-loading operation of the input-\
    \ (or weight-) stationary dataflow applied in systolic arrays. In each cycle,\
    \ another row of B is written into the memory block, using the one available write\
    \ port. Subsequently, multiplication is computed row-by-row, by feeding the engine\
    \ with all the non-zero elements of each row of stuctured-sparse matrix A.\n\n\
    Let us initially assume the trivial case where each row of A consists of at most\
    \ one non-zero element. This element is passed to the engine, together with its\
    \ column index, one after the other. For each {value, column index} pair, multiplication\
    \ is performed in two serial steps, as depicted in Fig. [2.](#page-1-0) First,\
    \ the column index is treated as a row address to the memory that stores matrix\
    \ B. This address allows us to read out all elements of the corresponding row\
    \ of B. In the second\n\n![](_page_1_Figure_12.jpeg)\n\n<span id=\"page-1-1\"\
    ></span>Fig. 3. Multiplying a sparse matrix with at most two non-zero elements\
    \ per row requires two separate memory read ports and two rows of multipliers.\
    \ The products of each port are then independently added in parallel to form the\
    \ final result of the output row.\n\n![](_page_1_Figure_14.jpeg)\n\n<span id=\"\
    page-1-2\"></span>Fig. 4. The overall organization of a DeMM engine that supports\
    \ relaxed structured sparsity of 4:64 using a memory block of four read ports\
    \ and four multipliers and one add-reduction unit per output element. The example\
    \ assumes that 64 outputs (columns) are computed in parallel.\n\nstep, the read\
    \ elements are all multiplied in parallel with the value of the non-zero element\
    \ of A. Repeating these two steps for all rows of matrix A would complete the\
    \ multiplication. The hardware engine required in this trivial case of a single\
    \ non-zero element per row is just a memory block with 1 read port and a row of\
    \ multipliers.\n\nTo support more than one non-zero element per row, one must\
    \ simply increase the number of read ports in the memory block and, correspondingly,\
    \ the number of multipliers per read port. Adders are also needed for the final\
    \ addition. Fig. [3](#page-1-1) illustrates an example of the operation of the\
    \ proposed engine when operating with a row sparsity of two non-zero elements\
    \ per row. In this case, the pairs of nonzero elements of each row of matrix A\
    \ are sent in parallel to the multiplication engine, one after the other. Each\
    \ non-zero element is forwarded to its dedicated read port. The column index of\
    \ each nonzero element selects (reads) the corresponding row of matrix B, and\
    \ the value of each non-zero element is multiplied in parallel with all the elements\
    \ of the selected row. The two products generated at each read port are added\
    \ to finalize the result for this output row.\n\nIn the general case, matrix A\
    \ follows an N:M row-sparsity pattern, where M is much larger than N, e.g., N=8\
    \ and M=128, or M=256. Hence, the proposed DeMM engine consists of a regular memory\
    \ of N read and 1 write port. Each read port of DeMM's memory block outputs one\
    \ data item per column, as selected by the column index address ('col idx') ,\
    \ which points to a row of matrix B. Fig. [4](#page-1-2) depicts a complete DeMM\
    \ engine supporting 4:64 structured sparsity (N=4 and M=64). Each read port is\
    \ associated with a multiplier and the products of all read ports are reduced\
    \ to one sum. Summation at the bottom of Fig. [4](#page-1-2) is implemented as\
    \ a pipelined multi-operand adder\n\n![](_page_2_Figure_1.jpeg)\n\n<span id=\"\
    page-2-0\"></span>Fig. 5. The overall architecture of the DeMM engine that supports\
    \ an N:M relaxed structured sparsity and can be reconfigured for all kN:M denser\
    \ variants.\n\nof logarithmic depth.\n\n## *B. Supporting denser structured sparsity*\n\
    \nTo support denser structured sparsity, e.g., kN:M, in a reconfigurable manner,\
    \ DeMM should be able to read more than N non-zero elements from *the same* M\
    \ rows of matrix B. Since the memory block of each DeMM engine consists of N read\
    \ ports, it means that reading the kN non-zero elements of the same row of A requires\
    \ time-sharing of the N read ports for k\xD7 more cycles. To enable this sharing,\
    \ every read port is associated with a k-to-1 multiplexer. Note that, irrespective\
    \ of the exact structured sparsity pattern supported, the memory of the DeMM engine\
    \ is pre-loaded with the same M rows of matrix B. The value of k just determines\
    \ how many times this block would be read before completing the computation for\
    \ a row of A.\n\nThe overall organization of the DeMM engine is depicted in Fig.\
    \ [5.](#page-2-0) The value chosen for k reflects the reconfiguration properties\
    \ of the proposed engine. For instance, by selecting k=4 for the engine depicted\
    \ in Fig. [4,](#page-1-2) which operates by default on 4:64 relaxed structured\
    \ sparsity (N=4 and M=64), it means that all denser (and more fine-grained) structured\
    \ sparsities can also be supported by the proposed design, e.g., 4:32 (as 8:64),\
    \ 4:16 (as 16:64). Moving to an even denser scenario, such as 4:8, implemented\
    \ in DeMM as a 32:64 pattern, would need larger multiplexers at each read port,\
    \ i.e., k=8.\n\nTo identify the various design options, we define the DeMM(N,\
    \ M, C, k) configuration as the one that operates on a structured-sparse matrix\
    \ A of row size M (this is also the number of rows in matrix B) and a matrix B\
    \ with C columns. Structured sparsity can be as relaxed as N:M, or as dense as\
    \ kN:M. The corresponding hardware block requires N \xD7C multipliers, C N-to-1\
    \ reduction trees, and an M \xD7 C memory block of N read ports.\n\n## III. EVALUATION\n\
    \nThe effectiveness of the proposed decoupled matrix engine is evaluated by running\
    \ inference in state-of-the-art CNN applications [\\[18\\]](#page-3-17),\n\n[\\\
    [19\\]](#page-3-18) with inputs from ImageNet. In the first set of experiments,\
    \ we consider highly sparse CNN models derived from unstructured pruning. As reported\
    \ in [\\[2\\]](#page-3-1), [\\[8\\]](#page-3-7), unstructured pruning can achieve\
    \ higher model compaction with better performance, as compared to structured pruning.\
    \ In particular, we employ ResNet50 [\\[18\\]](#page-3-17), pruned with RigL [\\\
    [3\\]](#page-3-2) at 95% sparsity that roughly matches the relaxed sparsity of\
    \ 8:128 targeted by this work. Any rows exceeding the sparsity of 8:128 are computed\
    \ in multiple consecutive cycles. For completeness, in the second set of experiments,\
    \ we also include scenarios with fine-grained structured sparsity of 1:2, 1:4\
    \ and 1:8, derived with Tensorflow for ResNet50 [\\[18\\]](#page-3-17) and ConvNeXt\
    \ [\\[19\\]](#page-3-18).\n\n## *A. Relaxed structured sparsity*\n\nFor relaxed\
    \ row sparsity of 8:128, we compare DeMM with three state-of-the-art architectures:\
    \ (a) VEGETA [\\[9\\]](#page-3-8), which is able to support such sparsity degrees\
    \ in a structured form of 1:16 blocks, operating with a weight-stationary dataflow\
    \ [\\[20\\]](#page-3-19); (b) a version of S2TA [\\[7\\]](#page-3-6) configured\
    \ to support block density 1:16 using output stationarity; and (c) SPOTS [\\[11\\\
    ]](#page-3-10), which skips groups of weights and input data that consist of only\
    \ zero elements, following an outputstationary dataflow. Specifically, we compare\
    \ DeMM(8,128,64,8) following an input-stationary dataflow to VEGETA-S-4-2 [\\\
    [9\\]](#page-3-8), to S2TA-4\xD716\xD74 8\xD74 [\\[7\\]](#page-3-6), and to SPOTS\
    \ [\\[11\\]](#page-3-10), with *all* designs under evaluation having *the same\
    \ amount of computational resources of 512 multiply-add units*. The evaluated\
    \ VEGETA and S2TA designs have array sizes of 32 \xD7 16 PEs, while SPOTS has\
    \ an array size of 128 \xD7 4 that can be reconfigured as four 32\xD74 blocks\
    \ operating in parallel. The configuration selected is the one that offers the\
    \ best performance depending on the size of the input matrices.\n\nThe obtained\
    \ results are summarized in Fig. [6,](#page-2-1) which shows the execution latencies\
    \ of all CNN layers of ResNet50 [\\[18\\]](#page-3-17). DeMM's performance in\
    \ the first layers is not the best, but it substantially outperforms the other\
    \ three designs in the later layers. This behavior is the combined result of DeMM's\
    \ engine architecture and the size of the stationary matrices in each case [\\\
    [20\\]](#page-3-19). DeMM leads to an *overall* (across all CNN layers) latency\
    \ improvement of 18%, 54% and 67%, as compared to S2TA [\\[7\\]](#page-3-6), VEGETA\
    \ [\\[9\\]](#page-3-8), and SPOTS [\\[11\\]](#page-3-10), respectively.\n\n##\
    \ *B. Hardware complexity*\n\nAll four evaluated designs were implemented in SystemVerilog\
    \ and synthesized using the Cadence digital implementation flow and a 28 nm standard-cell\
    \ library. The designs operate on 16-bit integer quantized inputs and weights,\
    \ while the accumulations are performed with 32 bits. A block density of 8:128\
    \ is used, which is DeMM's primary target. However, the instance of DeMM that\
    \ is evaluated can also support more fine-grained patterns, down to the equivalent\
    \ of 1:2. The equivalent density for S2TA and VEGETA is 1:16. All designs under\
    \ evaluation operate at a clock frequency of 500 MHz at 0.88 V.\n\nFig. [7\\(](#page-3-20)a)\
    \ compares the hardware area of the four architectures. Compared to S2TA and VEGETA,\
    \ the DeMM engine requires 2.7% and 10.4%, respectively, lower area, which is\
    \ a testament to the\n\n![](_page_2_Figure_18.jpeg)\n\n<span id=\"page-2-1\"></span>Fig.\
    \ 6. The execution latencies of all CNN layers of ResNet50 [\\[18\\]](#page-3-17),\
    \ when using the proposed DeMM(8,128, 64, 8) design, the S2TA-4\xD716\xD74 8\xD7\
    4 design [\\[7\\]](#page-3-6), the VEGETA-S-4-2 design [\\[9\\]](#page-3-8), and\
    \ the SPOTS design [\\[11\\]](#page-3-10). All three architectures have equal\
    \ amount of computational resources.\n\n![](_page_3_Figure_1.jpeg)\n\n<span id=\"\
    page-3-20\"></span>Fig. 7. Hardware (a) area, and (b) power consumption comparisons\
    \ between DeMM(8,128,64,8), S2TA-4\xD716\xD74 8\xD74 [\\[7\\]](#page-3-6), VEGETA-S-4-2\
    \ [\\[9\\]](#page-3-8) and SPOTS [\\[11\\]](#page-3-10). DeMM and SPOTS use a\
    \ block density of 8:128; S2TA and VEGETA use the equivalent 1:16.\n\nsimplicity\
    \ of its organization. DeMM is slightly larger than SPOTS (the overhead is less\
    \ than 10%), due to the additional multiplexing logic required for supporting\
    \ reconfigurability and multi-porting. Each additional read port added to the\
    \ 128\xD764 standard-cell-based memory block used in this DeMM setup costs 16%\
    \ more area.\n\nIn terms of power consumption, DeMM demonstrates significantly\
    \ better behavior than the other designs. As shown in Fig. [7\\(](#page-3-20)b),\
    \ DeMM consumes 36.4%, 45.8% and 56.1% lower power than SPOTS, S2TA, and VEGETA,\
    \ respectively. This substantial power reduction is mainly attributed to the minimization\
    \ of data movement in pipeline registers. Both S2TA and VEGETA have an input data\
    \ demand of a multiple-of-M inputs and C weights, while DeMM has a much lower\
    \ input data demand of C inputs and kN weights, for a kN:M structured sparsity,\
    \ along with their addresses. The extra hardware cost of VEGETA relative to S2TA\
    \ stems from its reconfiguration-rich organization that is also offered by the\
    \ DeMM engine. SPOTS has a low multiplexing overhead that reduces its area requirements.\
    \ However, its deeply pipelined operation increases its power consumption relative\
    \ to DeMM.\n\n## *C. Fine-grained structured sparsity*\n\nFurthermore, we also\
    \ compare DeMM to VEGETA and S2TA in use-cases that better fit these two architectures,\
    \ i.e., in scenarios with fine-grained block densities. The designs are evaluated\
    \ using ResNet50 [\\[18\\]](#page-3-17) and ConvNeXt [\\[19\\]](#page-3-18). The\
    \ selected workloads are pruned to fine-grained structured block sparsities of\
    \ 1:2, 1:4 and 1:8 to ensure optimal conditions for both VEGETA and S2TA, even\
    \ if this choice is not the best option for DeMM, which inherently supports a\
    \ wide range of sparsity formats. SPOTS is omitted in this comparison, since \u2013\
    \ under such fine-grained structured sparsity \u2013 it is very difficult to find\
    \ contiguous groups of zero data, as required by SPOTS. Consequently, SPOTS exhibits\
    \ significantly higher latencies.\n\nThe results are shown in Fig. [8.](#page-3-21)\
    \ DeMM engine *outperforms both* S2TA and VEGETA architectures in terms of overall\
    \ latency. Specifically, for block sparsity 1:8, DeMM achieves average latency\
    \ improvements of 29% and 39%, as compared to S2TA and VEGETA, respectively. For\
    \ block sparsity 1:4, DeMM's respective improvement in average latency is 19%\
    \ and 12%. Finally, for block sparsity 1:2, DeMM still yields 14% and 5% average\
    \ latency improvements, as compared to S2TA and VEGETA, respectively.\n\n## IV.\
    \ CONCLUSIONS\n\nThis paper proposes DeMM, a matrix-multiplication engine that\
    \ natively supports *relaxed* structured sparsity patterns, without sacrificing\
    \ the simplicity and regularity of the hardware organization. Contrary to conventional\
    \ systolic arrays, the DeMM design employs a disaggregated micro-architecture\
    \ that decouples the memory elements from the MAC units. Fine-grained sparsity\
    \ patterns are also supported\n\n![](_page_3_Figure_10.jpeg)\n\n<span id=\"page-3-21\"\
    ></span>Fig. 8. Overall execution latencies of ResNet50 [\\[18\\]](#page-3-17)\
    \ and ConvNeXt [\\[19\\]](#page-3-18) for configurations with block densities\
    \ of 1:8, 1:4 and 1:2.\n\nwith minimal reconfiguration. The experimental evaluation\
    \ demonstrates substantial improvements in the overall execution latency of modern\
    \ CNN workloads, as compared to two existing state-ofthe-art architectures [\\\
    [9\\]](#page-3-8), [\\[7\\]](#page-3-6), [\\[11\\]](#page-3-10) for structured\
    \ sparse workloads. Most importantly, these performance improvements are achieved\
    \ with markedly lower power consumption.\n\n## REFERENCES\n\n- <span id=\"page-3-0\"\
    ></span>[1] S. Han *et al.*, \"Deep compression: Compressing deep neural networks\
    \ with pruning, trained quantization and huffman coding,\" in *Inter. Conf. on\
    \ Learning Representations (ICLR)*, 2016.\n- <span id=\"page-3-1\"></span>[2]\
    \ T. Hoefler *et al.*, \"Sparsity in deep learning: Pruning and growth for efficient\
    \ inference and training in neural networks,\" *The Journal of Machine Learning\
    \ Research*, vol. 22, no. 1, pp. 10 882\u201311 005, 2021.\n- <span id=\"page-3-2\"\
    ></span>[3] U. Evci *et al.*, \"Rigging the lottery: Making all tickets winners,\"\
    \ in *Inter. Conf. on Machine Learning*, Jul. 2020, pp. 2943\u20132952.\n- <span\
    \ id=\"page-3-3\"></span>[4] A. Mishra *et al.*, \"Accelerating sparse deep neural\
    \ networks,\" *arXiv preprint arXiv:2104.08378*, 2021.\n- <span id=\"page-3-4\"\
    ></span>[5] A. Zhou *et al.*, \"Learning N:M fine-grained structured sparse neural\
    \ networks from scratch,\" in *Inter. Conf. on Learning Representations (ICLR)*,\
    \ May 2021.\n- <span id=\"page-3-5\"></span>[6] Z.-G. Liu *et al.*, \"Systolic\
    \ tensor array: An efficient structured-sparse gemm accelerator for mobile CNN\
    \ inference,\" *IEEE Computer Architecture Letters*, vol. 19, no. 1, pp. 34\u2013\
    37, 2020.\n- <span id=\"page-3-6\"></span>[7] \u2014\u2014, \"S2TA: Exploiting\
    \ structured sparsity for energy-efficient mobile CNN acceleration,\" in *IEEE\
    \ Inter. Symp. on High-Performance Computer Architecture (HPCA)*, Apr. 2022, pp.\
    \ 573\u2013586.\n- <span id=\"page-3-7\"></span>[8] M. Zhu *et al.*, \"Sparse\
    \ tensor core: Algorithm and hardware co-design for vector-wise sparse neural\
    \ networks on modern gpus,\" in *IEEE/ACM Inter. Symp. on Microarchitecture (MICRO)*,\
    \ Oct. 2019, pp. 359\u2013371.\n- <span id=\"page-3-8\"></span>[9] G. Jeong *et\
    \ al.*, \"Vegeta: Vertically-integrated extensions for sparse/dense gemm tile\
    \ acceleration on cpus,\" in *IEEE Inter. Symp. on High-Performance Computer Architecture\
    \ (HPCA)*, Feb. 2023, pp. 259\u2013272.\n- <span id=\"page-3-9\"></span>[10] S.\
    \ Muralidharan, \"Uniform sparsity in deep neural networks,\" *Proc. of Machine\
    \ Learning and Systems*, vol. 5, 2023.\n- <span id=\"page-3-10\"></span>[11] M.\
    \ Soltaniyeh *et al.*, \"An accelerator for sparse convolutional neural networks\
    \ leveraging systolic general matrix-matrix multiplication,\" *ACM Trans. on Arch.\
    \ and Code Opt. (TACO)*, vol. 19, no. 3, pp. 1\u201326, 2022.\n- <span id=\"page-3-11\"\
    ></span>[12] O. Hsu *et al.*, \"The sparse abstract machine,\" in *ACM Inter.\
    \ Conf. on Architectural Support for Programming Languages and Operating Systems\
    \ (ASPLOS)*, 2023, pp. 710\u2013726.\n- <span id=\"page-3-12\"></span>[13] G.\
    \ Zhang *et al.*, \"Gamma: Leveraging gustavson's algorithm to accelerate sparse\
    \ matrix multiplication,\" in *ACM Inter. Conf. on Architectural Support for Programming\
    \ Languages and Operating Systems (ASPLOS)*, 2021, pp. 687\u2013701.\n- <span\
    \ id=\"page-3-13\"></span>[14] G. Gerogiannis *et al.*, \"Spade: A flexible and\
    \ scalable accelerator for spmm and sddmm,\" in *Inter. Symp. on Computer Arch.\
    \ (ISCA)*, 2023.\n- <span id=\"page-3-14\"></span>[15] S. Feng *et al.*, \"MeNDA:\
    \ a near-memory multi-way merge solution for sparse transposition and dataflows,\"\
    \ in *Inter. Symp. on Comp. Arch. (ISCA)*, 2022, pp. 245\u2013258.\n- <span id=\"\
    page-3-15\"></span>[16] Z. Li *et al.*, \"RRAM-DNN: An RRAM and model-compression\
    \ empowered all-weights-on-chip DNN accelerator,\" *IEEE Journal of Solid-State\
    \ Circuits*, vol. 56, no. 4, pp. 1105\u20131115, 2020.\n- <span id=\"page-3-16\"\
    ></span>[17] N. Srivastava *et al.*, \"Matraptor: A sparse-sparse matrix multiplication\
    \ accelerator based on row-wise product,\" in *IEEE/ACM Inter. Symp. on Microarchitecture\
    \ (MICRO)*, Oct. 2020, pp. 766\u2013780.\n- <span id=\"page-3-17\"></span>[18]\
    \ K. He *et al.*, \"Deep residual learning for image recognition,\" in *IEEE CVPR*,\
    \ 2016, pp. 770\u2013778.\n- <span id=\"page-3-18\"></span>[19] Z. Liu *et al.*,\
    \ \"A ConvNet for the 2020s,\" in *IEEE Conf. on Comp. Vision and Pattern Recognition\
    \ (CVPR)*, 2022, pp. 11 976\u201311 986.\n- <span id=\"page-3-19\"></span>[20]\
    \ A. Samajdar *et al.*, \"Scale-sim: Systolic CNN accelerator simulator,\" *arXiv\
    \ preprint arXiv:1811.02883*, 2018."
  paper_id: 8
  scores:
    originality:
      raw: 4.5
      reason: The introduction of LRwait and SCwait as a novel atomic RMW instruction
        pair presents a significant advancement in synchronization methods for manycore
        systems.
    technical_soundness:
      raw: 4.0
      reason: The proposed methods are technically sound and build upon established
        principles, although further empirical validation may be needed.
    clarity:
      raw: 4.0
      reason: The writing is generally clear, but some sections could benefit from
        more detailed explanations of complex concepts.
    significance:
      raw: 4.5
      reason: The potential impact on performance and energy efficiency in manycore
        systems is substantial, addressing a critical issue in concurrent programming.
    relevance:
      raw: 5.0
      reason: The paper is highly relevant to current research in computer architecture
        and synchronization, particularly in the context of manycore systems.
  total_score: 4.3
  tier: A
- title: "An Irredundant and Compressed Data Layout to Optimize Bandwidth\n  Utilization\
    \ of FPGA Accelerators"
  abstract: "Batteryless energy harvesting systems enable a wide array of new sensing,\n\
    computation, and communication platforms untethered by power delivery or\nbattery\
    \ maintenance demands. Energy harvesters charge a buffer capacitor from\nan unreliable\
    \ environmental source until enough energy is stored to guarantee a\nburst of\
    \ operation despite changes in power input. Current platforms use a\nfixed-size\
    \ buffer chosen at design time to meet constraints on charge time or\napplication\
    \ longevity, but static energy buffers are a poor fit for the highly\nvolatile\
    \ power sources found in real-world deployments: fixed buffers waste\nenergy both\
    \ as heat when they reach capacity during a power surplus and as\nleakage when\
    \ they fail to charge the system during a power deficit.\n  To maximize batteryless\
    \ system performance in the face of highly dynamic\ninput power, we propose REACT:\
    \ a responsive buffering circuit which varies\ntotal capacitance according to\
    \ net input power. REACT uses a variable capacitor\nbank to expand capacitance\
    \ to capture incoming energy during a power surplus\nand reconfigures internal\
    \ capacitors to reclaim additional energy from each\ncapacitor as power input\
    \ falls. Compared to fixed-capacity systems, REACT\ncaptures more energy, maximizes\
    \ usable energy, and efficiently decouples system\nvoltage from stored charge\
    \ -- enabling low-power and high-performance designs\npreviously limited by ambient\
    \ power. Our evaluation on real-world platforms\nshows that REACT eliminates the\
    \ tradeoff between responsiveness, efficiency,\nand longevity, increasing the\
    \ energy available for useful work by an average\n25.6% over static buffers optimized\
    \ for reactivity and capacity, improving\nevent responsiveness by an average 7.7x\
    \ without sacrificing capacity, and\nenabling programmer directed longevity guarantees."
  url: http://arxiv.org/abs/2401.08806v1
  keywords: ''
  document: "# Energy-adaptive Buffering for Efficient, Responsive, and Persistent\
    \ Batteryless Systems\n\nHarrison Williams Virginia Tech hrwill@vt.edu\n\nMatthew\
    \ Hicks Virginia Tech mdhicks2@vt.edu\n\n## Abstract\n\nBatteryless energy harvesting\
    \ systems enable a wide array of new sensing, computation, and communication platforms\
    \ untethered by power delivery or battery maintenance demands. Energy harvesters\
    \ charge a buffer capacitor from an unreliable environmental source until enough\
    \ energy is stored to guarantee a burst of operation despite changes in power\
    \ input. Current platforms use a fixed-size buffer chosen at design time to meet\
    \ constraints on charge time or application longevity, but static energy buffers\
    \ are a poor fit for the highly volatile power sources found in real-world deployments:\
    \ fixed buffers waste energy both as heat when they reach capacity during a power\
    \ surplus and as leakage when they fail to charge the system during a power deficit.\n\
    \nTo maximize batteryless system performance in the face of highly dynamic input\
    \ power, we propose REACT: a responsive buffering circuit which varies total capacitance\
    \ according to net input power. REACT uses a variable capacitor bank to expand\
    \ capacitance to capture incoming energy during a power surplus and reconfigures\
    \ internal capacitors to reclaim additional energy from each capacitor as power\
    \ input falls. Compared to fixed-capacity systems, REACT captures more energy,\
    \ maximizes usable energy, and efficiently decouples system voltage from stored\
    \ charge\u2014enabling low-power and high-performance designs previously limited\
    \ by ambient power. Our evaluation on real-world platforms shows that REACT eliminates\
    \ the tradeoff between responsiveness, efficiency, and longevity, increasing the\
    \ energy available for useful work by an average 25.6% over static buffers optimized\
    \ for reactivity and capacity, improving event responsiveness by an average 7.7\
    \ without sacrificing capacity, and enabling programmer directed longevity guarantees.\n\
    \n## <span id=\"page-0-0\"></span>1 Introduction\n\nEver-shrinking computing and\
    \ sensing hardware has pushed mobile Internet-of-Things (IoT) type devices beyond\
    \ the limits of the batteries powering them. A typical low cost/power microcontroller\
    \ [\\[19\\]](#page-11-0) drains a 1 <sup>3</sup> battery nearly 14x its size in\
    \ just over 8 weeks of active operation [\\[36\\]](#page-12-0), rendering the\
    \ system useless without a potentially costly replacement effort. Cost, maintenance,\
    \ and safety concerns make batteries further incompatible with massive-scale (one\
    \ million devices per square kilometer [\\[5\\]](#page-11-1)) and deeply-deployed\
    \ (infrastructure [\\[2\\]](#page-11-2), healthcare [\\[34\\]](#page-12-1)) applications.\
    \ IoT engineers are turning to batteryless energy harvesting platforms\n\nto power\
    \ low-cost, perpetual systems capable of driving a ubiquitous computing revolution.\
    \ Increasingly efficient energy harvesting circuits enable batteryless systems\
    \ across a range of IoT use cases including feature-rich batteryless temperature\
    \ sensors 500x smaller than a grain of rice [\\[47\\]](#page-12-2) and batteryless\
    \ flow-meters [\\[13\\]](#page-11-3) supporting deep-sea drilling or geothermal\
    \ plants for decades without maintenance.\n\nThe energy harvesting design model\
    \ both enables new deployments previously limited by batteries and places new\
    \ demands on system developers. Harvested energy is highly unreliable: sensitive\
    \ environmental factors such as shadows over a photovoltaic cell or shifts in\
    \ the orientation of a rectenna produce rapid, outsized changes in the energy\
    \ scavenged by the harvester. Energy harvesters mitigate this unreliability by\
    \ charging an energy buffer to a given enable voltage, which the system periodically\
    \ discharges to supply a useful quantum of work despite potential power loss.\n\
    \nBuffer capacity is a key design element of any batteryless system. Past work\
    \ [\\[7\\]](#page-11-4) explores the tradeoff between buffer sizes: small buffers\
    \ are highly reactive\u2014charging rapidly and quickly enabling the system to\
    \ address time-sensitive events\u2014but sacrifice longevity because they rapidly\
    \ discharge during operation, guaranteeing only a short burst of uninterrupted\
    \ operation. Large buffers store more energy at a given voltage, improving longevity\
    \ by supporting a longer or more energy-intensive burst of operation at the cost\
    \ of reactivity because they require more energy to enable the system at all.\
    \ Matching buffer size to projected energy demand is critical to ensuring the\
    \ system is both reactive enough to address incoming events/deadlines (e.g., periodic\
    \ sensor readings) and long-lived enough to support uninterruptible operations\
    \ (e.g., radio transmissions). Designers choose the minimum size necessary to\
    \ power all atomic operations on the device, maximizing reactivity given a required\
    \ level of longevity.\n\nIn this work, we explore static energy buffer efficiency\
    \ as a third metric for buffer performance and find that it varies dramatically\
    \ with net energy input rather than simple energy demand. Small buffers reach\
    \ capacity quickly if power input exceeds instantaneous demand\u2014burning off\
    \ hard-won energy as heat to prevent overvoltage. Large buffers capture all incoming\
    \ power, but enable slowly and lose more harvested energy to leakage below the\
    \ minimum system voltage. The volatile nature of harvested power means that fixed-size\
    \ buffers experience both problems over the course of their\n\ndeployment, discharging\
    \ energy during a power surplus and losing large portions of energy to leakage\
    \ during a deficit.\n\nTo make the most of incoming energy in all circumstances,\
    \ we propose REACT[1](#page-1-0) : a dynamic energy buffering system that varies\
    \ its capacitance following changes in net power. REACT maximizes system responsiveness\
    \ and efficiency using a small static buffer capacitor, quickly enabling the system\
    \ to monitor events or do other low-power work under even low input power. If\
    \ input power rises beyond the current system demand and the static buffer approaches\
    \ capacity, REACT connects additional capacitor banks to absorb the surplus, yielding\
    \ the capacity benefits of large buffers without the responsiveness penalty. When\
    \ net power is negative, these capacitors hold the system voltage up and extend\
    \ operation beyond what is possible using the small static capacitor.\n\nWhile\
    \ expanding buffer size to follow net power input ensures the system can capture\
    \ all incoming energy, increasing capacitance also increases the amount of unusable\
    \ charge stored on the capacitor banks\u2014charge which could power useful work\
    \ if it were on a smaller capacitor and therefore available at a higher voltage.\
    \ As supply voltage falls and approaches a minimum threshold, REACT reclaims this\
    \ otherwise-unavailable energy by reconfiguring capacitor banks into series, shifting\
    \ the same amount of charge onto a smaller equivalent capacitance in order to\
    \ boost the voltage at the buffer output and ensure the system continues operating\
    \ for as long as possible. REACT effectively eliminates the design tradeoff between\
    \ reactivity and capacity by tuning buffer size within an arbitrarily large capacitance\
    \ range, only adding capacity when the buffer is already near full. REACT's charge\
    \ reclamation techniques maximize efficiency by moving charge out of large capacitor\
    \ banks onto smaller ones when net input power is negative, ensuring all energy\
    \ is available for useful work.\n\nWe integrate a hardware prototype of REACT\
    \ into a full energy harvesting platform to evaluate it against previous work,\
    \ operating under different input power conditions and with different power consumption\
    \ profiles. Our results indicate that REACT provides the \"best of both worlds\"\
    \ of both smalland large-buffer systems, rapidly reaching the operational voltage\
    \ under any power conditions while also expanding as necessary to capture all\
    \ available energy and provide software longevity guarantees as needed. Maximizing\
    \ buffer capacity and reclaiming charge using REACT's reconfigurable capacitor\
    \ banks eliminates the efficiency penalties associated with both small and large\
    \ static capacitor buffers, increasing the portion of harvested energy used for\
    \ application code by an average 39% over an equally-reactive static buffer and\
    \ 19% over an equal-capacity one. Compared to prior work exploring dynamic capacitance\
    \ for batteryless systems [\\[49\\]](#page-12-3), REACT improves performance by\
    \ an average 26% owing to\n\nits efficient charge management structure. This paper\
    \ makes the following technical contributions:\n\n- We evaluate the power dynamics\
    \ of common batteryless systems in real deployments and explore how common-case\
    \ volatility introduces significant energy waste in static or demand-driven buffers\
    \ (\xA7 [2\\)](#page-1-1).\n- We design REACT, a dynamic buffer system which varies\
    \ its capacitance according to system needs driven by net input power (\xA7 [3\\\
    )](#page-3-0). REACT's configurable arrays combine the responsiveness of small\
    \ buffers with the longevity and capacity of large ones, enables energy reclamation\
    \ to make the most of harvested power, and avoids the pitfalls of energy waste\
    \ inherent in other dynamic capacitance designs (\xA7 [3.3\\)](#page-4-0).\n-\
    \ We integrate REACT into a batteryless system and evaluate its effect on reactivity,\
    \ longevity, and efficiency under a variety of power conditions and workloads\
    \ (\xA7 [5\\)](#page-8-0). Our evaluation indicates that REACT eliminates the\
    \ responsiveness-longevity tradeoff inherent in static buffer design while increasing\
    \ overall system efficiency compared to any static system.\n\n## <span id=\"page-1-1\"\
    ></span>2 Background and Related Work\n\nScaling sensing, computation, and communication\
    \ down to smart dust [\\[23\\]](#page-11-5) dimensions requires harvesting power\
    \ ondemand rather than packaging energy with each device using a battery. Many\
    \ batteryless systems use photovoltaic [\\[9,](#page-11-6) [47\\]](#page-12-2)\
    \ or RF power [\\[40\\]](#page-12-4), while other use-cases are better suited\
    \ for sources such as vibration [\\[41\\]](#page-12-5), fluid flow [\\[13\\]](#page-11-3),\
    \ or heat gradients [\\[27\\]](#page-11-7). Commonalities between ambient power\
    \ sources have inspired researchers to develop general-purpose batteryless systems,\
    \ regardless of the actual power source: ambient power is unpredictable, dynamic,\
    \ and often scarce relative to the active power consumption of the system.\n\n\
    Batteryless systems isolate sensing and actuation components from volatile power\
    \ using a buffer capacitor. The harvester charges the capacitor to a pre-defined\
    \ enable voltage, after which the system turns on and begins consuming power.\
    \ Because many environmental sources cannot consistently power continuous execution,\
    \ systems operate intermittently\u2014draining the capacitor in short bursts of\
    \ operation punctuated by long recharge periods. This generalpurpose intermittent\
    \ operation model has enabled researchers to abstract away the behavior of incoming\
    \ power and focus on developing correct and efficient techniques for working under\
    \ intermittent power [\\[17,](#page-11-8) [26,](#page-11-9) [28,](#page-11-10)\
    \ [29,](#page-11-11) [44,](#page-12-6) [46\\]](#page-12-7).\n\n## 2.1 Choosing\
    \ Buffer Capacity\n\nBuffer size determines system behavior in several important\
    \ ways. Supercapacitors provide inexpensive and small-formfactor bulk capacitance\
    \ [\\[24\\]](#page-11-12), enabling designers to choose a capacitor according\
    \ to performance rather than cost or size concerns. Two metrics motivate past\
    \ work: reactivity refers\n\n<span id=\"page-1-0\"></span><sup>1</sup>Reconfigurable,\
    \ Energy-Adaptive CapaciTors\n\n<span id=\"page-2-0\"></span>![](_page_2_Figure_0.jpeg)\n\
    \nFigure 1. Static buffer operation on a simulated solar harvester. Highlighted\
    \ blocks indicate the system is running.\n\nto the system's ability to rapidly\
    \ charge to its enable voltage and begin operation. High reactivity ensures a\
    \ system is online to execute periodic tasks or address unpredictable input events.\
    \ Longevity refers to the energy available for an uninterrupted period of work\
    \ with no additional power input; long-lived systems support high-power and long-running\
    \ uninterruptible operations and reduce the overhead incurred with state recovery\
    \ after a power loss.\n\n2.1.1 Reactivity and Longevity: A batteryless system's\
    \ reactivity and longevity depend primarily on the charge and discharge rate of\
    \ the buffer capacitor. We illustrate the tradeoff using a simulated solar harvester\
    \ with a 22% efficient, 5 cm<sup>2</sup> panel, based on a pedestrian trace from\
    \ the EnHANTs solar dataset [\\[12\\]](#page-11-13). The system runs from 3.6V\
    \ down to 1.8V and draws 1.5 mA in active mode, representative of a typical deployment\
    \ [\\[25\\]](#page-11-14). Figure [1](#page-2-0) illustrates the reactivity-longevity\
    \ tradeoff inherent in static buffer systems at two design extremes, using a 1\
    \ mF and 300 mF capacitor. The 1 mF system charges rapidly and is therefore highly\
    \ reactive, reaching the enable voltage over 8 sooner than the 300 mF version.\
    \ However, the smaller capacitor also discharges quickly\u2014the mean length\
    \ of an uninterrupted power cycle using the 1 mF capacitor is 10 seconds versus\
    \ 880 seconds for the 300 mF capacitor, indicating the 300 mF system is far longer-lived\
    \ once charged. The relative importance of reactivity and longevity depends on\
    \ the use case, but often changes over time for a complex system\u2014complicating\
    \ design further.\n\n2.1.2 Power Volatility and Energy Efficiency: Buffer capacity\
    \ is also a major driver of end-to-end energy efficiency: using the 300 mF capacitor\
    \ our system is operational for 49% of the overall power trace, compared to only\
    \ 27% for the 1 mF platform. This stems from the high volatility of incoming power\u2014\
    82% of the total energy input is collected during\n\nshort-duration power spikes\
    \ when harvested power rises above 10 mW, despite the system spending 77% of its\
    \ time at input powers below 3 mW. A large buffer captures this excess energy\
    \ to use later while the smaller buffer quickly reaches capacity and discharges\
    \ energy as heat to avoid overvoltage.\n\nLarge buffers, however, are not always\
    \ more efficient: the energy used to charge the capacitor to the operational voltage\
    \ cannot power useful work, and is eventually lost to leakage while the system\
    \ is off. When power input is low, this \"cold-start\" energy represents a significant\
    \ portion of total harvested energy. For the system described above powered by\
    \ a solar panel at night [\\[12\\]](#page-11-13), the 1 mF buffer enables a duty\
    \ cycle of 5.7% versus only 3.3% using a 10 mF buffer. This low power environment\
    \ highlights another risk of oversized buffers: the system using the 300 mF capacitor\
    \ never reaches the enable voltage and so never begins operation.\n\nImprovements\
    \ in harvester efficiency and low-power chip design are closing the gap between\
    \ harvester output and chip power consumption. Power is increasingly limited by\
    \ volatile environmental factors rather than scarcity induced by low efficiency;\
    \ the result is that energy harvesters experience periods of both energy scarcity\
    \ and surplus. Rapidly changing power conditions place opposing demands on batteryless\
    \ systems, which must remain responsive with low input power, provide longevity\
    \ for long-running operations, and maximize efficiency by avoiding energy waste.\n\
    \n## 2.2 Power-Responsive Performance Scaling\n\nOne solution to volatile energy\
    \ input is modulating throughput according to incoming power, increasing execution\
    \ rate when power is plentiful and decreasing it to maintain availability when\
    \ power is scarce. Limiting net input power to the buffer by matching power consumption\
    \ with input enables systems to use small buffer capacitors without reaching the\
    \ buffer capacity, ensuring no power is wasted with an over-full buffer. Past\
    \ work realizes power-responsive scaling using heterogeneous architectures [\\\
    [10\\]](#page-11-15) or by adapting the rate and accuracy of software execution\
    \ [\\[1,](#page-11-16) [4,](#page-11-17) [30,](#page-12-8) [48\\]](#page-12-9).\n\
    \nUnfortunately, we find the assumptions underlying powerperformance scaling often\
    \ do not apply to batteryless systems. Increasing energy consumption by accelerating\
    \ execution only improves systems which have useful work to do exactly when input\
    \ power is high, but many batteryless systems focus on periodic sensing and actuation\
    \ deadlines which do not correlate with ambient power supply. Further, resource-constrained\
    \ platforms may have few on-chip operations which can be delayed until power is\
    \ plentiful; when these operations do exist, they are often not amenable to scaling\
    \ (e.g., transmitting data to a base station may be delayed but always requires\
    \ a fixed-cost radio operation). Flexible batteryless systems must capture energy\
    \ and use it on demand rather than fit operation to unreliable power input.\n\n\
    ## 2.3 Multiplexed Energy Storage\n\nRather than match power consumption to incoming\
    \ supply, systems may charge multiple static buffers according to projected demand.\
    \ Capybara [\\[7\\]](#page-11-4) switches capacitance using an array of heterogeneous\
    \ buffers: programmers set capacitance modes throughout the program, using a smaller\
    \ capacitor to maximize responsiveness for low-power or interruptible tasks and\
    \ switching to a larger capacitor for high-power atomic operations. UFoP and Flicker\
    \ [\\[15,](#page-11-18) [16\\]](#page-11-19) assign each peripheral on the system\
    \ a separate buffer and charging priority, enabling responsive low-power operation\
    \ while waiting to collect sufficient energy for high-power tasks. These systems\
    \ increase overall energy capacity by directing excess power to capacitors not\
    \ currently in use.\n\nStatic arrays increase capacity without reducing responsiveness,\
    \ but waste energy when charge is stored on unused buffers. Reserving energy in\
    \ secondary capacitors 1) requires error-prone [\\[30\\]](#page-12-8) speculation\
    \ about future energy supply and demand to decide charging priorities, which can\
    \ change between when energy is harvested and when it needs to be used; and 2),\
    \ wastes energy as leakage when secondary buffers are only partially charged,\
    \ failing to enable associated systems and keeping energy from higher-priority\
    \ work. To minimize programmer speculation, decouple tasks which compete for buffered\
    \ energy, and minimize leakage, energy must be fungible: the buffer must be capable\
    \ of directing all harvested energy to any part of the system on demand.\n\n##\
    \ 2.4 Unified Dynamic Buffering\n\nPast work has also explored varying the behavior\
    \ of a single unified buffer to capture the fungibility requirement described\
    \ above. Dewdrop [\\[6\\]](#page-11-20) varies the enable voltage to draw from\
    \ a single capacitor according to projected needs (e.g., begin operation at 2.2V\
    \ instead of 3.6V)\u2014providing complete energy fungibility\u2014but still suffers\
    \ from the reactivitylongevity tradeoff of capacitor size. Morphy [\\[49\\]](#page-12-3)\
    \ replaces static buffers using a set of capacitors in a unified switching network;\
    \ software can connect and disconnect arbitrary sets of capacitors in series or\
    \ parallel to produce different equivalent capacitances. Morphy addresses two\
    \ challenges facing batteryless systems: fast charging from low-voltage sources\
    \ by operating the capacitor network as a charge pump, and charge isolation to\
    \ allocate set amounts of energy to tasks and prevent buggy code from discharging\
    \ the entire buffer by isolating parts of the capacitor array.\n\nWe evaluate\
    \ REACT alongside Morphy because the Morphy architecture can also be used to target\
    \ the reactivity and longevity challenges discussed in \xA7 [1.](#page-0-0) By\
    \ starting with a small equivalent capacitance, the system rapidly reaches an\
    \ operational voltage. If the buffer as configured reaches capacity, software\
    \ can gradually reconfigure the capacitor network to increase capacitance. Charge\
    \ flows between the capacitors to equalize the voltage on the capacitor array\
    \ and reduce\n\n<span id=\"page-3-1\"></span>![](_page_3_Figure_6.jpeg)\n\nFigure\
    \ 2. REACT diagram and signal flow between components.\n\nthe output voltage of\
    \ the network, enabling the bank to harvest more energy without reaching capacity.\
    \ However, this current flow between capacitors to equalize voltage during reconfiguration\
    \ dissipates a significant amount of the energy stored in the network (we explore\
    \ these energy dynamics in detail in \xA7 [3.3.1\\)](#page-4-1). Our evaluation\
    \ in \xA7 [5](#page-8-0) shows that this internal power dissipation reduces common-case\
    \ end-to-end performance to below that of systems using appropriatelysized static\
    \ capacitors, making this approach impractical for energy-constrained devices.\
    \ An energy-focused approach prioritizing minimal power dissipation is key to\
    \ developing intermittent systems that can simultaneously maximize reactivity,\
    \ longevity, and overall efficiency.\n\n## <span id=\"page-3-0\"></span>3 Design\n\
    \nAn intelligent energy buffering strategy is key to effective and efficient batteryless\
    \ systems. Three performance objectives, informed by the advantages and limitations\
    \ of prior approaches, drive REACT's design:\n\n- Minimize charge time: Rapidly\
    \ reaching the operational voltage, even when buffered energy cannot support complex\
    \ operation, maximizes reactivity and enables systems to reason about power or\
    \ sensor events from within low-power sleep modes.\n- Maximize capacity: System-wide\
    \ longevity and efficiency require buffering large amounts of incoming energy\
    \ when power supply exceeds demand, either to power long-running uninterruptible\
    \ operations or support future power demand when supply is low.\n- Maximize energy\
    \ fungibility: Unpredictable power demand patterns mean that energy cannot be\
    \ preprovisioned to specific operations at harvest-time; systems need the ability\
    \ to draw all harvested energy from the buffer and direct it as needed.\n\n##\
    \ 3.1 REACT Overview\n\nREACT buffers energy using a fabric of reconfigurable\
    \ capacitor banks that software adjusts as needed. Figure [2](#page-3-1) shows\
    \ a\n\nhigh-level overview of REACT's hardware design. We design REACT's hardware\
    \ component as a drop-in replacement for a typical buffer between the harvester\
    \ and the rest of the system, while the buffer management software requires no\
    \ code modification or programmer input. The only system prerequisite is a set\
    \ of digital I/O pins to configure capacitor banks and receive voltage monitoring\
    \ information.\n\n## 3.2 Cold-start Operation and the Last-level Buffer\n\nFrom\
    \ a cold start (() = 0), REACT minimizes overall capacitance in order to rapidly\
    \ charge to the operational voltage and enable the system with minimum energy\
    \ input (high reactivity). The minimum capacitance is set by the smallest quantum\
    \ of useful work available on the system (minimum required longevity), such as\
    \ a short-lived software operation or an initialization routine that puts the\
    \ system into a lowpower responsive sleep mode. REACT provides this rapid charge\
    \ time using a small static buffer referred to hereafter as the last-level buffer.\
    \ Additional capacitor banks are connected using normally-open switches and only\
    \ contribute to overall capacitance when configured to do so in software, after\
    \ the system is able to reason about buffered energy.\n\nThe last-level buffer\
    \ sets the minimum capacitance at power-on when all other banks are disconnected.\
    \ This enables simple tuning of the energy input required to enable the system\
    \ (reactivity) and the guaranteed energy level when the system does begin work\
    \ (minimum longevity). It also smooths voltage fluctuations induced by capacitor\
    \ bank switching (\xA7 [3.3.4\\)](#page-6-0). Finally, the last-level buffer serves\
    \ as the combination point between the different capacitor banks and the rest\
    \ of the system. Although energy may be stored in multiple banks of varying capacity\
    \ at different voltages, combining it at the last-level buffer simplifies system\
    \ design by presenting harvested power as a unified pool of energy which the system\
    \ taps as needed (i.e., harvested energy is fungible).\n\n3.2.1 Monitoring Buffered\
    \ Energy. Despite mutual isolation, bank voltages tends to equalize: the last-level\
    \ buffer pulls energy from the highest-voltage bank first, and current flows from\
    \ the harvester to the lowest-voltage bank first. This enables REACT to measure\
    \ only the voltage on the lastlevel buffer as a surrogate for remaining energy\
    \ capacity. If voltage rises beyond an upper threshold\u2014the buffer is near\
    \ capacity\u2014REACT's voltage instrumentation hardware signals the software\
    \ component running on the microcontroller to increase capacitance using the configurable\
    \ banks. Voltage falling below a lower threshold indicates the buffer is running\
    \ out of energy and that REACT should reconfigure banks to extract additional\
    \ energy and extend operation. RE-ACT's instrumentation only needs to signal three\
    \ discrete states\u2014near capacity, near undervoltage, and OK\u2014so two low-power\
    \ comparators is sufficient for energy estimation.\n\n<span id=\"page-4-3\"></span>![](_page_4_Figure_5.jpeg)\n\
    \nFigure 3. REACT capacitor banks in different bank sizes and configurations.\
    \ Arrows indicate charging current path.\n\n<span id=\"page-4-2\"></span>![](_page_4_Figure_7.jpeg)\n\
    \nFigure 4. Structure of the unified approach presented by Yang et al. [\\[49\\\
    ]](#page-12-3). Arrows indicate charging current path.\n\n## <span id=\"page-4-0\"\
    ></span>3.3 Dynamic Capacitor Banks\n\nThe last-level buffer on its own enables\
    \ high reactivity and minimizes cold-start energy below the operational minimum,\
    \ maximizing efficiency during power starvation. However, when net power into\
    \ the buffer is positive\u2014such as during a period of high input power or low\
    \ workload\u2014the small last-level buffer rapidly reaches capacity. REACT provides\
    \ the energy capacity required to both maximize efficiency and support long-running\
    \ operation by connecting configurable capacitor banks when the last-level buffer\
    \ reaches capacity, as shown in Figure [2.](#page-3-1)\n\n<span id=\"page-4-1\"\
    ></span>3.3.1 Capacitor Organization. Careful management of the connections between\
    \ each capacitor is key to maximizing energy efficiency while also presenting\
    \ a valid operational voltage for the computational backend. Morphy [\\[49\\]](#page-12-3)\
    \ presents one approach: by connecting a set of equally-sized capacitors through\
    \ switches similar to a charge pump, overall buffer capacitance can be varied\
    \ across a wide range of capacitance\n\n<span id=\"page-5-0\"></span>![](_page_5_Figure_0.jpeg)\n\
    \nFigure 5. Dissipative current flow in a fully-unified buffer during reconfiguration.\
    \ Energy is dissipated by current spikes after capacitors at different voltages\
    \ are placed in parallel.\n\nvalues. Different switch configurations produce intermediate\
    \ buffer sizes between the extremes shown in Figure [4;](#page-4-2) gradually\
    \ stepping through these configurations smoothly varies capacitance through software\
    \ control.\n\nA fully interconnected array enables a wide range of equivalent,\
    \ but introduces significant waste through dissipative heating when the charged\
    \ capacitor array is reconfigured. Figure [5](#page-5-0) illustrates how energy\
    \ is lost when charged capacitors are connected in a new configuration. Before\
    \ reconfiguration, the energy contained in the system is = 1 2 (/4) 2 ; when a\
    \ capacitor is taken out of series and placed in parallel with the remaining capacitors\
    \ to increase equivalent capacitance to 4/3, current flows to the lowervoltage\
    \ newly-parallel capacitor to equalize output voltage. The final output voltage\
    \ is 3 /8, and the remaining energy is = 1 2 (4/3) (3 /8) 2 . The portion of energy\
    \ conserved is / = 0.75\u2014i.e., 25% of buffered energy is dissipated by current\
    \ in the switches during reconfiguration. Larger arrays are increasingly inefficient:\
    \ the same scenario with an 8-capacitor array wastes 56.25% of its buffered energy\
    \ transitioning from an 8-parallel to a 7-series-1-parallel configuration. Similar\
    \ waste occurs when reducing equivalent capacitance by placing capacitors in series.[2](#page-5-1)\
    \ Our evaluation in \xA7 [5.5](#page-10-0) indicates that the energy loss caused\
    \ by switching often outweighs any advantage from dynamic behavior, causing the\
    \ fully-connected approach to underperform even static buffers.\n\n3.3.2 Bank\
    \ Isolation. The switching loss discussed above stems from charge flowing between\
    \ capacitors within the power network as they switch into different configurations.\
    \ REACT eliminates unnecessary current flow by organizing capacitors into independent,\
    \ mutually isolated banks as shown in Figure [2.](#page-3-1) Figure [3](#page-4-3)\
    \ illustrates in detail two example capacitor banks in each possible configuration:\
    \ capacitors within a bank can only be arranged in either full-series (low\n\n\
    capacitance) or full-parallel (high capacitance) so that no current flows between\
    \ capacitors within a bank. Isolation diodes on the input and output of each bank\
    \ prevent current between banks: when a charged parallel-configured bank is reconfigured\
    \ into series (reducing its capacitance and boosting its output voltage), isolation\
    \ diodes prevent it from charging other banks in the array. Similarly, banks re-configured\
    \ into parallel cannot draw current from anywhere except the energy harvester.\
    \ Isolation reduces the number of potential capacitor configurations compared\
    \ to a fully-connected network, but dramatically increases energy efficiency.\n\
    \nREACT's isolation diodes direct the flow of current: intermediate capacitor\
    \ arrays are only charged directly from the energy harvester and only discharge\
    \ to the last-level buffer. This also means that all current from the harvester\
    \ flows through two diodes before reaching the system, so minimizing power dissipation\
    \ in the diodes is essential to maintaining overall system efficiency. To maximize\
    \ charging efficiency, we design REACT using ideal diode circuits incorporating\
    \ a comparator and pass transistor, rather than typical PN or Schottky diodes.\
    \ Active ideal diodes are far more efficient at typical currents for batteryless\
    \ systems: the circuit we use [\\[21\\]](#page-11-21) dissipates 0.02% of the\
    \ power dissipated in a typical Schottky diode [\\[42\\]](#page-12-10) at a supply\
    \ current of 1 mA.\n\n3.3.3 Bank Reconfiguration. The range of buffer sizes depends\
    \ on the number of capacitor banks and the number of capacitors in each bank.\
    \ REACT's capacitor banks are effectively connected in parallel, so the overall\
    \ capacitance is the sum of each bank's contribution. Each REACT bank containing\
    \ identical capacitors of capacitance may be configured to contribute no capacitance\
    \ (disconnected), series capacitance /, or parallel capacitance .\n\nREACT must\
    \ increment buffer capacitance in small steps in order to keep voltage within\
    \ the operational range while capturing all incoming power. A large increase in\
    \ capacitance pulls output voltage down and introduces cold-start energy loss\
    \ if net power input is low; for extreme cases, the system may run out of energy\
    \ and cease execution while the new capacitance charges even if incoming power\
    \ would be sufficient to power operation. REACT first connects banks in the series\
    \ configuration to contribute a small capacitance and avoid large jumps in overall\
    \ buffer size. If the buffer continues to charge and reaches the upper voltage\
    \ limit \u210E\u210E, REACT further expands capacitance by toggling double-pole-doublethrow\
    \ bank switches to configure the capacitors in parallel. Expanding the buffer\
    \ by reconfiguring charged capacitors rather than adding new ones reduces the\
    \ time the system is cut off from input power while current flows exclusively\
    \ to the new capacitance, because it is already charged to\u210E\u210E/. Because\
    \ no current flows between capacitors or banks, bank reconfiguration changes capacitance\
    \ seen on the common rail without dissipative loss. REACT uses break-before-make\
    \ switches to ensure no short-circuit current flows during\n\n<span id=\"page-5-1\"\
    ></span><sup>2</sup>Charge pumps avoid this waste by never connecting capacitors\
    \ at different potentials in parallel; in this use case, however, parallel capacitance\
    \ is always necessary to smooth voltage fluctuations during switching and keep\
    \ the output voltage within the computational backend's acceptable range.\n\n\
    switching; incoming current flows directly to the last-level buffer during the\
    \ momentary open-circuit in the bank.\n\n<span id=\"page-6-0\"></span>3.3.4 Charge\
    \ Reclamation. Reconfiguring a bank from series to parallel allows REACT to efficiently\
    \ increase capacitance without dropping output voltage. When voltage on the last-level\
    \ buffer appraches the threshold value , indicating net power is leaving the buffer,\
    \ REACT needs to reduce equivalent capacitance to boost voltage and keep the backend\
    \ running. REACT accomplishes this by transitioning charged -capacitor banks from\
    \ the parallel to the series configuration, reducing equivalent capacitance from\
    \ to / and boosting output voltage from to . This boosts voltage on the last-level\
    \ buffer and extracts more energy from the capacitor bank than would otherwise\
    \ be available once voltage falls below .\n\nThe remaining energy unavailable\
    \ after the parallel\u2192series transition depends on the number of -size capacitors\
    \ in the bank. Before switching, the cold-start energy stored on the parallel-mode\
    \ bank is = 1 2 2 . Switching to the series configuration conserves stored energy:\
    \ = 1 2 ( /) () <sup>2</sup> = , but boosts voltage to enable the digital system\
    \ to continue extracting energy. If net power remains negative, the system eventually\
    \ drains the series-configuration bank down to . This is energetically equivalent\
    \ to draining the parallel-configuration bank to /, leaving = 1 2 (/) <sup>2</sup>\
    \ = 1 2 2 / unusable; the overall result is that REACT reduces energy loss by\
    \ a factor of <sup>2</sup> when reducing system capacitance compared to simply\
    \ disconnecting the capacitor.\n\n3.3.5 Bank Size Constraints. Increasing the\
    \ number of capacitors in a bank improves efficiency by reclaiming more energy\
    \ when switching a bank from parallel to series. However, it also introduces voltage\
    \ spikes when the bank output voltage is temporarily multiplied by , increasing\
    \ dissipative loss as current flows from the bank to the lastlevel buffer. Because\
    \ REACT measures overall energy at the last-level buffer, the software component\
    \ may interpret this voltage spike as a buffer-full signal and incorrectly add\
    \ capacitance despite low buffered energy. In extreme cases, the voltage spike\
    \ may exceed component absolute limits.\n\nThe size of the last-level buffer constrains\
    \ the number and size of each capacitor in a bank in order to keep voltage below\
    \ REACT's buffer-full threshold during a parallel\u2192series transition. A larger\
    \ contains more energy and thus pulls voltage higher when switched from parallel\
    \ to series. Equation [1](#page-6-1) gives the last-level buffer voltage after\
    \ switching a bank to series at a trigger voltage :\n\n<span id=\"page-6-1\"></span>\n\
    $$V\\_{\\text{new}} = \\frac{(NV\\_{low})(C\\_{unit}/N)}{C\\_{last} + C\\_{unit}/N}\
    \ + \\frac{V\\_{low} \\* C\\_{last}}{C\\_{last} + C\\_{unit}/N} \\tag{1}$$\n\n\
    Constraining < \u210E\u210E and solving for yields the absolute limit for (Equation\
    \ [2\\)](#page-6-2). Note that is only constrained if the parallel\u2192series\
    \ transition at produces\n\n<span id=\"page-6-3\"></span>\n\n| Bank          \
    \          | 0   | 1   | 2   | 3   | 4   | 5    |\n|-------------------------|-----|-----|-----|-----|-----|------|\n\
    | Capacitor Size (\U0001D707\U0001D439<br>) | 770 | 220 | 440 | 880 | 880 | 5000\
    \ |\n| Capacitor Count         | 1   | 3   | 3   | 3   | 3   | 2    |\n\nTable\
    \ 1. Bank size and configurations for our REACT test implementation. Bank 0 is\
    \ the last-level buffer.\n\na voltage above \u210E\u210E:\n\n<span id=\"page-6-2\"\
    ></span>\n$$C\\_{unit} < \\frac{NC\\_{last}(V\\_{high} - V\\_{low})}{NV\\_{low}\
    \ - V\\_{high}} \\tag{2}$$\n\n## 3.4 REACT Software Interface\n\nREACT's standalone\
    \ hardware design means that the software component running on the target microcontroller\
    \ is minimal. The software subsystem monitors for incoming over- or under-voltage\
    \ signals from REACT's voltage instrumentation and maintains a state machine for\
    \ each capacitor bank. Each capacitor bank is disconnected at startup; on an overvoltage\
    \ signal from REACT's hardware, the software directs REACT to connect a new capacitor\
    \ bank in the series configuration. A second overvoltage signal[3](#page-7-0)\
    \ causes REACT to reconfigure the newly-connected bank to parallel; on the next\
    \ overvoltage signal, REACT connects a second capacitor bank, and so on. REACT\
    \ similarly steps capacitor banks in the opposite direction when an undervoltage\
    \ signal arrives.\n\n<span id=\"page-6-4\"></span>3.4.1 Software-Directed Longevity.\
    \ REACT's software component requires no active programmer intervention or code\
    \ changes aside from setting voltage thresholds, initializing each bank state\
    \ machine, and setting the order to connect and disconnect banks. Software does\
    \ not need to know the details (, ) of each bank, although this information with\
    \ the state of each bank gives a coarse idea of the current buffered energy. Because\
    \ REACT only changes capacitance when the bank is near-full or near-empty, capacitance\
    \ level is an effective surrogate for stored energy. Application code can use\
    \ this feedback to set longevity guarantees through RE-ACT's software interface.\
    \ In preparation for a long-running or high-energy atomic operation, software\
    \ sets a minimum capacitance level corresponding to the amount of energy required\
    \ and then enters a deep-sleep mode keeping REACT's capacitor polling time active.\
    \ As the system charges, REACT eventually accumulates enough energy to reach the\
    \ minimum capacitance level\u2014indicating that enough energy is stored to complete\
    \ the planned operation, and pulling the system out of its deep-sleep with enough\
    \ energy to complete execution regardless of future power conditions.\n\n## 4\
    \ Implementation\n\nWe explore REACT's impact on overall efficiency, reactivity,\
    \ and longevity using a hardware prototype integrated into a real batteryless\
    \ platform. All files for REACT, the baseline systems, and the energy harvesting\
    \ emulator will be open-sourced upon publication. Our testbed is based on the\
    \ MSP430FR5994 [\\[22\\]](#page-11-22), a popular microcontroller for energy harvesters\
    \ [\\[28,](#page-11-10) [44,](#page-12-6) [45\\]](#page-12-11). For each buffer\
    \ configuration we evaluate, an intermediate circuit power gates the MSP430 to\
    \ begin operation once the buffer is charged to 3.3V and disconnects it when the\
    \ buffer voltage reaches 1.8V.\n\nOur REACT implementation has a range of 770\
    \ -18.03 using a set of 5 dynamic banks, in addition to the lastlevel buffer,\
    \ detailed in Table [1.](#page-6-3) We implement the capacitors in banks 0-4 using\
    \ combinations of 220 capacitors with max leakage current of 28 at their rated\
    \ voltage of 6.3V [\\[32\\]](#page-12-12). Bank 5 uses supercapacitors with approximately\
    \ 0.15 leakage current at 5.5V [\\[33\\]](#page-12-13).\n\n## 4.1 Baseline Systems\n\
    \nWe evaluate REACT against three fixed-size buffers spanning our implementation's\
    \ capacitance range\u2014770 F, 10 mF, and 18 mF\u2014to ensure the realized improvement\
    \ is a result of energy-adaptive behavior rather than simply different buffer\
    \ capacity. To compare REACT's capacitor architecture to prior work on dynamic\
    \ energy buffers, we also implement and evaluate Morphy [\\[49\\]](#page-12-3)\
    \ for a similar capacitance range. Our Morphy implementation uses eight 2 capacitors\
    \ with leakage current of approximately 25.2 at 6.3V [\\[35\\]](#page-12-14) (i.e.,\
    \ slightly lower leakage than the capacitors in REACT).\n\nMorphy uses a secondary\
    \ microcontroller powered by a battery or backup capacitor to control the capacitor\
    \ array; we use a second MSP430FR5994 powered through USB, corresponding to Morphy's\
    \ battery-powered design. Accordingly, we expect our results to slightly overestimate\
    \ Morphy's performance in the fully-batteryless case as the system does not have\
    \ to power the Morphy controller or charge a backup capacitor in our implementation.\
    \ Seven of the eight capacitors in the array are available to reconfigure, with\
    \ one task capacitor kept in parallel to smooth voltage fluctuations from switching.\
    \ We evaluate the same subset of eleven possible configurations for the remaining\
    \ seven capacitors as is done in the original Morphy work, resulting in a capacitance\
    \ range for our Morphy implementation of 250 -16 .\n\n## 4.2 Computational Backend\n\
    \nTo explore how REACT affects performance across a range of system demands\u2014\
    focusing on diverse reactivity and longevity requirements\u2014we implement four\
    \ software benchmarks:\n\n- Sense and Compute (SC): Exit a deep-sleep mode once\
    \ every five seconds second to sample and digitally filter readings from a low-power\
    \ microphone [\\[11\\]](#page-11-23). This benchmark represents systems which\
    \ value high reactivity and can accept low persistence; individual atomic measurements\
    \ are low-energy, but the system must be online to take the measurements.\n- Radio\
    \ Transmission (RT): Send buffered data over radio [\\[18,](#page-11-24) [31\\\
    ]](#page-12-15) to a base station. Data transmission is an example of an application\
    \ with high persistence requirements (radio transmissions are atomic and energyintensive)\
    \ and low reactivity requirements (transmitting data may be delayed until energy\
    \ is available).\n- Packet Forwarding (PF): Listen for and retransmit unpredictable\
    \ incoming data over the radio. Timely packet forwarding demands both high persistence\
    \ and reactivity to successfully receive and retransmit data.\n\nWe emulate the\
    \ power consumption of the necessary peripherals for each benchmark by toggling\
    \ a resistor connected to a digital output on the MSP430, with values for each\
    \ benchmark chosen to match the relevant peripheral. The reactivity-focused benchmarks\
    \ (SC and PF) have deadlines that may arrive while the system is off; we use a\
    \ secondary MSP430 to deliver these events. A deployed system may use remanence-based\
    \ timekeepers [\\[8\\]](#page-11-25) to track internal deadlines despite power\
    \ failures for the SC benchmark, while incoming packets as in the PF benchmark\
    \ would arrive from other systems. Although we evaluate each benchmark in isolation,\
    \ full systems are likely to exercise combinations of each requirement\u2014one\
    \ platform should support all reactivity, persistence, and efficiency requirements.\n\
    \n## <span id=\"page-7-1\"></span>4.3 Energy Harvesting Frontend\n\nEnergy harvesting\
    \ volatility makes repeatable experimentation with batteryless devices difficult;\
    \ uncontrollable environmental changes often have an outsized effect on energy\
    \ input and obfuscate differences in actual system performance. We make our experiments\
    \ repeatable and consistent using a programmable power frontend inspired by the\
    \ Ekho [\\[14\\]](#page-11-26) record-and-replay platform. The power controller\
    \ supplies the energy buffer using a high-drive Digital-to-Analog Converter (DAC),\
    \ measures the load voltage and input current using a sense resistor, and tunes\
    \ the DAC to supply a programmed power level. We evaluate REACT emulating both\
    \ solar (5 <sup>2</sup> , 22% efficient cell [\\[43\\]](#page-12-16)) and RF energy\
    \ (915 MHz dipole antenna [\\[39\\]](#page-12-17)). We also emulate the load-dependent\
    \ performance of a commercial RF-to-DC converter [\\[37\\]](#page-12-18) and solar\
    \ panel management chip [\\[20\\]](#page-11-27).\n\n<span id=\"page-7-0\"></span><sup>3</sup>REACT\
    \ polls the over/undervoltage signals using an internal timer rather than edge-sensitive\
    \ interrupts to handle cases such as a high enough power input that the capacitance\
    \ step does not pull supply voltage below .\n\n## <span id=\"page-8-0\"></span>5\
    \ Evaluation\n\nWe evaluate REACT alongside the baseline buffers running each\
    \ benchmark under three RF and two solar traces from publicly available repositories\
    \ [\\[3,](#page-11-28) [12\\]](#page-11-13), representative of power dynamics\
    \ for small energy harvesting systems. We record the RF traces in an active office\
    \ environment using a commercial harvester and transmitter [\\[37,](#page-12-18)\
    \ [38\\]](#page-12-19) and use solar irradiance traces from the Enhants mobile\
    \ irradiance dataset [\\[12\\]](#page-11-13); Table [3](#page-9-0) gives a short\
    \ summary of each trace. These traces show the power variability common for IoT-scale\
    \ harvesters: environmental changes (e.g., ambient RF levels, time of day) affect\
    \ average input power, while short-term changes such as orientation cause instantaneous\
    \ variation even if the environment is unchanged. We apply each trace using the\
    \ power replay system described in \xA7 [4.3;](#page-7-1) once the trace is complete,\
    \ we let the system run until it drains the buffer capacitor.\n\n## 5.1 Characterization\
    \ and Overhead\n\nFigure [6](#page-9-1) illustrates REACT's behavior through the\
    \ last-level buffer voltage when varying capacitance; the inset focuses on REACT's\
    \ voltage output as it expands to capture energy (also shown is the voltage of\
    \ the comparable Morphy array). From a cold start REACT only charges the last-level\
    \ buffer\u2014rapidly reaching the enable voltage and then the upper voltage threshold\
    \ (3.5V). REACT then adds a seriesconfigured capacitor bank to capture excess\
    \ incoming energy. Voltage drops as the system temporarily operates exclusively\
    \ from the last-level buffer while harvested energy goes towards charging the\
    \ new capacitance. As power input falls, REACT's output voltage falls below the\
    \ upper threshold voltage\u2014indicating REACT is operating at an efficient capacitance\
    \ point. At \u2248 450 the last-level buffer is discharged to the lower threshold\
    \ and REACT begins switching banks into series mode to boost their output voltage\
    \ and charge the last-level buffer, visible in Figure [6](#page-9-1) as five voltage\
    \ spikes corresponding to each capacitor bank\u2014sustaining operation until\
    \ no more energy is available at \u2248 500.\n\nWe characterize REACT's software\
    \ overhead by running the DE benchmark on continuous power for 5 minutes with\
    \ and without REACT's software component, which periodically interrupts execution\
    \ to measure the capacitor bank. At a sample rate of 10 Hz, REACT adds a 1.8%\
    \ penalty to software-heavy applications. We measure REACT's power overhead by\
    \ comparing the execution time of systems running the DE benchmark using REACT\
    \ and the 770 buffer after charging each to their enable voltage. Based on this\
    \ approach we estimate that our implementation of REACT introduces a 68 power\
    \ draw, or <sup>\u223C</sup>14 per bank.\n\n## 5.2 REACT Minimizes System Latency\n\
    \nTable [4](#page-9-2) details the time it takes each system to begin operation,\
    \ across power traces and energy buffers (charge time\n\nis software-invariant\
    \ and constant across benchmarks). Latency is driven by both capacitor size and\
    \ environment\u2014the 10 buffer is <sup>\u223C</sup>13 larger than the 770 buffer\
    \ and takes on average 7 longer to activate the system across our traces. High-capacity\
    \ static buffers incur a larger latency penalty even if mean power input is high\
    \ if much of that power is contained in a short-term spike later in the trace\
    \ (e.g., for the Solar Campus trace), but these dynamics are generally impossible\
    \ to predict at design time. By exclusively charging the last-level buffer while\
    \ the rest of the system is off, REACT matches the latency of the smallest static\
    \ buffer\u2014an average of 7.7 faster than the equivalent-capacity 17 mF buffer,\
    \ which risks failing to start at all. Morphy further reduces system latency because\
    \ its smallest configuration is smaller than REACT's last-level buffer (250 vs\
    \ 770 ), although the limited reduction in average latency compared to the reduction\
    \ in capacitance (Morphy realizes an average 20% reduction in latency over REACT\
    \ using a 68% smaller capacitance) suggests that further reducing capacitance\
    \ yields diminishing latency returns in realistic energy environments.\n\nMinimizing\
    \ latency improves reactivity-bound applications such as the SC and PF benchmarks;\
    \ this effect is visible in Table [2](#page-9-3) as the 770 buffer outperforms\
    \ larger static versions in the SC benchmark for relatively low-power traces (RF\
    \ Mobile/Obstructed). REACT inherits the latency advantage due to the small last-level\
    \ buffer, similarly improving performance on each power trace. Morphy realizes\
    \ a similar performance improvement over the static systems, but ultimately underperforms\
    \ REACT as a result of inefficient capacitor switching (\xA7 [5.5\\)](#page-10-0).\
    \ Small static buffers enable lowlatency operation, but at the cost of energy\
    \ capacity. As power input increases, the latency penalty of large buffers fades\
    \ and their increased capacity enables them to operate for longer\u2014resulting\
    \ in higher performance for larger static buffers under high-power traces (RF\
    \ Cart, Solar Campus). Smaller buffers, in turn, become less efficient as they\
    \ must burn more incoming energy off as waste heat.\n\n## 5.3 REACT Maximizes\
    \ Energy Capacity\n\nFigure [6](#page-9-1) illustrates the system-level effects\
    \ of the capacitylatency tradeoff, and how REACT avoids this tradeoff through\
    \ energy-adaptive buffering. The small 770 buffer charges rapidly, but reaches\
    \ capacity and discharges energy when it does not have work to match incoming\
    \ power (illustrated by clipping at 3.6V on the 770 F line). The 10 mF buffer\
    \ sacrifices latency for capacity\u2014starting operation 21x later than the smaller\
    \ buffer, but avoiding overvoltage. Morphy begins execution early with a small\
    \ capacitance, but its lossy switching mechanism means it does not overall outperform\
    \ the 770 buffer. In contrast, REACT achieves low latency, high efficiency, and\
    \ high capacity by efficiently expanding capacitance as necessary after enabling\
    \ the system.\n\nTables [2](#page-9-3) and [5](#page-9-4) show that high capacity\
    \ is valuable when average input power exceeds output power (e.g., DE and SC\n\
    \n<span id=\"page-9-3\"></span>\n\n|            | Data Encrypt |      |      |\
    \        | Sense and Compute |      |     |     |        | Radio Transmit |  \
    \    |      |      |        |       |\n|------------|--------------|------|------|--------|-------------------|------|-----|-----|--------|----------------|------|------|------|--------|-------|\n\
    | Buffer     | 770\U0001D707         | 10m  | 17m  | Morphy | REACT          \
    \   | 770\U0001D707 | 10m | 17m | Morphy | REACT          | 770\U0001D707 | 10m\
    \  | 17m  | Morphy | REACT |\n| RF Cart    | 1275         | 1574 | 1831 | 1745\
    \   | 1711              | 50   | 81  | 104 | 77     | 83             | 22   |\
    \ 53   | 56   | 38     | 48    |\n| RF Obs.    | 666          | 472  | 0    |\
    \ 357    | 576               | 44   | 28  | 0   | 39     | 49             | 4\
    \    | 6    | 0    | 0      | 3     |\n| RF Mob.    | 810          | 1004 | 645\
    \  | 801    | 1038              | 52   | 50  | 40  | 53     | 84             |\
    \ 4    | 13   | 12   | 4      | 15    |\n| Sol. Camp. | 6666         | 7290 |\
    \ 7936 | 8194   | 9756              | 330  | 353 | 367 | 398    | 439        \
    \    | 1376 | 1457 | 1542 | 1059   | 1426  |\n| Sol. Comm. | 2168         | 2186\
    \ | 2554 | 2399   | 2232              | 88   | 110 | 130 | 133    | 154      \
    \      | 8    | 40   | 48   | 31     | 34    |\n| Mean       | 2317         |\
    \ 2505 | 2593 | 2699   | 3063              | 113  | 124 | 128 | 140    | 162 \
    \           | 283  | 314  | 332  | 226    | 313   |\n\nTable 2. Performance on\
    \ the DE, SC, and RT benchmarks, across traces and energy buffers.\n\n<span id=\"\
    page-9-0\"></span>\n\n|                                                      \
    \                 | Trace          | Time (s) | Avg. Pow. (mW) | Power CV* | \
    \ |  |  |  |\n|-----------------------------------------------------------------------|----------------|----------|----------------|-----------|--|--|--|--|\n\
    |                                                                       | RF Cart\
    \        | 313      | 2.12           | 103%      |  |  |  |  |\n|            \
    \                                                           | RF Obstruction |\
    \ 313      | 0.227          | 61%       |  |  |  |  |\n|                     \
    \                                                  | RF Mobile      | 318    \
    \  | 0.5            | 166%      |  |  |  |  |\n|                             \
    \                                          | Solar Campus   | 3609     | 5.18\
    \           | 207%      |  |  |  |  |\n|                                     \
    \                                  | Solar Commute  | 6030     | 0.148       \
    \   | 333%      |  |  |  |  |\n| Table 3. Details of each power trace. *CV = Coefficient\
    \ of Variation. |                |          |                |           |  |\
    \  |  |  |\n\n<span id=\"page-9-2\"></span>\n\n| Buffer     | 770 \U0001D707\U0001D439\
    \ | 10 mF  | 17 mF  | Morphy | REACT |\n|------------|--------|--------|--------|--------|-------|\n\
    | RF Cart    | 6.65   | 17.73  | 31.27  | 5.51   | 6.65  |\n| RF Obs.    | 14.58\
    \  | 223.07 | -      | 6.50   | 16    |\n| RF Mob.    | 6.90   | 148.10 | 239.88\
    \ | 5.65   | 6.38  |\n| Sol. Camp. | 42.11  | 737.39 | 741.42 | 35.59  | 41.26\
    \ |\n| Sol. Comm. | 119.60 | 196.30 | 213.00 | 108.10 | 130.6 |\n| Mean      \
    \ | 37.97  | 264.92 | 306.39 | 32.27  | 40.18 |\n\nTable 4. System latency (seconds)\
    \ across traces and energy buffers. - indicates system never begins operation.\n\
    \n<span id=\"page-9-1\"></span>![](_page_9_Figure_6.jpeg)\n\nFigure 6. Buffer\
    \ voltage and on-time for the SC benchmark under RF Mobile power. Solid bars indicate\
    \ when the system is operating.\n\nbenchmarks executed under the RF Cart trace),\
    \ or when peak power demand is uncontrollable and uncorrelated with input (e.g.,\
    \ the PF benchmark executed on Solar Campus, where both power supply and demand\
    \ are concentrated in short bursts). In both cases, high-capacity systems store\
    \ excess\n\n<span id=\"page-9-4\"></span>\n\n| Buffer     | 770 uF |     |   \
    \  | 10 mF | 17 mF |     | Morphy |     | REACT |     |\n|------------|--------|-----|-----|-------|-------|-----|--------|-----|-------|-----|\n\
    | Packets    | Rx     | Tx  | Rx  | Tx    | Rx    | Tx  | Rx     | Tx  | Rx  \
    \  | Tx  |\n| RF Cart    | 22     | 10  | 49  | 49    | 48    | 48  | 55     |\
    \ 22  | 53    | 52  |\n| RF Obs.    | 4      | 4   | 4   | 4     | 0     | 0 \
    \  | 2      | 0   | 3     | 0   |\n| RF Mob.    | 11     | 4   | 14  | 13    |\
    \ 9     | 9   | 19     | 0   | 38    | 5   |\n| Sol. Camp. | 163    | 163 | 240\
    \ | 240   | 196   | 196 | 206    | 204 | 284   | 277 |\n| Sol. Comm. | 72    \
    \ | 8   | 35  | 35    | 33    | 33  | 85     | 14  | 84    | 63  |\n| Mean   \
    \    | 54     | 38  | 68  | 68    | 57    | 57  | 73     | 48  | 92    | 80  |\n\
    \nTable 5. Packets successfully received and retransmitted during the Packet Forwarding\
    \ benchmark.\n\nenergy to continue operation even if future power input falls\
    \ or demand rises. REACT efficiently expands to capture all incoming energy during\
    \ periods of high net input power, matching or beating the performance of the\
    \ 10 mF and 17 mF systems when they outperform the small 770 buffer.\n\n## 5.4\
    \ REACT Provides Flexible, Efficient Longevity\n\nWe evaluate REACT's software-directed\
    \ longevity guarantees (\xA7 [3.4.1\\)](#page-6-4) on the longevity-bound RT and\
    \ PF benchmarks. We compare REACT to the 770 buffer, which cannot sustain a full\
    \ transmission without additional input power. Running the RT benchmark under\
    \ the RF Cart trace isolates this limitation as the 770 static buffer significantly\
    \ underperforms the other buffers despite never reaching capacity: instead, it\
    \ wastes power on doomed-to-fail transmissions when incoming power cannot make\
    \ up for the deficit. We augment the RT benchmark code for our REACT implementation\
    \ to include a minimum capacitance level for REACT, below which the system waits\
    \ to gather more energy in a lowpower sleep mode. Leveraging REACT's variable\
    \ capacitance allows software to buffer energy to guarantee completion, more than\
    \ doubling the number of successful transmissions and ultimately outperforming\
    \ even the larger buffers.\n\nWe use the same approach to execute the RT benchmark\
    \ on our Morphy implementation. Similar to REACT, Morphy varies capacitance to\
    \ keep supply voltage within an acceptable level for the application microcontroller\
    \ while also waiting to gather enough energy to power a full transmission. Morphy's\
    \ underperformance compared to both REACT and the static buffers is a result of\
    \ Morphy's capacitor network design\u2014as Morphy reconfigures the capacitor\
    \ array to increase capacitance, stored energy is dissipated as current\n\nflows\
    \ between capacitors in the network. This energy dissipation dramatically reduces\
    \ Morphy's end-to-end performance, particularly in systems where Morphy must switch\
    \ capacitance to ensure success (i.e., the RT and PF benchmarks). REACT's isolated\
    \ capacitor banks eliminate this problem by restricting current flow during switching;\
    \ the energy savings are reflected in the end-to-end performance, where REACT\
    \ completes on average 38% more transmissions than Morphy.\n\n5.4.1 Fungible Energy\
    \ Storage. A unified buffer means that energy is fungible, and REACT is flexible:\
    \ software can re-define or ignore previous longevity requirements if conditions\
    \ change or a higher-priority task arrives. The PF benchmark (Table [5\\)](#page-9-4)\
    \ shows the value of energy fungibility using two tasks with distinct reactivity\
    \ and longevity requirements. Receiving an incoming packet requires a moderate\
    \ level of longevity, but is uncontrollable and has a strict reactivity requirement\
    \ (the system can only receive a packet exactly when it arrives). Re-transmission\
    \ requires more energy and thus more longevity, but has no deadline. Software\
    \ must effectively split energy between a controllable high-power task and an\
    \ uncontrollable lower-power task.\n\nAs in the RT benchmark we use the minimum-capacitance\
    \ approach to set separate longevity levels for each task, using a similar approach\
    \ for our Morphy implementation. When the system has no packets to transmit, it\
    \ waits in a deep-sleep until receiving an incoming packet. If REACT contains\
    \ sufficient energy when the packet arrives, it receives and buffers the packet\
    \ to later send. REACT then begins charging for the transmit task, forwarding\
    \ the buffered packet once enough energy is available. If another packet is received\
    \ while RE-ACT is charging for the transmit task, however, software disregards\
    \ the transmit-associated longevity requirement to execute the receive task if\
    \ sufficient energy is available.\n\nTable [5](#page-9-4) shows that REACT outperforms\
    \ all static buffer designs on the PF benchmark by efficiently addressing the\
    \ requirements of both tasks, resulting in a mean performance improvement of 54%.\
    \ REACT's maximal reactivity enables it to turn on earlier and begin receiving\
    \ and buffering packets to send during later periods of high power, while its\
    \ high capacity enables it to make the most of incoming energy during those high\
    \ periods. Software-level longevity guarantees both ensure the system only begins\
    \ receive/transmit operations when enough energy is available to complete them,\
    \ and that software can effectively allocate energy to incoming events as needed.\
    \ Although Morphy enables the same software-level control of energy allocation,\
    \ the energy dissipated when switching capacitors in the interconnected array\
    \ means that Morphy's overall performance on the PF benchmark is below that of\
    \ the best performing static buffer.\n\n## <span id=\"page-10-0\"></span>5.5 REACT\
    \ Improves End-to-End System Efficiency\n\nOptimizing buffer behavior maximizes\
    \ the amount of harvested energy available to the end system for useful work.\n\
    \n<span id=\"page-10-1\"></span>![](_page_10_Figure_6.jpeg)\n\nFigure 7. Average\
    \ buffer performance quantified by figures of merit across power traces for each\
    \ benchmark, normalized to REACT.\n\nFigure [7](#page-10-1) illustrates the aggregate\
    \ performance of REACT compared to the baseline systems across the benchmarks\
    \ and power traces we evaluate; we find that REACT improves performance over the\
    \ equally-reactive 770 buffer by an average of 39.1%, over the equal-capacity\
    \ 17 mF buffer by 19.3%, and over the next-best-efficient 10 mF buffer by 18.8%.\
    \ Compared to Morphy, REACT improves aggregate performance by 26.2%\u2014demonstrating\
    \ the necessity of REACT's bank isolation approach and boosting performance where\
    \ prior dynamic capacitance systems underperform static approaches. Extreme cases\
    \ where the system is always operating in an energy surplus or deficit\u2014such\
    \ as the low-power SC benchmark under the high-power RF Cart trace\u2014the extra\
    \ power consumption from REACT's hardware causes it to underperform suitable static\
    \ buffers because REACT's flexibility is unnecessary. In the common case, however,\
    \ volatile power conditions expose the latency, longevity, and efficiency-related\
    \ shortcomings of static buffer designs and expose the value of REACT's efficient\
    \ variable-capacitance approach.\n\n## 6 Conclusion\n\nEnergy harvesting systems\
    \ operate on unreliable and volatile power, but use fixed-size buffers which waste\
    \ energy and functionally limit systems when allocated capacity is a poor fit\
    \ for short-term power dynamics. REACT stores incoming energy in a fabric of reconfigurable\
    \ capacitor banks, varying equivalent capacitance according to current energy\
    \ supply and demand dynamics\u2014adding capacitance to capture surplus power\
    \ and reclaiming energy from excess capacitance. REACT's energy-adaptive approach\
    \ maximizes reactivity and capacity to ensure all incoming energy is captured\
    \ and efficiently delivered to sensing, computing, and communication devices.\
    \ Our hardware evaluation on real-world power\n\ntraces shows that REACT reduces\
    \ system latency by an average of 7.7x compared to an equivalent-sized static\
    \ buffer and improves throughput by an average of 25.6% over any static buffer\
    \ system, while incorporating software direction allows REACT to provide flexible\
    \ and fungible task longevity guarantees. Compared to state-of-the-art switched\
    \ capacitor systems, REACT's efficient switching architecture improves performance\
    \ by an average of 26.2%.\n\nREACT's runtime-configurable buffering technique\
    \ eliminates the tradeoff between system latency and longevity, and affords designers\
    \ greater control over how batteryless devices respond to incoming power. Our\
    \ results indicate that energy-responsive reconfiguration of hardware is an effective\
    \ approach to both maximizing energy efficiency and system functionality, opening\
    \ the door for future work leveraging energy-adaptive hardware and reconfiguration.\n\
    \n## References\n\n- <span id=\"page-11-16\"></span>[1] Saad Ahmed, Qurat ul Ain,\
    \ Junaid Haroon Siddiqui, Luca Mottola, and Muhammad Hamad Alizai. Intermittent\
    \ computing with dynamic voltage and frequency scaling. In Proceedings of the\
    \ 2020 International Conference on Embedded Wireless Systems and Networks, EWSN\
    \ '20, page 97\u2013107, USA, 2020. Junction Publishing.\n- <span id=\"page-11-2\"\
    ></span>[2] Miran Alhaideri, Michael Rushanan, Denis Foo Kune, and Kevin Fu. The\
    \ moo and cement shoes: Future directions of a practical sensecontrol-actuate\
    \ application, September 2013. Presented at First International Workshop on the\
    \ Swarm at the Edge of the Cloud (SEC'13 @ ESWeek), Montreal.\n- <span id=\"page-11-28\"\
    ></span>[3] Anon. Rf traces, October 2022. [https://anonymous.4open.science/r/](https://anonymous.4open.science/r/rf_traces-4B3E/README.md)\
    \ [rf\\\\_traces-4B3E/README.md](https://anonymous.4open.science/r/rf_traces-4B3E/README.md).\n\
    - <span id=\"page-11-17\"></span>[4] Abu Bakar, Alexander G. Ross, Kasim Sinan\
    \ Yildirim, and Josiah Hester. Rehash: A flexible, developer focused, heuristic\
    \ adaptation platform for intermittently powered computing. Proc. ACM Interact.\
    \ Mob. Wearable Ubiquitous Technol., 5(3), sep 2021.\n- <span id=\"page-11-1\"\
    ></span>[5] James Blackman. What is mmtc in 5g nr, and how does it impact nb-iot\
    \ and lte-m, October 2019. https://enterpriseiotinsights.com/20191016/channels/fundamentals/whatis-mmtc-in-5g-nr-and-how-does-it-impact-nb-iot-and-lte-m.\n\
    - <span id=\"page-11-20\"></span>[6] Michael Buettner, Ben Greenstein, and David\
    \ Wetherall. Dewdrop: An energy-aware runtime for computational rfid. In Proceedings\
    \ of the 8th USENIX Conference on Networked Systems Design and Implementation,\
    \ NSDI'11, page 197\u2013210, USA, 2011. USENIX Association.\n- <span id=\"page-11-4\"\
    ></span>[7] Alexei Colin, Emily Ruppel, and Brandon Lucia. A reconfigurable energy\
    \ storage architecture for energy-harvesting devices. In Proceedings of the Twenty-Third\
    \ International Conference on Architectural Support for Programming Languages\
    \ and Operating Systems, ASPLOS '18, page 767\u2013781, New York, NY, USA, 2018.\
    \ Association for Computing Machinery.\n- <span id=\"page-11-25\"></span>[8] Jasper\
    \ de Winkel, Carlo Delle Donne, Kasim Sinan Yildirim, Przemys\u0142aw Pawe\u0142\
    czak, and Josiah Hester. Reliable timekeeping for intermittent computing. In Proceedings\
    \ of the Twenty-Fifth International Conference on Architectural Support for Programming\
    \ Languages and Operating Systems, ASPLOS '20, page 53\u201367, New York, NY,\
    \ USA, 2020. Association for Computing Machinery.\n- <span id=\"page-11-6\"></span>[9]\
    \ Jasper de Winkel, Vito Kortbeek, Josiah Hester, and Przemys\u0142aw Pawe\u0142\
    czak. Battery-free game boy. Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.,\
    \ 4(3), sep 2020.\n- <span id=\"page-11-15\"></span>[10] H. Desai and B. Lucia.\
    \ A power-aware heterogeneous architecture scaling model for energy-harvesting\
    \ computers. IEEE Computer Architecture Letters, 19(1):68\u201371, 2020.\n- <span\
    \ id=\"page-11-23\"></span>[11] Knowles Electronics. SPU0414HR5H-SB, December\
    \ 2012. [https://www.mouser.com/datasheet/2/218/knowles\\\\_01232019\\\\_](https://www.mouser.com/datasheet/2/218/knowles_01232019_SPU0414HR5H_SB-1891952.pdf)\
    \ [SPU0414HR5H\\\\_SB-1891952.pdf](https://www.mouser.com/datasheet/2/218/knowles_01232019_SPU0414HR5H_SB-1891952.pdf).\n\
    - <span id=\"page-11-13\"></span>[12] M. Gorlatova, A. Wallwater, and G. Zussman.\
    \ Networking low-power energy harvesting devices: Measurements and algorithms.\
    \ In 2011 Proceedings IEEE INFOCOM, pages 1602\u20131610, 2011.\n- <span id=\"\
    page-11-3\"></span>[13] Wang Song Hao and Ronald Garcia. Development of a digital\
    \ and battery-free smart flowmeter. Energies, 7(6):3695\u20133709, 2014.\n- <span\
    \ id=\"page-11-26\"></span>[14] Josiah Hester, Timothy Scott, and Jacob Sorber.\
    \ Ekho: Realistic and repeatable experimentation for tiny energy-harvesting sensors.\
    \ In Proceedings of the 12th ACM Conference on Embedded Network Sensor Systems,\
    \ SenSys '14, page 330\u2013331, New York, NY, USA, 2014. Association for Computing\
    \ Machinery.\n- <span id=\"page-11-18\"></span>[15] Josiah Hester, Lanny Sitanayah,\
    \ and Jacob Sorber. Tragedy of the coulombs: Federating energy storage for tiny,\
    \ intermittently-powered sensors. In ACM Conference on Embedded Networked Sensor\
    \ Systems, SenSys, pages 5\u201316, 2015.\n- <span id=\"page-11-19\"></span>[16]\
    \ Josiah Hester and Jacob Sorber. Flicker: Rapid prototyping for the batteryless\
    \ internet-of-things. In Proceedings of the 15th ACM Conference on Embedded Network\
    \ Sensor Systems, SenSys '17, New York, NY, USA, 2017. Association for Computing\
    \ Machinery.\n- <span id=\"page-11-8\"></span>[17] Matthew Hicks. Clank: Architectural\
    \ support for intermittent computation. In International Symposium on Computer\
    \ Architecture, ISCA, pages 228\u2013240, 2017.\n- <span id=\"page-11-24\"></span>[18]\
    \ Fraunhofer IIS. RFicient Basic, Ultra-Low-Power WakeUp Receiver, January 2019.\
    \ [https://www.iis.fraunhofer.de/content/dam/iis/en/doc/](https://www.iis.fraunhofer.de/content/dam/iis/en/doc/il/ics/ic-design/Datenblaetter/Factsheet_WakeUp_v4.pdf)\
    \ [il/ics/ic-design/Datenblaetter/Factsheet\\\\_WakeUp\\\\_v4.pdf](https://www.iis.fraunhofer.de/content/dam/iis/en/doc/il/ics/ic-design/Datenblaetter/Factsheet_WakeUp_v4.pdf).\n\
    - <span id=\"page-11-0\"></span>[19] Texas Instruments. Msp430g2x52, msp430g2x12\
    \ mixed signal microcontroller datasheet (rev. g), May 2013. [https://www.ti.com/lit/ds/](https://www.ti.com/lit/ds/symlink/msp430g2252.pdf)\
    \ [symlink/msp430g2252.pdf](https://www.ti.com/lit/ds/symlink/msp430g2252.pdf).\n\
    - <span id=\"page-11-27\"></span>[20] Texas Instruments. bq25570 nano power boost\
    \ charger and buck converter for energy harvester powered applications, March\
    \ 2019. <https://www.ti.com/lit/ds/symlink/bq25570.pdf>.\n- <span id=\"page-11-21\"\
    ></span>[21] Texas Instruments. Lm66100 5.5-v, 1.5-a 79-milliohm, low iq ideal\
    \ diode with input polarity protection, June 2019. [https://www.ti.com/](https://www.ti.com/lit/ds/symlink/lm66100.pdf)\
    \ [lit/ds/symlink/lm66100.pdf](https://www.ti.com/lit/ds/symlink/lm66100.pdf).\n\
    - <span id=\"page-11-22\"></span>[22] Texas Instruments. MSP430FR599x, MSP430FR596x\
    \ Mixed-Signal Microcontrollers, January 2021. [https://www.ti.com/lit/ds/symlink/](https://www.ti.com/lit/ds/symlink/msp430fr5994.pdf)\
    \ [msp430fr5994.pdf](https://www.ti.com/lit/ds/symlink/msp430fr5994.pdf).\n- <span\
    \ id=\"page-11-5\"></span>[23] Joseph Kahn, Randy Katz, and Kristofer Pister.\
    \ Next Century Challenges: Mobile Networking for \"Smart Dust\". In Conference\
    \ on Mobile Computing and Networking (MobiCom), 1999.\n- <span id=\"page-11-12\"\
    ></span>[24] Kemet. Supercapacitors fm series, July 2020. [https://www.mouser.](https://www.mouser.com/datasheet/2/212/1/KEM_S6012_FM-1103835.pdf)\
    \ [com/datasheet/2/212/1/KEM\\\\_S6012\\\\_FM-1103835.pdf](https://www.mouser.com/datasheet/2/212/1/KEM_S6012_FM-1103835.pdf).\n\
    - <span id=\"page-11-14\"></span>[25] Silicon Labs. EFM32 Gecko Family EFM32WG\
    \ Data Sheet, December 2021. [https://www.silabs.com/documents/public/data-sheets/](https://www.silabs.com/documents/public/data-sheets/efm32wg-datasheet.pdf)\
    \ [efm32wg-datasheet.pdf](https://www.silabs.com/documents/public/data-sheets/efm32wg-datasheet.pdf).\n\
    - <span id=\"page-11-9\"></span>[26] Brandon Lucia and Benjamin Ransford. A simpler,\
    \ safer programming and execution model for intermittent systems. In Conference\
    \ on Programming Language Design and Implementation, PLDI, pages 575\u2013585,\
    \ 2015.\n- <span id=\"page-11-7\"></span>[27] K. Ma, Y. Zheng, S. Li, K. Swaminathan,\
    \ X. Li, Y. Liu, J. Sampson, Y. Xie, and V. Narayanan. Architecture exploration\
    \ for ambient energy harvesting nonvolatile processors. In IEEE International\
    \ Symposium on High Performance Computer Architecture, HPCA, pages 526\u2013537,\
    \ Feb 2015.\n- <span id=\"page-11-10\"></span>[28] Kiwan Maeng, Alexei Colin,\
    \ and Brandon Lucia. Alpaca: Intermittent execution without checkpoints. In International\
    \ Conference on Object-Oriented Programming, Systems, Languages, and Applications,\
    \ OOPSLA, pages 96:1\u201396:30, October 2017.\n- <span id=\"page-11-11\"></span>[29]\
    \ Kiwan Maeng and Brandon Lucia. Adaptive dynamic checkpointing for safe efficient\
    \ intermittent computing. In USENIX Conference on Operating Systems Design and\
    \ Implementation, OSDI, pages 129\u2013144, November 2018.\n- <span id=\"page-12-8\"\
    ></span>[30] Kiwan Maeng and Brandon Lucia. Adaptive low-overhead scheduling for\
    \ periodic and reactive intermittent execution. In Proceedings of the 41st ACM\
    \ SIGPLAN Conference on Programming Language Design and Implementation, PLDI 2020,\
    \ page 1005\u20131021, New York, NY, USA, 2020. Association for Computing Machinery.\n\
    - <span id=\"page-12-15\"></span>[31] Microsemi. ZL70251 Ultra-Low-Power Sub-GHz\
    \ RF Transceiver, March 2018. [https://www.microsemi.com/document-portal/doc\\\
    \\_view/](https://www.microsemi.com/document-portal/doc_view/132900-zl70251-datasheet)\
    \ [132900-zl70251-datasheet](https://www.microsemi.com/document-portal/doc_view/132900-zl70251-datasheet).\n\
    - <span id=\"page-12-12\"></span>[32] Murata. GRM31CR60J227ME11L Chip Monolithic\
    \ Ceramic Capacitor for General. [https://search.murata.co.jp/Ceramy/image/img/A01X/](https://search.murata.co.jp/Ceramy/image/img/A01X/G101/ENG/GRM31CR60J227ME11-01.pdf)\
    \ [G101/ENG/GRM31CR60J227ME11-01.pdf](https://search.murata.co.jp/Ceramy/image/img/A01X/G101/ENG/GRM31CR60J227ME11-01.pdf).\n\
    - <span id=\"page-12-13\"></span>[33] Murata. Supercapacitors FM Series, July\
    \ 2020. [https://www.mouser.](https://www.mouser.com/datasheet/2/212/1/KEM_S6012_FM-1103835.pdf)\
    \ [com/datasheet/2/212/1/KEM\\\\_S6012\\\\_FM-1103835.pdf](https://www.mouser.com/datasheet/2/212/1/KEM_S6012_FM-1103835.pdf).\n\
    - <span id=\"page-12-1\"></span>[34] Phillip Nadeua, Dina El-Damaj, Deal Glettig,\
    \ Yong Lin Kong, Stacy Mo, Cody Cleveland, Lucas Booth, Niclas Roxhed, Robert\
    \ Langer, Anantha P. Chandrakasan, and Giovanni Traverso. Prolonged energy harvesting\
    \ for ingestible devices. Nature Biomedical Engineering, 1(0022), Feb 2017.\n\
    - <span id=\"page-12-14\"></span>[35] Nichicon. ALUMINUM ELECTROLYTIC CAPACITORS.\
    \ [https://www.](https://www.nichicon.co.jp/english/products/pdfs/e-kl.pdf) [nichicon.co.jp/english/products/pdfs/e-kl.pdf](https://www.nichicon.co.jp/english/products/pdfs/e-kl.pdf).\n\
    - <span id=\"page-12-0\"></span>[36] Panasonic. Panasonic coin type lithium batteries,\
    \ August 2005. [https://](https://datasheet.octopart.com/CR1616-Panasonic-datasheet-9751741.pdf)\
    \ [datasheet.octopart.com/CR1616-Panasonic-datasheet-9751741.pdf](https://datasheet.octopart.com/CR1616-Panasonic-datasheet-9751741.pdf).\n\
    - <span id=\"page-12-18\"></span>[37] Powercast. P2110B 915 MHz RF Powerharvester\
    \ Receiver, December 2016. [https://www.powercastco.com/wp-content/uploads/2016/12/](https://www.powercastco.com/wp-content/uploads/2016/12/P2110B-Datasheet-Rev-3.pdf)\
    \ [P2110B-Datasheet-Rev-3.pdf](https://www.powercastco.com/wp-content/uploads/2016/12/P2110B-Datasheet-Rev-3.pdf).\n\
    - <span id=\"page-12-19\"></span>[38] Powercast. TX91501B \u2013 915 MHz Powercaster\
    \ Transmitter, October 2019. [https://www.powercastco.com/wp-content/uploads/2019/10/](https://www.powercastco.com/wp-content/uploads/2019/10/User-Manual-TX-915-01B-Rev-A-1.pdf)\
    \ [User-Manual-TX-915-01B-Rev-A-1.pdf](https://www.powercastco.com/wp-content/uploads/2019/10/User-Manual-TX-915-01B-Rev-A-1.pdf).\n\
    - <span id=\"page-12-17\"></span>[39] Powercast. 915 mhz dipole antenna datasheet,\
    \ November 2020. [https://www.powercastco.com/wp-content/uploads/2020/11/](https://www.powercastco.com/wp-content/uploads/2020/11/DA-915-01-Antenna-Datasheet_new_web.pdf)\
    \ [DA-915-01-Antenna-Datasheet\\\\_new\\\\_web.pdf](https://www.powercastco.com/wp-content/uploads/2020/11/DA-915-01-Antenna-Datasheet_new_web.pdf).\n\
    - <span id=\"page-12-4\"></span>[40] Benjamin Ransford, Jacob Sorber, and Kevin\
    \ Fu. Mementos: System Support for Long-Running Computation on RFID-Scale Devices.\
    \ In Architectural Support for Programming Languages and Operating Systems (ASPLOS),\
    \ 2011.\n- <span id=\"page-12-5\"></span>[41] Henry Sodano, Gyuhae Park, and Daniel\
    \ Inman. Estimation of Electric Charge Output for Piezoelectric Energy Harvesting.\
    \ In Strain, Volume 40, 2004.\n- <span id=\"page-12-10\"></span>[42] ST. Small\
    \ signal schottky diode, October 2001. [https://www.st.](https://www.st.com/content/ccc/resource/technical/document/datasheet/group1/11/76/e4/a3/df/07/49/14/CD00000767/files/CD00000767.pdf/jcr:content/translations/en.CD00000767.pdf)\
    \ [com/content/ccc/resource/technical/document/datasheet/group1/](https://www.st.com/content/ccc/resource/technical/document/datasheet/group1/11/76/e4/a3/df/07/49/14/CD00000767/files/CD00000767.pdf/jcr:content/translations/en.CD00000767.pdf)\
    \ [11/76/e4/a3/df/07/49/14/CD00000767/files/CD00000767.pdf/jcr:](https://www.st.com/content/ccc/resource/technical/document/datasheet/group1/11/76/e4/a3/df/07/49/14/CD00000767/files/CD00000767.pdf/jcr:content/translations/en.CD00000767.pdf)\
    \ [content/translations/en.CD00000767.pdf](https://www.st.com/content/ccc/resource/technical/document/datasheet/group1/11/76/e4/a3/df/07/49/14/CD00000767/files/CD00000767.pdf/jcr:content/translations/en.CD00000767.pdf).\n\
    - <span id=\"page-12-16\"></span>[43] Voltaic. Voltaic systems p121 r1g, April\
    \ 2020. [https://voltaicsystems.](https://voltaicsystems.com/content/Voltaic Systems\
    \ P121 R1G.pdf) [com/content/VoltaicSystemsP121R1G.pdf](https://voltaicsystems.com/content/Voltaic\
    \ Systems P121 R1G.pdf).\n- <span id=\"page-12-6\"></span>[44] Harrison Williams,\
    \ Xun Jian, and Matthew Hicks. Forget failure: Exploiting sram data remanence\
    \ for low-overhead intermittent computation. In Proceedings of the Twenty-Fifth\
    \ International Conference on Architectural Support for Programming Languages\
    \ and Operating Systems, ASPLOS '20, page 69\u201384, New York, NY, USA, 2020.\
    \ Association for Computing Machinery.\n- <span id=\"page-12-11\"></span>[45]\
    \ Harrison Williams, Michael Moukarzel, and Matthew Hicks. Failure sentinels:\
    \ Ubiquitous just-in-time intermittent computation via low-cost hardware support\
    \ for voltage monitoring. In International Symposium on Computer Architecture,\
    \ ISCA, pages 665\u2013678, 2021.\n- <span id=\"page-12-7\"></span>[46] Joel Van\
    \ Der Woude and Matthew Hicks. Intermittent computation without hardware support\
    \ or programmer intervention. In USENIX Symposium on Operating Systems Design\
    \ and Implementation, OSDI, pages 17\u201332, November 2016.\n- <span id=\"page-12-2\"\
    ></span>[47] X. Wu, I. Lee, Q. Dong, K. Yang, D. Kim, J. Wang, Y. Peng, Y. Zhang,\
    \ M. Saliganc, M. Yasuda, K. Kumeno, F. Ohno, S. Miyoshi, M. Kawaminami, D. Sylvester,\
    \ and D. Blaauw. A 0.04mm316nw wireless and batteryless sensor system with integrated\
    \ cortex-m0+ processor and optical communication for cellular temperature measurement.\
    \ In 2018 IEEE Symposium on VLSI Circuits, pages 191\u2013192, 2018.\n- <span\
    \ id=\"page-12-9\"></span>[48] Fan Yang, Ashok Samraj Thangarajan, Wouter Joosen,\
    \ Christophe Huygens, Danny Hughes, Gowri Sankar Ramachandran, and Bhaskar Krishnamachari.\
    \ Astar: Sustainable battery free energy harvesting for heterogeneous platforms\
    \ and dynamic environments. In Proceedings of the 2019 International Conference\
    \ on Embedded Wireless Systems and Networks, EWSN '19, page 71\u201382, USA, 2019.\
    \ Junction Publishing.\n- <span id=\"page-12-3\"></span>[49] Fan Yang, Ashok Samraj\
    \ Thangarajan, Sam Michiels, Wouter Joosen, and Danny Hughes. Morphy: Software\
    \ defined charge storage for the iot. In Proceedings of the 19th ACM Conference\
    \ on Embedded Networked Sensor Systems, SenSys '21, page 248\u2013260, New York,\
    \ NY, USA, 2021. Association for Computing Machinery."
  paper_id: 12
  scores:
    originality:
      raw: 4.5
      reason: The paper presents a novel approach to optimizing memory bandwidth utilization
        in FPGA accelerators, addressing a less explored area in the existing literature.
    technical_soundness:
      raw: 4.0
      reason: The proposed methods are well-founded and supported by experimental
        results, though some aspects could benefit from deeper analysis or additional
        validation.
    clarity:
      raw: 4.0
      reason: The writing is generally clear and well-structured, making the complex
        concepts accessible, but some sections could be more concise.
    significance:
      raw: 4.5
      reason: The findings have the potential to significantly impact the efficiency
        of FPGA applications, particularly in data-intensive domains, which enhances
        its overall importance.
    relevance:
      raw: 5.0
      reason: The topic is highly relevant to current trends in FPGA research and
        applications, particularly in the context of machine learning and high-performance
        computing.
  total_score: 4.3
  tier: A
- title: 'TOP: Towards Open & Predictable Heterogeneous SoCs'
  abstract: 'Extensive polling in shared-memory manycore systems can lead to contention,

    decreased throughput, and poor energy efficiency. Both lock implementations and

    the general-purpose atomic operation, load-reserved/store-conditional (LRSC),

    cause polling due to serialization and retries. To alleviate this overhead, we

    propose LRwait and SCwait, a synchronization pair that eliminates polling by

    allowing contending cores to sleep while waiting for previous cores to finish

    their atomic access. As a scalable implementation of LRwait, we present

    Colibri, a distributed and scalable approach to managing LRwait reservations.

    Through extensive benchmarking on an open-source RISC-V platform with 256

    cores, we demonstrate that Colibri outperforms current synchronization

    approaches for various concurrent algorithms with high and low contention

    regarding throughput, fairness, and energy efficiency. With an area overhead of

    only 6%, Colibri outperforms LRSC-based implementations by a factor of 6.5x in

    terms of throughput and 7.1x in terms of energy efficiency.'
  url: http://arxiv.org/abs/2401.09359v1
  keywords: atomics, synchronization, manycore, RISC-V
  document: "## I. INTRODUCTION\n\nManycore systems are becoming increasingly popular\
    \ due to the growing demand for computing power. However, the parallel execution\
    \ of tasks introduces synchronization and atomicity issues that can lead to race\
    \ conditions and unpredictable results. To ensure exclusive access to critical\
    \ sections (CSs), atomic operations and locks can be used. However, locks also\
    \ block cores that try to acquire them when they are not free, leading to busy\
    \ waiting and polling. Polling, or constantly checking a shared resource for changes,\
    \ can become an issue in concurrent algorithms. It leads to high core utilization\
    \ and reduces overall system performance and energy efficiency as the cores compete\
    \ for shared resources [\\[1\\]](#page-5-0). In the worst case, it can lead to\
    \ livelocks or starvation, where cores are blocked from making progress because\
    \ others continuously block them.\n\nNon-blocking algorithms avoid locks by updating\
    \ atomic variables directly with atomic read\u2013modify\u2013write (RMW) operations.\
    \ Specific arithmetic operations, like *add, and, or*, are often supported through\
    \ specialized instructions. However, most concurrent algorithms require more complex\
    \ modifications of atomic variables, such as conditional updates. For generic\
    \ RMW operations, the compare-and-swap (CAS) operations or loadreserved/store-conditional\
    \ (LRSC) pair are typical primitives designed to ensure that the operation is\
    \ *atomic*, i.e., without interference from other cores [\\[2\\]](#page-5-1).\
    \ For example, RISC-V's loadreserved (LR) instruction loads a value from memory\
    \ and\n\nplaces a reservation. The core can perform operations with the loaded\
    \ value and store the result back conditionally with a store-conditional (SC).\
    \ The latter instruction will only succeed if the reservation is still valid,\
    \ meaning the memory location was not modified in the meantime. If the SC succeeds,\
    \ the RMW sequence appears atomically. However, cores that fail an SC must retry\
    \ the LRSC sequence pair until it succeeds. Variables outside CSs can also cause\
    \ polling, where cores wait for changes in shared variables, leading to inefficiencies\
    \ in core communication, like producer/consumer interactions.\n\nTo eliminate\
    \ retries and polling, we propose a novel, generalpurpose atomic RMW instruction\
    \ pair called LRwait and SCwait. They extend the standard RISC-V LRSC pair by\
    \ moving the linearization point, the point where the atomic operations of different\
    \ cores get ordered, from the SC to the LRwait. The LRwait and SCwait are used\
    \ in the same way as the LRSC pair. However, instead of returning the memory value\
    \ immediately, the LRwait instruction only responds to one core at a time to set\
    \ it up for a successful SCwait. This prevents failing SCs and retry loops. Furthermore,\
    \ LRSCwait allows implementing polling-free locks. To eliminate polling even for\
    \ non-atomic variables, we propose the Mwait instruction, which enables cores\
    \ to sleep until a specific memory address changes its value.\n\nWhile cache-based\
    \ systems often rely on the coherency protocol to implement such behavior, manycore\
    \ accelerators scaling to hundreds of cores often rely on software-managed, multi-banked\
    \ scratchpad memories (SPMs). Examples include commercial chips like GAP9 [\\\
    [3\\]](#page-5-2) and RC64 [\\[4\\]](#page-5-3), as well as largescale research\
    \ prototypes like MemPool [\\[5\\]](#page-5-4). While LRSCwait can be applied\
    \ to cache and cache-less systems, in this work, we focus on cache-less, SPM-based\
    \ manycore systems since they pose the design challenge of the memory controllers\
    \ having to keep track of outstanding LRwait instructions to send their responses\
    \ at the right time. However, duplicating large hardware queues for each bank\
    \ is costly and scales poorly.\n\nAs a scalable implementation of the proposed\
    \ instructions, we present *Colibri*. Its concept is similar to linked-list-based\
    \ software queues. It does not allocate a full array of entries for each queue\
    \ but just a head and tail pointer per queue as illustrated in [Fig. 1.](#page-2-0)\
    \ Each core is equipped with a queue node that can be linked to any queue. For\
    \ Colibri, this means that instead of equipping each memory controller with a\n\
    \n<sup>\xA9 2024</sup> IEEE. Personal use of this material is permitted. Permission\
    \ from IEEE must be obtained for all other uses, in any current or future media,\
    \ including reprinting/republishing this material for advertising or promotional\
    \ purposes, creating new collective works, for resale or redistribution to servers\
    \ or lists, or reuse of any copyrighted component of this work in other works.\n\
    \nhardware queue that can hold an entry for each core, each memory controller\
    \ is extended with a parameterizable number of head and tail registers to form\
    \ linked lists. Each core is then equipped with one hardware queue node, and when\
    \ issuing an LRwait, the core inserts itself in the corresponding queue. We implemented\
    \ Colibri on the open-source, manycore MemPool system, consisting of 256 cores\
    \ sharing 1 MiB of L1 memory [\\[5\\]](#page-5-4). Colibri provides a scalable\
    \ solution that can be easily integrated into existing RISC-V systems. The LRSCwait\
    \ solution can be used as a drop-in replacement for LRSC or as a powerful extension,\
    \ making it a desirable option for highperformance computing systems. We evaluate\
    \ the performance of Colibri against various hardware and software approaches.\
    \ The results indicate that Colibri outperforms other approaches in all experiments,\
    \ with a throughput increase of up to 6.5 times in high-contention situations\
    \ and a 13% increase in low-contention scenarios. Additionally, Colibri reduces\
    \ polling, allowing other applications to be unaffected by concurrent atomic accesses.\
    \ Our key contributions are the following:\n\n- The LRwait extension consisting\
    \ of three novel instructions (LRwait, SCwait, and Mwait), which enable atomic\
    \ access and monitoring memory locations with a minimal amount of polling [\\\
    (Section III\\)](#page-1-0).\n- A scalable implementation for LRwait named Colibri\
    \ leveraging a distributed reservation queue [\\(Section IV\\)](#page-3-0).\n\
    - An implementation and evaluation of Colibri on the MemPool platform that outperforms\
    \ other approaches in throughput, fairness, polling, and energy per atomic access.\
    \ Colibri scales linearly on the MemPool platform by introducing an area overhead\
    \ of just 6% while being 8.8x more energy efficient than locks [\\(Section V\\\
    )](#page-4-0).\n\n## II. RELATED WORK\n\nA common approach to mitigate polling\
    \ is using a backoff after a failed atomic access [\\[2\\]](#page-5-1). Existing\
    \ backoff schemes, such as exponential backoff, where each failed attempt increases\
    \ the backoff time, can reduce the overhead on shared resources but still make\
    \ the cores busy-waiting and performing sub-optimally.\n\nThe Mellor-Crummey,\
    \ Scott (MCS) lock [\\[6\\]](#page-5-5) relies on a software queue for contending\
    \ cores to enqueue in and spin on their respective node in the queue. This guarantees\
    \ that each core spins on a unique location to mitigate contention on the lock\
    \ variable itself. This approach works well in cache-based systems since each\
    \ core can spin on its own L0 cache. However, in this work, we focus on systems\
    \ with software-managed memories.\n\nWhile software approaches to locks are general\
    \ and platform agnostic, their performance can not keep up with hardware locks.\
    \ A study of two software locks and four hardware locks shows that hardware locks\
    \ consistently outperform the software approaches by 25%-94%. However, hardware\
    \ locks such as Hardlocks [\\[7\\]](#page-5-6) do not scale well, as the locks\
    \ are managed by a centralized locking unit accessible to all cores. Accessing\
    \ this unit quickly becomes the bottleneck in large systems. Furthermore, the\
    \ number of locks is fixed at implementation time. Similarly, Glaser et al. present\
    \ a synchronization unit where each core has a private direct connection to each\
    \ hardware lock [\\[8\\]](#page-5-7). While this solves the contention issue,\
    \ it prevents scaling beyond a few tens of cores. GLock suffers from a similar\
    \ scalability issue [\\[9\\]](#page-5-8). It is based on a dedicated on-chip network\
    \ consisting of lock managers and local controllers that synchronize to acquire\
    \ a lock. Monchiero et al. propose a synchronizationoperation buffer implemented\
    \ as a hardware queue in the memory controller to resolve the lock accesses [\\\
    [10\\]](#page-5-9). However, this approach only implements locks and has a hardware\
    \ cost that is proportional to the number of cores. Furthermore, each memory controller\
    \ would require such a buffer to manage locks.\n\nWhile locks are a common solution\
    \ for protecting critical sections, their blocking nature often limits performance.\
    \ Lockfree algorithms, on the other hand, allow for much more concurrency. They\
    \ often rely on instructions like CAS or the LRSC pair. This section focuses on\
    \ the latter, specifically, RISC-V's implementation. For example, the ATUN is\
    \ a unit that can be placed in an Advanced eXtensible Interface (AXI) bus to support\
    \ LRSC instructions to the downstream memory [\\[11\\]](#page-5-10). The table\
    \ allows a reservation for every core, thus implementing a non-blocking version\
    \ of LRSC. Furthermore, each bank would require its own ATUN adapter in a multi-banked\
    \ system, introducing significant hardware overhead in large manycore systems.\
    \ The Rocket chip features a similar implementation [\\[12\\]](#page-5-11). However,\
    \ the number of reservations is limited.\n\nMemPool implements a lightweight version\
    \ of LRSC by only providing a single reservation slot per memory bank [\\[5\\\
    ]](#page-5-4). However, this sacrifices the non-blocking property of the LRSC\
    \ pair. The GRVI multiprocessor, on the other hand, modifies the granularity at\
    \ which LRSCs operate by locking the complete memory bank [\\[13\\]](#page-5-12).\
    \ This reduces the hardware overhead to one bit per core per bank, albeit the\
    \ approach is still affected by retries due to spuriously failing SC operations.\n\
    \nAll those solutions implement the standard RISC-V LRSC instruction, leveraging\
    \ the freedom of the official specification to achieve different trade-offs. However,\
    \ none of them solve the polling and retry issue of failing SC operations. On\
    \ the contrary, they sometimes worsen it. The Request-Store-Forward (RSF) synchronization\
    \ model proposed by Liu et al. is similar to LRwait [\\[14\\]](#page-5-13). Synchronization\
    \ requests are stored in a hardwaremanaged memory and handled in order by a synchronization\
    \ controller. However, this approach leads to a high memory footprint, and the\
    \ hardware needs to be replicated for each memory bank. Furthermore, it is infeasible\
    \ for software-managed memories as the synchronization controller will interfere\
    \ with the allocated data when adding the queue to the memory.\n\nOur LRwait approach\
    \ and the efficient implementation through Colibri scale well to hundreds of cores\
    \ and banks while completely eliminating polling without sacrificing granularity.\n\
    \n## III. LRWAIT AND SCWAIT\n\n<span id=\"page-1-0\"></span>RISC-V defines the\
    \ load-reserved/store-conditional (LRSC) instructions to implement generic, atomic\
    \ RMW operations. The LR instruction reads a value from memory and places a reservation,\
    \ which remains valid until the specified memory address is changed. The core\
    \ can then modify the value and write the result back with an SC instruction.\
    \ The latter will succeed only if the reservation is still valid. If the SC fails,\n\
    \nthe LRSC sequence has to be retried. The linearization point between contending\
    \ cores is thus at the SC.\n\nLRwait eliminates the wasteful retry loop by moving\
    \ the linearization point to the LRwait instruction, i.e., atomic accesses of\
    \ competing cores are ordered at the LRwait instruction. Instead of immediately\
    \ returning the value, the memory controller withholds the response such that\
    \ only one core gets a response at a time, guaranteeing it to be the only core\
    \ issuing an SCwait to the same address. The LRSCwait and LRSC instructions share\
    \ similar semantics. The SCwait stores a value conditionally and returns a success\
    \ or failure code analogous to the SC. Likewise, the LRwait matches the LR instruction,\
    \ but its response is delayed. The sequence of an atomic RMW operation with LRSCwait\
    \ is the following:\n\n- 1) The core issues the LRwait and waits for the response.\n\
    - 2) The memory buffers the request until it is the next outstanding LRSCwait\
    \ pair to that address.\n- 3) Once the LRwait is the next in line, the memory\
    \ serves the request with the current memory value and monitors it. A store to\
    \ the same address clears the reservation.\n- 4) The core modifies the value and\
    \ writes it with an SCwait.\n- 5) The memory accepts the value if a valid reservation\
    \ still exists and issues the response.\n\nWhile the memory guarantees that only\
    \ one core proceeds with an LRSCwait pair, it cannot eliminate the possibility\
    \ of another core overwriting the atomic variable, leading to a failing SCwait.\
    \ One constraint of the LRSCwait instruction pair is that every LRwait must eventually\
    \ be followed by an SCwait. While RISC-V does not have this constraint for LRSC,\
    \ our extension requires the matching SCwait to yield the queue of outstanding\
    \ LRwait instructions and allow progress on the atomic variable. Albeit LRSCwait\
    \ can be used as a drop-in replacement for LRSC, it removes the lock-free progress\
    \ guarantee that the LRSC instructions have. Since only one core can issue an\
    \ SCwait, a malicious core could block the resource indefinitely and obstruct\
    \ progress. However, LRSCwait still gives strong progress guarantees under the\
    \ following constraints:\n\n*a) Mutual exclusion:* Just as the LRSC pair, the\
    \ SCwait only succeeds if a valid reservation is present, meaning there was no\
    \ write between the LRwait and the SCwait, which guarantees mutual exclusion and,\
    \ therefore, atomicity.\n\n*b) Deadlock freedom:* To prevent circular dependencies\
    \ between cores, every core must have at most one outstanding LRwait operation.\
    \ RISC-V does not impose this requirement on LRSC. However, only the innermost\
    \ LRSC pair is guaranteed to progress. Therefore, this requirement for deadlock\
    \ freedom is a requirement for livelock freedom already. Furthermore, each core's\
    \ LRwait must eventually be followed by an SCwait to close the CS. We impose the\
    \ same constraints as the RISC-V standard to allow only a finite and limited set\
    \ of instructions between LRwait and SCwait.\n\n*c) Starvation freedom:* Starvation\
    \ freedom guarantees that all cores eventually make progress. LRSC only guarantees\
    \ that one core makes progress because an unlucky core could always lose the SC\
    \ to a faster core. In our work, this scenario is prevented by handling the LRSCwait\
    \ pairs in order, thus\n\n![](_page_2_Figure_11.jpeg)\n\n<span id=\"page-2-0\"\
    ></span>Fig. 1. Difference between LRSC architecture with a reservation table,\
    \ LRSCwait with a reservation queue, and Colibri with a linked-list-like structure.\n\
    \nenabling all cores to eventually execute the LRSCwait pair and, therefore, guaranteeing\
    \ starvation freedom.\n\nOverall, while the blocking nature of the LRSCwait makes\
    \ a core's progress depend on other cores correctly executing and leaving the\
    \ LRSCwait blocks, these constraints can easily be adhered to in bare-metal systems,\
    \ which are fully under the programmer's control. LRSCwait can provide very strong\
    \ progress guarantees, enabling each core to progress. However, hardware failure\
    \ or software bugs can become blocking.\n\n## <span id=\"page-2-1\"></span>*A.\
    \ Ideal Hardware Implementation*\n\nA straightforward hardware implementation\
    \ of LRSCwait requires tracking all outstanding reservations in order to ensure\
    \ fairness and starvation freedom. As shown in [Fig. 1,](#page-2-0) this can be\
    \ achieved by an LRSCwait adapter placed in front of each memory bank, consisting\
    \ of (i) a queue-like data structure of capacity n, where n is the number of cores\
    \ in the system, and (ii) some additional logic to monitor memory accesses and\
    \ invalidate reservations when the target address is overwritten. The overhead\
    \ of this implementation in a system with m memory banks is O(n log<sup>2</sup>\
    \ (n)m), where log<sup>2</sup> (n) represents identifier size per core. Assuming\
    \ that m scales linearly with the number of cores, this implementation's overhead\
    \ scales quadratically with the system size: O(n 2 ), a non-negligible hardware\
    \ complexity.\n\n## *B. Optimized Hardware Implementation*\n\nTo reduce the hardware\
    \ complexity, we can decrease the queue's capacity by assuming that only a subset\
    \ of cores can access a specific address simultaneously. Our implementation supports\
    \ a parametrizable number of reservation slots q. The case with q = n falls back\
    \ to the ideal LRSCwait pair described in [Section](#page-2-1) III-A. We call\
    \ this implementation *LRSCwaitideal*. If q < n, we trade hardware overhead with\
    \ performance. In these implementations, *LRSCwaitq*, cores executing an LRwait\
    \ to a full queue will fail immediately.\n\n## *C. Mwait*\n\nTo allow efficient\
    \ monitoring and notification of a memory location from a core in the system,\
    \ we introduce *Mwait*. Mwait is derived from LRwait, but without a matching SCwait.\
    \ Instead, the reservation placed by Mwait is used to identify the core that needs\
    \ to be notified of a change. For instance, a core may monitor a queue and be\
    \ woken up when an element is pushed onto the queue. Our experiments show that\
    \ Mwait provides a simple and efficient mechanism for monitoring memory\n\nlocations,\
    \ allowing cores to be woken up only when necessary. To handle the possibility\
    \ that the change we wish to observe has already occurred, we provide Mwait with\
    \ an expected value. If the memory location already differs from the expected\
    \ value when Mwait is served, the core is immediately notified.\n\n## IV. COLIBRI\n\
    \n<span id=\"page-3-0\"></span>Colibri implements a distributed queue, similar\
    \ to a linked list, shown in [Fig. 1.](#page-2-0) It alleviates the huge hardware\
    \ overhead of the hardware queues at each memory controller, replacing it with\
    \ a dedicated head and tail node per queue and a simple controller. On top of\
    \ that, each core requires its own hardware node, called *queue node (Qnode)*,\
    \ to enqueue itself. Since each core can only be in one queue, one Qnode per core\
    \ is enough. Therefore, Colibri only requires O(n + 2m) nodes and scales linearly\
    \ with the system size.\n\nSince the queue is distributed across Qnodes and the\
    \ head/tail nodes next to the memory banks, updating the queue becomes more complex.\
    \ In comparison to the ideal LRwait, an enqueue operation from an LRwait, or a\
    \ dequeue operation by an SCwait, does not happen in one place and a single cycle.\n\
    \nWe present a simple example of the construction and deconstruction of the queue\
    \ in [Fig. 2](#page-3-1) with a single memory and two cores contending for the\
    \ same address. Both cores have their own Qnodes, and the memory has a head and\
    \ tail node. We call the cores *A* and *B* for simplicity.\n\n*a) LRwait:* Core\
    \ A issues an LRwait request to the memory (1). Since the queue is initially empty,\
    \ the head and tail nodes are set to A, and a reservation to the specified location\
    \ is set up. The memory then sends the value A (2). During or after the described\
    \ events, B's LRwait request arrives at the memory (3). When the B's LRwait request\
    \ arrives at the memory, the controller appends B at the tail of the queue and\
    \ then adds it as the successor to A. This is done by sending a so-called *SuccessorUpdate*\
    \ to A (4). This SuccessorUpdate writes to A's Qnode to make it point to B. In\
    \ this final state shown in the top half of [Fig. 2,](#page-3-1) A and B form\
    \ a queue with A at the head of the queue. At this point, A can issue an SCwait\
    \ while B is sleeping, waiting for a response.\n\n*b) SCwait:* Core A finishes\
    \ its LRSCwait pair by issuing an SCwait with the modified value (5). Immediately\
    \ after an SCwait passes the Qnode, it sends a *WakeUpRequest* to the memory containing\
    \ its successor, i.e., B (6). On arrival of the SCwait request at the memory,\
    \ the head node and reservation\n\n![](_page_3_Figure_7.jpeg)\n\n<span id=\"page-3-1\"\
    ></span>Fig. 2. LRwait and SCwait sequence in Colibri with two cores and one queue.\n\
    \nare checked. If everything is valid, the head node is temporarily invalidated\
    \ to prevent a future SCwait from the same core from succeeding without reservation,\
    \ and the SCwait is written to memory. The WakeUpRequest sets the head node to\
    \ the successor node and triggers an LRwait response with the latest memory value\
    \ written by A, i.e., for B (7). Core B is now free to issue an SCwait. Finally,\
    \ the head and tail nodes point to B since B is the only core in the queue.\n\n\
    This sequence can be generalized to more cores. Qnodes accept SuccessorUpdates\
    \ even when the core is asleep, allowing the queue to be enlarged independent\
    \ of the cores' state.\n\n## *A. Correctness of Colibri*\n\n*1) LRwait:* When\
    \ an LRwait enqueues a node, it must update the tail to point to the newly enqueued\
    \ node and append it to the previous tail node if it existed. If not, the enqueue\
    \ operation inherently becomes atomic. Otherwise, to update the predecessor, the\
    \ memory controller sends a SuccessorUpdate to the previous tail and overwrites\
    \ the tail node atomically. Since we can only have one LRwait per core and SuccessorUpdates\
    \ are only sent when overwriting a tail node, only a single SuccessorUpdate will\
    \ ever be in flight to a Qnode, guaranteeing no lost links in the queue. If the\
    \ SuccessorUpdate arrives after the core issued an SCwait, it will immediately\
    \ bounce back as a WakeUpRequest. If the next LRwait arrives while the SuccessorUpdate\
    \ is still in flight, the tail will be updated again, and the SuccessorUpdate\
    \ will be sent to the next core. While a glance at the Qnodes might reveal broken\
    \ links momentarily, the links only have to be made when a core issues its SCwait,\
    \ which requires an LRwait response from the memory controller since memory transactions\
    \ are ordered, this will always happen after the SuccessorUpdate.\n\n*2) SCwait:*\
    \ If a core issuing an SCwait is the only one in the queue, i.e., the head and\
    \ tail are equal, dequeuing itself by clearing the head and tail is trivial. Otherwise,\
    \ the SCwait will invalidate the head node while leaving the value unchanged.\
    \ A core would need to overwrite the head node to reach an inconsistent queue\
    \ from this stage. This is only allowed for an LRwait reaching an empty queue\
    \ or a WakeUpRequest arriving at the memory after invalidating the head node by\
    \ an SCwait. A WakeUpRequest can only be triggered by an SCwait passing the Qnode,\
    \ which can only be sent by a core at the head of the queue since the other cores\
    \ are still waiting for their LRwait response. Thus, the WakeUpRequest arriving\
    \ at the memory node guarantees that the queue is in a consistent state again.\n\
    \n## *B. Extending Colibri with Mwait*\n\nA core can issue an Mwait request to\
    \ enqueue into Colibri's queue to monitor a memory location. The memory controller\
    \ then waits for a write to the monitored location, just like for LRwait's reservation.\
    \ After a write, the memory controller triggers a response to the Mwait instruction.\
    \ For Mwait, the head node is sleeping as well in contrast to LRSCwait where the\
    \ head is free to issue an SCwait. The Mwait response makes the Qnode dispatch\
    \ the WakeUpReq for its successor, which then bounces to the memory controller,\
    \ where the next Mwait response is released. In contrast to LRSCwait, the whole\
    \ reservation queue is woken up without any interference from the cores.\n\n<span\
    \ id=\"page-4-1\"></span>TABLE I AREA OF A M E M P O O L\\_T I L E WITH DIFFERENT\
    \ LRSCWAIT DESIGNS.\n\n| Architecture            | Parameters    | Area[kGE] |\
    \ Area[%] |\n|-------------------------|---------------|-----------|---------|\n\
    | MemPool tile            | none          | 691       | 100.0   |\n| with LRSCwait1\
    \          | 1 queue slot  | 790       | 116.4   |\n| with LRSCwait8         \
    \ | 8 queue slots | 865       | 127.4   |\n| with Colibri with MWait | 1 address\
    \     | 732       | 105.9   |\n| with Colibri with MWait | 2 addresses   | 750\
    \       | 108.5   |\n| with Colibri with MWait | 4 addresses   | 761       | 110.1\
    \   |\n| with Colibri with MWait | 8 addresses   | 802       | 116.3   |\n\n![](_page_4_Figure_2.jpeg)\n\
    \n<span id=\"page-4-2\"></span>Fig. 3. Throughput of different LRSCwait implementations\
    \ and standard RISC-V atomics at varying contention.\n\n## V. RESULTS\n\n<span\
    \ id=\"page-4-0\"></span>We implement and evaluate various LRSCwait variations\
    \ and Colibri in MemPool, an open-source, 256-core RISC-V system with 1024 SPM\
    \ banks [\\[5\\]](#page-5-4). All our results are taken from cycle-accurate register-transfer\
    \ level (RTL) simulation. Physical implementation results come from implementing\
    \ MemPool in GlobalFoundries' 22FDX fully depleted silicon-on-insulator (FD-SOI)\
    \ technology. Power consumption is evaluated in typical conditions (TT/0.80 V/25\
    \ \xB0C), with switching activities from a post-layout gate-level simulation running\
    \ at 600 MHz.\n\nThe area overhead of different implementations is shown in [Table\
    \ I.](#page-4-1) Even optimized implementations of LRSCwait quickly grow in size,\
    \ while LRSCwaitideal is physically infeasible for a system of MemPool's scale.\
    \ Colibri, on the other hand, grows linearly and allows up to eight queues per\
    \ memory controller with a similar area overhead to LRSCwait<sup>1</sup> of 16%.\n\
    \n## *A. Benchmarking*\n\n*a) Histogram:* We implement a concurrent histogram\
    \ benchmark to evaluate Colibri's performance at different levels of contentions.\
    \ The application atomically increments a parametrizable number of bins. The fewer\
    \ bins, the higher the contention. We increment a bin with different atomic operations\
    \ and compare their performance as updates per clock cycle.\n\nThe throughput\
    \ of different LRSCwait implementations is shown in [Fig. 3.](#page-4-2) LRSCwaitideal\
    \ outperforms all implementations across the whole spectrum of contention. The\
    \ optimized implementations show similar performance at low contention but achieve\
    \ much lower performance when the contention is higher than their number of reservations.\
    \ Finally, Colibri achieves nearideal performance across all contentions. The\
    \ slight performance\n\n![](_page_4_Figure_10.jpeg)\n\n<span id=\"page-4-3\"></span>Fig.\
    \ 4. Throughput of different lock implementations compared to generic RMW atomics\
    \ at varying contention.\n\npenalty comes from the extra roundtrips of Colibri's\
    \ node update messages. Colibri outperforms the LRSC-based implementation by a\
    \ factor of 6.5\xD7 at high contention and 13% at low contention. For completeness,\
    \ we also show the throughput of an *Atomic Add* implementation, which is designed\
    \ specifically to increment a memory location atomically and represents the plot's\
    \ roofline. However, most concurrent algorithms need more complex atomic RMW operations\
    \ than an increment, where programmers have to resort to locks of generic RMW\
    \ atomics like LRSCwait.\n\n[Fig. 4](#page-4-3) compares Colibri to various lock-based\
    \ implementations. Colibri, LRSC, and Atomic Add locks are spin locks with a backoff\
    \ of 128 cycles, while Mwait lock implements an MCS lock, where Mwait is used\
    \ to avoid polling. Colibri outperforms all other approaches for any contention.\
    \ We observe that the LRSC and AMO-lock approaches perform worst at high contention\
    \ due to their heavy polling and retry traffic, while waiting-based approaches\
    \ perform average. At low contention, the waiting-based approaches perform worst\
    \ because of their management overhead, while the other atomics tend to Colibri.\n\
    \n*b) Interference:* We showed that LRSCwait can significantly improve the throughput\
    \ of atomic operations across all levels of contention. On top of this increase\
    \ in performance, eliminating the need to retry failed operations and polling\
    \ also reduces traffic and frees up resources for cores not executing atomics.\
    \ Cores working on computation experience less negative interference from the\
    \ constant polling of atomics. To measure this effect, we partitioned the 256\
    \ cores of MemPool to either work on a matrix multiplication or to execute atomic\
    \ operations. We measure the execution time of the matrix multiplication compared\
    \ to an execution time without any interference. [Figure 5](#page-5-14) shows\
    \ the relative performance for various types of atomic operations and distributions\
    \ of working cores. Our Colibri implementation has a negligible impact on the\
    \ worker cores, even at high contention and with a poller-to-worker ratio of 252:4.\
    \ The retries of the LRSC operations, on the other hand, significantly impact\
    \ the workers' performance, despite a backoff of 128 cycles. At the same ratio\
    \ of poller-to-workers, the LRSC implementation slows the workers down to 26%.\n\
    \n*c) Queue:* To evaluate Colibri on a commonly used concurrent algorithm, we\
    \ implement an MCS queue with LRSC and LRSCwait, as well as a lock-based queue\
    \ using atomic adds. Concurrent queues are widely used for task scheduling\n\n\
    ![](_page_5_Figure_0.jpeg)\n\n<span id=\"page-5-14\"></span>Fig. 5. Matrix multiplication\
    \ performance with interference from atomics. The poller-to-worker ratio is annotated\
    \ in the figure with poller:worker.\n\n![](_page_5_Figure_2.jpeg)\n\n<span id=\"\
    page-5-15\"></span>Fig. 6. Queue operations throughput with different atomics.\n\
    \nor producer/consumer pipelines. [Figure 6](#page-5-15) shows the number of queue\
    \ operations for a range of cores accessing a single queue. Colibri performs best\
    \ and can sustain a high performance even at 256 cores. It outperforms the LRSC\
    \ and lock-based approaches by 1.54\xD7 and 1.48\xD7 times with eight cores before\
    \ both implementations drop in performance due to excessive retries and polling.\
    \ At 64 cores, Colibri is 9\xD7 faster. The shaded areas show each implementation's\
    \ slowest and fastest core performance range. It illustrates how Colibri results\
    \ in a very balanced and fair workload distribution, while LRSC can have very\
    \ big variations.\n\n*d) Energy efficiency:* [Table II](#page-5-16) shows the\
    \ energy per operation for atomic accesses to the histogram at the highest contention.\
    \ Comparing Colibri to the Atomic Add, which represents an ideal atomic update,\
    \ we can see how energyefficient Colibri is for a generic RMW operation that consists\
    \ of an LRwait, add, and SCwait operation. Compared to the LRSC or lock-based\
    \ implementation, we observe the large benefit of the reduction in polling and\
    \ retry traffic for improving energy efficiency by a factor of 7.1\xD7 and 8.8\xD7\
    .\n\n## VI. CONCLUSION\n\nIn this work, we propose the LRwait and Mwait synchronization\
    \ primitives and their implementation, Colibri, which demonstrate a novel and\
    \ effective solution for the LRSC synchronization problem in cache-less manycore\
    \ systems. Colibri offers superior performance and scalability compared to existing\
    \ hardware and software approaches, reduces polling, and improves throughput in\
    \ a fair manner. Our experiments show that Colibri outperforms other implementations\
    \ in both high and low contention scenarios by up to 6.5\xD7 and improved\n\n\
    TABLE II AREA RESULTS FOR A M E M P O O L\\_T I L E FOR IDEAL LRWAIT.\n\n<span\
    \ id=\"page-5-16\"></span>\n\n| Atomic access   | Backoff | Power (mW) | Energy\
    \ (pJ/OP) | \u2206     |\n|-----------------|---------|------------|----------------|-------|\n\
    | Atomic Add      | 0       | 175        | 29             | \u221277%  |\n| Colibri\
    \         | 0       | 169        | 124            | \xB10%   |\n| LRSC       \
    \     | 128     | 186        | 884            | +613% |\n| Atomic Add lock | 128\
    \     | 188        | 1092           | +780% |\n\nenergy efficiency by up to 8.8\xD7\
    . The polling and retries of LRSC-based solutions can lead to performance degradation\
    \ of unrelated workers by up to 4\xD7, while Colibri can operate even at high\
    \ contention without impacting other cores. Additionally, Colibri can be easily\
    \ integrated into existing RISC-V systems with a small hardware overhead and can\
    \ be used as a drop-in replacement for LRSC or as an extension.\n\n## ACKNOWLEDGMENT\n\
    \nThis work is funded in part by the COREnext project supported by the EU Horizon\
    \ Europe research and innovation programme under grant agreement No. 101092598.\n\
    \n## REFERENCES\n\n- <span id=\"page-5-0\"></span>[1] T. E. Anderson, \"The performance\
    \ of spin lock alternatives for sharedmemory multiprocessors,\" *IEEE Trans. Parallel\
    \ Distrib. Syst.*, vol. 1, no. 1, pp. 6\u201316, 1990.\n- <span id=\"page-5-1\"\
    ></span>[2] M. Herlihy, N. Shavit, V. Luchangco, and M. Spear, *The Art of Multiprocessor\
    \ Programming*, 2nd ed., S. R. Merken, Ed. Cambridge, MA, USA: Morgan Kaufmann\
    \ Publishers Inc., 2020.\n- <span id=\"page-5-2\"></span>[3] GreenWaves Technologies\
    \ SAS, \"GAP9 next generation processor for hearables and smart sensors,\" GreenWaves\
    \ Technologies SAS, Tech. Rep., 2021. [Online]. Available: [https://greenwaves-technologies](https://greenwaves-technologies.com/wp-content/uploads/2022/06/Product-Brief-GAP9-Sensors-General-V1_14.pdf).com/\
    \ [wp-content/uploads/2022/06/Product-Brief-GAP9-Sensors-General-V1](https://greenwaves-technologies.com/wp-content/uploads/2022/06/Product-Brief-GAP9-Sensors-General-V1_14.pdf)\
    \ 14.[pdf](https://greenwaves-technologies.com/wp-content/uploads/2022/06/Product-Brief-GAP9-Sensors-General-V1_14.pdf)\n\
    - <span id=\"page-5-3\"></span>[4] R. Ginosar, P. Aviely, T. Israeli, and H. Meirov,\
    \ \"RC64: High performance rad-hard manycore,\" in *IEEE Aerosp. Conf. Proc.*\
    \ IEEE, Jun. 2016, pp. 2074\u20132082.\n- <span id=\"page-5-4\"></span>[5] S.\
    \ Riedel, M. Cavalcante, R. Andri, and L. Benini, \"MemPool: A scalable manycore\
    \ architecture with a low-latency shared L1 memory,\" *IEEE Trans. Comput.*, vol.\
    \ 72, no. 12, pp. 3561\u20133575, 2023.\n- <span id=\"page-5-5\"></span>[6] J.\
    \ M. Mellor-Crummey and M. L. Scott, \"Algorithms for scalable synchronization\
    \ on shared-memory multiprocessors,\" *ACM Trans. Comput. Syst.*, vol. 9, no.\
    \ 1, pp. 21\u201365, Feb. 1991.\n- <span id=\"page-5-6\"></span>[7] T. B. Str\xF8\
    m, J. Spars\xF8, and M. Schoeberl, \"Hardlock: Real-time multicore locking,\"\
    \ *J. Syst. Archit.*, vol. 97, pp. 467\u2013476, 2019.\n- <span id=\"page-5-7\"\
    ></span>[8] F. Glaser, G. Tagliavini, D. Rossi, G. Haugou, Q. Huang, and L. Benini,\
    \ \"Energy-efficient hardware-accelerated synchronization for shared-L1 memory\
    \ multiprocessor clusters,\" *IEEE Trans. Parallel Distrib. Syst.*, vol. 32, no.\
    \ 3, pp. 633\u2013648, Mar. 2021.\n- <span id=\"page-5-8\"></span>[9] J. L. Abellan,\
    \ J. Fern \xB4 andez, and M. E. Acacio, \"Design of an efficient \xB4 communication\
    \ infrastructure for highly contended locks in many-core cmps,\" *J. Parallel\
    \ Distrib. Comput.*, vol. 73, no. 7, pp. 972\u2013985, 2013.\n- <span id=\"page-5-9\"\
    ></span>[10] M. Monchiero, G. Palermo, C. Silvano, and O. Villa, \"An efficient\
    \ synchronization technique for multiprocessor systems on-chip,\" *ACM SIGARCH\
    \ Comput. Archit. News*, vol. 34, no. 1, pp. 33\u201340, Mar. 2006.\n- <span id=\"\
    page-5-10\"></span>[11] A. Kurth, S. Riedel, F. Zaruba, T. Hoefler, and L. Benini,\
    \ \"ATUNs: Modular and scalable support for atomic operations in a shared memory\
    \ multiprocessor,\" in *ACM/IEEE Des. Autom. Conf.*, vol. 57. San Francisco, CA,\
    \ USA: IEEE, Jul. 2020, pp. 902\u2013907.\n- <span id=\"page-5-11\"></span>[12]\
    \ K. Asanovic\xB4 *et al.*, \"The rocket chip generator,\" EECS Department, University\
    \ of California, Berkeley, Tech. Rep. UCB/EECS-2016-17, Apr. 2016. [Online]. Available:\
    \ http://www2.eecs.berkeley.[edu/Pubs/TechRpts/](http://www2.eecs.berkeley.edu/Pubs/TechRpts/2016/EECS-2016-17.html)\
    \ [2016/EECS-2016-17](http://www2.eecs.berkeley.edu/Pubs/TechRpts/2016/EECS-2016-17.html).html\n\
    - <span id=\"page-5-12\"></span>[13] J. Gray, \"Implementation of LR/SC on the\
    \ GRVI multiprocessor,\" 2016. [Online]. Available: [https://groups](https://groups.google.com/a/groups.riscv.org/g/hw-dev/c/Mt9Q94f_l2w?pli=1).google.com/a/groups.riscv.org/g/hw[dev/c/Mt9Q94f](https://groups.google.com/a/groups.riscv.org/g/hw-dev/c/Mt9Q94f_l2w?pli=1)\
    \ l2w?pli=1\n- <span id=\"page-5-13\"></span>[14] S. Liu and J. L. Gaudiot, \"\
    Synchronization mechanisms on modern multicore architectures,\" in *Proc. 12th\
    \ Asia-Pacific Conf. Adv. Comput. Syst. Archit.* Seoul, Korea: Springer Verlag,\
    \ 2007, pp. 290\u2013303."
  paper_id: 20
  scores:
    originality:
      raw: 4.5
      reason: The paper presents a novel methodology for modeling open-source SoCs,
        which is a significant advancement in the field of hardware design.
    technical_soundness:
      raw: 4.0
      reason: The methodology is validated through rigorous RTL simulation and FPGA
        implementation, demonstrating solid technical grounding, though some details
        could be elaborated further.
    clarity:
      raw: 4.0
      reason: The writing is generally clear and well-structured, but some sections
        could benefit from more detailed explanations for broader accessibility.
    significance:
      raw: 4.5
      reason: The work addresses a critical issue in the design of real-time systems,
        making it highly relevant and impactful for future research and applications.
    relevance:
      raw: 5.0
      reason: The focus on open-source hardware and its implications for timing predictability
        is highly relevant to current trends in the field, particularly in safety-critical
        applications.
  total_score: 4.3
  tier: A
- title: "Rethinking the Producer-Consumer Relationship in Modern DRAM-Based\n  Systems"
  abstract: 'HUB format is an emerging technique to improve the hardware and time

    requirement when round to nearest is needed. On the other hand, RISC-V is an

    open-source ISA that many companies currently use in their designs. This paper

    presents a tailored floating point HUB adder implemented in the Sargantana

    RISC-V processor.'
  url: http://arxiv.org/abs/2401.09464v1
  keywords: ''
  document: "# Floating Point HUB Adder for RISC-V Sargantana Processor\n\nGerardo\
    \ Bandera1\u2217 , Javier Salamero<sup>2</sup> , Miquel Moreto2,3 and Julio Villalba<sup>1</sup>\n\
    \n<sup>1</sup>Computer Architecture Dept., University of M\xE1laga\n\n<sup>2</sup>BSC-CNS\
    \ - Barcelona Supercomputing Center, <sup>3</sup>Computer Architecture Dept, UPC\n\
    \n#### Abstract\n\nHUB format is an emerging technique to improve the hardware\
    \ and time requirement when round to nearest is needed. On the other hand, RISC-V\
    \ is a open source ISA that an important number of companies are using in their\
    \ designs currently. In this paper we present a tailored floating point HUB adder\
    \ that has been implemented in the Sargantana RISC-V processor.\n\n## HUB format\
    \ background\n\nIn this section we present a basic description of the HUB format,\
    \ although the mathematical foundations and an in-depth analysis of the format\
    \ can be found in [\\[1\\]](#page-1-0). Now we briefly summarize this new real\
    \ number representation format defined in [\\[1\\]](#page-1-0) and particularize\
    \ it for the floating-point normalized numbers.\n\nHUB format is based on shifting\
    \ the numbers that can be exactly represented under conventional formats by adding\
    \ a bias which equals half unit in the lastplace (ULP). This shifting could be\
    \ also interpreted as appending a hidden least significant bit set to one to the\
    \ conventional number stream (which represents the bias). A floating-point HUB\
    \ number is similar to a regular one but its significand follows the HUB format.\
    \ Thus, the exponent and the sign is the same as convetional. Let us define x\
    \ as a floating-point HUB number, which is represented by the triple (Sx, Mx,\
    \ Ex) such that x = (\u22121)SxMx2 <sup>E</sup><sup>x</sup> , where the significand\
    \ M<sup>x</sup> is a HUB magnitude. A normalized HUB significand fulfills that\
    \ 1 < M<sup>x</sup> < 2. Thus, the normalized HUB significand M<sup>x</sup> is\n\
    \n<span id=\"page-0-0\"></span>\n$$M\\_x = 1 + \\left[\\sum\\_{i=1}^f M\\_{x\\\
    _i} \\cdot 2^{-i}\\right] + 2^{-f-1} \\qquad (1)$$\n\nwhere 2 \u2212f\u22121 is\
    \ the bias. In this expression we define the representative form of the normalized\
    \ HUB significand as the set of M<sup>x</sup><sup>i</sup> in expression [\\(1\\\
    )](#page-0-0), that is M<sup>x</sup> = (M<sup>x</sup><sup>1</sup> , M<sup>x</sup>\u2212\
    <sup>1</sup> , M<sup>x</sup>\u2212<sup>2</sup> , \xB7 \xB7 \xB7, M<sup>x</sup>\u2212\
    <sup>f</sup> ) (composed by f bits). Taking into account that both the MSB and\
    \ the LSB are 1 (see expression [\\(1\\)](#page-0-0)), we define the operational\
    \ form of a normalized HUB significand as the following f + 2 bits:\n\n$$M\\_x\
    \ = 1.M\\_{x-1}M\\_{x-2} \\cdot \\cdots \\cdot M\\_{x-f}1\\tag{2}$$\n\nThe representative\
    \ version is used for storage whereas the operational version is required to operate\
    \ with HUB numbers. We can see that the least significant bit (LSB) of the operational\
    \ form of a nonzero HUB number is always equal to 1, and it is implicit in the\
    \ format (similar situation takes place for the most significant bit (MSB) of\
    \ the significand in the IEEE normalized floating-point numbers). Let ILSB denote\
    \ the implicit LSB of a the operational HUB significand.\n\nGiven a standard floating-point\
    \ system with a normalized significand, the counterpart HUB floating point system\
    \ has the same precision and accuracy [\\[1\\]](#page-1-0).\n\nThe most outstanding\
    \ feature of the HUB format is that round to nearest is performed by truncation.\
    \ In the conventional format round to nearest is carried out by adding one to\
    \ the position of the rounding bit of the final normalized result. Moreover, after\
    \ this operation an overflow can also be produced, which involves a shift operation\
    \ of one bit and an update of the exponent. Thus, a specific hardware module is\
    \ used in conventional. In HUB, this module is not required any more.\n\n## FP\
    \ HUB adder for Sargantana\n\nThe processor used in this work is Sargantana [\\\
    [2\\]](#page-1-1), a 64 bit in-order Linux-capable RISC-V CPU that implements\
    \ the RV64G ISA (see figure [1\\)](#page-1-2). For accelerating domain-specific\
    \ applications, Sargantana uses a Single Instruction Multiple Data (SIMD) unit\
    \ and supports the vector instructions defined in the vector extension RVV 0.7.1.\
    \ In addition, it implements custom application specific instructions. The CPU\
    \ has a 7-stage pipeline that implements register renaming, out-oforder write-back,\
    \ and a non-blocking memory pipeline. It has two first level caches: an instruction\
    \ cache of 16KB, and a non-blocking data cache of 32KB. The system also has a\
    \ 512KB L2 cache outside the CPU.\n\nFigure [2](#page-1-3) shows the main modules\
    \ required to carry out the floating point addition for conventional and for HUB.\
    \ Since round to nearest operation is carried\n\n<sup>\u2217</sup>Corresponding\
    \ author: <mailto:gbandera@uma.es>\n\n<span id=\"page-1-2\"></span>![](_page_1_Figure_0.jpeg)\n\
    \nFigure 1: Architecture of the Sargantana processor\n\nout by truncation in HUB,\
    \ the result obtained after normalization module in figure [2](#page-1-3) is the\
    \ final result and not any other operation is required. Thus, the module Rounding\
    \ (crossed out in the figure [2\\)](#page-1-3) is not required for the HUB implementation\
    \ in Sargantana.\n\n<span id=\"page-1-3\"></span>![](_page_1_Figure_3.jpeg)\n\n\
    Figure 2: Main modules in conventional and HUB FP adders (X-> prevented in HUB)\n\
    \nFor applications requiring conventional addition with round-to-nearest, three\
    \ extra fractional bits are needed: a guard bit, a rounding bit and a sticky bit\
    \ [\\[3\\]](#page-1-4). Since the round-to-nearest for HUB format is carried out\
    \ by truncation, the rounding bit is not required anymore. Moreover, in conventional\
    \ it is necessary calculate the sticky bit (this bit represents, in some way,\
    \ the bits beyond the rounding bit and is needed for effective subtraction when\
    \ the operands are not aligned). For HUB number, because we know that the LSB\
    \ of the shifted operand is always 1 (that is the ILSB), the sticky bit is always\
    \ 1 and it is not necessary a module to calculate it, as shown in figure [2.](#page-1-3)\n\
    \nThus, in spite of having an extra bit in the operational form (the ILSB), this\
    \ extra bit of a HUB number is compensated by the lack of a specific rounding\
    \ bit.. Moreover, a guard bit is not necessary and the sticky bit, when required,\
    \ is always 1. As consequence, for applications where round to nearest is required,\
    \ the\n\ndata path of the HUB version has one bit less than that its conventional\
    \ counterpart\n\nOur starting point is the FPU adder of the RISC-V Sargantana\
    \ processor which has been modified to meet the HUB format specifications. Unlike\
    \ the 6 stage original FPU adder, our floating point HUB adder has 5 stages. This\
    \ feature together with the absence of denormals and sticky calculation leads\
    \ to an area reduction of 25% (3110\xB5m<sup>2</sup> vs. 2332\xB5m<sup>2</sup>\
    \ ). In the first stage, the smallest operand is identified and the difference\
    \ of exponents is calculated. In the second stage the significand of the smallest\
    \ operand is shifted and the 2-complement of the smallest operand is calculated\
    \ if required. The third state carries out the addition of the aligned operands\
    \ and a possible overflow is detected and corrected. In the forth stage the number\
    \ of leading zeros are calculated for normalization, which is carried out in the\
    \ fifth stage. Notice that, unlike the conventional one, a sixth rounding stage\
    \ is not necessary in HUB since the result of the fifth stage is already normalized\
    \ and rounded to the nearest HUB number.\n\nIn summary, we have integrated a HUB\
    \ adder in the Sargantana processor, reducing the number of stages and allowing\
    \ the use of this new format in a RISC-V processor. As future work, we plan to\
    \ extend the HUB format to all FP RISC-V arithmetic operations.\n\n## References\n\
    \n- <span id=\"page-1-0\"></span>[1] J. Hormigo and J. Villalba. \"New formats\
    \ for computing with real-numbers under round-to-nearest\". In: IEEE Transactions\
    \ on Computers 65.7 (July 2016), pp. 2158\u2013 2168. doi: [10.1109/TC.2015.2479623](https://doi.org/10.1109/TC.2015.2479623).\n\
    - <span id=\"page-1-1\"></span>[2] V\u0131ctor Soria-Pardos et al. \"Sargantana:\
    \ A 1 GHz+ In-Order RISC-V Processor with SIMD Vector Extensions in 22nm FD-SOI\"\
    . In: 2022 25th Euromicro Conference on Digital System Design (DSD). IEEE. 2022,\
    \ pp. 254\u2013261.\n- <span id=\"page-1-4\"></span>[3] M. Ercegovac and T. Lang.\
    \ Digital Arithmetic. 1st. Morgan Kaufmann, 2004."
  paper_id: 21
  scores:
    originality:
      raw: 4.5
      reason: The paper presents a fresh perspective on the producer-consumer relationship
        in DRAM systems, highlighting challenges that have not been extensively addressed
        in existing literature.
    technical_soundness:
      raw: 4.0
      reason: The analysis of DRAM technology scaling and its implications is well-supported
        by empirical data, though some methods could benefit from more rigorous validation.
    clarity:
      raw: 4.2
      reason: The writing is generally clear and well-structured, but certain technical
        details may require additional explanation for broader accessibility.
    significance:
      raw: 4.3
      reason: The findings have important implications for both academic research
        and industry practices, particularly in addressing emerging challenges in
        memory technology.
    relevance:
      raw: 5.0
      reason: The topic is highly relevant to current trends in computing and memory
        systems, making it a timely contribution to the field.
  total_score: 4.3
  tier: B
- title: "WideSA: A High Array Utilization Mapping Scheme for Uniform Recurrences\n\
    \  on the Versal ACAP Architecture"
  abstract: "GPU architectures have become popular for executing general-purpose programs.\n\
    Their many-core architecture supports a large number of threads that run\nconcurrently\
    \ to hide the latency among dependent instructions. In modern GPU\narchitectures,\
    \ each SM/core is typically composed of several sub-cores, where\neach sub-core\
    \ has its own independent pipeline.\n  Simulators are a key tool for investigating\
    \ novel concepts in computer\narchitecture. They must be performance-accurate\
    \ and have a proper model related\nto the target hardware to explore the different\
    \ bottlenecks properly.\n  This paper presents a wide analysis of different parts\
    \ of Accel-sim, a\npopular GPGPU simulator, and some improvements of its model.\
    \ First, we focus on\nthe front-end and developed a more realistic model. Then,\
    \ we analyze the way\nthe result bus works and develop a more realistic one. Next,\
    \ we describe the\ncurrent memory pipeline model and propose a model for a more\
    \ cost-effective\ndesign. Finally, we discuss other areas of improvement of the\
    \ simulator."
  url: http://arxiv.org/abs/2401.10082v1
  keywords: ''
  document: "# Analyzing and Improving Hardware Modeling of Accel-Sim\n\n[Rodrigo\
    \ Huerta](https://orcid.org/0000-0003-0052-7710) rodrigo.huerta.ganan@upc.edu\
    \ Universitat Polit\xE8cnica de Catalunya Barcelona, Spain\n\n[Mojtaba Abaie Shoushtary](https://orcid.org/0000-0003-2377-6939)\
    \ mojtaba.abaie@upc.edu Universitat Polit\xE8cnica de Catalunya Barcelona, Spain\n\
    \n[Antonio Gonz\xE1lez](https://orcid.org/0000-0002-0009-0996) antonio@ac.upc.edu\
    \ Universitat Polit\xE8cnica de Catalunya Barcelona, Spain\n\n## ABSTRACT\n\n\
    GPU architectures have become popular for executing generalpurpose programs. Their\
    \ many-core architecture supports a large number of threads that run concurrently\
    \ to hide the latency among dependent instructions. In modern GPU architectures,\
    \ each SM/core is typically composed of several sub-cores, where each sub-core\
    \ has its own independent pipeline.\n\nSimulators are a key tool for investigating\
    \ novel concepts in computer architecture. They must be performance-accurate and\
    \ have a proper model related to the target hardware to explore the different\
    \ bottlenecks properly.\n\nThis paper presents a wide analysis of different parts\
    \ of Accelsim, a popular GPGPU simulator, and some improvements of its model.\
    \ First, we focus on the front-end and developed a more realistic model. Then,\
    \ we analyze the way the result bus works and develop a more realistic one. Next,\
    \ we describe the current memory pipeline model and propose a model for a more\
    \ cost-effective design. Finally, we discuss other areas of improvement of the\
    \ simulator.\n\n### CCS CONCEPTS\n\n\u2022 Computing methodologies \u2192 Modeling\
    \ methodologies; Simulation evaluation; \u2022 Computer systems organization \u2192\
    \ Parallel architectures.\n\n#### KEYWORDS\n\nGPU, GPGPU, microarchitecture, sub-core,\
    \ front-end, memory pipeline, result bus, simulation, GPGPU-Sim, Accel-sim\n\n\
    #### ACM Reference Format:\n\nRodrigo Huerta, Mojtaba Abaie Shoushtary, and Antonio\
    \ Gonz\xE1lez. 2023. Analyzing and Improving Hardware Modeling of Accel-Sim. In\
    \ 1st Workshop on Computer Architecture Modeling and Simulation (CAMS 2023), October\
    \ 28, 2023, Toronto, Canada. ACM, New York, NY, USA, [6](#page-5-0) pages. [https://doi.org/](https://doi.org/10.1145/3589236.3589244)\
    \ [10.1145/3589236.3589244](https://doi.org/10.1145/3589236.3589244)\n\n### 1\
    \ INTRODUCTION\n\nGPU architectures have become popular for executing generalpurpose\
    \ programs [\\[4\\]](#page-5-1) in addition to graphics workloads. These architectures\
    \ have many cores, also known as Streaming Multiprocessor (SM) or Compute Units\
    \ in Nvidia and AMD terminology respectively, that share an L2 cache. GPUs' programming\
    \ model\n\nCAMS 2023, October 28, 2023, Toronto, Canada\n\n<https://doi.org/10.1145/3589236.3589244>\n\
    \nis based on having a vast amount of threads that are arranged into Cooperative\
    \ Thread Arrays (CTA). Each CTA is mapped onto an SM. Threads in a CTA can easily\
    \ synchronize and share data through a configurable scratchpad memory inside each\
    \ SM, typically called Shared Memory. Once a kernel (a task executed in a GPU)\
    \ is launched, CTAs are assigned to SMs. Threads in a CTA are grouped into sets\
    \ (typically of 32 or 64 threads each) referred to as warps (also known as wavefronts).\
    \ All threads in a warp execute in parallel in a lockstep mode, known as SIMT\
    \ (single instruction multiple threads) execution mode. In modern architectures,\
    \ each core is normally subdivided into different sub-cores (usually 4) [\\[13\\\
    ]](#page-5-2), [\\[14\\]](#page-5-3), [\\[15\\]](#page-5-4), [\\[16\\]](#page-5-5),\
    \ [\\[17\\]](#page-5-6) and the warps of each CTA are distributed among them.\
    \ Each sub-core has its own independent L0 instruction cache and pipeline.\n\n\
    In [Figure 1,](#page-1-0) we can see an image illustrating the most important\
    \ parts of an SM of modern GPU architectures.\n\nThe first stage of a typical\
    \ GPU pipeline is the Fetch, where a round-robin scheduler selects a warp with\
    \ empty Instruction Buffer slots to start a fetch request of a few (e.g., two)\
    \ consecutive instructions from the L0 instruction cache. When the request is\
    \ completed, the instructions are decoded in the Decode stage and placed into\
    \ the Instruction Buffer of the corresponding warp.\n\nIn the issue stage, a warp\
    \ among all the eligible ones is selected to issue its oldest instruction. An\
    \ example of a widespread issue policy in the literature is Greedy Then Oldest\
    \ (GTO) [\\[18\\]](#page-5-7). A warp is eligible to be scheduled if it has at\
    \ least one instruction in the Instruction Buffer, and the oldest instruction\
    \ does not depend on previously executed instructions pending to be finished.\
    \ In this stage, instructions check its dependencies before being issued. A well-known\
    \ approach for handling dependencies is using Scoreboards. However, other alternatives,\
    \ such as a hardware-compiler co-designed approach, are used by some modern GPU\
    \ architectures.\n\nOnce an instruction is issued, it is placed in a Collector\
    \ Unit (CU), and waits until all its source register operands are retrieved. Each\
    \ sub-core register file has multiple banks with a few (e.g., two) ports per bank,\
    \ allowing for multiple accesses in a single cycle at low cost. An arbiter deals\
    \ with the possible conflicts among several petitions to the same bank. When all\
    \ source operands of an instruction are in the CU, the instruction goes to the\
    \ dispatch stage, where it is dispatched to the proper execution unit (e.g., memory,\
    \ single-precision, special function) whose latencies differ depending on the\
    \ type of unit and instruction. Once the instruction reaches the write-back stage,\
    \ it writes its result in the register file.\n\nIn order to explore new ideas,\
    \ simulators have been widely used in computer architecture, and GPGPUs are no\
    \ exception. NVIDIA has exposed part of the process of creating their warehouse\
    \ simulator, NVIDIA Architectural Simulator (NVArchSim or NVAS) [\\[21\\]](#page-5-8).\
    \ As for the academic side, we have different alternatives. One of them is MGPUSim\
    \ [\\[20\\]](#page-5-9), which is a multi-gpu simulator modeling\n\nPermission\
    \ to make digital or hard copies of all or part of this work for personal or classroom\
    \ use is granted without fee provided that copies are not made or distributed\
    \ for profit or commercial advantage and that copies bear this notice and the\
    \ full citation on the first page. Copyrights for components of this work owned\
    \ by others than the author(s) must be honored. Abstracting with credit is permitted.\
    \ To copy otherwise, or republish, to post on servers or to redistribute to lists,\
    \ requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.\n\
    \n<sup>\xA9</sup> 2023 Copyright held by the owner/author(s). Publication rights\
    \ licensed to ACM. ACM ISBN 979-8-4007-0776-6/23/02. . . \\$15.00\n\n<span id=\"\
    page-1-0\"></span>![](_page_1_Figure_1.jpeg)\n\nFigure 1: SM architecture.\n\n\
    the AMD GCN 3 micro-architecture and supporting virtual memory. An alternative\
    \ simulator is the Accel-Sim [\\[9\\]](#page-5-10) framework, a cycleaccurate\
    \ state-of-the-art trace-driven simulator supporting CUDA applications trying\
    \ to resemble NVIDIA modern architectures.\n\nThis work focuses on improving the\
    \ modeling of three main components inside the SM and sub-cores in the Accel-Sim\
    \ framework: the front-end, the result and the memory pipeline.\n\nThe rest of\
    \ this paper is organized as follows. In [section 2](#page-1-1) we present the\
    \ different enhancements done to the simulator to have a more realistic model.\
    \ In [section 3,](#page-3-0) we describe the evaluation methodology that is later\
    \ used in [section 4](#page-3-1) to analyze the effects of the proposed modifications.\
    \ Then, we continue discussing in [section 5](#page-4-0) other improvements that\
    \ we plan to investigate in the future. Finally, we conclude in [section 6.](#page-5-11)\n\
    \n#### <span id=\"page-1-1\"></span>2 IMPROVEMENTS TO ACCEL-SIM\n\nThis section\
    \ discusses the SM modeling problems we found in the simulator and how we solved\
    \ them. First, we start talking about the front-end in [subsection 2.1.](#page-1-2)\
    \ Then, we focus on the result bus modeling in [subsection 2.2.](#page-2-0) Finally,\
    \ we improve the memory execution pipeline model in the [subsection 2.3.](#page-2-1)\n\
    \n#### <span id=\"page-1-2\"></span>2.1 Front-end\n\nThe latest architectures\
    \ of NVIDIA have a design of SMs with four sub-cores or processing blocks in NVIDIA\
    \ terminology [\\[13\\]](#page-5-2), [\\[14\\]](#page-5-3), [\\[15\\]](#page-5-4),\
    \ [\\[16\\]](#page-5-5), [\\[17\\]](#page-5-6). Each sub-core has a decode and\
    \ a fetch unit that accesses each sub-core's private L0 instruction cache. In\
    \ the upper\n\nmemory level, there is a L1 instruction cache shared by the four\
    \ sub-cores. As is reported by Zhe Jia et al. [\\[8\\]](#page-5-12) [\\[7\\]](#page-5-13)\
    \ and Barnes et al. [\\[3\\]](#page-5-14), warps are distributed between sub-cores\
    \ with the following formula: sub-core\\_id = warp\\_id%4.\n\nIn [Figure 2,](#page-1-3)\
    \ we can see a block diagram of what is modeled in Accel-Sim. As it can be seen,\
    \ there are no private L0 instruction caches per subcore, and the split among\
    \ sub-cores starts after the decode stage, meaning there is only one fetch and\
    \ decode unit for the whole SM instead of four. In spite of that, the pipeline\
    \ can be kept fully occupied because the fetch and decode stages are done four\
    \ times per cycle, but this model does not reflect modern architectures, and it\
    \ requires a much more costly solution, since the shared instruction cache must\
    \ be able to serve four different access per cycle. Furthermore, there are cases\
    \ where the fetching and decoding of the instruction is happening in just one\
    \ cycle, which is incorrect.\n\n<span id=\"page-1-3\"></span>![](_page_1_Figure_13.jpeg)\n\
    \nFigure 2: Current Accel-Sim front-end.\n\nWe have modified the front-end model\
    \ of Accel-Sim to model the architecture depicted in [Figure 3,](#page-2-2) which\
    \ is based on the sub-core architecture of modern GPUs. As it can be seen, there\
    \ is an L0 instruction cache, a fetch unit and a decode unit private to each subcore.\
    \ Besides, there is a round-robin priority arbiter for handling the requests from\
    \ different sub-cores to the L1 instruction cache.\n\nAnother improvement over\
    \ current Accel-Sim is regarding the way instructions are packed and stored in\
    \ the intruction cache. The current simulator fetches two instructions per warp\
    \ four times per cycle. The simulator is trace-driven and it packs together instructions\
    \ that may belong to different cache lines and instructions that do not have consecutive\
    \ addresses because branches (e.g., instruction at PC 0x100 is packed with instruction\
    \ at PC 0x460). Furthermore, instructions belonging to different kernel codes\
    \ have the same address, which makes the instruction cache to wrongly compute\
    \ many accesses as hits when they should be misses. For instance, the first two\
    \ instructions of each kernel are assumed to have the same memory addresses whereas\
    \ in reality they are different. In our model, we have fixed this issues, by mapping\
    \ instructions to cache lines in a correct manner, and giving different addresses\
    \ to instructions of different kernels.\n\nIn short, we have developed a more\
    \ accurate/realistic model of the front-end by adding private L0 instructions\
    \ caches, fetch and decode units per sub-cores, and have implemented a correct\
    \ and realistic mapping of instructions to cache lines.\n\nAnalyzing and Improving\
    \ Hardware Modeling of Accel-Sim CAMS 2023, October 28, 2023, Toronto, Canada\n\
    \n<span id=\"page-2-2\"></span>![](_page_2_Figure_1.jpeg)\n\nFigure 3: Proposed\
    \ front-end.\n\n### <span id=\"page-2-0\"></span>2.2 Result bus\n\nDuring the\
    \ dispatch of instructions with fixed execution latencies to the different execution\
    \ pipelines, the availability of result buses to write-back the register file\
    \ is checked. The result bus modeling in the original simulator is just searching\
    \ for a free result bus with the instruction latency after being dispatched to\
    \ the execution unit. However, it does not consider conflicts of register file\
    \ banks at write-backs of fixed latency operations. So, we have added support\
    \ to detect these conflicts and only allow the same number of instructions that\
    \ go to the same register file bank finishing in a given cycle as the number of\
    \ register bank ports is in the architecture.\n\nIt is remarkable that during\
    \ this modeling of the result bus, we assume two register file ports as the latest\
    \ architectures of NVIDIA are designed with two register file ports per register\
    \ file bank as discussed by Zhe Jia et al. [\\[8\\]](#page-5-12) [\\[7\\]](#page-5-13)\
    \ instead of only one. Moreover, each port can be used for a write-back or a read.\
    \ So, up to two instructions with the same destination register file bank being\
    \ executed in different execution units can be scheduled to finish in the same\
    \ cycle.\n\n#### <span id=\"page-2-1\"></span>2.3 Memory execution pipeline\n\n\
    Modern GPUs based on sub-core partitioning have a memory pipeline in each sub-core,\
    \ which accesses shared memory structures for the whole SM (L1 data cache, shared\
    \ memory, texture cache, constant\n\nmemory) [\\[13\\]](#page-5-2), [\\[14\\]](#page-5-3),\
    \ [\\[15\\]](#page-5-4), [\\[16\\]](#page-5-5), [\\[17\\]](#page-5-6). Even though\
    \ the behavior is unrevealed, it has the sense that each of these memory pipelines\
    \ is in charge of calculating the memory addresses and coalescing before sending\
    \ the requests to the memory structures.\n\nThe Accel-Sim choice is to have a\
    \ single memory pipeline for the whole SM instead of a dedicated unit per sub-core.\
    \ This creates a problem because there is a single dispatch latch for the whole\
    \ SM instead of one per sub-core. Instructions are maintained in this latch until\
    \ all the requests have been sent to the desired memory structure. Satisfying\
    \ all the requests of an instruction can lead to many cycle stalls depending on\
    \ the degree of achieved coalescing and bank conflicts of the instructions requests.\
    \ It prevents other memory instructions from progressing and stops other instructions\
    \ from being issued (even from different sub-cores). As memory instructions are\
    \ not dispatched, they are held in operand collector units (a limited resource\
    \ inside the sub-core), so any instruction is prevented from being issued because\
    \ there is no space in the operand collection stage. Moreover, address calculation,\
    \ coalescing, and request selection are designed to be done in the same cycle,\
    \ greatly increasing hardware requirements. As it is constrained to do all these\
    \ tasks in a single cycle, the more hardware-hungry part is the coalescing. The\
    \ reason is that each thread needs to know which threads have the same address,\
    \ so the total number of address comparators is 32,<sup>2</sup> = 496, which is\
    \ huge. Furthermore, there is a single write-back latch for the whole SM, which\
    \ may create some contention in case there are different memory structures with\
    \ ready accesses simultaneously for different sub-cores. This became worse if\
    \ the instruction in the write-back latch could not progress because register\
    \ bank ports were already occupied. In [Figure 4,](#page-2-3) we can see a graphical\
    \ representation of the baseline simulator model for the memory pipeline.\n\n\
    <span id=\"page-2-3\"></span>![](_page_2_Figure_11.jpeg)\n\nFigure 4: Memory execution\
    \ pipeline of Accel-Sim.\n\nRegarding our model, we have incorporated a Memory\
    \ pipeline unit in each sub-core that accesses the memory structures shared for\
    \ the whole SM. The details of this unit of commercial GPUs are unknown; therefore,\
    \ we have addressed an aggressive performance design that is reasonable, which\
    \ might differ from commercial designs. We have modeled a pipeline inside this\
    \ unit that splits the memory address calculation and coalescing in different\
    \ cycles. An instruction is maintained in the address latch until all the memory\
    \ addresses have been processed for being coalesced, but the instruction just\
    \ remains in the dispatch latch one cycle unless the address latch is not empty.\
    \ The number of cycles needed for coalescing will depend on the requests required\
    \ for each warp. For example, if all the accesses go to the same cache block,\
    \ it will need just a single cycle. Concerning the coalescing hardware, as it\
    \ dedicates one cycle for each thread, the number of address comparators is just\
    \ 32. Once an address has been processed, it is stored in the request buffer.\
    \ All the memory requests stored in that buffer are sent to the round-robin priority\
    \ arbiter between sub-cores. This means that if there is a memory structure or\
    \ a bank from L1D unused, it will be used for that request, even if that is not\
    \ the first one generated by the coalescing unit. This differs from the current\
    \ model from Accel-Sim, which only allows requests to progress in order of generation.\
    \ Finally, we have a write-back latch for each sub-core with an arbiter to prioritize\
    \ between different memory structures. Moreover, we include a write-back arbiter\
    \ to give priority between sub-cores for popping accesses from the different memory\
    \ structures. A depiction of this model can be found in [Figure 5.](#page-3-2)\n\
    \n<span id=\"page-3-2\"></span>![](_page_3_Figure_2.jpeg)\n\nFigure 5: Proposed\
    \ memory execution pipeline.\n\nTable 1: GPU specification\n\n<span id=\"page-3-3\"\
    ></span>\n\n| Parameter                              | Value    |\n|----------------------------------------|----------|\n\
    | Clock                                  | 1605 \U0001D440\U0001D43B\U0001D467\
    \ |\n| SP/INT/SFU/Tensor Units per sub-core   | 1/1/1/1  |\n| Warps per SM   \
    \                        | 32       |\n| Warp Width                          \
    \   | 32       |\n| Number of registers per SM             | 65536    |\n| Issue\
    \ Scheduler policy                 | GTO      |\n| Number of SMs             \
    \             | 40       |\n| Sub-cores per SM                       | 4     \
    \   |\n| Number of Collector Units per sub-core | 2        |\n| L1 instruction\
    \ cache size              | 32 KB    |\n| L1 data cache size                 \
    \    | 32 KB    |\n| Shared memory size                     | 64 KB    |\n| L2\
    \ cache size                          | 4 MB     |\n| Memory Partitions      \
    \                | 16       |\n| Fixes                                  |    \
    \      |\n| L0 instruction cache size              | 16 KB    |\n| Max. Num. requests\
    \ and replies of L0I  | 1        |\n| Register file ports per bank           |\
    \ 2        |\n\n#### <span id=\"page-3-0\"></span>3 EVALUATION METHODOLOGY\n\n\
    To evaluate how these improvements in the SM have affected the simulation, we\
    \ have measured the speed-up and the Absolute Variation in Cycles (AVC) over the\
    \ baseline simulator. We have used 42 benchmarks belonging to Rodinia 3.1 [\\\
    [6\\]](#page-5-15), Deepbench[\\[11\\]](#page-5-16), Parboil [\\[19\\]](#page-5-17),\
    \ Pannotia [\\[5\\]](#page-5-18), and ISPASS-2009 [\\[2\\]](#page-5-19) suites\
    \ simulated under completion.\n\nWe have configured the Accel-Sim [\\[9\\]](#page-5-10)\
    \ simulation infrastructure with the trace execution mode to reassemble an NVIDIA\
    \ RTX 2070 Super. The main configuration parameters in [Table 1.](#page-3-3) Moreover,\
    \ we have extended the simulator to include the different fixes explained in [section\
    \ 2.](#page-1-1)\n\n#### <span id=\"page-3-1\"></span>4 RESULTS\n\nIn this section,\
    \ we analyze the impact of the different incorporated fixes.\n\nEven these changes\
    \ report a tiny change in performance, just a 0.25% speed-up and a 3.67% of AVC\
    \ on average, there are significant changes if we look in detail at some benchmarks.\
    \ The reason why the average speed-up is smaller than the AVC is because there\
    \ are benchmarks that are gaining performance, and others are losing it. At the\
    \ same time, AVC measures the absolute difference between the original version\
    \ and the proposed one.\n\nIn [Figure 6,](#page-4-1) we can see the impact of\
    \ the different improvements in some significant benchmarks. The all configuration\
    \ may have less impact than separate fixes because they create differences in\
    \ cycles in opposite directions. Nevertheless, some benchmarks such as gemm-train\
    \ are affected in the same direction, so including all the changes produces an\
    \ effect of 12%.\n\nIn this figure, we can see that the result bus modeling is\
    \ the change that is affecting the less.\n\nAnalyzing and Improving Hardware Modeling\
    \ of Accel-Sim CAMS 2023, October 28, 2023, Toronto, Canada\n\n<span id=\"page-4-1\"\
    ></span>![](_page_4_Figure_2.jpeg)\n\nFigure 6: Absolute Variation of Cycles of\
    \ the most significant applications.\n\nHowever, the changes in the memory pipeline\
    \ are more noticeable in more benchmarks, where the most significant case is fw\
    \ reaching a 23% of AVC regarding the memory pipeline and 21% including all the\
    \ changes.\n\nAbout the modifications affecting the front-end, it is affecting\
    \ a lot dwt2d (17%) and lud (12.36%). The main reason behind these variations\
    \ in performance is the different enhancements regarding instruction caches. To\
    \ illustrate it, we can see in [Figure 7](#page-4-2) the miss rate increment factor\
    \ of the first level instruction cache.\n\n<span id=\"page-4-2\"></span>![](_page_4_Figure_6.jpeg)\n\
    \nFigure 7: Miss ratio.\n\nIf we compare gemm-train against dwt2d and lud, it\
    \ requires more than 27.4 and 7.8 cycles respectively to complete the execution.\
    \ This is why even though the miss rate increment factor is more significant in\
    \ gemm-train, it has less effect than in shorter benchmarks.\n\n#### <span id=\"\
    page-4-0\"></span>5 FUTURE WORK\n\nApart from the improvements in modeling that\
    \ we have presented in this paper, others will be necessary to improve the simulator\
    \ and open the door to new research areas in GPUs. We will start discussing what\
    \ can be done inside the GPU cores, and then we will move outside the SM.\n\n\
    #### 5.1 Inside SM\n\nFirst of all, as reported by Mishkin et al. [\\[10\\]](#page-5-20),\
    \ the simulator is not handling WAR dependencies correctly. Even though the issue\
    \ of instructions is in-order, the dispatch from collector units to the execution\
    \ pipelines is out-of-order. It could be the case that two instructions of the\
    \ same warp are in collector units simultaneously, and the younger one with WAR\
    \ hazard is dispatched earlier than the older one. Besides, this problem is not\
    \ frequent, as reported by Mishkin et al. [\\[10\\]](#page-5-20), hardware must\
    \ support these cases not to incur errors. We believe that commercial GPUs are\
    \ not suffering from WAR hazards, but as discussed by Zhe Jia et al. [\\[8\\]](#page-5-12)\
    \ [\\[7\\]](#page-5-13), the management of dependencies of modern GPUs seems that\
    \ is detecting instruction dependencies without scoreboards. However, we lack\
    \ detailed documentation on how the hardware supports it.\n\nRegarding the Operand\
    \ Collection stage and the register file of GPUs, it is known that the latest\
    \ architectures of NVIDIA are designed with two register file ports per register\
    \ file bank as discussed by Zhe Jia et al. [\\[8\\]](#page-5-12) [\\[7\\]](#page-5-13)\
    \ instead of only one. In Accel-Sim, they have modeled this feature focusing on\
    \ the throughput of reading operands. However, the way that is handled is by repeating\
    \ twice the Operand Collection stage per cycle. This means that the allocation\
    \ of collector units, dispatch of ready collector units, and arbitration of reads\
    \ are repeated twice per cycle. Moreover, it is assumed that each collector unit\
    \ has unlimited ports for reading the operands, which would mean having a huge\
    \ crossbar to support the worst case. It was one in previous architectures, but\
    \ this parameter is unknown in the current ones.\n\nAdditionally, the register\
    \ file caching system for instruction operands is not present. Instead, the number\
    \ of register banks is increased, which may be effective for having an equivalent\
    \ contention to commercial hardware. Due to this lack of modeling, it is difficult\
    \ to know if novel proposals such as Malekeh [\\[1\\]](#page-5-21) are beating\
    \ industry designs.\n\nFurthermore, the tracking usage of registers across the\
    \ execution is uncompleted. The tracer tool only captures the usage of regular\
    \ registers, but registers such as predication, uniform, and uniform predicate\
    \ are not detected. Furthermore, some instructions, such as tensor ones, use two\
    \ registers for some operands even though only one of them is captured or indicated\
    \ in the binary, so investigating this conduct will grant the community more opportunities\
    \ to investigate problems in operand collection.\n\nFinally, as the state-of-the-art\
    \ mode of the simulator is tracedriven, it focuses on capturing executed instructions.\
    \ In addition, control flow instructions like BMOV use special registers that\
    \ can have dependencies with general purpose registers. Therefore, the current\
    \ model is not enough to correctly analyze many applications' control flow behavior.\
    \ This complicates analyzing and creating new micro-architectural and compiler\
    \ proposals regarding this topic.\n\n#### 5.2 Ouside SM\n\nOutside of the SM,\
    \ other topics can be improved in this simulation tool. First of all, the NOC\
    \ between SMs and memory partitions is plain in the simulator without taking into\
    \ account the hierarchy of TPCs and GPCs, which seems to be the approach followed\
    \ by the industry, as can be seen in this patent [\\[12\\]](#page-5-22). This\
    \ hierarchy modeling will allow us to analyze new opportunities, including the\
    \ <span id=\"page-5-0\"></span>new Hopper architecture features such as thread-block\
    \ cluster and distributed shared memory between SM inside a GPC [\\[17\\]](#page-5-6).\
    \ Finally, adding virtual memory and multi-GPU support will be great for analyzing\
    \ new kinds of trendy workloads.\n\n#### <span id=\"page-5-11\"></span>6 CONCLUSION\n\
    \nIn this paper, we propose different improvements to the Accel-Sim framework\
    \ with the purpose of having a more real simulator. We have focused on the front-end\
    \ modeling (supporting sub-core split and better modeling of instruction caches).\
    \ Also, we have a better design of the result bus. Then, we explored how the baseline\
    \ simulator represents the memory pipeline and how we think it should be modeled\
    \ to have a more feasible approach. Finally, we have compared all these modifications\
    \ against the baseline simulator to show how they affect the performance of benchmarks.\n\
    \n#### ACKNOWLEDGMENTS\n\nThis work has been supported by the CoCoUnit ERC Advanced\
    \ Grant of the EU's Horizon 2020 program (grant No 833057), the Spanish State\
    \ Research Agency (MCIN/AEI) under grant PID2020- 113172RB-I00, and the ICREA\
    \ Academia program.\n\n#### REFERENCES\n\n- <span id=\"page-5-21\"></span>[1]\
    \ Mojtaba Abaie Shoushtary, Jose Maria Arnau, Jordi Tubella Murgadas, and Antonio\
    \ Gonzalez. 2023. Lightweight Register File Caching in Collector Units for GPUs.\
    \ Proceedings of the 15th Workshop on General Purpose Processing Using GPU (feb\
    \ 2023), 27\u201333.<https://doi.org/10.1145/3589236.3589245>\n- <span id=\"page-5-19\"\
    ></span>[2] Ali Bakhoda, George L. Yuan, Wilson W. L. Fung, Henry Wong, and Tor\
    \ M. Aamodt. 2009. Analyzing CUDA workloads using a detailed GPU simulator. In\
    \ 2009 IEEE International Symposium on Performance Analysis of Systems and Software.\
    \ 163\u2013174.<https://doi.org/10.1109/ISPASS.2009.4919648>\n- <span id=\"page-5-14\"\
    ></span>[3] Aaron Barnes, Fangjia Shen, and Timothy G. Rogers. 2023. Mitigating\
    \ GPU Core Partitioning Performance Effects. In 2023 IEEE International Symposium\
    \ on High-Performance Computer Architecture (HPCA). 530\u2013542. [https://doi.org/10.](https://doi.org/10.1109/HPCA56546.2023.10070957)\
    \ [1109/HPCA56546.2023.10070957](https://doi.org/10.1109/HPCA56546.2023.10070957)\n\
    - <span id=\"page-5-1\"></span>[4] Martin Burtscher, Rupesh Nasre, and Keshav\
    \ Pingali. 2012. A quantitative study of irregular programs on GPUs. In Proceedings\
    \ - 2012 IEEE International Symposium on Workload Characterization, IISWC 2012.\
    \ 141\u2013151. [https://doi.org/10.1109/](https://doi.org/10.1109/IISWC.2012.6402918)\
    \ [IISWC.2012.6402918](https://doi.org/10.1109/IISWC.2012.6402918)\n- <span id=\"\
    page-5-18\"></span>[5] Shuai Che, Bradford M. Beckmann, Steven K. Reinhardt, and\
    \ Kevin Skadron. 2013. Pannotia: Understanding irregular GPGPU graph applications.\
    \ In 2013 IEEE International Symposium on Workload Characterization (IISWC). 185\u2013\
    195. <https://doi.org/10.1109/IISWC.2013.6704684>\n- <span id=\"page-5-15\"></span>[6]\
    \ Shuai Che, Michael Boyer, Jiayuan Meng, David Tarjan, Jeremy W. Sheaffer, Sang\
    \ Ha Lee, and Kevin Skadron. 2009. Rodinia: A benchmark suite for heterogeneous\
    \ computing. In Proceedings of the 2009 IEEE International Symposium on Workload\
    \ Characterization, IISWC 2009. 44\u201354. [https://doi.org/10.1109/IISWC.](https://doi.org/10.1109/IISWC.2009.5306797)\
    \ [2009.5306797](https://doi.org/10.1109/IISWC.2009.5306797)\n- <span id=\"page-5-13\"\
    ></span>[7] Zhe Jia, Marco Maggioni, Jeffrey Smith, and Daniele Paolo Scarpazza.\
    \ 2019. Dissecting the NVidia Turing T4 GPU via Microbenchmarking Technical Report.\
    \ (2019).\n- <span id=\"page-5-12\"></span>[8] Zhe Jia, Marco Maggioni, Benjamin\
    \ Staiger, and Daniele Paolo Scarpazza. 2018. Dissecting the NVIDIA Volta GPU\
    \ Architecture via Microbenchmarking. CoRR abs/1804.06826 (2018). arXiv[:1804.06826](https://arxiv.org/abs/1804.06826)<http://arxiv.org/abs/1804.06826>\n\
    - <span id=\"page-5-10\"></span>[9] Mahmoud Khairy, Zhesheng Shen, Tor M. Aamodt,\
    \ and Timothy G. Rogers. 2020. Accel-Sim: An Extensible Simulation Framework for\
    \ Validated GPU Modeling. In 2020 ACM/IEEE 47th Annual International Symposium\
    \ on Computer Architecture (ISCA). 473\u2013486.<https://doi.org/10.1109/ISCA45697.2020.00047>\n\
    - <span id=\"page-5-20\"></span>[10] Michael Mishkin. 2016. Write-after-Read Hazard\
    \ Prevention in GPGPUsim. (2016).\n- <span id=\"page-5-16\"></span>[11] S. Narang\
    \ and G. Diamos. 2016. GitHub - baidu-research/DeepBench: Benchmarking Deep Learning\
    \ operations on different hardware. [https://github.com/baidu](https://github.com/baidu-research/DeepBench)[research/DeepBench](https://github.com/baidu-research/DeepBench)\n\
    - <span id=\"page-5-22\"></span><span id=\"page-5-2\"></span>[12] NVIDIA. 2010.\
    \ Consolidated crossbar that supports a multitude of traffic types. [13] NVIDIA.\
    \ 2017. NVIDIA Tesla V100 GPU architecture the world's most advanced data center\
    \ GPU. Technical Report. NVIDIA.\n- <span id=\"page-5-3\"></span>[14] NVIDIA.\
    \ 2018. NVIDIA TURING GPU architecture Graphics Reinvented NVIDIA Turing GPU Architecture.\
    \ Technical Report.\n- <span id=\"page-5-4\"></span>[15] NVIDIA. 2020. NVIDIA\
    \ AMPERE GA102 GPU architecture Second-Generation RTX NVIDIA Ampere GA102 GPU\
    \ Architecture. Technical Report.\n- <span id=\"page-5-5\"></span>[16] NVIDIA.\
    \ 2022. NVIDIA ADA GPU architecture. Technical Report.\n- <span id=\"page-5-6\"\
    ></span>[17] NVIDIA. 2022. NVIDIA H100 Tensor Core GPU Architecture. Technical\
    \ Report.\n- <span id=\"page-5-7\"></span>[18] Timothy G. Rogers, Mike Oconnor,\
    \ and Tor M. Aamodt. 2012. Cache-conscious wavefront scheduling. In Proceedings\
    \ - 2012 IEEE/ACM 45th International Symposium on Microarchitecture, MICRO 2012.\
    \ IEEE Computer Society, 72\u201383. [https:](https://doi.org/10.1109/MICRO.2012.16)\
    \ [//doi.org/10.1109/MICRO.2012.16](https://doi.org/10.1109/MICRO.2012.16)\n-\
    \ <span id=\"page-5-17\"></span>[19] J.A. Stratton, C. Rodrigues, I.J. Sung, N.\
    \ Obeid, L.W. Chang, N. Anssari, G.D. Liu, and W.W. Hwu. 2012. Parboil: A Revised\
    \ Benchmark Suite for Scientific and Commercial Throughput Computing. Center for\
    \ Reliable and High-Performance Computing (2012).\n- <span id=\"page-5-9\"></span>[20]\
    \ Yifan Sun, Trinayan Baruah, Saiful A. Mojumder, Shi Dong, Xiang Gong, Shane\
    \ Treadway, Yuhui Bao, Spencer Hance, Carter McCardwell, Vincent Zhao, Harrison\
    \ Barclay, Amir Kavyan Ziabari, Zhongliang Chen, Rafael Ubal, Jos\xE9 L. Abell\xE1\
    n, John Kim, Ajay Joshi, and David Kaeli. 2019. MGPUSim: Enabling Multi-GPU Performance\
    \ Modeling and Optimization. In 2019 ACM/IEEE 46th Annual International Symposium\
    \ on Computer Architecture (ISCA). 197\u2013209.\n- <span id=\"page-5-8\"></span>[21]\
    \ Oreste Villa, Daniel Lustig, Zi Yan, Evgeny Bolotin, Yaosheng Fu, Niladrish\
    \ Chatterjee, Nan Jiang, and David Nellans. 2021. Need for Speed: Experiences\
    \ Building a Trustworthy System-Level GPU Simulator. In 2021 IEEE International\
    \ Symposium on High-Performance Computer Architecture (HPCA). 868\u2013880. [https:](https://doi.org/10.1109/HPCA51647.2021.00077)\
    \ [//doi.org/10.1109/HPCA51647.2021.00077](https://doi.org/10.1109/HPCA51647.2021.00077)"
  paper_id: 23
  scores:
    originality:
      raw: 4.5
      reason: The proposed WideSA scheme presents a novel approach to mapping uniform
        recurrences on the Versal ACAP architecture, contributing significantly to
        the field of FPGA programming.
    technical_soundness:
      raw: 4.0
      reason: The methodology appears robust, but further details on implementation
        and performance metrics would strengthen the technical foundation.
    clarity:
      raw: 4.0
      reason: The writing is generally clear, though some sections could benefit from
        more concise explanations of complex concepts.
    significance:
      raw: 4.5
      reason: The work addresses a critical gap in high utilization of AI Engines,
        making it highly significant for both academia and industry.
    relevance:
      raw: 5.0
      reason: The paper is highly relevant to current trends in FPGA architecture
        and AI applications, aligning well with ongoing research and development efforts.
  total_score: 4.3
  tier: B
- title: "A Scalable RISC-V Vector Processor Enabling Efficient Multi-Precision\n\
    \  DNN Inference"
  abstract: 'In simulation of nuclear reactor physics using the Monte Carlo neutron

    transport method on GPUs, the sorting of particles plays a significant role in

    performance of calculation. Traditionally, CPUs and GPUs are separated devices

    connected at low data transfer rate and high data transfer latency. Emerging

    computing chips tend to integrate CPUs and GPUs. One example is the Apple

    silicon chips with unified memory. Such unified memory chips have opened doors

    for new strategies of collaboration between CPUs and GPUs for Monte Carlo

    neutron transport. Sorting particle on CPU and transport on GPU is an example

    of such new strategy, which has been suffering the high CPU-GPU data transfer

    latency on the traditional devices with separated CPU and GPU. The finding is

    that for the Apple M2 max chip, sorting on CPU leads to better performance per

    power than sorting on GPU for the ExaSMR whole core benchmark problems and the

    HTR-10 high temperature gas reactor fuel pebble problem. The partially sorted

    particle order has been identified to contribute to the higher performance with

    CPU sort than GPU. The in-house code using both CPU and GPU achieves 7.5 times

    power efficiency that of OpenMC on CPU for ExaSMR whole core benchmark with

    depleted fuel, and 150 times for HTR-10 fuel pebble benchmark with depleted

    fuel.'
  url: http://arxiv.org/abs/2401.11455v2
  keywords: sorting, Monte Carlo, neutron transport, GPU, apple, unified memory
  document: "### I. INTRODUCTION\n\nBeing the method with the highest fidelity, the\
    \ Monte Carlo method has been adopted as a verification tool to other methods\
    \ such as discrete ordinates and the method of characteristics. Because of its\
    \ heavy computation burden, the Monte Carlo method has not been considered as\
    \ the everyday reactor simulation tool. The great performance improvement on GPUs\
    \ demonstrated in recent studies makes the adoption of Monte Carlo method as a\
    \ routine practice more practical. Table [I](#page-0-0) summarizes some recent\
    \ work.\n\nAs discovered by Hamilton [\\[1\\]](#page-5-0), particle sorting is\
    \ important for achieving high neutron transport performance by increasing the\
    \ coherence in execution paths between GPU threads. Joo [\\[2\\]](#page-5-1) further\
    \ elaborates the particle sorting strategies. In previous study, most codes such\
    \ as Pragma [\\[2\\]](#page-5-1), Shift [\\[1\\]](#page-5-0) and MagiC [\\[3\\\
    ]](#page-5-2) (possibly) use GPUs for particle sorting, and OpenMC [\\[4\\]](#page-5-3)\
    \ possibly uses CPUs for particle sorting. The Warp [\\[5\\]](#page-5-4) code\
    \ seems sorting particles on CPUs too.\n\n<span id=\"page-0-0\"></span>TABLE I\
    \ SUMMARY OF CONTINUOUS ENERGY MONTE CARLO NEUTRON TRANSPORT CODE WITH GPU SUPPORT\n\
    \n| Code       | Developer                  | Sorting on CPUs or GPUs |\n|------------|----------------------------|-------------------------|\n\
    | Warp [5]   | Univ. California, Berkeley | CPUs                    |\n| Pragma\
    \ [2] | Seoul National Univ.       | GPUs                    |\n| Shift [1]  |\
    \ Oak Ridge National Lab.    | GPUs                    |\n| OpenMC [4] | Argonne\
    \ National Lab.      | CPUs (Possibly)         |\n| MagiC [3]  | Univ. South China\
    \          | GPU (Possibly)          |\n| In-house   | In-house              \
    \     | CPUs and GPUs           |\n|            |                            |\
    \                         |\n\nAs indicated in Figure [1,](#page-0-1) from chips\
    \ for personal entertainment such as Sony Playstation 5 [\\[6\\]](#page-5-5) to\
    \ chips for high performance computation such as AMD [\\[7\\]](#page-5-6) and\
    \ Nvidia [\\[8\\]](#page-5-7) have merged CPU and GPU chips. Some chips such as\
    \ PS5 and MI300A have unified memory with the CPU, GPU and memory connected with\
    \ the high speed bus called infinity fabric.\n\n![](_page_0_Figure_11.jpeg)\n\n\
    <span id=\"page-0-1\"></span>Fig. 1. A snapshot of the design of some recent merged\
    \ CPU and GPU chips\n\nThis work proposes to use Apple unified memory computing\
    \ devices to study the collaboration between CPUs and GPUs in Monte Carlo neutron\
    \ transport methods. This collaboration is previously uneconomic because of the\
    \ low data transfer rate and high data transfer latency between CPUs and GPUs\
    \ on computing devices with separated CPUs and GPUs. There are previous work study\
    \ the Apple silicon for Monte Carlo methods in areas such as: CPU performance\
    \ study [\\[9\\]](#page-5-8), multicore CPU work balance [\\[10\\]](#page-5-9),\
    \ and cross section lookup on GPU [\\[11\\]](#page-5-10).\n\nThe contributions\
    \ are summarized as followed.\n\n\u2022 Discussion about programming for Apple\
    \ M2 Max chip\n\n- Study of the sorting performance on CPU-GPU for partially sorted\
    \ data\n- Verification of in-house merged CPU-GPU code with VERA pincell and assembly\
    \ benchmark problems\n- Comparison of CPU and GPU sorting strategies on the simulation\
    \ power efficiency for ExaSMR whole core and HTR-10 fuel pebble benchmark problems\n\
    \n# II. DEVELOPMENT ON APPLE SILICON AS A UNIFIED MEMORY DEVICE\n\nThe Apple silicon\
    \ chips are system-on-chips (SoCs), where a cluster of more powerful performance\
    \ CPU cores, and a cluster of less powerful efficiency cores, and a cluster of\
    \ GPU cores are integrated on the same silicon die. All CPU and GPU clusters have\
    \ its private L2 cache, and these clusters are sharing an L3 cache named as the\
    \ System Level Cache (SLC).\n\n### *A. Apple M2 Max Chip*\n\nIn this work, the\
    \ Apple M2 Max chip is studied and Figure [2](#page-1-0) gives a snapshot [\\\
    [12\\]](#page-5-11) and an illustration of the chip components. There are four\
    \ memory chips surrounding the SoC in the center. The memory type is LPDDR5, which\
    \ offers an interface of 512 bit with a bandwidth of 400 GB/s. In most gaming\
    \ GPUs, GDDR6 and GDDR6X are the most common types, and in workstation GPUs, HBM2\
    \ and HBM3 are the most common types. Usually, LPDDR5 is used for power efficient\
    \ mobile devices, and LPDDR5 has higher latency. The way of Apple's use of LPDDR5\
    \ is unusual, and it has a tight connection with the SoC. The result of this tight\
    \ packaging is the lower latency than the usual LPDDR5 packaging while keeping\
    \ the power consumption at low level.\n\nThe SoC includes 8 performance CPU cores\
    \ sharing 32 MB L2 cache and 4 efficiency CPU cores sharing 4 MB L2 cache. The\
    \ L2 cache is much larger than Intel, AMD and many ARM based CPUs. There are 38\
    \ GPU cores sharing an unknown size of L2 cache. Moreover, there is a system level\
    \ cache (SLC) of 48 MB for all CPU cores and GPU cores.\n\nWhat makes the Apple\
    \ SoC unique is that the CPU and GPU are sharing the same memories and there is\
    \ a single SLC for both CPU and GPU. Such a design enables closer collaboration\
    \ between CPUs and GPUs. Table [II](#page-1-1) illustrates some of the difference\
    \ between Apple SoC and systems with discrete GPUs. The close connection between\
    \ CPU and GPU in Apple SoC enables collaborated CPU-GPU algorithms with frequent\
    \ communication between CPU and GPU.\n\n<span id=\"page-1-1\"></span>TABLE II\
    \ COMPARISON OF APPLE SOC AND SYSTEMS OF CPU WITH DISCRETE GPU\n\n|          \
    \          | Apple SoC  | Discrete GPU |\n|--------------------|------------|--------------|\n\
    | CPU-GPU bus        | in-silicon | PCI-E        |\n| Memory type        | sharing\
    \    | host/device  |\n| GPU memory latency | low        | high         |\n\n\
    ![](_page_1_Picture_11.jpeg)\n\nFig. 2. A snapshot [\\[12\\]](#page-5-11) (left)\
    \ and a sketch of the design (right) of Apple M2 Max chip. I-Cache stands for\
    \ instruction cache, and D-Cache stands for data cache. Avalanche and Blizzard\
    \ are architecture design code names.\n\n# <span id=\"page-1-0\"></span>*B. Objective-C/Swift\
    \ Programming Languages and Frameworks*\n\nThe operating systems MacOS for laptops\
    \ and workstations, and iPadOS for tablets, and iOS for mobile phones, and watchOS\
    \ for watches, and tvOS for home media stations, and visionOS for the recently\
    \ released space computing goggles are delivered with user interfaces with distinguished\
    \ styles. The basic design of such user interfaces is dated back to the 1980s,\
    \ where C++ has not yet been prevailing. Another object-oriented language Objective-C\
    \ [\\[13\\]](#page-5-12) inspired from the SmallTalk [\\[14\\]](#page-5-13) is\
    \ adopted by Apple to develop the user interfaces.\n\nLater, in the last decade,\
    \ the Swift [\\[15\\]](#page-5-14) language is further proposed for meeting the\
    \ demand of software developers for an easier to use languages. Applications developed\
    \ in Objective-C or Swift are integrated with system frameworks such as Cocoa\
    \ [\\[16\\]](#page-5-15) for user interfaces and Metal [\\[17\\]](#page-5-16)\
    \ for 3D graphics. Figure [3](#page-2-0) illustrates the layers of applications,\
    \ frameworks and OS kernel.\n\nAt the lowest level, Apple computing devices run\
    \ the Darwin OS kernel [\\[18\\]](#page-5-17), which is different from Linux.\
    \ Same as Linux, Darwin implements the Portable Operating Sys-\n\n![](_page_2_Figure_0.jpeg)\n\
    \n<span id=\"page-2-0\"></span>Fig. 3. A sketch of application development in\
    \ Objective-C & Swift programming language on Apple devices.\n\ntem Interface\
    \ (POSIX) [\\[19\\]](#page-5-18). So migration of lower level applications between\
    \ Linux and Darwin is much easier than that between Linux and Windows, where the\
    \ POSIX has not been completely implemented on Windows. As a side notice, Windows\
    \ has provided the Windows Subsystem for Linux (WSL) [\\[20\\]](#page-5-19) to\
    \ provide an embedded Linux environment, in order to execute Linux application\
    \ on Windows.\n\n### *C. Metal Shading Language & Framework*\n\nAt the beginning,\
    \ Apple did not design its own programming languages for GPUs. Instead, OpenGL\
    \ [\\[21\\]](#page-5-20) and OpenCL [\\[22\\]](#page-6-0) are adopted, which are\
    \ open standards conceived by many vendors.\n\nHowever, as the Apple GPUs get\
    \ more powerful, the OpenGL and OpenCL have been not able to keep the pace of\
    \ increased hardware features provided by Apple chips. So, the Metal Shading Language(MSL)\
    \ [\\[17\\]](#page-5-16) has been proposed.\n\nApplications written in MSL rely\
    \ on toolchains provided by the Metal framework for compilation and execution.\
    \ Although both MSL and CUDA [\\[23\\]](#page-6-1) C++ are based on C++, there\
    \ are differences in the code building stages. Figure [4](#page-2-1) illustrates\
    \ the major difference.\n\nIn CUDA, the host code running on CPU and device code\
    \ running on GPU are combined in the same CUDA C++ source code, while in Metal,\
    \ the host code in Objective-C or Swift and device code in Metal are separated.\
    \ Also, in CUDA the CPU and GPU binaries are packed in a single executable, while\
    \ in Metal, the CPU executable will load Metal GPU code in runtime.\n\n![](_page_2_Figure_8.jpeg)\n\
    \n<span id=\"page-2-1\"></span>Fig. 4. A sketch of CPU-GPU program compilation\
    \ scheme on Nvidia and Apple GPU devices.\n\n### *D. Apple GPU Programming Patterns*\n\
    \nProgramming with the Metal framework on Apple GPU begins with the creation of\
    \ command queue. Then, create command buffers to submit tasks to GPUs. Each task\
    \ may contain multiple stages. Each stage creates a command encoder, and each\
    \ GPU kernel function binds to a command encoder. After all commands in the buffer\
    \ are encoded, the buffer is committed, so that the GPU starts to execute the\
    \ commands as soon as possible. Figure [5](#page-2-2) illustrates this programming\
    \ pattern.\n\n![](_page_2_Figure_12.jpeg)\n\n<span id=\"page-2-2\"></span>Fig.\
    \ 5. Programming patterns for Apple GPU.\n\n### III. SORTING ALGORITHMS\n\n<span\
    \ id=\"page-2-4\"></span>In this section, the CPU and GPU sorting algorithms are\
    \ discussed and studied on Apple chips.\n\n### *A. Summary of Sorting Algorithms\
    \ on CPU & GPU*\n\nThere are two sorting codes on CPU, which are the C++ standard\
    \ library (stdlib) utility and Intel TBB [\\[24\\]](#page-6-2) library. The C++\
    \ stdlib adopts the Introsort algorithm and runs on single thread. The average,\
    \ best and worse case time complexity is O(n log n), where n is the number of\
    \ elements to sort. The Intel TBB library adopts the Quicksort algorithm and supports\
    \ multi-thread devices. The Quicksort algorithm has the same complexity as Introsort,\
    \ except that the worse case time complexity is O(n 2 ). As a side notice, Introsort\
    \ is a combination of the three algorthims: Quicksort, Heapsort, and Insertion\
    \ sort.\n\nBecause there are no sorting utilities shipped with the Metal framework,\
    \ an in-house code has been implemented using the Bitonic sorting algorithm. The\
    \ average, best and worse case time complexity is O(n log<sup>2</sup> n). The\
    \ Bitonic algorithm requires the data size to be power of 2. Figure [III](#page-2-3)\
    \ compares the CPU and GPU sorting algorithms.\n\nTABLE III SORTING ALGORITHMS\
    \ ON CPU & GPU\n\n<span id=\"page-2-3\"></span>\n\n| Device | Library        \
    \             | Algorithm | Time complexity |\n|--------|-----------------------------|-----------|-----------------|\n\
    | CPU    | C++ Stdlib (single thread ) | Introsort | O(n log n)      |\n| CPU\
    \    | Intel TBB (multi-thread)    | Quicksort | O(n log n)      |\n| GPU    |\
    \ In-house                    | Bitonic   | O(n log2 n)     |\n\nThe time complexity\
    \ is only a guidance, and the next two subsections propose two experiments to\
    \ illustrate the performance on Apple chips.\n\n### *B. Performance of Sorting\
    \ on Apple Chip*\n\n*1) Random Integers:* The first experiment studies the sorting\
    \ algorithms on an array of integers randomly sampled. If there are n integers,\
    \ then each integer is sampled using a uniform distribution between 0 and n \u2212\
    \ 1. Figure [6](#page-3-0) compares the time cost for sorting integer arrays with\
    \ size from 2 9 to 2 24 .\n\n![](_page_3_Figure_2.jpeg)\n\n<span id=\"page-3-0\"\
    ></span>Fig. 6. Comparison of time cost for sorting integer arrays with size from\
    \ 2 9 to 2 24\n\nOn Log-Log scale, the plot of time cost versus data size appears\
    \ as straight lines. On GPU, this 'straight line' appearance does not extend well\
    \ below 10<sup>5</sup> . This is because of the GPU execution overhead. Notice\
    \ that the time measured is purely the GPU execution cost, not including the GPU\
    \ kernel launch cost.\n\n*2) Partially Sorted Integers:* It worths notice that\
    \ the performance of sorting is limited by memory bandwidth. So, for partial sorted\
    \ data, since there are less data move operations than fully random data, some\
    \ algorithms may perform better.\n\nTo test the performance of sorting of partially\
    \ sorted integers, it begins with an array of fully sorted integers. If the are\
    \ n integers, then the array is 0, 1, 2, . . . n \u2212 1. Next, define a ratio\
    \ of swap r, and randomly swap \u230Anr\u230B pairs of integers in the array,\
    \ with the pair indices randomly sampled. Here, \u230Anr\u230B takes the max integer\
    \ less or equal to nr. Figure [7](#page-3-1) shows the time cost for integer arrays\
    \ of size 2 <sup>23</sup> with ratio of swap r from 10<sup>\u2212</sup><sup>7</sup>\
    \ to 1. When r = 10<sup>\u2212</sup><sup>7</sup> , there are no swaps, so the\
    \ ratio of swap is essentially 0.\n\nWhen the number of swaps varies, the GPU\
    \ Bitonic algorithm performance keeps nearly the same, but the CPU algorithms\
    \ drastically varies. When there are less than 10<sup>\u2212</sup><sup>5</sup>\
    \ of elements are swapped, CPU performs better than GPU.\n\n### *C. Sorting Strategy\
    \ for Monte Carlo Neutron Transport*\n\nThe particle sorting algorithm is important\
    \ for accelerating Monte Carlo neutron transport on GPU. Hamilton [\\[1\\]](#page-5-0),\
    \ Joo [\\[2\\]](#page-5-1), and Tramm [\\[4\\]](#page-5-3) have good summaries,\
    \ and Liu [\\[25\\]](#page-6-3) discusses the sorting algorithms on Apple computing\
    \ devices.\n\n![](_page_3_Figure_10.jpeg)\n\n<span id=\"page-3-1\"></span>Fig.\
    \ 7. Comparison of time cost for sorting integer arrays with size 2 <sup>23</sup>\
    \ and ratio of swaps from 0 to 1.\n\n### IV. REACTOR SIMULATION BENCHMARKS\n\n\
    The previous discussion of sorting algorithm on integer arrays is limited, and\
    \ the results may not reflect the situation of reactor physics simulation. In\
    \ this section, the VERA pincell and assembly problems [\\[26\\]](#page-6-4) are\
    \ simulated to verify the correctness of the program. Then the ExaSMR [\\[27\\\
    ]](#page-6-5) whole core and HTR-10 [\\[28\\]](#page-6-6) fuel pebble benchmark\
    \ problems are simulated to study the performance.\n\n### *A. Simulation Configuration*\n\
    \nThe in-house code on GPU uses 32-bit floating point number since Apple GPUs\
    \ only support 32-bit floating point numbers. Instead, OpenMC uses 64-bit floating\
    \ point numbers.\n\nThe cross sections are prepared in an optimal set of 13 temperatures\
    \ for the kernel reconstruction Doppler broadening method, which is suitable for\
    \ neutron transport in continuously variable media [\\[9\\]](#page-5-8). For OpenMC,\
    \ cross sections at the same set of temperatures are used, and the 2-point interpolation\
    \ Doppler broadening method is used.\n\nThe in-house code tallies flux of a 23-group\
    \ structure and the power. OpenMC code tallies nothing. Table [IV](#page-3-2)\
    \ summarizes the simulation configuration.\n\nTABLE IV SIMULATION CONFIGURATION\
    \ FOR NEUTRON TRANSPORT\n\n<span id=\"page-3-2\"></span>\n\n|                \
    \                                                                     | OpenMC\
    \ Code                                                                       \
    \                                                              |  |\n|-------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------|--|\n\
    |                                                                            \
    \         | 64-bit (double)                                                  \
    \                                                                            \
    \   |  |\n|                                                                  \
    \                   | turn off                                               \
    \                                                                            \
    \             |  |\n|                                                        \
    \                             | turn off                                     \
    \                                                                            \
    \                       |  |\n|                                              \
    \                                       | turn off                           \
    \                                                                            \
    \                                 |  |\n| Cross section temperatures (K)<br>300,\
    \ 304.252, 338.681, 412.408, 530.512, 705.793, |                             \
    \                                                                            \
    \                                        |  |\n| 951.89, 1283.538, 1704.703, 2189.43,\
    \ 2653.095,                                      |                           \
    \                                                                            \
    \                                          |  |\n| 2950, 3000                \
    \                                                          |                 \
    \                                                                            \
    \                                                    |  |\n|                 \
    \                                                                    | 2-point\
    \ linear interpolation                                                       \
    \                                                             |  |\n|        \
    \                                                                            \
    \ | None                                                                     \
    \                                                                       |  |\n\
    |                                                                            \
    \         | ENDF/B-VIII.0                                                    \
    \                                                                            \
    \   |  |\n|                                                                  \
    \                   | In-house Code<br>32-bit (single)<br>turned off<br>turned\
    \ off<br>turned off<br>kernel reconstruction<br>23-group flux and power<br>ENDF/B-VIII.0\
    \ |  |\n\n# *B. Verification: VERA Pincell & Assembly Benchmark Problem*\n\nIn\
    \ order to verify simulation on Apple GPU, the VERA pincell and assembly benchmark\
    \ problems are studied. Table [V](#page-4-0) compares K-effective values between\
    \ in-house code on Apple M2 Max CPU+GPU and OpenMC code on Apple M2 Max CPU. The\
    \ probability table, thermal scattering, and resonance scattering are not considered.\
    \ There are 1,048,576 particles per cycle with 100 inactive cycles and 200 total\
    \ cycles.\n\n<span id=\"page-4-0\"></span>TABLE V K-EFFECTIVE OF VERA PINCELL\
    \ ASSEMBLY BENCHMARK PROBLEMS\n\n|    | In-house     | OpenMC       |    | In-house\
    \    | OpenMC      |\n|----|--------------|--------------|----|-------------|-------------|\n\
    |    | CPU+GPU      | CPU only     |    | CPU+GPU     | CPU only    |\n| 1A |\
    \ 1.18705 (8)  | 1.18805 (8)  | 2E | 1.06910 (7) | 1.06995 (9) |\n| 1B | 1.18190\
    \ (9)  | 1.18290 (10) | 2F | 0.97484 (8) | 0.97557 (8) |\n| 1C | 1.17186 (9) \
    \ | 1.17257 (9)  | 2G | 0.84713 (6) | 0.84804 (9) |\n| 1D | 1.16345 (10) | 1.16405\
    \ (9)  | 2H | 0.78723 (7) | 0.78799 (8) |\n| 1E | 0.77405 (7)  | 0.77529 (6) \
    \ | 2I | 1.18092 (8) | 1.18178 (8) |\n| 2A | 1.18315 (8)  | 1.18391 (8)  | 2J\
    \ | 0.97392 (8) | 0.97481 (8) |\n| 2B | 1.18398 (8)  | 1.18471 (8)  | 2K | 1.02330\
    \ (8) | 1.02385 (8) |\n| 2C | 1.17466 (8)  | 1.17532 (9)  | 2L | 1.02126 (7) |\
    \ 1.02146 (9) |\n| 2D | 1.16689 (8)  | 1.16772 (8)  | 2M | 0.94233 (6) | 0.94209\
    \ (9) |\n\nThe GPU code underestimates the K-effective within 100 pcm, and the\
    \ using of single precision floating point numbers play an important role in this\
    \ discrepancy.\n\n# *C. Performance Study: ExaSMR Whole Core Benchmark Problem*\n\
    \nNext, the influence of the sorting on the performance of whole core nuclear\
    \ reactor simulation has been studied with the ExaSMR benchmark problems. Table\
    \ [VI](#page-4-1) summarizes these problems. There are two versions, one contains\
    \ fresh fuel with only 7 nuclides in fuel, and the other one contains depleted\
    \ fuel with 245 nuclides in fuel.\n\nTABLE VI SUMMARY OF EXASMR WHOLE CORE BENCHMARK\
    \ SIMULATION\n\n<span id=\"page-4-1\"></span>\n\n|                           \
    \        | Fresh fuel  | Depleted fuel                   |\n|-----------------------------------|-------------|---------------------------------|\n\
    | Number of nuclides                | 76          | 283                      \
    \       |\n| Number of nuclides in fuel        | 7           | 245           \
    \                  |\n| Number of cycles                  |             | 350\
    \                             |\n| Number of inactive cycles         |       \
    \      | 100                             |\n| OpenMC particles per cycle     \
    \   |             | 20)<br>1,048,576 (2             |\n| In-house code particles\
    \ per cycle |             | 23)<br>8,388,608 (2             |\n| OpenMC tally\
    \                      |             | None                            |\n| In-house\
    \ code tally               |             | fission power + 23-group fluxes |\n\
    | K-effective OpenMC CPU only       | 1.00656 (6) | 1.00660 (5)              \
    \       |\n| K-effective In-house CPU+GPU      | 1.00587 (2) | 1.00586 (2)   \
    \                  |\n\nThe simulation performance is summarized in Table [VII.](#page-4-2)\
    \ The sorting on CPU performs better than sorting on GPU. This attributes to the\
    \ partially sorted order in the particles as studied in Section [III.](#page-2-4)\
    \ For the fresh fuel problem, the in-house code with GPU transport achieves about\
    \ 3.0 times power efficiency that of OpenMC, and about 7.5 times for the depleted\
    \ fuel problem. The power efficiency has been visualized in Figure [8.](#page-4-3)\n\
    \n<span id=\"page-4-2\"></span>TABLE VII PERFORMANCE OF SORTING FOR EXASMR WHOLE\
    \ CORE BENCHMARK PROBLEMS\n\n| In-house<br>In-house<br>OpenMC                \
    \                 |\n|----------------------------------------------------------------|\n\
    |                                                                |\n| sorting\
    \ on CPU<br>sorting on GPU                               |\n| active cycles<br>active\
    \ cycles<br>active cycles                |\n| (particles/s/Watt)<br>(particles/s/Watt)<br>(particles/s/Watt)\
    \ |\n| Fresh fuel<br>4.5E3<br>3.7E3<br>1.5E3                          |\n| Depleted\
    \ fuel<br>3.0E3<br>2.5E3<br>4.0E2                       |\n\n![](_page_4_Figure_12.jpeg)\n\
    \n<span id=\"page-4-3\"></span>Fig. 8. Comparison of simulation efficiency in\
    \ particle per second per Watt for the ExaSMR whole core benchmark problem.\n\n\
    ### *D. Performance Study: Pebble Fuel from HTR-10 Test Reactor*\n\nIn order to\
    \ verify the influence of sorting algorithms on the performance of simulation\
    \ of emerging high temperature gas reactors, the fuel pebble benchmark problem\
    \ of the HTR-10 test reactor has been studied. High temperature gas reactors have\
    \ distinguished design from the light water reactors, and the previous study of\
    \ the performance with ExaSMR may not apply to HTR-10. The definition of the HTR-10\
    \ pebble benchmark problem and the simulation configuration and calculated K-effective\
    \ is summarized in Table [VIII.](#page-5-21) The code simulation configuration\
    \ follows Table [IV.](#page-3-2) The material for the HTR-10 pebble problem is\
    \ given in Appendix [A.](#page-6-7) As indicated in Table [IV,](#page-3-2) ENDF/B-VIII.0\
    \ is used.\n\nThe simulation performance of both in-house code using CPU and GPU\
    \ sorting and the OpenMC code on CPU is summarized in Table [IX.](#page-5-22)\
    \ The in-house code agrees with OpenMC in terms of K-effective within in 30 pcm\
    \ (consider 3 standard deviations). The K-effective is about 500 pcm lower than\
    \ the reference [\\[29\\]](#page-6-8), which is possibly attributed to the ignorance\
    \ of unresolved resonance (URR).\n\nSame as the ExaSMR whole benchmark, the CPU\
    \ sorting algorithms perform better than GPU sorting on the basis of performance\
    \ per power. And the in-house code is about 270 times more power efficient than\
    \ OpenMC on CPU for pebble with fresh fuel, and 150 times for pebble with depleted\
    \ fuel.\n\nTABLE VIII SUMMARY OF HTR-10 BENCHMARK SIMULATION\n\n<span id=\"page-5-21\"\
    ></span>\n\n|                                   | HTR-10 fuel pebble         \
    \     |                  |\n|-----------------------------------|---------------------------------|------------------|\n\
    |                                   | Fresh fuel                      | Depleted\
    \ fuel    |\n| Pebble/fuel region radius (cm)    | 3.0/2.5                   \
    \      |                  |\n| TRISO particles in fuel region    | 8,335     \
    \                      |                  |\n| TRISO fuel/buffer/PyC1/SiC/PyC2\
    \   | 0.025/0.034/0.038/0.0415/0.0455 |                  |\n| layers outer radius\
    \ (cm)          |                                 |                  |\n| Pebble\
    \ boundary condition         | Reflected                       |             \
    \     |\n| Number of nuclides                | 10                            \
    \  | 248              |\n| Number of nuclides in fuel        | 5             \
    \                  | 243              |\n| Temperature                       |\
    \ 300K                            |                  |\n| OpenMC particles per\
    \ cycle        | 18)<br>262,144 (2               | 16)<br>65,536 (2 |\n| In-house\
    \ code particles per cycle | 1,048,576 (2                    | 20)           \
    \   |\n| OpenMC tally                      | None                            |\
    \                  |\n| In-house code tally               | None             \
    \               |                  |\n| K-effective OpenMC CPU only       | 1.68732\
    \ (9)                     | 1.68714 (21)     |\n| K-effective In-house CPU+GPU\
    \      | 1.68728 (5)                     | 1.68726 (5)      |\n| Reference [29]*\
    \                   | 1.70534 (13)                    | N/A              |\n\n\
    \\* Assume room temperature with unresolved resonance considered\n\n<span id=\"\
    page-5-22\"></span>TABLE IX PERFORMANCE OF SORTING FOR HTR-10 BENCHMARK PROBLEMS\n\
    \n|                                          | In-house<br>sorting<br>on CPU |\
    \ In-house<br>sorting<br>on GPU<br>(particles/s/Watt) | OpenMC |\n|------------------------------------------|-------------------------------|-----------------------------------------------------|--------|\n\
    | HTR-10 fuel pebble<br>with fresh fuel    | 1.7E3                         | 1.3E3\
    \                                               | 6.4    |\n| HTR-10 fuel pebble<br>with\
    \ depleted fuel | 8.6E2                         | 7.7E2                      \
    \                         | 5.8    |\n\n### V. CONCLUSIONS\n\nIn this work, the\
    \ influence of particle sorting algorithms on the VERA pin and assembly, ExaSMR\
    \ whole core, and HTR-10 fuel pebble benchmark problems have been studied with\
    \ the Apple unified memory chips with merged CPU and GPU. First, it has reviewed\
    \ the programming details on Apple silicon chips. Second, it has demonstrated\
    \ that with partially sorted data, sorting on Apple M2 Max CPU can outperform\
    \ GPU. Third, it has verified the correctness of the in-house CPU-GPU code with\
    \ VERA pincell and assembly benchmarks. Fourth, it has given evidence that the\
    \ CPU sort is more efficient in power for the ExaSMR whole core benchmark than\
    \ GPU sort, and the in-house CPU-GPU code achieve 3.0 and 7.5 times power efficiency\
    \ that of OpenMC CPU code for the case of fresh and depleted fuel. And finally,\
    \ it has shown that the CPU sort is more efficient in performance per power than\
    \ GPU sort for the HTR-10 fuel pebble benchmark problem, and the in-house CPU-GPU\
    \ code achieves 270 and 150 times power efficiency that of OpenMC CPU code for\
    \ the case of fresh and depleted fuel. In the future, when unified memory chips\
    \ with merged CPU and GPU are prevailing, CPU and GPU collaboration methods might\
    \ be considered for Monte Carlo reactor neutron transport method with better power\
    \ efficiency.\n\n### ACKNOWLEDGMENT\n\nComputing technologies from New Compute\
    \ Laboratory are used to produce parts of the data in this article. New Compute\
    \ Laboratory & its information providers endeavor to ensure the accuracy & reliability\
    \ of the information provided, but do not guarantee completeness or reliability,\
    \ or that it is up-to-date & accepts no liability (whether in tort or contract\
    \ or otherwise) for any loss or damage, whether direct or indirect, arising from\
    \ errors, inaccuracies or omissions or the information being upto-date. Any information\
    \ provided is at the user's risk.\n\n### REFERENCES\n\n- <span id=\"page-5-0\"\
    ></span>[1] S. Hamilton and T. Evans, \"Continuous-energy Monte Carlo neutron\
    \ transport on GPUs in the Shift code,\" Annuals of Nuclear Energy, vol. 128,\
    \ pp. 236-247, 2019.\n- <span id=\"page-5-1\"></span>[2] N. Choi and H. Joo, \"\
    Domain decomposition for GPU-based continuous energy Monte Carlo power reactor\
    \ calculation,\" Nuclear Engineering and Technology, vol. 52, issue 11, pp. 2667-2677,\
    \ 2020.\n- <span id=\"page-5-2\"></span>[3] K. Gao, Z. Chen, A. Sun and T. Yu,\
    \ \"The research and application of GPU-based Monte Carlo Simulation in reactor\
    \ calculation,\" Proceedings of RPNM2023, Jul. 26-29 Lanzhou China, 2023.\n- <span\
    \ id=\"page-5-3\"></span>[4] J. Tramm, P. Romano, J. Doerfert, A. Lund, P. Shriwise,\
    \ A. Siegel, and et. al., \"Toward Portable GPU Acceleration of the OpenMC Monte\
    \ Carlo Particle Transport Code,\" Proceedings of PHYSOR2022, May 15- 20 Pittsburg\
    \ USA. 2022.\n- <span id=\"page-5-4\"></span>[5] R. Bergmman, J. Vujic, \"Algorithmic\
    \ choices in WARP - A framework \xB4 for continuous energy Monte Carlo neutron\
    \ transport in general 3D geometries on GPUs,\" Annuals of Nuclear Energy, vol.\
    \ 77, pp. 176\u2013 193, 2015.\n- <span id=\"page-5-5\"></span>[6] Sony Playstation\
    \ 5, https://www.playstation.com/en-us/ps5/ (Last retrieved: Jan. 21, 2024)\n\
    - <span id=\"page-5-6\"></span>[7] AMD Instinct\u2122 MI300A Accelerators, https://www.amd.com/en/products/accelerators/instinct/mi300/mi300a.html\
    \ (Last retrieved: Jan. 21, 2024)\n- <span id=\"page-5-7\"></span>[8] NVIDIA Grace\
    \ Hopper Superchip, https://www.nvidia.com/en-us/datacenter/grace-hopper-superchip/\
    \ (Last retrieved: Jan. 21, 2024)\n- <span id=\"page-5-8\"></span>[9] C. Liu,\
    \ \"Doppler broadening using discrete cosine transform and kernel reconstruction\
    \ for spatially variable media,\" Annuals of Nuclear Energy, vol. 174, pp. 109150,\
    \ 2012.\n- <span id=\"page-5-9\"></span>[10] P. Romano, J. Tramm, P. Shriwise,\
    \ \"Impact of Asymmetric Multicore Processors on Monte Carlo Particle Transport\
    \ Code Performance,\" Proceedings of M&C 2023 (394), Aug. 13-17 Niagara Falls\
    \ Canada, 2023.\n- <span id=\"page-5-10\"></span>[11] J. Tramm, K. Yoshii, P.\
    \ Romano, \"Power at Your Fingertips: Assessing the Performance of a Monte Carlo\
    \ Neutron Transport Mini-App on Consumer Laptop GPUs,\" Proceedings of M&C 2023\
    \ (433), Aug. 13-17 Niagara Falls Canada, 2023.\n- <span id=\"page-5-11\"></span>[12]\
    \ Apple unveils M2 Pro and M2 Max: nextgeneration chips for next-level workflows,\
    \ https://www.apple.com/newsroom/images/product/mac/standard/Apple-M2-chips-M2-Max-230117.zip\
    \ (Last retrieved: Jan. 21, 2024)\n- <span id=\"page-5-12\"></span>[13] About\
    \ Objective-C, https://developer.apple.com/library/archive/ documentation/Cocoa/Conceptual/ProgrammingWithObjectiveC/Introduction/\
    \ Introduction.html (Last retrieved: Jan. 21, 2024)\n- <span id=\"page-5-13\"\
    ></span>[14] GNU Smalltalk, https://www.gnu.org/software/smalltalk/ (Last retrieved:\
    \ Jan. 21, 2024)\n- <span id=\"page-5-14\"></span>[15] Swift: the powerful programming\
    \ language that's also easy to learn, https://developer.apple.com/swift/ (Last\
    \ retrieved: Jan. 21, 2024)\n- <span id=\"page-5-15\"></span>[16] What Is Cocoa?,\
    \ https://developer.apple.com/library/archive/ documentation/Cocoa/Conceptual/CocoaFundamentals/WhatIsCocoa/\
    \ WhatIsCocoa.html (Last retrieved: Jan. 21, 2024)\n- <span id=\"page-5-16\"></span>[17]\
    \ Accelerate graphics and much more with Metal, https://developer.apple.com/metal/\
    \ (Last retrieved: Jan. 21, 2024)\n- <span id=\"page-5-17\"></span>[18] Kernel\
    \ Architecture Overview, https://developer.apple.com/library/archive/ documentation/Darwin/Conceptual/KernelProgramming/Architecture/\
    \ Architecture.html (Last retrieved: Jan. 21, 2024)\n- <span id=\"page-5-18\"\
    ></span>[19] POSIX (The Portable Operating System Interface), https://www.gnu.org/software/libc/manual/html\
    \ node/POSIX.html (Last retrieved: Mar. 14, 2024)\n- <span id=\"page-5-19\"></span>[20]\
    \ How to install Linux on Windows with WSL, https://learn.microsoft.com/en-us/windows/wsl/install\
    \ (Last retrieved: Mar. 14, 2024)\n- <span id=\"page-5-20\"></span>[21] OpenGL:\
    \ The Industry's Foundation for High Performance Graphics, https://www.opengl.org\
    \ (Last retrieved: Jan. 21, 2024)\n- <span id=\"page-6-0\"></span>[22] OpenCL:\
    \ Open Standard for Parallel Programming of Heterogeneous Systems, https://www.khronos.org/opencl/\
    \ (Last retrieved: Jan. 21, 2024)\n- <span id=\"page-6-1\"></span>[23] CUDA Toolkit,\
    \ https://developer.nvidia.com/cuda-toolkit (Last retrieved: Jan. 21, 2024)\n\
    - <span id=\"page-6-2\"></span>[24] Intel\xAE oneAPI Threading Building Blocks,\
    \ https://www.intel.com/content/www/us/en/developer/tools/oneapi/onetbb.html (Last\
    \ retrieved: Mar. 14, 2024)\n- <span id=\"page-6-3\"></span>[25] C. Liu, \"Monte\
    \ Carlo neutron transport using low power mobile GPU devices\", Arxiv, https://arxiv.org/abs/2208.06296,\
    \ 2022\n- <span id=\"page-6-4\"></span>[26] B. Godfrey, \"VERA core physics benchmark\
    \ progression problem specifications, revision 4,\" CASL technical report CASL-U-2012-0131-\
    \ 004, 2014.\n- <span id=\"page-6-5\"></span>[27] E. Merzari, S. Hamilton, T.\
    \ Evans, M. Min and et. al., \"Exascale Multiphysics Nuclear Reactor Simulations\
    \ for Advanced Designs,\" Proceedings of SC23, Nov. 12-17 Denver USA, https://doi.org/10.1145/3581784.3627038,\
    \ 2023\n- <span id=\"page-6-6\"></span>[28] International Handbook of Reactor\
    \ Physics Experiments, \"Evaluation of the Initial Critical Configuration of the\
    \ HTR-10 Pebble-Bed Reactor,\" HTR10-GCR-RESR-001, NEA/NSC/DOC(2006)1, Rev. 0.,\
    \ 2006\n- <span id=\"page-6-8\"></span>[29] Y. Cheng, C. Hao and F. Li, \"Uncertainty\
    \ quantification of fuel pebble model and its effect on the uncertainty propagation\
    \ of nuclear data in pebble bed HTR,\" Annuals of Nuclear Energy, vol. 139, pp.\
    \ 107286, 2020.\n\n### <span id=\"page-6-7\"></span>APPENDIX\n\nThe definitions\
    \ of materials used in the HTR-10 pebble benchmark are summarized in Table [X.](#page-6-9)\n\
    \n<span id=\"page-6-9\"></span>\n\n| TABLE X                                 \
    \                   |  |\n|------------------------------------------------------------|--|\n\
    | DEFINITIONS OF MATERIALS IN THE HTR-10 PEBBLE PROBLEM WITH |  |\n| FRESH FUEL\
    \                                                 |  |\n\n| Material         \
    \      | Nuclide | Atomic density (1024cm\u22123<br>) |\n|------------------------|---------|-------------------------------|\n\
    | Pebble Carbon matrix   | B-10    | 2.49298E-8                    |\n|      \
    \                  | B-11    | 1.00345E-7                    |\n|            \
    \            | C-12    | 8.57768E-2                    |\n|                  \
    \      | C-13    | 9.60880E-4                    |\n| Fuel kernel            |\
    \ B-10    | 4.06384E-7                    |\n|                        | B-11 \
    \   | 1.63575E-6                    |\n|                        | O-16    | 4.64720E-2\
    \                    |\n|                        | U-235   | 3.99198E-3      \
    \              |\n|                        | U-238   | 1.92441E-2            \
    \        |\n| Buffer                 | B-10    | 1.58513E-8                  \
    \  |\n|                        | B-11    | 6.38035E-8                    |\n|\
    \                        | C-12    | 5.45401E-2                    |\n|      \
    \                  | C-13    | 6.10964E-4                    |\n| Pyrolytic Carbon\
    \ (PyC) | B-10    | 2.73795E-8                    |\n| inner and outer       \
    \ | B-11    | 1.10206E-7                    |\n|                        | C-12\
    \    | 9.42057E-2                    |\n|                        | C-13    | 1.05530E-3\
    \                    |\n| Silicon Carbide (SiC)  | C-12    | 4.72306E-2      \
    \              |\n|                        | C-13    | 5.29082E-4            \
    \        |\n|                        | Si-28   | 4.40486E-2                  \
    \  |\n|                        | Si-29   | 2.23666E-3                    |\n|\
    \                        | Si-30   | 1.47442E-3                    |"
  paper_id: 24
  scores:
    originality:
      raw: 4.5
      reason: The paper presents a novel approach to enhancing RISC-V vector processors
        for multi-precision DNN inference, which is a significant advancement in the
        field.
    technical_soundness:
      raw: 4.0
      reason: The proposed SPEED architecture is well-grounded in existing technology,
        though some aspects could benefit from further empirical validation.
    clarity:
      raw: 4.0
      reason: The writing is generally clear and structured, but some technical details
        could be elaborated for better understanding.
    significance:
      raw: 4.5
      reason: The improvements in throughput and efficiency for DNN inference are
        highly relevant to current trends in AI and hardware design, marking a strong
        contribution.
    relevance:
      raw: 5.0
      reason: The topic is highly relevant to both the fields of computer architecture
        and machine learning, addressing pressing challenges in DNN deployment.
  total_score: 4.3
  tier: B
- title: "SAL-PIM: A Subarray-level Processing-in-Memory Architecture with\n  LUT-based\
    \ Linear Interpolation for Transformer-based Text Generation"
  abstract: 'Memory bandwidth is known to be a performance bottleneck for FPGA

    accelerators, especially when they deal with large multi-dimensional data-sets.

    A large body of work focuses on reducing of off-chip transfers, but few authors

    try to improve the efficiency of transfers. This paper addresses the later

    issue by proposing (i) a compiler-based approach to accelerator''s data layout

    to maximize contiguous access to off-chip memory, and (ii) data packing and

    runtime compression techniques that take advantage of this layout to further

    improve memory performance. We show that our approach can decrease the I/O

    cycles up to $7\times$ compared to un-optimized memory accesses.'
  url: http://arxiv.org/abs/2401.12071v1
  keywords: ''
  document: "Corentin Ferry Univ Rennes, CNRS, Inria, IRISA Rennes, France cferry@mail.colostate.edu\n\
    \nSteven Derrien Univ Rennes, CNRS, Inria, IRISA Rennes, France Steven.Derrien@irisa.fr\n\
    \n#### ABSTRACT\n\nMemory bandwidth is known to be a performance bottleneck for\
    \ FPGA accelerators, especially when they deal with large multidimensional data-sets.\
    \ A large body of work focuses on reducing of off-chip transfers, but few authors\
    \ try to improve the efficiency of transfers. This paper addresses the later issue\
    \ by proposing (i) a compiler-based approach to accelerator's data layout to maximize\
    \ contiguous access to off-chip memory, and (ii) data packing and runtime compression\
    \ techniques that take advantage of this layout to further improve memory performance.\
    \ We show that our approach can decrease the I/O cycles up to 7\xD7 compared to\
    \ un-optimized memory accesses.\n\n#### KEYWORDS\n\nmemory access, redundancy,\
    \ data packing, padding, arbitrary precision, memory allocation\n\n#### 1 INTRODUCTION\n\
    \nFPGA accelerators have gained significant popularity in recent years, despite\
    \ their inherent programming complexity. High-Level Synthesis tools play a pivotal\
    \ role in reducing the design challenges associated with FPGA acceleration, and\
    \ facilitate their adoption in new application domains (e.g. machine learning).\n\
    \nFPGA accelerator boards offer massive computational capabilities, but their\
    \ performance is often hindered by an under-performing memory system, which becomes\
    \ a performance bottleneck [\\[16\\]](#page-10-0). This is especially true for\
    \ accelerators that target compute intensive kernels operating on large data-sets.\
    \ This issue is generally addressed through program transformations that increase\
    \ temporal reuse, trading off-chip memory transfers for on-chip storage resource.\
    \ However, this approach does not primarily seek to optimize bandwidth usage (i.e.\
    \ total amount of data transferred), leaving room for further improvement.\n\n\
    Another approach consists in improving the effectiveness of the memory subsystem\
    \ by reorganizing access patterns and data layout in order to exploit FPGA-specific\
    \ constraints[\\[17\\]](#page-10-1). One classical way of doing so is by exploiting\
    \ large burst-based transfers, which requires contiguous data in memory. This\
    \ is however not easy when dealing with the multi-dimensional data-sets found\
    \ in\n\nNicolas Derumigny Colorado State University, Univ. Grenoble Alpes, Inria,\
    \ CNRS, Grenoble INP, LIG 38000 Grenoble, France nicolas.derumigny@inria.fr\n\n\
    Sanjay Rajopadhye Colorado State University Fort Collins, CO, USA First.Last@colostate.edu\n\
    \nmany applications, since the usual row-major and/or column major layouts only\
    \ guarantee contiguity of data in only one dimension.\n\nThe problem becomes even\
    \ more difficult when considering custom data formats (fixed/floating point) whose\
    \ bitwidth do not correspond to the native memory bus interface. In such cases,\
    \ the designer is left with two choices : padding the format to fit the bus width\
    \ or deal with misaligned access, both choices incurring a loss of effective bandwidth.\n\
    \nNevertheless, the ability to design application-specific hardware also brings\
    \ opportunities to improve bandwidth efficiency. For example, the fact that successive\
    \ and/or ajacent values are numerically close (typical case in physical simulation)\
    \ makes runtime compression a viable strategy to increase the effectiveness of\
    \ memory transfers.\n\nIn this paper, we present an automatic HLS optimisation\
    \ flow that combines contiguity, data packing and compression to maximize the\
    \ utilization of bandwidth with custom data types. More precisely, our contributions\
    \ are the following:\n\n- an algorithm to automatically derive (i) burst-friendly\
    \ data layouts, and (ii) accelerator-specific access patterns that maximize contiguity\
    \ while enabling data packing and compression,\n- an automated code generation\
    \ framework implementing the algorithm that generates synthesizable hardware,\n\
    - an evaluation of our approach on FPGA accelerators generated using the code\
    \ generator that shows a up to 7\xD7 decrease in I/O cycles.\n\nThis paper is\
    \ organized as follows: Section [2](#page-0-0) presents the concepts and core\
    \ optimization techniques our work is relying on; Section [3](#page-3-0) describes\
    \ the memory layout transformation, and Section [4](#page-5-0) explains how we\
    \ automatically apply it to FPGA accelerators. Finally, Section [5](#page-7-0)\
    \ validates our approach and discusses it on a series of benchmarks.\n\n#### <span\
    \ id=\"page-0-0\"></span>2 BACKGROUND\n\n#### <span id=\"page-0-1\"></span>2.1\
    \ Locality optimizations\n\nAs manually optimizing an HLS design at the source\
    \ level is a tedious process, automated approaches are now routinely used for\
    \ HLS/FPGA targets in order to exploit parallelism and locality at\n\n<span id=\"\
    page-1-0\"></span>![](_page_1_Figure_1.jpeg)\n\nFigure 1: Domain of the Jacobi\
    \ stencil divided into tiles of size 6 \xD7 6. Each tile contains 18 (,) points\
    \ corresponding to 18 computations of , s.\n\n<span id=\"page-1-3\"></span>![](_page_1_Figure_3.jpeg)\n\
    \nFigure 2: Compiler flow (our contributions in green)\n\nmultiple levels, which\
    \ are often implemented as source-to-source compilers [\\[5,](#page-10-2) [14,](#page-10-3)\
    \ [15,](#page-10-4) [28\\]](#page-10-5). Due to the inherent regularity of HLS-valid\
    \ code, loop transformations engines such as PolyOpt/HLS [\\[21\\]](#page-10-6)\
    \ and POLSCA [\\[29\\]](#page-10-7) excel at this task. However, such transformations\
    \ are calibrated to improve computation time of benchmarks and do not seek to\
    \ change the memory layout to enforce memory access contiguity.\n\nIndeed, to\
    \ get the best runtime performance from an FPGA accelerator, it is necessary to\
    \ limit its off-chip memory accesses as much as possible. Only a fraction of large\
    \ data sets can fit the limited size of on-chip memory; programs operating on\
    \ large data sets must therefore be transformed to work on smaller workloads at\
    \ a time. Loop tiling does this: it breaks large spaces into smaller subproblems\
    \ called tiles, where the on-chip memory and parallelism requirements of each\
    \ tile match those available on the chip.\n\nAn accelerator for a tiled program\
    \ processes the domain tile by tile. To execute a tile, the accelerator needs\
    \ to retrieve intermediate results from previously executed tiles. These intermediate\
    \ results are located outside of the accelerator, in off-chip memory, and need\
    \ to be copied into on-chip memory.\n\nThe amount of on-chip memory needed to\
    \ run a tile is directly influenced by the tile's shape and size. In addition,\
    \ when the tile size increases, the overall off-chip memory access is reduced,\
    \ thus improving the overall arithmetic Intensity. Selection of the best tile\n\
    \n<span id=\"page-1-1\"></span>![](_page_1_Figure_9.jpeg)\n\nFigure 3: Inter-tile\
    \ communication pattern for the Jacobi stencil: red arrows indicate data input\
    \ into the tile shown in the center, and blue arrows indicate data output from\
    \ this tile.\n\nshape and size is outside the scope of this work, and mainly depends\
    \ on the performance / area trade-off desired by the designer.\n\n#### <span id=\"\
    page-1-2\"></span>2.2 Illustrative example: 1D Jacobi stencil\n\nTo illustrate\
    \ the flow proposed in this paper, we propose a Jacobi-1D stencil as running example.\
    \ This kernel updates a one-dimensional sequence of values, and computes each\
    \ point as a weighted average of it and its neighbors:\n\n$$c\\_{t+1,i} = \\frac{1}{3}\
    \ \\left( c\\_{t,i-1} + c\\_{t,i} + c\\_{t,i+1} \\right)^2$$\n\nA C implementation\
    \ of this stencil is provided in the Poly-Bench/C suite as the following code:\n\
    \n```\n1 for (t = 0; t < _PB_TSTEPS ; t ++) {\n2 for (i = 1; i < _PB_N - 1; i\
    \ ++)\n3 B[i] = 0.33 * (A[i -1] + A[i] + A[i +1]) ;\n4 for (i = 1; i < _PB_N -\
    \ 1; i ++)\n5 A[i] = 0.33 * (B[i -1] + B[i] + B[i +1]) ;\n6 }\n```\nThis stencil\
    \ operates over a two-dimensional iteration domain \xD7 where each point has a\
    \ coordinate (,). Because such a domain may be arbitrarily large, the whole dataset\
    \ may not fit into FPGA on-chip memory, and needs to be optimized before it can\
    \ be mapped to the FPGA. As stated in the previous subsection, this naive implementation\
    \ of Jacobi-1D cannot fit on-chip for gigabytescale problem sizes, thus requiring\
    \ tiling. For the sake of simplicity of the illustration, we have chosen small,\
    \ diamond-shaped tiles, illustrated in Figure [1.](#page-1-0)\n\nFor this tiling\
    \ scheme, intermediate results to be retrieved come from the tiles located below\
    \ the tile to execute; in Figure [3,](#page-1-1) these tiles are designated as\
    \ the source of incoming arrows into the tile to execute. Likewise, the outgoing\
    \ arrows show those intermediate results that will be used by other neighboring\
    \ tiles. All of these data\n\n<span id=\"page-2-0\"></span>![](_page_2_Figure_1.jpeg)\n\
    \nFigure 4: Macro-pipeline structure: read-execute-write. Our contribution focuses\
    \ on the read and write stages.\n\ntransfers are the ones this work seeks to optimize;\
    \ improvements of the compute engine fall out of the scope of this paper.\n\n\
    #### 2.3 Deriving parallel accelerators using HLS\n\nThe optimizations mentioned\
    \ section [2.1](#page-0-1) are only a part of all those optimizations that need\
    \ to be applied to get the best performance. One also needs to extract parallelism\
    \ to maximize utilization of operators on the FPGA, and create a macro-pipeline\
    \ to maximize the compute throughput.\n\nLoop tiling naturally yields a \"read-execute-writeback\"\
    \ macropipeline structure as illustrated in Figure [4:](#page-2-0) because tiles\
    \ can be executed atomically, all I/O operations can happen before and after execution.\
    \ HLS tools such as Vitis HLS support such macropipelines through manual code\
    \ annotation, but through a restricted set of conditions of the pipeline (i.e.\
    \ absence of cyclic dependency between stages). Moreover, automated macro-pipelining\
    \ further increase pressure on the on-chip memory usage, as buffers used for inter-stage\
    \ communication are either implemented using FIFOs or duplicated. While standard\
    \ coding techniques would pass the complete tile data buffer across pipeline stages,\
    \ this widely inefficient in hardware as communication only requires a subset\
    \ of the actual tile data. On one tile of our Jacobi-1D example from subsection\
    \ [2.2,](#page-1-2) the required data to be communicated is represented in blue\
    \ in Figure [6.](#page-3-1)\n\nUsually, the goal of the execute stage is to take\
    \ advantage of massive operation-level parallelism (thanks to loop pipelining\
    \ and unrolling), that have already been extensively addressed [\\[4,](#page-10-8)\
    \ [21,](#page-10-6) [22,](#page-10-9) [25\\]](#page-10-10). In this paper, we\
    \ only seek to optimize transfer times and memory bandwidth usage; optimisation\
    \ of the complete design including crafting and balancing of a coarse-grain pipeline\
    \ are not evaluated.\n\nIn applications with low operational intensity, the limited\
    \ offchip memory bandwidth turns the read and write stages into performance bottlenecks,\
    \ even with aggressive tiling transformations. In most cases, this is due to a\
    \ poor utilization of the off-chip memory interface, where only a fraction of\
    \ the peak bandwidth is effectively used due to inefficient access patterns. As\
    \ a matter of fact, approaching the peak memory bandwidth requires that almost\
    \ all access to external memory consist of large transfers over contiguous memory\
    \ locations (called memory burst).\n\nHLS tools can infer burst memory accesses\
    \ depending on the target interface. In the case of a shared bus (e.g. AXI, PCIe),\
    \ which is commonly found for off-chip accesses, a burst access may occur\n\n\
    <span id=\"page-2-1\"></span>![](_page_2_Figure_10.jpeg)\n\nFigure 5: Data packing\
    \ and compression reduce storage and transfer redundancy at the expense of address\
    \ alignment and, for compression, predictability of addresses.\n\nif the bus supports\
    \ it and the compiler recognizes access to a series of consecutive addresses.\
    \ Tools such as Vitis HLS 2022.2 exploit this using with either a call to a HLS-specific\
    \ memcpy routine, or through some form or pattern matching in the source code.\
    \ In burst mode, no cycle is spent stalling for a new value after a one-shot initialization\
    \ latency, which yields full utilization of the available bandwidth.\n\nThe goal\
    \ of this work is to propose a source level compiler optimisation to (i) reorganize\
    \ data in memory to enable contiguous burst access and (ii) further improve bandwidth\
    \ utilization through packing and compression. Our optimization pass is meant\
    \ to be integrated within an HLS polyhedral compilation flow, as illustrated in\
    \ Figure [2;](#page-1-3) aiming at sitting between the locality optimization phase\
    \ (tiling) and the HLS synthesis stage. In fact, our approach does not replace\
    \ locality optimizations, it complements them.\n\n#### 2.4 Padding vs packing\n\
    \nIn order to maximize the utilization of bandwidth, every bit of data transmitted\
    \ must be useful. However, with domain-specific data types (e.g., custom fixed\
    \ point), unused bits must usually be transmitted due to memory alignment requirements.\
    \ In the following, we explain how data contiguity can be leveraged to two ways:\
    \ packing data to reduce the unused bits transmitted; and compressing data to\
    \ further save bandwidth.\n\nMost memories are byte-addressable and most processor\
    \ architectures also require aligned accesses at word boundaries, usually at 32\
    \ or 64 bits. Although FPGA accelerators can operate on arbitraryprecision data\
    \ types, off-chip data transfers must abide by the addressing requirements of\
    \ the external memory. They therefore need to pad the incoming and outgoing data:\
    \ in practice, for a 17-bit access, 32 bits of data will be transferred, 15 bits\
    \ of them being wasted in padding.\n\nNote that padding is necessary to enable\
    \ random accesses to data: it provides the guarantee that a given memory cell\
    \ contains only the requested data and no manipulation needs to be done to extract\
    \ it. Data accessed in a contiguous manner does not need this guarantee and may\
    \ overlap multiple adjacent cells, as simple wire manipulations on the FPGA will\
    \ give back the original data.\n\nData packing, as illustrated in Figure [5,](#page-2-1)\
    \ consists in avoiding padding the data so that words are adjacent at the bit\
    \ level in memory. Figure [5](#page-2-1) shows buffer structure for unpacked and\
    \ packed data of 17 bits in 32-bit words. Unpacked data has aligned addresses,\
    \ but requires extra storage and transfers unused data; packed data has unaligned\
    \ addresses but saves storage and avoids some redundant bits from being transmitted.\
    \ It becomes however impossible to randomly seek in a packed stream due to misalignment\
    \ without additional data processing, but by definition such random seeks do not\
    \ happen with contiguous accesses.\n\nIn our approach, we leverage contiguous\
    \ accesses to (i) avoid the adverse effects of packing induced misalignment and\
    \ (ii) to maximize bandwidth utilization by not padding data.\n\n#### <span id=\"\
    page-3-4\"></span>2.5 Runtime data compression\n\nPacking data saves bandwidth\
    \ by eliminating the padding bits, and is applied independently of the data itself.\
    \ However, further optimisation is possible by exploiting properties of the data\
    \ (e.g., correlation between integers in an array) for compression. When this\
    \ technique is applied right before / after off-chip communications, the design\
    \ benefits from a reduction of I/O cycles (as the amount of data transferred is\
    \ reduced) without increase of the computation subsystem as the latency of the\
    \ compression module can be hidden by the pipelining structure\n\nCompression\
    \ is easy to apply to contiguous streams of data, but is not to data where indexed\
    \ or random accesses are necessary. We must exhibit access contiguity, as it is\
    \ in general impossible to seek within a compressed block without decompressing\
    \ more data than needed. Figure [5](#page-2-1) shows that the position of data\
    \ within a compressed block is unpredictable.\n\nOur approach performs runtime\
    \ compression and, to maximize its efficiency, creates data blocks with a contiguous\
    \ access guarantee to ensure every decompressed piece of data is used.\n\nIn general,\
    \ the compression algorithm is domain-specific, e.g., ADPCM for voice [\\[6\\\
    ]](#page-10-11) or JPEG for images [\\[24\\]](#page-10-12). For FPGA implementations,\
    \ the choice of the algorithm is also driven by its throughput: compression and\
    \ decompression must be able to sustain the input and output throughput not to\
    \ become the bottleneck. We choose to illustrate the idea with a simple differential\
    \ compression algorithm which encodes a sequence 0<sup>1</sup> . . . of -bit words\
    \ as follows:\n\n- Encode <sup>0</sup> as is.\n- For 1 \u2A7D \u2A7D :\n\t- (1)\
    \ Compute \u0394 = \u2212 \u22121,\n\t- (2) Let be the number of leading zeroes\
    \ of \u0394 if \u0394 \u2A7E 0, or leading ones if \u0394 < 0,\n\t- (3) Encode\
    \ <sup>\u2212</sup> using \u230A1 <sup>+</sup> log<sup>2</sup> ()\u230B bits,\
    \ followed by the sign bit of \u0394,\n\t- (4) Encode the \u2212 ( + 1) lowest\
    \ bits of \u0394.\n\nThis technique is especially effective when the distribution\
    \ of the transferred data is not spread, typically on benchmarks based on the\
    \ computation of the average such as our Jacobi-1D example from subsection [2.2.](#page-1-2)\n\
    \n#### <span id=\"page-3-0\"></span>3 MEMORY LAYOUT OPTIMIZATION\n\nThis work\
    \ seeks to minimize the I/O cycles for an accelerator to transmit and retrieve\
    \ its data into and from global memory. Locality\n\n<span id=\"page-3-1\"></span>![](_page_3_Figure_17.jpeg)\n\
    \nFigure 6: MARS: Groups of points within a tile which data is contiguous in global\
    \ memory. In blue, the MARS produced by the center tile (O1 to O4); in red, the\
    \ MARS consumed by that same tile (I1 to I7).\n\noptimizations having already\
    \ been applied, we do not want to store or retrieve fewer values from memory,\
    \ but rather make better accesses to memory, from the bandwidth utilization, and\
    \ therefore I/O cycles, standpoints.\n\nTo this aim, we build a contiguous, irredundant\
    \ and compressed data layout. This section details the steps taken: first, we\
    \ analyze the program and extract sets of on-chip data as contiguous data blocks;\
    \ second, we lay out these data blocks to obtain further contiguous accesses;\
    \ third, we compress and pack the data blocks together to save even more bandwidth.\n\
    \n#### <span id=\"page-3-3\"></span>3.1 Extracting Contiguous Data Blocks\n\n\
    The first step in our method consists in analyzing a program's behavior with respect\
    \ to memory, to determine which data can/should be grouped together as contiguous\
    \ blocks. The sought groups of data honor two properties:\n\n- Atomicity: If any\
    \ data in the group is needed for an instance of the accelerator's execution flow\
    \ (a tile), then the entire group is also needed for the same tile.\n- Irredundancy:\
    \ No data is retrieved or stored more than once into memory throughout the execution\
    \ of a single tile.\n\nThese groups of data are determined by using the analysis\
    \ technique from Ferry et al. [\\[9\\]](#page-10-13) within a polyhedral compiler.\
    \ This analysis yields sets of on-chip memory addresses, such that all the data\
    \ from these on-chip cells will be allocated a contiguous block of data in off-chip\
    \ memory.\n\n<span id=\"page-3-2\"></span>Example. Applying the MARS analysis\
    \ from [\\[9\\]](#page-10-13) to the Jacobi stencil of Section [2.2](#page-1-2)\
    \ gives the sets of addresses corresponding to the points illustrated in Figure\
    \ [6:](#page-3-1)\n\n\u2022 For the input of each tile, seven contiguous blocks\
    \ of data labeled I1 to I7 are to be taken, across three different producer tiles.\n\
    \n\u2022 For the output, each tile will produce four contiguous blocks of data\
    \ labeled O1 to O4.\n\nThere is a correspondence between output blocks (MARS)\
    \ O from a tile and input blocks I from other tiles: each O corresponds to one\
    \ I in several other tiles.\n\nWithout any further information, the result of\
    \ MARS analysis would make the accelerator require seven input and four output\
    \ burst accesses. This number could potentially be reduced. If I1, I2 and I3 were\
    \ adjacent in memory, it would be possible to make a single access instead of\
    \ three, and likewise for I5, I6 and I7. The total number of input accesses would\
    \ go down to just three.\n\nIn order to reduce the number of accesses to the above,\
    \ we have to show that it is actually achievable: the blocks I1 through I7 are\
    \ read by multiple tiles, and coalescing opportunities for one tile may be incompatible\
    \ with another tile's coalescing opportunities.\n\nThe next subsection formalizes\
    \ this example into an optimization problem seeking to minimize the number of\
    \ accesses.\n\n## <span id=\"page-4-1\"></span>3.2 Enabling Coalesced Accesses\
    \ across Contiguous Data Blocks\n\nFrom the polyhedral analysis of the previous\
    \ subsection, we have determined sets of on-chip data to be grouped as contiguous\
    \ blocks of data, called MARS. How these blocks are laid out in memory is important\
    \ for access performance: if multiple MARS happen to be accessed in a row and\
    \ they are adjacent in memory, the accesses to these MARS can be coalesced into\
    \ a single access and better utilize bandwidth.\n\nThis section explains how the\
    \ \"outer layout\" of the MARS is determined so as to maximize the coalescing\
    \ opportunities.\n\n<span id=\"page-4-0\"></span>3.2.1 Properties of the layout.\
    \ The goal of this work is to minimize the number of I/O cycles, and therefore\
    \ the data layout must exhibit contiguity (for both reading and writing). However,\
    \ that contiguity must not come at the price of an increase in I/O volume. To\
    \ model this constraint, we apply two hypotheses.\n\nContiguous tile-level allocation.\
    \ We are looking for a layout of MARS in memory, and know that compression will\
    \ be applied to them. Due to the size and position of compressed blocks being\
    \ unpredictable, it is not feasible to interleave MARS from multiple tiles in\
    \ memory. Therefore, we allocate each tile a contiguous block of memory for its\
    \ MARS output.\n\nThis allocation has two consequences: the write side can be\
    \ done entirely contiguously, and we only have to optimize contiguity at the read\
    \ side.\n\nIrredundancy of storage. Under the previous hypothesis, we want to\
    \ maximize the coalescing opportunities between MARS accesses for the read side\
    \ only. While it is possible to obtain this contiguity by replicating the MARS\
    \ in multiple layouts, one per consumer, doing so would defeat the goal to save\
    \ I/O cycles. We therefore choose to store each MARS only once in memory (irredundant\
    \ storage).\n\nThe goal is now to find a single layout for the MARS produced by\
    \ each tile, that exhibits as much read-side coalescing opportunities as possible.\
    \ We obtain it throgh an optimization problem that is defined in the next subsections.\n\
    \n3.2.2 Example. In the example of Section [3.1,](#page-3-2) it appeared that\
    \ the number of burst accesses could go from 7 to 3. Let us show there actually\
    \ exists a layout achieving these 3 bursts.\n\nFigure [6](#page-3-1) shows the\
    \ correspondence between input and output MARS:\n\n- I1, I2 and I3 come from the\
    \ southwest tile, corresponding to its O2, O3 and O4 blocks. We would like these\
    \ three MARS to be contiguous, regardless of which relative order, to make a single\
    \ burst.\n- I4 comes from the south tile, corresponding to its O2 block.\n- I5,\
    \ I6 and I7 come from the southeast tile, corresponding to its O1, O2 and O3 blocks.\
    \ We would also like them to be contiguous.\n\nWe do not make any hypothesis on\
    \ the relative location of data from the southwest tile, the south tile and the\
    \ southeast tile. This makes it impossible to obtain fewer than 3 burst accesses.\n\
    \nThe information we have at this point can be used as the constraints and objective\
    \ of an optimization problem: we want to maximize the number of contiguities in\
    \ the layout among those desired, under the irredundancy constraint. We provide\
    \ a solver with the following problem:\n\n- Maximize the contiguities among the\
    \ desired ones: make MARS O2, O3 and O4 contiguous in any order, and make MARS\
    \ O1, O2 and O3 also contiguous in any order.\n- Per the hypothesis of Section\
    \ [3.2.1,](#page-4-0) we want a layout of MARS O1, O2, O3 and O4.\n- There can\
    \ be no fewer read bursts than 3.\n\nThe solver returns the following layout of\
    \ the output MARS for each tile: O1, O3, O2, O4.\n\nLooking from the consumers,\
    \ I1, I2 and I3 (resp. southwest O2, O3 and O4) are contiguous; I5, I6 and I7\
    \ (southeast O1, O2 and O3) are also contiguous. We can therefore coalesce, for\
    \ each tile, the reads I1, I2 and I3 into a single burst, and I5, I6 and I7 into\
    \ another burst, achieving the three sought input bursts.\n\n3.2.3 General case.\
    \ In this section, we lay out the blocks of data from the MARS analysis to maximize\
    \ the coalescing opportunities between them.\n\nWith the allocation choice of\
    \ Sec. [3.2.1,](#page-4-0) writes are guaranteed to be done without discontiguity.\
    \ We therefore lay out the MARS to make the read side as contiguous as possible.\
    \ In other words, we need to lay out the MARS in memory so that as many MARS as\
    \ possible can be read as a coalesced burst.\n\nWe propose to model this problem\
    \ as an Integer Linear Programming optimization problem as described in Algorithm\
    \ [1.](#page-5-1) Intuitively, if a pair of MARS is needed by a consumer tile\
    \ and the two MARS are next to each other in memory, then a coalesced access for\
    \ the two (a \"contiguity\") is issued. We therefore seek to maximize the number\
    \ of such contiguities.\n\nThe solution to this optimization problem, given by\
    \ the solver is an ordered list of the MARS produced by each tile, that allows\
    \ the minimal number of transactions to read all MARS input of a tile.\n\nThe\
    \ layout created in this section honors the irredundancy property of the MARS\
    \ (see Section [3.1\\)](#page-3-3), but does not yet take full advantage of their\
    \ atomicity: the fact that the MARS are contiguous\n\nAlgorithm 1: Optimizing\
    \ the MARS layout\n\nInput: = ( ) : = 1, ..., = list of MARS, P = list of producer\
    \ tiles from which MARS are needed, Result: = ordered list of MARS Optimization\
    \ Variables: let , \u2208 {0, 1} = successor variables: , = 1 encodes that MARS\
    \ is immediately before MARS in memory. let \u2208 {1, . . . , } be a permutation;\
    \ is the position where MARS will be in the final layout. Problem Constants: let\
    \ ,, be equal to 1 if and (, \u2208 [[1, ]]) from tile \u2208 P are consumed together,\
    \ 0 otherwise. Maximize #{contiguities}: \xCD \u2208P \xCD =1 \xCD =1 \u2260 ,,\
    \ , subject to: \u2022 \u2200 : , = 0 (a MARS is not its own predecessor) \u2022\
    \ \u2200 : \xCD , \u2A7D 1 (a MARS has at most 1 precedecessor) \u2022 \u2200\
    \ : \xCD , \u2A7D 1 (a MARS has at most 1 successor) \u2022 \xCD \xCD , = \u2212\
    \ 1 (number of successor relations) \u2022 \u2200 : 0 \u2A7D \u2A7D \u2212 1 (permutation\
    \ of length ) \u2022 \u2200, : ( \u2212 = 1) \u21D4 (, = 1) (definition of successor)\
    \ \u2022 \u2200, : ( \u2260 ) \u21D2 | \u2212 | \u2A7E 1 (MARS have \u2260 positions)\n\
    \nreturn = ( (1) , ..., ( ) ) (-ordered list of MARS)\n\n<span id=\"page-5-2\"\
    ></span>![](_page_5_Figure_4.jpeg)\n\nFigure 7: MARS data shown without compression,\
    \ with compression (inside the MARS) and with MARS compression and packing. Packing\
    \ the compressed MARS preserves the contiguity of coalesced accesses.\n\nblocks\
    \ of data makes them ideal candidates for data packing and compression. This is\
    \ what we perform in the next subsection.\n\n#### 3.3 Contiguity-Preserving Block\
    \ Compression\n\nSo far, our approach has given a layout of data in memory enabling\
    \ coalesced accesses to contiguous blocks of data produced and consumed by an\
    \ accelerator. These blocks have an atomicity property that we can further exploit\
    \ to save bandwidth, by applying data packing and compression, as illustrated\
    \ in Figure [7.](#page-5-2)\n\n3.3.1 Combining compression and packing. Compressed\
    \ blocks of data must be considered atomic in the sense that no random accesses\
    \ into them are possible. This atomicity property is borne by the MARS, as each\
    \ MARS data block is entirely used when it is accessed, i.e. there are no partial\
    \ accesses to a MARS.\n\nCompressing the MARS reduces the size of the data and\
    \ therefore saves bandwidth and storage space; however, it can also break the\
    \ contiguity brought by the layout of Section [3.2](#page-4-1) as illustrated\
    \ by Figure [7.](#page-5-2) To preserve it, we also apply packing to the compressed\
    \ MARS, making them immediately adjacent to each other in memory. Packing compressed\
    \ MARS also spares the accelerators from unused reads due to padding.\n\n3.3.2\
    \ Need to preserve metadata. As the size of compressed blocks depends on their\
    \ data, it is impossible to know the exact size of each access. However, the size\
    \ of a burst access must be known prior to the request being issued; additionally,\
    \ using an estimation of the size or an over-approximation would result in unused\
    \ input data or additional requests to fill in missing data.\n\nIn order to be\
    \ able to exactly fetch the right size, it is necessary to keep track of the size\
    \ of each compressed MARS. Moreover, the packing of compressed MARS means that\
    \ the start of a compressed block may be improperly aligned. It is also therefore\
    \ necessary to keep track of the alignment of each MARS for proper decompression.\
    \ In our implementation, bookkeeping is done using on-chip markers that are filled\
    \ in after each MARS is compressed. Details are in Section [4.2.2.](#page-6-0)\n\
    \n<span id=\"page-5-1\"></span>Packing will cause unused input data to enter;\
    \ however, its size is bounded to one aligned word at the beginning and one aligned\
    \ word at the end of each transaction. This input redundancy is notably independent\
    \ of the size of the MARS.\n\n#### <span id=\"page-5-0\"></span>4 INTEGRATION\
    \ INTO HLS DESIGN FLOW\n\nIn this section, we show how we transform an HLS accelerator\
    \ description in order to optimize its off-chip memory accesses for bandwidth\
    \ utilization.\n\nThe off-chip data layout and compression proposed in Section\
    \ [3](#page-3-0) can be automatically implemented around the existing description\
    \ of a tile in HLS. The result is a sequence of steps:\n\n- Read MARS layout data\
    \ and non-MARS input data from off-chip memory into on-chip FIFOs,\n- Decompress\
    \ the input data into FIFOs,\n- Dispatch MARS data into on-chip buffers with an\
    \ allocation suitable for computation,\n- Perform the computations onto on-chip\
    \ buffers,\n- Collect MARS output data from the on-chip buffers into FIFOs,\n\
    - Compress the collected data,\n- Write back the results into MARS layout in off-chip\
    \ memory.\n\nThe next subsections explain how the complex data structures describing\
    \ the MARS are turned into two simple decompression/dispatch and collect/compression\
    \ steps.\n\n#### 4.1 From MARS to Collect/Dispatch Functions\n\nThe input and\
    \ output data of each tile is respectively copied into and out of on-chip buffers\
    \ before the tile execution takes place and after it has fully completed. This\
    \ is the step where the data goes from a contiguous layout to a non-contiguous\
    \ layout (suitable for execution) and vice-versa.\n\nImplementing these dispatch\
    \ and collect steps requires to describe each MARS so that the data contained\
    \ in it is placed into, or taken from, the right location in on-chip memory. Before\
    \ dispatch and after collect, the data is located into FIFOs in the contiguous\
    \ layout.\n\n```\n1 // MARS Dispatch\n2 for ( int i =0; i <33; ++i) {\n3 // take\
    \ on - chip address from ROM\n4 struct mars_transfert mt = FPGA_MARS_IN_TBL [i];\n\
    5 switch (mt. array ) {\n6 // on - chip random write\n7 case MARS_DATA_ENUM ::A:\
    \ {\n8 marsToMem_A (mt.dim0 , mt. dim1 ) ;\n9 break ;\n10 }\n11 case MARS_DATA_ENUM\
    \ ::B: {\n12 marsToMem_B (mt.dim0 , mt. dim1 ) ;\n13 break ;\n14 }\n15 }\n16 }\n\
    ```\nFigure 8: Structure of the MARS dispatch implementation (off-chip to on-chip\
    \ layout)\n\nMARS can have arbitrary complex shapes, and cannot in general be\
    \ described using simple loops. However, it is possible to fully unroll these\
    \ loops and obtain a list of on-chip addresses for each MARS. Such unrolled lists\
    \ are placed into read-only memories on chip. Iterating through these ROMs as\
    \ in Figure [8](#page-6-1) gives the corresponding addresses. The size of these\
    \ ROMs is notably only dependent on the tile size, and not on the problem size\
    \ or data type.\n\n#### 4.2 Automatic compression\n\nWhen the data is in the contiguous\
    \ layout in the form of MARS, it can be seamlessly compressed and decompressed,\
    \ and the compressed MARS can be packed to preserve contiguity. We explain here\
    \ the compression, packing and decompression steps, along with how the compression\
    \ metadata is taken care of.\n\n4.2.1 Compressing Data and Packing MARS. The compression\
    \ step is relatively straightforward: the compression module takes its input from\
    \ the collect step FIFO, and generates a compressed stream of data from it. The\
    \ layout of the data in this FIFO is not altered by the compression step. Likewise,\
    \ the decompression step takes a stream of compressed words and decompresses it\
    \ into a FIFO, which is then used by the MARS dispatch step. MARS packing is transparently\
    \ implemented by the compression step: because MARS are provided in a contiguous\
    \ manner from the collect step, the first word of each MARS will be immediately\
    \ adjacent to the last word of the previous MARS in the compressed data stream.\n\
    \nOur compressor, which algorithm is given in Section [2.5,](#page-3-4) is pipelined\
    \ with an initiation interval of 1 cycle, despite a loop-carried dependence.\n\
    \nThe difficult part to implement is decompression: because not all MARS from\
    \ a given tile are decompressed, we need to be able to seek at the start of a\
    \ particular MARS. This ability is given by metadata described in the next paragraph.\n\
    \n<span id=\"page-6-0\"></span>4.2.2 Metadata management. The consequence of MARS\
    \ compression is that their size is unknown a priori. To preserve the contiguity\
    \ of the layout from Section [3.2,](#page-4-1) we must avoid padding the compressed\
    \ MARS to preserve alignment, Therefore, the compressed\n\nMARS are packed and\
    \ immediately adjacent to each other in memory.\n\nTo keep track of the position\
    \ of each MARS, we use a data structure with two pieces of information: a coarse-grain\
    \ position indicating how far (in aligned words) to seek, and a fine-grain position\
    \ marker that specifies which bit is the first of the said MARS.\n\nBecause the\
    \ length of MARS is known at compile time and constant across tiles, the position\
    \ of the markers within the uncompressed stream is also constant. Therefore, like\
    \ the MARS descriptions, the positions of markers (i.e. start of each MARS) within\
    \ the uncompressed stream are put into a ROM:\n\n```\n1 # define NB_MARKERS 3\n\
    ```\n\n```\n2 # define MARKERS {62 , 63 , 64}\n```\nThe markers for the compressed\
    \ stream are maintained within an on-chip cache, which size is specified at synthesis\
    \ time via a macro:\n\n```\n1 struct compressed_marker < NB_MARS_POS_BITS ,\n\
    \       LOG_BUS_WIDTH > markers [ COMPRESSION_METADATA_SIZE ][\n       NB_MARKERS\
    \ ];\n```\nThe allocation within this cache is done from the host: registers are\
    \ used to specify whether a tile's MARS are compressed, whether its dependences\
    \ are, and where the markers for its dependences are located. This location depends\
    \ on the size of the space; for the Jacobi stencil, the formula is:\n\n```\n1\
    \ unsigned compressionMetadataAllocation (\n```\n\n```\n2 int tsteps , int n,\
    \ int M1 , int M2 , int k1 , int k2) {\n```\n\n```\n3 return (k2) + M2DEC_FORMULA\
    \ + M2 * (( k1 - 1) & 0x01) ;\n4 }\n```\nIt should be noted that the markers structure\
    \ is persistent between runs. It is updated by the MARS write step and used by\
    \ the MARS read step. This update prevents the current HLS tools from constructing\
    \ a macro-pipeline (e.g. using the HLS DATAFLOW pragma) unless the structure is\
    \ in a separate module.\n\n#### 4.3 Host/FPGA dispatching of tiles\n\nThe FPGA\
    \ accelerator must have a simple control structure to exhibit as much parallelism\
    \ as possible. Therefore, only full tiles are executed on FPGA. Full tiles also\
    \ all have the same volume of I/O, regardless of their position in the iteration\
    \ space.\n\nPartial tiles, i.e. those that contain space boundaries, are run on\
    \ the host CPU, using the original program's allocation. To permit this, data\
    \ computed on FPGA is taken back from MARS into the original program's memory,\
    \ and MARS are created back from partial tiles results. It can be demonstrated\
    \ that no FPGA tiles need any missing MARS data from partial tiles, and therefore\
    \ there is no issue in writing part of the MARS for these tiles.\n\nThe operations\
    \ performed to execute a partial tile (on the host) are:\n\n- Read MARS from neighboring\
    \ full tiles that were executed on FPGA, remap their data to its original location,\n\
    - Execute the tile's iterations using the original allocation,\n- Write back MARS\
    \ by copying data from the original allocation, skipping cells that would be in\
    \ MARS yet have no producer iteration.\n\nThe control flow necessary for compression\
    \ would significantly lengthen the execution of host tiles. Therefore, only tiles\
    \ which producers and consumers are all executed on FPGA will use compression.\n\
    \n#### <span id=\"page-7-0\"></span>5 EVALUATION\n\nWe evaluate our approach with\
    \ respect to the following questions:\n\n- Compile-time performance: How much\
    \ time does it take to compute the MARS layout?\n- Design quality: How does using\
    \ MARS affect the FPGA accelerator's area consumption?\n- Runtime performance:\
    \ How much I/O cycles do compressed MARS save with respect to a non-MARS memory\
    \ layout?\n- Applicability: How does the data type, tile size and problem size\
    \ affect the compression ratio?\n\n#### 5.1 Protocol and benchmarks\n\n5.1.1 Benchmarks.\
    \ We have selected the following applications from the PolyBench/C suite[\\[20\\\
    ]](#page-10-14):\n\n- jacobi-1d: Jacobi 1D stencil, as used in the running example;\n\
    - jacobi-2d: Two-dimensional version of the Jacobi stencil, exhibiting few and\
    \ simple MARS;\n- seidel-2d: More complex benchmark exhibiting a higher number\
    \ of MARS with more complex shapes.\n\nLayout determination was done using the\
    \ Gurobi solver (version 10.0.3 build v10.0.3rc0 (linux64)).\n\nThe data types\
    \ used are fixed-point numbers (18 bits, 24 bits, 28 bits) and floating-point\
    \ numbers (float, double). We also ran simulations with a 12-bit fixed-point data\
    \ type without synthesizing it, Vitis HLS being unable to infer bursts from that\
    \ data type.\n\nThe chosen applications provide a non-MARS data layout in their\
    \ original code. Because FPGA developers usually try to seek burst accesses where\
    \ possible, we have created two access patterns on the non-MARS layout to compare\
    \ against that try to exhibit bursts:\n\n- A minimal access pattern, fetching\
    \ and storing the exact I/O footprint of the tile, letting the HLS tool infer\
    \ bursts where possible.\n- A rectangular bounding box of the accessed data like\
    \ done in PolyOpt/HLS [\\[21\\]](#page-10-6), which description is simple enough\
    \ to infer only burst accesses.\n\nBoth access patterns are generated using a\
    \ polyhedral code generator available in ISL [\\[27\\]](#page-10-15).\n\n5.1.2\
    \ Hardware platform. We used a Xilinx ZCU104 evaluation board, equipped with a\
    \ xczu7ev MPSoC. We ran Pynq 3.0.1 with Linux 5.15 and synthesis was done using\
    \ the Vitis/Vivado suite version 2022.2.2 All benchmarks, are running at a clock\
    \ frequency of 187 MHz and communicate with the off-chip DDR using one non cache-coherent\
    \ AXI HP port.\n\n5.1.3 Protocol. Each benchmark is run for each data type, each\
    \ space size and tile size. Part of the computation is done on the host: incomplete\
    \ tiles are executed on a single thread on the Cortex-A53\n\nCPU of the MPSoC.\
    \ Transfer cycles are measured only for the FPGA tiles and do not account for\
    \ the host.\n\nCycle measurements are gathered using an on-FPGA counter and the\
    \ area measurements are extracted from Vivado place and route reports.\n\nTable\
    \ [1](#page-8-0) shows the characteristics of each benchmark, in terms of number\
    \ of MARS, and number of bursts after coalescing optimization of Sec. [3.2.](#page-4-1)\n\
    \n#### 5.2 Results and discussion\n\n5.2.1 Compile-time performance. Table [2](#page-8-1)\
    \ shows the time it took for each benchmark to be run through the layout determination\
    \ and code generation framework. The compilation process does not take more than\
    \ a few seconds to execute for the benchmarks we selected, starting from the polyhedral\
    \ representation of the program to the end of HLS code generation. Notably, the\
    \ layout determination ILP problem only depends on the number of MARS and is independent\
    \ of the tile size.\n\n5.2.2 Design quality. Figure [9](#page-8-2) shows the total\
    \ area occupied by our benchmarks, with respect to the different memory allocation\
    \ baselines. One tile size per benchmark is considered.\n\nMARS introduces extra\
    \ control logic and extra I/O functions that the other baselines do not have.\
    \ It is therefore normal to observe area increases with this baseline. The most\
    \ significant increases in Figure [9a](#page-8-2) are for jacobi1d; in this benchmark,\
    \ on-chip arrays are implemented in logic instead of Block RAM. Figure [9b](#page-8-2)\
    \ shows little DSP and BRAM consumption by this benchmark compared to others.\
    \ FIFOs holding all the MARS are implemented only on the MARS baseline, and require\
    \ extra BRAMs. The extra DSP blocks for MARS baselines come from the address computations\
    \ that are performed inside the I/O units; the size of the space is passed as\
    \ a parameter instead of being a constant, requiring true multipliers.\n\nFigure\
    \ [9a](#page-8-2) shows that the data width causes the logic area to increase\
    \ with it. This increase is more sensible in jacobi1d where the on-chip arrays\
    \ are implemented in logic instead of Block RAM, which effect is also visible\
    \ in Figure [9b.](#page-8-2)\n\n5.2.3 Runtime performance. Figure [10](#page-8-3)\
    \ shows the transfer time relative to compressed MARS for each data type and each\
    \ benchmark.\n\nImpact of dimensionality. For the 2d examples that have threedimensional\
    \ iteration spaces, using MARS layout is already profitable versus the non-MARS\
    \ layouts; most of the gains are due to contiguity more than compression. On the\
    \ one-dimensional Jacobi example, the gains are on the contrary more due to compression:\
    \ the data being one-dimensional, non-MARS layouts are already contiguous. For\
    \ small tile sizes like 6 \xD7 6, the gains are marginal if any: the number of\
    \ compressed elements is too small to exhibit large gains from compression.\n\n\
    Effect of data type. On the jacobi-1d benchmark, the choice of a 200 \xD7 200\
    \ tile size shows a more significant benefit in using compressed MARS for fixed-point\
    \ data types than floating-point. This is explained with the better compression\
    \ ratio: when modeling continuous spaces like those used on the Jacobi stencils,\
    \ neighboring fixed-point values will have more higher bits in common than\n\n\
    | Benchmark | Tile Sizes               | #MARS In | #MARS Out | Read bursts |\
    \ Write bursts |\n|-----------|--------------------------|----------|-----------|-------------|--------------|\n\
    | jacobi-1d | 6 \xD7 6 64 \xD7 64, 200 \xD7 200 | 7        | 4         | 3   \
    \        | 1            |\n| jacobi-2d | 4 \xD7 5 \xD7 7, 10 \xD7 10 \xD7 10 \
    \ | 28       | 13        | 10          | 1            |\n| seidel-2d | 4 \xD7\
    \ 10 \xD7 10              | 33       | 13        | 10          | 1           \
    \ |\n\n<span id=\"page-8-0\"></span>Table 1: Characteristics of the selected benchmarks.\
    \ The number of bursts per tile accounts for layout-induced access coalescing\
    \ and is independent of tile and problem size.\n\n<span id=\"page-8-2\"></span>![](_page_8_Figure_3.jpeg)\n\
    \nFigure 9: Area statistics for the benchmarks\n\n<span id=\"page-8-3\"></span>![](_page_8_Figure_5.jpeg)\n\
    \nFigure 10: Transfer time relative to compressed MARS (lower is better).\n\n\
    <span id=\"page-8-1\"></span>\n\n| Benchmark | Tile Size    | Compile Time (s)\
    \ |\n|-----------|--------------|------------------|\n| jacobi-1d | 6 \xD7 6 \
    \       | 0.76             |\n| jacobi-1d | 64 \xD7 64      | 0.68           \
    \  |\n| jacobi-1d | 200 \xD7 200    | 1.02             |\n| jacobi-2d | 4 \xD7\
    \ 5 \xD7 7    | 5.57             |\n| jacobi-2d | 10 \xD7 10 \xD7 10 | 5.09  \
    \           |\n| seidel-2d | 4 \xD7 10 \xD7 10  | 3.21             |\n\n![](_page_8_Figure_8.jpeg)\n\
    \nfloating-point data where neighboring values mostly only share the exponent.\n\
    \n5.2.4 Applicablity. Figure [11](#page-9-0) shows the compression rate for each\
    \ data type and tile size for the jacobi1d benchmark. Two ratios are shown: the\
    \ true ratio which accounts only for the bit savings due to compression, and a\
    \ ratio with padding that accounts for the savings due to not padding the data.\
    \ The ratio with padding is the one our accelerators really benefit from, because\
    \ the data is not packed in memory except in compressed MARS form.\n\n<span id=\"\
    page-9-0\"></span>![](_page_9_Figure_0.jpeg)\n\nFigure 11: Compression ratio vs.\
    \ data type and tile size for jacobi1d\n\nOverall, compressing the data for the\
    \ selected benchmarks is almost always profitable, possibly largely as the compression\
    \ ratio goes up to 5.09:1 for 200 \xD7 200 tiles and 18-bit type.\n\nWe can observe\
    \ that large tiles (64 \xD7 64, 200 \xD7 200) exhibit closer compression ratios\
    \ than smaller tiles (6 \xD7 6). This discrepancy can be explained by the compressed\
    \ chunks being too small to benefit from the data's low entropy; for the smallest\
    \ data type and tile size, compressing data is even worse than not compressing.\n\
    \n#### 6 RELATED WORK\n\nThis work comes as part of a global effort to relieve\
    \ memoryboundness of high-performance accelerators. In this section, we study\
    \ other techniques used to relieve the memory wall, some of which may not apply\
    \ to compilers due to not being automatable or breaking program semantics.\n\n\
    #### 6.1 Data Compression\n\nData compression saves bandwidth without requiring\
    \ to modify the program's algorithm. It is therefore suitable for many bandwidthbound\
    \ problems.\n\n6.1.1 Compression techniques. Data compression in FPGA accelerators\
    \ is already a necessity for some intrinsically memory-bound applications such\
    \ as deep convolutional networks, as no locality optimization can bring further\
    \ bandwidth savings. We here focus on two kinds of compression: lossless and lossy.\n\
    \nLossless compression. Lossless compression guarantees that the decompressed\
    \ data is exactly the same as the data before it was compressed. This property\
    \ makes it possible to do seamless, inline compression and decompression as is\
    \ done for MARS. This is commonly performed in deep neural network accelerators\
    \ [\\[1,](#page-10-16) [11\\]](#page-10-17)\n\nSparse encoding can be considered\
    \ a form of lossless compression, and is also commonly found in machine learning\
    \ applications [\\[8,](#page-10-18) [13\\]](#page-10-19). Sparse data structures\
    \ often require indirections, which make them unsuitable for use in polyhedral\
    \ compiler flows unless the sparse structure is immutable [\\[12\\]](#page-10-20).\n\
    \nLossy compression. It is possible to save more storage and bandwidth by using\
    \ lossy compression. Some applications in machine\n\nlearning can afford a loss\
    \ of precision without degrading the quality of the result, e.g. using JPEG-compressed\
    \ images [\\[18\\]](#page-10-21) as inputs. However, automatic compression alters\
    \ the data and cannot be automatically inserted by a compiler unless the user\
    \ explicitly requests it.\n\n6.1.2 Dynamic data compression. In this work, we\
    \ automate the compression and decompression of data and it is transparent to\
    \ the computation engine on FPGA. Other works [\\[19,](#page-10-22) [23\\]](#page-10-23)\
    \ perform dynamic, demand-driven compression without prior knowledge of the data\
    \ to be handled. Thanks to the static control flow of polyhedral codes, all the\
    \ data flow is statically known and it is not necessary to maintain a cache policy.\n\
    \n#### 6.2 Memory access optimization\n\nThe layout we propose in this work optimizes\
    \ memory accesses by exhibiting contiguity using polyhedral analysis. In this\
    \ section, we go through other polyhedral memory access optimizations, and explain\
    \ other non-polyhedral ways it is possible to improve memory accesses.\n\n6.2.1\
    \ Polyhedral-based optimizations. Using the polyhedral model and loop tiling to\
    \ capture the data flow is the subject of a number of works, proposing different\
    \ breakups of the dataflow. Datharthri et al. [\\[7\\]](#page-10-24) and Bondhugula\
    \ [\\[2\\]](#page-10-25) propose decompositions of the inter-tile communications\
    \ to minimze MPI communications. This work also seeks to optimize the passing\
    \ of intermediate results, but the data allocation is not statically determined\
    \ like in this work.\n\nA MARS-like decomposition of the inter-tile data flow\
    \ into coarse-grain blocks for MPI has been proposed by Zhao et al. [\\[30\\]](#page-10-26);\
    \ our work achieves irredundancy which requires a finer-grain modeling than the\
    \ one proposed by [\\[30\\]](#page-10-26).\n\n6.2.2 Domain-specific optimizations.\
    \ Memory access optimizations such as a change of data layout or access pattern\
    \ can also be specific to each problem. We show here two cases of domain-specific\
    \ optimizations.\n\nData blocking. Data blocking (or tiling) is memory layout\
    \ transformation that chunks multi-dimensional arrays into contiguous blocks.\
    \ Similar to loop tiling, data blocking allows to coalesce accesses to entire\
    \ regions of the input or output data.\n\nData blocking can be efficient when\
    \ the memory footprint of one iteration of an accelerator corresponds to a data\
    \ tile. Although it has been used to optimize machine learning accelerators [\\\
    [26\\]](#page-10-27), it may break spatial locality and degrade performance of\
    \ accesses that cross tile boundaries.\n\nData blocking can be combined with loop\
    \ tiling and polyhedral analysis to coalesce inter-tile accesses. Ferry et al.\
    \ [\\[10\\]](#page-10-28) seeks to exhibit the largest possible contiguous units\
    \ spanning multiple tiles.\n\nStencil optimization. Stencil computations have\
    \ regular and statically known memory access patterns. Domain-specific optimizers\
    \ like SODA [\\[3\\]](#page-10-29) derive an optimized FPGA architecture and memory\
    \ layout specific to each stencil.\n\n#### 7 CONCLUSION\n\nThis work gives a twofold\
    \ contribution: a compression-friendly, contiguous data layout, and an automated\
    \ adaptation of FPGA accelerators to use this layout thanks to polyhedral compilation\
    \ tools. Thanks to the compression and contiguity, we can automatically reduce\
    \ the number of I/O cycles spent by the accelerator.\n\n### REFERENCES\n\n- <span\
    \ id=\"page-10-16\"></span>[1] Thea Aarrestad, Vladimir Loncar, Nicol\xF2 Ghielmetti,\
    \ Maurizio Pierini, Sioni Summers, Jennifer Ngadiuba, Christoffer Petersson, Hampus\
    \ Linander, Yutaro Iiyama, Giuseppe Di Guglielmo, Javier Duarte, Philip Harris,\
    \ Dylan Rankin, Sergo Jindariani, Kevin Pedro, Nhan Tran, Mia Liu, Edward Kreinar,\
    \ Zhenbin Wu, and Duc Hoang. 2021. Fast convolutional neural networks on FPGAs\
    \ with hls4ml. Machine Learning: Science and Technology 2, 4 (jul 2021), 045015.\
    \ <https://doi.org/10.1088/2632-2153/ac0ea1>\n- <span id=\"page-10-25\"></span>[2]\
    \ Uday Bondhugula. 2013. Compiling Affine Loop Nests for Distributed-Memory Parallel\
    \ Architectures. In Proceedings of the International Conference on High Performance\
    \ Computing, Networking, Storage and Analysis. ACM. [https://doi.](https://doi.org/10.1145/2503210.2503289)\
    \ [org/10.1145/2503210.2503289](https://doi.org/10.1145/2503210.2503289)\n- <span\
    \ id=\"page-10-29\"></span>[3] Yuze Chi, Jason Cong, Peng Wei, and Peipei Zhou.\
    \ 2018. SODA: Stencil with optimized dataflow architecture. In 2018 IEEE/ACM International\
    \ Conference on Computer-Aided Design (ICCAD). IEEE, 1\u20138. [https://doi.org/10.1145/3240765.](https://doi.org/10.1145/3240765.3240850)\
    \ [3240850](https://doi.org/10.1145/3240765.3240850)\n- <span id=\"page-10-8\"\
    ></span>[4] Young-kyu Choi and Jason Cong. 2018. HLS-based optimization and design\
    \ space exploration for applications with variable loop bounds. In 2018 IEEE/ACM\
    \ International Conference on Computer-Aided Design (ICCAD). IEEE, 1\u20138.\n\
    - <span id=\"page-10-2\"></span>[5] Jason Cong, Muhuan Huang, Peichen Pan, Yuxin\
    \ Wang, and Peng Zhang. 2016. Source-to-source optimization for HLS. FPGAs for\
    \ Software Programmers (2016), 137\u2013163.\n- <span id=\"page-10-11\"></span>[6]\
    \ P Cummiskey, Nikil S. Jayant, and James L. Flanagan. 1973. Adaptive Quantization\
    \ in Differential PCM Coding of Speech. Bell System Technical Journal 52 (09 1973).\
    \ <https://doi.org/10.1002/j.1538-7305.1973.tb02007.x>\n- <span id=\"page-10-24\"\
    ></span>[7] Roshan Dathathri, Chandan Reddy, Thejas Ramashekar, and Uday Bondhugula.\
    \ 2013. Generating Efficient Data Movement Code for Heterogeneous Architectures\
    \ with Distributed-Memory. In Proceedings of the 22nd International Conference\
    \ on Parallel Architectures and Compilation Techniques. IEEE. [https:](https://doi.org/10.1109/PACT.2013.6618833)\
    \ [//doi.org/10.1109/PACT.2013.6618833](https://doi.org/10.1109/PACT.2013.6618833)\n\
    - <span id=\"page-10-18\"></span>[8] Yixiao Du, Yuwei Hu, Zhongchun Zhou, and\
    \ Zhiru Zhang. 2022. High-Performance Sparse Linear Algebra on HBM-Equipped FPGAs\
    \ Using HLS. In Proceedings of the 2022 ACM/SIGDA International Symposium on Field-Programmable\
    \ Gate Arrays. ACM.<https://doi.org/10.1145/3490422.3502368>\n- <span id=\"page-10-13\"\
    ></span>[9] Corentin Ferry, Steven Derrien, and Sanjay Rajopadhye. 2023. Maximal\
    \ Atomic irRedundant Sets: a Usage-based Dataflow Partitioning Algorithm. In 13th\
    \ International Workshop on Polyhedral Compilation Techniques (IMPACT'23).\n-\
    \ <span id=\"page-10-28\"></span>[10] Corentin Ferry, Tomofumi Yuki, Steven Derrien,\
    \ and Sanjay Rajopadhye. 2022. Increasing FPGA Accelerators Memory Bandwidth with\
    \ a Burst-Friendly Memory Layout. IEEE Transactions on Computer-Aided Design of\
    \ Integrated Circuits and Systems (2022), 1\u20131.<https://doi.org/10.1109/tcad.2022.3201494>\n\
    - <span id=\"page-10-17\"></span>[11] Yijin Guan, Ningyi Xu, Chen Zhang, Zhihang\
    \ Yuan, and Jason Cong. 2017. Using Data Compression for Optimizing FPGA-Based\
    \ Convolutional Neural Network Accelerators. In Lecture Notes in Computer Science.\
    \ Springer International Publishing, 14\u201326. [https://doi.org/10.1007/978-3-319-67952-5\\\
    \\_2](https://doi.org/10.1007/978-3-319-67952-5_2)\n- <span id=\"page-10-20\"\
    ></span>[12] Marcos Horro, Louis-No\xEBl Pouchet, Gabriel Rodr\xEDguez, and Juan\
    \ Touri\xF1o. 2023. Custom High-Performance Vector Code Generation for Data-Specific\
    \ Sparse Computations. In Proceedings of the International Conference on Parallel\
    \ Architectures and Compilation Techniques (Chicago, Illinois) (PACT '22). Association\
    \ for Computing Machinery, New York, NY, USA, 160\u2013171. [https://doi.org/10.1145/](https://doi.org/10.1145/3559009.3569668)\
    \ [3559009.3569668](https://doi.org/10.1145/3559009.3569668)\n- <span id=\"page-10-19\"\
    ></span>[13] Shiqing Li, Di Liu, and Weichen Liu. 2023. Efficient FPGA-based Sparse\
    \ Matrix-Vector Multiplication with Data Reuse-aware Compression. IEEE Transactions\
    \ on Computer-Aided Design of Integrated Circuits and Systems (2023), 1\u2013\
    1. [https:](https://doi.org/10.1109/tcad.2023.3281715) [//doi.org/10.1109/tcad.2023.3281715](https://doi.org/10.1109/tcad.2023.3281715)\n\
    - <span id=\"page-10-3\"></span>[14] Junyi Liu, Samuel Bayliss, and George A Constantinides.\
    \ 2015. Offline synthesis of online dependence testing: Parametric loop pipelining\
    \ for HLS. In 2015 IEEE 23rd Annual International Symposium on Field-Programmable\
    \ Custom Computing Machines. IEEE, 159\u2013162.\n- <span id=\"page-10-4\"></span>[15]\
    \ Junyi Liu, John Wickerson, Samuel Bayliss, and George A Constantinides. 2017.\
    \ Polyhedral-based dynamic loop pipelining for high-level synthesis. IEEE Transactions\
    \ on Computer-Aided Design of Integrated Circuits and Systems 37, 9 (2017), 1802\u2013\
    1815.\n- <span id=\"page-10-0\"></span>[16] Michael Lo, Young-kyu Choi, Weikang\
    \ Qiao, Mau-Chung Frank Chang, and Jason Cong. 2023. HMLib: Efficient Data Transfer\
    \ for HLS Using Host Memory. In Proceedings of the 2023 ACM/SIGDA International\
    \ Symposium on Field Programmable Gate Arrays. 50\u201350.\n- <span id=\"page-10-1\"\
    ></span>[17] Florian Mayer, Julian Brandner, and Michael Philippsen. 2023. Employing\
    \ Polyhedral Methods to Reduce Data Movement in FPGA Stencil Codes. In International\
    \ Workshop on Languages and Compilers for Parallel Computing. Springer, 47\u2013\
    63.\n- <span id=\"page-10-21\"></span>[18] Hiroki Nakahara, Zhiqiang Que, and\
    \ Wayne Luk. 2020. High-Throughput Convolutional Neural Network on an FPGA by\
    \ Customized JPEG Compression. In 2020 IEEE 28th Annual International Symposium\
    \ on Field-Programmable Custom Computing Machines (FCCM). 1\u20139.<https://doi.org/10.1109/FCCM48280.2020.00010>\n\
    - <span id=\"page-10-22\"></span>[19] O. Ozturk, M. Kandemir, and M.J. Irwin.\
    \ 2009. Using Data Compression for Increasing Memory System Utilization. IEEE\
    \ Transactions on Computer-Aided Design of Integrated Circuits and Systems 28,\
    \ 6 (jun 2009), 901\u2013914. [https:](https://doi.org/10.1109/tcad.2009.2017430)\
    \ [//doi.org/10.1109/tcad.2009.2017430](https://doi.org/10.1109/tcad.2009.2017430)\n\
    - <span id=\"page-10-14\"></span>[20] Louis-No\xEBl Pouchet and Tomofumi Yuki.\
    \ 2016. PolyBench/C 4.2.1. [http:](http://polybench.sf.net) [//polybench.sf.net](http://polybench.sf.net)\n\
    - <span id=\"page-10-6\"></span>[21] Louis-Noel Pouchet, Peng Zhang, P. Sadayappan,\
    \ and Jason Cong. 2013. Polyhedral-based data reuse optimization for configurable\
    \ computing. In Proceedings of the ACM/SIGDA international symposium on Field\
    \ programmable gate arrays - FPGA '13. ACM Press.<https://doi.org/10.1145/2435264.2435273>\n\
    - <span id=\"page-10-9\"></span>[22] Tiago Santos and Jo\xE3o MP Cardoso. 2020.\
    \ Automatic selection and insertion of hls directives via a source-to-source compiler.\
    \ In 2020 International Conference on Field-Programmable Technology (ICFPT). IEEE,\
    \ 227\u2013232.\n- <span id=\"page-10-23\"></span>[23] Somayeh Sardashti, Andre\
    \ Seznec, and David A. Wood. 2016. Yet Another Compressed Cache: A Low-Cost Yet\
    \ Effective Compressed Cache. ACM Trans. Archit. Code Optim. 13, 3, Article 27\
    \ (Sept. 2016), 25 pages. [https://doi.org/10.](https://doi.org/10.1145/2976740)\
    \ [1145/2976740](https://doi.org/10.1145/2976740)\n- <span id=\"page-10-12\"></span>[24]\
    \ A. Skodras, C. Christopoulos, and T. Ebrahimi. 2001. The JPEG 2000 still image\
    \ compression standard. IEEE Signal Processing Magazine 18, 5 (2001), 36\u2013\
    58. <https://doi.org/10.1109/79.952804>\n- <span id=\"page-10-10\"></span>[25]\
    \ Atefeh Sohrabizadeh, Cody Hao Yu, Min Gao, and Jason Cong. 2022. AutoDSE: Enabling\
    \ software programmers to design efficient FPGA accelerators. ACM Transactions\
    \ on Design Automation of Electronic Systems (TODAES) 27, 4 (2022), 1\u201327.\n\
    - <span id=\"page-10-27\"></span>[26] Teng Tian, Xi Jin, Letian Zhao, Xiaotian\
    \ Wang, Jie Wang, and Wei Wu. 2020. Exploration of Memory Access Optimization\
    \ for FPGA-based 3D CNN Accelerator. In 2020 Design, Automation & Test in Europe\
    \ Conference & Exhibition (DATE). 1650\u20131655.<https://doi.org/10.23919/DATE48585.2020.9116376>\n\
    - <span id=\"page-10-15\"></span>[27] Sven Verdoolaege. 2010. isl: An Integer\
    \ Set Library for the Polyhedral Model. In Mathematical Software \u2013 ICMS 2010,\
    \ Komei Fukuda, Joris van der Hoeven, Michael Joswig, and Nobuki Takayama (Eds.).\
    \ Springer Berlin Heidelberg, Berlin, Heidelberg, 299\u2013302.\n- <span id=\"\
    page-10-5\"></span>[28] Hanchen Ye, Cong Hao, Jianyi Cheng, Hyunmin Jeong, Jack\
    \ Huang, Stephen Neuendorffer, and Deming Chen. 2022. Scalehls: A new scalable\
    \ high-level synthesis framework on multi-level intermediate representation. In\
    \ 2022 IEEE International Symposium on High-Performance Computer Architecture\
    \ (HPCA). IEEE, 741\u2013755.\n- <span id=\"page-10-7\"></span>[29] Ruizhe Zhao,\
    \ Jianyi Cheng, Wayne Luk, and George A Constantinides. 2022. POLSCA: Polyhedral\
    \ High-Level Synthesis with Compiler Transformations. In 2022 32nd International\
    \ Conference on Field-Programmable Logic and Applications (FPL). IEEE, 235\u2013\
    242.\n- <span id=\"page-10-26\"></span>[30] Tuowen Zhao, Mary Hall, Hans Johansen,\
    \ and Samuel Williams. 2021. Improving communication by optimizing on-node data\
    \ movement with data layout. In Proceedings of the 26th ACM SIGPLAN Symposium\
    \ on Principles and Practice of Parallel Programming. ACM.<https://doi.org/10.1145/3437801.3441598>"
  paper_id: 25
  scores:
    originality:
      raw: 4.5
      reason: The proposed SAL-PIM architecture presents a novel approach to enhancing
        transformer-based text generation, showcasing significant innovation in PIM
        technology.
    technical_soundness:
      raw: 4.0
      reason: The methods employed are well-founded and supported by a cycle-accurate
        simulator, though some implementation details could be elaborated for clarity.
    clarity:
      raw: 4.0
      reason: The paper is generally well-structured and clear, but certain technical
        aspects may benefit from additional explanations for broader accessibility.
    significance:
      raw: 4.5
      reason: The work addresses a critical bottleneck in transformer-based models,
        making it highly significant for both academic research and practical applications.
    relevance:
      raw: 5.0
      reason: The topic is highly relevant to current trends in deep learning and
        natural language processing, particularly in the context of improving computational
        efficiency.
  total_score: 4.3
  tier: B
- title: "Optimization of a Line Detection Algorithm for Autonomous Vehicles on a\n\
    \  RISC-V with Accelerator"
  abstract: 'GPUs are widely used to accelerate many important classes of workloads
    today.

    However, we observe that several important emerging classes of workloads,

    including simulation engines for deep reinforcement learning and dynamic neural

    networks, are unable to fully utilize the massive parallelism that GPUs offer.

    These applications tend to have kernels that are small in size, i.e., have few

    thread blocks that do not saturate compute resources. Executing independent

    kernels concurrently is a promising approach to improve parallelism and

    utilization. However, this inter-kernel concurrency is difficult to leverage in

    such workloads with existing approaches: First, the inter-kernel dependencies

    and computational graph are input-dependent and vary each time the application

    is executed. Second, the computational graphs tend to be irregular, requiring

    fine-grain scheduling and synchronization; thus incurring significant

    synchronization overheads if kernel execution is parallelized. In this work, we

    propose ACS, a framework that enables lightweight detection of inter-kernel

    dependencies and low overhead kernel scheduling at runtime. The key idea behind

    ACS is to perform inter-kernel dependency checks for a small window of kernels

    at runtime, similar to out-of order instruction scheduling. This enables

    concurrent execution of kernels in applications whose computational graphs are

    input dependent and require fine-grained scheduling. We propose ACS-SW, a

    software-only open-source implementation of ACS and ACS-HW, a hardware-software

    cooperative implementation. ACS-HW further reduces synchronization overheads by

    reducing communication between the CPU and GPU. We evaluate ACS for deep RL

    simulation and dynamic DNNs on both real hardware and a GPU simulator. We

    demonstrate speedups of up to 2.19x (1.56x on average) by improving GPU

    utilization with concurrent kernel execution.'
  url: http://arxiv.org/abs/2401.12377v1
  keywords: ''
  document: "#### I. INTRODUCTION\n\nGraphics Processing Units (GPUs) today are commonly\
    \ used to accelerate a diverse set of applications, such as deep neural network\
    \ (DNN) processing, scientific computing, graphics, and cryptography. The massive\
    \ parallelism offered by GPUs enables efficient computations on large amounts\
    \ of data concurrently. However, we observe that certain important classes of\
    \ applications, such as simulation engines for deep reinforcement learning (RL)\
    \ [1]\u2013[5] and dynamic neural networks [6]\u2013[19], are unable to fully\
    \ utilize the significant compute capability that GPUs offer. This underutilization\
    \ is because these applications comprise a large number of small kernels, i.e.,\
    \ kernels with few thread blocks that are unable to fully saturate the GPU cores.\
    \ To understand the challenges in alleviating this underutilization, we evaluate\
    \ two important classes of applications and introduce their properties.\n\nSimulation\
    \ Engines for Deep RL. With reinforcement learning (RL) an agent (for example,\
    \ a robot) learns to perform tasks such as robotic locomotion, manipulation, and\
    \ navigation [20], [21] by trial and error from interactions with the environment.\
    \ Deep RL training involves using a DNN to learn policies that optimize for rewards\
    \ from data collected by interacting with a simulation environment. By leveraging\
    \ the benefits of DNNs, deep RL has recently gained widespread application for\
    \ many challenging and important tasks [20], [22]\u2013[28]. Despite leveraging\
    \ GPUs, a significant fraction of the deep RL runtime is the data collection phase\
    \ (up to 70% of the runtime), where physics simulations are used to generate training\
    \ data. We observe that these physics simulations heavily underutilize the GPU,\
    \ only achieving an occupancy of 34% on average. The underutilization is caused\
    \ by kernels that contain a small number of thread blocks that cannot fully utilize\
    \ the GPU. Programming larger kernels is impractical as each instance simulates\
    \ a different scenario, and large kernels would lead to thread divergence.\n\n\
    Dynamic DNNs. Several recent types of DNNs [6], [10], [12], [29] have emerged\
    \ as a promising approach to reduce inference latencies in resource-constrained\
    \ devices by reconfiguring/specializing the architecture based on the input to\
    \ the DNN. For example, InstaNAS [10] configures the network architecture at runtime\
    \ based on the input image. Our evaluations demonstrate that, while these architectures\
    \ require significantly fewer FLOPs and lower inference latencies, there is still\
    \ significant underutilization of GPU resources (achieving an occupancy of only\
    \ 39% on average). Similar to the simulation engines, we find that this underutilization\
    \ is caused by small kernels that do not fully utilize the GPU cores.\n\nGPU kernels\
    \ from such applications are typically executed *serially*, and thus the utilization\
    \ is determined by the size (i.e., the number of threads and thread blocks) of\
    \ the kernel. However, we observe that many kernels are independent and thus can\
    \ be executed concurrently. By concurrently executing independent kernels, we\
    \ can effectively improve GPU utilization and thus performance. Existing GPU architectures\
    \ allow for concurrent execution of kernels by using multiple command queues [30]\
    \ which are abstracted in software (such as CUDA Stream [31]), allowing the programmer\
    \ to identify and launch independent kernels in parallel. However, enabling concurrent\
    \ kernel execution for these applications is still a challenging task for two\
    \ major reasons.\n\nChallenge 1: Input-dependent computational graphs. For these\
    \ applications, the computational graph (i.e. the kernels to be executed and their\
    \ dependencies) is only resolved at runtime based on the input, and each input\
    \ or set of inputs leads to a different computational graph. This means that identifying\
    \ independent kernels to launch in parallel requires performing inter-kernel dependency\
    \ checks at runtime. These workloads have short running kernels that significantly\
    \ exacerbate the scheduling and dependency checking overheads, making this a challenging\
    \ problem to solve. Frameworks such as CUDA Graph [32] and AMD ATMI [33] allow\
    \ programmers to define the inter-kernel dependency information and construct\
    \ a directed acyclic graph (DAG) of kernels. These frameworks enable concurrent\
    \ kernel execution. However, when inter-kernel dependencies vary by input, we\
    \ must incur the significant latency of constructing the dependency graph and\
    \ scheduling independent kernels, every time the application is executed, significantly\
    \ increasing run time (\xA7 II-D and \xA7 VI).\n\nChallenge 2: Irregular inter-kernel\
    \ dependencies require fine-grain scheduling. The computational graph for a given\
    \ input tends to be highly irregular. In other words, the kernels cannot be easily\
    \ partitioned into independent streams and fine-grain scheduling is required to\
    \ expose inter-kernel parallelism. Thus, parallel execution of kernels requires\
    \ frequent synchronization to ensure correctness, leading to significant synchronization\
    \ overheads from communicating with the CPU and from kernel launches (\xA7 II-D).\n\
    \nTo address these challenges, our goal in this work is to enable kernel concurrency\
    \ with *(i)* lightweight scheduling and dependency checking of kernels that can\
    \ be performed at runtime and *(ii)* low overhead synchronization for scheduling\
    \ and kernel launch. To this end, we propose ACS, a new framework for Automatic\
    \ Concurrent Scheduling with two implementations: *(i)* ACS-SW, a software-only\
    \ mechanism to enable lightweight kernel scheduling at runtime and *(ii)* ACS-HW:\
    \ a hardware-software mechanism to further reduce synchronization overheads for\
    \ efficient kernel concurrency.\n\nThe key idea of ACS is to perform dependency\
    \ checks between sequentially launched kernels within a fixed window at runtime,\
    \ similar to out-of-order instruction scheduling. We refer to this window as the\
    \ *scheduling window*. When a kernel is inserted into the scheduling window, the\
    \ kernels that it is dependent on are identified. As kernels complete execution,\
    \ kernels in the scheduling window are marked ready based on the identified dependencies.\
    \ Ready kernels can then be concurrently launched as they have no more dependencies.\
    \ Since at any given time, only a small set of kernels are scheduled and tracked\
    \ (instead of the entire computational graph), this approach enables efficient\
    \ kernel parallelization and scheduling at runtime. To perform dependency checks\
    \ between kernels, ACS leverages annotations from the application that specify\
    \ the memory address ranges that are read/written by each kernel. This metadata\
    \ is then used to identify inter-kernel dependencies at runtime when kernels are\
    \ inserted into the scheduling window. Compared to prior approaches (\xA7 III-A),\
    \ this method alleviates the significant kernel scheduling and dependency-check\
    \ overheads for kernel parallelization.\n\nACS-SW implements the above out-of-order\
    \ runtime kernel scheduling in software as an application runtime system using\
    \ CUDA streams. ACS-SW however still incurs synchronization overheads from communication\
    \ with the CPU and kernel launch. On the other hand, ACS-HW implements the out-oforder\
    \ kernel scheduler in the GPU hardware and can alleviate the synchronization overheads.\
    \ We propose an efficient implementation of ACS-HW that reduces synchronization\
    \ and kernel overheads by reducing communication with the CPU.\n\nPrior works\
    \ such as task superscalar [34], carbon [35], TDM [36] and ADM [37] propose similar\
    \ out-of-order scheduling to leverage irregular parallelism between tasks in CPU\
    \ multiprocessors. However, the major challenge in CPUs is the latency of runtime\
    \ dependence checking. The primary bottleneck with GPUs is the latency for launch/signal\
    \ completion of kernels rather than dependence checking (\xA7 IV-D). ACS addresses\
    \ this challenge and provides an efficient approach to enable out-of-order kernel\
    \ scheduling in GPUs.\n\nWe demonstrate the effectiveness of ACS in improving\
    \ GPU utilization and thus performance for physics simulation workloads, a range\
    \ of dynamic neural networks, as well as static neural networks with small kernels.\
    \ We demonstrate an average speedup of up to 1.87\xD7 using our softwareonly approach\
    \ and up to 2.19\xD7 from the hardware-software implementation. The major contributions\
    \ of this work are:\n\n- We identify and characterize GPU underutilization as\
    \ a result of small GPU kernels in applications with inputdependent irregular\
    \ computational graphs, e.g., deepRL and dynamic DNNs.\n- We introduce ACS, a\
    \ runtime mechanism that improves GPU utilization by enabling concurrent execution\
    \ of GPU kernels with a lightweight dependency tracking and scheduling framework.\n\
    - We will provide an open-source software-only implementation of ACS that can\
    \ be used on real hardware to enable low overhead GPU kernel concurrency.\n- We\
    \ evaluate the effectiveness of ACS-SW and ACS-HW on a range of important GPU\
    \ applications and demonstrate significant speedups and improved GPU utilization.\n\
    \n#### II. MOTIVATION\n\n#### *A. Baseline GPU architecture*\n\nFigure 1 shows\
    \ an overview of the hardware model in modern GPU architectures [38]. The host\
    \ communicates with the command processor (CP) of the GPU via a virtual memory\
    \ region which is memory mapped to the GPU, accessible by the command processor.\
    \ This enables communication between the CPU and GPU through entries in the command\
    \ queue. The CPU transmits kernel launch packets to the GPU by writing them to\
    \ the user mode command queue. The CP is responsible for decoding and dispatching\
    \ the kernels in these command queues for execution. The CP accesses the command\
    \ queue and schedules the kernels at the head for execution. This ensures that\
    \ the kernels are dispatched for launch from these queues in order.\n\n![](_page_1_Figure_15.jpeg)\n\
    \nFig. 1: Scheduling kernels from multiple streamsw\n\n#### *B. Case Study 1:\
    \ Simulation Engines for Deep RL*\n\nDeep reinforcement learning (RL) has widely\
    \ gained attention as a promising approach to learning control policies in robotics\
    \ and dynamical systems for tasks such as locomotion on legged robots [1], [26],\
    \ [28], dexterous hand manipulation [21], autonomous driving [20], [24], and drone\
    \ control [22], [23], [25]. Deep RL involves training a DNN to learn policies\
    \ that maximize the reward, based on the actions that the agent (e.g., four-legged\
    \ robot) performs in a given environment. This training process requires data\
    \ from the agent interacting with a physics simulator. Typically, each training\
    \ step requires data from thousands of physics simulations. Recent works [1]\u2013\
    [5], [39] accelerate this data generation phase by leveraging GPUs. GPUs can accelerate\
    \ data generation by performing multiple simulations simultaneously and also parallelizing\
    \ within a single simulation. Hence this makes them an appropriate candidate workload\
    \ for GPU execution. Despite GPU acceleration, the simulation/data generation\
    \ phase is still the predominant computation in deep RL\u2014taking about 30\u2212\
    70% of training time depending on the complexity of the simulated environment.\
    \ Thus accelerating simulation engines is critical for deep RL performance.\n\n\
    To evaluate the efficiency of physics simulations, we analyzed a set of physics\
    \ simulations with different environments on a GPU (parameters in \xA7 V) with\
    \ the widely used Brax [1] framework. We evaluate the utilization of the GPU by\
    \ measuring achieved occupancy (average ratio of active warps to the maximum supported),\
    \ depicted in Fig. 2. We find that as much as 65% of the GPU cores are underutilized\
    \ on average (on both GPUs). To evaluate the cause of this underutilization, we\
    \ analyze the number of kernel launches required to generate one batch of training\
    \ data in Fig. 3. We also present the average number of CTAs per kernel in Fig.\
    \ 4 and depict the distribution of kernel sizes observed for the ant environment\
    \ in Fig. 5. We observe that physics simulations in our evaluations generate a\
    \ large number of *small* kernels that have few threads and CTAs. This is a fundamental\
    \ problem because the simulation engine cannot be efficiently mapped into large\
    \ kernels as the different threads will likely diverge in the execution path.\
    \ This is because each thread typically simulates a different scenario in the\
    \ environment. Thus the application is instead programmed as a large number of\
    \ short-running kernels. This phenomenon has also been observed by recent works\
    \ [39], [40].\n\n![](_page_2_Figure_3.jpeg)\n\nFig. 2: Simulation engines: Achieved\
    \ occupancy.\n\n![](_page_2_Figure_5.jpeg)\n\n**0-9 10-49 50-199 200+** Fig. 5:\
    \ Kernel size distribution for the ant environment power budgets as direct deployment\
    \ of large neural network architectures on these devices leads to high-inference\
    \ times. Automated DNN architecture design (neural architecture search) is a promising\
    \ approach to generate faster neural network architectures while retaining or\
    \ improving accuracy [41]\u2013[44]. These optimized architectures tend to have\
    \ irregular elaborate connections between convolution operations. Fig. 6a depicts\
    \ an example DNN with irregular structure. Additionally, an emerging trend in\
    \ recent research [29] shows that *dynamic inference models* [6]\u2013[8], [10],\
    \ [13]\u2013[19], [45]\u2013[48] are very promising to significantly reduce inference\
    \ latency and FLOPs. With these dynamic inference models, the path of execution\
    \ through the network is determined by the *input*. Thus, the computational graph\
    \ is not known ahead of time. For example, Fig. 6b shows an example CNN model\
    \ with different paths of execution based on the input [10].\n\nSimilar to \xA7\
    \ II-B, we evaluate the efficiency of these workloads on a GPU (an NVIDIA RTX\
    \ 3060 and an NVIDIA RTX 4090) and depict the resulting utilization in Fig. 7\
    \ (evaluation and workload settings are in \xA7 V). We find that the total achieved\
    \ occupancy is around 39% in the InstaNAS-A [10] workload for both GPUs. Similar\
    \ to the simulation engines, we root cause this underutilization to the existence\
    \ of a large number of small kernels, as depicted in Fig. 8, where a large fraction\
    \ of the kernels have fewer than 200 CTAs. Thus, these small kernels are unable\
    \ to fully utilize the GPU. In these workloads, the small kernels are due to convolution\
    \ layers that were optimized for fewer FLOPs with smaller filters. avg 3x3 max\
    \ 3x3\n\n![](_page_2_Figure_8.jpeg)\n\n# *C. Case Study 2: DNNs with dynamic irregular\
    \ graphs*\n\nRecent research has extensively investigated specialized DNNs for\
    \ edge devices with limited compute resources and\n\n#### *D. Key Observations*\n\
    \nWhile small-sized kernels lead to underutilization, we observe that there are\
    \ typically many kernels that can be executed *concurrently*. Thus we can improve\
    \ GPU utilization and reduce\n\n![](_page_3_Figure_0.jpeg)\n\nFig. 8: Kernel size\
    \ distribution (in CTAs) for InstaNAS-A [10]\n\nruntimes by identifying independent\
    \ kernels and scheduling them for concurrent execution. However, this is a challenging\
    \ task for these classes of applications for the following reasons.\n\n(1) Input-dependent\
    \ kernel dependencies. The computational graph, and hence, the dependencies between\
    \ kernels are only determined at *runtime* for each input. For example, with the\
    \ instance-aware dynamic DNNs [6]\u2013[8], [10] described in \xA7 II-C, for the\
    \ classification inference task, the computational graph is different for each\
    \ image. As a result, the determination of kernel dependencies and scheduling\
    \ of kernels for the entire computational graph needs to be done for *each input*.\
    \ This adds significant latencies to the runtime.\n\nCUDA Graphs [32] and AMD\
    \ ATMI [33] are software frameworks that allow developers to specify dependencies\
    \ between different kernels as edges of a directed acyclic graph (DAG). The challenge\
    \ with this approach is that the DAG needs to be constructed in full (with dependencies,\
    \ kernel launches, and barriers determined) before the application is executed\
    \ on the GPU, *for each input*. This process adds high latency in compiling the\
    \ complete dependency information. We perform an experiment to measure the DAG\
    \ construction and launch time on Brax [1] simulation engine (\xA7 V) compared\
    \ to the program execution time, shown in Fig. 9. We observe that the time taken\
    \ to construct the graph is exceedingly high (average of 47% of overall execution\
    \ time).\n\n![](_page_3_Figure_5.jpeg)\n\nFig. 9: DAG construction time as % of\
    \ execution time\n\nSimilarly, recent works for DNNs [50]\u2013[52] perform kernel\
    \ scheduling, fusion, or parallelization for better GPU utilization. These works,\
    \ for example, partition the computational graph into independent sub-graphs that\
    \ are scheduled into multiple streams. However, this scheduling and partitioning\
    \ is too time-consuming to be done for each input at runtime and thus cannot be\
    \ applied to these classes of workloads.\n\n(2) Irregular kernel dependencies.\
    \ These classes of applications have *irregular* computational graphs that are\
    \ challenging to easily partition into CUDA streams (\xA7 II-C). Popular deep\
    \ learning frameworks [53], [54] use a single stream by default. The stream abstraction\
    \ works best if the entire graph can be partitioned into independent streams of\
    \ kernels. However, these graphs with irregular dependencies would require finegrained\
    \ scheduling and heavy use of synchronization (e.g., cudaDeviceSynchronize and\
    \ cudaStreamSynchronize) when parallelizing using CUDA streams. This synchronization\
    \ may lead to large overheads as it requires communication between the GPU and\
    \ CPU. Fig. 10 depicts the different overheads when CUDA streams are used for\
    \ fine-grained scheduling with irregular graphs: kernel launch overheads 1 , CPU\
    \ execution overheads 2 and the synchronization overheads 3 . Based on our profiling,\
    \ the synchronization and launch overheads vary between 5-20us. CUDA Graphs [32]\
    \ and ATMI [33] can eliminate the synchronization and kernel launch overhead.\
    \ However, for input-dependent graphs, as demonstrated in (1), this benefit is\
    \ lost due to DAG construction overheads.\n\n![](_page_3_Figure_10.jpeg)\n\nFig.\
    \ 10: Kernel launch and synchronization overheads\n\n# III. APPROACH\n\nOur goal\
    \ in this work is to design a framework that enables efficient concurrent execution\
    \ of GPU kernels *(i)* whose computational graph may only be known at runtime,\
    \ *(ii)* without incurring significant synchronization overheads. To this end,\
    \ we introduce ACS, a new framework that concurrently schedules independent kernels\
    \ with a lightweight runtime mechanism.\n\n#### *A. Prior Mechanisms*\n\nWe consider\
    \ the baseline GPU architecture as described in \xA7 II-A. The GPU runtime can\
    \ launch kernels into different streams. These streams are mapped to one of the\
    \ command queues in the device-mapped memory of the GPU. The command processor\
    \ schedules kernels at the head of these queues concurrently, thus enabling concurrent\
    \ kernel execution. However, neither the command processor nor the kernel launch\
    \ packets in the command queues have information on inter-kernel data dependencies.\
    \ Kernels in different queues are assumed to be independent of each other and\
    \ all kernels within the same queue are executed in order. Hence, in order to\
    \ leverage parallelism in kernel executions, the task of checking inter-kernel\
    \ dependencies and determining the kernels which can execute concurrently (and\
    \ thus scheduling into different queues) *has to be done by the host application.*\
    \ However, this is a problem, as this adds significant dependency-checking/scheduling\
    \ latency to the run time. It also requires communication with the host (through\
    \ a synchronization routine) to be performed each time a kernel completes execution,\
    \ adding to the overhead. Several prior works describe approaches to efficiently\
    \ schedule kernels into multiple streams. Fig. 11 depicts approaches to scheduling\
    \ a computational graph (Fig. 11a). Fig. 11b is the baseline approach used by\
    \ many existing frameworks [53], [54], where a single CUDA stream is used to execute\
    \ all kernels serially. This approach leads to underutilization (\xA7 II-C). Fig.\
    \ 11c shows prior works [50], [51] that use the computational graph to identify\
    \ independent kernels and the *entire graph* is scheduled ahead of time into multiple\
    \ CUDA streams. However, this fine-grained scheduling and synchronization leads\
    \ to large overheads.\n\n![](_page_4_Figure_1.jpeg)\n\n![](_page_4_Figure_2.jpeg)\n\
    \nOne way to avoid using a device-level synchronization (like cudaDeviceSynchronize)\
    \ and enable asynchronous execution of kernels without communication with the\
    \ CPU is to use events provided by the CUDA stream management API. Events serve\
    \ as signaling mechanisms to indicate the occurrence of specific operations in\
    \ a stream. This allows synchronization between kernels across streams through\
    \ the cudaStreamWaitEvent API, facilitating asynchronous kernel execution without\
    \ blocking the host. By strategically placing events and using cudaStreamWaitEvent,\
    \ it is possible to orchestrate the order in which kernels are executed on the\
    \ GPU without communication with host. However, this approach still requires deriving\
    \ dependencies between all kernels beforehand, and thus incurs significant scheduling\
    \ overhead.\n\nAnother set of approaches [51], [52], [55], define static dependencies\
    \ between kernels as a DAG, which is then scheduled with DAG frameworks (CUDA\
    \ Graph [32]/ATMI [33]). These approaches cannot be applied to input-dependent\
    \ computation graphs, as constructing the entire computational graph is too time-consuming\
    \ to be done at runtime. To convey the DAG information, ATMI sends barrier packets\
    \ [56] along with kernel launch packets to the command queue. A barrier packet\
    \ [57] is a 64-byte data packet that contains id information about a kernel and\
    \ a set of kernels that depend on it. This packet can be inserted into the command\
    \ queue by the device runtime. The barrier packet blocks the launching of dependent\
    \ kernels until the independent kernel completes execution. The barrier packet\
    \ however does not contain any information regarding the current status of the\
    \ executing kernels in the GPU and thus cannot perform any additional runtime\
    \ reordering of kernels. It simply follows the dependencies already specified\
    \ by the DAG. While it is possible to devise a framework that dynamically launches\
    \ barrier packets and launch commands onto the GPU command queue in memory, this\
    \ would require hardware support and would still incur synchronization overheads\
    \ with the CPU. Our approach is specifically designed to mitigate this scheduling\
    \ cost by avoiding direct communication from the GPU to the CPU, thereby reducing\
    \ potential overheads.\n\nPersistent threads (PT) eliminate the scheduling and\
    \ launch overheads but are only effective when all kernels are homogeneous [58]\
    \ . CUDA dynamic parallelism [59] (CDP) or AMD's device enqueue [60] (DE) enables\
    \ parent kernels to launch child kernels, only allowing data dependencies between\
    \ one parent and its children. These workloads however involve kernels that depend\
    \ on multiple kernels, and it is an open problem how to use CDP for these types\
    \ of dependencies.\n\nK6 K5 K4 K3 K2 K1 N We summarize different approaches for\
    \ parallel kernel scheduling in Table I, in terms of applicability (whether inputdependent\
    \ irregular workloads can be effectively mapped), synchronization/launch overheads\
    \ and preparation overhead (resolving dependencies, constructing, and scheduling\
    \ the computational graph).\n\n| Method                  |   | Applicability Sync+Launch\
    \ Preparation<br>Overhead | Overhead |\n|-------------------------|---|---------------------------------------------------|----------|\n\
    | Multi-Stream [50], [51] | \u2713 | x                                       \
    \          | \u2713        |\n| DAG [32], [33], [52]    | \u2713 | \u2713    \
    \                                             | x        |\n| PT [58], [61], [62]\
    \     | x | \u2713                                                 | \u2713  \
    \      |\n| CDP [59] / DE [60]      | x | x                                  \
    \               | \u2713        |\n| ACS-SW (Our approach)   | \u2713 | x    \
    \                                             | \u2713        |\n| ACS-HW (Our\
    \ approach)   | \u2713 | \u2713                                              \
    \   | \u2713        |\n\nTABLE I: Comparison of ACS to other scheduling frameworks\n\
    \n#### *B. Key Idea of ACS*\n\nWith ACS, the key idea is to instead perform the\
    \ dependence checking and scheduling within a small window of kernels at *runtime*\
    \ similar to out-of-order instruction scheduling. We perform this scheduling over\
    \ a single command queue (or a single initialized stream). Fig. 12a depicts out-of-order\
    \ kernel dispatch with ACS. Fig. 12b shows the corresponding high-level hardware\
    \ modifications for ACS. A fixed number of kernels in the original stream (scheduling\
    \ window 1 ) are evaluated for dependencies. When a kernel completes execution,\
    \ we evaluate which kernels within the scheduling window are now ready for execution\
    \ 2 . All such kernels are marked ready and can be scheduled concurrently.\n\n\
    ![](_page_4_Figure_12.jpeg)\n\n(a) Out-of-order kernel dispatch from the scheduling\
    \ window (b) CP scheduling kernels in out of order manner\n\nFig. 12: ACS: Runtime\
    \ out-of-order kernel scheduling\n\nWe propose two implementations of ACS: ACS-SW,\
    \ a SWonly approach and ACS-HW, a hardware-software cooperative mechanism, which\
    \ we describe in the following sections. ACS-SW emulates the out-of-order kernel\
    \ scheduling mechanism by scheduling independent kernels into multiple streams\
    \ and can be implemented with purely software changes, however the hardware support\
    \ in ACS-HW is more efficient as it also alleviates synchronization overheads.\n\
    \n#### *C. Design Overview*\n\nTo design ACS to perform the runtime kernel scheduling\
    \ as depicted in Fig. 12a, we need *(i)* a mechanism to determine inter-kernel\
    \ dependencies in the scheduling window; *(ii)* to identify kernels that are ready\
    \ for execution; and *(iii)* alleviate synchronization and kernel launch overheads.\n\
    \nDetermining inter-kernel dependencies. In order to determine dependencies between\
    \ kernels, the application adds additional metadata to each kernel invocation.\
    \ This metadata defines the range of global memory addresses that are written\
    \ to and read from by each kernel. This metadata is provided to ACS by using a\
    \ kernel wrapper (described in \xA7 IV-B) and can be defined by the programmer,\
    \ library-writer, or compilation tools. By checking for overlaps between read\
    \ segments and write segments, we determine dependencies between kernels. The\
    \ kernel wrapper defines the pointers to the read and write data segments (start\\\
    _addr) along with the size of the segments (Fig. 13). The actual virtual addresses\
    \ associated with the pointers are resolved just before kernel launch in order\
    \ to perform the dependence checks (\xA7 IV-A). We refer to these memory ranges\
    \ as read\\_segments and write\\_segments.\n\n![](_page_5_Figure_3.jpeg)\n\nFig.\
    \ 13: Memory regions written to/accessed by the kernel\n\nTracking kernel state\
    \ at runtime. Fig. 14 depicts the scheduling window ( 1 ), with the additional\
    \ state required for scheduling. The kernels in the window can be ready, pending,\
    \ or executing ( 3 ). Kernels in the scheduling window become ready for launch\
    \ (ready) when the kernels it is dependent on (referred to as *upstream* kernels\
    \ 2 ) complete execution. For each kernel in the scheduling window, we track a\
    \ list of the corresponding upstream kernels. The upstream kernels are determined\
    \ using the above dependency checks when inserting into the scheduling window.\
    \ When the upstream list is empty, the kernel is marked ready for execution. After\
    \ each kernel completes execution, the upstream list is updated for all kernels\
    \ in the scheduling window. For ACS-SW, these checks are performed in the software\
    \ runtime system (\xA7 IV-B), and for ACS-HW, we implement them in hardware (\xA7\
    \ IV-C).\n\n![](_page_5_Figure_6.jpeg)\n\nFig. 14: Kernels in the scheduling window\
    \ with their state and corresponding upstream kernels (i.e., dependencies)\n\n\
    Eliminating CPU synchronization overheads. In order to eliminate synchronization\
    \ and kernel launch overheads resulting from communication between the CPU and\
    \ GPU, we implement the scheduling window in the GPU hardware in ACS-HW. We design\
    \ an efficient implementation of ACS-HW that reduces communication with the CPU.\
    \ The management of the scheduling window is done entirely in hardware, including\
    \ the determination of ready kernels. Similarly, once a kernel completes execution,\
    \ the scheduling window is updated without requiring synchronization with the\
    \ CPU.\n\n#### *D. Mechanism Walkthrough*\n\nstart\\_addr size Fig. 15 depicts\
    \ a high level walkthrough of ACS. For each GPU kernel invoked by the application\
    \ 1 , the read and write segments are resolved (detailed in \xA7 IV-A). All invoked\
    \ kernels along with the corresponding read/write segments are entered into the\
    \ input FIFO queue to await scheduling 2 . Kernels are then added to the fixed\
    \ size scheduling window in a FIFO manner 3 . When the kernel enters the scheduling\
    \ window 4 , the write segments of the current kernel are compared against read\
    \ and write segments of all kernels in the scheduling window. The kernels with\
    \ overlap are added to the corresponding upstream kernel list and are marked pending.\
    \ When an executing kernel completes execution, all corresponding upstream kernel\
    \ lists are updated. Any kernel that has an empty list is marked ready for the\
    \ scheduler to launch. input FIFO queue <sup>2</sup>\n\n![](_page_5_Figure_12.jpeg)\n\
    \nFig. 15: High level overview of ACS\n\n#### IV. DETAILED DESIGN\n\n#### *A.\
    \ ACS Kernel Wrappers*\n\nIn order to perform runtime dependency checks, the application\
    \ defines the read/write segments for each kernel. These segments are defined\
    \ using a kernel wrapper, ACS\\_wrapper (defined in Fig. 16). Since virtual addresses\
    \ can only be resolved at runtime, the programmer instead defines a function get\\\
    _addresses which populates the \\_\\_read\\_segments\\_\\_ and \\_\\_write\\_segments\\\
    _\\_ lists (lines 6 and 7 in Fig. 16). The get\\_addresses function takes the\
    \ kernel's launch arguments as the input arguments (lines 12 to 15). These arguments\
    \ are then used to compute the read/write segments.\n\nscheduling window 1 K2\
    \ none ready K1 none executing Just before kernel launch, the CUDA runtime calls\
    \ the get\\_addresses function. At this point, the \\_\\_read\\_segments\\_\\\
    _ and \\_\\_write\\_segments\\_\\_ lists are populated with the resolved virtual\
    \ addresses. In our implementation of ACS-SW, since the CUDA drivers are closed-source,\
    \ we implement an intermediate user-level kernel launch function that calls the\
    \ get\\_addresses function instead. Fig. 17 depicts an example implementation\
    \ of the get\\_addresses function. ACS assumes that the programmer or the kernel\
    \ library provider has knowledge of the memory regions accessed by the kernel\
    \ from the kernel function prototype. For a wide range of commonly used kernels,\
    \ such as matrix multiplication, convolution, addition, etc., which operate on\
    \ data stored as contiguous regions in memory, this task is straightforward. Additionally,\
    \ the get\\_address function can be obtained using a static binary analysis tool\
    \ like GPUOcelot [63]. However, in situations where it is not possible to determine\
    \ the range of memory accessed by the kernel (for example, indirect memory accesses),\
    \ our approach assumes that the entire GPU memory may be accessed by the kernel.\n\
    \n```\n1 struct ACE_wrapper {\n2 //list of read,write segments defined as\n3 //[{start_adr1,size1},{start_adr2,size2}..]\n\
    4 list __read_segments__;\n5 list __write_segments__;\n6 // function which gets\
    \ called at kernel\n7 // launch to populate read,write segments\n8 void get_addresses(\n\
    9 dim3 blocks, dim3 threads, ...\n10 );\n11 // function declaration of the kernel\n\
    12 static __global__ void kernel(...);\n13 };\n```\nFig. 16: The ACS\\_wrapper\
    \ definition\n\n```\n1 // get address function for matrix multiply\n2 // input\
    \ matrices: input1 (mxn), input2(nxk)\n3 // output matrix: output(mxk)\n4 void\
    \ ACE_wrapper::get_addresses(\n5 dim3 blocks, dim3 threads,\n6 int* input1, int*\
    \ input2, int* output1,\n7 int m, int n, int k) {\n8 // input1 reads m*n elements\n\
    9 // input2 reads n*k elements\n10 __read_segments__ = {\n11 {(void*)input1, m*n*sizeof(int)},\n\
    12 {(void*)input2, n*k*sizeof(int)}\n13 };\n14 // output reads m*k elements\n\
    15 __write_segments__ = {\n16 {(void*)output, m*k*sizeof(int)},\n17 };\n18 }\n\
    ```\nFig. 17: Example: get\\_addresses function\n\n### *B. ACS-SW Design*\n\n\
    ACS-SW is implemented as a user-level runtime that is called by the application.\
    \ The functionalities of ACS-SW are performed by multiple independent threads\
    \ that are launched simultaneously. The ACS-SW runtime performs two major tasks:\
    \ *(i)* implementing and maintaining the scheduling window (window module); and\
    \ *(ii)* scheduling kernels ready for execution (scheduling module).\n\n*1) The\
    \ window module:* The window module is implemented as a separate thread that manages\
    \ the input FIFO queue and the scheduling window. All the functionalities of the\
    \ scheduling window, dependency tracking, and state management are performed in\
    \ software within this module. This module is called in two ways: First, when\
    \ a kernel is invoked by the application thread, this module is called and the\
    \ kernel is inserted into the input queue. Second, the scheduler module (implemented\
    \ as a separate thread(s)) calls the window module when a kernel completes execution.\
    \ At this point, the state of upstream lists is updated and the kernel is removed\
    \ from the scheduling window. The window module constantly polls the input queue\
    \ and the scheduling window. When there is a vacancy in the scheduling window\
    \ and a pending kernel in the input queue, the kernel is moved into the scheduling\
    \ window. At this point, the window module performs the necessary dependency checks\
    \ and bookkeeping. Algorithm 1 describes how the dependency check is performed.\n\
    \n|  |  | Algorithm 1 Dependency check algorithm |  |  |\n|--|--|----------------------------------------|--|--|\n\
    |--|--|----------------------------------------|--|--|\n\n| Input: rslist1, wslist1,\
    \ wslist2<br>\u25B7 RW segments of scheduling window   |\n|--------------------------------------------------------------------------|\n\
    | kernel, w-segment of kernel in inputFIFO                                 |\n\
    | Output: is dependent<br>\u25B7                                             \
    \   |\n| 1: is dependent = false<br>\u25B7 initial state of is dependent     \
    \          |\n| S<br>2: rwslist1 \u2190 wslist1<br>rslist1<br>\u25B7 Read+Write\
    \ segments           |\n| 3: for each segment1 in rwslist1 do<br>\u25B7 Test for\
    \ every pair of segments |\n| for each ws2 in wslist2 do<br>4:               \
    \                          |\n| \u25B7 get start and end virtual memory addresses\
    \                             |\n| start1 \u2190 segment1.start<br>5:        \
    \                                    |\n| end1 \u2190 segment1.start + segment1.size<br>6:\
    \                              |\n| start2 \u2190 ws2.start<br>7:            \
    \                                     |\n| 8:<br>end2 \u2190 ws2.start + ws2.size\
    \                                        |\n| \u25B7 check overlaps between start\
    \ and end addresses                         |\n| 9:<br>if start1 < end2 and end1\
    \ > start2 then                            |\n| is dependent = true<br>\u25B7\
    <br>10:                                          |\n| 11:<br>end if          \
    \                                                  |\n| 12:<br>end for each  \
    \                                                    |\n| 13: end for each   \
    \                                                      |\n\n*2) The scheduler\
    \ module:* This module schedules and launches ready kernels for execution. This\
    \ module is implemented as a configurable fixed number of threads, each of which\
    \ launches kernels into an independent CUDA stream for concurrent execution, as\
    \ depicted in Fig. 18. Each stream contains only one kernel at any given time.\
    \ Threads with empty streams poll the scheduling window for a ready kernel 1 ,\
    \ which is then launched in its CUDA stream 2 . The thread then waits for the\
    \ kernel to complete execution using the StreamSync primitive 3 . Once the kernel\
    \ completes execution, the thread calls the window module as described above.\
    \ This algorithm is described in Algorithm 2. Thread 1 Thread 2\n\n![](_page_6_Figure_12.jpeg)\n\
    \nFig. 18: ACS-SW: The scheduler module\n\n|    | Algorithm 2 The scheduler module\
    \ in software |                                      |\n|----|----------------------------------------------|--------------------------------------|\n\
    |    | Input: SchedulingWindow SW, stream id        |                        \
    \              |\n|    | 1: while notstop() do                        | \u25B7\
    \ poll for kernels until stop signal |\n| 2: | ACQUIRE LOCK(SW)              \
    \               |                                      |\n| 3: | if SW.ready.exists(\
    \ )then                    | \u25B7 check ready kernels                |\n| 4:\
    \ | kernel \u2190 SW.ready.pop()                      | \u25B7 get ready kernel\
    \                   |\n| 5: | end if                                       | \
    \                                     |\n| 6: | RELEASE LOCK(SW)             \
    \                |                                      |\n| 7: | LAUNCH(kernel,\
    \ stream id)                    | \u25B7 launch kernel                      |\n\
    | 8: | STREAM SYNC(stream id)                       | \u25B7 wait for completion\
    \                |\n|    | 9: end while                                 |    \
    \                                  |\n\n#### *C. ACS-HW Design*\n\nWhile ACS-SW\
    \ enables concurrent execution of kernels and can be fully realized in software,\
    \ it still incurs overheads from *(i)* synchronization with the CPU when a kernel\
    \ completes execution, i.e., the StreamSync primitive that blocks the scheduler\
    \ module thread; and *(ii)* the kernel launch overhead when the scheduler module\
    \ launches a kernel in the CPU. ACS-HW is designed to alleviate these overheads\
    \ with hardware support for kernel scheduling in the GPU.\n\nFig. 19 depicts an\
    \ overview of ACS-HW. ACS-HW comprises a software runtime system similar to ACS-SW\
    \ that maintains an input FIFO queue containing the kernels that were invoked\
    \ by the application 1 . The scheduling window and its management are however\
    \ implemented in hardware on the GPU side 2 . The input queue is essentially implemented\
    \ as a CUDA stream that dispatches kernels to the GPU. In addition to the input\
    \ FIFO queue, the software runtime also maintains a list of kernels in the GPU's\
    \ scheduling window, which we call the scheduled\\_list 3 . To avoid frequent\
    \ synchronization between the CPU and GPU, we allow this list to be stale. Before\
    \ a kernel is inserted into the scheduling window, the software runtime performs\
    \ dependency checks with the scheduled\\_list to determine the upstream kernels.\
    \ Note that since the scheduled\\_list may be stale, this upstream list needs\
    \ to be further updated before insertion into the scheduling window (discussed\
    \ below).\n\n![](_page_7_Figure_2.jpeg)\n\nFig. 19: ACS-HW: Design overview\n\n\
    The hardware component 4 consists of two modules: *(i)* the scheduling window\
    \ and *(ii)* the upstream load module.\n\nThe hardware scheduling window structure\
    \ is depicted in Fig. 20 and comprises a fixed number of slots (N) 1 . Each slot\
    \ contains an 8-bit kernel identifier and (N-1) 8-bit upstream kernel identifiers\
    \ that are implemented with SRAM 2 . Each slot of the SRAM module is implemented\
    \ as a single bank of SRAM, contaning N-1 fully associated units to store upstream\
    \ kernel identifiers. These upstream identifiers are used to determine when a\
    \ kernel is ready. An additional two bits are used to identify the state of each\
    \ kernel (i.e., ready, pending, and executing). When a kernel completes execution,\
    \ the upstream identifiers are updated and the corresponding state of each kernel\
    \ is updated. The completed kernel is also removed from the scheduling window.\
    \ Any kernels that are now ready are then dispatched to the GPU's kernel dispatch\
    \ unit for execution 3 .\n\nThe upstream load module is responsible for refining\
    \ the upstream list provided by the CPU which may be stale in two ways. It may\
    \ contain kernels that have (1) already completed execution and (2) may miss long-running\
    \ kernels that are still executing. The first case is handled by the upstream\
    \ module by checking against a list of kernels in the scheduling window 4 . The\
    \ second case is avoided by ensuring that the scheduled\\_list (of size M) in\
    \ the CPU never misses kernels that are still executing. The upstream load module\n\
    \n![](_page_7_Figure_7.jpeg)\n\nFig. 20: HW scheduling window and upstream load\
    \ module tracks the oldest scheduled kernel 5 . If the number of newer kernels\
    \ exceeds M (size of the scheduled\\_list), this module blocks the insertion of\
    \ more kernels from the CPU 6 .\n\n#### *D. ACS Overheads*\n\n(1) Hardware area\
    \ overhead. ACS-HW introduces the hardware scheduling window which contains N\
    \ slots, where N is the size of the scheduling window. Each slot contains N kernel\
    \ ids of upstream data of 8 bytes each and 2 bits for status. Assuming a scheduling\
    \ window of length N = 32, we require 1KB of SRAM for the scheduling module (for\
    \ the entire GPU). The upstream module keeps track of the oldest executing kernel\
    \ with an 8-bit\n\nGPU K1 K3 window <sup>2</sup> 4 load module (2) Storage overheads.\
    \ The read and write segments that are saved as metadata in the input FIFO and\
    \ the scheduled\\_list by the software runtime in the CPU require memory storage.\
    \ Each read and write segment requires 48 bits to hold the start addresses and\
    \ the size.\n\n> (3) Mechanism latencies. ACS-HW requires updating all upstream\
    \ kernels in each slot of the scheduling window every time a kernel completes\
    \ execution. ACS-HW updates each slot in N-1 cycles (where N is the size of the\
    \ scheduling window). Additionally, ACS-HW requires N cycles to insert a kernel\
    \ ID with its upstream kernel IDs into the scheduling window. For a scheduling\
    \ window of size 64, this operation adds 64 cycles (about 50-100ns) overhead to\
    \ dispatch a ready kernel for launch. Thus, ACS-HW adds negligible runtime to\
    \ the application compared to the baseline kernel launch overhead (in the order\
    \ of a few microseconds).\n\n> (4) Dependency checking overheads To determine\
    \ the list of upstream kernels, the CPU checks for overlaps between the write\
    \ segments of the kernel in the input queue and the read-write segments of the\
    \ kernels in the scheduled\\_list. As the scheduled\\_list can fit completely\
    \ into the cache (4KB), dependency-checking is compute-bound and dependent on\
    \ the number of read and write segments. Table II presents the time required to\
    \ do dependence checking. For a processor with P execution units, effective utilization\
    \ requires dependency checks to be performed in no more than T /P, where T is\
    \ the task execution time [34], [36]. We estimate T /P to be around 4us, which\
    \ is much more than the dependency check latency.\n\n#### V. METHODOLOGY\n\nWe\
    \ evaluate ACS-SW on a real hardware setup with an Intel Core i7 11700K CPU (Table\
    \ III) and an NVIDIA RTX3060\n\n| Window<br>size | Number of<br>RW-segments |\
    \ Dependency<br>check time |\n|----------------|--------------------------|--------------------------|\n\
    | 16             | 6<br>10                  | 410ns<br>700ns           |\n| 32\
    \             | 6<br>10                  | 510ns<br>1640ns          |\n\nTABLE\
    \ II: Dependency checking overhead analysis\n\nGPU (Table IV). We model ACS-HW\
    \ on GPUs using the Accel-Sim simulator [64], configured with parameters of RTX3070\
    \ (Table V). We use AccelWattch [64] to model GPU power. We choose a scheduling\
    \ window size of 32.\n\n| CPU 3.6GHz, OOO 4-wide dispatch window, 32 entry LSQ\
    \    |\n|---------------------------------------------------------|\n| L1D + L1I\
    \ Cache 32KB, 4 way LRU, 1 cycle; 64 Byte line; |\n| L2 Cache 256KB, 8 way LRU,\
    \ 4 cycle; 64 Byte line;       |\n| L3 Cache 1MB, 16 way LRU, 20 cycle; 64 Byte\
    \ line;       |\n| DRAM 2-channel; 16-bank; open-row policy, 4GB DDR4      |\n\
    | TABLE III: CPU system configuration                     |\n\n| Shader core 28\
    \ SMs, 1.3GHz; 2 schedulers per SM             |\n|-------------------------------------------------------------|\n\
    | SM Resources 32768 Registers, 32KB Shared memory, 128KB L1D |\n| DRAM 2-channel;\
    \ 16-bank; open-row policy, 12GB DDR4         |\n| TABLE IV: GPU system configuration\
    \                          |\n\n| Shader core 46 SMs, 1.4GHz; 4 schedulers per\
    \ SM             |\n|-------------------------------------------------------------|\n\
    | SM Resources 32768 Registers, 32KB Shared memory, 128KB L1D |\n| DRAM 2-channel;\
    \ 16-bank; open-row policy, 16GB DDR4         |\n| TABLE V: Simulated GPU configuration\
    \                        |\n\nWorkloads. We evaluate ACS using:\n\n(1) Deep RL\
    \ physics simulations. Brax [1] is a GPU accelerated simulation engine for control\
    \ tasks in reinforcement learning. We evaluate ACS with the Ant (ant), Grasp (grasp),\
    \ Humanoid (human), Cheetah (ct), and Walker2d (w2d) simulation environments.\
    \ These environments are Mu-JoCo [65] simulations for training RL agents to perform\
    \ a specific task. For example, ant contains a 3d robot (the agent) with one torso\
    \ and 4 legs, each with a knee joint, and the goal is to move in a particular\
    \ direction by controlling its legs.\n\n(2) Dynamic DNNs. We evaluate our approach\
    \ for 3 dynamic DNN workloads: InstaNAS [10] (I-NAS) is a dynamic CNN for image\
    \ classification. We evaluate our approach using the InstaNAS-A architecture on\
    \ the CIFAR10 dataset. Dynamic routing [12] (DR) is a DNN trained for semantic\
    \ segmentation of images. We evaluate our approach on the Dynamic-A 16 layer architecture\
    \ using the Cityscapes dataset [66]. Conditional Convolution [46] (CC) is a mixtureof-experts\
    \ CNN model for image classification where the weights of the convolutions are\
    \ computed at runtime. We evaluate the version of Conditional Convolution with\
    \ 4 experts that uses an efficientnet b4 [67] network as the backbone. All three\
    \ dynamic DNNs are designed for a batch size of 1 and the input image defines\
    \ the DNN architecture. We use Pytorch [54] implementations.\n\n(3) Static DNNs.\
    \ CNN architectures optimized for low inference latency using neural architecture\
    \ search (NAS): NASNet [41] (NASNet), AmoebaNet [42] (Amoeba), SqueezeNet [68]\
    \ (Squeeze), and RandomWire [44] (RW). These CNNs have highly irregular structures\
    \ with many small kernels. We evaluate ACS with a batch size of 1 on CIFAR10.\n\
    \n#### VI. EVALUATION\n\nWe evaluate ACS using three designs: *(i)* Baseline:\
    \ cuDNN implementation (for DNNs) and a jax implementation [1] (for deep RL simulation),\
    \ both using CUDA streams. *(ii)* ACS-SW: Our software-only mechanism is evaluated\
    \ on real hardware. *(iii)* ACS-SW-Sim: Our software-only mechanism evaluated\
    \ on the GPU simulator. We also include these results to compare against ACS-HW.\
    \ *(iv)* ACS-HW: Our hardware-software cooperative mechanism evaluated on the\
    \ GPU simulator. *(v)* CUDAGraph: Framework where the inter-kernel dependencies\
    \ are prepared on the CPU as a directed acyclic graph and sent to the GPU ahead\
    \ of time. We only present ACS-SW results for the deep RL workloads as the dynamic\
    \ and static DNNs heavily use CuDNN libraries that do not currently allow modifications\
    \ to make use of different CUDA streams. We instead model the same effect with\
    \ ACS-SW-Sim.\n\n#### *A. Deep RL Physics Simulations*\n\nFig. 21 depicts the\
    \ runtimes for the generation of a single batch of training data from different\
    \ simulation environments using ACS-SW, normalized to the baseline approach.\n\
    \n![](_page_8_Figure_14.jpeg)\n\n**ant human ct w2d grasp gmean** Fig. 21: Deep\
    \ RL physics simulations: Normalized Speedup\n\nFig. 22 depicts the runtimes for\
    \ ACS-SW-Sim and ACS-HW normalized to the baseline implementation. We make two\
    \ observations. First, ACS-SW-Sim provides similar speedups as in real hardware\
    \ compared to the baseline implementation (up to 1.79\xD7 and 1.66\xD7 on average).\
    \ Second, ACS-HW is able to further improve performance compared to the software-only\
    \ approach by alleviating the synchronization and kernel launch overheads. We\
    \ observe a slowdown with CUDAGraph due to the significant latency of constructing\
    \ the kernel dependency graph and sending the information to the GPU.\n\n![](_page_8_Figure_17.jpeg)\n\
    \nFig. 22: Deep RL physics simulations: Normalized speedup\n\nThe end-to-end speedup\
    \ in training tasks (simulation + learning algorithm) as observed is shown in\
    \ Fig. 23. We observe a mean speedup of 1.42\xD7 on ACS-HW, and 1.30\xD7 on ACS-SW.\n\
    \n![](_page_8_Figure_20.jpeg)\n\nIn Fig. 24, we depict the achieved occupancy\
    \ for the three configurations. Achieved occupancy is calculated as the number\
    \ of active warps divided by the maximum number of active warps supported by the\
    \ GPU averaged over all clock cycles. We observe that the ACS is able to significantly\
    \ increase the achieved occupancy and thus the utilization.\n\n![](_page_9_Figure_1.jpeg)\n\
    \nFig. 24: Deep RL physics simulations: Achieved occupancy\n\n#### *B. Inference\
    \ on Dynamic DNNs*\n\nFig. 25 depicts speedup over the baseline for the dynamic\
    \ DNNs described in \xA7 V. We observe that ACS is able to provide speedups of\
    \ up to 1.39\xD7 on dynamic DNN workloads with ACS-HW and on average 1.05\xD7\
    \ with ACS-SW and 1.3\xD7 with ACS-HW. I-NAS suffers a slowdown with ACS-SW because\
    \ this workload has significant kernel launch overheads when parallelized but\
    \ are hidden in the baseline case where the kernels are simply launched serially\
    \ into a single stream without synchronization. We observe that CUDAGraph exhibits\
    \ a significant slowdown due to the overhead incurred during the construction\
    \ and communication of the DAG dependencies.\n\nFig. 26 depicts the corresponding\
    \ achieved occupancy. We find that the ACS configurations are able to significantly\
    \ improve utilization, leading to performance improvements.\n\n![](_page_9_Figure_6.jpeg)\n\
    \n*C. Inference on Static DNNs*\n\nWhile our approach is designed for applications\
    \ with dynamic computational graphs, we also evaluate its effectiveness in improving\
    \ the concurrency of static DNNs. We depict the speedups obtained normalized to\
    \ the baseline in Fig. 27. We observe an average speedup of 1.31\xD7 with ACS-HW,\
    \ and a speedup of 1.16\xD7 with ACS-SW. Fig. 28 depicts the corresponding achieved\
    \ occupancy. We find that ACS leads to higher GPU utilization, leading to performance\
    \ improvements. As expected, we observe that CUDAGraph exhibits similar execution\
    \ times as ACS-HW for static graphs. This is because the task graph needs to be\
    \ constructed only once.\n\n#### *D. Sensitivity Analysis*\n\nFig. 29 compares\
    \ the speedups obtained on using scheduling window sizes of 16 and 32 for ACS-HW\
    \ over baseline. We observe that the Brax simulations have higher performance\n\
    \n![](_page_9_Figure_11.jpeg)\n\n(4.5% on average) with a window size of 32 compared\
    \ to 16. However, the window size has less of an impact on the DNNs. This is because\
    \ the simulation engines have more inter-kernel parallelism that is exposed with\
    \ a larger scheduling window.\n\n![](_page_9_Figure_13.jpeg)\n\n*E. Comparison\
    \ with Persistent Thread Frameworks*\n\nPersistent threads (PT) [58], [61], [69],\
    \ [70] are used to efficiently schedule multiple tasks with dynamically determined\
    \ dependencies. These tasks are executed using threads of a *single kernel*. Thus,\
    \ it assumes all tasks are *homogeneous*, requiring the same number of registers\
    \ and shared memory. PT frameworks which allow heterogeneous kernels are nontrivial\
    \ and would be inefficient as the persistent kernel must be configured to use\
    \ the maximum registers/scratchpad used by any kernel [58]. We use the persistent\
    \ thread framework implementation from juggler [61] and adapted it to handle heterogeneous\
    \ kernels. We were only able to implement a section of a rigid body simulator\
    \ (used for finding contacts between pairs of rigid bodies). This routine invokes\
    \ a different kernel (with different register usages) for different pairs of geometries.\
    \ We implement these kernels as tasks of our PT framework and find that it is\
    \ 1.35\xD7 slower than baseline. This slowdown is due to inefficient use of registers/scratchpad\
    \ by the kernel that leads to lower parallelism.\n\n#### VII. RELATED WORK\n\n\
    In this work, we (i) observe that input-dependent interkernel dependencies and\
    \ small kernels are a significant performance bottleneck in a range of important\
    \ applications such as simulation engines in deep RL and dynamic neural networks;\
    \ and (ii) propose both a software-only and hardware-software cooperative mechanism\
    \ to enable concurrent execution of kernels with statically unknown inter-kernel\
    \ dependencies. In this section, we describe prior work that aim to improve GPU\
    \ utilization and kernel concurrency.\n\nLeveraging concurrent streams in DL workloads.\
    \ Mainstream deep learning frameworks like Tensorflow [53] and Pytorch [54] launch\
    \ GPU kernels into a single CUDA stream that executes them sequentially. Recent\
    \ works [50], [52], [71] propose software techniques to enable concurrent execution\
    \ of GPU kernels using multiple streams with static scheduling and stream assignment\
    \ before application execution. Inter-operator scheduling [50] partitions a computation\
    \ graph into sections of kernels that can execute in parallel. Out-of-order backprop\
    \ [52] observes that gradient computation can be parallelized using CUDA streams\
    \ into weight gradients and the output gradient computation during backpropagation.\
    \ However, these works are only applicable to DL workloads whose computation graph\
    \ is static and known ahead of time, often requiring significant compilation times.\
    \ Furthermore, these approaches incur high synchronization overheads.\n\nTask-based\
    \ programming frameworks in CPUs. Taskbased frameworks [72]\u2013[74] enable programmers\
    \ to describe a program as multiple tasks which are scheduled for execution in\
    \ multiprocessor architectures [75]. Works such as task superscalar [34], carbon\
    \ [35], TDM [36] and ADM [37] propose out-of-order scheduling of tasks to efficiently\
    \ leverage irregular parallelism in multiprocessors. The major bottleneck in outof-order\
    \ scheduling of tasks dynamically for multiprocessors is the long latency required\
    \ to do dependence checks. Thus, prior work [34]\u2013[37] propose hardware accelerators\
    \ to address the long latency dependence checking needed at runtime. However,\
    \ with GPUs, the primary bottleneck is the long latency to launch/signal completion\
    \ of kernels instead, requiring a different approach to enable out-of-order scheduling.\n\
    \nProgrammer annotations Prior works leverage programmer-specified annotations\
    \ as hints to the compiler to extract parallelism. DeNovo [76] uses programmer\
    \ annotations that encode the data read and written to by each method/function.\
    \ This information is used at compile time to determine independent tasks that\
    \ can be scheduled. Some frameworks [74], [77]\u2013[80] allow programmers to\
    \ annotate the array regions accessed by each task as a compile time directive.\
    \ In ACS, we use a similar approach of programmer annotations to help determine\
    \ parallelism at runtime to enable out-of-order kernel scheduling.\n\nSoftware\
    \ techniques to improve GPU utilization with concurrent kernel execution. CUDA\
    \ Graphs [32] and AMD ATMI [33], [81], [82] are frameworks that allow users to\
    \ define dependencies between kernels as a directed-acyclicgraph (DAG) prior to\
    \ execution. This approach eliminates synchronization and kernel launch overheads\
    \ due to communication with the CPU. Nimble [51] identifies independent GPU kernels\
    \ prior to execution and concurrently schedules independent kernels using CUDA\
    \ streams. This approach uses CUDA Graphs [32] to reduce synchronization and kernel\
    \ launch overheads. Irregular graphs are also seen in solving sparse linear equations\
    \ for CFD simulations [83] and hyperplane sweep routines [84], where DAG frameworks\
    \ have been shown to be effective.We quantitatively compared ACS against a CUDA\
    \ graph implementation in \xA7 VI. None of these approaches is applicable to dynamic\
    \ input-dependent computational graphs, as caching dependency information and\
    \ constructing CUDA Graphs incur non-trivial latencies ( \xA7 II-D).\n\nHardware\
    \ support for concurrent kernels. Wireframe [85] proposes merging multiple kernels\
    \ into a single large kernel and performs CTA scheduling with data dependency\
    \ checks between CTAs. Blockmaestro [86] enables concurrently running kernels\
    \ by identifying dependencies between their CTAs. These approaches however perform\
    \ dependence checks by tracing and extracting the memory loads and stores performed\
    \ by each thread block of every kernel. Similar to the software approaches, these\
    \ approaches are designed for static computational graphs. The proposed scheduling\
    \ and dependency check techniques would be too time-consuming for runtime scheduling.\
    \ GPU dynamic parallelism [59], [87]\u2013[89] enables launching kernels from\
    \ the device itself and allows data dependencies between a single parent and multiple\
    \ child kernels. However, Dynamic-NN and RL simulation workloads contain kernels\
    \ that depend on multiple kernels, making it difficult to apply GPU dynamic parallelism.\n\
    \nCompilers, runtime systems for dynamic neural networks. Prior software [11],\
    \ [90]\u2013[95] and hardware approaches [96] aim to optimize CPU-GPU communication\
    \ overheads, launch overheads, and blocking synchronization calls for dynamic\
    \ computational graphs. These approaches introduce techniques such as dynamic\
    \ batching and kernel fusion. However, these works are orthogonal to our approach.\
    \ Prior works [97], [98] have proposed software frameworks for CPU-GPU systems\
    \ that provide simplified and convenient abstractions to interface with GPU runtime\
    \ APIs. These frameworks encapsulate runtime-level code, simplifying code development\
    \ for programmers in single and multi-GPU environments. However, these works do\
    \ not specifically focus on input-dependent dynamic computation. Instead their\
    \ goal is to provide simpler abstractions for programming GPU tasks and expressing\
    \ dataflow dependencies between them. Efficient GPU sharing techniques, such as\
    \ Kernelet [99], GPUPool [100] introduce runtime systems to enable concurrent\
    \ kernel execution by scheduling kernels from different processes which have different\
    \ memory and compute usage intensities. However, while these works increase overall\
    \ GPU utilization by kernels launched from different processes, they do not leverage\
    \ the parallelism between kernels of a single application.\n\n# VIII. CONCLUSION\n\
    \nWe introduce ACS, the first framework that enables automatic concurrent kernel\
    \ execution with low overhead runtime scheduling and dependency checks. The key\
    \ idea behind ACS is to dynamically schedule a small window of kernels by identifying\
    \ which kernel(s) within the window is ready for execution. ACS leverages kernel\
    \ annotations to automatically identify kernel dependencies at runtime. We implement\
    \ ACS as both a software framework and a hardware-software mechanism that is able\
    \ to further reduce synchronization overheads from CPU-GPU communication. We demonstrate\
    \ that ACS can improve the performance of important emerging classes of workloads,\
    \ such as RL simulations and dynamic DNNs, whose kernel dependencies are irregular\
    \ and vary with input.\n\n#### REFERENCES\n\n- [1] C. D. Freeman, E. Frey, A.\
    \ Raichuk, S. Girgin, I. Mordatch, and O. Bachem, \"Brax - a differentiable physics\
    \ engine for large scale rigid body simulation,\" *ArXiv*, vol. abs/2106.13281,\
    \ 2021.\n- [2] V. Makoviychuk, L. Wawrzyniak, Y. Guo, M. Lu, K. Storey, M. Macklin,\
    \ D. Hoeller, N. Rudin, A. Allshire, A. Handa, and G. State, \"Isaac gym: High\
    \ performance gpu-based physics simulation for robot learning,\" *ArXiv*, vol.\
    \ abs/2108.10470, 2021.\n- [3] B. Shacklett, E. Wijmans, A. Petrenko, M. Savva,\
    \ D. Batra, V. Koltun, and K. Fatahalian, \"Large batch simulation for deep reinforcement\
    \ learning,\" *ArXiv*, vol. abs/2103.07013, 2021.\n- [4] S. Dalton and I. Frosio,\
    \ \"Accelerating reinforcement learning through gpu atari emulation,\" *arXiv:\
    \ Learning*, 2020.\n- [5] A. Petrenko, Z. Huang, T. Kumar, G. Sukhatme, and V.\
    \ Koltun, \"Sample factory: Egocentric 3d control from pixels at 100000 fps with\
    \ asynchronous reinforcement learning,\" in *ICML*, 2020.\n- [6] K. Yuan, Q. Li,\
    \ S. Guo, D. Chen, A. Zhou, F. Yu, and Z. Liu, \"Differentiable dynamic wirings\
    \ for neural networks,\" *2021 IEEE/CVF International Conference on Computer Vision\
    \ (ICCV)*, pp. 317\u2013326, 2021.\n- [7] L. Liu and J. Deng, \"Dynamic deep neural\
    \ networks: Optimizing accuracy-efficiency trade-offs by selective execution,\"\
    \ in *AAAI*, 2018.\n- [8] Z. Yuan, B. Wu, Z. Liang, S. Zhao, W. Bi, and G. Sun,\
    \ \"S2dnas: Transforming static cnn model for dynamic inference via neural architecture\
    \ search,\" *ArXiv*, vol. abs/1911.07033, 2020.\n- [9] Y. Han, G. Huang, S. Song,\
    \ L. Yang, H. Wang, and Y. Wang, \"Dynamic neural networks: A survey,\" *IEEE\
    \ Transactions on Pattern Analysis and Machine Intelligence*, vol. 44, pp. 7436\u2013\
    7456, 2022.\n- [10] A. Cheng, C. H. Lin, D.-C. Juan, W. Wei, and M. Sun, \"Instanas:\
    \ Instance-aware neural architecture search,\" in *AAAI*, 2020.\n- [11] J. Wei,\
    \ G. Gibson, V. Vasudevan, and E. Xing, \"Dynamic scheduling for dynamic control\
    \ flow in deep learning systems,\" *URL http://www. cs. cmu. edu/jinlianw/papers/dynamic\
    \ scheduling nips18 sysml. pdf*, 2018.\n- [12] S. Cai, Y. Shu, and W. Wang, \"\
    Dynamic routing networks,\" *2021 IEEE Winter Conference on Applications of Computer\
    \ Vision (WACV)*, pp. 3587\u20133596, 2021.\n- [13] H. Wang, S. Li, S.-C. Su,\
    \ Z. Qin, and X. Li, \"Rdi-net: Relational dynamic inference networks,\" *2021\
    \ IEEE/CVF International Conference on Computer Vision (ICCV)*, pp. 4601\u2013\
    4610, 2021.\n- [14] P. Singh and V. P. Namboodiri, \"Skipconv: skip convolution\
    \ for computationally efficient deep cnns,\" in *2020 International Joint Conference\
    \ on Neural Networks (IJCNN)*, pp. 1\u20138, IEEE, 2020.\n- [15] S. Teerapittayanon,\
    \ B. McDanel, and H. T. Kung, \"Branchynet: Fast inference via early exiting from\
    \ deep neural networks,\" *2016 23rd International Conference on Pattern Recognition\
    \ (ICPR)*, pp. 2464\u2013 2469, 2016.\n- [16] Z. Wu, T. Nagarajan, A. Kumar, S.\
    \ Rennie, L. S. Davis, K. Grauman, and R. Feris, \"Blockdrop: Dynamic inference\
    \ paths in residual networks,\" in *CVPR*, 2018.\n- [17] A. Veit and S. J. Belongie,\
    \ \"Convolutional networks with adaptive inference graphs,\" *International Journal\
    \ of Computer Vision*, vol. 128, pp. 730\u2013741, 2019.\n- [18] Y. Li, Y. Chen,\
    \ X. Dai, D. Chen, M. Liu, L. Yuan, Z. Liu, L. Zhang, and N. Vasconcelos, \"Micronet:\
    \ Improving image recognition with extremely low flops,\" *2021 IEEE/CVF International\
    \ Conference on Computer Vision (ICCV)*, pp. 458\u2013467, 2021.\n- [19] W. Xia,\
    \ H. Yin, X. Dai, and N. K. Jha, \"Fully dynamic inference with deep neural networks,\"\
    \ *IEEE Transactions on Emerging Topics in Computing*, vol. 10, pp. 962\u2013\
    972, 2022.\n- [20] A. Kendall, J. Hawke, D. Janz, P. Mazur, D. Reda, J. M. Allen,\
    \ V.-D. Lam, A. Bewley, and A. Shah, \"Learning to drive in a day,\" *2019 International\
    \ Conference on Robotics and Automation (ICRA)*, pp. 8248\u20138254, 2019.\n-\
    \ [21] T. Chen, J. Xu, and P. Agrawal, \"A system for general in-hand object re-orientation,\"\
    \ in *Conference on Robot Learning*, pp. 297\u2013307, PMLR, 2022.\n- [22] J.\
    \ Panerati, H. Zheng, S. Zhou, J. Xu, A. Prorok, A. P. S. U. of Toronto Institute\
    \ for A Studies, V. I. for Artificial Intelligence, and U. of Cambridge, \"Learning\
    \ to fly\u2014a gym environment with pybullet physics for reinforcement learning\
    \ of multi-agent quadcopter control,\" *2021 IEEE/RSJ International Conference\
    \ on Intelligent Robots and Systems (IROS)*, pp. 7512\u20137519, 2021.\n- [23]\
    \ L. Bartolomei, L. Teixeira, and M. Chli, \"Semantic-aware active perception\
    \ for uavs using deep reinforcement learning,\" in *2021 IEEE/RSJ International\
    \ Conference on Intelligent Robots and Systems (IROS)*, pp. 3101\u20133108, 2021.\n\
    - [24] J. Chen, S. E. Li, and M. Tomizuka, \"Interpretable end-to-end urban autonomous\
    \ driving with latent deep reinforcement learning,\" *arXiv preprint arXiv:2001.08726*,\
    \ 2020.\n- [25] S. Krishnan, B. Boroujerdian, W. Fu, A. Faust, and V. J. Reddi,\
    \ \"Air learning: a deep reinforcement learning gym for autonomous aerial robot\
    \ visual navigation,\" *Mach. Learn.*, vol. 110, pp. 2501\u20132540, 2021.\n-\
    \ [26] Z. Xie, X. Da, B. Babich, A. Garg, and M. van de Panne, \"Glide: Generalizable\
    \ quadrupedal locomotion in diverse environments with a centroidal model,\" *arXiv\
    \ preprint arXiv:2104.09771*, 2021.\n- [27] Z. Si and W. Yuan, \"Taxim: An example-based\
    \ simulation model for gelsight tactile sensors,\" *IEEE Robotics and Automation\
    \ Letters*, vol. 7, no. 2, pp. 2361\u20132368, 2022.\n- [28] N. Rudin, D. Hoeller,\
    \ P. Reist, and M. Hutter, \"Learning to walk in minutes using massively parallel\
    \ deep reinforcement learning,\" *ArXiv*, vol. abs/2109.11978, 2021.\n- [29] S.\
    \ Rajbhandari, C. Li, Z. Yao, M. Zhang, R. Y. Aminabadi, A. A. Awan, J. Rasley,\
    \ and Y. He, \"Deepspeed-moe: Advancing mixture-ofexperts inference and training\
    \ to power next-generation ai scale,\" in *ICML*, 2022.\n- [30] \"Nvidia inc,\
    \ hyperq.\" https://developer.download.nvidia.com/compute/ DevZone/C/html x64/6\
    \ Advanced/simpleHyperQ/doc/HyperQ.pdf. Accessed: 2023-07-21.\n- [31] \"Nvidia\
    \ inc, cuda programming guide.\" https://docs.nvidia.com/cuda/ cuda-c-programming-guide/index.html#streams.\
    \ Accessed: 2022-11- 21.\n- [32] \"Nvidia inc, getting started with cuda graphs.\"\
    \ https://developer.nvidia. com/blog/cuda-graphs/. Accessed: 2020-09-30.\n- [33]\
    \ \"Radeon open compute, atmi (asynchronous task and memory interface).\" https://github.com/RadeonOpenCompute/atmi.\
    \ Accessed: 2022- 09-30.\n- [34] Y. Etsion, F. Cabarcas, A. Rico, A. Ramirez,\
    \ R. M. Badia, E. Ayguade, J. Labarta, and M. Valero, \"Task superscalar: An out-of-order\
    \ task pipeline,\" in *2010 43rd Annual IEEE/ACM International Symposium on Microarchitecture*,\
    \ pp. 89\u2013100, IEEE, 2010.\n- [35] S. Kumar, C. J. Hughes, and A. D. Nguyen,\
    \ \"Carbon: architectural support for fine-grained parallelism on chip multiprocessors,\"\
    \ in *International Symposium on Computer Architecture*, 2007.\n- [36] E. Castillo,\
    \ L. Alvarez, M. Moreto, M. Casas, E. Vallejo, J. L. Bosque, \xB4 R. Beivide,\
    \ and M. Valero, \"Architectural support for task dependence management with flexible\
    \ software scheduling,\" *2018 IEEE International Symposium on High Performance\
    \ Computer Architecture (HPCA)*, pp. 283\u2013295, 2018.\n- [37] D. Sanchez, R.\
    \ M. Yoo, and C. E. Kozyrakis, \"Flexible architectural \xB4 support for fine-grain\
    \ scheduling,\" in *ASPLOS XV*, 2010.\n- [38] S. Puthoor, X. Tang, J. Gross, and\
    \ B. M. Beckmann, \"Oversubscribed command queues in gpus,\" *Proceedings of the\
    \ 11th Workshop on General Purpose GPUs*, 2018.\n- [39] J. Gleeson, D. Snider,\
    \ Y. Yang, M. Gabel, E. de Lara, and G. Pekhimenko, \"Optimizing data collection\
    \ in deep reinforcement learning,\" *ArXiv*, vol. abs/2207.07736, 2022.\n- [40]\
    \ J. Gleeson, S. Krishnan, M. Gabel, V. J. Reddi, E. de Lara, and G. Pekhimenko,\
    \ \"Rl-scope: Cross-stack profiling for deep reinforcement learning workloads,\"\
    \ *ArXiv*, vol. abs/2102.04285, 2021.\n- [41] B. Zoph, V. Vasudevan, J. Shlens,\
    \ and Q. V. Le, \"Learning transferable architectures for scalable image recognition,\"\
    \ *2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pp. 8697\u2013\
    8710, 2018.\n- [42] E. Real, A. Aggarwal, Y. Huang, and Q. V. Le, \"Regularized\
    \ evolution for image classifier architecture search,\" in *AAAI*, 2019.\n- [43]\
    \ H. Liu, K. Simonyan, and Y. Yang, \"Darts: Differentiable architecture search,\"\
    \ *arXiv preprint arXiv:1806.09055*, 2018.\n- [44] S. Xie, A. Kirillov, R. B.\
    \ Girshick, and K. He, \"Exploring randomly wired neural networks for image recognition,\"\
    \ *2019 IEEE/CVF International Conference on Computer Vision (ICCV)*, pp. 1284\u2013\
    1293, 2019.\n- [45] H. Bai, F. Zhou, L. Hong, N. Ye, S.-H. G. Chan, and Z. Li,\
    \ \"Nasood: Neural architecture search for out-of-distribution generalization,\"\
    \ *2021 IEEE/CVF International Conference on Computer Vision (ICCV)*, pp. 8300\u2013\
    8309, 2021.\n- [46] B. Yang, G. Bender, Q. V. Le, and J. Ngiam, \"Condconv: Conditionally\
    \ parameterized convolutions for efficient inference,\" in *NeurIPS*, 2019.\n\
    - [47] Z. You, S. Feng, D. Su, and D. Yu, \"Speechmoe: Scaling to large acoustic\
    \ models with dynamic routing mixture of experts,\" *arXiv preprint arXiv:2105.03036*,\
    \ 2021.\n- [48] N. M. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. V. Le,\
    \ G. E. Hinton, and J. Dean, \"Outrageously large neural networks: The sparsely-gated\
    \ mixture-of-experts layer,\" *ArXiv*, vol. abs/1701.06538, 2017.\n- [49] M. Sandler,\
    \ A. G. Howard, M. Zhu, A. Zhmoginov, and L.-C. Chen, \"Mobilenetv2: Inverted\
    \ residuals and linear bottlenecks,\" *2018 IEEE/CVF Conference on Computer Vision\
    \ and Pattern Recognition*, pp. 4510\u20134520, 2018.\n- [50] Y. Ding, L. Zhu,\
    \ Z. Jia, G. Pekhimenko, and S. Han, \"Ios: Inter-operator scheduler for cnn acceleration,\"\
    \ *ArXiv*, vol. abs/2011.01302, 2021.\n- [51] W. Kwon, G.-I. Yu, E. Jeong, and\
    \ B.-G. Chun, \"Nimble: Lightweight and parallel gpu task scheduling for deep\
    \ learning,\" in *NeurIPS*, 2020.\n- [52] H. Oh, J. Lee, H. Kim, and J. Seo, \"\
    Out-of-order backprop: an effective scheduling technique for deep learning,\"\
    \ *Proceedings of the Seventeenth European Conference on Computer Systems*, 2022.\n\
    - [53] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin, S.\
    \ Ghemawat, G. Irving, M. Isard, M. Kudlur, J. Levenberg, R. Monga, S. Moore,\
    \ D. G. Murray, B. Steiner, P. A. Tucker, V. Vasudevan, P. Warden, M. Wicke, Y.\
    \ Yu, and X. Zhang, \"Tensorflow: A system for large-scale machine learning,\"\
    \ *ArXiv*, vol. abs/1605.08695, 2016.\n- [54] A. Paszke, S. Gross, F. Massa, A.\
    \ Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga,\
    \ A. Desmaison, A. Kopf, \xA8 E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy,\
    \ B. Steiner, L. Fang, J. Bai, and S. Chintala, \"Pytorch: An imperative style,\
    \ highperformance deep learning library,\" *ArXiv*, vol. abs/1912.01703, 2019.\n\
    - [55] T. Chen, M. Li, Y. Li, M. Lin, N. Wang, M. Wang, T. Xiao, B. Xu, C. Zhang,\
    \ and Z. Zhang, \"Mxnet: A flexible and efficient machine learning library for\
    \ heterogeneous distributed systems,\" *ArXiv*, vol. abs/1512.01274, 2015.\n-\
    \ [56] S. Puthoor, A. M. Aji, S. Che, M. Daga, W. Wu, B. M. Beckmann, and G. P.\
    \ Rodgers, \"Implementing directed acyclic graphs with the heterogeneous system\
    \ architecture,\" *Proceedings of the 9th Annual Workshop on General Purpose Processing\
    \ using Graphics Processing Unit*, 2016.\n- [57] HSA Foundation, \"Hsa standard,\"\
    \ 2017. http://hsafoundation.com/ standards/, Last accessed on 2023-02-14.\n-\
    \ [58] Y. Chen, B. Brock, S. D. Porumbescu, A. Bulucc, K. A. Yelick, and J. D.\
    \ Owens, \"Atos: A task-parallel gpu dynamic scheduling framework for dynamic\
    \ irregular computations,\" *ArXiv*, vol. abs/2112.00132, 2021.\n- [59] \"Nvidia\
    \ inc, cuda dynamic parallelism.\" https://developer.nvidia.com/ blog/cuda-dynamic-parallelism-api-principles/.\
    \ Accessed: 2022-09-30.\n- [60] \"Amd inc, rocm device enqueue.\" https://sep5.readthedocs.io/en/latest/\
    \ Programming Guides/Opencl-programming-guide.html#device-sideenqueue. Accessed:\
    \ 2022-09-30.\n- [61] M. E. Belviranli, S. Lee, J. S. Vetter, and L. N. Bhuyan,\
    \ \"Juggler: a dependence-aware task-based execution framework for gpus,\" *Proceedings\
    \ of the 23rd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming*,\
    \ 2018.\n- [62] M. Steinberger, M. Kenzel, P. Boechat, B. Kerbl, M. Dokter, and\
    \ D. Schmalstieg, \"Whippletree: task-based scheduling of dynamic workloads on\
    \ the gpu,\" *ACM Trans. Graph.*, vol. 33, pp. 228:1\u2013228:11, 2014.\n- [63]\
    \ N. Farooqui, A. Kerr, G. Diamos, S. Yalamanchili, and K. Schwan, \"A framework\
    \ for dynamically instrumenting gpu compute applications within gpu ocelot,\"\
    \ in *Proceedings of the Fourth Workshop on General Purpose Processing on Graphics\
    \ Processing Units*, pp. 1\u20139, 2011.\n- [64] M. Khairy, Z. Shen, T. M. Aamodt,\
    \ and T. G. Rogers, \"Accel-sim: An extensible simulation framework for validated\
    \ gpu modeling,\" *2020 ACM/IEEE 47th Annual International Symposium on Computer\
    \ Architecture (ISCA)*, pp. 473\u2013486, 2020.\n- [65] E. Todorov, T. Erez, and\
    \ Y. Tassa, \"Mujoco: A physics engine for model-based control,\" in *2012 IEEE/RSJ\
    \ international conference on intelligent robots and systems*, pp. 5026\u2013\
    5033, IEEE, 2012.\n- [66] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler,\
    \ R. Benenson, U. Franke, S. Roth, and B. Schiele, \"The cityscapes dataset for\
    \ semantic urban scene understanding,\" in *Proceedings of the IEEE conference\
    \ on computer vision and pattern recognition*, pp. 3213\u20133223, 2016.\n- [67]\
    \ M. Tan and Q. Le, \"Efficientnet: Rethinking model scaling for convolutional\
    \ neural networks,\" in *International conference on machine learning*, pp. 6105\u2013\
    6114, PMLR, 2019.\n- [68] F. N. Iandola, M. W. Moskewicz, K. Ashraf, S. Han, W.\
    \ J. Dally, and K. Keutzer, \"Squeezenet: Alexnet-level accuracy with 50x fewer\
    \ parameters and \xA11mb model size,\" *ArXiv*, vol. abs/1602.07360, 2016.\n-\
    \ [69] K. Gupta, J. A. Stuart, and J. D. Owens, \"A study of persistent threads\
    \ style gpu programming for gpgpu workloads,\" *2012 Innovative Parallel Computing\
    \ (InPar)*, pp. 1\u201314, 2012.\n- [70] T. Aila and S. Laine, \"Understanding\
    \ the efficiency of ray traversal on gpus,\" *Proceedings of the Conference on\
    \ High Performance Graphics 2009*, 2009.\n- [71] H. Zhu, A. Phanishayee, and G.\
    \ Pekhimenko, \"Daydream: Accurately estimating the efficacy of optimizations\
    \ for DNN training,\" in *2020 USENIX Annual Technical Conference (USENIX ATC\
    \ 20)*, pp. 337\u2013 352, USENIX Association, July 2020.\n- [72] J. Reinders,\
    \ M. J. Voss, P. Reble, and R. Asenjo-Plaza, \"++ for heterogeneous programming:\
    \ oneapi (dpc++ and onetbb),\" in *C++ for Heterogeneous Programming: oneAPI (DPC++\
    \ and oneTBB)*, 2020.\n- [73] R. D. Blumofe, C. F. Joerg, B. C. Kuszmaul, C. E.\
    \ Leiserson, K. H. Randall, and Y. Zhou, \"Cilk: an efficient multithreaded runtime\
    \ system,\" in *PPOPP '95*, 1995.\n- [74] L. Dagum and R. Menon, \"Openmp: an\
    \ industry standard api for shared-memory programming,\" in *OpenMP: an industry\
    \ standard API for shared-memory programming*, 1998.\n- [75] A. Ram\xB4\u0131\
    rez, F. Cabarcas, B. H. H. Juurlink, M. Alvarez-Mesa, F. Sanchez, A. Azevedo,\
    \ C. Meenderinck, C. B. Ciobanu, S. Isaza, and \xB4 G. Gaydadjiev, \"The sarc\
    \ architecture,\" *IEEE Micro*, vol. 30, pp. 16\u201329, 2010.\n- [76] B. Choi,\
    \ R. Komuravelli, H. Sung, R. Smolinski, N. Honarmand, S. V. Adve, V. S. Adve,\
    \ N. P. Carter, and C.-T. Chou, \"Denovo: Rethinking the memory hierarchy for\
    \ disciplined parallelism,\" *2011 International Conference on Parallel Architectures\
    \ and Compilation Techniques*, pp. 155\u2013166, 2011.\n- [77] J. Planas, R. M.\
    \ Badia, E. Ayguade, and J. Labarta, \"Hierarchical task- \xB4 based programming\
    \ with starss,\" *The International Journal of High Performance Computing Applications*,\
    \ vol. 23, pp. 284 \u2013 299, 2009.\n- [78] A. Pop and A. Cohen, \"Openstream:\
    \ Expressiveness and data-flow compilation of openmp streaming programs,\" *ACM\
    \ Trans. Archit. Code Optim.*, vol. 9, pp. 53:1\u201353:25, 2012.\n- [79] G. Gupta\
    \ and G. S. Sohi, \"Dataflow execution of sequential imperative programs on multicore\
    \ architectures,\" in *Proceedings of the 44th annual IEEE/ACM international symposium\
    \ on Microarchitecture*, pp. 59\u201370, 2011.\n- [80] M. D. Allen, S. Sridharan,\
    \ and G. S. Sohi, \"Serialization sets: a dynamic dependence-based parallel execution\
    \ model,\" in *Proceedings of the 14th ACM SIGPLAN symposium on Principles and\
    \ practice of parallel programming*, pp. 85\u201396, 2009.\n- [81] AMD Research,\
    \ \"Dagee,\" 2017. https://github.com/AMDResearch/ DAGEE.git, Last accessed on\
    \ 2023-02-14.\n- [82] AMD Research, \"Hipgraph,\" 2017. https://github.com/HipGraph/,\
    \ Last accessed on 2023-02-14.\n- [83] A. E. Helal, A. M. Aji, M. L. Chu, B. M.\
    \ Beckmann, and W. chun Feng, \"Adaptive task aggregation for high-performance\
    \ sparse solvers on gpus,\" *2019 28th International Conference on Parallel Architectures\
    \ and Compilation Techniques (PACT)*, pp. 324\u2013336, 2019.\n- [84] A. M. Kaushik,\
    \ A. M. Aji, M. A. Hassaan, N. Chalmers, N. Wolfe, S. Moe, S. Puthoor, and B.\
    \ M. Beckmann, \"Optimizing hyperplane sweep operations using asynchronous multi-grain\
    \ gpu tasks,\" *2019 IEEE International Symposium on Workload Characterization\
    \ (IISWC)*, pp. 59\u201369, 2019.\n- [85] A. Abdolrashidi, D. Tripathy, M. E.\
    \ Belviranli, L. N. Bhuyan, and D. Wong, \"Wireframe: Supporting data-dependent\
    \ parallelism through dependency graph execution in gpus,\" *2017 50th Annual\
    \ IEEE/ACM International Symposium on Microarchitecture (MICRO)*, pp. 600\u2013\
    611, 2017.\n- [86] A. Abdolrashidi, H. A. Esfeden, A. Jahanshahi, K. Singh, N.\
    \ B. Abu-Ghazaleh, and D. Wong, \"Blockmaestro: Enabling programmertransparent\
    \ task-based execution in gpu systems,\" *2021 ACM/IEEE 48th Annual International\
    \ Symposium on Computer Architecture (ISCA)*, pp. 333\u2013346, 2021.\n- [87]\
    \ G. Chen and X. Shen, \"Free launch: Optimizing gpu dynamic kernel launches through\
    \ thread reuse,\" *2015 48th Annual IEEE/ACM International Symposium on Microarchitecture\
    \ (MICRO)*, pp. 407\u2013419, 2015.\n- [88] I. E. Hajj, J. Gomez-Luna, C. Li,\
    \ L.-W. Chang, D. S. Milojicic, and \xB4 W. mei W. Hwu, \"Klap: Kernel launch\
    \ aggregation and promotion for optimizing dynamic parallelism,\" *2016 49th Annual\
    \ IEEE/ACM International Symposium on Microarchitecture (MICRO)*, pp. 1\u2013\
    12, 2016.\n- [89] J. Wang, N. Rubin, A. Sidelnik, and S. Yalamanchili, \"Dynamic\
    \ thread block launch: A lightweight execution mechanism to support irregular\
    \ applications on gpus,\" *2015 ACM/IEEE 42nd Annual International Symposium on\
    \ Computer Architecture (ISCA)*, pp. 528\u2013540, 2015.\n- [90] P. Fegade, T.\
    \ Chen, P. Gibbons, and T. Mowry, \"Cortex: A compiler for recursive deep learning\
    \ models,\" *Proceedings of Machine Learning and Systems*, vol. 3, pp. 38\u2013\
    54, 2021.\n- [91] E. Jeong, S. Cho, G.-I. Yu, J. S. Jeong, D.-J. Shin, and B.-G.\
    \ Chun, \"{JANUS}: fast and flexible deep learning via symbolic graph execution\
    \ of imperative programs,\" in *16th USENIX Symposium on Networked Systems Design\
    \ and Implementation (NSDI 19)*, pp. 453\u2013 468, 2019.\n- [92] S. Xu, H. Zhang,\
    \ G. Neubig, W. Dai, J. K. Kim, Z. Deng, Q. Ho, G. Yang, and E. P. Xing, \"Cavs:\
    \ An efficient runtime system for dynamic neural networks,\" in *2018 USENIX Annual\
    \ Technical Conference (USENIX ATC 18)*, pp. 937\u2013950, 2018.\n- [93] H. Shen,\
    \ J. Roesch, Z. Chen, W. Chen, Y. Wu, M. Li, V. Sharma, Z. Tatlock, and Y. Wang,\
    \ \"Nimble: Efficiently compiling dynamic neural networks for model inference,\"\
    \ *ArXiv*, vol. abs/2006.03031, 2021.\n- [94] E. Jeong, J. S. Jeong, S. Kim, G.-I.\
    \ Yu, and B.-G. Chun, \"Improving the expressiveness of deep learning frameworks\
    \ with recursion,\" in *Proceedings of the Thirteenth EuroSys Conference*, pp.\
    \ 1\u201313, 2018.\n- [95] M. Looks, M. Herreshoff, D. S. Hutchins, and P. Norvig,\
    \ \"Deep learning with dynamic computation graphs,\" *ArXiv*, vol. abs/1702.02181,\
    \ 2017.\n- [96] F. Khorasani, H. A. Esfeden, N. B. Abu-Ghazaleh, and V. Sarkar,\
    \ \"In-register parameter caching for dynamic neural nets with virtual persistent\
    \ processor specialization,\" *2018 51st Annual IEEE/ACM International Symposium\
    \ on Microarchitecture (MICRO)*, pp. 377\u2013389, 2018.\n- [97] C. J. Rossbach,\
    \ J. Currey, M. Silberstein, B. Ray, and E. Witchel, \"Ptask: operating system\
    \ abstractions to manage gpus as compute devices,\" in *Proceedings of the Twenty-Third\
    \ ACM Symposium on Operating Systems Principles*, pp. 233\u2013248, 2011.\n- [98]\
    \ C. J. Rossbach, Y. Yu, J. Currey, J.-P. Martin, and D. Fetterly, \"Dandelion:\
    \ a compiler and runtime for heterogeneous systems,\" in *Proceedings of the Twenty-Fourth\
    \ ACM Symposium on Operating Systems Principles*, pp. 49\u201368, 2013.\n- [99]\
    \ J. Zhong and B. He, \"Kernelet: High-throughput gpu kernel executions with dynamic\
    \ slicing and scheduling,\" *IEEE Transactions on Parallel and Distributed Systems*,\
    \ vol. 25, no. 6, pp. 1522\u20131532, 2013.\n- [100] X. Tan, *GPUPool: A Holistic\
    \ Approach to Fine-Grained GPU Sharing in the Cloud*. PhD thesis, University of\
    \ Toronto (Canada), 2021."
  paper_id: 28
  scores:
    originality:
      raw: 4.2
      reason: The paper presents a novel approach to optimizing a line detection algorithm
        specifically for RISC-V architectures, contributing to the field of autonomous
        vehicles.
    technical_soundness:
      raw: 4.5
      reason: The methodology appears robust, with a clear analysis of computational
        bottlenecks and effective implementation of acceleration techniques, resulting
        in a significant speedup.
    clarity:
      raw: 4.0
      reason: The writing is generally clear, but some technical terms and concepts
        could benefit from further elaboration for broader accessibility.
    significance:
      raw: 4.3
      reason: The findings are significant for the development of real-time processing
        systems in autonomous vehicles, addressing critical safety and performance
        requirements.
    relevance:
      raw: 4.5
      reason: The topic is highly relevant to current trends in autonomous vehicle
        technology and the use of specialized hardware for enhanced performance.
  total_score: 4.3
  tier: B
- title: "The Landscape of Compute-near-memory and Compute-in-memory: A Research\n\
    \  and Commercial Overview"
  abstract: 'The flexibility and range of motion in human hands play a crucial role
    in

    human interaction with the environment and have been studied across different

    fields. Researchers explored various technological solutions for gathering

    information from the hands. These solutions include tracking hand motion

    through cameras or wearable sensors and using wearable sensors to measure the

    position and pressure of contact points. Data gloves can collect both types of

    information by utilizing inertial measurement units, flex sensors, magnetic

    trackers for motion tracking, and force resistors or touch sensors for contact

    measurement. Although there are commercially available data gloves, researchers

    often create custom data gloves to achieve the desired flexibility and control

    over the hardware. However, the existing literature lacks standardization and

    the reuse of previously designed data gloves. As a result, many gloves with

    unclear characteristics exist, which makes replication challenging and

    negatively impacts the reproducibility of studies. This work proposes a

    modular, open hardware and software architecture for creating customized data

    gloves based on IMU technology. We also provide an architecture implementation

    along with an experimental protocol to evaluate device performance.'
  url: http://arxiv.org/abs/2401.13254v1
  keywords: ''
  document: "# A modular architecture for IMU-based data gloves\n\nAlessandro Carf\xEC\
    \ [,](https://orcid.org/0000-0001-9208-6910) Mohamad Alame[h](https://orcid.org/0000-0002-6345-8313)\
    \ , Valerio Belcamino [,](https://orcid.org/0000-0002-9264-8191) and Fulvio Mastrogiovann[i](https://orcid.org/0000-0001-5913-1898)\n\
    \nDepartment of Informatics, Bioengineering, Robotics, and Systems Engineering,\
    \ University of Genoa, Via Opera Pia 13, 16145 Genoa, Italy alessandro.carfi@dibris.unige.it\n\
    \nAbstract. The flexibility and range of motion in human hands play a crucial\
    \ role in human interaction with the environment and have been studied across\
    \ different fields. Researchers explored various technological solutions for gathering\
    \ information from the hands. These solutions include tracking hand motion through\
    \ cameras or wearable sensors and using wearable sensors to measure the position\
    \ and pressure of contact points. Data gloves can collect both types of information\
    \ by utilizing inertial measurement units, flex sensors, magnetic trackers for\
    \ motion tracking, and force resistors or touch sensors for contact measurement.\
    \ Although there are commercially available data gloves, researchers often create\
    \ custom data gloves to achieve the desired flexibility and control over the hardware.\
    \ However, the existing literature lacks standardization and the reuse of previously\
    \ designed data gloves. As a result, many gloves with unclear characteristics\
    \ exist, which makes replication challenging and negatively impacts the reproducibility\
    \ of studies. This work proposes a modular, open hardware and software architecture\
    \ for creating customized data gloves based on IMU technology. We also provide\
    \ an architecture implementation along with an experimental protocol to evaluate\
    \ device performance.\n\nKeywords: Data Glove \xB7 Hand Tracking \xB7 Inertial\
    \ Measuerment Unit.\n\n## 1 Introduction\n\nHuman hands' flexibility and range\
    \ of motion are fundamental for how humans interact with each other and the world.\
    \ Many research communities have studied human hand motion, focusing on the hand's\
    \ motion and its interaction with the environment [\\[1\\]](#page-4-0). Hand studies\
    \ typically include information about hand kinematics and sensory input. Hand\
    \ kinematics are well-described, with minimal variation among individuals, except\
    \ for bone proportions [\\[3\\]](#page-4-1). On the other hand, humans primarily\
    \ rely on two senses when using their hands: proprioception and touch. Proprioception\
    \ helps determine limb position, while touch provides information about forces\
    \ and points of contact. Depending on the type of study, researchers may require\
    \ information from one or both of the hand senses. As\n\n#### 2 A. Carf\xEC et\
    \ al.\n\n![](_page_1_Figure_1.jpeg)\n\n<span id=\"page-1-0\"></span>Fig. 1. The\
    \ image compares the architecture on the left with the implementation on the right.\
    \ The core module is in blue, while the sensory module is in red.\n\na result,\
    \ various technological solutions have been explored. In terms of proprioception,\
    \ tracking the hand's motion has been achieved through the use of cameras or wearable\
    \ sensors. Instead, for touch, although a few attempts have been made to estimate\
    \ them using cameras [\\[5\\]](#page-4-2), the position and pressure of contacts\
    \ are primarily measured using wearable sensors. The data glove is a wearable\
    \ device embedding sensors that can collect all the previously defined information\
    \ [\\[4\\]](#page-4-3). The sensors embedded in a data glove can be adapted to\
    \ meet the application requirements. Inertial Measurement Units (IMU) [\\[7\\\
    ]](#page-4-4), Flex sensors [\\[8\\]](#page-4-5), and magnetic trackers [\\[2\\\
    ]](#page-4-6) are the most common choices for tracking motion, while force sensing\
    \ resistors [\\[10\\]](#page-4-7) or touch sensors can be used to measure contacts\
    \ [\\[9\\]](#page-4-8). While various data gloves are commercially available [\\\
    [6\\]](#page-4-9), researchers often opt to construct custom data gloves to achieve\
    \ the desired flexibility and control over the hardware. However, the existing\
    \ literature lacks standardization and the re-use of previously designed devices.\
    \ As a result, many gloves with unclear characteristics have been developed, making\
    \ replication challenging and affecting the reproducibility of the studies for\
    \ which they were created. This work aims to introduce a modular, open hardware\
    \ and software architecture for creating customized data gloves based on IMU technology.\n\
    \n## 2 Hardware Architecture\n\nTo provide maximum flexibility, a data glove should\
    \ support a variable IMUs number to adapt to the application's requirements, work\
    \ without needing external equipment, and be easy to repair and reproduce. Our\
    \ architecture addresses these requirements by defining two modules, displayed\
    \ in Figure [1:](#page-1-0) core and sensory modules. The core module includes\
    \ the MCU for data collection and processing, an antenna to send data to a PC,\
    \ interfaces for connecting other modules, a battery for power and an IMU. The\
    \ sensory module contains an IMU and the communication interface with the core\
    \ module. Each module has a structure that includes the basic functionalities\
    \ of an IMU data glove. However, each component can be expanded to incorporate\
    \ more functionalities while keeping the main structure intact. The only technical\
    \ constraint is the communication interface, which should remain fixed to ensure\
    \ compatibility across\n\n![](_page_2_Figure_1.jpeg)\n\n<span id=\"page-2-0\"\
    ></span>Fig. 2. On the left, we have the dataglove worn by a human, and on the\
    \ right, it is mounted on the AR10 Robotic hand.\n\ndifferent implementations\
    \ of this architecture. The communication interface has two key components: the\
    \ communication protocol and the physical medium. The two most commonly adopted\
    \ communication protocols for digital sensors are Serial Peripheral Interface\
    \ (SPI) and Inter Integrated Circuit (I2C). SPI is typically used for communication\
    \ between components on the printed circuit board (PCB). It is more vulnerable\
    \ to noise and requires more communication lanes. Instead, I2C allows the same\
    \ physical lane sharing across multiple devices without additional selection lanes\
    \ and is less affected by noise. Therefore, we have chosen I2C as the communication\
    \ protocol for the modules in our hardware architecture. Instead, in the literature,\
    \ solutions for the physical medium often involved soldering cables or using flexible\
    \ PCBs. However, these options make it hard to modify the number of sensors and\
    \ increase the complexity of reproducing and repairing the device. To solve these\
    \ issues, we opted for using a Flexible Flat Cable (FFC) connector as the physical\
    \ interface for the modules and FFC cables to connect them. This solution allows\
    \ easy addition of extra sensory modules or replacement of faulty modules. Additionally,\
    \ FFC cables are flexible and do not restrict hand motion.\n\n## 3 Implementation\n\
    \nThe implementation of the two modules can be seen in Figure [1](#page-1-0) while\
    \ the device is pictured in Figure [2.](#page-2-0) The sensory module design includes\
    \ a SparkFun IMU Breakout, which embeds the MPU-9250 from InvenSense, and a custom-designed\
    \ shield to support FFC connection and daisy chaining. Although the I2C protocol\
    \ only requires four lanes (ground, power, SCL, and SDA), we opted for a six-lane\
    \ FFC connector and cable for future expansion. Each I2C lane can connect only\
    \ two sensors since the MPU-9250 only offers two selectable addresses. The two\
    \ sensory modules of each I2C lane can be worn on the proximal and intermediate\
    \ phalanges to monitor the motion of a single finger. Instead, the core module\
    \ is\n\n#### 4 A. Carf\xEC et al.\n\nplaced on the back of the hand and consists\
    \ of a custom PCB to ensure a compact design. This PCB includes an ESP32 with\
    \ a WiFi antenna, 7 FFC connectors, an MPU-9250 IMU, and an I2C multiplexer. Each\
    \ finger requires a separate I2C lane for motion tracking, so we allocated one\
    \ FFC connector for each lane. For symmetry and compatibility with both right\
    \ and left hands, we included two connectors in symmetric positions for the thumb.\
    \ Since the ESP32 has only two I2C controllers, but the system requires 6 I2C\
    \ lanes (one for each finger and one for the hand back), the design incorporates\
    \ an I2C multiplexer to manage all the communication lanes. Finally, the core\
    \ module also includes a connector for battery power. Our software consists of\
    \ two modules. The first module runs on the ESP32 and collects data from connected\
    \ sensors, sending them via UDP communication. The data includes the sensor's\
    \ unique ID, accelerometer and gyroscope readings, and orientations in quaternions.\
    \ The ESP32 uses the I2C multiplexer to collect data from each sensor module.\
    \ The sensor orientation is estimated using a data fusion process run by the MPU-9250's\
    \ digital motion processor. The second module runs on the PC and receives sensory\
    \ data through UDP communication, acting as a driver.\n\n## 4 Results\n\nThe purpose\
    \ of the experimental setup is to demonstrate the general functionalities of the\
    \ data glove. For maximum reproducibility and accuracy, we installed our device\
    \ on an AR10 hand from Active8, mounted on the Baxter manipulator from Rethink\
    \ Robotics, see Figure [2.](#page-2-0) The first test assessed the autonomy and\
    \ acquisition frequencies of the data glove under static conditions. Equipped\
    \ with eleven IMUs and powered by a 220 mAh 3.7V battery, the glove had an average\
    \ autonomy of 62.89 minutes (SD = 4.89) and transmitted data with a frequency\
    \ of 21.8 Hz (SD = 9.47) across six independent tests. In the same static conditions,\
    \ we measured the drifting of the sensors' estimated orientations over time. The\
    \ root mean square error (RMSE), averaged across all sensors, was 8.91 degrees\
    \ (SD = 3.89) after 30 minutes. We also conducted experiments involving random\
    \ movements of the robot's hand and arm. Each experiment lasted 45 minutes and\
    \ was repeated five times. The overall RMSE averaged across all sensors and trials\
    \ was 9.17 degrees (STD = 9.30).\n\n## 5 Conclusions\n\nThis article presents\
    \ a modular architecture for an IMU-based dataglove and its early implementation.\
    \ The device, equipped with a small battery, can transmit data from eleven sensors\
    \ at a frequency higher than 20Hz for over an hour. Furthermore, tests conducted\
    \ under unfavourable conditions, without proper calibration or drifting compensation,\
    \ demonstrated a reasonably accurate tracking of motions. The error in dynamic\
    \ conditions is not significantly different from that in stationary conditions,\
    \ as shown in the result sections. This result suggests that most tracking errors\
    \ are due to sensor drifting, which can be compensated for with appropriate software\
    \ solutions. The proposed device represents an initial attempt to provide an easily\
    \ reproducible and modular platform for IMU-based hand tracking. Its extensibility\
    \ offers opportunities for future research to propose new versions or develop\
    \ more accurate tracking software solutions.\n\nAcknowledgments. This work is\
    \ supported by the CHIST-ERA (2014-2020) project InDex and received funding from\
    \ the Italian Ministry of Education and Research (MIUR). This work has been also\
    \ made with the Italian government support under the National Recovery and Resilience\
    \ Plan (NRRP), Mission 4, Component 2 Investment 1.5, funded from the European\
    \ Union NextGenerationEU.\n\nDisclosure of Interests. The authors have no competing\
    \ interests to declare that are relevant to the content of this article.\n\n##\
    \ References\n\n- <span id=\"page-4-0\"></span>1. Carf\xEC, A., Patten, T., Kuang,\
    \ Y., Hammoud, A., Alameh, M., Maiettini, E., Weinberg, A.I., Faria, D., Mastrogiovanni,\
    \ F., Aleny\xE0, G., et al.: Hand-object interaction: From human demonstrations\
    \ to robot manipulation. Frontiers in Robotics and AI 8, 714023 (2021)\n- <span\
    \ id=\"page-4-6\"></span>2. Cazacu, E., van der Grinten, C., Bax, J., Baeten,\
    \ G., Holtkamp, F., Lee, C.: A position sensing glove to aid ankle-foot orthosis\
    \ diagnosis and treatment. Sensors 21(19), 6631 (2021)\n- <span id=\"page-4-1\"\
    ></span>3. Cobos, S., Ferre, M., Uran, M.S., Ortego, J., Pena, C.: Efficient human\
    \ hand kinematics for manipulation tasks. In: 2008 IEEE/RSJ International Conference\
    \ on Intelligent Robots and Systems. pp. 2246\u20132251. IEEE (10 2008)\n- <span\
    \ id=\"page-4-3\"></span>4. Dipietro, L., Sabatini, A.M., Dario, P.: A survey\
    \ of glove-based systems and their applications. Ieee transactions on systems,\
    \ man, and cybernetics, part c (applications and reviews) 38(4), 461\u2013482\
    \ (2008)\n- <span id=\"page-4-2\"></span>5. Grady, P., Tang, C., Brahmbhatt, S.,\
    \ Twigg, C.D., Wan, C., Hays, J., Kemp, C.C.: Pressurevision: Estimating hand\
    \ pressure from a single rgb image. In: European Conference on Computer Vision.\
    \ pp. 328\u2013345. Springer (2022)\n- <span id=\"page-4-9\"></span>6. He, K.,\
    \ Choosri, N.: Commercial data glove selection for vr-based hand rehabilitation\
    \ gaming project. In: 2023 Joint International Conference on Digital Arts, Media\
    \ and Technology with ECTI Northern Section Conference on Electrical, Electronics,\
    \ Computer and Telecommunications Engineering (ECTI DAMT & NCON). pp. 177\u2013\
    182. IEEE (2023)\n- <span id=\"page-4-4\"></span>7. Huang, H., Liang, Z., Sun,\
    \ F., Dong, M., et al.: Virtual interaction and manipulation control of a hexacopter\
    \ through hand gesture recognition from a data glove. Robotica 40(12), 4375\u2013\
    4387 (2022)\n- <span id=\"page-4-5\"></span>8. Luo, Y., Chen, X., Li, X., Tian,\
    \ H., Li, S., Wang, L., He, J., Yang, Z., Shao, J.: Heterogeneous strain distribution\
    \ based programmable gated microchannel for ultrasensitive and stable strain sensing.\
    \ Advanced Materials p. 2207141 (2022)\n- <span id=\"page-4-8\"></span>9. Maiolino,\
    \ P., Mastrogiovanni, F., Cannata, G., et al.: Skinning a robot: Design methodologies\
    \ for large-scale robot skin. IEEE Robotics & Automation Magazine 23(4), 150\u2013\
    159 (2016)\n- <span id=\"page-4-7\"></span>10. Wang, J., Li, B., Li, Z., Zubrycki,\
    \ I., Granosik, G.: Grasping behavior of the human hand during tomato picking.\
    \ Computers and Electronics in Agriculture 180, 105901 (2021)"
  paper_id: 16
  scores:
    originality:
      raw: 4.2
      reason: The paper presents a comprehensive overview of CNM and CIM paradigms,
        which, while not entirely new, are explored in a contemporary context that
        highlights their relevance to current technological trends.
    technical_soundness:
      raw: 4.0
      reason: The discussion of underlying technologies and working principles is
        solid, but could benefit from deeper technical analysis and examples of implementations.
    clarity:
      raw: 4.3
      reason: The writing is generally clear and well-structured, making complex concepts
        accessible, though some sections could be more concise to enhance readability.
    significance:
      raw: 4.5
      reason: The exploration of CNM and CIM architectures addresses critical challenges
        in data processing, making it a significant contribution to the field.
    relevance:
      raw: 4.7
      reason: The paper is highly relevant to ongoing research and commercial interests
        in efficient computing, particularly in the context of data-intensive applications
        and machine learning.
  total_score: 4.27
  tier: B
- title: "Cocco: Hardware-Mapping Co-Exploration towards Memory\n  Capacity-Communication\
    \ Optimization"
  abstract: 'The development of architecture specifications is an initial and fundamental

    stage of the integrated circuit (IC) design process. Traditionally,

    architecture specifications are crafted by experienced chip architects, a

    process that is not only time-consuming but also error-prone. Mistakes in these

    specifications may significantly affect subsequent stages of chip design.

    Despite the presence of advanced electronic design automation (EDA) tools,

    effective solutions to these specification-related challenges remain scarce.

    Since writing architecture specifications is naturally a natural language

    processing (NLP) task, this paper pioneers the automation of architecture

    specification development with the advanced capabilities of large language

    models (LLMs). Leveraging our definition and dataset, we explore the

    application of LLMs in two key aspects of architecture specification

    development: (1) Generating architecture specifications, which includes both

    writing specifications from scratch and converting RTL code into detailed

    specifications. (2) Reviewing existing architecture specifications. We got

    promising results indicating that LLMs may revolutionize how these critical

    specification documents are developed in IC design nowadays. By reducing the

    effort required, LLMs open up new possibilities for efficiency and accuracy in

    this crucial aspect of chip design.'
  url: http://arxiv.org/abs/2401.13266v1
  keywords: ''
  document: "# I. INTRODUCTION\n\nDeveloping architecture specifications is a critical\
    \ initial step in the process of IC design. It lays the foundational framework\
    \ and guidelines necessary for the subsequent stages of design and development.\
    \ Traditionally, the task of writing and reviewing architecture specifications\
    \ is undertaken by skilled chip architects. This process, while expertise-driven,\
    \ tends to be time-consuming and can be susceptible to human errors, soliciting\
    \ a more automated methodology to enhance efficiency and accuracy.\n\nIn recent\
    \ years, LLMs such as ChatGPT [\\[1\\]](#page-7-0) have showcased remarkable capabilities\
    \ in the field of artificial intelligence, with a wide range of applications from\
    \ question answering to content creation. The growth of chip computing power will\
    \ endow LLMs with greater capabilities. Consequently, researchers have started\
    \ to investigate the potential of LLMs in augmenting the process of chip design,\
    \ reversely enhancing\n\n<span id=\"page-0-0\"></span>![](_page_0_Figure_11.jpeg)\n\
    \nFig. 1: The overall structure of this paper. We first propose basic definitions\
    \ and an organized dataset dedicated to architecture specifications. Leveraging\
    \ them, we explore the use of LLMs in the generation and review of architecture\
    \ specifications.\n\nthe computing power of the chips themselves. For instance,\
    \ recent studies [\\[2,](#page-8-0) [3,](#page-8-1) [4,](#page-8-2) [5,](#page-8-3)\
    \ [6,](#page-8-4) [7,](#page-8-5) [8,](#page-8-6) [9\\]](#page-8-7) have utilized\
    \ LLMs to generate RTL code like Verilog. Other works develop LLM-based solution\
    \ to control EDA tools [\\[9,](#page-8-7) [10\\]](#page-8-8), design AI accelerator\
    \ architectures [\\[11,](#page-8-9) [12\\]](#page-8-10), hardware security assertion\
    \ generation [\\[13\\]](#page-8-11), fix security bugs [\\[14\\]](#page-8-12),\
    \ etc. These research efforts imply a promising future for LLMs in chip design.\
    \ In this paper, we conduct a pioneering investigation in the potential and practicality\
    \ of LLMs in processing specifications. To the best of our knowledge, there has\
    \ been no prior design automation or LLM research focusing on this important topic.\n\
    \nThis paper focuses on employing LLMs to address the challenges inherent in the\
    \ traditional management of architecture specifications. Figure [1](#page-0-0)\
    \ highlights the overall structure of this paper. Considering the absence of a\
    \ formal definition or categorization of architecture specifications, we start\
    \ with investigating existing architecture specifications across a diverse range\
    \ of products. Then we categorize architecture specifications into three levels:\
    \ Highest-level Architecture Specification (HAS), Middle-level Architecture Specification\
    \ (MAS), and Lowest-level Architecture Specification (LAS). HAS is defined as\
    \ the specification that establishes standards applicable across\n\n<sup>1</sup>The\
    \ dataset will be available at https://github.com/hkust-zhiyao/SpecLLM\n\na range\
    \ of products. It is at the highest abstraction level. MAS represents the specification\
    \ that outlines the high-level architecture of a single-chip design. LAS is the\
    \ detailed specification focused on the microarchitecture design of an individual\
    \ chip.\n\nBuilding upon our definition, we have assembled a comprehensive dataset\
    \ that includes 46 public architecture specifications from various types of products.\
    \ For the HAS, MAS, or LAS, the architecture specifications related to RISC-V\
    \ are among the most accessible and widely available. In contrast, architecture\
    \ specifications for other instruction sets are relatively scarce. We have also\
    \ observed that current architecture specifications suffer from a lack of unified\
    \ writing standards, and the length of these specifications may exceed the processing\
    \ capacity of LLMs. Both of them complicate the process of handling architecture\
    \ specifications using LLMs.\n\nBased on the basic formulation, we investigate\
    \ the application of LLMs in both generation and reviewing architecture specifications.\
    \ Regarding specification generation, we suggest two potential approaches. The\
    \ first is to simplify the process of writing architecture specifications for\
    \ designers, making it more efficient and less error-prone. The second approach\
    \ is applicable when architecture specifications are absent for an already implemented\
    \ chip. In such scenario, we can transform the RTL code back into architecture\
    \ specifications, essentially reconstructing the original design documentation\
    \ from the implemented code. According to our experiment, when generating specifications\
    \ for simple logic circuits, the majority of the human tasks could be done by\
    \ LLMs. We are optimistic that, even for more complex logic circuits, it is promising\
    \ for LLMs to progressively take over a large portion of the work.\n\nWe also\
    \ demonstrate that the LLMs can efficiently review the architecture specifications.\
    \ We first propose our definition of various types of defects in architecture\
    \ specification document. Building on this, we utilized these defects as the target\
    \ responses and developed specific prompts, seeking for LLM's review. As for the\
    \ public architecture specifications, our experiments have demonstrated that the\
    \ LLMs could provide valuable feedback for enhancing these documents. Moving forward,\
    \ we plan to extend our research to include the evaluation of the review results\
    \ generated by the LLMs, streamlining the review process and enhancing the reliability\
    \ of the outcomes.\n\nIn summary, we formulate the task of adopting LLMs in the\
    \ development of architecture specifications. We provide fundamental definitions\
    \ and explore applications of LLMs in the realm of architecture specification\
    \ development, including the basic definitions of architecture specifications\
    \ and various tasks that LLMs can perform in this context. Our key contributions\
    \ are summarized below:\n\n- We provide structured definitions of architecture\
    \ specifications, facilitating efficient utilization of LLMs in developing architecture\
    \ specifications. (Section [II\\)](#page-1-0)\n- We generate a dataset of design\
    \ specifications by methodically collecting and systematically organizing architecture\
    \ specification documents from a variety of online sources. This paves the way\
    \ for exploring LLMs in the development of architecture specifications. (Section\
    \ [III\\)](#page-3-0)\n- We explore the use of LLMs as tools for generating architecture\
    \ specifications, including assisting designers in writing these specifications\
    \ and converting RTL code into comprehensive specifications. Our findings suggest\
    \ that LLMs hold considerable promise in efficiently generating architecture specifications.\
    \ (Section [IV\\)](#page-3-1)\n- We explore the use of LLMs in the review of architecture\
    \ specifications. We identify various potential defects that may arise in these\
    \ specifications. Based on these identified defects, we have crafted specific\
    \ processes and prompts to guide the LLMs in their review. Our experimental results\
    \ indicate that LLMs are capable of providing valuable feedback for improving\
    \ these documents in aspects of accuracy. (Section [V\\)](#page-5-0)\n\n# <span\
    \ id=\"page-1-0\"></span>II. OUR DEFINITION ON ARCHITECTURE SPECIFICATION\n\n\
    We define the architecture specification as the document that describes the chip\
    \ architecture prior to RTL coding. Writing architecture specifications for the\
    \ target chip is usually the starting point of the IC design flow. The term architecture\
    \ specification is specifically chosen to emphasize that the document captures\
    \ the architectural aspects of a chip, distinguishing it from general specifications.\
    \ We categorize the architecture specifications into three levels.\n\n- The Highest-level\
    \ Architecture Specification (HAS) establishes standards applicable to a range\
    \ of products. A notable example is the RISC-V specifications [\\[61\\]](#page-8-13).\
    \ It defines the ISA specifications (e.g., instruction formats, register usages)\
    \ and Non-ISA specifications (e.g., trace). Designing specific RISC-V chips should\
    \ comply with these specifications.\n- The Middle-level Architecture Specification\
    \ (MAS) outlines the high-level architecture of a single product. These specifications\
    \ encompass the essential information required to profile the chip design. For\
    \ example, one MAS for a RISC-V CPU may include an overview of the microarchitectures\
    \ (e.g., block diagram) and their primary parameters (e.g., cache size).\n- The\
    \ Lowest-level Architecture Specification (LAS) details the microarchitecture\
    \ design of a single product. Unlike HAS and MAS, LAS should give the implementation\
    \ details for each microarchitecture, which may involve the ports, internal signals,\
    \ pipelines, and the associated descriptions. By reading them, the designers are\
    \ expected to write the corresponding RTL code correctly.\n\nThe architecture\
    \ specifications are distinct from the user manuals. We define the user manuals\
    \ as the documents that succinctly outline the chip design post-production. In\
    \ contrast to architecture specifications, which primarily cater to chip designers,\
    \ user manuals are typically crafted for the end users and programmers. Nonetheless,\
    \ we note that certain contents in the user manuals exhibit similarities to the\
    \ HAS or MAS. For example, as described in Section [III,](#page-3-0) we identify\
    \ the *ARMv8-M Architecture Reference Manual* [\\[17\\]](#page-8-14), *Intel 64\
    \ and IA-32 Architectures Software Developer's Manual* [\\[18\\]](#page-8-15)\
    \ as HAS; and a series of *Arm Core Technical Reference Manual*s [\\[28,](#page-8-16)\
    \ [27,](#page-8-17) [26\\]](#page-8-18), *Intel 64 and IA-32 Architectures Optimization\
    \ Reference* [\\[29\\]](#page-8-19) as MAS.\n\n<span id=\"page-2-0\"></span>\n\
    \n| Type             | Highest-level Architecture<br>Specifications (HAS)    \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            |\
    \ Middle-level Architecture<br>Specifications (MAS)                          \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                     | Lowest-level Architecture<br>Specifications (LAS)    \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \  |\n|------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n\
    | CPU              | \u2022 RISC-V ISA Specifications, Unpriv<br>ileged Specification\
    \ [15]<br>\u2022 RISC-V ISA Specifications, Privi<br>leged Specification [16]<br>\u2022\
    \ ARMv8-M Architecture Reference<br>Manual [17]<br>\u2022 Intel 64 and IA-32 Architectures<br>Software\
    \ Developer's Manual [18]<br>\u2022 The SPARC Architecture Manual,<br>Version\
    \ 9 [19]<br>\u2022 OpenRISC 1000 Architecture Man<br>ual [20] | \u2022 *The NEORV32\
    \ RISC-V Processor:<br>Datasheet [21]<br>\u2022 *OpenSPARC T1 Microarchitecture<br>Specification\
    \ [22]<br>\u2022 *OpenSPARC T2 Core Microarchi<br>tecture Specification [23]<br>\u2022\
    \ E31 Core Complex Manual [24]<br>\u2022 E51 Core Complex Manual [25]<br>\u2022\
    \ Arm<br>Cortex-A78<br>Core<br>Technical<br>Reference Manual [26]<br>\u2022 Arm\
    \ Cortex-X2 Core Technical Ref<br>erence Manual [27]<br>\u2022 Arm Neoverse-N2\
    \ Core Technical<br>Reference Manual [28]<br>\u2022 Intel 64 and IA-32 Architectures<br>Optimization\
    \ Reference [29]<br>\u2022 OpenRISC 1200 IP Core Specifica<br>tion [30] | \u2022\
    \ The<br>NEORV32<br>RISC-V<br>Processor:<br>Datasheet [21]<br>\u2022 OpenSPARC<br>T1<br>Microarchitecture<br>Specification\
    \ [22]<br>\u2022 OpenSPARC T2 Core Microarchitecture<br>Specification [23]<br>\u2022\
    \ Amber 2 Core Specification [31]<br>\u2022 LXP32, a lightweight open source 32-bit<br>CPU\
    \ core, Technical Reference Manual<br>[32]<br>\u2022 OpenMSP430, Texas Instruments\
    \ [33]<br>\u2022 NEO430,<br>based<br>on<br>the<br>Texas<br>Instruments MSP430(TM)\
    \ ISA [34] |\n| SoC              | \u2022 Efficient Trace for RISC-V [35]<br>\u2022\
    \ RISC-V<br>External<br>Debug<br>Support<br>[36]<br>\u2022 RISC-V<br>IOMMU<br>Architecture<br>Specification\
    \ [37]<br>\u2022 RISC-V<br>Advanced<br>Interrupt<br>Architecture [38]<br>\u2022\
    \ RISC-V<br>Platform-Level<br>Interrupt<br>Controller Specification [39]     \
    \                                                                           |\
    \ \u2022 Freedom E310-G000 Manual [40]<br>\u2022 Freedom U540-C000 Manual [41]\
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                              | \u2022 #The<br>NEORV32<br>RISC-V<br>Processor:<br>Datasheet\
    \ [21]<br>\u2022 OpenSPARC T2 System-On-Chip (SoC)<br>Microarchitecture Specification\
    \ [42]                                                                       \
    \                                                                            \
    \                                                                            \
    \                                                                          |\n\
    | Accelerator      | \u2022 RISC-V \"V\" Vector Extension [43]<br>\u2022 Intel\
    \ Advanced Performance Exten<br>sions (Intel APX) Architecture Speci<br>fication\
    \ [44]<br>\u2022 Intel Advanced Vector Extensions<br>10 (Intel AVX10) Architecture\
    \ Speci<br>fication [45]                                                     \
    \                                                                            \
    \      |                                                                     \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                            | \u2022 NVIDIA<br>Deep<br>Learning<br>Accelerator<br>(NVDLA),\
    \ Hardware Architectural Spec<br>ification [46]                              \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            |\n\
    | Bus &<br>Network | \u2022 TileLink Specification [47]<br>\u2022 AMBA5 CHI Architecture\
    \ Specifi<br>cation [48]<br>\u2022 AMBA5 ACE Protocol Specification<br>(superseded\
    \ by CHI) [49]<br>\u2022 AMBA5 AXI Protocol Specification<br>[50]<br>\u2022 AMBA4\
    \ AXI and ACE Protocol<br>Specification [51]                                 \
    \                                                                            \
    \   |                                                                        \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                         | \u2022 10GE MAC Core Specification [52]<br>\u2022\
    \ Ethernet IP Core Specification [53]<br>\u2022 I2C-Master Core Specification\
    \ [54]<br>\u2022 UART to Bus Core Specifications [55]                        \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                          |\n| Arithmetic       |                           \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                            |                                               \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                  | \u2022 Elliptic Curve Group\
    \ Core Specification<br>[56]<br>\u2022 Tate Bilinear Pairing Core Specification<br>[57]<br>\u2022\
    \ Tiny Tate Bilinear Pairing Core Specifi<br>cation [58]                     \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                       |\n| Crypto           |                              \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                         |                                                  \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                               | \u2022 AES Core Specification\
    \ [59]<br>\u2022 SHA3 Core Specification [60]                                \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                    |\n\n| TABLE I: Our proposed dataset for\
    \ architecture specifications, including approximately 46 specification documents.\
    \ |  |  |  |\n|--------------------------------------------------------------------------------------------------------------------|--|--|--|\n\
    |                                                                            \
    \                                        |  |  |  |\n\n<sup>\u22C6</sup>Notes:\
    \ \\*NEORV32 [\\[21\\]](#page-8-24), OpenSPARC T1 [\\[22\\]](#page-8-25), OpenSPARC\
    \ T2 [\\[23\\]](#page-8-26) include chapters meeting the criterion of MAS. #NEORV32\
    \ [\\[21\\]](#page-8-24) encompasses chapters describing the SoC implementation.\n\
    \n# III. DATASET OVERVIEW\n\n<span id=\"page-3-0\"></span>Table [I](#page-2-0)\
    \ provides an overview of our proposed dataset for architecture specifications.\
    \ It shows a variety of public architecture specifications and user manuals, organized\
    \ by product types like CPU, SoC, Accelerator, Bus and Network, Arithmetic and\
    \ Crypto, and by levels such as HAS, MAS, and LAS. It is important to note that\
    \ in Table [I,](#page-2-0) we only display a representative selection of about\
    \ 50 architecture specifications. For documents exhibiting a high degree of similarity,\
    \ like additional *Arm Core Technical Reference Manuals* and *AMBA Protocol Specifications*,\
    \ we have omitted them for simplicity. Our collection has yielded several notable\
    \ findings.\n\nAvailability. For the HAS, MAS, or LAS, the architecture specifications\
    \ related to RISC-V are among the most accessible and widely available. Especially\
    \ for the HAS, there are various established standards that exist, catering to\
    \ different kinds of products, such as the CPU [\\[16,](#page-8-21) [15\\]](#page-8-20),\
    \ SoC [\\[35,](#page-8-34) [37,](#page-8-36) [38,](#page-8-37) [39\\]](#page-8-38),\
    \ Accelerator [\\[43\\]](#page-8-42) and Cache Coherence Bus [\\[47\\]](#page-8-46).\
    \ An interesting observation is that despite the vast number of open-source designs\
    \ in the RISC-V ecosystem, there is a scarcity of available MAS and LAS documents.\
    \ Many documents, such as the Boom Core [\\[62\\]](#page-8-60) and XiangShan Core\
    \ [\\[63\\]](#page-8-61), which describe the design of open-source chips, do not\
    \ fulfill the standards required for formal architecture specifications. Therefore,\
    \ we have chosen not to include these in our table. This gap highlights a potential\
    \ area for improvement in the documentation and standardization of open-source\
    \ chip designs. For the Arm and X86 ecosystem, the availability of formal architecture\
    \ specifications is weaker. We can only find some reference manuals intended for\
    \ programmers or end-users. Despite this limitation, upon investigation, it is\
    \ observed that the contents of these manuals partially align with what one would\
    \ expect in formal architecture specifications. Consequently, we have chosen to\
    \ include these reference manuals for conducting research.\n\nStandard. While\
    \ writing architecture specifications is a common practice in commercial chip\
    \ manufacturing, we observe the available open specification documents exhibit\
    \ a variety of writing styles and lack unified writing standards. Within individual\
    \ semiconductor companies, there's a tendency to maintain a consistent format\
    \ for architecture specifications. Yet, the absence of unified writing standards\
    \ across the industry leads to a diversity in the presentation and structuring\
    \ of these documents, especially evident in open-source chip design [\\[21,](#page-8-24)\
    \ [31,](#page-8-30) [32,](#page-8-31) [33,](#page-8-32) [34\\]](#page-8-33). This\
    \ variation in writing styles and lack of standardized formats in architecture\
    \ specifications pose additional challenges for LLMs in handling and accurately\
    \ interpreting these documents.\n\nLength. The length of HAS spans from just over\
    \ 100 pages to more than 1000 pages. The lengths of MAS and LAS can vary widely,\
    \ with their extent directly correlating to the complexity of the chip designs\
    \ they describe. MASs outline the chip designs, and thus are usually shorter than\
    \ the LASs, ranging from several pages to in excess of 100 pages. LASs extend\
    \ from over 10 pages to hundreds of pages. The longer the architecture specifications\
    \ are, the more contextual information the LLMs tend to process. However, current\
    \ commercial LLM products have a limitation on the number of tokens they can handle,\
    \ which is not infinite. In Section [V,](#page-5-0) as we will discuss, the increase\
    \ in contextual information due to longer architecture specifications poses challenges.\
    \ It can lead to a decrease in the accuracy of the LLM's output. Therefore, the\
    \ length of the architecture specifications must be carefully considered when\
    \ employing an LLM to manage these documents.\n\nIn the following sections, we\
    \ will choose portions of them to demonstrate LLM's capabilities in developing\
    \ architecture specifications. Constrained by the scarcity of existing architecture\
    \ specifications, we consider certain sections of user manuals to function as\
    \ architecture specifications.\n\n# <span id=\"page-3-1\"></span>IV. LLM GENERATES\
    \ ARCHITECTURE SPECIFICATION\n\n# <span id=\"page-3-2\"></span>*A. Motivation*\n\
    \nTraditionally, writing architecture specifications is a nontrivial, but critical\
    \ step in the process of IC design. Our research indicates that chip architects\
    \ spend a considerable amount of time writing architecture specifications for\
    \ several reasons:\n\n- Massive Information Organization: Whether for HAS, MAS,\
    \ or LAS, a significant amount of information must be organized. Systematically\
    \ arranging them is crucial for the ensuing RTL coding process. While criteria\
    \ for writing architecture specifications exist in separate companies, the complexity\
    \ of this task escalates with the increasing scale of the designs.\n- Diverse\
    \ Modules Interconnection: MAS is responsible for establishing the interconnection\
    \ between various hardware modules. This plays a critical role in ensuring the\
    \ overall correctness of the complete product. Despite this, the process of integrating\
    \ these modules often encounters challenges, such as the time-consuming tasks\
    \ of discerning different interfaces and identifying the functions of each module.\n\
    - Complicate Algorithm Implementation: LAS delineates the structures and behaviors\
    \ for each microarchitectural design. According to our observation, LAS is closest\
    \ to the final RTL design among the three types of architecture specifications.\
    \ When dealing with complex microarchitectural algorithms, such as branch prediction,\
    \ writing LAS presents significant challenges, which include accurately defining\
    \ signals and state machines.\n\nLLMs are promising in solving the above challenges\
    \ and assisting designers in writing architecture specifications. According to\
    \ Section [II,](#page-1-0) HASs set the standards for a variety of products and\
    \ are inherently determined by human decisions. Consequently, they are highly\
    \ flexible and challenging to generate. We thus mainly focus on using LLMs to\
    \ automatically generate the MAS and LAS. We explore two LLM-based applications:\n\
    \n- 1) LLMs can reduce human effort by writing architecture specifications (Section\
    \ [IV-B\\)](#page-4-0).\n- 2) Many open-source chip designs lack comprehensive\
    \ and formal architecture specifications. In situations where these specifications\
    \ are absent, LLMs could reversely convert the RTL code into detailed specification\
    \ documents. This reverse engineering process can bridge the\n\n## <span id=\"\
    page-4-3\"></span>**Prompt:**\n\nFrom now on, you act as a chip architect writing\
    \ the architecture specification. This specification must encompass detailed descriptions\
    \ of microarchitectures, including aspects such as ports, internal signals, state\
    \ machines. Subsequently, I will assign you a specific module to focus on in your\
    \ writing.\n\nFig. 2: An example prompt to initiate generating LAS. This generation\
    \ is based on the designers' brief description.\n\n## <span id=\"page-4-4\"></span>**Prompt:**\n\
    \nPlease write an architecture specification regarding a perpetual calendar. The\
    \ starting value of Secs, Mins, and Hours are all 0. Both Secs and Mins loop continuously\
    \ from 0 to 59. When Secs=59, Min increases by 1 at the next cycle, and when Min=59\
    \ && Secs=59, Hours increases by 1 at the next cycle. Hours is counted from the\
    \ 0-23 cycle.\n\nFig. 3: An example prompt to request GPT generating an architecture\
    \ specification for perpetual calendar. The response is shown in Figure [4.](#page-4-1)\n\
    \ngap in the documentation and enhance understanding and accessibility of the\
    \ chip designs (Section [IV-C\\)](#page-4-2).\n\nIn the upcoming sections, we\
    \ will leverage the widely-used commercial LLM product\u2014GPT-4 [\\[1\\]](#page-7-0),\
    \ to demonstrate how this technology can be effectively exploited to enhance the\
    \ process of writing architecture specifications.\n\n# <span id=\"page-4-0\"></span>*B.\
    \ Assist Designers to Write Architecture Specifications*\n\nFigure [2](#page-4-3)\
    \ presents an example of a prompt to initiate the generation of LAS. This example,\
    \ referencing [\\[64\\]](#page-8-62), indicates the specific role expected to\
    \ be performed by the LLMs, and the expected response format. Then we employ the\
    \ prompt in Figure [3](#page-4-4) to direct GPT generating the architecture specifications.\
    \ This particular example, sourced from [\\[4\\]](#page-8-2), has been selected\
    \ due to its moderate level of complexity, making it suitable for testing purposes.\
    \ Figure [4](#page-4-1) presents the outcomes of GPT-4. The experimental results\
    \ demonstrate that the GPT can accurately understand the questions. These specifications,\
    \ spanning from the high-level functional description to the lowlevel state machines,\
    \ offer in-depth information for subsequent RTL coding. Yet, the design of a perpetual\
    \ calendar is a classical task in the field of circuit design. The LLMs have abundant\
    \ corpus about it. The commercial products, usually include internal IPs, which\
    \ are not used to train the external LLMs like ChatGPT. We believe it is non-trivial\
    \ to generate that kind of architecture specifications. However, our experiments\
    \ at least demonstrate that the LLMs possess the potential to conduct the task\
    \ of generating architecture specifications. The speed of the LLMs in performing\
    \ this task, compared to the experienced chip architects, still has significant\
    \ advantages.\n\n# <span id=\"page-4-2\"></span>*C. Convert RTL code to Architecture\
    \ Specifications*\n\nFigure [5](#page-5-1) showcases the prompt employed for converting\
    \ the RTL code into LAS. Following this prompt, we provide the RTL code to the\
    \ LLM, which can be in the form of textual format or as a source code file. In\
    \ our example, we adopt the Verilog file that is open-sourced by RTLLM [\\[65\\\
    ]](#page-8-63) and it is uploaded to our GitHub repo. The response of GPT is shown\
    \ in Figure [6.](#page-5-2) In summary, the generated architecture\n\n## <span\
    \ id=\"page-4-1\"></span>**Response:**\n\n## **Architecture Specification for\
    \ a Perpetual Calendar Module**\n\n## **Overview**\n\nThis document outlines the\
    \ architectural specification for a Perpetual Calendar Module (PCM). The PCM is\
    \ responsible for tracking time in seconds, minutes, and hours, with a continuous\
    \ looping mechanism. The time is represented in a 24-hour format. The module initializes\
    \ with all time values (seconds, minutes, hours) at zero and increments according\
    \ to predefined rules.\n\n## **Functional Description**\n\n#### **Time Units**\n\
    \n**Seconds (Secs):** This unit ranges from 0 to 59. It resets to 0 after reaching\
    \ 59.\n\n**Minutes (Mins):** This unit also ranges from 0 to 59. It increments\
    \ by 1 when Secs roll over from 59 to 0.\n\n**Hours:** This unit ranges from 0\
    \ to 23. It increments by 1 when both Mins and Secs roll over from 59 to 0.\n\n\
    **Initial State**\n\nAll units (Secs, Mins, Hours) are initialized to 0 at the\
    \ start.\n\n**Increment Logic**\n\n**Secs Increment:** Occurs every cycle. Rolls\
    \ over to 0 after reaching 59. **Mins Increment:** Occurs when Secs roll over\
    \ from 59 to 0. **Hours Increment:** Occurs when both Mins and Secs simultaneously\
    \ roll over from 59 to 0.\n\n## **Microarchitecture**\n\n#### **Components**\n\
    \n**Counter Units:** Three independent counters for Secs, Mins, and Hours. **Control\
    \ Logic:** Determines the rollover conditions and triggers the increment of Mins\
    \ and Hours.\n\n**Clock Signal:** Drives the incrementation of Secs.\n\n# **Counter\
    \ Behavior**\n\nEach counter is a modulo-60 counter for Secs and Mins, and a modulo-24\
    \ counter for Hours.\n\nThe counters are synchronized with the clock signal.\n\
    \n**Control Logic**\n\nMonitors the state of Secs and Mins.\n\nWhen Secs reach\
    \ 59, it sends a pulse to increment Mins at the next cycle. When both Secs and\
    \ Mins are 59, it sends a pulse to increment Hours at the next cycle.\n\n# **Interface**\n\
    \n**Input:** Clock signal.\n\n**Output:** Current time in hours, minutes, and\
    \ seconds.\n\n## **State Machines**\n\nSeconds Counter (Secs) **State 0-58:**\
    \ Increment Secs by 1 each clock cycle. **State 59:** Reset Secs to 0 on the next\
    \ clock cycle and trigger a Min increment signal. Minutes Counter (Mins) **State\
    \ 0-58 (No Increment Signal):** Hold current value. **State 59 (No Increment Signal):**\
    \ Hold current value.\n\n**State 0-58 (With Increment Signal):** Increment Mins\
    \ by 1 on the next clock cycle and reset signal.\n\n**State 59 (With Increment\
    \ Signal):** Reset Mins to 0 on the next clock cycle, reset signal, and trigger\
    \ an Hour increment signal.\n\nHours Counter\n\n**State 0-22 (No Increment Signal):**\
    \ Hold current value.\n\n**State 23 (No Increment Signal):** Hold current value.\n\
    \n**State 0-22 (With Increment Signal):** Increment Hours by 1 on the next clock\
    \ cycle and reset signal.\n\n**State 23 (With Increment Signal):** Reset Hours\
    \ to 0 on the next clock cycle and reset signal.\n\nFig. 4: A GPT response example\
    \ to the prompt in Figure [3.](#page-4-4)\n\n## <span id=\"page-5-1\"></span>**Prompt:**\n\
    \nFrom now on, you act as a chip architect writing the architecture specification.\
    \ This specification must encompass detailed descriptions of microarchitectures,\
    \ including aspects such as ports, internal signals, state machines. Subsequently,\
    \ I will provide you with the RTL code. Please read the RTL code and generate\
    \ the corresponding architecture specification.\n\nFig. 5: An example prompt to\
    \ initiate writing architecture specifications on the basis of RTL code. The response\
    \ is shown in Figure [6.](#page-5-2)\n\nspecifications are slightly shorter than\
    \ those depicted in Figure [4,](#page-4-1) yet they effectively preserve the correct\
    \ meaning. We conduct experiments to analyze this phenomenon. Our findings indicate\
    \ that the provided RTL code confines the scope for the specification generation.\
    \ Without the RTL code, the LLM may generate the architecture specifications on\
    \ the basis of the previously trained corpus, which exhibits diverse explanations\
    \ for the single circuit design. On the contrary, after we offer the LLM with\
    \ a unique RTL code, its responses are likely to be more orientated.\n\n# <span\
    \ id=\"page-5-0\"></span>V. LLM REVIEWS ARCHITECTURE SPECIFICATION\n\n# *A. Motivation*\n\
    \nThe accuracy and precision of architecture specifications directly affect the\
    \ overall quality of chip designs. Ensuring these specifications are correct is\
    \ crucial for the successful development and functionality of the chips. Traditionally,\
    \ the chip company should spend efforts to review architecture specifications.\
    \ Analogous to the reasons mentioned in Section [IV-A,](#page-3-2) reviewing architecture\
    \ specifications also requires significant human efforts. LLMs can also help reviewers\
    \ in these tasks. At least, the reviewers could use these tools to get an overview\
    \ of the specification files and obtain some comments from them. Fortunately,\
    \ as we will demonstrate in Section [V-B](#page-5-3) and Section [V-C,](#page-6-0)\
    \ the LLMs show promise in achieving more profound objectives in the review of\
    \ architectural specifications, going beyond basic overviews to provide detailed\
    \ and meaningful insights.\n\n# <span id=\"page-5-3\"></span>*B. Defects Category*\n\
    \n<span id=\"page-5-4\"></span>\n\n| Type              | Potential Defects   \
    \                                                                            \
    \                                                                            \
    \                      |  |  |\n|-------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--|--|\n\
    | Common<br>Case    | \u2022 Typographical Error<br>\u2022 Inconsistence or Contradiction\
    \ Error<br>\u2022 Incomplete or Unclear Error                                \
    \                                                                  |  |  |\n|\
    \ Level<br>Specific | \u2022 Combinational Loops Error (LAS)<br>\u2022 Uninitialized\
    \ Register Value Error (LAS)<br>\u2022 Improvement<br>for<br>Micro-architectural<br>Design<br>(LAS)<br>\u2022\
    \ Improvement for Architectural Design (MAS) |  |  |\n| Level<br>Spanning | \u2022\
    \ Inconsistence or Contradiction Error<br>(Across Various Levels)            \
    \                                                                            \
    \                                         |  |  |\n\nTABLE II: Potential Defects\
    \ Occurred in Different Levels Architecture Specifications\n\nInitially, we have\
    \ pinpointed various types of potential defects that may occur in architecture\
    \ specifications. Table [II](#page-5-4)\n\n#### <span id=\"page-5-2\"></span>**Response:**\n\
    \nI have received the RTL (Register Transfer Level) code file named \"verified\\\
    _calendar.v\". I'll proceed to read and analyze this file to generate a corresponding\
    \ architecture specification. Let's start by examining the contents of the file.\n\
    \nThe RTL code in \"**verified\\_calendar.v**\" defines a module named verified\\\
    _calendar. This module appears to be designed as a simple digital clock. Here's\
    \ a brief overview of the module and its key components based on the initial part\
    \ of the code:\n\n# **Module: verified\\_calendar**\n\n## **Inputs:**\n\n- **CLK**:\
    \ Clock input.\n- **RST**: Reset input.\n- **Outputs:**\n- **Hours**: 6-bit output\
    \ representing hours.\n- **Mins:** 6-bit output representing minutes.\n- **Secs:**\
    \ 6-bit output representing seconds.\n\n## **Internal Registers:**\n\n\u2022 **Hours,\
    \ Mins, Secs**: 6-bit registers for storing hours, minutes, and seconds.\n\n##\
    \ **Behavioral Description:**\n\nThe module contains three **always** blocks,\
    \ each triggered on the positive edge of **CLK** or **RST**.\n\n- **Seconds Logic:**\n\
    - On reset (**RST**), seconds (**Secs**) are set to 0.\n- If seconds equal 59,\
    \ they roll over to 0; otherwise, they increment every clock cycle.\n\n## **Minutes\
    \ Logic:**\n\n- On reset, minutes (**Mins**) are set to 0.\n- Minutes increment\
    \ when seconds roll over from 59 to 0. If minutes are at 59 when seconds roll\
    \ over, they reset to 0.\n\n**Hours Logic:**\n\n\u2022 Not fully visible in the\
    \ provided snippet, but likely similar to minutes, incrementing as minutes roll\
    \ over.\n\n## **State Machine for Seconds:**\n\n## States: **Reset**, **Count**\n\
    \nTransitions:\n\n- **Reset** to **Count**: When **RST** de-asserted.\n- **Count**\
    \ to **Reset**: When **RST** asserted or **Secs** reach 59.\n- Output: Increments\
    \ **Secs** or resets to 0 based on the state.\n\n# **State Machine for Minutes:**\n\
    \nStates: **Reset**, **Count**\n\nTransitions:\n\n- **Reset** to **Count**: When\
    \ **RST** de-asserted.\n- **Count** to **Reset**: When **Mins** reach 59 and **Secs**\
    \ are 59.\n\nOutput: Increments **Mins** or resets to 0 based on the state and\
    \ **Secs**.\n\nFig. 6: A GPT response example to the prompt in Figure [5.](#page-5-1)\n\
    \nsummarizes these defects, categorizing them according to the level of architecture\
    \ specifications.\n\nIn many instances, there are defects that could manifest\
    \ in all three tiers of architecture specifications: HAS, MAS, and LAS. They include\
    \ the *Typographical Error*, *Inconsistence or Contradiction Error*, and *Incomplete\
    \ or Unclear Error*. The *Inconsistence or Contradiction Error* denotes situations\
    \ within a single specification file where either two concepts describing the\
    \ same object are inconsistent, or two related concepts are contradictory. The\
    \ *Incomplete or Unclear Error* refers to instances where certain concepts lack\
    \ essential information, resulting in sentences that are open to ambiguous interpretations.\n\
    \nFurthermore, the potential defects could also be levelspecific, indicating that\
    \ HAS, MAS, or LAS may each have distinct defects. For example, LAS might include\
    \ details about port connections between various modules. LLMs can be employed\
    \ to scrutinize the LAS for identifying potential *Combinational Loops Error*.\
    \ Such scrutiny, however, may not be applicable to HAS and MAS. In addition to\
    \ locating the writing issues, we also identify some high-level objectives for\
    \ the review of architecture specification, such as the *Improvement for Architectural\
    \ Design* and *Improvement for Micro-architectural Design*. These objectives are\
    \ aimed at leveraging the potential of the review process to enhance both the\
    \ architectural and micro-architectural design aspects. This goes beyond mere\
    \ error correction, focusing on the overall optimization and refinement of the\
    \ design.\n\nAdditionally, these defects might span across different levels. One\
    \ example of this is the *Inconsistence or Contradiction Error (Across Various\
    \ Levels)*, that the lower-level architecture specifications should not contradict\
    \ the contents established in the higher-level specifications. Note that it is\
    \ not equivalent to the *Inconsistence or Contradiction Error* established for\
    \ the common case. The former indicates the inconsistence or contradiction between\
    \ different levels of architecture specification files, while the latter confines\
    \ the error in one individual specification file.\n\n# <span id=\"page-6-0\"></span>*C.\
    \ Architecture Specification Review*\n\nWe exploit the potential defects in architecture\
    \ specifications as the types of feedback (Section [V-B\\)](#page-5-3) we aim\
    \ to receive from the review conducted by the LLMs. We have developed two methodologies\
    \ to guide the LLMs in reviewing architecture specifications. The first method\
    \ involves directly uploading the entire specification file and requesting the\
    \ LLMs to provide feedback section by section. The second method entails splitting\
    \ the specification file into sections, uploading each section individually, and\
    \ then asking the LLMs to conduct a review immediately upon the upload of each\
    \ section. Regarding the first approach, it has the advantage of not requiring\
    \ extra steps to divide the specification file. However, we've observed that even\
    \ when instructed to give feedback for each section, the LLMs struggle to accurately\
    \ recognize the boundaries between different sections. This often results in the\
    \ provision of incorrect or misleading indication messages by the LLMs. The second\
    \ approach, which involves splitting the specification file into sections and\
    \ uploading each one separately for review, can effectively alleviate this problem\
    \ by providing clearer boundaries for the LLMs to analyze and respond to each\
    \ section individually. In the subsequent sections, we will use part of the architecture\
    \ specifications, as shown in Table [I,](#page-2-0) to demonstrate LLM's strengths\
    \ and weaknesses in reviewing architecture specifications.\n\n*1) Reviewing One\
    \ File at a Time:* We employ the Amber 2 Core Specification [\\[31\\]](#page-8-30)\
    \ to conduct the experiments of reviewing architecture specifications. The tested\
    \ file used in our example is from [\\[66\\]](#page-8-64) and it has been uploaded\
    \ to our\n\n## <span id=\"page-6-2\"></span>**Prompt:**\n\nFrom now on, you act\
    \ as a chip architect reviewing the architecture specification. For the following\
    \ document, please find any typo, inconsistence/contradiction, incomplete/unclear\
    \ and propose the corresponding improvement suggestions. The responding format\
    \ should separately list the findings about typo, inconsistence/contradiction,\
    \ incomplete/unclear and the corresponding improvement suggestions.\n\nFig. 7:\
    \ An example prompt to initiate the review of architecture specifications. The\
    \ response is shown in Figure [8.](#page-6-1)\n\n## <span id=\"page-6-1\"></span>**Response:**\n\
    \nI have thoroughly reviewed the Amber Core Specification document. Here are the\
    \ summarized findings:\n\n1. **Typos and Grammatical Errors:** The document contains\
    \ some typographical and grammatical errors which can be corrected for clarity\
    \ and professionalism.\n\n2. **Inconsistencies/Contradictions:** Certain technical\
    \ specifications and descriptions appear to be inconsistent or contradictory,\
    \ which could lead to confusion in understanding the architecture and functionality\
    \ of the Amber Core.\n\n3. **Incomplete/Unclear Sections:** Some sections of the\
    \ document are either incomplete or not clearly explained, which may hinder the\
    \ complete understanding of the core's architecture and operations.\n\nFor a detailed\
    \ report including specific instances of these issues and improvement suggestions,\
    \ please let me know, and I will provide a comprehensive breakdown.\n\nFig. 8:\
    \ A GPT response example to the prompt in Figure [7.](#page-6-2)\n\n## <span id=\"\
    page-6-3\"></span>**Prompt:**\n\nGood, let's review Section 1, and please provide\
    \ specific and comprehensive review results. The response format should comply\
    \ with the previous requirements.\n\nFig. 9: An example prompt to review individual\
    \ sections of the specification document. The response is shown in Figure [10.](#page-7-1)\n\
    \nGitHub repo. Its concise contents enable current LLMs like GPT-4 to review it\
    \ at a time. According to Table [II,](#page-5-4) we develop a prompt to request\
    \ GPT-4 providing feedback on common defects. We use the prompt, as illustrated\
    \ in Figure [7,](#page-6-2) to initiate the process of writing architecture specifications.\
    \ Next, we upload the specification file selected for testing and ask GPT to summarize\
    \ its findings. As illustrated in Figure [8,](#page-6-1) the LLM will then provide\
    \ an overview of its analysis. This information can offer designers a quick glimpse\
    \ into the results of the review. Subsequently, as depicted in Figure [9,](#page-6-3)\
    \ we request GPT to provide us with detailed review results for each individual\
    \ section of the specification document. Figure [10](#page-7-1) displays the in-depth\
    \ review results specifically for Section 1 of the Amber Core Specification. In\
    \ general, we have observed that GPT-4 is adept at correctly identifying typographical\
    \ errors. Furthermore, in more complex cases such as inconsistence or contradiction\
    \ errors, as well as incomplete or unclear errors, GPT-4 is also capable of offering\
    \ constructive advice. It can give the reasons why these types of errors exist\
    \ and suggest ways to improve them. Nonetheless, our research has revealed that\
    \ GPT-4 faces challenges in effectively dividing an archi-\n\n## <span id=\"page-7-1\"\
    ></span>**Response:**\n\n## **Section 1 Review of the Amber Core Specification**\n\
    \nTypos:\n\n1. \"seperate\" (in the description of Amber 25) should be \"separate.\"\
    \n\n2. \"Mhz\" (in the capability description of Amber 25) should be \"MHz.\"\n\
    \n3. \"that\" (in the performance comparison of Amber 25 and Amber 23) should\
    \ be \"than.\"\n\n4. \"has\" (in the paragraph about Linux kernel verification)\
    \ should be \"have.\"\n\n5. \"pipline\" (in the last sentence) should be \"pipeline.\"\
    \n\n#### Inconsistencies/Contradictions:\n\n1. The document states that the Amber\
    \ 23 and Amber 25 cores are 100% software compatible, yet they have differing\
    \ pipeline stages and cache architectures. This could potentially lead to different\
    \ performance characteristics not addressed in the compatibility claim.\n\n##\
    \ Incomplete/Unclear Sections:\n\n1. The description of the Wishbone interface\
    \ is vague. More detailed specifications or references to the interface's documentation\
    \ would improve clarity.\n\n2. The performance metrics (DMIPS per MHz) are provided,\
    \ but there's no baseline comparison or explanation of what these figures imply\
    \ in practical terms.\n\n3. The document mentions that the cores have been verified\
    \ with a 2.4 Linux kernel but does not detail the extent or scope of this verification.\n\
    \n4. The information about the FPGA synthesis is incomplete. Details about the\
    \ synthesis process, along with any challenges or optimizations, would provide\
    \ a clearer picture of the implementation process.\n\n## Improvement Suggestions:\n\
    \n1. **Typos**: Correct all typographical errors for accuracy and professionalism.\n\
    \n2. **Inconsistencies/Contradictions:** Clarify the software compatibility claim\
    \ considering the different architectures of Amber 23 and Amber 25. Include any\
    \ limitations or specific scenarios where this compatibility might not hold.\n\
    \n## 3. **Incomplete/Unclear Sections:**\n\n- Expand the description of the Wishbone\
    \ interface, possibly including a subsection dedicated to its architecture and\
    \ how it interfaces with the Amber cores.\n- Provide a brief explanation or a\
    \ reference to what DMIPS per MHz represents, helping readers unfamiliar with\
    \ this metric understand the core's performance.\n- Detail the scope of the Linux\
    \ kernel verification. Specify what tests were performed, any limitations encountered,\
    \ and how these tests validate the core's functionality.\n- Elaborate on the FPGA\
    \ synthesis process, including challenges faced, optimizations made, and why specific\
    \ FPGA boards were chosen for testing.\n\nFig. 10: A GPT response example to the\
    \ prompt in Figure [9.](#page-6-3)\n\ntecture specification file into distinct\
    \ sections. This limitation often results in the feedback provided for one section\
    \ possibly originating from other sections of the document. To mitigate this problem,\
    \ we propose to split the complete specification file into individual sections,\
    \ and request GPT to review them sequentially.\n\n*2) Splitting and Reviewing\
    \ File Section-by-Section:* Similar to the first method, we begin by using prompts,\
    \ as shown in Figure [7,](#page-6-2) to initiate the review of architecture specifications.\
    \ Next, instead of uploading the entire architecture specification file for review,\
    \ we submit only the file containing the contents of the individual section as\
    \ an attachment. Simultaneously, we provide the prompt, as illustrated in Figure\
    \ [11,](#page-7-2) directing\n\n#### <span id=\"page-7-2\"></span>**Prompt:**\n\
    \nPlease review the section 1 first, and give me specific and comprehensive reviewing\
    \ results. The responding format should comply with the previous requirements.\n\
    \n**( Note: attach the divided file that represents Section 1 of the architecture\
    \ specification file)**\n\nFig. 11: An example prompt to review the split files.\
    \ Though the response from GPT is similar to the Figure [10,](#page-7-1) the feedback\
    \ is more likely to concentrate on the individual section.\n\nGPT to review each\
    \ section. In this strategy, each review task for different sections is paired\
    \ with its corresponding chapter file, ensuring that each section is reviewed\
    \ in isolation and in context. The responses given by the GPT in this review process\
    \ maintain a format similar to the outputs generated by the previous, more holistic\
    \ review method. However, we have observed that the feedbacks are more likely\
    \ to concentrate on each individual section. Furthermore, the GPT appears more\
    \ effective in identifying a broader range of errors, which increases the number\
    \ of available output items. The accurate and abundant review results enable designers\
    \ to quickly but comprehensively locate the defects in their specification documents.\n\
    \n# *D. Feedbacks Evaluation*\n\nNot all feedback provided by the LLM is correct\
    \ or useful. Therefore, it is essential to have strategies in place to evaluate\
    \ the review results of architecture specifications effectively. The most intuitive\
    \ method is to ask the designers to check the outputs of the LLM. This approach,\
    \ compared to directly reviewing the architecture specifications themselves, is\
    \ already more efficient, potentially saving significant time and effort. Nonetheless,\
    \ an alternative approach involves training a new language model specifically\
    \ for evaluating and filtering the review results provided by the LLMs. This specialized\
    \ model could be designed to assess the relevance and accuracy of the LLM's feedback\
    \ [\\[67\\]](#page-8-65), thereby streamlining the review process and enhancing\
    \ the reliability of the outcomes. We leave this part for future exploration.\n\
    \n# VI. CONCLUSION\n\nIn this paper, we propose a novel framework for utilizing\
    \ LLMs to generate and review architecture specifications. We defined architecture\
    \ specifications, categorized them into clear levels, and compiled a corresponding\
    \ dataset. Our approach focuses on two primary applications of LLMs: firstly,\
    \ in generating architecture specifications, including assistance in writing and\
    \ converting RTL code into specifications, and secondly, in the review of these\
    \ specifications. This innovative methodology signifies a transformative step\
    \ in architecture specification development, offering a path toward more efficient,\
    \ accurate, and streamlined processes in chip design.\n\n# REFERENCES\n\n<span\
    \ id=\"page-7-0\"></span>[1] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya,\
    \ F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat *et al.*, \"\
    Gpt-4 technical report,\" *arXiv preprint arXiv:2303.08774*, 2023.\n\n- <span\
    \ id=\"page-8-0\"></span>[2] S. Liu, W. Fang, Y. Lu, Q. Zhang, H. Zhang, and Z.\
    \ Xie, \"Rtlcoder: Outperforming gpt-3.5 in design rtl generation with our open-source\
    \ dataset and lightweight solution,\" *arXiv preprint arXiv:2312.08617*, 2023.\n\
    - <span id=\"page-8-1\"></span>[3] J. Blocklove, S. Garg, R. Karri, and H. Pearce,\
    \ \"Chip-chat: Challenges and opportunities in conversational hardware design,\"\
    \ *arXiv preprint arXiv:2305.13243*, 2023.\n- <span id=\"page-8-2\"></span>[4]\
    \ Y. Lu, S. Liu, Q. Zhang, and Z. Xie, \"Rtllm: An open-source benchmark for design\
    \ rtl generation with large language model,\" *arXiv preprint arXiv:2308.05345*,\
    \ 2023.\n- <span id=\"page-8-3\"></span>[5] M. Liu, N. Pinckney, B. Khailany,\
    \ and H. Ren, \"Verilogeval: Evaluating large language models for verilog code\
    \ generation,\" *arXiv preprint arXiv:2309.07544*, 2023.\n- <span id=\"page-8-4\"\
    ></span>[6] S. Thakur, B. Ahmad, Z. Fan, H. Pearce, B. Tan, R. Karri, B. Dolan-Gavitt,\
    \ and S. Garg, \"Benchmarking large language models for automated verilog rtl\
    \ code generation,\" in *DATE*, 2023.\n- <span id=\"page-8-5\"></span>[7] S. Thakur,\
    \ J. Blocklove, H. Pearce, B. Tan, S. Garg, and R. Karri, \"Autochip: Automating\
    \ hdl generation using llm feedback,\" *arXiv preprint arXiv:2311.04887*, 2023.\n\
    - <span id=\"page-8-6\"></span>[8] M. Nair, R. Sadhukhan, and D. Mukhopadhyay,\
    \ \"Generating secure hardware using chatgpt resistant to cwes,\" *Cryptology\
    \ ePrint Archive*, 2023.\n- <span id=\"page-8-7\"></span>[9] M. Liu, T.-D. Ene,\
    \ R. Kirby, C. Cheng, N. Pinckney, R. Liang, J. Alben, H. Anand, S. Banerjee,\
    \ I. Bayraktaroglu *et al.*, \"Chipnemo: Domainadapted llms for chip design,\"\
    \ *arXiv preprint arXiv:2311.00176*, 2023.\n- <span id=\"page-8-8\"></span>[10]\
    \ Z. He, H. Wu, X. Zhang, X. Yao, S. Zheng, H. Zheng, and B. Yu, \"Chateda: A\
    \ large language model powered autonomous agent for eda,\" in *MLCAD Workshop*,\
    \ 2023.\n- <span id=\"page-8-9\"></span>[11] Y. Fu, Y. Zhang, Z. Yu, S. Li, Z.\
    \ Ye, C. Li, C. Wan, and Y. Lin, \"Gpt4aigchip: Towards next-generation ai accelerator\
    \ design automation via large language models,\" *arXiv preprint arXiv:2309.10730*,\
    \ 2023.\n- <span id=\"page-8-10\"></span>[12] Z. Yan, Y. Qin, X. S. Hu, and Y.\
    \ Shi, \"On the viability of using llms for sw/hw co-design: An example in designing\
    \ cim dnn accelerators,\" *arXiv preprint arXiv:2306.06923*, 2023.\n- <span id=\"\
    page-8-11\"></span>[13] R. Kande, H. Pearce, B. Tan, B. Dolan-Gavitt, S. Thakur,\
    \ R. Karri, and J. Rajendran, \"Llm-assisted generation of hardware assertions,\"\
    \ *arXiv preprint arXiv:2306.14027*, 2023.\n- <span id=\"page-8-12\"></span>[14]\
    \ B. Ahmad, S. Thakur, B. Tan, R. Karri, and H. Pearce, \"Fixing hardware security\
    \ bugs with large language models,\" *arXiv preprint arXiv:2302.01215*, 2023.\n\
    - <span id=\"page-8-20\"></span>[15] *RISC-V ISA Specifications Volume 1, Unprivileged\
    \ Specification*, [\"https:](\"https://wiki.riscv.org/display/HOME/RISC-V+Technical+Specifications\"\
    \ ) [//wiki.riscv.org/display/HOME/RISC-V+Technical+Specifications\".](\"https://wiki.riscv.org/display/HOME/RISC-V+Technical+Specifications\"\
    \ )\n- <span id=\"page-8-21\"></span>[16] *RISC-V ISA Specifications Volume 2,\
    \ Privileged Specification*, [\"https:](\"https://wiki.riscv.org/display/HOME/RISC-V+Technical+Specifications\"\
    \ ) [//wiki.riscv.org/display/HOME/RISC-V+Technical+Specifications\".](\"https://wiki.riscv.org/display/HOME/RISC-V+Technical+Specifications\"\
    \ )\n- <span id=\"page-8-14\"></span>[17] *ARMv8-M Architecture Reference Manual*,\
    \ [\"https://](\"https://documentation-service.arm.com/static/5f8efe85f86e16515cdbe596?token=\"\
    \ ) [documentation-service.arm.com/static/5f8efe85f86e16515cdbe596?](\"https://documentation-service.arm.com/static/5f8efe85f86e16515cdbe596?token=\"\
    \ ) [token=\".](\"https://documentation-service.arm.com/static/5f8efe85f86e16515cdbe596?token=\"\
    \ )\n- <span id=\"page-8-15\"></span>[18] *Intel 64 and IA-32 Architectures Software\
    \ Developer's Manua*, [\"https:](\"https://cdrdv2.intel.com/v1/dl/getContent/671200\"\
    \ ) [//cdrdv2.intel.com/v1/dl/getContent/671200\".](\"https://cdrdv2.intel.com/v1/dl/getContent/671200\"\
    \ )\n- <span id=\"page-8-22\"></span>[19] S. I. Inc and D. L. Weaver, *The SPARC\
    \ architecture manual*, 1994.\n- <span id=\"page-8-23\"></span>[20] *OpenRISC\
    \ 1000 Architecture Manua*, [\"https://openrisc.io/or1k.html\".](\"https://openrisc.io/or1k.html\"\
    \ )\n- <span id=\"page-8-24\"></span>[21] *The NEORV32 RISC-V Processor: Datasheet*,\
    \ [\"https://stnolting.github.](\"https://stnolting.github.io/neorv32/\" ) [io/neorv32/\"\
    .](\"https://stnolting.github.io/neorv32/\" )\n- <span id=\"page-8-25\"></span>[22]\
    \ *OpenSPARC T1 Microarchitecture Specification*, [\"https://www.oracle.com/technetwork/systems/opensparc/](\"\
    https://www.oracle.com/technetwork/systems/opensparc/t1-01-opensparct1-micro-arch-1538959.html\"\
    \ ) [t1-01-opensparct1-micro-arch-1538959.html\".](\"https://www.oracle.com/technetwork/systems/opensparc/t1-01-opensparct1-micro-arch-1538959.html\"\
    \ )\n- <span id=\"page-8-26\"></span>[23] *OpenSPARC T2 Microarchitecture Specification*,\
    \ [\"https://www.oracle.com/technetwork/systems/opensparc/](\"https://www.oracle.com/technetwork/systems/opensparc/t2-06-opensparct2-core-microarch-1537749.html\"\
    \ ) [t2-06-opensparct2-core-microarch-1537749.html\".](\"https://www.oracle.com/technetwork/systems/opensparc/t2-06-opensparct2-core-microarch-1537749.html\"\
    \ )\n- <span id=\"page-8-27\"></span>[24] *E31 Core Complex Manual*, [\"https://d2pn104n81t9m2.cloudfront.net/](\"\
    https://d2pn104n81t9m2.cloudfront.net/documentation/\") [documentation/\".](\"\
    https://d2pn104n81t9m2.cloudfront.net/documentation/\")\n- <span id=\"page-8-28\"\
    ></span>[25] *E51 Core Complex Manual*, [\"https://d2pn104n81t9m2.cloudfront.net/](\"\
    https://d2pn104n81t9m2.cloudfront.net/documentation/\") [documentation/\".](\"\
    https://d2pn104n81t9m2.cloudfront.net/documentation/\")\n- <span id=\"page-8-18\"\
    ></span>[26] *Arm Cortex-A78 Core Technical Reference Manual*, [\"https://developer.](\"\
    https://developer.arm.com/documentation/101430/latest/\" ) [arm.com/documentation/101430/latest/\"\
    .](\"https://developer.arm.com/documentation/101430/latest/\" )\n- <span id=\"\
    page-8-17\"></span>[27] *Arm Cortex-X2 Core Technical Reference Manual*, [\"https://developer.](\"\
    https://developer.arm.com/documentation/101803/latest/\" ) [arm.com/documentation/101803/latest/\"\
    .](\"https://developer.arm.com/documentation/101803/latest/\" )\n- <span id=\"\
    page-8-16\"></span>[28] *Arm Neoverse N2 Core Technical Reference Manual*, [\"\
    https://developer.](\"https://developer.arm.com/documentation/102099/latest/The-Neoverse-N2--core\"\
    \ ) [arm.com/documentation/102099/latest/The-Neoverse-N2--core\".](\"https://developer.arm.com/documentation/102099/latest/The-Neoverse-N2--core\"\
    \ )\n- <span id=\"page-8-19\"></span>[29] *Intel 64 and IA-32 Architectures Optimization\
    \ Reference*, [\"https://www.intel.com/content/www/us/en/content-details/671488/](\"\
    https://www.intel.com/content/www/us/en/content-details/671488/intel-64-and-ia-32-architectures-optimization-reference-manual.html\"\
    \ ) [intel-64-and-ia-32-architectures-optimization-reference-manual.html\".](\"\
    https://www.intel.com/content/www/us/en/content-details/671488/intel-64-and-ia-32-architectures-optimization-reference-manual.html\"\
    \ )\n- <span id=\"page-8-29\"></span>[30] *OpenRISC 1200 IP Core Specification*,\
    \ [\"https://github.com/openrisc/](\"https://github.com/openrisc/or1200/blob/master/doc/openrisc1200_spec.pdf\"\
    ) [or1200/blob/master/doc/openrisc1200](\"https://github.com/openrisc/or1200/blob/master/doc/openrisc1200_spec.pdf\"\
    ) spec.pdf\".\n- <span id=\"page-8-30\"></span>[31] *Amber ARM-compatible core*,\
    \ [\"https://opencores.org/projects/amber\".](\"https://opencores.org/projects/amber\"\
    \ )\n- <span id=\"page-8-31\"></span>[32] *LXP32, a lightweight, open source and\
    \ FPGA-friendly 32-bit CPU IP core*, [\"https://opencores.org/projects/lxp32\"\
    .](\"https://opencores.org/projects/lxp32\" )\n- <span id=\"page-8-32\"></span>[33]\
    \ *OpenMSP430, Texas Instruments*, [\"https://opencores.org/projects/](\"https://opencores.org/projects/openmsp430\"\
    \ ) [openmsp430\".](\"https://opencores.org/projects/openmsp430\" )\n- <span id=\"\
    page-8-33\"></span>[34] *NEO430, based on the Texas Instruments MSP430(TM) ISA*,\
    \ [\"https://](\"https://opencores.org/projects/neo430\" ) [opencores.org/projects/neo430\"\
    .](\"https://opencores.org/projects/neo430\" )\n- <span id=\"page-8-34\"></span>[35]\
    \ *Efficient Trace for RISC-V*, [\"https://wiki.riscv.org/display/HOME/](\"https://wiki.riscv.org/display/HOME/RISC-V+Technical+Specifications\"\
    \ ) [RISC-V+Technical+Specifications\".](\"https://wiki.riscv.org/display/HOME/RISC-V+Technical+Specifications\"\
    \ )\n- <span id=\"page-8-35\"></span>[36] *RISC-V External Debug Support*, [\"\
    https://wiki.riscv.org/display/HOME/](\"https://wiki.riscv.org/display/HOME/RISC-V+Technical+Specifications\"\
    \ ) [RISC-V+Technical+Specifications\".](\"https://wiki.riscv.org/display/HOME/RISC-V+Technical+Specifications\"\
    \ )\n- <span id=\"page-8-36\"></span>[37] *RISC-V IOMMU Architecture Specification*,\
    \ [\"https://wiki.riscv.org/](\"https://wiki.riscv.org/display/HOME/RISC-V+Technical+Specifications\"\
    \ ) [display/HOME/RISC-V+Technical+Specifications\".](\"https://wiki.riscv.org/display/HOME/RISC-V+Technical+Specifications\"\
    \ )\n- <span id=\"page-8-37\"></span>[38] *RISC-V Advanced Interrupt Architecture*,\
    \ [\"https://wiki.riscv.org/display/](\"https://wiki.riscv.org/display/HOME/RISC-V+Technical+Specifications\"\
    \ ) [HOME/RISC-V+Technical+Specifications\".](\"https://wiki.riscv.org/display/HOME/RISC-V+Technical+Specifications\"\
    \ )\n- <span id=\"page-8-38\"></span>[39] *RISC-V Platform-Level Interrupt Controller\
    \ Specification*, [\"https://wiki.](\"https://wiki.riscv.org/display/HOME/RISC-V+Technical+Specifications\"\
    \ ) [riscv.org/display/HOME/RISC-V+Technical+Specifications\".](\"https://wiki.riscv.org/display/HOME/RISC-V+Technical+Specifications\"\
    \ )\n- <span id=\"page-8-39\"></span>[40] *Freedom E310-G000 Manual*, [\"https://www.sifive.com/documentation\"\
    .](\"https://www.sifive.com/documentation\" )\n- <span id=\"page-8-40\"></span>[41]\
    \ *Freedom U540-C000 Manual*, [\"https://www.sifive.com/documentation\".](\"https://www.sifive.com/documentation\"\
    \ )\n- <span id=\"page-8-41\"></span>[42] *OpenSPARC T2 System-On-Chip (SoC) Microarchitecture\
    \ Specification*, [\"https://www.oracle.com/docs/tech/systems/](\"https://www.oracle.com/docs/tech/systems/t2-07-opensparct2-socmicroarchvol1.pdf\"\
    \ ) [t2-07-opensparct2-socmicroarchvol1.pdf\".](\"https://www.oracle.com/docs/tech/systems/t2-07-opensparct2-socmicroarchvol1.pdf\"\
    \ )\n- <span id=\"page-8-42\"></span>[43] *RISC-V \"V\" Vector Extension*, [\"\
    https://inst.eecs.berkeley.edu/](\"https://inst.eecs.berkeley.edu/~cs152/sp20/handouts/sp20/riscv-v-spec.pdf\"\
    )\u223Ccs152/ [sp20/handouts/sp20/riscv-v-spec.pdf\".](\"https://inst.eecs.berkeley.edu/~cs152/sp20/handouts/sp20/riscv-v-spec.pdf\"\
    )\n- <span id=\"page-8-43\"></span>[44] *Intel Advanced Performance Extensions\
    \ (Intel APX) Architecture Specification*, [\"https://www.intel.com/content/www/us/en/developer/articles/](\"\
    https://www.intel.com/content/www/us/en/developer/articles/technical/intel-sdm.html\"\
    \ ) [technical/intel-sdm.html\".](\"https://www.intel.com/content/www/us/en/developer/articles/technical/intel-sdm.html\"\
    \ )\n- <span id=\"page-8-44\"></span>[45] *Intel Advanced Vector Extensions 10\
    \ (Intel AVX10) Architecture Specification*, [\"https://www.intel.com/content/www/us/en/developer/articles/](\"\
    https://www.intel.com/content/www/us/en/developer/articles/technical/intel-sdm.html\"\
    \ ) [technical/intel-sdm.html\".](\"https://www.intel.com/content/www/us/en/developer/articles/technical/intel-sdm.html\"\
    \ )\n- <span id=\"page-8-45\"></span>[46] *NVIDIA Deep Learning Accelerator (NVDLA),\
    \ Hardware Architectural Specification*, [\"http://nvdla.org/hw/v1/hwarch.html\"\
    .](\"http://nvdla.org/hw/v1/hwarch.html\")\n- <span id=\"page-8-46\"></span>[47]\
    \ *TileLink Specification*, [\"https://www.sifive.com/document-file/](\"https://www.sifive.com/document-file/tilelink-spec-1.9.3\"\
    ) [tilelink-spec-1.9.3\".](\"https://www.sifive.com/document-file/tilelink-spec-1.9.3\"\
    )\n- <span id=\"page-8-47\"></span>[48] *AMBA5 CHI Architecture Specification*,\
    \ [\"https://developer.arm.com/](\"https://developer.arm.com/documentation/ihi0050/latest/\"\
    ) [documentation/ihi0050/latest/\".](\"https://developer.arm.com/documentation/ihi0050/latest/\"\
    )\n- <span id=\"page-8-48\"></span>[49] *AMBA5 ACE Protocol Specification (superseded\
    \ by CHI)*, [\"https://](\"https://developer.arm.com/documentation/ihi0022/hc\"\
    \ ) [developer.arm.com/documentation/ihi0022/hc\".](\"https://developer.arm.com/documentation/ihi0022/hc\"\
    \ )\n- <span id=\"page-8-49\"></span>[50] *AMBA5 AXI Protocol Specification*,\
    \ [\"https://developer.arm.com/](\"https://developer.arm.com/documentation/ihi0022/latest\"\
    ) [documentation/ihi0022/latest\".](\"https://developer.arm.com/documentation/ihi0022/latest\"\
    )\n- <span id=\"page-8-50\"></span>[51] *AMBA4 AXI and ACE Protocol Specification*,\
    \ [\"https://developer.arm.](\"https://developer.arm.com/documentation/ihi0022/e\"\
    \ ) [com/documentation/ihi0022/e\".](\"https://developer.arm.com/documentation/ihi0022/e\"\
    \ )\n- <span id=\"page-8-51\"></span>[52] *10GE MAC Core Specification*, [\"https://opencores.org/ocsvn/xge](\"\
    https://opencores.org/ocsvn/xge_mac/xge_mac/trunk/doc/xge_mac_spec.pdf\" ) mac/\
    \ xge [mac/trunk/doc/xge](\"https://opencores.org/ocsvn/xge_mac/xge_mac/trunk/doc/xge_mac_spec.pdf\"\
    \ ) mac spec.pdf\".\n- <span id=\"page-8-52\"></span>[53] *Ethernet IP Core Specification*,\
    \ [\"http://www.cprover.org/firmware/doc/](\"http://www.cprover.org/firmware/doc/ethoc/eth_speci.pdf\"\
    \ ) ethoc/eth [speci.pdf\".](\"http://www.cprover.org/firmware/doc/ethoc/eth_speci.pdf\"\
    \ )\n- <span id=\"page-8-53\"></span>[54] *I2C-Master Core Specification*, [\"\
    http://www.cprover.org/firmware/doc/](\"http://www.cprover.org/firmware/doc/ethoc/eth_speci.pdf\"\
    \ ) ethoc/eth [speci.pdf\".](\"http://www.cprover.org/firmware/doc/ethoc/eth_speci.pdf\"\
    \ )\n- <span id=\"page-8-54\"></span>[55] *UART to Bus Core Specifications*, [\"\
    https://opencores.org/usercontent/](\"https://opencores.org/usercontent/doc/1330166592\"\
    \ ) [doc/1330166592\".](\"https://opencores.org/usercontent/doc/1330166592\" )\n\
    - <span id=\"page-8-55\"></span>[56] *Elliptic Curve Group Core Specification*,\
    \ [\"https://opencores.org/projects/](\"https://opencores.org/projects/ecg\" )\
    \ [ecg\".](\"https://opencores.org/projects/ecg\" )\n- <span id=\"page-8-56\"\
    ></span>[57] *Tate Bilinear Pairing Core Specification*, [\"https://opencores.org/projects/](\"\
    https://opencores.org/projects/pairing\" ) [pairing\".](\"https://opencores.org/projects/pairing\"\
    \ )\n- <span id=\"page-8-57\"></span>[58] *Tiny Tate Bilinear Pairing Core Specification*,\
    \ [\"https://opencores.org/](\"https://opencores.org/projects/tiny_tate_bilinear_pairing\"\
    \ ) [projects/tiny](\"https://opencores.org/projects/tiny_tate_bilinear_pairing\"\
    \ ) tate bilinear pairing\".\n- <span id=\"page-8-58\"></span>[59] *AES Core Specification*,\
    \ [\"https://opencores.org/usercontent/doc/](\"https://opencores.org/usercontent/doc/1354351714\"\
    \ ) [1354351714\".](\"https://opencores.org/usercontent/doc/1354351714\" )\n-\
    \ <span id=\"page-8-59\"></span>[60] *SHA3 Core Specification*, [\"https://opencores.org/usercontent/doc/](\"\
    https://opencores.org/usercontent/doc/1359445372\" ) [1359445372\".](\"https://opencores.org/usercontent/doc/1359445372\"\
    \ )\n- <span id=\"page-8-13\"></span>[61] *RISC-V Specifications*, [\"https://riscv.org/technical/specifications/\"\
    .](\"https://riscv.org/technical/specifications/\" )\n- <span id=\"page-8-60\"\
    ></span>[62] C. Celio, D. A. Patterson, and K. Asanovic, \"The berkeley out-of-order\
    \ machine (boom): An industry-competitive, synthesizable, parameterized risc-v\
    \ processor,\" *EECS Department, University of California, Berkeley, Tech. Rep.\
    \ UCB/EECS-2015-167*, 2015.\n- <span id=\"page-8-61\"></span>[63] Y. Xu, Z. Yu,\
    \ D. Tang, G. Chen, L. Chen, L. Gou, Y. Jin, Q. Li, X. Li, Z. Li *et al.*, \"\
    Towards developing high performance risc-v processors using agile methodology,\"\
    \ in *MICRO*, 2022.\n- <span id=\"page-8-62\"></span>[64] J. White, Q. Fu, S.\
    \ Hays, M. Sandborn, C. Olea, H. Gilbert, A. Elnashar, J. Spencer-Smith, and D.\
    \ C. Schmidt, \"A prompt pattern catalog to enhance prompt engineering with chatgpt,\"\
    \ *arXiv preprint arXiv:2302.11382*, 2023.\n- <span id=\"page-8-63\"></span>[65]\
    \ *verified calendar.v in RTLLM*, [\"https://github.com/hkust-zhiyao/](\"https://github.com/hkust-zhiyao/RTLLM/blob/main/calendar/verified_calendar.v\"\
    \ ) [RTLLM/blob/main/calendar/verified](\"https://github.com/hkust-zhiyao/RTLLM/blob/main/calendar/verified_calendar.v\"\
    \ ) calendar.v\".\n- <span id=\"page-8-64\"></span>[66] *Amber 2 Core Specification*,\
    \ [\"https://opencores.org/websvn/filedetails?](\"https://opencores.org/websvn/filedetails?repname=amber&path=%2Famber%2Ftrunk%2Fdoc%2Famber-core.pdf\"\
    \ ) [repname=amber&path=%2Famber%2Ftrunk%2Fdoc%2Famber-core.](\"https://opencores.org/websvn/filedetails?repname=amber&path=%2Famber%2Ftrunk%2Fdoc%2Famber-core.pdf\"\
    \ ) [pdf\".](\"https://opencores.org/websvn/filedetails?repname=amber&path=%2Famber%2Ftrunk%2Fdoc%2Famber-core.pdf\"\
    \ )\n- <span id=\"page-8-65\"></span>[67] Y. Chang, X. Wang, J. Wang, Y. Wu, K.\
    \ Zhu, H. Chen, L. Yang, X. Yi, C. Wang, Y. Wang *et al.*, \"A survey on evaluation\
    \ of large language models,\" *arXiv preprint arXiv:2307.03109*, 2023."
  paper_id: 30
  scores:
    originality:
      raw: 4.5
      reason: The paper presents a novel framework, Cocco, that integrates hardware
        mapping and memory optimization in a unique way, contributing to the advancement
        of DNN accelerator design.
    technical_soundness:
      raw: 4.2
      reason: The proposed methods are well-founded in optimization techniques and
        supported by experimental results, although some aspects could benefit from
        further validation across diverse scenarios.
    clarity:
      raw: 4.0
      reason: The writing is generally clear and structured, but certain technical
        details could be elaborated for better comprehension by a broader audience.
    significance:
      raw: 4.3
      reason: The findings have significant implications for the efficiency of DNN
        accelerators, addressing critical issues in memory management and communication
        overhead, which are highly relevant in the field.
    relevance:
      raw: 4.5
      reason: The topic is highly relevant to current trends in hardware design and
        deep learning, making it a timely contribution to the conference.
  total_score: 4.27
  tier: B
- title: "AssertLLM: Generating and Evaluating Hardware Verification Assertions\n\
    \  from Design Specifications via Multi-LLMs"
  abstract: 'In today''s data-centric world, where data fuels numerous application
    domains,

    with machine learning at the forefront, handling the enormous volume of data

    efficiently in terms of time and energy presents a formidable challenge.

    Conventional computing systems and accelerators are continually being pushed to

    their limits to stay competitive. In this context, computing near-memory (CNM)

    and computing-in-memory (CIM) have emerged as potentially game-changing

    paradigms. This survey introduces the basics of CNM and CIM architectures,

    including their underlying technologies and working principles. We focus

    particularly on CIM and CNM architectures that have either been prototyped or

    commercialized. While surveying the evolving CIM and CNM landscape in academia

    and industry, we discuss the potential benefits in terms of performance,

    energy, and cost, along with the challenges associated with these cutting-edge

    computing paradigms.'
  url: http://arxiv.org/abs/2401.14428v1
  keywords: ''
  document: "# **The Landscape of Compute-near-memory and Compute-in-memory: A Research\
    \ and Commercial Overview**\n\n[ASIF ALI KHAN,](HTTPS://ORCID.ORG/0000-0002-5130-9855)\
    \ TU Dresden, Germany\n\n[JO\xC3O PAULO C. DE LIMA,](HTTPS://ORCID.ORG/0000-0001-9295-3519)\
    \ TU Dresden and ScaDS.AI, Germany [HAMID FARZANEH,](HTTPS://ORCID.ORG/0000-0002-1780-6217)\
    \ TU Dresden, Germany [JERONIMO CASTRILLON,](HTTPS://ORCID.ORG/0000-0002-5007-445X)\
    \ TU Dresden and ScaDS.AI, Germany\n\nIn today's data-centric world, where data\
    \ fuels numerous application domains, with machine learning at the forefront,\
    \ handling the enormous volume of data efficiently in terms of time and energy\
    \ presents a formidable challenge. Conventional computing systems and accelerators\
    \ are continually being pushed to their limits to stay competitive. In this context,\
    \ computing near-memory (CNM) and computing-in-memory (CIM) have emerged as potentially\
    \ game-changing paradigms. This survey introduces the basics of CNM and CIM architectures,\
    \ including their underlying technologies and working principles. We focus particularly\
    \ on CIM and CNM architectures that have either been prototyped or commercialized.\
    \ While surveying the evolving CIM and CNM landscape in academia and industry,\
    \ we discuss the potential benefits in terms of performance, energy, and cost,\
    \ along with the challenges associated with these cutting-edge computing paradigms.\n\
    \n# **1 INTRODUCTION**\n\nIn conventional computing systems, the processor and\
    \ memory are two independent entities connected via communication pathways, known\
    \ as buses. When the CPU processes data, it requires fetching it from memory via\
    \ the bus, conducting the necessary computations, and subsequently storing the\
    \ results back in memory. This off-chip communication becomes a limiting factor\
    \ for data-intensive workloads due to the limited transfer rate and high energy\
    \ per bit of buses. For example, the data transfer between the logic (CPUs and\
    \ GPUs) and memory chips (DRAM or flash memory) requires approximately 10\u2013\
    100 times more energy than the logic operation itself [\\[1\\]](#page-33-0). *Compute-near-memory*\
    \ (CNM) and *compute-in-memory* [\\(CIM\\)](#page-39-0) concepts address this\
    \ bottleneck by enabling computations close to where the data resides. This is\
    \ achieved either by implementing CMOS logic on or closer to the memory chip,\
    \ or by leveraging the inherent physical properties of memory devices to perform\
    \ computations in place.\n\n<span id=\"page-0-1\"></span><span id=\"page-0-0\"\
    ></span>The core concept behind [CNM](#page-39-1)[/CIM](#page-39-0) is not entirely\
    \ new. However, the sudden surge in these systems can be attributed to two primary\
    \ factors, namely, the exponential increase in the volume of data required for\
    \ modern applications, and the technological readiness. Recent advancements in\
    \ machine learning, particularly the emergence of generative AI and *large language\
    \ models* [\\(LLM\\)](#page-41-0), demand the processing of terabytes of data,\
    \ substantial computational resources, and complex execution, thus highlighting\
    \ the limitations of traditional computing systems. A recent study revealed that\
    \ OpenAI utilized over 3600 of NVIDIA's HGX A100 servers, totaling around 29,000\
    \ GPUs, to train ChatGPT, resulting in a daily energy consumption of 564 MWh [\\\
    [2\\]](#page-34-0). Projections indicate that by 2027, AI is expected to consume\
    \ between 85 and 124 TWh annually, equivalent to approximately 0.5% of the world's\
    \ total electricity consumption. It is no surprise that Microsoft has announced\
    \ plans to develop its own nuclear reactors to power their data centers [\\[3\\\
    ]](#page-34-1).\n\nCurrently, machine learning applications primarily leverage\
    \ GPU accelerators like A100, H100, GH200, application-specific integrated circuits\
    \ (e.g., Google's TPU), and dataflow processors as in the case of companies like\
    \ GraphCore, Cerebras, Groq, and SambaNova [\\[4\\]](#page-34-2). Over the past\
    \ few\n\nAuthors' addresses: [Asif Ali Khan,](https://orcid.org/0000-0002-5130-9855)\
    \ TU Dresden, Dresden, Germany, asif\\_ali.khan@tu-dresden.de; [Jo\xE3o Paulo\
    \ C. de Lima,](https://orcid.org/0000-0001-9295-3519) TU Dresden and ScaDS.AI,\
    \ Dresden, Germany, joao.lima@tu-dresden.de; [Hamid Farzaneh,](https://orcid.org/0000-0002-1780-6217)\
    \ TU Dresden, Dresden, Germany, hamid.farzaneh@tu-dresden.de; [Jeronimo Castrillon,](https://orcid.org/0000-0002-5007-445X)\
    \ TU Dresden and ScaDS.AI, Dresden, Germany, jeronimo.castrillon@tudresden.de.\n\
    \nyears, [CNM](#page-39-1)[/CIM](#page-39-0) systems have also transcended their\
    \ prototypical stages and successfully entered the market. The timing of these\
    \ advancements in CIM/CNM systems is of paramount importance as it perfectly aligns\
    \ with the AI revolution. As a result, numerous companies have emerged in the\
    \ last few years offering CIM/CNM solutions for various use domains. This surge\
    \ reflects a competitive landscape where these companies are striving to leverage\
    \ the demand and cater to various market segments. All commercially available\
    \ solutions hold the promise of significantly reducing execution time and energy\
    \ consumption for data-intensive workloads.\n\nThis survey explores CNM and CIM\
    \ architectures, detailing their technologies, fundamental concepts, working principles,\
    \ and the evolving landscape in academia and industry. Addressing a broader audience,\
    \ it provides foundational concepts for non-experts while delivering state-of-the-art\
    \ insights for experts in the domain. It also summarizes the impact and challenges\
    \ associated with adopting the novel CIM/CNM computing paradigms. Concretely,\
    \ our discussion revolves around three key aspects:\n\n- (1) **Key technologies\
    \ and concepts:** In CNM systems, a specialized CMOS logic is integrated into\
    \ the memory chip. This logic can be either general-purpose, as in UPMEM systems\
    \ [\\[5\\]](#page-34-3), or domain-specific, as in systems developed by Samsung\
    \ [\\[6,](#page-34-4) [7\\]](#page-34-5) and SK Hynix [\\[8,](#page-34-6) [9\\\
    ]](#page-34-7), integrated within DRAM memory chips. While CNM significantly reduces\
    \ data movement, it does not eliminate it. In contrast, CIM nearly eliminates\
    \ data movement by performing computations within the same devices that store\
    \ the data. A particularly noteworthy operation is the analog dot-product in memory,\
    \ which is of significant importance to the machine learning domain and can be\
    \ performed in constant time. Initially demonstrated in crossbar-configured resistive\
    \ *non-volatile memory* [\\(NVM\\)](#page-40-0) technologies like *phase change\
    \ memory* [\\(PCM\\)](#page-40-1) [\\[10\\]](#page-34-8) and *resistive RAM* [\\\
    (RRAM\\)](#page-40-2) [\\[11\\]](#page-34-9), this concept has also been shown\
    \ with SRAM, *magnetic RAM* [\\(MRAM\\)](#page-40-3) [\\[12\\]](#page-34-10),\
    \ and *ferroelectric field-effect transistor* [\\(FeFET\\)](#page-40-4) [\\[13\\\
    ]](#page-34-11). While other arithmetic, search and boolean logic operations have\
    \ also been demonstrated using CIM, they have received comparatively less attention.\n\
    - <span id=\"page-1-5\"></span><span id=\"page-1-4\"></span><span id=\"page-1-3\"\
    ></span><span id=\"page-1-2\"></span><span id=\"page-1-1\"></span><span id=\"\
    page-1-0\"></span>(2) **Commercial trends:** As the demand for fast and efficient\
    \ computing systems continues to rise, the in-/near-memory computing market is\
    \ experiencing rapid expansion. In 2022, this market was valued at USD 15.5 billion,\
    \ with an anticipated *compound annual growth rate* [\\(CAGR\\)](#page-41-1) of\
    \ 17.5% over the next decade [\\[14\\]](#page-34-12). This growth is underscored\
    \ by the proliferation of startups offering CIM and CNM solutions. Some of these\
    \ companies have secured hundreds of millions of dollars in early funding rounds.\
    \ While many of these companies provide innovative solutions for data-intensive\
    \ applications (dominated by AI inference), there is no clear winner yet. At present,\
    \ these solutions are predominantly based on SRAM technology, although solutions\
    \ based on resistive NVM and flash technologies also exist [\\[15\\]](#page-34-13).\
    \ This trend can be attributed to the mature tools and design processes for SRAM\
    \ compared to emerging NVMs. However, considering the SRAM's scalability aspects\
    \ and its static power consumption, it is likely that NVMs, particularly [PCM,](#page-40-1)\
    \ [RRAM,](#page-40-2) [MRAM,](#page-40-3) and [FeFET,](#page-40-4) will progressively\
    \ replace or complement SRAM as these technologies mature.\n- (3) **Challenges:**\
    \ Although CIM/CNM systems are at the tipping point, they are yet to make substantial\
    \ inroads into the market. The predominant obstacle facing these systems is perhaps\
    \ the absence of a software ecosystem, which renders programmability and optimization\
    \ exceedingly challenging. This is also highlighted by a recent Meta article [\\\
    [16\\]](#page-34-14) stating, *We've investigated applying processing-in-memory\
    \ (PIM) to our workloads and determined there are several challenges to using\
    \ these approaches. Perhaps the biggest challenge of PIM is its programmability*.\
    \ Other challenges requiring attention include: addressing reliability concerns\
    \ associated with emerging NVMs (particularly in the CIM context), developing\n\
    \nnovel performance models, profiling and analysis tools for these systems, which\
    \ could be leveraged to exploit their potential effectively.\n\nThe remainder\
    \ of this paper is structured as follows: Section [2](#page-2-0) explains the\
    \ terminology associated with these domains and provides insights into the conventional\
    \ Von-Neumann computing approach, as well as the emerging data-centric paradigms.\
    \ In Section [3,](#page-4-0) a comprehensive overview of promising memory technologies\
    \ within the context of [CIM](#page-39-0) and [CNM](#page-39-1) systems is provided.\
    \ Section [4](#page-9-0) outlines various common [CIM](#page-39-0) and [CNM](#page-39-1)\
    \ systems including very recent prototype chips from various industries. Lastly,\
    \ Section [5](#page-25-0) presents a comprehensive overview of the commercial\
    \ landscape for these systems (start-ups), discussing their products details,\
    \ target application domain, and funding status. Finally, Section [6](#page-32-0)\
    \ concludes the paper by summarizing our key observations and providing insights\
    \ and recommendations into the future.\n\n# <span id=\"page-2-0\"></span>**2 TERMINOLOGY\
    \ AND BACKGROUND**\n\nThis section highlights the bottleneck in the Von Neumann\
    \ computing model by discussing its working mechanism, motivates the need for\
    \ memory-centric computing, and explains the terminology.\n\n#### <span id=\"\
    page-2-2\"></span>**2.1 Mainstream Von-Neumann Computing**\n\nAs depicted in Figure\
    \ [1a](#page-2-1), the interaction between memory and the processor in the Von\
    \ Neumann architecture is facilitated through address and data buses. However,\
    \ because CPU performance significantly outpaces memory performance, the Von Neumann\
    \ models are often bottlenecked by the memory. To address this challenge and mitigate\
    \ the impact of larger memory access latencies on the CPU, modern processors incorporate\
    \ a tiered hierarchy of caches. Caches are smaller memory units that, while being\
    \ much smaller compared to main memory and storage, are notably faster. The first-level\
    \ cache (L1) is typically integrated onto the CPU chip and operates nearly at\
    \ CPU speed, enabling single-cycle access. L2 cache is usually shared by multiple\
    \ cores and can vary in location depending on the design goals. Some systems even\
    \ include an L3 cache, usually larger and situated off-chip.\n\n<span id=\"page-2-1\"\
    ></span>![](_page_2_Figure_7.jpeg)\n\nFig. 1. (a) Conventional computing system\
    \ where an operation is performed on data in the CPU (b) Memory-centric design\
    \ where is computed in the proximity of and CPU is mainly working as a control\
    \ unit [\\[17\\]](#page-34-15).\n\n<span id=\"page-3-8\"></span>Note that Von\
    \ Neumann architectures are characterized by the sequential execution of instructions.\
    \ However, in multi-core CPU systems, parallel execution of instructions at various\
    \ levels of granularity, including instruction-level, data-level, and thread-level\
    \ parallelisms, is supported. To enhance performance and energy efficiency in\
    \ resource-constrained systems, specialized accelerators are often developed and\
    \ integrated on the same chip. For instance, in application domains such as embedded\
    \ systems, digital signal processing, and networking, *multiprocessor system-on-chip*\
    \ [\\(MPSoC\\)](#page-40-5) architectures are employed. These integrate multiple\
    \ processor cores, memory, input/output interfaces, and potentially specialized\
    \ hardware accelerators, enabling parallel execution of different tasks to meet\
    \ specific constraints.\n\nAlthough these designs may significantly enhance performance\
    \ when compared to conventional CPU-only systems, the underlying design principle\
    \ remains CPU-centric and follows the Von Neumann model of execution. Consequently,\
    \ the performance improvements, largely resulting from concurrent execution, heavily\
    \ rely on the nature of the application. In cases where an application is memory-bound,\
    \ i.e., most of the execution time is spent on the memory accesses and not on\
    \ the actual compute operations, the shared data bus is fully occupied and becomes\
    \ a bottleneck. Even for compute-bound applications, where these architectures\
    \ can yield substantial gains in execution time, power consumption remains largely\
    \ unaffected and might even increase due to the complex structure of these systems.\n\
    \n#### **2.2 Memory-centric computing**\n\nUnlike conventional computing systems\
    \ where CPU has a central role and is responsible for all computations, most computations\
    \ in the memory-centric designs are performed within or near memory. As depicted\
    \ in Figure [1b](#page-2-1), the core concept revolves around minimizing data\
    \ transfer on the bus by relocating a substantial share of computations closer\
    \ to the data (memory). The CPU's primary role becomes issuing commands and handling\
    \ computations that cannot be effectively executed in close proximity to the memory.\n\
    \nThe concept of memory-centric computing is not a novel one, but it has experienced\
    \ a significant surge in recent years. Consequently, various terms have emerged,\
    \ often referring to the same idea, and more detailed classifications have been\
    \ introduced in architectural designs. This section aims to clarify the terminology\
    \ surrounding these approaches.\n\n<span id=\"page-3-7\"></span><span id=\"page-3-5\"\
    ></span>*2.2.1 Compute-in-memory.* Computing systems can be broadly divided into\
    \ two categories: *compute-in-memory* [\\(CIM\\)](#page-39-0) systems and *compute-outside-memory*\
    \ [\\(COM\\)](#page-39-2) systems (see Section [2.1\\)](#page-2-2). In the literature,\
    \ there are different names for similar things. These architectures are often\
    \ named based on (1) the *location* of the compute units within the memory hierarchy\
    \ (near cache or near main memory), or (2) based on the underlying paradigm, i.e.,\
    \ whether the memory device itself is used to implement computation [\\(CIM\\\
    )](#page-39-0), or whether extra CMOS-logic is added near the memory to perform\
    \ computations (*compute-near-memory* [\\(CNM\\)](#page-39-1)). Figure [2](#page-4-1)\
    \ shows an overview of different processor and memory system configurations. A\
    \ compute operation can be a logic operation or an arithmetic operation such as\
    \ addition and multiplication. [CIM](#page-39-0) systems are also frequently referred\
    \ to as *in-memory-computing* [\\(IMC\\)](#page-39-3), *in-memory-processing*\
    \ [\\(IMP\\)](#page-39-4), *processing-in-memory* [\\(PIM\\)](#page-39-5), *processing-using-memory*\
    \ [\\(PUM\\)](#page-39-6) or *logic-in-memory* [\\(LIM\\)](#page-39-7) systems\
    \ [\\[18\\]](#page-34-16). For the purposes of this report, we will use the term\
    \ [CIM.](#page-39-0)\n\n<span id=\"page-3-6\"></span><span id=\"page-3-4\"></span><span\
    \ id=\"page-3-3\"></span><span id=\"page-3-2\"></span><span id=\"page-3-1\"></span><span\
    \ id=\"page-3-0\"></span>*2.2.2 Compute-near-memory.* When operations are computed\
    \ outside the memory [\\(COM\\)](#page-39-2) using conventional computing cores\
    \ (Fig [2.](#page-4-1)a), the architecture is a conventional Von Neumann system\
    \ (see Section [2.1\\)](#page-2-2). On the other hand, if the computations are\
    \ performed outside the memory but with a dedicated logic unit connected to the\
    \ memory module via a high-bandwidth channel (Fig [2.](#page-4-1)b), the system\
    \ is referred to as a *compute-near-memory* [\\(CNM\\)](#page-39-1) or *near-memory-computing*\
    \ [\\(NMC\\)](#page-39-8) or\n\n<span id=\"page-4-3\"></span><span id=\"page-4-2\"\
    ></span>*near-memory-processing* [\\(NMP\\)](#page-39-9), or *processing-near-memory*\
    \ [\\(PNM\\)](#page-39-10) system. In this report, we will restrict ourselves\
    \ to calling it [CNM.](#page-39-1)\n\n<span id=\"page-4-5\"></span>In the [CIM](#page-39-0)\
    \ category, computations can be carried out using memory cells within the memory\
    \ array, known as CIM-array [\\(CIM-A\\)](#page-39-11) (see Fig [2.](#page-4-1)d).\
    \ Alternatively, computations can occur in the memory peripheral circuitry, termed\
    \ CIM-peripheral [\\(CIM-P\\)](#page-39-12) (see Fig [2.](#page-4-1)c).\n\n<span\
    \ id=\"page-4-6\"></span><span id=\"page-4-1\"></span>![](_page_4_Figure_3.jpeg)\n\
    \nFig. 2. High-level overview of systems where computation is performed a) COM\
    \ (mainstream computing): outside of memory system, b) [CNM:](#page-39-1) using\
    \ a logic connected to the memory via the memory high-bandwidth channel, c) [CIM-P:](#page-39-12)\
    \ in the memory peripheral circuitry, and d) [CIM-A:](#page-39-11) using memory\
    \ cells within the memory array.\n\nIn [CIM-A,](#page-39-11) memory cells are\
    \ often modified to support logic design, e.g., in [\\[19\\]](#page-34-17). Sometimes,\
    \ it also necessitates changes to the periphery to support the modified cells.\
    \ Therefore, some literature further divides the [CIM-A](#page-39-11) designs\
    \ into basic [CIM-A](#page-39-11) that do not require any modifications to the\
    \ periphery, e.g., [\\[20\\]](#page-34-18), and hybrid [CIM-A](#page-39-11) that\
    \ requires support from the peripheral circuit. A well-known example of a hybrid\
    \ [CIM-A](#page-39-11) is the MAGIC design [\\[19\\]](#page-34-17) that requires\
    \ extending the peripheral circuit to write multiple memory rows simultaneously.\n\
    \n<span id=\"page-4-10\"></span><span id=\"page-4-7\"></span><span id=\"page-4-4\"\
    ></span>Typical examples of [CIM-P](#page-39-12) architectures are crossbars employing\
    \ *analog-to-digital converter*s [\\(ADCs](#page-39-13)) and digital-to-analog\
    \ converters [\\(DACs](#page-39-14)) to implement *matrix-vector multiplication*\
    \ [\\(MVM\\)](#page-41-2) and other logic operations [\\[21,](#page-34-19) [22\\\
    ]](#page-34-20). Additionally, [CIM-P](#page-39-12) designs employing customized\
    \ sense amplifiers also exist [\\[23\\]](#page-34-21). Similar to [CIM-A,](#page-39-11)\
    \ [CIM-P](#page-39-12) can be either basic, as in Pinatubo [\\[23\\]](#page-34-21),\
    \ requiring no changes to the memory array, or hybrid, as seen in ISAAC [\\[22\\\
    ]](#page-34-20). A summary of the terminology's classification is presented in\
    \ Figure [3.](#page-5-0)\n\nBoth [CIM-A](#page-39-11) and [CIM-P](#page-39-12)\
    \ can also be used together, wherein the memory array calculates partial results\
    \ that are later post-processed or accumulated in the peripheral circuit. In such\
    \ cases, it is referred to as a [CIM](#page-39-0) architecture.\n\n#### <span\
    \ id=\"page-4-0\"></span>**3 TECHNOLOGY OVERVIEW**\n\n<span id=\"page-4-9\"></span><span\
    \ id=\"page-4-8\"></span>In this section, we present an overview of the main memory\
    \ cells used in various [CNM](#page-39-1) and [CIM](#page-39-0) systems, encompassing\
    \ both volatile *static random-access memory* [\\(SRAM\\)](#page-40-6), *dynamic\
    \ random-access memory* [\\(DRAM\\)](#page-39-15)) and non-volatile types such\
    \ as [PCM,](#page-40-1) [MRAM,](#page-40-3) [RRAM](#page-40-2) and [FeFETs](#page-40-4).\
    \ These memory technologies are versatile enough to serve as main memory for data\
    \ storage and support [CNM](#page-39-1) without requiring any modification to\
    \ the memory chips. Additionally, we explore how these memory cells can be leveraged\
    \ for in-memory computation, considering technological aspects such as performance,\
    \ energy consumption, lifetime, CMOS compatibility, and other relevant factors.\
    \ Before going into the individual technologies, let us first explain the different\
    \ components of the memory subsystem.\n\n<span id=\"page-5-0\"></span>![](_page_5_Figure_1.jpeg)\n\
    \nFig. 3. [CIM](#page-39-0) and [CNM](#page-39-1) classification.\n\n#### **3.1\
    \ Memory subsystem**\n\nThe memory subsystem typically consists of a four-level\
    \ hierarchy, each with its own characteristics and access time. The fastest and\
    \ smallest level is the CPU registers, while the slowest one in the hierarchy\
    \ is storage devices (HDD/SSD). The [CNM/](#page-39-1)[CIM](#page-39-0) concepts\
    \ have been proposed at different levels in the memory hierarchy, e.g., in-cache\
    \ computing [\\[24\\]](#page-34-22), in-DRAM [\\[25\\]](#page-35-0), in-storage\
    \ computing [\\[26\\]](#page-35-1). However, [CNM](#page-39-1)[/CIM](#page-39-0)\
    \ at the main memory level have rightly gained more attention than others.\n\n\
    <span id=\"page-5-1\"></span>![](_page_5_Figure_5.jpeg)\n\nFig. 4. Typical [DRAM](#page-39-15)\
    \ system organization.\n\nThe main memory is also typically organized hierarchically,\
    \ as shown in Figure [4.](#page-5-1) The main components of the hierarchy are\
    \ memory cells, subarrays, banks, and ranks. A cell is the fundamental working\
    \ unit and serves as a building block to form arrays. An array is a 2D grid of\
    \ cells connected via *word lines* and *bitlines*. A word line is a horizontal\
    \ line that connects all the memory cells in a row, while bitlines, on the other\
    \ hand, connect all cells in a column. Memory cells are placed at the intersection\
    \ of word lines and bitlines. The combination of a specific word line and a bit\
    \ line uniquely identifies each memory cell. When a particular memory cell needs\
    \ to be accessed, the corresponding word line and bit line are activated, allowing\
    \ data to be read from or written into that cell.\n\n#### **3.2 [DRAM](#page-39-15)**\n\
    \n[DRAM](#page-39-15) is the most mature and widely used memory technology today.\
    \ A [DRAM](#page-39-15) cell is composed of a transistor and a capacitor. When\
    \ the capacitor is fully charged, it represents the logical value 1, while a discharged\
    \ capacitor represents the logical value 0. To access data from the [DRAM](#page-39-15)\
    \ array, the memory controller brings a particular row into the row buffer by\
    \ sending an *activate* command. A *read* command is then issued to read specific\
    \ column(s) from the row buffer and put them on the bus.\n\n[DRAM](#page-39-15)\
    \ has scaled nicely for decades and has been used across application domains and\
    \ systems ranging from HPC to portable devices. However, it is presently facing\
    \ several challenges to remain the dominant technology. The increasing demand\
    \ for higher capacity has put tremendous pressure on the [DRAM](#page-39-15) capacitor\
    \ size to shrink which makes it susceptible to errors. Also, the increase in capacity\
    \ is significantly increasing the refresh power budget.\n\n<span id=\"page-6-4\"\
    ></span><span id=\"page-6-3\"></span>To address the escalating demands for higher\
    \ bandwidth in modern applications, 3D stacked [DRAM](#page-39-15) architectures,\
    \ such as *high bandwidth memory* [\\(HBM\\)](#page-40-7) (see Section [4.1.3\\\
    )](#page-12-0), have been proposed. These architectures consist of stacked [DRAM](#page-39-15)\
    \ dies atop a logic layer, interconnected through *through-silicon vias* [\\(TSVs\\\
    )](#page-41-3), resulting in remarkably higher bandwidth. These structures are\
    \ also employed in a series of [CNM](#page-39-1) solutions, where the logic layer\
    \ is used to implement custom logic and perform computations in closer proximity\
    \ to the data [\\[27,](#page-35-2) [28\\]](#page-35-3).\n\nFrom the [CIM](#page-39-0)\
    \ perspective, the majority of in[-DRAM](#page-39-15) implementations rely on\
    \ charge sharing, wherein multiple rows are activated simultaneously. The shared\
    \ charge is then utilized in a controlled manner to carry out various logic and\
    \ data copy operations [\\[25\\]](#page-35-0). Moreover, cleverly manipulating\
    \ the memory timing parameters, deviating from standard timings, has also been\
    \ employed to implement different logic operations [\\[29\\]](#page-35-4).\n\n\
    #### **3.3 SRAM**\n\nSRAM is another mature memory technology that provides fast\
    \ and efficient memory accesses. It is commonly used in caches, register files,\
    \ and other high-speed memory applications where speed and low access latency\
    \ are critical. An [SRAM](#page-40-6) cell consists of multiple transistors arranged\
    \ in a specific configuration to hold one bit of data. The most common configuration\
    \ of an [SRAM](#page-40-6) cell is a pair of cross-coupled inverters that are\
    \ connected in a feedback loop, forming a latch.\n\n<span id=\"page-6-0\"></span>![](_page_6_Figure_8.jpeg)\n\
    \n(a) A sample 6T [SRAM](#page-40-6) cell [\\[30\\]](#page-35-5) (b) Metal-oxide\
    \ RRAM cell [\\[31\\]](#page-35-6) (c) [FeFET](#page-40-4) device [\\[32\\]](#page-35-7)\n\
    \n<span id=\"page-6-5\"></span><span id=\"page-6-2\"></span><span id=\"page-6-1\"\
    ></span>Fig. 5. Cell structures of various memory technologies\n\nTo read and\
    \ store data on the cell, the bitline terminals, *bitline* [\\(BL\\)](#page-39-16)\
    \ and *bitline bar* [\\(BLB\\)](#page-39-17) (see Fig. [5a\\)](#page-6-0), are\
    \ precharged and discharged, and the wordline(*word line* [\\(WL\\)](#page-41-4))\
    \ is activated or deactivated depending on the values reading/writing from/to\
    \ the cell.\n\nThere have been proposals for in[-SRAM](#page-40-6) computing,\
    \ especially at the last-level cache, which can be considerably slower compared\
    \ to the L1 cache (e.g., by an order of magnitude). Similar to [DRAM,](#page-39-15)\
    \ most in[-SRAM](#page-40-6) computing architectures also leverage charge sharing\
    \ in the bitlines. Specifically, precharging the bitlines in a controlled manner\
    \ and activating multiple rows simultaneously enables\n\nperforming logic operations\
    \ [\\[24\\]](#page-34-22). For bitwise multiplication in [SRAM,](#page-40-6) research\
    \ has demonstrated that the amplitude of the input voltage at the [WL](#page-41-4)\
    \ directly influences the discharge rate of the [BLB.](#page-39-17) The voltage\
    \ discharge on [BLB,](#page-39-17) achieved within a specific timeframe, effectively\
    \ represents a one-bit multiplication of the data stored in the [SRAM](#page-40-6)\
    \ cell.\n\n#### **3.4 Phase change memory [\\(PCM\\)](#page-40-1)**\n\n[PCM](#page-40-1)\
    \ is resistive memory technology that employs reversible phase changes in materials\
    \ to store data. The earliest demonstration of a 256-bit [PCM](#page-40-1) prototype\
    \ dates back to 1970 [\\[33\\]](#page-35-8). Today, [PCM](#page-40-1) stands as\
    \ one of the most extensively researched [NVM](#page-40-0) technologies. A [PCM](#page-40-1)\
    \ device comprises a phase-changing material sandwiched between two electrodes\
    \ (very similar to Fig. [5b\\)](#page-6-0), which transitions between crystalline\
    \ (low resistance state) and amorphous (high resistance state) phases. These two\
    \ resistance states represent binary logic states, i.e., 1 and 0.\n\nTypically,\
    \ [PCM](#page-40-1) requires a relatively high programming current (>200), but\
    \ this can be mitigated to less than 10 by scaling down the device size [\\[10,](#page-34-8)\
    \ [34\\]](#page-35-9). As [PCM](#page-40-1) stores data based on resistance, it\
    \ can be programmed to encompass more than two resistance states, allowing for\
    \ multi-level cells to represent more than a single bit of information. Nevertheless,\
    \ relying on multiple resistance states for prolonged periods poses challenges,\
    \ as the device resistance tends to drift over time, making it difficult to discern\
    \ between resistance states.\n\n#### **3.5 Resistive RAM [\\(RRAM\\)](#page-40-2)**\n\
    \n[RRAM](#page-40-2) is another class of resistive memory technologies that utilizes\
    \ the resistive switching phenomenon in metal oxide materials to store data [\\\
    [11\\]](#page-34-9). As shown in Fig. [5b,](#page-6-0) a typical [RRAM](#page-40-2)\
    \ cell comprises a top and a bottom electrode with a thin oxide layer sandwiched\
    \ in between. To achieve resistive switching, a high electric field is applied\
    \ to the [RRAM](#page-40-2) cell, leading to the creation of oxygen vacancies\
    \ within the metal oxide layer. This process results in the formation of conductive\
    \ filaments, causing the device state to transition from a high resistance to\
    \ a low resistance (set) state. To revert to the high resistance (reset) state,\
    \ the device is subjected to , which breaks the conductive filament, allowing\
    \ the oxygen ions to migrate back to the bulk. Compared to [PCM,](#page-40-1)\
    \ [RRAM](#page-40-2) exhibits several advantages, including higher write endurance\
    \ (>1010), faster write operations, larger resistance on-off ratios, and improved\
    \ scalability prospects [\\[11\\]](#page-34-9). However, ReRAM does suffer from\
    \ inconsistent electrical characteristics, meaning it exhibits larger variations\
    \ in resistance across different devices [\\[10\\]](#page-34-8).\n\n#### <span\
    \ id=\"page-7-0\"></span>**3.6 Magnetic RAM (MRAM)**\n\n<span id=\"page-7-3\"\
    ></span><span id=\"page-7-2\"></span><span id=\"page-7-1\"></span>[MRAM](#page-40-3)\
    \ store data in nano-scale ferromagnetic elements via magnetic orientation [\\\
    [35\\]](#page-35-10). An [MRAM](#page-40-3) cell is a *magnetic tunnel junction*\
    \ [\\(MTJ\\)](#page-40-8) device composed of two ferromagnetic layers, namely\
    \ a fixed reference layer and a free layer, separated by an insulating layer.\
    \ The free layer holds the data bit, and reading it involves passing an electric\
    \ current and measuring its resistance. For data writing into an [MRAM](#page-40-3)\
    \ cell, various techniques can be used. The most common method is the *spin-transfer-torque*\
    \ [\\(STT\\)](#page-40-9), which utilizes spin-polarized electric current to change\
    \ the free layer's magnetic orientation. *Spin-orbit-torque* [\\(SOT\\)](#page-40-10)[-MRAM,](#page-40-3)\
    \ on the other hand, uses an in-plane current through the heavy metal layer to\
    \ generate a spin current that exerts a torque on the magnetization of the free\
    \ layer.The relative orientations of the free and fixed layers result in different\
    \ resistance states. [MRAM](#page-40-3) exhibits virtually unlimited endurance\
    \ and acceptable access latency. However, it is faced with challenges such as\
    \ a larger cell size and a smaller on/off resistance ratio, limiting an [MRAM](#page-40-3)\
    \ cell to store only one bit of data [\\[36\\]](#page-35-11).\n\nCNM/CIM Landscape\
    \ 9\n\n<span id=\"page-8-0\"></span>\n\n| Device                 | SRAM      |\
    \ DRAM        | RRAM        | PCM         | STT-MRAM | FeFET  |\n|------------------------|-----------|-------------|-------------|-------------|----------|--------|\n\
    | Write time             | 1 \u2212 10\U0001D45B\U0001D460  | > 20\U0001D45B\U0001D460\
    \      | > 10\U0001D45B\U0001D460      | \u223C 50\U0001D45B\U0001D460      |\
    \ > 10\U0001D45B\U0001D460   | \u223C 10\U0001D45B\U0001D460 |\n| Read time  \
    \            | 1 \u2212 10\U0001D45B\U0001D460  | > 20\U0001D45B\U0001D460   \
    \   | > 10\U0001D45B\U0001D460      | > 10\U0001D45B\U0001D460      | > 10\U0001D45B\
    \U0001D460   | \u223C 10\U0001D45B\U0001D460 |\n| Drift                  | No\
    \        | No          | Weak        | Yes         | No       | No     |\n| Write\
    \ energy (per bit) | 1 \u2212 10\U0001D453 \U0001D43D | 10 \u2212 100\U0001D453\
    \ \U0001D43D | 0.1 \u2212 1\U0001D45D\U0001D43D   | 100\U0001D45D\U0001D43D  \
    \     | \u223C 100\U0001D453 \U0001D43D | > 1\U0001D453 \U0001D43D |\n| Density\
    \                | Low       | Medium      | High        | High        | Medium\
    \   | High   |\n| Endurance              | > 1016    | > 1016      | > 105 \u2212\
    \ 108 | > 105 \u2212 108 | > 1015   | > 1015 |\n| Retention              | Low\
    \       | Very Low    | Medium      | long        | Medium   | long   |\n\nTable\
    \ 1. A comparison of the key features across different mainstream CMOS and emerging\
    \ memristive technologies [\\[39\\]](#page-35-12).\n\n### **3.7 Ferroelectric\
    \ Field-Effect Transistor (FeFET)**\n\n<span id=\"page-8-1\"></span>Since the\
    \ discovery of ferroelectricity in hafnium oxide, [FeFETs](#page-40-4) have received\
    \ considerable attention. [FeFETs](#page-40-4) are non-volatile three-terminal\
    \ devices, offering high / ratios and low read voltage. Unlike *metal-oxide-semiconductor*\
    \ [\\(MOS\\)](#page-40-11)-FETs, [FeFETs](#page-40-4) incorporate a ferroelectric\
    \ oxide layer in the gate stack, as shown in Figure [5c.](#page-6-0) The nonvolatility\
    \ arises from hysteresis due to the coupling between the ferroelectric and CMOS\
    \ capacitances (C and C ). The three-terminal structure of [FeFETs](#page-40-4)\
    \ enables separate read and write paths. Reading involves sensing the drain-source\
    \ current, while writing involves switching the ferroelectric polarization with\
    \ an appropriate V voltage. Unlike two-terminal devices with variable resistance,\
    \ [FeFETs](#page-40-4) do not require a drain-source current during the writing\
    \ process, leading to low writing energy consumption [\\[37\\]](#page-35-13).\
    \ There are various [CIM](#page-39-0) architectures exploiting different properties\
    \ of [FeFETs](#page-40-4). For instance, for boolean operations, [\\[37\\]](#page-35-13)\
    \ proposes precharging the bitlines followed by simultaneous activation of the\
    \ target rows, and using differential sense amplifiers to discern the output.\n\
    \n#### **3.8 Comparison and discussion**\n\nTable [1](#page-8-0) presents a comparison\
    \ between mainstream and emerging memory devices, discussed in the preceding sections,\
    \ with respect to performance, reliability, and energy consumption, among others.\
    \ This analysis gives insights into their suitability for different application\
    \ domains. It is clear that no single memory device can optimize all metrics.\
    \ Nonetheless, recent investigations into machine learning use cases show that\
    \ different phases of machine learning tasks demand different memory device properties,\
    \ potentially offering the opportunity to employ various devices for various application\
    \ domains (or tasks within a domain) and achieve the best results [\\[38\\]](#page-35-14).\n\
    \n[PCM,](#page-40-1) [RRAM,](#page-40-2) [MRAM](#page-40-3) and [FeFET](#page-40-4)\
    \ fall under the category of memristive technologies, where devices can exhibit\
    \ multiple resistance states. This characteristic has been effectively leveraged\
    \ to perform [MVM,](#page-41-2) as depicted in Figure [15a.](#page-17-0) Although\
    \ the analog computation may not be entirely precise, some loss in accuracy is\
    \ acceptable in many application domains, particularly for machine learning applications.\
    \ Numerous [CIM](#page-39-0) architectures have been proposed using this technique\
    \ to accelerate neural network models (see Section [4.2\\)](#page-15-0).\n\nFigure\
    \ [6](#page-9-1) shows the importance of various device properties for neural\
    \ network training and inference. For training, frequent weight updates within\
    \ the memory are crucial, making memory technologies like [PCM](#page-40-1) and\
    \ [RRAM,](#page-40-2) with limited endurance and expensive write operations, poorly\
    \ suitable for training acceleration. However, in inference, where operations\
    \ are predominantly read-based with minimal writes to the crossbar array, the\
    \ same technology could outperform others by orders of magnitude. Similarly, retention\
    \ for training is the least important but is critical for inference.\n\nThe arguments\
    \ around Figure [6](#page-9-1) generally hold true for other metrics and other\
    \ application domains. Although machine learning constitutes a significant area,\
    \ it is not the only domain benefiting from the\n\n<span id=\"page-9-1\"></span>![](_page_9_Figure_1.jpeg)\n\
    \nFig. 6. A spider chart showing the importance of different attributes of the\
    \ NVM technologies for the neural network training and inference. More distance\
    \ from the center means more important [\\[38\\]](#page-35-14).\n\n[CIM](#page-39-0)\
    \ paradigm. Many other data-intensive application domains have also effectively\
    \ exploited the [CIM](#page-39-0) paradigm. Figure [7](#page-9-2) shows a landscape\
    \ of [CIM](#page-39-0) applications, emphasizing precision considerations, computational\
    \ complexity and memory access requirements. These applications are classified\
    \ into three categories based on their precision demands. This data pertains to\
    \ 2020. Over the past two years, additional application domains, such as databases,\
    \ bioinformatics, and solving more complex algebraic tasks, have gained significant\
    \ attention as well.\n\n<span id=\"page-9-2\"></span>![](_page_9_Figure_4.jpeg)\n\
    \nFig. 7. The applications landscape for [CIM](#page-39-0) and [CNM](#page-39-1)\
    \ [\\[17\\]](#page-34-15).\n\n# <span id=\"page-9-0\"></span>**4 SELECTED ARCHITECTURES**\n\
    \nIn this section, we discuss some prevalent [CIM](#page-39-0) and [CNM](#page-39-1)\
    \ architectures, explaining their programming models and systems integration.\
    \ It is important to highlight that there exist COM accelerators optimized for\
    \ specific domains, achieving throughput similar to [CNM/](#page-39-1)[CIM](#page-39-0)\
    \ counterparts, albeit at the expense of higher energy consumption. These accelerators\
    \ are beyond the scope of this paper.\n\n#### CNM/CIM Landscape 11\n\nThis section\
    \ is structured as follows: In Section [4.1,](#page-10-0) we provide an overview\
    \ of [CNM](#page-39-1) systems, starting with academic designs and progressing\
    \ to commercial [CNM](#page-39-1) systems, including both planar 2D and stacked\
    \ DRAM structures. Section [4.2](#page-15-0) follows a similar organization for\
    \ [CIM](#page-39-0) systems employing various technologies. In Section [4.3,](#page-24-0)\
    \ we conduct a comparative analysis of different [CIM](#page-39-0)[/CNM](#page-39-1)\
    \ systems, while Section [5.20](#page-31-0) outlines the key challenges faced\
    \ by these innovative architectures.\n\n#### <span id=\"page-10-0\"></span>**4.1\
    \ CNM architectures**\n\n<span id=\"page-10-4\"></span>The core principle of compute-near-memory\
    \ is to perform computations in the memory proximity by placing *processing unit*s\
    \ [\\(PUs](#page-40-12)) on/near the memory chip. The first [CNM](#page-39-1)\
    \ architecture dates back to the 1990s that aimed at integrating compute units\
    \ with embedded DRAM on the same chip to achieve higher bandwidth. However, due\
    \ to technological limitations and costly fabrication processes, even the promising\
    \ initial [CNM](#page-39-1) proposals like IRAM [\\[40\\]](#page-35-15), DIVA\
    \ [\\[41\\]](#page-35-16), and FlexRAM [\\[42\\]](#page-35-17) never commercialized.\n\
    \nIn recent years, due to the advancements in integration and die-stacking technologies,\
    \ [CNM](#page-39-1) has regained interest within both industry and academia. [PUs](#page-40-12)\
    \ are being integrated at different locations within memory devices, including\
    \ within the memory chip as well as outside the memory chip on the module level,\
    \ i.e., dual in-line memory module (DIMM). A DIMM typically consists of multiple\
    \ memory chips, each consisting of multiple ranks, banks, and subarrays. It is\
    \ worth noting that some researchers also classify [PUs](#page-40-12) integrated\
    \ at the memory controller level as [CNM.](#page-39-1) However, following the\
    \ classification and terminology adopted in this report, we categorize it under\
    \ the COM class.\n\nIn the following, we explain some of the common [CNM](#page-39-1)\
    \ architectures. We start by examining the planar 2D DRAM-based [CNM](#page-39-1)\
    \ designs, then transition to discussing the 2.5D and 3D DRAM-based [CNM](#page-39-1)\
    \ systems, and ultimately conclude on the NVM-based [CNM](#page-39-1) architectures.\n\
    \n<span id=\"page-10-2\"></span>*4.1.1 The UPMEM system.* UPMEM is a recent commercial\
    \ near-bank [CNM](#page-39-1) system and is publicly available [\\[5\\]](#page-34-3).\
    \ Figure [8](#page-10-1) gives a detailed overview of the UPMEM architecture.\
    \ The memory modules are divided into PIM-enabled memory and main memory (conventional).\
    \ The PIM-enabled memory combines co-processors known as *data processing unit*s\
    \ [\\(DPUs](#page-39-18)) with conventional DDR4 DRAM on the same die.\n\n<span\
    \ id=\"page-10-3\"></span><span id=\"page-10-1\"></span>![](_page_10_Figure_7.jpeg)\n\
    \nFig. 8. An overview of the UPMEM architecture [\\[43\\]](#page-35-18).\n\n[DPUs](#page-39-18)\
    \ are 32-bit general-purpose RISC processors, comprising a 64kB SRAM-based scratchpad\
    \ working memory known as WRAM, a 24kB SRAM-based instruction memory referred\
    \ to as IRAM, and a shared main memory named MRAM, based on DRAM technology. As\
    \ shown in the figure (lower left), each DIMM consists of 16 memory chips, with\
    \ each chip housing 8 banks, and each bank containing one [DPU.](#page-39-18)\
    \ The latest UPMEM systems can support up to 20 DIMMs.\n\n**[DPU-DPU](#page-39-18)\
    \ communication:** [DPUs](#page-39-18) in UPMEM can have up to 24 hardware threads\
    \ called tasklets.\n\n<span id=\"page-11-2\"></span>Within the same [DPU,](#page-39-18)\
    \ tasklets can share data through MRAM and WRAM, however, [DPUs](#page-39-18)\
    \ can not communicate with each other directly and must go through the host for\
    \ any possible data sharing. **Progammability:** For programmability, UPMEM offers\
    \ its own *software development kit* [\\(SDK\\)](#page-40-13) consisting of an\
    \ UPMEM compiler and runtime libraries. [DPU](#page-39-18) programs are written\
    \ in the C language including specific library calls. The runtime library provides\
    \ functions for data and instruction transfers between different memory, e.g.,\
    \ MRAM-IRAM, MRAM-WRAM etc.; executing various functions on the [DPUs](#page-39-18);\
    \ and synchronization (mutex locks, barriers, handshakes, and semaphores).\n\n\
    Although UPMEM claims they have an easily programmable [SDK,](#page-40-13) programming\
    \ the system has several challenges. The programmer is responsible for efficient\
    \ mapping of executions and load-balancing on thousands of [DPUs](#page-39-18),\
    \ managing data transfer, and ensuring coherence of data between CPU and [DPUs](#page-39-18).\n\
    \n*4.1.2 CNM for [MVM](#page-41-2) in DRAM.* McDRAM [\\[44\\]](#page-35-19) and\
    \ MViD [\\[45\\]](#page-35-20) (both involving Samsung Electronics) aimed at accelerating\
    \ machine learning workloads by embedding *multiply-accumulate* [\\(MAC\\)](#page-40-14)\
    \ units within the DRAM bank. Similar to UPMEM, both McDRAM and MViD utilize 2D\
    \ DRAM (LPDDR in these cases) and incorporate [PUs](#page-40-12) within the memory\
    \ banks to exploit the higher internal memory bandwidth. However, unlike UPMEM,\
    \ these architectures are domain-specific and hence employ fixed functional units\
    \ [\\(MACs](#page-40-14)) instead of general-purpose programmable cores.\n\n<span\
    \ id=\"page-11-3\"></span><span id=\"page-11-0\"></span>Figure [9](#page-11-0)\
    \ shows the McDRAM architecture along with the three locations (column decoder,\
    \ bitline *sense amplifier* [\\(SA\\)](#page-41-5)s, and I/O drivers) where MAC\
    \ units were employed and evaluated. Each McDRAM chip consists of 4 banks and\
    \ each bank has four 8-bit MAC units. The multiplication is performed in parallel\
    \ by multiplying rows of the matrix with the input vector.\n\n<span id=\"page-11-1\"\
    ></span>![](_page_11_Figure_5.jpeg)\n\nFig. 9. The McDRAM architecture with three\
    \ possible locations for MAC units [\\[44\\]](#page-35-19).\n\n**Programmability:**\
    \ McDRAM is a fixed-function accelerator and offers a single interface function\
    \ (matmul) that triggers the device driver to configure the control registers\
    \ of the memory controller.It operates in two modes, memory and compute modes,\
    \ determined by a configuration register. In compute mode, McDRAM performs [MVM](#page-41-2)\
    \ tasks. For the management of [MVM](#page-41-2) within compute mode, it introduces\
    \ six novel DRAM commands that leverage existing DRAM I/O signals, rendering no\
    \ modifications to DRAM I/O signals. McDRAM, a fixed-function accelerator, employs\
    \ a single interface function (matmul) triggering the device driver to configure\
    \ memory controller control registers. It operates in two modes, memory and compute,\
    \ determined by a configuration register. In compute mode, McDRAM performs [MVM](#page-41-2)\
    \ introducing six novel DRAM commands for [MVM](#page-41-2) management without\
    \ modifying existing DRAM I/O signals.\n\n<span id=\"page-12-1\"></span>![](_page_12_Figure_2.jpeg)\n\
    \n<span id=\"page-12-3\"></span>Fig. 10. The MViD architecture [\\[45\\]](#page-35-20)\
    \ where each bank has 16 MAC units.\n\nThe MViD architecture depicted in Figure\
    \ [10](#page-12-1) is similar to the McDRAM design and is specifically optimized\
    \ for edge devices. Much like McDRAM, MViD incorporates MAC units within the DRAM\
    \ I/O drivers to capitalize on the internal bandwidth of the DRAM. However, unlike\
    \ McDRAM, MViD introduces a partitioning of memory banks into two categories:\
    \ [MVM](#page-41-2) banks that are equipped for MAC units and two SRAM structures\
    \ to hold the input and output vectors and non[-MVM](#page-41-2) banks (traditional).\
    \ This division enables concurrent access to both types, meaning that multiplication\
    \ operations in [MVM](#page-41-2) banks can occur simultaneously with the CPU\
    \ accesses to the non[-MVM](#page-41-2) banks.\n\n<span id=\"page-12-0\"></span>*4.1.3\
    \ Samsung's CNM systems.* Samsung is probably ahead of everyone in the race for\
    \ commercial [CNM](#page-39-1) systems. In the following, we discuss two of their\
    \ recent promising (and complete) solutions.\n\n**PIM-HBM:** Samsung has recently\
    \ introduced a [CNM](#page-39-1) architecture referred to as Function-in-Memory\
    \ DRAM (FIMDRAM)[\\[6\\]](#page-34-4) or PIM-HBM[\\[7\\]](#page-34-5). It incorporates\
    \ 16 *single-instruction multiple-data* [\\(SIMD\\)](#page-40-15) engines within\
    \ the memory banks, enabling bank-level parallelism. As reported in [\\[7\\]](#page-34-5),\
    \ their design does not disrupt crucial elements on the memory side, such as the\
    \ sub-array and bank in conventional DRAM, making its integration seamless and\
    \ straightforward. Importantly, it does not require any modifications to contemporary\
    \ commercial processor components, including DRAM controllers. It is designed\
    \ for host processors to manage PIM operations via standard DRAM interfaces. This\
    \ feature allows for a straightforward substitution of existing JEDEC-compliant\
    \ DRAM with PIM-DRAM across various systems.\n\n<span id=\"page-12-2\"></span>![](_page_12_Figure_7.jpeg)\n\
    \nFig. 11. Samsung's PIM-HBM (a) HBM die organization (b) Bank coupled with a\
    \ PIM unit (c) PIM unit data path [\\[7\\]](#page-34-5).\n\nWhile Samsung reports\
    \ that their design is compatible with any DRAM family, they have showcased its\
    \ functionality using the 2.5D high bandwidth memory (HBM) DRAM. Figure [11](#page-12-2)\
    \ provides a <span id=\"page-13-1\"></span>high-level view of this architecture.\
    \ Each bank comprises 16 [SIMD](#page-40-15) *floating-point unit*s [\\(FPUs](#page-40-16)),\
    \ with each FPU consisting of a 16-bit floating-point adder and a 16-bit floating-point\
    \ multiplier. Furthermore, each FPU is equipped with data registers (GRFs), control\
    \ and instruction registers (CRF, SRF), and an internal control unit. The internal\
    \ control unit orchestrates operation sequences without necessitating modifications\
    \ to the memory controller. When operating in PIM mode, the PIM execution units\
    \ within all banks simultaneously respond to a standard DRAM column (Read or Write)\
    \ command initiated by the host processor and execute a wide SIMD operation with\
    \ deterministic latency in a lock-step manner.\n\n<span id=\"page-13-2\"></span>**Programmability:**\
    \ PIM-HBM comes with an *instruction set architecture* [\\(ISA\\)](#page-40-17),\
    \ a software stack, and a specific programming model. The software stack presents\
    \ a native execution path that does not require any modifications to the input\
    \ code. The framework takes the high-level representation of an application and\
    \ transforms it into device code. Furthermore, it offers a direct execution path\
    \ that permits direct invocation of various function calls using the \"PIM custom\
    \ op\". The PIM runtime includes a collection of modules responsible for tasks\
    \ like operations offloading, memory allocation, and execution on the FPUs.\n\n\
    HBM-PIM is a commercial accelerator and, as per, Samsung is already used by companies.\
    \ Here is an excerpt from Samsung's newsroom:\n\n*\"Xilinx has been collaborating\
    \ with Samsung Electronics to enable high-performance solutions for data center,\
    \ networking, and real-time signal processing applications starting with the Virtex\
    \ UltraScale+ HBM family, and recently introduced our new and exciting Versal\
    \ HBM series products,\" said Arun Varadarajan Rajagopal, senior director, Product\
    \ Planning at Xilinx, Inc. \"We are delighted to continue this collaboration with\
    \ Samsung as we help to evaluate HBM-PIM systems for their potential to achieve\
    \ major performance and energy-efficiency gains in AI applications.\"*\n\n**AxDIMM\
    \ (by Samsung-Facebook:)** Samsung is also working on the development of an FPGAenabled\
    \ [CNM](#page-39-1) platform named AxDIMM. In collaboration with Facebook, this\
    \ solution has showcased its effectiveness in a personalized recommender system.\
    \ As shown in Figure [12a,](#page-13-0) the [CNM](#page-39-1) architecture (RANK)\
    \ is the same as Samsung's HBM-PIM, but the controlling unit is FPGA that starts\
    \ the execution, maps computations to the RANK, and gets back the results. Like\
    \ the HBM-PIM, AxDIMM has a complete software stack that allows programming the\
    \ architecture without changing the input code or manually writing code using\
    \ AxDIMM python API.\n\n<span id=\"page-13-0\"></span>![](_page_13_Figure_6.jpeg)\n\
    \n(a) Samsung-Facebook AxDIMM hardware module and architecture [\\[46\\]](#page-35-21)\n\
    \n(b) AiM architecture [\\[9\\]](#page-34-7).\n\nFig. 12. PUMA tile and core architectures\
    \ [\\[47\\]](#page-36-0).\n\nFor this product, Samsung also seems to be in discussion\
    \ with SAP HANA. Here is another excerpt from the newsroom:\n\n*\"SAP has been\
    \ continuously collaborating with Samsung on their new and emerging memory technologies\
    \ to deliver optimal performance on SAP HANA and help database acceleration,\"\
    \ said Oliver Rebholz, head of HANA core research & innovation at SAP. \"Based\
    \ on performance projections and potential integration scenarios, we expect significant\
    \ performance improvements for in-memory database management system (IMDBMS) and\
    \ higher energy efficiency via disaggregated computing on AXDIMM. SAP is looking\
    \ to continue its collaboration with Samsung in this area\"*.\n\n*4.1.4 SK hynix's\
    \ accelerator-in-memory.* SK hynix's accelerator-in-memory (AiM) is another [CNM](#page-39-1)\
    \ system that targets the machine learning application domain [\\[8,](#page-34-6)\
    \ [9\\]](#page-34-7). As stated in [\\[9\\]](#page-34-7), \"Samsung's FIMDRAM\
    \ is near commercialization, but the required HBM technology may prevent it from\
    \ being applied to other applications due to its high cost\". AiM fundamentally\
    \ follows a very similar design approach to Samsung's FIMDRAM but utilizes GDDR6\
    \ instead.\n\nFigure [12b](#page-13-0) provides an overview of the AiM architecture.\
    \ As depicted, each bank is equipped with a processing unit (PU) that executes\
    \ a MAC operation using 16 multiplier units and an adder tree. The adder tree\
    \ can be deactivated for operations not requiring additions. Similar to the FIMDRAM\
    \ design, pairs of banks can establish direct communication. For inter-group communication,\
    \ an internal 2KB SRAM structure within the periphery facilitates the process.\n\
    \nAlthough the programming model is not explicitly explained, the presented set\
    \ of commands in AiM implies an interface enabling interaction with the device\
    \ for various operations. Some of these operations are particularly interesting,\
    \ such as the ability to perform computations within banks of different granularities\
    \ (1, 4, 16) and data movement functions that can be utilized to implement row-cloning\
    \ within DRAM.\n\n*4.1.5 AxRAM.* AxRAM targets optimizing for the off-chip memory\
    \ communication bottleneck in GPUs by integrating approximate MAC units in the\
    \ DRAM [\\[48\\]](#page-36-1). The fundamental idea is to exploit the inherent\
    \ approximability of numerous GPU applications and perform approximate calculations\
    \ directly within the memory banks, thereby reducing data movement and energy\
    \ consumption. AxRAM leverages the concept of neural transformation, a technique\
    \ that accelerates a wide range of applications by approximating specific sections\
    \ of GPU code and transforming them into a neural representation composed primarily\
    \ of [MAC](#page-40-14) and *look-up table* [\\(LUT\\)](#page-40-18) operations\
    \ for nonlinear function calculation. The multiplications in the MAC operations\
    \ are further approximated with limited iterations of shift-add and LUT accesses.\
    \ These approximate units are connected to the wide data lines that connect the\
    \ DRAM banks to the global I/O, keeping the banks and memory column unchanged.\n\
    \n<span id=\"page-14-0\"></span>Figure [13](#page-15-1) shows a sample example\
    \ where the GPU code is transformed into MAC and lookup operations. Once such\
    \ patterns are identified and transformed, they are offloaded to the in-DRAM accelerator.\
    \ The new instructions that invoke and configure the in-DRAM accelerators are\
    \ added to the GPU's ISA and are exposed to the compiler. As for the flow of execution,\
    \ initially, all data is assumed to be in one memory chip. The GPU starts normal\
    \ execution, and for the identified approximate regions, the GPU warps send an\
    \ initiation request to the on-chip memory controller. The additional logic in\
    \ the memory controller first sends an invalid signal to the on-chip caches (to\
    \ ensure data consistency) and subsequently drives the in-DRAM accelerator to\
    \ perform the computations and store the results in the designated location. To\
    \ check whether the execution is completed, the memory controller periodically\
    \ checks the memory-mapped mode register of the DRAM, which is updated by the\
    \ accelerator. Once the controller detects that this register is set, it signals\
    \ the GPU that execution is finalized, allowing the GPU to proceed with precise\
    \ execution of the subsequent instructions.\n\n<span id=\"page-15-1\"></span>![](_page_15_Figure_1.jpeg)\n\
    \n![](_page_15_Figure_2.jpeg)\n\n*4.1.6 CNM systems based on 3D-stacked DRAM.*\
    \ All the [CNM](#page-39-1) architectures discussed so far (except FIMDRAM) are\
    \ based on planar 2D DRAM. However, the resurgence in [CNM](#page-39-1) systems\
    \ is also primarily attributed to HBM and HMC technologies that seamlessly combine\
    \ logic and memory within the same package. There is a series of proposals for\
    \ [CNM](#page-39-1) systems leveraging these technologies. In the following, we\
    \ discuss some of the prominent architectures.\n\n<span id=\"page-15-2\"></span>**TESSERACT**\
    \ [\\[49\\]](#page-36-2) targets accelerating graph-based applications. Their\
    \ design comprises a host processor and an HMC with multiple vaults, each housing\
    \ an out-of-order processor. These processors exclusively access their local data\
    \ partition, while inter-communication is achieved through a messagepassing protocol.\
    \ The host processor, however, can access the complete address space of the HMC.\
    \ To capitalize on the substantial memory bandwidth available, they introduce\
    \ prefetching mechanisms. **TOP-PIM** [\\[50\\]](#page-36-3) is an architecture\
    \ that proposes an *accelerated processing unit* [\\(APU\\)](#page-39-19). Each\
    \ APU integrates a GPU and a CPU on the same silicon die. These APUs are linked\
    \ through high-speed serial connections to several 3D-stacked memory modules.\
    \ APUs are general-purpose and support a series of applications ranging from graph\
    \ processing to fluid and structure dynamics. The architecture allows code portability\
    \ and easy programmability.\n\n**Active memory cube (AMC)** [\\[51\\]](#page-36-4)\
    \ is also built upon HMC and proposes \"lanes\" in the HMC vault. Each AMC lane\
    \ consists of a register file, a computational unit, and a load/store unit to\
    \ support memory accesses. Communication among AMCs is only possible via the host\
    \ processor. AMC also offers a compiler based on OpenMP for C/C++ and FORTRAN.\n\
    \n**Heterogeneous reconfigurable logic (HRL)** [\\[52\\]](#page-36-5) leverages\
    \ the logic layer in the 3D stacked HMC to implement heterogeneous coarse-grained\
    \ (CGRAs) and fine-grained (FPGAs) logic blocks. The architecture separates routing\
    \ networks for control and data signals, employing specialized units to efficiently\
    \ handle branch operations and non-uniform data layouts commonly found in analytics\
    \ workloads.\n\n# <span id=\"page-15-0\"></span>**4.2 [CIM](#page-39-0) architectures**\n\
    \nMuch like [CNM,](#page-39-1) the concept of [CIM](#page-39-0) systems is not\
    \ entirely novel; however, it has gained significant momentum due to breakthroughs\
    \ in various NVM devices over the past decade. Figure [14](#page-16-0) shows a\
    \ partial landscape of [CIM](#page-39-0) systems, along with a corresponding timeline.\
    \ Most of the depicted [CIM](#page-39-0) accelerators originate from academia\
    \ and are not taped out. However, in recent years, several semiconductor industry\
    \ giants, including Intel, Samsung, TSMC, GlobalFoundries, and IBM, have invested\
    \ in developing their own [CIM](#page-39-0) prototypes, mostly focused on the\
    \ machine learning case. IBM,\n\n#### CNM/CIM Landscape 17\n\n![](_page_16_Figure_1.jpeg)\n\
    \n<span id=\"page-16-0\"></span>in particular, stands out among others when it\
    \ comes to the development of [CIM](#page-39-0) systems for different use cases.\n\
    \nFig. 14. A partial timeline of the evolution of [CIM](#page-39-0) systems (data\
    \ until 2018) [\\[28\\]](#page-35-3). The radius of the circle is proportional\
    \ to the amount of papers published that year.\n\nIn this section, we overview\
    \ some of the prominent [CIM](#page-39-0) designs from academia and industry.\
    \ However, before going into the details of individual [CIM](#page-39-0) designs,\
    \ we first introduce circuits that are typically used as basic [CIM](#page-39-0)\
    \ primitives in these architectures.\n\n*4.2.1 CIM primitives.* Each of the [CIM](#page-39-0)\
    \ architectures discussed in the following sections is either based on a crossbar,\
    \ content-addressable-memory, or a boolean and arithmetic logic unit. In the following,\
    \ we explain all three of them.\n\n**Crossbar:** A crossbar is a [CIM](#page-39-0)\
    \ configuration in which each input connects to every output through cross-points,\
    \ comprising memory cells and selectors. Figure [15a](#page-17-0) shows a technology-independent\
    \ crossbar configuration. As we will see in the following sections, crossbars\
    \ are particularly useful for the machine learning domain as they can compute\
    \ [MVM](#page-41-2) in constant time.\n\n<span id=\"page-16-1\"></span>**CAM:**\
    \ *content-addressable-memory* [\\(CAM\\)](#page-39-20) is associative memory\
    \ that enables parallel searches for a given query (input) across all stored content\
    \ within a CAM array. CAMs are used in pattern matching and search operations\
    \ from various application domains including databases, networking, and machine\
    \ learning [\\[53\\]](#page-36-6). Figure [15c](#page-17-0) shows a technology-independent\
    \ 3 \xD7 3 CAM structure.\n\n**Boolean and arithmetic logic in CIM:** In this\
    \ class of [CIM,](#page-39-0) the [CIM](#page-39-0) array facilitates a specific\
    \ set of general operations, such as Boolean logic and arithmetic, to be executed\
    \ using customized peripheral circuits integrated within the random-access memory\
    \ (RAM). The operands need to be stored in different rows of an array in a column-aligned\
    \ fashion where each column represents a bit position. For a [CIM](#page-39-0)\
    \ operation, multiple rows are typically activated simultaneously, and the output\
    \ is sensed and inferred by the peripherical circuitry [\\[23,](#page-34-21) [54\\\
    ]](#page-36-7). Figure [15b](#page-17-0) shows a technology-independent structure\
    \ implementing boolean logic.\n\n<span id=\"page-16-2\"></span>*4.2.2 ISAAC (by\
    \ Hewlett Packard Enterprise).* In-situ analog arithmetic in crossbars (ISAAC)\
    \ [\\[22\\]](#page-34-20) is among the first [CIM](#page-39-0) accelerators with\
    \ a complete design targeting *convolutional neural network* [\\(CNN\\)](#page-39-21)\
    \ in RRAM. As shown in Figure [16,](#page-17-1) ISAAC's architecture consists\
    \ of multiple interconnected tiles via a concentrated-mesh (c-mesh) network. Each\
    \ tile consists of 12 in-situ multiply-and-accumulate (IMA) units, a shift-and-add\
    \ (S&A) unit, two sigmoid units, one max-pooling unit, an embedded DRAM (eDRAM)\
    \ buffer for input data storage and an output register (OR) to accumulate (partial)\
    \ results. Each\n\n<span id=\"page-17-0\"></span>![](_page_17_Figure_1.jpeg)\n\
    \nFig. 15. Fundamental [CIM](#page-39-0) primitives [\\[53\\]](#page-36-6).\n\n\
    <span id=\"page-17-1\"></span>IMA integrates its own input register (IR), output\
    \ register, S&A units, and eight 128 \xD7 128 resistive crossbar arrays, also\
    \ abbreviated XB or XBars, that share analog-to-digital converters (ADCs). Each\
    \ XBar performs analog [MVM](#page-41-2) (see Figure [15a\\)](#page-17-0) and\
    \ is also equipped with a digital-to-analog converter (DAC) and an S&H circuitry.\
    \ Communication within a tile is facilitated by a 32-bit inter-tile link.\n\n\
    ![](_page_17_Figure_4.jpeg)\n\nFig. 16. ISAAC architecture hierarchy [\\[22\\\
    ]](#page-34-20).\n\nThe ISAAC design uses dataflow pipelining to optimize IMA\
    \ unit utilization and reduce buffering requirements. Depending on the network's\
    \ size, each [CNN](#page-39-21) layer is mapped to one or multiple IMAs or tiles.\
    \ Initially, input data is acquired through an I/O connection and stored within\
    \ a tile's eDRAM buffer. Before being fed to ReRAM XBars within each IMA, the\
    \ data goes through DACs. Once processed by XBars, the generated feature maps\
    \ are converted back to digital form and forwarded to max-pooling and activation\
    \ units. The outcome of the NN layer is then accumulated within the S&A and OR\
    \ units and subsequently written to a new eDRAM buffer (for the following layer).\
    \ The depth of the pipeline corresponds to the depth of the neural network, which\
    \ presents challenges when training *deep neural network*s [\\(DNNs](#page-39-22)).\
    \ Thus, ISAAC is specifically designed for inference and is not used for training.\
    \ ISAAC has no mention of the design tools and programming interface.\n\n<span\
    \ id=\"page-17-2\"></span>*4.2.3 PUMA (by Hewlett Packard Enterprise).* PUMA (programmable\
    \ ultra-efficient memristorbased accelerator) is a generalization of memristive\
    \ crossbars to accelerate a range of ML inference workloads [\\[47\\]](#page-36-0).\
    \ PUMA's microarchitecture techniques exposed via dedicated ISA ensure the efficiency\
    \ of in-memory computing and analog circuitry while providing a high degree of\
    \ programmability. The\n\narchitecture is organized into three hierarchy levels:\
    \ cores, tiles, and nodes. Nodes are connected and communicate via a chip-to-chip\
    \ network. Each individual node consists of tiles that are connected via an on-chip\
    \ network, where each tile comprises cores that communicate via shared memory,\
    \ as shown in Figure [17a.](#page-18-0) A PUMA's core consists of its own memory\
    \ and functional units, including the XBar array, referred to as the [MVM](#page-41-2)\
    \ unit (MVMU), see Figure [17b.](#page-18-0)\n\n<span id=\"page-18-0\"></span>![](_page_18_Figure_2.jpeg)\n\
    \nFig. 17. PUMA tile and core architectures [\\[47\\]](#page-36-0).\n\nUnlike\
    \ most other [CIM](#page-39-0) architectures, which are data parallel, PUMA is\
    \ a spatial architecture where distinct instructions are executed by each core\
    \ or tile. Since manually writing code for such architectures is extremely difficult,\
    \ particularly when they have thousands of cores, PUMA has a runtime compiler\
    \ implemented as a C++ library. The compiler takes the high-level input code and\
    \ extracts a dataflow graph from it. The graph is then divided into subgraphs,\
    \ considering the sizes of MVMUs, and hierarchically assigned to MVMUs, cores,\
    \ and tiles. The subgraph execution is carefully scheduled to ensure effective\
    \ resource utilization while avoiding potential deadlocks. Given the constraint\
    \ of serial read and write operations in RRAM, PUMA exclusively supports the ML\
    \ inference. However, to facilitate training, PUMA has been repurposed in a follow-up\
    \ work named PANTHER [\\[55\\]](#page-36-8).\n\n*4.2.4 Pinatubo: Accelerating\
    \ bulk bitwise logic operation.* Pinatubo is a memristor-based architecture that\
    \ harnesses data-level parallelism to conduct bulk bitwise operations [\\[23\\\
    ]](#page-34-21). Unlike the crossbar configurations, it performs computations\
    \ in the digital domain by modifying the [SAs](#page-41-5). The system architecture\
    \ is similar to a typical Von Neumann architecture that has a processor equipped\
    \ with caches and a non-volatile main memory. Pinatubo then exploits the physical\
    \ attributes of the NVM-based main memory and modifies the [SAs](#page-41-5) to\
    \ support it. The main idea is the operands are stored in different rows but the\
    \ same columns in an array, the rows are activated in parallel and the accumulated\
    \ current in the bitline is compared to a reference level in the [SAs](#page-41-5).\
    \ For different logic gates, the memory controller changes the reference levels\
    \ in the [SAs](#page-41-5) to different stats.\n\nThe Pinatubo's main memory structure\
    \ is illustrated in Figure [18,](#page-19-0) comprising multiple banks divided\
    \ into banks and mats. For operands within the same mat, the modified [SAs](#page-41-5)\
    \ work out of the box and can perform bitwise vector operations. For operations\
    \ where the data is spread across different mats, whether within the same bank\
    \ or not, additional logic gates are used for execution (within the global data\
    \ line or global I/O). The architecture supports only logic operations.\n\nFor\
    \ programmability, Pinatubo presents a software infrastructure containing both\
    \ the programming model and runtime support components. The programming model\
    \ offers two functions to allocate\n\n<span id=\"page-19-0\"></span>![](_page_19_Figure_1.jpeg)\n\
    \nFig. 18. Pinatubo architecture showing chip, bank and mat [\\[23\\]](#page-34-21).\n\
    \nbit-vectors and perform bitwise operations. The runtime support facet encompasses\
    \ adjustments to the C/C++ runtime library and the operating system (OS) and the\
    \ development of a dynamic linked driver library. The runtime library ensures\
    \ that bit-vectors are allocated to separate memory rows while the OS equipped\
    \ with PIM-aware memory management, ensures intelligent invocation of the operations.\n\
    \n*4.2.5 PRIME.* PRIME [\\[21\\]](#page-34-19) is another RRAM-based analog [CIM](#page-39-0)\
    \ accelerator. The architecture comprises multiple banks where each bank integrates\
    \ eight subarrays (chips) which are further (logically) split into memory (Mem)\
    \ units, two full function (FF) units, and one buffer. FFs can function conventionally\
    \ as memory or in an NN computation mode, controlled by the PRIME controller.\
    \ A typical FF unit is 256 \xD7 256 RRAM cells, with 6-bit reconfigurable local\
    \ [SAs](#page-41-5) reading their outputs. During computation mode, RRAM resolution\
    \ is 4-bit *multi-level cell* [\\(MLC\\)](#page-40-19), shifting to *single-level\
    \ cell* [\\(SLC\\)](#page-40-20) in memory mode. Distinct crossbar arrays are\
    \ utilized for storing positive and negative weights. The input to the mat comes\
    \ from a 3-bit fixed point signal originating from a wordline decoder and driver\
    \ (WDD). Analog subtraction and sigmoid functions within the NN are implemented\
    \ in the modified column multiplexers within the RRAM arrays.\n\n<span id=\"page-19-4\"\
    ></span><span id=\"page-19-1\"></span>![](_page_19_Figure_5.jpeg)\n\n<span id=\"\
    page-19-3\"></span><span id=\"page-19-2\"></span>Fig. 19. PRIME: Source code to\
    \ execution [\\[21\\]](#page-34-19).\n\nThe execution of an *neural-network* [\\\
    (NN\\)](#page-40-21) on PRIME involves three stages. Firstly, the [NN](#page-40-21)\
    \ is mapped onto FF subarrays, and synaptic weights are programmed into ReRAM\
    \ cells. In the optimization stage, depending on the [NN](#page-40-21) size, mapping\
    \ could occur in a single bank or across multiple banks. These first two stages\
    \ are executed by the CPU. Subsequently, a series of generated instructions are\
    \ transmitted to the PRIME controller in RRAM banks to perform computations. The\
    \ presence of latches and OR gates facilitates pipelined computation within PRIME.\n\
    \nAs shown in Figure [19,](#page-19-1) PRIME also comes with a compiler and an\
    \ API, exposing device capabilities as function calls. The process from code to\
    \ execution involves programming (coding), compiling (code optimization), and\
    \ code execution. PRIME offers application programming interfaces (APIs) that\
    \ empower developers to map [NN](#page-40-21) topologies onto FFs and configure\
    \ data paths etc.\n\n*4.2.6 Pipelayer.* Pipelayer is another RRAM-based accelerator\
    \ for [CNNs](#page-40-21) that supports both training and inference [\\[56\\]](#page-36-9).\
    \ The overall architecture of PipeLayer, shown in Figure [20](#page-20-0) features\
    \ RRAM crossbars, the spike Driver block to encode inputs as spikes and get rid\
    \ of DACs, and integration <span id=\"page-20-0\"></span>and fire components that\
    \ eliminate ADCs. In write mode, the spike driver updates RRAM array weights with\
    \ a 4-bit resolution. Within the cell, data processing occurs across morphable\
    \ and memory subarrays, where memory subarrays are conventional memory arrays\
    \ while morphable arrays can be configured in both compute and memory modes. Pipelayer\
    \ leverages these morphable subarrays for different purposes in training and inference.\n\
    \n![](_page_20_Figure_2.jpeg)\n\n<span id=\"page-20-1\"></span>Fig. 20. An overview\
    \ of the Pipelayer architecture [\\[56\\]](#page-36-9).\n\nPipeLayer allows interactive\
    \ configuration of the system on a per-layer basis. It provides an API that has\
    \ functions for different operations e.g., bidirectional transfer of data between\
    \ the CPU main memory and PipeLayer, the topology\\_set function, where the number\
    \ of compute groups can be specified by the programmer, the weight\\_load function\
    \ to load either pre-trained weights during testing or initial weights during\
    \ training into the arrays. Other functions include pipeline and mode set functions\
    \ for the morphable subarrays.\n\nThere are many other RRAM-based analog and digital\
    \ [CIM](#page-39-0) accelerators. Some other common ones that are mostly taped-out\
    \ and not discussed here include: AtomLayer [\\[57\\]](#page-36-10), RIMAC[\\\
    [58\\]](#page-36-11), FORM [\\[59\\]](#page-36-12), RRAMs for pattern recognition\
    \ [\\[60\\]](#page-36-13), RRAM accelerator for BNNs (ISSCC, 65 nm) [\\[61\\]](#page-36-14),\
    \ RRAM for edge processors (ISSCC, 55 nm) [\\[62\\]](#page-36-15), analog RRAM\
    \ with fully parallel MAC and extremely high TOPS/W (ISSCC, 130 nm but large array)\
    \ [\\[63\\]](#page-36-16).\n\n*4.2.7 In-DRAM computing.* Ambit [\\[25\\]](#page-35-0)\
    \ is a DRAM-based [CIM](#page-39-0) accelerator that, unlike all previous systems,\
    \ leverages the analog capabilities of current DRAM technology for executing bulk\
    \ bitwise operations. Ambit mainly comprises two components. First, Ambit-AND-OR\
    \ implements *triple row activation* [\\(TRA\\)](#page-41-6) in conventional DRAM.\
    \ Like memristors, the idea is to activate three rows in parallel and leverage\
    \ the *charge-sharing and charge accumulation principle*. [TRA](#page-41-6) produces\
    \ a bitwise majority function. Controlling the initial state of one of the three\
    \ rows enable performing AND and OR operation. The second component of Ambit is\
    \ Ambit-NOT, which uses the inverters in the DRAM [SAs](#page-41-5) to implement\
    \ the logic NOT operation. The basic components are then extended to implement\
    \ other logic operations and accelerate bulk bitwise operations in multiple applications.\
    \ With 8 DRAM banks, Ambit demonstrates a substantial improvement in bulk bitwise\
    \ operation throughput compared to an Intel Skylake processor and the NVIDIA GTX\
    \ 745 GPU.\n\nA follow-up work on the bulk bitwise logic in DRAM, ComputeDRAM\
    \ [\\[29\\]](#page-35-4) demonstrated that by deliberately violating timing parameters\
    \ between activation commands, certain existing off-the-shelf DRAM chips can implement\
    \ the [TRA](#page-41-6) operation of Ambit. This indicates that certain real-world\
    \ off-the-shelf DRAM chips, despite not being intended for Ambit operations, can\
    \ indeed perform in-DRAM AND and OR operations. This also suggests that the concepts\
    \ introduced in Ambit might not be too far from practical implementation. If existing\
    \ DRAM chips can perform such operations to some extent, then chips explicitly\
    \ designed for such functions could potentially be even more capable.\n\n*4.2.8\
    \ In-SRAM computing.* Neural Cache [\\[64\\]](#page-36-17) is an SRAM-based [CIM](#page-39-0)\
    \ accelerator primarily targeting [CNNs](#page-39-21). The core operations of\
    \ Neural Cache are bitwise AND and NOR operations, which are executed by simultaneously\
    \ activating multiple rows (charge sharing). It repurposes the cache memory by\
    \ modifying the peripheral circuitry to support operations such as convolution,\
    \ pooling, quantization, and fully-connected layers, all performed at an 8-bit\
    \ data precision. It is also capable of performing bit-serial operations like\
    \ addition, subtraction, multiplication, comparison, search, and copy for larger\
    \ data, utilizing carry latches linked to [SAs](#page-41-5). A transpose memory\
    \ unit is introduced that facilitates the reorganization of data into bit-serial\
    \ format within the memory when needed.\n\nIMAC [\\[30\\]](#page-35-5) is another\
    \ SRAM-based [CIM](#page-39-0) accelerator that uses the precharge circuit to\
    \ perform multi-bit analog multiplication by encoding the bit significance in\
    \ the pulse width of pre-charge pulse. IMAC also requires DAC/ADC converters to\
    \ facilitate the conversion between digital and analog forms. There are many other\
    \ instances of SRAM-based [CIM](#page-39-0) designs, some even **taped-out** [\\\
    [65](#page-36-18)[\u201368\\]](#page-36-19).\n\n*4.2.9 In-MRAM computing.* In\
    \ NVMs, Magnetic RAM (MRAM) is probably the most mature memory technology that\
    \ is commercially available and is already used in many embedded devices (see\
    \ Section [3.6\\)](#page-7-0). Therefore, it has also been intensively investigated\
    \ in the [CIM](#page-39-0) context and computing approaches implementing in-MRAM\
    \ basic boolean logic operations and more complex arithmetic functions have been\
    \ showcased. Like all other technologies, the basic [CIM](#page-39-0) methods\
    \ include bit-cell modification, reference adaptation, and in-memory analog computation.\n\
    \n<span id=\"page-21-0\"></span>![](_page_21_Figure_5.jpeg)\n\nFig. 21. A typical\
    \ SOT-MRAM subarray architecture for in-place logic [\\[69\\]](#page-37-0).\n\n\
    Figure [21](#page-21-0) shows a typical subarray architecture of an in-MRAM [CIM](#page-39-0)\
    \ [\\[69\\]](#page-37-0). The important difference here compared to already known\
    \ aspects is that it has separate read and write bit and word lines and three\
    \ reference resistance states in the sensing circuity (RAND/RM/ROR). RM is used\
    \ to perform normal memory operations, while RAND and ROR, as the names suggest,\
    \ are used to implement AND and OR operations, respectively.\n\nSimilar to other\
    \ technologies, the boolean logic is implemented with charge-sharing, and MAC\
    \ is implemented in the analog domain with current accumulation. Some prominent\
    \ MRAM-based [CIM](#page-39-0) designs include analog MACs for [TRA](#page-41-6)\
    \ inference [\\[70\\]](#page-37-1), MRAM[-CIM](#page-39-0) for BNNs [\\[71\\]](#page-37-2),\
    \ and MRAM crossbar [\\[72\\]](#page-37-3).\n\n*4.2.10 CIM using FeFETs.* FeFeTs\
    \ have also been shown to implement in-place logic gates, addition, and content-addressable\
    \ memories (CAMs). Notably, these logic operations can also be implemented with\
    \ a single [FeFET](#page-40-4) cell. For instance, if one operand is stored in\
    \ a cell (or a set of cells), the other operand can be applied as input to perform\
    \ logic operation [\\[73\\]](#page-37-4), akin to the working principle of crossbars.\
    \ Further, solutions proposing activating multiple rows and leveraging the bitline's\
    \ charge sharing (as in other memory technology) have also been presented [\\\
    [37\\]](#page-35-13).\n\nFeFETs have received particular interest in CAM designs.\
    \ CAMs are associative memories that can perform parallel searches for a query\
    \ across all stored contents within an array. FeFeTs have been used to implement\
    \ different types of CAMs for exact-search operations, approximate search operations,\
    \ range-based search operations, or a combination of them [\\[74,](#page-37-5)\
    \ [75\\]](#page-37-6).\n\n*4.2.11 Latest industrial chips.* In the previous sections,\
    \ we have extensively discussed a variety of notable [CIM](#page-39-0) and [CNM](#page-39-1)\
    \ solutions employing different technologies. While a few of these systems have\
    \ been developed in collaboration with industry partners and a subset has undergone\
    \ the tape-out process, the majority of these accelerators originate from academia.\
    \ In this section, we specifically present [CIM](#page-39-0) systems originating\
    \ from the industrial sector in the last couple of years. Note that these [CIM](#page-39-0)\
    \ systems also primarily show prototypes showcasing various research outcomes,\
    \ but they indicate their potential realization in the near future.\n\n**IBM's\
    \ PCM-based accelerators:** For more than five years, IBM has been using its PCM\
    \ device to do in-place operations for different use cases. Initially, they were\
    \ working with a reservoir of devices (millions of them) and implementing the\
    \ peripheral circuitry and additional CMOS logic in an FPGA. Their research has\
    \ progressed to consolidate all components onto a single chip, as exemplified\
    \ by HERMES, a core composed of 256\xD7256 PCM array with ADCs, a local digital\
    \ processing unit, and additional peripheries [\\[76\\]](#page-37-7). The core\
    \ effectively executes a fully parallel 256\xD7256 analog [MVM,](#page-41-2) where\
    \ each 8T4R unit cell encodes a positive/negative weight, with simultaneous handling\
    \ of 256 8-bit digital inputs/outputs. Positive weights are encoded by combining\
    \ the conductance of two PCM devices, while negative weights are represented by\
    \ the other two PCMs within the unit cell.\n\nThis year, IBM announced a newer\
    \ 64-core [CIM](#page-39-0) chip designed and fabricated in 14-nm CMOS technology\
    \ integrated with PCM [\\[77\\]](#page-37-8). The fully integrated chip comprises\
    \ 64 cores, each with a size of 256\xD7256, connected through an on-chip communication\
    \ network. It reportedly achieves an unparalleled maximal throughput of 63.1 TOPS\
    \ at an energy efficiency of 9.76 TOPS/W for 8-bit input/output [MVMs](#page-41-2).\n\
    \n**Samsung's MRAM crossbar:** Crossbar-based analog [MVM](#page-41-2) is well-explored\
    \ in RRAM and PCM technologies. However, implementing MRAM-based crossbars is\
    \ challenging due to the inherent low resistance of these devices, which could\
    \ lead to significant power consumption. In 2022, Samsung presented a 64x64 MRAM\
    \ crossbar array to address the low-resistance issue by employing an architecture\
    \ that uses resistance summation (instead of current summation) for analog multiplyaccumulate\
    \ operations [\\[72\\]](#page-37-3). Compared to the IBM HERMES cores, Samsung's\
    \ crossbar is significantly less sophisticates and limited in scale.\n\n<span\
    \ id=\"page-23-0\"></span>**TSMC's in-SRAM accelerator:** While other SRAM-based\
    \ [CIM](#page-39-0) chips exist, our focus is on the TSMC macro structure using\
    \ standard 8T cells [\\[78\\]](#page-37-9) due to its better noise margin, ensuring\
    \ stable activation for multiple rows operations in the [CIM](#page-39-0) mode,\
    \ albeit with approximately 30% increased area.\n\n![](_page_23_Figure_3.jpeg)\n\
    \nFig. 22. TSMC's [CIM](#page-39-0) SRAM structure [\\[78\\]](#page-37-9).\n\n\
    The proposed design shown in Figure [22](#page-23-0) has a 64\xD764 SRAM array\
    \ and enables parallel computations of the multiply-and-average (MAV) operations.\
    \ In a single cycle, the MAV computation of 64 4-bit inputs with 16 4-bit weight\
    \ can be completed. The 4-bit input is represented by the number of read word\
    \ line pulses which is precisely controlled by 4-bit digital counters. The 4-bit\
    \ weight is achieved through charge sharing across binary-weighted computation\
    \ capacitors. Each computation capacitor unit is constructed using the inherent\
    \ capacitor of the [SA](#page-41-5) within the 4-bit flash ADC to optimize space\
    \ and minimize the kick-back effect. This 64x64 8T macro is fabricated using 7nm\
    \ FinFET technology, exhibiting an energy efficiency of 351 TOPS/W and a throughput\
    \ of 372.4 GOPS for 1024 (64x16) 4x4b MAV operations.\n\n**Intel's SRAM-based\
    \ analog CIM design:** Intel has recently proposed an SRAM-based [CIM](#page-39-0)\
    \ macro utilizing their 22nm Low-Power FinFET process [\\[79\\]](#page-37-10).\
    \ Through the implementation of a 1-to-2 ratioed capacitor ladder (C-2C)-based\
    \ charge domain computing scheme, the presented prototype chip (shown in Figure\
    \ [23\\)](#page-24-1) achieves the capability to perform up to 2k MAC operations\
    \ in a single clock cycle, alongside achieving a peak power efficiency of 32.2-TOPS/W\
    \ with 8-bit precision for both input activation and weights. The chip also ensures\
    \ accurate [MVMs](#page-41-2) by restricting the computation error of less than\
    \ 0.5%.\n\n**Bosch+Fraunhofer and GlobalFoundries+Fraunhofer FeFET based CIM designs:**\
    \ Fraunhofer is also actively working on exploring the manufacturability and scalability\
    \ aspects of FeFET and MRAM devices at both the device and array levels. Together\
    \ with GlobalFoundries, they have demonstrated a\n\n<span id=\"page-24-1\"></span>![](_page_24_Figure_1.jpeg)\n\
    \nFig. 23. Chip level architecture diagram of Intel's analog [CIM](#page-39-0)\
    \ design [\\[79\\]](#page-37-10).\n\nFeFET-based crossbar array for multiply-accumulate\
    \ (MAC) operation [\\[80\\]](#page-37-11). The array was fabricated at GlobalFoundries\
    \ with 28nm CMOS technology coupled with FeFET. To prevent the accumulation of\
    \ errors on the bitline, the arrays were divided into 8\xD78 segments.\n\nIn a\
    \ recent work, Fraunhofer and Robert Bosch demonstrated a [CIM](#page-39-0) crossbar\
    \ using multi-level FeFET cells. In the proposed design, the input is encoded\
    \ into the applied voltage duration and magnitude while the weights are stored\
    \ in the multi-level FeFET cells. The MAC output is the accumulated capacitor\
    \ voltage that depends on the activation time and the number of FeFETs activated.\
    \ This reportedly reduces the impact of variations and the achieved performance\
    \ of 885.4 TOPS/W is also nearly-double compared to existing solutions.\n\n**HP's\
    \ CAM designs:** In a recent work, Hewlett Packard Labs proposed a memristive-based\
    \ analog CAM for tree-based machine learning [\\[81\\]](#page-37-12). Analog CAMs\
    \ are capable of performing searches based on analog signal levels rather than\
    \ digital data comparison. The proposed design combines analog CAMs with traditional\
    \ analog RAM and accelerates large random forest models with it. Figure [24](#page-25-1)\
    \ shows a high-level overview of the proposed system where the analog CAM can\
    \ perform root-to-leaf evaluation of an entire tree in a single step.\n\n####\
    \ <span id=\"page-24-0\"></span>**4.3 Comparative analysis and discussion**\n\n\
    The surge in [CIM](#page-39-0) and [CNM](#page-39-1) systems is largely attributed\
    \ to the revolution in data-intensive applications. According to recent research\
    \ by TSMC, traditional SRAM and DRAM technologies have effectively scaled to meet\
    \ capacity and bandwidth demands in the past decades [\\[82\\]](#page-37-13),\
    \ but their future scalability is uncertain due to reaching inherent technological\
    \ limits. This underscores the pivotal role that NVM will play in the future of\
    \ computing.\n\nEspecially in edge scenarios such as automotive, augmented reality,\
    \ and AI, where energy efficiency is paramount, NVM technologies are poised to\
    \ play a pivotal role. As energy efficiency increases through specialized hardware,\
    \ domain-specific architectures harnessing these NVMs for [CIM](#page-39-0) and\
    \ [CNM](#page-39-1) solutions are anticipated to experience an unprecedented surge\
    \ in the coming years. A recent article from Intel [\\[79\\]](#page-37-10) compares\
    \ the performance of conventional digital accelerators with the emerging analog\
    \ and digital [CIM](#page-39-0) and [CNM](#page-39-1) accelerators. Conventional\
    \ accelerators still achieve higher throughput because [CIM](#page-39-0) systems\
    \ are relatively less optimized, array sizes are small, and the peripheral circuitry\
    \ overhead is non-negligible. Yet, they are orders of magnitude better in terms\
    \ of\n\n<span id=\"page-25-1\"></span>![](_page_25_Figure_1.jpeg)\n\nFig. 24.\
    \ An overview of the HP [CIM](#page-39-0) system for tree-based learning [\\[81\\\
    ]](#page-37-12).\n\npower consumption. As of the time of writing, the most recent\
    \ comparison depicted in [\\[82\\]](#page-37-13) shows similar trends.\n\n![](_page_25_Figure_4.jpeg)\n\
    \n<span id=\"page-25-2\"></span>Fig. 25. Performance and power comparison of different\
    \ *outside memory processing* [\\(OMP\\)](#page-40-22) (we call it [COM](#page-39-2)\
    \ in this report), [NMP](#page-39-9) (ours [CNM\\)](#page-39-1) and [IMP](#page-39-4)\
    \ (ours [CIM\\)](#page-39-0) [\\[83\\]](#page-37-14).\n\nTable [2](#page-26-0)\
    \ presents a summary and comparison of the architectures discussed in this section.\
    \ For brevity, we only compare important parameters, such as the underlying memory\
    \ technology, available function (boolean logic, arithmetic, etc.), evaluation\
    \ technique (simulation, prototype, analytic), programming model, application\
    \ domain, and technology node.\n\n#### <span id=\"page-25-0\"></span>**5 COMMERCIAL\
    \ LANDSCAPE**\n\nThis section overviews CIM and CNM companies/startups, highlighting\
    \ their products, underlying technologies, customers (when known), and tools.\
    \ As not everything about companies is public,\n\n<span id=\"page-26-0\"></span>\n\
    \n| Accelerator                                | Year | Technology           \
    \          | Type | Programming<br>model | Logic unit                   | Implementation\
    \                   | Domain              |\n|--------------------------------------------|------|--------------------------------|------|----------------------|------------------------------|----------------------------------|---------------------|\n\
    | McDRAM                                     | 2018 | DRAM                   \
    \        | CNM  | Extended ISA         | MAC                          | Hardware\
    \                         | AI                  |\n| MViD                    \
    \                   | 2020 | DRAM                           | CNM  | Extended\
    \ ISA         | MAC                          | Hardware                      \
    \   | AI                  |\n| PIM-HBM                                    | 2021\
    \ | DRAM                           | CNM  | Full<br>software     | FPUs (add,\
    \ Mul)              | Hardware                         | AI                  |\n\
    | AiM                                        | 2022 | (HBM)<br>DRAM<br>(GDDR6)\
    \       | CNM  | stack<br>API         | MAC                          | Hardware\
    \                         | AI                  |\n| AxRAM                   \
    \                   | 2018 | DRAM (GPU                      | CNM  | API     \
    \             | MAC, LUTs                    | GPGPU-Sim                     \
    \   | AI                  |\n| TESSERACT                                  | 2015\
    \ | based system)<br>DRAM<br>(HMC) | CNM  | API                  | CPU       \
    \                   | Simulation                       | Graph processing    |\n\
    | TOP-PIM                                    | 2014 | DRAM<br>(HMC)          \
    \        | CNM  | OpenCL               | CPU+GPU                      | Simulation\
    \                       | Graph, HPC          |\n| AMC                       \
    \                 | 2015 | DRAM                           | CNM  | OpenMP    \
    \           | CPU                          | Simulation                      \
    \ | HPC                 |\n| HRL                                        | 2015\
    \ | (HMC)<br>DRAM<br>(HMC)         | CNM  | MapReduce            | CGRA+FPGA \
    \                   | Simulation                       | Data analytics      |\n\
    | CIM architectures (Academia/Research Labs) |      |                        \
    \        |      |                      |                              |      \
    \                            |                     |\n| ISAAC                \
    \                      | 2016 | RRAM                           | CIM  | NA   \
    \                | Analog Xbar                  | Analytical                 \
    \      | AI                  |\n| PUMA                                       |\
    \ 2019 | RRAM                           | CIM  | Compiler             | Xbar \
    \                        | PUMAsim<br>(arch.<br>simulation) | AI             \
    \     |\n| Pinatubo                                   | 2016 | RRAM          \
    \                 | CIM  | API, Runtime         | Boolean logic              \
    \  | In-house simulator               | Bitwise Logic       |\n| PRIME       \
    \                               | 2016 | RRAM                           | CIM\
    \  | Compiler+API         | Xbar                         | Analytical        \
    \               | AI                  |\n| PipeLayer                         \
    \         | 2017 | RRAM                           | CIM  | API               \
    \   | Xbar                         | Analytical                       | CNN (train\
    \ + infer) |\n| AtomLayer                                  | 2018 | RRAM     \
    \                      | CIM  | NA                   | Xbar                  \
    \       | Analytical                       | CNN (train + infer) |\n| RIMAC  \
    \                                    | 2023 | RRAM                           |\
    \ CIM  | NA                   | Xbar<br>(without<br>DAC/ADC) | In-house simulator\
    \               | DNN inference       |\n\nTable 2. A summary of the presented\
    \ architectures. They are grouped into three categories: [CNM,](#page-39-1) [CIM,](#page-39-0)\
    \ and [CIM](#page-39-0) (prototype chips/systems). All presented architectures\
    \ are either simulation-based or prototype-based (no products).\n\nwe only include\
    \ details that we extract from these companies' websites or are known to us via\
    \ our network.\n\n#### **5.1 Axelera**\n\n<span id=\"page-26-1\"></span>Axelera\
    \ [\\[84\\]](#page-37-15) is one of the notable Semiconductor startups in Europe.\
    \ Founded in 2021 and backed by tech giants like Bitfury and IMEC, it had already\
    \ taped out its first CIM chip, Thetis, in December 2021 (just four months after\
    \ its founding). Today, it offers a fully integrated *system-on-chip* [\\(SoC\\\
    )](#page-40-23) powered by its Metis AI processing units (AIPU).\n\nAbout the\
    \ AI core, as per the company's website: \"Axelera AI has fundamentally changed\
    \ the architecture of \"compute-in-place\" by introducing an SRAM-based digital\
    \ in-memory computing (D-IMC) engine. In contrast to analog in-memory computing\
    \ approaches, Axelera's D-IMC design is immune to noise and memory non-idealities\
    \ that affect the precision of the analog matrix-vector operations as well as\
    \ the deterministic nature and repeatability of the matrix-vector multiplication\
    \ results. Our D-IMC supports INT8 activations and weights, but the accumulation\
    \ maintains full precision at INT32, which enables state-of-the-art FP32 iso-accuracy\
    \ for a wide range of applications without the need for retraining\".\n\nAxelera's\
    \ latest SoC consists of 4 cores and a RISC-V based control core. For programming\
    \ these systems, Axelera provides an end-to-end integrated framework for application\
    \ development. The high-level framework takes users along the development processes\
    \ without needing to understand the underlying architecture or even the machine\
    \ learning concepts.\n\n**Funding:** \"Axelera AI, the provider of the world's\
    \ most powerful and advanced solutions for AI at the Edge, announces new investors\
    \ who have joined their oversubscribed Series A round, bringing the total amount\
    \ raised to \\$50 million. In the last several months, CDP Venture Capital, Verve\
    \ Ventures, and Fractionelera have joined the round\", Axelera AI, May 22, 2023.\n\
    \n# **5.2 d-Matrix**\n\nd-Matrix is at the forefront of driving the transformation\
    \ in data center architecture toward digital in-memory computing (DIMC) [\\[85\\\
    ]](#page-37-16). Founded in 2019, the company has received substantial support\
    \ from prominent investors and strategic partners, including Playground Global,\
    \ M12 (Microsoft Venture Fund), SK Hynix, Nautilus Venture Partners, Marvell Technology,\
    \ and Entrada Ventures.\n\nLeveraging their in-SRAM digital computing techniques,\
    \ a chipset-based design, high-bandwidth BoW interconnects, and a full stack of\
    \ machine learning and large language model tools and software, d-Matrix pioneers\
    \ best-performing solutions for large-scale inference requirements. A full stack\
    \ framework, compiler, and APIs (open-source as per the company's website but\
    \ couldn't find the link). Their latest product Jayhawk II can scale up to 150\
    \ TOPS/W using 6nm technology and can handle LLM models up to 20\xD7 more inferences\
    \ per second for LLM sizing to 40B parameters, compared to state-of-the-art GPUs.\n\
    \n**Funding:** Temasek, Playground Global and Microsoft Corp.\n\n# **5.3 Gyrfalcon\
    \ Technology**\n\nGyrfalcon Technology [\\[86\\]](#page-37-17) also leverages\
    \ CNM to accelerate AI on the edge. They offer an AI processing in memory (APiM)\
    \ architecture that combines a large MAC array directly with MRAM memory modules.\
    \ As of the current date, their software stack is not available.\n\n**Funding:**\
    \ Private.\n\n# **5.4 MemComputing**\n\nMemComputing [\\[87\\]](#page-37-18),\
    \ founded in 2016, uses a computational memory based on its self-organizing logic\
    \ gates (SOLG). SOLGs are terminal-agnostic elements (memristor or memcapacitor)\
    \ that implement various logic gates. Their target applications comprise industrial\
    \ computations associated with optimizations, big data analytics, and machine\
    \ learning. MemComputing provides a software stack and offers it as a software-as-a-service.\n\
    \n**Funding:** MemComputing mentions the US Space Force, ENSOS, NASA, Ball Aerospace,\
    \ PSA, US Air Force, Canvass Labs and Defence Innovation Unit as partners.\n\n\
    ### **5.5 Memverge**\n\nMemverge [\\[88\\]](#page-37-19) is not directly doing\
    \ any CIM or CNM but is relevant in the context. Backed by 9 investors including\
    \ tech giants like Intel, SK hynix, the company's main goal is to provide software\
    \ designed to accelerate and optimize data-intensive applications. Their main\
    \ target is to consider environments with \"Endless Memory\" and efficiently manage\
    \ the memory to get more performance.\n\n**Latest news**: \"Samsung, MemVerge,\
    \ H3 Platform, and XConn, today unveiled a 2TB Pooled CXL Memory System at Flash\
    \ Memory Summit. The system addresses performance challenges faced by highly distributed\
    \ AI/ML applications. These challenges include issues like spilling memory to\
    \ slow storage when main memory is full, excessive memory copying, I/O to storage,\
    \ serialization/deserialization, and Out-of-Memory errors that can crash an application.\"\
    , MemVerge, August 8, 2023.\n\n# **5.6 Mythic**\n\nMythic [\\[89\\]](#page-37-20)\
    \ offers an analog matrix processor (Mythic AMP) that uses their analog compute\
    \ engine (ACE) based on flash memory array and ADCs. Mythic ACE also has a 32b\
    \ RISC V processor, SIMD vector engine, and a 64KB SRAM along with a high-throughput\
    \ network-on-chip (NoC). Mythic workflow in Figure [26](#page-28-0) shows that\
    \ the software stack takes a trained NN model, optimizes it, and compiles it to\
    \ generate code for Mythic AMP. The optimization suit also transforms NN in a\
    \ way that can be accelerated on the analog CIM system.\n\n<span id=\"page-28-0\"\
    ></span>![](_page_28_Figure_3.jpeg)\n\nFig. 26. Mythic AI workflow [\\[89\\]](#page-37-20).\n\
    \n**Funding:** The company is supported by many investors: Micron, HP Enterprise,\
    \ SoftBank, Future ventures, Lam Research, Threshold, Catapult, DCVC and UDC ventures.\n\
    \n# **5.7 NeuroBlad**\n\nFounded in 2018, NeuroBlade offers the SPU (SQL Processing\
    \ Unit), the industry's first, proven processor architecture that delivers orders\
    \ of magnitude improvement by departing from Von Neumann model [\\[90\\]](#page-37-21).\
    \ Neuroblade is also a CNM architecture (more closed to near-storage computing)\
    \ where they integrate custom RISC processors on the DRAM chip (very similar to\
    \ UPMEM). The SPUs are installed as PCI-e cards that can be deployed in data centers.\
    \ As for the software stack, the company offers an SDK along with a set of APIs\
    \ that hide the complexity and programming model for these cores from the end\
    \ user and also allow optimizing for maximum parallelism and efficiency.\n\n**Funding:**\
    \ NeuroBlade is funded by Stage one, Grove Ventures, UMC, PSMC Intel capitals,\
    \ Pegratron, Marubeni, Marius Nacht, Corner and MediaTek.\n\n# **5.8 Rain AI**\n\
    \nFounded in 2017, Rain AI also focuses on radically cheaper AI computing [\\\
    [91\\]](#page-37-22). The company has no hardware product yet but is aiming to\
    \ be 100\xD7 better than GPU using their innovations in radical co-design (by\
    \ looking at the algorithms and the CIM hardware at the same time). They are targeting\
    \ AI training (along with the inference) on the edge with the ultimate goal of\
    \ putting models the size of ChatGPT into chips of the size of a thumbnail. They\
    \ are transforming the algorithms in a way that fundamentally matches the behavior\
    \ of the analog memristive devices. As per the CEO, they have a few tap-outs planned\
    \ for this year and the product (a complete platform) next year and they are working\
    \ on a software stack for ease of use and ease of integration.\n\n**Funding:**\
    \ The company is funded by Y combinator S18, Sam Altman (CEO OpenAI), Liquid 2\
    \ Ventures, Loup Ventures, Airbus Ventures, and Daniel Gross (founder Poineer).\n\
    \n# **5.9 SEMRON**\n\nFounded in 2020, Semron [\\[92\\]](#page-37-23) promises\
    \ to offer 3D solutions powered by analog CIM. At the core of their technology\
    \ is their innovative CapRAM devices which are semiconductor devices that store\
    \ multi-bit values in their variable capacitances (unlike variable resistance\
    \ states in memristors). Since CapRAM is capacitive, the noise in calculations\
    \ is much lower and the energy efficiency, as per their website, is unparalleled.\
    \ Although Semron has the device technology, there are no details of its products,\
    \ architecture, and software stack.\n\n**Funding:** As per crunchbase, the company\
    \ is funded by VentureOut.\n\n# **5.10 SureCore**\n\nSurecore [\\[93\\]](#page-37-24)\
    \ is working on many low-power products including custom application-specific.\
    \ They also have a product named \"CompuRAM\" that embeds arithmetic capability\
    \ within the SRAM array to enable low-power AI on the edge. Besides working on\
    \ SRAM-based solutions, in collaboration with Intrinsic, they have recently ventured\
    \ into RRAM technology. No information is provided regarding the software stack.\n\
    \n**Funding:** The company is supported by Capital-E, Finance Yorkshire and Mercia\
    \ Technologies.\n\n# **5.11 Synthara**\n\nSynthara is a Zurich-based Semiconductor\
    \ company that was founded in 2017 [\\[94\\]](#page-37-25). Their latest product,\
    \ ComputeRAM, integrates SRAM-based CIM macros with proprietary elements to accelerate\
    \ dot products. The solution delivers 50\xD7 compute efficiency and can be used\
    \ for AI, digital signal processing, and linear algebra-heavy routines. The CIM-powered\
    \ SRAM array can be operated just like conventional SRAM. ComputeRAM is not married\
    \ to a specific ISA and can work with any host processor. Synthara also provides\
    \ what they call Compiler hooks that can transparently offload any input application\
    \ to their ComputeRAM accelerator, without changing or rewriting the code.\n\n\
    **Funding:** The company is supported by EU funding for research & innovation,\
    \ High-tech Gr\xFCnderfonds, Intel.ignite, FNSNF, multicoreware, ventureKick and\
    \ others.\n\n# **5.12 Syntiant**\n\nFounded in 2017, Syntiant also leverages DRAM-based\
    \ [CNM](#page-39-1) and utilizes standard CMOS processes to design their neural\
    \ decision processors (NDPs) that perform direct processing of neural network\
    \ layers from platforms like TensorFlow [\\[95\\]](#page-37-26). Syntiant also\
    \ mainly targets AI on the edge having applications in many domains, including\
    \ always-on voice, audio, image, and sensor applications.\n\nSyntiant's TinyML\
    \ platform, powered by NDP101, aspires to democratize AI by presenting a comprehensive\
    \ system for those interested in initiating their own model training for edge\
    \ computing. **Funding:** Syntiant is funded by prominent investors including,\
    \ Atlantic Bridge, Rober Bosch Venture Capital, Embark Ventures, DHVC, Intel capitals,\
    \ M12 (Microsoft ventures), and Motorola Solutions.\n\n# **5.13 TetraMem**\n\n\
    Founded in 2018, TetraMem is set to offer the industry's most disruptive CIM technology\
    \ for edge application [\\[96\\]](#page-37-27). TetraMem is also leveraging memristors\
    \ for analog MAC operations, aiming at inference on the edge. Their systems are\
    \ built upon their patented devices and co-design solutions.\n\nTetraMem offers\
    \ (1) Platform as a service (PaaS), a complete hardware and software platform\
    \ designed to integrate into your own system; (2) Software as a service (SaaS),\
    \ to help develop your NN edge application and integrate it into your system.\
    \ Their verified full software stack provides an unmatched experience on actual\
    \ analog in-memory compute silicon; and (3) a neural processing unit (NPU) based\
    \ on memristive technology.\n\nTetraMem has recently announced a collaboration\
    \ with Andes Technologies and together with their research collaborators have\
    \ demonstrated a memristive device that can have thousands of conductance levels\
    \ (unmatched) [\\[97\\]](#page-37-28).\n\n#### **Funding:** Private.\n\n# **5.14\
    \ EnCharge AI**\n\nFounded in 2022, EnCharge AI promise to offer an end-to-end\
    \ scalable architecture for AI inference [\\[98\\]](#page-37-29). They leverage\
    \ SRAM-based CIM arrays for analog MVM operations and combine them with SIMD CNM\
    \ logic to perform custom element-wise operations. The architecture comprises\
    \ an array of CIM units (CIMUs), an on-chip network interconnecting CIMUs, buffers,\
    \ control circuitry, and off-chip interfaces. Each CIMU is equipped with an SRAM-based\
    \ CIM array featuring ADCs to convert computed outputs into digital values. Additionally,\
    \ CIMUs house SIMD units and FPUs with a custom instruction set, along with buffers\
    \ dedicated to both computation and data flow. According to the company's official\
    \ website, they offer a software platform that fits with standard ML frameworks,\
    \ such as PyTorch, TensorFlow, and ONNX. This also allows the implementation of\
    \ various ML models and their customizations. Specific implementation details\
    \ about the software stack are not available.\n\n**Funding:** Encharge AI is funded\
    \ by AlleyCorp, Scout Ventures, Silicon Catalyst Angels, Schams Ventures, E14\
    \ Fund, and Alumni Ventures. At their launch in December 2022, they announced\
    \ securing \\$21.7 Mio. in their series A round.\n\n### **5.15 Re(conceive) AI**\n\
    \nRe(conceive) is another CIM startup founded in 2019 that promises offering \"\
    the most power AI accelerator\" [\\[99\\]](#page-38-0). As per their website,\
    \ re(conceive) are pioneers in realizing the complete potential of CMOS-based\
    \ analog in-memory AI computing, achieving the utmost efficiency among all known\
    \ AI accelerators. However, no specific details are available on the company's\
    \ funding and technology (hardware/software).\n\n### **5.16 Fractile AI**\n\n\
    Established in 2022 by a team of Oxford University scientists, Fractile [\\[100\\\
    ]](#page-38-1) aims to transform the world by enabling large language models'\
    \ (LLM) inference at speeds up to 100 times faster than Nvidia's most recent H100\
    \ GPUs. This increase in performance primarily arises from in-memory computations.\
    \ However, the details of the technology, both hardware and software, as well\
    \ as the company's funding particulars, remain undisclosed.\n\n# **5.17 Untether\
    \ AI**\n\nFounded in 2018 [\\[101\\]](#page-38-2), Untether's main design integrates\
    \ RISC-V cores on the SRAM chips for processing AI workloads. Their latest product,\
    \ the tsunAImi accelerator card provides a phenomenal 2 POPS of compute power,\
    \ twice the amount of any available product. This compute power translates into\
    \ over 80,000 frames per second of ResNet-50 throughput, three times the throughput\
    \ of any product on the market. Untether AI provides an automated SDK for its\
    \ products. The SDK takes a network model implemented in common machine learning\
    \ frameworks like TensorFlow and PyTorch and lowers it into the kernel code that\
    \ runs on these RISC-V processors. It automatically takes care of low-level optimizations,\
    \ providing extensive visualization, a cycle-accurate simulator, and an easily\
    \ adoptable runtime API.\n\n<span id=\"page-31-1\"></span>\n\n| Company      \
    \     | Use-Case                        | Technology                    | Solution\
    \                          | Programmability                                 \
    \       | Funding<br>(Mio.<br>\\$)<br>(PitchBook) |\n|-------------------|---------------------------------|-------------------------------|-----------------------------------|--------------------------------------------------------|----------------------------------------|\n\
    | Axelera           | AI on the Edge                  | SRAM (digital MAC)   \
    \         | Hardware-SoC                      | SDK provided                 \
    \                          | 63.72 (Early stage VC)                 |\n| d-Matrix\
    \          | AI inference in dat<br>acenters | SRAM (digital MAC)            |\
    \ Chiplets                          | Open-source<br>Frame<br>work(s)        \
    \                | 161.3 (Early stage VC)                 |\n| Synthara      \
    \    | AI, DSP, Linear al<br>gebra     | SRAM (dot product)            | Accelerator\
    \                       | Compiler available                                 \
    \    | 3.33 (Grant)                           |\n| Mythic            | AI on the\
    \ Edge                  | Flash (Analog computing)      | Accelerator,<br>pro<br>cessor\
    \     | Software<br>stack<br>(does<br>rewriting, opt, mapping) | 177.41 (Later\
    \ stage VC)                |\n| Surecore          | AI on the edge           \
    \       | SRAM (CNM)                    | Chip                              |\
    \ No details                                             | 11.16 (Later stage\
    \ VC)                 |\n| SEMRON            | AI on the edge                \
    \  | Memcapacitor                  | 3D-Chip (planned)                 | No details\
    \                                             | 1.63 (Seed round)            \
    \          |\n| Untether AI       | AI everywhere                   | SRAM+RISC-V\
    \ (CNM)             | Chips, accelerator                | SDK<br>and<br>simulator<br>(Toolkit)\
    \                   | 153.52 (Early stage VC)                |\n| Syntiant   \
    \       | AI on the edge                  | SRAM+ARM<br>MCUs<br>(CNM)     | Processor\
    \                         | Available                                        \
    \      | 121.43 (Later stage VC)                |\n| Neuroblade        | Analytics\
    \                       | DRAM+RISC<br>cores<br>(CNM)   | Processor          \
    \               | Set of APIs                                            | 110.43\
    \ (Debt - General)                |\n| Rain AI           | LLMs on the edge<br>(Training)\
    \  | Memristors                    | Processors                        | NA  \
    \                                                   | 64.04 (Later stage VC) \
    \                |\n| TetraMem          | Edge applications               | Memristors\
    \                    | Processors,<br>soft<br>ware stack | HDK, SDK          \
    \                                     | NA                                   \
    \  |\n| Gyrfalcon<br>Tech | AI on the edge                  | CNM<br>(MACs<br>with<br>MRAM)\
    \ | Chip                              | NA                                   \
    \                  | 68.0 (Debt - PPP)                      |\n| UPMEM       \
    \      | General-purpose                 | DRAM+RISC cores               | System\
    \                            | APIs                                          \
    \         | 15.5 (Later stage VC)                  |\n| EnCharge AI       | AI\
    \ inference                    | SRAM (CIM) + SIMD<br>(CNM)    | Chip        \
    \                      | Software available                                  \
    \   | 21.7 (Angel - individ)                 |\n| Re(conveive)      | AI inference\
    \                    | SRAM (analog CIM)             | Chip                  \
    \            | NA                                                     | NA   \
    \                                  |\n| Fractile          | LLMs inference   \
    \               | NA                            | Chip                       \
    \       | NA                                                     | NA        \
    \                             |\n\nTable 3. CIM/CNM companies, with their products,\
    \ technologies, and funding status.\n\n**Funding:** Untether's investors include\
    \ CPPIB, GM Ventures, Intel Capital, Radical Ventures, and Tracker Capital.\n\n\
    # **5.18 UPMEM Technology**\n\nFounded in 2015, UPMEM is a tech company offering\
    \ programmable CNM systems for data-intensive applications. See more details on\
    \ the architecture and programmability in Section [4.1.1.](#page-10-2)\n\n**Funding:**\
    \ The company is funded by Western Digital, Partech, and super nova invest.\n\n\
    ### **5.19 Summary**\n\nTable [3](#page-31-1) and Figure [27](#page-32-1) summarize\
    \ the discussion in this section and provides a landscape of CIM, CNM companies,\
    \ their products, technologies, and funding status. Please note that this compilation\
    \ is not exhaustive; it includes only companies known to us and those that, based\
    \ on our understanding, fall within the CIM and CNM categories. As Figure [27](#page-32-1)\
    \ clearly illustrates, the current landscape is predominantly characterized by\
    \ conventional technologies, with a notable absence of a comprehensive software\
    \ ecosystem.\n\n# <span id=\"page-31-0\"></span>**5.20 Open challenges**\n\n[CIM](#page-39-0)\
    \ and [CNM](#page-39-1) systems have already entered the market, yet a series\
    \ of open challenges are expected to become more pronounced as time progresses.\
    \ It will take years to understand how these units will harmonize within the overall\
    \ system architecture and determine their optimal utilization. In the following,\
    \ we briefly discuss the important categories.\n\n**Materials:** During the era\
    \ of Moore's law in computing, the primary focus was on refining transistors to\
    \ be smaller, faster, and more energy-efficient. The selection of materials was\
    \ confined to only\n\n#### CNM/CIM Landscape 33\n\n<span id=\"page-32-1\"></span>![](_page_32_Figure_1.jpeg)\n\
    \n![](_page_32_Figure_2.jpeg)\n\nthose compatible with manufacturing processes.\
    \ However, the limitations of these materials to scale further are now exposed.\
    \ As a result, new materials have emerged and further research is needed to investigate\
    \ novel materials (to enable further transistor scaling: hopes with carbon nanotube,\
    \ and novel memory devices).\n\n**Devices:** Mainstream computing has largely\
    \ relied on digital logic and binary storage. Nonetheless, the emerging wave of\
    \ computing architectures, particularly [CIM](#page-39-0) requires novel multi-state\
    \ devices allowing both analog and digital operations. Existing devices, memristors\
    \ in particular, do offer such properties but have reliability and other associated\
    \ challenges.\n\n**Integration:** We have seen various architectures based on\
    \ various technologies. As is evident, there is no on-technology-fits-all solution.\
    \ Eventually, [CIM](#page-39-0) modules based on different technologies need to\
    \ be integrated into the same to get the best out of all these technologies. This\
    \ poses integration challenges that have received little to no attention.\n\n\
    **Processing systems:** These novel architectures require new tools, algorithms,\
    \ cost models, and software solutions. All of them are crucial to understanding\
    \ these architectures, enabling their design space exploration, and making them\
    \ accessible to a larger audience.\n\nWhile every challenge holds significance\
    \ and demands attention, programmability and userfriendliness are the most important\
    \ ones from the user's standpoint. Following is an excerpt from Facebook's recent\
    \ article on their inference accelerator that highlights the same.\n\n*\"We've\
    \ investigated applying processing-in-memory (PIM) to our workloads and determined\
    \ there are several challenges to using these approaches. Perhaps the biggest\
    \ challenge of PIM is its programmability\".*\n\nIn response to the challenges\
    \ associated with programmability, we have ourselves been working on high-level\
    \ programming and compilation frameworks for CNM and CIM systems [\\[102](#page-38-3)[\u2013\
    105\\]](#page-38-4). We have developed reusable abstractions and demonstrated\
    \ compilation flows for CIM systems with memristive crossbars, CAMs, CIM-logic\
    \ modules, and for CNM systems like UPMEM and Samsung CNM. However, much more\
    \ cross-layer work is needed to improve automation [\\[106\\]](#page-38-5), in\
    \ particular for heterogeneous systems integrating several paradigms and technologies.\n\
    \n#### <span id=\"page-32-0\"></span>**6 CONCLUSIONS**\n\nThis paper overviews\
    \ the landscape of compute-near-memory (CNM) and compute-in-memory (CIM) paradigms.\
    \ It starts with an explanation of the Von Neumann bottleneck, the necessity of\
    \ novel CIM/CNM paradigms, and the key terminology used in the related literature.\
    \ It offers a comprehensive\n\nbackground on major memory technologies and emphasizes\
    \ the importance of heterogeneous systems. The paper overviews prominent CIM and\
    \ CNM designs from both academia and industry. In contrast to other studies in\
    \ the literature that focus on either application domains or memory technologies,\
    \ this paper concentrates on designs that have either successfully transitioned\
    \ into product offerings or have reached a stage where commercialization is a\
    \ feasible prospect. We explain prevalent CNM architectures, including microarchitectural\
    \ details, associated technologies, software frameworks, and the results achieved\
    \ (usually measured as throughput). Subsequently, we survey the landscape of CIM\
    \ systems, explaining prevailing CIM designs that use prominent technologies such\
    \ as SRAM, DRAM, MRAM, RRAM, PCM, and FeFET. We overview CIM chips from industrial\
    \ giants (research centers), spanning from earlier designs like ISAAC and PUMA\
    \ by Hewlett Packard Enterprise to the most cutting-edge chips from IBM, Samsung,\
    \ TSMC, Intel, Meta (Facebook), Bosch, Frauenhofer, and GlobalFoundries. Current\
    \ trends in industrial research show that while conventional SRAM and DRAM technologies\
    \ are ready to be leveraged in CIM/CNM systems, emerging technologies like PCM,\
    \ RRAM, MRAM, and FeFETs are also poised to make partial inroads, particularly\
    \ for selected operations, such as dot products and pattern matching.\n\nFinally,\
    \ we describe the landscape of CIM and CNM start-ups, highlighting the emergence\
    \ of numerous new companies in recent years that have introduced innovative solutions\
    \ to cater to the thriving demands of AI and other data-intensive application\
    \ domains. These companies are targeting a diverse range of market segments, spanning\
    \ from power-efficient edge applications (AI at the edge) to high-performance\
    \ data center servers (e.g., for AI training), and many have successfully secured\
    \ substantial funding (hundreds of millions) in their initial funding rounds.\
    \ The paper shows that SRAM technology currently dominates this landscape. However,\
    \ with active research and breakthroughs in emerging NVMs (demonstrated by recent\
    \ industrial chips), it is anticipated that NVMs will play a more prominent role\
    \ in these paradigms in the near future.\n\nThe paper highlights that CIM and\
    \ CNM technologies (i) harbor significant potential to outperform conventional\
    \ systems, and (ii) have already made inroads into the market. However, their\
    \ true potential remains untapped. This is attributed to a number of challenges,\
    \ including the lack of accurate design space exploration tools, programming frameworks,\
    \ and a comprehensive software ecosystem in general, and cost and performance\
    \ models that can be leveraged to guide static and runtime optimizations for these\
    \ systems.\n\nCNM and CIM computing is an extremely active field. We believe that\
    \ we have captured a representative snapshot of this field, early in year 2024,\
    \ and remain excited about how technologies, devices, architectures and tools\
    \ will continue to develop moving forward.\n\n#### **ACKNOWLEDGEMENTS**\n\nThis\
    \ work was supported by Vsquared Ventures (VSQ). Special thanks to Max Odendahl\
    \ (Venture Partner at VSQ) for his feedback on previous versions of the manuscript.\
    \ This work was also supported by the German Research Council (DFG) through the\
    \ HetCIM project (project number 502388442) in the context of the DFG Priority\
    \ Program on Disruptive Memory Technologies (SPP2377 [https://spp2377.uos.de\\\
    )](https://spp2377.uos.de) and the German Federal Ministry of Education and Research\
    \ (BMBF, project number 01IS18026A-D) by funding the competence center for Big\
    \ Data and AI ScaDS.AI Dresden/Leipzig [\\(https://scads.ai\\)](https://scads.ai).\n\
    \n#### **REFERENCES**\n\n<span id=\"page-33-0\"></span>[1] S. Li, A. O. Glova,\
    \ X. Hu, P. Gu, D. Niu, K. T. Malladi, H. Zheng, B. Brennan, and Y. Xie, \"Scope:\
    \ A stochastic computing engine for dram-based in-situ accelerator,\" in *2018\
    \ 51st Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)*.\
    \ IEEE, 2018, pp. 696\u2013709.\n\n- <span id=\"page-34-0\"></span>[2] A. de Vries,\
    \ \"The growing energy footprint of artificial intelligence,\" *Joule*, vol. 9,\
    \ no. 4, p. 1\u20134, Oct 2023. [Online]. Available:<https://doi.org/10.1016/j.joule.2023.09.004>\n\
    - <span id=\"page-34-1\"></span>[3] J. Calma. (2023, September) Microsoft is going\
    \ nuclear to power its ai ambitions. The Verge. [Online]. Available: <https://www.theverge.com/2023/9/26/23889956/microsoft-next-generation-nuclear-energy-smr-job-hiring>\n\
    - <span id=\"page-34-2\"></span>[4] A. Reuther, P. Michaleas, M. Jones, V. Gadepally,\
    \ S. Samsi, and J. Kepner, \"Ai and ml accelerator survey and trends,\" in *2022\
    \ IEEE High Performance Extreme Computing Conference (HPEC)*. IEEE, 2022, pp.\
    \ 1\u201310.\n- <span id=\"page-34-3\"></span>[5] F. Devaux, \"The true processing\
    \ in memory accelerator,\" in *2019 IEEE Hot Chips 31 Symposium (HCS)*. IEEE Computer\
    \ Society, 2019, pp. 1\u201324.\n- <span id=\"page-34-4\"></span>[6] Y.-C. Kwon,\
    \ S. H. Lee, J. Lee, S.-H. Kwon, J. M. Ryu, J.-P. Son, O. Seongil, H.-S. Yu, H.\
    \ Lee, S. Y. Kim *et al.*, \"25.4 a 20nm 6gb function-in-memory dram, based on\
    \ hbm2 with a 1.2 tflops programmable computing unit using bank-level parallelism,\
    \ for machine learning applications,\" in *2021 IEEE International Solid-State\
    \ Circuits Conference (ISSCC)*, vol. 64. IEEE, 2021, pp. 350\u2013352.\n- <span\
    \ id=\"page-34-5\"></span>[7] S. Lee, S.-h. Kang, J. Lee, H. Kim, E. Lee, S. Seo,\
    \ H. Yoon, S. Lee, K. Lim, H. Shin *et al.*, \"Hardware architecture and software\
    \ stack for pim based on commercial dram technology: Industrial product,\" in\
    \ *2021 ACM/IEEE 48th Annual International Symposium on Computer Architecture\
    \ (ISCA)*. IEEE, 2021, pp. 43\u201356.\n- <span id=\"page-34-6\"></span>[8] M.\
    \ He, C. Song, I. Kim, C. Jeong, S. Kim, I. Park, M. Thottethodi, and T. Vijaykumar,\
    \ \"Newton: A dram-maker's accelerator-in-memory (aim) architecture for machine\
    \ learning,\" in *2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture\
    \ (MICRO)*. IEEE, 2020, pp. 372\u2013385.\n- <span id=\"page-34-7\"></span>[9]\
    \ S. Lee, K. Kim, S. Oh, J. Park, G. Hong, D. Ka, K. Hwang, J. Park, K. Kang,\
    \ J. Kim *et al.*, \"A 1ynm 1.25 v 8gb, 16gb/s/pin gddr6-based accelerator-in-memory\
    \ supporting 1tflops mac operation and various activation functions for deep-learning\
    \ applications,\" in *2022 IEEE International Solid-State Circuits Conference\
    \ (ISSCC)*, vol. 65. IEEE, 2022, pp. 1\u20133.\n- <span id=\"page-34-8\"></span>[10]\
    \ H.-S. Wong and S. Salahuddin, \"Memory leads the way to better computing,\"\
    \ *Nature nanotechnology*, vol. 10, pp. 191\u20134, 03 2015.\n- <span id=\"page-34-9\"\
    ></span>[11] H.-S. P. Wong, H.-Y. Lee, S. Yu, Y.-S. Chen, Y. Wu, P.-S. Chen, B.\
    \ Lee, F. T. Chen, and M.-J. Tsai, \"Metal\u2013oxide rram,\" *Proceedings of\
    \ the IEEE*, vol. 100, no. 6, pp. 1951\u20131970, 2012.\n- <span id=\"page-34-10\"\
    ></span>[12] W. J. Gallagher and S. S. P. Parkin, \"Development of the magnetic\
    \ tunnel junction mram at ibm: From first junctions to a 16-mb mram demonstrator\
    \ chip,\" *IBM J. Res. Dev.*, vol. 50, no. 1, pp. 5\u201323, Jan. 2006. [Online].\
    \ Available: <http://dx.doi.org/10.1147/rd.501.0005>\n- <span id=\"page-34-11\"\
    ></span>[13] J. Hoffman, X. Pan, J. W. Reiner, F. J. Walker, J. P. Han, C. H.\
    \ Ahn, and T. P. Ma, \"Ferroelectric field effect transistors for memory applications,\"\
    \ *Advanced Materials*, vol. 22, no. 26-27, pp. 2957\u20132961, 2010. [Online].\
    \ Available: <https://onlinelibrary.wiley.com/doi/abs/10.1002/adma.200904327>\n\
    - <span id=\"page-34-12\"></span>[14] P. M. Research, \"Market study on in-memory\
    \ computing: Adoption of fast-processing databases fuels the demand. report pmrrep33026,\"\
    \ 2022.\n- <span id=\"page-34-13\"></span>[15] P. Radojkovi\u0107, P. Carpenter,\
    \ P. Esmaili-Dokht, R. Cimadomo, H.-P. Charles, S. Abu, and P. Amato, \"Processing\
    \ in memory: the tipping point,\" *White paper: Processing in Memory: the Tipping\
    \ Point*, 2021.\n- <span id=\"page-34-14\"></span>[16] M. Anderson, B. Chen, S.\
    \ Chen, S. Deng, J. Fix, M. Gschwind, A. Kalaiah, C. Kim, J. Lee, J. Liang *et\
    \ al.*, \"First-generation inference accelerator deployment at facebook,\" *arXiv\
    \ preprint arXiv:2107.04140*, 2021.\n- <span id=\"page-34-15\"></span>[17] A.\
    \ Sebastian, M. Le Gallo, R. Khaddam-Aljameh, and E. Eleftheriou, \"Memory devices\
    \ and applications for in-memory computing,\" *Nature Nanotechnology*, pp. 1\u2013\
    16, 2020.\n- <span id=\"page-34-16\"></span>[18] F. Ottati, G. Turvani, G. Masera,\
    \ and M. Vacca, \"Custom memory design for logic-in-memory: Drawbacks and improvements\
    \ over conventional memories,\" *Electronics*, vol. 10, no. 18, p. 2291, 2021.\n\
    - <span id=\"page-34-17\"></span>[19] S. Kvatinsky, D. Belousov, S. Liman, G.\
    \ Satat, N. Wald, E. G. Friedman, A. Kolodny, and U. C. Weiser, \"Magic\u2014\
    memristor-aided logic,\" *IEEE Transactions on Circuits and Systems II: Express\
    \ Briefs*, vol. 61, no. 11, pp. 895\u2013899, 2014.\n- <span id=\"page-34-18\"\
    ></span>[20] E. Lehtonen, J. Poikonen, and M. Laiho, *Memristive Stateful Logic*,\
    \ 01 2014, pp. 603\u2013623.\n- <span id=\"page-34-19\"></span>[21] P. Chi et\
    \ al., \"Prime: A novel processing-in-memory architecture for neural network computation\
    \ in reram-based main memory,\" in *ACM/IEEE 43rd Annual International Symposium\
    \ on Computer Architecture (ISCA)*, June 2016, pp. 27\u201339.\n- <span id=\"\
    page-34-20\"></span>[22] A. Shafiee et al., \"Isaac: A convolutional neural network\
    \ accelerator with in-situ analog arithmetic in crossbars,\" *ACM SIGARCH Computer\
    \ Architecture News*, vol. 44, no. 3, pp. 14\u201326, 2016.\n- <span id=\"page-34-21\"\
    ></span>[23] S. Li, C. Xu, Q. Zou, J. Zhao, Y. Lu, and Y. Xie, \"Pinatubo: A processing-in-memory\
    \ architecture for bulk bitwise operations in emerging non-volatile memories,\"\
    \ in *Proceedings of the 53rd Annual Design Automation Conference*, 2016, pp.\
    \ 1\u20136.\n- <span id=\"page-34-22\"></span>[24] W. A. Simon, Y. M. Qureshi,\
    \ M. Rios, A. Levisse, M. Zapater, and D. Atienza, \"Blade: An in-cache computing\
    \ architecture for edge devices,\" *IEEE Transactions on Computers*, vol. 69,\
    \ no. 9, pp. 1349\u20131363, 2020.\n- <span id=\"page-35-0\"></span>[25] V. Seshadri,\
    \ D. Lee, T. Mullins, H. Hassan, A. Boroumand, J. Kim, M. A. Kozuch, O. Mutlu,\
    \ P. B. Gibbons, and T. C. Mowry, \"Ambit: In-memory accelerator for bulk bitwise\
    \ operations using commodity dram technology,\" in *Proceedings of the 50th Annual\
    \ IEEE/ACM International Symposium on Microarchitecture*, 2017, pp. 273\u2013\
    287.\n- <span id=\"page-35-1\"></span>[26] D. Fakhry, M. Abdelsalam, M. W. El-Kharashi,\
    \ and M. Safar, \"A review on computational storage devices and near memory computing\
    \ for high performance applications,\" *Memories-Materials, Devices, Circuits\
    \ and Systems*, p. 100051, 2023.\n- <span id=\"page-35-2\"></span>[27] G. Singh,\
    \ L. Chelini, S. Corda, A. J. Awan, S. Stuijk, R. Jordans, H. Corporaal, and A.-J.\
    \ Boonstra, \"Near-memory computing: Past, present, and future,\" *Microprocessors\
    \ and Microsystems*, vol. 71, p. 102868, 2019. [Online]. Available:<https://www.sciencedirect.com/science/article/pii/S0141933119300389>\n\
    - <span id=\"page-35-3\"></span>[28] A. Gebregiorgis, H. A. Du Nguyen, J. Yu,\
    \ R. Bishnoi, M. Taouil, F. Catthoor, and S. Hamdioui, \"A survey on memory-centric\
    \ computer architectures,\" *J. Emerg. Technol. Comput. Syst.*, vol. 18, no. 4,\
    \ oct 2022. [Online]. Available: <https://doi.org/10.1145/3544974>\n- <span id=\"\
    page-35-4\"></span>[29] F. Gao, G. Tziantzioulis, and D. Wentzlaff, \"Computedram:\
    \ In-memory compute using off-the-shelf drams,\" in *Proceedings of the 52nd annual\
    \ IEEE/ACM international symposium on microarchitecture*, 2019, pp. 100\u2013\
    113.\n- <span id=\"page-35-5\"></span>[30] M. Ali, A. Jaiswal, S. Kodge, A. Agrawal,\
    \ I. Chakraborty, and K. Roy, \"Imac: In-memory multi-bit multiplication and accumulation\
    \ in 6t sram array,\" *IEEE Transactions on Circuits and Systems I: Regular Papers*,\
    \ vol. 67, no. 8, pp. 2521\u20132531, 2020.\n- <span id=\"page-35-6\"></span>[31]\
    \ Z.-R. Wang, Y.-T. Su, Y. Li, Y.-X. Zhou, T.-J. Chu, K.-C. Chang, T.-C. Chang,\
    \ T.-M. Tsai, S. M. Sze, and X.-S. Miao, \"Functionally complete boolean logic\
    \ in 1t1r resistive random access memory,\" *IEEE Electron Device Letters*, vol.\
    \ 38, no. 2, pp. 179\u2013182, 2016.\n- <span id=\"page-35-7\"></span>[32] A.\
    \ Kazemi, F. M\xFCller, M. M. Sharifi, H. Errahmouni, G. Gerlach, T. K\xE4mpfe,\
    \ M. Imani, X. S. Hu, and M. Niemier, *Scientific reports*, vol. 12, no. 1, p.\
    \ 19201, 2022.\n- <span id=\"page-35-8\"></span>[33] R. Neale, D. Nelson, and\
    \ G. E. Moore, \"Nonvolatile and reprogrammable, the read-mostly memory is here,\"\
    \ *Electronics*, vol. 43, no. 20, pp. 56\u201360, 1970.\n- <span id=\"page-35-9\"\
    ></span>[34] G. W. Burr, M. J. Brightsky, A. Sebastian, H.-Y. Cheng, J.-Y. Wu,\
    \ S. Kim, N. E. Sosa, N. Papandreou, H.-L. Lung, H. Pozidis *et al.*, \"Recent\
    \ progress in phase-change memory technology,\" *IEEE Journal on Emerging and\
    \ Selected Topics in Circuits and Systems*, vol. 6, no. 2, pp. 146\u2013162, 2016.\n\
    - <span id=\"page-35-10\"></span>[35] Z. Guo, J. Yin, Y. Bai, D. Zhu, K. Shi,\
    \ G. Wang, K. Cao, and W. Zhao, \"Spintronics for energy-efficient computing:\
    \ An overview and outlook,\" *Proceedings of the IEEE*, vol. 109, no. 8, pp. 1398\u2013\
    1417, 2021.\n- <span id=\"page-35-11\"></span>[36] A. Kent and D. Worledge, \"\
    A new spin on magnetic memories,\" *Nature nanotechnology*, vol. 10, pp. 187\u2013\
    91, 03 2015.\n- <span id=\"page-35-13\"></span>[37] D. Reis, M. Niemier, and X.\
    \ S. Hu, \"Computing in memory with fefets,\" in *Proceedings of the international\
    \ symposium on low power electronics and design*, 2018, pp. 1\u20136.\n- <span\
    \ id=\"page-35-14\"></span>[38] J. D. Kendall and S. Kumar, \"The building blocks\
    \ of a brain-inspired computer,\" *Applied Physics Reviews*, vol. 7, no. 1, 2020.\n\
    - <span id=\"page-35-12\"></span>[39] V. Milo, G. Malavena, C. Compagnoni, and\
    \ D. Ielmini, \"Memristive and cmos devices for neuromorphic computing,\" *Materials*,\
    \ vol. 13, p. 166, 01 2020.\n- <span id=\"page-35-15\"></span>[40] D. Patterson,\
    \ K. Asanovic, A. Brown, R. Fromm, J. Golbus, B. Gribstad, K. Keeton, C. Kozyrakis,\
    \ D. Martin, S. Perissakis, R. Thomas, N. Treuhaft, and K. Yelick, \"Intelligent\
    \ ram (iram): the industrial setting, applications, and architectures,\" in *Proceedings\
    \ International Conference on Computer Design VLSI in Computers and Processors*,\
    \ 1997, pp. 2\u20137.\n- <span id=\"page-35-16\"></span>[41] J. Draper, J. Chame,\
    \ M. Hall, C. Steele, T. Barrett, J. LaCoss, J. Granacki, J. Shin, C. Chen, C.\
    \ W. Kang *et al.*, \"The architecture of the diva processing-in-memory chip,\"\
    \ in *Proceedings of the 16th international conference on Supercomputing*, 2002,\
    \ pp. 14\u201325.\n- <span id=\"page-35-17\"></span>[42] Y. Kang, W. Huang, S.-M.\
    \ Yoo, D. Keen, Z. Ge, V. Lam, P. Pattnaik, and J. Torrellas, \"Flexram: Toward\
    \ an advanced intelligent memory system,\" in *2012 IEEE 30th International Conference\
    \ on Computer Design (ICCD)*. IEEE, 2012, pp. 5\u201314.\n- <span id=\"page-35-18\"\
    ></span>[43] J. G\xF3mez-Luna, I. E. Hajj, I. Fernandez, C. Giannoula, G. F. Oliveira,\
    \ and O. Mutlu, \"Benchmarking a new paradigm: An experimental analysis of a real\
    \ processing-in-memory architecture,\" *arXiv preprint arXiv:2105.03814*, 2021.\n\
    - <span id=\"page-35-19\"></span>[44] H. Shin, D. Kim, E. Park, S. Park, Y. Park,\
    \ and S. Yoo, \"Mcdram: Low latency and energy-efficient matrix computations in\
    \ dram,\" *IEEE Transactions on Computer-Aided Design of Integrated Circuits and\
    \ Systems*, vol. 37, no. 11, pp. 2613\u20132622, 2018.\n- <span id=\"page-35-20\"\
    ></span>[45] B. Kim, J. Chung, E. Lee, W. Jung, S. Lee, J. Choi, J. Park, M. Wi,\
    \ S. Lee, and J. H. Ahn, \"Mvid: Sparse matrix-vector multiplication in mobile\
    \ dram for accelerating recurrent neural networks,\" *IEEE Transactions on Computers*,\
    \ vol. 69, no. 7, pp. 955\u2013967, 2020.\n- <span id=\"page-35-21\"></span>[46]\
    \ L. Ke, X. Zhang, J. So, J.-G. Lee, S.-H. Kang, S. Lee, S. Han, Y. Cho, J. H.\
    \ Kim, Y. Kwon *et al.*, \"Near-memory processing in action: Accelerating personalized\
    \ recommendation with axdimm,\" *IEEE Micro*, vol. 42, no. 1, pp. 116\u2013127,\
    \ 2021.\n- <span id=\"page-36-0\"></span>[47] A. Ankit et al., \"Puma: A programmable\
    \ ultra-efficient memristor-based accelerator for machine learning inference,\"\
    \ in *Proceedings of the Twenty-Fourth International Conference on Architectural\
    \ Support for Programming Languages and Operating Systems*, 2019, pp. 715\u2013\
    731.\n- <span id=\"page-36-1\"></span>[48] A. Yazdanbakhsh, C. Song, J. Sacks,\
    \ P. Lotfi-Kamran, H. Esmaeilzadeh, and N. S. Kim, \"In-dram near-data approximate\
    \ acceleration for gpus,\" in *Proceedings of the 27th International Conference\
    \ on Parallel Architectures and Compilation Techniques*, 2018, pp. 1\u201314.\n\
    - <span id=\"page-36-2\"></span>[49] J. Ahn, S. Hong, S. Yoo, O. Mutlu, and K.\
    \ Choi, \"A scalable processing-in-memory accelerator for parallel graph processing,\"\
    \ in *Proceedings of the 42nd Annual International Symposium on Computer Architecture*,\
    \ 2015, pp. 105\u2013117.\n- <span id=\"page-36-3\"></span>[50] D. Zhang, N. Jayasena,\
    \ A. Lyashevsky, J. L. Greathouse, L. Xu, and M. Ignatowski, \"Top-pim: Throughput-oriented\
    \ programmable processing in memory,\" in *Proceedings of the 23rd international\
    \ symposium on High-performance parallel and distributed computing*, 2014, pp.\
    \ 85\u201398.\n- <span id=\"page-36-4\"></span>[51] R. Nair, S. F. Antao, C. Bertolli,\
    \ P. Bose, J. R. Brunheroto, T. Chen, C.-Y. Cher, C. H. Costa, J. Doi, C. Evangelinos\
    \ *et al.*, \"Active memory cube: A processing-in-memory architecture for exascale\
    \ systems,\" *IBM Journal of Research and Development*, vol. 59, no. 2/3, pp.\
    \ 17\u20131, 2015.\n- <span id=\"page-36-5\"></span>[52] M. Gao and C. Kozyrakis,\
    \ \"Hrl: Efficient and flexible reconfigurable logic for near-data processing,\"\
    \ in *2016 IEEE International Symposium on High Performance Computer Architecture\
    \ (HPCA)*. Ieee, 2016, pp. 126\u2013137.\n- <span id=\"page-36-6\"></span>[53]\
    \ D. Reis, A. F. Laguna, M. Niemier, and X. S. Hu, \"In-memory computing accelerators\
    \ for emerging learning paradigms,\" in *Proceedings of the 28th Asia and South\
    \ Pacific Design Automation Conference*, 2023, pp. 606\u2013611.\n- <span id=\"\
    page-36-7\"></span>[54] L. Xie, H. A. Du Nguyen, J. Yu, A. Kaichouhi, M. Taouil,\
    \ M. AlFailakawi, and S. Hamdioui, \"Scouting logic: A novel memristor-based logic\
    \ design for resistive computing,\" in *2017 IEEE Computer Society Annual Symposium\
    \ on VLSI (ISVLSI)*. IEEE, 2017, pp. 176\u2013181.\n- <span id=\"page-36-8\"></span>[55]\
    \ A. Ankit, I. El Hajj, S. R. Chalamalasetti, S. Agarwal, M. Marinella, M. Foltin,\
    \ J. P. Strachan, D. Milojicic, W.-M. Hwu, and K. Roy, \"Panther: A programmable\
    \ architecture for neural network training harnessing energy-efficient reram,\"\
    \ *IEEE Transactions on Computers*, vol. 69, no. 8, pp. 1128\u20131142, 2020.\n\
    - <span id=\"page-36-9\"></span>[56] L. Song et al., \"Pipelayer: A pipelined\
    \ reram-based accelerator for deep learning,\" in *IEEE International Symposium\
    \ on High Performance Computer Architecture (HPCA)*, Feb 2017, pp. 541\u2013552.\n\
    - <span id=\"page-36-10\"></span>[57] X. Qiao, X. Cao, H. Yang, L. Song, and H.\
    \ Li, \"Atomlayer: A universal reram-based cnn accelerator with atomic layer computation,\"\
    \ in *Proceedings of the 55th Annual Design Automation Conference*, 2018, pp.\
    \ 1\u20136.\n- <span id=\"page-36-11\"></span>[58] P. Chen, M. Wu, Y. Ma, L. Ye,\
    \ and R. Huang, \"Rimac: An array-level adc/dac-free reram-based in-memory dnn\
    \ processor with analog cache and computation,\" in *Proceedings of the 28th Asia\
    \ and South Pacific Design Automation Conference*, 2023, pp. 228\u2013233.\n-\
    \ <span id=\"page-36-12\"></span>[59] G. Yuan, P. Behnam, Z. Li, A. Shafiee, S.\
    \ Lin, X. Ma, H. Liu, X. Qian, M. N. Bojnordi, Y. Wang *et al.*, \"Forms: Fine-grained\
    \ polarized reram-based in-situ computation for mixed-signal dnn accelerator,\"\
    \ in *2021 ACM/IEEE 48th Annual International Symposium on Computer Architecture\
    \ (ISCA)*. IEEE, 2021, pp. 265\u2013278.\n- <span id=\"page-36-13\"></span>[60]\
    \ Z. Wang, S. Joshi, S. Savel'Ev, W. Song, R. Midya, Y. Li, M. Rao, P. Yan, S.\
    \ Asapu, Y. Zhuo *et al.*, \"Fully memristive neural networks for pattern classification\
    \ with unsupervised learning,\" *Nature Electronics*, vol. 1, no. 2, pp. 137\u2013\
    145, 2018.\n- <span id=\"page-36-14\"></span>[61] W.-H. Chen, K.-X. Li, W.-Y.\
    \ Lin, K.-H. Hsu, P.-Y. Li, C.-H. Yang, C.-X. Xue, E.-Y. Yang, Y.-K. Chen, Y.-S.\
    \ Chang *et al.*, \"A 65nm 1mb nonvolatile computing-in-memory reram macro with\
    \ sub-16ns multiply-and-accumulate for binary dnn ai edge processors,\" in *2018\
    \ IEEE International Solid-State Circuits Conference-(ISSCC)*. IEEE, 2018, pp.\
    \ 494\u2013496.\n- <span id=\"page-36-15\"></span>[62] C.-X. Xue, W.-H. Chen,\
    \ J.-S. Liu, J.-F. Li, W.-Y. Lin, W.-E. Lin, J.-H. Wang, W.-C. Wei, T.-W. Chang,\
    \ T.-C. Chang *et al.*, \"24.1 a 1mb multibit reram computing-in-memory macro\
    \ with 14.6 ns parallel mac computing time for cnn based ai edge processors,\"\
    \ in *2019 IEEE International Solid-State Circuits Conference-(ISSCC)*. IEEE,\
    \ 2019, pp. 388\u2013390.\n- <span id=\"page-36-16\"></span>[63] Q. Liu, B. Gao,\
    \ P. Yao, D. Wu, J. Chen, Y. Pang, W. Zhang, Y. Liao, C.-X. Xue, W.-H. Chen *et\
    \ al.*, \"33.2 a fully integrated analog reram based 78.4 tops/w compute-in-memory\
    \ chip with fully parallel mac computing,\" in *2020 IEEE International Solid-State\
    \ Circuits Conference-(ISSCC)*. IEEE, 2020, pp. 500\u2013502.\n- <span id=\"page-36-17\"\
    ></span>[64] C. Eckert, X. Wang, J. Wang, A. Subramaniyan, R. Iyer, D. Sylvester,\
    \ D. Blaaauw, and R. Das, \"Neural cache: Bit-serial in-cache acceleration of\
    \ deep neural networks,\" in *2018 ACM/IEEE 45Th annual international symposium\
    \ on computer architecture (ISCA)*. IEEE, 2018, pp. 383\u2013396.\n- <span id=\"\
    page-36-18\"></span>[65] M. Kang, S. K. Gonugondla, S. Lim, and N. R. Shanbhag,\
    \ \"A 19.4-nj/decision, 364-k decisions/s, in-memory random forest multi-class\
    \ inference accelerator,\" *IEEE Journal of Solid-State Circuits*, vol. 53, no.\
    \ 7, pp. 2126\u20132135, 2018.\n- [66] H. Valavi, P. J. Ramadge, E. Nestler, and\
    \ N. Verma, \"A 64-tile 2.4-mb in-memory-computing cnn accelerator employing charge-domain\
    \ compute,\" *IEEE Journal of Solid-State Circuits*, vol. 54, no. 6, pp. 1789\u2013\
    1799, 2019.\n- [67] A. Biswas and A. P. Chandrakasan, \"Conv-sram: An energy-efficient\
    \ sram with in-memory dot-product computation for low-power convolutional neural\
    \ networks,\" *IEEE Journal of Solid-State Circuits*, vol. 54, no. 1, pp. 217\u2013\
    230, 2018.\n- <span id=\"page-36-19\"></span>[68] S. Yin, Z. Jiang, J.-S. Seo,\
    \ and M. Seok, \"Xnor-sram: In-memory computing sram macro for binary/ternary\
    \ deep neural networks,\" *IEEE Journal of Solid-State Circuits*, vol. 55, no.\
    \ 6, pp. 1733\u20131743, 2020.\n- <span id=\"page-37-0\"></span>[69] S. Angizi,\
    \ Z. He, A. S. Rakin, and D. Fan, \"Cmp-pim: an energy-efficient comparator-based\
    \ processing-in-memory neural network accelerator,\" in *Proceedings of the 55th\
    \ Annual Design Automation Conference*, 2018, pp. 1\u20136.\n- <span id=\"page-37-1\"\
    ></span>[70] J. Doevenspeck, K. Garello, B. Verhoef, R. Degraeve, S. Van Beek,\
    \ D. Crotti, F. Yasin, S. Couet, G. Jayakumar, I. Papistas *et al.*, \"Sot-mram\
    \ based analog in-memory computing for dnn inference,\" in *2020 IEEE Symposium\
    \ on VLSI Technology*. IEEE, 2020, pp. 1\u20132.\n- <span id=\"page-37-2\"></span>[71]\
    \ L. Chang, X. Ma, Z. Wang, Y. Zhang, Y. Xie, and W. Zhao, \"Pxnor-bnn: In/with\
    \ spin-orbit torque mram preset-xnor operation-based binary neural networks,\"\
    \ *IEEE Transactions on Very Large Scale Integration (VLSI) Systems*, vol. 27,\
    \ no. 11, pp. 2668\u20132679, 2019.\n- <span id=\"page-37-3\"></span>[72] S. Jung,\
    \ H. Lee, S. Myung, H. Kim, S. K. Yoon, S.-W. Kwon, Y. Ju, M. Kim, W. Yi, S. Han\
    \ *et al.*, \"A crossbar array of magnetoresistive memory devices for in-memory\
    \ computing,\" *Nature*, vol. 601, no. 7892, pp. 211\u2013216, 2022.\n- <span\
    \ id=\"page-37-4\"></span>[73] E. Breyer, H. Mulaosmanovic, T. Mikolajick, and\
    \ S. Slesazeck, \"Reconfigurable nand/nor logic gates in 28 nm hkmg and 22 nm\
    \ fd-soi fefet technology,\" in *2017 IEEE International Electron Devices Meeting\
    \ (IEDM)*. IEEE, 2017, pp. 28\u20135.\n- <span id=\"page-37-5\"></span>[74] X.\
    \ Yin, C. Li, Q. Huang, L. Zhang, M. Niemier, X. S. Hu, C. Zhuo, and K. Ni, \"\
    Fecam: A universal compact digital and analog content addressable memory using\
    \ ferroelectric,\" *IEEE Transactions on Electron Devices*, vol. 67, no. 7, pp.\
    \ 2785\u20132792, 2020.\n- <span id=\"page-37-6\"></span>[75] A. Kazemi, M. M.\
    \ Sharifi, A. F. Laguna, F. M\xFCller, R. Rajaei, R. Olivo, T. K\xE4mpfe, M. Niemier,\
    \ and X. S. Hu, \"In-memory nearest neighbor search with fefet multi-bit content-addressable\
    \ memories,\" in *2021 Design, Automation & Test in Europe Conference & Exhibition\
    \ (DATE)*. IEEE, 2021, pp. 1084\u20131089.\n- <span id=\"page-37-7\"></span>[76]\
    \ R. Khaddam-Aljameh, M. Stanisavljevic, J. F. Mas, G. Karunaratne, M. Braendli,\
    \ F. Liu, A. Singh, S. M. M\xFCller, U. Egger, A. Petropoulos *et al.*, \"Hermes\
    \ core\u2013a 14nm cmos and pcm-based in-memory compute core using an array of\
    \ 300ps/lsb linearized cco-based adcs and local digital processing,\" in *2021\
    \ Symposium on VLSI Circuits*. IEEE, 2021, pp. 1\u20132.\n- <span id=\"page-37-8\"\
    ></span>[77] M. Le Gallo, R. Khaddam-Aljameh, M. Stanisavljevic, A. Vasilopoulos,\
    \ B. Kersting, M. Dazzi, G. Karunaratne, M. Br\xE4ndli, A. Singh, S. M. Mueller\
    \ *et al.*, \"A 64-core mixed-signal in-memory compute chip based on phase-change\
    \ memory for deep neural network inference,\" *Nature Electronics*, pp. 1\u2013\
    14, 2023.\n- <span id=\"page-37-9\"></span>[78] Q. Dong, M. E. Sinangil, B. Erbagci,\
    \ D. Sun, W.-S. Khwa, H.-J. Liao, Y. Wang, and J. Chang, \"15.3 a 351tops/w and\
    \ 372.4 gops compute-in-memory sram macro in 7nm finfet cmos for machine-learning\
    \ applications,\" in *2020 IEEE International Solid-State Circuits Conference-(ISSCC)*.\
    \ IEEE, 2020, pp. 242\u2013244.\n- <span id=\"page-37-10\"></span>[79] H. Wang,\
    \ R. Liu, R. Dorrance, D. Dasalukunte, D. Lake, and B. Carlton, \"A charge domain\
    \ sram compute-in-memory macro with c-2c ladder-based 8-bit mac unit in 22-nm\
    \ finfet process for edge inference,\" *IEEE Journal of Solid-State Circuits*,\
    \ vol. 58, no. 4, pp. 1037\u20131050, 2023.\n- <span id=\"page-37-11\"></span>[80]\
    \ S. De, F. Mueller, N. Laleni, M. Lederer, Y. Raffel, S. Mojumder, A. Vardar,\
    \ S. Abdulazhanov, T. Ali, S. D\xFCnkel *et al.*, \"Demonstration of multiply-accumulate\
    \ operation with 28 nm fefet crossbar array,\" *IEEE Electron Device Letters*,\
    \ vol. 43, no. 12, pp. 2081\u20132084, 2022.\n- <span id=\"page-37-12\"></span>[81]\
    \ G. Pedretti, C. E. Graves, S. Serebryakov, R. Mao, X. Sheng, M. Foltin, C. Li,\
    \ and J. P. Strachan, \"Tree-based machine learning performed in-memory with memristive\
    \ analog cam,\" *Nature communications*, vol. 12, no. 1, p. 5806, 2021.\n- <span\
    \ id=\"page-37-13\"></span>[82] K. Akarvardar and H.-S. P. Wong, \"Technology\
    \ prospects for data-intensive computing,\" *Proceedings of the IEEE*, vol. 111,\
    \ no. 1, pp. 92\u2013112, 2023.\n- <span id=\"page-37-14\"></span>[83] C. Zhang,\
    \ H. Sun, S. Li, Y. Wang, H. Chen, and H. Liu, \"A survey of memory-centric energy\
    \ efficient computer architecture,\" *IEEE Transactions on Parallel and Distributed\
    \ Systems*, 2023.\n- <span id=\"page-37-16\"></span><span id=\"page-37-15\"></span>[84]\
    \ \"Axelera,\" [https://www.axelera.ai/digital-in-memory-computing-for-deep-learning-acceleration/.](https://www.axelera.ai/digital-in-memory-computing-for-deep-learning-acceleration/)\n\
    - [85] \"d-matrix,\" [https://www.d-matrix.ai/.](https://www.d-matrix.ai/)\n-\
    \ <span id=\"page-37-17\"></span>[86] \"Gyrfalcon tech,\" [https://www.gyrfalcontech.ai/about-us/company-overview/.](https://www.gyrfalcontech.ai/about-us/company-overview/)\n\
    - <span id=\"page-37-18\"></span>[87] \"Memcpu,\" [https://www.memcpu.com/.](https://www.memcpu.com/)\n\
    - <span id=\"page-37-19\"></span>[88] \"Memverge,\" [https://memverge.com/company/.](https://memverge.com/company/)\n\
    - <span id=\"page-37-20\"></span>[89] \"mythic,\" [https://mythic.ai/.](https://mythic.ai/)\n\
    - <span id=\"page-37-21\"></span>[90] \"Neuroblade,\" [https://www.neuroblade.com/product/.](https://www.neuroblade.com/product/)\n\
    - <span id=\"page-37-22\"></span>[91] \"Rain,\" [https://rain.ai/about-us/.](https://rain.ai/about-us/)\n\
    - <span id=\"page-37-23\"></span>[92] \"Semron,\" [https://www.semron.ai.](https://www.semron.ai)\n\
    - <span id=\"page-37-24\"></span>[93] \"Surecore,\" [https://www.sure-core.com.](https://www.sure-core.com)\n\
    - <span id=\"page-37-25\"></span>[94] \"Synthara,\" [https://www.synthara.ai.](https://www.synthara.ai)\n\
    - <span id=\"page-37-26\"></span>[95] \"Syntiant,\" [https://www.syntiant.com/.](https://www.syntiant.com/)\n\
    - <span id=\"page-37-27\"></span>[96] \"Tetramem,\" [https://www.tetramem.com.](https://www.tetramem.com)\n\
    - <span id=\"page-37-28\"></span>[97] M. Rao, H. Tang, J. Wu, W. Song, M. Zhang,\
    \ W. Yin, Y. Zhuo, F. Kiani, B. Chen, X. Jiang *et al.*, \"Thousands of conductance\
    \ levels in memristors integrated on cmos,\" *Nature*, vol. 615, no. 7954, pp.\
    \ 823\u2013829, 2023.\n- <span id=\"page-37-29\"></span>[98] \"Encharge ai,\"\
    \ [https://enchargeai.com.](https://enchargeai.com)\n- <span id=\"page-38-0\"\
    ></span>[99] \"Reconceive,\" [https://www.re-conceive.com/home.](https://www.re-conceive.com/home)\n\
    - <span id=\"page-38-1\"></span>[100] \"Fractile,\" [https://www.fractile.ai/.](https://www.fractile.ai/)\n\
    - <span id=\"page-38-2\"></span>[101] \"Untether,\" [https://www.untether.ai/.](https://www.untether.ai/)\n\
    - <span id=\"page-38-3\"></span>[102] A. Siemieniuk, L. Chelini, A. A. Khan, J.\
    \ Castrillon, A. Drebes, H. Corporaal, T. Grosser, and M. Kong, \"OCC: An automated\
    \ end-to-end machine learning optimizing compiler for computing-in-memory,\" *IEEE\
    \ Transactions on Computer-Aided Design of Integrated Circuits and Systems (TCAD)*,\
    \ vol. 41, no. 6, pp. 1674\u20131686, Aug. 2021. [Online]. Available:<https://ieeexplore.ieee.org/document/9502921>\n\
    - [103] A. A. Khan, H. Farzaneh, K. F. Friebel, L. Chelini, and J. Castrillon,\
    \ \"Cinm (cinnamon): A compilation infrastructure for heterogeneous compute in-memory\
    \ and compute near-memory paradigms,\" *arXiv preprint arXiv:2301.07486*, 2022.\n\
    - [104] H. Farzaneh, J. P. C. de Lima, M. Li, A. A. Khan, X. S. Hu, and J. Castrillon,\
    \ \"C4cam: A compiler for cam-based in-memory accelerators,\" *arXiv preprint\
    \ arXiv:2309.06418*, 2023.\n- <span id=\"page-38-4\"></span>[105] J. P. C. de\
    \ Lima, A. A. Khan, H. Farzaneh, and J. Castrillon, \"Full-stack optimization\
    \ for cam-only dnn inference,\" in *Proceedings of the 2024 Design, Automation\
    \ and Test in Europe Conference (DATE)*, ser. DATE'24. IEEE, Mar. 2024, pp. 1\u2013\
    6.\n- <span id=\"page-38-5\"></span>[106] J. Ryckaert, M. Niemier, Z. Enciso,\
    \ M. M. Sharifi, X. S. Hu, I. O'Connor, A. Graening, R. Sharma, P. Gupta, J. Castrillon,\
    \ J. P. C. de Lima, A. A. Khan, and H. Farzaneh, \"Smoothing disruption across\
    \ the stack: Tales of memory, heterogeneity, and compilers,\" in *Proceedings\
    \ of the 2024 Design, Automation and Test in Europe Conference (DATE)*, ser. DATE'24.\
    \ IEEE, Mar. 2024, pp. 1\u20136.\n\n# **ACRONYMS**\n\n<span id=\"page-39-22\"\
    ></span><span id=\"page-39-21\"></span><span id=\"page-39-20\"></span><span id=\"\
    page-39-19\"></span><span id=\"page-39-18\"></span><span id=\"page-39-17\"></span><span\
    \ id=\"page-39-16\"></span><span id=\"page-39-15\"></span><span id=\"page-39-14\"\
    ></span><span id=\"page-39-13\"></span><span id=\"page-39-12\"></span><span id=\"\
    page-39-11\"></span><span id=\"page-39-10\"></span><span id=\"page-39-9\"></span><span\
    \ id=\"page-39-8\"></span><span id=\"page-39-7\"></span><span id=\"page-39-6\"\
    ></span><span id=\"page-39-5\"></span><span id=\"page-39-4\"></span><span id=\"\
    page-39-3\"></span><span id=\"page-39-2\"></span><span id=\"page-39-1\"></span><span\
    \ id=\"page-39-0\"></span>\n\n| CIM<br>compute-in-memory             | 1  |\n\
    |--------------------------------------|----|\n| IMC<br>in-memory-computing  \
    \         | 4  |\n| IMP<br>in-memory-processing          | 4  |\n| LIM<br>logic-in-memory\
    \               | 4  |\n| PIM<br>processing-in-memory          | 4  |\n| PUM<br>processing-using-memory\
    \       | 4  |\n| CNM<br>compute-near-memory           | 4  |\n| NMC<br>near-memory-computing\
    \         | 4  |\n| PNM<br>processing-near-memory        | 4  |\n| NMP<br>near-memory-processing\
    \        | 4  |\n| ADC<br>analog-to-digital converter   | 5  |\n| APU<br>accelerated\
    \ processing unit   | 16 |\n| BL<br>bitline                        | 7  |\n| BLB<br>bitline\
    \ bar                   | 7  |\n| CIM-A<br>CIM-array                   | 5  |\n\
    | CIM-P<br>CIM-peripheral              | 5  |\n| CAM<br>content-addressable-memory\
    \    | 17 |\n| CNN<br>convolutional neural network  | 17 |\n| COM<br>compute-outside-memory\
    \        | 4  |\n| DAC<br>digital-to-analog converter . | 5  |\n| DNN<br>deep\
    \ neural network           | 18 |\n| DPU<br>data processing unit          | 11\
    \ |\n| DRAM<br>dynamic random-access memory | 5  |\n\n<span id=\"page-40-23\"\
    ></span><span id=\"page-40-22\"></span><span id=\"page-40-21\"></span><span id=\"\
    page-40-20\"></span><span id=\"page-40-19\"></span><span id=\"page-40-18\"></span><span\
    \ id=\"page-40-17\"></span><span id=\"page-40-16\"></span><span id=\"page-40-15\"\
    ></span><span id=\"page-40-14\"></span><span id=\"page-40-13\"></span><span id=\"\
    page-40-12\"></span><span id=\"page-40-11\"></span><span id=\"page-40-10\"></span><span\
    \ id=\"page-40-9\"></span><span id=\"page-40-8\"></span><span id=\"page-40-7\"\
    ></span><span id=\"page-40-6\"></span><span id=\"page-40-5\"></span><span id=\"\
    page-40-4\"></span><span id=\"page-40-3\"></span><span id=\"page-40-2\"></span><span\
    \ id=\"page-40-1\"></span><span id=\"page-40-0\"></span>\n\n| FeFET<br>ferroelectric\
    \ field-effect transistor | 2  |\n|------------------------------------------------|----|\n\
    | FPU<br>floating-point unit                     | 13 |\n| HBM<br>high bandwidth\
    \ memory                   | 7  |\n| ISA<br>instruction set architecture     \
    \       | 14 |\n| LUT<br>look-up table                           | 15 |\n| MAC<br>multiply-accumulate\
    \                     | 12 |\n| MLC<br>multi-level cell                      \
    \  | 20 |\n| MOS<br>metal-oxide-semiconductor               | 9  |\n| MRAM<br>magnetic\
    \ RAM                           | 2  |\n| MTJ<br>magnetic tunnel junction    \
    \            | 8  |\n| MPSoC<br>multiprocessor system-on-chip         | 4  |\n\
    | NN<br>neural-network                           | 20 |\n| NVM<br>non-volatile\
    \ memory                     | 2  |\n| OMP<br>outside memory processing      \
    \         | 26 |\n| PCM<br>phase change memory                     | 2  |\n| PU<br>processing\
    \ unit                          | 11 |\n| RRAM<br>resistive RAM              \
    \            | 2  |\n| SDK<br>software development kit                | 12 |\n\
    | SIMD<br>single-instruction multiple-data       | 13 |\n| SLC<br>single-level\
    \ cell                       | 20 |\n| SoC<br>system-on-chip                 \
    \         | 26 |\n| SRAM<br>static random-access memory            | 5  |\n| SOT<br>Spin-orbit-torque\
    \                       | 8  |\n| STT<br>spin-transfer-torque                \
    \    | 8  |\n\n<span id=\"page-41-6\"></span><span id=\"page-41-5\"></span><span\
    \ id=\"page-41-4\"></span><span id=\"page-41-3\"></span><span id=\"page-41-2\"\
    ></span><span id=\"page-41-1\"></span><span id=\"page-41-0\"></span>\n\n| TRA<br>triple\
    \ row activation        | 21 |\n|-------------------------------------|----|\n\
    | TSVs<br>through-silicon vias        | 7  |\n| WL<br>word line              \
    \       | 7  |\n| CAGR<br>compound annual growth rate | 2  |\n| MVM<br>matrix-vector\
    \ multiplication | 5  |\n| SA<br>sense amplifier               | 12 |\n| LLM<br>large\
    \ language models        | 1  |"
  paper_id: 27
  scores:
    originality:
      raw: 4.5
      reason: The paper presents a novel approach to generating hardware verification
        assertions using multi-LLMs, which is a fresh perspective in the field of
        VLSI design verification.
    technical_soundness:
      raw: 4.0
      reason: The methodology appears robust and well-founded, though additional details
        on the implementation could strengthen the overall technical rigor.
    clarity:
      raw: 4.2
      reason: The writing is generally clear and well-structured, but some sections
        could benefit from more concise explanations to enhance understanding.
    significance:
      raw: 4.3
      reason: The proposed solution addresses a significant challenge in hardware
        verification, potentially improving efficiency and accuracy in the design
        process.
    relevance:
      raw: 4.5
      reason: The topic is highly relevant to current trends in hardware design and
        verification, particularly with the increasing use of AI and machine learning
        in engineering applications.
  total_score: 4.25
  tier: B
- title: "DeMM: A Decoupled Matrix Multiplication Engine Supporting Relaxed\n  Structured\
    \ Sparsity"
  abstract: 'Terahertz (THz) communication is considered a viable approach to augmenting

    the communication capacity of prospective Internet-of-Things (IoT) resulting in

    enhanced spectral efficiency. This study first provides an outline of the

    design challenges encountered in developing THz transceivers. This paper

    introduces advanced approaches and a unique methodology known as Modified

    Pulse-width Modulation (MPWM) to address the issues in the THz domain. In this

    situation involving a transceiver that handles complex modulation schemes, the

    presence of a mixed signal through a high-resolution digital-to-analog

    converter (DAC) in the transmitter greatly contributes to the limitation in

    maintaining linearity at high frequencies. The utilization of Pulse-width

    Modulation-based Digital-to-Analog Converters (PWM-DACs) has garnered

    significant attention among scholars due to its efficiency and affordability.

    However, the converters'' performance is restricted by insufficient conversion

    speed and precision, especially in the context of high-resolution, high-order

    modulation schemes for THz wireless communications. The MPWM framework offers
    a

    multitude of adjustable options, rendering the final MPWM-DAC highly adaptable

    for a diverse array of application scenarios. Comparative performance

    assessments indicate that MPWM-DACs have enhanced conversion speed compared to

    standard PWM-DACs, and they also provide greater accuracy in comparison to

    Pulse-count Modulation DACs (PCM-DACs). The study presents a comprehensive

    examination of the core principles, spectrum characteristics, and evaluation

    metrics, as well as the development and experimental validation of the MPWM

    method. Furthermore, we present a RISC-V System-on-Chip (SoC) that incorporates

    an MPWM-DAC, offering a highly favorable resolution for THz IoT communications.'
  url: http://arxiv.org/abs/2401.14620v1
  keywords: ''
  document: "## I. INTRODUCTION\n\nThe exponential growth in the quantity of Internet\
    \ of Things (IoT) devices will inevitably lead to a substantial surge in wireless\
    \ network congestion. In [\\[1\\]](#page-14-0), the reason for this is that the\
    \ next wireless systems must offer increased system capacity along with exceptionally\
    \ dependable and low-latency communication. Additionally, they must possess more\
    \ flexible connectivity features to accommodate the evolving requirements of the\
    \ IoT network. RISC-V is an emerging technology that is increasingly being used\
    \ in low-power IoT applications. The architectural expansions of RISC-V and the\
    \ commercialization of Systemon-Chips (SOCs) employing this architecture have\
    \ contributed to the stability of these extensions. Additionally, the lowered\
    \ manufacturing costs and cheaper prices for end-consumers have facilitated the\
    \ commencement of commercialization. A wide range of additional RISC-V cores [\\\
    [2\\]](#page-14-1) have been produced using this instruction set architecture,\
    \ and devices based on RISC-V are now being developed for specialized applications\
    \ like the IoT, wearables, and embedded systems. The embedded IoT platform's application\
    \ processor carries out the many functions of the IoT application, such as reading,\
    \ storing, processing, and deciding when to send sensor data. The widespread use\
    \ of IoT applications and their often strict power efficiency needs have led to\
    \ the development of a wide range of application processors. It is conceivable\
    \ to encounter IoT processors with power consumption in the nanowatt range that\
    \ are specifically engineered for battery-less applications. Furthermore, cores\
    \ that target power consumption in the milliwatt and microwatt ranges are also\
    \ accessible. A method in [\\[3\\]](#page-16-0) for developing ultra-low power\
    \ software-defined radios and an instruction-set extension for the open-source\
    \ RISC-V ISA have been developed by the authors. With this extension, challenging\
    \ arithmetic operations utilized in the physical-layer protocols of IoT communication\
    \ systems are intended to be accelerated.\n\nMobile networks in the future are\
    \ anticipated to integrate nomadic, dispersed base stations that employ unmanned\
    \ aerial vehicles (UAVs). These networks will function as a supplementary element\
    \ of wireless networks, linking a significant number of individuals globally as\
    \ well as a multitude of stationary and portable cyber equipment situated in diverse\
    \ areas. Simultaneously, the data transmission speed that mobile devices can handle\
    \ is continuously rising due to the implementation of advanced networks like 5G\
    \ and enhancements made to current infrastructures. In [\\[4\\]](#page-16-1),\
    \ there will be a significant increase in the volume of data being transmitted\
    \ daily. This will lead to congestion in the communication between base stations\
    \ and the main network through traditional backhaul connections. It is anticipated\
    \ that the sub-6 Gigahertz (GHz) and millimeter-wave (mmWave) frequencies may\
    \ not have the capacity to facilitate communication for these consumers. Therefore,\
    \ Terahertz (THz) communication (0.1\u201310 THz) [\\[5\\]](#page-16-2) has been\
    \ considered a promising method to address the aforementioned issue because of\
    \ its exceptionally large bandwidth. When building THz transceivers, it is crucial\
    \ to take into account three primary performance aspects in [\\[6\\]](#page-16-3).\
    \ Operating in the THz frequency range offers a wide and untapped spectrum of\
    \ frequency bandwidth. However, increasing the amount of bandwidth assigned to\
    \ each user poses several architectural challenges.\n\n- 1) The bandwidth rises\
    \ when there is a significant difference between the two resonance frequencies.\
    \ The assumption of a linear or uniform phase response is no longer genuine, resulting\
    \ in phase distortion and the introduction of noise within the required frequency\
    \ range. Utilizing high-order bandpass matching circuits is advisable when the\
    \ fractional bandwidth exceeds 20%.\n- 2) The adoption of THz carrier frequencies\
    \ and the presence of inter-user interference (IUI) resulting from the implementation\
    \ of IoT users would pose significant challenges owing to the high path loss characteristics.\
    \ By leveraging multiplexing benefits from the propagation of distinct signal\
    \ streams across several unique paths in different spatial and polarization domains,\
    \ high diversity or rank order in THz channels can be used to boost channel capacity\
    \ or multi-user service.\n- 3) Transceivers operating at the THz frequency perform\
    \ modulation and demodulation using digital signal processing techniques. Consequently,\
    \ the digital realm may restrict the capacity of the analog baseband, mixed-signal,\
    \ and RF chain to process modulated signals with a wide range of values and a\
    \ high ratio of peak power to average power (PAPR). In addition, the effective\
    \ application of higher-order modulation necessitates the use of data converters\
    \ with better resolution, long dynamic range RF chains, and local oscillators\
    \ with reduced phase noise, all while taking high sensitivity and linearity into\
    \ account.\n\nThe ongoing challenge discussed in [\\[6\\]](#page-16-3) is to provide\
    \ energy-efficient resolutions for the DAC/ADC and DSP elements of integrated\
    \ transmitter and receiver chipsets, which need to handle data speeds beyond 50\
    \ Gbps. The back-end and mixedsignal processing use a costly commercial Arbitrary\
    \ Waveform Generator (AWG) and a real-time oscilloscope. Their objective is to\
    \ produce potent sub-channelized modulated signals externally. Subsequently, these\
    \ signals are used to provide power to the transmitting side, adjust for any discrepancies,\
    \ convert the modulated radio frequency signal into its original form, and retrieve\
    \ the unprocessed data stream on the receiving side. From a practical standpoint,\
    \ integrating an external AWG and a real-time oscilloscope into a front-end is\
    \ not a power-efficient approach. Expanding a transceiver from a single element\
    \ to a multi-antenna configuration for ultra-high data-rate applications in the\
    \ same region would greatly exacerbate the situation. To be more precise, transceivers\
    \ that use higher-order modulations have a lower RF bandwidth for a given data\
    \ rate. However, they need a much better resolution and a higher sampling rate\
    \ for the DAC and ADC relative to the signal baud rate. Also, in practice, the\
    \ sampling rate of a Nyquist-rate data converter is varied by five to six times\
    \ the baud rate for a better bit error rate. In addition, as the modulation intricacy\
    \ rises, the necessary resolution of the data converter also increases, which\
    \ becomes progressively more difficult to achieve as the data rate increases.\
    \ The ADC at the receiving end and the DAC at the transmit end are the main parts\
    \ that determine the mixed signal. This signal has to meet strict requirements\
    \ while high-speed, high-order modulation is being generated and processed. The\
    \ contributions of this study are summarised as follows:\n\n- 1) We present a\
    \ comprehensive overview that covers the prior methodologies and obstacles associated\
    \ with the design of THz IoT devices, including the design of multi-antenna arrays\
    \ and transceivers.\n- 2) By implementing a novel modulation technique called\
    \ Modified PWM (MPWM), DACs are capable of displaying pulse density and edge count\
    \ that can be adjusted. An MPWM module that incorporates an n-bit counter possesses\
    \ the capacity to accommodate (n-1) distinct edge count and pulse density configurations.\n\
    - 3) For the purpose of attaining exceptional outcomes, this approach provides\
    \ DAC designers with enhanced versatility to modify their designs to suit specific\
    \ applications.\n- 4) We present the design of an MPWM-DAC and RISC-V SoC specifically\
    \ tailored for implementation in THz IoT applications.\n\nSection II presents\
    \ a comprehensive examination of the design of high-performance THz antennas,\
    \ both on-chip and off-chip, as well as multi-antenna arrays. In Section III,\
    \ we tackle the task of developing multi-antenna transceivers for THz frequencies.\
    \ Section IV provides a detailed explanation and comparison of the concept and\
    \ spectrum of MPWM with other modulation techniques. The performance of MPWM-DAC\
    \ encompasses static error, integral nonlinearity, differential nonlinearity,\
    \ and dynamic features. Section V presents the design and FPGA testing of MPWM-DACs,\
    \ as well as the design of an MPWM-DAC and a RISC-V SoC for THz IoT applications.\n\
    \n# II. HIGH-THROUGHPUT THZ MULTI-ANTENNA ARRAY DESIGN\n\nAntennas play a crucial\
    \ role in connecting the communication system to the surrounding environment.\
    \ On-chip antennas [\\[7\\]](#page-16-4) offer a prospective option that is more\
    \ advantageous than off-chip antennas for THz communication. An on-chip antenna\
    \ can greatly simplify the matching network needed to connect the antenna to the\
    \ RF circuitry. Simplifying the matching network can significantly enhance the\
    \ system's performance by decreasing the loss and noise figure of the front end.\
    \ In order to achieve optimal radiation or prolong the battery life of the systems,\
    \ it is crucial to maximize the effectiveness of the antenna. In addition, the\
    \ size of the on-chip antenna is the primary influence in defining the chip area.\
    \ Therefore, it is imperative to minimize its dimensions to reduce manufacturing\
    \ costs. Antenna downsizing can be achieved by utilizing compact arrangements\
    \ and incorporating materials with high permittivity.\n\nTABLE I: Summary of the\
    \ state-of-the-art THz antennas on chip.\n\n| Paper | Technology         | Frequency\
    \ (GHz) | Efficiency (%) | Directivity (dBi) | Gain (dBi) |\n|-------|--------------------|-----------------|----------------|-------------------|------------|\n\
    | [8]   | 0.13 \xB5m SiGe       | 370             | 40             | 8       \
    \          | 4          |\n| [9]   | 50 \xB5m              | 300             |\
    \ 65             | NA                | 4.5        |\n| [10]  | 65-nm CMOS    \
    \     | 260-280         | 15             | 7                 | 22         |\n\
    | [11]  | 65-nm CMOS         | 438\u2013479         | 32             | 21.4  \
    \            | 12.4       |\n| [12]  | 0.18 \xB5m SiGe       | 340           \
    \  | 74             | NA                | 7.9        |\n| [13]  | 0.13 \xB5m SiGe\
    \       | 235-255         | 75             | NA                | 7          |\n\
    | [14]  | 130 nm SiGe BiCMOS | 165-175         | 45             | NA         \
    \       | 5          |\n| [15]  | 0.13 \xB5m SiGe       | 120             | 50\
    \             | NA                | 6          |\n\n## *A. On-chip antennas*\n\
    \nThe rectangle, dipole, bow-tie, and slot antennas are commonly used topologies\
    \ in on-chip design at frequencies above 300 GHz, with a gain above 2 dBi [\\\
    [16\\]](#page-16-13). The patch antenna in [\\[8\\]](#page-16-5), created using\
    \ 0.13 \xB5m SiGe technology, operates at a frequency of 370 GHz. It achieves\
    \ a gain of 4 dBi, an efficiency of 40%, and a directivity of 8 dBi. The gain\
    \ enhancement can be due to the increased electrical thickness of the dielectric\
    \ as the frequency increases. Within the high gain zone, the return loss has an\
    \ elevated value. Measuring the antenna above 100 GHz using on-probe methods gets\
    \ difficult because the metallic elements of the probe station reflect the incoming\
    \ radiated waves. This leads to a certain degree of discrepancy in the measured\
    \ gain outcomes. In addition, the use of highly delicate probe tips during the\
    \ measurements restricts the capacity to freely alter any components installed\
    \ on the probe station that are essential for facilitating the measurement procedure,\
    \ such as the mobility of the receiving reference antenna.\n\nThe antenna outlined\
    \ in [\\[9\\]](#page-16-6) consists of a square patch antenna positioned on a\
    \ silicon substrate, accompanied by a ground plane. The substrate-integrated waveguide\
    \ (SIW) is created by a patch that incorporates two T-shaped slots and short-circuited\
    \ edges using metal vias. This method increases the size of the aperture region\
    \ and reduces losses caused by surface waves and substrate. As a result, there\
    \ is an enhancement in impedance matching, bandwidth, isolation, gain, and radiation\
    \ efficiency. This arrangement minimizes the losses resulting from surface waves\
    \ and the silicon dielectric substrate. The structure may be activated by employing\
    \ two coaxial ports that are linked to the patch from the lower side of the silicon\
    \ substrate. The enhanced aperture area significantly enhances both the impedance\
    \ bandwidth and radiation characteristics within the frequency range of 0.28 THz\
    \ to 0.3 THz. The antenna has a mean gain and efficiency of 4.5 dBi and 65%, respectively.\
    \ Furthermore, it possesses a self-contained configuration that exhibits exceptional\
    \ isolation, surpassing 30 dB between the two ports. The on-chip antenna is 800\
    \ \xD7 800 \xD7 60 \xB5m<sup>3</sup> in size. In addition, the antenna in [\\\
    [17\\]](#page-16-14) was constructed on a GaAs substrate that had a thickness\
    \ of 0.5\xB5m. The transceiver consists of a Voltage-Controlled Oscillator (VCO),\
    \ buffer amplifier, modulator stage, power amplifier, frequency-tripler, and an\
    \ on-chip antenna. The on-chip antenna utilizes substrate-integrated waveguide\
    \ (SIW) technology and has a 4\xD74 configuration of slots in both the longitudinal\
    \ and transverse orientations, making use of metamaterial technology. The SIW\
    \ antenna utilizes a high-pass filter to efficiently eliminate undesired harmonics\
    \ and transmit the desired signal. The on-chip antenna has dimensions of 2 \xD7\
    \ 1 \xD7 0.0006 mm<sup>3</sup> and demonstrates a minimum gain of 0.25 decibels\
    \ isotropic (dBi), an average gain of 1.0 dBi, a minimum efficiency of 46.12%,\
    \ and an estimated efficiency of around 55%. The transceiver emits an average\
    \ output power of -15 dBm within the frequency range of 0.3-0.31 THz, which makes\
    \ it highly suitable for near-field imaging applications.\n\nThe antenna presented\
    \ in [\\[10\\]](#page-16-7) describes an antenna design that integrates an SIW\
    \ cavity with a wideband dual-slot antenna on a chip. This design was specifically\
    \ created for the use of frequency-modulated continuous-wave (FMCW) radars. Specifically,\
    \ the existence of dual slots induces two resonance modes, while the SIW cavity\
    \ induces supplementary modes that aid in achieving a broad bandwidth. The strength\
    \ of this antenna varies from -1 dB to 0 dB across the frequency range of 260-280\
    \ GHz. It has an efficiency of 15% for impedance matching, which corresponds to\
    \ a fractional bandwidth. However, due to the substrate's thinness, electric fields\
    \ are mostly limited to the area between the patch and the ground. As a result,\
    \ both the gain and bandwidth are reduced.\n\nTo create the radiation front end,\
    \ one can utilize conventional antenna designs, such as rectangular patches and\
    \ dipoles [\\[18\\]](#page-16-15). Given that the substrate thickness usually\
    \ falls within the range of 250-300 \xB5m, it is possible to position the ground\
    \ layer beneath the substrate to augment the total thickness between the antenna\
    \ and the ground layer. The lens-integrated on-chip antennas [\\[11\\]](#page-16-8),\
    \ which have been recently described, exhibit superior gain and bandwidth performance.\
    \ The lens is responsible for improving the impedance matching with the air and\
    \ also for collimating the output beam. However, these methods necessitate the\
    \ attachment of a large lens on their rear side. Antennas employing dielectric\
    \ resonators (DR) [\\[12\\]](#page-16-9) have a bandwidth reduction of less than\
    \ 15%, but make up for it with a higher gain of almost 6 dBi. By employing a dielectric\
    \ material that is a minimum of 400 \xB5m in thickness above the antenna, these\
    \ on-chip DR antennas increase their surface area.\n\nAccording to the reference\
    \ [\\[13\\]](#page-16-10), using the localized backside etching (LBE) method on\
    \ a group of folded dipole antennas made using 0.13 \xB5m SiGe technology results\
    \ in a gain of more than 7 dBi and an efficiency of over 75% within the frequency\
    \ range of 235-255 GHz. Applying the LBE approach described in [\\[14\\]](#page-16-11)\
    \ to eliminate the lossy silicon substrate around an on-chip antenna patch leads\
    \ to a notable enhancement in performance. Nevertheless, the LBE method requires\
    \ supplementary protocols and a designated area for etching, which must encompass\
    \ the patch. According to [\\[19\\]](#page-16-16), the substrate surrounding the\
    \ patch is vulnerable to data loss. In order to address this concern, a Lossless\
    \ Back End (LBE) method is implemented to eradicate the substrate responsible\
    \ for data loss. On the other hand, this approach necessitates supplementary processing\
    \ procedures and an extensive etching area. This technique offers various methods\
    \ for integrating lens antennas onto chips by achieving a sufficient balance in\
    \ bandwidth through the inclusion of a sizable lens. Positioning the ground layer\
    \ beneath the substrate enhances the electrical thickness of the dielectric. A\
    \ considerable improvement in the amplification is observed. The use of a folded\
    \ dipole antenna allows for achieving a maximum gain of 5 dBi, together with a\
    \ radiation efficiency of 45%, over the frequency range of 165-175 GHz.\n\nAn\
    \ improvement in radiation efficiency and a reduction in the space-wave quality\
    \ factor occurs when the depth of the superstate is a multiple of a quarter-wavelength\
    \ and an odd number. Thus, placing a high-dielectric superstrate over the antenna,\
    \ as explained in [\\[20\\]](#page-16-17), is an alternate approach to enhance\
    \ radiation efficiency. An elliptical slot antenna is manufactured using 0.13\
    \ \xB5m CMOS technology. This antenna is placed on the top metal layer, while\
    \ a ground layer is located at the LY metal layer, which is positioned 11 \xB5\
    m below the top metal layer. The antenna is attached to a 400 \xB5m thick quartz\
    \ superstrate. At 90 GHz, the antenna has a measured bandwidth of 3.9%, a peak\
    \ gain of 0.7 dBi, and a peak radiation efficiency of 30%. However, the research\
    \ carried out by [\\[15\\]](#page-16-12) shows that by adding a quartz superstrate\
    \ and a parasitic patch to a shorted patch antenna on a 0.13 \xB5m SiGe substrate,\
    \ a notable improvement of 6 dB may be achieved. The system operates at a frequency\
    \ of 120 GHz and achieves an efficiency of around 50%. Additionally, the system\
    \ has a gain bandwidth of 9 GHz, with a 1-dB increase within this bandwidth. Enhancing\
    \ the antenna's performance is achieved by utilizing a small superstrate, which\
    \ increases the complexity of the manufacturing process.\n\nThe unit cell of the\
    \ artificial magnetic conductor (AMC) layer is constructed by enclosing pairs\
    \ of walls, each composed of a perfect electric conductor and a perfect magnetic\
    \ conductor. Through a waveport positioned above the unit cell, a planar wave\
    \ strikes perpendicularly to the unit cell. It is possible to get an AMC layer\
    \ surface reflection phase of zero close to the center frequency by carefully\
    \ changing the sizes of the unit cell elements. Furthermore, to mitigate loss\
    \ in the AMC layer, it is necessary to decrease the magnitude of the reflection\
    \ coefficient. The bandwidth of a unit cell is defined as the range of the reflection\
    \ phase, which varies between -90 and 90 degrees. Therefore, the design of a double-rhomboid\
    \ bow-tie (DRBT) slot antenna with a back-to-back E-shaped slot and an anisotropic\
    \ magnetic conductor (AMC) layer is implemented using a 0.13 \xB5m BiCMOS technology\
    \ as presented in [\\[21\\]](#page-16-18). The antenna operates in the W-band\
    \ frequency range of 75-110 GHz. The maximum gain observed is -0.58 dBi and the\
    \ bandwidth is 6 GHz.\n\n## *B. off-chip antennas*\n\nIntegrating chipsets with\
    \ off-chip antennas using normal interface techniques is limited by strict frequency\
    \ limitations. Offchip antennas are frequently used on Printed Circuit Boards\
    \ (PCBs) when larger dielectric layers are necessary. The reduced manufacturing\
    \ expenses associated with off-chip antennas enable the development of extensive\
    \ off-chip antenna arrays, resulting in significantly amplified levels of radiated\
    \ power. These antennas offer superior efficiency, gain, and bandwidth compared\
    \ to on-chip antennas. While sacrificing bandwidth, the antenna described in [\\\
    [22\\]](#page-16-19) utilizes numerous parallel leaky-wave structures to enhance\
    \ the gain. The antenna off-chip featured in [\\[23\\]](#page-16-20) achieves\
    \ a high level of amplification and a broad range of frequencies by combining\
    \ a multi-layered board technology with an L-probe feedline.\n\nIt is crucial\
    \ to have broadband antenna designs that can be accommodated within a limited\
    \ number of metal layers and thin dielectric layers in order to manufacture antennas\
    \ using commonly accessible rigid or flexible PCBs. The antenna presented in [\\\
    [18\\]](#page-16-15) reveals a stacked patch antenna that is connected to an external\
    \ cavity backplane and operates in the frequency range of 91.5-134 GHz. The fractional\
    \ bandwidth of the object is around 38% and it has a peak gain of 8.1 dBi. This\
    \ antenna demonstrates the greatest fractional bandwidth compared to off-chip\
    \ antennas operating over 100 GHz. In addition, the ground and patch layers both\
    \ utilize copper cladding with a thickness of 12 \xB5m. The dielectric substrates\
    \ have a relative permittivity (\u03F5r) of 2.6 and a loss tangent of 0.003. In\
    \ light of the aforementioned methodologies, the viewpoint about off-chip design\
    \ problems can be succinctly summarized as follows:\n\n- 1) In order to provide\
    \ broad frequency compatibility between the input G-S-G port and the slot, the\
    \ feedline is constructed as a transmission line consisting of two sections. The\
    \ 50 \u03A9 impedance part is connected to the 50 \u03A9 G-S-G port, while a larger\
    \ low-impedance piece is connected to the impedance of the slot aperture. Undesirable\
    \ parallel plate modes, characterized by resonance at different frequencies, arise\
    \ when conductor layers are situated both above and below the feedline.\n- 2)\
    \ In order to enhance the performance of wideband systems, the use of FPC technology\
    \ permits a maximum thickness of 50 \xB5m for the substrate layers Sub1, Sub3,\
    \ and Sub4. Sub2, with a reduced thickness of 25 \xB5m, produces substantial coupling\
    \ between the feedline and slot aperture. The G-S-G probe port is situated within\
    \ the Gnd2 layer and is linked to the feedline. The feedline is a grounded co-planar\
    \ waveguide designed to be compatible with the G-S-G probe port.\n- 3) The process\
    \ of cavity generation entails the utilization of via arrays to construct a conductor\
    \ enclosure surrounding patches. The manufacturing process can cause surface waves\
    \ to pass through even the tiniest gaps between vias. Developing a broadband antenna\
    \ is a significant challenge in this matter. At higher frequencies in a wideband\
    \ antenna, the wavelength becomes shorter, necessitating a greater proximity between\
    \ vias compared to lower frequencies.\n\nThe primary issue associated with off-chip\
    \ antennas is effectively guiding signals from the semiconductor to the antenna\
    \ structure. The interface, equipped with pads on both ends, creates a network\
    \ that exhibits self-inductance as well as capacitance and resistance. Ideally,\
    \ this combination should operate as a direct path for electric current. The capacitors\
    \ and inductors in this network resonate together at high frequencies, establishing\
    \ a frequency limit called the self-resonance frequency (SRF). Regarded as the\
    \ prevailing method for packaging in [\\[24\\]](#page-16-21) and [\\[25\\]](#page-16-22),\
    \ aluminum or gold wirebonds can also be employed to provide a connection between\
    \ the chipset and antennas on the PCB. Standard wires typically have a diameter\
    \ ranging from 10 to 75 \xB5m. In high-power applications, many wirebonds may\
    \ be used in tandem. In the usual wirebonding procedure, the wire is aligned at\
    \ an almost right angle to the pads at one end. The wire becomes longer than required,\
    \ leading to substantial self-inductance. The wirebonds exhibit a range of self-inductance\
    \ values, which can span from a few hundred picohenries to a few nanohenries.\
    \ Consequently, the wirebonds exhibit considerable reactance at higher frequencies,\
    \ leading to a large reduction in their insertion loss and insufficient impedance\
    \ matching.\n\n## *C. On-chip and off-chip antenna array*\n\nAs the frequency\
    \ rises, the dielectric thickness increases as a percentage of the wavelength,\
    \ while each antenna element's area decreases, increasing the gain bandwidth of\
    \ the on-chip antennas. Following this logical sequence, this section explores\
    \ off-chip and on-chip antenna arrays for multi-antenna systems in cases where\
    \ employing a single-element radiator is impractical. In the THz frequency band,\
    \ a chip with a single antenna element generates maximum power that covers a limited\
    \ distance. Antenna arrays are utilized to enhance the coverage area of a communication\
    \ system by greatly amplifying the total radiated power and enhancing the beam's\
    \ directionality. The antenna components are spaced at a distance equivalent to\
    \ half the wavelength, enabling a coherent combination of radiated waves and limiting\
    \ interference.\n\nUsing an on-chip antenna array simplifies routing and matching\
    \ between transceiver circuitry and antenna components. As previously indicated,\
    \ on-chip antenna performance improves with higher frequencies, with most arrays\
    \ operating beyond 150 GHz. The authors in [\\[26\\]](#page-17-0) implement an\
    \ 8-element slot array with a 10 mm Si lens at the bottom in a 65 nm CMOS process.\
    \ Simulated directivity is 16.6 dB, the overall radiation efficiency is 42%, and\
    \ the impedance bandwidth exceeds 60 GHz in the 260 GHz range. The eight-element\
    \ on-chip antenna array in [\\[27\\]](#page-17-1) features a \u03BB/4 thick Quartz\
    \ superstrate on top. The Quartz superstrate boosts gain by 3.1 dB and efficiency\
    \ from 22% to 45% for each antenna element. This leads to an array gain of 11-12\
    \ dBi and a 10 GHz bandwidth at 385 GHz frequency. At 412-416 GHz, a 4 \xD7 4\
    \ patch antenna in an oscillator network obtains a peak effective isotropic radiated\
    \ power (EIRP) of 14 dBm in [\\[28\\]](#page-17-2).\n\nEnhanced gain and bandwidth\
    \ performance can be achieved by scaling antenna size and accessing thicker substrates\
    \ in off-chip technologies. The allocation of space for an antenna on a PCB board\
    \ is not a major issue, in contrast to an on-chip antenna, where it occupies a\
    \ substantial portion of the chip. This enables the use of larger off-chip antenna\
    \ arrays, hence improving antenna gain performance and transceiver chip EIRP.\
    \ The diminished amplification and frequency range capabilities shown in the following\
    \ investigations can likely be attributed to the reduced thickness of the interposer\
    \ layer and the scattered connection between the chipset and the interposer. The\
    \ designs in [\\[29\\]](#page-17-3) are pioneering examples of off-chip antenna\
    \ arrays operating at frequencies above 100 GHz. This work constructs two patch\
    \ arrays on a RO3003 substrate with a dielectric constant (\u03F5r) of 3. One\
    \ array is fed in series, while the other is supplied corporately. The serial-fed\
    \ antenna has a maximum gain of 6 dBi and a frequency range of 5 GHz centered\
    \ at 122 GHz. On the other hand, the corporate-fed antenna has a gain of 5 dBi\
    \ and a bandwidth of 7 GHz.\n\nTo showcase long-distance communication capabilities,\
    \ the authors in [\\[30\\]](#page-17-4) proposed the use of a 384-element array\
    \ operating at a frequency of 90 GHz. This array would consist of 11 layers of\
    \ metal stacks on a PCB, and would be linked to the chipset by flip-chip interconnects.\
    \ The array achieved an impressive EIRP of 60 dBm within a conservative 20 GHz\
    \ bandwidth. The authors in [\\[31\\]](#page-17-5) constructed 16 patch antennas\
    \ on an interposer board positioned between the chipset and PCB to facilitate\
    \ large-scale MIMO applications. This study employed an interposer board to establish\
    \ a connection between narrow signal lines on the chipset and wider lines on the\
    \ board. The bandwidth for return loss was measured to be between 71 and 84 GHz,\
    \ while the peak gain was about 5 dBi.\n\nThe on-chip antenna array in [\\[32\\\
    ]](#page-17-6) operates within the frequency range of 0.450-0.475 THz. It utilizes\
    \ two vertically oriented DRs on a silicon substrate, employing standard CMOS\
    \ technology. To reduce energy dissipation, one can create a winding pathway in\
    \ the silicon substrate and enclose it with a metallic barrier, thereby reducing\
    \ substrate loss and surface wave effects. The integration of slots and vias results\
    \ in the antenna adopting a metamaterial structure that occupies a very small\
    \ area. The dimensions of the antenna are 400 x 400 \xD7 135 \xB5m<sup>3</sup>\
    \ . The antenna achieves a peak gain of 4.5 decibels relative to an isotropic\
    \ radiator and has a radiation efficiency of 45.7% at a frequency of 0.4625 THz.\
    \ A series-fed double-DR on-chip antenna array is a promising candidate for THz\
    \ integrated circuits. By employing CMOS 20\xB5m silicon technology, the authors\
    \ in [\\[33\\]](#page-17-7) develop an innovative on-chip antenna array designed\
    \ for operation at frequencies ranging from 0.6 to 0.65 THz. This array configuration\
    \ has three vertically aligned layers of Silicon-metal-Silicon. The intermediate\
    \ metal layer functions as a ground plane sandwiched between two silicon layers.\
    \ The uppermost layer consists of a pair of antennas, each equipped with three\
    \ interconnected radiating elements. Radiation elements exhibit the behavior of\
    \ linked dual rings, like a metamaterial. This arrangement increases the effective\
    \ aperture area of the array. The inclusion of metallic via-holes between radiation\
    \ elements in three layers mitigates the effects of surface waves and substrate\
    \ losses. The antenna is operated via microstrip wires that are open-circuited\
    \ on the rear of the structure. The ground-plane layer incorporates slots that\
    \ facilitate the transmission of electromagnetic energy from the lower layer to\
    \ the radiating components located on the upper layer. The dimensions of the antenna\
    \ array are 0.4\xD7 0.4 \xD7 0.06 mm<sup>3</sup> . The on-chip antenna array in\
    \ [\\[33\\]](#page-17-7) attains an average radiation gain, efficiency, and isolation\
    \ of 7.62 dBi, 32.67 %, and negative 30 dB, respectively. The results confirm\
    \ the effectiveness of the antenna array for THz-integrated circuits.\n\nIn essence,\
    \ on-chip antennas that operate at frequencies over 150 GHz adhere to radiation\
    \ laws, enabling the creation of a fully integrated communication system. On-chip\
    \ antennas are frequently used at lower THz frequencies due to their ability to\
    \ provide a broad gain bandwidth while minimizing substrate elevation. Although\
    \ there have been improvements in radiation quality, the linkage between the chipset\
    \ and off-chip antenna remains a major obstacle to attaining optimal off-chip\
    \ radiation frequencies. Wirebonds have traditionally been employed to establish\
    \ connections with frequencies above 100 GHz. Furthermore, flip-chip technologies\
    \ [\\[34\\]](#page-17-8), namely copper-pillar and solder bump, offer an impressive\
    \ SRF of around 200 GHz. This makes them highly suitable for the development of\
    \ future THz communication systems.\n\n## III. THZ MULTI-ANTENNA TRANSCEIVERS\
    \ DESIGN\n\nThe majority of studies in multi-antenna transceiver design have focused\
    \ on beamforming using phased-array solutions. Following the first showcases of\
    \ single, four, and eight-element arrays on a single microchip, the phased-arrays\
    \ rapidly progressed to 16 and 32-element configurations on a single microchip.\
    \ In the transmit mode of a phased array with Nelements presented by the following\
    \ principle in [\\[35\\]](#page-17-9), the effective isotropic radiated power\
    \ (EIRP) is defined as the product of the total transmit power P<sup>t</sup> and\
    \ the transmit antenna gains Gt. P<sup>t</sup> represents the power emitted per\
    \ element, while Gt is directly proportional to N. The EIRP, on the other hand,\
    \ is directly proportional to N<sup>2</sup> . When operating in receive mode,\
    \ the gain of the phased array antenna is directly proportional to the number\
    \ of elements, denoted as N. Hence, the link budget, which is directly proportional\
    \ to the product of Pt, Gt, and receiver antenna gain Gr, exhibits a cubic dependency\
    \ on the number of N<sup>3</sup> . This section will discuss two designs: high-performance\
    \ multi-antenna systems and low-complexity MIMO precoding designs. These architectures\
    \ address the obstacles and concerns related to increasing bandwidth, modulation\
    \ order, and transmit power in THz technology.\n\n## *A. High-performance multi-antenna\
    \ systems*\n\nDue to the reduced dimensions of passive components in the THz frequency\
    \ range, it becomes feasible to consider the implementation of interconnected\
    \ multi-antenna transceiver arrays. All-digital beamforming enables the transmission\
    \ of several beams simultaneously, providing maximum flexibility and data transfer\
    \ rate. The digital beamforming (DBF) technique has three main benefits. By employing\
    \ digital precoding, it becomes possible to get precise resolution in both magnitude\
    \ and phase. Moreover, a DBF array possesses the potential to increase its capacity\
    \ by superimposing several beams to handle various data streams. In addition,\
    \ the complete DBF architecture enables independent beamforming precoding on each\
    \ subcarrier or resource block for multicarrier transmissions. This results in\
    \ exceptional performance across a broad spectrum of frequencies. As the authors\
    \ presented in [\\[36\\]](#page-17-10), DBF-based millimeter-wave MIMO systems\
    \ enable multi-user access and exhibit exceptional spectrum utilization. The principal\
    \ constraints associated with the development of a DBF-based millimeter-wave MIMO\
    \ transceiver are hardware complexity, financial investment, and power consumption.\
    \ In order to facilitate the further development of millimeter-wave MIMO systems\
    \ utilizing DBF, cost reduction in digital baseband processing will be the primary\
    \ objective. Novel semiconductor manufacturing processes and improved integration\
    \ techniques are assisting in the resolution of these constraints.\n\nLately,\
    \ there has been a concentration on a hybrid beamforming approach that merges\
    \ analog beamforming and digital MIMO coding. This technology offers a significant\
    \ advantage by minimizing the complexity of the digital baseband through the utilization\
    \ of a reduced number of up/down conversion chains in systems that have a high\
    \ number of antennas. Consequently, it has emerged as a feasible choice for both\
    \ outdoor and interior millimeter-wave/terahertz communication. By combining multibeam\
    \ digital baseband processing with analog beamforming, it is possible to simultaneously\
    \ improve both multiplexing and beamforming gain. RF phase shifters are primarily\
    \ used to alter the direction of the main lobe. RF variable gain attenuators and/or\
    \ amplifiers (VGAs) provide interference spatial filtering by aligning the zero\
    \ locations of each beamforming line with the incident angle of interference.\
    \ The number of parallel data streams K determines the minimum required number\
    \ of RF chains NRF in a hybrid design. On the other hand, the beamforming gain\
    \ is achieved by using NRF complex weighting factors that appear at each antenna.\
    \ In the context of sub-array systems, hybrid beamforming have the capability\
    \ to either receive or transmit an entire set of data streams from N antennas\
    \ when N = NRF or only a subset of data streams when partially interconnect RF\
    \ circuits supply subarrays, i.e., NRF < N, per antenna. A comprehensive array\
    \ fulfills the function of an exclusively DBF, upon closer inspection. NRF \xD7\
    \ N denotes the quantity of signal processing paths for the subarray, specifically\
    \ from the digital baseband to the antenna front-end. N<sup>2</sup> signifies\
    \ the total number of signal processing paths for the array. On the contrary,\
    \ the beamforming gain of the sub-array is equivalent to the full array's NRF\
    \ /N. Consequently, in hybrid beamforming, the objectives of beamforming gain\
    \ and signal processing complexity are in direct opposition. As the authors presented\
    \ in [\\[37\\]](#page-17-11), it allows for the receiving of two streams by utilizing\
    \ the Cartesian combining principle. The execution of this design for a two-stream\
    \ reception necessitates the use of eight splitters, twenty combiners, and twelve\
    \ mixers. The presence of several signal routes leads to electromagnetic cross-talk,\
    \ which arises from the frequent cross-overs that take place between these paths.\n\
    \nPhased arrays provide a range of capabilities found in a multi-antenna system,\
    \ such as enhancing capacity and diversity. As the authors presented in [\\[38\\\
    ]](#page-17-12), the initial integration of the phased-array system in silicon\
    \ involved the incorporation of local oscillator (LO) phase shifting. The SiGe\
    \ transceiver has four components for both transmitting and receiving signals,\
    \ as well as circuitry for generating and distributing LO frequencies. Additionally,\
    \ it features a local design that allows for phase-shifting in the LO path, enabling\
    \ beam steering. The main benefit of this architecture is that the phase-shifters\
    \ are positioned at a distance from the RF route. Consequently, the implementation\
    \ of the LO-path phase-shifting scheme at a local level allows for the creation\
    \ of a reliable distribution network that can effectively handle THz frequencies\
    \ and/or a larger number of components. The presence of large array sizes can\
    \ present a notable design obstacle for the LO distribution network. Nevertheless,\
    \ this worry can be mitigated by employing a phased-array transceiver that incorporates\
    \ RF phase shifting. In [\\[39\\]](#page-17-13), the described approach involves\
    \ circuitry that allows for accurate manipulation of beam direction, phase, and\
    \ amplitude at each individual front end. Additionally, independent control of\
    \ tapering and beam steering is achieved at the array level. The integrated circuit\
    \ is designed using 130-nm SiGe BiCMOS technology. It consists of 32 transceiver\
    \ components and supports simultaneous independent beams in two polarizations\
    \ for both transmission and reception operations.\n\n## *B. Low-complexity MIMO\
    \ precoding Designs*\n\nWithout investigating innovative architectural approaches,\
    \ it may be unfeasible to include a unified ultra-high data-rate wireless transceiver\
    \ that surpasses 50 Gbps. This is attributed to the substantial power consumption\
    \ of the baseband units and data converter, which may reach a maximum of 10 watts.\
    \ Utilizing a multi-antenna arrangement, rather than a single-element system will\
    \ result in an increase in the power consumption of the transceiver beyond the\
    \ frequency of 100 GHz. Streamlining the complexity of data converters and back-end\
    \ digital signal processing (DSP) is essential, as it allows for the creation\
    \ of energyefficient and cost-effective high-speed wireless connections that can\
    \ be accessed by a large number of customers. Conventional MIMO systems are solely\
    \ operational within the baseband domain. However, full-digital precoders are\
    \ impracticable due to the substantial energy consumption of high-frequency mixed-signal\
    \ components and the attendant manufacturing costs. The primary reason for this\
    \ is the considerable number of RF chains that are necessary for THz MIMO systems,\
    \ which frequently comprise several tens to hundreds of antennas. An array of\
    \ signals is processed using a hybrid cascaded RF precoder and baseband precoder.\
    \ The RF precoder employs analog phase shifters to precisely manipulate the phase\
    \ of signals entering and departing the antenna components. This enables the creation\
    \ of multiple beams that align with the dominant THz channel paths. The baseband\
    \ precoder exhibits more adaptability compared to the constant-gain/phase-only\
    \ functionalities of the RF precoder. The substantial size of the antenna in the\
    \ hybrid precoder architecture poses challenges in obtaining an optimum full-digital\
    \ precoder. To develop a hybrid precoder, an optimal full-digital precoder must\
    \ be employed. The complete digital precoder is often derived from the dominant\
    \ singular vectors of a channel matrix in the explicit spatial domain. The computation\
    \ of the singular value decomposition (SVD) for the explicit channel matrix is\
    \ complex because of the abundance of antennas. In addition, receiver implementation\
    \ of hybrid precoder designs is commonplace to reduce the quantity of feedback\
    \ demanded. The execution of SVD ensues as a result. Determining the optimal full-digital\
    \ precoder is the initial step in formulating the hybrid precoder design as a\
    \ sparse optimization problem. One potential solution for the hybrid precoder\
    \ design is to employ simultaneous orthogonal matching pursuit (SOMP) [\\[40\\\
    ]](#page-17-14), in which the approach aims to select a suitable combination of\
    \ analog beamforming vectors from a pre-determined set of options. Implementing\
    \ this approach results in near-optimal performance. However, the hybrid precoder\
    \ using the SOMP technique necessitates matrix inversion, resulting in a significant\
    \ increase in complexity. It then computes the matching baseband precoding matrix\
    \ in order to minimize the Euclidean distance between the current outcome and\
    \ the ideal precoding matrix. The authors of [\\[41\\]](#page-17-15) proposed\
    \ an alternating minimization (Alt-Min) approach to separate the hybrid precoder\
    \ design into two distinct sub-problems. Initially, a temporary digital precoder\
    \ is derived using the least square solution, while the initial analog beamformer\
    \ is generated with random phases. Following this, the associated analog beamformer\
    \ is modified via the phase extraction approach. The process of alternate iterations\
    \ persists until the criterion stated by the user is met. The SOMP-based hybrid\
    \ precoding can be interpreted as a method for sparse reconstruction, whilst the\
    \ Alt-Min algorithm can be seen as a strategy for manifold optimization.\n\nAlthough\
    \ there have been several suggestions for adopting hybrid precoding techniques,\
    \ the VLSI implementation of precoding algorithms has not received substantial\
    \ attention. The authors of the study described in [\\[42\\]](#page-17-16) established\
    \ a technique known as parallel-index-selection matrix-inverse-bypass simultaneous\
    \ orthogonal matching pursuit (PIS-MIB-SOMP). This approach seeks to avoid the\
    \ necessity of performing a complete matrix inversion in the traditional SOMP\
    \ algorithm. The researchers in [\\[43\\]](#page-17-17) developed a modified version\
    \ of the SOMP technique called orthogonality-based matching pursuit (OBMP) [\\\
    [44\\]](#page-17-18). OBMP employs a discrete Fourier transform (DFT) codebook\
    \ to replace the original candidate array solutions, hence reducing computing\
    \ expenses. The authors of [\\[45\\]](#page-17-19) introduced an improved iteration\
    \ of the orthogonal matching pursuit (OMP) method in their paper. The enhanced\
    \ OMP algorithm integrated a revolutionary least-squares approach that employed\
    \ QR decomposition. The authors also investigated the possible advantages of utilizing\
    \ the Coordinate Rotation Digital Computer (CORDIC) method for implementing this\
    \ algorithm within the framework of Very Large Scale Integration (VLSI). However,\
    \ for these designs to serve as a reference design, an ideal precoding matrix\
    \ is required. This matrix can only be constructed by using the SVD of the explicit\
    \ channel matrix. However, this process significantly increases the complexity\
    \ of the transceivers, making it impractically high.\n\nInspired by the approach\
    \ in [\\[46\\]](#page-17-20) to providing extensive bandwidths, short wavelengths\
    \ at mm-wave or THz generate a spatial signal space with a high number of dimensions.\
    \ This presents an opportunity to leverage high-dimensional MIMO techniques in\
    \ order to achieve substantial capacity increases. Through the integration of\
    \ a hybrid analog-digital transceiver and the beamspace MIMO communication concept,\
    \ continuous aperture-phased MIMO attains performance that is nearly optimal while\
    \ significantly reducing complexity. The researchers demonstrated in [\\[47\\\
    ]](#page-17-21) that beamspace singular value decomposition (SVD) is the most\
    \ efficient approach for obtaining a full-digital precoder with minimal effort.\
    \ It efficiently reduces power consumption for both the base station and user\
    \ equipment. The authors utilize compressed sensing (CS)-based channel estimators\
    \ to get reduced-dimension beamspace channel state information (CSI). This algorithm\
    \ performs SVD implicitly on the reduceddimensional beamspace channel instead\
    \ of explicitly on the large-dimensional spatial domain channel. The total complexity\
    \ is proportional to the number of antennas in the MIMO system, which is considerably\
    \ higher than the channel sparsity. The CS-BHP technique described in [\\[47\\\
    ]](#page-17-21) achieves a considerable reduction in complexity compared to the\
    \ state-of-the-art approach by 99.6% by utilizing low-dimensional beamspace CSI.\
    \ This eliminates the need for matrix inversion calculations and matching pursuit\
    \ rounds, making it more efficient than an ideal full-digital precoder. Furthermore,\
    \ it has a performance decrease of less than 5%. The suggested design in [\\[48\\\
    ]](#page-17-22) utilizes a two-stage precoding approach. The first stage involves\
    \ using a sparse matrix for precoding in the beamspace domain, followed by converting\
    \ the outcome to the antenna domain using an inverse fast Fourier transform. This\
    \ move is performed to simplify the complexity of multi-user (MU) precoding in\
    \ all-digital base station systems. This approach uses OMP to calculate sparse\
    \ precoding matrices in the beamspace domain. As a result, this technique reduces\
    \ the complexity of precoding compared to standard linear antenna-domain precoders\
    \ that involve dense matrix-vector multiplication. The output is preprocessed\
    \ and then transformed into the antenna domain using an inverse fast Fourier transform\
    \ (IFFT). The authors demonstrate that their methods provide a bit error-rate\
    \ (BER) performance that is similar to that of traditional antenna-domain Wiener\
    \ filter (WF) precoding, but with almost double the complexity.\n\n## IV. HIGH-SPEED\
    \ HIGH-RESOLUTION DAC FOR HIGH ORDER MODULATION\n\nAs described in prior research,\
    \ a Pulse-width Modulation-based Digital-to-Analog Converter (PWM-DAC) consists\
    \ primarily of a filter and a PWM generator [\\[49\\]](#page-17-23) [\\[50\\]](#page-17-24).\
    \ Integrated modules for Microcontroller Units (MCUs) or Digital Signal Processors\
    \ (DSPs) constitute the PWM generator, which is principally digital. Frequently,\
    \ the filter is a simple first-order RC filter. The primary benefits of PWM-DACs\
    \ are their low cost and straightforward implementation, which are outcomes of\
    \ their straightforward architecture. These characteristics have attracted significant\
    \ interest in both academic and industrial spheres. A considerable number of semiconductor\
    \ firms that specialize in DSPs and/or MCUs have published technical documents\
    \ detailing the construction of PWM-DACs [\\[51\\]](#page-17-25)\u2013[\\[53\\\
    ]](#page-17-26).\n\nDAC performance is contingent on filter design and modulation\
    \ technique. The bit breadth of the counter within the modulator and the efficacy\
    \ of the low pass filter impose constraints on their resolution. In order to implement\
    \ a DAC with a resolution of n bits, the bit width of the counter must be greater\
    \ than n. In addition, beyond the pass band, the filter must have a low cutoff\
    \ frequency and/or high attenuation, which restricts the speed and precision of\
    \ these DACs.\n\nSecond-order low-pass Butterworth filters are the most economically\
    \ viable option when it comes to PWM-DAC, PCM-DAC, and FONS-DAC. The normalized\
    \ cutoff frequency of the PWM filter, expressed in the least significant bits\
    \ (LSB), is approximate [\\[54\\]](#page-17-27) for the worst ripple of R<sup>W</sup>\
    \ C in this instance.\n\n$$f\\_c T = 0.9^2 \\sqrt{R\\_{WC}/2^n} \\,\\text{.}\\\
    tag{1}$$\n\nwhere f<sup>c</sup> is the cutoff frequency and T = 2<sup>n</sup>/fclk\
    \ is the PWM period (n is the bit width of the counter inside the PWM and PCM\
    \ circuit, and fclk is the working clock). The cutoff frequency of the second-order\
    \ low-pass Butterworth filter for a 12-bit PWM-DAC with a 100 MHz clock rate is\
    \ 250 Hz, and the DAC's settling time is 0.76 ms; therefore, the conversion rate\
    \ is less than 1 kHz, given that the settling time can only represent a negligible\
    \ portion of the conversion period. Moreover, as the resolution (n) of the DAC\
    \ increased, there was a significant decrease in fcT; consequently, achieving\
    \ a high-resolution PWM-DAC operating at a reasonable speed will be more difficult.\n\
    \nIn order to increase the speed, PCM-DAC and first-order noise shaping DAC (FONS-DAC)\
    \ reduce the stringent requirement for fcT through the application of extremely\
    \ frequent pulses. Nevertheless, PCM periods contain a large number of ascending\
    \ and descending edges, which leads to significant static error as a consequence\
    \ of the edges' nonideal properties.\n\nThe edge induces a maximum error of 2\
    \ <sup>11</sup> \xD7 \u2206W \xD7 fclk for a 12-bit PCM-DAC, where \u2206W represents\
    \ the pulse width deviation resulting from the delay differential between the\
    \ rising and falling edges. Consequently, a 2048-fold increase in the pulse width\
    \ deviation can significantly compromise the accuracy of the DAC. Additionally,\
    \ PCM-DAC and FONS-DAC exhibit the worst error performance as pulse density increases.\n\
    \n## <span id=\"page-8-0\"></span>*A. Principle and spectrum of MPWM*\n\n![](_page_8_Figure_1.jpeg)\n\
    \nFig. 1: The waveform of unfiltered MPWM digital output.\n\n<span id=\"page-8-1\"\
    ></span>The period of the MPWM signal for an n-bit counter may be calculated using\
    \ the formula T = 2n/fclk, where fclk represents the clock frequency. We define\
    \ two terms: the splitting factor (SF) and the splitting number (SN). These terms\
    \ are related by SN = 2SF . MPWM is divided into SN sub-regions. The waveform\
    \ of MPWM for n = 5 and SF = 1, 2, 4 is displayed in Fig. [1](#page-8-0) for a\
    \ duration of 32 clock periods. The matching spectrum can be seen in Fig. [2.](#page-8-1)\n\
    \n![](_page_8_Figure_4.jpeg)\n\nFig. 2: The spectrum of unfiltered MPWM digital\
    \ output.\n\n<span id=\"page-8-2\"></span>![](_page_8_Figure_6.jpeg)\n\nFig. 3:\
    \ The MPWM generation using a 5-bit counter with 2 address bits.\n\n<span id=\"\
    page-9-0\"></span>Fig. [3](#page-8-2) illustrates the principle of generating\
    \ the MPWM wave. The lower significant bits (SF) of the counter correspond to\
    \ the address, which represents the sub-region inside an MPWM period T. The upper\
    \ bits, namely (5 \u2212 SF) bits, indicate the specific \"wave\" that should\
    \ be placed in the sub-region. The counter's address bits are sent to an Address\
    \ Decoder, which produces an output of SN bits. When the output of the address\
    \ decoder is a signal of length SN with a binary value of 1 at location SNpos+1.\
    \ This signal represents the location of the \"wave\" inside the full MPWM signal.\
    \ The higher (5 \u2212 SF) bits of the counter are connected to the \"WAV decoder\"\
    , whose output WAV is a signal with a length of 2 <sup>5</sup>\u2212SF . Fig.\
    \ [4](#page-9-0) displays\n\n| SN pos | Waveform | WAV | Waveform            \
    \          |\n|--------|----------|-----|-------------------------------|\n| \
    \       | 1 0 0 0  | 000 | 1   0  0  0  0  0  0  0  0  0 |\n| o      |       \
    \   | 001 | 1 1                           |\n|        | 0 0 1 0  | 010 | 1 1 1\
    \                         |\n| 1      |          | 011 | 1 1 1<br>\u0A1C     \
    \               |\n|        | 0 1 0 0  | 100 | \u0964 1<br>1 1 1 1           \
    \     |\n| 2      |          | 101 | 1 1 1 1 1 1 1                 |\n|      \
    \  | 0 0 0 1  | 110 | 1 1 1 1 1 1 1                 |\n| \u0D28      |       \
    \   | 111 | 1 1 1 1 1 1 1 1               |\n\nFig. 4: The waveform of the output\
    \ of Address Decoder and Data Decoder.\n\n<span id=\"page-9-1\"></span>the waveform\
    \ that corresponds to the WAV format. The operation of the WAV Decoder is governed\
    \ by the Duty input. During an MPWM period, the MPWM creates a waveform as shown\
    \ in Figure [5](#page-9-1) for a specified number of clock cycles called the \"\
    Duty\". After that, it outputs a bit with a value of 0 until the next MPWM period\
    \ starts. In order to examine the spectrum of\n\n$$\\begin{array}{c} \\mathsf{SN}\\\
    _{\\mathsf{P}\\mathsf{A}\\mathsf{T}} \\stackrel{\\mathsf{0}\\ \\mathsf{0}\\ \\\
    mathsf{T}}{\\mathsf{T}} \\stackrel{\\mathsf{0}\\ \\mathsf{0}\\ \\mathsf{T}}{\\\
    longleftarrow} \\begin{array}{c} \\mathsf{WAV} : \\mathsf{T} \\stackrel{\\mathsf{0}\\\
    \ \\mathsf{0}\\ \\mathsf{0}\\ \\mathsf{0}\\ \\mathsf{0}\\ \\mathsf{0}\\ \\mathsf{0}\\\
    \ \\mathsf{0}\\ \\mathsf{0}\\ \\mathsf{0}\\ \\mathsf{0}\\ \\mathsf{0}\\ \\mathsf{0}\\\
    \ \\mathsf{0}\\ \\mathsf{0}\\ \\mathsf{0}\\ \\mathsf{0}\\ \\mathsf{0}\\ \\mathsf{0}\\\
    \ \\mathsf{0}\\ \\mathsf{0}\\ \\mathsf{0}\\ \\mathsf{0}\\ \\mathsf{0}\\ \\mathsf{0}\\\
    \ \\mathsf{0}\\ \\mathsf{0}\\ \\mathsf{0}\\ \\mathsf{0}\\ \\mathsf{0}\\ \\mathsf{0}\\\
    \ \\mathsf{0}\\ \\mathsf{0}\\ \\mathsf{0}\\ \\mathsf{0}\\ \\mathsf{0}\\ \\mathsf{0}\\\
    \ \\mathsf{0}\\ \\mathsf{0}\\ \\mathsf{0}\\ \\mathsf{0}\\ \\mathsf{0}\\ \\mathsf{0}\\\
    \ \\mathsf{0}\\ \\mathsf{0}\\ \\mathsf{0}\\ \\mathsf{0}\\ \\mathsf{0}\\ \\mathsf{0}\\\
    end{array}$$\n\nFig. 5: MPWM waveform in the case of Duty = 19, SNpos = 2, and\
    \ Data = 4.\n\nMPWM, we will configure the counter to have a bit width of 5 and\
    \ establish a unit signal as follows:\n\n$$x\\_{unit} = \\begin{cases} \\\\ 0,\
    \ \\end{cases} \\quad \\text{or} \\quad T/32 \\le t \\le (m+1)T/32 \\\\ \\tag{2}$$\n\
    \nwhich represents the signal at t = mT /32, the duration is T /32, and the period\
    \ is T. By changing the value of m, xunit(t) can be used to form various MPWM\
    \ waveforms. According to the Fourier series formula of the periodic signal as\
    \ follows:\n\n$$x(t) = \\sum\\_{k=-\\infty}^{\\infty} a\\_k e^{-jk\\frac{2\\pi}{T}t}\
    \ dt. \\tag{3}$$\n\n$$a\\_k = \\frac{1}{T} \\int\\_T x(t)e^{-jk\\frac{2\\pi}{T}t}dt.\
    \ \\tag{4}$$\n\nthe Fourier series of xunit(t) is calculated as\n\n$$a\\_k = \\\
    begin{cases} 1/32, & k = 0\\\\ e^{-jk(\\pi/32 + m\\pi/16)} \\sin(k\\pi/32)/k\\\
    pi, & k = 1, \\ldots, 31. \\end{cases} \\tag{5}$$\n\nBy adding xunit(t) with different\
    \ values of m, we can get MPWM with the splitting number SN = 2, 4, and 8, respectively.\
    \ According to the linear properties of the Fourier series, it is easy to calculate\
    \ the Fourier series of these MPWMs. The frequency spectrum of PWM, PCM, FONS,\
    \ and MPWM are shown in Fig. [6.](#page-10-0) The harmonic energy of the MPWM\
    \ is mainly at SN/T. Fig. [8](#page-10-1) and Fig. [7](#page-10-2) illustrate\
    \ the distinctions among PWM, PCM, FONS, and harmonic components. In comparison\
    \ to typical PWM, MPWM utilizes a low-pass filter with a greater cut-off frequency,\
    \ hence reducing the demands for filter design in DAC applications. MPWM exhibits\
    \ a lower number of edges in the time domain when compared to PCM and FONS. This\
    \ characteristic can effectively decrease the integral nonlinearity of the DAC,\
    \ as discussed in Section III.\n\n<span id=\"page-10-0\"></span>![](_page_10_Figure_0.jpeg)\n\
    \n<span id=\"page-10-2\"></span>Fig. 6: The Spectrum comparison of the unfiltered\
    \ digital output.\n\n![](_page_10_Figure_2.jpeg)\n\n<span id=\"page-10-1\"></span>Fig.\
    \ 7: The waveform comparison of the unfiltered digital output.\n\n|      |   \
    \   |                | Maximum of Amplitude |              |  |\n|------|------|----------------|----------------------|--------------|--|\n\
    |      |      | Number of Edge | f, Amp,              | f2,Amp2      |  |\n| PWM\
    \  |      | 17, 0.575<br>2 |                      | 2/7,0.430    |  |\n| PCM \
    \ |      | 25 = 32        | 16/p , 0.602         | 8/7 , 0.891  |  |\n| FONS |\
    \      | 25 = 32        | 16/p , 0.602         | 12/p , 0.640 |  |\n| MPWM | SF=1\
    \ | 2\xD72 = 4        | 2,426                | 4/7 , 0.619  |  |\n|      | SF=2\
    \ | 2\xD722 = 8       | 4/7, 0.617           | 8/7, 0.619   |  |\n|      | SF=3\
    \ | 2\xD723 =16       | 8/7, 0.614           | 16/7,0.607   |  |\n\nFig. 8: Differences\
    \ among four waveforms: f<sup>1</sup> and f<sup>2</sup> are the frequencies with\
    \ the maximum and the second largest amplitude. Amplitudes have been normalized\
    \ to their DC value.\n\n## *B. Performance of MPWM-DACs*\n\n*1) Static error:*\
    \ The DC component of the output of an MPWM-DAC can be expressed as in Eq[.6,](#page-11-0)\
    \ where u<sup>d</sup> is the digital output of MPWM and D is the nominal duty\
    \ (the actual duty is D/2 <sup>n</sup>, where n is the bit width of the counter).\n\
    \n<span id=\"page-11-0\"></span>\n$$u\\_d(D) = \\frac{1}{T} \\int\\_{t\\_1}^{t\\\
    _1 + T} u\\_d(t, D) dt. \\tag{6}$$\n\nThe static error of an MPWM-DAC is defined\
    \ as in Eq[.7,](#page-11-1) where u<sup>D</sup> is the ideal DC voltage value\
    \ corresponding to the duty D. ULSB is the voltage corresponding to the DAC's\
    \ LSB.\n\n<span id=\"page-11-1\"></span>\n$$e\\_s(D) = \\frac{\\bar{u\\_d}(D)\
    \ - u\\_D}{U\\_{LSB}} \\tag{7}$$\n\nThe primary cause of static error in an MPWM-DAC\
    \ is the combination of power supply error and error resulting from edges. Given\
    \ that the maximum value of the output voltage ud(D) is equal to the power supply\
    \ voltage Us, the direct current component of the MPWM-DAC's output can have the\
    \ same level of inaccuracy as the power supply. MPWM-DAC shares the same characteristics\
    \ as PWM-DAC, PCM-DAC, and FONS-DAC in this regard.\n\nAs for errors caused by\
    \ edges, since the rising and falling edges of the MPWM output are nonideal, errors\
    \ can be brought. The nonideal characteristics of edges can be modeled using Trapezoid\
    \ model [\\[55\\]](#page-17-28) as follows:\n\n<span id=\"page-11-2\"></span>\n\
    $$e\\_{edge} = E(D)(t\\_{dr} - t\\_{df})f\\_{clk} \\tag{8}$$\n\nwhere E(D) is\
    \ the number of positive (or negative) edges for duty D, tdr is the delay from\
    \ the half amplitude of the clock signal to that of the MPWM digital output for\
    \ the rising edge, and tdf is the delay for the falling edge.\n\nFor MPWM, the\
    \ number of edges is\n\n<span id=\"page-11-3\"></span>\n$$E\\_{MPWM} = \\begin{cases}\
    \ D, & D \\leqslant SN \\\\ SN, & SN < D \\leqslant (2^n - SN) \\\\ 2^n - D, &\
    \ D > (2^n - SN) . \\end{cases} \\tag{9}$$\n\nFor PWM, the maximum number of positive\
    \ edges is 1, thus has the least edge-caused error. For PCM, and FONS, the maximum\
    \ number of edges is 2 n\u22121 . When using a counter with 12 bits, there are\
    \ 2048 edges in most PCM (or FONS) periods, which can cause severe errors.\n\n\
    For MPWM, the maximum number of edges is SN=2 SF , where the SF can be chosen\
    \ from 1 to 10 for a 12-bit counter, hence MPWM has fewer edges and thus reduces\
    \ the static error caused by edges proportionally.\n\n*2) Integral nonlinearity\
    \ (INL):* The integral nonlinearity (INL) of a DAC is the maximum deviation of\
    \ the actual analog output from the ideal output. The INL for low-cost DAC can\
    \ be calculated as in [\\[56\\]](#page-17-29):\n\n$$e\\_{INL} = \\max\\_{D} |\\\
    \ e\\_{edge}(D)\\rangle|\\tag{10}$$\n\nFor MPWM-DAC, the INL is as follows:\n\n\
    $$|e\\_{INL}(MPWM)| = |\\, 2^{n-1}(t\\_dr - t\\_df)f\\_{clk}| \\, | \\, SF = 1,\
    \ \\ldots, n-2. \\tag{11}$$\n\nFor PWM-DAC, PCM-DAC, and FONS-DAC, the INL is\
    \ as follows:\n\n$$e\\_{INL} = \\begin{cases} |(t\\_d r - t\\_d f) f\\_{clk}|\
    \ \\, | \\, \\text{PWM} \\\\ |\\, 2^{n-1} (t\\_d r - t\\_d f) f\\_{clk}| \\, |\
    \ \\, \\text{PCM} \\, \\& \\text{FONS} . \\end{cases} \\tag{12}$$\n\nSince SF\
    \ ranges from 1 to n \u2212 1, MPWM can have much better INL performance than\
    \ PCM-DAC and FONS-DAC.\n\n*3) Differential nonlinearity (DNL):* The Differential\
    \ nonlinearity(DNL) for DAC is defined as the maximum voltage deviation of the\
    \ DAC output between two adjacent digital inputs in terms of an ideal output voltage\
    \ step corresponding to 1 LSB. It is calculated as:\n\n$$\\varepsilon\\_{DNL}\
    \ = \\frac{\\bar{u\\_d}(D+1) - \\bar{u\\_d}(D)}{U\\_{LSB}} - 1. \\tag{13}$$\n\n\
    Substitude Eq[.8](#page-11-2) and Eq[.9,](#page-11-3) the DNL for MPWM is\n\n\
    $$|e\\_{DNL}(MPWM)| = |(t\\_dr - t\\_df)f\\_{clk}|\\,. \\tag{14}$$\n\nThe DNL\
    \ performance of MPWM is the same as that of PWM-DAC, PCM-DAC, and FONS-DAC. To\
    \ improve the DNL of MPWM, the output stage needs to be as symmetric as possible,\
    \ i.e. having the same characteristics for the rising and falling edges, to reduce\
    \ the | tdr \u2212 tdf | item. This symmetry is crucial when the clock frequency\
    \ is high. Due to its operating principle, MPWM-DAC is monotonic.\n\n<span id=\"\
    page-12-0\"></span>![](_page_12_Figure_0.jpeg)\n\nFig. 9: Normalized cutoff frequency\
    \ vs. DAC resolution, using second order Butterworth low pass filter.\n\n<span\
    \ id=\"page-12-1\"></span>*4) Dynamic Characteristics:* For dynamic characteristics\
    \ analysis, getting a general analytical computation for the output from MPWM-DAC\
    \ is quite difficult, therefore, we use numerical computation. The ripple of the\
    \ output of an MPWM-DAC with second order Butterworth low pass filter is calculated\
    \ to get normalized cutoff frequency fcT of the filter for different SF in Fig.\
    \ [9.](#page-12-0) The fcT increases with the increase of SF. For instance, when\
    \ designing a 12-bit DAC, fcT is 0.01 for PWM-DAC, and for MPWM-DAC it increases\
    \ to 0.04 and 0.1 when SF equals 3 and 7 respectively. Thus the cutoff frequency\
    \ is 4 and 10 times that of PWM-DAC, and the settling time is 1/4 and 1/10 of\
    \ that of PWM-DAC.\n\n## V. FPGA DESIGN AND RISC-V SOC\n\n![](_page_12_Figure_4.jpeg)\n\
    \nFig. 10: MPWM Circuit.\n\n# *A. MPWM circuit design*\n\nThe block diagram for\
    \ an MPWM circuit is shown in Fig[.10,](#page-12-1) where the bit width of the\
    \ counter is n. For an MPWM wave with SF, the output of the counter is rearranged\
    \ according to CR={Cn\u2212SF <sup>\u2212</sup>1, Cn\u2212SF <sup>\u2212</sup>2,\
    \ \xB7 \xB7 \xB7 , C0, Cn\u2212SF , \xB7 \xB7 \xB7 , Cn\u22122, Cn\u22121}, where\
    \ C<sup>0</sup> to Cn\u2212<sup>1</sup> are the output of the counter, and C<sup>R</sup>\
    \ is the result of rearrangement. The duty target is input from dutyinput, which\
    \ ranges from 0 to 2 <sup>n</sup>\u2212<sup>1</sup> \u2212 1. Then dutyinput is\
    \ compared with CR, and MPWM outputs according to Eq. [15.](#page-12-2)\n\n<span\
    \ id=\"page-12-2\"></span>\n$$MPWM\\\\_OUT = \\begin{cases} 1, \\text{ } duty\\\
    \\_input \\ge C\\_r\\\\ 0, \\text{ } duty\\\\_input < C\\_r. \\end{cases} \\tag{15}$$\n\
    \n## *B. HR-MPWM design in FPGA*\n\nTo increase the DAC's resolution, one can\
    \ increase the bit width of the counter, but the clock rate needs to be raised\
    \ simultaneously, or the DAC's speed will decrease proportionally. In many cases,\
    \ it is very difficult to raise the clock rate due to the process limit or power\
    \ consumption. To solve this problem, the circuit to generate a pulse with a resolution\
    \ better than one clock cycle can be designed. An MPWM with this kind of circuit\
    \ is named high-resolution MPWM, i.e. HR-MPWM. When implemented in Xilinx FPGA,\
    \ the HR-MPWM circuit is shown in Fig. [11,](#page-13-0) where IODELAY E1 is the\
    \ I/O delay element and can allow a fine delay-time t<sup>d</sup> = 1/(32 \xD7\
    \ 2 \xD7 fclkref ). Thus, this circuit can provide an additional 6-bit resolution\
    \ for MPWM. Considering the HR-MPWM in the final SoC needs only 4 additional bits,\
    \ the design in FPGA uses a delay tap out of every 4 taps.\n\n<span id=\"page-13-0\"\
    ></span>![](_page_13_Figure_0.jpeg)\n\nFig. 11: Block diagram of HR-MPWM in FPGA.\n\
    \n# *C. SoC Structure with the proposed DAC*\n\nThe MPWM-DAC is integrated into\
    \ a RISC-V SoC for IoT applications. Its position within the SoC level range is\
    \ shown in Fig. [15,](#page-15-0) where it is located alongside its associated\
    \ auxiliary modules within the \"MPWM-DAC\".\n\nControl over the entire system\
    \ is exerted by an open-source 32-bit RISC-V core. For the support of complex\
    \ vector operations, commonly required in AI acceleration, a specialized parallel\
    \ computing core is strategically deployed. Configuration of the proposed MPWM-DAC\
    \ module is undertaken by the RISC-V core, facilitated through a high-speed bus\
    \ linking them. The responsibility for managing massive data transportation falls\
    \ to DMAs. Wireless communication capabilities are introduced via an off-chip\
    \ Wi-Fi chip, which is connected to a UART port. The system's memory requirements\
    \ are addressed with on-chip 128KB instruction SRAM and an equal capacity for\
    \ data SRAM. Additionally, the External Memory Interface is employed to access\
    \ further memory, with the capacity to address nearly the entirety of the remaining\
    \ 4GB.\n\n## <span id=\"page-13-1\"></span>*D. Test in FPGA*\n\n![](_page_13_Figure_6.jpeg)\n\
    \nFig. 12: DLL design in the RISC-V SoC.\n\nThe prototyping SoC with MPWM-DAC\
    \ is implemented in an Xilinx ARTIX-7 FPGA (XC7A200TFBG484-3), as shown in Fig.\
    \ [16.](#page-15-1) The second-order Butterworth filter is connected to the output\
    \ of the DAC. The INL and DNL for an MPWM-DAC with 10-bit resolution are measured,\
    \ and compared with PWM-DAC and PCM-DAC as presented in Fig. [13.](#page-14-2)\n\
    \nThese results show that MPWM-DAC has improved the INL performance greatly when\
    \ compared to PCM-DAC, and has a shorter settling time than PWM-DAC in Fig. [14.](#page-14-3)\
    \ When designing an MPWM-DAC, one can get a higher accurate DAC by choosing a\
    \ smaller SF or get a faster DAC by selecting a larger SF.\n\n## *E. RISC-V SOC\
    \ using MPWM-DAC for THz IoT devices*\n\nIn the SoC, the HRMPWM module is a fully\
    \ customized design, which comprises a typical PWM generator and delay elements\
    \ calibrated by a DLL. The DLL is applied to get 16 fine phases, refer to Fig.\
    \ [12.](#page-13-1) After the DLL is locked, vc is generated and applied to the\
    \ signal path for MPWM, which has the same delay element as in DLL. A multiplexer\
    \ circuit with the same delay in every branch is employed to get the required\
    \ phase. The laboratory experiment for the FPGA test is\n\n<span id=\"page-14-3\"\
    ></span><span id=\"page-14-2\"></span>![](_page_14_Figure_0.jpeg)\n\nFig. 14:\
    \ Setting time performance for MPWM.\n\nillustrated in Fig. [16](#page-15-1) and\
    \ the layout of the MPWM SoC is shown in Fig. [17.](#page-16-23) It is fabricated\
    \ (taped out in March 2021) in a 180nm CMOS process with a size of 3.8mm \xD7\
    \ 4.7mm. The size of HRMPWM is 0.53mm \xD7 0.27mm.\n\n## VI. CONCLUSION\n\nTHz\
    \ communications are expected to have a crucial impact on the development of wireless\
    \ systems in the sixth generation (6G). This study provides a comprehensive analysis\
    \ of THz IoT devices. In addition, the MPWM modulation mechanism, introduced in\
    \ an MPWM circuit with a n-bit counter, may exhibit (n\u22121) distinct configurations\
    \ and waveforms. The MPWM-DAC is developed using the MPWM technology, offering\
    \ superior conversion speed compared to PWM-DAC, and higher accuracy than the\
    \ PCM-DAC. By virtue of its versatility, the designer is able to modify their\
    \ design to suit unique applications, resulting in a superior outcome. An implementation\
    \ using a delay line has been developed in order to enhance the resolution of\
    \ MPWM-DAC. The MPWM-DAC is included in a RISC-V SoC and evaluated on an FPGA.\
    \ The SoC is specifically engineered using a 180nm CMOS fabrication technique.\n\
    \n## REFERENCES\n\n- <span id=\"page-14-0\"></span>[1] Y. Xu, Z. Liu, C. Huang,\
    \ and C. Yuen, \"Robust resource allocation algorithm for energy-harvesting-based\
    \ d2d communication underlaying uav-assisted networks,\" *IEEE Internet of Things\
    \ Journal*, vol. 8, no. 23, pp. 17 161\u201317 171, 2021.\n- <span id=\"page-14-1\"\
    ></span>[2] H. Jang, K. Han, S. Lee, J.-J. Lee, S.-Y. Lee, J.-H. Lee, and W. Lee,\
    \ \"Developing a multicore platform utilizing open risc-v cores,\" *IEEE Access*,\
    \ vol. 9, pp. 120 010\u2013120 023, 2021.\n\n<span id=\"page-15-0\"></span>![](_page_15_Figure_0.jpeg)\n\
    \n<span id=\"page-15-1\"></span>Fig. 15: A brief design architecture of the RISC-V\
    \ SoC for THz IoT devices.\n\n![](_page_15_Picture_2.jpeg)\n\nFig. 16: Experimental\
    \ setup with FPGA.\n\n**ESP8266 Wi-Fi Module**\n\n<span id=\"page-16-23\"></span>![](_page_16_Figure_0.jpeg)\n\
    \nCPU1 Fig. 17: Layout of the RISC-V SoC.\n\n- <span id=\"page-16-0\"></span>[3]\
    \ H. B. Amor, C. Bernier, and Z. P\u02C7rikryl, \"A risc-v isa extension for ultra-low\
    \ power iot wireless signal processing,\" *IEEE Transactions on Computers*, vol.\
    \ 71, no. 4, pp. 766\u2013778, 2022.\n- <span id=\"page-16-1\"></span>[4] T. Kurner\
    \ and S. Priebe, \"Towards thz communications - status in research, standardization\
    \ and regulation,\" \xA8 *Journal of Infrared, Millimeter, and Terahertz Waves*,\
    \ vol. 35, pp. 53\u201362, 2014. [Online]. Available:<https://api.semanticscholar.org/CorpusID:28353094>\n\
    - <span id=\"page-16-2\"></span>[5] H. Sarieddeen, M.-S. Alouini, and T. Y. Al-Naffouri,\
    \ \"Terahertz-band ultra-massive spatial modulation mimo,\" *IEEE Journal on Selected\
    \ Areas in Communications*, vol. 37, no. 9, pp. 2040\u20132052, 2019.\n- <span\
    \ id=\"page-16-3\"></span>[6] P. Heydari, \"Terahertz integrated circuits and\
    \ systems for high-speed wireless communications: Challenges and design perspectives,\"\
    \ *IEEE Open Journal of the Solid-State Circuits Society*, vol. 1, pp. 18\u2013\
    36, 2021.\n- <span id=\"page-16-4\"></span>[7] M. Alibakhshikenari, E. M. Ali,\
    \ M. Soruri, M. Dalarsson, M. Naser-Moghadasi, B. S. Virdee, C. Stefanovic, A.\
    \ Pietrenko-Dabrowska, S. Koziel, S. Szczepanski, and E. Limiti, \"A comprehensive\
    \ survey on antennas on-chip based on metamaterial, metasurface, and substrate\
    \ integrated waveguide principles for millimeter-waves and terahertz integrated\
    \ circuits and systems,\" *IEEE Access*, vol. 10, pp. 3668\u20133692, 2022.\n\
    - <span id=\"page-16-5\"></span>[8] H. Jalili and O. Momeni, \"A 0.34-thz wideband\
    \ wide-angle 2-d steering phased array in 0.13- \xB5 m sige bicmos,\" *IEEE Journal\
    \ of Solid-State Circuits*, vol. 54, no. 9, pp. 2449\u20132461, 2019.\n- <span\
    \ id=\"page-16-6\"></span>[9] M. Alibakhshikenari, B. S. Virdee, C. H. See, R.\
    \ A. Abd-Alhameed, F. Falcone, and E. Limiti, \"High-performance 50\xB5m silicon-based\
    \ on-chip antenna with high port-to-port isolation implemented by metamaterial\
    \ and siw concepts for thz integrated systems,\" in *2019 Thirteenth International\
    \ Congress on Artificial Materials for Novel Wave Phenomena (Metamaterials)*,\
    \ 2019, pp. X\u2013023\u2013X\u2013025.\n- <span id=\"page-16-7\"></span>[10]\
    \ X. Yi, C. Wang, X. Chen, J. Wang, J. Grajal, and R. Han, \"A 220-to-320-ghz\
    \ fmcw radar in 65-nm cmos using a frequency-comb architecture,\" *IEEE Journal\
    \ of Solid-State Circuits*, vol. 56, no. 2, pp. 327\u2013339, 2021.\n- <span id=\"\
    page-16-8\"></span>[11] H. Jalili and O. Momeni, \"A 0.46-thz 25-element scalable\
    \ and wideband radiator array with optimized lens integration in 65-nm cmos,\"\
    \ *IEEE Journal of Solid-State Circuits*, vol. 55, no. 9, pp. 2387\u20132400,\
    \ 2020.\n- <span id=\"page-16-9\"></span>[12] C.-H. Li and T.-Y. Chiu, \"340-ghz\
    \ low-cost and high-gain on-chip higher order mode dielectric resonator antenna\
    \ for thz applications,\" *IEEE Transactions on Terahertz Science and Technology*,\
    \ vol. 7, no. 3, pp. 284\u2013294, 2017.\n- <span id=\"page-16-10\"></span>[13]\
    \ K. Schmalz, R. Wang, J. Borngraber, W. Debski, W. Winkler, and C. Meliani, \"\
    245 ghz sige transmitter with integrated antenna and external pll,\" in \xA8 *2013\
    \ IEEE MTT-S International Microwave Symposium Digest (MTT)*, 2013, pp. 1\u2013\
    3.\n- <span id=\"page-16-11\"></span>[14] W. T. Khan, A. C\xB8 agr\u0131 Ulusoy,\
    \ G. Dufour, M. Kaynak, B. Tillack, J. D. Cressler, and J. Papapolymerou, \"A\
    \ d-band micromachined end-fire antenna in \u02D8 130-nm sige bicmos technology,\"\
    \ *IEEE Transactions on Antennas and Propagation*, vol. 63, no. 6, pp. 2449\u2013\
    2459, 2015.\n- <span id=\"page-16-12\"></span>[15] I. Sarkas, J. Hasch, A. Balteanu,\
    \ and S. P. Voinigescu, \"A fundamental frequency 120-ghz sige bicmos distance\
    \ sensor with integrated antenna,\" *IEEE Transactions on Microwave Theory and\
    \ Techniques*, vol. 60, no. 3, pp. 795\u2013812, 2012.\n- <span id=\"page-16-13\"\
    ></span>[16] Y. Tousi and E. Afshari, \"A high-power and scalable 2-d phased array\
    \ for terahertz cmos integrated systems,\" *IEEE Journal of Solid-State Circuits*,\
    \ vol. 50, no. 2, pp. 597\u2013609, 2015.\n- <span id=\"page-16-14\"></span>[17]\
    \ M. Alibakhshikenari, B. S. Virdee, C. H. See, R. Abd-Alhameed, F. Falcone, and\
    \ E. Limiti, \"A novel 0.3-0.31 thz gaas-based transceiver with on-chip slotted\
    \ metamaterial antenna based on siw technology,\" in *2019 IEEE Asia-Pacific Microwave\
    \ Conference (APMC)*, 2019, pp. 69\u201371.\n- <span id=\"page-16-15\"></span>[18]\
    \ M. H. Maktoomi, S. Saadat, O. Momeni, P. Heydari, and H. Aghasi, \"Broadband\
    \ antenna design for terahertz communication systems,\" *IEEE Access*, vol. 11,\
    \ pp. 20 897\u201320 911, 2023.\n- <span id=\"page-16-16\"></span>[19] H. J. Ng\
    \ and D. Kissinger, \"Highly miniaturized 120-ghz simo and mimo radar sensor with\
    \ on-chip folded dipole antennas for range and angular measurements,\" *IEEE Transactions\
    \ on Microwave Theory and Techniques*, vol. 66, no. 6, pp. 2592\u20132603, 2018.\n\
    - <span id=\"page-16-17\"></span>[20] J. M. Edwards and G. M. Rebeiz, \"High-efficiency\
    \ elliptical slot antennas with quartz superstrates for silicon rfics,\" *IEEE\
    \ Transactions on Antennas and Propagation*, vol. 60, no. 11, pp. 5010\u20135020,\
    \ 2012.\n- <span id=\"page-16-18\"></span>[21] M. Saad Khan, F. A. Tahir, A. Meredov,\
    \ A. Shamim, and H. M. Cheema, \"A w-band ebg-backed double-rhomboid bowtie-slot\
    \ on-chip antenna,\" *IEEE Antennas and Wireless Propagation Letters*, vol. 18,\
    \ no. 5, pp. 1046\u20131050, 2019.\n- <span id=\"page-16-19\"></span>[22] M. Frank,\
    \ F. Lurz, R. Weigel, and A. Koelpin, \"122 ghz low-cost substrate integrated\
    \ waveguide based leaky-wave antenna design,\" in *2019 IEEE Radio and Wireless\
    \ Symposium (RWS)*, 2019, pp. 1\u20134.\n- <span id=\"page-16-20\"></span>[23]\
    \ H. H. Bae, T. H. Jang, H. Y. Kim, and C. S. Park, \"Broadband 120 ghz l-probe\
    \ differential feed dual-polarized patch antenna with soft surface,\" *IEEE Transactions\
    \ on Antennas and Propagation*, vol. 69, no. 10, pp. 6185\u20136195, 2021.\n-\
    \ <span id=\"page-16-21\"></span>[24] A. Simsek, A. S. H. Ahmed, A. A. Farid,\
    \ U. Soylu, and M. J. W. Rodwell, \"A 140ghz two-channel cmos transmitter using\
    \ low-cost packaging technologies,\" in *2020 IEEE Wireless Communications and\
    \ Networking Conference Workshops (WCNCW)*, 2020, pp. 1\u20133.\n- <span id=\"\
    page-16-22\"></span>[25] A. Simsek, S.-K. Kim, M. Abdelghany, A. S. H. Ahmed,\
    \ A. A. Farid, U. Madhow, and M. J. W. Rodwell, \"A 146.7 ghz transceiver with\
    \ 5 gbaud data transmission using a low-cost series-fed patch antenna array through\
    \ wirebonding integration,\" in *2020 IEEE Radio and Wireless Symposium (RWS)*,\
    \ 2020, pp. 68\u201371.\n- <span id=\"page-17-0\"></span>[26] R. Han and E. Afshari,\
    \ \"A cmos high-power broadband 260-ghz radiator array for spectroscopy,\" *IEEE\
    \ Journal of Solid-State Circuits*, vol. 48, no. 12, pp. 3090\u20133104, 2013.\n\
    - <span id=\"page-17-1\"></span>[27] Y. Yang, O. D. Gurbuz, and G. M. Rebeiz,\
    \ \"An eight-element 370\u2013410-ghz phased-array transmitter in 45-nm cmos soi\
    \ with peak eirp of 8\u20138.5 dbm,\" *IEEE Transactions on Microwave Theory and\
    \ Techniques*, vol. 64, no. 12, pp. 4241\u20134249, 2016.\n- <span id=\"page-17-2\"\
    ></span>[28] H. Saeidi, S. Venkatesh, C. R. Chappidi, T. Sharma, C. Zhu, and K.\
    \ Sengupta, \"29.9 a 4\xD74 distributed multi-layer oscillator network for harmonic\
    \ injection and thz beamforming with 14dbm eirp at 416ghz in a lensless 65nm cmos\
    \ ic,\" in *2020 IEEE International Solid-State Circuits Conference - (ISSCC)*,\
    \ 2020, pp. 256\u2013258.\n- <span id=\"page-17-3\"></span>[29] P. Herrero and\
    \ J. Schoebel, \"Microstrip patch array antenna technology for 122 ghz ism sensing\
    \ applications,\" in *2009 German Microwave Conference*, 2009, pp. 1\u20134.\n\
    - <span id=\"page-17-4\"></span>[30] S. Shahramian, M. Holyoak, A. Singh, B. J.\
    \ Farahani, and Y. Baeyens, \"A fully integrated scalable w-band phased-array\
    \ module with integrated antennas, self-alignment and self-test,\" in *2018 IEEE\
    \ International Solid-State Circuits Conference-(ISSCC)*. IEEE, 2018, pp. 74\u2013\
    76.\n- <span id=\"page-17-5\"></span>[31] E. Naviasky, L. Iotti, G. LaCaille,\
    \ B. Nikolic, E. Alon, and A. M. Niknejad, \"A 71-to-86-ghz 16-element by 16-beam\
    \ multi-user beamforming integrated \xB4 receiver sub-array for massive mimo,\"\
    \ *IEEE Journal of Solid-State Circuits*, vol. 56, no. 12, pp. 3811\u20133826,\
    \ 2021.\n- <span id=\"page-17-6\"></span>[32] M. Alibakhshikenari, B. S. Virdee,\
    \ C. H. See, R. A. Abd-Alhameed, F. Falcone, and E. Limiti, \"Silicon-based 0.450-0.475\
    \ thz series-fed double dielectric resonator on-chip antenna array based on metamaterial\
    \ properties for integrated-circuits,\" in *2019 Thirteenth International Congress\
    \ on Artificial Materials for Novel Wave Phenomena (Metamaterials)*, 2019, pp.\
    \ X\u2013026\u2013X\u2013028.\n- <span id=\"page-17-7\"></span>[33] M. Alibakhshikenari,\
    \ B. S. Virdee, C. H. See, R. A. Abd-Alhameed, and E. Limiti, \"High performance\
    \ on-chip array antenna based on metasurface feeding structure for terahertz integrated\
    \ circuits,\" in *2019 44th International Conference on Infrared, Millimeter,\
    \ and Terahertz Waves (IRMMW-THz)*, 2019, pp. 1\u20132.\n- <span id=\"page-17-8\"\
    ></span>[34] R. Beica, \"Flip chip market and technology trends,\" in *2013 Eurpoean\
    \ Microelectronics Packaging Conference (EMPC)*. IEEE, 2013, pp. 1\u20134.\n-\
    \ <span id=\"page-17-9\"></span>[35] W. Shin, B.-H. Ku, O. Inac, Y.-C. Ou, and\
    \ G. M. Rebeiz, \"A 108\u2013114 ghz 4 \xD7 4 wafer-scale phased array transmitter\
    \ with high-efficiency on-chip antennas,\" *IEEE Journal of Solid-State Circuits*,\
    \ vol. 48, no. 9, pp. 2041\u20132055, 2013.\n- <span id=\"page-17-10\"></span>[36]\
    \ B. Yang, Z. Yu, J. Lan, R. Zhang, J. Zhou, and W. Hong, \"Digital beamforming-based\
    \ massive mimo transceiver for 5g millimeter-wave communications,\" *IEEE Transactions\
    \ on Microwave Theory and Techniques*, vol. 66, no. 7, pp. 3403\u20133418, 2018.\n\
    - <span id=\"page-17-11\"></span>[37] S. Mondal, R. Singh, A. I. Hussein, and\
    \ J. Paramesh, \"A 25\u201330 ghz fully-connected hybrid beamforming receiver\
    \ for mimo communication,\" *IEEE Journal of Solid-State Circuits*, vol. 53, no.\
    \ 5, pp. 1275\u20131287, 2018.\n- <span id=\"page-17-12\"></span>[38] A. Natarajan,\
    \ A. Komijani, X. Guan, A. Babakhani, and A. Hajimiri, \"A 77-ghz phased-array\
    \ transceiver with on-chip antennas in silicon: Transmitter and local lo-path\
    \ phase shifting,\" *IEEE Journal of Solid-State Circuits*, vol. 41, no. 12, pp.\
    \ 2807\u20132819, 2006.\n- <span id=\"page-17-13\"></span>[39] B. Sadhu, Y. Tousi,\
    \ J. Hallin, S. Sahl, S. K. Reynolds, O. Renstrom, K. Sjogren, O. Haapalahti,\
    \ N. Mazor, B. Bokinge, G. Weibull, H. Bengtsson, A. Carlinger, E. Westesson,\
    \ J.-E. Thillberg, L. Rexberg, M. Yeck, X. Gu, M. Ferriss, D. Liu, D. Friedman,\
    \ and A. Valdes-Garcia, \"A 28-ghz 32-element trx phased-array ic with concurrent\
    \ dual-polarized operation and orthogonal phase and gain control for 5g communications,\"\
    \ *IEEE Journal of Solid-State Circuits*, vol. 52, no. 12, pp. 3373\u20133391,\
    \ 2017.\n- <span id=\"page-17-14\"></span>[40] T. T. Cai and L. Wang, \"Orthogonal\
    \ matching pursuit for sparse signal recovery with noise,\" *IEEE Transactions\
    \ on Information Theory*, vol. 57, no. 7, pp. 4680\u20134688, 2011.\n- <span id=\"\
    page-17-15\"></span>[41] X. Yu, J.-C. Shen, J. Zhang, and K. B. Letaief, \"Alternating\
    \ minimization algorithms for hybrid precoding in millimeter wave mimo systems,\"\
    \ *IEEE Journal of Selected Topics in Signal Processing*, vol. 10, no. 3, pp.\
    \ 485\u2013500, 2016.\n- <span id=\"page-17-16\"></span>[42] Y.-Y. Lee, C.-H.\
    \ Wang, and Y.-H. Huang, \"A hybrid rf/baseband precoding processor based on parallel-index-selection\
    \ matrix-inversion-bypass simultaneous orthogonal matching pursuit for millimeter\
    \ wave mimo systems,\" *IEEE Transactions on Signal Processing*, vol. 63, no.\
    \ 2, pp. 305\u2013317, 2015.\n- <span id=\"page-17-17\"></span>[43] C.-K. Ho,\
    \ H.-Y. Cheng, and Y.-H. Huang, \"Hybrid precoding processor for millimeter wave\
    \ mimo communications,\" *IEEE Transactions on Circuits and Systems II: Express\
    \ Briefs*, vol. 66, no. 12, pp. 1992\u20131996, 2019.\n- <span id=\"page-17-18\"\
    ></span>[44] W.-L. Hung, C.-H. Chen, C.-C. Liao, C.-R. Tsai, and A.-Y. A. Wu,\
    \ \"Low-complexity hybrid precoding algorithm based on orthogonal beamforming\
    \ codebook,\" in *2015 IEEE Workshop on Signal Processing Systems (SiPS)*, 2015,\
    \ pp. 1\u20135.\n- <span id=\"page-17-19\"></span>[45] K.-T. Chen, Y.-T. Hwang,\
    \ and Y.-C. Liao, \"Vlsi design of a high throughput hybrid precoding processor\
    \ for wireless mimo systems,\" *IEEE Access*, vol. 7, pp. 85 925\u201385 936,\
    \ 2019.\n- <span id=\"page-17-20\"></span>[46] J. Brady, N. Behdad, and A. M.\
    \ Sayeed, \"Beamspace mimo for millimeter-wave communications: System architecture,\
    \ modeling, analysis, and measurements,\" *IEEE Transactions on Antennas and Propagation*,\
    \ vol. 61, no. 7, pp. 3814\u20133827, 2013.\n- <span id=\"page-17-21\"></span>[47]\
    \ C.-H. Chen, C.-R. Tsai, Y.-H. Liu, W.-L. Hung, and A.-Y. Wu, \"Compressive sensing\
    \ (cs) assisted low-complexity beamspace hybrid precoding for millimeter-wave\
    \ mimo systems,\" *IEEE Transactions on Signal Processing*, vol. 65, no. 6, pp.\
    \ 1412\u20131424, 2017.\n- <span id=\"page-17-22\"></span>[48] E. Gon\xA8 ultas\xB8\
    , S. Taner, A. Gallyas-Sanhueza, S. H. Mirfarshbafan, and C. Studer, \"Hardware-aware\
    \ beamspace precoding for all-digital mmwave massive \xA8 mu-mimo,\" *IEEE Communications\
    \ Letters*, vol. 25, no. 11, pp. 3709\u20133713, 2021.\n- <span id=\"page-17-23\"\
    ></span>[49] D. F. Hoeschele, \"Analog-to-digital and digital-to-analog conversion\
    \ techniques,\" pp. 397\u2013397, 1994.\n- <span id=\"page-17-24\"></span>[50]\
    \ D. S. The Engineering Staff of Analog Devices, \"Analog-digital conversion handbook,\"\
    \ 1986.\n- <span id=\"page-17-25\"></span>[51] I. Corp., \"Using a pwm as a digital-to-analog\
    \ converter,\" *AN035701-0915*.\n- [52] F. Zhou and W. Xiong, \"Using pwm output\
    \ as a digital-to-analog converter on dsp,\" in *2010 International Conference\
    \ on System Science, Engineering Design and Manufacturing Informatization*, vol.\
    \ 2. IEEE, 2010, pp. 278\u2013281.\n- <span id=\"page-17-26\"></span>[53] T. Instruments,\
    \ \"Using pwm output as a digital-to-analog converter on a tms320c240 dsp,\" *APPLICATION\
    \ REPORT: SPRA490*, November 1998.\n- <span id=\"page-17-27\"></span>[54] e. a.\
    \ Halper, \"Digital-to-analog conversion by pulse-count modulation methods,\"\
    \ *IEEE Transactions on Instrumentation and Measurement*, 1996.\n- <span id=\"\
    page-17-28\"></span>[55] B.-B. Corp., *Integrated Circuits Data Book Holland*,\
    \ 1986.\n- <span id=\"page-17-29\"></span>[56] H. Zander, *Datenwundler: AD/LlA-Wandler;\
    \ Schnittstellen der digitalen Signulverarbeitung*. Frankfurter Fachverlag, 1985."
  paper_id: 6
  scores:
    originality:
      raw: 4.5
      reason: The paper presents a novel approach to matrix multiplication that supports
        relaxed structured sparsity, which is a significant advancement in the field.
    technical_soundness:
      raw: 4.0
      reason: The methods proposed are well-founded and demonstrate a solid understanding
        of the underlying principles, though some experimental validation may be lacking.
    clarity:
      raw: 3.8
      reason: While the paper is generally well-written, certain sections could benefit
        from clearer explanations and more detailed illustrations of complex concepts.
    significance:
      raw: 4.2
      reason: The work addresses a relevant challenge in deep learning model efficiency,
        making it a valuable contribution to both academia and industry.
    relevance:
      raw: 4.5
      reason: The topic is highly relevant to current trends in deep learning and
        hardware acceleration, appealing to a wide audience in the field.
  total_score: 4.15
  tier: B
- title: "ACS: Concurrent Kernel Execution on Irregular, Input-Dependent\n  Computational\
    \ Graphs"
  abstract: 'Spin Transfer Torque Random Access Memory (STT-RAM) is an emerging

    Non-Volatile Memory (NVM) technology that has garnered attention to overcome

    the drawbacks of conventional CMOS-based technologies. However, such

    technologies must be evaluated before deployment under real workloads and

    architecture. But there is a lack of available open-source STT-RAM-based system

    evaluation framework, which hampers research and experimentation and impacts

    the adoption of STT- RAM in a system. This paper proposes a novel, extendable

    STT-RAM memory controller design integrated inside the gem5 simulator. Our

    framework enables understanding various aspects of STT-RAM, i.e., power, delay,

    clock cycles, energy, and system throughput. We will open-source our HOPE

    framework, which will fuel research and aid in accelerating the development of

    future system architectures based on STT-RAM. It will also facilitate the user

    for further tool enhancement.'
  url: http://arxiv.org/abs/2401.14888v1
  keywords: '* Non-volatile memory, STT-RAM, Power Estimation, gem5, Emerging Technologies'
  document: "Date of publication xxxx 00, 0000, date of current version xxxx 00, 0000.\
    \ *Digital Object Identifier 10.1109/ACCESS.XXXX.DOI*\n\n# **HOPE: Holistic STT-RAM\
    \ Architecture Exploration Framework for Future Cross-Platform Analysis**\n\n\
    **SAEED SEYEDFARAJI (Graduate Student Member, IEEE), MARKUS BICHL, ASAD AFTAB\
    \ (Graduate Student Member, IEEE),and SEMEEN REHMAN(Member, IEEE).<sup>1</sup>\
    \ ,**\n\n<sup>1</sup>Faculty of Electrical Engineering and Information Technology,\
    \ Vienna University of Technology (TU-Wien), 1040 Vienna, Austria Corresponding\
    \ author: Saeed Seyedfaraji (e-mail: saeed.seyedfaraji@tuwien.ac.at).\n\n**ABSTRACT**\
    \ Spin Transfer Torque Random Access Memory (STT-RAM) is an emerging Non-Volatile\
    \ Memory (NVM) technology that has garnered attention to overcome the drawbacks\
    \ of conventional CMOS-based technologies. However, such technologies must be\
    \ evaluated before deployment under real workloads and architecture. But there\
    \ is a lack of available open-source STT-RAM-based system evaluation framework,\
    \ which hampers research and experimentation and impacts the adoption of STT-RAM\
    \ in a system. This paper proposes a novel, extendable STT-RAM memory controller\
    \ design integrated inside the gem5 simulator. Our framework enables understanding\
    \ various aspects of STT-RAM, i.e., power, delay, clock cycles, energy, and system\
    \ throughput. We will open-source our HOPE framework, which will fuel research\
    \ and aid in accelerating the development of future system architectures based\
    \ on STT-RAM. It will also facilitate the user for further tool enhancement.\n\
    \n**INDEX TERMS** Non-volatile memory, STT-RAM, Power Estimation, gem5, Emerging\
    \ Technologies\n\n## **I. INTRODUCTION**\n\nSTT-RAM boastsseveral compelling featuresincluding\
    \ nonvolatility, high density, soft error reliability, CMOS compatibility, high\
    \ endurance, and scalability [1]\u2013[4]. According to the International Roadmap\
    \ for Devices and Systems (IRDS) [5], STT-RAM emerges as the most promising memory\
    \ option to replace conventional memory technologies. Table 1 presents a comprehensive\
    \ comparison of various design elements related to memory technologies, such as\
    \ endurance, associated read/write energy, latency, and compatibility with CMOS\
    \ technology size. It is worth noting that the listed suppliers are not exclusive\
    \ options, as alternative providers may also offer each type of memory technology.\
    \ Considering the decreasing technology node size over time due to Denard's law,\
    \ it is important to note that the suggested compatibility in the table is based\
    \ on the findings from the literature review of [1]\u2013[4], [6]\u2013[18]. Therefore,\
    \ it is plausible that each memory technology could potentially be fabricated\
    \ with other CMOS sizes.\n\nComparatively, the write and read energy of Dynamic\
    \ Random Access Memory (DRAM) Static Random Access Memory (SRAM) per operation\
    \ is lower than that of NVMs. Nevertheless, both DRAM and SRAM, as volatile memory\
    \ types, rely on a continuous power supply to retain stored\n\ndata. DRAM, commonly\
    \ employed as the main memory in computers, consumes more energy due to its constant\
    \ need for refreshing to preserve data integrity. Similarly, SRAM, another volatile\
    \ memory technology, also requires a consistent power supply for data retention.\
    \ On the other hand, NVMs like Flash memory can maintain data even when power\
    \ is off, resulting in lower energy consumption compared to DRAM and SRAM. Therefore,\
    \ NVM became an attractive alternative for main memories because of lower energy\
    \ consumption. Phase Change Memory (PCM), and STT-RAM [19] are some of the common\
    \ NVMs proposed to replace DRAM as main memory. They have also been explored for\
    \ building larger on-chip caches because of their high density. Amongst all, STT-RAM\
    \ has matured a lot in terms of its on-chip computation, and high energy efficiency\
    \ [20].\n\nHowever, the utilization of STT-RAM in widespread industrial applications\
    \ is hindered by several limitations. Notably, challenges such as write operation\
    \ delays and high write energy consumption. To overcome these issues, different\
    \ approaches have been explored at various levels of abstraction, including circuit-level\
    \ approaches [21]\u2013[25], architecture-level approaches [10], [26], [27], and\
    \ methods proposed from an application perspective [28], [29].\n\n![](_page_1_Picture_0.jpeg)\n\
    \n|                         | STT-RAM          | PCMRAM                  | RRAM\
    \              | Fe-FET               | FLASH           | SRAM            | DRAM\
    \              |\n|-------------------------|------------------|-------------------------|-------------------|----------------------|-----------------|-----------------|-------------------|\n\
    | Non-volatility          | +                | +                       | +   \
    \              | +                    | +               | -               | -\
    \                 |\n| Data Retention (years)  | 10               | 10       \
    \               | 10                | 10                   | 10              |\
    \ -               | -                 |\n| Cell Endurance (cycles) | 1016    \
    \         | 1012                    | 1010              | 1012               \
    \  | 106             | 1016            | 1015              |\n| Cell Size (F 2)\
    \         | 6-20             | 4-8                     | 4                 | 4-8\
    \                  | 4-6             | >100            | 4-12              |\n\
    | Technology node (nm)    | 45               | 65                      | 40  \
    \              | 5                    | 15              | 10              | 32\
    \                |\n| Read Latency (ns)       | 2-20             | 20-50     \
    \              | <10               | 10                   | 25x103          |\
    \ <5              | 2                 |\n| Write Latency (ns)      | 5-20    \
    \         | 30                      | 5                 | 10                 \
    \  | 500x103         | <5              | 5                 |\n| Erase Latency\
    \ (ns)      | 5-20             | 30                      | 10                |\
    \ 10                   | 2 (ms)          | <5              | 5               \
    \  |\n| Write Energy (pJ)       | 0.1-2.5          | 18                      |\
    \ 0.1               | 1                    | 0.1 - 1         | <0.1          \
    \  | <0.1              |\n| Erase Energy (pJ)       | 1                | 18  \
    \                    | 0.1               | 1                    | 1000       \
    \     | <1              | <1                |\n| Suppliers               | Toshiba,\
    \ Hitachi | Samsung, Intel, WD, IBM | Panasonic, Micron | Globalfoundries, FMC\
    \ | Micron, Samsung | Qualcomm, Intel | Samsung, SK Hynix |\n\nTABLE 1: Comprehensive\
    \ comparison of NVM technologies [1]\u2013[4], [6]\u2013[18]\n\n# *A. NEED FOR\
    \ STT-RAM BASED SYSTEM EVALUATION FRAMEWORKS*\n\nResearchers have made significant\
    \ contributions to enhancing comparison metrics within their respective levels\
    \ of abstraction [1], [2], [4], [6]\u2013[11], [13], [14], [24], [25]. However,\
    \ it is crucial to note that the current architectural perspective findings are\
    \ derived from a behavioral model of the circuit, which may not offer precise\
    \ and detailed outcomes comparable to those from a real computing system. This\
    \ approach falls short in addressing the need for a comprehensive system exploration\
    \ framework. Hence, the associated research challenge is *how to design a holistic\
    \ system evaluation framework that can be used to evaluate the impact of incorporating\
    \ STT-RAM memories in current systems, while accurately modeling the scaling,\
    \ energy consumption and performance characteristics of these devices and enabling\
    \ architectural design space exploration.*\n\nA number of simulation environments\
    \ are available for research and development of system-level exploration of computer\
    \ architectures, such as gem5 and ZSIM [30]. However, gem5 is widely used due\
    \ to its ability to emulate the full-system mode and help in the exploration of\
    \ systemlevel metrics, with different instruction set architectures (ISAs) such\
    \ as Alpha, ARM, SPARC, MIPS, RISC-V, and x86 ISAs), and various timing and CPU\
    \ modes [31], [32]. ZSIM, as an alternative simulation software, does not offer\
    \ full-system simulation capabilities, but also does not rely on event-driven\
    \ execution and is therefore faster. As this work targets the integration of STT-RAM\
    \ into a complete system, also showing capabilities of executing an operating\
    \ system on top of STT-RAM, gem5 is the selected choice. gem5 showed fast enough\
    \ simulation speed for benchmark applications on top of an operating system.\n\
    \n# *B. ANALYZING STT-RAM IMPACT ON DIFFERENT APPLICATIONS*\n\nIn order to verify\
    \ the framework design along with its advantages, a case study i.e., investigating\
    \ STT-RAM from the perspective of reducing the energy consumption of High Performance\
    \ Computing (HPC) applications, and its characteristics i.e., power, area, latency,\
    \ etc., is carried out. Until now an ideal platform for system-level evaluation\
    \ is the gem5 simulator, as it provides methods for generating system environments\
    \ with easily exchangeable separated components such as the memory controller,\
    \ DRAM inter-\n\n## face, and NVM interface.\n\n# *C. NOVEL CONTRIBUTION*\n\n\
    In order to meet the requirement of designing a holistic system exploration framework,\
    \ this paper introduces an innovative memory interface utilizing STT-RAM, making\
    \ it a notable contribution. The interface has been created and seamlessly integrated\
    \ into the gem5 simulator, establishing a connection with the included memory\
    \ controller.\n\n*The novel contributions of this paper are:*\n\n- *\u2022* We\
    \ propose HOPE which is an STT-RAM modeling and simulation framework integrated\
    \ into the full system simulator gem5.\n- *\u2022* We leverage the recently implemented\
    \ NVM interface in gem5 to integrate HOPE with existing gem5 memory interfaces.\
    \ This is in contrast to prior approaches that rely on external patches (like\
    \ NVMain), which become less maintainable over time, thus stymying further development.\
    \ Our proposed framework introduces a third memory interface tailored specifically\
    \ for STT-RAM. This extension offers highly detailed results comparable to the\
    \ existing DRAM implementation within gem5. Fortunately, integrating our framework\
    \ into gem5 requires only minimal changes to gem5 files, as all functionality\
    \ is implemented in new files that can be added seamlessly. Our implementation\
    \ can be used identically to the existing memory interfaces and can potentially\
    \ be integrated into the official gem5 repository by its core maintainers. Such\
    \ integration would be the ideal outcome for our work, ensuring ongoing compatibility\
    \ with gem5.\n- *\u2022* We also extend the power model in gem5 DRAM-Power to\
    \ support our proposed STT-RAM model.\n- *\u2022* We evaluate HOPE using HPC applications\
    \ from the SPEC CPU 2017 benchmark suite on our event-driven gem5 simulator and\
    \ successfully extract evaluation metrics from both the application and circuit\
    \ perspectives.\n- *\u2022* We will also open-source our framework to enable and\
    \ accelerate the development of future system architectures based on STT-RAM.\n\
    \nThe rest of the paper is organized as follows: Section II presents the various\
    \ state-of-the-art works w.r.t. different types of memory controllers implemented\
    \ inside gem5 and their drawbacks. In section III, the STT-RAM integration\n\n\
    with the memory controller inside gem5 is discussed in detail. Section IV provides\
    \ the evaluation metrics, results, and comparison. Finally, we conclude this paper\
    \ in section V.\n\n#### **II. BACKGROUND AND RELATED WORKS**\n\nSTT-RAM has been\
    \ recently exploited as an alternative to conventional on-chip memories because\
    \ of its low energy consumption, high-speed access rate, scalability, and boundless\
    \ endurance. However, several fundamental barriers, i.e., reliability issues due\
    \ to Read/Write failure, Process Variation (PV) effects leading to stochastic\
    \ switching time, should be considered before its vast industrial adaptation.\n\
    \n# *A. OPERATION PRINCIPLES AND STT-RAM STRUCTURE*\n\nThe most common structure\
    \ of the STT-RAM cell includes an MTJ cell for data storage in series with an\
    \ access transistor (1T1MTJ). MTJ cells include an oxide barrier sandwiched between\
    \ two ferromagnetic layers called Rotation Layer (RL) and Fixed Layer (FL). The\
    \ magnetization orientation between these two layers will result in two different\
    \ states (i.e., parallel(P) and anti-parallel(AP)). These states are interpreted\
    \ as an indicator of logic one and logic zero (see Fig. 1). The concept of Read\
    \ and Write operation in the cell is explained in detail as follows:\n\n#### 1)\
    \ Write Operation\n\nIn order to write the intended information into the MTJ cell,\
    \ a write current should be applied through the memory cell. A successful writing\
    \ operation demands a minimum barrier exploiting the energy of the MTJ cell (*Eb*).\
    \ If the required energy is supplied, then the state of the memory could be changed\
    \ based on the current direction through the memory cell (see Fig. 1). In a coherent\
    \ STT-RAM model, the required current to fulfill the required minimum barrier\
    \ exploiting energy could be expressed as:\n\n$$I\\_c = I\\_{c0}(1 - \\frac{1}{\\\
    Delta\\_{\\text{max}}} \\ln(f\\_0 t\\_p))\\tag{1}$$\n\n$$I\\_{c0} = \\frac{8aeM\\\
    _s t}{\\eta h \\pi d^2} H\\_k \\tag{2}$$\n\nwhere, *Ic*<sup>0</sup> : critical\
    \ write current of the STT-RAM model (at 273*\u25E6* Kelvin). This parameter is\
    \ related to the physical\n\n![](_page_2_Figure_12.jpeg)\n\nFIGURE 1: Schematic\
    \ representation of magnetic orientation and energy barrier between two Magnetic\
    \ Tunnel Junction (MTJ) states [33]\n\nproperty of the MTJ cell, such as: *Ms*:\
    \ Material saturation magnetization; *f*0: Attempt frequency which is typically\
    \ *\u223C*1 ns, *Hk*: Effective magnetic anisotropy field, *\u03B1*: Damping factor,\
    \ *\u03B7*: Spin polarization, *tp*: Operating pulse-width (inverse of frequency),\
    \ which provides an access frequency of 1 GHz; *t, d*: physical dimensions of\
    \ the MTJ cell; \u2206: Thermal stability factor, and it can be expressed as:\n\
    \n$$\n\\Delta = \\frac{(M\\_s H\\_k \\text{td})}{2k\\_B T} \\tag{3}\n$$\n\nwhere,\
    \ *kBT* : Describes the ambient energy in the system due to random thermal fluctuations\
    \ [33].\n\n#### 2) Read Operation\n\n*R<sup>P</sup>* Read operation in MTJ cell\
    \ is due to the current *Iread* passage through the cell. *Iread* must be less\
    \ than *Icritical* and must not lead to change in the cell. The MTJ exhibits resistance\
    \ value based on its two layers' magnetization (0*\u25E6* or 180*\u25E6* in Fig.\
    \ 1). Therefore, by sensing, and passing the *Iread* and measuring the resistance,\
    \ we could identify the state of the cell. Moreover, the parameter Tunnel Magnetoresistance\
    \ Ratio (TMR) is described as the difference between these two resistance states\
    \ and can be expressed as *TMR* = (*RAP <sup>\u2212</sup>R<sup>P</sup>* ) , where\
    \ *RAP* is the resistance in anti-parallel and *R<sup>P</sup>* is the resistance\
    \ in parallel state. TMR is directly related to the cell's read latency, which\
    \ means a higher TMR enables a faster and more precise read operation [33].\n\n\
    #### *B. RELATED WORK*\n\nRecent studies show NVM as the main memory element via\
    \ a simulation in gem5, as gem5 was isolated from an NVM interface till October\
    \ 2020. The lack of an NVM interface made it necessary for NVM research to use\
    \ external tools such as NVSim [34], NVMain [35], and NVMain 2.0 [36]. To maintain\
    \ compatibility, these NVM simulators must be developed simultaneously with the\
    \ gem5 simulator. Especially NVMain offers a patch to be applied to the gem5 simulator,\
    \ which connects the NVMain tool to the gem5 simulator. This patch directly modifies\
    \ the gem5 source code and needs to be updated to match the latest gem5 releases.\
    \ The result of the patch is a co-simulator where the gem5 simulator is in constant\
    \ interaction with NVMain. Small changes in the gem5 simulator can directly require\
    \ a change in the NVM simulation tools and the needed patches for integration\
    \ into a co-simulator. The official patch for integrating NVMain into the gem5\
    \ was last updated in December 2016 and is incompatible with recent releases of\
    \ the gem5. Furthermore, these state-of-theart STT-RAM gem5 tool flow lacks available\
    \ open-source models, which has hampered research and experimentation, impacting\
    \ the adoption of STT-RAM in the current systems.\n\nTherefore, in this manuscript,\
    \ we propose a holistic gem5 based framework that will be open-sourced to fuel\
    \ research and development in this area and further enhancement in the framework.\n\
    \n![](_page_3_Figure_2.jpeg)\n\nFIGURE 2: The STTDDR4 Interface integration into\
    \ the gem5 standard componentslibrary. Components in blue are modified or new\
    \ to gem5, and components in white are unmodified gem5 components.\n\nThe work\
    \ presented in [37] explores some architecturelevel modifications of STT-RAM structure\
    \ aiming to provide NVM-based row buffers and reports a 67% energy improvement\
    \ exploiting their approaches compared with state-ofthe-art techniques. Moreover,\
    \ authors in [29] explore the possibility of using STT-RAM to substitute the DRAM\
    \ in main memory and evaluate their approach based on the SPEC CPU2006 dataset\
    \ to be compared with DRAM-based memories. This study has been carried out on\
    \ a tracebased cycle-accurate simulator. In [38], an STT-RAM-based memory has\
    \ been proposed based on a 9F2-cell on the circuit level. The exploited MTJ model\
    \ in this work requires a low switching current to change the state from logicone\
    \ to logic-zero. The application-level analysis has been estimated based on an\
    \ HPC SPEC CPU 2017 benchmark for latency improvement. The aforementioned techniques\
    \ either perform only circuit-level simulations (NVSim) which is typically time-consuming\
    \ due to detailed hardware simulations, while other approaches that evaluate applications\
    \ at the system level are not available open-source (to the best of our knowledge),\
    \ thus obstructing the adoption of the STT-RAM model in the systems. *Our novel\
    \ proposed HOPE framework has a fully integrated architectural model of STT-RAM\
    \ in the gem5 simulator using a memory controller for exploiting system-level\
    \ characteristics. This is in contrast to prior approaches that rely on external\
    \ patches (like NVMain). HOPE is an event-driven gem5 simulator that facilitates\
    \ system-level evaluation and enablesthe extraction of comparative metrics across\
    \ all layers of the system hierarchy.*\n\n## **III. THE HOPE FRAMEWORK**\n\n####\
    \ *A. HOPE FRAMEWORK OVERVIEW*\n\nThe novel proposed STT-RAM interface in this\
    \ manuscript is an additional memory interface to the gem5. Therefore, in order\
    \ to satisfy the compatibility of the STT-RAM interface with the gem5's MemCtrl\
    \ component, there is a\n\nneed for some modifications to the MemCtrl component.\
    \ This tailoring has no effect on the existing functionality of connecting DRAM\
    \ or NVM memories.\n\nIn the system configuration of gem5, the MemCtrl component\
    \ offers a single port to connect a main memory instance to the MemCtrl, historically\
    \ called *DRAM*, used for all types of memories. We introduced the STT-RAM interface\
    \ to the gem5 as an alternative choice to the DRAMInterface and NVMInterface components\
    \ as shown in Fig. 2. The interface is implemented in C++ (Component functional\
    \ description), wrapped by a Python parameter configuration (part of Component\
    \ definitions). The Python wrapper defines and inherits parameters that are mandatory\
    \ for the component to work. The functionality of the STTDDR4Interface is placed\
    \ within its C++ class. The component STT\\_1333\\_4x16 is the test device, populated\
    \ with parameters from the EMD4E001G16G2 datasheet [39]. Our proposed framework\
    \ enables access to modify the memory via its integrated interface and fetch the\
    \ output data in our component definition. Thus, we integrated all the functionality\
    \ into the STT-RAM memory controller through the provided interface to configure\
    \ and evaluate the system analysis. This provides an edge over the state-of-the-art\
    \ proposed methods that are based on the co-simulation of gem5 and other simulation\
    \ tools (e.g., NVSIM, NVMAIN, etc.).\n\n#### *B. HOPE STT-RAM POWER MODEL*\n\n\
    The implemented STTDDR4 memory interface implementation represents a state machine\
    \ consisting of states for idle, storing, activating, power up, and power down\
    \ stages, as shown in Fig. 3. The MemCtrl instance of the simulated system and\
    \ the memory logic help in the transition of the state in the state machine. As\
    \ soon as the system is started, the very first state of the system is the PWR\\\
    _IDLE state. This is the state from where the transition to activate state PWR\\\
    _ACT, or activate with store state PWR\\_ACT\\_ST\n\n![](_page_4_Figure_1.jpeg)\n\
    \nFIGURE 3: Power state machine of STTDDR4 integration to gem5\n\ncan be achieved\
    \ using an ACT or ACT\\_ST command. The ACT\\_ST command is introduced to gem5\
    \ which is responsible for the STT-RAM-specific handling of data in the volatile\
    \ page buffer.\n\nThe main memory can be exploited using gem5 standard commands\
    \ i.e., for a bank activation ACT (activate), RD (read), WR (write), REF (refresh\
    \ all banks), SREF (Self-Refresh), PRE (explicit pre-charge of a single bank),\
    \ etc. In this manuscript, an additional command is been introduced i.e., ACT\\\
    _ST (activate with the store). In the EMD4E001G16G2 datasheet, [39], the introduced\
    \ ACT\\_ST command is known as ACT\\*. The ACT\\* command includes a store procedure\
    \ for the specific bank accessed. The implementation of the ACT\\_ST command to\
    \ gem5 also includes a new event, the *actStoreEvent*, to enable the transition\
    \ to the new power state PWR\\_ACT\\_ST. Moreover, the automatic execution of\
    \ REF commands needed for data persistence in DRAM is not included in STTDDR4,\
    \ as refreshes are not mandatory in STT-RAM devices as per the EMD4E001G16G2 devices.\n\
    \nAs we know that method calls are responsible for MemCtrl interaction with the\
    \ memory interface during simulation\n\n# **Algorithm 1** Select between ACT and\
    \ ACT\\_ST command on rank:bank:row bank.storingState *\u2190* PERSISTENT bank.lastRow\
    \ *\u2190* 0\n\n```\nprocedure ACTIVATEBANK(rank, bank, row)\n  cmd \u2190 ACT\n\
    \  if bank.lastRow \u0338= row then\n     if bank.storingState = BUFFER then\n\
    \        cmd \u2190 ACT _ST\n        cmdDelay \u2190 cmdDelay + tST\n     end\
    \ if\n  end if\n  cmdList.push_back(cmd, bank, delay) \n  bank.lastRow \u2190\
    \ row\n  bank.storingState \u2190 BU FFER\n  process cmd in drampower\n```\n####\
    \ **Algorithm 2** DRAM Power Extension nOfActsBanks[banks] *\u2190* zeros(banks)\
    \ nOfStoresBanks[banks] *\u2190* zeros(banks) **procedure** EVALUATECOMMANDS(*cmdList*)\
    \ **for all** *cmd \u2208 cmdList* **do if** *cmd.type* = *ACT* **then** *handleAct*(*cmd.bank*)\
    \ **else if** *cmd.type* = *ACT*\\_*ST* **then** *handleActSt*(*cmd.bank*) **else\
    \ if** <other command types> **then** <handle commands> **end if end for end procedure\
    \ procedure** HANDLEACTST(*bank*) **if** isPrecharged(bank) **then** *nOf ActsBanks*[*bank*]\
    \ + +; *nOf StoresBanks*[*bank*] + +; **end if end procedure procedure** POWER\\\
    _CALC() *calc*(*sum*(*nOf StoresBanks*) *\u2217 tST, idd*0 *\u2212 idd*3*n*) **for\
    \ all** *bank \u2208 banks* **do** *calc*(*nOf StoresBanks*[*bank*] *\u2217 tST,\
    \ idd*0 *\u2212 ione*) **end for end procedure**\n\n**procedure** CALC(*cycles*,\
    \ *current*) **return** (cycles\\*clkPeriod\\*current\\*voltage) **end procedure**\n\
    \ne.g., while reading or writing data, the MemCtrl initiates a burst access to\
    \ the memory device where MemCtrl provides data for which rank, bank and row the\
    \ burst access needs to be executed. This rank, bank, and row information are\
    \ handed over to the bank activation method, as shown in algorithm 1. The EMD4E001G16G2\
    \ device includes functionality for automatic storing of page memory data to the\
    \ persistent memory array when the page memory data would become lost otherwise.\
    \ The MemCtrl does not offer functionality for differentiation of the storing\
    \ states in STT-RAM.\n\nTherefore, the STTDDR4Interface got extended with functionalities\
    \ to track the storing state of the data in the page buffer of each bank. The\
    \ storing state of each bank supports the states BUFFER and PERSISTENT. All the\
    \ banks during startup are initialized with PERSISTENT, which indicates the page\
    \ buffer data to be saved to the persistent memory array. Moreover, the state\
    \ BUFFER proves the bank to be not saved to the bank's persistent memory array.\
    \ Also, the last row accessed will be saved in each bank and updated during each\
    \ bank activation. In order to change the storing state of a bank, or all banks,\
    \ to PERSISTENT, a store operation needs to be performed. This can be triggered\
    \ by an ACT\\_ST command, a REF command, or an SREF command. Within a REF or SREF\
    \ execution, store operations on all banks in storing state BUFFER will be performed.\
    \ When there are no banks in storing state BUFFER, the commands REF and SREF are\
    \ not effective.\n\nThe selection between the ACT or ACT\\_ST command is done\
    \ in the activated bank method with a sequence of procedure calls as Algorithm\
    \ 1. The requested row is compared\n\nto the last accessed row of the particular\
    \ bank. When the last accessed data is still the working data, which means the\
    \ last access row and requested row are identical, there is no need for a store\
    \ operation. In this case, a normal ACT command will be simulated. If the requested\
    \ row differs from the last accessed row, and the bank is in the storing state\
    \ \"BUFFER\", an ACT\\_ST command will be pushed to simulate. The difference in\
    \ simulating ACT or ACT\\_ST is implemented in a higher delay for the additional\
    \ store operation, which is known as the store time (tST). This store procedure\
    \ call impacts the energy consumption that can be calculated in the power library.\
    \ The bank's storing state can be changed to \"PERSISTENT\" by performing a store\
    \ operation on the particular bank, or on all banks. The ACT command is simulated\
    \ as with DRAM memory in gem5: The ACT command is saved in a list of pending commands\
    \ which is handed over to the modified DRAMPower tool [40], which is part of gem5\
    \ and performs energy calculations from the gem5 inputs.\n\n**Extensions to DRAMPower\
    \ Model:** To be able to include the ACT\\_ST command, DRAMPower got extended\
    \ by the command and energy calculation. The extensions to DRAMPower are presented\
    \ in Algorithm 2. These extensions include functionality for counting the number\
    \ of store procedures during runtime and calculating the resulting store energy\
    \ and power.\n\nMoreover, the energy calculation in gem5 is not updated on every\
    \ new command execution, but on specific simulation phases: suspension, end of\
    \ a refresh (REF) command, triggered from the gem5 system configuration script\
    \ or by the internal command line of the full system simulation using the m5 utility\
    \ (the gem5 utility used in full system disk images).\n\nFurthermore, the gem5\
    \ statistic output has been modified to include the store energy per rank and\
    \ power state time per rank in the simulation results. In this section, we present\
    \ our HOPE framework for an all-integrated STT-RAM with a gem5 simulator using\
    \ the memory controller. This helps in exploiting system-level meta-heuristics\
    \ that include power consumption, memory utilization, heat patterns, etc.\n\n\
    #### *C. HOPE CONFIGURATION*\n\nThe gem5 being an instruction-level simulator\
    \ enables the integration of different types of memories with a memory controller.\
    \ The memory controller is a component that enables an option of choosing memory\
    \ for system-level analysis. The memory controller has evolved a lot in the\n\n\
    TABLE 2: The Configuration of the Memory cell\n\n| Paramters             | Configuration\
    \                    |\n|-----------------------|----------------------------------|\n\
    | Memory                | 1Gbit x16                        |\n| Organization \
    \         | 8-banks (2-banks per Bank Group) |\n| Latency               | 667MHz\
    \ (1333MT/s)                |\n| Access Time           | 225ps               \
    \             |\n| Supply Voltage - Min: | 1.14 V                           |\n\
    | Supply Voltage - Max: | 1.26 V                           |\n\npast few years.\
    \ Recently, in May 2020 the gem5 introduced a new memory controller (MemCtrl)\
    \ component revision and introduced an NVM interface (NVMInterface) class to the\
    \ gem5, officially integrated into version 20.1 of the gem5 simulator. This NVM\
    \ interface is designed very generic in terms of its functionality and parameters\
    \ to be taken into consideration. The NVMInterface class offers three timing parameters:\
    \ tREAD, tWRITE, and tSEND.\n\nThere is also an already existing DRAM interface\
    \ (DRAMInterface) class. This class of the gem5 contains detailed logic on DRAM\
    \ timing and power state simulation and offers various timing, e.g. tRCD, tCL,\
    \ tRAS, and tREFI, and energy parameters, e.g. IDD0, IDD4R, and VDD. But there\
    \ is no such logic for calculating NVM energy and power consumption. Also, there\
    \ are no energy parameters available for NVMInterface.\n\nThus, to overcome such\
    \ shortcomings, HOPE provides another detailed memory interface targeting STT-RAM.\
    \ This memory targets real-world STT-RAM devices which are designed as STT on\
    \ top of DDR4. Therefore, this interface is named as STTDDR4Interface. This offers\
    \ a high level of detail timing and energy parameters, combined with a power state\
    \ and energy calculation logic. Fig. 2 depicts the detailed flow of our HOPE framework\
    \ within the extended and modified gem5 simulator.\n\nThe simulated system is\
    \ configured using the fs.py system configuration script. Using this script, a\
    \ System instance is set up according to the input values of the HOPE framework.\
    \ We use an X86 architecture-based system. The CPU we defined is the TimingCPU,\
    \ which offers detailed memory timings during simulation. The CPU is also equipped\
    \ with L1 and L2 caches. The system uses a Linux kernel and a disk image with\
    \ an Ubuntu operating system and workloads installed. Within gem5, communication\
    \ between system blocks is done via ports, also as real systems do. The system\
    \ block is connected to the Membus block. The memory bus selected is by default,\
    \ the SystemXBar. All CPU interactions to the main memory are forwarded by the\
    \ Membus to the memory controller. MemCtrl got modified to support STT-RAM connected\
    \ through the memory port.\n\nFig. 2, shows the proposed architecture with the\
    \ STT\\_1333\\_4x16 which is a class created for interfacing STT-RAM with the\
    \ memory controller. It has multiple parameters e.g., tCK, tST, tRCD, IDD0, IDD2N,\
    \ etc that has been extracted from the datasheet of the aforementioned device.\n\
    \nThe tCK is the clock period, depending on the device operating clock frequency\
    \ (fCK) (e.g., fCK = 667MHz results in 1.5ns tCK (=1/fCK)), tST is a special timing\
    \ parameter for STT-RAM and refers to the storing time of the memory (indicates\
    \ the time needed for storing the data from the row address buffer to the persistent\
    \ STT memory). The tST is a newly introduced parameter to gem5 unique to STT,\
    \ which was a missing timing parameter for gem5. The address buffer acts like\
    \ a cache, and the data placed in the cache needs to be written to the main STT\
    \ memory\n\nTABLE 3: Experimental systems configuration for STT-RAM and DRAM\n\
    \n| System elements | Processor                                 | L1 Instr.<br>cache\
    \ | L1 Data<br>cache | L2 cache         | Main Memory                        \
    \                                                            | Clock Speed | Row\
    \ Buffer<br>Size | Device Size | Channel<br>Cappacity | tRCDmin | tRCmin | tRASmin\
    \ | tFAWmin | tRPmin |\n|-----------------|-------------------------------------------|--------------------|------------------|------------------|------------------------------------------------------------------------------------------------|-------------|--------------------|-------------|----------------------|---------|--------|---------|---------|--------|\n\
    | STT-RAM         | 64-bit x86<br>single core,<br>timing 3GHz | Private,<br>32kB\
    \   | Private,<br>32kB | Shared,<br>256kB | 1 channel,<br>2 rank per channel,<br>4\
    \ chips per rank,<br>EMD4E001G16G2,<br>1Gbit x16, 1333MHz | 667MHz      | 256B\
    \               | 128MiB      | 1GiB                 | 135ns   | 44.5ns | 32ns\
    \    | 15ns    | 12.5ns |\n| DRAM            | \"                            \
    \             | \"                  | \"                | \"                |\
    \ 1 channel,<br>2 rank per channel,<br>8 chips per rank,<br>MT40A1G8SA,<br>1Gbit\
    \ x8, 2400MHz     | 1200MHz     | 1kB                | 1GiB        | 16GiB   \
    \             | 12.5ns  | 190ns  | 143ns   | 240ns   | 7.5ns  |\n\nduring the\
    \ Store operation. Therefore, tST is the time needed to process data moving from\
    \ the row address buffer to the STT persistent memory array. Researchers in the\
    \ field could optimize different metrics to minimize this value and evaluate the\
    \ performance of so-called in-memory processing approaches. The parameters such\
    \ as tRCD, IDD0, IDD2N, etc., are standardized DDR4 parameters.\n\nMoreover, this\
    \ interface makes it possible to simulate systems using the latest STT-RAM devices\
    \ including power states and energy consumption as it was never possible before\
    \ in a stand-alone gem5 environment. The integration carried out on the interface\
    \ is based on the parameters offered by the STT-RAM EMD4E001G16G2 from Everspin\
    \ Technologies [39]. These device parameters are shown in Table 2.\n\nAs per the\
    \ physical characteristics of STT-RAM, there are deviations to the DDR4 specification\
    \ for DRAM especially the *Refresh* command, which is mandatory to be issued in\
    \ a time interval tREFI on DRAM, is no longer used in STT-RAM. Therefore, tREFI\
    \ got removed for STT memory. Moreover, the STT-RAM also has a store time parameter\
    \ tST. The store operation of delay tST, is used to move recently written data\
    \ from the page buffer to the persistent memory array.\n\nSome other deviations\
    \ specific to the test devices (simulating the EMD4E001G16G2 device) include the\
    \ memory size that in the case of STT-RAM is a 1 Gbit device, whereas the DDR4\
    \ specification for DRAM only allows devices of 2, 4, 8 and 16 GBit. Furthermore,\
    \ there is also a limit of 667 Mhz for the clock frequency, while the DDR4 Specification\
    \ for DRAM allows 800, 933, 1067, and 1200MHz.\n\n### *D. EVALUATION SETUP CONFIGURATION*\n\
    \nFig. 4 presents a comprehensive overview of the HOPE framework setup configuration\
    \ steps, highlighting its key contributions depicted in blue. gem5 full system\
    \ simulations require a disk image prepared with an operating system and a kernel\
    \ compatible with the chosen operating system. The HOPE framework uses a 1 modified\
    \ Packer SPEC CPU 2017 setup script from the gem5 resources repository for generating\
    \ a disk image containing the Ubuntu operating system and the SPEC CPU 2017 benchmark\
    \ suite for X86 architecture. The 2 benchmark installation is then followed finalized\
    \ by mounting the disk image on the host system. Each benchmark from the disk\
    \ image has been run once for finalizing the benchmark installation, this includes\
    \ com-\n\n![](_page_6_Figure_10.jpeg)\n\nFIGURE 4: Overview of HOPE framework\n\
    \npiling, training, and running the benchmark. In 3 the gem5 full system simulation\
    \ including the HOPE extensions and modifications is run. Therefore the created\
    \ disk image is used. Each simulation runs a selected workload from the prepared\
    \ disk image, includes the creation of checkpoints after the operating system\
    \ boot, and the output of the 4 detailed statistics after the gem5 simulation\
    \ is completed. The gem5 full system simulation includes the introduced STT-RAM\
    \ extension and modifications to DRAMPower to allow detailed energy calculation\
    \ for our STT-RAM device. 5 shows the provided McPAT template file, modified to\
    \ support the extended outputs of gem5. Using the system configuration, simulation\
    \ statistics, and McPAT template, the 6 the GEM5ToMcPAT [41] tool is used to generate\
    \ an input file for later use with McPAT. HOPE includes the enhanced 7 cMcPAT\
    \ power, area, and timing modeling framework. cMcPAT [41] is capable of calculating\
    \ the power parameters of 9. Using the statistics output of gem5, and the power\
    \ model of cMcPAT, the script \"print\\_energy\" [41] is calculating the total\
    \ energy consumption of the simulated environment. The results also combine the\
    \ detailed output of gem5, especially, the instructions count.\n\nThe simulated\
    \ system is configured using the gem5 stdlib (gem5 standard library) based on\
    \ an X86 configuration. Table 3 lists the detailed configuration of the processor,\
    \ cache, and memory for both experimental systems using STT-RAM and DRAM. We selected\
    \ an existing STT-RAM device for simulation and paired it with a widely used DRAM\
    \ device to facilitate a functional comparison. It's\n\nimportant to note that\
    \ our chosen STT-RAM device does not align with JEDEC's JESD79-4A DDR4 standard,\
    \ which limits our ability to select a DDR4-based DRAM device with nearly identical\
    \ parameters.\n\nMoreover, Fig. 5 shows the system configuration used for benchmarks\
    \ using STT-RAM. gem5 offers a general full system default configuration script\
    \ (fs.py), which we used in this work to reflect the system architecture of our\
    \ simulated system. We used the gem5 full system emulation mode to reflect real-world\
    \ systems in the best way provided.\n\nThe operating system selected is an Ubuntu\
    \ 18.04 configured for gem5 and set up with a SPEC CPU 2017 benchmark suite [42]\
    \ instance. The kernel used is the linux kernel version 4.19.83. The gem5 configuration\
    \ script handles the creation of memory controllers and memory devices. The count\
    \ of memory ranks and banks is set in the memory device configuration. The parameters\
    \ used for the STT-RAM device configuration in gem5 are sourced from the EMD4E001G16G2\
    \ datasheet [39]. We performed SPEC CPU 2017 benchmarks on our simulated systems\
    \ using 2 checkpoints per benchmark to be able to perform detailed simulations\
    \ using the TimingCPU from gem5.\n\nThe first checkpoint has been saved after\
    \ the OS boot is finished. The second checkpoint has been saved after the first\
    \ 4.5 billion instructions of benchmark application to ensure the initialization\
    \ phase has been finished and the checkpoint defines a direct jump into the main\
    \ benchmark algorithm. Both checkpoints were performed in a fastforward method\
    \ using gem5's AtomicSimpleCPU. The main simulation run was done from the second\
    \ checkpoint for a total of 2 Billion instructions. This procedure has been performed\
    \ for all of the benchmark applications included in SPEC CPU 2017.\n\n## **IV.\
    \ RESULTS**\n\nIn our research endeavour, our primary focus revolved around conducting\
    \ simulations utilizing the parameters of real-world devices. Specifically, we\
    \ honed in on a selected STT-RAM device, which holds the distinction of being\
    \ DDR4-compatible, albeit with certain deviations from the official standard.\
    \ As we delved into the simulations, we\n\n![](_page_7_Figure_8.jpeg)\n\nFIGURE\
    \ 5: Architecture of the STT-RAM simulated system.\n\nobserved that these deviations\
    \ had a tangible impact on our results, which we duly documented.\n\nOur findings\
    \ shed light on the applicability of HOPE as a potent tool for evaluating the\
    \ feasibility of incorporating STT-RAM main memories into practical systems. By\
    \ harnessing the capabilities of HOPE, we were able to gain insightful glimpses\
    \ into the performance and energy efficiency of the SPEC CPU 2017 benchmarks when\
    \ paired with STT-RAM. This analytical approach not only offers a valuable lens\
    \ to understand the potential of STT-RAM but also opens up new vistas of exploration\
    \ and optimization in memory technology research.\n\nThe gem5 simulator provides\
    \ users with multiple avenues for creating system configurations through its gem5\
    \ standard library of components and functions, including CPUs, memory, boards,\
    \ cache hierarchies, and more. This comprehensive collection of building blocks\
    \ is commonly referred to as the gem5 standard library. For instance, if you wish\
    \ to modify the CPU architecture, you can simply select the corresponding board\
    \ (e.g., X86Board or ArmBoard) and fine-tune the memory configurations accordingly.\
    \ This flexible approach can be applied to alter any available system component.\n\
    \nAlternatively, there is an option to configure a system within gem5 without\
    \ utilizing a predefined board. Instead, you can manually establish connections\
    \ between a CPU and a selected memory device using a memory bus. Furthermore,\
    \ you have the flexibility to augment the CPU with either a straightforward single-level\
    \ cache or a more intricate cache hierarchy to suit your needs.\n\nOur proposed\
    \ framework takes a distinct approach by leveraging an existing system configuration\
    \ known as fs.py. This configuration can be effortlessly modified via command-line\
    \ inputs, enabling rapid adjustments to the system configuration with a single\
    \ bash script edit. Different fs.py configurations are available for various system\
    \ architectures, such as *configs/example/riscv/fs*\\_*linux.py* for RISC-V or\
    \ *configs/example/arm/fs*\\_*bigLITTLE.py* for ARM.\n\nIn our research endeavors,\
    \ we conducted extensive simulations employing both fs.py and custom gem5 system\
    \ configuration scripts. This comprehensive approach allowed us to thoroughly\
    \ assess and analyze our simulations.\n\n## *A. STT-RAM STATE TIME DISTRIBUTION*\n\
    \nAs depicted in Fig. 8(b), the power state times for the IDLE state exhibit variations\
    \ in the case of STT-RAM, contingent upon the specific workload. Notably, STT-RAM\
    \ distinguishes itself by eschewing the need for periodic refreshes to maintain\
    \ data states, leading to a complete absence of time spent in the REF power state.\
    \ Conversely, the bank activation (ACT) time, also illustrated in Fig. 8(b), demonstrates\
    \ only a minor variation within the STT-RAMbased system. This effect can be attributed\
    \ to the relatively prolonged delays observed in the bank activation process,\
    \ especially concerning store state (ACT\\_ST) for STT-RAM,\n\nas visually demonstrated\
    \ in Fig. 8(b). Such insights garnered from our analyses provide valuable perspectives\
    \ on the dynamic behaviour and efficiency of STT-RAM, imparting crucial knowledge\
    \ for potential real-world implementations and optimizations in-memory technologies.\n\
    \n# *B. STT-RAM PERFORMANCE EVALUATION*\n\nFig. 6(a) show the total amount of\
    \ read and write requests which are generally higher in the STT-RAM devices. High\
    \ read and write requests are explained by Fig. 8(a) which shows the hit rate\
    \ for read operations. The hit rates for STT-RAM depend on the application complexity\
    \ level. Fig. 8(c) shows the average read and write bandwidth with a simulated\
    \ STT-RAM. Also, the average latency of STT-RAM, shown in Fig. 6(b), for each\
    \ application highly depends on the hit ratio. As described in our STTDDR4 power\
    \ state machine description, a high alteration of accessed rows negatively affects\
    \ the energy and latency of our simulated STT-RAM device. A high alteration of\
    \ accessed rows further has an impact on store operations.\n\n# *C. STT-RAM POWER\
    \ AND ENERGY BREAKDOWN*\n\nIn Fig. 6 (c and d), we present a comprehensive view\
    \ of the power and energy breakdown for our simulated systems, offering valuable\
    \ insights into their performance characteristics. The shown parameters in d represent\
    \ the accumulated energy for different commands issued to the memory device. As\
    \ shown in Fig. 3, the memory devices move through different states during runtime.\
    \ The parameter \"Activation energy\" therefore shows the total energy consumption\
    \ for all ACT commands initiated. \"Store energy\" accumulates the energy of all\
    \ store operations during the evaluation. In the case of an ACT\\_ST operation,\
    \ the energy accumulated separately for the activation and the store energy results.\
    \ The parameter \"Refresh energy\" is associated to the REF command, while the\
    \ \"RD/WR energy\" is the accumulated energy during the processing of the read\
    \ and write burst accesses.\n\nParticularly noteworthy is the substantial count\
    \ of store operations, which, in conjunction with the notably high IDD0 current\
    \ of 437mA, prominently influences the calculated store energy. As evident from\
    \ the results, STT-RAM stands out for its lack of refresh energy requirements.\
    \ However, it should be noted that the stored energy demands contribute to an\
    \ overall increase in the total energy breakdown. These findings shed light on\
    \ the contrasting energy consumption patterns of STT-RAM compared to conventional\
    \ memory technologies, signifying the potential for more energy-efficient computing\
    \ paradigms. The comprehensive understanding gained from these power and energy\
    \ analyses is essential for devising strategies to optimize memory architectures,\
    \ thus fostering advancements in the realm of energy-efficient computing systems.\
    \ The presented energy parameters in d are not a full view of all calculated energy\
    \ parameters within gem5, but an excerpt of significant values. The full list\
    \ of energy parameters is extended by\n\nparameters for interface energy, standby\
    \ energy, self-refresh energy, power-down and power-up energy.\n\n# *D. DRAM METRICS*\n\
    \nTo maintain the extendibility and versatility of our framework, we have thoughtfully\
    \ retained the interface to the DRAM. This strategic decision allows our framework\
    \ to adapt effortlessly to various memory technologies, rendering it highly versatile\
    \ for a wide array of computing scenarios. In this section, we present an in-depth\
    \ analysis of the extracted data concerning the state time distribution, power\
    \ consumption, energy usage, and latency breakdown for the same applications from\
    \ SPEC 2017, with the DRAM serving as the primary memory. This comprehensive investigation\
    \ is instrumental in understanding the behavior and performance characteristics\
    \ of our framework when interfacing with DRAM.\n\nFig. 9(b) illustrates the state\
    \ time distribution of these applications when utilizing DRAM. Additionally, Fig.\
    \ 7(a) showcases the framework's memory requests. Analyzing memory requests offers\
    \ deeper insights into the applications\xB4 memory access patterns, shedding light\
    \ on potential areas for improvement in terms of data locality and cache utilization.\
    \ Fig. 9(c) provides insight into bandwidth usage with DRAM as the memory. Bandwidth\
    \ utilization is a critical metric for assessing memory system efficiency and\
    \ identifying potential bottlenecks that may impact application performance. Furthermore,\
    \ Fig. 9(a) reveals DRAM row hits, and Fig. 7(b) presents latency per application.\
    \ Finally, Fig. 7(c and d) exhibits DRAM average power and energy usage while\
    \ running the SPEC 2017 applications. These detailed analyses offer valuable insights\
    \ into our framework's performance and its potential for adaptation to future\
    \ memory technologies and diverse computing environments.\n\n## *E. COMPARING\
    \ STT-RAM AND DRAM RESULTS*\n\nBased on our evaluation of SPEC 2017 benchmarks,\
    \ it becomes evident that STT-RAM is not yet ready to replace DRAM-based main\
    \ memories for many applications due to its higher store latency and energy consumption.\
    \ STT-RAM needs storing from the page buffer to the persistent memory array whereas\
    \ DRAM does not need this. Due to the overhead for storing data in the STT persistent\
    \ memory array within the ACT-ST state, the delays are significantly higher than\
    \ in DRAM. Each store takes 380ns extra, in all cases of ACT-ST state. Furthermore,\
    \ STT-RAM is running at 1333 MHz, whereas DRAM is running at 2400 MHz. The impact\
    \ is especially pronounced in applications with high write-to-read ratios like\
    \ Ibm\\_s (see Fig. 6a). Before STT-RAM can be a feasible alternative to DRAM's\
    \ main memories, further technology, and architectural optimizations are necessary\
    \ to reduce the store latency and energy requirements. Fortunately, with the availability\
    \ of HOPE, we now have a systematic means to evaluate and optimize STT-RAM at\
    \ the system level. HOPE presents an invaluable opportunity to drive STT-RAM's\
    \ progress by allowing us to\n\n![](_page_9_Figure_2.jpeg)\n\nexplore and analyze\
    \ various architectural possibilities with greater precision.\n\n## **V. CONCLUSION**\n\
    \nWe presented an extension to the open-source full-system simulator gem5 for\
    \ enabling detailed evaluation of STT-RAM devices in an accurate manner. We have\
    \ shown our implemented power state machine, memory commands, power calculation,\
    \ and statistics output. We have also shown the results of an STT-RAM-based system\
    \ configured using real-world device parameters and compared the resulting metrics\
    \ to a DRAM-based system. The STT-RAM-specific characteristics of required store\
    \ operations and deviations to the DDR4 standard for DRAM have been discussed\
    \ based on the comparison of the simulation results. Our HOPE implementation is\
    \ easily configurable for other STT-RAM devices, by adding timing values, currents,\
    \ and voltages. We will open-source our HOPE framework to fuel research and accelerate\
    \ the development of future system architectures based on STT-RAM.\n\n## **VI.\
    \ ACKNOWLEDGMENT**\n\nThe authors acknowledge TU Wien Bibliothek for financial\
    \ support through its Open Access Funding Programme.\n\n#### **REFERENCES**\n\n\
    - [1] Y. Wang, C. Zhang, H. Yu, and W. Zhang, \"Design of low power 3d hybrid\
    \ memory by non-volatile cbram-crossbar with block-level data-retention,\" in\
    \ Proceedings of the 2012 ACM/IEEE international symposium on Low power electronics\
    \ and design, 2012, pp. 197\u2013202.\n- [2] Y. Shin, \"Non-volatile memory technologies\
    \ for beyond 2010,\" in Digest of Technical Papers. 2005 Symposium on VLSI Circuits,\
    \ 2005. IEEE, 2005, pp. 156\u2013159.\n- [3] B. Gervasi, \"Will carbon nanotube\
    \ memory replace dram?\" IEEE Micro, vol. 39, no. 2, pp. 45\u201351, 2019.\n-\
    \ [4] J. Lamb, S. Gibbons, R. Trichur, Y. Jiang, K. Mangelson, K. Kremer, and\
    \ D. Janzen, \"Advancements in microelectronics-grade carbon nanotube materials\
    \ for nram\xAE device manufacture and analysis of carbon nanotube mass in end\
    \ user devices.\"\n- [5] \"Irds 2022 beyond cmos and emerging materials integration,\"\
    \ Online, 2022, accessed on: November 7, 2023. [Online]. Available: https://irds.ieee.org/editions/2022\n\
    - [6] D. Jana, S. Roy, R. Panja, M. Dutta, S. Z. Rahaman, R. Mahapatra, and S.\
    \ Maikap, \"Conductive-bridging random access memory: challenges and opportunity\
    \ for 3d architecture,\" Nanoscale research letters, vol. 10, pp. 1\u201323, 2015.\n\
    - [7] S. Mittal and J. S. Vetter, \"A survey of software techniques for using\
    \ non-volatile memories for storage and main memory systems,\" IEEE Transactions\
    \ on Parallel and Distributed Systems, vol. 27, no. 5, pp. 1537\u2013 1550, 2015.\n\
    - [8] J. S. Meena, S. M. Sze, U. Chand, and T.-Y. Tseng, \"Overview of emerging\
    \ nonvolatile memory technologies,\" Nanoscale research letters, vol. 9, pp. 1\u2013\
    33, 2014.\n- [9] T. Mikolajick, C. Dehm, W. Hartner, I. Kasko, M. Kastner, N.\
    \ Nagel, M. Moert, and C. Mazure, \"Feram technology for high density applications,\"\
    \ Microelectronics Reliability, vol. 41, no. 7, pp. 947\u2013950, 2001.\n- [10]\
    \ M. Imani, S. Patil, and T. Rosing, \"Low power data-aware stt-ram based hybrid\
    \ cache architecture,\" in 2016 17th international symposium on quality electronic\
    \ design (isqed). IEEE, 2016, pp. 88\u201394.\n- [11] S. Jeloka, Z. Wang, R. Xie,\
    \ S. Khanna, S. Bartling, D. Sylvester, and D. Blaauw, \"Energy efficient adiabatic\
    \ fram with 0.99 pj/bit write for iot applications,\" in 2018 IEEE symposium on\
    \ VLSI circuits. IEEE, 2018, pp. 85\u201386.\n- [12] M. Moore et al., \"International\
    \ roadmap for devices and systems,\" Accessed: Jan, 2020.\n- [13] I. Yoon, A.\
    \ Anwar, T. Rakshit, and A. Raychowdhury, \"Transfer and online reinforcement\
    \ learning in stt-mram based embedded systems for autonomous drones,\" in 2019\
    \ Design, Automation & Test in Europe Conference & Exhibition (DATE). IEEE, 2019,\
    \ pp. 1489\u20131494.\n- [14] B. Narasimham, V. Chaudhary, M. Smith, L. Tsau,\
    \ D. Ball, and B. Bhuva, \"Scaling trends in the soft error rate of srams from\
    \ planar to 5-nm finfet,\" in 2021 IEEE International Reliability Physics Symposium\
    \ (IRPS). IEEE, 2021, pp. 1\u20135.\n- [15] J. Wang, N. Xiu, J. Wu, Y. Chen, Y.\
    \ Sun, H. Yang, V. Narayanan, S. George, and X. Li, \"An 8t/cell fefet-based nonvolatile\
    \ sram with improved density and sub-fj backup and restore energy,\" in 2022 IEEE\
    \ International Symposium on Circuits and Systems (ISCAS), 2022, pp. 3408\u2013\
    3412.\n- [16] J. Y. Kim, M.-J. Choi, and H. W. Jang, \"Ferroelectric field effect\
    \ transistors: Progress and perspective,\" APL Materials, vol. 9, no. 2, p. 021102,\
    \ 02 2021.\n- [17] S. Yu, Q. Wang, Y. Zhang, P. Yang, X. Luo, H. Liu, C. Chen,\
    \ Q. Li, and S. Liu, \"Multistate capability improvement of beol compatible fefet\
    \ by introducing an al2o3 interlayer,\" IEEE Transactions on Electron Devices,\
    \ vol. 70, no. 11, pp. 5632\u20135637, 2023.\n- [18] J. Y. Park, D.-H. Choe, D.\
    \ H. Lee, G. T. Yu, K. Yang, S. H. Kim, G. H. Park, S.-G. Nam, H. J. Lee, S. Jo,\
    \ B. J. Kuh, D. Ha, Y. Kim, J. Heo, and M. H. Park, \"Revival of ferroelectric\
    \ memories based on emerging fluoritestructured ferroelectrics,\" Advanced Materials,\
    \ vol. 35, no. 43, p. 2204904, 2023.\n- [19] S. Seyedfaraji, J. T. Daryani, M.\
    \ M. S. Aly, and S. Rehman, \"Extent: Enabling approximation-oriented energy efficient\
    \ stt-ram write circuit,\" IEEE Access, vol. 10, pp. 82 144\u201382 155, 2022.\n\
    - [20] S. M. Nair, R. Bishnoi, A. Vijayan, and M. B. Tahoori, \"Dynamic faults\
    \ based hardware trojan design in stt-mram,\" in 2020 Design, Automation & Test\
    \ in Europe Conference & Exhibition (DATE). IEEE, 2020, pp. 933\u2013938.\n- [21]\
    \ R. Bishnoi, M. Ebrahimi, F. Oboril, and M. B. Tahoori, \"Improving write performance\
    \ for stt-mram,\" IEEE Transactions on Magnetics, vol. 52, no. 8, pp. 1\u2013\
    11, 2016.\n- [22] S. Swami and K. Mohanram, \"Reliable nonvolatile memories: Techniques\
    \ and measures,\" IEEE Design & Test, vol. 34, no. 3, pp. 31\u201341, 2017.\n\
    - [23] S. Seyedfaraji, A. M. Hajisadeghi, J. Talafy, and H. R. Zarandi, \"Dysco:\
    \ Dynamic stepper current injector to improve write performance in stt-ram memories,\"\
    \ Microprocessors and Microsystems, vol. 73, p. 102963, 2020.\n- [24] E. Garzon,\
    \ R. De Rose, F. Crupi, L. Trojman, G. Finocchio, M. Carpentieri, and M. Lanuzza,\
    \ \"Assessment of stt-mrams based on double-barrier mtjs for cache applications\
    \ by means of a device-to-system level simulation framework,\" Integration, vol.\
    \ 71, pp. 56\u201369, 2020.\n- [25] R. Saha, Y. P. Pundir, and P. K. Pal, \"Design\
    \ of an area and energyefficient last-level cache memory using stt-mram,\" Journal\
    \ of Magnetism and Magnetic Materials, vol. 529, p. 167882, 2021.\n- [26] E. Cheshmikhani,\
    \ H. Farbeh, and H. Asadi, \"3rset: Read disturbance rate reduction in stt-mram\
    \ caches by selective tag comparison,\" IEEE Transactions on Computers, vol. 71,\
    \ no. 6, pp. 1305\u20131319, 2021.\n- [27] \u2014\u2014, \"Robin: Incremental\
    \ oblique interleaved ecc for reliability improvement in stt-mram caches,\" in\
    \ Proceedings of the 24th Asia and South Pacific Design Automation Conference,\
    \ 2019, pp. 173\u2013178.\n- [28] N. Mahdavi, F. Razaghian, and H. Farbeh, \"\
    Data block manipulation for error rate reduction in stt-mram based main memory,\"\
    \ The Journal of Supercomputing, vol. 78, no. 11, pp. 13 342\u201313 372, 2022.\n\
    - [29] E. K\xFClt\xFCrsay, M. Kandemir, A. Sivasubramaniam, and O. Mutlu, \"Evaluating\
    \ stt-ram as an energy-efficient main memory alternative,\" in 2013 IEEE International\
    \ Symposium on Performance Analysis of Systems and Software (ISPASS), 2013, pp.\
    \ 256\u2013267.\n- [30] D. Sanchez and C. Kozyrakis, \"Zsim: Fast and accurate\
    \ microarchitectural simulation of thousand-core systems,\" ACM SIGARCH Computer\
    \ architecture news, vol. 41, no. 3, pp. 475\u2013486, 2013.\n- [31] N. Binkert,\
    \ B. Beckmann, G. Black, S. K. Reinhardt, A. Saidi, A. Basu, J. Hestness, D. R.\
    \ Hower, T. Krishna, S. Sardashti et al., \"The gem5 simulator,\" ACM SIGARCH\
    \ computer architecture news, vol. 39, no. 2, pp. 1\u20137, 2011.\n- [32] A. Hansson,\
    \ N. Agarwal, A. Kolli, T. Wenisch, and A. N. Udipi, \"Simulating dram controllers\
    \ for future system architecture exploration,\" in 2014 IEEE International Symposium\
    \ on Performance Analysis of Systems and Software (ISPASS). IEEE, 2014, pp. 201\u2013\
    210.\n- [33] A. Gebregiorgis, L. Wu, C. M\xFCnch, S. Rao, M. B. Tahoori, and S.\
    \ Hamdioui, \"Special session: Stt-mrams: Technology, design and test,\" in 2022\
    \ IEEE 40th VLSI Test Symposium (VTS). IEEE, 2022, pp. 1\u201310.\n- [34] X. Dong,\
    \ C. Xu, Y. Xie, and N. P. Jouppi, \"Nvsim: A circuit-level performance, energy,\
    \ and area model for emerging nonvolatile memory,\" IEEE Transactions on Computer-Aided\
    \ Design of Integrated Circuits and Systems, vol. 31, no. 7, pp. 994\u20131007,\
    \ 2012.\n- [35] M. Poremba and Y. Xie, \"Nvmain: An architectural-level main memory\
    \ simulator for emerging non-volatile memories,\" in 2012 IEEE Computer Society\
    \ Annual Symposium on VLSI. IEEE, 2012, pp. 392\u2013397.\n- [36] M. Poremba,\
    \ T. Zhang, and Y. Xie, \"Nvmain 2.0: A user-friendly memory simulator to model\
    \ (non-) volatile memory systems,\" IEEE Computer Architecture Letters, vol. 14,\
    \ no. 2, pp. 140\u2013143, 2015.\n- [37] J. L. NMeza, Justin and O. Mutlu., \"\
    Evaluating row buffer locality in future non-volatile main memories,\" in arXiv\
    \ preprint arXiv:1812.06377. arXiv, 2018.\n- [38] S. Chung, K.-M. Rho, S.-D. Kim,\
    \ H.-J. Suh, D.-J. Kim, H.-J. Kim, S.-H. Lee, J.-H. Park, H.-M. Hwang, S.-M. Hwang,\
    \ J.-Y. Lee, Y.-B. An, J.-U. Yi, Y.-H. Seo, D.-H. Jung, M.-S. Lee, S.-H. Cho,\
    \ J.-N. Kim, G.-J. Park, G. Jin, A. Driskill-Smith, V. Nikitin, A. Ong, X. Tang,\
    \ Y. Kim, J.-S. Rho, S.-K. Park, S.-W. Chung, J.-G. Jeong, and S.-J. Hong, \"\
    Fully integrated 54nm stt-ram with the smallest bit cell dimension for high density\
    \ memory pplication,\" in 2010 International Electron Devices Meeting, 2010, pp.\
    \ 12.7.1\u201312.7.4.\n- [39] \"1 gb non-volatile st-ddr4 spin-transfer torque\
    \ mram.\" [Online]. Available: https://shorturl.at/bgsMR\n- [40] K. Chandrasekar,\
    \ C. Weis, Y. Li, B. Akesson, N. Wehn, and K. Goossens, \"Drampower: Open-source\
    \ dram power & energy estimation tool,\" [URL:http://www.](http://www/) drampower.\
    \ info, vol. 22, 2012.\n- [41] A. Brokalakis, N. Tampouratzis, A. Nikitakis, I.\
    \ Papaefstathiou, S. Andrianakis, D. Pau, E. Plebani, M. Paracchini, M. Marcon,\
    \ I. Sourdis, P. R. Geethakumari, M. C. Palacios, M. A. Anton, and A. Szasz, \"\
    Cossim: An open-source integrated solution to address the simulator gap for systems\
    \ of systems,\" in 2018 21st Euromicro Conference on Digital System Design (DSD),\
    \ 2018, pp. 115\u2013120.\n- [42] [Online]. Available: [https://www.S](http://www.spec.org/cpu2017)PEC.o[rg/cpu2017](http://www.spec.org/cpu2017)\n\
    \n![](_page_11_Picture_11.jpeg)\n\nSAEED SEYEDFARAJI is a Graduate Student Member,\
    \ IEEE, and holds a B.Sc. degree from Isfahan University of Technology in Isfahan,\
    \ Iran, and an M.Sc. degree from Amirkabir University of Technology (Tehran Polytechnique)\
    \ in Tehran, Iran. Currently, he is pursuing a Ph.D. in computer engineering at\
    \ the Technische Universit\xE4t Wien (TU Wien), Austria, where he also serves\
    \ as a University Assistant. His research interests encompass emerging non-volatile\
    \ memory\n\ntechnologies, in-memory processing, the integration of intelligence\
    \ into hardware, and system-on-chip design. Notably, he received the Design Automation\
    \ Conference 2020 Young Fellow (DAC YF 2020) Prize and was a part of the Best\
    \ Team at DAC YF 2020.\n\n![](_page_11_Picture_14.jpeg)\n\nMARKUS BICHL is currently\
    \ with the Technische Universit\xE4t Wien (TU Wien), Faculty of Electrical Engineering\
    \ and Information Technology (ETIT) as a student. He started his studies with\
    \ Technische Universit\xE4t Wien (TU Wien), Faculty of Informatics, Bachelore's\
    \ Programme of Computer Science in 2016. He is pursuing his Master's degree in\
    \ Electrical Engineering in the Master's Programme Embedded Systems. His main\
    \ research interests include Emerging\n\nMemory Technologies, low-power computing,\
    \ FPGA development, ASIC design, and cyber-physical systems. Besides his studies,\
    \ he is working on industry-leading electrical powertrains for the automotive\
    \ industry, with hundreds of thousands of units already produced. His passion\
    \ is to work further on Embedded Systems topics and gain a professional career\
    \ in research.\n\n![](_page_11_Picture_17.jpeg)\n\nASAD AFTAB is a Graduate Student\
    \ Member, IEEE, and received the B.S. degree in Computer Systems engineering from\
    \ University of Engineering and Technology (UET), Peshawar, in 2017 and the M.S.\
    \ degree in Electrical (Telecommunication and Computer Networks) engineering from\
    \ the National University of Sciences and Technology (NUST), Islamabad, in 2021.\
    \ He is currently pursuing a Ph.D. in Electrical engineering at the Technische\
    \ Universit\xE4t Wien (TU\n\nWien), Austria. His research interests encompass\
    \ designing both hardware and software-based sustainable security techniques for\
    \ autonomous CPS, which includes researching suitable ML algorithms for defence,\
    \ analyzing various adversarial attacks, and exploring innovative defence methods\
    \ to enhance the resilience of machine learning algorithms.\n\n![](_page_11_Picture_20.jpeg)\n\
    \nSEMEEN REHMAN is currently with the Technische Universit\xE4t Wien (TU Wien),\
    \ as an Assistant Professor. In October 2020, she received her Habilitation Degree\
    \ in the area of Embedded Systems from the Faculty of Electrical Engineering and\
    \ Information Technology, TU Wien. She has co-authored one book, multiple book\
    \ chapters, and more than 60+ publications in premier journals and conferences.\
    \ Her main research interests include dependable systems and energy-efficient\n\
    \nembedded system, approximate computing, security, IoT/CPS. She has received\
    \ the CODES+ISSS 2011 and 2015 Best Paper Awards, DATE 2017 Best Paper Award Nomination,\
    \ HiPEAC Paper Awards, DAC Richard Newton Young Student Fellow Award, and Research\
    \ Student Award at the KIT. She served as the Topic Track Chair and co-chair at\
    \ the DATE and ISVLSI conferences from 2020 and 2023, and has served as the TPC\
    \ of multiple premier conferences on design automation and embedded systems."
  paper_id: 13
  scores:
    originality:
      raw: 4.5
      reason: The paper presents a novel approach to concurrent kernel execution on
        irregular computational graphs, addressing a significant gap in GPU utilization
        for specific applications.
    technical_soundness:
      raw: 4.0
      reason: The methods proposed are well-founded and demonstrate a solid understanding
        of GPU architecture, though some implementation details could be better justified.
    clarity:
      raw: 3.8
      reason: While the paper is generally well-written, certain sections could benefit
        from clearer explanations of complex concepts and methodologies.
    significance:
      raw: 4.2
      reason: The findings have important implications for enhancing GPU performance
        in underutilized applications, making it a valuable contribution to the field.
    relevance:
      raw: 4.5
      reason: The topic is highly relevant to current trends in GPU computing and
        machine learning, particularly in the context of deep reinforcement learning
        and dynamic neural networks.
  total_score: 4.15
  tier: B
- title: "SpecLLM: Exploring Generation and Review of VLSI Design Specification\n\
    \  with Large Language Model"
  abstract: 'Providing end-to-end stochastic computing (SC) neural network acceleration

    for state-of-the-art (SOTA) models has become an increasingly challenging task,

    requiring the pursuit of accuracy while maintaining efficiency. It also

    necessitates flexible support for different types and sizes of operations in

    models by end-to-end SC circuits. In this paper, we summarize our recent

    research on end-to-end SC neural network acceleration. We introduce an accurate

    end-to-end SC accelerator based on a deterministic coding and sorting network.

    In addition, we propose an SC-friendly model that combines low-precision data

    paths with high-precision residuals. We introduce approximate computing

    techniques to optimize SC nonlinear adders and provide some new SC designs for

    arithmetic operations required by SOTA models. Overall, our approach allows for

    further significant improvements in circuit efficiency, flexibility, and

    compatibility through circuit design and model co-optimization. The results

    demonstrate that the proposed end-to-end SC architecture achieves accurate and

    efficient neural network acceleration while flexibly accommodating model

    requirements, showcasing the potential of SC in neural network acceleration.'
  url: http://arxiv.org/abs/2401.15332v1
  keywords: ''
  document: "### I. INTRODUCTION\n\nStochastic computing (SC) has emerged as a promising\
    \ alternative to traditional binary computing, offering simplified arithmetic\
    \ operations and improved error resilience [\\[1\\]](#page-4-0)\u2013[\\[5\\]](#page-4-1).\
    \ Both hybrid and end-to-end SC-based neural accelerators have been proposed [\\\
    [1\\]](#page-4-0)\u2013[\\[5\\]](#page-4-1). While hybrid accelerators involve\
    \ back-and-forth conversion between binary and SC representations, leading to\
    \ high power consumption and area overhead, end-to-end SC-based accelerators demonstrate\
    \ superior power, area efficiency, and fault tolerance [\\[3\\]](#page-4-2)\u2013\
    [\\[5\\]](#page-4-1). In this context, our research aims to further enhance the\
    \ capabilities of endto-end SC-based accelerators.\n\nExisting SC-based accelerators\
    \ primarily focus on multiplication, accumulation, and activation functions in\
    \ convolutional networks [\\[6\\]](#page-4-3)\u2013[\\[9\\]](#page-4-4). However,\
    \ these approaches have limitations. FSM-based activation modules suffer from\
    \ accuracy issues, particularly for ReLU with larger accumulation widths (Figure\
    \ [1\\)](#page-0-0). Furthermore, there exists a trade-off between inference efficiency\
    \ and accuracy (Figure [2\\)](#page-0-1), where high precision computing enhances\
    \ accuracy but exponentially increases costs, while low precision computing compromises\
    \ accuracy. Additionally, there is a lack of research on SC circuits supporting\
    \ functions like batch normalization (BN), residual connections, gaussian error\
    \ linear unit (GELU), and softmax for state-of-the-art (SOTA) models.\n\nTherefore,\
    \ in this paper, we will summarize our recent efforts on end-to-end SC-based NN\
    \ accelerators that address these limitations to meet the requirements in terms\
    \ of accuracy, efficiency, flexibility, and compatibility, as shown in Table [I.](#page-0-2)\n\
    \n## <span id=\"page-0-3\"></span>II. ACCURATE END-TO-END SC ACCELERATOR BASED\
    \ ON DETERMINISTIC THERMOMETER CODING\n\nIn this section, we introduce deterministic\
    \ coding called thermometer coding and the corresponding end-to-end SC accelerator\
    \ design. The proposed design achieves exact end-to-end SC NN acceleration.\n\n\
    This work was supported in part by the National Key R&D Program of China (2020YFB2205502),\
    \ NSFC (62125401) and the 111 Project (B18001).\n\n<sup>\u2217</sup>Corresponding\
    \ author: {meng.li, r.wang}@pku.edu.cn\n\n![](_page_0_Figure_14.jpeg)\n\n<span\
    \ id=\"page-0-0\"></span>Fig. 1. FSM-based design to implement (a) tanh and (b)\
    \ ReLU. Ideally, the circuit output is the same as the exact output, marked by\
    \ the red line.\n\n<span id=\"page-0-1\"></span>![](_page_0_Figure_16.jpeg)\n\n\
    Fig. 2. The trade-off between inference accuracy and efficiency (measured by area-delay\
    \ product, i.e., ADP). Here, we fix the weight BSL to 2-bit and sweep the activation\
    \ BSL.\n\n<span id=\"page-0-2\"></span>TABLE I COMPARISON OF DIFFERENT END-TO-END\
    \ SC ACCELERATORS.\n\n| Design            |      | Accuracy Efficiency | Flexibility\
    \                          | *Compatibility |\n|-------------------|------|---------------------|--------------------------------------|----------------|\n\
    | FSM-based [6]\u2013[9] | Low  | Low                 | Limited for large Conv\
    \               | Basic CNNs     |\n| Ours [3]\u2013[5]      | High | Low    \
    \             | Limited for variable Conv Basic CNNs |                |\n| Ours\
    \ [10], [11]   | High | High                | Flexible                       \
    \      | DNNs           |\n| Ours [12]         | High | High                |\
    \ Flexible                             | DNNs+ViT       |\n\n\\*Basic CNNs contain\
    \ convolution and ReLU. DNNs further require residual connection and BN. And transformer\
    \ models further require GeLU and softmax.\n\n### *A. Motivation*\n\nWe refer\
    \ to the accumulation and activation module as the SC non-linear adder. Typical\
    \ SC Non-linear adders employ stochastic coding with FSM to implement different\
    \ activation functions [\\[6\\]](#page-4-3)\u2013 [\\[9\\]](#page-4-4). FSM-based\
    \ designs serially process stochastic bitstream inputs, which results in inaccurate\
    \ outputs (Figure [1\\)](#page-0-0) that do not utilize all of the information\
    \ in the inputs and have random fluctuations in the inputs themselves. Thus, very\
    \ long bitstreams, e.g., 1024 bits, are used for accuracy and lead to an unacceptable\
    \ latency, which severely affects the hardware efficiency.\n\n### <span id=\"\
    page-0-4\"></span>*B. Accurate End-to-End SC Acceleration with Sorting Network*\n\
    \nIn our work, we employ the deterministic thermometer coding scheme (Table [II\\\
    )](#page-1-0) and the corresponding accurate SC circuit designs to achieve accurate\
    \ end-to-end SC acceleration. With thermometer coding, all the 1s appear at the\
    \ beginning of the bitstream and\n\n![](_page_1_Figure_0.jpeg)\n\n<span id=\"\
    page-1-1\"></span>Fig. 3. (a) The truth table and circuit of ternary SC multiplier.\
    \ (b) The BSN and the selective interconnect system for accumulation and activation\
    \ function.\n\n<span id=\"page-1-0\"></span>TABLE II THE CORRESPONDING BINARY\
    \ PRECISION AND THE REPRESENTED RANGE FOR THERMOMETER CODING OF DIFFERENT BSL.\n\
    \n| BSL | Binary<br>Precision | Range             | Thermometer Coding       \
    \                                                      |\n|-----|---------------------|-------------------|--------------------------------------------------------------------------------|\n\
    | 2   | -                   | -1, 0, 1          | 00, 10, 11                 \
    \                                                    |\n| 4   | 2            \
    \       | -2, -1, 0, 1, 2   | 0000, 1000, 1100, 1110, 1111                   \
    \                                |\n| 8   | 3                   | -4, -3 \xB7\
    \ \xB7 \xB7 3, 4 | 00000000, 10000000 \xB7 \xB7 \xB7 11111110, 11111111      \
    \                              |\n| 16  | 4                   | -8, -7 \xB7 \xB7\
    \ \xB7 7, 8 | 0000000000000000, 1000000000000000 \xB7 \xB7 \xB7<br>1111111111111110,\
    \ 1111111111111111 |\n\na value x is represented with a L-bit sequence as x =\
    \ \u03B1x<sup>q</sup> = \u03B1( P<sup>L</sup>\u2212<sup>1</sup> <sup>i</sup>=0\
    \ x[i] \u2212 L 2 ), where x<sup>q</sup> = P<sup>L</sup>\u2212<sup>1</sup> <sup>i</sup>=0\
    \ x[i] \u2212 L 2 is the quantized value of range [\u2212 L , L ] and \u03B1 is\
    \ a scaling factor obtained by training.\n\n2 2 Deterministic coding, in contrast\
    \ to stochastic coding, achieves hardware-efficient and accurate computations\
    \ with shorter bitstreams. By employing a 2-bit ternary bitstream, we can realize\
    \ multiplication with only 5 gates using a deterministic multiplier (Figure [3\\\
    (](#page-1-1)a)).\n\nTo achieve accurate accumulation and activation functions\
    \ simultaneously, we employ the bitonic sorting network (BSN). BSN is a parallel\
    \ sorting network that sorts inputs in thermometer coding, ensuring the output\
    \ is also in thermometer coding. The sorting process, performed by comparators\
    \ constructed with AND and OR gates, follows Batcher's bitonic sorting algorithm\
    \ [\\[13\\]](#page-4-8) (Figure [3\\(](#page-1-1)b)). The number of 1's in the\
    \ sorted bitstream output from BSN corresponds to the sum of 1's in all input\
    \ bitstreams, effectively representing the accumulation result.\n\nBy sorting\
    \ all the bits, the inputs and outputs of the selective interconnect (SI) [\\\
    [14\\]](#page-4-9) are deterministic. Therefore, when the SI selects different\
    \ bits from the BSN directly as outputs based on the selection signals, a deterministic\
    \ input-output correspondence is generated and different activation functions\
    \ are realized. The example in Figure [3\\(](#page-1-1)b) implements the two-step\
    \ activation function shown at the bottom when the SI selects the 3rd and 6th\
    \ bits of the BSN as outputs. We refer interested readers to [\\[3\\]](#page-4-2),\
    \ [\\[4\\]](#page-4-10) for more details.\n\n### *C. Experimental Results*\n\n\
    We prototype the proposed SC accelerator with a 28-nm CMOS process. The chip's\
    \ measured current consumption and energy efficiency in Figure [4](#page-1-2)\
    \ show a peak of 198.9 TOPS/W at 200 MHz and 650 mV. Compared to state-of-the-art\
    \ binary-based NN processors [\\[15\\]](#page-4-11)\u2013[\\[19\\]](#page-4-12),\
    \ the fabricated SC-based NN processor achieves an average\n\n![](_page_1_Figure_10.jpeg)\n\
    \n<span id=\"page-1-2\"></span>Fig. 4. (a) Current and (b) energy efficiency versus\
    \ supply voltage at different working frequencies.\n\n![](_page_1_Figure_12.jpeg)\n\
    \n<span id=\"page-1-3\"></span>Fig. 5. Accuracy loss of the conventional binary\
    \ design and proposed SC design versus bit error rate, at the soft accuracy of\
    \ 98.28%.\n\nenergy efficiency improvement of 10.75\xD7 (1.16\xD7 \u223C 17.30\xD7\
    ). And the area efficiency improves by 4.20\xD7 (2.09\xD7 \u223C 6.76\xD7). We\
    \ also compare the accuracy under varying bit error rates (BER) using a ternary\
    \ neural network that achieves 98.28% accuracy on the MNIST dataset, as shown\
    \ in Figure [5.](#page-1-3) The proposed SC design demonstrates significant fault\
    \ tolerance, as the average reduction of accuracy loss by 70%. It is the first\
    \ silicon-proven end-to-end SC accelerator, to the best of the authors' knowledge.\n\
    \n## III. ACCURATE YET EFFICIENT SC WITH HIGH PRECISION RESIDUAL FUSION\n\nThe\
    \ SC accelerator above validated the effectiveness of deterministic thermometer\
    \ coding and the corresponding SC design on the basic small model (MNIST). In\
    \ this section, we propose SC-friendly models as well as new SC circuit blocks\
    \ to support SOTA model requirements and greatly improve the accuracy of the SC\
    \ accelerators.\n\n### *A. Motivation*\n\nThe SC TNN accelerator in Section [II](#page-0-3)\
    \ lacks support for batch normalization (BN) and residual connections, limiting\
    \ its accuracy on complex datasets like CIFAR10 or CIFAR100. Increasing precision\
    \ can enhance accuracy but compromises hardware efficiency. Figure [2](#page-0-1)\
    \ demonstrates that increasing BSL from 2 to 8 bits improves accuracy at the expense\
    \ of a 3 to 10 times efficiency overhead. Accurate yet efficient SC acceleration\
    \ is very challenging.\n\n### *B. SC-Friendly Low Precision Network*\n\nTo understand\
    \ the origin of the accuracy degradation, we quantize the network weight and activation\
    \ to low precision separately. Table [III](#page-2-0) shows similar accuracy between\
    \ low precision weight quantization and the floating point baseline, while 2b\
    \ BSL activation quantization results in a 10% accuracy drop. Hence, low precision\
    \ activation is the root cause of the accuracy loss due to its limited representation\
    \ capacity. After quantization, the range of activations is\n\n<span id=\"page-2-0\"\
    ></span>TABLE III NETWORK ACCURACY COMPARISON OF DIFFERENT QUANTIZED NETWORKS\
    \ ON CIFAR10.\n\n| Network              | Weight/BSL | Act/BSL | Top-1 Accuracy\
    \ (%) |\n|----------------------|------------|---------|--------------------|\n\
    | baseline             | FP         | FP      | 94.27              |\n| weight\
    \ quantized     | 2          | FP      | 93.98              |\n| activation quantized\
    \ | FP         | 2       | 84.18              |\n| fully quantized      | 2  \
    \        | 2       | 83.51              |\n|                      |          \
    \  |         |                    |\n\n![](_page_2_Figure_2.jpeg)\n\n<span id=\"\
    page-2-1\"></span>Fig. 6. High precision residual helps to achieve better representation\
    \ capability.\n\nreduced to {\u22121, 0, +1} for 2b BSL encoding, significantly\
    \ reducing the number of possible configurations.\n\nAs a remedy, we add the high-precision\
    \ activation input through residual connections to the result of the low-precision\
    \ convolution (Figure [6\\)](#page-2-1). By increasing the activation range to\
    \ {\u22128, \u22127, . . . , 7, 8}, we enhance representation capacity to 17<sup>H</sup>\xD7\
    W\xD7<sup>C</sup> . This significantly improves inference accuracy while maintaining\
    \ efficiency by preserving energy-efficient convolution computation.\n\n$$\\text{ReLU}(\\\
    text{BN}(x)) = \\begin{cases} \\gamma(x-\\beta) & x \\ge \\beta \\\\ 0 & x < \\\
    beta \\end{cases} \\tag{1}$$\n\nBesides the high precision residual, another remaining\
    \ question is how to efficiently process BN. And BN(x) = \u03B3(x \u2212 \u03B2\
    ), where \u03B3 and \u03B2 are trainable parameters. We propose to fuse BN with\
    \ the ReLU activation function as Equation [1.](#page-2-2) Consequently, we achieve\
    \ an SC-friendly low precision model with high precision residual fusion depicted\
    \ in Figure [6\\(](#page-2-1)b).\n\n### *C. End-to-End SC Accelerator with High\
    \ Precision Residual*\n\nCompared to the proposed accelerator in Section [II-B,](#page-0-4)\
    \ the model in Figure [6\\(](#page-2-1)b) further requires the implementation\
    \ of SC circuits for BN fusion and residual connection.\n\nThe above fused BN\
    \ and ReLU function can be efficiently and accurately processed in SC, leveraging\
    \ the selective interconnect described in Section [II-B.](#page-0-4) Figure [7](#page-2-3)\
    \ demonstrates how different BN parameters affect the objective function of the\
    \ SI.\n\n![](_page_2_Figure_11.jpeg)\n\n<span id=\"page-2-3\"></span>Fig. 7. BN-fused\
    \ activation function with 16b BSL output. The blue dots are\n\n![](_page_2_Figure_13.jpeg)\n\
    \n<span id=\"page-2-4\"></span>Fig. 8. (a) The proposed model optimization helps\
    \ to achieve much better inference accuracy; (b) 16b BSL residual achieves 5.78%\
    \ accuracy improvement, almost the same as floating point residual.\n\n<span id=\"\
    page-2-5\"></span>TABLE IV INFERENCE EFFICIENCY AND ACCURACY COMPARISON.\n\n|\
    \ W-A-R/BSL | Area (um\xB2) | ADP (um\xB2\xB7us) | Accuracy (%) |\n|-----------|------------|--------------|--------------|\n\
    | 2-2-2     | 4349.7     | 225.36       | 82.58        |\n| 2-4-4     | 10683.3\
    \    | 687.47       | 92.35        |\n| 2-2-16    | 4406.9     | 228.32      \
    \ | 92.01        |\n\nFor the accumulation of residual and multiplication products,\
    \ the different scaling factors \u03B1 of residual and convolution results can\
    \ lead to errors in the accumulation operation. The residual re-scaling block\
    \ is proposed to align the \u03B1 before accumulation. In the re-scaling block,\
    \ we multiply or divide the residual by a factor of 2 <sup>N</sup> (where N is\
    \ an integer). To multiply the residual by 2 <sup>N</sup> , we replicate it 2\
    \ N times in the buffer. For division by 2 <sup>N</sup> , we select 1 out of 2\
    \ bits of the residual per cycle and generate the final result after N cycles.\
    \ To maintain a constant BSL for the residual, we append 8 bits of '11110000'\
    \ (equal to 0) per division cycle.\n\n### <span id=\"page-2-2\"></span>*D. Experimental\
    \ Results*\n\nFigure [8](#page-2-4) demonstrate significant improvement in network\
    \ accuracy. With the high precision residual, network accuracy is improved significantly\
    \ by 8.69% and 8.12% for low precision ResNet18 on CIFAR10 and CIFAR100, respectively.\
    \ Combined with the novel training techniques, network accuracy can be improved\
    \ in total by 9.43% and 15.42%. Compared to baseline accelerators, it achieves\
    \ a 9.4% accuracy improvement with only a 1.3% efficiency overhead compared to\
    \ the efficient baseline and achieves a 3\xD7 efficiency improvement with comparable\
    \ accuracy to the accurate baseline design, as shown in Table [IV.](#page-2-5)\
    \ In this way, the proposed method achieves accurate yet efficient SC acceleration.\n\
    \n## IV. FLEXIBLE AND EFFICIENT SC ACCELERATOR WITH APPROXIMATE SPATIAL-TEMPORAL\
    \ BSN\n\nIn this section, we greatly improve the flexibility and hardware efficiency\
    \ of the SC accelerator by compressing the BSN.\n\n### *A. Motivation*\n\nBSN\
    \ accumulates all the input in parallel through sorting, so as to generate an\
    \ accurate output based on all the information input. However, it also forces\
    \ the hardware cost to increase super linearly with the accumulation widths (Figure\
    \ [9\\(](#page-3-0)a)). And the BSN has to support the largest accumulation widths\
    \ among all layers. The large BSN, however, leads to very high hardware redundancy\
    \ at shallow layers where the accumulation is always small (Figure [9\\(](#page-3-0)b)).\
    \ This makes our previous design still inefficient for SOTA models.\n\n![](_page_3_Figure_0.jpeg)\n\
    \n**Motivation**\n\n**Why End-to-End SC?**\n\n70.0 75.7 81.2\n\nAxPC +FSM\n\n\u2022\
    \ Stochastic encoding\n\nMUX +FSM\n\n**FSM-based Non-Linear Adder**\n\n**SI-based\
    \ Non-Linear Adder**\n\n\u2022 Deterministic thermometer coding: all 1s appearing\
    \ at the beginning of bitstream \u0D4C = \u123A\u2211 \u123E\u123F \u0B3F\u0B35\
    \n\n\u2022 **BSN** accumulates all the input bitstream simultaneously through\
    \ **sorting** \u2022 **SI** enables accurate and flexible\n\n\u0B40 \u0D46 /2)\n\
    \n**sorted**\n\n\u2022 Limited accuracy: random fluctuation\n\nArea Breakdown\n\
    \nAPC +FSM\n\n22.2 15.1 7.9\n\n7.8 9.1 10.9 7.6\n\n\u2022 Removing the conversion\
    \ logic \u2022 Higher energy/area efficiency \u2022 More tolerable to the soft\
    \ errors\n\n**Challenge: Non-Linear Addition** \u2022 Non-linear Add = accum +\
    \ activation \u2022 High hardware cost: ~90% area\n\n20.4\n\nBSN +SI\n\n**Previous\
    \ Works**\n\n72.0\n\nMultiplication Accumulation Activation\n\nArea Breakdown\n\
    \n17.0 20.4 30.9\n\n64 128 256 Accumulation width\n\n73.0 72.0 63.7\n\n10.0 7.6\
    \ 5.4\n\n<span id=\"page-3-0\"></span>Fig. 9. The inefficiency of the BSN design:\
    \ (a) BSN hardware cost increases significantly with the accumulation widths;\
    \ (b) ADP overhead using a large BSN for small accumulation widths.\n\n![](_page_3_Figure_2.jpeg)\n\
    \n<span id=\"page-3-1\"></span>Fig. 10. (a) Reducing BSN output BSL has little\
    \ effect on the accuracy of SI; (b) Parameterized BSN design space.\n\n### *B.\
    \ Approximate Spatial-Temporal Sorting Network*\n\nTo address the inefficiency\
    \ and inflexibility of BSN, we find a significant precision gap between the input\
    \ and output of SI, as revealed in Figure [6\\(](#page-2-1)b), making the high\
    \ precision SC input redundant. We reduce the BSN output BSL, resulting in a small\
    \ accuracy loss for the tanh function and negligible impact on the ReLU function,\
    \ as shown in Figure [10\\(](#page-3-1)a).\n\nTo further reduce hardware cost,\
    \ we adopt a progressive sorting and sub-sampling approach for the BSN. Figure\
    \ [10\\(](#page-3-1)b) presents a parameterized BSN design space that determines\
    \ the location, number of sampling times, and method of sampling. The parameterized\
    \ BSN consists of N stages and in the ith stage, there are m<sup>i</sup> sub-BSN\
    \ modules, each taking an input bitstream of li-bit BSL. Within each sub-BSN,\
    \ there is a sub-sampling block that implements truncated quantization. It clips\
    \ out c<sup>i</sup> bits on each end of the BSN while sampling 1 bit every s<sup>i</sup>\
    \ bit from the remaining. Considering the input distribution resembles a Gaussian\
    \ distribution with a small variance due to inputs from a large number of multipliers,\
    \ significant clipping can be performed with negligible errors, as illustrated\
    \ in Figure [11.](#page-3-2)\n\nThanks to the fact that the output BSL of the\
    \ approximate BSN is much shorter than the input, we can further fold the accumulation\
    \ temporally to achieve more flexibility. In this case, as shown in Figure [12,](#page-3-3)\
    \ a large BSN is implemented by multi-cycle reuse of a single small BSN circuit.\
    \ In the proposed spatial-temporal BSN architecture, the approximation level of\
    \ BSN, i.e., the BSL of partial sums, and its corresponding reuse can be controlled\
    \ through control signals. This allows for flexible handling of various accumulation\
    \ widths with different approximate configurations.\n\n### *C. Experimental Results*\n\
    \nFor the largest convolution in the ResNet18, the two proposed approximate BSN\
    \ reduced the ADP of BSN by 2.8\xD7 and 4.1\xD7 compared to the baseline, as shown\
    \ in Table [V.](#page-3-4) When handling the four different sizes of convolutions\
    \ in ResNet18, the spatial-temporal\n\n![](_page_3_Figure_10.jpeg)\n\n **Clipping**\
    \ some bits from both ends of the bitstream with **truncation error Scaling**\
    \ by taking 1bit for every bits with\n\nPartial sums are **distributed around\
    \ 0:** we can\n\n**Yixuan Hu1**, Tengyu Zhang1, Meng Li\\*213, Renjie Wei1, Liangzhen\
    \ Lai5, Yuan Wang13, Runsheng Wang134, Ru Huang134 *<sup>1</sup>School of Integrated\
    \ Circuits, <sup>2</sup>Institute for Artificial Intelligence & <sup>4</sup>Institute\
    \ of Electronic Design Automation, Peking University*\n\n**Observations of BSN**\n\
    \n\u2022 **Hardware cost increase super linearly**  with the input size (Accumulation\
    \ width) \u2022 Using a single large BSN for different layers\n\nincurs **very\
    \ high redundancy**\n\n\u2022 Exists a **huge precision redundancy**  between\
    \ the input and output for SI\n\nHardware redundancy\n\nAccumulation width\n\n\
    Sub-sampling BSN output as SI input ReLU: sampling has almost **no effect** on\
    \ accuracy\n\n576 1152 2304 4608 <sup>0</sup>\n\n**Spatial-Temporal Approximate\
    \ BSN Architecture**\n\n\u2022 Flexible approximation and reuse to **support**\
    \ \n\n**Experimental Results**\n\n\u2022 Find 47 pareto optimal in total 3362\
    \ design \u2022 **Flexible** choose suitable configuration\n\n> Total 3362 BSN\
    \ 128 input non-linear adder for ReLU\n\n\u2022 Reduce the ADP by 1.9x/1.9x/2.5x\
    \ for tanh/ sigmoid/ReLU with negligible errors\n\n\u2022 Spatial / spatial-temporal\
    \ approximate BSN reduce the ADP by 2.8x and **4.1x**\n\nLargest convolution (3x3x512)\
    \ in ResNet18\n\n**Large Convolution Comparison**\n\n**Neural Network Task Evaluation**\
    \ \u2022 Four scaled layers in ResNet18: spatialtemporal approximate BSN reduce\
    \ the ADP\n\nby **4.1~23.3x (average 8.47x)** \u2022 Also reduce the area of datapath\
    \ by 2.2x\n\n*We propose much more efficient and flexible BSN\u2010based non\u2010\
    linear adder design for end\u2010to\u2010end SC neural network acceleration*\n\
    \n47 pareto optimal\n\n1x103 1x104 1x105 1x106\n\n APC+FSM MUX+FSM Baseline BSN+SI\
    \ Design space Pareto front ADP / (um\xB2\uA78Fns)\n\nCycle9 Final BSN reuse\n\
    \n8 \\* 72b in 8 cycles 576b\n\n576b Appr. BSN Cycle2 reuse Cycle8 reuse 72b 72b\
    \ 72b\n\nSpatial-Temporal 4608to256 Accumulation in 9 cycles\n\n**256b** Reused\
    \ 576b BSN\n\n\u2026\u2026\n\n**different accumulation widths**\n\n**Opportunity\
    \ from approximation** \u2022 BSN output has fewer bits than input \u2022 Use\
    \ much smaller BSNs for later stages\n\n**Spatial-Temporal BSN Design** \u2022\
    \ **Fold the accumulation** temporally by reusing a single BSN over **multiple\
    \ cycles**\n\n576b\n\n576b 576b\n\n**4608b**\n\n**Design Space Exploration**\n\
    \n1x10-4\n\n1x10-3\n\nComputation error (MSE)\n\n1x10-2\n\n**Approximate Spatial-Temporal\
    \ Sorting Network**\n\n**Efficient Non-Linear Adder for Stochastic Computing with**\
    \ \n\n*<sup>3</sup>Beijing Advanced Innovation Center for Integrated Circuits*\
    \  \\*meng.li@pku.edu.cn *<sup>5</sup>Meta*\n\nADP / (um\xB2\uA78Fns)\n\n> 0.03\
    \ 0.04 0.05 0.06 0.07\n\nComputing error\n\n**Why BSN Ineffective?** \u2022 Completely\
    \ **parallel** sorting\n\nAccumulation width\n\n1 1/2 1/4 1/6 1/8\n\nbitstreams\
    \ to **reduce costs**\n\n\u2022 Sub-sample methods trade off\n\nclip a lot with\
    \ negligible errors\n\n**quantization error**\n\n**Parameterized Approximation**\n\
    \n\u2022 Approximate BSN: sorts and sub-sample the\n\n\u2022 Root: BSN recursively\
    \ sorts all input\n\nSub-sample ratio\n\n**Sub-Sample the BSN**\n\n576 1152 2304\
    \ 4608 <sup>215</sup>\n\n tanh ReLU\n\n<span id=\"page-3-2\"></span>\u2022 Number\
    \ of sub-samples? \u2022 Where to sub-sample? Fig. 11. Input distribution of the\
    \ intermediate sub-sampling blocks in different stages of the BSN provides an\
    \ opportunity to reduce the BSN via clipping.\n\n![](_page_3_Figure_12.jpeg)\n\
    \n<span id=\"page-3-3\"></span>Fig. 12. Spatial-temporal BSN architecture with\
    \ an example: a 576-bit BSN is reused over 9 cycles for 4608b accumulation.\n\n\
    <span id=\"page-3-4\"></span>TABLE V PERFORMANCE OF DIFFERENT DESIGNS FOR A 3X3X512\
    \ CONVOLUTION.\n\n| Design                        | Area (um\xB2) | Delay (ns)\
    \ | ADP (um\xB2\xB7ns) | MSE       |\n|-------------------------------|------------|------------|--------------|-----------|\n\
    | Baseline BSN                  | 2.95\xD7105   | 4.33       | 1.26\xD7106   \
    \  | -         |\n| Spatial Appr. BSN             | 1.32\xD7105   | 3.36     \
    \  | 4.55\xD7105     | 3.79\xD710\u22127 |\n| Spatial-Temporal<br>Appr. BSN |\
    \ 8.18\xD7103   | 1.92       | 3.06\xD7105*    | 3.79\xD710\u22127 |\n\n\\*Spatial-temporal\
    \ BSN considers 19\xD7 area to achieve the same throughput.\n\n![](_page_3_Figure_17.jpeg)\n\
    \n<span id=\"page-3-5\"></span>Fig. 13. (a) ADP and (b) MSE comparison on 4 size\
    \ of layers in ResNet18.\n\nBSN needs fewer cycles for smaller convolutions and\
    \ achieved ADP reductions from 8.2\xD7 to 23.3\xD7 with negligible errors, as\
    \ shown in Figure [13.](#page-3-5) On average, the spatial-temporal BSN reduces\
    \ the 2.2\xD7 area of datapath by reducing the average ADP of BSN by 8.5\xD7.\
    \ This shows that the proposed SC design is more flexible and efficient.\n\n###\
    \ V. SUMMARY AND FUTURE WORK\n\nIn this paper, we review our recent works on end-to-end\
    \ SC neural network acceleration. [\\[4\\]](#page-4-10) implemented a parallel\
    \ fully SCbased TNN processor using deterministic thermometer encoding and sorting\
    \ networks on the MNIST, achieving energy efficiency of 198.9 TOPS/W. In addition,\
    \ [\\[10\\]](#page-4-5) propose SC-friendly models with highprecision residual\
    \ fusion and corresponding SC circuits to greatly improve the network accuracy.\
    \ [\\[11\\]](#page-4-6) further proposed a more flexible and efficient spatial-temporal\
    \ approximate BSN, enabling accurate, efficient, and flexible end-to-end SC acceleration.\
    \ In future work, we explore SOTA transformer acceleration based on end-to-end\
    \ stochastic computing, which has been submitted [\\[12\\]](#page-4-7).\n\n###\
    \ REFERENCES\n\n- <span id=\"page-4-0\"></span>[1] W. Romaszkan *et al.*, \"ACOUSTIC:\
    \ Accelerating Convolutional Neural Networks through Or-Unipolar Skipped Stochastic\
    \ Computing,\" in *Design, Automation & Test in Europe Conference & Exhibition\
    \ (DATE)*, 2020, pp. 768\u2013773.\n- [2] W. Romaszkan *et al.*, \"A 4.4\u2013\
    75-TOPS/W 14-nm Programmable, Performance- and Precision-Tunable All-Digital Stochastic\
    \ Computing Neural Network Inference Accelerator,\" *IEEE Solid-State Circuits\
    \ Letters*, vol. 5, pp. 206\u2013209, 2022.\n- <span id=\"page-4-2\"></span>[3]\
    \ Y. Zhang *et al.*, \"When sorting network meets parallel bitstreams: A fault-tolerant\
    \ parallel ternary neural network accelerator based on stochastic computing,\"\
    \ in *Design, Automation & Test in Europe Conference & Exhibition (DATE)*. IEEE,\
    \ 2020, pp. 1287\u20131290.\n- <span id=\"page-4-10\"></span>[4] Y. Hu *et al.*,\
    \ \"A 28-nm 198.9-TOPS/W Fault-Tolerant Stochastic Computing Neural Network Processor,\"\
    \ *IEEE Solid-State Circuits Letters*, vol. 5, pp. 198\u2013201, 2022.\n- <span\
    \ id=\"page-4-1\"></span>[5] Y. Zhang *et al.*, \"Accurate and Energy-Efficient\
    \ Implementation of Non-Linear Adder in Parallel Stochastic Computing using Sorting\
    \ Network,\" in *IEEE International Symposium on Circuits and Systems (ISCAS)*,\
    \ 2020, pp. 1\u20135.\n- <span id=\"page-4-3\"></span>[6] K. Kim *et al.*, \"\
    Dynamic energy-accuracy trade-off using stochastic computing in deep neural networks,\"\
    \ in *Proceedings of the 53rd Annual Design Automation Conference*, 2016, pp.\
    \ 1\u20136.\n- [7] J. Li *et al.*, \"Towards acceleration of deep convolutional\
    \ neural networks using stochastic computing,\" in *2017 22nd Asia and South Pacific\
    \ Design Automation Conference (ASP-DAC)*. IEEE, 2017, pp. 115\u2013120.\n- [8]\
    \ Z. Li *et al.*, \"HEIF: Highly efficient stochastic computing-based inference\
    \ framework for deep neural networks,\" *IEEE Transactions on Computer-Aided Design\
    \ of Integrated Circuits and Systems*, vol. 38, no. 8, pp. 1543\u20131556, 2018.\n\
    - <span id=\"page-4-4\"></span>[9] J. Li *et al.*, \"Hardware-driven nonlinear\
    \ activation for stochastic computing based deep convolutional neural networks,\"\
    \ in *2017 International Joint Conference on Neural Networks (IJCNN)*, 2017, pp.\
    \ 1230\u20131236.\n- <span id=\"page-4-5\"></span>[10] Y. Hu *et al.*, \"Accurate\
    \ yet Efficient Stochastic Computing Neural Acceleration with High Precision Residual\
    \ Fusion,\" in *Design, Automation & Test in Europe Conference & Exhibition (DATE)*,\
    \ 2023.\n- <span id=\"page-4-6\"></span>[11] Y. Hu *et al.*, \"Efficient Non-Linear\
    \ Adder for Stochastic Computing with Approximate Spatial-Temporal Sorting Network,\"\
    \ in *ACM/IEEE Design Automation Conference (DAC)*, 2023.\n- <span id=\"page-4-7\"\
    ></span>[12] Y. Hu *et al.*, \"ASCEND: Accurate yet Efficient End-to-End Stochastic\
    \ Computing Acceleration of Vision Transformer,\" in *submitted*.\n- <span id=\"\
    page-4-8\"></span>[13] K. E. Batcher, \"Sorting networks and their applications,\"\
    \ in *Proceedings of the April 30\u2013May 2, 1968, spring joint computer conference*,\
    \ 1968, pp. 307\u2013314.\n- <span id=\"page-4-9\"></span>[14] S. Mohajer *et\
    \ al.*, \"Routing magic: Performing computations using routing networks and voting\
    \ logic on unary encoded data,\" in *Proceedings of the 2018 ACM/SIGDA International\
    \ Symposium on Field-Programmable Gate Arrays*, 2018, pp. 77\u201386.\n- <span\
    \ id=\"page-4-11\"></span>[15] J. Lee *et al.*, \"UNPU: A 50.6 TOPS/W unified\
    \ deep neural network accelerator with 1b-to-16b fully-variable weight bit-precision,\"\
    \ in *2018 IEEE International Solid-State Circuits Conference-(ISSCC)*. IEEE,\
    \ 2018, pp. 218\u2013220.\n- [16] J. Song *et al.*, \"7.1 An 11.5 TOPS/W 1024-MAC\
    \ butterfly structure dualcore sparsity-aware neural processing unit in 8nm flagship\
    \ mobile SoC,\" in *2019 IEEE International Solid-State Circuits Conference-(ISSCC)*.\
    \ IEEE, 2019, pp. 130\u2013132.\n- [17] C.-H. Lin *et al.*, \"7.1 A 3.4-to-13.3\
    \ TOPS/W 3.6 TOPS dual-core deeplearning accelerator for versatile AI applications\
    \ in 7nm 5G smartphone SoC,\" in *2020 ieee international solid-state circuits\
    \ conference-(isscc)*. IEEE, 2020, pp. 134\u2013136.\n- [18] F. Tu *et al.*, \"\
    Evolver: A deep learning processor with on-device quantization\u2013voltage\u2013\
    frequency tuning,\" *IEEE Journal of Solid-State Circuits*, vol. 56, no. 2, pp.\
    \ 658\u2013673, 2020.\n- <span id=\"page-4-12\"></span>[19] H. Mo *et al.*, \"\
    9.2 A 28nm 12.1 TOPS/W dual-mode CNN processor using effective-weight-based convolution\
    \ and error-compensation-based prediction,\" in *2021 IEEE International Solid-State\
    \ Circuits Conference (ISSCC)*, vol. 64. IEEE, 2021, pp. 146\u2013148."
  paper_id: 15
  scores:
    originality:
      raw: 4.5
      reason: The paper presents a novel approach by applying LLMs to the domain of
        architecture specification in VLSI design, which has not been extensively
        explored before.
    technical_soundness:
      raw: 4.0
      reason: The methodology appears robust, but further details on implementation
        and evaluation metrics would strengthen the technical foundation.
    clarity:
      raw: 3.8
      reason: While the paper is generally well-structured, some sections could benefit
        from clearer explanations of complex concepts and terminology.
    significance:
      raw: 4.2
      reason: The research addresses a critical gap in the automation of architecture
        specification, potentially impacting the efficiency of IC design processes.
    relevance:
      raw: 4.5
      reason: The topic is highly relevant to current trends in AI and chip design,
        aligning well with ongoing research in both fields.
  total_score: 4.15
  tier: B
- title: "HOPE: Holistic STT-RAM Architecture Exploration Framework for Future\n \
    \ Cross-Platform Analysis"
  abstract: 'Ensuring predictability in modern real-time Systems-on-Chip (SoCs) is
    an

    increasingly critical concern for many application domains such as automotive,

    robotics, and industrial automation. An effective approach involves the

    modeling and development of hardware components, such as interconnects and

    shared memory resources, to evaluate or enforce their deterministic behavior.

    Unfortunately, these IPs are often closed-source, and these studies are limited

    to the single modules that must later be integrated with third-party IPs in

    more complex SoCs, hindering the precision and scope of modeling and

    compromising the overall predictability. With the coming-of-age of open-source

    instruction set architectures (RISC-V) and hardware, major opportunities for

    changing this status quo are emerging. This study introduces an innovative

    methodology for modeling and analyzing State-of-the-Art (SoA) open-source SoCs

    for low-power cyber-physical systems. Our approach models and analyzes the

    entire set of open-source IPs within these SoCs and then provides a

    comprehensive analysis of the entire architecture. We validate this methodology

    on a sample heterogenous low-power RISC-V architecture through RTL simulation

    and FPGA implementation, minimizing pessimism in bounding the service time of

    transactions crossing the architecture between 28% and 1%, which is

    considerably lower when compared to similar SoA works.'
  url: http://arxiv.org/abs/2401.15639v2
  keywords: "*\u2014Heterogeneous SoC, Cyber-Physical-Systems, Timing Predictable\
    \ Architectures, Open-Source Hardware."
  document: "# TOP: Towards Open & Predictable Heterogeneous SoCs\n\nLuca Valente,\
    \ Francesco Restuccia, Davide Rossi, *Member, IEEE* Ryan Kastner, *Fellow, IEEE*\
    \ Luca Benini, *Fellow, IEEE*\n\n**Abstract**\u2014Ensuring predictability in\
    \ modern real-time Systems-on-Chip (SoCs) is an increasingly critical concern\
    \ for many application domains such as automotive, robotics, and industrial automation.\
    \ An effective approach involves the modeling and development of hardware components,\
    \ such as interconnects and shared memory resources, to evaluate or enforce their\
    \ deterministic behavior. Unfortunately, these IPs are often closed-source, and\
    \ these studies are limited to the single modules that must later be integrated\
    \ with third-party IPs in more complex SoCs, hindering the precision and scope\
    \ of modeling and compromising the overall predictability. With the coming-of-age\
    \ of open-source instruction set architectures (RISC-V) and hardware, major opportunities\
    \ for changing this status quo are emerging. This study introduces an innovative\
    \ methodology for modeling and analyzing State-of-the-Art (SoA) open-source SoCs\
    \ for low-power cyber-physical systems. Our approach models and analyzes the entire\
    \ set of open-source IPs within these SoCs and then provides a comprehensive analysis\
    \ of the entire architecture. We validate this methodology on a sample heterogenous\
    \ low-power RISC-V architecture through RTL simulation and FPGA implementation,\
    \ minimizing pessimism in bounding the service time of transactions crossing the\
    \ architecture between 28% and 1%, which is considerably lower when compared to\
    \ similar SoA works.\n\n\u2726\n\n**Index Terms**\u2014Heterogeneous SoC, Cyber-Physical-Systems,\
    \ Timing Predictable Architectures, Open-Source Hardware.\n\n# **1 INTRODUCTION**\n\
    \nThe exponential growth of cyber-physical systems (CPS) (e.g., self-driving cars,\
    \ autonomous robots, ...) and related applications has been fueled by the increase\
    \ in computational capabilities of heterogeneous low-power Systems-on-Chip (SoCs).\
    \ These SoCs are complex computing platforms composed of a set of different hardware\
    \ computing units (e.g., CPUs, hardware accelerators), each tailored to a specific\
    \ target application, sharing a set of resources (memory, sensors) through interconnects\
    \ [\\[1\\]](#page-12-0)\u2013[\\[5\\]](#page-12-1). While integrating multiple\
    \ computing units on the same platform has enabled efficient scale-up of computational\
    \ capabilities, it also poses significant challenges when it comes to assessing\
    \ their *timing predictability*, which is a requirement for CPSs dealing with\
    \ real-time and safety-critical applications: the primary challenge arises from\
    \ resource contentions that emerge when multiple active agents within the SoC\
    \ must access the same shared resources [\\[1\\]](#page-12-0)\u2013[\\[7\\]](#page-12-2).\n\
    \nNumerous research efforts have focused on enhancing the timing predictability\
    \ of heterogeneous Systems-on-Chip (SoCs). This includes safely upper bounding\
    \ execution times for data transfers [\\[8\\]](#page-13-0)\u2013[\\[10\\]](#page-13-1)\
    \ or the deadline miss ratio for critical tasks [\\[1\\]](#page-12-0)\u2013[\\\
    [3\\]](#page-12-3), with the smallest possible pessimism. These efforts have predominantly\
    \ focused on modeling and analyzing commercial DDR protocols [\\[8\\]](#page-13-0),\
    \ memory IPs [\\[11\\]](#page-13-2), and memory controllers [\\[12\\]](#page-13-3),\
    \ but also predictable interconnects [\\[1\\]](#page-12-0), [\\[4\\]](#page-12-4)\
    \ and on-chip communication protocols [\\[13\\]](#page-13-4). Regrettably, despite\
    \ their value, these studies are scattered, with each one focusing on only one\
    \ of these resources at a time, resulting in being overly pessimistic [\\[5\\\
    ]](#page-12-1).\n\nModeling and analysis of communication protocols are done speculatively\
    \ on abstract models, thus reducing their real-world applicability. Recent works\
    \ for modeling and analysis of IPs (memories, memory controllers, interconnect,\
    \ etc.) have to address the unavailability of cycle-accurate RTL descriptions.\
    \ Many of these IPs are either entirely closed-source [\\[8\\]](#page-13-0) or\
    \ provide loosely-timed behavioral models [\\[5\\]](#page-12-1), [\\[12\\]](#page-13-3)\
    \ or just \xB5architectural descriptions [\\[1\\]](#page-12-0), [\\[3\\]](#page-12-3),\
    \ [\\[4\\]](#page-12-4). In essence, the fragmented and proprietary nature of\
    \ commercial and research IPs restricts studies to the particular IP, greatly\
    \ reducing the accuracy achievable through systemlevel analysis. For example,\
    \ Restuccia et al. in [\\[9\\]](#page-13-5) bound the access times of multiple\
    \ initiators on FPGA reading and writing from/to the shared DDR memory. The proposed\
    \ upper bounds' pessimism is between 50% and 90%: even though they finely modeled\
    \ and analyzed the proprietary interconnect, the authors did not have access to\
    \ its RTL nor to the memory controller and IP. The same applies to Ditty [\\[10\\\
    ]](#page-13-1), which is a predictable cache coherence mechanism. In Ditty, even\
    \ though the caches' timing is finely modeled, the overall execution time can\
    \ be up to 3\xD7 bigger than the theoretical upper bounds, as the authors did\
    \ not model other components. Another example is AXI-ICRT [\\[1\\]](#page-12-0),\
    \ an advanced AXI interconnect with a sophisticated scheduler which allows transaction\
    \ prioritization based on importance. While proposing a highly advanced interconnect\
    \ with a\n\n<sup>\u2022</sup> *Luca Valente, Luca Benini, and Davide Rossi are\
    \ with the Department of Electrical, Electronic and Information Engineering, University\
    \ of Bologna, 40136 Bologna, Italy. Luca Benini is also with the Integrated Systems\
    \ Laboratory (IIS), ETH Z \xA8urich, 8092 Z \xA8urich, Switzerland.*\n\n<sup>\u2022\
    </sup> *Francesco Restuccia and Ryan Kastner are with the Computer Science and\
    \ Engineering, University of California at San Diego, San Diego, CA 92093 USA.*\n\
    \n*This work was supported by Technology Innovation Institute, Secure Systems\
    \ Research Center, Abu Dhabi, UAE, PO Box: 9639, by the Spoke 1 on Future HPC\
    \ of the Italian Research Center on High-Performance Computing, Big Data and Quantum\
    \ Computing (ICSC) funded by MUR Mission 4 - Next Generation EU, and by the European\
    \ Project EuroHPC JU The European Pilot (g.a. 101034126), and by KDT TRISTAN project\
    \ (g.a.101095947).*\n\n<span id=\"page-1-0\"></span>![](_page_1_Figure_0.jpeg)\n\
    \nFig. 1: Proposed methodology.\n\ntightly coupled model, the authors do not extend\
    \ the model to the other components of the SoC, even when assessing the deadline\
    \ miss ratio and benchmarking the architecture.\n\nThe emergence of open-source\
    \ hardware creates a major opportunity for building accurate end-to-end models\
    \ for realtime analysis of cutting-edge heterogeneous low-power SoCs [\\[14\\\
    ]](#page-13-6)\u2013[\\[16\\]](#page-13-7): the openness of the IPs allows for\
    \ cycle-accurate analysis of the whole architecture from the interconnects to\
    \ the shared resources. Yet, investigations and successful demonstrations in this\
    \ direction are still scarce, primarily because open hardware has only very recently\
    \ reached the maturity and completeness levels required to build full heterogeneous\
    \ SoCs [\\[17\\]](#page-13-8). In this context, this is the first work to bridge\
    \ the gap between open-source hardware and timing analysis, demonstrating a methodology\
    \ that successfully exploits the availability of the source code to provide finegrained\
    \ upper bounds of the system-level data transfers. We leverage a set of open-source\
    \ IPs from the PULP family, one of the most popular open-hardware platforms proposed\
    \ by the research community [\\[14\\]](#page-13-6), [\\[18\\]](#page-13-9).\n\n\
    Figure [1](#page-1-0) shows the proposed methodology, highlighting the novel contributions\
    \ in yellow. It consists of (i) a model for standalone IPs composing modern heterogeneous\
    \ lowpower SoCs, (ii) a static analysis of the RTL code of such components, and\
    \ (iii) a compositional mathematical analysis of the whole system to upper bound\
    \ the response time of the interactions between managers (initiators) and shared\
    \ subordinates (targets), considering the maximum interference generated by the\
    \ interfering managers. Figure [1](#page-1-0) highlights the differences between\
    \ the proposed methodology and previous studies also based on a static and compositional\
    \ approach [\\[5\\]](#page-12-1), [\\[7\\]](#page-12-2), [\\[9\\]](#page-13-5).\
    \ While previous works typically focus on one IP at a time [\\[9\\]](#page-13-5),\
    \ or rely on loosely-timed models [\\[5\\]](#page-12-1), thereby limiting the\
    \ overall accuracy, our approach is the first to model and analyze all the IPs\
    \ directly from the RTL source code to build a holistic system-level analysis.\
    \ This limits the proposed upper bounds' pessimism between 28% and just 1%, in\
    \ isolation and under interference, which is considerably lower when compared\
    \ to similar SoA works for closedsource or loosely-timed platforms [\\[1\\]](#page-12-0)\u2013\
    [\\[4\\]](#page-12-4), [\\[8\\]](#page-13-0), [\\[10\\]](#page-13-1), as better\
    \ detailed in Section [7.](#page-11-0) We demonstrate our methodology on a completely\
    \ open-source prototype of a heterogeneous lowpower open-source SoC for embedded\
    \ systems composed of a Linux-capable host core, a parallel accelerator, a set\
    \ of IOs, and on-chip and off-chip memories. the system-level analysis of the\
    \ architecture. Finally, Section 2-banks 512KiB SPM\n\nThe manuscript is organized\
    \ as follows: Section [2](#page-1-1) presents the target open-source RISC-V-based\
    \ SoC architecture, and Section [3](#page-2-0) discusses the model we apply to\
    \ its different components. Section [4](#page-3-0) analyzes the components to\
    \ specialize the generic model to each of them, and Section [5](#page-7-0) provides\n\
    \n<span id=\"page-1-2\"></span>![](_page_1_Figure_6.jpeg)\n\nFig. 2: Sample architecture.\n\
    \n[6](#page-9-0) validates the results with cycle-accurate experiments (on simulation\
    \ and FPGA), Section [7](#page-11-0) compares this work with the SoA. Section\
    \ [8](#page-12-5) concludes the manuscript.\n\n# <span id=\"page-1-1\"></span>**2\
    \ ARCHITECTURE**\n\nFig. [2](#page-1-2) shows the architectural template we target.\
    \ It also shows the four classes of hardware modules we identify in the architecture\
    \ under analysis, namely (i) *controllers*, (ii) the main *crossbar*, (iii) *bridges*,\
    \ and (iv) *peripherals*, which we model in the next Section. The architecture\
    \ leverages a set of fully open-source PULP IPs [\\[18\\]](#page-13-9). It is\
    \ based on Cheshire [\\[15\\]](#page-13-10), an open-source host platform consisting\
    \ of an RV64 Linuxcapable CPU, a set of commodity IOs (SPI, SDIO, UART, ...),\
    \ and an AXI-based crossbar with a configurable number of subordinate and manager\
    \ ports for easy integration of accelerators and resources. Our platform includes\
    \ a parallel accelerator and a low-power lightweight HyperBUS memory controller\
    \ [\\[19\\]](#page-13-11), connected to the crossbar.\n\nThe host CPU is CVA6\
    \ [\\[20\\]](#page-13-12), which is a six stages, singleissue, in-order, 64-bit\
    \ Linux-capable RISC-V core, supporting the RV64GC ISA variant, SV39 virtual memory\
    \ with a dedicated Memory Management Unit (MMU), three levels of privilege (Machine,\
    \ Supervisor, User), and PMP [\\[21\\]](#page-13-13). CVA6 features private L1\
    \ instruction and caches, operating in parallel, with the latter being able to\
    \ issue multiple transactions. When needed, CVA6 can offload computationintensive\
    \ tasks to the parallel hardware accelerator, the socalled PULP cluster [\\[22\\\
    ]](#page-13-14). It is built around 8 CV32E4-based cores [\\[23\\]](#page-13-15)\
    \ sharing 16\xD78 kB SRAM banks, composing a 128 kB L1 Scratchpad Memory (SPM).\
    \ The cluster features a DMA to perform data transfers between the private L1SPM\
    \ and the main memory: data movement is performed via softwareprogrammed DMA transfers.\
    \ Once the data are available inside the L1SPM, the accelerator starts the computation.\n\
    \nCVA6 and the cluster are the managers of the systems connected to the main AXI\
    \ crossbar [\\[24\\]](#page-13-16), which routes their requests to the desired\
    \ subordinates according to the memory map. A manager can access any subordinate\
    \ in the system. The main subordinates of the systems are, respectively, (i) the\
    \ on-chip SRAM memory, (ii) the IO subsystem, and (iii) the off-chip main memory\
    \ with a tightly coupled Last Level Cache (LLC). The on-chip memory is used for\
    \ low-latency, high-bandwidth data storage. The APB subsystem is used to communicate\
    \ with off-chip sensors or memories through the\n\ncommodity IOs. The off-chip\
    \ main memory is where the code and the shared data are stored. Differently from\
    \ high-end embedded systems relying on relatively power-hungry and expensive DDR3/4/5\
    \ memories, the platform under analysis adopts HyperRAMs as off-chip main memory,\
    \ which are fully-digital low-power small-area DRAMs with less than 14 IO pins\
    \ and that provide enough capacity to boot Linux [\\[16\\]](#page-13-7) and bandwidth\
    \ for IoT applications [\\[19\\]](#page-13-11), [\\[25\\]](#page-13-17).\n\n#\
    \ <span id=\"page-2-0\"></span>**3 MODEL**\n\nThis section presents the model\
    \ we construct for the different components of our SoC. Our aim is to propose\
    \ a general model that describes the characteristics of the components and that\
    \ can be re-targeted to different IPs and novel architectures, regardless of the\
    \ number of integrated controllers and peripherals. This work is also an effort\
    \ to provide base support to stimulate further studies in predictability improvements\
    \ and analysis for open hardware architectures.\n\n#### **3.1 Communication model**\n\
    \nWe identify four classes of hardware modules in the architecture under analysis,\
    \ shown in Fig. [2,](#page-1-2) namely (i) *controllers*, (ii) the main *crossbar*,\
    \ (iii) *bridges*, and (iv) *peripherals*. As the AXI standard is the main communication\
    \ standard used to implement non-coherent on-chip communications [\\[24\\]](#page-13-16),\
    \ we discuss here its main features. It defines a managersubordinate interface\
    \ enabling simultaneous, bi-directional data exchange and multiple outstanding\
    \ transactions. Fig. [3](#page-2-1) shows the AXI channel architecture and information\
    \ flow. Bus transactions are initiated by a *controller* (exporting a manager\
    \ interface), submitting a transaction request to read/write data to/from a subordinate\
    \ interface through AR or AW channels, respectively. A request describes the starting\
    \ target address and a *burst length*. After the request phase, in case of a read,\
    \ data are transmitted through the R channel. In case of a write, data are provided\
    \ by the *controller* to the target *peripheral* through the W channel. Upon completing\
    \ a write transaction, the *peripheral* also sends a beat on the B channel to\
    \ acknowledge the transaction's completion. For multiple in-flight write transactions,\
    \ the standard enforces strict in-order access to the W channel: the data on the\
    \ W channel must be propagated in the same order as the AW channel requests. Even\
    \ though the standard does not require it, many commercial and open-source platforms\
    \ apply the same policy for reads, typically to limit the system's overall complexity,\
    \ as reported in their documentation [\\[26\\]](#page-13-18), [\\[27\\]](#page-13-19).\n\
    \n#### **3.2 Controller model**\n\n*Controllers* have an active role on the bus.\
    \ Each *controller* exports an AXI manager interface, through which it initiates\
    \ requests for bus transactions directed to the *peripherals*. A generic *controller*\
    \ C<sup>i</sup> can be described through two parameters: the maximum number of\
    \ outstanding read/write transactions that it can issue in parallel, denoted with\
    \ \u03D5 C<sup>i</sup> R/W , and their relative burst length \u03B2<sup>i</sup>\
    \ . While our model and analysis can be applied to a generic architecture, the\
    \ system under analysis features as *controllers* a CVA6 core [\\[20\\]](#page-13-12)\
    \ and a cluster accelerator [\\[22\\]](#page-13-14) (see Section [2\\)](#page-1-1).\
    \ Bus transactions issued by the cluster interfere with those issued by CVA6\n\
    \n<span id=\"page-2-1\"></span>\n\n| READ TRANSACTION                        \
    \ |                     |  |  |  |  |\n|------------------------------------------|---------------------|--|--|--|--|\n\
    | 1 AR (ADDR+ LEN)                         | SUBORDINATE         |  |  |  |  |\n\
    | 2 R (DATA + COMPL.)                      | INTERFACE           |  |  |  |  |\n\
    | WRITE TRANSACTION<br>1<br>AW (ADDR+ LEN) |                     |  |  |  |  |\n\
    | W (DATA)<br>2                            | SUBORDINATE         |  |  |  |  |\n\
    |                                          | INTERFACE           |  |  |  |  |\n\
    |                                          | 3<br>B (COMPLETION) |  |  |  |  |\n\
    \nFig. 3: AXI Channel architecture\n\nand vice-versa. CVA6 is assumed to compute\
    \ a critical periodic workload, running on top of a Real-time Operating System\
    \ (RTOS). The PULP cluster executes computationintensive tasks and issues bus\
    \ transactions through its DMA. Contention internal to the PULP cluster has been\
    \ profiled in detail in [\\[28\\]](#page-13-20). However, our analysis provides\
    \ the worstcase data transfer time in accessing the shared *peripherals* to support\
    \ the safe scheduling and execution of critical tasks within their deadline. We\
    \ specifically focus on interference in accessing the shared resources. Modeling\
    \ the internal effects of *controllers*, such as pipeline stalls in the core or\
    \ contention within the accelerator, is beyond the scope of this work.\n\n####\
    \ <span id=\"page-2-2\"></span>**3.3 Peripheral model**\n\n*Peripherals* export\
    \ a *subordinate* interface through which they receive and serve the bus transactions.\
    \ The *peripherals* deployed in the system are heterogeneous. Nonetheless, our\
    \ model offers a set of parameters representative of a generic peripheral, and\
    \ it is not tied to a specific communication protocol. It works as the baseline\
    \ for the analysis of any *peripheral* deployed in the system under analysis.\
    \ The generic *peripheral* P<sup>j</sup> is characterized with two sets of parameters:\
    \ (i) the maximum number of supported outstanding reads (\u03C7 P<sup>j</sup>\
    \ <sup>R</sup> ) and write (\u03C7 P<sup>j</sup> <sup>W</sup> ) transactions;\
    \ (ii) the maximum number of cycles incurred from the reception of the request\
    \ to its completion, for a read (d P<sup>j</sup> <sup>R</sup> ) and a write (d\
    \ P<sup>j</sup> <sup>W</sup> ) transaction in isolation. d P<sup>j</sup> <sup>R</sup>\
    \ and d P<sup>j</sup> <sup>W</sup> are composed of two contributions: (i) the\
    \ *data time*, defined as the time required for the *peripheral* to send or receive\
    \ one word of data (tDATA) multiplied by the burst length of the transaction in\
    \ service (\u03B2i) and (ii) the *control overhead* tCTRL, defined as the maximum\
    \ time elapsing between accepting the request and the availability of the first\
    \ word of data (reads) or availability to receive data (writes). From the previous\
    \ considerations, d P<sup>j</sup> R/W = t P<sup>j</sup> CTRL + t P<sup>j</sup>\
    \ DATA \xB7 \u03B2. We define two extra parameters \u03C1 <sup>P</sup><sup>j</sup>\
    \ and \u03B8 P<sup>j</sup> . The first indicates the level of pipelining in serving\
    \ multiple transactions. \u03C1 <sup>P</sup><sup>j</sup> = 1 means that each stage\
    \ of P<sup>j</sup> does not stall the previous, and transactions are served in\
    \ a pipelined fashion, while \u03C1 <sup>P</sup><sup>j</sup> = 0 indicates that\
    \ no pipeline is implemented. \u03B8 <sup>P</sup><sup>j</sup> = 0 indicates that\
    \ read and write transactions interfere with each other. \u03B8 <sup>P</sup><sup>j</sup>\
    \ = 1 indicates that read and write transactions can be handled in parallel by\
    \ P<sup>j</sup> .\n\n#### <span id=\"page-2-3\"></span>**3.4 Main crossbar model**\n\
    \nWe provide here the model of the main *crossbar*, the routing component enabling\
    \ communication among *controller*s and *peripheral*s. Each *controller* has its\
    \ manager port connected to a subordinate port of the *crossbar*. Each *peripheral*\
    \ has its subordinate port connected to a manager port of the *crossbar*. We model\
    \ the *crossbar* R<sup>0</sup> with two sets of parameters: (i) the\n\nmaximum\
    \ amount of outstanding read and write transactions that a subordinate port can\
    \ accept (\u03C7 R<sup>0</sup> <sup>R</sup> and \u03C7 R<sup>0</sup> <sup>W</sup>\
    \ , respectively); and (ii) the maximum overall latency introduced by R<sup>0</sup>\
    \ on each read (d R<sup>0</sup> <sup>R</sup> ) and write transaction (d R<sup>0</sup>\
    \ <sup>W</sup> ). d R<sup>0</sup> R and d R<sup>0</sup> <sup>W</sup> are composed\
    \ of two contributions: (i) the overall delay introduced by the *crossbar* on\
    \ a transaction in isolation (tPROP); (ii) the maximum time a request is delayed\
    \ at the arbitration stage due to the contention generated by interfering transactions\
    \ (t R<sup>0</sup> CON). From the previous considerations, the propagation latency\
    \ is modeled as d R<sup>0</sup> R/W = t R<sup>0</sup> PROP + t R<sup>0</sup> CON.\
    \ Such parameters depend on the arbitration policies and routing mechanisms, as\
    \ we investigate in detail in Section [4.](#page-3-0)\n\n#### <span id=\"page-3-1\"\
    ></span>**3.5 Bridge model**\n\nBridges export a single manager interface and\
    \ a single subordinate interface. They perform protocol/clock conversion between\
    \ a *controller* and the *crossbar*. Bridges require a certain number of clock\
    \ cycles to be crossed but do not limit the number of in-flight transactions and\
    \ do not create any contention. We model the bridges with two parameters: the\
    \ overall maximum delay introduced over a whole transaction for (a) read (d Q<sup>j</sup>\
    \ <sup>R</sup> ) and (b) write (d Q<sup>j</sup> <sup>W</sup> ) transactions.\n\
    \n# <span id=\"page-3-0\"></span>**4 ANALYSIS OF THE HARDWARE MODULES**\n\nThis\
    \ Section aims to analyze the worst-case behavior of the *peripherals*, *bridges*,\
    \ and the *crossbar* present in the platform under analysis. Our approach is compositional\
    \ \u2013 in this Section, we analyze each hardware component separately, specializing\
    \ in the generic models introduced in Section [3,](#page-2-0) and bounding the\
    \ service times at the IP level in isolation. In the next Section, we provide\
    \ an overall worst-case analysis at the system level, in isolation and under interference.\
    \ We define t P<sup>j</sup> CK as the period period of the clock fed to P<sup>j</sup>\
    \ .\n\n#### <span id=\"page-3-4\"></span>**4.1 AXI CDC FIFO queues**\n\nAXI CDC\
    \ FIFOs are leveraged to perform clock-domain crossing between two AXI-based devices.\
    \ The generic AXI CDC FIFO F<sup>i</sup> is a *bridge*: we apply here the model\
    \ presented in Section [3.5.](#page-3-1) It exports a manager interface and a\
    \ subordinate interface. It is composed of five independent CDC FIFOs, each serving\
    \ as a buffer for an AXI channel, having depth D<sup>i</sup> CDC (design parameter\
    \ for the IP under analysis).\n\n#### *4.1.1 RTL IP structure*\n\nFigure [4](#page-4-0)\
    \ shows the block diagram of a CDC FIFO in the platform under analysis. They are\
    \ structured following established clock domain crossing (CDC) principles [\\\
    [24\\]](#page-13-16). The design is split into two parts, the transmitter (TX)\
    \ and the receiver (RX), having different clock domains. TX and RX interface through\
    \ asynchronous signals, namely a counter for data synchronization (synchronized\
    \ with two-stage Flip-Flops (FFs)) and the payload data signal.\n\n#### *4.1.2\
    \ Delays analysis*\n\nAs mentioned earlier, CDC FIFOs are *bridges*: we apply\
    \ the model presented in Section [3.5.](#page-3-1) The CDC FIFO under analysis\
    \ behaves as follows: TX samples the payload data into an FF. In the following\
    \ cycle, the TX counter is updated. The TX\n\ncrossing the CDC FIFO introduces\
    \ a fixed delay of one clock cycle of the TX domain (t CK) and four clock cycles\
    \ of the RX domain (t RX CK). This means that the delay in crossing the CDC FIFO\
    \ is equal to tCDC(t TX CK, tRX CK) = t TX CK + 4 \xB7 t RX CK.We leverage this\
    \ baseline delay to build the overall latency introduced by F<sup>i</sup> , interposed\
    \ between a manager (clocked at t C CK) and a subordinate (clocked at t P CK).\n\
    \n*Read transaction:* A read transaction AR<sup>k</sup> is composed of two phases:\
    \ (i) the address propagation phase and (ii) the data phase. This means that F<sup>i</sup>\
    \ is crossed twice to complete ARk: during phase (i), the manager is on the TX\
    \ side, propagating the request. In phase (ii), the subordinate is on the TX side,\
    \ propagating the data. Hence, the propagation latency is tCDC(t C CK, t<sup>P</sup>\
    \ CK) in phase (i) and tCDC(t P CK, t<sup>C</sup> CK) in phase (ii). Adding them\
    \ together, the propagation latency introduced by F<sup>i</sup> on AR<sup>k</sup>\
    \ is equal to:\n\n<span id=\"page-3-2\"></span>\n$$d\\_R^{\\rm CDC} = t\\_{\\\
    rm CDC}(t\\_{\\rm CK}^C, t\\_{\\rm CK}^P) + t\\_{\\rm CDC}(t\\_{\\rm CK}^P, t\\\
    _{\\rm CK}^C) = 5(t\\_{\\rm CK}^C + t\\_{\\rm CK}^P) \\tag{1}$$\n\n*Write transaction:*\
    \ A write transaction is composed of three phases: (i) an address phase (manager\
    \ on the TX side), (ii) a data phase (manager on the TX side), and (iii) a write\
    \ response phase (subordinate on the TX side). Phases (i) and (ii) happen in parallel\
    \ (see [\\[29\\]](#page-13-21) p. 45). Thus, tCDC(t C CK, t<sup>P</sup> CK) is\
    \ incurred for phases (i) and (ii), and tCDC(t P CK, t<sup>C</sup> CK) for phase\
    \ (iii). The delay introduced by F<sup>i</sup> on AW<sup>k</sup> is equal to the\
    \ delay introduced in Equation [1,](#page-3-2) d CDC <sup>W</sup> = d CDC <sup>R</sup>\
    \ .\n\n#### <span id=\"page-3-3\"></span>**4.2 AXI SRAM scratchpad memory (SPM)**\n\
    \nThe AXI SPM is a high-speed, low-latency memory component used for temporary\
    \ data storage \u2013 a block design representation is reported in Figure [5.](#page-4-0)\
    \ The SPM memory is a *peripheral*: we apply here the model presented in Section\
    \ [3.3.](#page-2-2)\n\n#### *4.2.1 RTL IP structure*\n\nThe first stage of the\
    \ SPM architecture is represented by a protocol converter (AXI-SRAM-Interface),\
    \ translating the read and write AXI channels into SRAM-compatible transactions.\
    \ Following the converter, an internal demux directs the SRAM transactions to\
    \ the desired SRAM bank, where the data is stored. Each SRAM bank provides two\
    \ independent SRAM ports, one for reads and one for writes, as from the specification\
    \ of industry-standard SRAM resources [\\[30\\]](#page-13-22).\n\n*The AXI-SRAM-Interface*\
    \ is structured in two submodules, independently managing read and write transactions.\
    \ The first stage of each submodule is a FIFO queue (of depth DSPM FIFO) buffering\
    \ the AXI AW or AR channel, respectively. Each submodule features the logic for\
    \ protocol translation, consisting of (i) saving transaction metadata (starting\
    \ address and length) and (ii) producing the output SRAM requests. For writes,\
    \ the incoming data on the W channel are directly propagated towards the banks.\
    \ The logic operating the protocol conversion generates the address for each W\
    \ beat. For reads, the data coming from the SRAM banks are directly driven on\
    \ the R channel. The logic keeps compliance with the AXI standard, adding the\
    \ last signal or generating write responses when required. *The demux* is fully\
    \ combinatorial\n\n<span id=\"page-4-0\"></span>![](_page_4_Figure_1.jpeg)\n\n\
    Fig. 4: CDC FIFO block diagram.\n\nFig. 5: AXI SPM block diagram\n\n0 1 N-1 ...\n\
    \nDual-Port SRAM Interface\n\nBank 1\n\nAR FIFO AW FIFO ADDR & REQ CTRL ADDR GEN\
    \ & CTRL\n\nAR R AW B W\n\nSRAM Interface\n\n0 1 N-1 ... Bank 0\n\nDual-Port SRAM\
    \ Interface\n\nCONVERTER\n\nDEMUXING\n\nBANKS\n\n![](_page_4_Figure_4.jpeg)\n\n\
    Fig. 6: IO subsystem block diagram.\n\nand selects the target bank according to\
    \ the request's address. *The SRAM banks* are technology-specific macros instantiated\
    \ at design time. Each SRAM bank's port exports an enable signal, an address signal,\
    \ and a signal to determine if a transaction is a read or a write. The SRAM interface\
    \ expects simultaneous propagation of data and commands for writes; for reads,\
    \ the data are sent the cycle following the command.\n\n#### *4.2.2 Delays and\
    \ parallelism analysis*\n\n*AXI-SRAM-Interface:* the FIFOs in the converter are\
    \ only in charge of data buffering \u2013 each FIFO introduces a fixed delay of\
    \ one clock cycle (t SPM CK ). After the FIFOs, the control logic requires at\
    \ most one clock cycle (t SPM CK ) to set up the propagation of a burst transaction\
    \ \u2013 the direct connection over the W and R channels makes the data streaming\
    \ in a pipeline fashion, adding no further latency. At the end of a write transaction,\
    \ the converter takes two clock cycles (2t SPM CK ) to generate the write response:\
    \ one to acknowledge that the last W beat has been accepted and one to provide\
    \ the B response. The same applies to reads, to generate the AXI last signal.\
    \ Summing up the contributions, the control latency introduced by the AXI-SRAM-Interface\
    \ to each transaction is upper bound by 4t SPM CK for both reads and writes.\n\
    \n*Demux:* The demultiplexing is combinatorial: it connects the transaction to\
    \ the SRAM bank in one clock cycle (t SPM CK ).\n\n*Banks:* As by the definition\
    \ of the SRAM interface [\\[30\\]](#page-13-22), an SRAM bank serves one transaction\
    \ per clock cycle, which makes t SPM DATA,R/W = t SPM CK . For write transactions,\
    \ the protocol guarantees that the SRAM bank samples the data in parallel with\
    \ the request (in the same clock cycle). For read transactions, the data are served\
    \ the clock cycle after the bank samples the request. So, it contributes to t\
    \ SPM CTRL,R with one clock cycle (t SPM CK ). Summing up the contributions, the\
    \ service time of the SPM in isolation is upper bound by:\n\n$$t\\_{\\rm{CTRL},W}^{\\\
    rm{SPM}} = 5 \\cdot t\\_{\\rm{CK}}^{\\rm{SPM}}; t\\_{\\rm{CTRL},R}^{\\rm{SPM}}\
    \ = 6 \\cdot t\\_{\\rm{CK}}^{\\rm{SPM}}; t\\_{\\rm{DATA},R/W}^{\\rm{SPM}} = t\\\
    _{\\rm{CK}}^{\\rm{SPM}}; \\tag{2}$$\n\nConsider now the parallelism supported\
    \ by the SPM. The maximum number of accepted outstanding transactions at the SPM\
    \ \u03C7 SPM <sup>R</sup> is defined by the depth DSPM FIFO of the input buffers\
    \ implemented in the AXI-SRAM-Interface. Thus,\n\n$$\n\\chi\\_R^{\\text{SPM}}\
    \ = \\chi\\_W^{\\text{SPM}} = D\\_{\\text{FIFO}}^{\\text{SPM}} \\tag{3}\n$$\n\n\
    The *SPM* module under analysis is aggressively pipelined, operations are executed\
    \ in one clock cycle, and no stall sources are present in the design. Also, as\
    \ mentioned earlier, read and write transactions do not interfere with each other.\
    \ From the previous considerations, \u03C1 SPM = 1 and \u03B8 SPM = 1.\n\n####\
    \ <span id=\"page-4-1\"></span>**4.3 IO Subsystem**\n\nSRAM Interface\n\nBank\
    \ B-1\n\n0 1 N-1 ...\n\nInterface\n\nThe IO subsystem is the *peripheral* in charge\
    \ of writing/reading data to/from the off-chip I/Os. We apply here the model presented\
    \ in Section [3.3.](#page-2-2) It is composed of a set of memory-mapped peripheral\
    \ registers that are accessed through a demux and that manage the datapaths issuing\
    \ the transactions on the I/O interfaces (e.g., SPI, I2C, etc.).\n\n#### *4.3.1\
    \ RTL IP structure*\n\nFigure [6](#page-4-0) shows the block diagram of the IO\
    \ subsystem. It is composed of an AXI-REG-Interface, a demux, and a set of registers.\
    \ The first stage of the *AXI-REG-Interface* is composed of two FIFOs (of depth\
    \ DIO FIFO), buffering read and write transactions, respectively. After the FIFOs,\
    \ a round-robin arbiter manages read and write transactions, allowing only one\
    \ at a time to pass to the protocol conversion. Since the IO subsystem is meant\
    \ for low-power reads and writes, registers' transactions share the same set of\
    \ signals for reads and writes and are limited to single-word accesses. For such\
    \ a reason, the IO subsystem does not support burst transactions (requests having\
    \ \u03B2<sup>i</sup> > 1 are suppressed). *The demux* stage decodes the request\
    \ and directs it to the proper register destination, where it is finally served\
    \ as a register read or write.\n\n#### *4.3.2 Delays and parallelism analysis*\n\
    \nThe IO subsystem is a *peripheral*, thus, we apply the model proposed in Section\
    \ [3.5.](#page-3-1) Considering the maximum service delays, overall, the IO subsystem\
    \ is composed of four stages: (i) the FIFOs, (ii) the protocol conversion, (iii)\
    \ demultiplexing, and (iv) target register access. The first three stages, contributing\
    \ to the control overhead, introduce a fixed delay of one clock cycle (t IO CK)\
    \ each for a total of 3 \xB7 t IO CK clock cycles. Consider now stage (iv). In\
    \ the case of a write, the request and the corresponding data are propagated in\
    \ parallel in one clock cycle. In the case of a read, the register provides the\
    \ data in the clock cycle following the request \u2013 t IO CTRL requires one\
    \ extra clock cycle. Summing all the contributions, the service time of the I/O\
    \ subsystem is upper bounded by:\n\n$$t\\_{\\rm{CTRL,W}}^{IO} = 3 \\cdot t\\_{\\\
    rm{CK}}^{IO}; \\quad t\\_{\\rm{CTRL,R}}^{IO} = 4 \\cdot t\\_{\\rm{CK}}^{IO}; \\\
    quad t\\_{\\rm{DATA,W/R}}^{IO} = t\\_{\\rm{CK}}^{IO} \\tag{4}$$\n\nConsider now\
    \ the parallelism. Similarly to the SPM module, the IO subsystem is capable of\
    \ buffering up to DIO FIFO of each type in its input FIFO queues. Thus, the maximum\
    \ number of outstanding transactions supported by the IO subsystem is equal to:\n\
    \n$$\n\\chi\\_W^{\\rm IO} = \\chi\\_R^{\\rm IO} = D\\_{\\rm FIFO}^{\\rm IO} \\\
    tag{5}\n$$\n\nThe IO subsystem serves read and write transactions one at a time,\
    \ and no pipelining is implemented among the different stages. This means that\
    \ \u03C1 IO = 0 and \u03B8 SPM = 0.\n\n<span id=\"page-5-0\"></span>![](_page_5_Figure_0.jpeg)\n\
    \nFig. 7: Block diagrams of the components of the main memory subsystem. (a) LLC\
    \ block diagram, (b) Transaction control flow diagram, (c) Memory controller block\
    \ diagram.\n\n#### <span id=\"page-5-1\"></span>**4.4 The main memory subsystem**\n\
    \nThe main memory subsystem is a *peripheral*: we apply here the model presented\
    \ in Section [3.3.](#page-2-2) It is composed of three macro submodules: (i) the\
    \ *AXI Last-level Cache (LLC)*; (ii) the *HyperRAM memory controller (HMC)*; and\
    \ (iii) the *HyperRAM memory (HRAM)*. It is based on HyperRAM memories leveraging\
    \ the HyperBUS protocol [\\[25\\]](#page-13-17). HyperRAMs are optimized for low-overhead\
    \ data storage while offering up to 3.2Gbps bandwidth. HyperRAMs expose a low\
    \ pin count, a fully digital 8-bit double-data-rate (DDR) interface used for commands\
    \ and data. HyperRAMs serve transactions in order, one at a time, as required\
    \ by the protocol [\\[25\\]](#page-13-17). While a pure in-order strategy is simpler\
    \ than those deployed by high-end commercial memory controllers, it is important\
    \ to note that these controllers are typically complex closedsource IPs, making\
    \ detailed analysis extremely challenging. Notably, our analysis is the first\
    \ to explore this level of detail. Furthermore, the memory subsystem under analysis\
    \ has shown to be effective in tape-outs of Linux-capable chips [\\[16\\]](#page-13-7).\
    \ We model the service times of a single transaction in case of an LLC hit and\
    \ miss. By doing so, we provide upper bounds that can be leveraged by future studies\
    \ focusing on LLC interference between different *controllers* at the application\
    \ level. For example, advanced cache management studies for real-time applications\
    \ (e.g., cache coloring) could leverage the upper bounds provided here to bound\
    \ overall task execution times.\n\n#### *4.4.1 RTL IP structure*\n\n*The AXI Last-Level\
    \ Cache* is the interface of the memory subsystem with the platform. The LLC under\
    \ analysis has configurable cache line length, defined as LWLLC. Figure [7\\(](#page-5-0)a)\
    \ shows the LLC's block diagram, composed of 5 pipelined units: (i) burst splitter,\
    \ (ii) hit-miss detection, (iii) eviction/refill, (iv) data read/write, and (v)\
    \ data ways. Figure [7\\(](#page-5-0)b) shows how these units cooperate to serve\
    \ the requests. The burst splitter buffers and splits the incoming AXI requests\
    \ into multiple sub-requests that have the same length of the cache line, and\
    \ it calculates the tags of the sub-transactions. A \u03B2i-word AXI burst request\
    \ is split internally into \u2308 \u03B2i LWLLC \u2309 requests of length LWLLC.\
    \ The tags are the input to the hitmiss detection unit, which analyzes them to\
    \ determine if any sub-request will be a (a) hit or (b) miss. In case (a), the\
    \ transaction is directed to the read/write unit: if it is a (a.i) read, the read\
    \ response is generated and immediately sent through the AXI subordinate port,\
    \ completing the transaction. In the case of a (a.ii) write, the locally cached\
    \ value is updated, and a write response is generated and sent back to the\n\n\
    AXI interface to complete the transaction. In case (b), the transaction is submitted\
    \ to the eviction/refill unit. Refill is performed on every miss and consists\
    \ of issuing a read to the memory controller to fetch the missing data and update\
    \ the data way. Eviction is performed when a cache set is full to free the necessary\
    \ spot before a refill. A Least Recently Used (LRU) algorithm is used in the module\
    \ under analysis.\n\n*The HyperRAM memory controller* [\\[31\\]](#page-13-23)\
    \ is depicted in Figure [7\\(](#page-5-0)c). It consists of two tightly coupled\
    \ modules working in two separated frequency domains: (i) the AXI *front-end*\
    \ and (ii) the *back-end* PHY controller. The front-end handles and converts the\
    \ AXI transactions into data packets for the PHY controller; it runs at the same\
    \ clock as the LLC (t HMC CK ). The back-end features a Finite State Machine (FSM)\
    \ to send/receive the data packets and keep compliance with the HyperBUS protocol\
    \ timings and data flow; it runs at the same clock as the HyperRAMs (t HRAM CK\
    \ ). The back-end handles two off-chip HyperRAMs in parallel, configured with\
    \ interleaved addresses. As each HyperRAM arranges data as 16-bit words, the word\
    \ size of the back-end is DWHYPER = 32 bits.\n\nThe first stage of the front-end\
    \ is composed of two FIFOs buffering incoming AXI read and write requests. Then,\
    \ a serializer solves conflicts among reads and writes, allowing only one AW or\
    \ AR request at a time. Following, three modules translate between AXI and the\
    \ back-end protocol: (i) AXTOPHY, translating the AXI AW or AR requests into commands\
    \ for the back-end; (ii) PHYTOR converting the data words from the back-end into\
    \ AXI read beats for the AXI interface; and (iii) WTOPHY, converting AXI W data\
    \ beats into data words and generating write response at the end of the transaction.\
    \ Three CDC FIFOs are deployed between the AXTOPHY, WTOPHY, and PHYTOR and the\
    \ back-end. The back-end deploys an internal FSM arranging the requests coming\
    \ from the front-end into 48-bit vector requests, as required in the HyperBUS\
    \ protocol, and propagating the data packets to/from the two physical HyperRAM\
    \ memories through two *transceivers* (TRX).\n\n*The HyperRAM memory* is an off-chip\
    \ memory IP [\\[25\\]](#page-13-17). It is provided with a cycle-accurate model,\
    \ fundamental for our analysis purposes [\\[32\\]](#page-13-24). Each HyperRAM\
    \ is organized as an array of 16-bit words and supports one outstanding burst\
    \ transaction, up to 1kB long. As two HyperRAM are interleaved, the overall burst\
    \ can be up to 2kB long [\\[19\\]](#page-13-11).\n\n#### *4.4.2 Delays and parallelism\
    \ analysis*\n\nWe now bound the worst-case service time of the main memory subsystem,\
    \ analyzing its components one at a time. Starting with the LLC, we follow the\
    \ control flow diagram\n\nreported in Figure [7\\(](#page-5-0)b) to guide the\
    \ explanation. The LLC collects the requests incoming to the main memory. Three\
    \ scenarios can happen: (i) LLC cache hit, (ii) LLC cache miss with refill, and\
    \ (iii) LLC cache miss with eviction and refill.\n\nIn case (i), the LLC directly\
    \ manages the request, and no commands are submitted to the HMC. The request proceeds\
    \ through the LLC splitter, hit/miss unit, read/write unit, and data way stages.\
    \ By design, each stage of the LLC requires a fixed number of clock cycles. The\
    \ burst splitter executes in one clock cycle (t LLC CK ). The hit/miss detection\
    \ stage takes two clock cycles (2t LLC CK ): one for tag checking and one to propagate\
    \ the request to the read/write unit or the evict/refill unit. The read/write\
    \ unit requires one clock cycle (t LLC CK ) to route the transaction to the data\
    \ ways. The data ways accept the incoming request in one clock cycle (t LLC CK\
    \ ) to then access the internal SRAM macros (same as the SPM, Section [4.2\\)](#page-3-3).\
    \ The internal SRAM takes one clock cycle to provide the read data (t LLC CK ),\
    \ but no further latency is required on writes. Once it gets the response, the\
    \ read/write unit routes the read channel to the AX interface, whereas it takes\
    \ one clock cycle (t LLC CK ) to generate the write B response at the end. Thus,\
    \ read/write unit and data ways take together three clock cycles (3t LLC CK ).\
    \ Summing up the contributions, the service time in case of a hit is upper bound\
    \ by:\n\n$$t\\_{\\rm{CTRL},\\rm{R}/\\rm{W}}^{\\rm{MS-HIT}} = 6 \\cdot t\\_{\\\
    rm{CK}}^{\\rm{LLC}}; \\quad t\\_{\\rm{DATA},\\rm{R}/\\rm{W}}^{\\rm{MS-HIT}} =\
    \ t\\_{\\rm{CK}}^{\\rm{LLC}}; \\tag{6}$$\n\nConsider now cases (ii) and (iii):\
    \ the eviction and refill stage is also involved, and a read (for refill) and,\
    \ optionally, a write (for eviction) is issued to the main memory. Eviction and\
    \ refill are run in parallel. Each operation performs two steps, each taking one\
    \ clock cycle: (a) generating a transaction for the main memory and (b) generating\
    \ a transaction for the data way. Thus, summing the latency introduced by the\
    \ eviction and refill stage (2t LLC CK ) with the ones from the other stages,\
    \ the LLC's contribution to the overall control time in case of a miss is upper\
    \ bound by:\n\n$$t\\_{\\text{CTRL,R/W}}^{\\text{LLC-MISS}} = t\\_{\\text{CTRL,R/W}}^{\\\
    text{MS-HTT}} + 2t\\_{\\text{CK}}^{\\text{LLC}} \\tag{7}$$\n\nConsider now the\
    \ delay introduced by the HMC on a generic request. Later, we will use it to bound\
    \ the service time for the batch of transactions issued by the LLC. As described\
    \ earlier, the HMC is composed of (a) the frontend, (b) the CDC FIFOs, and (c)\
    \ the back-end. Consider (a): each one of the front-end's submodules takes one\
    \ clock cycle to sample and process the transaction, except for the serializer,\
    \ which takes two. As transactions pass through 4 modules (FIFOs, serializer,\
    \ AXITOPHY, and either WTOPHY or PHYTOR), the overall delay contribution of the\
    \ front-end is equal to 5t HMC CK . Consider now (b): these are the CDC FIFOs\
    \ composing the AXI CDC FIFOs introduced in Section [4.1.](#page-3-4) For writes,\
    \ the transmitter (TX) is the front-end, sending data to the back-end from the\
    \ AXTOPHY and the WTOPHY. As both transfers happen in parallel, the delay introduced\
    \ by the CDC on a write is upper bound by tCDC(t HMC CK , tHRAM CK ). For reads,\
    \ first, the front-end transmits (TX) the AXTOPHY request, and then the back-end\
    \ transmits the data beats: the delay introduced by the CDC on a read is upper\
    \ bound by tCDC(t HMC CK , tHRAM CK ) + tCDC(t HRAM CK , tHMC CK ). Consider now\
    \ (c): the back-end's FSM parses the incoming request into a HyperRAM command\
    \ in one cycle (t HRAM CK ). Following this, an extra cycle is required for the\
    \ data to cross the back-end. Summing up the contributions just described, the\
    \ control time of the HMC on a generic transaction is upper bound by:\n\n$$\\\
    begin{aligned} t\\_{\\rm{CIRL},\\rm{R}}^{\\rm{HMC}} &= 5 \\cdot t\\_{\\rm{CK}}^{\\\
    rm{HMC}} + t\\_{\\rm{CIRC}} (t\\_{\\rm{CK}}^{\\rm{HMC}}, t\\_{\\rm{CK}}^{\\rm{HRM}})\
    \ + t\\_{\\rm{CIRC}} (t\\_{\\rm{CK}}^{\\rm{HRM}}, t\\_{\\rm{CK}}^{\\rm{HMC}})\
    \ + 2 \\cdot t\\_{\\rm{CK}}^{\\rm{HRM}} \\\\ t\\_{\\rm{CIRL},\\rm{W}}^{\\rm{HMC}}\
    \ &= 5 \\cdot t\\_{\\rm{CK}}^{\\rm{HMC}} + t\\_{\\rm{CDC}} (t\\_{\\rm{CK}}^{\\\
    rm{HMC}}, t\\_{\\rm{CK}}^{\\rm{HRM}}) + 2 \\cdot t\\_{\\rm{CK}}^{\\rm{HRM}} \\\
    end{aligned} \\tag{8}$$\n\nConsider now the delays introduced by the HyperRAM\
    \ memories on a generic request. The control overhead time to access the HyperRAM\
    \ memory is defined by the HyperBUS protocol [\\[25\\]](#page-13-17). First, the\
    \ 48-bit HyperRAM command vector is sent over the two memories in 3 \xB7 t HRAM\
    \ CK clock cycles, as the HyperBUS command bus is 16 bits. Following, the HyperBUS\
    \ provides a fixed latency for the maximum time to access the first data word,\
    \ accounting for refresh effects and crossing row boundaries. The specifications\
    \ [\\[33\\]](#page-13-25) bound such a delay between 7 and 16 clock cycles. In\
    \ our case, this is set to 12 \xB7 t HRAM CK . Thus, the total control latency\
    \ of the HyperRAM memory is upper bound by:\n\n$$t\\_{\\text{CTRL,R/W}}^{\\text{HRAM}}\
    \ = 15 \\cdot t\\_{\\text{CK}}^{\\text{HRAM}} \\tag{9}$$\n\nAt this point, data\
    \ are ready to be propagated. As the AXI domain and the HyperRAM have different\
    \ data widths, the number of cycles to send/receive an AXI word is:\n\n$$t\\_{\\\
    text{DATA,R}/\\text{W}}^{\\text{HRAM}} = DW\\_{\\text{HYPER}} \\cdot \\lceil \\\
    frac{DW\\_{\\text{AXI}}}{DW\\_{\\text{HYPER}}} \\rceil \\cdot t\\_{\\text{CK}}^{\\\
    text{HRAM}} \\tag{10}$$\n\nWe now have all the elements to bound the overall service\
    \ time of the whole main memory subsystem in case of a miss (ii) with refill and\
    \ (iii) eviction and refill. First, we bound the service time to serve a refill\
    \ (read) request. A \u03B2i-long transaction is split by the LLC into \u2308\u03B2\
    i/LWLLC\u2309 subtransactions to the memory, each LWLLC-long. Therefore, by multiplying\
    \ the control time of each sub-transaction (t HMC CTRL,R+ t HRAM CTRL,R) by the\
    \ number of transactions issued (\u2308 \u03B2i LWLLC \u2309), we bound the control\
    \ time introduced by the memory controller and the off-chip memories. To this,\
    \ we sum the control time of the LLC in case of a miss (t MS-MISS CTRL,W/R) and\
    \ obtain the whole control overhead. The same reasoning applies to the data time:\
    \ the total number of values requested by the LLC to the memory will be equal\
    \ to LWLLC \xB7 \u2308 <sup>\u03B2</sup><sup>i</sup> LWLLC \u2309 and the overall\
    \ time spent reading LWLLC \xB7 \u2308 <sup>\u03B2</sup><sup>i</sup> LWLLC \u2309\
    t HRAM DATA,R/W. It follows that the time to serve one word is LWLLC \u03B2i \xB7\
    \ \u2308 <sup>\u03B2</sup><sup>i</sup> LWLLC \u2309 \xB7 t HRAM DATA,R/W. Summing\
    \ it with the data time of the LLC (t MS-HIT DATA,R/W), we obtain the following\
    \ upper bounds for case (ii):\n\n<span id=\"page-6-0\"></span>\n$$\\begin{split}\
    \ t\\_{\\text{CTRL,R/W}}^{\\text{MS-MESS-REF}} &= t\\_{\\text{CTRL,R}}^{\\text{LLC-MESS}}\
    \ + \\left[ \\frac{\\beta\\_i}{LW\\_{\\text{LLC}}} \\right] \\cdot (t\\_{\\text{CTRL,R}}^{\\\
    text{HDAC}} + t\\_{\\text{CTRL,R}}^{\\text{HERAM}}); \\\\ t\\_{\\text{DATA,R/W}}^{\\\
    text{MS-MESS-REF}} &= t\\_{\\text{DATA,R/W}}^{\\text{MS-HIT}} + \\frac{LW\\_{\\\
    text{LLC}}}{\\beta\\_i} \\cdot \\left[ \\frac{\\beta\\_i}{LW\\_{\\text{LLC}}}\
    \ \\right] \\cdot t\\_{\\text{DATA,R}}^{\\text{HRAM}}; \\end{split} \\tag{11}$$\n\
    \nIf the eviction is also required, \u2308 \u03B2i LWLLC \u2309 extra write transactions\
    \ of length \u03B2<sup>i</sup> are performed to save the evicted data. Following\
    \ the same reasoning as earlier, this batch of transactions will introduce \u2308\
    \ \u03B2i LWLLC \u2309(t HMC CTRL,W + t HRAM CTRL,W) clock cycles to the control\
    \ time and LWLLC \u03B2i \xB7 \u2308 <sup>\u03B2</sup><sup>i</sup> LWLLC \u2309\
    \ \xB7t HRAM DATA,W to the data time. We sum these numbers to eq. [11](#page-6-0)\
    \ to upper bound the overall control and data time as follows:\n\n$$t\\_{\\text{CTIL,W/R}}^{\\\
    text{MS-MIS-REF-EV}} = t\\_{\\text{CTIL,W/R}}^{\\text{MS-MIS-REF}} + \\left[\\\
    frac{\\beta\\_i}{LW\\_{\\text{LLC}}}\\right] (t\\_{\\text{CTIL,W}}^{\\text{HMCC}}\
    \ + t\\_{\\text{CTIL,W}}^{\\text{HBM}});$$\n\n$$t\\_{\\text{DATA,W/R}}^{\\text{MS-MIS-REF-EV}}\
    \ = t\\_{\\text{DATA,W/R}}^{\\text{MS-MIS-REF}} + \\frac{LW\\_{\\text{LLC}}}{\\\
    beta\\_i} \\cdot \\left[\\frac{\\beta\\_i}{LW\\_{\\text{LLC}}}\\right] \\cdot\
    \ t\\_{\\text{DATA,W}}^{\\text{HBM}};$$\n\nConsider now the parallelism of the\
    \ main memory subsystem. This is defined by the LLC, which acts as an interface\
    \ with the rest of the platform, buffering up to DLLC FIFO read and write transactions.\
    \ This means that the maximum number of supported outstanding transactions is\
    \ as follows:\n\n$$\n\\chi\\_R^{MS} = \\chi\\_W^{MS} = D\\_{\\rm FIFO}^{\\rm LLC}\
    \ \\tag{13}\n$$\n\nThe LLC is pipelined: in the case all the enqueued accesses\
    \ are hits, there is no stalling. However, the memory controller handles only\
    \ one transaction at a time, stalling the preceding ones, and only serves one\
    \ read or one write at a time. Hence, as soon as one access is a miss, \u03C1\
    \ MS = 0 and \u03B8 MS = 0.\n\n#### <span id=\"page-7-2\"></span>**4.5 AXI host\
    \ crossbar**\n\nThe AXI host crossbar under analysis is a consolidated AXI crossbar\
    \ already validated in multiple silicon tapeouts [\\[16\\]](#page-13-7), [\\[15\\\
    ]](#page-13-10), [\\[24\\]](#page-13-16). We apply here the generic model for\
    \ the *crossbar* proposed in Section [3.4.](#page-2-3) The crossbar is referred\
    \ as R0.\n\n#### *4.5.1 RTL IP structure*\n\nAs detailed in Figure [8,](#page-7-1)\
    \ the crossbar exports a set of input subordinate ports (S) and output manager\
    \ ports (M). Each S port is connected to a demultiplexer, which routes the incoming\
    \ AW and AR requests and W data to the proper destination. Each M port is connected\
    \ to a multiplexer, which (i) arbitrates AW and AR requests directed to the same\
    \ *peripheral*, (ii) connects the selected W channel from the *controller* to\
    \ the *peripheral*, and (iii) routes back the R read data and B write responses.\
    \ The crossbar under analysis can be configured for a fully combinatorial (i.e.,\
    \ decoding and routing operations in one clock cycle) or pipelined structure with\
    \ up to three pipeline stages. In the platform under analysis, it is configured\
    \ to be fully combinatorial. ble of granting one AW and one AR request for each\
    \ clock CUT\n\n#### *4.5.2 Delays and parallelism analysis*\n\nTo analyze the\
    \ maximum propagation delays introduced by the crossbar, we upper bound the overall\
    \ latency on a transaction by combining the delays introduced on each AXI channel.\
    \ We provide two upper bounds, one for transactions in isolation (i.e., t R<sup>0</sup>\
    \ PROP,R/W as defined in Section [3\\)](#page-2-0) and the other for transactions\
    \ under contention (i.e., t R<sup>0</sup> PROP,R/W+t R<sup>0</sup> CON,R/W as\
    \ defined in Section [3\\)](#page-2-0). We will use both of them in our architectural\
    \ analysis reported in Section [5.](#page-7-0)\n\n*Maximum delays in isolation:*\
    \ Thanks to the combinatorial structure, it is guaranteed by design that a request\
    \ for a transaction, a data word, or a write response crosses the crossbar in\
    \ one clock cycle (t R<sup>0</sup> CK). Consider a whole AXI transaction. For\
    \ a read transaction, the crossbar is crossed twice: on the AR and R AXI channels,\
    \ respectively. For each AXI write transaction, the crossbar is crossed two times:\
    \ the first time is crossed by the AW and W beats (propagated in parallel), and\
    \ the second time by the B response. Thus, the propagation delays in isolation\
    \ are equal to:\n\n<span id=\"page-7-3\"></span>\n$$t\\_{\\text{PROP}\\text{R}/\\\
    text{W}}^{R\\_0} = 2 \\cdot t\\_{\\text{CK}}^{R\\_0};\\tag{14}$$\n\n*Maximum delays\
    \ under contention:* Under contention, multiple *controllers* connected to the\
    \ crossbar can attempt to concurrently send requests to the same *peripheral*,\
    \ generating interference. The arbiters deploy a round-robin scheme capa-\n\n\
    <span id=\"page-7-1\"></span>![](_page_7_Figure_12.jpeg)\n\nFig. 8: AXI Crossbar\
    \ block diagram\n\ncycle. In the worst-case scenario, the request under analysis\
    \ loses the round-robin and is served last, experiencing a delay of MR<sup>0</sup>\
    \ \u2212 1 clock cycles (with MR<sup>0</sup> the number of *controller* capable\
    \ of interfering with the request under analysis). From the previous considerations,\
    \ the maximum propagation time introduced by the crossbar is upper bound by:\n\
    \n<span id=\"page-7-4\"></span>\n$$t\\_{\\text{CON,R}}^{R\\_0} = t\\_{\\text{CON,W}}^{R\\\
    _0} = M\\_{R\\_0} - 1 \\tag{15}$$\n\nConsider now the parallelism. Concerning\
    \ reads, the crossbar does not keep track of the inflight transactions. To route\
    \ the responses back, it appends information to the AXI ID. Doing so does not\
    \ limit the maximum number of outstanding transactions. The behavior is different\
    \ for writes: AXI enforces a strict in-order execution of write transactions (see\
    \ [\\[29\\]](#page-13-21) p. 98). This requires the crossbar to implement a table\
    \ to know the order of granted transactions. The maximum number of outstanding\
    \ write transactions per S port is limited by the depth of such tables, refereed\
    \ as D R<sup>0</sup> TAB. From the previous consideration: \u03C7 R<sup>0</sup>\
    \ <sup>W</sup> = D R<sup>0</sup> TAB. In the architecture under analysis, \u03C7\
    \ R<sup>0</sup> <sup>W</sup> is set to be bigger than the parallelism supported\
    \ by the *peripherals* so that the crossbar does not limit the overall parallelism\
    \ of the system.\n\n# <span id=\"page-7-0\"></span>**5 SYSTEM-LEVEL WORST-CASE\
    \ RESPONSE TIME ANALYSIS**\n\nThis section introduces our system-level end-to-end\
    \ analysis to upper bound the overall response times of read and write transactions\
    \ issued by a generic *controller* and directed to a generic *peripheral*, considering\
    \ the maximum interference generated by the other *controllers* in the system.\
    \ Our approach is static [\\[34\\]](#page-13-26) and compositional [\\[35\\]](#page-13-27).\
    \ Specifically, we leverage the component-level static analysis introduced in\
    \ Section [4](#page-3-0) to then compose, step-by-step, the system-level worst-case\
    \ service time of transactions traversing the whole architecture.\n\nWe make an\
    \ assumption aligned with the SoA [\\[3\\]](#page-12-3), [\\[4\\]](#page-12-4),\
    \ [\\[8\\]](#page-13-0), [\\[11\\]](#page-13-2), [\\[12\\]](#page-13-3), [\\[36\\\
    ]](#page-13-28) to ensure independence among *peripherals* while not compromising\
    \ the generality of the analysis. It is assumed that multiple outstanding transactions\
    \ of the same type (either read or write) issued by the same *controller* target\
    \ the same *peripheral*: before issuing a transaction targeting a *peripheral*\
    \ P<sup>j</sup> , a *controller* completes the pending transactions of the same\
    \ type targeting a different *peripheral* Pz. Without such an assumption, due\
    \ to the strict ordering imposed by the AXI standard [\\[29\\]](#page-13-21) on\
    \ the W channel, and the structure of some *peripherals* generating interference\
    \ between reads and writes (i.e., \u03C1 <sup>P</sup><sup>j</sup> = 0), transactions\
    \ issued by C<sup>k</sup> and directed to P<sup>j</sup> might interfere with transactions\
    \ issued by C<sup>i</sup> and directed to Pz, if C<sup>i</sup> also issues in\
    \ parallel transactions\n\nto P<sup>j</sup> , and vice-versa. This assumption\
    \ allows us to relax our analysis, removing such pathological cases. It is worth\
    \ noticing that it does not enforce any relationship between read and write transactions.\
    \ Such an assumption can either be enforced at the software level or at the hardware\
    \ level. The results of our analysis can be extended to such corner cases if required.\
    \ We leave this exploration for future works.\n\nThe first step of the analysis\
    \ is to bound the overall response time of a transaction in isolation (Lemma [1\\\
    )](#page-8-0). Secondly, we bound the maximum number of transactions that can\
    \ interfere with a transaction under analysis, either of the same type (e.g.,\
    \ reads interfering with a read, Lemma [2\\)](#page-8-1) or of a different type\
    \ (e.g., write interfering with a read, and vice versa, Lemma [3\\)](#page-8-2).\
    \ Lemma [4](#page-8-3) bounds the maximum temporal delay each interfering transaction\
    \ can delay a transaction under analysis. Finally, Theorem [1](#page-9-1) combines\
    \ the results of all the lemmas to upper bound the overall worstcase response\
    \ time of a transaction under analysis under interference. We report the lemmas\
    \ in a general form. AXi,j can represent either a read or write transaction issued\
    \ by the generic *controller* C<sup>i</sup> and directed to the generic *peripheral*\
    \ P<sup>j</sup> . The *crossbar* is referred to as R0. To make our analysis general,\
    \ we assume that \u03A8<sup>j</sup> = [C0, ..., CM\u22121] is the generic set\
    \ of interfering *controllers* capable of interfering with C<sup>i</sup> issuing\
    \ transactions to P<sup>j</sup> and that that a generic set of *bridges* \u0398\
    <sup>i</sup> = {Q0, ..., Qq\u22121} can be present between each *controller* C<sup>i</sup>\
    \ and the crossbar R0. The cardinality of \u03A8<sup>j</sup> is referred to as\
    \ \f \u03A8<sup>j</sup> \f and corresponds to the number of *controllers* interfering\
    \ with AXi,j .\n\n<span id=\"page-8-0\"></span>**Lemma 1.** *The response time\
    \ in isolation of* AXi,j *is upper bounded by:*\n\n<span id=\"page-8-4\"></span>\n\
    $$d\\_{i,j}^{X} = d\\_{\\mathbb{R}\\mathcal{W}}^{P\\_j} + \\sum\\_{Q\\_l \\in\
    \ \\Theta\\_{i,j}} d\\_{\\mathbb{R}\\mathcal{W}}^{Q\\_l} + d\\_{\\mathbb{R}\\\
    mathcal{W}}^{R\\_0} \\tag{16}$$\n\n*Proof.* Section [4](#page-3-0) upper bounds\
    \ the worst-case delays in isolation introduced by each component in the platform.\
    \ According to their definition, such delays account for all of the phases of\
    \ the transaction. The components in the platform are independent of each other.\
    \ Thus, the delay introduced by each traversed component is independent of the\
    \ behavior of the other components. It derives that the overall delay incurred\
    \ in traversing the set of components between C<sup>i</sup> and P<sup>j</sup>\
    \ is upper bounded by the sum of the worst-case delays introduced by all of the\
    \ components in the set. Summing up the maximum delay introduced by the target\
    \ *peripheral* P<sup>j</sup> (d P<sup>j</sup> R/W), by the set of traversed *bridges*\
    \ \u0398<sup>i</sup> , and by the *crossbar* R<sup>0</sup> (d R<sup>0</sup> R/W),\
    \ the lemma derives.\n\n<span id=\"page-8-1\"></span>**Lemma 2.** *The maximum\
    \ number of transactions of the same type that can interfere with* AXi,j *is upper\
    \ bounded by:*\n\n$$S\\_{i,j}^X = \\min\\left(\\sum\\_{C\\_y \\in \\Psi\\_j} \\\
    phi\\_X^{C\\_y}, \\chi\\_X^{P\\_j} + \\mid \\Psi\\_j \\mid \\right) \\tag{17}$$\n\
    \n*Proof.* The min in the formula has two components. As from the AXI standard\
    \ definition, an interfering *controller* C<sup>k</sup> cannot have more than\
    \ \u03D5 C<sup>k</sup> <sup>X</sup> pending outstanding transactions. This means\
    \ that summing up the maximum number of outstanding transactions for each interfering\
    \ *controller* in \u03A8<sup>j</sup> provides an upper bound on the number of\
    \ transactions of the same type interfering with AXi,j \u2013 the left member\n\
    \nof the min derives. From our *peripheral* analysis reported in Section [4,](#page-3-0)\
    \ P<sup>j</sup> and R<sup>0</sup> can limit the maximum amount of transactions\
    \ accepted by the system: P<sup>j</sup> accepts overall at most \u03C7 P<sup>j</sup>\
    \ R/W transactions \u2013 when such a limit is reached, any further incoming transaction\
    \ directed to P<sup>j</sup> is stalled. After P<sup>j</sup> serves a transaction,\
    \ R<sup>0</sup> restarts forwarding transactions to the *peripheral* following\
    \ a round-robin scheme (see Section [4\\)](#page-3-0). In the worst-case scenario,\
    \ C<sup>i</sup> loses the round-robin arbitration against all of the \f \u03A8\
    <sup>j</sup> interfering *controllers* in \u03A8<sup>j</sup> , each ready to submit\
    \ an interfering request. Summing up the contributions, also \u03C7 P<sup>j</sup>\
    \ <sup>R</sup> + \f \u03A8<sup>j</sup> \f upper bounds the maximum number of transactions\
    \ interfering with AXi,j \u2013 the right member of the min derives. Both of the\
    \ bounds are valid \u2013 the minimum between them is an upper bound providing\
    \ the least pessimism \u2013 Lemma [2](#page-8-1) derives.\n\n<span id=\"page-8-2\"\
    ></span>**Lemma 3.** *The maximum number of transactions of a different type (represented\
    \ here as Y, i.e., write transactions interfering with a read under analysis,\
    \ and vice versa) interfering with* AXi,j *is upper bounded by:*\n\n$$U\\_{i,j}^{Y}\
    \ = (S\\_{i,j}^{X} + 1) \\cdot (1 - \\theta^{P\\_j}) \\tag{18}$$\n\n*Proof.* According\
    \ to Section [4.5,](#page-7-2) R<sup>0</sup> manages transactions of different\
    \ types independently \u2013 thus, no interference of this type is generated at\
    \ the R<sup>0</sup> level. From Section [3,](#page-2-0) \u03B8 <sup>P</sup><sup>j</sup>\
    \ = 1 represents the case in which the *peripheral* is capable of serving read\
    \ and write transactions in parallel (e.g., the SPM *peripheral*, Section [4.2\\\
    )](#page-3-3). Thus, no interference is generated among them \u2013 the second\
    \ equation derives. From Section [3,](#page-2-0) \u03B8 <sup>P</sup><sup>j</sup>\
    \ = 0 represents the case in which P<sup>j</sup> does not feature parallelism\
    \ in serving read and write transactions (i.e., also write transactions interfere\
    \ with reads, e.g., main memory subsystem, Section [4.4\\)](#page-5-1). Considering\
    \ lemma [2,](#page-8-1) at most S X i,j transactions of the same type can interfere\
    \ with AXi,j . With \u03B8 <sup>P</sup><sup>j</sup> = 0, and assuming a round-robin\
    \ scheme arbitrating between reads and writes at the *peripheral* level, each\
    \ one of the S X i,j interfering transaction of the same type can be preceded\
    \ by a transaction of the opposite type, which can, therefore, create interference.\
    \ The same applies to AXi,j , which can lose the arbitration at the *peripheral*\
    \ level as well. Summing up the contribution, it follows that S X i,j + 1 can\
    \ overall interfere with AXi,j \u2013 the first equation derives.\n\n<span id=\"\
    page-8-3\"></span>**Lemma 4.** *The maximum time delay that a transaction of any\
    \ kind* AXk,j *issued by the generic interfering* controller C<sup>k</sup> *can\
    \ cause on* AXi,j *is upper bounded by:*\n\n<span id=\"page-8-5\"></span>\n$$\n\
    \\Delta\\_{k,j} = d\\_{\\text{R\\\\$}\\mathcal{W}}^{R\\_0} + (1 - \\rho^{P\\_j})\
    \ \\cdot t\\_{\\text{CTRL,R\\\\$}\\mathcal{W}}^{P\\_j} + t\\_{\\text{DATT},\\\
    text{R\\\\$}\\mathcal{W}}^{P\\_j} \\cdot \\beta\\_k \\tag{19}\n$$\n\n*Proof.*\
    \ In traversing the path between C<sup>k</sup> and P<sup>j</sup> , AXk,j shares\
    \ a portion of the path with AXi,j , i.e., the target *peripheral* P<sup>j</sup>\
    \ and the crossbar R<sup>0</sup> \u2013 no *bridges* from \u0398<sup>k</sup> belongs\
    \ to the shared path, thus the delay propagation of AXk,j do not contribute in\
    \ delaying AXk,j . Considering the delay generated by AXk,j at R0, this is upper\
    \ bounded by d R<sup>0</sup> R/W in Section [3.4.](#page-2-3) As from Section\
    \ [3.3,](#page-2-2) t P<sup>j</sup> CTRL,R/W + t P<sup>j</sup> DATA,R/W \xB7 \u03B2\
    <sup>k</sup> is the maximum service time of P<sup>j</sup> for the transaction\
    \ AXk,j and upper bounds the maximum temporal delay that AXk,j can cause on AXi,j\
    \ at P<sup>j</sup> . As from the definition of an interfering transaction, AXk,j\
    \ is served by P<sup>j</sup> before AXi,j . As defined by the model in Section\
    \ [3.3,](#page-2-2) when \u03C1 <sup>P</sup><sup>j</sup> = 1, the *peripheral*\
    \ works in a pipeline fashion. This means that\n\nfor \u03C1 <sup>P</sup><sup>j</sup>\
    \ = 1, the control time t P<sup>j</sup> CTRL,R/W of an interfering transaction\
    \ is pipelined and executed in parallel with the transaction under analysis. Differently,\
    \ when \u03C1 <sup>P</sup><sup>j</sup> = 0, no pipeline is implemented, and the\
    \ control time of the interfering transaction can partially or totally interfere\
    \ with the transaction under analysis. From the previous considerations, the contribution\
    \ (1 \u2212 \u03C1 <sup>P</sup><sup>j</sup> )\xB7t P<sup>j</sup> CTRL,R/W + t\
    \ P<sup>j</sup> DATA,R/W \xB7 \u03B2<sup>k</sup> derives. Summing up the contributions,\
    \ the lemma follows.\n\n<span id=\"page-9-1\"></span>**Theorem 1.** *The overall\
    \ response time of* AXi,j *under the interference generated by the other* controllers\
    \ *in the system is upper bounded by:*\n\n<span id=\"page-9-3\"></span>\n$$H\\\
    _{i,j}^X = d\\_{i,j}^X + (S\\_{i,j}^X + U\\_{i,j}^Y) \\cdot \\Delta\\_{k,j} \\\
    tag{20}$$\n\n*Proof.* Summing up the contribution in isolation for AXi,j (Lemma\
    \ [1\\)](#page-8-0) with the sum of the maximum number of interfering transactions\
    \ of the same type (Lemma [2\\)](#page-8-1) and of a different type (Lemma [3\\\
    )](#page-8-2) multiplied by the maximum delay generated by each interfering transaction\
    \ (Lemma [4\\)](#page-8-3), Theorem [1](#page-9-1) derives.\n\nThe results presented\
    \ in this Section represent analytical upper bounds derived through static code\
    \ analysis and the formulation of mathematical proofs. Section [6](#page-9-0)\
    \ will validate them through a comprehensive set of cycle-accurate experiments\
    \ and measurements.\n\n# <span id=\"page-9-0\"></span>**6 EXPERIMENTAL VALIDATION**\n\
    \nThis Section describes the experimental campaign we conducted to validate the\
    \ methodology and models. The aim of the experimental campaign is to assess that\
    \ the results presented in the previous Sections correctly upper bound the maximum\
    \ delays and response times at the component level and the architectural level.\
    \ We follow a hierarchical approach: at first, Section [6.1](#page-9-2) aims to\
    \ validate the results at the component level we proposed in Section [4.](#page-3-0)\
    \ Following, in Section [6.2,](#page-10-0) we experimentally validate the system-level\
    \ analysis we proposed in Section [5.](#page-7-0) The experiments are conducted\
    \ in a simulated environment (leveraging the Siemens QuestaSIM simulator) and\
    \ by deploying the design on an FPGA platform. In the simulated experiments, we\
    \ deploy custom AXI managers for *ad-hoc* traffic generation and cycle-accurate\
    \ performance monitors. The generic custom manager represents a generic configurable\
    \ *controller* C<sup>i</sup> issuing requests for transactions \u2013 we will\
    \ refer to that as GC<sup>i</sup> . In the FPGA, we leverage CVA6 and the PULP\
    \ cluster to generate the traffic with synthetic software benchmarks and rely\
    \ on their performance-monitoring registers to collect the measurements. The experimental\
    \ designs are deployed on the AMD-Xilinx VCU118, using the Vitis 2022.1 toolchain.\
    \ Similar State-of-the-Art works upper bounding the execution time of a single\
    \ transaction leverage synthetic benchmarks to measure the worst-case access times\
    \ since generic applications fail to do so [\\[8\\]](#page-13-0)\u2013[\\[10\\\
    ]](#page-13-1). For this reason, we concentrate on synthetic benchmarks at the\
    \ IP and the system level.\n\n#### <span id=\"page-9-2\"></span>**6.1 Component-level\
    \ hardware modules**\n\n#### *6.1.1 Delays analysis*\n\nThis subsection presents\
    \ the tests run to measure the worstcase access latency time in isolation for\
    \ the *peripherals* (d P<sup>j</sup> R/W ), 10\n\nthe *crossbar* (d R<sup>0</sup>\
    \ R/W ) and the *bridges* (d Q<sup>j</sup> R/W ) from Section [4.](#page-3-0)\
    \ We connect the generic controller CG<sup>i</sup> to the IP under analysis for\
    \ these experiments. We let CG<sup>i</sup> issue 100'000 transactions, one at\
    \ a time, with random burst length (\u03B2i). We monitor the service times and\
    \ then pick the longest ones for different \u03B2<sup>i</sup> .\n\nFigure [9](#page-10-1)\
    \ compares the maximum measured experimental delays with the upper bound proposed\
    \ in Section [4.](#page-3-0) Figure [9\\(](#page-10-1)a) reports the maximum service\
    \ time of the main memory subsystem in case of a miss as a function of the burst\
    \ length of the transaction under analysis, either when (i) only a refill is necessary\
    \ and (ii) both refill and eviction are necessary, compared with the bounds proposed\
    \ in Section [4.4.](#page-5-1) The measured service times are lower than the bounds.\
    \ The pessimism is between 3% and 10.1%; the larger \u03B2<sup>i</sup> , the higher\
    \ the pessimism. Higher pessimism on longer transactions is due to the internal\
    \ splitting at the LLC. As from our analysis, the memory subsystem is not fully\
    \ pipelined (\u03C1MS = 0). However, in practice, the control and data phases\
    \ of consecutive sub-transactions might be partially served in parallel by the\
    \ LLC and the memory controller. This means that the longer the transaction, the\
    \ higher the number of sub-transactions and their overlap, and the lower the service\
    \ time compared to our model. Thus, the pessimism increases. Figure [9\\(](#page-10-1)b)\
    \ reports the measured results on the main memory subsystem but in case of a hit,\
    \ compared with the bounds proposed in Section [4.4.](#page-5-1) As we consider\
    \ an LLC hit, the access to the HyperRAM is not performed: this test analyzes\
    \ the service time of the LLC. Our bounds are always upper bounds for the maximum\
    \ measured results. The trend here is reversed w.r.t. Figure [9\\(](#page-10-1)a)\
    \ \u2013 as \u03B2<sup>i</sup> increases, the relative pessimism decreases from\
    \ 7.7% down to 0.4%. In this case, the source of the pessimism comes only from\
    \ the control time, which does not depend on \u03B2<sup>i</sup> , while there\
    \ is no pessimism on the data time. Hence, this pessimism gets amortized as the\
    \ burst length and the overall service time increase. We conduct the same experimental\
    \ campaign also on the AXI SPM \u2013 the measured results, compared with the\
    \ bounds proposed in Section [4.2,](#page-3-3) are reported in Figure [9\\(](#page-10-1)c).\
    \ The trends are similar to the ones reported in Figure [9\\(](#page-10-1)b) for\
    \ LLC hits \u2013 the pessimism of our analysis is limited to 1 and 2 clock cycles\
    \ for reads and writes on the control time, respectively. As in the case of the\
    \ LLC HITs, the upper bound on the control overhead gets amortized for longer\
    \ transactions, and the pessimism reduces from 8.8% to 0.5%.\n\nFigure [9\\(](#page-10-1)d)\
    \ reports the maximum measured latency to cross an AXI CDC FIFO as a function\
    \ of the manager clock period (the subordinate clock period is fixed to 30 ns)\
    \ and compared with the bounds proposed in Section [4.1.](#page-3-4) The results\
    \ are independent of the length of the transaction. To stimulate the highest variability,\
    \ the phases of the clocks are randomly selected on a uniform distribution. The\
    \ first bar reports the crossing delays from the manager to the subordinate side,\
    \ corresponding to the delays introduced on the AW, W, and AR AXI channels. The\
    \ second bar reports the crossing delays from the subordinate to the manager side,\
    \ corresponding to the overall delays on the AXI R and B channels. The third bar\
    \ shows the overall delay on a complete transaction, corresponding to the sum\
    \ of the two previously introduced contributions (see Section [4.1\\)](#page-3-4).\
    \ The pessimism of our bounds is, at most, one clock cycle of the slowest clock\
    \ between manager and subordinate.\n\n<span id=\"page-10-1\"></span>![](_page_10_Figure_0.jpeg)\n\
    \nFig. 9: Services time in isolation.\n\nFigure [9\\(](#page-10-1)e) reports the\
    \ measured propagation delays introduced by the crossbar over an entire write\
    \ and read transaction, compared with the bounds of Section [4.5,](#page-7-2)\
    \ varying the number of *controllers*. As explained in Section [4.5,](#page-7-2)\
    \ the propagation delay is the sum of the propagation latency without interference\
    \ (eq. [14\\)](#page-7-3) and the additional contention latency (eq. [15\\)](#page-7-4),\
    \ which depends on the number of *controllers*. Thanks to the simplicity of the\
    \ arbitration operated by the crossbar (pure round-robin), our proposed bounds\
    \ exactly match the measurements. We conducted the experimental campaign also\
    \ on the IO subsystem. We measured the maximum service time and compared it with\
    \ the upper bounds of Section [4.3,](#page-4-1) which we do not show for space\
    \ reasons: such IP supports only single-word transactions. Our upper bounds exceed\
    \ the maximum measured service time with pessimism of down to 2 clock cycles,\
    \ with service times of 4 (write) and 5 (read) clock cycles.\n\n#### *6.1.2 Parallelism*\n\
    \nWe also demonstrate our analysis of parallelism of the *peripherals* (\u03C7\
    \ P<sup>j</sup> R/W ) and the *crossbar* (\u03C7 R<sup>0</sup> R/W ) analyzed\
    \ in Section [4.](#page-3-0) To do so, we configured CG<sup>i</sup> to issue unlimited\
    \ outstanding transactions to the *peripheral* under test. In parallel, we monitor\
    \ the maximum number of accepted outstanding transactions. Our measurements match\
    \ our analysis: the maximum number of outstanding transactions is defined by the\
    \ maximum parallelism accepted at the input stage of the peripherals and the crossbar.\n\
    \n#### <span id=\"page-10-0\"></span>**6.2 System-level experiments**\n\nWhile\
    \ the previous experiments focused on the evaluation at the IP level, this set\
    \ of experiments aims to evaluate the system-level bounds proposed in Section\
    \ [5.](#page-7-0) We first validate our analysis in simulation. We developed a\
    \ System Verilog testbench with two configurable AXI synthetic *controllers* CG<sup>i</sup>\
    \ connected to the target architecture (see Figure [2\\)](#page-1-2) stimulating\
    \ overload conditions to highlight worst-case scenarios. We also validate our\
    \ results on FPGA, generating traffic with CVA6 and the PULP cluster.\n\nAt first,\
    \ we evaluate the results in isolation *at the system level* as a function of\
    \ the burst length, leveraging the same strategy used for the previous experiments.\
    \ Namely, these tests are meant to validate Lemma [1](#page-8-0) (eq. [16\\)](#page-8-4).\
    \ To measure the service time at the system level in isolation, we let one GC<sup>i</sup>\
    \ issue 100'000 transactions, one at a time, with different \u03B2<sup>i</sup>\
    \ , while the other GC<sup>k</sup> is inactive. We monitor the service times and\
    \ then pick the longest ones for each\n\n\u03B2i . Figures [10](#page-11-1) (a)\
    \ and (b) report the maximum measured system-level response times in isolation\
    \ for completing a transaction issued by the generic *controller* GC<sup>i</sup>\
    \ and directed to (a) the main memory subsystem (case of cache miss, causing either\
    \ refill or both refill and eviction) and (b) to the SPM memory, compared with\
    \ the bounds proposed in Lemma [1.](#page-8-0) The measured service times are\
    \ smaller than the bounds in all the tested scenarios. The measure and the trends\
    \ reported in Figure [10\\(](#page-11-1)a) are aligned with the ones found at\
    \ the IP level and reported in Figure [9\\(](#page-10-1)a). This is because the\
    \ overhead introduced by the crossbar (in isolation) and the CDC FIFOs is negligible\
    \ compared to the memory subsystem's service time. Figure [10\\(](#page-11-1)b)\
    \ shows a trend aligned with the results at the IP-level reported in Figure [9\\\
    (](#page-10-1)c): the lower \u03B2<sup>i</sup> , the higher the pessimism. It\
    \ is worth mentioning that the analysis shows higher pessimism at the system level\
    \ than at the IP level. This is due to the extra pessimism from the crossbar and\
    \ the CDC, which is nevertheless amortized on longer transactions, down to 1.9%.\n\
    \nWe now analyze the results under maximum interference, to verify the results\
    \ of Lemma [2](#page-8-1) and [3](#page-8-2) and Theorem [1.](#page-9-1) For these\
    \ tests, the execution of GC<sup>i</sup> (100'000 transactions, one at a time)\
    \ receives interference by *controller* GCk. \u03B2<sup>k</sup> is fixed and equal\
    \ to \u03B2<sup>i</sup> , while we vary the amount of outstanding transactions\
    \ GC<sup>k</sup> can issue (\u03D5 CG<sup>k</sup> R/W ). Figures [10](#page-11-1)\
    \ (c), (d), and (e) report the maximum measured systemlevel response times for\
    \ completing a transaction issued by the generic *controller* GC<sup>i</sup> and\
    \ directed to (c) the main memory with an LLC miss considering \u03B2<sup>i</sup>\
    \ = 16, (d) the SPM memory, considering \u03B2<sup>i</sup> = 16, and (e) the SPM\
    \ memory, considering \u03B2<sup>i</sup> = 256, and compare them with the upper\
    \ bounds proposed in equation [20.](#page-9-3) Figures [10](#page-11-1) (c), (d),\
    \ and (e) verify the results of Lemma [2](#page-8-1) and [3:](#page-8-2) when\
    \ \u03D5 CG<sup>k</sup> R/W > \u03C7MS R/W (two bars on the right), the total\
    \ service time is defined by the parallelism of the peripheral itself \u2013 as\
    \ expected, after saturating the number of interfering transactions accepted by\
    \ the peripheral, the measured results are the same regardless of the increase\
    \ of \u03D5 CG<sup>k</sup> R/W . Differently, when \u03D5 CG<sup>k</sup> R/W \u2264\
    \ \u03C7MS R/W , a reduced value of \u03D5 CG<sup>k</sup> R/W corresponds to lower\
    \ interference and response times. Figure [10\\(](#page-11-1)c) refers to the\
    \ case of an LLC miss under interference when \u03B2<sup>k</sup> = 16. The results\
    \ confirm the safeness of our analysis, which correctly upper bounds the overall\
    \ response times with a pessimism around 15%, which is slightly higher than the\
    \ pessimism of a transaction in isolation at the system level. As explained in\
    \ the previous subsection, when multiple transactions are enqueued, the memory\
    \ subsystem can partially serve their data and control\n\n12\n\n<span id=\"page-11-1\"\
    ></span>![](_page_11_Figure_1.jpeg)\n\nFig. 10: Services times under interference.\n\
    \nphases in parallel. However, our model only allows \u03C1MS = 1 or \u03C1MS\
    \ = 0, i.e., either the *peripheral* is fully pipelined or not pipelined at all.\
    \ Since \u03C1MS = 0, the pessimism is slightly higher when more transactions\
    \ are enqueued (and partially served in parallel) as equation [19](#page-8-5)\
    \ counts the service time of a transaction fully when \u03C1MS = 0. Varying \u03B2\
    <sup>k</sup> of GC<sup>k</sup> gives comparable results \u2013 we do not report\
    \ such results for briefness and lack of space. We provide two charts for the\
    \ SPM, in Figure [10\\(](#page-11-1)d) and Figure [10\\(](#page-11-1)e). The comparison\
    \ of the two charts highlights how the interfering transactions' length impacts\
    \ the analysis's pessimism, ranging between 19.7% for \u03B2 = 16 to 1% for \u03B2\
    \ = 256. The trend here is aligned with the service time at the system level in\
    \ isolation: the pessimism comes from the control times of SPM and propagation\
    \ latency of the crossbar and the CDC, which are amortized as the data time increases\
    \ with \u03B2k.\n\n#### **6.3 Discussion**\n\nIn this Section, we validated the\
    \ analysis of Sections [4](#page-3-0) and [5](#page-7-0) through an extensive\
    \ set of tests. We demonstrated how the proposed approach enables detailed explanations\
    \ of the analysis's pessimism and facilitates iterative refinement. This allows\
    \ us to derive upper bounds that are safe yet not overly pessimistic, particularly\
    \ when compared to similar stateof-the-art works based on closed-source or loosely-timed\
    \ IPs. Nevertheless, while the methodology is promising, the resulting analysis\
    \ may seem limited in comparison to other works that model more sophisticated\
    \ closed-source IPs. Here, we discuss the limitations of our analysis, focusing\
    \ on its dependence on the underlying characteristics of the available open-source\
    \ hardware.\n\nIt is noteworthy how the analysis leverages the roundrobin policy\
    \ of the main interconnect and the in-order nature of *peripherals* in Lemmas\
    \ [2](#page-8-1) and [3.](#page-8-2) The absence of internal reordering allows\
    \ to derive the number of transactions preceding the one under interference directly\
    \ from the arbitration policy. As long as the *peripherals* serve the transactions\
    \ in order, extending the analysis to support other arbitration policies is expected\
    \ to require minimal effort. Instead, supporting *peripherals* with internal transaction\
    \ reordering can lead to *timing anomalies* [\\[7\\]](#page-12-2) and make the\
    \ proposed model unsafe, as previously demonstrated in [\\[5\\]](#page-12-1).\
    \ Our analysis focuses on the available *peripherals* within the target architecture,\
    \ as out-of-order *peripherals* are not available open-source to us. We envision\
    \ expanding the\n\nanalysis to match higher-performance platforms as opensource\
    \ hardware evolves.\n\nLastly, it is important to note that the analysis bounds\
    \ only a single transaction issued by C<sup>i</sup> \u2013 this limitation is\
    \ not imposed on the interfering controllers. Lemma [2](#page-8-1) does not consider\
    \ C<sup>i</sup> to have more pending transactions, except for the ones already\
    \ accepted by P<sup>j</sup> . In other words, Lemma [2](#page-8-1) assumes that\
    \ there is not a queue of transactions buffered in the *bridges* between C<sup>i</sup>\
    \ and R0, which could exist when P<sup>j</sup> is full. We could potentially extend\
    \ the model to define a batch of enqueued transactions and then modify Lemma [2](#page-8-1)\
    \ to analyze this scenario. Such an extension would further build upon the proposed\
    \ model and analysis, which is limited to bound the access time of a single transaction.\n\
    \n# <span id=\"page-11-0\"></span>**7 RELATED WORK**\n\nIn this Section, we provide\
    \ a thorough comparison with previous works focusing on enhancing the timing predictability\
    \ of digital circuits. Traditionally, the majority of these works leverage commercial\
    \ off-the-shelf devices [\\[34\\]](#page-13-26), [\\[38\\]](#page-13-29) or predictable\
    \ architectures modeled with a mix of cycleaccurate and behavioral simulators\
    \ [\\[39\\]](#page-13-30). Also, they focus on bounding the execution times for\
    \ predefined specific software tasks rather than the individual transaction service\
    \ times [\\[7\\]](#page-12-2), [\\[38\\]](#page-13-29)\u2013[\\[40\\]](#page-13-31).\
    \ Furthermore, they build the models from dynamic experiments rather than from\
    \ static analysis, largely due to the dearth of detailed hardware specifications\
    \ [\\[35\\]](#page-13-27), limiting the generality of their approach. More recent\
    \ works advocate for static modeling and analysis of protocols [\\[8\\]](#page-13-0),\
    \ [\\[13\\]](#page-13-4), interconnect [\\[1\\]](#page-12-0), [\\[3\\]](#page-12-3),\
    \ [\\[9\\]](#page-13-5), and shared memory resources [\\[5\\]](#page-12-1), [\\\
    [10\\]](#page-13-1) to provide more generic and comprehensive models. While their\
    \ value is undeniable, due to the unavailability of the source RTL, each one focuses\
    \ on only one of these resources, resulting in a significant penalty to the pessimism\
    \ of the upper bounds [\\[5\\]](#page-12-1). Our work breaks from this convention,\
    \ presenting a holistic static model of an entire open-source architecture rigorously\
    \ validated through RTL cycle-accurate simulation and FPGA emulation. As Table\
    \ [1](#page-12-6) shows, this is the first work to analyze and model the open-source\
    \ siliconproven RTL of all the IPs composing a whole SoC to build the least pessimistic\
    \ upper bounds for data transfers within the architecture when compared to similar\
    \ SoA works.\n\nBiondi et al. [\\[13\\]](#page-13-4) developed a model of the\
    \ memory-access regulation mechanisms in the ARM MPAM and provided detailed instantiations\
    \ of such mechanisms, which they\n\n<span id=\"page-12-6\"></span>\n\n|  |  |\
    \  |  |  |  |  |  |  |  |  |  |  |  |  | TABLE 1: Comparison with State-of-the-Art\
    \ works for predictability. IC = Interconnect. DMR = Deadline miss ratio. |  |\
    \  |  |  |  |\n|--|--|--|--|--|--|--|--|--|--|--|--|--|--|--|-------------------------------------------------------------------------------------------------------------------|--|--|--|--|--|\n\
    |--|--|--|--|--|--|--|--|--|--|--|--|--|--|--|-------------------------------------------------------------------------------------------------------------------|--|--|--|--|--|\n\
    \n|                       | Target                             | Analysis on \
    \                   | Pessimism   | Technology  | Open RTL |\n|-----------------------|------------------------------------|--------------------------------|-------------|-------------|----------|\n\
    | Biondi et. al. [13]   | ARM MPAM Protocol                  | Protocol specs\
    \ (Model)         | No HW       | \u2717           | \u2717        |\n| Hassan\
    \ et. al. [8]    | JEDEC DDR3 Protocol                | Protocol specs (Model)\
    \         | 0% \u2212 200%   | \u2717           | \u2717        |\n| Abdelhalim\
    \ et.al. [5] | Whole mem. hier.                   | IPs & System (C++ Model) \
    \      | 16% \u2212 50%   | \u2717           | \u2717        |\n| BlueScale [3]\
    \         | Hier. mem. IC                      | IC uArch (Black-box)        \
    \   | DMR         | FPGA        | \u2717        |\n| AXI-RT-IC [1]         | AXI\
    \ SoC IC                         | IC uArch (Black-box)           | DMR      \
    \   | FPGA        | \u2717        |\n| Restuccia et. al. [9] | AXI Hier. mem.\
    \ IC                  | IC uArch (Black-box)           | 50% \u2212 90%   | FPGA\
    \        | \u2717        |\n| AXI-REALM [37]        | AXI traffic regulator  \
    \            | No analysis                    | No model    | FPGA & ASIC | \u2713\
    \        |\n| Ditty [10]            | Cache coher. mechanism             | IP\
    \ (Fine-grained RTL)          | 100% \u2212 200% | FPGA        | \u2713      \
    \  |\n| This Work             | SoC IC, peripherals & system-level | IP & System\
    \ (Fine-grained RTL) | 1% \u2212 28%    | FPGA & ASIC | \u2713        |\n\nthen\
    \ evaluated with IBM CPLEX, a decision optimization software for solving complex\
    \ optimization models. While elegant, this approach is not validated on hardware\
    \ and, therefore, is limited in terms of applicability and precision. A more practical\
    \ and adopted approach is the one proposed by Hassan and Pellizzoni [\\[8\\]](#page-13-0).\
    \ The authors develop a finegrained model of the JEDEC DDR3 protocol, validated\
    \ with MCsim [\\[12\\]](#page-13-3), a cycle-accurate C++ memory controller simulator.\
    \ Unfortunately, not having access to the RTL prevents fine-grained modeling and\
    \ analysis and mandates over-provisioning, strongly impacting the overall pessimism\
    \ of the system, which can be as high as 200%. Abdelhalim et al. in [\\[5\\]](#page-12-1)\
    \ present a study bounding the access times of memory requests traversing the\
    \ entire memory hierarchy and propose \xB5architectural modifications to the arbiters\
    \ in such hierarchy. Their modifications result in very low pessimism (down to\
    \ 16%) on synthetic and real-world benchmarks. However, the results are validated\
    \ on C++ models of the cores, interconnect, and memory controllers, not RTL code\
    \ targeting silicon implementation.\n\nMore recently, different researchers proposed\
    \ models of hardware IPs that they could validate through cycle-accurate experiments\
    \ [\\[1\\]](#page-12-0), [\\[4\\]](#page-12-4), [\\[9\\]](#page-13-5). In [\\\
    [9\\]](#page-13-5), Restuccia et al. focused on upper bounding the response times\
    \ of AXI bus transactions on FPGA SoCs through the modeling and analysis of generic\
    \ hierarchical interconnects arbitrating the accesses of multiple hardware accelerators\
    \ towards a shared DDR memory. In this work, the interconnect under analysis is\
    \ a proprietary Xilinx IP, which had to be treated as a black box. Also, due to\
    \ the unavailability of the RTL code, the authors did not model the other IPs\
    \ composing the target platform, limiting the precision of the proposed upper\
    \ bounds, which achieve a pessimism between 50% and 90%. Jiang et al. modeled,\
    \ analyzed, and developed AXI-ICRT [\\[1\\]](#page-12-0) and Bluescale [\\[3\\\
    ]](#page-12-3), two sophisticated interconnects providing predictability features\
    \ and coming with a comprehensive model. However, the model and analysis proposed\
    \ in AXI-ICRT [\\[1\\]](#page-12-0), and Bluescale [\\[3\\]](#page-12-3) are not\
    \ as fine-grained as ours: the authors do not provide upper bounds of the access\
    \ times but rather focus on the deadline miss ratio given a fixed workload for\
    \ the different controllers in the system. Moreover, the authors do not provide\
    \ the RTL of such solutions. AXI-REALM [\\[37\\]](#page-13-32) proposes completely\
    \ open-source IPs supporting predictable communications. However, it misses a\
    \ holistic model and analysis. In Ditty [\\[10\\]](#page-13-1), researchers propose\
    \ an open-source predictable directory-based cache coherence mechanism for multicore\
    \ safety-critical systems that guarantees a worst-case latency (WCL) on data accesses\
    \ with almost cycle-accurate precision. However, Ditty's model only covers the\
    \ coherency protocol latency and the core subsystem, overlooking systemlevel analysis\
    \ and achieving very pessimistic boundaries. In this landscape, it emerges clearly\
    \ that our work is the first one covering both modeling and analysis of the interconnects\
    \ and the shared memory resources, with an in-depth analysis of silicon-proven\
    \ open-source RTL IPs and achieving the lowest pessimism when compared to similar\
    \ SoA works.\n\n# <span id=\"page-12-5\"></span>**8 CONCLUSIONS**\n\nIn conclusion,\
    \ this is the first work to bridge the gap between open-source hardware and predictability\
    \ modeling and analysis. It presented (i) a fine-grained model and analysis for\
    \ the typical building blocks composing modern heterogeneous low-power SoCs directly\
    \ based on the source RTL, and (ii) a full mathematical analysis to upper bound\
    \ data transfer execution times. Namely, we demonstrated a methodology that successfully\
    \ exploits the availability of the source code to provide safe, but not overly\
    \ pessimistic, upper bounds for the execution times of data transfers when compared\
    \ to similar SoA works based on closed-source IPs.\n\nAs discussed in Section\
    \ [6,](#page-9-0) after this thorough evaluation, we envision extending our results\
    \ to other popular opensource IPs and different arbitration policies. To hopefully\
    \ stimulate novel research contributions, we open-source a guide to replicate\
    \ the results shown in Section [6](#page-9-0) at [https:](https://github.com/pulp-platform/soc_model_rt_analysis)\
    \ [//github.com/pulp-platform/soc](https://github.com/pulp-platform/soc_model_rt_analysis)\
    \ model rt analysis, comprehensive of the simulated environment and the software\
    \ benchmarks to run on a sophisticated Cheshire-based SoC targeting automotive\
    \ applications.\n\n# **REFERENCES**\n\n- <span id=\"page-12-0\"></span>[1] Z.\
    \ Jiang *et al.*, \"AXI-ICRT RT : Towards a Real-Time AXI-Interconnect for Highly\
    \ Integrated SoCs,\" *IEEE Transactions on Computers*, vol. 72, no. 3, pp. 786\u2013\
    799, 2022.\n- [2] A. Biondi *et al.*, \"SPHERE: A multi-SoC architecture for nextgeneration\
    \ cyber-physical systems based on heterogeneous platforms,\" *IEEE Access*, vol.\
    \ 9, pp. 75 446\u201375 459, 2021.\n- <span id=\"page-12-3\"></span>[3] Z. Jiang\
    \ *et al.*, \"BlueScale: a scalable memory architecture for predictable real-time\
    \ computing on highly integrated SoCs,\" in *Proceedings of the 59th ACM/IEEE\
    \ Design Automation Conference*, 2022, pp. 1261\u20131266.\n- <span id=\"page-12-4\"\
    ></span>[4] F. Restuccia *et al.*, \"AXI HyperConnect: A Predictable, Hypervisorlevel\
    \ Interconnect for Hardware Accelerators in FPGA SoC,\" in *2020 57th ACM/IEEE\
    \ Design Automation Conference (DAC)*, 2020.\n- <span id=\"page-12-1\"></span>[5]\
    \ S. Abdelhalim *et al.*, \"A Tight Holistic Memory Latency Bound Through Coordinated\
    \ Management of Memory Resources,\" in *35th Euromicro Conference on Real-Time\
    \ Systems (ECRTS 2023)*, vol. 262, 2023, pp. 17:1\u201317:25.\n- [6] G. Fernandez\
    \ *et al.*, \"Contention in multicore hardware shared resources: Understanding\
    \ of the state of the art,\" in *Proceedings of the 14th International Workshop\
    \ on Worst-Case Execution Time Analysis (WCET 2014)*, 2014, pp. 31\u201342.\n\
    - <span id=\"page-12-2\"></span>[7] S. Hahn, M. Jacobs, and J. Reineke, \"Enabling\
    \ Compositionality for Multicore Timing Analysis,\" in *Proceedings of the 24th\
    \ International Conference on Real-Time Networks and Systems*. Association for\
    \ Computing Machinery, 2016, p. 299\u2013308.\n- <span id=\"page-13-0\"></span>[8]\
    \ M. Hassan and R. Pellizzoni, \"Bounding DRAM Interference in COTS Heterogeneous\
    \ MPSoCs for Mixed Criticality Systems,\" *IEEE Transactions on Computer-Aided\
    \ Design of Integrated Circuits and Systems*, vol. 37, no. 11, pp. 2323\u2013\
    2336, 2018.\n- <span id=\"page-13-5\"></span>[9] F. Restuccia *et al.*, \"Bounding\
    \ Memory Access Times in Multi-Accelerator Architectures on FPGA SoCs,\" *IEEE\
    \ Transactions on Computers*, vol. 72, no. 1, pp. 154\u2013167, 2022.\n- <span\
    \ id=\"page-13-1\"></span>[10] Z. Wu, M. Bekmyrza, N. Kapre, and H. Patel, \"\
    Ditty: Directorybased Cache Coherence for Multicore Safety-critical Systems,\"\
    \ in *2023 Design, Automation & Test in Europe Conference & Exhibition (DATE)*.\
    \ IEEE, 2023, pp. 1\u20136.\n- <span id=\"page-13-2\"></span>[11] M. Hassan, \"\
    On the Off-Chip Memory Latency of Real-Time Systems: Is DDR DRAM Really the Best\
    \ Option?\" in *2018 IEEE Real-Time Systems Symposium (RTSS)*, 2018, pp. 495\u2013\
    505.\n- <span id=\"page-13-3\"></span>[12] R. Mirosanlou, D. Guo, M. Hassan, and\
    \ R. Pellizzoni, \"Mcsim: An extensible dram memory controller simulator,\" *IEEE\
    \ Computer Architecture Letters*, vol. 19, no. 2, pp. 105\u2013109, 2020.\n- <span\
    \ id=\"page-13-4\"></span>[13] M. Zini, D. Casini, and A. Biondi, \"Analyzing\
    \ Arm's MPAM From the Perspective of Time Predictability,\" *IEEE Transactions\
    \ on Computers*, vol. 72, no. 1, pp. 168\u2013182, 2023.\n- <span id=\"page-13-6\"\
    ></span>[14] A. Herrera, \"The Promises and Challenges of Open Source Hardware,\"\
    \ *Computer*, vol. 53, no. 10, pp. 101\u2013104, 2020.\n- <span id=\"page-13-10\"\
    ></span>[15] A. Ottaviano, T. Benz, P. Scheffler, and L. Benini, \"Cheshire: A\
    \ Lightweight, Linux-Capable RISC-V Host Platform for Domain-Specific Accelerator\
    \ Plug-In,\" *IEEE Transactions on Circuits and Systems II: Express Briefs*, pp.\
    \ 1\u20131, 2023.\n- <span id=\"page-13-7\"></span>[16] L. Valente *et al.*, \"\
    Shaheen: An Open, Secure, and Scalable RV64 SoC for Autonomous Nano-UAVs,\" in\
    \ *2023 IEEE Hot Chips 35 Symposium (HCS)*, 2023, pp. 1\u201312.\n- <span id=\"\
    page-13-8\"></span>[17] M. B. Taylor, \"Your Agile Open Source HW Stinks (Because\
    \ It Is Not a System),\" in *2020 IEEE/ACM International Conference On Computer\
    \ Aided Design (ICCAD)*, 2020, pp. 1\u20136.\n- <span id=\"page-13-9\"></span>[18]\
    \ PULP, \"PULP Platform Github,\" [https://github.com/](https://github.com/pulp-platform)\
    \ [pulp-platform,](https://github.com/pulp-platform) 2022.\n- <span id=\"page-13-11\"\
    ></span>[19] L. Valente *et al.*, \"HULK-V: a Heterogeneous Ultra-low-power Linux\
    \ capable RISC-V SoC,\" in *2023 Design, Automation & Test in Europe Conference\
    \ & Exhibition (DATE)*, 2023, pp. 1\u20136.\n- <span id=\"page-13-12\"></span>[20]\
    \ OpenHW-Group, \"CVA6,\" [https://github.com/openhwgroup/](https://github.com/openhwgroup/cva6)\
    \ [cva6,](https://github.com/openhwgroup/cva6) 2022.\n- <span id=\"page-13-13\"\
    ></span>[21] M. Schneider *et al.*, \"Composite Enclaves: Towards Disaggregated\
    \ Trusted Execution,\" *IACR Transactions on Cryptographic Hardware and Embedded\
    \ Systems*, vol. 2022, no. 1, p. 630\u2013656, Nov. 2021.\n- <span id=\"page-13-14\"\
    ></span>[22] P. Platform, \"PULP cluster,\" [https://github.com/pulp-platform/](https://github.com/pulp-platform/pulp_cluster)\
    \ pulp [cluster,](https://github.com/pulp-platform/pulp_cluster) 2022.\n- <span\
    \ id=\"page-13-15\"></span>[23] OpenHW-Group, \"CV32E40P,\" [https://github.com/](https://github.com/openhwgroup/cv32e40p)\
    \ [openhwgroup/cv32e40p,](https://github.com/openhwgroup/cv32e40p) 2023.\n- <span\
    \ id=\"page-13-16\"></span>[24] A. Kurth *et al.*, \"An Open-Source Platform for\
    \ High-Performance Non-Coherent On-Chip Communication,\" *IEEE Transactions on\
    \ Computers*, pp. 1\u20131, 2021.\n- <span id=\"page-13-17\"></span>[25] B. John,\
    \ \"HyperRAM as a low pin-count expansion memory for embedded systems,\" [https://www.infineon.com/dgdl/](https://www.infineon.com/dgdl/Infineon-HyperRAM_as_a_low_pin-count_expansion_memory_for_embedded_systems-Whitepaper-v01_00-EN.pdf?fileId=8ac78c8c7d0d8da4017d0fb28970272c&da=t)\
    \ [Infineon-HyperRAM](https://www.infineon.com/dgdl/Infineon-HyperRAM_as_a_low_pin-count_expansion_memory_for_embedded_systems-Whitepaper-v01_00-EN.pdf?fileId=8ac78c8c7d0d8da4017d0fb28970272c&da=t)\
    \ as a low pin-count expansion memory for embedded [systems-Whitepaper-v01](https://www.infineon.com/dgdl/Infineon-HyperRAM_as_a_low_pin-count_expansion_memory_for_embedded_systems-Whitepaper-v01_00-EN.pdf?fileId=8ac78c8c7d0d8da4017d0fb28970272c&da=t)\
    \ 00-EN.pdf?fileId= [8ac78c8c7d0d8da4017d0fb28970272c&da=t,](https://www.infineon.com/dgdl/Infineon-HyperRAM_as_a_low_pin-count_expansion_memory_for_embedded_systems-Whitepaper-v01_00-EN.pdf?fileId=8ac78c8c7d0d8da4017d0fb28970272c&da=t)\
    \ 2020.\n- <span id=\"page-13-18\"></span>[26] AMD, \"Zynq-7000 - Technical Reference\
    \ Manual, UG585,\" [https:](https://docs.xilinx.com/r/en-US/ug585-zynq-7000-SoC-TRM)\
    \ [//docs.xilinx.com/r/en-US/ug585-zynq-7000-SoC-TRM.](https://docs.xilinx.com/r/en-US/ug585-zynq-7000-SoC-TRM)\n\
    - <span id=\"page-13-19\"></span>[27] A. Noami, B. Pradeep Kumar, and P. Chandrasekhar,\
    \ \"High Performance AXI4 Interface Protocol for Multi-Core Memory Controller\
    \ on SoC,\" in *Data Engineering and Communication Technology*, K. A. Reddy, B.\
    \ R. Devi, B. George, and K. S. Raju, Eds. Singapore: Springer Singapore, 2021,\
    \ pp. 131\u2013140.\n- <span id=\"page-13-20\"></span>[28] D. Rossi, I. Loi, G.\
    \ Haugou, and L. Benini, \"Ultra-low-latency lightweight dma for tightly coupled\
    \ multi-core clusters,\" in *Proceedings of the 11th ACM Conference on Computing\
    \ Frontiers*, ser. CF '14. New York, NY, USA: Association for Computing Machinery,\
    \ 2014. [Online]. Available:<https://doi.org/10.1145/2597917.2597922>\n- <span\
    \ id=\"page-13-21\"></span>[29] ARM, \"AMBA AXI Protocol Specification,\" [https://developer.arm.](https://developer.arm.com/documentation/ihi0022/j/?lang=en)\
    \ [com/documentation/ihi0022/j/?lang=en,](https://developer.arm.com/documentation/ihi0022/j/?lang=en)\
    \ 2022.\n- <span id=\"page-13-22\"></span>[30] Xilinx-AMD, \"Dual Port SRAM specifications,\"\
    \ [https://docs.xilinx.com/r/2022.1-English/](https://docs.xilinx.com/r/2022.1-English/ug1483-model-composer-sys-gen-user-guide/Dual-Port-RAM)\
    \ [ug1483-model-composer-sys-gen-user-guide/Dual-Port-RAM.](https://docs.xilinx.com/r/2022.1-English/ug1483-model-composer-sys-gen-user-guide/Dual-Port-RAM)\n\
    - <span id=\"page-13-23\"></span>[31] PULP, \"HyperRAM Controller RTL,\" [https://github.com/](https://github.com/pulp-platform/hyperbus)\
    \ [pulp-platform/hyperbus,](https://github.com/pulp-platform/hyperbus) 2022.\n\
    - <span id=\"page-13-24\"></span>[32] Infineon, \"HyperRAM RTL,\" [https://www.](https://www.infineon.com/dgdl/Infineon-S27KL0641_S27KS0641_VERILOG-SimulationModels-v05_00-EN.zip?fileId=8ac78c8c7d0d8da4017d0f6349a14f68)\
    \ [infineon.com/dgdl/Infineon-S27KL0641](https://www.infineon.com/dgdl/Infineon-S27KL0641_S27KS0641_VERILOG-SimulationModels-v05_00-EN.zip?fileId=8ac78c8c7d0d8da4017d0f6349a14f68)\
    \ S27KS0641 [VERILOG-SimulationModels-v05](https://www.infineon.com/dgdl/Infineon-S27KL0641_S27KS0641_VERILOG-SimulationModels-v05_00-EN.zip?fileId=8ac78c8c7d0d8da4017d0f6349a14f68)\
    \ 00-EN.zip?fileId= [8ac78c8c7d0d8da4017d0f6349a14f68,](https://www.infineon.com/dgdl/Infineon-S27KL0641_S27KS0641_VERILOG-SimulationModels-v05_00-EN.zip?fileId=8ac78c8c7d0d8da4017d0f6349a14f68)\
    \ 2022.\n- <span id=\"page-13-25\"></span>[33] Infineon, \"HyperBUS specifications,\"\
    \ [https://www.](https://www.infineon.com/dgdl/Infineon-HYPERBUS_SPECIFICATION_LOW_SIGNAL_COUNT_HIGH_PERFORMANCE_DDR_BUS-AdditionalTechnicalInformation-v09_00-EN.pdf?fileId=8ac78c8c7d0d8da4017d0ed619b05663)\
    \ [infineon.com/dgdl/Infineon-HYPERBUS](https://www.infineon.com/dgdl/Infineon-HYPERBUS_SPECIFICATION_LOW_SIGNAL_COUNT_HIGH_PERFORMANCE_DDR_BUS-AdditionalTechnicalInformation-v09_00-EN.pdf?fileId=8ac78c8c7d0d8da4017d0ed619b05663)\
    \ SPECIFICATION LOW SIGNAL COUNT HIGH [PERFORMANCE](https://www.infineon.com/dgdl/Infineon-HYPERBUS_SPECIFICATION_LOW_SIGNAL_COUNT_HIGH_PERFORMANCE_DDR_BUS-AdditionalTechnicalInformation-v09_00-EN.pdf?fileId=8ac78c8c7d0d8da4017d0ed619b05663)\
    \ DDR [BUS-AdditionalTechnicalInformation-v09](https://www.infineon.com/dgdl/Infineon-HYPERBUS_SPECIFICATION_LOW_SIGNAL_COUNT_HIGH_PERFORMANCE_DDR_BUS-AdditionalTechnicalInformation-v09_00-EN.pdf?fileId=8ac78c8c7d0d8da4017d0ed619b05663)\
    \ 00-EN.pdf?fileId= [8ac78c8c7d0d8da4017d0ed619b05663,](https://www.infineon.com/dgdl/Infineon-HYPERBUS_SPECIFICATION_LOW_SIGNAL_COUNT_HIGH_PERFORMANCE_DDR_BUS-AdditionalTechnicalInformation-v09_00-EN.pdf?fileId=8ac78c8c7d0d8da4017d0ed619b05663)\
    \ 2022.\n- <span id=\"page-13-26\"></span>[34] R. Wilhelm *et al.*, \"The worst-case\
    \ execution-time problem\u2014overview of methods and survey of tools,\" *ACM\
    \ Trans. Embed. Comput. Syst.*, vol. 7, no. 3, may 2008. [Online]. Available:\
    \ <https://doi.org/10.1145/1347375.1347389>\n- <span id=\"page-13-27\"></span>[35]\
    \ T. Mitra, J. Teich, and L. Thiele, \"Time-critical systems design: A survey,\"\
    \ *IEEE Design & Test*, vol. 35, no. 2, pp. 8\u201326, 2018.\n- <span id=\"page-13-28\"\
    ></span>[36] F. Restuccia *et al.*, \"Modeling and analysis of bus contention\
    \ for hardware accelerators in FPGA SoCs,\" in *32nd Euromicro Conference on Real-Time\
    \ Systems (ECRTS 2020)*. Schloss Dagstuhl-Leibniz-Zentrum fur Informatik, 2020.\
    \ \xA8\n- <span id=\"page-13-32\"></span>[37] B. Thomas *et al.*, \"AXI-REALM:\
    \ A Lightweight and Modular Interconnect Extension for Traffic Regulation and\
    \ Monitoring of Heterogeneous Real-Time SoCs,\" in *2024 Design, Automation &\
    \ Test in Europe Conference & Exhibition (DATE)*, 2024.\n- <span id=\"page-13-29\"\
    ></span>[38] J. P. Cerrolaza *et al.*, \"Multi-Core Devices for Safety-Critical\
    \ Systems: A Survey,\" *ACM Comput. Surv.*, vol. 53, no. 4, aug 2020. [Online].\
    \ Available:<https://doi.org/10.1145/3398665>\n- <span id=\"page-13-30\"></span>[39]\
    \ M. Schoeberl *et al.*, \"T-CREST: Time-predictable multi-core architecture for\
    \ embedded systems,\" *Journal of Systems Architecture*, vol. 61, no. 9, pp. 449\u2013\
    471, 2015.\n- <span id=\"page-13-31\"></span>[40] G. Fernandez *et al.*, \"Increasing\
    \ confidence on measurement-based contention bounds for real-time round-robin\
    \ buses,\" in *Proceedings of the 52nd Annual Design Automation Conference*, ser.\
    \ DAC '15. New York, NY, USA: Association for Computing Machinery, 2015.\n\n**Luca\
    \ Valente** received the MSc degree in electronic engineering from the Politecnico\
    \ of Turin in 2020. He is currently a PhD student at the University of Bologna\
    \ in the Department of Electrical, Electronic, and Information Technologies Engineering\
    \ (DEI). His main research interests are hardware-software co-design of heterogeneous\
    \ SoCs.\n\n**Francesco Restuccia** received a PhD degree in computer engineering\
    \ (cum laude) from Scuola Superiore Sant'Anna Pisa, in 2021. He is a postdoctoral\
    \ researcher at the University of California, San Diego. His main research interests\
    \ include hardware security, on-chip communications, timing analysis for heterogeneous\
    \ platforms, cyber-physical systems, and time-predictable hardware acceleration\
    \ of deep neural networks on commercial FPGA SoC platforms.\n\n**Davide Rossi**\
    \ received the Ph.D. degree from the University of Bologna in 2012. He has been\
    \ a Post-Doctoral Researcher with the Department of Electrical, Electronic and\
    \ Information Engineering \"Guglielmo Marconi,\" University of Bologna, since\
    \ 2015, where he is currently an Associate Professor position. His research interests\
    \ focus on energy-efficient digital architectures. In this field, he has published\
    \ more than 100 papers in international peer-reviewed conferences and journals.\n\
    \n**Ryan Kastner** is a professor in the Department of Computer Science and Engineering\
    \ at UC San Diego. He received a PhD in Computer Science at UCLA, a masters degree\
    \ in engineering and bachelor degrees in Electrical Engineering and Computer Engineering\
    \ from Northwestern University. His current research interests fall into three\
    \ areas: hardware acceleration, hardware security, and remote sensing.\n\n**Luca\
    \ Benini** holds the chair of Digital Circuits and Systems at ETHZ and is Full\
    \ Professor at the Universita di Bologna. He received a PhD from ` Stanford University.\
    \ His research interests are in energy-efficient parallel computing systems, smart\
    \ sensing micro-systems and machine learning hardware. He has published more than\
    \ 1000 peer-reviewed papers and 5 books. He is a Fellow of the ACM and a member\
    \ of the Academia Europaea."
  paper_id: 18
  scores:
    originality:
      raw: 4.5
      reason: The proposed HOPE framework presents a novel approach to STT-RAM architecture
        exploration, addressing a significant gap in the existing literature and tools.
    technical_soundness:
      raw: 4.0
      reason: The integration of the STT-RAM memory controller within the gem5 simulator
        demonstrates a solid technical foundation, though further validation and testing
        details would enhance the robustness of the findings.
    clarity:
      raw: 3.5
      reason: While the paper conveys its ideas, some sections could benefit from
        clearer explanations and more structured presentation to improve reader comprehension.
    significance:
      raw: 4.0
      reason: The framework has the potential to significantly impact research and
        development in STT-RAM technologies, making it a valuable contribution to
        the field.
    relevance:
      raw: 5.0
      reason: The topic is highly relevant to current trends in memory technology
        and system architecture, aligning well with ongoing research in non-volatile
        memory solutions.
  total_score: 4.1
  tier: B
- title: Analyzing and Improving Hardware Modeling of Accel-Sim
  abstract: "Generational improvements to commodity DRAM throughout half a century\
    \ have\nlong solidified its prevalence as main memory across the computing industry.\n\
    However, overcoming today's DRAM technology scaling challenges requires new\n\
    solutions driven by both DRAM producers and consumers. In this paper, we\nobserve\
    \ that the separation of concerns between producers and consumers\nspecified by\
    \ industry-wide DRAM standards is becoming a liability to progress\nin addressing\
    \ scaling-related concerns.\n  To understand the problem, we study four key directions\
    \ for overcoming DRAM\nscaling challenges using system-memory cooperation: (i)\
    \ improving memory access\nlatencies; (ii) reducing DRAM refresh overheads; (iii)\
    \ securely defending\nagainst the RowHammer vulnerability; and (iv) addressing\
    \ worsening memory\nerrors. We find that the single most important barrier to\
    \ advancement in all\nfour cases is the consumer's lack of insight into DRAM reliability.\
    \ Based on an\nanalysis of DRAM reliability testing, we recommend revising the\
    \ separation of\nconcerns to incorporate limited information transparency between\
    \ producers and\nconsumers. Finally, we propose adopting this revision in a two-step\
    \ plan,\nstarting with immediate information release through crowdsourcing and\n\
    publication and culminating in widespread modifications to DRAM standards."
  url: http://arxiv.org/abs/2401.16279v1
  keywords: ''
  document: "## 1. Introduction\n\nDynamic Random Access Memory (DRAM) [\\[1,](#page-13-0)\
    \ [2\\]](#page-13-1) is the dominant main memory technology across a broad range\
    \ of computing systems because of its high capacity at low cost [\\[3\u2013](#page-13-2)[6\\\
    ]](#page-13-3). Building modern DRAM chips requires specialized design and manufacturing\
    \ techniques (e.g., custom process nodes [\\[7\\]](#page-13-4) and bespoke materials\
    \ [\\[8\\]](#page-13-5)) developed across more than half a century of rich history\
    \ [\\[9\\]](#page-13-6), so the computing industry employs a separation of concerns\
    \ to explicitly divide responsibilities between DRAM producers (e.g., manufacturers)\
    \ and DRAM consumers (e.g., cloud architects, processor and system-on-a-chip designers,\
    \ memory module vendors, etc.). DRAM producers typically develop highly-optimized\
    \ DRAM chips as standalone mass-market components with clearly-specified interfaces,\
    \ independently addressing design concerns related to DRAM technology. DRAM consumers\
    \ then integrate these DRAM chips to develop a broad range of DRAM-based systems.\
    \ This approach enables both parties to specialize their designs and preserve\
    \ trade secrets while working around a common interface.\n\nIn the last decade,\
    \ however, this long-standing separation of concerns has been challenged by worsening\
    \ DRAM technology scaling difficulties that manifest in two problematic ways.\n\
    \nFirst, improvements to DRAM access latency and storage capacity are slowing\
    \ down due to DRAM technology scaling challenges [\\[10,](#page-13-7) [11\\]](#page-13-8).\
    \ To illustrate this problem, we survey 58 publicly-available DRAM chip datasheets[1](#page-0-0)\
    \ in Section [3.1,](#page-3-0) broadly sampling chips from across 19 DRAM manufacturers\
    \ spanning the past five decades. We study the evolution of access latency and\
    \ storage capacity characteristics, showing slowdowns in improvements to both\
    \ metrics in the past ten to twenty years. For example, average annual improvements\
    \ to the data access latency (governed by the timing parameter tRCD) reduced by\
    \ 69.5% (from 2.66% to 0.81%) before and after the year 2000, with little to no\
    \ improvement in the past decade. This is consistent with similar surveys conducted\
    \ by prior works [\\[12\u2013](#page-13-9)[22\\]](#page-13-10).\n\nSecond, conventional\
    \ approaches to managing scaling challenges used by DRAM producers (e.g., in-DRAM\
    \ mechanisms to mitigate worsening memory errors) no longer suffice to hide the\
    \ underlying problems from DRAM consumers. For example, memory errors caused by\
    \ the RowHammer vulnerability are a serious and growing threat to system-level\
    \ robustness (i.e., security, reliability, and safety) [\\[23](#page-13-11)[\u2013\
    27\\]](#page-13-12). Experimental studies [\\[28](#page-13-13)[\u201332\\]](#page-13-14)\
    \ throughout the past decade demonstrate that these errors can be consistently\
    \ induced across a broad range of DRAM chips. In particular, Kim et al. [\\[28\\\
    ]](#page-13-13) show that RowHammer errors can be induced much more quickly in\
    \ modern chips, i.e., with only 4.8K memory activations for chips from 2019\u2013\
    \ 2020, which is 14.4\xD7 lower than the 69.2K activations required for older\
    \ DRAM chips from 2010\u20132013. Section [3](#page-3-1) further discuss how DRAM\
    \ technology design concerns that were previously hidden by the separation are\
    \ now breaking through to become consumer-facing problems that impact system-wide\
    \ performance and robustness.\n\nWe believe that overcoming the DRAM scaling problem\
    \ requires creative, holistic thinking from everyone involved, including both\
    \ producers and consumers throughout industry and academia. Unfortunately, we\
    \ observe two key limitations with today's separation of concerns that discourage\
    \ progress toward addressing scaling challenges:\n\n1. The industry-wide DRAM\
    \ standards that specify how to implement the separation (e.g., JEDEC DDRn [\\\
    [33,](#page-13-15) [34\\]](#page-13-16), HBMx [\\[35,](#page-13-17) [36\\]](#page-13-18))[2](#page-0-1)\
    \ do so imperfectly, requiring laborious revision to adapt to failures in separation.\n\
    \n<span id=\"page-0-0\"></span><sup>1</sup>We conduct a best-effort survey of\
    \ publicly-available datasheets. We itemize the specific datasheets we reference\
    \ in Appendix [B.](#page-24-0)\n\n<span id=\"page-0-1\"></span><sup>2</sup>DRAM\
    \ standards specify a DRAM chip's microarchitecture, including its interface,\
    \ configuration, and performance characteristics as visible to DRAM consumers.\
    \ Section [2](#page-2-0) explains standards in detail.\n\n2. The existing separation\
    \ is too strict, which constrains each party's solution space and stifles opportunities\
    \ to explore new ways to address the scaling challenges.\n\nThese observations\
    \ stem from a combination of two key sources of evidence.\n\nFirst, recent robustness\
    \ challenges caused by memory errors have already broken the separation of concerns\
    \ established by current DRAM standards. Section [3.2](#page-4-0) references two\
    \ specific instances of this problem, RowHammer [\\[25](#page-13-19)[\u201327,](#page-13-12)\
    \ [29,](#page-13-20) [37\\]](#page-13-21) and ondie error correction [\\[38](#page-13-22)[\u2013\
    41\\]](#page-13-23), showing that both cases expose memory errors caused by DRAM\
    \ technology behavior in a way that is undefined by existing DRAM standards. Although\
    \ recent changes to the standards [\\[34,](#page-13-16) [42](#page-13-24)[\u2013\
    44\\]](#page-13-25) discuss these problems and provide limited solutions, undefined\
    \ chip behavior remains a serious and worsening problem for DRAM consumers [\\\
    [27,](#page-13-12) [45](#page-13-26)[\u201348\\]](#page-13-27).\n\nSecond, many\
    \ promising approaches to address DRAM scaling challenges in today's chips [\\\
    [16,](#page-13-28) [22,](#page-13-10) [29,](#page-13-20) [49\u2013](#page-13-29)[110\\\
    ]](#page-15-0) rely upon exploiting the benefits of deliberately breaking the\
    \ separation of concerns. These approaches employ system-memory cooperation [\\\
    [10,](#page-13-7) [27,](#page-13-12) [111\u2013](#page-15-1)[113\\]](#page-15-2),\
    \ demonstrating significant system-level benefits from implementing mechanisms\
    \ outside the DRAM chip to supplement on-chip solutions built by DRAM producers.\
    \ Section [4](#page-5-0) surveys these proposals categorized by the particular\
    \ DRAM scaling challenge they tackle: (1) improving memory access latencies (Section\
    \ [4.1\\)](#page-5-1); (2) reducing DRAM refresh overheads (Section [4.2\\)](#page-6-0);\
    \ (3) securely defending against the RowHammer vulnerability (Section [4.3\\)](#page-6-1);\
    \ and (4) addressing worsening memory errors (Section [4.4\\)](#page-7-0). Unfortunately,\
    \ we observe that today's separation of concerns does not support producers and\
    \ consumers to adopt these methods with ease. Instead, doing so requires them\
    \ to work around DRAM standards, which is impractical for the overwhelming majority\
    \ of consumers due to the risks and costs inherent in custom modifications to\
    \ DRAM chips and unstandardized behavior.\n\nBased on these observations, we conclude\
    \ that both the separation of concerns and the standards that specify them are\
    \ outdated for today's DRAM landscape. To rethink the separation of concerns in\
    \ a modern context, we refer to each of the four cases of system-memory cooperation\
    \ that we study in Section [4.](#page-5-0) In each case, we review how prior proposals\
    \ break the separation of concerns so that we can better understand its limitations\
    \ today.\n\nWe find that the single most important barrier to advancement in all\
    \ four cases is the consumer's lack of insight into DRAM reliability. For example,\
    \ Section [4.3](#page-6-1) explains how knowing certain properties of memory errors\
    \ (e.g., correlation with physical chip locations, memory access patterns, and\
    \ operating parameters) is essential for developing secure defenses against the\
    \ RowHammer vulnerability. The existing separation of concerns effectively abstracts\
    \ details of a DRAM chip's internal operation away from consumers to the extent\
    \ that consumers do not have the necessary context to properly reason about and\
    \ evaluate how operating the chip in a particular way will impact its reliable\
    \ operation. This encompasses operating points both within and outside of manufacturer\
    \ recommendations;\n\nin either case, the consumer lacks the context necessary\
    \ to accurately determine how their design decisions outside of the DRAM chip\
    \ (e.g., in the memory controller) affect the DRAM chip's reliable operation.\n\
    \nTo gain further insight into this problem, we study the general process of memory\
    \ reliability testing in Section [5.](#page-8-0) Our analysis suggests that the\
    \ entire testing process is grounded on knowing relevant properties of a DRAM\
    \ chip's microarchitectural design, such as the physical organization of cells\
    \ within the storage array and how they encode data at a circuit level. Using\
    \ this information, a DRAM consumer can build models and test methodologies to\
    \ explore the full design space surrounding commodity DRAM chips.\n\nBased on\
    \ our analysis, we advocate revising both the separation of concerns and the standards\
    \ that specify them to incorporate limited information transparency between DRAM\
    \ producers and consumers. In particular, explicitly communicating basic DRAM\
    \ design and test characteristics from DRAM producers to consumers empowers the\
    \ consumer with the context to understand how different system-level design choices\
    \ (e.g., optimizations in the processor and memory controller) will affect DRAM\
    \ chip operation. Section [7.1](#page-11-0) identifies information to communicate\
    \ by drawing on examples from research studies, including (1) basic microarchitectural\
    \ properties (e.g., organization of physical rows, sizes of internal storage arrays)\
    \ and (2) best practice guidelines for reliability testing (e.g., test patterns\
    \ for key error mechanisms).[3](#page-1-0) Section [6](#page-9-0) further explains\
    \ how access to this information provides a practical degree of insight into DRAM\
    \ operation for DRAM consumers to work with, without compromising DRAM producers'\
    \ trade secrets or the cost advantages of commodity DRAM.\n\nWe advocate for industry\
    \ to incorporate information transparency into DRAM standards through a two-step\
    \ approach involving all DRAM stakeholders, including producers and consumers.\n\
    \nStep 1: Early Adoption. Initially, we recommend conceptually revising the separation\
    \ of concerns without yet revising DRAM standards. This step targets DRAM chips\
    \ already in the field, asking both DRAM producers and consumers to voluntarily\
    \ release information they already have at hand. We propose each party to take\
    \ a different approach as follows:\n\n- Consumers: Contribute to a crowdsourced\
    \ database of information obtained through third-party testing of commodity DRAM\
    \ chips on the market (e.g., as conducted in numerous studies discussed in Section\
    \ [4\\)](#page-5-0).\n- Producers: Publish information (e.g., using datasheet\
    \ revisions, whitepapers, or online resources) about their products, possibly\
    \ limited to information that they already have on hand from past records (i.e.,\
    \ information that requires minimal logistical effort to release).\n\nThrough\
    \ these two avenues, all DRAM stakeholders will benefit from the release of information:\
    \ consumers will gain ac-\n\n<span id=\"page-1-0\"></span><sup>3</sup>Consumers\
    \ with access to appropriate testing infrastructure can reverseengineer much of\
    \ this information [\\[16,](#page-13-28) [22,](#page-13-10) [32,](#page-13-14)\
    \ [73,](#page-14-0) [74,](#page-14-1) [78,](#page-14-2) [114\u2013](#page-15-3)[118\\\
    ]](#page-15-4). However, existing techniques may not reveal all possible necessary\
    \ information (e.g., due to inaccessible components such as remapped rows).\n\n\
    cess to a wider solution space for addressing DRAM scaling challenges, and producers\
    \ will gain access to insights and new solution directions from consumers' efforts.\
    \ In particular, these benefits require neither changes to existing DRAM hardware\
    \ or standards (though standardizing the information release could streamline\
    \ the process) nor forced disclosure of sensitive information by either party.\n\
    \nStep 2: Long-Term Revision. In the long run, we propose revising DRAM standards\
    \ to include industry-standard information, tools, and specifications for a wider\
    \ range of DRAM operating points. In addition to the information requested by\
    \ Step 1, we identify two recommendations that our studies show would be beneficial\
    \ for consumers: (1) reliability guarantees for how a chip is expected to behave\
    \ under certain operating conditions (e.g., predictable behavior of faults [\\\
    [119\\]](#page-15-5)); (2) disclosure of industry-validated DRAM reliability models\
    \ and testing strategies suitable for commodity DRAM chips (e.g., similar to how\
    \ JEDEC JEP122 [\\[120\\]](#page-15-6), JESD218 [\\[121\\]](#page-15-7), and JESD219\
    \ [\\[122\\]](#page-15-8) address Flash-memory-specific error mechanisms [\\[123\u2013\
    ](#page-15-9)[125\\]](#page-15-10) such as floating-gate data retention [\\[126\u2013\
    ](#page-15-11)[129\\]](#page-15-12) and models for physical phenomena such as\
    \ threshold voltage distributions [\\[130\u2013](#page-15-13)[135\\]](#page-15-14)).\
    \ Revising DRAM standards in this way will align the standards with a more permissive\
    \ separation of concerns going forward, thereby encouraging cooperation between\
    \ producers and consumers in pursuit of building more robust and higher-performance\
    \ future computing systems.\n\nWe make the following contributions:\n\n- We make\
    \ a case to rethink the long-standing separation of concerns between DRAM producers\
    \ and consumers to enable and encourage new solutions for addressing worsening\
    \ DRAM technology scaling challenges.\n- We motivate and support our case by thoroughly\
    \ reviewing prior work, including (1) four case studies that survey system-memory\
    \ cooperative techniques to address DRAM scaling challenges and (2) a historical\
    \ survey of DRAM chip capacity, latency, and energy characteristics based on datasheets.\
    \ We open-source [\\[136\\]](#page-15-15) our dataset.\n- We provide a new perspective\
    \ on memory reliability testing from the viewpoint of DRAM consumers, identifying\
    \ access to a DRAM chip's microarchitectural details as both a key challenge and\
    \ enabler for system-memory cooperative solutions to DRAM scaling challenges.\n\
    - We propose a practical plan to encourage new solutions to modern DRAM scaling\
    \ challenges by revising both the separation of concerns and how it is specified\
    \ by DRAM standards today.\n\n## <span id=\"page-2-0\"></span>2. DRAM Standards\
    \ as a Separation of Concerns\n\nIndustry-wide DRAM standards split the responsibilities\
    \ of building and integrating commodity DRAM chips to DRAM producers and consumers,\
    \ respectively. This section reviews how standards achieve this and its implications\
    \ for DRAM producers and consumers.\n\n## 2.1. DRAM Standards\n\nDRAM standards\
    \ carefully balance the needs of both producers and consumers through industry-wide\
    \ consensus. Today, the\n\nJEDEC consortium [\\[137\\]](#page-15-16) maintains\
    \ a limited set of standards describing commodity DRAM chips with different target\
    \ applications, e.g., general-purpose DDRn [\\[33,](#page-13-15) [34,](#page-13-16)\
    \ [138\\]](#page-15-17), bandwidthoptimized HBMn [\\[35,](#page-13-17) [36\\]](#page-13-18),\
    \ mobile-oriented LPDDRn [\\[44,](#page-13-25) [139\\]](#page-15-18), graphics-oriented\
    \ GDDRn [\\[140,](#page-15-19) [141\\]](#page-15-20).\n\nDRAM standards specify\
    \ all aspects of a DRAM chip's design that pertain to the interface between producers\
    \ and consumers, including the chip's access interfaces, configuration mechanisms,\
    \ and performance characteristics. By doing so, standards effectively abstract\
    \ DRAM technology-level concerns (e.g., cellto-cell variation, reliability challenges)\
    \ into predictable usage patterns, thereby reducing a complex storage technology\
    \ into a modular computing component.\n\n#### 2.2. Advantages of Standardized\
    \ DRAM\n\nStandardized DRAM enables the widespread use of highlyoptimized DRAM\
    \ chips. This is because standards scope DRAM technology to a few fixed components\
    \ (i.e., standardized chips) with clearly-defined interfaces that both producers\
    \ and consumers can optimize towards without concerning themselves with the other\
    \ party's design challenges. This gives each party the freedom to explore different\
    \ solutions to DRAM design challenges based on their design goals. For example,\
    \ empirical studies of DRAM chips [\\[23,](#page-13-11) [40,](#page-13-30) [48,](#page-13-27)\
    \ [118,](#page-15-4) [142\\]](#page-15-21) show that different DRAM producers\
    \ mitigate memory errors using a broad range of different on-chip error mitigation\
    \ techniques. In general, producers are free to innovate in any way that does\
    \ not violate the specifications established by standards, and consumers can build\
    \ upon those specifications in any way to meet their systems' needs.\n\n2.2.1.\
    \ DRAM Producers' Trade Secrets. DRAM producers closely guard their innovations\
    \ because trade secrets are a key component of business competitiveness and success\
    \ [\\[86,](#page-14-3) [143\u2013](#page-15-22)[145\\]](#page-15-23). Producers\
    \ who build standards-compliant chips are competing in a commodity market, so\
    \ they seek profitability though economies of scale [\\[146](#page-15-24)[\u2013\
    149\\]](#page-15-25). Each producer develops and uses home-grown, highly-optimized\
    \ design, manufacturing, and testing processes that amortize costs in high volume\
    \ production, thereby maximizing per-chip profit margins.\n\nAs a result, DRAM\
    \ producers publish only what information DRAM standards require, such as access\
    \ timing specifications and circuit characteristics needed for chip integration.\
    \ Additional information not specified by standards (e.g., internal circuit designs,\
    \ chip error rates) is kept in-house. Although such details can often be inferred\
    \ through reverse-engineering studies [\\[16,](#page-13-28) [23,](#page-13-11)\
    \ [24,](#page-13-31) [28,](#page-13-13) [32,](#page-13-14) [40,](#page-13-30)\
    \ [74,](#page-14-1) [78,](#page-14-2) [92,](#page-14-4) [114,](#page-15-3) [117,](#page-15-26)\
    \ [150](#page-15-27)[\u2013155\\]](#page-15-28) and chip teardowns [\\[156,](#page-15-29)\
    \ [157\\]](#page-15-30) (discussed further in Section [4\\)](#page-5-0), producers\
    \ have no obligation to communicate this information to consumers.\n\n#### <span\
    \ id=\"page-2-1\"></span>2.3. Using Non-Commodity DRAM Chips\n\nConsumers can\
    \ use non-commodity DRAM chips by either (i) privately working with DRAM producers\
    \ to build customized chips or (ii) buying specialized or otherwise domain-optimized\
    \ DRAM chips (e.g., high reliability [\\[158,](#page-15-31) [159\\]](#page-15-32),\
    \ low latency [\\[160\\]](#page-15-33)). These chips are generally still compliant\
    \ with JEDEC standards, though they may provide additional unstandardized features.\n\
    \nDeveloping and using non-commodity DRAM can benefit industry-wide standards.\
    \ For example, the HBM standard (JESD235 [\\[35\\]](#page-13-17)) largely grew\
    \ from private industry collaborations between AMD and SK Hynix [\\[161\\]](#page-15-34).\
    \ Similarly, features pioneered in non-commodity DRAM can be integrated into newer\
    \ standards.\n\nUnfortunately, innovating through non-commodity DRAM is a slow\
    \ and costly process because it forgoes the advantages of mainstream DRAM chips.\
    \ Non-commodity chips are typically feasible only for consumers in specific industries\
    \ (e.g., imaging, networking [\\[162\\]](#page-15-35)) or with significant stake\
    \ in the global DRAM market (e.g., large-scale cloud vendors). Our work takes\
    \ inspiration from non-commodity DRAM to enable all consumers to pursue such innovations,\
    \ ultimately benefitting the DRAM industry as a whole.\n\n#### <span id=\"page-3-5\"\
    ></span>2.4. Creating or Modifying DRAM Standards\n\nChanges to DRAM standards\
    \ require participation from all stakeholders throughout the DRAM industry, including\
    \ JEDEC personnel, DRAM producers, and DRAM consumers. Therefore, making changes\
    \ is a slow process that can involve nontechnical elements, such as political\
    \ motivations and business goals [\\[163\\]](#page-15-36). Major changes typically\
    \ follow one of three different paths. First, a standards committee comprising\
    \ experts from all stakeholders may directly draft a new standard. Second, new\
    \ standards may grow out of the development and use of non-commodity DRAM as discussed\
    \ in Section [2.3.](#page-2-1) Third, existing standards may be updated or supplemented\
    \ by JEDEC committees for special issues, such as LPDDR4X [\\[164\\]](#page-16-0),\
    \ 3Dstacked DRAM [\\[165\\]](#page-16-1), and RowHammer [\\[42,](#page-13-24)\
    \ [43\\]](#page-13-32).\n\n## <span id=\"page-3-1\"></span>3. Challenges of DRAM\
    \ Technology Scaling\n\nDRAM's primary competitive advantage is its low cost-percapacity\
    \ [\\[3](#page-13-2)[\u20136\\]](#page-13-3), which DRAM producers maintain by\
    \ continually improving chip storage densities. Doing so across successive product\
    \ generations requires carefully balancing shrinking physical feature sizes, optimizing\
    \ circuit areas, and mitigating worsening memory errors [\\[38,](#page-13-22)\
    \ [86,](#page-14-3) [166\u2013](#page-16-2)[170\\]](#page-16-3).\n\nUnfortunately,\
    \ today's DRAM faces two key challenges to continued technology scaling: (i) the\
    \ slowdown of generational improvements to storage capacity, access latency, and\
    \ power consumption [\\[13,](#page-13-33) [17,](#page-13-34) [171\\]](#page-16-4);\
    \ and (ii) the breakdown of conventional approaches to mitigate memory errors.\
    \ This section reviews both challenges in detail.\n\n#### <span id=\"page-3-0\"\
    ></span>3.1. Slowdown of Generational Improvements\n\nAdvancements in DRAM chip\
    \ storage density have been central to increasing demands for memory capacity\
    \ since the inception of DRAM technology [\\[9,](#page-13-6) [172\\]](#page-16-5).\
    \ Today's emerging data-intensive applications and systems in domains such as\
    \ AI, cloud, and HPC continue to demand greater memory capacity at an unprecedented\
    \ scale [\\[173\u2013](#page-16-6)[177\\]](#page-16-7). Unfortunately, technology\
    \ shrinkage throughout the past two decades has yielded diminishing benefits for\
    \ chip storage capacity, access latency, and refresh overheads because of the\
    \ growing costs and overheads of maintaining reliable chip operation at smaller\
    \ technology node sizes [\\[10,](#page-13-7) [11\\]](#page-13-8).\n\nTo better\
    \ understand this slowdown of generational improvements, we survey manufacturer-reported\
    \ DRAM chip capacities, access timings, and current consumption characteristics\
    \ given by 58 publicly-available DRAM chip datasheets from across 19 different\
    \ DRAM manufacturers with datasheet publication dates between 1970 and 2021.[4](#page-3-2)\
    \ The remainder of this section individually analyzes chip capacity, access timings,\
    \ and refresh overheads in the context of our survey.\n\n3.1.1. Chip Storage Capacity.\
    \ Figure [1](#page-3-3) shows the time evolution of per-chip storage capacity\
    \ and four key DRAM operating timings (all shown in log scale). We observe that\
    \ storage capacity has grown exponentially over time alongside improvements to\
    \ all four timing parameters (timings are discussed in Section [3.1.2\\)](#page-3-4).\
    \ However, storage capacity growth has slowed down markedly since 2010, dropping\
    \ from an exponential growth factor of 0.341 per year for 1970-2000 to 0.278 for\
    \ 2000-2020. This is consistent with recent challenges in scaling beyond 16 Gb\
    \ chip densities, and this slowdown is expected to continue going forward [\\\
    [11,](#page-13-8) [178\\]](#page-16-8).\n\n<span id=\"page-3-3\"></span>![](_page_3_Figure_11.jpeg)\n\
    \nFigure 1: Semi-log plot showing the evolution of key DRAM access timings (left)\
    \ and per-chip storage capacity (right)<sup>i</sup> across each 5-year period\
    \ of time.\n\n| i<br>JEDEC-standardized parameters[34] found in DRAM chip datasheets:\
    \ |            |\n|-----------------------------------------------------------------------|------------|\n\
    | Parameter                                                             | Definition\
    \ |\n\n| tRCD        | minimum row activation to column operation delay |\n|-------------|--------------------------------------------------|\n\
    | CAS Latency | read operation to data access latency            |\n| tRAS   \
    \     | minimum row activation to precharge delay        |\n| tRC         | minimum\
    \ delay between accesses to different rows |\n|             |                \
    \                                  |\n\n<span id=\"page-3-4\"></span>3.1.2. Chip\
    \ Access Latency. DRAM access latency has not significantly improved relative\
    \ to storage capacity over the last two decades [\\[12](#page-13-9)[\u201314,](#page-13-35)\
    \ [16\u2013](#page-13-28)[21\\]](#page-13-36). This makes DRAM an increasingly\
    \ significant system performance bottleneck today, especially for workloads with\
    \ large footprints that are sensitive to DRAM access latency [\\[12,](#page-13-9)\
    \ [77,](#page-14-5) [111,](#page-15-1) [112,](#page-15-37) [179](#page-16-9)[\u2013\
    202\\]](#page-16-10). Therefore, there is significant opportunity for improving\
    \ overall system performance by reducing the memory access latency [\\[16,](#page-13-28)\
    \ [22,](#page-13-10) [52,](#page-13-37) [59,](#page-14-6) [74,](#page-14-1) [77,](#page-14-5)\
    \ [78,](#page-14-2) [82,](#page-14-7) [106,](#page-15-38) [109,](#page-15-39)\
    \ [110,](#page-15-0) [203,](#page-16-11) [204\\]](#page-16-12). Although conventional\
    \ latencyhiding techniques (e.g., caching, prefetching, multithreading) can potentially\
    \ help mitigate many of the performance concerns, these techniques (1) fundamentally\
    \ do not change the\n\n<span id=\"page-3-2\"></span><sup>4</sup>This data encompasses\
    \ DRAM chips from both asynchronous (e.g., page mode, extended data out) and synchronous\
    \ (e.g., SDRAM, DDRn) DRAM chips. Appendix [A](#page-21-0) describes our data\
    \ collection methodology in further detail, and Appendix [B](#page-24-0) provides\
    \ an overview of our dataset, which is publicly available on GitHub [\\[136\\\
    ]](#page-15-15).\n\nlatency of each memory access and (2) fail to work in many\
    \ cases (e.g., irregular memory access patterns, random accesses, huge memory\
    \ footprints).\n\nFigure [1](#page-3-3) shows that none of the four timings we\
    \ study have improved significantly in the last two decades. For example, the\
    \ median tRCD/CAS Latency/tRAS/tRC reduced by 2.66/3.11/2.89/2.89% per year on\
    \ average between 1970 and 2000, but only 0.81/0.97/1.33/1.53% between 2000 and\
    \ 2015.[5](#page-4-1) This is consistent with similar studies in prior work [\\\
    [12\u2013](#page-13-9)[21\\]](#page-13-36).\n\n<span id=\"page-4-5\"></span>3.1.3.\
    \ Worsening Refresh Overheads. The circuits that DRAM uses to store data are inherently\
    \ susceptible to a wide range of different leakage mechanisms (e.g., capacitor\
    \ charge leakage), which ultimately cause data loss if ignored. To prevent this,\
    \ DRAM standards mandate periodic refresh operations that intermittently restore\
    \ data values throughout the entire DRAM chip. Unfortunately, DRAM refresh incurs\
    \ significant system performance and power overheads [\\[49,](#page-13-29) [69,](#page-14-8)\
    \ [80,](#page-14-9) [82,](#page-14-7) [86,](#page-14-3) [89,](#page-14-10) [104,](#page-14-11)\
    \ [205](#page-16-13)[\u2013207\\]](#page-16-14), sacrificing almost half of the\
    \ total memory throughput and consuming almost half of the total DRAM power for\
    \ projected 64 Gb chips [\\[80\\]](#page-14-9).\n\nFigure [2](#page-4-2) illustrates\
    \ the performance overheads of DRAM refresh across the different DRAM chips in\
    \ our datasheet survey. The data shows the refresh penalty, [6](#page-4-3) which\
    \ is defined as the ratio of two key timing parameters used to govern refresh\
    \ operations: tRF C , the duration of each refresh command, and tREF I , the time\
    \ between consecutive refresh commands. The refresh penalty represents the average\
    \ time that a DRAM rank (or bank) is unavailable for access due to refresh operations\
    \ [\\[208,](#page-16-15) [210\u2013](#page-16-16)[213\\]](#page-16-17).\n\n<span\
    \ id=\"page-4-2\"></span>![](_page_4_Figure_4.jpeg)\n\nFigure 2: Refresh penalty\
    \ (computed as the ratio between tRF C and tREF I ) for DRAM chips of different\
    \ storage capacities.\n\nWe observe that the refresh penalty worsens from a median\
    \ of 1.04% for 1 Kib chips to 2.05% for 16 Kib chips, then improves to 0.43% for\
    \ 128 Mib chips, and finally worsens to a median of 4.48% (worst-case of 7.56%\
    \ for DDR5 chips) for 16 Gib chips.This non-monotonic trend is due to the relative\
    \ rates of improvement in DRAM access latency and storage capacity: access (and\
    \ therefore, refresh) latencies stagnated around the introduction of 128 Mib chips\
    \ while capacity improvements did not. This data is consistent with both (i) our\
    \ more detailed\n\nanalysis of DRAM refresh timings in Appendix [A.4](#page-23-0)\
    \ and (ii) trends observed in prior work [\\[21,](#page-13-36) [80,](#page-14-9)\
    \ [94,](#page-14-12) [206,](#page-16-19) [214,](#page-16-20) [215\\]](#page-16-21),\
    \ which expect that future, higher-capacity DRAM chips will spend an even larger\
    \ proportion of time refreshing unless the DRAM refresh algorithm and its implementation\
    \ are improved.\n\n#### <span id=\"page-4-0\"></span>3.2. Breakdown of the Separation\
    \ of Concerns\n\nDRAM is susceptible to a variety of error mechanisms that worsen\
    \ with technology scaling and can impact system-wide reliability if not carefully\
    \ managed. Today's separation of concerns largely puts the burden to address these\
    \ errors on DRAM producers, i.e., to hide them from consumers and provide the\
    \ illusion of an error-free memory chip. Unfortunately, two classes of scaling-related\
    \ errors are breaking through the separation to impact consumers regardless: random,\
    \ single-bit errors and RowHammer errors. This section discusses these errors\
    \ and how they break the separation of concerns.\n\n<span id=\"page-4-7\"></span>3.2.1.\
    \ Worsening Reliability. DRAM suffers from a range of circuit-level error mechanisms\
    \ that can impact consumers (e.g., data loss, system failure) if mismanaged [\\\
    [84,](#page-14-13) [216\u2013](#page-16-22)[221\\]](#page-17-0). To mitigate these\
    \ errors on-chip, DRAM producers typically use a combination of conservative operating\
    \ timings (e.g., with added safety margins) and simple error-correcting hardware\
    \ within the DRAM chip (e.g., manufacturing spare rows and columns to replace\
    \ faulty ones [\\[33,](#page-13-15) [34,](#page-13-16) [216,](#page-16-22) [222,](#page-17-1)\
    \ [223\\]](#page-17-2)).[7](#page-4-4) These low-cost techniques enable DRAM producers\
    \ to provide the illusion of a fully reliable DRAM chip, thereby preserving the\
    \ separation of concerns between producers and consumers.\n\nIn contrast, modern\
    \ DRAM chips exhibit much higher error rates because technology scaling exacerbates\
    \ the underlying circuit-level error mechanisms that cause errors [\\[86,](#page-14-3)\
    \ [142,](#page-15-21) [143,](#page-15-22) [166,](#page-16-2) [167,](#page-16-23)\
    \ [224,](#page-17-3) [225\\]](#page-17-4). To combat these errors, DRAM producers\
    \ use stronger error-mitigation mechanisms in modern DRAM chips (e.g., on-die\
    \ ECC [\\[34,](#page-13-16) [38,](#page-13-22) [40,](#page-13-30) [41,](#page-13-23)\
    \ [87,](#page-14-14) [115,](#page-15-40) [119,](#page-15-5) [167,](#page-16-23)\
    \ [224,](#page-17-3) [226](#page-17-5)[\u2013230\\]](#page-17-6), post-package\
    \ repair [\\[33,](#page-13-15) [34,](#page-13-16) [216,](#page-16-22) [222,](#page-17-1)\
    \ [223\\]](#page-17-2), target row refresh [\\[23,](#page-13-11) [32,](#page-13-14)\
    \ [45,](#page-13-26) [46\\]](#page-13-38), refresh management [\\[34,](#page-13-16)\
    \ [47\\]](#page-13-39)), which are more expensive and incur higher performance\
    \ and energy overheads.\n\nUnfortunately, worsening memory reliability remains\
    \ a serious problem for DRAM consumers, especially high-volume consumers for whom\
    \ even modest chip error rates are significant at scale [\\[142,](#page-15-21)\
    \ [221\\]](#page-17-0). Although stronger in-DRAM error mitigations are effective\
    \ against growing error rates [\\[142,](#page-15-21) [224\\]](#page-17-3), they\
    \ introduce new overheads and challenges for consumers. For example, neither on-die\
    \ ECC nor target row refresh correct all errors, and the remaining errors (e.g.,\
    \ uncorrectable errors) are difficult for consumers to predict and mitigate because\
    \ their manifestation depends on the particular on-die ECC and/or TRR mechanism\
    \ used by a given chip [\\[23,](#page-13-11) [32,](#page-13-14) [38,](#page-13-22)\
    \ [40,](#page-13-30) [41,](#page-13-23) [46,](#page-13-38) [115,](#page-15-40)\
    \ [152,](#page-15-41) [229,](#page-17-7) [231\\]](#page-17-8). As a result, DRAM\
    \ consumers face errors that are growing in both magnitude and complexity, making\
    \ reliability a key design concern for continued DRAM scaling.\n\n<span id=\"\
    page-4-6\"></span>3.2.2. The RowHammer Vulnerability. RowHammer [\\[25,](#page-13-19)\
    \ [26,](#page-13-40) [29,](#page-13-20) [37,](#page-13-21) [232\u2013](#page-17-9)[234\\\
    ]](#page-17-10) is a scaling-related read-disturb phenomenon\n\n<span id=\"page-4-1\"\
    ></span><sup>5</sup>We report 2015 instead of 2020 because 2020 shows a regression\
    \ in CAS latency due to first-generation DDR5 chips, which we believe is not representative\
    \ because of its immature technology.\n\n<span id=\"page-4-3\"></span><sup>6</sup>Also\
    \ referred to as refresh overhead [\\[208\\]](#page-16-15) and refresh duty cycle\
    \ [\\[209\\]](#page-16-18).\n\n<span id=\"page-4-4\"></span><sup>7</sup>Enterprise-class\
    \ computing providers (e.g., cloud, HPC) may use supplementary error-mitigation\
    \ mechanisms discussed further in Section [4.4.1.](#page-7-1)\n\naffecting modern\
    \ DRAM chips in which memory accesses to a given physical memory location can\
    \ induce bit-flips at other locations. Significant work [\\[23](#page-13-11)[\u2013\
    26,](#page-13-40) [102,](#page-14-15) [235\u2013](#page-17-11)[258\\]](#page-17-12)\
    \ shows that RowHammer is a security threat that can be exploited to exfiltrate\
    \ sensitive data, escalate user privileges, or otherwise compromise a system.\
    \ Because RowHammer is fundamental to DRAM circuit designs, it is a problem for\
    \ all DRAM-based systems and becomes significantly worse with continued process\
    \ technology scaling [\\[25,](#page-13-19) [26,](#page-13-40) [28,](#page-13-13)\
    \ [29\\]](#page-13-20).\n\nModern DRAM partially addresses RowHammer through best\
    \ practices laid out in DRAM specifications [\\[34,](#page-13-16) [42,](#page-13-24)\
    \ [43\\]](#page-13-32) and RowHammer mitigation mechanisms built into DRAM chips\
    \ by producers, including target row refresh (TRR) [\\[23,](#page-13-11) [28,](#page-13-13)\
    \ [32,](#page-13-14) [258\u2013](#page-17-12)[261\\]](#page-17-13) and refresh\
    \ management (RFM) [\\[34,](#page-13-16) [47\\]](#page-13-39). Unfortunately,\
    \ neither approach suffices to provide fully secure solutions against RowHammer\
    \ [\\[23,](#page-13-11) [32,](#page-13-14) [48,](#page-13-27) [118,](#page-15-4)\
    \ [258,](#page-17-12) [262\\]](#page-17-14). Therefore, RowHammer remains a serious\
    \ consumer-visible problem that challenges the illusion of a robust DRAM chip\
    \ established by the separation of concerns.\n\n## <span id=\"page-5-0\"></span>4.\
    \ Challenges in Overcoming Scaling Problems\n\nDeveloping new solutions to address\
    \ the DRAM scaling challenges discussed in Section [3](#page-3-1) requires creative\
    \ thinking that today's separation of concerns constrains. This section explores\
    \ new techniques based on system-memory cooperation that holistically address\
    \ DRAM scaling challenges with the help of hardware and software mechanisms at\
    \ the system level. We review techniques that target each of the four key DRAM\
    \ scaling challenges discussed in Section [3:](#page-3-1) access latency, refresh\
    \ overheads, RowHammer, and worsening reliability. In each case study, we survey\
    \ relevant prior work to understand how and why today's separation of concerns\
    \ discourages systemmemory cooperative solutions.\n\n#### <span id=\"page-5-1\"\
    ></span>4.1. Study 1: Long DRAM Access Latency\n\nAs Section [3.1.2](#page-3-4)\
    \ discusses, slow generational improvements to DRAM access latency make DRAM a\
    \ worsening bottleneck for latency-sensitive workloads today [\\[12,](#page-13-9)\
    \ [77,](#page-14-5) [111,](#page-15-1) [112,](#page-15-37) [179\u2013](#page-16-9)[202\\\
    ]](#page-16-10). System-memory cooperation promises to overcome this problem by\
    \ directly reducing the DRAM access latency beyond what commodity DRAM chips provide.\n\
    \n4.1.1. Solutions to Reduce the DRAM Access Latency. Prior works have taken two\
    \ major directions to improve the DRAM access latency. First, many works [\\[16,](#page-13-28)\
    \ [22,](#page-13-10) [52,](#page-13-37) [59,](#page-14-6) [74,](#page-14-1) [77,](#page-14-5)\
    \ [78,](#page-14-2) [82,](#page-14-7) [106,](#page-15-38) [109,](#page-15-39)\
    \ [110\\]](#page-15-0) show that the average DRAM access latency can be shortened\
    \ by reducing DRAM access timings for particular memory locations that can tolerate\
    \ faster accesses. This can be done safely because, although DRAM standards call\
    \ for constant access timings across all memory locations, the minimum viable\
    \ access timings that the hardware can support actually differ across memory locations\
    \ due to factors such as heterogeneity in the circuit design [\\[17,](#page-13-34)\
    \ [78\\]](#page-14-2) and manufacturing process variation in circuit components\
    \ [\\[16,](#page-13-28) [22,](#page-13-10) [52,](#page-13-37) [74,](#page-14-1)\
    \ [263\\]](#page-17-15).\n\nExploiting these variations in access timings to reduce\
    \ the average memory access latency provides significant system performance improvement.\
    \ For example, Chang et al. [\\[16\\]](#page-13-28) experimentally show that exploiting\
    \ access latency variations provides an average 8-core system performance improvement\
    \ of 13.3%/17.6%/19.5% for real DRAM chips from three major DRAM manufacturers.\
    \ Similarly, Kim et al. [\\[74\\]](#page-14-1) show that exploiting access latency\
    \ variations induced by DRAM sense amplifiers provides an average (maximum) 4-core\
    \ system performance improvement of 4.97% (8.79%) versus using default DRAM access\
    \ timings based on data obtained from 282 commodity LPDDR4 DRAM chips.\n\nSecond,\
    \ other works [\\[55,](#page-14-16) [58,](#page-14-17) [73,](#page-14-0) [75,](#page-14-18)\
    \ [90,](#page-14-19) [91,](#page-14-20) [96\u2013](#page-14-21)[100,](#page-14-22)\
    \ [264\u2013](#page-17-16)[270\\]](#page-17-17) show that commodity DRAM can perform\
    \ massively-parallel computations (e.g., at the granularity of an 8 KiB DRAM row)\
    \ by exploiting the underlying analog behavior of DRAM operations (e.g., charge\
    \ sharing between cells). These works show that such computations can significantly\
    \ improve overall system performance and energy-efficiency by both (1) reducing\
    \ the amount of data transferred between the processor and DRAM and (2) exploiting\
    \ the relatively high throughput of rowgranularity operations. For example, Gao\
    \ et al. [\\[55\\]](#page-14-16) show that in-DRAM 8-bit vector addition is 9.3\xD7\
    \ more energy-efficient than the same computation in the processor, primarily\
    \ due to avoiding the need for off-chip data transfers. Similarly, Olgun et al.\
    \ [\\[90\\]](#page-14-19) experimentally demonstrate that in-DRAM copy and initialization\
    \ techniques can improve the performance of system-level copy and initialization\
    \ operations by 12.6\xD7 and 14.6\xD7, respectively.\n\n<span id=\"page-5-2\"\
    ></span>4.1.2. Application to Today's Commodity DRAM Chips. Unfortunately, both\
    \ reducing DRAM access timings and exploiting DRAM's massively-parallel analog\
    \ behavior are discouraged by today's separation of concerns. In both cases, new\
    \ DRAM access timings must be determined that ensure new or modified DRAM operations\
    \ can be performed predictably and reliably under all operating conditions.\n\n\
    To identify new access timings, the majority of prior works [\\[13,](#page-13-33)\
    \ [16,](#page-13-28) [17,](#page-13-34) [22,](#page-13-10) [52,](#page-13-37)\
    \ [55,](#page-14-16) [73,](#page-14-0) [75,](#page-14-18) [91,](#page-14-20) [171,](#page-16-4)\
    \ [263,](#page-17-15) [271](#page-17-18)[\u2013273\\]](#page-18-0) perform extensive\
    \ experimental characterization studies across many (e.g., hundreds or thousands\
    \ of) DRAM chips. These studies account for three primary sources of variation\
    \ that affect the access timings of a given memory location. First, process variation\
    \ introduces random variations between DRAM chip components (e.g., cells, rows,\
    \ columns). Second, a manufacturer's particular circuit design introduces structural\
    \ variation (called design-induced variation [\\[78\\]](#page-14-2)) that deterministically\
    \ affects access timings based on a component's location in the overall DRAM design\
    \ (e.g., cells along the same bitline [\\[74\\]](#page-14-1), cells at the borders\
    \ of internal storage arrays [\\[78\\]](#page-14-2)). Third, the charge level\
    \ of a DRAM cell varies over time due to leakage and the effects of DRAM accesses\
    \ [\\[59,](#page-14-6) [198\\]](#page-16-24). Experimentally determining the new\
    \ predictable and reliable access timings requires properly accounting for all\
    \ three sources of variation under all operating conditions.\n\nFor a typical\
    \ DRAM consumer, determining new access timings using a custom DRAM testing methodology\
    \ is impractical without assistance from DRAM producers. Choosing runtime (e.g.,\
    \ data and access patterns) and environmental (e.g., temperature, voltage) testing\
    \ conditions in a meaningful way requires understanding the error mechanisms involved\
    \ in timingrelated errors [\\[68\\]](#page-14-23), including (but not limited\
    \ to) microarchitectural design details, such as internal substructure dimensions\
    \ (e.g., subarray sizing) [\\[74,](#page-14-1) [78\\]](#page-14-2), the correspondence\
    \ between logical DRAM bus addresses and physical cell locations [\\[16,](#page-13-28)\
    \ [22,](#page-13-10) [67\\]](#page-14-24), and the order of rows refreshed by\
    \ each auto-refresh operation [\\[198\\]](#page-16-24). Therefore, consumers who\
    \ lack trustworthy access to this information are discouraged from exploring improvements\
    \ to the commodity DRAM access latency.\n\n### <span id=\"page-6-0\"></span>4.2.\
    \ Study 2: DRAM Refresh Overheads\n\nDRAM refresh overheads are a key design concern\
    \ in modern systems as discussed in Section [3.1.3.](#page-4-5) System-memory\
    \ cooperation can overcome this problem by eliminating or otherwise mitigating\
    \ unnecessary refresh operations, thereby improving overall system performance\
    \ and energy efficiency.\n\n4.2.1. Solutions to Reduce DRAM Refresh Overheads.\
    \ Prior works [\\[56,](#page-14-25) [61,](#page-14-26) [64\u2013](#page-14-27)[70,](#page-14-28)\
    \ [79,](#page-14-29) [80,](#page-14-9) [82,](#page-14-7) [86,](#page-14-3) [89,](#page-14-10)\
    \ [92,](#page-14-4) [94,](#page-14-12) [103,](#page-14-30) [104\\]](#page-14-11)\
    \ address refresh overheads by exploiting the fact that most refresh operations\
    \ are unnecessary.[8](#page-6-2) The standardized DRAM refresh algorithm refreshes\
    \ all cells frequently (i.e., at the worst-case rate) to simplify DRAM refresh\
    \ and guarantee correctness. However, each cell's data retention characteristics\
    \ vary significantly due to a combination of data-dependence [\\[65,](#page-14-31)\
    \ [67,](#page-14-24) [68,](#page-14-23) [92,](#page-14-4) [150\\]](#page-15-27)\
    \ and process variation [\\[80,](#page-14-9) [86,](#page-14-3) [104,](#page-14-11)\
    \ [143,](#page-15-22) [150,](#page-15-27) [276,](#page-18-1) [277\\]](#page-18-2).\
    \ As a result, eliminating unnecessary refresh operations provides significant\
    \ power reduction and performance improvement. For example, Liu et al. [\\[80\\\
    ]](#page-14-9) demonstrate an average energyper-access and system performance\
    \ improvement of 8.3% and 4.1%, respectively, for 4 Gib chips (49.7% and 107.9%\
    \ for 64 Gib chips) when relaxing the refresh rate at the row granularity. Therefore,\
    \ reducing refresh overheads potentially benefits all DRAM-based systems.\n\n\
    <span id=\"page-6-4\"></span>4.2.2. Application to Today's Commodity DRAM Chips.\
    \ Reducing unnecessary refresh operations in commodity DRAM chips comprises two\
    \ key steps. First, the memory controller must reduce the frequency of periodic\
    \ refresh operations. This is achievable (though not necessarily supported to\
    \ arbitrary values) using commodity DRAM chips because the memory controller manages\
    \ DRAM refresh timings. For example, the memory controller might relax the rate\
    \ at which it issues refresh operations to half of the DDRn standard of 3.9 or\
    \ 7.8 \xB5s, which is supported by standards at extended temperature ranges [\\\
    [33,](#page-13-15) [34,](#page-13-16) [44,](#page-13-25) [138,](#page-15-17) [139\\\
    ]](#page-15-18), or even to over an order of magnitude less often [\\[64,](#page-14-27)\
    \ [80,](#page-14-9) [86,](#page-14-3) [103\\]](#page-14-30).\n\nSecond, the system\
    \ must mitigate any errors that may occur within the small number of DRAM cells\
    \ that require frequent refreshing. Doing so requires using either (i) additional\
    \ operations to mitigate data loss (e.g., issuing extra row activations to\n\n\
    those cells' rows [\\[80\\]](#page-14-9)) or (ii) supplementary error-mitigation\
    \ mechanisms within processor (e.g., ECC [\\[94\\]](#page-14-12) and/or bit-repair\
    \ techniques [\\[79,](#page-14-29) [86,](#page-14-3) [103\\]](#page-14-30)). Although\
    \ both approaches can introduce new performance and energy overheads, the benefits\
    \ of reducing unnecessary refresh operations outweigh the overheads introduced\
    \ [\\[56,](#page-14-25) [79,](#page-14-29) [80,](#page-14-9) [86,](#page-14-3)\
    \ [89,](#page-14-10) [92,](#page-14-4) [94,](#page-14-12) [103,](#page-14-30)\
    \ [104,](#page-14-11) [278\\]](#page-18-5). For example, Liu et al. [\\[80\\]](#page-14-9)\
    \ project that DRAM refresh overheads cause a 187.6% increase in the energy-per\
    \ access and a 63.7% system performance degradation for 64 Gib chips. By reducing\
    \ the overall number of DRAM refresh operations, the authors' proposal (RAIDR)\
    \ mitigates these overheads by 49.7% and 107.9%, respectively.\n\nUnfortunately,\
    \ this second step is difficult for the average DRAM consumer because it requires\
    \ a trustworthy method for discriminating DRAM cells' data retention characteristics.\
    \ These characteristics vary with both the DRAM chip circuit design (e.g., random\
    \ cell-to-cell variations, locations of true and anti-cells [\\[115,](#page-15-40)\
    \ [117,](#page-15-26) [150\\]](#page-15-27)) and usage characteristics (e.g.,\
    \ operating conditions such as voltage and temperature, workload access patterns),\
    \ so identifying which cells to refresh more often requires access to both internal\
    \ knowledge of a given DRAM chip and how the chip's end use will impact data retention.\
    \ Prior works propose reliability testing [\\[65,](#page-14-31) [67,](#page-14-24)\
    \ [79,](#page-14-29) [80,](#page-14-9) [82,](#page-14-7) [92\\]](#page-14-4) and\
    \ monitoring [\\[94,](#page-14-12) [229,](#page-17-7) [279,](#page-18-6) [280\\\
    ]](#page-18-7) techniques to work around the lack of this knowledge, which the\
    \ separation of concerns hides from DRAM consumers. Ultimately, this means that\
    \ system-memory cooperation to improve the standardized DRAM refresh algorithm\
    \ are discouraged today.\n\n#### <span id=\"page-6-1\"></span>4.3. Study 3: RowHammer\
    \ Mitigation\n\nSection [3.2.2](#page-4-6) discusses the severity of the RowHammer\
    \ vulnerability, motivating the need for secure defenses beyond those currently\
    \ implemented. System-memory cooperative mechanisms are capable of supplementing\
    \ these defenses from outside of the DRAM chip to provide strong security without\
    \ requiring changes to DRAM chip hardware. Such a solution is attractive for a\
    \ system designer with interest in building a secure system because the designer\
    \ can guarantee correctness using their own methods rather than taking the word\
    \ of external parties [\\[45,](#page-13-26) [46\\]](#page-13-38).\n\n<span id=\"\
    page-6-3\"></span>4.3.1. Solutions to Securely Mitigate RowHammer. We classify\
    \ secure RowHammer mitigations into four groups based on categorization by recent\
    \ work [\\[107\\]](#page-15-42).\n\n- 1. Access-agnostic mitigation hardens a\
    \ DRAM chip against RowHammer independently of the memory access pattern. This\
    \ includes increasing the overall DRAM refresh rate [\\[29,](#page-13-20) [281,](#page-18-8)\
    \ [282\\]](#page-18-9) and memory-wide error correction and/or integrity-checking\
    \ mechanisms such as strong ECC [\\[29,](#page-13-20) [45,](#page-13-26) [260\\\
    ]](#page-17-19). These mechanisms are algorithmically simple but can introduce\
    \ significant system hardware, performance, and/or energy-efficiency overheads\
    \ (e.g., a large number of additional refresh operations [\\[28,](#page-13-13)\
    \ [29,](#page-13-20) [207\\]](#page-16-14)).\n- 2. Proactive mitigations [\\[29,](#page-13-20)\
    \ [57,](#page-14-32) [85,](#page-14-33) [107\\]](#page-15-42) adjust the DRAM\
    \ access pattern to prevent the possibility of RowHammer errors.\n- 3. Physically\
    \ isolating mitigations [\\[50,](#page-13-41) [76,](#page-14-34) [95,](#page-14-35)\
    \ [102,](#page-14-15) [245,](#page-17-20) [283\u2013](#page-18-10)[287\\]](#page-18-11)\
    \ physically separate data such that accesses to one portion\n\n<span id=\"page-6-2\"\
    ></span><sup>8</sup>Latency-hiding techniques (e.g, prefetching, memory command\
    \ scheduling, on-chip caching, etc.) and parallelization of refresh and access\
    \ operations [\\[21,](#page-13-36) [211,](#page-16-25) [215,](#page-16-21) [274,](#page-18-3)\
    \ [275\\]](#page-18-4) help mitigate performance overheads but do not change the\
    \ total number of refresh operations issued. As a result, such techniques do not\
    \ reduce refresh energy consumption. These techniques are also imperfect in many\
    \ cases where latency-hiding is impractical (e.g., row conflicts between refresh\
    \ and access commands, larger memory footprints than available caching resources)\
    \ [\\[208,](#page-16-15) [214,](#page-16-20) [215,](#page-16-21) [274\\]](#page-18-3).\n\
    \nof the data cannot cause RowHammer errors in another.\n\n4. Reactive mitigations\
    \ [\\[29,](#page-13-20) [34,](#page-13-16) [288\u2013](#page-18-12)[304\\]](#page-18-13)\
    \ identify symptoms of an ongoing RowHammer attack (e.g., excessive row activations)\
    \ and issue additional row activation or refresh operations to prevent bit-flips\
    \ from occurring.\n\nChoosing a secure RowHammer defense for a given system depends\
    \ on the system's particular threat model and the overheads (e.g., performance,\
    \ energy, hardware area, complexity) it can tolerate. For example, if DRAM is\
    \ accessible only through processor cores (e.g., peripherals are incapable of\
    \ direct memory access), secure defenses may be possible solely through on-chip\
    \ cache management [\\[305\\]](#page-18-14).\n\n<span id=\"page-7-7\"></span>4.3.2.\
    \ Application to Today's Commodity DRAM Chips. Unfortunately, implementing secure\
    \ RowHammer defenses is discouraged in today's DRAM chips because the separation\
    \ of concerns hides the mechanics of how RowHammer occurs from DRAM consumers.\
    \ The defenses discussed throughout Section [4.3.1](#page-6-3) all require understanding\
    \ one or more of a chip's RowHammer error characteristics, which are summarized\
    \ in Table [1.](#page-7-2) The first is known as HCfirst [\\[28,](#page-13-13)\
    \ [118\\]](#page-15-4) or RowHammer Threshold [\\[29,](#page-13-20) [107,](#page-15-42)\
    \ [306\\]](#page-18-15), which describes the worst-case number of RowHammer memory\
    \ accesses required to induce a bit-flip. The second is known as the blast radius\
    \ [\\[28,](#page-13-13) [29\\]](#page-13-20), which describes how many rows are\
    \ affected by hammering a single row. The third is the DRAM's internal physical\
    \ row address mapping [\\[29,](#page-13-20) [307\\]](#page-18-16), which is used\
    \ to identify the locations of victim rows.\n\n<span id=\"page-7-2\"></span>\n\
    \n|                      | Required Information |              |             |\n\
    |----------------------|----------------------|--------------|-------------|\n\
    | Strategy             | HCfirst              | Blast Radius | Row Mapping |\n\
    | Access-Agnostic      | \u2713                    |              |          \
    \   |\n| Proactive            | \u2713                    | \u2713           \
    \ |             |\n| Physically Isolating | \u2713                    | \u2713\
    \            | \u2713           |\n| Reactive             | \u2713           \
    \         | \u2713            | \u2713           |\n\nTable 1: Information needed\
    \ by each of the four RowHammermitigation strategies.\n\nAll three RowHammer error\
    \ characteristics vary between DRAM manufacturers, chips, and cells based on a\
    \ combination of random process variation, a chip's particular circuit design\
    \ (including yield-management techniques such as postmanufacturing repair, target\
    \ row refresh, and error correcting codes), and operating conditions such as temperature\
    \ and voltage [\\[28\u2013](#page-13-13)[31,](#page-13-42) [118,](#page-15-4)\
    \ [152,](#page-15-41) [300,](#page-18-17) [308](#page-18-18)[\u2013311\\]](#page-18-19).\n\
    \nWithout trustworthy access this information, DRAM consumers are discouraged\
    \ from adopting secure RowHammer defenses. To work around this limitation, proposals\
    \ for secure RowHammer defenses conduct extensive experimental testing to estimate\
    \ RowHammer error characteristics that are needed to design and/or configure their\
    \ proposals. Unfortunately, similar to efforts that improve DRAM access latency\
    \ and refresh timings discussed in Sections [4.2.2](#page-6-4) and [4.1.2,](#page-5-2)\
    \ deploying these methods in practice is impractical for most consumers. These\
    \ observations are consistent with prior works [\\[45,](#page-13-26) [46,](#page-13-38)\
    \ [231\\]](#page-17-8) that discuss the difficulty in practically determining\
    \ and relying on this information without support from DRAM manufacturers.\n\n\
    #### <span id=\"page-7-0\"></span>4.4. Study 4: Improving Memory Reliability\n\
    \nMain memory reliability is a key system design concern because memory errors\
    \ can cause data loss or system failure if mismanaged (discussed further in Section\
    \ [3.2.1\\)](#page-4-7). Systemmemory cooperation can supplement the memory chip\
    \ with additional mechanisms to improve its base reliability beyond what producers\
    \ alone can provide.[9](#page-7-3)\n\n<span id=\"page-7-1\"></span>4.4.1. Solutions\
    \ to Improve Memory Reliability. Systemmemory cooperative solutions that DRAM\
    \ consumers can implement to improve memory reliability identify and/or address\
    \ memory errors before they impact the system at large. Hardware solutions include\
    \ rank-level ECC [\\[51,](#page-13-43) [53,](#page-13-44) [54,](#page-13-45) [62,](#page-14-36)\
    \ [63,](#page-14-37) [71,](#page-14-38) [72,](#page-14-39) [81,](#page-14-40)\
    \ [87,](#page-14-14) [93,](#page-14-41) [101,](#page-14-42) [105,](#page-14-43)\
    \ [108\\]](#page-15-43), rank-level ECC scrubbing [\\[94,](#page-14-12) [221,](#page-17-0)\
    \ [279,](#page-18-6) [280,](#page-18-7) [315,](#page-18-20) [315\u2013](#page-18-20)[319\\\
    ]](#page-18-21), and bit repair techniques [\\[79,](#page-14-29) [86,](#page-14-3)\
    \ [320](#page-18-22)[\u2013327\\]](#page-18-23). Software-based approaches include\
    \ retiring known-bad memory pages [\\[49,](#page-13-29) [60,](#page-14-44) [83,](#page-14-45)\
    \ [84,](#page-14-13) [88,](#page-14-46) [103\\]](#page-14-30), and predicting\
    \ failures [\\[328\u2013](#page-18-24) [333\\]](#page-19-0).\n\nThese solutions\
    \ all enable DRAM consumers to adapt unreliable DRAM chips to systems that require\
    \ reliable main memory at reasonable cost.[10](#page-7-4) For example, HOTH [\\\
    [321\\]](#page-18-25) supplements rank-level ECC with a cache-like hardware mechanism\
    \ to track faulty memory locations, enabling the system to detect and correct\
    \ one additional error for each ECC word (i.e., extend SECDED to 2EC3ED).\n\n\
    4.4.2. Application to Today's Commodity DRAM Chips. Unfortunately, exposing memory\
    \ errors outside of the DRAM chip is at best a gray area within the separation\
    \ of concerns between DRAM producers and consumers. Commodity DRAM is designed\
    \ to work for a wide variety of systems at a reasonable (albeit unspecified)[11](#page-7-5)\
    \ error rate. In general, a consumer who is concerned about the remaining errors\
    \ they may encounter must design and build their own solutions (i.e., outside\
    \ of the DRAM chip) to tolerate memory errors.[12](#page-7-6)\n\nHowever, these\
    \ solutions fundamentally rely on understanding how those errors might manifest\
    \ in the first place. Each error-mitigation mechanism targets a particular error\
    \ model, which defines the scope of the errors that it is designed to mitigate.\
    \ As a result, although a given mechanism efficiently mitigates errors within\
    \ its target error model, it may fail to do so if errors no longer fit the model.\
    \ In such cases, a different error-mitigation mechanism (or possibly, a combination\
    \ of multiple mechanisms) may be more suitable.\n\nFor example, a coarse-grained\
    \ approach such as page retirement [\\[49,](#page-13-29) [60,](#page-14-44) [83,](#page-14-45)\
    \ [84,](#page-14-13) [88,](#page-14-46) [103\\]](#page-14-30) efficiently mitigates\
    \ a small number\n\n<span id=\"page-7-3\"></span><sup>9</sup>These mechanisms\
    \ commonly fall under the umbrella of memory reliability, availability and serviceability\
    \ (RAS) features [\\[312](#page-18-26)[\u2013314\\]](#page-18-27).\n\n<span id=\"\
    page-7-4\"></span><sup>10</sup>Consumers with exceptional reliability needs, such\
    \ as those targeting extreme or hostile environments (e.g., military, automotive,\
    \ industrial, extraterrestrial), may take more extreme measures (e.g., custom\
    \ components [\\[158,](#page-15-31) [159,](#page-15-32) [334](#page-19-1)[\u2013\
    340\\]](#page-19-2), redundant resources [\\[93,](#page-14-41) [341,](#page-19-3)\
    \ [342\\]](#page-19-4)) to ensure that memory errors do not compromise their systems.\n\
    \n<span id=\"page-7-5\"></span><sup>11</sup>Academic works speculate that commodity\
    \ DRAM targets a bit error rate (BER) within the range of 10\u2212<sup>16</sup>\
    \ \u2212 10\u2212<sup>12</sup> [\\[86,](#page-14-3) [92,](#page-14-4) [321,](#page-18-25)\
    \ [343\\]](#page-19-5), but we are not aware of industry-provided values.\n\n\
    <span id=\"page-7-6\"></span><sup>12</sup>Even designers who adopt custom DRAM\
    \ solutions that sacrifice the cost advantages of commodity memory (e.g., high-reliability\
    \ DRAM [\\[158,](#page-15-31) [159\\]](#page-15-32)) may supplement the DRAM chips\
    \ with additional error-mitigation mechanisms outside of the DRAM chip.\n\nof\
    \ errors at fixed bit positions. However, page retirement exhibits significant\
    \ capacity and performance overheads at high error rates or when mitigating errors\
    \ that change positions over time [\\[83,](#page-14-45) [84,](#page-14-13) [344\\\
    ]](#page-19-6). In contrast, a fine-grained hardware-based approach such as a\
    \ block error-correcting code [\\[345\u2013](#page-19-7)[350\\]](#page-19-8) can\
    \ efficiently mitigate a limited number of randomly-distributed errors but can\
    \ fail silently (and even exacerbate the number of errors present [\\[38,](#page-13-22)\
    \ [40,](#page-13-30) [115,](#page-15-40) [119,](#page-15-5) [226,](#page-17-5)\
    \ [230,](#page-17-6) [351\\]](#page-19-9)) when its correction capability is exceeded.\
    \ We conclude that it is essential for the system designer to know when and how\
    \ errors occur in a given memory chip in order to make an informed choice of which\
    \ error-mitigation mechanism to use in a particular system.\n\nUnfortunately,\
    \ DRAM consumers generally do not have access to definitive error models for commodity\
    \ DRAM chips. Obtaining this information without cooperation from producers requires\
    \ extensive reliability testing (as discussed throughout Section [5\\)](#page-8-0),\
    \ guidance from external (possibly untrustworthy) sources, or otherwise reasoning\
    \ about memory errors at a high level (e.g., disregarding uninteresting failure\
    \ modes). As a result, the separation of concerns between DRAM producers and consumers\
    \ effectively discourages consumers from exploring the full design space for error-mitigation\
    \ mechanisms.\n\n## <span id=\"page-8-0\"></span>5. DRAM Reliability Testing\n\
    \nAs our case studies in Sections [4](#page-5-0) show, enabling DRAM consumers\
    \ to reason about how a given DRAM operating point way will impact its reliable\
    \ operation is essential for enabling them to adopt system-memory cooperative\
    \ mechanisms to address DRAM scaling challenges. This section formalizes the information\
    \ that a DRAM consumer may need (but does not necessarily have access to today)\
    \ in order to quantitatively reason about DRAM reliability.\n\n#### 5.1. Information\
    \ Flow During Testing\n\nFigure [3](#page-8-1) describes the flow of information\
    \ necessary for a consumer to quantitatively estimate[13](#page-8-2) a DRAM chip's\
    \ error characteristics \u20DD5 starting from basic properties of the chip \u20DD\
    1 . In principle, these characteristics can comprise any aspect of DRAM reliability\
    \ that a consumer wants to quantify while exploring their system's design and/or\
    \ configuration space.\n\n<span id=\"page-8-2\"></span><sup>13</sup>\"Estimate\"\
    \ because, in general, no model or experiment is likely to be perfect, including\
    \ those provided by manufacturers.\n\nExamples include: (1) worst-case error rates\
    \ (e.g., bit error rate (BER) or failures in time (FIT)) across a given set of\
    \ operating points; (2) a profile of error-prone memory locations; or (3) a list\
    \ of error-free operating points (e.g., as identified in a shmoo analysis [\\\
    [352\\]](#page-19-10)). The error characteristics can be estimated in two different\
    \ ways: testing or modeling.\n\n5.1.1. Determination from Testing. First, a consumer\
    \ may estimate error characteristics using measurements from detailed experimental\
    \ testing \u20DD3 across a variety of operating conditions. Examples of measured\
    \ quantities include: aggregate error rates, per-cell probabilities of error,\
    \ and spatial/temporal error distributions. These measurements can be made using\
    \ testing infrastructures ranging from industry-standard largescale testing equipment\
    \ [\\[353,](#page-19-11) [354\\]](#page-19-12) to home-grown tools based on commodity\
    \ FPGAs [\\[16,](#page-13-28) [29,](#page-13-20) [55,](#page-14-16) [65,](#page-14-31)\
    \ [90,](#page-14-19) [171,](#page-16-4) [263,](#page-17-15) [272,](#page-17-21)\
    \ [355](#page-19-13)[\u2013358\\]](#page-19-14) or DRAM-based computing systems\
    \ [\\[236,](#page-17-22) [262,](#page-17-14) [273,](#page-18-0) [359,](#page-19-15)\
    \ [360\\]](#page-19-16).\n\nTo conduct accurate and rigorous testing, the consumer\
    \ must use an effective test methodology \u20DD2 that suits the particular DRAM\
    \ chip under test. Prior works extensively study key aspects of effective test\
    \ methodologies, including appropriate data and access patterns, the effects of\
    \ enabling/disabling DRAM chip features such as target row refresh (TRR) [\\[23,](#page-13-11)\
    \ [28,](#page-13-13) [32,](#page-13-14) [48,](#page-13-27) [301\\]](#page-18-28)\
    \ and on-die error correcting codes (on-die ECC) [\\[38\u2013](#page-13-22)[40,](#page-13-30)\
    \ [87,](#page-14-14) [115,](#page-15-40) [143,](#page-15-22) [166,](#page-16-2)\
    \ [168,](#page-16-26) [230,](#page-17-6) [361\u2013](#page-19-17)[363\\]](#page-19-18),\
    \ and the viability of different DRAM command sequences (e.g., sequences that\
    \ enable in-DRAM row copy operations [\\[55,](#page-14-16) [90,](#page-14-19)\
    \ [96,](#page-14-21) [364\\]](#page-19-19), true randomnumber generation [\\[75,](#page-14-18)\
    \ [91,](#page-14-20) [266,](#page-17-23) [365\\]](#page-19-20), and physically\
    \ unclonable functions [\\[73,](#page-14-0) [267\\]](#page-17-24)).\n\nIn turn,\
    \ choosing an effective test methodology requires knowing basic properties about\
    \ a DRAM chip's design and/or error mechanisms \u20DD1 . For example, DRAM manufacturer's\
    \ design choices for the sizes of internal storage arrays (i.e., mats [\\[12,](#page-13-9)\
    \ [78,](#page-14-2) [91,](#page-14-20) [366\\]](#page-19-21)), charge encoding\
    \ conventions of each cell (i.e., the true- and anti-cell organization [\\[117,](#page-15-26)\
    \ [150\\]](#page-15-27)), use of ondie reliability-improving mechanisms (e.g.,\
    \ on-die ECC, TRR), and organization of row and column addresses all play key\
    \ roles in determining whether a DRAM chip is likely to show errors from key error\
    \ mechanisms (e.g., data retention [\\[115,](#page-15-40) [117,](#page-15-26)\
    \ [150,](#page-15-27) [277,](#page-18-2) [367\u2013](#page-19-22)[369\\]](#page-19-23),\
    \ access-latency-related failures [\\[14,](#page-13-35) [16,](#page-13-28) [22,](#page-13-10)\
    \ [52,](#page-13-37) [74,](#page-14-1) [77,](#page-14-5) [78,](#page-14-2) [91\\\
    ]](#page-14-20), and RowHammer [\\[25,](#page-13-19) [26,](#page-13-40) [29,](#page-13-20)\
    \ [30,](#page-13-46) [232,](#page-17-9) [370\\]](#page-19-24)).\n\n5.1.2. Determination\
    \ from Modeling. Second, the consumer may make predictions from analytical or\
    \ empirical er-\n\n<span id=\"page-8-1\"></span>![](_page_8_Figure_12.jpeg)\n\n\
    Figure 3: Flow of information necessary to determine key error characteristics\
    \ for a given DRAM device.\n\nror models \u20DD4 based on a previous understanding\
    \ of DRAM errors (e.g., from past experiments or scientific studies). Examples\
    \ of such error models include: analytical models based on understanding DRAM\
    \ failure modes (e.g., sources of runtime faults [\\[51,](#page-13-43) [60,](#page-14-44)\
    \ [149,](#page-15-25) [371](#page-19-25)[\u2013373\\]](#page-19-26)), parametric\
    \ statistical models that provide useful summary statistics (e.g., lognormal distribution\
    \ of cell data-retention times [\\[276,](#page-18-1) [277,](#page-18-2) [374](#page-19-27)[\u2013\
    380\\]](#page-19-28), exponential distribution of the time-in-state of cells susceptible\
    \ to variable-retention time (VRT) [\\[65,](#page-14-31) [94,](#page-14-12) [150,](#page-15-27)\
    \ [166,](#page-16-2) [367,](#page-19-22) [381](#page-19-29)[\u2013389\\]](#page-19-30)),\
    \ physics-based simulation models (e.g., TCAD [\\[232,](#page-17-9) [374,](#page-19-27)\
    \ [390\u2013](#page-20-0) [392\\]](#page-20-1) and SPICE models [\\[14,](#page-13-35)\
    \ [59,](#page-14-6) [78,](#page-14-2) [106,](#page-15-38) [109,](#page-15-39)\
    \ [283,](#page-18-10) [393](#page-20-2)[\u2013395\\]](#page-20-3)), and empirically-determined\
    \ curves that predict observations well (e.g., single-bit error rates [\\[30,](#page-13-46)\
    \ [65,](#page-14-31) [67,](#page-14-24) [92,](#page-14-4) [94,](#page-14-12) [150\\\
    ]](#page-15-27)). Similar to testing, using error models to predict error characteristics\
    \ ultimately relies on understanding the DRAM chip being tested because the accuracy\
    \ of the predictions requires choosing appropriate models and model parameters\
    \ (e.g., through testing \u20DD3 or directly from fundamental chip design properties\
    \ \u20DD1 ).\n\n### 5.2. Access to Modeling and Testing Information\n\nFigure\
    \ [3](#page-8-1) shows that determining a DRAM chip's error characteristics through\
    \ modeling or testing ultimately relies on understanding the chip's fundamental\
    \ design properties. This reliance can be implicit (e.g., inherent within a pre-existing\
    \ workflow designed for a specific chip) or explicit (e.g., chosen as part of\
    \ a home-grown testing methodology). Therefore, a consumer must be vigilant of\
    \ the information they (perhaps unknowingly) rely upon at each step of their design\
    \ process concerning commodity DRAM.\n\nFortunately, the consumer only needs to\
    \ be concerned with the information flow at the children of a node whose information\
    \ is already known from a trustworthy source. For example, a consumer who wants\
    \ to identify the locations of error-prone cells (i.e., \u20DD5 ) using testing\
    \ need not be concerned with chip design properties (i.e., \u20DD1 ) if DRAM manufacturers\
    \ provide appropriate test methodologies (i.e., \u20DD2 ) or detailed test results\
    \ (i.e., \u20DD3 ). Unfortunately, to our knowledge, neither DRAM standards nor\
    \ manufacturers provide the information in any of the nodes today, much less in\
    \ a clear, industry-validated manner. Therefore, the consumer lacks a base of\
    \ trustworthy information to build upon. This creates a barrier to entry for a\
    \ consumer who wants to explore optimizations to commodity DRAM by compromising\
    \ the consumer's ability to make well-informed or effective decisions.\n\nIn general,\
    \ except for the few major DRAM customers who may be able to secure confidentiality\
    \ agreements,[14](#page-9-1) consumers would need to rely on (possibly incorrect\
    \ or incomplete) inferences or assumptions based on domain knowledge or reverseengineering\
    \ studies (e.g., similar in spirit to [\\[16,](#page-13-28) [23,](#page-13-11)\
    \ [24,](#page-13-31) [28,](#page-13-13) [32,](#page-13-14) [40,](#page-13-30)\
    \ [74,](#page-14-1) [78,](#page-14-2) [92,](#page-14-4) [114,](#page-15-3) [117,](#page-15-26)\
    \ [150\u2013](#page-15-27)[155\\]](#page-15-28)) that are not verified or supported\
    \ by the DRAM industry.[15](#page-9-2) As a result, the need for assumptions\n\
    \ncan discourage practitioners from exploring the full design space even when\
    \ a given design choice is otherwise beneficial. We conclude that the lack of\
    \ information transparency is a serious impediment to adopting new designs for\
    \ addressing DRAM scaling challenges today.\n\n## <span id=\"page-9-0\"></span>6.\
    \ Rethinking Today's Separation of Concerns\n\nOur case studies throughout Section\
    \ [4](#page-5-0) demonstrate that although DRAM consumers have explored new and\
    \ creative ways to address DRAM scaling challenges, their solutions face serious\
    \ practicality concerns because of limited access to information about DRAM chip\
    \ reliability. In this section, we hypothesize that the unavailability of this\
    \ information is caused by a lack of transparency within DRAM standards which\
    \ provide control over, but not insight into, DRAM operations.\n\n#### 6.1. The\
    \ Problem of Information Unavailability\n\nAddressing DRAM scaling challenges\
    \ fundamentally requires understanding how those challenges impact system operation.\
    \ Therefore, it is unsurprising that reliability analysis and testing is central\
    \ to each of the approaches we survey in our case studies. In some cases, solutions\
    \ explicitly require reliability testing (e.g., identifying and monitoring the\
    \ physical locations of error-prone cells). Other solutions implicitly rely on\
    \ the results of reliability analysis (e.g., configuring RowHammer defenses based\
    \ on a chip's degree of vulnerability). Ultimately, deploying consumer-driven\
    \ solutions to DRAM scaling challenges requires some degree of understanding of\
    \ how different (representative) operating conditions impact DRAM and overall\
    \ system reliability.\n\nUnfortunately, the separation of concerns does not convey\
    \ this information, which discourages consumers from adopting such solutions in\
    \ practice. For example, recent works [\\[45,](#page-13-26) [46\\]](#page-13-38)\
    \ discuss the pitfalls of designing RowHammer defense mechanisms that rely on\
    \ knowledge of how RowHammer errors behave (e.g., HCfirst, dependence on a chip's\
    \ internal cell organization), calling into question the practicality of accurately\
    \ determining these details given an arbitrary DRAM chip. Knowing or determining\
    \ this information is essential to guarantee protection against RowHammer. However,\
    \ determining it without guidance from DRAM manufacturers requires trusting in\
    \ a home-grown testing or reverse-engineering methodology, which ultimately relies\
    \ on knowledge of DRAM chip details that likely needs to be assumed or inferred\
    \ (as discussed in Sections [4.3.2](#page-7-7) and [5\\)](#page-8-0).\n\nAs a\
    \ result, system-memory cooperative solutions to overcome scaling challenges largely\
    \ remain theoretical ideas or proofs-of-concept based on performance and reliability\
    \ characteristics that are assumed, inferred, or reverse-engineered from a limited\
    \ set of observations and DRAM products (e.g., using in-house experimental studies\
    \ [\\[13,](#page-13-33) [16,](#page-13-28) [17,](#page-13-34) [22,](#page-13-10)\
    \ [29,](#page-13-20) [52,](#page-13-37) [55,](#page-14-16) [73\u2013](#page-14-0)[75,](#page-14-18)\
    \ [78,](#page-14-2) [91,](#page-14-20) [150,](#page-15-27) [171,](#page-16-4)\
    \ [263,](#page-17-15) [266,](#page-17-23) [267,](#page-17-24) [271\u2013](#page-17-18)[273\\\
    ]](#page-18-0)) without DRAM manufacturers' support.\n\nUnfortunately, this lack\
    \ of a trustworthy base of information to build upon can discourage even the most\
    \ enterprising consumers from exploring new designs. Such exploration would\n\n\
    <span id=\"page-9-1\"></span><sup>14</sup>Even under confidentiality, DRAM manufacturers\
    \ may be unwilling to reveal certain proprietary design details (e.g., on-die\
    \ error correction [\\[40,](#page-13-30) [224\\]](#page-17-3), target row refresh\
    \ [\\[46\\]](#page-13-38)) or provide specifically requested numbers.\n\n<span\
    \ id=\"page-9-2\"></span><sup>15</sup>DRAM manufacturers may make assumptions\
    \ during their own testing. However, they have full transparency into their own\
    \ designs (i.e, the root node in the information flow), so they can make the most\
    \ informed decision.\n\n<span id=\"page-10-1\"></span>\n\n| Design Property  \
    \                                                     | Reverse-Engineered By\
    \                     | Use-Case(s) Relying on Knowing the Property          \
    \                                                                            \
    \                                                                            \
    \                                        |\n|-----------------------------------------------------------------------|-------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n\
    | Cell charge encoding convention<br>(i.e., true- and anti-cell layout) | Testing\
    \ [92, 115, 117, 150]               | Data-retention error modeling and testing\
    \ for mitigating refresh<br>overheads (e.g., designing worst-case test patterns)\
    \ [68, 117, 150]                                                             \
    \                                                 |\n| On-die ECC details    \
    \                                                | Modeling and testing [40,<br>115]\
    \         | Improving reliability (e.g., designing ECC within the memory<br>controller)\
    \ [38, 41, 119, 167], mitigating RowHammer [28, 32, 48, 260]                 \
    \                                                                            \
    \                  |\n| Target row refresh (TRR) details                     \
    \                 | Testing [23, 32]                          | Modeling and mitigating\
    \ RowHammer [23, 32, 48]                                                     \
    \                                                                            \
    \                                                                      |\n| Mapping\
    \ between internal and external<br>row addresses                | Testing [28,\
    \ 78, 114, 151,<br>153, 245]   | Mitigating RowHammer [28, 29, 114, 151, 152]\
    \                                                                            \
    \                                                                            \
    \                                                 |\n| Row addresses refreshed\
    \ by each<br>refresh operation                  | Testing [32]               \
    \               | Mitigating RowHammer [32], improving access timings [106, 198]\
    \                                                                            \
    \                                                                            \
    \                               |\n| Substructure organization (e.g., cell<br>array\
    \ dimensions)            | Modeling [78] and<br>testing [16, 74, 78] | Improving\
    \ DRAM access timings [16, 74, 78] and security [287]                        \
    \                                                                            \
    \                                                                            \
    \        |\n| Analytical model parameters<br>(e.g., bitline capacitance)     \
    \       | Modeling and testing [150,<br>277]        | Developing and using error\
    \ models for improving overall<br>reliability [376], mitigating refresh overheads\
    \ (e.g., data-retention [277,<br>371, 375] and VRT [383, 384] models), improving\
    \ access timings [78],<br>and mitigating RowHammer [30, 396] |\n\n#### Table 2:\
    \ Basic DRAM chip microarchitectural design properties that are typically assumed\
    \ or inferred for experimental studies.\n\nrequire weighing any potential benefits\
    \ (e.g., improved performance, security, etc.) against both: (1) risks (e.g.,\
    \ failures in the field) associated with potentially operating outside manufacturer\
    \ recommendations and (2) limitations due to compatibility with only a subset\
    \ of all commodity DRAM products (e.g., only those that have been accurately reverse-engineered).\
    \ These risks and limitations are a serious barrier to adoption; therefore, we\
    \ conclude that the lack of information transparency today discourages system\
    \ designers from exploring alternative designs that have been shown to provide\
    \ tangible benefits.\n\n#### 6.2. DRAM Standards Lack Transparency\n\nHistorically,\
    \ DRAM standards have not discussed DRAM chip reliability because the separation\
    \ of concerns assigns DRAM producers (near)[16](#page-10-0) full responsibility\
    \ to address DRAM-related reliability concerns. Therefore, DRAM producers are\
    \ expected to address reliability-related issues, leaving consumers to integrate\
    \ reliable DRAM chips. As technology scaling continues to degrade DRAM chip reliability,\
    \ modern DRAM standards are exposing new reliability-related features, such as\
    \ on-die ECC scrubbing [\\[34,](#page-13-16) [119,](#page-15-5) [319\\]](#page-18-21),\
    \ post-package repair [\\[33,](#page-13-15) [34,](#page-13-16) [216,](#page-16-22)\
    \ [222,](#page-17-1) [223\\]](#page-17-2), target row refresh [\\[23,](#page-13-11)\
    \ [32\\]](#page-13-14), and refresh management [\\[34,](#page-13-16) [47\\]](#page-13-39).\
    \ Unfortunately, more general reasoning about reliability remains elusive for\
    \ consumers at large.\n\nWe believe that this state of affairs naturally arises\
    \ from establishing a a clear and explicit interface between producers and consumers.\
    \ Consequently, ensuring that the standards leave enough flexibility for diverse\
    \ consumer use-cases requires careful and explicit attention. This is because\
    \ the standards are susceptible to abstraction inversion [\\[397\\]](#page-20-5),\
    \ a design antipattern in which a previously agreed-upon interface becomes an\
    \ obstacle, forcing system designers to re-implement basic functionality in terms\
    \ of the outdated abstraction. A rigid interface limits what is and is not possible,\
    \ potentially requiring unproductive reverse-engineering to work around.\n\nWe\
    \ contend that the difficulty that consumers face today in addressing DRAM scaling\
    \ challenges clearly indicates abstraction inversion: the separation of concerns\
    \ has aged without sufficient attention to flexibility. Although a fixed operating\
    \ point defines a clear interface, we believe that leaving room for (and potentially\
    \ even encouraging) different operating points is essential today.\n\n#### <span\
    \ id=\"page-10-2\"></span>6.3. Benefits for Both Producers and Consumers\n\nToday's\
    \ separation of concerns discourages not only consumers from exploring new ways\
    \ to work with commodity DRAM chips but also producers from adopting consumer-driven\
    \ ideas that help address DRAM scaling challenges. In other words, the separation\
    \ of concerns effectively discourages both parties from exploring solutions outside\
    \ their areas of concern. As a result, neither party explores the full design\
    \ space surrounding commodity DRAM chips.\n\nWe believe that rethinking the separation\
    \ to encourage cooperation stands to benefit all aspects of DRAM technology, encompassing\
    \ both the producers and consumers who build and use DRAM, respectively. Producers\
    \ gain access to a broad base of innovation from consumers who prototype solutions\
    \ (with or without additional investment from producers themselves), thereby creating\
    \ new opportunities for producers to make DRAM a more competitive product. Consumers\
    \ gain access to new ways to improve system-level metrics, such as performance\
    \ and energy efficiency, that were previously not practical. Ultimately, both\
    \ producers and consumers benefit from the best possible version of DRAM technology.\n\
    \n## 7. DRAM Standards as the Solution\n\nSeparating design concerns between producers\
    \ and consumers is practical for enabling each party to focus on their respective\
    \ areas of expertise. However, we recommend that the separation be crafted in\
    \ a way that not only enables both parties to help address DRAM scaling challenges,\
    \ but ideally encourages and aids it. To achieve both goals, we propose extending\
    \ DRAM standards in a way that enables consumers to make informed\n\n<span id=\"\
    page-10-0\"></span><sup>16</sup>Consumers with exceptional reliability requirements\
    \ may then choose to supplement DRAM chips with additional error-mitigation mechanisms,\
    \ as discussed in Section [3.2.1.](#page-4-7)\n\ndecisions about how their design\
    \ choices will affect a DRAM chip's reliable operation. In other words, instead\
    \ of modifying DRAM designs, we advocate modifying standards to facilitate transparency\
    \ of DRAM reliability characteristics. Armed with this information, consumers\
    \ can freely explore how to best use commodity DRAM chips to solve their own design\
    \ challenges while preserving the separation of concerns that allows DRAM designers\
    \ to focus on building the best possible standardscompliant DRAM chips.\n\n####\
    \ <span id=\"page-11-0\"></span>7.1. Choosing Information to Release\n\nWe identify\
    \ what information to release based on our analysis of information flow throughout\
    \ DRAM reliability testing in Section [5.](#page-8-0) We observe that, given the\
    \ information at any node in Figure [3,](#page-8-1) consumers can self-determine\
    \ the information at each of its child nodes. As a result, access to trustworthy\
    \ information at any node provides consumers with a foundation on which to build\
    \ their own designs. Therefore, we recommend that the DRAM industry release information\
    \ at at least one node, but that producers be free to choose that information\
    \ based on their interests and capabilities. This section examines realistic possibilities\
    \ for communicating information at each node of the flowchart.\n\n7.1.1. Microarchitectural\
    \ Design Properties. At the lowest level, DRAM producers can communicate basic\
    \ microarchitectural design properties that enable consumers to develop robust\
    \ test methodologies and error models. This is the most general and flexible approach\
    \ because it places no limitations on what types of studies consumers may pursue\
    \ (e.g., in contrast to providing information that is useful for reasoning about\
    \ select error mechanism(s)). Table [2](#page-10-1) reviews example properties\
    \ used by prior works to build system-level solutions for addressing DRAM scaling\
    \ challenges. For each design property, we list prior works that reverse-engineer\
    \ it and describe use-cases that rely on its knowledge.\n\nWe believe that releasing\
    \ these properties will minimally (if at all) impact DRAM producer's business\
    \ interests given that each of the properties can be reverse-engineered with existing\
    \ methods (as shown by Table [2,](#page-10-1) Column 2) and access to appropriate\
    \ tools, as demonstrated by prior studies [\\[16,](#page-13-28) [23,](#page-13-11)\
    \ [28,](#page-13-13) [29,](#page-13-20) [32,](#page-13-14) [40,](#page-13-30)\
    \ [65,](#page-14-31) [74,](#page-14-1) [78,](#page-14-2) [92,](#page-14-4) [114](#page-15-3)[\u2013\
    117,](#page-15-26) [150\u2013](#page-15-27)[153,](#page-15-45) [277\\]](#page-18-2).\
    \ Releasing this information in an official capacity confirms what is already\
    \ demonstrated publicly through experiment, yielding no further information than\
    \ others already have the capability to identify. On the other hand, knowing this\
    \ information empowers all consumers to confidently explore a larger design space,\
    \ benefiting both designers and producers in the long run (as discussed in Section\
    \ [6.3\\)](#page-10-2).\n\n7.1.2. Test Methodologies. Abstracting beyond microarchitectural\
    \ details, DRAM producers can disclose effective test methodologies for consumers\
    \ to conduct their own reliability studies (e.g., to explore new viable operating\
    \ points). Providing test methodologies absolves (1) producers from needing to\
    \ disclose chip design details; and (2) consumers from needing the DRAM-related\
    \ expertise to determine the test methodologies from those details.[17](#page-11-1)\
    \ As a limitation, disclosing only test methodologies constrains consumers to\
    \ work with only the particular error mechanisms that the methodologies are designed\
    \ for (e.g., data-retention, RowHammer). Table [3](#page-11-2) provides example\
    \ test parameters that prior works generally depend on (e.g., assume or reverse-engineer)\
    \ to conduct reliability testing.\n\n<span id=\"page-11-2\"></span>\n\n| Test\
    \ Parameter              | Description                                       \
    \                                                                            \
    \           |  |\n|-----------------------------|------------------------------------------------------------------------------------------------------------------------------------------|--|\n\
    | Data pattern                | Data pattern that maximizes the chance of<br>errors\
    \ occurring [28, 29, 32, 48, 65, 74, 92, 116,<br>116\u2013118, 150, 245, 262,\
    \ 356, 398\u2013400] |  |\n| Environmental<br>conditions | Temperature and voltage\
    \ that lead to<br>worst-case behavior [31, 75, 118, 150, 218, 277,<br>356, 357,\
    \ 382, 401]                          |  |\n| Test algorithm              | Sequence\
    \ of representative and/or worst-case<br>DRAM operations to test [22, 29, 32,\
    \ 48, 73, 75,<br>150, 262, 402]                       |  |\n\nTable 3: Testing\
    \ parameters that are typically assumed or inferred during experimental studies.\n\
    \n7.1.3. Test Results and/or Error Models. At the highest level of abstraction,\
    \ DRAM producers can directly disclose test results and/or error models related\
    \ to specific studies useful to consumers. For example, these could take the form\
    \ of parametric error models (e.g., the statistical relationship between operating\
    \ timings and error rates) along with parameter values for each chip; fine-granularity\
    \ error characteristics (e.g., per-column minimum viable access timings); and/or\
    \ specific summary statistics (e.g., HCfirst in studies pertaining to RowHammer).\
    \ In this way, consumers exploring new designs can avoid the need to conduct reliability\
    \ testing to identify the producer-provided information. As a limitation, directly\
    \ releasing test results and/or error models constrains consumers to developing\
    \ solutions only for those design concerns that pertain to the released information.\
    \ Table [4](#page-12-0) provides examples of key test results and error models\
    \ that prior works leverage in order to explore new design points based on commodity\
    \ DRAM chips.\n\n#### 7.2. Choosing When to Release the Information\n\nWe recommend\
    \ decoupling the release of information from the requirement to do so because\
    \ modifying DRAM standards is a slow process due to the need for consensus among\
    \ DRAM stakeholders (discussed in Section [2.4\\)](#page-3-5). To this end, we\
    \ recommend a practical two-step process with different approaches in the short-\
    \ and long-term.\n\n7.2.1. Step 1: Immediate Disclosure of Information. We recommend\
    \ two independent approaches to quickly release information in the short-term.\
    \ First, we recommend establishing a publicly-accessible database for researchers\
    \ and practitioners to aggregate information (e.g., reverse-engineered design\
    \ details) through crowdsourcing. We believe this is practical given the significant\
    \ academic and industry interest in addressing\n\n<span id=\"page-11-1\"></span><sup>17</sup>We\
    \ believe that interested parties already have such expertise, as shown by the\
    \ fact that many studies [\\[16,](#page-13-28) [23,](#page-13-11) [28,](#page-13-13)\
    \ [29,](#page-13-20) [32,](#page-13-14) [40,](#page-13-30) [65,](#page-14-31)\
    \ [74,](#page-14-1) [78,](#page-14-2) [92,](#page-14-4) [114\u2013](#page-15-3)[117,](#page-15-26)\
    \ [150](#page-15-27)[\u2013153,](#page-15-45) [277\\]](#page-18-2) establish test\
    \ methodologies through experimentation.\n\n<span id=\"page-12-0\"></span>\n\n\
    | Test Result or                          | Description                      \
    \                    |  |\n|-----------------------------------------|------------------------------------------------------|--|\n\
    | Error Model                             |                                  \
    \                    |  |\n| Data-retention<br>times                 | Minimum\
    \ refresh rate required for different          |  |\n|                       \
    \                  | DRAM regions (e.g., rows, cells) [65, 66, 79, 80,    |  |\n\
    |                                         | 86, 150, 403]                    \
    \                    |  |\n| Error profile                           | List of\
    \ cells susceptible to errors (e.g., VRT [65,  |  |\n|                       \
    \                  | 94, 150], latency-related [16, 52, 73\u201375])         \
    \  |  |\n| Error rate<br>summary<br>statistics     | Aggregate error rates (e.g.,\
    \ BER [92, 115, 150,      |  |\n|                                         | 166,\
    \ 356], FIT [218, 404, 405]), distribution        |  |\n|                    \
    \                     | parameters (e.g., copula [384], lognormal [276,      |\
    \  |\n|                                         | 277, 376], exponential [80,\
    \ 387])                    |  |\n| RowHammer<br>blast radius               | Maximum\
    \ number of rows affected by                   |  |\n|                       \
    \                  | hammering one or more row(s) [28, 29, 107, 231,      |  |\n\
    |                                         | 300, 396]                        \
    \                    |  |\n| HCfirst<br>or<br>RowHammer<br>Threshold |       \
    \                                               |  |\n|                      \
    \                   | Minimum number of RowHammer accesses                 | \
    \ |\n|                                         | required to induce bit-flips\
    \ [28, 29, 107, 118, 306] |  |\n\nTable 4: Examples of key test results and error\
    \ models from prior works that study and/or optimize commodity DRAM.\n\nDRAM scaling\
    \ challenges. Such a database would provide an opportunity for peer review of\
    \ posted information, increasing the likelihood that the information is trustworthy.\
    \ In the long run, we believe such a database would facilitate information release\
    \ from DRAM producers themselves because the producers could simply validate database\
    \ information, if not contribute directly.\n\nSecond, we recommend that commodity\
    \ DRAM producers individually release information for current DRAM chips and those\
    \ already in the field. For example, producers may update chip datasheets to incorporate\
    \ relevant design properties or make more extensive information available online\
    \ (e.g., similar to how some producers already provide compliance documents and\
    \ functional simulation models on their websites [\\[406](#page-20-13)[\u2013\
    408\\]](#page-20-14)). Releasing the information described in Section [7.1](#page-11-0)\
    \ requires no changes to DRAM designs or standards, though modifying standards\
    \ (e.g., via an addendum, as we suggest in Step 2) would help unify the information\
    \ release across all producers. Regardless, we believe it is important to release\
    \ information in the near term (even if not standardized) so that it is available\
    \ as soon as possible.\n\n7.2.2. Step 2: Explicit DRAM Reliability Standards.\
    \ In the long term, we recommend modifying DRAM standards to promote (or even\
    \ require) producers to disclose information that can have consumer-visible impact\
    \ to DRAM reliability. This may include any or all of the information discussed\
    \ throughout this paper; we believe that the DRAM stakeholders themselves (i.e.,\
    \ DRAM producers and consumers) must collectively determine and standardize which\
    \ information is the most relevant and useful to regulate.\n\nAs a concrete example\
    \ of how such changes to standards may occur, we reference test methodologies\
    \ [\\[121,](#page-15-7) [122\\]](#page-15-8) and error models [\\[120\\]](#page-15-6)\
    \ that JEDEC provides for NAND flash memory endurance [\\[123](#page-15-9)[\u2013\
    125\\]](#page-15-10), including floating-gate data retention [\\[126](#page-15-11)[\u2013\
    129\\]](#page-15-12) and threshold voltage distributions [\\[130\u2013](#page-15-13)[133\\\
    ]](#page-15-47). These documents outline standardized best practices for studying\
    \ and characterizing endurance properties of SSD devices. We envision analogous\
    \ documents released for key DRAM error mechanisms (e.g., data-retention, access-timing-related,\
    \ RowHammer), providing a standardized and trustworthy alternative to inferring\
    \ the same information through unofficial channels.\n\n#### 7.3. Natural Progression\
    \ Toward Transparency\n\nAs a final note, we anticipate that efforts to overcome\
    \ DRAM technology scaling challenges will naturally bring DRAM producers and consumers\
    \ closer together in pursuit of the best possible solutions. Diversifying consumer\
    \ needs, increasing use of system-memory cooperation [\\[10,](#page-13-7) [29,](#page-13-20)\
    \ [112,](#page-15-37) [409,](#page-20-15) [410\\]](#page-20-16), and emerging,\
    \ non-traditional DRAM architectures [\\[185,](#page-16-28) [200,](#page-16-29)\
    \ [201,](#page-16-30) [410](#page-20-16)[\u2013417\\]](#page-20-17) all challenge\
    \ existing DRAM design and use practices today. As scaling challenges continue\
    \ to worsen, the opportunity costs of maintaining today's separation of concerns\
    \ will do so as well.\n\nHowever, if we are to step ahead of worsening DRAM scaling\
    \ challenges, we must ensure that the standards of the future proactively enable\
    \ the whole community (both industry and academia) to collectively develop creative\
    \ and effective solutions. Although recent changes to DRAM standards such as refresh\
    \ management [\\[34,](#page-13-16) [47\\]](#page-13-39) and on-die ECC scrubbing\
    \ [\\[34,](#page-13-16) [119,](#page-15-5) [319\\]](#page-18-21) are increasing\
    \ signs of cooperation, these changes are reactions to long-standing problems.\
    \ Our work preempts this post hoc approach to system design, preemptively forging\
    \ a path toward cooperative solutions capable of holistically address the scaling\
    \ challenge.\n\n## 8. Conclusion\n\nWe show that the separation of concerns between\
    \ DRAM producers and consumers is an impediment to overcoming modern DRAM scaling\
    \ challenges because it discourages exploring the full design space around standardized\
    \ designs. Our case studies that support this observation find that consumers'\
    \ lack of insight into DRAM reliability is the key factor discouraging more efficient\
    \ solutions based on system-memory cooperation. We then analyze how consumers\
    \ can obtain this insight through DRAM reliability testing, and we introduce a\
    \ twostep approach to revise the separation of concerns to encourage system-memory\
    \ cooperation. We start with conceptual changes to the separation of concerns\
    \ and build toward modifying the DRAM standards that specify the separation. Our\
    \ work is a call-to-action for more open and flexible practices for DRAM design\
    \ and use, harnessing the synergy between researchers and practitioners to fully\
    \ explore the potential of DRAM technology.\n\n## Acknowledgment\n\nWe thank the\
    \ members of the SAFARI Research Group for their valuable feedback and the constructively\
    \ critical environment that they provide. We specifically thank Geraldo F. Oliveira,\
    \ Jisung Park, Haiyu Mao, Jawad Haj-Yahya, Jeremie S. Kim, Hasan Hassan, Joel\
    \ Lindegger, and Meryem Banu Cavlak for the feedback they provided on earlier\
    \ versions of this paper. We thank external experts who helped shape our arguments,\
    \ including Mattan Erez, Moinuddin Qureshi, Vilas Sridharan, Christian Weis, and\
    \ Tanj Bennett. This work was supported in part by the generous gifts provided\
    \ by our industry partners, including Google, Huawei, Intel, Microsoft, and VMware,\
    \ and support from the ETH Future Computing Laboratory and the Semiconductor Research\
    \ Corporation. A much earlier version of this work was placed on arXiv in 2022\
    \ [\\[418\\]](#page-20-18).\n\n## <span id=\"page-13-0\"></span>References\n\n\
    - [1] R. H. Dennard, \"Field-Effect Transistor Memory,\" 1968, US Patent 3,387,286.\n\
    - <span id=\"page-13-1\"></span>[2] R. H. Dennard, F. H. Gaensslen, H.-N. Yu,\
    \ V. L. Rideout, E. Bassous, and A. R. LeBlanc, \"Design of Ion-Implanted MOSFET's\
    \ with Very Small Physical Dimensions,\" JSSC, 1974.\n- <span id=\"page-13-2\"\
    ></span>[3] B. Keeth, R. J. Baker, B. Johnson, and F. Lin, DRAM Circuit Design:\
    \ Fundamental and High-Speed Topics. John Wiley & Sons, 2007.\n- [4] J. Markoff,\
    \ \"IBM's Robert H. Dennard and the Chip That Changed the World,\" 2019, [https://www.ibm.com/blogs/think/2019/11/ibms-r](https://www.ibm.com/blogs/think/2019/11/ibms-robert-h-dennard-and-the-chip-that-changed-the-world/)\
    \ [obert-h-dennard-and-the-chip-that-changed-the-world/.](https://www.ibm.com/blogs/think/2019/11/ibms-robert-h-dennard-and-the-chip-that-changed-the-world/)\n\
    - [5] Nature Electronics, \"Memory Lane,\" 2018.\n- <span id=\"page-13-3\"></span>[6]\
    \ \"DRAM: The Invention of On-Demand Data,\" [https://www.ibm.com/](https://www.ibm.com/ibm/history/ibm100/us/en/icons/dram/transform/)\
    \ [ibm/history/ibm100/us/en/icons/dram/transform/,](https://www.ibm.com/ibm/history/ibm100/us/en/icons/dram/transform/)\
    \ 2021.\n- <span id=\"page-13-4\"></span>[7] Y.-B. Kim and T. W. Chen, \"Assessing\
    \ Merged DRAM/Logic Technology,\" Integration, 1999.\n- <span id=\"page-13-5\"\
    ></span>[8] J. Kittl, K. Opsomer, M. Popovici, N. Menou, B. Kaczer, X. P. Wang,\
    \ C. Adelmann, M. Pawlak, K. Tomida, A. Rothschild et al., \"High-K Dielectrics\
    \ for Future Generation Memory Devices,\" Microelectronic engineering, 2009.\n\
    - <span id=\"page-13-6\"></span>[9] IBM, \"Icons of Progress: DRAM,\" [https://www.ibm.com/ibm/history/](https://www.ibm.com/ibm/history/ibm100/us/en/icons/dram/)\
    \ [ibm100/us/en/icons/dram/,](https://www.ibm.com/ibm/history/ibm100/us/en/icons/dram/)\
    \ 2023.\n- <span id=\"page-13-7\"></span>[10] O. Mutlu, \"Main Memory Scaling:\
    \ Challenges and Solution Directions,\" in More Than Moore Technologies for Next\
    \ Generation Computer Design. Springer, 2015, pp. 127\u2013153.\n- <span id=\"\
    page-13-8\"></span>[11] IEEE, \"More Moore: 2022 Update,\" International Roadmap\
    \ for Devices and Systems, 2022.\n- <span id=\"page-13-9\"></span>[12] Y. H. Son,\
    \ O. Seongil, Y. Ro, J. W. Lee, and J. H. Ahn, \"Reducing Memory Access Latency\
    \ with Asymmetric DRAM Bank Organizations,\" in ISCA, 2013.\n- <span id=\"page-13-33\"\
    ></span>[13] K. K. Chang, \"Understanding and Improving Latency of DRAM-Based\
    \ Memory Systems,\" Ph.D. dissertation, Carnegie Mellon University, 2017.\n- <span\
    \ id=\"page-13-35\"></span>[14] D. Lee, Y. Kim, V. Seshadri, J. Liu, L. Subramanian,\
    \ and O. Mutlu, \"Tiered-Latency DRAM: A Low Latency and Low Cost DRAM Architecture,\"\
    \ in HPCA, 2013.\n- [15] J. L. Hennessy and D. A. Patterson, Computer Architecture:\
    \ A Quantitative Approach. Elsevier, 2011.\n- <span id=\"page-13-28\"></span>[16]\
    \ K. K. Chang, A. Kashyap, H. Hassan, S. Ghose, K. Hsieh, D. Lee, T. Li, G. Pekhimenko,\
    \ S. Khan, and O. Mutlu, \"Understanding Latency Variation in Modern DRAM Chips:\
    \ Experimental Characterization, Analysis, and Optimization,\" in SIGMETRICS,\
    \ 2016.\n- <span id=\"page-13-34\"></span>[17] D. Lee, \"Reducing DRAM Latency\
    \ at Low Cost by Exploiting Heterogeneity,\" Ph.D. dissertation, Carnegie Mellon\
    \ University, 2016.\n- [18] R. Isaac, \"The Remarkable Story of the DRAM Industry,\"\
    \ IEEE SSCS News, 2008.\n- [19] J. Choi, W. Shin, J. Jang, J. Suh, Y. Kwon, Y.\
    \ Moon, and L.-S. Kim, \"Multiple Clone Row DRAM: A Low Latency and Area Optimized\
    \ DRAM,\" in ISCA, 2015.\n- [20] S. Borkar and A. A. Chien, \"The Future of Microprocessors,\"\
    \ CACM, 2011.\n- <span id=\"page-13-36\"></span>[21] K. Nguyen, K. Lyu, X. Meng,\
    \ V. Sridharan, and X. Jian, \"Nonblocking Memory Refresh,\" in ISCA, 2018.\n\
    - <span id=\"page-13-10\"></span>[22] D. Lee, Y. Kim, G. Pekhimenko, S. Khan,\
    \ V. Seshadri, K. Chang, and O. Mutlu, \"Adaptive-Latency DRAM: Optimizing DRAM\
    \ Timing for the Common-Case,\" in HPCA, 2015.\n- <span id=\"page-13-11\"></span>[23]\
    \ P. Frigo, E. Vannacci, H. Hassan, V. van der Veen, O. Mutlu, C. Giuffrida, H.\
    \ Bos, and K. Razavi, \"TRRespass: Exploiting the Many Sides of Target Row Refresh,\"\
    \ in IEEE S&P, 2020.\n- <span id=\"page-13-31\"></span>[24] P. Pessl, D. Gruss,\
    \ C. Maurice, M. Schwarz, and S. Mangard, \"DRAMA: Exploiting DRAM Addressing\
    \ for Cross-CPU Attacks,\" in USENIX Sec., 2016.\n- <span id=\"page-13-19\"></span>[25]\
    \ O. Mutlu, \"The RowHammer Problem and Other Issues we may Face as Memory Becomes\
    \ Denser,\" in DATE, 2017.\n- <span id=\"page-13-40\"></span>[26] O. Mutlu and\
    \ J. Kim, \"RowHammer: A Retrospective,\" in TCAD, 2019.\n- <span id=\"page-13-12\"\
    ></span>[27] O. Mutlu, A. Olgun, and A. G. Yagl\u0131kc\u0131, \"Fundamentally\
    \ Understand- \u02D8 ing and Solving RowHammer,\" in ASP-DAC, 2023.\n- <span id=\"\
    page-13-13\"></span>[28] J. S. Kim, M. Patel, A. G. Yagl\u0131k \u02D8 c\xB8\u0131\
    , H. Hassan, R. Azizi, L. Orosa, and O. Mutlu, \"Revisiting RowHammer: An Experimental\
    \ Analysis of Modern Devices and Mitigation Techniques,\" in ISCA, 2020.\n- <span\
    \ id=\"page-13-20\"></span>[29] Y. Kim, R. Daly, J. Kim, C. Fallin, J. H. Lee,\
    \ D. Lee, C. Wilkerson, K. Lai, and O. Mutlu, \"Flipping Bits in Memory Without\
    \ Accessing Them: An Experimental Study of DRAM Disturbance Errors,\" in ISCA,\
    \ 2014.\n- <span id=\"page-13-46\"></span>[30] K. Park, D. Yun, and S. Baeg, \"\
    Statistical Distributions of Row-Hammering Induced Failures in DDR3 Components,\"\
    \ Microelectronics Reliability, 2016.\n- <span id=\"page-13-42\"></span>[31] K.\
    \ Park, C. Lim, D. Yun, and S. Baeg, \"Experiments and Root Cause Analysis for\
    \ Active-Precharge Hammering Fault In DDR3 SDRAM Under 3\xD7 Nm Technology,\"\
    \ Microelectronics Reliability, 2016.\n- <span id=\"page-13-14\"></span>[32] H.\
    \ Hassan, Y. C. Tugrul, J. S. Kim, V. Van der Veen, K. Razavi, and O. Mutlu, \"\
    Uncovering In-DRAM RowHammer Protection Mechanisms: A New Methodology, Custom\
    \ RowHammer Patterns, and Implications,\" in MICRO, 2021.\n- <span id=\"page-13-16\"\
    ></span><span id=\"page-13-15\"></span>[33] JEDEC, DDR4 SDRAM Specification, 2012.\n\
    - [34] JEDEC, DDR5 SDRAM Specification, 2020.\n- <span id=\"page-13-17\"></span>[35]\
    \ JEDEC, \"High Bandwidth Memory (HBM) DRAM,\" JEDEC Standard JESD235D, 2021.\n\
    - <span id=\"page-13-18\"></span>[36] JEDEC, \"High Bandwidth Memory DRAM (HBM3),\"\
    \ JEDEC Standard JESD238, 2022.\n- <span id=\"page-13-21\"></span>[37] K. Bains,\
    \ J. Halbert, C. Mozak, T. Schoenborn, and Z. Greenfield, \"Row Hammer Refresh\
    \ Command,\" 2014, US Patent 9,117,544 B2.\n- <span id=\"page-13-22\"></span>[38]\
    \ Y. H. Son, S. Lee, O. Seongil, S. Kwon, N. S. Kim, and J. H. Ahn, \"CiDRA: A\
    \ cache-Inspired DRAM resilience architecture,\" in HPCA, 2015.\n- [39] N. Kwak,\
    \ S.-H. Kim, K. H. Lee, C.-K. Baek, M. S. Jang, Y. Joo, S.-H. Lee, W. Y. Lee,\
    \ E. Lee, D. Han, J. Kang, J. H. Lim, J.-B. Park, K.-T. Kim, S. Cho, S. W. Han,\
    \ J. Y. Keh, J. H. Chun, J. Oh, and S. H. Lee, \"A 4.8 Gb/s/pin 2Gb LPDDR4 SDRAM\
    \ with Sub-100\xB5A Self-Refresh Current for IoT Applications,\" in ISSCC, 2017.\n\
    - <span id=\"page-13-30\"></span>[40] M. Patel, J. Kim, T. Shahroodi, H. Hassan,\
    \ and O. Mutlu, \"Bit-Exact ECC Recovery (BEER): Determining DRAM On-Die ECC Functions\
    \ by Exploiting DRAM Data Retention Characteristics,\" in MICRO, 2020.\n- <span\
    \ id=\"page-13-23\"></span>[41] S.-L. Gong, J. Kim, S. Lym, M. Sullivan, H. David,\
    \ and M. Erez, \"DUO: Exposing On-Chip Redundancy to Rank-Level ECC for High Reliability,\"\
    \ in HPCA, 2018.\n- <span id=\"page-13-24\"></span>[42] JEDEC, \"System Level\
    \ RowHammer Mitigation,\" JEDEC Standard JEP301-1, 2021.\n- <span id=\"page-13-32\"\
    ></span>[43] JEDEC, \"Near-Term DRAM Level Rowhammer Mitigation,\" JEDEC Standard\
    \ JEP300-1, 2021.\n- <span id=\"page-13-25\"></span>[44] JEDEC, \"Low Power Double\
    \ Data Rate 5 (LPDDR5) SDRAM Specification,\" JEDEC Standard JESD209\u20135A,\
    \ 2020.\n- <span id=\"page-13-26\"></span>[45] M. K. Qureshi, \"Rethinking ECC\
    \ in the Era of Row-Hammer,\" in DRAMSec, 2021.\n- <span id=\"page-13-38\"></span>[46]\
    \ S. Saroiu, A. Wolman, and L. Cojocar, \"The Price of Secrecy: How Hiding Internal\
    \ DRAM Topologies Hurts Rowhammer Defenses,\" in IRPS, 2022.\n- <span id=\"page-13-39\"\
    ></span>[47] S. Saroiu and A. Wolman, \"How to Configure Row-Sampling-Based Rowhammer\
    \ Defenses,\" DRAMSec, 2022.\n- <span id=\"page-13-27\"></span>[48] P. Jattke,\
    \ V. van der Veen, P. Frigo, S. Gunter, and K. Razavi, \"Blacksmith: Scalable\
    \ Rowhammering in the Frequency Domain,\" in SP, 2022.\n- <span id=\"page-13-29\"\
    ></span>[49] S. Baek, S. Cho, and R. Melhem, \"Refresh Now and Then,\" in TC,\
    \ 2014.\n- <span id=\"page-13-41\"></span>[50] F. Brasser, L. Davi, D. Gens, C.\
    \ Liebchen, and A.-R. Sadeghi, \"CAn't Touch This: Software-Only Mitigation Against\
    \ Rowhammer Attacks Targeting Kernel Memory,\" in USENIX Sec., 2017.\n- <span\
    \ id=\"page-13-43\"></span>[51] G. C. Cardarilli, P. Marinucci, and A. Salsano,\
    \ \"Development of an Evaluation Model for the Design of Fault-Tolerant Solid\
    \ State Mass Memory,\" in ISCAS, 2000.\n- <span id=\"page-13-37\"></span>[52]\
    \ K. Chandrasekar, S. Goossens, C. Weis, M. Koedam, B. Akesson, N. Wehn, and K.\
    \ Goossens, \"Exploiting Expendable Process-Margins in DRAMs for Run-Time Performance\
    \ Optimization,\" in DATE, 2014.\n- <span id=\"page-13-44\"></span>[53] L. Chen,\
    \ Y. Cao, and Z. Zhang, \"E3CC: A Memory Error Protection Scheme With Novel Address\
    \ Mapping for Subranked And Low-Power Memories,\" TACO, 2013.\n- <span id=\"page-13-45\"\
    ></span>[54] H.-M. Chen, A. Arunkumar, C.-J. Wu, T. Mudge, and C. Chakrabarti,\
    \ \"E-ECC: Low Power Erasure And Error Correction Schemes For Increasing Reliability\
    \ Of Commodity DRAM Systems,\" in Proceedings of the 2015 International Symposium\
    \ on Memory Systems, 2015, pp. 60\u201370.\n- <span id=\"page-14-16\"></span>[55]\
    \ F. Gao, G. Tziantzioulis, and D. Wentzlaff, \"ComputeDRAM: In-Memory Compute\
    \ using Off-the-Shelf DRAMs,\" in MICRO, 2019.\n- <span id=\"page-14-25\"></span>[56]\
    \ M. Ghosh and H.-H. S. Lee, \"Smart Refresh: An Enhanced Memory Controller Design\
    \ for Reducing Energy in Conventional and 3D Die-Stacked DRAMs,\" in MICRO, 2007.\n\
    - <span id=\"page-14-32\"></span>[57] Z. Greenfield and T. Levy, \"Throttling\
    \ Support for Row-Hammer Counters,\" 2016, U.S. Patent 9,251,885.\n- <span id=\"\
    page-14-17\"></span>[58] N. Hajinazar, G. F. Oliveira, S. Gregorio, J. Ferreira,\
    \ N. M. Ghiasi, M. Patel, M. Alser, S. Ghose, J. G. Luna, and O. Mutlu, \"SIMDRAM:\
    \ An End-to-End Framework for Bit-Serial SIMD Computing in DRAM,\" ASPLOS, 2021.\n\
    - <span id=\"page-14-6\"></span>[59] H. Hassan, G. Pekhimenko, N. Vijaykumar,\
    \ V. Seshadri, D. Lee, O. Ergin, and O. Mutlu, \"ChargeCache: Reducing DRAM Latency\
    \ by Exploiting Row Access Locality,\" in HPCA, 2016.\n- <span id=\"page-14-44\"\
    ></span>[60] A. A. Hwang, I. A. Stefanovici, and B. Schroeder, \"Cosmic Rays Don't\
    \ Strike Twice: Understanding the Nature of DRAM Errors and the Implications for\
    \ System Design,\" in ASPLOS, 2012.\n- <span id=\"page-14-26\"></span>[61] S.\
    \ M. Jafri, H. Hassan, A. Hemani, and O. Mutlu, \"Refresh Triggered Computation:\
    \ Improving the Energy Efficiency of Convolutional Neural Network Accelerators,\"\
    \ TACO, 2020.\n- <span id=\"page-14-36\"></span>[62] X. Jian and R. Kumar, \"\
    Adaptive Reliability Chipkill Correct (ARCC),\" in HPCA, 2013.\n- <span id=\"\
    page-14-37\"></span>[63] X. Jian, H. Duwe, J. Sartori, V. Sridharan, and R. Kumar,\
    \ \"Low-Power, Low-Storage-Overhead Chipkill Correct via Multi-Line Error Correction,\"\
    \ in SC, 2013.\n- <span id=\"page-14-27\"></span>[64] Y. Katayama, E. J. Stuckey,\
    \ S. Morioka, and Z. Wu, \"Fault-Tolerant Refresh Power Reduction of DRAMs for\
    \ Quasi-Nonvolatile Data Retention,\" in EFT, 1999.\n- <span id=\"page-14-31\"\
    ></span>[65] S. Khan, D. Lee, Y. Kim, A. R. Alameldeen, C. Wilkerson, and O. Mutlu,\
    \ \"The Efficacy of Error Mitigation Techniques for DRAM Retention Failures: A\
    \ Comparative Experimental Study,\" in SIGMETRICS, 2014.\n- <span id=\"page-14-47\"\
    ></span>[66] S. Khan, C. Wilkerson, D. Lee, A. R. Alameldeen, and O. Mutlu, \"\
    A Case for Memory Content-Based Detection and Mitigation of Data-Dependent Failures\
    \ in DRAM,\" in IEEE CAL, 2016.\n- <span id=\"page-14-24\"></span>[67] S. Khan,\
    \ D. Lee, and O. Mutlu, \"PARBOR: An Efficient System-Level Technique to Detect\
    \ Data-Dependent Failures in DRAM,\" in DSN, 2016.\n- <span id=\"page-14-23\"\
    ></span>[68] S. Khan, C. Wilkerson, Z. Wang, A. R. Alameldeen, D. Lee, and O.\
    \ Mutlu, \"Detecting and Mitigating Data-Dependent DRAM Failures by Exploiting\
    \ Current Memory Content,\" in MICRO, 2017.\n- <span id=\"page-14-8\"></span>[69]\
    \ J. Kim and M. C. Papaefthymiou, \"Dynamic Memory Design for Low Data-Retention\
    \ Power,\" in PATMOS, 2000.\n- <span id=\"page-14-28\"></span>[70] J. Kim and\
    \ M. C. Papaefthymiou, \"Block-Based Multiperiod Dynamic Memory Design for Low\
    \ Data-Retention Power,\" in TVLSI, 2003.\n- <span id=\"page-14-38\"></span>[71]\
    \ J. Kim, M. Sullivan, and M. Erez, \"Bamboo ECC: Strong, Safe, and Flexible Codes\
    \ For Reliable Computer Memory,\" in HPCA, 2015.\n- <span id=\"page-14-39\"></span>[72]\
    \ J. Kim, M. Sullivan, S.-L. Gong, and M. Erez, \"Frugal ECC: Efficient And Versatile\
    \ Memory Error Protection Through Fine-Grained Compression,\" in SC, 2015.\n-\
    \ <span id=\"page-14-0\"></span>[73] J. S. Kim, M. Patel, H. Hassan, and O. Mutlu,\
    \ \"The DRAM Latency PUF: Quickly Evaluating Physical Unclonable Functions by\
    \ Exploiting the Latency-Reliability Tradeoff in Modern Commodity DRAM Devices,\"\
    \ in HPCA, 2018.\n- <span id=\"page-14-1\"></span>[74] J. S. Kim, M. Patel, H.\
    \ Hassan, and O. Mutlu, \"Solar-DRAM: Reducing DRAM Access Latency by Exploiting\
    \ the Variation in Local Bitlines,\" in ICCD, 2018.\n- <span id=\"page-14-18\"\
    ></span>[75] J. S. Kim, M. Patel, H. Hassan, L. Orosa, and O. Mutlu, \"D-RaNGe:\
    \ Using Commodity DRAM Devices to Generate True Random Numbers With Low Latency\
    \ And High Throughput,\" in HPCA, 2019.\n- <span id=\"page-14-34\"></span>[76]\
    \ R. K. Konoth, M. Oliverio, A. Tatar, D. Andriesse, H. Bos, C. Giuffrida, and\
    \ K. Razavi, \"ZebRAM: Comprehensive and Compatible Software Protection Against\
    \ Rowhammer Attacks,\" in OSDI, 2018.\n- <span id=\"page-14-5\"></span>[77] S.\
    \ Koppula, L. Orosa, A. G. Yagl\u0131k \u02D8 c\xB8\u0131, R. Azizi, T. Shahroodi,\
    \ K. Kanellopoulos, and O. Mutlu, \"EDEN: Enabling Energy-Efficient, High-Performance\
    \ Deep Neural Network Inference Using Approximate DRAM,\" in MICRO, 2019.\n- <span\
    \ id=\"page-14-2\"></span>[78] D. Lee, S. Khan, L. Subramanian, S. Ghose, R. Ausavarungnirun,\
    \ G. Pekhimenko, V. Seshadri, and O. Mutlu, \"Design-Induced Latency Variation\
    \ in Modern DRAM Chips: Characterization, Analysis, and Latency Reduction Mechanisms,\"\
    \ in SIGMETRICS, 2017.\n- <span id=\"page-14-29\"></span>[79] C. H. Lin, D.-Y.\
    \ Shen, Y.-J. Chen, C.-L. Yang, and M. Wang, \"SECRET: Selective Error Correction\
    \ for Refresh Energy Reduction in DRAMs,\" in ICCD, 2012.\n- <span id=\"page-14-9\"\
    ></span>[80] J. Liu, B. Jaiyen, R. Veras, and O. Mutlu, \"RAIDR: Retention-Aware\
    \ Intelligent DRAM Refresh,\" in ISCA, 2012.\n- <span id=\"page-14-40\"></span>[81]\
    \ E. Manzhosov, A. Hastings, M. Pancholi, R. Piersma, M. T. I. Ziad, and S. Sethumadhavan,\
    \ \"MUSE: Multi-Use Error Correcting Codes,\" arXiv:2107.09245, 2021.\n- <span\
    \ id=\"page-14-7\"></span>[82] D. M. Mathew, E. F. Zulian, M. Jung, K. Kraft,\
    \ C. Weis, B. Jacob, and \xB4 N. Wehn, \"Using Run-Time Reverse-Engineering to\
    \ Optimize DRAM Refresh,\" in MEMSYS, 2017.\n- <span id=\"page-14-45\"></span>[83]\
    \ Bad Page Offlining, mcelog, 2021, [https://mcelog.org/badpageofflini](https://mcelog.org/badpageofflining.html)\
    \ [ng.html.](https://mcelog.org/badpageofflining.html)\n- <span id=\"page-14-13\"\
    ></span>[84] J. Meza, Q. Wu, S. Kumar, and O. Mutlu, \"Revisiting Memory Errors\
    \ in Large-Scale Production Data Centers: Analysis and Modeling of New Trends\
    \ from the Field,\" in DSN, 2015.\n- <span id=\"page-14-33\"></span>[85] O. Mutlu,\
    \ \"RowHammer,\" [https://people.inf.ethz.ch/omutlu/pub/](https://people.inf.ethz.ch/omutlu/pub/onur-Rowhammer-TopPicksinHardwareEmbeddedSecurity-November-8-2018.pdf)\
    \ [onur-Rowhammer-TopPicksinHardwareEmbeddedSecurity-N](https://people.inf.ethz.ch/omutlu/pub/onur-Rowhammer-TopPicksinHardwareEmbeddedSecurity-November-8-2018.pdf)\
    \ [ovember-8-2018.pdf,](https://people.inf.ethz.ch/omutlu/pub/onur-Rowhammer-TopPicksinHardwareEmbeddedSecurity-November-8-2018.pdf)\
    \ 2018, Top Picks in Hardware and Embedded Security.\n- <span id=\"page-14-3\"\
    ></span>[86] P. J. Nair, D.-H. Kim, and M. K. Qureshi, \"ArchShield: Architectural\
    \ Framework for Assisting DRAM Scaling by Tolerating High Error Rates,\" in ISCA,\
    \ 2013.\n- <span id=\"page-14-14\"></span>[87] P. J. Nair, V. Sridharan, and M.\
    \ K. Qureshi, \"XED: Exposing On-Die Error Detection Information for Strong Memory\
    \ Reliability,\" in ISCA, 2016.\n- <span id=\"page-14-46\"></span>[88] Dynamic\
    \ Page Retirement, NVIDIA, 2020, [https://docs.nvidia.com/de](https://docs.nvidia.com/deploy/dynamic-page-retirement/index.html)\
    \ [ploy/dynamic-page-retirement/index.html.](https://docs.nvidia.com/deploy/dynamic-page-retirement/index.html)\n\
    - <span id=\"page-14-10\"></span>[89] T. Ohsawa, K. Kai, and K. Murakami, \"Optimizing\
    \ the DRAM Refresh Count for Merged DRAM/logic LSIs,\" in ISLPED, 1998.\n- <span\
    \ id=\"page-14-19\"></span>[90] A. Olgun, J. G. Luna, K. Kanellopoulos, B. Salami,\
    \ H. Hassan, O. Ergin, and O. Mutlu, \"PiDRAM: A Holistic End-to-end FPGA-based\
    \ Framework for Processing-in-DRAM,\" arXiv:2111.00082, 2021.\n- <span id=\"page-14-20\"\
    ></span>[91] A. Olgun, M. Patel, A. G. Yagl\u0131k \u02D8 c\xB8\u0131, H. Luo,\
    \ J. S. Kim, N. Bostanc\u0131, N. Vijaykumar, O. Ergin, and O. Mutlu, \"QUAC-TRNG:\
    \ High-Throughput True Random Number Generation Using Quadruple Row Activation\
    \ in Commodity DRAM Chips,\" in ISCA, 2021.\n- <span id=\"page-14-4\"></span>[92]\
    \ M. Patel, J. S. Kim, and O. Mutlu, \"The Reach Profiler (REAPER): Enabling the\
    \ Mitigation of DRAM Retention Failures via Profiling at Aggressive Conditions,\"\
    \ in ISCA, 2017.\n- <span id=\"page-14-41\"></span>[93] A. Patil, V. Nagarajan,\
    \ R. Balasubramonian, and N. Oswald, \"Dve:\xB4 Improving DRAM Reliability and\
    \ Performance On-Demand via Coherent Replication,\" in ISCA, 2021.\n- <span id=\"\
    page-14-12\"></span>[94] M. K. Qureshi, D.-H. Kim, S. Khan, P. J. Nair, and O.\
    \ Mutlu, \"AVATAR: A Variable-Retention-Time (VRT) Aware Refresh for DRAM Systems,\"\
    \ in DSN, 2015.\n- <span id=\"page-14-35\"></span>[95] G. Saileshwar, B. Wang,\
    \ M. Qureshi, and P. J. Nair, \"Randomized Row-Swap: Mitigating Row Hammer by\
    \ Breaking Spatial Correlation Between Aggressor and Victim Rows,\" in ASPLOS,\
    \ 2022.\n- <span id=\"page-14-21\"></span>[96] V. Seshadri, Y. Kim, C. Fallin,\
    \ D. Lee, R. Ausavarungnirun, G. Pekhimenko, Y. Luo, O. Mutlu, P. B. Gibbons,\
    \ M. A. Kozuch, and T. C. Mowry, \"RowClone: Fast and Energy-Efficient In-DRAM\
    \ Bulk Data Copy and Initialization,\" in MICRO, 2013.\n- [97] V. Seshadri, K.\
    \ Hsieh, A. Boroum, D. Lee, M. A. Kozuch, O. Mutlu, P. B. Gibbons, and T. C. Mowry,\
    \ \"Fast Bulk Bitwise AND and OR in DRAM,\" IEEE CAL, 2015.\n- [98] V. Seshadri,\
    \ D. Lee, T. Mullins, H. Hassan, A. Boroumand, J. Kim, M. A. Kozuch, O. Mutlu,\
    \ P. B. Gibbons, and T. C. Mowry, \"Buddy-RAM: Improving the Performance and Efficiency\
    \ of Bulk Bitwise Operations Using DRAM,\" in arXiv, 2016.\n- [99] V. Seshadri,\
    \ D. Lee, T. Mullins, H. Hassan, A. Boroumand, J. Kim, M. A. Kozuch, O. Mutlu,\
    \ P. B. Gibbons, and T. C. Mowry, \"Ambit: In-Memory Accelerator for Bulk Bitwise\
    \ Operations Using Commodity DRAM Technology,\" in MICRO, 2017.\n- <span id=\"\
    page-14-22\"></span>[100] V. Seshadri and O. Mutlu, \"In-DRAM Bulk Bitwise Execution\
    \ Engine,\" arXiv:1905.09822, 2019.\n- <span id=\"page-14-42\"></span>[101] A.\
    \ N. Udipi, N. Muralimanohar, R. Balsubramonian, A. Davis, and N. P. Jouppi, \"\
    LOT-ECC: Localized And Tiered Reliability Mechanisms For Commodity Memory Systems,\"\
    \ in ISCA, 2012.\n- <span id=\"page-14-15\"></span>[102] V. van der Veen, M. Lindorfer,\
    \ Y. Fratantonio, H. P. Pillai, G. Vigna, C. Kruegel, H. Bos, and K. Razavi, \"\
    GuardION: Practical Mitigation of DMA-Based Rowhammer Attacks on ARM,\" in DIMVA,\
    \ 2018.\n- <span id=\"page-14-30\"></span>[103] R. K. Venkatesan, S. Herr, and\
    \ E. Rotenberg, \"Retention-Aware Placement in DRAM (RAPID): Software Methods\
    \ for Quasi-Non-Volatile DRAM,\" in HPCA, 2006.\n- <span id=\"page-14-11\"></span>[104]\
    \ J. Wang, X. Dong, and Y. Xie, \"ProactiveDRAM: A DRAM-Initiated Retention Management\
    \ Scheme,\" in ICCD, 2014.\n- <span id=\"page-14-43\"></span>[105] S. Wang, M.\
    \ N. Bojnordi, X. Guo, and E. Ipek, \"Content Aware Refresh: Exploiting the Asymmetry\
    \ of DRAM Retention Errors to Reduce the Refresh Frequency of Less Vulnerable\
    \ Data,\" TOC, 2018.\n- <span id=\"page-15-38\"></span>[106] Y. Wang, A. Tavakkol,\
    \ L. Orosa, S. Ghose, N. M. Ghiasi, M. Patel, J. S. Kim, H. Hassan, M. Sadrosadati,\
    \ and O. Mutlu, \"Reducing DRAM Latency Via Charge-Level-Aware Look-Ahead Partial\
    \ Restoration,\" in MICRO, 2018.\n- <span id=\"page-15-42\"></span>[107] A. G.\
    \ Yagl\u0131k \u02D8 c\xB8\u0131, M. Patel, J. S. Kim, R. Azizibarzoki, A. Olgun,\
    \ L. Orosa, H. Hassan, J. Park, K. Kanellopoullos, T. Shahroodi, S. Ghose, and\
    \ O. Mutlu, \"BlockHammer: Preventing RowHammer at Low Cost by Blacklisting Rapidly-Accessed\
    \ DRAM Rows,\" in HPCA, 2021.\n- <span id=\"page-15-43\"></span>[108] D. H. Yoon\
    \ and M. Erez, \"Virtualized and Flexible ECC for Main Memory,\" in ASPLOS, 2010.\n\
    - <span id=\"page-15-39\"></span>[109] X. Zhang, Y. Zhang, B. R. Childers, and\
    \ J. Yang, \"Restore Truncation for Performance Improvement in Future DRAM Systems,\"\
    \ in HPCA, 2016.\n- <span id=\"page-15-0\"></span>[110] D. Zhang, G. Panwar, J.\
    \ B. Kotra, N. DeBardeleben, S. Blanchard, and X. Jian, \"Quantifying Server Memory\
    \ Frequency Margin and Using it to Improve Performance in HPC Systems,\" in ISCA,\
    \ 2021.\n- <span id=\"page-15-1\"></span>[111] O. Mutlu, \"Memory Scaling: A Systems\
    \ Architecture Perspective,\" in IMW, 2013.\n- <span id=\"page-15-37\"></span>[112]\
    \ O. Mutlu and L. Subramanian, \"Research Problems and Opportunities in Memory\
    \ Systems,\" in SUPERFRI, 2014.\n- <span id=\"page-15-2\"></span>[113] K. Loughlin,\
    \ S. Saroiu, A. Wolman, and B. Kasikci, \"Software-Defined Memory Controllers:\
    \ An Idea Whose Time Has Come,\" WACI, 2022.\n- <span id=\"page-15-3\"></span>[114]\
    \ M. Jung, C. C. Rheinlander, C. Weis, and N. Wehn, \"Reverse En- \xA8 gineering\
    \ of DRAMs: Row Hammer with Crosshair,\" in MEMSYS, 2016.\n- <span id=\"page-15-40\"\
    ></span>[115] M. Patel, J. S. Kim, H. Hassan, and O. Mutlu, \"Understanding and\
    \ Modeling On-Die Error Correction in Modern DRAM: An Experimental Study Using\
    \ Real Devices,\" in DSN, 2019.\n- <span id=\"page-15-46\"></span>[116] L. Mukhanov,\
    \ D. S. Nikolopoulos, and G. Karakonstantis, \"DStress: Automatic Synthesis of\
    \ DRAM Reliability Stress Viruses using Genetic Algorithms,\" in MICRO, 2020.\n\
    - <span id=\"page-15-26\"></span>[117] K. Kraft, C. Sudarshan, D. M. Mathew, C.\
    \ Weis, N. Wehn, and M. Jung, \"Improving the Error Behavior of DRAM by Exploiting\
    \ its Z-Channel Property,\" in DATE, 2018.\n- <span id=\"page-15-4\"></span>[118]\
    \ L. Orosa, A. G. Yagl\u0131k \u02D8 c\xB8\u0131, H. Luo, A. Olgun, J. Park, H.\
    \ Hassan, M. Patel, J. S. Kim, and O. Mutlu, \"A Deeper Look into RowHammer's\
    \ Sensitivities: Experiemental Analysis of Real DRAM Chips and Implications on\
    \ Future Attacks and Defenses,\" in MICRO, 2021.\n- <span id=\"page-15-5\"></span>[119]\
    \ K. Criss, K. Bains, R. Agarwal, T. Bennett, T. Grunzke, J. K. Kim, H. Chung,\
    \ and M. Jang, \"Improving Memory Reliability by Bounding DRAM Faults: DDR5 Improved\
    \ Reliability Features,\" in MEMSYS, 2020.\n- <span id=\"page-15-6\"></span>[120]\
    \ JEDEC, JEP122H: Failure Mechanisms and Models for Semiconductor Devices, 2016.\n\
    - <span id=\"page-15-7\"></span>[121] JEDEC, JESD218: Solid-State Drive (SSD)\
    \ Requirements and Endurance Test Method, 2010.\n- <span id=\"page-15-9\"></span><span\
    \ id=\"page-15-8\"></span>[122] JEDEC, JESD219: Solid-State Drive (SSD) Endurance\
    \ Workloads, 2010.\n- [123] Y. Cai, S. Ghose, E. F. Haratsch, Y. Luo, and O. Mutlu,\
    \ \"Error Characterization, Mitigation, and Recovery In Flash-Memory-Based Solid-State\
    \ Drives,\" Proc. IEEE, 2017.\n- [124] Y. Cai, E. F. Haratsch, O. Mutlu, and K.\
    \ Mai, \"Error Patterns in MLC NAND Flash Memory: Measurement, Characterization,\
    \ and Analysis,\" in DATE, 2012.\n- <span id=\"page-15-10\"></span>[125] Y. Cai,\
    \ S. Ghose, E. F. Haratsch, Y. Luo, and O. Mutlu, \"Errors in Flash-Memory-Based\
    \ Solid-State Drives: Analysis, Mitigation, and Recovery,\" Inside Solid State\
    \ Drives, 2018.\n- <span id=\"page-15-11\"></span>[126] Y. Cai, Y. Luo, E. F.\
    \ Haratsch, K. Mai, and O. Mutlu, \"Data Retention in MLC NAND Flash Memory: Characterization,\
    \ Optimization, and Recovery,\" in HPCA, 2015.\n- [127] Y. Luo, S. Ghose, Y. Cai,\
    \ E. F. Haratsch, and O. Mutlu, \"HeatWatch: Improving 3D NAND Flash Memory Device\
    \ Reliability by Exploiting Self-Recovery and Temperature Awareness,\" in HPCA,\
    \ 2018.\n- [128] Y. Luo, S. Ghose, Y. Cai, E. F. Haratsch, and O. Mutlu, \"Improving\
    \ 3D NAND Flash Memory Lifetime by Tolerating Early Retention Loss and Process\
    \ Variation,\" SIGMETRICS, 2018.\n- <span id=\"page-15-12\"></span>[129] Y. Cai,\
    \ G. Yalcin, O. Mutlu, E. F. Haratsch, A. Cristal, O. S. Unsal, and K. Mai, \"\
    Flash Correct-And-Refresh: Retention-Aware Error Management for Increased Flash\
    \ Memory Lifetime,\" in ICCD, 2012.\n- <span id=\"page-15-13\"></span>[130] Y.\
    \ Cai, E. F. Haratsch, O. Mutlu, and K. Mai, \"Threshold Voltage Distribution\
    \ in MLC NAND Flash Memory: Characterization, Analysis, and Modeling,\" in DATE,\
    \ 2013.\n- [131] Y. Cai, O. Mutlu, E. F. Haratsch, and K. Mai, \"Program Interference\
    \ in MLC NAND Flash Memory: Characterization, Modeling, and Mitigation,\" in ICCD,\
    \ 2013.\n- [132] Y. Cai, Y. Luo, S. Ghose, and O. Mutlu, \"Read Disturb Errors\
    \ in MLC NAND Flash Memory: Characterization, Mitigation, and Recovery,\" in DSN,\
    \ 2015.\n- <span id=\"page-15-47\"></span>[133] Y. Luo, S. Ghose, Y. Cai, E. F.\
    \ Haratsch, and O. Mutlu, \"Enabling Accurate and Practical Online Flash Channel\
    \ Modeling for Modern MLC NAND Flash Memory,\" in JSAC, 2016.\n- [134] Y. Cai,\
    \ S. Ghose, Y. Luo, K. Mai, O. Mutlu, and E. F. Haratsch, \"Vulnerabilities in\
    \ MLC NAND Flash Memory Programming: Experimental Analysis, Exploits, and Mitigation\
    \ Techniques,\" in HPCA, 2017.\n- <span id=\"page-15-14\"></span>[135] Y. Cai,\
    \ G. Yalcin, O. Mutlu, E. F. Haratsch, O. Unsal, A. Cristal, and K. Mai, \"Neighbor-Cell\
    \ Assisted Error Correction for MLC NAND Flash Memories,\" in SIGMETRICS, 2014.\n\
    - <span id=\"page-15-15\"></span>[136] \"DRAM Datasheet Survey,\" [https://github.com/CMU-SAFARI/DR](https://github.com/CMU-SAFARI/DRAM-Datasheet-Survey)\
    \ [AM-Datasheet-Survey.](https://github.com/CMU-SAFARI/DRAM-Datasheet-Survey)\n\
    - <span id=\"page-15-16\"></span>[137] JEDEC, \"JC-42 Solid State Memories,\"\
    \ [https://www.jedec.org/commit](https://www.jedec.org/committees/jc-42) [tees/jc-42.](https://www.jedec.org/committees/jc-42)\n\
    - <span id=\"page-15-17\"></span>[138] JEDEC, DDR3 SDRAM Specification, 2008.\n\
    - <span id=\"page-15-18\"></span>[139] JEDEC, \"Low Power Double Data Rate 4 (LPDDR4)\
    \ SDRAM Specification,\" JEDEC Standard JESD209\u20134B, 2014.\n- <span id=\"\
    page-15-19\"></span>[140] JEDEC, \"Graphics Double Data Rate (GDDR5) SGRAM Standard,\"\
    \ JEDEC Standard JESD212C, 2016.\n- <span id=\"page-15-20\"></span>[141] JEDEC,\
    \ \"Graphics Double Data Rate (GDDR6) SGRAM Standard,\" JEDEC Standard JESD250C,\
    \ 2021.\n- <span id=\"page-15-21\"></span>[142] M. V. Beigi, Y. Cao, S. Gurumurthi,\
    \ C. Recchia, A. Walton, and V. Sridharan, \"A Systematic Study of DDR4 DRAM Faults\
    \ in the Field,\" in HPCA, 2023.\n- <span id=\"page-15-22\"></span>[143] S.-L.\
    \ Gong, J. Kim, and M. Erez, \"DRAM Scaling Error Evaluation Model Using Various\
    \ Retention Time,\" in DSN-W, 2017.\n- [144] B. R. Childers, J. Yang, and Y. Zhang,\
    \ \"Achieving Yield, Density and Performance Effective DRAM at Extreme Technology\
    \ Sizes,\" in MEMSYS, 2015.\n- <span id=\"page-15-23\"></span>[145] L. Peters,\
    \ D. Potter, and R. Bowman, Cost Effective IC Manufacturing, 1998\u20131999. Integrated\
    \ Circuit Engineering Corporation, 1997.\n- <span id=\"page-15-24\"></span>[146]\
    \ J. Kang, \"A Study of the DRAM Industry,\" Master's thesis, Massachusetts Institute\
    \ of Technology, 2010.\n- [147] T. J. Dell, \"A White Paper on the Benefits of\
    \ Chipkill-Correct ECC for PC Server Main Memory,\" IBM Microelectronics Division,\
    \ 1997.\n- [148] K. H. Lee, \"A Strategic Analysis of the DRAM Industry After\
    \ the Year 2000,\" Master's thesis, Massachusetts Institute of Technology, 2013.\n\
    - <span id=\"page-15-25\"></span>[149] J. A. Croswell, \"A Model for Analysis\
    \ of the Effects of Redundancy and Error Correction on DRAM Memory Yield and Reliability,\"\
    \ Master's thesis, MIT, 2000.\n- <span id=\"page-15-27\"></span>[150] J. Liu,\
    \ B. Jaiyen, Y. Kim, C. Wilkerson, and O. Mutlu, \"An Experimental Study of Data\
    \ Retention Behavior in Modern DRAM Devices: Implications for Retention Time Profiling\
    \ Mechanisms,\" in ISCA, 2013.\n- <span id=\"page-15-44\"></span>[151] A. Barenghi,\
    \ L. Breveglieri, N. Izzo, and G. Pelosi, \"Software-Only Reverse Engineering\
    \ of Physical DRAM Mappings For RowHammer Attacks,\" in IVSW, 2018.\n- <span id=\"\
    page-15-41\"></span>[152] M. Farmani, M. Tehranipoor, and F. Rahman, \"RHAT: Efficient\
    \ RowHammer-Aware Test for Modern DRAM Modules,\" in ETS, 2021.\n- <span id=\"\
    page-15-45\"></span>[153] M. Wang, Z. Zhang, Y. Cheng, and S. Nepal, \"Dramdig:\
    \ A Knowledge-Assisted Tool To Uncover DRAM Address Mapping,\" in DAC, 2020.\n\
    - [154] Y. Jiang, H. Zhu, H. Shan, X. Guo, X. Zhang, and Y. Jin, \"TRRScope: Understanding\
    \ Target Row Refresh Mechanism for Modern DDR Protection,\" in HOST, 2021.\n-\
    \ <span id=\"page-15-28\"></span>[155] H. Nam, S. Baek, M. Wi, M. J. Kim, J. Park,\
    \ C. Song, N. S. Kim, and J. H. Ahn, \"X-ray: Discovering DRAM Internal Structure\
    \ and Error Characteristics by Issuing Memory Commands,\" IEEE CAL, 2023.\n- <span\
    \ id=\"page-15-29\"></span>[156] D. James, \"Silicon Chip Teardown to the Atomic\
    \ Scale\u2013Challenges Facing the Reverse Engineering of Semiconductors,\" Microscopy\
    \ and Microanalysis, 2010.\n- <span id=\"page-15-30\"></span>[157] R. Torrance\
    \ and D. James, \"The State-of-the-Art in IC Reverse Engineering,\" in CHES, 2009.\n\
    - <span id=\"page-15-31\"></span>[158] Rugged Memory, SMART Modular Technologies,\
    \ 2021, [https://www.](https://www.smartm.com/product/rugged-memory) [smartm.com/product/rugged-memory.](https://www.smartm.com/product/rugged-memory)\n\
    - <span id=\"page-15-32\"></span>[159] Intelligent Memory, \"IM ECC DRAM with\
    \ Integrated Error Correcting Code,\" 2016, Product Brief.\n- <span id=\"page-15-33\"\
    ></span>[160] RLDRAM Memory, Micron Technology, 2021, [https://www.micron.c](https://www.micron.com/products/dram/rldram-memory)\
    \ [om/products/dram/rldram-memory.](https://www.micron.com/products/dram/rldram-memory)\n\
    - <span id=\"page-15-34\"></span>[161] J. Macri, \"AMD's Next Generation GPU and\
    \ High Bandwidth Memory Architecture: FURY,\" in HCS, 2015.\n- <span id=\"page-15-35\"\
    ></span>[162] Intel Corporation, \"RLDRAM II and RLDRAM 3 Features,\" 2023, [https:](https://www.intel.com/content/www/us/en/docs/programmable/710283/17-0/rldram-ii-and-rldram-3-features.html)\
    \ [//www.intel.com/content/www/us/en/docs/programmable/710283](https://www.intel.com/content/www/us/en/docs/programmable/710283/17-0/rldram-ii-and-rldram-3-features.html)\
    \ [/17-0/rldram-ii-and-rldram-3-features.html.](https://www.intel.com/content/www/us/en/docs/programmable/710283/17-0/rldram-ii-and-rldram-3-features.html)\n\
    - <span id=\"page-15-36\"></span>[163] J. Church, \"Strategic Networking in Standard\
    \ Setting Organizations: The Case of JEDEC,\" Master's thesis, Cornell University,\
    \ 2007.\n- <span id=\"page-16-0\"></span>[164] JEDEC, \"Addendum No. 1 to JESD209-4,\
    \ Low Power Double Data Rate 4x (LPDDR4X),\" JEDEC Standard JESD209\u20134\u2013\
    1A, 2021.\n- <span id=\"page-16-1\"></span>[165] JEDEC, \"Addendum No. 1 to JESD79-4,\
    \ 3D Stacked DRAM,\" JEDEC Standard JESD79-4-1B, 2021.\n- <span id=\"page-16-2\"\
    ></span>[166] U. Kang, H.-s. Yu, C. Park, H. Zheng, J. Halbert, K. Bains, S. Jang,\
    \ and J. S. Choi, \"Co-Architecting Controllers and DRAM to Enhance DRAM Process\
    \ Scaling,\" in The Memory Forum, 2014.\n- <span id=\"page-16-23\"></span>[167]\
    \ S. Cha, O. Seongil, H. Shin, S. Hwang, K. Park, S. J. Jang, J. S. Choi, G. Y.\
    \ Jin, Y. H. Son, H. Cho, J. H. Ahn, and N. S. Kim, \"Defect Analysis and Cost-Effective\
    \ Resilience Architecture for Future DRAM Devices,\" in HPCA, 2017.\n- <span id=\"\
    page-16-26\"></span>[168] Micron Technology Inc., \"ECC Brings Reliability and\
    \ Power Efficiency to Mobile Devices,\" Micron Technology Inc., Tech. Rep., 2017.\n\
    - [169] S.-K. Park, \"Technology Scaling Challenge and Future Prospects of DRAM\
    \ and NAND Flash Memory,\" in IMW, 2015.\n- <span id=\"page-16-3\"></span>[170]\
    \ Micron Technologies, Inc., \"Quarterly Report on Form 10-Q,\" [https:](https://investors.micron.com/quarterly-results)\
    \ [//investors.micron.com/quarterly-results,](https://investors.micron.com/quarterly-results)\
    \ 2022.\n- <span id=\"page-16-4\"></span>[171] S. Ghose, A. G. Yagl\u0131k \u02D8\
    \ c\xB8\u0131, R. Gupta, D. Lee, K. Kudrolli, W. X. Liu, H. Hassan, K. K. Chang,\
    \ N. Chatterjee, A. Agrawal, M. O'Connor, and O. Mutlu, \"What Your DRAM Power\
    \ Models Are Not Telling You: Lessons from a Detailed Experimental Study,\" SIGMETRICS,\
    \ 2018.\n- <span id=\"page-16-5\"></span>[172] K. Akarvardar and H.-S. P. Wong,\
    \ \"Technology Prospects for Data-Intensive Computing,\" Proc. IEEE, 2023.\n-\
    \ <span id=\"page-16-6\"></span>[173] B. Van Essen, R. Pearce, S. Ames, and M.\
    \ Gokhale, \"On the Role of NVRAM in Data-Intensive Architectures: An Evaluation,\"\
    \ in IPDPS, 2012.\n- [174] J. Ousterhout, P. Agrawal, D. Erickson, C. Kozyrakis,\
    \ J. Leverich, D. Mazieres, S. Mitra, A. Narayanan, G. Parulkar, M. Rosenblum\
    \ ` et al., \"The Case for RAMClouds: Scalable High-Performance Storage Entirely\
    \ in DRAM,\" SIGOPS OSR, 2010.\n- [175] S. Lee, G.-J. Kim, N. Lee, K. Lee, B.\
    \ Woo, J. Jin, J. Kim, Y. Lee, H. Kim, and S. Pae, \"Reliability Characterization\
    \ for Advanced DRAM using HK/MG+ EUV Process Technology,\" in IEDM, 2021.\n- [176]\
    \ X. Sun, H. Wan, Q. Li, C.-L. Yang, T.-W. Kuo, and C. J. Xue, \"RM-SSD: In-Storage\
    \ Computing for Large-Scale Recommendation Inference,\" in HPCA, 2022.\n- <span\
    \ id=\"page-16-7\"></span>[177] S. Park, H. Kim, K. Kim, J. So, J. Ahn, W. Lee,\
    \ D. Kim, Y. Kim, J. Seok, J. Lee et al., \"Scaling of Memory Performance and\
    \ Capacity with CXL Memory Expander,\" in HCS, 2022.\n- <span id=\"page-16-8\"\
    ></span>[178] B. Gervasi, \"Will Carbon Nanotube Memory Replace DRAM?\" IEEE Micro,\
    \ 2019.\n- <span id=\"page-16-9\"></span>[179] K. Hsieh, S. Khan, N. Vijaykumar,\
    \ K. K. Chang, A. Boroumand, S. Ghose, and O. Mutlu, \"Accelerating Pointer Chasing\
    \ In 3D-Stacked Memory: Challenges, Mechanisms, Evaluation,\" in ICCD, 2016.\n\
    - [180] M. Ferdman, A. Adileh, O. Kocberber, S. Volos, M. Alisafaee, D. Jevdjic,\
    \ C. Kaynak, A. D. Popescu, A. Ailamaki, and B. Falsafi, \"Clearing The Clouds:\
    \ A Study Of Emerging Scale-Out Workloads On Modern Hardware,\" ASPLOS, 2012.\n\
    - [181] A. Gutierrez, R. G. Dreslinski, T. F. Wenisch, T. Mudge, A. Saidi, C.\
    \ Emmons, and N. Paver, \"Full-System Analysis And Characterization Of Interactive\
    \ Smartphone Applications,\" in IISWC, 2011.\n- [182] J. Hestness, S. W. Keckler,\
    \ and D. A. Wood, \"A Comparative Analysis Of Microarchitecture Effects On CPU\
    \ and GPU Memory System Behavior,\" in IISWC, 2014.\n- [183] Y. Huang, Z. Zha,\
    \ M. Chen, and L. Zhang, \"Moby: A Mobile Benchmark Suite For Architectural Simulators,\"\
    \ in ISPASS, 2014.\n- [184] Y. Zhu, D. Richins, M. Halpern, and V. J. Reddi, \"\
    Microarchitectural Implications Of Event-Driven Server-Side Web Applications,\"\
    \ in MI-CRO, 2015.\n- <span id=\"page-16-28\"></span>[185] G. F. Oliveira, J.\
    \ Gomez-Luna, S. Ghose, L. Orosa, N. Vijaykumar, \xB4 I. Fernandez, M. Sadrosadati,\
    \ and O. Mutlu, \"DAMOV: A New Methodology and Benchmark Suite for Evaluating\
    \ Data Movement Bottlenecks,\" in IEEE Access, 2021.\n- [186] A. Boroumand, S.\
    \ Ghose, Y. Kim, R. Ausavarungnirun, E. Shiu, R. Thakur, D. Kim, A. Kuusela, A.\
    \ Knies, P. Ranganathan, and O. Mutlu, \"Google Workloads for Consumer Devices:\
    \ Mitigating Data Movement Bottlenecks,\" in ASPLOS, 2018.\n- [187] A. Boroumand,\
    \ S. Ghose, B. Akin, R. Narayanaswami, G. F. Oliveira, X. Ma, E. Shiu, and O.\
    \ Mutlu, \"Google Neural Network Models for Edge Devices: Analyzing and Mitigating\
    \ Machine Learning Inference Bottlenecks,\" in PACT, 2021.\n- [188] K. Kanellopoulos,\
    \ N. Vijaykumar, C. Giannoula, R. Azizi, S. Koppula, N. M. Ghiasi, T. Shahroodi,\
    \ J. G. Luna, and O. Mutlu, \"SMASH: Co-Designing Software Compression and Hardware-Accelerated\
    \ Indexing for Efficient Sparse Matrix Operations,\" in MICRO, 2019.\n- [189]\
    \ M. V. Wilkes, \"The Memory Gap and The Future of High Performance Memories,\"\
    \ SIGARCH Computer Architecture News, 2001.\n- [190] W. A. Wulf and S. A. McKee,\
    \ \"Hitting the Memory Wall: Implications of the Obvious,\" SIGARCH Computer Architecture\
    \ News, 1995.\n- [191] O. Mutlu and T. Moscibroda, \"Stall-Time Fair Memory Access\
    \ Scheduling for Chip Multiprocessors,\" in MICRO, 2007.\n- [192] O. Mutlu, J.\
    \ Stark, C. Wilkerson, and Y. N. Patt, \"Runahead Execution: An Alternative to\
    \ Very Large Instruction Windows for Out-of-Order Processors,\" in HPCA, 2003.\n\
    - [193] S. Kanev, J. P. Darago, K. Hazelwood, P. Ranganathan, T. Moseley, G.-Y.\
    \ Wei, and D. Brooks, \"Profiling a Warehouse-scale Computer,\" in ISCA, 2015.\n\
    - [194] R. Bera, A. V. Nori, O. Mutlu, and S. Subramoney, \"DSPatch: Dual Spatial\
    \ Pattern Prefetcher,\" in MICRO, 2019.\n- [195] R. Bera, K. Kanellopoulos, A.\
    \ Nori, T. Shahroodi, S. Subramoney, and O. Mutlu, \"Pythia: A Customizable Hardware\
    \ Prefetching Framework using Online Reinforcement Learning,\" in MICRO, 2021.\n\
    - [196] X. Liu, D. Roberts, R. Ausavarungnirun, O. Mutlu, and J. Zhao, \"Binary\
    \ Star: Coordinated Reliability in Heterogeneous Memory Systems for High Performance\
    \ and Scalability,\" in MICRO, 2019.\n- [197] S. Ghose, A. Boroumand, J. S. Kim,\
    \ J. Gomez-Luna, and O. Mutlu, \xB4 \"Processing-in-Memory: A Workload-driven\
    \ Perspective,\" IBM JRD, 2019.\n- <span id=\"page-16-24\"></span>[198] W. Shin,\
    \ J. Yang, J. Choi, and L.-S. Kim, \"NUAT: A Non-Uniform Access Time Memory Controller,\"\
    \ in HPCA, 2014.\n- [199] S. Ghose, T. Li, N. Hajinazar, D. S. Cali, and O. Mutlu,\
    \ \"Demystifying Complex Workload-DRAM Interactions: An Experimental Study,\"\
    \ SIGMETRICS, 2019.\n- <span id=\"page-16-29\"></span>[200] J. Gomez-Luna, J.\
    \ El Hajj, I. Fernandez, and C. Giannoula, \"Bench- \xB4 marking a New Paradigm:\
    \ Understanding a Modern Processing-in-Memory Architecture,\" in arXiv:2105.03814,\
    \ 2021.\n- <span id=\"page-16-30\"></span>[201] J. Gomez-Luna, I. El Hajj, I.\
    \ Fernandez, C. Giannoula, G. F. Oliveira, \xB4 and O. Mutlu, \"Benchmarking Memory-Centric\
    \ Computing Systems: Analysis of Real Processing-in-Memory Hardware,\" in CUT,\
    \ 2021.\n- <span id=\"page-16-10\"></span>[202] C. Giannoula, I. Fernandez, J.\
    \ Gomez-Luna, N. Koziris, G. Goumas, \xB4 and O. Mutlu, \"Towards Efficient Sparse\
    \ Matrix Vector Multiplication on Real Processing-in-Memory Architectures,\" SIGMETRICS,\
    \ 2022.\n- <span id=\"page-16-11\"></span>[203] J. S. Kim, \"Improving DRAM Performance,\
    \ Security, and Reliability by Understanding and Exploiting DRAM Timing Parameter\
    \ Margins,\" Ph.D. dissertation, Carnegie Mellon University, 2020.\n- <span id=\"\
    page-16-12\"></span>[204] D. Lee, S. Ghose, G. Pekhimenko, S. Khan, and O. Mutlu,\
    \ \"Simultaneous Multi-Layer Access: Improving 3D-Stacked Memory Bandwidth at\
    \ Low Cost,\" TACO, 2016.\n- <span id=\"page-16-13\"></span>[205] J. Laudon, \"\
    UltraSPARC T1: Architecture and Physical Design of a 32-threaded General Purpose\
    \ CPU,\" in ISSCC, 2006.\n- <span id=\"page-16-19\"></span>[206] I. Bhati, Z.\
    \ Chishti, S.-L. Lu, and B. Jacob, \"Flexible Auto-Refresh: Enabling Scalable\
    \ and Energy-Efficient DRAM Refresh Reductions,\" in ISCA, 2015.\n- <span id=\"\
    page-16-14\"></span>[207] I. Bhati, M.-T. Chang, Z. Chishti, S.-L. Lu, and B.\
    \ Jacob, \"DRAM Refresh Mechanisms, Penalties, and Trade-Offs,\" in TC, 2016.\n\
    - <span id=\"page-16-15\"></span>[208] T. Zhang, M. Poremba, C. Xu, G. Sun, and\
    \ Y. Xie, \"CREAM: A Concurrent-Refresh-Aware DRAM Memory Architecture,\" in HPCA,\
    \ 2014.\n- <span id=\"page-16-18\"></span>[209] P. J. Nair, C.-C. Chou, and M.\
    \ K. Qureshi, \"Refresh Pausing in DRAM Memory Systems,\" in TACO, 2014.\n- <span\
    \ id=\"page-16-16\"></span>[210] R. Balasubramonian, \"A DRAM Refresh Tutorial,\"\
    \ [http://utaharch.b](http://utaharch.blogspot.com/2013/11/a-dram-refresh-tutorial.html)\
    \ [logspot.com/2013/11/a-dram-refresh-tutorial.html,](http://utaharch.blogspot.com/2013/11/a-dram-refresh-tutorial.html)\
    \ 2013.\n- <span id=\"page-16-25\"></span>[211] J. Stuecheli, D. Kaseridis, H.\
    \ C. Hunter, and L. K. John, \"Elastic Refresh: Techniques to Mitigate Refresh\
    \ Penalties in High Density Memory,\" in MICRO, 2010.\n- <span id=\"page-16-31\"\
    ></span>[212] R. Balasubramonian, \"Innovations in the Memory System,\" Synthesis\
    \ Lectures on Computer Architecture, 2019.\n- <span id=\"page-16-17\"></span>[213]\
    \ W.-K. Cheng, P.-Y. Shen, and X.-L. Li, \"Retention-Aware DRAM Auto-Refresh Scheme\
    \ for Energy and Performance Efficiency,\" Micromachines, 2019.\n- <span id=\"\
    page-16-20\"></span>[214] P. Nair, C.-C. Chou, and M. K. Qureshi, \"A Case for\
    \ Refresh Pausing in DRAM Memory Systems,\" in HPCA, 2013.\n- <span id=\"page-16-21\"\
    ></span>[215] K. K. Chang, D. Lee, Z. Chishti, A. R. Alameldeen, C. Wilkerson,\
    \ Y. Kim, and O. Mutlu, \"Improving DRAM Performance by Parallelizing Refreshes\
    \ with Accesses,\" in HPCA, 2014.\n- <span id=\"page-16-22\"></span>[216] M. Horiguchi\
    \ and K. Itoh, Nanoscale Memory Repair. Springer SBM, 2011.\n- [217] V. Sridharan\
    \ and D. Liberty, \"A Study of DRAM Failures in the Field,\" in SC, 2012.\n- <span\
    \ id=\"page-16-27\"></span>[218] B. Schroeder, E. Pinheiro, and W.-D. Weber, \"\
    DRAM Errors in the Wild: a Large-Scale Field Study,\" in SIGMETRICS, 2009.\n-\
    \ [219] T. C. May and M. H. Woods, \"Alpha-Particle-Induced Soft Errors in Dynamic\
    \ Memories,\" TED, 1979.\n- [220] T. J. O'Gorman, J. M. Ross, A. H. Taber, J.\
    \ F. Ziegler, H. P. Muhlfeld, C. J. Montrose, H. W. Curtis, and J. L. Walsh, \"\
    Field Testing for Cosmic Ray Soft Errors in Semiconductor Memories,\" IBM JRD,\
    \ 1996.\n- <span id=\"page-17-0\"></span>[221] V. Sridharan, N. DeBardeleben,\
    \ S. Blanchard, K. B. Ferreira, J. Stearley, J. Shalf, and S. Gurumurthi, \"Memory\
    \ Errors in Modern Systems: The Good, the Bad, and the Ugly,\" in ASPLOS, 2015.\n\
    - <span id=\"page-17-1\"></span>[222] D.-H. Kim and L. S. Milor, \"ECC-ASPIRIN:\
    \ An ECC-assisted Post-Package Repair Scheme for Aging Errors in DRAMs,\" in VTS,\
    \ 2016.\n- <span id=\"page-17-2\"></span>[223] O. Wada, T. Namekawa, H. Ito, A.\
    \ Nakayama, and S. Fujii, \"Post-Packaging Auto Repair Techniques for Fast Row\
    \ Cycle Embedded DRAM,\" in TEST, 2004.\n- <span id=\"page-17-3\"></span>[224]\
    \ S. Gurumurthi, K. Lee, M. Jang, V. Sridharan, A. Nygren, Y. Ryu, K. Sohn, T.\
    \ Kim, and H. Chung, \"HBM3: Enabling Memory Resilience at Scale,\" IEEE CAL,\
    \ 2021.\n- <span id=\"page-17-4\"></span>[225] H. Lee, Y. Yoo, S. H. Shin, and\
    \ S. Kang, \"ECMO: ECC Architecture Reusing Content-Addressable Memories for Obtaining\
    \ High Reliability in DRAM,\" TVLSI, 2022.\n- <span id=\"page-17-5\"></span>[226]\
    \ S. Jeong, S. Kang, and J.-S. Yang, \"PAIR: Pin-aligned In-DRAM ECC architecture\
    \ using expandability of Reed-Solomon code,\" in DAC, 2020.\n- [227] S.-I. Pae,\
    \ V. Kozhikkottu, D. Somasekar, W. Wu, S. G. Ramasubramanian, M. Dadual, H. Cho,\
    \ and K.-W. Kwon, \"Minimal Aliasing Single-Error-Correction Codes for DRAM Reliability\
    \ Improvement,\" IEEE Access, 2021.\n- [228] Y. Luo, S. Govindan, B. Sharma, M.\
    \ Santaniello, J. Meza, A. Kansal, J. Liu, B. Khessib, K. Vaid, and O. Mutlu,\
    \ \"Characterizing Application Memory Error Vulnerability to Optimize Datacenter\
    \ Cost via Heterogeneous-Reliability Memory,\" in DSN, 2014.\n- <span id=\"page-17-7\"\
    ></span>[229] M. Patel, G. F. de Oliveira, and O. Mutlu, \"HARP: Practically and\
    \ Effectively Identifying Uncorrectable Errors in Memory Chips That Use On-Die\
    \ Error-Correcting Codes,\" in MICRO, 2021.\n- <span id=\"page-17-6\"></span>[230]\
    \ M. Patel, \"Enabling Effective Error Mitigation in Memory Chips That Use On-Die\
    \ Error-Correcting Codes,\" Ph.D. dissertation, ETH Zurich, \xA8 2021.\n- <span\
    \ id=\"page-17-8\"></span>[231] K. Loughlin, S. Saroiu, A. Wolman, and B. Kasikci,\
    \ \"Stop! Hammer Time: Rethinking Our Approach to Rowhammer Mitigations,\" in\
    \ HotOS, 2021.\n- <span id=\"page-17-9\"></span>[232] T. Yang and X.-W. Lin, \"\
    Trap-Assisted DRAM Row Hammer Effect,\" EDL, 2019.\n- [233] Z. Greenfield, J.\
    \ B. Halbert, and K. S. Bains, \"Method, Apparatus and System for Determining\
    \ a Count of Accesses to a Row of Memory,\" 2014, US Patent US20140085995A1.\n\
    - <span id=\"page-17-10\"></span>[234] O. Mutlu, \"Retrospective: Flipping Bits\
    \ in Memory Without Accessing Them: An Experimental Study of DRAM Disturbance\
    \ Errors,\" in ISCA@50 25-Year Retrospective: 1996-2020, J. F. Mart\xB4\u0131\
    nez and L. K. John, Eds., 2023.\n- <span id=\"page-17-11\"></span>[235] M. Seaborn\
    \ and T. Dullien, \"Exploiting the DRAM Rowhammer Bug to Gain Kernel Privileges,\"\
    \ Black Hat, 2015.\n- <span id=\"page-17-22\"></span>[236] V. van der Veen, Y.\
    \ Fratantonio, M. Lindorfer, D. Gruss, C. Maurice, G. Vigna, H. Bos, K. Razavi,\
    \ and C. Giuffrida, \"Drammer: Deterministic Rowhammer Attacks on Mobile Platforms,\"\
    \ in CCS, 2016.\n- [237] D. Gruss, C. Maurice, and S. Mangard, \"Rowhammer.js:\
    \ A Remote Software-Induced Fault Attack in Javascript,\" in DIMVA, 2016.\n- [238]\
    \ K. Razavi, B. Gras, E. Bosman, B. Preneel, C. Giuffrida, and H. Bos, \"Flip\
    \ Feng Shui: Hammering a Needle in the Software Stack,\" in USENIX Sec., 2016.\n\
    - [239] Y. Xiao, X. Zhang, Y. Zhang, and R. Teodorescu, \"One Bit Flips, One Cloud\
    \ Flops: Cross-VM Row Hammer Attacks and Privilege Escalation,\" in USENIX Sec.,\
    \ 2016.\n- [240] E. Bosman, K. Razavi, H. Bos, and C. Giuffrida, \"Dedup Est Machina:\
    \ Memory Deduplication as An Advanced Exploitation Vector,\" in S&P, 2016.\n-\
    \ [241] S. Bhattacharya and D. Mukhopadhyay, \"Curious Case of RowHammer: Flipping\
    \ Secret Exponent Bits using Timing Analysis,\" in CHES, 2016.\n- [242] R. Qiao\
    \ and M. Seaborn, \"A New Approach for Rowhammer Attacks,\" in HOST, 2016.\n-\
    \ [243] Y. Jang, J. Lee, S. Lee, and T. Kim, \"SGX-Bomb: Locking Down the Processor\
    \ via Rowhammer Attack,\" in SysTEX, 2017.\n- [244] M. T. Aga, Z. B. Aweke, and\
    \ T. Austin, \"When Good Protections go Bad: Exploiting anti-DoS Measures to Accelerate\
    \ Rowhammer Attacks,\" in HOST, 2017.\n- <span id=\"page-17-20\"></span>[245]\
    \ A. Tatar, C. Giuffrida, H. Bos, and K. Razavi, \"Defeating Software Mitigations\
    \ Against Rowhammer: A Surgical Precision Hammer,\" in RAID, 2018.\n- [246] D.\
    \ Gruss, M. Lipp, M. Schwarz, D. Genkin, J. Juffinger, S. O'Connell, W. Schoechl,\
    \ and Y. Yarom, \"Another Flip in the Wall of Rowhammer Defenses,\" in IEEE S&P,\
    \ 2018.\n- [247] M. Lipp, M. Schwarz, L. Raab, L. Lamster, M. T. Aga, C. Maurice,\
    \ and D. Gruss, \"Nethammer: Inducing Rowhammer Faults Through Network Requests,\"\
    \ in EuroS&PW, 2020.\n- [248] P. Frigo, C. Giuffrida, H. Bos, and K. Razavi, \"\
    Grand Pwning Unit: Accelerating Microarchitectural Attacks with the GPU,\" in\
    \ S&P, 2018.\n- [249] L. Cojocar, K. Razavi, C. Giuffrida, and H. Bos, \"Exploiting\
    \ Correcting Codes: On the Effectiveness of ECC Memory Against Rowhammer Attacks,\"\
    \ in S&P, 2019.\n- [250] S. Ji, Y. Ko, S. Oh, and J. Kim, \"Pinpoint Rowhammer:\
    \ Suppressing Unwanted Bit Flips on Rowhammer Attacks,\" in ASIACCS, 2019.\n-\
    \ [251] S. Hong, P. Frigo, Y. Kaya, C. Giuffrida, and T. Dumitras\xB8, \"Terminal\
    \ Brain Damage: Exposing the Graceless Degradation in Deep Neural Networks Under\
    \ Hardware Fault Attacks,\" in USENIX Sec., 2019.\n- [252] A. Kwong, D. Genkin,\
    \ D. Gruss, and Y. Yarom, \"RAMBleed: Reading Bits in Memory Without Accessing\
    \ Them,\" in S&P, 2020.\n- [253] L. Cojocar, J. Kim, M. Patel, L. Tsai, S. Saroiu,\
    \ A. Wolman, and O. Mutlu, \"Are We Susceptible to Rowhammer? An End-to-End Methodology\
    \ for Cloud Providers,\" in S&P, 2020.\n- [254] Z. Weissman, T. Tiemann, D. Moghimi,\
    \ E. Custodio, T. Eisenbarth, and B. Sunar, \"JackHammer: Efficient Rowhammer\
    \ on Heterogeneous FPGA\u2013CPU Platforms,\" in TCHES, 2020.\n- [255] Z. Zhang,\
    \ Y. Cheng, D. Liu, S. Nepal, Z. Wang, and Y. Yarom, \"PTHammer: Cross-User-Kernel-Boundary\
    \ Rowhammer Through Implicit Accesses,\" in MICRO, 2020.\n- [256] F. Yao, A. S.\
    \ Rakin, and D. Fan, \"Deephammer: Depleting the Intelligence of Deep Neural Networks\
    \ Through Targeted Chain of Bit Flips,\" in USENIX Sec., 2020.\n- [257] A. Kogler,\
    \ J. Juffinger, S. Qazi, Y. Kim, M. Lipp, N. Boichat, E. Shiu, M. Nissler, and\
    \ D. Gruss, \"Half-Double: Hammering From the Next Row Over,\" in USENIX Sec.,\
    \ 2022.\n- <span id=\"page-17-12\"></span>[258] F. de Ridder, P. Frigo, E. Vannacci,\
    \ H. Bos, C. Giuffrida, and K. Razavi, \"SMASH: Synchronized Many-sided Rowhammer\
    \ Attacks from JavaScript,\" in USENIX Sec., 2021.\n- [259] J. Lee, \"Green Memory\
    \ Solution,\" Investor's Forum, Samsung Electronics, 2014.\n- <span id=\"page-17-19\"\
    ></span>[260] L. Cojocar, K. Razavi, C. Giuffrida, and H. Bos, \"Exploiting Correcting\
    \ Codes: On The Effectiveness Of ECC Memory Against Rowhammer Attacks,\" in S&P,\
    \ 2019.\n- <span id=\"page-17-13\"></span>[261] Micron Technology Inc., \"8Gb:\
    \ x4, x8, x16 DDR4 SDRAM Features - Excessive Row Activation,\" Micron Technology\
    \ Inc., Tech. Rep., 2020.\n- <span id=\"page-17-14\"></span>[262] L. Cojocar,\
    \ J. Kim, M. Patel, L. Tsai, S. Saroiu, A. Wolman, and O. Mutlu, \"Are We Susceptible\
    \ to Rowhammer? An End-to-End Methodology for Cloud Providers,\" in IEEE S&P,\
    \ 2020.\n- <span id=\"page-17-15\"></span>[263] K. K. Chang, A. G. Yagl\u0131\
    k \u02D8 c\xB8\u0131, S. Ghose, A. Agrawal, N. Chatterjee, A. Kashyap, D. Lee,\
    \ M. O'Connor, H. Hassan, and O. Mutlu, \"Understanding Reduced-Voltage Operation\
    \ in Modern DRAM Devices: Experimental Characterization, Analysis, and Mechanisms,\"\
    \ in SIG-METRICS, 2017.\n- <span id=\"page-17-16\"></span>[264] V. Seshadri and\
    \ O. Mutlu, \"Simple Operations in Memory to Reduce Data Movement,\" in Advances\
    \ in Computers, 2017.\n- [265] V. Seshadri and O. Mutlu, \"In-DRAM Bulk Bitwise\
    \ Execution Engine,\" Advances in Computers, 2020.\n- <span id=\"page-17-23\"\
    ></span>[266] B. Talukder, J. Kerns, B. Ray, T. Morris, and M. T. Rahman, \"Exploiting\
    \ DRAM Latency Variations for Generating True Random Numbers,\" ICCE, 2019.\n\
    - <span id=\"page-17-24\"></span>[267] B. Talukder, B. Ray, M. Tehranipoor, D.\
    \ Forte, and M. T. Rahman, \"LDPUF: Exploiting DRAM Latency Variations to Generate\
    \ Robust Device Signatures,\" arXiv preprint arXiv:1808.02584, 2018.\n- [268]\
    \ M. Yue, N. Karimian, W. Yan, N. A. Anagnostopoulos, and F. Tehranipoor, \"DRAM-Based\
    \ Authentication Using Deep Convolutional Neural Networks,\" IEEE Consumer Electronics\
    \ Magazine, 2020.\n- [269] M. S. Hashemian, B. Singh, F. Wolff, D. Weyer, S. Clay,\
    \ and C. Papachristou, \"A Robust Authentication Methodology Using Physically\
    \ Unclonable Functions in DRAM Arrays,\" in DATE, 2015.\n- <span id=\"page-17-17\"\
    ></span>[270] A. Schaller, W. Xiong, N. A. Anagnostopoulos, M. U. Saleem, S. Gabmeyer,\
    \ B. Skori \u02C7 c, S. Katzenbeisser, and J. Szefer, \"Decay-Based DRAM \xB4\
    \ PUFs in Commodity Devices,\" TDSC, 2018.\n- <span id=\"page-17-18\"></span>[271]\
    \ B. B. Talukder, B. Ray, D. Forte, and M. T. Rahman, \"PreLatPUF: Exploiting\
    \ DRAM Latency Variations For Generating Robust Device Signatures,\" IEEE Access,\
    \ 2019.\n- <span id=\"page-17-21\"></span>[272] H. Hassan, N. Vijaykumar, S. Khan,\
    \ S. Ghose, K. Chang, G. Pekhimenko, D. Lee, O. Ergin, and O. Mutlu, \"SoftMC:\
    \ A Flexible and Practical Open-Source Infrastructure for Enabling Experimental\
    \ DRAM Studies,\" in HPCA, 2017.\n- <span id=\"page-18-0\"></span>[273] H. David,\
    \ C. Fallin, E. Gorbatov, U. R. Hanebutte, and O. Mutlu, \"Memory Power Management\
    \ via Dynamic Voltage/Frequency Scaling,\" in ICAC, 2011.\n- <span id=\"page-18-3\"\
    ></span>[274] X. Pan and F. Mueller, \"Hiding DRAM Refresh Overhead in Real-Time\
    \ Cyclic Executives,\" in RTSS, 2019.\n- <span id=\"page-18-4\"></span>[275] J.\
    \ Mukundan, H. Hunter, K.-h. Kim, J. Stuecheli, and J. F. Mart\xB4\u0131nez, \"\
    Understanding and Mitigating Refresh Overheads in High-Density DDR4 DRAM Systems,\"\
    \ in ISCA, 2013.\n- <span id=\"page-18-1\"></span>[276] T. Hamamoto, S. Sugiura,\
    \ and S. Sawada, \"Well Concentration: A Novel Scaling Limitation Factor Derived\
    \ From DRAM Retention Time and Its Modeling,\" in IEDM, 1995.\n- <span id=\"page-18-2\"\
    ></span>[277] T. Hamamoto, S. Sugiura, and S. Sawada, \"On the Retention Time\
    \ Distribution of Dynamic Random Access Memory (DRAM),\" in TED, 1998.\n- <span\
    \ id=\"page-18-5\"></span>[278] D.-T. Nguyen, N.-M. Ho, M.-S. Le, W.-F. Wong,\
    \ and I.-J. Chang, \"ZEM: Zero-Cycle Bit-Masking Module for Deep Learning Refresh-Less\
    \ DRAM,\" IEEE Access, 2021.\n- <span id=\"page-18-6\"></span>[279] Y. Han, Y.\
    \ Wang, H. Li, and X. Li, \"Data-Aware DRAM Refresh to Squeeze the Margin of Retention\
    \ Time in Hybrid Memory Cube,\" in ICCAD, 2014.\n- <span id=\"page-18-7\"></span>[280]\
    \ H. Choi, D. Hong, J. Lee, and S. Yoo, \"Reducing DRAM Refresh Power Consumption\
    \ by Runtime Profiling of Retention Time and Dual-Row Activation,\" Microprocessors\
    \ and Microsystems, 2020.\n- <span id=\"page-18-8\"></span>[281] Apple Inc., \"\
    About the Security Content of Mac EFI Security Update 2015-001,\" [https://support.apple.com/en-us/HT204934,](https://support.apple.com/en-us/HT204934)\
    \ 2015.\n- <span id=\"page-18-9\"></span>[282] B. Aichinger, \"DDR Memory Errors\
    \ Caused by Row Hammer,\" in HPEC, 2015.\n- <span id=\"page-18-10\"></span>[283]\
    \ H. Hassan, M. Patel, J. S. Kim, A. G. Yagl\u0131k \u02D8 c\xB8\u0131, N. Vijaykumar,\
    \ N. M. Ghiasi, S. Ghose, and O. Mutlu, \"CROW: A Low-Cost Substrate for Improving\
    \ DRAM Performance, Energy Efficiency, and Reliability,\" in ISCA, 2019.\n- [284]\
    \ A. Di Dio, K. Koning, H. Bos, and C. Giuffrida, \"Copy-on-Flip: Hardening ECC\
    \ Memory Against Rowhammer Attacks,\" in NDSS, 2023.\n- [285] F. Brasser, L. Davi,\
    \ D. Gens, C. Liebchen, and A.-R. Sadeghi, \"Can't Touch This: Practical and Generic\
    \ Software-Only Defenses Against Rowhammer Attacks,\" in CoRR, 2016.\n- [286]\
    \ A. Saxena, G. Saileshwar, P. J. Nair, and M. Qureshi, \"Aqua: Scalable RowHammer\
    \ Mitigation by Quarantining Aggressor Rows at Runtime,\" in MICRO, 2022.\n- <span\
    \ id=\"page-18-11\"></span>[287] K. Loughlin, J. Rosenblum, S. Saroiu, A. Wolman,\
    \ D. Skarlatos, and B. Kasikci, \"Siloz: Leveraging DRAM Isolation Domains to\
    \ Prevent Inter-VM Rowhammer,\" in SOSP, 2023.\n- <span id=\"page-18-12\"></span>[288]\
    \ Z. B. Aweke, S. F. Yitbarek, R. Qiao, R. Das, M. Hicks, Y. Oren, and T. Austin,\
    \ \"ANVIL: Software-Based Protection Against Next-Generation Rowhammer Attacks,\"\
    \ in ASPLOS, 2016.\n- [289] M. Son, H. Park, J. Ahn, and S. Yoo, \"Making DRAM\
    \ Stronger Against Row Hammering,\" in DAC, 2017.\n- [290] S. M. Seyedzadeh, A.\
    \ K. Jones, and R. Melhem, \"Mitigating Wordline Crosstalk Using Adaptive Trees\
    \ of Counters,\" in ISCA, 2018.\n- [291] J. M. You and J.-S. Yang, \"MRLoc : Mitigating\
    \ Row-Hammering Based on Memory Locality,\" in DAC, 2019.\n- [292] E. Lee, I.\
    \ Kang, S. Lee, G. Edward Suh, and J. Ho Ahn, \"TWiCe: Preventing Row-Hammering\
    \ by Exploiting Time Window Counters,\" in ISCA, 2019.\n- [293] Y. Park, W. Kwon,\
    \ E. Lee, T. J. Ham, J. H. Ahn, and J. W. Lee, \"Graphene: Strong yet Lightweight\
    \ Row Hammer Protection,\" in MICRO, 2020.\n- [294] D.-H. Kim, P. J. Nair, and\
    \ M. K. Qureshi, \"Architectural Support for Mitigating Row Hammering in DRAM\
    \ Memories,\" IEEE CAL, 2014.\n- [295] I. Kang, E. Lee, and J. H. Ahn, \"CAT-TWO:\
    \ Counter-Based Adaptive Tree, Time Window Optimized for DRAM Row-Hammer Prevention,\"\
    \ IEEE Access, 2020.\n- [296] K. Bains, J. Halbert, C. Mozak, T. Schoenborn, and\
    \ Z. Greenfield, \"Row Hammer Refresh Command,\" 2015, U.S. Patent 9,117,544.\n\
    - [297] K. S. Bains and J. B. Halbert, \"Distributed Row Hammer Tracking,\" 2016,\
    \ U.S. Patent 9,299,400.\n- [298] K. S. Bains and J. B. Halbert, \"Row Hammer\
    \ Monitoring Based on Stored Row Hammer Threshold Value,\" 2016, U.S. Patent 9,384,821.\n\
    - [299] F. Devaux and R. Ayrignac, \"Method and Circuit for Protecting a DRAM\
    \ Memory Device from the Row Hammer Effect,\" 2021, 10,885,966.\n- <span id=\"\
    page-18-17\"></span>[300] A. G. Yagl\u0131k \u02D8 c\xB8\u0131, J. S. Kim, F.\
    \ Devaux, and O. Mutlu, \"Security Analysis of the Silver Bullet Technique for\
    \ RowHammer Prevention,\" 2021.\n- <span id=\"page-18-28\"></span>[301] M. Marazzi,\
    \ P. Jattke, S. Flavien, and K. Razavi, \"ProTRR: Principled yet Optimal In-DRAM\
    \ Target Row Refresh,\" in SP, 2022.\n- [302] Y. Kim, \"Architectural Techniques\
    \ to Enhance DRAM Scaling,\" Ph.D. dissertation, Carnegie Mellon University, 2015.\n\
    - [303] M. Marazzi, F. Solt, P. Jattke, K. Takashi, and K. Razavi, \"REGA: Scalable\
    \ Rowhammer Mitigation with Refresh-Generating Activations,\" in S&P, 2023.\n\
    - <span id=\"page-18-13\"></span>[304] M. Qureshi, A. Rohan, G. Saileshwar, and\
    \ P. J. Nair, \"Hydra: Enabling Low-Overhead Mitigation of Row-Hammer at Ultra-Low\
    \ Thresholds via Hybrid Tracking,\" in ISCA, 2022.\n- <span id=\"page-18-14\"\
    ></span>[305] S. Lee, K.-D. Kang, G. Park, N. S. Kim, and D. Kim, \"NoHammer:\
    \ Preventing Row Hammer with Last-Level Cache Management,\" IEEE CAL, 2023.\n\
    - <span id=\"page-18-15\"></span>[306] T. Bennett, S. Saroiu, A. Wolman, and L.\
    \ Cojocar, \"Panopticon: A Complete In-DRAM Rowhammer Mitigation,\" in DRAMSec,\
    \ 2021.\n- <span id=\"page-18-16\"></span>[307] Y. Kim, V. Seshadri, D. Lee, J.\
    \ Liu, and O. Mutlu, \"A Case for Exploiting Subarray-Level Parallelism (SALP)\
    \ in DRAM,\" in ISCA, 2012.\n- <span id=\"page-18-18\"></span>[308] D. Yun, M.\
    \ Park, C. Lim, and S. Baeg, \"Study of TID Effects on One Row Hammering using\
    \ Gamma in DDR4 SDRAMs,\" in IRPS, 2018.\n- [309] C. Lim, K. Park, and S. Baeg,\
    \ \"Active Precharge Hammering to Monitor Displacement Damage using High-Energy\
    \ Protons in 3x-nm SDRAM,\" IEEE Trans. Nucl. Sci., 2016.\n- [310] A. Olgun, M.\
    \ Osseiran, A. G. Yagl\u0131k \u02D8 c\xB8\u0131, Y. C. Tugrul, H. Luo, S. Rhyner,\
    \ \u02D8 B. Salami, J. G. Luna, and O. Mutlu, \"An Experimental Analysis of RowHammer\
    \ in HBM2 DRAM Chips,\" in DSN-S, 2023.\n- <span id=\"page-18-19\"></span>[311]\
    \ Z. Lang, P. Jattke, M. Marazzi, and K. Razavi, \"BLASTER: Characterizing the\
    \ Blast Radius of Rowhammer,\" in DRAMSec, 2023.\n- <span id=\"page-18-26\"></span>[312]\
    \ Synopsys, \"Reliability, Availability and Serviceability (RAS) for Memory Interfaces,\"\
    \ Synopsys, Tech. Rep., 2015.\n- [313] T. J. Dell, \"System RAS Implications of\
    \ DRAM Soft Errors,\" IBM JRD, 2008.\n- <span id=\"page-18-27\"></span>[314] C.\
    \ Slayman, M. Ma, and S. Lindley, \"Impact of Error Correction Code and Dynamic\
    \ Memory Reconfiguration on High-Reliability/Low-Cost Server Memory,\" in IRWS,\
    \ 2006.\n- <span id=\"page-18-20\"></span>[315] R. Sharifi and Z. Navabi, \"Online\
    \ Profiling for Cluster-Specific Variable Rate Refreshing in High-Density DRAM\
    \ Systems,\" in ETS, 2017.\n- [316] A. R. Alameldeen, I. Wagner, Z. Chishti, W.\
    \ Wu, C. Wilkerson, and S.-L. Lu, \"Energy-Efficient Cache Design Using Variable-Strength\
    \ Error-Correcting Codes,\" ISCA, 2011.\n- [317] H. Naeimi, C. Augustine, A. Raychowdhury,\
    \ S.-L. Lu, and J. Tschanz, \"STTRAM Scaling and Retention Failure,\" Intel Technology\
    \ Journal, 2013.\n- [318] M. Awasthi, M. Shevgoor, K. Sudan, B. Rajendran, R.\
    \ Balasubramonian, and V. Srinivasan, \"Efficient Scrub Mechanisms for Error-Prone\
    \ Emerging Memories,\" in HPCA, 2012.\n- <span id=\"page-18-21\"></span>[319]\
    \ M. J. M. Rahman, \"Utilizing Two Stage Scrubbing to Handle Single-Fault Multi-Error\
    \ Cases in DRAM Systems,\" Master's thesis, Iowa State University, 2021.\n- <span\
    \ id=\"page-18-22\"></span>[320] D. Kline, J. Zhang, R. Melhem, and A. K. Jones,\
    \ \"Flower and Fame: A Low Overhead Bit-Level Fault-Map and Fault-Tolerance Approach\
    \ for Deeply Scaled Memories,\" in HPCA, 2020.\n- <span id=\"page-18-25\"></span>[321]\
    \ S. Longofono, D. Kline Jr, R. Melhem, and A. K. Jones, \"Predicting and Mitigating\
    \ Single-Event Upsets in DRAM using HOTH,\" Microelectronics Reliability, 2021.\n\
    - [322] D. Kline, R. Melhem, and A. K. Jones, \"Sustainable Fault Management and\
    \ Error Correction for Next-Generation Main Memories,\" in IGSC, 2017.\n- [323]\
    \ S. Schechter, G. H. Loh, K. Strauss, and D. Burger, \"Use ECP, Not ECC, for\
    \ Hard Failures in Resistive Memories,\" ISCA, 2010.\n- [324] P. J. Nair, B. Asgari,\
    \ and M. K. Qureshi, \"SuDoku: Tolerating High-Rate of Transient Failures for\
    \ Enabling Scalable STTRAM,\" in DSN, 2019.\n- [325] J. Zhang, D. Kline, L. Fang,\
    \ R. Melhem, and A. K. Jones, \"Dynamic Partitioning To Mitigate Stuck-At Faults\
    \ in Emerging Memories,\" in ICCAD, 2017.\n- [326] H. Wang, \"Architecting Memory\
    \ Systems Upon Highly Scaled Error-Prone Memory Technologies,\" Ph.D. dissertation,\
    \ Rensselaer Polytechnic Institute, 2017.\n- <span id=\"page-18-24\"></span><span\
    \ id=\"page-18-23\"></span>[327] D. W. Kim and M. Erez, \"RelaxFault Memory Repair,\"\
    \ in ISCA, 2016.\n- [328] L. Mukhanov, K. Tovletoglou, H. Vandierendonck, D. S.\
    \ Nikolopoulos, and G. Karakonstantis, \"Workload-Aware DRAM Error Prediction\
    \ Using Machine Learning,\" in IISWC, 2019.\n- [329] E. Baseman, N. DeBardeleben,\
    \ K. Ferreira, S. Levy, S. Raasch, V. Sridharan, T. Siddiqua, and Q. Guan, \"\
    Improving DRAM Fault Characterization Through Machine Learning,\" in DSN-W, 2016.\n\
    - [330] I. Giurgiu, J. Szabo, D. Wiesmann, and J. Bird, \"Predicting DRAM Reliability\
    \ in the Field with Machine Learning,\" in Middleware, 2017.\n- [331] Z. Lan,\
    \ J. Gu, Z. Zheng, R. Thakur, and S. Coghlan, \"A Study of Dynamic Meta-Learning\
    \ for Failure Prediction in Large-Scale Systems,\" PDC, 2010.\n- [332] Y. Liang,\
    \ Y. Zhang, A. Sivasubramaniam, M. Jette, and R. Sahoo, \"Bluegene/L Failure Analysis\
    \ and Prediction Models,\" in DSN, 2006.\n- <span id=\"page-19-0\"></span>[333]\
    \ I. Boixaderas, D. Zivanovic, S. More, J. Bartolome, D. Vicente, \xB4 M. Casas,\
    \ P. M. Carpenter, P. Radojkovic, and E. Ayguad \xB4 e, \"Cost- \xB4 Aware Prediction\
    \ of Uncorrected DRAM Errors in the Field,\" in SC, 2020.\n- <span id=\"page-19-1\"\
    ></span>[334] G. Agrawal, L. Massengill, and K. Gulati, \"A Proposed SEU Tolerant\
    \ Dynamic Random Access Memory (DRAM) Cell,\" IEEE Trans. Nucl. Sci., 1994.\n\
    - [335] Infineon, \"Radiation Hardened & High Reliability Memories,\" [https:](https://www.infineon.com/cms/en/product/memories/radiation-hardened-high-reliability-memories/)\
    \ [//www.infineon.com/cms/en/product/memories/radiation-hardene](https://www.infineon.com/cms/en/product/memories/radiation-hardened-high-reliability-memories/)\
    \ [d-high-reliability-memories/,](https://www.infineon.com/cms/en/product/memories/radiation-hardened-high-reliability-memories/)\
    \ 2022.\n- [336] N. C. Lu, \"Advanced Cell Structures for Dynamic RAMs,\" IEEE\
    \ Circuits and Devices Magazine, 1989.\n- [337] S. K. Banerjee, \"Two-Transistor\
    \ DRAM Cell with High Alpha Particle Immunity,\" 1989, US Patent 4,864,374.\n\
    - [338] P. Mazumder, \"Design of a Fault-Tolerant Three-Dimensional Dynamic Random-Access\
    \ Memory with On-Chip Error-Correcting Circuit,\" TOC, 1993.\n- [339] Data Device\
    \ Corporation, \"Rad Hard Memories,\" [https://www.ddc-w](https://www.ddc-web.com/en/radhard/memories)\
    \ [eb.com/en/radhard/memories,](https://www.ddc-web.com/en/radhard/memories) 2022.\n\
    - <span id=\"page-19-2\"></span>[340] 3D PLUS, \"DDR4 SDRAM,\" [https://www.3d-plus.com/product.php?](https://www.3d-plus.com/product.php?fam=8&prod=43)\
    \ [fam=8&prod=43,](https://www.3d-plus.com/product.php?fam=8&prod=43) 2022.\n\
    - <span id=\"page-19-3\"></span>[341] D. M. Mathew, H. Kattan, C. Weis, J. Henkel,\
    \ N. Wehn, and H. Amrouch, \"Thermoelectric Cooling to Survive Commodity DRAMs\
    \ in Harsh Environment Automotive Electronics,\" IEEE Access, 2021.\n- <span id=\"\
    page-19-4\"></span>[342] K. Kobayashi, \"Highly-reliable Integrated Circuits for\
    \ Ground and Space Applications,\" ASICON, 2017.\n- <span id=\"page-19-5\"></span>[343]\
    \ J. Kim, M. Sullivan, S. Lym, and M. Erez, \"All-Inclusive ECC: Thorough End-to-End\
    \ Protection for Reliable Computer Memory,\" in ISCA, 2016.\n- <span id=\"page-19-6\"\
    ></span>[344] S. Lee, N. S. Kim, and D. Kim, \"Exploiting OS-Level Memory Offlining\
    \ for DRAM Power Management,\" IEEE CAL, 2019.\n- <span id=\"page-19-7\"></span>[345]\
    \ T. K. Moon, Error Correction Coding: Mathematical Methods and Algorithms. John\
    \ Wiley & Sons, 2005.\n- [346] T. Richardson and R. Urbanke, Modern Coding Theory.\
    \ Cambridge University Press, 2008.\n- [347] R. M. Roth, Introduction to Coding\
    \ Theory. Cambridge University Press, 2006.\n- [348] G. C. Clark Jr and J. B.\
    \ Cain, Error-Correction Coding for Digital Communications. Springer SBM, 2013.\n\
    - [349] D. J. Costello and S. Lin, Error Control Coding: Fundamentals and Applications.\
    \ Prentice Hall, 1982.\n- <span id=\"page-19-8\"></span>[350] S. Lin and D. J.\
    \ Costello, Error Control Coding: Fundamentals and Applications. Prentice Hall,\
    \ 2004.\n- <span id=\"page-19-9\"></span>[351] I. Alam, \"Lightweight Opportunistic\
    \ Memory Resilience,\" Ph.D. dissertation, University of California, Los Angeles,\
    \ 2021.\n- <span id=\"page-19-10\"></span>[352] K. Baker and J. Van Beers, \"\
    Shmoo Plotting: The Black Art of IC Testing,\" IEEE Des Test, 1997.\n- <span id=\"\
    page-19-11\"></span>[353] Advantest, T5833/T5833ES Memory Test System, [https://www.advant](https://www.advantest.com/products/memory/t5833.html)\
    \ [est.com/products/memory/t5833.html,](https://www.advantest.com/products/memory/t5833.html)\
    \ 2022.\n- <span id=\"page-19-12\"></span>[354] Teradyne, Magnum EPIC Ultra-high\
    \ Performance Solution for Memory Device Test, [https://www.teradyne.com/products/magnum-epic/,](https://www.teradyne.com/products/magnum-epic/)\
    \ 2022.\n- <span id=\"page-19-13\"></span>[355] C.-S. Hou, J.-F. Li, C.-Y. Lo,\
    \ D.-M. Kwai, Y.-F. Chou, and C.-W. Wu, \"An FPGA-Based Test Platform for Analyzing\
    \ Data Retention Time Distribution of DRAMs,\" in VLSI-DAT, 2013.\n- <span id=\"\
    page-19-35\"></span>[356] C. Weis, M. Jung, P. Ehses, C. Santos, P. Vivet, S.\
    \ Goossens, M. Koedam, and N. Wehn, \"Retention Time Measurements and Modelling\
    \ of Bit Error Rates of Wide I/O DRAM in MPSoCs,\" in DATE, 2015.\n- <span id=\"\
    page-19-36\"></span>[357] F. Wang, T. Vogelsang, B. Haukness, and S. C. Magee,\
    \ \"DRAM Retention at Cryogenic Temperatures,\" in IMW, 2018.\n- <span id=\"page-19-14\"\
    ></span>[358] R. Ladbury, M. Berg, E. Wilcox, K. LaBel, H. Kim, A. Phan, and C.\
    \ Seidleck, \"Use of Commercial FPGA-Based Evaluation Boards for Single-Event\
    \ Testing of DDR2 and DDR3 SDRAMS,\" IEEE Trans. Nucl. Sci., 2013.\n- <span id=\"\
    page-19-15\"></span>[359] P. Software, \"MemTest86 Overview,\" [https://www.memtest86.com/in](https://www.memtest86.com/index.html)\
    \ [dex.html,](https://www.memtest86.com/index.html) 2019.\n- <span id=\"page-19-16\"\
    ></span>[360] P. Francis-Mezger and V. M. Weaver, \"A Raspberry Pi Operating System\
    \ for Exploring Advanced Memory System Concepts,\" in MEMSYS, 2018.\n- <span id=\"\
    page-19-17\"></span>[361] T.-Y. Oh, H. Chung, J.-Y. Park, K.-W. Lee, S. oh, S.-Y.\
    \ Doo, H.-J. Kim, C. Lee, H.-R. Kim, J.-H. Lee, J.-I. Lee, K.-S. Ha, Y. Choi,\
    \ Y.-C. Cho, Y.-C. Bae, T. Jang, C. Park, K. Park, S. Jang, and J. Choi, \"A 3.2Gbps/pin\
    \ 8Gb 1.0V LPDDR4 SDRAM with Integrated ECC Engine for Sub-1V DRAM Core Operation,\"\
    \ in ISSCC, 2014.\n- [362] T.-Y. Oh, H. Chung, J.-Y. Park, K.-W. Lee, S. Oh, S.-Y.\
    \ Doo, H.-J. Kim, C. Lee, H.-R. Kim, J.-H. Lee, J.-I. Lee, K.-S. Ha, Y. Choi,\
    \ Y.-C. Cho, Y.-C. Bae, T. Jang, C. Park, K. Park, S. Jang, and J. S. Choi, \"\
    A 3.2 Gbps/Pin 8 Gbit 1.0 V LPDDR4 SDRAM with Integrated ECC Engine for Sub-1\
    \ V DRAM Core Operation,\" JSSC, 2014.\n- <span id=\"page-19-18\"></span>[363]\
    \ S. Kwon, Y. H. Son, and J. H. Ahn, \"Understanding DDR4 in Pursuit of In-DRAM\
    \ ECC,\" in ISOCC, 2014.\n- <span id=\"page-19-19\"></span>[364] K. K. Chang,\
    \ P. J. Nair, D. Lee, S. Ghose, M. K. Qureshi, and O. Mutlu, \"Low-Cost Inter-Linked\
    \ Subarrays (LISA): Enabling Fast Inter-Subarray Data Movement in DRAM,\" in HPCA,\
    \ 2016.\n- <span id=\"page-19-20\"></span>[365] F. Bostanc\u0131, A. Olgun, L.\
    \ Orosa, A. G. Yagl\u0131k \u02D8 c\xB8\u0131, J. S. Kim, H. Hassan, O. Ergin,\
    \ and O. Mutlu, \"DR-STRaNGe: End-to-End System Design for DRAM-based True Random\
    \ Number Generators,\" HPCA, 2022.\n- <span id=\"page-19-21\"></span>[366] T.\
    \ Zhang, K. Chen, C. Xu, G. Sun, T. Wang, and Y. Xie, \"Half-DRAM: A High-Bandwidth\
    \ and Low-Power DRAM Architecture from the Rethinking of Fine-Grained Activation,\"\
    \ in ISCA, 2014.\n- <span id=\"page-19-22\"></span>[367] A. Bacchini, M. Rovatti,\
    \ G. Furano, and M. Ottavi, \"Characterization of Data Retention Faults in DRAM\
    \ Devices,\" in DFT, 2014.\n- [368] A. Weber, A. Birner, and W. Krautschneider,\
    \ \"Data Retention Analysis on Individual Cells of 256Mb DRAM in 110nm Technology,\"\
    \ in ESSDERC, 2005.\n- <span id=\"page-19-23\"></span>[369] K. Yamaguchi, \"Theoretical\
    \ Study of Deep-Trap-Assisted Anomalous Currents in Worst-Bit Cells of Dynamic\
    \ Random-Access Memories (DRAM's),\" TED, 2000.\n- <span id=\"page-19-24\"></span>[370]\
    \ A. J. Walker, S. Lee, and D. Beery, \"On DRAM Rowhammer and the Physics of Insecurity,\"\
    \ TED, 2021.\n- <span id=\"page-19-25\"></span>[371] A. Das, H. Hassan, and O.\
    \ Mutlu, \"VRL-DRAM: Improving DRAM Performance Via Variable Refresh Latency,\"\
    \ in DAC, 2018.\n- [372] T. Siddiqua, A. E. Papathanasiou, A. Biswas, S. Gurumurthi,\
    \ I. Corp, and T. Aster, \"Analysis and Modeling of Memory Errors From Large-Scale\
    \ Field Data Collection,\" in SELSE, 2013.\n- <span id=\"page-19-26\"></span>[373]\
    \ J. Meza, Q. Wu, S. Kumar, and O. Mutlu, \"A Large-Scale Study of Flash Memory\
    \ Errors in the Field,\" in SIGMETRICS, 2015.\n- <span id=\"page-19-27\"></span>[374]\
    \ S. Jin, J.-H. Yi, J. H. Choi, D. G. Kang, Y. J. Park, and H. S. Min, \"Prediction\
    \ of Data Retention Time Distribution of DRAM by Physics-Based Statistical Simulation,\"\
    \ TED, 2005.\n- <span id=\"page-19-32\"></span>[375] A. Hiraiwa, M. Ogasawara,\
    \ N. Natsuaki, Y. Itoh, and H. Iwai, \"Statistical Modeling of Dynamic Random\
    \ Access Memory Data Retention Characteristics,\" JAP, 1996.\n- <span id=\"page-19-31\"\
    ></span>[376] Y. Li, H. Schneider, F. Schnabel, R. Thewes, and D. Schmitt-Landsiedel,\
    \ \"DRAM Yield Analysis and Optimization by a Statistical Design Approach,\" in\
    \ CSI, 2011.\n- [377] A. Hiraiwa, M. Ogasawara, N. Natsuaki, Y. Itoh, and H. Iwai,\
    \ \"Local-Field-Enhancement Model of DRAM Retention Failure,\" in IEDM, 1998.\n\
    - [378] N. Edri, P. Meinerzhagen, A. Teman, A. Burg, and A. Fish, \"Silicon-Proven,\
    \ Per-Cell Retention Time Distribution Model for Gain-Cell Based eDRAMs,\" IEEE\
    \ TOCS, 2016.\n- [379] K. Kim and J. Lee, \"A New Investigation of Data Retention\
    \ Time in Truly Nanoscaled DRAMs,\" in EDL, 2009.\n- <span id=\"page-19-28\"></span>[380]\
    \ W. Kong, P. C. Parries, G. Wang, and S. S. Iyer, \"Analysis of Retention Time\
    \ Distribution of Embedded DRAM-A New Method to Characterize Across-Chip Threshold\
    \ Voltage Variation,\" in ITC, 2008.\n- <span id=\"page-19-29\"></span>[381] D.-H.\
    \ Kim, S. Cha, and L. S. Milor, \"AVERT: An Elaborate Model for Simulating Variable\
    \ Retention Time in DRAMs,\" Microelectronics Reliability, 2015.\n- <span id=\"\
    page-19-37\"></span>[382] D. S. Yaney, C.-Y. Lu, R. A. Kohler, M. J. Kelly, and\
    \ J. T. Nelson, \"A Meta-Stable Leakage Phenomenon in DRAM Charge Storage-Variable\
    \ Hold Time,\" in IEDM, 1987.\n- <span id=\"page-19-33\"></span>[383] P. J. Restle,\
    \ J. Park, and B. F. Lloyd, \"DRAM Variable Retention Time,\" in IEDM, 1992.\n\
    - <span id=\"page-19-34\"></span>[384] C. G. Shirley and W. R. Daasch, \"Copula\
    \ Models of Correlation: A DRAM Case Study,\" in TC, 2014.\n- [385] H. Kim, B.\
    \ Oh, Y. Son, K. Kim, S.-Y. Cha, J.-G. Jeong, S.-J. Hong, and H. Shin, \"Characterization\
    \ of the Variable Retention Time in Dynamic Random Access Memory,\" TED, 2011.\n\
    - [386] H. Kim, B. Oh, Y. Son, K. Kim, S.-Y. Cha, J.-G. Jeong, S.-J. Hong, and\
    \ H. Shin, \"Study of Trap Models Related to the Variable Retention Time Phenomenon\
    \ in DRAM,\" TED, 2011.\n- <span id=\"page-19-38\"></span>[387] N. Kumar, \"Detection\
    \ of Variable Retention Time in DRAM,\" Master's thesis, Portland State University,\
    \ Portland, Oregon, 2014.\n- [388] Y. Mori, K. Ohyu, K. Okonogi, and R. i. Yamada,\
    \ \"The Origin of Variable Retention Time in DRAM,\" in IEDM, 2005.\n- <span id=\"\
    page-19-30\"></span>[389] K. Ohyu, T. Umeda, K. Okonogi, S. Tsukada, M. Hidaka,\
    \ S. Fujieda, and Y. Mochizuki, \"Quantitative Identification for the Physical\
    \ Origin of Variable Retention Time: A Vacancy-Oxygen Complex Defect Model,\"\
    \ in IEDM, 2006.\n- <span id=\"page-20-0\"></span>[390] Sentaurus Sdevice User's\
    \ Manual, Synopsys, 2018.\n- [391] M. Duan, F. Adam-Lema, B. Cheng, C. Navarro,\
    \ X. Wang, V. Georgiev, F. Gamiz, C. Millar, and A. Asenov, \"2D-TCAD Simulation\
    \ on Retention Time of Z2FET for DRAM Application,\" in SISPAD, 2017.\n- <span\
    \ id=\"page-20-1\"></span>[392] P. Pfaffli, H. Wong, X. Xu, L. Silvestri, X. Lin,\
    \ T. Yang, R. Tiwari, \xA8 S. Mahapatra, S. Motzny, V. Moroz, and T. Ma, \"TCAD\
    \ Modeling for Reliability,\" Microelectronics Reliability, 2018.\n- <span id=\"\
    page-20-2\"></span>[393] H. Luo, T. Shahroodi, H. Hassan, M. Patel, A. Giray Yagl\u0131\
    k \u02D8 c\xB8\u0131, L. Orosa, J. Park, and O. Mutlu, \"CLR-DRAM: A Low-Cost\
    \ DRAM Architecture Enabling Dynamic Capacity-Latency Trade-Off,\" in ISCA, 2020.\n\
    - [394] H. H. Shin and E.-Y. Chung, \"In-DRAM Cache Management for Low Latency\
    \ and Low Power 3D-Stacked DRAMs,\" Micromachines, 2019.\n- <span id=\"page-20-3\"\
    ></span>[395] Y. Wang, L. Orosa, X. Peng, Y. Guo, S. Ghose, M. Patel, J. S. Kim,\
    \ J. G. Luna, M. Sadrosadati, N. M. Ghiasi, and O. Mutlu, \"FIGARO: Improving\
    \ System Performance via Fine-Grained In-DRAM Data Relocation and Caching,\" in\
    \ MICRO, 2020.\n- <span id=\"page-20-4\"></span>[396] A. J. Walker, S. Lee, and\
    \ D. Beery, \"On DRAM Rowhammer and the Physics of Insecurity,\" TED, 2021.\n\
    - <span id=\"page-20-6\"></span><span id=\"page-20-5\"></span>[397] T. Baker,\
    \ \"Opening Up Ada-Tasking,\" ACM SIGAda Ada Letters, 1990.\n- [398] K. Duganapalli,\
    \ \"Modelling and Test Generation for Crosstalk Faults in DSM Chips,\" Ph.D. dissertation,\
    \ Universitat Bremen, 2016. \xA8\n- [399] L. Cojocar, K. Loughlin, S. Saroiu,\
    \ B. Kasikci, and A. Wolman, \"mFIT: A Bump-in-the-Wire Tool for Plug-and-Play\
    \ Analysis of Rowhammer Susceptibility Factors,\" Microsoft Research, Tech. Rep.,\
    \ 2021.\n- <span id=\"page-20-7\"></span>[400] L. Borucki, G. Schindlbeck, and\
    \ C. Slayman, \"Comparison of Accelerated DRAM Soft Error Rates Measured at Component\
    \ and System Level,\" in IEEE IRPS, 2008.\n- <span id=\"page-20-8\"></span>[401]\
    \ A. G. Yagl\u0131k \u02D8 c\xB8\u0131, H. Luo, A. Olgun, G. F. de Oliveira Junior,\
    \ J. Park, M. Patel, H. Hassan, L. Orosa, J. Kim, and O. Mutlu, \"Understanding\
    \ the RowHammer Vulnerability Under Reduced Wordline Voltage: An Experimental\
    \ Study Using Real Devices,\" in DSN, 2022.\n- <span id=\"page-20-9\"></span>[402]\
    \ Q. Salman, K. Yoongu, B. Nicolas, S. Eric, and N. Mattias, \"Half-Double: Next-Row-Over\
    \ Assisted Rowhammer,\" 2021.\n- <span id=\"page-20-10\"></span>[403] J. Kim and\
    \ M. C. Papaefthymiou, \"Block-Based Multi-Period Refresh For Energy Efficient\
    \ Dynamic Memory,\" in IEEE International ASIC/SOC Conference, 2001.\n- <span\
    \ id=\"page-20-11\"></span>[404] S. Levy, K. B. Ferreira, N. DeBardeleben, T.\
    \ Siddiqua, V. Sridharan, and E. Baseman, \"Lessons Learned from Memory Errors\
    \ Observed Over the Lifetime of Cielo,\" in SC, 2018.\n- <span id=\"page-20-12\"\
    ></span>[405] F. Wang and V. D. Agrawal, \"Soft Error Rates with Inertial and\
    \ Logical Masking,\" in VLSI, 2009.\n- <span id=\"page-20-13\"></span>[406] Micron\
    \ Technology, \"DRAM,\" [https://www.micron.com/products/dr](https://www.micron.com/products/dram/)\
    \ [am/.](https://www.micron.com/products/dram/)\n- [407] ISSI, \"DDR4 SDRAM,\"\
    \ [https://www.issi.com/US/product-dram-ddr4.](https://www.issi.com/US/product-dram-ddr4.shtml)\
    \ [shtml.](https://www.issi.com/US/product-dram-ddr4.shtml)\n- <span id=\"page-20-14\"\
    ></span>[408] ISSI, \"NT5AD256M16E4-JR,\" [https://www.nanya.com/en/Product/4](https://www.nanya.com/en/Product/4596/NT5AD256M16E4-JR)\
    \ [596/NT5AD256M16E4-JR.](https://www.nanya.com/en/Product/4596/NT5AD256M16E4-JR)\n\
    - <span id=\"page-20-15\"></span>[409] D. Patterson, T. Anderson, N. Cardwell,\
    \ R. Fromm, K. Keeton, C. Kozyrakis, R. Thomas, and K. Yelick, \"A Case for Intelligent\
    \ RAM,\" IEEE Micro, 1997.\n- <span id=\"page-20-16\"></span>[410] O. Mutlu, S.\
    \ Ghose, J. Gomez-Luna, and R. Ausavarungnirun, \"A Modern Primer on Processing\
    \ in Memory,\" in arXiv, 2020.\n- [411] F. Devaux, \"The True Processing in Memory\
    \ Accelerator,\" in HCS, 2019.\n- [412] Y.-C. Kwon, S. H. Lee, J. Lee, S.-H. Kwon,\
    \ J. M. Ryu, J.-P. Son, O. Seongil, H.-S. Yu, H. Lee, S. Y. Kim, Y. Cho, J. G.\
    \ Kim, J. Choi, H.-S. Shin, J. Kim, B. Phuah, H. Kim, M. J. Song, A. Choi, D.\
    \ Kim, S. Kim, E.-B. Kim, D. Wang, S. Kang, Y. Ro, S. Seo, J. Song, J. Youn, K.\
    \ Sohn, and N. S. Kim, \"25.4 A 20nm 6GB Function-In-Memory DRAM, Based on HBM2\
    \ with a 1.2 TFLOPS Programmable Computing Unit Using Bank-Level Parallelism,\
    \ for Machine Learning Applications,\" in ISSCC, 2021.\n- [413] M. He, C. Song,\
    \ I. Kim, C. Jeong, S. Kim, I. Park, M. Thottethodi, and T. Vijaykumar, \"Newton:\
    \ A DRAM-Maker's Accelerator-In-Memory (AiM) Architecture for Machine Learning,\"\
    \ in MICRO, 2020.\n- [414] D. Niu, S. Li, Y. Wang, W. Han, Z. Zhang, Y. Guan,\
    \ T. Guan, F. Sun, F. Xue, L. Duan, Y. Fang, H. Zheng, X. Jiang, S. Wang, F. Zuo,\
    \ Y. Wang, B. Yu, Q. Ren, and Y. Xie, \"184QPS/W 64Mb/mm 2 3D Logic-to-DRAM Hybrid\
    \ Bonding with Process-Near-Memory Engine for Recommendation System,\" in ISSCC,\
    \ 2022.\n- [415] J. Ahn, S. Hong, S. Yoo, O. Mutlu, and K. Choi, \"A Scalable\
    \ Processing-In-Memory Accelerator for Parallel Graph Processing,\" ISCA, 2016.\n\
    - [416] S. Lee, K. Kim, S. Oh, J. Park, G. Hong, D. Ka, K. Hwang, J. Park, K.\
    \ Kang, J. Kim et al., \"A 1ynm 1.25 V 8Gb, 16Gb/s/pin GDDR6-based Accelerator-in-Memory\
    \ supporting 1TFLOPS MAC Operation and Various Activation Functions for Deep-Learning\
    \ Applications,\" in ISSCC, 2022.\n- <span id=\"page-20-17\"></span>[417] O. Mutlu,\
    \ S. Ghose, J. Gomez-Luna, and R. Ausavarungnirun, \"Pro- \xB4 cessing Data Where\
    \ It Makes Sense: Enabling In-Memory Computation,\" Microprocessors and Microsystems,\
    \ 2019.\n- <span id=\"page-20-18\"></span>[418] M. Patel, T. Shahroodi, A. Manglik,\
    \ A. G. Yaglikci, A. Olgun, H. Luo, and O. Mutlu, \"A Case for Transparent Reliability\
    \ in DRAM Systems,\" arXiv:2204.10378, 2022.\n\n## <span id=\"page-21-0\"></span>A.\
    \ DRAM Trends Survey\n\nWe survey manufacturer-recommended DRAM operating parameters\
    \ as specified in commodity DRAM chip datasheets in order to understand how the\
    \ parameters have evolved over time. We extract values from 58 independent DRAM\
    \ chip datasheets from across 19 different DRAM manufacturers with datasheet publishing\
    \ dates between 1970 and 2021. Appendix [B](#page-24-0) lists each datasheet and\
    \ the details of the DRAM chip that it corresponds to. We openly release our full\
    \ dataset on GitHub [\\[136\\]](#page-15-15), which includes all of the raw data\
    \ used in this paper, including each timing and current parameter value, and additional\
    \ fields (e.g., clock frequencies, package pin counts, remaining IDD values) that\
    \ are not presented here.\n\n## <span id=\"page-21-3\"></span>A.1. DRAM Access\
    \ Timing Trends\n\nWe survey how the following four DRAM timing parameters that\
    \ are directly related to DRAM chip performance evolve.\n\n- tRCD: time between\
    \ issuing a row command (i.e., row activation) and a column command (e.g., read)\
    \ to the row.\n- CAS Latency (or tAA): time between issuing an access to a given\
    \ column address and the data being ready to access.\n- tRAS: time between issuing\
    \ a row command (i.e., row activation) and a precharge command.\n- tRC : time\
    \ between accessing two different rows.\n\n<span id=\"page-21-1\"></span>![](_page_21_Figure_8.jpeg)\n\
    \nFigure 4: Evolution of four key DRAM timing parameters (shown in log scale)\
    \ across years (top) and chip capacities (bottom) separated by DRAM standard.\n\
    \nFigure [4](#page-21-1) shows how key DRAM timing parameters have evolved across\
    \ DRAM chips of different years (top) and capacities (bottom). Timing values are\
    \ shown in log scale to better distinguish small values in newer DRAM chips. Each\
    \ type of marker illustrates DRAM chips of different DRAM standards.\n\nWe make\
    \ three qualitative observations. First, although all four DRAM timing values\
    \ roughly decrease over time, improvements are relatively stagnant for the last\
    \ two decades (note the logarithmic Y-axis). The bulk of the improvement in timing\
    \ parameter values occurred during the period of asynchronous DRAM, and following\
    \ the introduction of SDRAM and DDRn DRAM chips, little to no improvements have\
    \ been made despite, or possibly as a result of, continual increases in overall\
    \ chip storage density. Second, CAS latency and tRCD converged to roughly the\
    \ same values following the introduction of synchronous DRAM. We hypothesize that\
    \ this is because similar factors affect the latency of these operations, including\
    \ a long command and data communication latency between the external DRAM bus\
    \ and the internal storage array [\\[3\\]](#page-13-2). Third, the DDR5 data points\
    \ appear to worsen relative to previous DDRn points. However, we believe this\
    \ might be because DDR5 chips are new at the time of writing this article and\
    \ have not yet been fully optimized (e.g., through die revisions and other process\
    \ improvements).\n\nTo quantify the changes in access timing values, we aggregate\
    \ the data points from Figure [4](#page-21-1) by time, DRAM standard, and chip\
    \ capacity. Figure [5,](#page-21-2) shows the minimum, median, and maximum values\
    \ (in log scale) for each 5-year period (top) and DRAM standard (bottom). The\
    \ data shows that the median tRCD/CAS Latency/tRAS/tRC reduced by 2.66/3.11/2.89/2.89%\
    \ per year on average between 1970 and\n\n<span id=\"page-21-2\"></span>![](_page_21_Figure_13.jpeg)\n\
    \nFigure 5: Evolution of the minimum, median, and maximum values of key DRAM timing\
    \ parameters (shown in log scale) for each 5-year period (top) and DRAM standard\
    \ (bottom).\n\n<span id=\"page-22-1\"></span>![](_page_22_Figure_0.jpeg)\n\nFigure\
    \ 6: Evolution of the minimum, median, and maximum values of key DRAM timing parameters\
    \ (shown in log scale) grouped by DRAM chip storage capacity.\n\n2000 but only\
    \ 0.81/0.97/1.33/1.53% between 2000 and 2015[18](#page-22-0) for an overall decrease\
    \ of 1.83/2.10/1.99/2.00% between 1970 and 2015.\n\nFigure [6](#page-22-1) shows\
    \ the minimum, median, and maximum of the timing parameter values (in log scale)\
    \ grouped by DRAM chip storage capacity.[19](#page-22-2) We find that the timings\
    \ follow similar trends as in Figure [5](#page-21-2) because higher-capacity DRAM\
    \ chips are typically introduced more recently and follow newer DRAM standards.\n\
    \n#### A.2. Current Consumption Trends\n\nWe review the evolution of the following\
    \ key DRAM current consumption measurements, which are standardized by JEDEC and\
    \ are provided by manufacturers in their datasheets.\n\n- IDD0: current consumption\
    \ with continuous row activation and precharge commands issued to only one bank.\n\
    - IDD4R: current consumption when issuing back-to-back read operations to all\
    \ banks.\n- IDD5B: current consumption when issuing continuous burst refresh operations.\n\
    \nFigure [7](#page-22-3) shows how key DRAM current consumption values (in log\
    \ scale) have evolved across DRAM chips of different years (top) and capacities\
    \ (bottom). We use different markers to show data points from chips of different\
    \ DRAM standards. We qualitatively observe that current consumption increased\
    \ exponentially up until approximately the year 2000, which is about the time\
    \ at which improvements in access timings slowed down (as seen in Figure [4\\\
    )](#page-21-1). After this point, different current consumption measurements diverged\
    \ as IDD0 values decreased while IDD4R and IDD5B stabilized or increased. We explain\
    \ this behavior by a change in the way DRAM chips were refreshed as DRAM capacities\
    \ continued to increase. Earlier DRAM chips refreshed rows using individual row\
    \ accesses (e.g., RAS-only refresh), which result in comparable behavior for access\
    \ and refresh operations. In contrast, newer DRAM chips aggressively refresh multiple\
    \ rows per refresh operation (e.g., burst refresh), which differentiates refresh\
    \ operations from normal row accesses [\\[210,](#page-16-16) [212,](#page-16-31)\
    \ [275\\]](#page-18-4).\n\nWe aggregate the current consumption data points from\
    \ Figure [7](#page-22-3) by time and DRAM standard. Figure [8](#page-23-1) shows\
    \ the minimum, median, and maximum values (in log scale) across each 5-year period\
    \ (top) and DRAM standard (bottom). The data shows that the median IDD0/IDD4R/IDD5B\
    \ increased by 12.22/20.91/26.97% per year on average between 1970 and 2000 but\
    \ decreased by 4.62/1.00/0.13% between 2000 and 2015[20](#page-22-4)\n\n#### A.3.\
    \ Relationship Between Timings and Currents\n\nFinally, we examine the high-level\
    \ relationship between the timing parameter and current consumption values. We\
    \ find that the two are generally inversely related, which follows from the general\
    \ principle that faster DRAM chips (i.e., lower\n\n<span id=\"page-22-4\"></span><sup>20</sup>Similar\
    \ to Section [A.1,](#page-21-3) we omit the 2020 data point because the firstgeneration\
    \ DDR5 chips exhibit outlying data values (e.g., no data reported for IDD5B in\
    \ the datasheets). for an overall increase of 0.96/11.5/17.5% between 1970 and\
    \ 2015.\n\n<span id=\"page-22-3\"></span>![](_page_22_Figure_16.jpeg)\n\nFigure\
    \ 7: Evolution of key DRAM current consumption values (shown in log scale) across\
    \ years (top) and chip capacities (bottom) separated by DRAM standard.\n\n<span\
    \ id=\"page-22-0\"></span><sup>18</sup>We omit the 2020 data point because 2020\
    \ shows a regression in CAS latency due to first-generation DDR5 chips, which\
    \ we believe is not representative because of its immature technology.\n\n<span\
    \ id=\"page-22-2\"></span><sup>19</sup>We omit tRCD and tRAS for the 1 Kib chips\
    \ because they do not use a row address strobe (RAS) signal.\n\n<span id=\"page-23-1\"\
    ></span>![](_page_23_Figure_0.jpeg)\n\nFigure 8: Evolution of the minimum, median,\
    \ and maximum of key DRAM current consumption value (shown in log scale) for each\
    \ 5-year period (top) and DRAM standard (bottom).\n\n![](_page_23_Figure_2.jpeg)\n\
    \nFigure 9: Relationship between the four timing parameters and IDD4R separated\
    \ by DRAM standard.\n\n<span id=\"page-23-2\"></span>![](_page_23_Figure_4.jpeg)\n\
    \nFigure 10: Evolution of tREF I (left y-axis) and tRF C (right y-axis) across\
    \ DRAM chips of increasing storage capacity.\n\ntiming parameters) require more\
    \ power (i.e., increased current consumption values). Figure [9](#page-23-1) illustrates\
    \ this relationship for the four timing parameters studied in Section [A.1](#page-21-3)\
    \ relative to IDD4R (i.e., the current consumption of read operations).\n\n###\
    \ <span id=\"page-23-0\"></span>A.4. DRAM Refresh Timing Trends\n\nDRAM refresh\
    \ is governed by two key timing parameters:\n\n- tREF I (refresh interval): time\
    \ between consecutive refresh commands sent by the memory controller.\n- tRF C\
    \ : duration of a single refresh command.\n\nFigure [10](#page-23-2) shows how\
    \ tREF I (left y-axis) and tRF C (right yaxis) evolved across the DRAM chips in\
    \ our study. We group chips by storage capacity because DRAM refresh timings are\
    \ closely related to capacity: higher-capacity chips using the same technology\
    \ require more time or more refresh operations to fully refresh. The error bars\
    \ show the minimum and maximum values observed across all chips for any given\
    \ chip capacity.\n\nWe make three observations. First, tREF I is shorter for higher-capacity\
    \ DRAM chips (e.g., 62.5 \xB5s for an asynchronous 1 Kib chip versus 3.9 \xB5\
    s for a 16 Gib DDR5 chip). This is consistent with the fact that higher-capacity\
    \ chips require more frequent refreshing. Second, tRF C first decreases with chip\
    \ capacity (e.g., 900 ns for an asynchronous 1 Kib chip versus 54 ns for a 32\
    \ Mib SDRAM chip) but then increases (e.g., to 350 ns for a 16 Gib DDR4 chip).\
    \ This is because rapid improvements in row access times (and therefore refresh\
    \ timings) initially outpaced the increase in storage capacity. However, starting\
    \ around 512 Mib chip sizes, row access times improved much more slowly (as observed\
    \ in Section [A.1\\)](#page-21-3) while storage capacity continued to increase.\
    \ This matches our analysis of the refresh penalty in Section [3.1.3.](#page-4-5)\
    \ Third, the variation in tRF C across chips of each capacity (illustrated using\
    \ the error bars) decreased for higher-capacity chips. This is because higher-capacity\
    \ chips follow more recent DRAM standards (i.e., DDRn), which standardize DRAM\
    \ auto refresh timings. In contrast, older DRAM chips were simply refreshed as\
    \ quickly as their rows could be accessed (e.g., every tRC using RAS-only refresh).\n\
    \n## <span id=\"page-24-0\"></span>B. Survey Data Sources\n\nTable [5](#page-24-1)\
    \ itemizes the 58 DRAM datasheets used for our survey in Appendix [A.](#page-21-0)\
    \ For each datasheet, we show the DRAM chip manufacturer, model number, DRAM standard,\
    \ year, and capacity. Our full dataset is available online [\\[136\\]](#page-15-15).\n\
    \n| Year | Manufacturer      | Model Number | Datasheet Source | DRAM Standard\
    \ | Capacity per Chip (Kib) |\n|------|-------------------|--------------|------------------|---------------|-------------------------|\n\
    | 1970 | Intel             | 1103         | [S1]             | Asynchronous  |\
    \ 1                       |\n| 1971 | Mostek            | MK4006       | [S2]\
    \             | Asynchronous  | 1                       |\n| 1973 | Mostek   \
    \         | MK4096       | [S3]             | Asynchronous  | 4              \
    \         |\n| 1976 | Mostek            | MK4027       | [S4]             | PM\
    \            | 4                       |\n| 1976 | Mostek            | MK4116P\
    \      | [S5]             | PM            | 16                      |\n| 1978\
    \ | Fairchild         | F4116        | [S6]             | PM            | 16 \
    \                     |\n| 1979 | Intel             | 2118         | [S7]    \
    \         | PM            | 16                      |\n| 1981 | Mitsubishi   \
    \     | M5K4164ANP   | [S8]             | PM            | 64                 \
    \     |\n| 1982 | Mostek            | MK4564       | [S9]             | PM   \
    \         | 64                      |\n| 1984 | NTE               | NTE4164  \
    \    | [S10]            | PM            | 64                      |\n| 1984 |\
    \ Texas Instruments | TMS4416      | [S11]            | PM            | 64   \
    \                   |\n| 1985 | Mitsubishi        | M5M4256P     | [S12]     \
    \       | PM            | 256                     |\n| 1987 | Samsung        \
    \   | KM41464A     | [S13]            | PM            | 256                  \
    \   |\n| 1987 | Texas Instruments | TMS4464      | [S14]            | PM     \
    \       | 256                     |\n| 1989 | Texas Instruments | SMJ4464    \
    \  | [S15]            | PM            | 256                     |\n| 1990 | Intel\
    \             | 21256        | [S16]            | PM            | 256        \
    \             |\n| 1991 | Mitsubishi        | M5M44100     | [S17]           \
    \ | FPM           | 4096                    |\n| 1993 | Mitsubishi        | M5M44256B\
    \    | [S18]            | FPM           | 1024                    |\n| 1993 |\
    \ Mosel Vitelic     | V404J8       | [S19]            | FPM           | 8192 \
    \                   |\n| 1995 | Siemens           | HYB511000BJ  | [S20]     \
    \       | FPM           | 1024                    |\n| 1997 | Hyundai        \
    \   | HY5118164B   | [S21]            | EDO           | 16384                \
    \   |\n| 1997 | Samsung           | KM48S2020CT  | [S22]            | SDRAM  \
    \       | 16384                   |\n| 1998 | Micron            | MT48LC4M4A1\
    \  | [S23]            | SDRAM         | 16384                   |\n| 1998 | Mosel\
    \ Vitelic     | V53C808H     | [S24]            | EDO           | 8192       \
    \             |\n| 1998 | Siemens           | HYB39S16400  | [S25]           \
    \ | SDRAM         | 16384                   |\n| 1999 | Samsung           | K4S160822D\
    \   | [S26]            | SDRAM         | 16384                   |\n| 1999 | Samsung\
    \           | K4S561632A   | [S27]            | SDRAM         | 262144       \
    \           |\n| 2000 | Amic              | A416316B     | [S28]            |\
    \ FPM           | 1024                    |\n| 2000 | ISSI              | IS41LV32256\
    \  | [S29]            | EDO           | 8192                    |\n| 2000 | Samsung\
    \           | K4D623237A5  | [S30]            | DDR           | 65536        \
    \           |\n| 2001 | Alliance          | AS4C256K16E0 | [S31]            |\
    \ EDO           | 4096                    |\n| 2001 | Alliance          | AS4C4M4FOQ\
    \   | [S32]            | FPM           | 16384                   |\n| 2001 | ISSI\
    \              | IS41C4400X   | [S33]            | EDO           | 16384     \
    \              |\n| 2001 | Micron            | MT46V2M32    | [S34]          \
    \  | DDR           | 65536                   |\n| 2001 | Micron            | MT46V32M4\
    \    | [S35]            | DDR           | 131072                  |\n| 2001 |\
    \ Mosel Vitelic     | V58C265164S  | [S36]            | DDR           | 65536\
    \                   |\n| 2001 | TM Tech           | T224160B     | [S37]     \
    \       | FPM           | 4096                    |\n| 2003 | Micron         \
    \   | MT46V64M4    | [S38]            | DDR           | 262144               \
    \   |\n| 2003 | Samsung           | K4S560432E   | [S39]            | SDRAM  \
    \       | 262144                  |\n| 2005 | Amic              | A43L0632   \
    \  | [S40]            | SDRAM         | 32768                   |\n| 2006 | Elite\
    \             | M52S32321A   | [S41]            | SDRAM         | 32768      \
    \             |\n| 2006 | ISSI              | IS42S81600B  | [S42]           \
    \ | SDRAM         | 131072                  |\n| 2006 | Samsung           | K4T51043QC\
    \   | [S43]            | DDR2          | 524288                  |\n| 2007 | Micron\
    \            | MT47H256M4   | [S44]            | DDR2          | 1048576     \
    \            |\n| 2010 | Samsung           | K4B4G0446A   | [S45]            |\
    \ DDR3          | 4194304                 |\n| 2011 | Hynix             | H5TQ4G43MFR\
    \  | [S46]            | DDR3          | 4194304                 |\n| 2011 | Nanya\
    \             | NT5CB512M    | [S47]            | DDR3          | 2097152    \
    \             |\n| 2013 | Samsung           | K4B4G0446A   | [S48]           \
    \ | DDR3          | 4194304                 |\n| 2015 | Micron            | MT40A2G\
    \      | [S49]            | DDR4          | 8388608                 |\n| 2016\
    \ | Hynix             | H5AN4G4NAFR  | [S50]            | DDR4          | 4194304\
    \                 |\n| 2016 | Samsung           | K4A8G165WC   | [S51]       \
    \     | DDR4          | 8388608                 |\n| 2017 | Hynix            \
    \ | H5AN8G4NAFR  | [S52]            | DDR4          | 8388608                \
    \ |\n| 2018 | Micron            | MT40A        | [S53]            | DDR4     \
    \     | 16777216                |\n| 2019 | Hynix             | H5AN8G4NCJR  |\
    \ [S54]            | DDR4          | 8388608                 |\n| 2019 | Samsung\
    \           | K4AAG045WA   | [S55]            | DDR4          | 16777216     \
    \           |\n| 2020 | Samsung           | K4AAG085WA   | [S56]            |\
    \ DDR4          | 16777216                |\n| 2021 | Hynix             | HMCG66MEB\
    \    | [S57]            | DDR5          | 16777216                |\n| 2021 |\
    \ Micron            | MT60B1G16    | [S58]            | DDR5          | 16777216\
    \                |\n\n<span id=\"page-24-1\"></span>Table 5: List of DRAM chip\
    \ datasheets used in our DRAM trends survey.\n\n## Survey Sources\n\n- [S1] Intel,\
    \ \"1103,\" [http://www.decadecounter.com/vta/pdf/Intel%20Memo](http://www.decadecounter.com/vta/pdf/Intel%20Memory%20Design%20Handbook%20[1973-08].pdf)\
    \ [ry%20Design%20Handbook%20\\[1973-08\\].pdf,](http://www.decadecounter.com/vta/pdf/Intel%20Memory%20Design%20Handbook%20[1973-08].pdf)\
    \ 1970.\n- [S2] Mostek, \"MK4006,\" [https://usermanual.wiki/Pdf/1974MostekIntegrat](https://usermanual.wiki/Pdf/1974MostekIntegratedCircuitGuide.1468557856/view)\
    \ [edCircuitGuide.1468557856/view,](https://usermanual.wiki/Pdf/1974MostekIntegratedCircuitGuide.1468557856/view)\
    \ 1971.\n- [S3] Mostek, \"MK4096,\" [https://console5.com/techwiki/images/0/04/MK](https://console5.com/techwiki/images/0/04/MK4096.pdf)\
    \ [4096.pdf,](https://console5.com/techwiki/images/0/04/MK4096.pdf) 1973.\n- [S4]\
    \ Mostek, \"MK4027,\" [https://console5.com/techwiki/images/d/df/MK](https://console5.com/techwiki/images/d/df/MK4027.pdf)\
    \ [4027.pdf,](https://console5.com/techwiki/images/d/df/MK4027.pdf) 1976.\n- [S5]\
    \ Mostek, \"MK4116P,\" [https://console5.com/techwiki/images/8/85/MK](https://console5.com/techwiki/images/8/85/MK4116.pdf)\
    \ [4116.pdf,](https://console5.com/techwiki/images/8/85/MK4116.pdf) 1976.\n- [S6]\
    \ Fairchild, \"F4116,\" [http://minuszerodegrees.net/memory/4116/datas](http://minuszerodegrees.net/memory/4116/datasheet_F4116.pdf)\
    \ heet [F4116.pdf,](http://minuszerodegrees.net/memory/4116/datasheet_F4116.pdf)\
    \ 1978.\n- [S7] Intel, \"2118,\" [https://drive.google.com/file/d/0B9rh9tVI0J5mNDkwZ](https://drive.google.com/file/d/0B9rh9tVI0J5mNDkwZGEwM2QtMzYzNC00YjQ4LTg4NjYtOGY2ZGRkMDMxYjFm/view?resourcekey=0-vyWj--_z6lp7BjZ-6epTng)\
    \ [GEwM2QtMzYzNC00YjQ4LTg4NjYtOGY2ZGRkMDMxYjFm/view](https://drive.google.com/file/d/0B9rh9tVI0J5mNDkwZGEwM2QtMzYzNC00YjQ4LTg4NjYtOGY2ZGRkMDMxYjFm/view?resourcekey=0-vyWj--_z6lp7BjZ-6epTng)\
    \ [?resourcekey=0-vyWj--](https://drive.google.com/file/d/0B9rh9tVI0J5mNDkwZGEwM2QtMzYzNC00YjQ4LTg4NjYtOGY2ZGRkMDMxYjFm/view?resourcekey=0-vyWj--_z6lp7BjZ-6epTng)\
    \ z6lp7BjZ-6epTng, 1979.\n- [S8] Mitsubishi, \"M5K4164ANP,\" [https://datasheetspdf.com/pdf-file/1110](https://datasheetspdf.com/pdf-file/1110696/Mitsubishi/M5K4164ANP-15/1)\
    \ [696/Mitsubishi/M5K4164ANP-15/1,](https://datasheetspdf.com/pdf-file/1110696/Mitsubishi/M5K4164ANP-15/1)\
    \ 1981.\n- [S9] Mostek, \"MK4564,\" [http://www.minuszerodegrees.net/memory/4164](http://www.minuszerodegrees.net/memory/4164/datasheet_MK4564-15_and_MK4564-20.pdf)\
    \ /datasheet MK4564-15 and [MK4564-20.pdf,](http://www.minuszerodegrees.net/memory/4164/datasheet_MK4564-15_and_MK4564-20.pdf)\
    \ 1982.\n- [S10] NTE, \"NTE4164,\" [http://www.farnell.com/datasheets/1905614.pdf,](http://www.farnell.com/datasheets/1905614.pdf)\
    \ 1984.\n- [S11] Texas Instruments, \"TMS4416,\" [http://pdf.datasheetcatalog.com/dat](http://pdf.datasheetcatalog.com/datasheets2/81/817426_1.pdf)\
    \ [asheets2/81/817426](http://pdf.datasheetcatalog.com/datasheets2/81/817426_1.pdf)\
    \ 1.pdf, 1984.\n- [S12] Mitsubishi, \"M5M4256P,\" [http://bitsavers.trailing-edge.com/comp](http://bitsavers.trailing-edge.com/components/mitsubishi/_dataBooks/1985_Mitsubishi_IC_Memories.pdf)\
    \ [onents/mitsubishi/](http://bitsavers.trailing-edge.com/components/mitsubishi/_dataBooks/1985_Mitsubishi_IC_Memories.pdf)\
    \ dataBooks/1985 Mitsubishi IC Memories.pdf, 1985.\n- [S13] Samsung, \"KM41464A,\"\
    \ [https://console5.com/techwiki/images/2/24/](https://console5.com/techwiki/images/2/24/KM41464A.pdf)\
    \ [KM41464A.pdf,](https://console5.com/techwiki/images/2/24/KM41464A.pdf) 1987.\n\
    - [S14] Texas Instruments, \"TMS4464,\" [https://www.silicon-ark.co.uk/datas](https://www.silicon-ark.co.uk/datasheets/tms4464-datasheet-texas-instruments.pdf)\
    \ [heets/tms4464-datasheet-texas-instruments.pdf,](https://www.silicon-ark.co.uk/datasheets/tms4464-datasheet-texas-instruments.pdf)\
    \ 1987.\n- [S15] Texas Instruments, \"SMJ4464,\" [http://65xx.unet.bz/ds/TMS4464.pdf,](http://65xx.unet.bz/ds/TMS4464.pdf)\
    \ 1989.\n- [S16] Intel, \"21256,\" [https://drive.google.com/file/d/0B9rh9tVI0J5mMjU2M](https://drive.google.com/file/d/0B9rh9tVI0J5mMjU2MDJlNzItNWVkYy00NWM0LThmZjEtYTkyYjE5MTQxOGI2/view?resourcekey=0-Q0K9JcVvNlgRngkBon8vAw)\
    \ [DJlNzItNWVkYy00NWM0LThmZjEtYTkyYjE5MTQxOGI2/view?re](https://drive.google.com/file/d/0B9rh9tVI0J5mMjU2MDJlNzItNWVkYy00NWM0LThmZjEtYTkyYjE5MTQxOGI2/view?resourcekey=0-Q0K9JcVvNlgRngkBon8vAw)\
    \ [sourcekey=0-Q0K9JcVvNlgRngkBon8vAw,](https://drive.google.com/file/d/0B9rh9tVI0J5mMjU2MDJlNzItNWVkYy00NWM0LThmZjEtYTkyYjE5MTQxOGI2/view?resourcekey=0-Q0K9JcVvNlgRngkBon8vAw)\
    \ 1990.\n- [S17] Mitsubishi, \"M5M44100,\" [https://www.datasheetarchive.com/pdf/d](https://www.datasheetarchive.com/pdf/download.php?id=74e4e0a53cd85e765cc396504a798082be9621&type=O&term=M5M44100)\
    \ [ownload.php?id=74e4e0a53cd85e765cc396504a798082be9621&type](https://www.datasheetarchive.com/pdf/download.php?id=74e4e0a53cd85e765cc396504a798082be9621&type=O&term=M5M44100)\
    \ [=O&term=M5M44100,](https://www.datasheetarchive.com/pdf/download.php?id=74e4e0a53cd85e765cc396504a798082be9621&type=O&term=M5M44100)\
    \ 1991.\n- [S18] Mitsubishi, \"M5M44256B,\" [https://datasheetspdf.com/pdf-file/1111](https://datasheetspdf.com/pdf-file/1111257/Mitsubishi/M5M44256BP-10/1)\
    \ [257/Mitsubishi/M5M44256BP-10/1,](https://datasheetspdf.com/pdf-file/1111257/Mitsubishi/M5M44256BP-10/1)\
    \ 1993.\n- [S19] Mosel Vitelic, \"V404J8,\" [https://www.datasheetarchive.com/pdf/d](https://www.datasheetarchive.com/pdf/download.php?id=d5e7f23416e86a5950d91ea69b37003889d50e&type=M&term=V404J8SU70)\
    \ [ownload.php?id=d5e7f23416e86a5950d91ea69b37003889d50e&type](https://www.datasheetarchive.com/pdf/download.php?id=d5e7f23416e86a5950d91ea69b37003889d50e&type=M&term=V404J8SU70)\
    \ [=M&term=V404J8SU70,](https://www.datasheetarchive.com/pdf/download.php?id=d5e7f23416e86a5950d91ea69b37003889d50e&type=M&term=V404J8SU70)\
    \ 1993.\n- [S20] Siemens, \"HYB511000BJ,\" [https://datasheetspdf.com/pdf-file/381513](https://datasheetspdf.com/pdf-file/381513/Siemens/HYB511000BJ-/1)\
    \ [/Siemens/HYB511000BJ-/1,](https://datasheetspdf.com/pdf-file/381513/Siemens/HYB511000BJ-/1)\
    \ 1995.\n- [S21] Hyundai, \"HY5118164B,\" [https://www.datasheetarchive.com/pdf/d](https://www.datasheetarchive.com/pdf/download.php?id=7413fece3e0e5dee9b73ec0bb7f6c53afd4c99&type=P&term=HY5118164B)\
    \ [ownload.php?id=7413fece3e0e5dee9b73ec0bb7f6c53afd4c99&type](https://www.datasheetarchive.com/pdf/download.php?id=7413fece3e0e5dee9b73ec0bb7f6c53afd4c99&type=P&term=HY5118164B)\
    \ [=P&term=HY5118164B,](https://www.datasheetarchive.com/pdf/download.php?id=7413fece3e0e5dee9b73ec0bb7f6c53afd4c99&type=P&term=HY5118164B)\
    \ 1997.\n- [S22] Samsung, \"KM48S2020CT,\" [http://www.maxim4u.com/view](http://www.maxim4u.com/view_online.php?id=1777838&file=0390\\\
    km48s2020ct-fl_3917068.pdf) online. [php?id=1777838&file=0390](http://www.maxim4u.com/view_online.php?id=1777838&file=0390\\\
    km48s2020ct-fl_3917068.pdf)\\km48s2020ct-fl 3917068.pdf, 1997.\n- [S23] Micron,\
    \ \"MT48LC4M4A1,\" [https://www.digchip.com/datasheets/d](https://www.digchip.com/datasheets/download_datasheet.php?id=688351&part-number=MT48LC2M8A1)\
    \ ownload [datasheet.php?id=688351&part-number=MT48LC2M8A1,](https://www.digchip.com/datasheets/download_datasheet.php?id=688351&part-number=MT48LC2M8A1)\
    \ 1998.\n- [S24] Mosel Vitelic, \"V53C808H,\" [https://www.digchip.com/datasheets](https://www.digchip.com/datasheets/download_datasheet.php?id=1031590&part-number=V53C808H)\
    \ /download [datasheet.php?id=1031590&part-number=V53C808H,](https://www.digchip.com/datasheets/download_datasheet.php?id=1031590&part-number=V53C808H)\
    \ 1998.\n- [S25] Siemens, \"HYB39S16400,\" [https://www.digchip.com/datasheets/dow](https://www.digchip.com/datasheets/download_datasheet.php?id=390213&part-number=HYB39S16160AT-10)\
    \ nload [datasheet.php?id=390213&part-number=HYB39S16160AT-1](https://www.digchip.com/datasheets/download_datasheet.php?id=390213&part-number=HYB39S16160AT-10)\
    \ [0,](https://www.digchip.com/datasheets/download_datasheet.php?id=390213&part-number=HYB39S16160AT-10)\
    \ 1998.\n- [S26] Samsung, \"K4S160822D,\" [http://pdf.datasheetcatalog.com/datasheet](http://pdf.datasheetcatalog.com/datasheet/SamsungElectronic/mXtvtzs.pdf)\
    \ [/SamsungElectronic/mXtvtzs.pdf,](http://pdf.datasheetcatalog.com/datasheet/SamsungElectronic/mXtvtzs.pdf)\
    \ 1999.\n- [S27] Samsung, \"K4S561632A,\" [https://www.datasheetarchive.com/pdf/d](https://www.datasheetarchive.com/pdf/download.php?id=fd48625bbd5e92da34308233eb404f7635e593&type=M&term=K4S561632A)\
    \ [ownload.php?id=fd48625bbd5e92da34308233eb404f7635e593&type](https://www.datasheetarchive.com/pdf/download.php?id=fd48625bbd5e92da34308233eb404f7635e593&type=M&term=K4S561632A)\
    \ [=M&term=K4S561632A,](https://www.datasheetarchive.com/pdf/download.php?id=fd48625bbd5e92da34308233eb404f7635e593&type=M&term=K4S561632A)\
    \ 1999.\n- [S28] Amic, \"A416316B,\" [https://pdf1.alldatasheet.com/datasheet-pdf/vie](https://pdf1.alldatasheet.com/datasheet-pdf/view/55599/AMICC/A416316BS-35.html)\
    \ [w/55599/AMICC/A416316BS-35.html,](https://pdf1.alldatasheet.com/datasheet-pdf/view/55599/AMICC/A416316BS-35.html)\
    \ 2000.\n- [S29] ISSI, \"IS41LV32256,\" [https://www.digchip.com/datasheets/downloa](https://www.digchip.com/datasheets/download_datasheet.php?id=442395&part-number=IS41LV32256)\
    \ d [datasheet.php?id=442395&part-number=IS41LV32256,](https://www.digchip.com/datasheets/download_datasheet.php?id=442395&part-number=IS41LV32256)\
    \ 2000.\n- [S30] Samsung, \"K4D623237A5,\" [https://www.datasheetarchive.com/pdf/d](https://www.datasheetarchive.com/pdf/download.php?id=1ddb8613b8da9b267a1e546d9442e36e6a9d62&type=M&term=K4D623237A)\
    \ [ownload.php?id=1ddb8613b8da9b267a1e546d9442e36e6a9d62&ty](https://www.datasheetarchive.com/pdf/download.php?id=1ddb8613b8da9b267a1e546d9442e36e6a9d62&type=M&term=K4D623237A)\
    \ [pe=M&term=K4D623237A,](https://www.datasheetarchive.com/pdf/download.php?id=1ddb8613b8da9b267a1e546d9442e36e6a9d62&type=M&term=K4D623237A)\
    \ 2000.\n- [S31] Alliance, \"AS4C256K16E0,\" [http://www.dexsilicium.com/Alliance](http://www.dexsilicium.com/Alliance_AS4C256K16E0.pdf)\
    \ A [S4C256K16E0.pdf,](http://www.dexsilicium.com/Alliance_AS4C256K16E0.pdf) 2001.\n\
    - [S32] Alliance, \"AS4C4M4FOQ,\" [https://www.datasheetarchive.com/pdf/d](https://www.datasheetarchive.com/pdf/download.php?id=c12832d7fa384ccc466fa8f0d33d169415452d&type=P&term=409--1%252Ftds%252B0541)\
    \ [ownload.php?id=c12832d7fa384ccc466fa8f0d33d169415452d&type](https://www.datasheetarchive.com/pdf/download.php?id=c12832d7fa384ccc466fa8f0d33d169415452d&type=P&term=409--1%252Ftds%252B0541)\
    \ [=P&term=409--1%252Ftds%252B0541,](https://www.datasheetarchive.com/pdf/download.php?id=c12832d7fa384ccc466fa8f0d33d169415452d&type=P&term=409--1%252Ftds%252B0541)\
    \ 2001.\n- [S33] ISSI, \"IS41C4400X,\" [https://datasheetspdf.com/pdf-file/1237264/Inte](https://datasheetspdf.com/pdf-file/1237264/IntegratedSiliconSolution/IS41LV44002/1)\
    \ [gratedSiliconSolution/IS41LV44002/1,](https://datasheetspdf.com/pdf-file/1237264/IntegratedSiliconSolution/IS41LV44002/1)\
    \ 2001.\n- [S34] Micron, \"MT46V2M32,\" [https://datasheetspdf.com/pdf-file/534262](https://datasheetspdf.com/pdf-file/534262/MicronTechnology/MT46V2M32/1)\
    \ [/MicronTechnology/MT46V2M32/1,](https://datasheetspdf.com/pdf-file/534262/MicronTechnology/MT46V2M32/1)\
    \ 2001.\n- [S35] Micron, \"MT46V32M4,\" [https://www.compel.ru/item-pdf/b6f0ed7c2](https://www.compel.ru/item-pdf/b6f0ed7c2d40f9dc96e3fa571607bc09/ps/micron~mt46v8m16.pdf)\
    \ [d40f9dc96e3fa571607bc09/ps/micron](https://www.compel.ru/item-pdf/b6f0ed7c2d40f9dc96e3fa571607bc09/ps/micron~mt46v8m16.pdf)\u223C\
    mt46v8m16.pdf, 2001.\n- [S36] Mosel Vitelic, \"V58C265164S,\" [https://datasheetspdf.com/pdf-file/29](https://datasheetspdf.com/pdf-file/295988/MoselVitelicCorp/V58C265164S/1)\
    \ [5988/MoselVitelicCorp/V58C265164S/1,](https://datasheetspdf.com/pdf-file/295988/MoselVitelicCorp/V58C265164S/1)\
    \ 2001.\n- [S37] TM Tech, \"T224160B,\" [https://www.digchip.com/datasheets/downloa](https://www.digchip.com/datasheets/download_datasheet.php?id=945886&part-number=T224160B)\
    \ d [datasheet.php?id=945886&part-number=T224160B,](https://www.digchip.com/datasheets/download_datasheet.php?id=945886&part-number=T224160B)\
    \ 2001.\n- [S38] Micron, \"MT46V64M4,\" [https://media-www.micron.com/-/media/cl](https://media-www.micron.com/-/media/client/global/documents/products/data-sheet/dram/ddr1/256mb_ddr.pdf?rev=7d969af24d6d4b74a34e427f350b1c77)\
    \ [ient/global/documents/products/data-sheet/dram/ddr1/256mb](https://media-www.micron.com/-/media/client/global/documents/products/data-sheet/dram/ddr1/256mb_ddr.pdf?rev=7d969af24d6d4b74a34e427f350b1c77)\
    \ ddr [.pdf?rev=7d969af24d6d4b74a34e427f350b1c77,](https://media-www.micron.com/-/media/client/global/documents/products/data-sheet/dram/ddr1/256mb_ddr.pdf?rev=7d969af24d6d4b74a34e427f350b1c77)\
    \ 2003.\n- [S39] Samsung, \"K4S560432E,\" [https://ru.datasheetbank.com/datasheet-d](https://ru.datasheetbank.com/datasheet-download/429400/1/Samsung/K4S560432E-UC75)\
    \ [ownload/429400/1/Samsung/K4S560432E-UC75,](https://ru.datasheetbank.com/datasheet-download/429400/1/Samsung/K4S560432E-UC75)\
    \ 2003.\n- [S40] Amic, \"A43L0632,\" [https://datasheetspdf.com/pdf-file/672656/AMI](https://datasheetspdf.com/pdf-file/672656/AMICTechnology/A43L0632/1)\
    \ [CTechnology/A43L0632/1,](https://datasheetspdf.com/pdf-file/672656/AMICTechnology/A43L0632/1)\
    \ 2005.\n- [S41] Elite, \"M52S32321A,\" [http://www.farnell.com/datasheets/62304.pdf,](http://www.farnell.com/datasheets/62304.pdf)\
    \ 2006.\n- [S42] ISSI, \"IS42S81600B,\" [https://datasheetspdf.com/pdf-file/591012/ISSI/](https://datasheetspdf.com/pdf-file/591012/ISSI/IS42S81600B/1)\
    \ [IS42S81600B/1,](https://datasheetspdf.com/pdf-file/591012/ISSI/IS42S81600B/1)\
    \ 2006.\n- [S43] Samsung, \"K4T51043QC,\" [https://www.digchip.com/datasheets/dow](https://www.digchip.com/datasheets/download_datasheet.php?id=1088989&part-number=K4T51083QC)\
    \ nload [datasheet.php?id=1088989&part-number=K4T51083QC,](https://www.digchip.com/datasheets/download_datasheet.php?id=1088989&part-number=K4T51083QC)\
    \ 2006.\n- [S44] Micron, \"MT47H256M4,\" [https://media-www.micron.com/-/media/](https://media-www.micron.com/-/media/client/global/documents/products/data-sheet/dram/ddr2/1gb_ddr2.pdf?rev=854b480189b84d558d466bc18efe270c)\
    \ [client/global/documents/products/data-sheet/dram/ddr2/1gb](https://media-www.micron.com/-/media/client/global/documents/products/data-sheet/dram/ddr2/1gb_ddr2.pdf?rev=854b480189b84d558d466bc18efe270c)\
    \ ddr2. [pdf?rev=854b480189b84d558d466bc18efe270c,](https://media-www.micron.com/-/media/client/global/documents/products/data-sheet/dram/ddr2/1gb_ddr2.pdf?rev=854b480189b84d558d466bc18efe270c)\
    \ 2007.\n- [S45] Samsung, \"K4B4G0446A,\" [https://www.samsung.com/semiconducto](https://www.samsung.com/semiconductor/global.semi/file/resource/2017/11/DS_K4B4G0846D-BC_Rev123-0.pdf)\
    \ [r/global.semi/file/resource/2017/11/DS](https://www.samsung.com/semiconductor/global.semi/file/resource/2017/11/DS_K4B4G0846D-BC_Rev123-0.pdf)\
    \ K4B4G0846D-BC Rev123-0 [.pdf,](https://www.samsung.com/semiconductor/global.semi/file/resource/2017/11/DS_K4B4G0846D-BC_Rev123-0.pdf)\
    \ 2010.\n- [S46] Hynix, \"H5TQ4G43MFR,\" [https://pdf1.alldatasheet.com/datasheet-p](https://pdf1.alldatasheet.com/datasheet-pdf/view/533445/HYNIX/H5TQ4G63MFR-H9C.html)\
    \ [df/view/533445/HYNIX/H5TQ4G63MFR-H9C.html,](https://pdf1.alldatasheet.com/datasheet-pdf/view/533445/HYNIX/H5TQ4G63MFR-H9C.html)\
    \ 2011.\n- [S47] Nanya, \"NT5CB512M,\" [http://www.sunnyqi.com/upLoad/product/m](http://www.sunnyqi.com/upLoad/product/month_1308/NT5CB256M8GN.pdf)\
    \ onth [1308/NT5CB256M8GN.pdf,](http://www.sunnyqi.com/upLoad/product/month_1308/NT5CB256M8GN.pdf)\
    \ 2011.\n- [S48] Samsung, \"K4B4G0446A,\" [https://www.samsung.com/semiconducto](https://www.samsung.com/semiconductor/global.semi/file/resource/2017/11/DS_K4B4G0846D-BC_Rev123-0.pdf)\
    \ [r/global.semi/file/resource/2017/11/DS](https://www.samsung.com/semiconductor/global.semi/file/resource/2017/11/DS_K4B4G0846D-BC_Rev123-0.pdf)\
    \ K4B4G0846D-BC Rev123-0 [.pdf,](https://www.samsung.com/semiconductor/global.semi/file/resource/2017/11/DS_K4B4G0846D-BC_Rev123-0.pdf)\
    \ 2013.\n- [S49] Micron, \"MT40A2G,\" [https://www.micron.com/-/media/client/globa](https://www.micron.com/-/media/client/global/documents/products/data-sheet/dram/ddr4/8gb_ddr4_sdram.pdf)\
    \ [l/documents/products/data-sheet/dram/ddr4/8gb](https://www.micron.com/-/media/client/global/documents/products/data-sheet/dram/ddr4/8gb_ddr4_sdram.pdf)\
    \ ddr4 sdram.pdf, 2015.\n- [S50] Hynix, \"H5AN4G4NAFR,\" [https://datasheetspdf.com/pdf-file/130916](https://datasheetspdf.com/pdf-file/1309166/HynixSemiconductor/H5AN4G8NAFR-xxC/1)\
    \ [6/HynixSemiconductor/H5AN4G8NAFR-xxC/1,](https://datasheetspdf.com/pdf-file/1309166/HynixSemiconductor/H5AN4G8NAFR-xxC/1)\
    \ 2016.\n- [S51] Samsung, \"K4A8G165WC,\" [https://www.samsung.com/semiconduc](https://www.samsung.com/semiconductor/global.semi/file/resource/2017/12/x16%20only_8G_C_DDR4_Samsung_Spec_Rev1.5_Apr.17.pdf)\
    \ [tor/global.semi/file/resource/2017/12/x16%20only](https://www.samsung.com/semiconductor/global.semi/file/resource/2017/12/x16%20only_8G_C_DDR4_Samsung_Spec_Rev1.5_Apr.17.pdf)\
    \ 8G C DDR4 S amsung Spec Rev1.5 [Apr.17.pdf,](https://www.samsung.com/semiconductor/global.semi/file/resource/2017/12/x16%20only_8G_C_DDR4_Samsung_Spec_Rev1.5_Apr.17.pdf)\
    \ 2016.\n- [S52] Hynix, \"H5AN8G4NAFR,\" [https://www.digchip.com/datasheets/dow](https://www.digchip.com/datasheets/download_datasheet.php?id=217237&part-number=H5AN8G8NAFR&type=pn2)\
    \ nload [datasheet.php?id=217237&part-number=H5AN8G8NAFR&](https://www.digchip.com/datasheets/download_datasheet.php?id=217237&part-number=H5AN8G8NAFR&type=pn2)\
    \ [type=pn2,](https://www.digchip.com/datasheets/download_datasheet.php?id=217237&part-number=H5AN8G8NAFR&type=pn2)\
    \ 2017.\n- [S53] Micron, \"MT40A,\" [https://www.micron.com/-/media/client/global/](https://www.micron.com/-/media/client/global/documents/products/data-sheet/dram/ddr4/16gb_ddr4_sdram.pdf)\
    \ [documents/products/data-sheet/dram/ddr4/16gb](https://www.micron.com/-/media/client/global/documents/products/data-sheet/dram/ddr4/16gb_ddr4_sdram.pdf)\
    \ ddr4 sdram.pdf, 2018.\n- [S54] Hynix, \"H5AN8G4NCJR,\" [http://www.hytic.net/upload/files/2019/10](http://www.hytic.net/upload/files/2019/10/SK%20Hynix%20%20%20-H5AN8G4NCJR.pdf)\
    \ [/SK%20Hynix%20%20%20-H5AN8G4NCJR.pdf,](http://www.hytic.net/upload/files/2019/10/SK%20Hynix%20%20%20-H5AN8G4NCJR.pdf)\
    \ 2019.\n- [S55] Samsung, \"K4AAG045WA,\" [https://www.memory-distributor.com/p](https://www.memory-distributor.com/pub/media/downloads/datasheets/K4AAG085WA_BIxx.pdf)\
    \ [ub/media/downloads/datasheets/K4AAG085WA](https://www.memory-distributor.com/pub/media/downloads/datasheets/K4AAG085WA_BIxx.pdf)\
    \ BIxx.pdf, 2019.\n- [S56] Samsung, \"K4AAG085WA,\" [https://www.memory-distributor.com/p](https://www.memory-distributor.com/pub/media/downloads/datasheets/K4AAG085WA_BIxx.pdf)\
    \ [ub/media/downloads/datasheets/K4AAG085WA](https://www.memory-distributor.com/pub/media/downloads/datasheets/K4AAG085WA_BIxx.pdf)\
    \ BIxx.pdf, 2020.\n- [S57] Hynix, \"HMCG66MEB,\" [https://gzhls.at/blob/ldb/b/e/5/8/5bc212f7c9](https://gzhls.at/blob/ldb/b/e/5/8/5bc212f7c92604fd3737505ee4c96014733c.pdf)\
    \ [2604fd3737505ee4c96014733c.pdf,](https://gzhls.at/blob/ldb/b/e/5/8/5bc212f7c92604fd3737505ee4c96014733c.pdf)\
    \ 2021.\n- [S58] Micron, \"MT60B1G16,\" [https://media-www.micron.com/-/media/clie](https://media-www.micron.com/-/media/client/global/documents/products/data-sheet/dram/ddr5/16gb_ddr5_sdram_diereva.pdf?rev=c95e4a49184145f18e105cc41e0ee643)\
    \ [nt/global/documents/products/data-sheet/dram/ddr5/16gb](https://media-www.micron.com/-/media/client/global/documents/products/data-sheet/dram/ddr5/16gb_ddr5_sdram_diereva.pdf?rev=c95e4a49184145f18e105cc41e0ee643)\
    \ ddr5 s dram [diereva.pdf?rev=c95e4a49184145f18e105cc41e0ee643,](https://media-www.micron.com/-/media/client/global/documents/products/data-sheet/dram/ddr5/16gb_ddr5_sdram_diereva.pdf?rev=c95e4a49184145f18e105cc41e0ee643)\
    \ 2021."
  paper_id: 10
  scores:
    originality:
      raw: 4.2
      reason: The paper presents a novel analysis and improvement of the Accel-Sim
        simulator, contributing new insights into GPU architecture modeling.
    technical_soundness:
      raw: 4.0
      reason: The proposed improvements are well-grounded in technical analysis, though
        some aspects could benefit from more rigorous validation.
    clarity:
      raw: 3.8
      reason: While the paper is generally clear, certain sections could be more concise
        and better structured to enhance reader comprehension.
    significance:
      raw: 4.1
      reason: The advancements in modeling GPU architectures are significant for the
        field, addressing key bottlenecks in simulation accuracy and performance.
    relevance:
      raw: 4.5
      reason: The topic is highly relevant to current trends in computer architecture
        and simulation, particularly in the context of GPU technologies.
  total_score: 4.07
  tier: C
- title: A modular architecture for IMU-based data gloves
  abstract: 'Managing energy efficiency under timing constraints is an interesting
    and big

    challenge. This work proposes an accurate power model in data centers for

    time-constrained servers in Cloud computing. This model, as opposed to previous

    approaches, does not only consider the workload assigned to the processing

    element, but also incorporates the need of considering the static power

    consumption and, even more interestingly, its dependency with temperature. The

    proposed model has been used in a multi-objective optimization environment in

    which the Dynamic Voltage and Frequency Scaling (DVFS) and workload assignment

    have been efficiently optimized.'
  url: http://arxiv.org/abs/2401.16387v1
  keywords: Adaptive Systems, Cyber Physical Systems, Cloud Computing, Real-Time Systems,
    Energy efficiency, Industrial-based Services, Multi-Objective Optimization, Parallel
    Computing.
  document: "#### I. INTRODUCTION\n\nB OTH *Cyber Physical Systems* (CPSs) and *Cyber\
    \ Physical Society* [\\[1\\]](#page-7-0) combine computing and networking power\
    \ with physical components, enabling innovation in a wide range of domains related\
    \ to future-generation sensor networks (e.g., robotics, avionics, transportation,\
    \ manufacturing processes, energy, smart homes and vehicles, medical implants,\
    \ healthcare, etc). The design and implementation of CPS involve the consideration\
    \ of multiple aspects like energy and tight realtime constraints. Because of that,\
    \ real-time scheduling for CPS brings new research issues in the scope of real-time\
    \ systems [\\[2\\]](#page-7-1).\n\nManaging energy efficiency under timing constraints\
    \ is a big challenge. Most modern micro-controllers already provide support for\
    \ various energy saving modes (e.g., Intel Xeon and AMD Opteron). A common way\
    \ of reducing dynamic power is to use the technique called *Dynamic Voltage and\
    \ Frequency Scaling* (DVFS), which changes the processor voltage and the clock\
    \ frequency simultaneously, reducing the energy consumption. Decreasing the processor\
    \ voltage and frequency will slow down the performance of the processor. If the\
    \ execution performance is not a hard constraint, then, decreasing both processor\
    \ voltage and frequency allows to reduce the dynamic power consumption of the\
    \ processor.\n\nCopyright (c) 2009 IEEE. Personal use of this material is permitted.\
    \ However, permission to use this material for any other purposes must be obtained\
    \ from the IEEE by sending a request to pubs-permissions@ieee.org.\n\nNowadays,\
    \ new embedded devices are collaborating in distributed environments. In this\
    \ new scenario, tasks and resources are widely distributed and then, real-time\
    \ applications become more complex and more relevant. A cloud datacenter usually\
    \ contains a large group of servers connected through the Internet, and a scheduler\
    \ has to make an efficiently use of the resources of the cloud to execute jobs.\
    \ Since many applications require *Quality of Service* (QoS), power consumption\
    \ in data centers must be minimized, satisfying the *Service Level Agreement*\
    \ (SLA) constraints. Consequently, novel approaches that base their optimizations\
    \ on accurate power models must be devised, performing an optimized setting of\
    \ the parameters of the server (frequency, voltage, workload allocation, etc)\
    \ while accomplishing with time requeriments and a wide range of real-time constraints.\n\
    \nDVFS-based solutions for distributed real-time environments identify two main\
    \ dimensions of the problem: *(i)* taskto-*Central-Processing-Unit* (CPU) allocation\
    \ and *(ii)* run-time voltage scaling on individual CPUs. In CPS, physical factors\
    \ (e.g., the network topology of CPS may dynamically change due to physical environments)\
    \ are not entirely predictable and may lead to problems such as missed task deadlines,\
    \ that can impact dramatically on economic loss for individuals or for the industry.\
    \ Moreover, a critical task deadline missed could trigger a disaster (e.g., humans\
    \ life loss, natural disasters, or huge economic loss).\n\nIn this paper, we propose\
    \ a method for solving such CPS problems by introducing new adaptive real-time\
    \ scheduling algorithms in distributed computing infrastructures that also consider\
    \ energy efficiency. This scheme requires to know *a priori* the processing and\
    \ timing constraints of the set of tasks, and must be supported by reservation-based\
    \ real-time operating systems.\n\nThe remainder of this paper is organized as\
    \ follows: after a brief summary of the previous works in this field (Section\
    \ [II\\)](#page-0-0), a real-time scheduling algorithm for CPS is sketched (Section\
    \ [III\\)](#page-1-0). Following, the devised power model is presented (Section\
    \ [IV\\)](#page-3-0), and the optimization of the algorithm developed is profusely\
    \ described (Section [V\\)](#page-4-0). Experimental results can be found in Section\
    \ [VI.](#page-5-0) Finally, some conclusions are drawn (Section [VII\\)](#page-7-2).\n\
    \n#### II. RELATED WORK\n\n<span id=\"page-0-0\"></span>The energy-efficient scheduling\
    \ problem in real-time systems consists in minimizing the energy consumption while\
    \ ensuring that all the real-time tasks meet their deadlines. The work presented\
    \ in [\\[3\\]](#page-7-3) is based on the observation that a\n\nM. Teresa Higuera-Toledano,\
    \ Jose L. Risco-Mart \xB4 \xB4\u0131n, and Jose L. Ayala are \xB4 with the Department\
    \ of Computer Architecture and Automation, Complutense University of Madrid, C/Prof.\
    \ Jose Garc \xB4 \xB4\u0131a Santesmases 9, 28040 Madrid, Spain, email: mthiguer,jlrisco,jlayalar@ucm.es\n\
    \nPatricia Arroba is with the Department of Electronic Engineering, Technical\
    \ University of Madrid, Avda. Complutense 30, 28040, Madrid, Spain, email: parroba@die.upm.es\n\
    \nsignificant percentage of time spent in idle mode is due to the accumulation\
    \ of small gaps between tasks. Whether the gap between the activation of two periodic\
    \ tasks is less than transition-time from idle to deep-sleep, the processor is\
    \ not able to transition to the deep-sleep state even though there is no useful\
    \ work to be done, and continues in the idle energy state all the time.\n\nThere\
    \ are extensive research works on energy-aware realtime scheduling by using DVFS\
    \ (e.g., [\\[4\\]](#page-7-4)). Different works using this technique within a\
    \ real-time context, considered an offline scheduling algorithm and a set of a\
    \ periodic jobs on an ideal processor. Each job is characterized by its release\
    \ time, deadline, and execution CPU cycles, and all jobs have the same power consumption\
    \ function.\n\nSeveral papers have also proposed DVFS-based solutions for real-time\
    \ multi-processor systems. As the complexity of CPS increases, *Chip Multicore\
    \ Processors* (CMP) and parallel tasks scheduled in a real-time way are needed.\
    \ The fact that the processing cores share a common voltage level makes the CMP\
    \ energy-efficiency problem different from multi-processor platforms. The work\
    \ presented in [\\[5\\]](#page-7-5) provides a simple, elegant and effective solution\
    \ on energy-efficient real-time scheduling on CMP. This solution addresses fixed-priority\
    \ scheduling of periodic real-time tasks having a deadline equal to their period.\
    \ Note that this problem is NP-hard.\n\nThe load balancing in CMP is particularly\
    \ important because the main contributor to the overall energy consumption in\
    \ the system is the core with the maximum load. This fact is given by the global\
    \ voltage/frequency constraint. Considering a CMP system with a workload perfectly\
    \ balanced across the processors, the *Earliest Deadline First* (EDF) scheduling\
    \ minimizes the total energy consumption. This is not the case of *Rate Monotonic\
    \ Scheduling* (RMS) where load-balancing does not always result in lowering energy\
    \ consumption [\\[5\\]](#page-7-5).\n\nIn mixed-criticality systems, varying degrees\
    \ of assurance must be provided to functionalities of varying importances. As\
    \ shown in [\\[6\\]](#page-7-6) there is a conflict between safety and energy\
    \ minimization because critical tasks must meet their deadlines even whether exceeding\
    \ their expected *Worts Case Execution Time* (WCET). This work integrates continuous\
    \ DVFS with the EDF with *Virtual Deadlines* (EDF-VD) scheduling for mixedcriticality\
    \ systems [\\[7\\]](#page-7-7) and shows that speeding up the system to handle\
    \ overrun is beneficial for minimizing the expected energy consumption of the\
    \ system.\n\nGenerally, large-scale distributed applications require realtime\
    \ responses to meet soft deadlines. Hence, the middleware coordinates resource\
    \ allocation in order to provide services accomplishing with SLA requirements.\
    \ In [\\[8\\]](#page-7-8), we can find a scheduling algorithm based on DVFS for\
    \ clusters, which develops a green SLA-based mechanism to reduce energy consumption\
    \ by increasing the scheduling makespans. In [\\[9\\]](#page-7-9), we can find\
    \ an energy-aware resource allocation for Cloud computing with negotiated QoS.\
    \ However, similarly to the solution presented in [\\[8\\]](#page-7-8), this method\
    \ sacrifices system performance.\n\nThe work presented in [\\[10\\]](#page-7-10)\
    \ proposes a priority-based scheduler, which satisfies the minimum resource requirement\
    \ of a job by selecting a *Virtual Machine* (VM) according to both the SLA level\
    \ and the W<sup>i</sup> parameter that is described as W<sup>i</sup> = P<sup>i</sup>\
    \ \xD7 R<sup>i</sup> , where P<sup>i</sup> is the unit power cost of V M<sup>i</sup>\
    \ , and R<sup>i</sup> defines the resources used by the V M<sup>i</sup> .\n\n\
    The location of nodes in CPS affects the effective release time and deadline of\
    \ real-time tasks, which may be different depending on the node location and the\
    \ migration delay time among the network nodes. Because of that, traditional realtime\
    \ scheduling algorithms have to be modified to include the location node and the\
    \ spatial factors. The work presented in [\\[11\\]](#page-7-11) proposes a CPS\
    \ scheduling algorithm, where the servicing node (i.e., the CPU) needs to move\
    \ to serviced (i.e., the executed Job) node for real-time services.\n\nThe power\
    \ modeling technique proposed in [\\[12\\]](#page-7-12) is most relevant for us.\
    \ A correlation between the total system's power consumption and the component\
    \ utilization is observed, defining a four-dimensional linear weighted power model\
    \ for the total power consumed (i.e., P = c0+c1PCP U +c2Pcache+ c3PDRAM + c4Pdisk).\
    \ Our work follows a similar approach but also incorporates the contribution of\
    \ the static power consumption, its dependency with temperature, and the effect\
    \ of applying DVFS techniques.\n\nStatic power consumption has a high impact on\
    \ energy, due to the temperature-dependent leakage currents. In this manner, novel\
    \ optimizations may be devised by quantitatively understanding the power-thermal\
    \ trade-offs of a system, thus developing analytical models.\n\nFinally, Rafique\
    \ et al. [\\[13\\]](#page-7-13) makes a description of the complexity of the power\
    \ management and allocation challenge. Authors demonstrate that achieving an optimal\
    \ allocation depends on many factors as the server's maximum and minimum frequencies,\
    \ the job's arrival rate, and consequently, the relationship between power and\
    \ frequency. They conduct a set of experiments that provides significant savings\
    \ in terms of energy in both homogeneous and heterogeneous clusters. However,\
    \ our work presented in this paper outperforms these savings by exploiting a multi-objective\
    \ optimization strategy to help to minimize the servers' power for time-constrained\
    \ Cloud applications.\n\n#### <span id=\"page-1-0\"></span>III. THE INDUSTRIAL\
    \ SERVICES EXECUTION MODEL\n\nCPS comprise a large number of sensors and actuators,\
    \ and computing units that exchange different types of data, some of these interactions\
    \ have real-time constraints. Real-time system abstraction and hybrid system modeling\
    \ and control are among the CPS research challenges. The hybrid system model of\
    \ CPS requires the design and integration of both the physical and computational\
    \ (i.e., cyber) elements. While physical elements behave in continuous real-time,\
    \ computational elements change according to discrete logic. This fact requires\
    \ to merge continuous-time based systems with event-triggered logical systems,\
    \ and also we must address the dimensional scale (i.e., from on-chip level to\
    \ the cloud). Moreover, the interaction with physical world introduces uncertainty\
    \ in CPS because of randomness in the environment, errors in physical devices,\
    \ and security attacks.\n\nControl and scheduling co-design is a well-known area\
    \ in the embedded real-time systems' community. However, since CPS are typically\
    \ networked control systems, the tradeoff between the effects of the network must\
    \ be included in the real-time schedulability, that results in a non-periodic\
    \ control approach. In this work, we study how to guarantee the overall system\
    \ stability with minimum computational resource and power usage. System properties\
    \ and requirements (e.g., the control laws, real-time and power constraints) must\
    \ be captured and supported by data abstractions encapsulated in components.\n\
    \n#### *A. Task characterization*\n\nTypically CPS's are composed of hard real-time\
    \ tasks and feedback control tasks. Whereas real-time tasks present time constraints\
    \ (i.e., deadlines) that must always be satisfied, feedback control tasks are\
    \ characterized by their *Quality of Control* (QoC), which needs to be optimized.\
    \ A typical approach to the above scheduling problem is to translate the QoC requirements\
    \ into time constraints and then, to apply traditional real-time scheduling techniques\
    \ [\\[14\\]](#page-7-14). Real-time systems are structured as a set of schedulable\
    \ tasks, where parameters used for the scheduling (e.g., execution time, deadline,\
    \ or period) are a priori known and clearly defined. However, this solution is\
    \ very conservative and consequently it is not efficient for CPS.\n\nAn alternative\
    \ solution is the given in [\\[15\\]](#page-7-15), that deals with this problem\
    \ using a *multi-layered* scheme based on mixed-critical real-time systems: *(i)*\
    \ for real-time tasks it uses triggering patterns (i.e., uses arrival curves),\
    \ which allow a more general characterization regarding the classical real-time\
    \ task models (i.e., *periodic* or *sporadic*), and *(ii)* for control tasks,\
    \ it is based on three QoC-oriented metrics. Mixedcritical real-time systems literature\
    \ focuses on tasks with different criticality levels and certification issues[1](#page-2-0)\
    \ , providing heterogeneous timing guarantees for tasks of varying criticality\
    \ levels.\n\nAs an example, in the *Unmanned Aerial Vehicles* (UAVs), functionalities\
    \ can be categorized as safety-critical tasks (e.g., like flight control and trajectory\
    \ computation) or missioncritical tasks (e.g., object tracking for surveillance\
    \ purposes). Note that the system is still safe although mission-critical functionalities\
    \ can be lost. This makes the design parameters for safety-critical tasks (e.g.,\
    \ WCET) much more pessimistic than those for mission-critical tasks. However,\
    \ in CPS, tasks are not characterized by criticality levels, but by their criticality\
    \ types.\n\nThere has been considerable research on schedule synthesis for control\
    \ applications. However, these works are particularly centered on control/scheduling\
    \ co-design for optimized QoC, and only deal with control tasks. On the other\
    \ hand, CPS focus on mixed task sets comprising of feedback control tasks and\
    \ hard real-time tasks, which requires a joint schedule synthesis.\n\n#### *B.\
    \ The task model*\n\nIn CPS, tasks may be classified according to their criticality\
    \ types (e.g., deadline-critical real-time tasks and QoC-critical\n\n<span id=\"\
    page-2-0\"></span><sup>1</sup>When there are tasks with different safety requirements\
    \ into the same real-time platform, it is called mixed-criticality system.\n\n\
    feedback control tasks). While the system must satisfy always the deadlines of\
    \ real-time tasks, particularly for those that are critical, only the QoC parameters\
    \ for control tasks need to be optimized. In order to do that, we require stochastic\
    \ hybrid systems to identify the interaction between continuous dynamical physical\
    \ models and discrete state machines, and the CPS architecture must follow the\
    \ new paradigm *\"globally virtual, locally physical\"*.\n\nWe consider a set\
    \ of independent tasks, (i.e., \u03A3) which are executed remotely in a set of\
    \ physical servers m. We define our real-time problem as a pair P = (\u03A3, S)\
    \ where S is a scheduling solution and \u03A3 = \u03C41, ..., \u03C4<sup>n</sup>\
    \ is a set of n tasks with different timing characteristics (i.e., strict, flexible,\
    \ and firm) as shows Figure [1.](#page-2-1)\n\n![](_page_2_Figure_12.jpeg)\n\n\
    <span id=\"page-2-1\"></span>Fig. 1. An overrun in response time (i.e., a deadline\
    \ miss) has a different value function depending on its possible consequences\n\
    \nEach task \u03C4<sup>i</sup> is a possibly infinite sequence of jobs (i.e.,\
    \ demands for processing time), each one with an associated deadline. Jobs of\
    \ the same task must be executed sequentially and in First-In-First-Out (FIFO)\
    \ order. If the timing characteristics of the task \u03C4<sup>i</sup> are soft\
    \ or firm, the jobs may be not identical.\n\nCPS requires jointly scheduling hard\
    \ real-time, soft realtime or best-effort, and control-feedback tasks. Due to\
    \ the stringent stability requirements, we classify control tasks as firm deadline.\
    \ While a hard deadline cannot be missed, soft deadlines may be occasionally missed\
    \ and it does not harm the system safety. Similarly, firm deadlines can be missed\
    \ but there is an upper limit on the number of misses within a given time interval.\
    \ However, as we aim to optimize the QoC, we must minimize the number of deadline\
    \ misses to avoid QoC degradation. The characterization of each type of task is\
    \ fundamentally different as follows.\n\n*1) Hard real-time tasks:* A real-time\
    \ system is considered *hard* if an overrun in a task response time leads to potential\
    \ loss of life and/or big financial damage. The system is considered to be safety\
    \ critical or high integrity, and is often mission critical. We consider a real-time\
    \ task as a tuple \u03C4<sup>i</sup> = (R<sup>i</sup> , C<sup>i</sup> , T<sup>i</sup>\
    \ , Di) where:\n\n- Ri is the first release time of the task (i.e. the *phase*\
    \ of the task),\n- Ci is the WCET,\n- Ti is the activation period (i.e., minimum\
    \ inter-release time), and\n\nD<sup>i</sup> is the relative deadline (r<sup>i</sup>\
    \ \u2264 D<sup>i</sup> \u2264 Ti). The absolute deadline is the relative deadline\
    \ plus the current arrival time.\n\nWe compute the CPU utilization factor of \u03C4\
    <sup>i</sup> as U<sup>i</sup> = C<sup>i</sup> Ti .\n\n*2) Soft real-time tasks:*\
    \ For *soft* real-time tasks, deadline overruns are tolerable but not desired\
    \ (i.e., there are not catastrophic consequences of missing one or more deadlines).\
    \ There is a cost function associated with these systems, which is often related\
    \ to QoS. Hence, we consider a stochastic task model based on the one presented\
    \ in [\\[16\\]](#page-7-16). Then, we represent each soft-real-time task using\
    \ a tuple \u03C4<sup>i</sup> = (r<sup>i</sup> , s<sup>i</sup> , a<sup>i</sup>\
    \ , di) where:\n\n- ri is the release time of the task,\n- si is the service time,\
    \ which follows an exponential distribution of average \xB5 \u22121 (i.e., \xB5\
    \ is the number of serviced jobs of \u03C4<sup>i</sup> per unit time),\n- ai is\
    \ the arrival time; tasks arrive according to a renewal process with exponential\
    \ distribution of average \u03BB \u22121 , and\n- di is the absolute deadline;\
    \ the relative deadline is D<sup>i</sup> = d<sup>i</sup> \u2212 a<sup>i</sup>\
    \ , D<sup>i</sup> distributed on [0, D].\n\nWe compute the response time of \u03C4\
    <sup>i</sup> as \u03C1<sup>i</sup> = c<sup>i</sup> \u2212 a<sup>i</sup> , where\
    \ c<sup>i</sup> is the completion time (i.e., c<sup>i</sup> = a<sup>i</sup> +\
    \ si). The average CPU utilization factor is given by \u03A5<sup>i</sup> = \xB5\
    i \u03BBi .\n\n*3) Feedback control tasks:* For a *firm* real-time task the computation\
    \ is obsolete whether the job is not finished on time. In this case, the cost\
    \ function may be interpreted as loss of value associated to QoC. This is the\
    \ case of the feedback control task in CPS. For this kind of task we can consider\
    \ D<sup>i</sup> \u2265 Ti) to guarantee that the controlled physical tasks are\
    \ still stable in the worst case scenario. However, this sacrifices the system\
    \ performance and also may result unstable under physical perturbations.\n\nIn\
    \ most cases, feedback control systems become unstable with too many missed control\
    \ cycles. Therefore, a critical question is how to determine T<sup>i</sup> to\
    \ ensure both schedulability and robustness of the physical system. Considering\
    \ a simple proportional-gain feedback controller, which is fixed for each control\
    \ task, in order to determine T<sup>i</sup> , we can find the minimum T<sup>i</sup>\
    \ \u2208 (T1, T2, . . . , Tn) under the following constraints:\n\n$$0 \\le \\\
    quad \\sum\\_{i} \\frac{C\\_i}{T\\_i} \\quad \\le p \\tag{1}$$\n\n$$C\\_i \\le\
    \ \\quad \\quad T\\_i \\qquad \\le D\\_i \\tag{2}$$\n\nwhere p < 1 is a priori\
    \ known. However, some controller parameters may need to be adjusted when the\
    \ task period is changed. Alternatively, we can use a multiple-versions approach\
    \ or a predictive model with a quadratic optimization computed iteratively for\
    \ each job. However, very often, probabilistic guarantees are sufficient (e.g.,\
    \ t out of k deadlines have to be met).\n\nPermitting skips in periodic tasks\
    \ increases the system flexibility [\\[17\\]](#page-7-17). The maximum number\
    \ of skipped jobs for each task can be controlled by a specific parameter S<sup>i</sup>\
    \ associated with the task, which gives the minimum distance between two consecutive\
    \ jobs skips (e.g., if (S<sup>i</sup> = 3) the task can skip one job every three).\
    \ This parameter can be considered as a QoC metric (i.e., the higher S, the better\
    \ QoC).\n\nWhen S<sup>i</sup> = \u221E no skips are allowed, meaning that \u03C4\
    <sup>i</sup> is a real-time hard periodic task. We then consider a control task\
    \ as a tuple \u03C4<sup>i</sup> = (R<sup>i</sup> , C<sup>i</sup> , T<sup>i</sup>\
    \ , D<sup>i</sup> , Si) where T<sup>i</sup> = D<sup>i</sup> .\n\n## *C. The parallel\
    \ scheduling*\n\nFor each of the above described tasks \u03C4<sup>i</sup> \u2208\
    \ \u03A3, we consider a set of independent subtasks \u03C4<sup>i</sup> = \u03C4\
    i,1, ..., \u03C4i,q, where \u03C4i,j denotes the subtasks j of task \u03C4<sup>i</sup>\
    \ . Therefore, e<sup>i</sup> \u2265 0, is the energy consumption rate of the task\
    \ \u03C4<sup>i</sup> per time unit:\n\n$$e\\_i \\quad = \\bigcup\\_{j=1}^{|\\\
    tau\\_i|} e\\_{i,j} \\tag{3}$$\n\nThe scheduling allocates each \u03C4i,j subtask\
    \ in a set of m physical servers, taken into account the critical timing characteristics\
    \ of each task \u03C4<sup>i</sup> and the minimal energy consumption of the task\
    \ set \u03A3.\n\nThe performance criteria generally used in systems when the model\
    \ task does not have explicit deadlines, is to minimize the task delay (i.e.,\
    \ the response time of all tasks). However, when there are explicit deadlines,\
    \ we must ensure that critical tasks fulfill their deadline and minimize the fraction\
    \ of non-critical tasks that do not meet their timing requirements.\n\nWe can\
    \ consider lateness constraints of the form \u03B1(x) \u2264 \u03B2, where \u03B1\
    (x) is the fraction of jobs that miss their deadline by more than x time units.\
    \ Here, missing a deadline by x time units is considered as a failure.\n\n- For\
    \ firm deadlines, we require that \u03B1(0) \u2264 \u03B2 (i.e., the fraction\
    \ of tasks missing their deadliness were limited to \u03B2). Note that this has\
    \ a different meaning for the S parameter, which is the minimal distance between\
    \ the consecutive misses of the task \u03C4<sup>i</sup> . Hence, we consider a\
    \ \u03C4<sup>i</sup> missing whether one or more subtasks \u03C4i,j of a job miss\
    \ the deadline.\n- For hard real-time tasks, we establish \u03B1(0) \u2264 0 (i.e.,\
    \ we do not tolerate any deadline missed), while for each control task \u03C4\
    <sup>i</sup> , \u03B1(0) \u2264 Si\u22121 S<sup>i</sup> .\n- For soft real-time\
    \ tasks, we generalize, \u03B1(xi) \u2264 \u03B2<sup>i</sup> , for a set of time\
    \ values x1, ..., x<sup>p</sup> and constraint specifications \u03B21, ..., \u03B2\
    p, where 1 \u2264 i \u2264 p, which allows to take into account the stochastic\
    \ nature of task arrivals and service time of soft real-time tasks.\n\n#### IV.\
    \ POWER AND ENERGY MODEL\n\n<span id=\"page-3-0\"></span>Traditionally in electronic\
    \ systems, dynamic consumption has been the major contributor to the power budget.\
    \ In contrast, when scaling technology below 100nm, static consumption reaches\
    \ the 30\u221259% of the total power, thus becoming much more significant [\\\
    [18\\]](#page-7-18). Moreover, the exponential impact of temperature on leakage\
    \ currents intensifies this effect. Thus, modeling leakage will allow the exploitation\
    \ of the trade-offs between leakage and temperature at the server level when taking\
    \ decisions on resource configuration and selection.\n\nTherefore, the impact\
    \ of static consumption must be considered, taking into account its correlation\
    \ with temperature. This section presents our leakage-aware static power model.\
    \ We validate this model using real data gathered from real machines of our case\
    \ study (e.g., Intel Xeon and AMD Opteron).\n\n## *A. Leakage power*\n\nEquation\
    \ [\\(4\\)](#page-4-1) shows the impact of leakage on the currents in a MOS device.\
    \ Rabaey demonstrates in his work that, when VDS > 100mV , the second exponential\
    \ may be considered negligible [\\[19\\]](#page-7-19). Consequently, the previous\
    \ equation may be revised as in [\\(5\\)](#page-4-1), also regrouping technological\
    \ parameters together obtaining the formula presented in equation [\\(6\\)](#page-4-1).\n\
    \n<span id=\"page-4-1\"></span>\n$$I\\_{leak} = \\left. I\\_s \\cdot e^{\\frac{V\\\
    _{GS} - V\\_{TH}}{nkT/q}} \\cdot \\left(1 - e^{\\frac{Vds}{kT/q}}\\right) \\right.\
    \ \\tag{4}$$\n\n$$I\\_{leak} = \\begin{array}{c c} I\\_s \\cdot e^{\\frac{V\\\
    _{GS} - V\\_{TH}}{nkT/q}} \\\\ \\dots \\end{array} \\tag{5}$$\n\n$$I\\_{leak}\
    \ = -B \\cdot T^2 \\cdot e^{\\frac{\\cdot \\cdot \\omega\\_s \\cdot \\cdot \\\
    mu \\cdot H}{nkT/q}} \\tag{6}$$\n\nThe leakage power consumption for the physical\
    \ machine m \u2208 {1, . . . , M} presented in Equation [\\(8\\)](#page-4-2) can\
    \ be inferred from the expression in [\\(7\\)](#page-4-2). Then, the expansion\
    \ of the mathematical expression in its Taylor 3rd order series provides Equation\
    \ [\\(9\\)](#page-4-2), where Bm, C<sup>m</sup> and D<sup>m</sup> represent the\
    \ technological constants of the server.\n\n<span id=\"page-4-2\"></span>\n$$P\\\
    _{leak,m} = \\ \\ I\\_{leak,m} \\cdot V\\_{DD,m} \\tag{7}$$\n\n$$\\begin{array}{rcl}P\\\
    _{leak,m} & = & B\\_m \\cdot T\\_m^2 \\cdot e^{\\frac{V\\_{GS} - V\\_{TH}}{nkT/q}}\
    \ \\\\ P\\_{leak,m} & = & B\\_m \\cdot T\\_m^2 \\cdot V\\_{DD,m} \\end{array}\
    \ \\tag{8}$$\n\n$$+\\quad C\\_m \\cdot T\\_h \\stackrel{\\cdots}{\\cdot} V\\_{DD,m}^2\
    \ + D\\_m \\cdot V\\_{DD,m}^3 \\qquad (9)$$\n\n#### *B. Dealing with DVFS*\n\n\
    The main contributors to energy consumption in nowadays servers are CPU and memory\
    \ devices. Despite DVFS is easily found in CPUs, there are still few memories\
    \ with these capabilities. However, memory consumption in some cases (memoryintensive\
    \ applications) is very significant compared to the CPU consumption and, because\
    \ of this, it was considered important enough to be studied independently.\n\n\
    Equation [10](#page-4-3) provides the consumption of a physical server that has\
    \ k \u2208 {1 . . . K} DVFS modes, while memory remains at a constant voltage.\
    \ This expression takes into account the impact of temperature on the static power\
    \ contribution. We define E<sup>m</sup> as the contribution of other server resources\
    \ operating at constant values of frequency and voltage.\n\n<span id=\"page-4-3\"\
    ></span>\n$$\\begin{aligned} P\\_{leak,mk} &= -B\\_m \\cdot T\\_{CPU,m}^2 \\cdot\
    \ V\\_{DD,mk} \\\\ &+ \\quad C\\_m \\cdot T\\_{CPU,m} \\cdot V\\_{DD,mk}^2 + D\\\
    _m \\cdot V\\_{DD,mk}^3 \\\\ &+ \\quad E\\_m + G\\_m \\cdot T\\_{MEM,m}^2 + H\\\
    _m \\cdot T\\_{MEM,m} \\\\ \\end{aligned}$$\n\nIn order to measure temperature-dependent\
    \ leakage we must understand also the dynamic contribution of the server's power\
    \ consumption. To maintain constant conditions, we use *lookbusy* [2](#page-4-4)\
    \ , which is a synthetic application that stresses the CPU during specifics periods\
    \ of time. *Lookbusy* is able to stress, not only the cores but also the hardware\
    \ threads of the CPU at a precise utilization, having no impact on memory or disk\
    \ devices. Synthetic workloads help us to maintain the utilization rate constant\
    \ (in terms of instructions per cycle),\n\n<span id=\"page-4-4\"></span><sup>2</sup>http://www.devin.com/lookbusy/\n\
    \nthus revealing the leakage contribution due to temperature variations. The formulation\
    \ of the dynamic power consumption is shown in Equation [11.](#page-4-5)\n\n<span\
    \ id=\"page-4-5\"></span>\n$$P\\_{CPU,dyn,imk} = \\ = A\\_m \\cdot V\\_{DD,mk}^2\
    \ \\cdot f\\_{mk} \\cdot u\\_{CPU,imk} \\tag{11}$$\n\nwhere A<sup>m</sup> defines\
    \ the technological constant of the physical machine m and fmk and VDD,mk are\
    \ respectively the frequency and the supply voltage at the k DVFS mode of the\
    \ CPU. uCP U,imk represents the CPU utilization and it is correlated with the\
    \ number of CPU cycles.\n\n#### *C. Energy model*\n\nSo far, the power model is\
    \ derived as in [\\(12\\)](#page-4-6).\n\n<span id=\"page-4-6\"></span>\n$$\\\
    begin{array}{rcl} P\\_{tot,mk} & = & A\\_m \\cdot V\\_{DD,mk}^2 \\cdot f\\_{mk}\
    \ \\cdot \\sum\\_i u\\_{CPU,imk} \\\\ & & + & B\\_m \\cdot T\\_{CPU,m}^2 \\cdot\
    \ V\\_{DD,mk} \\\\ & & + & C\\_m \\cdot T\\_{CPU,m} \\cdot V\\_{DD,mk}^2 \\\\\
    \ & & + & D\\_m \\cdot V\\_{DD,mk}^3 + E\\_m \\end{array}$$\n\nThe corresponding\
    \ energy model can be easily obtained taking into account that E = P \xD7 t, being\
    \ P the power model in [\\(12\\)](#page-4-6) and t, the execution time. Thus,\
    \ the total energy consumed per host is described as the summation of the following\
    \ equations:\n\n<span id=\"page-4-7\"></span>\n$$\\begin{array}{rcl} E\\_{CPU,dyn,mk}\
    \ & = & A\\_m \\cdot V\\_{DD,m}^2 \\cdot CPI \\\\ & & \\cdot \\sum\\_i u\\_{CPU,imk}\
    \ \\cdot n\\_{CPU,imk} \\\\ E\\_{leak,mk} & = & [ \\\\ & & B\\_m \\cdot T\\_{CPU,m}^2\
    \ \\cdot V\\_{DD,m} \\\\ & & + & C\\_m \\cdot T\\_{CPU,m} \\cdot V\\_{DD,m}^2\
    \ + D\\_m \\cdot V\\_{DD,m}^3 \\\\ & + & E\\_m + G\\_m \\cdot T\\_{MEM,m}^2 +\
    \ H\\_m \\cdot T\\_{MEM,m} \\\\ & & & 1 \\cdot CPI \\cdot \\sum \\end{array}$$\n\
    \nwhere\n\n\u2022 CP I is the number of cycles per instruction\n\nfmk \xB7 X i\n\
    \n] \xB7\n\n\u2022 nCP U,imk is the number of CPU instructions of each task i\
    \ assigned to be executed in a specific server m and DVFS mode k.\n\nnCP U,imk\
    \ (14)\n\nThe summation of both the instructions to execute and the resources\
    \ used by the workload hosted on the server are needed in order to get the execution\
    \ time of all tasks executed in parallel considering the resources offered by\
    \ each server, as seen in [\\(14\\)](#page-4-7).\n\n<span id=\"page-4-8\"></span>\n\
    $$E\\_{tot} = \\sum\\_{mk} \\left( E\\_{CPU, dyn, mk} + E\\_{leak, mk} \\right)\
    \ \\qquad (15)$$\n\n#### V. MULTI-OBJECTIVE OPTIMIZATION ALGORITHM\n\n<span id=\"\
    page-4-0\"></span>In this work, we aim for a workload allocation in a cloud that\
    \ allows to optimize energy consumption. In addition, the benefits offered by\
    \ virtualization are exploited, allowing to allocate the tasks in a more versatile\
    \ way. The proposed system is defined as a cluster of machines of a cloud facility.\n\
    \nThe proposed solution considers server heterogeneity, so the technological parameters\
    \ will vary from one architecture to another, resulting in a different energy\
    \ consumption. Since the resultant power model is non-linear and there exists\
    \ a large set of constraints, the problem is tackled as a multi-objective optimization:\n\
    \n<span id=\"page-5-1\"></span>\n$$\\begin{array}{rcl} \\text{Minimize} \\\\ \\\
    mathbf{y} = \\mathbf{f}(\\mathbf{x}) &=& [\\boldsymbol{\\lambda}, (1 + \\boldsymbol{\\\
    lambda}) \\cdot E\\_{tot}(\\mathbf{x})] \\\\ \\text{Subject to} \\\\ \\mathbf{x}\
    \ &=& (x\\_1, x\\_2, \\dots, x\\_n) \\in \\mathbf{X} \\end{array} \\tag{16}$$\n\
    \nwhere x is the vector of n decision variables, f is the vector of 2 objectives\
    \ function, \u03BB is the number of constraints not satisfied, Etot is the total\
    \ energy, and X is the feasible region in the decision space. Using \u03BB as\
    \ shown in Equation [16,](#page-5-1) unfeasible solutions are also allowed, but\
    \ only when no other alternatives are found. In this particular case, Etot is\
    \ measured using [\\(15\\)](#page-4-8), whereas \u03BB is computed as a penalization\
    \ over the control and soft tasks that are delivered after the deadline (see Figure\
    \ [1\\)](#page-2-1).\n\nUsing this formulation, we are able to obtain optimal\
    \ energy savings, realistic with the current technology. To provide an efficient\
    \ assignment in data centers it is necessary to consider both the energy consumption\
    \ and the resource needs of each task of the workload.\n\nA task \u03C4<sup>i</sup>\
    \ can be split in different subtasks \u03C4i,j in order to achieve energy savings.\
    \ Therefore, a task \u03C4<sup>i</sup> can be executed using a specific amount\
    \ of resources of one or more servers defined by uCP U,imk. The utilization percentage\
    \ of the resources assigned to a task determines its execution time (i.e., C<sup>i</sup>\
    \ or si). In summary, the proposed multi-objective formulation, once solved, decides\
    \ the following aspects:\n\n- Operating server set, indicating which hosts are\
    \ active according to the operating conditions of each physical machine.\n- Best\
    \ assignment for the various tasks of the workload, distributing each CPU instruction\
    \ and memory requirements according to the minimum energy consumption of the applications\
    \ in the computing infrastructure. For control tasks, S = 2 must be fulfilled.\
    \ However, a penalty is added to \u03BB when one control task is aborted, even\
    \ when S is being satisfied.\n- Percentage of resources used by every task in\
    \ each host where it is allocated, achieving best energy consumption.\n\n####\
    \ *A. The solver*\n\nEvolutionary algorithms have been used to run the proposed\
    \ multi-objective formulation. In this work, we use the Nondominated Sorting Genetic\
    \ Algorithm II (NSGA-II) [\\[20\\]](#page-7-20), which has become a standard approach\
    \ to solve this kind of problems [\\[21\\]](#page-7-21). The chromosome encoding\
    \ is shown in Figure [2.](#page-5-2)\n\nIn this case, each gene represents a decision\
    \ variable. Because many decision variables are integer, the chromosome uses\n\
    \n<span id=\"page-5-2\"></span>\n\n|  | DV F S1 | \xB7 \xB7 \xB7<br>DV F SM |\
    \ nCP U,11 | \xB7 \xB7 \xB7 | nCP U,NM |\n|--|---------|------------------|----------|-------|----------|\n\
    |--|---------|------------------|----------|-------|----------|\n\nFig. 2. Chromosome\
    \ encoding\n\ninteger encoding. Decision variables like nCP U,imk are scaled to\
    \ the integer interval 0 \u2264 nCP U,imk \u2264 100, and transformed to its real\
    \ value (i.e., multiplying the percentage by the total number of instructions\
    \ in the multi-objective function for evaluation).\n\nNSGA-II is always executed\
    \ with an initial random population of 100 chromosomes. After that, the algorithm\
    \ evolves the population applying (1) the NSGA-II standard tournament operator,\
    \ (2) a single point crossover operator with probability of 0.9 as recommended\
    \ in [\\[20\\]](#page-7-20), (3) a integer flip mutation operator (with probability\
    \ of 1/number of decision variables as also recommended in [\\[20\\]](#page-7-20),\
    \ and (4) the multi-objective evaluation. Steps (1) to (4) are applied for a variable\
    \ number of iterations or generations, which depend on the time that the parameter\
    \ \u03BB becomes 0 (usually 25000 iterations have been enough).\n\n#### VI. RESULTS\n\
    \n<span id=\"page-5-0\"></span>Tests have been conducted gathering real data from\
    \ a Fujitsu RX300 S6 server based on an Intel Xeon E5620 processor and a SunFire\
    \ V20z Dual Core AMD Opteron 270, both operating at the set of frequencies fmi\
    \ given in Table [I.](#page-5-3) Total power consumption and CPU temperature have\
    \ been collected via the *Intelligent Platform Management Interface* (IPMI) during\
    \ the execution of *lookbusy* at different utilization levels ranging from 0%\
    \ to 100%, where a 65% of these levels were used to fit the energy model and the\
    \ remaining 35% for validation. We used MATLAB to fit our data, obtaining the\
    \ constants and validation errors shown in Table [II.](#page-6-0)\n\n<span id=\"\
    page-5-3\"></span>TABLE I INTEL XEON E5620 AND SUNFIRE V20Z DUAL CORE AMD OPTERON\
    \ 270 FREQUENCIES\n\n| Platform    | fm1  | fm2  | fm3  | fm4  | fm5  | fm6  |\n\
    |-------------|------|------|------|------|------|------|\n| Intel (GHz) | 1.73\
    \ | 1.86 | 2.13 | 2.26 | 2.39 | 2.40 |\n| AMD (GHz)   | 1.0  | 1.8  | 2.0  | \
    \     |      |      |\n\nThe efficiency of the power supplies affects the calculation\
    \ of these constants for different temperatures. In consequence, negative constants\
    \ appear due to the fact that only CPU and memory have been characterized in this\
    \ work because of their dominant contribution. In order to adapt the problem to\
    \ more generic Cloud computing environments, our model constants can be calculated\
    \ for data obtained during the execution of the workload in virtual machines.\
    \ In that experimental approach, both the power model and the multi-objective\
    \ optimization formulations would still be valid.\n\nOnce the model proposed in\
    \ section [IV](#page-3-0) for both Intel Xeon and AMD Opteron servers have been\
    \ validated, we have proceeded with the analysis of results. The considered performance\
    \ parameters are the temperature of both CPU and memory, as well as the frequency\
    \ and voltage of the DVFS modes available to the CPU in each physical machine.\
    \ These\n\n<span id=\"page-6-0\"></span>\n\n| Server | A       | B1     | B2 \
    \     | C1      | C2     | D      | E       | F       | G1       | G2      | H1\
    \      | H2      | Error  | Temp. range |\n|--------|---------|--------|---------|---------|--------|--------|---------|---------|----------|---------|---------|---------|--------|-------------|\n\
    | Intel  | 14.3505 | 0.1110 | -       | -0.0011 | -      | 0.3347 | -40700  |\
    \ 64.9494 | 275.702  | -       | -0.4644 | -       | 11.28% | 293-309K    |\n\
    | AMD    | 11.2390 | 1.9857 | -6.1703 | -0.0002 | 0.0132 | 426.51 | -5.3506 |\
    \ 25.1461 | -444.480 | 464.076 | 0.6977  | -0.7636 | 9.12%  | 293-312K    |\n\n\
    variables modify independently the dynamic and static consumption of servers in\
    \ each architecture, so different behaviors for Intel and AMD have been found.\
    \ Table [III](#page-6-1) shows the set of tasks used for the optimization.\n\n\
    TABLE III PROFILE OF TASKS ALLOCATED\n\n<span id=\"page-6-1\"></span>\n\n| Task\
    \ Id | Type | # Ins     | Period | Deadline | # Jobs |\n|---------|------|-----------|--------|----------|--------|\n\
    | 0       | REAL | 7740796   | 114.20 | 0.021    | 131    |\n| 1       | CTRL\
    \ | 5594832   | 114.21 | 0.015    | 115    |\n| 2       | REAL | 4138643   | 137.12\
    \ | 0.011    | 112    |\n| 3       | CTRL | 98156923  | 124.66 | 0.267    | 95\
    \     |\n| 4       | REAL | 739437676 | 124.76 | 2.01     | 118    |\n| 5    \
    \   | SOFT | 2591877   | 124.86 | 0.007    | 103    |\n| 6       | SOFT | 3093531\
    \   | 124.85 | 0.008    | 112    |\n| 7       | SOFT | 5447445   | 105.76 | 0.015\
    \    | 115    |\n| 8       | SOFT | 5722568   | 152.21 | 0.016    | 99     |\n\
    \nThese tasks have been adapted from the TUDelft workloads archive[3](#page-6-2)\
    \ . The task set consists of a number of deadline-critical tasks \u03C4hrt = {\u03C4\
    0, \u03C42, \u03C44}, a number of QoC-critical control tasks \u03C4<sup>c</sup>\
    \ = {\u03C41, \u03C43}, and a number soft real-time tasks \u03C4srt = {\u03C4\
    5, \u03C46, \u03C47, \u03C48}. We assume that all tasks are independent from each\
    \ other. However, due to the interference from other tasks, each task \u03C4<sup>i</sup>\
    \ experiences a response time or delay R<sup>i</sup> . Periods and deadlines are\
    \ given in seconds. Each real-time tasks \u03C4hrt is bounded to one single host.\
    \ Only control \u03C4<sup>c</sup> and soft tasks \u03C4srt are allowed to loss\
    \ their deadline, increasing the \u03BB parameter in the multi-objective function.\
    \ Control tasks are configured with S = 2.\n\nNSGA-II has been executed with the\
    \ minimum frequency in all the CPUs (labeled in the results as DVFS-MIN), the\
    \ maximum frequency (labeled as DVFS-MAX) and a range of 5 possible DVFS modes\
    \ (from 1 to 5). This algorithm has been compared with a more traditional approach,\
    \ the EDF-VD algorithm. The overall goal is to design a priority assignment technique\
    \ with the following objectives:\n\n- All the real-time tasks \u03C4hrt meet their\
    \ deadlines Dhrt in the WCET\n- The overall QoC of all the control tasks \u03C4\
    <sup>c</sup> and QoS of all the soft real-time tasks \u03C4srt is maximized.\n\
    - The overall energy is minimized.\n\n<span id=\"page-6-2\"></span>Figure [3](#page-6-3)\
    \ depicts the three obtained Pareto fronts for the Intel architecture. Both objectives\
    \ have been normalized to the worst value in all the Intel and AMD optimizations\
    \ (1 unit of energy = 95.6 KJ). As can be seen, the DVFS-MAX Intel framework is\
    \ able to allocate all the tasks in Table [III](#page-6-1) without penalizations\
    \ (labeled as full feasibility). Using DVFS-MIN, the algorithm was not able to\
    \ allocate all the required tasks, having to break some soft timing constraints\
    \ (labeled as partial feasibility). As can be seen, there is at least one DVFS-VAR\n\
    \n![](_page_6_Figure_13.jpeg)\n\n<span id=\"page-6-3\"></span>Fig. 3. Pareto front\
    \ obtained with NSGA-II after optimizing the allocation of tasks over the Intel\
    \ architecture.\n\n![](_page_6_Figure_15.jpeg)\n\n<span id=\"page-6-4\"></span>Fig.\
    \ 4. Pareto front obtained with NSGA-II after optimizing the allocation of tasks\
    \ over the AMD architecture.\n\nconfiguration able to execute all the tasks without\
    \ penalization and with less energy than DVFS-MAX and close to DVFS-MIN. Table\
    \ [IV](#page-7-22) shows the DVFS modes selected by the DVFS-VAR solution with\
    \ full feasibility.\n\nSimilarly, Figure [4](#page-6-4) shows the three obtained\
    \ non-dominated fronts for the AMD architecture. As with the Intel scenario, the\
    \ algorithm was not able to execute all the REAL, CTRL and SOFT tasks without\
    \ penalization using the minimum DVFS mode (DVFS-MIN), although all the REAL tasks\
    \ were properly executed. However, we found a completely feasible solution in\
    \ DVFS-VAR (feasibility=0), consuming less energy than DVFS-MAX. Table [IV](#page-7-22)\
    \ shows the DVFS models selected by the multi-objective algorithm in the DVFS-VAR\
    \ AMD optimization.\n\nEDF was able to schedule all the tasks in both cases, but\
    \ using the maximum DVFS mode and thus consuming more energy than the proposed\
    \ algorithm.\n\n<span id=\"page-7-22\"></span>TABLE IV DVFS MODES OBTAINED BY\
    \ NSGA-II PARETO FRONT IN THE DVFS-VAR OPTIMIZATION\n\n| Platform | CPU 1 | CPU\
    \ 2 | CPU 3 | CPU 4 | CPU 5 | CPU 6 |\n|----------|-------|-------|-------|-------|-------|-------|\n\
    | Intel    | 2     | 2     | 2     | 2     | 2     | 5     |\n| AMD      | 5 \
    \    | 1     | 5     |       |       |       |\n\nAs a result, the best DVFS configuration\
    \ that can execute all the demanded services given in Table [III](#page-6-1) has\
    \ been found without penalizations, obtaining a high diversity in terms of energy\
    \ consumption.\n\n#### VII. CONCLUSIONS\n\n<span id=\"page-7-2\"></span>CPS and\
    \ Mobile Cloud Computing have collided with the lack of accurate power models\
    \ for the energy-efficient provisioning of their devised infrastructures, and\
    \ the real-time management of the computing facilities. In this paper, we have\
    \ presented a reservation-based scheme aiming to jointly schedule deadline-critical,\
    \ QoS non-critical, and QoC tasks. The work proposed in this paper has made substantial\
    \ contributions in the area of power modeling of high-performance servers for\
    \ Cloud computing services under timing constraints, which is an interesting and\
    \ big challenge.\n\nWe have proposed an accurate power model in data centers for\
    \ time constrained servers in Cloud computing, which does not only consider the\
    \ workload assigned to the processing element, but also incorporates the need\
    \ of considering the static power consumption and its dependency with temperature.\n\
    \nThe proposed model has been used in a multi-objective optimization environment\
    \ in which the DVFS and workload assignment have been efficiently optimized in\
    \ a realistic scenario composed of Fujitsu RX300 S6 servers based on an Intel\
    \ Xeon E5620 and SunFire V20z Dual Core AMD Opteron 270. Results show that the\
    \ proposed multi-objective optimization framework is able to find the best DVFS\
    \ configuration that can execute all the given demanded services without penalizations.\
    \ In addition, the set of non-dominated solutions found presents a high diversity\
    \ in terms of energy consumption.\n\nThe obtained results open a motivating research\
    \ line that could enable intensely sought Green computing paradigm with hard timing\
    \ constraints. Future work envisages to extend the scheduling model to integrate\
    \ the concept of criticality levels.\n\n#### ACKNOWLEDGMENT\n\nThis work is supported\
    \ by the Spanish Ministry of Economy and Competitivity under research grants TIN2013-40968-P\
    \ and TIN2014-54806-R.\n\n#### REFERENCES\n\n- <span id=\"page-7-0\"></span>[1]\
    \ E. A. Lee, \"CPS Foundations,\" in *Proceedings of the 47th Design Automation\
    \ Conference*, ser. DAC '10. New York, NY, USA: ACM, 2010, pp. 737\u2013742.\n\
    - <span id=\"page-7-1\"></span>[2] A. Banerjee, K. K. Venkatasubramanian, T. Mukherjee,\
    \ and S. K. S. Gupta, \"Ensuring safety, security, and sustainability of mission-critical\
    \ cyber-physical systems.\" *Proceedings of the IEEE*, vol. 100, no. 1, pp. 283\u2013\
    299, 2012. [Online]. Available: [http://dblp.uni-trier.de/db/journals/](http://dblp.uni-trier.de/db/journals/pieee/pieee100.html)\
    \ [pieee/pieee100.html](http://dblp.uni-trier.de/db/journals/pieee/pieee100.html)\n\
    - <span id=\"page-7-3\"></span>[3] A. Rowe, K. Lakshmanan, H. Zhu, and R. Rajkumar,\
    \ \"Rate-harmonized scheduling for saving energy,\" in *Proceedings of the 2008\
    \ Real-Time Systems Symposium*, ser. RTSS '08. Washington, DC, USA: IEEE Computer\
    \ Society, 2008, pp. 113\u2013122. [Online]. Available: <http://dx.doi.org/10.1109/RTSS.2008.50>\n\
    - <span id=\"page-7-4\"></span>[4] J. Chen and C. Kuo, \"Energy-efficient scheduling\
    \ for real-time systems on dynamic voltage scaling (DVS) platforms,\" in *13th\
    \ IEEE International Conference on Embedded and Real-Time Computing Systems and\
    \ Applications (RTCSA 2007), 21-24 August 2007, Daegu, Korea*, 2007, pp. 28\u2013\
    38.\n- <span id=\"page-7-5\"></span>[5] J. Kim, H. Kim, K. Lakshmanan, and R.\
    \ R. Rajkumar, \"Parallel scheduling for cyber-physical systems: Analysis and\
    \ case study on a self-driving car,\" in *Proceedings of the ACM/IEEE 4th International\
    \ Conference on Cyber-Physical Systems*, ser. ICCPS '13. New York, NY, USA: ACM,\
    \ 2013, pp. 31\u201340. [Online]. Available: <http://doi.acm.org/10.1145/2502524.2502530>\n\
    - <span id=\"page-7-6\"></span>[6] P. Huang, P. Kumar, G. Giannopoulou, and L.\
    \ Thiele, \"Energy efficient dvfs scheduling for mixed-criticality systems,\"\
    \ in *Proceedings of the 14th International Conference on Embedded Software*,\
    \ ser. EMSOFT '14. New York, NY, USA: ACM, 2014, pp. 11:1\u201311:10. [Online].\
    \ Available:<http://doi.acm.org/10.1145/2656045.2656057>\n- <span id=\"page-7-7\"\
    ></span>[7] S. K. Baruah, V. Bonifaci, G. D'Angelo, H. Li, A. Marchetti-Spaccamela,\
    \ S. van der Ster, and L. Stougie, \"The preemptive uniprocessor scheduling of\
    \ mixed-criticality implicit-deadline sporadic task systems.\" in *ECRTS*, R.\
    \ Davis, Ed. IEEE Computer Society, 2012, pp. 145\u2013154.\n- <span id=\"page-7-8\"\
    ></span>[8] L. Wang, G. von Laszewski, J. Dayal, and F. Wang, \"Towards energy\
    \ aware scheduling for precedence constrained parallel tasks in a cluster with\
    \ DVFS,\" in *Cluster, Cloud and Grid Computing (CCGrid), 2010 10th IEEE/ACM International\
    \ Conference on*, May 2010, pp. 368\u2013377.\n- <span id=\"page-7-9\"></span>[9]\
    \ A. Beloglazov, J. Abawajy, and R. Buyya, \"Energy-aware resource allocation\
    \ heuristics for efficient management of data centers for cloud computing,\" *Future\
    \ Gener. Comput. Syst.*, vol. 28, no. 5, pp. 755\u2013768, May 2012. [Online].\
    \ Available: [http://dx.doi.org/10.1016/j.future.2011.](http://dx.doi.org/10.1016/j.future.2011.04.017)\
    \ [04.017](http://dx.doi.org/10.1016/j.future.2011.04.017)\n- <span id=\"page-7-10\"\
    ></span>[10] C. Wu, R. Chang, and H. Chan, \"A green energy-efficient scheduling\
    \ algorithm using the DVFS technique for cloud datacenters,\" *Future Generation\
    \ Comp. Syst.*, vol. 37, pp. 141\u2013147, 2014.\n- <span id=\"page-7-11\"></span>[11]\
    \ S. Park, J. Kim, and G. Fox, \"Effective real-time scheduling algorithm for\
    \ cyber physical systems society,\" *Future Generation Comp. Syst.*, vol. 32,\
    \ pp. 253\u2013259, 2014.\n- <span id=\"page-7-12\"></span>[12] C. Mobius, W.\
    \ Dargie, and A. Schill, \"Power consumption estimation models for processors,\
    \ virtual machines, and servers,\" *Parallel and Distributed Systems, IEEE Transactions\
    \ on*, vol. 25, no. 6, pp. 1600\u2013 1614, June 2014.\n- <span id=\"page-7-13\"\
    ></span>[13] M. M. Rafique and et al., \"Power management for heterogeneous clusters:\
    \ An experimental study,\" in *IGCC*, Washington, DC, USA, 2011, pp. 1\u20138.\n\
    - <span id=\"page-7-14\"></span>[14] R. I. Davis and A. Burns, \"A survey of hard\
    \ real-time scheduling for multiprocessor systems,\" *ACM Comput. Surv.*, vol.\
    \ 43, no. 4, pp. 35:1\u201335:44, Oct. 2011. [Online]. Available: [http://doi.acm.org/10.1145/](http://doi.acm.org/10.1145/1978802.1978814)\
    \ [1978802.1978814](http://doi.acm.org/10.1145/1978802.1978814)\n- <span id=\"\
    page-7-15\"></span>[15] R. Schneider, D. Goswami, A. Masrur, M. Becker, and S.\
    \ Chakraborty, \"Multi-layered scheduling of mixed-criticality cyber-physical\
    \ systems,\" *Journal of Systems Architecture (JSA)*, vol. 59, no. 10-D, 2013.\n\
    - <span id=\"page-7-16\"></span>[16] L. Abeni, N. Manica, and L. Palopoli, \"\
    Efficient and robust probabilistic guarantees for real-time tasks,\" *J. Syst.\
    \ Softw.*, vol. 85, no. 5, pp. 1147\u20131156, May 2012. [Online]. Available:\
    \ [http:](http://dx.doi.org/10.1016/j.jss.2011.12.042) [//dx.doi.org/10.1016/j.jss.2011.12.042](http://dx.doi.org/10.1016/j.jss.2011.12.042)\n\
    - <span id=\"page-7-17\"></span>[17] T. Chantem, X. Hu, and M. Lemmon, \"Generalized\
    \ elastic scheduling for real-time tasks,\" *Computers, IEEE Transactions on*,\
    \ vol. 58, no. 4, pp. 480\u2013495, 2009.\n- <span id=\"page-7-18\"></span>[18]\
    \ S. Narendra and A. Chandrakasan, *Leakage in Nanometer CMOS Technologies*, ser.\
    \ Integrated Circuits and Systems. Springer, 2010.\n- <span id=\"page-7-19\"></span>[19]\
    \ J. Rabaey, *Low Power Design Essentials*, ser. Engineering (Springer-11647).\
    \ Springer, 2009.\n- <span id=\"page-7-20\"></span>[20] K. Deb, A. Pratap, S.\
    \ Agarwal, and T. Meyarivan, \"A Fast and Elitist Multiobjective Genetic Algorithm:\
    \ NSGA-II,\" *IEEE Transactions on Evolutionary Computation*, vol. 6, no. 2, pp.\
    \ 182\u2013197, 2002.\n- <span id=\"page-7-21\"></span>[21] A. Sayyad and H. Ammar,\
    \ \"Pareto-optimal search-based software engineering (posbse): A literature survey,\"\
    \ in *Realizing Artificial Intelligence Synergies in Software Engineering (RAISE),\
    \ 2013 2nd International Workshop on*, May 2013, pp. 21\u201327."
  paper_id: 14
  scores:
    originality:
      raw: 4.2
      reason: The proposed modular architecture for IMU-based data gloves presents
        a novel approach to standardizing and customizing data gloves, which is a
        significant contribution to the field.
    technical_soundness:
      raw: 4.0
      reason: The methodology appears robust and well-structured, although further
        details on experimental validation could enhance confidence in the findings.
    clarity:
      raw: 3.8
      reason: While the paper is generally well-written, some sections could benefit
        from clearer explanations and more concise language to improve overall readability.
    significance:
      raw: 4.1
      reason: The work addresses a critical gap in the standardization of data gloves,
        which has implications for reproducibility in research, thus holding substantial
        significance in the field.
    relevance:
      raw: 4.5
      reason: The topic is highly relevant to current trends in human-computer interaction
        and wearable technology, making it pertinent for both academic and practical
        applications.
  total_score: 4.07
  tier: C
- title: Efficient yet Accurate End-to-End SC Accelerator Design
  abstract: "The Versal Adaptive Compute Acceleration Platform (ACAP) is a new\narchitecture\
    \ that combines AI Engines (AIEs) with reconfigurable fabric. This\narchitecture\
    \ offers significant acceleration potential for uniform recurrences\nin various\
    \ domains, such as deep learning, high-performance computation, and\nsignal processing.\
    \ However, efficiently mapping these computations onto the\nVersal ACAP architecture\
    \ while achieving high utilization of AIEs poses a\nchallenge.\n  To address this\
    \ issue, we propose a mapping scheme called \\fname, which aims\nto accelerate\
    \ uniform recurrences on the Versal ACAP architecture by leveraging\nthe features\
    \ of both the hardware and the computations. Considering the array\narchitecture\
    \ of AIEs, our approach utilizes space-time transformations based on\nthe polyhedral\
    \ model to generate legally optimized systolic array mappings.\nConcurrently,\
    \ we have developed a routing-aware PLIO assignment algorithm\ntailored for communication\
    \ on the AIE array, and the algorithm aims at\nsuccessful compilation while maximizing\
    \ array utilization. Furthermore, we\nintroduce an automatic mapping framework.\
    \ This framework is designed to\ngenerate the corresponding executable code for\
    \ uniform recurrences, which\nencompasses the AIE kernel program, programmable\
    \ logic bitstreams, and the host\nprogram. The experimental results validate the\
    \ effectiveness of our mapping\nscheme. Specifically, when applying our scheme\
    \ to matrix multiplication\ncomputations on the VCK5000 board, we achieve a throughput\
    \ of 4.15TOPS on float\ndata type, which is 1.11$\\times$ higher compared to the\
    \ state-of-the-art\naccelerator on the Versal ACAP architecture."
  url: http://arxiv.org/abs/2401.16792v1
  keywords: Mapping, Re-configurable Array Architecture, Versal ACAP
  document: "#### I. INTRODUCTION\n\nModern heterogeneous FPGA architectures, like\
    \ AMD/Xilinx Versal Adaptive Compute Acceleration Platform (ACAP) [\\[1\\]](#page-5-0),\
    \ combine AI Engines (AIEs) with programmable logic (PL) to boost applications\
    \ in the AI and intelligent signal processing domains. In these domains, uniform\
    \ recurrences [\\[2\\]](#page-5-1), which comprise nested loops with uniform dependencies,\
    \ are prevalent types of computations. Regrettably, there is currently a lack\
    \ of established development methodologies for efficiently mapping large-scale\
    \ uniform recurrences onto the Versal ACAP architecture with high utilization\
    \ of AI Engines.\n\nThe ACAP architecture comprise an array of several hundred\
    \ AIE cores, such as 8\xD750 in the VC1902 architecture [\\[3\\]](#page-5-2),\
    \ interconnected through a mesh network-on-chip (NoC). Each AIE core consists\
    \ of vector processing and load/store units, functioning as a very-long-instruction-word\
    \ (VLIW) [\\[4\\]](#page-5-3) processor to deliver high-performance vectorized\
    \ computations. To facilitate communication among the AIE cores, the NoC is utilized\
    \ for inter-core communication, enabling efficient data transfers between cores.\
    \ Moreover, neighboring cores utilize shared buffers, providing higher bandwidth\
    \ for data exchange. When it comes to data transfer to and from the AIEs, there\
    \ are hundreds of I/O ports available, supporting terabytes of bandwidth.\n\n\
    As ACAP demonstrates a remarkable capacity for intense computation, developing\
    \ acceleration designs on the architecture has become an urgent trend in recent\
    \ times. However, current efforts have not succeeded in achieving high utilization\
    \ of the AIE array. For example, Vitis-AI [\\[5\\]](#page-5-4) introduces the\
    \ DPU [\\[6\\]](#page-5-5) for the VC1902 architecture, but only accomplishes\
    \ a 64% AIE utilization. There are several ongoing challenges associated with\
    \ developing designs with high array utilization on the Versal ACAP architecture:\n\
    \n- Increased programming complexity: Higher AIE utilization results in more cores\
    \ that need to be programmed with certain intrinsics. In some situations, different\
    \ cores execute different programs, necessitating significant human effort to\
    \ develop such accelerators.\n- Increased placement and routing difficulty: Mapping\
    \ computations onto the Versal ACAP architecture with high utilization of AIEs\
    \ often necessitates careful placement and routing of AIEs and data communications.\
    \ From the perspective of AIE compilation, attaining high AIE utilization typically\
    \ results in difficulties in placing cores and buffers, as well as routing streaming\
    \ communications on the NoC. For example, CHARM [\\[7\\]](#page-5-6) struggles\
    \ to compile large designs on Vitis 2022.1.\n- Extended compilation time: The\
    \ default compilation tools provided by AMD/Xilinx Vitis employ ILP algorithms\
    \ to find placement and routing solutions. Consequently, a larger number of cores\
    \ results in a longer time to find a legal solution.\n\nTo address these challenges,\
    \ we propose WideSA, a high array utilization mapping scheme for uniform recurrences\
    \ on the Versal ACAP architecture. By leveraging the AIE array architecture, we\
    \ apply space-time transformation and loop nest transformation using the polyhedral\
    \ model, generating systolic-like mappings on the AIE array. On one hand, systolic\
    \ designs assign similar workloads to different cores, enabling us to reuse a\
    \ single core program and thereby reduce human\n\n<span id=\"page-1-0\"></span>![](_page_1_Figure_0.jpeg)\n\
    \nFig. 1: Versal ACAP Architecture\n\neffort. On the other hand, systolic designs\
    \ regularize both the placement and communication of cores, simplifying the placement\
    \ and routing process. Additionally, we designed a routingaware PLIO assignment\
    \ algorithm to improve the success rate of compilation. We also developed an automatic\
    \ framework to generate the corresponding code for heterogeneous backends, including\
    \ AIEs, PL, and host. In the evaluation section, we demonstrate the effectiveness\
    \ of WideSA by successfully implementing executable acceleration systems for various\
    \ uniform recurrences, accommodating different data types. Our approach achieves\
    \ high throughput with high utilization of AIEs.\n\nWe summarize our contributions\
    \ as follows:\n\n- We propose a mapping scheme, based on the polyhedral model,\
    \ for uniform recurrences that generates a systolic design on ACAP with high AIE\
    \ utilization.\n- We design a routing-aware PLIO assignment algorithm that takes\
    \ into account the characteristics of systolic mappings and the AIE architecture,\
    \ thereby facilitating an efficient compilation process.\n- We develop an automatic\
    \ framework that generates corresponding code for heterogeneous backends based\
    \ on the mapping results.\n- We achieve high throughput across different computations\
    \ and data types, outperforming state-of-the-art methods.\n\n#### II. BACKGROUND\n\
    \n#### *A. Versal ACAP Architecture and Workflow*\n\n*1) Hardware Features:* AMD/Xilinx\
    \ has developed the Versal ACAP architecture to cater to the increasing demands\
    \ of next-generation wireless processing and machine learning applications. Figure\
    \ [1](#page-1-0) illustrates the detailed architecture of VCK5000, an evaluation\
    \ kit for Versal ACAP, comprising the CPU, PL, and AIE components. The AIE array\
    \ on VCK5000 consists of 8 \xD7 50 AIE cores, with each core capable of generating\
    \ 128 MACs of int8 data type every cycle at a frequency of 1 GHz or higher. Moreover,\
    \ the AIE cores operate in single-instruction-multiple-data (SIMD) mode using\n\
    \n<span id=\"page-1-1\"></span>TABLE I: Different Data Communication Bandwidth\
    \ on the Versal ACAP Architecture\n\n| Methods        | Frequency | Bitwidth |\
    \ Channels | Total      |\n|----------------|-----------|----------|----------|------------|\n\
    | AIE DMA        | 1.25 GHz  | 256 bits | 400      | 15.6 TB/s  |\n| AIE NoC Stream\
    \ | 1.25 GHz  | 32 bits  | 400      | 1.95 TB/s  |\n| PLIO-PL        | 1.25 GHz\
    \  | 128 bits | 78       | 1.52 TB/s  |\n| GMIO-DRAM      | 1.25 GHz  | 64 bits\
    \  | 16       | 0.125 TB/s |\n| PL-DRAM        | 0.50 GHz  | -        | 4    \
    \    | 0.100 TB/s |\n\na VLIW pattern, enabling acceleration of a large number\
    \ of vectorized computations.\n\nIn Figure [1,](#page-1-0) we identify five data\
    \ transfer methods, including those within the AIE array and among the AIE, PL,\
    \ and DRAM components. These methods are referred to as AIE DMA, AIE NoC stream,\
    \ PLIO-PL, PL-DRAM, and GMIO-DRAM interfaces. We profile these data transfer methods\
    \ on VCK5000 and present the results in Table [I.](#page-1-1) Within the AIE array,\
    \ each AIE core has direct memory access (DMA) ports connected to four neighboring\
    \ local buffers with a width of 256 bits. Using the AIE DMA method, a total data\
    \ transfer rate of up to 15.6 TB/s can be achieved. Furthermore, each AIE core\
    \ is linked to the NoC through a stream interface with a width of 32 bits. The\
    \ data transfer bandwidth through the AIE NoC stream method reaches a maximum\
    \ of 2 TB/s, which is lower compared to the DMA method. The PLIO ports, responsible\
    \ for data communication between the PL and AIE array, can provide a maximum bandwidth\
    \ of 1.52 TB/s. Based on the profiling results, utilizing the AIE DMA method for\
    \ data transfer proves beneficial in overcoming communication bottlenecks, aligning\
    \ with the dataflow in systolic array designs. In terms of data communication\
    \ with DRAM, the bandwidth is approximately 0.1 TB/s, significantly lower than\
    \ the on-chip data transfer methods. This observation inspires us to exploit data\
    \ locality within computations to enhance overall performance.\n\n*2) Software\
    \ Programming Model:* AMD/Xilinx offers a development tool for AIEs and Versal\
    \ ACAP integrated into Vitis. The programming model [\\[8\\]](#page-5-7) designed\
    \ for AIEs consists of two levels: a graph program across the AIE array with each\
    \ node representing an AIE kernel program. The graph program represents the dataflow\
    \ information among AIE kernels and between the AIE and I/O ports. The compiler\
    \ in Vitis transforms the dataflow graph into a subnetwork of physical AIE cores,\
    \ determines the placement of buffers, and configures NoC stream routing. Since\
    \ placement and routing are NP-hard problems, the compiler employs ILP solvers\
    \ to process these two phases. However, as the design scale increases and AIE\
    \ utilization becomes high, finding a legal solution efficiently becomes challenging\
    \ for the solvers [\\[9\\]](#page-5-8). To address this, incorporating constraints\
    \ for placement and routing helps alleviate the congestions and accelerates the\
    \ solvers in finding solutions. The systolic design scheme provides a regular\
    \ pattern for placement and routing, which is suitable for constructing these\
    \ constraints.\n\n## *B. Uniform Recurrences and Systolic Array Mapping*\n\nUniform\
    \ recurrences refer to computations that consist of nested loops, where all dependencies\
    \ are uniform. These types of computations are commonly found in AI and signal\
    \ processing applications, such as matrix multiplication, 2D convolution, FIR\
    \ filtering, and so on. Several prior works [\\[10\\]](#page-5-9)\u2013 [\\[12\\\
    ]](#page-5-10) have focused on generating systolic array designs for uniform recurrences\
    \ on FPGAs, employing the polyhedral model for loop transformations to explore\
    \ successful mappings. The polyhedral model [\\[13\\]](#page-5-11), [\\[14\\]](#page-5-12)\
    \ serves as a compilation framework for loop transformation, encompassing space-time\
    \ transformation, latency hiding, SIMD vectorization, fusion, and more. A legal\
    \ combination of these transformations represents a schedule within the polyhedral\
    \ model, and the goal of systolic design mapping is to find the optimal schedule.\n\
    \nAn AIE kernel handles more computations compared to a PE in typical systolic\
    \ arrays. Additionally, specific hardware features of the AIE array differ from\
    \ those of common systolic arrays. As a result, the mapping problem on the Versal\
    \ ACAP architecture is not a straightforward systolic array mapping. Consequently,\
    \ it is necessary to model corresponding transformations and constraints within\
    \ the polyhedral model, an area that has not yet been extensively researched.\n\
    \n## III. SYSTOLIC MAPPING SCHEME ON ACAP\n\n#### *A. Kernel Scope Demarcation*\n\
    \nAccording to the programming model of AIEs, it is necessary to demarcate the\
    \ scope of codes mapped to execute on a single AIE core and the outer loop nests\
    \ to be mapped to the AIE array. This demarcation allows us to decompose the mapping\
    \ problem into graph-level mapping and kernel-level mapping, which are independent\
    \ of each other after selecting tiling factors.\n\nPolygonal tiling [\\[15\\]](#page-5-13),\
    \ [\\[16\\]](#page-5-14), an effective solution for workload partitioning in uniform\
    \ polyhedral domains, plays a crucial role in determining the innermost and outer\
    \ loop nests for tiling. We illustrate the tiling process using the MM example\
    \ with (N0, M0, K0) as the tiling factors, involving loop re-indexing, tiling,\
    \ and rewriting, as depicted in Figure [2.](#page-2-0) Building on prior works,\
    \ we consider the specific features of the AIE array when performing the demarcation.\n\
    \n#### *B. Systolic Mapping Generation*\n\nTo generate systolic array designs\
    \ on the AIE array following kernel scope demarcation, we utilize the polyhedral\n\
    \n<span id=\"page-2-0\"></span>![](_page_2_Figure_9.jpeg)\n\nFig. 2: Kernel Scope\
    \ Demarcation\n\n<span id=\"page-2-1\"></span>![](_page_2_Figure_11.jpeg)\n\n\
    Fig. 3: Polyhedral Model-Based Systolic Mapping\n\nmodel, drawing inspiration\
    \ from AutoSA [\\[11\\]](#page-5-15), to facilitate loop transformations. To be\
    \ specific, we employ four types of transformation techniques, as depicted in\
    \ Figure [3.](#page-2-1)\n\n*1) Space-time Transformation:* The first step involves\
    \ performing space-time transformation to map the graph-level loop nests to a\
    \ systolic array design. We identify loops in the outermost loop band with dependence\
    \ distances no greater than one and consider them as candidate space loops. Subsequently,\
    \ we enumerate all possible combinations of space loops from the candidate pool.\
    \ The selected space loops are then permuted in the outermost position, while\
    \ the loops below them are designated as time loops. Due to the constraints imposed\
    \ by the hardware shape of the AIE array, the mapper generates only 1D and 2D\
    \ systolic arrays. This step results in the generation of multiple systolic arrays,\
    \ each with a unique schedule. As shown in Figure [3,](#page-2-1) we choose loops\
    \ i and j as the space loops (on dark gray background) and loop k as the time\
    \ loop (on light gray background) in the MM example.\n\n*2) Array Partition:*\
    \ To accommodate the limited number of AIEs in the horizontal and vertical directions\
    \ of the AIE array, array partitioning becomes necessary when mapping a large\
    \ array. In order to achieve this, we apply tiling to the outermost permutable\
    \ loop that contains the space loops. In Figure [3,](#page-2-1) we illustrate\
    \ an example where we tile the outermost loop band in the MM example using the\
    \ tiling factors (N1, M1, K1). The point loops originating from the original loops\
    \ are retained as the space loops. This results in a 2D systolic array with dimensions\
    \ of N<sup>1</sup> \xD7 M<sup>1</sup> (on dark gray background).\n\n*3) Latency\
    \ Hiding:* Latency hiding plays a crucial role in mitigating the pipeline stalls\
    \ caused by loop-carried dependencies in computational statements. In the case\
    \ of the MM example, the accumulate operations in the statement introduce loop-carried\
    \ dependence within the loop, resulting in long latency in the systolic chain.\
    \ To address this issue, we identify parallel loops in the polyhedral model schedules,\
    \ applies tiling to these loops, and permutes the point loops to the innermost\
    \ position. As an illustration, loops i and j are identified as parallel loops\
    \ in the MM example. We extract them using the tiling factors (N2, M2) and permute\
    \ the point loops to the innermost position. Since there are no loop-carried dependencies\
    \ on the innermost loop, the latency of design reduce as the chain length shortened.\n\
    \n<span id=\"page-3-0\"></span>![](_page_3_Figure_0.jpeg)\n\nFig. 4: Communication\
    \ Methods for PLIO Ports Utilization Reduction\n\n*4) Multiple Threading:* As\
    \ AIE cores execute concurrently, the AIE array inherently supports multiple threading.\
    \ Leveraging this characteristic, utilizing multiple AIEs to execute the same\
    \ instructions but different indexing can significantly enhance overall performance.\
    \ We identify parallelizable loops in the time loops that do not have data dependence.\
    \ In the MM example, the loop k is identified as a parallelizable loop. We can\
    \ apply tiling to this loop using the factors K2. The point loop is permuted to\
    \ the innermost position and completely unrolled to generate multiple threads\
    \ of AIEs.\n\n#### *C. Placement and Routing Constraints Construction*\n\nThe\
    \ systolic design generated in the previous section represents an abstract mapping\
    \ scheme. Consequently, it is essential to use the space loops as input and generate\
    \ an actual mapped graph for AIE array that considers placement and routing constraints.\
    \ The mapped graph consists of nodes, representing AIE cores and input/output\
    \ ports, and edges, which connect the ports of the nodes. The placement and routing\
    \ constraints involve assigning coordinates to the AIE cores, buffers, and input/output\
    \ ports, as well as determining the routing paths for the edges. In the subsequent\
    \ subsections, we introduce the graph builder and routing-aware PLIO assignment,\
    \ which are responsible for constructing the mapped graph and generating the associated\
    \ placement and routing constraints, respectively.\n\n*1) Graph Builder:* To construct\
    \ the mapped graph, we iterate through all coordinates in the space loops and\
    \ create a node for each pair of coordinates in the 2D systolic array, representing\
    \ an AIE core. Next, we identify the data communications between AIE cores based\
    \ on the dependencies within the space loops. Following the definitions in AutoSA\
    \ [\\[11\\]](#page-5-15), there are three types of data dependences:\n\n- Read\
    \ dependence: Transfer the read-only data.\n- Flow dependence: Transfer the intermediate\
    \ data.\n- Output dependence: Transfer the output-only data.\n\nBased on these\
    \ data dependences and the space loops, we define the I/O ports and edge directions.\
    \ Since AIEs do not support intermediate results between different iterations,\
    \ we treat flow dependences as input dependencies when constructing I/O ports.\
    \ The polyhedral model for the array access to matrix A in the MM recurrences\
    \ is {i, j, k} \u2192 {i, j+1, k}, and when loops j, k are the space loops, the\
    \ direction is (1, 0). We connect the input ports from the corresponding nodes\
    \ with a constant and non-zero distant direction.\n\nAs for the output ports,\
    \ the boundary input ports, and the zero distant direction ports, we create PLIO\
    \ ports as the other end of the connection edge. To adhere to the limitation on\
    \ the number of PLIO ports, we utilize packet-switch communications and broadcast\
    \ communications to reduce the number of used ports, as depicted in Figure [4.](#page-3-0)\n\
    \n*2) Routing-Aware PLIO Assignment:* Once we have the mapped graph, we search\
    \ for available cores on the AIE array to place the AIE kernels. To facilitate\
    \ efficient communication between neighboring cores, we assign the buffers of\
    \ ports connecting these cores to the shared buffer of the cores, forming part\
    \ of the placement constraints. These constraints enable the transformation of\
    \ the kernels' placement into a regular duplicate pattern of a single kernel.\n\
    \nAside from facilitating neighboring communication, it is necessary to construct\
    \ paths between PLIO ports and AIE cores for data input and output. Considering\
    \ the mesh structure of the NoC on the AIE array, and given that PLIOs are always\
    \ located in Row 0, we can compute the routing congestion by counting the horizontal\
    \ data transfer numbers. For instance, we compute the congestion for the *west*\
    \ direction as follows:\n\n$$\\operatorname{Cong}\\_{i}^{\\text{west}} = \\sum\\\
    _{p \\in \\text{PL.IOs}, x \\in \\text{AIEs}} W\\_{i}[p][x],$$\n\n$$W\\_i[p][x]\
    \ = \\begin{cases} 1 & (p\\_{\\text{col}} < i \\text{ and } x\\_{\\text{col}}\
    \ > i \\text{ and } (x, p) \\in \\text{Edges}) \\text{ or } \\\\ & (p\\_{\\text{col}}\
    \ > i \\text{ and } x\\_{\\text{col}} < i \\text{ and } (p, x) \\in \\text{Edges})\
    \ \\\\\\ 0 & \\text{Otherwise} \\end{cases}$$\n\nwhere pcol and xcol represent\
    \ the column coordinates of PLIO p and AIE x, respectively.\n\nThe computation\
    \ of the congestion for the *east* direction is symmetrical.\n\nConsequently,\
    \ the routing challenges essentially transform into issues of PLIO assignment.\
    \ We formulate the assignment of PLIO ports as a satisfiability problem subject\
    \ to routing resource constraints. We check if there exists a set of values for\
    \ PLIOs that satisfies the following constraints:\n\n$$\\forall i \\in \\text{Columns},\
    \ \\mathsf{Cong}\\_i^{\\text{west}} \\le \\mathsf{RC}\\_{\\text{west}}, \\quad\
    \ \\mathsf{Cong}\\_i^{\\text{east}} \\le \\mathsf{RC}\\_{\\text{east}}$$\n\nwhere\
    \ RC*west* and RC*east* denote the available routing resources in the AIE array.\n\
    \nTo seek the feasible assignment of PLIO ports, we employ a heuristic greedy\
    \ algorithm outlined in Algorithm [1.](#page-4-0) In this algorithm, we initialize\
    \ the placement of the PLIO ports by calculating the median value of the row numbers\
    \ of the connected AIE cores. If the initially computed placement coordinate is\
    \ not available, we search for the nearest available coordinate instead. This\
    \ heuristic greedy algorithm balances the routing congestion among the PLIO ports.\
    \ By considering the connectivity with the AIE cores, it generates an optimal\
    \ placement for the PLIO ports, ensuring successful routing on the NoC. The algorithm\
    \ takes into account the availability of coordinates and selects the most suitable\
    \ placement to minimize congestion.\n\nBy generating these constraints for the\
    \ placement and routing of AIE kernels, buffers, and PLIO ports, we can\n\n<span\
    \ id=\"page-4-0\"></span>Algorithm 1 Routing-Aware PLIO Assignment Algortihm\n\
    \n| Algorithm 1 Routing-Aware PLIO Assignment Algortihm         |\n|-------------------------------------------------------------|\n\
    | Require: Numbers of PLIO ports N, AIE cores X               |\n| Ensure: Initialized\
    \ PLIO assignment set P                   |\n| 1: Initialization available placement\
    \ sets A as all columns |\n| that have PLIO ports.                           \
    \            |\n| 2: for i \u2190 1 to N do                                  \
    \      |\n| S = [], num = 0<br>3:                                       |\n| for\
    \ x \u2208 X do<br>4:                                          |\n| if (p, x)\
    \ \u2208 Edges then<br>5:                                |\n| S.append(xcol)<br>6:\
    \                                        |\n| num+ = 1<br>7:                 \
    \                             |\n| end if<br>8:                              \
    \                  |\n| end for<br>9:                                        \
    \       |\n| Sort S to find the median: sort(S, S + num)<br>10:          |\n|\
    \ P[i] = find nearest(A, S[num/2])<br>11:                     |\n| remove(A, P[i])<br>12:\
    \                                      |\n| 13: end for                      \
    \                           |\n| 14: return<br>P                             \
    \                |\n\nsignificantly simplify the task for the AIE compiler. These\
    \ constraints provide valuable information and guidelines for the compilers to\
    \ optimize the placement and routing process, ultimately leading to a high utilization\
    \ of the AIE array.\n\n#### IV. AUTOMATIC MAPPING FRAMEWORK\n\nTo facilitate the\
    \ computation of uniform recurrence, we have developed an automatic mapping framework\
    \ that implements the full functional modules on the Versal ACAP architecture,\
    \ as shown in Figure [5.](#page-4-1)\n\n<span id=\"page-4-1\"></span>![](_page_4_Figure_5.jpeg)\n\
    \nFig. 5: Overview of WideSA Automatic Framework\n\nSpecifically, we introduce\
    \ a kernel-level mapper, a DMA module constructor, and a host program generator,\
    \ which work in conjunction with the kernel scope and graph mapper described in\
    \ the previous section.\n\nThe kernel-level mapper and optimizer transform the\
    \ C++ program into a program with AIE intrinsics, leveraging the capabilities\
    \ of the AIE vector processor to exploit parallelism and optimize performance.\
    \ Moreover, we design the architecture of efficient DMA modules, which serve as\
    \ the buffers of AIEs on the PL, in the DMA module constructor. This architecture\
    \ is tailored to the characteristics of both the hardware and the computations\
    \ involved. In addition, we engineer a host program generator to generate a controller\
    \ program that oversees global scheduling.\n\nTABLE II: Evaluation Benchmarks\n\
    \n<span id=\"page-4-2\"></span>\n\n| Benchmarks | Dimension    | Problem Size\
    \          | Data Types |\n|------------|--------------|-----------------------|------------|\n\
    |            |              | [8192, 8192, 8192]    | Float      |\n|        \
    \    |              | [10240, 10240, 10240] | Int8       |\n| MM         | [i,\
    \ j, k]    | [9600, 9600, 9600]    | Int16      |\n|            |            \
    \  | [8192, 8192, 8192]    | Int32      |\n| 2D-Conv    |              | [10240,\
    \ 10240, 4, 4]  | Float      |\n|            | [h, w, p, q] | [10240, 10240, 8,\
    \ 8]  | Int8       |\n|            |              | [10240, 10240, 4, 4]  | Int16\
    \      |\n|            |              | [10240, 10240, 4, 4]  | Int32      |\n\
    |            |              | [8192, 8192]          | Cfloat     |\n| 2D-FFT \
    \    | [row, col]   | [8192, 8192]          | Cint16     |\n|            |   \
    \           | [1048576, 15]         | Float      |\n| FIR Filter |           \
    \   | [1048576, 15]         | Int8       |\n|            | [n, taps]    | [1048576,\
    \ 15]         | Int16      |\n|            |              | [1048576, 15]    \
    \     | Cfloat     |\n\n#### V. EVALUATION\n\n#### *A. Benchmark and Experimental\
    \ Setup*\n\nIn this section, we select four representative uniform recurrences\
    \ with various data types as benchmarks to evaluate the performance of WideSA.\
    \ The selected benchmarks include matrix multiplication (MM), 2D convolution (2D-Conv),\
    \ 2D Fast Fourier Transformation (2D-FFT), and FIR filter [\\[17\\]](#page-5-16).\
    \ The problem sizes and corresponding data types are provided in Table [II.](#page-4-2)\
    \ Here, Cfloat refers to the complex float data type and Cint16 refers to the\
    \ complex 16-bit integer data type. All the experiments are conducted on VCK5000\
    \ with 250 MHz on PL and 1.25 GHz on AIE. AMD/Xilinx Vitis 2022.1 is used as the\
    \ compilation backend tool.\n\n### *B. Full System Performance*\n\nWe conducted\
    \ a comparison of the throughput between WideSA and other state-of-the-art AIE\
    \ designs for the same problem size. For the MM benchmark, we successfully compiled\
    \ the CHARM code [\\[7\\]](#page-5-6) for the target VCK5000 with AMD/Xilinx Vitis\
    \ 2022.1, incorporating placement and routing constraints, as the baseline. As\
    \ for the 2D-Conv benchmark, we selected the released 8-PEs version of Vitis-AI\
    \ DPU [\\[5\\]](#page-5-4) which only supports Int8 data type, utilizing 256 AIEs\
    \ running at 1.33 GHz and the PL at 350 MHz, as the baseline. Furthermore, we\
    \ used the open-source designs from the Vitis DSP Library [\\[18\\]](#page-5-17)\
    \ as the baselines for the 2D-FFT and FIR filter benchmarks.\n\nThe results presented\
    \ in Table [III](#page-5-18) demonstrate that WideSA achieves significantly higher\
    \ throughput with high utilization of AIEs. Additionally, we computed the AIE\
    \ efficiency by considering the throughput and the number of used AIEs. The results\
    \ indicate that WideSA maintains similar efficiency to [\\[7\\]](#page-5-6) for\
    \ MM, as both approaches exhibit AIE utilization over 95%. When compared to the\
    \ baselines with lower AIE utilizations, WideSA trades AIE efficiency (TOPS/#AIEs)\
    \ for a high overall performance (TOPS) and is bounded by memory bandwidth.\n\n\
    Moreover, we conducted a comparison of the performance and energy efficiency of\
    \ MM using WideSA and PL-only designs on the VCK5000 target, which has 1968 DSP58\
    \ IPs at total. For the PL-only designs, we utilize AutoSA [\\[11\\]](#page-5-15)\
    \ as the systolic array generator. The results presented in Table [IV](#page-5-19)\n\
    \nTABLE III: Comparison of Throughput and AIE Efficiency on Benchmarks\n\n<span\
    \ id=\"page-5-18\"></span>\n\n| Method   | Metric     |       |       | MM   \
    \ |       | 2D-Conv |       |       | 2D-FFT |        | FIR Filter |       | \
    \      |       |        |\n|----------|------------|-------|-------|-------|-------|---------|-------|-------|--------|--------|------------|-------|-------|-------|--------|\n\
    |          | Data type  | Float | Int8  | Int16 | Int32 | Float   | Int8  | Int16\
    \ | Int32  | Cfloat | Cint16     | Float | Int8  | Int16 | Cfloat |\n|       \
    \   | #AIEs      | 384   | 384   | 384   | 384   | -       | 256   | -     | -\
    \      | 10     | 10         | 10    | 10    | 10    | 10     |\n| Baseline |\
    \ TOPS       | 3.73  | 29.78 | 7.82  | 3.72  | -       | 31.40 | -     | -   \
    \   | 0.04   | 0.13       | 0.15  | 2.56  | 0.62  | 0.15   |\n|          | TOPS/#AIEs\
    \ | 0.010 | 0.077 | 0.020 | 0.010 | -       | 0.123 | -     | -      | 0.004 \
    \ | 0.013      | 0.015 | 0.256 | 0.062 | 0.015  |\n|          | #AIES      | 400\
    \   | 400   | 400   | 400   | 400     | 400   | 400   | 400    | 320    | 320\
    \        | 256   | 256   | 256   | 256    |\n| WideSA   | TOPS       | 4.15  |\
    \ 32.49 | 8.10  | 3.92  | 4.50    | 36.02 | 10.35 | 4.48   | 1.10   | 3.83   \
    \    | 2.92  | 39.3  | 9.47  | 2.89   |\n|          | TOPS/#AIEs | 0.010 | 0.081\
    \ | 0.020 | 0.010 | 0.011   | 0.090 | 0.025 | 0.011  | 0.003  | 0.012      | 0.012\
    \ | 0.100 | 0.037 | 0.011  |\n\n<span id=\"page-5-19\"></span>TABLE IV: MM Performance\
    \ Comparison between PL-only and WideSA Design\n\n|              |       |   \
    \    | PL-only |       | WideSA |       |       |       |  |\n|--------------|-------|-------|---------|-------|--------|-------|-------|-------|--|\n\
    | Data Type    | Float | Int8  | Int16   | Int32 | Float  | Int8  | Int16 | Int32\
    \ |  |\n| DSPs         | 1536  | 1528  | 1516    | 1536  | 152    | 60    | 67\
    \    | 65    |  |\n| #AIEs        | 0     | 0     | 0       | 0     | 400    |\
    \ 400   | 400   | 400   |  |\n| TOPS         | 0.59  | 5.77  | 2.16    | 0.60\
    \  | 4.15   | 32.49 | 8.10  | 3.92  |  |\n| Power (W)    | 19.5  | 18.8  | 18.6\
    \    | 19.5  | 55.8   | 54.4  | 54.9  | 55.6  |  |\n| TOPS/W       | 0.03  | 0.31\
    \  | 0.12    | 0.03  | 0.07   | 0.60  | 0.15  | 0.07  |  |\n| Norm. TOPS/W | 1.00x\
    \ | 1.00x | 1.00x   | 1.00x | 2.25x  | 1.94x | 1.29x | 2.25x |  |\n\ndemonstrate\
    \ that our approach achieves up to 2.25\xD7 higher energy efficiency compared\
    \ to the PL-only designs.\n\n#### *C. Scalability of WideSA on MM examples*\n\n\
    We evaluate the scalability of WideSA while increasing AIE utilization and analyze\
    \ how various factors influence performance. The results, presented in Figure\
    \ [6,](#page-5-20) show a significant increase in throughput as the number of\
    \ AIEs increases. In addition, the AIE efficiency results demonstrate that our\
    \ approach scales effectively from small-scale to largescale designs. However,\
    \ when the number exceeds 200, the efficiency of a single AIE core decreases due\
    \ to the memorybound condition caused by the number of PLIOs and the size of the\
    \ PL buffer. The increase in PLIO numbers and buffer sizes leads to increased\
    \ throughput, suggesting that enhancing the bandwidth between different fabrics\
    \ of ACAP can improve performance. This indicates that managing the resources\
    \ and data flow between different components of the ACAP is crucial for achieving\
    \ better performance.\n\n#### VI. CONCLUSION\n\nIn this paper, we present a high\
    \ array utilization mapping scheme for uniform recurrences on the Versal ACAP\
    \ architecture. Additionally, we propose several optimizations aimed at enhancing\
    \ overall performance within an automatic mapping framework. Through extensive\
    \ evaluations using typical benchmarks and diverse data types, we assess the efficiency\
    \ of the WideSA framework. In the future work, we aim to integrate WideSA into\
    \ the MLIR-AIE workflow and develop an end-toend compilation tool that incorporates\
    \ automatic design space exploration.\n\n#### ACKNOWLEDGEMENT\n\nThis work was\
    \ partly supported by the National Natural Science Foundation of China (Grant\
    \ No. 62090021) and the National Key R&D Program of China (Grant No. 2022YFB4500500).\n\
    \n<span id=\"page-5-20\"></span>![](_page_5_Figure_11.jpeg)\n\nFig. 6: Throughput\
    \ Evaluation of Different AIE Numbers, PLIO Numbers, and PL Buffer Sizes\n\n####\
    \ REFERENCES\n\n- <span id=\"page-5-0\"></span>[1] AMD/Xilinx. Versal Adaptive\
    \ Compute Acceleration Platform.\n- <span id=\"page-5-1\"></span>[2] R. M. Karp\
    \ *et al.*, \"The organization of computations for uniform recurrence equations,\"\
    \ *J. ACM*, 1967.\n- <span id=\"page-5-2\"></span>[3] S. Ahmad *et al.*, \"Xilinx\
    \ first 7nm device: Versal AI Core (VC1902),\" in *HCS*, 2019.\n- <span id=\"\
    page-5-3\"></span>[4] J. A. Fisher, \"Very long instruction word architectures\
    \ and the ELI-512,\" in *ISCA*, 1983.\n- <span id=\"page-5-4\"></span>[5] Vitis\
    \ AI Library User Guide. [Online]. Available: [https://xilinx.github.](https://xilinx.github.io/Vitis-AI/)\
    \ [io/Vitis-AI/](https://xilinx.github.io/Vitis-AI/)\n- <span id=\"page-5-5\"\
    ></span>[6] X. Jia *et al.*, \"XVDPU: A high performance CNN accelerator on the\
    \ Versal platform powered by the AI Engine,\" in *FPL*, 2022.\n- <span id=\"page-5-6\"\
    ></span>[7] J. Zhuang *et al.*, \"CHARM: Composing heterogeneous accelerators\
    \ for matrix multiply on Versal ACAP architecture,\" in *FPGA*, 2023.\n- <span\
    \ id=\"page-5-7\"></span>[8] AMD/Xilinx. AI Engine Kernel and Graph Programming\
    \ Guide.\n- <span id=\"page-5-8\"></span>[9] W. Cook *et al.*, \"An exact rational\
    \ mixed-integer programming solver,\" in *IPCO*, 2011.\n- <span id=\"page-5-9\"\
    ></span>[10] J. Cong *et al.*, \"PolySA: Polyhedral-based systolic array autocompilation,\"\
    \ in *ICCAD*, 2018.\n- <span id=\"page-5-15\"></span>[11] J. Wang *et al.*, \"\
    AutoSA: A polyhedral compiler for high-performance systolic arrays on FPGA,\"\
    \ in *FPGA*, 2021.\n- <span id=\"page-5-10\"></span>[12] Y.-H. Lai *et al.*, \"\
    SuSy: A programming model for productive construction of high-performance systolic\
    \ arrays on FPGAs,\" in *ICCAD*, 2020.\n- <span id=\"page-5-11\"></span>[13] M.-W.\
    \ Benabderrahmane *et al.*, \"The polyhedral model is more widely applicable than\
    \ you think,\" in *CC*, 2010.\n- <span id=\"page-5-12\"></span>[14] U. Bondhugula,\
    \ \"Compiling affine loop nests for distributed-memory parallel architectures,\"\
    \ in *SC*, 2013.\n- <span id=\"page-5-13\"></span>[15] R. Andonov *et al.*, \"\
    Optimal semi-oblique tiling,\" *IEEE TPDS*, 2003.\n- <span id=\"page-5-14\"></span>[16]\
    \ C. Rossetti *et al.*, \"Algebraic tiling,\" in *IMPACT*, 2023.\n- <span id=\"\
    page-5-16\"></span>[17] K. K. Parhi, \"VLSI digital signal processing systems:\
    \ Design and implementation,\" 2007.\n- <span id=\"page-5-17\"></span>[18] AMD/Xilinx.\
    \ Vitis DSP Library for digital signal processing."
  paper_id: 19
  scores:
    originality:
      raw: 4.2
      reason: The paper presents a novel approach to end-to-end SC accelerators, introducing
        deterministic thermometer coding, which adds a fresh perspective to the existing
        body of work.
    technical_soundness:
      raw: 4.0
      reason: The proposed methods appear well-founded and technically sound, though
        some details on implementation could be elaborated for clarity.
    clarity:
      raw: 3.8
      reason: While the introduction sets the stage effectively, certain sections
        could benefit from clearer explanations and more intuitive figures to enhance
        understanding.
    significance:
      raw: 4.1
      reason: The research addresses significant gaps in SC-based accelerator design,
        particularly in improving accuracy and efficiency, making it a valuable contribution
        to the field.
    relevance:
      raw: 4.5
      reason: The topic is highly relevant to current trends in computing, particularly
        in the context of neural networks and efficient computing architectures.
  total_score: 4.07
  tier: C
- title: 'Reuse Detector: Improving the Management of STT-RAM SLLCs'
  abstract: 'RISC-V processors encounter substantial challenges in deploying

    multi-precision deep neural networks (DNNs) due to their restricted precision

    support, constrained throughput, and suboptimal dataflow design. To tackle

    these challenges, a scalable RISC-V vector (RVV) processor, namely SPEED, is

    proposed to enable efficient multi-precision DNN inference by innovations from

    customized instructions, hardware architecture, and dataflow mapping. Firstly,

    dedicated customized RISC-V instructions are proposed based on RVV extensions,

    providing SPEED with fine-grained control over processing precision ranging

    from 4 to 16 bits. Secondly, a parameterized multi-precision systolic array

    unit is incorporated within the scalable module to enhance parallel processing

    capability and data reuse opportunities. Finally, a mixed multi-precision

    dataflow strategy, compatible with different convolution kernels and data

    precision, is proposed to effectively improve data utilization and

    computational efficiency. We perform synthesis of SPEED in TSMC 28nm

    technology. The experimental results demonstrate that SPEED achieves a peak

    throughput of 287.41 GOPS and an energy efficiency of 1335.79 GOPS/W at 4-bit

    precision condition, respectively. Moreover, when compared to the pioneer

    open-source vector processor Ara, SPEED provides an area efficiency improvement

    of 2.04$\times$ and 1.63$\times$ under 16-bit and 8-bit precision conditions,

    respectively, which shows SPEED''s significant potential for efficient

    multi-precision DNN inference.'
  url: http://arxiv.org/abs/2401.16872v2
  keywords: ''
  document: "## I. INTRODUCTION\n\nRISC-V processors [\\[1\\]](#page-4-0)\u2013[\\\
    [11\\]](#page-4-1) are distinguished by the concise open-source RISC-V instruction\
    \ set architecture (ISA) [\\[12\\]](#page-4-2), which defines a fundamental set\
    \ of instructions while offering opportunities to incorporate application-specific\
    \ custom instructions. This unique feature enables RISC-V processors as a promising\
    \ solution ranging from energy-efficient embedded systems [\\[10\\]](#page-4-3)\
    \ to high-throughput servers [\\[1\\]](#page-4-0). On these RISC-V hardware platforms,\
    \ deep neural networks (DNNs) exhibit substantial deployment demands [\\[13\\\
    ]](#page-4-4)\u2013[\\[15\\]](#page-4-5) but face the extensive storage and computational\
    \ costs. Aiming to reduce resource overhead and improve inference speed, multiprecision\
    \ quantized DNNs [\\[16\\]](#page-4-6)\u2013[\\[18\\]](#page-4-7) have emerged\
    \ as an efficient choice for deployment. However, deploying multiprecision quantized\
    \ DNNs also incurs a series of significant challenges on prior RISC-V processors.\n\
    \nThe emergence of the RISC-V Vector (RVV) extension instruction set has made\
    \ RVV processors a promising choice for deploying multi-precision DNNs. These\
    \ RVV processors [\\[5\\]](#page-4-8)\u2013[\\[9\\]](#page-4-9) excel in throughput\
    \ enhancement through their parallel processing capabilities and minimize instruction\
    \ overhead by employing a minimal number of configuration-setting instructions\
    \ to define data precision. Notably, Ara [\\[6\\]](#page-4-10) stands out as the\
    \ pioneer open-source vector processor compatible with the standard RVV 1.0 ISA.\
    \ It achieves an exceptional throughput improvement of up to 380\xD7 in comparison\
    \ to a scalar core [\\[19\\]](#page-4-11), rendering it highly efficient for tasks\
    \ such as DNN inference. However, the potential performance improvement of the\
    \ Ara is constrained due to limited support for low-bitwidth (e.g., 4-bit) operations,\
    \ which are widely used in quantized DNNs to reduce computational complexity and\
    \ accelerate inference processing with little loss in accuracy [\\[20\\]](#page-4-12).\
    \ Furthermore, Ara's parallelism is constrained by the number of scalable modules.\
    \ When handling intensive computational tasks, increasing the number of scalable\
    \ modules can cause excessive hardware consumption. Meanwhile, inefficient dataflow\
    \ of Ara can lead to increased off-chip data movement and underutilization of\
    \ on-chip memory, thereby lowering computational efficiency. In summary, deploying\
    \ multi-precision quantized DNNs on prior RISC-V processors still struggles with\
    \ (1) limited precision support, (2) constrained throughput, and (3) inefficient\
    \ dataflow mapping.\n\nTo address the issues above, we propose SPEED, a scalable\
    \ RVV processor enabling efficient DNN inference across 4, 8, and 16 bits through\
    \ customized instructions based on the standard RVV ISA. Moreover, SPEED significantly\
    \ improves data throughput and computational efficiency by increasing the data-level\
    \ parallelism of scalable modules and utilizing a mixed multi-precision dataflow\
    \ strategy tailored for various convolution kernels. Specifically, the contributions\
    \ of this work can be summarized in the three following aspects:\n\n- 1) Customized\
    \ instructions: Dedicated customized RISC-V instructions are proposed based on\
    \ RVV extensions, providing SPEED with fine-grained control over processing precision\
    \ and dataflow strategy, while supporting multi-precision computations ranging\
    \ from 4 to 16 bits.\n- 2) Hardware architecture: A scalable RVV processor is\
    \ developed, namely SPEED, for efficient multi-precision DNN inference by enhancing\
    \ parallel processing capability of scalable modules. Compared to Ara [\\[6\\\
    ]](#page-4-10), SPEED improves an area efficiency by 2.04\xD7 and 1.63\xD7 under\
    \ 16-bit and 8-bit precision conditions, respectively.\n- 3) Dataflow mapping:\
    \ A mixed dataflow strategy embracing feature map-first (FF) and channel-first\
    \ (CF) strategies is proposed, aiming to achieve high computational\n\nefficiency\
    \ of convolution kernels across various kernel sizes and data precisions. The\
    \ mixed dataflow strategy improves area efficiency of SPEED by 1.87\u223C3.53\xD7\
    \ over Ara [\\[6\\]](#page-4-10) under various convolution kernels.\n\n![](_page_1_Figure_2.jpeg)\n\
    \nFig. 1. Customized instructions and overall architecture of SPEED.\n\n## <span\
    \ id=\"page-1-0\"></span>II. THE PROPOSED SPEED ARCHITECTURE\n\nSPEED is a scalable\
    \ RVV processor augmented with customized instructions enabling high-throughput\
    \ multi-precision DNN inference, which is built on the RVV v1.0 ISA. Additionally,\
    \ the RVV processor is tightly coupled to a RISC-V scalar core for programmable\
    \ instructions and an external memory for fetching necessary data. In this section,\
    \ SPEED will be described from three perspectives: customized instructions, hardware\
    \ architecture, and dataflow mapping.\n\n## *A. Customized Instructions*\n\nTo\
    \ facilitate efficient inference of multi-precision DNNs, several customized instructions\
    \ are proposed to enhance the computational efficiency of DNN operations compared\
    \ to the standard RVV extensions. Specifically, the customized instructions mainly\
    \ contain the configuration-setting (VSACFG), load (VSALD), and arithmetic (VSAM),\
    \ as depicted in Fig. [1.](#page-1-0)\n\nVSACFG serves as a vector configuration-setting\
    \ instruction that effectively provides the necessary information for subsequent\
    \ instructions, such as data precision (4-\u223C16-bit) and dataflow strategy\
    \ (FF/CF strategy). This information is encoded within the *zimm9* and *uimm5*\
    \ encoding spaces, as shown in Fig. [1,](#page-1-0) to prepare for processing\
    \ of subsequent RVV and customized instructions.\n\nVSALD is a customized load\
    \ instruction responsible for loading data from the base address of external memory\
    \ and storing it into vector register files (VRFs) at the specified destination\
    \ address, aiming to maximize data utilization. In contrast to the ordered allocation\
    \ operation of the standard RVV load instruction VLE, the loaded data from the\
    \ external memory are broadcast to each lane for improving data reuse.\n\nVSAM\
    \ is a customized arithmetic instruction that exploits data-level parallelism\
    \ in efficiency, thereby enhancing computational efficiency. As shown in Fig.\
    \ [1,](#page-1-0) VSAM requests data from the base addresses *vs1* and *vs2* where\
    \ they locate in the VRFs, respectively. The performed results of above data are\
    \ stored at the accumulation address *Acc Addr* within the VRFs.\n\n## <span id=\"\
    page-1-1\"></span>*B. Hardware Architecture*\n\nThe overall hardware architecture\
    \ of SPEED is shown in Fig. [1.](#page-1-0) To realize efficient multi-precision\
    \ DNN inference, \u278A vector instruction decode unit (VIDU) is developed to\
    \ decode customized instructions as well as the standard RVV instruction set.\
    \ Furthermore, \u278B vector load unit (VLDU) is designed to distribute data through\
    \ broadcast or ordered allocation, enabling our design to meet the diverse computation\
    \ requirements of mixed dataflow strategy. Scalable modules for vector processors,\
    \ namely \u278C lane, serve as the main computational components of the proposed\
    \ processor, which consists of lane sequencer, VRFs, systolic array unit (SAU)\
    \ and arithmetic logic unit (ALU).\n\nTo enhance the processor's parallel processing\
    \ capability and fully exploit data reuse opportunities, a highly flexible and\
    \ parameterized multi-precision \u278D SAU is presented as the main computing\
    \ unit of SPEED, which is composed of three components: operand requester, queues,\
    \ and systolic array core (SA Core). The operand requester consists of an address\
    \ generator and a request arbiter, enabling efficient data access by concurrently\
    \ generating addresses and prioritizing data requests. The queue is responsible\
    \ for buffering the data involved in the computation, including inputs, weights,\
    \ accumulation results, and outputs. The SA Core is a reconfigurable twodimensional\
    \ array of processing elements (PEs) determined by the parameters T ILE R and\
    \ T ILE C, which can be flexibly adjusted for different computation requirements.\
    \ For convolution operations, SA Core architecture employs three levels of parallelism:\
    \ within each PE on input channel dimension, across the PE array within each lane\
    \ on output channel dimension and on the height dimension of feature map. Moreover,\
    \ each PE consists of sixteen 4-bit multipliers that can be dynamically combined\
    \ to perform multiply-accumulate operation (MAC) with 16-bit precision, four sets\
    \ of MACs at 8-bit precision, or sixteen sets of MACs at 4-bit precision.\n\n\
    ## *C. Dataflow Mapping*\n\nTo enhance SPEED's computational efficiency, an optimized\
    \ multi-precision dataflow strategy, combined with FF and CF strategies, is developed\
    \ to flexibly handle DNN layers with varying kernel sizes. Specifically, the proposed\
    \ FF strategy is suitable for convolutional layers with large kernel size, while\
    \ the CF strategy is better suited for those with small kernel size. Detailed\
    \ descriptions are provided as follows.\n\nTo unify multi-precision data representation,\
    \ all inputs and weights are preprocessed into multiple elements along the input\
    \ channel dimension. Specifically, every adjacent 1, 4, and 16 operands are combined\
    \ into a unified element under 16 bit, 8-bit, and 4-bit precision modes, respectively.\
    \ Both the FF and CF strategies support continuous multi-stage convolution computations.\
    \ To clearly show how these strategies work, we select the first and second stages\
    \ as examples of broadcasting the inputs and allocating the weights to the lanes,\
    \ as shown in Fig. [2.](#page-2-0) Here, the size of weight kernel is 3 \xD7 3,\
    \ and the parameter T ILE H is set to 4, which is determined by the parallelism\
    \ of the SAU and kernel size.\n\n![](_page_2_Figure_1.jpeg)\n\n<span id=\"page-2-0\"\
    ></span>Fig. 2. Examples on how CF strategy and FF strategy work with multiprecision\
    \ elements. Note that the 4-bit data is operated in the same way as 16-bit and\
    \ 8-bit.\n\nFor example, as shown on the left of Fig. [2](#page-2-0) (a), 16-bit\
    \ input (X16) elements are pre-fetched to the VRF during two-stage convolution\
    \ operations. In FF strategy, to facilitate parallel computing and reduce off-chip\
    \ data movement, we pre-fetch 4\xD74 elements on a single input channel of inputs.\
    \ These prefetched elements are requested from the VRFs and calculated in the\
    \ SAU during the two-stage convolution operations, and the request process is\
    \ as depicted in the right of Fig. [2](#page-2-0) (a). When the first stage is\
    \ completed, the results are stored to the VRFs, and the elements in the overlapping\
    \ areas of the blue and red boxes are reused. However, multi-stage convolution\
    \ results occupy a large portion of the storage space in VRFs, and extra time\
    \ is wasted in transferring the partial results between stages. In order to reduce\
    \ the memory footprint of VRFs and avoid output transfer latency, the CF strategy\
    \ is proposed to pre-fetch elements along the input channel dimension. The two-stage\
    \ convolution operations pre-fetch elements from the two input channels of inputs.\
    \ The first stage and the second stage respectively request the first and the\
    \ second input channels of the pre-fetched elements for computations, and the\
    \ results of the two stages accumulate inside the SAU.\n\nFig. [2](#page-2-0)\
    \ (b) and (c) illustrate how to the pre-fetch and the request of 16-bit weight\
    \ (W16) and 8-bit weight (W8) elements in a two-stage convolution operations.\
    \ In W16+CF mode, we prefetch 2 weights along the output channel dimension to\
    \ enhance parallel computing within a single lane, where the number of weights\
    \ is determined by T ILE C. The elements at the same position in the first channel\
    \ of both weights are simultaneously requested by the SAU and participate in the\
    \ convolution computations for the first stage. After the first stage, the second\
    \ channel elements of the weights participate in the computations. In W8+FF mode,\
    \ as discussed in Sec. [II-B,](#page-1-1) unified W8 element has a parallelism\
    \ of 4 along the input channel dimension, thereby enhancing computational efficiency.\
    \ Weights are reused in the second stage to minimize off-chip data movement when\
    \ the computations in the first stage are completed. In summary, the FF strategy\
    \ takes advantage of calculating larger convolution kernels due to its high data\
    \ reuse. The CF strategy is suitable for smaller convolution kernels with low\
    \ reuse requirements, as it reduces the memory footprint of partial results and\
    \ avoids additional output transfer consumption.\n\n## III. EXPERIMENTAL RESULTS\n\
    \n## *A. Experimental Setup*\n\nTo highlight the effectiveness of our design,\
    \ we select Ara [\\[6\\]](#page-4-10) as a baseline design for comparison, which\
    \ is the first opensource implementation of RISC-V vector processor. Several popular\
    \ DNNs are implemented as benchmarks for evaluation, including VGG16 [\\[21\\\
    ]](#page-4-13), ResNet18 [\\[22\\]](#page-4-14), GoogLeNet [\\[23\\]](#page-4-15),\
    \ and SqueezeNet [\\[24\\]](#page-4-16). Note that the evaluated metric is area\
    \ efficiency (GOPS/mm<sup>2</sup> ), measured across the convolutional layers\
    \ in the DNN model using cycle-accurate simulation with QuestaSim, consistent\
    \ with the experimental method in [\\[6\\]](#page-4-10). To evaluate the impact\
    \ of our architectural modifications at the hardware level, we synthesize both\
    \ SPEED and Ara using Synopsys Design Compiler 2014.09 on the TSMC 28 nm process.\
    \ For a fair comparison, we use 4 lanes and a vector length of 4096 bits for both\
    \ SPEED and Ara. And we set both T ILE R and T ILE C of SAU in each SPEED's lane\
    \ to 4 for these experiments. Finally, we conduct a comprehensive comparison between\
    \ our design and Ara to demonstrate the effectiveness of SPEED in multi-precision\
    \ DNN deployments.\n\n![](_page_2_Figure_9.jpeg)\n\n<span id=\"page-2-1\"></span>Fig.\
    \ 3. Layer-wise area efficiency breakdown of GoogLeNet on SPEED under 16-bit precision.\
    \ Our mixed dataflow strategy surpasses the FF-only and CFonly strategies by 1.88\xD7\
    \ and 1.38\xD7, respectively.\n\n## *B. Model Evaluation*\n\nTo evaluate the impact\
    \ of convolution kernel size on different dataflow strategies, a layer-wise evaluation\
    \ of GoogleNet, which employs diverse convolution kernel sizes, is conducted on\
    \ SPEED using various strategies under the 16-bit precision condition. The mixed\
    \ strategy dynamically selects the FF-only or CF-only strategy with the best performance\
    \ in each layer to further improve area efficiency. As shown in Fig. [3,](#page-2-1)\
    \ the mixed dataflow strategy achieves a area efficiency improvement of 1.88\xD7\
    \ and 1.38\xD7 in comparison with the FF-only and CFonly dataflow strategies,\
    \ respectively. Meanwhile, compared with Ara, the FF-only and CF-only dataflow\
    \ strategies have a area efficiency improvement of 1.87\xD7 and 2.55\xD7, respectively,\
    \ and the mixed dataflow strategy achieves a 3.53\xD7 increase in area efficiency.\
    \ To more clearly illustrate the composition of the mixed strategy in the evaluation\
    \ of GoogleNet, Fig. [3](#page-2-1) presents a layer-wise breakdown of the mixed\
    \ strategy and annotates the specific strategy used. The results indicate that\
    \ the CF-only strategy is better suited for conv1x1, while the FFonly strategy\
    \ is suitable for other convolution kernels, where convKxK denotes a convolutional\
    \ operator with a kernel size of K. Meanwhile, it shows that with larger convolution\
    \ kernel sizes, the area efficiency improves. Therefore, employing the mixed dataflow\
    \ strategy can significantly enhance SPEED's performance for DNN deployment.\n\
    \n![](_page_3_Figure_2.jpeg)\n\n<span id=\"page-3-0\"></span>Fig. 4. Average area\
    \ efficiency under multi-precision DNN benchmarks, SPEED outperforms Ara by 2.77\xD7\
    \ and 6.39\xD7 at 16-bit and 8-bit precision, respectively.\n\nAs illustrated\
    \ in Fig. [4,](#page-3-0) we conduct a comprehensive area efficiency evaluation\
    \ of SPEED with the mixed dataflow strategy using multiple popular DNNs across\
    \ various precisions. SPEED achieves 2.77\xD7 and 6.39\xD7 higher area efficiency\
    \ over Ara on average under 16-bit and 8-bit precision conditions, respectively.\
    \ Moreover, SPEED enables efficient 4 bit inference with an average area efficiency\
    \ of up to 94.6 GOPS/mm<sup>2</sup> , surpassing the best of Ara by 12.78\xD7\
    \ on these DNN benchmarks.\n\n![](_page_3_Figure_5.jpeg)\n\n<span id=\"page-3-1\"\
    ></span>Fig. 5. Area Breakdown of (a) SPEED and (b) a single lane. SAU occupies\
    \ only 26% of the area in a single lane while achieving significant computational\
    \ performance.\n\n## *C. Analysis of Synthesized Results*\n\nFig. [5](#page-3-1)\
    \ (a) illustrates the area breakdown of SPEED with a total area of 1.10 mm<sup>2</sup>\
    \ . It reveals that the majority of SPEED's area, up to 90%, is occupied by the\
    \ lanes. Moreover, Fig. [5](#page-3-1) (b) shows that the area of a lane is primarily\
    \ consumed by four components: OP Queues (25%), OP Requester (17%), VRFs (18%)\
    \ and SAU (26%). Notably, SAU serves as a key component of SPEED, enabling a 12.78\xD7\
    \ area efficiency improvement in DNN inference performance on SPEED compared to\
    \ Ara. Despite the substantial performance boost, SAU accounts for only 26% of\
    \ the lane area, which corresponds to about 24% of the total area of SPEED.\n\n\
    TABLE I SYNTHESIZED RESULTS OF ARA AND SPEED\n\n<span id=\"page-3-2\"></span>\n\
    \n|                                             | Ara [6]           | SPEED (Ours)\
    \         |\n|---------------------------------------------|-------------------|----------------------|\n\
    | ISA                                         | RV64GCV1.0        | RV64GCV1.0\
    \           |\n| Frequency                                   | 500 MHz @ 0.9 V\
    \   | 500 MHz @ 0.9 V      |\n| Chip Area (mm2<br>)                         |\
    \ 0.44              | 1.10                 |\n| Int. Formats (bit)           \
    \               | 8, 16, 32, and 64 | 4, 8, 16, 32, and 64 |\n| Power (mW)   \
    \                               | 61.14             | 215.16               |\n\
    | Peak Int. Throughput<br>(GOPS)              | 6.82 (16b)        | 34.89 (16b)\
    \          |\n|                                             | 22.95 (8b)     \
    \   | 93.65 (8b)           |\n|                                             |\
    \ -                 | 287.41 (4b)          |\n| Peak Int. Area Efficiency<br>(GOPS/mm2<br>)\
    \ | 15.51 (16b)       | 31.72 (16b)          |\n|                            \
    \                 | 52.16 (8b)        | 85.13 (8b)           |\n|            \
    \                                 | -                 | 261.28 (4b)          |\n\
    | Peak Int. Energy Efficiency<br>(GOPS/W)     | 111.61 (16b)      | 162.15 (16b)\
    \         |\n|                                             | 373.68 (8b)     \
    \  | 435.25 (8b)          |\n|                                             | -\
    \                 | 1335.79 (4b)         |\n|                                \
    \             |                   |                      |\n\nTable [I](#page-3-2)\
    \ provides a comprehensive comparison between SPEED and Ara [\\[6\\]](#page-4-10),\
    \ showing the peak throughput results of Ara and SPEED across various precision\
    \ conditions through evaluating each convolutional layer in all DNN benchmarks.\
    \ Based on the results in Table [I,](#page-3-2) SPEED shows 5.12\xD7 and 4.14\xD7\
    \ throughput improvement under 16-bit and 8-bit precision condition, respectively.\
    \ In addition, SPEED enables efficient 16-bit and 8-bit inference that surpasses\
    \ the best of Ara by 2.04\xD7 and 1.63\xD7, respectively, on area efficiency.\
    \ In terms of energy efficiency, SPEED also demonstrates enhancements of 1.45\xD7\
    \ for 16-bit and 1.16\xD7 for 8-bit cases, respectively.\n\n## IV. CONCLUSION\n\
    \nIn this paper, a scalable RISC-V vector processor, namely SPEED, is proposed\
    \ to enable efficient multi-precision DNN inference. SPEED develops several customized\
    \ RISC-V vector instructions to efficiently support DNN operations ranging from\
    \ 4-bit to 16-bit precision. It also enhances the parallel processing capabilities\
    \ of scalable modules to boost throughput. Moreover, a mixed dataflow strategy\
    \ is presented that significantly improves the computational efficiency for various\
    \ convolution kernels. Experimental results show that SPEED achieves 2.04\xD7\
    \ and 1.63\xD7 higher area efficiency over the pioneer open-source vector processor,\
    \ Ara, under 16-bit and 8-bit precision conditions, respectively, demonstrating\
    \ its good potential for multi-precision DNN inference.\n\n## ACKNOWLEDGMENT\n\
    \nThis work was supported in part by the National Key R&D Program of China under\
    \ Grant 2022YFB4400604, in part by the National Natural Science Foundation of\
    \ China under Grant 62174084 and 62341408, and in part by the AI & AI for Science\
    \ Project of Nanjing University.\n\n## REFERENCES\n\n- <span id=\"page-4-0\"></span>[1]\
    \ Y. Xu *et al.*, \"Towards Developing High Performance RISC-V Processors Using\
    \ Agile Methodology,\" in *2022 55th IEEE/ACM International Symposium on Microarchitecture\
    \ (MICRO)*, 2022, pp. 1178\u20131199.\n- [2] D. Rossi *et al.*, \"Vega: A Ten-Core\
    \ SoC for IoT Endnodes With DNN Acceleration and Cognitive Wake-Up From MRAM-Based\
    \ State-Retentive Sleep Mode,\" *IEEE Journal of Solid-State Circuits (JSSC)*,\
    \ vol. 57, no. 1, pp. 127\u2013139, 2022.\n- [3] A. Garofalo *et al.*, \"Dustin:\
    \ A 16-Cores Parallel Ultra-Low-Power Cluster With 2b-to-32b Fully Flexible Bit-Precision\
    \ and Vector Lockstep Execution Mode,\" *IEEE Transactions on Circuits and Systems\
    \ I: Regular Papers (TCAS-I)*, vol. 70, no. 6, pp. 2450\u20132463, 2023.\n- [4]\
    \ A. Garofalo *et al.*, \"DARKSIDE: A Heterogeneous RISC-V Compute Cluster for\
    \ Extreme-Edge On-Chip DNN Inference and Training,\" *IEEE Open Journal of the\
    \ Solid-State Circuits Society (OJSSCS)*, vol. 2, pp. 231\u2013243, 2022.\n- <span\
    \ id=\"page-4-8\"></span>[5] M. Perotti *et al.*, \"Yun: An Open-Source, 64-Bit\
    \ RISC-V-Based Vector Processor With Multi-Precision Integer and Floating-Point\
    \ Support in 65-nm CMOS,\" *IEEE Transactions on Circuits and Systems II: Express\
    \ Briefs (TCAS-II)*, vol. 70, no. 10, pp. 3732\u20133736, 2023.\n- <span id=\"\
    page-4-10\"></span>[6] M. Perotti *et al.*, \"A \"New Ara\" for Vector Computing:\
    \ An Open Source Highly Efficient RISC-V V 1.0 Vector Processor Design,\" in *2022\
    \ IEEE 33rd International Conference on Application-specific Systems, Architectures\
    \ and Processors (ASAP)*, 2022, pp. 43\u201351.\n- [7] M. Cavalcante *et al.*,\
    \ \"Ara: A 1-GHz+ Scalable and Energy-Efficient RISC-V Vector Processor With Multiprecision\
    \ Floating-Point Support in 22-nm FD-SOI,\" *IEEE Transactions on Very Large Scale\
    \ Integration Systems (TVLSI)*, vol. 28, no. 2, pp. 530\u2013543, 2020.\n- [8]\
    \ M. Askarihemmat *et al.*, \"Quark: An Integer RISC-V Vector Processor for Sub-Byte\
    \ Quantized DNN Inference,\" in *2023 IEEE International Symposium on Circuits\
    \ and Systems (ISCAS)*, 2023, pp. 1\u20135.\n- <span id=\"page-4-9\"></span>[9]\
    \ T. Dupuis *et al.*, \"Sparq: A Custom RISC-V Vector Processor for Efficient\
    \ Sub-Byte Quantized Inference,\" in *2023 21st IEEE Interregional NEWCAS Conference\
    \ (NEWCAS)*, 2023, pp. 1\u20135.\n- <span id=\"page-4-3\"></span>[10] Z. He *et\
    \ al.*, \"Agile Hardware and Software Co-design for RISC-V-based Multi-precision\
    \ Deep Learning Microprocessor,\" in *2023 28th Asia and South Pacific Design\
    \ Automation Conference (ASP-DAC)*, 2023, pp. 490\u2013495.\n- <span id=\"page-4-1\"\
    ></span>[11] L. Huang *et al.*, \"A Precision-Scalable RISC-V DNN Processor with\
    \ On-Device Learning Capability at the Extreme Edge,\" *arXiv preprint arXiv:2309.08186*,\
    \ 2023.\n- <span id=\"page-4-2\"></span>[12] A. Waterman, \"Design of the RISC-V\
    \ Instruction Set Architecture,\" Ph.D. dissertation, EECS Department, University\
    \ of California, Berkeley, Jan 2016. [Online]. Available: [http://www2.eecs.berkeley.edu/](http://www2.eecs.berkeley.edu/Pubs/TechRpts/2016/EECS-2016-1.html)\
    \ [Pubs/TechRpts/2016/EECS-2016-1.html](http://www2.eecs.berkeley.edu/Pubs/TechRpts/2016/EECS-2016-1.html)\n\
    - <span id=\"page-4-4\"></span>[13] M. Askarihemmat *et al.*, \"BARVINN: Arbitrary\
    \ Precision DNN Accelerator Controlled by a RISC-V CPU,\" in *2023 28th Asia and\
    \ South Pacific Design Automation Conference (ASP-DAC)*, 2023, pp. 483\u2013489.\n\
    - [14] X. Wu *et al.*, \"A Flexible and Efficient FPGA Accelerator for Various\
    \ Large-scale and Lightweight CNNs,\" *IEEE Transactions on Circuits and Systems\
    \ I: Regular Papers (TCAS-I)*, vol. 69, no. 3, pp. 1185\u20131198, 2022.\n- <span\
    \ id=\"page-4-5\"></span>[15] C. Fang *et al.*, \"Accelerating 3D Convolutional\
    \ Neural Networks Using 3D Fast Fourier Transform,\" in *2021 IEEE International\
    \ Symposium on Circuits and Systems (ISCAS)*, 2021, pp. 1\u20135.\n- <span id=\"\
    page-4-6\"></span>[16] R. Ding *et al.*, \"Quantized Deep Neural Networks for\
    \ Energy Efficient Hardware-based Inference,\" in *2018 23rd Asia and South Pacific\
    \ Design Automation Conference (ASP-DAC)*, 2018, pp. 1\u20138.\n- [17] J. Tian\
    \ *et al.*, \"BEBERT: Efficient and Robust Binary Ensemble BERT,\" in *2023 IEEE\
    \ International Conference on Acoustics, Speech and Signal Processing (ICASSP)*,\
    \ 2023, pp. 1\u20135.\n- <span id=\"page-4-7\"></span>[18] A. Zhou *et al.*, \"\
    Explicit Loss-Error-Aware Quantization for Low-Bit Deep Neural Networks,\" in\
    \ *2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)*,\
    \ 2018, pp. 9426\u20139435.\n- <span id=\"page-4-11\"></span>[19] F. Zaruba *et\
    \ al.*, \"The Cost of Application-Class Processing: Energy and Performance Analysis\
    \ of a Linux-Ready 1.7-GHz 64-Bit RISC-V Core in 22-nm FDSOI Technology,\" *IEEE\
    \ Transactions on Very Large Scale Integration Systems (TVLSI)*, vol. 27, no.\
    \ 11, pp. 2629\u20132640, Nov 2019.\n- <span id=\"page-4-12\"></span>[20] K. Li\
    \ *et al.*, \"A Precision-Scalable Energy-Efficient Bit-Split-and-Combination\
    \ Vector Systolic Accelerator for NAS-Optimized DNNs on Edge,\" in *2022 Design,\
    \ Automation & Test in Europe Conference & Exhibition (DATE)*, 2022, pp. 730\u2013\
    735.\n- <span id=\"page-4-13\"></span>[21] S. Liu *et al.*, \"Very deep convolutional\
    \ neural network based image classification using small training sample size,\"\
    \ in *2015 3rd IAPR Asian Conference on Pattern Recognition (ACPR)*, 2015, pp.\
    \ 730\u2013734.\n- <span id=\"page-4-14\"></span>[22] K. He *et al.*, \"Deep Residual\
    \ Learning for Image Recognition,\" in *2016 IEEE Conference on Computer Vision\
    \ and Pattern Recognition (CVPR)*, 2016, pp. 770\u2013778.\n- <span id=\"page-4-15\"\
    ></span>[23] C. Szegedy *et al.*, \"Going Deeper with Convolutions,\" in *2015\
    \ IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*, 2015, pp.\
    \ 1\u20139.\n- <span id=\"page-4-16\"></span>[24] F. N. Iandola *et al.*, \"SqueezeNet:\
    \ AlexNet-level Accuracy with 50x Fewer Parameters and <0.5MB Model Size,\" *CoRR*,\
    \ 2016. [Online]. Available: <http://arxiv.org/abs/1602.07360>"
  paper_id: 29
  scores:
    originality:
      raw: 4.2
      reason: The paper introduces a novel mechanism for content selection in STT-RAM
        SLLCs, which is a fresh approach to addressing existing inefficiencies in
        cache management.
    technical_soundness:
      raw: 4.0
      reason: The proposed method is well-grounded in existing literature and demonstrates
        a solid understanding of the technical challenges involved, although some
        experimental validation may be needed.
    clarity:
      raw: 3.8
      reason: While the paper presents its ideas clearly, certain sections could benefit
        from more concise explanations and better organization to enhance readability.
    significance:
      raw: 4.1
      reason: The work addresses a critical issue in memory hierarchy design, making
        it significant for both academic research and practical applications in chip
        multiprocessors.
    relevance:
      raw: 4.5
      reason: The topic is highly relevant to current trends in computer architecture,
        particularly with the increasing adoption of non-volatile memory technologies
        in modern systems.
  total_score: 4.07
  tier: C
- title: A Statically and Dynamically Scalable Soft GPGPU
  abstract: 'Text generation is a compelling sub-field of natural language processing,

    aiming to generate human-readable text from input words. In particular, the

    decoder-only generative models, such as generative pre-trained transformer

    (GPT), are widely used for text generation, with two major computational

    stages: summarization and generation. Unlike the summarization stage, which can

    process the input tokens in parallel, the generation stage is difficult to

    accelerate due to its sequential generation of output tokens through iteration.

    Moreover, each iteration requires reading a whole model with little data reuse

    opportunity. Therefore, the workload of transformer-based text generation is

    severely memory-bound, making the external memory bandwidth system bottleneck.

    In this paper, we proposed a subarray-level processing-in-memory architecture

    named SAL-PIM, HBM-based PIM architecture for the end-to-end acceleration of

    transformer-based text generation. The SAL-PIM architecture includes three

    architectural features. First, the SAL-PIM architecture utilizes higher

    internal bandwidth by integrating multiple subarray-level arithmetic logic

    units with optimized data mapping schemes. Second, the SAL-PIM architecture

    adopts LUT-based linear interpolation to perform complex non-linear functions

    in PIM. Third, the SAL-PIM architecture accelerates end-to-end inference on PIM

    in text generation. Furthermore, to validate the SAL-PIM architecture, we built

    cycle-accurate simulator and implemented the SAL-PIM''s logic units in 28-nm

    CMOS technology. As a result, when the input size is from 32 to 128 and the

    output size is from 1 to 256, SAL-PIM achieves a maximum of 4.72 times speedup

    and an average of 1.83 times speedup for the text generation based on the GPT-2

    medium model compared to the server-level GPU.'
  url: http://arxiv.org/abs/2401.17005v1
  keywords: "*\u2014Processing-in-memory, DRAM, Transformer, Text generation."
  document: "# SAL-PIM: A Subarray-level Processing-in-Memory Architecture with LUT-based\
    \ Linear Interpolation for Transformer-based Text Generation\n\nWontak Han, *Student\
    \ Member, IEEE,* Hyunjun Cho, *Student Member, IEEE,* Donghyuk Kim, *Student Member,\
    \ IEEE,* and Joo-Young Kim, *Senior Member, IEEE*\n\n**Abstract**\u2014Text generation\
    \ is a compelling sub-field of natural language processing, aiming to generate\
    \ human-readable text from input words. Although many deep learning models have\
    \ been proposed, the recent emergence of transformer-based large language models\
    \ advances its academic research and industry development, showing remarkable\
    \ qualitative results in text generation. In particular, the decoder-only generative\
    \ models, such as generative pre-trained transformer (GPT), are widely used for\
    \ text generation, with two major computational stages: summarization and generation.\
    \ Unlike the summarization stage, which can process the input tokens in parallel,\
    \ the generation stage is difficult to accelerate due to its sequential generation\
    \ of output tokens through iteration. Moreover, each iteration requires reading\
    \ a whole model with little data reuse opportunity. Therefore, the workload of\
    \ transformer-based text generation is severely memory-bound, making the external\
    \ memory bandwidth system bottleneck. In this paper, we propose a subarray-level\
    \ processing-in-memory (PIM) architecture named SAL-PIM, the first HBM-based PIM\
    \ architecture for the end-to-end acceleration of transformer-based text generation.\
    \ With optimized data mapping schemes for different operations, SAL-PIM utilizes\
    \ higher internal bandwidth by integrating multiple subarray-level arithmetic\
    \ logic units (S-ALUs) next to memory subarrays. To minimize the area overhead\
    \ for S-ALU, it uses shared MACs leveraging slow clock frequency of commands for\
    \ the same bank. In addition, a few subarrays in the bank are used as look-up\
    \ tables (LUTs) to handle non-linear functions in PIM, supporting multiple addressing\
    \ to select sections for linear interpolation. Lastly, the channel-level arithmetic\
    \ logic unit (C-ALU) is added in the buffer die of HBM to perform the accumulation\
    \ and reduce-sum operations of data across multiple banks, completing end-to-end\
    \ inference on PIM. To validate the SAL-PIM architecture, we built a cycle-accurate\
    \ simulator based on Ramulator. We also implemented the SAL-PIM's logic units\
    \ in 28-nm CMOS technology and scaled the results to DRAM technology to verify\
    \ its feasibility. We measured the end-to-end latency of SAL-PIM when it runs\
    \ various text generation workloads on the GPT-2 medium model (with 345 million\
    \ parameters), in which the input and output token numbers vary from 32 to 128\
    \ and from 1 to 256, respectively. As a result, with 4.81% area overhead, SAL-PIM\
    \ achieves up to 4.72\xD7 speedup (1.83\xD7 on average) over the Nvidia Titan\
    \ RTX GPU running FasterTransformer Framework.\n\n\u2726\n\n**Index Terms**\u2014\
    Processing-in-memory, DRAM, Transformer, Text generation.\n\n# **1 INTRODUCTION**\n\
    \nD EEP learning technology has made significant progress on various cognitive\
    \ tasks, but the vast adoption also reveals its shortcomings, such as limited\
    \ generalizability and lack of interpretability. Witnessing the performance saturation\
    \ of early models such as multi-layer perceptron (MLP), convolutional neural network\
    \ (CNN), and recurrent neural network (RNN), one notable recent innovation in\
    \ deep learning architecture is transformer [\\[1\\]](#page-11-0). It has two\
    \ good properties towards artificial general intelligence over conventional models.\
    \ First, the performance of transformer models continues to grow with their model\
    \ sizes and training data. Second, transformers can be pre-trained with tons of\
    \ unlabeled data either through unsupervised or selfsupervised learning and can\
    \ be fine-tuned quickly for each\n\n*E-mail:* {*11tak, jooyoung1203*}*@kaist.ac.kr*\n\
    \napplication. With the above characteristics, transformerbased models quickly\
    \ become mainstream in natural language processing (NLP) as well as other applications\
    \ such as image classification [\\[2\\]](#page-12-0) and object detection [\\\
    [3\\]](#page-12-1), achieving higher accuracy than other deep-learning models.\n\
    \nText generation is one of the most popular applications in natural language\
    \ processing, in which its task is to generate human-readable and plausible sentences\
    \ from input words called tokens. Text generation is highly demanded in the conversational\
    \ AI industry, including chatbots [\\[4\\]](#page-12-2) and article writing [\\\
    [5\\]](#page-12-3). It is also essential for automatic code generation within\
    \ the context of no-code development paradigm [\\[6\\]](#page-12-4). Among many\
    \ transformer models, a decoderonly transformer model from OpenAI named generative\
    \ pretrained transformer (GPT) [\\[7\\]](#page-12-5), [\\[8\\]](#page-12-6) achieves\
    \ notable performance in text generation. GPT models are pretrained on massive\
    \ internet data with enormous parameter numbers, raising the quality of text generation\
    \ close to the human level.\n\nThe text generation process consists of summarization\n\
    \n<sup>\u2022</sup> *The authors are with the Department of Electrical Engineering,\
    \ Korea Advanced Institute of Science and Technology (KAIST), Daejeon 34141, South\
    \ Korea.*\n\n*Manuscript received April 19, 2005; revised August 26, 2015.*\n\n\
    ![](_page_1_Figure_0.jpeg)\n\n<span id=\"page-1-0\"></span>Fig. 1. The execution\
    \ time by input and output size for the GPT-2 medium model on a GPU.\n\nand generation\
    \ stages. In the summarization stage, all input tokens are computed simultaneously\
    \ to generate a single output token that will be the input token of the generation\
    \ stage. On the other hand, the generation stage sequentially generates output\
    \ tokens through iterations (i.e., an output token per iteration) because the\
    \ output token becomes the input token of the next iteration. Figure [1](#page-1-0)\
    \ shows the execution time of the text generation based on the GPT-2 medium model\
    \ running on Nvidia Titan RTX GPU, where the input size (i.e., the number of input\
    \ tokens) and output size (i.e., the number of output tokens) vary. As the graph\
    \ shows, an increase in the output size linearly increases the total execution\
    \ time, while an increase in the input size has little impact. This result is\
    \ because the GPU can efficiently handle a batch of input tokens, but it cannot\
    \ perform parallel processing in the generation stage due to its sequential nature.\
    \ In addition, the GPT operations in each iteration of the generation stage are\
    \ mostly memory-bounded, with a large model size. Furthermore, the size of transformer\
    \ models has a trend of exponential increase (e.g., max 1.5 billion parameters\
    \ in GPT-2 [\\[7\\]](#page-12-5) and 175 billion parameters in GPT-3 [\\[8\\]](#page-12-6)).\
    \ It is easily concluded that the performance of text generation depends on the\
    \ number of iterations in the generation stage and the system's effective memory\
    \ bandwidth.\n\nProcessing-in-memory (PIM) is a promising solution for the memory\
    \ bandwidth problem. By performing operations near memory, PIM can utilize higher\
    \ memory bandwidth, hence improving performance significantly for memorybound\
    \ operations. Accordingly, many PIM works have been proposed to accelerate memory-bound\
    \ applications [\\[9\\]](#page-12-7), [\\[10\\]](#page-12-8), [\\[11\\]](#page-12-9),\
    \ [\\[12\\]](#page-12-10), [\\[13\\]](#page-12-11), [\\[14\\]](#page-12-12), [\\\
    [15\\]](#page-12-13), [\\[16\\]](#page-12-14). Most studies have focused on accelerating\
    \ the bit-wise operation and general matrixvector multiplication (GEMV). It is\
    \ reasonable because GEMV is a typical memory-bound operation and occupies a large\
    \ portion of operations. However, the transformerbased models also include additional\
    \ non-linear functions, which require complex operations that are hard to compute\
    \ using multiplication and addition. The non-linear functions are compute-bound\
    \ operations than memory-bound operations, and these functions are not negligible\
    \ in the entire execution time.\n\nAll things considered, there are two ways to\
    \ achieve higher speedup in PIM. First, the higher bandwidth for memory-bound\
    \ operation enables higher speedup. GEMV operations for larger weights and biases\
    \ require higher bandwidth. Second, acceleration for non-linear functions also\
    \ significantly improves overall performance. Not only do non-linear functions\
    \ take up a large part, but if PIM accelerates GEMV operation, the proportion\
    \ of non-linear functions in total execution time increases further. Moreover,\
    \ supporting non-linear functions in PIM enables accelerating end-to-end model\
    \ inference. End-to-end acceleration in PIM removes data movement for the intermediate\
    \ data and prevents the overhead caused by switching between PIM operation and\
    \ host (generally CPU or GPU) operations [\\[12\\]](#page-12-10), [\\[17\\]](#page-12-15).\n\
    \nThe acceleration of transformer-decoder-based generative models in PIM faces\
    \ several challenges due to its characteristics. First, previous bank-level PIMs\
    \ have limited bandwidth, and the triple-row activation scheme induces a high\
    \ latency. It is necessary to use higher bandwidth with low latency to cope with\
    \ the number of parameters that continue to grow. Second, integrating hardware\
    \ units in PIM for all necessary operations of the model is challenging. PIM cannot\
    \ use sufficient metal and area because it is implemented in DRAM technology.\
    \ Therefore, PIM should support complex operations with limited hardware units.\
    \ Lastly, data movement is time-consuming, and it is a critical overhead for the\
    \ entire execution time in PIM. Performing operations simultaneously at the bank-level\
    \ causes data movement between banks. Furthermore, data movement between PIM and\
    \ the host also affects overall execution time. Accordingly, supporting end-to-end\
    \ model inference in PIM can eliminate wasteful data movement.\n\nTo address the\
    \ above challenges, we propose SAL-PIM, a subarray-level PIM architecture for\
    \ accelerating the endto-end transformer-decoder-based generative model in PIM.\
    \ We make the following contributions.\n\n- We present the SAL-PIM architecture\
    \ that includes subarray-level ALUs (S-ALUs), bank-level units, and channel-level\
    \ ALUs (C-ALUs). Two types of ALUs (S-ALU and C-ALU) are integrated into the subarray-level\
    \ and the channel-level, respectively. S-ALU utilizes higher bandwidth than bank-level\
    \ PIM to compute memorybound operation, and C-ALU supports accumulation and reduce-sum\
    \ operation for multiple banks, eliminating data movement between banks.\n- We\
    \ propose acceleration for non-linear functions using a look-up table (LUT)-based\
    \ linear interpolation in PIM. It enables the computation of complex functions\
    \ using S-ALUs without additional hardware. Moreover, we optimized a few subarrays,\
    \ named LUT-embedded subarray, in the bank for LUT operations in DRAM.\n- We present\
    \ a mapping scheme across subarray, bank, and channel for the SAL-PIM architecture.\
    \ The mapping method enables higher utilization for subarray-level computation\
    \ and removes data reshaping operations, such as transpose operation, using two\
    \ input feeding methods and two accumulation directions.\n- We evaluated the SAL-PIM\
    \ architecture using a simulator based on the latest DRAM simulator [\\[18\\]](#page-12-16).\
    \ SAL-PIM achieves a maximum 4.72\xD7 speedup and an average 1.83\xD7 speedup\
    \ compared to GPU in text generation (input size 32 to 128 and output size 1 to\
    \ 256) with the GPT-2 medium model. Furthermore, in order to verify the feasibility\
    \ of PIM, we implemented units of the SAL-PIM\n\n![](_page_2_Figure_1.jpeg)\n\n\
    <span id=\"page-2-0\"></span>Fig. 2. GPT structure and text generation process.\n\
    \narchitecture in 28-nm logic technology and scaled to 20 nm DRAM technology.\n\
    \n# <span id=\"page-2-2\"></span>**2 BACKGROUND**\n\n#### **2.1 Workload Analysis\
    \ of GPT-based Text Generation**\n\nText generation's task is generating sentences\
    \ from given input words. Figure [2](#page-2-0) shows the text generation process\
    \ and structure of GPT. As aforementioned, text generation consists of two stages:\
    \ summarization and generation. The summarization stage simultaneously computes\
    \ input tokens (*\"Hello, my name\"*), as shown in Figure [2.](#page-2-0) Each\
    \ token is embedded in a vector, so input tokens are embedded in a matrix. Therefore,\
    \ general matrix-matrix multiplications (GEMMs) are mainly performed in the summarization\
    \ stage, and the operations are compute-bounded due to the parameters being reused\
    \ for vectors in the embedded matrix. At the end of the summarization stage, GPT\
    \ makes one output token (*'is'*). Unlike the summarization stage, the generation\
    \ stage's input is only the former output token (*'is'*). The one input token\
    \ is embedded into one vector. Thus, the overall operation in the generation stage\
    \ consists of GEMVs which are memory-bound operations because the input is one\
    \ vector. Furthermore, the generation stage sequentially generates output tokens,\
    \ and the model's parameters should be loaded repeatedly for each token generation.\
    \ Correspondingly, the higher memory bandwidth improves overall inference speed.\n\
    \nGPT is one of the most famous transformer-decoderbased generative models. As\
    \ shown in Figure [2,](#page-2-0) GPT comprises an embedding layer and decoder\
    \ layers. In the embedding layer, input tokens are converted to vectors through\
    \ the embedding table and added with positional vectors by the token's position.\
    \ After passing through the embedding layer, the vector (generation stage) or\
    \ matrix (summarization stage) goes to the decoder layers. The decoder layer consists\
    \ of a multi-head attention (MHA), a feed-forward network (FFN), layerNorms, and\
    \ residual operations.\n\n#### *2.1.1 Multi-Head Attention (MHA)*\n\nAn MHA is\
    \ the most time-consuming layer for GPT in text generation. In the MHA, query\
    \ (*Q*), key (*K*), and value (*V*)\n\n![](_page_2_Figure_10.jpeg)\n\n<span id=\"\
    page-2-1\"></span>Fig. 3. The breakdown of execution time for the GPT-2 medium\
    \ model on a GPU.\n\nare derived from the input vectors. Generated *K* and *V*\
    \ are concatenated with *Ks* and *Vs*, which are generated in the former token\
    \ generations, respectively. Accordingly, the number of *K* and *V* increases\
    \ as token generation progresses. The MHA consists of three steps. First, *Q*,\
    \ *K*, and *V* are computed from the input vectors through multiplication with\
    \ weights and addition with biases. The generated *Q*, *K*, and *V* are separated\
    \ by heads and calculated between each other from the same head. Second, *Q* is\
    \ multiplied by *K* T , and the result is called the score. The score is masked\
    \ by the token position and generates an attention score (*S*) through softmax.\
    \ Then *S* is multiplied by *V*, and the results are concatenated for all heads.\
    \ *Q*\xD7*K* T and *S*\xD7*V* are GEMV in each head, but due to the transpose,\
    \ directions of the matrixvector multiplication are different. Lastly, the concatenated\
    \ vectors are computed by a fully-connected layer, then added with residual input.\n\
    \n#### *2.1.2 Feed-Forward Network (FFN)*\n\nAn FFN is composed of two fully-connected\
    \ layers and an activation function. The fully-connected layers perform multiplication\
    \ with weight and addition with bias. Its intermediate vector size is four times\
    \ larger than the input vector of the decoder layer, so the weight and bias are\
    \ larger than the fully-connected layer of MHA. Moreover, GELU, which consists\
    \ of complex functions (*tanh* and *sqrt*), is usually used as an activation function.\
    \ These complex functions are generally done with series calculations, which are\
    \ timeconsuming. Therefore, the FFN occupies a large part of the execution time\
    \ of the decoder layer due to the large GEMV and complex operations in the activation\
    \ function.\n\n#### *2.1.3 Layer Normalization (layerNorm)*\n\nThe remained parts\
    \ of the decoder layer are two layerNorms. The layerNorm is a non-linear function\
    \ like the softmax in the MHA and GELU in the FFN. Firstly, average and standard\
    \ deviation are computed position-wise for the input vector. Then, each data of\
    \ the input vector is subtracted from the average and divided by the standard\
    \ deviation. The result goes to a fully-connected operation to generate an output\
    \ vector.\n\nFigure [3](#page-2-1) shows the breakdown of execution time for the\
    \ GPT-2 medium model on Nvidia Titan RTX GPU. The ratio\n\nTABLE 1 DRAM-based\
    \ Processing-in-Memory\n\n<span id=\"page-3-0\"></span>\n\n| Type            \
    \      | Bandwidth | Latency | Operation              | Example              \
    \                      |\n|-----------------------|-----------|---------|------------------------|--------------------------------------------|\n\
    | Triple-row Activation | Very high | High    | Bit-wise operation     | Ambit[9],\
    \ ComputeDRAM[10],<br>TransPIM[11] |\n| Bank-level PIM        | Medium    | Low\
    \     | ALUs                   | FIM[12, 20], AiM[13, 21]                   |\n\
    | Subarray-level PIM    | High      | Low     | Logic gates or<br>ALUs | DRISA[14],\
    \ Fulcrum[15], SAL-PIM            |\n\nof the MHA and FFN is 50.26% and 29.36%,\
    \ respectively, which accounts for the most significant portion. Therefore, a\
    \ higher speedup can be expected when the MHA and FFN are accelerated with higher\
    \ bandwidth, so previous studies aimed to accelerate GEMV, the main operation\
    \ of the MHA and FFN, showed notable improvement. However, as shown in Figure\
    \ [3,](#page-2-1) the non-linear functions are also important in the total execution\
    \ time. Softmax in the MHA, GELU in the FFN, and layerNorm are the non-linear\
    \ function, which is a compute-bound operation, occupying 23.45%. In addition,\
    \ if the MHA and FFN are accelerated, the portion of non-linear functions is further\
    \ increased. As a result, accelerating the non-linear functions enable higher\
    \ speedup than only accelerating GEMV.\n\n## **2.2 DRAM-based Processing-in-Memory**\n\
    \nConventional systems using the original DRAM have limited memory bandwidth by\
    \ a memory standard called JEDEC [\\[19\\]](#page-12-17). In machine learning\
    \ applications, the limitation remarkably affects the overall latency because\
    \ most operations are memory-bound operations, such as multiplication and addition,\
    \ on a tremendous number of data. PIM enables leveraging higher bandwidth thanks\
    \ to performing computation nearby memory cells. Accordingly, PIM has been actively\
    \ studied to exploit the higher bandwidth. [\\[9\\]](#page-12-7), [\\[10\\]](#page-12-8),\
    \ [\\[11\\]](#page-12-9), [\\[12\\]](#page-12-10), [\\[13\\]](#page-12-11), [\\\
    [14\\]](#page-12-12), [\\[15\\]](#page-12-13), [\\[16\\]](#page-12-14).\n\nTable\
    \ [1](#page-3-0) summarizes DRAM-based PIMs. PIM architecture has two types. The\
    \ first type computes data using existing memory operations, such as activation,\
    \ read, and precharge, without additional ALU. For example, Ambit [\\[9\\]](#page-12-7)\
    \ and ComputeDRAM [\\[10\\]](#page-12-8) perform the bit-wise functions by sharing\
    \ bit-line with triple-row activation. These accomplish impressive speedup using\
    \ enormous bandwidth. TransPIM [\\[11\\]](#page-12-9) also adopts triple-row activation\
    \ with subarray-level adder tree, accelerating transformer models. However, it\
    \ is challenging to fully utilize bandwidth on complex operations for a small-size\
    \ vector, and the latency is long because row activation is repeated in DRAM.\
    \ Moreover, in order to compute complex functions, a Taylor series-like approach\
    \ [\\[11\\]](#page-12-9) is applied, which requires a lot of additions and multiplications.\n\
    \nThe second type computes data using ALUs in the memory. FIM [\\[12\\]](#page-12-10),\
    \ [\\[20\\]](#page-12-18) and AiM [\\[13\\]](#page-12-11), [\\[21\\]](#page-12-19)\
    \ perform MAC operations in the bank, and these are optimized to compute GEMV.\
    \ However, the number of parameters in the model is increasing, and the higher\
    \ bandwidth makes PIM expect a high speedup. Therefore, bank-level parallelism\
    \ is insufficient and should be extended to the subarray-level. There are also\
    \ previously proposed subarray-level PIM studies [\\[14\\]](#page-12-12), [\\\
    [15\\]](#page-12-13). DRISA [\\[14\\]](#page-12-12) implements logic gates or\
    \ adders\n\n![](_page_3_Figure_7.jpeg)\n\n<span id=\"page-3-1\"></span>Fig. 4.\
    \ Linear interpolation with a look-up table for GELU.\n\nin subarray and achieves\
    \ higher bandwidth. Nevertheless, it is not appropriate for computing non-linear\
    \ functions. Fulcrum [\\[15\\]](#page-12-13) puts an ALU in a subarray, which\
    \ is flexible with the operation by subarray. Although Fulcrum has flexibility,\
    \ which can perform various operations in PIM, it is more critical for PIM to\
    \ target transformer-based models to optimize memory-bound operations, such as\
    \ GEMV. As a result, a PIM structure that can perform memory-bound operations\
    \ with high bandwidth and accelerate non-linear function is required.\n\n## **2.3\
    \ Linear Interpolation**\n\nLinear interpolation is one of the approximation methods\
    \ to compute complex functions with limited hardware. Nonlinear functions in GPT\
    \ have complex functions, such as *exp*, *tanh*, and *sqrt*. Hence, to accelerate\
    \ the non-linear functions in PIM, a method of computing these with limited hardware\
    \ is required because PIM has limited area and power to implement computing units.\
    \ Figure [4](#page-3-1) shows linear interpolation for GELU. The range of input\
    \ data is divided into sections, and an LUT stores pre-calculated slopes (W) and\
    \ intercepts (B) for each section. Then, when the input comes in, find the section\
    \ that the input belongs to through decoding. The slope and intercept of the corresponding\
    \ section are multiplied and added with input, respectively. Thus, the complex\
    \ functions can be calculated with only one multiplication and addition. In other\
    \ words, the compute-bound operation is projected into the memory-bound operation\
    \ with LUT, so PIM is appropriate with linear interpolation. Naturally, if the\
    \ number of sections is insufficient, there is the possibility of accuracy loss.\
    \ So, we measured accuracy loss by linear interpolation with the GPT-2 medium\
    \ model on text generation, and the accuracy was kept when the number of sections\
    \ was larger than 32.\n\nSome previous works use linear interpolation. MVP [\\\
    [22\\]](#page-12-20) uses dynamic linear interpolation, which dynamically sets\
    \ the section size, to calculate various types of activation functions. MVP has\
    \ additional memory to store LUT, but PIM can use DRAM cells as LUT. Also, there\
    \ is PIM-based linear interpolation. AiM [\\[21\\]](#page-12-19) applies linear\
    \ interpolation to the results of adder trees. However, it only computes the activation\
    \ function. Therefore, to accelerate the end-to-end inference of the model, a\
    \ PIM structure that can calculate linear interpolation with DRAM cell-based LUT\
    \ is needed.\n\n# <span id=\"page-3-2\"></span>**3 SAL-PIM ARCHITECTURE**\n\n\
    SAL-PIM supports three architectural features to address challenges when accelerating\
    \ the execution of GPT endto-end. First, it exploits subarray-level parallelism\
    \ for\n\n![](_page_4_Figure_0.jpeg)\n\n<span id=\"page-4-0\"></span>Fig. 5. Overall\
    \ architecture of SAL-PIM.\n\nsubarray-level ALUs. The memory-bound operations\
    \ are computed with a higher bandwidth than the previous banklevel PIM. Second,\
    \ the SAL-PIM architecture adopts LUTbased linear interpolation, thus minimizing\
    \ circuit overhead for the complex non-linear functions required. It further eliminates\
    \ the LUT overhead by reusing the existing DRAM cells. Third, the SAL-PIM architecture\
    \ includes channellevel ALUs, which perform operations for all banks in the same\
    \ channel to support the entire GPT operations in memory. Accordingly, data movement\
    \ between the host and PIM is minimized.\n\n#### **3.1 Overall Architecture**\n\
    \nThe overall architecture of SAL-PIM is based on HBM2, as shown in Figure [5.](#page-4-0)\
    \ The SAL-PIM architecture is composed of four DRAM dies and a buffer die, which\
    \ are connected through silicon vias (TSVs). The DRAM die has a hierarchical structure\
    \ of channels, banks, and subarrays. Each channel consists of 16 banks, which\
    \ are connected to data buses shared with the other banks in the same channel.\
    \ Hence, the original HBM2 cannot access multiple banks simultaneously. In contrast,\
    \ the SAL-PIM architecture can access multiple banks simultaneously by integrating\
    \ computation logic units near each bank. Each bank includes subarrays with 512\
    \ rows [\\[23\\]](#page-12-21). Each subarray reads data to a bit-line sense amplifier\
    \ (BLSA) connected to the local bitlines (LBLs) for the column address. Then,\
    \ the LBLs are connected to global bit-lines (GBLs), which are used as the path\
    \ for subarray-level ALUs. In addition, the SAL-PIM architecture includes channel-level\
    \ ALUs and interconnection, which connects channels, on the buffer die.\n\nThe\
    \ SAL-PIM architecture includes three types of logic units: subarray-level ALU\
    \ (S-ALU), bank-level unit, and channel-level ALU (C-ALU).\n\n\u2022 S-ALU is\
    \ responsible for principle operations, such as element-wise addition, element-wise\
    \ multiplication, MAC, and max, with subarray-level parallelism. In order to exploit\
    \ subarray-level parallelism, the SAL-PIM architecture employs previous research\
    \ [\\[23\\]](#page-12-21), utilizing multiple subarrays. Multiple subarrays in\
    \ the bank are activated simultaneously by using a BLSA as a cache for each subarray.\
    \ Ideally, simultaneously operating all of the subarrays can utilize the maximum\
    \ bandwidth. However, integrating S-ALU into all subarrays causes impractical\
    \ area overhead, so the subarrays in the bank are grouped to use the S-ALU. For\
    \ example, if the number of S-ALU is 4 in a bank, the subarray group consists\
    \ of 15 subarrays without LUT-embedded subarray. The S-ALU is connected to the\
    \ GBLs to receive weight from memory. Therefore, when multiple subarrays are activated,\
    \ these are floated and transfer data to each S-ALU, while GBLs generally operate\
    \ as a single connected data path. Consequently, the SAL-PIM architecture enables\
    \ utilizing higher internal bandwidth.\n\nIn addition, linear interpolation for\
    \ the non-linear functions is also performed by multiplication and addition using\
    \ S-ALU. As aforementioned in Section [2,](#page-2-2) the nonlinear functions\
    \ are compute-bound which occupies a large portion of the total execution time.\
    \ The SAL-PIM architecture uses LUT-based linear interpolation to reduce the computation\
    \ intensity and accelerate non-linear functions. In the SAL-PIM architecture,\
    \ the S-ALU computes linear interpolation by multiplying slope and adding intercept\
    \ while subarrays are used as LUT. Thus, the acceleration of non-linear functions\
    \ is achieved in the SAL-PIM architecture without additional hardware.\n\n- A\
    \ bank-level unit supports feeding input data for S-ALU and generating select\
    \ signals for LUT-embedded subarray. MHA in GPT needs two types of matrix multiplication\
    \ (*Q*\xD7*K* T and *S*\xD7*V*), so the SAL-PIM architecture has two directional\
    \ input feeding schemes for eliminating transpose operation. The bank-level unit\
    \ selects whether to broadcast the same input for all MACs in S-ALUs or different\
    \ inputs for each MAC in S-ALUs. Also, decoding units in the bank-level units\
    \ generate select signals for LUT-embedded subarray. When the LUT-embedded subarrays\
    \ operate like conventional memory, the decoding units decode addresses. In contrast,\
    \ when they act as LUT, the decoding units decode data from a register in the\
    \ bank-level unit.\n- C-ALU performs accumulation and reduce-sum operations for\
    \ multiple banks. To maximize the utilization of S-ALUs, All banks in SAL-PIM\
    \ operate in parallel. Hence, the SAL-PIM architecture needs to merge each bank's\
    \ output, so C-ALU merges the outputs of banks and broadcasts the results to all\
    \ banks in the same channel. C-ALU minimizes data movement between each bank in\
    \ the same channel.\n\nAs a result, by using these three types of units, SAL-PIM\
    \ supports all operations of the transformer-decoderbased generative model, including\
    \ non-linear functions, fully-connected operations, and multi-head operations.\
    \ The detailed circuit of each unit is described in Section [4.](#page-6-0)\n\n\
    ## **3.2 Data Mapping**\n\nSAL-PIM adopts flexible data mapping schemes across\
    \ various layers of GPT to run end-to-end inference efficiently. The data mapping\
    \ of SAL-PIM considers the hardware architecture and dataflows, maximizing the\
    \ utilization of increased bandwidth. Moreover, the flexible data mapping minimizes\
    \ the data movement that occurs when the outputs are distributed back to the subarrays\
    \ for the next layer as inputs. As mentioned in Section [2,](#page-2-2) the decoder\
    \ layer of GPT consists of FFN, MHA, and layerNorms. These are decomposed into\
    \ three types of computations (i.e., non-linear function, matrix-vector operation,\
    \ and multi-head operation), where each has a different dataflow. In addition,\
    \ the SAL-PIM ar-\n\n![](_page_5_Figure_0.jpeg)\n\n<span id=\"page-5-0\"></span>Fig.\
    \ 6. Data mapping schemes for SAL-PIM. (a) Non-linear function. (b) Matrix-vector\
    \ operation. (c) Multi-head operation accumulated by subarrays. (d) Multi-head\
    \ operation accumulated by banks.\n\nchitecture supports three levels of hierarchy\
    \ that can operate in parallel (i.e., channels, banks, and subarrays), each of\
    \ which has different physical characteristics. The higher level hierarchically,\
    \ the further away from the memory cell, the higher the cost of moving data. For\
    \ example, data movement between channels is more time-consuming because data\
    \ moves between them through interconnection on the buffer die. Thus, each channel\
    \ should be mapped with independent weight, which does not require accumulation.\
    \ For the banks, the output can be merged in C-ALU, so the weight is mapped to\
    \ utilize parallelism supremely. The subarrays are tightly coupled, allowing faster\
    \ and wider data movement among the subarrays. Thus, SAL-PIM can adopt data mapping,\
    \ which performs the accumulation of partial sums among the subarrays.\n\nData\
    \ mapping schemes of the SAL-PIM architecture reflect three considerations. First,\
    \ SAL-PIM supports the data mapping scheme that minimizes data movement within\
    \ and between the computations. Since the weight data of GPT is large and must\
    \ be tiled, the additional data movement is incurred by partial sums among the\
    \ tiles, which are generated within each computation. Furthermore, data movement\
    \ between the computations is incurred since the output of each computation is\
    \ used as the input of the following computation. Second, SAL-PIM's data mapping\
    \ also maximizes bandwidth utilization for memory-bound operations. Most operations\
    \ in the generation stage have no reuse of weight at all, which can be executed\
    \ faster with higher bandwidth. The extended bandwidth from subarraylevel parallelism\
    \ is fully utilized through data mapping. Third, SAL-PIM supports the data mapping\
    \ scheme that eliminates any data movement of intermediate data for the two featured\
    \ operations of MHA, such as concatenation and transposition. Concatenation needs\
    \ frequent data movement for the concatenated matrices because it is performed\
    \ by reading all of the data and concatenating them. The data mapping removes\
    \ the data movement by sequentially mapping them to banks. Also, transposition\
    \ is time-consuming due to reshaping extensive data and requires additional buffers.\
    \ The SAL-PIM architecture eliminates transpose operation using data mapping and\
    \ input feeding schemes.\n\nFigure [6](#page-5-0) illustrates the data mapping\
    \ schemes for SAL-PIM. The parameters associated with the data mapping in SAL-PIM\
    \ are PCh, PBa, and PSub, and each refers to parallelism by channels, banks, and\
    \ S-ALUs, respectively.\n\nFirst, the data mapping scheme reduces the data movement\
    \ within and between computations. As shown in Figure [6\\(](#page-5-0)b), [6\\\
    (](#page-5-0)c), and [6\\(](#page-5-0)d), the SAL-PIM architecture is mapped column\
    \ of matrix or head of the multi-head operation to PCh, minimizing data movement\
    \ for partial sums within the computation. Furthermore, data movement caused by\
    \ the accumulation of partial sums is reduced between banks in the same channel\
    \ using C-ALU. In addition, the output of computations leads to the input of the\
    \ following computations in GPT. In particular, the non-linear functions are in\
    \ the middle of MHA and FFN, so these functions are connected to the other computations\
    \ directly. As shown in Figure [6\\(](#page-5-0)a), in the case of the non-linear\
    \ function, two data mapping schemes exist to minimize data movement when the\
    \ non-linear function continues to other computations. For example, if the matrix-vector\
    \ operation is followed by the non-linear function, the input vector is tiled\
    \ in banks and duplicated in channels. On the contrary, if the multi-head operation\
    \ is followed, the input vector is tiled both banks and channels. The mapping\
    \ schemes for non-linear functions support the same tiling shapes as the other\
    \ computations input, so data movements are eliminated. Therefore, the flexible\
    \ data mapping schemes enable the non-linear function's output to seamlessly be\
    \ used as input for the following computation.\n\nIn addition, the data mapping\
    \ scheme maximizes bandwidth utilization. In the case of the non-linear function,\
    \ the number of computations is small due to linear interpolation. So, SAL-PIM\
    \ cannot utilize the bandwidth fully, and then the data mapping scheme aims to\
    \ minimize data movement. On the other hand, in the case of matrix-vector and\
    \ multihead operations, which are memory-bounded. Therefore, these operations\
    \ utilize all three types of parallelism, as shown in Figures [6\\(](#page-5-0)b),\
    \ [6\\(](#page-5-0)c), and [6\\(](#page-5-0)d). For the matrix-vector operation,\
    \ the rows of the matrix are mapped according to PCh and PSub, and the columns\
    \ are divided into PBa. In contrast, for the multi-head operation, the heads,\
    \ which are independent of other heads, are mapped on PCh, and PBa and PSub divide\
    \ the row or column of the matrix. Both cases need accumulation between banks,\
    \ and SAL-PIM uses C-ALUs to simplify bank-level accumulation without data movement\
    \ between banks.\n\nLastly, the data mapping scheme enables two operations in\
    \ MHA for text generation: concatenation and transposition. The multi-head operation\
    \ concatenates *Ks* and *Vs* in text generation in order to project the previous\
    \ word in the generation of the next word. Therefore, the hardware must enable\
    \ supporting concatenation of *K* and *V*, and in SAL-PIM, concatenation is performed\
    \ by mapping the bank sequentially, as shown in Figure [6\\(](#page-5-0)c) and\
    \ [6\\(](#page-5-0)d). Furthermore, in the multi-head operation, two types of\
    \ matrix multiplication are needed (*Q*\xD7*K* T and *S*\xD7*V*). To accelerate\
    \ these, PIM should support transpose operation for matrix, which is time-consuming\
    \ and requires an additional buffer. In contrast, SAL-PIM supports two accumulation\
    \ directions and two input feeding methods, which eliminates the need for transpose\
    \ operations in PIM (Figure [6\\(](#page-5-0)c) and [6\\(](#page-5-0)d)).\n\n\
    #### *3.2.1 Dataflow for GPT in SAL-PIM*\n\nThese data mapping schemes reduce\
    \ data movement and enable acceleration of the overall computation of the depicted\
    \ GPT in Figure 2 using PIM. The input tokens of GPT generate an output token\
    \ through the embedding layer, positional encoding, layerNorm, MHA, FFN, and residual\
    \ addition. Firstly, the input token is translated into a vector through the embedding\
    \ operation. The embedding operation simply finds the vector corresponding to\
    \ the dictionary, which can be performed by reading the vector at the corresponding\
    \ address in DRAM. The vector is divided into banks and added with a position\
    \ vector duplicated in all channels. Then, the vector performs layerNorm. layerNorm\
    \ subtracts the mean and divides the vector by the standard deviation, followed\
    \ by the GEMV operation. The mean and standard deviation are obtained through\
    \ reduction operation in the S-ALU and C-ALU and linear interpolation to compute\
    \ the reciprocal square root. The mean and standard deviation are broadcasted\
    \ to all banks and subtracted and multiplied, respectively. The calculated vector\
    \ performs GEMV operation within its channel, and then the computed vector is\
    \ broadcasted to all channels for further computations.\n\nMoving on to the MHA,\
    \ it consists of *Q*, *K*, *V* generation, *Q*\xD7*K* T , softmax, *S*\xD7*V*,\
    \ and a GEMV operation. Firstly, *Q*, *K*, and *V* generations perform GEMV operation\
    \ three times each. At this stage, the heads are divided on each channel, allowing\
    \ independent operations for each head. *Q* is duplicated to all banks, and *K*\
    \ and *V* values are sequentially mapped to the banks and concatenated. Then,\
    \ *Q*\xD7*K* T is performed as shown in Figure 6(d), and the results are stored\
    \ back in each bank. The computed data is linearly interpolated for exponential\
    \ calculation and summed in the C-ALU for softmax operation. The sum is broadcasted\
    \ to all banks, where it performs linear interpolation for the reciprocal operation\
    \ and is multiplied to compute *S*. Finally, *S* is multiplied by *V* in each\
    \ bank as depicted in Figure 6(c), and the output is accumulated across all banks\
    \ in C-ALU. Then, the GEMV operation is performed to complete MHA.\n\nThe output\
    \ of the MHA is reshaped into a single channel, and layerNorm is applied again\
    \ after residual addition. The resulting vector is then broadcasted across all\
    \ channels, similar to the previous stage. The FFN stage consists of two GEMV\
    \ operations and an activation function(GELU). The FFN stage consists of two GEMV\
    \ operations and an activation function called GELU. The two GEMV operations are\
    \ performed as shown in Figure 6(b), and the GELU activation is applied using\
    \ linear interpolation. The resulting vector from the FFN stage undergoes another\
    \ residual addition. All decoder layers in GPT are configured in the same manner,\
    \ so SAL-PIM iterates these operations. As a result, SAL-PIM can minimize data\
    \ movement and maximize parallelism in the GPT inference.\n\n# <span id=\"page-6-0\"\
    ></span>**4 IN-MEMORY CIRCUIT DESIGN**\n\nSAL-PIM has three compute units (S-ALUs,\
    \ Bank-level units, and C-ALUs) and optimized subarrays (LUTembedded subarrays).\
    \ This chapter describes detailed circuits and operation flow.\n\n![](_page_6_Figure_6.jpeg)\n\
    \nS-ALU **-** Regs\n\nBank-level Register\n\n**>>**\n\n<span id=\"page-6-1\"></span>Fig.\
    \ 7. Circuit design of S-ALU.\n\n**Max**\n\n*32,768 rows = 64 subarrays*\n\n##\
    \ **4.1 Subarray-level ALU (S-ALU)**\n\nAs aforementioned in Section [3,](#page-3-2)\
    \ S-ALU performs principal arithmetic operations of SAL-PIM with subarray-level\
    \ parallelism. Figure [7](#page-6-1) shows the circuit of S-ALU. S-ALU consists\
    \ of 16-bit fixed-point MAC units, 16\xD732-bit registers, and right shifters.\
    \ The S-ALU's operation proceeds in three steps. First, read data from memory\
    \ to the bank-level unit via GBLs, and the data is used as input for S-ALUs. Second,\
    \ S-ALU calculates the data read from memory and the input broadcasted from the\
    \ bank-level unit. The results are stored and accumulated in S-ALU's registers.\
    \ Lastly, the data of the S-ALU's registers are written in the memory. When writing\
    \ the data from S-ALU's registers, the results stored in the registers are 32-bit\
    \ precision, but the GBL has half of the precision. Therefore, the results are\
    \ shifted and truncated by fraction bit using shifters. Then, the tri-state buffer\
    \ connects the register's output and the GBLs to write the result to the memory.\n\
    \nAs shown in the table of Figure [7,](#page-6-1) S-ALU's MAC supports four operations:\
    \ element-wise addition and multiplication, MAC, and max. The max operation is\
    \ used for linear interpolation of exponential in softmax. The exponential function\
    \ has a wide range of data; thus, it is computed using linear interpolation after\
    \ subtracting by the maximum. The MAC units enable the computation of three types\
    \ of operands from memory (GBL), a bank-level register in the bank-level unit,\
    \ and registers in S-ALU. The primary operand of S-ALU is data from memory, and\
    \ GBL is connected during a single operation to receive data from all subarrays.\
    \ Therefore, one S-ALU is performed at a time to compute linear interpolation\
    \ using LUT-embedded subarrays. On the contrary, when S-ALUs compute simultaneously\
    \ in the case of the matrix-vector operation or the multi-head operation, the\
    \ control signal cuts off the GBLs connected with other S-ALUs. So the S-ALUs\
    \ operate individually read memory at a time.\n\nIn general, most transformer-based\
    \ models use 32-bit floating-point precision. However, in the previous work [\\\
    [24\\]](#page-12-22), the transformer models are sufficient with 8-bit precision.\
    \ Furthermore, the high bit precision of the register enables minimizing the data\
    \ loss of accumulation. In order to verify accuracy when fixed-point precision\
    \ is used, we evaluated the accuracy of the lambada dataset [\\[25\\]](#page-12-23)\
    \ using\n\n![](_page_7_Figure_0.jpeg)\n\n<span id=\"page-7-0\"></span>Fig. 8.\
    \ Circuit of LUT-embedded subarray and bank-level unit.\n\n16-bit fixed-point\
    \ precision. As a result, the accuracy only dropped about 2.8% on the GPT-2 medium\
    \ model without other quantization schemes.\n\nThe subarray-level parallelism\
    \ enables the PIM architecture to achieve outstanding performance, but it has\
    \ a drawback for the area overhead. As PSub increases, the area overhead increases\
    \ proportionally. So, SAL-PIM uses shared MACs in S-ALU, which leverage faster\
    \ computation units than memory read. Several recent studies, including [\\[26\\\
    ]](#page-12-24), [\\[27\\]](#page-12-25), have utilized MAC units operating at\
    \ a frequency of 1GHz in logic technology. Moreover, in the recent PIM research\
    \ [\\[28\\]](#page-12-26) uses 1GHz ALU by utilizing faster transistors in DRAM\
    \ peripheral area. On the other hand, ALUs in recent PIM works mostly can be only\
    \ utilized at 250MHz. In HBM2, the clock frequency is 1GHz, and HBM2 reads data\
    \ on tccds = 2tck (500MHz) for maximum bandwidth with bank interleaving. However,\
    \ in PIM, the all-bank mode operates more slowly than the bank-interleaving case\
    \ because the same banks are consecutively accessed. Accordingly, the memory reads\
    \ or writes data on tccdl = 4tck (250MHz), whereas the frequency limitation of\
    \ ALU is higher than the memory read or write speed. Therefore, by computing part\
    \ of the data several times while memory moves to the next address, the total\
    \ number of MACs is reduced, and the area is optimized. For example, 16\xD716-bit\
    \ inputs are read from memory at one read, and the 16\xD716-bit MACs are required.\
    \ However, 8\xD716-bit MACs can compute the inputs in two computations thanks\
    \ to a faster speed (500MHz). We implemented shared MAC using a standard cell\
    \ library to verify its feasibility, achieving a clock frequency of over 800MHz.\
    \ Considering the 22% performance degradation between DRAM and logic technology\
    \ [\\[29\\]](#page-12-27), it is confirmed that the shared MACs are feasible.\
    \ As a result, when we implemented the S-ALU, the 8 MACs on 500MHz were about\
    \ 30% smaller than the 16 MACs on 250MHz.\n\n#### **4.2 LUT-embedded Subarray**\n\
    \nSAL-PIM uses LUT-based linear interpolation to compute complex functions. However,\
    \ the original DRAM is unsuitable for performing LUT operations. In the DRAM,\
    \ a subarray is divided into several MATs [\\[30\\]](#page-12-28). All MATs of\
    \ the subarray are operated simultaneously, and a columnselect signal selects\
    \ which column in the MAT is connected to the GBLs. However, LUT needs different\
    \ addresses for the various data. Suppose the original subarray is used as\n\n\
    ![](_page_7_Figure_6.jpeg)\n\n<span id=\"page-7-1\"></span>Fig. 9. Operation flow\
    \ of LUT-embedded subarray.\n\nLUT. In that case, it is necessary to repeatedly\
    \ approach the subarray as many times as the number of data to find the corresponding\
    \ value in LUT. These are inefficient tasks for DRAM, and these are time-consuming\
    \ due to a lot of activation and precharge. Therefore, an LUT-embedded subarray\
    \ is optimized for LUT operation in DRAM.\n\nThe circuit of the LUT-embedded subarray\
    \ is shown in Figure [8.](#page-7-0) The LUT-embedded subarray is almost identical\
    \ to the original subarray except for the path to the columnselect signal. The\
    \ LUT-embedded subarray has different column-select signal paths for each MAT,\
    \ and each MAT receives a different column-select signal. The column-select signals\
    \ are generated from a column address or data in the bank-level register. However,\
    \ although MATs operate independently on column addresses, it is not a sufficient\
    \ solution if one row cannot store all of LUT due to the many sections for linear\
    \ interpolation. Accordingly, the LUT-embedded subarrays are activated together,\
    \ and an LUT selector determines which LUT-embedded subarray is connected with\
    \ GBLs. As a result, a number of LUT-embedded subarrays in the bank and the number\
    \ of column addresses enable using many sections for linear interpolation.\n\n\
    Figure [9](#page-7-1) shows the operation flow of the LUT-embedded subarray. The\
    \ timing parameter is the same as the parameters in the original DRAM. The operation\
    \ flow consists of four parts. First, activate all rows for source, destination,\
    \ slope (W), and intercept (B). Thanks to existing research [\\[23\\]](#page-12-21),\
    \ it is possible to activate simultaneously for different subarrays. Second, read\
    \ source data from the memory to the bank-level register. The data is used for\
    \ generating columnselect signals and LUT-select signals. Third, S-ALU multiplies\
    \ the slopes and adds intercepts for linear interpolation. The data read from\
    \ the LUT-embedded subarray moves through GBL and is computed in an S-ALU. Lastly,\
    \ the data of register in S-ALU, results of linear interpolation, is written to\
    \ memory cells. At the end of the operation, all subarrays are precharged. As\
    \ shown in Figure [9,](#page-7-1) the activation and precharge commands are issued\
    \ once at the start and the end, respectively. Consequently, the LUT-embedded\
    \ subarray enables linear interpolation for the entire data in bank-level register\
    \ at a time, which can be up to 16 times faster than the original DRAM, which\
    \ performs linear interpolation done one by one.\n\n#### **4.3 Bank-level Unit**\n\
    \nThe bank-level unit is responsible for generating select signals to perform\
    \ LUT-embedded subarrays and broadcasting input data to S-ALUs. The bank-level\
    \ unit consists of a banklevel register and decoding units. The bank-level register\n\
    \n![](_page_8_Figure_0.jpeg)\n\n<span id=\"page-8-0\"></span>Fig. 10. Circuit\
    \ design of channel-level ALU.\n\nbroadcasts the input to S-ALUs in the same bank\
    \ with two methods. The first is each data of the bank-level register goes to\
    \ each MAC of S-ALU. It is for element-wise computations. The other method is\
    \ only single data of the bank-level register is broadcasted to all of the MACs\
    \ in the bank. It is used in MAC operation and enables each MAC to accumulate\
    \ the output.\n\nFurthermore, data in the bank-level register is used as addresses\
    \ of LUT-embedded subarray. Figure [8](#page-7-0) also shows the circuit of the\
    \ bank-level unit. The decoding units in the bank-level unit have two types: column\
    \ decoder and subsel decoder. Each is responsible for generating column-select\
    \ signals and LUT-select signals, respectively. Making the select signals through\
    \ decoding in an appropriate bit position for fixed-point bit precision is possible.\
    \ The bit position means linear interpolation region. If the slope and intercept\
    \ are generated in intervals from -4 to 4, the decoding units decode addresses\
    \ based on the interval. Therefore, the right shifters select the bit position\
    \ since each function's proper linear interpolation range differs.\n\n#### **4.4\
    \ Channel-level ALU (C-ALU)**\n\nC-ALU supports merging operations for multiple\
    \ banks in the same channel. Because the SAL-PIM architecture operates multiple\
    \ banks simultaneously, the merging procedures for banks should be performed.\
    \ When the data is moved from PIM to the host, the energy caused by data movement\
    \ is consumed, and PIM should wait for the host's work. In addition, since the\
    \ summed result is used as input for the following operation, the result returns\
    \ immediately from the host to the PIM and is broadcasted to the banks. These\
    \ operations, such as the accumulation of banks and reduce-sum operation, are\
    \ performed in C-ALU, which is implemented in the buffer die of HBM2 for each\
    \ channel. At the buffer die, although there is no bandwidth, it is possible to\
    \ eliminate data movement by performing simple addition. Likewise, if bank-level\
    \ adder trees exist, PIM can reduce the execution time for merging operations.\
    \ However, since accumulations by banks are small in GEMV, it is wasteful to implement\
    \ it at the bank, considering the area overhead.\n\nFigure [10](#page-8-0) illustrates\
    \ the circuit of C-ALU, which consists of two channel vector registers (16\xD7\
    16-bit), two scalar registers (16-bit), and configurable adders. The configurable\
    \ adders have sixteen adders, which act as two types of modules, either an accumulator\
    \ or an adder tree, depending on the command. In the case of accumulation, the\
    \ configurable adders accumulate memory output to the channel vector register.\
    \ The accumulated data is broadcasted to memory or summed into one data. In the\
    \ case of reduce-sum operation, the configurable adders operate as an adder tree\
    \ to compute summation about the channel vector register. The summed result is\
    \ stored in the channel scalar register. The\n\nTABLE 2 SAL-PIM configuration\n\
    \n<span id=\"page-8-1\"></span>\n\n| HBM2 Configuration | Channels/die = 8 (pch/die\
    \ = 16), Banks/channel = 32 (Banks/pch = 16),<br>Subarrays/bank = 64, Row/subarray\
    \ = 512,<br>Row Size = 1KB, MAT Size = 512x512, DQ Size = 128-bit/channel |  |\
    \  |  |  |\n|--------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--|--|--|--|\n\
    |                    | Timing parameters(ns) BL = 4, tRC = 45, tRCD = 16, tRAS\
    \ = 29, tCL = 16, tRRD = 2, tCCDS = 2, tCCDL = 4                             \
    \                                                |  |  |  |  |\n| LUT-subarray\
    \       | Number of LUT-subarray (Slope & Intercept) = 4,<br>Number of Sections\
    \ for Linear Interpolation = 64                                              \
    \                                  |  |  |  |  |\n| S-ALU              | PSub\
    \ = 4, MACs/S-ALU = 8, Register Size = 16\xD732-bit                          \
    \                                                                            \
    \                          |  |  |  |  |\n| Bank-level Unit    | Column Decoder\
    \ = 16\xD75 to 32 decoders,<br>Sub-sel Decoder = 16\xD71 to 2 decoders,<br>Bank-level\
    \ register = 16\xD716-bit                                                    \
    \              |  |  |  |  |\n| C-ALU              | Channel-vector register =\
    \ 16\xD716-bit<br>Channel scalar register = 16-bit<br>Flexible adders = 16\xD7\
    16-bit adders                                                                \
    \      |  |  |  |  |\n\nchannel scalar register's data is broadcasted to memory\
    \ for layerNorm or softmax.\n\n# **5 EVALUATION**\n\n#### **5.1 Methodology &\
    \ Configuration**\n\nWe evaluated the SAL-PIM architecture on the simulator modified\
    \ from the latest DRAM simulator [\\[18\\]](#page-12-16). The SAL-PIM is based\
    \ on conventional HBM2 [\\[19\\]](#page-12-17), and the configuration with timing\
    \ parameters [\\[31\\]](#page-12-29) is shown in Table [2.](#page-8-1) As shown\
    \ in Table [2,](#page-8-1) we evaluated the SAL-PIM architecture of PSub=1, 2,\
    \ and 4, so the maximum bandwidth is four times larger than bank-level PIM when\
    \ PSub is 4. In addition, the SAL-PIM architecture applied linear interpolation\
    \ with 64 sections on GELU, *exp*, *sqrt*, and reciprocal operations. As aforementioned,\
    \ when the number of sections is larger than 32, it has no accuracy drop by linear\
    \ interpolation.\n\nThe transformer-decoder-based generative model we used for\
    \ the evaluation is the GPT-2 medium model (with 345 million parameters) [\\[7\\\
    ]](#page-12-5), which computes the vector with a size of 1,024 and has 24 decoder\
    \ layers. The model's overall structure is the same as in Figure [2](#page-2-0)\
    \ of Section [2.](#page-2-2) We compared the SAL-PIM architecture to the serverlevel\
    \ GPU, Nvidia Titan RTX [\\[32\\]](#page-12-30). The GPU used 24GB GDDR6 memory\
    \ with 1.77GHz clock frequency. The maximum available bandwidth is 672GB/s, 2.63\xD7\
    \ larger than the maximum bandwidth of HBM2. The GPU executed the GPT-2 medium\
    \ model from FasterTransformer Framework [\\[33\\]](#page-12-31) for comparison\
    \ baseline.\n\n## **5.2 Area & Power**\n\nFirstly, we implemented units of the\
    \ SAL-PIM architecture to verify its feasibility. These units are implemented\
    \ using standard cells of the TSMC 28-nm technology with Design Compiler. Since\
    \ the previous HBM2-PIM [\\[20\\]](#page-12-18) was fabricated on the Samsung\
    \ 20-nm DRAM technology, we scaled the implemented 28-nm area to the 20-nm area\
    \ on DRAM technology. Prior research [\\[11\\]](#page-12-9), [\\[14\\]](#page-12-12),\
    \ [\\[15\\]](#page-12-13), [\\[29\\]](#page-12-27) has shown that the area difference\
    \ between DRAM technology and CMOS logic technology is approximately 1.8 times.\
    \ To be conservative, we further increased this area overhead by 3.6 times, doubling\
    \ the initial scaling factor. Additionally, we measured the power consumption\
    \ of the units using the PrimePower tool [\\[34\\]](#page-12-32).\n\n![](_page_9_Figure_0.jpeg)\n\
    \n<span id=\"page-9-1\"></span>Fig. 11. Speedup of SAL-PIM compared to GPU for\
    \ text generation by input and output sizes.\n\nTABLE 3 Summarization of area\
    \ and power consumption of units in the SAL-PIM architecture\n\n<span id=\"page-9-0\"\
    ></span>\n\n|                 | Area/Unit | Area/Channel         | Power Consumption\
    \  |\n|-----------------|-----------|----------------------|--------------------|\n\
    | HBM2 8GB        | -         | 53.15mm2             | 60W (power budget) |\n\
    | S-ALU           | 18,744um2 | 2.40mm2 (128 S-ALUs) | 5.298mW            |\n\
    | Bank-level Unit | 4,847um2  | 0.16mm2 (32 banks)   | 0.926mW            |\n\
    | C-ALU           | 19,126um2 | 0.02mm2 (1 C-ALUs)   | 2.749mW            |\n\n\
    ![](_page_9_Figure_4.jpeg)\n\nTable [3](#page-9-0) summarizes the area and power\
    \ consumption of each unit. Accordingly, When the SAL-PIM has PSub=4, the number\
    \ of S-ALU is 64 in a channel, and the area overhead is 4.81% compared to conventional\
    \ HBM2 [\\[11\\]](#page-12-9). The overhead is far below than 25% threshold mentioned\
    \ in previous work [\\[13\\]](#page-12-11). Accordingly, the area overhead is\
    \ acceptable, and SAL-PIM does not need to sacrifice memory capacity. Furthermore,\
    \ we checked the power consumption of SAL-PIM architecture. When the power consumption\
    \ is maximum, all of S-ALU performs MAC operations simultaneously, it is only\
    \ 9.04% of the total power budget of HBM2 [\\[31\\]](#page-12-29). The results\
    \ indicate that the SAL-PIM architecture is feasible with the permissible area\
    \ and power overheads.\n\n#### **5.3 Comparison with GPU**\n\nWe evaluated the\
    \ performance of the SAL-PIM architecture for text generation by various input\
    \ and output sizes. As aforementioned in Section [2,](#page-2-2) when the input\
    \ size is large, the number of the input vectors in the summarization stage is\
    \ increased, and when the output size is large, the number of token generations\
    \ in the generation stage is increased. Figure [11](#page-9-1) shows the speedup\
    \ compared to the GPU by input and output sizes. The input lengths are 32, 64,\
    \ and 128 tokens, and the output length is from 1 to 256 tokens. The input and\
    \ output sizes were determined by the typical ranges of user requests based on\
    \ previous work [\\[35\\]](#page-12-33). In addition, because the SAL-PIM architecture\
    \ supports GPT end-to-end, the overhead from data movement is considered on the\
    \ simulator. Therefore, comparing the SAL-PIM architecture using the software\
    \ simulator with GPU is reliable.\n\nAs shown in Figure [11,](#page-9-1) in the\
    \ case of increasing the input size, The speedup decreases. The decrement in performance\
    \ is because the increase in summarization means an increase in operations that\
    \ can be performed in parallel, and the high-performance GPU performs operations\
    \ much faster.\n\nHowever, SAL-PIM operates on a slower frequency, and the number\
    \ of ALU is far less than the GPU.\n\n<span id=\"page-9-2\"></span>Fig. 12. Speedup\
    \ of SAL-PIM for GEMV compared to bank-level PIM.\n\nOn the contrary, in the case\
    \ of increasing the output size, the speedup tends to grow for the same input\
    \ size. The improvement is because the time to execute the model once for one\
    \ input vector is much shorter than that of the GPU. Hence, the longer the generation\
    \ stage, the higher the speedup of the SAL-PIM. As a result, when PSub is 4, the\
    \ maximum speedup of SAL-PIM is 4.72\xD7 compared to the GPU when the input size\
    \ is 32, and the output size is 128. Then, the average speedup is 1.83\xD7 compared\
    \ to the GPU. This result indicates the maximum speedup lower than the gain obtained\
    \ from subarray-level parallelism. This means means that even though SAL-PIM supports\
    \ memory-bound operation of generation stage much faster than the GPU, the summarization\
    \ stage and compute-bound operations degrade overall performance. However, this\
    \ performance increase in the generation stage is meaningful because the output\
    \ tokens tend to be much longer in general workloads of text generation application.\n\
    \n## **5.4 Comparison with Bank-level PIM**\n\nSAL-PIM architecture has a much\
    \ higher maximum bandwidth compared to Bank-level PIM, so we evaluated the SAL-PIM\
    \ architecture compared to the bank-level PIM architecture to verify whether this\
    \ high bandwidth is being fully utilized is necessary. The bank-level PIM architecture\
    \ is based on Newton [\\[13\\]](#page-12-11), which has multipliers and adder\
    \ trees in each bank. We modified the SAL-PIM simulator to the Bank-level PIM\
    \ simulator with the same configuration as Table [2.](#page-8-1) Figure [12](#page-9-2)\
    \ shows the SAL-PIM's speedup for GEMV operation compared to the bank-level PIM.\n\
    \n![](_page_10_Figure_0.jpeg)\n\n<span id=\"page-10-0\"></span>Fig. 13. Execution\
    \ time comparison for the LUT operation between LUTembedded subarray and two other\
    \ linear interpolation cases.\n\nThe maximum bandwidth of SAL-PIM is 4\xD7 larger\
    \ than bank-level PIM because of PSub=4. However, in the case of the small size\
    \ of the input and output vector, the minimum speedup is only 1.75\xD7, as shown\
    \ in Figure [12.](#page-9-2) The degradation is because accumulation is needed\
    \ for the SAL-PIM by mapping scheme, but bank-level PIM does not require the bank-level\
    \ data movement. Hence, with larger input and output vector sizes, the portion\
    \ of data movement is smaller, and the speedup is closer to maximum gain. In the\
    \ GPT-2 medium model, the vector length is only 1,024, but the latest transformer-decoder-based\
    \ generative model [\\[8\\]](#page-12-6) has a longer vector length of up to 12,288.\
    \ Therefore, acceleration through subarray-level parallelism is required for a\
    \ higher performance increase for the large-size model.\n\n# **6 ADDITIONAL ANALYSIS**\n\
    \n## **6.1 LUT-embedded subarray**\n\nThe SAL-PIM architecture uses the linear\
    \ interpolation method to compute the non-linear functions. Furthermore, SAL-PIM\
    \ has the LUT-embedded subarray to optimize LUT operations on DRAM. However, the\
    \ LUT operation can be performed through other methods in the original DRAM subarrays.\
    \ So. we evaluated the LUT-embedded subarray between the other two cases. Case\
    \ 1 is *Scan*, which scans all regions of LUT storing slope and intercept. For\
    \ example, when the number of sections is 64, the slopes and intercepts are stored\
    \ in 128 addresses. In that case, starting from the beginning, read to the end\
    \ and find the slope and intercept corresponding to the section. Case 2 is *Select*.\
    \ This is the case that the LUT decodes the corresponding address from the first\
    \ data to the last data to which the LUT is applied and finds the slopes and intercepts\
    \ sequentially.\n\nFigure [13](#page-10-0) shows the Execution time comparison\
    \ of the LUT-embedded subarray and two linear interpolation cases without the\
    \ LUT-embedded subarray. Case 1 shows the worst execution time because the size\
    \ of the bank-level register is limited. Furthermore, in this case, the number\
    \ of sections is only 64. For a large number of sections, Case 1 is worsened due\
    \ to the large region to scan. In Case 2, the LUT operation is performed on only\
    \ one data at a time in a bank. Therefore, In the case of large vectors, more\
    \ speedup can be expected for the LUT-embedded subarray. Accordingly, the LUT-embedded\
    \ subarray shows a 3.57\xD7 speedup in a case with a vector size of 16,384, as\
    \ shown in Figure [13.](#page-10-0)\n\n![](_page_10_Figure_7.jpeg)\n\n<span id=\"\
    page-10-1\"></span>Fig. 14. The execution time and average bandwidth by subarray-level\
    \ parallelism on text generation.\n\n![](_page_10_Figure_9.jpeg)\n\n<span id=\"\
    page-10-2\"></span>Fig. 15. The power consumption for subarray-level parallelism\
    \ in 32 token generations of the GPT-2 medium model.\n\n#### **6.2 Subarray-level\
    \ Parallelism**\n\nThe subarray-level parallelism enables to use of an enormous\
    \ bandwidth maximum of 8TB/s when PSub is 4. However, if the higher bandwidth\
    \ is not fully used, there is no advantage for the area and power overheads caused\
    \ by S-ALUs. Therefore, we evaluated the SAL-PIM architecture on various PSub.\
    \ Figure [14](#page-10-1) shows the execution time and average bandwidth by the\
    \ number of subarray-level parallelisms on text generation.\n\nThe SAL-PIM architecture\
    \ uses subarray-level parallelism for matrix-vector and multi-head operations,\
    \ occupying about 60% of the total execution time in text generation applications.\
    \ Hence, the SAL-PIM architecture achieves a 2.11\xD7 speedup when PSub is 4 compared\
    \ to when PSub is 1, as shown in Figure [14,](#page-10-1) showing utilizes higher\
    \ bandwidth efficiently for memory-bound operations. Similarly, the average bandwidth\
    \ of the case of PSub is 4 is also about two times larger than the case of PSub\
    \ is 1. Furthermore, considering there are larger models than the GPT-2 medium\
    \ model, the higher bandwidth can be used efficiently to accelerate memory-bound\
    \ operations.\n\nSubarray-level parallelism offers increased bandwidth but consumes\
    \ more energy. Operating multiple banks and subarrays simultaneously result in\
    \ higher energy consumption compared to conventional DRAM operations. However,\
    \ it provides advantages for energy due to the shorter data movement distances.\
    \ Figure [15](#page-10-2) illustrates the power consumption of SAL-PIM for various\
    \ values of PSub.\n\nIn our evaluation, we conducted 32 token generations using\
    \ the GPT-2 medium model. The energy consumption was assigned as follows [\\[31\\\
    ]](#page-12-29): eact = 909pJ, epre\u2212gsa = 1.51pJ/bit, epost\u2212gsa = 1.17pJ/bit,\
    \ and eio = 0.80pJ/bit. We assumed that 26% of the total HBM poewr budget is allocated\
    \ for refresh operations [\\[36\\]](#page-12-34). As depicted in Figure [15,](#page-10-2)\
    \ when PSub is 1 or 2, the power consumption remains significantly lower than\
    \ the power budget, while for PSub is 4, it exceeds the power budget by 24.0%.\
    \ Despite this, considering the high power usage expected for computations in\
    \ CPUs or GPUs, SAL-PIM can still be considered sufficiently power-efficient for\
    \ transformer-decoder-based text generation. Furthermore, This result assumes\
    \ that the ALUs are always operating. Therefore, there is room for further power\
    \ savings through increasing bulk capacitance and optimization techniques such\
    \ as clock gating or power gating.\n\n## **6.3 Future Work**\n\nWhile SAL-PIM\
    \ presents a novel architecture for efficient end-to-end text generation within\
    \ the PIM paradigm, leveraging its inherent high bandwidth, further investigation\
    \ is necessary to address two key challenges. These areas represent valuable future\
    \ directions for optimizing SAL-PIM's performance and scalability.\n\nThe first\
    \ challenge concerns the performance bottleneck by the summarization stage. The\
    \ compute-bound nature of summarization significantly impedes SAL-PIM's overall\
    \ speedup, diminishing the potential gains from its high bandwidth advantage.\
    \ To mitigate this, future research should explore mapping and operation strategies\
    \ that offload the summarization stage to dedicated accelerators like GPUs or\
    \ NPUs while reserving PIM for the generation stage. This heterogeneous execution\
    \ scheme could potentially unlock the full performance potential of SAL-PIM.\n\
    \nThe second challenge stems from the ever-growing size of modern LLMs, which\
    \ often exceed the capacity of single PIM units. Consequently, SAL-PIM requires\
    \ a new approach to accommodate these expansive models. Two promising avenues\
    \ for future research lie in exploiting parallelism: Intra-PIM parallelism through\
    \ pipelined weight write, read, and computation operations and Inter-PIM parallelism\
    \ via synchronization and workload distribution across multiple PIM instances.\
    \ Additionally, recent research [\\[37\\]](#page-12-35) has proposed leveraging\
    \ the inherent bias in output tokens to selectively load weights, minimizing memory\
    \ footprint. Therefore, sparsity-aware data mapping and handling is another intriguing\
    \ direction for PIM research.\n\nBy addressing these challenges and actively pursuing\
    \ the outlined future work, SAL-PIM holds immense potential as a groundbreaking\
    \ architecture for efficient and scalable endto-end text generation within the\
    \ PIM domain.\n\n# **7 RELATED WORKS**\n\n#### **7.1 Accelerators for Transformer\
    \ models**\n\nThere are previous works for accelerating transformer models. SpAtten\
    \ [\\[26\\]](#page-12-24) accelerates the attention through sparsity handling\
    \ using cascade token and head pruning. Also, ELSA [\\[27\\]](#page-12-25) suggests\
    \ the architecture with hardware-software co-design for self-attention. These\
    \ achieve higher performance improvements than GPU but only accelerate attention\
    \ in the model. DFX [\\[35\\]](#page-12-33) proposes the architecture for accelerating\
    \ end-to-end text generation on multi-FPGA. It was actually implemented and verified\
    \ using FPGAs and showed a higher performance increase in the generation stage\
    \ compared to GPU. However, DFX uses many HBMs in FPGAs, so SAL-PIM is a promising\
    \ solution that uses high bandwidth with fewer HBMs.\n\n## **7.2 PIM using Look-up-table**\n\
    \nThere are a few architectures PIM using LUT. Lacc [\\[38\\]](#page-12-36), pluto\
    \ [\\[39\\]](#page-12-37), and LT-PIM [\\[40\\]](#page-12-38) use DRAM cells as\
    \ LUT and suggest novel circuit designs for LUT operation. Accordingly, these\
    \ show notable performance for massive computations. However, there is a challenge\
    \ to calculate using only the LUT. In order to scan cases in LUT, the latency\
    \ increases exponentially in the case of higher bit-precision. In addition, most\
    \ machine learning uses a higher bit precision of 16 bit or more. Therefore, SAL-PIM\
    \ uses LUT-based linear interpolation to support higher bit precision. AiM [\\\
    [21\\]](#page-12-19) also utilizes LUT-based linear interpolation to support various\
    \ activation functions. However, it is limited to supporting activation functions\
    \ and is only applied to the result data of a bank-level adder tree. On the other\
    \ hand, SAL-PIM supports a wide range of non-linear functions and utilizes LUT-embedded\
    \ subarrays to apply LUT-based linear interpolation to multiple data at once.\n\
    \n# **8 CONCLUSION**\n\nWith the emergence of the transformer, the performance\
    \ of machine learning has increased faster in several applications. However, as\
    \ larger pretrained transformer-based models have been developed, the overall\
    \ execution time has slowed. To address this issue, We propose the SAL-PIM architecture,\
    \ the first PIM architecture to accelerate the endto-end transformer-decoder-based\
    \ generative model. SAL-PIM uses S-ALU to accelerate memory-bound operation by\
    \ utilizing subarray-level parallelism with the optimized mapping scheme. Moreover,\
    \ it optimizes the area overhead of S-ALU by sharing MAC units leveraging the\
    \ slow clock frequency of commands for the same bank. C-ALU is integrated on the\
    \ buffer die in HBM2 and performs accumulation and reduce-sum operations for multiple\
    \ banks. Furthermore, SAL-PIM adopts linear interpolation to compute complex non-linear\
    \ functions to support end-to-end inference. Then, a LUT-embedded subarray is\
    \ suggested to optimize LUT operation in DRAM.\n\nWe have implemented the proposed\
    \ logic units of SAL-PIM architecture in a 28-nm technology to verify its feasibility.\
    \ Also, we built the SAL-PIM simulator based on Ramulator to evaluate the architecture.\
    \ As a result, when the input size is from 32 to 128 and the output size is from\
    \ 1 to 256, SAL-PIM achieves a maximum of 4.72\xD7 speedup and an average of 1.83\xD7\
    \ speedup for the text generation based on the GPT-2 medium model compared to\
    \ the GPU.\n\n# **REFERENCES**\n\n<span id=\"page-11-0\"></span>[1] A. Vaswani,\
    \ N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \u0141. Kaiser,\
    \ and I. Polosukhin, \"Attention is all you need,\" *Advances in neural information\
    \ processing systems*, vol. 30, 2017.\n\n- <span id=\"page-12-0\"></span>[2] A.\
    \ Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner,\
    \ M. Dehghani, M. Minderer, G. Heigold, S. Gelly *et al.*, \"An image is worth\
    \ 16x16 words: Transformers for image recognition at scale,\" *arXiv preprint\
    \ arXiv:2010.11929*, 2020.\n- <span id=\"page-12-1\"></span>[3] N. Carion, F.\
    \ Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S. Zagoruyko, \"End-to-end\
    \ object detection with transformers,\" in *Computer Vision\u2013ECCV 2020: 16th\
    \ European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part\
    \ I*, 2020, pp. 213\u2013229.\n- <span id=\"page-12-2\"></span>[4] J. Schulman,\
    \ B. Zoph, C. Kim, J. Hilton, J. Menick, J. Weng, J. Uribe, L. Fedus, L. Metz,\
    \ M. Pokorny *et al.*, \"Chatgpt: Optimizing language models for dialogue,\" 2022.\n\
    - <span id=\"page-12-3\"></span>[5] \"A robot wrote this entire article. are you\
    \ scared yet, human? gpt-3,\" Sep 2020. [Online]. Available: [https://www.theguardian.](https://www.theguardian.com/commentisfree/2020/sep/08/robot-wrote-this-article-gpt-3)\
    \ [com/commentisfree/2020/sep/08/robot-wrote-this-article-gpt-3](https://www.theguardian.com/commentisfree/2020/sep/08/robot-wrote-this-article-gpt-3)\n\
    - <span id=\"page-12-4\"></span>[6] R. Sanchis, O. Garc \xB4 \xB4\u0131a-Perales,\
    \ F. Fraile, and R. Poler, \"Low-code as enabler of digital transformation in\
    \ manufacturing industry,\" *Applied Sciences*, vol. 10, no. 1, p. 12, 2019.\n\
    - <span id=\"page-12-5\"></span>[7] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei,\
    \ I. Sutskever *et al.*, \"Language models are unsupervised multitask learners,\"\
    \ *OpenAI blog*, vol. 1, no. 8, p. 9, 2019.\n- <span id=\"page-12-6\"></span>[8]\
    \ T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan,\
    \ P. Shyam, G. Sastry, A. Askell *et al.*, \"Language models are few-shot learners,\"\
    \ *Advances in neural information processing systems*, vol. 33, pp. 1877\u2013\
    1901, 2020.\n- <span id=\"page-12-7\"></span>[9] V. Seshadri, D. Lee, T. Mullins,\
    \ H. Hassan, A. Boroumand, J. Kim, M. A. Kozuch, O. Mutlu, P. B. Gibbons, and\
    \ T. C. Mowry, \"Ambit: In-memory accelerator for bulk bitwise operations using\
    \ commodity dram technology,\" in *2017 50th Annual IEEE/ACM International Symposium\
    \ on Microarchitecture (MICRO)*. IEEE, 2017, pp. 273\u2013287.\n- <span id=\"\
    page-12-8\"></span>[10] F. Gao, G. Tziantzioulis, and D. Wentzlaff, \"Computedram:\
    \ Inmemory compute using off-the-shelf drams,\" in *Proceedings of the 52nd annual\
    \ IEEE/ACM international symposium on microarchitecture*, 2019, pp. 100\u2013\
    113.\n- <span id=\"page-12-9\"></span>[11] M. Zhou, W. Xu, J. Kang, and T. Rosing,\
    \ \"Transpim: A memorybased acceleration via software-hardware co-design for transformer,\"\
    \ in *2022 IEEE International Symposium on High-Performance Computer Architecture\
    \ (HPCA)*. IEEE, 2022, pp. 1071\u20131085.\n- <span id=\"page-12-10\"></span>[12]\
    \ S. Lee, S.-h. Kang, J. Lee, H. Kim, E. Lee, S. Seo, H. Yoon, S. Lee, K. Lim,\
    \ H. Shin *et al.*, \"Hardware architecture and software stack for pim based on\
    \ commercial dram technology: Industrial product,\" in *2021 ACM/IEEE 48th Annual\
    \ International Symposium on Computer Architecture (ISCA)*. IEEE, 2021, pp. 43\u2013\
    56.\n- <span id=\"page-12-11\"></span>[13] M. He, C. Song, I. Kim, C. Jeong, S.\
    \ Kim, I. Park, M. Thottethodi, and T. Vijaykumar, \"Newton: A dram-maker's accelerator-inmemory\
    \ (aim) architecture for machine learning,\" in *2020 53rd Annual IEEE/ACM International\
    \ Symposium on Microarchitecture (MICRO)*. IEEE, 2020, pp. 372\u2013385.\n- <span\
    \ id=\"page-12-12\"></span>[14] S. Li, D. Niu, K. T. Malladi, H. Zheng, B. Brennan,\
    \ and Y. Xie, \"Drisa: A dram-based reconfigurable in-situ accelerator,\" in *2017\
    \ 50th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)*.\
    \ IEEE, 2017, pp. 288\u2013301.\n- <span id=\"page-12-13\"></span>[15] M. Lenjani,\
    \ P. Gonzalez, E. Sadredini, S. Li, Y. Xie, A. Akel, S. Eilert, M. R. Stan, and\
    \ K. Skadron, \"Fulcrum: a simplified control and access mechanism toward flexible\
    \ and practical insitu accelerators,\" in *2020 IEEE International Symposium on\
    \ High Performance Computer Architecture (HPCA)*. IEEE, 2020, pp. 556\u2013 569.\n\
    - <span id=\"page-12-14\"></span>[16] D. Kim, C. Yu, S. Xie, Y. Chen, J.-Y. Kim,\
    \ B. Kim, J. Kulkarni, and T. T.-H. Kim, \"An overview of processing-in-memory\
    \ circuits for artificial intelligence and machine learning,\" *IEEE Journal on\
    \ Emerging and Selected Topics in Circuits and Systems*, 2022.\n- <span id=\"\
    page-12-15\"></span>[17] F. Devaux, \"The true processing in memory accelerator,\"\
    \ in *2019 IEEE Hot Chips 31 Symposium (HCS)*. IEEE Computer Society, 2019, pp.\
    \ 1\u201324.\n- <span id=\"page-12-16\"></span>[18] Y. Kim, W. Yang, and O. Mutlu,\
    \ \"Ramulator: A fast and extensible dram simulator,\" *IEEE Computer architecture\
    \ letters*, vol. 15, no. 1, pp. 45\u201349, 2015.\n- <span id=\"page-12-17\"></span>[19]\
    \ J. Standard, \"High bandwidth memory (hbm) dram,\" *Jesd235*, vol. 16, 2013.\n\
    - <span id=\"page-12-18\"></span>[20] Y.-C. Kwon, S. H. Lee, J. Lee, S.-H. Kwon,\
    \ J. M. Ryu, J.-P. Son, O. Seongil, H.-S. Yu, H. Lee, S. Y. Kim *et al.*, \"25.4\
    \ a 20nm 6gb function-in-memory dram, based on hbm2 with a 1.2 tflops programmable\
    \ computing unit using bank-level parallelism, for machine learning applications,\"\
    \ in *2021 IEEE International Solid-State Circuits Conference (ISSCC)*, vol. 64.\
    \ IEEE, 2021, pp. 350\u2013352.\n- <span id=\"page-12-19\"></span>[21] S. Lee,\
    \ K. Kim, S. Oh, J. Park, G. Hong, D. Ka, K. Hwang, J. Park, K. Kang, J. Kim *et\
    \ al.*, \"A 1ynm 1.25 v 8gb, 16gb/s/pin gddr6-\n\nbased accelerator-in-memory\
    \ supporting 1tflops mac operation and various activation functions for deep-learning\
    \ applications,\" in *2022 IEEE International Solid-State Circuits Conference\
    \ (ISSCC)*, vol. 65. IEEE, 2022, pp. 1\u20133.\n\n- <span id=\"page-12-20\"></span>[22]\
    \ J.-H. Kim, S. Lee, S. Moon, S. Yoo, and J.-Y. Kim, \"19.2 a 409.6 gops and 204.8\
    \ gflops mixed-precision vector processor system for general-purpose machine learning\
    \ acceleration,\" in *2022 IEEE Asian Solid-State Circuits Conference (A-SSCC)*.\
    \ IEEE, 2022.\n- <span id=\"page-12-21\"></span>[23] Y. Kim, V. Seshadri, D. Lee,\
    \ J. Liu, and O. Mutlu, \"A case for exploiting subarray-level parallelism (salp)\
    \ in dram,\" in *2012 39th Annual International Symposium on Computer Architecture\
    \ (ISCA)*. IEEE, 2012, pp. 368\u2013379.\n- <span id=\"page-12-22\"></span>[24]\
    \ A. H. Zadeh, I. Edo, O. M. Awad, and A. Moshovos, \"Gobo: Quantizing attention-based\
    \ nlp models for low latency and energy efficient inference,\" in *2020 53rd Annual\
    \ IEEE/ACM International Symposium on Microarchitecture (MICRO)*. IEEE, 2020,\
    \ pp. 811\u2013 824.\n- <span id=\"page-12-23\"></span>[25] D. Paperno, G. Kruszewski,\
    \ A. Lazaridou, Q. N. Pham, R. Bernardi, S. Pezzelle, M. Baroni, G. Boleda, and\
    \ R. Fernandez, \xB4 \"The lambada dataset: Word prediction requiring a broad\
    \ discourse context,\" *arXiv preprint arXiv:1606.06031*, 2016.\n- <span id=\"\
    page-12-24\"></span>[26] H. Wang, Z. Zhang, and S. Han, \"Spatten: Efficient sparse\
    \ attention architecture with cascade token and head pruning,\" in *2021 IEEE\
    \ International Symposium on High-Performance Computer Architecture (HPCA)*. IEEE,\
    \ 2021, pp. 97\u2013110.\n- <span id=\"page-12-25\"></span>[27] T. J. Ham, Y.\
    \ Lee, S. H. Seo, S. Kim, H. Choi, S. J. Jung, and J. W. Lee, \"Elsa: Hardware-software\
    \ co-design for efficient, lightweight selfattention mechanism in neural networks,\"\
    \ in *2021 ACM/IEEE 48th Annual International Symposium on Computer Architecture\
    \ (ISCA)*. IEEE, 2021, pp. 692\u2013705.\n- <span id=\"page-12-26\"></span>[28]\
    \ S. Cho, H. Choi, E. Park, H. Shin, and S. Yoo, \"Mcdram v2: In-dynamic random\
    \ access memory systolic array accelerator to address the large model problem\
    \ in deep neural networks on the edge,\" *IEEE Access*, vol. 8, pp. 135 223\u2013\
    135 243, 2020.\n- <span id=\"page-12-27\"></span>[29] Y.-B. Kim and T. W. Chen,\
    \ \"Assessing merged dram/logic technology,\" *Integration, the VLSI Journal*,\
    \ vol. 2, no. 27, pp. 179\u2013194, 1999.\n- <span id=\"page-12-28\"></span>[30]\
    \ V. Seshadri and O. Mutlu, \"In-dram bulk bitwise execution engine,\" *arXiv\
    \ preprint arXiv:1905.09822*, 2019.\n- <span id=\"page-12-29\"></span>[31] M.\
    \ O'Connor, N. Chatterjee, D. Lee, J. Wilson, A. Agrawal, S. W. Keckler, and W.\
    \ J. Dally, \"Fine-grained dram: Energy-efficient dram for extreme bandwidth systems,\"\
    \ in *Proceedings of the 50th Annual IEEE/ACM International Symposium on Microarchitecture*,\
    \ 2017, pp. 41\u201354.\n- <span id=\"page-12-30\"></span>[32] Nvidia titan rtx.\
    \ [Online]. Available: [https:](https://www.nvidia.com/content/dam/en-zz/Solutions/titan/documents/titan-rtx-for-creators-us-nvidia-1011126-r6-web.pdf)\
    \ [//www.nvidia.com/content/dam/en-zz/Solutions/titan/](https://www.nvidia.com/content/dam/en-zz/Solutions/titan/documents/titan-rtx-for-creators-us-nvidia-1011126-r6-web.pdf)\n\
    - <span id=\"page-12-31\"></span>[documents/titan-rtx-for-creators-us-nvidia-1011126-r6-web.pdf](https://www.nvidia.com/content/dam/en-zz/Solutions/titan/documents/titan-rtx-for-creators-us-nvidia-1011126-r6-web.pdf)\
    \ [33] Nvidia fastertransformer. [Online]. Available: [https://github.](https://github.com/NVIDIA/FasterTransformer)\
    \ [com/NVIDIA/FasterTransformer](https://github.com/NVIDIA/FasterTransformer)\n\
    - <span id=\"page-12-32\"></span>[34] Primepower. [Online]. Available: [https://www.synopsys.com/](https://www.synopsys.com/implementation-and-signoff/signoff/primepower.html)\
    \ [implementation-and-signoff/signoff/primepower.html](https://www.synopsys.com/implementation-and-signoff/signoff/primepower.html)\n\
    - <span id=\"page-12-33\"></span>[35] S. Hong, S. Moon, J. Kim, S. Lee, M. Kim,\
    \ D. Lee, and J.-Y. Kim, \"Dfx: A low-latency multi-fpga appliance for accelerating\
    \ transformer-based text generation,\" in *2022 55th IEEE/ACM International Symposium\
    \ on Microarchitecture (MICRO)*. IEEE, 2022, pp. 616\u2013630.\n- <span id=\"\
    page-12-34\"></span>[36] H. H. Shin, Y. M. Park, D. Choi, B. J. Kim, D.-H. Cho,\
    \ and E.- Y. Chung, \"Extreme: Exploiting page table for reducing refresh power\
    \ of 3d-stacked dram memory,\" *IEEE Transactions on Computers*, vol. 67, no.\
    \ 1, pp. 32\u201344, 2017.\n- <span id=\"page-12-35\"></span>[37] K. Alizadeh,\
    \ I. Mirzadeh, D. Belenko, K. Khatamifard, M. Cho, C. C. Del Mundo, M. Rastegari,\
    \ and M. Farajtabar, \"Llm in a flash: Efficient large language model inference\
    \ with limited memory,\" *arXiv preprint arXiv:2312.11514*, 2023.\n- <span id=\"\
    page-12-36\"></span>[38] Q. Deng, Y. Zhang, M. Zhang, and J. Yang, \"Lacc: Exploiting\
    \ lookup table-based fast and accurate vector multiplication in dram-based cnn\
    \ accelerator,\" in *Proceedings of the 56th Annual Design Automation Conference\
    \ 2019*, 2019, pp. 1\u20136.\n- <span id=\"page-12-37\"></span>[39] J. D. Ferreira,\
    \ G. Falcao, J. Gomez-Luna, M. Alser, L. Orosa, \xB4 M. Sadrosadati, J. S. Kim,\
    \ G. F. Oliveira, T. Shahroodi, A. Nori *et al.*, \"Pluto: Enabling massively\
    \ parallel computation in dram via lookup tables,\" in *2022 55th IEEE/ACM International\
    \ Symposium on Microarchitecture (MICRO)*. IEEE, 2022, pp. 900\u2013919.\n- <span\
    \ id=\"page-12-38\"></span>[40] R. Zhou, S. Tabrizchi, A. Roohi, and S. Angizi,\
    \ \"Lt-pim: An lut-based processing-in-dram architecture with rowhammer selftracking,\"\
    \ *IEEE Computer Architecture Letters*, 2022.\n\n![](_page_13_Picture_0.jpeg)\n\
    \n**Wontak Han** (Graduate Student Member, IEEE) received the B.S. degree in Electronic\
    \ Engineering from Hanyang University, Seoul, South Korea, in 2021, and the M.S.\
    \ degree in electrical engineering from Korea Advanced Institute of Science and\
    \ Technology (KAIST), Daejeon, South Korea, in 2023. He is currently an engineer\
    \ in Samsung Electronics DRAM Design Team.\n\nHis research interests include energy-efficient\
    \ processing-in/near-memory architecture, and\n\ndeep-learning network and database\
    \ management system (DBMS) accelerators.\n\n![](_page_13_Picture_4.jpeg)\n\n**Hyunjun\
    \ Cho** (Graduate Student Member, IEEE) received the B.S. degree in Electrical\
    \ Engineering from Korea Advanced Institute of Science and Technology (KAIST),\
    \ Daejeon, South Korea, in 2023, where he is currently pursuing the M.S. degree.\n\
    \nHis research interests include energy-efficient processing-in/near memory architecture,\
    \ design of neural processing units (NPUs), and the design of hardware architectures\
    \ that preserve security.\n\n![](_page_13_Picture_7.jpeg)\n\n**Donghyuk Kim**\
    \ (Graduate Student Member, IEEE) received the B.S. degree in electrical and computer\
    \ engineering from University of Washington, Seattle, USA, in 2020. He received\
    \ the M.S. degree in electrical and computer engineering from Korea Advanced Institute\
    \ of Science and Technology (KAIST), Daejeon, South Korea, in 2022, where he is\
    \ currently pursuing the Ph.D. degree.\n\nHis research interests include energy-efficient\
    \ processing-in-memory architecture for machine\n\nlearning and database, and\
    \ deep neural network accelerators.\n\n![](_page_13_Picture_11.jpeg)\n\n**Joo-Young\
    \ Kim** (Senior Member, IEEE) received the B.S., M.S., and Ph. D. degrees in Electrical\
    \ Engineering from KAIST, Daejeon, South Korea, in 2005, 2007, and 2010, respectively.\
    \ He is currently an Associate Professor in the School of Electrical Engineering\
    \ at KAIST. He is also the Director of AI Semiconductor Systems Research Center.\
    \ His research interests span various aspects of hardware design, including VLSI\
    \ design, computer architecture, FPGA, domain-specific accelerators, hard-\n\n\
    ware/software co-design, and agile hardware development. Before joining KAIST,\
    \ he was a Senior Hardware Engineering Lead at Microsoft Azure, Redmond, WA, USA,\
    \ working on hardware acceleration for the hyper-scale big data analytics platform\
    \ named Azure Data Lake. He was also one of the initial members of Catapult project\
    \ at Microsoft Research, Redmond, WA, USA, where he deployed a fabric of FPGA\
    \ accelerators in datacenters to accelerate critical cloud services, such as machine\
    \ learning, data storage, and networking."
  paper_id: 2
  scores:
    originality:
      raw: 4.5
      reason: The paper presents a novel approach to soft GPGPU architecture that
        leverages FPGA capabilities, contributing significantly to the field.
    technical_soundness:
      raw: 4.0
      reason: The methodology for achieving high clock rates is well-documented, but
        the benchmarks could be more comprehensive to fully validate the claims.
    clarity:
      raw: 3.5
      reason: While the concepts are generally well-explained, some sections could
        benefit from clearer definitions and more structured presentation to enhance
        understanding.
    significance:
      raw: 4.0
      reason: The work addresses a critical gap in soft processor architectures, making
        it relevant for both academic and commercial applications in FPGA design.
    relevance:
      raw: 4.5
      reason: The topic is highly relevant to current trends in hardware acceleration
        and FPGA utilization, appealing to both researchers and industry practitioners.
  total_score: 4.05
  tier: C
- title: Floating Point HUB Adder for RISC-V Sargantana Processor
  abstract: 'RRAM crossbars have been studied to construct in-memory accelerators
    for

    neural network applications due to their in-situ computing capability. However,

    prior RRAM-based accelerators show efficiency degradation when executing the

    popular attention models. We observed that the frequent softmax operations

    arise as the efficiency bottleneck and also are insensitive to computing

    precision. Thus, we propose STAR, which boosts the computing efficiency with an

    efficient RRAM-based softmax engine and a fine-grained global pipeline for the

    attention models. Specifically, STAR exploits the versatility and flexibility

    of RRAM crossbars to trade off the model accuracy and hardware efficiency. The

    experimental results evaluated on several datasets show STAR achieves up to

    30.63x and 1.31x computing efficiency improvements over the GPU and the

    state-of-the-art RRAM-based attention accelerators, respectively.'
  url: http://arxiv.org/abs/2401.17582v1
  keywords: RRAM Crossbar, Attention Model, Softmax, Processing-in-memory
  document: "## I. INTRODUCTION\n\nThough some RRAM-based accelerators specialized\
    \ for attention models have been discussed [\\[1\\]](#page-1-0)\u2013[\\[3\\]](#page-1-1),\
    \ they primarily focus on implementing the matrix multiplications on the RRAM\
    \ crossbar. In this work, we observed the execution time of softmax operation\
    \ grows quickly in attention models when the input sequence length increases.\
    \ The latency of softmax exceeds that of matrix multiplication when the input\
    \ sequence length is 512 in the BERT-base model, which reaches up to 59.20% of\
    \ the whole execution time. Though our results are observed on a GPU platform,\
    \ the softmax latency problem would be exacerbated on the RRAM-based accelerators\
    \ because the matrix multiplication is significantly optimized by being implemented\
    \ in RRAM crossbars [\\[4\\]](#page-1-2) but the softmax still runs on the same\
    \ circuits. Thus, it is of significance to tailor an efficient softmax engine\
    \ in RRAM-based accelerators for attention models. To this end, we propose STAR,\
    \ which features an RRAM-based softmax engine by exploring the versatility and\
    \ flexibility of RRAM crossbars to balance to the computing precision and efficiency.\
    \ Moreover, an enhanced pipeline to balance the matrix multiplication and softmax\
    \ operation in the attention is introduced. The effectiveness of STAR is verified\
    \ by the comparison results with the recent RRAM-based accelerators for attention\
    \ models [\\[3\\]](#page-1-1).\n\n## II. RRAM-BASED SOFTMAX ENGINE\n\nSTAR is\
    \ primarily composed of two types of crossbar-based processing engines: *MatMul\
    \ engine* for the VMM-dominated\n\n![](_page_0_Figure_10.jpeg)\n\n<span id=\"\
    page-0-0\"></span>Fig. 1. The x<sup>i</sup> \u2212 xmax operation design.\n\n\
    operations and *Softmax engine* for the softmax operation, respectively. The MatMul\
    \ engine follows the design in Re-Transformer [\\[3\\]](#page-1-1). As for the\
    \ Softmax engine, different function units based on RRAM crossbars cooperate with\
    \ each other to complete the softmax operation. The Softmax engine has two distinct\
    \ stages, x<sup>i</sup> \u2212xmax and the exponential operation, which desire\
    \ crossbars having different functions.\n\n*1)* x<sup>i</sup> \u2212 xmax*:* The\
    \ x<sup>i</sup> \u2212 xmax is achieved by one crossbar in a time-multiplex manner\
    \ to complete the finding maximum and subtraction, respectively. Thus, the crossbar\
    \ is denoted as CAM/SUB crossbar.\n\nFig. [1](#page-0-0) shows the workflow of\
    \ a 4\xD78 CAM/SUB crossbar to find out the max value in [x<sup>1</sup> \xB7 \xB7\
    \ \xB7 x4]. The crossbar works as a CAM first. For each x<sup>i</sup> , all the\
    \ W Ls of the crossbar are searched in parallel and the matchlines output a one-hot\
    \ vector in which '1' denotes the matched line. For example, if the data stored\
    \ in the W L<sup>3</sup> in Fig. [1](#page-0-0) is consistent with x1, the output\
    \ vector would be [0,0,1,0](\u20DD2 ). The outputs of matchlines cascade the OR\
    \ gates that merge the search results of all input xi(\u20DD3 ). Because the data\
    \ are stored in descending order in the CAM crossbar, the index of the first '1'\
    \ in the result vector corresponds to the row number of CAM storing the xmax.\
    \ In the example of Fig. [1,](#page-0-0) xmax stores at W L2. Next, the crossbar\
    \ executes the subtraction x<sup>i</sup> \u2212 xmax. The match vector outputs\
    \ will be used as the input voltage vector. Instead, the input for the xmax row\
    \ is a negative voltage (\u20DD4 ). Thus, the output from the SLs represents the\
    \ results of x<sup>i</sup> \u2212 xmax(\u20DD5 ).\n\n*2) Exponential Operation:*\
    \ The exponential operation is implemented by CAM crossbar and LUT crossbar. A\
    \ VMM crossbar collaborates with them to complete the summation in the softmax.\
    \ All possible values of x<sup>i</sup> \u2212 xmax and their exponential results\
    \ are preloaded in CAM crossbar and LUT crossbar, respectively. Since the x<sup>i</sup>\
    \ \u2212 xmax is always negative,\n\nCorresponding author: Bing Li, *bing.li@cnu.edu.cn*\n\
    \n![](_page_1_Figure_0.jpeg)\n\nFig. 2. The exponential operation design in our\
    \ softmax engine. TABLE I\n\n<span id=\"page-1-6\"></span>\n\n| Softmax Design\
    \ | Area  | Power |\n|----------------|-------|-------|\n| Softermax [5]  | 0.33\xD7\
    \ | 0.12\xD7 |\n| Ours (8-bit)   | 0.06\xD7 | 0.05\xD7 |\n\nwe remove the sign\
    \ bit to save the area of CAM crossbar. Each input enters CAM crossbar and the\
    \ output from the LUT crossbar is its exponential result. At the same time, the\
    \ match vector for CAM crossbar is sent to the counter for accumulation. When\
    \ all x<sup>i</sup> complete the exponential computation, the results of the counter\
    \ are sent to the VMM crossbar which stores exactly the same values as LUT crossbar\
    \ to compute P<sup>d</sup> <sup>j</sup>=1 e <sup>x</sup>j\u2212xmax . Then the\
    \ outputs of LUT crossbar and VMM crossbar enter the divider to complete the final\
    \ division in the softmax.\n\nSince the efficiency of the proposed softmax engine\
    \ relates to the computing precision determined by the attention model, we analyzed\
    \ the data range of all x<sup>i</sup> across three popular datasets for the BERT-base\
    \ model such that balances the computing precision and hardware efficiency with\
    \ STAR. To achieve high model accuracy, the required bitwidth for CNEWS, MRPC,\
    \ and CoLA are 8 bits (6-bit integer, 2-bit decimal), 9 bits (6-bit integer, 3-bit\
    \ decimal), and 7 bits (5-bit integer, 2-bit decimal), respectively.\n\nWith the\
    \ proposed RRAM-based Softmax engine, we introduce a vector-grained pipeline to\
    \ improve the execution parallelism and efficiency for attention models. Thanks\
    \ to the crossbar-based softmax engine, the complete attention mechanism operations\
    \ could be in parallel in the vector granularity rather than the operand granularity\
    \ in previous work.\n\n## III. EXPERIMENTAL RESULTS\n\nWe compared the proposed\
    \ RRAM-based Softmax engine with an optimized COMS-based softmax accelerator,\
    \ Softermax [\\[5\\]](#page-1-3) and a baselined CMOS-based softmax and compared\
    \ STAR with a NVIDIA Titan RTX GPU platform and two ReRAM-based accelerators PipeLayer\
    \ [\\[6\\]](#page-1-4) and ReTransformer [\\[3\\]](#page-1-1) to verify the collaboration\
    \ of the proposed pipeline and Softmax engine.\n\nThe simulation of STAR is performed\
    \ on NeuroSim [\\[7\\]](#page-1-5) (for RRAM crossbar) and Synopsys Design Compiler\
    \ (for the CMOS circuit), respectively. In the MatMul engine, the RRAM crossbar\
    \ size is 128\xD7128 and the precision of ADC is 5-bit by referring to [\\[3\\\
    ]](#page-1-1). In the proposed Softmax engine, the size of the CAM/SUB crossbar\
    \ is 512\xD718 and the CAM (LUT, VMM)\n\n![](_page_1_Figure_9.jpeg)\n\n<span id=\"\
    page-1-7\"></span>Fig. 3. Computing efficiency comparison results.\n\ncrossbar\
    \ size is 256\xD718 to support 9-bit data and computing precision.\n\nTable [I](#page-1-6)\
    \ is the comparison results of our Softmax engine with Softermax and the baseline\
    \ CMOS-based softmax. Here, the evaluated model is the BERT-base model on the\
    \ CNEWS dataset with a sequence length of 128. Compared to the baseline and Softermax,\
    \ our Softmax engine is 0.06\xD7 and 0.20\xD7 smaller, respectively. As for power,\
    \ it achieves 0.05\xD7 and 0.44\xD7 power efficient than baseline and Softermax,\
    \ respectively. The results show our proposed Softmax engine offers a much better\
    \ area efficiency and power efficiency than the baseline and Softermax. Fig. [3](#page-1-7)\
    \ compares the computing efficiency of GPU, Pipelayer [\\[6\\]](#page-1-4), ReTransformer\
    \ [\\[3\\]](#page-1-1) and STAR. Computing efficiency here measures the number\
    \ of operations that can be performed by a computing unit every unit time and\
    \ every watt of power consumed. STAR achieves the computing efficiency of 612.66GOPs/s/W.\
    \ Compared to GPU, Pipelayer and ReTransformer, STAR improves the computing efficiency\
    \ by 30.63\xD7, 4.32\xD7 and 1.31\xD7, respectively.\n\n## ACKNOWLEDGEMENT\n\n\
    This paper is supported by the National Natural Science Foundation of China (NSFC)\
    \ under grant No. 62204164, 62222411.\n\n## REFERENCES\n\n- <span id=\"page-1-0\"\
    ></span>[1] S. Lu, M. Wang, S. Liang, J. Lin, and Z. Wang, \"Hardware accelerator\
    \ for multi-head attention and position-wise feed-forward in the transformer,\"\
    \ *arXiv preprint arXiv:2009.08605*, 2020.\n- [2] H. Guo, L. Peng, J. Zhang, Q.\
    \ Chen, and T. D. LeCompte, \"Att: A fault-tolerant reram accelerator for attention-based\
    \ neural networks,\" *IEEE International Conference on Computer Design: VLSI in\
    \ Computers and Processors,ICCD*, 2020.\n- <span id=\"page-1-1\"></span>[3] X.\
    \ Yang, B. Yan, H. Li, and Y. Chen, \"Retransformer: Reram-based processing-in-memory\
    \ architecture for transformer acceleration,\" in *Proceedings of the 39th International\
    \ Conference on Computer-Aided Design*, pp. 1\u20139, 2020.\n- <span id=\"page-1-2\"\
    ></span>[4] B. Li, L. Song, F. Chen, X. Qian, Y. Chen, and H. H. Li, \"Reram-based\
    \ accelerator for deep learning,\" in *2018 Design, Automation & Test in Europe\
    \ Conference & Exhibition (DATE)*, pp. 815\u2013820, IEEE, 2018.\n- <span id=\"\
    page-1-3\"></span>[5] J. R. Stevens, R. Venkatesan, S. Dai, B. Khailany, and A.\
    \ Raghunathan, \"Softermax: Hardware/software co-design of an efficient softmax\
    \ for transformers,\" *arXiv preprint arXiv:2103.09301*, 2021.\n- <span id=\"\
    page-1-4\"></span>[6] L. Song, X. Qian, H. Li, and Y. Chen, \"Pipelayer: A pipelined\
    \ reram-based accelerator for deep learning,\" in *2017 IEEE International Symposium\
    \ on High Performance Computer Architecture (HPCA)*, pp. 541\u2013552, IEEE, 2017.\n\
    - <span id=\"page-1-5\"></span>[7] P.-Y. Chen, X. Peng, and S. Yu, \"Neurosim+:\
    \ An integrated device-toalgorithm framework for benchmarking synaptic devices\
    \ and array architectures,\" in *2017 IEEE International Electron Devices Meeting\
    \ (IEDM)*, pp. 6\u20131, IEEE, 2017."
  paper_id: 9
  scores:
    originality:
      raw: 4.5
      reason: The introduction of the HUB format for floating-point representation
        is innovative and provides a fresh perspective on improving hardware efficiency.
    technical_soundness:
      raw: 4.0
      reason: The paper presents a well-structured implementation of the HUB adder,
        though some technical details could benefit from further elaboration.
    clarity:
      raw: 3.5
      reason: While the paper is generally clear, certain sections could be more concise
        and accessible to enhance understanding for a broader audience.
    significance:
      raw: 4.0
      reason: The work addresses a relevant issue in processor design, particularly
        for RISC-V, making it significant for both academia and industry.
    relevance:
      raw: 4.5
      reason: The focus on RISC-V and floating-point operations aligns well with current
        trends in computer architecture, ensuring high relevance to the field.
  total_score: 4.05
  tier: C
- title: "Study on the Particle Sorting Performance for Reactor Monte Carlo\n  Neutron\
    \ Transport on Apple Unified Memory GPUs"
  abstract: 'Assertion-based verification (ABV) is a critical method for ensuring
    design

    circuits comply with their architectural specifications, which are typically

    described in natural language. This process often requires human interpretation

    by verification engineers to convert these specifications into functional

    verification assertions. Existing methods for generating assertions from

    natural language specifications are limited to sentences extracted by

    engineers, discouraging its practical application. In this work, we present

    AssertLLM, an automatic assertion generation framework that processes complete

    specification files. AssertLLM breaks down the complex task into three phases,

    incorporating three customized Large Language Models (LLMs) for extracting

    structural specifications, mapping signal definitions, and generating

    assertions. Our evaluation of AssertLLM on a full design, encompassing 23 I/O

    signals, demonstrates that 89\% of the generated assertions are both

    syntactically and functionally accurate.'
  url: http://arxiv.org/abs/2402.00386v3
  keywords: ''
  document: "## I. INTRODUCTION\n\n<span id=\"page-0-2\"></span>Hardware functional\
    \ verification is critical in the VLSI design flow, primarily addressing whether\
    \ an implementation adheres to its specification. For instance, the register-transfer\
    \ level (RTL) design of a processor must comply with the given instruction set\
    \ architecture (ISA) specification. Typically, the architects first develop the\
    \ specifications in a natural language document. Subsequently, RTL designers translate\
    \ these specifications into RTL code, while the verification engineers are responsible\
    \ for checking the functional correctness of the RTL designs according to the\
    \ specifications.\n\nDuring the verification process, assertion-based verification\
    \ (ABV) [\\[1\\]](#page-8-0) is a widely adopted technique, which utilizes assertions\
    \ crafted from specifications to verify the functional behavior of RTL designs.\
    \ ABV can be conducted either through simulation with testbenches or using formal\
    \ property verification (FPV) techniques. Temporal logic, particularly SystemVerilog\
    \ Assertions (SVA), is commonly employed for specifying these properties. However,\
    \ a significant challenge in ABV is the generation of sufficient, high-quality\
    \ assertions. Currently, designing SVAs manually is a time-consuming and error-prone\
    \ task, demanding unignorable human effort.\n\nTo address this challenge, research\
    \ has focused on generating SVAs automatically. The automatic approaches can be\
    \ categorized into two types: dynamic mining from simulation traces and static\
    \ analysis of specifications. Dynamic methods [\\[2\\]](#page-8-1)\u2013[\\[4\\\
    ]](#page-8-2) generate assertions by combining simulating test traces and static\
    \ analysis of design constraints. However, a critical limitation of dynamic methods\
    \ is that both the generation and evaluation of assertions are on the same RTL\n\
    \n<span id=\"page-0-0\"></span>![](_page_0_Figure_11.jpeg)\n\nFig. 1: AssertLLM\
    \ in VLSI design and verification flow. AssertLLM automatically generates SVAs\
    \ from natural language specifications, facilitating functional verification for\
    \ both bug avoidance and bug hunting.\n\ndesign without referring to a golden\
    \ reference model. This could lead to the generation of incorrect SVAs due to\
    \ flaws in the RTL design, which these methods might not detect. On the other\
    \ hand, existing static methods depend either on the pre-defined templates [\\\
    [5\\]](#page-8-3), [\\[6\\]](#page-8-4) or on machine learning (ML) technologies\
    \ [\\[7\\]](#page-8-5)\u2013[\\[17\\]](#page-8-6). The template-based methods\
    \ also require a deep understanding of the design function to fill in the templates.\
    \ As for the ML-based methods, they explore both traditional natural language\
    \ processing (NLP) and emerging Generative AI techniques like Large Language Models\
    \ (LLMs). We further categorize the existing static ML-based methods based on\
    \ their application in different design phases: the RTL and pre-RTL stages.\n\n\
    Table [I](#page-1-0) details these ML-based SVA generation methods in both the\
    \ RTL stage and the pre-RTL stage. During the RTL stage, the process typically\
    \ involves using LLMs to process both human-written specification sentences and\
    \ the RTL design to generate SVAs describing security or functional properties\
    \ [\\[7\\]](#page-8-5)\u2013[\\[9\\]](#page-8-7). However, similar to the dynamic\
    \ methods, inaccuracies in RTL implementations could result in flawed SVAs.\n\n\
    When it comes to the pre-RTL stage, with the natural language specification document\
    \ finalized, RTL designers proceed to implement behavior satisfying this golden\
    \ specification. Numerous studies [\\[10\\]](#page-8-8)\u2013[\\[17\\]](#page-8-6)\
    \ have employed NLP techniques to generate SVAs from sentences extracted by humans.\
    \ These works focused on processing sentences identified from a comprehensive\
    \ document of specification. However, specification extraction requires tremendous\
    \ human efforts, and the NLP-based generation process faces challenges in\n\n\
    <span id=\"page-1-0\"></span>\n\n| Stage   | Works     | Generation<br>Method\
    \ | NL Specification |                  | Evaluation  |                      \
    \       |                          |  |\n|---------|-----------|----------------------|------------------|------------------|-------------|-----------------------------|--------------------------|--|\n\
    |         |           |                      | Auto Extract.    | Source     \
    \      | Full Design | Target                      | Open-Source<br>Benchmark\
    \ |  |\n| RTL     | [7]       | LLM-based            | %                | Sentences\
    \        | %           | Security                    | %                     \
    \   |  |\n|         | [8], [9]  |                      |                  | from\
    \ Engineers   |             | Function                    | \u22C6           \
    \             |  |\n| Pre-RTL | [10]\u2013[17] | NLP-based            | %    \
    \            | Sentences        |             | Function (specialized       |\
    \ %                        |  |\n|         |           |                     \
    \ |                  | from SPEC file   |             | checkers/ artificial cases)\
    \ |                          |  |\n|         | Ours      | LLM-based         \
    \   |                  | Entire SPEC file |             | Function           \
    \         |                          |  |\n|         |           |           \
    \           |                  |                  |             | (general benchmark)\
    \         |                          |  |\n\n<sup>\u22C6</sup> Work [\\[8\\]](#page-8-9)\
    \ only open-source the result on a FIFO, while work [\\[9\\]](#page-8-7) only\
    \ demonstrate based on tiny designs such as FSM and DFF.\n\nTABLE I: Existing\
    \ works on generating SVAs from natural language specifications. AssertLLM is\
    \ the first work that can handle full-size specification files and generate comprehensive\
    \ types of SVAs for each architectural signal. We also propose the first open-source\
    \ benchmark for assertion generation and evaluation from natural language specifications.\n\
    \ngeneralizing across diverse grammatical variations. Additionally, the evaluation\
    \ of SVAs generated through these methods typically depends on design-specific\
    \ checkers, such as protocol and processor checkers, and is therefore difficult\
    \ to extend to other design types.\n\nHere we summarize three key challenges that\
    \ currently hinder the practical application of SVA generation from natural language\
    \ specifications:\n\n- 1) Natural language VLSI specifications are inherently\
    \ unstructured and are hard to be directly used for assertion generation.\n- 2)\
    \ Even with structured specifications, translating natural language into assertions\
    \ remains a highly complex task, requiring both a deep understanding of the design\
    \ functionality and specialized expertise in SVA.\n- 3) Currently, there is a\
    \ lack of a universal evaluation method and benchmarks capable of addressing the\
    \ diverse types of VLSI designs.\n\nTo tackle the identified challenges in SVA\
    \ generation, in our work, we propose AssertLLM, a novel automatic assertion generation\
    \ framework incorporating multiple specialized LLMs to deal with the decomposed\
    \ tasks separately. This framework is designed to process complete natural language\
    \ specification files, automatically producing SVAs for each architectural signal.\
    \ This approach significantly benefits both design-time bug prevention and verification-time\
    \ bug detection. The role of AssertLLM within the standard VLSI design and verification\
    \ flow is illustrated in Fig. [1.](#page-0-0) AssertLLM effectively addresses\
    \ the outlined challenges by combining three customized LLMs, each focused on\
    \ a specific task: extracting relevant information from specifications, mapping\
    \ signal definitions, and translating natural language specifications into SVAs.\
    \ The resulting SVAs contain various types, including bit-width, connectivity,\
    \ and functional assertions.\n\nAdditionally, our work provides an open-source\
    \ benchmark, designed to evaluate the quality of the generated SVAs. This benchmark,\
    \ coupled with a general evaluation method, is adaptable to various design types.\n\
    \nTo the best of our knowledge, AssertLLM is the first automatic assertion generation\
    \ method that can handle fullsize specification files and generate various types\
    \ of SVAs for each architectural signal. It also provides the first open-source\
    \ benchmark for assertion generation and evaluation from design specifications,\
    \ which can deal with different design types. Our contributions in this work are\
    \ summarized below:\n\n- To the best of our knowledge, AssertLLM is the first\
    \ automatic assertion generation method that can handle the complete specification\
    \ files and generate comprehensive types of SVAs for each architectural signal.\n\
    - We incorporate three customized LLMs, each enhanced with specific techniques\
    \ for the decomposed tasks: extracting structural information from specifications,\
    \ mapping signal declarations, and translating specifications into various SVA\
    \ types. These SVAs support checks for bit-width, connectivity, and function.\n\
    - We provide the first open-source benchmarks[1](#page-0-1) for assertion generation\
    \ and evaluation, which include both golden specification documents and golden\
    \ RTL designs. The generated SVAs are evaluated on the golden RTL implementations\
    \ using model checking tools. Our evaluation method is designed to be applicable\
    \ across a variety of design types.\n- To demonstrate the effectiveness of AssertLLM,\
    \ we conducted a comprehensive evaluation on a complete design. This resulted\
    \ in the generation of 56 SVAs for 23 signals, with 23 for bit-width, 16 for interface,\
    \ and 17 for function. Impressively, 89% of these generated SVAs are evaluated\
    \ to be correct both syntactically and functionally.\n\nII. PRELIMINARIES AND\
    \ PROBLEM FORMULATION\n\n# <span id=\"page-1-1\"></span>*A. Natural Language Specification*\n\
    \nA well-defined natural language specification mainly contains the following\
    \ six parts: (1) introduction: introduces the concepts and the features of the\
    \ target design. (2) IO ports: provides detailed information on the prime input\
    \ and prime output ports essential for interfacing. (3) registers: describe all\
    \ the architecture-level registers in the design. (4) operation: explains the\
    \ operational procedures for dataflow and control. (5) architecture: the high-level\
    \ workflow and dataflow of the design. (6) usage examples: offers basic usage\
    \ scenarios and corresponding waveform illustrations for the design.\n\nSpecifically\
    \ for the signals, the specification only defines the necessary architecture-level\
    \ IO ports and registers, while leaving the definition of the internal signals\
    \ used in the detailed RTL implementations for the RTL designers.\n\n<sup>1</sup>\
    \ It will be open-sourced in https://github.com/hkust-zhiyao/AssertLLM\n\n## *B.\
    \ LLM for EDA*\n\nRecent advancements in LLMs like ChatGPT [\\[18\\]](#page-8-10)\
    \ have not only demonstrated remarkable capability in content generation but also\
    \ evolved to assist humans in various roles as agents. The application of LLMs\
    \ in the field of electronic design automation (EDA) is an emerging area of exploration.\
    \ Besides employing LLMs for assertion generation [\\[7\\]](#page-8-5)\u2013[\\\
    [9\\]](#page-8-7), recent studies have employed LLMs for tasks such as RTL code\
    \ generation [\\[19\\]](#page-8-11)\u2013[\\[26\\]](#page-8-12) and syntax correction\
    \ [\\[27\\]](#page-8-13). Additionally, LLM-based solutions have been developed\
    \ to facilitate the interaction with EDA tools [\\[26\\]](#page-8-12), [\\[28\\\
    ]](#page-8-14), design architecture for AI accelerators [\\[29\\]](#page-8-15),\
    \ [\\[30\\]](#page-8-16), fix security bugs [\\[31\\]](#page-8-17), generate and\
    \ review specification documents [\\[32\\]](#page-8-18), etc. These diverse applications\
    \ and research efforts indicate a highly promising future for LLMs in enhancing\
    \ and revolutionizing chip design processes.\n\n## *C. Problem Fromulation*\n\n\
    We denote a well-defined specification file as S, where each architectural signal\
    \ detailed in the specification is denoted as sg<sup>i</sup> . Note that the architectural\
    \ signal contains both the input and output ports and the architecture-level registers,\
    \ but excludes the internal signals further implemented in the RTL design. Our\
    \ generation process, denoted as Gen, is designed to analyze the specification\
    \ file S and generate a comprehensive set of assertions A for each signal sg<sup>i</sup>\
    \ . The assertion generation can be expressed as follows:\n\nProblem 1 (Assertion\
    \ Generation from Specification).\n\n$$\\forall s g\\_i \\in \\mathcal{S}, Gen(S,\
    \ s g\\_i) \\to \\mathcal{A}(s g\\_i) \\tag{1}$$\n\nFollowing the generation of\
    \ assertions, it is crucial to evaluate their quality. We denote this evaluation\
    \ process as Eval. To assess the correctness of the generated assertions, we utilize\
    \ the golden RTL implementations, symbolized as R. The correctness of the assertions\
    \ is denoted as Correct. The evaluation can be formulated below:\n\nProblem 2\
    \ (Generated Assertion Evaluation).\n\n$$\\forall sg\\_i \\in \\mathcal{S}, Eval(\\\
    mathcal{R}, \\mathcal{A}(sg\\_i)) \\to \\ correct(\\mathcal{A}(sg\\_i)) \\quad\
    \ (2)$$\n\n## III. METHODOLOGY\n\n## *A. Workflow Overview*\n\nFig. [2](#page-3-0)\
    \ illustrates the SVA generation and evaluation flow for AssertLLM. Our approach\
    \ to generating hardware verification assertions from natural language specifications,\
    \ particularly from comprehensive specification documents, involves the integration\
    \ of three customized LLMs. These LLMs are designed to break down this complex\
    \ task into manageable components, thereby facilitating a comprehensive generation\
    \ workflow. Additionally, we contribute an open-source benchmark and establish\
    \ evaluation methodologies to assess the quality of the generated SVAs.\n\nThe\
    \ assertion generation process is decomposed into three primary steps: (1) Extraction\
    \ of relevant information from the original specification necessary for SVA generation.\
    \ (2) Alignment of signal names between the natural language specifications and\
    \ their corresponding declarations in HDL code. (3) Generation of high-quality\
    \ SVAs based on the extracted natural language specifications.\n\nIn the subsequent\
    \ subsections, we will detail the functionalities of each customized LLM of the\
    \ comprehensive assertion generation flow. Following this, our SVA evaluation\
    \ methodology will be presented.\n\n## *B. Specification Information Extraction*\n\
    \nThe first step of our AssertLLM framework is to extract structured information\
    \ from natural language specification documents to enable SVA generation. As we\
    \ discussed in Section [I,](#page-0-2) the first key challenge of SVA generation\
    \ lies in the inherent unstructured nature of the original specifications, which\
    \ contain background information, functional descriptions, microarchitecture designs,\
    \ and various diagrams, including dataflow and waveform, etc. Meanwhile, the existence\
    \ of assertion-relevant information across different sections further complicates\
    \ the direct utilization of the original specifications for SVA generation.\n\n\
    Facing this challenge, existing methods can only deal with sentence-level specifications.\
    \ Some works [\\[14\\]](#page-8-19), [\\[16\\]](#page-8-20) utilize ML methods\
    \ to assess the relevance of manually extracted specification sentences to the\
    \ intended assertions. Other studies [\\[10\\]](#page-8-8)\u2013[\\[12\\]](#page-8-21),\
    \ [\\[15\\]](#page-8-22), [\\[17\\]](#page-8-6) directly use the human-identified\
    \ or human-written assertion-related sentences. This reliance on sentence-level\
    \ analysis limits the ability of the above NLPbased methods to fully automate\
    \ SVA generation for practical applications\n\nTo address the challenge of processing\
    \ original, unstructured, full-size specification documents, we propose a customized\
    \ LLM, tailored to extract structural and relevant information for each defined\
    \ signal, thereby further facilitating the SVA generation process.\n\nSpecifically,\
    \ in our LLM 1 SPEC Analyzer, we first utilize system instructions to customize\
    \ the LLM, shown as Fig. [3.](#page-3-1) The model takes the full-size specification\
    \ file as the input, and the multi-modal function is employed to analyze the file\
    \ containing text, table, figures, etc. Then for each signal, the LLM is required\
    \ to extract all the related information of the signal. Here, we design a structured\
    \ template to guide the LLM in extracting all essential signal-related information.\
    \ This template contains three key components: the signal's name, its description,\
    \ and the interconnection signals. We demonstrate the details of each part as\
    \ follows:\n\n- Name: The identifier of the signal in the specification, ensuring\
    \ clear and unambiguous reference.\n- Description: To facilitate SVA generation,\
    \ we divide the descriptions into four categories, including (1) definitions such\
    \ as bit-width and signal type. (2) functionality which contains all the function-related\
    \ information of the target signal in the entire specification file. (3) interconnection\
    \ relationship with all other signals. (4) additional information that is not\
    \ included in the above three types.\n- Interconnection Signals: A list of signals\
    \ that interact or are associated with the target signal, which are essential\
    \ for the assertion generation, and will be processed in the next LLM.\n\n<span\
    \ id=\"page-3-0\"></span>![](_page_3_Figure_0.jpeg)\n\nFig. 2: AssertLLM generation\
    \ and evaluation workflow. AssertLLM incorporates three customized LLMs, each\
    \ enhanced with specific techniques for the decomposed tasks: extracting structural\
    \ information from specifications, mapping signal definitions, and translating\
    \ specifications into various SVA types. To evaluate the performance of the generation\
    \ methods, the generated SVAs are further assessed based on the golden RTL implementations\
    \ using model checking tools.\n\nNote that the extracted information is summarized\
    \ across different sections of the original specification, which contains all\
    \ the information needed for assertion generation.\n\n<span id=\"page-3-1\"></span>**[Custom\
    \ Instructions for SPEC Analyzer]**\n\n- Please act as a professional VLSI specification\
    \ analyzer.\n- Firstly, I'll upload the the **design specification file** (in\
    \ PDF format).\n- Each time I ask a question, please read and analyze the specification\
    \ PDF file carefully, and answer the question **only based on the uploaded files**,\
    \ don't use any content outside the file.\n- When I ask for information on a **[xxx]\
    \ signal**, please **extract all the information related to this signal** in the\
    \ spec file, note that the extracted information should be **suitable for the\
    \ SystemVerilog Assertions (SVAs) generation**, and output all the information\
    \ in the following format:\n\n| [Signal name]: name in SPEC                  \
    \                       |\n|---------------------------------------------------------------------|\n\
    | [Description]: including but not limited to                         |\n| [Definition]:\
    \ such as bit-width, signal type (in/out/register), etc |\n| [Functionality]:\
    \ all the function information illustrated in SPEC   |\n| [Interconnection]: the\
    \ relationship with all other signals          |\n| [Additional information]:\
    \ all other related information             |\n| [Related Signals]: names of all\
    \ other signals mentioned in the      |\n| Description part                  \
    \                                  |\n|                                      \
    \                               |\n\nFig. 3: Custom Instructions for LLM 1 SPEC\
    \ Analyzer\n\n## *C. Signal Definition Mapping*\n\nAfter extracting the structural\
    \ specification information, we also face another problem: the target assertion\
    \ contains internal signals that are not clearly defined in the specification\
    \ file. As we illustrated in Subsection [II-A,](#page-1-1) the specification document\
    \ typically details only the input/output ports and architecturelevel registers,\
    \ while leaving the internal signals used to implement the detailed functions\
    \ in RTL code (e.g., internal wires and registers) undefined.\n\nTo solve this\
    \ problem, we introduce the second customized LLM 2 Signal Mapper to analyze the\
    \ signal definitions in the initialized HDL code and align these signal declarations\
    \ with the natural language signal names found in the specification document.\n\
    \nSpecifically, we also use the custom instructions to guide LLM to specialize\
    \ in the mapping task, shown in Fig. [5.](#page-3-2) The model processes both\
    \ the original specification file and the signal definition HDL code as inputs.\
    \ It employs a code interpreter to carefully examine both the declarations and\
    \ the comments within the HDL code snippet. Subsequently, the LLM analyzes the\
    \ contents of the two files to establish a **[Prompt for SPEC Analyzer]**\n\n\
    - [Upload the original SPEC file] Here is the design specification file, please\
    \ analyze it carefully.\n- [For each signal] Please extract all the information\
    \ related to the signal control register (CTR) from the spec file.\n\n#### **[Response\
    \ from SPEC Analyzer]**\n\nThe Control Register (CTR) in the I2C-Master Core specification\
    \ has the following characteristics:\n\n1. [Signal Name]: In Spec - Control Register\
    \ (CTR). 2. [Description]: 2.1. [Definition]: - Bit-width: 8 bits - Signal Type:\
    \ Register (Access Type: RW - Read/Write) 2.2. [Functionality]: - Bit 7: I2C core\
    \ enable bit. When set to '1', the core is enabled, and when set to '0', the core\
    \ is disabled. - Bit 6: I2C core interrupt enable bit. When set to '1', interrupts\
    \ are enabled, and when set to '0', interrupts are disabled. - Bits 5:0: Reserved.\
    \ These bits are read/write but are reserved for future use. 2.3. [Interconnection]:\
    \ This register directly controls the operation of the I2C core, enabling or disabling\
    \ it and controlling interrupt behavior. 2.4. [Additional Information]: - Reset\
    \ Value for the Control Register is 0x00 3. [Related Signals]: - EN: Controls\
    \ the enable state of the I2C core. - IEN: Controls the interrupt enable state\
    \ of the I2C core.\n\nFig. 4: Prompt and Response Example of LLM 1 SPEC Analyzer\n\
    \nmapping relationship between the specification and the HDL code.\n\n<span id=\"\
    page-3-2\"></span>\n\n| [Custom Instruction for Signal Mapper]               \
    \                       |  |  |  |  |  |\n|-----------------------------------------------------------------------------|--|--|--|--|--|\n\
    | Please act as a signal name mapping tool to link the specification file    \
    \ |  |  |  |  |  |\n|                                                        \
    \                     |  |  |  |  |  |\n| Firstly, I'll upload the the design\
    \ specification file (in PDF format), and |  |  |  |  |  |\n| a Verilog file containing\
    \ all the signal definitions (*.v format).          |  |  |  |  |  |\n| Please\
    \ analyze the specification file and use the code interpreter to       |  |  |\
    \  |  |  |\n| analyze the Verilog file (both the signal declarations and comments).\
    \       |  |  |  |  |  |\n| Then map every signal (including IO ports, wires,\
    \ and registers) defined    |  |  |  |  |  |\n| in Verilog with the description\
    \ in the specification. Finally, please       |  |  |  |  |  |\n| output each\
    \ signal in the following format:                                 |  |  |  | \
    \ |  |\n| [Signal name in Verilog]: Signal definition in Specification file  \
    \         |  |  |  |  |  |\n|                                                \
    \                             |  |  |  |  |  |\n\nFig. 5: Custom Instructions\
    \ for LLM 2 Signal Mapper\n\n## *D. Automatic Assertion Generation*\n\nWhile previous\
    \ research has delved into SVA generation using either traditional NLP techniques\
    \ at the pre-RTL stage\n\n#### **[Prompt for Signal Mapper]**\n\n- [Upload the\
    \ original SPEC file and the Verilog signal definition file] Here are the files\
    \ for the design specification and the signal definitions, please analyze the\
    \ two files carefully.\n- Please output each signal with the mapping relationship\n\
    \n**[Response from Signal Mapper]** Based on the analysis of both the design specification\
    \ and the Verilog file, here is the mapping of signals, wires, and regs in the\
    \ Verilog file to their respective descriptions in the specification:\n\n| (IO\
    \ port)                                                  |\n|------------------------------------------------------------|\n\
    | wb_clk_i: Master clock                                     |\n| wb_we_i: Write\
    \ enable input                                |\n| \u2026\u2026              \
    \                                           |\n| (Register)                  \
    \                               |\n| ctr: Control register                   \
    \                   |\n| sr: Status register                                 \
    \       |\n| \u2026\u2026                                                    \
    \     |\n| (Wire)                                                     |\n| en:\
    \ Enable state of the I2C core.                          |\n| ien: Interrupt enable\
    \ state of the I2C core                |\n| \u2026\u2026                     \
    \                                    |\n| (Unmapped)                         \
    \                        |\n| The additional elements such as wires and regs like\
    \        |\n| 'wb_wacc', 'sta' have been identified in the Verilog file. |\n|\
    \ However, their specific descriptions are not directly      |\n| found in the\
    \ provided section of the specification.        |\n\nFig. 6: Prompt and Response\
    \ Example of LLM 2 Signal Mapper\n\nor LLM-based approaches for RTL designs, these\
    \ methods have their limitations. NLP-based techniques demand careful analysis\
    \ of the syntax and semantics of assertion-related sentences, which limit their\
    \ adaptability to variations in sentence structures. LLM-based methods, typically\
    \ focused on the RTL stage, rely on HDL code and accompanying human-written comments\
    \ or properties, but their dependence on the unverified RTL code poses a risk\
    \ of generating inaccurate SVAs that could mislead the verification process.\n\
    \nTo address these challenges, our work introduces the LLM 3 SVA Generator, dedicated\
    \ to generating assertions for each signal utilizing the previously extracted\
    \ structural specifications and the established signal relationships.\n\nConsidering\
    \ the precise syntax and writing rules inherent to SVAs and the potential for\
    \ the original LLM failing to generate syntactically correct SVAs, as discussed\
    \ in [\\[8\\]](#page-8-9), we incorporate the Retrieval Augmented Generation (RAG)\
    \ technique to enhance the LLM's capability for SVA generation. This approach\
    \ is enriched by a knowledge database comprising tutorials and textbooks on SVA\
    \ and formal property verification [\\[33\\]](#page-8-23)\u2013[\\[35\\]](#page-8-24),\
    \ providing a robust foundation for the LLM to access and retrieve relevant SVA\
    \ knowledge based on the input query, thereby enhancing the quality of the generated\
    \ SVAs.\n\nBesides the RAG technique, we also provide custom instructions for\
    \ SVA Generator, shown in Fig. [7.](#page-4-0) After uploading the overall architecture\
    \ diagram of the design, for each signal, the extracted structural specifications\
    \ and the mapped signal relationship from the above two LLMs are provided. Then\
    \ the LLM is required to generate SVAs strictly according to the specification,\
    \ and as much and high coverage as possible. To guide the LLM to generate high-quality\
    \ SVAs, we define five SVA categories as follows:\n\nIn addition to the RAG technique,\
    \ we improve the SVA Generator with custom instructions, as illustrated in Fig.\
    \ [7.](#page-4-0) Upon inputting the overall architecture diagram of the design,\
    \ the LLM is provided with the structured specifications and mapped signal relationships\
    \ from the previous LLMs for each signal. Then the LLM is required to generate\
    \ SVAs that adhere strictly to the specifications, aiming for maximal quantity\
    \ and quality. To facilitate the generation of high-quality SVAs, we categorize\
    \ SVAs into three distinct groups, guiding the LLM toward producing comprehensive\
    \ and accurate assertions for effective verification.\n\n- Width: Check if the\
    \ signal bit width is satisfied with the specification.\n- Connectivity: Check\
    \ if the signal can be correctly exercised and also the value propagation among\
    \ all connected signals.\n- Function: Check if the function defined in the specification\
    \ is implemented as expected.\n\nBased on these well-designed SVA types, the customized\
    \ LLM can generate numerous SVAs for each signal. Fig. [8](#page-5-0) demonstrates\
    \ an example of generating SVAs for a signal.\n\n<span id=\"page-4-0\"></span>**[Custom\
    \ Instructions for SVA Generator]** \u2022 Please act as a professional VLSI verification\
    \ engineer. You can refer to the uploaded **knowledge base** about the SystemVerilog\
    \ assertion (SVA) and formal verification techniques. \u2022 Firstly, I'll upload\
    \ an workflow **diagram** of the design architecture. \u2022 Then every time I\
    \ provide the **specification of a signal** of the design, please write **all\
    \ the corresponding SVAs** based on the defined Verilog signals that benefit both\
    \ the RTL design and verification processes. Please note that don't generate signals\
    \ that do not exist in the signal declaration Verilog file. \u2022 Please make\
    \ sure that the generated SVAs have **no syntax error**, and strictly **follow\
    \ the function** of the given specification/description. \u2022 The generated\
    \ SVAs should include but not be limited to the following **types**: [*width*]:\
    \ Check the signal width using \\$bits operator [*connectivity*]: Check if the\
    \ signal can be correctly exercised and also the value propagation among all connected\
    \ signals. [*function*]: Check if the function defined in the specification is\
    \ implemented as expected.\n\nFig. 7: Custom Instructions for LLM 3 SVA Generator\n\
    \n## *E. Generated Assertion Evaluation*\n\nAfter the SVAs are generated, evaluating\
    \ their effectiveness is crucial. While some previous studies such as [\\[12\\\
    ]](#page-8-21), [\\[14\\]](#page-8-19) suggest using specific property checkers\
    \ for this purpose, such an approach is limited to particular design types like\
    \ protocols and processors and lacks generalizability to all VLSI designs. Other\
    \ methods like [\\[8\\]](#page-8-9) involve manual verification by engineers using\
    \ EDA tools, which is vulnerable to human error.\n\nIn our approach, we propose\
    \ leveraging the golden RTL implementations to assess the quality of the generated\
    \ SVAs. Our generation method is solely based on the specification file, and the\
    \ bug-free golden RTL designs serve as a robust benchmark to evaluate our generation\
    \ technique's efficacy.\n\nFor evaluation, we utilize the formal property verification\
    \ (FPV) method. The generated SVAs and the golden RTL\n\n#### <span id=\"page-5-0\"\
    ></span>**[Prompt for SVA Generator]**\n\n\u2022 [Upload the architecture diagram]\
    \ Here is the overall diagram workflow of the design architecture.\n\n![](_page_5_Figure_2.jpeg)\n\
    \n\u2022 [For each signal, upload the extracted specification and signal mapping]\
    \ Please generate SVAs for the signal control register (CTR).\n\n#### **[Response\
    \ from SVA Generator]**\n\nBased on the provided specification for the control\
    \ register (CTR), we can write SVAs to validate its behavior. Let's break down\
    \ the SVAs according to the types you specified:\n\n![](_page_5_Figure_6.jpeg)\n\
    \nFig. 8: Prompt and Response Example of LLM 3 SVA Generator\n\ndesigns are inputted\
    \ into a model checker tool. After executing FPV, we employ the following two\
    \ metrics to evaluate the quality of SVAs for each target signal:\n\n- Syntax:\
    \ Check if the generated SVAs have syntax errors.\n- FPV Pass/Fail: Given the\
    \ RTL designs are bug-free, an SVA that passes the FPV check is considered correct,\
    \ and conversely, a failure indicates an incorrect SVA.\n\n## *F. Proposed Benchmark*\n\
    \nRecognizing the absence of open-source benchmarks for evaluating LLMs in the\
    \ generation of SVAs from natural language specifications, we introduce a comprehensive\
    \ benchmark suite tailored for this purpose. Our benchmark consists of 20 open-source\
    \ designs, covering a diverse array of applications including microprocessors,\
    \ system-on-chip architectures, communication protocols, arithmetic units, and\
    \ cryptographic modules. For each design within the benchmark, the benchmark provides\
    \ the following components across three distinct files:\n\n- Specification: This\
    \ file contains the complete natural language specification for the design, offering\
    \ a detailed description of the system's intended architecture and functionality.\n\
    - Signal Definition: Presented in HDL code format, this file outlines the signal\
    \ declarations essential for the generation of SVAs. It includes definitions for\
    \ both input/output ports and internal signals, providing the foundation for assertion\
    \ generation.\n- Golden RTL Implementation: This file comprises the RTL design\
    \ implementations that are strictly implemented according to the specification.\
    \ The designs are verified to ensure it is free from bugs, serving as a reliable\
    \ standard for evaluating the accuracy and effectiveness of generated SVAs.\n\n\
    ## IV. EXPERIMENTAL RESULTS\n\n## *A. Experimental Setup*\n\nIn our study, the\
    \ original specification documents are provided in PDF format, including a variety\
    \ of multi-modal content including text, tables, and figures. The signal definition\
    \ files and the golden RTL designs are formatted in Verilog. To assess the quality\
    \ of the generated SVAs, we utilize Cadence JasperGold\xAE, one of the leading\
    \ commercial model checking tools. This evaluation leverages the FPV app in JasperGold\
    \ to ensure a thorough analysis.\n\nOur experimental setup involves the evaluation\
    \ of three types of LLMs using our developed generation and evaluation methodology:\n\
    \n- 1) GPT-3.5: This model is the freely available commercial version, GPT-3.5\
    \ Turbo, which supports a context window of up to 16K tokens.\n- 2) GPT-4: The\
    \ state-of-the-art commercial solution, GPT-4 Turbo, offers a 128K token context\
    \ window and multimodal capabilities, making it adept at handling the diverse\
    \ content found in specification documents.\n- 3) AssertLLM: Cutomized GPT-4 Turbo\
    \ by incorporating specialized techniques such as RAG and custom instructions,\
    \ tailoring the models specialized to the SVA generation task.\n\nIn our experimental\
    \ evaluation, we focus on the quality of the SVAs generated for each signal across\
    \ the designs. Note that all SVAs are produced from a single query to the LLMs\
    \ without any subsequent iterative modifications. The SVA containing unmapped\
    \ signals is considered an unsuccessful attempt at SVA generation. These SVAs\
    \ are identified and excluded by human engineers prior to the evaluation process.\n\
    \n## *B. Evaluation Metrics*\n\nTo conduct a thorough evaluation of the generated\
    \ SystemVerilog SVAs, we propose a set of metrics that align with our evaluation\
    \ methodology. This approach ensures a detailed assessment of the SVAs' quality\
    \ on both a per-signal and perdesign basis.\n\nFor each assertion type of an individual\
    \ signal, our evaluation includes the following metrics: (1) number of generated\n\
    \n<span id=\"page-6-0\"></span>\n\n|              |         |            |   \
    \                                                                |           \
    \              | AssertLLM    |          | GPT-4   | GPT-3.5                 \
    \                               |\n|--------------|---------|------------|-------------------------------------------------------------------|-------------------------|--------------|----------|---------|--------------------------------------------------------|\n\
    |              | Signal  |            | Assertion Evaluation (#. Generated/#.\
    \ Syntax Correct/#. FPV Pass) |                         |              |     \
    \     |         |                                                        |\n|\
    \ Name<br>Type |         | Width      | Connect.                             \
    \                             | Function                | Signal Total | Function\
    \ |         |                                                        |\n|    \
    \          | Clock   | wb clk i   | 1/1/1                                    \
    \                         |                         | /            | 1/1/1   \
    \ | 3/1/0   |                                                        |\n|    \
    \          | Reset   | wb rst i   | 1/1/1                                    \
    \                         |                         |              | 1/1/1   \
    \ | 3/1/0   | Can not handle<br>the original<br>specification files. |\n|    \
    \          |         | arst i     | 1/1/1                                    \
    \                         |                         | /            | 1/1/1   \
    \ | 3/1/0   |                                                        |\n|    \
    \          | Control | wb stb i   | 1/1/1                                    \
    \                         | 2/2/1<br>1/1/0<br>1/1/0 | /            | 3/3/2   \
    \ | 3/1/0   |                                                        |\n|    \
    \          |         | wb ack o   | 1/1/1                                    \
    \                         |                         |              | 2/2/1   \
    \ | 3/1/0   |                                                        |\n|    \
    \          |         | wb inta o  | 1/1/1                                    \
    \                         |                         |              | 2/2/1   \
    \ | 3/1/0   |                                                        |\n|    \
    \          | Data    | wb adr i   | 1/1/1                                    \
    \                         | /                       | /            | 1/1/1   \
    \ | 3/1/0   |                                                        |\n|    \
    \          |         | wb dat i   | 1/1/1                                    \
    \                         |                         |              | 1/1/1   \
    \ | 3/1/0   |                                                        |\n| IO Port\
    \      |         | wb cyc i   | 1/1/1                                        \
    \                     |                         |              | 1/1/1    | 3/1/0\
    \   |                                                        |\n|            \
    \  |         | wb dat o   | 1/1/1                                            \
    \                 |                         |              | 1/1/1    | 3/1/0\
    \   |                                                        |\n|            \
    \  |         | wb we i    | 1/1/1                                            \
    \                 |                         |              | 1/1/1    | 3/1/0\
    \   |                                                        |\n|            \
    \  |         | scl pad i  | 1/1/1                                            \
    \                 |                         |              | 1/1/1    | 3/1/0\
    \   |                                                        |\n|            \
    \  |         | scl pad o  | 1/1/1                                            \
    \                 |                         |              | 1/1/1    | 3/1/0\
    \   |                                                        |\n|            \
    \  |         | sda pad i  | 1/1/1                                            \
    \                 |                         |              | 1/1/1    | 3/1/0\
    \   |                                                        |\n|            \
    \  |         | sda pad o  | 1/1/1                                            \
    \                 |                         |              | 1/1/1    | 3/1/0\
    \   |                                                        |\n|            \
    \  |         | scl pad oe | 1/1/1                                            \
    \                 |                         |              | 1/1/1    | 3/1/0\
    \   |                                                        |\n|            \
    \  |         | sda pad oe | 1/1/1                                            \
    \                 |                         |              | 1/1/1    | 3/1/0\
    \   |                                                        |\n|            \
    \  | Control | ctr        | 1/1/1                                            \
    \                 | 4/4/4                   | 5/5/5        | 10/10/10 | 3/1/1\
    \   |                                                        |\n| Register   \
    \  |         | sr         | 1/1/1                                            \
    \                 | 6/6/5                   | 8/8/8        | 15/15/14 | 3/1/1\
    \   |                                                        |\n|            \
    \  | Data    | prer       | 1/1/1                                            \
    \                 | /                       | 3/3/1        | 4/4/2    | 4/1/1\
    \   |                                                        |\n|            \
    \  |         | txr        | 1/1/1                                            \
    \                 | /                       | 1/1/1        | 2/2/2    | 3/1/1\
    \   |                                                        |\n|            \
    \  |         | rxr        | 1/1/1                                            \
    \                 | /                       | 1/1/1        | 2/2/2    | 3/1/1\
    \   |                                                        |\n|            \
    \  |         | cr         | 1/1/1                                            \
    \                 | /                       | 1/1/1        | 2/2/2    | 4/1/1\
    \   |                                                        |\n| Design Total\
    \ |         |            | 23/23/23                                          \
    \                | 16/16/12                | 17/17/15     | 56/56/50 | 71/23/6\
    \ |                                                        |\n|              |\
    \         |            | 100%/100%                                           \
    \              | 100%/75%                | 100%/88%     | 100%/89% | 32%/8%  |\
    \                                                        |\n\nTABLE II: Evaluation\
    \ of the generated SVAs for design \"I2C\". AssertLLM generates 56 properties\
    \ for a total of 23 signals, with 23 for bit-width, 16 for connectivity, and 17\
    \ for function. 89% of these generated SVAs are evaluated to be correct both syntactically\
    \ and functionally.\n\nSVAs. (2) number of syntax-correct SVAs. (3) number of\
    \ FPVpassed SVAs.\n\nOnce the evaluation for each signal is complete, we aggregate\
    \ the statistics of the generated SVAs for each design and then calculate the\
    \ proportion of these SVAs that are syntactically correct and passed the FPV checks,\
    \ respectively.\n\n# *C. Assertion Generation Quality*\n\nTo illustrate the efficacy\
    \ of AssertLLM, we apply it to a comprehensive design case: the \"I2C\" protocol.\
    \ The I2C specification describes the architecture of a serial communication bus\
    \ that provides a simple and efficient method of data exchange between devices.\
    \ The complete specification document for the \"I2C\" design is structured into\
    \ six main sections, similar to those illustrated in Subsection [II-A.](#page-1-1)\
    \ Note that for each signal, the specification is unstructured and mainly across\
    \ the IO ports, registers, and operation sections.\n\nAdditionally, we provide\
    \ the signal definition file containing not only the IO ports and architectural\
    \ registers but also all the internal wires and registers defined for detailed\
    \ RTL implementation.\n\nTo facilitate the generation of SVAs, the AssertLLM framework\
    \ processes specification and signal definition files using two specialized LLMs:\
    \ SPEC Analyzer for extracting structured specifications for each signal and Signal\
    \ Mapper for mapping signal relationships. Then the SVA Generator is utilized\
    \ to automatically generate three types of SVAs based on the processed information\
    \ from the first two LLMs.\n\nThe specification for the \"I2C\" design defines\
    \ 23 signals, comprising 17 IO ports and 6 architecture-level registers. For the\
    \ IO ports, we categorize them into 4 functional types: clock, reset, control\
    \ signal, and data signal. The architecture-level registers are similarly categorized,\
    \ based on their functionality, into control and data types.\n\nThe evaluation\
    \ of SVAs generated by our AsserLLM is demonstrated in Table [II.](#page-6-0)\
    \ For each signal, we first verify each type of the generated SVAs separately.\
    \ Then we summarize all the SVAs to provide a design-level inspection. We have\
    \ multiple interesting observations in Table [II](#page-6-0) as follows:\n\n-\
    \ AssertLLM demonstrates excellent proficiency in generating SVAs for bit-width\
    \ checking. Although bit-width checking is relatively straightforward, it is crucial\
    \ for early design stages to avoid potential bugs that cannot be checked through\
    \ a syntax checker.\n- For the connectivity SVAs, since clear guidelines are provided\
    \ only for control signals within the architecture-level specification documents,\
    \ AssertLLM can only generate connectivity SVAs for them. The connectivity of\
    \ data signals often depend on specific internal signals defined in the RTL implementation,\
    \ which are not detailed in the\n\nspecification document.\n\n- For the function\
    \ SVAs, the specification provides explicit details only for registers. The descriptions\
    \ of IO ports primarily focus on data transformation functions, without extensive\
    \ functional details, which results in the lack of related SVAs.\n- For the quantity\
    \ of generated SVAs, AssertLLM produced a total of 56 SVAs, with 23 dedicated\
    \ to width checking, 16 to connectivity checking, and 17 to function checking.\n\
    - Regarding the quality of generated SVAs, all SVAs related to bit-width checking\
    \ performed correctly. However, a minor portion of connectivity and function SVAs\
    \ contained errors, attributed mainly to misinterpretations of the specification\
    \ or LLM-generated hallucinations. Overall, AssertLLM achieved a correct accuracy\
    \ rate of 89% for the entire I2C design.\n\n## *D. Ablation Study*\n\nIn addition\
    \ to assessing AssertLLM's performance, we conducted an ablation study to compare\
    \ the SVA generation capabilities of the original GPT-4 and GPT-3.5 models without\
    \ the additional techniques. This study provides insights into the effectiveness\
    \ of enhancements incorporated in AssertLLM.\n\nThe evaluation results for the\
    \ two commercial solutions are demonstrated in Table [II.](#page-6-0) For GPT-3.5,\
    \ since the lack of multimodal processing capabilities, it is unable to directly\
    \ generate SVAs from the original, multi-modal specification files.\n\nWhen utilizing\
    \ the original GPT-4, the unstructured specification file and signal definitions\
    \ are provided, with prompts designed to guide SVA generation to the best of the\
    \ model's ability. The generation results indicate that the absence of a mechanism\
    \ to extract structured specifications for each signal significantly hampers GPT-4's\
    \ ability to compile all useful information for SVA generation, resulting in a\
    \ maximum of only 4 SVAs generated per signal. Additionally, without specific\
    \ assertion type guidance, GPT-4 only generates functional assertions.\n\nFor\
    \ the SVA quality of commercial solutions, the original GPT-4 model tends to produce\
    \ SVAs with syntax errors, similar to observations made in the previous study\
    \ [\\[8\\]](#page-8-9). This issue is addressed in AssertLLM through the application\
    \ of RAG techniques, which enrich the model with specific knowledge on SVA and\
    \ Formal Property Verification (FPV). In the evaluation results, GPT-4 failed\
    \ to generate any correct SVAs for IO ports and only succeeded in creating accurate\
    \ reset check assertions for registers, leading to an overall correct proportion\
    \ of only 8%.\n\n## V. DISCUSSION\n\n## *A. Coverage in SVA Evaluation*\n\nSome\
    \ works [\\[8\\]](#page-8-9) propose to leverage the coverage metric, especially\
    \ the cone-of-influence (COI) coverage to evaluate the quality of generated SVAs.\
    \ COI coverage relies on analyzing the signals exercised during simulation or\
    \ formal verification, which significantly involves internal signals within the\
    \ design.\n\nGiven that our SVA generation process is based solely on the information\
    \ available in the specification documents, which typically detail external interfaces\
    \ like IO ports and architectural-level registers rather than internal signals,\
    \ COI coverage does not align well with our evaluation criteria. This coverage\
    \ metric assumes a level of design implementation detail that goes beyond the\
    \ scope of natural language specifications, making it less applicable for assessing\
    \ the completeness or effectiveness of SVAs generated at this pre-RTL stage.\n\
    \n## *B. Evaluating and Enhancing Specification Quality with AssertLLM*\n\nThe\
    \ generation of high-quality SVAs from natural language specifications relies\
    \ not only on the capabilities of LLMs but also on the intrinsic quality of the\
    \ specification documents themselves. A specification that provides only the basic\
    \ information of signals, such as their names and simple descriptions, without\
    \ delving into detailed functionalities or connectivities, inherently limits the\
    \ potential for generating meaningful SVAs, regardless of the power of the LLMs\
    \ employed. Conversely, specifications that offer comprehensive details, including\
    \ clear definitions of signal functionalities and connectivities, can facilitate\
    \ the generation of SVAs even with relatively simple LLMs.\n\nHere we identify\
    \ a novel application for AssertLLM beyond its primary role in verification: utilizing\
    \ AssertLLM as a tool for assessing the quality of natural language specifications.\
    \ This application leverages AssertLLM's ability to process and interpret specification\
    \ documents to determine their *verification-friendliness*. Specifications that\
    \ enable AssertLLM to generate a broad and accurate range of SVAs can be considered\
    \ high-quality and well-suited for verification purposes. This approach to evaluating\
    \ specification quality offers several benefits:\n\n- Identifying Gaps between\
    \ Specifications and Verification: AssertLLM can highlight contents within a specification\
    \ that lack sufficient detail for SVA generation, guiding architects to provide\
    \ more comprehensive information.\n- Enhancing Verification: Ensuring specifications\
    \ are verification-friendly can potentially reduce the time and effort required\
    \ for verification.\n- Standardizing Specification Writing: The feedback from\
    \ AssertLLM can help establish best practices for writing specifications to facilitate\
    \ the automated verification, and promote consistency across design stages.\n\n\
    ## VI. CONCLUSION\n\nIn this study, we introduce AssertLLM, an automated framework\
    \ designed for generating assertions from entire specification documents. AssertLLM\
    \ breaks down the intricate task into three sequential phases, leveraging specialized\
    \ LLMs for structural specification extraction, signal definition mapping, and\
    \ assertion creation. We also offer an open-source benchmark to evaluate the efficacy\
    \ of assertion generation. Evaluating AssertLLM on a comprehensive design with\
    \ 23 signals revealed that 89% of the assertions generated were accurate both\
    \ syntactically and functionally. We also discuss the potential of using AssertLLM\
    \ to evaluate and enhance the quality of specifications.\n\n## REFERENCES\n\n\
    - <span id=\"page-8-0\"></span>[1] H. Witharana, Y. Lyu, S. Charles, and P. Mishra,\
    \ \"A survey on assertionbased hardware verification,\" *ACM Computing Surveys\
    \ (CSUR)*, vol. 54, no. 11s, pp. 1\u201333, 2022.\n- <span id=\"page-8-1\"></span>[2]\
    \ S. Germiniani and G. Pravadelli, \"Harm: a hint-based assertion miner,\" *IEEE\
    \ Transactions on Computer-Aided Design of Integrated Circuits and Systems*, vol.\
    \ 41, no. 11, pp. 4277\u20134288, 2022.\n- [3] A. Danese, N. D. Riva, and G. Pravadelli,\
    \ \"A-team: Automatic templatebased assertion miner,\" in *Proceedings of the\
    \ 54th Annual Design Automation Conference 2017*, 2017, pp. 1\u20136.\n- <span\
    \ id=\"page-8-2\"></span>[4] S. Vasudevan, D. Sheridan, S. Patel, D. Tcheng, B.\
    \ Tuohy, and D. Johnson, \"Goldmine: Automatic assertion generation using data\
    \ mining and static analysis,\" in *2010 Design, Automation & Test in Europe Conference\
    \ & Exhibition (DATE 2010)*. IEEE, 2010, pp. 626\u2013629.\n- <span id=\"page-8-3\"\
    ></span>[5] M. Orenes-Vera, A. Manocha, D. Wentzlaff, and M. Martonosi, \"Autosva:\
    \ Democratizing formal verification of rtl module interactions,\" in *2021 58th\
    \ ACM/IEEE Design Automation Conference (DAC)*. IEEE, 2021, pp. 535\u2013540.\n\
    - <span id=\"page-8-4\"></span>[6] W. Fang, G. Hu, and H. Zhang, \"r-map: Relating\
    \ implementation and specification in hardware refinement checking,\" *IEEE Transactions\
    \ on Computer-Aided Design of Integrated Circuits and Systems*, 2023.\n- <span\
    \ id=\"page-8-5\"></span>[7] R. Kande, H. Pearce, B. Tan, B. Dolan-Gavitt, S.\
    \ Thakur, R. Karri, and J. Rajendran, \"Llm-assisted generation of hardware assertions,\"\
    \ *arXiv preprint arXiv:2306.14027*, 2023.\n- <span id=\"page-8-9\"></span>[8]\
    \ M. Orenes-Vera, M. Martonosi, and D. Wentzlaff, \"Using llms to facilitate formal\
    \ verification of rtl,\" *arXiv e-prints*, pp. arXiv\u20132309, 2023.\n- <span\
    \ id=\"page-8-7\"></span>[9] C. Sun, C. Hahn, and C. Trippel, \"Towards improving\
    \ verification productivity with circuit-aware translation of natural language\
    \ to systemverilog assertions,\" in *First International Workshop on Deep Learning-aided\
    \ Verification (DAV)*, 2023.\n- <span id=\"page-8-8\"></span>[10] C. B. Harris\
    \ and I. G. Harris, \"Glast: Learning formal grammars to translate natural language\
    \ specifications into hardware assertions,\" in *2016 Design, Automation & Test\
    \ in Europe Conference & Exhibition (DATE)*. IEEE, 2016, pp. 966\u2013971.\n-\
    \ [11] R. Krishnamurthy and M. S. Hsiao, \"Controlled natural language framework\
    \ for generating assertions from hardware specifications,\" in *2019 IEEE 13th\
    \ International Conference on Semantic Computing (ICSC)*. IEEE, 2019, pp. 367\u2013\
    370.\n- <span id=\"page-8-21\"></span>[12] J. Zhao and I. G. Harris, \"Automatic\
    \ assertion generation from natural language specifications using subtree analysis,\"\
    \ in *2019 Design, Automation & Test in Europe Conference & Exhibition (DATE)*.\
    \ IEEE, 2019, pp. 598\u2013601.\n- [13] R. Krishnamurthy and M. S. Hsiao, \"Ease:\
    \ Enabling hardware assertion synthesis from english,\" in *Rules and Reasoning:\
    \ Third International Joint Conference, RuleML+ RR 2019, Bolzano, Italy, September\
    \ 16\u201319, 2019, Proceedings 3*. Springer, 2019, pp. 82\u201396.\n- <span id=\"\
    page-8-19\"></span>[14] S. J. Frederiksen, J. Aromando, and M. S. Hsiao, \"Automated\
    \ assertion generation from natural language specifications,\" in *2020 IEEE International\
    \ Test Conference (ITC)*. IEEE, 2020, pp. 1\u20135.\n- <span id=\"page-8-22\"\
    ></span>[15] O. Keszocze and I. G. Harris, \"Chatbot-based assertion generation\
    \ from natural language specifications,\" in *2019 Forum for Specification and\
    \ Design Languages (FDL)*. IEEE, 2019, pp. 1\u20136.\n- <span id=\"page-8-20\"\
    ></span>[16] G. Parthasarathy, S. Nanda, P. Choudhary, and P. Patil, \"Spectosva:\
    \ Circuit specification document to systemverilog assertion translation,\" in\
    \ *2021 Second Document Intelligence Workshop at KDD*.\n- <span id=\"page-8-6\"\
    ></span>[17] F. Aditi and M. S. Hsiao, \"Hybrid rule-based and machine learning\
    \ system for assertion generation from natural language specifications,\" in *2022\
    \ IEEE 31st Asian Test Symposium (ATS)*. IEEE, 2022, pp. 126\u2013131.\n- <span\
    \ id=\"page-8-10\"></span>[18] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya,\
    \ F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat *et al.*, \"\
    Gpt-4 technical report,\" *arXiv preprint arXiv:2303.08774*, 2023.\n- <span id=\"\
    page-8-11\"></span>[19] S. Liu, W. Fang, Y. Lu, Q. Zhang, H. Zhang, and Z. Xie,\
    \ \"Rtlcoder: Outperforming gpt-3.5 in design rtl generation with our open-source\
    \ dataset and lightweight solution,\" *arXiv preprint arXiv:2312.08617*, 2023.\n\
    - [20] J. Blocklove, S. Garg, R. Karri, and H. Pearce, \"Chip-chat: Challenges\
    \ and opportunities in conversational hardware design,\" *arXiv preprint arXiv:2305.13243*,\
    \ 2023.\n- [21] Y. Lu, S. Liu, Q. Zhang, and Z. Xie, \"Rtllm: An open-source benchmark\
    \ for design rtl generation with large language model,\" *arXiv preprint arXiv:2308.05345*,\
    \ 2023.\n- [22] M. Liu, N. Pinckney, B. Khailany, and H. Ren, \"Verilogeval: Evaluating\
    \ large language models for verilog code generation,\" *arXiv preprint arXiv:2309.07544*,\
    \ 2023.\n- [23] S. Thakur, B. Ahmad, Z. Fan, H. Pearce, B. Tan, R. Karri, B. Dolan-Gavitt,\
    \ and S. Garg, \"Benchmarking large language models for automated verilog rtl\
    \ code generation,\" in *DATE*, 2023.\n- [24] S. Thakur, J. Blocklove, H. Pearce,\
    \ B. Tan, S. Garg, and R. Karri, \"Autochip: Automating hdl generation using llm\
    \ feedback,\" *arXiv preprint arXiv:2311.04887*, 2023.\n- [25] M. Nair, R. Sadhukhan,\
    \ and D. Mukhopadhyay, \"Generating secure hardware using chatgpt resistant to\
    \ cwes,\" *Cryptology ePrint Archive*, 2023.\n- <span id=\"page-8-12\"></span>[26]\
    \ M. Liu, T.-D. Ene, R. Kirby, C. Cheng, N. Pinckney, R. Liang, J. Alben, H. Anand,\
    \ S. Banerjee, I. Bayraktaroglu *et al.*, \"Chipnemo: Domainadapted llms for chip\
    \ design,\" *arXiv preprint arXiv:2311.00176*, 2023.\n- <span id=\"page-8-13\"\
    ></span>[27] Y. Tsai, M. Liu, and H. Ren, \"Rtlfixer: Automatically fixing rtl\
    \ syntax errors with large language models,\" *arXiv preprint arXiv:2311.16543*,\
    \ 2023.\n- <span id=\"page-8-14\"></span>[28] Z. He, H. Wu, X. Zhang, X. Yao,\
    \ S. Zheng, H. Zheng, and B. Yu, \"Chateda: A large language model powered autonomous\
    \ agent for eda,\" in *MLCAD Workshop*, 2023.\n- <span id=\"page-8-15\"></span>[29]\
    \ Y. Fu, Y. Zhang, Z. Yu, S. Li, Z. Ye, C. Li, C. Wan, and Y. Lin, \"Gpt4aigchip:\
    \ Towards next-generation ai accelerator design automation via large language\
    \ models,\" *arXiv preprint arXiv:2309.10730*, 2023.\n- <span id=\"page-8-16\"\
    ></span>[30] Z. Yan, Y. Qin, X. S. Hu, and Y. Shi, \"On the viability of using\
    \ llms for sw/hw co-design: An example in designing cim dnn accelerators,\" *arXiv\
    \ preprint arXiv:2306.06923*, 2023.\n- <span id=\"page-8-17\"></span>[31] B. Ahmad,\
    \ S. Thakur, B. Tan, R. Karri, and H. Pearce, \"Fixing hardware security bugs\
    \ with large language models,\" *arXiv preprint arXiv:2302.01215*, 2023.\n- <span\
    \ id=\"page-8-18\"></span>[32] M. Li, W. Fang, Q. Zhang, and Z. Xie, \"Specllm:\
    \ Exploring generation and review of vlsi design specification with large language\
    \ model,\" *arXiv preprint arXiv:2401.13266*, 2024.\n- <span id=\"page-8-23\"\
    ></span>[33] E. Seligman, T. Schubert, and M. A. K. Kumar, *Formal verification:\
    \ an essential toolkit for modern VLSI design*. Elsevier, 2023.\n- [34] A. B.\
    \ Mehta, *SystemVerilog Assertions and Functional Coverage*. Springer, 2020.\n\
    - <span id=\"page-8-24\"></span>[35] S. Vijayaraghavan and M. Ramanathan, *A practical\
    \ guide for SystemVerilog assertions*. Springer Science & Business Media, 2005."
  paper_id: 11
  scores:
    originality:
      raw: 4.2
      reason: The paper presents a novel approach to utilizing Apple unified memory
        for Monte Carlo neutron transport, which is a relatively unexplored area in
        the field.
    technical_soundness:
      raw: 4.0
      reason: The methodology appears robust, but further details on experimental
        validation and performance metrics are needed to fully assess its soundness.
    clarity:
      raw: 3.5
      reason: While the introduction provides a good overview, some sections lack
        clarity and could benefit from more detailed explanations and definitions.
    significance:
      raw: 4.3
      reason: The findings could significantly impact the efficiency of Monte Carlo
        simulations in reactor physics, making it a valuable contribution to the field.
    relevance:
      raw: 4.5
      reason: The topic is highly relevant to current trends in computational physics
        and GPU utilization, addressing a critical need for improved simulation techniques.
  total_score: 4.05
  tier: C
- title: "Green Adaptation of Real-Time Web Services for Industrial CPS within a\n\
    \  Cloud Environment"
  abstract: 'In recent years, autonomous vehicles have attracted the attention of
    many

    research groups, both in academia and business, including researchers from

    leading companies such as Google, Uber and Tesla. This type of vehicles are

    equipped with systems that are subject to very strict requirements, essentially

    aimed at performing safe operations -- both for potential passengers and

    pedestrians -- as well as carrying out the processing needed for decision

    making in real time. In many instances, general-purpose processors alone cannot

    ensure that these safety, reliability and real-time requirements are met, so it

    is common to implement heterogeneous systems by including accelerators. This

    paper explores the acceleration of a line detection application in the

    autonomous car environment using a heterogeneous system consisting of a

    general-purpose RISC-V core and a domain-specific accelerator. In particular,

    the application is analyzed to identify the most computationally intensive

    parts of the code and it is adapted accordingly for more efficient processing.

    Furthermore, the code is executed on the aforementioned hardware platform to

    verify that the execution effectively meets the existing requirements in

    autonomous vehicles, experiencing a 3.7x speedup with respect to running

    without accelerator.'
  url: http://arxiv.org/abs/2402.00496v1
  keywords: ''
  document: "# Optimization of a Line Detection Algorithm for Autonomous Vehicles\
    \ on a RISC-V with Accelerator\n\nOptimizacion de un Algoritmo de Detecci \xB4\
    \ on de L \xB4 \xB4\u0131neas para Veh\xB4\u0131culos Autonomos en un \xB4 RISC-V\
    \ con Acelerador\n\nMar\xB4\u0131a Jose Belda \xB4 1 [,](https://orcid.org/0000-0002-2870-7679)\
    \ Katzalin Olcoz<sup>1</sup> [,](https://orcid.org/0000-0002-1821-124X) Fernando\
    \ Castro<sup>1</sup> [,](https://orcid.org/0000-0002-2773-3023) and Francisco\
    \ Tirado<sup>1</sup>\n\n> <sup>1</sup>*Complutense University of Madrid, Madrid\
    \ 28040, Espana\u02DC* {mbelda,katzalin,fcastror,ptirado}@ucm.es\n\n# Abstract\n\
    \nIn recent years, autonomous vehicles have attracted the attention of many research\
    \ groups, both in academia and business, including researchers from leading companies\
    \ such as Google, Uber and Tesla. This type of vehicles are equipped with systems\
    \ that are subject to very strict requirements, essentially aimed at performing\
    \ safe operations \u2013both for potential passengers and pedestrians\u2013 as\
    \ well as carrying out the processing needed for decision making in real time.\
    \ In many instances, general-purpose processors alone cannot ensure that these\
    \ safety, reliability and real-time requirements are met, so it is common to implement\
    \ heterogeneous systems by including accelerators. This paper explores the acceleration\
    \ of a line detection application in the autonomous car environment using a heterogeneous\
    \ system consisting of a general-purpose RISC-V core and a domain-specific accelerator.\
    \ In particular, the application is analyzed to identify the most computationally\
    \ intensive parts of the code and it is adapted accordingly for more efficient\
    \ processing. Furthermore, the code is executed on the aforementioned hardware\
    \ platform to verify that the execution effectively meets the existing requirements\
    \ in autonomous vehicles, experiencing a 3.7x speedup with respect to running\
    \ without accelerator.\n\nKeywords: Autonomous vehicles, Firesim, Image processing,\
    \ Matrix accelerator, RISC-V\n\n# Resumen\n\nEn los ultimos a \xB4 nos los veh\
    \ \u02DC \xB4\u0131culos autonomos est \xB4 an\xB4 atrayendo la atencion de muchos\
    \ grupos de investi- \xB4 gacion, tanto del \xB4 ambito acad \xB4 emico como del\
    \ em- \xB4 presarial, entre los que se incluyen investigadores pertenecientes\
    \ a empresas punteras como Google, Uber o Tesla. Los sistemas de los que estan\
    \ dota- \xB4 dos este tipo de veh\xB4\u0131culos estan sometidos a requisitos\
    \ \xB4 muy estrictos relacionados esencialmente con la realizacion de operaciones\
    \ seguras, tanto para los poten- \xB4 ciales pasajeros como para los peatones,\
    \ as\xB4\u0131 como con\n\nque el procesamiento necesario para la toma de decisiones\
    \ se realice en tiempo real. En muchas ocasiones, los procesadores de proposito\
    \ general no pueden por \xB4 s\xB4\u0131 solos garantizar el cumplimiento de estos\
    \ requisitos de seguridad, fiabilidad y tiempo real, por lo que es comun implementar\
    \ sistemas heterog \xB4 eneos mediante \xB4 la inclusion de aceleradores. En este\
    \ art \xB4 \xB4\u0131culo se explora la aceleracion de una aplicaci \xB4 on de\
    \ detecci \xB4 on de \xB4 l\xB4\u0131neas en el entorno de veh\xB4\u0131culos\
    \ autonomos utilizando \xB4 para ello un sistema heterogeneo formado por un core\
    \ \xB4 RISC-V de proposito general y un acelerador de do- \xB4 minio espec\xB4\
    \u0131fico. En particular, se analiza dicha aplicacion para identificar las partes\
    \ del c \xB4 odigo m \xB4 as cos- \xB4 tosas computacionalmente y se adapta el\
    \ codigo para \xB4 un procesamiento mas eficiente. Adem \xB4 as, se ejecuta \xB4\
    \ dicho codigo en la mencionada plataforma hardware \xB4 y se comprueba que su\
    \ procesamiento efectivamente cumple con los requisitos presentes en los veh\xB4\
    \u0131culos autonomos, experimentando una reducci \xB4 on de 3.7x en \xB4 su tiempo\
    \ de ejecucion con respecto a su ejecuci \xB4 on sin \xB4 acelerador.\n\nPalabras\
    \ claves: Veh\xB4\u0131culos autonomos, Firesim, \xB4 Procesamiento de imagenes,\
    \ Acelerador de matrices, \xB4 RISC-V\n\n# 1 Introduction\n\nIn the technological\
    \ era in which we live, we every day strive to make all the usual tasks as automatic\
    \ as possible in order to gain free time. In addition, we try to achieve scenarios\
    \ that are impossible right now, such as smarter power grids, fully autonomous\
    \ vehicles or smart cities. This is why the Internet of Things (IoT) arises, as\
    \ we need new technologies to design these systems. Most of them are on-board\
    \ systems, so they need to get a trade-off between power consumption and delivered\
    \ performance. In particular, in this work we focus on autonomous vehicles.\n\n\
    Autonomous driving systems aim to enable vehicles to drive on the road without\
    \ human intervention [\\[1,](#page-10-0) [2,](#page-10-1) [3\\]](#page-10-2).\
    \ Therefore, these systems must guarantee the safety and integrity of the vehicle,\
    \ for which they must take a series of decisions in real time, including\n\nmoving\
    \ the steering wheel to ensure that the correct trajectory is followed, detecting\
    \ obstacles in the path (pedestrians, animals, objects...), activating the braking\
    \ mechanism when necessary and others. For this purpose, it is essential that\
    \ the vehicle has a camera that records images of the route and processes them\
    \ in real time to ensure the correct and safe operation of the vehicle. This image\
    \ processing requires considerable computing power, but at the same time, when\
    \ talking about on-board systems, it is essential to keep energy consumption at\
    \ low levels so the vehicle does not loose autonomy [\\[4\\]](#page-10-3).\n\n\
    For these reasons, autonomous vehicles require onboard automatic systems to process\
    \ the recorded images that allow certain operations such as line and edge detection.\
    \ Currently, the most widely used algorithms for this type of processing require\
    \ high performance and their basic kernel is matrix and vector multiplication.\
    \ It is therefore highly desirable that this type of algorithms could be executed\
    \ in one of the many domain specific accelerators that have emerged in recent\
    \ years.\n\nIn this paper we propose to accelerate a line detection application\
    \ employed in autonomous cars by using different heterogeneous systems made up\
    \ of a general-purpose RISC-V core working at low frequency and a domain-specific\
    \ accelerator. For this purpose, the application is deeply analyzed in order to\
    \ identify the computationally intensive parts of the code and adapted consequently\
    \ for a more efficient processing. As it will be explained in Section 3, the hardware\
    \ platform used in this work includes, on the one hand, a general-purpose BOOM\
    \ processor, which is an out-of-order RISC-V core [\\[5\\]](#page-10-4), and on\
    \ the other hand, the Gemmini [\\[6\\]](#page-10-5) accelerator, specifically\
    \ designed for matrix multiplication. This platform was chosen because the RISC-V\
    \ architecture, in addition to being open source, allows the integration of accelerators\
    \ and their potential adaptation in a very simple way. Furthermore, the RISC-V\
    \ instruction set architecture (ISA) is highly modular, allowing to choose exactly\
    \ the functionalities needed, which is especially useful in IoT environments.\n\
    \nThis paper leverages two image processing algorithms: 1) the Canny algorithm\
    \ for edge detection of an image, and 2) the Hough transform, oriented to find\
    \ imperfect instances of objects within a certain class of shapes by means of\
    \ a voting procedure. In Section 4 we perform a detailed analysis of both algorithms\
    \ codes, in order to identify the computational load of the different functions\
    \ included in these programs, as well as the available parallelism. Moreover,\
    \ we schedule some functions to run on the accelerator, while the rest of the\
    \ algorithm is executed on the processor, aimed to optimize the total execution\
    \ time and consequently to meet the strict requirements of performance, consumption\
    \ and safety imposed by autonomous vehicles. The experimental evaluation carried\
    \ out in\n\nSection 5 reports a speedup of 3.7x when executing these algorithms\
    \ with respect to the baseline where no accelerator is employed. Finally, Section\
    \ 6 concludes the paper.\n\n# 2 Basic notions and state of the art\n\nIn this\
    \ section we explain some basic notions related to autonomous vehicles. We also\
    \ provide details on the RISC-V-based development environment that we employ,\
    \ including the tools used that make it possible the evaluation of the proposal\
    \ presented in this paper.\n\n## 2.1 Autonomous vehicles\n\nAutonomous vehicles\
    \ are equipped with several sensors, as shown in Fig. [1,](#page-2-0) including\
    \ video cameras, which are responsible for obtaining the data that serve as input\
    \ to the processing system. The purpose of this data processing is to recognize\
    \ the environment which the vehicle is driving through, and as a result, to make\
    \ the appropriate decisions at any time, so as to ensure that the vehicle can\
    \ reach its destination efficiently and safely. In this aspect, autonomous vehicles\
    \ have levels of driving automation from 0 (No automation) to 5 (Full automation),\
    \ as explained in [\\[7\\]](#page-11-0). In the first levels, from 0 to 2, the\
    \ vehicle has very little capacity to act (in level 2 it can only perform steering\
    \ and acceleration) and all the responsibility lies on the driver. In contrast,\
    \ the automatic system monitors the driving environment in levels 3 to 5, being\
    \ this last one the ideal scenario in which the vehicle is completely autonomous,\
    \ even not providing controls for the driver. So, there is a gap between levels\
    \ 2 and 3. Between these levels there is also a technological gap, since generating\
    \ hardware and software capable of monitoring the environment in real time becomes\
    \ significantly difficult. However, this gap is progressively disappearing and\
    \ this work aims to contribute to this.\n\nNotably, certain safety decisions are\
    \ related to the correct recognition of the trajectory to be followed by the vehicle,\
    \ based on the images recorded by the camera. In addition to allowing the car\
    \ to follow the correct route, this functionality also involves restricting the\
    \ likelihood of an accident. For this purpose, computer vision algorithms are\
    \ commonly used in these processing systems [\\[9,](#page-11-1) [10\\]](#page-11-2)\
    \ and, in particular, quite approaches use Canny algorithm to detect edges combined\
    \ with the Hough transform to detect road lines [\\[11,](#page-11-3) [12\\]](#page-11-4).\
    \ Therefore, in this paper we focus on improving the performance of these algorithms\
    \ which are the basis of lane detection. The problem with these algorithms is\
    \ their very high computational cost. In addition to this, there is a need for\
    \ data processing to be performed in real time so that the vehicle could react\
    \ with immediacy to changing situations that may occur during the journey. It\
    \ is also highly desirable that the energy consumption associated with such processing\n\
    \n![](_page_2_Figure_0.jpeg)\n\n<span id=\"page-2-0\"></span>Figure 1: Integrated\
    \ sensors on an autonomous vehicle [\\[8\\]](#page-11-5).\n\nwas as low as possible,\
    \ so that the vehicle's autonomy was not affected.\n\nAutonomous driving systems\
    \ are essentially composed of three classes of sub-systems [\\[1,](#page-10-0)\
    \ [2\\]](#page-10-1): *scene recognition*, *route planning* and *vehicle control*,\
    \ consisting of a set of algorithms each. In particular, as shown in Fig. [2,](#page-2-1)\
    \ *scene recognition*, the class in which this article falls, comprises three\
    \ essential tasks, namely 1) *localization*, which precisely establishes the vehicle's\
    \ location, 2) *object detection*, which identifies objects of interest in the\
    \ vehicle's environment (such as other vehicles, pedestrians or road signs, with\
    \ the aforementioned objective of avoiding accidents and also traffic violations),\
    \ and 3) *object tracking*, which, since the object detection algorithm is carried\
    \ out on each frame of the image, is responsible for relating its results to other\
    \ frames in order to predict the trajectories of moving objects. These three tasks\
    \ account for a very high percentage of the total computation time required [\\\
    [1\\]](#page-10-0) and therefore constitute bottlenecks that significantly limit\
    \ the ability of conventional processors to satisfy the existing restrictions\
    \ in the design of this type of systems. For this reason, it is being proposed\
    \ to incorporate some type of accelerator to the on-board processing systems that\
    \ helps the processor to fulfill the strict time limits in which it must operate.\n\
    \n## 2.2 RISC-V-based development environment\n\nIn order to carry out the implementation\
    \ and evaluation of our proposal, which will be explained in the following section,\
    \ a series of software tools have been used, as detailed next:\n\n![](_page_2_Figure_7.jpeg)\n\
    \n<span id=\"page-2-1\"></span>Figure 2: Schematic of the subsystems of an autonomous\
    \ vehicle.\n\n#### 2.2.1 Chipyard.\n\nChipyard [\\[13\\]](#page-11-6) is an environment\
    \ for the design and evaluation of hardware systems that consists of a set of\
    \ tools and libraries designed to provide an integration path between open-source\
    \ tools and commercial tools for the development of Systems on Chip (SoC). The\
    \ environment provides a range of components for design construction as well as\
    \ for compilation and simulation. Among these components there are several RISC-V\
    \ cores and accelerators, including the BOOM core and Gemmini accelerator that\
    \ make up the heterogeneous system chosen in this paper and that will be detailed\
    \ in Section [3.](#page-3-0) The simulation of the complete system accelerated\
    \ with FPGA is one of the types of simulation supported by Chipyard, using the\
    \ FireSim tool described below.\n\n#### 2.2.2 FireSim.\n\nFireSim [\\[14\\]](#page-11-7)\
    \ is a hardware simulation platform that runs on Amazon cloud services and automatically\
    \ deploys the FPGA services in the cloud when needed. In particular, the user\
    \ can generate the RTL of an own design and run it on these FPGAs, obtaining the\
    \ same results as if the circuit was physically deployed.\n\n#### 2.2.3 Amazon\
    \ Web Services.\n\nAmazon Web Services [\\[15\\]](#page-11-8) is a cloud services\
    \ platform that offers from training courses in new technologies \u2013such as\
    \ artificial intelligence or IoT\u2013 to infrastructure services \u2013such as\
    \ storage or cloud computing. We focus on cloud computing because it offers a\
    \ wide range of hardware platforms, including EC2 F1 instances that correspond\
    \ to FPGAs, giving us the versatility we need to synthesize designs and to simulate\
    \ the execution of applications on them.\n\n# <span id=\"page-3-0\"></span>3 Platform\
    \ design\n\nThe platform employed in our experiments features a general-purpose\
    \ processor equipped with an accelerator \u2013implemented as a systolic array\
    \ architecture\u2013 for matrix multiplication. Both components have been developed\
    \ by the Computer Architecture group at Berkeley University [\\[6\\]](#page-10-5).\
    \ The accelerator communicates with the processor through the RoCC (Rocket Co-Processor)\
    \ interface, which allows the accelerator to receive the specific instructions\
    \ that the processor sends, as shown in Fig. [3.](#page-3-1) In the following\
    \ two sections we describe the processors and the accelerator used.\n\n![](_page_3_Figure_6.jpeg)\n\
    \n<span id=\"page-3-1\"></span>Figure 3: Architecture of our heterogeneous platform\
    \ [\\[6\\]](#page-10-5).\n\n## 3.1 Processors\n\nAs Fig. [3](#page-3-1) illustrates,\
    \ our system features a core plus an accelerator. In our experiments we opted\
    \ to employ either the Rocket or the BOOM (Berkeley Out-of-Order Machine) processor.\
    \ Both of them are written in Chisel and implement the RV64GC instruction set.\
    \ Also, they\n\nare easily parameterizable and can be synthesized. Notably, the\
    \ cores are configured by using the Rocket Chip SoC generator [\\[16\\]](#page-11-9).\n\
    \nThe main differences between both cores lie in the pipeline characteristics:\
    \ while the Rocket core features an in-order 5-stage pipeline, the BOOM core is\
    \ equipped with a deeper out-of-order pipeline, which is inspired by those of\
    \ MIPS R10000 and Alpha 212645 [\\[5\\]](#page-10-4). Consequently, the BOOM core\
    \ is expected to deliver higher performance when executing our line detection\
    \ algorithm. However, this comes at the expense of higher energy consumption than\
    \ that of the Rocket core. Therefore, we experiment with both processors in order\
    \ to check if the speedup reported by the BOOM core is significant enough to cancel\
    \ out the energy constraints.\n\n## 3.2 The Gemmini Accelerator\n\nThe Gemmini\
    \ matrix multiplication accelerator relies on a 2D systolic array architecture,\
    \ as shown in Fig. [3,](#page-3-1) to perform matrix multiplications in an efficient\
    \ fashion. In addition to this systolic array, it also features a scratchpad memory\
    \ with multiple banks and an accumulator, which has more bits than that of the\
    \ systolic array. Besides, the implementation allows to choose, at compile time,\
    \ between two specific calculation mechanisms: output-stationary or weight-stationary.\n\
    \nCustomized instructions \u2013out of RISC-V standard\u2013 are available for\
    \ the Gemmini accelerator, so that it is equipped with its own instruction queues\
    \ that make it possible to execute concurrently with the processor. The Gemmini\
    \ programming model can be broken down into three different levels. In the high-level\
    \ we can run Open Neural Network Exchange (ONNX) models, being the accelerator\
    \ itself in charge of mapping the ONNX kernel to the accelerator by means of dynamic\
    \ dispatch. In the mid-level we use a handtuned library including C macros to\
    \ perform data transfers between the main memory and the accelerator's scratchpad\
    \ memory, which should be explicitly defined, as well as to automate the calculation\
    \ of the block size used to split a matrix and to perform the full multiplication\
    \ in a transparent way for users. Among available functions we highlight the following:\
    \ *tiled matmul*, to run a tiled matrix multiplication with hardcoded tiling factors;\
    \ *tiled conv*, to apply a convolution with hardcoded tiling factors; *tiled matmul\
    \ auto*, to run a tiled matrix multiplication with automatically calculated tiling\
    \ factors; *gemmini mvin*, to move data from the main memory to the scratchpad\
    \ and *gemmini mvout*, to move data from the scratchpad to the main memory. Finally,\
    \ at the low-level, we can write our own mid-level kernels with low-level assembly\
    \ instructions.\n\n# <span id=\"page-4-2\"></span>4 Adapting image processing\
    \ algorithms\n\nAs stated previously, the aim of this work is to accelerate image\
    \ processing algorithms employed to guide autonomous vehicles. Notably, we focus\
    \ on those algorithms targeted to detect road lines from road images. In this\
    \ section we first introduce the basic algorithms used (the Canny algorithm and\
    \ the Hough transform). Then, we show the full algorithm that we have employed\
    \ in this work as starting point for line detection and, finally, we propose some\
    \ changes to this algorithm oriented to improve its efficiency and performance\
    \ without impacting on accuracy.\n\n## 4.1 Canny Algorithm\n\nAmong the edge detection\
    \ methods developed to date, the Canny algorithm is one of the methods more strictly\
    \ defined that provides a satisfactory and reliable detection. Thus, it has become\
    \ one of the most popular algorithms targeting edge detection.\n\nThis algorithm\
    \ relies on calculus of variations, which allows to find an analytical function\
    \ to approximate the real curve (i.e., the road lines) as accurately as possible.\
    \ The procedure followed by the Canny algorithm [\\[17\\]](#page-11-10) can be\
    \ broken down into 5 stages as shown next:\n\n- 1. Noise reduction: applying the\
    \ Gauss filter for image smoothing.\n- 2. To find the intensity gradient of the\
    \ image.\n- 3. Magnitude threshold to the gradient: applying a threshold to the\
    \ gradient for discarding edge false positives.\n- 4. Double threshold: applying\
    \ again a threshold to the gradient for highlighting the potential edges.\n- 5.\
    \ Hysteresis: removing weak or disconnected edges.\n\nAlgorithm [1](#page-4-0)\
    \ shows the pseudo-code we employed to apply the Canny algorithm, broken down\
    \ into the 5 stages aforementioned. Essentially, it includes multiplications of\
    \ consecutive matrices and conditions checking in order to detect edge points.\n\
    \n## 4.2 Hough Transform\n\nThe Hough transform is a technique of features extraction\
    \ which is employed in multiple fields involving image processing, as computer\
    \ vision or image digital processing. The goal of the algorithm is to find imperfect\
    \ objects among certain classes of objects by means of a voting procedure. This\
    \ procedure lies in creating a space with the values assigned to each pixel, so\
    \ that the resulting local maximums in the so called accumulator space are the\
    \ possible detected objects.\n\n<span id=\"page-4-0\"></span>Algorithm 1 Canny\
    \ algorithm summarized pseudocode.\n\n- 1: float *NR* \u2190 mask \u2217 image\
    \ \u25B7 Stage 1: Noise reduction\n- 2: float *G<sup>x</sup>* \u2190 mask \u2217\
    NR \u25B7 Stage 2: Gradient intensity\n- 3: float *G<sup>y</sup>* \u2190 mask\
    \ \u2217NR\n- 4: float *G* \u2190 q *G*<sup>2</sup> *<sup>x</sup>* +*G*<sup>2</sup>\
    \ *y*\n- 5: float \u03C6 \u2190 arctan(|*Gy*|/|*Gx*|)\n- 6: if \u03C6[\u2217]\
    \ \u2265 threshold<sup>\u03C6</sup> then \u25B7 Stage 3: Gradient threshold\n\
    - 7: float \u03C6 \u2208 {0,45,90,135}\n- 8: end if\n- 9: if \u03C6[\u2217] \u2265\
    \ threshold<sup>\u03C6</sup> && *G*[\u2217] \u2265 threshold*<sup>G</sup>* then\
    \ \u25B7 Stage 4: Double threshold\n- 10: int edge[\u2217] \u2190 1\n- 11: end\
    \ if\n- 12: if *G*[\u2217] \u2265 threshold*<sup>G</sup>* && edge[\u2217] == 1\
    \ then \u25B7 Stage 5: Hysteresis\n- 13: int image out[\u2217] \u2190 255\n\n\
    14: end if\n\nGenerally, the classical Hough transform was only applied to the\
    \ detection of straight lines, but in recent years it has been modified and currently\
    \ it is employed for the detection of arbitrary curves, as ellipses or circles.\n\
    \nAlgorithm [2](#page-4-1) illustrates the code we employed to apply the Hough\
    \ transform [\\[18\\]](#page-11-11). In this code, for each edge point previously\
    \ detected with the Canny algorithm, the Hough transform draws a set of straight\
    \ lines going through that point, recording the amount of lines going through\
    \ each image pixel. Hence, those points with more lines going through them will\
    \ correspond to a line in the original image.\n\n<span id=\"page-4-1\"></span>Algorithm\
    \ 2 Hough transform summarized pseudocode.\n\n1: For each edge point (*i*, *j*)\
    \ 2: if image[*i* \u2217*width*+ *j*] \u2265 250 then 3: \u03B8 \u2190 0 4: while\
    \ \u03B8 \u2264 180 do 5: float \u03C1 \u2190 *j* \u2217 cos\u03B8 +*i* \u2217\
    \ sin\u03B8 6: accumulators[(\u03C1 +*c*<sup>\u03C1</sup> ) \u2217 180+\u03B8\
    ]++ 7: \u03B8++ 8: end while 9: end if\n\n## 4.3 Line Detection\n\nOnce we have\
    \ described the two previous algorithms, we now employ a combination of both as\
    \ well as another specific code targeted to detect with higher accuracy the lines\
    \ that demarcate lanes in conventional ways. For this purpose, given a certain\
    \ input image, we first apply the Canny algorithm and then the Hough\n\ntransform,\
    \ so that we can apply a function (*Get lines coordinates*) to detect lines in\
    \ the resulting image. In Algorithm [3](#page-5-0) we show the code of the mentioned\
    \ function, which involves a search of local maximums in the preprocessed image\
    \ and the generation of a straight line going through closest maximums.\n\n<span\
    \ id=\"page-5-0\"></span>Algorithm 3 Get lines coordinates algorithm summarized\
    \ pseudo-code.\n\n- 1: For each image point (\u03C1,\u03B8)\n- 2: if accumulators[\u2217\
    ] \u2265 threshold then \u25B7 It is a local maximum\n- 3: *max* \u2190 accumulators[\\\
    *]\n- 4: if accumulators[neighbourhood(\\*)] \u2265 *max* then \u25B7 We check\
    \ its neighborhood\n- 5: *max* \u2190 accumulators[neighbourhood(\\*)]\n- 6: end\
    \ if\n- 7: end if\n- 8: lines.add(*x*1, *y*1, *x*2, *y*2) \u25B7 We save the two\
    \ points that demarcate the straight line\n\n## 4.4 Delivering higher performance\n\
    \nIn the previous sections we have described the original code of the algorithms,\
    \ which indeed deploys many floating point variables. Therefore, it is advisable\
    \ to replace them by integer variables without any loss in accuracy. We effectively\
    \ made these changes in the code and we verified that no accuracy loss occurs\
    \ when detecting lines in an image. Fig. [4](#page-6-0) shows the original image\
    \ with detected lines highlighted in red. The analytical results corresponding\
    \ to the lines detected with the original algorithm and with the simplified one\
    \ do match, and also the second algorithm has performed slightly faster. Details\
    \ on these modifications can be found in [\\[19\\]](#page-11-12).\n\nApart from\
    \ this change, we also performed a profiling of the full code divided into three\
    \ stages: 1) original image loading, 2) lines detection and 3) generation of an\
    \ output image with the detected lines. Accordingly to the results obtained, we\
    \ opted for not generating an output image (that is not needed by our system)\
    \ due to the high cost associated, as shown in Table [1.](#page-5-1) In doing\
    \ so, we are able to reduce the execution time by 4.2x as we can derive from data\
    \ reported in Table [2.](#page-5-2) It is worth noting that the time values illustrated\
    \ in the mentioned tables are approximate since the profiling was not performed\
    \ on the target platform, but on an Intel i7 processor running Linux. However,\
    \ in order to report time values as accurate as possible, the algorithms were\
    \ run several times so that the tables show the average values obtained. According\
    \ to numbers from Table [2,](#page-5-2) our attention is drawn to the line detection\
    \ phase since it accounts for almost 70% of the execution time.\n\nIn addition,\
    \ we have performed another specific profiling of the stages of the line detection\
    \ algorithm in\n\n<span id=\"page-5-1\"></span>\n\n| Table 1: Phased profiling\
    \ of the full code. |  |  |\n|---------------------------------------------|--|--|\n\
    |                                             |  |  |\n\n|                  |\
    \ Time(\xB5s) | % over total |\n|------------------|----------|--------------|\n\
    | Image load       | 43803    | 7,32%        |\n| Line detection   | 98171   \
    \ | 16,42%       |\n| Image generation | 456030   | 76,26%       |\n| Total  \
    \          | 598004   |              |\n\n<span id=\"page-5-2\"></span>Table 2:\
    \ Phased *profiling* of the full code excluding the generation of the output image.\n\
    \n|                | Time(\xB5s) | % over total |\n|----------------|----------|--------------|\n\
    | Image load     | 43485    | 30,58%       |\n| Line detection | 98714    | 69,42%\
    \       |\n| Total          | 142199   |              |\n\norder to know in which\
    \ parts of the processing the acceleration efforts should be focused. Table [3](#page-5-3)\
    \ illustrates that the most time-consuming part is the application of the Canny\
    \ algorithm, which accounts for more than 87% of the total execution time. Therefore,\
    \ we will focus on accelerating this stage of image processing.\n\n<span id=\"\
    page-5-3\"></span>Table 3: Phased *profiling* of the line detection algorithm.\n\
    \n|                 | Time(\xB5s) | % over total |\n|-----------------|----------|--------------|\n\
    | Canny algorithm | 90265    | 87,64%       |\n| Hough transform | 12275    |\
    \ 11,92%       |\n| Get coordinates | 459      | 0,45%        |\n| Total     \
    \      | 102999   |              |\n\n# 5 Experimental results\n\nIn this section,\
    \ we first describe the hardware platforms as well as the workloads employed in\
    \ our experiments, and then we detail the results obtained.\n\n## 5.1 Platforms\
    \ generated\n\nAll the components used in the designs generated are written in\
    \ Scala, so it is easy to modify their main features such as number of registers\
    \ or number of Re-Order Buffer (ROB) entries. Notably, we generate several designs:\
    \ while all of them include one (or more) Rocket or BOOM cores, they may include\
    \ or not the Gemmini accelerator.\n\nApart from the cores, for the sake of fairness\
    \ the remaining components in the different designs generated (such as memory,\
    \ clock frequency or buses) are the same in all of them. Hence, all designs have\
    \ an L2 \u2013shared in multicore platforms\u2013 4MB size. In order to optimize\
    \ the design to fit into smaller FPGAs, the option MCRams is enabled in the FireSim\
    \ platform configuration for all designs. This option allows the FPGA simulation\
    \ tool (Golden Gate [\\[20\\]](#page-11-13)) to simulate the RAM via serialized\
    \ accesses with a decoupled model [\\[14\\]](#page-11-7).\n\n![](_page_6_Picture_0.jpeg)\n\
    \nFigure 4: Original image with detected lines highlighted in red.\n\nPlatforms\
    \ including the Gemmini accelerator can only be designed to work at 50MHz while\
    \ the remaining ones can reach 80MHz. Thus, the later have been designed both\
    \ at 50 and 80 MHz for a fair comparison against designs equipped with Gemmini.\
    \ Notably, the platforms generated are:\n\n1. *Platform* 1: Rocket single core.\n\
    \nThis architecture includes a single *Big* Rocket core. There are four different\
    \ sizes for the core, namely *Big, Medium, Small* and *Tiny*, with different features\
    \ such as the size of L1-cache. The *Big* Rocket is the only one providing Floating\
    \ Point Unit. It also has by default the parameters shown in Table [4.](#page-7-0)\
    \ More information on the details of the configuration can be found in [\\[19\\\
    ]](#page-11-12).\n\n2. *Platform* 2: Rocket dual core.\n\nThis is the same configuration\
    \ as Platform 1 but it includes two *Big* Rocket cores. This dual configuration\
    \ also has the option MTModels enabled in the FireSim platform configuration,\
    \ so that each core is simulated with a separate thread of execution on a shared\
    \ underlying physical implementation [\\[14\\]](#page-11-7).\n\n3. *Platform*\
    \ 3: Heterogeneous Rocket single core + Gemmini Accelerator.\n\nThis architecture\
    \ is made up by a *Big* Rocket core and a Gemmini matrix multiplication accelerator,\
    \ which has been designed with default options: 16x16 8-bit systolic array, both\
    \ dataflows supported (output-stationary and weight-stationary), float data type\
    \ supported, a set of accumulator registers with 64B of total capacity, a 256KB\
    \ scratchpad with 4 banks, a small TLB with 4 entries and a bus width of 128 bits.\n\
    \n<span id=\"page-6-0\"></span>4. *Platform* 4: BOOM Single core.\n\nThis architecture\
    \ includes a single *Large* BOOM core. There are different macros for defining\
    \ BOOM cores of *Giga, Mega, Large, Medium* and *Small* sizes. The main differences\
    \ between the one that we are using and the rest is the number of entries in the\
    \ ROB and some L1-cache parameters. Thus, in the configuration *WithN-LargeBooms*\
    \ the value of notable parameters are shown in Table [4.](#page-7-0) More information\
    \ on the details of the configuration can be found in [\\[19\\]](#page-11-12).\
    \ The Large size was chosen because it is just big enough to provide the required\
    \ performance with minimum power consumption.\n\n5. *Platform* 5: BOOM dual core.\n\
    \nThis is the same configuration as Platform 4 but it includes two *Large* BOOM\
    \ cores, with the MT-Models option enabled.\n\n6. *Platform* 6: Heterogeneous\
    \ BOOM single core + Gemmini Accelerator.\n\nThis architecture is made up by a\
    \ *Large* BOOM core and a Gemmini matrix multiplication accelerator, which has\
    \ been designed with the default options explained earlier.\n\n# 5.2 Workloads\
    \ generated\n\nDifferent workloads were designed for running on the platforms\
    \ described in the previous section. They are the following:\n\n1. *Workload*\
    \ 1: Multithreaded application on top of Linux buildroot distribution.\n\nIn this\
    \ workload, a multithreaded application (with each thread computing the addition\
    \ of 2\n\n|                  |             | Big Rocket | Large Boom |\n|------------------|-------------|------------|------------|\n\
    | I&D Cache        | Size        | 16KB       | 32KB       |\n|              \
    \    | Sets        | 64         | 64         |\n|                  | Ways    \
    \    | 4          | 8          |\n|                  | Prefetching | no      \
    \   | disabled   |\n| TLB              | Sets        | 1          | 1        \
    \  |\n|                  | Ways        | 32         | 512        |\n| BTB Entries\
    \      |             | 28         | 28         |\n| ROB Entries      |       \
    \      | no         | 96         |\n| FPU              |             | yes   \
    \     | yes        |\n| Branch predictor |             |            |        \
    \    |\n| entries          |             | no         | 128        |\n\n<span\
    \ id=\"page-7-0\"></span>Table 4: Platform configuration options.\n\nlong arrays,\
    \ as explained in [\\[19\\]](#page-11-12)) is executed on top of Linux. It has\
    \ been specifically designed to fully exploit the parallel features of the platforms,\
    \ so that it can be used to evaluate the maximum performance obtainable in the\
    \ different multicore designs. This value will serve as an upper bound when we\
    \ evaluate the performance achieved by our target application.\n\n<span id=\"\
    page-7-1\"></span>2. *Workload* 2: Line detection algorithm on top of Linux buildroot\
    \ distribution.\n\nIn this workload, the modified version of the line detection\
    \ application explained in Section [4](#page-4-2) is executed on top of Linux.\n\
    \n<span id=\"page-7-2\"></span>3. *Workload* 3: Line detection algorithm for baremetal\
    \ platforms with Gemmini.\n\nIn this workload, in addition to the modifications\
    \ in Section 4, we have modified the line detection algorithm to add matrix multiplications.\
    \ In the original version, this algorithm multiplies some mask values to a pixel\
    \ neighborhood manually by writing the corresponding scalar multiplications. We\
    \ have rewritten these multiplications in a matrix form, obtaining a 5x5 matrix\
    \ for the mask and a 5x5 neighborhood matrix for each pixel. As for the platform,\
    \ the differences with respect to the previous workload are that this platform\
    \ includes a Gemmini accelerator for matrix multiplication and the fact that no\
    \ operating system is available for this platform. Thus, matrix multiplications\
    \ in the code have to be replaced by calls to a Gemmini multiplication. As previously\
    \ explained, some C macros are provided with the designs that make it possible\
    \ to easily programming the accelerator. First, data need to be moved from the\
    \ main memory to the scratchpad memory in Gemmini, then the multiplication is\
    \ performed in tiles and finally the results are transferred back to the main\
    \ memory. We will use the *tiled matmul auto* function that receives the dimensions\
    \ of both matrices as input parameters and automatically splits the multiplication\
    \ in\n\nblocks of suitable size for the systolic array and memory, thus performing\
    \ the whole multiplication. Finally, system calls not available outside Linux\
    \ were removed from the code and their functionality was implemented in an equivalent\
    \ way.\n\n# 5.3 Experiments\n\nIn this section we show the results obtained from\
    \ the execution of the workloads on the different platforms designed. The metrics\
    \ measured are clock cycles and instructions retired provided by the performance\
    \ counters of the target platforms.\n\n#### 5.3.1 Experiment 1: Execution of a\
    \ multithreaded application on single core and dual core platforms both with Rocket\
    \ and BOOM cores.\n\nThe goal of this experiment is to verify the maximum performance\
    \ attainable in the different platforms by using a massively parallel application.\
    \ Therefore we employ *Workload 1*, configured with as many independent threads\
    \ as the number of cores in the system, i.e., 1 or 2 depending on the specific\
    \ platform.\n\nThe target platforms in this case include both single and dual\
    \ core processors (either Rocket or BOOM, running at 80MHz) that correspond to\
    \ the *Platforms* 1, 2, 4 and 5 previously described.\n\nThe results of the experiment\
    \ are shown in Table [5,](#page-8-0) both for a simulation in which the main loop\
    \ is executed once (column labelled *N times = 1*) and 8 times (column *N times\
    \ = 8*). The number of clock cycles for the experiment with 8 iterations is 8\
    \ times the one of the single iteration experiment. Besides, speedup of the dual\
    \ core version with respect to the single core is very close to 2x for both Rocket\
    \ and BOOM. Finally, comparing the performance of the different cores, BOOM achieves\
    \ almost 2.2x higher performance than Rocket, so that a single BOOM core outperforms\
    \ a dual core Rocket running at the same frequency for this highly parallel application.\n\
    \nThus, it has been verified that multithreaded applications are being correctly\
    \ simulated in the multicore\n\n|                   | N times = 1 | N times=8\
    \ |  |\n|-------------------|-------------|-----------|--|\n|                \
    \   | Cycles      |           |  |\n| Rocket singlecore | 2.01\xD7109    | 1.59\xD7\
    1010 |  |\n| BOOM singlecore   | 9.17\xD7108    | 7.31\xD7109  |  |\n| Rocket\
    \ dualcore   | 9.97\xD7108    | 7.99\xD7109  |  |\n| BOOM dualcore     | 4.53\xD7\
    108    | 3.66\xD7109  |  |\n| Speedup BOOM      |             |           |  |\n\
    | vs Rocket         | 2.19x       | 2.18x     |  |\n| Speedup Rocket    |    \
    \         |           |  |\n| dual vs single    | 2.02x       | 1.99x     |  |\n\
    | Speedup BOOM      |             |           |  |\n| dual vs single    | 2.02x\
    \       | 1.99x     |  |\n\n<span id=\"page-8-0\"></span>Table 5: Cycles when\
    \ executing Workload 1 on Platforms 1, 2, 4 and 5.\n\nplatforms, achieving the\
    \ expected speedup. Furthermore, the comparison between both types of cores has\
    \ been established.\n\n#### 5.3.2 Experiment 2: Execution of the line detection\
    \ application on Rocket and BOOM single cores.\n\nThis second experiment involves\
    \ simulating the execution of the line detection application (*workload* [2\\\
    )](#page-7-1) on the Rocket and BOOM single core platforms employed in the previous\
    \ experiment (Platforms 1 and 4), also running at 80MHz. In Table [6](#page-9-0)\
    \ we report the number of clock cycles and instructions retired corresponding\
    \ to each of the different parts of the line detection algorithm, as well as the\
    \ average cycles per instructions (CPI) value. In addition, we calculate the actual\
    \ time from the cycles and clock frequency, resulting in times of around half\
    \ second. In particular, for the Rocket core we obtain a total execution time\
    \ of 0.648s and for the Boom core 0.327s. As shown, the CPI for the Hough transform\
    \ is higher than 3 in both Rocket and BOOM platforms. Moreover, its execution\
    \ on the BOOM processor almost matches the time reported on the Rocket platform,\
    \ as the multiple data dependencies in the code make out-of-order capabilities\
    \ useless.\n\nOn the other hand, the Canny and the GetCoordinates algorithms exhibit\
    \ lower CPI numbers in both platforms, achieving a speedup of 2x when executing\
    \ on the Boom processor with respect to Rocket, due to the greater instruction\
    \ level parallelism that can be extracted from both algorithms. Recall that the\
    \ Canny algorithm is the most relevant part of the line detection application,\
    \ consuming close to 90% of the total execution time (as shown in Table [3\\)](#page-5-3).\
    \ In conclusion, using the BOOM core for the execution of the workload is interesting\
    \ in terms of the global speedup achieved.\n\n#### 5.3.3 Experiment 3: Execution\
    \ of the line detection application on heterogeneous platforms with a Rocket or\
    \ BOOM single core and a Gemmini matrix multiplication accelerator.\n\nThis experiment\
    \ consists on simulating the execution of the modified line detection application\
    \ (*workload* [3\\)](#page-7-2) on the heterogeneous single core platforms made\
    \ up by a Rocket or BOOM processor plus a Gemmini matrix multiplication accelerator\
    \ running at 50MHz.\n\nTable [7](#page-9-1) shows first the results obtained in\
    \ the simulation of Workload 3 (line detection application for bare metal) on\
    \ a Rocket single core (used as baseline for computing speedups) and a BOOM single\
    \ core, both running at 50MHz. As the first row shows, BOOM is 41% faster than\
    \ Rocket. The execution results from the previous section, that is, those corresponding\
    \ to Workload 2 (line detection application for Linux) on Rocket and BOOM single\
    \ core at 80MHz are also compared to the baseline execution, achieving speedups\
    \ of 2.09x and 3.76x respectively. It is worth noting that although the code of\
    \ Workloads 2 and 3 does not exactly match, it performs the same functionality.\
    \ Finally, the results from the simulation of Workload 3 on heterogeneous platforms\
    \ in which matrix multiplications are performed using the Gemmini accelerator\
    \ are also recap in Table [7.](#page-9-1) According to them, speedups of 2.36x\
    \ and 3.7x are reported for Rocket and BOOM based platforms respectively, with\
    \ respect to the baseline. Although these speedups can be considered as significant,\
    \ they are far from the maximum values attainable by the accelerator. The reason\
    \ is that the size of the matrices employed is smaller than that of the systolic\
    \ array, which indeed is not fully utilized.\n\nFurthermore, in the graph shown\
    \ in Fig. [5](#page-10-6) we can see the time corresponding to all the single\
    \ core and heterogeneous experiments. The first thing we notice is that the out-of-order\
    \ execution of the Boom core is beneficial for the Canny algorithm, leaving the\
    \ Rocket core as the slowest by far at both 50 and 80MHz. Furthermore, we see\
    \ how the combination of the cores with the Gemmini accelerator at 50MHz gives\
    \ us a similar time to the same cores without accelerator at 80MHz, which gives\
    \ us a great benefit in terms of consumption by running at a lower clock frequency\
    \ which should be taken into account in the field of autonomous vehicles, as it\
    \ would provide greater autonomy. In addition, we note that the shortest time\
    \ is under half a second, in particular 300ms, and we achieve it with the combination\
    \ of the Boom core and the Gemmini accelerator at a clock frequency of 50MHz.\
    \ Thus, a vehicle travelling at 50km/h could run the algorithm every 4 metres\
    \ approximately and if necessary, options such as mounting several systems in\
    \ parallel or slightly increasing the clock frequency for faster processing could\
    \ be explored.\n\nIn conclusion, for this application with small matrices, both\
    \ platforms based on the BOOM core deliver similar performance (speedup of around\
    \ 3.7x with re-\n\n<span id=\"page-9-0\"></span>\n\n|                        |\
    \             | Cycles   | Instructions | CPI   | Time(ms) |\n|------------------------|-------------|----------|--------------|-------|----------|\n\
    | Rocket singlecore      | Canny       | 2.18\xD7109 | 9.06\xD7108     | 2.40\
    \  | 648,38   |\n|                        | Hough       | 3.32\xD7108 | 9.35\xD7\
    107     | 3.55  | 98,86    |\n|                        | Coordinates | 6.49\xD7\
    106 | 3.47\xD7106     | 1.87  | 1,93     |\n| Boom singlecore        | Canny \
    \      | 1.08\xD7109 | 9.06\xD7108     | 1.19  | 327,10   |\n|               \
    \         | Hough       | 3.16\xD7108 | 9.35\xD7107     | 3.38  | 96,07    |\n\
    |                        | Coordinates | 3.2\xD7106  | 3.47\xD7106     | 0.92\
    \  | 0,97     |\n| Speedup Boom vs Rocket | Canny       | 2.02x    | 1.00x   \
    \     | 2.02x | 1.98x    |\n|                        | Hough       | 1.05x   \
    \ | 1.00x        | 1.05x | 1.03x    |\n|                        | Coordinates\
    \ | 2.03x    | 1.00x        | 2.03x | 1.99x    |\n\nTable 6: Cycles, instructions\
    \ retired and CPI when executing Workload 2 on Platforms 1 and 4 at 80MHz.\n\n\
    <span id=\"page-9-1\"></span>Table 7: Speedup results when executing Workload\
    \ 2 on Platforms 1 and 4 at 80MHz, and Workload 3 on Platforms 3, 4, 6 at 50MHz,\
    \ with respect to execution of Workload 3 on Platform 1 at 50 MHz.\n\n|      \
    \                   | Speedup vs Rocket singlecore 50MHz |       |           \
    \  |       |  |\n|-------------------------|------------------------------------|-------|-------------|-------|--|\n\
    |                         | Canny                              | Hough | Coordinates\
    \ | Total |  |\n| Boom singlecore 50MHz   | 1.44x                            \
    \  | 1.04x | 1.85x       | 1.41x |  |\n| Rocket singlecore 80MHz | 2.26x     \
    \                         | 0.98x | 1.07x       | 2.09x |  |\n| Boom singlecore\
    \ 80MHz   | 4.57x                              | 1.03x | 2.18x       | 3.76x |\
    \  |\n| Rocket + Gemmini 50MHz  | 2.54x                              | 1.16x |\
    \ 1.03x       | 2.36x |  |\n| Boom + Gemmini 50MHz    | 4.43x                \
    \              | 1.07x | 1.98x       | 3.70x |  |\n\nspect to the Rocket baseline),\
    \ being the BOOM single core at 80MHz slightly faster than the BOOM + Gemmini\
    \ at 50MHz. Even in this non favourable scenario, the accelerator allows to report\
    \ high performance working at a lower frequency, being more power efficient than\
    \ the single core platform running at higher frequency.\n\n# 6 Conclusions and\
    \ future work\n\nIn this paper we have explored the acceleration of a line detection\
    \ algorithm in the autonomous car environment using a heterogeneous system consisting\
    \ of a general-purpose RISC-V core and a domain-specific accelerator. In particular,\
    \ we analyzed the application to identify the most computationally intensive parts\
    \ of the code and adapted it accordingly for more efficient processing.\n\nThe\
    \ first conclusion we extract from this work is that RISC-V architecture provides\
    \ a hw-sw ecosystem that is well suited for IoT in general and autonomous vehicle\
    \ systems in particular, due to its versatility and modularity, which allows to\
    \ generate platforms adapted to different scenarios. In fact, in this work, we\
    \ designed six different platforms covering a wide spectrum of alternatives: on\
    \ one side single and dual core homogeneous systems, and on the other side heterogeneous\
    \ platforms with a single core plus a matrix multiplication accelerator \u2013\
    all of them including high performance BOOM cores or more efficient Rocket cores.\n\
    \nAlso, a multithreaded application with high data parallelism has been designed\
    \ to analyze the performance of the homogeneous platforms built. Thus, it has\
    \ been\n\nverified that multithreaded applications are being correctly simulated\
    \ in the multicore platforms, achieving the expected speedup. Furthermore, the\
    \ comparison between both types of cores determined that a single BOOM core is\
    \ up to 2.19 times faster than a Rocket one.\n\nFinally, the original application\
    \ of line detection has been modified in order to decrease its execution time\
    \ without losing accuracy, and it has also been adapted for bare metal and Gemmini\
    \ execution. We simulated the application on all designed platforms. BOOM-based\
    \ platforms reported the best performance numbers, achieving speedups of 3.7x\
    \ with respect to the baseline (a single Rocket core running at 50MHz), and being\
    \ the single BOOM core running at 80MHz slightly faster than the BOOM + Gemmini\
    \ platform at 50MHz. As previously stated, even working at a lower frequency the\
    \ accelerator allows to report high performance, being more power efficient than\
    \ the single core counterpart working at a higher frequency. It is worth noting\
    \ that our goal in this work was to explore how an domain-specific accelerator\
    \ was able to accelerate the baseline execution (just using a conventional single\
    \ core) in applications belonging to autonomous vehicles environment.\n\nAs future\
    \ work, other applications which involve multiplication of big matrices can be\
    \ adapted to heterogeneous platforms in order to implement more of the functionalities\
    \ required for autonomous vehicles. Moreover, Gemmini is expected to achieve much\
    \ higher speedups for inference using neural networks, as shown in [\\[6\\]](#page-10-5),\
    \ so exploring this issue constitutes an interesting avenue for future work.\n\
    \n![](_page_10_Figure_0.jpeg)\n\n<span id=\"page-10-6\"></span>Figure 5: Time\
    \ results when executing Workload 2 on Platforms 1 and 4 at 50MHz and 80MHz, and\
    \ Workload 3 on Platforms 3 and 6 at 50MHz.\n\n## Competing interests\n\nThe authors\
    \ have declared that no competing interests exist.\n\n#### Funding\n\nThe present\
    \ work has been funded by the Comunidad de Madrid through project S2018/TCS-4423\
    \ and by the Ministry of Science, Innovation and Universities through project\
    \ RTI2018-093684-B-I00.\n\n#### Authors' contribution\n\nMJB wrote the programs,\
    \ conducted the experiments, analyzed the results and wrote the manuscript; KO\
    \ and FC conceived the idea, analyzed the results and wrote the manuscript; FT\
    \ revised the manuscript. All authors read and approved the final manuscript.\n\
    \n# References\n\n<span id=\"page-10-0\"></span>[1] S.-C. Lin *et al.*, \"The\
    \ architectural implications of autonomous driving: Constraints and acceleration,\"\
    \ in *Proceedings of the Twenty-Third International Conference on Architectural\
    \ Support for Programming*\n\n*Languages and Operating Systems*, ASPLOS '18, p.\
    \ 751\u2013766, 2018.\n\n- <span id=\"page-10-1\"></span>[2] S. Kato, E. Takeuchi,\
    \ Y. Ishiguro, Y. Ninomiya, K. Takeda, and T. Hamada, \"An open approach to autonomous\
    \ vehicles,\" *IEEE Micro*, vol. 35, pp. 60\u201368, 11 2015.\n- <span id=\"page-10-2\"\
    ></span>[3] P. Bose, A. J. Vega, S. V. Adve, V. S. Adve, and V. J. Reddi, \"Secure\
    \ and resilient socs for autonomous vehicles,\" in *Proceedings of the 3rd International\
    \ Workshop on Domain Specific System Architecture (DOSSA)*, pp. 1\u20136, 2021.\n\
    - <span id=\"page-10-3\"></span>[4] B. Yu *et al.*, \"Building the computing system\
    \ for autonomous micromobility vehicles: Design constraints and architectural\
    \ optimizations,\" in *Proceedings of 53rd Annual IEEE/ACM International Symposium\
    \ on Microarchitecture (MICRO)*, pp. 1067\u20131081, 2020.\n- <span id=\"page-10-4\"\
    ></span>[5] J. Zhao, B. Korpan, A. Gonzalez, and K. Asanovic, \"Sonicboom: The\
    \ 3rd generation berkeley out-of-order machine,\" in *Proceedings of the 4th Workshop\
    \ on Computer Architecture Research with RISC-V (CARRV)*, pp. 1\u20137, 2020.\n\
    - <span id=\"page-10-5\"></span>[6] H. Genc *et al.*, \"Gemmini: Enabling systematic\
    \ deeplearning architecture evaluation via full-stack integration,\" in *Proceedings\
    \ of the 58th Annual Design Automation Conference (DAC)*, pp. 769\u2013774, 2021.\n\
    - <span id=\"page-11-0\"></span>[7] \"The 6 levels of vehicle autonomy explained.\"\
    \ Available at: [https://www.synopsys.com/](https://www.synopsys.com/automotive/autonomous-driving-levels.html)\
    \ [automotive/autonomous-driving-levels.](https://www.synopsys.com/automotive/autonomous-driving-levels.html)\
    \ [html](https://www.synopsys.com/automotive/autonomous-driving-levels.html).\
    \ Accessed on 2022-09-07.\n- <span id=\"page-11-5\"></span>[8] O. Vermesan *et\
    \ al.*, *IoT technologies for connected and automated driving applications. Internet\
    \ of Things - The Call of the Edge*, pp. 306\u2013332. River Publishers, Oct.\
    \ 2020.\n- <span id=\"page-11-1\"></span>[9] R. Coppola and M. Morisio, \"Connected\
    \ car: technologies, issues, future trends,\" *ACM Computing Surveys (CSUR)*,\
    \ vol. 49, no. 3, pp. 1\u201336, 2016.\n- <span id=\"page-11-2\"></span>[10] T.\
    \ Rateke *et al.*, \"Passive vision region-based road detection: A literature\
    \ review,\" *ACM Computing Surveys (CSUR)*, vol. 52, no. 2, pp. 1\u201334, 2019.\n\
    - <span id=\"page-11-3\"></span>[11] F. Bounini, D. Gingras, V. Lapointe, and\
    \ H. Pollart, \"Autonomous vehicle and real time road lanes detection and tracking,\"\
    \ in *IEEE Vehicle Power and Propulsion Conference (VPPC)*, pp. 1\u20136, 2015.\n\
    - <span id=\"page-11-4\"></span>[12] G. Zhang, N. Zheng, C. Cui, Y. Yan, and Z.\
    \ Yuan, \"An efficient road detection method in noisy urban environment,\" in\
    \ *Proceedings of IEEE Intelligent Vehicles Symposium*, pp. 556 \u2013 561, 2009.\n\
    - <span id=\"page-11-6\"></span>[13] A. Amid *et al.*, \"Chipyard: Integrated\
    \ design, simulation, and implementation framework for custom socs,\" *IEEE Micro*,\
    \ vol. 40, no. 4, pp. 10\u201321, 2020.\n- <span id=\"page-11-7\"></span>[14]\
    \ S. Karandikar *et al.*, \"Firesim: Fpga-accelerated cycleexact scale-out system\
    \ simulation in the public cloud,\"\n\nin *Proceedings of ACM/IEEE 45th Annual\
    \ International Symposium on Computer Architecture (ISCA)*, pp. 29\u201342, 2018.\n\
    \n- <span id=\"page-11-8\"></span>[15] \"Amazon web services (aws).\" Available\
    \ at: [https:](https://aws.amazon.com/es) [//aws.amazon.com/es](https://aws.amazon.com/es),\
    \ 2021. Accessed on 2022-09- 07.\n- <span id=\"page-11-9\"></span>[16] K. Asanovic\
    \ *et al.*, \"The rocket chip generator. eecs department,\" *University of California,\
    \ Berkeley, Tech. Rep. UCB/EECS-2016-17*, vol. 4, 2016.\n- <span id=\"page-11-10\"\
    ></span>[17] J. F. Canny, \"Finding edges and lines in images,\" *Theory of Computing\
    \ Systems - Mathematical Systems Theory*, p. 16, 1983.\n- <span id=\"page-11-11\"\
    ></span>[18] R. O. Duda and P. E. Hart, \"Use of the hough transformation to detect\
    \ lines and curves in pictures,\" *Communications of the ACM*, vol. 15, no. 1,\
    \ p. 11\u201315, 1972.\n- <span id=\"page-11-12\"></span>[19] M. J. Belda, \"\
    Image processing in autonomous vehicles on a risc-v with accelerator,\" *Master\
    \ Thesis, UCM*, 2022.\n- <span id=\"page-11-13\"></span>[20] A. Magyar *et al.*,\
    \ \"Golden gate: Bridging the resourceefficiency gap between asics and fpga prototypes,\"\
    \ in *Proceedings of IEEE/ACM International Conference on Computer-Aided Design\
    \ (ICCAD)*, pp. 1\u20138, 2019.\n\nCitation: M.J. Belda, K. Olcoz, F. Castro and\
    \ F. Tirado. *Optimization of a line detection algorithm for autonomous vehicles\
    \ on a RISC-V with accelerator*. Journal of Computer Science & Technology, vol.\
    \ xx, no. x, pp. x\u2013x, 202x.\n\n\u2714\n\n\u2715\n\nDOI: 10.24215/16666038.18.e01\n\
    \n\u2717\n\n\u2716\n\nReceived: August x, 2022 Accepted: xxx. Copyright: This\
    \ article is distributed under the terms of the Creative Commons License CC-BY-NC."
  paper_id: 22
  scores:
    originality:
      raw: 4.2
      reason: The paper presents a novel approach to energy-efficient real-time scheduling
        in CPS, which is a relevant and emerging area of research.
    technical_soundness:
      raw: 4.0
      reason: The methodologies discussed are grounded in established principles like
        DVFS, but the implementation details could be more thoroughly explored.
    clarity:
      raw: 3.5
      reason: While the introduction outlines the problem well, some technical terms
        and concepts could be better defined for clarity.
    significance:
      raw: 4.3
      reason: The focus on optimizing power consumption while meeting real-time constraints
        is highly significant for the future of industrial applications.
    relevance:
      raw: 4.5
      reason: The topic is highly relevant to current trends in cloud computing and
        CPS, addressing pressing challenges in energy management and real-time processing.
  total_score: 4.05
  tier: C
- title: 'STAR: An Efficient Softmax Engine for Attention Model with RRAM Crossbar'
  abstract: 'Various constraints of Static Random Access Memory (SRAM) are leading
    to

    consider new memory technologies as candidates for building on-chip shared

    last-level caches (SLLCs). Spin-Transfer Torque RAM (STT-RAM) is currently

    postulated as the prime contender due to its better energy efficiency, smaller

    die footprint and higher scalability. However, STT-RAM also exhibits some

    drawbacks, like slow and energy-hungry write operations, that need to be

    mitigated. In this work we address these shortcomings by leveraging a new

    management mechanism for STT-RAM SLLCs. This approach is based on the previous

    observation that the stream of references arriving at the SLLC of a Chip

    MultiProcessor (CMP) exhibits reuse locality, i.e., those blocks referenced

    several times manifest high probability of forthcoming reuse. In this paper, we

    employ a cache management mechanism that selects the contents of the SLLC aimed

    to exploit reuse locality instead of temporal locality. Specifically, our

    proposal consists in the inclusion of a Reuse Detector between private cache

    levels and the STT-RAM SLLC to detect blocks that do not exhibit reuse, in

    order to avoid their insertion in the SLLC, hence reducing the number of write

    operations and the energy consumption in the STT-RAM. Our evaluation reveals

    that our scheme reports on average, energy reductions in the SLLC in the range

    of 37-30\%, additional energy savings in the main memory in the range of 6-8\%

    and performance improvements of 3\% up to 14\% (16-core) compared to an STT-RAM

    SLLC baseline where no reuse detector is employed. More importantly, our

    approach outperforms DASCA, the state-of-the-art STT-RAM SLLC management,

    reporting SLLC energy savings in the range of 4-11\% higher than those of

    DASCA, delivering higher performance in the range of 1.5-14\%, and additional

    improvements in DRAM energy consumption in the range of 2-9\% higher than

    DASCA.'
  url: http://arxiv.org/abs/2402.00533v1
  keywords: ''
  document: "# 1. INTRODUCTION\n\nIn the last years chip multiprocessors have become\
    \ majority on many off-the-shelf systems, such as high performance servers, desktop\
    \ systems, mobile devices and embedded systems. In all of them the designers usually\
    \ include a multilevel memory hierarchy, where the shared last-level cache (SLLC)\
    \ plays an important role in terms of cost, performance and energy consumption.\
    \ As for the cost, the SLLC generally occupies a chip area similar or even bigger\
    \ than that of cores. Regarding performance and energy consumption, the SLLC is\
    \ the last resource before accessing the main memory, which delivers higher energy\
    \ consumption and lower performance as it is located outside\n\nthe chip.\n\n\
    The technologies currently employed in building SLLCs are mainly SRAM or embedded\
    \ DRAM. However, they both reveal as power-hungry, especially for the large sizes\
    \ required as the number of cores increases. One way of mitigating this problem\
    \ is to employ emerging non-volatile memory technologies. Among them, Spin-Transfer\
    \ Torque RAM (STT-RAM) is clearly the prime contender. STT-RAM removes almost\
    \ all the static power consumption and, compared to SRAM, provides higher density\
    \ and therefore much higher capacity within the same budget. Moreover, it delivers\
    \ higher read efficiency in terms of latency and energy. Nevertheless, some obstacles\
    \ restrict the adoptio n of STT-RAM as last-level cache for the next generation\n\
    \n2 RODRIGUEZ-RODRIGUEZ ET AL\n\nof CMPs: its write operation is slower and requires\
    \ more energy than an SRAM cache. These constraints may lead to a performance\
    \ drop and even to almost cancel the energy savings derived from the minimal static\
    \ power consumption of STT-RAM.\n\nIn addition, previous research states that\
    \ conventional SLLC designs are inefficient since they waste most storage space\
    \ [\\[1,](#page-20-0) [2\\]](#page-20-1). This is due to the fact that SLLC management\
    \ policies often lead to store *dead blocks*, i.e., blocks that will not be referenced\
    \ again before eviction. Indeed, it is frequent that blocks were dead the very\
    \ first time they enter into the SLLC. This is mainly because the cache levels\
    \ closer to the processor exploit most of the *temporal locality*, which therefore\
    \ becomes largely filtered before accessing the SLLC. With the goal of avoiding\
    \ this effect and hence increasing the hit rate, various mechanisms that modify\
    \ the SLLC insertion and replacement policies have been proposed recently.\n\n\
    This work addresses the shortcomings aforementioned by focusing on improving the\
    \ efficiency, in terms of both performance and energy, of a non-inclusive and\
    \ nonexclusive STT-RAM SLLC in a chip multiprocessor system. Notably, we present\
    \ a new mechanism of content selection for last-level caches that benefits from\
    \ the *reuse locality* that SLLC references exhibit [\\[3,](#page-20-2) [4\\]](#page-20-3).\
    \ This locality lies in the following principle: when a block is referenced twice\
    \ in the last level cache (i.e. it is reused), this block has a good chance of\
    \ being referenced again in the near future. Our approach pursues to insert in\
    \ the SLLC only those blocks that exhibit reuse at that level. For this purpose,\
    \ we propose to include a new hardware resource between the SLLC and the private\
    \ cache levels \u2013referred to as *Reuse Detector*\u2013 which determines for\
    \ each block evicted from the private cache levels if it has been reused or not\
    \ at the SLLC. If the block is determined to having been reused, it is inserted\
    \ (or it updates) in the SLLC. Otherwise, the block *bypasses* the SLLC and is\
    \ sent directly to main memory.\n\nOur proposal is evaluated in quad, eight and\
    \ 16-core systems running multiprogrammed workloads, and our experimental results\
    \ reveal that the reuse detector avoids the insertion of low-utility blocks in\
    \ the STT-RAM SLLC, making it easier to retain most of reused blocks. This enables\
    \ us to reduce the amount of the slow and energyhungry writes to the STT-RAM SLLC,\
    \ which translates into energy consumption reduction and system performance improvement,\
    \ outperforming other recent approaches.\n\nThe rest of the paper is organized\
    \ as follows: Section 2 motivates our work and explains some necessary background.\
    \ Section 3 presents our proposal to improve the STT-RAM LLC management. Sections\
    \ 4 and 5 detail the experimental framework used and the obtained results, respectively.\
    \ Section 6 recaps some related work and finally, Section 7 concludes the paper.\n\
    \n# 2. BACKGROUND AND MOTIVATION\n\nIn this section we first motivate the need\
    \ of a new SLLC management scheme by describing the main limitations of\n\n<span\
    \ id=\"page-1-1\"></span><span id=\"page-1-0\"></span>![](_page_1_Figure_8.jpeg)\n\
    \nFIGURE 1. (a) STT-RAM memory cell structure. (b) STT-RAM equivalent circuit.\n\
    \nSRAM technology and conventional management. Then we briefly describe the DASCA\
    \ scheme, which is the closest approach to our work and the state-of-the-art STT-RAM\
    \ SLLC management scheme [\\[1\\]](#page-20-0).\n\n# 2.1. SRAM and STT-RAM technologies\n\
    \nAs stated above, various emerging technologies are currently considered to replace\
    \ SRAM as the building-block for SLLCs, being STT-RAM the best placed to overcome\
    \ SRAM constraints, such as energy consumption and read operation latency.\n\n\
    The main difference between STT-RAM and SRAM is that the information carrier of\
    \ the former is a Magnetic Tunnel Junction (MTJ) instead of electric charges.\
    \ An MTJ contains two ferromagnetic layers (denoted as free and fixed layers)\
    \ and one tunnel barrier layer, see Figure [1\\(a\\).](#page-1-0) The fixed layer\
    \ has a fixed magnetic direction while the free layer can change its magnetic\
    \ direction by passing a current through the MTJ. If the two ferromagnetic layers\
    \ have different directions, the MTJ resistance is high, indicating a \"1\" state;\
    \ if the two layers have the same direction, the MTJ resistance is low, indicating\
    \ a \"0\" state. A read operation to an MTJ is performed by applying a small voltage\
    \ difference between two electrodes of the MTJ and sensing the current flow (see\
    \ Figure [1\\(b\\),](#page-1-1) where the STT-RAM cell is represented as a variable\
    \ resistor). A write operation is performed by applying a large voltage difference\
    \ between two electrodes for a given duration called write pulse width.\n\nTable\
    \ [1](#page-2-0) shows the key features of an 1-bank 1MB LLC implemented with\
    \ SRAM and STT-RAM 22 nm technology, modeled with CACTI 6.5 [\\[5\\]](#page-21-0)\
    \ and NVSim [\\[6\\]](#page-21-1), respectively. As shown, an STT-RAM cache exhibits\
    \ smaller die footprint and better efficiency in read operations than an SRAM\
    \ cache. More importantly, an STT-RAM cache consumes almost two orders of magnitude\
    \ less static power compared to SRAM. Conversely, the STT-RAM cache exhibits a\
    \ significant drawback that needs to be mitigated: the poor write performance\
    \ both in terms of latency and energy consumption.\n\n# 2.2. SLLC management techniques\n\
    \nRegardless of implementation technology, last-level caches usually suffer from\
    \ the same problem: they keep data\n\n| Parameter          | SRAM   | STT-RAM\
    \ | Ratio SRAM/STT-RAM |\n|--------------------|--------|---------|--------------------|\n\
    | Area (mm2<br>)     | 0.94   | 0.35    | 2.68               |\n| Read Latency\
    \ (ns)  | 8.75   | 5.61    | 1.56               |\n| Write Latency (ns) | 8.75\
    \   | 16.5    | 0.53               |\n| Read Energy (nJ)   | 0.56   | 0.32   \
    \ | 1.75               |\n| Write Energy (nJ)  | 0.56   | 1.31    | 0.43     \
    \          |\n| Leakage Power (mW) | 190.58 | 3.09    | 61.67              |\n\
    \n<span id=\"page-2-0\"></span>TABLE 1. Area, latency and energy consumption for\
    \ 22 nm SRAM and STT-RAM 1MB caches.\n\nassuming that recently referenced lines\
    \ are likely to appear in the near future (*temporal locality*). Nevertheless,\
    \ various recent studies point out that the reference stream entered in the SLLC\
    \ does not usually exhibit temporal locality. Notably, in [\\[3\\]](#page-20-2)\
    \ the authors observe that this reference stream exhibits *reuse locality* instead\
    \ of temporal locality. Essentially, that term describes the property that the\
    \ second reference to a line is a good indicator of forthcoming reuse and also\
    \ that recently reused lines are more valuable than other lines reused longer.\n\
    \nThe studies carried out in [\\[3,](#page-20-2) [4,](#page-20-3) [7\\]](#page-21-2)\
    \ demonstrate, considering a large amount of multiprogrammed workloads and different\
    \ multiprocessor configurations, two important aspects: first, most lines in the\
    \ SLLC are dead (they will not receive any further hits during their lifetime\
    \ in the SLLC) and second, most SLLC hits come from a small subset of lines. We\
    \ have performed our own analysis about the behavior of the blocks evicted from\
    \ the SLLC, in terms of the amount of accesses they receive before eviction, in\
    \ the scenario with the configuration detailed in Table [2,](#page-7-0) but, as\
    \ a starting point, employing just one core. Figure [2](#page-2-1) illustrates\
    \ this behavior, grouping the blocks into three different categories: no reuse,\
    \ just one reuse or more than one reuse (multiple reuse).\n\n![](_page_2_Figure_5.jpeg)\n\
    \n<span id=\"page-2-1\"></span>FIGURE 2. Breakdown of blocks replaced from the\
    \ LLC according to the number of accesses they receive before eviction.\n\nAs\
    \ shown, our experimental results confirm that most lines in the LLC are dead.\
    \ Notably, around 70% of the blocks do not receive any further access since the\
    \ moment they enter the LLC. Only around 5% of the blocks just receives one further\
    \ hit (i.e. one reuse) and around 25% exhibit more than one reuse.\n\nConsequently,\
    \ getting blocks with just one use (the block fill, so no reuse) to bypass the\
    \ LLC when they are evicted from the previous level caches, and just storing blocks\
    \ with reuse (at least two LLC accesses), should allow to hold the small fraction\
    \ of blocks with multiple reuses, increasing the LLC hit rate and improving system\
    \ performance.\n\nFurthermore, Figure [3](#page-2-2) shows that most LLC hits\
    \ are to blocks having multiple reuses, which together with the aforementioned\
    \ fact that most blocks inserted in the LLC do not experience any reuse, highly\
    \ justify the idea of a content selector based on reuse detection between private\
    \ caches and LLC.\n\n![](_page_2_Figure_11.jpeg)\n\n<span id=\"page-2-2\"></span>FIGURE\
    \ 3. Breakdown of block hits at the LLC according to the number of accesses they\
    \ have received before read.\n\n# 2.3. DASCA scheme\n\nIn [\\[1\\]](#page-20-0),\
    \ the authors propose *Dead Write Prediction Assisted STT-RAM Cache Architecture*\
    \ (DASCA) to predict and bypass dead writes (writes to data in last level caches\
    \ not referenced again during the lifetime of corresponding cache blocks) for\
    \ write energy reduction. In this work dead writes are classified into three categories:\
    \ dead-on-arrival fills, dead-value fills and closing writes, as a theoretical\
    \ model for redundant write elimination. On top of that they also present a dead\
    \ write predictor based on a state-of-the-art dead block predictor [\\[2\\]](#page-20-1).\
    \ Thus, DASCA bypasses a write operation to the SLLC only if it is predicted not\
    \ to incur extra cache misses.\n\nNotably, DASCA adds a specific field to each\
    \ line at the private levels to store the PC (program counter) of the instructions\
    \ writing a block, being this PC only updated upon write operations. Also, a PC-signature\
    \ table (prediction table) is included in the design in order to make the prediction\
    \ about dead writes (this table is updated according to the mechanism shown in\
    \ the Table 2 of the paper itself [\\[1\\]](#page-20-0)). Specifically, the mechanism\
    \ samples a few cache sets and keeps track of PC information only for those sets.\
    \ Predictions are made via the predictor table, made up of saturating counters\
    \ similar to those used in a bimodal branch predictor, being the counters indexed\
    \ by the signatures stored in the sampler entries. Thus, this PC-based predictor\
    \ correlates dead blocks with addresses of memory instructions (signatures), so\
    \ that different signatures are used depending on the kind of dead write predicted.\n\
    \n# 3. DESIGN\n\nIn this section we first describe the baseline system we start\
    \ from. Then we describe in detail the proposed design built on top of that.\n\
    \n# 3.1. Baseline system\n\nThe memory hierarchy used in the baseline multi-core\
    \ system includes two private levels (L1 and L2) and a lastlevel cache shared\
    \ among all the cores (SLLC). All caches are write-back, write-allocate and LRU.\
    \ L1 and L2 are inclusive while the SLLC is non inclusive.\n\nThe baseline management\
    \ of this memory hierarchy is as follows: When a block is requested to Main Memory\
    \ (MM), it is copied to the private cache levels of the requester core, but not\
    \ to the SLLC. During its lifetime at the private levels of the core, the block\
    \ can be requested by other cores, in which case a copy will be sent from a private\
    \ L2 cache to the L1-L2 caches of the requesting core, as dictated by the directory-based\
    \ coherency mechanism (please refer to Section [4](#page-6-0) for more details\
    \ on the coherency mechanism).\n\nWhen a block is evicted from an L2 cache, the\
    \ SLLC is checked: In case the block is not present there (either because it has\
    \ not been inserted yet or because it has already been inserted and evicted by\
    \ the SLLC replacement mechanism), it is inserted in the SLLC, otherwise if the\
    \ block is already in the SLLC, the block is updated or just discarded, depending\
    \ respectively on whether the block is dirty or clean. Thus, in our hierarchy,\
    \ SLLC insertions never come from MM but from an L2, in a similar way to an exclusive\
    \ policy. Note however that our mechanism differs from an exclusive policy in\
    \ that, as a result of a hit in the SLLC, the block is copied to the private cache\
    \ levels of the requester core, but maintained in the SLLC.\n\n# 3.2. The Reuse\
    \ Detector\n\nAs explained earlier, several works have demonstrated that a notable\
    \ percentage of the blocks inserted/updated in the SLLC are in fact useless, as\
    \ they are dead-blocks (i.e. blocks which will not be referenced any more during\
    \ their lifetime in the SLLC) [\\[1,](#page-20-0) [2\\]](#page-20-1). These useless\
    \ blocks are harmful, as they evict other blocks which could potentially be useful\
    \ in the future, and moreover, they increase the amount of writes to the SLLC,\
    \ which in the context of NVMs (Non-Volatile Memories) is far from convenient,\
    \ as explained in previous sections.\n\nIn this paper we leverage the technique\
    \ for reducing the amount of dead-blocks inserted/updated in the SLLC [\\[7\\\
    ]](#page-21-2) to improve the efficiency of a STT-RAM SLLC. In [\\[7\\]](#page-21-2),\
    \ the authors present a proposal that, in an exclusive memory hierarchy, reduces\
    \ the amount of blocks inserted in a conventional SLLC by around 90%. We apply\
    \ this technique to a different context, i.e., to a non-inclusive STT-RAM SLLC\
    \ design within a memory hierarchy where L1-L2 are inclusive. The exclusion policy\
    \ employed in [\\[7\\]](#page-21-2) implies that, upon a SLLC hit, the block is\
    \ copied to the private cache levels and removed from the SLLC. In our case, the\
    \ block is inserted in the SLLC at the end of its usage in the private caches\
    \ and remains in the SLLC until eviction. For our purpose, we include an intermediate\
    \ element between each private L2 cache and the SLLC (Figure [4\\)](#page-3-0).\
    \ A block evicted from the private L2 caches is targeted to the corresponding\
    \ element, which we denote as Reuse Detector (RD), instead\n\nof accessing directly\
    \ to the SLLC as it would do in the baseline system. The RD decides whether to\
    \ send the block to the SLLC or not (i.e. to bypass the shared last-level cache),\
    \ by means of a prediction about the future usefulness of the block. We must highlight\
    \ that, being the RD out of the block request path to the SLLC, it does not impact\
    \ the SLLC hit or miss latencies.\n\n![](_page_3_Figure_9.jpeg)\n\n<span id=\"\
    page-3-0\"></span>FIGURE 4. Placement of a Reuse Detector between each private\
    \ L2 level and STT-RAM SLLC.\n\nFor accomplishing the RD prediction, we apply\
    \ Albericio's concept of reuse locality [\\[3,](#page-20-2) [4\\]](#page-20-3).\
    \ As such, if the evicted block from the L2 has never been requested to the SLLC\
    \ since the time it entered the cache hierarchy (i.e. it has never been reused\
    \ at the SLLC), the block is predicted as a deadblock and thus it bypasses the\
    \ SLLC, directly updating MM (if the block is dirty) or being just discarded (if\
    \ it is clean). Otherwise, if the block has been reused (i.e. it has been requested\
    \ to the SLLC at least once since the time it entered the cache hierarchy) and\
    \ thus it is predicted as a non-deadblock, it is inserted/updated in the SLLC.\n\
    \nThe RD consists of a FIFO buffer and some management logic. The buffer stores\
    \ the tag addresses of the blocks evicted by the private levels in order to maintain\
    \ their reuse state. Moreover, an extra bit, called *reuse bit*, is added to each\
    \ cache line in the private levels. This bit distinguishes if the block was inserted\
    \ at the private cache levels from the SLLC or from another private cache level\
    \ (*reuse* bit is 1), or main memory (*reuse* bit is 0). In the following sections,\
    \ we analyze in detail the Reuse Detector operation and implementation.\n\n# *3.2.1.\
    \ Reuse Detector operation*\n\nAs we said in a previous section, our proposal\
    \ aims to reduce the amount of writes to the STT-RAM SLLC and to improve the management\
    \ of SLLC blocks, which translate into system performance improvement and energy\
    \ consumption reduction on the system.\n\nFigure [5](#page-4-0) shows a flow diagram\
    \ of a block request from a\n\ncore to its associated private caches. If the request\
    \ hits in L1 or L2 the reuse bit is untouched, and the block is copied in L1 if\
    \ it was not there (inclusive policy). Otherwise, the request is forwarded to\
    \ the SLLC. If the access hits in the SLLC, the block is provided to the core\
    \ and copied in the private levels with the reuse bit set. If the access misses\
    \ in the SLLC but the coherency mechanism informs that the block is present in\
    \ another private cache, the block is provided by that cache. In this case, the\
    \ access is recognized as a reuse, so the reuse bits are also set. Finally, if\
    \ no copy of the block is present in the cache hierarchy, it is requested to MM\
    \ and copied in L1-L2 with the reuse bit unset.\n\n![](_page_4_Figure_2.jpeg)\n\
    \n<span id=\"page-4-0\"></span>FIGURE 5. Block request and reuse bit management.\n\
    \nFigure [6](#page-4-1) shows a flow diagram of a block eviction from an L2 cache\
    \ (if required, the corresponding L1 cache is invalidated). When a block is evicted\
    \ from a last private level cache, its reuse bit is checked. If the reuse bit\
    \ is set, it means that the block was inserted into the private caches either\
    \ from the SLLC or from another private cache after checking the SLLC and the\
    \ coherency mechanism. In any case, the block is considered as having been reused,\
    \ and it should be inserted in the SLLC (if not present yet) or just updated (if\
    \ the block is dirty but it is already present in the SLLC). Note that if the\
    \ block is clean and already present in the SLLC, it can just be discarded. If\
    \ the reuse bit is unset (i.e. the block was brought into the private caches directly\
    \ from main memory) but the block's tag is found in the RD buffer, the block is\
    \ also considered as having been reused, and thus it is handled as in the previous\
    \ situation. Finally, if the reuse bit is unset and its tag is not present in\
    \ the RD buffer, it means that the block is considered as not having been reused\
    \ yet. Based again on Albericio's observations [\\[3,](#page-20-2) [4\\]](#page-20-3),\
    \ the block should bypass the SLLC, as it is predicted as a dead-block, and it\
    \ should be sent to MM (if the block is dirty) or just discarded (if it is clean).\
    \ Note that in all cases the coherency mechanism must be updated.\n\n![](_page_4_Figure_5.jpeg)\n\
    \n<span id=\"page-4-1\"></span>FIGURE 6. Block eviction from a private cache and\
    \ SLLC insertion.\n\n# *3.2.2. Example*\n\nFor the sake of clarifying the Reuse\
    \ Detector operation, in this subsection we provide a straightforward example\
    \ illustrating the flow of five memory blocks (A, B, C, D and E) through the different\
    \ cache levels under a given access pattern. In this example, we consider a dual-core\
    \ system (*Core*<sup>0</sup> and *Core*1) with private first level caches (*L*1<sup>0</sup>\
    \ and *L*11), a shared second level cache (SLLC), and the corresponding Reuse\
    \ Detectors between both cache levels. In the example we assume a simplified configuration\
    \ where: 1) direct-mapped L1s, 2-way set associative RDs and 4-way set associative\
    \ SLLC are considered; 2) all memory blocks map to the same L1 frame and to the\
    \ same RD and SLLC set; and 3) initially, all caches and RDs are empty. Next,\
    \ we detail the access sequence of our example and show the contents of the memory\
    \ hierarchy after each access in Figure [7.](#page-5-0) Note that we specify as\
    \ a subindex the dirty bit followed by the reuse bit (*Xd*,*<sup>r</sup>* ) for\
    \ each block *X* in the private cache levels, and only the dirty bit (*Xd*) for\
    \ each block *X* in the SLLC.\n\n1. *Core*<sup>0</sup> requests a word within\
    \ block A for reading: The access misses in *L*10, it is forwarded to the SLLC,\
    \ and given that the access to SLLC also misses and the block is not present in\
    \ any other private cache, it is forwarded to MM. According to Figure [5,](#page-4-0)\
    \ block A is copied to *L*1<sup>0</sup> with its reuse bit unset, and the requested\
    \ word is provided to *Core*0.\n\n![](_page_5_Figure_2.jpeg)\n\n<span id=\"page-5-0\"\
    ></span>FIGURE 7. Example of the Reuse Detector operation.\n\n- 2. *Core*<sup>1</sup>\
    \ requests a word within block A for reading: The access misses in *L*1<sup>1</sup>\
    \ and SLLC. However, the coherency mechanism informs that the block is at *L*10,\
    \ so the request is forwarded to that cache. According to Figure [5,](#page-4-0)\
    \ the block is copied to *L*1<sup>1</sup> and both reuse bits are set, as we recognize\
    \ this access as an SLLC reuse.\n- 3. *Core*<sup>1</sup> requests a word within\
    \ block B for reading: The access misses in *L*1<sup>1</sup> and SLLC, and the\
    \ block is not present in any other private cache, so the request is forwarded\
    \ to MM. According to Figure [5,](#page-4-0) block B is copied to *L*1<sup>1</sup>\
    \ (replacing block A) with its reuse bit unset, and the requested word is provided\
    \ to *Core*1. According to Figure [6,](#page-4-1) given that block A had its reuse\
    \ bit set, it is inserted into the SLLC.\n- 4. *Core*<sup>1</sup> requests a word\
    \ within block C for reading: Block C is inserted in *L*1<sup>1</sup> and block\
    \ B is replaced. As the reuse bit of block B was unset and its tag was not in\
    \ *RD*1, according to Figure [6](#page-4-1) the tag is stored in *RD*<sup>1</sup>\
    \ and, given that the block is clean, it is not inserted in the SLLC but just\
    \ discarded.\n- 5. *Core*<sup>1</sup> requests a word within block B for reading:\
    \ This\n\naccess is handled analogously to the previous access.\n\n- 6. *Core*<sup>1</sup>\
    \ requests a word within block D for reading: Block D is inserted in *L*1<sup>1</sup>\
    \ and block B is replaced. As the reuse bit of block B was unset but its tag was\
    \ present in *RD*1, according to Figure [6](#page-4-1) block B is inserted in\
    \ the SLLC.\n- 7. *Core*<sup>0</sup> writes to a word within block A: The access\
    \ hits in *L*10. The dirty bit for the block is set.\n- 8. *Core*<sup>0</sup>\
    \ requests a word within block E for reading: Block E is inserted in *L*1<sup>0</sup>\
    \ and block A is replaced. As the dirty bit of block A is set and A is already\
    \ present in the SLLC, the block is updated at this level.\n\n# <span id=\"page-5-1\"\
    ></span>*3.2.3. Implementation details*\n\nAlthough a typical set-associative\
    \ design could be used for the RD implementation, where a whole block tag, a validity\
    \ bit and some information related with the replacement policy is included for\
    \ each line, as in [\\[7\\]](#page-21-2) we use two techniques aimed at reducing\
    \ the required space: sectoring and compression. A sector is a set of consecutive\
    \ memory blocks aligned to the sector size. Storing sector tags in\n\nthe RD allows\
    \ to merge in a single line of the RD the information related with several blocks.\
    \ Note that for each entry it is necessary to store a presence bit. For example,\
    \ with a sector comprising 4-blocks, each line is made up of a tag derived from\
    \ the sector address, a validity bit, some bits storing the replacement state\
    \ and 4 presence bits.\n\nThe compression of the tags is achieved based on the\
    \ following process: let *t* be the amount of bits of the full tag and *c* the\
    \ amount of bits of the compressed tag, being t>c. We first divide the full tag\
    \ into several pieces, each of size *c* (the last piece is filled with *0s* if\
    \ necessary). Then, we *xor* all the pieces, obtaining the compressed tag. Note\
    \ that each compressed tag is shared among various sectors, thus false positives\
    \ are possible where non-reused blocks are delivered to the SLLC. This situation\
    \ does not cause a functional problem, but it may degrade system performance,\
    \ so the value of *c* must be chosen carefully.\n\nAs for the storage overhead\
    \ of the RD implementation, i.e., the total amount of extra bits required compared\
    \ to the baseline, we need the following hardware: The RD has 1024 sets and 16\
    \ ways (our simulations reveal that this value provides similar performance to\
    \ that of higher associativity values), and a sector size of 2 blocks. Each RD\
    \ entry requires 14 bits (10 for the compressed tag, 2 for the block presence,\
    \ 1 for the replacement policy and 1 validity bit) as Figure [8](#page-6-1) illustrates.\
    \ Given that the amount of entries in the RD is 8K, the total extra storage required\
    \ per core is 14 KB, which represents a negligible 1.3% of an 1MB last level cache.\n\
    \n| CompressedTag | S1 S1   SpL   V |                                        \
    \             |  |\n|---------------|-----------------|-----------------------------------------------------|--|\n\
    | Bits 13-4     |                 | Bit 3 Bit 2 Bit 2 Bit 1 - Bit 1 - Bit 1 -\
    \ Bit 0 - / |  |\n\n<span id=\"page-6-1\"></span>FIGURE 8. Reuse Detector entry.\n\
    \nFinally, as RD replacement policy we use a 1-bit FIFO. Again based on our simulations,\
    \ this scheme delivers a similar performance as other policies that would require\
    \ more storage. In a FIFO policy, age information is updated only when a new address\
    \ is inserted, and not during subsequent hits. This approach is fully consistent\
    \ with the main RD buffer goal of detecting the first reuse of a block.\n\n# 3.3.\
    \ Reuse Detector vs DASCA\n\nIn this section we briefly discuss the main differences\
    \ between the RD approach and the DASCA scheme, which will be further extended\
    \ and analyzed in the evaluation section. As for the operation of both approaches,\
    \ note that the DASCA mechanism tries to predict dead writes based on a PC-based\
    \ predictor. For this purpose, the PC signature of each block that accesses the\
    \ SLLC must be recorded. Conversely, the RD scheme tries to predict dead-blocks\
    \ based on their reuse state. Our prediction is based on the addresses of the\
    \ accessed data instead of the instruction addresses used in DASCA. Also, in our\
    \ approach we directly store the mentioned addresses while in the DASCA scheme\
    \ the authors employ a PC-signatures table which is trained by an auxiliary cache\
    \ that works in parallel with the conventional cache.\n\nFocusing on the specific\
    \ implementation of the DASCA scheme for the evaluation of this proposal, it is\
    \ worthy to note that our approach employs a memory hierarchy where L1 and L2\
    \ are inclusive while the SLLC (L3) is non inclusive, whereas the original DASCA\
    \ scheme is evaluated in [\\[1\\]](#page-20-0) employing a memory hierarchy with\
    \ just two cache levels and assuming non-inclusive caches by default, although\
    \ the authors also propose a bypassing scheme that supports both inclusive and\
    \ non-inclusive caches. Therefore, and looking for a fair comparison between RD\
    \ and DASCA, we implement DASCA using exactly the same three-level non-inclusive\
    \ non-exclusive hierarchy employed in the RD approach. As a result of that, the\
    \ only high-level change with respect to the original version of DASCA lies in\
    \ that one of the three possible cases they use to classify the dead writes (\"\
    dead-value fills\", blocks that receive a writeback request from lower-level caches\
    \ right after the block is filled, but before any read operation, i.e., the filled\
    \ block data are overwritten before being read), can not exist since they are\
    \ removed by the inclusion mechanism we employ in our approach. Notably, this\
    \ is due to the fact that, in our configuration, all the insertions in the SLLC\
    \ are motivated by an L2 block eviction, and, as L1 and L2 are inclusive, these\
    \ evicted blocks are only located in the SLLC after the eviction from L2. If after\
    \ that the processor generates a write request on one of these blocks, a write\
    \ hit occurs, and consequently the block is copied to the private cache levels\
    \ and therefore this block in the SLLC can not be written again in the SLLC before\
    \ being read. Hence, we consider this evaluation as fair, since this way we are\
    \ evaluating DASCA under the same conditions as our approach, so that we are not\
    \ giving an advantage to our RD by the fact that it directly avoids the \"dead-value\
    \ fills\" with the inclusiveness management.\n\n# <span id=\"page-6-0\"></span>4.\
    \ EXPERIMENTAL FRAMEWORK\n\nFor our experiments we use the *gem5* simulator [\\\
    [8\\]](#page-21-3) and we employ the *ruby* memory model, specifically the MOESI\\\
    _CMP-directory coherence policy provided by the simulator. It is worth noting\
    \ that we focus on a MOESI policy since protocols with owned state (e.g. MOESI\
    \ and MOSI) are able to reduce the number of writes to the LLC, as demonstrated\
    \ by Chang et.al. [\\[9\\]](#page-21-4). We modify the coherence protocol, encoding\
    \ the proposed reuse detector. We simulate both a single and a multi-core scenario.\
    \ For the sake of a better accuracy in both execution modes, an O3 processor type\
    \ (detailed mode of simulation) was used.\n\nThe main features of both the processor\
    \ and the memory hierarchy are shown in Table [2.](#page-7-0) The network used\
    \ is a crossbar modeled with Garnet [\\[10\\]](#page-21-5), a detailed interconnection\
    \ network model inside gem5. As explained above, for the evaluation of our proposed\
    \ RDs we implement them in the cache hierarchy modifying the coherence protocol.\
    \ For modeling the DRAM main memory we use DRAMSIM2 [\\[11\\]](#page-21-6). We\
    \ adapt the LLC read and 8 RODRIGUEZ-RODRIGUEZ ET AL\n\n| Architecture       \
    \  | x86                                                                     \
    \                    |\n|----------------------|---------------------------------------------------------------------------------------------|\n\
    | CPUS                 | 1/4/8, 2GHz                                         \
    \                                        |\n| Pipeline             | 8 Fetch,\
    \ 8 Decode, 8 Rename, 8 Issue/Execute/Writeback, 8 Commit                    \
    \        |\n| Registers            | Integer (256), Floating Point (256)     \
    \                                                    |\n| Buffers            \
    \  | Reorder Buffer (192), Instruction Queue (64)                            \
    \                    |\n| Branch Predictor     | TournamentBP                \
    \                                                                |\n| Functional\
    \ Units     | IntALU=6, IntMulDiv=2, FPALU=4, FPMultDiv=2, SIMD-Unit=4, RdWrPort=4,\
    \ IprPort=1             |\n| Private Cache L1 D/I | 32 KB, 8 ways, LRU replacement,\
    \ Block Size 64B, Access Latency 2 cycles, SRAM               |\n| Private Cache\
    \ L2 D/I | 256 KB, 16 ways, LRU replacement, Block Size 64B, Access Latency 5\
    \ cycles, SRAM             |\n| Interconnection      | Crossbar network, modeled\
    \ using Garnet, latency 3 cycles                                    |\n| Shared\
    \ Cache L3      | 1 bank/1MB/core, 16 ways, LRU replacement, Block Size 64B, R/W\
    \ Latency 6/17 cycles, STT-RAM |\n| DRAM                 | 2 Ranks, 8 Banks, 4kB\
    \ Page Size, DDR3 1066MHz                                               |\n| DRAM\
    \ Bus             | 2 channels with a 8 bus of 8 bits                        \
    \                                   |\n\n<span id=\"page-7-0\"></span>TABLE 2.\
    \ CPU and Memory Hierarchy specification.\n\n| High<br>Medium                \
    \    |                                    | Low                              \
    \  |  |\n|-----------------------------------|------------------------------------|------------------------------------|--|\n\
    | lbm, mcf, libquantum, bwaves,     | bzip2, soplex, gcc, wrf, astar,    | gromacs,\
    \ calculix, h264ref, tonto, |  |\n| milc, cactusADM, zeusmp, leslie3d | hmmer,\
    \ xalancbmk, gobmk, perlbench | omnetpp, namd, sphinx3, GemsFDTD   |  |\n\n<span\
    \ id=\"page-7-1\"></span>TABLE 3. Benchmark characterization according to the\
    \ number of LLC writes per Kinstruction (WPKI).\n\nwrite latencies according to\
    \ the STT-RAM target. Both latencies and energy consumption values are obtained\
    \ from NVSim [\\[6\\]](#page-21-1) for a 1MB (1 bank) cache and are illustrated\
    \ in Table [1.](#page-2-0) For scaling the LLC to larger sizes, we multiply the\
    \ leakage power by the number of cores.\n\nOur experiments make use of the SPEC\
    \ CPU2006 benchmark suite [\\[12\\]](#page-21-7). When we evaluate our proposal\
    \ in a single core scenario (LLC 1MB size) we employ *reference* inputs and simulate\
    \ 1 billion instructions from the checkpoint determined using PinPoints [\\[13\\\
    ]](#page-21-8). Note that results from 4 out of 29 benchmarks are not considered\
    \ in the evaluation section due to experimental framework constraints. We also\
    \ report results of 28 multiprogrammed mixes employing SPEC CPU2006 programs in\
    \ 4, 8 and 16- CMP systems with 4, 8 and 16MB SLLC sizes, respectively. In all\
    \ the cases, we fast forward 100M instructions, warm up caches for 200M instructions\
    \ and then report results for at least 500M instructions per core.\n\nFor selecting\
    \ the aforementioned multiprogrammed mixes, we employ the following methodology:\
    \ we execute each benchmark alone, using an LLC of 1MB and without any reuse detector,\
    \ and we measure the amount of LLC writes that it generates. We then obtain for\
    \ each benchmark the *number of writes to LLC per 1000 instructions*ratio\n\n\
    (WPKI). Based on these values, we include each benchmark into the *high*, *medium*\
    \ or *low* category. Specifically, the *high* category includes benchmarks with\
    \ a WPKI higher than 8, the *medium* one those with a WPKI satisfying 1 <*WPKI*\
    \ < 8 and finally, in the *low* category we include the programs with a WPKI lower\
    \ than 1. Table [3](#page-7-1) shows this classification. Based on this classification,\
    \ and as detailed in Section [5,](#page-9-0) we build some mixes made up of programs\
    \ with high WPKI, some with medium WPKI, some with low WPKI, and some combining\
    \ applications from different WPKI categories trying to fill most of the combinations\
    \ high-medium, high-low, medium-low and high-medium-low. Tables [4,](#page-7-2)\
    \ [5](#page-8-0) and [6](#page-8-1) show the built mixes for the 4-core, 8-core\
    \ and 16-core CMP systems, respectively, where for each mix the applications are\
    \ sorted first by decreasing WPKI categories (from the high category to the low\
    \ one) and then within each of the categories they are also sorted alphabetically.\
    \ We employ the symbol \";\" to separate applications from different categories\
    \ and also the parenthesis to indicate how many instances of an application (when\
    \ employ more than one) we are using in each mix.\n\nEnergy model: The DRAM energy\
    \ is obtained directly from the simulator. For computing the LLC energy we employ\
    \ a model that includes both dynamic and static\n\n| Mixes    | Applications \
    \                        | Mixes    | Applications                           |\n\
    |----------|--------------------------------------|----------|----------------------------------------|\n\
    | mix.H0   | cactusADM, leslie3d, libquantum, mcf | mix.H1   | bwaves, cactusADM,\
    \ leslie3d, milc      |\n| mix.H2   | libquantum, mcf, milc, zeusmp        | mix.H3\
    \   | bwaves, cactusADM, lbm, leslie3d       |\n| mix.M0   | bzip2, gobmk, soplex,\
    \ xalancbmk      | mix.M1   | gcc, perlbench, wrf, xalancbmk         |\n| mix.M2\
    \   | gcc, gobmk, hmmer, soplex            | mix.M3   | astar, gobmk, perlbench,\
    \ wrf           |\n| mix.L0   | calculix, GemsFDTD, namd, sphinx3    | mix.L1\
    \   | gromacs, h264ref, omnetpp, tonto       |\n| mix.L2   | calculix, GemsFDTD,\
    \ omnetpp, sphinx3 | mix.L3   | gromacs, h264ref, namd, tonto          |\n| mix.HM0\
    \  | milc, zeusmp; astar, gcc             | mix.HM1  | lbm, leslie3d, libquantum;\
    \ gobmk       |\n| mix.HM2  | milc, zeusmp; gcc, gobmk             | mix.HM3 \
    \ | bwaves, mcf; soplex, xalancbmk         |\n| mix.HL0  | bwaves, cactusADM,\
    \ leslie3d; omnetpp | mix.HL1  | bwaves, lbm, libquantum; omnetpp       |\n| mix.HL2\
    \  | lbm, leslie3d; gromacs, namd         | mix.HL3  | leslie3d, milc; GemsFDTD,\
    \ omnetpp      |\n| mix.ML0  | perlbench; gromacs, sphinx3, tonto   | mix.ML1\
    \  | hmmer, wrf; gromacs,h264ref            |\n| mix.ML2  | perlbench, wrf; GemsFDTD,\
    \ namd       | mix.ML3  | soplex, xalancbmk; sphinx3, tonto      |\n| mix.HML0\
    \ | mcf; hmmer; h264ref, omnetpp         | mix.HML1 | milc; hmmer; GemsFDTD, h264ref\
    \         |\n| mix.HML2 | milc; bzip2, wrf; GemsFDTD           | mix.HML3 | bwaves,\
    \ leslie3d; xalancbmk; GemsFDTD, |\n\n<span id=\"page-7-2\"></span>TABLE 4. SPEC\
    \ 2006 multiprogrammed mixes for the 4-core CMP.\n\n| Mixes    | Applications\
    \                                                             |\n|----------|--------------------------------------------------------------------------|\n\
    | mix.H0   | bwaves, cactusADM, lbm, leslie3d, libquantum, mcf, milc, zeusmp \
    \         |\n| mix.H1   | bwaves, cactusADM(2), leslie3d(2), libquantum, mcf,\
    \ milc                 |\n| mix.H2   | bwaves, cactusADM, leslie3d, libquantum,\
    \ mcf, milc(2), zeusmp            |\n| mix.H3   | cactusADM, lbm, leslie3d, libquantum(2),\
    \ milc, zeusmp(2)                 |\n| mix.M0   | astar, gcc, gobmk, hmmer, perlbench,\
    \ soplex, wrf, xalancbmk              |\n| mix.M1   | bzip2, gcc, perlbench(2),\
    \ soplex, wrf, xalancbmk(2),                     |\n| mix.M2   | gcc, gobmk(2),\
    \ hmmer, perlbench, soplex(2), xalancbmk                    |\n| mix.M3   | astar,\
    \ gobmk(2), hmmer, perlbench, soplex, wrf(2)                        |\n| mix.L0\
    \   | calculix, GemsFDTD, gromacs, h264ref, namd, omnetpp, sphinx3, tonto    \
    \  |\n| mix.L1   | GemsFDTD, gromacs, h264ref(2), namd, omnetpp(2), tonto    \
    \               |\n| mix.L2   | gromacs(2), h264ref(2), namd, omnetpp, tonto(2)\
    \                          |\n| mix.L3   | calculix, GemsFDTD(2), gromacs, namd,\
    \ omnetpp, tonto(2)                  |\n| mix.HM0  | cactusADM, leslie3d, libquantum,\
    \ mcf; bzip2, gobmk, soplex, xalancbmk    |\n| mix.HM1  | lbm, mcf, milc, zeusmp;\
    \ gcc, hmmer, perlbench, wrf                       |\n| mix.HM2  | bwaves, cactusADM,\
    \ libquantum, milc; astar, gobmk, perlbench, soplex     |\n| mix.HM3  | cactusADM,\
    \ lbm, leslie3d, zeusmp; gcc, gobmk, soplex, wrf                |\n| mix.HL0 \
    \ | cactusADM, leslie3d, libquantum, mcf; calculix, GemsFDTD, namd, sphinx3  |\n\
    | mix.HL1  | lbm, mcf, milc, zeusmp; gromacs, h264ref, omnetpp, tonto        \
    \         |\n| mix.HL2  | bwaves, lbm, libquantum, milc; GemsFDTD, h264ref, sphinx3,\
    \ tonto         |\n| mix.HL3  | cactusADM, lbm, leslie3d, zeusmp; calculix, gromacs,\
    \ namd, omnetpp       |\n| mix.ML0  | bzip2, gobmk, soplex, xalancbmk; calculix,\
    \ GemsFDTD, namd, sphinx3       |\n| mix.ML1  | astar, hmmer, perlbench, wrf;\
    \ h264ref, gromacs, omnetpp, tonto           |\n| mix.ML2  | gcc, gobmk, perlbench,\
    \ xalancbmk; GemsFDTD, gromacs, omnetpp, sphinx3    |\n| mix.ML3  | bzip2, hmmer,\
    \ soplex, wrf; calculix, h264ref, namd, tonto                |\n| mix.HML0 | bwaves,\
    \ cactusADM, leslie3d; gobmk, soplex, xalancbmk; GemsFDTD, sphinx3 |\n| mix.HML1\
    \ | lbm, libquantum, mcf; astar, perlbench; calculix, h264ref, namd          |\n\
    | mix.HML2 | cactusADM, milc, zeusmp; hmmer, wrf, xalancbmk; h264ref, tonto  \
    \         |\n| mix.HML3 | leslie3d, mcf, milc; perlbench, soplex; gromacs, namd,\
    \ omnetpp           |\n\n<span id=\"page-8-0\"></span>TABLE 5. SPEC 2006 multiprogrammed\
    \ mixes for the 8-core CMP.\n\n| Mixes    | Applications                     \
    \                                                                            \
    \                             |\n|----------|-------------------------------------------------------------------------------------------------------------------------------------------|\n\
    | mix.H0   | bwaves(2), cactusADM(3), lbm, leslie3d(3), libquantum(2), mcf(2),\
    \ milc(2), zeusmp                                                         |\n\
    | mix.H1   | bwaves(2), cactusADM, lbm(2), libquantum(2), mcf(3), milc(4), zeusmp(2)\
    \                                                                   |\n| mix.H2\
    \   | bwaves, cactusADM, lbm(3), leslie3d(4), libquantum(3), mcf(2), milc, zeusmp\
    \                                                               |\n| mix.H3  \
    \ | bwaves(3), cactusADM(2), lbm(2), leslie3d(2), libquantum, mcf, milc(2), zeusmp(3)\
    \                                                         |\n| mix.HM0  | bwaves,\
    \ cactusADM, lbm, leslie3d, libquantum, mcf, milc, zeusmp; astar, gcc, gobmk,\
    \ hmmer, perlbench, soplex, wrf, xalancbmk              |\n| mix.HM1  | bwaves,\
    \ cactusADM(2), leslie3d(2), libquantum, mcf, milc; bzip2, gcc, perlbench(2) soplex,\
    \ wrf, xalancbmk(2)                              |\n| mix.HM2  | bwaves, cactusADM,\
    \ libquantum, leslie3d, mcf, milc(2), zeusmp; gcc, gobmk(2), hmmer, perlbench,\
    \ soplex(2), xalancbmk                      |\n| mix.HM3  | bwaves, cactusADM,\
    \ lbm, leslie3d, libquantum, mcf, milc, zeusmp; astar, gobmk(2), hmmer, perlbench,\
    \ soplex, wrf(2)                        |\n| mix.M0   | astar(2), bzip2(2), gcc,\
    \ gobmk(3), hmmer(2), perlbench(2), soplex, wrf(2), xalancbmk                \
    \                                      |\n| mix.M1   | astar, bzip2(2), gcc, gobmk,\
    \ hmmer, perlbench, soplex(2), wrf(3), xalancbmk(4)                          \
    \                                  |\n| mix.M2   | astar, bzip2, gcc, gobmk(3),\
    \ hmmer(3), perlbench(2), soplex(4), xalancbmk                               \
    \                                  |\n| mix.M3   | astar(2), bzip2, gcc(3), perlbench,\
    \ soplex(2), wrf(4), xalancbmk(3)                                            \
    \                           |\n| mix.L0   | calculix(2), GemsFDTD(2), gromacs(2),\
    \ h264ref(2), namd(2), omnetpp(2), sphinx3(2), tonto(2)                      \
    \                         |\n| mix.L1   | calculix, GemsFDTD(3), gromacs(3), h264ref,\
    \ namd(2), omnetpp(2), sphinx3(3), tonto                                     \
    \                   |\n| mix.L2   | calculix(2), GemsFDTD(3), gromacs(3), h264ref(3),\
    \ namd(3), tonto(2)                                                          \
    \             |\n| mix.L3   | calculix(4), gromacs(2), h264ref(2), omnetpp(4),\
    \ sphinx3(2), tonto(2)                                                       \
    \              |\n| mix.HL0  | cactusADM, lbm, leslie3d, libquantum, mcf(2), milc,\
    \ zeusmp; calculix, GemsFDTD, gromacs, h264ref, namd, omnetpp, sphinx3, tonto\
    \           |\n| mix.HL1  | bwaves, lbm(2), libquantum, mcf, milc(2), zeusmp;\
    \ GemsFDTD, gromacs, h264ref(2), omnetpp, sphinx3, tonto(2)                  \
    \             |\n| mix.HL2  | cactusADM, lbm(2), leslie3d, mcf, milc, zeusmp(2);\
    \ calculix, gromacs(2), h264ref, namd, omnetpp(2), tonto                     \
    \            |\n| mix.HL3  | bwaves, cactusADM, lbm, leslie3d, libquantum(2),\
    \ mcf, milc; calculix, GemsFDTD(2), h264ref, namd, sphinx3(2), tonto         \
    \              |\n| mix.ML0  | astar, bzip2, gobmk, hmmer, perlbench, soplex,\
    \ wrf, xalancbmk; calculix, GemsFDTD, gromacs, h264ref, namd, omnetpp, sphinx3,\
    \ tonto        |\n| mix.ML1  | astar(2), gcc, perlbench(2), soplex(2), xalancbmk(2);\
    \ GemsFDTD(3), h264ref(2), tonto(2)                                          \
    \         |\n| mix.ML2  | astar, bzip2, hmmer(2), perlbench, soplex, wrf(2); calculix,\
    \ gromacs, h264ref(2), namd, omnetpp, tonto(2)                               \
    \  |\n| mix.ML3  | bzip2, gcc(2), gobmk(2), perlbench, soplex, xalancbmk(2); calculix,\
    \ GemsFDTD, gromacs, namd, omnetpp, sphinx3(2)                          |\n| mix.HML0\
    \ | bwaves, cactusADM, lbm, leslie3d, libquantum, mcf; astar, gobmk, perlbench,\
    \ soplex, xalancbmk; calculix, GemsFDTD, h264ref, namd, sphinx3 |\n| mix.HML1\
    \ | cactusADM, leslie3d, mcf, milc(2), zeusmp; hmmer, perlbench, soplex, wrf,\
    \ xalancbmk; gromacs, h264ref, namd, omnetpp, tonto               |\n| mix.HML2\
    \ | lbm, leslie3d, libquantum, mcf(2), milc; astar, perlbench(2), soplex; calculix,\
    \ gromacs, h264ref, namd(2), omnetpp                        |\n| mix.HML3 | bwaves,\
    \ cactusADM(2), leslie3d, milc, zeusmp; gobmk, hmmer, soplex, wrf, xalancbmk(2);\
    \ GemsFDTD, h264ref, sphinx3, tonto                  |\n\n<span id=\"page-8-1\"\
    ></span>TABLE 6. SPEC 2006 multiprogrammed mixes for the 16-core CMP.\n\ncontributions.\
    \ The static component is calculated using NVSim [\\[6\\]](#page-21-1), which\
    \ reports the leakage number for 1MB LLC. Thus, we multiply that number by the\
    \ execution time and the number of cores to obtain the total static energy. In\
    \ the case of the dynamic component, we again use NVSim for determining the dynamic\
    \ energy consumption per access to the LLC. Then, we compute the dynamic energy\
    \ consumption as follows:\n\n$$\\begin{aligned} \\text{Dynamic Energy} &= H\\\
    _{LLC} \\ast H E\\_{LLC} + W\\_{LLC} \\ast W E\\_{LLC} + \\\\ &M\\_{LLC} \\ast\
    \ M E\\_{LLC} \\end{aligned} \\tag{1}$$\n\nwhere H*LLC*, W*LLC* and M*LLC* denote\
    \ the number of hits, writes and misses in the LLC respectively, and HE*LLC*,\
    \ WE*LLC* and ME*LLC* correspond to the energy consumption of a hit, a write and\
    \ a miss in the LLC respectively.\n\n# <span id=\"page-9-0\"></span>5. EVALUATION\n\
    \nThis section compares how well RD and DASCA behave when managing an STT-RAM\
    \ LLC, both in terms of performance and energy consumption of LLC and main memory.\
    \ Single, four and eight-core systems are discussed in Sections [5.1,](#page-9-1)\
    \ [5.2,](#page-11-0) and [5.3,](#page-15-0) respectively. Finally, in Section\
    \ [5.4,](#page-17-0) we show the results derived from the evaluation in a 16-core\
    \ CMP and we analyze the performance of both approaches as the number of cores\
    \ augments.\n\n# <span id=\"page-9-1\"></span>5.1. Evaluation in a single-core\
    \ scenario\n\nFirst, we show the number of writes to the LLC that each evaluated\
    \ proposal involves as well as the performance delivered. Then, we focus on the\
    \ involved energy consumption in both the STT-RAM and the main memory according\
    \ to the model detailed in Section [4.](#page-6-0) Finally, we discuss the obtained\
    \ results. All the graphs shown in this section report individual data for each\
    \ benchmark, adding at the right end the arithmetic mean considering all data\
    \ (labeled as *AVG*) or the geometric mean (labeled as *GMEAN*) in the case of\
    \ the performance metric, and also the arithmetic mean of the eight most write-intensive\
    \ benchmarks according to Table [3](#page-7-1) or the corresponding geometric\
    \ mean for the performance metric (labeled as *HIGH* in both cases).\n\n# *5.1.1.\
    \ Write filtering*\n\nFigure [9](#page-10-0) illustrates the number of writes\
    \ to the STT-RAM LLC generated by the DASCA scheme and our proposal (using a RD\
    \ of 8K entries) normalized to a baseline system without any write reduction/filtering\
    \ scheme.\n\nAs shown, our proposal significantly outperforms DASCA. Notably,\
    \ in 20 out of 25 benchmarks evaluated the Reuse Detector exhibits higher ability\
    \ in cutting the write traffic to the STT-RAM LLC. Overall, the block bypassing\
    \ decisions commanded by RD reduce the number of LLC writes in the baseline system\
    \ around 65% whereas DASCA just achieves a 52% reduction. In addition, if we zoom\
    \ just in the 8 programs with highest WPKI numbers (those labeled as *high* in\
    \ Table [3\\)](#page-7-1), RD reduces the number of LLC writes by around 80% with\
    \ respect to the baseline, while DASCA cuts the write traffic by 66%.\n\n# *5.1.2.\
    \ Performance*\n\nApart from the goal of decreasing the STT-RAM LLC energy consumption\
    \ (quantified later in this section), it is clear that energy efficiency should\
    \ not come at the expense of a performance drop. Thus, to further evaluate the\
    \ benefits of RD, Figure [10](#page-10-1) shows the performance (IPC) delivered.\n\
    \nOverall our scheme performs moderately better than DASCA: RD delivers 1.9% performance\
    \ improvement compared to the baseline while DASCA just improves IPC by 0.3%.\
    \ If we focus on the write-intensive applications RD clearly outperforms DASCA,\
    \ achieving performance improvements of 5% and 1.4%, respectively. This reveals,\
    \ as we will confirm later in the multi-core environment, that\n\nour approach\
    \ works especially well for those applications for which the number of writes\
    \ performed to the LLC is high.\n\n# *5.1.3. Energy savings*\n\nFigure [11](#page-10-2)\
    \ shows the total energy savings (adding both the dynamic and the static components)\
    \ in the LLC. Overall, our proposal reports 34.5% energy reduction compared to\
    \ the baseline while DASCA reports 29.5%. Considering only the write-intensive\
    \ programs, the numbers are 60% and 49%, respectively. If we split the total energy\
    \ savings with respect to the baseline into the dynamic and static parts, our\
    \ proposal achieves 50% of reduction in the dynamic part considering all the applications\
    \ (68% for the *high* programs), while DASCA obtains 42% (57% for the *high* benchmarks).\
    \ As for the static part RD is able to obtain 2% energy savings (around 5% for\
    \ the *high programs*) while DASCA just achieves 0.3% (1.4% for the write-intensive\
    \ applications). Note that avoiding LLC writes reduces dynamic energy, whereas\
    \ increasing performance translates into static energy savings. It is also worth\
    \ noting that, as Figure [12](#page-11-1) illustrates, the dynamic energy consumption\
    \ in the LLC of the baseline system is, for most of the applications evaluated,\
    \ significantly higher than the static contribution.\n\nFinally, we have also\
    \ explored the impact on the energy consumption in the DRAM main memory. For the\
    \ sake of simplicity, we do not show the results for all the applications. However,\
    \ as expected, the DRAM energy reduction follows the trend of performance improvement.\
    \ Overall, our proposal manages to reduce the DRAM energy consumption by 2% (4.7%\
    \ for the write-intensive programs) with respect to the baseline while DASCA just\
    \ improves the memory energy consumption by 0.2% (1.1% for the *high* applications).\n\
    \n# *5.1.4. Discussion*\n\nIf we zoom into specific benchmarks, there are some\
    \ special cases that deserve further detail to get a deeper insight. Note that\
    \ globally, the relative trend shown in the amount of writes to the LLC between\
    \ our approach and DASCA for each benchmark, is mainly held in the energy consumption\
    \ differences, although modulated with the relative performance numbers. However,\
    \ there are some few exceptions such as *namd*, *GemsFDTD* or *omnetpp*, where\
    \ RD is able to reduce the amount of LLC writes significantly more than DASCA\
    \ but the energy savings and also the performance improvements obtained by both\
    \ techniques are almost the same (and quite low compared to the baseline) in all\
    \ these three cases. The reason is that these programs are three of the four benchmarks\
    \ that exhibit the lowest values of WPKI, so although the write reductions that\
    \ RD achieves in relative terms compared to DASCA is significant for these applications,\
    \ the corresponding reduction in absolute values are very modest, and therefore\
    \ the impact on the energy is almost negligible.\n\nAlso, in other applications\
    \ such as *mcf*, *cactusADM* or *hmmer*, our approach is able to report IPC numbers\
    \ significantly higher than in DASCA, while both techniques\n\n![](_page_10_Figure_0.jpeg)\n\
    \nFIGURE 9. Number of writes to the STT-RAM LLC normalized to the baseline: SPEC\
    \ CPU2006 suite.\n\n<span id=\"page-10-0\"></span>![](_page_10_Figure_2.jpeg)\n\
    \n<span id=\"page-10-1\"></span>FIGURE 10. Performance (Instructions per Cycle)\
    \ normalized to the baseline: SPEC CPU2006 suite.\n\nexhibit quite similar write\
    \ reduction capabilities. In order to explain that, first note that there are\
    \ many different aspects involved in the system performance delivered. Among others,\
    \ one key aspect is that reducing the amount of writes to the LLC is not sufficient\
    \ in itself to guarantee performance improvements: although the main goals when\
    \ bypassing blocks from the LLC to main memory are both to save energy and improve\
    \ performance by increasing the hit rate in the LLC, obviously the bypassing may\
    \ fail in the sense that a bypassed block could be referenced again soon, leading\
    \ to a LLC miss and even a performance drop with respect to the case where bypassing\
    \ is not carried out. Thus, for all these\n\nthree benchmarks, the experimental\
    \ data reveal that with our proposal the amount of hits in the LLC clearly outperforms\
    \ both the baseline and the DASCA mechanism. Notably, the amount of LLC hits experienced\
    \ in the *cactusADM* and *mcf* programs are 7.23x and 2x the values obtained in\
    \ the baseline, while DASCA obtains 1.89x and 0.89x, respectively. Also, the amount\
    \ of misses in the LLC is lower than that of the baseline and DASCA, with values\
    \ ranging between 0.77-0.87x those obtained in the baseline. Considering all the\
    \ evaluated benchmarks, RD is able to improve the amount of hits around 31% with\
    \ respect to the baseline (106% if we only consider the write-intensive\n\n![](_page_10_Figure_6.jpeg)\n\
    \n<span id=\"page-10-2\"></span>FIGURE 11. Energy consumption in the STT-RAM LLC\
    \ normalized to the baseline: SPEC CPU2006 suite.\n\n![](_page_11_Figure_1.jpeg)\n\
    \n<span id=\"page-11-1\"></span>FIGURE 12. Breakdown of energy consumption in\
    \ the LLC into the static and dynamic contributions for the baseline in the singlecore\
    \ system.\n\napplications) while DASCA experiments only 5% increment when considering\
    \ all the benchmarks and 31% for the *high* applications.\n\nAt a first glance,\
    \ the behavior of the *libquantum* application may seem somehow strange: Neither\
    \ RD nor DASCA are able to significantly reduce the amount of writes to the LLC,\
    \ but however this benchmark running under RD reports a performance improvement\
    \ of 7% with respect to the baseline while the performance remains largely unchanged\
    \ under DASCA. In addition, and as one would expect since the number of bypasses\
    \ is low, the number of hits in the LLC is practically the same in the three cases.\
    \ The reason to explain the performance improvement lies in the LLC bank contention\
    \ due to the write activity: this application is by far the most stalled one due\
    \ to write contention. Thus, although the write reduction is very limited with\
    \ our scheme, it is enough to reduce stalls with respect to the baseline by around\
    \ 8%, which in absolute numbers implies various millions of these kind of situations\
    \ avoided, which leads to the performance improvement obtained.\n\nConversely,\
    \ although other benchmarks such as *gromacs*, *calculix* or *wrf* exhibit moderate\
    \ LLC writes reduction with RD and DASCA, they all perform worse than in the baseline.\
    \ For these three programs the amount of hits experienced in the LLC is, in RD\
    \ and DASCA, lower than in the baseline, which suggests that the bypassing performed\
    \ is not efficient for these benchmarks. Recall that the energy savings achieved\
    \ in the LLC as a consequence of the reduction in the number of writes performed\
    \ in this cache level may be partially offset with the performance drop derived\
    \ from the increment in the amount of LLC misses, as in these three programs occurs.\
    \ Note also that, although the write operations are outside the critical path,\
    \ the performance improvement derived from avoiding the long write operations\
    \ may be mitigated if bank contention exists between the writes effectively performed.\n\
    \n# <span id=\"page-11-0\"></span>5.2. Evaluation in a 4-core CMP system\n\nIn\
    \ this section we extend the previous single-core analysis to a more up-to-date\
    \ environment: a multi-core scenario where the LLC is shared among different cores.\
    \ For this purpose, we measure again the number of writes to the SLLC, the performance\
    \ and the energy consumption in both the STT-RAM SLLC and the DRAM main memory\
    \ for RD and DASCA and report results normalized to the baseline. However, due\
    \ to the inherent non-determinism that all simulators exhibit (especially in multi-core\
    \ environments, where the number of instructions executed across different schemes\
    \ are not stable owing to the random interleaving among memory accesses of different\
    \ programs) and for the sake of higher accuracy, we employ in this scenario, as\
    \ well as in the 8 and 16-core CMP systems, the arithmetic mean of the number\
    \ of writes and energy consumption (per application) but *divided by the total\
    \ number of instructions executed*. Note that, conversely, in the single-core\
    \ scenario both kind of metrics match, since all the benchmarks execute the same\
    \ amount of instructions (1B) in all the runs.\n\nWe employ 28 mixes made up of\
    \ applications from the SPEC CPU2006 suite chosen accordingly to the WPKI categories\
    \ illustrated in Table [3.](#page-7-1) First, we randomly compose three groups\
    \ of 4 mixes made up of applications belonging to just one WPKI category (mixes\
    \ referred to as H*i*, M*i* and L*i* for high, medium and low WPKI respectively).\
    \ Then, we build other 16 mixes merging applications with WPKI corresponding to\
    \ different categories and trying to construct them in a balanced and homogeneous\
    \ fashion. Again, the workload name encodes the WPKI categories of the applications.\
    \ For example, HL2 is the third mix we build consisting of applications with high\
    \ WPKI (2 in this case) and applications with low WPKI (other two). The detailed\
    \ mixes are illustrated in Table [4.](#page-7-2) Most graphs in this section report\
    \ individual results for each mix, the arithmetic mean (AVG) considering all the\
    \ mixes, just the 4 H*i* mixes (HIGH), the 4 H*i* and the 4 HM*i* mixes together\
    \ (H+HM), the 4 H*i*, the 4 HM*i* and the 4 HML*i* mixes together (H+HM+HML) and\
    \ all the mixes including at least a high program (SomeH). Again, in the case\
    \ of the performance metric we employ the geometric mean instead of the arithmetic\
    \ one.\n\n# *5.2.1. Write filtering*\n\nFigure [13](#page-12-0) illustrates the\
    \ number of writes to the STT-RAM SLLC generated by using DASCA and an 8K-entry\
    \ RD per core normalized to a baseline STT-RAM without any write reduction mechanism.\n\
    \nThe experimental results reveal that RD exhibits a significantly greater ability\
    \ to decrease the amount of writes to the SLLC than DASCA. Notably, in 25 out\
    \ of the 28 mixes evaluated RD outperforms DASCA. Overall, the number of writes\
    \ in the baseline system gets reduced to 37% by using RD, in contrast with DASCA\
    \ which only achieves a 48%. As for the write-intensive mixes the RD and DASCA\
    \ makes around 40% and 46% of the writes the baseline performs, respectively.\n\
    \n![](_page_12_Figure_0.jpeg)\n\n<span id=\"page-12-0\"></span>FIGURE 13. Number\
    \ of writes to the STT-RAM SLLC normalized to the baseline in the 4-core CMP system.\n\
    \n# <span id=\"page-12-1\"></span>*5.2.2. Performance*\n\nIn order to evaluate\
    \ performance when executing multiprogrammed workloads, we analyze the *Instruction\
    \ Throughput* (IT) and the *Weighted Speedup* (WS) metrics. The IT metric is defined\
    \ as the sum of all instructions committed per cycle in the entire chip (\u2211\
    \ *n i*=1 *IPC<sup>i</sup>* , being *n* the number of threads), while the WS is\
    \ defined as the slowdown experienced by each application in a mix, compared to\
    \ its run under the same configuration when no other application is running on\
    \ other cores (\u2211 *n i*=1 (*IPCshared i* /*IPCalone i* )). For the sake of\
    \ simplicity and since in our context the WS does not constitute a metric as significant\
    \ as IT, we do not show the WS results obtained. Anyway, these results follow\
    \ an analogous trend to those obtained when we evaluate the instruction throughput.\
    \ Figure [14](#page-13-0) illustrates the IT that each evaluated policy delivers\
    \ normalized to the baseline.\n\nAs shown, RD moderately outperforms DASCA. This\
    \ is a key contribution of RD, since our approach, managing to reduce the amount\
    \ of writes to the SLLC to a greater extent than DASCA, is also able to deliver\
    \ higher performance (which also allows to report higher energy savings in both\
    \ the SLLC and the main memory as shown later). The data reveal that, overall,\
    \ RD improves performance by around 3% compared to the baseline, while DASCA just\
    \ improves it by around 1.2%. Moreover, we can observe that, in almost all of\
    \ the 28 mixes evaluated (except mainly those mixes made up of benchmarks with\
    \ a reduced WPKI, those labeled as *low*, where the performance of both techniques\
    \ essentially matches that of the baseline), our technique performs better. Zooming\
    \ into particular mixes, the results reveal that RD performs especially better\
    \ than DASCA in those mixes made up of write-intensive applications. Thus, our\
    \ approach reports a performance improvement of more than 7% when considering\
    \ just the H*<sup>i</sup>* mixes while DASCA just reports 1.7% IT improvement\
    \ with respect to the baseline. Also, RD delivers significantly higher performance\
    \ than DASCA and the baseline for those mixes which contain any application with\
    \ high WPKI.\n\n# *5.2.3. Energy savings*\n\nFigure [15](#page-13-1) illustrates\
    \ the energy savings in the SLLC. As in the single-core scenario, the graph follows\
    \ a similar relative trend between our approach and DASCA to that observed in\
    \ the write reduction numbers (Figure [13\\)](#page-12-0), just slightly modulated\
    \ with the performance numbers since, as shown in Figure [16,](#page-13-2) the\
    \ dynamic contribution to the energy consumption in the SLLC is higher than the\
    \ static part (except in the mixes made up of applications with low WPKI only),\
    \ so that the ability to reduce the amount of writes to the SLLC (dynamic contribution)\
    \ impacts the total energy consumption more than the ability to improve performance,\
    \ which mainly affects the static contribution. Overall, our proposal reports\
    \ around 37% energy reduction in the STT-RAM SLLC compared to the baseline while\
    \ DASCA reduces it by around 31%. If we zoom into the write-intensive mixes, both\
    \ RD and DASCA are able to save around 45% and 39% of SLLC energy consumption,\
    \ respectively. If we break the SLLC energy numbers down into the static and dynamic\
    \ contributions, our results reveal that, overall, RD is able to reduce \u2013\
    considering all mixes\u2013 the static energy part by around 2.7% with respect\
    \ to the baseline (around 6% for the write-intensive mixes) while DASCA reduces\
    \ the static contribution by 1.2% (1.7% for the *high* mixes). In addition, our\
    \ approach reports dynamic energy savings of around 50% (51% for the *high* mixes)\
    \ while DASCA numbers are 42% (46% for the *high* mixes).\n\nAlso, we explore\
    \ the energy savings obtained in the DRAM main memory, where the leakage contribution\
    \ has far greater significance than in the STT-RAM SLLC, so that the trends obtained\
    \ essentially follow those of the IT graph, but inverted (higher performance translates\
    \ into lower DRAM energy consumption). Figure [17](#page-14-0) illustrates that\
    \ RD manages to additionally reduce the energy consumption of the main memory\
    \ by around 6.2% on average compared to the baseline (8.3% for the write-intensive\
    \ mixes), while DASCA barely reaches a 3.6% energy reduction (around 2% for the\
    \ *high* mixes), mainly due to the higher performance improvement that our proposal\
    \ exhibits.\n\n# *5.2.4. Discussion*\n\nFor the sake of clarity, we next explain\
    \ where the performance improvements of our technique come from. First, as Figure\
    \ [13](#page-12-0) illustrated earlier, the write reductions to the SLLC that\
    \ RD achieves are greater than those\n\n![](_page_13_Figure_1.jpeg)\n\n<span id=\"\
    page-13-0\"></span>FIGURE 14. Instruction throughput normalized to the baseline\
    \ in the 4-core CMP system.\n\n![](_page_13_Figure_3.jpeg)\n\n<span id=\"page-13-1\"\
    ></span>FIGURE 15. Energy consumption in the STT-RAM SLLC normalized to the baseline\
    \ in the 4-core CMP system.\n\n![](_page_13_Figure_5.jpeg)\n\n<span id=\"page-13-2\"\
    ></span>FIGURE 16. Breakdown of energy consumption in the SLLC into the static\
    \ and dynamic contributions for the baseline in the 4-core CMP system.\n\nof DASCA.\
    \ Second, and more importantly, as Figure [14](#page-13-0) reveals, the bypasses\
    \ dictated by RD translate into more performance than that of DASCA. As in the\
    \ single-core scenario, the rationale behind that is related with the hit rate\
    \ experimented in the SLLC with both schemes. Figure [18](#page-14-1) illustrates\
    \ the number of hits in the SLLC per kilo instruction that each mix experiments\
    \ normalized to the baseline.\n\nThe results reveal that in most of the mixes\
    \ evaluated the amount of hits in the SLLC is higher under our approach than using\
    \ DASCA. Again, this is especially evident for the case of the mixes including\
    \ write-intensive applications such as H1, H3 and HL2 where the number of hits\
    \ is 2.87x, 2.45x and 1.37x those of the baseline, respectively. This is the key\
    \ to explain our performance improvements: the efficient management of the SLLC\
    \ contents by exploiting the reuse locality. In addition, there are other factors\
    \ that also contribute to the throughput gain such as less write operations to\
    \ the SLLC, less main memory accesses, and increased row buffer hit rates. In\
    \ order to perform a deeper comparison between RD and DASCA, Table [7](#page-14-2)\
    \ recaps the average values of different metrics involved in the performance delivered\
    \ by RD and DASCA, normalized to those of the baseline. As shown, our scheme improves\
    \ DASCA and the baseline (especially in the data from writeintensive mixes) in\
    \ all the metrics considered.\n\nAs in the single-core scenario, next we zoom\
    \ into particular mixes that need further detail to get a better understanding.\
    \ First, in some mixes such as H0, HM3 or HML0 we can observe that the DASCA scheme\
    \ is able to reduce the amount of writes to the SLLC and also the energy consumption\
    \ in the STT-RAM more than our scheme does (Figures [13](#page-12-0) and [15\\\
    )](#page-13-1). Conversely, the RD manages to deliver more throughput than DASCA\
    \ (Figure [14\\)](#page-13-0). However, these performance improvements our approach\
    \ achieves are not enough to offset the higher energy savings in the SLLC that\
    \ the DASCA scheme reports for these mixes as a consequence of the lower number\
    \ of writes to the STT-\n\n![](_page_14_Figure_0.jpeg)\n\nFIGURE 17. Energy consumption\
    \ in the DRAM normalized to the baseline in the 4-core CMP system.\n\n<span id=\"\
    page-14-0\"></span>![](_page_14_Figure_2.jpeg)\n\n<span id=\"page-14-1\"></span>FIGURE\
    \ 18. Number of STT-RAM SLLC hits per kilo instruction normalized to the baseline\
    \ in the 4-core CMP system.\n\n# RAM.\n\nSecond, data for mix L2 reveal that the\
    \ RD is able to reduce the amount of writes to the SLLC much more than DASCA with\
    \ respect to the baseline (81% vs. 48%). However, this great difference translates\
    \ into just 22% of energy savings in RD vs. 13% of DASCA. As shown, the difference\
    \ between both policies has been significantly reduced due to the low contribution\
    \ of the dynamic energy to the total energy consumption in the SLLC that this\
    \ mix exhibits, as Figure [16](#page-13-2) illustrates.\n\n# *5.2.5. Sensitivity\
    \ to Reuse Detector size*\n\nThe RD size is a key design aspect of our proposal.\
    \ In order to evaluate its impact we show in Figure [19](#page-15-1) the amount\
    \ of writes to the SLLC, the Instruction Throughput, and the energy consumption\
    \ in both the SLLC and the main memory for different RD sizes per core, namely\
    \ 8K, 16K, 32K and 64K entries.\n\nAs shown, the major impact is observed on the\
    \ capability to reduce the number of writes in the SLLC, ranging from an average\
    \ reduction of 63% with respect to the baseline when an 8K-entry RD per core is\
    \ employed (60% for the write-intensive mixes) to a reduction of around 51% for\
    \ a 64K-entry RD per core (50% for the *high mixes*). Note that maybe these data\
    \ might appear contradictory at first sight. However, they are not: As the size\
    \ of RD increases, it also augments the probability that a block finds its tag\
    \ in the RD, so the probability of bypassing decreases, leading to minor reduction\
    \ of writes to the SLLC. We can also observe a moderate impact on the average\
    \ energy consumed in the SLLC, with values in the range 63-69% as the size of\
    \ RD gets increased: again, note that these numbers follow a similar trend to\
    \ that exhibited by the amount of writes. Finally, the impact over the performance\
    \ and the energy consumption of the main memory is much reduced, falling the average\
    \ IT variations into a small range of 1% (4% for the write-intensive mixes) and\
    \ the average DRAM energy variations into a range of 1.5% (5% for the write-intensive\
    \ mixes).\n\n| \u2773<br>\u2773\u2773\u2773\u2773<br>Metrics | SLLC      | SLLC\
    \      | Row buffer Read | DRAM      | DRAM      | Bank contention in |\n|----------------------|-----------|-----------|-----------------|-----------|-----------|--------------------|\n\
    | \u2773\u2773\u2773\u2773\u2773<br>Policies    | Misses    | Hits      | Hit\
    \ Rate        | reads     | Writes    | SLLC               |\n| DASCA (All/High)\
    \     | 1.01/1.05 | 1.04/1.10 | 1.03/1.00       | 1.01/1.05 | 1.04/1.06 | 0.45/0.16\
    \          |\n| RD (All/High)        | 0.94/0.95 | 1.20/1.83 | 1.05/1.01     \
    \  | 0.98/0.98 | 0.96/0.94 | 0.29/0.08          |\n\n<span id=\"page-14-2\"></span>TABLE\
    \ 7. Average values of different metrics normalized to the baseline in the 4-core\
    \ CMP system.\n\n# 16 RODRIGUEZ-RODRIGUEZ ET AL\n\n![](_page_15_Figure_2.jpeg)\n\
    \n<span id=\"page-15-1\"></span>FIGURE 19. Writes to SLLC, IT and energy consumption\
    \ in both SLLC and main memory normalized to the baseline for different RD sizes\
    \ per core in the 4-core CMP system.\n\n# *5.2.6. Overhead analysis*\n\nIn Section\
    \ [3.2.3](#page-5-1) we outlined that an 8K-entry RD for a 1MB LLC requires an\
    \ extra storage of 14KB, which represents a 1.37% overhead with respect to the\
    \ LLC size. In this section we previously noted that for the 4-CMP system under\
    \ evaluation (4MB SLLC) we employ an 8K-entry RD per core. The reason is that\
    \ we are maintaining for each evaluated system the 1.37% overhead with respect\
    \ the SLLC size. Therefore, in the 8-CMP evaluated later, we also employ an 8K-entry\
    \ RD per core. Hence, the total extra storage (overhead) of RD is 56KB and 112KB\
    \ for the 4- CMP and 8-CMP systems respectively, representing in all the cases\
    \ a 1.37% overhead with respect to the SLLC size.\n\n# *5.2.7. RD in a two-level\
    \ cache hierarchy*\n\nWe have evaluated the operation of our proposal in a threelevel\
    \ cache hierarchy since most current processors employ this configuration. Furthermore,\
    \ two private levels are more likely to filter better the temporal locality than\
    \ using just one private level. However, for a fair comparison, we have also evaluated\
    \ our proposal and the DASCA scheme in a configuration with just two cache levels.\
    \ Notably, we reproduce the same configuration (4-CMP) used by the authors in\
    \ [\\[1\\]](#page-20-0) when presenting the DASCA technique (32 KB IL1 and DL1\
    \ as private caches and a 1MB per core shared L2 cache). Table [8](#page-16-0)\
    \ illustrates the main results.\n\nAs shown, RD maintains higher capability than\
    \ DASCA (around 11% higher) in reducing the amount of writes to the SLLC. However,\
    \ as expected, the amount of writes avoided (and also the hits experienced in\
    \ the SLLC) is significantly lower than that exhibited in an scenario with 3 cache\
    \ levels. Recall that this is due to the fact that with two cache levels only,\
    \ most temporal locality has not been filtered, so that the reuse locality can\
    \ not be fully exploitable. Also, as a consequence of this lower capability in\
    \ cutting the write traffic to the SLLC, the energy savings achieved in the shared\
    \ L2 are significantly lower than those obtained with three cache levels, although\
    \ RD still reports better numbers than DASCA. Finally, RD again improves the Instruction\
    \ Throughput to a greater extent than DASCA, and consequently also delivers higher\
    \ energy savings in the main memory. Note that we have also evaluated 28 mixes\
    \ in this configuration following the same criteria explained earlier, but they\
    \ are not exactly the same as in the threelevel cache hierarchy experiments since\
    \ the WPKI values that the benchmarks exhibit do not match those of the threelevel\
    \ configuration and therefore some programs changed the category (high, medium\
    \ or low) in which they were classified.\n\n# <span id=\"page-15-0\"></span>5.3.\
    \ Evaluation in an 8-core CMP system\n\nIn this section we illustrate and analyze\
    \ the main results obtained when using RD and DASCA in an 8-core CMP system with\
    \ an 8MB SLLC. Like in the previous section, in this scenario we create 28 mixes\
    \ following the same criteria as in a 4-CMP system. The mixes evaluated are shown\
    \ in Table [5.](#page-8-0) Given that a detailed analysis of the 8-core system\
    \ would show similar results as the 4-core scenario, in this section we will not\
    \ zoom into details but will only describe the average results and main trends.\n\
    \n# *5.3.1. Write filtering*\n\nFigure [20](#page-16-1) illustrates the number\
    \ of writes to the STT-RAM SLLC generated with DASCA and with RD (assuming an\
    \ 8K-entry RD per core). Both schemes are normalized to a baseline STT-RAM without\
    \ any content selection mechanism.\n\nSimilarly to the results for the 4-core\
    \ scenario, the experimental results reveal that RD just performs 46% of the writes\
    \ made in the baseline scheme, whereas DASCA produces 56% of the writes that the\
    \ baseline did. For the write-intensive mixes, RD and DASCA reduce the amount\
    \ of writes compared to the baseline in 44% and 35% respectively.\n\n# *5.3.2.\
    \ Performance*\n\nAs we did in Section [5.2.2,](#page-12-1) we employ the *Instruction\
    \ Throughput* (IT) to evaluate the performance when executing multiprogrammed\
    \ workloads. Figure [21](#page-16-2) illustrates the IT that each evaluated policy\
    \ delivers normalized to the baseline.\n\nSimilarly to the results obtained for\
    \ a 4-core CMP system, RD outperforms DASCA in the 8-core scenario. Moreover,\
    \ in the 8-core scenario, higher performance improvements are achieved in both\
    \ schemes over the baseline. The\n\n| \u2773<br>\u2773\u2773\u2773\u2773<br>Metrics\
    \  | Writes    | Instr.     | Energy consumpt. | Energy consumpt. | SLLC     \
    \ |\n|-----------------------|-----------|------------|------------------|------------------|-----------|\n\
    | \u2773\u2773\u2773\u2773<br>Policies<br>\u2773 | SLLC      | Throughput | SLLC\
    \             | DRAM             | Hits      |\n| DASCA (All/High)      | 0.70/0.79\
    \ | 1.01/1.03  | 0.81/0.86        | 0.97/0.96        | 0.98/1.00 |\n| RD (All/High)\
    \         | 0.59/0.76 | 1.03/1.04  | 0.75/0.83        | 0.95/0.95        | 0.98/1.00\
    \ |\n\nTABLE 8. Average values of different metrics normalized to the baseline\
    \ in a 4-core CMP system with two cache levels.\n\n<span id=\"page-16-0\"></span>![](_page_16_Figure_3.jpeg)\n\
    \n<span id=\"page-16-1\"></span>FIGURE 20. Number of writes to the STT-RAM SLLC\
    \ normalized to the baseline in the 8-core CMP system.\n\nresults reveal that\
    \ RD improves performance by around 7% compared to the baseline, while DASCA improves\
    \ it by around 4%. As for write-intensive mixes, RD improves the baseline by 20%\
    \ and DASCA by 11%. As shown in Figure [21,](#page-16-2) RD significantly overcomes\
    \ DASCA and the baseline scheme in those mixes which contain any application with\
    \ high WPKI.\n\n# *5.3.3. Energy savings*\n\nFigure [22](#page-17-1) illustrates\
    \ the energy savings in the shared LLC. In general, the results in the 8-core\
    \ scenario follow the trend observed for the 4-core environment. Specifically,\
    \ RD reports around 32.5% energy reduction in the STT-RAM SLLC compared to the\
    \ baseline while DASCA reduces energy by around 27%. In the case of write-intensive\
    \ mixes, both RD and DASCA reduce the SLLC energy consumption by 34% and 27.5%,\
    \ respectively. Analyzing the static and dynamic contributions on the SLLC energy\
    \ consumption, overall, RD is able to reduce \u2013for all mixes\u2013 the static\
    \ energy part by around 6% with respect to the baseline (around 15% for the write-intensive\
    \ mixes) while DASCA reduces the static contribution by 3.6% (9.5% for the *high*\
    \ mixes). In addition, our approach reports dynamic energy savings of around 43%\
    \ (36% for the *high* mixes) while DASCA numbers are 36% (30% for the *high* mixes).\
    \ Note that mixes made up of applications with low WPKI exhibit the lowest energy\
    \ savings across the board. This is consistent with the modest write reduction\
    \ they report and especially with the high contribution of the static part to\
    \ the total SLLC energy consumption that they exhibit, as Figure [23](#page-17-2)\
    \ shows.\n\nFigure [24](#page-18-0) illustrates the energy savings obtained in\
    \ the DRAM main memory, where it is shown that RD reduces the energy consumption\
    \ of the main memory by around 6% on average compared to the baseline (3% for\
    \ the write-intensive mixes), while DASCA reaches a 2.8% energy reduction and\
    \ actually wastes more energy, around 6%, for the *high* mixes.\n\n![](_page_16_Figure_10.jpeg)\n\
    \n<span id=\"page-16-2\"></span>FIGURE 21. Instruction throughput normalized to\
    \ the baseline in the 8-core CMP system.\n\n# 18 RODRIGUEZ-RODRIGUEZ ET AL\n\n\
    ![](_page_17_Figure_2.jpeg)\n\n<span id=\"page-17-1\"></span>FIGURE 22. Energy\
    \ consumption in the STT-RAM SLLC normalized to the baseline in the 8-core CMP\
    \ system.\n\n![](_page_17_Figure_4.jpeg)\n\n<span id=\"page-17-2\"></span>FIGURE\
    \ 23. Breakdown of energy consumption in the SLLC into the static and dynamic\
    \ contributions for the baseline in the 8-core CMP system.\n\nThis energy waste\
    \ may look surprising, given that DASCA is able to reduce the number of writes\
    \ with respect to the baseline by 35% and to deliver a performance improvement\
    \ higher than 10%. However, this can be explained by the fact that DASCA suffers\
    \ a very significant increase in the amount of SLLC misses, which translates into\
    \ high values of DRAM accesses (as shown in Table [9\\)](#page-18-1).\n\n# *5.3.4.\
    \ Discussion*\n\nAs in the 4-core configuration, in this section we explain the\
    \ reasons for the higher performance improvement achieved in our technique (RD)\
    \ against DASCA in the 8-core scenario.\n\nAs we already reasoned in the previous\
    \ section, the better performance of RD is due to several factors, being the most\
    \ important one the high efficiency achieved from the reuse locality exploitation.\
    \ For demonstrating that fact, Figure [25](#page-18-2) shows the number of hits\
    \ in the SLLC per kilo instruction that each mix experiments normalized to the\
    \ baseline. As the figure shows, our approach achieves in most mixes a higher\
    \ or much higher number of hits than DASCA, which confirms that RD uses a more\
    \ efficient policy than DASCA.\n\nIn addition to the hit rate improvement, there\
    \ are other metrics that also justify achieving a better performance, such as\
    \ SLLC misses, DRAM reads and writes, row buffer read hit rate and bank contention\
    \ in the SLLC. All these metrics are shown in Table [9,](#page-18-1) for both\
    \ RD and DASCA and also for both all and write-intensive mixes. Note that the\
    \ RD beats DASCA in all the metrics considered.\n\n# *5.3.5. Sensitivity to Reuse\
    \ Detector size*\n\nGiven that the RD size is a determining factor in our proposal,\
    \ and as done in the 4-CMP system, in Figure [26](#page-18-3) we show the amount\
    \ of writes to the SLLC, the Instruction Throughput, and the energy consumption\
    \ in both the SLLC and the main memory for different RD sizes per core, namely\
    \ 8K, 16K, 32K and 64K entries.\n\nThe trends are very similar to those observed\
    \ in the 4 core scenario. Notably, a significant impact is observed on the capability\
    \ to reduce the number of writes in the SLLC, especially for the All mixes, whereas\
    \ a moderate (or even negligible in some cases) impact is seen on the average\
    \ energy consumed in that cache level or main memory and performance of the overall\
    \ system.\n\n# <span id=\"page-17-0\"></span>5.4. RD performance in multi-core\
    \ scenarios\n\nIn this section we briefly inspect the main hints about the performance\
    \ of RD when we augment the number of cores. So far, we have evaluated the RD\
    \ in systems with one, four and eight cores. In order to further explore the RD\
    \ behavior we have also evaluated our proposal in a system with 16 cores, employing\
    \ the mixes shown in Table [6.](#page-8-1)\n\nIn Table [10](#page-19-0) we recap\
    \ the main numbers derived from RD and DASCA evaluation across the different configurations\
    \ (for the sake of simplicity, we show just the numbers considering all applications\
    \ or mixes, not those corresponding to the write-intensive ones). Notably, we\
    \ illustrate the average LLC write reduction capability, energy savings in the\
    \ LLC, the performance delivered and the energy savings in the DRAM.\n\nAs shown,\
    \ the write reduction capability in percentage terms gets reduced with the number\
    \ of cores for both RD and DASCA. However, despite this decrease in the write\
    \ filtering numbers, the most important consequence derived from this aspect,\
    \ i.e. the net energy savings in the LLC, essentially remains in the range of\
    \ 30-37% for RD, slightly decreasing\n\n![](_page_18_Figure_0.jpeg)\n\nFIGURE\
    \ 24. Energy consumption in the DRAM normalized to the baseline in the 8-core\
    \ CMP system.\n\n<span id=\"page-18-0\"></span>![](_page_18_Figure_2.jpeg)\n\n\
    ![](_page_18_Figure_3.jpeg)\n\n<span id=\"page-18-2\"></span>![](_page_18_Figure_4.jpeg)\n\
    \n<span id=\"page-18-3\"></span>FIGURE 26. Writes to SLLC, IT and energy consumption\
    \ in both SLLC and main memory normalized to the baseline for different RD sizes\
    \ per core in the 8-core CMP system.\n\nwith the number of cores as a consequence\
    \ of the increment in the number of writes, but keeping the benefit with respect\
    \ to DASCA largely unchanged. Moreover, the performance improvement increases\
    \ as the number of cores augments for both RD and DASCA, reporting significant\
    \ numbers especially for the 8-core and 16-core system. Indeed, the\n\n| \u2773\
    <br>\u2773\u2773\u2773\u2773<br>Metrics | SLLC      | SLLC      | Row buffer Read\
    \ | DRAM      | DRAM      | Bank contention in |\n|----------------------|-----------|-----------|-----------------|-----------|-----------|--------------------|\n\
    | \u2773\u2773\u2773\u2773\u2773<br>Policies    | Misses    | Hits      | Hit\
    \ Rate        | reads     | Writes    | SLLC               |\n| DASCA (All/High)\
    \     | 1.08/1.30 | 0.92/0.84 | 1.00/0.99       | 1.08/1.30 | 1.09/1.21 | 0.40/0.13\
    \          |\n| RD (All/High)        | 0.98/1.00 | 1.05/1.01 | 1.02/1.04     \
    \  | 1.00/1.06 | 1.02/1.05 | 0.24/0.07          |\n\nREUSE DETECTOR: IMPROVING\
    \ THE MANAGEMENT OF STT-RAM SLLCS 19\n\n<span id=\"page-18-1\"></span>TABLE 9.\
    \ Average values of different metrics normalized to the baseline in the 8-core\
    \ CMP system.\n\n20 RODRIGUEZ-RODRIGUEZ ET AL\n\n| \u2775<br>\u2775\u2775\u2775\
    \u2775<br>Metrics<br>\u2775\u2775\u2775<br>Scenario | LLC Write<br>reduction (%)<br>(DASCA/RD)\
    \ | LLC Energy<br>Savings (%)<br>(DASCA/RD) | Performance<br>improvement (%)<br>(DASCA/RD)\
    \ | DRAM Energy<br>Savings (%)<br>(DASCA/RD) |\n|-----------------------------------------|------------------------------------------|-----------------------------------------|----------------------------------------------|------------------------------------------|\n\
    | Single core                             | 51.8 / 65.2                      \
    \        | 29.5 / 34.5                             | 0.3 / 1.9               \
    \                     | 0.2 / 2.0                                |\n| 4-core CMP\
    \                              | 51.6 / 62.7                              | 30.8\
    \ / 37.0                             | 1.2 / 2.9                             \
    \       | 3.6 / 6.2                                |\n| 8-core CMP           \
    \                   | 43.8 / 54.0                              | 27.3 / 32.5 \
    \                            | 3.7 / 6.7                                    |\
    \ 2.8 / 6.0                                |\n| 16-core CMP                  \
    \           | 38.6 / 46.7                              | 26.0 / 30.5         \
    \                    | 10.2 / 14.5                                  | 4.0 / 7.9\
    \                                |\n\n<span id=\"page-19-0\"></span>TABLE 10.\
    \ Average values of different metrics normalized to the baseline across different\
    \ configurations.\n\nperformance improvement that RD reports with respect to the\
    \ DASCA scheme also increases with the number of cores (which is especially significant\
    \ in the case of write-intensive mixes for the 16-CMP system, where RD is able\
    \ to increase the performance of the baseline by around 38%, whereas DASCA only\
    \ achieves 24% of improvement). This is due to the fact that the difference in\
    \ LLC misses reported by both approaches increases as the number of cores augments\
    \ (notably, these numbers are 1%, 6%, 10% and 13% for 1, 4, 8 and 16-core systems\
    \ respectively, always reporting RD the best value) and also the difference in\
    \ other factors as the number of hits in the LLC, the number of DRAM reads and\
    \ writes or the row buffer hit rate also increase, benefiting RD more with the\
    \ number of cores. Finally, and derived from the higher performance delivered,\
    \ the same trend is observed in the energy savings experienced in the main memory\
    \ (where it is worthy to note that, in the 16-core scenario, RD manages to reduce\
    \ the energy consumption of the baseline by more than 15% for the write-intensive\
    \ mixes, while DASCA hardly achieves 8%). Overall we can conclude that, as the\
    \ number of cores increases, RD is able to significantly increase the performance\
    \ delivered (and also to increase the energy savings achieved in the main memory)\
    \ at the expense of a slight decrease in the LLC energy savings obtained.\n\n\
    # 6. RELATED WORK\n\nTo address the problems of energy consumption and performance\
    \ of STT-RAM SLLCs, in the last years different researchers have proposed solutions\
    \ aiming to reduce either the amount of writes or the per-write energy.\n\nA great\
    \ body of work mainly tries to cut the write traffic to the STT-RAM: In [\\[14\\\
    ]](#page-21-9) the authors propose an obstructionaware cache management policy\
    \ called OAP. OAP monitors the cache to periodically detect LLC-obstruction processes,\
    \ and manage the cache accesses from different processes, so that when an LLC-obstruction\
    \ is detected the data is forwarded to the next cache level or Main Memory as\
    \ appropriate. In [\\[15\\]](#page-21-10) two techniques are proposed to reduce\
    \ the number of writes to a last level (L2) STT-RAM cache and also save energy.\
    \ The first one adds a small cache between L1 and L2 \u2013called write-cache\
    \ (WC)\u2013 which is mutually exclusive with L2 and stores only the dirty lines\
    \ evicted from L1. On a cache access, both L2 and WC are accessed in parallel.\
    \ The write misses are allocated in WC and the load misses are allocated in L2.\
    \ WC reduces the number of L2 writes by absorbing most of the L1 writebacks. Other\
    \ authors propose a coding scheme for STT-RAM last level cache based on the concept\
    \ of value locality. They reduce switching probability in cache by swapping common\
    \ patterns with limited weight codes to make writes less often as well as more\
    \ uniform [\\[16\\]](#page-21-11). Other techniques [\\[17\\]](#page-21-12) rely\
    \ on the observation that on average, a large fraction of bytes and words written\
    \ to the L2 cache are only zero-valued data. Based on this, this technique adds\
    \ additional \"all-zero-data\" flags in the tag arrays at the granularity of a\
    \ single byte and a single word. Before any cache write, the data value is checked.\
    \ If the all-zero bytes or words are detected, the corresponding flags are set\
    \ and only the non-zero bytes or words are written. During a cache read operation,\
    \ only the non-zero bytes or words are read and then the actual data are constructed\
    \ by combining the information from the allzero flags. Another proposal [\\[18\\\
    ]](#page-21-13) logically divides the STT-RAM cache line into multiple partial\
    \ lines. In L1 cache, a history bit is kept for each partial line to track which\
    \ partial lines have changed. Using this information, when a dirty L1 block is\
    \ written to last level cache, only those partial lines which have been changed\
    \ are written. Other authors propose techniques for mitigating the write pressure\
    \ caused due to prefetching in STT-RAM based LLC [\\[19\\]](#page-21-14). One\
    \ of these techniques prioritizes different types of LLC requests such as load,\
    \ store, prefetch, or write back, etc. based on their criticality. The critical\
    \ requests are assigned a high priority and hence, they are served earlier. In\
    \ multicore systems, the excessive requests generated from a cache-intensive program\
    \ may block those generated from a cache-unintensive program which may lead to\
    \ its starvation. To address this, they propose another technique which prioritizes\
    \ the requests from a cache-unintensive program, so that they are served promptly.\
    \ Also, authors in [\\[9\\]](#page-21-4) analyze the cache coherence protocols\
    \ impact in the number of write to a LLC based on STT-RAM, showing that the protocols\
    \ with a owned state (MOESI and MOSI) reduce the number of writes to LLC.\n\n\
    Another body of work mainly deals with performance of STT-RAM caches: In [\\[20\\\
    ]](#page-21-15) a cache revive technique to calculate retention time is proposed.\
    \ Some cache blocks retain data even after completion of retention time. The retention\
    \ time is chosen so that it will minimize the number of unrefreshed cache blocks.\
    \ Other authors propose the use of STT-RAM to design combinational logic, register\
    \ files and on-chip storage (I/D L1 caches, TLBs and L2 cache) [\\[21\\]](#page-21-16).\
    \ Also, to hide the write latency of STT-RAM, they propose subbank buffering which\
    \ allows the writes to complete locally within each sub-bank, while the reads\
    \ from\n\nother locations within the array can complete unobstructed. They show\
    \ that by carefully designing the pipeline, the STT-RAM based design can significantly\
    \ reduce the leakage power, while also maintaining the performance level close\
    \ to the CMOS design. Also, an STT-RAM cache design for lower level caches where\
    \ different cache ways are designed with different retention periods is proposed\
    \ in [\\[22\\]](#page-21-17). For example, in a 16-way cache, way 0 is designed\
    \ with a fast STT-RAM design with low retention period and the remaining 15 ways\
    \ are designed with a slow STT-RAM design which has higher retention period. Their\
    \ technique uses hardware to detect whether a block is read or write intensive.\
    \ The write intensive blocks are primarily allocated to way 0, while the read\
    \ intensive blocks are allocated to the other ways. Also, to avoid refreshing\
    \ dying blocks in way 0, their technique uses data migration to move such blocks\
    \ to banks with higher retention period. Finally, a write-buffer design to address\
    \ the long write latency of last level (L2) STT-RAM cache is proposed in [\\[23\\\
    ]](#page-21-18). The L2 may receive a request from both L1 and the write buffer.\
    \ Since read latency of STT-RAM is smaller than the write latency and also reads\
    \ are performance-critical, the buffer uses a readpreemptive management policy,\
    \ which ensures that a read request receives higher priority than a write request.\
    \ The authors also propose a hybrid SRAM and STT-RAM cache design which aims to\
    \ move the most write-intensive blocks to SRAM.\n\nIt is worth mentioning that\
    \ we compare our RD scheme just against DASCA due to three reasons: 1) DASCA,\
    \ which is a more recent proposal than all the aforementioned works, is the closest\
    \ work to ours in the sense that both schemes try to reduce the energy consumption\
    \ of an STT-RAM LLC by bypassing write operations predicted to be dead (or not\
    \ showing reuse), 2) some of the mentioned approaches are already evaluated in\
    \ the DASCA paper, such as [\\[14\\]](#page-21-9), being clearly outperformed\
    \ by the DASCA mechanism, and 3) some other schemes, such as [\\[16,](#page-21-11)\
    \ [17,](#page-21-12) [18,](#page-21-13) [19,](#page-21-14) [20,](#page-21-15)\
    \ [22,](#page-21-17) [23\\]](#page-21-18), although addressing the same problem,\
    \ are completely orthogonal to our approach. Thus, the RD could be built on top\
    \ of them, making a direct comparison meaningless. As an example, in [\\[16\\\
    ]](#page-21-11) the authors address the STT-RAM write energy problem at a circuit\
    \ level, trying to reduce the number of writes and also to balance the wear among\
    \ the cells. The proposal relies in identifying the most frequent values stored\
    \ in the LLC (value locality) and encoding these patterns to reduce the number\
    \ of writes. Thus, unlike our proposal, this approach operates at a bit-level.\n\
    \n# 7. CONCLUSIONS\n\nIn this paper we have addressed the main constraints of\
    \ conventional SRAM last-level caches: power-hungry operation and inefficient\
    \ management. In order to overcome these drawbacks we propose to employ a STT-RAM\
    \ SLLC where its contents are selected according to a *Reuse Detector* which exploits\
    \ the reuse locality of the stream of references arriving at the SLLC. The Reuse\
    \ Detector is a hardware component that tracks block reuse and determines,\n\n\
    according to its predicted future utility, if they must be inserted in the SLLC\
    \ or bypassed to the main memory. The Reuse Detector succeeds in managing the\
    \ STT-RAM SLLC contents in two complementary ways. First, it is able to bypass\
    \ to main memory a significant fraction of the blocks coming to the SLLC, thus\
    \ decreasing the amount of the energy-hungry writes to be performed. Second, it\
    \ increases significantly the SLLC hit rate, which leads to moderate performance\
    \ improvements. In addition, the energy consumption in the main memory is also\
    \ reduced. This way, our approach is able to outperform other strategies also\
    \ oriented to decrease the energy consumption in STT-RAM SLLCs, such as the DASCA\
    \ scheme. Although DASCA exhibits slightly lower ability to cut the write operations\
    \ to the SLLCs, this technique, which predicts if a block will not be reused again\
    \ instead of predicting if a block is going to be reused as ours, achieves lower\
    \ accuracy in the prediction, hence also significantly lower hit rates at this\
    \ cache level and therefore much lower performance improvements. Overall RD reports\
    \ on average energy reductions in the SLLC in the range of 37-30%, additional\
    \ energy savings in the main memory in the range of 6-8% and performance improvements\
    \ of 3% (quadcore), 7% (eight-core) and 14% (16-core) compared to an STT-RAM SLLC\
    \ baseline where no reuse detector is employed. More importantly, our approach\
    \ outperforms DASCA, the state-of-the-art STT-RAM SLLC management, reporting \u2013\
    depending on the specific scenario and the kind of applications used\u2013 SLLC\
    \ energy savings in the range of 4-11% higher than those of DASCA, delivering\
    \ higher performance in the range of 1.5-14%, and an additional improvement in\
    \ DRAM energy consumption in the range of 2-9% higher than DASCA.\n\n# ACKNOWLEDGEMENTS\n\
    \nThis work has been supported in part by the Spanish government through the research\
    \ contracts TIN2012-32180, TIN2015-65277-R, TIN2015-65316-P and by the HIPEAC-4\
    \ European Network of Excellence. It has been also supported by a grant scholarship\
    \ from the University of Costa Rica and the Costa Rican Ministry of Science and\
    \ Technology MICIT and CONICIT.\n\n# <span id=\"page-20-0\"></span>REFERENCES\n\
    \n- [1] Ahn, J., Yoo, S., and Choi, K. (2014) Dasca: Dead write prediction assisted\
    \ stt-ram cache architecture. *IEEE 20th International Symposium on High Performance\
    \ Computer Architecture (HPCA), 2014*, pp. 25\u201336. IEEE.\n- <span id=\"page-20-1\"\
    ></span>[2] Khan, S. M., Tian, Y., and Jimenez, D. A. (2010) Sampling dead block\
    \ prediction for last-level caches. *Proceedings of the 2010 43rd Annual IEEE/ACM\
    \ International Symposium on Microarchitecture*, Washington, DC, USA MICRO '43,\
    \ pp. 175\u2013186. IEEE Computer Society.\n- <span id=\"page-20-2\"></span>[3]\
    \ Albericio, J., Ib\xE1\xF1ez, P., Vi\xF1als, V., and Llaber\xEDa, J. M. (2013)\
    \ Exploiting reuse locality on inclusive shared last-level caches. *ACM Trans.\
    \ Archit. Code Optim.*, 9, 38:1\u201338:19.\n- <span id=\"page-20-3\"></span>[4]\
    \ Albericio, J., Ib\xE1\xF1ez, P., Vi\xF1als, V., and Llaber\xEDa, J. M. (2013)\
    \ The reuse cache: Downsizing the shared last-level cache.\n\n*Proceedings of\
    \ the 46th Annual IEEE/ACM International Symposium on Microarchitecture*, New\
    \ York, NY, USA MICRO-46, pp. 310\u2013321. ACM.\n\n- <span id=\"page-21-1\"></span><span\
    \ id=\"page-21-0\"></span>[5] (2013). http://www.hpl.hp.com/research/cacti/.\n\
    - [6] Dong, X. et al. (2012) NVSim: A circuit-level performance, energy, and area\
    \ model for emerging nonvolatile memory. *IEEE Transaction on Compter-Aided Design\
    \ of Integrated Circuits and Systems*, 31, 994\u20131007.\n- <span id=\"page-21-2\"\
    ></span>[7] D\xEDaz, J., Monreal, T., Vi\xF1als, V., Ib\xE1\xF1ez, P., and Llaber\xED\
    a, J. M. (2015) Selecci\xF3n de contenidos basada en reuso para caches compartidas\
    \ en exclusi\xF3n. *Proceedings of the XXVI Jornadas de Paralelismo* JP-2015,\
    \ pp. 433\u2013442.\n- <span id=\"page-21-4\"></span><span id=\"page-21-3\"></span>[8]\
    \ Binkert, N. et al. (2011) The gem5 simulator. *ACM SIGARCH Computer Architecture\
    \ News*, 39, 1.\n- [9] Chang, M.-T., Lu, S.-L., and Jacob, B. (2014) Impact of\
    \ cache coherence protocols on the power consumption of stt-rambased llc. *The\
    \ Memory Forum Workshop*.\n- <span id=\"page-21-5\"></span>[10] Agarwal, N., Krishna,\
    \ T., Peh, L.-S., and Jha, N. K. (2009) Garnet: A detailed on-chip network model\
    \ inside a fullsystem simulator. *Performance Analysis of Systems and Software,\
    \ 2009. ISPASS 2009. IEEE International Symposium on*, pp. 33\u201342. IEEE.\n\
    - <span id=\"page-21-6\"></span>[11] Rosenfeld, P., Cooper-Balis, E., and Jacob,\
    \ B. (2011) Dramsim2: A cycle accurate memory system simulator. *Computer Architecture\
    \ Letters*, 10, 16 \u201319.\n- <span id=\"page-21-8\"></span><span id=\"page-21-7\"\
    ></span>[12] (2013). http://www.spec.org/cpu2006/.\n- [13] Patil, H., Cohn, R.\
    \ S., Charney, M., Kapoor, R., Sun, A., and Karunanidhi, A. (2004) Pinpointing\
    \ representative portions of large intel\xAE itanium\xAE programs with dynamic\
    \ instrumentation. *MICRO*, pp. 81\u201392. IEEE Computer Society.\n- <span id=\"\
    page-21-9\"></span>[14] Wang, J., Dong, X., and Xie, Y. (2013) Oap: an obstructionaware\
    \ cache management policy for stt-ram last-level caches. *Proceedings of the Conference\
    \ on Design, Automation and Test in Europe*, pp. 847\u2013852. EDA Consortium.\n\
    - <span id=\"page-21-10\"></span>[15] Rasquinha, M. (2011) An energy efficient\
    \ cache design using spin torque transfer (STT) RAM. Master of science in the\
    \ school of electrical and computer engineering Georgia Institute of Technology.\n\
    - <span id=\"page-21-11\"></span>[16] Yazdanshenas, S., Ranjbar Pirbast, M., Fazeli,\
    \ M., and Patooghy, A. (2014) Coding last level stt-ram cache for high endurance\
    \ and low power. *IEEE Computer Architecture Letters*, 13, 73\u201376.\n- <span\
    \ id=\"page-21-12\"></span>[17] Jung, J., Nakata, Y., Yoshimoto, M., and Kawaguchi,\
    \ H. (2013) Energy-efficient spin-transfer torque ram cache exploiting additional\
    \ all-zero-data flags. *Quality Electronic Design (ISQED), 2013 14th International\
    \ Symposium on*, pp. 216\u2013222. IEEE.\n- <span id=\"page-21-13\"></span>[18]\
    \ Park, S. P., Gupta, S., Mojumder, N., Raghunathan, A., and Roy, K. (2012) Future\
    \ cache design using stt mrams for improved energy efficiency: devices, circuits\
    \ and architecture. *Proceedings of the 49th Annual Design Automation Conference*,\
    \ pp. 492\u2013497. ACM.\n- <span id=\"page-21-14\"></span>[19] Mao, M., Li, H.\
    \ H., Jones, A. K., and Chen, Y. (2013) Coordinating prefetching and stt-ram based\
    \ last-level cache management for multicore systems. *Proceedings of the 23rd\
    \ ACM international conference on Great lakes symposium on VLSI*, pp. 55\u2013\
    60. ACM.\n- <span id=\"page-21-15\"></span>[20] Jog, A., Mishra, A. K., Xu, C.,\
    \ Xie, Y., Narayanan, V., Iyer, R., and Das, C. R. (2012) Cache revive: architecting\
    \ volatile stt-ram caches for enhanced performance in cmps. *Proceedings of the\
    \ 49th Annual Design Automation Conference*, pp. 243\u2013252. ACM.\n- <span id=\"\
    page-21-16\"></span>[21] Guo, X., Ipek, E., and Soyata, T. (2010) Resistive computation:\
    \ avoiding the power wall with low-leakage, stt-mram based computing. *ACM SIGARCH\
    \ Computer Architecture News*, pp. 371\u2013382. ACM.\n- <span id=\"page-21-17\"\
    ></span>[22] Sun, Z., Bi, X., Li, H. H., Wong, W.-F., Ong, Z.-L., Zhu, X., and\
    \ Wu, W. (2011) Multi retention level stt-ram cache designs with a dynamic refresh\
    \ scheme. *Proceedings of the 44th Annual IEEE/ACM International Symposium on\
    \ Microarchitecture*, pp. 329\u2013338. ACM.\n- <span id=\"page-21-18\"></span>[23]\
    \ Sun, G., Dong, X., Xie, Y., Li, J., and Chen, Y. (2009) A novel architecture\
    \ of the 3d stacked mram l2 cache for cmps. *High Performance Computer Architecture,\
    \ 2009. HPCA 2009. IEEE 15th International Symposium on*, pp. 239\u2013249. IEEE.\n\
    \nThis figure \"CeldaSTTRAM.jpeg\" is available in \"jpeg\" format from:\n\nThis\
    \ figure \"CeldaSTTRAM.png\" is available in \"png\" format from:\n\nThis figure\
    \ \"STTRAMCircuit.jpeg\" is available in \"jpeg\" format from:\n\nThis figure\
    \ \"STTRAMCircuit.png\" is available in \"png\" format from:"
  paper_id: 26
  scores:
    originality:
      raw: 4.5
      reason: The paper presents a novel approach to optimizing the softmax operation
        in RRAM-based accelerators, which is a significant advancement in the field.
    technical_soundness:
      raw: 4.0
      reason: The proposed STAR architecture is well-structured and technically sound,
        although some implementation details could be further clarified.
    clarity:
      raw: 3.5
      reason: While the paper conveys its ideas effectively, certain sections are
        dense and could benefit from clearer explanations and more illustrative examples.
    significance:
      raw: 4.0
      reason: Addressing the softmax latency issue in attention models is crucial,
        making this work significant for future research and applications in deep
        learning.
    relevance:
      raw: 4.5
      reason: The topic is highly relevant to current trends in machine learning and
        hardware acceleration, particularly for attention-based models.
  total_score: 4.05
  tier: C
- title: "A RISC-V SOC for Terahertz IoT Devices: Implementation and design\n  challenges"
  abstract: "Memory is a critical design consideration in current data-intensive DNN\n\
    accelerators, as it profoundly determines energy consumption, bandwidth\nrequirements,\
    \ and area costs. As DNN structures become more complex, a larger\non-chip memory\
    \ capacity is required to reduce data movement overhead, but at\nthe expense of\
    \ silicon costs. Some previous works have proposed memory-oriented\noptimizations,\
    \ such as different data reuse and layer fusion schemes. However,\nthese methods\
    \ are not general and potent enough to cope with various graph\nstructures.\n\
    \  In this paper, we explore the intrinsic connection between network structures\n\
    and memory features to optimize both hardware and mapping. First, we introduce\n\
    a graph-level execution scheme with a corresponding dataflow and memory\nmanagement\
    \ method. This scheme enables the execution of arbitrary graph\npatterns with\
    \ high data reuse and low hardware overhead. Subsequently, we\npropose Cocco,\
    \ a hardware-mapping co-exploration framework leveraging\ngraph-level features\
    \ of networks. It aims to minimize communication overhead,\nsuch as energy consumption\
    \ and bandwidth requirements, with a smaller memory\ncapacity. We formulate the\
    \ graph-partition scheduling and memory configuration\nsearch as an optimization\
    \ problem and employ a genetic-based method to achieve\nefficient co-exploration\
    \ for large and irregular networks. Experiments\ndemonstrate that Cocco obtains\
    \ lower external memory access, lower bandwidth\nrequirements, and more stable\
    \ optimization for graph partition compared to the\ngreedy algorithm and dynamic\
    \ programming introduced in prior works. Cocco also\nreduces the costs by 1.89%\
    \ to 50.33% using co-exploration compared to other\ntypical methods."
  url: http://arxiv.org/abs/2402.00629v1
  keywords: ''
  document: "# Cocco: Hardware-Mapping Co-Exploration towards Memory Capacity-Communication\
    \ Optimization\n\nZhanhong Tan tanzh19@mails.tsinghua.edu.cn IIIS, Tsinghua University\
    \ Beijing, China\n\nZijian Zhu zhuzj23@mails.tsinghua.edu.cn IIIS, Tsinghua University\
    \ Beijing, China\n\nKaisheng Ma<sup>\u2217</sup> kaisheng@mail.tsinghua.edu.cn\
    \ IIIS, Tsinghua University Beijing, China\n\n# Abstract\n\nMemory is a critical\
    \ design consideration in current dataintensive DNN accelerators, as it profoundly\
    \ determines energy consumption, bandwidth requirements, and area costs. As DNN\
    \ structures become more complex, a larger on-chip memory capacity is required\
    \ to reduce data movement overhead, but at the expense of silicon costs. Some\
    \ previous works have proposed memory-oriented optimizations, such as different\
    \ data reuse and layer fusion schemes. However, these methods are not general\
    \ and potent enough to cope with various graph structures.\n\nIn this paper, we\
    \ explore the intrinsic connection between network structures and memory features\
    \ to optimize both hardware and mapping. First, we introduce a graph-level execution\
    \ scheme with a corresponding dataflow and memory management method. This scheme\
    \ enables the execution of arbitrary graph patterns with high data reuse and low\
    \ hardware overhead. Subsequently, we propose Cocco, a hardware-mapping co-exploration\
    \ framework leveraging graph-level features of networks. It aims to minimize communication\
    \ overhead, such as energy consumption and bandwidth requirements, with a smaller\
    \ memory capacity. We formulate the graph-partition scheduling and memory configuration\
    \ search as an optimization problem and employ a genetic-based method to achieve\
    \ efficient co-exploration for large and irregular networks. Experiments demonstrate\
    \ that Cocco obtains lower external memory access, lower bandwidth requirements,\
    \ and more stable optimization for graph partition compared to the greedy algorithm\
    \ and dynamic programming introduced in prior works. Cocco also reduces the costs\
    \ by 1.89% to 50.33% using co-exploration compared to other typical methods.\n\
    \nASPLOS '24, April 27-May 1, 2024, La Jolla, CA, USA \xA9 2024 Copyright held\
    \ by the owner/author(s). ACM ISBN 979-8-4007-0372-0/24/04. <https://doi.org/10.1145/3617232.3624865>\n\
    \nCCS Concepts: \u2022 Hardware \u2192 Design reuse and communication-based design;\
    \ On-chip resource management; \u2022 Computer systems organization \u2192 Parallel\
    \ architectures; \u2022 Software and its engineering \u2192 Compilers.\n\nKeywords:\
    \ Design space exploration, Memory, Graph analysis, Subgraph, Genetic algorithm,\
    \ Deep learning accelerator\n\n#### ACM Reference Format:\n\nZhanhong Tan, Zijian\
    \ Zhu, and Kaisheng Ma. 2024. Cocco: Hardware-Mapping Co-Exploration towards Memory\
    \ Capacity-Communication Optimization. In 29th ACM International Conference on\
    \ Architectural Support for Programming Languages and Operating Systems, Volume\
    \ 1 (ASPLOS '24), April 27-May 1, 2024, La Jolla, CA, USA. ACM, New York, NY,\
    \ USA, [16](#page-15-0) pages. <https://doi.org/10.1145/3617232.3624865>\n\n##\
    \ <span id=\"page-0-0\"></span>1 Introduction\n\nThe evolution of neural network\
    \ topology has driven the remarkable progress of artificial intelligence from\
    \ the early single-layer perceptron (SLP) [\\[45,](#page-14-0) [54\\]](#page-14-1)\
    \ and multi-layer perceptron (MLP) [\\[17,](#page-13-0) [22,](#page-13-1) [39\\\
    ]](#page-13-2) to modern DNNs with plain [\\[36,](#page-13-3) [57\\]](#page-14-2)/inception\
    \ [\\[59\\]](#page-14-3)/residual [\\[20,](#page-13-4) [55\\]](#page-14-4) structures\
    \ based on manual design, and even irregular structures using neural architecture\
    \ search (NAS) [\\[53,](#page-14-5) [75\\]](#page-15-1) or random network generation\
    \ [\\[68\\]](#page-14-6). These technological innovations have resulted in increasingly\
    \ complex computation graphs, which pose challenges for efficient memory design\
    \ and deployment.\n\nMemory design is crucial in the accelerator system, as it\
    \ performs data preparation at the start of each processing stage according to\
    \ the scheduling scheme, determining energy consumption, bandwidth requirements,\
    \ and area costs. Figure [1](#page-1-0) shows the trade-off between the on-chip\
    \ memory size and the external memory access in DNN accelerators. A smaller on-chip\
    \ buffer (left side) saves area but requires more data reloading. A larger buffer\
    \ (right side) can reduce external memory access and save energy and bandwidth\
    \ but at the cost of increasing the memory overhead. An excessively large SRAM\
    \ may not be feasible due to the high silicon area cost, typically ranging from\
    \ 1 to 2 mm<sup>2</sup> /MB in 12nm, and the high energy overhead, dozens of times\
    \ that of a MAC operation for a large SRAM.\n\nTherefore, the key problem is:\
    \ between the two extremes in Figure [1,](#page-1-0) how to find an appropriate\
    \ memory configuration with efficient workload mapping and data management, especially\
    \ under the growing complexity of neural network architectures.\n\n<sup>\u2217\
    </sup>Corresponding author.\n\nPermission to make digital or hard copies of part\
    \ or all of this work for personal or classroom use is granted without fee provided\
    \ that copies are not made or distributed for profit or commercial advantage and\
    \ that copies bear this notice and the full citation on the first page. Copyrights\
    \ for thirdparty components of this work must be honored. For all other uses,\
    \ contact the owner/author(s).\n\nASPLOS '24, April 27-May 1, 2024, La Jolla,\
    \ CA, USA Zhanhong Tan, Zijian Zhu, and Kaisheng Ma\n\n<span id=\"page-1-0\"></span>![](_page_1_Figure_1.jpeg)\n\
    \nFigure 1. The effect of different memory capacities for a computation graph.\
    \ Intermediate results can be buffered in the on-chip memory if it is large enough.\
    \ The on-chip memory of small capacity can only buffer two nodes (marked in the\
    \ red dotted box), and the larger memory can cover a larger subgraph (right side).\n\
    \nThe critical status of memory design has attracted extensive research. Most\
    \ previous studies focus on simple layer-level optimization (the left one of Figure\
    \ [1\\)](#page-1-0) by applying loop transformation techniques such as tiling\
    \ and reordering to fit the memory size and reuse the on-chip data [\\[23,](#page-13-5)\
    \ [43,](#page-14-7) [44,](#page-14-8) [61,](#page-14-9) [70\\]](#page-15-2). In\
    \ addition, several works also guide the memory capacity and hierarchy design\
    \ using designspace exploration [\\[12,](#page-12-0) [32,](#page-13-6) [37,](#page-13-7)\
    \ [66,](#page-14-10) [67\\]](#page-14-11). However, these layerlevel optimizations\
    \ are confined to the limited intra-layer reuse, which is insufficient for memory-intensive\
    \ networks. A subgraph-level scheme (e.g., the middle one and the right one of\
    \ Figure [1\\)](#page-1-0) provides a larger optimization space via inter-layer\
    \ reuse [\\[3,](#page-12-1) [4,](#page-12-2) [38,](#page-13-8) [73\\]](#page-15-3)\
    \ to reduce the I/O overhead. Therefore, this paper aims to leverage the subgraph-level\
    \ computing flow to optimize the memory capacity and external communication for\
    \ networks with any topology.\n\nHowever, there are three primary challenges to\
    \ fully exploit the subgraph-level optimization.\n\nFirst, we need a general execution\
    \ flow for any sub-graph. Due to the various kernel sizes and strides, a parent\
    \ node in a subgraph may have unbalanced data requirements from its consumers,\
    \ which makes it difficult to determine the tensor tiling scheme and the memory\
    \ allocation for each node (layer). In the traditional single-layer execution,\
    \ we usually divide a large tensor into loop tiles, which are processed through\
    \ a series of regular computing steps. Similarly, we want the sub-graph execution\
    \ to be a series of elementary computing steps with a simple control flow.\n\n\
    Second, we require a suitable memory management method for the subgraph execution.\
    \ Due to complicated dependency among nodes in a subgraph, careful management\
    \ is needed to reuse overlapping and inter-layer intermediate data.\n\nSolving\
    \ these two challenges contributes to a basic hardware execution model compatible\
    \ with subgraph-level optimization. However, we also encounter the third challenge:\
    \ how to partition a model into subgraphs and how much memory to allocate. The\
    \ optimization space is huge, so we need to devise a search method with high sampling\
    \ efficiency to find a proper subgraph partition and memory configuration result.\n\
    \nIn this paper, we first introduce a complete graph-level scheme for memory.\
    \ In particular, it contains a consumptioncentric flow that enables the execution\
    \ of arbitrary subgraphs with low memory footprints (for challenge 1). Accordingly,\
    \ we provide an explicit memory dataflow and the corresponding memory management\
    \ scheme for effective data reuse (for challenge 2). Building on the graph-level\
    \ memory scheme, we propose Cocco, a hardware-mapping co-exploration framework,\
    \ to establish a connection between model features and the memory configuration\
    \ (for challenge 3).\n\nCocco aims to find a combination of on-chip buffers and\
    \ the corresponding graph-level scheduling for lower memory and communication\
    \ overhead. In particular, we develop a genetic-based algorithm to efficiently\
    \ explore the search space of graph partitions and the associated memory configuration\
    \ for a series of neural networks.\n\nIn summary, this work makes the following\
    \ contributions:\n\n- Subgraph execution scheme. We first introduce a consumption-centric\
    \ flow to determine a low-cost execution sequence by throttling and aligning the\
    \ dataflow.\n- Efficient dataflow and memory management for subgraph data reuse.\
    \ We propose a memory management scheme featuring multiple reconfigurable regions\
    \ and the corresponding dataflow to support arbitrary subgraph execution with\
    \ full data reuse.\n- Hardware-mapping co-exploration framework. Based on the\
    \ subgraph execution scheme and memory dataflow, we propose Cocco, a genetic-based\
    \ framework combining the graph-level partition and memory design-space exploration\
    \ together. Cocco achieves 1.89% to 50.33% lower costs (lower communication with\
    \ a smaller size) using co-exploration in contrast to other methods.\n\n## 2 Background\
    \ and Motivation\n\n#### 2.1 Design of Neural Network Accelerators\n\nThe DNN\
    \ accelerator unit is the most basic execution unit in a computing system, on\
    \ top of which, we can scale it out to many-core, many-socket, and many-drawer\
    \ systems [\\[24,](#page-13-9) [40,](#page-13-10) [48,](#page-14-12) [60\\]](#page-14-13).\
    \ An accelerator unit usually employs a processing element (PE) array on a sophisticated\
    \ interconnection network to enable efficient tensor-level computation. Each PE\
    \ typically contains local scratchpads and ALUs to process basic data packets.\
    \ The global buffer and the weight buffer store activations and weights, and they\
    \ are generally\n\n<span id=\"page-2-0\"></span>![](_page_2_Figure_1.jpeg)\n\n\
    *\\* Those designs only support INT8 precision for tensor, we scale to FP16 performance\
    \ by a factor of 0.5. \\*\\* For most designs fabricated under 12nm (or close\
    \ to) process, we align all areas to 12nm. The SRAM area is estimated as 1.2mm<sup>2</sup>/MB.*\n\
    \nFigure 2. Left: performance v.s. memory capacity of several industrial NPUs.\
    \ Right: a summary of SRAM area ratio in these accelerators.\n\nlocated next to\
    \ the PE array to serve as the data interface and manage data between the PE array\
    \ and the external memory (e.g., DRAM or other cores). Due to the limited capacity\
    \ of the global buffer, the compiler has to partition the network execution into\
    \ a series of elementary workloads that are scheduled along the parallel spatial\
    \ resources and the temporal dimension [\\[18,](#page-13-11) [61,](#page-14-9)\
    \ [72\\]](#page-15-4). The capacity of the global buffer usually dominates the\
    \ external memory access and bandwidth requirements, significantly impacting system\
    \ performance. If the global memory is larger, it is more likely to buffer more\
    \ intermediate data and avoid data being evicted to DRAM. As shown in Figure [1,](#page-1-0)\
    \ a larger buffer expands the scope of elementary workloads from a single layer\
    \ to a larger subgraph, reducing the communication overhead.\n\nHowever, choosing\
    \ an appropriate memory specification is always a challenge. In Figure [2,](#page-2-0)\
    \ we surveyed 16 popular industrial neural network processors with various memory/performance/area\
    \ characteristics, where nine of them target the training domain [\\[6,](#page-12-3)\
    \ [11,](#page-12-4) [24,](#page-13-9) [34,](#page-13-12) [35,](#page-13-13) [40,](#page-13-10)\
    \ [41,](#page-13-14) [48,](#page-14-12) [60,](#page-14-13) [63,](#page-14-14)\
    \ [69\\]](#page-14-15) and seven target model inference [\\[1,](#page-12-5) [7,](#page-12-6)\
    \ [8,](#page-12-7) [26\u2013](#page-13-15)[28,](#page-13-16) [49,](#page-14-16)\
    \ [65\\]](#page-14-17). According to the survey, we can observe several trends\
    \ as follows:\n\n- 1. Memory occupies a significant portion of the silicon footprint\
    \ on an NPU chip, ranging from 4% to 79% of the area, with capacities from 2.5MB\
    \ to 896MB.\n- 2. Figure [2](#page-2-0) Left shows a trend of diminishing marginal\
    \ benefit of memory capacity. This is because there is a critical capacity to\
    \ meet the data reuse and bandwidth requirement at the beginning, and the increments\
    \ become negligible with higher memory capacity.\n- 3. We can infer that there\
    \ is a saturated capacity equivalent to the ideal unlimited memory, especially\
    \ for the inference design. For example, Hanguang [\\[26\\]](#page-13-15) is a\
    \ special SRAM-only inference system without DDR, and the 394MB buffers are large\
    \ enough to hold the intermediate data in their scenarios.\n\n<span id=\"page-2-1\"\
    ></span>![](_page_2_Figure_10.jpeg)\n\nFigure 3. Evaluations on subgraphs fusing\
    \ different number of layers (denoted as L=1,3,5). Y-axis is in the log domain.\
    \ The 2TOPS NPU accelerator is configured with a 1MB global buffer and a 1.125MB\
    \ weight buffer. The bandwidth requirement of weights is from the prefetch of\
    \ the next subgraph, while that of activations is from the inputs and outputs\
    \ of each subgraph.\n\nThis survey implies a design trade-off between memory capacity\
    \ and performance based on workloads and commercial considerations. Motivated\
    \ by the observations above, this paper aims to provide several memory design\
    \ considerations and study the connection between workload features and memory\
    \ capacity in an NPU accelerator.\n\n#### 2.2 Workload Deployment\n\nA neural\
    \ network is usually executed in a DNN accelerator with layer or graph granularities\
    \ based on the buffer capacity and dataflow.\n\n2.2.1 Layer-level Assignment.\
    \ This manner assigns tasks layer by layer. Most previous studies employ a tiling-based\
    \ layer-wise execution manner [\\[10,](#page-12-8) [21,](#page-13-17) [30,](#page-13-18)\
    \ [37,](#page-13-7) [50,](#page-14-18) [61\\]](#page-14-9), which elaborates the\
    \ tiling sizes of tensors to fit in the accelerator buffers and maintain performance.\
    \ A proper tiling scheme should overlap the data loading latency with the computing\
    \ time of each tile and try to reduce the repeated access of local weight buffers.\
    \ Tiles of data are transferred between the external memory and the global buffer,\
    \ and PEs subsequently fetch data from the global to their local buffers. Given\
    \ the larger bit-width of partial sums (e.g., 24bit partial sums v.s. 8bit inputs\
    \ in Simba), the output-centric tiling scheme is more commonly used to calculate\
    \ the final results before writing back to the global buffer [\\[61\\]](#page-14-9).\n\
    \n2.2.2 Graph-level Assignment. Unlike the layer-level assignment that restrains\
    \ from leveraging inter-layer reuse, a graph-level assignment processes several\
    \ layers of a neural network as a whole. To demonstrate the effectiveness of the\
    \ layer-level assignment, we evaluate four networks on a 2TOPS accelerator model,\
    \ as shown in Figure [3.](#page-2-1) The results show that fusing layers into\
    \ subgraphs significantly reduces external memory access by 42.3% \u223C 74.7%\
    \ and average bandwidth requirements by 26.8% \u223C 67.8%. However, the improvements\
    \ of larger subgraphs are marginal, indicating that there is an optimal trade-off\
    \ between inter-layer\n\nreuse and subgraph size, which determines the memory\
    \ requirement. For example, executing three-layer subgraphs reduces external memory\
    \ access by 53.7% in ResNet50, while executing five-layer subgraphs only further\
    \ reduces it by 13.6%.\n\nSeveral works have studied inter-layer reuse and graph\
    \ partition. However, they have several limitations in terms of performance and\
    \ flexibility. LCP [\\[42\\]](#page-14-19) groups similar layers into a cluster\
    \ and executes them as a whole, which makes it challenging to generalize into\
    \ an arbitrary graph. Fused-CNN [\\[4\\]](#page-12-2) and SR-CNN [\\[38\\]](#page-13-8)\
    \ fuse large contiguous layers for plain networks using manually-designed strategies.\
    \ Irregular-NN [\\[73\\]](#page-15-3) attempts to execute a complex subgraph using\
    \ a DP-based algorithm, but the constrained search space limits the exploration.\n\
    \nTo overcome these challenges, we propose an end-to-end framework that automatically\
    \ optimizes the graph partition and memory configuration for any neural network.\
    \ Our framework consists of two main components: a graph-level dataflow and a\
    \ hardware-mapping co-exploration algorithm. We first introduce the graph-level\
    \ dataflow and its hardware implementation. Then, we present Cocco, an efficient\
    \ algorithm that explores the trade-offs among memory configurations and graph\
    \ partition schemes based on workload features.\n\n## <span id=\"page-3-1\"></span>3\
    \ The Proposed Graph-Level Scheme\n\nTo execute layers on an NPU core in a graph-level\
    \ manner, we need an effective approach to reuse intermediate data and decide\
    \ the memory allocation. This section presents our comprehensive scheme for subgraph\
    \ execution, which addresses the first two challenges mentioned in Section [1.](#page-0-0)\
    \ First, we describe a multi-layer execution flow that minimizes the memory footprint\
    \ by a friendly tiling approach (for challenge 1). Second, we explain how to implement\
    \ this flow on a real NPU using an efficient data reuse pattern (for challenge\
    \ 2). The consistent target is to reduce the memory footprint and be friendly\
    \ to implementation.\n\n#### 3.1 Subgraph execution scheme\n\nIt is common practice\
    \ for the layer-level scheduling to partition the output tensor into several tiles\
    \ as layer-level elementary operations [\\[56,](#page-14-20) [61,](#page-14-9)\
    \ [72,](#page-15-4) [74\\]](#page-15-5), simplifying the scheduling and instruction\
    \ generation. Likewise, our high-level idea is also to generate a series of explicit\
    \ subgraph-level elementary operations. However, we need to address the challenges\
    \ of various kernel sizes and strides in different paths to prevent unbalanced\
    \ data production and unnecessary memory.\n\nA model's subgraph consists of multiple\
    \ layers (nodes) with dependencies. Section [4](#page-5-0) provides detailed information\
    \ on subgraph partition. In Figure [4\\(](#page-3-0)a), we present a straightforward\
    \ production-centric scheme for executing a subgraph\n\nwith different kernel\
    \ sizes in two branches, deriving tile sizes of the subsequent layers based on\
    \ the predetermined input tile sizes. For example, we can produce a 1 \xD7 1 tile\
    \ of Node(0) and a 2 \xD7 2 tile of Node(2) with a given 5 \xD7 5 feature map\
    \ of input Node(-1). In this case, these intermediate results only reduce to 1\
    \ \xD7 1 in Node(3), limited by the smallest input of Node(0), so the remaining\
    \ results of Node(2) can not be consumed immediately. As shown in Figure [4,](#page-3-0)\
    \ three extra data of Node(2) along with sixteen extra source data of Node(1)\
    \ take up extra memory space. There are more redundant cached data when the subgraph\
    \ becomes larger and more complicated. Disadvantages of this manner are attributed\
    \ to the production-centric idea that consumes all related activations from the\
    \ producers at once.\n\nTo avoid the memory overhead of storing unused data, we\
    \ propose a consumption-centric scheme in Figure [4\\(](#page-3-0)b), where results\
    \ of each node are produced on demand based on consumer(s) (i.e., output node(s)).\
    \ For example, given a 1 \xD7 1 tile of Node(3), we derive the 1 \xD7 1 tile size\
    \ for Node(2), which subsequently decides a 3 \xD7 3 tile for Node(1).\n\nThe\
    \ backward-derivation for each producer node is nontrivial because of diverse\
    \ kernel sizes and strides in different paths. Therefore, we propose a three-stage\
    \ flow to determine the behavior of each node, as illustrated in Figure [5.](#page-4-0)\
    \ The highlevel idea is to let output nodes drive the whole execution and match\
    \ the data consumption and production in each subgraph-level elementary operation.\n\
    \nThe stage-1 is similar to the traditional single-layer scheduling, where the\
    \ tile size is optimized for higher computation utilization. In order to hold\
    \ a larger subgraph, the tile size\n\n<span id=\"page-3-0\"></span>![](_page_3_Figure_14.jpeg)\n\
    \nFigure 4. A conceptual comparison between two manners to process a subgraph.\
    \ The node marked with a negative number represents the input node. The corresponding\
    \ subgraph is shown in the upper right, where \xD7 / refers to the convolution\
    \ kernel size ( ) and stride ().\n\nCocco: Hardware-Mapping Co-Exploration towards\
    \ Memory ... ASPLOS '24, April 27-May 1, 2024, La Jolla, CA, USA\n\n<span id=\"\
    page-4-0\"></span>![](_page_4_Figure_1.jpeg)\n\nFigure 5. The flow to determine\
    \ the execution scheme of a subgraph (i.e., the computed tile size of each node,\
    \ the tile offset, and the processing sequence of nodes). For simplicity, we discuss\
    \ the 1D-CONV in this example and it is similar in the 2D-CONV case.\n\ntends\
    \ to be smaller. In the 1D-CONV example, we set the tile size to be 2 for output\
    \ nodes.\n\nThe stage-2 aims to determine the data update offset \u0394 and the\
    \ memory allocation size for each node based on the consumer(s), processing in\
    \ the reverse topological order. We use the least common multiply (LCM) operation\
    \ to determine \u0394 () of producers for aligning different input offset requirements\
    \ (\u0394 () () ) from consumers. Hence, one producer update may correspond to\
    \ multiple updates of a consumer. For example, \u0394 (\u22122) = lcm{\u0394 (0)\
    \ (0) , \u0394 (1) (1) } = 4 = 2\u0394 (1) (1) , one update of Node(-2) corresponds\
    \ to two updates of Node(1). As for the tile size deduction, (\u0394 () / () )\
    \ is to derive the required input tile size (,) for output node [1](#page-4-1)\
    \ , where \u0394 () / () is the consumer offset (updated data) per producer update.\
    \ The maximum result (,) of all outputs is the tile size () of input node . In\
    \ this example, (\u22122) = max{<sup>0</sup> (2), <sup>1</sup> (4)} = 6 and (\u2212\
    1) = max{<sup>1</sup> (2), <sup>2</sup> (2)} = 4.\n\nAs mentioned above, since\
    \ we use LCM to align production and consumption, one producer update may correspond\
    \ to multiple updates of a consumer. In the stage-3, we use \\_ to represent the\
    \ number of memory update per subgraph elementary operation. The generated result\
    \ of the example in Figure [5](#page-4-0) is shown in Figure [6.](#page-4-2) \\\
    _ of Node(-1), Node(1), and Node(2) are two, where the second updates are highlighted\
    \ in red boxes. Note that the {\\_(\u22122) , . . . , \\_(2) } solution is not\
    \ unique, but the unique co-prime one {1, 2, 1, 2, 2} corresponds to the minimal\
    \ elementary operation.\n\n<span id=\"page-4-2\"></span>![](_page_4_Figure_8.jpeg)\n\
    \nFigure 6. The memory snapshot during two subgraph elementary operations based\
    \ on the execution scheme of Figure [5](#page-4-0) example. The allocated memory\
    \ size and update offset correspond to and \u0394, respectively (the [:] notation\
    \ denotes data ranging from index to ). The arrows denote the data dependency\
    \ according to the node relation in the subgraph.\n\nThe proposed flow is based\
    \ on a general directed acyclic computation graph and is not limited to specific\
    \ layer features. In this way, we can determine the execution scheme for any complex\
    \ irregular network like NasNet [\\[75\\]](#page-15-1) and RandWire [\\[68\\]](#page-14-6).\n\
    \n#### 3.2 Memory Management for the subgraph execution\n\nUp to now, we have\
    \ inferred the execution scheme for subgraphs, and the remaining challenge is\
    \ how to implement it on hardware efficiently. Figure [7](#page-5-1) shows the\
    \ memory allocation and update scheme for the subgraph execution. Before computing\
    \ a subgraph, the compiler determines logical blocks for input, intermediate,\
    \ and output nodes, where the block sizes depend on the tile sizes derived from\
    \ the execution flow.\n\nFor convenient management, we introduce two types of\
    \ memory regions: MAIN and SIDE. The MAIN region stores the source data for PE\
    \ (i.e., the tile of <sup>0</sup> \xD7 <sup>0</sup> \xD7 in Figure [7\\)](#page-5-1).\
    \ The SIDE region reserves the horizontally overlapping data[2](#page-4-3) . Considering\
    \ no reuse requirement for some output nodes, we only need a MAIN region to buffer\
    \ the results of the current tile. Except for the input nodes (negative numbers)\
    \ loading data from DRAM, the other nodes update data locally based on the computed\
    \ results of the input node(s).\n\nIn detail, the update scheme leverages the\
    \ collaboration between the MAIN region and the SIDE region to achieve full reuse\
    \ across sliding tiles (we consider kernel size > stride). As shown in Figure\
    \ [7,](#page-5-1) when the convolution windows slide across the feature maps,\
    \ the vertical overlap data (e.g., column = 5) are reused locally in the MAIN\
    \ region. In contrast, the horizontally overlapping data (e.g., the first row\
    \ of = 6 \u223C 8) are loaded from the SIDE region (path \u2460). Only a subset\
    \ of data is replaced by the newly calculated results\n\n<span id=\"page-4-1\"\
    ></span><sup>1</sup>For example, assume node is a convolution layer with kernel\
    \ size () and stride () , then ( ) = () + ( \u2212 1) \xD7 () .\n\n<span id=\"\
    page-4-3\"></span><sup>2</sup>We assume the column is the inner loop while the\
    \ row is the outer loop.\n\n(marked in green). Besides, the bottom horizontal\
    \ slices write new data to the SIDE region for the next row loop (path \u2461\
    ).\n\nThe extra hardware overhead for the proposed memory scheme is slight. Figure\
    \ [8](#page-5-2) presents our 12nm NPU core for the subgraph processing, with\
    \ a buffer region manager to logically partition the global buffer to support\
    \ contiguous layer processing. The buffer region manager is a 2-depth register\
    \ file, where determines the maximum subgraph size, and each entry pair indicates\
    \ the start and the end address for each region. The area overhead is quite small,\
    \ and in our test chip, the area ratio is only 0.18% with = 64 and 272-byte size\
    \ (17-bit address for the 1MB 64bit-width global buffer).\n\nIn summary, our high-level\
    \ idea is to divide the buffer into logical blocks for different layers and try\
    \ to reuse data for sliding convolution windows. The memory management approach\
    \ can be compatible with an accelerator as long as it supports the data movement\
    \ inside the on-chip memory and flexible data assignment for computing. Coupled\
    \ with our subgraph execution scheme introduced before, intermediate outputs in\
    \ the subgraph can avoid being recomputed. Only those layers required by other\
    \ subgraphs are written back to DRAM for further reuse.\n\n# <span id=\"page-5-0\"\
    ></span>4 Memory Communication-Capacity Co-Exploration\n\n<span id=\"page-5-1\"\
    ></span>**A subgraph example**\n\nThe aforementioned hardware model enables arbitrary\
    \ subgraph execution, but there is always limited buffer capacity\n\n**Data update\
    \ diagram**\n\n![](_page_5_Figure_6.jpeg)\n\nFigure 7. Memory allocation and data\
    \ update scheme in the global buffer for full data reuse. The data layout used\
    \ in our implementation is NWHC8c (aligned to 8 channels), which can be changed\
    \ in another design. <sup>0</sup> and <sup>0</sup> are the height and width of\
    \ an input tile; is the input channel size; is the global width-dimension index\
    \ of the input tensor; and <sup>0</sup> is the width-dimension index of an input\
    \ tile.\n\n<span id=\"page-5-2\"></span>![](_page_5_Figure_9.jpeg)\n\nFigure 8.\
    \ Hardware implementation with the buffer region manager in our 12nm NPU as a\
    \ demonstration. The layout is an NPU core extracted from part of our in-house\
    \ chip.\n\nin hardware. Therefore, we need to partition the whole computation\
    \ graph into a series of subgraphs that fit the memory. Below, we move up to the\
    \ optimization for graph partition and memory design-space exploration for challenge\
    \ 3.\n\n#### 4.1 Problem Formulation\n\n4.1.1 Graph-Level Partition. Formally,\
    \ a DNN model can be represented as a computation graph = ( , ), where is the\
    \ vertex set consisting of all the layers in a DNN model, and is the edge set\
    \ that defines the structure of DNN. In particular, an edge (, ) \u2208 represents\
    \ that the output of layer is an input of layer .\n\nWe aim to find a partition\
    \ scheme : \u2192 N that assigns each layer to a subgraph, where layer \u2208\
    \ is computed in the ()-th subgraph. A valid partition scheme should satisfy that\
    \ any layer is computed before use. Therefore, for any (, ) \u2208 , we have ()\
    \ \u2264 (). Moreover, any subgraph should be connected in , otherwise meaningless.\n\
    \nWe cast the partition exploration as an optimization problem. The objective\
    \ is to find a valid partition scheme that minimizes the total cost:\n\n<span\
    \ id=\"page-5-4\"></span>\n$$\\sum\\_{i} Cost\\_{\\mathcal{M}}(\\{v \\in V \\\
    mid P(v) = i\\}),\\tag{1}$$\n\nwhere is a cost function of a given subgraph based\
    \ on a target metric (e.g., external memory access (EMA) and energy). For each\
    \ subgraph, the EMA cost contains the loading of weights and input activations\
    \ and the storage of output activations[3](#page-5-3) . The energy cost includes\
    \ the overhead of EMA, on-chip buffers, and computation units.\n\n4.1.2 Design-Space\
    \ Exploration (DSE). Our work further extends the optimization to combine with\
    \ the memory design-space exploration. In this paper, we focus on the global buffer\
    \ and the weight buffer, given that they dominate\n\n<span id=\"page-5-3\"></span><sup>3</sup>The\
    \ nodes that are required to write-back to DRAM can be the model output layer\
    \ or the layers required by the future subgraph.\n\nthe overhead of energy and\
    \ area in an NPU core. As illustrated in Figure [1,](#page-1-0) a larger buffer\
    \ capacity can take in more layers inside a subgraph, reducing communication costs\
    \ but compromising the silicon area. To co-explore the hardware configuration\
    \ and mapping, we construct an objective function by a linear combination of the\
    \ hardware and mapping costs:\n\n<span id=\"page-6-0\"></span>\n$$\\text{BUF\\\
    \\_SIZE} + \\alpha \\cdot \\sum\\_{i} Cost\\_{\\mathcal{M}}(\\{v \\in V \\mid\
    \ P(v) = i\\}),\\qquad \\text{(2)}$$\n\nwhere is a preference hyper-parameter\
    \ to adjust the proportion between two costs.\n\n#### 4.2 Baseline Methods\n\n\
    Several optimization methods that exist today can perform graph-level partition.\
    \ However, most of them fail to directly co-explore hardware and partition. Below,\
    \ we list four typical methods as our baselines and sketch their features.\n\n\
    4.2.1 Enumeration-based Algorithm. Fused-CNN [\\[4\\]](#page-12-2) applies a straightforward\
    \ way to enumerate all possible partition schemes and return the best one. Jangda\
    \ et al. [\\[25\\]](#page-13-19) proposed state compression dynamic programming\
    \ to speed up the enumeration-based algorithm. We migrate their methods as our\
    \ baseline and further improve them by only recording one subgraph in the state\
    \ to reduce the time complexity.\n\nNonetheless, there are still exponential states\
    \ in the improved implementation. Let be the number of nodes in a graph, and the\
    \ enumeration-based method may explore up to (2 <sup>2</sup> ) states for irregular\
    \ networks. Consequently, the search is hard to complete within a reasonable search\
    \ time for large-scale networks, not to mention the co-exploration with DSE.\n\
    \n4.2.2 Greedy Algorithm. Halide [\\[47\\]](#page-14-21) employs a greedy algorithm\
    \ to perform function grouping, which can be applied to the graph-level partition.\
    \ Specifically, it first assigns each layer into a single-layer subgraph. Then\
    \ it iteratively fuses a pair of subgraphs contributing the greatest benefit until\
    \ all benefits are negative.\n\nTherefore, this algorithm tends to be trapped\
    \ at the local minimum. Moreover, since the fusion decision rules are based on\
    \ a given hardware, the greedy method cannot co-explore with DSE.\n\n4.2.3 Dynamic\
    \ Programming (DP)-based Algorithm. For the irregular network scheduling problem,\
    \ Zheng et al. [\\[73\\]](#page-15-3) proposed a DP-based algorithm. They arrange\
    \ the layers based on their depth and perform DP in a sequential manner.\n\nThis\
    \ method is restricted to assigning layers that are contiguous in the depth order\
    \ into a subgraph, hence the exploration is confined to constrained search space.\
    \ It is unlikely to find the global optimum, especially for non-plain network\n\
    \nstructures. In addition, since the state transition of DP depends on the predefined\
    \ buffer size, it is also tough to carry out co-exploration.\n\n4.2.4 Simulated\
    \ Annealing (SA). SA [\\[33\\]](#page-13-20) is a popular optimization algorithm\
    \ that samples a point and updates it iteratively to improve. It adopts the new\
    \ sample points with a probability affected by the performance difference and\
    \ a hyper-parameter named temperature. We employ the customized mutation operations\
    \ (described in Section [4.4.3\\)](#page-7-0) to update the sample points and\
    \ implement an SA-based algorithm as a baseline.\n\nSA is an alternative optimization\
    \ method for our framework with compatible operators, but it is not stable as\
    \ the genetic algorithm in a range of benchmarks, which will be shown in later\
    \ experiments.\n\n#### 4.3 Genetic Algorithm\n\nPrevious research shows competitive\
    \ performance of the Genetic Algorithm (GA) in several scheduling optimization\
    \ problems [\\[30,](#page-13-18) [31\\]](#page-13-21). We summarize several benefits\
    \ of GA for our hardware-mapping co-exploration problem:\n\n- 1. White-box property:\
    \ We can track and tune its optimization process conveniently. Therefore, it is\
    \ easy and intuitive to understand.\n- 2. Complete search space: It has the potential\
    \ to explore the complete search space by customized mutation and crossover operations.\n\
    - 3. Avoid local optima: In contrast to the greedy algorithm, GA can naturally\
    \ jump out of the local minimum benefiting from the diversity of the population.\n\
    - 4. Flexible initialization: We can use the results of other optimization algorithms\
    \ to initialize GA and use GA to finetune the result.\n- 5. Co-exploration: Through\
    \ the proposed GA operations and genome encoding, it can further support partition-DSE\
    \ co-exploration.\n\nWe encode each candidate solution (partition scheme and the\
    \ corresponding memory configuration for our problem) as a genome, and the population\
    \ contains a set of genomes. The GA goes through a series of generations to obtain\
    \ a lower cost. It performs the crossover and mutation operations on the population\
    \ in each generation. Specifically, a crossover operation blends two genomes selected\
    \ from the population to generate one offspring while a mutation operation modifies\
    \ a genome randomly. At the end of each generation, the evaluation environment\
    \ evaluates the fitness of each genome, and the population in the new generation\
    \ is selected based on the fitness results.\n\n#### 4.4 Cocco Optimization Framework\n\
    \nCocco is a GA-based optimization framework that enables the co-exploration of\
    \ memory configuration and graph-level partition, as shown in Figure [10.](#page-7-1)\
    \ The core of Cocco is a\n\n<span id=\"page-7-2\"></span>![](_page_7_Figure_2.jpeg)\n\
    \nFigure 9. Illustration of crossover and mutation operations in Cocco.\n\nseries\
    \ of operations that explore a complete search space. We build a genetic algorithm\
    \ based on these customized operations. Fed with the neural network structure\
    \ and DSE requirements, Cocco goes through several steps to get the optimization\
    \ results. The execution model described in Section [3](#page-3-1) is embedded\
    \ in the evaluation environment. In the following, we introduce the five stages\
    \ of Cocco.\n\n4.4.1 Initialization. The first step in Cocco is to generate the\
    \ initial population, where each genome contains a partition scheme of the computation\
    \ graph and a memory configuration for DSE. For the DSE part, every genome selects\
    \ a capacity value in a given range following a uniform distribution. There are\
    \ two options in Cocco to initialize the partition scheme of each genome. The\
    \ first option is random initialization. Precisely, we determine the () for each\
    \ layer \u2208 in topological order, and each () is selected randomly within the\
    \ valid range. The other option is to initialize the partition scheme from other\
    \ optimization algorithms.\n\n4.4.2 Crossover. We designed a customized crossover\
    \ operation to inherit and blend the features of two parents selected from the\
    \ population. Specifically, each hardware configuration (i.e., memory capacity)\
    \ in the offspring is the average of its parents and then rounds to the nearest\
    \ candidate value. For the partition scheme, we assign layers to subgraphs in\
    \ topological order. Each undecided layer chooses one parent randomly to reproduce\
    \ the corresponding subgraph. If the reproduced subgraph contains layers that\
    \ have been decided, we split out a new one excluding those layers, or merge it\
    \ with one of the subgraphs to which the decided layers belong.\n\nAs shown in\
    \ Figure [9\\(](#page-7-2)b), layer 1 and layer 3 select Dad as the parent to\
    \ reproduce the subgraphs {1, 2} and {3, 4}, respectively. Next, layer 5 selects\
    \ Mom as its parent, so it\n\n<span id=\"page-7-1\"></span>![](_page_7_Figure_8.jpeg)\n\
    \nFigure 10. Cocco framework overview.\n\nintends to reproduce subgraph {4, 5,\
    \ 6}. However, since we have already decided on layer 4 in subgraph {3, 4}, there\
    \ are two alternatives: creating a new subgraph {5, 6} (Child-1) or merging with\
    \ subgraph {3, 4} to obtain {3, 4, 5, 6} (Child-2).\n\n<span id=\"page-7-0\"></span>4.4.3\
    \ Mutation. Four mutation operations are customized for the optimization flow\
    \ to explore the search space extensively. We guarantee the validity of genomes\
    \ after each mutation in the implementation. At the bottom of Figure [9,](#page-7-2)\
    \ we show a node-level operation (modify-node) and two subgraph-level ones (split-subgraph\
    \ and merge-subgraph):\n\n- modify-node (Figure [9\\(](#page-7-2)c)): Modify the\
    \ assignment of a randomly selected node : from \u2192 () to \u2192 \u2032 (),\
    \ where \u2032 () can be an existed subgraph or a new one.\n- split-subgraph (Figure\
    \ [9\\(](#page-7-2)d)): Split a randomly selected subgraph into two or more subgraphs.\n\
    - merge-subgraph (Figure [9\\(](#page-7-2)e)): Merge two randomly selected subgraphs\
    \ into one subgraph.\n- mutation-DSE (not shown): Modify the memory configuration\
    \ to a random one within the range. The new values are sampled based on a normal\
    \ distribution, where the average is the original value, and the variance is a\
    \ hyper-parameter.\n\n4.4.4 Evaluation. Since GA tries to maximize the fitness\
    \ of the genomes, we set fitness to be the opposite of the cost (e.g., Formula\
    \ [1](#page-5-4) and [2\\)](#page-6-0). To evaluate the fitness of each genome\
    \ in the population, we use our simulator (introduced in the next section) to\
    \ extract the execution costs of subgraphs (e.g., EMA and energy).\n\nDuring the\
    \ evaluation, the simulator decodes the subgraph and hardware configuration of\
    \ each genome and calculates the fitness by aggregating the cost of each subgraph.\
    \ Particularly, when a large subgraph exceeds the buffer capacity, we perform\
    \ the split-subgraph operation to ensure genome validity. This kind of in-situ\
    \ tuning can increase the number of valid samples during the optimization operations\
    \ and thus, improve the sample efficiency.\n\n4.4.5 Selection. At the end of each\
    \ generation, Cocco performs the tournament selection. Specifically, it holds\
    \ multiple tournaments among a few randomly selected genomes, and the winners\
    \ (the genome with the best fitness) of these tournaments form the population\
    \ of a new generation. This operation facilitates superior fitness in the new\
    \ generation. The number of genomes in each tournament is decided by a hyper-parameter.\
    \ The new generation subsequently starts from the crossover step again.\n\n##\
    \ 5 Experiments\n\nIn the evaluations, we first present the superiority of Cocco\
    \ for the graph partition; and then demonstrate its outstanding stability and\
    \ sample efficiency of the co-exploration for the hardware optimization, followed\
    \ by additional discussions about the results under different configurations.\n\
    \n#### 5.1 Methodology\n\n5.1.1 Evaluated Models. In the following evaluations,\
    \ we consider three types of model structures: plain (VGG16 [\\[57\\]](#page-14-2)),\
    \ multi-branch (ResNet50, ResNet152 [\\[20\\]](#page-13-4), GoogleNet [\\[59\\\
    ]](#page-14-3), Transformer [\\[64\\]](#page-14-22), and GPT [\\[52\\]](#page-14-23)),\
    \ and irregular structure (RandWire-A/B [\\[68\\]](#page-14-6) and NasNet [\\\
    [75\\]](#page-15-1)). RandWire-A/B are\n\ngenerated based on the small and regular\
    \ regime configurations introduced in the paper [\\[68\\]](#page-14-6). FC layers\
    \ are transformed to 1\xD71 CONV while pooling and element-wise layers are analyzed\
    \ as depth-wise CONV without weights. The scalar operations (e.g., activation\
    \ function) are hidden in the pipeline (e.g., the post-process module following\
    \ PE in Simba [\\[56\\]](#page-14-20)) and their overhead can be ignored.\n\n\
    5.1.2 Accelerator Platform. As shown at the top of Figure [10,](#page-7-1) we\
    \ consider a SIMBA-like hierarchical accelerator with a global buffer, a weight\
    \ buffer, and a 4\xD74 PE array in each core used in several previous works [\\\
    [56,](#page-14-20) [61,](#page-14-9) [71\\]](#page-15-6). Each PE contains an\
    \ 8\xD78 MAC array to process a sub-tile from the global buffer. In particular,\
    \ we model the execution flow based on the scheme described in Section [3.](#page-3-1)\
    \ The parallelism of two dimensions of the PE array can be dynamically configured\
    \ by the mapper results to ensure high utilization. We schedule subgraphs in topological\
    \ order and prefetch weights of the next subgraph during the current computing.\
    \ We also extend our platform to support fundamental multi-core studies by interconnecting\
    \ cores with a crossbar. They share weights to release the burden of each core.\n\
    \nThe arithmetic and memory overhead is extracted in a 12nm library based on the\
    \ synthesized RTL implementations (SRAM based on the ARM memory compiler) with\
    \ 1GHz. The DRAM energy is set as 12.5pJ/bit [\\[70\\]](#page-15-2). The extra\
    \ footprint of the plug-in design is mainly a 272-Byte register file to store\
    \ the head and end logical region addresses of maximal 64 nodes, which is negligible.\
    \ Based on off-the-shelf evaluators Timeloop [\\[50\\]](#page-14-18) and MAESTRO\
    \ [\\[37\\]](#page-13-7) for spatial accelerators, we developed a modified simulator\
    \ that supports the evaluation of latency and energy. It employs the consumption-centric\
    \ scheme to determine the tile size of each layer, and the memory access in the\
    \ model is free from padding data. The latency per subgraph depends on the maximum\
    \ of the calculation and external communication cycles. We allocate 16GB/s DRAM\
    \ bandwidth per accelerator core for loading weights and input activations and\
    \ writing back data for subsequent subgraphs. The off-chip communication consists\
    \ of weight loading of each layer and the inputs and outputs of each subgraph.\
    \ As described in Section [3,](#page-3-1) our subgraph execution scheme avoids\
    \ recomputing of intermediate outputs.\n\n5.1.3 Baselines. Three optimization\
    \ baselines for graph partition are the greedy algorithm used in Halide [\\[47\\\
    ]](#page-14-21), dynamic programming (DP) used in Irregular-NN [\\[73\\]](#page-15-3)\
    \ , and the enumeration-based method as a reference.\n\nFor the DSE studies, we\
    \ compare Cocco with simulated annealing (SA) [\\[33\\]](#page-13-20) to demonstrate\
    \ the better stability of GA. These two methods are both the co-optimization scheme\
    \ that optimizes partition and hardware settings at the same time. In contrast\
    \ to co-optimization, the two-step scheme is another method for design-space exploration.\
    \ Specifically, we\n\n<span id=\"page-9-0\"></span>![](_page_9_Figure_2.jpeg)\n\
    \nFigure 11. The evaluation results for graph partition using the EMA-opt configuration\
    \ (EMA as the optimization metric). The enumeration-based method is deterministic,\
    \ which figures out the optimal solution as a reference in the first four models.\
    \ It cannot complete for large-scale models (Transformer, GPT, RandWire-A, and\
    \ RandWire-B) in a reasonable time because of the exponential search space.\n\n\
    use random search (RS) or grid search (GS) to sample memory capacity candidates\
    \ and then explore the corresponding partition schemes. During the search, we\
    \ evaluate 5,000 samples for each capacity candidate and keep the best candidate\
    \ as the output. As for the sampling method, RS randomly samples memory capacity\
    \ candidates while GS uses a coarser granularity to enumerate the candidates.\n\
    \n#### 5.2 Graph Partition Evaluations\n\nWe start by presenting the partition\
    \ performance on the single-core hardware with a 1MB global buffer and a 1.125MB\
    \ weight buffer. The number of samples in Cocco is set to be 400,000. We evaluate\
    \ the external memory access (EMA) and bandwidth requirements of eight models\
    \ shown in Figure [11,](#page-9-0) where the results are normalized to the Halide\
    \ baseline. This experiment aims to validate the effectiveness of our Cocco framework\
    \ in graph partition. For networks with simpler structures, Cocco can find out\
    \ the optimal solutions same as the enumeration-based results. For large-scale\
    \ irregular networks (Transformer, GPT, RandWire-A, and RandWire-B), the enumeration-based\
    \ method cannot complete in a reasonable time, while Cocco provides better solutions\
    \ than Halide and DP. A better subgraph partition strategy helps to ease the communication\
    \ burden, thus reducing the EMA cost and bandwidth requirements.\n\n#### 5.3 Hardware-Mapping\
    \ Co-Exploration\n\nAfter learning the superiority of Cocco for the graph partition,\
    \ we further co-explore the memory configuration and graph partition mapping as\
    \ the core study of this work. Three categories of exploration methods are used,\
    \ including the fixed hardware scheme, the two-step scheme as baselines, and the\
    \ proposed co-optimization scheme. We set three fixed memory configurations with\
    \ Small capacity, Medium capacity, and Large capacity, followed by a partition-only\
    \ procedure. The two-step scheme is implemented with decoupled steps for capacity\
    \ search (RS or GS) and partition (GA). The cooptimization methods include the\
    \ proposed Cocco and an SA-based one as the comparison. All methods sample up\
    \ to\n\n<span id=\"page-9-2\"></span>Table 1. Hardware-mapping co-exploration\
    \ for separate buffer. In this table, A refers to the global buffer, and W refers\
    \ to the weight buffer. We evaluate the cost using Formula [2](#page-6-0) (the\
    \ lower cost, the better), where the metric is energy. We use RandWire-A as RandWire\
    \ in the following experiments.\n\n| Optimization |        | ResNet50      | \
    \                  |        | GoogleNet |                   |        |  |\n|--------------|--------|---------------|-------------------|--------|-----------|-------------------|--------|--|\n\
    |              |        |               | Size (A) Size (W) | Cost   |       \
    \    | Size (A) Size (W) | Cost   |  |\n|              | Buf(S) | 512KB      \
    \   | 576KB             | 1.04E7 | 512KB     | 576KB             | 4.07E6 |  |\n\
    | Fixed<br>HW  |        | Buf(M) 1024KB | 1152KB            | 1.07E7 | 1024KB\
    \    | 1152KB            | 5.06E6 |  |\n|              | Buf(L) | 2048KB     \
    \   | 2304KB            | 1.24E7 | 2048KB    | 2304KB            | 7.18E6 |  |\n\
    |              | RS+GA  | 448KB         | 864KB             | 1.04E7 | 384KB \
    \    | 432KB             | 3.88E6 |  |\n| Two-Step     | GS+GA  | 128KB      \
    \   | 864KB             | 1.07E7 | 128KB     | 144KB             | 3.80E6 |  |\n\
    |              | SA     | 256KB         | 360KB             | 1.06E7 | 192KB \
    \    | 144KB             | 3.78E6 |  |\n| Co-Opt       | Cocco  | 704KB      \
    \   | 864KB             | 1.04E7 | 192KB     | 432KB             | 3.75E6 |  |\n\
    |              |        |               |                   |        |       \
    \    |                   |        |  |\n|              |        |            \
    \   | RandWire          |        |           | NasNet            |        |  |\n\
    | Optimization |        |               | Size (A) Size (W) | Cost   |       \
    \    | Size (A) Size (W) | Cost   |  |\n|              | Buf(S) | 512KB      \
    \   | 576KB             | 3.23E6 | 512KB     | 576KB             | 6.14E7 |  |\n\
    | Fixed        |        | Buf(M) 1024KB | 1152KB            | 3.92E6 | 1024KB\
    \    | 1152KB            | 5.83E7 |  |\n| HW           | Buf(L) | 2048KB     \
    \   | 2304KB            | 6.00E6 | 2048KB    | 2304KB            | 5.66E7 |  |\n\
    |              | RS+GA  | 448KB         | 792KB             | 3.31E6 | 1152KB\
    \    | 2016KB            | 5.60E7 |  |\n| Two-Step     | GS+GA  | 128KB      \
    \   | 144KB             | 3.02E6 | 2048KB    | 2304KB            | 5.66E7 |  |\n\
    | Co-Opt       | SA     | 192KB         | 144KB             | 3.00E6 | 2048KB\
    \    | 1872KB            | 5.61E7 |  |\n\n50,000 points. The energy-capacity co-optimization\
    \ is used in the following evaluations.\n\n5.3.1 DSE analysis using separate and\
    \ shared buffer. We first perform the hardware-mapping co-exploration to determine\
    \ the suitable memory configuration (except for the fixed-HW scheme) with = 0.002[4](#page-9-1)\
    \ and then solely execute the partition-only Cocco to obtain the final cost. In\
    \ particular, we also compared the results using two memory designs: separate\
    \ buffer and shared buffer. For the separate buffer design, activations and weights\
    \ are stored in different buffers while they share the same space in the shared\
    \ buffer design. The memory capacity candidates for the global buffer\n\n<span\
    \ id=\"page-9-1\"></span><sup>4</sup>The energy and the capacity units are pJ\
    \ and Byte, respectively.\n\nCocco: Hardware-Mapping Co-Exploration towards Memory\
    \ ... ASPLOS '24, April 27-May 1, 2024, La Jolla, CA, USA\n\n<span id=\"page-10-0\"\
    ></span>Table 2. Hardware-mapping co-exploration for shared buffer. We evaluate\
    \ the cost using Formula [2](#page-6-0) (the lower cost, the better), where the\
    \ metric is energy.\n\n| Optimization |        |        | ResNet50 | GoogleNet\
    \ |        |  |\n|--------------|--------|--------|----------|-----------|--------|--|\n\
    |              |        | Size   | Cost     | Size      | Cost   |  |\n| Fixed<br>HW\
    \  | Buf(S) | 576KB  | 1.01E7   | 576KB     | 3.66E6 |  |\n|              | Buf(M)\
    \ | 1152KB | 1.00E7   | 1152KB    | 4.04E6 |  |\n|              | Buf(L) | 2304KB\
    \ | 1.04E7   | 2304KB    | 5.09E6 |  |\n|              | RS+GA  | 1280KB | 0.98E7\
    \   | 640KB     | 3.65E6 |  |\n| Two-Step     | GS+GA  | 1344KB | 0.98E7   | 512KB\
    \     | 3.65E6 |  |\n|              | SA     | 896KB  | 1.00E7   | 192KB     |\
    \ 3.75E6 |  |\n| Co-Opt       | Cocco  | 1344KB | 0.98E7   | 384KB     | 3.60E6\
    \ |  |\n|              |        |        |          |           |        |  |\n\
    |              |        |        | RandWire | NasNet    |        |  |\n| Optimization\
    \ |        | Size   | Cost     | Size      | Cost   |  |\n|              | Buf(S)\
    \ | 576KB  | 2.83E6   | 576KB     | 6.36E7 |  |\n| Fixed        | Buf(M) | 1152KB\
    \ | 3.03E6   | 1152KB    | 5.73E7 |  |\n| HW           | Buf(L) | 2304KB | 3.90E6\
    \   | 2304KB    | 5.51E7 |  |\n|              | RS+GA  | 320KB  | 2.85E6   | 2560KB\
    \    | 5.47E7 |  |\n| Two-Step     | GS+GA  | 832KB  | 2.86E6   | 3072KB    |\
    \ 5.42E7 |  |\n| Co-Opt       | SA     | 256KB  | 2.92E6   | 1728KB    | 5.56E7\
    \ |  |\n\n(for activations) range from 128KB to 2048KB with a 64KB interval, while\
    \ that for the weight buffer range from 144KB to 2304KB with a 72KB interval.\
    \ The exploration range of the shared buffer is from 128KB to 3072KB with an interval\
    \ of 64KB.\n\nThe evaluation using separate buffers is shown in Table [1,](#page-9-2)\
    \ where Cocco achieves better optimization with up to 1.89% (compared to SA in\
    \ ResNet50) to 50.33% (compared to Fixed-HW(L) in RandWire) lower cost compared\
    \ to various baselines across four models. The two-step scheme fails to combine\
    \ the information between different sizes, so it is generally worse than the co-optimization\
    \ method.\n\nThe capacity results also reflect the inherent capacity preference\
    \ of different models. The data amount in GoogleNet and RandWire is relatively\
    \ smaller, and thus their capacity requirements are lower. In contrast, the data\
    \ amount in NasNet is larger, so a high capacity is preferred.\n\nAs shown in\
    \ Table [2,](#page-10-0) the evaluation of the shared buffer setting shows a similar\
    \ trend. Furthermore, we can observe that most of the cost results of the shared\
    \ buffer are lower than those using the separate configuration. Although the shared\
    \ buffer design requires additional control flows, it indeed improves efficiency\
    \ than the separate buffer design.\n\n5.3.2 Sample efficiency analysis. We next\
    \ study the sample efficiency of the two-step and the co-optimization scheme in\
    \ Figure [12.](#page-10-1) We record the cost trends of the first 50,000 samples\
    \ on ResNet50, GoogleNet, and RandWire during the exploration. Overall, Cocco\
    \ shows a consistent convergence trend on these three networks. And it converges\
    \ faster and\n\n<span id=\"page-10-1\"></span>![](_page_10_Figure_9.jpeg)\n\n\
    Figure 12. The convergence curve of Cocco compared with other baselines in the\
    \ hardware-mapping co-explorations. The optimization method requiring fewer samples\
    \ in (d) has higher sample efficiency.\n\n<span id=\"page-10-2\"></span>![](_page_10_Figure_11.jpeg)\n\
    \nFigure 13. The visualization of sample points distribution during optimization.\
    \ The slope of the red dashed line denotes the preference between energy and capacity\
    \ cost. The point on the line with a lower intercept has a smaller cost.\n\nachieves\
    \ lower costs compared to other baselines, exhibiting a higher sample efficiency.\
    \ The two-step methods perform graph-partition separately under different capacities,\
    \ so they fail to utilize the partition information between capacities. Particularly,\
    \ the GS method uses a deterministic search direction (search from large to small\
    \ capacity in this experiment), so the convergence time depends on the optimal\
    \ capacity. Since GoogleNet and RandWire require relatively small buffers, GS\
    \ takes a considerable number of samples to converge.\n\n5.3.3 Optimization procedure\
    \ analysis. We next study how the distribution of sample points changes during\
    \ the optimization procedure of Cocco. While searching for 20 generations with\
    \ 500 genomes each, we divided them into ten groups with different colors in Figure\
    \ [13.](#page-10-2) The results show that the distribution moves towards a lower\
    \ intercept\n\n<span id=\"page-11-0\"></span>![](_page_11_Figure_1.jpeg)\n\nFigure\
    \ 14. The trade-off between energy and memory capacity. The optimization target\
    \ is to minimize the cost defined in Formula [2,](#page-6-0) where the metric\
    \ is energy. Energy results of each model are normalized to the first (= 0.0005)\
    \ results.\n\n<span id=\"page-11-1\"></span>Table 3. Multi-core and batch evaluation\
    \ using the energycapacity co-opt configuration. Size denotes the shared buffer\
    \ size in each core.\n\n| Core# Batch |             |            | ResNet50  \
    \   |            | GoogleNet           |                 |              |  |\n\
    |-------------|-------------|------------|--------------|------------|---------------------|-----------------|--------------|--|\n\
    |             |             | Energy(mJ) | Lat.(ms)     |            | Size(KB)\
    \ Energy(mJ) | Lat.(ms)        | Size(KB)     |  |\n| 1           | 1        \
    \   | 4.21       | 4.59         | 1344       | 1.61                | 2.05    \
    \        | 384          |  |\n|             | 2           | 6.32       | 8.98\
    \         | 1728       | 2.18                | 3.91            | 896         \
    \ |  |\n|             | 8           | 11.88      | 35.93        | 2880       |\
    \ 5.64                | 15.53           | 1472         |  |\n|             | 1\
    \           | 4.38       | 2.48         | 768        | 1.66                | 1.04\
    \            | 192          |  |\n| 2           | 2           | 6.46       | 4.78\
    \         | 1088       | 2.34                | 1.99            | 384         \
    \ |  |\n|             | 8           | 13.01      | 19.12        | 1664       |\
    \ 5.84                | 7.97            | 960          |  |\n|             | 1\
    \           | 4.29       | 1.39         | 448        | 1.34                | 0.54\
    \            | 192          |  |\n| 4           | 2           | 6.58       | 2.68\
    \         | 640        | 2.20                | 1.07            | 192         \
    \ |  |\n|             | 8           | 11.50      | 10.71        | 1664       |\
    \ 6.24                | 4.30            | 448          |  |\n|             | \
    \            |            |              |            |                     |\
    \                 |              |  |\n|             |             |         \
    \   | RandWire     |            |                     | NasNet          |    \
    \          |  |\n|             | Core# Batch | Energy(mJ) | Lat.(ms)     |   \
    \         | Size(KB) Energy(mJ) | Lat.(ms)        | Size(KB)     |  |\n|     \
    \        | 1           | 1.26       | 1.47         | 384        | 28.57      \
    \         | 49.92           | 2624         |  |\n| 1           | 2           |\
    \ 2.25       | 2.74         | 704        | 47.68               | 99.87       \
    \    | 3072         |  |\n|             | 8           | 8.66       | 10.85   \
    \     | 1664       | 133.03              | 396.90          | 3072         |  |\n\
    |             | 1           | 1.41       | 0.95         | 192        | 29.18 \
    \              | 24.93           | 1728         |  |\n| 2           | 2      \
    \     | 2.37       | 1.80         | 384        | 48.80               | 49.73 \
    \          | 2624         |  |\n|             | 8           | 8.39       | 7.16\
    \         | 1280       | 153.25              | 227.19          | 3072        \
    \ |  |\n|             | 1           | 1.39       | 0.71         | 192        |\
    \ 28.00               | 14.56           | 960          |  |\n| 4           | 2\
    \           | 2.91       | 1.40<br>5.55 | 192<br>960 | 45.03               | 28.58<br>133.38\
    \ | 1664<br>2816 |  |\n\nof the -slope line and gets more centralized in the later\
    \ generations during the optimization process of Cocco.\n\n#### 5.4 Sensitivity\
    \ Study about Cocco framework\n\n5.4.1 Study of in the cost function. The results\
    \ shown in Figure [14](#page-11-0) demonstrate the effectiveness of in adjusting\
    \ the preference between the memory capacity and the given metric (energy is used\
    \ here). The optimization trades the memory capacity for lower energy cost with\
    \ the increase of . In addition, a larger memory capacity indeed contributes to\
    \ lower energy, but the yields show differences because of their various model-inherent\
    \ graph and layer patterns. For example, NasNet is more memory-intensive and more\
    \ structure-complex than the other three models, so it requires a larger memory\
    \ capacity for less energy consumption.\n\n5.4.2 Study of performance v.s. memory\
    \ capacity. Figure [2](#page-2-0) shows that the increase of capacity is sub-linear\
    \ with\n\nperformance. To study this observation, we scale our model to the multi-core\
    \ version and share weights of a subgraph across cores. Different cores only buffer\
    \ a subset of weights and transfer the data between cores, similar to BSD in Tangram\
    \ [\\[18\\]](#page-13-11) or data-rotation in NN-Baton [\\[61\\]](#page-14-9).\
    \ The overhead of the interconnection crossbar is extracted from the implemented\
    \ Arteries IP [\\[5\\]](#page-12-9).\n\nAn accelerator with more cores can cover\
    \ a larger subgraph but bring more core-to-core overhead. As shown in Table [3,](#page-11-1)\
    \ in most cases, energy increases from the single-core to dual-core configuration\
    \ because of the communication overhead. Moreover, profiting from the data-sharing\
    \ mechanism, the required memory of each core drops with the increase of core\
    \ number.\n\n5.4.3 Batch size study. For the batch size evaluation shown in Table\
    \ [3,](#page-11-1) the latency with a larger batch size principally presents a\
    \ sub-linear increase, which benefits from the lower bandwidth requirement of\
    \ weights via the inter-sample data reuse. In addition, such data reuse amortizes\
    \ the energy burden per batch processing. And owing to the better weight reuse\
    \ in multi-batch processing, a larger batch size does not require a proportional\
    \ capacity.\n\n## 6 Related Works\n\n#### 6.1 Intra-layer Optimization\n\nPrior\
    \ works focus on the data reuse for intra-layer assignments, like output-stationary\
    \ in ShiDianNao [\\[14\\]](#page-13-22) and Envision [\\[46\\]](#page-14-24),\
    \ weight-stationary in NeuFlow [\\[15\\]](#page-13-23) and Nvdla [\\[49\\]](#page-14-16),\
    \ input-stationary in SCNN [\\[51\\]](#page-14-25), and row-stationary in Eyeriss\
    \ [\\[13\\]](#page-12-10). Based on these primitive dataflow patterns, extensive\
    \ studies explored the optimal tiling and reordering schemes via brute-force,\
    \ feedback-based, and constraint optimization approaches [\\[23,](#page-13-5)\
    \ [30,](#page-13-18) [50\\]](#page-14-18). These works focus on layer-level optimization,\
    \ missing the graph information at a higher level. The efficiency of tile updates\
    \ depends on the memory architecture. Simba [\\[56,](#page-14-20) [74\\]](#page-15-5)\
    \ and NN-Baton [\\[61\\]](#page-14-9) view each tile as an independent workload\
    \ so that the tile size has a prominent impact on memory access due to halo regions.\
    \ Motivated by traditional vision processors, Ascend [\\[40\\]](#page-13-10) and\
    \ DRQ [\\[58\\]](#page-14-26) employ line buffers to achieve data reuse in the\
    \ row direction, but the line buffer cannot well support the 2D-tiling reuse in\
    \ both row and column directions.\n\n#### 6.2 Inter-layer Optimization\n\nIntra-layer\
    \ scheduling is sub-optimal, which is limited by the data reuse within a layer.\
    \ Therefore, Fused-CNN [\\[4\\]](#page-12-2), SR-CNN [\\[38\\]](#page-13-8), and\
    \ LCP [\\[42\\]](#page-14-19) introduce layer fusion method that cache intermediate\
    \ data on-chip to reduce data transfer overhead using handcrafted or heuristic\
    \ methods for fusion partition. Although Irregular-NN [\\[73\\]](#page-15-3) suggests\
    \ a customized-DP algorithm, the exploration space is constrained because the\
    \ layers in an assignment need to be successive in a specific\n\norder. A recent\
    \ work named DNNFuser [\\[29\\]](#page-13-24) employs an RLbased method, but their\
    \ formulation towards 1D layer-fusion is hard to handle complex irregular networks.\
    \ Tangram [\\[18\\]](#page-13-11) and Atomic [\\[72\\]](#page-15-4) schedule DNN\
    \ workloads on a multi-core (scalable) accelerator, but they focus on executing\
    \ a single layer on each core at a time rather than processing multiple layers\
    \ with local data reuse. Also, some previous works [\\[2,](#page-12-11) [19,](#page-13-25)\
    \ [62\\]](#page-14-27) tackle the workload placement problem for multiple devides\
    \ without discussing the downstream execution on each device.\n\nCocco proposes\
    \ an automatic framework for inter-layer scheduling with a comprehensive memory\
    \ scheme. It focuses on the fundamental core-level temporal execution that can\
    \ be potentially scaled up to the multi-core or multi-device scenario with a spatial\
    \ parallelism mechanism.\n\n#### 6.3 Design-Space Exploration for Memory\n\nMemory\
    \ design exploration methods lie primarily on two sides: analysis-driven and search-driven.\
    \ For the analysisdriven method, Chen et al. [\\[12\\]](#page-12-0) leverage red-blue\
    \ pebble models to derive the proper memory capacity representations. Subsequently,\
    \ Cai et al. [\\[9\\]](#page-12-12) propose Olympus, which generalizes a framework\
    \ to a batch of successive layers and also fills up with more scheduling and data\
    \ reuse techniques. However, they are difficult to represent a subgraph with complex\
    \ inter-layer connections. As for the search-driven method, Xiao et al. [\\[67\\\
    ]](#page-14-11), Kwon et al. [\\[37\\]](#page-13-7), and Feng et al. [\\[16\\\
    ]](#page-13-26) explore the memory configuration for the layer-level assignment\
    \ using the brute-force search, while Kao et al. [\\[32\\]](#page-13-6) employ\
    \ a genetic algorithm to improve the efficiency. These works principally focus\
    \ on the layer-level information, while in comparison, Cocco exploits graph-level\
    \ features for the better optimization.\n\n## 7 Conclusion\n\nWhile layer-level\
    \ scheduling is widely studied to improve memory efficiency, graph-level optimization\
    \ remains relatively unexplored. This paper proposed a graph-level dataflow with\
    \ the corresponding memory management scheme that enables flexible graph partitions\
    \ with high memory utilization. On top of it, we propose Cocco, a framework to\
    \ provide a recommended memory configuration with graph-level scheduling strategies.\
    \ Cocco shows outstanding graph partition ability compared to the greedy algorithm\
    \ and DP employed in previous works and enables efficient graph-level hardware-mapping\
    \ co-exploration. This paper helps to provide an implementation philosophy for\
    \ the accelerator memory and better deployment for it.\n\n## Acknowledgments\n\
    \nThis research was partially supported by National Key R&D Program of China (2022YFB2804103),\
    \ Tsinghua University Dushi Program, and Tsinghua University Talent Program. We\
    \ would like to appreciate all the anonymous reviewers for their valuable feedback.\n\
    \n## References\n\n- <span id=\"page-12-5\"></span>[1] Dennis Abts, Jonathan Ross,\
    \ Jonathan Sparling, Mark Wong-VanHaren, Max Baker, Tom Hawkins, Andrew Bell,\
    \ John Thompson, Temesghen Kahsai, Garrin Kimmell, Jennifer Hwang, Rebekah Leslie-Hurd,\
    \ Michael Bye, E. R. Creswick, Matthew Boyd, Mahitha Venigalla, Evan Laforge,\
    \ Jon Purdy, Purushotham Kamath, Dinesh Maheshwari, Michael Beidler, Geert Rosseel,\
    \ Omar Ahmad, Gleb Gagarin, Richard Czekalski, Ashay Rane, Sahil Parmar, Jeff\
    \ Werner, Jim Sproch, Adrian Macias, and Brian Kurtz. 2020. Think Fast: A Tensor\
    \ Streaming Processor (TSP) for Accelerating Deep Learning Workloads. In Proceedings\
    \ of the 47th ACM/IEEE Annual International Symposium on Computer Architecture\
    \ (ISCA). IEEE, Valencia, Spain, 145\u2013158.\n- <span id=\"page-12-11\"></span>[2]\
    \ Ravichandra Addanki, Shaileshh Bojja Venkatakrishnan, Shreyan Gupta, Hongzi\
    \ Mao, and Mohammad Alizadeh. 2019. Learning Generalizable Device Placement Algorithms\
    \ for Distributed Machine Learning. In Advances in Neural Information Processing\
    \ Systems (NeurIPS), Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence\
    \ d'Alch\xE9-Buc, Emily B. Fox, and Roman Garnett (Eds.). OpenReview.net, Vancouver,\
    \ BC, Canada, 3983\u20133993.\n- <span id=\"page-12-1\"></span>[3] Byung Hoon\
    \ Ahn, Jinwon Lee, Jamie Menjay Lin, Hsin-Pai Cheng, Jilei Hou, and Hadi Esmaeilzadeh.\
    \ 2020. Ordering Chaos: Memory-Aware Scheduling of Irregularly Wired Neural Networks\
    \ for Edge Devices. In Proceedings of Machine Learning and Systems (MLSys), Inderjit\
    \ S. Dhillon, Dimitris S. Papailiopoulos, and Vivienne Sze (Eds.). mlsys.org,\
    \ Austin, TX, USA, 1\u201314.\n- <span id=\"page-12-2\"></span>[4] Manoj Alwani,\
    \ Han Chen, Michael Ferdman, and Peter A. Milder. 2016. Fused-layer CNN accelerators.\
    \ In Proceedings of the 49th IEEE/ACM International Symposium on Microarchitecture\
    \ (MICRO). IEEE Computer Society, Taipei, Taiwan, 22:1\u201322:12.\n- <span id=\"\
    page-12-9\"></span>[5] Arteries. 2022. Arteries IP Homepage. <https://www.arteris.com>.\n\
    - <span id=\"page-12-3\"></span>[6] Ljubisa Bajic and Jasmina Vasiljevic. 2020.\
    \ Compute substrate for Software 2.0. In Proceedings of the IEEE Hot Chips 32\
    \ Symposium (HCS). IEEE, Palo Alto, CA, USA, 1\u201331.\n- <span id=\"page-12-6\"\
    ></span>[7] Pete Bannon, Ganesh Venkataramanan, Debjit Das Sarma, and Emil Talpes.\
    \ 2019. Computer and Redundancy Solution for the Full Self-Driving Computer. In\
    \ Proceedings of the IEEE Hot Chips 31 Symposium (HCS). IEEE, Cupertino, CA, USA,\
    \ 1\u201322.\n- <span id=\"page-12-7\"></span>[8] John Burgess. 2019. RTX ON -\
    \ The NVIDIA TURING GPU. In Proceedings of the IEEE Hot Chips 31 Symposium (HCS).\
    \ IEEE, Cupertino, CA, USA, 1\u201327.\n- <span id=\"page-12-12\"></span>[9] Xuyi\
    \ Cai, Ying Wang, Kaijie Tu, Chengsi Gao, and Lei Zhang. 2022. Olympus: Reaching\
    \ Memory-Optimality on DNN Processors. IEEE Transactions on Computers (TC) 71,\
    \ 8 (2022), 1939\u20131951.\n- <span id=\"page-12-8\"></span>[10] Prasanth Chatarasi,\
    \ Hyoukjun Kwon, Angshuman Parashar, Michael Pellauer, Tushar Krishna, and Vivek\
    \ Sarkar. 2022. Marvel: A Data-Centric Approach for Mapping Deep Learning Operators\
    \ on Spatial Accelerators. ACM Transactions on Architecture and Code Optimization\
    \ 19, 1 (2022), 6:1\u20136:26.\n- <span id=\"page-12-4\"></span>[11] Karam Chatha.\
    \ 2021. Qualcomm\xAE Cloud Al-100: 12TOPS/W Scalable, High Performance and Low\
    \ Latency Deep Learning Inference Accelerator. In Proceedings of the IEEE Hot\
    \ Chips 33 Symposium (HCS). IEEE, Palo Alto, CA, USA, 1\u201319.\n- <span id=\"\
    page-12-0\"></span>[12] Xiaoming Chen, Yinhe Han, and Yu Wang. 2020. Communication\
    \ Lower Bound in Convolution Accelerators. In Proceedings of the IEEE International\
    \ Symposium on High Performance Computer Architecture (HPCA). IEEE, San Diego,\
    \ CA, USA, 529\u2013541.\n- <span id=\"page-12-10\"></span>[13] Yu-Hsin Chen,\
    \ Joel S. Emer, and Vivienne Sze. 2016. Eyeriss: A Spatial Architecture for Energy-Efficient\
    \ Dataflow for Convolutional Neural Networks. In Proceedings of the ACM/IEEE Annual\
    \ International Symposium on Computer Architecture (ISCA). IEEE Computer Society,\
    \ Seoul,\n\nSouth Korea, 367\u2013379.\n\n- <span id=\"page-13-22\"></span>[14]\
    \ Zidong Du, Robert Fasthuber, Tianshi Chen, Paolo Ienne, Ling Li, Tao Luo, Xiaobing\
    \ Feng, Yunji Chen, and Olivier Temam. 2015. ShiDianNao: shifting vision processing\
    \ closer to the sensor. In Proceedings of the ACM/IEEE Annual International Symposium\
    \ on Computer Architecture (ISCA). ACM, Portland, OR, USA, 92\u2013104.\n- <span\
    \ id=\"page-13-23\"></span>[15] Cl\xE9ment Farabet, Berin Martini, B. Corda, Polina\
    \ Akselrod, Eugenio Culurciello, and Yann LeCun. 2011. NeuFlow: A runtime reconfigurable\
    \ dataflow processor for vision. In Proceedings of the IEEE Conference on Computer\
    \ Vision and Pattern Recognition (CVPR) Workshops. IEEE Computer Society, Colorado\
    \ Springs, CO, USA, 109\u2013116.\n- <span id=\"page-13-26\"></span>[16] Kaijie\
    \ Feng, Xiaoya Fan, Jianfeng An, Xiping Wang, Kaiyue Di, Jiangfei Li, Minghao\
    \ Lu, and Chuxi Li. 2021. ERDSE: efficient reinforcement learning based design\
    \ space exploration method for CNN accelerator on resource limited platform. Graphics\
    \ and Visual Computing 4 (2021), 1\u201311.\n- <span id=\"page-13-0\"></span>[17]\
    \ Ken-ichi Funahashi. 1989. On the approximate realization of continuous mappings\
    \ by neural networks. Neural Networks 2, 3 (1989), 183\u2013192.\n- <span id=\"\
    page-13-11\"></span>[18] Mingyu Gao, Xuan Yang, Jing Pu, Mark Horowitz, and Christos\
    \ Kozyrakis. 2019. TANGRAM: Optimized Coarse-Grained Dataflow for Scalable NN\
    \ Accelerators. In Proceedings of the International Conference on Architectural\
    \ Support for Programming Languages and Operating Systems (ASPLOS). ACM, Providence,\
    \ RI, USA, 807\u2013820.\n- <span id=\"page-13-25\"></span>[19] Yuanxiang Gao,\
    \ Li Chen, and Baochun Li. 2018. Spotlight: Optimizing Device Placement for Training\
    \ Deep Neural Networks. In Proceedings of the 35th International Conference on\
    \ Machine Learning (ICML) (Proceedings of Machine Learning Research, Vol. 80),\
    \ Jennifer G. Dy and Andreas Krause (Eds.). PMLR, Stockholm, Sweden, 1662\u2013\
    1670.\n- <span id=\"page-13-4\"></span>[20] Kaiming He, Xiangyu Zhang, Shaoqing\
    \ Ren, and Sun Jian. 2016. Deep Residual Learning for Image Recognition. In Proceedings\
    \ of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE\
    \ Computer Society, Las Vegas, NV, USA, 770\u2013778.\n- <span id=\"page-13-17\"\
    ></span>[21] Kartik Hegde, Po-An Tsai, Sitao Huang, Vikas Chandra, Angshuman Parashar,\
    \ and Christopher W. Fletcher. 2021. Mind mappings: enabling efficient algorithm-accelerator\
    \ mapping space search. In Proceedings of the 26th ACM International Conference\
    \ on Architectural Support for Programming Languages and Operating Systems (ASPLOS),\
    \ Tim Sherwood, Emery D. Berger, and Christos Kozyrakis (Eds.). ACM, Virtual Event,\
    \ USA, 943\u2013958.\n- <span id=\"page-13-1\"></span>[22] Kurt Hornik, Maxwell\
    \ B. Stinchcombe, and Halbert White. 1989. Multilayer feedforward networks are\
    \ universal approximators. Neural Networks 2, 5 (1989), 359\u2013366.\n- <span\
    \ id=\"page-13-5\"></span>[23] Qijing Huang, Aravind Kalaiah, Minwoo Kang, James\
    \ Demmel, Grace Dinh, John Wawrzynek, Thomas Norell, and Yakun Sophia Shao. 2021.\
    \ CoSA: Scheduling by Constrained Optimization for Spatial Accelerators. In Proceedings\
    \ of the ACM/IEEE Annual International Symposium on Computer Architecture (ISCA).\
    \ IEEE, Valencia, Spain, 554\u2013566.\n- <span id=\"page-13-9\"></span>[24] Drago\
    \ Ignjatovic, Daniel W. Bailey, and Ljubisa Bajic. 2022. The Wormhole AI Training\
    \ Processor. In Proceedings of the IEEE International Solid-State Circuits Conference\
    \ (ISSCC). IEEE, San Francisco, CA, USA, 356\u2013358.\n- <span id=\"page-13-19\"\
    ></span>[25] Abhinav Jangda and Uday Bondhugula. 2018. An effective fusion and\
    \ tile size model for optimizing image processing pipelines. In Proceedings of\
    \ the 23rd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming\
    \ (PPoPP), Andreas Krall and Thomas R. Gross (Eds.). ACM, Vienna, Austria, 261\u2013\
    275.\n- <span id=\"page-13-15\"></span>[26] Yang Jiao, Liang Han, Rong Jin, Yi-Jung\
    \ Su, Chiente Ho, Li Yin, Yun Li, Long Chen, Zhen Chen, Lu Liu, Zhuyu He, Yu Yan,\
    \ Jun He, Jun Mao, Xiaotao Zai, Xuejun Wu, Yongquan Zhou, Mingqiu Gu, Guocai Zhu,\
    \ Rong Zhong, Wenyuan Lee, Ping Chen, Yiping Chen, Weiliang Li, Deyu Xiao, Qing\
    \ Yan, Mingyuan Zhuang, Jiejun Chen, Yun Tian, Yingzi Lin, Wei Wu, Hao Li, and\
    \ Zesheng Dou. 2020. A 12nm Programmable Convolution-Efficient Neural-Processing-Unit\
    \ Chip\n\nAchieving 825TOPS. In Proceedings of the IEEE International Solid-State\
    \ Circuits Conference (ISSCC). IEEE, San Francisco, CA, USA, 136\u2013140.\n\n\
    - [27] Yang Jiao, Liang Han, and Xin Long. 2020. Hanguang 800 NPU The Ultimate\
    \ AI Inference Solution for Data Centers. In Proceedings of the IEEE Hot Chips\
    \ 32 Symposium (HCS). IEEE, Palo Alto, CA, USA, 1\u201329.\n- <span id=\"page-13-16\"\
    ></span>[28] Norman P. Jouppi, Doe Hyun Yoon, Matthew Ashcraft, Mark Gottscho,\
    \ Thomas B. Jablin, George Kurian, James Laudon, Sheng Li, Peter C. Ma, Xiaoyu\
    \ Ma, Thomas Norrie, Nishant Patil, Sushma Prasad, Cliff Young, Zongwei Zhou,\
    \ and David A. Patterson. 2021. Ten Lessons From Three Generations Shaped Google's\
    \ TPUv4i : Industrial Product. In Proceedings of the 48th ACM/IEEE Annual International\
    \ Symposium on Computer Architecture (ISCA). IEEE, Valencia, Spain, 1\u201314.\n\
    - <span id=\"page-13-24\"></span>[29] Sheng-Chun Kao, Xiaoyu Huang, and Tushar\
    \ Krishna. 2022. DNNFuser: Generative Pre-Trained Transformer as a Generalized\
    \ Mapper for Layer Fusion in DNN Accelerators. arXiv preprint arXiv:2201.11218\
    \ abs/2201.11218 (2022), 1\u20138.\n- <span id=\"page-13-18\"></span>[30] Sheng-Chun\
    \ Kao and Tushar Krishna. 2020. GAMMA: Automating the HW Mapping of DNN Models\
    \ on Accelerators via Genetic Algorithm. In Proceedings of the IEEE/ACM International\
    \ Conference On Computer Aided Design (ICCAD). IEEE, San Diego, CA, USA, 44:1\u2013\
    44:9.\n- <span id=\"page-13-21\"></span>[31] Sheng-Chun Kao and Tushar Krishna.\
    \ 2022. MAGMA: An Optimization Framework for Mapping Multiple DNNs on Multiple\
    \ Accelerator Cores. In IEEE International Symposium on High-Performance Computer\
    \ Architecture, (HPCA). IEEE, Seoul, South Korea, 814\u2013830.\n- <span id=\"\
    page-13-6\"></span>[32] Sheng-Chun Kao, Michael Pellauer, Angshuman Parashar,\
    \ and Tushar Krishna. 2022. DiGamma: Domain-aware Genetic Algorithm for HW-Mapping\
    \ Co-optimization for DNN Accelerators. In Proceedings of the Design, Automation\
    \ & Test in Europe Conference & Exhibition (DATE), Cristiana Bolchini, Ingrid\
    \ Verbauwhede, and Ioana Vatajelu (Eds.). IEEE, Antwerp, Belgium, 232\u2013237.\n\
    - <span id=\"page-13-20\"></span>[33] Scott Kirkpatrick, D. Gelatt Jr., and Mario\
    \ P. Vecchi. 1983. Optimization by Simmulated Annealing. Sci. 220, 4598 (1983),\
    \ 671\u2013680.\n- <span id=\"page-13-12\"></span>[34] Simon Knowles. 2017. Scalable\
    \ Silicon Compute. In Workshop on Deep Learning At Supercomputer Scale, NIPS.\
    \ OpenReview.net, Long Beach, CA, USA, 1\u201322.\n- <span id=\"page-13-13\"></span>[35]\
    \ Simon Knowles. 2021. Graphcore. In Proceedings of the IEEE Hot Chips 33 Symposium\
    \ (HCS). IEEE, Palo Alto, CA, USA, 1\u201325.\n- <span id=\"page-13-3\"></span>[36]\
    \ Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. 2012. ImageNet Classification\
    \ with Deep Convolutional Neural Networks. In Proceedings of the 26th Annual Conference\
    \ on Neural Information Processing Systems (NIPS). Curran Associates, Inc., Lake\
    \ Tahoe, Nevada, United States, 1106\u20131114.\n- <span id=\"page-13-7\"></span>[37]\
    \ Hyoukjun Kwon, Prasanth Chatarasi, Michael Pellauer, Angshuman Parashar, Vivek\
    \ Sarkar, and Tushar Krishna. 2019. Understanding Reuse, Performance, and Hardware\
    \ Cost of DNN Dataflow: A Data-Centric Approach. In Proceedings of the IEEE/ACM\
    \ International Symposium on Microarchitecture (MICRO). ACM, Columbus, OH, USA,\
    \ 754\u2013768.\n- <span id=\"page-13-8\"></span>[38] Juhyoung Lee, Dongjoo Shin,\
    \ Jinsu Lee, Jinmook Lee, Sanghoon Kang, and Hoi-Jun Yoo. 2019. A Full HD 60 fps\
    \ CNN Super Resolution Processor with Selective Caching based Layer Fusion for\
    \ Mobile Devices. In Proceedings of the Symposium on VLSI Circuits. IEEE, Kyoto,\
    \ Japan, 302\u2013303.\n- <span id=\"page-13-2\"></span>[39] Grzegorz Lewicki\
    \ and Giuseppe Marino. 2004. Approximation of functions of finite variation by\
    \ superpositions of a Sigmoidal function. Appl. Math. Lett. 17, 10 (2004), 1147\u2013\
    1152.\n- <span id=\"page-13-10\"></span>[40] Heng Liao, Jiajin Tu, Jing Xia, Hu\
    \ Liu, Xiping Zhou, Honghui Yuan, and Yuxing Hu. 2021. Ascend: a Scalable and\
    \ Unified Architecture for Ubiquitous Deep Neural Network Computing : Industry\
    \ Track Paper. In Proceedings of the IEEE International Symposium on High-Performance\
    \ Computer Architecture, HPCA. IEEE, Seoul, South Korea, 789\u2013801.\n- <span\
    \ id=\"page-13-14\"></span>[41] Heng Liao, Jiajin Tu, Jing Xia, and Xiping Zhou.\
    \ 2019. DaVinci: A Scalable Architecture for Neural Network Computing. In Proceedings\n\
    \nCocco: Hardware-Mapping Co-Exploration towards Memory ... ASPLOS '24, April\
    \ 27-May 1, 2024, La Jolla, CA, USA\n\nof the IEEE Hot Chips 31 Symposium (HCS).\
    \ IEEE, Cupertino, CA, USA, 1\u201344.\n\n- <span id=\"page-14-19\"></span>[42]\
    \ Xinhan Lin, Shouyi Yin, Fengbin Tu, Leibo Liu, Xiangyu Li, and Shaojun Wei.\
    \ 2018. LCP: a layer clusters paralleling mapping method for accelerating inception\
    \ and residual networks on FPGA. In Proceedings of the 55th Annual Design Automation\
    \ Conference (DAC). ACM, San Francisco, CA, USA, 16:1\u201316:6.\n- <span id=\"\
    page-14-7\"></span>[43] Wenyan Lu, Guihai Yan, Jiajun Li, Shijun Gong, Yinhe Han,\
    \ and Xiaowei Li. 2017. FlexFlow: A Flexible Dataflow Accelerator Architecture\
    \ for Convolutional Neural Networks. In Proceedings of the IEEE International\
    \ Symposium on High Performance Computer Architecture (HPCA). IEEE Computer Society,\
    \ Austin, TX, USA, 553\u2013564.\n- <span id=\"page-14-8\"></span>[44] Yufei Ma,\
    \ Yu Cao, Sarma B. K. Vrudhula, and Jae-sun Seo. 2017. Optimizing Loop Operation\
    \ and Dataflow in FPGA Acceleration of Deep Convolutional Neural Networks. In\
    \ Proceedings of the ACM/SIGDA International Symposium on Field-Programmable Gate\
    \ Arrays (FPGA). ACM, Monterey, CA, USA, 45\u201354.\n- <span id=\"page-14-0\"\
    ></span>[45] Marvin Minsky and Seymour Papert. 1987. Perceptrons - an introduction\
    \ to computational geometry. MIT Press, .\n- <span id=\"page-14-24\"></span>[46]\
    \ Bert Moons, Roel Uytterhoeven, Wim Dehaene, and Marian Verhelst. 2017. Envision:\
    \ A 0.26-to-10TOPS/W subword-parallel dynamicvoltage-accuracy-frequency-scalable\
    \ Convolutional Neural Network processor in 28nm FDSOI. In Proceedings of the\
    \ IEEE International Solid-State Circuits Conference (ISSCC). IEEE, San Francisco,\
    \ CA, USA, 246\u2013247.\n- <span id=\"page-14-21\"></span>[47] Ravi Teja Mullapudi,\
    \ Andrew Adams, Dillon Sharlet, Jonathan Ragan-Kelley, and Kayvon Fatahalian.\
    \ 2016. Automatically scheduling halide image processing pipelines. ACM Trans.\
    \ Graph. 35, 4 (2016), 83:1\u2013 83:11.\n- <span id=\"page-14-12\"></span>[48]\
    \ Thomas Norrie, Nishant Patil, Doe Hyun Yoon, George Kurian, Sheng Li, James\
    \ Laudon, Cliff Young, Norman P. Jouppi, and David A. Patterson. 2020. Google's\
    \ Training Chips Revealed: TPUv2 and TPUv3. In Proceedings of the IEEE Hot Chips\
    \ 32 Symposium (HCS). IEEE, Palo Alto, CA, USA, 1\u201370.\n- <span id=\"page-14-16\"\
    ></span>[49] NVIDIA. 2018. THE NVIDIA DEEP LEARNING ACCELERATOR. In Proceedings\
    \ of the IEEE Hot Chips 30 Symposium (HCS). IEEE, Cupertino, CA, USA, 1\u2013\
    18.\n- <span id=\"page-14-18\"></span>[50] Angshuman Parashar, Priyanka Raina,\
    \ Yakun Sophia Shao, Yu-Hsin Chen, Victor A. Ying, Anurag Mukkara, Rangharajan\
    \ Venkatesan, Brucek Khailany, Stephen W. Keckler, and Joel S. Emer. 2019. Timeloop:\
    \ A Systematic Approach to DNN Accelerator Evaluation. In Proceedings of the IEEE\
    \ International Symposium on Performance Analysis of Systems and Software (ISPASS).\
    \ IEEE, Madison, WI, USA, 304\u2013315.\n- <span id=\"page-14-25\"></span>[51]\
    \ Angshuman Parashar, Minsoo Rhu, Anurag Mukkara, Antonio Puglielli, Rangharajan\
    \ Venkatesan, Brucek Khailany, Joel S. Emer, Stephen W. Keckler, and William J.\
    \ Dally. 2017. SCNN: An Accelerator for Compressed-sparse Convolutional Neural\
    \ Networks. In Proceedings of the 44th Annual International Symposium on Computer\
    \ Architecture (ISCA). ACM, Toronto, ON, Canada, 27\u201340.\n- <span id=\"page-14-23\"\
    ></span>[52] Alec Radford and Karthik Narasimhan. 2018. Improving Language Understanding\
    \ by Generative Pre-Training. In Preprint. OpenAI, , 1\u2013 12.\n- <span id=\"\
    page-14-5\"></span>[53] Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc V.\
    \ Le. 2019. Regularized Evolution for Image Classifier Architecture Search. In\
    \ Proceedings of the 33rd Conference on Artificial Intelligence (AAAI). AAAI Press,\
    \ Honolulu, Hawaii, USA, 4780\u20134789.\n- <span id=\"page-14-1\"></span>[54]\
    \ Frank Rosenblatt. 1957. The perceptron, a perceiving and recognizing automaton\
    \ Project Para. Cornell Aeronautical Laboratory, .\n- <span id=\"page-14-4\"></span>[55]\
    \ Mark Sandler, Andrew G. Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh\
    \ Chen. 2018. MobileNetV2: Inverted Residuals and Linear Bottlenecks. In Proceedings\
    \ of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Computer\
    \ Vision Foundation / IEEE Computer Society, Salt Lake City, UT, USA, 4510\u2013\
    4520.\n- <span id=\"page-14-20\"></span>[56] Yakun Sophia Shao, Jason Clemons,\
    \ Rangharajan Venkatesan, Brian Zimmer, Matthew Fojtik, Nan Jiang, Ben Keller,\
    \ Alicia Klinefelter, Nathaniel Pinckney, Priyanka Raina, Stephen G. Tell, Yanqing\
    \ Zhang, William J. Dally, Joel Emer, C. Thomas Gray, Brucek Khailany, and Stephen\
    \ W. Keckler. 2019. Simba: Scaling Deep-Learning Inference with Multi-Chip-Module-Based\
    \ Architecture. In Proceedings of the IEEE/ACM International Symposium on Microarchitecture\
    \ (MICRO). ACM, Columbus, OH, USA, 14\u201327.\n- <span id=\"page-14-2\"></span>[57]\
    \ Karen Simonyan and Andrew Zisserman. 2015. Very Deep Convolutional Networks\
    \ for Large-Scale Image Recognition. In Proceedings of the International Conference\
    \ on Learning Representations (ICLR). Computational and Biological Learning Society,\
    \ San Diego, CA, USA, 1\u201314.\n- <span id=\"page-14-26\"></span>[58] Zhuoran\
    \ Song, Bangqi Fu, Feiyang Wu, Zhaoming Jiang, Li Jiang, Naifeng Jing, and Xiaoyao\
    \ Liang. 2020. DRQ: Dynamic Region-based Quantization for Deep Neural Network\
    \ Acceleration. In Proceedings of the 47th ACM/IEEE Annual International Symposium\
    \ on Computer Architecture (ISCA). IEEE, Valencia, Spain, 1010\u20131021.\n- <span\
    \ id=\"page-14-3\"></span>[59] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre\
    \ Sermanet, Scott E. Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke,\
    \ and Andrew Rabinovich. 2015. Going deeper with convolutions. In Proceedings\
    \ of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE\
    \ Computer Society, Boston, MA, USA, 1\u20139.\n- <span id=\"page-14-13\"></span>[60]\
    \ Emil Talpes, Douglas Williams, and Debjit Das Sarma. 2022. DOJO: The Microarchitecture\
    \ of Tesla's Exa-Scale Computer. In Proceedings of the IEEE Hot Chips 34 Symposium\
    \ (HCS). IEEE, Cupertino, CA, USA, 1\u201328.\n- <span id=\"page-14-9\"></span>[61]\
    \ Zhanhong Tan, Hongyu Cai, Runpei Dong, and Kaisheng Ma. 2021. NN-Baton: DNN\
    \ Workload Orchestration and Chiplet Granularity Exploration for Multichip Accelerators.\
    \ In Proceedings of the IEEE Annual International Symposium on Computer Architecture\
    \ (ISCA). IEEE, Valencia, Spain, 1013\u20131026.\n- <span id=\"page-14-27\"></span>[62]\
    \ Jakub Tarnawski, Amar Phanishayee, Nikhil R. Devanur, Divya Mahajan, and Fanny\
    \ Nina Paravecino. 2020. Efficient Algorithms for Device Placement of DNN Graph\
    \ Operators. In Advances in Neural Information Processing Systems (NeurIPS), Hugo\
    \ Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien\
    \ Lin (Eds.). Open-Review.net, Virtual, 1\u201313.\n- <span id=\"page-14-14\"\
    ></span>[63] Tenstorrent. 2021. Grayskull. <https://tenstorrent.com/grayskull/>.\n\
    - <span id=\"page-14-22\"></span>[64] Ashish Vaswani, Noam Shazeer, Niki Parmar,\
    \ Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin.\
    \ 2017. Attention is All you Need. In Advances in Neural Information Processing\
    \ Systems (NIPS), Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach,\
    \ Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (Eds.). OpenReview.net,\
    \ Long Beach, CA, USA, 5998\u20136008.\n- <span id=\"page-14-17\"></span>[65]\
    \ Ofri Wechsler, Michael Behar, and Bharat Daga. 2019. Spring Hill (NNP-I 1000)\
    \ Intel's Data Center Inference Chip. In Proceedings of the IEEE Hot Chips 31\
    \ Symposium (HCS). IEEE, Cupertino, CA, USA, 1\u201312.\n- <span id=\"page-14-10\"\
    ></span>[66] Jian Weng, Sihao Liu, Vidushi Dadu, Zhengrong Wang, Preyas Shah,\
    \ and Tony Nowatzki. 2020. DSAGEN: Synthesizing Programmable Spatial Accelerators.\
    \ In Proceedings of the 47th ACM/IEEE Annual International Symposium on Computer\
    \ Architecture (ISCA). IEEE, Valencia, Spain, 268\u2013281.\n- <span id=\"page-14-11\"\
    ></span>[67] Qingcheng Xiao, Size Zheng, Bingzhe Wu, Pengcheng Xu, Xuehai Qian,\
    \ and Yun Liang. 2021. HASCO: Towards Agile HArdware and Software CO-design for\
    \ Tensor Computation. In Proceedings of the 48th ACM/IEEE Annual International\
    \ Symposium on Computer Architecture (ISCA). IEEE, Valencia, Spain, 1055\u2013\
    1068.\n- <span id=\"page-14-6\"></span>[68] Saining Xie, Alexander Kirillov, Ross\
    \ B. Girshick, and Kaiming He. 2019. Exploring Randomly Wired Neural Networks\
    \ for Image Recognition. In Proceedings of the IEEE/CVF International Conference\
    \ on Computer Vision (ICCV). IEEE, Seoul, South Korea, 1284\u20131293.\n- <span\
    \ id=\"page-14-15\"></span>[69] Andrew Yang. 2019. Deep Learning Training At Scale\
    \ Spring Crest Deep Learning Accelerator (Intel\xAE Nervana\u2122 NNP-T). In Proceedings\
    \ of the IEEE Hot Chips 31 Symposium (HCS). IEEE, Cupertino, CA, USA,\n\n<span\
    \ id=\"page-15-0\"></span>1\u201320.\n\n- <span id=\"page-15-2\"></span>[70] Xuan\
    \ Yang, Mingyu Gao, Qiaoyi Liu, Jeff Setter, Jing Pu, Ankita Nayak, Steven Bell,\
    \ Kaidi Cao, Heonjae Ha, Priyanka Raina, Christos Kozyrakis, and Mark Horowitz.\
    \ 2020. Interstellar: Using Halide's Scheduling Language to Analyze DNN Accelerators.\
    \ In Proceedings of the International Conference on Architectural Support for\
    \ Programming Languages and Operating Systems (ASPLOS). ACM, Lausanne, Switzerland,\
    \ 369\u2013383.\n- <span id=\"page-15-6\"></span>[71] Size Zheng, Renze Chen,\
    \ Anjiang Wei, Yicheng Jin, Qin Han, Liqiang Lu, Bingyang Wu, Xiuhong Li, Shengen\
    \ Yan, and Yun Liang. 2022. AMOS: enabling automatic mapping for tensor computations\
    \ on spatial accelerators with hardware abstraction. In Proceedings of the 49th\
    \ Annual International Symposium on Computer Architecture (ISCA). ACM, New York,\
    \ New York, USA, 874\u2013887.\n- <span id=\"page-15-4\"></span>[72] Shixuan Zheng,\
    \ Xianjue Zhang, Leibo Liu, Shaojun Wei, and Shouyi Yin. 2022. Atomic Dataflow\
    \ based Graph-Level Workload Orchestration for Scalable DNN Accelerators. In Proceedings\
    \ of the IEEE International Symposium on High-Performance Computer Architecture\
    \ (HPCA). IEEE, Seoul, South Korea, 475\u2013489.\n- <span id=\"page-15-3\"></span>[73]\
    \ Shixuan Zheng, Xianjue Zhang, Daoli Ou, Shibin Tang, Leibo Liu, Shaojun Wei,\
    \ and Shouyi Yin. 2020. Efficient Scheduling of Irregular Network Structures on\
    \ CNN Accelerators. IEEE Transactions on Computer-Aided Design of Integrated Circuits\
    \ and Systems (TCAD) 39, 11 (2020), 3408\u20133419.\n- <span id=\"page-15-5\"\
    ></span>[74] Brian Zimmer, Rangharajan Venkatesan, Yakun Sophia Shao, Jason Clemons,\
    \ Matthew Fojtik, Nan Jiang, Ben Keller, Alicia Klinefelter, Nathaniel Ross Pinckney,\
    \ Priyanka Raina, Stephen G. Tell, Yanqing Zhang, William J. Dally, Joel S. Emer,\
    \ C. Thomas Gray, Stephen W. Keckler, and Brucek Khailany. 2019. A 0.11 pJ/Op,\
    \ 0.32-128 TOPS, Scalable Multi-Chip-Module-based Deep Neural Network Accelerator\
    \ with Ground-Reference Signaling in 16nm. In Proceedings of the IEEE Symposium\
    \ on VLSI Circuits (VLSI). IEEE, Kyoto, Japan, 300.\n- <span id=\"page-15-1\"\
    ></span>[75] Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V. Le. 2018.\
    \ Learning Transferable Architectures for Scalable Image Recognition. In IEEE\
    \ Conference on Computer Vision and Pattern Recognition, (CVPR). Computer Vision\
    \ Foundation / IEEE Computer Society, Salt Lake City, UT, USA, 8697\u20138710."
  paper_id: 17
  scores:
    originality:
      raw: 4.2
      reason: The paper presents a novel approach to utilizing RISC-V architecture
        in the context of terahertz IoT devices, which is a relatively unexplored
        area.
    technical_soundness:
      raw: 4.0
      reason: The methods and designs discussed appear to be well-founded, though
        some technical details may require further elaboration for full validation.
    clarity:
      raw: 3.5
      reason: While the introduction outlines the context and importance of the research,
        the writing could benefit from improved organization and clearer explanations
        of complex concepts.
    significance:
      raw: 4.1
      reason: The implications of this research for future IoT applications and network
        efficiency are substantial, addressing a critical need in the field.
    relevance:
      raw: 4.5
      reason: The topic is highly relevant to current trends in IoT and wireless communication,
        making it significant for both academia and industry.
  total_score: 4.01
  tier: C
