papers:
- title: A modular architecture for IMU-based data gloves
  abstract: ''
  keywords: ''
  document: '# A modular architecture for IMU-based data gloves


    Alessandro Carfì [,](https://orcid.org/0000-0001-9208-6910) Mohamad Alame[h](https://orcid.org/0000-0002-6345-8313)
    , Valerio Belcamino [,](https://orcid.org/0000-0002-9264-8191) and Fulvio Mastrogiovann[i](https://orcid.org/0000-0001-5913-1898)


    Department of Informatics, Bioengineering, Robotics, and Systems Engineering,
    University of Genoa, Via Opera Pia 13, 16145 Genoa, Italy alessandro.carfi@dibris.unige.it


    Abstract. The flexibility and range of motion in human hands play a crucial role
    in human interaction with the environment and have been studied across different
    fields. Researchers explored various technological solutions for gathering information
    from the hands. These solutions include tracking hand motion through cameras or
    wearable sensors and using wearable sensors to measure the position and pressure
    of contact points. Data gloves can collect both types of information by utilizing
    inertial measurement units, flex sensors, magnetic trackers for motion tracking,
    and force resistors or touch sensors for contact measurement. Although there are
    commercially available data gloves, researchers often create custom data gloves
    to achieve the desired flexibility and control over the hardware. However, the
    existing literature lacks standardization and the reuse of previously designed
    data gloves. As a result, many gloves with unclear characteristics exist, which
    makes replication challenging and negatively impacts the reproducibility of studies.
    This work proposes a modular, open hardware and software architecture for creating
    customized data gloves based on IMU technology. We also provide an architecture
    implementation along with an experimental protocol to evaluate device performance.


    Keywords: Data Glove · Hand Tracking · Inertial Measuerment Unit.


    ## 1 Introduction


    Human hands'' flexibility and range of motion are fundamental for how humans interact
    with each other and the world. Many research communities have studied human hand
    motion, focusing on the hand''s motion and its interaction with the environment
    [\[1\]](#page-4-0). Hand studies typically include information about hand kinematics
    and sensory input. Hand kinematics are well-described, with minimal variation
    among individuals, except for bone proportions [\[3\]](#page-4-1). On the other
    hand, humans primarily rely on two senses when using their hands: proprioception
    and touch. Proprioception helps determine limb position, while touch provides
    information about forces and points of contact. Depending on the type of study,
    researchers may require information from one or both of the hand senses. As


    #### 2 A. Carfì et al.


    ![](_page_1_Figure_1.jpeg)


    <span id="page-1-0"></span>Fig. 1. The image compares the architecture on the
    left with the implementation on the right. The core module is in blue, while the
    sensory module is in red.


    a result, various technological solutions have been explored. In terms of proprioception,
    tracking the hand''s motion has been achieved through the use of cameras or wearable
    sensors. Instead, for touch, although a few attempts have been made to estimate
    them using cameras [\[5\]](#page-4-2), the position and pressure of contacts are
    primarily measured using wearable sensors. The data glove is a wearable device
    embedding sensors that can collect all the previously defined information [\[4\]](#page-4-3).
    The sensors embedded in a data glove can be adapted to meet the application requirements.
    Inertial Measurement Units (IMU) [\[7\]](#page-4-4), Flex sensors [\[8\]](#page-4-5),
    and magnetic trackers [\[2\]](#page-4-6) are the most common choices for tracking
    motion, while force sensing resistors [\[10\]](#page-4-7) or touch sensors can
    be used to measure contacts [\[9\]](#page-4-8). While various data gloves are
    commercially available [\[6\]](#page-4-9), researchers often opt to construct
    custom data gloves to achieve the desired flexibility and control over the hardware.
    However, the existing literature lacks standardization and the re-use of previously
    designed devices. As a result, many gloves with unclear characteristics have been
    developed, making replication challenging and affecting the reproducibility of
    the studies for which they were created. This work aims to introduce a modular,
    open hardware and software architecture for creating customized data gloves based
    on IMU technology.


    ## 2 Hardware Architecture


    To provide maximum flexibility, a data glove should support a variable IMUs number
    to adapt to the application''s requirements, work without needing external equipment,
    and be easy to repair and reproduce. Our architecture addresses these requirements
    by defining two modules, displayed in Figure [1:](#page-1-0) core and sensory
    modules. The core module includes the MCU for data collection and processing,
    an antenna to send data to a PC, interfaces for connecting other modules, a battery
    for power and an IMU. The sensory module contains an IMU and the communication
    interface with the core module. Each module has a structure that includes the
    basic functionalities of an IMU data glove. However, each component can be expanded
    to incorporate more functionalities while keeping the main structure intact. The
    only technical constraint is the communication interface, which should remain
    fixed to ensure compatibility across


    ![](_page_2_Figure_1.jpeg)


    <span id="page-2-0"></span>Fig. 2. On the left, we have the dataglove worn by
    a human, and on the right, it is mounted on the AR10 Robotic hand.


    different implementations of this architecture. The communication interface has
    two key components: the communication protocol and the physical medium. The two
    most commonly adopted communication protocols for digital sensors are Serial Peripheral
    Interface (SPI) and Inter Integrated Circuit (I2C). SPI is typically used for
    communication between components on the printed circuit board (PCB). It is more
    vulnerable to noise and requires more communication lanes. Instead, I2C allows
    the same physical lane sharing across multiple devices without additional selection
    lanes and is less affected by noise. Therefore, we have chosen I2C as the communication
    protocol for the modules in our hardware architecture. Instead, in the literature,
    solutions for the physical medium often involved soldering cables or using flexible
    PCBs. However, these options make it hard to modify the number of sensors and
    increase the complexity of reproducing and repairing the device. To solve these
    issues, we opted for using a Flexible Flat Cable (FFC) connector as the physical
    interface for the modules and FFC cables to connect them. This solution allows
    easy addition of extra sensory modules or replacement of faulty modules. Additionally,
    FFC cables are flexible and do not restrict hand motion.


    ## 3 Implementation


    The implementation of the two modules can be seen in Figure [1](#page-1-0) while
    the device is pictured in Figure [2.](#page-2-0) The sensory module design includes
    a SparkFun IMU Breakout, which embeds the MPU-9250 from InvenSense, and a custom-designed
    shield to support FFC connection and daisy chaining. Although the I2C protocol
    only requires four lanes (ground, power, SCL, and SDA), we opted for a six-lane
    FFC connector and cable for future expansion. Each I2C lane can connect only two
    sensors since the MPU-9250 only offers two selectable addresses. The two sensory
    modules of each I2C lane can be worn on the proximal and intermediate phalanges
    to monitor the motion of a single finger. Instead, the core module is


    #### 4 A. Carfì et al.


    placed on the back of the hand and consists of a custom PCB to ensure a compact
    design. This PCB includes an ESP32 with a WiFi antenna, 7 FFC connectors, an MPU-9250
    IMU, and an I2C multiplexer. Each finger requires a separate I2C lane for motion
    tracking, so we allocated one FFC connector for each lane. For symmetry and compatibility
    with both right and left hands, we included two connectors in symmetric positions
    for the thumb. Since the ESP32 has only two I2C controllers, but the system requires
    6 I2C lanes (one for each finger and one for the hand back), the design incorporates
    an I2C multiplexer to manage all the communication lanes. Finally, the core module
    also includes a connector for battery power. Our software consists of two modules.
    The first module runs on the ESP32 and collects data from connected sensors, sending
    them via UDP communication. The data includes the sensor''s unique ID, accelerometer
    and gyroscope readings, and orientations in quaternions. The ESP32 uses the I2C
    multiplexer to collect data from each sensor module. The sensor orientation is
    estimated using a data fusion process run by the MPU-9250''s digital motion processor.
    The second module runs on the PC and receives sensory data through UDP communication,
    acting as a driver.


    ## 4 Results


    The purpose of the experimental setup is to demonstrate the general functionalities
    of the data glove. For maximum reproducibility and accuracy, we installed our
    device on an AR10 hand from Active8, mounted on the Baxter manipulator from Rethink
    Robotics, see Figure [2.](#page-2-0) The first test assessed the autonomy and
    acquisition frequencies of the data glove under static conditions. Equipped with
    eleven IMUs and powered by a 220 mAh 3.7V battery, the glove had an average autonomy
    of 62.89 minutes (SD = 4.89) and transmitted data with a frequency of 21.8 Hz
    (SD = 9.47) across six independent tests. In the same static conditions, we measured
    the drifting of the sensors'' estimated orientations over time. The root mean
    square error (RMSE), averaged across all sensors, was 8.91 degrees (SD = 3.89)
    after 30 minutes. We also conducted experiments involving random movements of
    the robot''s hand and arm. Each experiment lasted 45 minutes and was repeated
    five times. The overall RMSE averaged across all sensors and trials was 9.17 degrees
    (STD = 9.30).


    ## 5 Conclusions


    This article presents a modular architecture for an IMU-based dataglove and its
    early implementation. The device, equipped with a small battery, can transmit
    data from eleven sensors at a frequency higher than 20Hz for over an hour. Furthermore,
    tests conducted under unfavourable conditions, without proper calibration or drifting
    compensation, demonstrated a reasonably accurate tracking of motions. The error
    in dynamic conditions is not significantly different from that in stationary conditions,
    as shown in the result sections. This result suggests that most tracking errors
    are due to sensor drifting, which can be compensated for with appropriate software
    solutions. The proposed device represents an initial attempt to provide an easily
    reproducible and modular platform for IMU-based hand tracking. Its extensibility
    offers opportunities for future research to propose new versions or develop more
    accurate tracking software solutions.


    Acknowledgments. This work is supported by the CHIST-ERA (2014-2020) project InDex
    and received funding from the Italian Ministry of Education and Research (MIUR).
    This work has been also made with the Italian government support under the National
    Recovery and Resilience Plan (NRRP), Mission 4, Component 2 Investment 1.5, funded
    from the European Union NextGenerationEU.


    Disclosure of Interests. The authors have no competing interests to declare that
    are relevant to the content of this article.


    ## References


    - <span id="page-4-0"></span>1. Carfì, A., Patten, T., Kuang, Y., Hammoud, A.,
    Alameh, M., Maiettini, E., Weinberg, A.I., Faria, D., Mastrogiovanni, F., Alenyà,
    G., et al.: Hand-object interaction: From human demonstrations to robot manipulation.
    Frontiers in Robotics and AI 8, 714023 (2021)

    - <span id="page-4-6"></span>2. Cazacu, E., van der Grinten, C., Bax, J., Baeten,
    G., Holtkamp, F., Lee, C.: A position sensing glove to aid ankle-foot orthosis
    diagnosis and treatment. Sensors 21(19), 6631 (2021)

    - <span id="page-4-1"></span>3. Cobos, S., Ferre, M., Uran, M.S., Ortego, J.,
    Pena, C.: Efficient human hand kinematics for manipulation tasks. In: 2008 IEEE/RSJ
    International Conference on Intelligent Robots and Systems. pp. 2246–2251. IEEE
    (10 2008)

    - <span id="page-4-3"></span>4. Dipietro, L., Sabatini, A.M., Dario, P.: A survey
    of glove-based systems and their applications. Ieee transactions on systems, man,
    and cybernetics, part c (applications and reviews) 38(4), 461–482 (2008)

    - <span id="page-4-2"></span>5. Grady, P., Tang, C., Brahmbhatt, S., Twigg, C.D.,
    Wan, C., Hays, J., Kemp, C.C.: Pressurevision: Estimating hand pressure from a
    single rgb image. In: European Conference on Computer Vision. pp. 328–345. Springer
    (2022)

    - <span id="page-4-9"></span>6. He, K., Choosri, N.: Commercial data glove selection
    for vr-based hand rehabilitation gaming project. In: 2023 Joint International
    Conference on Digital Arts, Media and Technology with ECTI Northern Section Conference
    on Electrical, Electronics, Computer and Telecommunications Engineering (ECTI
    DAMT & NCON). pp. 177–182. IEEE (2023)

    - <span id="page-4-4"></span>7. Huang, H., Liang, Z., Sun, F., Dong, M., et al.:
    Virtual interaction and manipulation control of a hexacopter through hand gesture
    recognition from a data glove. Robotica 40(12), 4375–4387 (2022)

    - <span id="page-4-5"></span>8. Luo, Y., Chen, X., Li, X., Tian, H., Li, S., Wang,
    L., He, J., Yang, Z., Shao, J.: Heterogeneous strain distribution based programmable
    gated microchannel for ultrasensitive and stable strain sensing. Advanced Materials
    p. 2207141 (2022)

    - <span id="page-4-8"></span>9. Maiolino, P., Mastrogiovanni, F., Cannata, G.,
    et al.: Skinning a robot: Design methodologies for large-scale robot skin. IEEE
    Robotics & Automation Magazine 23(4), 150–159 (2016)

    - <span id="page-4-7"></span>10. Wang, J., Li, B., Li, Z., Zubrycki, I., Granosik,
    G.: Grasping behavior of the human hand during tomato picking. Computers and Electronics
    in Agriculture 180, 105901 (2021)'
- title: 'WideSA: A High Array Utilization Mapping Scheme for Uniform Recurrences
    on ACAP'
  abstract: ''
  keywords: Mapping, Re-configurable Array Architecture, Versal ACAP
  document: '#### I. INTRODUCTION


    Modern heterogeneous FPGA architectures, like AMD/Xilinx Versal Adaptive Compute
    Acceleration Platform (ACAP) [\[1\]](#page-5-0), combine AI Engines (AIEs) with
    programmable logic (PL) to boost applications in the AI and intelligent signal
    processing domains. In these domains, uniform recurrences [\[2\]](#page-5-1),
    which comprise nested loops with uniform dependencies, are prevalent types of
    computations. Regrettably, there is currently a lack of established development
    methodologies for efficiently mapping large-scale uniform recurrences onto the
    Versal ACAP architecture with high utilization of AI Engines.


    The ACAP architecture comprise an array of several hundred AIE cores, such as
    8×50 in the VC1902 architecture [\[3\]](#page-5-2), interconnected through a mesh
    network-on-chip (NoC). Each AIE core consists of vector processing and load/store
    units, functioning as a very-long-instruction-word (VLIW) [\[4\]](#page-5-3) processor
    to deliver high-performance vectorized computations. To facilitate communication
    among the AIE cores, the NoC is utilized for inter-core communication, enabling
    efficient data transfers between cores. Moreover, neighboring cores utilize shared
    buffers, providing higher bandwidth for data exchange. When it comes to data transfer
    to and from the AIEs, there are hundreds of I/O ports available, supporting terabytes
    of bandwidth.


    As ACAP demonstrates a remarkable capacity for intense computation, developing
    acceleration designs on the architecture has become an urgent trend in recent
    times. However, current efforts have not succeeded in achieving high utilization
    of the AIE array. For example, Vitis-AI [\[5\]](#page-5-4) introduces the DPU
    [\[6\]](#page-5-5) for the VC1902 architecture, but only accomplishes a 64% AIE
    utilization. There are several ongoing challenges associated with developing designs
    with high array utilization on the Versal ACAP architecture:


    - Increased programming complexity: Higher AIE utilization results in more cores
    that need to be programmed with certain intrinsics. In some situations, different
    cores execute different programs, necessitating significant human effort to develop
    such accelerators.

    - Increased placement and routing difficulty: Mapping computations onto the Versal
    ACAP architecture with high utilization of AIEs often necessitates careful placement
    and routing of AIEs and data communications. From the perspective of AIE compilation,
    attaining high AIE utilization typically results in difficulties in placing cores
    and buffers, as well as routing streaming communications on the NoC. For example,
    CHARM [\[7\]](#page-5-6) struggles to compile large designs on Vitis 2022.1.

    - Extended compilation time: The default compilation tools provided by AMD/Xilinx
    Vitis employ ILP algorithms to find placement and routing solutions. Consequently,
    a larger number of cores results in a longer time to find a legal solution.


    To address these challenges, we propose WideSA, a high array utilization mapping
    scheme for uniform recurrences on the Versal ACAP architecture. By leveraging
    the AIE array architecture, we apply space-time transformation and loop nest transformation
    using the polyhedral model, generating systolic-like mappings on the AIE array.
    On one hand, systolic designs assign similar workloads to different cores, enabling
    us to reuse a single core program and thereby reduce human


    <span id="page-1-0"></span>![](_page_1_Figure_0.jpeg)


    Fig. 1: Versal ACAP Architecture


    effort. On the other hand, systolic designs regularize both the placement and
    communication of cores, simplifying the placement and routing process. Additionally,
    we designed a routingaware PLIO assignment algorithm to improve the success rate
    of compilation. We also developed an automatic framework to generate the corresponding
    code for heterogeneous backends, including AIEs, PL, and host. In the evaluation
    section, we demonstrate the effectiveness of WideSA by successfully implementing
    executable acceleration systems for various uniform recurrences, accommodating
    different data types. Our approach achieves high throughput with high utilization
    of AIEs.


    We summarize our contributions as follows:


    - We propose a mapping scheme, based on the polyhedral model, for uniform recurrences
    that generates a systolic design on ACAP with high AIE utilization.

    - We design a routing-aware PLIO assignment algorithm that takes into account
    the characteristics of systolic mappings and the AIE architecture, thereby facilitating
    an efficient compilation process.

    - We develop an automatic framework that generates corresponding code for heterogeneous
    backends based on the mapping results.

    - We achieve high throughput across different computations and data types, outperforming
    state-of-the-art methods.


    #### II. BACKGROUND


    #### *A. Versal ACAP Architecture and Workflow*


    *1) Hardware Features:* AMD/Xilinx has developed the Versal ACAP architecture
    to cater to the increasing demands of next-generation wireless processing and
    machine learning applications. Figure [1](#page-1-0) illustrates the detailed
    architecture of VCK5000, an evaluation kit for Versal ACAP, comprising the CPU,
    PL, and AIE components. The AIE array on VCK5000 consists of 8 × 50 AIE cores,
    with each core capable of generating 128 MACs of int8 data type every cycle at
    a frequency of 1 GHz or higher. Moreover, the AIE cores operate in single-instruction-multiple-data
    (SIMD) mode using


    <span id="page-1-1"></span>TABLE I: Different Data Communication Bandwidth on
    the Versal ACAP Architecture


    | Methods        | Frequency | Bitwidth | Channels | Total      |

    |----------------|-----------|----------|----------|------------|

    | AIE DMA        | 1.25 GHz  | 256 bits | 400      | 15.6 TB/s  |

    | AIE NoC Stream | 1.25 GHz  | 32 bits  | 400      | 1.95 TB/s  |

    | PLIO-PL        | 1.25 GHz  | 128 bits | 78       | 1.52 TB/s  |

    | GMIO-DRAM      | 1.25 GHz  | 64 bits  | 16       | 0.125 TB/s |

    | PL-DRAM        | 0.50 GHz  | -        | 4        | 0.100 TB/s |


    a VLIW pattern, enabling acceleration of a large number of vectorized computations.


    In Figure [1,](#page-1-0) we identify five data transfer methods, including those
    within the AIE array and among the AIE, PL, and DRAM components. These methods
    are referred to as AIE DMA, AIE NoC stream, PLIO-PL, PL-DRAM, and GMIO-DRAM interfaces.
    We profile these data transfer methods on VCK5000 and present the results in Table
    [I.](#page-1-1) Within the AIE array, each AIE core has direct memory access (DMA)
    ports connected to four neighboring local buffers with a width of 256 bits. Using
    the AIE DMA method, a total data transfer rate of up to 15.6 TB/s can be achieved.
    Furthermore, each AIE core is linked to the NoC through a stream interface with
    a width of 32 bits. The data transfer bandwidth through the AIE NoC stream method
    reaches a maximum of 2 TB/s, which is lower compared to the DMA method. The PLIO
    ports, responsible for data communication between the PL and AIE array, can provide
    a maximum bandwidth of 1.52 TB/s. Based on the profiling results, utilizing the
    AIE DMA method for data transfer proves beneficial in overcoming communication
    bottlenecks, aligning with the dataflow in systolic array designs. In terms of
    data communication with DRAM, the bandwidth is approximately 0.1 TB/s, significantly
    lower than the on-chip data transfer methods. This observation inspires us to
    exploit data locality within computations to enhance overall performance.


    *2) Software Programming Model:* AMD/Xilinx offers a development tool for AIEs
    and Versal ACAP integrated into Vitis. The programming model [\[8\]](#page-5-7)
    designed for AIEs consists of two levels: a graph program across the AIE array
    with each node representing an AIE kernel program. The graph program represents
    the dataflow information among AIE kernels and between the AIE and I/O ports.
    The compiler in Vitis transforms the dataflow graph into a subnetwork of physical
    AIE cores, determines the placement of buffers, and configures NoC stream routing.
    Since placement and routing are NP-hard problems, the compiler employs ILP solvers
    to process these two phases. However, as the design scale increases and AIE utilization
    becomes high, finding a legal solution efficiently becomes challenging for the
    solvers [\[9\]](#page-5-8). To address this, incorporating constraints for placement
    and routing helps alleviate the congestions and accelerates the solvers in finding
    solutions. The systolic design scheme provides a regular pattern for placement
    and routing, which is suitable for constructing these constraints.


    ## *B. Uniform Recurrences and Systolic Array Mapping*


    Uniform recurrences refer to computations that consist of nested loops, where
    all dependencies are uniform. These types of computations are commonly found in
    AI and signal processing applications, such as matrix multiplication, 2D convolution,
    FIR filtering, and so on. Several prior works [\[10\]](#page-5-9)– [\[12\]](#page-5-10)
    have focused on generating systolic array designs for uniform recurrences on FPGAs,
    employing the polyhedral model for loop transformations to explore successful
    mappings. The polyhedral model [\[13\]](#page-5-11), [\[14\]](#page-5-12) serves
    as a compilation framework for loop transformation, encompassing space-time transformation,
    latency hiding, SIMD vectorization, fusion, and more. A legal combination of these
    transformations represents a schedule within the polyhedral model, and the goal
    of systolic design mapping is to find the optimal schedule.


    An AIE kernel handles more computations compared to a PE in typical systolic arrays.
    Additionally, specific hardware features of the AIE array differ from those of
    common systolic arrays. As a result, the mapping problem on the Versal ACAP architecture
    is not a straightforward systolic array mapping. Consequently, it is necessary
    to model corresponding transformations and constraints within the polyhedral model,
    an area that has not yet been extensively researched.


    ## III. SYSTOLIC MAPPING SCHEME ON ACAP


    #### *A. Kernel Scope Demarcation*


    According to the programming model of AIEs, it is necessary to demarcate the scope
    of codes mapped to execute on a single AIE core and the outer loop nests to be
    mapped to the AIE array. This demarcation allows us to decompose the mapping problem
    into graph-level mapping and kernel-level mapping, which are independent of each
    other after selecting tiling factors.


    Polygonal tiling [\[15\]](#page-5-13), [\[16\]](#page-5-14), an effective solution
    for workload partitioning in uniform polyhedral domains, plays a crucial role
    in determining the innermost and outer loop nests for tiling. We illustrate the
    tiling process using the MM example with (N0, M0, K0) as the tiling factors, involving
    loop re-indexing, tiling, and rewriting, as depicted in Figure [2.](#page-2-0)
    Building on prior works, we consider the specific features of the AIE array when
    performing the demarcation.


    #### *B. Systolic Mapping Generation*


    To generate systolic array designs on the AIE array following kernel scope demarcation,
    we utilize the polyhedral


    <span id="page-2-0"></span>![](_page_2_Figure_9.jpeg)


    Fig. 2: Kernel Scope Demarcation


    <span id="page-2-1"></span>![](_page_2_Figure_11.jpeg)


    Fig. 3: Polyhedral Model-Based Systolic Mapping


    model, drawing inspiration from AutoSA [\[11\]](#page-5-15), to facilitate loop
    transformations. To be specific, we employ four types of transformation techniques,
    as depicted in Figure [3.](#page-2-1)


    *1) Space-time Transformation:* The first step involves performing space-time
    transformation to map the graph-level loop nests to a systolic array design. We
    identify loops in the outermost loop band with dependence distances no greater
    than one and consider them as candidate space loops. Subsequently, we enumerate
    all possible combinations of space loops from the candidate pool. The selected
    space loops are then permuted in the outermost position, while the loops below
    them are designated as time loops. Due to the constraints imposed by the hardware
    shape of the AIE array, the mapper generates only 1D and 2D systolic arrays. This
    step results in the generation of multiple systolic arrays, each with a unique
    schedule. As shown in Figure [3,](#page-2-1) we choose loops i and j as the space
    loops (on dark gray background) and loop k as the time loop (on light gray background)
    in the MM example.


    *2) Array Partition:* To accommodate the limited number of AIEs in the horizontal
    and vertical directions of the AIE array, array partitioning becomes necessary
    when mapping a large array. In order to achieve this, we apply tiling to the outermost
    permutable loop that contains the space loops. In Figure [3,](#page-2-1) we illustrate
    an example where we tile the outermost loop band in the MM example using the tiling
    factors (N1, M1, K1). The point loops originating from the original loops are
    retained as the space loops. This results in a 2D systolic array with dimensions
    of N<sup>1</sup> × M<sup>1</sup> (on dark gray background).


    *3) Latency Hiding:* Latency hiding plays a crucial role in mitigating the pipeline
    stalls caused by loop-carried dependencies in computational statements. In the
    case of the MM example, the accumulate operations in the statement introduce loop-carried
    dependence within the loop, resulting in long latency in the systolic chain. To
    address this issue, we identify parallel loops in the polyhedral model schedules,
    applies tiling to these loops, and permutes the point loops to the innermost position.
    As an illustration, loops i and j are identified as parallel loops in the MM example.
    We extract them using the tiling factors (N2, M2) and permute the point loops
    to the innermost position. Since there are no loop-carried dependencies on the
    innermost loop, the latency of design reduce as the chain length shortened.


    <span id="page-3-0"></span>![](_page_3_Figure_0.jpeg)


    Fig. 4: Communication Methods for PLIO Ports Utilization Reduction


    *4) Multiple Threading:* As AIE cores execute concurrently, the AIE array inherently
    supports multiple threading. Leveraging this characteristic, utilizing multiple
    AIEs to execute the same instructions but different indexing can significantly
    enhance overall performance. We identify parallelizable loops in the time loops
    that do not have data dependence. In the MM example, the loop k is identified
    as a parallelizable loop. We can apply tiling to this loop using the factors K2.
    The point loop is permuted to the innermost position and completely unrolled to
    generate multiple threads of AIEs.


    #### *C. Placement and Routing Constraints Construction*


    The systolic design generated in the previous section represents an abstract mapping
    scheme. Consequently, it is essential to use the space loops as input and generate
    an actual mapped graph for AIE array that considers placement and routing constraints.
    The mapped graph consists of nodes, representing AIE cores and input/output ports,
    and edges, which connect the ports of the nodes. The placement and routing constraints
    involve assigning coordinates to the AIE cores, buffers, and input/output ports,
    as well as determining the routing paths for the edges. In the subsequent subsections,
    we introduce the graph builder and routing-aware PLIO assignment, which are responsible
    for constructing the mapped graph and generating the associated placement and
    routing constraints, respectively.


    *1) Graph Builder:* To construct the mapped graph, we iterate through all coordinates
    in the space loops and create a node for each pair of coordinates in the 2D systolic
    array, representing an AIE core. Next, we identify the data communications between
    AIE cores based on the dependencies within the space loops. Following the definitions
    in AutoSA [\[11\]](#page-5-15), there are three types of data dependences:


    - Read dependence: Transfer the read-only data.

    - Flow dependence: Transfer the intermediate data.

    - Output dependence: Transfer the output-only data.


    Based on these data dependences and the space loops, we define the I/O ports and
    edge directions. Since AIEs do not support intermediate results between different
    iterations, we treat flow dependences as input dependencies when constructing
    I/O ports. The polyhedral model for the array access to matrix A in the MM recurrences
    is {i, j, k} → {i, j+1, k}, and when loops j, k are the space loops, the direction
    is (1, 0). We connect the input ports from the corresponding nodes with a constant
    and non-zero distant direction.


    As for the output ports, the boundary input ports, and the zero distant direction
    ports, we create PLIO ports as the other end of the connection edge. To adhere
    to the limitation on the number of PLIO ports, we utilize packet-switch communications
    and broadcast communications to reduce the number of used ports, as depicted in
    Figure [4.](#page-3-0)


    *2) Routing-Aware PLIO Assignment:* Once we have the mapped graph, we search for
    available cores on the AIE array to place the AIE kernels. To facilitate efficient
    communication between neighboring cores, we assign the buffers of ports connecting
    these cores to the shared buffer of the cores, forming part of the placement constraints.
    These constraints enable the transformation of the kernels'' placement into a
    regular duplicate pattern of a single kernel.


    Aside from facilitating neighboring communication, it is necessary to construct
    paths between PLIO ports and AIE cores for data input and output. Considering
    the mesh structure of the NoC on the AIE array, and given that PLIOs are always
    located in Row 0, we can compute the routing congestion by counting the horizontal
    data transfer numbers. For instance, we compute the congestion for the *west*
    direction as follows:


    $$\operatorname{Cong}\_{i}^{\text{west}} = \sum\_{p \in \text{PL.IOs}, x \in \text{AIEs}}
    W\_{i}[p][x],$$


    $$W\_i[p][x] = \begin{cases} 1 & (p\_{\text{col}} < i \text{ and } x\_{\text{col}}
    > i \text{ and } (x, p) \in \text{Edges}) \text{ or } \\ & (p\_{\text{col}} >
    i \text{ and } x\_{\text{col}} < i \text{ and } (p, x) \in \text{Edges}) \\\ 0
    & \text{Otherwise} \end{cases}$$


    where pcol and xcol represent the column coordinates of PLIO p and AIE x, respectively.


    The computation of the congestion for the *east* direction is symmetrical.


    Consequently, the routing challenges essentially transform into issues of PLIO
    assignment. We formulate the assignment of PLIO ports as a satisfiability problem
    subject to routing resource constraints. We check if there exists a set of values
    for PLIOs that satisfies the following constraints:


    $$\forall i \in \text{Columns}, \mathsf{Cong}\_i^{\text{west}} \le \mathsf{RC}\_{\text{west}},
    \quad \mathsf{Cong}\_i^{\text{east}} \le \mathsf{RC}\_{\text{east}}$$


    where RC*west* and RC*east* denote the available routing resources in the AIE
    array.


    To seek the feasible assignment of PLIO ports, we employ a heuristic greedy algorithm
    outlined in Algorithm [1.](#page-4-0) In this algorithm, we initialize the placement
    of the PLIO ports by calculating the median value of the row numbers of the connected
    AIE cores. If the initially computed placement coordinate is not available, we
    search for the nearest available coordinate instead. This heuristic greedy algorithm
    balances the routing congestion among the PLIO ports. By considering the connectivity
    with the AIE cores, it generates an optimal placement for the PLIO ports, ensuring
    successful routing on the NoC. The algorithm takes into account the availability
    of coordinates and selects the most suitable placement to minimize congestion.


    By generating these constraints for the placement and routing of AIE kernels,
    buffers, and PLIO ports, we can


    <span id="page-4-0"></span>Algorithm 1 Routing-Aware PLIO Assignment Algortihm


    | Algorithm 1 Routing-Aware PLIO Assignment Algortihm         |

    |-------------------------------------------------------------|

    | Require: Numbers of PLIO ports N, AIE cores X               |

    | Ensure: Initialized PLIO assignment set P                   |

    | 1: Initialization available placement sets A as all columns |

    | that have PLIO ports.                                       |

    | 2: for i ← 1 to N do                                        |

    | S = [], num = 0<br>3:                                       |

    | for x ∈ X do<br>4:                                          |

    | if (p, x) ∈ Edges then<br>5:                                |

    | S.append(xcol)<br>6:                                        |

    | num+ = 1<br>7:                                              |

    | end if<br>8:                                                |

    | end for<br>9:                                               |

    | Sort S to find the median: sort(S, S + num)<br>10:          |

    | P[i] = find nearest(A, S[num/2])<br>11:                     |

    | remove(A, P[i])<br>12:                                      |

    | 13: end for                                                 |

    | 14: return<br>P                                             |


    significantly simplify the task for the AIE compiler. These constraints provide
    valuable information and guidelines for the compilers to optimize the placement
    and routing process, ultimately leading to a high utilization of the AIE array.


    #### IV. AUTOMATIC MAPPING FRAMEWORK


    To facilitate the computation of uniform recurrence, we have developed an automatic
    mapping framework that implements the full functional modules on the Versal ACAP
    architecture, as shown in Figure [5.](#page-4-1)


    <span id="page-4-1"></span>![](_page_4_Figure_5.jpeg)


    Fig. 5: Overview of WideSA Automatic Framework


    Specifically, we introduce a kernel-level mapper, a DMA module constructor, and
    a host program generator, which work in conjunction with the kernel scope and
    graph mapper described in the previous section.


    The kernel-level mapper and optimizer transform the C++ program into a program
    with AIE intrinsics, leveraging the capabilities of the AIE vector processor to
    exploit parallelism and optimize performance. Moreover, we design the architecture
    of efficient DMA modules, which serve as the buffers of AIEs on the PL, in the
    DMA module constructor. This architecture is tailored to the characteristics of
    both the hardware and the computations involved. In addition, we engineer a host
    program generator to generate a controller program that oversees global scheduling.


    TABLE II: Evaluation Benchmarks


    <span id="page-4-2"></span>


    | Benchmarks | Dimension    | Problem Size          | Data Types |

    |------------|--------------|-----------------------|------------|

    |            |              | [8192, 8192, 8192]    | Float      |

    |            |              | [10240, 10240, 10240] | Int8       |

    | MM         | [i, j, k]    | [9600, 9600, 9600]    | Int16      |

    |            |              | [8192, 8192, 8192]    | Int32      |

    | 2D-Conv    |              | [10240, 10240, 4, 4]  | Float      |

    |            | [h, w, p, q] | [10240, 10240, 8, 8]  | Int8       |

    |            |              | [10240, 10240, 4, 4]  | Int16      |

    |            |              | [10240, 10240, 4, 4]  | Int32      |

    |            |              | [8192, 8192]          | Cfloat     |

    | 2D-FFT     | [row, col]   | [8192, 8192]          | Cint16     |

    |            |              | [1048576, 15]         | Float      |

    | FIR Filter |              | [1048576, 15]         | Int8       |

    |            | [n, taps]    | [1048576, 15]         | Int16      |

    |            |              | [1048576, 15]         | Cfloat     |


    #### V. EVALUATION


    #### *A. Benchmark and Experimental Setup*


    In this section, we select four representative uniform recurrences with various
    data types as benchmarks to evaluate the performance of WideSA. The selected benchmarks
    include matrix multiplication (MM), 2D convolution (2D-Conv), 2D Fast Fourier
    Transformation (2D-FFT), and FIR filter [\[17\]](#page-5-16). The problem sizes
    and corresponding data types are provided in Table [II.](#page-4-2) Here, Cfloat
    refers to the complex float data type and Cint16 refers to the complex 16-bit
    integer data type. All the experiments are conducted on VCK5000 with 250 MHz on
    PL and 1.25 GHz on AIE. AMD/Xilinx Vitis 2022.1 is used as the compilation backend
    tool.


    ### *B. Full System Performance*


    We conducted a comparison of the throughput between WideSA and other state-of-the-art
    AIE designs for the same problem size. For the MM benchmark, we successfully compiled
    the CHARM code [\[7\]](#page-5-6) for the target VCK5000 with AMD/Xilinx Vitis
    2022.1, incorporating placement and routing constraints, as the baseline. As for
    the 2D-Conv benchmark, we selected the released 8-PEs version of Vitis-AI DPU
    [\[5\]](#page-5-4) which only supports Int8 data type, utilizing 256 AIEs running
    at 1.33 GHz and the PL at 350 MHz, as the baseline. Furthermore, we used the open-source
    designs from the Vitis DSP Library [\[18\]](#page-5-17) as the baselines for the
    2D-FFT and FIR filter benchmarks.


    The results presented in Table [III](#page-5-18) demonstrate that WideSA achieves
    significantly higher throughput with high utilization of AIEs. Additionally, we
    computed the AIE efficiency by considering the throughput and the number of used
    AIEs. The results indicate that WideSA maintains similar efficiency to [\[7\]](#page-5-6)
    for MM, as both approaches exhibit AIE utilization over 95%. When compared to
    the baselines with lower AIE utilizations, WideSA trades AIE efficiency (TOPS/#AIEs)
    for a high overall performance (TOPS) and is bounded by memory bandwidth.


    Moreover, we conducted a comparison of the performance and energy efficiency of
    MM using WideSA and PL-only designs on the VCK5000 target, which has 1968 DSP58
    IPs at total. For the PL-only designs, we utilize AutoSA [\[11\]](#page-5-15)
    as the systolic array generator. The results presented in Table [IV](#page-5-19)


    TABLE III: Comparison of Throughput and AIE Efficiency on Benchmarks


    <span id="page-5-18"></span>


    | Method   | Metric     |       |       | MM    |       | 2D-Conv |       |       |
    2D-FFT |        | FIR Filter |       |       |       |        |

    |----------|------------|-------|-------|-------|-------|---------|-------|-------|--------|--------|------------|-------|-------|-------|--------|

    |          | Data type  | Float | Int8  | Int16 | Int32 | Float   | Int8  | Int16
    | Int32  | Cfloat | Cint16     | Float | Int8  | Int16 | Cfloat |

    |          | #AIEs      | 384   | 384   | 384   | 384   | -       | 256   | -     |
    -      | 10     | 10         | 10    | 10    | 10    | 10     |

    | Baseline | TOPS       | 3.73  | 29.78 | 7.82  | 3.72  | -       | 31.40 | -     |
    -      | 0.04   | 0.13       | 0.15  | 2.56  | 0.62  | 0.15   |

    |          | TOPS/#AIEs | 0.010 | 0.077 | 0.020 | 0.010 | -       | 0.123 | -     |
    -      | 0.004  | 0.013      | 0.015 | 0.256 | 0.062 | 0.015  |

    |          | #AIES      | 400   | 400   | 400   | 400   | 400     | 400   | 400   |
    400    | 320    | 320        | 256   | 256   | 256   | 256    |

    | WideSA   | TOPS       | 4.15  | 32.49 | 8.10  | 3.92  | 4.50    | 36.02 | 10.35
    | 4.48   | 1.10   | 3.83       | 2.92  | 39.3  | 9.47  | 2.89   |

    |          | TOPS/#AIEs | 0.010 | 0.081 | 0.020 | 0.010 | 0.011   | 0.090 | 0.025
    | 0.011  | 0.003  | 0.012      | 0.012 | 0.100 | 0.037 | 0.011  |


    <span id="page-5-19"></span>TABLE IV: MM Performance Comparison between PL-only
    and WideSA Design


    |              |       |       | PL-only |       | WideSA |       |       |       |  |

    |--------------|-------|-------|---------|-------|--------|-------|-------|-------|--|

    | Data Type    | Float | Int8  | Int16   | Int32 | Float  | Int8  | Int16 | Int32
    |  |

    | DSPs         | 1536  | 1528  | 1516    | 1536  | 152    | 60    | 67    | 65    |  |

    | #AIEs        | 0     | 0     | 0       | 0     | 400    | 400   | 400   | 400   |  |

    | TOPS         | 0.59  | 5.77  | 2.16    | 0.60  | 4.15   | 32.49 | 8.10  | 3.92  |  |

    | Power (W)    | 19.5  | 18.8  | 18.6    | 19.5  | 55.8   | 54.4  | 54.9  | 55.6  |  |

    | TOPS/W       | 0.03  | 0.31  | 0.12    | 0.03  | 0.07   | 0.60  | 0.15  | 0.07  |  |

    | Norm. TOPS/W | 1.00x | 1.00x | 1.00x   | 1.00x | 2.25x  | 1.94x | 1.29x | 2.25x
    |  |


    demonstrate that our approach achieves up to 2.25× higher energy efficiency compared
    to the PL-only designs.


    #### *C. Scalability of WideSA on MM examples*


    We evaluate the scalability of WideSA while increasing AIE utilization and analyze
    how various factors influence performance. The results, presented in Figure [6,](#page-5-20)
    show a significant increase in throughput as the number of AIEs increases. In
    addition, the AIE efficiency results demonstrate that our approach scales effectively
    from small-scale to largescale designs. However, when the number exceeds 200,
    the efficiency of a single AIE core decreases due to the memorybound condition
    caused by the number of PLIOs and the size of the PL buffer. The increase in PLIO
    numbers and buffer sizes leads to increased throughput, suggesting that enhancing
    the bandwidth between different fabrics of ACAP can improve performance. This
    indicates that managing the resources and data flow between different components
    of the ACAP is crucial for achieving better performance.


    #### VI. CONCLUSION


    In this paper, we present a high array utilization mapping scheme for uniform
    recurrences on the Versal ACAP architecture. Additionally, we propose several
    optimizations aimed at enhancing overall performance within an automatic mapping
    framework. Through extensive evaluations using typical benchmarks and diverse
    data types, we assess the efficiency of the WideSA framework. In the future work,
    we aim to integrate WideSA into the MLIR-AIE workflow and develop an end-toend
    compilation tool that incorporates automatic design space exploration.


    #### ACKNOWLEDGEMENT


    This work was partly supported by the National Natural Science Foundation of China
    (Grant No. 62090021) and the National Key R&D Program of China (Grant No. 2022YFB4500500).


    <span id="page-5-20"></span>![](_page_5_Figure_11.jpeg)


    Fig. 6: Throughput Evaluation of Different AIE Numbers, PLIO Numbers, and PL Buffer
    Sizes


    #### REFERENCES


    - <span id="page-5-0"></span>[1] AMD/Xilinx. Versal Adaptive Compute Acceleration
    Platform.

    - <span id="page-5-1"></span>[2] R. M. Karp *et al.*, "The organization of computations
    for uniform recurrence equations," *J. ACM*, 1967.

    - <span id="page-5-2"></span>[3] S. Ahmad *et al.*, "Xilinx first 7nm device:
    Versal AI Core (VC1902)," in *HCS*, 2019.

    - <span id="page-5-3"></span>[4] J. A. Fisher, "Very long instruction word architectures
    and the ELI-512," in *ISCA*, 1983.

    - <span id="page-5-4"></span>[5] Vitis AI Library User Guide. [Online]. Available:
    [https://xilinx.github.](https://xilinx.github.io/Vitis-AI/) [io/Vitis-AI/](https://xilinx.github.io/Vitis-AI/)

    - <span id="page-5-5"></span>[6] X. Jia *et al.*, "XVDPU: A high performance CNN
    accelerator on the Versal platform powered by the AI Engine," in *FPL*, 2022.

    - <span id="page-5-6"></span>[7] J. Zhuang *et al.*, "CHARM: Composing heterogeneous
    accelerators for matrix multiply on Versal ACAP architecture," in *FPGA*, 2023.

    - <span id="page-5-7"></span>[8] AMD/Xilinx. AI Engine Kernel and Graph Programming
    Guide.

    - <span id="page-5-8"></span>[9] W. Cook *et al.*, "An exact rational mixed-integer
    programming solver," in *IPCO*, 2011.

    - <span id="page-5-9"></span>[10] J. Cong *et al.*, "PolySA: Polyhedral-based
    systolic array autocompilation," in *ICCAD*, 2018.

    - <span id="page-5-15"></span>[11] J. Wang *et al.*, "AutoSA: A polyhedral compiler
    for high-performance systolic arrays on FPGA," in *FPGA*, 2021.

    - <span id="page-5-10"></span>[12] Y.-H. Lai *et al.*, "SuSy: A programming model
    for productive construction of high-performance systolic arrays on FPGAs," in
    *ICCAD*, 2020.

    - <span id="page-5-11"></span>[13] M.-W. Benabderrahmane *et al.*, "The polyhedral
    model is more widely applicable than you think," in *CC*, 2010.

    - <span id="page-5-12"></span>[14] U. Bondhugula, "Compiling affine loop nests
    for distributed-memory parallel architectures," in *SC*, 2013.

    - <span id="page-5-13"></span>[15] R. Andonov *et al.*, "Optimal semi-oblique
    tiling," *IEEE TPDS*, 2003.

    - <span id="page-5-14"></span>[16] C. Rossetti *et al.*, "Algebraic tiling," in
    *IMPACT*, 2023.

    - <span id="page-5-16"></span>[17] K. K. Parhi, "VLSI digital signal processing
    systems: Design and implementation," 2007.

    - <span id="page-5-17"></span>[18] AMD/Xilinx. Vitis DSP Library for digital signal
    processing.'
- title: 'MX: Enhancing RISC-V''s Vector ISA for Ultra-Low Overhead, Energy-Efficient
    Matrix Multiplication'
  abstract: ''
  keywords: RISC-V, Matrix, Vector, Efficiency
  document: '#### I. INTRODUCTION


    The exponential growth of the computational requirements in Machine Learning (ML)
    and Artificial Intelligence (AI) applications is a major challenge for hardware
    architects. The rise of application-specific accelerators [1] and single instruction,
    multiple data (SIMD) programmable systems [2] demonstrates the need for novel
    architectures able to cope with the rising computational demand. Furthermore,
    AI/ML applications also found their way into edge computing, with benefits such
    as higher privacy, user personalization, and lower power consumption. However,
    edge-AI/ML systems have the additional challenge of balancing large computational
    demands against a very tight power envelope and minimal area footprint. The quest
    for energy efficiency and cost (i.e., area) minimization is even more pressing
    today since ML/AI computation at the edge does not only involve inference but
    also training in the so-called AI on Edge [3].


    Matrix Multiplication (MatMul) is a cornerstone in ML and AI, and essential in
    scientific computing, graphics, and Digital Signal Processing (DSP). The importance
    of MatMul is testified by market-leading companies, such as Google, which developed
    the first Tensor-Processing Unit (TPU) in 2015 to accelerate matrix operations
    [4] and updated it in 2018 with an edgeoriented version achieving 4 TOPS within
    a 2 W power envelope [5]. As with other Domain-Specific Accelerators (DSAs) for
    specific neural-network tasks [6], the TPU is an added resource to which a general-purpose
    processor offloads the workload (for example, through a PCIe interface). This
    brings an area and power overhead that is not affordable in constrained systems
    at the edge, especially when they need to compute non-AI tasks as well. Moreover,
    an excessively specialized accelerator risks becoming useless when the ML/AI algorithm
    evolves.


    Most proprietary Instruction Set Architectures (ISAs) offer dedicated matrix extensions,
    such as Arm''s Scalable Matrix Extension (SME), Intel Advanced Matrix Extension
    (AMX), and IBM Matrix-Multiply Assist (MMA). Unluckily, the microarchitectural
    details of the implementations remain company secrets. So far, the RISC-V open-source
    ISA features only a vector extension (RISC-V Vector (RVV)), even though researchers
    developed multiple unofficial AI/matrix extensions. Still, they add tightly coupled
    matrix units [7] or a new matrix register file [8] used only during matrix operations,
    which add area and power consumption.


    RVV recently showed to be a valid solution to efficiently accelerate MatMul while
    keeping a well-known programming model to handle diverse data-parallel workloads,
    also in the embedded domain [9]. Vector processors execute multiple operations
    with one instruction, amortizing its fetch/decode cost. Moreover, they feature
    a Vector Register File (VRF) to buffer the vector elements, decreasing the accesses
    to memory without changing the computational balance for the architecture [10].
    Even if the VRF helps decrease the power associated with the memory accesses,
    it is an additional block at the bottom of the memory hierarchy, one of the key
    drivers for performance and energy efficiency [11]. Its size can be way larger
    than the one of a scalar register file, and it is usually connected to multiple
    functional units in parallel, which leads to energyhungry interconnects. Hence,
    the VRF access-related energy is usually non-negligible [9], [12].


    With this paper, we present Matrix eXtension (MX), a non-intrusive ISA extension
    to RVV that creates a generalpurpose hybrid matrix/vector architecture with minimal
    area impact and superior energy efficiency. To cut the power consumption, we reduce
    the expensive accesses to/from the


    The first two authors contributed equally to this work.


    <sup>©</sup> 2023 IEEE. Personal use of this material is permitted. Permission
    from IEEE must be obtained for all other uses, in any current or future media,
    including reprinting/republishing this material for advertising or promotional
    purposes, creating new collective works, for resale or redistribution to servers
    or lists, or reuse of any copyrighted component of this work in other works.


    VRF by featuring a software-transparent lightweight accumulator close to the processing
    units. MX does not add a matrix unit to the architecture but re-uses the already
    available processing resources to keep the area and energy overhead at its minimum
    and exploit the energy efficiency savings that come from the reduced VRF accesses.


    To validate MX across multiple domains, we add MX to a constrained embedded Dual-Core
    cluster built upon the opensource energy-optimized RVV-based Spatz [9] vector
    processor and to a scaled-up MemPool architecture [13] with 64 Spatz processors
    and implement both systems in a competitive 12-nm technology. We provide a quantitative
    justification of the energy savings and a detailed power, performance, and area
    (PPA) analysis on matrix multiplications on different data precisions, finding
    that our matrix extension can boost not only energy efficiency but also performance.


    With this paper, we present the following contributions:


    - We define MX, a lightweight and non-intrusive ISA extension based on RVV 1.0
    aimed at supporting memory and computational operations directly on matrices.
    MX reduces the power consumption of the architecture with similar or better performance
    by introducing a near-Floating Point Unit (FPU) tile buffer, a per-vector-element
    broadcast system, and minimal modifications to the Vector Load/Store Unit (VLSU).

    - We provide a theoretical justification of the benefits that the ISA has on the
    power consumption when executing a matrix multiplication kernel, effectively reducing
    the expensive VRF accesses.

    - We implement MX on a constrained Dual-Core and a complex 64-core clusters based
    on the energy-efficient RVV vector processor Spatz, and characterize MX''s impact
    on performance and PPA metrics in a 12-nm technology. For less than 3% area overhead,
    we get a maximum of 56% and 25% performance and energy efficiency gains, respectively.


    #### II. ANALYSIS


    In the following, we discuss the tiling of a General Matrix Multiply (GEMM) problem
    through a multi-level memory hierarchy. When C is a zero matrix, GEMM becomes
    a MatMul.


    $$D\_{M \times N} = A\_{M \times K} \cdot B\_{K \times N} + C\_{M \times N} \tag{l}$$


    For convenience, let us consider a memory hierarchy composed of a memory, a VRF,
    and a near-FPU buffer. The memory connects to the VRF, which is connected to a
    buffer that feeds the FPUs, as reported in Figure 1. The following analysis can
    be easily extended to memory hierarchies with a different number of levels.


    #### *A. The tiling problem*


    The lower level of the hierarchy is usually not large enough to keep the input
    and output matrices all at once. Therefore, the matrices are divided into chunks
    (tiles), and the hardware works on one output tile at a time, and the outer product
    algorithm is often used to maximize parallelism. The number of elements


    ![](_page_1_Figure_12.jpeg)


    Figure 1. The tiling problem over a memory hierarchy composed of three levels,
    ending with the processing elements (FPUs).


    transferred between two consecutive levels of the hierarchy impacts both performance
    and power consumption and depends on how the matrices are tiled. Usually, the
    number of transfers is partially encoded in the *arithmetic intensity*, i.e.,
    the total number of operations divided by the total number of Bytes transferred
    between the memory and the core.


    In the following, we provide equations to fine-grain count how many memory accesses
    happen between each pair of consecutive levels of the hierarchy. Each equation
    contains four terms, which correspond to 1) the elements of matrix A, 2) the elements
    of matrix B, 3) the elements of matrix C (or D) from the upper level to the lower
    one (load/fetch), and 4) the elements of the matrix D from the lower level back
    to the upper one (store/write-back).


    In the most generic scenario, without buffering the output tile in the VRF for
    more than updates, the number of elements moved between the memory and the VRF
    is:


    $$\#E\,lm\_{VRF}^{MEM} = \frac{N}{n}MK + \frac{M}{m}NK + \frac{K}{k}MN + \frac{K}{k}MN
    \qquad (2)$$


    Where the A, B, and D (C) matrices stored in memory have sizes , , , and we tile
    the problem between the memory and the VRF with tiles of size , , .


    For each matrix tile, the number of elements exchanged between the VRF and the
    buffer is:


    $$\#Elim\_{BUF}^{VRF} = \frac{n}{n''}mk + \frac{m}{m''}nk + \frac{k}{k''}mn +
    \frac{k}{k''}mn\tag{3}$$


    Where the tiles stored in the VRF have sizes , , , and we sub-tile the problem
    between the VRF and the buffer with sub-tiles of size ′ ′ , ′ ′ , ′ ′ .


    For each matrix sub-tile, the number of elements exchanged between the buffer
    and the FPUs is:


    $$\#E\,l m\_{FPU}^{BUF} = \frac{n''}{t\_B} m'' k'' + \frac{m''}{t\_A} n'' k''
    + k'' m'' n'' + k'' m'' n'' \qquad (4)$$


    Where the sub-tiles stored in the buffer have sizes ′ ′ , ′ ′ , ′ ′ , and we access
    and elements from tiles A and B, respectively.


    ## *B. Total number of transfers*


    To get the total number of transfers between each pair of hierarchy levels, we
    need to take into account how many output tiles and sub-tiles we calculate throughout
    the program.


    Table I NUMBER OF ACCESSES BETWEEN CONSECUTIVE LEVELS OF THE MEMORY HIERARCHY.


    | Ref.           | Metric                                             | A (↓)                                        |
    B (↓)                                       | C, D (↓)                                                             |
    D (↑)                                                                |

    |----------------|----------------------------------------------------|----------------------------------------------|---------------------------------------------|----------------------------------------------------------------------|----------------------------------------------------------------------|

    | 1)<br>2)<br>3) | #ElmMEM<br>VRF<br>#ElmVRF<br>BUF<br>#ElmBUF<br>FPU | N<br>n
    MK<br>N<br>′ MK<br>n<br>N<br>MK<br>tB | M<br>NK<br>m<br>M<br>m′ NK<br>M<br>NK<br>tA
    | K<br>k MN<br>k<br>K<br>k MN<br>′<br>k<br>k′ k<br>K<br>k MN<br>′<br>k | K<br>k
    MN<br>k<br>K<br>k MN<br>′<br>k<br>k′ k<br>K<br>k MN<br>′<br>k |


    (a) ↓ /↑ indicate transfers to a lower/higher level of the memory.


    *a) Memory and VRF:* In the most generic case, we load the C (first iteration)
    and D (from the second iteration on) tiles from memory before consuming the input
    , -sized tiles, and we store the D tile back to memory after updates. Without
    inter-k-tile buffering in the VRF, we load ( ) output tiles with size from the
    memory to the VRF, and we store back the same amount. If we buffer the output
    tiles until they are completely calculated over the whole K dimension, the formula
    simplifies to . Instead, we load a total of input A and B tiles, with sizes ,
    .


    *b) VRF and buffer:* The ′ ′ output sub-tiles of a tile are fetched for ( ′ )
    ′ ′ times from the VRF before consuming the input ′ ′ , ′ ′ -sized sub-tiles in
    the buffer, and writtenback to the VRF for the same number of times if there is
    no inter-k-tile buffering in the buffer. Instead, the input A and B sub-tiles,
    with sizes ′ ′ , ′ ′ , are loaded times. If we keep into account how many times
    each tile is loaded/stored from/to memory, these formulas become ( ) ( ′ ) ′ ′
    for the sub-tiles fetch, the same amount for the sub-tiles writes-back, and ′
    ′ ′ for the A and B sub-tiles fetch.


    We summarize all the transfers across the hierarchy in Table I.


    #### *C. Optimizations*


    *a) Inter-k-buffering:* If the output tile (sub-tile) is buffered in the VRF (buffer)
    until the whole K (k) dimension is traversed and the whole output tile (sub-tile)
    is ready, we can simplify the equations above. If the buffering happens in the
    VRF, = 1 in Table I Ref. 1), while if it happens in the buffer until the whole
    dimension, ′ = 1, in Table I Ref. 2) (if the buffering only happens until the
    whole dimension is traversed, ′ = 1).


    Inter-k-buffering is ultimately limited by the size of the lower memory level,
    which should be able to host the whole output tile (sub-tile) for the whole computation
    on the () dimension. Therefore, keeping the output tile in the buffer for the
    whole dimension requires that = ′ , = ′ . On the other hand, relaxing this constraint,
    e.g. = ′ , = × ′ , allows for fewer overall transfers between the memory and the
    VRF. In this case, the inter-k-buffering can be done only between the memory and
    the VRF.


    *b) C-tile reset:* When C is a zero matrix, it''s possible to avoid loading it
    from memory and initialize the VRF with zeroes or reset the buffer. If we use
    inter-k-buffering and the zero initialization is applied to the VRF and the buffer,
    the third term of the equations related to the load/fetch of matrix C, D, becomes
    zero in Table I Ref. 1) and 2), respectively.


    ![](_page_2_Figure_10.jpeg)


    Figure 2. Spatz''s VLSU, VRF, and VFU with MX architectural schematic.


    #### III. MX IMPLEMENTATION


    #### *A. ISA Extension*


    We implement MX in Spatz [9], an open-source, RVV-based, highly-optimized compact
    vector processor, targeting minimal area and power overhead. Thus, we do not add
    any dedicated matrix units or software-addressable registers, as shown in Figure
    2. MX adds three configure instructions (msettile[m,n,k]), three memory instructions
    (mld.[a,b], mst.c), and two computation instructions (mx[f]macc). The first three
    instructions set up the sub-tile sizes ′ , ′ , ′ on which the matrix instructions
    operate ( ′ ′ = , ′ ′ ≤ , and is the vector length in elements). We enhance the
    VLSU to enable matrix load and store operations, which are composed of multiple
    unitand non-unit-stride memory operations already supported by Spatz. The new
    memory instructions are introduced to load tiles from matrix *A* and *B* and to
    store the computed tile back to memory, while the two computational instructions
    perform a MatMul between the two ′ ′ and ′ ′ sub-tiles, storing the resulting
    ′ ′ sub-tile in the VRF. Close to the FPUs, we introduce a tiny broadcast block
    consisting only of a register and some multiplexers to broadcast single elements
    from the *A* tile across multiple elements of tile *B*. This block increases the
    data reuse of tile *A* at a minimal cost. Finally, we implement a latch-based
    result tile buffer in the vector functional unit (VFU) to reduce energy consumption
    by minimizing VRF accesses by intermediate result accumulation, limiting the buffer
    size to <sup>1</sup> 8 of the VRF size (i.e., = 256) to maintain low energy and
    area overhead. Since Spatz''s VLSU has four parallel memory ports and the buffer
    is constrained in size, ′ , ′ , ′ ∈ {4, 8}.


    #### *B. MX Benefits*


    Table II summarizes the number of elements transferred between consecutive memory
    hierarchies for a baseline vectoronly MatMul and a MX-ready MatMul. The baseline
    employs a traditional scalar-vector algorithm to load *m* scalar elements from
    the input matrix *A* and an *n*-long vector from matrix *B*. The MX-ready configuration
    loads A tiles with size ′ ′ and *B* tiles with size ′ ′ . While the MX algorithm
    does not further sub-tile the tiles on or (*m''* = *m* and *k''* = *k*), it sub-tiles


    | Config                     | Metric                                             |
    A (↓)                                           | B (↓)                                    |
    C, D (↓)                   | D (↑)                       |

    |----------------------------|----------------------------------------------------|-------------------------------------------------|------------------------------------------|----------------------------|-----------------------------|

    | Baseline(a)<br>Baseline(a) | #ElmMEM<br>VRF<br>#ElmVRF<br>FPU                   |
    N<br>n MK<br>N<br>F MK                          | M<br>NK<br>m<br>MNK                      |
    0<br>KMN                   | MN<br>KMN                   |

    | MX<br>MX<br>MX             | #ElmMEM<br>VRF<br>#ElmVRF<br>BUF<br>#ElmBUF<br>FPU
    | N<br>′ MK<br>B×n<br>N<br>′ MK<br>n<br>N<br>F MK | M<br>m′ NK<br>M<br>m′ NK<br>M<br>NK<br>F
    | 0<br>K<br>′ MN<br>k<br>KMN | MN<br>K<br>′ MN<br>k<br>KMN |


    Table II DATA TRANSFERS: MX-READY VS BASELINE.


    (a) Elements from A are loaded/fetched to/from the scalar register file;


    (b) *F* represents the number of FPUs;


    (c) ↓ /↑ indicate transfers to a lower/higher level of the memory.


    along *n* such that *n* = ×*n''*, where ∈ {2, 4}. In the following, we highlight
    the benefits brought by the MX algorithm.


    *1) Matrix A operands:* In the baseline approach, operands from matrix *A* are
    fetched as scalars from the scalar register file and individually forwarded to
    the vector unit. In contrast, the matrix algorithm retrieves multiple elements
    from *A* in a tiled-vector manner, improving the access pattern and enabling the
    data reuse of the *A* tile by means of the broadcast engine.


    *2) Instruction count:* In the baseline algorithm, each vector instruction is
    amortized over operations. With MX, the total number of instructions fetched and
    decoded is lower, as each mxfmacc instruction is amortized over ′ ′ ′ operations,
    ′ ′ = , and ′ > 1. This boosts the SIMD ratio, i.e., the average number of operations
    per instruction.


    *3) Tile window:* The matrix algorithm exploits the *k* dimension to increase
    the size of the tile window when the dimensions *M* and *N* are limited. This
    is especially beneficial as the SIMD ratio is further improved by allowing each
    core to work on a larger output tile window in a multi-core environment when processing
    matrices with a limited dimension.


    *4) Scalar-vector interactions:* With the baseline algorithm, the scalar core
    must remain active to compute operand addresses and forward scalar operands to
    the Vector Processing Unit (VPU). In contrast, MX pushes the whole computation
    to the vector unit, freeing up the scalar core.


    *5) Performance:* The computing performance is significantly impacted by the number
    of data transfers between the memory and the VRF and the related latency. The
    MX-ready VLSU regularizes the memory accesses, which can reduce conflicts in both
    the interconnect and memory banks.


    *6) Energy:* In the VPU, the power consumption of the VRF normally constitutes
    a non-negligible portion of the overall energy usage. MX''s inexpensive broadcast
    engine and tile buffers enhance data reuse for the tiled matrix *A* and reduce
    the VRF access by a ′ factor. Moreover, the reduced instruction count and more
    regular memory access pattern alleviate the pressure on the instruction and data
    memories, further improving the energy efficiency of the overall system.


    #### IV. EXPERIMENT SETUP AND RESULTS


    # *A. Computing Clusters and Methodology*


    We integrate the baseline and the MX-ready versions of the Spatz VPU into two
    floating-point-capable computing clusters: a 64-bit constrained Dual-Core cluster
    for in-depth analysis of various tile and sub-tile configurations, and a 32-bit
    large-scale 64-Core cluster for performance evaluation in a complex system.


    *1) Dual-Core Cluster:* The Dual-Core cluster is a 64 bit shared-L1-memory cluster,
    implemented with 128 KiB of Tightly Coupled Data Memory (TCDM) across 16 Static
    Random-Access Memory (SRAM) banks. This cluster features 2 Snitch cores, each
    controlling a Spatz instance equipped with 4 double-precision FPUs and 2 KiB VRF
    each, supporting a vector length of 512 bits. The peak achievable performance
    is 16 DP−FLOP/cycle.


    *2) 64-Core MemPool Cluster:* MemPool, a large-scale 32 bit shared-L1-memory cluster,
    scales up to 256 RISC-V cores and includes 1 MiB of L1 TCDM [13]. The cluster
    is hierarchically organized into 4 groups, each containing 16 tiles. A fully connected
    logarithmic crossbar is employed between the cores and memories, achieving non-uniform
    memory access (NUMA) with a maximum latency of 5 cycles. We equip each Spatz instance
    with 4 32-bit FPUs and 2 KiB of VRFs each, supporting a vector length of 512 bits,
    and pair each instance with a scalar Snitch core to form a Core Complex (CC).
    This cluster configuration, labeled MemPool64Spatz4, consists of 64 CCs, one for
    each tile, and achieves a peak performance of 512 SP−FLOP/cycle, as detailed further
    in [9].


    We implement our designs in GlobalFoundries'' 12 nm LP-PLUS FinFET technology
    through Synopsys Fusion Compiler 2022.03 for synthesis and Place-and-Route (PnR).
    We analyze the PPA metrics of the MX-ready clusters at the post-PnR implementation
    stage and compare them to their respective non-MX baseline architectures. We calculate
    power consumption using Synopsys'' PrimeTime 2022.03 under typical operating conditions
    (TT/0.80 V/25 °C), with switching activities obtained from QuestaSim 2021.3 post-layout
    gate-level simulations and back-annotated parasitic information. In the used MatMul
    kernels, all the input and output matrices are kept in the L1 memory and each
    core of the cluster calculates one portion of the output matrix. The kernel executes
    in parallel across the entire cluster, partitioning the matrix equally among multiple
    cores. At the end of each parallel task, the cores are synchronized to ensure
    consistent write-back of the results.


    #### *B. Implementation Area and Frequency*


    The logic area breakdown of the clusters is presented in Table III. For the MX-ready
    Dual-Core cluster, the main area increase originates from the VFU (+5.3%) due
    to the near-FPU tile buffer and is followed by a slight increase in the VLSU (+5.94
    kGE), which is related to supporting matrix loads/stores. The total area overhead
    of MX is negligible, amounting to an increase of 2.5 %. The MemPool64Spatz<sup>4</sup>
    cluster follows the same trend, resulting in a similar 2.89 % area overhead. MX
    does not affect the critical path of the two systems in analysis, which runs through
    Snitch to a TCDM bank. Thus, the MXready dual- and 64-core systems achieve 920
    MHz and 720 MHz in the (SS/0.72 V/125 °C) corner, respectively, with no frequency
    degradation with respect to the baseline clusters.


    Table III LOGIC AREA BREAKDOWN IN 12-NM TECHNOLOGY.


    |         |          | Dual-Core Cluster[kGE] |        | 64-Core Cluster[MGE]
    |       |          |  |  |

    |---------|----------|------------------------|--------|----------------------|-------|----------|--|--|

    |         | Baseline | MX<br>Overhead         |        | Baseline<br>MX       |       |
    Overhead |  |  |

    | Snitch  | 47.82    | 48.01                  | +0.40% | 1.50                 |
    1.47  | -2.04%   |  |  |

    | i-Cache | 149.67   | 149.56                 | -0.07% | 4.96                 |
    4.95  | -0.20%   |  |  |

    | TCDM(a) | 1191.89  | 1192.03                | +0.01% | 20.46                |
    20.48 | +0.09%   |  |  |

    | VRF     | 345.04   | 348.87                 | +1.11% | 9.32                 |
    9.32  | 0.0%     |  |  |

    | VFU     | 1532.11  | 1613.39                | +5.31% | 12.91                |
    13.97 | +8.21%   |  |  |

    | VLSU    | 111.66   | 117.60                 | +5.32% | 2.54                 |
    3.07  | +20.87%  |  |  |

    | Other   | 570.63   | 575.97                 | +0.94% | 7.28                 |
    7.39  | +1.51%   |  |  |

    | Total   | 3948.82  | 4045.43                | +2.45% | 59.70                |
    61.43 | +2.89%   |  |  |


    (a) Including Memory Banks and Interconnect Logic.


    ![](_page_4_Figure_3.jpeg)


    Figure 3. Power breakdown for Dual-Core (Left) and 64-Core clusters (Right) executing
    MatMul. Dual-Core: at *TT@1GHz*, executing non-MX (4 vectors, length 32) and MX-ready
    algorithms (′ = 8, ′ = 4, ′ = 4, = 4). 64- Core: at *TT@910MHz*, executing non-MX
    (8 vectors, length 32) and MX-ready algorithms (′ = 8, ′ = 4, ′ = 8, = 8).


    #### *C. Performance, Power and Energy Efficiency*


    *1) Dual-Core Cluster:* The upper part of Table IV summarizes the kernel information,
    execution performance, and energy efficiency for the Dual-Core cluster when executing
    a 64-bit MatMul across various problem sizes and tile/sub-tile configurations,
    highlighting the rows where the kernel''s tile and sub-tile configurations achieve
    the best energy efficiency. The MX-ready cluster with a sub-tile size of (8, 4,
    4) achieves performance similar to the best-performing execution on the baseline
    cluster with efficiency gains by +10.9 % (16 × 16 × 16), +10.3 % (32 × 32 × 32),
    and +5.2 % (64 × 64 × 64).


    We evaluate the baseline algorithm using two different output tile configurations
    with constant sizes. For small problems (16 × 16 × 16), the output tile size of
    (8, 16, 1) yields higher FPU utilization. As discussed in Section II, although
    (8, 16, 1) has higher arithmetic intensity and fewer transfers between TCDM and
    VRF compared to (4, 32, 1), the latter configuration benefits from a 2× increase
    in SIMD ratio, leading to better performance for larger problem sizes. For the
    MX-ready algorithm, the output tiles with larger and equal ′ sub-tile dimensions
    consistently yield better performance and energy efficiency. This improvement
    is attributed to their higher arithmetic intensity and average SIMD ratio. A similar
    trend is observed for the energy efficiency when increasing the ′ dimension of
    the sub-tile. Due to the higher arithmetic intensity, the power decreases when
    the output tile size changes from (4, 16, 4) to (8, 8, 4). However, the (4, 16,
    4) configuration achieves higher performance thanks to more and shorter matrix
    result stores, which can be interleaved with computational instructions to hide
    latency.


    The left part of Figure 3 presents the power breakdown of the Dual-Core cluster''s
    baseline and MX-ready execution of a 64 × 64 × 64 MatMul, with the most energy-efficient
    tile and sub-tile size in our benchmarks. MX reduces VRF access for the B tile
    and intermediate result storage, leading to a 53.5 % reduction in VRF power consumption.
    Although the sub-tile buffer integration results in a slight 9.4 % power increase
    in VFU, the overall VPU power decreases by 4.1 %. We also observed a power decrease
    across the rest of the cluster components, including the Snitch core, instruction
    caches, and TCDM. This reduction is attributed to the higher SIMD ratio and tiled
    memory request pattern in MX-ready execution, which eliminates the multiple requests
    for scalar operands generated by the Snitch core in the baseline. As a result,
    the total power savings for the Dual-Core cluster achieved through MX amounts
    to 10.4 %.


    *2) 64-Core Cluster:* Our benchmark results for various problem sizes on MemPool64Spatz<sup>4</sup>
    are presented in the bottom section of Table IV. In such a large interconnected
    memory, contentions may occur when memory requests in the same tile access the
    same local bank or the same remote group in the same cycle. This generates stalls
    of the VLSU and increases the access latency. Although such contentions could
    be mitigated by allocating data structures in a local tile''s memory [14], this
    approach is hard to implement for MatMul, which inherently requires an extremely
    global data access pattern.


    MX regular memory accesses alleviate contention and improve VLSU utilization by
    distributing vector element loads/stores across different banks and groups in
    a strided fashion, contrasting with the baseline where vector elements are fetched
    from continuous addresses within the same group by both scalar and vector core.
    This is even more evident with small matrices, where the initial vector load and
    final result store constitute a significant portion of the total runtime due to
    the inability to hide latency. FPU utilization increases from 50.4% to 78.7%,
    leading to a 56% improvement in cluster performance.


    Despite a power consumption increase due to the higher FPU utilization, the MX-ready
    cluster achieves 25 % better energy efficiency. Even though the baseline kernels
    already achieve near-peak utilization for matrix sizes of 128 × 128 × 128 and
    256 × 256 × 256, with the same arithmetic intensity, MX still improves performance
    by 5.6 % and 2.3 %, with energy efficiency gains by 13.4 % and 9.8 %, respectively.
    The right side of Figure 3 presents the MemPool64Spatz4-related power breakdown
    comparison for a 256 × 256 × 256 MatMul. MX reduces the VRF power consumption
    by 60 %, thanks to fewer accesses achieved by buffering intermediate results.
    The VFU power increases by only 6 %, which comes from the sub-tile buffer and
    higher FPU utilization. Overall, MX leads to a 6.9 % cluster power reduction with
    near-peak FPU utilization.


    These analyses on both small- and large-scale vector clusters demonstrate that
    MX significantly improves the energy efficiency by reducing the power consumption
    related to the VRF accesses. MX also pushes the FPU utilization closer to its
    peak with a negligible area overhead. A quantitative comparison of MX against
    [7], [8] is hard since none of them presents area or power results, and the effective
    MatMul speed-up is unclear [7].


    Table IV THE SUMMARY OF KERNEL INFORMATION, EXECUTION PERFORMANCE AND ENERGY EFFICIENCY


    | Config                   | Mtx Size<br>[M, N, K] | Tile Size<br>[m, n, k] |
    Sub-Tile Size<br>[m'', n'', k''] | Mem-VRF<br>Transfers | Arithmetic<br>Intensity<br>[FLOP/B]
    | SIMD Ratio<br>[FLOP/vinsn] | Utilization | Performance<br>@ss freq<br>[GFLOPS]
    | Performance<br>@tt freq<br>[GFLOPS] | Power<br>@tt freq<br>[W] | En. Efficiency<br>@tt
    freq<br>[GFLOPS/W] |

    |--------------------------|-----------------------|------------------------|-------------------------------|----------------------|-------------------------------------|----------------------------|-------------|-------------------------------------|-------------------------------------|--------------------------|------------------------------------------|

    | Dual-Core Cluster(a) (b) |                       |                        |                               |                      |                                     |                            |             |                                     |                                     |                          |                                          |

    | Baseline                 | 64x64x64              | 8,16,1                 |
    -                             | 53248                | 1.23                                |
    16.00                      | 95.9%       | 14.13                               |
    15.34                               | 0.21                     | 71.49                                    |

    | Baseline                 | 64x64x64              | 4,32,1                 |
    -                             | 77824                | 0.84                                |
    32.00                      | 97.8%       | 14.41                               |
    15.65                               | 0.21                     | 73.48                                    |

    | Baseline                 | 32x32x32              | 8,16,1                 |
    -                             | 7168                 | 1.14                                |
    16.00                      | 90.0%       | 13.26                               |
    14.40                               | 0.20                     | 70.95                                    |

    | Baseline                 | 32x32x32              | 4,32,1                 |
    -                             | 10240                | 0.80                                |
    32.00                      | 93.3%       | 13.75                               |
    14.93                               | 0.20                     | 72.87                                    |

    | Baseline                 | 16x16x16              | 8,16,1                 |
    -                             | 1024                 | 1.00                                |
    16.00                      | 70.1%       | 10.33                               |
    11.22                               | 0.16                     | 71.69                                    |

    | Baseline                 | 16x16x16              | 4,32,1                 |
    -                             | 1408                 | 0.73                                |
    32.00                      | 64.7%       | 9.53                                |
    10.35                               | 0.16                     | 66.70                                    |

    | MX-ready                 | 64x64x64              | 4,8,4                  |
    4,4,4                         | 102400               | 0.64                                |
    34.73                      | 94.1%       | 13.86                               |
    15.06                               | 0.21                     | 72.91                                    |

    | MX-ready                 | 64x64x64              | 8,8,4                  |
    8,4,4                         | 69632                | 0.94                                |
    63.22                      | 95.6%       | 14.08                               |
    15.30                               | 0.19                     | 79.15                                    |

    | MX-ready                 | 64x64x64              | 4,16,4                 |
    4,4,4                         | 86016                | 0.76                                |
    36.76                      | 96.4%       | 14.20                               |
    15.42                               | 0.21                     | 75.19                                    |

    | MX-ready                 | 64x64x64              | 8,16,4                 |
    8,4,4                         | 53248                | 1.23                                |
    66.59                      | 97.2%       | 14.32                               |
    15.55                               | 0.19                     | 81.49                                    |

    | MX-ready                 | 32x32x32              | 4,8,4                  |
    4,4,4                         | 13312                | 0.62                                |
    34.29                      | 88.4%       | 13.02                               |
    14.14                               | 0.20                     | 71.90                                    |

    | MX-ready                 | 32x32x32              | 8,8,4                  |
    8,4,4                         | 9216                 | 0.89                                |
    62.48                      | 89.7%       | 13.22                               |
    14.35                               | 0.18                     | 77.68                                    |

    | MX-ready                 | 32x32x32              | 4,16,4                 |
    4,4,4                         | 11264                | 0.73                                |
    36.21                      | 92.7%       | 13.66                               |
    14.83                               | 0.20                     | 74.36                                    |

    | MX-ready                 | 32x32x32              | 8,16,4                 |
    8,4,4                         | 7168                 | 1.14                                |
    65.68                      | 93.5%       | 13.78                               |
    14.96                               | 0.19                     | 80.38                                    |

    | MX-ready                 | 16x16x16              | 4,8,4                  |
    4,4,4                         | 1792                 | 0.57                                |
    33.45                      | 63.1%       | 9.30                                |
    10.10                               | 0.15                     | 67.45                                    |

    | MX-ready                 | 16x16x16              | 8,8,4                  |
    8,4,4                         | 1280                 | 0.80                                |
    61.09                      | 66.1%       | 9.74                                |
    10.58                               | 0.14                     | 75.03                                    |

    | MX-ready                 | 16x16x16              | 4,16,4                 |
    4,4,4                         | 1536                 | 0.67                                |
    35.20                      | 71.6%       | 10.55                               |
    11.46                               | 0.16                     | 72.03                                    |

    | MX-ready                 | 16x16x16              | 8,16,4                 |
    8,4,4                         | 1024                 | 1.00                                |
    64.00                      | 70.3%       | 10.36                               |
    11.25                               | 0.15                     | 75.41                                    |

    | 64-Core Cluster(c)       |                       |                        |                               |                      |                                     |                            |             |                                     |                                     |                          |                                          |

    | Baseline                 | 256x256x256           | 8,32,1                 |
    -                             | 2686976              | 3.12                                |
    32                         | 94.5%       | 372.26                              |
    439.94                              | 1.57                     | 279.86                                   |

    | Baseline                 | 128x128x128           | 8,32,1                 |
    -                             | 344064               | 3.05                                |
    32                         | 90.7%       | 357.34                              |
    422.31                              | 1.57                     | 268.64                                   |

    | Baseline                 | 64x64x64              | 8,8,1                  |
    -                             | 69632                | 1.88                                |
    8                          | 50.4%       | 198.57                              |
    234.68                              | 1.20                     | 194.91                                   |

    | MX-ready                 | 256x256x256           | 8,32,8                 |
    8,4,8                         | 2686976              | 3.12                                |
    137.74                     | 96.7%       | 380.74                              |
    449.97                              | 1.46                     | 307.35                                   |

    | MX-ready                 | 128x128x128           | 8,32,8                 |
    8,4,8                         | 344064               | 3.05                                |
    136.23                     | 95.8%       | 377.27                              |
    445.86                              | 1.46                     | 304.55                                   |

    | MX-ready                 | 64x64x64              | 8,8,8                  |
    8,4,8                         | 69632                | 1.88                                |
    123.43                     | 78.7%       | 309.99                              |
    366.35                              | 1.50                     | 244.24                                   |


    (a) In bold, we highlight the best metrics for both the Baseline and MX-ready
    Dual-Core Cluster execution across various matrix sizes.


    (b) Dual-Core Cluster: Double-Precision operations; ss freq = 920MHz; tt freq
    = 1GHz.


    (c) 64-Core Cluster: Single-Precision operations; ss freq = 770MHz; tt freq =
    910MHz.


    ## V. CONCLUSION


    In this paper, we presented MX, an RVV-based ISA extension to support tiled matrix
    operations for energy-efficient MatMuls. With an embedded-device-friendly and
    extremely low footprint overhead, MX enhances the energy efficiency of MatMul
    by means of a small tile buffer near the FPUs, which minimizes the VRF accesses
    by storing and reusing both input and output matrix tiles. Moreover, MX reduces
    the number of instructions fetched by the scalar core, decreases the interaction
    between the scalar and vector cores, and regularizes the memory access pattern,
    further reducing power consumption. We characterized MX by implementing it on
    two multi-core clusters in a modern 12-nm technology node. With less than 3 %
    area overhead and no impact on the operating frequency, MX significantly boosts
    MatMul''s energy efficiency of a Dual-Core cluster by up to 10.9 %. In a 64-Core
    cluster and 64 × 64 matrices, performance and energy efficiency improve by 56
    % and 25 %, respectively, further pushing the FPU utilization toward the theoretical
    peak.


    ### ACKNOWLEDGMENTS


    This project has received funding from the ISOLDE project, No. 101112274, supported
    by the Chips Joint Undertaking of the European Union''s Horizon Europe''s research
    and innovation program and its members Austria, Czechia, France, Germany, Italy,
    Romania, Spain, Sweden, Switzerland.


    #### REFERENCES


    [1] B. Peccerillo, M. Mannino, A. Mondelli, and S. Bartolini, "A survey on hardware
    accelerators: Taxonomy, trends, challenges, and perspectives," *Journal of Systems
    Architecture*, vol. 129, p. 102561, 2022.


    - [2] H. Amiri and A. Shahbahrami, "Simd programming using Intel vector extensions,"
    *J. of Parallel and Distr. Comp.*, vol. 135, pp. 83–100, 2020.

    - [3] S. Deng, H. Zhao, W. Fang, J. Yin, S. Dustdar, and A. Y. Zomaya, "Edge intelligence:
    The confluence of edge computing and artificial intelligence," *IEEE Internet
    of Things Journal*, vol. 7, no. 8, pp. 7457–7469, 2020.

    - [4] N. Jouppi, C. Young, N. Patil, and D. Patterson, "Motivation for and evaluation
    of the first tensor processing unit," *IEEE Micro*, vol. 38, no. 3, pp. 10–19,
    2018.

    - [5] C. AI, "Edge TPU performance benchmarks," 2020. [Online]. Available: https://coral.ai/docs/edgetpu/benchmarks

    - [6] Y.-H. Chen, T. Krishna, J. S. Emer, and V. Sze, "Eyeriss: An energyefficient
    reconfigurable accelerator for deep convolutional neural networks," *IEEE Journal
    of Solid-State Circuits*, vol. 52, no. 1, pp. 127–138, 2017.

    - [7] V. Verma, T. Tracy II, and M. R. Stan, "EXTREM-EDGE EXtensions To RISC-V
    for Energy-efficient ML inference at the EDGE of IoT," *Sust. Comp.: Informatics
    and Systems*, vol. 35, p. 100742, 2022.

    - [8] T-Head Semiconductor, *RISC-V Matrix Multiplication Extension Specification*,
    T-Head Semiconductor, 2023. [Online]. Available: https:// github.com/T-head-Semi/riscv-matrix-extension-spec/releases/tag/v0.3.0

    - [9] M. Cavalcante, D. Wuthrich, M. Perotti, S. Riedel, and L. Benini, "Spatz:
    A ¨ compact vector processing unit for high-performance and energy-efficient shared-L1
    clusters," in *Proc. of the 41st ICCAD*. San Diego, CA, USA: IEEE/ACM, Oct. 2022.

    - [10] H. T. Kung, "Memory requirements for balanced computer architectures,"
    *SIGARCH Comp. Arch. News*, vol. 14, no. 2, p. 49–54, May 1986.

    - [11] B. Dally, "Hardware for deep learning," in *Hot Chips*, Stanford, CA, USA,
    Aug. 2023.

    - [12] M. Perotti, M. Cavalcante, N. Wistoff, R. Andri, L. Cavigelli, and L. Benini,
    "A ''New Ara'' for Vector Computing: an Open Source Highly Efficient RISC-V V
    1.0 Vector Processor Design," in *Proceedings of the 33rd IEEE Int. Conf. on ASAP*.
    Gothenburg, Sweden: IEEE, Jul. 2022.

    - [13] S. Riedel, M. Cavalcante, R. Andri, and L. Benini, "MemPool: A scalable
    manycore architecture with a low-latency shared L1 memory," *IEEE Transactions
    on Computers*, 2023, early access.

    - [14] M. Bertuletti, Y. Zhang, A. Vanelli-Coralli, and L. Benini, "Efficient
    parallelization of 5G-PUSCH on a scalable RISC-V many-core processor," in *Proc.
    of the 2023 DATE Conf.* Antwerp, Belgium: IEEE, Mar. 2023.'
- title: '**Quantum-dot Cellular Automata (QCA): A Survey**'
  abstract: ''
  keywords: '**— QCA (Quantum-dot Cellular Automata), Defect, fault model, testing.**'
  document: "#### I. INTRODUCTION\n\nContinued and fast dimensional scaling of CMOS\
    \ eventually will approach the fundamental limit [1]. Also, Short channel effects,\
    \ high power dissipation, quantum effects are limiting the further scaling of\
    \ current CMOS technology devices [2-3]. Emerging device technology can overcome\
    \ the scaling limitation in the current CMOS technology [1]. Single Electron Transistor\
    \ (SET) [4], Quantum-dot Cellular Automata (QCA) [5] and Resonant Tunneling Diodes\
    \ (RTD) [6] are some of the \"Beyond CMOS\" technologies. Among these evolving\
    \ nanotechnologies, Quantum-dot Cellular Automata is the most favorable technology\
    \ [1]. QCA is transistorless computational paradigm which can achieve device density\
    \ of 10<sup>12</sup> devices/cm<sup>2</sup> and operating speed of THz. QCA device\
    \ paradigm replaces FET based logic and exploit the quantum effects of small size.\n\
    \nQuantum-dot Cellular Automata is a mean of representing binary information on\
    \ cells, through which no current flows, and achieving device performance by the\
    \ coupling of those cells [5,7].\n\nThis paper presents the state of art survey\
    \ on QCA basics, implementation, fabrication, tools, defect characterization,\
    \ fault model and testing. Also the paper addresses the issues in some of the\
    \ methods and techniques. Further, the paper suggests the possible research area\
    \ of QCA.\n\nThe rest of the paper is organized as follows, Section II describes\
    \ the QCA background. Section III describes QCA defect analysis, fault model and\
    \ testing. The paper concludes in Section IV.\n\n#### II. QCA BACKGROUND\n\n#\
    \ A. *QCA Cell*\n\nUnlike current switching in CMOS technology, QCA encodes the\
    \ binary information as per the position of individual electrons. QCA is the array\
    \ of cells in which each cell consists of quantum dots also considered as sites\
    \ that are positioned at the corners of the square cell. The charge is localized\
    \ in the dots. Also, the cell consists of two mobile electrons that can tunnel\
    \ between the dots. Electron tunneling out of the cell is not possible due to\
    \ the potential barriers between cells. Two free electrons resides at the corners\
    \ of the cells, always diagonally due to Coulombic repulsion. Cell configurations\
    \ with four, five and six are available [5,7-8]. The four dot QCA cell with quantum\
    \ dot's number (site) is shown in Fig. 1 (a). The polarization (P) of the cell\
    \ is calculated by equation (1) [8].\n\n$$P = \\frac{\\left(\\rho\\_{\\perp} +\
    \ \\rho\\_{\\perp}\\right) - \\left(\\rho\\_{\\perp} + \\rho\\_{\\perp}\\right)}{\\\
    rho\\_{\\perp} + \\rho\\_{\\perp} + \\rho\\_{\\perp} + \\rho\\_{\\perp}}\\tag{1}$$\n\
    \nWhere *ρi* is expectation value of the number operator on site (dot) for the\
    \ ground state eigenfunction as given by equation (2). Where i is the quantum\
    \ dot's number 1, 2, 3, and 4 as depicted in Fig. 1 (a).\n\n$$\\rho\\_i = \\langle\
    \ \\mathbb{\\psi}\\_{\\,^0} \\vert \\stackrel{\\frown}{\\mathcal{U}}\\_{\\,^l}\
    \ \\vert \\mathbb{\\psi}\\_{\\,^0} \\rangle \\tag{2}$$\n\nWhere ψ 0 is ground\
    \ state of the cell and it is given in the equation (3)\n\n$$\\left|\\boldsymbol{\\\
    psi}\\_{\\;0}\\right\\rangle = \\sum\\_{j} \\boldsymbol{\\psi}^{\\;0}\\_{\\;j}\
    \ \\left|\\boldsymbol{\\phi}\\_{\\;j}\\right\\rangle \\tag{3}$$\n\nWhere φ *j*\
    \ is the *j th* basis vector and 0 ψ *j* is the coefficient of the basis vector.\
    \ It is determined by direct diagonalization of the Hamiltonian.\n\n![](_page_0_Figure_20.jpeg)\n\
    \nFig. 1. QCA cell (a) schematic (b) with polarization P = \"- 1\" (c) with polarization\
    \ P = \"+1\".\n\nElectrons are located diagonally for which cell polarization\
    \ is calculated. If the electrons are located as shown in Fig. 1(b) then according\
    \ to the equation 1, cell polarization P = - 1 is encoded as binary 0 (Logic 0).\
    \ In the same way, considering the electrons location as shown in Fig. 1(c), the\
    \ cell polarization P = +1 is encoded as binary 1 (Logic 1). Coulombic coupling\
    \ between cells causes the information flow in the QCA array.\n\nCell to Cell\
    \ response function is shown in Fig. 2. [9].\n\n![](_page_1_Figure_2.jpeg)\n\n\
    Fig. 2. The cell–cell response [9].\n\n, *ji*\n\nThe cell to cell response is\
    \ computed by solving the two particle Schrodinger equation [8]. In two-cells\
    \ *i, j* system, polarization of cell *j* is aligned with its neighbor cell *i*.\
    \ In this case cell *i* is considered as a driver. In N-cell system, for single\
    \ cell *i*, the two state model is calculated by the Hamiltonian given in equation\
    \ (4).\n\n$$\n\\hat{H}\\_i = \\begin{pmatrix}\n-\\frac{1}{2} \\sum\\_j E\\_{i,j}^k\
    \ P\\_j & -\\mathcal{Y}\\_i \\\\\n\\mathcal{Y}\\_i & \\frac{1}{2} E\\_{i,j}^k\
    \ P\\_j\n\\end{pmatrix} \\tag{4}\n$$\n\nWhere *<sup>i</sup>* γ is the tunneling\
    \ energy, *<sup>k</sup> E* , *ji* is the Kink energy between cells *i* and *j*\
    \ given by equation (5). Pj is the polarization of cell *j*. The Coulombic interaction\
    \ between two cells can be described by the kink energy Ekink. Kink energy is\
    \ the difference of electrostatic energies of two cells with opposite polarization\
    \ and same polarization [8].\n\n$$E\\_{kink}^{i,j} = E\\_{opposite}^{i,j} - E\\\
    _{same}^{i,j} \\tag{5}$$\n\n*<sup>E</sup>opposite* : Energy between cell *i* &\
    \ *j* with opposite polarization.\n\n, *ji <sup>E</sup>same* : Energy between\
    \ cell *i* & *j* with same polarization.\n\nThe electrostatic energy between two\
    \ cells is used to find the state energy. The electrostatic energy between cells\
    \ i and j is given by (6).\n\n$$E^{i,j} = \\frac{1}{4\\pi\\varepsilon\\_0\\varepsilon\\\
    _r} \\sum\\_{n=1}^{4} \\sum\\_{m=1}^{4} \\frac{q\\_n^i q\\_m^j}{\\left| r\\_n^i\
    \ - r\\_m^j \\right|} \\tag{6}$$\n\nWhere ε0 is the permittivity of free space,\
    \ εr is the relative permittivity of material, *i n q* is the charge in dot *n*\
    \ of cell *i*, *j m q* is the charge in dot m of cell *j*, *i n r* is the position\
    \ of nth dot in cell *i*, *j m r* is the position of mth dot in cell *j*, thus\
    \ *j m i n r* − *r* is the distance between nth dot in cell *i* and mth dot in\
    \ cell *j.*\n\n### B. *QCA Implementation*\n\nAccording to the material used to\
    \ realize QCA cell, the types of QCA are metal island [10, 11, 13-15], molecular\
    \ [16-20], magnetic [21-23] and semiconductor [9,24]. Among these realizations,\
    \ Molecular QCA is the most favorable, since it can operate at room temperature.\
    \ Also, from the fabrication point of view, molecular QCA is the most viable type.\
    \ Molecular QCA will be able to operate at the speed of THz with ultra low power\
    \ and extremely high device density [25].\n\n### 1. Metal QCA\n\nIn [10], the\
    \ Al-AlOx-Al tunnel junctions are fabricated on an oxidized Si substrate by a\
    \ standard electron beam– lithography and shadow evaporation techniques. This\
    \ metal QCA cell consists of four aluminum islands as dots, D1 to D4 which are\
    \ coupled with aluminum oxide tunnel junctions and capacitors. The two dots E1\
    \ and E2 are SET electrometers for sensing the output [11]. Ballistic pointcontacts,\
    \ STM method, and SET electrometer can be used to read the output. The cell is\
    \ shown in Fig. 3. [11, 12]. The metal based QCA latch is implemented and demonstrated\
    \ in [15] which was operated at the temperature of 70mK.\n\n![](_page_1_Picture_17.jpeg)\n\
    \nFig. 3. Metal QCA cell (a) Simplified Schematic Diagram of Four-dot Metal QCA\
    \ Cell [12] (b) shows the scanning electron micrograph of this QCA cell [13,14].\n\
    \n# 2. Molecular QCA\n\nMolecular QCA (mQCA) is considered as the promising implementation\
    \ for QCA circuits. It operates at room temperature with high device density,\
    \ and high operating speed. Molecular QCA cell is presented in [16-18].\n\nMolecular\
    \ implementation could also be fabricated with much higher uniformity than those\
    \ achievable with semiconductor or metal-island QCA implementations. Each molecule\
    \ acts as a cell in which redox centers acts as dots and tunneling is provided\
    \ by bridging ligands. Binary information is encoded with charge configurations.\
    \ Molecular QCA cell is shown in Fig.4. [19, 20].\n\n![](_page_2_Figure_1.jpeg)\n\
    \nFig. 4. Three states of six dot molecular QCA [16, 18]\n\nAn ab initio quantum\
    \ chemistry analysis of a molecular QCA, {(η5-C5H5)Fe(η5-C5H4)}4(η4-C4)Co(η5-C5H5)2+\
    \ molecule has been carried out in [19].\n\n#### 3. Magnetic\n\nCowburn and Welland\
    \ have developed and proposed a magnetic implementation of quantum dot cellular\
    \ automata (MQCA) [21, 22]. Unlike other types of QCA, it is based on the interaction\
    \ between magnetic nano-particles. In MQCA, information is propagated through\
    \ magnetic interactions. It is predicted that magnetic QCA can reach a speed of\
    \ about 100 MHz. The binary logic representation in magnetic quantum-dot cellular\
    \ automata is shown in Fig.5. A threeinput majority gate in magnetic QCA has been\
    \ fabricated in [23].\n\n![](_page_2_Figure_6.jpeg)\n\nFig. 5. Binary logic representation\
    \ in magnetic QCA. (a) Logic '1', (b) Logic '0', and (c) Null state.\n\n### 4.\
    \ Semiconductor\n\nSemiconductor quantum dots are nano-structured created from\
    \ a standard semiconductor material such as InAs/GaAs and GaAs/AlGaAs [9]. In\
    \ a semiconductor QCA, four semiconductor quantum-dots are placed at four different\
    \ corners of the substrate. Cell polarization is encoded as charge position, and\
    \ quantum-dot interaction relies on the electrostatic coupling. The advantage\
    \ of using semiconductor QCA is that it is based on materials that are well understood\
    \ and many fabrication techniques have been created to work with them. A possible\
    \ physics implementation of a QCA cell is shown in Fig 6. The\n\nelectric field\
    \ in the substrate is introduced by the top metal gate to deplete electrons in\
    \ the 2–D electron gas formed at the junction of the dielectric layer and the\
    \ substrate. Quantum dots are formed at locations where the metal gate is removed\
    \ to leave an exposed surface. Fabrication using GaAs/AlGaAs heterostructure with\
    \ a high-mobility twodimensional electron gas below the surface is demonstrated\
    \ in [24].\n\n![](_page_2_Figure_11.jpeg)\n\nFig. 6. The physical representation\
    \ of the semiconductor QCA cell.\n\n# *C. QCA Clocking*\n\nIn CMOS technology,\
    \ the clock is used to control the timing, mostly in sequential circuits. In QCA,\
    \ Clock provides the switching and power gain to the circuits [26]. The clock\
    \ signal is given to the each QCA cell in combinational as well as sequential\
    \ circuits to raise or lower the tunneling barrier between dots. The clock signals\
    \ are generated using the electric field. It is originated from wires buried below\
    \ the QCA surface using CMOS wires or CNT<sup>S</sup> (Carbon Nano Tube) [27].\n\
    \nIn [9], two types of switching method, abrupt and adiabatic are discussed. QCA\
    \ circuit may enter into metastable state in abrupt switching, hence it is not\
    \ suitable. Hence, adiabatic switching is used in which the cells are in the nonpolarized\
    \ state at the low barrier and allowed to change the polarization at the high\
    \ barrier.\n\nThe clocking scheme is proposed in [20]. It consists of 4 clock\
    \ signals or zones and each clock signal consists of four phases namely switch,\
    \ hold, release and relax as shown in Fig. 7. The frequency of each clock is same\
    \ with the phase shift of 90<sup>0</sup>each. One of the clock signals can be\
    \ considered the reference (phase = 0) and the others are delayed one (phase =\
    \ π/2), two (phase = π) and three (phase = 3π/2) quarters of a period as shown\
    \ in Fig. 7. Clocking is done by electrostatically switching the cell from a null\
    \ state, in which cell holds no binary information. In switch phase, the cell\
    \ state is determined by its neighbors, to a locked state, in which the state\
    \ is independent of its neighbors. The whole QCA circuit is divided into different\
    \ clock zones and each clock signal is given to its respective zone. During the\
    \ information flow, the cells of each clock zone passes through all four phases\
    \ of that respective clock zone. The information is transfer after these four\
    \ phases.\n\n![](_page_3_Figure_0.jpeg)\n\nFig. 7. Clocking scheme.\n\nIn the\
    \ switch phase, the barriers are raised and the cells become polarized according\
    \ to the polarization of their driver. This is the clock phase during which actual\
    \ computation occurs. At the end of this clock phase, the barriers are high and\
    \ it suppress any kind of tunneling. Now the cell polarization is fixed. During\
    \ the hold phase, the barriers are held at high value and the cell is now acting\
    \ as an input to the next stage. Next, in the release phase, the barriers are\
    \ lowered and the cells are allowed to relax to an unpolarized state. In the relaxed\
    \ phase, the cell barriers remain lowered, keeping the cells in an unpolarized,\
    \ neutral state. After this fourth phase, the subsystem will return to the first\
    \ clock phase and repolarize. For reliable kink-free computation the number of\
    \ cells allowed in one clock zone *E*\n\nmust be <= *TK B k e* , Where Ek is kink\
    \ energy, KB is Boltzmann constant and T is the operating temperature in degree\
    \ Kelvin. Two dimensional (2D) clocking scheme is proposed in [28**,** 29]. It\
    \ is based on the parallel execution and processing in clocking zones within a\
    \ different timing framework. Ripple Clock Schemes for QCA Circuits is proposed\
    \ in [30]. Bennett clocking of QCA has been described in [31].\n\n*Assigning four\
    \ clock signal causes delay in the final output as per the number of clocks required\
    \ for the particular circuit. So large circuit may have more delay. Clock zone\
    \ assignment is the critical part during QCA circuit layout implementation and\
    \ simulation. Hence, development of concrete and novel clocking scheme is required.\
    \ Research in advanced clocking scheme can be carried out.* \n\n#### D. *QCA Basic\
    \ Elements*\n\nThe basic building blocks of QCA are Majority voter (MV), inverter\
    \ and binary wire [32]. Also, 45<sup>0</sup> inverter chain, coplanar wire crossing\
    \ and multilayer crossover are proposed in [32].\n\nThe MV is the 3-input basic\
    \ primitives which consists of 5 cell configuration as shown in Fig. 8 (a). The\
    \ cells A, B, C are input cells, the middle cell is the device cell which has\
    \ the polarization of the majority of the inputs and right hand side cell is the\
    \ output cell which has same polarization as device cell.\n\n![](_page_3_Figure_8.jpeg)\n\
    \nFig. 8. QCA Elements (a) Majority Voter (b) MV as AND and OR gates (c) inverter\
    \ (d) binary wire\n\nThe output of the MV, as indicated by the name is determined\
    \ by the majority of its three inputs. MV implements the Boolean function F =\
    \ AB + BC + AC. Where F is the output and A, B and C are the inputs.\n\nFor example,\
    \ if two of the inputs are low, the output is low. If two of the inputs are high,\
    \ then output is high. Here high refers to the polarization state P = +1, and\
    \ low refers to the polarization state P = -1. The truth table of a majority gate\
    \ covering all possible combination of inputs and corresponding output is shown\
    \ in Table 1. 2-input AND and OR logic implementations are possible by keeping\
    \ one of the inputs of MV to fixed polarization P = -1 and P = +1 respectively\
    \ as shown in Fig. 8(b).\n\nTable 1 Truth table of majority gate\n\n| A | B |\
    \ C | F |\n|---|---|---|---|\n| 0 | 0 | 0 | 0 |\n| 0 | 0 | 1 | 0 |\n| 0 | 1 |\
    \ 0 | 0 |\n| 0 | 1 | 1 | 1 |\n| 1 | 0 | 0 | 0 |\n| 1 | 0 | 1 | 1 |\n| 1 | 1 |\
    \ 0 | 1 |\n| 1 | 1 | 1 | 1 |\n\nQCA inverter is shown in Fig. 8(c), is another\
    \ fundamental logic gate in QCA with one input and one output. It takes the input\
    \ logic and produces its complement logic on output. The truth table of it is\
    \ given in Table 2. If the input is logic high, the output will be low. If the\
    \ input is logic low, the output will be high. Many implementations are possible\
    \ for inverter but the inverter shown in Fig. 8(c) is considered as robust as\
    \ inversion take place in two paths.\n\n|    |     | Table 2 Truth table for QCA\
    \ inverter |\n|----|-----|--------------------------------------|\n| In | Out\
    \ |                                      |\n| 0  | 1   |                     \
    \                 |\n| 1  | 0   |                                      |\n\nThe\
    \ QCA binary wire shown in Fig. 8(d) is used to transfer information from one\
    \ part of the circuit to another. In a QCA circuit, a wire not only helps in information\
    \ transfer, it actually can performs some computational operation on the information\
    \ to be transferred.\n\n# E. *Fabrication*\n\nThe key issue in the QCA fabrication\
    \ is the expensive nanoscale lithography. Self-assembly method is the alternative\
    \ to it. The possibility of molecular manipulation through the use of Scanning\
    \ Acoustic Microscopy (SAMs) and supramolecular Chemistry using self-assembly\
    \ is presented in [33]. Lithographic resolution of 5 nm using a cold-development\
    \ technique is demonstrated in [34]. In [34] high-resolution Electron Beam Lithography\
    \ (EBL) and molecular lift-off is applied to pattern Creutz–Taube molecules on\
    \ the scale of a few nanometers for QCA. Molecular QCA array by electric field\
    \ from the FeIII-RuII configuration to the FeII-RuIII configuration is demonstrated\
    \ in [35]. Two ferrocene and two ferrocenium moieties as a component for charge-coupled\
    \ QCA circuits as a assembly of a symmetric square Cell is shown in [36]. Ions\
    \ and the associated counter ions can disrupt the correct flow of information\
    \ in molecular QCA. Self-doping mechanism which incorporates the counterion covalently\
    \ into the structure of a neutral molecular cell is presented in [37]. All discussed\
    \ fabrication of QCA is done as experimental point of view only.\n\n# F. *Tools*\n\
    \nThe research has been done in QCA simulation and synthesis tools. The available\
    \ tools are MAQUINAS [38], QBert [39], QCA-LG [40], and QCADesigner [41]. MAQUINAS\
    \ and QCADesigner are rely on a simulation engine that solves the Schrödinger\
    \ equation for modeling physical interactions within the considered set of cells\
    \ in the circuit. MAQUINAS uses adiabatic switching and time independent Schrodinger\
    \ equation is solved to calculate polarization state of each cell. Fast simulator\
    \ for QCA digital logic, QBART is developed in [39]. Automatic layout generation\
    \ tool QCA-LG is developed in [40]. QCADesigner [41] is the most widely used QCA\
    \ layout simulator. It uses two simulation engines namely bistable approximation\
    \ and coherence vector. All versions of QCADesigner are available on [42]. The\
    \ library of basic QCA elements in Hardware Description Language, Verilog (HDLQ)\
    \ is proposed in [43]. This library also incorporated with fault injection and\
    \ bidirectionality. In [44] a SPICE macro model for QCA has been proposed and\
    \ experimentally verified. Polarization error and power estimation tool for QCA,\
    \ QCAPro is proposed in [45]. CAD tool, HDLM for magnetic QCA, based on Verilog\
    \ HDL is proposed in [46]. HDL models for a magnetic QCA cell and building blocks\
    \ are proposed which ensure magnetization, clocking, and signal propagation.\n\
    \n Synthesis is the significant process in any digital design flow. Synthesis\
    \ methods for digital logics into QCA basic primitives are proposed in [47-51].\
    \ Zhang et al [48] have proposed AND/OR-based logic synthesis for QCA combinational\
    \ circuit. This method reduces the number of MVs to compute three variable Boolean\
    \ functions by simplifying the conversion of Sum-of-Product (SOP) expressions\
    \ into QCA majority logic. Zhang et al [48] have proposed the logic synthesis\
    \ tool MALS for MV-based logic. In [48], each decomposed subcircuits using maximum\
    \ four MVs are implemented. In [49] efficient decomposition scheme is introduced\
    \ which removes the redundancies produced in the process of converting a decomposed\
    \ network into a majority network. Minimal majority gate mapping with a Majority\
    \ expression Look-Up Table (MLUT) based algorithm is developed in [50].\n\nAbove\
    \ discussed methods for logic synthesis are limited for small Boolean functions\
    \ and so fundamental algebra for nmajority is developed in [51]. Recently, heuristic\
    \ based majority/minority logic synthesis methodology is proposed in [52]. More\
    \ research is required for the development of synthesis methods and tools.\n\n\
    *Research in QCA CAD tool development needs much attention and can be carried\
    \ out. As of now no commercial simulation and synthesis tool is available.* \n\
    \n# III. DEFECT, FAULT, FAULT MODELS AND TESTING OF QCA DEVICES AND CIRCUITS\n\
    \n# A. *Defects*\n\nQCA devices are prone to defects due to the nanoscale. The\
    \ survey of defect characterization has been carried out and presented in [53].\
    \ QCA defect classification is shown in Fig. 9. Defects during molecular QCA manufacturing\
    \ can occur in two phases namely synthesis and deposition. In the synthesis phase,\
    \ the individual cells (molecule) are manufactured and in the deposition phase\
    \ the cells are placed in a specific location on the surface. Possibility of defects\
    \ in metal and molecular QCA implementations are discussed in [54]. It is stated\
    \ that wrong dot size or shape is possible because the targeted region is either\
    \ under or over exposed during EBL. Single Electron Fault (SEF) in QCA are characterized\
    \ and analysed in [55].\n\nAs mentioned in [56], occurrence of missing and extra\
    \ dot or electrons is very less due to the ease of purification of small inorganic\
    \ molecules. It is considered that these defects causes fatal errors and are easy\
    \ to detect. Perhaps defects like cell misalignment, rotation, displacement, missing,\
    \ addition occurs in the deposition phase are to be analyzed.\n\n![](_page_5_Figure_0.jpeg)\n\
    \nFig. 9. QCA Defect Classification [48].\n\n# 1. Cell Misalignment Defects\n\n\
    In misalignment defect, the cell of the defect free QCA device get displaced horizontally\
    \ from its original position. If the cell is displace laterally then it is considered\
    \ as cell displacement defects. Initially, Tahoori et al [57, 58] described the\
    \ cell misalignment and displacement defects in basic QCA primitive MV, binary\
    \ wire and inverter chain. Extensive simulation analysis of cell misalignment\
    \ and displacement defects for MV is carried out using QCADesigner.\n\nThe displacement\
    \ and misalignment of input cells of MV is shown Fig. 10 and 11 respectively [57].\
    \ Displacement defect in double inverter chain and binary wire is also reported\
    \ in [57]. It observed that the horizontal cell input cell (input B) is most dominant\
    \ cell. In MV, cell misalignment by the distance of greater or equal to half of\
    \ the cell size causes undesired output. The cell displacement defect has less\
    \ catastrophic effects on the functionality of an MV compared to cell misalignment\
    \ defects. It is analysed that the double binary wire is more defect tolerant\
    \ than the inverter chain in case of cell displacement defects.\n\nKink energy\
    \ discussed earlier can be used to calculate the output cell polarization of any\
    \ QCA device in case of cell misalignment and displacement defects.\n\nThe misalignment\
    \ defects have more catastrophic effect on MV functionality compared to displacement\
    \ defect, because if any cell get misaligned in right or left direction then the\
    \ kink energy calculation differs and the output cell polarization may be undesired\
    \ depending upon the distance. In cell displacement defect in MV, if cell get\
    \ displaced by the distance less than or equal to the radius of effect then the\
    \ output remains unaffected. Radius of effect is a distance from the centre of\
    \ one cell to another cell which interacted with each other. Cells are not interacting\
    \ if a cell to cell distance is more than the radius of effect. The radius of\
    \ effect can be set during the simulation in QCADesigner tool.\n\n![](_page_5_Figure_8.jpeg)\n\
    \nFig. 10. Displacement in MV. (a) Fault free. (b) Displace A. (c) Displace B.\
    \ (d) Displace all inputs and output. (e) Displace all inputs. (f) Displace A\
    \ and B.\n\n![](_page_5_Figure_10.jpeg)\n\nFig. 11. Misalignments in MV (a) A\
    \ misalignment (b) A misalignment (c) C misalignment (d) C misalignment (e) A,\
    \ C misalignment (f) A, C misalignment (g) B misalignment.\n\n*Yongqiang et al*\
    \ [59] examined the effect of cell movement in horizontal and vertical directions\
    \ at the same time (twodimensions) for QCA fundamental devices MV, Inverter and\
    \ wire. *Gabriel et al* [60] analyzed the behavior of QCA building blocks under\
    \ the influence of random cell displacement defect. In [61], cell displacements\
    \ of interconnect is analysed and maximum allowable displacement is determined.\n\
    \n#### 2. Cell Rotation Defects\n\nYang et al [62] analysed the behaviour of QCA\
    \ devices in presence of cell rotation. Further they have presented model for\
    \ cell rotation effect using modified coherence vector formalism for permissible\
    \ rotational angle.\n\n3. Missing and Additional Cell Defects\n\nDuring the lithography\
    \ process, improper removal of resist causes extra cell attachment (additional)\
    \ or missing cell defects. These defects also depends on the chemical\n\ncompound\
    \ used during the lithography. Modelling of additional cell can be done by adding\
    \ extra cell to the periphery of device and circuit. In the same way modelling\
    \ of missing cell can be done by removing the cell from device or circuit [56].\
    \ *Dysart et al* [56] analyzed the effects of missing cell defects in QCA wire\
    \ assembled by a molecular implementation.\n\n*Momenzadeh et al* [63] modeled\
    \ single missing and additional cell defects in molecular QCA devices like Majority\
    \ Voter (MV), inverter, fanout and L-Shaped. Also fault set corresponding to single\
    \ missing and additional cell defects is proposed in [63] and mentioned in Table\
    \ 3. It is observed that binary wire is less prone to the single missing cell\
    \ defects. Additional cell does not alter the functionality of QCA devices except\
    \ inverter for some cases. Huang et al [64] presented the single missing and additional\
    \ cell defect characterization of sequential QCA circuits which is based on molecular\
    \ QCA.\n\nTable 3 Fault Set caused by single missing cell [63]\n\n| Device   \
    \         | Fault Set       |  |\n|-------------------|-----------------|--|\n\
    | Majority<br>Voter | S_a_B           |  |\n| (MV)              | Maj (A', B,\
    \ C') |  |\n| Inverter          | S_a_A           |  |\n| L-shaped wire     |\
    \ S_a_A'          |  |\n| Fanout            | S_a_A'          |  |\n\nSimulation\
    \ based single cell omission defect in MV, binary wire and wire crossing for 1-bit\
    \ full adder has been reported in [57]. It is analyzed that the input cell B of\
    \ MV is dominating cell. Output of MV doesn't altered due to missing of input\
    \ cell B. Inversion of input A and C takes place in case of device missing cell.\
    \ Single additional and missing cell defects in QCA sequential circuit are analyzed\
    \ in [65].\n\nAgain the kink based energy calculation can be applied to calculate\
    \ the polarization of device output cell in case of single missing and additional\
    \ cell defect.\n\n# 4. Defects in Clock Circuitry\n\nPossibility of defect occurrence\
    \ in clock circuitry is discussed in [66]. Phase shifts can result from manufacturing\
    \ variations in each of the four required clock sources or from uneven path lengths\
    \ [66]. The impact of QCA device scaling on defects is presented in [67]. Liu\
    \ et al [68] explored the behavior of metal-dot QCA systems under stressed caused\
    \ by the high temperature operation, high speed operation, and random variation\
    \ in parameter values. Defect caused by fabrication variations in Magnetic Quantum-dot\
    \ Cellular Automata at device, circuit, and architectural level is analyzed in\
    \ [69]. Information-theoretic approach for investigation relationship between\
    \ defect tolerance and redundancy in QCA devices is presented in [70].\n\nFault\
    \ tolerant QCA devices and circuits are presented in [71-77]. Fault analysis of\
    \ QCA combination circuit at layout and logic level is presented in [78]. Fabrication\
    \ defects of a real molecular QCA wire built with ad hoc synthesized bisferrocene\
    \ Molecules are analyzed in [79]. Also, in [80], the possible defects and causes\
    \ of faults for a molecular QCA device are identified. The process variations\
    \ effects in terms of yield and output error Rate for QCA based NanoMagnet Logic\
    \ has been studied in [81].\n\n*Apart from the available defect, multiple missing\
    \ or additional cell defects can occur and possibly must be analyzed. Since self-assembly\
    \ fabrication process for molecular QCA is prone to defects, in near future if\
    \ QCA circuit and system exists, the key issue, defects must be addressed to avoid\
    \ the failure of it. Looking into the reliability of QCA, extreme need for fault\
    \ tolerant circuit is required. Also analysis of QCA oriented defects, its modeling\
    \ and development of corresponding fault model is required.* \n\n# *B. Faults,\
    \ Fault models and Testing*\n\nThe present-day technology uses single stuck at\
    \ fault model (SSF) where the line is observed to be permanently stuck at either\
    \ logic 0 (S\\_A\\_0) or logic 1 (S\\_A\\_1). The existing stuck-at-fault model\
    \ can provide test generation for QCA circuits. Also, conventional ATPG tool can\
    \ be exploited for QCA [58]\n\nInitially the logic level testing of QCA is presented\
    \ in [57]. Following properties for QCA MV with A, B, C as input lines of MV and\
    \ Z as output line are investigated in [57-58, 82]\n\n- For MV with input values\
    \ *a, b* and *c* and output *z,* if all inputs are flipped, *abc→a΄b΄c΄*, then\
    \ the output will also be flipped, i.e. *z→z΄.*\n- If there is inversion at any\
    \ input and/or the output of the MV, above property still holds\n- The stuck-at-v\
    \ fault on any input or output line of the voter is detectable by *abc* if and\
    \ only if the stuck-at-*v΄* fault on that line is detectable by *a΄b΄c΄*\n- If\
    \ there are some inversions at any inputs and/or the output of the MV, then above\
    \ property still holds\n- Any vector that detects an input S\\_A\\_0 fault will\
    \ also detect an output S\\_A\\_0 fault\n\nThe stuck at test set is developed\
    \ to test all the simulated defects in MV [57]. It shown that the few test vector\
    \ set is effectively detects all the simulated defects in MV with 100% test coverage.\
    \ Design for Testability (DFT) scheme for QCA circuit is proposed in [57-58,82].\
    \ Two lines used to make MV as AND and OR gate can be viewed as control lines\
    \ to implement DFT scheme in the network of MVs acting as AND and OR gates.\n\n\
    First comprehensive test generation methodology based on Boolean satisfiability\
    \ (SAT) for combinational QCA circuits is proposed in [83,84]. The authors have\
    \ shown that SSF set is not sufficient to detect all the simulated QCA defects\
    \ in MV sothat further test generation is required for additional test vectors.\
    \ Proposed ATPG generated test vectors are given to the fault simulator, the track\
    \ on test vector applied to each MV is kept.\n\nIf there exists a majority gate\
    \ which does not receive a 100% defect test set, additional test generation is\
    \ performed. Once all the defects have been covered, a test set containing the\
    \ SSF test set and additional QCA vectors is obtained. The proposed ATPG is tested\
    \ on MCNC *(*Microelectronics Center of North Carolina*)* benchmark circuits.\
    \ All of the benchmarks were first synthesized into multi-level majority networks\
    \ using the logic synthesis tool MALS [48]. The bridging faults in QCA interconnects\
    \ are also targeted in [83,84].\n\n*Faisal Karim et al* [85] also developed the\
    \ combinational ATPG using extended version of PODEM algorithm for majority and\
    \ minority logic networks mostly targeting QCA. A genetic algorithm was used to\
    \ fill-in the unspecified values in the test patterns produced by the ATPG in\
    \ order to achieve compaction on the final test set size. The modified PODEM algorithm\
    \ was tested on a set of MCNC benchmark circuits. Probability based controllability\
    \ and observability approaches were taken into consideration to guide this ATPG.\n\
    \n*Probability based testability approach is applicable to the fanout-free combinational\
    \ circuits. It fails for the circuits containing fanout points due to the reconvergent\
    \ path. So, Sandia Controllability Observability Analysis Program (SCOAP) based\
    \ testability approach can be the alternative to probability based testability\
    \ approach.* \n\nBasic fault model for single input missing cell deposition defect\
    \ for QCA MV is developed in [86]. In [86], test generation for single input missing\
    \ cell deposition defects in the QCA circuit consists of MV as AND and OR gate\
    \ is carried out using the proposed properties and corresponding fault model.\
    \ The proposed properties are as follows:\n\n**Property 1**. If S\\_A\\_B fault\
    \ is present at the output of MV as AND gate then assign A=0 or B' and B=1.\n\n\
    **Property 2.** If S\\_A\\_B fault is present at the output of MV as OR gate then\
    \ assign A=1 or B' and B=0.\n\n**Property 3.** While propagating the values at\
    \ the output of MV as AND or OR gate, justify remaining fanin by the noncontrolling\
    \ value of the gate.\n\nIn [87], test vectors to exhaustively test the functionality\
    \ of any 3-input majority gate and arithmatic circuit are generated to find crosstalk\
    \ which is modelled as dominant bringing faults. Method for In-Circuit-Testing\
    \ of QCA circuits is shown in [88].\n\n*Available literature suggested the application\
    \ of test generation method for current silicon technology to the QCA. Different\
    \ type of test generation for QCA oriented fault models can be possible and explored.\
    \ Also the conventional test generation algorithms and fault models can be explored\
    \ for QCA combinational and sequential circuits.* \n\n# IV. CONCLUSION AND DISCUSSION\n\
    \nThe systematic survey on QCA, its defect analysis and testing method is carried\
    \ out in this paper. Solid need of CAD tools for QCA circuit simulation and synthesis\
    \ is required.\n\nAs occurrence of defects are possible in QCA devices due to\
    \ the nanoscale nature, this paper more emphasizes on the QCA defect, fault model\
    \ and testing.\n\nMostly the simulation based defect analysis is carried out in\
    \ the referred literature. Defect analysis through mathematical representation\
    \ is required. Apart from the available defects in synthesis and deposition phase,\
    \ defects like multiple missing cells, multiple additional cell during the deposition\
    \ phase must be looked upon.\n\nNovel QCA oriented fault model must be evolved\
    \ to develop the test generation methods. Perhaps, the available test generation\
    \ algorithms can also be explored.\n\nThis paper will be useful to understand\
    \ the QCA in depth for the beginners. Also, it will be helpful for the researcher\
    \ to find the various area to work upon.\n\n#### REFERENCES\n\n[1] \"International\
    \ Technology Roadmap for Semiconductors (ITRS)\", 2015 Edition,\n\nhttps://www.semiconductors.org/clientuploads/Research\\\
    _Technology/I TRS/2015/6\\_2015 ITRS 2.0 Beyond CMOS.pdf\n\n- [2] Peercy, Paul\
    \ S. \"The drive to miniaturization.\" *Nature* 406, no. 6799 (2000): 1023-1026.\n\
    - [3] Meindl, James D. \"Beyond Moore's law: The interconnect era.\" *Computing\
    \ in Science & Engineering* 5, no. 1 (2003): 20-24.\n- [4] Likharev, Konstantin\
    \ K. \"Single-electron devices and their applications.\" *Proceedings of the IEEE*\
    \ 87, no. 4 (1999): 606-632.\n- [5] Lent, Craig S., and P. Douglas Tougaw. \"\
    Lines of interacting quantum‐ dot cells: A binary wire.\" *Journal of applied\
    \ Physics* 74, no. 10 (1993): 6227-6233.\n- [6] Chen, Kevin J., Koichi Maezawa,\
    \ and Masafumi Yamamoto. \"InP-based high-performance monostable-bistable transition\
    \ logic elements (MOBILEs) using integrated multiple-input resonant-tunneling\
    \ devices.\" *IEEE Electron Device Letters* 17, no. 3 (1996): 127-129.\n- [7]\
    \ Lent, Craig S., P. Douglas Tougaw, Wolfgang Porod, and Gary H. Bernstein. \"\
    Quantum cellular automata.\" *Nanotechnology* 4, no. 1 (1993): 49-57.\n- [8] Lent,\
    \ Craig S., P. Douglas Tougaw, and Wolfgang Porod. \"Quantum cellular automata:\
    \ the physics of computing with arrays of quantum dot molecules.\" In *Physics\
    \ and Computation, 1994. PhysComp'94, Proceedings. Workshop on*, pp. 5-13. IEEE,\
    \ 1994.\n- [9] Lent, Craig S., and P. Douglas Tougaw. \"A device architecture\
    \ for computing with quantum dots.\" *Proceedings of the IEEE* 85, no. 4 (1997):\
    \ 541-557.\n- [10]Orlov, A. O., I. Amlani, G. H. Bernstein, C. S. Lent, and G.\
    \ L. Snider. \"Realization of a functional cell for quantum-dot cellular automata.\"\
    \ *Science* 277, no. 5328 (1997): 928-930.\n- [11]Amlani, Islamshah, Alexei O.\
    \ Orlov, Gregory L. Snider, Craig S. Lent, and Gary H. Bernstein. \"External charge\
    \ state detection of a double-dot system.\" *Applied physics letters* 71, no.\
    \ 12 (1997): 1730-1732.\n\n[12]Lombardi, Fabrizo, and Jing Huang. Design and test\
    \ of digital circuits by quantum-dot cellular automata. Artech House, Inc., 2007.\n\
    \n- [13]Amlani, Islamshah, Alexei O. Orlov, Gregory L. Snider, Craig S. Lent,\
    \ and Gary H. Bernstein. \"Demonstration of a functional quantum-dot cellular\
    \ automata cell.\" *Journal of Vacuum Science & Technology B: Microelectronics\
    \ and Nanometer Structures Processing, Measurement, and Phenomena* 16, no. 6 (1998):\
    \ 3795-3799.\n- [14]Amlani, Islamshah, Alexei O. Orlov, Gregory L. Snider, Craig\
    \ S. Lent, and Gary H. Bernstein. \"Demonstration of a six-dot quantum cellular\
    \ automata system.\" *Applied Physics Letters* 72, no. 17 (1998): 2179-2181.\n\
    - [15]Orlov, Alexei O., Ravi K. Kummamuru, Rajagopal Ramasubramaniam, Geza Toth,\
    \ Craig S. Lent, Gary H. Bernstein, and Gregory L. Snider. \"Experimental demonstration\
    \ of a latch in clocked quantum-dot cellular automata.\" *Applied Physics Letters*\
    \ 78, no. 11 (2001): 1625-1627.\n- [16]Lieberman, Marya, Sudha Chellamma, Bindhu\
    \ Varughese, Yuliang Wang, Craig Lent, Gary H. Bernstein, Gregory Snider, and\
    \ Frank C. Peiris. \"Quantum‐dot cellular automata at a molecular scale.\" *Annals\
    \ of the New York Academy of Sciences* 960, no. 1 (2002): 225-239.\n- [17]Lent,\
    \ Craig S., Beth Isaksen, and Marya Lieberman. \"Molecular quantum-dot cellular\
    \ automata.\" *Journal of the American Chemical Society* 125, no. 4 (2003): 1056-1063.\n\
    - [18]Blair, Enrique P., and Craig S. Lent. \"Quantum-dot cellular automata: an\
    \ architecture for molecular computing.\" In *Simulation of Semiconductor Processes\
    \ and Devices, 2003. SISPAD 2003. International Conference on*, pp. 14-18. IEEE,\
    \ 2003.\n- [19]Lu, Yuhui, and Craig S. Lent. \"Theoretical study of molecular\
    \ quantumdot cellular automata.\" *Journal of Computational Electronics* 4, no.\
    \ 1-2 (2005): 115-118.\n- [20]Lent, Craig S., and Beth Isaksen. \"Clocked molecular\
    \ quantum-dot cellular automata.\" *IEEE Transactions on Electron Devices* 50,\
    \ no. 9 (2003): 1890-1896.\n- [21]Cowburn, R. P., and M. E. Welland. \"Room temperature\
    \ magnetic quantum cellular automata.\" *Science* 287, no. 5457 (2000): 1466-1468.\n\
    - [22]Bernstein, Gary H., Alexandra Imre, V. Metlushko, A. Orlov, L. Zhou, L.\
    \ Ji, György Csaba, and Wolfgang Porod. \"Magnetic QCA systems.\" *Microelectronics\
    \ Journal* 36, no. 7 (2005): 619-624.\n- [23]Imre, Alexandra, G. Csaba, L. Ji,\
    \ A. Orlov, G. H. Bernstein, and W. Porod. \"Majority logic gate for magnetic\
    \ quantum-dot cellular automata.\" *Science* 311, no. 5758 (2006): 205-208.\n\
    - [24]Perez-Martinez, F., I. Farrer, D. Anderson, G. A. C. Jones, D. A. Ritchie,\
    \ S. J. Chorley, and C. G. Smith. \"Demonstration of a quantum cellular automata\
    \ cell in a Ga As⁄ Al Ga As heterostructure.\" *Applied physics letters* 91, no.\
    \ 3 (2007): 032102.\n- [25]Lent, Craig S., and Gregory L. Snider. \"The development\
    \ of quantumdot cellular automata.\" In *Field-Coupled Nanocomputing*, pp. 3-20.\
    \ Springer Berlin Heidelberg, 2014.\n- [26]Toth, Geza, and Craig S. Lent. \"Quasiadiabatic\
    \ switching for metalisland quantum-dot cellular automata.\" *Journal of Applied\
    \ physics* 85, no. 5 (1999): 2977-2984.\n- [27]Frost, Sarah E., Timothy J. Dysart,\
    \ Peter M. Kogge, and C. S. Lent. \"Carbon nanotubes for quantum-dot cellular\
    \ automata clocking.\" In *Nanotechnology, 2004. 4th IEEE Conference on*, pp.\
    \ 171-173. IEEE, 2004.\n- [28]Vankamamidi, Vamsi, Marco Ottavi, and Fabrizio Lombardi.\
    \ \"Twodimensional schemes for clocking/timing of QCA circuits.\" *IEEE Transactions\
    \ on Computer-Aided Design of Integrated Circuits and Systems* 27, no. 1 (2008):\
    \ 34-44.\n- [29]Vankamamidi, Vamsi, Marco Ottavi, and Fabrizio Lombardi. \"Clocking\
    \ and cell placement for QCA.\" In *Nanotechnology, 2006. IEEE-NANO 2006. Sixth\
    \ IEEE Conference on*, vol. 1, pp. 343-346. IEEE, 2006.\n- [30]Purohit, Prafull.\
    \ *Ripple clock schemes for quantum-dot cellular automata circuits*. Rochester\
    \ Institute of Technology, 2012.\n- [31]Lent, Craig S., Mo Liu, and Yuhui Lu.\
    \ \"Bennett clocking of quantumdot cellular automata and the limits to binary\
    \ logic scaling.\" *Nanotechnology* 17, no. 16 (2006): 4240.\n- [32]Tougaw, P.\
    \ Douglas, and Craig S. Lent. \"Logical devices implemented using quantum cellular\
    \ automata.\" *Journal of Applied physics* 75, no. 3 (1994): 1818-1825.\n- [33]Parviz,\
    \ B. Amir, Declan Ryan, and George M. Whitesides. \"Using selfassembly for the\
    \ fabrication of nano-scale electronic and photonic\n\ndevices.\" *IEEE transactions\
    \ on advanced packaging* 26, no. 3 (2003): 233-241.\n\n- [34]Hu, Wenchuang, Koshala\
    \ Sarveswaran, Marya Lieberman, and Gary H. Bernstein. \"High-resolution electron\
    \ beam lithography and DNA nanopatterning for molecular QCA.\" *IEEE Transactions\
    \ on Nanotechnology* 4, no. 3 (2005): 312-316.\n- [35]Niemier, Michael T., and\
    \ Peter M. Kogge. \"The\" 4-diamond circuit\"-a minimally complex nano-scale computational\
    \ building block in qca.\" In *VLSI, 2004. Proceedings. IEEE Computer society\
    \ Annual Symposium on*, pp. 3-10. IEEE, 2004.\n- [36]Jiao, J., G. L. Long, F.\
    \ Grandjean, A. M. Beatty, and T. P. Fehiner. \"Building Blocking for the Molecular\
    \ Expressionof QCA, Isolation and Characterization of a Covalently Bounded Square\
    \ Array of two Ferrocenium and TwoFerrocene Complexes.\" *Journal of the Am. Chem.\
    \ Society (JACS Communications)* 125, no. 25 (2003): 7522-7523.\n- [37]Lu, Yuhui,\
    \ and Craig Lent. \"Self-doping of molecular quantum-dot cellular automata: mixed\
    \ valence zwitterions.\" *Physical Chemistry Chemical Physics* 13, no. 33 (2011):\
    \ 14928-14936.\n- [38]Blair, E. P., \"Tools for the Design and Simulation of Clocked\
    \ Molecular Quantum-dot Cellular Automata Circuits,\" Master's thesis, University\
    \ of Notre Dame, Department of Electrical Engineering, 2003.\n- [39]Niemier, M.,\
    \ M. Kontz, and P. Kogge, \"A design of and Design tools for A novel quantum dot\
    \ based microprocessor*,\" in Proc. of the 37th Annual Design Automation Conference*,\
    \ pp. 227–232, 2000.\n- [40]Teodósio, Tiago, and Leonel Sousa. \"QCA-LG: A tool\
    \ for the automatic layout generation of QCA combinational circuits.\" In *Norchip,\
    \ 2007*, pp. 1-5. IEEE, 2007.\n- [41]K. Walus, T. Dysart, G. A. Jullien, and R.\
    \ A. Budiman, \"QCADesigner: A rapid design and simulation tool for quantum-dot\
    \ cellular automata,\" *IEEE Trans. Nanotechnology,* vol. 3, no. 1, pp. 26–31,\
    \ Mar. 2004.\n- [42]http://waluslab.ece.ubc.ca/qcadesigner/qca-designer-downloads/\n\
    - [43]M. Ottavi, L. Schiano, and F. Lombardi, \"HDLQ: A HDL environment for QCA\
    \ design,\" *ACM J. Emerging Technol. Comput. Syst.,* vol. 2, no. 4, pp. 243–261,\
    \ Oct. 2006.\n- [44]Tang, R., F. Zhang, and Y. B. Kim, \"Quantum-Dot Automata\
    \ SPICE Macro Model\", *ACM Great Lake Symposium on VLSI 2005*, 2005, pp. 108-111.\n\
    - [45]Srivastava, S., et al., \"QCAPro-An Error-Power Estimation Tool for QCA\
    \ Circuit Design*,\" in Proc. of the IEEE International Symposium on Circuits\
    \ and Systems,* pp. 2377–2380, 2011.\n- [46]Ottavi, Marco, et al. \"An HDL Model\
    \ of Magnetic Quantum-Dot Cellular Automata Devices and Circuits.\" Nanoelectronic\
    \ Device Applications Handbook\n- [47]Zhang, R., et al., \"A method of majority\
    \ logic reduction for quantum cellular automata,\" *IEEE Transactions on Nanotechnology,*\
    \ vol 3 (4), pp. 443-450, 2004.\n- [48]Zhang, R., P. Gupta, N. K. Jha, \"Synthesis\
    \ of majority and minority networks and its application to QCA, TPL, and SET based\
    \ nanotechnologies\", *IEEE Conference on VLSI Design held jointly with International\
    \ Conference on Embedded Systems Design,* 2005, pp. 229- 234.\n- [49]Kong, Kun,\
    \ Yun Shang, and Ruqian Lu. \"An optimized majority logic synthesis methodology\
    \ for quantum-dot cellular automata.\" *IEEE Transactions on Nanotechnology* 9,\
    \ no. 2 (2010): 170-183.\n- [50]Wang, Peng, Mohammed Y. Niamat, Srinivasa R. Vemuru,\
    \ Mansoor Alam, and Taylor Killian. \"Synthesis of Majority/Minority Logic Networks.\"\
    \ *IEEE Transactions on Nanotechnology* 14, no. 3 (2015): 473-483.\n- [51]Devadoss,\
    \ Rajeswari, Kolin Paul, and M. Balakrishnan. \"MajSynth: Ann-inputMajorityAlgebrabasedLogicSynthesisToolfor\
    \ QuantumdotCellularAutomata.\" (2015).\n- [52]Mishra, Vipul Kumar, and Himanshu\
    \ Thapliyal. \"Heuristic Based Majority/Minority Logic Synthesis for Emerging\
    \ Technologies.\" In *VLSI Design and 2017 16th International Conference on Embedded\
    \ Systems (VLSID), 2017 30th International Conference on*, pp. 295-300. IEEE,\
    \ 2017.\n- [53]Dhare, Vaishali, and Usha Mehta. \"Defect characterization and\
    \ testing of QCA devices and circuits: A survey.\" In *VLSI Design and Test (VDAT),\
    \ 2015 19th International Symposium on*, pp. 1-2. IEEE, 2015.\n\n[54]Dysart, Timothy\
    \ J., and Peter M. Kogge. \"Strategy and prototype tool for doing fault modeling\
    \ in a nano-technology.\" In *Nanotechnology, 2003. IEEE-NANO 2003. 2003 Third\
    \ IEEE Conference on*, vol. 1, pp. 356-359. IEEE, 2003.\n\n- [55]Mukherjee, Rijoy,\
    \ et al. \"Characterization and analysis of single electron fault of QCA primitives.\"\
    \ *Microelectronics, Computing and Communications (MicroCom), 2016 International\
    \ Conference on*. IEEE, 2016.\n- [56]Dysart, Timothy J., Peter M. Kogge, Craig\
    \ S. Lent, and Mo Liu. \"An analysis of missing cell defects in quantum-dot cellular\
    \ automata.\" In *IEEE International Workshop on Design and Test of Defect-Tolerant\
    \ Nanoscale Architectures (NANOARCH)*, vol. 3, pp. 1-8. 2005.\n- [57]Tahoori,\
    \ Mehdi Baradaran, Mariam Momenzadeh, Jin Huang, and Fabrizio Lombardi. \"Defects\
    \ and faults in quantum cellular automata at nano scale.\" In *VLSI Test Symposium,\
    \ 2004. Proceedings. 22nd IEEE*, pp. 291-296. IEEE, 2004.\n- [58]Tahoori, Mehdi\
    \ B., Jing Huang, Mariam Momenzadeh, and Fabrizio Lombardi. \"Testing of quantum\
    \ cellular automata.\" *IEEE Transactions on Nanotechnology* 3, no. 4 (2004):\
    \ 432-442.\n- [59]Zhang, Yongqiang, Hongjun Lv, Shuai Liu, Yunlong Xiang, and\
    \ Guangjun Xie. \"Defect-tolerance analysis of fundamental quantum-dot cellular\
    \ automata devices.\" *The Journal of Engineering* 1, no. 1 (2015).\n- [60]Schulhof,\
    \ Gabriel, Konrad Walus, and Graham A. Jullien. \"Simulation of random cell displacements\
    \ in QCA.\" *ACM Journal on Emerging Technologies in Computing Systems (JETC)*\
    \ 3, no. 1 (2007): 2.\n- [61]Karim, Faizal, and Konrad Walus. \"Characterization\
    \ of the displacement tolerance of QCA interconnects.\" *Design and Test of Nano\
    \ Devices, Circuits and Systems, 2008 IEEE International Workshop on*. IEEE, 2008.\n\
    - [62]Yang, Xiaokuo, Li Cai, Shuzhao Wang, Zhuo Wang, and Chaowen Feng. \"Reliability\
    \ and performance evaluation of QCA devices with rotation cell defect.\" *IEEE\
    \ Transactions on Nanotechnology* 11, no. 5 (2012): 1009-1018.\n- [63]Momenzadeh,\
    \ Mariam, Marco Ottavi, and Fabrizio Lombardi. \"Modeling QCA defects at molecular-level\
    \ in combinational circuits.\" In *Defect and Fault Tolerance in VLSI Systems,\
    \ 2005. DFT 2005. 20th IEEE International Symposium on*, pp. 208-216. IEEE, 2005.\n\
    - [64]Huang, Jing, Mariam Momenzadeh, and Fabrizio Lombardi. \"Analysis of missing\
    \ and additional cell defects in sequential quantum-dot cellular automata.\" *INTEGRATION,\
    \ the VLSI journal* 40, no. 4 (2007): 503-515.\n- [65]Momenzadeh, Mariam, Jing\
    \ Huang, and Fabrizio Lombardi. \"Defect characterization and tolerance of QCA\
    \ sequential devices and circuits.\" In *Defect and Fault Tolerance in VLSI Systems,\
    \ 2005. DFT 2005. 20th IEEE International Symposium on*, pp. 199-207. IEEE, 2005.\n\
    - [66]Ottavi, Marco, Hamid Hashempour, Vamsi Vankamamidi, Faizal Karim, Konrad\
    \ Walus, and André Ivanov. \"On the error effects of random clock shifts in quantum-dot\
    \ cellular automata circuits.\" In *Defect and Fault-Tolerance in VLSI Systems,\
    \ 2007. DFT'07. 22nd IEEE International Symposium on*, pp. 487-498. IEEE, 2007.\n\
    - [67]Huang, Jing, Mariam Momenzadeh, Mehdi Baradaran Tahoori, and Fabrizio Lombardi.\
    \ \"Defect characterization for scaling of QCA devices [quantum dot cellular automata].\"\
    \ In *Defect and Fault Tolerance in VLSI Systems, 2004. DFT 2004. Proceedings.\
    \ 19th IEEE International Symposium on*, pp. 30-38. IEEE, 2004.\n- [68]Liu, Mo,\
    \ and Craig S. Lent. \"Reliability and defect tolerance in metallic quantum-dot\
    \ cellular automata.\" *Journal of Electronic Testing* 23, no. 2- 3 (2007): 211-218.\n\
    - [69]Niemier, Michael, Michael Crocker, and X. Sharon Hu. \"Fabrication variations\
    \ and defect tolerance for nanomagnet-based QCA.\" In *Defect and Fault Tolerance\
    \ of VLSI Systems, 2008. DFTVS'08. IEEE International Symposium on*, pp. 534-542.\
    \ IEEE, 2008.\n- [70]Dai, Jianwei, Lei Wang, and Fabrizio Lombardi. \"An informationtheoretic\
    \ analysis of quantum-dot cellular automata for defect tolerance.\" *ACM Journal\
    \ on Emerging Technologies in Computing Systems (JETC)* 6, no. 3 (2010): 9.\n\
    - [71]Wei, Tongquan, Kaijie Wu, Ramesh Karri, and Alex Orailoglu. \"Fault tolerant\
    \ quantum cellular array (QCA) design using triple modular redundancy with shifted\
    \ operands.\" In *Proceedings of the 2005 Asia and South Pacific Design Automation\
    \ Conference*, pp. 1192-1195. ACM, 2005.\n\n[72]Ma, Xiaojun, and Fabrizio Lombardi.\
    \ \"Fault tolerant schemes for QCA systems.\" In *Defect and Fault Tolerance of\
    \ VLSI Systems, 2008. DFTVS'08. IEEE International Symposium on*, pp. 236-244.\
    \ IEEE, 2008.\n\n- [73]Dalui, Mamata, Bibhash Sen, and Biplab K. Sikdar. \"Fault\
    \ tolerant QCA logic design with coupled majority-minority gate.\" *Int. J. Comput.\
    \ Appl* 1, no. 29 (2010): 81-87.\n- [74]Farazkish, Razieh. \"A new quantum-dot\
    \ cellular automata fault-tolerant five-input majority gate.\" *Journal of nanoparticle\
    \ research* 16, no. 2 (2014): 2259.\n- [75]Roohi, Arman, Ronald F. DeMara, and\
    \ Navid Khoshavi. \"Design and evaluation of an ultra-area-efficient fault-tolerant\
    \ QCA full adder.\" *Microelectronics Journal* 46, no. 6 (2015): 531-542.\n- [76]Farazkish,\
    \ Razieh. \"A new quantum-dot cellular automata fault-tolerant full-adder.\" *Journal\
    \ of Computational Electronics* 14, no. 2 (2015): 506- 514.\n- [77]Sen, Bibhash,\
    \ Yashraj Sahu, Rijoy Mukherjee, Rajdeep Kumar Nath, and Biplab K. Sikdar. \"\
    On the reliability of majority logic structure in quantum-dot cellular automata.\"\
    \ *Microelectronics Journal* 47 (2016): 7- 18.\n- [78]Dhare, Vaishali, and Usha\
    \ Mehta. \"Fault analysis of QCA combinational circuit at layout & logic level.\"\
    \ In *Electrical and Computer Engineering (WIECON-ECE), 2015 IEEE International\
    \ WIE Conference on*, pp. 22- 26. IEEE, 2015.\n- [79]Pulimeno, Azzurra, Mariagrazia\
    \ Graziano, Alessandro Sanginario, Valentina Cauda, Danilo Demarchi, and Gianluca\
    \ Piccinini. \"Bisferrocene molecular QCA wire: Ab initio simulations of fabrication\
    \ driven fault tolerance.\" *IEEE transactions on nanotechnology* 12, no. 4 (2013):\
    \ 498-507.\n- [80]Graziano, Mariagrazia, Azzurra Pulimeno, Ruiyu Wang, Xiang Wei,\
    \ Massimo Ruo Roch, and Gianluca Piccinini. \"Process variability and electrostatic\
    \ analysis of molecular QCA.\" *ACM Journal on Emerging Technologies in Computing\
    \ Systems (JETC)* 12, no. 2 (2015): 18.\n- [81]Turvani, Giovanna, Fabrizio Riente,\
    \ Mariagrazia Graziano, and Maurizio Zamboni. \"A quantitative approach to testing\
    \ in quantum dot cellular automata: Nanomagnet logic case.\" In *Ph. D. Research\
    \ in Microelectronics and Electronics (PRIME), 2014 10th Conference on*, pp. 1-4.\
    \ IEEE, 2014.\n- [82]Tahoori, Mehdi Baradaran, and Fabrizio Lombardi. \"Testing\
    \ of quantum dot cellular automata based designs.\" In *Proceedings of the conference\
    \ on Design, automation and test in Europe-Volume 2*, p. 21408. IEEE Computer\
    \ Society, 2004.\n- [83]Gupta, Pallav, Niraj K. Jha, and Loganathan Lingappan.\
    \ \"A test generation framework for quantum cellular automata circuits.\" *IEEE\
    \ transactions on very large scale integration (VLSI) systems* 15, no. 1 (2007):\
    \ 24-36.\n- [84]Gupta, Pallav, Niraj K. Jha, and Loganathan Lingappan. \"Test\
    \ generation for combinational quantum cellular automata (QCA) circuits.\" In\
    \ *Design, Automation and Test in Europe, 2006. DATE'06. Proceedings*, vol. 1,\
    \ pp. 1-6. IEEE, 2006.\n- [85]Karim, Faizal, Konrad Walus, and Andre Ivanov. \"\
    Testing of combinational majority and minority logic networks.\" In *Mixed-Signals,\
    \ Sensors, and Systems Test Workshop, 2008. IMS3TW 2008. IEEE 14th International*,\
    \ pp. 1-6. IEEE, 2008.\n- [86]Dhare, Vaishali, and Usha Mehta. \"Development of\
    \ basic fault model and corresponding ATPG for single input missing cell deposition\
    \ defects in Majority Voter of QCA.\" In *Region 10 Conference (TENCON), 2016\
    \ IEEE*, pp. 2354-2359. IEEE, 2016.\n- [87]Karim, Faizal, Konrad Walus, and André\
    \ Ivanov. \"Crosstalk in QCA arithmetic circuits.\" In *PROCEEDINGS-SPIE THE INTERNATIONAL\
    \ SOCIETY FOR OPTICAL ENGINEERING*, vol. 6313, p. 631306. International Society\
    \ for Optical Engineering; 1999, 2006.\n- [88]Kazemi-fard, Nasim, Maryam Ebrahimpour,\
    \ Mostafa Rahimi, Mohammad Tehrani, and Keivan Navi. \"Performance evaluation\
    \ of incircuit testing on QCA based circuits.\" In *Design & Test Symposium (EWDTS),\
    \ 2008 East-West*, pp. 375-378. IEEE, 2008."
- title: 'Machine Learning in VLSI Design: A Comprehensive Review'
  abstract: ''
  keywords: Machine Learning, VLSI, EDA, CAD
  document: '#### I. INTRODUCTION


    As ASIC development grows in complexity, manufacturing constraints emerge, influencing
    both cost and time-tomarket. The escalating number of physical features in VLSI
    circuits presents a serious challenge. This growth results in a massive flux of
    data processed by EDA tools'' engines[1]. Fig 1 illustrates the data growth across
    technology advancements, complicating the task of gathering and identifying underlying
    data correlations and patterns. Consequently, this surge in data directly impacts
    costs and extends the development runtime.


    ![](_page_0_Figure_9.jpeg)


    Fig. 1 EDA Data capacity vs. chip technology node


    Recently, Machine Learning (ML) became the focal point and is extensively applied
    in VLSI design. It has empowered designers with the capability to discover patterns
    and functions within complex unexplored data, and provides the opportunity to
    gain more knowledge on circuit behavior. As we advance toward physical layout
    implementation, more detailed and accurate physical information becomes accessible.
    However, there is a point where metrics settle, and EDA heuristics face limitations
    due to excessive pessimism. This presents a challenge for EDA companies, urging
    them to explore new methodologies, particularly in handling significant amounts
    of data. ML emerges as a solution to surpass these predictability limitations.
    Through training on massive datasets, ML-based models may offer high-accuracy
    predictions for Quality of Results (QoR) as in Fig 2, which depicts the potential
    impact of ML predictions on modifying the trade-off curve between cost and accuracy
    [2].


    ![](_page_0_Figure_14.jpeg)


    Fig. 2 ML Accuracy improvement versus Cost/Runtime


    This paper provides a comprehensive exploration of ML in EDA and VLSI design.
    Our focus involved reviewing several state-of-the-art ML applications and outlining
    the key challenges and limitations that researchers encounter. The scope of this
    review spans from high-level synthesis to postlayout verification of ASIC flow.
    We close the review by identifying potential promises and future directives. The
    remainder of this paper is structured as follows. Section 2 provides an overview
    of the fundamentals of ML and Neural network. Section 3 offers a literature review,
    this section is divided into multiple subsections based on different parts of
    the ASIC flow. Section 4, presents a results discussion and offers insights into
    future perspectives.


    #### II. BACKGROUND AND ML FUNDAMENTALS


    #### *A. AI/ML Role*


    AI frameworks consist of data-driven models able to process, perceive, and interpret
    the environment, engage in reasoning, and make informed decisions. The AI comprises
    several sub-fields, among which are Machine Learning and Deep Learning. ML focuses
    on creating models that improve their performance by learning from data using
    predefined algorithms, aiming to discern patterns and correlations within datasets.
    The model acquires knowledge through training and can subsequently generate predictions
    for new unseen data. ML techniques are classified into three primary subsets:
    *Supervised*, *Unsupervised*, and *Reinforcement Learning*.


    #### *B. Dataset and Feature Engineering*


    Each ML models build knowledge by learning from a training dataset. Datasets typically
    include input and output variables, often referred to as features (independent
    variables) and targets (dependent variables). Consequently, each line in the dataset
    represents one sample along with the corresponding target value. We often represent
    independent variables as matrix *X* with *m* rows of data, each with *n* features,
    while *Y* is a vector containing m target dependent variables. [3]


    $$X = \begin{bmatrix} X\_{11} & X\_{12} & \dots & X\_{1n} \\ X\_{21} & X\_{22}
    & \dots & X\_{2n} \\ \vdots & \vdots & \ddots & \vdots \\ X\_{m1} & X\_{m2} &
    \dots & X\_{mn} \end{bmatrix} \begin{bmatrix} Y\_1 \\ Y\_2 \\ \vdots \\ Y\_m \end{bmatrix}$$


    Real-world data requires preprocessing before training. In the following paragraph,
    we detail various methods used for data reprocessing. [4] [5] [6]


    - *Data cleaning* addresses missing and duplicate values using the *imputation
    technique* which refers to replacing missing values with estimations.

    - *Handling Outliers* aims to filter noise and irregular data points that may
    significantly deviate from the rest of data using methods such as *Z-Score* and
    *Interquartile Range (IQR)*.

    - *Data Scaling* brings features to a common scale and makes sure that all features
    have the same range, preventing large values from disproportionating and dominating
    the model. The common scaling techniques are *Min-Max*, *normal scaling*, *Z-score
    Standardization*, and *Robust Scaling*.

    - *Feature Engineering* aims to create new features from the existing features
    to form an informative dataset.

    - *Handling categorical data* involves employing techniques such as *one-hot encoding*,
    which transform categorical variables into binary vectors.

    - *Feature selection* involves choosing the most relevant features to train the
    model and improve model performance, as not all features have equal weight and
    contribution.

    - *Dataset splitting* balances between training, testing, and evaluation sets.
    The *training set* is the portion that enables the model to learn the patterns
    and feature''s relationships. The model adjusts its internal parameters to minimize
    prediction error. The *validation set* is used to fine-tune the model''s hyperparameters,
    a common method is *k-fold cross-validation*, where the dataset is divided into
    k subsets/folds. The model is trained and validated *k* times, with each fold
    serving as a validation, while the remaining *k-1* folds form the training set.
    In case the performance degrades on the validation set, it indicates an overfitting.
    The *testing set* evaluates the model''s performance on unseen data, offering
    an estimate of the model''s accuracy when applied to real-world new data.


    ### *C. Supervised Learning: Labeled Data*


    Supervised learning algorithms learn from labeled datasets, focusing on adjusting
    the model''s parameters and creating an inferred function that maps inputs to
    outputs with a minimized prediction error. The supervised models learn from a
    pair of input vectors and a corresponding target value. Two primary types of supervised
    learning exist, *Classification* and *Regression*. Classification algorithms allocate
    the input vector to a predefined category or class. The *classification* is either
    binary classification (two target categories) or multi-class classification (multiple
    categories). While Regression algorithms focus on predicting continuous numeric
    values.


    Various regression algorithms exist, each for a distinct needs. *Linear Regression*
    (LR) presumes a linear association between features and the target. The *Polynomial
    Regression* (PR) captures non-linear relationships through polynomial functions.
    The *Decision Trees* (DT) recursively split the dataset into subsets based on
    the most significant attributes, thus creating a tree structure that leads to
    average prediction. The *Random Forest* (RF) is an ensemble method that combines
    multiple decision trees to improve prediction accuracy. The *Extra-Trees* or *Extremely
    Randomized Trees* is another ensemble method that constructs decision trees with
    randomized feature splits. The *Support Vector Regressor* (SVR) aims to find a
    hyperplane that minimizes the prediction error while allowing a tolerance margin.
    The *k-Nearest Neighbors* (KNN) is a non-parametric algorithm that predicts the
    target value by averaging the values of its k-nearest neighbors. While *Naive
    Bayes Regressor* (NBR) relies on probabilistic principles. The *Gradient Boosting*
    (GB) constructs a model by combining multiple weak decision tree models and gradually
    reducing the prediction error by fitting each tree to the residual errors of the
    previous trees. The list is still extensive, other methods and neural network
    algorithms exist that have not been included. [7] [8] [9] [3] [10] [6]


    A model exhibits good *generalization capabilities* when it provides accurate
    predictions for unseen data. If the inferred model is too simplistic and predicts
    inaccurate values for the training set, it risks *Underfitting* the training data.
    On the other hand, when the training data is insufficient, we risk having an *Overfitting*
    where the model produces good predictions on the training set but fails when facing
    new data, the model then has a low generalization capability. As a result, it''s
    crucial to strike a balance for model complexity and find a well-balanced spot
    between underfitting and overfitting as depicted in Fig 3.


    ![](_page_1_Figure_17.jpeg)


    Fig. 3 Underfitting and Overfitting


    Journal of Integrated Circuits and Systems, vol. 19, n. 2, 2024 3


    ## *D. Unsupervised Learning: Discovering Patterns in Data*


    In unsupervised learning, we provide input data without the guidance of labeled
    outputs. The purpose is to extract the underlying knowledge and common structure
    within the given inputs without target values. One of the most common unsupervised
    learning techniques is clustering, a method that groups input values into clusters
    based on their similarities and resemblances. Another known technique is *Transformation*
    or *Dimensionality Reduction*, which transforms the high-dimensional data to low
    dimensions while preserving essential features.


    Unsupervised learning is frequently applicable in the visualization of representative
    data. Among the *Clustering* algorithms, the *K-means* divides data into k-clusters
    by minimizing the sum of squared distances between data points and cluster centroids.
    Meanwhile, *DBSCAN* (Density-Based Spatial Clustering of Applications with Noise)
    identifies clusters grounded in their density. The *Principal Component Analysis*
    (PCA) is also a dimensionality reduction technique, which discovers a set of orthogonal
    axes, referred to as principal components that capture the data variance. [7]
    [8] [9] [3] [10] [6]


    #### *E. Reinforcement: Learning from Experience*


    Reinforcement Learning (RL) stands apart from supervised and unsupervised learning.
    It is an interactive learning paradigm where an agent interacts with the environment
    and makes decisions to achieve specific predictions. The agent receives feedback
    in the form of rewards or punishments based on its prediction choices, and it
    learns to optimize its strategy over time to maximize cumulative rewards. Several
    prominent Reinforcement algorithms include *Q-Learning*, *Policy Gradient Methods*,
    *Markov Decision Processes* (MDP), and *Monte Carlo Tree Search* (MCTS).[11][10]


    #### *F. Deep Learning and Neural Networks*


    Neural Networks (NN) can be adapted for supervised, unsupervised, and reinforcement
    learning. NNs have shown remarkable performance in handling complex data structures.
    In this section, we introduce a single neuron basic building block structure and
    the architecture of a vanilla feed-forward neural network. We provide an overview
    of the concept of Backpropagation and review commonly used Deep Neural Networks
    (DNNs).


    *F..1 Perceptron*A neuron or perceptron, is the fundamental component in neural
    networks that processes input and makes decisions. It assigns weights to its multi-input
    values based on its importance, then linearly combines the weighted inputs and
    introduces a bias term. The result undergoes a non-linearity through an *activation
    function*. In Fig 4, multiple inputs represent features, denoted as *x1, x2,...,
    xn*. Each input has an associated weight or importance to the output. (eq.1)[12]


    $$Yin = \sum\_{i=1}^{n} (x\_i \cdot w\_i) + Bo \tag{1}$$


    ![](_page_2_Figure_10.jpeg)


    Fig. 4 Perceptron


    Non-linearity is introduced through an activation function, unless the network
    would operate as a linear function. As a result, when numerous neurons with non-linear
    activation functions are stacked in a large network, it becomes capable of approximating
    highly complex functions. Some commonly used activation functions are presented
    below with their respective equations (eq. 2). Fig 5 illustrates the graphical
    representations of below activation functions. [12]


    - The *Step function* produces binary outputs determined by a predefined threshold.
    It operates as a binary function, yielding an output of ''1'' when the input exceeds
    ''0'' and ''-1'' otherwise.

    - The *Sigmoid* function maps the weighted sum to the range ''0;1''. It''s often
    used in binary classification. The sigmoid smoothly transforms input values into
    a probability-like output. It maps large negative inputs to ''0'' and large positive
    inputs to ''1''.

    - The *Hyperbolic Tangent* (tanh) is similar to the sigmoid. However, the output
    ranges from ''-1'' to ''1''. It maps the weighted sum to -1;1, offering a centered
    output.

    - The *Rectified Linear Unit* (ReLU) activation function outputs the weighted
    sum if the result is positive and ''0'' otherwise.


    $$\begin{aligned} \text{Step:} \quad f(x) &= \begin{cases} 1 & \text{if } x >
    0 \\ -1 & \text{if } x \le 0 \end{cases} \\ \text{Sigmoid:} \quad f(x) &= \frac{1}{1
    + e^{-x}} \\ \tanh: \quad f(x) &= \tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}
    \\ \text{ReLU:} \quad f(x) &= \max(0, x) \end{aligned} \tag{2}$$


    Equation eq.3 represents the output *Y* after being passed through the activation
    function, where *Yin* is the result from eq.1 (Fig 4). The Sigmoid function then
    constrains the result between ''0'' and ''1''.


    $$Y = \frac{1}{1 + e^{-(Y \dot{m})}} \tag{3}$$


    ![](_page_3_Figure_2.jpeg)


    Fig. 5 Common activation functions


    *F..2 Multilayer*A Multilayer Perceptron (MLP) is the fundamental Artificial Neural
    Network (ANN). It comprises multiple layers of interconnected neurons. In Fig
    6, the network is a Feedforward neural network (FNN) of three hidden layers. The
    network adjusts weights during training. and each neuron in the hidden layer receives
    the results of all previous layer neurons, processes the linear weighted combination,
    and passes the results through the activation function. [12]


    ![](_page_3_Figure_5.jpeg)


    Fig. 6 Feedforwad Neural Network


    Eventually, the *Output layer* represents the predicted targets variables, either
    for regression tasks or classes in classification tasks. The output layer may
    comprise multiple neurons, each corresponding to a specific class. The matrix
    in eq.4 represents the output values (Y ) for m hidden neuron.


    $$Y = \sigma \left( X \cdot W + b \right) \tag{4}$$


    $$

    \begin{bmatrix} Y\_1 \\ Y\_2 \\ \vdots \\ Y\_m \end{bmatrix} = \sigma(\begin{bmatrix}
    x\_1 \\ x\_2 \\ \vdots \\ x\_n \end{bmatrix}, \begin{bmatrix} w\_{11} & w\_{12}
    & \dots & w\_{1m} \\ w\_{21} & w\_{22} & \dots & w\_{2m} \\ \vdots & \vdots &
    \ddots & \vdots \\ w\_{n1} & w\_{n2} & \dots & w\_{nm} \end{bmatrix} + \begin{bmatrix}
    b\_1 \\ b\_2 \\ \vdots \\ b\_m \end{bmatrix})

    $$


    - Y: output values in the 1st hidden layer, for *m* neuron.

    - X: *n* input features.

    - W: weight, *Wij* with *i* input and *j* neuron.

    - b: biases vector for *m* neuron..

    - σ is the activation function applied element-wise.


    *F..3 Cost function*During the forward propagation, each neuron computes its weighted
    sum and applies non-linearity. The *Cost function* or *Loss function* refers to
    the error on the output layer obtained by comparing the predicted versus target
    values. The cost function estimates the error for the entire network, then propagate
    it backward through through the gradient descent of the cost. This process is
    the *Backpropagation* and allows the adjustment of weights and biases for all
    neurons to minimize the final cost. Some common loss functions are *Mean Squared
    Error* (MSE) for regression or *Cross-entropy* for classification.


    *F..4 Backpropagation*The Backpropagation is fundamental for weights and bias
    updates. The network calculates the *cost gradient* for each weight and bias in
    the output layer and backpropagates to adjust all weights and biases. The gradient
    reflects how small or large the weight and bias changes affect the cost function,
    and points the direction for weight adjustments to minimize the error. The weight
    update is the subtraction of the learning rate multiplied by the gradient eq.5.
    The learning rate controls the step size of the weight updates and influences
    convergence speed and training stability.[12]


    $$

    \Delta w\_{ij} = -\eta \frac{\partial E}{\partial w\_{ij}}\tag{5}

    $$


    *F..5 Deep Learning*A Deep Neural Network (DNN) is a feed-forward network with
    multiple hidden layers trained using backpropagation. *Convolutional Neural Networks*
    (CNNs) are tailored for grid-like data, such as image classification and object
    detection, to detect local patterns. *Recurrent Neural Networks* (RNNs) are specialized
    for sequential data, where the element order is significant. They utilize recurrent
    connections to manage and update hidden states and are valuable in *Natural Language
    Processing* (NLP) and speech recognition. The *Long Short-Term Memory Networks*
    (LSTMs) are a specialized type of RNN designed to overcome the vanishing gradient
    problem and capture long-range dependencies in sequential data. *Gated Recurrent
    Unit Networks* (GRUs) are also RNNs, with a simpler architecture, offering efficiency
    for tasks like natural language understanding and speech synthesis. Lastly, *Autoencoders*
    are networks used for unsupervised learning and dimensionality reduction, mapping
    input data to a lowerdimensional representation and a decoder network reconstructing
    the input.[12]


    Even though deep learning has been around for decades, the hardware support for
    neural networks has only recently come to realization. The development of accelerated
    parallel computing CPU-GPU architectures has made deep learning achievable. As
    a result, model training times have been reduced from weeks to hours.


    ## III. LITERATURE REVIEW: ML IN VLSI DESIGN AND CAD EDA


    The field of Computer-Aided Design (CAD) is rapidly evolving to address the increasing
    complexities of modern VLSI chips [1]. AI integration into design automation tools
    represents an approach to stay at the forefront of technological advancements.
    Extensive research has been conducted focusing on reducing design runtime and
    improving QoR through AI/ML integration [13][14][15][16][14][17][16][18]. A similar
    review has been provided in [13], offering an extensive examination of the AI/ML
    methodologies suggested in existing literature. This review primarily encompasses
    all stages of VLSI abstraction, including architectural considerations, physical
    design, circuit simulation, manufacturing, and VLSI testing. In this section,
    we provide an overview of the state-of-the-art and survey recent research and
    draw some key limitations and highlight possible enhancements.


    #### *A. ML in VLSI functional Verification*


    The study made in [64] and [43] demonstrates the importance of data mining in
    EDA. In [19], data mining and pattern extraction have exhibited the potential
    of the Support Vector Machine (SVM) in reducing runtime and enhancing simulation
    coverage during functional verification. As illustrated in Fig 7, a standard set
    of unit tests typically attains maximum functional coverage upon 6,000 tests.
    By employing the SVM-based model, a subset of merely 310 tests accomplished equivalent
    coverage, leading to a notable 95% reduction in simulation runtime. The model
    is capable of predicting coverage overlap and captures test similarities, as depicted
    in Fig 8. In [20], the author proposed a classification learning method to enhance
    functional coverage based on assertions. The model identifies how frequently an
    assertion is triggered, highlighting the significance of each unit test. The objective
    is to extract knowledge that can activate assertions with lower coverage. To achieve
    this, the authors employed a feature-based analysis, utilizing supervised classification
    and unsupervised association rules.


    ![](_page_4_Figure_4.jpeg)


    Fig. 7 Runtime saving using SVM-based mode


    ![](_page_4_Figure_6.jpeg)


    Fig. 8 Increasing Coverage using SVM-based mode


    #### *B. ML at High Level Synthesis*


    ML application on High-Level Synthesis (HLS) has raised the challenge of design
    space exploration (DSE). In [21], the author presents a learning-based model for
    DSE that speeds up the convergence toward the optimal RTL design architecture.
    The results from Random Forest and randomized selection algorithms yielded the
    highest accuracy for optimal Pareto for RTL architecture. Similarly, [22] harnesses
    the power of RF and Extra-Tree to guide DSE in discovering Pareto-optimal combinations
    of area and performance. [23] implemented a Simulated Annealing (SA) probabilistic
    algorithm. The author introduces a faster SA method based on decision trees (DT),
    resulting in a similar performance to a standard SA but with a gain up to 43%
    on average runtime improvement. Lastly, [24] delves into the challenges posed
    by DSE on systems with multicore processors. The author leverages reinforcement
    techniques such as Imitation Learning (IL) to enhance the computational efficiency
    of these manycore systems.


    ### *C. ML in Physical Design*


    ML finds application also in the physical design stages, where data continues
    to grow alongside technological progress. In this section, we explore cutting-edge
    applications within backend design.


    *C..1 ML for Floorplanning and Placement Optimization* Traditional Place-and-Rout
    (PnR) tools typically generate a floorplan without exploring multiple alternatives,
    regardless of timing, wire length, congestion, power, routability, and other QoR
    metrics. In [25], the author introduces a deeplearning neural network that explores
    various floorplan alternatives, considering different aspect ratios and placement
    styles. The model automatically generates an optimal floorplan for subsequent
    PnR stages based on dataflow and DSE information. In [26], the author presents
    a reinforcement learning agent that undergoes training across multiple chip blocks
    to produce optimized chip placements. This approach involves the sequential macro
    and standard cell placement on the chip canvas. The model''s rewards are based
    on the cost associated with wirelength and routing congestion.


    In [27] and [28], two deep learning-based models are introduced to enhance design
    testability (DFT). The model uses a Graph Convolutional Neural Network (CNN) for
    control and observation (CP-OP) point insertion. The graph CNN aims to minimize
    the number of CP-OPs while maximizing fault coverage. In [29] employs a DNN framework
    to accelerate cell placement. The results significantly improved the QoR and routing
    congestion. While the model still achieves good placement quality comparable to
    state-ofthe-art placers. The model has accelerated the global placement runtime
    by a factor of 30.


    Typically, an effective placement aims to minimize the half-perimeter wirelength
    (HPWL). Nevertheless, handling datapaths can yield varying QoR, thus offering
    different placements. In both [30] and [31], the authors proposed a model that
    combines SVM and ANN. The models classify datapaths by their order of importance
    and guide a wire length-driven placement strategy, focusing on the highly weighted
    datapaths. The results led to a reduction of 7% in HPWL and 12% in Steiner Wire
    Length (StWL).


    *C..2 ML for Clock Network Optimization*In synchronous circuits, the primary challenges
    concern the clock network as it is one of the most critical networks. Achieving
    a zeroskew clock network has always been a challenge. A common strategy to optimize
    clock skew, minimize clock-tree length, and mitigate clock network power consumption
    involves placing latches in proximity to local clock buffers, a technique discussed
    in [32] and [30]. The authors introduce


    |      |      |       | Stage |            |                 |     |       |               |               |
    ML algorithm           |              |               |

    |------|------|-------|-------|------------|-----------------|-----|-------|---------------|---------------|------------------------|--------------|---------------|

    |      | Year | Tech. | Verif | HLS<br>DSE | Floor.<br>Place | CTS | Route | Phy.<br>Verif
    | Power<br>Grid | Supervised             | Unsupervised | Reinforcement |

    | [19] | 2012 | 28nm  | ✓     |            |                 |     |       |               |               |
    SVM                    |              |               |

    | [20] | 2013 | 28nm  | ✓     |            |                 |     |       |               |               |
    Classification         | Association  |               |

    | [21] | 2013 | 45nm  |       | ✓          |                 |     |       |               |               |
    RF Random Selection    |              |               |

    | [22] | 2008 | -     |       | ✓          |                 |     |       |               |               |
    RF Extra-Tree          |              |               |

    | [23] | 2014 | 45nm  |       | ✓          |                 |     |       |               |               |
    Decision Tree          |              |               |

    | [24] | 2018 | -     |       | ✓          |                 |     |       |               |               |                        |              |
    Imitation     |

    |      |      |       |       |            |                 |     |       |               |               |                        |              |
    Learning      |

    | [25] | 2020 | -     |       |            | ✓               |     |       |               |               |                        |              |
    -             |

    | [26] | 2020 | -     |       |            | ✓               |     |       |               |               |                        |              |
    MDP           |

    | [27] | 2020 | -     | ✓     |            | ✓               |     |       |               |               |
    Graph CNN              |              |               |

    | [28] | 2019 |       |       |            |                 |     |       |               |               |                        |              |               |

    | [29] | 2019 |       |       |            |                 |     |       |               |               |                        |              |               |

    | [30] | 2012 | -     |       |            | ✓               |     |       |               |               |
    SVM ANN                |              |               |

    | [31] | 2015 |       |       |            |                 |     |       |               |               |                        |              |               |

    | [32] | 2013 | 22nm  |       |            | ✓               | ✓   |       |               |               |
    Decision Tree          |              |               |

    | [33] | 2019 | -     |       |            | ✓               |     |       | ✓             |               |
    CNN                    |              |               |

    | [34] | 2016 | 28nm  |       |            | ✓               |     | ✓     |               |               |
    SVM MARS               |              |               |

    |      |      | 45nm  |       |            |                 |     |       |               |               |                        |              |               |

    |      |      | 65nm  |       |            |                 |     |       |               |               |                        |              |               |

    | [35] | 2016 | 28nm  |       |            | ✓               |     | ✓     |               |
    ✓             | GPR                    |              |               |

    | [36] | 2017 | 14nm  |       |            | ✓               |     | ✓     | ✓             |               |
    LR LogR SVM classifier |              |               |

    | [37] | 2018 | -     |       |            |                 |     | ✓     | ✓             |               |
    ANN                    |              |               |

    | [38] | 2014 | 45nm  |       |            |                 |     | ✓     | ✓             |               |
    MARS                   |              |               |

    | [39] | 2015 |       |       |            |                 |     |       |               |               |                        |              |               |

    | [40] | 2019 | -     |       |            | ✓               |     | ✓     | ✓             |               |
    CNN                    |              |               |

    | [41] | 2015 | 28nm  |       |            |                 |     | ✓     | ✓             |               |
    ANN SVM                |              |               |

    | [42] | 2019 | -     |       |            |                 |     | ✓     | ✓             |               |                        |
    Autoencoders |               |

    | [43] | 2016 | 45nm  |       |            |                 |     |       | ✓             |               |
    SVM Classifier         |              |               |

    | [44] | 2011 |       |       |            |                 |     |       |               |               |                        |              |               |

    | [45] | 2012 | -     |       |            |                 |     |       | ✓             |               |
    CNN                    |              |               |

    | [46] | 2018 | -     |       |            |                 |     |       |               |
    ✓             | XGB CNN                |              |               |

    | [47] | 2012 | 90nm  |       |            |                 |     |       |               |
    ✓             | LR                     |              |               |

    | [48] | 2018 | 16nm  |       |            |                 |     |       |               |
    ✓             | ANN                    |              |               |

    |      |      | 45nm  |       |            |                 |     |       |               |               |                        |              |               |

    | [49] | 2017 | 180nm |       |            |                 |     |       |               |
    ✓             |                        | K-means      |               |

    |      |      |       |       |            |                 |     |       |               |               |                        |
    Mean-Shift   |               |

    |      |      |       |       |            |                 |     |       |               |               |                        |
    DBSCAN       |               |

    | [50] | 2014 | 45nm  |       |            |                 |     |       |               |
    ✓             | SVM                    |              |               |

    | [51] | 2020 | -     |       |            |                 |     |       |               |
    ✓             | ANN                    |              |               |

    | [52] | 2021 | -     |       |            |                 |     |       |               |
    ✓             | ANN LogR               |              |               |

    | [53] | 2020 | 15nm  |       | ✓          | ✓               |     |       |               |               |
    PR SVR ANN             |              |               |

    | [54] | 2007 | 90nm  |       |            | ✓               |     | ✓     |               |               |
    SVM classifier         |              |               |

    | [55] | 2008 | 90nm  |       |            | ✓               |     | ✓     |               |               |
    ϵ-SVR                  |              |               |

    | [56] | 2014 | 28nm  |       |            |                 |     | ✓     |               |               |
    ANN SVR RF             |              |               |

    |      |      | 45nm  |       |            |                 |     |       |               |               |                        |              |               |

    | [57] | 2019 | 45nm  |       |            |                 |     | ✓     |               |               |
    ANN RF                 |              |               |

    | [58] | 2013 | -     |       |            |                 |     | ✓     |               |               |
    Regressor classifier   |              |               |

    | [59] | 2018 | 28nm  |       |            | ✓               |     | ✓     |               |               |
    RF                     |              |               |

    | [60] | 2016 | 28nm  |       |            | ✓               |     | ✓     |               |               |
    Lasso SVM GB ANN       |              |               |

    | [61] | 2021 | 7nm   |       |            | ✓               |     |       |               |               |
    LR RF DT               |              |               |

    | [62] | 2022 | 130nm |       |            |                 |     | ✓     |               |               |
    GNN                    |              |               |

    | [63] | 2020 | 7nm   |       |            |                 |     | ✓     |               |               |
    LR ANN RF XGBoost      |              |               |


    Table I. State-of-the-art works of Machine Learning application in VLSI Design.


    LR:Linear Regression, LogR:Logistic Regression, PR:Polynomila Regression, DT:Decision
    Tree, RF:Random Forest, GB:Gradient Boosting, XGBoost:eXtreme Gradient Boosting,
    SVM:SupportVectorMachine ,SVR:SupportVectorRegressor, MDP:Markov Decision Processes,
    ANN:Artificial Neural Networks, CNN:Convolutional Neural Network, MARS:Multivariate
    Adaptive Regression Splines, GPR:Gaussian process regression, GNN:Graph Neural
    Network


    a DT model to reduce latch redundancies and propose an optimized latch placement
    solution. The approach significantly reduces clock skew and has a positive impact
    on placement, indirectly benefiting power consumption.


    *C..3 ML for Congestion*Routing congestion is a critical factor that significantly
    affects the timing behavior and routeability. However, congestion is not always
    accurately predicted from early placement stages, which misleads the router and
    results in longer wires and routing detours. Tools can restructure the logic and
    adjust functionality to mitigate routing congestion hotspots. In [33], the author
    introduces a deep learning approach based on CNN to predict routing congestion
    hotspots on a pre-placed netlist. The model utilizes various features, including
    the netlist graph, cell type, function, pin count, geometry, and other cell characteristics
    for training. The ground truth during training was the routed congestion map.
    The model employs Graph Attention Networks (GAT) [65] and identifies common patterns
    in gatelevel netlists, helping to pinpoint the logic elements contributing to
    congestion. The model achieves 75% accuracy in predicting congestion at lower
    metal layers, compared to the baseline congestion map''s accuracy of 29%.


    *C..4 ML for Routing Optimization*Advanced technology nodes have raised new challenges
    in routeability. Numerous factors, such as placement quality, timing constraints,
    and aspect ratio, significantly influence the design routeability. A bad routeability
    results in excessive runtime, stretching to weeks for large designs, and sometimes
    it ends up unrouteable. Although the congestion map can aid in predicting routeability,
    however, it may still prove insufficiency or mislead the router. New research
    has focused on ML to predict placement solution routeability without fully performing
    global or detailed routing.


    In [34], the author developed SVM-based and Multivariate Adaptive Regression Splines
    (MARS) models to predict routeability from the placement stage. The models were
    trained using designs at 28nm and 45nm technology nodes to predict the Pareto
    frontiers of utilization. After dividing the layout into grids, various grid division
    features were extracted, including pin density per grid area, and pin proximity,
    cell count, net count, and edges count. The classification model achieved a prediction
    accuracy of 85.9%, and 90.4% for 45nm, and 28nm, respectively. Thus, surpassing
    the standard prediction based on the congestion map, which barely achieved 61.7%
    and 73.5%.


    In [35], the authors focus on predicting wirelength based on the circuit''s power
    distribution network (PDN). An optimized PDN network reduces wirelength, while
    an unoptimized PDN can lead to inefficient placement of power rails and vias,
    resulting in suboptimal wire routing. Inefficient routing increases wirelength
    as signals take longer paths to avoid congested areas and routing obstacles caused
    by inefficient power networks. To mitigate this, the authors employ a Gaussian
    process regression (GPR) model, which considers relevant PDN attributes and placement
    features to reduce the total wirelength.


    Congestion maps identify potential Design Rule violations (DRV) at the routing
    stage. These congestion maps aid in optimizing placement by adjusting cell positions
    and reducing detailed-route DRV. However, in advanced sub-microns, congestion
    map-based placement may leave significant design rule checks (DRC) violations
    to be addressed manually or through iterating, making it a less reliable predictor
    and potentially misleading the global router. Fig 9 illustrates the mismatch between
    actual DRC violations and congestion map hotspots.


    In [36], the authors employ multiple learning models, including Linear Regression,
    Logistic Regression, and SVM classifiers, to reduce DRC violations during detailed
    routing. Binary classifiers categorize globally routed cells based on whether
    they contribute to a DRC violation. The models were trained using features such
    as fan-in, fan-out, connectivity parameters, pin proximity, local pin density,
    and local overflow. The SVM model has successfully reduced the DRC violations
    by an average of around 20%, with some cases achieving up to a remarkable 76%,
    all without impacting design timing. Fig 10 illustrates a DRC hotspot detection
    using the SVM model.


    ![](_page_6_Figure_9.jpeg)


    Fig. 9 Actual DRC vs. Congestion map DRC violations


    ![](_page_6_Figure_11.jpeg)


    Fig. 10 Actual DRC vs. model''s DRC hotspot predictions


    In a similar context, a study conducted in [37] predicts detailed routing violations
    from an early placement stage by estimating congested regions based on StWL estimations
    and pin density, thus avoiding the need for a global router. The authors developed
    a binary classification model where the output indicates the presence or absence
    of violations. The model was trained on features extracted from the placement
    stage, targeting detailed routing shorts of already routed designs. The implemented
    neural network model consists of 20 nodes in one hidden layer. It has achieved
    an average shorts prediction accuracy of 90%, within a reduced runtime compared
    to the standard congestion map method.


    Similar efforts to estimate routability and routing congestion from an early placement
    stage using supervised learning have been conducted in [38] and [39] with the
    use of multivariate adaptive regression splines models. The learning framework
    aims to detect routing violations directly from the placement stage without relying
    on a global router, resulting in reliably accurate results and shorter runtime.


    Routability may also be impacted from the macro placement stage, particularly
    in large complex designs with high macro and IP counts that occupy significant
    chip areas. In [40], the authors propose a routability-driven macro placement
    prediction using CNN to find the optimal macroplacement with minimal DRC violations.
    The model forecasts design routeability for optimal macro-placement by exploring
    different configurations and evaluating wirelength, power, and timing constraints.
    The CNN model is trained using extracted features such as macro density map, pin
    density map, and connectivity density map. The CNN model has reduced the DRV count
    and lowered the average total wirelength, then it was integrated into the original
    macro placement engine. Simulated annealing optimization was then applied to assess
    whether the resulting macro placement was near-optimal.


    Signal integrity (SI) may also impact delays as it influences the propagation
    of signals and overall timing performance. SI effects create coupling capacitance
    due to the switching activity in neighboring nets which alters wire delays and
    transition time (slew) in adjacent nets. Most EDA tools include a Static Timing
    Analysis (STA) engine with an SI mode, which introduces additional pessimism to
    the total delay based on aggressor and victim dependencies. However, timing analysis
    with SI mode enabled can be timeconsuming, especially for large designs.


    In [41], the authors developed a model to predict transition time, incremental
    delays, and path delays in SI mode. Fig 11 illustrates the incremental delay divergence
    in SI mode between a commercial tool and a signoff SI tool, with an inaccuracy
    reaching 60ps. The training parameters include diverse design features, such as
    clock period, toggle rate, coupling capacitance, resistance, aggressors count,
    differences in max-min arrival times, transition time, and incremental delay in
    non-SI mode. The authors trained ANN and SVM models using a 28nm technology library
    and combined the predictions to obtain final values for incremental transition
    time, incremental delay, and path delay in SI mode. The prediction accuracy reduced
    the absolute error by 15.7%. Fig 12 shows the actual versus predicted incremental
    delays considering SI, with a worst-case absolute error of 5.2ps.


    ![](_page_7_Figure_4.jpeg)


    Fig. 11 Delay Inaccuracy between SI and Non SI mode


    [42] presents an approach to estimating SI effects. The framework uses autoencoder
    and anomaly detection (AD) methods to uncover relevant features from the circuit
    outputs and to detect anomalies. The authors proposed a semisupervised LSAnomaly
    algorithm to identify anomalies in the output signal waveform based on time-domain
    waveform signals.


    ![](_page_7_Figure_7.jpeg)


    Fig. 12 Model''s predictions of Delay on SI mode


    *C..5 ML for Physical Verification*Machine Learning is also employed in detecting
    layout hotspots, a topic discussed in depth in both [43] and [44]. Conventionally,
    hotspots are detected using lithography commercial simulation tools. However,
    ML have remarkably enhanced the hotspot detection accuracy while preserving short
    runtime. A lithography simulator determines the good and bad layout samples. These
    samples are used to train an SVM binary classifier to highlight the boundary overlaps,
    as in Fig 13. Fig14 compares the model-predicted hotspots and the original lithography
    simulation.


    ![](_page_7_Figure_10.jpeg)


    Fig. 13: ML flow for Layout hotspots detection through lithography simulation


    ![](_page_7_Figure_12.jpeg)


    Fig. 14: Layout hotspots detection using lithography simulator vs. model prediction


    Lithography layout distortions may occur even if the design passes design rule
    checks (DRC). The design may still contain layout hotspots that are sensitive
    to the lithographic process. In a similar approach presented in [45], the authors
    propose an accurate hotspot detection CNN-based framework that applies a two-stage
    filter to identify these layout hotspots. Compared to a state-of-the-art DRC tool,
    the model''s result shows nearly 100% accuracy in a short runtime.


    *C..6 ML for Power Optimization*One of the challenges in power and timing performance
    is the IR drop. IR drop may slow down circuit timing behavior and lead to timing
    violations. Conventionally, IR drop signoff analysis is performed at the end of
    the IC design flow, especially during engineering change orders (ECO), before
    tape out. The challenge in fixing IR drop is that it cannot be fixed simultaneously,
    during the design phase. Resolving IR drop during the early design phase might
    be challenging as the primary focus is meeting timing, area, constraints, and
    routeability. However, IR drop fixing is addressed mostly during ECO, using power
    integrity analysis tools and simulations that help identify potential IR drop
    hotspots. In [46], the authors employed XG-Boost and CNN models to predict ground-bounce
    and dynamic cell IR-drop regions where IR-drop violations may occur. Various features
    were extracted, including total path resistance from the power pad to the cell,
    total cell power, peak and average current, toggle rate, load cell capacitance,
    cell type, and cell timing windows.


    Similar research has explored the static IR-drop prediction using Linear Regression
    in [47] and ANN in [48]. In [49], the author introduced a clustering method that
    partitioned the layout into multiple clusters, identifying areas with high power
    density. This approach helps to prevent IR drops and Electromigration (EM) noise
    at early design stages. The authors applied K-means, Mean Shift, and DBSCAN, to
    predict power-critical areas and prepare countermeasures for power hotspots.


    Voltage droop is a phenomenon that occurs momentarily when a high current demand
    occurs in a portion of logic gates, leading to timing violations as it impacts
    the rise and fall delays of cells. Voltage droop can potentially result in setup
    and hold time violations. It''s a common practice to include a static pessimism
    based on worst-case conditions to mitigate voltage droop, which often leads to
    unnecessary pessimism. In [50], an SVM model was developed to predict real-time
    voltage droop.


    IR-drop and Electromigration effects need to be addressed during the power grid
    (PG) design phase to prevent impacting chip interconnects and power grid network
    failure. [51] aimed at minimizing the impact of IR drop and EM on the chip''s
    power grid, the authors developed an ML approach for predicting chip power grid
    design and creating an EM-aware grid network. The model considers the current
    source coordinates (x and y) and the metal line width and employs a neural network
    to predict the optimal widths of the metal lines in the power network. Once the
    optimal widths are determined, the model generates an IR drop map. The predicted
    IR drop map closely matches the map obtained from the conventional approach, as
    illustrated in Fig 15. The model also achieved a significantly faster runtime
    of up to 6 times compared to the conventional approach.


    ![](_page_8_Figure_5.jpeg)


    Fig. 15 Actual IR drop vs. model prediction


    A similar approach in [52] predicts the on-chip power grid network aging using
    an EM-aware model. The author involved a neural network-based regression and a
    logistic regression classifier to identify potential EM-affected metal segments
    within the PG network.


    *C..7 ML for Timing Optimization*Data mining was also employed to identify the
    timing discrepancies. The research detailed in [66] uses the Support Vector Classifier
    (SVC) to uncover features that reveal timing inconsistencies between observed
    silicon timing and estimated timing provided by commercial analysis tools. The
    research findings revealed that a portion of predicted critical timing paths were
    found to be non-critical in silicon. Conversely, numerous critical paths in silicon
    were not flagged as critical during prediction. The learning model revealed significant
    rules and identified paths that exhibit slower performance than estimated.


    The study in [53] addresses the discrepancy in gate delay timing between single-input
    switching (SIS) and multi-input switching (MIS) gates. The influence of MIS on
    gate delay calculations could lead to either delay overestimations or underestimations.
    To mitigate the gate delay discrepancies, polynomial regression, SVR, and ANN
    models were employed. The models were trained using gate load and slew attributes,
    and then it was integrated into the timing library to facilitate timing adjustments
    and tested on benchmark designs. The results obtained using the ANN model demonstrated
    a significant improvement in delay prediction accuracy, as it has reduced misestimations
    from 120% in traditional SIS to less than 3%. In Fig 16, the approach involved
    extracting SIS and MIS for each standard cell using SPICE simulation. The SIS
    referred to commonly used delay computation models like NLDM and CCS, while MIS
    represented the golden multi-input switching models. The flow in Fig 16 feeds
    SIS, MIS, and MIS-SIS difference (MSD) to the ANN model. The ANN model aims to
    make SIS delay predictions based on the gate''s input transitions (trA, trB),
    load capacitance (CL), and the temporal distance or skew between both input transitions
    (SA-B). This work led to significant enhancement in timing accuracy at early design
    stages.


    ![](_page_8_Figure_11.jpeg)


    Fig. 16 MIS and SIS delay convergence using ANN


    In [54], the focus was on the pre-silicon and post-silicon timing correlation.
    The author extracted the post-silicon path delay testing (PDT) and their corresponding
    estimated presilicon delays from a static timing analyzer. Eq.6 describes a path-based
    cell and net timing differences, where P P c<sup>i</sup> and n<sup>i</sup> express
    the total cell and net delay and α<sup>c</sup> and α<sup>n</sup> are the correlation
    coefficients. This study encompassed 240 different designs and revealed a noticeable
    over-pessimism in


    $$\alpha\_c \* \sum c\_i = \sum c''\_i \quad ; \quad \alpha\_n \* \sum n\_i =
    \sum n''\_i \qquad (6)$$


    - αc, αn: Cell/Net correlation factors on critical path.

    - c<sup>i</sup> , n<sup>i</sup> : Post-silicon (PDT) measured cell/net delays.

    - c ′ i , n ′ i : Pre-silicon (STA) estimated cell/net delays.


    ![](_page_9_Figure_5.jpeg)


    Fig. 17: Discrepancies in path delays between Post-Silicon and Pre-Silicon estimations


    Multiple factors can cause timing discrepancies, including circuit implementation,
    process, environment variations, and tool uncertainties. It becomes critical to
    pinpoint the factors contributing to the timing differences. In [55], the focus
    was identifying timing path variations between estimated and observed. To accomplish
    this, the author employed a statistical regression approach using support vector
    ϵ-insensitive regression (ϵ-SVR) to rank the feature''s importance in terms of
    their impact on path timing deviations. The model considered a wide range of features,
    including cell and net attributes, as well as layout-related features such as
    the number of vias in the path, transistor types, location-related parameters,
    and dynamic environment characteristics such as temperature, power grid behavior,
    voltage droop, and switching activity. This analysis aimed to shed light on the
    root causes of timing mismatches and improve the overall understanding of circuit
    performance.


    The study in [56] aimed to improve timing correlation in setup time, cell delays,
    wire delays, stage delays, and endpoints slack between an implementation tool
    and a signoff tool, in 28nm and 45nm technology designs. The approach employed
    ANN, SVM Regressor, and RF. Fig18 depict timing discrepancies that reach 110ps
    between two commercial signoff timing tools, *T1* and *T2*, and path slack discrepancies
    of 100ps between a signoff timing tool and a commercial design implementation
    tool *T1* and *D1*.


    The study in [57] presents a pre-routing timing convergence model, employing Neural
    Networks and Random Forest to reduce net delay pessimism at the pre-routing phase.
    Fig 19 illustrates a pre-routing slack estimation for a design operating at 2ns
    clock period. The red line indicates an ideal estimation of pre-routing slack
    delays using a sign-off timing analysis. The figure highlights a significant worst-case


    ![](_page_9_Figure_10.jpeg)


    Fig. 18: Slack discrepancies between signoff tools *T1* and *T2*, and commercial
    design implementation tool *D1* vs. *T1*


    ![](_page_9_Figure_12.jpeg)


    Fig. 19: Slack discrepancies at pre-pouting stage for a 2ns Clock Period Design


    scenario where slack surpasses the clock period, leading to an over-pessimism
    and over-design.


    In [58], the focus was on aligning net delays and transition time (Slew) using
    learning models. The research aimed to converge estimated wire and slew delays
    to wire and slew delays obtained from the signoff STA tool. Consequently, the
    proposed methodology reduces accumulated timing errors and maintains the correlation
    between wire and slew delays. [59] investigated the Graph-based timing analysis
    (GBA) and Path-based timing analysis (PBA) convergence. Employing the RF algorithm,
    the study aimed to align PBA estimated arrival times and slack to GBA values.
    The model implementation has effectively decreased PBA-GBA timing pessimism within
    a brief inference time.


    In [60], a learning-based method identifies memory timing errors from the pre-placement
    stage. The research uses Lasso, SVM, Gradient Boosting, and ANN models to predict
    post-layout slack from early design phases. The author in [61] tries to improve
    the circuit switching power accuracy using LR, RF, and DT models. The methodology
    trains models using multiple runs with back-annotated RC parasitics extracted
    from Standard Parasitic Exchange Format (SPEF) files. The study developed a "Spefless
    flow" capable of predicting a cell''s switching power regardless of the SPEF file.


    [62] improves pre-routing net delay and pin arrival times without relying on the
    routing engine. The author presents a Graph Neural Networks (GNN) model to rectify
    endpoint arrival times and slack at the pre-route stage. This study is conducted
    on 21 benchmark designs of 130nm. In [63], the author investigates the LR, ANN,
    RF, and XGBoost to predict crosstalk-prone nets, aiming to enhance design routability
    and timing accuracy during the routing phase. The study encompasses 12 benchmark
    designs at the 7nm scale, ranging from 3,500 to 74,000 instances. For model training,
    20 net features were employed, covering aspects such as net topology, wire delay,
    output slew, and both electrical and logical characteristics, including source
    and sink capacitance, fan-in, fan-out, wirelength, congestion, as well as neighboring
    nets'' drive strength. Among the models assessed, XGBoost exhibited the highest
    accuracy, achieving 91.12% accuracy in predicting coupling capacitance, 84.63%
    in crosstalk noise, and 87.56% in incremental delay.


    #### IV. RESULTS ANALYSIS AND DISCUSSION


    In Table I, we summarize the state-of-the-art works of ML applications in VLSI
    and CAD. We extracted relevant information such as the implemented models, the
    related PnR stage, and the design technology node employed during the training
    process. Our goal is to highlight the main findings from the literature review,
    identify gaps and limitations or weaknesses in the existing literature, and propose
    future directives.


    Most studies have implemented supervised learning models due to their reliance
    on labeled data. Regression and classification algorithms stand out as extensively
    applied, especially tree-based models Random Forest and Decision Trees, along
    with Support Vector Regressor and Gradientbased models. However, there is an evident
    surge in deep learning and Neural Networks applications. Most studies tend to
    adopt Artificial Neural Networks (ANNs), Convolutional Neural Networks (CNNs),
    and Graph Neural Networks (GNNs).


    Although significant progress has been made to enhance design placement, verification,
    timing, and power accuracy at PnR early stages, the majority of prior studies
    have focused on small and medium-sized designs, mainly within higher technology
    nodes like 28nm, 45nm, and higher. There have been limited works, such as those
    referenced in [36][48][53][63], and [61], that use 16nm, 15nm, and 7nm design
    technologies, thus representing a minority.


    Previous studies have consistently demonstrated the superior predictive accuracy
    of tree-based methodologies and neural networks. Nevertheless, these investigations
    often encountered limitations by relying on a limited set of features, typically
    around 4 to 20 input parameters. This restriction in feature selection might have
    a repercussion on the overall accuracy.


    In this review, we evaluated the quality of data used for the training process.
    The findings revealed that previous studies tend to use a limited selection of
    design architectures. This limitation restricts the scope of training data, which
    fails to achieve robust generalization capabilities, while models need exposure
    to varied design architectures during the training phase. Consequently, when subjected
    to new, unseen data, these models exhibit a lower ability to generalize, resulting
    in reduced overall accuracy.


    Another crucial aspect lies in tuning hyperparameters which is often overlooked
    in many studies. It is essential to optimize further the model''s performance.
    By neglecting hyperparameter fine-tuning, models may not achieve their maximum
    potential. Most studies have either omitted or did not mention due to space limitations.
    In addition, various studies use a reduced set of features to train models. The
    inclusion of multiple features enhances the prediction accuracy depending on the
    feature''s importance and contribution to the predictions.


    As future directives and key contributions, we suggest:


    - *Involving diverse technology nodes on the same training set*: this emphasizes
    the importance of developing models capable of accommodating different cutting-edge
    technology nodes within the same learning framework. This approach intends to
    enhance the models'' adaptability and accuracy.

    - *Incorporating large-scale industrial designs with diverse internal architectures*:
    during training, models have to learn and adapt to various design architectures
    to reflect real-world scenarios. This exposure to diverse implementations enhances
    models'' adaptability and improves the generalization.

    - *Multi-frequency runs*: one crucial aspect often overlooked and which could
    significantly enhance models'' accuracy and generalization, is conducting design
    runs at various clock frequencies. Many studies have neglected to incorporate
    running designs across a spectrum of clock frequencies. By including frequency
    as a feature in the training dataset, the model gains exposure to the behavior
    at different speeds. This offers an opportunity to enrich the model''s learning
    dataset.

    - *Use of relevant features*: a varied training feature set captures various aspects
    of circuit behavior and characteristics. This broad coverage enhances prediction
    accuracy.

    - *Hyperparameter tuning*. Tuning hyperparameters involves meticulous adjustments
    to find the most effective configuration, ensuring the model operates at its peak
    performance. This procedure identifies the best hyperparameters that enhance the
    overall accuracy.


    #### V. CONCLUSION


    Machine learning presents promising solutions to enhance the VLSI design and EDA.
    As technology progresses, following Moore''s law and the reduction in transistor
    sizes, the increase in design complexity continues. The integration of Learning
    frameworks has advanced EDA tools resulting in considerable enhancements across
    various applications. These advancements have contributed to improved chip performance,
    detection of power leaks, optimization of chip area, reduction in power consumption,
    identification of potential fabrication defects, and improved QoR accuracy.


    After conducting our review, we have identified certain limitations. Earlier studies
    were constrained by reliance on small to medium-sized designs within higher technology
    nodes like 28nm and 45nm. The incorporation of large-scale industrial designs
    enriches model learning, improving generalization. Additionally, we suggest conducting
    design runs at varied clock frequencies to exploit data at different operational
    speeds with a rich set of input features and hyperparameter tuning.


    Our ongoing endeavor involves NN application leveraging an extensive dataset of
    multiple cutting-edge 7nm and 16nm technology designs with around 100 extracted
    features. This initiative aims to enhance timing prediction accuracy at the pre-placement
    stage. This forthcoming project is built upon our previous work referenced in
    [67]. Addressing these contributions in future research endeavors will potentially
    enhance model accuracy and adaptability.


    #### REFERENCES


    - [1] M. Pandey, "Machine learning and systems for building the next generation
    of eda tools," in *2018 23rd Asia and South Pacific Design Automation Conference
    (ASP-DAC)*. IEEE, 2018, pp. 411–415.

    - [2] A. B. Kahng, "Machine learning for cad/eda: the road ahead," *IEEE Design
    & Test*, vol. 40, no. 1, pp. 8–16, 2022.

    - [3] A. C. Muller and S. Guido, ¨ *Introduction to machine learning with Python:
    a guide for data scientists*. " O''Reilly Media, Inc.", 2016.

    - [4] G. Dong and H. Liu, *Feature engineering for machine learning and data analytics*.
    CRC press, 2018.

    - [5] A. Zheng and A. Casari, *Feature engineering for machine learning: principles
    and techniques for data scientists*. " O''Reilly Media, Inc.", 2018.

    - [6] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel,
    M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg *et al.*, "Scikit-learn: Machine
    learning in python," *the Journal of machine Learning research*, vol. 12, pp.
    2825–2830, 2011.

    - [7] L. Buitinck, G. Louppe, M. Blondel, F. Pedregosa, A. Mueller, O. Grisel,
    V. Niculae, P. Prettenhofer, A. Gramfort, J. Grobler, R. Layton, J. VanderPlas,
    A. Joly, B. Holt, and G. Varoquaux, "API design for machine learning software:
    experiences from the scikit-learn project," in *ECML PKDD Workshop: Languages
    for Data Mining and Machine Learning*, 2013, pp. 108–122.

    - [8] T. Hastie, R. Tibshirani, J. H. Friedman, and J. H. Friedman, *The elements
    of statistical learning: data mining, inference, and prediction*. Springer, 2009,
    vol. 2.

    - [9] A. Geron, ´ *Hands-on machine learning with Scikit-Learn, Keras, and TensorFlow*.
    " O''Reilly Media, Inc.", 2022.

    - [10] S. Rogers and M. Girolami, *A first course in machine learning*. Chapman
    and Hall/CRC, 2016.

    - [11] R. S. Sutton and A. G. Barto, *Reinforcement learning: An introduction*.
    MIT press, 2018.

    - [12] A. M. Wichert and L. Sa-Couto, *Machine Learning-A Journey to Deep Learning:
    With Exercises and Answers*. World Scientific, 2021.

    - [13] D. Amuru, A. Zahra, H. V. Vudumula, P. K. Cherupally, S. R. Gurram, A.
    Ahmad, and Z. Abbas, "Ai/ml algorithms and applications in vlsi design and technology,"
    *Integration*, 2023.

    - [14] S. Saini, K. Lata, and G. Sinha, *VLSI and Hardware Implementations Using
    Modern Machine Learning Methods*. CRC Press, 2021.

    - [15] P. A. Beerel and M. Pedram, "Opportunities for machine learning in electronic
    design automation," in *2018 IEEE International Symposium on Circuits and Systems
    (ISCAS)*. IEEE, 2018, pp. 1–5.

    - [16] A. Malhotra and A. Singh, "Implementation of ai in the field of vlsi: A
    review," in *2022 Second International Conference on Power, Control and Computing
    Technologies (ICPC2T)*. IEEE, 2022, pp. 1–5.

    - [17] L. Wang and M. Luo, "Machine learning applications and opportunities in
    ic design flow," in *2019 international symposium on VLSI design, automation and
    test (VLSI-DAT)*. IEEE, 2019, pp. 1–3.

    - [18] A. B. Kahng, "Machine learning applications in physical design: Recent
    results and directions," in *Proceedings of the 2018 international symposium on
    physical design*, 2018, pp. 68–73.

    - [19] W. Chen, N. Sumikawa, L.-C. Wang, J. Bhadra, X. Feng, and M. S. Abadir,
    "Novel test detection to improve simulation efficiency: a commercial experiment,"
    in *Proceedings of the International Conference on Computer-Aided Design*, 2012,
    pp. 101–108.

    - [20] W. Chen, L.-C. Wang, J. Bhadra, and M. Abadir, "Simulation knowledge extraction
    and reuse in constrained random processor verification," in *Proceedings of the
    50th Annual Design Automation Conference*, 2013, pp. 1–6.

    - [21] H.-Y. Liu and L. P. Carloni, "On learning-based methods for designspace
    exploration with high-level synthesis," in *Proceedings of the 50th annual design
    automation conference*, 2013, pp. 1–7.

    - [22] B. Ozisikyilmaz, G. Memik, and A. Choudhary, "Efficient system design space
    exploration using machine learning techniques," in *Proceedings of the 45th annual
    design automation conference*, 2008, pp. 966–969.

    - [23] A. Mahapatra and B. C. Schafer, "Machine-learning based simulated annealer
    method for high level synthesis design space exploration," in *Proceedings of
    the 2014 Electronic System Level Synthesis Conference (ESLsyn)*. IEEE, 2014, pp.
    1–6.

    - [24] R. G. Kim, J. R. Doppa, and P. P. Pande, "Machine learning for design space
    exploration and optimization of manycore systems," in *2018 IEEE/ACM International
    Conference on Computer-Aided Design (IC-CAD)*. IEEE, 2018, pp. 1–6.

    - [25] T.-C. Chen, P.-Y. Lee, and T.-C. Chen, "Automatic floorplanning for ai
    socs," in *2020 International Symposium on VLSI Design, Automation and Test (VLSI-DAT)*.
    IEEE, 2020, pp. 1–2.

    - [26] A. Mirhoseini, A. Goldie, M. Yazgan, J. Jiang, E. Songhori, S. Wang, Y.-J.
    Lee, E. Johnson, O. Pathak, S. Bae *et al.*, "Chip placement with deep reinforcement
    learning," *arXiv preprint arXiv:2004.10746*, 2020.

    - [27] C.-K. Lee, "Deep learning creativity in eda," in *2020 International Symposium
    on VLSI Design, Automation and Test (VLSI-DAT)*. IEEE, 2020, pp. 1–1.

    - [28] Y. Ma, H. Ren, B. Khailany, H. Sikka, L. Luo, K. Natarajan, and B. Yu,
    "High performance graph convolutional networks with applications in testability
    analysis," in *Proceedings of the 56th Annual Design Automation Conference 2019*,
    2019, pp. 1–6.

    - [29] Y. Lin, S. Dhar, W. Li, H. Ren, B. Khailany, and D. Z. Pan, "Dreamplace:
    Deep learning toolkit-enabled gpu acceleration for modern vlsi placement," in
    *Proceedings of the 56th Annual Design Automation Conference 2019*, 2019, pp.
    1–6.

    - [30] B. Yu, D. Z. Pan, T. Matsunawa, and X. Zeng, "Machine learning and pattern
    matching in physical design," in *The 20th Asia and South Pacific design automation
    conference*. IEEE, 2015, pp. 286–293.

    - [31] S. Ward, D. Ding, and D. Z. Pan, "Pade: A high-performance placer with
    automatic datapath extraction and evaluation through high dimensional data learning,"
    in *Proceedings of the 49th Annual Design Automation Conference*, 2012, pp. 756–761.

    - [32] S. I. Ward, N. Viswanathan, N. Y. Zhou, C. C. Sze, Z. Li, C. J. Alpert,
    and D. Z. Pan, "Clock power minimization using structured latch templates and
    decision tree induction," in *2013 IEEE/ACM International Conference on Computer-Aided
    Design (ICCAD)*. IEEE, 2013, pp. 599–606.

    - [33] R. Kirby, S. Godil, R. Roy, and B. Catanzaro, "Congestionnet: Routing congestion
    prediction using deep graph neural networks," in *2019 IFIP/IEEE 27th International
    Conference on Very Large Scale Integration (VLSI-SoC)*. IEEE, 2019, pp. 217–222.

    - [34] W.-T. J. Chan, Y. Du, A. B. Kahng, S. Nath, and K. Samadi, "Beol stack-aware
    routability prediction from placement using data mining techniques," in *2016
    IEEE 34th international conference on computer design (ICCD)*. IEEE, 2016, pp.
    41–48.


    Journal of Integrated Circuits and Systems, vol. 19, n. 2, 2024 13


    - [35] W.-H. Chang, L.-D. Chen, C.-H. Lin, S.-P. Mu, M. C.-T. Chao, C.- H. Tsai,
    and Y.-C. Chiu, "Generating routing-driven power distribution networks with machine-learning
    technique," in *Proceedings of the 2016 on International Symposium on Physical
    Design*, 2016, pp. 145–152.

    - [36] W.-T. J. Chan, P.-H. Ho, A. B. Kahng, and P. Saxena, "Routability optimization
    for industrial designs at sub-14nm process nodes using machine learning," in *Proceedings
    of the 2017 ACM on International Symposium on Physical Design*, 2017, pp. 15–21.

    - [37] A. F. Tabrizi, N. K. Darav, S. Xu, L. Rakai, I. Bustany, A. Kennings, and
    L. Behjat, "A machine learning framework to identify detailed routing short violations
    from a placed netlist," in *Proceedings of the 55th Annual Design Automation Conference*,
    2018, pp. 1–6.

    - [38] Z. Qi, Y. Cai, and Q. Zhou, "Accurate prediction of detailed routing congestion
    using supervised data learning," in *2014 IEEE 32nd international conference on
    computer design (ICCD)*. IEEE, 2014, pp. 97–103.

    - [39] Q. Zhou, X. Wang, Z. Qi, Z. Chen, Q. Zhou, and Y. Cai, "An accurate detailed
    routing routability prediction model in placement," in *2015 6th Asia Symposium
    on Quality Electronic Design (ASQED)*. IEEE, 2015, pp. 119–122.

    - [40] Y.-H. Huang, Z. Xie, G.-Q. Fang, T.-C. Yu, H. Ren, S.-Y. Fang, Y. Chen,
    and J. Hu, "Routability-driven macro placement with embedded cnn-based prediction
    model," in *2019 Design, Automation & Test in Europe Conference & Exhibition (DATE)*.
    IEEE, 2019, pp. 180–185.

    - [41] A. B. Kahng, M. Luo, and S. Nath, "Si for free: machine learning of interconnect
    coupling delay and transition effects," in *2015 ACM/IEEE International Workshop
    on System Level Interconnect Prediction (SLIP)*. IEEE, 2015, pp. 1–8.

    - [42] R. Medico, D. Spina, D. V. Ginste, D. Deschrijver, and T. Dhaene, "Machine-learning-based
    error detection and design optimization in signal integrity applications," *IEEE
    Transactions on Components, Packaging and Manufacturing Technology*, vol. 9, no.
    9, pp. 1712– 1720, 2019.

    - [43] L.-C. Wang, "Experience of data analytics in eda and test—principles, promises,
    and challenges," *IEEE Transactions on Computer-Aided Design of Integrated Circuits
    and Systems*, vol. 36, no. 6, pp. 885– 898, 2016.

    - [44] D. Ding, J. A. Torres, and D. Z. Pan, "High performance lithography hotspot
    detection with successively refined pattern identifications and machine learning,"
    *IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems*,
    vol. 30, no. 11, pp. 1621–1634, 2011.

    - [45] Y.-T. Yu, Y.-C. Chan, S. Sinha, I. H.-R. Jiang, and C. Chiang, "Accurate
    process-hotspot detection using critical design rule extraction," in *Proceedings
    of the 49th Annual Design Automation Conference*, 2012, pp. 1167–1172.

    - [46] Y.-C. Fang, H.-Y. Lin, M.-Y. Sui, C.-M. Li, and E. J.-W. Fang, "Machine-learning-based
    dynamic ir drop prediction for eco," in *2018 IEEE/ACM International Conference
    on Computer-Aided Design (IC-CAD)*. IEEE, 2018, pp. 1–7.

    - [47] Y. Yamato, T. Yoneda, K. Hatayama, and M. Inoue, "A fast and accurate per-cell
    dynamic ir-drop estimation method for at-speed scan test pattern validation,"
    in *2012 IEEE International Test Conference*. IEEE, 2012, pp. 1–8.

    - [48] S.-Y. Lin, Y.-C. Fang, Y.-C. Li, Y.-C. Liu, T.-S. Yang, S.-C. Lin, C.-M.
    Li, and E. J.-W. Fang, "Ir drop prediction of eco-revised circuits using machine
    learning," in *2018 IEEE 36th VLSI Test Symposium (VTS)*. IEEE, 2018, pp. 1–6.

    - [49] H. Dhotre, S. Eggersgluß, and R. Drechsler, "Identification of efficient
    ¨ clustering techniques for test power activity on the layout," in *2017 IEEE
    26th Asian Test Symposium (ATS)*. IEEE, 2017, pp. 108–113.

    - [50] F. Ye, F. Firouzi, Y. Yang, K. Chakrabarty, and M. B. Tahoori, "On-chip
    voltage-droop prediction using support-vector machines," in *2014 IEEE 32nd VLSI
    Test Symposium (VTS)*. IEEE, 2014, pp. 1–6.

    - [51] S. Dey, S. Nandi, and G. Trivedi, "Powerplanningdl: Reliability-aware framework
    for on-chip power grid design using deep learning," in *2020 Design, Automation
    & Test in Europe Conference & Exhibition (DATE)*. IEEE, 2020, pp. 1520–1525.

    - [52] ——, "Machine learning for vlsi cad: A case study in on-chip power grid
    design," in *2021 IEEE Computer Society Annual Symposium on VLSI (ISVLSI)*. IEEE,
    2021, pp. 378–383.

    - [53] O. S. Ram and S. Saurabh, "Modeling multiple-input switching in timing
    analysis using machine learning," *IEEE Transactions on Computer-Aided Design
    of Integrated Circuits and Systems*, vol. 40, no. 4, pp. 723–734, 2020.

    - [54] L.-C. Wang, P. Bastani, and M. S. Abadir, "Design-silicon timing correlation:
    A data mining perspective," in *Proceedings of the 44th annual Design Automation
    Conference*, 2007, pp. 384–389.

    - [55] P. Bastani, N. Callegari, L.-C. Wang, and M. S. Abadir, "Statistical diagnosis
    of unmodeled systematic timing effects," in *Proceedings of the 45th annual Design
    Automation Conference*, 2008, pp. 355–360.

    - [56] S.-S. Han, A. B. Kahng, S. Nath, and A. S. Vydyanathan, "A deep learning
    methodology to proliferate golden signoff timing," in *2014 Design, Automation
    & Test in Europe Conference & Exhibition (DATE)*. IEEE, 2014, pp. 1–6.

    - [57] E. C. Barboza, N. Shukla, Y. Chen, and J. Hu, "Machine learningbased pre-routing
    timing prediction with reduced pessimism," in *2019 56th ACM/IEEE Design Automation
    Conference (DAC)*. IEEE, 2019, pp. 1–6.

    - [58] A. B. Kahng, S. Kang, H. Lee, S. Nath, and J. Wadhwani, "Learningbased
    approximation of interconnect delay and slew in signoff timing tools," in *2013
    ACM/IEEE International Workshop on System Level Interconnect Prediction (SLIP)*.
    IEEE, 2013, pp. 1–8.

    - [59] A. B. Kahng, U. Mallappa, and L. Saul, "Using machine learning to predict
    path-based slack from graph-based timing analysis," in *2018 IEEE 36th International
    Conference on Computer Design (ICCD)*. IEEE, 2018, pp. 603–612.

    - [60] W.-T. J. Chan, K. Y. Chung, A. B. Kahng, N. D. MacDonald, and S. Nath,
    "Learning-based prediction of embedded memory timing failures during initial floorplan
    design," in *2016 21st Asia and South Pacific Design Automation Conference (ASP-DAC)*.
    IEEE, 2016, pp. 178–185.

    - [61] M. Chentouf, C. Naimy, and Z. E. A. A. Ismaili, "Machine learning application
    for early power analysis accuracy improvement: A case study for cells switching
    power," in *2021 International Conference on Microelectronics (ICM)*. IEEE, 2021,
    pp. 17–20.

    - [62] Z. Guo, M. Liu, J. Gu, S. Zhang, D. Z. Pan, and Y. Lin, "A timing engine
    inspired graph neural network model for pre-routing slack prediction," 2022.

    - [63] R. Liang, Z. Xie, J. Jung, V. Chauha, Y. Chen, J. Hu, H. Xiang, and G.-
    J. Nam, "Routing-free crosstalk prediction," in *2020 IEEE/ACM International Conference
    On Computer Aided Design (ICCAD)*. IEEE, 2020, pp. 1–9.

    - [64] L.-C. Wang and M. S. Abadir, "Data mining in eda-basic principles, promises,
    and constraints," in *Proceedings of the 51st Annual Design Automation Conference*,
    2014, pp. 1–6.

    - [65] P. Velickovic, G. Cucurull, A. Casanova, A. Romero, P. Lio, Y. Bengio *et
    al.*, "Graph attention networks," *stat*, vol. 1050, no. 20, pp. 10– 48 550, 2017.

    - [66] J. Chen, B. Bolin, L.-C. Wang, J. Zeng, D. Drmanac, and M. Mateja, "Mining
    ac delay measurements for understanding speed-limiting paths," in *2010 IEEE International
    Test Conference*. IEEE, 2010, pp. 1–10.

    - [67] Y. Attaoui, M. Chentouf, Z. E. A. A. Ismaili, and A. El Mourabit, "Machine
    learning application for cell delay accuracy improvement at post-placement stage:
    A case study for combinational cells," *Integration*, 2023.'
- title: LLM-Based Human-Robot Collaboration Framework for Manipulation Tasks
  abstract: ''
  keywords: ''
  document: '## 1. Introduction


    As robotics technology advances, the potential for robots to assist with domestic
    chores becomes increasingly promising. With the ability to understand and process
    natural language, these robots become more adaptable and flexible to accommodate
    a wide range of user instructions[1]. However, the previous works with LLM-based
    control sometimes show a relatively low accuracy for high intelligence task decisionmaking[1].
    Our work introduces the idea of "LLM-Based task planning with human-robot collaboration",
    which is a novel approach to enhance human supervision in LLMbased autonomy. The
    contributions of this paper are summarized as follows: (1) Our LLM converts high-level
    language commands into sequences of executable motion functions, enabling adaptability
    to various user instructions. (2) Additionally, teleoperation and DMP are utilized
    for motion demonstration which allows the robot to learn from human guidance and
    potentially improves task feasibility and generalizability. (3) Furthermore, the
    robot is empowered with environmental perception through YOLO-based perception
    module for targeted tasks. The position of objects will be registered once recognized
    and update with the real-time position. Combining these elements, the proposed
    approach opens new possibilities for seamless human-robot collaboration in household
    tasks, making robots more practical and adaptable.


    <span id="page-0-0"></span>![](_page_0_Figure_12.jpeg)


    Fig. 1. Framework of LLM-based task planning with enhanced HRC.


    ## 2. LLM-based human-robot collaboration framework


    The system diagram is illustrated in Fig. [1.](#page-0-0) The system consists
    of three main components: the user, LLM, and the robot, which forms an interactive
    loop. Additionally, we introduce a skilled teleoperator as an assistant to enhance
    the overall system''s generalizability and feasibility.


    ## 2.1 LLM-based task planning


    In our approach, we build our model based on LLM (GPT-2) and train it using a
    text corpus following previous work done by other researchers[2], enabling LLM
    to provide accurate function predictions in response to specific instructions.
    Subsequently, we integrate the perceived target position information and motion
    functions obtained from LLM into a prepared code template, enabling the robot
    to execute the corresponding tasks effectively. To efficiently manage task execution,
    we adopt a hierarchical approach in our work-treating long-horizon tasks, short-horizon
    tasks, and motion functions as three layers. For long-horizon tasks which include
    motion functions of more than 10, we consider them first-layer tasks. In such
    cases, these tasks are separated into multiple short-horizon tasks through LLM.
    However, short-horizon tasks which involve less than 10 motion functions, are
    treated as the second layer task. When LLM receives commands from these second-layer
    tasks, it directly returns the functions necessary to accomplish the designated
    tasks.


    <sup>∗</sup>Corresponding author email: zhu@robo.mein.nagoya-u.ac.jp


    This work has been submitted to the IEEE for possible publication. Copyright may
    be transferred without notice, after which this version may no longer be accessible


    | Tasks                             | Objects Num                         |    |
    Fins | SIR  | Bxec | TESIB |

    |-----------------------------------|-------------------------------------|----|------|------|------|-------|

    | Catch                             | bottle,<br>cup                      | 10
    | 4    | 0.80 | 1.00 | 1.00  |

    | *Catch                            | bowl                                | -  |
    4    | 0.00 | 1.00 | 0.00  |

    | Put/<br>Place                     | bowl,<br>cabinet,<br>cup,<br>bottle | 10
    | 7    | 0.80 | 1.00 | 1.00  |

    | Open                              | cabinet                             | ഗ  |
    7    | 0.60 | 1.00 | 1.00  |

    | Clean<br>the top<br>of<br>cabinet | cabinet                             | ળ  |
    17   | 0.40 | 0.80 | 0.80  |


    ## Fig. 2. Experiment data.


    On the other hand, the first-layer tasks can be separated into multiple short-horizon
    tasks through LLM. Subsequently, we process each short-horizon task following
    its specific procedure to divide them into motion functions as mentioned. Finally,
    motion functions are organized by following a planned task sequence to construct
    the complete long-horizon task. This hierarchical task handling allows for a more
    organized and effective execution of both short and long-horizon tasks, contributing
    to our system''s overall efficiency and accuracy.


    ## 2.2 DMP-based task correction


    To enhance the generalizability of LLM-based autonomy, We propose to integrate
    DMP-based task correction with human teleoperation-driven demonstrations. Dynamic
    Movement Primitives (DMP) is a generic approach for trajectory modeling in an
    attractor landscape based on differential dynamical systems [3]. In this paper,
    we leverage our previously developed teleoperation system[4], [5] which can intuitively
    control the robot motion through a VR device, and also utilize DMP to record trajectories
    obtained from manual teleoperation. These trajectories can then be reproduced
    to complement any deficiencies in the LLM-based autonomy, particularly in failed
    function sequence generation or function sequence impracticality.


    For instance, when we issue the command "catch the bowl," the default motion function
    for bowl grasping could be inadequate to complete the task. To address this issue,
    we switch to the DMP-based teleoperation mode and provide instructions for the
    desired action. The robot can then accurately reproduce the trajectory using DMP.
    This approach will be continually developed to manage a wider range of long-horizon
    tasks, with the ultimate goal of creating an effective Human-Robot Collaboration
    (HRC) system. This system will strategically take advantage of both human flexibility,
    in terms of adaptability and problem-solving skills, and robot autonomy, in terms
    of precision and efficiency.


    ## 3. Experiment and Result


    We conducted multiple experiments by providing "catch", "put", "open" and long-horizon
    tasks-"clean the top of the cabinet" for several objects to assess their success
    rates (SR), executability (Exec), and feasibility (FSB). The indicator Num means
    the number of trials, and Fns shows the motion functions used in completing the
    task. Additionally, Exec is defined as if the task is executable in the environment,
    and FSB represents if the motion is feasible to reach the goal. The experimental
    results are presented in Fig. 2. In the case of the indicator "Exec" showing 0.80
    in the "clean the top of the cabinet" task, the reasonable explanation is the
    randomness of LLM, which has a low probability of generating incorrect responses
    (*Exec* = 0.2). As for the FBS of 0.00 in the "catch the bowl" task, this outcome
    can be attributed to the task being impossible to complete due to the default
    motion function being unsuitable for the target object''s shape. In such cases,
    the DMP-based task correction is used to make necessary demonstrations.


    ## 4. Conclusion


    In this work, we have successfully proposed a LLM-based task-planning method.
    An interface is built to integrate the LLM, perception pipeline, teleoperation
    system, and DMPbased task correction. The results show that the robot can execute
    the command from the user with a considerable success rate for short-horizon tasks
    like "catch", "put", or "open". Especially, for the task with 0.00 FBS, such as
    "catch the bowl", DMP-based correction is introduced to improve it. However, for
    long-horizon tasks, it shows a relatively low success rate. The reason could be
    the error accumulating with motion. The future work includes the improvement of
    DMPbased task correction and fine-tuning teleoperation which can complement the
    error from hardware to improve the success rate and feasibility.


    ## 5. Acknowledgement


    This work was supported in part by JST Trilateral AI Research, Japan, under Grant
    JPMJCR20G8; and in part by JSPS KAKENHI under Grant JP22K14222; and in part by
    NCGG under Chojuiryou Kenkyukaihatsuhi Nos. 19–5, 21- 21.


    ## References


    - [1] I. Singh, V. Blukis, A. Mousavian, A. Goyal, D. Xu, J. Tremblay, D. Fox,
    J. Thomason, and A. Garg, "Progprompt: Generating situated robot task plans using
    large language models," in *2023 IEEE International Conference on Robotics and
    Automation (ICRA)*, 2023, pp. 11 523–11 530.

    - [2] G. Chalvatzaki, A. Younes, D. Nandha, A. T. Le, L. F. R. Ribeiro, and I.
    Gurevych, "Learning to reason over scene graphs: A case study of finetuning GPT-2
    into a robot language model for grounded task planning," *CoRR*, vol. abs/2305.07716,
    2023. [Online]. Available: <https://doi.org/10.48550/arXiv.2305.07716>

    - [3] R. Wang, Y. Wu, W. L. Chan, and K. P. Tee, "Dynamic movement primitives
    plus: For enhanced reproduction quality and efficient trajectory modification
    using truncated kernels and local biases," in *2016 IEEE/RSJ International Conference
    on Intelligent Robots and Systems (IROS)*, 2016, pp. 3765–3771.

    - [4] J. Nakanishi, S. Itadera, T. Aoyama, and Y. Hasegawa, "Towards the development
    of an intuitive teleoperation system for human support robot using a vr device,"
    *Advanced Robotics*, vol. 34, no. 19, pp. 1239– 1253, 2020.

    - [5] Y. Zhu, B. Jiang, Q. Chen, T. Aoyama, and Y. Hasegawa, "A shared control
    framework for enhanced grasping performance in teleoperation," *IEEE Access*,
    vol. 11, pp. 69 204–69 215, 2023.'
- title: 'Cocco: Hardware-Mapping Co-Exploration towards Memory Capacity-Communication
    Optimization'
  abstract: 'Memory is a critical design consideration in current dataintensive DNN
    accelerators, as it profoundly determines energy consumption, bandwidth requirements,
    and area costs. As DNN structures become more complex, a larger on-chip memory
    capacity is required to reduce data movement overhead, but at the expense of silicon
    costs. Some previous works have proposed memory-oriented optimizations, such as
    different data reuse and layer fusion schemes. However, these methods are not
    general and potent enough to cope with various graph structures.

    In this paper, we explore the intrinsic connection between network structures
    and memory features to optimize both hardware and mapping. First, we introduce
    a graph-level execution scheme with a corresponding dataflow and memory management
    method. This scheme enables the execution of arbitrary graph patterns with high
    data reuse and low hardware overhead. Subsequently, we propose Cocco, a hardware-mapping
    co-exploration framework leveraging graph-level features of networks. It aims
    to minimize communication overhead, such as energy consumption and bandwidth requirements,
    with a smaller memory capacity. We formulate the graph-partition scheduling and
    memory configuration search as an optimization problem and employ a genetic-based
    method to achieve efficient co-exploration for large and irregular networks. Experiments
    demonstrate that Cocco obtains lower external memory access, lower bandwidth requirements,
    and more stable optimization for graph partition compared to the greedy algorithm
    and dynamic programming introduced in prior works. Cocco also reduces the costs
    by 1.89% to 50.33% using co-exploration compared to other typical methods.

    ASPLOS ''24, April 27-May 1, 2024, La Jolla, CA, USA © 2024 Copyright held by
    the owner/author(s). ACM ISBN 979-8-4007-0372-0/24/04. <https://doi.org/10.1145/3617232.3624865>

    CCS Concepts: • Hardware → Design reuse and communication-based design; On-chip
    resource management; • Computer systems organization → Parallel architectures;
    • Software and its engineering → Compilers.

    Keywords: Design space exploration, Memory, Graph analysis, Subgraph, Genetic
    algorithm, Deep learning accelerator'
  keywords: ''
  document: '# Cocco: Hardware-Mapping Co-Exploration towards Memory Capacity-Communication
    Optimization


    Zhanhong Tan tanzh19@mails.tsinghua.edu.cn IIIS, Tsinghua University Beijing,
    China


    Zijian Zhu zhuzj23@mails.tsinghua.edu.cn IIIS, Tsinghua University Beijing, China


    Kaisheng Ma<sup>∗</sup> kaisheng@mail.tsinghua.edu.cn IIIS, Tsinghua University
    Beijing, China


    # Abstract


    Memory is a critical design consideration in current dataintensive DNN accelerators,
    as it profoundly determines energy consumption, bandwidth requirements, and area
    costs. As DNN structures become more complex, a larger on-chip memory capacity
    is required to reduce data movement overhead, but at the expense of silicon costs.
    Some previous works have proposed memory-oriented optimizations, such as different
    data reuse and layer fusion schemes. However, these methods are not general and
    potent enough to cope with various graph structures.


    In this paper, we explore the intrinsic connection between network structures
    and memory features to optimize both hardware and mapping. First, we introduce
    a graph-level execution scheme with a corresponding dataflow and memory management
    method. This scheme enables the execution of arbitrary graph patterns with high
    data reuse and low hardware overhead. Subsequently, we propose Cocco, a hardware-mapping
    co-exploration framework leveraging graph-level features of networks. It aims
    to minimize communication overhead, such as energy consumption and bandwidth requirements,
    with a smaller memory capacity. We formulate the graph-partition scheduling and
    memory configuration search as an optimization problem and employ a genetic-based
    method to achieve efficient co-exploration for large and irregular networks. Experiments
    demonstrate that Cocco obtains lower external memory access, lower bandwidth requirements,
    and more stable optimization for graph partition compared to the greedy algorithm
    and dynamic programming introduced in prior works. Cocco also reduces the costs
    by 1.89% to 50.33% using co-exploration compared to other typical methods.


    ASPLOS ''24, April 27-May 1, 2024, La Jolla, CA, USA © 2024 Copyright held by
    the owner/author(s). ACM ISBN 979-8-4007-0372-0/24/04. <https://doi.org/10.1145/3617232.3624865>


    CCS Concepts: • Hardware → Design reuse and communication-based design; On-chip
    resource management; • Computer systems organization → Parallel architectures;
    • Software and its engineering → Compilers.


    Keywords: Design space exploration, Memory, Graph analysis, Subgraph, Genetic
    algorithm, Deep learning accelerator


    #### ACM Reference Format:


    Zhanhong Tan, Zijian Zhu, and Kaisheng Ma. 2024. Cocco: Hardware-Mapping Co-Exploration
    towards Memory Capacity-Communication Optimization. In 29th ACM International
    Conference on Architectural Support for Programming Languages and Operating Systems,
    Volume 1 (ASPLOS ''24), April 27-May 1, 2024, La Jolla, CA, USA. ACM, New York,
    NY, USA, [16](#page-15-0) pages. <https://doi.org/10.1145/3617232.3624865>


    ## <span id="page-0-0"></span>1 Introduction


    The evolution of neural network topology has driven the remarkable progress of
    artificial intelligence from the early single-layer perceptron (SLP) [\[45,](#page-14-0)
    [54\]](#page-14-1) and multi-layer perceptron (MLP) [\[17,](#page-13-0) [22,](#page-13-1)
    [39\]](#page-13-2) to modern DNNs with plain [\[36,](#page-13-3) [57\]](#page-14-2)/inception
    [\[59\]](#page-14-3)/residual [\[20,](#page-13-4) [55\]](#page-14-4) structures
    based on manual design, and even irregular structures using neural architecture
    search (NAS) [\[53,](#page-14-5) [75\]](#page-15-1) or random network generation
    [\[68\]](#page-14-6). These technological innovations have resulted in increasingly
    complex computation graphs, which pose challenges for efficient memory design
    and deployment.


    Memory design is crucial in the accelerator system, as it performs data preparation
    at the start of each processing stage according to the scheduling scheme, determining
    energy consumption, bandwidth requirements, and area costs. Figure [1](#page-1-0)
    shows the trade-off between the on-chip memory size and the external memory access
    in DNN accelerators. A smaller on-chip buffer (left side) saves area but requires
    more data reloading. A larger buffer (right side) can reduce external memory access
    and save energy and bandwidth but at the cost of increasing the memory overhead.
    An excessively large SRAM may not be feasible due to the high silicon area cost,
    typically ranging from 1 to 2 mm<sup>2</sup> /MB in 12nm, and the high energy
    overhead, dozens of times that of a MAC operation for a large SRAM.


    Therefore, the key problem is: between the two extremes in Figure [1,](#page-1-0)
    how to find an appropriate memory configuration with efficient workload mapping
    and data management, especially under the growing complexity of neural network
    architectures.


    <sup>∗</sup>Corresponding author.


    Permission to make digital or hard copies of part or all of this work for personal
    or classroom use is granted without fee provided that copies are not made or distributed
    for profit or commercial advantage and that copies bear this notice and the full
    citation on the first page. Copyrights for thirdparty components of this work
    must be honored. For all other uses, contact the owner/author(s).


    ASPLOS ''24, April 27-May 1, 2024, La Jolla, CA, USA Zhanhong Tan, Zijian Zhu,
    and Kaisheng Ma


    <span id="page-1-0"></span>![](_page_1_Figure_1.jpeg)


    Figure 1. The effect of different memory capacities for a computation graph. Intermediate
    results can be buffered in the on-chip memory if it is large enough. The on-chip
    memory of small capacity can only buffer two nodes (marked in the red dotted box),
    and the larger memory can cover a larger subgraph (right side).


    The critical status of memory design has attracted extensive research. Most previous
    studies focus on simple layer-level optimization (the left one of Figure [1\)](#page-1-0)
    by applying loop transformation techniques such as tiling and reordering to fit
    the memory size and reuse the on-chip data [\[23,](#page-13-5) [43,](#page-14-7)
    [44,](#page-14-8) [61,](#page-14-9) [70\]](#page-15-2). In addition, several works
    also guide the memory capacity and hierarchy design using designspace exploration
    [\[12,](#page-12-0) [32,](#page-13-6) [37,](#page-13-7) [66,](#page-14-10) [67\]](#page-14-11).
    However, these layerlevel optimizations are confined to the limited intra-layer
    reuse, which is insufficient for memory-intensive networks. A subgraph-level scheme
    (e.g., the middle one and the right one of Figure [1\)](#page-1-0) provides a
    larger optimization space via inter-layer reuse [\[3,](#page-12-1) [4,](#page-12-2)
    [38,](#page-13-8) [73\]](#page-15-3) to reduce the I/O overhead. Therefore, this
    paper aims to leverage the subgraph-level computing flow to optimize the memory
    capacity and external communication for networks with any topology.


    However, there are three primary challenges to fully exploit the subgraph-level
    optimization.


    First, we need a general execution flow for any sub-graph. Due to the various
    kernel sizes and strides, a parent node in a subgraph may have unbalanced data
    requirements from its consumers, which makes it difficult to determine the tensor
    tiling scheme and the memory allocation for each node (layer). In the traditional
    single-layer execution, we usually divide a large tensor into loop tiles, which
    are processed through a series of regular computing steps. Similarly, we want
    the sub-graph execution to be a series of elementary computing steps with a simple
    control flow.


    Second, we require a suitable memory management method for the subgraph execution.
    Due to complicated dependency among nodes in a subgraph, careful management is
    needed to reuse overlapping and inter-layer intermediate data.


    Solving these two challenges contributes to a basic hardware execution model compatible
    with subgraph-level optimization. However, we also encounter the third challenge:
    how to partition a model into subgraphs and how much memory to allocate. The optimization
    space is huge, so we need to devise a search method with high sampling efficiency
    to find a proper subgraph partition and memory configuration result.


    In this paper, we first introduce a complete graph-level scheme for memory. In
    particular, it contains a consumptioncentric flow that enables the execution of
    arbitrary subgraphs with low memory footprints (for challenge 1). Accordingly,
    we provide an explicit memory dataflow and the corresponding memory management
    scheme for effective data reuse (for challenge 2). Building on the graph-level
    memory scheme, we propose Cocco, a hardware-mapping co-exploration framework,
    to establish a connection between model features and the memory configuration
    (for challenge 3).


    Cocco aims to find a combination of on-chip buffers and the corresponding graph-level
    scheduling for lower memory and communication overhead. In particular, we develop
    a genetic-based algorithm to efficiently explore the search space of graph partitions
    and the associated memory configuration for a series of neural networks.


    In summary, this work makes the following contributions:


    - Subgraph execution scheme. We first introduce a consumption-centric flow to
    determine a low-cost execution sequence by throttling and aligning the dataflow.

    - Efficient dataflow and memory management for subgraph data reuse. We propose
    a memory management scheme featuring multiple reconfigurable regions and the corresponding
    dataflow to support arbitrary subgraph execution with full data reuse.

    - Hardware-mapping co-exploration framework. Based on the subgraph execution scheme
    and memory dataflow, we propose Cocco, a genetic-based framework combining the
    graph-level partition and memory design-space exploration together. Cocco achieves
    1.89% to 50.33% lower costs (lower communication with a smaller size) using co-exploration
    in contrast to other methods.


    ## 2 Background and Motivation


    #### 2.1 Design of Neural Network Accelerators


    The DNN accelerator unit is the most basic execution unit in a computing system,
    on top of which, we can scale it out to many-core, many-socket, and many-drawer
    systems [\[24,](#page-13-9) [40,](#page-13-10) [48,](#page-14-12) [60\]](#page-14-13).
    An accelerator unit usually employs a processing element (PE) array on a sophisticated
    interconnection network to enable efficient tensor-level computation. Each PE
    typically contains local scratchpads and ALUs to process basic data packets. The
    global buffer and the weight buffer store activations and weights, and they are
    generally


    <span id="page-2-0"></span>![](_page_2_Figure_1.jpeg)


    *\* Those designs only support INT8 precision for tensor, we scale to FP16 performance
    by a factor of 0.5. \*\* For most designs fabricated under 12nm (or close to)
    process, we align all areas to 12nm. The SRAM area is estimated as 1.2mm<sup>2</sup>/MB.*


    Figure 2. Left: performance v.s. memory capacity of several industrial NPUs. Right:
    a summary of SRAM area ratio in these accelerators.


    located next to the PE array to serve as the data interface and manage data between
    the PE array and the external memory (e.g., DRAM or other cores). Due to the limited
    capacity of the global buffer, the compiler has to partition the network execution
    into a series of elementary workloads that are scheduled along the parallel spatial
    resources and the temporal dimension [\[18,](#page-13-11) [61,](#page-14-9) [72\]](#page-15-4).
    The capacity of the global buffer usually dominates the external memory access
    and bandwidth requirements, significantly impacting system performance. If the
    global memory is larger, it is more likely to buffer more intermediate data and
    avoid data being evicted to DRAM. As shown in Figure [1,](#page-1-0) a larger
    buffer expands the scope of elementary workloads from a single layer to a larger
    subgraph, reducing the communication overhead.


    However, choosing an appropriate memory specification is always a challenge. In
    Figure [2,](#page-2-0) we surveyed 16 popular industrial neural network processors
    with various memory/performance/area characteristics, where nine of them target
    the training domain [\[6,](#page-12-3) [11,](#page-12-4) [24,](#page-13-9) [34,](#page-13-12)
    [35,](#page-13-13) [40,](#page-13-10) [41,](#page-13-14) [48,](#page-14-12) [60,](#page-14-13)
    [63,](#page-14-14) [69\]](#page-14-15) and seven target model inference [\[1,](#page-12-5)
    [7,](#page-12-6) [8,](#page-12-7) [26–](#page-13-15)[28,](#page-13-16) [49,](#page-14-16)
    [65\]](#page-14-17). According to the survey, we can observe several trends as
    follows:


    - 1. Memory occupies a significant portion of the silicon footprint on an NPU
    chip, ranging from 4% to 79% of the area, with capacities from 2.5MB to 896MB.

    - 2. Figure [2](#page-2-0) Left shows a trend of diminishing marginal benefit
    of memory capacity. This is because there is a critical capacity to meet the data
    reuse and bandwidth requirement at the beginning, and the increments become negligible
    with higher memory capacity.

    - 3. We can infer that there is a saturated capacity equivalent to the ideal unlimited
    memory, especially for the inference design. For example, Hanguang [\[26\]](#page-13-15)
    is a special SRAM-only inference system without DDR, and the 394MB buffers are
    large enough to hold the intermediate data in their scenarios.


    <span id="page-2-1"></span>![](_page_2_Figure_10.jpeg)


    Figure 3. Evaluations on subgraphs fusing different number of layers (denoted
    as L=1,3,5). Y-axis is in the log domain. The 2TOPS NPU accelerator is configured
    with a 1MB global buffer and a 1.125MB weight buffer. The bandwidth requirement
    of weights is from the prefetch of the next subgraph, while that of activations
    is from the inputs and outputs of each subgraph.


    This survey implies a design trade-off between memory capacity and performance
    based on workloads and commercial considerations. Motivated by the observations
    above, this paper aims to provide several memory design considerations and study
    the connection between workload features and memory capacity in an NPU accelerator.


    #### 2.2 Workload Deployment


    A neural network is usually executed in a DNN accelerator with layer or graph
    granularities based on the buffer capacity and dataflow.


    2.2.1 Layer-level Assignment. This manner assigns tasks layer by layer. Most previous
    studies employ a tiling-based layer-wise execution manner [\[10,](#page-12-8)
    [21,](#page-13-17) [30,](#page-13-18) [37,](#page-13-7) [50,](#page-14-18) [61\]](#page-14-9),
    which elaborates the tiling sizes of tensors to fit in the accelerator buffers
    and maintain performance. A proper tiling scheme should overlap the data loading
    latency with the computing time of each tile and try to reduce the repeated access
    of local weight buffers. Tiles of data are transferred between the external memory
    and the global buffer, and PEs subsequently fetch data from the global to their
    local buffers. Given the larger bit-width of partial sums (e.g., 24bit partial
    sums v.s. 8bit inputs in Simba), the output-centric tiling scheme is more commonly
    used to calculate the final results before writing back to the global buffer [\[61\]](#page-14-9).


    2.2.2 Graph-level Assignment. Unlike the layer-level assignment that restrains
    from leveraging inter-layer reuse, a graph-level assignment processes several
    layers of a neural network as a whole. To demonstrate the effectiveness of the
    layer-level assignment, we evaluate four networks on a 2TOPS accelerator model,
    as shown in Figure [3.](#page-2-1) The results show that fusing layers into subgraphs
    significantly reduces external memory access by 42.3% ∼ 74.7% and average bandwidth
    requirements by 26.8% ∼ 67.8%. However, the improvements of larger subgraphs are
    marginal, indicating that there is an optimal trade-off between inter-layer


    reuse and subgraph size, which determines the memory requirement. For example,
    executing three-layer subgraphs reduces external memory access by 53.7% in ResNet50,
    while executing five-layer subgraphs only further reduces it by 13.6%.


    Several works have studied inter-layer reuse and graph partition. However, they
    have several limitations in terms of performance and flexibility. LCP [\[42\]](#page-14-19)
    groups similar layers into a cluster and executes them as a whole, which makes
    it challenging to generalize into an arbitrary graph. Fused-CNN [\[4\]](#page-12-2)
    and SR-CNN [\[38\]](#page-13-8) fuse large contiguous layers for plain networks
    using manually-designed strategies. Irregular-NN [\[73\]](#page-15-3) attempts
    to execute a complex subgraph using a DP-based algorithm, but the constrained
    search space limits the exploration.


    To overcome these challenges, we propose an end-to-end framework that automatically
    optimizes the graph partition and memory configuration for any neural network.
    Our framework consists of two main components: a graph-level dataflow and a hardware-mapping
    co-exploration algorithm. We first introduce the graph-level dataflow and its
    hardware implementation. Then, we present Cocco, an efficient algorithm that explores
    the trade-offs among memory configurations and graph partition schemes based on
    workload features.


    ## <span id="page-3-1"></span>3 The Proposed Graph-Level Scheme


    To execute layers on an NPU core in a graph-level manner, we need an effective
    approach to reuse intermediate data and decide the memory allocation. This section
    presents our comprehensive scheme for subgraph execution, which addresses the
    first two challenges mentioned in Section [1.](#page-0-0) First, we describe a
    multi-layer execution flow that minimizes the memory footprint by a friendly tiling
    approach (for challenge 1). Second, we explain how to implement this flow on a
    real NPU using an efficient data reuse pattern (for challenge 2). The consistent
    target is to reduce the memory footprint and be friendly to implementation.


    #### 3.1 Subgraph execution scheme


    It is common practice for the layer-level scheduling to partition the output tensor
    into several tiles as layer-level elementary operations [\[56,](#page-14-20) [61,](#page-14-9)
    [72,](#page-15-4) [74\]](#page-15-5), simplifying the scheduling and instruction
    generation. Likewise, our high-level idea is also to generate a series of explicit
    subgraph-level elementary operations. However, we need to address the challenges
    of various kernel sizes and strides in different paths to prevent unbalanced data
    production and unnecessary memory.


    A model''s subgraph consists of multiple layers (nodes) with dependencies. Section
    [4](#page-5-0) provides detailed information on subgraph partition. In Figure
    [4\(](#page-3-0)a), we present a straightforward production-centric scheme for
    executing a subgraph


    with different kernel sizes in two branches, deriving tile sizes of the subsequent
    layers based on the predetermined input tile sizes. For example, we can produce
    a 1 × 1 tile of Node(0) and a 2 × 2 tile of Node(2) with a given 5 × 5 feature
    map of input Node(-1). In this case, these intermediate results only reduce to
    1 × 1 in Node(3), limited by the smallest input of Node(0), so the remaining results
    of Node(2) can not be consumed immediately. As shown in Figure [4,](#page-3-0)
    three extra data of Node(2) along with sixteen extra source data of Node(1) take
    up extra memory space. There are more redundant cached data when the subgraph
    becomes larger and more complicated. Disadvantages of this manner are attributed
    to the production-centric idea that consumes all related activations from the
    producers at once.


    To avoid the memory overhead of storing unused data, we propose a consumption-centric
    scheme in Figure [4\(](#page-3-0)b), where results of each node are produced on
    demand based on consumer(s) (i.e., output node(s)). For example, given a 1 × 1
    tile of Node(3), we derive the 1 × 1 tile size for Node(2), which subsequently
    decides a 3 × 3 tile for Node(1).


    The backward-derivation for each producer node is nontrivial because of diverse
    kernel sizes and strides in different paths. Therefore, we propose a three-stage
    flow to determine the behavior of each node, as illustrated in Figure [5.](#page-4-0)
    The highlevel idea is to let output nodes drive the whole execution and match
    the data consumption and production in each subgraph-level elementary operation.


    The stage-1 is similar to the traditional single-layer scheduling, where the tile
    size is optimized for higher computation utilization. In order to hold a larger
    subgraph, the tile size


    <span id="page-3-0"></span>![](_page_3_Figure_14.jpeg)


    Figure 4. A conceptual comparison between two manners to process a subgraph. The
    node marked with a negative number represents the input node. The corresponding
    subgraph is shown in the upper right, where × / refers to the convolution kernel
    size ( ) and stride ().


    Cocco: Hardware-Mapping Co-Exploration towards Memory ... ASPLOS ''24, April 27-May
    1, 2024, La Jolla, CA, USA


    <span id="page-4-0"></span>![](_page_4_Figure_1.jpeg)


    Figure 5. The flow to determine the execution scheme of a subgraph (i.e., the
    computed tile size of each node, the tile offset, and the processing sequence
    of nodes). For simplicity, we discuss the 1D-CONV in this example and it is similar
    in the 2D-CONV case.


    tends to be smaller. In the 1D-CONV example, we set the tile size to be 2 for
    output nodes.


    The stage-2 aims to determine the data update offset Δ and the memory allocation
    size for each node based on the consumer(s), processing in the reverse topological
    order. We use the least common multiply (LCM) operation to determine Δ () of producers
    for aligning different input offset requirements (Δ () () ) from consumers. Hence,
    one producer update may correspond to multiple updates of a consumer. For example,
    Δ (−2) = lcm{Δ (0) (0) , Δ (1) (1) } = 4 = 2Δ (1) (1) , one update of Node(-2)
    corresponds to two updates of Node(1). As for the tile size deduction, (Δ () /
    () ) is to derive the required input tile size (,) for output node [1](#page-4-1)
    , where Δ () / () is the consumer offset (updated data) per producer update. The
    maximum result (,) of all outputs is the tile size () of input node . In this
    example, (−2) = max{<sup>0</sup> (2), <sup>1</sup> (4)} = 6 and (−1) = max{<sup>1</sup>
    (2), <sup>2</sup> (2)} = 4.


    As mentioned above, since we use LCM to align production and consumption, one
    producer update may correspond to multiple updates of a consumer. In the stage-3,
    we use \_ to represent the number of memory update per subgraph elementary operation.
    The generated result of the example in Figure [5](#page-4-0) is shown in Figure
    [6.](#page-4-2) \_ of Node(-1), Node(1), and Node(2) are two, where the second
    updates are highlighted in red boxes. Note that the {\_(−2) , . . . , \_(2) }
    solution is not unique, but the unique co-prime one {1, 2, 1, 2, 2} corresponds
    to the minimal elementary operation.


    <span id="page-4-2"></span>![](_page_4_Figure_8.jpeg)


    Figure 6. The memory snapshot during two subgraph elementary operations based
    on the execution scheme of Figure [5](#page-4-0) example. The allocated memory
    size and update offset correspond to and Δ, respectively (the [:] notation denotes
    data ranging from index to ). The arrows denote the data dependency according
    to the node relation in the subgraph.


    The proposed flow is based on a general directed acyclic computation graph and
    is not limited to specific layer features. In this way, we can determine the execution
    scheme for any complex irregular network like NasNet [\[75\]](#page-15-1) and
    RandWire [\[68\]](#page-14-6).


    #### 3.2 Memory Management for the subgraph execution


    Up to now, we have inferred the execution scheme for subgraphs, and the remaining
    challenge is how to implement it on hardware efficiently. Figure [7](#page-5-1)
    shows the memory allocation and update scheme for the subgraph execution. Before
    computing a subgraph, the compiler determines logical blocks for input, intermediate,
    and output nodes, where the block sizes depend on the tile sizes derived from
    the execution flow.


    For convenient management, we introduce two types of memory regions: MAIN and
    SIDE. The MAIN region stores the source data for PE (i.e., the tile of <sup>0</sup>
    × <sup>0</sup> × in Figure [7\)](#page-5-1). The SIDE region reserves the horizontally
    overlapping data[2](#page-4-3) . Considering no reuse requirement for some output
    nodes, we only need a MAIN region to buffer the results of the current tile. Except
    for the input nodes (negative numbers) loading data from DRAM, the other nodes
    update data locally based on the computed results of the input node(s).


    In detail, the update scheme leverages the collaboration between the MAIN region
    and the SIDE region to achieve full reuse across sliding tiles (we consider kernel
    size > stride). As shown in Figure [7,](#page-5-1) when the convolution windows
    slide across the feature maps, the vertical overlap data (e.g., column = 5) are
    reused locally in the MAIN region. In contrast, the horizontally overlapping data
    (e.g., the first row of = 6 ∼ 8) are loaded from the SIDE region (path ①). Only
    a subset of data is replaced by the newly calculated results


    <span id="page-4-1"></span><sup>1</sup>For example, assume node is a convolution
    layer with kernel size () and stride () , then ( ) = () + ( − 1) × () .


    <span id="page-4-3"></span><sup>2</sup>We assume the column is the inner loop
    while the row is the outer loop.


    (marked in green). Besides, the bottom horizontal slices write new data to the
    SIDE region for the next row loop (path ②).


    The extra hardware overhead for the proposed memory scheme is slight. Figure [8](#page-5-2)
    presents our 12nm NPU core for the subgraph processing, with a buffer region manager
    to logically partition the global buffer to support contiguous layer processing.
    The buffer region manager is a 2-depth register file, where determines the maximum
    subgraph size, and each entry pair indicates the start and the end address for
    each region. The area overhead is quite small, and in our test chip, the area
    ratio is only 0.18% with = 64 and 272-byte size (17-bit address for the 1MB 64bit-width
    global buffer).


    In summary, our high-level idea is to divide the buffer into logical blocks for
    different layers and try to reuse data for sliding convolution windows. The memory
    management approach can be compatible with an accelerator as long as it supports
    the data movement inside the on-chip memory and flexible data assignment for computing.
    Coupled with our subgraph execution scheme introduced before, intermediate outputs
    in the subgraph can avoid being recomputed. Only those layers required by other
    subgraphs are written back to DRAM for further reuse.


    # <span id="page-5-0"></span>4 Memory Communication-Capacity Co-Exploration


    <span id="page-5-1"></span>**A subgraph example**


    The aforementioned hardware model enables arbitrary subgraph execution, but there
    is always limited buffer capacity


    **Data update diagram**


    ![](_page_5_Figure_6.jpeg)


    Figure 7. Memory allocation and data update scheme in the global buffer for full
    data reuse. The data layout used in our implementation is NWHC8c (aligned to 8
    channels), which can be changed in another design. <sup>0</sup> and <sup>0</sup>
    are the height and width of an input tile; is the input channel size; is the global
    width-dimension index of the input tensor; and <sup>0</sup> is the width-dimension
    index of an input tile.


    <span id="page-5-2"></span>![](_page_5_Figure_9.jpeg)


    Figure 8. Hardware implementation with the buffer region manager in our 12nm NPU
    as a demonstration. The layout is an NPU core extracted from part of our in-house
    chip.


    in hardware. Therefore, we need to partition the whole computation graph into
    a series of subgraphs that fit the memory. Below, we move up to the optimization
    for graph partition and memory design-space exploration for challenge 3.


    #### 4.1 Problem Formulation


    4.1.1 Graph-Level Partition. Formally, a DNN model can be represented as a computation
    graph = ( , ), where is the vertex set consisting of all the layers in a DNN model,
    and is the edge set that defines the structure of DNN. In particular, an edge
    (, ) ∈ represents that the output of layer is an input of layer .


    We aim to find a partition scheme : → N that assigns each layer to a subgraph,
    where layer ∈ is computed in the ()-th subgraph. A valid partition scheme should
    satisfy that any layer is computed before use. Therefore, for any (, ) ∈ , we
    have () ≤ (). Moreover, any subgraph should be connected in , otherwise meaningless.


    We cast the partition exploration as an optimization problem. The objective is
    to find a valid partition scheme that minimizes the total cost:


    <span id="page-5-4"></span>

    $$\sum\_{i} Cost\_{\mathcal{M}}(\{v \in V \mid P(v) = i\}),\tag{1}$$


    where is a cost function of a given subgraph based on a target metric (e.g., external
    memory access (EMA) and energy). For each subgraph, the EMA cost contains the
    loading of weights and input activations and the storage of output activations[3](#page-5-3)
    . The energy cost includes the overhead of EMA, on-chip buffers, and computation
    units.


    4.1.2 Design-Space Exploration (DSE). Our work further extends the optimization
    to combine with the memory design-space exploration. In this paper, we focus on
    the global buffer and the weight buffer, given that they dominate


    <span id="page-5-3"></span><sup>3</sup>The nodes that are required to write-back
    to DRAM can be the model output layer or the layers required by the future subgraph.


    the overhead of energy and area in an NPU core. As illustrated in Figure [1,](#page-1-0)
    a larger buffer capacity can take in more layers inside a subgraph, reducing communication
    costs but compromising the silicon area. To co-explore the hardware configuration
    and mapping, we construct an objective function by a linear combination of the
    hardware and mapping costs:


    <span id="page-6-0"></span>

    $$\text{BUF\\_SIZE} + \alpha \cdot \sum\_{i} Cost\_{\mathcal{M}}(\{v \in V \mid
    P(v) = i\}),\qquad \text{(2)}$$


    where is a preference hyper-parameter to adjust the proportion between two costs.


    #### 4.2 Baseline Methods


    Several optimization methods that exist today can perform graph-level partition.
    However, most of them fail to directly co-explore hardware and partition. Below,
    we list four typical methods as our baselines and sketch their features.


    4.2.1 Enumeration-based Algorithm. Fused-CNN [\[4\]](#page-12-2) applies a straightforward
    way to enumerate all possible partition schemes and return the best one. Jangda
    et al. [\[25\]](#page-13-19) proposed state compression dynamic programming to
    speed up the enumeration-based algorithm. We migrate their methods as our baseline
    and further improve them by only recording one subgraph in the state to reduce
    the time complexity.


    Nonetheless, there are still exponential states in the improved implementation.
    Let be the number of nodes in a graph, and the enumeration-based method may explore
    up to (2 <sup>2</sup> ) states for irregular networks. Consequently, the search
    is hard to complete within a reasonable search time for large-scale networks,
    not to mention the co-exploration with DSE.


    4.2.2 Greedy Algorithm. Halide [\[47\]](#page-14-21) employs a greedy algorithm
    to perform function grouping, which can be applied to the graph-level partition.
    Specifically, it first assigns each layer into a single-layer subgraph. Then it
    iteratively fuses a pair of subgraphs contributing the greatest benefit until
    all benefits are negative.


    Therefore, this algorithm tends to be trapped at the local minimum. Moreover,
    since the fusion decision rules are based on a given hardware, the greedy method
    cannot co-explore with DSE.


    4.2.3 Dynamic Programming (DP)-based Algorithm. For the irregular network scheduling
    problem, Zheng et al. [\[73\]](#page-15-3) proposed a DP-based algorithm. They
    arrange the layers based on their depth and perform DP in a sequential manner.


    This method is restricted to assigning layers that are contiguous in the depth
    order into a subgraph, hence the exploration is confined to constrained search
    space. It is unlikely to find the global optimum, especially for non-plain network


    structures. In addition, since the state transition of DP depends on the predefined
    buffer size, it is also tough to carry out co-exploration.


    4.2.4 Simulated Annealing (SA). SA [\[33\]](#page-13-20) is a popular optimization
    algorithm that samples a point and updates it iteratively to improve. It adopts
    the new sample points with a probability affected by the performance difference
    and a hyper-parameter named temperature. We employ the customized mutation operations
    (described in Section [4.4.3\)](#page-7-0) to update the sample points and implement
    an SA-based algorithm as a baseline.


    SA is an alternative optimization method for our framework with compatible operators,
    but it is not stable as the genetic algorithm in a range of benchmarks, which
    will be shown in later experiments.


    #### 4.3 Genetic Algorithm


    Previous research shows competitive performance of the Genetic Algorithm (GA)
    in several scheduling optimization problems [\[30,](#page-13-18) [31\]](#page-13-21).
    We summarize several benefits of GA for our hardware-mapping co-exploration problem:


    - 1. White-box property: We can track and tune its optimization process conveniently.
    Therefore, it is easy and intuitive to understand.

    - 2. Complete search space: It has the potential to explore the complete search
    space by customized mutation and crossover operations.

    - 3. Avoid local optima: In contrast to the greedy algorithm, GA can naturally
    jump out of the local minimum benefiting from the diversity of the population.

    - 4. Flexible initialization: We can use the results of other optimization algorithms
    to initialize GA and use GA to finetune the result.

    - 5. Co-exploration: Through the proposed GA operations and genome encoding, it
    can further support partition-DSE co-exploration.


    We encode each candidate solution (partition scheme and the corresponding memory
    configuration for our problem) as a genome, and the population contains a set
    of genomes. The GA goes through a series of generations to obtain a lower cost.
    It performs the crossover and mutation operations on the population in each generation.
    Specifically, a crossover operation blends two genomes selected from the population
    to generate one offspring while a mutation operation modifies a genome randomly.
    At the end of each generation, the evaluation environment evaluates the fitness
    of each genome, and the population in the new generation is selected based on
    the fitness results.


    #### 4.4 Cocco Optimization Framework


    Cocco is a GA-based optimization framework that enables the co-exploration of
    memory configuration and graph-level partition, as shown in Figure [10.](#page-7-1)
    The core of Cocco is a


    <span id="page-7-2"></span>![](_page_7_Figure_2.jpeg)


    Figure 9. Illustration of crossover and mutation operations in Cocco.


    series of operations that explore a complete search space. We build a genetic
    algorithm based on these customized operations. Fed with the neural network structure
    and DSE requirements, Cocco goes through several steps to get the optimization
    results. The execution model described in Section [3](#page-3-1) is embedded in
    the evaluation environment. In the following, we introduce the five stages of
    Cocco.


    4.4.1 Initialization. The first step in Cocco is to generate the initial population,
    where each genome contains a partition scheme of the computation graph and a memory
    configuration for DSE. For the DSE part, every genome selects a capacity value
    in a given range following a uniform distribution. There are two options in Cocco
    to initialize the partition scheme of each genome. The first option is random
    initialization. Precisely, we determine the () for each layer ∈ in topological
    order, and each () is selected randomly within the valid range. The other option
    is to initialize the partition scheme from other optimization algorithms.


    4.4.2 Crossover. We designed a customized crossover operation to inherit and blend
    the features of two parents selected from the population. Specifically, each hardware
    configuration (i.e., memory capacity) in the offspring is the average of its parents
    and then rounds to the nearest candidate value. For the partition scheme, we assign
    layers to subgraphs in topological order. Each undecided layer chooses one parent
    randomly to reproduce the corresponding subgraph. If the reproduced subgraph contains
    layers that have been decided, we split out a new one excluding those layers,
    or merge it with one of the subgraphs to which the decided layers belong.


    As shown in Figure [9\(](#page-7-2)b), layer 1 and layer 3 select Dad as the parent
    to reproduce the subgraphs {1, 2} and {3, 4}, respectively. Next, layer 5 selects
    Mom as its parent, so it


    <span id="page-7-1"></span>![](_page_7_Figure_8.jpeg)


    Figure 10. Cocco framework overview.


    intends to reproduce subgraph {4, 5, 6}. However, since we have already decided
    on layer 4 in subgraph {3, 4}, there are two alternatives: creating a new subgraph
    {5, 6} (Child-1) or merging with subgraph {3, 4} to obtain {3, 4, 5, 6} (Child-2).


    <span id="page-7-0"></span>4.4.3 Mutation. Four mutation operations are customized
    for the optimization flow to explore the search space extensively. We guarantee
    the validity of genomes after each mutation in the implementation. At the bottom
    of Figure [9,](#page-7-2) we show a node-level operation (modify-node) and two
    subgraph-level ones (split-subgraph and merge-subgraph):


    - modify-node (Figure [9\(](#page-7-2)c)): Modify the assignment of a randomly
    selected node : from → () to → ′ (), where ′ () can be an existed subgraph or
    a new one.

    - split-subgraph (Figure [9\(](#page-7-2)d)): Split a randomly selected subgraph
    into two or more subgraphs.

    - merge-subgraph (Figure [9\(](#page-7-2)e)): Merge two randomly selected subgraphs
    into one subgraph.

    - mutation-DSE (not shown): Modify the memory configuration to a random one within
    the range. The new values are sampled based on a normal distribution, where the
    average is the original value, and the variance is a hyper-parameter.


    4.4.4 Evaluation. Since GA tries to maximize the fitness of the genomes, we set
    fitness to be the opposite of the cost (e.g., Formula [1](#page-5-4) and [2\)](#page-6-0).
    To evaluate the fitness of each genome in the population, we use our simulator
    (introduced in the next section) to extract the execution costs of subgraphs (e.g.,
    EMA and energy).


    During the evaluation, the simulator decodes the subgraph and hardware configuration
    of each genome and calculates the fitness by aggregating the cost of each subgraph.
    Particularly, when a large subgraph exceeds the buffer capacity, we perform the
    split-subgraph operation to ensure genome validity. This kind of in-situ tuning
    can increase the number of valid samples during the optimization operations and
    thus, improve the sample efficiency.


    4.4.5 Selection. At the end of each generation, Cocco performs the tournament
    selection. Specifically, it holds multiple tournaments among a few randomly selected
    genomes, and the winners (the genome with the best fitness) of these tournaments
    form the population of a new generation. This operation facilitates superior fitness
    in the new generation. The number of genomes in each tournament is decided by
    a hyper-parameter. The new generation subsequently starts from the crossover step
    again.


    ## 5 Experiments


    In the evaluations, we first present the superiority of Cocco for the graph partition;
    and then demonstrate its outstanding stability and sample efficiency of the co-exploration
    for the hardware optimization, followed by additional discussions about the results
    under different configurations.


    #### 5.1 Methodology


    5.1.1 Evaluated Models. In the following evaluations, we consider three types
    of model structures: plain (VGG16 [\[57\]](#page-14-2)), multi-branch (ResNet50,
    ResNet152 [\[20\]](#page-13-4), GoogleNet [\[59\]](#page-14-3), Transformer [\[64\]](#page-14-22),
    and GPT [\[52\]](#page-14-23)), and irregular structure (RandWire-A/B [\[68\]](#page-14-6)
    and NasNet [\[75\]](#page-15-1)). RandWire-A/B are


    generated based on the small and regular regime configurations introduced in the
    paper [\[68\]](#page-14-6). FC layers are transformed to 1×1 CONV while pooling
    and element-wise layers are analyzed as depth-wise CONV without weights. The scalar
    operations (e.g., activation function) are hidden in the pipeline (e.g., the post-process
    module following PE in Simba [\[56\]](#page-14-20)) and their overhead can be
    ignored.


    5.1.2 Accelerator Platform. As shown at the top of Figure [10,](#page-7-1) we
    consider a SIMBA-like hierarchical accelerator with a global buffer, a weight
    buffer, and a 4×4 PE array in each core used in several previous works [\[56,](#page-14-20)
    [61,](#page-14-9) [71\]](#page-15-6). Each PE contains an 8×8 MAC array to process
    a sub-tile from the global buffer. In particular, we model the execution flow
    based on the scheme described in Section [3.](#page-3-1) The parallelism of two
    dimensions of the PE array can be dynamically configured by the mapper results
    to ensure high utilization. We schedule subgraphs in topological order and prefetch
    weights of the next subgraph during the current computing. We also extend our
    platform to support fundamental multi-core studies by interconnecting cores with
    a crossbar. They share weights to release the burden of each core.


    The arithmetic and memory overhead is extracted in a 12nm library based on the
    synthesized RTL implementations (SRAM based on the ARM memory compiler) with 1GHz.
    The DRAM energy is set as 12.5pJ/bit [\[70\]](#page-15-2). The extra footprint
    of the plug-in design is mainly a 272-Byte register file to store the head and
    end logical region addresses of maximal 64 nodes, which is negligible. Based on
    off-the-shelf evaluators Timeloop [\[50\]](#page-14-18) and MAESTRO [\[37\]](#page-13-7)
    for spatial accelerators, we developed a modified simulator that supports the
    evaluation of latency and energy. It employs the consumption-centric scheme to
    determine the tile size of each layer, and the memory access in the model is free
    from padding data. The latency per subgraph depends on the maximum of the calculation
    and external communication cycles. We allocate 16GB/s DRAM bandwidth per accelerator
    core for loading weights and input activations and writing back data for subsequent
    subgraphs. The off-chip communication consists of weight loading of each layer
    and the inputs and outputs of each subgraph. As described in Section [3,](#page-3-1)
    our subgraph execution scheme avoids recomputing of intermediate outputs.


    5.1.3 Baselines. Three optimization baselines for graph partition are the greedy
    algorithm used in Halide [\[47\]](#page-14-21), dynamic programming (DP) used
    in Irregular-NN [\[73\]](#page-15-3) , and the enumeration-based method as a reference.


    For the DSE studies, we compare Cocco with simulated annealing (SA) [\[33\]](#page-13-20)
    to demonstrate the better stability of GA. These two methods are both the co-optimization
    scheme that optimizes partition and hardware settings at the same time. In contrast
    to co-optimization, the two-step scheme is another method for design-space exploration.
    Specifically, we


    <span id="page-9-0"></span>![](_page_9_Figure_2.jpeg)


    Figure 11. The evaluation results for graph partition using the EMA-opt configuration
    (EMA as the optimization metric). The enumeration-based method is deterministic,
    which figures out the optimal solution as a reference in the first four models.
    It cannot complete for large-scale models (Transformer, GPT, RandWire-A, and RandWire-B)
    in a reasonable time because of the exponential search space.


    use random search (RS) or grid search (GS) to sample memory capacity candidates
    and then explore the corresponding partition schemes. During the search, we evaluate
    5,000 samples for each capacity candidate and keep the best candidate as the output.
    As for the sampling method, RS randomly samples memory capacity candidates while
    GS uses a coarser granularity to enumerate the candidates.


    #### 5.2 Graph Partition Evaluations


    We start by presenting the partition performance on the single-core hardware with
    a 1MB global buffer and a 1.125MB weight buffer. The number of samples in Cocco
    is set to be 400,000. We evaluate the external memory access (EMA) and bandwidth
    requirements of eight models shown in Figure [11,](#page-9-0) where the results
    are normalized to the Halide baseline. This experiment aims to validate the effectiveness
    of our Cocco framework in graph partition. For networks with simpler structures,
    Cocco can find out the optimal solutions same as the enumeration-based results.
    For large-scale irregular networks (Transformer, GPT, RandWire-A, and RandWire-B),
    the enumeration-based method cannot complete in a reasonable time, while Cocco
    provides better solutions than Halide and DP. A better subgraph partition strategy
    helps to ease the communication burden, thus reducing the EMA cost and bandwidth
    requirements.


    #### 5.3 Hardware-Mapping Co-Exploration


    After learning the superiority of Cocco for the graph partition, we further co-explore
    the memory configuration and graph partition mapping as the core study of this
    work. Three categories of exploration methods are used, including the fixed hardware
    scheme, the two-step scheme as baselines, and the proposed co-optimization scheme.
    We set three fixed memory configurations with Small capacity, Medium capacity,
    and Large capacity, followed by a partition-only procedure. The two-step scheme
    is implemented with decoupled steps for capacity search (RS or GS) and partition
    (GA). The cooptimization methods include the proposed Cocco and an SA-based one
    as the comparison. All methods sample up to


    <span id="page-9-2"></span>Table 1. Hardware-mapping co-exploration for separate
    buffer. In this table, A refers to the global buffer, and W refers to the weight
    buffer. We evaluate the cost using Formula [2](#page-6-0) (the lower cost, the
    better), where the metric is energy. We use RandWire-A as RandWire in the following
    experiments.


    | Optimization |        | ResNet50      |                   |        | GoogleNet
    |                   |        |  |

    |--------------|--------|---------------|-------------------|--------|-----------|-------------------|--------|--|

    |              |        |               | Size (A) Size (W) | Cost   |           |
    Size (A) Size (W) | Cost   |  |

    |              | Buf(S) | 512KB         | 576KB             | 1.04E7 | 512KB     |
    576KB             | 4.07E6 |  |

    | Fixed<br>HW  |        | Buf(M) 1024KB | 1152KB            | 1.07E7 | 1024KB    |
    1152KB            | 5.06E6 |  |

    |              | Buf(L) | 2048KB        | 2304KB            | 1.24E7 | 2048KB    |
    2304KB            | 7.18E6 |  |

    |              | RS+GA  | 448KB         | 864KB             | 1.04E7 | 384KB     |
    432KB             | 3.88E6 |  |

    | Two-Step     | GS+GA  | 128KB         | 864KB             | 1.07E7 | 128KB     |
    144KB             | 3.80E6 |  |

    |              | SA     | 256KB         | 360KB             | 1.06E7 | 192KB     |
    144KB             | 3.78E6 |  |

    | Co-Opt       | Cocco  | 704KB         | 864KB             | 1.04E7 | 192KB     |
    432KB             | 3.75E6 |  |

    |              |        |               |                   |        |           |                   |        |  |

    |              |        |               | RandWire          |        |           |
    NasNet            |        |  |

    | Optimization |        |               | Size (A) Size (W) | Cost   |           |
    Size (A) Size (W) | Cost   |  |

    |              | Buf(S) | 512KB         | 576KB             | 3.23E6 | 512KB     |
    576KB             | 6.14E7 |  |

    | Fixed        |        | Buf(M) 1024KB | 1152KB            | 3.92E6 | 1024KB    |
    1152KB            | 5.83E7 |  |

    | HW           | Buf(L) | 2048KB        | 2304KB            | 6.00E6 | 2048KB    |
    2304KB            | 5.66E7 |  |

    |              | RS+GA  | 448KB         | 792KB             | 3.31E6 | 1152KB    |
    2016KB            | 5.60E7 |  |

    | Two-Step     | GS+GA  | 128KB         | 144KB             | 3.02E6 | 2048KB    |
    2304KB            | 5.66E7 |  |

    | Co-Opt       | SA     | 192KB         | 144KB             | 3.00E6 | 2048KB    |
    1872KB            | 5.61E7 |  |


    50,000 points. The energy-capacity co-optimization is used in the following evaluations.


    5.3.1 DSE analysis using separate and shared buffer. We first perform the hardware-mapping
    co-exploration to determine the suitable memory configuration (except for the
    fixed-HW scheme) with = 0.002[4](#page-9-1) and then solely execute the partition-only
    Cocco to obtain the final cost. In particular, we also compared the results using
    two memory designs: separate buffer and shared buffer. For the separate buffer
    design, activations and weights are stored in different buffers while they share
    the same space in the shared buffer design. The memory capacity candidates for
    the global buffer


    <span id="page-9-1"></span><sup>4</sup>The energy and the capacity units are pJ
    and Byte, respectively.


    Cocco: Hardware-Mapping Co-Exploration towards Memory ... ASPLOS ''24, April 27-May
    1, 2024, La Jolla, CA, USA


    <span id="page-10-0"></span>Table 2. Hardware-mapping co-exploration for shared
    buffer. We evaluate the cost using Formula [2](#page-6-0) (the lower cost, the
    better), where the metric is energy.


    | Optimization |        |        | ResNet50 | GoogleNet |        |  |

    |--------------|--------|--------|----------|-----------|--------|--|

    |              |        | Size   | Cost     | Size      | Cost   |  |

    | Fixed<br>HW  | Buf(S) | 576KB  | 1.01E7   | 576KB     | 3.66E6 |  |

    |              | Buf(M) | 1152KB | 1.00E7   | 1152KB    | 4.04E6 |  |

    |              | Buf(L) | 2304KB | 1.04E7   | 2304KB    | 5.09E6 |  |

    |              | RS+GA  | 1280KB | 0.98E7   | 640KB     | 3.65E6 |  |

    | Two-Step     | GS+GA  | 1344KB | 0.98E7   | 512KB     | 3.65E6 |  |

    |              | SA     | 896KB  | 1.00E7   | 192KB     | 3.75E6 |  |

    | Co-Opt       | Cocco  | 1344KB | 0.98E7   | 384KB     | 3.60E6 |  |

    |              |        |        |          |           |        |  |

    |              |        |        | RandWire | NasNet    |        |  |

    | Optimization |        | Size   | Cost     | Size      | Cost   |  |

    |              | Buf(S) | 576KB  | 2.83E6   | 576KB     | 6.36E7 |  |

    | Fixed        | Buf(M) | 1152KB | 3.03E6   | 1152KB    | 5.73E7 |  |

    | HW           | Buf(L) | 2304KB | 3.90E6   | 2304KB    | 5.51E7 |  |

    |              | RS+GA  | 320KB  | 2.85E6   | 2560KB    | 5.47E7 |  |

    | Two-Step     | GS+GA  | 832KB  | 2.86E6   | 3072KB    | 5.42E7 |  |

    | Co-Opt       | SA     | 256KB  | 2.92E6   | 1728KB    | 5.56E7 |  |


    (for activations) range from 128KB to 2048KB with a 64KB interval, while that
    for the weight buffer range from 144KB to 2304KB with a 72KB interval. The exploration
    range of the shared buffer is from 128KB to 3072KB with an interval of 64KB.


    The evaluation using separate buffers is shown in Table [1,](#page-9-2) where
    Cocco achieves better optimization with up to 1.89% (compared to SA in ResNet50)
    to 50.33% (compared to Fixed-HW(L) in RandWire) lower cost compared to various
    baselines across four models. The two-step scheme fails to combine the information
    between different sizes, so it is generally worse than the co-optimization method.


    The capacity results also reflect the inherent capacity preference of different
    models. The data amount in GoogleNet and RandWire is relatively smaller, and thus
    their capacity requirements are lower. In contrast, the data amount in NasNet
    is larger, so a high capacity is preferred.


    As shown in Table [2,](#page-10-0) the evaluation of the shared buffer setting
    shows a similar trend. Furthermore, we can observe that most of the cost results
    of the shared buffer are lower than those using the separate configuration. Although
    the shared buffer design requires additional control flows, it indeed improves
    efficiency than the separate buffer design.


    5.3.2 Sample efficiency analysis. We next study the sample efficiency of the two-step
    and the co-optimization scheme in Figure [12.](#page-10-1) We record the cost
    trends of the first 50,000 samples on ResNet50, GoogleNet, and RandWire during
    the exploration. Overall, Cocco shows a consistent convergence trend on these
    three networks. And it converges faster and


    <span id="page-10-1"></span>![](_page_10_Figure_9.jpeg)


    Figure 12. The convergence curve of Cocco compared with other baselines in the
    hardware-mapping co-explorations. The optimization method requiring fewer samples
    in (d) has higher sample efficiency.


    <span id="page-10-2"></span>![](_page_10_Figure_11.jpeg)


    Figure 13. The visualization of sample points distribution during optimization.
    The slope of the red dashed line denotes the preference between energy and capacity
    cost. The point on the line with a lower intercept has a smaller cost.


    achieves lower costs compared to other baselines, exhibiting a higher sample efficiency.
    The two-step methods perform graph-partition separately under different capacities,
    so they fail to utilize the partition information between capacities. Particularly,
    the GS method uses a deterministic search direction (search from large to small
    capacity in this experiment), so the convergence time depends on the optimal capacity.
    Since GoogleNet and RandWire require relatively small buffers, GS takes a considerable
    number of samples to converge.


    5.3.3 Optimization procedure analysis. We next study how the distribution of sample
    points changes during the optimization procedure of Cocco. While searching for
    20 generations with 500 genomes each, we divided them into ten groups with different
    colors in Figure [13.](#page-10-2) The results show that the distribution moves
    towards a lower intercept


    <span id="page-11-0"></span>![](_page_11_Figure_1.jpeg)


    Figure 14. The trade-off between energy and memory capacity. The optimization
    target is to minimize the cost defined in Formula [2,](#page-6-0) where the metric
    is energy. Energy results of each model are normalized to the first (= 0.0005)
    results.


    <span id="page-11-1"></span>Table 3. Multi-core and batch evaluation using the
    energycapacity co-opt configuration. Size denotes the shared buffer size in each
    core.


    | Core# Batch |             |            | ResNet50     |            | GoogleNet           |                 |              |  |

    |-------------|-------------|------------|--------------|------------|---------------------|-----------------|--------------|--|

    |             |             | Energy(mJ) | Lat.(ms)     |            | Size(KB)
    Energy(mJ) | Lat.(ms)        | Size(KB)     |  |

    | 1           | 1           | 4.21       | 4.59         | 1344       | 1.61                |
    2.05            | 384          |  |

    |             | 2           | 6.32       | 8.98         | 1728       | 2.18                |
    3.91            | 896          |  |

    |             | 8           | 11.88      | 35.93        | 2880       | 5.64                |
    15.53           | 1472         |  |

    |             | 1           | 4.38       | 2.48         | 768        | 1.66                |
    1.04            | 192          |  |

    | 2           | 2           | 6.46       | 4.78         | 1088       | 2.34                |
    1.99            | 384          |  |

    |             | 8           | 13.01      | 19.12        | 1664       | 5.84                |
    7.97            | 960          |  |

    |             | 1           | 4.29       | 1.39         | 448        | 1.34                |
    0.54            | 192          |  |

    | 4           | 2           | 6.58       | 2.68         | 640        | 2.20                |
    1.07            | 192          |  |

    |             | 8           | 11.50      | 10.71        | 1664       | 6.24                |
    4.30            | 448          |  |

    |             |             |            |              |            |                     |                 |              |  |

    |             |             |            | RandWire     |            |                     |
    NasNet          |              |  |

    |             | Core# Batch | Energy(mJ) | Lat.(ms)     |            | Size(KB)
    Energy(mJ) | Lat.(ms)        | Size(KB)     |  |

    |             | 1           | 1.26       | 1.47         | 384        | 28.57               |
    49.92           | 2624         |  |

    | 1           | 2           | 2.25       | 2.74         | 704        | 47.68               |
    99.87           | 3072         |  |

    |             | 8           | 8.66       | 10.85        | 1664       | 133.03              |
    396.90          | 3072         |  |

    |             | 1           | 1.41       | 0.95         | 192        | 29.18               |
    24.93           | 1728         |  |

    | 2           | 2           | 2.37       | 1.80         | 384        | 48.80               |
    49.73           | 2624         |  |

    |             | 8           | 8.39       | 7.16         | 1280       | 153.25              |
    227.19          | 3072         |  |

    |             | 1           | 1.39       | 0.71         | 192        | 28.00               |
    14.56           | 960          |  |

    | 4           | 2           | 2.91       | 1.40<br>5.55 | 192<br>960 | 45.03               |
    28.58<br>133.38 | 1664<br>2816 |  |


    of the -slope line and gets more centralized in the later generations during the
    optimization process of Cocco.


    #### 5.4 Sensitivity Study about Cocco framework


    5.4.1 Study of in the cost function. The results shown in Figure [14](#page-11-0)
    demonstrate the effectiveness of in adjusting the preference between the memory
    capacity and the given metric (energy is used here). The optimization trades the
    memory capacity for lower energy cost with the increase of . In addition, a larger
    memory capacity indeed contributes to lower energy, but the yields show differences
    because of their various model-inherent graph and layer patterns. For example,
    NasNet is more memory-intensive and more structure-complex than the other three
    models, so it requires a larger memory capacity for less energy consumption.


    5.4.2 Study of performance v.s. memory capacity. Figure [2](#page-2-0) shows that
    the increase of capacity is sub-linear with


    performance. To study this observation, we scale our model to the multi-core version
    and share weights of a subgraph across cores. Different cores only buffer a subset
    of weights and transfer the data between cores, similar to BSD in Tangram [\[18\]](#page-13-11)
    or data-rotation in NN-Baton [\[61\]](#page-14-9). The overhead of the interconnection
    crossbar is extracted from the implemented Arteries IP [\[5\]](#page-12-9).


    An accelerator with more cores can cover a larger subgraph but bring more core-to-core
    overhead. As shown in Table [3,](#page-11-1) in most cases, energy increases from
    the single-core to dual-core configuration because of the communication overhead.
    Moreover, profiting from the data-sharing mechanism, the required memory of each
    core drops with the increase of core number.


    5.4.3 Batch size study. For the batch size evaluation shown in Table [3,](#page-11-1)
    the latency with a larger batch size principally presents a sub-linear increase,
    which benefits from the lower bandwidth requirement of weights via the inter-sample
    data reuse. In addition, such data reuse amortizes the energy burden per batch
    processing. And owing to the better weight reuse in multi-batch processing, a
    larger batch size does not require a proportional capacity.


    ## 6 Related Works


    #### 6.1 Intra-layer Optimization


    Prior works focus on the data reuse for intra-layer assignments, like output-stationary
    in ShiDianNao [\[14\]](#page-13-22) and Envision [\[46\]](#page-14-24), weight-stationary
    in NeuFlow [\[15\]](#page-13-23) and Nvdla [\[49\]](#page-14-16), input-stationary
    in SCNN [\[51\]](#page-14-25), and row-stationary in Eyeriss [\[13\]](#page-12-10).
    Based on these primitive dataflow patterns, extensive studies explored the optimal
    tiling and reordering schemes via brute-force, feedback-based, and constraint
    optimization approaches [\[23,](#page-13-5) [30,](#page-13-18) [50\]](#page-14-18).
    These works focus on layer-level optimization, missing the graph information at
    a higher level. The efficiency of tile updates depends on the memory architecture.
    Simba [\[56,](#page-14-20) [74\]](#page-15-5) and NN-Baton [\[61\]](#page-14-9)
    view each tile as an independent workload so that the tile size has a prominent
    impact on memory access due to halo regions. Motivated by traditional vision processors,
    Ascend [\[40\]](#page-13-10) and DRQ [\[58\]](#page-14-26) employ line buffers
    to achieve data reuse in the row direction, but the line buffer cannot well support
    the 2D-tiling reuse in both row and column directions.


    #### 6.2 Inter-layer Optimization


    Intra-layer scheduling is sub-optimal, which is limited by the data reuse within
    a layer. Therefore, Fused-CNN [\[4\]](#page-12-2), SR-CNN [\[38\]](#page-13-8),
    and LCP [\[42\]](#page-14-19) introduce layer fusion method that cache intermediate
    data on-chip to reduce data transfer overhead using handcrafted or heuristic methods
    for fusion partition. Although Irregular-NN [\[73\]](#page-15-3) suggests a customized-DP
    algorithm, the exploration space is constrained because the layers in an assignment
    need to be successive in a specific


    order. A recent work named DNNFuser [\[29\]](#page-13-24) employs an RLbased method,
    but their formulation towards 1D layer-fusion is hard to handle complex irregular
    networks. Tangram [\[18\]](#page-13-11) and Atomic [\[72\]](#page-15-4) schedule
    DNN workloads on a multi-core (scalable) accelerator, but they focus on executing
    a single layer on each core at a time rather than processing multiple layers with
    local data reuse. Also, some previous works [\[2,](#page-12-11) [19,](#page-13-25)
    [62\]](#page-14-27) tackle the workload placement problem for multiple devides
    without discussing the downstream execution on each device.


    Cocco proposes an automatic framework for inter-layer scheduling with a comprehensive
    memory scheme. It focuses on the fundamental core-level temporal execution that
    can be potentially scaled up to the multi-core or multi-device scenario with a
    spatial parallelism mechanism.


    #### 6.3 Design-Space Exploration for Memory


    Memory design exploration methods lie primarily on two sides: analysis-driven
    and search-driven. For the analysisdriven method, Chen et al. [\[12\]](#page-12-0)
    leverage red-blue pebble models to derive the proper memory capacity representations.
    Subsequently, Cai et al. [\[9\]](#page-12-12) propose Olympus, which generalizes
    a framework to a batch of successive layers and also fills up with more scheduling
    and data reuse techniques. However, they are difficult to represent a subgraph
    with complex inter-layer connections. As for the search-driven method, Xiao et
    al. [\[67\]](#page-14-11), Kwon et al. [\[37\]](#page-13-7), and Feng et al. [\[16\]](#page-13-26)
    explore the memory configuration for the layer-level assignment using the brute-force
    search, while Kao et al. [\[32\]](#page-13-6) employ a genetic algorithm to improve
    the efficiency. These works principally focus on the layer-level information,
    while in comparison, Cocco exploits graph-level features for the better optimization.


    ## 7 Conclusion


    While layer-level scheduling is widely studied to improve memory efficiency, graph-level
    optimization remains relatively unexplored. This paper proposed a graph-level
    dataflow with the corresponding memory management scheme that enables flexible
    graph partitions with high memory utilization. On top of it, we propose Cocco,
    a framework to provide a recommended memory configuration with graph-level scheduling
    strategies. Cocco shows outstanding graph partition ability compared to the greedy
    algorithm and DP employed in previous works and enables efficient graph-level
    hardware-mapping co-exploration. This paper helps to provide an implementation
    philosophy for the accelerator memory and better deployment for it.


    ## Acknowledgments


    This research was partially supported by National Key R&D Program of China (2022YFB2804103),
    Tsinghua University Dushi Program, and Tsinghua University Talent Program. We
    would like to appreciate all the anonymous reviewers for their valuable feedback.


    ## References


    - <span id="page-12-5"></span>[1] Dennis Abts, Jonathan Ross, Jonathan Sparling,
    Mark Wong-VanHaren, Max Baker, Tom Hawkins, Andrew Bell, John Thompson, Temesghen
    Kahsai, Garrin Kimmell, Jennifer Hwang, Rebekah Leslie-Hurd, Michael Bye, E. R.
    Creswick, Matthew Boyd, Mahitha Venigalla, Evan Laforge, Jon Purdy, Purushotham
    Kamath, Dinesh Maheshwari, Michael Beidler, Geert Rosseel, Omar Ahmad, Gleb Gagarin,
    Richard Czekalski, Ashay Rane, Sahil Parmar, Jeff Werner, Jim Sproch, Adrian Macias,
    and Brian Kurtz. 2020. Think Fast: A Tensor Streaming Processor (TSP) for Accelerating
    Deep Learning Workloads. In Proceedings of the 47th ACM/IEEE Annual International
    Symposium on Computer Architecture (ISCA). IEEE, Valencia, Spain, 145–158.

    - <span id="page-12-11"></span>[2] Ravichandra Addanki, Shaileshh Bojja Venkatakrishnan,
    Shreyan Gupta, Hongzi Mao, and Mohammad Alizadeh. 2019. Learning Generalizable
    Device Placement Algorithms for Distributed Machine Learning. In Advances in Neural
    Information Processing Systems (NeurIPS), Hanna M. Wallach, Hugo Larochelle, Alina
    Beygelzimer, Florence d''Alché-Buc, Emily B. Fox, and Roman Garnett (Eds.). OpenReview.net,
    Vancouver, BC, Canada, 3983–3993.

    - <span id="page-12-1"></span>[3] Byung Hoon Ahn, Jinwon Lee, Jamie Menjay Lin,
    Hsin-Pai Cheng, Jilei Hou, and Hadi Esmaeilzadeh. 2020. Ordering Chaos: Memory-Aware
    Scheduling of Irregularly Wired Neural Networks for Edge Devices. In Proceedings
    of Machine Learning and Systems (MLSys), Inderjit S. Dhillon, Dimitris S. Papailiopoulos,
    and Vivienne Sze (Eds.). mlsys.org, Austin, TX, USA, 1–14.

    - <span id="page-12-2"></span>[4] Manoj Alwani, Han Chen, Michael Ferdman, and
    Peter A. Milder. 2016. Fused-layer CNN accelerators. In Proceedings of the 49th
    IEEE/ACM International Symposium on Microarchitecture (MICRO). IEEE Computer Society,
    Taipei, Taiwan, 22:1–22:12.

    - <span id="page-12-9"></span>[5] Arteries. 2022. Arteries IP Homepage. <https://www.arteris.com>.

    - <span id="page-12-3"></span>[6] Ljubisa Bajic and Jasmina Vasiljevic. 2020.
    Compute substrate for Software 2.0. In Proceedings of the IEEE Hot Chips 32 Symposium
    (HCS). IEEE, Palo Alto, CA, USA, 1–31.

    - <span id="page-12-6"></span>[7] Pete Bannon, Ganesh Venkataramanan, Debjit Das
    Sarma, and Emil Talpes. 2019. Computer and Redundancy Solution for the Full Self-Driving
    Computer. In Proceedings of the IEEE Hot Chips 31 Symposium (HCS). IEEE, Cupertino,
    CA, USA, 1–22.

    - <span id="page-12-7"></span>[8] John Burgess. 2019. RTX ON - The NVIDIA TURING
    GPU. In Proceedings of the IEEE Hot Chips 31 Symposium (HCS). IEEE, Cupertino,
    CA, USA, 1–27.

    - <span id="page-12-12"></span>[9] Xuyi Cai, Ying Wang, Kaijie Tu, Chengsi Gao,
    and Lei Zhang. 2022. Olympus: Reaching Memory-Optimality on DNN Processors. IEEE
    Transactions on Computers (TC) 71, 8 (2022), 1939–1951.

    - <span id="page-12-8"></span>[10] Prasanth Chatarasi, Hyoukjun Kwon, Angshuman
    Parashar, Michael Pellauer, Tushar Krishna, and Vivek Sarkar. 2022. Marvel: A
    Data-Centric Approach for Mapping Deep Learning Operators on Spatial Accelerators.
    ACM Transactions on Architecture and Code Optimization 19, 1 (2022), 6:1–6:26.

    - <span id="page-12-4"></span>[11] Karam Chatha. 2021. Qualcomm® Cloud Al-100:
    12TOPS/W Scalable, High Performance and Low Latency Deep Learning Inference Accelerator.
    In Proceedings of the IEEE Hot Chips 33 Symposium (HCS). IEEE, Palo Alto, CA,
    USA, 1–19.

    - <span id="page-12-0"></span>[12] Xiaoming Chen, Yinhe Han, and Yu Wang. 2020.
    Communication Lower Bound in Convolution Accelerators. In Proceedings of the IEEE
    International Symposium on High Performance Computer Architecture (HPCA). IEEE,
    San Diego, CA, USA, 529–541.

    - <span id="page-12-10"></span>[13] Yu-Hsin Chen, Joel S. Emer, and Vivienne Sze.
    2016. Eyeriss: A Spatial Architecture for Energy-Efficient Dataflow for Convolutional
    Neural Networks. In Proceedings of the ACM/IEEE Annual International Symposium
    on Computer Architecture (ISCA). IEEE Computer Society, Seoul,


    South Korea, 367–379.


    - <span id="page-13-22"></span>[14] Zidong Du, Robert Fasthuber, Tianshi Chen,
    Paolo Ienne, Ling Li, Tao Luo, Xiaobing Feng, Yunji Chen, and Olivier Temam. 2015.
    ShiDianNao: shifting vision processing closer to the sensor. In Proceedings of
    the ACM/IEEE Annual International Symposium on Computer Architecture (ISCA). ACM,
    Portland, OR, USA, 92–104.

    - <span id="page-13-23"></span>[15] Clément Farabet, Berin Martini, B. Corda,
    Polina Akselrod, Eugenio Culurciello, and Yann LeCun. 2011. NeuFlow: A runtime
    reconfigurable dataflow processor for vision. In Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition (CVPR) Workshops. IEEE Computer Society,
    Colorado Springs, CO, USA, 109–116.

    - <span id="page-13-26"></span>[16] Kaijie Feng, Xiaoya Fan, Jianfeng An, Xiping
    Wang, Kaiyue Di, Jiangfei Li, Minghao Lu, and Chuxi Li. 2021. ERDSE: efficient
    reinforcement learning based design space exploration method for CNN accelerator
    on resource limited platform. Graphics and Visual Computing 4 (2021), 1–11.

    - <span id="page-13-0"></span>[17] Ken-ichi Funahashi. 1989. On the approximate
    realization of continuous mappings by neural networks. Neural Networks 2, 3 (1989),
    183–192.

    - <span id="page-13-11"></span>[18] Mingyu Gao, Xuan Yang, Jing Pu, Mark Horowitz,
    and Christos Kozyrakis. 2019. TANGRAM: Optimized Coarse-Grained Dataflow for Scalable
    NN Accelerators. In Proceedings of the International Conference on Architectural
    Support for Programming Languages and Operating Systems (ASPLOS). ACM, Providence,
    RI, USA, 807–820.

    - <span id="page-13-25"></span>[19] Yuanxiang Gao, Li Chen, and Baochun Li. 2018.
    Spotlight: Optimizing Device Placement for Training Deep Neural Networks. In Proceedings
    of the 35th International Conference on Machine Learning (ICML) (Proceedings of
    Machine Learning Research, Vol. 80), Jennifer G. Dy and Andreas Krause (Eds.).
    PMLR, Stockholm, Sweden, 1662–1670.

    - <span id="page-13-4"></span>[20] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and
    Sun Jian. 2016. Deep Residual Learning for Image Recognition. In Proceedings of
    the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE Computer
    Society, Las Vegas, NV, USA, 770–778.

    - <span id="page-13-17"></span>[21] Kartik Hegde, Po-An Tsai, Sitao Huang, Vikas
    Chandra, Angshuman Parashar, and Christopher W. Fletcher. 2021. Mind mappings:
    enabling efficient algorithm-accelerator mapping space search. In Proceedings
    of the 26th ACM International Conference on Architectural Support for Programming
    Languages and Operating Systems (ASPLOS), Tim Sherwood, Emery D. Berger, and Christos
    Kozyrakis (Eds.). ACM, Virtual Event, USA, 943–958.

    - <span id="page-13-1"></span>[22] Kurt Hornik, Maxwell B. Stinchcombe, and Halbert
    White. 1989. Multilayer feedforward networks are universal approximators. Neural
    Networks 2, 5 (1989), 359–366.

    - <span id="page-13-5"></span>[23] Qijing Huang, Aravind Kalaiah, Minwoo Kang,
    James Demmel, Grace Dinh, John Wawrzynek, Thomas Norell, and Yakun Sophia Shao.
    2021. CoSA: Scheduling by Constrained Optimization for Spatial Accelerators. In
    Proceedings of the ACM/IEEE Annual International Symposium on Computer Architecture
    (ISCA). IEEE, Valencia, Spain, 554–566.

    - <span id="page-13-9"></span>[24] Drago Ignjatovic, Daniel W. Bailey, and Ljubisa
    Bajic. 2022. The Wormhole AI Training Processor. In Proceedings of the IEEE International
    Solid-State Circuits Conference (ISSCC). IEEE, San Francisco, CA, USA, 356–358.

    - <span id="page-13-19"></span>[25] Abhinav Jangda and Uday Bondhugula. 2018.
    An effective fusion and tile size model for optimizing image processing pipelines.
    In Proceedings of the 23rd ACM SIGPLAN Symposium on Principles and Practice of
    Parallel Programming (PPoPP), Andreas Krall and Thomas R. Gross (Eds.). ACM, Vienna,
    Austria, 261–275.

    - <span id="page-13-15"></span>[26] Yang Jiao, Liang Han, Rong Jin, Yi-Jung Su,
    Chiente Ho, Li Yin, Yun Li, Long Chen, Zhen Chen, Lu Liu, Zhuyu He, Yu Yan, Jun
    He, Jun Mao, Xiaotao Zai, Xuejun Wu, Yongquan Zhou, Mingqiu Gu, Guocai Zhu, Rong
    Zhong, Wenyuan Lee, Ping Chen, Yiping Chen, Weiliang Li, Deyu Xiao, Qing Yan,
    Mingyuan Zhuang, Jiejun Chen, Yun Tian, Yingzi Lin, Wei Wu, Hao Li, and Zesheng
    Dou. 2020. A 12nm Programmable Convolution-Efficient Neural-Processing-Unit Chip


    Achieving 825TOPS. In Proceedings of the IEEE International Solid-State Circuits
    Conference (ISSCC). IEEE, San Francisco, CA, USA, 136–140.


    - [27] Yang Jiao, Liang Han, and Xin Long. 2020. Hanguang 800 NPU The Ultimate
    AI Inference Solution for Data Centers. In Proceedings of the IEEE Hot Chips 32
    Symposium (HCS). IEEE, Palo Alto, CA, USA, 1–29.

    - <span id="page-13-16"></span>[28] Norman P. Jouppi, Doe Hyun Yoon, Matthew Ashcraft,
    Mark Gottscho, Thomas B. Jablin, George Kurian, James Laudon, Sheng Li, Peter
    C. Ma, Xiaoyu Ma, Thomas Norrie, Nishant Patil, Sushma Prasad, Cliff Young, Zongwei
    Zhou, and David A. Patterson. 2021. Ten Lessons From Three Generations Shaped
    Google''s TPUv4i : Industrial Product. In Proceedings of the 48th ACM/IEEE Annual
    International Symposium on Computer Architecture (ISCA). IEEE, Valencia, Spain,
    1–14.

    - <span id="page-13-24"></span>[29] Sheng-Chun Kao, Xiaoyu Huang, and Tushar Krishna.
    2022. DNNFuser: Generative Pre-Trained Transformer as a Generalized Mapper for
    Layer Fusion in DNN Accelerators. arXiv preprint arXiv:2201.11218 abs/2201.11218
    (2022), 1–8.

    - <span id="page-13-18"></span>[30] Sheng-Chun Kao and Tushar Krishna. 2020. GAMMA:
    Automating the HW Mapping of DNN Models on Accelerators via Genetic Algorithm.
    In Proceedings of the IEEE/ACM International Conference On Computer Aided Design
    (ICCAD). IEEE, San Diego, CA, USA, 44:1–44:9.

    - <span id="page-13-21"></span>[31] Sheng-Chun Kao and Tushar Krishna. 2022. MAGMA:
    An Optimization Framework for Mapping Multiple DNNs on Multiple Accelerator Cores.
    In IEEE International Symposium on High-Performance Computer Architecture, (HPCA).
    IEEE, Seoul, South Korea, 814–830.

    - <span id="page-13-6"></span>[32] Sheng-Chun Kao, Michael Pellauer, Angshuman
    Parashar, and Tushar Krishna. 2022. DiGamma: Domain-aware Genetic Algorithm for
    HW-Mapping Co-optimization for DNN Accelerators. In Proceedings of the Design,
    Automation & Test in Europe Conference & Exhibition (DATE), Cristiana Bolchini,
    Ingrid Verbauwhede, and Ioana Vatajelu (Eds.). IEEE, Antwerp, Belgium, 232–237.

    - <span id="page-13-20"></span>[33] Scott Kirkpatrick, D. Gelatt Jr., and Mario
    P. Vecchi. 1983. Optimization by Simmulated Annealing. Sci. 220, 4598 (1983),
    671–680.

    - <span id="page-13-12"></span>[34] Simon Knowles. 2017. Scalable Silicon Compute.
    In Workshop on Deep Learning At Supercomputer Scale, NIPS. OpenReview.net, Long
    Beach, CA, USA, 1–22.

    - <span id="page-13-13"></span>[35] Simon Knowles. 2021. Graphcore. In Proceedings
    of the IEEE Hot Chips 33 Symposium (HCS). IEEE, Palo Alto, CA, USA, 1–25.

    - <span id="page-13-3"></span>[36] Alex Krizhevsky, Ilya Sutskever, and Geoffrey
    E. Hinton. 2012. ImageNet Classification with Deep Convolutional Neural Networks.
    In Proceedings of the 26th Annual Conference on Neural Information Processing
    Systems (NIPS). Curran Associates, Inc., Lake Tahoe, Nevada, United States, 1106–1114.

    - <span id="page-13-7"></span>[37] Hyoukjun Kwon, Prasanth Chatarasi, Michael
    Pellauer, Angshuman Parashar, Vivek Sarkar, and Tushar Krishna. 2019. Understanding
    Reuse, Performance, and Hardware Cost of DNN Dataflow: A Data-Centric Approach.
    In Proceedings of the IEEE/ACM International Symposium on Microarchitecture (MICRO).
    ACM, Columbus, OH, USA, 754–768.

    - <span id="page-13-8"></span>[38] Juhyoung Lee, Dongjoo Shin, Jinsu Lee, Jinmook
    Lee, Sanghoon Kang, and Hoi-Jun Yoo. 2019. A Full HD 60 fps CNN Super Resolution
    Processor with Selective Caching based Layer Fusion for Mobile Devices. In Proceedings
    of the Symposium on VLSI Circuits. IEEE, Kyoto, Japan, 302–303.

    - <span id="page-13-2"></span>[39] Grzegorz Lewicki and Giuseppe Marino. 2004.
    Approximation of functions of finite variation by superpositions of a Sigmoidal
    function. Appl. Math. Lett. 17, 10 (2004), 1147–1152.

    - <span id="page-13-10"></span>[40] Heng Liao, Jiajin Tu, Jing Xia, Hu Liu, Xiping
    Zhou, Honghui Yuan, and Yuxing Hu. 2021. Ascend: a Scalable and Unified Architecture
    for Ubiquitous Deep Neural Network Computing : Industry Track Paper. In Proceedings
    of the IEEE International Symposium on High-Performance Computer Architecture,
    HPCA. IEEE, Seoul, South Korea, 789–801.

    - <span id="page-13-14"></span>[41] Heng Liao, Jiajin Tu, Jing Xia, and Xiping
    Zhou. 2019. DaVinci: A Scalable Architecture for Neural Network Computing. In
    Proceedings


    Cocco: Hardware-Mapping Co-Exploration towards Memory ... ASPLOS ''24, April 27-May
    1, 2024, La Jolla, CA, USA


    of the IEEE Hot Chips 31 Symposium (HCS). IEEE, Cupertino, CA, USA, 1–44.


    - <span id="page-14-19"></span>[42] Xinhan Lin, Shouyi Yin, Fengbin Tu, Leibo
    Liu, Xiangyu Li, and Shaojun Wei. 2018. LCP: a layer clusters paralleling mapping
    method for accelerating inception and residual networks on FPGA. In Proceedings
    of the 55th Annual Design Automation Conference (DAC). ACM, San Francisco, CA,
    USA, 16:1–16:6.

    - <span id="page-14-7"></span>[43] Wenyan Lu, Guihai Yan, Jiajun Li, Shijun Gong,
    Yinhe Han, and Xiaowei Li. 2017. FlexFlow: A Flexible Dataflow Accelerator Architecture
    for Convolutional Neural Networks. In Proceedings of the IEEE International Symposium
    on High Performance Computer Architecture (HPCA). IEEE Computer Society, Austin,
    TX, USA, 553–564.

    - <span id="page-14-8"></span>[44] Yufei Ma, Yu Cao, Sarma B. K. Vrudhula, and
    Jae-sun Seo. 2017. Optimizing Loop Operation and Dataflow in FPGA Acceleration
    of Deep Convolutional Neural Networks. In Proceedings of the ACM/SIGDA International
    Symposium on Field-Programmable Gate Arrays (FPGA). ACM, Monterey, CA, USA, 45–54.

    - <span id="page-14-0"></span>[45] Marvin Minsky and Seymour Papert. 1987. Perceptrons
    - an introduction to computational geometry. MIT Press, .

    - <span id="page-14-24"></span>[46] Bert Moons, Roel Uytterhoeven, Wim Dehaene,
    and Marian Verhelst. 2017. Envision: A 0.26-to-10TOPS/W subword-parallel dynamicvoltage-accuracy-frequency-scalable
    Convolutional Neural Network processor in 28nm FDSOI. In Proceedings of the IEEE
    International Solid-State Circuits Conference (ISSCC). IEEE, San Francisco, CA,
    USA, 246–247.

    - <span id="page-14-21"></span>[47] Ravi Teja Mullapudi, Andrew Adams, Dillon
    Sharlet, Jonathan Ragan-Kelley, and Kayvon Fatahalian. 2016. Automatically scheduling
    halide image processing pipelines. ACM Trans. Graph. 35, 4 (2016), 83:1– 83:11.

    - <span id="page-14-12"></span>[48] Thomas Norrie, Nishant Patil, Doe Hyun Yoon,
    George Kurian, Sheng Li, James Laudon, Cliff Young, Norman P. Jouppi, and David
    A. Patterson. 2020. Google''s Training Chips Revealed: TPUv2 and TPUv3. In Proceedings
    of the IEEE Hot Chips 32 Symposium (HCS). IEEE, Palo Alto, CA, USA, 1–70.

    - <span id="page-14-16"></span>[49] NVIDIA. 2018. THE NVIDIA DEEP LEARNING ACCELERATOR.
    In Proceedings of the IEEE Hot Chips 30 Symposium (HCS). IEEE, Cupertino, CA,
    USA, 1–18.

    - <span id="page-14-18"></span>[50] Angshuman Parashar, Priyanka Raina, Yakun
    Sophia Shao, Yu-Hsin Chen, Victor A. Ying, Anurag Mukkara, Rangharajan Venkatesan,
    Brucek Khailany, Stephen W. Keckler, and Joel S. Emer. 2019. Timeloop: A Systematic
    Approach to DNN Accelerator Evaluation. In Proceedings of the IEEE International
    Symposium on Performance Analysis of Systems and Software (ISPASS). IEEE, Madison,
    WI, USA, 304–315.

    - <span id="page-14-25"></span>[51] Angshuman Parashar, Minsoo Rhu, Anurag Mukkara,
    Antonio Puglielli, Rangharajan Venkatesan, Brucek Khailany, Joel S. Emer, Stephen
    W. Keckler, and William J. Dally. 2017. SCNN: An Accelerator for Compressed-sparse
    Convolutional Neural Networks. In Proceedings of the 44th Annual International
    Symposium on Computer Architecture (ISCA). ACM, Toronto, ON, Canada, 27–40.

    - <span id="page-14-23"></span>[52] Alec Radford and Karthik Narasimhan. 2018.
    Improving Language Understanding by Generative Pre-Training. In Preprint. OpenAI,
    , 1– 12.

    - <span id="page-14-5"></span>[53] Esteban Real, Alok Aggarwal, Yanping Huang,
    and Quoc V. Le. 2019. Regularized Evolution for Image Classifier Architecture
    Search. In Proceedings of the 33rd Conference on Artificial Intelligence (AAAI).
    AAAI Press, Honolulu, Hawaii, USA, 4780–4789.

    - <span id="page-14-1"></span>[54] Frank Rosenblatt. 1957. The perceptron, a perceiving
    and recognizing automaton Project Para. Cornell Aeronautical Laboratory, .

    - <span id="page-14-4"></span>[55] Mark Sandler, Andrew G. Howard, Menglong Zhu,
    Andrey Zhmoginov, and Liang-Chieh Chen. 2018. MobileNetV2: Inverted Residuals
    and Linear Bottlenecks. In Proceedings of the IEEE Conference on Computer Vision
    and Pattern Recognition (CVPR). Computer Vision Foundation / IEEE Computer Society,
    Salt Lake City, UT, USA, 4510–4520.

    - <span id="page-14-20"></span>[56] Yakun Sophia Shao, Jason Clemons, Rangharajan
    Venkatesan, Brian Zimmer, Matthew Fojtik, Nan Jiang, Ben Keller, Alicia Klinefelter,
    Nathaniel Pinckney, Priyanka Raina, Stephen G. Tell, Yanqing Zhang, William J.
    Dally, Joel Emer, C. Thomas Gray, Brucek Khailany, and Stephen W. Keckler. 2019.
    Simba: Scaling Deep-Learning Inference with Multi-Chip-Module-Based Architecture.
    In Proceedings of the IEEE/ACM International Symposium on Microarchitecture (MICRO).
    ACM, Columbus, OH, USA, 14–27.

    - <span id="page-14-2"></span>[57] Karen Simonyan and Andrew Zisserman. 2015.
    Very Deep Convolutional Networks for Large-Scale Image Recognition. In Proceedings
    of the International Conference on Learning Representations (ICLR). Computational
    and Biological Learning Society, San Diego, CA, USA, 1–14.

    - <span id="page-14-26"></span>[58] Zhuoran Song, Bangqi Fu, Feiyang Wu, Zhaoming
    Jiang, Li Jiang, Naifeng Jing, and Xiaoyao Liang. 2020. DRQ: Dynamic Region-based
    Quantization for Deep Neural Network Acceleration. In Proceedings of the 47th
    ACM/IEEE Annual International Symposium on Computer Architecture (ISCA). IEEE,
    Valencia, Spain, 1010–1021.

    - <span id="page-14-3"></span>[59] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre
    Sermanet, Scott E. Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke,
    and Andrew Rabinovich. 2015. Going deeper with convolutions. In Proceedings of
    the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE Computer
    Society, Boston, MA, USA, 1–9.

    - <span id="page-14-13"></span>[60] Emil Talpes, Douglas Williams, and Debjit
    Das Sarma. 2022. DOJO: The Microarchitecture of Tesla''s Exa-Scale Computer. In
    Proceedings of the IEEE Hot Chips 34 Symposium (HCS). IEEE, Cupertino, CA, USA,
    1–28.

    - <span id="page-14-9"></span>[61] Zhanhong Tan, Hongyu Cai, Runpei Dong, and
    Kaisheng Ma. 2021. NN-Baton: DNN Workload Orchestration and Chiplet Granularity
    Exploration for Multichip Accelerators. In Proceedings of the IEEE Annual International
    Symposium on Computer Architecture (ISCA). IEEE, Valencia, Spain, 1013–1026.

    - <span id="page-14-27"></span>[62] Jakub Tarnawski, Amar Phanishayee, Nikhil
    R. Devanur, Divya Mahajan, and Fanny Nina Paravecino. 2020. Efficient Algorithms
    for Device Placement of DNN Graph Operators. In Advances in Neural Information
    Processing Systems (NeurIPS), Hugo Larochelle, Marc''Aurelio Ranzato, Raia Hadsell,
    Maria-Florina Balcan, and Hsuan-Tien Lin (Eds.). Open-Review.net, Virtual, 1–13.

    - <span id="page-14-14"></span>[63] Tenstorrent. 2021. Grayskull. <https://tenstorrent.com/grayskull/>.

    - <span id="page-14-22"></span>[64] Ashish Vaswani, Noam Shazeer, Niki Parmar,
    Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin.
    2017. Attention is All you Need. In Advances in Neural Information Processing
    Systems (NIPS), Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach,
    Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (Eds.). OpenReview.net, Long
    Beach, CA, USA, 5998–6008.

    - <span id="page-14-17"></span>[65] Ofri Wechsler, Michael Behar, and Bharat Daga.
    2019. Spring Hill (NNP-I 1000) Intel''s Data Center Inference Chip. In Proceedings
    of the IEEE Hot Chips 31 Symposium (HCS). IEEE, Cupertino, CA, USA, 1–12.

    - <span id="page-14-10"></span>[66] Jian Weng, Sihao Liu, Vidushi Dadu, Zhengrong
    Wang, Preyas Shah, and Tony Nowatzki. 2020. DSAGEN: Synthesizing Programmable
    Spatial Accelerators. In Proceedings of the 47th ACM/IEEE Annual International
    Symposium on Computer Architecture (ISCA). IEEE, Valencia, Spain, 268–281.

    - <span id="page-14-11"></span>[67] Qingcheng Xiao, Size Zheng, Bingzhe Wu, Pengcheng
    Xu, Xuehai Qian, and Yun Liang. 2021. HASCO: Towards Agile HArdware and Software
    CO-design for Tensor Computation. In Proceedings of the 48th ACM/IEEE Annual International
    Symposium on Computer Architecture (ISCA). IEEE, Valencia, Spain, 1055–1068.

    - <span id="page-14-6"></span>[68] Saining Xie, Alexander Kirillov, Ross B. Girshick,
    and Kaiming He. 2019. Exploring Randomly Wired Neural Networks for Image Recognition.
    In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV).
    IEEE, Seoul, South Korea, 1284–1293.

    - <span id="page-14-15"></span>[69] Andrew Yang. 2019. Deep Learning Training
    At Scale Spring Crest Deep Learning Accelerator (Intel® Nervana™ NNP-T). In Proceedings
    of the IEEE Hot Chips 31 Symposium (HCS). IEEE, Cupertino, CA, USA,


    <span id="page-15-0"></span>1–20.


    - <span id="page-15-2"></span>[70] Xuan Yang, Mingyu Gao, Qiaoyi Liu, Jeff Setter,
    Jing Pu, Ankita Nayak, Steven Bell, Kaidi Cao, Heonjae Ha, Priyanka Raina, Christos
    Kozyrakis, and Mark Horowitz. 2020. Interstellar: Using Halide''s Scheduling Language
    to Analyze DNN Accelerators. In Proceedings of the International Conference on
    Architectural Support for Programming Languages and Operating Systems (ASPLOS).
    ACM, Lausanne, Switzerland, 369–383.

    - <span id="page-15-6"></span>[71] Size Zheng, Renze Chen, Anjiang Wei, Yicheng
    Jin, Qin Han, Liqiang Lu, Bingyang Wu, Xiuhong Li, Shengen Yan, and Yun Liang.
    2022. AMOS: enabling automatic mapping for tensor computations on spatial accelerators
    with hardware abstraction. In Proceedings of the 49th Annual International Symposium
    on Computer Architecture (ISCA). ACM, New York, New York, USA, 874–887.

    - <span id="page-15-4"></span>[72] Shixuan Zheng, Xianjue Zhang, Leibo Liu, Shaojun
    Wei, and Shouyi Yin. 2022. Atomic Dataflow based Graph-Level Workload Orchestration
    for Scalable DNN Accelerators. In Proceedings of the IEEE International Symposium
    on High-Performance Computer Architecture (HPCA). IEEE, Seoul, South Korea, 475–489.

    - <span id="page-15-3"></span>[73] Shixuan Zheng, Xianjue Zhang, Daoli Ou, Shibin
    Tang, Leibo Liu, Shaojun Wei, and Shouyi Yin. 2020. Efficient Scheduling of Irregular
    Network Structures on CNN Accelerators. IEEE Transactions on Computer-Aided Design
    of Integrated Circuits and Systems (TCAD) 39, 11 (2020), 3408–3419.

    - <span id="page-15-5"></span>[74] Brian Zimmer, Rangharajan Venkatesan, Yakun
    Sophia Shao, Jason Clemons, Matthew Fojtik, Nan Jiang, Ben Keller, Alicia Klinefelter,
    Nathaniel Ross Pinckney, Priyanka Raina, Stephen G. Tell, Yanqing Zhang, William
    J. Dally, Joel S. Emer, C. Thomas Gray, Stephen W. Keckler, and Brucek Khailany.
    2019. A 0.11 pJ/Op, 0.32-128 TOPS, Scalable Multi-Chip-Module-based Deep Neural
    Network Accelerator with Ground-Reference Signaling in 16nm. In Proceedings of
    the IEEE Symposium on VLSI Circuits (VLSI). IEEE, Kyoto, Japan, 300.

    - <span id="page-15-1"></span>[75] Barret Zoph, Vijay Vasudevan, Jonathon Shlens,
    and Quoc V. Le. 2018. Learning Transferable Architectures for Scalable Image Recognition.
    In IEEE Conference on Computer Vision and Pattern Recognition, (CVPR). Computer
    Vision Foundation / IEEE Computer Society, Salt Lake City, UT, USA, 8697–8710.'
- title: Towards Next-Generation Intelligent Assistants Leveraging LLM Techniques
  abstract: ''
  keywords: conversational AI, large language models, multi-modal conversation, knowledge
    grounding, personalization, federated learning
  document: '#### 1.1 Introduction


    We will start with an introduction discussing the basics of virtual assistant,
    and the new challenges we face in building an AR/VR assistant. We will discuss
    what is an ideal assistant; that is, an agent that knows the user and the world,
    can receive requests from the user in a reactive fashion, or predict the users
    needs in a proactive fashion, then provide the user the right services at the
    right time, with the user''s permission.


    #### 1.2 Conversational AI Basics


    Before diving deep into cutting-edge techniques addressing the novel challenges
    for AR/VR assistants, we first provide a high-level overview of the conventional
    language-oriented assistant system. We introduce the overall design of both open-domain
    end-to-end conversational AI and modularized task-oriented dialog systems, but
    put more focus on the latter as they are the backbone of virtual assistants in
    industry. We dive deeper into key components: Automatic Speech Recognition (ASR),
    Natural Language Understanding (NLU), Dialog State Tracking, Dialog Policy Learning,
    Natural Language Generation (NLG), and Text-to-Speech (TTS). Due to time constraints,
    the tutorial will review basic construction with focus on modern modeling designs
    of each component, and leave implementation details as references. Links to public
    datasets will also be provided for training and evaluating dialogue-based assistants.


    Permission to make digital or hard copies of part or all of this work for personal
    or classroom use is granted without fee provided that copies are not made or distributed
    for profit or commercial advantage and that copies bear this notice and the full
    citation on the first page. Copyrights for third-party components of this work
    must be honored. For all other uses, contact the owner/author(s).


    <span id="page-1-0"></span>


    #### 1.3 Multi-modal Context-Aware Conversations


    We envision that the next generation of virtual assistants will be able to process
    multimodal inputs and provide multimodal outputs beyond the traditional NLP stack.
    Specifically, the AR/VR settings pose a situated multimodal context, where the
    user and the assistant are continually co-observing the same context, dynamically
    updated over time. The assistant for AR/VR thus requires (1) understanding of
    the multimodal context from diverse sources (e.g. vision, gestures, sensor signals),
    and (2) joint grounding of the situated context with the conversational context.


    We review the related literature across multiple domains and tasks, including
    the visual question and answering systems the visual navigation tasks and more
    general task-oriented multimodal agents. We will then dive deeper into the state-of-the-art
    modeling techniques that address the key challenges, such as multimodal co-reference
    resolution, multimodal dialog state tracking, and contextual understanding of
    user states.


    ## 1.4 Knowledge-Enhanced and Personalized Conversations


    To better assist a user in a personalized and contextualized manner, it is important
    for an assistant system to be able to (1) manage personal knowledge through memory
    grounded dialogue system, and (2) incorporate the world and personal knowledge
    as part of the grounding context for conversations. Incorporating personal memories
    as part of conversational interactions will be particularly important for AR devices
    that reside more closely to users'' everyday life, incurring more frequent usage.
    We will go through in detail the relevant work in the recent literature on inferring
    new knowledge from unstructured utterances, utilizing memory graphs, knowledge
    graphs and online-searches for conversational recommendations, question and answering,
    media retrieval, and knowledge grounded open-ended conversations.


    ### 1.5 On-device & Federated Learning for Privacy Preserving Assistant


    AR/VR devices have strict privacy constraints because they can access sensitive
    data from cameras, microphones and other sensors. This necessitates running model
    inference and training on-device, under challenging compute, memory and power
    constraints. We first review the state-of-the-art on-device modeling methods used
    in practice to downsize neural models (e.g. quantization, knowledge distillation,
    automated architecture search, accommodating for the limited memory and compute
    resources of wearable devices.


    We then discuss the unique challenge of training these models while preserving
    user privacy. We start with an overview of production federated learning systems,
    and then discuss the problems that arise when training data is spread across edge
    devices, like the impact of data and device heterogeneity, model personalization,
    and differential privacy.


    #### 1.6 Conclusions & Future Directions


    We conclude our tutorial by stating a number of open problems we need to solve
    to move towards the goal of building next-generation assistants for AR/VR devices.
    The open problems include 1) ondevice machine learning to provide assistant services
    with sporadic connections, 2) seamless integration of search, question answering,
    recommendation for information-driven needs, 3) proactive service suggestion at
    the right time, 4) leveraging public and personal knowledge graphs to improve
    context-aware services, 5) scalable graph mining from knowledge graphs, social
    graphs, and behavior graphs for better assistance, and many others.


    #### 2 PREVIOUS EDITIONS


    Below is a list of the tutorials on similar topics.


    - A related tutorial, titled ["Deeper Conversational AI",](https://neurips.cc/media/Slides/nips/2020/virtual(07-08-00)-07-08-00UTC-16657-track2_deeper.pdf)
    was given at NeurIPS 2020. The authors first reviewed general conversational AI
    architectures of the key components. Then they deep dived into generational seq2seq
    deep conversational AI techniques, pointed out limitations and solutions of vanilla
    models such as lack of diversity, consistency, knowledge etc. Finally, the authors
    touched upon various challenges and research directions, including reinforcement
    learning, few/zero shot learning, lifelong learning.

    - Another related tutorial titled ["Achieving Common Ground](https://acl2020.org/program/tutorials/#t5-achieving-common-ground-in-multi-modal-dialogue-cutting-edge-)
    [in Multi-modal Dialogue"](https://acl2020.org/program/tutorials/#t5-achieving-common-ground-in-multi-modal-dialogue-cutting-edge-)
    was given at ACL 2020. It reviewed theories and practices of incrementally achieving
    common ground between a robotic agent and a human participant during an open-domain
    dialog, with an emphasis on leveraging verbal and non-verbal behavior signals
    to orchestrate an effective engagements. The authors considered roles of a wide
    range of modalities including gazing, pointing, nodding, facial expressions and
    other non-verbal cues.


    Our tutorial is more focused on the context of building AR/VR assistants. On the
    one hand, we go beyond multi-modal conversations to discuss contextual AI and
    personalized assistant services. On the other hand, we discuss the practical challenges
    we face and the industrial solutions in building real assistant systems.


    Finally, a first edition of this tutorial is accepted by WebConf 2023. Our new
    version of the tutorial would add how the recent progress on LLM (Large Language
    Model) can further improve intelligent assistant.


    #### 3 AUDIENCE PARTICIPATION


    To help the audience understand challenges brought by multimodality, contextualization,
    and personalization, we will use one real-world scenario with progressive add-ons
    throughout the tutorial to illustrate the challenges and opportunities, and demonstrate
    the technical solutions. In addition, we will set aside time for Q & A in each
    subsection and encourage the audience to ask questions to ensure they understand
    the material and stays engaged. All slides will be made available publicly before
    the start of the tutorial.


    #### 4 POTENTIAL SOCIETAL IMPACTS


    In recent years, the use of virtual assistants has become increasingly popular,
    impacting various aspects of people''s lives. Our tutorial will encourage discussions
    on developing virtual assistants with integrity and responsibility in mind. Such
    considerations include but not limited to ensuring transparency and privacy, improving
    accessibility, avoiding bias, and promoting inclusivity.'
- title: Accelerating Boolean Constraint Propagation for Efficient SAT-Solving on
    FPGAs
  abstract: 'We present a hardware-accelerated SAT solver targeting processor/Field
    Programmable Gate Arrays (FPGA) SoCs. Our solution accelerates the most expensive
    subroutine of the Davis-Putnam-Logemann-Loveland (DPLL) algorithm, Boolean Constraint
    Propagation (BCP) through fine-grained FPGA parallelism. Unlike prior state-of-the-art
    solutions, our solver eliminates costly clause lookup operations by assigning
    clauses directly to clause processors on the FPGA and dividing large formulas
    into smaller partitions manageable by FPGA. Partitions are hot-swapped during
    runtime as required and the supported formula size is limited only by available
    external memory, not on-chip FPGA memory.

    We evaluate our solver on a Xilinx Zynq platform with results showing quicker
    execution time across various formula sizes, subject to formula partitioning strategy.
    Compared to prior state-of-theart, we achieve 1.7x and 1.1x speed up on BCP for
    2 representative benchmarks and up to 6x total speedup over software-only implementation.'
  keywords: FPGA, SAT, acceleration, embedded, boolean, satisfiability
  document: '# Accelerating Boolean Constraint Propagation for Efficient SAT-Solving
    on FPGAs


    Hari Govindasamy Carleton University Ottawa, Canada hari@sce.carleton.ca


    Babak Esfandiari Carleton University Ottawa, Canada babak@sce.carleton.ca


    Paulo Garcia Chulalongkorn University Bangkok, Thailand paulo.g@chula.ac.th


    #### ABSTRACT


    We present a hardware-accelerated SAT solver targeting processor/Field Programmable
    Gate Arrays (FPGA) SoCs. Our solution accelerates the most expensive subroutine
    of the Davis-Putnam-Logemann-Loveland (DPLL) algorithm, Boolean Constraint Propagation
    (BCP) through fine-grained FPGA parallelism. Unlike prior state-of-the-art solutions,
    our solver eliminates costly clause lookup operations by assigning clauses directly
    to clause processors on the FPGA and dividing large formulas into smaller partitions
    manageable by FPGA. Partitions are hot-swapped during runtime as required and
    the supported formula size is limited only by available external memory, not on-chip
    FPGA memory.


    We evaluate our solver on a Xilinx Zynq platform with results showing quicker
    execution time across various formula sizes, subject to formula partitioning strategy.
    Compared to prior state-of-theart, we achieve 1.7x and 1.1x speed up on BCP for
    2 representative benchmarks and up to 6x total speedup over software-only implementation.


    ## CCS CONCEPTS


    • Computer systems organization → Robotic autonomy; • Networks → Cyber-physical
    networks; • Applied computing → Industry and manufacturing; • Hardware → Hardware
    accelerators; Application specific processors; • Theory of computation → Equational
    logic and rewriting.


    ### KEYWORDS


    FPGA, SAT, acceleration, embedded, boolean, satisfiability


    #### ACM Reference Format:


    Hari Govindasamy, Babak Esfandiari, and Paulo Garcia. 2024. Accelerating Boolean
    Constraint Propagation for Efficient SAT-Solving on FPGAs. In Great Lakes Symposium
    on VLSI 2024 (GLSVLSI ''24), June 12–14, 2024, Clearwater, FL, USA. ACM, New York,
    NY, USA, [5](#page-4-0) pages. [https://doi.org/10.1145/](https://doi.org/10.1145/3649476.3658808)
    [3649476.3658808](https://doi.org/10.1145/3649476.3658808)


    #### 1 INTRODUCTION


    The Boolean Satisfiability problem (SAT) is a fundamental problem in computer
    science, the first NP-Complete problem [\[7\]](#page-4-1). SAT solvers have become
    the backbone of several engineering domains, as any


    GLSVLSI ''24, June 12–14, 2024, Clearwater, FL, USA


    © 2024 Copyright held by the owner/author(s).


    ACM ISBN 979-8-4007-0605-9/24/06.


    <https://doi.org/10.1145/3649476.3658808>


    NP-Complete problem can be encoded as instance of SAT [\[2,](#page-4-2) [7\]](#page-4-1).
    SAT solvers determine whether a given boolean formula is satisfiable by identifying
    an assignment to the formulas'' free variables that evaluate the formula to true.
    The formula is unsatisfiable otherwise. Most SAT solvers target CNF-SAT, a subset
    of SAT that determines the satisfiability of formulas encoded in Conjunctive Normal
    Form (CNF) . Formulas in CNF are conjunctions of clauses, where each clause is
    a disjunction of one or more literals (a variable or its negation).


    With the advent of modern Systems-on-Chip (SoC) comprised of both hard embedded
    processors and configurable FPGA fabric offering myriad implementation opportunities
    [\[17\]](#page-4-3), deployed from the embedded to the high performance computing
    domain [\[1\]](#page-4-4), accelerating SAT-solving through hardware is an attractive
    approach. We present a novel architecture for hardware-accelerated SAT-solving
    that outperforms state of the art solutions, released in open-source form for
    the Xilinx Zynq platform. Specifically, this article offers the following contributions:


    - We describe a methodology to map and runtime-manage clauses across a processor
    and connected FPGA, making efficient use of FPGA resources and avoiding recurring
    performance pitfalls.

    - We describe the implementation of an open-source prototype system, deployed
    on a Xilinx Zynq chip, identifying how the hardware architecture effects the aforementioned
    strategy.

    - We evaluate our design against the state of the art using two representative
    benchmarks, showing speed-ups of 1.7x and 1.1x, respective, and overall a 6x improvement
    over vanilla software execution.


    Section [2](#page-0-0) describes necessary background knowledge on a particular
    SAT-solving algorithm required to understand the remainder of this paper. Section
    [3](#page-1-0) presents an overview of historical solutions and state of the art,
    directly compared against in this paper. Section [4](#page-2-0) presents our contribution,
    evaluated in Section [5,](#page-3-0) with concluding remarks and suggestions for
    future work described in Section [6.](#page-4-5)


    ### <span id="page-0-0"></span>2 BACKGROUND: DPLL AND BCP


    SAT solvers are categorized into complete and incomplete solvers[\[15\]](#page-4-6).
    Complete solvers evaluate every possible variable assignment, ending on the first
    satisfying assignment or after exhausting the search space. A formula is unsatisfiable
    if the complete solver concludes without finding a satisfying assignment. Most
    incomplete solvers use Stochastic Local Search (SLS) to greedily search for a
    satisfying assignment in the formula''s variable assignment search space[\[16\]](#page-4-7).
    While typically quicker than complete solvers, incomplete solvers do not guarantee
    results as they tend to get stuck in local maxima or skip satisfying assignments.
    Since they don''t explore the solution


    Permission to make digital or hard copies of part or all of this work for personal
    or classroom use is granted without fee provided that copies are not made or distributed
    for profit or commercial advantage and that copies bear this notice and the full
    citation on the first page. Copyrights for third-party components of this work
    must be honored. For all other uses, contact the owner/author(s).


    GLSVLSI ''24, June 12–14, 2024, Clearwater, FL, USA Govindasamy, Esfandiari, and
    Garcia


    <span id="page-1-1"></span>![](_page_1_Figure_2.jpeg)


    Figure 1: Interface between processor and FPGA-based BCP Coprocessor. DPLL''s
    BCP is accelerated through fine-grained parallelization across Clause Processors.


    space exhaustively they can never conclude that a formula is unsatisfiable. Davis-Putnam-Logemann-Loveland
    (DPLL) and DPLL-based algorithms are the most predominant complete solvers [\[16\]](#page-4-7).
    DPLL performs two primary operations: 1) decision and 2) Boolean Constraint Propagation
    (BCP). DPLL-based algorithms follow DPLL''s core structure, and propose improved
    decision heuristics, learning and BCP mechanisms. During decision, DPLL heuristically
    picks and assigns truth values to free variables. BCP subsequently propagates
    the effect of the decision using the unit implication rule[\[5\]](#page-4-8).
    The unit implication rule identifies unit clauses where all but one of its literals
    are false. Unit clauses can only be satisfied by assigning the variable to true
    if the literal is positive or to false on negative literals. The resulting assignment
    is known as an implication. BCP repeatedly applies this rule until all clauses
    are satisfied (formula is therefore satisfiable) or at least one clause evaluates
    false (conflict). On conflicts, DPLL backtracks by retracting and/or inverting
    assignments from earlier decisions. BCP is expensive, accounting for 80-90% of
    DPLL''s CPU time, rendering it a prime candidate for hardware acceleration [\[5,](#page-4-8)
    [18\]](#page-4-9). BCP coprocessors accelerate DPLL by implementing specialized
    BCP processing engines on FPGA. These run alongside a General Purpose Processor
    (GPP) that performs the remaining DPLL operations: decision heuristics and backtracking.
    Using this architecture, the BCP coprocessor is first configured with the clauses,
    and then waits to evaluate decisions from the GPP. Any DPLL-based software solver
    can integrate with a BCP-coprocessor by replacing software BCP with the hardware
    accelerated BCP-coprocessor [\[18,](#page-4-9) [19\]](#page-4-10). FPGA-based
    BCP coprocessors are either instance-specific or application-specific. Instancespecific
    solver are built to solve a single SAT instance and designed by translating an
    input formula into its equivalent logical circuit. However, to solve new instances,
    the FPGA requires a complete rebuild (synthesis and FPGA programming may take
    up to several hours). Although these solvers can be significantly quicker than
    their software counterparts, their performance becomes notably slower when build
    times are included. For instance, Ivan et al''s best result against the hole7
    benchmark achieves a 6.66x speedup against MiniSAT[\[11,](#page-4-11) [12\]](#page-4-12);
    however, when build times are included, compilation alone takes 50 seconds, whereas
    MiniSAT finishes in under 0.064 seconds [\[10\]](#page-4-13). Application-specific
    solvers eliminate the need to rebuild the FPGA by instantiating general-purpose
    processing units capable of tackling any SAT instance (given that it fits in hardware).
    The BCP coprocessor is configured with the target problem by simply overwriting
    FPGA memory.


    #### <span id="page-1-0"></span>3 STATE OF THE ART


    Algorithmic techniques for efficient SAT solving have been extensively researched,
    and the literature contains several surveys that describe the history and state
    of the art of the problem ([\[8\]](#page-4-14), [\[13\]](#page-4-15)). Techniques
    aimed at accelerating the execution of a particular SAT solving algorithm include
    software parallelization [\[9\]](#page-4-16), deployment on specialized GPUs [\[14\]](#page-4-17),
    and even acceleration through machinelearning approaches [\[20\]](#page-4-18).


    Our approach sits within FPGA-based acceleration, which began roughly 3 decades
    ago [\[6\]](#page-4-19), with a few prominent results at the turn of the century
    ([\[21\]](#page-4-20), [\[3\]](#page-4-21)). However, it was not until significant
    advances in FPGA performance occurred in the last decade, and the rise of SoC
    platforms combining FPGA fabric with hard processors, that FPGA-based SAT acceleration
    matured. The most notable architectures were proposed by Davis et al [\[5\]](#page-4-8)
    and Thong et al [\[18,](#page-4-9) [19\]](#page-4-10): both exploring the use
    of FPGA to implement BCP coprocessors, keeping the remainder of DPLL in software.


    Davis et al calculate implications in parallel by using several inference engines
    (IE), each assigned a list of clauses (partitions) [\[5\]](#page-4-8). For every
    decision/implication, the clause containing the assignment variable is first retrieved
    before calculating implications. Implications are forwarded to a conflict detector
    that ensures that two or more IEs have not implied opposing values for the same
    variable. Implications are then sent to the processor and queued up for propagation.


    To keep clause retrieval time low, a variable only occurs once in each IEs partition
    (i.e clauses within the same IE share no common variables). This limits the number
    of clauses affected by a decision to one, thereby also limiting implications per
    IE to one, constraining the effected performance. While some strategies to increase
    this limit have been proposed [\[4\]](#page-4-22), they remain unexplored.


    Thong et al. propose a concurrent BCP coprocessor comprising multiple sequential
    processing engines (PE) [\[19\]](#page-4-10). Identifying that Davis et al.''s
    clause lookup is slower than direct access [\[18\]](#page-4-9), they develop a
    clause storage and encoding scheme that efficiently links clauses with shared
    variables. The processor sends decisions to the FPGA and starts BCP execution
    at a single PE. Using the linked list, the PE traverses every clause containing
    the decision variable and calculates implications, which are then added to a local
    queue and propagated. The running PE triggers BCP execution in another PE when
    it arrives at a link to a clause that is located elsewhere. The coprocessor supports
    multithreaded software execution, hiding communication and software latency by
    keeping the coprocessor busy while software threads make decisions when possible.


    Davis et al. and Thong et al. have laid a strong foundation in developing application-specific
    FPGA-based BCP coprocessors; we extend their work and propose a solution that
    processes clauses in parallel without the need for clause lookup.


    Accelerating Boolean Constraint Propagation for Efficient SAT-Solving on FPGAs
    GLSVLSI ''24, June 12–14, 2024, Clearwater, FL, USA


    #### <span id="page-2-0"></span>4 THE SAT SOLVER ARCHITECTURE


    We present a BCP coprocessor that works alongside vanilla DPLL (and should, in
    theory, work seamlessly with any DPLL-based solver). Like Thong et al., we forgo
    clause lookup and allow clauses to share variables within the same partition.
    However, we still achieve Davis et al.''s high degree of parallelism by placing
    clauses directly in clause processors (explained in Section [4.1\)](#page-2-1).


    SAT instances larger than the available number of Clause Processors (CPs) are
    partitioned, stored in external memory (i.e., software) and hot-swapped into the
    BCP coprocessors as required during runtime. Solvable instance size is limited
    only by the GPP''s RAM, not on-chip FPGA memory. We deploy our solution on the
    Zynq chip, and available here[1](#page-2-2) for use. To our knowledge, this is
    the first open-source hardware-accelerated SAT solver.


    #### <span id="page-2-1"></span>4.1 The BCP accelerator architecture


    Figure [1](#page-1-1) illustrates our approach, comprising a GPP and an FPGA accelerated
    BCP coprocessor. The GPP executes DPLL''s remaining elements (decisions, backtrack,
    etc.), partitions large SAT instances (explained in Section [4.2\)](#page-2-3)
    and swaps partitions into hardware as required. Its default state is idle, awaiting
    instructions to execute. Once a decision is received, the systems loops until
    all unit clauses are exhausted. The BCP coprocessor, depicted in Figure [1,](#page-1-1)
    comprises a control unit (1), an array of clause processors (2) and an implication
    selector (3). The central control unit communicates directly with the GPP and
    each CP. Based on the received GPP command, it loads clauses into CPs, broadcasts
    decisions, or clears assignments during backtrack. At its core, the BCP coprocessor
    consists of an array of CPs that calculate decision and implication results in
    parallel. CPs store clauses as an array of literals maintain a local copy of each
    literal''s respective variable assignment. Partitions are hot-swapped into FPGA
    by overwriting a CPs array of literals with the literals of the new clause. Variable
    assignments are updated during decisions and BCP, and cleared during backtrack.
    Finally, the implication selector chooses a single implication to propagate when
    multiple implications arise as a result of BCP. Rather than using an explicit
    implication conflict detector, as done by Davis et al [\[5\]](#page-4-8), we propagate
    the chosen implication, and identify conflicts during evaluation.


    #### <span id="page-2-3"></span>4.2 Formulae partitioning


    SAT instances contain an arbitrary number of variables and clauses. The problem
    size solvable on FPGA is limited by its available Configurable Logic Block (CLB)
    and memory, and requires large problems be partitioned into smaller manageable
    sizes. Partitions are stored in the GPP, and swapped into FPGA during run time
    by overwriting CPs clauses. BCP is performed individually on each partition, and
    implications are relayed back to the GPP. Implications are subsequently propagated
    to other partitions. We aim to make partitions as large as possible, limited by
    the coprocessor''s clause and variable threshold. Consider Equation [1,](#page-2-4)
    composed of four clauses, and an instance of our coprocessor that supports two
    clauses and three variables. Equation [2](#page-2-5) and [3](#page-2-6) outline
    the two possible ways to partition Equation [1.](#page-2-4) Equation [2](#page-2-5)
    describes a scenario where the partitions


    <span id="page-2-7"></span>![](_page_2_Figure_10.jpeg)


    Figure 2: (a) Davis et al. store the formula directly on FPGA. Clauses within
    partitions contain no shared variables, and partitions are mapped directly to
    Implication Engines. (b) Thong et al. store formula directly on FPGA. Clauses
    are linked to other clauses with shared variables and are processed sequentially.
    (c) Formula stored in external memory ("software" view). Clauses in partitions
    mapped directly to Clause Processors, and hot-swapped as required.


    reach the clause limit, while the Equation [3](#page-2-6) reaches the variable
    limit.


    <span id="page-2-4"></span>

    $$f = (\neg a \lor b \lor \neg c) \land (a \lor \neg b \lor \neg c) \land (\neg
    d \lor e \lor f) \land (d \lor e \lor f) \tag{1}$$


    <span id="page-2-5"></span>

    $$\begin{aligned} \{ \begin{aligned} \{ \begin{aligned} \{ \begin{array}{l} (\neg
    a \lor b \lor \neg c) \land (a \lor \neg b \lor \neg c) \end{array} \} \end{aligned}
    \} \end{aligned} \} $$


    <span id="page-2-6"></span>

    $$

    \begin{aligned}

    \{\neg variation\\_2 &= \{\{ (\neg a \lor b \lor \neg c) \}, \{ (a \lor \neg b
    \lor \neg c) \} \}, \\

    \{ (\neg d \lor e \lor f) \}, &\{ (d \lor e \lor f) \} \end{aligned}

    \begin{aligned}

    \{ \neg variation\\_2 \}, \end{aligned}

    $$


    Results (refer to Section [5\)](#page-3-0) indicate that partitioning is a bottleneck
    in our approach. Performance improvement is dictated by the amount of required
    partition swapping and the number of unused CPs (occurs when the number of clauses
    in a partition is less than the available number of CPs). Thus, performance improvement
    is observed with certain partition assignments, while others lead to performance
    degradation. System performance can be improved by developing a more effective
    partitioning algorithm, but beyond the scope of this paper and reserved for future
    work.


    #### 4.3 Execution


    Each clause processor is only associated with a single clause; thus, no clause
    look-up or traversal is required to retrieve the affected clause for processing.
    All clauses on the FPGA are processed in parallel as soon as a decision is received.
    Consider Equation [1''](#page-2-4)s


    <span id="page-2-2"></span><sup>1</sup>[https://github.com/harigovind1998/FPGA\\_BCP\\_acceleration](https://github.com/harigovind1998/FPGA_BCP_acceleration)


    GLSVLSI ''24, June 12–14, 2024, Clearwater, FL, USA Govindasamy, Esfandiari, and
    Garcia


    <span id="page-3-1"></span>


    | Step                 |      | 0    | 1                              | 2                       |
    3                   | 4    |

    |----------------------|------|------|--------------------------------|-------------------------|---------------------|------|

    | Our<br>Approach      | CP   | Rx   | Process<br>Decision  Clause 1  | Done                    |                     |      |

    |                      | CP 2 | Rx   | Process<br>Decision  Clause 2  | Done                    |                     |      |

    | Davis et<br>al.      | IE 1 | Rx   | Retrieve<br>Decision Clause 1  | Process<br>Clause
    1     | Done                |      |

    |                      | IE 2 | Rx   | Retrieve<br>Decision  Clause 2 | Process<br>Clause
    2     | Done                |      |

    | Thong et PE 1<br>al. |      | Rx   | Process<br>Decision  Clause 1  | Traverse
    to<br>Clause 2 | Process<br>Clause 2 | Done |

    |                      | PE 2 | ldle | ldle                           | ldle                    |
    ldle                | ldle |


    Figure 3: Execution steps of each described approach.


    mapping of partitions to hardware as presented in Figure [2.](#page-2-7) Figure
    [3](#page-3-1) summarizes the execution stages for Davis et al.''s, Thong et al.''s
    and our approach for the theoretical execution for a decision of variable . In
    our approach, clauses 1 and 2 are processed by Clause Processor 1 and 2 in parallel
    once the decision is received. Since clauses 3 and 4 do not contain variable ,
    Partition 2 remains in external memory and is not processed. Though Davis et al.
    also process clause 1 and 2 in parallel, each Implication Engine first performs
    a clause look-up to retrieve the affected clause. Results of the decision on the
    affected clause are then calculated. Thong et al.''s approach starts BCP on Processing
    Engine 1. After clause 1 is processed Processing Engine 1 traverses to clause
    2. In the manner, clauses in a partition are processed sequentially. Execution
    concludes after computing Partitions 1''s final element, clause 2. Processing
    Engine 2 remains idle for the entire duration as clauses in partition 2 do not
    contain variable .


    #### 4.4 Processor-FPGA interface


    The BCP coprocessor implements the Advanced eXtensible Interface 4-Lite (AXI-Lite)
    IP interface, acting as a subordinate to a processor (AXI master). Using AXI,
    the processor writes directly to the coprocessor''s registers to send instructions
    and data, and continues polling for status updates and new implication until the
    coprocessor completes.


    Status changes dictate DPLL''s flow, either allowing the search to continue assigning
    additional variables, or triggers backtracking on conflicts. A copy of all the
    implications are saved on the processor to avoid re-assigning implied variables,
    and further propagated to the remaining partitions.


    #### <span id="page-3-0"></span>5 EXPERIMENTS AND RESULTS


    On a Xilinx Zynq chip with total capacity of 14400 LUTs and 28800 FF, our solution
    supports 224 parallel Clause Processors and 63 variables. We achieve a clock frequency
    of 106.66 MHz, utilizing 647 LUTRAM of on-chip memory, 13151 LUTs, and 11059 FFs.


    Related work calculates throughput (in BPCs performed per second), assuming full
    data availability: i.e., not taking into account software execution and communication/data
    transfer latency. Whilst this is a useful metric to assess hardware performance
    in isolation (and we report equivalent results in Table [1\)](#page-3-2), it does
    not accurately depict system performance; to do so, we break down


    <span id="page-3-2"></span>


    |               | Millions of BCP/s |                     |            |  |  |

    |---------------|-------------------|---------------------|------------|--|--|

    | SAT Instance  | Davis et al [5]   | Thong et al<br>[19] | Our Design |  |  |

    | bmc-galileo-8 | 40                | 102                 | 175        |  |  |

    | bmc-ibm-12    | 33                | 150                 | 169        |  |  |


    Table 1: Comparison of BCP engine throughput (BCPs/s) with related work. Results
    reflect maximum theoretical throughput, achieved only data is fully available
    to BCP engines.


    <span id="page-3-3"></span>![](_page_3_Figure_13.jpeg)


    Figure 4: Breakdown of the total execution time across constituent components.


    <span id="page-3-4"></span>


    |         |       | Variables  |           |           |           |  |  |  |

    |---------|-------|------------|-----------|-----------|-----------|--|--|--|

    |         |       | 63         | 126       | 252       | 630       |  |  |  |

    | Clauses | 224   | 362M BCP/s | 17K BCP/s |           | NA        |  |  |  |

    |         |       | 2.2x       | 0.17x     | NA        |           |  |  |  |

    |         | 448   | 702K BCP/s | 21K BCP/s | 13K BCP/s | NA        |  |  |  |

    |         |       | 1.6x       | 0.21x     | 0.08x     |           |  |  |  |

    |         | 2240  | 441K BCP/s | 22K BCP/s | 16K BCP/s | 12K BCP/s |  |  |  |

    |         |       | 1.91x      | 1.26x     | 0.61x     | 0.10x     |  |  |  |

    |         | 22400 | 313K BCP/s | 20K BCP/s | 16K BCP/s | 14K BCP/s |  |  |  |

    |         |       | 6.32x      | 5.04x     | 4.86x     | 3.31x     |  |  |  |


    Table 2: Varied clause/variable sizes and their impact on the relative speedup
    of hardware/software and the effective throughput of BCP engines.


    the full execution in Figure [4](#page-3-3) and evaluate speedup over vanilla
    software implementation, evaluating combinations of clause and variable sizes,
    with speedup depicted in Table [2](#page-3-4) for meaningful combinations. For
    each combination, we also depict real throughput, in the form of BCPs/s averaged
    over total execution time (63 variables and 224 clauses is the theoretical upper
    bound, without the need for hot swapping). To evaluate the different effects of
    clause/variable sizes on execution, we fix one and vary the other, measuring total
    execution time: results are depicted in Figures [5](#page-4-23) and [6.](#page-4-24)


    <span id="page-4-0"></span>Accelerating Boolean Constraint Propagation for Efficient
    SAT-Solving on FPGAs GLSVLSI ''24, June 12–14, 2024, Clearwater, FL, USA


    <span id="page-4-23"></span>![](_page_4_Figure_1.jpeg)


    ![](_page_4_Figure_2.jpeg)


    <span id="page-4-24"></span>![](_page_4_Figure_3.jpeg)


    Figure 6: Effect of increasing variables size on total execution time, for 22400
    clauses.


    #### <span id="page-4-5"></span>6 CONCLUSIONS


    We described a SAT-solver hardware-accelerated architecture that outperforms state
    of the art by hot-swapping clause assignment at runtime, making efficient use
    of FPGA resources. Our solution prototype, on a Xilinx Zynq chip, is available
    in open-source. Practitioners may use the presented solution in their designs,
    whenever a problem is encoded in SAT form and performance is critical.


    An important open question remains: our performance is constrained by how clauses
    are partitioned. A partitioning scheme that minimizes the distribution of variables
    among clauses will minimize runtime swapping, resulting in improved execution.
    However, how to best partition a formula to achieve this is not yet known. Future
    work must formulate this challenge as an optimization problem, and methods for
    its efficient solution must be devised. Once that is achieved, they can be applied
    (offline) prior to deployment on our architecture.


    #### ACKNOWLEDGMENTS


    We acknowledge the support of the Natural Sciences and Engineering Research Council
    of Canada (NSERC).


    #### REFERENCES


    - <span id="page-4-4"></span>[1] Rabie Ben Atitallah and Karim MA Ali. 2017. FPGA-Centric
    High Performance Embedded Computing: Challenges and Trends. In 2017 Euromicro
    Conference on Digital System Design (DSD). IEEE, 390–395.

    - <span id="page-4-2"></span>[2] Stephen A Cook. 2023. The complexity of theorem-proving
    procedures. In Logic, Automata, and Computational Complexity: The Works of Stephen
    A. Cook. 143–152.

    - <span id="page-4-21"></span>[3] Andreas Dandalis and Viktor K Prasanna. 2002.
    Run-time performance optimization of an FPGA-based deduction engine for SAT solvers.
    ACM Transactions on Design Automation of Electronic Systems (TODAES) 7, 4 (2002),
    547–562.

    - <span id="page-4-22"></span>[4] John D Davis, Zhangxi Tan, Fang Yu, and Lintao
    Zhang. 2008. Designing an efficient hardware implication accelerator for SAT solving.
    In International Conference on Theory and Applications of Satisfiability Testing.
    Springer, 48–62.

    - <span id="page-4-8"></span>[5] John D. Davis, Zhangxi Tan, Fang Yu, and Lintao
    Zhang. 2008. A practical reconfigurable hardware accelerator for boolean satisfiability
    solvers. In 2008 45th ACM/IEEE Design Automation Conference. 780–785. [https://doi.org/10.1145/](https://doi.org/10.1145/1391469.1391669)
    [1391469.1391669](https://doi.org/10.1145/1391469.1391669)

    - <span id="page-4-19"></span>[6] Amir H Farrahi and Majid Sarrafzadeh. 1994.
    FPGA technology mapping for power minimization. In International Workshop on Field
    Programmable Logic and Applications. Springer, 66–77.

    - <span id="page-4-1"></span>[7] Michael R. Garey and David S. Johnson. 1990.
    Computers and Intractability; A Guide to the Theory of NP-Completeness. W. H.
    Freeman & Co., USA.

    - <span id="page-4-14"></span>[8] Weiwei Gong and Xu Zhou. 2017. A survey of SAT
    solver. In AIP Conference Proceedings, Vol. 1836. AIP Publishing.

    - <span id="page-4-16"></span>[9] Youssef Hamadi, Said Jabbour, and Lakhdar Sais.
    2010. ManySAT: a parallel SAT solver. Journal on Satisfiability, Boolean Modeling
    and Computation 6, 4 (2010), 245–262.

    - <span id="page-4-13"></span>[10] Anping He, Lvying Yu, Haitao Zhang, Lian Li,
    and Jinzhao Wu. 2018. A FPGA Based SAT Solver with High Random and Concurrent
    Strategies. In 2018 IEEE International Conference on Software Quality, Reliability
    and Security Companion (QRS-C). 221–228.<https://doi.org/10.1109/QRS-C.2018.00049>

    - <span id="page-4-11"></span>[11] Teodor Ivan and El Mostapha Aboulhamid. 2013.
    An Efficient Hardware Implementation of a SAT Problem Solver on FPGA. In 2013
    Euromicro Conference on Digital System Design. 209–216.<https://doi.org/10.1109/DSD.2013.31>

    - <span id="page-4-12"></span>[12] Teodor Ivan and El Mostapha Aboulhamid. 2013.
    Exploring limits of parallelism in FPGA-based Boolean satisfiability. In 2013
    2nd Mediterranean Conference on Embedded Computing (MECO). 62–65.<https://doi.org/10.1109/MECO.2013.6601319>

    - <span id="page-4-15"></span>[13] Ruben Martins, Vasco Manquinho, and Inês Lynce.
    2012. An overview of parallel SAT solving. Constraints 17 (2012), 304–347.

    - <span id="page-4-17"></span>[14] Muhammad Osama, Anton Wijs, and Armin Biere.
    2021. SAT solving with GPU accelerated inprocessing. In International Conference
    on Tools and Algorithms for the Construction and Analysis of Systems. Springer,
    133–151.

    - <span id="page-4-6"></span>[15] I. Skliarova and A.B. Ferrari. 2004. A software/reconfigurable
    hardware SAT solver. IEEE Transactions on Very Large Scale Integration (VLSI)
    Systems 12, 4 (2004), 408–419.<https://doi.org/10.1109/TVLSI.2004.825859>

    - <span id="page-4-7"></span>[16] Ali Asgar Sohanghpurwala, Mohamed W. Hassan,
    and Peter Athanas. 2017. Hardware accelerated SAT solvers: A survey. J. Parallel
    and Distrib. Comput. 106 (2017), 170–184.<https://doi.org/10.1016/j.jpdc.2016.12.014>

    - <span id="page-4-3"></span>[17] Robert Stewart, Bernard Berthomieu, Paulo Garcia,
    Idris Ibrahim, Greg Michaelson, and Andrew Wallace. 2019. Verifying parallel dataflow
    transformations with model checking and its application to FPGAs. Journal of Systems
    Architecture 101 (2019), 101657.

    - <span id="page-4-9"></span>[18] Jason Thong and Nicola Nicolici. 2013. FPGA
    acceleration of enhanced boolean constraint propagation for SAT solvers. In 2013
    IEEE/ACM International Conference on Computer-Aided Design (ICCAD). 234–241. [https://doi.org/10.1109/](https://doi.org/10.1109/ICCAD.2013.6691124)
    [ICCAD.2013.6691124](https://doi.org/10.1109/ICCAD.2013.6691124)

    - <span id="page-4-10"></span>[19] Jason Thong and Nicola Nicolici. 2015. SAT
    solving using FPGA-based heterogeneous computing. In 2015 IEEE/ACM International
    Conference on Computer-Aided Design (ICCAD). 232–239.<https://doi.org/10.1109/ICCAD.2015.7372575>

    - <span id="page-4-18"></span>[20] Haoze Wu. 2017. Improving SAT-solving with
    machine learning. In Proceedings of the 2017 ACM SIGCSE Technical Symposium on
    Computer Science Education. 787–788.

    - <span id="page-4-20"></span>[21] Peixin Zhong, Margaret Martonosi, and Pranav
    Ashar. 2000. FPGA-based SAT solver architecture with near-zero synthesis and layout
    overhead. IEE Proceedings-Computers and Digital Techniques 147, 3 (2000), 135–141.'
- title: '**European Data Protection Supervisor**'
  abstract: ''
  keywords: ''
  document: '> … > [TechSonar](https://edps.europa.eu/data-protection/technology-monitoring/techsonar_en)
    > Large language models (LLM)


    ![](_page_0_Picture_0.jpeg)


    # **European Data Protection Supervisor**


    # Translate this page **Large language models (LLM)**


    ![](_page_0_Picture_4.jpeg)


    **Author:** Xabier Lareo


    Language models are artificial intelligence (AI) systems designed to learn grammar,
    syntax and semantics of one or more languages to generate coherent and context-relevant
    language. Language models have been developed using neural networks since the
    1990s, but the results were modest.


    The evolution to large language models (LLMs) was made possible by technical developments
    that improved the performance and efficiency of AI systems.


    These developments included the advent of large-scale pre-trained models, the
    development of transformers (which learn context and meaning by tracking relationships
    in sequential data), and self-attention mechanisms (which allow models to weigh
    the importance of different elements in an input sequence and dynamically adjust
    their influence on the output).


    As a type of generative AI system, LLMs create new content in response to user
    commands based on their training data. They are trained on huge amounts of text
    sources (from billions to billions of words) from a variety of sources, including
    public sources, and their size can be measured by the number of parameters used.


    They''re also considered a type of ''foundation model'', which is a model trained
    on large amounts of data (usually using large-scale selfmonitoring) that can be
    adapted to a variety of applications, including text generation, summarising,
    translating, answering questions, and more.


    The number of parameters in LLMs has increased over time: while version 2 of the
    Generative Pre-trained Transformer (GPT-2) had 1.5 billion parameters, the Pathways
    Language Model (PaLM) reached 540 billion parameters. At a certain point, the
    development of competitive highperformance LLMs seemed to be something that only
    the most resourceful technology companies, such as Google, Meta or OpenAI, could
    achieve.


    However, two developments changed that trend and made LLM development more broadly
    available. First, the publication of research showing that there is an optimal
    set of values when selecting computing power, model size and training dataset
    size. Second, the appearance of parameter efficient fine-tuning techniques (e.g.
    LoRA), which have greatly reduced the amount of resources needed to train an LLM
    - PALM 2 already following this trend and, although it appears to have been trained
    with a much larger dataset, it has fewer parameters than its predecessor (340
    billion against PaLM''s 540 billion).


    Some LLM service providers have made their models publicly available – previous
    registration and, in several cases, using a subscription model through web interfaces
    that allow users to enter commands (prompts) and view the output generated by
    the models. Publicly accessible models are sometimes presented as research previews
    or testing versions that might produce erroneous or harmful output. LLM service
    providers also tend to offer access to their models (usually for a fee) through
    an application programming interface (API) that allows their LLM to be embedded
    into customers'' IT systems.


    LLMs are currently being used or tested for a wide variety of tasks in different
    domains, including translation; customer care (e.g. chatbots); education (e.g.
    language training); natural language processing (e.g. named entity recognition
    or summarisation); supporting the generation of images from a given prompt output;
    preparation of programming code; or even the creation of artistic works.


    As LLMs continue to evolve, they both offer opportunities and important challenges
    for privacy and data protection.


    # **Positive impacts foreseen on data protection:**


    LLMs could be used to support certain privacy activities in very specific scenarios,
    if designed, developed and deployed in a responsible and trustworthy manner, respecting
    the principles of data protection, privacy, human control and transparency.


    For example:


    **Detection of personal data**


    Identifying personal data in unstructured data, such as in text fields is relatively
    easy for humans, but difficult to automate using simple rules. However, human
    review does not scale well and becomes impractical or unfeasible in large-text
    files or web-scraped datasets. The natural language processing capabilities of
    LLMs could help detect and better manage personal data on unstructured information
    (e.g. a text field containing family history). LLMs could also help reduce the
    personal data included in their training datasets, by automatically identifying,
    redacting or obfuscating personal data.


    # **Negative impacts foreseen on data protection:**


    The vast majority of the data used to train state-of-the-art LLMs are texts scraped
    from publicly available Internet resources (e.g. the latest Common Crawl dataset,
    which contains data from more than 3 billion pages). These web-scraped datasets
    contain personal data of public figures, but also of other individuals. Personal
    data contained in these datasets could be accurate or inaccurate. These datasets
    could also contain plain misinformation. Implementing controls to address the
    data protection risks posed by the use of these datasets is very challenging.
    Moreover, if not properly secured, LLM output might reveal sensitive or private
    information included in the datasets used for training, leading to potential or
    real data breaches.


    LLMs sometimes suffer from so-called ''hallucinations'', meaning they produce
    erroneous information that appears to be correct. When hallucinating, an LLM can
    produce false or misleading information about individuals. Inaccurate information
    can affect individuals not only because it can damage their public image, but
    also because it can lead to decisions that affect them. LLMs, if trained on biased
    data, could perpetuate or even amplify biases present in their training data.
    This might lead to unfair or discriminatory outputs, potentially violating the
    principle of fair processing of personal data.


    LLMs store the data they learn in the form of the value of billions or trillions
    of parameters, rather than in a traditional database. For this reason, rectifying,
    deleting or even requesting access to personal data learned by LLMs, whether it
    is accurate or made up of "hallucinations", may be difficult or impossible.


    ## **Suggestions for further reading:**


    [https://edps.europa.eu/system/files/2023-10/edps-gpa-resolution-on-generative-ai-systems\\_en.pdf](https://edps.europa.eu/system/files/2023-10/edps-gpa-resolution-on-generative-ai-systems_en.pdf)


    ## **Training LLMs is a data-intensive activity, which can include personal data**


    ### **"Hallucinations", data accuracy and bias**


    ### **Implementing data subjects'' rights is difficult**


    Vaswani, Ashish, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan
    N. Gomez, Lukasz Kaiser and Illia Polosukhin. "Attention is All you Need.", 2017,<https://doi.org/10.48550/arXiv.1706.03762>


    Kaplan, Jared, Sam McCandlish, T. J. Henighan, Tom B. Brown, Benjamin Chess, Rewon
    Child, Scott Gray, Alec Radford, Jeff Wu and Dario Amodei. "Scaling Laws for Neural
    Language Models.", 2020, <https://doi.org/10.48550/arXiv.2001.08361>


    Hu, Edward J., Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean
    Wang, Lu Wang, and Weizhu Chen. "LoRA: Low-rank adaptation of large language models",
    2021, [https://arxiv.org/abs/2106.09685v2.](https://arxiv.org/abs/2106.09685v2)


    Naveed, Humza, Asad Ullah Khan, Shi Qiu, Muhammad Saqib, Saeed Anwar, Muhammad
    Usman, Nick Barnes, and Ajmal Mian. "A comprehensive overview of large language
    models." , 2023,<https://doi.org/10.48550/arXiv.2307.06435>


    Global Privacy Assembly Resolution on Generative Artificial Intelligence Systems,
    2023,'
- title: '**HOPE: Holistic STT-RAM Architecture Exploration Framework for Future Cross-Platform
    Analysis**'
  abstract: ''
  keywords: '* Non-volatile memory, STT-RAM, Power Estimation, gem5, Emerging Technologies'
  document: "Date of publication xxxx 00, 0000, date of current version xxxx 00, 0000.\
    \ *Digital Object Identifier 10.1109/ACCESS.XXXX.DOI*\n\n# **HOPE: Holistic STT-RAM\
    \ Architecture Exploration Framework for Future Cross-Platform Analysis**\n\n\
    **SAEED SEYEDFARAJI (Graduate Student Member, IEEE), MARKUS BICHL, ASAD AFTAB\
    \ (Graduate Student Member, IEEE),and SEMEEN REHMAN(Member, IEEE).<sup>1</sup>\
    \ ,**\n\n<sup>1</sup>Faculty of Electrical Engineering and Information Technology,\
    \ Vienna University of Technology (TU-Wien), 1040 Vienna, Austria Corresponding\
    \ author: Saeed Seyedfaraji (e-mail: saeed.seyedfaraji@tuwien.ac.at).\n\n**ABSTRACT**\
    \ Spin Transfer Torque Random Access Memory (STT-RAM) is an emerging Non-Volatile\
    \ Memory (NVM) technology that has garnered attention to overcome the drawbacks\
    \ of conventional CMOS-based technologies. However, such technologies must be\
    \ evaluated before deployment under real workloads and architecture. But there\
    \ is a lack of available open-source STT-RAM-based system evaluation framework,\
    \ which hampers research and experimentation and impacts the adoption of STT-RAM\
    \ in a system. This paper proposes a novel, extendable STT-RAM memory controller\
    \ design integrated inside the gem5 simulator. Our framework enables understanding\
    \ various aspects of STT-RAM, i.e., power, delay, clock cycles, energy, and system\
    \ throughput. We will open-source our HOPE framework, which will fuel research\
    \ and aid in accelerating the development of future system architectures based\
    \ on STT-RAM. It will also facilitate the user for further tool enhancement.\n\
    \n**INDEX TERMS** Non-volatile memory, STT-RAM, Power Estimation, gem5, Emerging\
    \ Technologies\n\n## **I. INTRODUCTION**\n\nSTT-RAM boastsseveral compelling featuresincluding\
    \ nonvolatility, high density, soft error reliability, CMOS compatibility, high\
    \ endurance, and scalability [1]–[4]. According to the International Roadmap for\
    \ Devices and Systems (IRDS) [5], STT-RAM emerges as the most promising memory\
    \ option to replace conventional memory technologies. Table 1 presents a comprehensive\
    \ comparison of various design elements related to memory technologies, such as\
    \ endurance, associated read/write energy, latency, and compatibility with CMOS\
    \ technology size. It is worth noting that the listed suppliers are not exclusive\
    \ options, as alternative providers may also offer each type of memory technology.\
    \ Considering the decreasing technology node size over time due to Denard's law,\
    \ it is important to note that the suggested compatibility in the table is based\
    \ on the findings from the literature review of [1]–[4], [6]–[18]. Therefore,\
    \ it is plausible that each memory technology could potentially be fabricated\
    \ with other CMOS sizes.\n\nComparatively, the write and read energy of Dynamic\
    \ Random Access Memory (DRAM) Static Random Access Memory (SRAM) per operation\
    \ is lower than that of NVMs. Nevertheless, both DRAM and SRAM, as volatile memory\
    \ types, rely on a continuous power supply to retain stored\n\ndata. DRAM, commonly\
    \ employed as the main memory in computers, consumes more energy due to its constant\
    \ need for refreshing to preserve data integrity. Similarly, SRAM, another volatile\
    \ memory technology, also requires a consistent power supply for data retention.\
    \ On the other hand, NVMs like Flash memory can maintain data even when power\
    \ is off, resulting in lower energy consumption compared to DRAM and SRAM. Therefore,\
    \ NVM became an attractive alternative for main memories because of lower energy\
    \ consumption. Phase Change Memory (PCM), and STT-RAM [19] are some of the common\
    \ NVMs proposed to replace DRAM as main memory. They have also been explored for\
    \ building larger on-chip caches because of their high density. Amongst all, STT-RAM\
    \ has matured a lot in terms of its on-chip computation, and high energy efficiency\
    \ [20].\n\nHowever, the utilization of STT-RAM in widespread industrial applications\
    \ is hindered by several limitations. Notably, challenges such as write operation\
    \ delays and high write energy consumption. To overcome these issues, different\
    \ approaches have been explored at various levels of abstraction, including circuit-level\
    \ approaches [21]–[25], architecture-level approaches [10], [26], [27], and methods\
    \ proposed from an application perspective [28], [29].\n\n![](_page_1_Picture_0.jpeg)\n\
    \n|                         | STT-RAM          | PCMRAM                  | RRAM\
    \              | Fe-FET               | FLASH           | SRAM            | DRAM\
    \              |\n|-------------------------|------------------|-------------------------|-------------------|----------------------|-----------------|-----------------|-------------------|\n\
    | Non-volatility          | +                | +                       | +   \
    \              | +                    | +               | -               | -\
    \                 |\n| Data Retention (years)  | 10               | 10       \
    \               | 10                | 10                   | 10              |\
    \ -               | -                 |\n| Cell Endurance (cycles) | 1016    \
    \         | 1012                    | 1010              | 1012               \
    \  | 106             | 1016            | 1015              |\n| Cell Size (F 2)\
    \         | 6-20             | 4-8                     | 4                 | 4-8\
    \                  | 4-6             | >100            | 4-12              |\n\
    | Technology node (nm)    | 45               | 65                      | 40  \
    \              | 5                    | 15              | 10              | 32\
    \                |\n| Read Latency (ns)       | 2-20             | 20-50     \
    \              | <10               | 10                   | 25x103          |\
    \ <5              | 2                 |\n| Write Latency (ns)      | 5-20    \
    \         | 30                      | 5                 | 10                 \
    \  | 500x103         | <5              | 5                 |\n| Erase Latency\
    \ (ns)      | 5-20             | 30                      | 10                |\
    \ 10                   | 2 (ms)          | <5              | 5               \
    \  |\n| Write Energy (pJ)       | 0.1-2.5          | 18                      |\
    \ 0.1               | 1                    | 0.1 - 1         | <0.1          \
    \  | <0.1              |\n| Erase Energy (pJ)       | 1                | 18  \
    \                    | 0.1               | 1                    | 1000       \
    \     | <1              | <1                |\n| Suppliers               | Toshiba,\
    \ Hitachi | Samsung, Intel, WD, IBM | Panasonic, Micron | Globalfoundries, FMC\
    \ | Micron, Samsung | Qualcomm, Intel | Samsung, SK Hynix |\n\nTABLE 1: Comprehensive\
    \ comparison of NVM technologies [1]–[4], [6]–[18]\n\n# *A. NEED FOR STT-RAM BASED\
    \ SYSTEM EVALUATION FRAMEWORKS*\n\nResearchers have made significant contributions\
    \ to enhancing comparison metrics within their respective levels of abstraction\
    \ [1], [2], [4], [6]–[11], [13], [14], [24], [25]. However, it is crucial to note\
    \ that the current architectural perspective findings are derived from a behavioral\
    \ model of the circuit, which may not offer precise and detailed outcomes comparable\
    \ to those from a real computing system. This approach falls short in addressing\
    \ the need for a comprehensive system exploration framework. Hence, the associated\
    \ research challenge is *how to design a holistic system evaluation framework\
    \ that can be used to evaluate the impact of incorporating STT-RAM memories in\
    \ current systems, while accurately modeling the scaling, energy consumption and\
    \ performance characteristics of these devices and enabling architectural design\
    \ space exploration.*\n\nA number of simulation environments are available for\
    \ research and development of system-level exploration of computer architectures,\
    \ such as gem5 and ZSIM [30]. However, gem5 is widely used due to its ability\
    \ to emulate the full-system mode and help in the exploration of systemlevel metrics,\
    \ with different instruction set architectures (ISAs) such as Alpha, ARM, SPARC,\
    \ MIPS, RISC-V, and x86 ISAs), and various timing and CPU modes [31], [32]. ZSIM,\
    \ as an alternative simulation software, does not offer full-system simulation\
    \ capabilities, but also does not rely on event-driven execution and is therefore\
    \ faster. As this work targets the integration of STT-RAM into a complete system,\
    \ also showing capabilities of executing an operating system on top of STT-RAM,\
    \ gem5 is the selected choice. gem5 showed fast enough simulation speed for benchmark\
    \ applications on top of an operating system.\n\n# *B. ANALYZING STT-RAM IMPACT\
    \ ON DIFFERENT APPLICATIONS*\n\nIn order to verify the framework design along\
    \ with its advantages, a case study i.e., investigating STT-RAM from the perspective\
    \ of reducing the energy consumption of High Performance Computing (HPC) applications,\
    \ and its characteristics i.e., power, area, latency, etc., is carried out. Until\
    \ now an ideal platform for system-level evaluation is the gem5 simulator, as\
    \ it provides methods for generating system environments with easily exchangeable\
    \ separated components such as the memory controller, DRAM inter-\n\n## face,\
    \ and NVM interface.\n\n# *C. NOVEL CONTRIBUTION*\n\nIn order to meet the requirement\
    \ of designing a holistic system exploration framework, this paper introduces\
    \ an innovative memory interface utilizing STT-RAM, making it a notable contribution.\
    \ The interface has been created and seamlessly integrated into the gem5 simulator,\
    \ establishing a connection with the included memory controller.\n\n*The novel\
    \ contributions of this paper are:*\n\n- *•* We propose HOPE which is an STT-RAM\
    \ modeling and simulation framework integrated into the full system simulator\
    \ gem5.\n- *•* We leverage the recently implemented NVM interface in gem5 to integrate\
    \ HOPE with existing gem5 memory interfaces. This is in contrast to prior approaches\
    \ that rely on external patches (like NVMain), which become less maintainable\
    \ over time, thus stymying further development. Our proposed framework introduces\
    \ a third memory interface tailored specifically for STT-RAM. This extension offers\
    \ highly detailed results comparable to the existing DRAM implementation within\
    \ gem5. Fortunately, integrating our framework into gem5 requires only minimal\
    \ changes to gem5 files, as all functionality is implemented in new files that\
    \ can be added seamlessly. Our implementation can be used identically to the existing\
    \ memory interfaces and can potentially be integrated into the official gem5 repository\
    \ by its core maintainers. Such integration would be the ideal outcome for our\
    \ work, ensuring ongoing compatibility with gem5.\n- *•* We also extend the power\
    \ model in gem5 DRAM-Power to support our proposed STT-RAM model.\n- *•* We evaluate\
    \ HOPE using HPC applications from the SPEC CPU 2017 benchmark suite on our event-driven\
    \ gem5 simulator and successfully extract evaluation metrics from both the application\
    \ and circuit perspectives.\n- *•* We will also open-source our framework to enable\
    \ and accelerate the development of future system architectures based on STT-RAM.\n\
    \nThe rest of the paper is organized as follows: Section II presents the various\
    \ state-of-the-art works w.r.t. different types of memory controllers implemented\
    \ inside gem5 and their drawbacks. In section III, the STT-RAM integration\n\n\
    with the memory controller inside gem5 is discussed in detail. Section IV provides\
    \ the evaluation metrics, results, and comparison. Finally, we conclude this paper\
    \ in section V.\n\n#### **II. BACKGROUND AND RELATED WORKS**\n\nSTT-RAM has been\
    \ recently exploited as an alternative to conventional on-chip memories because\
    \ of its low energy consumption, high-speed access rate, scalability, and boundless\
    \ endurance. However, several fundamental barriers, i.e., reliability issues due\
    \ to Read/Write failure, Process Variation (PV) effects leading to stochastic\
    \ switching time, should be considered before its vast industrial adaptation.\n\
    \n# *A. OPERATION PRINCIPLES AND STT-RAM STRUCTURE*\n\nThe most common structure\
    \ of the STT-RAM cell includes an MTJ cell for data storage in series with an\
    \ access transistor (1T1MTJ). MTJ cells include an oxide barrier sandwiched between\
    \ two ferromagnetic layers called Rotation Layer (RL) and Fixed Layer (FL). The\
    \ magnetization orientation between these two layers will result in two different\
    \ states (i.e., parallel(P) and anti-parallel(AP)). These states are interpreted\
    \ as an indicator of logic one and logic zero (see Fig. 1). The concept of Read\
    \ and Write operation in the cell is explained in detail as follows:\n\n#### 1)\
    \ Write Operation\n\nIn order to write the intended information into the MTJ cell,\
    \ a write current should be applied through the memory cell. A successful writing\
    \ operation demands a minimum barrier exploiting the energy of the MTJ cell (*Eb*).\
    \ If the required energy is supplied, then the state of the memory could be changed\
    \ based on the current direction through the memory cell (see Fig. 1). In a coherent\
    \ STT-RAM model, the required current to fulfill the required minimum barrier\
    \ exploiting energy could be expressed as:\n\n$$I\\_c = I\\_{c0}(1 - \\frac{1}{\\\
    Delta\\_{\\text{max}}} \\ln(f\\_0 t\\_p))\\tag{1}$$\n\n$$I\\_{c0} = \\frac{8aeM\\\
    _s t}{\\eta h \\pi d^2} H\\_k \\tag{2}$$\n\nwhere, *Ic*<sup>0</sup> : critical\
    \ write current of the STT-RAM model (at 273*◦* Kelvin). This parameter is related\
    \ to the physical\n\n![](_page_2_Figure_12.jpeg)\n\nFIGURE 1: Schematic representation\
    \ of magnetic orientation and energy barrier between two Magnetic Tunnel Junction\
    \ (MTJ) states [33]\n\nproperty of the MTJ cell, such as: *Ms*: Material saturation\
    \ magnetization; *f*0: Attempt frequency which is typically *∼*1 ns, *Hk*: Effective\
    \ magnetic anisotropy field, *α*: Damping factor, *η*: Spin polarization, *tp*:\
    \ Operating pulse-width (inverse of frequency), which provides an access frequency\
    \ of 1 GHz; *t, d*: physical dimensions of the MTJ cell; ∆: Thermal stability\
    \ factor, and it can be expressed as:\n\n$$\n\\Delta = \\frac{(M\\_s H\\_k \\\
    text{td})}{2k\\_B T} \\tag{3}\n$$\n\nwhere, *kBT* : Describes the ambient energy\
    \ in the system due to random thermal fluctuations [33].\n\n#### 2) Read Operation\n\
    \n*R<sup>P</sup>* Read operation in MTJ cell is due to the current *Iread* passage\
    \ through the cell. *Iread* must be less than *Icritical* and must not lead to\
    \ change in the cell. The MTJ exhibits resistance value based on its two layers'\
    \ magnetization (0*◦* or 180*◦* in Fig. 1). Therefore, by sensing, and passing\
    \ the *Iread* and measuring the resistance, we could identify the state of the\
    \ cell. Moreover, the parameter Tunnel Magnetoresistance Ratio (TMR) is described\
    \ as the difference between these two resistance states and can be expressed as\
    \ *TMR* = (*RAP <sup>−</sup>R<sup>P</sup>* ) , where *RAP* is the resistance in\
    \ anti-parallel and *R<sup>P</sup>* is the resistance in parallel state. TMR is\
    \ directly related to the cell's read latency, which means a higher TMR enables\
    \ a faster and more precise read operation [33].\n\n#### *B. RELATED WORK*\n\n\
    Recent studies show NVM as the main memory element via a simulation in gem5, as\
    \ gem5 was isolated from an NVM interface till October 2020. The lack of an NVM\
    \ interface made it necessary for NVM research to use external tools such as NVSim\
    \ [34], NVMain [35], and NVMain 2.0 [36]. To maintain compatibility, these NVM\
    \ simulators must be developed simultaneously with the gem5 simulator. Especially\
    \ NVMain offers a patch to be applied to the gem5 simulator, which connects the\
    \ NVMain tool to the gem5 simulator. This patch directly modifies the gem5 source\
    \ code and needs to be updated to match the latest gem5 releases. The result of\
    \ the patch is a co-simulator where the gem5 simulator is in constant interaction\
    \ with NVMain. Small changes in the gem5 simulator can directly require a change\
    \ in the NVM simulation tools and the needed patches for integration into a co-simulator.\
    \ The official patch for integrating NVMain into the gem5 was last updated in\
    \ December 2016 and is incompatible with recent releases of the gem5. Furthermore,\
    \ these state-of-theart STT-RAM gem5 tool flow lacks available open-source models,\
    \ which has hampered research and experimentation, impacting the adoption of STT-RAM\
    \ in the current systems.\n\nTherefore, in this manuscript, we propose a holistic\
    \ gem5 based framework that will be open-sourced to fuel research and development\
    \ in this area and further enhancement in the framework.\n\n![](_page_3_Figure_2.jpeg)\n\
    \nFIGURE 2: The STTDDR4 Interface integration into the gem5 standard componentslibrary.\
    \ Components in blue are modified or new to gem5, and components in white are\
    \ unmodified gem5 components.\n\nThe work presented in [37] explores some architecturelevel\
    \ modifications of STT-RAM structure aiming to provide NVM-based row buffers and\
    \ reports a 67% energy improvement exploiting their approaches compared with state-ofthe-art\
    \ techniques. Moreover, authors in [29] explore the possibility of using STT-RAM\
    \ to substitute the DRAM in main memory and evaluate their approach based on the\
    \ SPEC CPU2006 dataset to be compared with DRAM-based memories. This study has\
    \ been carried out on a tracebased cycle-accurate simulator. In [38], an STT-RAM-based\
    \ memory has been proposed based on a 9F2-cell on the circuit level. The exploited\
    \ MTJ model in this work requires a low switching current to change the state\
    \ from logicone to logic-zero. The application-level analysis has been estimated\
    \ based on an HPC SPEC CPU 2017 benchmark for latency improvement. The aforementioned\
    \ techniques either perform only circuit-level simulations (NVSim) which is typically\
    \ time-consuming due to detailed hardware simulations, while other approaches\
    \ that evaluate applications at the system level are not available open-source\
    \ (to the best of our knowledge), thus obstructing the adoption of the STT-RAM\
    \ model in the systems. *Our novel proposed HOPE framework has a fully integrated\
    \ architectural model of STT-RAM in the gem5 simulator using a memory controller\
    \ for exploiting system-level characteristics. This is in contrast to prior approaches\
    \ that rely on external patches (like NVMain). HOPE is an event-driven gem5 simulator\
    \ that facilitates system-level evaluation and enablesthe extraction of comparative\
    \ metrics across all layers of the system hierarchy.*\n\n## **III. THE HOPE FRAMEWORK**\n\
    \n#### *A. HOPE FRAMEWORK OVERVIEW*\n\nThe novel proposed STT-RAM interface in\
    \ this manuscript is an additional memory interface to the gem5. Therefore, in\
    \ order to satisfy the compatibility of the STT-RAM interface with the gem5's\
    \ MemCtrl component, there is a\n\nneed for some modifications to the MemCtrl\
    \ component. This tailoring has no effect on the existing functionality of connecting\
    \ DRAM or NVM memories.\n\nIn the system configuration of gem5, the MemCtrl component\
    \ offers a single port to connect a main memory instance to the MemCtrl, historically\
    \ called *DRAM*, used for all types of memories. We introduced the STT-RAM interface\
    \ to the gem5 as an alternative choice to the DRAMInterface and NVMInterface components\
    \ as shown in Fig. 2. The interface is implemented in C++ (Component functional\
    \ description), wrapped by a Python parameter configuration (part of Component\
    \ definitions). The Python wrapper defines and inherits parameters that are mandatory\
    \ for the component to work. The functionality of the STTDDR4Interface is placed\
    \ within its C++ class. The component STT\\_1333\\_4x16 is the test device, populated\
    \ with parameters from the EMD4E001G16G2 datasheet [39]. Our proposed framework\
    \ enables access to modify the memory via its integrated interface and fetch the\
    \ output data in our component definition. Thus, we integrated all the functionality\
    \ into the STT-RAM memory controller through the provided interface to configure\
    \ and evaluate the system analysis. This provides an edge over the state-of-the-art\
    \ proposed methods that are based on the co-simulation of gem5 and other simulation\
    \ tools (e.g., NVSIM, NVMAIN, etc.).\n\n#### *B. HOPE STT-RAM POWER MODEL*\n\n\
    The implemented STTDDR4 memory interface implementation represents a state machine\
    \ consisting of states for idle, storing, activating, power up, and power down\
    \ stages, as shown in Fig. 3. The MemCtrl instance of the simulated system and\
    \ the memory logic help in the transition of the state in the state machine. As\
    \ soon as the system is started, the very first state of the system is the PWR\\\
    _IDLE state. This is the state from where the transition to activate state PWR\\\
    _ACT, or activate with store state PWR\\_ACT\\_ST\n\n![](_page_4_Figure_1.jpeg)\n\
    \nFIGURE 3: Power state machine of STTDDR4 integration to gem5\n\ncan be achieved\
    \ using an ACT or ACT\\_ST command. The ACT\\_ST command is introduced to gem5\
    \ which is responsible for the STT-RAM-specific handling of data in the volatile\
    \ page buffer.\n\nThe main memory can be exploited using gem5 standard commands\
    \ i.e., for a bank activation ACT (activate), RD (read), WR (write), REF (refresh\
    \ all banks), SREF (Self-Refresh), PRE (explicit pre-charge of a single bank),\
    \ etc. In this manuscript, an additional command is been introduced i.e., ACT\\\
    _ST (activate with the store). In the EMD4E001G16G2 datasheet, [39], the introduced\
    \ ACT\\_ST command is known as ACT\\*. The ACT\\* command includes a store procedure\
    \ for the specific bank accessed. The implementation of the ACT\\_ST command to\
    \ gem5 also includes a new event, the *actStoreEvent*, to enable the transition\
    \ to the new power state PWR\\_ACT\\_ST. Moreover, the automatic execution of\
    \ REF commands needed for data persistence in DRAM is not included in STTDDR4,\
    \ as refreshes are not mandatory in STT-RAM devices as per the EMD4E001G16G2 devices.\n\
    \nAs we know that method calls are responsible for MemCtrl interaction with the\
    \ memory interface during simulation\n\n# **Algorithm 1** Select between ACT and\
    \ ACT\\_ST command on rank:bank:row bank.storingState *←* PERSISTENT bank.lastRow\
    \ *←* 0\n\n```\nprocedure ACTIVATEBANK(rank, bank, row)\n  cmd ← ACT\n  if bank.lastRow\
    \ ̸= row then\n     if bank.storingState = BUFFER then\n        cmd ← ACT _ST\n\
    \        cmdDelay ← cmdDelay + tST\n     end if\n  end if\n  cmdList.push_back(cmd,\
    \ bank, delay) \n  bank.lastRow ← row\n  bank.storingState ← BU FFER\n  process\
    \ cmd in drampower\n```\n#### **Algorithm 2** DRAM Power Extension nOfActsBanks[banks]\
    \ *←* zeros(banks) nOfStoresBanks[banks] *←* zeros(banks) **procedure** EVALUATECOMMANDS(*cmdList*)\
    \ **for all** *cmd ∈ cmdList* **do if** *cmd.type* = *ACT* **then** *handleAct*(*cmd.bank*)\
    \ **else if** *cmd.type* = *ACT*\\_*ST* **then** *handleActSt*(*cmd.bank*) **else\
    \ if** <other command types> **then** <handle commands> **end if end for end procedure\
    \ procedure** HANDLEACTST(*bank*) **if** isPrecharged(bank) **then** *nOf ActsBanks*[*bank*]\
    \ + +; *nOf StoresBanks*[*bank*] + +; **end if end procedure procedure** POWER\\\
    _CALC() *calc*(*sum*(*nOf StoresBanks*) *∗ tST, idd*0 *− idd*3*n*) **for all**\
    \ *bank ∈ banks* **do** *calc*(*nOf StoresBanks*[*bank*] *∗ tST, idd*0 *− ione*)\
    \ **end for end procedure**\n\n**procedure** CALC(*cycles*, *current*) **return**\
    \ (cycles\\*clkPeriod\\*current\\*voltage) **end procedure**\n\ne.g., while reading\
    \ or writing data, the MemCtrl initiates a burst access to the memory device where\
    \ MemCtrl provides data for which rank, bank and row the burst access needs to\
    \ be executed. This rank, bank, and row information are handed over to the bank\
    \ activation method, as shown in algorithm 1. The EMD4E001G16G2 device includes\
    \ functionality for automatic storing of page memory data to the persistent memory\
    \ array when the page memory data would become lost otherwise. The MemCtrl does\
    \ not offer functionality for differentiation of the storing states in STT-RAM.\n\
    \nTherefore, the STTDDR4Interface got extended with functionalities to track the\
    \ storing state of the data in the page buffer of each bank. The storing state\
    \ of each bank supports the states BUFFER and PERSISTENT. All the banks during\
    \ startup are initialized with PERSISTENT, which indicates the page buffer data\
    \ to be saved to the persistent memory array. Moreover, the state BUFFER proves\
    \ the bank to be not saved to the bank's persistent memory array. Also, the last\
    \ row accessed will be saved in each bank and updated during each bank activation.\
    \ In order to change the storing state of a bank, or all banks, to PERSISTENT,\
    \ a store operation needs to be performed. This can be triggered by an ACT\\_ST\
    \ command, a REF command, or an SREF command. Within a REF or SREF execution,\
    \ store operations on all banks in storing state BUFFER will be performed. When\
    \ there are no banks in storing state BUFFER, the commands REF and SREF are not\
    \ effective.\n\nThe selection between the ACT or ACT\\_ST command is done in the\
    \ activated bank method with a sequence of procedure calls as Algorithm 1. The\
    \ requested row is compared\n\nto the last accessed row of the particular bank.\
    \ When the last accessed data is still the working data, which means the last\
    \ access row and requested row are identical, there is no need for a store operation.\
    \ In this case, a normal ACT command will be simulated. If the requested row differs\
    \ from the last accessed row, and the bank is in the storing state \"BUFFER\"\
    , an ACT\\_ST command will be pushed to simulate. The difference in simulating\
    \ ACT or ACT\\_ST is implemented in a higher delay for the additional store operation,\
    \ which is known as the store time (tST). This store procedure call impacts the\
    \ energy consumption that can be calculated in the power library. The bank's storing\
    \ state can be changed to \"PERSISTENT\" by performing a store operation on the\
    \ particular bank, or on all banks. The ACT command is simulated as with DRAM\
    \ memory in gem5: The ACT command is saved in a list of pending commands which\
    \ is handed over to the modified DRAMPower tool [40], which is part of gem5 and\
    \ performs energy calculations from the gem5 inputs.\n\n**Extensions to DRAMPower\
    \ Model:** To be able to include the ACT\\_ST command, DRAMPower got extended\
    \ by the command and energy calculation. The extensions to DRAMPower are presented\
    \ in Algorithm 2. These extensions include functionality for counting the number\
    \ of store procedures during runtime and calculating the resulting store energy\
    \ and power.\n\nMoreover, the energy calculation in gem5 is not updated on every\
    \ new command execution, but on specific simulation phases: suspension, end of\
    \ a refresh (REF) command, triggered from the gem5 system configuration script\
    \ or by the internal command line of the full system simulation using the m5 utility\
    \ (the gem5 utility used in full system disk images).\n\nFurthermore, the gem5\
    \ statistic output has been modified to include the store energy per rank and\
    \ power state time per rank in the simulation results. In this section, we present\
    \ our HOPE framework for an all-integrated STT-RAM with a gem5 simulator using\
    \ the memory controller. This helps in exploiting system-level meta-heuristics\
    \ that include power consumption, memory utilization, heat patterns, etc.\n\n\
    #### *C. HOPE CONFIGURATION*\n\nThe gem5 being an instruction-level simulator\
    \ enables the integration of different types of memories with a memory controller.\
    \ The memory controller is a component that enables an option of choosing memory\
    \ for system-level analysis. The memory controller has evolved a lot in the\n\n\
    TABLE 2: The Configuration of the Memory cell\n\n| Paramters             | Configuration\
    \                    |\n|-----------------------|----------------------------------|\n\
    | Memory                | 1Gbit x16                        |\n| Organization \
    \         | 8-banks (2-banks per Bank Group) |\n| Latency               | 667MHz\
    \ (1333MT/s)                |\n| Access Time           | 225ps               \
    \             |\n| Supply Voltage - Min: | 1.14 V                           |\n\
    | Supply Voltage - Max: | 1.26 V                           |\n\npast few years.\
    \ Recently, in May 2020 the gem5 introduced a new memory controller (MemCtrl)\
    \ component revision and introduced an NVM interface (NVMInterface) class to the\
    \ gem5, officially integrated into version 20.1 of the gem5 simulator. This NVM\
    \ interface is designed very generic in terms of its functionality and parameters\
    \ to be taken into consideration. The NVMInterface class offers three timing parameters:\
    \ tREAD, tWRITE, and tSEND.\n\nThere is also an already existing DRAM interface\
    \ (DRAMInterface) class. This class of the gem5 contains detailed logic on DRAM\
    \ timing and power state simulation and offers various timing, e.g. tRCD, tCL,\
    \ tRAS, and tREFI, and energy parameters, e.g. IDD0, IDD4R, and VDD. But there\
    \ is no such logic for calculating NVM energy and power consumption. Also, there\
    \ are no energy parameters available for NVMInterface.\n\nThus, to overcome such\
    \ shortcomings, HOPE provides another detailed memory interface targeting STT-RAM.\
    \ This memory targets real-world STT-RAM devices which are designed as STT on\
    \ top of DDR4. Therefore, this interface is named as STTDDR4Interface. This offers\
    \ a high level of detail timing and energy parameters, combined with a power state\
    \ and energy calculation logic. Fig. 2 depicts the detailed flow of our HOPE framework\
    \ within the extended and modified gem5 simulator.\n\nThe simulated system is\
    \ configured using the fs.py system configuration script. Using this script, a\
    \ System instance is set up according to the input values of the HOPE framework.\
    \ We use an X86 architecture-based system. The CPU we defined is the TimingCPU,\
    \ which offers detailed memory timings during simulation. The CPU is also equipped\
    \ with L1 and L2 caches. The system uses a Linux kernel and a disk image with\
    \ an Ubuntu operating system and workloads installed. Within gem5, communication\
    \ between system blocks is done via ports, also as real systems do. The system\
    \ block is connected to the Membus block. The memory bus selected is by default,\
    \ the SystemXBar. All CPU interactions to the main memory are forwarded by the\
    \ Membus to the memory controller. MemCtrl got modified to support STT-RAM connected\
    \ through the memory port.\n\nFig. 2, shows the proposed architecture with the\
    \ STT\\_1333\\_4x16 which is a class created for interfacing STT-RAM with the\
    \ memory controller. It has multiple parameters e.g., tCK, tST, tRCD, IDD0, IDD2N,\
    \ etc that has been extracted from the datasheet of the aforementioned device.\n\
    \nThe tCK is the clock period, depending on the device operating clock frequency\
    \ (fCK) (e.g., fCK = 667MHz results in 1.5ns tCK (=1/fCK)), tST is a special timing\
    \ parameter for STT-RAM and refers to the storing time of the memory (indicates\
    \ the time needed for storing the data from the row address buffer to the persistent\
    \ STT memory). The tST is a newly introduced parameter to gem5 unique to STT,\
    \ which was a missing timing parameter for gem5. The address buffer acts like\
    \ a cache, and the data placed in the cache needs to be written to the main STT\
    \ memory\n\nTABLE 3: Experimental systems configuration for STT-RAM and DRAM\n\
    \n| System elements | Processor                                 | L1 Instr.<br>cache\
    \ | L1 Data<br>cache | L2 cache         | Main Memory                        \
    \                                                            | Clock Speed | Row\
    \ Buffer<br>Size | Device Size | Channel<br>Cappacity | tRCDmin | tRCmin | tRASmin\
    \ | tFAWmin | tRPmin |\n|-----------------|-------------------------------------------|--------------------|------------------|------------------|------------------------------------------------------------------------------------------------|-------------|--------------------|-------------|----------------------|---------|--------|---------|---------|--------|\n\
    | STT-RAM         | 64-bit x86<br>single core,<br>timing 3GHz | Private,<br>32kB\
    \   | Private,<br>32kB | Shared,<br>256kB | 1 channel,<br>2 rank per channel,<br>4\
    \ chips per rank,<br>EMD4E001G16G2,<br>1Gbit x16, 1333MHz | 667MHz      | 256B\
    \               | 128MiB      | 1GiB                 | 135ns   | 44.5ns | 32ns\
    \    | 15ns    | 12.5ns |\n| DRAM            | \"                            \
    \             | \"                  | \"                | \"                |\
    \ 1 channel,<br>2 rank per channel,<br>8 chips per rank,<br>MT40A1G8SA,<br>1Gbit\
    \ x8, 2400MHz     | 1200MHz     | 1kB                | 1GiB        | 16GiB   \
    \             | 12.5ns  | 190ns  | 143ns   | 240ns   | 7.5ns  |\n\nduring the\
    \ Store operation. Therefore, tST is the time needed to process data moving from\
    \ the row address buffer to the STT persistent memory array. Researchers in the\
    \ field could optimize different metrics to minimize this value and evaluate the\
    \ performance of so-called in-memory processing approaches. The parameters such\
    \ as tRCD, IDD0, IDD2N, etc., are standardized DDR4 parameters.\n\nMoreover, this\
    \ interface makes it possible to simulate systems using the latest STT-RAM devices\
    \ including power states and energy consumption as it was never possible before\
    \ in a stand-alone gem5 environment. The integration carried out on the interface\
    \ is based on the parameters offered by the STT-RAM EMD4E001G16G2 from Everspin\
    \ Technologies [39]. These device parameters are shown in Table 2.\n\nAs per the\
    \ physical characteristics of STT-RAM, there are deviations to the DDR4 specification\
    \ for DRAM especially the *Refresh* command, which is mandatory to be issued in\
    \ a time interval tREFI on DRAM, is no longer used in STT-RAM. Therefore, tREFI\
    \ got removed for STT memory. Moreover, the STT-RAM also has a store time parameter\
    \ tST. The store operation of delay tST, is used to move recently written data\
    \ from the page buffer to the persistent memory array.\n\nSome other deviations\
    \ specific to the test devices (simulating the EMD4E001G16G2 device) include the\
    \ memory size that in the case of STT-RAM is a 1 Gbit device, whereas the DDR4\
    \ specification for DRAM only allows devices of 2, 4, 8 and 16 GBit. Furthermore,\
    \ there is also a limit of 667 Mhz for the clock frequency, while the DDR4 Specification\
    \ for DRAM allows 800, 933, 1067, and 1200MHz.\n\n### *D. EVALUATION SETUP CONFIGURATION*\n\
    \nFig. 4 presents a comprehensive overview of the HOPE framework setup configuration\
    \ steps, highlighting its key contributions depicted in blue. gem5 full system\
    \ simulations require a disk image prepared with an operating system and a kernel\
    \ compatible with the chosen operating system. The HOPE framework uses a 1 modified\
    \ Packer SPEC CPU 2017 setup script from the gem5 resources repository for generating\
    \ a disk image containing the Ubuntu operating system and the SPEC CPU 2017 benchmark\
    \ suite for X86 architecture. The 2 benchmark installation is then followed finalized\
    \ by mounting the disk image on the host system. Each benchmark from the disk\
    \ image has been run once for finalizing the benchmark installation, this includes\
    \ com-\n\n![](_page_6_Figure_10.jpeg)\n\nFIGURE 4: Overview of HOPE framework\n\
    \npiling, training, and running the benchmark. In 3 the gem5 full system simulation\
    \ including the HOPE extensions and modifications is run. Therefore the created\
    \ disk image is used. Each simulation runs a selected workload from the prepared\
    \ disk image, includes the creation of checkpoints after the operating system\
    \ boot, and the output of the 4 detailed statistics after the gem5 simulation\
    \ is completed. The gem5 full system simulation includes the introduced STT-RAM\
    \ extension and modifications to DRAMPower to allow detailed energy calculation\
    \ for our STT-RAM device. 5 shows the provided McPAT template file, modified to\
    \ support the extended outputs of gem5. Using the system configuration, simulation\
    \ statistics, and McPAT template, the 6 the GEM5ToMcPAT [41] tool is used to generate\
    \ an input file for later use with McPAT. HOPE includes the enhanced 7 cMcPAT\
    \ power, area, and timing modeling framework. cMcPAT [41] is capable of calculating\
    \ the power parameters of 9. Using the statistics output of gem5, and the power\
    \ model of cMcPAT, the script \"print\\_energy\" [41] is calculating the total\
    \ energy consumption of the simulated environment. The results also combine the\
    \ detailed output of gem5, especially, the instructions count.\n\nThe simulated\
    \ system is configured using the gem5 stdlib (gem5 standard library) based on\
    \ an X86 configuration. Table 3 lists the detailed configuration of the processor,\
    \ cache, and memory for both experimental systems using STT-RAM and DRAM. We selected\
    \ an existing STT-RAM device for simulation and paired it with a widely used DRAM\
    \ device to facilitate a functional comparison. It's\n\nimportant to note that\
    \ our chosen STT-RAM device does not align with JEDEC's JESD79-4A DDR4 standard,\
    \ which limits our ability to select a DDR4-based DRAM device with nearly identical\
    \ parameters.\n\nMoreover, Fig. 5 shows the system configuration used for benchmarks\
    \ using STT-RAM. gem5 offers a general full system default configuration script\
    \ (fs.py), which we used in this work to reflect the system architecture of our\
    \ simulated system. We used the gem5 full system emulation mode to reflect real-world\
    \ systems in the best way provided.\n\nThe operating system selected is an Ubuntu\
    \ 18.04 configured for gem5 and set up with a SPEC CPU 2017 benchmark suite [42]\
    \ instance. The kernel used is the linux kernel version 4.19.83. The gem5 configuration\
    \ script handles the creation of memory controllers and memory devices. The count\
    \ of memory ranks and banks is set in the memory device configuration. The parameters\
    \ used for the STT-RAM device configuration in gem5 are sourced from the EMD4E001G16G2\
    \ datasheet [39]. We performed SPEC CPU 2017 benchmarks on our simulated systems\
    \ using 2 checkpoints per benchmark to be able to perform detailed simulations\
    \ using the TimingCPU from gem5.\n\nThe first checkpoint has been saved after\
    \ the OS boot is finished. The second checkpoint has been saved after the first\
    \ 4.5 billion instructions of benchmark application to ensure the initialization\
    \ phase has been finished and the checkpoint defines a direct jump into the main\
    \ benchmark algorithm. Both checkpoints were performed in a fastforward method\
    \ using gem5's AtomicSimpleCPU. The main simulation run was done from the second\
    \ checkpoint for a total of 2 Billion instructions. This procedure has been performed\
    \ for all of the benchmark applications included in SPEC CPU 2017.\n\n## **IV.\
    \ RESULTS**\n\nIn our research endeavour, our primary focus revolved around conducting\
    \ simulations utilizing the parameters of real-world devices. Specifically, we\
    \ honed in on a selected STT-RAM device, which holds the distinction of being\
    \ DDR4-compatible, albeit with certain deviations from the official standard.\
    \ As we delved into the simulations, we\n\n![](_page_7_Figure_8.jpeg)\n\nFIGURE\
    \ 5: Architecture of the STT-RAM simulated system.\n\nobserved that these deviations\
    \ had a tangible impact on our results, which we duly documented.\n\nOur findings\
    \ shed light on the applicability of HOPE as a potent tool for evaluating the\
    \ feasibility of incorporating STT-RAM main memories into practical systems. By\
    \ harnessing the capabilities of HOPE, we were able to gain insightful glimpses\
    \ into the performance and energy efficiency of the SPEC CPU 2017 benchmarks when\
    \ paired with STT-RAM. This analytical approach not only offers a valuable lens\
    \ to understand the potential of STT-RAM but also opens up new vistas of exploration\
    \ and optimization in memory technology research.\n\nThe gem5 simulator provides\
    \ users with multiple avenues for creating system configurations through its gem5\
    \ standard library of components and functions, including CPUs, memory, boards,\
    \ cache hierarchies, and more. This comprehensive collection of building blocks\
    \ is commonly referred to as the gem5 standard library. For instance, if you wish\
    \ to modify the CPU architecture, you can simply select the corresponding board\
    \ (e.g., X86Board or ArmBoard) and fine-tune the memory configurations accordingly.\
    \ This flexible approach can be applied to alter any available system component.\n\
    \nAlternatively, there is an option to configure a system within gem5 without\
    \ utilizing a predefined board. Instead, you can manually establish connections\
    \ between a CPU and a selected memory device using a memory bus. Furthermore,\
    \ you have the flexibility to augment the CPU with either a straightforward single-level\
    \ cache or a more intricate cache hierarchy to suit your needs.\n\nOur proposed\
    \ framework takes a distinct approach by leveraging an existing system configuration\
    \ known as fs.py. This configuration can be effortlessly modified via command-line\
    \ inputs, enabling rapid adjustments to the system configuration with a single\
    \ bash script edit. Different fs.py configurations are available for various system\
    \ architectures, such as *configs/example/riscv/fs*\\_*linux.py* for RISC-V or\
    \ *configs/example/arm/fs*\\_*bigLITTLE.py* for ARM.\n\nIn our research endeavors,\
    \ we conducted extensive simulations employing both fs.py and custom gem5 system\
    \ configuration scripts. This comprehensive approach allowed us to thoroughly\
    \ assess and analyze our simulations.\n\n## *A. STT-RAM STATE TIME DISTRIBUTION*\n\
    \nAs depicted in Fig. 8(b), the power state times for the IDLE state exhibit variations\
    \ in the case of STT-RAM, contingent upon the specific workload. Notably, STT-RAM\
    \ distinguishes itself by eschewing the need for periodic refreshes to maintain\
    \ data states, leading to a complete absence of time spent in the REF power state.\
    \ Conversely, the bank activation (ACT) time, also illustrated in Fig. 8(b), demonstrates\
    \ only a minor variation within the STT-RAMbased system. This effect can be attributed\
    \ to the relatively prolonged delays observed in the bank activation process,\
    \ especially concerning store state (ACT\\_ST) for STT-RAM,\n\nas visually demonstrated\
    \ in Fig. 8(b). Such insights garnered from our analyses provide valuable perspectives\
    \ on the dynamic behaviour and efficiency of STT-RAM, imparting crucial knowledge\
    \ for potential real-world implementations and optimizations in-memory technologies.\n\
    \n# *B. STT-RAM PERFORMANCE EVALUATION*\n\nFig. 6(a) show the total amount of\
    \ read and write requests which are generally higher in the STT-RAM devices. High\
    \ read and write requests are explained by Fig. 8(a) which shows the hit rate\
    \ for read operations. The hit rates for STT-RAM depend on the application complexity\
    \ level. Fig. 8(c) shows the average read and write bandwidth with a simulated\
    \ STT-RAM. Also, the average latency of STT-RAM, shown in Fig. 6(b), for each\
    \ application highly depends on the hit ratio. As described in our STTDDR4 power\
    \ state machine description, a high alteration of accessed rows negatively affects\
    \ the energy and latency of our simulated STT-RAM device. A high alteration of\
    \ accessed rows further has an impact on store operations.\n\n# *C. STT-RAM POWER\
    \ AND ENERGY BREAKDOWN*\n\nIn Fig. 6 (c and d), we present a comprehensive view\
    \ of the power and energy breakdown for our simulated systems, offering valuable\
    \ insights into their performance characteristics. The shown parameters in d represent\
    \ the accumulated energy for different commands issued to the memory device. As\
    \ shown in Fig. 3, the memory devices move through different states during runtime.\
    \ The parameter \"Activation energy\" therefore shows the total energy consumption\
    \ for all ACT commands initiated. \"Store energy\" accumulates the energy of all\
    \ store operations during the evaluation. In the case of an ACT\\_ST operation,\
    \ the energy accumulated separately for the activation and the store energy results.\
    \ The parameter \"Refresh energy\" is associated to the REF command, while the\
    \ \"RD/WR energy\" is the accumulated energy during the processing of the read\
    \ and write burst accesses.\n\nParticularly noteworthy is the substantial count\
    \ of store operations, which, in conjunction with the notably high IDD0 current\
    \ of 437mA, prominently influences the calculated store energy. As evident from\
    \ the results, STT-RAM stands out for its lack of refresh energy requirements.\
    \ However, it should be noted that the stored energy demands contribute to an\
    \ overall increase in the total energy breakdown. These findings shed light on\
    \ the contrasting energy consumption patterns of STT-RAM compared to conventional\
    \ memory technologies, signifying the potential for more energy-efficient computing\
    \ paradigms. The comprehensive understanding gained from these power and energy\
    \ analyses is essential for devising strategies to optimize memory architectures,\
    \ thus fostering advancements in the realm of energy-efficient computing systems.\
    \ The presented energy parameters in d are not a full view of all calculated energy\
    \ parameters within gem5, but an excerpt of significant values. The full list\
    \ of energy parameters is extended by\n\nparameters for interface energy, standby\
    \ energy, self-refresh energy, power-down and power-up energy.\n\n# *D. DRAM METRICS*\n\
    \nTo maintain the extendibility and versatility of our framework, we have thoughtfully\
    \ retained the interface to the DRAM. This strategic decision allows our framework\
    \ to adapt effortlessly to various memory technologies, rendering it highly versatile\
    \ for a wide array of computing scenarios. In this section, we present an in-depth\
    \ analysis of the extracted data concerning the state time distribution, power\
    \ consumption, energy usage, and latency breakdown for the same applications from\
    \ SPEC 2017, with the DRAM serving as the primary memory. This comprehensive investigation\
    \ is instrumental in understanding the behavior and performance characteristics\
    \ of our framework when interfacing with DRAM.\n\nFig. 9(b) illustrates the state\
    \ time distribution of these applications when utilizing DRAM. Additionally, Fig.\
    \ 7(a) showcases the framework's memory requests. Analyzing memory requests offers\
    \ deeper insights into the applications´ memory access patterns, shedding light\
    \ on potential areas for improvement in terms of data locality and cache utilization.\
    \ Fig. 9(c) provides insight into bandwidth usage with DRAM as the memory. Bandwidth\
    \ utilization is a critical metric for assessing memory system efficiency and\
    \ identifying potential bottlenecks that may impact application performance. Furthermore,\
    \ Fig. 9(a) reveals DRAM row hits, and Fig. 7(b) presents latency per application.\
    \ Finally, Fig. 7(c and d) exhibits DRAM average power and energy usage while\
    \ running the SPEC 2017 applications. These detailed analyses offer valuable insights\
    \ into our framework's performance and its potential for adaptation to future\
    \ memory technologies and diverse computing environments.\n\n## *E. COMPARING\
    \ STT-RAM AND DRAM RESULTS*\n\nBased on our evaluation of SPEC 2017 benchmarks,\
    \ it becomes evident that STT-RAM is not yet ready to replace DRAM-based main\
    \ memories for many applications due to its higher store latency and energy consumption.\
    \ STT-RAM needs storing from the page buffer to the persistent memory array whereas\
    \ DRAM does not need this. Due to the overhead for storing data in the STT persistent\
    \ memory array within the ACT-ST state, the delays are significantly higher than\
    \ in DRAM. Each store takes 380ns extra, in all cases of ACT-ST state. Furthermore,\
    \ STT-RAM is running at 1333 MHz, whereas DRAM is running at 2400 MHz. The impact\
    \ is especially pronounced in applications with high write-to-read ratios like\
    \ Ibm\\_s (see Fig. 6a). Before STT-RAM can be a feasible alternative to DRAM's\
    \ main memories, further technology, and architectural optimizations are necessary\
    \ to reduce the store latency and energy requirements. Fortunately, with the availability\
    \ of HOPE, we now have a systematic means to evaluate and optimize STT-RAM at\
    \ the system level. HOPE presents an invaluable opportunity to drive STT-RAM's\
    \ progress by allowing us to\n\n![](_page_9_Figure_2.jpeg)\n\nexplore and analyze\
    \ various architectural possibilities with greater precision.\n\n## **V. CONCLUSION**\n\
    \nWe presented an extension to the open-source full-system simulator gem5 for\
    \ enabling detailed evaluation of STT-RAM devices in an accurate manner. We have\
    \ shown our implemented power state machine, memory commands, power calculation,\
    \ and statistics output. We have also shown the results of an STT-RAM-based system\
    \ configured using real-world device parameters and compared the resulting metrics\
    \ to a DRAM-based system. The STT-RAM-specific characteristics of required store\
    \ operations and deviations to the DDR4 standard for DRAM have been discussed\
    \ based on the comparison of the simulation results. Our HOPE implementation is\
    \ easily configurable for other STT-RAM devices, by adding timing values, currents,\
    \ and voltages. We will open-source our HOPE framework to fuel research and accelerate\
    \ the development of future system architectures based on STT-RAM.\n\n## **VI.\
    \ ACKNOWLEDGMENT**\n\nThe authors acknowledge TU Wien Bibliothek for financial\
    \ support through its Open Access Funding Programme.\n\n#### **REFERENCES**\n\n\
    - [1] Y. Wang, C. Zhang, H. Yu, and W. Zhang, \"Design of low power 3d hybrid\
    \ memory by non-volatile cbram-crossbar with block-level data-retention,\" in\
    \ Proceedings of the 2012 ACM/IEEE international symposium on Low power electronics\
    \ and design, 2012, pp. 197–202.\n- [2] Y. Shin, \"Non-volatile memory technologies\
    \ for beyond 2010,\" in Digest of Technical Papers. 2005 Symposium on VLSI Circuits,\
    \ 2005. IEEE, 2005, pp. 156–159.\n- [3] B. Gervasi, \"Will carbon nanotube memory\
    \ replace dram?\" IEEE Micro, vol. 39, no. 2, pp. 45–51, 2019.\n- [4] J. Lamb,\
    \ S. Gibbons, R. Trichur, Y. Jiang, K. Mangelson, K. Kremer, and D. Janzen, \"\
    Advancements in microelectronics-grade carbon nanotube materials for nram® device\
    \ manufacture and analysis of carbon nanotube mass in end user devices.\"\n- [5]\
    \ \"Irds 2022 beyond cmos and emerging materials integration,\" Online, 2022,\
    \ accessed on: November 7, 2023. [Online]. Available: https://irds.ieee.org/editions/2022\n\
    - [6] D. Jana, S. Roy, R. Panja, M. Dutta, S. Z. Rahaman, R. Mahapatra, and S.\
    \ Maikap, \"Conductive-bridging random access memory: challenges and opportunity\
    \ for 3d architecture,\" Nanoscale research letters, vol. 10, pp. 1–23, 2015.\n\
    - [7] S. Mittal and J. S. Vetter, \"A survey of software techniques for using\
    \ non-volatile memories for storage and main memory systems,\" IEEE Transactions\
    \ on Parallel and Distributed Systems, vol. 27, no. 5, pp. 1537– 1550, 2015.\n\
    - [8] J. S. Meena, S. M. Sze, U. Chand, and T.-Y. Tseng, \"Overview of emerging\
    \ nonvolatile memory technologies,\" Nanoscale research letters, vol. 9, pp. 1–33,\
    \ 2014.\n- [9] T. Mikolajick, C. Dehm, W. Hartner, I. Kasko, M. Kastner, N. Nagel,\
    \ M. Moert, and C. Mazure, \"Feram technology for high density applications,\"\
    \ Microelectronics Reliability, vol. 41, no. 7, pp. 947–950, 2001.\n- [10] M.\
    \ Imani, S. Patil, and T. Rosing, \"Low power data-aware stt-ram based hybrid\
    \ cache architecture,\" in 2016 17th international symposium on quality electronic\
    \ design (isqed). IEEE, 2016, pp. 88–94.\n- [11] S. Jeloka, Z. Wang, R. Xie, S.\
    \ Khanna, S. Bartling, D. Sylvester, and D. Blaauw, \"Energy efficient adiabatic\
    \ fram with 0.99 pj/bit write for iot applications,\" in 2018 IEEE symposium on\
    \ VLSI circuits. IEEE, 2018, pp. 85–86.\n- [12] M. Moore et al., \"International\
    \ roadmap for devices and systems,\" Accessed: Jan, 2020.\n- [13] I. Yoon, A.\
    \ Anwar, T. Rakshit, and A. Raychowdhury, \"Transfer and online reinforcement\
    \ learning in stt-mram based embedded systems for autonomous drones,\" in 2019\
    \ Design, Automation & Test in Europe Conference & Exhibition (DATE). IEEE, 2019,\
    \ pp. 1489–1494.\n- [14] B. Narasimham, V. Chaudhary, M. Smith, L. Tsau, D. Ball,\
    \ and B. Bhuva, \"Scaling trends in the soft error rate of srams from planar to\
    \ 5-nm finfet,\" in 2021 IEEE International Reliability Physics Symposium (IRPS).\
    \ IEEE, 2021, pp. 1–5.\n- [15] J. Wang, N. Xiu, J. Wu, Y. Chen, Y. Sun, H. Yang,\
    \ V. Narayanan, S. George, and X. Li, \"An 8t/cell fefet-based nonvolatile sram\
    \ with improved density and sub-fj backup and restore energy,\" in 2022 IEEE International\
    \ Symposium on Circuits and Systems (ISCAS), 2022, pp. 3408–3412.\n- [16] J. Y.\
    \ Kim, M.-J. Choi, and H. W. Jang, \"Ferroelectric field effect transistors: Progress\
    \ and perspective,\" APL Materials, vol. 9, no. 2, p. 021102, 02 2021.\n- [17]\
    \ S. Yu, Q. Wang, Y. Zhang, P. Yang, X. Luo, H. Liu, C. Chen, Q. Li, and S. Liu,\
    \ \"Multistate capability improvement of beol compatible fefet by introducing\
    \ an al2o3 interlayer,\" IEEE Transactions on Electron Devices, vol. 70, no. 11,\
    \ pp. 5632–5637, 2023.\n- [18] J. Y. Park, D.-H. Choe, D. H. Lee, G. T. Yu, K.\
    \ Yang, S. H. Kim, G. H. Park, S.-G. Nam, H. J. Lee, S. Jo, B. J. Kuh, D. Ha,\
    \ Y. Kim, J. Heo, and M. H. Park, \"Revival of ferroelectric memories based on\
    \ emerging fluoritestructured ferroelectrics,\" Advanced Materials, vol. 35, no.\
    \ 43, p. 2204904, 2023.\n- [19] S. Seyedfaraji, J. T. Daryani, M. M. S. Aly, and\
    \ S. Rehman, \"Extent: Enabling approximation-oriented energy efficient stt-ram\
    \ write circuit,\" IEEE Access, vol. 10, pp. 82 144–82 155, 2022.\n- [20] S. M.\
    \ Nair, R. Bishnoi, A. Vijayan, and M. B. Tahoori, \"Dynamic faults based hardware\
    \ trojan design in stt-mram,\" in 2020 Design, Automation & Test in Europe Conference\
    \ & Exhibition (DATE). IEEE, 2020, pp. 933–938.\n- [21] R. Bishnoi, M. Ebrahimi,\
    \ F. Oboril, and M. B. Tahoori, \"Improving write performance for stt-mram,\"\
    \ IEEE Transactions on Magnetics, vol. 52, no. 8, pp. 1–11, 2016.\n- [22] S. Swami\
    \ and K. Mohanram, \"Reliable nonvolatile memories: Techniques and measures,\"\
    \ IEEE Design & Test, vol. 34, no. 3, pp. 31–41, 2017.\n- [23] S. Seyedfaraji,\
    \ A. M. Hajisadeghi, J. Talafy, and H. R. Zarandi, \"Dysco: Dynamic stepper current\
    \ injector to improve write performance in stt-ram memories,\" Microprocessors\
    \ and Microsystems, vol. 73, p. 102963, 2020.\n- [24] E. Garzon, R. De Rose, F.\
    \ Crupi, L. Trojman, G. Finocchio, M. Carpentieri, and M. Lanuzza, \"Assessment\
    \ of stt-mrams based on double-barrier mtjs for cache applications by means of\
    \ a device-to-system level simulation framework,\" Integration, vol. 71, pp. 56–69,\
    \ 2020.\n- [25] R. Saha, Y. P. Pundir, and P. K. Pal, \"Design of an area and\
    \ energyefficient last-level cache memory using stt-mram,\" Journal of Magnetism\
    \ and Magnetic Materials, vol. 529, p. 167882, 2021.\n- [26] E. Cheshmikhani,\
    \ H. Farbeh, and H. Asadi, \"3rset: Read disturbance rate reduction in stt-mram\
    \ caches by selective tag comparison,\" IEEE Transactions on Computers, vol. 71,\
    \ no. 6, pp. 1305–1319, 2021.\n- [27] ——, \"Robin: Incremental oblique interleaved\
    \ ecc for reliability improvement in stt-mram caches,\" in Proceedings of the\
    \ 24th Asia and South Pacific Design Automation Conference, 2019, pp. 173–178.\n\
    - [28] N. Mahdavi, F. Razaghian, and H. Farbeh, \"Data block manipulation for\
    \ error rate reduction in stt-mram based main memory,\" The Journal of Supercomputing,\
    \ vol. 78, no. 11, pp. 13 342–13 372, 2022.\n- [29] E. Kültürsay, M. Kandemir,\
    \ A. Sivasubramaniam, and O. Mutlu, \"Evaluating stt-ram as an energy-efficient\
    \ main memory alternative,\" in 2013 IEEE International Symposium on Performance\
    \ Analysis of Systems and Software (ISPASS), 2013, pp. 256–267.\n- [30] D. Sanchez\
    \ and C. Kozyrakis, \"Zsim: Fast and accurate microarchitectural simulation of\
    \ thousand-core systems,\" ACM SIGARCH Computer architecture news, vol. 41, no.\
    \ 3, pp. 475–486, 2013.\n- [31] N. Binkert, B. Beckmann, G. Black, S. K. Reinhardt,\
    \ A. Saidi, A. Basu, J. Hestness, D. R. Hower, T. Krishna, S. Sardashti et al.,\
    \ \"The gem5 simulator,\" ACM SIGARCH computer architecture news, vol. 39, no.\
    \ 2, pp. 1–7, 2011.\n- [32] A. Hansson, N. Agarwal, A. Kolli, T. Wenisch, and\
    \ A. N. Udipi, \"Simulating dram controllers for future system architecture exploration,\"\
    \ in 2014 IEEE International Symposium on Performance Analysis of Systems and\
    \ Software (ISPASS). IEEE, 2014, pp. 201–210.\n- [33] A. Gebregiorgis, L. Wu,\
    \ C. Münch, S. Rao, M. B. Tahoori, and S. Hamdioui, \"Special session: Stt-mrams:\
    \ Technology, design and test,\" in 2022 IEEE 40th VLSI Test Symposium (VTS).\
    \ IEEE, 2022, pp. 1–10.\n- [34] X. Dong, C. Xu, Y. Xie, and N. P. Jouppi, \"Nvsim:\
    \ A circuit-level performance, energy, and area model for emerging nonvolatile\
    \ memory,\" IEEE Transactions on Computer-Aided Design of Integrated Circuits\
    \ and Systems, vol. 31, no. 7, pp. 994–1007, 2012.\n- [35] M. Poremba and Y. Xie,\
    \ \"Nvmain: An architectural-level main memory simulator for emerging non-volatile\
    \ memories,\" in 2012 IEEE Computer Society Annual Symposium on VLSI. IEEE, 2012,\
    \ pp. 392–397.\n- [36] M. Poremba, T. Zhang, and Y. Xie, \"Nvmain 2.0: A user-friendly\
    \ memory simulator to model (non-) volatile memory systems,\" IEEE Computer Architecture\
    \ Letters, vol. 14, no. 2, pp. 140–143, 2015.\n- [37] J. L. NMeza, Justin and\
    \ O. Mutlu., \"Evaluating row buffer locality in future non-volatile main memories,\"\
    \ in arXiv preprint arXiv:1812.06377. arXiv, 2018.\n- [38] S. Chung, K.-M. Rho,\
    \ S.-D. Kim, H.-J. Suh, D.-J. Kim, H.-J. Kim, S.-H. Lee, J.-H. Park, H.-M. Hwang,\
    \ S.-M. Hwang, J.-Y. Lee, Y.-B. An, J.-U. Yi, Y.-H. Seo, D.-H. Jung, M.-S. Lee,\
    \ S.-H. Cho, J.-N. Kim, G.-J. Park, G. Jin, A. Driskill-Smith, V. Nikitin, A.\
    \ Ong, X. Tang, Y. Kim, J.-S. Rho, S.-K. Park, S.-W. Chung, J.-G. Jeong, and S.-J.\
    \ Hong, \"Fully integrated 54nm stt-ram with the smallest bit cell dimension for\
    \ high density memory pplication,\" in 2010 International Electron Devices Meeting,\
    \ 2010, pp. 12.7.1–12.7.4.\n- [39] \"1 gb non-volatile st-ddr4 spin-transfer torque\
    \ mram.\" [Online]. Available: https://shorturl.at/bgsMR\n- [40] K. Chandrasekar,\
    \ C. Weis, Y. Li, B. Akesson, N. Wehn, and K. Goossens, \"Drampower: Open-source\
    \ dram power & energy estimation tool,\" [URL:http://www.](http://www/) drampower.\
    \ info, vol. 22, 2012.\n- [41] A. Brokalakis, N. Tampouratzis, A. Nikitakis, I.\
    \ Papaefstathiou, S. Andrianakis, D. Pau, E. Plebani, M. Paracchini, M. Marcon,\
    \ I. Sourdis, P. R. Geethakumari, M. C. Palacios, M. A. Anton, and A. Szasz, \"\
    Cossim: An open-source integrated solution to address the simulator gap for systems\
    \ of systems,\" in 2018 21st Euromicro Conference on Digital System Design (DSD),\
    \ 2018, pp. 115–120.\n- [42] [Online]. Available: [https://www.S](http://www.spec.org/cpu2017)PEC.o[rg/cpu2017](http://www.spec.org/cpu2017)\n\
    \n![](_page_11_Picture_11.jpeg)\n\nSAEED SEYEDFARAJI is a Graduate Student Member,\
    \ IEEE, and holds a B.Sc. degree from Isfahan University of Technology in Isfahan,\
    \ Iran, and an M.Sc. degree from Amirkabir University of Technology (Tehran Polytechnique)\
    \ in Tehran, Iran. Currently, he is pursuing a Ph.D. in computer engineering at\
    \ the Technische Universität Wien (TU Wien), Austria, where he also serves as\
    \ a University Assistant. His research interests encompass emerging non-volatile\
    \ memory\n\ntechnologies, in-memory processing, the integration of intelligence\
    \ into hardware, and system-on-chip design. Notably, he received the Design Automation\
    \ Conference 2020 Young Fellow (DAC YF 2020) Prize and was a part of the Best\
    \ Team at DAC YF 2020.\n\n![](_page_11_Picture_14.jpeg)\n\nMARKUS BICHL is currently\
    \ with the Technische Universität Wien (TU Wien), Faculty of Electrical Engineering\
    \ and Information Technology (ETIT) as a student. He started his studies with\
    \ Technische Universität Wien (TU Wien), Faculty of Informatics, Bachelore's Programme\
    \ of Computer Science in 2016. He is pursuing his Master's degree in Electrical\
    \ Engineering in the Master's Programme Embedded Systems. His main research interests\
    \ include Emerging\n\nMemory Technologies, low-power computing, FPGA development,\
    \ ASIC design, and cyber-physical systems. Besides his studies, he is working\
    \ on industry-leading electrical powertrains for the automotive industry, with\
    \ hundreds of thousands of units already produced. His passion is to work further\
    \ on Embedded Systems topics and gain a professional career in research.\n\n![](_page_11_Picture_17.jpeg)\n\
    \nASAD AFTAB is a Graduate Student Member, IEEE, and received the B.S. degree\
    \ in Computer Systems engineering from University of Engineering and Technology\
    \ (UET), Peshawar, in 2017 and the M.S. degree in Electrical (Telecommunication\
    \ and Computer Networks) engineering from the National University of Sciences\
    \ and Technology (NUST), Islamabad, in 2021. He is currently pursuing a Ph.D.\
    \ in Electrical engineering at the Technische Universität Wien (TU\n\nWien), Austria.\
    \ His research interests encompass designing both hardware and software-based\
    \ sustainable security techniques for autonomous CPS, which includes researching\
    \ suitable ML algorithms for defence, analyzing various adversarial attacks, and\
    \ exploring innovative defence methods to enhance the resilience of machine learning\
    \ algorithms.\n\n![](_page_11_Picture_20.jpeg)\n\nSEMEEN REHMAN is currently with\
    \ the Technische Universität Wien (TU Wien), as an Assistant Professor. In October\
    \ 2020, she received her Habilitation Degree in the area of Embedded Systems from\
    \ the Faculty of Electrical Engineering and Information Technology, TU Wien. She\
    \ has co-authored one book, multiple book chapters, and more than 60+ publications\
    \ in premier journals and conferences. Her main research interests include dependable\
    \ systems and energy-efficient\n\nembedded system, approximate computing, security,\
    \ IoT/CPS. She has received the CODES+ISSS 2011 and 2015 Best Paper Awards, DATE\
    \ 2017 Best Paper Award Nomination, HiPEAC Paper Awards, DAC Richard Newton Young\
    \ Student Fellow Award, and Research Student Award at the KIT. She served as the\
    \ Topic Track Chair and co-chair at the DATE and ISVLSI conferences from 2020\
    \ and 2023, and has served as the TPC of multiple premier conferences on design\
    \ automation and embedded systems."
- title: 'TOP: Towards Open & Predictable Heterogeneous SoCs'
  abstract: ''
  keywords: '*—Heterogeneous SoC, Cyber-Physical-Systems, Timing Predictable Architectures,
    Open-Source Hardware.'
  document: "# TOP: Towards Open & Predictable Heterogeneous SoCs\n\nLuca Valente,\
    \ Francesco Restuccia, Davide Rossi, *Member, IEEE* Ryan Kastner, *Fellow, IEEE*\
    \ Luca Benini, *Fellow, IEEE*\n\n**Abstract**—Ensuring predictability in modern\
    \ real-time Systems-on-Chip (SoCs) is an increasingly critical concern for many\
    \ application domains such as automotive, robotics, and industrial automation.\
    \ An effective approach involves the modeling and development of hardware components,\
    \ such as interconnects and shared memory resources, to evaluate or enforce their\
    \ deterministic behavior. Unfortunately, these IPs are often closed-source, and\
    \ these studies are limited to the single modules that must later be integrated\
    \ with third-party IPs in more complex SoCs, hindering the precision and scope\
    \ of modeling and compromising the overall predictability. With the coming-of-age\
    \ of open-source instruction set architectures (RISC-V) and hardware, major opportunities\
    \ for changing this status quo are emerging. This study introduces an innovative\
    \ methodology for modeling and analyzing State-of-the-Art (SoA) open-source SoCs\
    \ for low-power cyber-physical systems. Our approach models and analyzes the entire\
    \ set of open-source IPs within these SoCs and then provides a comprehensive analysis\
    \ of the entire architecture. We validate this methodology on a sample heterogenous\
    \ low-power RISC-V architecture through RTL simulation and FPGA implementation,\
    \ minimizing pessimism in bounding the service time of transactions crossing the\
    \ architecture between 28% and 1%, which is considerably lower when compared to\
    \ similar SoA works.\n\n✦\n\n**Index Terms**—Heterogeneous SoC, Cyber-Physical-Systems,\
    \ Timing Predictable Architectures, Open-Source Hardware.\n\n# **1 INTRODUCTION**\n\
    \nThe exponential growth of cyber-physical systems (CPS) (e.g., self-driving cars,\
    \ autonomous robots, ...) and related applications has been fueled by the increase\
    \ in computational capabilities of heterogeneous low-power Systems-on-Chip (SoCs).\
    \ These SoCs are complex computing platforms composed of a set of different hardware\
    \ computing units (e.g., CPUs, hardware accelerators), each tailored to a specific\
    \ target application, sharing a set of resources (memory, sensors) through interconnects\
    \ [\\[1\\]](#page-12-0)–[\\[5\\]](#page-12-1). While integrating multiple computing\
    \ units on the same platform has enabled efficient scale-up of computational capabilities,\
    \ it also poses significant challenges when it comes to assessing their *timing\
    \ predictability*, which is a requirement for CPSs dealing with real-time and\
    \ safety-critical applications: the primary challenge arises from resource contentions\
    \ that emerge when multiple active agents within the SoC must access the same\
    \ shared resources [\\[1\\]](#page-12-0)–[\\[7\\]](#page-12-2).\n\nNumerous research\
    \ efforts have focused on enhancing the timing predictability of heterogeneous\
    \ Systems-on-Chip (SoCs). This includes safely upper bounding execution times\
    \ for data transfers [\\[8\\]](#page-13-0)–[\\[10\\]](#page-13-1) or the deadline\
    \ miss ratio for critical tasks [\\[1\\]](#page-12-0)–[\\[3\\]](#page-12-3), with\
    \ the smallest possible pessimism. These efforts have predominantly focused on\
    \ modeling and analyzing commercial DDR protocols [\\[8\\]](#page-13-0), memory\
    \ IPs [\\[11\\]](#page-13-2), and memory controllers [\\[12\\]](#page-13-3), but\
    \ also predictable interconnects [\\[1\\]](#page-12-0), [\\[4\\]](#page-12-4)\
    \ and on-chip communication protocols [\\[13\\]](#page-13-4). Regrettably, despite\
    \ their value, these studies are scattered, with each one focusing on only one\
    \ of these resources at a time, resulting in being overly pessimistic [\\[5\\\
    ]](#page-12-1).\n\nModeling and analysis of communication protocols are done speculatively\
    \ on abstract models, thus reducing their real-world applicability. Recent works\
    \ for modeling and analysis of IPs (memories, memory controllers, interconnect,\
    \ etc.) have to address the unavailability of cycle-accurate RTL descriptions.\
    \ Many of these IPs are either entirely closed-source [\\[8\\]](#page-13-0) or\
    \ provide loosely-timed behavioral models [\\[5\\]](#page-12-1), [\\[12\\]](#page-13-3)\
    \ or just µarchitectural descriptions [\\[1\\]](#page-12-0), [\\[3\\]](#page-12-3),\
    \ [\\[4\\]](#page-12-4). In essence, the fragmented and proprietary nature of\
    \ commercial and research IPs restricts studies to the particular IP, greatly\
    \ reducing the accuracy achievable through systemlevel analysis. For example,\
    \ Restuccia et al. in [\\[9\\]](#page-13-5) bound the access times of multiple\
    \ initiators on FPGA reading and writing from/to the shared DDR memory. The proposed\
    \ upper bounds' pessimism is between 50% and 90%: even though they finely modeled\
    \ and analyzed the proprietary interconnect, the authors did not have access to\
    \ its RTL nor to the memory controller and IP. The same applies to Ditty [\\[10\\\
    ]](#page-13-1), which is a predictable cache coherence mechanism. In Ditty, even\
    \ though the caches' timing is finely modeled, the overall execution time can\
    \ be up to 3× bigger than the theoretical upper bounds, as the authors did not\
    \ model other components. Another example is AXI-ICRT [\\[1\\]](#page-12-0), an\
    \ advanced AXI interconnect with a sophisticated scheduler which allows transaction\
    \ prioritization based on importance. While proposing a highly advanced interconnect\
    \ with a\n\n<sup>•</sup> *Luca Valente, Luca Benini, and Davide Rossi are with\
    \ the Department of Electrical, Electronic and Information Engineering, University\
    \ of Bologna, 40136 Bologna, Italy. Luca Benini is also with the Integrated Systems\
    \ Laboratory (IIS), ETH Z ¨urich, 8092 Z ¨urich, Switzerland.*\n\n<sup>•</sup>\
    \ *Francesco Restuccia and Ryan Kastner are with the Computer Science and Engineering,\
    \ University of California at San Diego, San Diego, CA 92093 USA.*\n\n*This work\
    \ was supported by Technology Innovation Institute, Secure Systems Research Center,\
    \ Abu Dhabi, UAE, PO Box: 9639, by the Spoke 1 on Future HPC of the Italian Research\
    \ Center on High-Performance Computing, Big Data and Quantum Computing (ICSC)\
    \ funded by MUR Mission 4 - Next Generation EU, and by the European Project EuroHPC\
    \ JU The European Pilot (g.a. 101034126), and by KDT TRISTAN project (g.a.101095947).*\n\
    \n<span id=\"page-1-0\"></span>![](_page_1_Figure_0.jpeg)\n\nFig. 1: Proposed\
    \ methodology.\n\ntightly coupled model, the authors do not extend the model to\
    \ the other components of the SoC, even when assessing the deadline miss ratio\
    \ and benchmarking the architecture.\n\nThe emergence of open-source hardware\
    \ creates a major opportunity for building accurate end-to-end models for realtime\
    \ analysis of cutting-edge heterogeneous low-power SoCs [\\[14\\]](#page-13-6)–[\\\
    [16\\]](#page-13-7): the openness of the IPs allows for cycle-accurate analysis\
    \ of the whole architecture from the interconnects to the shared resources. Yet,\
    \ investigations and successful demonstrations in this direction are still scarce,\
    \ primarily because open hardware has only very recently reached the maturity\
    \ and completeness levels required to build full heterogeneous SoCs [\\[17\\]](#page-13-8).\
    \ In this context, this is the first work to bridge the gap between open-source\
    \ hardware and timing analysis, demonstrating a methodology that successfully\
    \ exploits the availability of the source code to provide finegrained upper bounds\
    \ of the system-level data transfers. We leverage a set of open-source IPs from\
    \ the PULP family, one of the most popular open-hardware platforms proposed by\
    \ the research community [\\[14\\]](#page-13-6), [\\[18\\]](#page-13-9).\n\nFigure\
    \ [1](#page-1-0) shows the proposed methodology, highlighting the novel contributions\
    \ in yellow. It consists of (i) a model for standalone IPs composing modern heterogeneous\
    \ lowpower SoCs, (ii) a static analysis of the RTL code of such components, and\
    \ (iii) a compositional mathematical analysis of the whole system to upper bound\
    \ the response time of the interactions between managers (initiators) and shared\
    \ subordinates (targets), considering the maximum interference generated by the\
    \ interfering managers. Figure [1](#page-1-0) highlights the differences between\
    \ the proposed methodology and previous studies also based on a static and compositional\
    \ approach [\\[5\\]](#page-12-1), [\\[7\\]](#page-12-2), [\\[9\\]](#page-13-5).\
    \ While previous works typically focus on one IP at a time [\\[9\\]](#page-13-5),\
    \ or rely on loosely-timed models [\\[5\\]](#page-12-1), thereby limiting the\
    \ overall accuracy, our approach is the first to model and analyze all the IPs\
    \ directly from the RTL source code to build a holistic system-level analysis.\
    \ This limits the proposed upper bounds' pessimism between 28% and just 1%, in\
    \ isolation and under interference, which is considerably lower when compared\
    \ to similar SoA works for closedsource or loosely-timed platforms [\\[1\\]](#page-12-0)–[\\\
    [4\\]](#page-12-4), [\\[8\\]](#page-13-0), [\\[10\\]](#page-13-1), as better detailed\
    \ in Section [7.](#page-11-0) We demonstrate our methodology on a completely open-source\
    \ prototype of a heterogeneous lowpower open-source SoC for embedded systems composed\
    \ of a Linux-capable host core, a parallel accelerator, a set of IOs, and on-chip\
    \ and off-chip memories. the system-level analysis of the architecture. Finally,\
    \ Section 2-banks 512KiB SPM\n\nThe manuscript is organized as follows: Section\
    \ [2](#page-1-1) presents the target open-source RISC-V-based SoC architecture,\
    \ and Section [3](#page-2-0) discusses the model we apply to its different components.\
    \ Section [4](#page-3-0) analyzes the components to specialize the generic model\
    \ to each of them, and Section [5](#page-7-0) provides\n\n<span id=\"page-1-2\"\
    ></span>![](_page_1_Figure_6.jpeg)\n\nFig. 2: Sample architecture.\n\n[6](#page-9-0)\
    \ validates the results with cycle-accurate experiments (on simulation and FPGA),\
    \ Section [7](#page-11-0) compares this work with the SoA. Section [8](#page-12-5)\
    \ concludes the manuscript.\n\n# <span id=\"page-1-1\"></span>**2 ARCHITECTURE**\n\
    \nFig. [2](#page-1-2) shows the architectural template we target. It also shows\
    \ the four classes of hardware modules we identify in the architecture under analysis,\
    \ namely (i) *controllers*, (ii) the main *crossbar*, (iii) *bridges*, and (iv)\
    \ *peripherals*, which we model in the next Section. The architecture leverages\
    \ a set of fully open-source PULP IPs [\\[18\\]](#page-13-9). It is based on Cheshire\
    \ [\\[15\\]](#page-13-10), an open-source host platform consisting of an RV64\
    \ Linuxcapable CPU, a set of commodity IOs (SPI, SDIO, UART, ...), and an AXI-based\
    \ crossbar with a configurable number of subordinate and manager ports for easy\
    \ integration of accelerators and resources. Our platform includes a parallel\
    \ accelerator and a low-power lightweight HyperBUS memory controller [\\[19\\\
    ]](#page-13-11), connected to the crossbar.\n\nThe host CPU is CVA6 [\\[20\\]](#page-13-12),\
    \ which is a six stages, singleissue, in-order, 64-bit Linux-capable RISC-V core,\
    \ supporting the RV64GC ISA variant, SV39 virtual memory with a dedicated Memory\
    \ Management Unit (MMU), three levels of privilege (Machine, Supervisor, User),\
    \ and PMP [\\[21\\]](#page-13-13). CVA6 features private L1 instruction and caches,\
    \ operating in parallel, with the latter being able to issue multiple transactions.\
    \ When needed, CVA6 can offload computationintensive tasks to the parallel hardware\
    \ accelerator, the socalled PULP cluster [\\[22\\]](#page-13-14). It is built\
    \ around 8 CV32E4-based cores [\\[23\\]](#page-13-15) sharing 16×8 kB SRAM banks,\
    \ composing a 128 kB L1 Scratchpad Memory (SPM). The cluster features a DMA to\
    \ perform data transfers between the private L1SPM and the main memory: data movement\
    \ is performed via softwareprogrammed DMA transfers. Once the data are available\
    \ inside the L1SPM, the accelerator starts the computation.\n\nCVA6 and the cluster\
    \ are the managers of the systems connected to the main AXI crossbar [\\[24\\\
    ]](#page-13-16), which routes their requests to the desired subordinates according\
    \ to the memory map. A manager can access any subordinate in the system. The main\
    \ subordinates of the systems are, respectively, (i) the on-chip SRAM memory,\
    \ (ii) the IO subsystem, and (iii) the off-chip main memory with a tightly coupled\
    \ Last Level Cache (LLC). The on-chip memory is used for low-latency, high-bandwidth\
    \ data storage. The APB subsystem is used to communicate with off-chip sensors\
    \ or memories through the\n\ncommodity IOs. The off-chip main memory is where\
    \ the code and the shared data are stored. Differently from high-end embedded\
    \ systems relying on relatively power-hungry and expensive DDR3/4/5 memories,\
    \ the platform under analysis adopts HyperRAMs as off-chip main memory, which\
    \ are fully-digital low-power small-area DRAMs with less than 14 IO pins and that\
    \ provide enough capacity to boot Linux [\\[16\\]](#page-13-7) and bandwidth for\
    \ IoT applications [\\[19\\]](#page-13-11), [\\[25\\]](#page-13-17).\n\n# <span\
    \ id=\"page-2-0\"></span>**3 MODEL**\n\nThis section presents the model we construct\
    \ for the different components of our SoC. Our aim is to propose a general model\
    \ that describes the characteristics of the components and that can be re-targeted\
    \ to different IPs and novel architectures, regardless of the number of integrated\
    \ controllers and peripherals. This work is also an effort to provide base support\
    \ to stimulate further studies in predictability improvements and analysis for\
    \ open hardware architectures.\n\n#### **3.1 Communication model**\n\nWe identify\
    \ four classes of hardware modules in the architecture under analysis, shown in\
    \ Fig. [2,](#page-1-2) namely (i) *controllers*, (ii) the main *crossbar*, (iii)\
    \ *bridges*, and (iv) *peripherals*. As the AXI standard is the main communication\
    \ standard used to implement non-coherent on-chip communications [\\[24\\]](#page-13-16),\
    \ we discuss here its main features. It defines a managersubordinate interface\
    \ enabling simultaneous, bi-directional data exchange and multiple outstanding\
    \ transactions. Fig. [3](#page-2-1) shows the AXI channel architecture and information\
    \ flow. Bus transactions are initiated by a *controller* (exporting a manager\
    \ interface), submitting a transaction request to read/write data to/from a subordinate\
    \ interface through AR or AW channels, respectively. A request describes the starting\
    \ target address and a *burst length*. After the request phase, in case of a read,\
    \ data are transmitted through the R channel. In case of a write, data are provided\
    \ by the *controller* to the target *peripheral* through the W channel. Upon completing\
    \ a write transaction, the *peripheral* also sends a beat on the B channel to\
    \ acknowledge the transaction's completion. For multiple in-flight write transactions,\
    \ the standard enforces strict in-order access to the W channel: the data on the\
    \ W channel must be propagated in the same order as the AW channel requests. Even\
    \ though the standard does not require it, many commercial and open-source platforms\
    \ apply the same policy for reads, typically to limit the system's overall complexity,\
    \ as reported in their documentation [\\[26\\]](#page-13-18), [\\[27\\]](#page-13-19).\n\
    \n#### **3.2 Controller model**\n\n*Controllers* have an active role on the bus.\
    \ Each *controller* exports an AXI manager interface, through which it initiates\
    \ requests for bus transactions directed to the *peripherals*. A generic *controller*\
    \ C<sup>i</sup> can be described through two parameters: the maximum number of\
    \ outstanding read/write transactions that it can issue in parallel, denoted with\
    \ ϕ C<sup>i</sup> R/W , and their relative burst length β<sup>i</sup> . While\
    \ our model and analysis can be applied to a generic architecture, the system\
    \ under analysis features as *controllers* a CVA6 core [\\[20\\]](#page-13-12)\
    \ and a cluster accelerator [\\[22\\]](#page-13-14) (see Section [2\\)](#page-1-1).\
    \ Bus transactions issued by the cluster interfere with those issued by CVA6\n\
    \n<span id=\"page-2-1\"></span>\n\n| READ TRANSACTION                        \
    \ |                     |  |  |  |  |\n|------------------------------------------|---------------------|--|--|--|--|\n\
    | 1 AR (ADDR+ LEN)                         | SUBORDINATE         |  |  |  |  |\n\
    | 2 R (DATA + COMPL.)                      | INTERFACE           |  |  |  |  |\n\
    | WRITE TRANSACTION<br>1<br>AW (ADDR+ LEN) |                     |  |  |  |  |\n\
    | W (DATA)<br>2                            | SUBORDINATE         |  |  |  |  |\n\
    |                                          | INTERFACE           |  |  |  |  |\n\
    |                                          | 3<br>B (COMPLETION) |  |  |  |  |\n\
    \nFig. 3: AXI Channel architecture\n\nand vice-versa. CVA6 is assumed to compute\
    \ a critical periodic workload, running on top of a Real-time Operating System\
    \ (RTOS). The PULP cluster executes computationintensive tasks and issues bus\
    \ transactions through its DMA. Contention internal to the PULP cluster has been\
    \ profiled in detail in [\\[28\\]](#page-13-20). However, our analysis provides\
    \ the worstcase data transfer time in accessing the shared *peripherals* to support\
    \ the safe scheduling and execution of critical tasks within their deadline. We\
    \ specifically focus on interference in accessing the shared resources. Modeling\
    \ the internal effects of *controllers*, such as pipeline stalls in the core or\
    \ contention within the accelerator, is beyond the scope of this work.\n\n####\
    \ <span id=\"page-2-2\"></span>**3.3 Peripheral model**\n\n*Peripherals* export\
    \ a *subordinate* interface through which they receive and serve the bus transactions.\
    \ The *peripherals* deployed in the system are heterogeneous. Nonetheless, our\
    \ model offers a set of parameters representative of a generic peripheral, and\
    \ it is not tied to a specific communication protocol. It works as the baseline\
    \ for the analysis of any *peripheral* deployed in the system under analysis.\
    \ The generic *peripheral* P<sup>j</sup> is characterized with two sets of parameters:\
    \ (i) the maximum number of supported outstanding reads (χ P<sup>j</sup> <sup>R</sup>\
    \ ) and write (χ P<sup>j</sup> <sup>W</sup> ) transactions; (ii) the maximum number\
    \ of cycles incurred from the reception of the request to its completion, for\
    \ a read (d P<sup>j</sup> <sup>R</sup> ) and a write (d P<sup>j</sup> <sup>W</sup>\
    \ ) transaction in isolation. d P<sup>j</sup> <sup>R</sup> and d P<sup>j</sup>\
    \ <sup>W</sup> are composed of two contributions: (i) the *data time*, defined\
    \ as the time required for the *peripheral* to send or receive one word of data\
    \ (tDATA) multiplied by the burst length of the transaction in service (βi) and\
    \ (ii) the *control overhead* tCTRL, defined as the maximum time elapsing between\
    \ accepting the request and the availability of the first word of data (reads)\
    \ or availability to receive data (writes). From the previous considerations,\
    \ d P<sup>j</sup> R/W = t P<sup>j</sup> CTRL + t P<sup>j</sup> DATA · β. We define\
    \ two extra parameters ρ <sup>P</sup><sup>j</sup> and θ P<sup>j</sup> . The first\
    \ indicates the level of pipelining in serving multiple transactions. ρ <sup>P</sup><sup>j</sup>\
    \ = 1 means that each stage of P<sup>j</sup> does not stall the previous, and\
    \ transactions are served in a pipelined fashion, while ρ <sup>P</sup><sup>j</sup>\
    \ = 0 indicates that no pipeline is implemented. θ <sup>P</sup><sup>j</sup> =\
    \ 0 indicates that read and write transactions interfere with each other. θ <sup>P</sup><sup>j</sup>\
    \ = 1 indicates that read and write transactions can be handled in parallel by\
    \ P<sup>j</sup> .\n\n#### <span id=\"page-2-3\"></span>**3.4 Main crossbar model**\n\
    \nWe provide here the model of the main *crossbar*, the routing component enabling\
    \ communication among *controller*s and *peripheral*s. Each *controller* has its\
    \ manager port connected to a subordinate port of the *crossbar*. Each *peripheral*\
    \ has its subordinate port connected to a manager port of the *crossbar*. We model\
    \ the *crossbar* R<sup>0</sup> with two sets of parameters: (i) the\n\nmaximum\
    \ amount of outstanding read and write transactions that a subordinate port can\
    \ accept (χ R<sup>0</sup> <sup>R</sup> and χ R<sup>0</sup> <sup>W</sup> , respectively);\
    \ and (ii) the maximum overall latency introduced by R<sup>0</sup> on each read\
    \ (d R<sup>0</sup> <sup>R</sup> ) and write transaction (d R<sup>0</sup> <sup>W</sup>\
    \ ). d R<sup>0</sup> R and d R<sup>0</sup> <sup>W</sup> are composed of two contributions:\
    \ (i) the overall delay introduced by the *crossbar* on a transaction in isolation\
    \ (tPROP); (ii) the maximum time a request is delayed at the arbitration stage\
    \ due to the contention generated by interfering transactions (t R<sup>0</sup>\
    \ CON). From the previous considerations, the propagation latency is modeled as\
    \ d R<sup>0</sup> R/W = t R<sup>0</sup> PROP + t R<sup>0</sup> CON. Such parameters\
    \ depend on the arbitration policies and routing mechanisms, as we investigate\
    \ in detail in Section [4.](#page-3-0)\n\n#### <span id=\"page-3-1\"></span>**3.5\
    \ Bridge model**\n\nBridges export a single manager interface and a single subordinate\
    \ interface. They perform protocol/clock conversion between a *controller* and\
    \ the *crossbar*. Bridges require a certain number of clock cycles to be crossed\
    \ but do not limit the number of in-flight transactions and do not create any\
    \ contention. We model the bridges with two parameters: the overall maximum delay\
    \ introduced over a whole transaction for (a) read (d Q<sup>j</sup> <sup>R</sup>\
    \ ) and (b) write (d Q<sup>j</sup> <sup>W</sup> ) transactions.\n\n# <span id=\"\
    page-3-0\"></span>**4 ANALYSIS OF THE HARDWARE MODULES**\n\nThis Section aims\
    \ to analyze the worst-case behavior of the *peripherals*, *bridges*, and the\
    \ *crossbar* present in the platform under analysis. Our approach is compositional\
    \ – in this Section, we analyze each hardware component separately, specializing\
    \ in the generic models introduced in Section [3,](#page-2-0) and bounding the\
    \ service times at the IP level in isolation. In the next Section, we provide\
    \ an overall worst-case analysis at the system level, in isolation and under interference.\
    \ We define t P<sup>j</sup> CK as the period period of the clock fed to P<sup>j</sup>\
    \ .\n\n#### <span id=\"page-3-4\"></span>**4.1 AXI CDC FIFO queues**\n\nAXI CDC\
    \ FIFOs are leveraged to perform clock-domain crossing between two AXI-based devices.\
    \ The generic AXI CDC FIFO F<sup>i</sup> is a *bridge*: we apply here the model\
    \ presented in Section [3.5.](#page-3-1) It exports a manager interface and a\
    \ subordinate interface. It is composed of five independent CDC FIFOs, each serving\
    \ as a buffer for an AXI channel, having depth D<sup>i</sup> CDC (design parameter\
    \ for the IP under analysis).\n\n#### *4.1.1 RTL IP structure*\n\nFigure [4](#page-4-0)\
    \ shows the block diagram of a CDC FIFO in the platform under analysis. They are\
    \ structured following established clock domain crossing (CDC) principles [\\\
    [24\\]](#page-13-16). The design is split into two parts, the transmitter (TX)\
    \ and the receiver (RX), having different clock domains. TX and RX interface through\
    \ asynchronous signals, namely a counter for data synchronization (synchronized\
    \ with two-stage Flip-Flops (FFs)) and the payload data signal.\n\n#### *4.1.2\
    \ Delays analysis*\n\nAs mentioned earlier, CDC FIFOs are *bridges*: we apply\
    \ the model presented in Section [3.5.](#page-3-1) The CDC FIFO under analysis\
    \ behaves as follows: TX samples the payload data into an FF. In the following\
    \ cycle, the TX counter is updated. The TX\n\ncrossing the CDC FIFO introduces\
    \ a fixed delay of one clock cycle of the TX domain (t CK) and four clock cycles\
    \ of the RX domain (t RX CK). This means that the delay in crossing the CDC FIFO\
    \ is equal to tCDC(t TX CK, tRX CK) = t TX CK + 4 · t RX CK.We leverage this baseline\
    \ delay to build the overall latency introduced by F<sup>i</sup> , interposed\
    \ between a manager (clocked at t C CK) and a subordinate (clocked at t P CK).\n\
    \n*Read transaction:* A read transaction AR<sup>k</sup> is composed of two phases:\
    \ (i) the address propagation phase and (ii) the data phase. This means that F<sup>i</sup>\
    \ is crossed twice to complete ARk: during phase (i), the manager is on the TX\
    \ side, propagating the request. In phase (ii), the subordinate is on the TX side,\
    \ propagating the data. Hence, the propagation latency is tCDC(t C CK, t<sup>P</sup>\
    \ CK) in phase (i) and tCDC(t P CK, t<sup>C</sup> CK) in phase (ii). Adding them\
    \ together, the propagation latency introduced by F<sup>i</sup> on AR<sup>k</sup>\
    \ is equal to:\n\n<span id=\"page-3-2\"></span>\n$$d\\_R^{\\rm CDC} = t\\_{\\\
    rm CDC}(t\\_{\\rm CK}^C, t\\_{\\rm CK}^P) + t\\_{\\rm CDC}(t\\_{\\rm CK}^P, t\\\
    _{\\rm CK}^C) = 5(t\\_{\\rm CK}^C + t\\_{\\rm CK}^P) \\tag{1}$$\n\n*Write transaction:*\
    \ A write transaction is composed of three phases: (i) an address phase (manager\
    \ on the TX side), (ii) a data phase (manager on the TX side), and (iii) a write\
    \ response phase (subordinate on the TX side). Phases (i) and (ii) happen in parallel\
    \ (see [\\[29\\]](#page-13-21) p. 45). Thus, tCDC(t C CK, t<sup>P</sup> CK) is\
    \ incurred for phases (i) and (ii), and tCDC(t P CK, t<sup>C</sup> CK) for phase\
    \ (iii). The delay introduced by F<sup>i</sup> on AW<sup>k</sup> is equal to the\
    \ delay introduced in Equation [1,](#page-3-2) d CDC <sup>W</sup> = d CDC <sup>R</sup>\
    \ .\n\n#### <span id=\"page-3-3\"></span>**4.2 AXI SRAM scratchpad memory (SPM)**\n\
    \nThe AXI SPM is a high-speed, low-latency memory component used for temporary\
    \ data storage – a block design representation is reported in Figure [5.](#page-4-0)\
    \ The SPM memory is a *peripheral*: we apply here the model presented in Section\
    \ [3.3.](#page-2-2)\n\n#### *4.2.1 RTL IP structure*\n\nThe first stage of the\
    \ SPM architecture is represented by a protocol converter (AXI-SRAM-Interface),\
    \ translating the read and write AXI channels into SRAM-compatible transactions.\
    \ Following the converter, an internal demux directs the SRAM transactions to\
    \ the desired SRAM bank, where the data is stored. Each SRAM bank provides two\
    \ independent SRAM ports, one for reads and one for writes, as from the specification\
    \ of industry-standard SRAM resources [\\[30\\]](#page-13-22).\n\n*The AXI-SRAM-Interface*\
    \ is structured in two submodules, independently managing read and write transactions.\
    \ The first stage of each submodule is a FIFO queue (of depth DSPM FIFO) buffering\
    \ the AXI AW or AR channel, respectively. Each submodule features the logic for\
    \ protocol translation, consisting of (i) saving transaction metadata (starting\
    \ address and length) and (ii) producing the output SRAM requests. For writes,\
    \ the incoming data on the W channel are directly propagated towards the banks.\
    \ The logic operating the protocol conversion generates the address for each W\
    \ beat. For reads, the data coming from the SRAM banks are directly driven on\
    \ the R channel. The logic keeps compliance with the AXI standard, adding the\
    \ last signal or generating write responses when required. *The demux* is fully\
    \ combinatorial\n\n<span id=\"page-4-0\"></span>![](_page_4_Figure_1.jpeg)\n\n\
    Fig. 4: CDC FIFO block diagram.\n\nFig. 5: AXI SPM block diagram\n\n0 1 N-1 ...\n\
    \nDual-Port SRAM Interface\n\nBank 1\n\nAR FIFO AW FIFO ADDR & REQ CTRL ADDR GEN\
    \ & CTRL\n\nAR R AW B W\n\nSRAM Interface\n\n0 1 N-1 ... Bank 0\n\nDual-Port SRAM\
    \ Interface\n\nCONVERTER\n\nDEMUXING\n\nBANKS\n\n![](_page_4_Figure_4.jpeg)\n\n\
    Fig. 6: IO subsystem block diagram.\n\nand selects the target bank according to\
    \ the request's address. *The SRAM banks* are technology-specific macros instantiated\
    \ at design time. Each SRAM bank's port exports an enable signal, an address signal,\
    \ and a signal to determine if a transaction is a read or a write. The SRAM interface\
    \ expects simultaneous propagation of data and commands for writes; for reads,\
    \ the data are sent the cycle following the command.\n\n#### *4.2.2 Delays and\
    \ parallelism analysis*\n\n*AXI-SRAM-Interface:* the FIFOs in the converter are\
    \ only in charge of data buffering – each FIFO introduces a fixed delay of one\
    \ clock cycle (t SPM CK ). After the FIFOs, the control logic requires at most\
    \ one clock cycle (t SPM CK ) to set up the propagation of a burst transaction\
    \ – the direct connection over the W and R channels makes the data streaming in\
    \ a pipeline fashion, adding no further latency. At the end of a write transaction,\
    \ the converter takes two clock cycles (2t SPM CK ) to generate the write response:\
    \ one to acknowledge that the last W beat has been accepted and one to provide\
    \ the B response. The same applies to reads, to generate the AXI last signal.\
    \ Summing up the contributions, the control latency introduced by the AXI-SRAM-Interface\
    \ to each transaction is upper bound by 4t SPM CK for both reads and writes.\n\
    \n*Demux:* The demultiplexing is combinatorial: it connects the transaction to\
    \ the SRAM bank in one clock cycle (t SPM CK ).\n\n*Banks:* As by the definition\
    \ of the SRAM interface [\\[30\\]](#page-13-22), an SRAM bank serves one transaction\
    \ per clock cycle, which makes t SPM DATA,R/W = t SPM CK . For write transactions,\
    \ the protocol guarantees that the SRAM bank samples the data in parallel with\
    \ the request (in the same clock cycle). For read transactions, the data are served\
    \ the clock cycle after the bank samples the request. So, it contributes to t\
    \ SPM CTRL,R with one clock cycle (t SPM CK ). Summing up the contributions, the\
    \ service time of the SPM in isolation is upper bound by:\n\n$$t\\_{\\rm{CTRL},W}^{\\\
    rm{SPM}} = 5 \\cdot t\\_{\\rm{CK}}^{\\rm{SPM}}; t\\_{\\rm{CTRL},R}^{\\rm{SPM}}\
    \ = 6 \\cdot t\\_{\\rm{CK}}^{\\rm{SPM}}; t\\_{\\rm{DATA},R/W}^{\\rm{SPM}} = t\\\
    _{\\rm{CK}}^{\\rm{SPM}}; \\tag{2}$$\n\nConsider now the parallelism supported\
    \ by the SPM. The maximum number of accepted outstanding transactions at the SPM\
    \ χ SPM <sup>R</sup> is defined by the depth DSPM FIFO of the input buffers implemented\
    \ in the AXI-SRAM-Interface. Thus,\n\n$$\n\\chi\\_R^{\\text{SPM}} = \\chi\\_W^{\\\
    text{SPM}} = D\\_{\\text{FIFO}}^{\\text{SPM}} \\tag{3}\n$$\n\nThe *SPM* module\
    \ under analysis is aggressively pipelined, operations are executed in one clock\
    \ cycle, and no stall sources are present in the design. Also, as mentioned earlier,\
    \ read and write transactions do not interfere with each other. From the previous\
    \ considerations, ρ SPM = 1 and θ SPM = 1.\n\n#### <span id=\"page-4-1\"></span>**4.3\
    \ IO Subsystem**\n\nSRAM Interface\n\nBank B-1\n\n0 1 N-1 ...\n\nInterface\n\n\
    The IO subsystem is the *peripheral* in charge of writing/reading data to/from\
    \ the off-chip I/Os. We apply here the model presented in Section [3.3.](#page-2-2)\
    \ It is composed of a set of memory-mapped peripheral registers that are accessed\
    \ through a demux and that manage the datapaths issuing the transactions on the\
    \ I/O interfaces (e.g., SPI, I2C, etc.).\n\n#### *4.3.1 RTL IP structure*\n\n\
    Figure [6](#page-4-0) shows the block diagram of the IO subsystem. It is composed\
    \ of an AXI-REG-Interface, a demux, and a set of registers. The first stage of\
    \ the *AXI-REG-Interface* is composed of two FIFOs (of depth DIO FIFO), buffering\
    \ read and write transactions, respectively. After the FIFOs, a round-robin arbiter\
    \ manages read and write transactions, allowing only one at a time to pass to\
    \ the protocol conversion. Since the IO subsystem is meant for low-power reads\
    \ and writes, registers' transactions share the same set of signals for reads\
    \ and writes and are limited to single-word accesses. For such a reason, the IO\
    \ subsystem does not support burst transactions (requests having β<sup>i</sup>\
    \ > 1 are suppressed). *The demux* stage decodes the request and directs it to\
    \ the proper register destination, where it is finally served as a register read\
    \ or write.\n\n#### *4.3.2 Delays and parallelism analysis*\n\nThe IO subsystem\
    \ is a *peripheral*, thus, we apply the model proposed in Section [3.5.](#page-3-1)\
    \ Considering the maximum service delays, overall, the IO subsystem is composed\
    \ of four stages: (i) the FIFOs, (ii) the protocol conversion, (iii) demultiplexing,\
    \ and (iv) target register access. The first three stages, contributing to the\
    \ control overhead, introduce a fixed delay of one clock cycle (t IO CK) each\
    \ for a total of 3 · t IO CK clock cycles. Consider now stage (iv). In the case\
    \ of a write, the request and the corresponding data are propagated in parallel\
    \ in one clock cycle. In the case of a read, the register provides the data in\
    \ the clock cycle following the request – t IO CTRL requires one extra clock cycle.\
    \ Summing all the contributions, the service time of the I/O subsystem is upper\
    \ bounded by:\n\n$$t\\_{\\rm{CTRL,W}}^{IO} = 3 \\cdot t\\_{\\rm{CK}}^{IO}; \\\
    quad t\\_{\\rm{CTRL,R}}^{IO} = 4 \\cdot t\\_{\\rm{CK}}^{IO}; \\quad t\\_{\\rm{DATA,W/R}}^{IO}\
    \ = t\\_{\\rm{CK}}^{IO} \\tag{4}$$\n\nConsider now the parallelism. Similarly\
    \ to the SPM module, the IO subsystem is capable of buffering up to DIO FIFO of\
    \ each type in its input FIFO queues. Thus, the maximum number of outstanding\
    \ transactions supported by the IO subsystem is equal to:\n\n$$\n\\chi\\_W^{\\\
    rm IO} = \\chi\\_R^{\\rm IO} = D\\_{\\rm FIFO}^{\\rm IO} \\tag{5}\n$$\n\nThe IO\
    \ subsystem serves read and write transactions one at a time, and no pipelining\
    \ is implemented among the different stages. This means that ρ IO = 0 and θ SPM\
    \ = 0.\n\n<span id=\"page-5-0\"></span>![](_page_5_Figure_0.jpeg)\n\nFig. 7: Block\
    \ diagrams of the components of the main memory subsystem. (a) LLC block diagram,\
    \ (b) Transaction control flow diagram, (c) Memory controller block diagram.\n\
    \n#### <span id=\"page-5-1\"></span>**4.4 The main memory subsystem**\n\nThe main\
    \ memory subsystem is a *peripheral*: we apply here the model presented in Section\
    \ [3.3.](#page-2-2) It is composed of three macro submodules: (i) the *AXI Last-level\
    \ Cache (LLC)*; (ii) the *HyperRAM memory controller (HMC)*; and (iii) the *HyperRAM\
    \ memory (HRAM)*. It is based on HyperRAM memories leveraging the HyperBUS protocol\
    \ [\\[25\\]](#page-13-17). HyperRAMs are optimized for low-overhead data storage\
    \ while offering up to 3.2Gbps bandwidth. HyperRAMs expose a low pin count, a\
    \ fully digital 8-bit double-data-rate (DDR) interface used for commands and data.\
    \ HyperRAMs serve transactions in order, one at a time, as required by the protocol\
    \ [\\[25\\]](#page-13-17). While a pure in-order strategy is simpler than those\
    \ deployed by high-end commercial memory controllers, it is important to note\
    \ that these controllers are typically complex closedsource IPs, making detailed\
    \ analysis extremely challenging. Notably, our analysis is the first to explore\
    \ this level of detail. Furthermore, the memory subsystem under analysis has shown\
    \ to be effective in tape-outs of Linux-capable chips [\\[16\\]](#page-13-7).\
    \ We model the service times of a single transaction in case of an LLC hit and\
    \ miss. By doing so, we provide upper bounds that can be leveraged by future studies\
    \ focusing on LLC interference between different *controllers* at the application\
    \ level. For example, advanced cache management studies for real-time applications\
    \ (e.g., cache coloring) could leverage the upper bounds provided here to bound\
    \ overall task execution times.\n\n#### *4.4.1 RTL IP structure*\n\n*The AXI Last-Level\
    \ Cache* is the interface of the memory subsystem with the platform. The LLC under\
    \ analysis has configurable cache line length, defined as LWLLC. Figure [7\\(](#page-5-0)a)\
    \ shows the LLC's block diagram, composed of 5 pipelined units: (i) burst splitter,\
    \ (ii) hit-miss detection, (iii) eviction/refill, (iv) data read/write, and (v)\
    \ data ways. Figure [7\\(](#page-5-0)b) shows how these units cooperate to serve\
    \ the requests. The burst splitter buffers and splits the incoming AXI requests\
    \ into multiple sub-requests that have the same length of the cache line, and\
    \ it calculates the tags of the sub-transactions. A βi-word AXI burst request\
    \ is split internally into ⌈ βi LWLLC ⌉ requests of length LWLLC. The tags are\
    \ the input to the hitmiss detection unit, which analyzes them to determine if\
    \ any sub-request will be a (a) hit or (b) miss. In case (a), the transaction\
    \ is directed to the read/write unit: if it is a (a.i) read, the read response\
    \ is generated and immediately sent through the AXI subordinate port, completing\
    \ the transaction. In the case of a (a.ii) write, the locally cached value is\
    \ updated, and a write response is generated and sent back to the\n\nAXI interface\
    \ to complete the transaction. In case (b), the transaction is submitted to the\
    \ eviction/refill unit. Refill is performed on every miss and consists of issuing\
    \ a read to the memory controller to fetch the missing data and update the data\
    \ way. Eviction is performed when a cache set is full to free the necessary spot\
    \ before a refill. A Least Recently Used (LRU) algorithm is used in the module\
    \ under analysis.\n\n*The HyperRAM memory controller* [\\[31\\]](#page-13-23)\
    \ is depicted in Figure [7\\(](#page-5-0)c). It consists of two tightly coupled\
    \ modules working in two separated frequency domains: (i) the AXI *front-end*\
    \ and (ii) the *back-end* PHY controller. The front-end handles and converts the\
    \ AXI transactions into data packets for the PHY controller; it runs at the same\
    \ clock as the LLC (t HMC CK ). The back-end features a Finite State Machine (FSM)\
    \ to send/receive the data packets and keep compliance with the HyperBUS protocol\
    \ timings and data flow; it runs at the same clock as the HyperRAMs (t HRAM CK\
    \ ). The back-end handles two off-chip HyperRAMs in parallel, configured with\
    \ interleaved addresses. As each HyperRAM arranges data as 16-bit words, the word\
    \ size of the back-end is DWHYPER = 32 bits.\n\nThe first stage of the front-end\
    \ is composed of two FIFOs buffering incoming AXI read and write requests. Then,\
    \ a serializer solves conflicts among reads and writes, allowing only one AW or\
    \ AR request at a time. Following, three modules translate between AXI and the\
    \ back-end protocol: (i) AXTOPHY, translating the AXI AW or AR requests into commands\
    \ for the back-end; (ii) PHYTOR converting the data words from the back-end into\
    \ AXI read beats for the AXI interface; and (iii) WTOPHY, converting AXI W data\
    \ beats into data words and generating write response at the end of the transaction.\
    \ Three CDC FIFOs are deployed between the AXTOPHY, WTOPHY, and PHYTOR and the\
    \ back-end. The back-end deploys an internal FSM arranging the requests coming\
    \ from the front-end into 48-bit vector requests, as required in the HyperBUS\
    \ protocol, and propagating the data packets to/from the two physical HyperRAM\
    \ memories through two *transceivers* (TRX).\n\n*The HyperRAM memory* is an off-chip\
    \ memory IP [\\[25\\]](#page-13-17). It is provided with a cycle-accurate model,\
    \ fundamental for our analysis purposes [\\[32\\]](#page-13-24). Each HyperRAM\
    \ is organized as an array of 16-bit words and supports one outstanding burst\
    \ transaction, up to 1kB long. As two HyperRAM are interleaved, the overall burst\
    \ can be up to 2kB long [\\[19\\]](#page-13-11).\n\n#### *4.4.2 Delays and parallelism\
    \ analysis*\n\nWe now bound the worst-case service time of the main memory subsystem,\
    \ analyzing its components one at a time. Starting with the LLC, we follow the\
    \ control flow diagram\n\nreported in Figure [7\\(](#page-5-0)b) to guide the\
    \ explanation. The LLC collects the requests incoming to the main memory. Three\
    \ scenarios can happen: (i) LLC cache hit, (ii) LLC cache miss with refill, and\
    \ (iii) LLC cache miss with eviction and refill.\n\nIn case (i), the LLC directly\
    \ manages the request, and no commands are submitted to the HMC. The request proceeds\
    \ through the LLC splitter, hit/miss unit, read/write unit, and data way stages.\
    \ By design, each stage of the LLC requires a fixed number of clock cycles. The\
    \ burst splitter executes in one clock cycle (t LLC CK ). The hit/miss detection\
    \ stage takes two clock cycles (2t LLC CK ): one for tag checking and one to propagate\
    \ the request to the read/write unit or the evict/refill unit. The read/write\
    \ unit requires one clock cycle (t LLC CK ) to route the transaction to the data\
    \ ways. The data ways accept the incoming request in one clock cycle (t LLC CK\
    \ ) to then access the internal SRAM macros (same as the SPM, Section [4.2\\)](#page-3-3).\
    \ The internal SRAM takes one clock cycle to provide the read data (t LLC CK ),\
    \ but no further latency is required on writes. Once it gets the response, the\
    \ read/write unit routes the read channel to the AX interface, whereas it takes\
    \ one clock cycle (t LLC CK ) to generate the write B response at the end. Thus,\
    \ read/write unit and data ways take together three clock cycles (3t LLC CK ).\
    \ Summing up the contributions, the service time in case of a hit is upper bound\
    \ by:\n\n$$t\\_{\\rm{CTRL},\\rm{R}/\\rm{W}}^{\\rm{MS-HIT}} = 6 \\cdot t\\_{\\\
    rm{CK}}^{\\rm{LLC}}; \\quad t\\_{\\rm{DATA},\\rm{R}/\\rm{W}}^{\\rm{MS-HIT}} =\
    \ t\\_{\\rm{CK}}^{\\rm{LLC}}; \\tag{6}$$\n\nConsider now cases (ii) and (iii):\
    \ the eviction and refill stage is also involved, and a read (for refill) and,\
    \ optionally, a write (for eviction) is issued to the main memory. Eviction and\
    \ refill are run in parallel. Each operation performs two steps, each taking one\
    \ clock cycle: (a) generating a transaction for the main memory and (b) generating\
    \ a transaction for the data way. Thus, summing the latency introduced by the\
    \ eviction and refill stage (2t LLC CK ) with the ones from the other stages,\
    \ the LLC's contribution to the overall control time in case of a miss is upper\
    \ bound by:\n\n$$t\\_{\\text{CTRL,R/W}}^{\\text{LLC-MISS}} = t\\_{\\text{CTRL,R/W}}^{\\\
    text{MS-HTT}} + 2t\\_{\\text{CK}}^{\\text{LLC}} \\tag{7}$$\n\nConsider now the\
    \ delay introduced by the HMC on a generic request. Later, we will use it to bound\
    \ the service time for the batch of transactions issued by the LLC. As described\
    \ earlier, the HMC is composed of (a) the frontend, (b) the CDC FIFOs, and (c)\
    \ the back-end. Consider (a): each one of the front-end's submodules takes one\
    \ clock cycle to sample and process the transaction, except for the serializer,\
    \ which takes two. As transactions pass through 4 modules (FIFOs, serializer,\
    \ AXITOPHY, and either WTOPHY or PHYTOR), the overall delay contribution of the\
    \ front-end is equal to 5t HMC CK . Consider now (b): these are the CDC FIFOs\
    \ composing the AXI CDC FIFOs introduced in Section [4.1.](#page-3-4) For writes,\
    \ the transmitter (TX) is the front-end, sending data to the back-end from the\
    \ AXTOPHY and the WTOPHY. As both transfers happen in parallel, the delay introduced\
    \ by the CDC on a write is upper bound by tCDC(t HMC CK , tHRAM CK ). For reads,\
    \ first, the front-end transmits (TX) the AXTOPHY request, and then the back-end\
    \ transmits the data beats: the delay introduced by the CDC on a read is upper\
    \ bound by tCDC(t HMC CK , tHRAM CK ) + tCDC(t HRAM CK , tHMC CK ). Consider now\
    \ (c): the back-end's FSM parses the incoming request into a HyperRAM command\
    \ in one cycle (t HRAM CK ). Following this, an extra cycle is required for the\
    \ data to cross the back-end. Summing up the contributions just described, the\
    \ control time of the HMC on a generic transaction is upper bound by:\n\n$$\\\
    begin{aligned} t\\_{\\rm{CIRL},\\rm{R}}^{\\rm{HMC}} &= 5 \\cdot t\\_{\\rm{CK}}^{\\\
    rm{HMC}} + t\\_{\\rm{CIRC}} (t\\_{\\rm{CK}}^{\\rm{HMC}}, t\\_{\\rm{CK}}^{\\rm{HRM}})\
    \ + t\\_{\\rm{CIRC}} (t\\_{\\rm{CK}}^{\\rm{HRM}}, t\\_{\\rm{CK}}^{\\rm{HMC}})\
    \ + 2 \\cdot t\\_{\\rm{CK}}^{\\rm{HRM}} \\\\ t\\_{\\rm{CIRL},\\rm{W}}^{\\rm{HMC}}\
    \ &= 5 \\cdot t\\_{\\rm{CK}}^{\\rm{HMC}} + t\\_{\\rm{CDC}} (t\\_{\\rm{CK}}^{\\\
    rm{HMC}}, t\\_{\\rm{CK}}^{\\rm{HRM}}) + 2 \\cdot t\\_{\\rm{CK}}^{\\rm{HRM}} \\\
    end{aligned} \\tag{8}$$\n\nConsider now the delays introduced by the HyperRAM\
    \ memories on a generic request. The control overhead time to access the HyperRAM\
    \ memory is defined by the HyperBUS protocol [\\[25\\]](#page-13-17). First, the\
    \ 48-bit HyperRAM command vector is sent over the two memories in 3 · t HRAM CK\
    \ clock cycles, as the HyperBUS command bus is 16 bits. Following, the HyperBUS\
    \ provides a fixed latency for the maximum time to access the first data word,\
    \ accounting for refresh effects and crossing row boundaries. The specifications\
    \ [\\[33\\]](#page-13-25) bound such a delay between 7 and 16 clock cycles. In\
    \ our case, this is set to 12 · t HRAM CK . Thus, the total control latency of\
    \ the HyperRAM memory is upper bound by:\n\n$$t\\_{\\text{CTRL,R/W}}^{\\text{HRAM}}\
    \ = 15 \\cdot t\\_{\\text{CK}}^{\\text{HRAM}} \\tag{9}$$\n\nAt this point, data\
    \ are ready to be propagated. As the AXI domain and the HyperRAM have different\
    \ data widths, the number of cycles to send/receive an AXI word is:\n\n$$t\\_{\\\
    text{DATA,R}/\\text{W}}^{\\text{HRAM}} = DW\\_{\\text{HYPER}} \\cdot \\lceil \\\
    frac{DW\\_{\\text{AXI}}}{DW\\_{\\text{HYPER}}} \\rceil \\cdot t\\_{\\text{CK}}^{\\\
    text{HRAM}} \\tag{10}$$\n\nWe now have all the elements to bound the overall service\
    \ time of the whole main memory subsystem in case of a miss (ii) with refill and\
    \ (iii) eviction and refill. First, we bound the service time to serve a refill\
    \ (read) request. A βi-long transaction is split by the LLC into ⌈βi/LWLLC⌉ subtransactions\
    \ to the memory, each LWLLC-long. Therefore, by multiplying the control time of\
    \ each sub-transaction (t HMC CTRL,R+ t HRAM CTRL,R) by the number of transactions\
    \ issued (⌈ βi LWLLC ⌉), we bound the control time introduced by the memory controller\
    \ and the off-chip memories. To this, we sum the control time of the LLC in case\
    \ of a miss (t MS-MISS CTRL,W/R) and obtain the whole control overhead. The same\
    \ reasoning applies to the data time: the total number of values requested by\
    \ the LLC to the memory will be equal to LWLLC · ⌈ <sup>β</sup><sup>i</sup> LWLLC\
    \ ⌉ and the overall time spent reading LWLLC · ⌈ <sup>β</sup><sup>i</sup> LWLLC\
    \ ⌉t HRAM DATA,R/W. It follows that the time to serve one word is LWLLC βi · ⌈\
    \ <sup>β</sup><sup>i</sup> LWLLC ⌉ · t HRAM DATA,R/W. Summing it with the data\
    \ time of the LLC (t MS-HIT DATA,R/W), we obtain the following upper bounds for\
    \ case (ii):\n\n<span id=\"page-6-0\"></span>\n$$\\begin{split} t\\_{\\text{CTRL,R/W}}^{\\\
    text{MS-MESS-REF}} &= t\\_{\\text{CTRL,R}}^{\\text{LLC-MESS}} + \\left[ \\frac{\\\
    beta\\_i}{LW\\_{\\text{LLC}}} \\right] \\cdot (t\\_{\\text{CTRL,R}}^{\\text{HDAC}}\
    \ + t\\_{\\text{CTRL,R}}^{\\text{HERAM}}); \\\\ t\\_{\\text{DATA,R/W}}^{\\text{MS-MESS-REF}}\
    \ &= t\\_{\\text{DATA,R/W}}^{\\text{MS-HIT}} + \\frac{LW\\_{\\text{LLC}}}{\\beta\\\
    _i} \\cdot \\left[ \\frac{\\beta\\_i}{LW\\_{\\text{LLC}}} \\right] \\cdot t\\\
    _{\\text{DATA,R}}^{\\text{HRAM}}; \\end{split} \\tag{11}$$\n\nIf the eviction\
    \ is also required, ⌈ βi LWLLC ⌉ extra write transactions of length β<sup>i</sup>\
    \ are performed to save the evicted data. Following the same reasoning as earlier,\
    \ this batch of transactions will introduce ⌈ βi LWLLC ⌉(t HMC CTRL,W + t HRAM\
    \ CTRL,W) clock cycles to the control time and LWLLC βi · ⌈ <sup>β</sup><sup>i</sup>\
    \ LWLLC ⌉ ·t HRAM DATA,W to the data time. We sum these numbers to eq. [11](#page-6-0)\
    \ to upper bound the overall control and data time as follows:\n\n$$t\\_{\\text{CTIL,W/R}}^{\\\
    text{MS-MIS-REF-EV}} = t\\_{\\text{CTIL,W/R}}^{\\text{MS-MIS-REF}} + \\left[\\\
    frac{\\beta\\_i}{LW\\_{\\text{LLC}}}\\right] (t\\_{\\text{CTIL,W}}^{\\text{HMCC}}\
    \ + t\\_{\\text{CTIL,W}}^{\\text{HBM}});$$\n\n$$t\\_{\\text{DATA,W/R}}^{\\text{MS-MIS-REF-EV}}\
    \ = t\\_{\\text{DATA,W/R}}^{\\text{MS-MIS-REF}} + \\frac{LW\\_{\\text{LLC}}}{\\\
    beta\\_i} \\cdot \\left[\\frac{\\beta\\_i}{LW\\_{\\text{LLC}}}\\right] \\cdot\
    \ t\\_{\\text{DATA,W}}^{\\text{HBM}};$$\n\nConsider now the parallelism of the\
    \ main memory subsystem. This is defined by the LLC, which acts as an interface\
    \ with the rest of the platform, buffering up to DLLC FIFO read and write transactions.\
    \ This means that the maximum number of supported outstanding transactions is\
    \ as follows:\n\n$$\n\\chi\\_R^{MS} = \\chi\\_W^{MS} = D\\_{\\rm FIFO}^{\\rm LLC}\
    \ \\tag{13}\n$$\n\nThe LLC is pipelined: in the case all the enqueued accesses\
    \ are hits, there is no stalling. However, the memory controller handles only\
    \ one transaction at a time, stalling the preceding ones, and only serves one\
    \ read or one write at a time. Hence, as soon as one access is a miss, ρ MS =\
    \ 0 and θ MS = 0.\n\n#### <span id=\"page-7-2\"></span>**4.5 AXI host crossbar**\n\
    \nThe AXI host crossbar under analysis is a consolidated AXI crossbar already\
    \ validated in multiple silicon tapeouts [\\[16\\]](#page-13-7), [\\[15\\]](#page-13-10),\
    \ [\\[24\\]](#page-13-16). We apply here the generic model for the *crossbar*\
    \ proposed in Section [3.4.](#page-2-3) The crossbar is referred as R0.\n\n####\
    \ *4.5.1 RTL IP structure*\n\nAs detailed in Figure [8,](#page-7-1) the crossbar\
    \ exports a set of input subordinate ports (S) and output manager ports (M). Each\
    \ S port is connected to a demultiplexer, which routes the incoming AW and AR\
    \ requests and W data to the proper destination. Each M port is connected to a\
    \ multiplexer, which (i) arbitrates AW and AR requests directed to the same *peripheral*,\
    \ (ii) connects the selected W channel from the *controller* to the *peripheral*,\
    \ and (iii) routes back the R read data and B write responses. The crossbar under\
    \ analysis can be configured for a fully combinatorial (i.e., decoding and routing\
    \ operations in one clock cycle) or pipelined structure with up to three pipeline\
    \ stages. In the platform under analysis, it is configured to be fully combinatorial.\
    \ ble of granting one AW and one AR request for each clock CUT\n\n#### *4.5.2\
    \ Delays and parallelism analysis*\n\nTo analyze the maximum propagation delays\
    \ introduced by the crossbar, we upper bound the overall latency on a transaction\
    \ by combining the delays introduced on each AXI channel. We provide two upper\
    \ bounds, one for transactions in isolation (i.e., t R<sup>0</sup> PROP,R/W as\
    \ defined in Section [3\\)](#page-2-0) and the other for transactions under contention\
    \ (i.e., t R<sup>0</sup> PROP,R/W+t R<sup>0</sup> CON,R/W as defined in Section\
    \ [3\\)](#page-2-0). We will use both of them in our architectural analysis reported\
    \ in Section [5.](#page-7-0)\n\n*Maximum delays in isolation:* Thanks to the combinatorial\
    \ structure, it is guaranteed by design that a request for a transaction, a data\
    \ word, or a write response crosses the crossbar in one clock cycle (t R<sup>0</sup>\
    \ CK). Consider a whole AXI transaction. For a read transaction, the crossbar\
    \ is crossed twice: on the AR and R AXI channels, respectively. For each AXI write\
    \ transaction, the crossbar is crossed two times: the first time is crossed by\
    \ the AW and W beats (propagated in parallel), and the second time by the B response.\
    \ Thus, the propagation delays in isolation are equal to:\n\n<span id=\"page-7-3\"\
    ></span>\n$$t\\_{\\text{PROP}\\text{R}/\\text{W}}^{R\\_0} = 2 \\cdot t\\_{\\text{CK}}^{R\\\
    _0};\\tag{14}$$\n\n*Maximum delays under contention:* Under contention, multiple\
    \ *controllers* connected to the crossbar can attempt to concurrently send requests\
    \ to the same *peripheral*, generating interference. The arbiters deploy a round-robin\
    \ scheme capa-\n\n<span id=\"page-7-1\"></span>![](_page_7_Figure_12.jpeg)\n\n\
    Fig. 8: AXI Crossbar block diagram\n\ncycle. In the worst-case scenario, the request\
    \ under analysis loses the round-robin and is served last, experiencing a delay\
    \ of MR<sup>0</sup> − 1 clock cycles (with MR<sup>0</sup> the number of *controller*\
    \ capable of interfering with the request under analysis). From the previous considerations,\
    \ the maximum propagation time introduced by the crossbar is upper bound by:\n\
    \n<span id=\"page-7-4\"></span>\n$$t\\_{\\text{CON,R}}^{R\\_0} = t\\_{\\text{CON,W}}^{R\\\
    _0} = M\\_{R\\_0} - 1 \\tag{15}$$\n\nConsider now the parallelism. Concerning\
    \ reads, the crossbar does not keep track of the inflight transactions. To route\
    \ the responses back, it appends information to the AXI ID. Doing so does not\
    \ limit the maximum number of outstanding transactions. The behavior is different\
    \ for writes: AXI enforces a strict in-order execution of write transactions (see\
    \ [\\[29\\]](#page-13-21) p. 98). This requires the crossbar to implement a table\
    \ to know the order of granted transactions. The maximum number of outstanding\
    \ write transactions per S port is limited by the depth of such tables, refereed\
    \ as D R<sup>0</sup> TAB. From the previous consideration: χ R<sup>0</sup> <sup>W</sup>\
    \ = D R<sup>0</sup> TAB. In the architecture under analysis, χ R<sup>0</sup> <sup>W</sup>\
    \ is set to be bigger than the parallelism supported by the *peripherals* so that\
    \ the crossbar does not limit the overall parallelism of the system.\n\n# <span\
    \ id=\"page-7-0\"></span>**5 SYSTEM-LEVEL WORST-CASE RESPONSE TIME ANALYSIS**\n\
    \nThis section introduces our system-level end-to-end analysis to upper bound\
    \ the overall response times of read and write transactions issued by a generic\
    \ *controller* and directed to a generic *peripheral*, considering the maximum\
    \ interference generated by the other *controllers* in the system. Our approach\
    \ is static [\\[34\\]](#page-13-26) and compositional [\\[35\\]](#page-13-27).\
    \ Specifically, we leverage the component-level static analysis introduced in\
    \ Section [4](#page-3-0) to then compose, step-by-step, the system-level worst-case\
    \ service time of transactions traversing the whole architecture.\n\nWe make an\
    \ assumption aligned with the SoA [\\[3\\]](#page-12-3), [\\[4\\]](#page-12-4),\
    \ [\\[8\\]](#page-13-0), [\\[11\\]](#page-13-2), [\\[12\\]](#page-13-3), [\\[36\\\
    ]](#page-13-28) to ensure independence among *peripherals* while not compromising\
    \ the generality of the analysis. It is assumed that multiple outstanding transactions\
    \ of the same type (either read or write) issued by the same *controller* target\
    \ the same *peripheral*: before issuing a transaction targeting a *peripheral*\
    \ P<sup>j</sup> , a *controller* completes the pending transactions of the same\
    \ type targeting a different *peripheral* Pz. Without such an assumption, due\
    \ to the strict ordering imposed by the AXI standard [\\[29\\]](#page-13-21) on\
    \ the W channel, and the structure of some *peripherals* generating interference\
    \ between reads and writes (i.e., ρ <sup>P</sup><sup>j</sup> = 0), transactions\
    \ issued by C<sup>k</sup> and directed to P<sup>j</sup> might interfere with transactions\
    \ issued by C<sup>i</sup> and directed to Pz, if C<sup>i</sup> also issues in\
    \ parallel transactions\n\nto P<sup>j</sup> , and vice-versa. This assumption\
    \ allows us to relax our analysis, removing such pathological cases. It is worth\
    \ noticing that it does not enforce any relationship between read and write transactions.\
    \ Such an assumption can either be enforced at the software level or at the hardware\
    \ level. The results of our analysis can be extended to such corner cases if required.\
    \ We leave this exploration for future works.\n\nThe first step of the analysis\
    \ is to bound the overall response time of a transaction in isolation (Lemma [1\\\
    )](#page-8-0). Secondly, we bound the maximum number of transactions that can\
    \ interfere with a transaction under analysis, either of the same type (e.g.,\
    \ reads interfering with a read, Lemma [2\\)](#page-8-1) or of a different type\
    \ (e.g., write interfering with a read, and vice versa, Lemma [3\\)](#page-8-2).\
    \ Lemma [4](#page-8-3) bounds the maximum temporal delay each interfering transaction\
    \ can delay a transaction under analysis. Finally, Theorem [1](#page-9-1) combines\
    \ the results of all the lemmas to upper bound the overall worstcase response\
    \ time of a transaction under analysis under interference. We report the lemmas\
    \ in a general form. AXi,j can represent either a read or write transaction issued\
    \ by the generic *controller* C<sup>i</sup> and directed to the generic *peripheral*\
    \ P<sup>j</sup> . The *crossbar* is referred to as R0. To make our analysis general,\
    \ we assume that Ψ<sup>j</sup> = [C0, ..., CM−1] is the generic set of interfering\
    \ *controllers* capable of interfering with C<sup>i</sup> issuing transactions\
    \ to P<sup>j</sup> and that that a generic set of *bridges* Θ<sup>i</sup> = {Q0,\
    \ ..., Qq−1} can be present between each *controller* C<sup>i</sup> and the crossbar\
    \ R0. The cardinality of Ψ<sup>j</sup> is referred to as \f Ψ<sup>j</sup> \f and\
    \ corresponds to the number of *controllers* interfering with AXi,j .\n\n<span\
    \ id=\"page-8-0\"></span>**Lemma 1.** *The response time in isolation of* AXi,j\
    \ *is upper bounded by:*\n\n<span id=\"page-8-4\"></span>\n$$d\\_{i,j}^{X} = d\\\
    _{\\mathbb{R}\\mathcal{W}}^{P\\_j} + \\sum\\_{Q\\_l \\in \\Theta\\_{i,j}} d\\\
    _{\\mathbb{R}\\mathcal{W}}^{Q\\_l} + d\\_{\\mathbb{R}\\mathcal{W}}^{R\\_0} \\\
    tag{16}$$\n\n*Proof.* Section [4](#page-3-0) upper bounds the worst-case delays\
    \ in isolation introduced by each component in the platform. According to their\
    \ definition, such delays account for all of the phases of the transaction. The\
    \ components in the platform are independent of each other. Thus, the delay introduced\
    \ by each traversed component is independent of the behavior of the other components.\
    \ It derives that the overall delay incurred in traversing the set of components\
    \ between C<sup>i</sup> and P<sup>j</sup> is upper bounded by the sum of the worst-case\
    \ delays introduced by all of the components in the set. Summing up the maximum\
    \ delay introduced by the target *peripheral* P<sup>j</sup> (d P<sup>j</sup> R/W),\
    \ by the set of traversed *bridges* Θ<sup>i</sup> , and by the *crossbar* R<sup>0</sup>\
    \ (d R<sup>0</sup> R/W), the lemma derives.\n\n<span id=\"page-8-1\"></span>**Lemma\
    \ 2.** *The maximum number of transactions of the same type that can interfere\
    \ with* AXi,j *is upper bounded by:*\n\n$$S\\_{i,j}^X = \\min\\left(\\sum\\_{C\\\
    _y \\in \\Psi\\_j} \\phi\\_X^{C\\_y}, \\chi\\_X^{P\\_j} + \\mid \\Psi\\_j \\mid\
    \ \\right) \\tag{17}$$\n\n*Proof.* The min in the formula has two components.\
    \ As from the AXI standard definition, an interfering *controller* C<sup>k</sup>\
    \ cannot have more than ϕ C<sup>k</sup> <sup>X</sup> pending outstanding transactions.\
    \ This means that summing up the maximum number of outstanding transactions for\
    \ each interfering *controller* in Ψ<sup>j</sup> provides an upper bound on the\
    \ number of transactions of the same type interfering with AXi,j – the left member\n\
    \nof the min derives. From our *peripheral* analysis reported in Section [4,](#page-3-0)\
    \ P<sup>j</sup> and R<sup>0</sup> can limit the maximum amount of transactions\
    \ accepted by the system: P<sup>j</sup> accepts overall at most χ P<sup>j</sup>\
    \ R/W transactions – when such a limit is reached, any further incoming transaction\
    \ directed to P<sup>j</sup> is stalled. After P<sup>j</sup> serves a transaction,\
    \ R<sup>0</sup> restarts forwarding transactions to the *peripheral* following\
    \ a round-robin scheme (see Section [4\\)](#page-3-0). In the worst-case scenario,\
    \ C<sup>i</sup> loses the round-robin arbitration against all of the \f Ψ<sup>j</sup>\
    \ interfering *controllers* in Ψ<sup>j</sup> , each ready to submit an interfering\
    \ request. Summing up the contributions, also χ P<sup>j</sup> <sup>R</sup> + \f\
    \ Ψ<sup>j</sup> \f upper bounds the maximum number of transactions interfering\
    \ with AXi,j – the right member of the min derives. Both of the bounds are valid\
    \ – the minimum between them is an upper bound providing the least pessimism –\
    \ Lemma [2](#page-8-1) derives.\n\n<span id=\"page-8-2\"></span>**Lemma 3.** *The\
    \ maximum number of transactions of a different type (represented here as Y, i.e.,\
    \ write transactions interfering with a read under analysis, and vice versa) interfering\
    \ with* AXi,j *is upper bounded by:*\n\n$$U\\_{i,j}^{Y} = (S\\_{i,j}^{X} + 1)\
    \ \\cdot (1 - \\theta^{P\\_j}) \\tag{18}$$\n\n*Proof.* According to Section [4.5,](#page-7-2)\
    \ R<sup>0</sup> manages transactions of different types independently – thus,\
    \ no interference of this type is generated at the R<sup>0</sup> level. From Section\
    \ [3,](#page-2-0) θ <sup>P</sup><sup>j</sup> = 1 represents the case in which\
    \ the *peripheral* is capable of serving read and write transactions in parallel\
    \ (e.g., the SPM *peripheral*, Section [4.2\\)](#page-3-3). Thus, no interference\
    \ is generated among them – the second equation derives. From Section [3,](#page-2-0)\
    \ θ <sup>P</sup><sup>j</sup> = 0 represents the case in which P<sup>j</sup> does\
    \ not feature parallelism in serving read and write transactions (i.e., also write\
    \ transactions interfere with reads, e.g., main memory subsystem, Section [4.4\\\
    )](#page-5-1). Considering lemma [2,](#page-8-1) at most S X i,j transactions\
    \ of the same type can interfere with AXi,j . With θ <sup>P</sup><sup>j</sup>\
    \ = 0, and assuming a round-robin scheme arbitrating between reads and writes\
    \ at the *peripheral* level, each one of the S X i,j interfering transaction of\
    \ the same type can be preceded by a transaction of the opposite type, which can,\
    \ therefore, create interference. The same applies to AXi,j , which can lose the\
    \ arbitration at the *peripheral* level as well. Summing up the contribution,\
    \ it follows that S X i,j + 1 can overall interfere with AXi,j – the first equation\
    \ derives.\n\n<span id=\"page-8-3\"></span>**Lemma 4.** *The maximum time delay\
    \ that a transaction of any kind* AXk,j *issued by the generic interfering* controller\
    \ C<sup>k</sup> *can cause on* AXi,j *is upper bounded by:*\n\n<span id=\"page-8-5\"\
    ></span>\n$$\n\\Delta\\_{k,j} = d\\_{\\text{R\\\\$}\\mathcal{W}}^{R\\_0} + (1\
    \ - \\rho^{P\\_j}) \\cdot t\\_{\\text{CTRL,R\\\\$}\\mathcal{W}}^{P\\_j} + t\\\
    _{\\text{DATT},\\text{R\\\\$}\\mathcal{W}}^{P\\_j} \\cdot \\beta\\_k \\tag{19}\n\
    $$\n\n*Proof.* In traversing the path between C<sup>k</sup> and P<sup>j</sup>\
    \ , AXk,j shares a portion of the path with AXi,j , i.e., the target *peripheral*\
    \ P<sup>j</sup> and the crossbar R<sup>0</sup> – no *bridges* from Θ<sup>k</sup>\
    \ belongs to the shared path, thus the delay propagation of AXk,j do not contribute\
    \ in delaying AXk,j . Considering the delay generated by AXk,j at R0, this is\
    \ upper bounded by d R<sup>0</sup> R/W in Section [3.4.](#page-2-3) As from Section\
    \ [3.3,](#page-2-2) t P<sup>j</sup> CTRL,R/W + t P<sup>j</sup> DATA,R/W · β<sup>k</sup>\
    \ is the maximum service time of P<sup>j</sup> for the transaction AXk,j and upper\
    \ bounds the maximum temporal delay that AXk,j can cause on AXi,j at P<sup>j</sup>\
    \ . As from the definition of an interfering transaction, AXk,j is served by P<sup>j</sup>\
    \ before AXi,j . As defined by the model in Section [3.3,](#page-2-2) when ρ <sup>P</sup><sup>j</sup>\
    \ = 1, the *peripheral* works in a pipeline fashion. This means that\n\nfor ρ\
    \ <sup>P</sup><sup>j</sup> = 1, the control time t P<sup>j</sup> CTRL,R/W of an\
    \ interfering transaction is pipelined and executed in parallel with the transaction\
    \ under analysis. Differently, when ρ <sup>P</sup><sup>j</sup> = 0, no pipeline\
    \ is implemented, and the control time of the interfering transaction can partially\
    \ or totally interfere with the transaction under analysis. From the previous\
    \ considerations, the contribution (1 − ρ <sup>P</sup><sup>j</sup> )·t P<sup>j</sup>\
    \ CTRL,R/W + t P<sup>j</sup> DATA,R/W · β<sup>k</sup> derives. Summing up the\
    \ contributions, the lemma follows.\n\n<span id=\"page-9-1\"></span>**Theorem\
    \ 1.** *The overall response time of* AXi,j *under the interference generated\
    \ by the other* controllers *in the system is upper bounded by:*\n\n<span id=\"\
    page-9-3\"></span>\n$$H\\_{i,j}^X = d\\_{i,j}^X + (S\\_{i,j}^X + U\\_{i,j}^Y)\
    \ \\cdot \\Delta\\_{k,j} \\tag{20}$$\n\n*Proof.* Summing up the contribution in\
    \ isolation for AXi,j (Lemma [1\\)](#page-8-0) with the sum of the maximum number\
    \ of interfering transactions of the same type (Lemma [2\\)](#page-8-1) and of\
    \ a different type (Lemma [3\\)](#page-8-2) multiplied by the maximum delay generated\
    \ by each interfering transaction (Lemma [4\\)](#page-8-3), Theorem [1](#page-9-1)\
    \ derives.\n\nThe results presented in this Section represent analytical upper\
    \ bounds derived through static code analysis and the formulation of mathematical\
    \ proofs. Section [6](#page-9-0) will validate them through a comprehensive set\
    \ of cycle-accurate experiments and measurements.\n\n# <span id=\"page-9-0\"></span>**6\
    \ EXPERIMENTAL VALIDATION**\n\nThis Section describes the experimental campaign\
    \ we conducted to validate the methodology and models. The aim of the experimental\
    \ campaign is to assess that the results presented in the previous Sections correctly\
    \ upper bound the maximum delays and response times at the component level and\
    \ the architectural level. We follow a hierarchical approach: at first, Section\
    \ [6.1](#page-9-2) aims to validate the results at the component level we proposed\
    \ in Section [4.](#page-3-0) Following, in Section [6.2,](#page-10-0) we experimentally\
    \ validate the system-level analysis we proposed in Section [5.](#page-7-0) The\
    \ experiments are conducted in a simulated environment (leveraging the Siemens\
    \ QuestaSIM simulator) and by deploying the design on an FPGA platform. In the\
    \ simulated experiments, we deploy custom AXI managers for *ad-hoc* traffic generation\
    \ and cycle-accurate performance monitors. The generic custom manager represents\
    \ a generic configurable *controller* C<sup>i</sup> issuing requests for transactions\
    \ – we will refer to that as GC<sup>i</sup> . In the FPGA, we leverage CVA6 and\
    \ the PULP cluster to generate the traffic with synthetic software benchmarks\
    \ and rely on their performance-monitoring registers to collect the measurements.\
    \ The experimental designs are deployed on the AMD-Xilinx VCU118, using the Vitis\
    \ 2022.1 toolchain. Similar State-of-the-Art works upper bounding the execution\
    \ time of a single transaction leverage synthetic benchmarks to measure the worst-case\
    \ access times since generic applications fail to do so [\\[8\\]](#page-13-0)–[\\\
    [10\\]](#page-13-1). For this reason, we concentrate on synthetic benchmarks at\
    \ the IP and the system level.\n\n#### <span id=\"page-9-2\"></span>**6.1 Component-level\
    \ hardware modules**\n\n#### *6.1.1 Delays analysis*\n\nThis subsection presents\
    \ the tests run to measure the worstcase access latency time in isolation for\
    \ the *peripherals* (d P<sup>j</sup> R/W ), 10\n\nthe *crossbar* (d R<sup>0</sup>\
    \ R/W ) and the *bridges* (d Q<sup>j</sup> R/W ) from Section [4.](#page-3-0)\
    \ We connect the generic controller CG<sup>i</sup> to the IP under analysis for\
    \ these experiments. We let CG<sup>i</sup> issue 100'000 transactions, one at\
    \ a time, with random burst length (βi). We monitor the service times and then\
    \ pick the longest ones for different β<sup>i</sup> .\n\nFigure [9](#page-10-1)\
    \ compares the maximum measured experimental delays with the upper bound proposed\
    \ in Section [4.](#page-3-0) Figure [9\\(](#page-10-1)a) reports the maximum service\
    \ time of the main memory subsystem in case of a miss as a function of the burst\
    \ length of the transaction under analysis, either when (i) only a refill is necessary\
    \ and (ii) both refill and eviction are necessary, compared with the bounds proposed\
    \ in Section [4.4.](#page-5-1) The measured service times are lower than the bounds.\
    \ The pessimism is between 3% and 10.1%; the larger β<sup>i</sup> , the higher\
    \ the pessimism. Higher pessimism on longer transactions is due to the internal\
    \ splitting at the LLC. As from our analysis, the memory subsystem is not fully\
    \ pipelined (ρMS = 0). However, in practice, the control and data phases of consecutive\
    \ sub-transactions might be partially served in parallel by the LLC and the memory\
    \ controller. This means that the longer the transaction, the higher the number\
    \ of sub-transactions and their overlap, and the lower the service time compared\
    \ to our model. Thus, the pessimism increases. Figure [9\\(](#page-10-1)b) reports\
    \ the measured results on the main memory subsystem but in case of a hit, compared\
    \ with the bounds proposed in Section [4.4.](#page-5-1) As we consider an LLC\
    \ hit, the access to the HyperRAM is not performed: this test analyzes the service\
    \ time of the LLC. Our bounds are always upper bounds for the maximum measured\
    \ results. The trend here is reversed w.r.t. Figure [9\\(](#page-10-1)a) – as\
    \ β<sup>i</sup> increases, the relative pessimism decreases from 7.7% down to\
    \ 0.4%. In this case, the source of the pessimism comes only from the control\
    \ time, which does not depend on β<sup>i</sup> , while there is no pessimism on\
    \ the data time. Hence, this pessimism gets amortized as the burst length and\
    \ the overall service time increase. We conduct the same experimental campaign\
    \ also on the AXI SPM – the measured results, compared with the bounds proposed\
    \ in Section [4.2,](#page-3-3) are reported in Figure [9\\(](#page-10-1)c). The\
    \ trends are similar to the ones reported in Figure [9\\(](#page-10-1)b) for LLC\
    \ hits – the pessimism of our analysis is limited to 1 and 2 clock cycles for\
    \ reads and writes on the control time, respectively. As in the case of the LLC\
    \ HITs, the upper bound on the control overhead gets amortized for longer transactions,\
    \ and the pessimism reduces from 8.8% to 0.5%.\n\nFigure [9\\(](#page-10-1)d)\
    \ reports the maximum measured latency to cross an AXI CDC FIFO as a function\
    \ of the manager clock period (the subordinate clock period is fixed to 30 ns)\
    \ and compared with the bounds proposed in Section [4.1.](#page-3-4) The results\
    \ are independent of the length of the transaction. To stimulate the highest variability,\
    \ the phases of the clocks are randomly selected on a uniform distribution. The\
    \ first bar reports the crossing delays from the manager to the subordinate side,\
    \ corresponding to the delays introduced on the AW, W, and AR AXI channels. The\
    \ second bar reports the crossing delays from the subordinate to the manager side,\
    \ corresponding to the overall delays on the AXI R and B channels. The third bar\
    \ shows the overall delay on a complete transaction, corresponding to the sum\
    \ of the two previously introduced contributions (see Section [4.1\\)](#page-3-4).\
    \ The pessimism of our bounds is, at most, one clock cycle of the slowest clock\
    \ between manager and subordinate.\n\n<span id=\"page-10-1\"></span>![](_page_10_Figure_0.jpeg)\n\
    \nFig. 9: Services time in isolation.\n\nFigure [9\\(](#page-10-1)e) reports the\
    \ measured propagation delays introduced by the crossbar over an entire write\
    \ and read transaction, compared with the bounds of Section [4.5,](#page-7-2)\
    \ varying the number of *controllers*. As explained in Section [4.5,](#page-7-2)\
    \ the propagation delay is the sum of the propagation latency without interference\
    \ (eq. [14\\)](#page-7-3) and the additional contention latency (eq. [15\\)](#page-7-4),\
    \ which depends on the number of *controllers*. Thanks to the simplicity of the\
    \ arbitration operated by the crossbar (pure round-robin), our proposed bounds\
    \ exactly match the measurements. We conducted the experimental campaign also\
    \ on the IO subsystem. We measured the maximum service time and compared it with\
    \ the upper bounds of Section [4.3,](#page-4-1) which we do not show for space\
    \ reasons: such IP supports only single-word transactions. Our upper bounds exceed\
    \ the maximum measured service time with pessimism of down to 2 clock cycles,\
    \ with service times of 4 (write) and 5 (read) clock cycles.\n\n#### *6.1.2 Parallelism*\n\
    \nWe also demonstrate our analysis of parallelism of the *peripherals* (χ P<sup>j</sup>\
    \ R/W ) and the *crossbar* (χ R<sup>0</sup> R/W ) analyzed in Section [4.](#page-3-0)\
    \ To do so, we configured CG<sup>i</sup> to issue unlimited outstanding transactions\
    \ to the *peripheral* under test. In parallel, we monitor the maximum number of\
    \ accepted outstanding transactions. Our measurements match our analysis: the\
    \ maximum number of outstanding transactions is defined by the maximum parallelism\
    \ accepted at the input stage of the peripherals and the crossbar.\n\n#### <span\
    \ id=\"page-10-0\"></span>**6.2 System-level experiments**\n\nWhile the previous\
    \ experiments focused on the evaluation at the IP level, this set of experiments\
    \ aims to evaluate the system-level bounds proposed in Section [5.](#page-7-0)\
    \ We first validate our analysis in simulation. We developed a System Verilog\
    \ testbench with two configurable AXI synthetic *controllers* CG<sup>i</sup> connected\
    \ to the target architecture (see Figure [2\\)](#page-1-2) stimulating overload\
    \ conditions to highlight worst-case scenarios. We also validate our results on\
    \ FPGA, generating traffic with CVA6 and the PULP cluster.\n\nAt first, we evaluate\
    \ the results in isolation *at the system level* as a function of the burst length,\
    \ leveraging the same strategy used for the previous experiments. Namely, these\
    \ tests are meant to validate Lemma [1](#page-8-0) (eq. [16\\)](#page-8-4). To\
    \ measure the service time at the system level in isolation, we let one GC<sup>i</sup>\
    \ issue 100'000 transactions, one at a time, with different β<sup>i</sup> , while\
    \ the other GC<sup>k</sup> is inactive. We monitor the service times and then\
    \ pick the longest ones for each\n\nβi . Figures [10](#page-11-1) (a) and (b)\
    \ report the maximum measured system-level response times in isolation for completing\
    \ a transaction issued by the generic *controller* GC<sup>i</sup> and directed\
    \ to (a) the main memory subsystem (case of cache miss, causing either refill\
    \ or both refill and eviction) and (b) to the SPM memory, compared with the bounds\
    \ proposed in Lemma [1.](#page-8-0) The measured service times are smaller than\
    \ the bounds in all the tested scenarios. The measure and the trends reported\
    \ in Figure [10\\(](#page-11-1)a) are aligned with the ones found at the IP level\
    \ and reported in Figure [9\\(](#page-10-1)a). This is because the overhead introduced\
    \ by the crossbar (in isolation) and the CDC FIFOs is negligible compared to the\
    \ memory subsystem's service time. Figure [10\\(](#page-11-1)b) shows a trend\
    \ aligned with the results at the IP-level reported in Figure [9\\(](#page-10-1)c):\
    \ the lower β<sup>i</sup> , the higher the pessimism. It is worth mentioning that\
    \ the analysis shows higher pessimism at the system level than at the IP level.\
    \ This is due to the extra pessimism from the crossbar and the CDC, which is nevertheless\
    \ amortized on longer transactions, down to 1.9%.\n\nWe now analyze the results\
    \ under maximum interference, to verify the results of Lemma [2](#page-8-1) and\
    \ [3](#page-8-2) and Theorem [1.](#page-9-1) For these tests, the execution of\
    \ GC<sup>i</sup> (100'000 transactions, one at a time) receives interference by\
    \ *controller* GCk. β<sup>k</sup> is fixed and equal to β<sup>i</sup> , while\
    \ we vary the amount of outstanding transactions GC<sup>k</sup> can issue (ϕ CG<sup>k</sup>\
    \ R/W ). Figures [10](#page-11-1) (c), (d), and (e) report the maximum measured\
    \ systemlevel response times for completing a transaction issued by the generic\
    \ *controller* GC<sup>i</sup> and directed to (c) the main memory with an LLC\
    \ miss considering β<sup>i</sup> = 16, (d) the SPM memory, considering β<sup>i</sup>\
    \ = 16, and (e) the SPM memory, considering β<sup>i</sup> = 256, and compare them\
    \ with the upper bounds proposed in equation [20.](#page-9-3) Figures [10](#page-11-1)\
    \ (c), (d), and (e) verify the results of Lemma [2](#page-8-1) and [3:](#page-8-2)\
    \ when ϕ CG<sup>k</sup> R/W > χMS R/W (two bars on the right), the total service\
    \ time is defined by the parallelism of the peripheral itself – as expected, after\
    \ saturating the number of interfering transactions accepted by the peripheral,\
    \ the measured results are the same regardless of the increase of ϕ CG<sup>k</sup>\
    \ R/W . Differently, when ϕ CG<sup>k</sup> R/W ≤ χMS R/W , a reduced value of\
    \ ϕ CG<sup>k</sup> R/W corresponds to lower interference and response times. Figure\
    \ [10\\(](#page-11-1)c) refers to the case of an LLC miss under interference when\
    \ β<sup>k</sup> = 16. The results confirm the safeness of our analysis, which\
    \ correctly upper bounds the overall response times with a pessimism around 15%,\
    \ which is slightly higher than the pessimism of a transaction in isolation at\
    \ the system level. As explained in the previous subsection, when multiple transactions\
    \ are enqueued, the memory subsystem can partially serve their data and control\n\
    \n12\n\n<span id=\"page-11-1\"></span>![](_page_11_Figure_1.jpeg)\n\nFig. 10:\
    \ Services times under interference.\n\nphases in parallel. However, our model\
    \ only allows ρMS = 1 or ρMS = 0, i.e., either the *peripheral* is fully pipelined\
    \ or not pipelined at all. Since ρMS = 0, the pessimism is slightly higher when\
    \ more transactions are enqueued (and partially served in parallel) as equation\
    \ [19](#page-8-5) counts the service time of a transaction fully when ρMS = 0.\
    \ Varying β<sup>k</sup> of GC<sup>k</sup> gives comparable results – we do not\
    \ report such results for briefness and lack of space. We provide two charts for\
    \ the SPM, in Figure [10\\(](#page-11-1)d) and Figure [10\\(](#page-11-1)e). The\
    \ comparison of the two charts highlights how the interfering transactions' length\
    \ impacts the analysis's pessimism, ranging between 19.7% for β = 16 to 1% for\
    \ β = 256. The trend here is aligned with the service time at the system level\
    \ in isolation: the pessimism comes from the control times of SPM and propagation\
    \ latency of the crossbar and the CDC, which are amortized as the data time increases\
    \ with βk.\n\n#### **6.3 Discussion**\n\nIn this Section, we validated the analysis\
    \ of Sections [4](#page-3-0) and [5](#page-7-0) through an extensive set of tests.\
    \ We demonstrated how the proposed approach enables detailed explanations of the\
    \ analysis's pessimism and facilitates iterative refinement. This allows us to\
    \ derive upper bounds that are safe yet not overly pessimistic, particularly when\
    \ compared to similar stateof-the-art works based on closed-source or loosely-timed\
    \ IPs. Nevertheless, while the methodology is promising, the resulting analysis\
    \ may seem limited in comparison to other works that model more sophisticated\
    \ closed-source IPs. Here, we discuss the limitations of our analysis, focusing\
    \ on its dependence on the underlying characteristics of the available open-source\
    \ hardware.\n\nIt is noteworthy how the analysis leverages the roundrobin policy\
    \ of the main interconnect and the in-order nature of *peripherals* in Lemmas\
    \ [2](#page-8-1) and [3.](#page-8-2) The absence of internal reordering allows\
    \ to derive the number of transactions preceding the one under interference directly\
    \ from the arbitration policy. As long as the *peripherals* serve the transactions\
    \ in order, extending the analysis to support other arbitration policies is expected\
    \ to require minimal effort. Instead, supporting *peripherals* with internal transaction\
    \ reordering can lead to *timing anomalies* [\\[7\\]](#page-12-2) and make the\
    \ proposed model unsafe, as previously demonstrated in [\\[5\\]](#page-12-1).\
    \ Our analysis focuses on the available *peripherals* within the target architecture,\
    \ as out-of-order *peripherals* are not available open-source to us. We envision\
    \ expanding the\n\nanalysis to match higher-performance platforms as opensource\
    \ hardware evolves.\n\nLastly, it is important to note that the analysis bounds\
    \ only a single transaction issued by C<sup>i</sup> – this limitation is not imposed\
    \ on the interfering controllers. Lemma [2](#page-8-1) does not consider C<sup>i</sup>\
    \ to have more pending transactions, except for the ones already accepted by P<sup>j</sup>\
    \ . In other words, Lemma [2](#page-8-1) assumes that there is not a queue of\
    \ transactions buffered in the *bridges* between C<sup>i</sup> and R0, which could\
    \ exist when P<sup>j</sup> is full. We could potentially extend the model to define\
    \ a batch of enqueued transactions and then modify Lemma [2](#page-8-1) to analyze\
    \ this scenario. Such an extension would further build upon the proposed model\
    \ and analysis, which is limited to bound the access time of a single transaction.\n\
    \n# <span id=\"page-11-0\"></span>**7 RELATED WORK**\n\nIn this Section, we provide\
    \ a thorough comparison with previous works focusing on enhancing the timing predictability\
    \ of digital circuits. Traditionally, the majority of these works leverage commercial\
    \ off-the-shelf devices [\\[34\\]](#page-13-26), [\\[38\\]](#page-13-29) or predictable\
    \ architectures modeled with a mix of cycleaccurate and behavioral simulators\
    \ [\\[39\\]](#page-13-30). Also, they focus on bounding the execution times for\
    \ predefined specific software tasks rather than the individual transaction service\
    \ times [\\[7\\]](#page-12-2), [\\[38\\]](#page-13-29)–[\\[40\\]](#page-13-31).\
    \ Furthermore, they build the models from dynamic experiments rather than from\
    \ static analysis, largely due to the dearth of detailed hardware specifications\
    \ [\\[35\\]](#page-13-27), limiting the generality of their approach. More recent\
    \ works advocate for static modeling and analysis of protocols [\\[8\\]](#page-13-0),\
    \ [\\[13\\]](#page-13-4), interconnect [\\[1\\]](#page-12-0), [\\[3\\]](#page-12-3),\
    \ [\\[9\\]](#page-13-5), and shared memory resources [\\[5\\]](#page-12-1), [\\\
    [10\\]](#page-13-1) to provide more generic and comprehensive models. While their\
    \ value is undeniable, due to the unavailability of the source RTL, each one focuses\
    \ on only one of these resources, resulting in a significant penalty to the pessimism\
    \ of the upper bounds [\\[5\\]](#page-12-1). Our work breaks from this convention,\
    \ presenting a holistic static model of an entire open-source architecture rigorously\
    \ validated through RTL cycle-accurate simulation and FPGA emulation. As Table\
    \ [1](#page-12-6) shows, this is the first work to analyze and model the open-source\
    \ siliconproven RTL of all the IPs composing a whole SoC to build the least pessimistic\
    \ upper bounds for data transfers within the architecture when compared to similar\
    \ SoA works.\n\nBiondi et al. [\\[13\\]](#page-13-4) developed a model of the\
    \ memory-access regulation mechanisms in the ARM MPAM and provided detailed instantiations\
    \ of such mechanisms, which they\n\n<span id=\"page-12-6\"></span>\n\n|  |  |\
    \  |  |  |  |  |  |  |  |  |  |  |  |  | TABLE 1: Comparison with State-of-the-Art\
    \ works for predictability. IC = Interconnect. DMR = Deadline miss ratio. |  |\
    \  |  |  |  |\n|--|--|--|--|--|--|--|--|--|--|--|--|--|--|--|-------------------------------------------------------------------------------------------------------------------|--|--|--|--|--|\n\
    |--|--|--|--|--|--|--|--|--|--|--|--|--|--|--|-------------------------------------------------------------------------------------------------------------------|--|--|--|--|--|\n\
    \n|                       | Target                             | Analysis on \
    \                   | Pessimism   | Technology  | Open RTL |\n|-----------------------|------------------------------------|--------------------------------|-------------|-------------|----------|\n\
    | Biondi et. al. [13]   | ARM MPAM Protocol                  | Protocol specs\
    \ (Model)         | No HW       | ✗           | ✗        |\n| Hassan et. al. [8]\
    \    | JEDEC DDR3 Protocol                | Protocol specs (Model)         | 0%\
    \ − 200%   | ✗           | ✗        |\n| Abdelhalim et.al. [5] | Whole mem. hier.\
    \                   | IPs & System (C++ Model)       | 16% − 50%   | ✗       \
    \    | ✗        |\n| BlueScale [3]         | Hier. mem. IC                   \
    \   | IC uArch (Black-box)           | DMR         | FPGA        | ✗        |\n\
    | AXI-RT-IC [1]         | AXI SoC IC                         | IC uArch (Black-box)\
    \           | DMR         | FPGA        | ✗        |\n| Restuccia et. al. [9]\
    \ | AXI Hier. mem. IC                  | IC uArch (Black-box)           | 50%\
    \ − 90%   | FPGA        | ✗        |\n| AXI-REALM [37]        | AXI traffic regulator\
    \              | No analysis                    | No model    | FPGA & ASIC |\
    \ ✓        |\n| Ditty [10]            | Cache coher. mechanism             | IP\
    \ (Fine-grained RTL)          | 100% − 200% | FPGA        | ✓        |\n| This\
    \ Work             | SoC IC, peripherals & system-level | IP & System (Fine-grained\
    \ RTL) | 1% − 28%    | FPGA & ASIC | ✓        |\n\nthen evaluated with IBM CPLEX,\
    \ a decision optimization software for solving complex optimization models. While\
    \ elegant, this approach is not validated on hardware and, therefore, is limited\
    \ in terms of applicability and precision. A more practical and adopted approach\
    \ is the one proposed by Hassan and Pellizzoni [\\[8\\]](#page-13-0). The authors\
    \ develop a finegrained model of the JEDEC DDR3 protocol, validated with MCsim\
    \ [\\[12\\]](#page-13-3), a cycle-accurate C++ memory controller simulator. Unfortunately,\
    \ not having access to the RTL prevents fine-grained modeling and analysis and\
    \ mandates over-provisioning, strongly impacting the overall pessimism of the\
    \ system, which can be as high as 200%. Abdelhalim et al. in [\\[5\\]](#page-12-1)\
    \ present a study bounding the access times of memory requests traversing the\
    \ entire memory hierarchy and propose µarchitectural modifications to the arbiters\
    \ in such hierarchy. Their modifications result in very low pessimism (down to\
    \ 16%) on synthetic and real-world benchmarks. However, the results are validated\
    \ on C++ models of the cores, interconnect, and memory controllers, not RTL code\
    \ targeting silicon implementation.\n\nMore recently, different researchers proposed\
    \ models of hardware IPs that they could validate through cycle-accurate experiments\
    \ [\\[1\\]](#page-12-0), [\\[4\\]](#page-12-4), [\\[9\\]](#page-13-5). In [\\\
    [9\\]](#page-13-5), Restuccia et al. focused on upper bounding the response times\
    \ of AXI bus transactions on FPGA SoCs through the modeling and analysis of generic\
    \ hierarchical interconnects arbitrating the accesses of multiple hardware accelerators\
    \ towards a shared DDR memory. In this work, the interconnect under analysis is\
    \ a proprietary Xilinx IP, which had to be treated as a black box. Also, due to\
    \ the unavailability of the RTL code, the authors did not model the other IPs\
    \ composing the target platform, limiting the precision of the proposed upper\
    \ bounds, which achieve a pessimism between 50% and 90%. Jiang et al. modeled,\
    \ analyzed, and developed AXI-ICRT [\\[1\\]](#page-12-0) and Bluescale [\\[3\\\
    ]](#page-12-3), two sophisticated interconnects providing predictability features\
    \ and coming with a comprehensive model. However, the model and analysis proposed\
    \ in AXI-ICRT [\\[1\\]](#page-12-0), and Bluescale [\\[3\\]](#page-12-3) are not\
    \ as fine-grained as ours: the authors do not provide upper bounds of the access\
    \ times but rather focus on the deadline miss ratio given a fixed workload for\
    \ the different controllers in the system. Moreover, the authors do not provide\
    \ the RTL of such solutions. AXI-REALM [\\[37\\]](#page-13-32) proposes completely\
    \ open-source IPs supporting predictable communications. However, it misses a\
    \ holistic model and analysis. In Ditty [\\[10\\]](#page-13-1), researchers propose\
    \ an open-source predictable directory-based cache coherence mechanism for multicore\
    \ safety-critical systems that guarantees a worst-case latency (WCL) on data accesses\
    \ with almost cycle-accurate precision. However, Ditty's model only covers the\
    \ coherency protocol latency and the core subsystem, overlooking systemlevel analysis\
    \ and achieving very pessimistic boundaries. In this landscape, it emerges clearly\
    \ that our work is the first one covering both modeling and analysis of the interconnects\
    \ and the shared memory resources, with an in-depth analysis of silicon-proven\
    \ open-source RTL IPs and achieving the lowest pessimism when compared to similar\
    \ SoA works.\n\n# <span id=\"page-12-5\"></span>**8 CONCLUSIONS**\n\nIn conclusion,\
    \ this is the first work to bridge the gap between open-source hardware and predictability\
    \ modeling and analysis. It presented (i) a fine-grained model and analysis for\
    \ the typical building blocks composing modern heterogeneous low-power SoCs directly\
    \ based on the source RTL, and (ii) a full mathematical analysis to upper bound\
    \ data transfer execution times. Namely, we demonstrated a methodology that successfully\
    \ exploits the availability of the source code to provide safe, but not overly\
    \ pessimistic, upper bounds for the execution times of data transfers when compared\
    \ to similar SoA works based on closed-source IPs.\n\nAs discussed in Section\
    \ [6,](#page-9-0) after this thorough evaluation, we envision extending our results\
    \ to other popular opensource IPs and different arbitration policies. To hopefully\
    \ stimulate novel research contributions, we open-source a guide to replicate\
    \ the results shown in Section [6](#page-9-0) at [https:](https://github.com/pulp-platform/soc_model_rt_analysis)\
    \ [//github.com/pulp-platform/soc](https://github.com/pulp-platform/soc_model_rt_analysis)\
    \ model rt analysis, comprehensive of the simulated environment and the software\
    \ benchmarks to run on a sophisticated Cheshire-based SoC targeting automotive\
    \ applications.\n\n# **REFERENCES**\n\n- <span id=\"page-12-0\"></span>[1] Z.\
    \ Jiang *et al.*, \"AXI-ICRT RT : Towards a Real-Time AXI-Interconnect for Highly\
    \ Integrated SoCs,\" *IEEE Transactions on Computers*, vol. 72, no. 3, pp. 786–799,\
    \ 2022.\n- [2] A. Biondi *et al.*, \"SPHERE: A multi-SoC architecture for nextgeneration\
    \ cyber-physical systems based on heterogeneous platforms,\" *IEEE Access*, vol.\
    \ 9, pp. 75 446–75 459, 2021.\n- <span id=\"page-12-3\"></span>[3] Z. Jiang *et\
    \ al.*, \"BlueScale: a scalable memory architecture for predictable real-time\
    \ computing on highly integrated SoCs,\" in *Proceedings of the 59th ACM/IEEE\
    \ Design Automation Conference*, 2022, pp. 1261–1266.\n- <span id=\"page-12-4\"\
    ></span>[4] F. Restuccia *et al.*, \"AXI HyperConnect: A Predictable, Hypervisorlevel\
    \ Interconnect for Hardware Accelerators in FPGA SoC,\" in *2020 57th ACM/IEEE\
    \ Design Automation Conference (DAC)*, 2020.\n- <span id=\"page-12-1\"></span>[5]\
    \ S. Abdelhalim *et al.*, \"A Tight Holistic Memory Latency Bound Through Coordinated\
    \ Management of Memory Resources,\" in *35th Euromicro Conference on Real-Time\
    \ Systems (ECRTS 2023)*, vol. 262, 2023, pp. 17:1–17:25.\n- [6] G. Fernandez *et\
    \ al.*, \"Contention in multicore hardware shared resources: Understanding of\
    \ the state of the art,\" in *Proceedings of the 14th International Workshop on\
    \ Worst-Case Execution Time Analysis (WCET 2014)*, 2014, pp. 31–42.\n- <span id=\"\
    page-12-2\"></span>[7] S. Hahn, M. Jacobs, and J. Reineke, \"Enabling Compositionality\
    \ for Multicore Timing Analysis,\" in *Proceedings of the 24th International Conference\
    \ on Real-Time Networks and Systems*. Association for Computing Machinery, 2016,\
    \ p. 299–308.\n- <span id=\"page-13-0\"></span>[8] M. Hassan and R. Pellizzoni,\
    \ \"Bounding DRAM Interference in COTS Heterogeneous MPSoCs for Mixed Criticality\
    \ Systems,\" *IEEE Transactions on Computer-Aided Design of Integrated Circuits\
    \ and Systems*, vol. 37, no. 11, pp. 2323–2336, 2018.\n- <span id=\"page-13-5\"\
    ></span>[9] F. Restuccia *et al.*, \"Bounding Memory Access Times in Multi-Accelerator\
    \ Architectures on FPGA SoCs,\" *IEEE Transactions on Computers*, vol. 72, no.\
    \ 1, pp. 154–167, 2022.\n- <span id=\"page-13-1\"></span>[10] Z. Wu, M. Bekmyrza,\
    \ N. Kapre, and H. Patel, \"Ditty: Directorybased Cache Coherence for Multicore\
    \ Safety-critical Systems,\" in *2023 Design, Automation & Test in Europe Conference\
    \ & Exhibition (DATE)*. IEEE, 2023, pp. 1–6.\n- <span id=\"page-13-2\"></span>[11]\
    \ M. Hassan, \"On the Off-Chip Memory Latency of Real-Time Systems: Is DDR DRAM\
    \ Really the Best Option?\" in *2018 IEEE Real-Time Systems Symposium (RTSS)*,\
    \ 2018, pp. 495–505.\n- <span id=\"page-13-3\"></span>[12] R. Mirosanlou, D. Guo,\
    \ M. Hassan, and R. Pellizzoni, \"Mcsim: An extensible dram memory controller\
    \ simulator,\" *IEEE Computer Architecture Letters*, vol. 19, no. 2, pp. 105–109,\
    \ 2020.\n- <span id=\"page-13-4\"></span>[13] M. Zini, D. Casini, and A. Biondi,\
    \ \"Analyzing Arm's MPAM From the Perspective of Time Predictability,\" *IEEE\
    \ Transactions on Computers*, vol. 72, no. 1, pp. 168–182, 2023.\n- <span id=\"\
    page-13-6\"></span>[14] A. Herrera, \"The Promises and Challenges of Open Source\
    \ Hardware,\" *Computer*, vol. 53, no. 10, pp. 101–104, 2020.\n- <span id=\"page-13-10\"\
    ></span>[15] A. Ottaviano, T. Benz, P. Scheffler, and L. Benini, \"Cheshire: A\
    \ Lightweight, Linux-Capable RISC-V Host Platform for Domain-Specific Accelerator\
    \ Plug-In,\" *IEEE Transactions on Circuits and Systems II: Express Briefs*, pp.\
    \ 1–1, 2023.\n- <span id=\"page-13-7\"></span>[16] L. Valente *et al.*, \"Shaheen:\
    \ An Open, Secure, and Scalable RV64 SoC for Autonomous Nano-UAVs,\" in *2023\
    \ IEEE Hot Chips 35 Symposium (HCS)*, 2023, pp. 1–12.\n- <span id=\"page-13-8\"\
    ></span>[17] M. B. Taylor, \"Your Agile Open Source HW Stinks (Because It Is Not\
    \ a System),\" in *2020 IEEE/ACM International Conference On Computer Aided Design\
    \ (ICCAD)*, 2020, pp. 1–6.\n- <span id=\"page-13-9\"></span>[18] PULP, \"PULP\
    \ Platform Github,\" [https://github.com/](https://github.com/pulp-platform) [pulp-platform,](https://github.com/pulp-platform)\
    \ 2022.\n- <span id=\"page-13-11\"></span>[19] L. Valente *et al.*, \"HULK-V:\
    \ a Heterogeneous Ultra-low-power Linux capable RISC-V SoC,\" in *2023 Design,\
    \ Automation & Test in Europe Conference & Exhibition (DATE)*, 2023, pp. 1–6.\n\
    - <span id=\"page-13-12\"></span>[20] OpenHW-Group, \"CVA6,\" [https://github.com/openhwgroup/](https://github.com/openhwgroup/cva6)\
    \ [cva6,](https://github.com/openhwgroup/cva6) 2022.\n- <span id=\"page-13-13\"\
    ></span>[21] M. Schneider *et al.*, \"Composite Enclaves: Towards Disaggregated\
    \ Trusted Execution,\" *IACR Transactions on Cryptographic Hardware and Embedded\
    \ Systems*, vol. 2022, no. 1, p. 630–656, Nov. 2021.\n- <span id=\"page-13-14\"\
    ></span>[22] P. Platform, \"PULP cluster,\" [https://github.com/pulp-platform/](https://github.com/pulp-platform/pulp_cluster)\
    \ pulp [cluster,](https://github.com/pulp-platform/pulp_cluster) 2022.\n- <span\
    \ id=\"page-13-15\"></span>[23] OpenHW-Group, \"CV32E40P,\" [https://github.com/](https://github.com/openhwgroup/cv32e40p)\
    \ [openhwgroup/cv32e40p,](https://github.com/openhwgroup/cv32e40p) 2023.\n- <span\
    \ id=\"page-13-16\"></span>[24] A. Kurth *et al.*, \"An Open-Source Platform for\
    \ High-Performance Non-Coherent On-Chip Communication,\" *IEEE Transactions on\
    \ Computers*, pp. 1–1, 2021.\n- <span id=\"page-13-17\"></span>[25] B. John, \"\
    HyperRAM as a low pin-count expansion memory for embedded systems,\" [https://www.infineon.com/dgdl/](https://www.infineon.com/dgdl/Infineon-HyperRAM_as_a_low_pin-count_expansion_memory_for_embedded_systems-Whitepaper-v01_00-EN.pdf?fileId=8ac78c8c7d0d8da4017d0fb28970272c&da=t)\
    \ [Infineon-HyperRAM](https://www.infineon.com/dgdl/Infineon-HyperRAM_as_a_low_pin-count_expansion_memory_for_embedded_systems-Whitepaper-v01_00-EN.pdf?fileId=8ac78c8c7d0d8da4017d0fb28970272c&da=t)\
    \ as a low pin-count expansion memory for embedded [systems-Whitepaper-v01](https://www.infineon.com/dgdl/Infineon-HyperRAM_as_a_low_pin-count_expansion_memory_for_embedded_systems-Whitepaper-v01_00-EN.pdf?fileId=8ac78c8c7d0d8da4017d0fb28970272c&da=t)\
    \ 00-EN.pdf?fileId= [8ac78c8c7d0d8da4017d0fb28970272c&da=t,](https://www.infineon.com/dgdl/Infineon-HyperRAM_as_a_low_pin-count_expansion_memory_for_embedded_systems-Whitepaper-v01_00-EN.pdf?fileId=8ac78c8c7d0d8da4017d0fb28970272c&da=t)\
    \ 2020.\n- <span id=\"page-13-18\"></span>[26] AMD, \"Zynq-7000 - Technical Reference\
    \ Manual, UG585,\" [https:](https://docs.xilinx.com/r/en-US/ug585-zynq-7000-SoC-TRM)\
    \ [//docs.xilinx.com/r/en-US/ug585-zynq-7000-SoC-TRM.](https://docs.xilinx.com/r/en-US/ug585-zynq-7000-SoC-TRM)\n\
    - <span id=\"page-13-19\"></span>[27] A. Noami, B. Pradeep Kumar, and P. Chandrasekhar,\
    \ \"High Performance AXI4 Interface Protocol for Multi-Core Memory Controller\
    \ on SoC,\" in *Data Engineering and Communication Technology*, K. A. Reddy, B.\
    \ R. Devi, B. George, and K. S. Raju, Eds. Singapore: Springer Singapore, 2021,\
    \ pp. 131–140.\n- <span id=\"page-13-20\"></span>[28] D. Rossi, I. Loi, G. Haugou,\
    \ and L. Benini, \"Ultra-low-latency lightweight dma for tightly coupled multi-core\
    \ clusters,\" in *Proceedings of the 11th ACM Conference on Computing Frontiers*,\
    \ ser. CF '14. New York, NY, USA: Association for Computing Machinery, 2014. [Online].\
    \ Available:<https://doi.org/10.1145/2597917.2597922>\n- <span id=\"page-13-21\"\
    ></span>[29] ARM, \"AMBA AXI Protocol Specification,\" [https://developer.arm.](https://developer.arm.com/documentation/ihi0022/j/?lang=en)\
    \ [com/documentation/ihi0022/j/?lang=en,](https://developer.arm.com/documentation/ihi0022/j/?lang=en)\
    \ 2022.\n- <span id=\"page-13-22\"></span>[30] Xilinx-AMD, \"Dual Port SRAM specifications,\"\
    \ [https://docs.xilinx.com/r/2022.1-English/](https://docs.xilinx.com/r/2022.1-English/ug1483-model-composer-sys-gen-user-guide/Dual-Port-RAM)\
    \ [ug1483-model-composer-sys-gen-user-guide/Dual-Port-RAM.](https://docs.xilinx.com/r/2022.1-English/ug1483-model-composer-sys-gen-user-guide/Dual-Port-RAM)\n\
    - <span id=\"page-13-23\"></span>[31] PULP, \"HyperRAM Controller RTL,\" [https://github.com/](https://github.com/pulp-platform/hyperbus)\
    \ [pulp-platform/hyperbus,](https://github.com/pulp-platform/hyperbus) 2022.\n\
    - <span id=\"page-13-24\"></span>[32] Infineon, \"HyperRAM RTL,\" [https://www.](https://www.infineon.com/dgdl/Infineon-S27KL0641_S27KS0641_VERILOG-SimulationModels-v05_00-EN.zip?fileId=8ac78c8c7d0d8da4017d0f6349a14f68)\
    \ [infineon.com/dgdl/Infineon-S27KL0641](https://www.infineon.com/dgdl/Infineon-S27KL0641_S27KS0641_VERILOG-SimulationModels-v05_00-EN.zip?fileId=8ac78c8c7d0d8da4017d0f6349a14f68)\
    \ S27KS0641 [VERILOG-SimulationModels-v05](https://www.infineon.com/dgdl/Infineon-S27KL0641_S27KS0641_VERILOG-SimulationModels-v05_00-EN.zip?fileId=8ac78c8c7d0d8da4017d0f6349a14f68)\
    \ 00-EN.zip?fileId= [8ac78c8c7d0d8da4017d0f6349a14f68,](https://www.infineon.com/dgdl/Infineon-S27KL0641_S27KS0641_VERILOG-SimulationModels-v05_00-EN.zip?fileId=8ac78c8c7d0d8da4017d0f6349a14f68)\
    \ 2022.\n- <span id=\"page-13-25\"></span>[33] Infineon, \"HyperBUS specifications,\"\
    \ [https://www.](https://www.infineon.com/dgdl/Infineon-HYPERBUS_SPECIFICATION_LOW_SIGNAL_COUNT_HIGH_PERFORMANCE_DDR_BUS-AdditionalTechnicalInformation-v09_00-EN.pdf?fileId=8ac78c8c7d0d8da4017d0ed619b05663)\
    \ [infineon.com/dgdl/Infineon-HYPERBUS](https://www.infineon.com/dgdl/Infineon-HYPERBUS_SPECIFICATION_LOW_SIGNAL_COUNT_HIGH_PERFORMANCE_DDR_BUS-AdditionalTechnicalInformation-v09_00-EN.pdf?fileId=8ac78c8c7d0d8da4017d0ed619b05663)\
    \ SPECIFICATION LOW SIGNAL COUNT HIGH [PERFORMANCE](https://www.infineon.com/dgdl/Infineon-HYPERBUS_SPECIFICATION_LOW_SIGNAL_COUNT_HIGH_PERFORMANCE_DDR_BUS-AdditionalTechnicalInformation-v09_00-EN.pdf?fileId=8ac78c8c7d0d8da4017d0ed619b05663)\
    \ DDR [BUS-AdditionalTechnicalInformation-v09](https://www.infineon.com/dgdl/Infineon-HYPERBUS_SPECIFICATION_LOW_SIGNAL_COUNT_HIGH_PERFORMANCE_DDR_BUS-AdditionalTechnicalInformation-v09_00-EN.pdf?fileId=8ac78c8c7d0d8da4017d0ed619b05663)\
    \ 00-EN.pdf?fileId= [8ac78c8c7d0d8da4017d0ed619b05663,](https://www.infineon.com/dgdl/Infineon-HYPERBUS_SPECIFICATION_LOW_SIGNAL_COUNT_HIGH_PERFORMANCE_DDR_BUS-AdditionalTechnicalInformation-v09_00-EN.pdf?fileId=8ac78c8c7d0d8da4017d0ed619b05663)\
    \ 2022.\n- <span id=\"page-13-26\"></span>[34] R. Wilhelm *et al.*, \"The worst-case\
    \ execution-time problem—overview of methods and survey of tools,\" *ACM Trans.\
    \ Embed. Comput. Syst.*, vol. 7, no. 3, may 2008. [Online]. Available: <https://doi.org/10.1145/1347375.1347389>\n\
    - <span id=\"page-13-27\"></span>[35] T. Mitra, J. Teich, and L. Thiele, \"Time-critical\
    \ systems design: A survey,\" *IEEE Design & Test*, vol. 35, no. 2, pp. 8–26,\
    \ 2018.\n- <span id=\"page-13-28\"></span>[36] F. Restuccia *et al.*, \"Modeling\
    \ and analysis of bus contention for hardware accelerators in FPGA SoCs,\" in\
    \ *32nd Euromicro Conference on Real-Time Systems (ECRTS 2020)*. Schloss Dagstuhl-Leibniz-Zentrum\
    \ fur Informatik, 2020. ¨\n- <span id=\"page-13-32\"></span>[37] B. Thomas *et\
    \ al.*, \"AXI-REALM: A Lightweight and Modular Interconnect Extension for Traffic\
    \ Regulation and Monitoring of Heterogeneous Real-Time SoCs,\" in *2024 Design,\
    \ Automation & Test in Europe Conference & Exhibition (DATE)*, 2024.\n- <span\
    \ id=\"page-13-29\"></span>[38] J. P. Cerrolaza *et al.*, \"Multi-Core Devices\
    \ for Safety-Critical Systems: A Survey,\" *ACM Comput. Surv.*, vol. 53, no. 4,\
    \ aug 2020. [Online]. Available:<https://doi.org/10.1145/3398665>\n- <span id=\"\
    page-13-30\"></span>[39] M. Schoeberl *et al.*, \"T-CREST: Time-predictable multi-core\
    \ architecture for embedded systems,\" *Journal of Systems Architecture*, vol.\
    \ 61, no. 9, pp. 449–471, 2015.\n- <span id=\"page-13-31\"></span>[40] G. Fernandez\
    \ *et al.*, \"Increasing confidence on measurement-based contention bounds for\
    \ real-time round-robin buses,\" in *Proceedings of the 52nd Annual Design Automation\
    \ Conference*, ser. DAC '15. New York, NY, USA: Association for Computing Machinery,\
    \ 2015.\n\n**Luca Valente** received the MSc degree in electronic engineering\
    \ from the Politecnico of Turin in 2020. He is currently a PhD student at the\
    \ University of Bologna in the Department of Electrical, Electronic, and Information\
    \ Technologies Engineering (DEI). His main research interests are hardware-software\
    \ co-design of heterogeneous SoCs.\n\n**Francesco Restuccia** received a PhD degree\
    \ in computer engineering (cum laude) from Scuola Superiore Sant'Anna Pisa, in\
    \ 2021. He is a postdoctoral researcher at the University of California, San Diego.\
    \ His main research interests include hardware security, on-chip communications,\
    \ timing analysis for heterogeneous platforms, cyber-physical systems, and time-predictable\
    \ hardware acceleration of deep neural networks on commercial FPGA SoC platforms.\n\
    \n**Davide Rossi** received the Ph.D. degree from the University of Bologna in\
    \ 2012. He has been a Post-Doctoral Researcher with the Department of Electrical,\
    \ Electronic and Information Engineering \"Guglielmo Marconi,\" University of\
    \ Bologna, since 2015, where he is currently an Associate Professor position.\
    \ His research interests focus on energy-efficient digital architectures. In this\
    \ field, he has published more than 100 papers in international peer-reviewed\
    \ conferences and journals.\n\n**Ryan Kastner** is a professor in the Department\
    \ of Computer Science and Engineering at UC San Diego. He received a PhD in Computer\
    \ Science at UCLA, a masters degree in engineering and bachelor degrees in Electrical\
    \ Engineering and Computer Engineering from Northwestern University. His current\
    \ research interests fall into three areas: hardware acceleration, hardware security,\
    \ and remote sensing.\n\n**Luca Benini** holds the chair of Digital Circuits and\
    \ Systems at ETHZ and is Full Professor at the Universita di Bologna. He received\
    \ a PhD from ` Stanford University. His research interests are in energy-efficient\
    \ parallel computing systems, smart sensing micro-systems and machine learning\
    \ hardware. He has published more than 1000 peer-reviewed papers and 5 books.\
    \ He is a Fellow of the ACM and a member of the Academia Europaea."
- title: '**A Comprehensive Review of the Influence of Technology on Psychology**'
  abstract: ''
  keywords: ''
  document: "![](_page_0_Picture_0.jpeg)\n\n**International Journal of Scientific\
    \ Research in \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\\
    _\\_\\_\\_\\_\\_\\_ Review Paper. Multidisciplinary Studies E-ISSN:** 2454-9312\
    \ Vol.7, Issue.9, pp.60-65, September (2021) **P-ISSN:** 2454-6143\n\n# **A Comprehensive\
    \ Review of the Influence of Technology on Psychology**\n\n**S. Bhattacharya1\\\
    *, S. Samaddar<sup>2</sup> , A. Banerjee<sup>3</sup>**\n\n<sup>1</sup>Masters\
    \ of Technology, Department of Electronics and Communication Engineering (VLSI\
    \ design), Heritage Institute of Technology, Kolkata, India\n\n<sup>2</sup>Bachelor\
    \ of Arts, Department of Psychology, Bagbazar Women's College, University of Calcutta,\
    \ Kolkata, India\n\n<sup>3</sup>Bachelor of Technology, Department of Electronics\
    \ and Communication Engineering, Techno India University, Kolkata,\n\nIndia\n\n\
    *\\*Corresponding Author: [sohambhattacharya36@gmail.com,](mailto:sohambhattacharya36@gmail.com)\
    \ Tel: +91 8910018323* \n\n#### **Available online at[: www.isroset.org](http://www.isroset.org/)**\n\
    \n## Received: 18/Sept/2021, Accepted: 20/Sept/2021, Online: 30/Sept/2021\n\n\
    *Abstract-* Psychology is the logical investigation of the mind and its behavioral\
    \ activities. Analysts are effectively engaged with contemplating and understanding\
    \ mental cycles, cerebrum capacities, and conduct. The area of Psychology is considered\
    \ as a \"Hub Science\" with solid associations with the clinical sciences, social\
    \ sciences, education, and much more. Technology is the branch of knowledge concerned\
    \ with the development and application of specialized tools, as well as their\
    \ interactions with life, society, and the environment, drawing on areas such\
    \ as mechanical engineering, applied science, and pure science. This study aims\
    \ to shed light on the relationship between the field of psychology and technological\
    \ progress nowadays. This paper also provides a detailed investigation of the\
    \ effect of technology on the discipline of psychology.\n\n*Keywords-* Psychology,\
    \ Technology, Machine Learning, Artificial Intelligence, Nanotechnology.\n\n#\
    \ **I. INTRODUCTION**\n\nPsychology is the coherent examination of how people\
    \ act, think, and feel. Sometime before 400-500 years B.C., Socrates, Plato, Aristotle\
    \ underscored the philosophical direction. Freedom of thought versus determinism,\
    \ memory; nature versus support, the fascination was their spaces of conversation.\
    \ In the early days, there were two predominant viewpoints: structuralism and\
    \ functionalism. It is said that the name 'structuralism' was spearheaded by Wilhelm\
    \ Wundt (1832-1920). The methodology predominantly centered on the breaking of\
    \ the psychological cycles into the most fundamental parts. Wundt's idea was huge\
    \ because it recognized psychology from theory, considering a more coordinated\
    \ examination of the psyche's activities. Then again, William James, an American\
    \ psychologist, fostered one more methodology which came to be known as 'functionalism'.\
    \ Both structuralism and functionalism were repudiated. Our brains are continually\
    \ changing, as indicated by William James, so searching for the construction of\
    \ mindful experience is awkward; all things considered, he focused on how and\
    \ why a creature performs something, for example, the mind's motivation.\n\nSigmund\
    \ Freud, who was the father of psychology, gave us psychoanalysis. He accepted\
    \ that individuals might be treated by acquiring an understanding of their oblivious\
    \ thoughts and inspirations and making them cognizant. Sigmund Freud's therapy\
    \ was the first psychodynamic hypothesis, yet it envelops different speculations\
    \ dependent on his ideas, including Jung, Adler, and Erikson. Afterward, different\
    \ methodologies made a gigantic commitment to the area of psychology, specifically\
    \ the behaviorist methodology and the humanistic methodology. The behaviorist\
    \ technique, regularly known as 'behavioral psychology', is a hypothesis that\
    \ affirms that all practices are instructed through some type of contact with\
    \ the climate named molding. Behaviorism just spotlights improvement practices.\
    \ Afterward, the humanistic methodology turned into one more methodology in which\
    \ the essential spotlight is on emotional experience and self-awareness.\n\nDuring\
    \ the 1960s and 1970s, an intellectual upset dependent on lab tests started in\
    \ the discipline of psychology, with applications to memory, discernment, and\
    \ intellectual turn of events, mental sickness, and significantly more. Analysts\
    \ focus on everything about the human experience, from the basic activities of\
    \ the human brain to insight, memory, thinking, and language to the character\
    \ and enthusiastic prosperity. At whatever point any individual is managing any\
    \ sort of mental pressure, confusion, or any type of human conduct change, clinicians\
    \ partake in tackling issues in our day-to-day routines. As a piece of science,\
    \ psychology not just deals with the means of human reasoning and conduct, but\
    \ additionally assists with taking care of the issues confronted and managing\
    \ the imperative cycles. As technology grows from day to day, major research areas\
    \ are now focused on presenting the needs of human behavior to match them with\
    \ the rehabilitation of devices or machines. A major hot topic which is the cup\
    \ of tea of our daily lives is 'Engineering Psychology' or 'Human Factors Engineering'.\
    \ 'Engineering Psychology' is the study of human behavior and capability, which\
    \ applies to the design of any device or system, or technology upbringing [1].\
    \ It is an application of ergonomics that states the relationship of human beings\
    \ and machines by reforming types of equipment, interactions, or environments\
    \ they can take place.\n\nTechnology nowadays takes a great step towards making\
    \ an impact in the field of psychology through computer simulations, algorithms,\
    \ and applications. Different treatments are nowadays being provided to patients\
    \ with the help of technological growth or advancement. New issues have been created\
    \ to get further inspection of the research areas through the help of technology\
    \ in this immense field.\n\n## **II. RELATED WORK**\n\nSome definite objectives\
    \ or intentions can be said in the area of psychology that does not just guide\
    \ in molding the conduct of one just like others. It is shown that describing,\
    \ explaining, predicting, and changing others' manner and mental cycles are the\
    \ four fundamental motivations behind psychology.\n\n- •**To describe:** The main\
    \ objective of psychology is to describe and assist scientists in fostering certain\
    \ overall laws of human conduct.\n- •**To explain:** After describing, analysts\
    \ will generally clarify the course of the conduct, hence clarifying how or why\
    \ this happens.\n- •**To predict:** Through exact exploration, psychology plans\
    \ to foresee future behavior.\n- •**To change:** Once the entirety of the objectives\
    \ have been met, endeavoring to change or control conduct is conceivable. The\
    \ four points have a wide scope of utilization in our regular routines, including\
    \ business, marketing, education, medication, and self-improvement.\n\nTechnology\
    \ isn't simply changing how individuals interface with the world; it's additionally\
    \ changing how researchers concentrate on human conduct and also the brain. San\
    \ Francisco is a world-popular center of innovation and a fitting area for a discussion\
    \ on research on tech and the human experience. In a Cross-Cutting Theme Program\
    \ at the 30th APS Annual Convention, speakers introduced interdisciplinary work\
    \ on how innovation shapes learning, consideration, conduct, and our public activities\
    \ from youth through advanced age. Inperson treatment, for example, is not available\
    \ nowadays. Several apps are available through which patients and analysts can\
    \ converse with the help of websites, applications, and teleconferencing, which\
    \ is ultimately the growth of modern-day's technology. Moreover, treatments or\
    \ detection of any kind of stress or disorder nowadays is\n\nIn this current world,\
    \ we as individuals are turning out to be more careful about our emotional well-being\
    \ and, along these lines; we are offering significance to our state of mind, which,\
    \ in another way, assists us with acquiring compassion towards one another. With\
    \ the improvement of this field or this subject, we can comprehend or we can realize\
    \ that individuals can experience the ill effects of mental torment, enthusiastic\
    \ agony, and not simply actual torment. Many psychological applications focus\
    \ on protecting people from emotional and physical risks while also giving them\
    \ the basic mental transmission capacity to deal with the mental threats that\
    \ many people experience daily. An analyst can assist an individual with further\
    \ developing their dynamic, their capacity to think, and in particular, they can\
    \ allow the patient to discuss whatever they feel like. The investigation of psychology\
    \ is farreaching today, and various parts of psychology are generally perceived\
    \ and oftentimes utilized in many pieces of business.\n\nThe rise of computer\
    \ technology has characterized this mental function as information processing.\
    \ This gave rise to the cognitive dominant model of the mind, which is combined\
    \ with the internal mental beliefs and study of the mind. With the progress of\
    \ technology, neuropsychology and cognitive neuroscience have taken an immense\
    \ part in the most active areas in modern-day psychology. With the involvement\
    \ of different fields like computer science, philosophy, and neuroscience, cognitive\
    \ science has created a great impact on such studies in a positive way [2].\n\n\
    In [3], it has been stated that technology has turned into a crucial power in\
    \ forming the personality, intellectual, and full of feeling measures, and social\
    \ exercises of our understudies, customers, and exploration members. Though the\
    \ family was once, by a wide margin, the most significant climate for molding\
    \ mentalities and convictions, today's teenagers are exposed to a lot more good\
    \ examples, values, perspectives, and decisions than at any other time. Computer\
    \ games permit individuals to build and experience augmented realities unconstrained\
    \ by the standards and upsides of general society (Gentile, Saleem, and Anderson,\
    \ 2007). The Internet upholds the framing of elective networks (Turkle, 1996)\
    \ around shared thoughts as opposed to as if they were through actual contact.\
    \ Virtual people groups can provide friendship for people who are unable to connect\
    \ with others in their immediate surroundings.\n\nPsychology has never been held\
    \ back by technology; rather, it has always attempted to apply itself to everyday\
    \ practices. If we look into the history of science, we can find out that many\
    \ psychologists, psychiatrists, and neurologists created devices based on technological\
    \ evolution over time. Many scientists, analysts, and\n\n#### Int. J. Sci. Res.\
    \ in Multidisciplinary Studies Vol.**7**, Issue.**9**, Sept **2021**\n\nphysicians\
    \ have used magnets, sensors, and computer programming languages to detect several\
    \ mental disorders. For example, Frank Mesmer, an Australian physician, used magnets\
    \ to detect human mental disorders in the eighteenth century. Then, he created\
    \ his famous \"health tanks\" and they were used by the patients to apply one\
    \ of these points to the aching side.\n\n## **III. METHODOLOGY AND THE BACKGROUND**\n\
    \nTechnology makes it possible to get numerous sets of data in real-time, including\
    \ self-report, physiological data, observed behavior, and so on. Scientists are\
    \ always attempting to bridge the gap between human feelings and technological\
    \ advancement. Why do we need computers to comprehend humans, when we're talking\
    \ about technology and psychology? For example, if a system does not recognize\
    \ that the user is becoming annoyed, the user may find it more difficult to use\
    \ the system. According to studies, when dealing with humans and robots, robots\
    \ who apologize more are significantly more likely to be favored than robots who\
    \ do not apologize.\n\nCell phones and wearable devices (e.g., smartwatches, fitness\
    \ trackers) make it simple to collect data as it happens, rather than relying\
    \ on self-reporting later. Human memory is fallible, and therefore the quicker\
    \ the knowledge is entered, the more likely it's accurate. Some software systems\
    \ mechanically enter data, like programs that measure exercise and sleep habits.\
    \ Alternative data must still be entered manually (for example, a food log), but\
    \ it can be done more quickly and conveniently on a phone we carry with us all\
    \ the time. Technology additionally permits researchers to perform measurements\
    \ in natural settings instead of being confined to the research laboratory.\n\n\
    Sensors in little devices that we use every day play an important part in tracking\
    \ and measuring human behavior, such as blood pressure, heart rate, sleep tracking,\
    \ dozing, skin conductance, and much more, but the issue remains as to what we\
    \ will do with all of this information. According to Dr. Eric Topol, author of\
    \ \"*The Creative Destruction of Medicine: How the Digital Revolution Will Create\
    \ Better Health Care,*\" when information about something is constantly returned\
    \ to a person, he or she becomes aware of stress or health abnormalities that\
    \ they were previously unaware of.\n\nTechnology has introduced new ways of accumulating\
    \ data, some of which are tremendous upgrades over more seasoned techniques. One\
    \ of the most challenging aspects of psychological research is repeating the results.\
    \ So, as technology is moving ahead, scientists have created some special instruments\
    \ which can obtain précised measurements from the larger samples. The bigger the\
    \ number of samples, the higher the measurement capacity. For example, computer-generated\
    \ reality tech permits analysts to assemble information without really going to\
    \ a particular climate. In addition to the fact that this is less costly and more\
    \ helpful, it likewise disposes of certain moral concerns and dependence on self-report.\
    \ Capacity innovation has permitted the advancement of immense data sets of data.\
    \ A large number of these index human conduct that can be utilized in the investigation\
    \ of psychology. For instance, data sets have data on everything from wrongdoing\
    \ measurements to lack of sleep. Moreover, these data sets gather data from a\
    \ huge and varied population, making them ideal for fulfilling legitimacy concerns.\
    \ This presents clinicians with colossal stores of data in which to investigate.\n\
    \n Nowadays, several methods have been used to detect various disorders or changes\
    \ in the human psyche or behavior. Some of them are Artificial Intelligence, Machine\
    \ Learning, Deep Learning, and much more. Using these technological instances,\
    \ nowadays, psychologists, scientists, and analysts have been creating different\
    \ devices to detect different conflicts in the human mind or behavior. These innovations\
    \ have stepped up with a great movement in recent years to detect several disorders,\
    \ stress, and changes in humans. Recognizing emotional information necessitates\
    \ the collection of data and the identification of various patterns, which is\
    \ accomplished with the use of machine learning, which processes voice recognition,\
    \ facial detection, and other tasks. 'Affective computing' comes into play here.\
    \ Rosalind Picard's 1995 work on emotional computing gave birth to the more contemporary\
    \ area of computer science. It's the study of machines and systems that can detect\
    \ and analyze human emotions. Changes in a user's language, tone of voice, and\
    \ variations in facial expression are detected and responded to by affective computing-enabled\
    \ devices. They do it by gathering data from users via physical sensors such as\
    \ video cameras and microphones and evaluating it using past experiences and data\
    \ sets. As a result of its activities and growth in recent years, technology is\
    \ revolutionizing the area of psychology.\n\n![](_page_2_Picture_10.jpeg)\n\n\
    Source: \"How is Technology changing Psychology?\" (2021) Figure 1: The effect\
    \ of Technology on the Human Mind.\n\n#### **IV. IN-DEPTH DISCUSSION OF PSYCHOLOGY\
    \ IN RELATION TO TECHNOLOGY**\n\nIn [4], the authors have stated that throughout\
    \ the nineteenth and twentieth century, a wide range of technologies has been\
    \ created to check the human state of mind. The authors also gave an example of\
    \ a subject that is whether telling the truth or lying using technology. Furthermore,\
    \ several computing technologies have recently been developed which use different\
    \ computer algorithms and face recognition software to detect the state of the\
    \ human mind.\n\nBig Data is the arrangement of innovations made to store, dissect\
    \ and deal with this mass information, a large-scale apparatus made to recognize\
    \ designs in the disorder of this blast in data to configuration keen arrangements.\
    \ It is now used in fields as diverse as medicine, farming, gambling, and environmental\
    \ insurance. In [5], the author has stated the use of 'Big Data' upon psychological\
    \ science. The author also stated an example relating it to the daily crime statistics\
    \ to the environmental state. Data sets exist that consider detailed investigations\
    \ of the impacts of conventional mental ward factors like lack of sleep, cold,\
    \ and abundance levels on wonders identified with everything from working memory\
    \ to hazard perspectives — and at just remarkable scales.\n\nIn 1956, a small\
    \ group of experts from diverse areas, including mathematics, psychology, engineering,\
    \ economics, and political science, founded the subject of artificial intelligence\
    \ study. They started talking about creating an artificial brain. Neurological\
    \ studies have recently revealed that the brain is an electrical network of neurons\
    \ that fire in all or nothing pulses. Artificial intelligence systems prefer to\
    \ make choices based on realtime data. The objective of artificial intelligence\
    \ is to give programming that can reason and include and clarify output. Artificial\
    \ intelligence helps in furnishing humanlike connections with programming and\
    \ proposition choice help for explicit undertakings, but it's not a replacement\
    \ for the human mind and behavior and will not be at any point shortly. Forms\
    \ of Artificial Intelligence are classified into four parts, such as reactive\
    \ machines, limited memory, theory of mind, and self-awareness. In [6], the authors\
    \ state that the need for artificial intelligence towards the lifestyle of human\
    \ beings. In contemporary times, Artificial Intelligence has utilized its methods\
    \ to create systems that can act, think, and learn just like human beings. It\
    \ is stated that artificial intelligence is an integral part of our existence.\
    \ For example, starting from our phone, whenever anyone searches for any music\
    \ videos on any music streaming platform, the system, which is specialized in\
    \ those Artificial Intelligence methodologies, recommends other music videos related\
    \ to that person's preferences. So, using artificial intelligence, a machine can\
    \ perform the same task as the human thought process.\n\n Mainly, artificial intelligence\
    \ is very much related to human psychology. It mainly focuses on human neurological\
    \ functions. A new form of psychology that was established in the late 19th century\
    \ is known as \"Artificial Psychology\" [7] and it is with the recent utilization\
    \ of Artificial Intelligence. In [8,9], the authors have analyzed human psychology\
    \ methods with the help of information science research methods and artificial\
    \ intelligence to investigate more deeply human mind philosophy. In 1963, Dan\
    \ Curtis considered artificial intelligence, which approaches the level of complex\
    \ analyzed intelligence is measured based on two conditions:\n\n## **The first\
    \ condition is:**\n\n- Makes all of the decisions independently.\n- Makes any\
    \ decision based on data that is new, abstract, or incomplete.\n- Capable of reprogramming\
    \ the new data.\n- It is capable of solving its programming disputes, even if\
    \ the data is incomplete.\n\n# **Condition II:**\n\n All the criteria are not\
    \ in line with the original operating system.\n\n![](_page_3_Picture_14.jpeg)\n\
    \nSource: \"Implementing Artificial Intelligence\" (2021) Figure 2: The interconnection\
    \ of Artificial Intelligence with the human touch.\n\nNow, the facts about how\
    \ machine learning is helpful in the field of psychology. Machine learning is\
    \ nothing but a sub-field of artificial intelligence through which it entertains\
    \ any system or device which can learn from data, identify patterns, and make\
    \ decisions with minimal human intervention. In [10], the authors have shown the\
    \ possibilities of some prediction of Internet addiction based on a set of predictor\
    \ variables. The indicator variable set was chosen with the end goal of ensuring\
    \ that there exists a solid connection between the boundaries considered to have\
    \ an impact on tricky Internet utilization.\n\nBipolar disorder is a mental illness\
    \ with lots of mood swings and emotional highs and lows. Whenever any person becomes\
    \ depressed, he or she faces instant mood highs and lows and loses interest in\
    \ any activities. The authors of [11] used a random Forest Algorithm with Magnetic\
    \ Resonance Imaging (MRI) data to detect this disorder. The author had also shown\
    \ that CNN-MDRP (Multimodal Disease Risk Prediction) had higher accuracy in predicting\
    \ than other algorithms using machine learning for Bipolar Disorder detection.\n\
    \n Web-based intellectual conduct treatment has shown positive outcomes for an\
    \ assortment of mental problems, including misery, uneasiness, and post-awful\
    \ pressure issues (PTSD). There is a blast of web-related psychotherapeutic treatment.\
    \ Quite a bit of this is gotten to through sites and applications. Although some\
    \ applications are just conductors to teletherapy administrations, many are crossovers\
    \ that offer education, self-improvement, and online help on a case-by-case basis.\
    \ Most web-based treatments utilize intellectual conduct standards.\n\nSeveral\
    \ apps in today's technology world are available to help people with their mental\
    \ disorders, behavioral changes, and many more. Some surveys have been conducted\
    \ on a daily or monthly basis to detect the changes in human behavior, to detect\
    \ their stress and depression with percentages. For example, an application named\
    \ \"Code Blue\" was created to help teenagers suffering from depression and bullying\
    \ with support whenever they need it. As the commonness of psychological instabilities\
    \ like depression and anxiety keeps on developing, clinicians have gone to versatile\
    \ applications as instruments for helping their patients' treatment. These applications\
    \ can be particularly useful for teens and youthful grown-ups experiencing psychological\
    \ instability because of their incessant utilization of innovation as a method\
    \ for correspondence.\n\n[From an internet source of the \"Top 10 Mental Apps\"\
    ]\n\n Also, in 2018, Melina Uncapher of the University of California stated in\
    \ a report \"Technology meets Neuroscience\" that it's not possible to bring an\
    \ MRI scanner to every classroom, but they have turned it into a mobile technology\
    \ so that everyone can access it. The Neuroscape Center at UCSF has created 'ACE',\
    \ a tabletbased intellectual evaluation, which has permitted Uncapher and her\
    \ partners to concentrate on leader work inside a gathering of more than 1,000\
    \ rudimentary and center school understudies across nine diverse Bay Area schools.\
    \ These tweaked Neuroscape computer games utilize versatile calculations to change\
    \ the degree of game trouble, permitting analysts to utilize similar precise intellectual\
    \ assignments for offspring of any age across tests, and across time. Fundamentally,\
    \ this permits highaccuracy, high-dimensional estimation of insight across improvement.\n\
    \nIn this modern era of technology, the relationship between machines and humans\
    \ is very diverse and emergent in all fields. The authors of [12] have reviewed\
    \ the effects of technology on humans. The authors claimed that the situational\
    \ changing of human behavior affected most of the interactions between machines\
    \ and humans. The authors showed that the branch of Engineering Psychology helped\
    \ to improve the relationship between humans and machines by interacting with\
    \ each other. The authors also reviewed the existing research in the field of\
    \ Engineering Psychology.\n\nThe authors of [13] developed a mobile application\
    \ that can analyze mental illnesses based on lifestyle and psychometric data from\
    \ people of all ages. The authors also proposed an ideology on how Artificial\
    \ Neural Networks (ANN) can be applied to the dataset to analyze and detect the\
    \ type of mental disorder it contains.\n\nIn [14], the author has utilized a dataset\
    \ comprised of substance abuse patients from the United States and the Convolutional\
    \ Neural Organization characterization comprised of opioid addicts to get the\
    \ precision of the psychological issue forecast framework. The author has also\
    \ proposed a new method for detecting mental disorders using data mining.\n\n\
    Another field that has recently attracted the attention of the contemporary field\
    \ is \"Nanotechnology\". \"Nanotechnology\" is the study of matter at the nanoscale,\
    \ with dimensions ranging from 1 to 100 nanometers. Nanotechnology involves imaging,\
    \ measuring, and modeling. Nanotechnology encompasses disciplines such as physics,\
    \ biology, chemistry, and nanometer-scale technology. In psychiatry, nanotechnology\
    \ has a wide range of uses. Pharmacology, living analysis, and central nervous\
    \ system modeling are the three most common applications. If properly used, nanotechnology\
    \ and quantum physics can be used to create artificial intelligence and mental\
    \ disease models [15]. Psychology is linked with several areas, or we might say\
    \ that it incorporates every significant technological advancement [16]. The National\
    \ Science Foundation has invested heavily in building a foundation on which nanotechnologies\
    \ can be further developed. The National Institutes of Health, on the other hand,\
    \ is putting in a lot of effort because of the potential for medical applications.\
    \ The National Nanotechnology Initiative, on the other hand, involves more than\
    \ a dozen agencies. With the advancement of nanotechnology, psychologists interested\
    \ in the dissemination of innovation, decision-making, and social impact should\
    \ dive right in. Certain psychological groups have identified nanotechnology as\
    \ a powerful tool for addressing challenges related to human cognition, perception,\
    \ emotion, and activity. Day by day, new technologies emerge, which will ultimately\
    \ aid the study of psychology. Psychophysiological recording, for example, accurately\
    \ supplies visual and auditory stimuli as well as response time assessment.\n\n\
    #### **V. CONCLUSION**\n\nThe exploration of this article is an entry through\
    \ which the greatness of technology can be moved forward through its innovations\
    \ in the field of psychology. Starting from the usage of smartphones, applications,\
    \ websites to the concept of using technology for medical as well as mental illness\
    \ treatments of human minds, technology has created a greater impact and it is\
    \ also progressing daily through several innovations and research. Different methodological\
    \ innovations provide scientists with fresh ways to deal with the mysteries of\
    \ human minds in research labs everywhere on the planet. These days, various medications\
    \ are being given to patients with the assistance of mechanical development or\
    \ progression. As a whole, we are welcoming this immense innovation for the growth\
    \ of the field of psychology in several spheres every day. It has prompted advancement\
    \ in treatment, education, estimation, and exploration. Technology, as a rule,\
    \ gives a more advantageous and less expensive elective when utilized for assessment\
    \ and treatment purposes. Perhaps most importantly, it has enabled more people\
    \ to acquire data and access emotional wellness administrations. The major goal\
    \ is to develop new technologies that will help psychologists work more correctly\
    \ and efficiently in the future.\n\n#### **ACKNOWLEDGMENT**\n\nWe would like to\
    \ thank all of our professors and our prestigious institutions for providing us\
    \ with the information we needed to conduct the research.\n\n#### **REFERENCES**\n\
    \n- [1] Wikipedia on *\"Engineering Psychology\"*.\n- [2] Mandler, G., \"*A history\
    \ of modern experimental psychology: From James and Wundt to cognitive science*.\"\
    \ **Cambridge**, MA: MIT Press, **2007**.\n- [3] Annual Report of the APA Policy\
    \ and Planning Board on *\"How Technology Changes Everything (Or Nothing) in Psychology\"\
    ,*  **2008**.\n- [4] *\"Mind Reading as Cultural Practice\",* International Conference\
    \ to be held at the Institute for Cultural Theory and History, Humboldt University\
    \ Berlin, **Germany**, **22-23 March 2018**, Laurens Schlicht and Christian Fassung\
    \ (Humboldt University Berlin, Germany), Simone Natale (Loughborough University,\
    \ UK).\n- [5] *\"Technology, Psychology, and a Coming Revolution in the Study\
    \ of Decision Making\",* Paul W. Glimcher, **2014**.\n- [6] \"*Artificial psychology:\
    \ an attainable scientific research on the human brain*\", (**1999**), Proceedings\
    \ of the Second International Conference on Intelligent Processing and Manufacturing\
    \ of Materials. IPMM'99 (Cat. No.99EX296), Intelligent Processing and Manufacturing\
    \ of Materials, **1999**. IPMM '**99**. Proceedings of the Second International\
    \ Conference On, **1067**.\n- [7] Wikipedia on *\"Artificial Psychology\".*\n\
    - [8] Wang*, Zhiliang, Smith, Michael J.; Salvendy, Gavriel (eds.). \"Artificial\
    \ Psychology\". Human Interface and the Management of Information. Methods, Techniques,\
    \ and Tools in Information Design.\",* Lecture Notes in Computer Science. Springer\
    \ Berlin Heidelberg**. 4557**: **208–217**, **2007**.\n- [9] *Zhiliang Wang; Lun\
    \ Xie \"Artificial psychology: an attainable scientific research on the human\
    \ brain\",* Proceedings of the Second International Conference on Intelligent\
    \ Processing and Manufacturing of Materials, IPMM'99 (Cat. No.99EX296). **2**:\
    \ **1067–1072** vol.**2**, **July 1999.**\n- [10] Suma S.N., Nataraja P., Sharma\
    \ M.K *\"Internet Addiction Predictor: Applying Machine Learning in Psychology\"\
    .* In: Chiplunkar N., Fukao T. (eds) Advances in Artificial Intelligence and Data\
    \ Engineering. Advances in Intelligent Systems and Computing, vol. **1133**. Springer,\
    \ **Singapore**, **2021**.\n- [11] *\"Detection of bipolar disorder using machine\
    \ learning with MRI\"*, Sudha Radha, **2021** International Semantic Intelligence\
    \ Conference: At **New Delhi**, **2021**.\n- [12] Grether, F, *\"Engineering psychology\
    \ in the United States\",* American Psychologist. **23 (10)**: **743–751**, **1962**.\n\
    - [13] *\"Prediction of Mental Disorder using Artificial Neural Network and Psychometric\
    \ Analysis\",* D.D. Sapkal, Chintan Mehta,\n\nMohit Nimgaonkar, Rohan Devasthale,\
    \ Shreyas Phansalkar, Chapter: Data Management, Analytics, and Innovation, **2020**.\n\
    \n- [14] *\"Predicting Mental Disorders Using Convolutional Neural Networks Classifier\"\
    ,* Karim Hashim Kraidi Al- Saedi, Journal of Physics Conference Series, **2021**.\n\
    - [15] G. Fond et.al Eur Neuropsychopharmacol: *\"Nanopsychiatry – the potential\
    \ role of nanotechnologies in the future of psychiatry: a systematic review\"\
    ,* National Library of Medicine, National Center of Biotechnology Information,\
    \ **2013**.\n- [16] Steven Breckler, \"*Small Science is big*\", Psychological\
    \ Science Agenda, **2008**, APA.\n\n#### **AUTHORS PROFILE**\n\n**Soham Bhattacharya**\
    \ received the degree of Masters of Technology from Heritage Institute of Technology,\
    \ Kolkata in 2020. Before that, he had done a Bachelor of Technology in Electronics\
    \ and Communication Engineering from the same institution. At present, he is currently\
    \ looking for a doctoral position in the field of\n\n![](_page_5_Picture_26.jpeg)\n\
    \nElectrical and Computer Engineering. He has published many international and\
    \ national articles in the field of VLSI and Electronics and Communication Engineering.\
    \ He had been awarded as 'Young Researcher' by the Institute of Scholars. His\
    \ fields of research interest include VLSI, Reversible Computing, Embedded systems,\
    \ Artificial Intelligence, Electronics, and Communication Engineering.\n\n**Srija\
    \ Samaddar** is currently pursuing a Bachelor of Arts in Psychology from Bagbazar\
    \ Women's College, Kolkata under the affiliation of Calcutta University. Her fields\
    \ of research interest include counseling psychology, clinical psychology. She\
    \ did internships on Basic Counseling skills,\n\n![](_page_5_Picture_29.jpeg)\n\
    \nClinical Psychology, Fashion Psychology, which was organized by Fortis Mental\
    \ Healthcare services.\n\n**Anwesha Banerjee** is currently pursuing a Bachelor\
    \ of Technology from Techno India University, Kolkata. Her fields of research\
    \ interest include Digital Circuits, VLSI, Artificial Intelligence, and Big Data.\n\
    \n![](_page_5_Picture_32.jpeg)"
- title: Use of large language model (LLM) to enhance content and structure of a school
    of dentistry LibGuide Emily P. Jones
  abstract: ''
  keywords: ''
  document: '96


    # Use of large language model (LLM) to enhance content and structure of a school
    of dentistry LibGuide Emily P. Jones


    *See end of article for authors'' affiliations.*


    A librarian used a large language model (LLM) to revise a dentistry subject LibGuide.
    Prompts were used to identify methods for optimizing navigational structure for
    usability, highlight library-specific information students need additional help
    with, and write summaries of page content. Post-revision, LibGuide access increased,
    and students provided anecdotal feedback that they perceive the changes positively.
    LLMs may enhance LibGuide discoverability and usability without adding significant
    time and resource burdens for librarians.


    Keywords: Generative AI; Artificial Intelligence (AI); LibGuides; Large Language
    Models


    Virtual Projects are published on an annual basis in the *Journal of the Medical
    Library Association* (*JMLA*) following an annual call for virtual projects in
    *MLAConnect* and announcements to encourage submissions from all types of libraries.
    An advisory committee of recognized technology experts selects project entries
    based on their currency, innovation, and contribution to health sciences librarianship.


    # BACKGROUND


    Large language models (LLMs) lik[e Chat-GPT](https://chatgpt.com/) [\[1\]](https://sciwheel.com/work/citation?ids=16941172&pre=&suf=&sa=0&dbf=0)
    and [Claude.ai](https://claude.ai/) [\[2\]](https://sciwheel.com/work/citation?ids=16941177&pre=&suf=&sa=0&dbf=0)
    are useful tools for summarizing, predicting, and generating text. These tools
    have potential to increase productivity and decrease the time burden of common,
    text-based tasks for librarians like LibGuide content creation.


    ## VIRTUAL PROJECT DESCRIPTION


    In June 2024, a librarian used an LLM, Claude.ai, to facilitate a major redesign
    of a [dentistry LibGuide.](https://guides.lib.unc.edu/dentistry) Through a series
    of prompts, the librarian consulted the LLM to generate introductions summarizing
    content of specific pages and to restructure the LibGuide, formerly organized
    by resource format. Screenshots of the LibGuide pre- and post-revision, as well
    as examples of prompts provided to, and responses received from, Claude.ai are
    accessibl[e via the author''s institutional repository.](https://cdr.lib.unc.edu/concern/scholarly_works/g445ct56k)


    There was a 131% increase in LibGuide access from June - September 2024 (n = 2,288)
    compared to the same period the year before (n = 989). To the author''s knowledge,
    no other changes were made that would significantly impact usage like new outreach
    or instruction. In addition to the increase in usage statistics, students have
    provided anecdotal feedback that they perceive the LibGuide to be more user-friendly
    and useful after the revision.


    # DISCUSSION


    LLMs are cost-effective, as most are free, low-cost, or institutionally provided,
    and time-saving. Large amounts of text can be generated in a matter of seconds,
    whereas comparable output by a librarian may take hours. Additionally, LLMs can
    be used across various aspects of medical librarianship across any discipline
    and can be used to generate or clarify text about complex research topics like
    systematic reviews and data management.


    While there are advantages to using LLMs like increasing efficiency and productivity,
    there are challenges as well. Concerns have been raised about accuracy of responses,
    privacy, and algorithm bias [\[3\].](https://sciwheel.com/work/citation?ids=15520745&pre=&suf=&sa=0&dbf=0)
    While LLMs are skilled at text-based tasks, they may not be able to adequately
    produce responses that require nuance, context, or complexity of thought. Therefore,
    it is best practice to review LLM responses for clarity and accuracy before using
    them. Additionally, LLM responses are highly dependent upon prompts received.
    Responses also change each time they''re provided, even if the same prompt is
    provided, whether by the same or different individuals.


    ### CONCLUSION


    This project demonstrates a practical example of how librarians can apply generative
    artificial intelligence (AI) technologies to routine tasks like LibGuide revision
    and content creation. Using LLMs to develop page


    DOI: dx.doi.org/10.5195/jmla.2025.2084


    introductions and to reorganize content resulted in a usage increase. While not
    causative, while not causative, this increase may be correlated to increased discoverability
    and usability from using generative AI developed text and suggestions. LLMs can
    enhance the instructional component of LibGuides without adding a significant
    time burden for the creator.


    #### REFERENCES


    - 1. OpenAI. ChatGPT-4 [Internet]. 2024 [cited 2024 Sep 18]. Available from[:
    https://chatgpt.com/.](https://chatgpt.com/)

    - 2. Anthropic. Claude 3.5 Sonnet [Internet]. 2024 [cited 2024 Sep 18]. Available
    from[: https://claude.ai/new.](https://claude.ai/new)

    - 3. Meyer JG, Urbanowicz RJ, Martin PCN, O''Connor K, Li R, Peng P-C, Bright
    TJ, Tatonetti N, Won KJ, Gonzalez-Hernandez G, Moore JH. ChatGPT and large language
    models in academia: opportunities and challenges. BioData Min. 2023 Jul 13;16(1):20.


    #### AUTHORS'' AFFILIATIONS


    Emily P. Jones[, epjones3@email.unc.edu,](mailto:epjones3@email.unc.edu) [https://orcid.org/0000-](https://orcid.org/0000-0002-4294-7564)
    [0002-4294-7564,](https://orcid.org/0000-0002-4294-7564) University of North Carolina
    at Chapel Hill (UNC) Health Sciences Library, Chapel Hill, NC


    *Received October 2024; accepted October 2024*


    ![](_page_1_Picture_10.jpeg)


    Articles in this journal are licensed under [a Creative](https://creativecommons.org/licenses/by/4.0/)
    [Commons Attribution 4.0 International License.](https://creativecommons.org/licenses/by/4.0/)


    ![](_page_1_Picture_12.jpeg)


    This journal is published by th[e University Library System](http://www.library.pitt.edu/)
    of the [University of Pittsburgh](http://www.pitt.edu/) as part of it[s D-Scribe](http://www.library.pitt.edu/d-scribe-digital-collections)  [Digital
    Publishing Program](http://www.library.pitt.edu/d-scribe-digital-collections)
    and is cosponsored by the [University of Pittsburgh Press.](http://upress.pitt.edu/)


    ISSN 1558-9439 (Online)


    ![](_page_1_Picture_18.jpeg)'
- title: A Survey on Effective Invocation Methods of Massive LLM Services
  abstract: Language models as a service (LMaaS) enable users to accomplish tasks
    without requiring specialized knowledge, simply by paying a service provider.
    However, numerous providers offer massive large language model (LLM) services
    with variations in latency, performance, and pricing. Consequently, constructing
    the cost-saving LLM services invocation strategy with low-latency and high-performance
    responses that meet specific task demands becomes a pressing challenge. This paper
    provides a comprehensive overview of the LLM services invocation methods. Technically,
    we give a formal definition of the problem of constructing effective invocation
    strategy in LMaaS and present the LLM services invocation framework. The framework
    classifies existing methods into four different components, including input abstract,
    semantic cache, solution design, and output enhancement, which can be freely combined
    with each other. Finally, we emphasize the open challenges that have not yet been
    well addressed in this task and shed light on future research.
  keywords: ''
  document: '# A Survey on Effective Invocation Methods of Massive LLM Services


    Can Wang<sup>1</sup> , Bolin Zhang<sup>1</sup> , Dianbo Sui<sup>1</sup> , Zhiying
    Tu<sup>1</sup> <sup>∗</sup> , Xiaoyu Liu <sup>1</sup> , Jiabao Kang <sup>1</sup>
    ,


    <sup>1</sup>Harbin Institute of Technology,


    23B903072@stu.hit.edu.cn, {brolin, tzy hit,suidianbo}@hit.edu.cn, 2201110719@stu.hit.edu.cn,
    18538796936@163.com


    ### Abstract


    Language models as a service (LMaaS) enable users to accomplish tasks without
    requiring specialized knowledge, simply by paying a service provider. However,
    numerous providers offer massive large language model (LLM) services with variations
    in latency, performance, and pricing. Consequently, constructing the cost-saving
    LLM services invocation strategy with low-latency and high-performance responses
    that meet specific task demands becomes a pressing challenge. This paper provides
    a comprehensive overview of the LLM services invocation methods. Technically,
    we give a formal definition of the problem of constructing effective invocation
    strategy in LMaaS and present the LLM services invocation framework. The framework
    classifies existing methods into four different components, including input abstract,
    semantic cache, solution design, and output enhancement, which can be freely combined
    with each other. Finally, we emphasize the open challenges that have not yet been
    well addressed in this task and shed light on future research.


    ### 1 Introduction


    Large Language Models (LLM) are becoming a fundamental tool for various natural
    language processing tasks [\[Yang](#page-8-0) *et al.*, [2023\]](#page-8-0), as
    they have shown amazing emergent abilities, like in-context learning, multi-step
    reasoning, instruction following and tool learning. Due to commercial reasons,
    the potential risk of misuse and expensive tuning cost, LLMs, such as GPT-3, GPT-4
    and Claude, are usually released as LLM services through application programming
    interface (API) instead of open sourcing model weights [Yu *et al.*[, 2023\]](#page-8-1),
    which is called Language Models as a Service (LMaaS).


    Via accessing these powerful LLMs as services through their opened API, novice
    users do not need to possess extensive computational resources and expertise in
    deep learning, as they can solve the tasks of interest by crafting task-specific
    input query. However, invoking LLM services is not free and using them for high-throughput
    applications can be very expensive. Estimated by Claudia Slowik, a business supporting
    15,000 customer interactions with text-davinci-003 could have a monthly cost exceeding
    \$14,400.


    <span id="page-0-0"></span>


    | Provider  | LLM                  | Input Cost | Output Cost |

    |-----------|----------------------|------------|-------------|

    | OpenAI    | gpt-4                | \$30.0     | \$60.0      |

    |           | gpt-4-turbo          | \$10.0     | \$30.0      |

    |           | gpt-3.5-turbo-1106   | \$1.00     | \$2.00      |

    | Anthropic | Claude-2.0           | \$11.02    | \$32.68     |

    |           | Claude-instant-1.2   | \$1.63     | \$5.51      |

    | AI21      | Jurassic-2 Ultra     | \$15.0     | \$15.0      |

    |           | Jurassic-2 Mid       | \$10.0     | \$10.0      |

    |           | Jurassic-2 Light     | \$3.00     | \$3.00      |

    | Textsynth | M2M100 1.2B          | \$0.15     | \$3.00      |

    |           | GPT-J 6B             | \$0.20     | \$5.00      |

    |           | Falcon 7B            | \$0.20     | \$5.00      |

    |           | Mistral 7B           | \$0.20     | \$2.00      |

    |           | Llama2 7B            | \$0.20     | \$2.00      |

    |           | Flan-T5-XXL          | \$0.20     | \$5.00      |

    |           | Falcon 40B           | \$3.30     | \$10.00     |

    | Cohere    | command              | \$1.00     | \$2.00      |

    |           | command-light        | \$0.30     | \$0.60      |

    | Baidu     | Llama-2-13B-Chat     | ¥6.00      | ¥6.00       |

    |           | Llama-2-70B-Chat     | ¥35.0      | ¥35.0       |

    |           | ERNIE-Bot 4.0        | ¥150       | ¥300        |

    |           | ChatGLM2-6B-32K      | ¥4.00      | ¥4.00       |

    |           | Llama-2-7B-Chat      | ¥4.00      | ¥4.00       |

    |           | ERNIE-Bot            | ¥12.0      | ¥12.0       |

    |           | BLOOMZ-7B            | ¥4.00      | ¥4.00       |

    |           | ERNIE-Bot-turbo-0922 | ¥8.00      | ¥12.0       |


    Table 1: Price list of different LMaaS. The cost is priced per 1 million tokens.
    Note that Baidu''s LLM services are priced in Chinese Yuan (), while other LLM
    services are priced in US Dollars (\$). The data updated to 24 January 2024.


    Typically, the cost of invoking LLM services consists of two components: (1) input
    cost (proportional to the length of the input prompt), (2) output cost (proportional
    to the length of the generated sequence). In Table [1,](#page-0-0) we present
    the cost associated with using 25 different LLM services from some top-tier providers,
    like OpenAI, Anthropic, AI21 and Textsynth. From the table, we could find that
    the costs of different LLM services can vary by up to two orders of magnitude:
    the input cost for 1 million tokens is \$10 for OpenAI''s GPT-4 but only \$0.2
    for Mistral 7B hosted by Textsyth.


    In addition to cost considerations, various factors, including performance for
    the same input query and response time, can also impact the user experience in
    the usage of LLM services. [Ahia *et al.*[, 2023;](#page-7-0) Lai *et al.*[, 2023\]](#page-7-1)
    find that different languages, prompt methods or the inclusion of simple


    <sup>∗</sup>Corresponding author.


    <span id="page-1-0"></span>![](_page_1_Figure_0.jpeg)


    Figure 1: Vision of efficient invocation strategy construction for massive LLM
    services.


    enhancements can also lead to notable alterations in performance. Meanwhile, [Chen
    *et al.*[, 2023\]](#page-7-2) discovers that affordable LLMs often complement
    expensive ones. For example, on the CoQA [\[Reddy](#page-8-2) *et al.*, 2019]
    dataset, GPT-4 makes a mistake about 11% of the questions, where cheaper and smaller
    GPJ-J can give right answers.


    Considering the heterogeneity in pricing does not necessarily correlate with the
    user experience, it is a great need to explore effective invocation methods for
    LLM services in practice. As shown in Figure [1,](#page-1-0) we expect to make
    use of massive LLM services to construct an effective invocation strategy according
    to different methods, meeting targets in different scenarios. To this end, we
    attempts to provide a comprehensive study of the development and recent advances
    on effective invocation methods in LMaaS. In detail, We first formalizes the task
    of constructing effective invocation strategy as a multiobjective optimization
    problem. This entails simultaneous consideration of latency, performance, and
    cost factors. Then, we propose a taxonomy to provide a unified view on effective
    invocation methods in LMaaS where the existing methods are categorized into: input
    abstract, semantic cache, solution design, and output enhancement. These four
    components can be flexibly combined and unified in a flexible framework. Finally,
    we highlight the challenges and potential directions and hope our work can provide
    a useful roadmap for beginners interested in this area and shed light on future
    research.


    The contributions of this survey can be concluded as follows:


    - Comprehensive Taxonomy. As shown in Figure [2,](#page-2-0) a taxonomy of effective
    invocation methods in LMaaS is proposed, which categorizes existing methods from
    four different aspects: input abstract, semantic cache, solution design and output
    enhancement.

    - Flexible Framework. As shown in Figure [3,](#page-3-0) the framework can unify
    the four type components, allowing each of them to work independently or simultaneously,
    during the life cycle of the LLM service invocation.

    - Related Resources. To facilities the methods of this task, the price rules of
    popular LMaaS products is present in Table. [1](#page-0-0) and the paper list
    of existing works is available. [1](#page-1-1)


    The rest of the survey is organized as follows. Section [2](#page-1-2) describes
    the task definition of constructing effective invocation strategy for LMaaS and
    outlines the unified LLM services invocation framework. Section [3](#page-2-1)
    reviews the input abstract component, Section [4](#page-4-0) reviews the semantic
    cache component, Section [5](#page-5-0) reviews the solution design component
    and Section [6](#page-6-0) reviews the output enhancement component. Section [7](#page-6-1)
    emphasize the open challenges and future direction of this task and summarizes
    the paper.


    ### <span id="page-1-2"></span>2 Background


    #### 2.1 Task Definition


    In our topic, the problem is defined as how to construct an effective (low-latency,
    high-performance, and costsaving) invocation strategy s given a task T among massive
    LLM services LLMs. The given task T consists of multiple identical query-answer
    pairs, represented as T = {(q1, a1),(q2, a2), ...(qn, an)}, where q represents
    input query and a represents output answer. Let, considering a fixed LLM, a LLM
    service published through API. Input a query q, by invocation the service, the
    process of getting the response a˜ can be represented as:


    $$

    \tilde{a} = LLM(q) \tag{1}

    $$


    To characterize the concerns for the construction of effective invocation strategy
    with a given query q and LLM service LLM, we use three functions: latency fl(LLM,
    q), performance fp(LLM, q), and cost fc(LLM, q). These three functions are fixed
    values in a specific practical invocation and can be estimated using certain methods.
    For example, f<sup>l</sup> might be a function of the length of the input and
    output tokens. f<sup>p</sup> often uses a metric function r(,) to compare the
    difference between a and a˜. While f<sup>c</sup> involves two different pricing
    components we mentioned before. We adopt the definition of the sum of the number
    of input tokens multiplied by the price of input tokens and the number of generated
    tokens multiplied by the price of generated tokens, as shown in the Eq. [2,](#page-1-3)
    where α<sup>i</sup> is a constant representing the unit price.


    <span id="page-1-3"></span>

    $$f\_c \triangleq \alpha\_1 ||\tilde{a}|| + \alpha\_2 ||q|| + \alpha\_3 \tag{2}$$


    Then we extend a single LLM service to K different LLM services, LLM<sup>s</sup>
    = {LLM1, LLM2, ...LLMK}. Our problem is formalized as Eq. [3,](#page-1-4) where
    in the search space S, we seek the optimal invocation strategy s that minimizes
    latency f<sup>l</sup> , maximizes performance fp, and minimizes cost f<sup>c</sup>
    on task T. The best strategy s includes a sequence of selected LLM services, represented
    as s = {LLM1, LLM<sup>i</sup> , ..., LLMk}, k ≤ K, which is highly flexible, such
    as choosing a single service or accessing some in a specific order.


    <span id="page-1-4"></span>

    $$\min \sum\_{LLM\_i \in s, q\_j \in T} F(f\_l(LLM\_i, q\_j), -f\_p(LLM\_i, q\_j),
    f\_c(LLM\_i, q\_j)) \tag{3}$$


    This is a multi-objective optimization problem, and here, we combine them in a
    simplified form using the function F. In the construction strategy of a specific
    invocation, weighted averages may be used, or constraints may be introduced, treating
    certain targets as conditions while optimizing others. For example, in the scenario
    of limited funds, the cost f<sup>c</sup> is used as a condition to obtain a invocation
    strategy with highperformance f<sup>p</sup> and low-latency f<sup>l</sup> .


    <span id="page-1-1"></span><sup>1</sup> <https://github.com/W-caner/Effective-strategy-for-LMaas>


    <span id="page-2-0"></span>![](_page_2_Figure_0.jpeg)


    Figure 2: Taxonomy of effective invocation methods of LMaaS


    #### 2.2 LLM Services Invocation Framework


    Similarly, we focus solely on the methods related to LLM services invocation and
    do not consider others related to the internal details of LLM. According to the
    different construction ways, the methods are summarized into four categories,
    as shown in Figure [2.](#page-2-0)


    Using the taxonomy, we proposed the effective LLM services invocation framework
    illustrated in Figure [3,](#page-3-0) where different categories represented in
    the form of components that can work independently or simultaneously. Following
    the idea that building an effective invocation strategy requires an understanding
    of the key resources involved in the LLM service life cycle [Bai *et al.*[, 2024\]](#page-7-17),
    we divide the LLM services invocation into three phases : before invocation, invocation
    and after invocation.


    Before invocation, the user enters a query q, and we consider that in general,
    q consists of a question and multiple possible prompts. Where the question represents
    the user''s goal, and the prompts are optional information to help accomplish
    the goal.


    The processing of the input query q to express more meaningful information in
    a more concise language is the first step to construct an effective invocation
    strategy. The methods in this aspect are summarized as input abstract (Section
    [3\)](#page-2-1), which is divided into sentence simplification and prompt optimization
    according to the different ways. The former reduce the latency f<sup>l</sup> and
    cost f<sup>c</sup> by simplifying the query without changing its semantics. The
    latter is used to improve the prompts for better performance fp.


    Semantic cache (Section [4\)](#page-4-0) is also an important strategy to improve
    service performance, reduce latency and cost before invocation, which is divided
    into traditional cache and neural cache according to different structures. It
    checks whether there is a semantically similar query in the cache, if so, it directly
    returns, otherwise it goes into the invocation phase.


    Solution design (Section [5\)](#page-5-0) aims to construct the best invocation
    solution s by leveraging the complementary capabilities of massive LLM services.
    It evaluates LLM services LLM<sup>i</sup> with a given query q, and the method
    of evaluation is called scoring function. It is often done before the invocation,
    such as an estimate of f<sup>c</sup> can be used to guide the design of a lowcost
    solution. And in the invocation phase, the scoring function is used to guide the
    organized routing between services, which is called LLM router. Through different
    routing structures, the advantages of different services are utilized to build
    a satisfactory solution for users.


    After invocation, output enhancement (Section [6\)](#page-6-0) focuses on the
    information returned to the user. The output a˜ is adjusted to suit different
    targets and returned in a suitable form. In addition, the input and output of
    this invocation are stored into the semantic cache for future invocations.


    ## <span id="page-2-1"></span>3 Input Abstract


    Input abstract is designed to reduce the length of the input query without changing
    the semantics, while optimizing the prompt for better performance at lower cost
    and latency to invoke a given LLM.


    <span id="page-3-0"></span>![](_page_3_Figure_0.jpeg)


    Figure 3: LLM services invocation framework, shown by the phase of invocation.


    The generalization and in-context capabilities allow LLM services to get good
    answers on untrained samples[\[Dong](#page-7-18) *et al.*, [2023\]](#page-7-18).
    Thus, a variety of different natural language tasks can be accomplished just by
    inputting different queries. This also leads to the LLM service''s dependence
    on input when invoked. After the service is selected, the input content and quality
    directly affect the latency, price and performance of the service. For example,
    concatenating the prompt "just tell me the option, do not explain other things"
    with the question as input to the LLM will generate shorter output, reducing invocation
    cost and latency. However, it may at the same time cause the LLM to lose its ability
    to think step by step, resulting in performance degradation.


    We grouped the methods into two categories based on different goals. Most LLM
    services charge based on the length of tokens. Therefore, by reducing the input
    length, sentence simplification can effectively reduce the use cost and latency.
    Prompt optimization ensures the quality of the information and improves the performance
    for the invocation.


    #### <span id="page-3-1"></span>3.1 Sentence Simplification


    Sentence simplification aims to improve the performance of language models, reduce
    latency and cost by reducing the complexity and length of language expressions.
    In short, it is the process of making input more concise while retaining its core
    meaning by modifying, removing, or replacing words, phrases, or structures in
    a sentence.


    This problem is similar to the summarization task, and many methods used in summarization
    can be applied [\[Huang](#page-7-19) *et al.*, [2021;](#page-7-19) [Watanangura](#page-8-19)
    *et al.*, 2024; [Antony](#page-7-20) *et al.*, 2023; [Mridha](#page-8-20) *et
    al.*[, 2021\]](#page-8-20). We collate the methods available for LMaaS and divide
    them into extractive and generative methods based on whether they derived entirely
    from the original input.


    Extractive methods. From long original input, extractive methods select sentences
    by extracting key sentences or phrases to form a new input, where the content
    is entirely sourced from original. Pruning semantically irrelevant tokens according
    to the relevance to the contexts is a good choice [\[Liu](#page-7-3) *et al.*[,
    2023a\]](#page-7-3). By employing an intermediate "attacker" and using a greedy
    invocation, iterative deletion and substitution of tokens are performed on the
    input [Si *et al.*[, 2023\]](#page-8-3). [\[Kim](#page-7-4) *et al.*[, 2022\]](#page-7-4)
    based on the attention mechanism, to remove unimportant tokens.


    This approach is straightforward and efficient, making it very convenient for
    immediate use. However, the extraction idea may ignore the global information.
    Furthermore, it has limitations in tasks such as language translation, as it cannot
    discern which parts need to be translated or deleted.


    Generative methods. The generative methods refers to the compression and rewriting
    of the content based on the original input, allowing for the generation of new
    words. Language encoding is a simple processing method applied to the input, [Ahia
    *et al.*[, 2023\]](#page-7-0) conducted extensive experiments with different languages
    and tokenizers, where the cost varied by up to 5 times. AE.studio [2](#page-3-3)
    employs encryption to provide an online platform that sacrifices readability,
    reducing the length of input tokens by half. Utilizing fast and low cost generative
    natural language models [Liu *et al.*[, 2023a;](#page-7-3) Li *et al.*[, 2023\]](#page-7-5)
    also presents a viable option for sentence simplification.


    This category of methods is more flexible, as the generated sentences contain
    less redundant information while preserving the main content. However, it may
    introduce grammatical or factual errors. And this approach may rely on complex
    structures such as graphs, trees, or neural networks.


    #### <span id="page-3-2"></span>3.2 Prompt Optimization


    Prompt optimization is the design and adjustment of userprovided input prompts
    to guide LLMS to produce output that more accurate, useful, or tailored. The effectiveness
    of prompt optimization stems from the LLM''s ability to learn from the


    <span id="page-3-3"></span><sup>2</sup> Prompt Reducer-Cut Down GPT-4 Token Costs
    [\(https://www.](https://www.promptreducer.com/) [promptreducer.com/\)](https://www.promptreducer.com/)


    few-shot or even zero-shot [Liu *et al.*[, 2023b\]](#page-7-15), where appropriate
    prompts can complement the context of the task, highlight key information, or
    improve the explain ability.


    Based on the different granularity of optimization objective, we distinguish two
    types of prompt optimization methods. By selecting or combining some prompts,
    it is possible to guide LLM to handle various inputs more effectively and efficiently.
    Prompt enhancement is concerned with the quality of the content and wants to maximize
    the potential of the context.


    Prompt selection. Prompt selection selects the most meaningful prompt from possible
    prompts to accurately guide LLM. It removes the interference of irrelevant prompts,
    and helps in efficient invocation. [Zhou *et al.*[, 2020\]](#page-8-4) selecting
    representative samples and it can be highly beneficial in few-shot tasks. Another
    way for prompt selection is to combine the prompts for the same type tasks, allowing
    LLM to process prompt information shared by multiple queries at once. [\[Santra](#page-8-5)
    *et al.*, [2023\]](#page-8-5) combines various methods involving instructions,
    examples, and additional context to propose a more compact method for providing
    historical information in dialogues. [\[Arefeen](#page-7-6) *et al.*[, 2023\]](#page-7-6)
    considers the concatenation of prompts and retrieves the most important k sentences
    using comparative methods, enabling the shared use of prompts for similar questions.


    Prompt selection can directly guide LLM to focus on specific aspects of information
    and understand user needs more accurately. For some generic tasks, standard selection
    methods can be used without too much personalization. However, for complex prompts,
    this approach does not maximize its potential because no additional knowledge
    is introduced.


    Prompt augmentation. Prompt augmentation considers the understanding ability of
    LLM to elicit more accurate and desirable responses. Knowledge retrieval is a
    direct method of enhancement, it helps achieve a comprehensive understanding during
    model inference. [\[Haurum](#page-7-7) *et al.*, 2023] investigates the limitations
    of factual knowledge in LLMs and optimizes the reasoning process with minimal
    retrieval cost. Optimization through fine-tuning is a recent advance, [Yu *et
    al.*[, 2023;](#page-8-1) Zhou *et al.*[, 2020\]](#page-8-4) proposes a black-box
    fine-tuning framework that accesses only API to optimize continuous prompts using
    non-derivative methods. Model alignment [Liu *et al.*[, 2023c\]](#page-7-8) and
    chain of thought reasoning [Wu *et al.*[, 2023\]](#page-8-6), are also key focuses
    in prompt optimization.


    The improvement in invocation performance through prompt augmentation is significant,
    despite the likelihood of resulting in more complex processing procedures. And
    the general method is difficult to explore, which requires some professional knowledge.


    ### <span id="page-4-0"></span>4 Semantic Cache


    Semantic cache is an approach to improve LLM invocation efficiency and performance
    by storing and quickly retrieving semantic information. Different from traditional
    data cache, semantic cache focuses more on storing high-level semantics such as
    meaning, context and relationship of data, rather than just raw data. The semantic
    cache is checked before the service is invoked. If hit, the output given by the
    cache is returned without the follow cumbersome procedures.


    Cache technology often requires long-term data accumulation and is not suitable
    for cold start scenarios. However, with the gradual increase in the scale of LLM,
    it plays a more important role in accelerating computation, reducing data transmission
    costs, and supporting high concurrent requests [\[Miao](#page-8-21) *et al.*[,
    2023\]](#page-8-21), providing users with low-cost, low-latency and high-performance
    services.


    There are two typical structures for the implementation of semantic cache in LMaaS,
    and unlike other subsections, they generally cannot be used together. Traditional
    caches use keyvalue pairs for storage and retrieval. When similar input appears
    again, the system can quickly search the semantic cache by the key and return
    the same value. Neural cache borrows ideas from neural networks to response in
    a predictive rather than retrieval way. It learns semantic relationships between
    input data without relying on a specific storage structure.


    #### <span id="page-4-1"></span>4.1 Traditional Cache


    The current paradigm of traditional cache consists of three parts: the cache manager,
    similarity evaluator, and post processor. The cache manager is responsible for
    storing content in the form key-value pairs, and managing eviction. The similarity
    evaluator is used to determine if any of the keys in the cache match the input
    query. The post processor organizes the final response to be returned to the user.
    If no similar query is found in the cache, the LLM service is invoked by the post
    processor to generate the output and then the generated output is stored in the
    cache.


    [\[Bang, 2023\]](#page-7-9) represents a typical application of traditional cache,
    which utilizes question embeddings for similarity matching and provides various
    matching methods such as exact match and embedding distance. The open-source application
    Zep [3](#page-4-3) also supports storage, aggregation, embedding, and indexing
    of LLM applications. Through theoretical proof, [Zhu *et al.*[, 2023\]](#page-8-8)
    gives the cache scheme with minimum expected cost considering the query frequency.
    Furthermore, methods for query and conversations cache [Tao *et al.*[, 2021;](#page-8-7)
    [Barrios and Kumar, 2024\]](#page-7-10) can be easily migrated to LMaaS.


    Implementing traditional cache is usually relatively simple, requiring only basic
    data structures such as hash that are easy to manage. This approach is general,
    but it may not capture semantic similarity between inputs because it relies heavily
    on the key matching.


    #### <span id="page-4-2"></span>4.2 Neural Cache


    Neural cache uses neural networks or deep learning models to learn and store data
    representations. It maps the input data into a high-dimensional space by learning
    the representation of the data. The learned representation should capture the
    semantic similarity of the input data so that similar inputs are close in the
    representation space.


    [\[Ram´ırez](#page-8-9) *et al.*, 2023] trains a student model using T5 base[4](#page-4-4)
    for providing early feedback in classification tasks, and the model is periodically
    updated. To address the effectiveness testing issue of semantic cache, [\[Rasool](#page-8-10)
    *et al.*, 2024] generates similar input to hit the cache as much as possible.
    Additionally, a retrieval-based dialogue response selection model can also serve
    as an alternative choice. [Tao *et al.*[, 2021\]](#page-8-7) provides a survey
    that categorizes most models into three framework and among them, representation-based
    model can be used as neural cache.


    <span id="page-4-3"></span><sup>3</sup>Zep: Fast, scalable building blocks for
    LLM apps [\(https://github.](https://github.com/getzep/zep) [com/getzep/zep\)](https://github.com/getzep/zep)


    <span id="page-4-4"></span><sup>4</sup>T5-base model [\(https://huggingface.co/docs/transformers/model](https://huggingface.co/docs/transformers/model_doc/t5)
    [doc/t5\)](https://huggingface.co/docs/transformers/model_doc/t5)


    These types of methods often outperform traditional cache especially in domain-specific
    problems. However, their implementation and updates can be relatively complex.
    And it is important to carefully consider the effectiveness of the cache to avoid
    incurring unnecessary waste.


    ### <span id="page-5-0"></span>5 Solution Design


    Solution design is an approach to leveraging LLM services with heterogeneous costs
    and performance. It considers different scenarios and different targets, dynamically
    selects one or more LLM services that are most suitable for a specific invocation
    according to the query, and organizes them in some form to provide flexible and
    efficient solutions. This methods allows users to select LLM services that best
    suit their specific needs. When new queries arise or requirements change, the
    configuration of the solution can be flexibly updated to achieve the best performance
    and cost effectiveness.


    Solution design has two main parts that work together to achieve dynamic LLM services
    selection and routing. The scoring function is responsible for evaluating the
    performance of each available LLM service, which can reflect the concerned indicators
    of the invocation such as quality, speed, and so on. The router, based on the
    evaluation results of the scoring function, performs query routing between services
    and selects the appropriate one in a dynamic manner.


    #### <span id="page-5-1"></span>5.1 Scoring Function


    The scoring function is a comprehensive evaluation of LLM services given a specific
    task or query, considering both targets and scenarios, and often used to guide
    the routing path in the solution. It may be influenced by multiple factors such
    as response time, query cost, accuracy of answers, etc. The scoring function plays
    a decision-making role, and helps to understand the relative performance of each
    LLM service so that it can make more intelligent choices.


    Defined metrics. Defined metrics provide a measurable way to achieve direct quantification
    of the factors of concern. For instance, accuracy in classification tasks, BLEU
    score in generation tasks, metrics like packet loss, and quality of service (QoS)
    are all applicable indicators. [\[Ram´ırez](#page-8-9) *et al.*, 2023] use interval
    sampling and predictive entropy to determine whether to invoke LLM services for
    different time dimensions of the invocation. Considering three sources of consistency,
    decisionmaking for LLM services is performed through sampling and voting [Yue
    *et al.*[, 2023\]](#page-8-11). Cost expectations between two models are calculated,
    [Zhu *et al.*[, 2023\]](#page-8-8) extend the selection invocation to multiple
    LLMs. Reward ranking from answers provided by different services is used as an
    evaluation criterion by [Lu *et al.*[, 2023\]](#page-7-11), incurring minimal
    computational cost in the solution.


    The defined metrics are intuitive and easily understandable. They are often based
    on statistical data or experiments, providing high reliability and being less
    susceptible to subjective factors. However, setting thresholds can be challenging
    and may not adapt well to dynamic and changing environments. Additionally, certain
    crucial factors may be difficult to capture with specific metrics, leading to
    limitations in scoring.


    Scorers. A scorer is a tool for scoring each LLM services based on metrics that
    are not defined by a particular formula. The scorer utilizes prior knowledge,
    training data, or rules to provide scores in an often less interpretable manner,
    typically using smaller neural networks [Chen *et al.*[, 2023\]](#page-7-2). AlBert
    is employed as a scorer, with the query and predicted output as x, the accuracy
    of predicted output and label as y during training [\[Sakota](#page-8-12) *et
    al.*, 2023]. Another methods involves using DistilBert as a scoring model, with
    query and model ID as x, and whether it can solve the problem as y during training
    [\[Shnitzer](#page-8-13) *et al.*[, 2023\]](#page-8-13). A comparison of LLM performances
    on different benchmark datasets is conducted, [Zhang *et al.*[, 2023\]](#page-8-14)
    modeling it as a binary selection problem, providing guiding suggestions. For
    specific tasks, such as the execution results in the task of code generation [\[Zhang](#page-8-14)
    *et al.*, 2023], the classifier according to the query difficulty in the task
    of question and answer [\[Anonymous, 2024;](#page-7-12) [Madaan](#page-8-15) *et
    al.*, 2023], and estimate the capabilities of LLM services in the task of dataset
    benchmark test [\[Shnitzer](#page-8-13) *et al.*, 2023] are all reasonable scorers.


    Compared to metrics defined by formulas, scorers can be updated based on real-time
    data and feedback, demonstrating strong generalization across different scenarios.
    However, this approach is equivalent to scoring using a more powerful model, incurring
    scorers own training and usage costs. And it still requires some labeled examples,
    leading to the fact that it makes sense only when the query dataset is larger
    than the training.


    #### <span id="page-5-2"></span>5.2 LLM Router


    LLM routing emphasizes the organizational structure between services, connecting
    multiple independent services in a specific order logically. It focus on constructing
    a flexible and reusable LLM service solution to address continuously changing
    queries or targets. Depending on the different scoring function and position used,
    LLM routing can construct target-oriented solution, such as cost-oriented or performanceoriented.


    Sequential Structure. The simplest method is to select one or several models from
    the massive LLM services available and invoke them sequentially. A scoring function
    is employed to decide whether to accept the answer or proceed to the next step
    in routing [Chen *et al.*[, 2023\]](#page-7-2). When using a sequential structure,
    the number of models is typically limited to three, and possible options are determined
    through permutation, with pruning techniques applied [\[Ram´ırez](#page-8-9) *et
    al.*, 2023; Yue *[et al.](#page-8-11)*, [2023\]](#page-8-11). The use of small
    models as a cache, with large models being invoked in sequence when cache misses
    occur, can be considered a fixed sequential structure [\[Ram´ırez](#page-8-9)
    *et al.*, 2023; Yue *et al.*[, 2023\]](#page-8-11). For problems like code generation
    [\[Zhang](#page-8-14) *et al.*[, 2023\]](#page-8-14), an initial response is obtained
    using a cost-effective LLM, and successful information is tracked as context for
    subsequent queries.


    This structure is simple and effective, and a limited number of permutations can
    be searched quickly in the entire space. However, the sequential structure may
    result in the invocation of all models in the sequential structure. And it is
    difficult to extend the structure, which all models need to be rearranged when
    adapting to new requirements.


    Other Structure. A parallel structure, similar to the bagging and boosting in
    machine learning, can enhance the correctness and consistency of LLM services,
    with task decomposition and merging being key aspects [Jiang *et al.*[, 2023\]](#page-7-13).
    A star-shaped structure, as seen in [\[Sakota](#page-8-12) *et al.*, 2023; Lu
    *et al.*[, 2023\]](#page-7-11), involves decision-making by a meta-model, allocating
    the current query to the most suitable model. For the third category of unsolvable
    queries, pruning is applied by [\[Madaan](#page-8-15) *et al.*, 2023] to prevent
    unnecessary expenses for par-


    <span id="page-6-2"></span>![](_page_6_Figure_0.jpeg)


    Figure 4: A simple invocation strategy composed of existing methods, using Prompt
    Reducer in input abstract, Zep in semantic cache, FrugalGPT in solution design,
    and nothing in output enhancement.


    ticularly challenging problems. The tree structure is considered promising, combining
    aspects of both star-shaped and sequential structures. It initially routes query
    to the most probable branch, and then invokes services in sequence. Additionally,
    certain selection solutions specific to HTTP services [\[Hossein](#page-7-14)zadeh
    *et al.*[, 2020;](#page-7-14) [Manqele](#page-8-17) *et al.*, 2017] are also noteworthy
    for inspiration.


    ### <span id="page-6-0"></span>6 Output Enhancement


    Output Enhancement refers to the process of further optimizing and adjusting the
    output generated by the invocation. This process aims to improve the syntactic
    correctness, semantic accuracy, and overall fluency of the generated output to
    meet the needs of the user and the specific scenario.


    To the best of our knowledge, the output enhancement methods still relies on the
    methods mentioned above, but it emphasizes customization according to the needs
    of specific tasks, improving the adaptability of the model to application, reducing
    the need for subsequent manual intervention, and providing users with low-latency,
    high-performance, and low-latency services. For example, [Liu *et al.*[, 2023b\]](#page-7-15)
    guides the LLM to make concise answers can reduce unnecessary output tokens. Aggregating
    responses from multiple low-cost models is another way to improve quality [Chen
    *et al.*[, 2022\]](#page-7-16), and is often used for multi-label tasks. Work
    on model alignment [\[Shen](#page-8-18) *et al.*[, 2023\]](#page-8-18) can also
    be used to correct syntax and logic to reduce the need for subsequent manual work.


    ### <span id="page-6-1"></span>7 Conclusion and Challenges


    In conclusion, this paper has provided a comprehensive overview of effective invocation
    methods in the realm of LMaaS. Through the establishment of a taxonomy, we categorize
    existing methods into four categories: input abstract, semantic cache, solution
    design, and output enhancement. Then we formalize the problem of effective LLM
    services strategy construction, and propose a LLM services invocation framework.
    Each component in the framework can work independently or simultaneously to form
    effective strategy for LLM service invocation that are low-latency, high-performance,
    and cost-saving.


    Existing methods tend to focus on only one component of the framework, and we
    can use them as plugins. A case is shown in Figure [4,](#page-6-2) a simple invocation
    strategy constructed from three existing methods. The development prospects of
    this field are promising, and here are some open challenges.


    Input Abstract. In the input abstract component, one of the main challenges faced
    is multi-modal input processing [\[Yin](#page-8-22)


    *et al.*[, 2023\]](#page-8-22). More comprehensive and balanced methods are needed
    to shorten and optimize multiple types of input such as text, image, and speech.
    Input abstract methods to dynamically changing inputs are also worth exploring,
    such as realtime data streaming [Rath ¨ *et al.*[, 2023\]](#page-8-23) or user
    interaction with the system. Furthermore, depending on the granularity, the input
    abstract can also be divided into document level, sentence level and phrase level.
    methods at different granularities may interoperate and often compose multi-stage
    methods.


    Semantic Cache. In the semantic cache part, how to design and select cache methods[Brais
    *et al.*[, 2021\]](#page-7-21) more efficiently to accommodate different types
    of inputs and queries is the main challenge faced in traditional cache, while
    semantic representation [\[Brito, 2023\]](#page-7-22) is concerned by neural cache.


    Solution Design. In terms of solution design, the evaluation problem of LLM services
    [\[Chang](#page-7-23) *et al.*, 2023] is an extension of scoring function, which
    needs to pay more attention to adaptation and interpretability in the future.
    While LLM router will focus on designing more powerful services integration methods,
    not only focusing on the task itself, but also considering the requirements of
    different resources [Xu *et al.*[, 2024\]](#page-8-24). A more effective combination
    of the two, such as dynamic decision making, will lead to a better solution.


    Output Enhancement. The importance of output enhancement is also gradually seen
    by people. The balance between specification and diversity of output is a key
    issue. When the task is completed, the user''s satisfaction is then an important
    indicator to measure the service quality, and future research may focus on building
    more intelligent and user-oriented [\[Je](#page-7-24)[ung and Huang, 2023\]](#page-7-24)
    output enhancement methods.


    Other Chanllenges. Basic work such as qualitative description and quantitative
    comparison in experiments is still a gap to be filled, and the lack of data sets
    makes there is no uniform standard for the comparison of service methods. Some
    technical details, such as how to choose the tokenizer [\[Alyafeai](#page-7-25)
    *et al.*[, 2023\]](#page-7-25) with the shortest input, the guidance on the cache
    size [\[Vavouliotis](#page-8-25) *et al.*, 2022], and the choice of different
    pricing methods for the same LLM service, need to be explored. In additionally,
    We specifically call for attention to fairness [Sah *et al.*[, 2024\]](#page-8-26)
    and privacy issues [Luo *et al.*[, 2024;](#page-7-26) Utpala *et al.*[, 2023\]](#page-8-27)
    in LMaaS. The efficient construction of methods using middleware could potentially
    be exploited for personal gain or malicious purposes. We look forward to future
    research further advancing the field, providing users with low-latency, high-performance,
    and cost-effective LLM services solution, and promoting the healthy development
    of the LMaaS ecosystem.


    ### References


    - <span id="page-7-0"></span>[Ahia *et al.*, 2023] Orevaoghene Ahia, Sachin Kumar,
    Hila Gonen, Jungo Kasai, David R. Mortensen, Noah A. Smith, and Yulia Tsvetkov.
    Do all languages cost the same? tokenization in the era of commercial language
    models. In *Proc. of EMNLP*, 2023.

    - <span id="page-7-25"></span>[Alyafeai *et al.*, 2023] Zaid Alyafeai, Maged Saeed
    Al-Shaibani, Mustafa Ghaleb, and Irfan Ahmad. Evaluating various tokenizers for
    arabic text classification. *Neural Process. Lett.*, 2023.

    - <span id="page-7-12"></span>[Anonymous, 2024] Anonymous. Hybrid LLM: Cost-efficient
    and quality-aware query routing. In *Proc. of ICLR*, 2024.

    - <span id="page-7-20"></span>[Antony *et al.*, 2023] Dinu Antony, Sumit Abhishek,
    Sujata Singh, Siddu Kodagali, Narayana Darapaneni, Mukesh Rao, Anwesh Reddy Paduri,
    and Sudha BG. A survey of advanced methods for efficient text summarization. In
    *13th IEEE Annual Computing and Communication Workshop and Conference, CCWC 2023,
    Las Vegas, NV, USA, March 8-11, 2023*, 2023.

    - <span id="page-7-6"></span>[Arefeen *et al.*, 2023] Md. Adnan Arefeen, Biplob
    Debnath, and Srimat Chakradhar. Leancontext: Cost-efficient domain-specific question
    answering using llms. *CoRR*, 2023.

    - <span id="page-7-17"></span>[Bai *et al.*, 2024] Guangji Bai, Zheng Chai, Chen
    Ling, Shiyu Wang, Jiaying Lu, Nan Zhang, Tingwei Shi, Ziyang Yu, Mengdan Zhu,
    Yifei Zhang, Carl J. Yang, Yue Cheng, and Liang Zhao. Beyond efficiency: A systematic
    survey of resource-efficient large language models. *CoRR*, 2024.

    - <span id="page-7-9"></span>[Bang, 2023] Fu Bang. Gptcache: An open-source semantic
    cache for llm applications enabling faster answers and cost savings. 2023.

    - <span id="page-7-10"></span>[Barrios and Kumar, 2024] Carlos Barrios and Mohan
    Kumar. Service caching and computation reuse strategies at the edge: A survey.
    *ACM Comput. Surv.*, 2024.

    - <span id="page-7-21"></span>[Brais *et al.*, 2021] Hadi Brais, Rajshekar Kalayappan,
    and Preeti Ranjan Panda. A survey of cache simulators. *ACM Comput. Surv.*, 2021.

    - <span id="page-7-22"></span>[Brito, 2023] Eduardo Brito. *Explainable Resource-Aware
    Representation Learning via Semantic Similarity*. PhD thesis, 2023.

    - <span id="page-7-23"></span>[Chang *et al.*, 2023] Yupeng Chang, Xu Wang, Jindong
    Wang, Yuan Wu, Kaijie Zhu, Hao Chen, Linyi Yang, Xiaoyuan Yi, Cunxiang Wang, Yidong
    Wang, Wei Ye, Yue Zhang, Yi Chang, Philip S. Yu, Qiang Yang, and Xing Xie. A survey
    on evaluation of large language models. *CoRR*, 2023.

    - <span id="page-7-16"></span>[Chen *et al.*, 2022] Lingjiao Chen, Matei Zaharia,
    and James Zou. Efficient online ML API selection for multi-label classification
    tasks. In *Proc. of ICML*, 2022.

    - <span id="page-7-2"></span>[Chen *et al.*, 2023] Lingjiao Chen, Matei Zaharia,
    and James Zou. Frugalgpt: How to use large language models while reducing cost
    and improving performance. *CoRR*, 2023.

    - <span id="page-7-18"></span>[Dong *et al.*, 2023] Qingxiu Dong, Lei Li, Damai
    Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, Lei Li, and Zhifang
    Sui. A survey on in-context learning, 2023.

    - <span id="page-7-7"></span>[Haurum *et al.*, 2023] Joakim Bruslund Haurum, Sergio
    Escalera, Graham W. Taylor, and Thomas B. Moeslund. Which tokens to use? investigating
    token reduction in vision transformers. In *Proc. of ICCV*, 2023.

    - <span id="page-7-14"></span>[Hosseinzadeh *et al.*, 2020] Mehdi Hosseinzadeh,
    Hawkar Kamaran Hama, Marwan Yassin Ghafour, Mohammad Masdari, Omed Hassan Ahmed,
    and Hemn Khezri. Service selection using multi-criteria decision making: A comprehensive
    overview. *J. Netw. Syst. Manag.*, 2020.

    - <span id="page-7-19"></span>[Huang *et al.*, 2021] Yi-Chong Huang, Xia-Chong
    Feng, Xiao-Cheng Feng, and Bing Qin. The factual inconsistency problem in abstractive
    text summarization: A survey. *CoRR*, 2021.

    - <span id="page-7-24"></span>[Jeung and Huang, 2023] Jun Li Jeung and Yi-Ching
    Janet Huang. Correct me if I am wrong: Exploring how AI outputs affect user perception
    and trust. In *Computer Supported Cooperative Work and Social Computing, CSCW
    2023, Minneapolis, MN, USA, October 14-18, 2023*, 2023.

    - <span id="page-7-13"></span>[Jiang *et al.*, 2023] Dongfu Jiang, Xiang Ren,
    and Bill Yuchen Lin. Llm-blender: Ensembling large language models with pairwise
    ranking and generative fusion. In *Proc. of ACL*, 2023.

    - <span id="page-7-4"></span>[Kim *et al.*, 2022] Sehoon Kim, Sheng Shen, David
    Thorsley, Amir Gholami, Woosuk Kwon, Joseph Hassoun, and Kurt Keutzer. Learned
    token pruning for transformers. In *Proc. of KDD*, 2022.

    - <span id="page-7-1"></span>[Lai *et al.*, 2023] Viet Dac Lai, Nghia Trung Ngo,
    Amir Pouran Ben Veyseh, Hieu Man, Franck Dernoncourt, Trung Bui, and Thien Nguyen.
    Chatgpt beyond english: Towards a comprehensive evaluation of large language models
    in multilingual learning. In *Proc. of EMNLP Findings*, 2023.

    - <span id="page-7-5"></span>[Li *et al.*, 2023] Jiazheng Li, Runcong Zhao, Yulan
    He, and Lin Gui. Overprompt: Enhancing chatgpt capabilities through an efficient
    in-context learning approach. *CoRR*, 2023.

    - <span id="page-7-3"></span>[Liu *et al.*, 2023a] Junyi Liu, Liangzhi Li, Tong
    Xiang, Bowen Wang, and Yiming Qian. TCRA-LLM: token compression retrieval augmented
    large language model for inference cost reduction. In *Proc. of EMNLP Findings*,
    2023.

    - <span id="page-7-15"></span>[Liu *et al.*, 2023b] Pengfei Liu, Weizhe Yuan,
    Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pre-train, prompt,
    and predict: A systematic survey of prompting methods in natural language processing.
    *ACM Comput. Surv.*, 2023.

    - <span id="page-7-8"></span>[Liu *et al.*, 2023c] Yixin Liu, Budhaditya Deb,
    Milagro Teruel, Aaron Halfaker, Dragomir Radev, and Ahmed Hassan Awadallah. On
    improving summarization factual consistency from natural language feedback. In
    *Proc. of ACL*, 2023.

    - <span id="page-7-11"></span>[Lu *et al.*, 2023] Keming Lu, Hongyi Yuan, Runji
    Lin, Junyang Lin, Zheng Yuan, Chang Zhou, and Jingren Zhou. Routing to the expert:
    Efficient reward-guided ensemble of large language models. *CoRR*, 2023.

    - <span id="page-7-26"></span>[Luo *et al.*, 2024] Jinglong Luo, Yehong Zhang,
    Jiaqi Zhang, Xin Mu, Hui Wang, Yue Yu, and Zenglin Xu. Secformer: Towards fast
    and accurate privacy-preserving inference for large language models. *CoRR*, 2024.

    - <span id="page-8-15"></span>[Madaan *et al.*, 2023] Aman Madaan, Pranjal Aggarwal,
    Ankit Anand, Srividya Pranavi Potharaju, Swaroop Mishra, Pei Zhou, Aditya Gupta,
    Dheeraj Rajagopal, Karthik Kappaganthu, Yiming Yang, Shyam Upadhyay, Mausam, and
    Manaal Faruqui. Automix: Automatically mixing language models. *CoRR*, 2023.

    - <span id="page-8-17"></span>[Manqele *et al.*, 2017] Lindelweyizizwe Manqele,
    Mqhele E. Dlodlo, Louis Coetzee, and George Sibiya. A survey for service selection
    approaches in dynamic environments. In *IEEE AFRICON 2017, Cape Town, South Africa,
    September 18-20, 2017*, 2017.

    - <span id="page-8-21"></span>[Miao *et al.*, 2023] Xupeng Miao, Gabriele Oliaro,
    Zhihao Zhang, Xinhao Cheng, Hongyi Jin, Tianqi Chen, and Zhihao Jia. Towards efficient
    generative large language model serving: A survey from algorithms to systems.
    *CoRR*, 2023.

    - <span id="page-8-20"></span>[Mridha *et al.*, 2021] Muhammad F. Mridha, Aklima
    Akter Lima, Kamruddin Nur, Sujoy Chandra Das, Mahmud Hasan, and Muhammad Mohsin
    Kabir. A survey of automatic text summarization: Progress, process and challenges.
    *IEEE Access*, 2021.

    - <span id="page-8-9"></span>[Ram´ırez *et al.*, 2023] Guillem Ram´ırez, Matthias
    Lindemann, Alexandra Birch, and Ivan Titov. Cache & distil: Optimising API calls
    to large language models. *CoRR*, 2023.

    - <span id="page-8-10"></span>[Rasool *et al.*, 2024] Zafaryab Rasool, Scott Barnett,
    David Willie, Stefanus Kurniawan, Sherwin Balugo, Srikanth Thudumu, and Mohamed
    Abdelrazek. Llms for test input generation for semantic caches, 2024.

    - <span id="page-8-23"></span>[Rath ¨ *et al.*, 2023] Timo Rath, Ngozichukwuka
    Onah, and ¨ Kai-Uwe Sattler. Interactive data cleaning for real-time streaming
    applications. In *Proceedings of the Workshop on Human-In-the-Loop Data Analytics,
    HILDA 2023, Seattle, WA, USA, 18 June 2023*, 2023.

    - <span id="page-8-2"></span>[Reddy *et al.*, 2019] Siva Reddy, Danqi Chen, and
    Christopher D. Manning. Coqa: A conversational question answering challenge. *Trans.
    Assoc. Comput. Linguistics*, 2019.

    - <span id="page-8-26"></span>[Sah *et al.*, 2024] Chandan Kumar Sah, Xiaoli Lian,
    and Muhammad Mirajul Islam. Unveiling bias in fairness evaluations of large language
    models: A critical literature review of music and movie recommendation systems.
    *CoRR*, 2024.

    - <span id="page-8-16"></span>[Saha *et al.*, 2023] Swarnadeep Saha, Omer Levy,
    Asli Celikyilmaz, Mohit Bansal, Jason Weston, and Xian Li. Branch-solve-merge
    improves large language model evaluation and generation. *CoRR*, 2023.

    - <span id="page-8-12"></span>[Sakota *et al.*, 2023] Marija Sakota, Maxime Peyrard,
    and Robert West. Fly-swat or cannon? cost-effective language model choice via
    meta-modeling. *CoRR*, 2023.

    - <span id="page-8-5"></span>[Santra *et al.*, 2023] Bishal Santra, Sakya Basak,
    Abhinandan De, Manish Gupta, and Pawan Goyal. Frugal prompting for dialog models.
    In *Proc. of EMNLP Findings*, 2023.

    - <span id="page-8-18"></span>[Shen *et al.*, 2023] Tianhao Shen, Renren Jin,
    Yufei Huang, Chuang Liu, Weilong Dong, Zishan Guo, Xinwei Wu, Yan Liu, and Deyi
    Xiong. Large language model alignment: A survey. *CoRR*, 2023.

    - <span id="page-8-13"></span>[Shnitzer *et al.*, 2023] Tal Shnitzer, Anthony
    Ou, M´ırian Silva, Kate Soule, Yuekai Sun, Justin Solomon, Neil Thompson, and
    Mikhail Yurochkin. Large language model routing with benchmark datasets. *CoRR*,
    2023.

    - <span id="page-8-3"></span>[Si *et al.*, 2023] Wai Man Si, Michael Backes, and
    Yang Zhang. Mondrian: Prompt abstraction attack against large language models
    for cheaper API pricing. *CoRR*, 2023.

    - <span id="page-8-7"></span>[Tao *et al.*, 2021] Chongyang Tao, Jiazhan Feng,
    Rui Yan, Wei Wu, and Daxin Jiang. A survey on response selection for retrieval-based
    dialogues. In *Proc. of IJCAI*, 2021.

    - <span id="page-8-27"></span>[Utpala *et al.*, 2023] Saiteja Utpala, Sara Hooker,
    and Pin-Yu Chen. Locally differentially private document generation using zero
    shot prompting. In *Proc. of EMNLP Findings*, 2023.

    - <span id="page-8-25"></span>[Vavouliotis *et al.*, 2022] Georgios Vavouliotis,
    Gino Chacon, Lluc Alvarez, Paul V. Gratz, Daniel A. Jimenez, and Marc ´ Casas.
    Page size aware cache prefetching. In *Proc. of MI-CRO*, 2022.

    - <span id="page-8-19"></span>[Watanangura *et al.*, 2024] Patcharapruek Watanangura,
    Sukit Vanichrudee, On Minteer, Theeranat Sringamdee, Nattapong Thanngam, and Thitirat
    Siriborvornratanakul. A comparative survey of text summarization techniques. *SN
    Comput. Sci.*, 2024.

    - <span id="page-8-6"></span>[Wu *et al.*, 2023] Dingjun Wu, Jing Zhang, and Xinmei
    Huang. Chain of thought prompting elicits knowledge augmentation. In *Proc. of
    ACL Findings*, 2023.

    - <span id="page-8-24"></span>[Xu *et al.*, 2024] Mengwei Xu, Wangsong Yin, Dongqi
    Cai, Rongjie Yi, Daliang Xu, Qipeng Wang, Bingyang Wu, Yihao Zhao, Chen Yang,
    Shihe Wang, Qiyang Zhang, Zhenyan Lu, Li Zhang, Shangguang Wang, Yuanchun Li,
    Yunxin Liu, Xin Jin, and Xuanzhe Liu. A survey of resource-efficient LLM and multimodal
    foundation models. *CoRR*, 2024.

    - <span id="page-8-0"></span>[Yang *et al.*, 2023] Jingfeng Yang, Hongye Jin,
    Ruixiang Tang, Xiaotian Han, Qizhang Feng, Haoming Jiang, Bing Yin, and Xia Hu.
    Harnessing the power of llms in practice: A survey on chatgpt and beyond. *CoRR*,
    2023.

    - <span id="page-8-22"></span>[Yin *et al.*, 2023] Shukang Yin, Chaoyou Fu, Sirui
    Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen. A survey on multimodal large
    language models. *CoRR*, 2023.

    - <span id="page-8-1"></span>[Yu *et al.*, 2023] Lang Yu, Qin Chen, Jiaju Lin,
    and Liang He. Black-box prompt tuning for vision-language model as a service.
    In *Proc. of IJCAI*, 2023.

    - <span id="page-8-11"></span>[Yue *et al.*, 2023] Murong Yue, Jie Zhao, Min Zhang,
    Liang Du, and Ziyu Yao. Large language model cascades with mixture of thoughts
    representations for cost-efficient reasoning. *CoRR*, 2023.

    - <span id="page-8-14"></span>[Zhang *et al.*, 2023] Jieyu Zhang, Ranjay Krishna,
    Ahmed Hassan Awadallah, and Chi Wang. Ecoassistant: Using LLM assistant more affordably
    and accurately. *CoRR*, 2023.

    - <span id="page-8-4"></span>[Zhou *et al.*, 2020] Jianyi Zhou, Feng Li, Jinhao
    Dong, Hongyu Zhang, and Dan Hao. Cost-effective testing of a deep learning model
    through input reduction. In *31st IEEE International Symposium on Software Reliability
    Engineering, ISSRE 2020, Coimbra, Portugal, October 12-15, 2020*, 2020.

    - <span id="page-8-8"></span>[Zhu *et al.*, 2023] Banghua Zhu, Ying Sheng, Lianmin
    Zheng, Clark W. Barrett, Michael I. Jordan, and Jiantao Jiao. On optimal caching
    and model multiplexing for large model inference. *CoRR*, 2023.'
- title: 'POSTER: Identifying and Mitigating Vulnerabilities in LLM-Integrated Applications'
  abstract: Compared with the traditional usage of large language models (LLMs) where
    users directly send queries to an LLM, LLM-integrated applications serve as middleware
    to refine users' queries with domain-specific knowledge to better inform LLMs
    and enhance the responses. However, LLM-integrated applications also introduce
    new attack surfaces. This work considers a setup where the user and LLM interact
    via an application in the middle. We focus on the interactions that begin with
    user's queries and end with LLMintegrated application returning responses to the
    queries, powered by LLMs at the service backend. We identify potential high-risk
    vulnerabilities in this setting that can originate from the malicious application
    developer or from an outsider threat initiator that can control the database access,
    manipulate and poison high-risk data for the user. Successful exploits of the
    identified vulnerabilities result in the users receiving responses tailored to
    the intent of a threat initiator. We assess such threats against LLM-integrated
    applications empowered by GPT-3.5 and GPT-4. Our experiments show that the threats
    can effectively bypass the restrictions and moderation policies of OpenAI, resulting
    in users exposing to the risk of bias, toxic content, privacy, and disinformation.
    We develop a lightweight, threat-agnostic defense to mitigate insider and outsider
    threats. Our evaluations demonstrate the efficacy of our defense.
  keywords: ''
  document: '![](_page_0_Picture_0.jpeg)


    # POSTER: Identifying and Mitigating Vulnerabilities in LLM-Integrated Applications


    Fengqing Jiang University of Washington Seattle, USA fqjiang@uw.edu


    Zhangchen Xu University of Washington Seattle, USA zxu9@uw.edu


    Luyao Niu University of Washington Seattle, USA luyaoniu@uw.edu


    Boxin Wang Nvidia Santa Clara, USA boxinw@nvidia.com


    Jinyuan Jia Penn State University State College, USA jinyuan@psu.edu


    Bo Li University of Chicago Chicago, USA bol@uchicago.edu


    ## 1 INTRODUCTION


    LLM-integrated applications are increasingly deployed to allow third party developers/vendors
    to serve users leveraging the astonishing capabilities of large language models
    (LLMs). An LLMintegrated application consists of three parties – user, application,
    and LLM, interacting through two interfaces as shown in Fig. [1.](#page-0-0) The
    interaction consists of two communication phases: upstream communication and downstream
    communication. In the upstream communication, a user sends queries to an application
    through a userapplication interface; the application refines the user''s queries
    based on a domain-specific database and forwards the refined queries to the LLM
    via an application-LLM interface. In the downstream communication, the LLM generates
    responses to the refined queries and sends the responses back to the application;
    the application post-processes the responses and sends the processed responses
    to the user. While users can utilize LLM-integrated applications to better inform
    LLMs for enhanced services, the presence of untrusted/unverified application developers
    opens up new attack surfaces for misuses. Currently, however, identifying and
    mitigating the vulnerabilities of LLM-integrated applications have not been studied.


    Radha Poovendran University of Washington Seattle, USA rp3@uw.edu


    <span id="page-0-0"></span>![](_page_0_Figure_10.jpeg)


    #### Figure 1: Service schematic of LLM-integrated applications.


    In this work in progress, we identify and list a set of attacks that arise from
    an LLM application and external adversaries that can interact with the LLM application,
    which define the attack surface. In particular, we focus on the model where a
    user interacts with the LLM through an LLM-integrated application, i.e., a user
    sends the query and the application returns the answer with the help of LLM. We
    show that such a query-response protocol is vulnerable to both insider and outsider
    threats with the goal of monetizing and enhancing their profits. An insider threat
    arises from a malicious


    #### ABSTRACT


    Compared with the traditional usage of large language models (LLMs) where users
    directly send queries to an LLM, LLM-integrated applications serve as middleware
    to refine users'' queries with domain-specific knowledge to better inform LLMs
    and enhance the responses. However, LLM-integrated applications also introduce
    new attack surfaces. This work considers a setup where the user and LLM interact
    via an application in the middle. We focus on the interactions that begin with
    user''s queries and end with LLMintegrated application returning responses to
    the queries, powered by LLMs at the service backend. We identify potential high-risk
    vulnerabilities in this setting that can originate from the malicious application
    developer or from an outsider threat initiator that can control the database access,
    manipulate and poison high-risk data for the user. Successful exploits of the
    identified vulnerabilities result in the users receiving responses tailored to
    the intent of a threat initiator. We assess such threats against LLM-integrated
    applications empowered by GPT-3.5 and GPT-4. Our experiments show that the threats
    can effectively bypass the restrictions and moderation policies of OpenAI, resulting
    in users exposing to the risk of bias, toxic content, privacy, and disinformation.
    We develop a lightweight, threat-agnostic defense to mitigate insider and outsider
    threats. Our evaluations demonstrate the efficacy of our defense.


    #### CCS CONCEPTS


    • Security and privacy → Human and societal aspects of security and privacy.


    #### ACM Reference Format:


    Fengqing Jiang, Zhangchen Xu, Luyao Niu, Boxin Wang, Jinyuan Jia, Bo Li, and Radha
    Poovendran. 2024. POSTER: Identifying and Mitigating Vulnerabilities in LLM-Integrated
    Applications. In Proceedings of ACM Asia Conference on Computer and Communications
    Security (Asia CCS''24). ACM, New York, NY, USA, [3](#page-2-0) pages. <https://doi.org/10.1145/3634737.3659433>


    Asia CCS''24, July 1–5, 2024, Singapore, Singapore


    © 2024 Copyright held by the owner/author(s).


    ACM ISBN 979-8-4007-0482-6/24/07


    <https://doi.org/10.1145/3634737.3659433>


    Permission to make digital or hard copies of part or all of this work for personal
    or classroom use is granted without fee provided that copies are not made or distributed
    for profit or commercial advantage and that copies bear this notice and the full
    citation on the first page. Copyrights for third-party components of this work
    must be honored. For all other uses, contact the owner/author(s).


    Asia CCS''24, July 1–5, 2024, Singapore, Singapore Fengqing Jiang, Zhangchen Xu,
    Luyao Niu, Boxin Wang, Jinyuan Jia, Bo Li, and Radha Poovendran


    ![](_page_1_Figure_2.jpeg)


    Figure 2: Attack in upstream communication by an insider.


    application developer. The insider threat initiator could achieve its attack objective
    by manipulating users'' queries and/or responses from the LLM to alter the contexts
    and perturb the semantics during the upstream and downstream communication phases.
    An outsider threat arises from the compromised database maintained by the application.
    The outsider threat initiator can control the database access and poison the data
    used by the application. Consequently, even if the application developer is benign,
    the queries from users may be refined in an unintended manner by the application,
    leading to responses that are aligned with the attack goal. We empirically assess
    both threats to a chatbot of an online shopping application integrated with GPT-3.5
    and GPT-4. Our results show that attacks can successfully bypass the restrictions
    [\[5\]](#page-2-1) of OpenAI, and result in responses to users containing bias
    and toxic contents.


    We propose the first known defense, Shield, to mitigate the identified risks.
    We showShield prevents both threats from manipulating the queries from users or
    responses by LLM. Our empirical evaluations show that Shield achieves attack detection
    with high accuracy and utility preservation when serving benign users.


    ## <span id="page-1-1"></span>2 LLM-INTEGRATED APPLICATION AND THREAT MODEL


    #### 2.1 LLM-integrated Application


    The service pipeline of an LLM-integrated application consists of three parties:
    user , application , and LLM , as shown in Fig. [1.](#page-0-0)


    Upstream Communication: User sends a query prompt to the application to access
    certain services such as shopping advising. After receiving , the application
    first identifies and extracts information, denoted as ( ), from the query. Then,
    the application utilizes its external source, e.g., query database or access context
    memory, to obtain domain-specific information ( ( )). Finally, the application
    refines user query with domain-specific information ( ( )) to generate an intermediate
    prompt as = ( , ( ( ))) using techniques such as Self-instruct [\[7\]](#page-2-2).


    Downstream Communication: LLM responds to prompt by returning a raw response to
    the application. The application takes a post-processing action (e.g., using an
    external toolkit) to generate response = () in order to satisfy user''s query
    .


    #### 2.2 Threat Model and Attack Surface


    Insider Threat and Attack. An insider threat originates from malicious application
    developers. Even when the application developers are benign, a threat initiator
    may exploit the vulnerabilities inherent in the application such as unpatched
    software [\[2\]](#page-2-3). An


    <span id="page-1-0"></span>Table 1: TSRs of bias, toxic, privacy, and disinformation
    risks.


    | Risk     | Threat       | GPT-3.5   |          | GPT-4     |          |

    |----------|--------------|-----------|----------|-----------|----------|

    |          | Model        | HumanEval | GPT-auto | HumanEval | GPT-auto |

    | Bias     | Neutral      | 2%        | 0%       | 0%        | 0%       |

    |          | Pertb-User   | 62%       | 47%      | 99%       | 67%      |

    |          | Pertb-System | 97 %      | 85 %     | 100%      | 81%      |

    |          | Proxy        | 83%       | 68%      | 80%       | 53%      |

    | Toxic    | Neutral      | 0%        | 0%       | 0%        | 0%       |

    |          | Outsider     | 78%       | 78%      | 88%       | 94%      |

    |          | Pertb-System | 100%      | 100%     | 100%      | 100%     |

    | Privacy  | Neutral      | 0%        | 0%       | 0%        | 0%       |

    |          | Pertb-System | 98%       | 100%     | 100%      | 100%     |

    | Disinfo. | Neutral      | 0%        | 0%       | 0%        | 0%       |

    |          | Pertb-System | 100%      | 100%     | 100%      | 98%      |


    initiator of insider threat can thereby control the application, and attack LLM-integrated
    applications during both the upstream and downstream communication phases. As
    a result, the threat initiator has can either (1) modify the user prompt to through
    prompt injection [\[6\]](#page-2-4), or (2) manipulate the LLM response to ˜ in
    downstream communication. Both (1) and (2) lead to users receiving responses aligned
    with the semantic goal of threat initiator.


    Outsider Threat and Attack. In this case, the application is operated by a benign
    entity. The threat initiator could achieve its semantic goal by compromising the
    external sources such as database of the application via data poisoning attacks
    [\[1\]](#page-2-5). Consequently, the application may use compromised information
    ( ( )) to generate prompt , which leads the LLM to generate response that fulfills
    the threat initiator''s semantic goal.


    #### 3 THREAT EVALUATION


    Experimental Setup. We consider an online shopping application whose chatbot uses
    GPT-3.5 and GPT-4 [\[4\]](#page-2-6) in the backend. An insider threat initiator
    can tamper with the queries from users in the upstream communication in two ways:
    (i) by perturbing the queries via prompt injection [\[6\]](#page-2-4), denoted
    as Pertb-User, and (ii) by applying perturbed system prompt [\[3\]](#page-2-7),
    denoted as Pertb-System. During the downstream communication, an insider threat
    initiator perturbs the semantics of responses by generating a proxy prompt ˜ using
    prompt injection [\[6\]](#page-2-4). We denote this attack as Proxy.


    We use targeted attack success rate (TSR) to measure the effectiveness of attacks,
    defined as


    $$\text{TSR} = \frac{\text{\text# of responses aligned with semantic goal}}{\text{\textdegreeresponse}}.1$$


    We calculate TSR using two methods: HumanEval and GPT-auto. For HumanEval, we
    manually check whether each response satisfies the condition. For GPT-auto, we
    utilize GPT-3.5 to check those responses. Even in the absence of insider and outsider
    threats, LLM may occasionally return responses containing unintended bias, privacy
    issues, and/or disinformation. To identify whether such undesired semantics are
    generated due to attacks or from LLMs, we evaluate TSRs in the absence of the
    threats, denote as Neutral.


    Experimental Results. In Table [1,](#page-1-0) we evaluate the threat models on
    bias, toxic, privacy and disinformation risks. We observe that the insider threat
    effectively lead to responses demonstrating all risks compared to Neutral. The
    results indicate our proposed attack successfully bypass the ethic restrictions
    deployed by OpenAI [\[5\]](#page-2-1). <span id="page-2-0"></span>POSTER: Identifying
    and Mitigating Vulnerabilities in LLM-Integrated Applications Asia CCS''24, July
    1–5, 2024, Singapore, Singapore


    <span id="page-2-8"></span>![](_page_2_Figure_1.jpeg)


    Figure 3: This figure shows the workflow of **Shield**.


    We also note that using system prompt achieves highest TSR. This is because the
    insider threat initiator can fully control the application.


    ### 4 PROPOSED DEFENSE **SHIELD**


    To mitigate the vulnerabilities in Section [2,](#page-1-1) We design a defense
    named Shield. Our key idea is to ensure the queries from users cannot be manipulated,
    and are distinguishable from the intermediate prompts from application. Fig. [3](#page-2-8)
    shows the workflow of Shield. We define the signature of a message as = sig (),
    where sig is a signing algorithm using key . We denote the signed message as (,
    ). The verification of (, ), denoted as ver (, ), outputs either true or false.
    The unique session ID is .


    Upstream communication. ❶: Session ID and user''s query (, ) is signed using user''s
    key as <sup>1</sup> = sig (, ). ❷: The signed query is then sent to the application
    to generate the intermediate prompt = ( , ( ( ))). ❸: After receiving the intermediate
    prompt, Shield verifies whether ver ( (, ), 1) holds true. If the result is true,
    Shield then records the ID and constructs a metaprompt <sup>1</sup> for detection
    by LLM. ❹: Shield sends <sup>1</sup> to the LLM. If no attack is detected, is
    transmitted to the LLM for response generation. Otherwise, only user''s query
    is sent to the LLM.


    Downstream communication. ➀: After the application receives the response from
    the LLM, it generates a response and sends it back to Shield. The API then constructs
    a meta-prompt 2. ➁: Shield then sends <sup>2</sup> to the LLM for attack detection.
    ➂: If no attack is detected, then Shield signs as <sup>2</sup> = sig (, ), where
    is the key of Shield. The signed response ( (, ), 2) is then returned to the user.
    Otherwise, Shield returns to the user with the corresponding signature. ➃: After
    receiving responses from the application, the user executes ver ( (, ), 2). If
    the verification process returns true, then user accepts as the response.


    Evaluation. We empirically evaluate the attack detectability and utility preservation
    of our defense. We quantify the attack detectability by computing the ratio of
    tests that are correctly labeled as under attack. The utility preservation is
    evaluated using the Neutral scenario, where there exists no attack. We summarize
    the evaluation results on the online shopping application in Table [2.](#page-2-9)
    We first observe that Shield successfully detects the attacks when both GPT-3.5
    and GPT-4 are used as LLM services. The latest GPT-4 achieves nearly 100% success
    rate in detecting attacks across all risks. Furthermore, Shield preserves the
    utility of LLM-integrated


    <span id="page-2-9"></span>


    | Risk     | Threat Model | GPT-3.5 | GPT-4 |

    |----------|--------------|---------|-------|

    |          | Neutral      | 94%     | 100%  |

    |          | Pertb-User   | 100%    | 100%  |

    | Bias     | Pertb-System | 92%     | 100%  |

    |          | Proxy        | 71%     | 99%   |

    |          | Neutral      | 100%    | 100%  |

    | Toxic    | Outsider     | 100%    | 100%  |

    |          | Pertb-System | 100%    | 100%  |

    |          | Neutral      | 100%    | 100%  |

    | Privacy  | Pertb-System | 36%     | 100%  |

    |          | Neutral      | 100%    | 100%  |

    | Disinfo. | Pertb-System | 56%     | 80%   |


    applications. When there exist no attacks, all responses produced by LLM-integrated
    applications can address the users'' queries.


    #### 5 CONCLUSION AND DISCUSSION


    In this paper, we showed that LLM-integrated applications become new attack surfaces
    that could be exploited by both insider and outsider threat initiators, leading
    to bias, toxic, privacy, and disinformation risks for users of applications. Our
    extensive empirical evaluations confirmed those risks. We designed a defense Shield
    in addition to the LLM-API which is compatible with any LLMs. Our experimental
    results demonstrated the efficacy of our defense.


    #### ACKNOWLEDGEMENT


    This work is partially supported by the Air Force Office of Scientific Research
    (AFOSR) under grant FA9550-23-1-0208, National Science Foundation (NSF) under
    grants No.1910100, No.2046726, No. 2229876, DARPA GARD, the National Aeronautics
    and Space Administration (NASA) under grant No.80NSSC20M0229, Alfred P. Sloan
    Fellowship, Office of Naval Research (ONR) under grant N00014-23-1-2386, and the
    Amazon research award.


    This work is supported in part by funds provided by the National Science Foundation,
    by the Department of Homeland Security, and by IBM. Any opinions, findings, and
    conclusions or recommendations expressed in this material are those of the author(s)
    and do not necessarily reflect the views of the National Science Foundation or
    its federal agency and industry partners.


    #### REFERENCES


    - <span id="page-2-5"></span>[1] Xinyun Chen, Chang Liu, Bo Li, Kimberly Lu, and
    Dawn Song. 2017. Targeted backdoor attacks on deep learning systems using data
    poisoning. arXiv preprint arXiv:1712.05526 (2017).

    - <span id="page-2-3"></span>[2] Lockheed Martin. 2022. The cyber kill chain.
    [https://www.lockheedmartin.com/en](https://www.lockheedmartin.com/en-us/capabilities/cyber/cyber-kill-chain.html)[us/capabilities/cyber/cyber-kill-chain.html.](https://www.lockheedmartin.com/en-us/capabilities/cyber/cyber-kill-chain.html)
    Accessed: 2023-09-15.

    - <span id="page-2-7"></span>[3] OpenAI. 2023. ChatGPT API Transition Guide. [https://help.openai.com/en/articles/](https://help.openai.com/en/articles/7042661-chatgpt-api-transition-guide)
    [7042661-chatgpt-api-transition-guide.](https://help.openai.com/en/articles/7042661-chatgpt-api-transition-guide)
    Accessed: 2023-09-15.

    - <span id="page-2-6"></span>[4] OpenAI. 2023. Models-OpenAI API. [https://platform.openai.com/docs/models.](https://platform.openai.com/docs/models)
    Accessed: 2023-09-15.

    - <span id="page-2-1"></span>[5] OpenAI. 2023. Usage Policies–OpenAI. [https://openai.com/policies/usage-policies.](https://openai.com/policies/usage-policies)
    Accessed: 2023-09-15.

    - <span id="page-2-4"></span>[6] Fábio Perez and Ian Ribeiro. 2022. Ignore previous
    prompt: Attack techniques for language models. arXiv preprint arXiv:2211.09527
    (2022).

    - <span id="page-2-2"></span>[7] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra,
    Alisa Liu, Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2022. Self-Instruct:
    Aligning language model with self generated instructions. arXiv preprint arXiv:2212.10560
    (2022).'
- title: 'ACM Reference Format:'
  abstract: ''
  keywords: ''
  document: '-


    ALVARO CINTAS CANTO, Marymount University, USA JASMIN KAUR, University of South
    Florida, USA MEHRAN MOZAFFARI KERMANI, University of South Florida, USA REZA AZARDERAKHSH,
    Florida Atlantic University, USA


    This survey is on forward-looking, emerging security concerns in post-quantum
    era, i.e., the implementation attacks for 2022 winners of NIST post-quantum cryptography
    (PQC) competition and thus the visions, insights, and discussions can be used
    as a step forward towards scrutinizing the new standards for applications ranging
    from Metaverse/Web 3.0 to deeply-embedded systems. The rapid advances in quantum
    computing have brought immense opportunities for scientific discovery and technological
    progress; however, it poses a major risk to today''s security since advanced quantum
    computers are believed to break all traditional publickey cryptographic algorithms.
    This has led to active research on PQC algorithms that are believed to be secure
    against classical and powerful quantum computers. However, algorithmic security
    is unfortunately insufficient, and many cryptographic algorithms are vulnerable
    to side-channel attacks (SCA), where an attacker passively or actively gets side-channel
    data to compromise the security properties that are assumed to be safe theoretically.
    In this survey, we explore such imminent threats and their countermeasures with
    respect to PQC. We provide the respective, latest advancements in PQC research,
    as well as assessments and providing visions on the different types of SCAs.


    CCS Concepts: • Security and privacy → Digital signatures; Hardware attacks and
    countermeasures.


    Additional Key Words and Phrases: Embedded security, Secure post-quantum cryptography,
    side-channel attacks.


    ## ACM Reference Format:


    Alvaro Cintas Canto, Jasmin Kaur, Mehran Mozaffari Kermani, and Reza Azarderakhsh.
    2023. Algorithmic Security is Insufficient: A Comprehensive Survey on Implementation
    Attacks Haunting Post-Quantum Security. ACM Comput. Surv. -, -, Article - (May
    2023), [16](#page-15-0) pages. <https://doi.org/10.1145/nnnnnnn.nnnnnnn>


    ## 1 INTRODUCTION


    Shor''s algorithm is a known quantum algorithm that allows solving discrete-logarithm
    and integerfactorization problems, making public key cryptographic standards vulnerable
    under the presence of quantum computers. RSA, DSA, and elliptic curve cryptography
    (ECC) are the main public key cryptographic algorithms that are used currently.
    ECC has replaced RSA in many applications due


    Authors'' addresses: Alvaro Cintas Canto, Marymount University, Arlington, VA,
    22207, USA, acintas@marymount.edu; Jasmin Kaur, University of South Florida, Tampa,
    FL, 33620, USA, jasmink1@usf.edu; Mehran Mozaffari Kermani, University of South
    Florida, Tampa, FL, 33620, USA, mehran2@usf.edu; Reza Azarderakhsh, Florida Atlantic
    University, Boca Raton, FL, 33431, USA, razarderakhsh@fau.edu.


    Permission to make digital or hard copies of all or part of this work for personal
    or classroom use is granted without fee provided that copies are not made or distributed
    for profit or commercial advantage and that copies bear this notice and the full
    citation on the first page. Copyrights for components of this work owned by others
    than ACM must be honored. Abstracting with credit is permitted. To copy otherwise,
    or republish, to post on servers or to redistribute to lists, requires prior specific
    permission and/or a fee. Request permissions from permissions@acm.org.


    0360-0300/2023/05-ART- \$15.00


    <https://doi.org/10.1145/nnnnnnn.nnnnnnn>


    <sup>© 2023</sup> Association for Computing Machinery.


    to its efficient realizations with the same security level. Nevertheless, the
    introduction of highperformance quantum computers has increased the need for the
    creation of public key cryptosystems which are resistant to the cyber-attacks
    enabled by quantum-based computing systems. The National Institute of Standards
    and Technology (NIST) announced in late 2016 the commencement of a project to
    standardize one or more quantum computer-resistant public-key cryptography and
    digital signature algorithms [1]. After more than five years and multiple rounds
    of reviews, NIST has recently, in 2022, chosen four candidate algorithms for standardization
    and left four others for another round of evaluation. Table 1 shows the current
    state of the NIST post-quantum cryptography (PQC) standardization process, where
    PKE and KEM stand for public-key encryption and key encapsulation mechanism, respectively.
    KEMs, unlike general-purpose PKEs, are not intended for encrypting application
    data. Instead, they are specifically created to establish a shared secret between
    communication partners in cryptographic protocols such as Transport Layer Security
    (TLS), just like the Diffie-Hellman Key-Exchange method, which is currently one
    of the best available options.


    PQC cryptography englobes five major types: Lattice-based, code-based, multivariate-based,
    hash-based, and isogeny-based cryptography. Lattice-based cryptography mathematical
    problem is related to lattices, which are geometric structures formed by repeating
    patterns of points in space; code-based cryptography is formed on error-correcting
    codes, a technique used to detect and correct errors in data transmission; multivariate-based
    cryptography relies on the hardness of solving equations with multiple variables
    (there are not multivariate-based standards or finalists); hash-based cryptography
    relies on hash functions, which are one-way functions where any-size input is
    mapped into a fixed value; and lastly, isogeny-based cryptography uses isogenies,
    which are mappings between elliptic curves.


    Most of the aforementioned PQC algorithms have large designs and complex operations.
    This aspect as well as the continuous advancements in very-large-scale integration
    (VLSI) technologies make post-quantum cryptosystems vulnerable to implementation
    attacks which are commonly referred to as side-channel attacks (SCAs). SCAs can
    be divided into passive or active attacks. Passive attacks are those whose aim
    is to exfiltrate sensitive information by analyzing various physical parameters
    of the system, e.g., power consumption, timing information, or electromagnetic
    radiation. Active attacks, on the other hand, intend to reveal the internal states
    of cryptographic implementations by injecting transient faults into the system,
    e.g., differential fault analysis (DFA). The consequences of these attacks range
    from the exploitation of sensitive data by third parties to causing an entire
    system to malfunction.


    Therefore, it is extremely important and necessary to explore countermeasures
    against SCAs for securing emerging post-quantum cryptosystems. This survey is
    on forward-looking, emerging security concerns in post-quantum era, i.e., the
    implementation attacks for 2022 winners of NIST PQC competition and thus the visions,
    insights, and discussions can be used as a step forward towards scrutinizing the
    new standards for applications ranging from Metaverse/Web 3.0 to deeplyembedded
    systems.


    The remainder of this survey is outlined as follows: Section 2 gives a technical
    background of the different PQC standards and finalists that are currently under
    a fourth evaluation round in the NIST PQC standardization process; Section 3 comprehensively
    reviews and analyzes different types of SCAs that have been implemented as well
    several countermeasures to counter them; and lastly, Section 4 concludes the survey.


    #### 2 PRELIMINARIES


    As shown in Table 1, three out of four standards are lattice-based, i.e., CRYSTALS-Kyber,
    CRYSTALS-Dilithium, and FALCON, while one of them, SPHINCS<sup>+</sup> is hash-based.
    The finalists are mostly


    code-based, except for SIKE, which is isogeny-based. However, after a classic
    attacks on only one core mounted by excellent research works, SIKE''s team acknowledged
    that SIKE and SIDH are insecure and should not be used.


    CRYSTALS-Kyber: It is the only PQC PKE/KEM that has been standardized. Its security
    depends on the hardness of solving the learning-with-errors (LWE) problem over
    module lattices. Both PKE and KEM are very similar; how-


    ever, the KEM uses a slightly tweaked Fujisaki–Okamoto (FO) transform. The LWE
    problem involves finding a small secret vector 𝑠 (secret key) when given a matrix𝐴
    over a constantsize polynomial ring and a vector 𝑏 = 𝐴𝑠 + 𝑒. To encode a message
    𝑚, a particular seed value 𝜇 is used, binomial sampling is employed to select
    random values (𝑟, 𝑒1, 𝑒2), and a uniform distribution is used to sample 𝐴 𝑇 .
    The values of 𝑢 and 𝑣 are then calculated by combining these elements with the
    message. Lastly, the ciphertext 𝑐 is formed by compressing 𝑢 and 𝑣 using a com-


    |              | Table 1. Current state of the NIST PQC Standardiza |

    |--------------|----------------------------------------------------|

    | tion Process |                                                    |


    | PQC Algorithm      | Status   | Type    | PKE/KEM vs.<br>Signature |  |

    |--------------------|----------|---------|--------------------------|--|

    | CRYSTALS-Kyber     |          |         | PEK/KEM                  |  |

    | CRYSTALS-Dilithium | Standard | Lattice | Signature                |  |

    | FALCON             |          |         |                          |  |

    | SPHINCS+           |          | Hash    |                          |  |

    | BIKE               |          |         |                          |  |

    | Classic McEliece   | Round 4  | Code    | PKE/KEM                  |  |

    | HQC                |          |         |                          |  |

    | SIKE               | Broken   | Isogeny |                          |  |


    pression algorithm. To decode the message, an approximation of 𝑣 is recovered
    by computing the product of the secret key and 𝑢. CRYSTALS-Kyber requires polynomial
    ring multiplications and it uses number-theoretic transform (NTT), which is an
    efficient way to perform multiplications in lattice-based cryptosystems; however,
    it is one of the major vulnerable points against SCA.


    CRYSTALS-Dilithium: Its security is based on the hardness of finding short vectors
    in lattices, known as the Shortest Vector Problem (SVP), and it operates over
    the ring 𝑍<sup>𝑞</sup> [𝑋]/(𝑋 <sup>𝑛</sup> + 1) with 𝑞 = 2 <sup>23</sup>−2 <sup>13</sup>+1
    and 𝑛 = 256. The key generation algorithm creates a matrix𝐴 and secret key vectors
    𝑠<sup>1</sup> and 𝑠<sup>2</sup> in such polynomial ring. The public key is then
    computed as𝑡 = 𝐴·𝑠<sup>1</sup> +𝑆2. The bulk of the signing and verification procedures
    in CRYSTALS-Dilithium involve two operations: expanding an XOF (eXtendable Output
    Function) using SHAKE-128 or SHAKE-256, and performing polynomial ring multiplication
    using NTT. To compute the signature, a masking vector of polynomials 𝑦 is multiplied
    with 𝐴 (where 𝑤<sup>1</sup> are the high-order bits), and a challenge 𝑐 is then
    created as the hash of the message and 𝑤1. Lastly, the potential signature is
    computed as 𝑧 = 𝑦 + 𝑐 · 𝑠1. Then, the verifier uses the public key and computes
    𝑤 ′ 1 to be the high-order bits of 𝐴 · 𝑧 − 𝑐 · 𝑡 and accepts the signature if
    all coefficients of 𝑧 are less than a threshold.


    FALCON: One of the major drawbacks of CRYSTALS-Dilithium is their large size signatures.
    Therefore, FALCON, another lattice-based cryptosystem whose signatures are of
    smaller size, has been standardized. Its underlying hard problem is the short
    integer solution problem (SIS) over NTRU lattices. The FALCON scheme is based
    on a GPV framework, which provides a way to construct signature schemes using
    lattice-based primitives. Another important aspect of FALCON is the use of Fast
    Fourier sampling, which improves FALCON''s efficiency and performance. In the
    key generation, two random polynomials 𝑓 and 𝑔 are chosen and the NTRU equation
    is solved to find a matching 𝐹 and 𝐺. To sign, the message is hashed along with
    a random nonce, into a polynomial 𝑐 modulo 𝜙, where 𝜙 = 𝑥 <sup>𝑛</sup> + 1 (𝑛
    is typically 512 or 1024). The signer then uses the secret lattice basis (𝑓 ,
    𝑔, 𝐹 , 𝐺) to produce a pair of short polynomials (𝑠1, 𝑠2) such that 𝑠1 = 𝑐 − 𝑠2ℎ
    mod 𝜙 mod 𝑞 (where ℎ = 𝑔/𝑓 and 𝑞 = 12289) and 𝑠<sup>2</sup> is the signature.
    The verifier needs to recompute 𝑠<sup>1</sup> from 𝑐 and 𝑠<sup>2</sup> and verify
    that (𝑠1, 𝑠2) is an appropriately short vector.


    SPHINCS<sup>+</sup> : It is the only stateless hash-based PQC cryptosystem that
    has been standardized to avoid relying only on the security of lattices for signatures.
    Depending on the hash function that


    -


    SPHINCS<sup>+</sup> is instantiated with, there are three different schemes: SPHINCS
    + -SHAKE256, SPHINCS<sup>+</sup> SHA-256, and SPHINCS<sup>+</sup> -Haraka. SPHINCS<sup>+</sup>
    is constructed using a Merkle tree structure where its leaves are the hash values
    of the message to be signed. First, the public and private keys are generated
    using a deterministic algorithm that takes a seed of length 𝑛 as input. SPHINCS<sup>+</sup>
    iteratively hashes and concatenates the leaf values with the intermediate values
    of the Merkle tree until the root value is obtained. The root value is then signed
    using the private key to generate the signature.


    BIKE: BItFlipping Key Encapsulation, or BIKE, may be regarded as the utilization
    of quasicyclic moderate density parity check (QC-MDPC) codes to instantiate the
    McEliece cryptosystem, using the equivalent Niederreiter scheme. BIKE has three
    different variants targeting two different security properties: Chosen plaintext
    attacks (CPA) and chosen ciphertext attacks (CCA) security. The key generation
    is almost identical in both the PKE and KEM processes. First, the secret key 𝑠𝑘
    is formed by two low-weight vectors ℎ<sup>0</sup> and ℎ<sup>1</sup> of length
    𝑟 that are uniformly picked from a secret key space 𝐻<sup>𝑤</sup> (a value 𝜎 is
    also used in case there is an error in the decapsulation KEM process). Then, the
    public key is computed as ℎ = ℎ1ℎ<sup>0</sup> − 1. For the PKE encryption process,
    the plaintext is represented by the sparse vector (𝑒0, 𝑒1), and the ciphertext
    by its syndrome 𝑠 obtained by following 𝑠 = 𝑒<sup>0</sup> + 𝑒<sup>1</sup> · ℎ.
    To decrypt it, a Black-Gray-Flip (BGF) decoder, which is defined in [2], is used
    to obtain the plaintext such as 𝐷𝑒𝑐𝑜𝑑𝑒𝑟(𝑠 · ℎ0, ℎ0, ℎ1). In the KEM encapsulation
    process, a random bitstring 𝑚 is selected and hashed, obtaining an error vector
    (𝑒0, 𝑒1) of weight 𝑡. A ciphertext 𝑐 is then calculated in two parts such as𝑐
    = (𝑒<sup>0</sup> +𝑒<sup>1</sup> ·ℎ,𝑚 ⊕𝐿(𝑒0, 𝑒1)), where 𝐿 is a hash function.
    Lastly, a shared key 𝐾 is obtained by hashing𝑚 and 𝑐. In the KEM decapsulation
    process, the shared key is obtained by using 𝑐 and 𝑠𝑘. First, an error vector
    𝑒 ′ is obtained by 𝑒 ′ = 𝐷𝑒𝑐𝑜𝑑𝑒𝑟( (𝑒0+𝑒1ℎ)ℎ0,ℎ0, ℎ1). Then, 𝑚′ = (𝑚 ⊕ 𝐿(𝑒0, 𝑒1))
    ⊕ 𝐿(𝑒 ′ ); if 𝑒 ′ matches 𝐻(𝑚) then 𝐾 is 𝐾(𝑚′ , 𝑐), otherwise 𝐾 is 𝐾(𝜎, 𝑐).


    Classic McEliece: It is a code-based cryptosystem based on binary Goppa codes
    and is widely regarded as secure. However, its large public key size is not desirable.
    McEliece generates a pair of keys using a code subspace dimension 𝑚, a maximum
    number of errors that can be corrected 𝑡, and a code length 𝑛. The private key
    consists of a monic irreducible polynomial called the Goppa polynomial with degree
    𝑡, which is generated randomly and all its coefficients are elements of a finite
    field 𝐺𝐹 (2 <sup>𝑚</sup>). The public key is obtained by constructing a control
    matrix 𝐻 based on the private key, permutating it using a random permutation matrix
    𝑃, and transforming it into a systematic form 𝐺. To encode a plaintext message
    𝑝, a random error vector 𝑒 of length 𝑛 and weight 𝑡 is created and the ciphertext𝑐
    is calculated as𝑐 = 𝑝 ·𝐺 ⊕ 𝑒. To decode the ciphertext, the error vector 𝑒 is
    first located using an error locator polynomial 𝜎(𝑥) and then the original plaintext
    is reconstructed.


    HQC: Hamming Quasi-Cyclic, or HQC, is an efficient encryption scheme based on
    coding theory. To have smaller keys than other code-based cryptosystems, HQC uses
    two different types of codes: A decodable [𝑛, 𝑘] code 𝐶 with a fixed generator
    matrix 𝐺 ∈ 𝐹 𝑘×𝑛 2 and error correction capability based on concatenated Reed-Muller
    and Reed-Solomon codes, and a random doublecirculant [2𝑛, 𝑛] code with a parity
    check matrix ℎ. In the key generation for both PKE and KEM, the parity check matrix
    ℎ in 𝑅 is generated and the secret key 𝑠𝑘 is created using polynomials 𝑥 and 𝑦
    in 𝑅 2 . Next, the public key 𝑝𝑘 is set such as 𝑝𝑘 = (𝐻, 𝑠 = 𝑥 +𝐻 ·𝑦). In the
    KEM process, three hash functions, named 𝐺, 𝐾, and 𝐻, are required. To encapsulate
    any random generated message 𝑚, the randomness 𝜃 for the encryption is first derived
    by 𝐺(𝑚). Then, a ciphertext𝑐 is generated by encrypting 𝑚 using 𝑝𝑘 and 𝜃. Lastly,
    a symmetric key 𝐾 is derived such as 𝑘 = 𝐾(𝑚, 𝑐) and the other party receives
    (𝑐, 𝑑), where 𝑑 = 𝐻(𝑚). To decapsulate,𝑐 is decrypted using 𝑠𝑘, obtaining 𝑚′ .
    To verify the integrity of 𝑐, 𝑚′ is re-encrypted using 𝜃 ′ , obtain another ciphertext𝑐
    ′ . If 𝑐 ′ matches 𝑐 and 𝑑 matches 𝐻(𝑚′ ), then 𝐾(𝑚, 𝑐) is the shared key. On
    the other hand, in the PKE process,


    | Algorithm       | Security<br>level | 𝑠𝑘 size<br>(bytes) | 𝑝𝑘 size<br>(bytes)
    | 𝑐𝑡 size<br>(bytes) | 𝑠𝑠 size<br>(bytes) | Other Parameters                          |

    |-----------------|-------------------|--------------------|--------------------|--------------------|--------------------|-------------------------------------------|

    | Kyber-512       | 1                 | 1,632              | 800                |
    768                | 32                 | 𝑛 = 256; 𝑘 = 2; 𝑞 = 3,329                 |

    | Kyber-768       | 3                 | 2,400              | 1184               |
    1088               | 32                 | 𝑛 = 256; 𝑘 = 3; 𝑞 = 3,329                 |

    | Kyber-1024      | 5                 | 3,168              | 1568               |
    1568               | 32                 | 𝑛 = 256; 𝑘 = 4; 𝑞 = 3,329                 |

    | BIKE-1          | 1                 | 2,244              | 12,323             |
    12,579             | 32                 | 𝑟 = 12,323; 𝑤 = 142; 𝑡 = 134              |

    | BIKE-3          | 3                 | 3,346              | 24,659             |
    24,915             | 32                 | 𝑟 = 24,659; 𝑤 = 206; 𝑡 = 199              |

    | BIKE-5          | 5                 | 4,640              | 40,973             |
    41,229             | 32                 | 𝑟 = 40,973; 𝑤 = 274; 𝑡 = 264              |

    | mceliece348864  | 1                 | 6,492              | 261,120            |
    96                 | 32                 | 𝑚 = 12; 𝑛 = 3,488; 𝑡 = 64                 |

    | mceliece460896  | 3                 | 13,608             | 524,160            |
    156                | 32                 | 𝑚 = 13; 𝑛 = 4,608; 𝑡 = 96                 |

    | mceliece6688128 | 5                 | 13,932             | 1,044,992          |
    208                | 32                 | 𝑚 = 13; 𝑛 = 6,688; 𝑡 = 128                |

    | mceliece6960119 | 5                 | 13,948             | 1,047,319          |
    194                | 32                 | 𝑚 = 13; 𝑛 = 6,960; 𝑡 = 119                |

    | mceliece8192128 | 5                 | 14,120             | 1,357,824          |
    208                | 32                 | 𝑚 = 13; 𝑛 = 8,192; 𝑡 = 128                |

    | hqc-128         | 1                 | 40                 | 2,249              |
    4,481              | 64                 | 𝑛1 = 46; 𝑛2 = 384;<br>𝑛 = 17,669; 𝑤
    = 66  |

    | hqc-192         | 3                 | 40                 | 4,522              |
    9,026              | 64                 | 𝑛1 = 56; 𝑛2 = 640;<br>𝑛 = 35,851; 𝑤
    = 100 |

    | hqc-256         | 5                 | 40                 | 7,245              |
    14,469             | 64                 | 𝑛1 = 90; 𝑛2 = 640;<br>𝑛 = 57,637; 𝑤
    = 131 |


    Table 2. Parameters of different post-quantum PKE/KEM algorithms


    Parameters for Kyber: 𝑛 is the polynomial length; 𝑘 is the size of polynomial
    vectors; 𝑞 is the prime modulus, BIKE: 𝑟 is the block size; 𝑤 is the row weight;
    𝑡 is the error weight, McEliece: 𝑚 is the code subspace; 𝑛 is the code length;
    𝑡 is the guaranteed error-correction capability, HQC: 𝑛<sup>1</sup> is the Reed-Solomon
    code length; 𝑛<sup>2</sup> is the Reed-Muller code length; 𝑛 is the vectors dimension;
    𝑤 is the vectors weight.


    vectors𝑟1, 𝑟2, and 𝑒, with a fixed hamming weight, are first sampled. Then, the
    ciphertext𝑐, which is a tuple 𝑒 with 𝑢 = 𝑟<sup>1</sup> + ℎ · 𝑟<sup>2</sup> and
    𝑣 = 𝑚𝐺 + 𝑠 · 𝑟<sup>2</sup> + 𝑒, is calculated. To decrypt 𝑐 and obtain the original
    message 𝑚, the term 𝑣 − 𝑢 · 𝑦 is decoded.


    #### 3 SCA AGAINST POST-QUANTUM ALGORITHMS AND COUNTERMEASURES


    This section evaluates some of the most up-to-date works on SCAs and respective
    countermeasures. Tables 2 and 3 show the different PQC scheme variants depending
    on their parameters.


    As previously mentioned, there are two types of attacks: Passive attacks and active
    attacks, also known as invasive attacks. For both types of attacks, the adversary
    needs to have access to the actual device where the cryptographic implementation
    is taking place. Once the adversary has access to the system, they can passively
    observe and analyze different leakages or actively influence it and evaluate their
    effects as shown in Fig. 1. In the following subsections, we summarize first the
    most common types of SCAs,


    Table 3. Parameters of different post-quantum signature algorithms.


    | Algorithm         | Security<br>level | 𝑝𝑘 size<br>(bytes) | signature<br>size
    (bytes) | Other parameters       |

    |-------------------|-------------------|--------------------|---------------------------|------------------------|

    | Dilithium2        | 2                 | 1,312              | 2,420                     |
    𝑛 = 256; 𝑞 = 8,380,417 |

    | Dilithium3        | 3                 | 1,952              | 3,293                     |
    𝑛 = 256; 𝑞 = 8,380,417 |

    | Dilithium5        | 5                 | 2,592              | 4,595                     |
    𝑛 = 256; 𝑞 = 8,380,417 |

    | FALCON-512 I      | 1                 | 897                | 666                       |
    𝑛 = 512; 𝑞 = 12,289    |

    | FALCON-1024 V     | 5                 | 1,793              | 1,280                     |
    𝑛 = 1,024; 𝑞 = 12,289  |

    | SPHINCS+<br>-128s | 1                 | 32                 | 7,856                     |
    𝑛 = 16; ℎ = 63; 𝑑 = 7  |

    | SPHINCS+<br>-128f | 1                 | 32                 | 17,088                    |
    𝑛 = 16; ℎ = 66; 𝑑 = 22 |

    | SPHINCS+<br>-192s | 3                 | 48                 | 16,224                    |
    𝑛 = 24; ℎ = 63; 𝑑 = 7  |

    | SPHINCS+<br>-192f | 3                 | 48                 | 35,664                    |
    𝑛 = 24; ℎ = 66; 𝑑 = 22 |

    | SPHINCS+<br>-256s | 5                 | 64                 | 29,792                    |
    𝑛 = 32; ℎ = 64; 𝑑 = 8  |

    | SPHINCS+<br>-256f | 5                 | 64                 | 49,856                    |
    𝑛 = 32; ℎ = 68; 𝑑 = 17 |


    Parameters for Dilithium: 𝑛 is the ring degree; 𝑞 is the prime modulus, FALCON:
    𝑛 is the ring degree; 𝑞 is the prime modulus, McEliece:𝑛 is the size of the hash
    output and the WOTS<sup>+</sup> and FORS signatures; ℎ is the height of each Merkle
    tree (determines the number of WOTS<sup>+</sup> signatures per layer); 𝑑 is the
    depth of the hypertree.


    then we discuss the most well-known countermeasures, and lastly, we present different
    SCA attacks found in the literature and several countermeasures against them for
    each PQC algorithm.


    #### 3.1 Types of SCAs and Countermeasures


    As mentioned previously, there are many types of SCAs and they can be either active
    or passive. Most of the current research focuses on passive differential power
    analysis (DPA) by analyzing the power consumption during one or multiple operations
    and active differential fault analysis (DFA); however, there are some other attacks
    that need to be considered, e.g., deep-learning-based SCAs to analyze patterns
    from the information extracted, and also timing/cache/algebraic/electromagnetic
    attacks. Profiling attacks entail the attacker possessing prior knowledge of the
    cryptosystem''s implementation for training and testing before the attack. Conversely,
    non-profiling attacks are characterized by the attacker''s lack of knowledge about
    the cryptosystem''s implementation, making them more challenging to execute than
    profiling attacks.


    Robust and adaptive countermeasures are essential for secure data communication
    against SCAs.


    These countermeasures can be implemented as either software-based solutions for
    passive SCAs or hardware-based implementations for active SCAs such as fault detection.
    Passive SCA countermeasures rely on obfuscating sensitive information via masking
    or shuffling to avoid any correlation between the plaintext data and the information
    leaked through power consumption, electromagnetic emissions, or timing variations.
    The software-based countermeasures include 1) Algorithmic modifica-


    ![](_page_5_Figure_5.jpeg)


    Fig. 1. SCA representation.


    tions, such as masking and blinding techniques, 2) Compiler-based modifications
    that obfuscate code order, and 3) Code obfuscation to create incoherence. Hardware-based
    countermeasures concentrate on physically securing the cryptographic algorithms
    against active SCA by adopting techniques such as power and electromagnetic shielding
    (threshold implementation) and error detection/correction codes to identify fault
    injections. Another effective strategy against SCAs involves increasing the system''s
    entropy.


    ### 3.2 CRYSTALS-Kyber SCAs and Countermeasures


    CRYSTALS-Kyber is the only PKE/KEM standardized in the PQC NIST competition and
    thus, one of the most evaluated and tested against SCAs. Carrera et al. [3] propose
    a non-profiled correlation electromagnetic analysis against a field programmable
    gate array (FPGA) implementation of Kyber-512, recovering the secret subkeys with
    a success rate of 100%, given the knowledge of register reference values (not
    full knowledge). Ji et al. [4] demonstrate a successful message (session key)
    recovery by using a profiling SCA, in particular a deep learning-based power analysis
    on a hardware implementation of Kyber768. All messages with the same enumeration
    were recovered due to their novel method called sliced multi-bit error injection.


    Some other attacks target specific building blocks or operations. In 2017, Primas
    et al. [5] presented the first single-trace attack on lattice-based encryption,
    claiming that a single side-channel observation is needed for full key recovery.
    This attack targets the NTT building block, which is part of the CRYSTALS-Kyber
    cryptosystem. Xu et al. [6] also targeted the NTT computation and proposed adaptive
    electromagnetic SCAs with carefully constructed ciphertexts, extracting the full
    secret key with between 8 and 960 traces. Pessl and Primas [7] changed the target
    to encryption to increase the single-trace attack performance. They implemented
    a successful attack against


    CRYSTALS-Kyber on an ARM Cortex M4 microcontroller assembly-optimized and designed
    to operate in constant time. Ravi et al. targeted the message decoding by proposing
    electromagnetic emanation-based SCAs and fault injection attacks [8].


    Dubrova et al. [9] perform deep learning-based message recovery attacks against
    CRYSTALS-Kyber using a new neural network training method called recursive learning.
    To train such neural networks, in the profiling stage, 30K power traces were collected
    from the decapsulation process of different ciphertexts for the same KEM pair
    and with a known keypair. The results showed that recovering a message bit from
    a single trace of a first-order masked implementation without cyclic rotations
    has a probability of 0.127%, but with cyclic rotations, the percentage increases
    to 87%. Furthermore, works [10, 11] present side-channel assisted message recovery
    attacks against CRYSTALS-Kyber to demonstrate that secret key recovery is possible
    in shuffled and masked implementations.


    In terms of active attacks (even though some previous works involved some fault
    injection), Espitau et al. [12] presented loop-abort faults on several lattice-based
    cryptosystems including CRYSTALS-Kyber. In this attack, a fault is injected into
    the cryptosystem causing a loop that samples random Gaussian secret coefficients
    to abort prematurely. This premature abortion results in the generation of abnormally
    low-dimensional secrets, which can be exploited to carry out a key recovery attack.
    However, the actual attack was not carried out for CRYSTALS-Kyber. In 2021, Pessl
    and Prokop [13] presented an attack requiring a single instruction-skipping fault
    in the decoding process. Through fault simulations, they demonstrated that a minimum
    of 6,500 faulty decapsulations are necessary to completely recover the key for
    Kyber512 running on a Cortex M4. Pessl and Prokop claimed that shuffling may make
    their attack unsuccessful. Therefore, in the same year, Hermelink et al. [14]
    use a combination of fault injections with chosen-ciphertext attacks against CRYSTALS-Kyber
    claiming that their attack may not be mitigated by shuffling the decoder. Their
    results show a successful secret key recovery with 7,500 inequalities for Kyber-512,
    10,500 inequalities for Kyber-768, and 11,000 inequalities for Kyber-1024. A year
    later, Delvaux [15] overhauled the SCA from [14] to make it easier to perform
    and harder to protect against by following four different strategies: Enlargement
    of the attack surface; relaxation of the fault model; applying masking and blinding
    methods; and accelerating and improving the error tolerance of solving the system
    of linear inequalities.


    Several SCA countermeasures have been proposed and a few of them have been implemented.
    Masking is one of the most common forms of protecting CRYSTALS-Kyber against SCAs,
    especially DPA attacks [16, 17, 18, 19]. Schneider et al. [16] introduce a secure
    binomial sampler that can provide protection against SCAs at any order. This is
    achieved through a Boolean and arithmetic (B2A) masking scheme conversion for
    prime moduli, suitable for CRYSTALS-Kyber. In [17], Bache et al. develop a more
    efficient higher-order masking scheme for lattice-based schemes with prime modulus.
    The scheme is proven in a probing model and tested on an ARM Cortex-M4F microcontroller,
    taking only 1.5-2.2 𝑚𝑠 to execute and protecting first-order leakage after collecting
    1 million power traces and applying 𝑡-test methodology. Bos et al. [18] also propose
    a masking implementation for a complete CRYSTALS-Kyber decapsulation, at both
    first and higher orders. Their approaches mask a one-bit compression and decompressed
    comparison and do not detect leakage after a Test Vector Leakage Assessment (TVLA)
    of 100,000 measurements. Kamucheka et al. [19] also propose a masked pure-hardware
    implementation of Kyber-512 and obtain 1.08x and 1.06x overheads in clock cycles
    and hardware resources when hiding and masking techniques are applied.


    Howe et al. [20] propose countermeasures against SCAs that use the statistical
    characteristics of the error samples, which are either Gaussian or binomial. The
    proposed countermeasures involve conducting statistical tests to ensure that the
    samplers are functioning correctly and take around 85% of the overall area consumption.
    Ausmita et al. [21] introduce new error detection schemes based on recomputing
    and embedded efficiently in the NTT accelerator architecture on FPGA. The results
    show a low overhead to detect close to 100% of errors. Moreover, also using recomputing,
    Cintas-Canto et al. [22] propose error detection schemes for lattice-based KEMs
    and implemented them on FPGA. Lastly, Heinz and Poppelmann [23] proposed an updated
    redundant number representation (RNR) approach to protect CRYSTALS-Kyber''s NTT
    architecture. Furthermore, a novel DFA countermeasure is derived and implemented
    using the Chinese Remainder Theorem (CRT). These techniques aim to protect the
    arithmetic operations of lattice-based cryptosystems and obtained a 2.2x computational
    overhead when applied to one execution of NTT of the Kyber-768 decryption process.


    #### 3.3 CRYSTALS-Dilithium SCAs and Countermeasures


    As we have seen for CRYSTALS-Kyber, the NTT architecture is a point of vulnerability
    against SCAs, especially DPA. While some works explore attacks on the NTT building
    block of specific cryptosystems, e.g., CRYSTALS-Kyber, they might apply to other
    lattice-based cryptosystems that use NTT such as CRYSTALS-Dilithium and FALCON.
    In [24], Steffen et al. presented the first power SCAs of CRYSTALS-Dilithium in
    reconfigurable hardware which include: Several profiled simple power analyses
    on Dilithium-2 and Dilithium-5 targeting the decoding and first NTT stage; and
    a correlation power analysis attack on the polynomial multiplication. The former
    had a 94.2% success probability to recover the correct coefficient when using
    single-trace attacks; successfully recovered the target coefficient with 50,000
    profiling traces when using multi-trace attacks on decoding; and was capable of
    full key recovery with 350,000 profiling traces when using multitrace attacks
    on first NTT stage. In regards to the CPA attack, they successfully recovered
    secret coefficients with 66,000 traces.


    Before such research, other works on DPA against CRYPTALS-Dilithium were investigated.
    In [25], Ravi et al. proposed a power analysis attack on the polynomial multiplier
    in CRYPTALS-Dilithium''s signing process, successfully retrieving a part of the
    secret key. Next, Karabulut et al. [26] proposed a single-trace SCA on 𝜔-small
    polynomial sampling software that reduces the challenge of polynomial''s entropy
    for CRYSTALS-Dilithium between 39 to 60 bits. The experiment was done using ARM
    Cortex-M4F. In the same year, Marzougui et al. [27] proposed an end-to-end (equivalent)
    key recovery attack on CRYSTALS-Dilithium based on a profiling-based power analysis
    attack combined with machine learning. The process only runs sections of the signature
    process and collects only the relevant power trace snippet to increase the attack
    efficiency, recovering the secret key after tracing the unpack polynomial function
    for 756,589 signatures.


    In 2018, Bruinderink and Pessl [28] presented a DFA attack on deterministic lattice
    signatures, which included CRYSTALS-Dilithium. By using linear algebra and lattice-basis
    reduction techniques, they show that a single random fault in the signing process
    can lead to a scenario of nonce-reuse (enabling key recovery) and that 65.2% of
    CRYSTALS-Dilithium''s execution time is susceptible to an unprofiled attack. A
    year later, also pointing out the determinism in lattice-based signatures, Ravi
    et al. [29] performed skip-addition fault attacks targeting the signing operation
    to extract a portion of the secret key. Additionally, they introduced a novel
    forgery method, enabling an attacker to sign any message using only that portion
    of the secret key. In [29], the authors also present a zero-cost mitigation strategy
    based on re-ordering the operations within the signing procedure to defend CRYSTALS-Dilithium
    against their attack, which increases the attack''s time and effort complexity
    by a 2<sup>20</sup> .


    Other CRYSTALS-Dilithium SCA countermeasures are found in [30, 20, 24]. Although
    there are no specific countermeasures for the CRYSTALS-Dilithium cryptosystem
    in [30], Bindel et al. mentioned several countermeasures such as masking, switching
    the order of operands, or storing the


    result of the addition in a variable different from the operands, applicable to
    several lattice-based signature schemes. Howe et al. [20], as mentioned earlier,
    propose fault attack countermeasures based on statistical tests for error samplers,
    which are designed to introduce noise and hide computations on secret information.
    The work of Steffen et al. [24] also presents different countermeasures based
    on arithmetic masking and integration of decoding into the first NTT stage, being
    able to protect the CRYSTALS-Dilithium cryptosystem from the attack previously
    mentioned.


    ## 3.4 FALCON SCAs and Countermeasures


    While several works perform SCAs against the Gaussian sampling algorithms used
    in FALCON, there are not too many specific attacks against the FALCON cryptosystem.
    In 2019, McCarthy et al. [31] proposed the first fault attack on the FALCON signature
    scheme, using a Basis Extraction by Aborting Recursion or Zeroing (BEARZ) technique.
    Through this attack, it is shown that FALCON is vulnerable to fault attacks on
    its Gaussian sampler and the output can reveal the private key. Moreover, three
    different countermeasures are proposed in [31]: Computing the signature twice,
    running the verification process immediately after signing, and applying a zero-check
    scheme, where the sampled vector is checked that does not go to zero at some point
    along its length at the end of the 𝑓 𝑓 𝑆𝑎𝑚𝑝𝑙𝑒𝑟 algorithm. The latter is proven
    to be the more successful against the SCAs carried in their work.


    A year later, Fouque et al. [32] pinpoint a particular timing leakage in the FALCON
    implementations, employing algebraic number theoretic techniques to retrieve the
    secret key. Such key retrieval transpires as a result of information exposure
    regarding the Gram-Schmidt norm, a crucial component for converting a group of
    linearly independent vectors into an orthonormal basis within the FALCON encryption
    system. The Gram-Schmidt process inherently reveals certain numerical properties
    of the original vectors allowing the full recovery of the secret key in FALCON-512.


    Karabulut and Aysu [33] propose an electromagnetic attack on the FALCON-512 cryptosystem
    to extract the secret signing keys by targeting the floating-point multiplications
    within FALCON''s Fast Fourier Transform. Their extend-and-prune strategy extracts
    the sign, mantissa, and exponent variables without false positives; showing that
    ~10k measurements are enough to reveal the secret key. Guerreau et al.[34] improve
    the attack of [33] in 2022 by exploiting the fact that the polynomial coefficients
    are integers. This leads to a reduction of the amount of traces needed (~5,000
    traces) for full key recovery. Additionally, they propose a practical but computationally
    expensive power analysis of FALCON''s Gaussian sampling algorithm, applying a
    parallelepiped-learning attack and needing ~10<sup>6</sup> traces for full key
    recovery in FALCON-512.


    Due to such expense, Zhang et al. [35] have developed several power analysis attacks
    on FAL-CON to significantly lower the requirement of measurements and computation
    resources from [34]. For the first attack, they discovered that the covariance
    of the samples in the slice, i.e., filtered signatures, suffices to reveal the
    secret, needing 220,000 traces instead of 10<sup>6</sup> . Moreover, they perform
    a practical power analysis targeting the integer Gaussian sampler of FALCON, relying
    on the leakage of random sign flip within the integer Gaussian sampling. This
    allows practical key recovery of FALCON-512 with 170,000 traces.


    In terms of SCAs countermeasures, besides [31] and [34], which briefly discuss
    a small modification of the C code to practically lower the Hamming weight gap,
    Sarker et al. [36] provide error detection schemes based on recomputing for FALCON''s
    sampler. Such schemes can detect close to 100% of the errors induced in the Gaussian
    sampler.


    # 3.5 SPHINCS<sup>+</sup> SCAs and Countermeasures


    SPHINCS<sup>+</sup> is the third and last PQC signature algorithm that has been
    standardized. The majority of SPHINCS<sup>+</sup> SCAs have been active attacks,
    and research has found that SPHINCS<sup>+</sup> is the most sensitive to fault
    attacks [37, 38, 40]. Castelnovi et al. proposed the first fault attack on the
    foundation of the SPHINCS<sup>+</sup> cryptosystem [37]. This two-phase attack
    allows the forgery of any message signature with just one faulty message. The
    first stage, known as the faulting phase, involves requesting two signatures for
    the same message. During the computation of the second signature, a fault is induced,
    causing a one-time signature (OTS) within the SPHINCS framework to sign a different
    value than previously. The subsequent stage, referred to as the grafting phase,
    demonstrates that the information from both signatures—the accurate one and the
    faulty one—can be utilized to uncover portions of the secret key from the OTS
    that experienced the fault, resulting in a partial compromise. The attacker then
    exploits this weakened OTS as a means of authenticating a distinct tree from the
    one it was intended to authenticate. The assailant generates a tree entirely under
    their control and employs the compromised OTS to graft it onto the SPHINCS tree.


    In efforts to provide a practical verification of [37], Genet et al. propose the
    first practical fault attack applied on an Arduino board for SPHINCS in [38],
    showing how a low-cost injection of a single glitch is sufficient to obtain exploitable
    faulty signatures. In the same year, 2018, Amiet et al. [39] presented the first
    hardware-based implementation of SPHINCS<sup>+</sup> and a fault attack against
    such hardware implementation. Amiet et al. discovered that a fault occurring in
    WOTS<sup>+</sup> subtree computations results in an altered root node value. This
    incorrect root node is subsequently signed with the next WOTS<sup>+</sup> level,
    leaking portions of the associated WOTS<sup>+</sup> private key. Consequently,
    such work demonstrates that, through a glitch attack, gathering private data to
    forge a signature can be accomplished in a matter of seconds. Additionally, a
    countermeasure based on doubling the entire SPHINCS<sup>+</sup> coprocessor is
    proposed in [40], similar to the recomputing approach suggested by [9]. Kannwischer
    et al. entirely exclude fault injection attacks to analyze the DPA vulnerability
    of XMSS and SPHINCS [40], and show a practical attack on the BLAKE-256-based PRF
    used within SPHINCS-256. Other works exclusively focus on providing SCA countermeasures
    for SPHINCS<sup>+</sup> cryptosystem [41, 42, 43]. Mozaffari-Kermani et al. [41,
    42] propose reliable and error detection hash trees for stateless hash-based signatures
    suitable to SPHINCS<sup>+</sup> . Their work presents two different approaches:
    Recomputing with swapped nodes (RESN) in the hash-tree constructions and combined
    signatures, and recomputing with encoded operands (REEO) for ChaCha, which is
    a stream cipher that SPHINCS uses for deriving two hash functions. The schemes
    detected close to 100% transient and permanent faults, adding up to 14.6% degradation
    overhead on applicationspecific integrated circuit (ASIC). The issue with these
    countermeasures is that they do not cover the entire SPHINCS<sup>+</sup> signing
    procedure. With this in mind, Genet [43] introduces a fault attack countermeasure
    based on caching the intermediate W-OTS<sup>+</sup> . However, this approach is
    useful for stateful schemes such as XMSS𝑀𝑇 but not for stateless schemes such
    as SPHINCS<sup>+</sup> . Therefore, recomputing schemes are suggested to be used
    to protect SPHINCS<sup>+</sup> against fault attacks [43].


    ## 3.6 BIKE SCAs and Countermeasures


    BIKE can be described as the McEliece scheme instantiated with QC-MDPC codes.
    In 2016, Guo et al. [44] introduced an attack using a recognized correlation between
    error patterns in decoding failures and the secret key, under the assumption that
    the scheme operates in a static key environment needing IND-CCA security. Such
    attack is implemented for 80-bit security QC-MDPC scheme, recovering the key in
    minutes. Two years later, an error amplification attack, built on the previous
    attack, is proposed [45]. This attack improves it by using just a single initial
    error vector,


    which leads to a decoding failure. It then adjusts this vector to efficiently
    produce numerous additional error vectors that also result in decoding failures.
    However, the attacks from [44, 45] can be avoided by stronger parameters.


    A more recent generic power/electromagnetic attack based on the Fujisaki–Okamoto
    (FO) transformation and its variants are proposed by Ueno et al. in [46]. This
    attack exploits side-channel leakage during the non-protected pseudorandom function
    (PRF) execution in the re-encryption of the KEM decapsulation and can be applied
    to CRYSTALS-Kyber, HQC, and BIKE.


    Since none of these attacks considered the non-constant time rejection sampling
    routine, which BIKE and HQC use to generate random vectors with a specific Hamming
    weight, Guo et al. [47] propose two novel timing attacks against BIKE and HQC
    achieving full secret key recovery. These attacks examine the time discrepancies
    caused by rejection sampling, as they could reveal whether the input message to
    the deterministic re-encryption process (or a hash function) in the IND-CCA transformation
    remains unaltered. Possessing such secret information is sufficient for retrieving
    the secret key of BIKE and HQC schemes. To fix the non-isochronous design of BIKE,
    Sendrier [48] replaces the rejection sampling in the encapsulation and the decapsulation
    with an algorithm that has no rejection, generating a non-uniform distribution
    of the indices. Additionally, Drucker et al. [49] propose to use the fixed sampling
    number (FSN) version of the errors-vector generation (EVG), with some predetermined
    value of X. This value does not change the required uniform distribution property
    of the generated errors-vector.


    Chou et al. [50] also propose a constant-time implementation for QC-MDPC code-based
    cryptography to counter timing attacks. Nevertheless, this countermeasure was
    later identified as susceptible to a DPA in private syndrome computation [51],
    although the attack was unable to fully retrieve the correct secret indices. Thus,
    Sim et al. [52] enhance existing multiple-trace attacks on timing attack countermeasures
    and propose a novel single-trace attack, which allows to recover secret indices
    even when using ephemeral keys or DPA countermeasures.


    #### 3.7 McEliece SCAs and Countermeasures


    Timing attacks are one of the first SCAs carried on the McEliece cryptosystem
    [53, 54, 55]. Strenzke et al. [53] present a timing attack on the degree of the
    error locator polynomial, which is executed successfully against a software implementation
    of the McEliece cryptosystem. Therefore, raising its degree artificially is proposed
    as a countermeasure. Avanzi et al. improve the timing attack from [53] with a
    setup stage that involves profiling the algorithm for all correctable error weights,
    followed by an iterative procedure that approximates the random error vector.
    Additionally, a "non-support" countermeasure is proposed. In [54], Shoufan et
    al. propose a timing attack against the Patterson algorithm in the McEliece cryptosystem.
    In [56], Lahr et al. adapt the side-channel attack from [54] and perform an electromagnetic
    attack using a reaction-based attack combined with a technique that they call
    iterative chunking. This method allows them to progressively increase the quantity
    of discovered error positions (chunks) within a single (cumulative) query. Such
    attack is performed on a microcontroller targeting the matrix-vector multiplication
    of the encryption process and recovering the message from one faulty syndrome
    and the public key. A practical evaluation of the attack is performed on FPGA
    and it is shown that ~560 measurements are sufficient to mount a successful plaintext
    recovery attack. Moreover, Strenzke [55] develops a strategy how to exploit a
    vulnerability in the Patterson algorithm, enabling the attacker to obtain information
    about the secret permutation via a timing side channel.


    Not only timing attacks have been studied, but also fault injection attacks [57,
    58, 59], power analysis attacks [60, 61], and message-recovery attacks [62]. In
    [57], Cayrel and Dusart present a fault injection attack on different variables
    of the McEliece schemes and the possible outcomes are discussed; however, no implementation
    is performed. A few years later, Cayrel et al. [58] perform a message-recovery
    laser fault injection attack targeting the syndrome decoding problem on the Classic
    McEliece cryptosystem. Several experiments are conducted on a 6-core CPU clocked
    at 2.8 GHz and 32 GB of RAM desktop computer to validate the success of the attack,
    which show the secret message can be retrieved in less than three seconds. Pircher
    et al. [59] recently introduced a key-recovery fault injection attack targeting
    the Goppa code''s error-locator polynomial and the decryption algorithm''s validity
    checks, thus making a chosen ciphertext attack feasible.


    When considering power analysis attacks, Molter et al. [60] introduced a simple
    SCA on a McEliece cryptoprocessor using power analysis. This FPGA-based attack
    exploits an information leak resulting from the correlation between the error
    vector weight and the number of iterations in the extended Euclidean algorithm
    used in the Patterson Algorithm (as in [54]). In a separate study, Guo et al.
    [61] formulated an attack algorithm where unique ciphertexts, corresponding to
    single-error cases, are submitted to the decryption oracle. Decoding these ciphertexts
    involves only a single entry in an extensive secret permutation, which forms part
    of the secret key. By identifying a leak in the additive FFT step, which is used
    to evaluate the error locator polynomial, it is possible to determine a single
    entry of the secret permutation. Repeating this process for other entries results
    in full secret key recovery. The attack employs power analysis on FPGA and ARM
    Cortex-M4, alongside a machine-learning-based classification algorithm to identify
    the error locator polynomial from a single trace. The findings show that full
    key recovery can be achieved with less than 800 traces. Lastly, in [62], Colombier
    et al. conduct a side-channel attack by analyzing power consumption during the
    matrix-vector multiplication phase of the encryption process.


    Other SCA countermeasures are discussed in [63, 64, 65, 66, 67, 68, 69]. The simple
    power analysis countermeasure proposed in [63] is based on avoiding branch statements
    and data-dependent timing on the implementation of the McEliece cryptosystem.
    This countermeasure is tested on an ARM Cortex-M3, preventing simple power analysis
    and timing attacks but increasing the latency by a factor of 3. In [64, 65], natural
    and injected fault detection schemes based on CRC and cyclic codes are proposed,
    respectively. These schemes target the finite field multipliers used in codebased
    cryptosystems such as Classic McEliece and are implemented on FPGA detection close
    to 100% faults. Moreover, Cintas-Canto et al. present error detection schemes
    based on single parity, interleaved parity, CRC, and Hamming codes for the 𝐺𝐹
    (2 <sup>𝑚</sup>) inversion block [66, 67] and composite field arithmetic architectures
    [68] that the McEliece cryptosystem employs. Additionally, fault detection schemes
    based on CRC are proposed for the different blocks of the McEliece key generator
    in [69]. After being implemented on FPGA, the schemes detected close to 100% of
    faults and added a worst-case area and delay overhead of 49%.


    ## 3.8 HQC SCAs and Countermeasures


    Some BIKE SCAs are applicable to the HQC cryptosystem since they share some operational
    architectures. One example of this is the work of Guo et al. [47], which as it
    was mentioned before, proposes two novel timing attacks against BIKE and HQC achieving
    full secret key recovery. Another timing attack is presented in [70] by Huang
    et al., in which a cache-timing-based distinguisher for implementing a plaintext-checking
    (PC) oracle is presented. This PC oracle employs side-channel information to verify
    whether a given ciphertext decrypts to a specific message. Furthermore, a practical
    attack is presented on an HQC execution on Intel SGX, necessitating an average
    of 53,857 traces for complete key recovery. This attack demands significantly
    fewer PC oracle calls than Guo et al.''s timing attack in [47].


    Apart from timing attacks, HQC has also been a target of power analysis attacks.
    In [71], Schamberger et al. propose the first power SCA on the KEM version of
    HQC. This attack uses a power side-channel to create an oracle that determines
    whether the BCH decoder in HQC''s decryption algorithm rectifies an error for
    a chosen ciphertext. Considering the decoding algorithm employed


    in HQC, it is demonstrated how to craft queries so that the oracle''s response
    enables the extraction of a significant portion of the secret key. The remaining
    part of the key can subsequently be discovered using a linear algebra-based algorithm.
    Experiments show that fewer than 10,000 measurements are enough to successfully
    execute the attack on the HQC reference implementation running on an ARM Cortex-M4
    microcontroller. Another power analysis attack is presented in [72], where the
    authors showcase a novel, proven power SCA that enables a successful power SCA
    against the updated round three version of the HQC cryptosystem (a Reed-Muller
    and Reed-Solomon version of HQC). This attack reduces the required attack queries
    of [47] by a factor of 12 and eliminates the inherent uncertainty of their employed
    timing oracle. The general idea of the attack is to choose 𝑣 such that the decoding
    result depends on 𝑦 (0) 𝑖 (where 𝑦 is the secret key polynomial), revealing its
    support. This attack is also implemented on an ARM Cortex-M4 microcontroller.
    Lastly, Goy et al. [73] introduce a new key recovery side-channel attack on HQC
    with chosen ciphertext. This attack exploits the reuse of a static secret key
    on a microcontroller, recovering the static secret key by targeting the Reed-Muller
    decoding step of the decapsulation, specifically focusing on the Hadamard transform.
    The side-channel information obtained in the function is used to build an Oracle
    that distinguishes between several decoding patterns of the Reed-Muller codes.
    Moreover, they show how to query the Oracle such that the responses give full
    information about the static secret key. Experiment results indicate that fewer
    than 20,000 electromagnetic attack traces are enough to recover the entire static
    secret key that the decapsulation uses. As a countermeasure, the authors propose
    a masking-based structure against Reed-Muller decoding distinguisher.


    # 4 CONCLUSION


    Due to the imminent threat that quantum computers pose to current public-key cryptographic
    algorithms, there has been a extensive research on PQC. This survey englobes a
    comprehensive exploration of PQC, highlighting that PQC, while designed to be
    secured against classical and quantum computers, is still vulnerable to SCAs.
    These attacks, both passive and active, are a significant risk as they facilitate
    key recovery. This review further elaborates on several forms of SCAs and countermeasures
    to mitigate them. It is evident that while advancements in PQC are significant,
    the reliability of these algorithms is greatly influenced by their vulnerability
    to SCA. Thus, the field of PQC needs ongoing research and development to ensure
    not just security from quantum computing threats, but also reliability against
    SCAs.


    ## ACKNOWLEDGEMENTS


    This work was supported by the US National Science Foundation (NSF) award SaTC-1801488.


    ## REFERENCES


    - [1] D. Moody. Post-Quantum Cryptography: NIST''s Plan for the Future. Feb. 2016.

    - [2] N. Drucker, S. Gueron, D. Kostic. QC-MDPC decoders with several shades of
    gray. PQCrypto, pp. 35-50, 2020.

    - [3] R. C. Rodriguez, F. Bruguier, E. Valea, and P. Benoit. Correlation electromagnetic
    analysis on an FPGA implementation of CRYSTALS-Kyber. Cryptology ePrint Archive,
    2022.

    - [4] Y. Ji, R. Wang, K. Ngo, and E. Dubrova. A side-channel attack on a HW implementation
    of Kyber. ETS, 2023.

    - [5] R. Primas, P. Pessl, and S. Mangard. Single-trace side-channel attacks on
    masked lattice-based encryption. CHES 2017, pp. 513-533, Springer, 2017.

    - [6] Z. Xu, O. Pemberton, S. Roy, D. Oswald, and Z. Zheng. Magnifying side-channel
    leakage of lattice-based cryptosystems with chosen ciphertexts: The case study
    of kyber. IEEE Trans. Comput., vol. 71, no. 9, pp. 2163-2176, 2021.

    - [7] P. Pessl and R. Primas. More practical single-trace attacks on the number
    theoretic transform. LATINCRYPT. vol. 6, pp. 130-149, Springer, 2019.

    - [8] P. Ravi, S. Bhasin, S. S. Roy, and A. Chattopadhyay. Drop by drop you break
    the rock-exploiting generic vulnerabilities in lattice-based PKE/KEMs using EM-based
    physical attacks. IACR Cryptology ePrint Archives, Report 549, 2020.

    - [9] E. Dubrova, K. Ngo, and J. Grtner. Breaking a fifth-order masked implementation
    of CRYSTALS-Kyber by copy-paste. Cryptology ePrint Archive, 2022.

    - [10] L. Backlund, K. Ngo, J. Grtner, and E. Dubrova. Secret key recovery attacks
    on masked and shuffled implementations of CRYSTALS-Kyber and Saber. Cryptology
    ePrint Archive, 2022.

    - [11] P. Ravi, S. Bhasin, S. S. Roy, and A. Chattopadhyay. On exploiting message
    leakage in (few) NIST PQC candidates for practical message recovery attacks. IEEE
    Trans. Information Forensics and Security, vol. 17, pp. 684-699, 2021.

    - [12] T. Espitau, P. A. Fouque, B. Gerard, and M. Tibouchi. Loop-abort faults
    on lattice-based signature schemes and key exchange protocols. IEEE Transactions
    on Computers, vol. 67, no. 11, pp.1535-1549, 2018.

    - [13] P. Pessl and L. Prokop. Fault attacks on CCA-secure lattice KEMs. IACR
    Transactions on Cryptographic Hardware and Embedded Systems, pp. 37-60, 2021.

    - [14] J. Hermelink, P. Pessl, and T. Poppelmann. Fault-enabled chosen-ciphertext
    attacks on Kyber. INDOCRYPT, pp. 311- 334. Springer, 2021.

    - [15] J. Delvaux. Roulette: A diverse family of feasible fault attacks on masked
    Kyber. Cryptology ePrint Archive, 2021.

    - [16] T. Schneider, C. Paglialonga, T. Oder, and T. Guneysu. Efficiently masking
    binomial sampling at arbitrary orders for lattice-based crypto. PKC 2019. pp.
    534-564, Springer, 2019.

    - [17] F. Bache, C. Paglialonga, T. Oder, T. Schneider, and T. Guneysu. High-speed
    masking for polynomial comparison in lattice-based KEMs. IACR Transactions on
    Cryptographic Hardware and Embedded Systems, pp. 483-507, 2020.

    - [18] J. W. Bos, M. Gourjon, J. Renes, T. Schneider, and C. V. Vredendaal. Masking
    Kyber: First-and higher-order implementations. IACR Transactions on Cryptographic
    Hardware and Embedded Systems, pp. 173-214, 2021.

    - [19] T. Kamucheka, A. Nelson, D. Andrews, and M. Huang. A masked pure-hardware
    implementation of Kyber cryptographic algorithm. ICFPT. pp. 1-9, 2022.

    - [20] J. Howe, A. Khalid, M. Martinoli, F. Regazzoni, and E. Oswald. Fault attack
    countermeasures for error samplers in lattice-based cryptography. ISCAS. pp. 1-5,
    2019.

    - [21] A. Sarker, A. Cintas-Canto, M. Mozaffari-Kermani, and R. Azarderakhsh.
    Error detection architectures for hardware/software co-design approaches of number
    theoretic transform. IEEE Transactions on Computer-Aided Design Integrated Circuits
    Systems, accepted, to appear 2023.

    - [22] A. Cintas-Canto, A. Sarker, J. Kaur, M. Mozaffari-Kermani, and R. Azarderakhsh.
    Error detection schemes assessed on FPGA for multipliers in lattice-based key
    encapsulation mechanisms in post-quantum cryptography. IEEE Transactions on Emerging
    Topics in Computing, accepted, to appear 2023.

    - [23] D. Heinz and T. Poppelmann. Combined fault and DPA protection for lattice-based
    cryptography. IEEE Transactions on Computers, 2022.

    - [24] H. Steffen, G. Land, L. Kogelheide, and T. Guneysu. Breaking and protecting
    the crystal: Side-channel analysis of Dilithium in hardware. Cryptology ePrint
    Archive, 2022.

    - [25] P. Ravi, M. P. Jhanwar, J. Howe, A. Chattopadhyay, and S. Bhasin. Side-channel
    assisted existential forgery attack on Dilithium-a NIST PQC candidate. Cryptology
    ePrint Archive, 2018.

    - [26] E. Karabulut, E. Alkim, and A. Aysu. Single-trace side-channel attacks
    on w-small polynomial sampling: with applications to NTRU, NTRU prime, and CRYSTALS-Dilithium.
    HOST, pp. 35-45, 2021.

    - [27] S. Marzougui, V. Ulitzsch, M. Tibouchi, and J. P. Seifert. Profiling side-channel
    attacks on Dilithium: A small bit-fiddling leak breaks it all. Cryptology ePrint
    Archive, 2022.

    - [28] L. G. Bruinderink and P. Pessl. Differential fault attacks on deterministic
    lattice signatures. IACR Transactions on Cryptographic Hardware and Embedded Systems,
    pp. 21-43, 2018.

    - [29] P. Ravi, M. P. Jhanwar, J. Howe, A. Chattopadhyay, and S. Bhasin. Exploiting
    determinism in lattice-based signatures: Practical fault attacks on pqm4 implementations
    of NIST candidates. ACM Asia CCS, pp. 427-440, 2019.

    - [30] N. Bindel, J. Krmer, and J. Schreiber. Hampering fault attacks against
    lattice-based signature schemes: countermeasures and their efficiency (special
    session). Hardware/Software Codesign and System Synthesis Companion, pp. 1-3,
    2017.

    - [31] S. McCarthy, J. Howe, N. Smyth, S. Brannigan, and M. O''Neill. BEARZ attack
    FALCON: implementation attacks with countermeasures on the FALCON signature scheme.
    Cryptology ePrint Archiv, 2019.

    - [32] P. A. Fouque, P. Kirchner, M. Tibouchi, A. Wallet, and Y. Yu. Key recovery
    from Gram–Schmidt norm leakage in hash-and-sign signatures over NTRU lattices.
    EUROCRYPT, pp. 34-63, Springer, 2020.

    - [33] E. Karabulut and A. Aysu. FALCON down: Breaking FALCON post-quantum signature
    scheme through side-channel attacks. DAC, pp. 691-696, 2021.

    - [34] M. Guerreau, A. Martinelli, T. Ricosset, and M. Rossi. The hidden parallelepiped
    is back again: Power analysis attacks on FALCON. IACR Transactions on Cryptographic
    Hardware and Embedded Systems, pp. 141-164, 2022.

    - [35] S. Zhang, X. Lin, Y. Yu, and W. Wang. Improved Power Analysis Attacks on
    FALCON. EUROCRYPT. pp. 565-595, Springer, 2023.


    - [36] A. Sarker, M. Mozaffari-Kermani, and R. Azarderakhsh. Efficient error detection
    architectures for post-quantum signature FALCON''s Sampler and KEM Saber. IEEE
    Trans. VLSI Systems, vol. 30, no. 6, pp. 794-802, 2022.

    - [37] L. Castelnovi, A. Martinelli, and T. Prest. Grafting trees: a fault attack
    against the SPHINCS framework. PQCrypto, Proceedings 9, pp. 165-184, Springer,
    2018.

    - [38] A. Gent, M. J. Kannwischer, H. Pelletier, and A. McLauchlan. Practical
    fault injection attacks on SPHINCS. Cryptology ePrint Archive, 2018.

    - [39] D. Amiet, L. Leuenberger, A. Curiger, and P. Zbinden. FPGA-based SPHINCS<sup>+</sup>
    implementations: Mind the glitch. DSD, pp. 229-237, 2020.

    - [40] M. J. Kannwischer, A. Gent, D. Butin, J. Krmer, and J. Buchmann. Differential
    power analysis of XMSS and SPHINCS. COSADE, pp. 168-188, Springer, 2018.

    - [41] M. Mozaffari-Kermani, R. Azarderakhsh, and A. Aghaie. Fault detection architectures
    for post-quantum cryptographic stateless hash-based secure signatures benchmarked
    on ASIC. ACM Transactions on Embedded Computing Systems, vol. 16, no. 2, pp. 59:1-19,
    2016.

    - [42] M. Mozaffari-Kermani and R. Azarderakhsh. Reliable hash trees for post-quantum
    stateless cryptographic hash-based signatures. DFTS, pp. 103-108, 2015.

    - [43] A. Gent. On protecting SPHINCS<sup>+</sup> against fault attacks. Cryptology
    ePrint Archive, 2023.

    - [44] Q. Guo, T Johansson, and P. Stankovski. A key recovery attack on MDPC with
    CCA security using decoding errors. ASIACRYPT, pp. 789-815, Springer, 2016.

    - [45] A. Nilsson, T. Johansson, and P. S. Wagner. Error amplification in code-based
    cryptography. Cryptology ePrint, 2018.

    - [46] R. Ueno, K. Xagawa, Y. Tanaka, A. Ito, J. Takahashi, and N. Homm. Curse
    of re-encryption: A generic power/em analysis on post-quantum kems. IACR Trans.
    Cryptographic Hardware and Embedded Systems, pp. 296-322, 2022.

    - [47] Q. Guo, C. Hlauschek, T. Johansson, N. Lahr, A. Nilsson, and R. L. Schrder.
    Don''t reject this: Key-recovery timing attacks due to rejection-sampling in HQC
    and BIKE. IACR Trans. CHES, pp. 223-263, 2022.

    - [48] N. Sendrier. Secure sampling of constant-weight words–application to bike.
    Cryptology ePrint Archive, 2021.

    - [49] N. Drucker, S. Gueron, and D. Kostic. To reject or not reject: That is
    the question. The case of BIKE post quantum KEM. Information Technology-New Generations,
    pp. 125-131, Springer, 2012.

    - [50] T. Chou. QcBits: constant-time small-key code-based cryptography. CHES,
    pp. 280-300, Springer, 2016.

    - [51] M. Rossi, M. Hamburg, M. Hutter, and M. E. Marson. A side-channel assisted
    cryptanalytic attack against QcBits. CHES, pp. 3-23, Springer, 2017.

    - [52] B.Y. Sim, J. Kwon, K. Y. Choi, J. Cho, A. Park, and D.-G. Han. Novel side-channel
    attacks on quasi-cyclic code-based cryptography. IACR Transactions on Cryptographic
    Hardware and Embedded Systems, pp. 180-212, 2019.

    - [53] F. Strenzke, E. Tews, H. G. Molter, R. Overbeck, and A. Shoufan. Side channels
    in the McEliece PKC. PQCrypto, pp. 216-229, Springer, 2008.

    - [54] A. Shoufan, F. Strenzke, H. G. Molter, and M. Stttinger. A timing attack
    against Patterson algorithm in the McEliece PKC. ICISC, pp. 161-175, Springer,
    2010.

    - [55] F. Strenzke. A timing attack against the secret permutation in the McEliece
    PKC.PQCrypto, pp. 95-107, Springer, 2010.

    - [56] N. Lahr, R. Niederhagen, R. Petri, and S. Samardjiska. Side channel information
    set decoding using iterative chunking: Plaintext recovery from the Classic McEliece
    hardware reference implementation. ASIACRYPT, pp. 881-910, 2020.

    - [57] P. L. Cayrel and P. Dusart. McEliece/Niederreiter PKC: Sensitivity to fault
    injection. Fut. Inf. Tech., pp. 1-6, 2010.

    - [58] P. L. Cayrel, B. Colombier, V. F. Drăgoi, A. Menu, and L. Bossuet. Message-recovery
    laser fault injection attack on the classic McEliece cryptosystem. EUROCRYPT,
    pp. 438-467, Springer, 2021.

    - [59] S. Pircher, J. Geier, J. Danner, D. Mueller-Gritschneder, and A. Wachter-Zeh.
    Key-recovery fault injection attack on the Classic McEliece KEM. Code-Based Cryptography
    Workshop, pp. 37-61, Springer, 2023.

    - [60] H. G. Molter, M. Stttinger, A. Shoufan, and F. Strenzke. A simple power
    analysis attack on a McEliece cryptoprocessor. Journal of Cryptographic Engineering
    vol. 1, pp. 29-36, 2011.

    - [61] Q. Guo, A. Johansson, and T. Johansson. A key-recovery side-channel attack
    on classic McEliece. ePrint, 2022.

    - [62] B. Colombier, V. F. Drăgoi, P. L. Cayrel, and V. Grosso. Profiled side-channel
    attack on cryptosystems based on the binary syndrome decoding problem. IEEE Trans.
    Information Forensics and Security, vol. 17, pp.3407-3420, 2022.

    - [63] M. Petrvalsky, T. Richmond, M. Drutarovsky, P. L. Cayrel, and V. Fischer.
    Countermeasure against the SPA attack on an embedded McEliece cryptosystem. RADIOELEKTRONIKA,
    pp. 462-466, 2015.

    - [64] A. Cintas-Canto, M. Mozaffari-Kermani, R. Azarderakhsh. Reliable CRC-based
    error detection constructions for finite field multipliers with applications in
    cryptography. IEEE Trans. VLSI Systems, vol. 29, no. 1, pp. 232-236, 2021.

    - [65] A. Cintas-Canto, M. Mozaffari-Kermani, and R. Azarderakhsh. Reliable architectures
    for finite field multipliers using cyclic codes on FPGA utilized in classic and
    post-quantum cryptography. IEEE Trans. VLSI Systems, vol. 1, no. 31, pp. 157-161,
    2023.

    - [66] A. Cintas-Canto, M. Mozaffari-Kermani, and R. Azarderakhsh. CRC-based error
    detection constructions for FLT and ITA finite field inversions over 𝐺𝐹 (2<sup>𝑚</sup>
    ). IEEE Trans. VLSI Systems, vol. 29, no. 5, pp. 1033-1037, 2021.

    - <span id="page-15-0"></span>[67] A. Cintas-Canto, M. Mozaffari-Kermani, and
    R. Azarderakhsh. Error detection constructions for ITA finite field inversions
    over 𝐺𝐹 (2<sup>𝑚</sup> ) on FPGA using CRC and hamming codes. IEEE Trans. Reliability,
    to appear 2023.

    - [68] A. Cintas-Canto, M. Mozaffari-Kermani, and R. Azarderakhsh. Reliable architectures
    for composite-field-oriented constructions of McEliece post-quantum cryptography
    on FPGA. IEEE Transactions on Computer-Aided Design Integr. Circuits Syst., vol.
    40, no. 5, pp. 999-1003, 2021.

    - [69] A. Cintas-Canto, M. Mozaffari-Kermani, and R. Azarderakhsh. Reliable constructions
    for the key generator of codebased post-quantum cryptosystems on FPGA. ACM Emerging
    Technologies in Computing Systems (special issue on CAD for Hardware Security),
    vol. 29, no. 1, pp. 5:1-5:20, 2023.

    - [70] S. Huang, R. Sim, C. Chuengsatiansup, Q. Guo, T. Johansson. Cache-timing
    attack against HQC. ePrint, 2023.

    - [71] T. Schamberger, J. Renner, G. Sigl, and A. Wachter-Zeh. A power side-channel
    attack on the CCA2-secure HQC KEM. CARDIS 2020, pp. 119-134, Springer, 2021.

    - [72] T. Schamberger, L. Holzbaur, J. Renner, A. Wachter-Zeh, and G. Sigl. A
    power side-channel attack on the reed-muller reed-solomon version of the HQC cryptosystem.
    PQCrypto, pp. 327-352, Springer, 2022.

    - [73] G. Goy, A. Loiseau, and P. Gaborit. A new key recovery side-channel attack
    on HQC with chosen ciphertext. PQCrypto 2022, pp. 353-371, Springer, 2022.'
- title: 'LRSCwait: Enabling Scalable and Efficient Synchronization in Manycore Systems
    through Polling-Free and Retry-Free Operation'
  abstract: ''
  keywords: atomics, synchronization, manycore, RISC-V
  document: '## I. INTRODUCTION


    Manycore systems are becoming increasingly popular due to the growing demand for
    computing power. However, the parallel execution of tasks introduces synchronization
    and atomicity issues that can lead to race conditions and unpredictable results.
    To ensure exclusive access to critical sections (CSs), atomic operations and locks
    can be used. However, locks also block cores that try to acquire them when they
    are not free, leading to busy waiting and polling. Polling, or constantly checking
    a shared resource for changes, can become an issue in concurrent algorithms. It
    leads to high core utilization and reduces overall system performance and energy
    efficiency as the cores compete for shared resources [\[1\]](#page-5-0). In the
    worst case, it can lead to livelocks or starvation, where cores are blocked from
    making progress because others continuously block them.


    Non-blocking algorithms avoid locks by updating atomic variables directly with
    atomic read–modify–write (RMW) operations. Specific arithmetic operations, like
    *add, and, or*, are often supported through specialized instructions. However,
    most concurrent algorithms require more complex modifications of atomic variables,
    such as conditional updates. For generic RMW operations, the compare-and-swap
    (CAS) operations or loadreserved/store-conditional (LRSC) pair are typical primitives
    designed to ensure that the operation is *atomic*, i.e., without interference
    from other cores [\[2\]](#page-5-1). For example, RISC-V''s loadreserved (LR)
    instruction loads a value from memory and


    places a reservation. The core can perform operations with the loaded value and
    store the result back conditionally with a store-conditional (SC). The latter
    instruction will only succeed if the reservation is still valid, meaning the memory
    location was not modified in the meantime. If the SC succeeds, the RMW sequence
    appears atomically. However, cores that fail an SC must retry the LRSC sequence
    pair until it succeeds. Variables outside CSs can also cause polling, where cores
    wait for changes in shared variables, leading to inefficiencies in core communication,
    like producer/consumer interactions.


    To eliminate retries and polling, we propose a novel, generalpurpose atomic RMW
    instruction pair called LRwait and SCwait. They extend the standard RISC-V LRSC
    pair by moving the linearization point, the point where the atomic operations
    of different cores get ordered, from the SC to the LRwait. The LRwait and SCwait
    are used in the same way as the LRSC pair. However, instead of returning the memory
    value immediately, the LRwait instruction only responds to one core at a time
    to set it up for a successful SCwait. This prevents failing SCs and retry loops.
    Furthermore, LRSCwait allows implementing polling-free locks. To eliminate polling
    even for non-atomic variables, we propose the Mwait instruction, which enables
    cores to sleep until a specific memory address changes its value.


    While cache-based systems often rely on the coherency protocol to implement such
    behavior, manycore accelerators scaling to hundreds of cores often rely on software-managed,
    multi-banked scratchpad memories (SPMs). Examples include commercial chips like
    GAP9 [\[3\]](#page-5-2) and RC64 [\[4\]](#page-5-3), as well as largescale research
    prototypes like MemPool [\[5\]](#page-5-4). While LRSCwait can be applied to cache
    and cache-less systems, in this work, we focus on cache-less, SPM-based manycore
    systems since they pose the design challenge of the memory controllers having
    to keep track of outstanding LRwait instructions to send their responses at the
    right time. However, duplicating large hardware queues for each bank is costly
    and scales poorly.


    As a scalable implementation of the proposed instructions, we present *Colibri*.
    Its concept is similar to linked-list-based software queues. It does not allocate
    a full array of entries for each queue but just a head and tail pointer per queue
    as illustrated in [Fig. 1.](#page-2-0) Each core is equipped with a queue node
    that can be linked to any queue. For Colibri, this means that instead of equipping
    each memory controller with a


    <sup>© 2024</sup> IEEE. Personal use of this material is permitted. Permission
    from IEEE must be obtained for all other uses, in any current or future media,
    including reprinting/republishing this material for advertising or promotional
    purposes, creating new collective works, for resale or redistribution to servers
    or lists, or reuse of any copyrighted component of this work in other works.


    hardware queue that can hold an entry for each core, each memory controller is
    extended with a parameterizable number of head and tail registers to form linked
    lists. Each core is then equipped with one hardware queue node, and when issuing
    an LRwait, the core inserts itself in the corresponding queue. We implemented
    Colibri on the open-source, manycore MemPool system, consisting of 256 cores sharing
    1 MiB of L1 memory [\[5\]](#page-5-4). Colibri provides a scalable solution that
    can be easily integrated into existing RISC-V systems. The LRSCwait solution can
    be used as a drop-in replacement for LRSC or as a powerful extension, making it
    a desirable option for highperformance computing systems. We evaluate the performance
    of Colibri against various hardware and software approaches. The results indicate
    that Colibri outperforms other approaches in all experiments, with a throughput
    increase of up to 6.5 times in high-contention situations and a 13% increase in
    low-contention scenarios. Additionally, Colibri reduces polling, allowing other
    applications to be unaffected by concurrent atomic accesses. Our key contributions
    are the following:


    - The LRwait extension consisting of three novel instructions (LRwait, SCwait,
    and Mwait), which enable atomic access and monitoring memory locations with a
    minimal amount of polling [\(Section III\)](#page-1-0).

    - A scalable implementation for LRwait named Colibri leveraging a distributed
    reservation queue [\(Section IV\)](#page-3-0).

    - An implementation and evaluation of Colibri on the MemPool platform that outperforms
    other approaches in throughput, fairness, polling, and energy per atomic access.
    Colibri scales linearly on the MemPool platform by introducing an area overhead
    of just 6% while being 8.8x more energy efficient than locks [\(Section V\)](#page-4-0).


    ## II. RELATED WORK


    A common approach to mitigate polling is using a backoff after a failed atomic
    access [\[2\]](#page-5-1). Existing backoff schemes, such as exponential backoff,
    where each failed attempt increases the backoff time, can reduce the overhead
    on shared resources but still make the cores busy-waiting and performing sub-optimally.


    The Mellor-Crummey, Scott (MCS) lock [\[6\]](#page-5-5) relies on a software queue
    for contending cores to enqueue in and spin on their respective node in the queue.
    This guarantees that each core spins on a unique location to mitigate contention
    on the lock variable itself. This approach works well in cache-based systems since
    each core can spin on its own L0 cache. However, in this work, we focus on systems
    with software-managed memories.


    While software approaches to locks are general and platform agnostic, their performance
    can not keep up with hardware locks. A study of two software locks and four hardware
    locks shows that hardware locks consistently outperform the software approaches
    by 25%-94%. However, hardware locks such as Hardlocks [\[7\]](#page-5-6) do not
    scale well, as the locks are managed by a centralized locking unit accessible
    to all cores. Accessing this unit quickly becomes the bottleneck in large systems.
    Furthermore, the number of locks is fixed at implementation time. Similarly, Glaser
    et al. present a synchronization unit where each core has a private direct connection
    to each hardware lock [\[8\]](#page-5-7). While this solves the contention issue,
    it prevents scaling beyond a few tens of cores. GLock suffers from a similar scalability
    issue [\[9\]](#page-5-8). It is based on a dedicated on-chip network consisting
    of lock managers and local controllers that synchronize to acquire a lock. Monchiero
    et al. propose a synchronizationoperation buffer implemented as a hardware queue
    in the memory controller to resolve the lock accesses [\[10\]](#page-5-9). However,
    this approach only implements locks and has a hardware cost that is proportional
    to the number of cores. Furthermore, each memory controller would require such
    a buffer to manage locks.


    While locks are a common solution for protecting critical sections, their blocking
    nature often limits performance. Lockfree algorithms, on the other hand, allow
    for much more concurrency. They often rely on instructions like CAS or the LRSC
    pair. This section focuses on the latter, specifically, RISC-V''s implementation.
    For example, the ATUN is a unit that can be placed in an Advanced eXtensible Interface
    (AXI) bus to support LRSC instructions to the downstream memory [\[11\]](#page-5-10).
    The table allows a reservation for every core, thus implementing a non-blocking
    version of LRSC. Furthermore, each bank would require its own ATUN adapter in
    a multi-banked system, introducing significant hardware overhead in large manycore
    systems. The Rocket chip features a similar implementation [\[12\]](#page-5-11).
    However, the number of reservations is limited.


    MemPool implements a lightweight version of LRSC by only providing a single reservation
    slot per memory bank [\[5\]](#page-5-4). However, this sacrifices the non-blocking
    property of the LRSC pair. The GRVI multiprocessor, on the other hand, modifies
    the granularity at which LRSCs operate by locking the complete memory bank [\[13\]](#page-5-12).
    This reduces the hardware overhead to one bit per core per bank, albeit the approach
    is still affected by retries due to spuriously failing SC operations.


    All those solutions implement the standard RISC-V LRSC instruction, leveraging
    the freedom of the official specification to achieve different trade-offs. However,
    none of them solve the polling and retry issue of failing SC operations. On the
    contrary, they sometimes worsen it. The Request-Store-Forward (RSF) synchronization
    model proposed by Liu et al. is similar to LRwait [\[14\]](#page-5-13). Synchronization
    requests are stored in a hardwaremanaged memory and handled in order by a synchronization
    controller. However, this approach leads to a high memory footprint, and the hardware
    needs to be replicated for each memory bank. Furthermore, it is infeasible for
    software-managed memories as the synchronization controller will interfere with
    the allocated data when adding the queue to the memory.


    Our LRwait approach and the efficient implementation through Colibri scale well
    to hundreds of cores and banks while completely eliminating polling without sacrificing
    granularity.


    ## III. LRWAIT AND SCWAIT


    <span id="page-1-0"></span>RISC-V defines the load-reserved/store-conditional
    (LRSC) instructions to implement generic, atomic RMW operations. The LR instruction
    reads a value from memory and places a reservation, which remains valid until
    the specified memory address is changed. The core can then modify the value and
    write the result back with an SC instruction. The latter will succeed only if
    the reservation is still valid. If the SC fails,


    the LRSC sequence has to be retried. The linearization point between contending
    cores is thus at the SC.


    LRwait eliminates the wasteful retry loop by moving the linearization point to
    the LRwait instruction, i.e., atomic accesses of competing cores are ordered at
    the LRwait instruction. Instead of immediately returning the value, the memory
    controller withholds the response such that only one core gets a response at a
    time, guaranteeing it to be the only core issuing an SCwait to the same address.
    The LRSCwait and LRSC instructions share similar semantics. The SCwait stores
    a value conditionally and returns a success or failure code analogous to the SC.
    Likewise, the LRwait matches the LR instruction, but its response is delayed.
    The sequence of an atomic RMW operation with LRSCwait is the following:


    - 1) The core issues the LRwait and waits for the response.

    - 2) The memory buffers the request until it is the next outstanding LRSCwait
    pair to that address.

    - 3) Once the LRwait is the next in line, the memory serves the request with the
    current memory value and monitors it. A store to the same address clears the reservation.

    - 4) The core modifies the value and writes it with an SCwait.

    - 5) The memory accepts the value if a valid reservation still exists and issues
    the response.


    While the memory guarantees that only one core proceeds with an LRSCwait pair,
    it cannot eliminate the possibility of another core overwriting the atomic variable,
    leading to a failing SCwait. One constraint of the LRSCwait instruction pair is
    that every LRwait must eventually be followed by an SCwait. While RISC-V does
    not have this constraint for LRSC, our extension requires the matching SCwait
    to yield the queue of outstanding LRwait instructions and allow progress on the
    atomic variable. Albeit LRSCwait can be used as a drop-in replacement for LRSC,
    it removes the lock-free progress guarantee that the LRSC instructions have. Since
    only one core can issue an SCwait, a malicious core could block the resource indefinitely
    and obstruct progress. However, LRSCwait still gives strong progress guarantees
    under the following constraints:


    *a) Mutual exclusion:* Just as the LRSC pair, the SCwait only succeeds if a valid
    reservation is present, meaning there was no write between the LRwait and the
    SCwait, which guarantees mutual exclusion and, therefore, atomicity.


    *b) Deadlock freedom:* To prevent circular dependencies between cores, every core
    must have at most one outstanding LRwait operation. RISC-V does not impose this
    requirement on LRSC. However, only the innermost LRSC pair is guaranteed to progress.
    Therefore, this requirement for deadlock freedom is a requirement for livelock
    freedom already. Furthermore, each core''s LRwait must eventually be followed
    by an SCwait to close the CS. We impose the same constraints as the RISC-V standard
    to allow only a finite and limited set of instructions between LRwait and SCwait.


    *c) Starvation freedom:* Starvation freedom guarantees that all cores eventually
    make progress. LRSC only guarantees that one core makes progress because an unlucky
    core could always lose the SC to a faster core. In our work, this scenario is
    prevented by handling the LRSCwait pairs in order, thus


    ![](_page_2_Figure_11.jpeg)


    <span id="page-2-0"></span>Fig. 1. Difference between LRSC architecture with a
    reservation table, LRSCwait with a reservation queue, and Colibri with a linked-list-like
    structure.


    enabling all cores to eventually execute the LRSCwait pair and, therefore, guaranteeing
    starvation freedom.


    Overall, while the blocking nature of the LRSCwait makes a core''s progress depend
    on other cores correctly executing and leaving the LRSCwait blocks, these constraints
    can easily be adhered to in bare-metal systems, which are fully under the programmer''s
    control. LRSCwait can provide very strong progress guarantees, enabling each core
    to progress. However, hardware failure or software bugs can become blocking.


    ## <span id="page-2-1"></span>*A. Ideal Hardware Implementation*


    A straightforward hardware implementation of LRSCwait requires tracking all outstanding
    reservations in order to ensure fairness and starvation freedom. As shown in [Fig.
    1,](#page-2-0) this can be achieved by an LRSCwait adapter placed in front of
    each memory bank, consisting of (i) a queue-like data structure of capacity n,
    where n is the number of cores in the system, and (ii) some additional logic to
    monitor memory accesses and invalidate reservations when the target address is
    overwritten. The overhead of this implementation in a system with m memory banks
    is O(n log<sup>2</sup> (n)m), where log<sup>2</sup> (n) represents identifier
    size per core. Assuming that m scales linearly with the number of cores, this
    implementation''s overhead scales quadratically with the system size: O(n 2 ),
    a non-negligible hardware complexity.


    ## *B. Optimized Hardware Implementation*


    To reduce the hardware complexity, we can decrease the queue''s capacity by assuming
    that only a subset of cores can access a specific address simultaneously. Our
    implementation supports a parametrizable number of reservation slots q. The case
    with q = n falls back to the ideal LRSCwait pair described in [Section](#page-2-1)
    III-A. We call this implementation *LRSCwaitideal*. If q < n, we trade hardware
    overhead with performance. In these implementations, *LRSCwaitq*, cores executing
    an LRwait to a full queue will fail immediately.


    ## *C. Mwait*


    To allow efficient monitoring and notification of a memory location from a core
    in the system, we introduce *Mwait*. Mwait is derived from LRwait, but without
    a matching SCwait. Instead, the reservation placed by Mwait is used to identify
    the core that needs to be notified of a change. For instance, a core may monitor
    a queue and be woken up when an element is pushed onto the queue. Our experiments
    show that Mwait provides a simple and efficient mechanism for monitoring memory


    locations, allowing cores to be woken up only when necessary. To handle the possibility
    that the change we wish to observe has already occurred, we provide Mwait with
    an expected value. If the memory location already differs from the expected value
    when Mwait is served, the core is immediately notified.


    ## IV. COLIBRI


    <span id="page-3-0"></span>Colibri implements a distributed queue, similar to
    a linked list, shown in [Fig. 1.](#page-2-0) It alleviates the huge hardware overhead
    of the hardware queues at each memory controller, replacing it with a dedicated
    head and tail node per queue and a simple controller. On top of that, each core
    requires its own hardware node, called *queue node (Qnode)*, to enqueue itself.
    Since each core can only be in one queue, one Qnode per core is enough. Therefore,
    Colibri only requires O(n + 2m) nodes and scales linearly with the system size.


    Since the queue is distributed across Qnodes and the head/tail nodes next to the
    memory banks, updating the queue becomes more complex. In comparison to the ideal
    LRwait, an enqueue operation from an LRwait, or a dequeue operation by an SCwait,
    does not happen in one place and a single cycle.


    We present a simple example of the construction and deconstruction of the queue
    in [Fig. 2](#page-3-1) with a single memory and two cores contending for the same
    address. Both cores have their own Qnodes, and the memory has a head and tail
    node. We call the cores *A* and *B* for simplicity.


    *a) LRwait:* Core A issues an LRwait request to the memory (1). Since the queue
    is initially empty, the head and tail nodes are set to A, and a reservation to
    the specified location is set up. The memory then sends the value A (2). During
    or after the described events, B''s LRwait request arrives at the memory (3).
    When the B''s LRwait request arrives at the memory, the controller appends B at
    the tail of the queue and then adds it as the successor to A. This is done by
    sending a so-called *SuccessorUpdate* to A (4). This SuccessorUpdate writes to
    A''s Qnode to make it point to B. In this final state shown in the top half of
    [Fig. 2,](#page-3-1) A and B form a queue with A at the head of the queue. At
    this point, A can issue an SCwait while B is sleeping, waiting for a response.


    *b) SCwait:* Core A finishes its LRSCwait pair by issuing an SCwait with the modified
    value (5). Immediately after an SCwait passes the Qnode, it sends a *WakeUpRequest*
    to the memory containing its successor, i.e., B (6). On arrival of the SCwait
    request at the memory, the head node and reservation


    ![](_page_3_Figure_7.jpeg)


    <span id="page-3-1"></span>Fig. 2. LRwait and SCwait sequence in Colibri with
    two cores and one queue.


    are checked. If everything is valid, the head node is temporarily invalidated
    to prevent a future SCwait from the same core from succeeding without reservation,
    and the SCwait is written to memory. The WakeUpRequest sets the head node to the
    successor node and triggers an LRwait response with the latest memory value written
    by A, i.e., for B (7). Core B is now free to issue an SCwait. Finally, the head
    and tail nodes point to B since B is the only core in the queue.


    This sequence can be generalized to more cores. Qnodes accept SuccessorUpdates
    even when the core is asleep, allowing the queue to be enlarged independent of
    the cores'' state.


    ## *A. Correctness of Colibri*


    *1) LRwait:* When an LRwait enqueues a node, it must update the tail to point
    to the newly enqueued node and append it to the previous tail node if it existed.
    If not, the enqueue operation inherently becomes atomic. Otherwise, to update
    the predecessor, the memory controller sends a SuccessorUpdate to the previous
    tail and overwrites the tail node atomically. Since we can only have one LRwait
    per core and SuccessorUpdates are only sent when overwriting a tail node, only
    a single SuccessorUpdate will ever be in flight to a Qnode, guaranteeing no lost
    links in the queue. If the SuccessorUpdate arrives after the core issued an SCwait,
    it will immediately bounce back as a WakeUpRequest. If the next LRwait arrives
    while the SuccessorUpdate is still in flight, the tail will be updated again,
    and the SuccessorUpdate will be sent to the next core. While a glance at the Qnodes
    might reveal broken links momentarily, the links only have to be made when a core
    issues its SCwait, which requires an LRwait response from the memory controller
    since memory transactions are ordered, this will always happen after the SuccessorUpdate.


    *2) SCwait:* If a core issuing an SCwait is the only one in the queue, i.e., the
    head and tail are equal, dequeuing itself by clearing the head and tail is trivial.
    Otherwise, the SCwait will invalidate the head node while leaving the value unchanged.
    A core would need to overwrite the head node to reach an inconsistent queue from
    this stage. This is only allowed for an LRwait reaching an empty queue or a WakeUpRequest
    arriving at the memory after invalidating the head node by an SCwait. A WakeUpRequest
    can only be triggered by an SCwait passing the Qnode, which can only be sent by
    a core at the head of the queue since the other cores are still waiting for their
    LRwait response. Thus, the WakeUpRequest arriving at the memory node guarantees
    that the queue is in a consistent state again.


    ## *B. Extending Colibri with Mwait*


    A core can issue an Mwait request to enqueue into Colibri''s queue to monitor
    a memory location. The memory controller then waits for a write to the monitored
    location, just like for LRwait''s reservation. After a write, the memory controller
    triggers a response to the Mwait instruction. For Mwait, the head node is sleeping
    as well in contrast to LRSCwait where the head is free to issue an SCwait. The
    Mwait response makes the Qnode dispatch the WakeUpReq for its successor, which
    then bounces to the memory controller, where the next Mwait response is released.
    In contrast to LRSCwait, the whole reservation queue is woken up without any interference
    from the cores.


    <span id="page-4-1"></span>TABLE I AREA OF A M E M P O O L\_T I L E WITH DIFFERENT
    LRSCWAIT DESIGNS.


    | Architecture            | Parameters    | Area[kGE] | Area[%] |

    |-------------------------|---------------|-----------|---------|

    | MemPool tile            | none          | 691       | 100.0   |

    | with LRSCwait1          | 1 queue slot  | 790       | 116.4   |

    | with LRSCwait8          | 8 queue slots | 865       | 127.4   |

    | with Colibri with MWait | 1 address     | 732       | 105.9   |

    | with Colibri with MWait | 2 addresses   | 750       | 108.5   |

    | with Colibri with MWait | 4 addresses   | 761       | 110.1   |

    | with Colibri with MWait | 8 addresses   | 802       | 116.3   |


    ![](_page_4_Figure_2.jpeg)


    <span id="page-4-2"></span>Fig. 3. Throughput of different LRSCwait implementations
    and standard RISC-V atomics at varying contention.


    ## V. RESULTS


    <span id="page-4-0"></span>We implement and evaluate various LRSCwait variations
    and Colibri in MemPool, an open-source, 256-core RISC-V system with 1024 SPM banks
    [\[5\]](#page-5-4). All our results are taken from cycle-accurate register-transfer
    level (RTL) simulation. Physical implementation results come from implementing
    MemPool in GlobalFoundries'' 22FDX fully depleted silicon-on-insulator (FD-SOI)
    technology. Power consumption is evaluated in typical conditions (TT/0.80 V/25
    °C), with switching activities from a post-layout gate-level simulation running
    at 600 MHz.


    The area overhead of different implementations is shown in [Table I.](#page-4-1)
    Even optimized implementations of LRSCwait quickly grow in size, while LRSCwaitideal
    is physically infeasible for a system of MemPool''s scale. Colibri, on the other
    hand, grows linearly and allows up to eight queues per memory controller with
    a similar area overhead to LRSCwait<sup>1</sup> of 16%.


    ## *A. Benchmarking*


    *a) Histogram:* We implement a concurrent histogram benchmark to evaluate Colibri''s
    performance at different levels of contentions. The application atomically increments
    a parametrizable number of bins. The fewer bins, the higher the contention. We
    increment a bin with different atomic operations and compare their performance
    as updates per clock cycle.


    The throughput of different LRSCwait implementations is shown in [Fig. 3.](#page-4-2)
    LRSCwaitideal outperforms all implementations across the whole spectrum of contention.
    The optimized implementations show similar performance at low contention but achieve
    much lower performance when the contention is higher than their number of reservations.
    Finally, Colibri achieves nearideal performance across all contentions. The slight
    performance


    ![](_page_4_Figure_10.jpeg)


    <span id="page-4-3"></span>Fig. 4. Throughput of different lock implementations
    compared to generic RMW atomics at varying contention.


    penalty comes from the extra roundtrips of Colibri''s node update messages. Colibri
    outperforms the LRSC-based implementation by a factor of 6.5× at high contention
    and 13% at low contention. For completeness, we also show the throughput of an
    *Atomic Add* implementation, which is designed specifically to increment a memory
    location atomically and represents the plot''s roofline. However, most concurrent
    algorithms need more complex atomic RMW operations than an increment, where programmers
    have to resort to locks of generic RMW atomics like LRSCwait.


    [Fig. 4](#page-4-3) compares Colibri to various lock-based implementations. Colibri,
    LRSC, and Atomic Add locks are spin locks with a backoff of 128 cycles, while
    Mwait lock implements an MCS lock, where Mwait is used to avoid polling. Colibri
    outperforms all other approaches for any contention. We observe that the LRSC
    and AMO-lock approaches perform worst at high contention due to their heavy polling
    and retry traffic, while waiting-based approaches perform average. At low contention,
    the waiting-based approaches perform worst because of their management overhead,
    while the other atomics tend to Colibri.


    *b) Interference:* We showed that LRSCwait can significantly improve the throughput
    of atomic operations across all levels of contention. On top of this increase
    in performance, eliminating the need to retry failed operations and polling also
    reduces traffic and frees up resources for cores not executing atomics. Cores
    working on computation experience less negative interference from the constant
    polling of atomics. To measure this effect, we partitioned the 256 cores of MemPool
    to either work on a matrix multiplication or to execute atomic operations. We
    measure the execution time of the matrix multiplication compared to an execution
    time without any interference. [Figure 5](#page-5-14) shows the relative performance
    for various types of atomic operations and distributions of working cores. Our
    Colibri implementation has a negligible impact on the worker cores, even at high
    contention and with a poller-to-worker ratio of 252:4. The retries of the LRSC
    operations, on the other hand, significantly impact the workers'' performance,
    despite a backoff of 128 cycles. At the same ratio of poller-to-workers, the LRSC
    implementation slows the workers down to 26%.


    *c) Queue:* To evaluate Colibri on a commonly used concurrent algorithm, we implement
    an MCS queue with LRSC and LRSCwait, as well as a lock-based queue using atomic
    adds. Concurrent queues are widely used for task scheduling


    ![](_page_5_Figure_0.jpeg)


    <span id="page-5-14"></span>Fig. 5. Matrix multiplication performance with interference
    from atomics. The poller-to-worker ratio is annotated in the figure with poller:worker.


    ![](_page_5_Figure_2.jpeg)


    <span id="page-5-15"></span>Fig. 6. Queue operations throughput with different
    atomics.


    or producer/consumer pipelines. [Figure 6](#page-5-15) shows the number of queue
    operations for a range of cores accessing a single queue. Colibri performs best
    and can sustain a high performance even at 256 cores. It outperforms the LRSC
    and lock-based approaches by 1.54× and 1.48× times with eight cores before both
    implementations drop in performance due to excessive retries and polling. At 64
    cores, Colibri is 9× faster. The shaded areas show each implementation''s slowest
    and fastest core performance range. It illustrates how Colibri results in a very
    balanced and fair workload distribution, while LRSC can have very big variations.


    *d) Energy efficiency:* [Table II](#page-5-16) shows the energy per operation
    for atomic accesses to the histogram at the highest contention. Comparing Colibri
    to the Atomic Add, which represents an ideal atomic update, we can see how energyefficient
    Colibri is for a generic RMW operation that consists of an LRwait, add, and SCwait
    operation. Compared to the LRSC or lock-based implementation, we observe the large
    benefit of the reduction in polling and retry traffic for improving energy efficiency
    by a factor of 7.1× and 8.8×.


    ## VI. CONCLUSION


    In this work, we propose the LRwait and Mwait synchronization primitives and their
    implementation, Colibri, which demonstrate a novel and effective solution for
    the LRSC synchronization problem in cache-less manycore systems. Colibri offers
    superior performance and scalability compared to existing hardware and software
    approaches, reduces polling, and improves throughput in a fair manner. Our experiments
    show that Colibri outperforms other implementations in both high and low contention
    scenarios by up to 6.5× and improved


    TABLE II AREA RESULTS FOR A M E M P O O L\_T I L E FOR IDEAL LRWAIT.


    <span id="page-5-16"></span>


    | Atomic access   | Backoff | Power (mW) | Energy (pJ/OP) | ∆     |

    |-----------------|---------|------------|----------------|-------|

    | Atomic Add      | 0       | 175        | 29             | −77%  |

    | Colibri         | 0       | 169        | 124            | ±0%   |

    | LRSC            | 128     | 186        | 884            | +613% |

    | Atomic Add lock | 128     | 188        | 1092           | +780% |


    energy efficiency by up to 8.8×. The polling and retries of LRSC-based solutions
    can lead to performance degradation of unrelated workers by up to 4×, while Colibri
    can operate even at high contention without impacting other cores. Additionally,
    Colibri can be easily integrated into existing RISC-V systems with a small hardware
    overhead and can be used as a drop-in replacement for LRSC or as an extension.


    ## ACKNOWLEDGMENT


    This work is funded in part by the COREnext project supported by the EU Horizon
    Europe research and innovation programme under grant agreement No. 101092598.


    ## REFERENCES


    - <span id="page-5-0"></span>[1] T. E. Anderson, "The performance of spin lock
    alternatives for sharedmemory multiprocessors," *IEEE Trans. Parallel Distrib.
    Syst.*, vol. 1, no. 1, pp. 6–16, 1990.

    - <span id="page-5-1"></span>[2] M. Herlihy, N. Shavit, V. Luchangco, and M. Spear,
    *The Art of Multiprocessor Programming*, 2nd ed., S. R. Merken, Ed. Cambridge,
    MA, USA: Morgan Kaufmann Publishers Inc., 2020.

    - <span id="page-5-2"></span>[3] GreenWaves Technologies SAS, "GAP9 next generation
    processor for hearables and smart sensors," GreenWaves Technologies SAS, Tech.
    Rep., 2021. [Online]. Available: [https://greenwaves-technologies](https://greenwaves-technologies.com/wp-content/uploads/2022/06/Product-Brief-GAP9-Sensors-General-V1_14.pdf).com/
    [wp-content/uploads/2022/06/Product-Brief-GAP9-Sensors-General-V1](https://greenwaves-technologies.com/wp-content/uploads/2022/06/Product-Brief-GAP9-Sensors-General-V1_14.pdf)
    14.[pdf](https://greenwaves-technologies.com/wp-content/uploads/2022/06/Product-Brief-GAP9-Sensors-General-V1_14.pdf)

    - <span id="page-5-3"></span>[4] R. Ginosar, P. Aviely, T. Israeli, and H. Meirov,
    "RC64: High performance rad-hard manycore," in *IEEE Aerosp. Conf. Proc.* IEEE,
    Jun. 2016, pp. 2074–2082.

    - <span id="page-5-4"></span>[5] S. Riedel, M. Cavalcante, R. Andri, and L. Benini,
    "MemPool: A scalable manycore architecture with a low-latency shared L1 memory,"
    *IEEE Trans. Comput.*, vol. 72, no. 12, pp. 3561–3575, 2023.

    - <span id="page-5-5"></span>[6] J. M. Mellor-Crummey and M. L. Scott, "Algorithms
    for scalable synchronization on shared-memory multiprocessors," *ACM Trans. Comput.
    Syst.*, vol. 9, no. 1, pp. 21–65, Feb. 1991.

    - <span id="page-5-6"></span>[7] T. B. Strøm, J. Sparsø, and M. Schoeberl, "Hardlock:
    Real-time multicore locking," *J. Syst. Archit.*, vol. 97, pp. 467–476, 2019.

    - <span id="page-5-7"></span>[8] F. Glaser, G. Tagliavini, D. Rossi, G. Haugou,
    Q. Huang, and L. Benini, "Energy-efficient hardware-accelerated synchronization
    for shared-L1 memory multiprocessor clusters," *IEEE Trans. Parallel Distrib.
    Syst.*, vol. 32, no. 3, pp. 633–648, Mar. 2021.

    - <span id="page-5-8"></span>[9] J. L. Abellan, J. Fern ´ andez, and M. E. Acacio,
    "Design of an efficient ´ communication infrastructure for highly contended locks
    in many-core cmps," *J. Parallel Distrib. Comput.*, vol. 73, no. 7, pp. 972–985,
    2013.

    - <span id="page-5-9"></span>[10] M. Monchiero, G. Palermo, C. Silvano, and O.
    Villa, "An efficient synchronization technique for multiprocessor systems on-chip,"
    *ACM SIGARCH Comput. Archit. News*, vol. 34, no. 1, pp. 33–40, Mar. 2006.

    - <span id="page-5-10"></span>[11] A. Kurth, S. Riedel, F. Zaruba, T. Hoefler,
    and L. Benini, "ATUNs: Modular and scalable support for atomic operations in a
    shared memory multiprocessor," in *ACM/IEEE Des. Autom. Conf.*, vol. 57. San Francisco,
    CA, USA: IEEE, Jul. 2020, pp. 902–907.

    - <span id="page-5-11"></span>[12] K. Asanovic´ *et al.*, "The rocket chip generator,"
    EECS Department, University of California, Berkeley, Tech. Rep. UCB/EECS-2016-17,
    Apr. 2016. [Online]. Available: http://www2.eecs.berkeley.[edu/Pubs/TechRpts/](http://www2.eecs.berkeley.edu/Pubs/TechRpts/2016/EECS-2016-17.html)
    [2016/EECS-2016-17](http://www2.eecs.berkeley.edu/Pubs/TechRpts/2016/EECS-2016-17.html).html

    - <span id="page-5-12"></span>[13] J. Gray, "Implementation of LR/SC on the GRVI
    multiprocessor," 2016. [Online]. Available: [https://groups](https://groups.google.com/a/groups.riscv.org/g/hw-dev/c/Mt9Q94f_l2w?pli=1).google.com/a/groups.riscv.org/g/hw[dev/c/Mt9Q94f](https://groups.google.com/a/groups.riscv.org/g/hw-dev/c/Mt9Q94f_l2w?pli=1)
    l2w?pli=1

    - <span id="page-5-13"></span>[14] S. Liu and J. L. Gaudiot, "Synchronization
    mechanisms on modern multicore architectures," in *Proc. 12th Asia-Pacific Conf.
    Adv. Comput. Syst. Archit.* Seoul, Korea: Springer Verlag, 2007, pp. 290–303.'
- title: 'AI for Education (AI4EDU): Advancing Personalized Education with LLM and
    Adaptive Learning'
  abstract: Recent advanced AI technologies, especially large language models (LLMs)
    like GPTs, have significantly advanced the field of data mining and led to the
    development of various LLM-based applications. AI for education (AI4EDU) is a
    vibrant multi-disciplinary field of data mining, machine learning, and education,
    with increasing importance and extraordinary potential. In this field, LLM and
    adaptive learning-based models can be utilized as interfaces in human-in-the-loop
    education systems, where the model serves as a mediator among the teacher, students,
    and machine capabilities, including its own. This perspective has several benefits,
    including the ability to personalize interactions, allow unprecedented flexibility
    and adaptivity for human-AI collaboration and improve the user experience. However,
    several challenges still exist, including the need for more robust and efficient
    algorithms, designing effective user interfaces, and ensuring ethical considerations
    are addressed. This workshop aims to bring together researchers and practitioners
    from academia and industry to explore cutting-edge AI technologies for personalized
    education, especially the potential of LLMs and adaptive learning technologies.
  keywords: 'Education, Edtech, Adaptive Learning, LLM KDD ''24, August 25–29, 2024,
    Barcelona, Spain © 2024 Copyright held by the owner/author(s). ACM ISBN 979-8-4007-0490-1/24/08.
    <https://doi.org/10.1145/3637528.3671498> ACM Reference Format: Qingsong Wen,
    Jing Liang, Carles Sierra, Rose Luckin, Richard Tong, Zitao Liu, Peng Cui, and
    Jiliang Tang. 2024. AI for Education (AI4EDU): Advancing Personalized Education
    with LLM and Adaptive Learning. In Proceedings of the 30th ACM SIGKDD Conference
    on Knowledge Discovery and Data Mining (KDD ''24), August 25–29, 2024, Barcelona,
    Spain. ACM, New York, NY, USA, [2](#page-1-0) pages.<https://doi.org/10.1145/3637528.3671498>'
  document: '![](_page_0_Picture_0.jpeg)


    # AI for Education (AI4EDU): Advancing Personalized Education with LLM and Adaptive
    Learning


    Qingsong Wen Squirrel Ai Learning Bellevue, USA qingsongedu@gmail.com


    Rose Luckin University College London London, UK r.luckin@ucl.ac.uk


    Jing Liang Squirrel Ai Learning Shanghai, China joleenliang@squirrelai.com


    Richard Tong Squirrel Ai Learning Bellevue, USA richard.tong@ieee.org


    Peng Cui Tsinghua University Beijing, China cuip@tsinghua.edu.cn


    Carles Sierra IIIA of the Spanish National Research Council Barcelona, Spain sierra@iiia.csic.es


    > Zitao Liu Jinan University Guangzhou, China liuzitao@jnu.edu.cn


    Jiliang Tang Michigan State University East Lansing, USA tangjili@msu.edu


    #### ABSTRACT


    Recent advanced AI technologies, especially large language models (LLMs) like
    GPTs, have significantly advanced the field of data mining and led to the development
    of various LLM-based applications. AI for education (AI4EDU) is a vibrant multi-disciplinary
    field of data mining, machine learning, and education, with increasing importance
    and extraordinary potential. In this field, LLM and adaptive learning-based models
    can be utilized as interfaces in human-in-the-loop education systems, where the
    model serves as a mediator among the teacher, students, and machine capabilities,
    including its own. This perspective has several benefits, including the ability
    to personalize interactions, allow unprecedented flexibility and adaptivity for
    human-AI collaboration and improve the user experience. However, several challenges
    still exist, including the need for more robust and efficient algorithms, designing
    effective user interfaces, and ensuring ethical considerations are addressed.
    This workshop aims to bring together researchers and practitioners from academia
    and industry to explore cutting-edge AI technologies for personalized education,
    especially the potential of LLMs and adaptive learning technologies.


    ### CCS CONCEPTS


    • Applied computing → Education; • Human-centered computing→Human computer interaction
    (HCI);• Computing methodologies → Machine learning.


    # KEYWORDS


    Education, Edtech, Adaptive Learning, LLM


    KDD ''24, August 25–29, 2024, Barcelona, Spain


    © 2024 Copyright held by the owner/author(s).


    ACM ISBN 979-8-4007-0490-1/24/08. <https://doi.org/10.1145/3637528.3671498> ACM
    Reference Format: Qingsong Wen, Jing Liang, Carles Sierra, Rose Luckin, Richard
    Tong, Zitao Liu, Peng Cui, and Jiliang Tang. 2024. AI for Education (AI4EDU):
    Advancing Personalized Education with LLM and Adaptive Learning. In Proceedings
    of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD
    ''24), August 25–29, 2024, Barcelona, Spain. ACM, New York, NY, USA, [2](#page-1-0)
    pages.<https://doi.org/10.1145/3637528.3671498>


    #### 1 INTRODUCTION


    Following the success of the previous multiple AI4EDU workshops, which were co-organized
    by our co-organizers, we are looking forward to hosting the new AI4EDU workshop
    in KDD 2024. Different from those previous workshops, the AI4EDU workshop at KDD
    2024 will (1) provide a platform for both academia and industrial researchers
    from different fields, e.g., data mining, machine learning, artificial intelligence,
    education, etc, to exchange ideas and promote collaborations, and (2) focus on
    new emerging LLMs and adaptive learning in education. Specifically, this workshop
    aims to explore cutting-edge LLMs and adaptive learning technologies for personalized
    education. The objectives of the workshop are to: 1) Review the current state-of-the-art
    in LLM-based systems and their applications in education. 2) Discuss the state-of-the-art
    technologies of adaptive learning and mining that tailor education to the individual
    needs, learning styles, proficiency levels, and problem areas of each student,
    for personalized learning experience. 3) Identify challenges and opportunities
    in using LLMs as both communication and collaboration interfaces in adaptive learning
    systems, educational games and intelligent educational assistants. 4) Explore
    ethical considerations and standardization issues in the use of LLMs. 5) Introduce
    and design new approaches such as prompt engineering, local fine tuning, integrated
    reasoning, and delegation framework for dialog-based systems that not only generate
    content but also shape the behavior of the system.


    Permission to make digital or hard copies of part or all of this work for personal
    or classroom use is granted without fee provided that copies are not made or distributed
    for profit or commercial advantage and that copies bear this notice and the full
    citation on the first page. Copyrights for third-party components of this work
    must be honored. For all other uses, contact the owner/author(s).


    <span id="page-1-0"></span>KDD ''24, August 25–29, 2024, Barcelona, Spain Qingsong
    Wen, et al.


    ## 2 TOPICS OF INTEREST


    This workshop encourages submissions of innovative solutions for a broad range
    of AI for Education problems. Topics of interest include but are not limited to
    the following:


    - Mining multimodal data for comprehensive learning analytics in LLM-aided education.

    - Challenges and opportunities in integrating LLMs with existing adaptive learning
    systems.

    - Adaptive learning and mining systems and their applications in educational settings.

    - Predictive modeling in education using LLMs for student success and retention.

    - The potential of LLMs in education from both the theoretical and practical angles.

    - Ethical considerations in the use of LLMs as interfaces in educational settings,
    especially with AI standards committee.

    - Frameworks for standardization and benchmarking of LLMs in educational technology.

    - Data-driven approaches to curriculum development using LLM insights.

    - Data-driven approaches to curriculum development using LLM insights.

    - LLMs role in automated assessment and real-time feedback for students.

    - The future of LLMs in education: trends, potentials, and unforeseen consequences.


    # 3 PAPER SUBMISSION AND REVIEW PROCESS


    We invite high-quality paper submissions of theoretical and experimental nature
    on the broad AI4EDU topics. The workshop solicits 4-7 pages double-blind paper
    submissions from participants. Submissions of the following flavors will be sought:
    (1) research ideas, (2) case studies (or deployed projects), (3) review papers,
    (4) best practice papers, and (5) lessons learned. All submissions will be peer-reviewed.
    Some will be selected for spotlight talks, and some for the poster session.


    Each submitted paper will be evaluated by at least two reviewers, using the same
    criteria as SIGKDD Research Track papers: novelty, technical quality, potential
    impact, and clarity. As with Research Track papers, we will consider sufficiently
    innovative application-oriented papers, in addition to theoretical and methodological
    contributions. The program committee members and other researchers recommended
    by the program committee members will review the papers. Additional reviews may
    be solicited if the two reviews are not conclusive. Only papers that are deemed
    to be satisfactorily novel and technically sound by the program committee will
    be accepted.


    ### 4 WORKSHOP FORMAT


    The workshop will be held in person and will include interactive sessions and
    online resources to support ongoing collaboration. The workshop will consist of
    paper presentations, panel discussions, and group activities to facilitate the
    exchange of ideas and promote collaborative learning as follows:


    • Paper Presentations: 20 minutes for each paper.


    - Invited Talks: We intend to have 5-7 speakers to present their expert insights.

    - Panel Discussions: Panel discussion sessions are intended to explore various
    heated topics related to the implementation of AI in education, ongoing initiatives,
    and projects.

    - Poster Sessions: Optional for papers that could not fit into the main session
    - Allow breakout rooms.


    # 5 INTENDED AUDIENCE


    The workshop is intended for researchers and practitioners in the areas of data
    mining, NLP, HCI, machine learning, educational technology, and other related
    fields. Participants will benefit from the opportunity to learn about the latest
    research and developments in the use of LLMs and adaptive learning systems, as
    well as engage in a collaborative design and prototyping activity.


    # 6 EXPECTED OUTCOMES


    The expected outcomes of the workshop include a better understanding of the potential
    of LLMs and adaptive learning for personalized education systems, a review of
    the current state-of-the-art, identification of challenges and opportunities,
    and recommendations for future research and practice. Additionally, we expect
    to identify potential collaborations and partnerships that can lead to the development
    of more robust LLMs and adaptive learning systems, as well as prototype designs
    for new LLM-based systems in educational settings.


    # 7 WORKSHOP ORGANIZERS


    - Qingsong Wen, Head of AI Research & Chief Scientist at Squirrel Ai Learning.

    - Jing Liang, Co-Founder of Squirrel Ai Learning.

    - Carles Sierra, Professor and Director of the Artificial Intelligence Research
    Institute (IIIA) of the Spanish National Research Council (CSIC).

    - Rose Luckin, Professor of Learner Centred Design at the UCL Knowledge Lab.

    - Richard Tong, Chief Architect of Squirrel Ai Learning.

    - Zitao Liu, Dean of Guangdong Institute of Smart Education, Jinan University.

    - Peng Cui, Associate Professor with tenure in Tsinghua University.

    - Jiliang Tang, University Foundation Professor in the computer science and engineering
    department at Michigan State University.'
- title: Dimensions for Designing LLM-based Writing Support
  abstract: ''
  keywords: ''
  document: '# Dimensions for Designing LLM-based Writing Support


    Frederic Gmeiner HCI Institute Carnegie Mellon University Pittsburgh, PA, USA
    gmeiner@cmu.edu


    ### 1 INTRODUCTION


    Advances in large language models (LLMs) have enabled a myriad of unprecedented
    capabilities over the past few years: computers can write code, translate between
    languages, and generate conversations. Among the potential use cases, writing
    has been a domain that continually fascinated researchers. Nowadays, as LLMs move
    from research labs to the real world, there is a growing interest in exploring
    the capabilities of these systems to support writing tasks across domains such
    as fiction writing [\[3,](#page-1-0) [11\]](#page-1-1), scientific writing [\[7\]](#page-1-2),
    poetry [\[4\]](#page-1-3), and theatre scripts and screenplays [\[8\]](#page-1-4).


    As HCI researchers and designers, we have been "playing with" GPT to understand
    its capabilities as a design material to support writing. A common pitfall when
    designing AI experiences is the tendency to envision "holy grail use cases": places
    that require nearperfect AI performance for delivering high-quality outputs that
    match human intelligence or creativity [\[5,](#page-1-5) [10\]](#page-1-6). For
    instance, our initial experiments mostly focused on prompting the model to come
    up with novel content: writing stories, podcast scripts, characters, and plots.
    Similar to others [\[8,](#page-1-4) [11\]](#page-1-1), we were soon disappointed
    by how generic and bland the outputs were. However, over time we learned to lower
    our expectations and focus on low hanging fruit: less complex writing tasks where
    average quality LLM output could be useful. These included recommending word associations,
    listing things (e.g., places, names, objects) as inspiration, or simply reformatting
    writing (e.g., creating an outline based on prose).


    In our experience, there are three key considerations when designing LLM experiences
    for writing support: LLM capabilities, task complexity and output quality. We
    propose that these dimensions are likely to complement common taxonomy dimensions
    coming from human-centered and socio-technical perspectives, such as feasibility,
    value co-creation, usability, etc. [\[2\]](#page-1-7). In this position paper,
    we argue that a taxonomy of writing assistants capturing these dimensions holistically
    could scaffold the process of designing experiences that writers find valuable.
    The remainder of this paper details each dimension and how these could inform
    the exploration of LLM''s design space.


    ## 2 LLM CAPABILITIES AS DESIGN MATERIAL


    Gaining an understanding of AI''s capabilities and limitations is a major challenge
    for designers and end users who do not have a technical background [\[10\]](#page-1-6).
    One of our goals throughout our experiments with chatGPT was to gain a better
    understanding of what it can do, and what it can do reasonably well. At a high
    level, LLMs are "trained to predict the most likely next word given a textual
    prompt" [\[1\]](#page-1-8). However, this high-level description does not capture
    the wide range of things LLMs can do.


    We found that explicating distinct LLM capabilities was a good starting place
    to understand LLMs as a design material. We first


    Nur Yildirim HCI Institute Carnegie Mellon University Pittsburgh, PA, USA yildirim@cmu.edu


    <span id="page-0-0"></span>Table 1: Non-exhaustive list of LLM capabilities and
    example writing tasks where they might be useful.


    | LLM Capabilities     | Writing Tasks                            |

    |----------------------|------------------------------------------|

    | Text summarization   | Reviewing, Reflection                    |

    | Paraphrasing         | Refining, reviewing                      |

    | Elaboration          | Detailing, scene setting                 |

    | Dialog generation    | Writing scripts, screenplay              |

    | Story seeding        | Unblocking                               |

    | Sentence completion  | Detailing plots, dialog, etc.            |

    | Rewriting in a tone  | Reviewing, characters'' speech            |

    | Rewriting in a style | Reviewing, conveying setting, time, mood |

    | Listing              | Detailing places, characters, etc.       |

    | Formatting           | Prose to outline                         |

    | Keyword association  | Inspiration, ideation                    |


    reviewed prior literature and looked for emerging UX patterns for prompting [\[3,](#page-1-0)
    [11\]](#page-1-1). We then curated a subset of example prompts demonstrating LLM
    capabilities, such as text summarization, paraphrasing, dialog generation, elaboration,
    story seeding, sentence completion, rewriting in a tone, rewriting in a style,
    etc. (Table [1\)](#page-0-0). As we played around with each capability, we tried
    to assess the quality of outputs – however, this was dependent on the use case
    context. For example, we prompted chatGPT to "write a rap battle between Harry
    Potter and Lord Voldemort" or "write the lyrics of a song where Rousseau and Voltaire
    argue on the nature of mankind". While these prompts resulted in reasonably well
    outputs (i.e., coherence and rhyme), we did not find them particularly useful.
    We wondered whether we could produce a podcast script instead, which could be
    immediately useful. However, this seemed to be a complex writing task. Our trials
    resulted in generic scripts that professional podcasters would not find useful.
    In search of a target user group, we thought the scripts could provide value for
    content creators on Youtube who review products. A draft script for product review
    based on a few bullet points might be better than having no script.


    Through these experiments, we became aware of the interplay between the task complexity
    and output quality for a given LLM capability. In the next section, we detail
    how these dimensions are likely to impact the end user experience for writing
    support.


    ### 3 TASK COMPLEXITY AND OUTPUT QUALITY


    Writing tasks vary vastly: some are trivial, some are more challenging. Some tasks
    are tedious, while others have a great impact on writers'' enjoyment and ownership
    [\[11\]](#page-1-1). We asked two questions as we tried to gain a better sense
    of writing tasks:


    - (1) How complex is this writing task for a user to do? Does it require a high
    level of expertise or a deep contextual understanding?

    - (2) What is the level of quality for LLM outputs to be perceived as useful?
    Does the task require particular qualities to be acceptable? (e.g., factuality,
    consistency, etc.)


    <span id="page-1-10"></span>Figure 1: Task Complexity-Output Quality matrix for
    LLM capabilities. Our exploration revealed that writing tasks where average quality
    outputs are acceptable, provide a rich design space for LLM experiences.


    ![](_page_1_Figure_3.jpeg)


    Average *AccepWable OXWpXW QXaliW\* E[cellent


    We started to map the writing tasks we have been thinking about based on the task
    complexity and required output quality. Prior work delineated writing tasks based
    on the writing goals for specific writing processes (i.e., planning, translation,
    reviewing) and noted that each part of the writing process might impose different
    levels of constraints [\[6\]](#page-1-9). In our case, we focused on the quality
    of output as a key dimension that makes or breaks the usefulness of LLMs for writing
    support.


    We realized that our initial explorations mostly focused on complex writing tasks
    that required high quality outputs: things such as generating original story plots,
    podcast content, characters, or styles. These tasks were part and parcel of the
    writing process where writer expertise and involvement were high (Figure [1\)](#page-1-10).
    On the other hand, we found many writing tasks that were relatively less complex.
    Things such as asking for inspirational keywords, word or sentence completion,
    listing names or places, and rewriting sentences to be longer or more concise
    – these were places where vague, unrelated, or even inconsistent outputs could
    be useful. For example, we prompted chatGPT to play a word association game that
    could help us describe a mood and feel in a story. We found the listed words useful
    even though we did not necessarily use them, as they sensitized us to better think
    and reflect on our story.


    We view the task complexity-output quality mapping as a valuable perspective to
    navigate LLM''s design space. Can researchers, designers, and technologists think
    of low complexity writing tasks where average quality LLM outputs could be useful?
    What are the ways LLMs can support high complexity writing tasks without


    providing original, factual, or consistent outputs? Similar to others [\[1\]](#page-1-8),
    we suspect that more contained and tedious writing tasks lend themselves better
    for LLM support and AI-assisted writing in general.


    From a machine learning perspective, the classification of LLM capabilities within
    a complexity-output quality matrix could also serve as guidance for model developers
    for improving the support of specific writing tasks – for example, when part of
    a reinforcement learning from human feedback approach. Furthermore, for developing
    writing assistive tools that can adapt to users'' expertise and context, a complexity-output
    quality matrix might help determine which capabilities best support different
    users'' needs and expectations.


    ### 4 PROMPTS FOR WORKSHOP DISCUSSION


    Below, we highlight two open questions as starting points for workshop discussion:


    - What dimensions should a taxonomy of writing assistants capture? Our exploration
    focused on dimensions that are critical for finding use cases for LLM-based writing
    support. However, this is a partial perspective; there are many critical dimensions
    for designing LLM experiences, including factuality, bias, stereotyping, and homogenization.
    Recent work that proposed a taxonomy of LLM risks [\[9\]](#page-1-11) could provide
    a starting point for further dimensions to consider.

    - How to assess the success of LLM-based writing support? Evaluating writing assistants
    is an open research area. The workshop could expand on dimensions that could be
    used to assess LLM experiences.


    ### REFERENCES


    - <span id="page-1-8"></span>[1] 2022. Wordcraft Writers Workshop.<https://g.co/research/wordcraft>

    - <span id="page-1-7"></span>[2] Robert P Bostrom and J Stephen Heinen. 1977.
    MIS problems and failures: A socio-technical perspective. Part I: The causes.
    MIS quarterly (1977), 17–32.

    - <span id="page-1-0"></span>[3] Alex Calderwood, Vivian Qiu, Katy Ilonka Gero,
    and Lydia B Chilton. 2020. How Novelists Use Generative Language Models: An Exploratory
    User Study.. In HAI-GEN+ user2agent@ IUI.

    - <span id="page-1-3"></span>[4] Tuhin Chakrabarty, Vishakh Padmakumar, and He
    He. 2022. Help me write a poem: Instruction Tuning as a Vehicle for Collaborative
    Poetry Writing. arXiv preprint arXiv:2210.13669 (2022).

    - <span id="page-1-5"></span>[5] Graham Dove, Kim Halskov, Jodi Forlizzi, and
    John Zimmerman. 2017. UX design innovation: Challenges for working with machine
    learning as a design material. In Proceedings of the 2017 chi conference on human
    factors in computing systems. 278–288.

    - <span id="page-1-9"></span>[6] Katy Gero, Alex Calderwood, Charlotte Li, and
    Lydia Chilton. 2022. A design space for writing support tools using a cognitive
    process model of writing. In Proceedings of the First Workshop on Intelligent
    and Interactive Writing Assistants (In2Writing 2022). 11–24.

    - <span id="page-1-2"></span>[7] Katy Ilonka Gero, Vivian Liu, and Lydia Chilton.
    2022. Sparks: Inspiration for science writing using language models. In Designing
    Interactive Systems Conference. 1002–1019.

    - <span id="page-1-4"></span>[8] Piotr Mirowski, Kory W Mathewson, Jaylen Pittman,
    and Richard Evans. [n. d.]. Co-writing screenplays and theatre scripts alongside
    language models using Dramatron. ([n. d.]).

    - <span id="page-1-11"></span>[9] Laura Weidinger, Jonathan Uesato, Maribeth Rauh,
    Conor Griffin, Po-Sen Huang, John Mellor, Amelia Glaese, Myra Cheng, Borja Balle,
    Atoosa Kasirzadeh, et al. 2022. Taxonomy of risks posed by language models. In
    2022 ACM Conference on Fairness, Accountability, and Transparency. 214–229.

    - <span id="page-1-6"></span>[10] Qian Yang, Aaron Steinfeld, Carolyn Rosé, and
    John Zimmerman. 2020. Reexamining whether, why, and how human-ai interaction is
    uniquely difficult to design. In Proceedings of the 2020 chi conference on human
    factors in computing systems. 1–13.

    - <span id="page-1-1"></span>[11] Ann Yuan, Andy Coenen, Emily Reif, and Daphne
    Ippolito. 2022. Wordcraft: story writing with large language models. In 27th International
    Conference on Intelligent User Interfaces. 841–852.'
- title: 'KG4Diagnosis: A Hierarchical Multi-Agent LLM Framework with Knowledge Graph
    Enhancement for Medical Diagnosis'
  abstract: 'Integrating Large Language Models (LLMs) in healthcare diagnosis demands
    systematic frameworks that can handle complex medical scenarios while maintaining
    specialized expertise. We present KG4Diagnosis, a novel hierarchical multi-agent
    framework that combines LLMs with automated knowledge graph construction, encompassing
    362 common diseases across medical specialties. Our framework mirrors real-world
    medical systems through a two-tier architecture: a general practitioner (GP) agent
    for initial assessment and triage, coordinating with specialized agents for in-depth
    diagnosis in specific domains. The core innovation lies in our end-to-end knowledge
    graph generation methodology, incorporating: (1) semantic-driven entity and relation
    extraction optimized for medical terminology, (2) multi-dimensional decision relationship
    reconstruction from unstructured medical texts, and (3) human-guided reasoning
    for knowledge expansion. KG4Diagnosis serves as an extensible foundation for specialized
    medical diagnosis systems, with capabilities to incorporate new diseases and medical
    knowledge. The framework''s modular design enables seamless integration of domain-specific
    enhancements, making it valuable for developing targeted medical diagnosis systems.
    We provide architectural guidelines and protocols to facilitate adoption across
    medical contexts.'
  keywords: ''
  document: '## Introduction


    Knowledge graphs (KGs) have emerged as transformative tools across numerous domains,
    showcasing their ability to organize complex datasets and support advanced reasoning
    and decision-making. In finance, KGs play a pivotal role in risk assessment and
    fraud detection by linking disparate financial datasets to uncover hidden patterns
    and relationships. For example, the application of KGs in detecting fraudulent
    related party transactions enables financial institutions to model complex interdependencies
    between entities, improving accuracy in identifying fraudulent activities (Zhang,
    Li, and Wang 2023). Similarly, in education, KGs enhance personalized learning
    by structuring knowledge from vast academic resources to recommend tailored learning
    paths. A notable implementation includes the use of KGs to integrate data from
    curriculum design, student assessments, and teaching resources, creating adaptive
    systems that improve student engagement and outcomes. In manufacturing, knowledge
    graphs (KGs) enable automation and optimization of processes by integrating heterogeneous
    data sources. A recent study highlighted their role in Reconfigurable Manufacturing
    Systems (RMS), where semantic models and KGs support automated asset capability
    matching and reconfiguration solutions. This approach demonstrated significant
    improvements in efficiency, cost reduction, and productivity by leveraging structured
    knowledge for dynamic decision-making in manufacturing systems (Mo et al. 2024).
    Du et al. constructed highly efficient manufacturing knowledge graphs using multi-feature
    fusion technology, which has been successfully used in automobile manufacturing
    (Du et al. 2022).


    In the medical domain, KGs (Abdulla, Mukherjee, and Ranganathan 2023; Alam, Giglou,
    and Malik 2023; Wu et al. 2024) serve as crucial infrastructure for organizing
    diverse healthcare data and supporting clinical decisionmaking. However, constructing
    and reasoning over medical KGs (Abdulla, Mukherjee, and Ranganathan 2023; Al Khatib
    et al. 2024), particularly from unstructured and multimodal data, presents significant
    challenges that existing approaches have not fully addressed.


    Current methods for medical KG construction span traditional rule-based systems
    to advanced AI models. Rulebased and ontology-driven approaches using SNOMED-CT
    (Chang and Mostafa 2021) and UMLS (Amos et al. 2020) offer reliability but lack
    scalability and struggle with unstructured data. While Large Language Models (LLMs)
    like GPT (OpenAI 2022, 2023; Touvron et al. 2023; Garc´ıa-Ferrero et al. 2024)
    and MedPaLM (Qian et al. 2024) show promise in generating structured knowledge
    from unstructured data, they face challenges with hallucination and accuracy (Huang
    et al. 2023; Tonmoy et al. 2024; Guo et al. 2024). Hybrid approaches incorporating
    Graph Neural Networks (GNNs) attempt to balance symbolic reasoning with deep learning
    but remain computationally complex and dependent on well-structured inputs (Zhang
    2021; Zhang et al. 2024; Shuifa et al. 2023).


    For diagnosis and treatment, medical KGs provide a critical foundation for identifying
    patterns and relationships within patient data, medical literature, and clinical
    guidelines (Li et al. 2020). In diagnosis, KGs help map symp-


    <sup>\*</sup>Corresponding author.


    Copyright © 2025, Association for the Advancement of Artificial Intelligence (www.aaai.org).
    All rights reserved.


    toms to potential conditions, identify relevant tests, and prioritize differential
    diagnoses (Tang et al. 2023). In treatment, KGs assist in recommending personalized
    treatment plans based on patient-specific factors such as comorbidities, drug
    interactions, and genetic markers (Bonner et al. 2022). These processes enhance
    clinical decision-making by offering structured, evidence-based recommendations.


    To address limitations in current methods and enhance the overall clinical workflow,
    we propose KG4Diagnosis, a novel end-to-end framework for the construction, diagnosis,
    treatment and reasoning of automated medical knowledge graphs. Our framework uniquely
    integrates a hierarchical multi-agent architecture, mirroring real-world medical
    systems: a general practitioner (GP) agent conducts the initial assessment and
    triage before coordinating with specialized agents for domain-specific analysis.
    This approach combines the broad capabilities of LLMs with the precision of specialized
    medical knowledge, ensuring accurate diagnosis, personalized treatment suggestions,
    and enhanced clinical decision-making.


    The framework innovatively incorporates advanced techniques for semantic entity
    extraction, decision-making reconstruction, and scalable knowledge expansion,
    specifically designed to handle unstructured and multimodal medical data. By bridging
    the gap between traditional KG approaches and modern AI capabilities, KG4Diagnosis
    aims to enable more robust and adaptable healthcare decision support systems.


    In this paper, we make the following key contributions:


    - We propose KG4Diagnosis, a novel hierarchical multiagent framework that mirrors
    real-world medical systems, consisting of a GP agent for initial assessment and
    specialized agents for domain-specific diagnosis across 362 common diseases.

    - We develop an innovative end-to-end knowledge graph construction pipeline incorporating
    three key components: semantic-driven entity extraction, multidimensional decision
    relationship reconstruction, and human-guided reasoning for knowledge expansion.

    - We implement robust mechanisms to address LLM hallucination challenges in medical
    diagnosis through multiagent verification and knowledge graph constraints, validated
    using comprehensive benchmarks.

    - We demonstrate the framework''s practical value through real-world healthcare
    scenarios.

    - We provide a modular and extensible architecture that supports the seamless
    integration of new medical domains and knowledge, with detailed implementation
    protocols for widespread adoption in various medical contexts.


    ## Methodology


    ### System Architecture Overview


    KG4Diagnosis is designed as a hierarchical multi-agent framework that integrates
    LLMs with automated knowledge graph construction for medical diagnosis (see Figure
    1). The system architecture consists of two primary components: a knowledge graph
    construction pipeline that processes and structures medical knowledge and a Camel-based
    multiagent system that enables hierarchical medical decisionmaking. This design
    mirrors real-world medical practices, where general practitioners collaborate
    with specialists to provide comprehensive patient care (see Figure 2).


    ### Knowledge Graph Construction Pipeline


    This framework implements a three-stage process for the automated construction
    of a medical knowledge graph. Initially, medical documents are segmented into
    data chunks that adhere to the contextual constraints of the knowledge graph.
    Subsequently, a semantic-driven entity and relationship extraction module is employed
    to extract entities and relationships from these data chunks. This process leverages
    BioBERT, a model specifically designed for the biomedical domain, which ensures
    the precise extraction of medical entities and the identification of relationships
    between them. In the following stage, based on the extracted entities and relationships,
    a knowledge graph is constructed, thereby facilitating its automatic generation.
    We also enhance the medical knowledge graph by using LLMs to identify broader,
    context-aware entities and relations, complementing BioBERT''s domain-specific
    extractions.


    In the last stage, the expansion and validation of the knowledge graph will be
    facilitated through expert evaluation. Medical experts will manually validate
    the relationships that have been constructed, and the verified knowledge will
    be used to train large-scale models to facilitate future knowledge expansion.


    The details of each part of the construction pipeline are as follows:


    Stage 1: Data Chunking and Segmentation In the first stage, medical documents
    are segmented into data chunks based on contextual constraints. Let D = {d1, d2,
    . . . , dn} represent a set of medical documents. Each document d<sup>i</sup>
    is segmented into m data chunks:


    $$C\_i = \{c\_{i1}, c\_{i2}, \dots, c\_{im}\}$$


    These chunks cij are generated using context-based segmentation rules. The segmentation
    process can be mathematically represented as:


    $$f\_{\text{seg}}(d\_i) \to C\_i$$


    where fseg is a function that maps a document d<sup>i</sup> to a set of data chunks
    C<sup>i</sup> .


    Stage 2: Semantic-driven Entity and Relationship Extraction The pipeline leverages
    BioBERT''s contextual embeddings along with medical ontologies, such as SNOMED-CT
    and UMLS, to extract entities and relationships from the segmented data chunks.
    The process of extraction can be represented as follows:


    • *Entity Extraction:* The set of extracted entities E is defined as:


    $$E = \{e\_1, e\_2, \dots, e\_n\}$$


    where E represents the set of medical entities, such as diseases, drugs, symptoms,
    etc.


    ![](_page_2_Figure_0.jpeg)


    Figure 1: An overview of the KG4Diagnosis framework. The system includes the following
    components: (1) input medical text is segmented into chunks and processed through
    entity extraction and relation extraction modules; (2) extracted entities and
    relations are stored in dedicated databases; (3) these databases are utilized
    to construct the medical KG; (4) the medical KG is integrated with LLMs and MAS
    to enhance diagnostic reasoning; (5) diagnostic responses are delivered to user
    endpoints, supported by human-guided reasoning. The framework highlights a structured
    approach to medical text processing, accurate knowledge graph construction, and
    collaborative reasoning for advanced diagnostic outcomes.


    **Diagnosis Example**


    ![](_page_2_Figure_3.jpeg)


    Figure 2: An example of a diagnostic conversation illustrating interactions between
    a patient, a doctor, and an AI medical assistant. The patient describes symptoms,
    the doctor asks clarifying questions, and the AI provides explanations and suggestions.
    This dialogue highlights the collaborative diagnostic process and how AI systems
    can assist in providing personalized medical advice.


    • *Relationship Extraction:* The set of relationships R between entities e<sup>i</sup>
    and e<sup>j</sup> is represented as:


    $$R = \{ (e\_i, r, e\_j) \mid e\_i, e\_j \in E \}$$


    where r denotes the relationship between entities e<sup>i</sup> and


    e<sup>j</sup> that are extracted from the medical text.


    In this stage, BioBERT captures the semantic meaning of the medical text and maps
    it to standardized medical ontologies, ensuring accurate entity and relationship
    extraction.


    Stage 3: Knowledge Graph Construction Once entities and relationships are extracted,
    a knowledge graph is constructed. A knowledge graph can be represented as a graph
    G, where:


    $$G = (V, E)$$


    Here, the set of nodes V = {e1, e2, . . . , ek} represents the medical entities,
    and the set of edges E = {r1, r2, . . . , rl} represents the relationships between
    these entities.


    The construction of the knowledge graph is based on the extracted entities and
    relationships. Thus, the knowledge graph can be represented as:


    $$G = (V, E) \quad \text{where} \quad V = E \text{ and } E = R$$


    The nodes represent entities, and the edges represent relationships.


    Stage 4: LLM-Augmented Knowledge Graph We utilize LLMs to enhance the medical
    knowledge graph by identifying entities and relations that extend beyond BioBERT''s
    extraction capabilities. While BioBERT excels in precise, domain-specific extractions
    within the biomedical field, LLMs contribute broader, context-aware semantic extractions,
    especially from complex or ambiguous medical texts. The enriched entities and
    relations are stored in dedicated databases and integrated into the knowledge
    graph, which is then optimized for reasoning with LLMs. This enhanced knowledge
    graph supports advanced diagnostic workflows by enabling more robust reasoning
    and decision-making through the synergistic capabilities of multi-agent systems
    and LLM-driven diagnostic reasoning.


    Stage 5: Human-Guided Reasoning In this final stage, expert validation is crucial
    in ensuring the quality and accuracy of the constructed relationships and entities
    in the knowledge graph. The expert validation process involves active learning
    and reinforcement learning techniques to expand the graph with verified and reliable
    information.


    - *Expert Validation of Relationships:* Medical experts manually review the extracted
    relationships R between entities to validate their clinical relevance. If a relationship
    (e<sup>i</sup> , r, e<sup>j</sup> ) is confirmed to be accurate, it is retained
    in the knowledge graph. If a relationship is deemed invalid or uncertain, it is
    either corrected or removed.

    - *Graph Expansion with Expert-Verified Relationships:* After validation, the
    knowledge graph is expanded by incorporating new, expert-verified entities and
    relationships. The validated graph is enriched with these confirmed connections,
    improving the graph''s reliability and comprehensiveness.


    Gexpanded = G ∪ Validated Entities and Relationships


    where Gexpanded represents the expanded knowledge graph that includes both previously
    extracted and expertverified entities and relationships.


    Through this expert-guided validation and expansion process, the knowledge graph
    evolves into a robust and reliable resource for medical research and clinical
    decision-making.


    ### Hierarchical Multi-Agent Framework for Medical Diagnosis


    To address the complexity of medical diagnostic reasoning, we developed a hierarchical
    multi-agent framework that processes user queries for diagnosis. This framework
    integrates a General Practitioner Large Language Model (GP-LLM) and multiple domain-specific
    Consultant Large Language Models (Consultant-LLMs). The diagnostic process is
    mathematically modelled as follows:


    GP-LLM: Primary Diagnostic Agent The GP-LLM serves as the initial interface for
    analyzing user queries. Let the user query be denoted by q ∈ Q, where Q is the
    set of all possible user queries. The diagnostic confidence for a query q producing
    a preliminary diagnosis x is defined as:


    $$P\_{\rm GP}(x \mid q) = f\_{\rm GP}(q) \tag{1}$$


    where PGP(x | q) ∈ [0, 1] is the confidence assigned by the GP-LLM to the diagnosis
    x, and fGP represents the probabilistic diagnostic function based on a broad-spectrum
    knowledge base.


    The GP-LLM initiates a referral when:


    $$P\_{\text{GP}}(x \mid q) < \tau \quad \text{or} \quad x \in X\_s \tag{2}$$


    Here:


    • τ is the confidence threshold for referral (set to 0.7).


    • X<sup>s</sup> ⊂ X is the subset of diagnoses requiring specialized expertise.


    The output of the GP-LLM is expressed as:


    $$\text{Output}\_{\text{GP}} = \begin{cases} \text{Reflex1 to Consulant-L.LM},
    & \text{if } P\_{\text{GP}}(x \mid q) < \tau \text{ or } x \in X\_{\text{s}},\\
    \text{Diagnosis: } x, & \text{otherwise.} \end{cases} \tag{3}$$


    Consultant-LLMs: Specialized Diagnostic Agents Each Consultant-LLM is optimized
    for a specific medical domain, such as rheumatology. Let Agent<sup>i</sup> represent
    the i th Consultant-LLM, where i = 1, 2, . . . , n and n = 4 (cardiology, neurology,
    endocrinology and rheumatology) in this framework. The confidence function for
    Agent<sup>i</sup> diagnosing a condition y from query q is defined as:


    $$P\_{\text{Agent}\_i}(y \mid q) = f\_{\text{Agent}\_i}(q) \tag{4}$$


    where PAgent<sup>i</sup> (y | q) ∈ [0, 1] and fAgent<sup>i</sup> is the probabilistic
    diagnostic function based on domain-specific training datasets and clinical guidelines.


    For cases requiring collaborative reasoning between multiple agents, the final
    diagnosis confidence is computed as:


    $$P\_{\text{final}}(z \mid q) = \sum\_{i=1}^{n} w\_i P\_{\text{Agent}\_i}(z \mid
    q) \tag{5}$$


    where w<sup>i</sup> represents the weight assigned to Agent<sup>i</sup> ''s contribution,
    normalized such that P<sup>n</sup> <sup>i</sup>=1 w<sup>i</sup> = 1.


    Inter-Agent Communication Protocol The referral and communication processes ensure
    the seamless transfer of cases and collaborative refinement. Let T(A, B, q) denote
    the transfer of the user query q from agent A to agent B. The transfer function
    is modeled as:


    $$T(A, B, q) = \phi(q), \quad \phi: Q \to Q'' \tag{6}$$


    where ϕ transforms q into a format compatible with the receiving agent B. Feedback
    to the GP-LLM updates its knowledge base KGP as follows:


    $$K\_{\rm GP}^{(t+1)} = K\_{\rm GP}^{(t)} + \Delta K \tag{7}$$


    where ∆K is the incremental knowledge derived from Consultant-LLMs.


    Referral Decision Threshold The referral decision is mathematically defined as:


    $$\text{Referal} = \begin{cases} 1, & \text{if } P\_{\text{GP}}(x \mid q) < \tau
    \text{ or } x \in X\_s \\ 0, & \text{otherwise.} \end{cases} \quad (8)$$


    Here:


    - Referral = 1 indicates escalation to a Consultant-LLM.

    - Referral = 0 implies retention of the query within the GP-LLM.


    Advanced Diagnosis with Multi-Agent Collaboration For complex queries requiring
    input from multiple Consultant-LLMs, the final diagnosis confidence is calculated
    as:


    $$P\_{\text{final}}(z \mid q) = \frac{1}{n} \sum\_{i=1}^{n} P\_{\text{Agent}\_i}(z
    \mid q), \quad z \in Z \qquad (9)$$


    where Z ⊂ X represents the space of complex diagnoses requiring multi-domain expertise.


    Summary This modelling formalizes the diagnostic reasoning within the hierarchical
    multi-agent framework. The confidence functions PGP(x | q), PAgent<sup>i</sup>
    (y | q), and Pfinal(z | q) define the probabilistic outputs of the GP-LLM, individual
    Consultant-LLMs, and the collaborative multi-agent system, respectively. The confidence
    threshold (τ = 0.7) ensures accurate and efficient escalation to specialized diagnostic
    agents when necessary.


    ### Future Training and Evaluation Work


    The system''s training approach encompasses a comprehensive coverage of 362 common
    diseases across multiple medical specialties, representing a significant scope
    in medical diagnosis. The training process is strategically designed to be multi-faceted,
    combining general medical knowledge with specialized domain expertise. For each
    disease category, we implement targeted fine-tuning protocols for the respective
    specialist agents, ensuring deep domain-specific knowledge while maintaining coherent
    integration within the broader framework.


    The example of the knowledge graph presented by Figure 3, 4 showcases two advanced
    obesity medications (Ozempic and Wegovy), demonstrating how our framework effectively
    simulates real-world clinical consultations. The full structure of the knowledge
    graph resulting, as illustrated in Figure 5 demonstrates the complex interconnections
    between different entities of disease, symptoms, and diagnostic patterns. The
    visualization reveals the hierarchical nature of medical knowledge organization,
    with clear pathways from general diagnostic patterns to specialized medical domains.
    This structure enables efficient knowledge navigation and supports the system''s
    hierarchical decision-making processes.


    Our continuous learning mechanism enhances the initial training through dynamic
    agent interactions and feedback loops. This approach allows the system to evolve
    and refine its diagnostic capabilities over time, adapting to new medical insights
    and patterns identified through agent collaboration. The framework, implemented
    using PyTorch for neural network components and Neo4j for knowledge graph management,
    currently encompasses all 362 diseases in its knowledge base, with structured
    pathways for knowledge expansion.


    Given the framework''s comprehensive scope and innovative approach to medical
    diagnosis, a comprehensive benchmark is currently being developed to evaluate
    performance across multiple dimensions, including diagnostic accuracy, hallucination
    prevention, and multi-agent coordination efficiency. This benchmark will provide
    standardized metrics


    ![](_page_4_Figure_9.jpeg)


    Figure 3: Example 1 illustrates the complexity of obesity, highlighting its core
    condition along with related factors such as patient status and bariatric surgery.
    It also depicts associated drug and BMI categorization, emphasizing the interconnectedness
    of these elements in understanding obesity as a multifaceted health condition.


    for assessing medical AI systems and will be made publicly available through our
    GitHub repository upon completion. The forthcoming benchmark aims to establish
    new standards for evaluating hierarchical multi-agent systems in medical applications,
    facilitating future research and development in this critical domain.


    ## Discussion


    The development and evaluation of KG4Diagnosis, encompassing 362 common diseases
    across multiple medical specialties, reveals significant insights into integrating
    hierarchical multi-agent systems with medical knowledge graphs for healthcare
    applications. Our comprehensive framework demonstrates both promising capabilities
    and important challenges that warrant further investigation.


    ### Technical Achievements and Innovations


    The combination of automated knowledge graph construction with hierarchical multi-agent
    architecture shows encouraging results in addressing key challenges in medical
    AI systems. Our framework''s ability to maintain diagnostic accuracy while preventing
    hallucination represents a significant advancement over traditional single-agent
    approaches. Particularly noteworthy is the effectiveness of our semanticdriven
    entity extraction and relationship reconstruction modules in handling complex
    medical terminology and relationships, achieving higher precision compared to
    conventional


    ![](_page_5_Figure_0.jpeg)


    Figure 4: Example 2 illustrates the expertise of the knowledge graph in the field
    of obesity. This knowledge graph highlights how certain drugs, such as Ozempic,
    not only aid in weight management but also reduce cardiovascular risk. Connections
    between obesity, Type 2 Diabetes, and cardiovascular diseases are depicted, showing
    their shared symptoms, treatments, and comorbidities. The graph underscores the
    multifaceted role of medications in addressing complex health conditions.


    methods.


    The hierarchical multi-agent structure, implemented through the MAS, proves especially
    valuable in managing complex medical cases. The GP agent''s ability to effectively
    triage cases and coordinate with specialist agents mirrors real-world medical
    practices, potentially reducing the computational overhead associated with full
    specialist consultation for every case. Furthermore, our approach to hallucination
    prevention through multiple validation layers, with the knowledge graph serving
    as an effective constraint system, significantly reduces incorrect diagnoses compared
    to standalone LLM implementations.


    ### System Adaptability and Scalability


    The resulting knowledge graph structure demonstrates the complex interconnections
    between different disease entities, symptoms, and diagnostic patterns. This comprehensive
    coverage supports efficient knowledge navigation and hierarchical decision-making
    processes. The modularity of our framework shows particular strength in incorporating
    new medical domains and knowledge, making it well-suited for the dynamic nature
    of medical knowledge.


    However, scalability analysis reveals important considerations. While the hierarchical
    structure efficiently manages computational resources through its tiered decision-making
    process, the system faces increasing complexity in coordinating multiple specialist
    agents as the number of medical domains expands. This highlights the need for
    more sophisticated coordination mechanisms in future iterations.


    ### Limitations and Challenges


    The system''s performance can be influenced by the quality and comprehensiveness
    of the underlying knowledge graph, particularly in rare or complex medical conditions.
    Challenges remain in handling edge cases where medical knowledge is rapidly evolving
    or when dealing with rare disease combinations not well-represented in the training
    data.


    Future research will involve conducting experiments on the state-of-the-art MedQA
    dataset to validate the superiority of our framework. MEDQA can be used to perform
    benchmark tests, thus evaluating the reproducibility of the perfect functioning
    of LLM. Meanwhile, this will allow us to benchmark our framework against other
    prominent models, such as ESM-1b, Med-PaLM, and BioGPT. By evaluating performance
    in MedQA, we aim not only to demonstrate the competitive advantages of our system
    but also to identify areas for further improvement. Additionally, the system''s
    heavy reliance on high-quality medical data for both knowledge graph construction
    and agent training presents challenges for deployment in regions with limited
    medical data resources. While our framework shows strong performance in well-documented
    medical conditions, its effectiveness in handling rare diseases or unusual symptom
    combinations requires further investigation.


    ## Related Work


    Rule-Based and Ontology-Driven Approaches: Recent advances in the construction
    and reasoning of medical KG have spawned various methodological approaches (Lu
    et al. 2024; Li et al. 2020; Peng et al. 2023), each offering unique advantages
    while facing distinct challenges. Traditional approaches to medical KG construction
    primarily rely on rulebased systems and ontology-driven techniques. While these
    methods excel in producing interpretable outputs and maintaining structural consistency
    through established medical ontologies, they face significant limitations in scalability
    and processing unstructured data (Abdulla, Mukherjee, and Ranganathan 2023).


    Deep Learning and Pre-Trained Models: The emergence of deep learning methods,
    particularly pre-trained language models such as BERT and BioBERT (Masoumi et
    al. 2024), has substantially improved information extraction capabilities in clinical
    texts. However, these models often struggle with domain-specific nuances and require
    considerable computational resources (Alsentzer et al. 2019).LLMs represent an
    advancement in processing unstructured medical data. While recent studies demonstrate
    their potential in generating structured knowledge and understanding complex medical
    relationships, challenges persist regarding hallucination and validation (Brown
    et al. 2020). The Med-HALT benchmark and contrastive decoding techniques have
    emerged as promising approaches to address these concerns (Liu et al. 2023). Furthermore,
    integrating LLMs with multi-agent systems (MAS) has shown particular promise in
    medical applications (Singhal et al. 2023b).


    Hybrid Symbolic-Neural Approaches: Hybrid approaches combining symbolic reasoning
    with neural architectures have gained traction for their ability to bal-


    ![](_page_6_Figure_0.jpeg)


    Figure 5: A visualization of the KG4Diagnosis full medical knowledge graph. Nodes
    represent different medical concepts, such as actions, symptoms, categories, and
    conditions, as indicated by the color legend. Edges signify relationships between
    these concepts, enabling structured representation and advanced diagnostic reasoning.
    The densely connected central region highlights the core interactions between
    treatments, symptoms, and diagnostics, while peripheral nodes provide additional
    contextual details. This hierarchical structure integrates medical data to facilitate
    multi-agent collaboration and human-guided reasoning.


    ance interpretability with adaptability. These systems integrate knowledge-driven
    reasoning with data-driven learning, though they require well-curated inputs and
    face computational scalability challenges (Wu, Zhang, and Lin 2023). Recent innovations
    in multimodal integration have expanded KG capabilities to incorporate diverse
    data types, including clinical notes, medical imaging, and laboratory results,
    although standardization and fusion challenges remain (Zhou et al. 2022).


    Advancements in Medical LLMs: Recent advancements in medical LLMs have significantly
    enhanced the field of natural language understanding in healthcare. Models like
    ESM-1b (Rives et al. 2021), originally developed for protein representation, have
    shown promise in biomedical applications, leveraging evolutionary scale modeling
    to analyze biological sequences with high accuracy. Med-PaLM (Singhal et al. 2023a),,
    on the other hand, represents a specialized adaptation of general-purpose LLMs
    for clinical use, focusing on answering medical questions and reasoning within
    structured datasets. Similarly, MediTron (Bosselut et al. 2024) and BioGPT (Luo
    et al. 2022) have been designed to extract biomedical knowledge, with MediTron
    excelling in multimodal data integration and BioGPT being fine-tuned specifically
    on biomedical literature for entity and relation extraction tasks. The recent
    development of GPT-4 medprompt (Nori et al. 2023) further pushes the boundaries
    of medical LLMs by integrating domain-specific prompts to guide reasoning, improving
    contextual accuracy and reducing hallucination in medical applications.


    Hierarchical Multi-Agent Architectures: The emergence of hierarchical multi-agent
    architectures represents a particularly promising direction. Pandey et al. (Pandey,
    Amod, and Kumar 2024) demonstrate that such architectures can effectively mirror
    real-world medical systems, with general-purpose agents handling initial assessment
    and specialized agents managing domain-specific diagnoses. This approach not only
    improves diagnostic accuracy but also enhances system scalability and reliability.


    Despite these advancements, the field continues to grapple with several critical
    challenges. The processing of unstructured medical data remains a significant
    hurdle, requiring more sophisticated approaches for accurate information extraction
    and structuring (Avula et al. 2022). The prevention and detection of LLM hallucinations
    in medical contexts demands continued innovation in verification mechanisms and
    validation protocols (Huang et al. 2023). Additionally, the integration of multimodal
    medical information presents ongoing challenges in data standardization and fusion.
    The coordination of multiple specialized agents within medical systems requires
    further refinement of communication protocols and decision-making frameworks.
    Furthermore, the development of comprehensive and standardized evaluation protocols
    for medical KG systems remains an active area of research, which is essential
    for ensuring the reliability and effectiveness of these systems in clinical applications.
    These interconnected challenges present opportunities for innovative solutions
    that combine the strengths of various approaches while addressing their individual
    limitations.


    ## Conclusion


    This paper presents KG4Diagnosis, a novel hierarchical multi-agent framework that
    integrates automated knowledge graph construction with specialized LLMs for medical
    diagnosis. Our implementation, covering 362 common diseases, demonstrates the
    effectiveness of combining knowledge graphs with a hierarchical multi-agent architecture
    to address critical challenges in medical AI systems. The framework''s innovations
    lie in its three-stage knowledge graph construction pipeline and hierarchical-based
    agent structure, where semantic-driven processing and human-guided reasoning create
    a robust knowledge foundation, while the multi-tiered agent architecture mirrors
    real-world medical practices. The system demonstrates significant advantages in
    preventing hallucination through multiple validation layers and managing computational
    resources through targeted specialist consultation. Although our current implementation
    shows promising results, we are developing comprehensive benchmarks to provide
    standardized evaluation metrics for the community. This work not only contributes
    a practical solution for current medical AI challenges but also establishes a
    foundation for future developments in hierarchical multi-agent systems for healthcare
    applications, potentially improving healthcare delivery and patient outcomes.


    ## References


    Abdulla, K.; Mukherjee, S.; and Ranganathan, P. 2023. Integrating Multimodal Data
    for Enhancing Knowledge Graphs: Current Challenges and Opportunities. *Journal
    of Big Data*, 10(1): 1–15.


    Al Khatib, H. S.; Neupane, S.; Kumar Manchukonda, H.; Golilarz, N. A.; Mittal,
    S.; Amirlatifi, A.; and Rahimi, S. 2024. Patient-centric Knowledge Graphs: A Survey
    of Current Methods, Challenges, and Applications. *Frontiers in Artificial Intelligence*,
    7: 1388479.


    Alam, F.; Giglou, H. B.; and Malik, K. M. 2023. Automated Clinical Knowledge Graph
    Generation Framework for Evidence-based Medicine. *Expert Systems with Applications*,
    233: 120964.


    Alsentzer, E.; Murphy, J.; Boag, W.; Weng, W.; Jin, D.; Naumann, T.; and McDermott,
    M. 2019. Publicly Available Clinical BERT Embeddings. arXiv:1901.08746.


    Amos, L.; Anderson, D.; Brody, S.; Ripple, A.; and Humphreys, B. L. 2020. UMLS
    Users and Uses: A Current Overview. *Journal of the American Medical Informatics
    Association*, 27(10): 1606–1611.


    Avula, R.; et al. 2022. Data-Driven Decision-Making in Healthcare Through Advanced
    Data Mining Techniques: A Survey on Applications and Limitations. *International
    Journal of Applied Machine Learning and Computational Intelligence*, 12(4): 64–85.


    Bonner, S.; Barrett, I. P.; Ye, C.; Swiers, R.; Engkvist, O.; Bender, A.; Hoyt,
    C. T.; and Hamilton, W. L. 2022. A Review of Biomedical Datasets Relating to Drug
    Discovery: A Knowledge Graph Perspective. *Briefings in Bioinformatics*, 23(6):
    bbac404.


    Bosselut, A.; Chen, Z.; Romanou, A.; Bonnet, A.; Hernandez-Cano, A.; Alkhamissi,
    B.; Matoba, K.; Salvi, F.; ´ Pagliardini, M.; Fan, S.; et al. 2024. MEDITRON:
    Open Medical Foundation Models Adapted for Clinical Practice.


    Brown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J.; Dhariwal, P.; Neelakantan,
    A.; Shyam, P.; Sastry, G.; and Askell, A. 2020. Language Models Are Few-Shot Learners.
    arXiv:2005.14165.


    Chang, E.; and Mostafa, J. 2021. The Use of SNOMED CT, 2013-2020: A Literature
    Review. *Journal of the American Medical Informatics Association*, 28(9): 2017–2026.


    Du, K.; Yang, B.; Wang, S.; Chang, Y.; Li, S.; and Yi, G. 2022. Relation extraction
    for manufacturing knowledge graphs based on feature fusion of attention mechanism
    and graph convolution network. *Knowledge-Based Systems*, 255: 109703.


    Garc´ıa-Ferrero, I.; Agerri, R.; Salazar, A. A.; Cabrio, E.; de la Iglesia, I.;
    Lavelli, A.; Magnini, B.; Molinet, B.; Ramirez-Romero, J.; Rigau, G.; et al. 2024.
    Medical mT5: An Open-Source Multilingual Text-to-Text LLM for the Medical Domain.
    *arXiv preprint arXiv:2404.07613*.


    Guo, T.; Chen, X.; Wang, Y.; Chang, R.; Pei, S.; Chawla, N. V.; Wiest, O.; and
    Zhang, X. 2024. Large Language Model Based Multi-Agents: A Survey of Progress
    and Challenges. *arXiv preprint arXiv:2402.01680*.


    Huang, L.; Yu, W.; Ma, W.; Zhong, W.; Feng, Z.; Wang, H.; Chen, Q.; Peng, W.;
    Feng, X.; Qin, B.; et al. 2023. A Survey on Hallucination in Large Language Models:
    Principles, Taxonomy, Challenges, and Open Questions. *ACM Transactions on Information
    Systems*.


    Li, L.; Wang, P.; Yan, J.; Wang, Y.; Li, S.; Jiang, J.; Sun, Z.; Tang, B.; Chang,
    T.-H.; Wang, S.; et al. 2020. Real-World Data Medical Knowledge Graph: Construction
    and Applications. *Artificial Intelligence in Medicine*, 103: 101817.


    Liu, Y.; Ott, M.; Goyal, N.; Du, J.; Joshi, M.; Chen, D.; Levy, O.; Lewis, M.;
    Zettlemoyer, L.; and Stoyanov, V. 2023. Med-HALT: Evaluating Hallucinations in
    Medical LLMs. arXiv:2410.15702.


    Lu, Z.; Afridi, I.; Kang, H. J.; Ruchkin, I.; and Zheng, X. 2024. Surveying Neuro-Symbolic
    Approaches for Reliable Artificial Intelligence of Things. *Journal of Reliable
    Intelligent Environments*, 10(3): 257–279.


    Luo, R.; Sun, L.; Xia, Y.; Qin, T.; Zhang, S.; Poon, H.; and Liu, T.-Y. 2022.
    BioGPT: Generative Pre-Trained Transformer for Biomedical Text Generation and
    Mining. *Briefings in Bioinformatics*, 23(6): bbac409.


    Masoumi, S.; Amirkhani, H.; Sadeghian, N.; and Shahraz, S. 2024. Natural Language
    Processing (NLP) to Facilitate Abstract Review in Medical Research: The Application
    of BioBERT to Exploring the 20-Year Use of NLP in Medical Research. *Systematic
    Reviews*, 13(1): 107.


    Mo, F.; Chaplin, J. C.; Sanderson, D.; Mart´ınez-Arellano, G.; and Ratchev, S.
    2024. Semantic models and knowledge graphs as manufacturing system reconfiguration
    enablers. *Robotics and Computer-Integrated Manufacturing*, 86: 102625.


    Nori, H.; Lee, Y. T.; Zhang, S.; Carignan, D.; Edgar, R.; Fusi, N.; King, N.;
    Larson, J.; Li, Y.; Liu, W.; et al. 2023. Can Generalist Foundation Models Outcompete
    Special-Purpose Tuning? Case Study in Medicine. *arXiv preprint arXiv:2311.16452*.


    OpenAI. 2022. ChatGPT: Optimizing Language Models for Dialogue. OpenAI Technical
    Blog.


    OpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774.


    Pandey, H. G.; Amod, A.; and Kumar, S. 2024. Advancing Healthcare Automation:
    Multi-Agent System for Medical Necessity Justification. In Demner-Fushman, D.;
    Ananiadou, S.; Miwa, M.; Roberts, K.; and Tsujii, J., eds., *Proceedings of the
    23rd Workshop on Biomedical Natural Language Processing*, 39–49. Bangkok, Thailand:
    Association for Computational Linguistics.


    Peng, C.; Xia, F.; Naseriparsa, M.; and Osborne, F. 2023. Knowledge Graphs: Opportunities
    and Challenges. *Artificial Intelligence Review*, 56(11): 13071–13102.


    Qian, J.; Jin, Z.; Zhang, Q.; Cai, G.; and Liu, B. 2024. A Liver Cancer Question-Answering
    System Based on Next-Generation Intelligence and the Large Model Med-PaLM 2. *International
    Journal of Computer Science and Information Technology*, 2(1): 28–35.


    Rives, A.; Meier, J.; Sercu, T.; Goyal, S.; Lin, Z.; Liu, J.; Guo, D.; Ott, M.;
    Zitnick, C. L.; Ma, J.; et al. 2021. Biological Structure and Function Emerge
    from Scaling Unsupervised Learning to 250 Million Protein Sequences. *Proceedings
    of the National Academy of Sciences*, 118(15): e2016239118.


    Shuifa, S.; Xiaolong, L.; Weisheng, L.; Dajiang, L.; Sihui, L.; Liu, Y.; and Yirong,
    W. 2023. Review of Graph Neural Networks Applied to Knowledge Graph Reasoning.
    *Journal of Frontiers of Computer Science & Technology*, 17(1): 27.


    Singhal, K.; Azizi, S.; Tu, T.; Mahdavi, S. S.; Wei, J.; Chung, H. W.; Scales,
    N.; Tanwani, A.; Cole-Lewis, H.; Pfohl, S.; et al. 2023a. Large Language Models
    Encode Clinical Knowledge. *Nature*, 620(7972): 172–180.


    Singhal, K.; Tu, T.; Gottweis, J.; Sayres, R.; and Wulczyn, E. 2023b. Towards
    Expert-Level Medical Question Answering with Large Language Models. *Nature Medicine*,
    29(1): 50– 58.


    Tang, X.; Chi, G.; Cui, L.; Ip, A. W.; Yung, K. L.; and Xie, X. 2023. Exploring
    Research on the Construction and Application of Knowledge Graphs for Aircraft
    Fault Diagnosis. *Sensors*, 23(11): 5295.


    Tonmoy, S.; Zaman, S.; Jain, V.; Rani, A.; Rawte, V.; Chadha, A.; and Das, A.
    2024. A Comprehensive Survey of Hallucination Mitigation Techniques in Large Language
    Models. *arXiv preprint arXiv:2401.01313*.


    Touvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux, M.-A.; Lacroix, T.;
    Roziere, B.; Goyal, N.; Hambro, E.; ` Azhar, F.; et al. 2023. LLaMA: Open and
    Efficient Foundation Language Models. *arXiv preprint arXiv:2302.13971*.


    Wu, J.; Zhu, J.; Qi, Y.; Chen, J.; Xu, M.; Menolascina, F.; and Grau, V. 2024.
    Medical Graph RAG: Towards Safe Medical Large Language Model via Graph Retrieval-Augmented
    Generation. *arXiv preprint arXiv:2408.04187*.


    Wu, Z.; Zhang, Y.; and Lin, X. 2023. Scalability Challenges in Medical Knowledge
    Graph Construction. *IEEE Transactions on Medical Informatics*, 14(2): 120–132.


    Zhang, J.; Zan, H.; Wu, S.; Zhang, K.; and Huo, J. 2024. Adaptive Graph Neural
    Network with Incremental Learning Mechanism for Knowledge Graph Reasoning. *Electronics*,
    13(14): 2778.


    Zhang, Y. 2021. Knowledge Reasoning with Graph Neural Networks. *Georgia Institute
    of Technology: Atlanta, GA, USA*.


    Zhang, Y.; Li, X.; and Wang, J. 2023. Knowledge Graph for Fraud Detection: Case
    of Fraudulent Related Party Transactions. In *Advances in Knowledge Discovery
    and Data Mining*, 182–194. Springer.


    Zhou, J.; Cui, G.; Zhang, Z.; Yang, C.; Liu, Z.; Wang, L.; Li, C.; and Sun, M.
    2022. Graph Neural Networks: A Review of Methods and Applications. *AI Open*,
    1(1): 1–12.'
- title: A Statically and Dynamically Scalable Soft GPGPU
  abstract: 'Current soft processor architectures for FPGAs do not utilize the potential
    of the massive parallelism available. FPGAs now support many thousands of embedded
    floating point operators, and have similar computational densities to GPGPUs.
    Several soft GPGPU or SIMT processors have been published, but the reported large
    areas and modest Fmax makes their widespread use unlikely for commercial designs.
    In this paper we take an alternative approach, building the soft GPU microarchitecture
    around the FPGA resource mix available. We demonstrate a statically scalable soft
    GPGPU processor (where both parameters and feature set can be determined at configuration
    time) that always closes timing at the peak speed of the slowest embedded component
    in the FPGA (DSP or hard memory), with a completely unconstrained compile into
    a current Intel Agilex FPGA. We also show dynamic scalability, where a subset
    of the thread space can be specified on an instruction-by-instruction basis.

    For one example core type, we show a logic range – depending on the configuration
    – of 4k to 10k ALMs, along with 24 to 32 DSP Blocks, and 50 to 250 M20K memories.
    All of these instances close timing at 771 MHz, a performance level limited only
    by the DSP Blocks. We describe our methodology for reliably achieving this clock
    rate by matching the processor pipeline structure to the physical structure of
    the FPGA fabric. We also benchmark several algorithms across a range of data sizes,
    and compare to a commercial soft RISC processor.'
  keywords: ''
  document: '# A Statically and Dynamically Scalable Soft GPGPU


    Martin Langhammer Intel Corporation & Imperial College London London, UK martin.langhammer@intel.com


    ## ABSTRACT


    Current soft processor architectures for FPGAs do not utilize the potential of
    the massive parallelism available. FPGAs now support many thousands of embedded
    floating point operators, and have similar computational densities to GPGPUs.
    Several soft GPGPU or SIMT processors have been published, but the reported large
    areas and modest Fmax makes their widespread use unlikely for commercial designs.
    In this paper we take an alternative approach, building the soft GPU microarchitecture
    around the FPGA resource mix available. We demonstrate a statically scalable soft
    GPGPU processor (where both parameters and feature set can be determined at configuration
    time) that always closes timing at the peak speed of the slowest embedded component
    in the FPGA (DSP or hard memory), with a completely unconstrained compile into
    a current Intel Agilex FPGA. We also show dynamic scalability, where a subset
    of the thread space can be specified on an instruction-by-instruction basis.


    For one example core type, we show a logic range – depending on the configuration
    – of 4k to 10k ALMs, along with 24 to 32 DSP Blocks, and 50 to 250 M20K memories.
    All of these instances close timing at 771 MHz, a performance level limited only
    by the DSP Blocks. We describe our methodology for reliably achieving this clock
    rate by matching the processor pipeline structure to the physical structure of
    the FPGA fabric. We also benchmark several algorithms across a range of data sizes,
    and compare to a commercial soft RISC processor.


    ## 1 INTRODUCTION


    FPGAs are capable platforms, with multiple thousands of embedded memories as well
    as DSP Blocks, many of which now support IEEE 754 floating point numerics. In
    addition, there is a significant amount of high performance IP available for FPGAs,
    e.g. FFTs [\[2,](#page-10-0) [8\]](#page-10-1) and error correction such as Reed-Solomon
    codecs [\[3\]](#page-10-2). High performance systems can readily be assembled
    using a combination of original design and these IP Blocks. The value of the FPGA
    is integration: although each individual IP or function is lower performance than
    ASIC, this is offset by the flexibility. However, modifying IP - even your own
    - requires significant effort. FPGA hardware compile times (synthesis, place and
    route) can take hours, and timing closure can be a significant unknown. Implementing
    (and modifying) a complex subset of a system by a pure software approach, where
    the result of the compile or assembly is essentially instantly available, and
    loaded onto an already placed and routed processor, may be very attractive.


    Soft RISC cores (Nios [\[9\]](#page-10-3) and MicroBlaze [\[4\]](#page-10-4))
    for FPGA have been used for over two decades, and allow the inclusion of complex
    control flow, or the offload of ancillary functions. Although these RISC processors
    are very flexible, they also have a rather low


    George A. Constantinides Imperial College London London, UK g.constantinides@imperial.ac.uk


    performance. Parallel processor architectures may offer better performance, and
    SIMT (GPGPU) processors may be able to efficiently use the large number of memory
    and DSP Blocks distributed across the FPGA device. There have been a number of
    soft SIMT FPGA architectures published [\[14,](#page-10-5) [15,](#page-10-6) [17,](#page-10-7)
    [18,](#page-10-8) [24,](#page-10-9) [25,](#page-10-10) [29\]](#page-10-11), but
    these are often very large (50K-300K LUTs), and typically have a low clock frequency
    (30MHz-100MHz). Other types of parallel processors are also known for FPGA [\[22,](#page-10-12)
    [30,](#page-10-13) [31\]](#page-10-14) (and commercialized [\[5\]](#page-10-15)),
    but the Fmax is relatively low at ∼150MHz.


    A different approach has been taken by Xilinx (now AMD) in the Versal devices,
    with arrays of AI Engines, a hardened VLIW processor. This motivates us to consider
    whether we can combine the flexibility of a soft processor (where any number can
    be instantiated into the soft fabric), but with the performance of an ASIC implementation
    (in this case, running at the speed of the embedded hardened features).


    Our design, which we call the eGPU (for embeddedGPU), is both statically and dynamically
    scalable, features which make it particularly useful and performant for FPGA applications.
    Static scalability is the ability to parameterize the thread space, shared memory
    space, integer ALU functions, as well as major processor features (such as predicates).
    Dynamic scalability allows us to operate on a defined subset of the thread space,
    and change this on an instruction by instruction basis, without any dead time.
    We will see that this can greatly reduce the number of cycles required in some
    portions of the program, such as during a vector reduction (which is a common
    kernel of GPGPU applications).


    We make the following contributions:


    - Describe a novel parameterized SIMT processor for FPGA, with a wide range of
    user defined instructions, as well as architectural trade-offs (such as predicates).

    - Demonstrate that a soft processor can consistently close timing at a level limited
    only by the embedded features such as DSP and memory, and do so with a completely
    unconstrained compile.

    - Compare the absolute and normalized (by resource cost) results of a soft GPGPU
    with a soft RISC processor, and show that the SIMT architecture is better in the
    general case, and significantly better when using dedicated hardware extensions.


    ## 2 BACKGROUND


    Our goal for this project was to architect and implement a compact, high performance
    SIMT processor, that can be used for commercial FPGA system designs. We can use
    current and prior FPGA processors both to understand the limitations of previous
    projects, and to validate some of our design choices. The axes of comparison to
    other work include memory systems, complexity (such as workload balancing), and
    trade-offs between hard and soft implementation.


    #### Table 1: Resource Comparison


    <span id="page-1-0"></span>


    | Architecture<br>Config. |          | LUTs | DSP  | FMax | PPA | Device     |  |

    |-------------------------|----------|------|------|------|-----|------------|--|

    | FGPU [15]               | 2CUx8PE  | 57K  | 48   | 250  | 36  | Zynq-7000  |  |

    | DO-GPU [29]             | 4CUx8PE  | 360K | 1344 | 208  | 133 | Stratix 10 |  |

    | FlexGrip [17]           | 1SMx16PE | 114K | 300  | 100  | 175 | Virtex-6   |  |

    | eGPU                    | 1SMx16SP | 5K   | 24   | 771  | 1   | Agilex     |  |


    Many of the previously published GPGPUs [\[15,](#page-10-6) [17,](#page-10-7)
    [24,](#page-10-9) [29\]](#page-10-11) are SIMT processors which were compiled
    to an FPGA, whereas eGPU was designed for FPGA. The eGPU has an power-performancearea
    (PPA) metric which is one or two orders of magnitude (OOM) smaller than some of
    the earlier soft GPGPUs. Comparisons between high-performance processor designs
    are complex and multidimensional. For example, some existing soft GPUs have more
    complex memory systems, including caches and dynamic workload balancing. This
    does come with a cost, with a typical order of magnitude resource difference,
    as can be seen in Table [1,](#page-1-0) where we compare configurations of the
    other soft GPGPUs that are closest in computational structure to eGPU (PEs are
    roughly the same as SPs). Despite the much deeper pipelines (e.g. FlexGrip [\[17\]](#page-10-7)
    has a 21 deep pipeline, FGPU has a 18 deep pipeline [\[15\]](#page-10-6)), they
    also run at a considerably slower clock frequency. Although they are implemented
    in older FPGA technology (FlexGrip is in Virtex-6 at 100MHz), this does not fully
    explain the performance level, as there are soft processors that run at 450MHz
    in those devices [\[20\]](#page-10-16) [\[21\]](#page-10-17). In the benchmarking
    section we will also see that the benchmarks also run slower than expected on
    the earlier GPGPUs based on the difference in clock frequency.


    Instead, we validate eGPU against existing soft RISC processors [\[9\]](#page-10-3),
    which are extensively used in real applications. We will normalize the benchmark
    results based on cost i.e. FPGA resources consumed. The eGPU, being a parallel
    processor (with essentially 16 smaller multi-threaded processors) will naturally
    be larger; to be effective and usable, it must have a clear advantage in both
    absolute performance and normalized efficiency over the RISC processors.


    eGPU uses a single local data memory, which is configurable in size, and does
    not support a cache. Larger datasets need to be externally managed. Like the eGPU,
    the Xilinx AI Engines [\[13\]](#page-10-18), which are organized as hard VLIW
    hard processor arrays, have only a single local data memory per CPU, the loading
    and unloading of which has to be managed externally. Algorithms with larger amounts
    of data (such as 4K FFTs) need to be split across multiple AI Engines [\[6\]](#page-10-19).
    The eGPU has a greater memory flexibility, as we are able to configure a larger
    shared memory instance (we show examples with up to 128KB in this paper). The
    AI Engines give us an example of a commercial FPGA parallel processor, where using
    multiple simpler processors have been found to have an advantage over using complex
    memory systems.


    ## 3 ARCHITECTURE DESCRIPTION


    The architecture of the eGPU is based on an earlier proof-of-concept design [\[28\]](#page-10-20).
    Our new design adds significant scalability - thread and register space, shared
    memory size, instruction set support, as well as optional predicates for thread
    divergence. Figure [1](#page-2-0) shows the top level architecture of the eGPU.
    The streaming multi-processor (SM) contains 16 parallel scalar processors (SP),
    although only 8 are shown in the figure for clarity. An optional dot-product core
    and special function unit (SFU) reciprocal square root can be attached. We target
    the Intel Agilex [\[23\]](#page-10-21) family of FPGAs in this work. The eGPU
    has a very short pipeline (8 stages) compared to other GPUs; therefore, hazards
    are hidden for most programs. Consequently, we do not provide hardware support
    for tracking hazards in the current version, which in turn gives us an efficient
    and fast processor.


    Two types of embedded memories are now supported, simple dual port (DP) and the
    emulated quad port (QP) blocks [\[12\]](#page-10-22). One of the largest performance
    limitations of the earlier eGPU architecture was memory bandwidth. The QP memory
    will double the write bandwidth, while at the same time reducing the number of
    embedded memory blocks required (the 20K-bit M20K blocks) by half. The trade-off
    is that in QP mode, the memory speed is reduced from 1 GHz to 600 MHz, which then
    becomes the critical path in the processor. Resource, Fmax, and benchmark results
    are all described later in this paper.


    ## 3.1 Dynamic Scalability


    Most GPGPUs support thread divergence by predicates (threadspecific conditionals)
    but these have a potential significant performance impact, as all threads are
    run, whether or not they are written back. In addition to predicates, the eGPU
    sequencer supports an instruction by instruction specification of a subset of
    the thread space, where only the indicated threads are run. If the program can
    be constructed such that the data of interest can be written to the threads that
    can be isolated by the dynamic thread allocation, then a large number of processing
    cycles can be skipped. This is particularly noticeable in programs with many multi-cycle
    instructions, such as reads and writes to shared memory. This will have a direct
    impact on the benchmark performance (number of cycles).


    We define a wavefront as the maximum number of operations that can be run per
    clock cycle; with 16 SPs we have a wavefront width of 16. The thread block depth
    (alternately, the wavefront depth) is the number of wavefronts per instruction,
    which is the initialized thread size / 16. We feel these terms allow us to describe
    our dynamic thread scalability more concisely.


    The eGPU can be configured, on a cycle by cycle basis, to act as a standard SIMT
    processor, a multi-threaded CPU, or a single threaded MCU. While the number of
    clock cycles to execute all the threads for an operation instruction (e.g. FP
    or INT) is dependent on the depth of the thread block, loads and stores are multi-cycle
    (because of the limited number of ports to shared memory). The impact of dynamically
    adjusting the width of certain instructions (e.g. reduction, where the writeback
    data can be orders of magnitude less than the read data) can be seen in the benchmark
    section later in this paper.


    The upper 4-bit field in the instruction word (IW) allows the wavefront width
    and depth to be coded for that instruction. Perhaps the most common case will
    be using only the first SP, or even the first thread in the first SP; many GPU
    applications will have vector reduction kernels, where a reduction result(s) may
    end up in the leftmost SP. If we can operate on this SP exclusively for a certain
    subset of time during the execution of the program, we A Statically and Dynamically
    Scalable Soft GPGPU


    <span id="page-2-0"></span>![](_page_2_Figure_1.jpeg)


    Figure 1: eGPU SM Top Level Architecture


    can save significant processing time, and power. The coding of the personality
    is described in the Instruction Set section.


    Hence, we can change the scale of the SIMT span by reducing the wavefront width
    and/or depth. The eGPU can act as a multithreaded CPU if we set the wavefront
    width to one, and if we also set the thread depth to one, each instruction will
    only act on thread 0 of the first SP - this SP can then be used like a MCU. We
    will use these modes to good effect in our benchmarks later in this paper.


    <span id="page-2-1"></span>![](_page_2_Figure_5.jpeg)


    ## 3.2 Predicates


    The eGPU optionally - by user configuration - supports predicates, which enable
    thread divergence. Conditionals can be applied to each thread individually using
    a condition instruction (see Table [2\)](#page-3-0).


    As with many aspects of eGPU, the number and type of conditions can be selected
    at compile time. Although these will have only a minimal impact on area, the additional
    wireload may impact performance because of the large number of individual predicate
    stacks. There is one predicate stack per initialized thread, so there may be thousands
    of stacks per eGPU instance.


    Some algorithms, such as the bitonic sort benchmark in this paper, require predicates.
    On the other hand, many of the signal processing applications that we expect that
    the eGPU will be used for (such as FFTs and matrix decomposition) do not use data
    dependent decisions. These do not need predicates, and can be programmed using
    only loop constructs, which are supported in the eGPU sequencer. For this reason,
    the presence and complexity of predication is a parameter of our design, especially
    considering the large potential cost of the feature.


    Figure [2](#page-2-1) shows the structure of a single predicate block. Each SP
    has a separate block, which are comprised of multiple predicate stacks. Each thread
    has a unique predicate stack. Multiple nested levels of conditional operations
    (IF/ELSE/END IF) are supported


    Figure 2: One Predicate Block


    per stack, with the maximum supported depth of nesting being parameterized.


    The incremental cost of adding one level of nesting is trivial, as the control
    logic of each predicate stack is the dominant user of logic here. The wavefront
    value (for example, in our base eGPU configuration of 512 threads with 16 SPs,
    there will be 32 wavefronts - i.e. 32 threads per SP) enables the correct predicate
    stack for the current thread. If the condition instruction (IF) condition is true
    for that thread, a ''1'' will be set at the top of the predicate stack, and the
    rest of the stack pushed down. An ELSE instruction will invert the top of the
    stack, and an END IF will pop the stack and return to the previous nesting level.


    The eGPU is configured at compile time for a maximum number of threads; if the
    run time configuration of threads is less than this, there is no issue as only
    the selected threads will trigger the operation of the predicate block.


    <span id="page-3-1"></span>


    | [43:40]  |        |      | [39:34] [33:32] [31:27] | [26:22] | [21:17] | [16:1]    |

    |----------|--------|------|-------------------------|---------|---------|-----------|

    | Variable | Opcode | Typ. | RD                      | RA      | RB      | Immediate
    |


    Figure 3: Instruction Word


    The conditional value will only be applied to the current predicate block, and
    all others ignored in that clock cycle. The current thread activation thread\_active
    signal will be muxed from all the predicate blocks, selected by the current wavefront.
    The thread\_active signal is used to pass or zero the write\_enable signals to
    either the register files or shared memory, whichever is the destination for that
    instruction.


    ## 4 INSTRUCTION SET


    Table [2](#page-3-0) shows most of the instruction set for the eGPU. There are
    a total of 61 instructions, including 18 conditional cases (we omit the FP conditional
    instructions here for brevity). Usually, only a subset of instructions are included
    (by the user defined configuration of the eGPU). The 18 conditional cases depend
    on predicates being included in the parameters - as predicates typically increase
    soft logic cost by 50% they are only used when the expected class of applications
    need them. Many of the intended applications, such as FFT, matrix multiplication
    and decomposition, do not, and the required loops can be handled with the dedicated
    loop instructions. Some instructions can support multiple TYPES, such as signed
    (INT32) and unsigned (UINT32) formats for integer instructions.


    The integer ALU uses a large proportion of the soft logic (≈100 ALMs to ≈400 ALMs),
    so selecting only the required precision (16 bit or 32-bit) and feature subset
    can reduce the cost of the eGPU substantially. Extension instructions are also
    optional. We will use the dot product instruction for some of the benchmarks in
    this paper; if used, it can make significant difference to the performance of
    some functions. We can also add elementary functions (currently we support only
    reciprocal square root), which are required for algorithms such as matrix decomposition.
    In contrast, the FP instructions are almost completely contained inside the DSP
    Block, with only the FP Max() and Min() instructions having a potential impact
    on area or performance.


    Figure [3](#page-3-1) shows an instruction word, here shown in a 43-bit form.
    As the number of registers per thread changes, the three register field widths
    also change; the displayed word is for a 32 registers per thread configuration,
    which requires 5 bits to encode the register number. The 2-bit representation
    field encodes whether the number is unsigned integer, signed integer, or FP32.
    The four most significant bits encode the processing type, which allow the wavefront
    depth and the width of the wavefront to be changed on an instruction by instruction
    basis.


    Writing these results into shared memory using subset write can be 16x faster
    than using the generic write. An instruction, whether used for a full or a partial
    thread space, is almost identical, with only the four instruction type bits used
    to control the subset of the thread space. Table [3](#page-3-2) shows how the
    upper 4 bits of the IW control the width and depth of the thread space.


    | Table 2: Instruction Set |  |

    |--------------------------|--|

    |--------------------------|--|


    <span id="page-3-0"></span>


    | Group              | Instruction            |                         |  |  |  |  |

    |--------------------|------------------------|-------------------------|--|--|--|--|

    |                    | ADD.TYPE Rd,Ra,Rb      | Rd = Ra + Rb            |  |  |  |  |

    |                    | SUB.TYPE Rd,Ra,Rb      | Rd = Ra - Rb            |  |  |  |  |

    | Integer Arithmetic | NEG.TYPE Rd,Ra         | Rd = -Ra                |  |  |  |  |

    |                    | ABS.TYPE Rd,Ra         | Rd = absolute(Ra)       |  |  |  |  |

    |                    | MUL16LO.TYPE Rd,Ra,Rb  | Rd = Ra * Rb            |  |  |  |  |

    |                    | MUL16HI.TYPE Rd,Ra,Rb  | Rd = (Ra * Rb)»16       |  |  |  |  |

    | Integer Multiply   | MUL24.LO.TYPE Rd,Ra,Rb | Rd = Ra * Rb            |  |  |  |  |

    |                    | MUL24.HI.TYPE Rd,Ra,Rb | Rd = (Ra * Rb)»24       |  |  |  |  |

    |                    | AND Rd,Ra,Rb           | Rd = Ra & Rb            |  |  |  |  |

    |                    | OR Rd,Ra,Rb            | Rd = Ra ∥ Rb            |  |  |  |  |

    |                    | XOR Rd,Ra,Rb           | Rd = Ra ⊕ Rb            |  |  |  |  |

    | Integer Logic      | NOT Rd,Ra              | Rd = !Ra                |  |  |  |  |

    |                    | cNOT Rd,Ra             | Rd = (Ra == 0)?1:0      |  |  |  |  |

    |                    | BVS Rd,Ra              | Rd = bit_reverse(Ra)    |  |  |  |  |

    |                    | SHL.TYPE Rd,Ra,Rb      | Rd = Ra ≪ Rb            |  |  |  |  |

    | Integer Shift      | SHR.TYPE Rd,Ra,Rb      | Rd = Ra ≫ Rb            |  |  |  |  |

    |                    | POP Rd,Ra              | Rd = unary(Ra)          |  |  |  |  |

    | Integer Other      | MAX.TYPE Rd,Ra,Rb      | Rd = (Ra>Rb)?Ra:Rb      |  |  |  |  |

    |                    | MIN.TYPE Rd,Ra,Rb      | Rd = (Ra<Rb)?Ra:Rb      |  |  |  |  |

    |                    | ADD.FP32 Rd,Ra,Rb      | Rd = Ra + Rb            |  |  |  |  |

    |                    | SUB.FP32 Rd,Ra,Rb      | Rd = Ra - Rb            |  |  |  |  |

    |                    | NEG.FP32 Rd,Ra         | Rd = -Ra                |  |  |  |  |

    | FP ALU             | ABS.FP32 Rd,Ra         | Rd = absolute(Ra)       |  |  |  |  |

    |                    | MUL.FP32 Rd,Ra,Rb      | Rd = Ra*Rb              |  |  |  |  |

    |                    | MAX.FP32 Rd,Ra,Rb      | Rd = (Ra>Rb)?Ra:Rb      |  |  |  |  |

    |                    | MIN.FP32 Rd,Ra,Rb      | Rd = (Ra<Rb)?Ra:Rb      |  |  |  |  |

    |                    | eq                     | 𝑅𝑎 == 𝑅𝑏                |  |  |  |  |

    |                    | ne                     | 𝑅𝑎 ≠ 𝑅𝑏                 |  |  |  |  |

    | Int Compare        | lt (INT), lo (UINT)    | 𝑅𝑎 < 𝑅𝑏                 |  |  |  |  |

    |                    | le (INT), ls (UINT)    | 𝑅𝑎 ≤ 𝑅𝑏                 |  |  |  |  |

    |                    | gt (INT), hi (UINT)    | 𝑅𝑎 > 𝑅𝑏                 |  |  |  |  |

    |                    | ge (INT), hs (UINT)    | 𝑅𝑎 ≥ 𝑅𝑏                 |  |  |  |  |

    | Memory             | LOD Rd (Ra)+offset     | Read from Shared        |  |  |  |  |

    |                    | STO Rd (Ra)+offset     | Write to Shared         |  |  |  |  |

    | Immediate          | LOD Rd #Imm            | Rd = Imm                |  |  |  |  |

    | Thread             | TDx Rd                 | Rd = Thread IDx         |  |  |  |  |

    |                    | TDy Rd                 | Rd = Thread IDy         |  |  |  |  |

    |                    | DOT Rd,Ra,Rb           | Dot Product ⟨𝑅𝑎, 𝑅𝑏⟩    |  |  |  |  |

    | Extension          | SUM Rd,Ra,Rb           | Reduction ⟨𝑅𝑎, 𝑅𝑏⟩<br>√ |  |  |  |  |

    |                    | INVSQR Rd,Ra           | 𝑅𝑑 = 1/<br>𝑅𝑎           |  |  |  |  |

    |                    | JMP address            | Jump to Address         |  |  |  |  |

    |                    | JSR address            | Subroutine Address      |  |  |  |  |

    |                    | RTS                    | Return from Subroutine  |  |  |  |  |

    | Control            | LOOP address           | Jump and Dec Loop Ctr   |  |  |  |  |

    |                    | INIT loops             | Set Loop Ctr            |  |  |  |  |

    |                    | STOP                   | Stop and Set Flag       |  |  |  |  |

    |                    | IF.cc                  | if cc true              |  |  |  |  |

    | Conditional        | ELSE                   | if cc false             |  |  |  |  |

    |                    | ENDIF                  | clear cc                |  |  |  |  |


    Table 3: Thread Space Control


    <span id="page-3-2"></span>


    | Coding | Width [4:3]             | Depth [2:1]          |  |  |  |

    |--------|-------------------------|----------------------|--|--|--|

    | "00"   | All (16 SPs)            | Wavefront 0 only     |  |  |  |

    | "01"   | 1/4 width (first 4 SPs) | all wavefronts       |  |  |  |

    | "10"   | SP0 only                | first 1/2 wavefronts |  |  |  |

    | "11"   | Undefined               | first 1/4 wavefronts |  |  |  |


    ## 5 RESULTS


    We compiled a number of different eGPU instances, using both DP and QP memory
    versions. We used Quartus Pro V22.4 and targeted an Intel Agilex AGIB027R29A1E1V
    device [\[7\]](#page-10-23). All of our results are reported for a single compilation
    attempt (we did not use seed sweeps).


    The DP memory results are tabulated in Table [4.](#page-6-0) We define three categories
    - small, medium, and large - to show the effects of different thread space, shared
    memory, and ALU features, as well as the impact of supporting predicates. The
    base eGPU architecture is the same for all instances: one SM with 16 SPs, a two
    read port register memory, and a four read and one write port shared memory. We
    configured all of these cases to use 512 threads, but with varying numbers of
    registers per thread. QP memory results are shown in Table [5,](#page-6-1) the
    main architectural change being the two write port shared memory.


    The ''small'' category uses a 16-bit ALU, which will likely only be used for address
    generation. The minimum specification supports only a single bit shift, as well
    as a 16-bit adder/subtractor, and arithmetic logic (AND/OR/XOR) operations. The
    memory requirements for the SPs is reduced by providing 16 registers per thread.
    The ''large'' category implements 64 registers per thread, and larger shared memory
    sizes with up to 128KB. The integer ALU supports the full set of integer instructions
    defined in the previous section. We also include a ''medium'' category for further
    examples. Many other combinations of parameters and features sets are possible
    as well.


    ## 5.1 Impact of Register and Shared Memory Size


    Both the thread registers and the shared memories are implemented using M20K memories,
    which can be configured into either DP (one read port and one write port active
    simultaneously) or QP memories (two read ports and two write ports active simultaneously).
    The natural datapath size of the eGPU is 32-bits, defined by the native IEEE 754
    single precision (FP32) floating point DSPs which will be doing the majority of
    the calculations. In DP mode, a M20K can be configured as a 512x32-bit memory.
    Port restrictions mean that in QP mode the M20K is a 2048x8-bit block, which requires
    a larger minimum thread register space to take advantage of the extra ports.


    In DP mode thread registers are implemented in two dual port memories, providing
    two read ports and one write port per clock cycle. In our most common eGPU configuration
    (with 16 registers per thread), a 512 thread machine will require two M20Ks per
    SP, or 32 M20Ks for thread registers in total, which is also the minimum size.
    Both the number of registers per thread and the number of total threads are parameterized,
    but the number of M20Ks will increase accordingly. In QP mode, the 8-bit data
    port width means that there is no point in using less than 2 thread registers
    per SP, although we will use half the number of M20K blocks compared to the DP
    version when we configure at least these number of registers.


    The shared memory is implemented as a four read port, one write port per memory
    in DP mode. The smallest possible shared memory is 512 words (2KB), which would
    require four M20Ks. This is very small, and unlikely to be useful, as the shared
    memory size would only be as large as the register space in a single SP. A more
    realistic shared memory size would be 2 words (8KB), which would require 16 M20Ks;
    the total memory usage for a small eGPU instance, including registers, would therefore
    be 48 M20Ks. The shared memory is set by parameter, and significantly larger sizes
    are possible without frequency impact. For example, a 64KB shared memory needs
    128 M20Ks, and a 128KB shared memory 256 M20Ks, which is a small fraction of the
    memories on the device. In QP


    mode, the number of M20Ks is halved, and the number of write ports doubled to
    two.


    ## 5.2 Integer ALU Architecture and Resources


    Unlike the floating point arithmetic, which can be mapped directly to a DSP Block,
    the simpler integer operations need to be implemented in soft logic. We will see
    that up to half of the soft logic and registers in an eGPU is required for the
    integer ALU. Table [6](#page-6-2) shows the resources, split by operation type,
    for a wide range of integer ALUs.


    The smallest reasonable integer ALU is a 16 bit version with single bit shifts,
    which consumes 90 ALMs and 136 registers, most of which are used for the 5 stage
    pipeline. Here we have a signed adder/subtractor, as well as logic functions (in
    this case, only AND, OR, and XOR are supported). The more typical full 16-bit
    ALU implementation supports signed and unsigned arithmetic, a more complete set
    of logic operations (AND/OR/XOR/NOT/cNOT/BVS), full 16-bit left and right shifts,
    population count, as well as max/min functions. The resource cost is approximately
    double that of the minimum ALU. The 5 stage pipeline 32-bit version again doubles
    the logic, as might be expected, but the number of registers triples, as individual
    functions (specifically the adder/subtractor and shifters) are themselves pipelined
    to ensure that the ALU always exceeds 800MHz. This contrasts with the 16-bit ALU,
    where the pipelining is used to improve the placement of the entire ALU, rather
    than improving the performance of any individual function. There is also a 4 stage
    pipeline version of the 32-bit integer ALU, which is about the size of the 16-bit
    full function ALU. This returns a lower performance (typically 700 MHz), and is
    used in order to save logic for the QP version of the eGPU (which has a lower
    target speed of 600MHz). The individual resource counts in Table [6](#page-6-2)
    may not accurately reflect the impact of each function to the overall ALU size,
    as synthesis may combine aspects of some functions together.


    ## 5.3 Predicate Resources


    In Table [4](#page-6-0) and [5](#page-6-1) the area impact of predicate support
    is clearly visible, increasing the soft logic resources by about 50%. While each
    predicate stack (including its control) is very small, each thread has a unique
    stack. The base predicate area consists of only a thread comparator (which checks
    that the SP currently executing the thread that the predicate circuit is associated
    with), an instruction decode (IF/ELSE/ENDIF), and the single bit-wide predicate
    stack. This may only be 5 ALMs per thread, but if a typical eGPU contains 1 threads,
    the predicate circuitry can quickly grow to be as large as the rest of the soft
    logic. Increasing the stack depth will have only a minimal impact on area, as
    each additional level consists of only a two input mux and a register.


    ## 5.4 Instruction Fetch, Decode, and Control


    This section will always have a modest footprint, requiring 200 to 250 ALMs, and
    a handful of M20Ks to store the instruction words. The instruction decoder takes
    about 40 ALMs, and the thread generator around 25 ALMs. A single M20K can store
    512 40-bit instruction words; the benchmarks we analyse later in this paper range
    from


    30 instructions (32 element reduction) to 250 instructions (256 element bitonic
    sort), so a multi-tenancy of programs would only need several M20Ks.


    Increasing the IW to 43 or 46 bits (which is required to support a 32 and 64 registers
    per thread, respectively), adds only a single M20K per 2 instructions, as the
    M20K containing the upper bits would be configured in 8 format. In any case, the
    number of M20Ks needed for program storage is small compared to the thread registers
    and shared memory. For example, a 1 word program space would require three M20Ks,
    and a 4 program space nine M20Ks.


    ## 5.5 Calculating Resources and Setting Performance


    Although the eGPU has a parameterized pipeline depth between the SPs and shared
    memory, it can achieve the target performance (771MHz and 600MHz respectively)
    using the minimum depth of 8 stages. The parameterized pipelining can be used
    for future applications with larger shared memories, or when the shared memories
    are placed elsewhere on the device, and not located near the SP array. We also
    report the slowest path outside the embedded (M20K and DSP) resources (see Table
    [4](#page-6-0) and [5\)](#page-6-1). If needed, there are also additional pipelining
    parameters inside the SP for the paths both to and from the FP and Integer ALUs.
    We will show in the next section how additional pipelining may not improve Fmax
    as the eGPU has been designed to fit into an Agilex sector in the base configuration.


    We can see that the SP overhead (mux and control) is ≈150 ALMs, the integer ALU
    ranges from ≈100 ALMs to ≈400 ALMs, and the predicates, if used, start from ≈
    150ALMs. A single SP will therefore be as small as 250 ALMs, and can be as large
    as 650 ALMs; this translates into a small eGPU core (16 SPs) requiring 4 ALMs,
    and over 10 ALMs for fully featured example.


    The number of M20Ks for the register memory for the DP eGPU can be calculated
    as threads×registers/256; for the shared memory the number of blocks is 2 × size().
    The number of M20K blocks required for the QP eGPU are half of this, except that
    there is a minimum size (threads × registers\_per\_thread/16 > 2047) for the number
    of registers, in which case the QP eGPU will need the same number of register
    blocks as the DP version.


    ## 5.6 FPGA Sector Resources and Impact


    It is most beneficial to select eGPU parameters around the available FPGA resources
    and their on-chip organization. The Intel Agilex devices are arranged in sectors,
    the most common of which contains about 16400 ALMs, 240 M20K memories, and 160
    DSP Blocks. Although we are not limited to a single sector (additional pipelining
    may be required to maintain performance across sector boundaries), this ratio
    of resources provides a good guide how to parameterize a eGPU instance. In particular,
    creating too large a register or memory space will be inefficient, as the ALMs
    between the M20K columns will likely be unreachable by other designs in the FPGA.
    Likewise, there is no point in specifying a small register space or shared memory,
    as the M20Ks between the logic structures may not be accessible by other functions.
    Further analysis is provided in the following section where we demonstrate that
    by selecting parameters in this way, the eGPU consistently achieves the reported


    performance levels by matching its architecture with the sector structure.


    ## 6 REPEATABLE HIGH PERFORMANCE


    This section provides the required information to make our design process repeatable
    for those wishing to achieve high performance in their own designs. It is therefore
    necessarily ''close to metal'' in abstraction. Although the details are specific
    to Intel FPGAs, we believe the same approaches are valid for all other FPGAs as
    well.


    The eGPU is designed to give consistent performance, which will always be limited
    by the slowest embedded (DSP or M20K memory) resource in that configuration. The
    clock network in Agilex is specified at 1GHz, which is the absolute limit of performance
    for any design. The M20K memories in DP mode also achieve 1GHz, but only 600 MHz
    in QP mode. The DSP Blocks can run at 771 MHz when implementing a FP32 multiply-add
    datapath with a 4 stage pipeline [\[11\]](#page-10-24). We are therefore limited
    to a maximum speed of 771MHz, unless we use QP memory, in which case the maximum
    frequency drops to 600MHz. The lower performance of the QP memory, however, will
    allow us to support a higher density storage, and the doubled write bandwidth
    may offer an overall higher throughput for some applications. We will examine
    some of these trade-offs in the benchmarking section.


    Using the sector architecture effectively enables the eGPU performance and efficiency.
    Sector resources are arranged in columns, each approximately 41 rows high (several
    columns are shorter because of non-user accessible device features). Achieving
    a 1GHz speed for soft logic does not require logic to be immediately adjacent
    to each other, as there are different horizontal and vertical wire lengths - too
    much pipelining can negatively impact performance as much as too little. More
    important is using the minimal number of wire resources per connection. In the
    Agilex devices, there is a constant 4 columns of logic between each column of
    either DSP or M20K. In a sector we will have 40 columns of logic, 4 columns of
    DSP, and 6 columns of M20K. There is little point in saving logic or memory if
    it is not accessible by other portions of design.


    As we have shown in the previous section, the results are deterministic and repeatable,
    in both area and performance. Ideally, the resource use would be balanced to realize
    the maximum efficiency from the device.


    To map eGPU to the device, we first sketched out a LUT level construct of an SP,
    and adjusted it so that the number of logic levels would align with the sector
    column ratios described above. Paths directly between M20K memories (which implement
    the thread registers in each SP) and the DSP Blocks had to fit into a 4 column
    group of LABs, and longer pure logic paths (e.g. the integer ALUs) were organized
    so that the total area did not spill over into a M20K or DSP column that might
    be used by another SP.


    We can see from the results (Table [6\)](#page-6-2) that a 16-bit Integer ALU
    is in the range of 100-200 ALMs and the 32-bit version requires 200- 400 ALMs.
    If predicates are used, they will cost an additional 125-250 ALMs per SP, depending
    on the defined thread space. The remaining 150 ALMs per SP are used for the data
    muxing and alignment shown in Figure [1.](#page-2-0) We were able to implement
    a small eGPU (the first example in Table [4\)](#page-6-0) that was able to close
    timing over 771 MHz with no dedicated soft logic registers (i.e. registers that
    were not


    <span id="page-6-0"></span>


    | Scale  | ALU<br>Precision | Shift<br>Precision | Threads | Reg.<br>Thread |
    Shared<br>Memory | Predicate<br>Levels | ALM   | Registers | DSP | M20K | Freq<br>(MHz)
    | SP<br>(ALM/Reg.) |

    |--------|------------------|--------------------|---------|----------------|------------------|---------------------|-------|-----------|-----|------|---------------|------------------|

    | Small  | 16               | 1                  | 512     | 16             |
    8KB              | 0                   | 4243  | 13635     | 24  | 50   | 1018/771      |
    224/707          |

    | Small  | 16               | 16                 | 512     | 16             |
    32KB             | 5                   | 7518  | 18992     | 24  | 98   | 898/771       |
    413/979          |

    | Medium | 16               | 16                 | 512     | 32             |
    32KB             | 5                   | 7579  | 19155     | 24  | 131  | 883/771       |
    426/1043         |

    | Medium | 32               | 16                 | 512     | 32             |
    32KB             | 5                   | 9754  | 25425     | 24  | 131  | 902/771       |
    461/1277         |

    | Large  | 32               | 16                 | 512     | 64             |
    32KB             | 8                   | 10127 | 26040     | 32  | 195  | 860/771       |
    575/1505         |

    | Large  | 32               | 32                 | 512     | 64             |
    64KB             | 16                  | 10697 | 26618     | 32  | 259  | 841/771       |
    600/1476         |


    #### Table 4: Fitting Results - DP Memory


    #### Table 5: Fitting Results - QP Memory


    <span id="page-6-1"></span>


    | Scale  | ALU       | Shift     | Threads | Regs./ | Shared | Predicate | ALM   |
    Registers | DSP | M20K | Freq    | SP         |

    |--------|-----------|-----------|---------|--------|--------|-----------|-------|-----------|-----|------|---------|------------|

    |        | Precision | Precision |         | Thread | Memory | Levels    |       |           |     |      |
    (MHz)   | (ALM/Reg.) |

    | Small  | 32        | 1         | 512     | 64     | 32KB   | 0         | 5468  |
    14487     | 24  | 98   | 840/600 | 287/830    |

    | Medium | 32        | 32        | 1024    | 32     | 64KB   | 0         | 7057  |
    16722     | 32  | 131  | 763/600 | 396/1016   |

    | Large  | 32        | 32        | 1024    | 32     | 64KB   | 16        | 11314
    | 25050     | 32  | 131  | 763/600 | 685/1601   |

    | Large  | 32        | 32        | 1024    | 32     | 128KB  | 10        | 10174
    | 23094     | 32  | 195  | 714/600 | 556/1391   |


    Table 6: Fitting Results - Integer ALU


    <span id="page-6-2"></span>


    | Prec. | Type  | ALM | Registers | Add/<br>Sub | Logic | SHL | SHR | Pop |

    |-------|-------|-----|-----------|-------------|-------|-----|-----|-----|

    | 16    | Min   | 90  | 136       | 3           | 9     | -   | -   | -   |

    | 16    | Small | 134 | 207       | 9           | 10    | 20  | 23  | -   |

    | 16    | Full  | 199 | 269       | 9           | 18    | 20  | 23  | 11  |

    | 32    | Min   | 208 | 406       | 5           | 27    | 28  | 28  | -   |

    | 32    | Full  | 394 | 704       | 27          | 36    | 50  | 53  | 27  |


    directly connected with a logic function, such as a mux), but for the generic
    parameterized case, we added a single additional pipeline stage register between
    the thread registers and the functional units, and also one level in the write-back
    path between the functional units and the thread registers. For all of the examples
    in Table [4](#page-6-0) there are also single pipeline stages to and from the
    shared memory. We parameterized the pipeline depth for all of these stages, along
    with the appropriate balancing delays for the data and control paths into the
    write paths of the thread registers, but found that these were not needed to be
    increased beyond one pipeline stage for any of the reported examples.


    Figure [4](#page-7-0) shows the unconstrained placement of the largest instance
    of Table [4.](#page-6-0) The shared memory and 8 out of the 16 SPs have been color
    coded for identification. The shared memory creates a spine in the middle of the
    core, with 8 SPs placed on either side of it. For purposes of illustration we
    have colored a subset of SPs: three random SPs and the left of the spine, and
    five contiguous ones on the right. Three things are evident with all SPs: (a)
    the majority of the logic is in one contiguous block, (b) there is a separate
    contiguous structure (the predicate block) placed some distance away, and (c),
    the SP straddles a columns of DSP Blocks. All of the instances of Table [4](#page-6-0)
    and [5](#page-6-1) display this pattern, including the shared memory spine.


    Figure [5](#page-7-1) shows one of the SPs in greater detail (this SP is the one
    marked by the black boxes in Figure [4\)](#page-7-0). The largest component is
    the integer ALU. The operators (adder/subtractor, shifters, arithmetic logic,
    etc.) are in the 4 columns to the right of the two DSP blocks (the DSP Block for
    the floating point operators is adjacent to the integer multiplier). To the left
    of the DSP Blocks is largely pipelining logic - of the 5 pipeline stages in the
    ALU, only one is used for pipelining the operators - the rest is used to break
    up the paths between the thread register memories and the ALUs. We examined all
    of the SP placements, and the placement of the M20Ks for the register memories
    (8 M20Ks for this instance) was in one of three layouts: (a) a contiguous single
    column (b) most of the registers in one column, with a smaller number in the next
    column further away from the integer ALU, and (c) equally split between two columns
    on either side of the integer ALU. In all of these cases, the pipeline wrapper
    around the ALU was usually grouped together, and essentially separate from the
    actual operators. Rather than having to be in a specific location relative to
    the M20Ks and operators, the ability to split up a bus so that it can be mapped
    to the same number of wire hops is what was important. Fewer pipeline stages would
    have introduced a two stage routing path, which would have likely become the critical
    path in the eGPU. (In the QP memory version, we can remove one of the pipeline
    stages as the M20K becomes the slowest component at 600 MHz, and we can see that
    the removal of some of the pipeline path reduce the non-memory path performance
    to just over 700 MHz). On the other hand, more than a 5 stage integer ALU could
    potentially decrease performance as it could spread out the placement of the SP.


    The predicate circuitry is placed in another contiguous block, but well away from
    the SP core it is associated with. From Figure [4](#page-7-0) we can see that
    the majority of the other predicates have a similar relationship with their respective
    SP. All of these have been automatically placed by Quartus. This is possible because
    the interface to and from the predicate block is very narrow, with only a single
    bit (thread\_active signal) returned. The signals to the block are relatively
    few: a thread index (typically 5 to 8 bits wide), a 3-bit decoded instruction
    signal (IF/ELSE/ENDIF), and a single bit valid condition code. Although there
    are many possible conditions from many different instructions, these can be decoded
    into a single


    <span id="page-7-0"></span>![](_page_7_Figure_1.jpeg)


    Figure 4: eGPU Placement


    valid condition bit in the main SP body. These narrow busses give us flexibility
    to wrap multiple pipes around the relatively simple (consisting largely of a chained
    registers organized in individual stacks) predicate blocks, which makes it possible
    for the tool to place them almost completely independently of the main datapaths.


    To create repeatable high performance designs, we need to understand both the
    structure, and the position of embedded features to each other. Here we are using
    integer ALUs which range in size by four times, our logic and memory density is
    very high, but our performance always exceeds that of the slowest embedded feature.
    It is possible to build a completely different type of CPU (or indeed any other
    type of core) and achieve this type of performance via a push button flow, but
    the architecture of the FPGA needs to be considered at every stage of the IP architecture
    phase.


    ## 7 BENCHMARKS


    We ran a number of workloads of different types to evaluate absolute and relative
    performance of the eGPU for varying data sizes that we might expect for embedded
    applications. We also profiled all the workloads to examine the efficiency of
    the eGPU. For continuity we selected many of the same benchmarks as used by Flexgrip
    [\[16\]](#page-10-25). We chose vector reduction, matrix transpose, and matrix-matrix
    multiply (MMM), as these would be common building blocks for many GPGPU applications.
    Bitonic sort [\[19\]](#page-10-26) is a sorting algorithm suited for parallel
    processing. Instead of the simpler autocorrelation, we used the FFT, as we felt
    this would be more representative of the


    <span id="page-7-1"></span>![](_page_7_Figure_7.jpeg)


    Figure 5: Single SP Placement


    workloads expected for the eGPU. All benchmarks were written in assembly code
    (we have not written our compiler yet).


    We report the comparison to FlexGrip only for the MMM, as the larger dataset size
    would be less affected by any overheads for setup and data transfer. We see that
    there is a significant performance advantage in favor of eGPU in cycle time alone.
    We ran all benchmarks, except the FFT, for which there are no reported FlexGrip
    results. FlexGrip underperforms eGPU by a factor of ≈31x, averaged over all benchmarks.
    We did not compare against DO-GPU (which is the latest iteration of FGPU), as
    DO-GPU normalized size is 50x-100x greater than eGPU.


    Our reported measurements are all based on core performance: we start the clock
    once the data has been loaded into the shared memory, and stop the clock once
    the final result has been written back to the shared memory. The most likely use
    of the eGPU is to apply multiple algorithms to the same data - .. there is no
    loading and unloading of data between different algorithms. For completeness,
    we also ran all of our benchmarks taking into account the time to load and unload
    the data over the 32-bit wide data bus. The performance impact was only 4.7%,
    averaged over all benchmarks.


    Clock frequency was 771 MHz for eGPU (including where the Dot Product operator
    is used), and 600 MHz for eGPU-QP variant. We compare both cycle counts and elapsed
    time for the eGPUs with the two shared memory architectures, and also the impact
    of the optional Dot Product core for reduction and MMM benchmarks. We then normalize
    the performance (time), by the resource cost, which we calculated on the basis
    of ALMs and DSP Blocks. We estimate that the effective cost of a DSP block is
    100 ALMs, which we calculate as follows: we start with the ALM count of the pure
    soft logic implementation of a FP32 multiply and adder (approximately 650 ALMs
    [\[10\]](#page-10-27)), and add 50% area to this number for DSP Block overhead
    (a DSP Block contains considerable additional features). We then divide by 10
    for an approximate soft logic to hard logic


    <span id="page-8-1"></span><span id="page-8-0"></span>


    |                                         |                                             |
    Vector Reduction               |                |                  | Matrix Transpose   |                      |                      |
    Matrix x Matrix          |                             |                                       |              |                            |                                             |

    |-----------------------------------------|---------------------------------------------|--------------------------------|----------------|------------------|--------------------|----------------------|----------------------|--------------------------|-----------------------------|---------------------------------------|--------------|----------------------------|---------------------------------------------|

    | Dimension                               | Metric                                      |
    Nios                           | eGPU           | eGPU             | eGPU               |
    Nios                 | eGPU                 | eGPU                     | Nios                        |
    FlexGrip                              | eGPU         | eGPU                       |
    eGPU                                        |

    |                                         |                                             |                                |
    DP             | QP               | Dot                |                      |
    DP                   | QP                       |                             |                                       |
    DP           | QP                         | Dot                                         |

    |                                         | Cycles                                      |
    459                            | 168            | 160              | 62                 |
    21809                | 1720                 | 1208                     | 1.45M                       |
    2.14M                                 | 111546       | 103354                     |
    19800                                       |

    |                                         | Time(us)                                    |
    1.32                           | 0.22           | 0.27             | 0.08               |
    62.85                | 2.23                 | 2.01                     | 4179                        |
    21400                                 | 144.7        | 172.3                      |
    25.7                                        |

    | 32                                      | Ratio(cycles)                               |
    2.73                           | 1.0            | 0.95             | 0.37               |
    12.68                | 1.0                  | 0.7                      | 13.03                       |
    19.2                                  | 1.0          | 0.93                       |
    0.18                                        |

    |                                         | Ratio(time)                                 |
    6.01                           | 1.0            | 1.23             | 0.37               |
    28.18                | 1.0                  | 0.9                      | 28.97                       |
    147.9                                 | 1.0          | 1.19                       |
    0.18                                        |

    |                                         | Normalized                                  |
    1.14                           | 1.0            | 1.4              | 0.45               |
    5.33                 | 1.0                  | 1.02                     | 5.48                        |
    -                                     | 1.0          | 1.35                       |
    0.21                                        |

    |                                         | Cycles                                      |
    1803                           | 202            | 194              | 94                 |
    86609                | 5529                 | 3481                     | 11.6M                       |
    16.6M                                 | 451066       | 418671                     |
    84425                                       |

    |                                         | Time(us)                                    |
    5.20                           | 0.26           | 0.32             | 0.12               |
    249.6                | 7.17                 | 5.80                     | 33383                       |
    166000                                | 585.0        | 697.8                      |
    109.5                                       |

    | 64                                      | Ratio(cycles)                               |
    8.93                           | 1.0            | 0.96             | 0.47               |
    15.66                | 1.0                  | 0.63                     | 25.7                        |
    36.8                                  | 1.0          | 0.93                       |
    0.19                                        |

    |                                         | Ratio(time)                                 |
    19.98                          | 1.0            | 1.23             | 0.47               |
    34.81                | 1.0                  | 0.81                     | 57.1                        |
    284                                   | 1.0          | 1.19                       |
    0.19                                        |

    |                                         | Normalized                                  |
    3.78                           | 1.0            | 1.4              | 0.60               |
    6.59                 | 1.0                  | 0.92                     | 10.80                       |
    -                                     | 1.0          | 1.35                       |
    0.23                                        |

    |                                         | Cycles                                      |
    3595                           | 216            | 208              | 101                |
    345233               | 20481                | 12649                    | 92.5M                       |
    441.2M                                | 2342356      | 2212136                    |
    886452                                      |

    |                                         | Time(us)                                    |
    10.36                          | 0.28           | 0.35             | 0.13               |
    994.91               | 26.56                | 21.08                    | 266491                      |
    4412.1                                | 3038.1       | 3686.9                     |
    1149.7                                      |

    | 128                                     | Ratio(cycles)                               |
    16.64                          | 1.0            | 0.96             | 0.47               |
    16.86                | 1.0                  | 0.62                     | 39.47                       |
    188.3                                 | 1.0          | 0.94                       |
    0.38                                        |

    |                                         | Ratio(time)                                 |
    37.00                          | 1.0            | 1.23             | 0.47               |
    37.45                | 1.0                  | 0.79                     | 87.71                       |
    1452                                  | 1.0          | 1.21                       |
    0.38                                        |

    |                                         | Normalized                                  |
    7.00                           | 1.0            | 1.4              | 0.60               |
    7.09                 | 1.0                  | 0.90                     | 1659                        |
    -                                     | 1.0          | 1.37                       |
    0.46                                        |

    |                                         |                                             |                                |                |                  |                    |                      |                      |                          |                             |                                       |              |                            |                                             |

    | Bitonic 64<br>Bitonic 32<br>Bitonic 128 | Bitonic 256<br>Bitonic 32QP<br>Bitonic
    64QP | Bitonic 128QP<br>Bitonic 256QP | FFT32<br>FFT64 | FFT128<br>FFT256 | FFT32QP<br>FFT64QP
    | FFT128QP<br>FFT256QP | Reduce32<br>Reduce64 | Reduce32Dot<br>Reduce128 | Reduce64Dot<br>Reduce128Dot
    | MMM 64x64<br>MMM 32x32<br>MMM 128x128 | MMM32x32QP   | MMM64x64QP<br>MMM128x128QP
    | MMM32x32Dot<br>MMM64x64Dot<br>MMM128x128Dot |

    | FP OP                                   | INT OP                                      |                                |
    IMM OP         |                  | Branch             | Load                 |                      |
    Save                     | Predicate                   |                                       |
    Thread Setup |                            | NOP                                         |


    Table 7: Vector and Matrix Benchmarks


    Figure 6: Benchmark Profiling (Y-Axis shows proportion of instructions executed
    by type).


    scaling factor (earlier work [\[26\]](#page-10-28) suggested a higher ratio in
    the general case, but recent work [\[27\]](#page-10-29) described more efficient
    ways of mapping arithmetic, especially multipliers, to FPGAs). We report normalized
    cost (considering both elapsed time and resources, with eGPU-DP as the baseline).


    As a comparison, we ran all of the benchmarks on Nios IIe [\[1\]](#page-10-30),
    which is a mature RISC processor for Intel FPGAs. The configuration we used consumed
    1100 ALMs (plus 3 DSP Blocks, giving a normalized cost of 1400), and closed timing
    at 347 MHz. We did not profile the Nios code, but analyzed the efficiency of operation
    (CPI). Most of the benchmarks retired an instruction every 1.7 clock cycles, except
    for the matrix-matrix multiplies and FFT, which required about 3 clocks, because
    of the way that 32×32 multipliers


    were implemented. (For simplicity, we replaced the FP32 arithmetic with INT32
    for the Nios examples).


    For the vector and matrix benchmarks, we chose an eGPU configuration with 32 registers
    per thread, with a 32 bit ALU, and a 128KB shared memory. This configuration has
    an equivalent cost (see Table [4](#page-6-0) and [5\)](#page-6-1) of 7400, 8400,
    and 9000 ALMs for the eGPU-DP, eGPU-QP, and eGPU-Dot variants respectively. Depending
    on the configuration, eGPU is 5× to 6× larger than Nios (but also more than twice
    the operating frequency). We would therefore expect (or at least hope) that eGPU
    would give an OOM performance increase over Nios.


    We can deduce the mechanism of the matrix transpose benchmarks from Table [7](#page-8-0)
    directly. For a given × matrix, we know that the eGPU will need 2 cycles to write
    the transposed elements


    to shared memory and 1/4th of those cycles to initially read them into the SP
    threads. We can see that the number of cycles clocked is marginally larger than
    this; these are largely used for the integer instructions needed to generate the
    transposed write addresses. We expect that the eGPU-QP will require about 40%
    fewer cycles, being able to write two transposed elements per clock, which indeed
    is the case.


    The vector reduction needs inter-SP communication, which go through the shared
    memory, which is the performance bottleneck in the eGPU. Table [7](#page-8-0)
    shows the impact of memory accesses on reduction performance. The actual floating
    point operations are a relatively small (≈10%) component of the reduction, with
    the majority of the cycles used by the memory operations. If we are using the
    dot product operator, there are even fewer FP operations required, and most of
    the time is spent waiting (NOPs) for the dot product to write back to the SP.
    All final vector reductions end up in the first SP, and we can use the multi-threaded
    CPU or MCU eGPU dynamic scaling personalities to write these values to the shared
    memory.


    The MMMs are much more complex. Although the algorithm itself is very simple,
    consisting only of a three level loop, the standard GPU implementation requires
    a vector reduction. While the cycle count increases as expected (∼ 4×) from 32×32
    to 64×64, there is an unexpected jump from 64×64 to 128×128, which is particularly
    evident in the eGPU-Dot case. Analysis of the code shows that while we are able
    to store the entire matrix (or at least a majority of the matrix) in the SP registers
    (there are 16384 total registers across the 16 SPs in the configuration we have
    chosen here) for the 32×32 and 64×64 cases, we need to keep reloading portions
    of the matrix in the 128×128 case, which can also be seen in the profile stack
    in Figure [6.](#page-8-1) Of course, there is always the option of increasing
    the maximum thread space or registers per thread (through parameterization) if
    the expected workloads were larger matrices. Compared to the vector reduction
    (where we profile a single vector), the thread initialization and integer operations
    are amortized away as we operate on many vectors. The NOPs also disappear as the
    the thread depth increases here.


    The bitonic sort benchmark requires a wider mix of instructions. Predicates are
    required, which increases the effective cost of the eGPU core by about 50%. The
    smaller sorts require many NOPs, which progressively reduce as the number of wavefronts
    increase for the larger datasets. The nature of the bitonic sort tends to use
    many subroutine calls, which we can see here in the relatively large number of
    branch operations. Again, the memory operations take the majority of all cycles,
    as each pass of the sort requires a redistribution of the data among the SPs.
    While the eGPU-QP version requires fewer clock cycles because of the increased
    write bandwidth, the normalized cost of the QP version is higher, largely because
    of the lower clock frequency.


    A similar pattern of instruction distribution is seen in the FFT. Increasing wavefront
    depth for larger datasets reduces NOPs significantly. The number of FP instructions
    (which are doing the actual FFT calculations) is relatively small, at about 10%.
    The largest proportion of operations are once again the memory accesses, especially
    in the write to shared memory; using the QP version of the eGPU results in a 20%
    to 30% decrease in total cycles. The normalized cost of the two eGPU versions,
    however, is approximately the


    Table 8: Bitonic Sort and FFT Benchmarks


    |     |               |        | Bitonic Sort |       | FFT    |      |            |  |  |

    |-----|---------------|--------|--------------|-------|--------|------|------------|--|--|

    | Dim | Metric        |        | eGPU         | eGPU  |        | eGPU | eGPU<br>QP
    |  |  |

    |     |               | Nios   | DP           | QP    | Nios   | DP   |            |  |  |

    |     | Cycles        | 8457   | 1742         | 1543  | 9165   | 876  | 714        |  |  |

    |     | Time(us)      | 24.37  | 2.25         | 2.51  | 26.41  | 1.14 | 1.19       |  |  |

    | 32  | Ratio(cycles) | 4.89   | 1.0          | 0.86  | 10.46  | 1.0  | 0.82       |  |  |

    |     | Ratio(time)   | 10.8   | 1.0          | 1.1   | 23.16  | 1.0  | 1.04       |  |  |

    |     | Normalized    | 1.24   | 1.0          | 1.24  | 4.38   | 1.0  | 1.18       |  |  |

    |     | Cycles        | 20687  | 3728         | 3054  | 20848  | 1695 | 1312       |  |  |

    |     | Time(us)      | 59.6   | 4.83         | 5.09  | 60.08  | 2.20 | 2.19       |  |  |

    | 64  | Ratio(cycles) | 5.54   | 1.0          | 0.82  | 12.30  | 1.0  | 0.82       |  |  |

    |     | Ratio(time)   | 12.3   | 1.0          | 1.05  | 27.31  | 1.0  | 1.01       |  |  |

    |     | Normalized    | 1.42   | 1.0          | 1.18  | 5.17   | 1.0  | 1.13       |  |  |

    |     | Cycles        | 49741  | 8326         | 6536  | 46667  | 3463 | 2558       |  |  |

    |     | Time(us)      | 143.3  | 10.8         | 10.9  | 134.49 | 4.29 | 4.26       |  |  |

    | 128 | Ratio(cycles) | 5.97   | 1.0          | 0.79  | 13.48  | 1.0  | 0.74       |  |  |

    |     | Ratio(time)   | 13.2   | 1.0          | 1.01  | 31.35  | 1.0  | 0.95       |  |  |

    |     | Normalized    | 1.48   | 1.0          | 1.13  | 5.93   | 1.0  | 1.08       |  |  |

    |     | Cycles        | 149271 | 16578        | 11974 | 103636 | 6813 | 4736       |  |  |

    |     | Time(us)      | 430.2  | 21.5         | 19.9  | 298.66 | 8.84 | 7.89       |  |  |

    | 256 | Ratio(cycles) | 9.0    | 1.0          | 0.72  | 15.21  | 1.0  | 0.70       |  |  |

    |     | Ratio(time)   | 20.0   | 1.0          | 0.93  | 33.79  | 1.0  | 0.89       |  |  |

    |     | Normalized    | 2.24   | 1.0          | 1.05  | 6.39   | 1.0  | 1.01       |  |  |


    same, with the high clock frequency of the base version offsetting the higher
    memory bandwidth of the QP version. These results also point to a better optimization
    for the FFT: by using a higher radix FFT, there will be correspondingly fewer
    passes through the shared memory. (We have a extensive flexibility in specifying
    the register and thread parameters, we can easily support much higher radices,
    which will require much larger register spaces).


    Comparing against Nios, we can see that the eGPU performs very well. We see at
    least an OOM performance difference based on time, and in almost all cases on
    a cycle basis as well. This tells us that eGPU is a more efficient architecture
    than a RISC processor, and is a viable candidate for a soft accelerator core.


    ## 8 CONCLUSIONS


    We have demonstrated a GPGPU that consistently beats 770 MHz for a wide range
    of parameters, and described the design approach required to reach such frequencies.
    We are able to swap in and out features as well as change the precision of the
    integer ALU to optimize for area and resource balancing in the FPGA.


    For the eGPU to be useful in an actual system design, it must offer an improvement
    over known methods. We compare the eGPU to a mature commercial soft CPU (Nios)
    over a number of benchmarks. The eGPU is much better on a cycle by cycle or elasped
    time basis in all cases we tried (typically by one to two OOM), and is still better
    on an area normalized basis. When we add the dot product core which can be used
    directly by the eGPU in a regular GPGPU context - the advantage can increase again
    by several times. A soft GPU therefore can offer a valid implementation option
    for many types of algorithms. This does not mean that a GPGPU will replace the
    RISC, anymore than a discrete GPGPU will replace a discrete RISC, only that we
    have shown that the soft GPGPU can now be considered for commercial designs, rather
    than just being of academic interest. The eGPU only uses 1%-2% of a current mid-range
    device, making it a cost effective option to implement complex algorithms in a
    larger FPGA system design, even if multiple cores are required.


    A Statically and Dynamically Scalable Soft GPGPU


    ## REFERENCES


    - <span id="page-10-30"></span>[1] 2016. Nios II Classic Processor Reference GuideNios
    II Classic Processor Reference Guide. [https://www.intel.com/content/www/us/en/docs/programmable/683620/](https://www.intel.com/content/www/us/en/docs/programmable/683620/current/overview-67435.html)
    [current/overview-67435.html.](https://www.intel.com/content/www/us/en/docs/programmable/683620/current/overview-67435.html)

    - <span id="page-10-0"></span>[2] 2017. FFT IP Core: User Guide. [https://www.intel.co.uk/content/www/uk/en/](https://www.intel.co.uk/content/www/uk/en/products/details/fpga/intellectual-property/dsp/fft.html)
    [products/details/fpga/intellectual-property/dsp/fft.html.](https://www.intel.co.uk/content/www/uk/en/products/details/fpga/intellectual-property/dsp/fft.html)

    - <span id="page-10-2"></span>[3] 2017. High-speed Reed-Solomon IP Core User Guide.
    [https://www.intel.com/](https://www.intel.com/content/www/us/en/docs/programmable/683120/17-1/about-the-high-speed-reed-solomon-ip-core.html)
    [content/www/us/en/docs/programmable/683120/17-1/about-the-high-speed](https://www.intel.com/content/www/us/en/docs/programmable/683120/17-1/about-the-high-speed-reed-solomon-ip-core.html)[reed-solomon-ip-core.html.](https://www.intel.com/content/www/us/en/docs/programmable/683120/17-1/about-the-high-speed-reed-solomon-ip-core.html)

    - <span id="page-10-4"></span>[4] 2018. Microblaze Processor Reference Guide.
    [https://docs.xilinx.com/v/u/2018.2-](https://docs.xilinx.com/v/u/2018.2-English/ug984-vivado-microblaze-ref)
    [English/ug984-vivado-microblaze-ref.](https://docs.xilinx.com/v/u/2018.2-English/ug984-vivado-microblaze-ref)

    - <span id="page-10-15"></span>[5] 2020. HB0919 Handbook CoreVectorBlox. [https://www.microsemi.com/existing](https://www.microsemi.com/existing-parts/parts/152678)[parts/parts/152678.](https://www.microsemi.com/existing-parts/parts/152678)

    - <span id="page-10-19"></span>[6] 2021. Block-by-Block Configurable Fast Fourier
    Transform Implementation on AI Engine (XAPP1356). [https://docs.xilinx.com/r/en-US/xapp1356-fft-ai-engine/FFT](https://docs.xilinx.com/r/en-US/xapp1356-fft-ai-engine/FFT-on-Multiple-AI-Engines)[on-Multiple-AI-Engines.](https://docs.xilinx.com/r/en-US/xapp1356-fft-ai-engine/FFT-on-Multiple-AI-Engines)

    - <span id="page-10-23"></span>[7] 2021. Intel Agilex7 FPGAs and SoCs F-Series:
    Product Table. [https://www.intel.](https://www.intel.com/content/dam/www/programmable/us/en/pdfs/literature/pt/intel-agilex-f-series-product-table.pdf)
    [com/content/dam/www/programmable/us/en/pdfs/literature/pt/intel-agilex-f](https://www.intel.com/content/dam/www/programmable/us/en/pdfs/literature/pt/intel-agilex-f-series-product-table.pdf)[series-product-table.pdf.](https://www.intel.com/content/dam/www/programmable/us/en/pdfs/literature/pt/intel-agilex-f-series-product-table.pdf)

    - <span id="page-10-1"></span>[8] 2022. Fast Fourier Transform v9.1. [https://www.xilinx.com/content/dam/xilinx/](https://www.xilinx.com/content/dam/xilinx/support/documents/ip_documentation/xfft/v9_1/pg109-xfft.pdf)
    [support/documents/ip\\_documentation/xfft/v9\\_1/pg109-xfft.pdf.](https://www.xilinx.com/content/dam/xilinx/support/documents/ip_documentation/xfft/v9_1/pg109-xfft.pdf)

    - <span id="page-10-3"></span>[9] 2022. Nios V Processor Reference Manual. [https://www.intel.com/content/www/](https://www.intel.com/content/www/us/en/products/details/fpga/nios-processor/v.html)
    [us/en/products/details/fpga/nios-processor/v.html.](https://www.intel.com/content/www/us/en/products/details/fpga/nios-processor/v.html)

    - <span id="page-10-27"></span>[10] 2023. Floating-Point IP Cores User Guide.
    [https://www.intel.com/content/www/](https://www.intel.com/content/www/us/en/docs/programmable/683750/23-1/about-floating-point-ip-cores.html)
    [us/en/docs/programmable/683750/23-1/about-floating-point-ip-cores.html.](https://www.intel.com/content/www/us/en/docs/programmable/683750/23-1/about-floating-point-ip-cores.html)

    - <span id="page-10-24"></span>[11] 2023. Intel Agilex 7 Variable Precision DSP
    Blocks. [https://www.intel.com/](https://www.intel.com/content/www/us/en/docs/programmable/683037/23-3/variable-precision-dsp-blocks-overview.html)
    [content/www/us/en/docs/programmable/683037/23-3/variable-precision-dsp](https://www.intel.com/content/www/us/en/docs/programmable/683037/23-3/variable-precision-dsp-blocks-overview.html)[blocks-overview.html.](https://www.intel.com/content/www/us/en/docs/programmable/683037/23-3/variable-precision-dsp-blocks-overview.html)

    - <span id="page-10-22"></span>[12] 2023. Intel Agilex7 Embedded Memory User Guide.
    [https://www.intel.com/](https://www.intel.com/content/www/us/en/docs/programmable/683241/23-2/embedded-memory-overview.html)
    [content/www/us/en/docs/programmable/683241/23-2/embedded-memory](https://www.intel.com/content/www/us/en/docs/programmable/683241/23-2/embedded-memory-overview.html)[overview.html.](https://www.intel.com/content/www/us/en/docs/programmable/683241/23-2/embedded-memory-overview.html)

    - <span id="page-10-18"></span>[13] 2023. Versal Adaptive SoC AI Engine Architecture
    Manual (AM009). [https:](https://docs.xilinx.com/v/u/en-US/wp506-ai-engine) [//docs.xilinx.com/v/u/en-US/wp506-ai-engine.](https://docs.xilinx.com/v/u/en-US/wp506-ai-engine)

    - <span id="page-10-5"></span>[14] Abdullah Al-Dujaili, Florian Deragisch, Andrei
    Hagiescu, and Weng-Fai Wong. 2012. Guppy: A GPU-like soft-core processor. In 2012
    International Conference on Field-Programmable Technology. 57–60.<https://doi.org/10.1109/FPT.2012.6412112>

    - <span id="page-10-6"></span>[15] Muhammed Al Kadi, Benedikt Janssen, and Michael
    Huebner. 2016. FGPU: An SIMT-Architecture for FPGAs. In Proceedings of the 2016
    ACM/SIGDA International Symposium on Field-Programmable Gate Arrays (Monterey,
    California, USA) (FPGA ''16). Association for Computing Machinery, New York, NY,
    USA, 254–263.<https://doi.org/10.1145/2847263.2847273>

    - <span id="page-10-25"></span>[16] Kevin Andryc. 2018. An Architecture Evaluation
    and Implementaiton of a Soft GPGPU for FPGAs. (2018).<https://doi.org/10.7275/12722172>

    - <span id="page-10-7"></span>[17] Kevin Andryc, Murtaza Merchant, and Russell
    Tessier. 2013. FlexGrip: A soft GPGPU for FPGAs. In 2013 International Conference
    on Field-Programmable Technology (FPT). 230–237.<https://doi.org/10.1109/FPT.2013.6718358>

    - <span id="page-10-8"></span>[18] Raghuraman Balasubramanian, Vinay Gangadhar,
    Ziliang Guo, Chen-Han Ho, Cherin Joseph, Jaikrishnan Menon, Mario Paulo Drumond,
    Robin Paul, Sharath Prasad, Pradip Valathol, and Karthikeyan Sankaralingam. 2015.
    Enabling GPGPU Low-Level Hardware Explorations with MIAOW: An Open-Source RTL
    Implementation of a GPGPU. ACM Trans. Archit. Code Optim. 12, 2, Article 21 (jun
    2015), 25 pages.<https://doi.org/10.1145/2764908>

    - <span id="page-10-26"></span>[19] K. E. Batcher. 1968. Sorting Networks and
    Their Applications. In Proceedings of the April 30–May 2, 1968, Spring Joint Computer
    Conference (Atlantic City, New Jersey) (AFIPS ''68 (Spring)). Association for
    Computing Machinery, New York,


    NY, USA, 307–314.<https://doi.org/10.1145/1468075.1468121>


    - <span id="page-10-16"></span>[20] Hui Yan Cheah, Fredrik Brosser, Suhaib A.
    Fahmy, and Douglas L. Maskell. 2014. The IDEA DSP Block-Based Soft Processor for
    FPGAs. ACM Trans. Reconfigurable Technol. Syst. 7, 3, Article 19 (sep 2014), 23
    pages.<https://doi.org/10.1145/2629443>

    - <span id="page-10-17"></span>[21] Hui Yan Cheah, Suhaib A. Fahmy, and Nachiket
    Kapre. 2014. Analysis and optimization of a deeply pipelined FPGA soft processor.
    In 2014 International Conference on Field-Programmable Technology (FPT). 235–238.
    [https://doi.org/10.](https://doi.org/10.1109/FPT.2014.7082783) [1109/FPT.2014.7082783](https://doi.org/10.1109/FPT.2014.7082783)

    - <span id="page-10-12"></span>[22] Christopher Han-Yu Chou, Aaron Severance,
    Alex D. Brant, Zhiduo Liu, Saurabh Sant, and Guy G. Lemieux. 2011. VEGAS: soft
    vector processor with scratchpad memory. In Proceedings of the ACM/SIGDA 19th
    International Symposium on Field Programmable Gate Arrays, FPGA 2011, Monterey,
    California, USA, February 27, March 1, 2011, John Wawrzynek and Katherine Compton
    (Eds.). ACM, 15–24. <https://doi.org/10.1145/1950413.1950420>

    - <span id="page-10-21"></span>[23] Jeffrey Chromczak, Mark Wheeler, Charles Chiasson,
    Dana How, Martin Langhammer, Tim Vanderhoek, Grace Zgheib, and Ilya Ganusov. 2020.
    Architectural Enhancements in Intel® Agilex™ FPGAs. In FPGA ''20: The 2020 ACM/SIGDA
    International Symposium on Field-Programmable Gate Arrays, Seaside, CA, USA, February
    23-25, 2020, Stephen Neuendorffer and Lesley Shannon (Eds.). ACM, 140–149.<https://doi.org/10.1145/3373087.3375308>

    - <span id="page-10-9"></span>[24] Pedro Duarte, Pedro Tomas, and Gabriel Falcao.
    2017. SCRATCH: An End-to-End Application-Aware Soft-GPGPU Architecture and Trimming
    Tool. In Proceedings of the 50th Annual IEEE/ACM International Symposium on Microarchitecture
    (Cambridge, Massachusetts) (MICRO-50 ''17). Association for Computing Machinery,
    New York, NY, USA, 165–177.<https://doi.org/10.1145/3123939.3123953>

    - <span id="page-10-10"></span>[25] Jeffrey Kingyens and J. Gregory Steffan. 2010.
    A GPU-inspired soft processor for high-throughput acceleration. In 2010 IEEE International
    Symposium on Parallel and Distributed Processing, Workshops and Phd Forum (IPDPSW).
    1–8. [https:](https://doi.org/10.1109/IPDPSW.2010.5470679) [//doi.org/10.1109/IPDPSW.2010.5470679](https://doi.org/10.1109/IPDPSW.2010.5470679)

    - <span id="page-10-28"></span>[26] Ian Kuon and Jonathan Rose. 2006. Measuring
    the gap between FPGAs and ASICs. In Proceedings of the ACM/SIGDA 14th International
    Symposium on Field Programmable Gate Arrays, FPGA 2006, Monterey, California,
    USA, February 22- 24, 2006, Steven J. E. Wilton and André DeHon (Eds.). ACM, 21–30.
    [https:](https://doi.org/10.1145/1117201.1117205) [//doi.org/10.1145/1117201.1117205](https://doi.org/10.1145/1117201.1117205)

    - <span id="page-10-29"></span>[27] Martin Langhammer and Gregg Baeckler. 2018.
    High Density and Performance Multiplication for FPGA. In 25th IEEE Symposium on
    Computer Arithmetic, ARITH 2018, Amherst, MA, USA, June 25-27, 2018. IEEE, 5–12.
    [https://doi.org/10.1109/](https://doi.org/10.1109/ARITH.2018.8464695) [ARITH.2018.8464695](https://doi.org/10.1109/ARITH.2018.8464695)

    - <span id="page-10-20"></span>[28] Martin Langhammer and George A. Constantinides.
    2023. eGPU: A 750 MHz Class Soft GPGPU for FPGA. In 2023 33rd International Conference
    on Field-Programmable Logic and Applications (FPL). 277–282. [https://doi.org/10.1109/](https://doi.org/10.1109/FPL60245.2023.00047)
    [FPL60245.2023.00047](https://doi.org/10.1109/FPL60245.2023.00047)

    - <span id="page-10-11"></span>[29] Rui Ma, Jia-Ching Hsu, Tian Tan, Eriko Nurvitadhi,
    Rajesh Vivekanandham, Aravind Dasu, Martin Langhammer, and Derek Chiou. 2021.
    DO-GPU: Domain Optimizable Soft GPUs. In 2021 31st International Conference on
    Field-Programmable Logic and Applications (FPL). 140–144. [https://doi.org/10.1109/FPL53798.2021.](https://doi.org/10.1109/FPL53798.2021.00031)
    [00031](https://doi.org/10.1109/FPL53798.2021.00031)

    - <span id="page-10-13"></span>[30] Aaron Severance and Guy Lemieux. 2012. VENICE:
    A compact vector processor for FPGA applications. In 2012 International Conference
    on Field-Programmable Technology, FPT 2012, Seoul, Korea (South), December 10-12,
    2012. IEEE, 261–268. <https://doi.org/10.1109/FPT.2012.6412146>

    - <span id="page-10-14"></span>[31] Aaron Severance and Guy G. F. Lemieux. 2013.
    Embedded supercomputing in FPGAs with the VectorBlox MXP Matrix Processor. In
    Proceedings of the International Conference on Hardware/Software Codesign and
    System Synthesis, CODES+ISSS 2013, Montreal, QC, Canada, September 29 - October
    4, 2013. IEEE, 6:1–6:10.<https://doi.org/10.1109/CODES-ISSS.2013.6658993>'
- title: '**1. Introduction**'
  abstract: ''
  keywords: ''
  document: "![](_page_0_Picture_0.jpeg)\n\n*Review*\n\n![](_page_0_Picture_1.jpeg)\n\
    \n**Kwame Nyako , Suman Devkota, Frank Li and Vamsi Borra [\\\\*](https://orcid.org/0000-0003-0348-2044)**\n\
    \nElectrical and Computer Engineering Program, Rayen School of Engineering, Youngstown\
    \ State University, Youngstown, OH 44555, USA\n\n**\\*** Correspondence: vsborra@ysu.edu\n\
    \n**Abstract:** The field of microelectronics has experienced extensive integration\
    \ into various aspects of our everyday lives, evident via its utilization across\
    \ a wide range of devices such as cellphones, airplanes, computers, wristwatches,\
    \ and other similar technologies. Microelectronics are vital to the healthcare\
    \ and defense industries, making them vulnerable to counterfeit products. Currently,\
    \ the complicated global microelectronics supply chain involves the production\
    \ of varied components in multiple places, resulting in tremendous risk. In this\
    \ scenario, it is possible for hostile or adversarial actors to exploit the situation\
    \ by intentionally introducing counterfeit components. This hostile be‑ havior\
    \ could steal data or use these components as remote kill switches. To address\
    \ these problems, enormous resources are being committed to research, innovation,\
    \ and development to build trust in microelectronics. This research study provides\
    \ a thorough analysis of the taxonomy associated with prominent attack, detection,\
    \ and avoidance models in the realm of counterfeit microelectronics. This research\
    \ aims to improve our understanding of dependable microelectronics. Prevention\
    \ strate‑ gies like Physical Unclonable Functions (PUFs) and machine learning\
    \ (ML), and detection methods like aging‑based fingerprints are reviewed in this\
    \ study. Finally, we underscore the significance of interdisciplinary cooperation,\
    \ commitment to norms, and proactive methods.\n\n**Keywords:** counterfeit; detection;\
    \ trust; microelectronics\n\n# **1. Introduction**\n\nCybersecurity plays a pivotal\
    \ and indispensable role in today's technological land‑ scape. Yet, cybercriminal\
    \ tactics continue to evolve, making their identification increas‑ ingly challenging.\
    \ Phishing, IoT hacks, and ransomware have resulted in substantial losses in the\
    \ tech industry. Thus, hardware security, which protects electronic gear throughout\
    \ production, has grown in popularity. Hardware security, akin to other security\
    \ domains, strives to shield hardware from threats that could compromise or obliterate\
    \ it [\\[1](#page-17-0)]. Ensur‑ ing 'Assurance' and 'Trust' in the context of\
    \ securing hardware systems translates to the confidence that electronic equipment\
    \ will perform as intended, free from the peril of com‑ promised components[[2\\\
    ]](#page-17-1).\n\nAs the global supply chain grows more intricate and the prevalence\
    \ of counterfeit components surges, it becomes paramount to verify the authenticity\
    \ of electrical chips. The infiltration of counterfeit components, potentially\
    \ finding their way into electronic equip‑ ment, raises concerns as workers contend\
    \ with mounting client demands[[3\\]](#page-17-2). National security, economic\
    \ stability, and individual privacy hang in the balance when hardware systems\
    \ lack adequate security. Reports from the Department of Defense reveal that over\
    \ a million components in military aviation and combat missiles have been identified\
    \ as coun‑ terfeit [\\[4](#page-17-3),[5\\]](#page-17-4).\n\nDuring the first\
    \ Iraq War in 1991, fighter planes were disabled by a secret activation code embedded\
    \ in the hardware [\\[6](#page-17-5)]. Experts believe that the presence of a\
    \ wicked electri‑ cal circuit that was remotely programmable and triggerable played\
    \ a part in aiding such\n\n![](_page_0_Picture_11.jpeg)\n\n**Citation:** Nyako,\
    \ K.; Devkota, S.; Li, F.; Borra, V. Building Trust in Microelectronics: A Comprehensive\
    \ Review of Current Techniques and Adoption Challenges. *Electronics* **2023**,\
    \ *12*, 4618. [https://doi.org/](https://doi.org/10.3390/electronics12224618)\
    \ [10.3390/electronics12224618](https://doi.org/10.3390/electronics12224618)\n\
    \nAcademic Editors: Wei Hu, Jiaji He and Haoqi Shan\n\nReceived: 13 October 2023\
    \ Revised: 7 November 2023 Accepted: 9 November 2023 [Published: 11 November 2023](https://creativecommons.org/)\n\
    \n![](_page_0_Picture_15.jpeg)\n\n**Copyright:** © 2023 by the authors. Licensee\
    \ MDPI, Basel, Switzerland. This article is an open access article distributed\
    \ under the terms and conditions of the Creative Commons Attribution (CC BY) license\
    \ [\\(https://](https://creativecommons.org/licenses/by/4.0/) [creativecommons.org/licenses/by/](https://creativecommons.org/licenses/by/4.0/)\
    \ 4.0/).\n\n![](_page_0_Picture_19.jpeg)\n\na catastrophic catastrophe. The Semiconductor\
    \ Industry Association (SIA) says that an‑ nual losses to manufacturers owing\
    \ to counterfeits total USD 7.5 billion [\\[7](#page-17-6)], amounting to around\
    \ 11,000 job losses in the United States[[8\\]](#page-17-7). Other sources assert\
    \ even higher losses, estimating annual sales losses of around USD 100 billion\
    \ to counterfeiting[[9,](#page-17-8)[10](#page-17-9)]. such a catastrophic catastrophe.\
    \ The Semiconductor Industry Association (SIA) says that annual losses to manufacturers\
    \ owing to counterfeits total USD 7.5 billion [7], amounting to around 11,000\
    \ job losses in the United States [8]. Other sources assert even higher losses,\
    \ estimating annual sales losses of around USD 100 billion to counterfeiting [9,10].\n\
    \nelectrical circuit that was remotely programmable and triggerable played a part\
    \ in aiding\n\n*Electronics* **2023**, *12*, x FOR PEER REVIEW 2 of 22\n\nTo combat\
    \ this menace, cutting‑edge strategies for detecting and preventing counter‑ feits\
    \ from infiltrating the market are of paramount importance [\\[11](#page-17-10)].\
    \ Figure [1](#page-1-0) illustrates the alarming increase in reported counterfeit\
    \ components between 2021 and 2022, a period during which worldwide semiconductor\
    \ sales remained relatively stable. To combat this menace, cutting-edge strategies\
    \ for detecting and preventing counterfeits from infiltrating the market are of\
    \ paramount importance [11]. Figure 1 illustrates the alarming increase in reported\
    \ counterfeit components between 2021 and 2022, a period during which worldwide\
    \ semiconductor sales remained relatively stable.\n\n<span id=\"page-1-0\"></span>![](_page_1_Figure_3.jpeg)\n\
    \n**Figure 1.** Reported counterfeits increased significantly between 2021 and\
    \ 2022 [12]. **Figure 1.** Reported counterfeits increased significantly between\
    \ 2021 and 2022 [\\[12](#page-17-11)].\n\nNeglecting hardware security and component\
    \ verification carries a spectrum of potential consequences, ranging from threats\
    \ to national security to severe economic repercussions, underscoring the criticality\
    \ of this facet of technology. The burgeoning presence of counterfeit components\
    \ in the global supply chain mandates substantial investments in advanced detection\
    \ technologies and concerted proactive initiatives. In this first section, we\
    \ adopt a review-style approach to investigate the intricacies between semiconductor\
    \ sales and the reporting of counterfeit parts in the industry. The aim is to\
    \ understand if an increasing trend in semiconductor sales correlates with the\
    \ rise in counterfeit parts. The objectives are to present the data, understand\
    \ the underlying patterns, and provide insights on the significance of these trends.\
    \ Neglecting hardware security and component verification carries a spectrum of\
    \ po‑ tential consequences, ranging from threats to national security to severe\
    \ economic reper‑ cussions, underscoring the criticality of this facet of technology.\
    \ The burgeoning presence of counterfeit components in the global supply chain\
    \ mandates substantial investments in advanced detection technologies and concerted\
    \ proactive initiatives. In this first section, we adopt a review‑style approach\
    \ to investigate the intricacies between semiconductor sales and the reporting\
    \ of counterfeit parts in the industry. The aim is to understand if an increasing\
    \ trend in semiconductor sales correlates with the rise in counterfeit parts.\
    \ The ob‑ jectives are to present the data, understand the underlying patterns,\
    \ and provide insights on the significance of these trends.\n\nBrief statistics\
    \ below underscore the issues. Figure 1 below indicates that while global semiconductor\
    \ sales remained steady from 2021 to 2022, reported counterfeit components surged\
    \ by 35 percent. Over the years, semiconductor sales have shown a definitive upward\
    \ trajectory. However, when juxtaposed with the total counterfeit parts reported,\
    \ the relationship is not immediately apparent. While there is a spike observed\
    \ between 2021 and 2022, a more substantial increase can be seen between 2010\
    \ and 2011. This suggests Brief statistics below underscore the issues. Figure\
    \ [1](#page-1-0) below indicates that while global semiconductor sales remained\
    \ steady from 2021 to 2022, reported counterfeit components surged by 35 percent.\
    \ Over the years, semiconductor sales have shown a definitive up‑ ward trajectory.\
    \ However, when juxtaposed with the total counterfeit parts reported, the relationship\
    \ is not immediately apparent. While there is a spike observed between 2021 and\
    \ 2022, a more substantial increase can be seen between 2010 and 2011. This suggests\
    \ that drawing conclusions based solely on the 2021–2022 data might be premature.\n\
    \nthat drawing conclusions based solely on the 2021–2022 data might be premature.\
    \ To offer a more comprehensive view, we calculated the Pearson's r correlation\
    \ between semiconductor sales and the total counterfeit parts reported. The results\
    \ indicate a Pearson's r of 0.01366, which shows a positive correlation between\
    \ semiconductor sales and counterfeit parts, albeit a weak one. The low correlation\
    \ can also be attributed to the To offer a more comprehensive view, we calculated\
    \ the Pearson's r correlation be‑ tween semiconductor sales and the total counterfeit\
    \ parts reported. The results indicate a Pearson's r of 0.01366, which shows a\
    \ positive correlation between semiconductor sales and counterfeit parts, albeit\
    \ a weak one. The low correlation can also be attributed to the fact that the\
    \ Pearson's r was computed from the average of yearly values. Other potential\
    \ drivers for this trend deviation include the following.\n\nThe initial impact\
    \ of the COVID‑19 pandemic led to many businesses downsizing or suspending their\
    \ operations, causing significant disruptions in global supply chains. However,\
    \ as the global economy slowly rebounds, businesses are returning to their pre‑\
    \ pandemic activities, resulting in a surge in demand for electrical components.\
    \ This height‑ ened demand could create opportunities for counterfeiters to exploit\
    \ weaknesses in the supply chain to meet the increased market needs. Additionally,\
    \ as pandemic‑related re‑ strictions ease, global supply networks are gradually\
    \ recovering and reopening, poten‑ tially enabling the cross‑border spread of\
    \ counterfeit parts. Moreover, the growing aware‑ ness of and reporting on counterfeit\
    \ component issues likely contribute to the observed rise in reported instances\
    \ of counterfeit electrical components infiltrating the market. To address the\
    \ risks associated with counterfeit components, stakeholders should invest in\
    \ ad‑ vanced detection and prevention technologies, establish industry standards,\
    \ and promote collaboration within the global supply chain. suspending their operations,\
    \ causing significant disruptions in global supply chains. However, as the global\
    \ economy slowly rebounds, businesses are returning to their prepandemic activities,\
    \ resulting in a surge in demand for electrical components. This heightened demand\
    \ could create opportunities for counterfeiters to exploit weaknesses in the supply\
    \ chain to meet the increased market needs. Additionally, as pandemic-related\
    \ restrictions ease, global supply networks are gradually recovering and reopening,\
    \ potentially enabling the cross-border spread of counterfeit parts. Moreover,\
    \ the growing awareness of and reporting on counterfeit component issues likely\
    \ contribute to the observed rise in reported instances of counterfeit electrical\
    \ components infiltrating the market. To address the risks associated with counterfeit\
    \ components, stakeholders should invest in advanced detection and prevention\
    \ technologies, establish industry standards, and promote collaboration within\
    \ the global supply chain. Figure 2 is a trend analysis of the most frequently\
    \ reported component types over the\n\nThe initial impact of the COVID-19 pandemic\
    \ led to many businesses downsizing or\n\n*Electronics* **2023**, *12*, x FOR\
    \ PEER REVIEW 3 of 22\n\ndrivers for this trend deviation include the following.\n\
    \nFigure [2](#page-2-0) is a trend analysis of the most frequently reported component\
    \ types over the past decade, which reveals interesting information about the\
    \ microelectronics market. One noteworthy tendency is the flattening of the capacitor\
    \ spike, which could indicate ei‑ ther an advance in capacitor dependability and\
    \ affordability or a shift in the counterfeiting community's focus. Since the\
    \ Electronics Research and Analysis Institute (ERAI) began tracking this data,\
    \ the demand for analog devices has expanded faster than any other type of component\
    \ in the last year. past decade, which reveals interesting information about the\
    \ microelectronics market. One noteworthy tendency is the flattening of the capacitor\
    \ spike, which could indicate either an advance in capacitor dependability and\
    \ affordability or a shift in the counterfeiting community's focus. Since the\
    \ Electronics Research and Analysis Institute (ERAI) began tracking this data,\
    \ the demand for analog devices has expanded faster than any other type of component\
    \ in the last year.\n\n<span id=\"page-2-0\"></span>![](_page_2_Figure_3.jpeg)\n\
    \n**Figure 2.** The most counterfeited semiconductors in 2022 compared with 10\
    \ and 5 years ago. **Figure 2.** The most counterfeited semiconductors in 2022\
    \ compared with 10 and 5 years ago.\n\nOther reasons for the recent rise of analog\
    \ devices are likely to include growing prices, improving technology, and even\
    \ sophisticated counterfeiting efforts. Stakeholders in the microelectronics sector\
    \ would do well to keep an eye on these trends and determine their root causes;\
    \ doing so would help in the creation of efficient methods for the identification,\
    \ prevention, and avoidance of counterfeit products. In addition, having an awareness\
    \ of these tendencies will aid in guaranteeing the security and dependability\
    \ of microelectronic components. Other reasons for the recent rise of analog devices\
    \ are likely to include growing prices, improving technology, and even sophisticated\
    \ counterfeiting efforts. Stakeholders in the microelectronics sector would do\
    \ well to keep an eye on these trends and determine their root causes; doing so\
    \ would help in the creation of efficient methods for the identification, prevention,\
    \ and avoidance of counterfeit products. In addition, having an awareness of these\
    \ tendencies will aid in guaranteeing the security and dependability of microelectronic\
    \ components.\n\nConsidering all the challenges above, we have adopted a review-style\
    \ methodology, to meticulously examine the literature and advancements in the\
    \ microelectronics domain. Additionally, we offer a thorough insight into the\
    \ challenges and ever-changing threats Considering all the challenges above, we\
    \ have adopted a review‑style methodology, to meticulously examine the literature\
    \ and advancements in the microelectronics domain. Additionally, we offer a thorough\
    \ insight into the challenges and ever‑changing threats and solutions concerning\
    \ trusted microelectronics. The research aims to provide an in‑ depth understanding\
    \ of the taxonomy of counterfeit attack, detection, and avoidance within the industry,\
    \ articulating the complexities and current developments. The objectives en‑\n\
    \ncompass tracing microelectronics' evolution and its modern relevance, understanding\
    \ ex‑ ternal impacts like the COVID‑19 pandemic on the sector's vulnerabilities,\
    \ emphasizing the escalating sophistication of counterfeit threats, addressing\
    \ ethical and security implica‑ tions from technology convergence, and forecasting\
    \ the future of microelectronics security with a focus on innovative solutions.\
    \ These objectives lay the groundwork for the subse‑ quent sections, ensuring\
    \ readers receive a coherent and insightful exploration of the topic. phasizing\
    \ the escalating sophistication of counterfeit threats, addressing ethical and\
    \ security implications from technology convergence, and forecasting the future\
    \ of microelectronics security with a focus on innovative solutions. These objectives\
    \ lay the groundwork for the subsequent sections, ensuring readers receive a coherent\
    \ and insightful exploration of the topic.\n\nand solutions concerning trusted\
    \ microelectronics. The research aims to provide an in-\n\ntives encompass tracing\
    \ microelectronics' evolution and its modern relevance, understanding external\
    \ impacts like the COVID-19 pandemic on the sector's vulnerabilities, em-\n\n\
    *Electronics* **2023**, *12*, x FOR PEER REVIEW 4 of 22\n\n#### <span id=\"page-3-1\"\
    ></span>**2. Counterfeit Attack Modes 2. Counterfeit Attack Modes**\n\nAs depicted\
    \ in Figure [3,](#page-3-0) counterfeit integrated circuits (ICs) are primarily\
    \ susceptible to four categories of attack mechanisms: software, hardware, network,\
    \ and information security. In the subsequent sections, we delve into these potential\
    \ attack techniques for counterfeit ICs, elaborating on the potential consequences\
    \ they may entail. As depicted in Figure 3, counterfeit integrated circuits (ICs)\
    \ are primarily susceptible to four categories of attack mechanisms: software,\
    \ hardware, network, and information security. In the subsequent sections, we\
    \ delve into these potential attack techniques for counterfeit ICs, elaborating\
    \ on the potential consequences they may entail.\n\n<span id=\"page-3-0\"></span>![](_page_3_Figure_4.jpeg)\n\
    \n**Figure 3.** Chart showing counterfeit attack modes and the potential consequences\
    \ of such attacks. **Figure 3.** Chart showing counterfeit attack modes and the\
    \ potential consequences of such attacks.\n\n#### *2.1. Software Security*\n\n\
    *2.1. Software Security*  As the significance of software security continues to\
    \ escalate, the role of trusted microelectronics in fortifying software applications\
    \ and ensuring their stability has gained paramount importance. Software security\
    \ revolves around the safeguarding of software applications against vulnerabilities\
    \ and attacks that could be exploited by counterfeit integrated circuits (ICs).\
    \ Counterfeit ICs provide malicious actors with the means to inject malicious\
    \ code, circumvent security protocols, or manipulate the functionality of software\
    \ applications. They can also facilitate software-based side-channel attacks and\
    \ privilege As the significance of software security continues to escalate, the\
    \ role of trusted mi‑ croelectronics in fortifying software applications and ensuring\
    \ their stability has gained paramount importance. Software security revolves\
    \ around the safeguarding of software applications against vulnerabilities and\
    \ attacks that could be exploited by counterfeit in‑ tegrated circuits (ICs).\
    \ Counterfeit ICs provide malicious actors with the means to inject malicious\
    \ code, circumvent security protocols, or manipulate the functionality of software\
    \ applications. They can also facilitate software‑based side‑channel attacks and\
    \ privilege escalation, thereby granting unauthorized access to sensitive data\
    \ or system resources.\n\nescalation, thereby granting unauthorized access to\
    \ sensitive data or system resources. Notably, the Plundervolt attack, elucidated\
    \ by [13], exploits the dynamic frequency and voltage scaling features of modern\
    \ CPUs, specifically targeting Intel SGX enclave operations. By manipulating the\
    \ processor's voltage, attackers can induce known faults in the processor package,\
    \ compromising security. This vulnerability can lead to the compromise of cryptographic\
    \ keys and the introduction of memory-safety vulnerabilities. The repercussions,\
    \ as outlined in [14], encompass unauthorized access to private data, system Notably,\
    \ the Plundervolt attack, elucidated by [\\[13](#page-17-12)], exploits the dynamic\
    \ frequency and voltage scaling features of modern CPUs, specifically targeting\
    \ Intel SGX enclave op‑ erations. By manipulating the processor's voltage, attackers\
    \ can induce known faults in the processor package, compromising security. This\
    \ vulnerability can lead to the compro‑ mise of cryptographic keys and the introduction\
    \ of memory‑safety vulnerabilities. The repercussions, as outlined in[[14\\]](#page-17-13),\
    \ encompass unauthorized access to private data, system instability, software\
    \ performance degradation, and reduced overall system security.\n\ninstability,\
    \ software performance degradation, and reduced overall system security. Furthermore,\
    \ speculative execution, a form of software security attack that occurs when the\
    \ CPU speculatively executes tasks it anticipates needing in the future without\
    \ explicit instruction [15], introduces potential risks. This approach eliminates\
    \ the need to Furthermore, speculative execution, a form of software security\
    \ attack that occurs when the CPU speculatively executes tasks it anticipates\
    \ needing in the future without explicit instruction[[15\\]](#page-17-14), introduces\
    \ potential risks. This approach eliminates the need to await the completion of\
    \ previous commands before executing new ones, thereby enhanc‑ ing speed by reducing\
    \ latency and increasing parallelism[[16\\]](#page-17-15). However, speculative\
    \ exe‑ cution can inadvertently execute potentially harmful programs, raising\
    \ security concerns.\n\nTo enhance software security and instill trust, Physically\
    \ Unclonable Functions (PUFs) have been proposed. PUFs are employed to generate\
    \ unique and unpredictable crypto‑ graphic keys for authentication and encryption,\
    \ constituting robust hardware‑based secu‑ rity mechanisms. Notable PUF projects\
    \ include the Arbiter PUF, which leverages differing data‑line delays [\\[17](#page-17-16)],\
    \ the Ring Oscillator PUF, which relies on variations in the frequencies of two‑ring\
    \ oscillators[[18\\]](#page-17-17), the SRAM PUF, exploiting idiosyncrasies in\
    \ SRAM‑cell startup behavior [\\[19](#page-17-18)], and the Memristor PUF, capitalizing\
    \ on the resistance‑changing properties of memristive devices [\\[20](#page-17-19)].\
    \ It is worth noting that PUFs have faced attacks, as elucidated in [\\[21](#page-17-20)],\
    \ where modeling attacks seek to emulate a PUF's behavior through mathematical\
    \ modeling. These attacks have been successfully executed with the aid of machine\
    \ learning tools such as Support Vector Machines (SVMs) and neural networks.\n\
    \n#### *2.2. Hardware Security*\n\nThe embedding of Hardware Trojans, backdoors,\
    \ and other malicious circuits within counterfeit integrated circuits (ICs) poses\
    \ a significant threat, endangering the security, confidentiality, and availability\
    \ of electronic systems [\\[22](#page-17-21)]. This underscores the critical need\
    \ for rigorous hardware security measures, including supply chain oversight, secure\
    \ manufacturing processes, and state‑of‑the‑art counterfeit detection technologies,\
    \ to fore‑ stall unauthorized access [\\[23](#page-17-22)], data breaches[[24\\\
    ]](#page-17-23), or system malfunctions[[25\\]](#page-17-24).\n\nHardware security\
    \ presents multifaceted challenges, encompassing vulnerabilities to a range of\
    \ attacks (e.g., side‑channel or Trojan attacks) at various layers (e.g., chip\
    \ or PCB), further complicating the landscape of hardware security. Concurrently,\
    \ hardware trust concerns stem from interactions with untrustworthy third parties\
    \ at any stage of a device's production and distribution, spanning from IP or\
    \ CAD tool providers to manufacturing facilities and warehouses.\n\nAmong common\
    \ hardware security breaches, Reverse Engineering Attacks, which aim to pilfer\
    \ a device's intellectual property and design details for illicit purposes such\
    \ as duplication or counterfeiting, are prominent[[26\\]](#page-18-0). These attacks\
    \ can be executed through methods like deprocessing, optical imaging, and circuit\
    \ extraction[[27\\]](#page-18-1). In contrast, fault injection attacks intentionally\
    \ induce system malfunctions to gain access to or control over the targeted system[[28\\\
    ]](#page-18-2), employing tools such as lasers, electromagnetic pulses, or temperature‑dependent\
    \ fault injections[[29\\]](#page-18-3).\n\nSide‑channel attacks focus on unintentional\
    \ data leakage from a device's physical im‑ plementation, encompassing aspects\
    \ like power consumption, electromagnetic radiation, or timing data[[30\\]](#page-18-4).\
    \ Techniques such as differential power analysis, simple power anal‑ ysis, and\
    \ correlation power analysis are employed to infer device behaviors and poten‑\
    \ tially extract sensitive information, such as encryption keys, from power consumption\
    \ pat‑ terns [\\[31](#page-18-5)].\n\nFurthermore, Hardware Trojans represent\
    \ malevolent hardware additions introduced during product assembly, serving as\
    \ latent security or functionality vulnerabilities that can be activated at a\
    \ later stage[[32\\]](#page-18-6).\n\n#### *2.3. Network Security*\n\nNetwork\
    \ attacks often manifest in the deployment of counterfeit network interface controllers\
    \ or routers, potentially leading to the theft of sensitive information, service\
    \ dis‑ ruptions, or the illicit takeover of networked devices via unauthorized\
    \ remote access [\\[33](#page-18-7)].\n\nIn safeguarding critical infrastructure,\
    \ the Internet of Things (IoT), and cloud‑based services, trusted microelectronics\
    \ play a central role [\\[34](#page-18-8)]. These components are instru‑ mental\
    \ in ensuring the privacy, integrity, and authenticity of stored data[[18\\]](#page-17-17),\
    \ commonly relying on cryptographic primitives and secure key storage.\n\nOne\
    \ particular area of scrutiny is spear‑phishing, an exceptionally targeted and\
    \ so‑ phisticated form of phishing attack that surpasses conventional phishing\
    \ attempts in terms of complexity and personalization. This issue is explored\
    \ extensively in a research pa‑ per authored by a single individual[[35\\]](#page-18-9).\
    \ The paper underscores the urgency for enter‑ prises to proactively counter the\
    \ escalating threat of spear‑phishing. To fortify themselves against sophisticated\
    \ cyberattacks, businesses are advised to prioritize user education, im‑ plement\
    \ robust security measures, and maintain a comprehensive and up‑to‑date incident\
    \ response plan.\n\n# *2.4. Information Security*\n\nIn addition to bypassing\
    \ security measures to gain unauthorized access to sensitive data, counterfeit\
    \ cryptographic integrated circuits also have the potential to disrupt encryp‑\
    \ tion or authentication methods. The advent of social media and cloud computing\
    \ has ne‑ cessitated a heavy investment by businesses in information security\
    \ in order to safeguard data. The Federal Communications Commission offers tips\
    \ to businesses for cybersecu‑ rity[[36\\]](#page-18-10). The CIA Triad, comprising\
    \ of Confidentiality, Integrity, and Availability, serves as a fundamental framework\
    \ within the field of information security. An all‑encompassing information security\
    \ strategy encompasses policies and security controls that effectively mitigate\
    \ risks to these three essential components.\n\nThe CIA triad serves as a comprehensive\
    \ framework for overseeing information secu‑ rity and is also valuable for effectively\
    \ managing research products and data.\n\n#### <span id=\"page-5-1\"></span>**3.\
    \ Counterfeit Detection Methods**\n\nNumerous previous research endeavors have\
    \ conducted comprehensive analyses and comparisons of both destructive and non‑destructive\
    \ techniques employed for the identi‑ fication of counterfeit integrated circuits\
    \ (ICs) [\\[37](#page-18-11)]. Destructive methods, including de‑ layering and\
    \ cross‑sectioning, offer profound insights into the internal structure of ICs,\
    \ uncovering potential anomalies or tampering. However, the use of these methodologies\
    \ often results in the degradation of the examined components. Conversely, non‑destructive\
    \ techniques such as X‑ray imaging, optical microscopy, and electrical testing\
    \ provide the means to scrutinize integrated circuits without causing any harm\
    \ to their structural in‑ tegrity. Consequently, these approaches are better suited\
    \ for conducting extensive screen‑ ings on a broader scale. This section underscores\
    \ the importance of integrating multiple detection techniques to enhance the accuracy\
    \ and effectiveness of counterfeit IC identifi‑ cation, thereby contributing to\
    \ the overall enhancement of security and reliability in elec‑ tronic systems.\
    \ A concise overview of the taxonomy of counterfeit detection strategies, along\
    \ with its subdivisions is provided. These are depicted in Figures [4](#page-5-0)\
    \ and [5](#page-6-0). *Electronics* **2023**, *12*, x FOR PEER REVIEW 7 of 22\n\
    \n<span id=\"page-5-0\"></span>![](_page_5_Figure_8.jpeg)\n\n**Figure 4.** Counterfeit\
    \ detection modes and subcategories [38]. **Figure 4.** Counterfeit detection\
    \ modes and subcategories[[38\\]](#page-18-12).\n\n**Figure 5.** Subdivisions\
    \ under physical inspection.\n\nTo identify imperfections, physical examinations\
    \ involve a meticulous assessment of the components' external attributes. These\
    \ examinations encompass both internal and external evaluations aimed at scrutinizing\
    \ the component's construction, packaging, and\n\n*3.1. Physical Inspections*\
    \ \n\n<span id=\"page-6-0\"></span>![](_page_6_Figure_2.jpeg)\n\n**Figure 4.**\
    \ Counterfeit detection modes and subcategories [38].\n\n**Figure 5.** Subdivisions\
    \ under physical inspection. **Figure 5.** Subdivisions under physical inspection.\n\
    \n## *3.1. Physical Inspections*\n\n*3.1. Physical Inspections*  To identify imperfections,\
    \ physical examinations involve a meticulous assessment of the components' external\
    \ attributes. These examinations encompass both internal and external evaluations\
    \ aimed at scrutinizing the component's construction, packaging, and To identify\
    \ imperfections, physical examinations involve a meticulous assessment of the\
    \ components' external attributes. These examinations encompass both internal\
    \ and external evaluations aimed at scrutinizing the component's construction,\
    \ packaging, and leads. External tests, such as low‑power visual inspection (LPVI)[[39\\\
    ]](#page-18-13), blacktop testing [\\[40](#page-18-14)[–42](#page-18-15)], microblast\
    \ analysis[[43–](#page-18-16)[45\\]](#page-18-17), hermiticity testing[[46–](#page-18-18)[48\\\
    ]](#page-18-19), scanning electron microscopy (SEM) [\\[49](#page-18-20)[–52](#page-19-0)],\
    \ and scanning acoustic microscopy (SAM) [\\[53](#page-19-1)[–55](#page-19-2)],\
    \ focus on as‑ sessing the component's exterior characteristics. The method of\
    \ low‑power visual examina‑ tion (LPVI) uses tools like microscopes, digital cameras,\
    \ or infrared light sources to inspect essential markings on package‑level electronics\
    \ and identify indications of previously used or recycled items, including marks\
    \ on the package or leftover solder on IC connectors. Fur‑ thermore, X‑ray visualization,\
    \ which is a non‑destructive testing technique, is applied to identify irregularities\
    \ within the internal components, dies, and connecting wires by com‑ paring them\
    \ to a standard component. Microblast testing is used to determine if markings\
    \ or scratches on reused or fake components have been intentionally erased using\
    \ a dry sand‑ blasting method. In contrast, internal assessments necessitate decapping\
    \ the component to expose its internal structure, which can then be subjected\
    \ to techniques like optical in‑ spection[[56–](#page-19-3)[58\\]](#page-19-4),\
    \ wire pull and die/ball shear[[59–](#page-19-5)[61\\]](#page-19-6). Essential\
    \ details about die markings, such as the company emblem, manufacturing date,\
    \ chip identification, and origin coun‑ try, among others, should be recorded.\
    \ Wire pull is used to check the bond's consistency with the die. If the component\
    \ has been used for an extended period, the bond between the die and bond wires\
    \ may weaken. By comparing the tension (or pulling strength) be‑ tween the standard\
    \ and examined components, one can ascertain if the component had prior usage.\
    \ The die shear method is employed to confirm the die attach's reliability, but\
    \ it is relevant only for sealed devices. The ball bond's robustness at the die\
    \ is assessed using a ball shear test. Scanning electron microscopy (SEM) captures\
    \ images of the die, package, or leads by scanning them with a concentrated electron\
    \ beam. This method is effective in detecting any irregularities present. With\
    \ a resolution that can reach a few nanome‑ ters, SEM allows for the detailed\
    \ analysis of the die, even down to its gate level. For an examination of the\
    \ material composition of the package, leads, and die, techniques such as X‑ray\
    \ fluorescence (XRF) [\\[62](#page-19-7)[–64](#page-19-8)], Fourier transform\
    \ infrared spectroscopy [\\[65](#page-19-9)[–67](#page-19-10)], and energy‑dispersive\
    \ spectroscopy[[68,](#page-19-11)[69](#page-19-12)] are employed. XRF Spectroscopy\
    \ is a non‑invasive technique used for material analysis. When a material is subjected\
    \ to intense X‑rays, its outer electrons are energized to higher, unstable orbits.\
    \ As these electrons revert to their original state, they emit radiation. This\
    \ emission is specific to each element, resulting in a distinctive spectral peak.\
    \ By using XRF, a unique signature from a component's package is obtained. The\
    \ component's authenticity is ascertained by comparing this signature to a reference\
    \ sample or, if accessible, the manufacturer's specifications. On the other hand,\
    \ Energy Dispersive Spectroscopy (EDS) is a technique that determines the chemical\
    \ prop‑ erties of a component by stimulating it with X‑rays. By directing a high‑powered\
    \ stream of charged particles onto the component's surface, X‑rays are emitted.\
    \ An X‑ray detector then captures this emission to produce the EDS spectrum. This\
    \ process yields a distinctive X‑ray signature based on the materials present\
    \ in the component's outer casing.\n\nLastly, Fourier Transform Infrared (FTIR)\
    \ Spectroscopy leverages the principles of infrared (IR) spectroscopy. When subjected\
    \ to IR radiation, a material will both absorb and transmit portions of it. The\
    \ captured IR radiation provides insights into molecular behaviors, both in terms\
    \ of absorption and transmission. Through this method, a specific molecular pattern\
    \ is derived, which can then be compared to a known reference or \"gold standard\"\
    \ for material verification. FTIR is versatile, suitable for examining both organic\
    \ and inorganic substances in a component. It involves validating a component's\
    \ specific ma‑ terials, such as polymers or coatings; spotting remnants from procedures\
    \ like sandblasting, which erase prior inscriptions; and detecting traces from\
    \ chemical methods. These meth‑ ods are commonly found in counterfeit parts, repurposed\
    \ from circuit boards or resulting from unsanctioned refurbishing processes.\n\
    \nA comprehensive physical inspection constitutes the initial step in thwarting\
    \ the infil‑ tration of counterfeit components. This systematic examination is\
    \ consistently applied to various categories of incoming components, regardless\
    \ of their condition, encompassing new, used, or aged components. The security\
    \ and reliability of electronic systems hinge upon this methodical approach to\
    \ detecting counterfeit parts.\n\n#### 3.1.1. Incoming Inspections\n\nFor counterfeit\
    \ integrated circuit (IC) detection, the physical inspection process ini‑ tiates\
    \ with inbound examinations. This procedure involves scrutinizing newly acquired\
    \ components to ensure their authenticity and quality before their integration\
    \ into electronic systems. Incoming inspections serve to identify anomalies, defects,\
    \ or potential signs of counterfeiting through a careful examination of the components'\
    \ physical attributes. Com‑ puter vision techniques play a pivotal role in addressing\
    \ hardware security challenges. Techniques like Keypoint Extraction using SIFT\
    \ and SURF, Image Segmentation, and Tem‑ plate Matching help in identifying and\
    \ analyzing various elements of printed circuit boards (PCBs) and integrated circuits.\
    \ With the evolution of deep learning and artificial neural networks (ANNs), feature\
    \ extraction has become more efficient, particularly with models like AlexNet,\
    \ ResNet, and Inception‑v3. Despite these advancements, computer vision‑ based\
    \ hardware security faces challenges, including the absence of large, labeled\
    \ datasets and the inherent noise and clutter in imagery, especially in high‑density\
    \ PCBs. To ad‑ dress these challenges, future research can consider multi‑modal\
    \ imaging, develop pub‑ licly available datasets, and apply deep learning earlier\
    \ in the computer vision pipeline. Collaborative efforts that combine hardware\
    \ design, imaging, computer vision, and ma‑ chine learning expertise are essential\
    \ for more holistic solutions. The complexity of con‑ temporary digital systems\
    \ presents challenges in verifying chip authenticity, prompting the authors Akter\
    \ et al. [\\[70](#page-19-13)] to advocate for the use of terahertz (THz) and\
    \ sub‑terahertz (sub‑THz) scanning combined with AI processing to detect counterfeit\
    \ Integrated Circuits (ICs) and assess their reliability. This technology, tested\
    \ on devices like the i7 microproces‑\n\nsor, uses unique THz signatures from\
    \ circuit pins to distinguish genuine from counterfeit chips. Using MATLAB‑based\
    \ software, the THz response data undergoes a multi‑step pro‑ cessing procedure,\
    \ with techniques like Hough transform applied for image classification. The research\
    \ suggests that this combined approach of THz scanning and AI processing can serve\
    \ as a significant tool for cybersecurity, ensuring the reliability and genuineness\
    \ of ICs in sectors vulnerable to counterfeit threats, such as defense and healthcare.\n\
    \nAnother use of machine learning in counterfeit IC detection proposed by Sukhwan\
    \ et al. [\\[71](#page-19-14)] uses simulated circuits to pinpoint temperatures\
    \ that best emphasize the dis‑ parities between genuine and counterfeit circuits.\
    \ By using RLC circuits, non‑inverting amplifier circuits, and the NSGA II algorithm,\
    \ the research establishes optimal testing con‑ ditions, notably in extreme temperatures,\
    \ that accentuate these differences. When tested on genuine Intel and counterfeit\
    \ Soviet clone circuits, the counterfeit circuits exhibited sig‑ nificant output\
    \ differences, especially in cold environments. This machine learning model is\
    \ adaptable for industrial‑grade counterfeit detection tools and offers a cost‑efficient\
    \ alter‑ native. In a study by Lu et al.[[67\\]](#page-19-10), the authors underscore\
    \ the potential of X‑ray imaging as a means to detect counterfeit and recycled\
    \ ICs. They suggest using a method based on deep learning to analyze X‑ray images\
    \ of integrated circuits in order to spot fakes. The au‑ thors describe Hardware\
    \ Trojans as malicious modifications in integrated circuits. These can include\
    \ simple changes like adding, removing, or altering circuit cells (like gates)\
    \ or their connections, as shown in Figure [6.](#page-8-0) When processing X‑ray\
    \ pictures, convolutional neural networks, also known as CNNs, are utilized, which\
    \ enables the automatic extrac‑ tion of distinguishing characteristics. *Electronics*\
    \ **2023**, *12*, x FOR PEER REVIEW 10 of 22\n\n<span id=\"page-8-0\"></span>![](_page_8_Figure_3.jpeg)\n\
    \n**Figure 6.** An SEM image of an IC layout showing **a2** being points of cell\
    \ insertion, **b2** being points of cell deletion, and **c2** being points of\
    \ cell replacement [67]. (Figure adopted with permission) **Figure 6.** An SEM\
    \ image of an IC layout showing **a2** being points of cell insertion, **b2**\
    \ being points of cell deletion, and **c2** being points of cell replacement[[67\\\
    ]](#page-19-10). (Figure adopted with permission).\n\nConsequently, these networks\
    \ categorize integrated circuits as either authentic or counterfeit/recycled.\
    \ The incorporation of deep learning in this technology holds the promise of surpassing\
    \ current manual inspection procedures in terms of accuracy, speed, and scalability.\
    \ Consequently, these networks categorize integrated circuits as either authentic\
    \ or counterfeit/recycled. The incorporation of deep learning in this technology\
    \ holds the promise of surpassing current manual inspection procedures in terms\
    \ of accuracy, speed, and scalability.\n\n#### 3.1.2. Exterior Tests 3.1.2. Exterior\
    \ Tests\n\nnents.\n\n3.1.3. Interior Tests\n\nBlacktop testing, as defined by\
    \ researchers in [38,72], involves inspecting the surface of the components for\
    \ discrepancies, such as the existence of a blacktop coating intended to disguise\
    \ remarking or other tampering, with the goal of discovering counterfeit ICs.\
    \ The longevity of the parts under scrutiny is determined by subjecting them to\
    \ a battery of different solvents. Hermeticity, as described in [73], necessitates\
    \ a special method of package assessment, especially for hermetically sealed components.\
    \ This evaluation ensures that the Hermetic Seal is intact and the product will\
    \ work as expected in the designated setting. When the circuit's performance is\
    \ at stake and great dependability is required, such as in the military or industry,\
    \ hermetic enclosures for integrated circuit packages are commonly used. Examining\
    \ a Fine Leak or a Gross Leak in a Hermetic Seal is a simple and low-cost approach\
    \ to check for seal failure. To analyze a Fine Leak Hermetic Seal, one can use\
    \ commercially available leak detection systems that use either helium or radioisotope\
    \ tracer gases. A vacuum is applied, the integrated circuit is pressurized for\
    \ a certain amount of time (allowing the gas to seep into the package cavity),\
    \ and then the released gas is detected. Gross Leak Hermetic Seal assessments,\
    \ on the other hand, may be readily carried out with just a bit of pressured air,\
    \ a vacuum system, a pressure chamber, and two Blacktop testing, as defined by\
    \ researchers in [\\[38](#page-18-12),[72\\]](#page-19-15), involves inspecting\
    \ the surface of the components for discrepancies, such as the existence of a\
    \ blacktop coating intended to disguise remarking or other tampering, with the\
    \ goal of discovering counterfeit ICs. The longevity of the parts under scrutiny\
    \ is determined by subjecting them to a battery of different solvents. Hermeticity,\
    \ as described in [\\[73](#page-19-16)], necessitates a special method of pack‑\
    \ age assessment, especially for hermetically sealed components. This evaluation\
    \ ensures that the Hermetic Seal is intact and the product will work as expected\
    \ in the designated setting. When the circuit's performance is at stake and great\
    \ dependability is required, such as in the military or industry, hermetic enclosures\
    \ for integrated circuit packages are commonly used. Examining a Fine Leak or\
    \ a Gross Leak in a Hermetic Seal is a simple and low‑cost approach to check for\
    \ seal failure. To analyze a Fine Leak Hermetic Seal, one can use commercially\
    \ available leak detection systems that use either helium or radioiso‑ tope tracer\
    \ gases. A vacuum is applied, the integrated circuit is pressurized for a certain\
    \ amount of time (allowing the gas to seep into the package cavity), and then\
    \ the released gas is detected. Gross Leak Hermetic Seal assessments, on the other\
    \ hand, may be read‑ ily carried out with just a bit of pressured air, a vacuum\
    \ system, a pressure chamber, and\n\nliquids that will not interfere with any\
    \ electronic devices [74–76]. Scanning electron microscopy uses a concentrated\
    \ electron beam rather than light to create a high-resolution picture. The process\
    \ begins when a beam of electrons is created by an electron cannon at the microscope's\
    \ focal point. The extensive test time of SEM, since it might take several hours\
    \ to analyze a single component, limits its applicability. However, SEM is particularly\
    \ beneficial for identifying various faults and abnormalities prevalent in counterfeit\
    \ compo-\n\nThe term \"interior tests\" describes the process of inspecting the\
    \ inner workings of a gadget or product to ensure its legitimacy. The component's\
    \ protective covering must be\n\nBump connections, which are used in modern chip\
    \ technologies instead of wire bonds, enhance the density and stress concentration\
    \ on the connecting part by a substantial amount [77]. Ball shear testing has\
    \ been created to examine the reliability of these bumps. To use this technique,\
    \ one places an implement next to the base and presses down on the ball until\
    \ it breaks. The idea behind both ball and die shear testing is quite similar.\
    \ These experiments provide a direct measure of the interaction's trustworthiness;\
    \ nonetheless, they call for the handling of the samples and a decapping procedure\
    \ [78]. The electronic connections between various IC layers are critical to the\
    \ reliability of the IC package as a whole. The quality and durability of bonding\
    \ in microelectronic applications may be evaluated with tests like wire-pull testing.\
    \ In this kind of testing, wires are subjected to an\n\nremoved so that its internal\
    \ structure may be examined for this test.\n\ntwo liquids that will not interfere\
    \ with any electronic devices [\\[74](#page-19-17)[–76](#page-19-18)]. Scanning\
    \ electron microscopy uses a concentrated electron beam rather than light to create\
    \ a high‑resolution picture. The process begins when a beam of electrons is created\
    \ by an electron cannon at the microscope's focal point. The extensive test time\
    \ of SEM, since it might take several hours to analyze a single component, limits\
    \ its applicability. However, SEM is particularly beneficial for identifying various\
    \ faults and abnormalities prevalent in counterfeit compo‑ nents.\n\n## 3.1.3.\
    \ Interior Tests\n\nThe term \"interior tests\" describes the process of inspecting\
    \ the inner workings of a gadget or product to ensure its legitimacy. The component's\
    \ protective covering must be removed so that its internal structure may be examined\
    \ for this test.\n\nBump connections, which are used in modern chip technologies\
    \ instead of wire bonds, enhance the density and stress concentration on the connecting\
    \ part by a substantial amount [\\[77](#page-20-0)]. Ball shear testing has been\
    \ created to examine the reliability of these bumps. To use this technique, one\
    \ places an implement next to the base and presses down on the ball until it breaks.\
    \ The idea behind both ball and die shear testing is quite similar. These experiments\
    \ provide a direct measure of the interaction's trustworthiness; nonetheless,\
    \ they call for the handling of the samples and a decapping procedure[[78\\]](#page-20-1).\
    \ The electronic connections between various IC layers are critical to the reliability\
    \ of the IC package as a whole. The quality and durability of bonding in microelectronic\
    \ applications may be evaluated with tests like wire‑pull testing. In this kind\
    \ of testing, wires are subjected to an upward force applied by a hook, which\
    \ is then used to draw the wire away from the substrate or die until the bond\
    \ fails or the wire breaks. A tiny hook is inserted beneath the wire and pulled\
    \ upwards to impart strain to the bond wire and determine its quality. In order\
    \ to pass the standard non‑destructive wire pull test used in the industry, all\
    \ bonds must remain intact despite the light loading tension used[[79\\]](#page-20-2).\
    \ The goals of wire pull test‑ ing are to assess the durability of the bond, investigate\
    \ the causes of bond failure, and guarantee that the specified bond strength criteria\
    \ are met. The die shear test is used in semiconductors and other microelectronic\
    \ devices as a quality control measure to evaluate the adhesion and bonded area\
    \ of bare die attached with media such as epoxy, solder, and sinter materials\
    \ to substrate materials such as metal lead frames, ceramic packages, and printed\
    \ circuit boards.\n\n#### 3.1.4. Material Analysis\n\nThe non‑destructive qualities,\
    \ rapidity, and compatibility of Raman spectroscopy with a diverse array of materials\
    \ render it exceptionally well suited for implementation in hard‑ ware security\
    \ applications [\\[80](#page-20-3)]. This spectroscopic technique harnesses the\
    \ scattering of light to provide valuable insights into the molecular composition\
    \ and structure of mate‑ rials. In the realm of hardware security, Raman‑active\
    \ compounds like nanotags play a pivotal role in the authentication and tracking\
    \ of components[[81\\]](#page-20-4). Nanotags, infused with Raman‑active materials\
    \ can be affixed to integrated circuits or other hardware components, allowing\
    \ for their secure identification and traceability. An exemplary illustration\
    \ of Ra‑ man spectroscopy's efficacy in hardware security can be found in the\
    \ work of Vaskova et al.[[82](#page-20-5)], where it was employed to scrutinize\
    \ the dielectric materials within electronic assemblies. The objective was to\
    \ uncover instances of counterfeit capacitors, a prevalent concern in the electronics\
    \ industry. By subjecting the components to Raman spectroscopic analysis, the\
    \ researchers could non‑destructively assess the molecular composition of the\
    \ dielectric materials. Any disparities or inconsistencies in the spectral signatures\
    \ could sig‑ nify the presence of counterfeit or substandard components, thus\
    \ enabling precise detec‑ tion and mitigation of potential security threats. Raman\
    \ spectroscopy's versatility and accuracy make it an invaluable tool in the ongoing\
    \ efforts to enhance the security and reli‑ ability of electronic systems.\n\n\
    ### *3.2. Electrical Inspections*\n\nParametric, Burn‑In, and Structural Tests\n\
    \nSinanoglu et al.[[83\\]](#page-20-6) introduced an innovative approach for detecting\
    \ counterfeit in‑ tegrated circuits (ICs) based on a two‑dimensional space of\
    \ parametric measurements. In their method, they employed a one‑class support\
    \ vector machine (SVM) trained using mea‑ surements obtained from a collection\
    \ of new devices sourced from reputable suppliers. These devices naturally exhibit\
    \ some degree of process variation. The one‑class SVM, act‑ ing as a machine‑learning\
    \ model, establishes a nonlinear boundary within the parametric measurement space.\
    \ This boundary effectively discriminates between genuine and coun‑ terfeit ICs.\n\
    \nThe conventional one‑class SVM, as described by Schölkopf[[84\\]](#page-20-7)\
    \ and colleagues, rep‑ resent an objective function as:\n\n$$\\frac{1}{2}|w|^2\
    \ - \\rho + \\mathcal{C}\\sum\\_{j} \\mathfrak{f}\\_j \\tag{1}$$\n\nUnder the\
    \ constraints, *w · ϕ*(*xj*) *≥ ρ − ξj and ξj ≥* 0. In this context, the equation\
    \ *w · ϕ*(*x*) = *ρ* identifies a hyperplane within the feature domain. The symbol\
    \ *|·|* is represen‑ tative of the Euclidean magnitude, while *ξj* are the slack\
    \ variables. A pre‑set parameter, *C*, determines the proportion of anomalies,\
    \ as discussed by Müller et al. and Schölkopf et al. [\\[84](#page-20-7),[85\\\
    ]](#page-20-8).\n\nAnother study by different authors[[86\\]](#page-20-9) employed\
    \ Support Vector Machines (SVMs) to detect counterfeit ICs, with a specific focus\
    \ on distinguishing previously used ICs from unused ones. They accomplished this\
    \ by training a one‑class SVM classifier on a set of new devices that exhibit\
    \ process variations. Notably, this approach obviated the need for prior knowledge\
    \ concerning how transistor degradation may affect IC functionality. The classifier\
    \ utilized straightforward parametric measurements and validation data from burn‑in\
    \ experiments simulating the aging process. This cost‑effective method eliminated\
    \ additional identification expenses and exhibited high effectiveness in identifying\
    \ counter‑ feit ICs falsely represented as new. By incorporating various parametric\
    \ measurements, this technique demonstrated exceptional precision in identifying\
    \ used components falsely marketed as brand‑new.\n\nA recent study using SVM was\
    \ performed by Kent et al. [\\[87\\]](#page-20-10) and utilized a linear support\
    \ vector machine (SVM), a supervised machine‑learning model, to classify distinct\
    \ categories. SVM works by creating a hyperplane that best separates data points\
    \ from dif‑ ferent categories, maximizing the margin between them. Its principle\
    \ lies in ensuring the hyperplane is positioned to maximize the distance from\
    \ the nearest data points of differ‑ ing categories. The application of SVM extends\
    \ beyond just this study; it has traditionally been employed for routine maintenance\
    \ tasks like fault detection in hardware and soft‑ ware systems. For instance,\
    \ SVM was applied to distinguish performance issues in heating ventilation air‑conditioning\
    \ and cooling chillers by categorizing the data into two classes: \"fault detected\"\
    \ and \"no fault detected\". While machine learning techniques have been ex‑ plored\
    \ to address sensor location challenges for daylight harvesting, this particular\
    \ study showcased SVM's distinct application in this context.\n\nFurthermore,\
    \ the impact of Negative Bias Temperature Instability (NBTI), a primary cause\
    \ of circuit performance deterioration, can be assessed through a structural test\
    \ to gauge the integrity of ICs. NBTI aging has been shown to influence the threshold\
    \ voltage in PMOS devices[[83](#page-20-6)[,88](#page-20-11)[–91](#page-20-12)].\
    \ As mathematically developed by Wang et al. [\\[92](#page-20-13)] Equation (1)\
    \ details the calculation for \"∆*Vth*\" as influenced by NBTI for a given time\
    \ \"*t*\". In this for‑ mula, \"*Kv*\" signifies the impact of the electric field,\
    \ temperature, and carrier concentration. The time exponential constant is denoted\
    \ as \"*n*\". The term \"*α*\" indicates the signal prob‑\n\n(2)\n\n(2)\n\nability,\
    \ representing the portion of time a transistor is under NBTI stress within a\
    \ given period. \"*T*\" is the temperature, derives continuous values capable\
    \ of generating unique chip identifiers. The wealth of available test vectors\
    \ enhances the diversity of challenge-response pairings, opening up new avenues\
    \ for IC monitoring and security protocols [93–95]. This innovative approach derives\
    \ continuous values capable of generating unique chip identifiers. The wealth\
    \ of available test vectors enhances the diversity of challenge-response pairings,\
    \ opening up new avenues for IC monitoring and security protocols [93–95]. This\
    \ innovative approach\n\nmajority of ICs produced with cutting-edge technology,\
    \ all without the need for additional chip components. Leveraging non-invasive\
    \ gate-level characterization, this method\n\n*Electronics* **2023**, *12*, x\
    \ FOR PEER REVIEW 12 of 22\n\nstudy showcased SVM's distinct application in this\
    \ context.\n\ngiven period. \"*T*\" is the temperature,\n\n∆ℎ <sup>=</sup> (〖√\n\
    \nstudy showcased SVM's distinct application in this context.\n\n<sup>2</sup>\
    \ × ×\n\n∆ℎ <sup>=</sup> (〖√\n\nIn the realm of IC verification through structural\
    \ tests, there is a noteworthy exploration of using ICs' distinctive timing path\
    \ signatures, stemming from inherent process variations, as a basis for creating\
    \ Physical Unclonable Functions (PUFs) for identification purposes. Structural\
    \ tests encompass assessments like leakage, timing, and dynamic\n\n<sup>2</sup>\
    \ × ×\n\nIn the realm of IC verification through structural tests, there is a\
    \ noteworthy exploration of using ICs' distinctive timing path signatures, stemming\
    \ from inherent process variations, as a basis for creating Physical Unclonable\
    \ Functions (PUFs) for identification purposes. Structural tests encompass assessments\
    \ like leakage, timing, and dynamic\n\n1− 1 2 )〗 2\n\n1− 1 2 )〗 2\n\ngiven period.\
    \ \"*T*\" is the temperature,\n\nmarketed as brand-new.\n\nmarketed as brand-new.\n\
    \nAnother study by different authors [86] employed Support Vector Machines (SVMs)\
    \ to detect counterfeit ICs, with a specific focus on distinguishing previously\
    \ used ICs from unused ones. They accomplished this by training a one-class SVM\
    \ classifier on a set of new devices that exhibit process variations. Notably,\
    \ this approach obviated the need for prior knowledge concerning how transistor\
    \ degradation may affect IC functionality. The classifier utilized straightforward\
    \ parametric measurements and validation data from burn-in experiments simulating\
    \ the aging process. This cost-effective method eliminated additional identification\
    \ expenses and exhibited high effectiveness in identifying counterfeit ICs falsely\
    \ represented as new. By incorporating various parametric measurements, this technique\
    \ demonstrated exceptional precision in identifying used components falsely\n\n\
    Another study by different authors [86] employed Support Vector Machines (SVMs)\
    \ to detect counterfeit ICs, with a specific focus on distinguishing previously\
    \ used ICs from unused ones. They accomplished this by training a one-class SVM\
    \ classifier on a set of new devices that exhibit process variations. Notably,\
    \ this approach obviated the need for prior knowledge concerning how transistor\
    \ degradation may affect IC functionality. The classifier utilized straightforward\
    \ parametric measurements and validation data from burn-in experiments simulating\
    \ the aging process. This cost-effective method eliminated additional identification\
    \ expenses and exhibited high effectiveness in identifying counterfeit ICs falsely\
    \ represented as new. By incorporating various parametric measurements, this technique\
    \ demonstrated exceptional precision in identifying used components falsely\n\n\
    A recent study using SVM was performed by Kent et al. [87] and utilized a linear\
    \ support vector machine (SVM), a supervised machine-learning model, to classify\
    \ distinct categories. SVM works by creating a hyperplane that best separates\
    \ data points from different categories, maximizing the margin between them. Its\
    \ principle lies in ensuring the hyperplane is positioned to maximize the distance\
    \ from the nearest data points of differing categories. The application of SVM\
    \ extends beyond just this study; it has traditionally been employed for routine\
    \ maintenance tasks like fault detection in hardware and software systems. For\
    \ instance, SVM was applied to distinguish performance issues in heating ventilation\
    \ air-conditioning and cooling chillers by categorizing the data into two classes:\
    \ \"fault detected\" and \"no fault detected.\" While machine learning techniques\
    \ have been explored to address sensor location challenges for daylight harvesting,\
    \ this particular\n\nFurthermore, the impact of Negative Bias Temperature Instability\
    \ (NBTI), a primary cause of circuit performance deterioration, can be assessed\
    \ through a structural test to gauge the integrity of ICs. NBTI aging has been\
    \ shown to influence the threshold voltage in PMOS devices [83,88–91]. As mathematically\
    \ developed by Wang et al. [92] equation (1) details the calculation for \"∆ℎ\"\
    \ as influenced by NBTI for a given time \"\". In this formula, \"\" signifies\
    \ the impact of the electric field, temperature, and carrier concentration. The\
    \ time exponential constant is denoted as \"\". The term \"\" indicates the signal\
    \ probability, representing the portion of time a transistor is under NBTI stress\
    \ within a\n\n*Electronics* **2023**, *12*, x FOR PEER REVIEW 12 of 22\n\nA recent\
    \ study using SVM was performed by Kent et al. [87] and utilized a linear support\
    \ vector machine (SVM), a supervised machine-learning model, to classify distinct\
    \ categories. SVM works by creating a hyperplane that best separates data points\
    \ from different categories, maximizing the margin between them. Its principle\
    \ lies in ensuring the hyperplane is positioned to maximize the distance from\
    \ the nearest data points of differing categories. The application of SVM extends\
    \ beyond just this study; it has traditionally been employed for routine maintenance\
    \ tasks like fault detection in hardware and software systems. For instance, SVM\
    \ was applied to distinguish performance issues in heating ventilation air-conditioning\
    \ and cooling chillers by categorizing the data into two classes: \"fault detected\"\
    \ and \"no fault detected.\" While machine learning techniques have been explored\
    \ to address sensor location challenges for daylight harvesting, this particular\n\
    \nFurthermore, the impact of Negative Bias Temperature Instability (NBTI), a primary\
    \ cause of circuit performance deterioration, can be assessed through a structural\
    \ test to gauge the integrity of ICs. NBTI aging has been shown to influence the\
    \ threshold voltage in PMOS devices [83,88–91]. As mathematically developed by\
    \ Wang et al. [92] equation (1) details the calculation for \"∆ℎ\" as influenced\
    \ by NBTI for a given time \"\". In this formula, \"\" signifies the impact of\
    \ the electric field, temperature, and carrier concentration. The time exponential\
    \ constant is denoted as \"\". The term \"\" indicates the signal probability,\
    \ representing the portion of time a transistor is under NBTI stress within a\n\
    \n$$\n\\Delta V\\_{th} = \\left( \\mathbb{E} \\sqrt{K\\_o^2 \\times T\\_{clk}\
    \ \\times \\frac{\\alpha}{1 - \\rho\\_t^{\\frac{1}{2n}}}} \\mathbb{I} \\right)^{2n}\
    \ \\tag{2}\n$$\n\nmajority of ICs produced with cutting-edge technology, all without\
    \ the need for additional chip components. Leveraging non-invasive gate-level\
    \ characterization, this method\n\nAging-based fingerprints are distinct characteristics\
    \ that develop in electronic devices, such as integrated circuits, over time due\
    \ to aging effects. These fingerprints offer valuable potential for enhancing\
    \ hardware security by enabling the development of Aging-based fingerprints are\
    \ distinct characteristics that develop in electronic devices, such as integrated\
    \ circuits, over time due to aging effects. These fingerprints offer valuable\
    \ potential for enhancing hardware security by enabling the development of In\
    \ the realm of IC verification through structural tests, there is a noteworthy\
    \ explo‑ ration of using ICs' distinctive timing path signatures, stemming from\
    \ inherent process variations, as a basis for creating Physical Unclonable Functions\
    \ (PUFs) for identification purposes. Structural tests encompass assessments like\
    \ leakage, timing, and dynamic power measurements. These evaluations have the\
    \ potential to uniquely identify the vast majority of ICs produced with cutting‑edge\
    \ technology, all without the need for additional chip components. Leveraging\
    \ non‑invasive gate‑level characterization, this method derives continuous values\
    \ capable of generating unique chip identifiers. The wealth of available test\
    \ vectors enhances the diversity of challenge‑response pairings, opening up new\
    \ av‑ enues for IC monitoring and security protocols[[93–](#page-20-14)[95\\]](#page-20-15).\
    \ This innovative approach holds promise for bolstering IC authentication and\
    \ security measures while maintaining cost‑ efficiency and scalability.\n\n####\
    \ *3.3. Aging‑Based Fingerprint Testing*\n\nAging‑based fingerprints are distinct\
    \ characteristics that develop in electronic devices, such as integrated circuits,\
    \ over time due to aging effects. These fingerprints offer valuable potential\
    \ for enhancing hardware security by enabling the development of identification\
    \ or authentication methods based on the natural wear and tear of these devices.\
    \ Various factors, including Negative Bias Temperature Instability, Hot Carrier\
    \ Injection, and Time‑ dependent Dielectric Breakdown, contribute to changes in\
    \ the performance of transistors and other components as electronic devices undergo\
    \ aging. These changes are unique to each device due to process variations and\
    \ patterns of usage, resulting in the creation of individualized aging‑based fingerprints[[72](#page-19-15)[,96](#page-20-16)].\n\
    \nIn the context of this paper[[97\\]](#page-20-17), a cost‑effective approach\
    \ is presented for safeguarding secret keys using Physical Unclonable Functions\
    \ (PUFs), leveraging the unique hardware identity of sensor nodes. Additionally,\
    \ a resource‑efficient fingerprint recognition system is introduced, designed\
    \ specifically for deployment in low‑cost sensor nodes. PUFs are also employed\
    \ for obfuscation to protect sensitive biometric data. The authors propose a two‑factor\
    \ authentication method to verify the source of collected data, relying on the\
    \ unique physical identity of the trusted sensor node and the physical presence\
    \ of an autho‑ rized individual overseeing data transfer. Experimental results\
    \ indicate the feasibility of implementing the proposed PUF‑based solution in\
    \ the SRAMs of commercially available Bluetooth Low‑energy chips within sensor\
    \ nodes. The fingerprint identification technol‑ ogy is based on \"QFingerMap16\"\
    , utilizing unique texture‑based features. The research further delves into the\
    \ resilience, security, and privacy aspects of the suggested sensor nodes, drawing\
    \ from experimental data involving PUFs and fingerprints sourced from public and\
    \ standardized databases. This multifaceted approach offers promising implica‑\
    \ tions for enhancing security and privacy in low‑cost sensor node applications.\n\
    \n#### **4. Counterfeit Avoidance Method**\n\n#### *4.1. PUF‑Based Avoidance Techniques*\n\
    \nThere are two main categories of Physical Unclonable Functions (PUFs): delay‑based\
    \ PUFs and memory‑based PUFs. Each of these utilize different aspects of the underlying\
    \ technology. These PUFs offer several advantages, including unpredictability,\
    \ resistance to tampering, cost‑effectiveness, and dynamic key generation. However,\
    \ they also present challenges such as sensitivity to environmental factors, low\
    \ entropy, and susceptibility to modeling attacks. Ongoing research and development\
    \ in PUF design, error correction, and countermeasures aim to address these issues,\
    \ making PUFs a promising solution for hardware security.\n\nOne notable implementation\
    \ is the Ring‑Oscillator PUF, mentioned in article[[98\\]](#page-20-18), which\
    \ uses an oscillator with an odd number of gates to generate distinct signatures\
    \ sen‑ sitive to manufacturing variations. However, delay‑based PUFs suffer from\
    \ spatial corre‑ lations in process parameters, limiting their uniqueness and\
    \ making them susceptible to side‑channel attacks.\n\nTo overcome these limitations,\
    \ the authors of paper[[22\\]](#page-17-21) introduce the Process and Environmental\
    \ (PE)‑PUF. This PUF design takes into account process and ambient vari‑ ables\
    \ like temperature, power supply noise, and crosstalk, enhancing the randomness\
    \ and uniqueness of the generated signatures. The study employs a 90 nm‑implemented\
    \ seven‑ inverter ring oscillator with nearby interconnects, simulated using HSPICE.\n\
    \nIn [\\[99](#page-20-19)], researchers explore the application of deep learning\
    \ techniques to model at‑ tacks on double arbiter PUFs. The results demonstrate\
    \ that deep learning methods out‑ perform conventional machine learning approaches\
    \ like logistic regression and support vector machines in terms of predictive\
    \ accuracy. The success rate in attacks against 3‑1 DAPUFs exceeds 86%, surpassing\
    \ the previous record of 76%. Similarly, the accuracy in attacks against 4‑1 DAPUFs\
    \ ranges from 71% to 81.5%, surpassing the prior high of 63%.\n\nFinally, in[[100\\\
    ]](#page-20-20), authors propose an innovative SRAM architecture that facilitates\
    \ cost‑effective and widespread key generation by integrating dynamic and multi‑bit\
    \ static entropy generation in memory. This design retains a commercial bitcell,\
    \ a pitch‑matched peripheral, and compatibility with memory compiler designs.\
    \ Additionally, it incorpo‑ rates a True Random Number Generator (TRNG) and a\
    \ physically unclonable function (PUF) to enhance security.\n\n#### *4.2. Machine\
    \ Learning and Artificial Intelligence*\n\nAI and machine learning (ML) approaches\
    \ are increasingly being integrated into hard‑ ware design processes, providing\
    \ a fresh approach to addressing various phases and lay‑ ers of abstraction. By\
    \ estimating hardware overhead[[101\\]](#page-20-21), optimizing logic[[102\\\
    ]](#page-21-0), rout‑ ing[[103\\]](#page-21-1), and introducing test points [\\\
    [104](#page-21-2)], these techniques address scalability difficulties and accelerate\
    \ design completion. Using AI and ML in hardware design enables better op‑ timization,\
    \ more efficiency, and shorter development cycles.\n\nThe authors of the study[[105\\\
    ]](#page-21-3) analyze the viability of repurposing an existing neural network\
    \ to construct a robust Physically Unclonable Function in order to ensure safety\
    \ and reliability in Internet of Things and smart sensor applications. The Multilayer\
    \ Perceptron is the primary subject of this work. It is a feed‑forward neural\
    \ network with multiple lay‑ ers of completely coupled neurons. They consider\
    \ several network designs, each with its unique hidden layer depth and synaptic\
    \ weight accuracy. PUF criteria such as uniformity, uniqueness, bit‑aliasing,\
    \ and reliability are used to assess the quality of the proposed solu‑ tion. Another\
    \ work[[106\\]](#page-21-4) introduces \"HW2VEC\", a free and open source graph‑learning\
    \ tool developed to let researchers investigate hardware security applications\
    \ using graph representations. HW2VEC is a tool that translates non‑Euclidean\
    \ hardware designs into an embedding in a Euclidean network and extracts graph\
    \ representations from hardware designs at various abstraction levels.\n\n####\
    \ *4.3. Hardware Metering*\n\nHardware metering, also referred to as integrated\
    \ circuit metering, serves as a crucial mechanism for monitoring and safeguarding\
    \ integrated circuits (ICs) once they have been manufactured. This becomes particularly\
    \ significant as many businesses choose to out‑ source their IC manufacturing\
    \ to companies located in different countries, exposing their designs to potential\
    \ theft or replication risks. To address this challenge, experts have devel‑ oped\
    \ various methods for monitoring and managing ICs, with passive and active metering\
    \ emerging as the two predominant approaches.\n\nIn passive metering systems,\
    \ individual chips are initially identified separately to de‑ tect any unauthorized\
    \ or counterfeit chips effectively. Active metering, on the other hand, provides\
    \ designers with the capability to control specific chip operations, enhancing\
    \ chip security. This article provides an overview of hardware metering, exploring\
    \ its key con‑ cepts and diverse methodologies.\n\nIt is worth noting that hardware\
    \ watermarking, while related, differs from hardware metering. While hardware\
    \ metering involves actively or passively tagging individual chips, hardware watermarking\
    \ embeds its mark within the design file rather than on the individ‑ ual chips.\
    \ Watermarking, however, has limitations in combatting counterfeiting as it can‑\
    \ not differentiate between chips of the same design. In contrast, hardware metering\
    \ offers a more effective solution by assigning a unique identity to each chip\
    \ or its functionality, enabling differentiation among chips with identical architectures.\n\
    \n#### Passive and Active IC Metering\n\nPassive metering represents a method\
    \ for the identification of counterfeit chips by monitoring and analyzing their\
    \ operational data. In essence, this approach aims to dis‑ tinguish genuine chips\
    \ from counterfeits that replicate the control behaviors of legitimate chips.\
    \ Passive metering proves particularly effective when dealing with a large number\
    \ of chips that can be coupled, allowing for the examination of their individual\
    \ control paths. This examination is achieved through techniques like XOR operations\
    \ and additional par‑ ity tests [\\[107](#page-21-5)].\n\nHowever, as noted in[[108\\\
    ]](#page-21-6), current passive metering methods suffer from various limitations,\
    \ including challenges in quantifying chip IDs accurately, high associated costs,\
    \ and issues related to scalability. To address these issues, the authors propose\
    \ two signif‑ icant changes as potential solutions. Firstly, they suggest utilizing\
    \ manifestation proper‑ ties to extract physical‑level characteristics, such as\
    \ gate threshold voltage, which remain independent of aging, temperature variations,\
    \ and supply voltage. Secondly, to reduce expenses, expedite time‑to‑market, and\
    \ enhance scalability, they advocate for IC segmen‑ tation. This segmentation\
    \ involves selecting only a subset of gates for detailed characteri‑ zation.\n\
    \nIn[[107\\]](#page-21-5), the authors delve into the horizontal semiconductor\
    \ business model, which exposes designers' intellectual property (IP) to piracy\
    \ and excessive production of inte‑ grated circuits due to the transparency prevalent\
    \ across the production chain. To com‑ bat these challenges and enable chip‑tracking\
    \ post production, they introduce the concept of active hardware metering. The\
    \ authors also discuss potential risks and countermea‑ sures while presenting\
    \ a low‑overhead hardware solution based on an autonomous syn‑ thesis method.\n\
    \nMoreover, Ref. [\\[109](#page-21-7)] introduces a novel approach to external\
    \ active IC metering that utilizes a PUF (Physically Unclonable Function) design\
    \ to generate keys. In contrast to tra‑ ditional encryption modules, they employ\
    \ a modified Finite State Machine (FSM) to pro‑ tect PUF‑based keys from unauthorized\
    \ access. By integrating the retrieval method within the high‑level design of\
    \ the FSM, they significantly reduce the time and effort required to securely\
    \ recover PUF‑based keys, especially when the original FSM is reused.\n\n### *4.4.\
    \ Secure Split Testing*\n\nThe \"Secure Split‑Test\" (SST) is a newly introduced\
    \ approach that restores testing au‑ thority to the owner of Intellectual Property\
    \ (IP). With SST, chips are securely locked during the evaluation phase. Only\
    \ the IP proprietor has the capability to decipher the locked test outcomes and\
    \ grant access to the chips that meet the set criteria. SST's main objective is\
    \ to halt the distribution of excess or flawed chips within the supply chain.\
    \ Compared to its predecessor, this method streamlines the dialogue between the\
    \ chip‑making foundry and the IP owner. Evidence suggests that SST not only bolsters\
    \ security but also mitigates communication obstacles. The researchers Contreras\
    \ et al.[[110\\]](#page-21-8) introduced a unique \"SST Structure\" to augment\
    \ the protection of integrated circuits. This design incorporates a\n\nlocking\
    \ mechanism known as the \"XORF mask\", which consists of three‑way XOR gates\
    \ situated in less crucial circuit routes. The XORF acts as a switch; it serves\
    \ as a conduit when two inputs match, and as a converter when they differ. Placing\
    \ these XORFs, es‑ pecially near scan flip‑flop entry points, can alter specific\
    \ circuit feedback. True Random Number Generators (TRNGs) are employed to add\
    \ an element of randomness, drawing from physical occurrences such as clock inconsistencies\
    \ and temperature variations for en‑ tropy. TRNG outputs are saved in a non‑reusable\
    \ memory for consistency. The design also incorporates RSA encryption to fortify\
    \ the IC's security, with a complex key system (TKEY and FKEY) governing the XORF\
    \ operations. An additional \"Scan‑Locking Block\", employing three‑way XOR gates\
    \ and key‑driven functions (KDFs), has been integrated to enhance defenses against\
    \ potential threats.\n\nIn the following sections, we present two comprehensive\
    \ tables that encapsulate the myriad of challenges encountered in the realm of\
    \ counterfeit electronics. Table [1](#page-14-0), titled \"Implementation Challenges\
    \ of Counterfeit Detection Methods\", delves into the obstacles faced when identifying\
    \ fake components through various detection strategies. It outlines the practical\
    \ difficulties and technical intricacies inherent to the current detection method‑\
    \ ologies. Following this, Table [2,](#page-14-1) \"Implementation Challenges\
    \ of Counterfeit Avoidance Methods\", shifts the focus to preventative strategies.\
    \ It scrutinizes the hurdles in imple‑ menting effective systems designed to thwart\
    \ the infiltration of counterfeit electronics into the supply chain, highlighting\
    \ the proactive measures necessary to safeguard against such threats. Together,\
    \ these tables provide a dual perspective on the fight against electronic counterfeiting,\
    \ offering insights into both reactive detection and proactive prevention.\n\n\
    #### <span id=\"page-14-0\"></span>**Table 1.** Implementation challenges of counterfeit\
    \ detection methods.\n\n| Detection Scheme                                   |\
    \ Dependability | Distinctiveness  | Tamper Proofing | Chip Area<br>Requirement\
    \ | Target Component           | Deployment Cost  |\n|----------------------------------------------------|---------------|------------------|-----------------|--------------------------|----------------------------|------------------|\n\
    | Incoming<br>Inspections                            | Varies        | Moderate\
    \         | Low             | Low                      | Digital/Analog/RF,<br>etc.\
    \ | Low              |\n| Exterior Tests                                     |\
    \ Moderate      | Moderate         | Moderate        | Low                   \
    \   | Digital/Analog/RF,<br>etc. | Moderate         |\n| Interior Tests      \
    \                               | High          | High             | High    \
    \        | High                     | Digital/Analog/RF,<br>etc. | High      \
    \       |\n| Material Analysis                                  | High       \
    \   | Moderate to High | High            | Very High                | Digital/Analog/RF,<br>etc.\
    \ | Very High        |\n| Parametric/Burn‑in<br>Test and Structural<br>Tests |\
    \ Very High     | High             | Very High       | Moderate              \
    \   | Digital ICs                | Moderate to High |\n\n<span id=\"page-14-1\"\
    ></span>**Table 2.** Implementation challenges of counterfeit avoidance methods.\n\
    \n| Avoidance Scheme                             | Dependability     | Distinctiveness\
    \  | Tamper Proofing | Chip Area<br>Requirement | Target Component | Deployment\
    \ Cost |\n|----------------------------------------------|-------------------|------------------|-----------------|--------------------------|------------------|-----------------|\n\
    | Physically<br>Unclonable<br>Functions (PUFs) | Moderate          | High    \
    \         | High            | Low                      | Digital ICs      | Moderate\
    \        |\n| Passive Hardware<br>Metering                 | Moderate to High\
    \  | High             | Moderate        | Low                      | Digital ICs\
    \      | Moderate        |\n| Active Hardware<br>Metering                  | High\
    \ to Very High | High             | Moderate        | Moderate               \
    \  | Digital ICs      | Moderate        |\n| Machine<br>Learning/Computer<br>Vision\
    \       | High              | Moderate to High | Low             | Varies    \
    \               | Digital ICs      | Low             |\n| Secure Split Test<br>(SST)\
    \                   | NA                | NA               | Moderate        |\
    \ Moderate                 | Digital ICs      | High            |\n\n- (a) **Dependability:**\
    \ Many of these methods grapple with the challenge of consistent per‑ formance.\
    \ For instance, a PUF's reaction should remain unchanged across different environmental\
    \ conditions, disturbances, and over time. Such issues do not plague active and\
    \ passive hardware metering, though its ability to prevent counterfeiting is still\
    \ under examination. Machine Learning, since the accuracy of its results depends\
    \ on vast dataset, has a high reliability. Incoming Tests ensure initial quality\
    \ but might vary in dependability based on the test's comprehensiveness.\n- (b)\
    \ **Distinctiveness:** This evaluates the dissimilarity between chip identifications.\
    \ Ide‑ ally, two identifiers should have a 50% probability of differing under\
    \ identical condi‑ tions. Strong distinctiveness hinders the ability of counterfeiters\
    \ to predict new IDs after obtaining a collection. PUFs and magnetic PUFs yield\
    \ almost perfect results in this aspect. Common programming languages can produce\
    \ truly random numbers, typically used for chip identification.\n- (c) **Tamper\
    \ Proofing:** This gauges the challenges counterfeiters face in trying to bypass\
    \ anti‑counterfeit measures. The locked results of SSTs offer an appreciably high\
    \ taper resistance to the chips. Material analysis imposes a high level of difficulty\
    \ in detection because counterfeiting happens at the material composition level.\
    \ Meanwhile, exte‑ rior tests detect tampering at the surface level. Machine Learning,\
    \ combined with Material Analysis, can detect counterfeit actions at a compositional\
    \ level.\n- (d) **Chip Area Requirement:** This represents the space required\
    \ on the chip that is needed for anti‑counterfeit tools. Machine Learning/Computer\
    \ Vision, on the other hand, might demand significant computational resources\
    \ but not necessarily chip space. In contrast, hardware metering, SST, and poly\
    \ fuse‑based sensors require more space.\n- (e) **Targeted Component Types:**\
    \ This details the component kinds these anti‑counterfeit tools are suited for.\
    \ Parametric/Burn‑in and Structural Tests are mostly targeted at digital components,\
    \ while Incoming Tests can apply to both. PUFs can be used in both analog and\
    \ digital parts while other tools are more suited for digital components.\n- (f)\
    \ **Deployment Cost:** Setting up a PUF involves maintaining a secure challenge‑response\
    \ database, alongside the space it occupies. For hardware metering and SST, extensive\
    \ communication between the designer and the manufacturer hikes up the price.\
    \ Tools like CDIR come with their own spatial costs. Verifying integrated circuits\
    \ demands affordable equipment, but the intricate verification for applied plant\
    \ DNA on the IC as an interior test is high.\n\n#### **5. Challenges Facing the\
    \ Microelectronics Industry in Adopting Trust**\n\nThe COVID‑19 pandemic has had\
    \ a profound impact on the electronics industry, sig‑ nificantly increasing the\
    \ prevalence and quality of counterfeit electronics. These attacks have grown\
    \ more sophisticated over time, necessitating equally complex countermeasures\
    \ in response. Traditional physical inspection countermeasures and confidence‑building\
    \ strategies are both expensive and risky, presenting substantial challenges.\
    \ The time‑consuming process of physically inspecting and testing counterfeit\
    \ electronics further exacerbates the problem. Additionally, as commercial and\
    \ military technologies converge, ethical concerns in the IT industry have arisen,\
    \ prompting businesses to evaluate how their products' capabilities and applications,\
    \ whether used by the Department of Defense or its adversaries, impact national\
    \ security.\n\nThe growing reliance on microelectronics across various industries\
    \ has heightened the demand for reliable and secure hardware solutions. In the\
    \ realm of hardware security, verifying the authenticity and integrity of microelectronic\
    \ components has become a daunt‑ ing task. The intricacy of modern supply chains\
    \ makes comprehensive monitoring from inception to final assembly challenging,\
    \ increasing the risk of electronic equipment being composed of subpar materials,\
    \ infected with malware, or subject to intellectual property (IP) theft.\n\nAnother\
    \ challenge in the domain of trusted microelectronics is the ever‑evolving land‑\
    \ scape of threats and attack vectors. Hackers continuously devise new methods\
    \ to exploit\n\nhardware systems, necessitating ongoing vigilance from security\
    \ researchers and design‑ ers. Side‑channel attacks, for instance, rely on extracting\
    \ sensitive data through the phys‑ ical implementation of a system. Given these\
    \ trends, microelectronics security remains a formidable challenge. As technology\
    \ advances, physical components become increasingly complex and interconnected,\
    \ making it challenging to defend against both existing and emerging threats.\
    \ Consequently, it is evident that addressing these evolving dangers re‑ quires\
    \ the integration of Physically Unclonable Functions (PUFs), robust design principles,\
    \ and machine learning‑based approaches to hardware security.\n\n#### **6. Conclusions**\n\
    \nThis comprehensive review underscores the significant challenges and threats\
    \ con fronting the microelectronics industry, particularly from counterfeit components.\
    \ As ex‑ plored in Section [2,](#page-3-1) \"Counterfeit Attack Modes\", the industry\
    \ is battling a range of so‑ phisticated methods employed by malicious entities\
    \ to introduce counterfeit components, undermining the integrity of both individual\
    \ electronic units and larger systems. The vul‑ nerabilities these attack modes\
    \ present are not just technical but also ripple into economic, ethical, and security\
    \ realms.\n\nIn response to these threats, Section [3,](#page-5-1) \"Counterfeit\
    \ Detection\", delves into the multi‑ faceted strategies and methods to identify\
    \ and mitigate the presence of counterfeit com‑ ponents. These detection mechanisms\
    \ are essential in ensuring the security, reliability, and efficiency of electronic\
    \ systems. However, the ever‑evolving nature of attack vectors demands ongoing\
    \ research, innovation, and refinement in these detection methodologies.\n\nBeyond\
    \ the direct threats of counterfeiting, broader challenges have surfaced, such\
    \ as the ethical dilemmas stemming from the overlap of commercial and military\
    \ technologies and the heightened risks brought on by the COVID‑19 pandemic. The\
    \ complex interplay of these challenges necessitates a cohesive, interdisciplinary\
    \ response, ranging from techno‑ logical solutions like Physically Unclonable\
    \ Functions (PUFs) and machine learning‑based approaches to policy interventions\
    \ and universally accepted industry standards.\n\nFurthermore, a comparison of\
    \ the implementation challenges involved in both the avoidance and detection techniques\
    \ have been explored under the headings of dependabil‑ ity, distinctiveness, tamper\
    \ proofing, chip area overhead and deployment costs. It must be noted, however,\
    \ that this provides a generalized perspective. The actual dependabil‑ ity, distinctiveness,\
    \ tamper proofing, chip area overhead, and ease of implementation may vary depending\
    \ on the specific methodologies and tools used within each scheme.\n\nIn summary,\
    \ the microelectronics industry is at a critical juncture. Trust and security\
    \ are non‑negotiable pillars for its sustained growth and evolution. By integrating\
    \ insights from various sections, it is evident that proactive measures, collaborative\
    \ efforts, and a commitment to continuous learning are vital to navigate the multifarious\
    \ challenges and ensure a resilient future for microelectronics.\n\n**Author Contributions:**\
    \ Conceptualization, K.N., S.D. and V.B.; writing—original draft preparation,\
    \ K.N.; writing—review and editing, V.B.; supervision, V.B.; funding acquisition,\
    \ F.L. and V.B. All authors have read and agreed to the published version of the\
    \ manuscript.\n\n**Funding:** This research was funded by the U.S. Air Force via\
    \ the Assured Digital Microelectronics Education and Training Ecosystem (ADMETE)\
    \ grant (FA8650‑20‑2‑1136).\n\n**Data Availability Statement:** The datasets generated\
    \ and/or analyzed during the current study are available in the \"ERAI\" repository\
    \ at <https://www.erai.com/> (accessed on 16 April 2022).\n\n**Conflicts of Interest:**\
    \ The authors declare no conflict of interest. Additionally, The funders had no\
    \ role in the design of the study; in the collection, analyses, or interpretation\
    \ of data; in the writing of the manuscript; or in the decision to publish the\
    \ results.\n\n# **References**\n\n- <span id=\"page-17-0\"></span>1. Bhunia, S.;\
    \ Tehranipoor, M. Chapter 1—Introduction to Hardware Security. In *Hardware Security*;\
    \ Bhunia, S., Tehranipoor, M., Eds.; Morgan Kaufmann: Burlington, MA, USA, 2019;\
    \ pp. 1–20.\n- <span id=\"page-17-1\"></span>2. Fazzari Booz, S.; Hamilton, A.;\
    \ Narumi, R. *New & Old Challenges for Trusted and Assured Microelectronics*;\
    \ Booz Allen Hamilton: Arlington, VA, USA, 2019.\n- <span id=\"page-17-2\"></span>3.\
    \ Shah, A. Europe, US Warn of Fake‑Chip Danger to National Security, Critical\
    \ Systems. The Register. 2022. Available online: [https://www.theregister.com/2022/03/18/eu\\\
    \\_us\\\\_counterfeit\\\\_chips/](https://www.theregister.com/2022/03/18/eu_us_counterfeit_chips/)\
    \ (accessed on 13 April 2023).\n- <span id=\"page-17-3\"></span>4. Zeljka, Z.\
    \ Supply Chain Compromise: Adding Undetectable Hardware Trojans to Integrated\
    \ Circuits. Help Net Security. 2018. Available online: [https://www.helpnetsecurity.com/2018/12/10/hardware‑trojans/](https://www.helpnetsecurity.com/2018/12/10/hardware-trojans/)\
    \ (accessed on 19 March 2022).\n- <span id=\"page-17-4\"></span>5. Uppal, R. Threats\
    \ to ICT Supply Chains including Counterfeit Electronic Components and Hardware\
    \ Trojans Present Crit‑ ical Risk to Military and Security Systems. International\
    \ Defense Security & Technology Inc. 2020. Available online: [https://idstch.com/threats/threats‑to‑ict‑supply‑chains‑including‑counterfeit‑electronic‑components‑and‑hardware‑trojans‑](https://idstch.com/threats/threats-to-ict-supply-chains-including-counterfeit-electronic-components-and-hardware-trojans-present-critical-risk-to-military-and-security-systems/)\
    \ [present‑critical‑risk‑to‑military‑and‑security‑systems/](https://idstch.com/threats/threats-to-ict-supply-chains-including-counterfeit-electronic-components-and-hardware-trojans-present-critical-risk-to-military-and-security-systems/)\
    \ (accessed on 19 March 2022).\n- <span id=\"page-17-5\"></span>6. Hambling, D.\
    \ Pentagon's \"Kill Switch\": Urban Myth? Wired. 2008. Available online: [https://www.wired.com/2008/05/kill‑](https://www.wired.com/2008/05/kill-switch-urb/)\
    \ [switch‑urb/](https://www.wired.com/2008/05/kill-switch-urb/) (accessed on 27\
    \ March 2023).\n- <span id=\"page-17-6\"></span>7. McKeefry, H. Counter the Counterfeiters.\
    \ DigiKey. 2021. Available online: [https://www.digikey.com/en/blog/counter‑the‑](https://www.digikey.com/en/blog/counter-the-counterfeiters)\
    \ [counterfeiters](https://www.digikey.com/en/blog/counter-the-counterfeiters)\
    \ (accessed on 18 April 2022).\n- <span id=\"page-17-7\"></span>8. Brett, D. Counterfeit\
    \ Electronic Parts: A Multibillion‑Dollar Black Market. Trenton Systems. 2020.\
    \ Available online: [https:](https://www.trentonsystems.com/blog/counterfeit-electronic-parts)\
    \ [//www.trentonsystems.com/blog/counterfeit‑electronic‑parts](https://www.trentonsystems.com/blog/counterfeit-electronic-parts)\
    \ (accessed on 19 March 2022).\n- <span id=\"page-17-8\"></span>9. The Threat\
    \ of Counterfeit Components to Electronic Supply Chains. Nanotech. Available online:\
    \ [https://www.nanosecurity.ca/](https://www.nanosecurity.ca/counterfeit-electronic-components/)\
    \ [counterfeit‑electronic‑components/](https://www.nanosecurity.ca/counterfeit-electronic-components/)\
    \ (accessed on 19 March 2022).\n- <span id=\"page-17-9\"></span>10. IEEE Transactions\
    \ on Components and Packaging Technologies Publication Information. *IEEE Trans.\
    \ Compon. Packag. Technol.* **2007**, *30*, C2. [\\[CrossRef\\]](https://doi.org/10.1109/TCAPT.2007.912781)\n\
    - <span id=\"page-17-10\"></span>11. Bastia, S. Next generation technologies to\
    \ combat counterfeiting of electronic components. *Compon. Packag. Technol. IEEE\
    \ Trans.* **2002**, *25*, 175–176.[[CrossRef](https://doi.org/10.1109/6144.991192)]\n\
    - <span id=\"page-17-11\"></span>12. Akhoundov, D. *2022 Annual Report*; ERAI,\
    \ Inc.: Naples, FL, USA, 2022.\n- <span id=\"page-17-12\"></span>13. Murdock,\
    \ K.; Oswald, D.; Garcia, F.D.; Van Bulck, J.; Gruss, D.; Piessens, F. Plundervolt:\
    \ Software‑Based Fault Injection Attacks against Intel SGX. In Proceedings of\
    \ the 2020 IEEE Symposium on Security and Privacy (SP), San Francisco, CA, USA,\
    \ 18–21 May 2020; pp. 1466–1482.\n- <span id=\"page-17-13\"></span>14. Tehranipoor,\
    \ M.; Koushanfar, F. A Survey of Hardware Trojan Taxonomy and Detection. *IEEE\
    \ Des. Test Comput.* **2010**, *27*, 10–25. [\\[CrossRef\\]](https://doi.org/10.1109/MDT.2010.7)\n\
    - <span id=\"page-17-14\"></span>15. Intel. Speculative Execution. 2018. Available\
    \ online: [https://www.intel.com/content/www/us/en/developer/articles/technical/](https://www.intel.com/content/www/us/en/developer/articles/technical/software-security-guidance/technical-documentation/introduction-speculative-side-channel-methods.html)\
    \ [software‑security‑guidance/technical‑documentation/introduction‑speculative‑side‑channel‑methods.html](https://www.intel.com/content/www/us/en/developer/articles/technical/software-security-guidance/technical-documentation/introduction-speculative-side-channel-methods.html)\
    \ (accessed on 16 March 2023).\n- <span id=\"page-17-15\"></span>16. Dewan, M.C.\
    \ Study of Speculative Execution and Branch Prediction. 2006. Available online:\
    \ [https://citeseerx.ist.psu.edu/](https://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=EBE980ABF71E4B8C0055F14D3DDAC3F2?doi=10.1.1.119.2934&rep=rep1&type=pdf)\
    \ [viewdoc/download;jsessionid=EBE980ABF71E4B8C0055F14D3DDAC3F2?doi=10.1.1.119.2934&rep=rep1&type=pdf](https://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=EBE980ABF71E4B8C0055F14D3DDAC3F2?doi=10.1.1.119.2934&rep=rep1&type=pdf)\
    \ (accessed on 16 March 2023).\n- <span id=\"page-17-16\"></span>17. Lee, J.W.;\
    \ Lim, D.; Gassend, B.; Suh, G.E.; Van Dijk, M.; Devadas, S. A Technique to Build\
    \ a Secret Key in Integrated Circuits for Identification and Authentication Applications.\
    \ In Proceedings of the 2004 Symposium on VLSI Circuits, Honolulu, HI, USA, 17–19\
    \ June 2004; Digest of Technical Papers (IEEE Cat. No.04CH37525). IEEE: Piscataway,\
    \ PJ, USA, 2004; pp. 176–179.\n- <span id=\"page-17-17\"></span>18. Suh, G.E.;\
    \ Devadas, S. Physical Unclonable Functions for Device Authentication and Secret\
    \ Key Generation. In Proceedings of the 2007 44th ACM/IEEE Design Automation Conference,\
    \ San Diego, CA, USA, 4–8 June 2007; pp. 9–14.\n- <span id=\"page-17-18\"></span>19.\
    \ Gassend, B.; Clarke, D.; Van Dijk, M.; Devadas, S. Silicon Physical Random Functions.\
    \ In Proceedings of the 9th ACM Conference on Computer and Communications Security,\
    \ Washington, DC, USA, 18–22 November 2002.\n- <span id=\"page-17-19\"></span>20.\
    \ Ranasinghe, C.; Engels, D.W.; Cole, P.H. Security and Privacy: Modest Proposals\
    \ for Low‑Cost RFID Systems. Available on‑ line: [https://www.semanticscholar.org/paper/Security‑and‑Privacy%3A‑Modest‑Proposals‑for‑Low‑Cost‑Ranasinghe‑Engels/](https://www.semanticscholar.org/paper/Security-and-Privacy%3A-Modest-Proposals-for-Low-Cost-Ranasinghe-Engels/4c755bb9751f148a769737addc3e0fb14de42341)\
    \ [4c755bb9751f148a769737addc3e0fb14de42341](https://www.semanticscholar.org/paper/Security-and-Privacy%3A-Modest-Proposals-for-Low-Cost-Ranasinghe-Engels/4c755bb9751f148a769737addc3e0fb14de42341)\
    \ (accessed on 27 September 2023).\n- <span id=\"page-17-20\"></span>21. Rührmair,\
    \ U.; Sehnke, F.; Sölter, J.; Dror, G.; Devadas, S.; Schmidhuber, J. Modeling\
    \ Attacks on Physical Unclonable Functions. In Proceedings of the 17th ACM Conference\
    \ on Computer and Communications Security, Chicago, IL, USA, 4–8 October 2010;\
    \ Association for Computing Machinery: New York, NY, USA, 2010; pp. 237–249.\n\
    - <span id=\"page-17-21\"></span>22. Wang, X.; Tehranipoor, M. Novel Physical\
    \ Unclonable Function with Process and Environmental Variations. In Proceedings\
    \ of the 2010 Design, Automation & Test in Europe Conference & Exhibition (DATE\
    \ 2010), Dresden, Germany, 8–12 March 2010; pp. 1065–1070.\n- <span id=\"page-17-22\"\
    ></span>23. Monjur, M.M.R.; Heacock, J.; Calzadillas, J.; Mahmud, M.; Roth, J.;\
    \ Mankodiya, K.; Sazonov, E.; Yu, Q. Hardware Security in Sensor and its Networks.\
    \ *Front. Sens.* **2022**, *3*, 850056.[[CrossRef\\]](https://doi.org/10.3389/fsens.2022.850056)\n\
    - <span id=\"page-17-23\"></span>24. Shivakumara, T.; Patil, R.M.; Muneshwara,\
    \ M.S. Review Paper on Dynamic Mechanisms of Data Leakage Detection and Preven‑\
    \ tion. *Int. J. Comput. Sci. Eng.* **2019**, *7*, 349–358.\n- <span id=\"page-17-24\"\
    ></span>25. Asadizanjani, N.; Rahman, M.T.; Tehranipoor, M. (Eds.) Package Security.\
    \ In *Physical Assurance: For Electronic Devices and Systems*; Springer International\
    \ Publishing: Cham, Switzerland, 2021; pp. 155–177.\n- <span id=\"page-18-0\"\
    ></span>26. Sharief, S.; Chahal, P.; Alocilja, E. Application of DNA sequences\
    \ in anti‑counterfeiting: Current progress and challenges. *Int. J. Pharm.* **2021**,\
    \ *602*, 120580. [\\[CrossRef\\]](https://doi.org/10.1016/j.ijpharm.2021.120580)\n\
    - <span id=\"page-18-1\"></span>27. Torrance, R.; James, D. The State‑of‑the‑Art\
    \ in IC Reverse Engineering. In *International Workshop on Cryptographic Hardware\
    \ and Embedded Systems*; Springer: Berlin/Heidelberg, Germany, 2009; pp. 363–381.\n\
    - <span id=\"page-18-2\"></span>28. Barenghi, A.; Breveglieri, L.; Koren, I.;\
    \ Naccache, D. Fault Injection Attacks on Cryptographic Devices: Theory, Practice,\
    \ and Countermeasures. *Proc. IEEE* **2012**, *100*, 3056–3076. [\\[CrossRef\\\
    ]](https://doi.org/10.1109/JPROC.2012.2188769)\n- <span id=\"page-18-3\"></span>29.\
    \ Balasch, J.; Gierlichs, B.; Verbauwhede, I. An In‑Depth and Black‑Box Characterization\
    \ of the Effects of Clock Glitches on 8‑Bit MCUs. In Proceedings of the 2011 Workshop\
    \ on Fault Diagnosis and Tolerance in Cryptography, Tokyo, Japan, 29 September\
    \ 2011; IEEE Computer Society: Washington, DC, USA, 2011; pp. 105–114.\n- <span\
    \ id=\"page-18-4\"></span>30. Kocher, P.; Jaffe, J.; Jun, B.; Rohatgi, P. Introduction\
    \ to differential power analysis. *J. Cryptogr. Eng.* **2011**, *1*, 5–27. [\\\
    [CrossRef\\]](https://doi.org/10.1007/s13389-011-0006-y)\n- <span id=\"page-18-5\"\
    ></span>31. Mangard, S.; Oswald, E.; Popp, T. *Power Analysis Attacks: Revealing\
    \ the Secrets of Smart Cards*; Springer: New York, NY, USA, 2007.\n- <span id=\"\
    page-18-6\"></span>32. Brier, E.; Clavier, C.; Olivier, F. Correlation Power Analysis\
    \ with a Leakage Model. In *Cryptographic Hardware and Embedded Systems—CHES 2004*;\
    \ Joye, M., Quisquater, J.‑J., Eds.; Springer: Berlin/Heidelberg, Germany, 2004;\
    \ pp. 16–29.\n- <span id=\"page-18-7\"></span>33. York, D. Chapter 3—Eavesdropping\
    \ and Modification. In *Seven Deadliest Unified Communications Attacks*; York,\
    \ D., Ed.; Syngress: Boston, MA, USA, 2010; pp. 41–69.\n- <span id=\"page-18-8\"\
    ></span>34. Alves, T.; Das, R.; Werth, A.; Morris, T. Virtualization of SCADA\
    \ Testbeds for Cybersecurity Research: A Modular Approach. *Comput. Secur.* **2018**,\
    \ *77*, 531–546.[[CrossRef\\]](https://doi.org/10.1016/j.cose.2018.05.002)\n-\
    \ <span id=\"page-18-9\"></span>35. Parmar, B. Protecting against spear‑phishing.\
    \ *Comput. Fraud. Secur.* **2012**, *2012*, 8–11.[[CrossRef\\]](https://doi.org/10.1016/S1361-3723(12)70007-6)\n\
    - <span id=\"page-18-10\"></span>36. Cybersecurity for Small Businesses. Federal\
    \ Communications Commission. Available online: <https://www.fcc.gov/communicat>\
    \ [ions‑business‑opportunities/cybersecurity‑small‑businesses](ions-business-opportunities/cybersecurity-small-businesses)\
    \ (accessed on 7 September 2023).\n- <span id=\"page-18-11\"></span>37. Subramanyan,\
    \ P.; Ray, S.; Malik, S. Evaluating the Security of Logic Encryption Algorithms.\
    \ In Proceedings of the 2015 IEEE International Symposium on Hardware Oriented\
    \ Security and Trust (HOST), Washington, DC, USA, 5–7 May 2015; pp. 137–143.\n\
    - <span id=\"page-18-12\"></span>38. Guin, U.; Huang, K.; DiMase, D.; Carulli,\
    \ J.M.; Tehranipoor, M.; Makris, Y. Counterfeit integrated circuits: A rising\
    \ threat in the global semiconductor supply chain. *Proc. IEEE* **2014**, *102*,\
    \ 1207–1228. [\\[CrossRef\\]](https://doi.org/10.1109/JPROC.2014.2332291)\n- <span\
    \ id=\"page-18-13\"></span>39. Baldini, G.; Steri, G.; Dimc, F.; Giuliani, R.;\
    \ Kamnik, R. Experimental Identification of Smartphones Using Fingerprints of\
    \ Built‑In Micro‑Electro Mechanical Systems (MEMS). *Sensors* **2016**, *16*,\
    \ 818. [\\[CrossRef\\]](https://doi.org/10.3390/s16060818) [\\[PubMed](https://www.ncbi.nlm.nih.gov/pubmed/27271630)]\n\
    - <span id=\"page-18-14\"></span>40. A Novel Technique for Effective Detection\
    \ of Recycled ICs Using Joint Parameter Analysis. IJARTET Journal—Academia.edu.\
    \ Available online: [https://www.academia.edu/9939115/A\\\\_Novel\\\\_Technique\\\
    \\_for\\\\_Effective\\\\_Detection\\\\_of\\\\_Recycled\\\\_ICs\\\\_Using\\\\_](https://www.academia.edu/9939115/A_Novel_Technique_for_Effective_Detection_of_Recycled_ICs_Using_Joint_Parameter_Analysis)\
    \ [Joint\\\\_Parameter\\\\_Analysis](https://www.academia.edu/9939115/A_Novel_Technique_for_Effective_Detection_of_Recycled_ICs_Using_Joint_Parameter_Analysis)\
    \ (accessed on 27 October 2023).\n- 41. Vosatka, J.; Stern, A.; Hossain, M.M.;\
    \ Rahman, F.; Allen, J.; Allen, M.; Farahmandi, F.; Tehranipoor, M. Tracking Cloned\
    \ Elec‑ tronic Components using a Consortium‑based Blockchain Infrastructure.\
    \ In Proceedings of the 2020 IEEE International Con‑ ference on Physical Assurance\
    \ and Inspection on Electronics, Washington, DC, USA, 28–29 July 2020; PAINE:\
    \ Durham, NH, USA, 2020.[[CrossRef](https://doi.org/10.1109/PAINE49178.2020.9337735)]\n\
    - <span id=\"page-18-15\"></span>42. Xiao, K. Techniques for Improving Security\
    \ and Trustworthiness of Integrated Circuits. Ph.D. Thesis, University of Connecticut,\
    \ Storrs, CT, USA, 2015.\n- <span id=\"page-18-16\"></span>43. Klocke, F.; Gorgels,\
    \ C.; Bouzakis, E.; Stuckenberg, A. Tool life increase of coated carbide tools\
    \ by micro blasting. *Prod. Eng.* **2009**, *3*, 453–459.[[CrossRef](https://doi.org/10.1007/s11740-009-0173-1)]\n\
    - 44. Melentiev, R.; Kang, C.; Shen, G.; Fang, F. Study on surface roughness generated\
    \ by micro‑blasting on Co‑Cr‑Mo bio‑implant. *Wear* **2019**, *428–429*, 111–126.[[CrossRef](https://doi.org/10.1016/j.wear.2019.03.005)]\n\
    - <span id=\"page-18-17\"></span>45. Gadge, M.; Lohar, G.; Chinchanikar, S. A\
    \ review on micro‑blasting as surface treatment technique for improved cutting\
    \ tool performance. *Mater. Today Proc.* **2022**, *64*, 725–730.[[CrossRef](https://doi.org/10.1016/j.matpr.2022.05.196)]\n\
    - <span id=\"page-18-18\"></span>46. Candler, R.N.; Park, W.T.; Hopcroft, M.;\
    \ Kim, B.; Kenny, T.W. Hydrogen diffusion and pressure control of encapsulated\
    \ mems resonators. In Proceedings of the International Conference on Solid State\
    \ Sensors and Actuators and Microsystems, Seoul, Re‑ public of Korea, 5–9 June\
    \ 2005; Digest of Technical Papers, TRANSDUCERS '05. IEEE: Piscataway, NJ, USA,\
    \ 2005; Volume 1, pp. 920–923. [\\[CrossRef\\]](https://doi.org/10.1109/SENSOR.2005.1496568)\n\
    - 47. Ding, C.; Soni, G.; Bozorgi, P.; Piorek, B.D.; Meinhart, C.D.; MacDonald,\
    \ N.C. A flat heat pipe architecture based on nanostruc‑ tured titania. *J. Microelectromech.\
    \ Syst.* **2010**, *19*, 878–884. [\\[CrossRef\\]](https://doi.org/10.1109/JMEMS.2010.2051019)\n\
    - <span id=\"page-18-19\"></span>48. Dandapat, N.; Ghosh, S. Interfacial and Cross‑sectional\
    \ Studies of Thermally Cycled Alumina‑Monel Brazed Joint. *Trans. Indian Ceram.\
    \ Soc.* **2020**, *79*, 152–157. [\\[CrossRef](https://doi.org/10.1080/0371750X.2020.1787865)]\n\
    - <span id=\"page-18-20\"></span>49. Rahman, M.T.; Asadizanjani, N. Failure Analysis\
    \ for Hardware Assurance and Security. *Electron. Device Fail. Anal.* **2019**,\
    \ *21*, 16–24. [\\[CrossRef](https://doi.org/10.31399/ASM.EDFA.2019-3.P016)]\n\
    - 50. Vashistha, N.; Lu, H.; Shi, Q.; Rahman, M.T.; Shen, H.; Woodard, D.L.; Asadizanjani,\
    \ N.; Tehranipoor, M. Trojan Scanner: Detecting Hardware Trojans with Rapid SEM\
    \ Imaging combined with Image Processing and Machine Learning. In *ISTFA 2018:\
    \ Proceedings from the 44th International Symposium for Testing and Failure Analysis,\
    \ Phoenix, AZ, USA, 28 October–1 November 2018*; ASM International. Available\
    \ online: [https://books.google.com/books?hl=en&lr=&id=Mx59DwAAQBAJ&oi=fnd&pg=PA256](https://books.google.com/books?hl=en&lr=&id=Mx59DwAAQBAJ&oi=fnd&pg=PA256&dq=SEM+hardware+security&ots=-ibwWTUyG4&sig=l7llYBLLFmyYdJ6SbK-socj3Tx0#v=onepage&q=SEM%20hardware%20security&f=false)\
    \ [&dq=SEM+hardware+security&ots=‑ibwWTUyG4&sig=l7llYBLLFmyYdJ6SbK‑socj3Tx0#v=onepage&q=SEM%20hardware%20](https://books.google.com/books?hl=en&lr=&id=Mx59DwAAQBAJ&oi=fnd&pg=PA256&dq=SEM+hardware+security&ots=-ibwWTUyG4&sig=l7llYBLLFmyYdJ6SbK-socj3Tx0#v=onepage&q=SEM%20hardware%20security&f=false)\
    \ [security&f=false](https://books.google.com/books?hl=en&lr=&id=Mx59DwAAQBAJ&oi=fnd&pg=PA256&dq=SEM+hardware+security&ots=-ibwWTUyG4&sig=l7llYBLLFmyYdJ6SbK-socj3Tx0#v=onepage&q=SEM%20hardware%20security&f=false)\
    \ (accessed on 27 October 2023).\n- 51. Courbon, F.; Loubet‑Moundi, P.; Fournier,\
    \ J.J.A.; Tria, A. A high efficiency Hardware Trojan detection technique based\
    \ on fast SEM imaging. In Proceedings of the 2015 Design, Automation and Test\
    \ in Europe Conference & Exhibition (DATE), Grenoble, France, 9–13 March 2015;\
    \ pp. 788–793. [\\[CrossRef](https://doi.org/10.7873/DATE.2015.1104)]\n- <span\
    \ id=\"page-19-0\"></span>52. Rahman, M.T.; Shi, Q.; Tajik, S.; Shen, H.; Woodard,\
    \ D.L.; Tehranipoor, M.; Asadizanjani, N. Physical inspection attacks: New frontier\
    \ in hardware security. In Proceedings of the 2018 IEEE 3rd International Verification\
    \ and Security Workshop, IVSW, Costa Brava, Spain, 2–4 July 2018; pp. 93–102.[[CrossRef](https://doi.org/10.1109/IVSW.2018.8494856)]\n\
    - <span id=\"page-19-1\"></span>53. Thomas‑Brans, F.; Heckmann, T.; Markantonakis,\
    \ K.; Sauveron, D. New Diagnostic Forensic Protocol for Damaged Secure Digital\
    \ Memory Cards. *IEEE Access* **2022**, *10*, 33742–33757. [\\[CrossRef\\]](https://doi.org/10.1109/ACCESS.2022.3158958)\n\
    - 54. Xi, C.; Khan, A.A.; Jessurun, N.; Vashisthan, N.; Tehranipoor, M.M.; Asadizanjani,\
    \ N. Physical Assurance for Heterogeneous Integration: Challenges and Opportunities.\
    \ In Proceedings of the International Symposium on the Physical and Failure Analysis\
    \ of Integrated Circuits, IPFA, Singapore, 18–20 July 2022. [\\[CrossRef](https://doi.org/10.1109/IPFA55383.2022.9915749)]\n\
    - <span id=\"page-19-2\"></span>55. Klima, S.J.; Baaklini, G.Y.; Abel, P.B. *Nondestructive\
    \ Evaluation of Structural Ceramics*; NASA: Washington, DC, USA, 1987.\n- <span\
    \ id=\"page-19-3\"></span>56. Asadizanjani, N.; Rahman, M.T.; Tehranipoor, M.\
    \ Optical Inspection and Attacks. In *Physical Assurance*; Springer: Cham, Switzer‑\
    \ land, 2021; pp. 133–153.[[CrossRef](https://doi.org/10.1007/978-3-030-62609-9_6)]\n\
    - 57. Kulkarni, A.; Xu, C. A Deep Learning Approach in Optical Inspection to Detect\
    \ Hidden Hardware Trojans and Secure Cyberse‑ curity in Electronics Manufacturing\
    \ Supply Chains. *Front. Mech. Eng.* **2021**, *7*, 709924.[[CrossRef](https://doi.org/10.3389/fmech.2021.709924)]\n\
    - <span id=\"page-19-4\"></span>58. Vashistha, N.; Rahman, M.T.; Shen, H.; Woodard,\
    \ D.L.; Asadizanjani, N.; Tehranipoor, M. Detecting Hardware Trojans Inserted\
    \ by Untrusted Foundry Using Physical Inspection and Advanced Image Processing.\
    \ *J. Hardw. Syst. Secur.* **2018**, *2*, 333–344. [\\[CrossRef\\]](https://doi.org/10.1007/s41635-018-0055-0)\n\
    - <span id=\"page-19-5\"></span>59. van Gils, M.A.; van der Sluis, O.; Zhang,\
    \ G.Q.; Janssen, J.H.; Voncken, R.M. Analysis of Cu/low‑k bond pad delamination\
    \ by using a novel failure index. In Proceedings of the 6th International Conference\
    \ on Thermal, Mechanical and Multi‑Physics Sim‑ ulation and Experiments in Micro‑Electronics\
    \ and Micro‑Systems, EuroSimE, Berlin, Germany, 18–20 April 2005; pp. 190–196.\
    \ [\\[CrossRef\\]](https://doi.org/10.1109/ESIME.2005.1502798)\n- 60. Viswanath,\
    \ A.G.; Fang, W.; Zhang, X.; Ganesh, V.P.; Lim, L.A. Numerical analysis by 3D\
    \ finite element wire bond simulation on Cu/low‑K structures. In Proceedings of\
    \ the 7th Electronics Packaging Technology Conference, EPTC, Singapore, 7–9 December\
    \ 2005; Volume 1, pp. 215–220.[[CrossRef\\]](https://doi.org/10.1109/EPTC.2005.1614396)\n\
    - <span id=\"page-19-6\"></span>61. Wang, C.; Sun, R. The Quality Test of Wire\
    \ Bonding. *Mod. Appl. Sci.* **2009**, *3*, 50–56. [\\[CrossRef\\]](https://doi.org/10.5539/mas.v3n12p50)\n\
    - <span id=\"page-19-7\"></span>62. Zamalloa Jara, M.A.; Luízar Obregón, C.; Araujo\
    \ Del Castillo, C. Exploratory analysis for the identification of false banknotes\
    \ using portable X‑ray Fluorescence spectrometer. *Appl. Radiat. Isot.* **2018**,\
    \ *135*, 212–218.[[CrossRef](https://doi.org/10.1016/j.apradiso.2018.01.043)]\
    \ [\\[PubMed](https://www.ncbi.nlm.nih.gov/pubmed/29427957)]\n- 63. Camp, D.C.\
    \ K‑Edge X‑ray Fluorescence Analysis for Actinide and Heavy Elements Solution\
    \ Concentration Measurements. *Adv. X‑ray Anal.* **1984**, *28*, 91–98. [\\[CrossRef\\\
    ]](https://doi.org/10.1154/S0376030800013823)\n- <span id=\"page-19-8\"></span>64.\
    \ Anceau, S.; Bleuet, P.; Clédière, J.; Maingault, L.; Rainard, J.L.; Tucoulou,\
    \ R. Nanofocused X‑ray beam to reprogram secure cir‑ cuits. In Proceedings of\
    \ the Cryptographic Hardware and Embedded Systems–CHES 2017: 19th International\
    \ Conference, Taipei, Taiwan, 25–28 September 2017; Lecture Notes in Computer\
    \ Science (including subseries Lecture Notes in Artificial Intelligence and Lecture\
    \ Notes in Bioinformatics) 10529 LNCS. pp. 175–188. [\\[CrossRef](https://doi.org/10.1007/978-3-319-66787-4_9)]\n\
    - <span id=\"page-19-9\"></span>65. Chan, K.L.A.; Kazarian, S.G. Detection of\
    \ trace materials with Fourier transform infrared spectroscopy using a multi‑channel\
    \ detector. *Analyst* **2006**, *131*, 126–131.[[CrossRef](https://doi.org/10.1039/B511243E)][[PubMed\\\
    ]](https://www.ncbi.nlm.nih.gov/pubmed/16365673)\n- 66. Chen, H.; Ferrari, C.;\
    \ Angiuli, M.; Yao, J.; Raspi, C.; Bramanti, E. Qualitative and quantitative analysis\
    \ of wood samples by Fourier transform infrared spectroscopy and multivariate\
    \ analysis. *Carbohydr. Polym.* **2010**, *82*, 772–778. [\\[CrossRef\\]](https://doi.org/10.1016/j.carbpol.2010.05.052)\n\
    - <span id=\"page-19-10\"></span>67. Lu, H.; Capecci, D.E.; Ghosh, P.; Forte,\
    \ D.; Woodard, D.L. Computer vision for hardware security. In *Emerging Topics\
    \ in Hardware Security*; Tehranipoor, M., Ed.; Springer International Publishing:\
    \ Cham, Switzerland, 2021; pp. 493–525.[[CrossRef\\]](https://doi.org/10.1007/978-3-030-64448-2_18)\n\
    - <span id=\"page-19-11\"></span>68. Huynh, N.; Cherian, H.; Ahn, E.C. Hardware\
    \ security of emerging non‑volatile memory devices under imaging attacks. In Proceedings\
    \ of the International Conference on Applied Electronics, Pilsen, Czech Republic,\
    \ 7–8 September 2021. [\\[CrossRef\\]](https://doi.org/10.23919/AE51540.2021.9542884)\n\
    - <span id=\"page-19-12\"></span>69. Hadjikhani, A.; Rodzinski, A.; Wang, P.;\
    \ Nagesetti, A.; Guduru, R.; Liang, P.; Runowicz, C.; Shahbazmohamadi, S.; Khizroev,\
    \ S. Biodistribution and clearance of magnetoelectric nanoparticles for nanomedical\
    \ applications using energy dispersive spec‑ troscopy. *Nanomedicine* **2017**,\
    \ *12*, 1801–1822.[[CrossRef](https://doi.org/10.2217/nnm-2017-0080)]\n- <span\
    \ id=\"page-19-13\"></span>70. Akter, N.; Karabiyik, M.; Shur, M.; Suarez, J.;\
    \ Pala, N. AI Powered THz VLSI Testing Technology. In Proceedings of the 29th\
    \ North Atlantic Test Workshop, NATW 2020, Albany, NY, USA, 17–24 June 2020. [\\\
    [CrossRef\\]](https://doi.org/10.1109/NATW49237.2020.9153077)\n- <span id=\"page-19-14\"\
    ></span>71. Ishibuchi, H.; Kwoh, C.K.; Tan, A.H.; Srinivasan, D.; Miao, C.; Trivedi,\
    \ A.; Crockett, K.; Institute of Electrical and Electron‑ ics Engineers. In Proceedings\
    \ of the 2022 IEEE Symposium Series on Computational Intelligence (SSCI 2022),\
    \ Singapore, 4–7 December 2022.\n- <span id=\"page-19-15\"></span>72. Xu, Z.;\
    \ Cui, A.; Qu, G. A New Aging Sensor for the Detection of Recycled ICs. In Proceedings\
    \ of the 2020 on Great Lakes Symposium on VLSI, Beijing, China, 8–11 September\
    \ 2020; pp. 223–228.\n- <span id=\"page-19-16\"></span>73. Guin, U.; DiMase, D.;\
    \ Tehranipoor, M. Counterfeit Integrated Circuits: Detection, Avoidance, and the\
    \ Challenges Ahead. *J. Electron. Test.* **2014**, *30*, 9–23.[[CrossRef](https://doi.org/10.1007/s10836-013-5430-8)]\n\
    - <span id=\"page-19-17\"></span>74. Doyle, E.J. *Morris Bill Failure Analysis\
    \ Techniques*; Rome Air Development Center: Oneida County, NY, USA, 1981.\n- 75.\
    \ Kim, B.; Park, W.‑T. MEMS Packaging. In *Encyclopedia of Nanotechnology*; Bhushan,\
    \ B., Ed.; Springer: Dordrecht, The Netherlands, 2012; pp. 1351–1359.\n- <span\
    \ id=\"page-19-18\"></span>76. Davy, J. Calculations for Leak Rates of Hermetic\
    \ Packages. *IEEE Trans. Parts Hybrids Packag.* **1975**, *11*, 177–189. [\\[CrossRef](https://doi.org/10.1109/TPHP.1975.1135069)]\n\
    - <span id=\"page-20-0\"></span>77. Tu, K.N. Reliability challenges in 3D IC packaging\
    \ technology. *Microelectron. Reliab.* **2011**, *51*, 517–523. [\\[CrossRef](https://doi.org/10.1016/j.microrel.2010.09.031)]\n\
    - <span id=\"page-20-1\"></span>78. Xi, C.; Jessurun, N.; Asadizanjani, N. A Framework\
    \ to Assess the Security of Advanced Integrated Circuit (IC) Packaging. In Pro‑\
    \ ceedings of the 2020 IEEE 8th Electronics System‑Integration Technology Conference\
    \ (ESTC), Tonsberg, Norway, 15–18 Septem‑ ber 2020; pp. 1–7.\n- <span id=\"page-20-2\"\
    ></span>79. Fang, K. 3—Encapsulation Process Technology. In *Encapsulation Technologies\
    \ for Electronic Applications*, 2nd ed.; Ardebili, H., Zhang, J., Pecht, M.G.,\
    \ Eds.; William Andrew Publishing: Norwich, NY, USA, 2019; pp. 123–181.\n- <span\
    \ id=\"page-20-3\"></span>80. Zumbusch, A.; Holtom, G.R.; Xie, X.S. Three‑Dimensional\
    \ Vibrational Imaging by Coherent Anti‑Stokes Raman Scattering. *Phys. Rev. Lett.*\
    \ **1999**, *82*, 4142–4145.[[CrossRef](https://doi.org/10.1103/PhysRevLett.82.4142)]\n\
    - <span id=\"page-20-4\"></span>81. Sánchez‑Purrà, M.; Roig‑Solvas, B.; Rodriguez‑Quijada,\
    \ C.; Leonardo, B.M.; Hamad‑Schifferli, K. Reporter Selection for Nano‑ tags in\
    \ Multiplexed Surface Enhanced Raman Spectroscopy Assays. *ACS Omega* **2018**,\
    \ *3*, 10733–10742. [\\[CrossRef\\]](https://doi.org/10.1021/acsomega.8b01499)\n\
    - <span id=\"page-20-5\"></span>82. Vaskova, H.; Neumann, P.; Kozubik, M.; Jelinek,\
    \ K. Raman Spectroscopic Study of Counterfeit Electronic Components. *WSEAS Trans.\
    \ Syst. Control* **2018**, *13*, 453–459.\n- <span id=\"page-20-6\"></span>83.\
    \ Sinanoglu, O.; Karimi, N.; Rajendran, J.; Karri, R.; Jin, Y.; Huang, K.; Makris,\
    \ Y. Reconciling the IC test and security dichotomy. In Proceedings of the 2013\
    \ 18th IEEE European Test Symposium (ETS), Avignon, France, 27–30 May 2013; pp.\
    \ 1–6.\n- <span id=\"page-20-7\"></span>84. Advances in Kernel Methods—Support\
    \ Vector Learning. Available online: [https://www.researchgate.net/publication/2346087\\\
    \\_](https://www.researchgate.net/publication/2346087_Advances_in_Kernel_Methods_-_Support_Vector_Learning)\
    \ [Advances\\\\_in\\\\_Kernel\\\\_Methods\\\\_‑\\\\_Support\\\\_Vector\\\\_Learning](https://www.researchgate.net/publication/2346087_Advances_in_Kernel_Methods_-_Support_Vector_Learning)\
    \ (accessed on 28 October 2023).\n- <span id=\"page-20-8\"></span>85. Müller,\
    \ K.R.; Mika, S.; Tsuda, K.; Schölkopf, K. An introduction to kernel‑based learning\
    \ algorithms. *IEEE Trans. Neural Netw.* **2001**, *12*, 181–201.[[CrossRef](https://doi.org/10.1109/72.914517)]\
    \ [\\[PubMed](https://www.ncbi.nlm.nih.gov/pubmed/18244377)]\n- <span id=\"page-20-9\"\
    ></span>86. Huang, K.; Carulli, J.M.; Makris, Y. Parametric counterfeit IC detection\
    \ via Support Vector Machines. In Proceedings of the 2012 IEEE International Symposium\
    \ on Defect and Fault Tolerance in VLSI and Nanotechnology Systems (DFT), Austin,\
    \ TX, USA, 3–5 October 2012; pp. 7–12.\n- <span id=\"page-20-10\"></span>87. Kent,\
    \ M.; Huynh, N.K.; Schiavon, S.; Selkowitz, S. Using support vector machine to\
    \ detect desk illuminance sensor blockage for closed‑loop daylight harvesting.\
    \ *Energy Build.* **2022**, *274*, 112443. [\\[CrossRef\\]](https://doi.org/10.1016/j.enbuild.2022.112443)\n\
    - <span id=\"page-20-11\"></span>88. Alam, M.A.; Mahapatra, S. A comprehensive\
    \ model of PMOS NBTI degradation. *Microelectron. Reliab.* **2005**, *45*, 71–81.\
    \ [\\[Cross‑](https://doi.org/10.1016/j.microrel.2004.03.019) [Ref\\]](https://doi.org/10.1016/j.microrel.2004.03.019)\n\
    - 89. Bhardwaj, S.; Wang, W.; Vattikonda, R.; Cao, Y.; Vrudhula, S. Predictive\
    \ modeling of the NBTI effect for reliable design. In Proceedings of the Custom\
    \ Integrated Circuits Conference, San Jose, CA, USA, 10–13 September 2006; pp.\
    \ 189–192. [\\[CrossRef\\]](https://doi.org/10.1109/CICC.2006.320885)\n- 90. Kumar,\
    \ S.V.; Kim, C.H.; Sapatnekar, S.S. An analytical model for negative bias temperature\
    \ instability. In Proceedings of the IEEE/ACM International Conference on Computer‑Aided\
    \ Design, Digest of Technical Papers, ICCAD, San Jose, CA, USA, 5–8 November 2006;\
    \ pp. 493–496.[[CrossRef](https://doi.org/10.1109/ICCAD.2006.320163)]\n- <span\
    \ id=\"page-20-12\"></span>91. Vattikonda, R.; Wang, W.; Cao, Y. Modeling and\
    \ minimization of PMOS NBTI effect for robust nanometer design. *Proc. Des. Autom.\
    \ Conf.* **2006**, 1047–1052.[[CrossRef](https://doi.org/10.1145/1146909.1147172)]\n\
    - <span id=\"page-20-13\"></span>92. Wang, W.; Wei, Z.; Yang, S.; Cao, Y. An efficient\
    \ method to identify critical gates under circuit aging. In Proceedings of the\
    \ IEEE/ACM International Conference on Computer‑Aided Design, Digest of Technical\
    \ Papers, ICCAD, San Jose, CA, USA, 4–8 November 2007; pp. 735–740.[[CrossRef](https://doi.org/10.1109/ICCAD.2007.4397353)]\n\
    - <span id=\"page-20-14\"></span>93. Rührmair, U.; Devadas, S.; Koushanfar, F.\
    \ Security Based on Physical Unclonability and Disorder. In *Introduction to Hardware\
    \ Security and Trust*; Springer: New York, NY, USA, 2012; pp. 65–102.\n- 94. Alkabani,\
    \ Y.; Koushanfar, F.; Kiyavash, N.; Potkonjak, M. Trusted Integrated Circuits:\
    \ A Nondestructive Hidden Characteristics Extraction Approach. In *Information\
    \ Hiding: 10th International Workshop, IH 2008, Santa Barbara, CA, USA, 19–21\
    \ May 2008*; Revised Selected Papers; Springer: Berlin/Heidelberg, Germany, 2008;\
    \ pp. 102–117.\n- <span id=\"page-20-15\"></span>95. Gassend, B.; Lim, D.; Clarke,\
    \ D.; Van Dijk, M.; Marten; Devadas, S. Identification and authentication of integrated\
    \ circuits. *Concurr. Comput.* **2004**, *16*, 1077–1098.[[CrossRef](https://doi.org/10.1002/cpe.805)]\n\
    - <span id=\"page-20-16\"></span>96. Zhang, X.; Xiao, K.; Tehranipoor, M. Path‑delay\
    \ fingerprinting for identification of recovered ICs. In Proceedings of the 2012\
    \ IEEE International Symposium on Defect and Fault Tolerance in VLSI and Nanotechnology\
    \ Systems (DFT), Austin, TX, USA, 3–5 October 2012.\n- <span id=\"page-20-17\"\
    ></span>97. Arjona, R.; Prada‑Delgado, M.A.; Arcenegui, J.; Baturone, I. A PUF‑\
    \ and Biometric‑Based Lightweight Hardware Solution to Increase Security at Sensor\
    \ Nodes. *Sensors* **2018**, *18*, 2429. [\\[CrossRef](https://doi.org/10.3390/s18082429)]\n\
    - <span id=\"page-20-18\"></span>98. Chakraborty, R.S.; Narasimhan, S.; Bhunia,\
    \ S. Hardware Trojan: Threats and emerging solutions. In Proceedings of the 2009\
    \ IEEE International High Level Design Validation and Test Workshop, San Francisco,\
    \ CA, USA, 4–6 November 2009; pp. 166–171.\n- <span id=\"page-20-19\"></span>99.\
    \ Khalafalla, M.; Gebotys, C. PUFs Deep Attacks: Enhanced modeling attacks using\
    \ deep learning techniques to break the security of double arbiter PUFs. In Proceedings\
    \ of the 2019 Design, Automation & Test in Europe Conference & Exhibition (DATE),\
    \ Florence, Italy, 25–29 March 2019; pp. 204–209.\n- <span id=\"page-20-20\"></span>100.\
    \ Taneja, S.; Rajanna, V.K.; Alioto, M. In‑Memory Unified TRNG and Multi‑Bit PUF\
    \ for Ubiquitous Hardware Security. *IEEE J. Solid‑State Circuits* **2022**, *57*,\
    \ 153–166. [\\[CrossRef\\]](https://doi.org/10.1109/JSSC.2021.3125255)\n- <span\
    \ id=\"page-20-21\"></span>101. Servadei, L.; Zennaro, E.; Devarajegowda, K.;\
    \ Manzinger, M.; Ecker, W.; Wille, R. Accurate Cost Estimation of Memory Systems\
    \ Inspired by Machine Learning for Computer Vision. In Proceedings of the 2019\
    \ Design, Automation & Test in Europe Conference & Exhibition (DATE), Florence,\
    \ Italy, 25–29 March 2019; pp. 1277–1280.\n- <span id=\"page-21-0\"></span>102.\
    \ Neto, W.L.; Austin, M.; Temple, S.; Amaru, L.; Tang, X.; Gaillardon, P.E. LSOracle:\
    \ A Logic Synthesis Framework Driven by Ar‑ tificial Intelligence: Invited Paper.\
    \ In Proceedings of the 2019 IEEE/ACM International Conference on Computer‑Aided\
    \ Design (ICCAD), Westminster, CO, USA, 4–7 November 2019; pp. 1–6.\n- <span id=\"\
    page-21-1\"></span>103. Xie, Z.; Huang, Y.H.; Fang, G.Q.; Ren, H.; Fang, S.Y.;\
    \ Chen, Y.; Hu, J. RouteNet: Routability prediction for Mixed‑Size Designs Using\
    \ Convolutional Neural Network. In Proceedings of the 2018 IEEE/ACM International\
    \ Conference on Computer‑Aided Design (ICCAD), San Diego, CA, USA, 5–8 November\
    \ 2018; pp. 1–8.\n- <span id=\"page-21-2\"></span>104. Ma, Y.; Ren, H.; Khailany,\
    \ B.; Sikka, H.; Luo, L.; Natarajan, K.; Yu, B. High Performance Graph Convolutional\
    \ Networks with Applications in Testability Analysis. In Proceedings of the 56th\
    \ Annual Design Automation Conference 2019, Las Vegas, NV, USA, 2–6 June 2019.\n\
    - <span id=\"page-21-3\"></span>105. Regazzoni, F.; Bhasin, S.; Pour, A.A.; Alshaer,\
    \ I.; Aydin, F.; Aysu, A.; Beroulle, V.; Di Natale, G.; Franzon, P.; Hely, D.;\
    \ et al. Machine Learning and Hardware Security: Challenges and Opportunities.\
    \ In Proceedings of the 39th International Conference on Computer‑Aided Design,\
    \ San Diego, CA, USA, 2–5 November 2020.\n- <span id=\"page-21-4\"></span>106.\
    \ Yu, S.Y.; Yasaei, R.; Zhou, Q.; Nguyen, T.; Al Faruque, M.A. HW2VEC: A Graph\
    \ Learning Tool for Automating Hardware Security. In Proceedings of the 2021 IEEE\
    \ International Symposium on Hardware Oriented Security and Trust (HOST), San\
    \ Jose, CA, USA, 13–14 December 2021; pp. 13–23.\n- <span id=\"page-21-5\"></span>107.\
    \ Koushanfar, F. Provably Secure Active IC Metering Techniques for Piracy Avoidance\
    \ and Digital Rights Management. *IEEE Trans. Inf. Forensics Secur.* **2012**,\
    \ *7*, 51–63. [\\[CrossRef\\]](https://doi.org/10.1109/TIFS.2011.2163307)\n- <span\
    \ id=\"page-21-6\"></span>108. Wei, S.; Nahapetian, A.; Potkonjak, M. Robust passive\
    \ hardware metering. In Proceedings of the 2011 IEEE/ACM International Conference\
    \ on Computer‑Aided Design (ICCAD), San Jose, CA, USA, 7–10 November 2011; pp.\
    \ 802–809.\n- <span id=\"page-21-7\"></span>109. Cui, A.; Yang, Y.; Qu, G.; Li,\
    \ H. A Secure and Low‑overhead Active IC Metering Scheme. In Proceedings of the\
    \ 2019 IEEE 37th VLSI Test Symposium (VTS), Monterey, CA, USA, 23–25 April 2019;\
    \ pp. 1–6.\n- <span id=\"page-21-8\"></span>110. Contreras, G.K.; Rahman, M.T.;\
    \ Tehranipoor, M. Secure Split‑Test for preventing IC piracy by untrusted foundry\
    \ and assembly. In Proceedings of the IEEE International Symposium on Defect and\
    \ Fault Tolerance in VLSI Systems 2013, New York, NY, USA, 2–4 October 2013; pp.\
    \ 196–203.[[CrossRef](https://doi.org/10.1109/DFT.2013.6653606)]\n\n**Disclaimer/Publisher's\
    \ Note:** The statements, opinions and data contained in all publications are\
    \ solely those of the individual au‑ thor(s) and contributor(s) and not of MDPI\
    \ and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for\
    \ any injury to people or property resulting from any ideas, methods, instructions\
    \ or products referred to in the content."
- title: A Survey of Aging Monitors and Reconfiguration Techniques
  abstract: CMOS technology scaling makes aging effects an important concern for the
    design and fabrication of integrated circuits. Aging deterioration reduces the
    useful life of a circuit, making it fail earlier. This deterioration can affect
    all portions of a circuit and impacts its performance and reliability. Contemporary
    literature shows solutions to monitor and mitigate aging using hardware and software
    monitoring mechanisms and reconfiguration techniques. The goal of this review
    of the state-of-the-art is to identify existing monitoring and reconfiguration
    solutions for aging. This survey evaluates the aging research, focusing the years
    from 2012 to 2019, and proposes a classification for monitors and reconfiguration
    techniques. Results show that the most common monitor type used for aging detection
    is to monitor timing errors, and the most common reconfiguration technique used
    to deal with aging is voltage scaling. Furthermore, most of the literature contributions
    are in the digital field, using hardware solutions for monitoring aging in circuits.
    There are few literature contributions in the analog area, being the scope of
    this survey in the digital domain. By scrutinizing these solutions, this survey
    points directions for further research and development of aging monitors and reconfiguration
    techniques.
  keywords: Aging monitors, reconfiguration techniques, survey.
  document: '#### I. INTRODUCTION


    With the scaling of CMOS technology circuit reliability issues become increasingly
    relevant for the design of integrated circuits. Among these issues, circuit aging
    is getting critical, as it inevitably affects all circuits. The literature shows
    an increase in the number of papers related to aging between 2000 to 2020 [\[1\]](#page-15-0),
    [\[2\]](#page-15-1), [\[3\]](#page-15-2). This trend is due to the development
    of ultra-deep submicron technologies, where reliability became crucial [\[4\]](#page-15-3).


    Aging is the deterioration of circuit performance over time [\[5\]](#page-15-4),
    which can reduce the useful life of a circuit. This deterioration can increase
    circuit delay and affect all portions of a System-on-Chip (SoC), analog circuit,
    digital logic, and memory. One important factor that accelerates aging is power.
    The increase of power in modern circuits increases the temperature and makes these
    circuits susceptible to effects like bias temperature instability (BTI) and hot
    carrier injection (HCI) [\[6\]](#page-15-5).


    One solution to deal with aging effects is to add design safety margins to the
    circuit, such as clock margins. These margins ensure correct operation even in
    the presence of aging effects once they are designed according to worst-case conditions.
    However, these margins decrease performance, as they increase the clock period.
    The literature presents other solutions to mitigate aging, allowing to extend
    the lifetime of chips. These solutions monitor parameters that indicate aging
    effects. The monitored parameters include temperature, frequency variation, delay
    variation, among others. Decision methods, as threshold voltage analysis, evaluate
    whether the monitored parameters have an appropriate range of values. Activation
    mechanisms, such as voltage and frequency adaptations, bring the circuit back
    to the safe parameters if these do not meet predefined constraints.


    The *goal* of this review is to study existing solutions for monitoring aging
    effects in circuits and dealing with them. This study seeks to answer two questions:
    *(i)* what solutions the literature presents for aging monitoring? and *(ii)*
    what solutions the literature presents for circuit reconfiguration? This work
    surveys the literature from 2012 to 2019, while works [\[7\]](#page-15-6), [\[8\]](#page-15-7),
    [\[9\]](#page-15-8) map aging monitors since 1998. Thus, this survey fills the
    gap related to aging surveys, covering the most recent works.


    The literature review considered three main search terms:


    • *Integrated Circuits*: this category limits the search to the area of integrated
    circuits. The search string presents the terms used to reference this area, like
    VLSI, system, and architectures;


    - *Monitoring and Self-reconfigurable Circuits*: this category includes the monitoring
    area and the problems induced by aging, as Negative-Bias Temperature Instability
    (NBTI);

    - *Aging*: this category limits the research by aging problems.


    The Scopus abstract and citation database [\(https://www.scopus.com\)](https://www.scopus.com)
    was the start point of the research. After that, the review researched in IEEE
    [\(https://ieeexplore.ieee.org\)](https://ieeexplore.ieee.org) and ACM [\(https://www.acm.org\)](https://www.acm.org)
    databases. Finally, the research focused on Google Scholar [\(https://scholar.google.com\)](https://scholar.google.com)
    database. The review considered journals and conference papers published since
    2014.


    This survey is organized as follows. Section [II](#page-1-0) presents basic definitions
    to the understating of this work. Section [III](#page-1-1) presents a review of
    other surveys available in the literature. Section [IV](#page-2-0) presents the
    proposed classification for the reviewed papers. Section [V](#page-4-0) discusses
    comparatively the reviewed works. Section [VI](#page-6-0) presents in detail the
    literature survey. Section [VII](#page-14-0) concludes this survey.


    #### II. BASIC DEFINITIONS


    <span id="page-1-0"></span>This section provides definitions required for the
    understanding of this survey.


    <span id="page-1-9"></span>Definition 1. *Critical path*: is the purely combinational
    path with the most significant delay between any two registers or primary I/O
    ports in a design.


    <span id="page-1-8"></span>Definition 2. *Soft-errors*: errors that can occur
    in a circuit when it is exposed to radiation, like cosmic rays and neutrons particles.
    Single-event upset (SEU) is an example of soft-error, when the exposition to radiation
    can generate a change in the memory elements values, generating erroneous outputs
    [\[10\]](#page-15-9).


    Important effects are considered in the aging research: NBTI (Definition [3\)](#page-1-2),
    PBTI (Definition [4\)](#page-1-3), HCI (Definition [5\)](#page-1-4).


    <span id="page-1-2"></span>Definition 3. *NBTI* – Negative Bias Temperature Instability:
    increase in the threshold voltage and consequently decrease drain current and
    transconductance of a transistor [\[11\]](#page-15-10). It occurs in PMOS transistors
    and is caused by circuit aging.


    <span id="page-1-3"></span>Definition 4. *PBTI* – Positive Bias Temperature Instability:
    similar to NBTI, but in NMOS transistors. Since the introduction of the high-K
    gate dielectrics and metal gates transistors, the effect of PBTI becomes comparable
    to the NBTI one.


    <span id="page-1-4"></span>Definition 5. *HCI* – Hot-Carrier Injection: consists
    of a voltage drop that produces a large electric field in the region near to the
    drain of a transistor in saturation mode. It can result in the change of transistor
    characteristics such as the threshold voltage [\[12\]](#page-15-11). According
    to Novak et al., from Intel, [\[13\]](#page-16-0), tri-gate technologies as 14nm
    can help to mitigate HCI effects toghter with body bias adjust techniques.


    Classic test approaches are adopted for aging monitoring and reconfiguration techniques:
    DfT (Definition [6\)](#page-1-5), BIST (Definition [7\)](#page-1-6), ATPG (Definition
    [8\)](#page-1-7).


    <span id="page-1-5"></span>Definition 6. *DfT* – Design for Testability: a set
    of techniques used to improve circuit testability by increasing controllability
    and observability in the design [\[14\]](#page-16-1).


    <span id="page-1-6"></span>Definition 7. *BIST* – Built-in Self-Test: a mechanism
    that tests the circuit itself, verifying all or a portion of the internal functionality
    of the design [\[14\]](#page-16-1). The hardware and/or the software is built
    into integrated circuits allowing them to test their operation. The main advantage
    of BIST is the ability to test internal circuits having no direct connections
    to external pins or external testers. In addition, BIST allows testing in the
    field.


    <span id="page-1-7"></span>Definition 8. *ATPG* –Automatic Test Pattern Generation:
    a method that finds an input sequence (called test vectors) that enables automatic
    test equipment (ATE) to distinguish between the correct circuit behavior and the
    faulty circuit behavior caused by defects.


    #### III. PREVIOUS AGING SURVEYS


    <span id="page-1-1"></span>Rahimipour et al. [\[7\]](#page-15-6) review on-chip
    monitors for temperature, soft-errors (Definition [2\)](#page-1-8), and critical
    paths (Definition [1\)](#page-1-9), covering the period 1998-2011 (21 works).
    Figure [1\(](#page-2-1)a) presents their classification divided in: monitors for
    high temperature induced problems; monitors for soft-errors; monitors for critical
    paths; and collaborative monitors, which combines the previously mentioned class
    of monitors. Soft-errors detects change caused in memory elements due radiation.
    Thermal monitors identify problems induced by high temperatures. Critical path
    delay monitors detect changes in the critical path, as delay increases. Collaborative
    monitors are a combination of more than one class of monitors. Figure [1\(](#page-2-1)b)
    summarizes the monitor types and also informs cause, effect, and control action
    for each effect. Juracy *et al.*: A Survey of Aging Monitors and Reconfiguration
    Techniques


    ![](_page_2_Figure_1.jpeg)


    | Magnitude     | Cause                                              | Adverse
    Effect                         | Control Action       |

    |---------------|----------------------------------------------------|----------------------------------------|----------------------|

    |               | Localized                                          | Performance
    /                          | DVFS /               |

    | Temperature   | power                                              | Reliability                            |
    Clock                |

    |               | consumption                                        | loss                                   |
    throttling           |

    | Critical Path | Timing<br>uncertainties                            | Speed loss                             |
    DVFS                 |

    | Soft Error    | ncreasing in<br>system<br>operating<br>frequencies | Reliability<br>loss                    |
    Backward<br>recovery |

    | Aging         | Prolonged<br>Usage                                 | Speed<br>reduction/<br>Malfunction     |
    DVFS                 |

    |               |                                                    | (b) Monitor
    types and control actions. |                      |


    (a) Typical examples of monitors.


    <span id="page-2-1"></span>**FIGURE 1.** Monitors types and control actions. Adapted
    from [4]. Fig. 1. Monitors types and control actions. Adapted from [\[7\]](#page-15-6).


    (controlled resource wearout), or availability of expendable resources (spatial
    redundancy). **AGING ADAPTATION AND MITIGATION TECHNIQUES WORST-CASE DESIGN DESIGN
    TIME AGING-AWARE BALANCING DYNAMIC ADAPTATION TECHNIQUES ADAPTIVE RESOURCE MANAGEMENT
    VOLTAGE MARGIN GATE SIZING FREQUENCY MARGIN GATES PATHS INSTRUCTION PIPELINE STAGES
    VOLTAGE AND FREQUENCY SCALE COMPUTATIONAL SPRITING CONTROLLED RESOURCE WEAROUT
    ITL SCHEME SPATIAL REDUNDANCY** system clock and the same path controlled by a
    delayed clock [11]; • *Clock frequency*: clock frequency monitors measure the
    frequency variation of a clock circuit under aging. This type of monitor uses
    a reference frequency to identify a change in the circuit operating frequency;
    • *Workload*: workload monitors measure the workload of a circuit to identify
    the stress level. Similar to the CPU load measure. Khoshavi et al. [\[8\]](#page-15-7)
    review the period 2004-2015 (30 works). Their work analyzes aging monitors and
    also aging models and techniques for aging mitigation. Aging models are used to
    predict the degradation of the circuit due to aging. Mitigation techniques are
    used to deal with aging effects and ensure correct behavior of the circuit under
    these effects. Besides, their work proposes a taxonomy to classify the aging mitigation
    techniques, shown in Figure [2.](#page-3-0) Worst-case design techniques add safety
    margins to the circuit characteristics, like frequency and supply voltage, at
    design-time. Design time aging-aware balancing focuses on balancing circuit delay
    to reduce aging effects. Dynamic adaptation techniques are online approaches to
    tune the design under aging during circuit operation. Adaptive resource management
    techniques mitigate the aging effects either through the management of idle time
    (Idle-Time Leveraging schemes, also called ITL schemes), power management and
    task scheduling (controlled resource wearout), or availability of expendable resources
    (spatial redundancy).


    **FIGURE 2.** Taxonomy of aging mitigation techniques. Adapted from [5]. Kochte
    et al. [6] review the period 2007-2017 (14 works). Their work analyses self-test,
    self-checking, and self-diagnosis online techniques for self-awareness. Online
    • *Circuit state*: circuit state monitors consider the state of the entire system.
    For example, some techniques use the BIST circuit output to evaluate aging effects;
    • *Voltage*: Voltage monitors analyze the threshold voltage of the circuit. Aging
    effects as NBTI can change the threshold voltage of transistors. Kochte et al.
    [\[9\]](#page-15-8) review the period 2007-2017 (14 works). Their work analyses
    self-test, self-checking, and selfdiagnosis online techniques for self-awareness.
    Online techniques include aging monitors. Figure [3](#page-3-1) shows the classification
    for self-test and self-checking, which contains non-concurrent and concurrent
    approaches. Non-concurrent approaches regard self-testing methods, including classic
    methods, as BIST, and techniques like suspending the circuit operation to execute
    the test. Concurrent approaches are self-checking methods executed during the
    circuit operation, which includes aging monitors.


    #### techniques include aging monitors. Figure 3 shows the classification for
    self-test and self-checking, which contains non-Table 1 presents the aging monitors
    addressed in this IV. PROPOSED CLASSIFICATION OF AGING MONITORS AND RECONFIGURATION
    TECHNIQUES


    <span id="page-2-0"></span>concurrent and concurrent approaches. Non-concurrent
    approaches regard self-testing methods, including classic methods, as BIST, and
    techniques like suspending the circuit survey and compares with those covered
    by previous works. Note that the Table does not include survey [5] because it
    does not consider aging monitors. Our work addresses eight Our proposed classification
    extends the taxonomy of previous work [\[7\]](#page-15-6), [\[8\]](#page-15-7),
    [\[9\]](#page-15-8), considering parameters included in the state-of-the-art and
    features of the circuits monitored by them. Thus, this work proposes the following
    set of parameters for aging monitors classification:


    > operation to execute the test. Concurrent approaches are self-checking methods
    executed during the circuit operation,


    > **IV. PROPOSED CLASSIFICATION OF AGING MONITORS**


    Our proposed classification extends the taxonomy of previous work [4]–[6], considering
    parameters included in the state-of-the-art and features of the circuits monitored
    by them. Thus, this work proposes the following set of parame-


    • *Temperature*: a temperature monitor detects aging effects by measuring the
    variation in the temperature of the circuit. Temperature variations cause aging
    effects


    • *Critical path*: critical path monitors can be used to identify timing errors
    due to wrong transitions, SEU,


    which includes aging monitors.


    **AND RECONFIGURATION TECHNIQUES**


    ters for aging monitors classification:


    as NBTI and PBTI;


    **NON-CONCURRENT**


    **POWER-ON ON-DEMAND PERIODICAL**


    **SELF TEST MEMORY BIST**


    **SELF-TEST USING STORED DETERMINISTIC PATTERNS**


    **BIST**


    **SOFTWARE-BASED**


    works.


    uration techniques:


    VOLUME 4, 2016 3


    **ONLINE**


    **SELF-TESTING SELF-CHECKING**


    **FIGURE 3.** Self-test and self-checking circuits classification. Adapted from
    [6].


    monitors types, including monitors not covered by previous


    **TABLE 1.** Comparison among the reviewed surveys related to aging monitor types,
    and this survey coverage. Legend: "A": addressed, "NA": not addressed.


    Critical Path: Timing Errors NA A A


    Temperature AA A


    Workload NA A A Circuit State NA NA A Voltage NA NA A


    Critical Path: SEU AA A Critical Path: Delay A NA A Clock Frequency NA NA A


    This work adopts the following classification of reconfig-


    • *Dynamic voltage scaling*: Dynamic Voltage Scaling


    [4] [6] This Survey


    or delay increase on circuit paths. These errors are detectable by comparing a
    circuit path controlled by the


    **SELF-TEST AND SELF-CHECK METHODS**


    **CONCURRENT**


    **WATCHDOG**


    **ALGORITHM-BASED FAULT TOLERANCE**


    **NON-FUNCTIONAL MONITORS**


    **CONCURRENT BIST SELF CHECKING DESIGN / CONCURRENT ERROR DETECTION LOCKSTEP EXECUTION
    SYNTHESIZED ASSERTIONS**


    ![](_page_3_Figure_0.jpeg)


    <span id="page-3-0"></span>Fig. 2. Taxonomy of aging mitigation techniques. Adapted
    from [\[8\]](#page-15-7).


    ![](_page_3_Figure_2.jpeg)


    #### **SELF-TEST AND SELF-CHECK METHODS**


    <span id="page-3-1"></span>Fig. 3. Self-test and self-checking circuits classification.
    Adapted from [\[9\]](#page-15-8).


    - *Temperature*: a temperature monitor detects aging effects by measuring the
    variation in the temperature of the circuit. Temperature variations cause aging
    effects as NBTI and PBTI;

    - *Critical path*: critical path monitors can be used to identify timing errors
    due to wrong transitions, SEU, or delay increase on circuit paths. These errors
    are detectable by comparing a circuit path controlled by the system clock and
    the same path controlled by a delayed clock [\[15\]](#page-16-2);

    - *Clock frequency*: clock frequency monitors measure the frequency variation
    of a clock circuit under aging. This type of monitor uses a reference frequency
    to identify a change in the circuit operating frequency;

    - *Workload*: workload monitors measure the workload of a circuit to identify
    the stress level. Similar to the CPU load measure.

    - *Circuit state*: circuit state monitors consider the state of the entire system.
    For example, some techniques use the BIST circuit output to evaluate aging effects;

    - *Voltage*: Voltage monitors analyze the threshold voltage of the circuit. Aging
    effects as NBTI can change the threshold voltage of transistors.


    Table [I](#page-4-1) presents the aging monitors addressed in this survey and
    compares with those covered by previous works. Note that the Table does not include
    survey [\[8\]](#page-15-7) because it does not consider aging monitors. Our work
    addresses eight monitors types, including monitors not covered by previous works.


    This work adopts the following classification of reconfiguration techniques:


    • *Dynamic voltage scaling*: Dynamic Voltage Scaling (DVS) is a power management
    technique where the voltage used in a component increases or decreases, according
    to some criteria [\[16\]](#page-16-3). An example of an approach to manage power
    dissipation is the adoption of a closed-loop control technique where the voltage
    is a knob to


    <span id="page-4-1"></span>TABLE I COMPARISON AMONG THE REVIEWED SURVEYS RELATED
    TO AGING MONITOR TYPES, AND THIS SURVEY COVERAGE. LEGEND: "A": ADDRESSED, "NA":
    NOT ADDRESSED.


    |                              | [7] | [9] | This Survey |

    |------------------------------|-----|-----|-------------|

    | Temperature                  | A   | A   | A           |

    | Critical Path: Timing Errors | NA  | A   | A           |

    | Critical Path: SEU           | A   | A   | A           |

    | Critical Path: Delay         | A   | NA  | A           |

    | Clock Frequency              | NA  | NA  | A           |

    | Workload                     | NA  | A   | A           |

    | Circuit State                | NA  | NA  | A           |

    | Voltage                      | NA  | NA  | A           |


    meet the power goal [\[17\]](#page-16-4).


    - *Dynamic frequency scaling*: Dynamic frequency scaling (DFS) is a technique
    similar to DVS, but applied to the circuit frequency. Similarly, if the circuit
    needs a boost in performance, the frequency is increased. If the circuit or application
    can tolerate lower performance, the frequency may be decreased to allow power
    savings;

    - *Aging compensation*: Aging compensation is a technique that enables the circuit
    to alleviate aging effects. For example, some circuits activate extra devices,
    in parallel to the main circuit, to increase the driving strength of an output
    driver, compensating the degradation due to HCI and BTI effects [\[18\]](#page-16-5);

    - *Body-bias adaptive*: Body-bias adaptive (BBA) is a technique that allows tuning
    the transistor threshold voltage [\[19\]](#page-16-6). This technique helps to
    mitigate and compensate for the NBTI impact in the circuit;

    - *Workload reduction*: Workload reduction is a series of software approaches
    to reduce the workload system by introducing, for instance, no operation (NOP)
    instructions during the system operation.


    Table [II](#page-4-2) compares reconfiguration approaches covered by the previous
    works, and the ones addressed in this survey. The Table does not include survey
    [\[9\]](#page-15-8) because it does not address reconfiguration techniques. Also,
    note that this survey does not address three types of reconfiguration techniques
    (clock throttling, backward recovering and computational sprinting) that were
    covered in the previous surveys because these techniques were not adopted in the
    research papers from 2012 to 2019. Our work addresses five reconfiguration approaches,
    including two types not covered by previous works.


    <span id="page-4-2"></span>TABLE II COMPARISON AMONG THE REVIEWED SURVEYS RELATED
    TO RECONFIGURATION TECHNIQUES TYPES, AND THIS SURVEY COVERAGE. LEGEND: "A": ADDRESSED,
    "NA": NOT ADDRESSED.


    |                           | [7] | [8] | This Survey |

    |---------------------------|-----|-----|-------------|

    | Dynamic Voltage Scaling   | A   | A   | A           |

    | Dynamic Frequency Scaling | A   | A   | A           |

    | Aging Compensation        | NA  | A   | A           |

    | Body Bias Adaptive        | NA  | NA  | A           |

    | Workload Reduction        | NA  | NA  | A           |

    | Clock Throttling          | A   | NA  | NA          |

    | Backward Recovering       | A   | NA  | NA          |

    | Computational Sprinting   | NA  | A   | NA          |


    ### V. DISCUSSION RELATED TO THE STATE-OF-THE-ART


    <span id="page-4-0"></span>This Section brings a summary of the presented techniques,
    remarks, and insights about how to deal with aging. Also, this Section answers
    the research questions presented in the Introduction Section. The classified state-of-the-art
    works are described in Section [VI.](#page-6-0)


    About industry, most applications rely on sensors built in SoCs that allow measuring
    variations in such parameters as the circuit ages. These sensors are commonly
    distributed across the die and accessible through DfT infrastructure or as peripherals
    to CPUs. The most common sensor consists of a set of ring oscillators that control
    asynchronous counters. These counters provide an overview of how the overall speed
    of the circuit is being impacted by aging [\[20\]](#page-16-7).


    # *A. Summary and remarks of the literature review*


    Table [III](#page-6-1) summarizes the reviewed works. The "Overall Monitoring"
    column means monitoring the entire circuit, not just the critical paths using,
    for example, the voltage or the current. The "Monitor Insertion Strategy" column
    corresponds to approaches that use some strategy to insert the monitors, such
    as statistical methods, and not based only on critical paths. The "Structure Reuse"
    column shows designs that use structures available in the circuit for monitoring
    aging effects. In these cases, only DfT structures are reused. The "Metastability
    Concern" column contains works concerned with metastability issues.


    Circuits monitoring the overall system may present a low area overhead, once one
    mechanism can be applied for the entire design. This approach may be better in
    terms of area overhead when compared to solutions focusing on inserting monitors
    in all critical paths. However, designs that use methods to select paths to insert
    monitor are also promising in terms of area overhead reduction. The reuse of DfT
    structures is a promising strategy to choose paths, once test insertion overhead
    is already present in the circuit. This allows reducing the impact of aging monitors
    on area.


    A "*no*" in the first column (Overall Monitoring) means that only part of the
    system is monitored. This means that specialized mechanisms may be required for
    different parts of the system. Particularly, works with a "*no*" in the second
    and/or third columns in Table [III](#page-6-1) (Monitor Insertion Strategy and
    Structure Reuse) can present a significant overhead in the system. Nevertheless,
    these solutions may be useful for detecting aging and could be combined with path
    selection strategies to reduce the system overhead, making their use feasible.


    Metastability is a concern present in only two works. It is an important issue,
    once its effects may propagate through designs, reducing circuit reliability,
    making them fail even in the absence of aging effects. Metastability can also
    be an issue to systems with more than one clock domain, due to the clock synchronization
    between domains. Thus, these two works have an advantage compared to other approaches
    since they use the same circuitry to deal with both aging and metastability effects.


    A feature that can be observed is that voltage and frequency scaling are reconfiguration
    techniques associated mostly with critical path monitors, once it is possible
    to control the circuit speed by these two parameters. Similarly, body bias adjustment
    is associated with voltage monitors, once this reconfiguration technique changes
    the threshold voltage for compensating aging effects.


    #### *B. Remarks about aging monitors*


    Figure [4](#page-7-0) and Table [IV](#page-7-1) present the answer to the first
    research question of this survey, "What solutions the literature presents for
    aging monitoring?". The Figure shows the monitor types covered by this survey.
    The most common monitor type used for aging detection are timing error monitors,
    which is present in 36.58% of the reviewed papers. The second most common monitor
    type used for aging detection is temperature monitor, which is present in 27%
    of the papers.


    # *C. Remarks about reconfiguration techniques*


    This Section answers the second research question of this survey: "What solutions
    the literature presents for circuit reconfiguration?". Figure [5](#page-7-2) and
    Table [V](#page-7-3) present the reconfiguration techniques covered in this survey.
    According to the Table, voltage scaling is the most adopted reconfiguration technique,
    present in 50% of the papers about reconfiguration techniques. The second most
    common reconfiguration technique used for aging detection is frequency scaling,
    which is present in 28.57% of the papers.


    <span id="page-6-1"></span>


    |            | Overall<br>Monitoring | Monitor Insertion<br>Strategy | Structure<br>Reuse
    | Metastability<br>Concern |

    |------------|-----------------------|-------------------------------|--------------------|--------------------------|

    | [21], 2015 | yes                   | no                            | no                 |
    no                       |

    | [22], 2017 | yes                   | no                            | no                 |
    no                       |

    | [23], 2017 | yes                   | no                            | no                 |
    no                       |

    | [24], 2017 | yes                   | no                            | no                 |
    no                       |

    | [25], 2015 | yes                   | no                            | no                 |
    no                       |

    | [26], 2016 | yes                   | no                            | no                 |
    no                       |

    | [18], 2014 | yes                   | no                            | no                 |
    no                       |

    | [27], 2016 | yes                   | no                            | no                 |
    no                       |

    | [28], 2019 | yes                   | no                            | no                 |
    no                       |

    | [29], 2015 | no                    | no                            | yes                |
    no                       |

    | [30], 2017 | no                    | no                            | no                 |
    no                       |

    | [31], 2018 | no                    | no                            | no                 |
    yes                      |

    | [32], 2016 | no                    | no                            | no                 |
    no                       |

    | [33], 2015 | yes                   | no                            | no                 |
    no                       |

    | [34], 2017 | no                    | no                            | no                 |
    no                       |

    | [35], 2014 | no                    | no                            | no                 |
    no                       |

    | [36], 2017 | no                    | no                            | no                 |
    no                       |

    | [37], 2014 | no                    | yes                           | no                 |
    no                       |

    | [38], 2014 | no                    | yes                           | no                 |
    no                       |

    | [39], 2018 | no                    | no                            | no                 |
    no                       |

    | [40], 2017 | no                    | yes                           | no                 |
    no                       |

    | [41], 2015 | no                    | yes                           | no                 |
    no                       |

    | [42], 2019 | yes                   | no                            | no                 |
    no                       |

    | [43], 2019 | yes                   | yes                           | no                 |
    no                       |

    | [44], 2018 | no                    | no                            | no                 |
    no                       |

    | [45], 2012 | no                    | yes                           | no                 |
    no                       |

    | [46], 2014 | no                    | yes                           | no                 |
    no                       |

    | [47], 2014 | no                    | yes                           | no                 |
    no                       |

    | [48], 2015 | yes                   | no                            | no                 |
    no                       |

    | [49], 2016 | yes                   | no                            | no                 |
    no                       |

    | [50], 2014 | no                    | no                            | no                 |
    no                       |

    | [51], 2014 | no                    | yes                           | no                 |
    no                       |

    | [52], 2015 | yes                   | no                            | no                 |
    yes                      |

    | [53], 2017 | yes                   | no                            | no                 |
    no                       |

    | [54], 2015 | yes                   | no                            | yes                |
    no                       |

    | [55], 2015 | yes                   | no                            | yes                |
    no                       |

    | [56], 2019 | yes                   | no                            | no                 |
    no                       |

    | [57], 2017 | yes                   | no                            | yes                |
    no                       |

    | [58], 2015 | yes                   | no                            | no                 |
    no                       |

    | [59], 2015 | yes                   | no                            | no                 |
    no                       |

    | [60], 2018 | yes                   | no                            | no                 |
    no                       |


    TABLE III SUMMARY OF THE LITERATURE REVIEW.


    ## VI. LITERATURE REVIEW


    <span id="page-6-0"></span>This Section describes the papers related to aging
    monitors and reconfiguration techniques covering the years from 2012 to 2019.
    The subsections are grouped by monitoring approaches, according to the classification
    proposed in Section [IV](#page-2-0) for aging monitors. When a paper also presents
    a reconfiguration technique, it is described together, in the same paragraph.


    ![](_page_7_Figure_0.jpeg)


    <span id="page-7-1"></span><span id="page-7-0"></span>Fig. 4. Monitor types covered
    by the survey.


    TABLE IV CLASSIFICATION OF THE MONITORS'' WORKS ACCORDING TO THE PROPOSED CLASSIFICATION.


    | Types                             | Works |      |      |      |      |      |

    |-----------------------------------|-------|------|------|------|------|------|

    | Temperature                       | [21]  | [22] | [23] | [24] | [25] | [26]
    |

    |                                   | [18]  | [27] | [28] | [46] | [47] |      |

    | Critical path: timing errors      | [29]  | [30] | [31] | [32] | [33] | [34]
    |

    |                                   | [35]  | [36] | [37] | [38] | [39] | [40]
    |

    |                                   | [41]  | [42] | [43] |      |      |      |

    | Critical path: single event upset | [44]  |      |      |      |      |      |

    | Critical path: delay              | [45]  | [48] | [49] | [50] | [51] |      |

    | Clock frequency                   | [52]  | [53] |      |      |      |      |

    | Circuit state                     | [54]  | [55] | [56] | [57] |      |      |

    | Workload                          | [58]  |      |      |      |      |      |

    | Voltage                           | [59]  | [60] |      |      |      |      |


    ![](_page_7_Figure_4.jpeg)


    <span id="page-7-3"></span><span id="page-7-2"></span>Fig. 5. Reconfiguration
    techniques covered by the survey.


    TABLE V WORKS CLASSIFICATION REGARDING RECONFIGURATION TECHNIQUES.


    | Techiniques                                                                                          |                                      |              |              |
    Works        |      |      |      |

    |------------------------------------------------------------------------------------------------------|--------------------------------------|--------------|--------------|--------------|------|------|------|

    | Voltage scaling<br>Frequency scaling<br>Aging compensation<br>Body bias adjust<br>Workload
    reduction | [21]<br>[21]<br>[18]<br>[59]<br>[40] | [32]<br>[45] | [39]<br>[52]
    | [45]<br>[56] | [48] | [50] | [56] |


    #### *A. Temperature Monitors*


    NBTI, PBTI, and HCI are aging effects accelerated by temperature increase in chips.
    System workload affects power dissipation, which has a direct impact on the temperature.
    Thus, monitoring temperature helps to deal with these effects. Some sensors use
    ring-oscillators to capture the aging effects caused by temperature increase.
    This kind of sensor provides an indirect measurement of temperature, and can be
    considered a temperature monitor.


    Igarashi et al. [\[21\]](#page-16-8) propose an aging monitor implemented with
    ring-oscillator (RO) to measures BTI and AC hot-carrier-injection (AC-HCI). The
    monitor consists of a symmetric RO (SRO) and an asymmetric RO (ASRO). ASRO is
    an RO composed of standard cells of different drives, while SRO is implemented
    only by standard cells with the same driving strength. With these two types of
    RO, it is possible to separate NBTI and PBTI effects for analyses by observing
    them under DC stress conditions. Also, the speed degradation caused by AC-HCI
    can be detected because unbalanced delay with a long/short transition in ASRO
    has high sensitivity against AC-HCI under AC stress. A dynamic voltage and frequency
    scaling (DVFS) technique controlled by software is used to change the supply voltage
    and clock activity dynamically and reconfigure the circuit. A test chip, including
    both SRO and ASRO using NAND2 standard cells, was implemented in a 16 nm Fin-FET
    bulk CMOS technology. Results show that Vth shift due to PBTI measured from frequency
    degradation is 2mV, which is still 1/10 of NBTI in Fin-FET technology, and that
    is possible to reduce the BTI guard bands in 45% at the nominal frequency operation.


    Majerus et al. [\[22\]](#page-16-9) use ROs to measure changes in transistor and
    resistor parameters as a function of the stress caused by aging. The result is
    a data-driven aging model that provides information that can be used to ensure
    system reliability.


    Sengupta and Sapatnekar [\[23\]](#page-16-10) present two methods that use sensors
    implemented with ROs to detect the delay shifts in circuits as a result of BTI
    and HCI effects. The first method uses a pre-silicon analysis of the circuit to
    compute calibration factors that can translate the delay shifts in the ROs with
    a delay estimate of 1% of the real values. The second method uses an analysis
    where sensor measurements are combined with infrequent online delay measurements
    to reduce the circuit guard bands and allows 8% lower delay guard banding overheads
    compared to the conventional methods.


    Kim et al. [\[24\]](#page-16-11) propose a new test structure of RO that helps
    to measure the stress duty cycle (SDC) of the HCI. SDC is the ratio between the
    stress caused by the aging effect and the circuit cycle time [\[61\]](#page-17-17).
    The structure is composed of a NAND gate that has the function of enabling the
    oscillator mode and inverters. VDD and GND bias of the HCI stress inverter are
    set up in a complementary way during the stress, making the device not suffering
    from BTI aging. Also, a buffer is designed as a compensator for the signal falling
    by as much as Vth. Results demonstrated that HCI SDC increases with frequency,
    but the maximum duty cycle was much less than 2%.


    Shakya et al. [\[25\]](#page-16-12) propose an NBTI sensor that uses two ROs,
    one without circuit influence used as a reference and other to evaluate the degradation
    circuit under stress, and compare the output of both. This approach gives the
    manufacturer the exact control over the yield and accuracy of the sensors, which
    not occurs with ad hoc approaches that determine parameters such as the decision
    threshold.


    Miyake et al. [\[26\]](#page-16-13) propose an aging-tolerant monitor that analyzes
    frequencies of more than one RO. Thus, it is possible to derive the values for
    temperature and voltage from the frequencies using multiple regression analysis.
    Besides, three techniques to select the RO types are proposed to improve the accuracy
    of the measurement. The method was validated with simulations in 180 nm, 90 nm,
    and 45 nm CMOS technologies. In the 180 nm technology, temperature accuracy is
    about 0.99◦C, and voltage accuracy is about 4.17 mV. Also, the authors fabricated
    test chips with 180 nm CMOS technology to confirm its feasibility.


    Kumar [\[18\]](#page-16-5) presents an aging compensation technique for a CMOS
    transistor output driver. It contains an aging compensation cell which monitors
    the degradation in the ON current (ION ) of output driver due to the HCI and BTI
    effect. Based on this degradation, the aging compensation cell generates compensation
    codes for PMOS and NMOS transistors drivers. These aging compensation codes turn
    on devices in parallel to the main driver, increasing the drive strength of output,
    which compensates the degradation in NMOS and PMOS driver due to HCI and BTI effects.
    The design was implemented in 40 nm CMOS process by using 1.8V thick-oxide devices.
    Results show that by using the proposed aging compensation technique, the impact
    of aging on output driver reduces by 70% after ten years of operation.


    Ali et al. [\[27\]](#page-16-14) use IJTAG to manage temperature health monitors
    on the chip. The temperature health monitors are based on a Wheatstone bridge,
    which is composed of four resistors, one operational amplifier, one filter, and
    one analog to digital converter. Results show that the proposed solution can be
    used to manage and control instruments to ensure the reliable operation of the
    chip over its lifetime. According to the authors, the proposed method can reduce
    the overall time spent on the test, once there is no off-chip interface, and the
    system clock can be used instead of the test clock.


    Rathore et al. [\[28\]](#page-16-15) propose to use temperature and NBTI sensors
    to implement a task mapping strategy to manycore systems, called LifeGuard. LifeGuard
    considers performance and aging as parameters to perform task mapping, and is
    based on reinforcement learning. At each tile of the network-on-chip (NoC), an
    NBTI and a thermal sensor are added and used to perform the task mapping. As a
    benefit, LifeGuard prevents the rapid aging of cores that map a more significant
    number of tasks. Also, it improves the aggregate safe operating frequency of the
    system. Experimental results using a 256-core system showed that LifeGuard improved
    the health of the cores for 57% when compared to the HiMap strategy [\[62\]](#page-17-18),
    and 74% when compared to the Hayat [\[63\]](#page-17-19). Also, LifeGuard shows
    a better system performance.


    #### *B. Critical Path Monitors*


    Contemporary literature presents monitors that capture timing errors on the critical
    paths of circuits (Definition [1\)](#page-1-9). This technique contains extra
    components that capture the memory register output of the critical paths and its
    processed data. A comparison between these data is executed to identify if a timing
    error occurred.


    Savanur et al. [\[29\]](#page-16-16) present a BIST (Definition [7\)](#page-1-6)
    approach to detect aging effects on the circuit during test mode. The BIST circuitry
    uses two identical buffer chains and a logic block to compare the output of these
    chains. If there is a difference between the outputs of the chains, an error caused
    by aging is detected. The approach needs extra components, which are a D flip-flop,
    a NAND gate, an AND gate, and two buffers. HSPICE simulations on 45 nm and 65
    nm were performed to extract results. This paper focuses only on the NBTI aging
    factor, and results show that the solution can detect minimal stress levels in
    the presence of process variations and that the aging detection depends on the
    time that a path of the circuit keeps at logic level zero.


    Sai et al. [\[30\]](#page-16-17) present a Parity Check Circuit (PPC) for monitoring
    delay-faults and compare it with the Canary flipflop approach [\[64\]](#page-17-20).
    Unlike the Canary flip-flop approach, PPC can monitor more than one logic path
    simultaneously, which allows reducing the number of sensors. PPC is implemented
    using a multiple-input XOR gate, a delay element (DE), a matched delay element
    (MD), a flip-flop, a shadow flip-flop, and a 2-input XOR gate. The multiple-input
    XOR gate is responsible for computing the data parity, and the MD is responsible
    for adding a delay to the clock signal to compensate delay produced by the multiple-input
    XOR gate. The PPC was validated in a 32 bit MIPS processor using a 65 nm technology.
    Results indicate that the use of the circuit reduces area overhead by 66% and
    power by 33% when compared to the Canary flip-flop approach.


    Sai et al. [\[31\]](#page-16-18) propose a metastability-free aging sensor called
    Differential Multiple Error Detection Sensor (DMEDS). The sensor monitors multiple
    paths concurrently. It is composed of a Multiple Detection Unit (MDU) and a stability
    checker, which allows monitoring two or more critical paths simultaneously. Any
    transition in the data active the MDU output signal that is captured by the stability
    checker, signaling a delay fault. The stability checker checks the stability of
    the delayed signal while the clock is high. DMEDS was designed at transistor level
    using a 32 nm technology and applied to a 32-bit MIPS processor to monitor ten
    paths concurrently. Results show that using DMEDS for monitoring ten paths can
    save 197.1% and 97.1% in area overhead when compared to Razor [\[15\]](#page-16-2)
    and Canary [\[64\]](#page-17-20), respectively.


    Copetti et al. [\[32\]](#page-16-19) propose a hardware-based technique able to
    increase ICs lifetime. The technique is based on a sensor able to monitor IC aging
    and to adjust its power supply voltage to minimize NBTI effects, increasing the
    circuit lifetime. The approach is composed of: (i) an aging sensor, which contains
    a delay element and the stability checker; (ii) an actuator, which contains a
    counter and a decoder block; and (iii) a flip-flop inserted at the critical path
    output. The flip-flop output of the critical path passes through the delay element
    and the stability checker while the clock signal is high, analyzing data transitions.
    If a transition while the clock is high occurs, it means that the delay of the
    circuit increased, changing the output of the stability checker and indicating
    a timing violation. Also, the actuator receives the signal from the stability
    checker and increases its counter, allowing the decoder to adjust the power supply.
    Experimental results obtained by simulations demonstrate that the technique increases
    the circuit lifetime by 150%.


    Sadeghi-Kohan et al. [\[33\]](#page-16-20) propose a self-adjusting age monitoring
    method to pipeline circuits, which allows detecting progressive changes in the
    timing of a circuit. The output of the critical paths are captured using an age
    monitoring clock (that occurs before the system clock), and this captured data
    is compared with the same output but captured at the rising edge of the system
    clock. The circuit used to adjust the age monitoring clock has an age indicator
    counter that counts RO pulses to adjust the clock phase and is initialized with
    the core process characteristic. As the core ages, the age indicator counter is
    incremented, causing a more extended clock phase shift, and shorter slack time.
    The monitors are designed targeting low hardware overhead and accuracy in reported
    timing changes.


    In another work, Sadeghi-Kohan et al. [\[34\]](#page-16-21) use a similar strategy
    to monitor paths of a circuit and to detect its continuous age growth. This approach
    can provide the aging rate and the aging state of the circuit. The proposed strategy
    uses a clock generator to feed the register responsible for capturing the data
    before the system clock. Results show an area overhead of 2.13%, a power overhead
    of 0.69%, and a low-performance overhead. Yi et al. [\[35\]](#page-16-22) present
    a scan-based on-line monitoring that monitors aging during system operation and
    gives an alarm if the system detects aging effects. This work inserts an extra
    scan chain that captures early the functional data (at the opposite edge of the
    system clock) within a given guard-band interval during system operation. After
    that, the extra scan chain output is compared to the original scan chain output,
    which allows detecting violations. The scan-chain scheme contains two scan-chains,
    one with conventional scan cells (SC scan-chain) and the other one with the early
    capture scan cells (ECSC scan-chain). The SC flip-flops capture the data at the
    clock rising edge, while the ECSC flip-flops capture the data at the clock falling
    edge. The modified scan chain uses an XOR gate to compare the captured data.


    Jung [\[36\]](#page-16-23) presents an aging level estimating flip-flop that exploits
    the frequency guard band of a device to estimate the aging level with a small
    power overhead, called performance estimation flip-flop (PEFF). The PEFF has five
    elements: i) a scan flip-flop; ii) a shadow latch; iii) a sampling time indicator;
    iv) a logic block to controls the input of the performance result cell (PERC);
    v) the PERC. The shadow-latch is used to sample the data earlier than the functional
    flip-flop. Results show a reduction in monitoring time, and the power consumption
    is reduced by 50% when compared to the Yi et al. [\[35\]](#page-16-22).


    Vazquez-Hernandez [\[37\]](#page-16-24) proposes a solution for error prediction
    using an aging sensor based on the Error-Detection Sequential (EDS) circuit [\[65\]](#page-17-21).
    The EDS has a decoder module to monitors the critical paths. When one of the paths
    is activated, the decoder active the EDS to allow detect errors. The methodology
    for path selection uses statistical static timing analysis. Results show that
    the EDS can reduce power overhead form 102% to 6% and area overhead from 69% to
    22% when compared to [\[5\]](#page-15-4) and [\[35\]](#page-16-22), considering
    the circuit characteristics as the number of gates and buffers.


    Chandra [\[38\]](#page-16-25) proposes the SlackProbe monitor. This approach inserts
    timing monitors at endpoints and intermediate nodes of the circuit paths. If a
    monitor is inserted at an intermediate node, an AND gate is used as delay matching,
    and a transition detector is connected to the intermediate node with a minimum
    size inverter. If a signal transition at the intermediate node occurs, it arrives
    at the transition detector through the delay chain, and the signal is compared
    with the incoming clock edge. If the transition is close to its required arrival
    time, a corresponding signal transition arrives at the transition detector input
    after the clock edge. This transition triggers the transition detector and flags
    a signal indicating a delay failure. The monitor inserted at the intermediate
    node is capable of monitoring the delay of all critical paths passing through
    it, and its output can be used for mitigating failures due to aging (based on
    hardware or software). The results show that SlackProbe can achieve up to 16x
    reduction in the total number of monitors.


    Masuda and Hashimoto [\[39\]](#page-16-26) propose an error prediction adaptive
    voltage scaling (EP-AVS) and a mean time to failure aware (MTTF-aware) design
    methodology for EP-AVS circuits. The EP-AVS has a main circuit plus a timing error
    predictive flip-flop (TEP-FF) and a voltage control unit. The TEP-FF has a flip-flop,
    delay buffers, and a comparator implemented with an XOR gate. Also, TEP-FF works
    with the main flip-flop. When the timing margin is gradually decreasing, a timing
    error occurs at the TEP-FF before the main flip-flop captures a wrong value due
    to the delay buffer. This wrong value produces an error prediction signal, which
    allows the voltage control logic to provide a higher supply voltage and reduce
    the circuit delay. Evaluation results show that the proposed EP-AVS design methodology
    achieves a 20.8% voltage reduction while satisfying the target MTTF.


    Vijayan et al. [\[40\]](#page-16-27) propose an aging monitor based on hardware
    and software. The system is composed of representative flip-flops (RFF) that are
    selected in an offline phase and connected to the monitoring hardware. The aging
    effect consists of two phases: i) stress phase, where the transistor is under
    the aging effect; ii) recovery phase, where the transistor is recovering from
    the stress phase. A switching event in the RFF corresponds to the recovery phase
    of the corresponding flip-flop group, which is captured to report the recovery
    event to the software. The representative flip-flops are observed by a switching-event
    detector to perform the capture operation, which is composed of an XOR gate and
    a shadow flip-flop that generates a pulse at its output when a logic transition
    occurs in the corresponding flip-flop. The output of the switching-event detector
    is encoded using a priority encoder. In a determined clock cycle, the output of
    the priority encoder indicates the index of the flip-flop that switches its state
    in that particular clock cycle. If two representative flip-flops change their
    states at the same time, the priority encoder ensures a valid output. The critical-flag
    register (CFF) keeps the criticality word to represent the recovery/aging state
    of the corresponding representative flip-flop. This paper uses a software subroutine
    to mitigate aging by inserting NOPs instructions, which allows a relaxation on
    BTI stress, reducing the workload. Results show that area and power overheads
    imposed by the monitoring hardware are less than 0.25% for a Leon3 processor and
    Fabscalar processor.


    Saliva et al. [\[41\]](#page-16-28) propose monitors based on delay elements called
    pre-error flip-flops. The approaches are composed of a shadow flip-flop that stores
    delayed data, and is works in parallel to the regular flip-flop. The approach
    compares the two flip-flop outputs, and a pre-error signal is generated to predict
    the occurrence of timing errors. Each monitor uses different delay approaches.
    The first approach is the buffer delay, where buffers produce the delay. The second
    is the passive delay, where a resistor generates the delay. The last is the master
    delay, where master-slave latches replace the regular flip-flop, and the slave
    latch outputs feed the shadow flip-flop to generate the delay. Results show that
    the detection window of the in-situ monitors with passive delay is less deviant
    than the buffer and master delay ones with Vdd decrease. However, using a passive
    element in a digital circuit is not common. The in-situ monitor with buffer delay
    is a better choice because it uses standard cells in its implementation.


    Di Natale et al. [\[42\]](#page-16-29) propose a hidden-delay-fault sensor that
    can be used to detect small delay faults. The sensor allows the circuit to operate
    at the nominal frequency, and it is inserted in a critical path. The monitor works
    sampling a signal in both clock edges. After that, the monitor compares the two
    samples using an XOR gate and stores the result on the next falling edge of the
    clock suing a shadow flip-flop. Result extraction is performed using a classic
    scan chain. The authors propose that the sensor can be used during the lifetime
    of the circuit to identify timing violations in short paths caused by aging. Also,
    the authors mention that it is possible to use the sensor combined with reconfiguration
    techniques such as DVFS. The paper does not present results.


    Wang et al. [\[43\]](#page-16-30) uses a timing margin detector (TMD) to monitor
    aging and capture delay behavior. Also, the output detector is used in a machine
    learning engine based on a support vector machine (SVM) to predict aging. The
    TMD is used to capture late transitions. It is composed of two D flip-flops, and
    by an OR gate at the flipflops output. Results show that it is possible to obtain
    a 97.40% of accuracy in aging prediction, with 4.14% area overhead on average.


    Rohbani and Miremadi [\[44\]](#page-17-0) propose an aging sensor combined with
    the flip-flops of the design that monitors the critical path output before the
    rising edge of the clock signal. This signal follows the system clock by an adjusted
    delay of about 10% to 20% of the clock period. When the clock is at logic level
    one, the sensor is activated. Any change in the input signal during the period
    where the sensor clock is at logic level one and the system clock is at level
    logic level zero represents a delay extension of the critical path due to aging
    effects. Results show that the precision of the proposed sensor is about 2.7 higher,
    with almost 33% less area overhead compared with state-ofthe-art aging sensors.
    Furthermore, the presented sensor can detect and correct 50% of the Single Event
    Upsets (SEUs), which lead to a bit-flip in the flip-flops. Besides, the SEU detection
    circuitry can reduce Bias Temperature Instability (BTI) by balancing the duty
    cycle of the flip-flop with negligible extra overhead.


    Pachito et al. [\[45\]](#page-17-1) propose an aging-aware power supply or frequency
    reconfiguration approach that uses global and local sensors. Global sensors perform
    periodic or on-demand delay monitoring, while local sensors predict errors locally.
    Both allow adjusting frequency or power supply voltage. Results show that performance
    and power can be improved by, respectively, increasing the frequency and reducing
    the voltage while still preventing errors. In other work, Semio et al. [\[46\]](#page-17-2),
    [\[47\]](#page-17-3) propose improvements in the global and local sensors cited
    previously. The global sensor was improved to detect Negative-bias temperature
    instability (NBTI) and Positive-bias temperature instability (PBTI), while the
    local sensor was improved to tolerate delay-faults.


    Cho et al. [\[48\]](#page-17-4) examine the effectiveness of the aging-aware Adaptive
    Voltage Scaling (AVS) for logic circuit blocks using a Tunable Replica Circuit
    (TRC) aging monitors. The TRC is calibrated off-line, based on the critical paths
    and on-line monitoring of the operational conditions of the circuit, as temperature
    variations. The Power Management Unit (PMU) communicates with the TRC periodically.
    The PMU tunes the Voltage Regulator Module (VRM) when the TRC detects an aging
    delay degradation until the TRC detects the correct behavior based on the clock.
    Simulation results in a 22 nm High-K/Metal-Gate Tri-Gate CMOS process show a 7%
    power reduction with the removal of the guard-bands of a conventional fixed Vcc.


    Ding et al. [\[49\]](#page-17-5) propose a delay amplified digital (DAD) aging
    sensor circuit composed of a delay sensor and a signal amplification circuit.
    It uses a reference delay circuit designed according to the monitored combinational
    logic circuit. The delay sensor is used to detect aging effects, while a timing
    multiplier circuit eliminates the effects on the environment, improving the aging
    sensor data accuracy. A digital sample module uses nine T flip-flops to count
    the number of falling-edge during the amplified enable pulse. Using the parameters
    of TSMC 65 nm CMOS technology, the DAD sensor circuit is designed and simulated
    using SPECTRE.


    Li and Seok [\[50\]](#page-17-6) propose a technique to pipeline circuits that
    enables accurate of aging monitoring even under environmental variations. The
    technique scales the supply voltage for a temperature-insensitive delay and reconfigures
    the target paths into ring oscillators. The oscillation periods are measured and
    compared to pre-aging measurements to estimate the delay degradation caused by
    aging. Also, the technique presents a new register implementation, that has an
    area overhead of 12 transistors when compared to a standard flip-flop, and a relatively
    low delay overhead, once it adds a small amount of load between the path from
    input to output. The technique adds a feedback network between the input and output
    registers of target paths, which can present a small portion of the total oscillation
    period, making little impact on monitoring accuracy. The area overhead of feedback
    network can be minimized by sharing feedback paths among multiple target paths,
    as like the counter. The counter is used to measure the periods of the ring oscillator
    operation. Results show that the technique achieves highly-accurate monitoring
    with an error of 15.5% across the temperature variations in self-test phases from
    0◦C to 80◦C, exhibiting more than 30 times improvement in accuracy as compared
    to the conventional technique operating at the nominal supply voltage.


    Jang et al. [\[51\]](#page-17-7) propose a aging sensor that detects failures
    caused by BTI and HCI. This aging sensor is based on timing warning windows to
    detect a guardband violation of sequential circuits and generates a warning right
    before circuit failures occur. It monitors the moment when the critical path delays
    of the logic exceed a standard value, which guarantees a correct circuit operation.
    The aging sensor is composed by: i) a guardband generator; ii) a path delay monitor;
    iii) a hold circuit; iv) a signal that controls the aging sensor. The circuit
    was implemented in 110 nm, and results show that the aging sensor achieves a good
    aging failure prediction with low overhead.


    Table [VI](#page-13-0) summarizes the reviewed path delay monitors. Most of the
    monitors use delay elements and comparison mechanism to detect timing errors.
    Also, most of the approaches monitor just the critical paths without monitoring
    other system elements.


    #### *C. Clock Frequency Monitors*


    Aging can affect clock frequency and decreases system performance. Thus, clock
    frequency monitoring is a way to detect aging in a design.


    Wang et al. [\[52\]](#page-17-8) present a sensor for reliability analysis of
    digital circuits using standard-cells called Radic. The Authors also propose a
    low-cost built-in aging adaption system based on the Radic sensor to perform in-field
    aging adaption. Radic allows frequency, aging, and metastability measurements.
    Also, the sensor is designed to obtain the frequency difference between the waveform
    under test and a reference frequency by measuring how much clock cycles the wave
    under test is faster or slower than the reference frequency. A stable external
    source such as automatic test equipment, or a waveform generator, or an internal
    source such as a phase-locked loop (PLL) can generate the reference frequency.
    An m-bit timer stores the length of the measurement window by counting the number
    of clock cycles of the reference frequency. The Radic-based aging monitor system
    is inserted into a Freescale IP and an ITC''99 b19 benchmark. Results show that
    the system reduces the fixed aging guardband by 80%. A comparison between the
    original design and the design with the proposed adaption system shows an area
    reduction of about 1.02% to 3.16% in most cases. Power is also reduced, as the
    design can be synthesized using smaller drive strength.


    Kfloglu et al. [\[53\]](#page-17-9) propose an aging sensor based on RO that uses
    two identical aging paths. Both paths can be either equally sensitive to BTI or
    modulated to be more sensitive to aging effects. Using DC biased with opposite
    polarity inputs, both paths have the PBTI and NBTI effects stressed alternatively
    at every other stage. Unlike a conventional RO based aging sensor, a control loop
    logic links all stressed devices into one measuring RO loop which its output is
    the frequency degraded due to the aging. Also, the control loop logic links all
    non-stressed devices are linked into a second RO loop, wich the output is the
    reference frequency. The frequency delta between aging frequency and reference
    frequency is used to monitors aging.


    <span id="page-13-0"></span>


    | TABLE VI                                                                                     |  |

    |----------------------------------------------------------------------------------------------|--|

    | COMPARISON AMONG CRITICAL-PATH MONITORS PAPERS. LEGEND: "A": ADDRESSED, "NA":
    NOT ADDRESSED. |  |


    |            | System<br>monitoring | Extra<br>clock | Delay<br>element | Comparison<br>mechanism
    |

    |------------|----------------------|----------------|------------------|-------------------------|

    | [29], 2015 | NA                   | NA             | A                | A                       |

    | [30], 2017 | NA                   | NA             | A                | A                       |

    | [31], 2018 | NA                   | NA             | A                | A                       |

    | [32], 2016 | NA                   | NA             | A                | A                       |

    | [33], 2015 | NA                   | A              | NA               | A                       |

    | [34], 2017 | NA                   | A              | NA               | A                       |

    | [35], 2014 | NA                   | NA             | NA               | A                       |

    | [36], 2017 | NA                   | A              | NA               | A                       |

    | [37], 2014 | NA                   | NA             | NA               | A                       |

    | [38], 2014 | NA                   | NA             | A                | A                       |

    | [39], 2018 | NA                   | NA             | A                | A                       |

    | [40], 2017 | NA                   | NA             | NA               | A                       |

    | [41], 2015 | NA                   | NA             | A                | A                       |

    | [42], 2019 | NA                   | NA             | NA               | A                       |

    | [43], 2019 | NA                   | NA             | NA               | A                       |

    | [44], 2018 | NA                   | NA             | A                | NA                      |

    | [45], 2012 | A                    | NA             | A                | A                       |

    | [48], 2015 | NA                   | NA             | NA               | NA                      |

    | [49], 2016 | NA                   | NA             | NA               | A                       |

    | [50], 2014 | NA                   | NA             | NA               | NA                      |

    | [51], 2014 | NA                   | NA             | A                | A                       |


    #### *D. Circuit State and Workload Monitors*


    It is possible to detect aging by monitoring the whole system instead of just
    critical paths. The circuit state and its workload are metrics that can be monitored
    and provide insightful information. For example, they can indicate stress levels
    that will increase the aging effects impact.


    Koneru et al. [\[54\]](#page-17-10) reuse the design for testability (DfT – Definition
    [6\)](#page-1-5) infrastructure to perform a fine-grain workload-induced stress
    monitoring for accurate aging prediction. A multiple-input signature register
    (MISR) is used to capture the workload effect on the circuit. An aging prediction
    software, based on support vector machine (SVM) learning technique, performs aging
    mitigation. The DfT controller, implemented as a finite-state machine (FSM), periodically
    switches the circuit into scan mode to capture the circuit state. After capturing
    the state, the contents of the scan chains are then shifted out to the MISR. The
    scan-chains are modified to keep its value during the aging monitoring phase,
    which overwrites the state of the flip-flops and not allow the circuit to return
    to normal operation until completing the shift to MISR. The Authors conducted
    experiments on two open-source processor benchmarks, namely OpenRISC 1200 and
    Leon3, and on four ISCAS''89 benchmarks, to evaluate the accuracy of the proposed
    technique. Simulation results show that the proposed approach can accurately predict
    workloadinduced aging trends. In a similar work, Firouzi et al. [\[55\]](#page-17-11)
    reuse the BIST structure to predict the fine-grained circuit-delay degradation
    with minimal area and performance overhead and high accuracy.


    Khan and Kundu [\[56\]](#page-17-12) propose a system-level reliability management
    scheme (SRM) that dynamically adjusts the operating frequency and supply voltage
    according to the system aging. The proposal allows continuous runtime adjustments
    based on parameters such as actual room temperature and power supply tolerance.
    The SRM communicates with the voltage and frequency control registers to enable
    frequency and voltage reconfiguration. It is implemented in software and assumes
    a Virtual Machine Monitor (VMM) running underneath the OS software stack, which
    is primarily used to enter and exit the SRM. The SRM software enables carefully
    crafted functional stress tests or built-in self-test control to identify degradation
    at a component granularity and provides adjustments for sustained performance
    levels at the target reliability. The software allows the system to adapt to the
    aging effects and invokes aging device management at determined periods. The results
    show that the device can operate near peak frequency throughout product life.
    Also, the approach ensures protection against failure due to insufficient lifetime
    guardband and no system downtime or change.


    Sadi et al. [\[57\]](#page-17-13) presents a framework for designing lifetime-reliable
    system-on-chip (SoC) with reconfiguration capability to deal with aging effects.
    The proposed flow uses a BIST (Definition [7\)](#page-1-6), and a machine learning
    linear regression predictor software to activate aging countermeasures. The aging
    status of the chip is monitored at regular intervals by the BIST hardware. Based
    on the observations, proactive adaptation methods are taken to counteract the
    reliability degradation effect. The framework allows testing patterns from SoC''s
    existing BIST hardware, collect the response, and tune the linear regression software.
    A gate-overlap and path-delay-aware algorithm selects a minimum set of patterns,
    which activate the target paths used as features of the linear regression predictor.
    The seeds of the selected patterns are stored in on-chip memory and applied at
    the BIST hardware at multiple test clock frequencies when required. The corresponding
    responses of these patterns are collected in a separate response storage flipflop
    chain. The software-implemented machine learning classifier is trained with the
    collected multiple-frequency responses, and the trained predictor accurately predicts
    the state of aging degradation at runtime. The paths to be monitored by the BIST
    hardware are selected at the design time based on timing analysis. The adaptive
    methods used in this approach are frequency scaling, voltage scaling, and adaptive
    body biasing. Simulation results show that the proposed technique allows accurate
    and fine-grained in-field aging prediction, with a precision that can reach 94%.


    Baranowski et al. [\[58\]](#page-17-14) present a method for aging rate prediction,
    which is based on workload monitoring and linear regression machine learning technique.
    The monitoring technique enables the on-line prediction of the degradation rate
    caused by the currently running application. The degradation rate monitoring system
    is composed of a workload monitor and a temperature sensor. The linear regression
    machine learning technique is used to find the representative critical gates that
    are monitored. Results show that this method delivers sufficient accuracy at an
    area overhead of 4.2%, which decreases with the size of the monitored circuit.


    #### *E. Voltage Monitors*


    Circuit voltage can also be use to monitor aging effects. Similar to what happens
    in clock frequency variations, as the circuit ages, negative effects will be observed,
    such as timing errors and path delay extension.


    Narang and Srivastava [\[59\]](#page-17-15) propose an approach that uses an inverter
    chain and counter-based technique to detect the variation in the threshold voltage.
    A voltage sensing methodology is used to monitor the circuit''s voltage variation.
    This variation can be fed to an Adaptive Body Bias (ABB) circuit to mitigate the
    effects of NBTI. The approach uses a counter to determine the total path delay.
    The path delay feeds an inverter connected to an amplifier. The amplifier output
    pass trough a voltage to current converter, which feeds a logarithmic amplifier.
    The output of this logarithmic amplifier feds an exponential function generator
    that passes through a subtractor circuit that is used to detect a change in the
    threshold voltage due to the NBTI effect. The circuit was validated in a set of
    circuits such as 32-bit OR gate, 64-input OR gate, and 32-bit comparator in 32
    nm technology. The simulation results show that this methodology is efficient
    as it reduces the delay to a large extent with minimal increase in power.


    Xiaojin et al. [\[60\]](#page-17-16) propose a digital on-chip detector that uses
    the circuit output voltage phase to detect aging. The approach consists of duplicating
    the circuit, generating two circuits: a reference circuit and an aging stressed
    circuit. The output of both is compared using an XNOR gate. If a pulse occurs
    in the XNOR output, aging is detected. Also, the XNOR output pass trough time
    to digital converter to facilitate the circuit state analyses and verification.
    The authors claim that the aging detecting circuit can be applied in adaptive
    systems to mitigate the aging, but do not present a solution in this work. The
    validation was made by a chip implementation and by simulation. The chip demonstrates
    results close to the simulated regarding aging time.


    #### VII. CONCLUSION


    <span id="page-14-0"></span>This Section concludes the aging monitors and reconfiguration
    techniques survey, providing insights for future research and development. Table
    [VII](#page-15-12) presents the implementation methods.


    Most of the literature contributions are in the digital area, specifically using
    hardware solutions for monitoring aging in circuits. Few works use software approaches
    for aging monitoring. Most software applications act on reconfiguring the circuit
    after aging detection. We observed works using learning methods (software) for
    taking proactive actions, detecting events related to aging before they occur.
    These learning-based methods can point out


    | TABLE VII                                              |

    |--------------------------------------------------------|

    | CLASSIFICATION ACCORDING TO THE IMPLEMENTATION METHOD. |


    <span id="page-15-12"></span>


    | Classification                                                                       |
    Works                                                                                                                                                                  |

    |--------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------|

    | Digital, Hardware, Synchronous                                                       |
    [23] [24] [25] [26] [18] [28] [29]<br>[30] [31] [32] [33] [34] [35] [36]<br>[37]
    [38] [39] [42] [43] [44] [45]<br>[46] [47] [48] [49] [50] [51] [52]<br>[53] [55]
    [60] |

    | Digital, Hardware & Software, Synchronous<br>Analog & Digital, Hardware, Synchronous
    | [21] [40] [54] [56] [57] [58]<br>[22] [27] [41] [59]                                                                                                                   |


    a promising way to detect the effects of aging, as long as they are associated
    with a rich set of hardware monitors, such as temperature monitors. Also, few
    contributions combine analog and digital techniques to perform aging monitoring.


    As mentioned, the most common monitor used for aging detection is the timing error
    monitor. However, this technique is a function of the paths chosen to be monitored
    during design time. If this choice is executed incorrectly and the set of critical
    paths does not represent the critical paths, the aging counter-effects solutions
    are inefficient. With the CMOS scaling, the number of critical paths increases,
    which increases the number of components to monitor the aging effects. Thus, it
    is likely that the use of solutions focused on timing errors and path monitoring
    will decrease in the future or be limited to older technologies nodes.


    We believe that solutions based on system-wide monitoring parameters such as temperature,
    voltage, and frequency will be more prevalent once they do not depend on a specific
    parameter.


    Summarizing, current techniques heavily rely on timing error monitors for detecting
    aging and use voltage scaling to compensate the effects in integrated circuits.
    We believe that new solutions are required due to the fact that as technology
    nodes advance, it becomes harder to identify critical paths of a circuit. This
    limitation not only makes the task of defining where to place timing error monitors
    more challenging, but can also require an increase in the number of monitors,
    inducing larger area and power overheads. Furthermore, with the availability of
    different sensors, such as frequency and temperature, in industrial SoCs, we believe
    there is a gap to be filled in aging detection algorithms. Especially with emerging
    artificial intelligence capabilities, algorithms can analyze the data of the available
    sensors and configure the SoC to counter aging effects.


    #### REFERENCES


    - <span id="page-15-0"></span>[1] H. Kim, J. Kim, H. Amrouch, J. Henkel, A. Gerstlauer,
    K. Choi, and H. Park, "Aging compensation with dynamic computation approximation,"
    *IEEE Transactions on Circuits and Systems I: Regular Papers*, 2020.

    - <span id="page-15-1"></span>[2] G. Sai, M. Zwolinski, and B. Halak, "A cost-efficient
    aging sensor based on multiple paths delay fault," *Ageing of Integrated Circuits:
    Causes, Effects and Mitigation Techniques*, p. 211, 2020.

    - <span id="page-15-2"></span>[3] S. R. Sahoo and K. Mahapatra, "A novel area
    efficient on-chip ro-sensor for recycled ic detection," *Integration*, vol. 70,
    pp. 138–150, 2020.

    - <span id="page-15-3"></span>[4] Y. Lu, L. Shang, H. Zhou, H. Zhu, F. Yang, and
    X. Zeng, "Statistical Reliability Analysis under Process Variation and Aging Effects,"
    in *Design Automation Conference (DAC)*. IEEE, 2009, pp. 514–519.

    - <span id="page-15-4"></span>[5] M. Agarwal, V. Balakrishnan, A. Bhuyan, K. Kim,
    B. C. Paul, W. Wang, B. Yang, Y. Cao, and S. Mitra, "Optimized Circuit Failure
    Prediction for Aging: Practicality and Promise," in *International Test Conference
    (ITC)*. IEEE, 2008, pp. 1–10.

    - <span id="page-15-5"></span>[6] R. J. Baker, *CMOS: Circuit Design, Layout,
    and Simulation*. Wiley-IEEE press, 2019.

    - <span id="page-15-6"></span>[7] S. Rahimipour, W. N. Flayyih, I. El-Azhary,
    S. Shafie, and F. Z. Rokhani, "A survey of On-chip Monitors," in *International
    Conference on Circuits and Systems (ICCAS)*. IEEE, 2012, pp. 243–248.

    - <span id="page-15-7"></span>[8] N. Khoshavi, R. A. Ashraf, R. F. DeMara, S.
    Kiamehr, F. Oboril, and M. B. Tahoori, "Contemporary CMOS Aging Mitigation Techniques:
    Survey, Taxonomy, and Methods," *Integration, the VLSI Journal*, vol. 59, pp.
    10–22, 2017.

    - <span id="page-15-8"></span>[9] M. A. Kochte and H.-J. Wunderlich, "Self-Test
    and Diagnosis for Self-Aware Systems," *IEEE Design & Test*, vol. 35, no. 7, pp.
    7–18, 2018.

    - <span id="page-15-9"></span>[10] A. Jagirdar, R. Oliveira, and T. J. Chakraborty,
    "Efficient flip-flop designs for SET/SEU mitigation with tolerance to crosstalk
    induced signal delays," in *IEEE Silicon Errors Logic System Effects (SELSE)*,
    2007, pp. 1–6.

    - <span id="page-15-10"></span>[11] D. K. Schroder and J. A. Babcock, "Negative
    bias temperature instability: Road to cross in deep submicron silicon semiconductor
    manufacturing," *Journal of applied Physics*, vol. 94, no. 1, pp. 1–18, 2003.

    - <span id="page-15-11"></span>[12] E. Maricau and G. Gielen, "Transistor aging-induced
    degradation of analog circuits: Impact analysis and design guidelines," in *European
    Solid-State Device Research Conference (ESSCIRC)*. IEEE, 2011, pp. 243–246.

    - <span id="page-16-0"></span>[13] S. Novak, C. Parker, D. Becher, M. Liu, M.
    Agostinelli, M. Chahal, P. Packan, P. Nayak, S. Ramey, and S. Natarajan, "Transistor
    aging and reliability in 14nm tri-gate technology," in *2015 IEEE International
    Reliability Physics Symposium*. IEEE, 2015, pp. 2F–2.

    - <span id="page-16-1"></span>[14] M. Abramovici, M. A. Breuer, and A. D. Friedman,
    *Digital Systems Testing And Testable Design*, 1st ed. Wiley-IEEE Press, 1994.

    - <span id="page-16-2"></span>[15] D. Ernst, N. S. Kim, S. Das, S. Pant, R. Rao,
    T. Pham, C. Ziesler, D. Blaauw, T. Austin, K. Flautner *et al.*, "Razor: A Low-power
    Pipeline Based on Circuit-level Timing Speculation," in *International Symposium
    on Microarchitecture (MICRO)*. IEEE, 2003, pp. 7–18.

    - <span id="page-16-3"></span>[16] S. Mittal, "A survey of techniques for improving
    energy efficiency in embedded computing systems," *International Journal of Computer
    Aided Engineering and Technology*, vol. 6, no. 4, pp. 440–459, 2014.

    - <span id="page-16-4"></span>[17] A. L. D. M. Martins, A. H. L. da Silva, A.
    M. Rahmani, N. D. Dutt, and F. G. Moraes, "Hierarchical adaptive Multi-objective
    resource management for many-core systems," *Journal of Systems Architecture*,
    vol. 97, pp. 416–427, 2019.

    - <span id="page-16-5"></span>[18] V. Kumar, "On-chip Aging Compensation for Output
    Driver," in *International Reliability Physics Symposium (IRPS)*. IEEE, 2014,
    pp. CA.3.1–CA.3.5.

    - <span id="page-16-6"></span>[19] T. Chen and S. Naffziger, "Comparison of adaptive
    body bias (ABB) and Adaptive Supply Voltage (ASV) for Improving Delay and Leakage
    Under the Presence of Process Variation," *IEEE Transactions on Very Large Scale
    Integration (VLSI) Systems*, vol. 11, no. 5, pp. 888–899, 2003.

    - <span id="page-16-7"></span>[20] X. Wang, J. Keane, T. T.-H. Kim, P. Jain, Q.
    Tang, and C. H. Kim, "Silicon odometers: Compact in situ aging sensors for robust
    system design," *IEEE micro*, vol. 34, no. 6, pp. 74–85, 2014.

    - <span id="page-16-8"></span>[21] M. Igarashi, K. Takeuchi, T. Okagaki, K. Shibutani,
    H. Matsushita, and K. Nii, "An on-die digital aging monitor against HCI and xBTI
    in 16 nm Fin-FET bulk CMOS technology," in *European Solid-State Circuits Conference
    (ESSCIRC)*. IEEE, 2015, pp. 112–115.

    - <span id="page-16-9"></span>[22] S. Majerus, X. Tang, J. Liang, and S. Mandal,
    "Embedded silicon odometers for monitoring the aging of high-temperature integrated
    circuits," in *National Aerospace and Electronics Conference (NAECON)*. IEEE,
    2017, pp. 98–103.

    - <span id="page-16-10"></span>[23] D. Sengupta and S. S. Sapatnekar, "Estimating
    Circuit Aging due to BTI and HCI using Ring-Oscillator-Based Sensors," *IEEE Transactions
    on Computer-Aided Design of Integrated Circuits and Systems*, vol. 36, no. 10,
    pp. 1688–1701, 2017.

    - <span id="page-16-11"></span>[24] Y. Kim, H. Shim, M. Jin, J. Bae, C. Liu, and
    S. Pae, "Investigation of HCI effects in FinFET based ring oscillator circuits
    and IP blocks," in *International Reliability Physics Symposium (IRPS)*. IEEE,
    2017, pp. 4C–2.1–4C–2.4.

    - <span id="page-16-12"></span>[25] B. Shakya, U. Guin, M. Tehranipoor, and D.
    Forte, "Performance optimization for on-chip sensors to detect recycled ICs,"
    in *International Conference on Computer Design (ICCD)*. IEEE, 2015, pp. 289–295.

    - <span id="page-16-13"></span>[26] Y. Miyake, Y. Sato, S. Kajihara, and Y. Miura,
    "Temperature and Voltage Measurement for Field Test Using an Aging-Tolerant Monitor,"
    *IEEE Transactions on Very Large Scale Integration (VLSI) Systems*, vol. 24, no.
    11, pp. 3282–3295, 2016.

    - <span id="page-16-14"></span>[27] G. Ali, A. Badawy, and H. G. Kerkhoff, "Accessing
    on-chip temperature health monitors using the IEEE 1687 standard," in *International
    Conference on Electronics, Circuits and Systems (ICECS)*. IEEE, 2016, pp. 776–779.

    - <span id="page-16-15"></span>[28] V. Rathore, V. Chaturvedi, A. K. Singh, T.
    Srikanthan, and M. Shafique, "LifeGuard: A Reinforcement Learning-Based Task Mapping
    Strategy for Performance-Centric Aging Management," in *Design Automation Conference
    (DAC)*. ACM, 2019, p. 179.

    - <span id="page-16-16"></span>[29] P. R. Savanur, P. Alladi, and S. Tragoudas,
    "A BIST approach for counterfeit circuit detection based on NBTI degradation,"
    in *International Symposium on Defect and Fault Tolerance in VLSI and Nanotechnology
    Systems (DFTS)*. IEEE, 2015, pp. 123–126.

    - <span id="page-16-17"></span>[30] G. Sai, B. Halak, and M. Zwolinski, "A cost-efficient
    delay-fault monitor," in *International Symposium on Circuits and Systems (ISCAS)*.
    IEEE, 2017, pp. 1–4.

    - <span id="page-16-18"></span>[31] ——, "Multi-Path Aging Sensor for Cost-Efficient
    Delay Fault Prediction," *IEEE Transactions on Circuits and Systems II: Express
    Briefs*, vol. 65, no. 4, pp. 491–495, 2018.

    - <span id="page-16-19"></span>[32] T. Copetti, G. C. Medeiros, L. B. Poehls,
    and F. Vargas, "NBTI-Aware Design of Integrated Circuits: A Hardware-Based Approach
    for Increasing Circuits Life Time," *Journal of Electronic Testing*, vol. 32,
    no. 3, pp. 315–328, 2016.

    - <span id="page-16-20"></span>[33] S. Sadeghi-Kohan, M. Kamal, J. McNeil, P.
    Prinetto, and Z. Navabi, "Online self adjusting progressive age monitoring of
    timing variations," in *International Conference on Design Technology of Integrated
    Systems in Nanoscale Era (DTIS)*. IEEE, 2015, pp. 1–2.

    - <span id="page-16-21"></span>[34] S. Sadeghi-Kohan, M. Kamal, and Z. Navabi,
    "Self-Adjusting Monitor for Measuring Aging Rate and Advancement," *IEEE Transactions
    on Emerging Topics in Computing*, vol. (preprint), no. 1, pp. 1–1, 2017.

    - <span id="page-16-22"></span>[35] H. Yi, T. Yoneda, and M. Inoue, "A Scan-Based
    On-Line Aging Monitoring Scheme," *JSTS: Journal of Semiconductor Technology and
    Science*, vol. 14, no. 1, pp. 124–130, 2014.

    - <span id="page-16-23"></span>[36] J. Jung, M. A. Ansari, D. Kim, H. Yi, and
    S. Park, "On Diagnosing the Aging Level of Automotive Semiconductor Devices,"
    *IEEE Transactions on Circuits and Systems II: Express Briefs*, vol. 64, no. 7,
    pp. 822–826, 2017.

    - <span id="page-16-24"></span>[37] J. Vazquez-Hernandez, "Error prediction and
    detection methodologies for reliable circuit operation under NBTI," in *International
    Test Conference (ITC)*. IEEE, 2014, pp. 1–10.

    - <span id="page-16-25"></span>[38] V. Chandra, "Monitoring Reliability in Embedded
    Processors - A Multi-layer View," in *Design Automation Conference (DAC)*. ACM,
    2014, pp. 1–6.

    - <span id="page-16-26"></span>[39] Y. Masuda and M. Hashimoto, "MTTF-aware design
    methodology of error prediction based adaptively voltage-scaled circuits," in
    *Asia and South Pacific Design Automation Conference (ASP-DAC)*. IEEE, 2018, pp.
    159–165.

    - <span id="page-16-27"></span>[40] A. Vijayan, S. Kiamehr, F. Oboril, K. Chakrabarty,
    and M. B. Tahoori, "Workload-aware Static Aging Monitoring and Mitigation of Timing-critical
    Flip-flops," *IEEE Transactions on Computer-Aided Design of Integrated Circuits
    and Systems*, vol. 37, no. 10, pp. 2098–2110, 2017.

    - <span id="page-16-28"></span>[41] M. Saliva, F. Cacho, V. Huard, X. Federspiel,
    D. Angot, A. Benhassain, A. Bravaix, and L. Anghel, "Digital circuits reliability
    with in-situ monitors in 28nm fully depleted SOI," in *Design, Automation & Test
    in Europe Conference & Exhibition (DATE)*. IEEE, 2015, pp. 441–446.

    - <span id="page-16-29"></span>[42] G. Di Natale, E. I. Vatajelu, K. S. Kannan,
    and L. Anghel, "Hidden-Delay-Fault Sensor for Test, Reliability and Security,"
    in *Design, Automation & Test in Europe Conference & Exhibition (DATE)*. IEEE,
    2019, pp. 316–319.

    - <span id="page-16-30"></span>[43] Y.-T. Wang, K.-C. Wu, C.-H. Chou, and S.-C.
    Chang, "Aging-aware chip health prediction adopting an innovative monitoring strategy,"
    in *Asia and South Pacific Design Automation Conference (ASP-DAC)*. ACM, 2019,
    pp. 179–184.

    - <span id="page-17-1"></span><span id="page-17-0"></span>[45] J. Pachito, C.
    V. Martins, B. Jacinto, J. Semiao, J. C. Vazquez, V. Champac, M. B. Santos, I.
    C. Teixeira, and J. P. Teixeira, "Aging-aware ˜ power or frequency tuning with
    predictive fault detection," *IEEE Design & Test of Computers*, vol. 29, no. 5,
    pp. 27–36, 2012.

    - <span id="page-17-2"></span>[46] J. Semio, C. Leong, A. Romo, M. B. Santos,
    I. C. Teixeira, and J. P. Teixeira, "Aging-aware Dynamic Voltage or Frequency
    Scaling," in *Design of Circuits and Integrated Circuits (DCIS)*. IEEE, 2014,
    pp. 1–6.

    - <span id="page-17-3"></span>[47] J. Semio, D. Saraiva, C. Leong, A. Romo, M.
    B. Santos, I. C. Teixeira, and J. P. Teixeira, "Performance Sensor for Tolerance
    and Predictive Detection Of Delay-faults," in *Defect and Fault Tolerance in VLSI
    and Nanotechnology Systems (DFT)*. IEEE, 2014, pp. 110–115.

    - <span id="page-17-4"></span>[48] M. Cho, C. Tokunaga, M. M. Khellah, J. W. Tschanz,
    and V. De, "Aging-aware Adaptive Voltage Scaling in 22nm high-K/metal-gate tri-gate
    CMOS," in *Custom Integrated Circuits Conference (CICC)*. IEEE, 2015, pp. 1–4.

    - <span id="page-17-5"></span>[49] D. Ding, Y. Zhang, P. Wang, H. Qian, and G.
    Li, "Design a Delay Amplified Digital Aging Sensor Circuit in 65nm CMOS," in *International
    Conference on Solid-State and Integrated Circuit Technology (ICSICT)*. IEEE, 2016,
    pp. 1449–1451.

    - <span id="page-17-6"></span>[50] J. Li and M. Seok, "Robust and In-situ Self-testing
    Technique for Monitoring Device Aging Effects In Pipeline Circuits," in *Design
    Automation Conference (DAC)*. IEEE, 2014, pp. 1–6.

    - <span id="page-17-7"></span>[51] B. Jang, J. K. Lee, M. Choi, and K. K. Kim,
    "On-chip aging prediction circuit in nanometer digital circuits," in *International
    SoC Design Conference (ISOCC)*. IEEE, 2014, pp. 68–69.

    - <span id="page-17-8"></span>[52] X. Wang, L. Winemberg, D. Su, D. Tran, S. George,
    N. Ahmed, S. Palosh, A. Dobin, and M. Tehranipoor, "Aging adaption in integrated
    circuits using a novel built-in sensor," *IEEE Transactions on Computer-Aided
    Design of Integrated Circuits and Systems*, vol. 34, no. 1, pp. 109–121, 2015.

    - <span id="page-17-9"></span>[53] H. Kfloglu, M. Chen, S. Lu, A. Rabindranath,
    R. Kakoee, and S. Hu, "Thermally-aware sensor allocation for real-time monitoring
    and mitigation of FEOL aging in System-on-Chip (SoC) applications," in *International
    Reliability Physics Symposium (IRPS)*. IEEE, 2017, pp. 4C–6.1–4C–6.5.

    - <span id="page-17-10"></span>[54] A. Koneru, A. Vijayan, K. Chakrabarty, and
    M. B. Tahoori, "Fine-grained aging prediction based on the monitoring of run-time
    stress using DfT infrastructure," in *International Conference on Computer-Aided
    Design (ICCAD)*. IEEE, 2015, pp. 51–58.

    - <span id="page-17-11"></span>[55] F. Firouzi, F. Ye, A. Vijayan, A. Koneru,
    K. Chakrabarty, and M. B. Tahoori, "Re-using BIST for Circuit Aging Monitoring,"
    in *European Test Symposium (ETS)*. IEEE, 2015, pp. 1–2.

    - <span id="page-17-12"></span>[56] O. Khan and S. Kundu, "A self-adaptive system
    architecture to address transistor aging," in *Design, Automation & Test in Europe
    Conference & Exhibition (DATE)*. IEEE, 2009, pp. 81–86.

    - <span id="page-17-13"></span>[57] M. Sadi, G. K. Contreras, J. Chen, L. Winemberg,
    and M. Tehranipoor, "Design of Reliable SoCs With BIST Hardware and Machine Learning,"
    *IEEE Transactions on Very Large Scale Integration (VLSI) Systems*, vol. 25, no.
    11, pp. 3237–3250, 2017.

    - <span id="page-17-14"></span>[58] R. Baranowski, F. Firouzi, S. Kiamehr, C.
    Liu, M. Tahoori, and H.-J. Wunderlich, "On-line Prediction of NBTI-induced Aging
    Rates," in *Design, Automation & Test in Europe Conference & Exhibition (DATE)*.
    IEEE, 2015, pp. 589–592.

    - <span id="page-17-15"></span>[59] S. Narang and A. P. Srivastava, "NBTI detection
    methodology for building tolerance with respect to NBTI effects employing adaptive
    body bias," in *International Conference on Circuit, Power and Computing Technologies
    (ICCPCT)*. IEEE, 2015, pp. 1–7.

    - <span id="page-17-16"></span>[60] X. Li, J. Qing, Y. Sun, Y. Zeng, Y. Shi, and
    Y. Wang, "Linear and resolution adjusted on-chip aging detection of NBTI degradation,"
    *IEEE Transactions on Device and Materials Reliability*, vol. 18, no. 3, pp. 383–390,
    2018.

    - <span id="page-17-17"></span>[61] K. Hofmann, H. Reisinger, K. Ermisch, C. Schlnder,
    W. Gustin, T. Pompl, G. Georgakos, K. v. Arnim, J. Hatsch, T. Kodytek, T. Baumann,
    and C. Pacha, "Highly Accurate Product-level Aging Monitoring in 40nm CMOS," in
    *Symposium on VLSI Technology (VLSIT)*. IEEE, 2010, pp. 27–28.

    - <span id="page-17-18"></span>[62] V. Rathore, V. Chaturvedi, A. K. Singh, T.
    Srikanthan, R. Rohith, S.-K. Lam, and M. Shaflque, "HiMap: A hierarchical mapping
    approach for enhancing lifetime reliability of dark silicon manycore systems,"
    in *Design, Automation & Test in Europe Conference & Exhibition (DATE)*. IEEE,
    2018, pp. 991–996.

    - <span id="page-17-19"></span>[63] D. Gnad, M. Shafique, F. Kriebel, S. Rehman,
    D. Sun, and J. Henkel, "Hayat: Harnessing Dark Silicon and Variability for Aging
    Deceleration And Balancing," in *Design Automation Conference (DAC)*. IEEE, 2015,
    pp. 1–6.

    - <span id="page-17-20"></span>[64] H. Fuketa, M. Hashimoto, Y. Mitsuyama, and
    T. Onoye, "Adaptive performance compensation with in-situ timing error predictive
    sensors for subthreshold circuits," *IEEE Transactions on very large scale integration
    (VLSI) systems*, vol. 20, no. 2, pp. 333–343, 2012.

    - <span id="page-17-21"></span>[65] K. A. Bowman, J. W. Tschanz, S.-L. L. Lu,
    P. A. Aseron, M. M. Khellah, A. Raychowdhury, B. M. Geuskens, C. Tokunaga, C.
    B. Wilkerson, T. Karnik *et al.*, "A 45 nm Resilient Microprocessor Core for Dynamic
    Variation Tolerance," *IEEE Journal of Solid-State Circuits*, vol. 46, no. 1,
    pp. 194–208, 2011.


    #### ACKNOWLEDGMENTS


    Author Fernando Gehm Moraes is supported by FAPERGS (17/2551-0001196-1) and CNPq
    (302531/2016-5), Brazilian funding agencies. Leonardo Rezende Juracy was financed
    in part by the Coordenao de Aperfeioamento de Pessoal de Nivel Superior - Brasil
    (CAPES) - Finance Code 001.


    #### BIOGRAPHY


    Leonardo Rezende Juracy received a bachelor degree from the Pontifical Catholic
    University of Rio Grande do Sul (PUCRS), Brazil, in Computer Engineering in 2015,
    an M.Sc. degree from the PUCRS, Brazil, in Computer Science in 2018, and is currently
    an Ph.D. student at PUCRS. His research interests include design for testability,
    fault-tolerant designs, asynchronous designs, resilient designs, networks-on-chip
    and multi-processor systems-onchip.


    Matheus Trevisan Moreira received a B.S.E degree in Computer Engineering from
    Pontifcia Universidade Catlica do Rio Grande do Sul (PUCRS) in 2011. He also received
    a M.Sc. degree in Computer Science from the graduate program in Computer Science
    (PPGCC) at PUCRS in 2012. He has over 50 published articles, in conferences and
    journals. Also, his Thesis received an award from the Brazilian Society of Microelectronics
    (SBMICRO) and CEITEC S.A. as the best Ph.D. Thesis in Design, EDA and Test of
    Integrated Circuits in 2016. He is currently the Director of Technology at Chronos
    Tech, in San Diego, CA, USA. He has experience in different fields of microelectronics
    with emphasis on non-synchronous circuits design.


    Alexandre de Morais Amory received bachelor and master degrees in computer science
    from the PUCRS University, in 2001 and 2003, respectively. In 2007 he received
    the Ph.D. in computer science from UFRGS University, Porto Alegre, Brazil. His
    thesis received an Honorable Mention in the CAPES Thesis Award, in 2008. His professional
    experience include an internship at Philips Research Laboratories, The Netherlands,
    in 2005; as a lead verification engineer at CEITEC design house from 2007 to 2009;
    and as a postdoctoral fellow at PUCRS, from 2009 to 2012. Alexandre is currently
    a professor at PUCRS University. His research interest include design, test, fault-tolerance,
    and verification of digital systems, particularly MPSoCs and NoCs.


    Fernando Gehm Moraes (M''1997–SM''2002) received the Electrical Engineering and
    M.Sc. degrees from the Universidade Federal do Rio Grande do Sul (UFRGS), Porto
    Alegre, Brazil, in 1987 and 1990, respectively. In 1994 he received the Ph.D.
    degree from the Laboratoire dInformatique, Robotique et Microlectronique de Montpellier),
    France. He is currently at PUCRS, where he has been an Associate Professor from
    1996 to 2002, and Full Professor since 2002. He has authored and co-authored 37
    peer refereed journal articles in the field of VLSI design. His primary research
    interests include Microelectronics, FPGAs, reconfigurable architectures, NoCs
    and MPSoCs.'
