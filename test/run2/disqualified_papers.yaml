papers:
- title: Floating Point HUB Adder for RISC-V Sargantana Processor
  abstract: 'HUB format is an emerging technique to improve the hardware and time

    requirement when round to nearest is needed. On the other hand, RISC-V is an

    open-source ISA that many companies currently use in their designs. This paper

    presents a tailored floating point HUB adder implemented in the Sargantana

    RISC-V processor.'
  url: http://arxiv.org/abs/2401.09464v1
  keywords: ''
  document: '# Floating Point HUB Adder for RISC-V Sargantana Processor


    Gerardo Bandera1∗ , Javier Salamero<sup>2</sup> , Miquel Moreto2,3 and Julio Villalba<sup>1</sup>


    <sup>1</sup>Computer Architecture Dept., University of Málaga


    <sup>2</sup>BSC-CNS - Barcelona Supercomputing Center, <sup>3</sup>Computer Architecture
    Dept, UPC


    #### Abstract


    HUB format is an emerging technique to improve the hardware and time requirement
    when round to nearest is needed. On the other hand, RISC-V is a open source ISA
    that an important number of companies are using in their designs currently. In
    this paper we present a tailored floating point HUB adder that has been implemented
    in the Sargantana RISC-V processor.


    ## HUB format background


    In this section we present a basic description of the HUB format, although the
    mathematical foundations and an in-depth analysis of the format can be found in
    [\[1\]](#page-1-0). Now we briefly summarize this new real number representation
    format defined in [\[1\]](#page-1-0) and particularize it for the floating-point
    normalized numbers.


    HUB format is based on shifting the numbers that can be exactly represented under
    conventional formats by adding a bias which equals half unit in the lastplace
    (ULP). This shifting could be also interpreted as appending a hidden least significant
    bit set to one to the conventional number stream (which represents the bias).
    A floating-point HUB number is similar to a regular one but its significand follows
    the HUB format. Thus, the exponent and the sign is the same as convetional. Let
    us define x as a floating-point HUB number, which is represented by the triple
    (Sx, Mx, Ex) such that x = (−1)SxMx2 <sup>E</sup><sup>x</sup> , where the significand
    M<sup>x</sup> is a HUB magnitude. A normalized HUB significand fulfills that 1
    < M<sup>x</sup> < 2. Thus, the normalized HUB significand M<sup>x</sup> is


    <span id="page-0-0"></span>

    $$M\_x = 1 + \left[\sum\_{i=1}^f M\_{x\_i} \cdot 2^{-i}\right] + 2^{-f-1} \qquad
    (1)$$


    where 2 −f−1 is the bias. In this expression we define the representative form
    of the normalized HUB significand as the set of M<sup>x</sup><sup>i</sup> in expression
    [\(1\)](#page-0-0), that is M<sup>x</sup> = (M<sup>x</sup><sup>1</sup> , M<sup>x</sup>−<sup>1</sup>
    , M<sup>x</sup>−<sup>2</sup> , · · ·, M<sup>x</sup>−<sup>f</sup> ) (composed by
    f bits). Taking into account that both the MSB and the LSB are 1 (see expression
    [\(1\)](#page-0-0)), we define the operational form of a normalized HUB significand
    as the following f + 2 bits:


    $$M\_x = 1.M\_{x-1}M\_{x-2} \cdot \cdots \cdot M\_{x-f}1\tag{2}$$


    The representative version is used for storage whereas the operational version
    is required to operate with HUB numbers. We can see that the least significant
    bit (LSB) of the operational form of a nonzero HUB number is always equal to 1,
    and it is implicit in the format (similar situation takes place for the most significant
    bit (MSB) of the significand in the IEEE normalized floating-point numbers). Let
    ILSB denote the implicit LSB of a the operational HUB significand.


    Given a standard floating-point system with a normalized significand, the counterpart
    HUB floating point system has the same precision and accuracy [\[1\]](#page-1-0).


    The most outstanding feature of the HUB format is that round to nearest is performed
    by truncation. In the conventional format round to nearest is carried out by adding
    one to the position of the rounding bit of the final normalized result. Moreover,
    after this operation an overflow can also be produced, which involves a shift
    operation of one bit and an update of the exponent. Thus, a specific hardware
    module is used in conventional. In HUB, this module is not required any more.


    ## FP HUB adder for Sargantana


    The processor used in this work is Sargantana [\[2\]](#page-1-1), a 64 bit in-order
    Linux-capable RISC-V CPU that implements the RV64G ISA (see figure [1\)](#page-1-2).
    For accelerating domain-specific applications, Sargantana uses a Single Instruction
    Multiple Data (SIMD) unit and supports the vector instructions defined in the
    vector extension RVV 0.7.1. In addition, it implements custom application specific
    instructions. The CPU has a 7-stage pipeline that implements register renaming,
    out-oforder write-back, and a non-blocking memory pipeline. It has two first level
    caches: an instruction cache of 16KB, and a non-blocking data cache of 32KB. The
    system also has a 512KB L2 cache outside the CPU.


    Figure [2](#page-1-3) shows the main modules required to carry out the floating
    point addition for conventional and for HUB. Since round to nearest operation
    is carried


    <sup>∗</sup>Corresponding author: <mailto:gbandera@uma.es>


    <span id="page-1-2"></span>![](_page_1_Figure_0.jpeg)


    Figure 1: Architecture of the Sargantana processor


    out by truncation in HUB, the result obtained after normalization module in figure
    [2](#page-1-3) is the final result and not any other operation is required. Thus,
    the module Rounding (crossed out in the figure [2\)](#page-1-3) is not required
    for the HUB implementation in Sargantana.


    <span id="page-1-3"></span>![](_page_1_Figure_3.jpeg)


    Figure 2: Main modules in conventional and HUB FP adders (X-> prevented in HUB)


    For applications requiring conventional addition with round-to-nearest, three
    extra fractional bits are needed: a guard bit, a rounding bit and a sticky bit
    [\[3\]](#page-1-4). Since the round-to-nearest for HUB format is carried out by
    truncation, the rounding bit is not required anymore. Moreover, in conventional
    it is necessary calculate the sticky bit (this bit represents, in some way, the
    bits beyond the rounding bit and is needed for effective subtraction when the
    operands are not aligned). For HUB number, because we know that the LSB of the
    shifted operand is always 1 (that is the ILSB), the sticky bit is always 1 and
    it is not necessary a module to calculate it, as shown in figure [2.](#page-1-3)


    Thus, in spite of having an extra bit in the operational form (the ILSB), this
    extra bit of a HUB number is compensated by the lack of a specific rounding bit..
    Moreover, a guard bit is not necessary and the sticky bit, when required, is always
    1. As consequence, for applications where round to nearest is required, the


    data path of the HUB version has one bit less than that its conventional counterpart


    Our starting point is the FPU adder of the RISC-V Sargantana processor which has
    been modified to meet the HUB format specifications. Unlike the 6 stage original
    FPU adder, our floating point HUB adder has 5 stages. This feature together with
    the absence of denormals and sticky calculation leads to an area reduction of
    25% (3110µm<sup>2</sup> vs. 2332µm<sup>2</sup> ). In the first stage, the smallest
    operand is identified and the difference of exponents is calculated. In the second
    stage the significand of the smallest operand is shifted and the 2-complement
    of the smallest operand is calculated if required. The third state carries out
    the addition of the aligned operands and a possible overflow is detected and corrected.
    In the forth stage the number of leading zeros are calculated for normalization,
    which is carried out in the fifth stage. Notice that, unlike the conventional
    one, a sixth rounding stage is not necessary in HUB since the result of the fifth
    stage is already normalized and rounded to the nearest HUB number.


    In summary, we have integrated a HUB adder in the Sargantana processor, reducing
    the number of stages and allowing the use of this new format in a RISC-V processor.
    As future work, we plan to extend the HUB format to all FP RISC-V arithmetic operations.


    ## References


    - <span id="page-1-0"></span>[1] J. Hormigo and J. Villalba. "New formats for
    computing with real-numbers under round-to-nearest". In: IEEE Transactions on
    Computers 65.7 (July 2016), pp. 2158– 2168. doi: [10.1109/TC.2015.2479623](https://doi.org/10.1109/TC.2015.2479623).

    - <span id="page-1-1"></span>[2] Vıctor Soria-Pardos et al. "Sargantana: A 1 GHz+
    In-Order RISC-V Processor with SIMD Vector Extensions in 22nm FD-SOI". In: 2022
    25th Euromicro Conference on Digital System Design (DSD). IEEE. 2022, pp. 254–261.

    - <span id="page-1-4"></span>[3] M. Ercegovac and T. Lang. Digital Arithmetic.
    1st. Morgan Kaufmann, 2004.'
  decisions:
    evaluation_prompt: '- Disqualified: no evaluation. Reason: The paper lacks any
      form of empirical, experimental, or quantitative evaluation. There are no sections
      or discussions on evaluation, experiments, results, benchmarks, metrics, comparisons,
      datasets, or performance figures. The paper primarily focuses on the theoretical
      aspects and implementation details of the HUB adder without providing evidence
      of evaluation.'
    related_work_prompt: '- Qualified. Reason: The paper meaningfully engages with
      prior research by citing and discussing previous work related to the HUB format
      and the Sargantana processor. It provides context and comparison to existing
      methods, particularly in the sections discussing the HUB format background and
      the FP HUB adder for Sargantana.'
    novelty_prompt: 'Qualified. Reason: The paper proposes a novel integration of
      the HUB format into the Sargantana RISC-V processor, specifically by developing
      a tailored floating-point HUB adder. This integration reduces the number of
      stages in the adder and eliminates the need for certain rounding operations,
      which is a clear contribution to the field. The paper also claims novelty by
      stating, "Unlike the 6 stage original FPU adder, our floating point HUB adder
      has 5 stages," and discusses the unique benefits of using the HUB format in
      this context.'
    review_only_prompt: '- Qualified. Reason: The paper introduces a new contribution
      by presenting a tailored floating point HUB adder implemented in the Sargantana
      RISC-V processor, rather than primarily summarizing existing work.'
- title: "SpecLLM: Exploring Generation and Review of VLSI Design Specification\n\
    \  with Large Language Model"
  abstract: 'The development of architecture specifications is an initial and fundamental

    stage of the integrated circuit (IC) design process. Traditionally,

    architecture specifications are crafted by experienced chip architects, a

    process that is not only time-consuming but also error-prone. Mistakes in these

    specifications may significantly affect subsequent stages of chip design.

    Despite the presence of advanced electronic design automation (EDA) tools,

    effective solutions to these specification-related challenges remain scarce.

    Since writing architecture specifications is naturally a natural language

    processing (NLP) task, this paper pioneers the automation of architecture

    specification development with the advanced capabilities of large language

    models (LLMs). Leveraging our definition and dataset, we explore the

    application of LLMs in two key aspects of architecture specification

    development: (1) Generating architecture specifications, which includes both

    writing specifications from scratch and converting RTL code into detailed

    specifications. (2) Reviewing existing architecture specifications. We got

    promising results indicating that LLMs may revolutionize how these critical

    specification documents are developed in IC design nowadays. By reducing the

    effort required, LLMs open up new possibilities for efficiency and accuracy in

    this crucial aspect of chip design.'
  url: http://arxiv.org/abs/2401.13266v1
  keywords: ''
  document: '# I. INTRODUCTION


    Developing architecture specifications is a critical initial step in the process
    of IC design. It lays the foundational framework and guidelines necessary for
    the subsequent stages of design and development. Traditionally, the task of writing
    and reviewing architecture specifications is undertaken by skilled chip architects.
    This process, while expertise-driven, tends to be time-consuming and can be susceptible
    to human errors, soliciting a more automated methodology to enhance efficiency
    and accuracy.


    In recent years, LLMs such as ChatGPT [\[1\]](#page-7-0) have showcased remarkable
    capabilities in the field of artificial intelligence, with a wide range of applications
    from question answering to content creation. The growth of chip computing power
    will endow LLMs with greater capabilities. Consequently, researchers have started
    to investigate the potential of LLMs in augmenting the process of chip design,
    reversely enhancing


    <span id="page-0-0"></span>![](_page_0_Figure_11.jpeg)


    Fig. 1: The overall structure of this paper. We first propose basic definitions
    and an organized dataset dedicated to architecture specifications. Leveraging
    them, we explore the use of LLMs in the generation and review of architecture
    specifications.


    the computing power of the chips themselves. For instance, recent studies [\[2,](#page-8-0)
    [3,](#page-8-1) [4,](#page-8-2) [5,](#page-8-3) [6,](#page-8-4) [7,](#page-8-5)
    [8,](#page-8-6) [9\]](#page-8-7) have utilized LLMs to generate RTL code like
    Verilog. Other works develop LLM-based solution to control EDA tools [\[9,](#page-8-7)
    [10\]](#page-8-8), design AI accelerator architectures [\[11,](#page-8-9) [12\]](#page-8-10),
    hardware security assertion generation [\[13\]](#page-8-11), fix security bugs
    [\[14\]](#page-8-12), etc. These research efforts imply a promising future for
    LLMs in chip design. In this paper, we conduct a pioneering investigation in the
    potential and practicality of LLMs in processing specifications. To the best of
    our knowledge, there has been no prior design automation or LLM research focusing
    on this important topic.


    This paper focuses on employing LLMs to address the challenges inherent in the
    traditional management of architecture specifications. Figure [1](#page-0-0) highlights
    the overall structure of this paper. Considering the absence of a formal definition
    or categorization of architecture specifications, we start with investigating
    existing architecture specifications across a diverse range of products. Then
    we categorize architecture specifications into three levels: Highest-level Architecture
    Specification (HAS), Middle-level Architecture Specification (MAS), and Lowest-level
    Architecture Specification (LAS). HAS is defined as the specification that establishes
    standards applicable across


    <sup>1</sup>The dataset will be available at https://github.com/hkust-zhiyao/SpecLLM


    a range of products. It is at the highest abstraction level. MAS represents the
    specification that outlines the high-level architecture of a single-chip design.
    LAS is the detailed specification focused on the microarchitecture design of an
    individual chip.


    Building upon our definition, we have assembled a comprehensive dataset that includes
    46 public architecture specifications from various types of products. For the
    HAS, MAS, or LAS, the architecture specifications related to RISC-V are among
    the most accessible and widely available. In contrast, architecture specifications
    for other instruction sets are relatively scarce. We have also observed that current
    architecture specifications suffer from a lack of unified writing standards, and
    the length of these specifications may exceed the processing capacity of LLMs.
    Both of them complicate the process of handling architecture specifications using
    LLMs.


    Based on the basic formulation, we investigate the application of LLMs in both
    generation and reviewing architecture specifications. Regarding specification
    generation, we suggest two potential approaches. The first is to simplify the
    process of writing architecture specifications for designers, making it more efficient
    and less error-prone. The second approach is applicable when architecture specifications
    are absent for an already implemented chip. In such scenario, we can transform
    the RTL code back into architecture specifications, essentially reconstructing
    the original design documentation from the implemented code. According to our
    experiment, when generating specifications for simple logic circuits, the majority
    of the human tasks could be done by LLMs. We are optimistic that, even for more
    complex logic circuits, it is promising for LLMs to progressively take over a
    large portion of the work.


    We also demonstrate that the LLMs can efficiently review the architecture specifications.
    We first propose our definition of various types of defects in architecture specification
    document. Building on this, we utilized these defects as the target responses
    and developed specific prompts, seeking for LLM''s review. As for the public architecture
    specifications, our experiments have demonstrated that the LLMs could provide
    valuable feedback for enhancing these documents. Moving forward, we plan to extend
    our research to include the evaluation of the review results generated by the
    LLMs, streamlining the review process and enhancing the reliability of the outcomes.


    In summary, we formulate the task of adopting LLMs in the development of architecture
    specifications. We provide fundamental definitions and explore applications of
    LLMs in the realm of architecture specification development, including the basic
    definitions of architecture specifications and various tasks that LLMs can perform
    in this context. Our key contributions are summarized below:


    - We provide structured definitions of architecture specifications, facilitating
    efficient utilization of LLMs in developing architecture specifications. (Section
    [II\)](#page-1-0)

    - We generate a dataset of design specifications by methodically collecting and
    systematically organizing architecture specification documents from a variety
    of online sources. This paves the way for exploring LLMs in the development of
    architecture specifications. (Section [III\)](#page-3-0)

    - We explore the use of LLMs as tools for generating architecture specifications,
    including assisting designers in writing these specifications and converting RTL
    code into comprehensive specifications. Our findings suggest that LLMs hold considerable
    promise in efficiently generating architecture specifications. (Section [IV\)](#page-3-1)

    - We explore the use of LLMs in the review of architecture specifications. We
    identify various potential defects that may arise in these specifications. Based
    on these identified defects, we have crafted specific processes and prompts to
    guide the LLMs in their review. Our experimental results indicate that LLMs are
    capable of providing valuable feedback for improving these documents in aspects
    of accuracy. (Section [V\)](#page-5-0)


    # <span id="page-1-0"></span>II. OUR DEFINITION ON ARCHITECTURE SPECIFICATION


    We define the architecture specification as the document that describes the chip
    architecture prior to RTL coding. Writing architecture specifications for the
    target chip is usually the starting point of the IC design flow. The term architecture
    specification is specifically chosen to emphasize that the document captures the
    architectural aspects of a chip, distinguishing it from general specifications.
    We categorize the architecture specifications into three levels.


    - The Highest-level Architecture Specification (HAS) establishes standards applicable
    to a range of products. A notable example is the RISC-V specifications [\[61\]](#page-8-13).
    It defines the ISA specifications (e.g., instruction formats, register usages)
    and Non-ISA specifications (e.g., trace). Designing specific RISC-V chips should
    comply with these specifications.

    - The Middle-level Architecture Specification (MAS) outlines the high-level architecture
    of a single product. These specifications encompass the essential information
    required to profile the chip design. For example, one MAS for a RISC-V CPU may
    include an overview of the microarchitectures (e.g., block diagram) and their
    primary parameters (e.g., cache size).

    - The Lowest-level Architecture Specification (LAS) details the microarchitecture
    design of a single product. Unlike HAS and MAS, LAS should give the implementation
    details for each microarchitecture, which may involve the ports, internal signals,
    pipelines, and the associated descriptions. By reading them, the designers are
    expected to write the corresponding RTL code correctly.


    The architecture specifications are distinct from the user manuals. We define
    the user manuals as the documents that succinctly outline the chip design post-production.
    In contrast to architecture specifications, which primarily cater to chip designers,
    user manuals are typically crafted for the end users and programmers. Nonetheless,
    we note that certain contents in the user manuals exhibit similarities to the
    HAS or MAS. For example, as described in Section [III,](#page-3-0) we identify
    the *ARMv8-M Architecture Reference Manual* [\[17\]](#page-8-14), *Intel 64 and
    IA-32 Architectures Software Developer''s Manual* [\[18\]](#page-8-15) as HAS;
    and a series of *Arm Core Technical Reference Manual*s [\[28,](#page-8-16) [27,](#page-8-17)
    [26\]](#page-8-18), *Intel 64 and IA-32 Architectures Optimization Reference*
    [\[29\]](#page-8-19) as MAS.


    <span id="page-2-0"></span>


    | Type             | Highest-level Architecture<br>Specifications (HAS)                                                                                                                                                                                                                                                                                                                    |
    Middle-level Architecture<br>Specifications (MAS)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |
    Lowest-level Architecture<br>Specifications (LAS)                                                                                                                                                                                                                                                                                                                                                                                                  |

    |------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|

    | CPU              | • RISC-V ISA Specifications, Unpriv<br>ileged Specification
    [15]<br>• RISC-V ISA Specifications, Privi<br>leged Specification [16]<br>• ARMv8-M
    Architecture Reference<br>Manual [17]<br>• Intel 64 and IA-32 Architectures<br>Software
    Developer''s Manual [18]<br>• The SPARC Architecture Manual,<br>Version 9 [19]<br>•
    OpenRISC 1000 Architecture Man<br>ual [20] | • *The NEORV32 RISC-V Processor:<br>Datasheet
    [21]<br>• *OpenSPARC T1 Microarchitecture<br>Specification [22]<br>• *OpenSPARC
    T2 Core Microarchi<br>tecture Specification [23]<br>• E31 Core Complex Manual
    [24]<br>• E51 Core Complex Manual [25]<br>• Arm<br>Cortex-A78<br>Core<br>Technical<br>Reference
    Manual [26]<br>• Arm Cortex-X2 Core Technical Ref<br>erence Manual [27]<br>• Arm
    Neoverse-N2 Core Technical<br>Reference Manual [28]<br>• Intel 64 and IA-32 Architectures<br>Optimization
    Reference [29]<br>• OpenRISC 1200 IP Core Specifica<br>tion [30] | • The<br>NEORV32<br>RISC-V<br>Processor:<br>Datasheet
    [21]<br>• OpenSPARC<br>T1<br>Microarchitecture<br>Specification [22]<br>• OpenSPARC
    T2 Core Microarchitecture<br>Specification [23]<br>• Amber 2 Core Specification
    [31]<br>• LXP32, a lightweight open source 32-bit<br>CPU core, Technical Reference
    Manual<br>[32]<br>• OpenMSP430, Texas Instruments [33]<br>• NEO430,<br>based<br>on<br>the<br>Texas<br>Instruments
    MSP430(TM) ISA [34] |

    | SoC              | • Efficient Trace for RISC-V [35]<br>• RISC-V<br>External<br>Debug<br>Support<br>[36]<br>•
    RISC-V<br>IOMMU<br>Architecture<br>Specification [37]<br>• RISC-V<br>Advanced<br>Interrupt<br>Architecture
    [38]<br>• RISC-V<br>Platform-Level<br>Interrupt<br>Controller Specification [39]                                                                                |
    • Freedom E310-G000 Manual [40]<br>• Freedom U540-C000 Manual [41]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |
    • #The<br>NEORV32<br>RISC-V<br>Processor:<br>Datasheet [21]<br>• OpenSPARC T2
    System-On-Chip (SoC)<br>Microarchitecture Specification [42]                                                                                                                                                                                                                                                                                                         |

    | Accelerator      | • RISC-V "V" Vector Extension [43]<br>• Intel Advanced Performance
    Exten<br>sions (Intel APX) Architecture Speci<br>fication [44]<br>• Intel Advanced
    Vector Extensions<br>10 (Intel AVX10) Architecture Speci<br>fication [45]                                                                                                                                       |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
    • NVIDIA<br>Deep<br>Learning<br>Accelerator<br>(NVDLA), Hardware Architectural
    Spec<br>ification [46]                                                                                                                                                                                                                                                                                                                                              |

    | Bus &<br>Network | • TileLink Specification [47]<br>• AMBA5 CHI Architecture
    Specifi<br>cation [48]<br>• AMBA5 ACE Protocol Specification<br>(superseded by
    CHI) [49]<br>• AMBA5 AXI Protocol Specification<br>[50]<br>• AMBA4 AXI and ACE
    Protocol<br>Specification [51]                                                                                                                |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
    • 10GE MAC Core Specification [52]<br>• Ethernet IP Core Specification [53]<br>•
    I2C-Master Core Specification [54]<br>• UART to Bus Core Specifications [55]                                                                                                                                                                                                                                                                                      |

    | Arithmetic       |                                                                                                                                                                                                                                                                                                                                                                       |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
    • Elliptic Curve Group Core Specification<br>[56]<br>• Tate Bilinear Pairing Core
    Specification<br>[57]<br>• Tiny Tate Bilinear Pairing Core Specifi<br>cation [58]                                                                                                                                                                                                                                                                                |

    | Crypto           |                                                                                                                                                                                                                                                                                                                                                                       |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
    • AES Core Specification [59]<br>• SHA3 Core Specification [60]                                                                                                                                                                                                                                                                                                                                                                                    |


    | TABLE I: Our proposed dataset for architecture specifications, including approximately
    46 specification documents. |  |  |  |

    |--------------------------------------------------------------------------------------------------------------------|--|--|--|

    |                                                                                                                    |  |  |  |


    <sup>⋆</sup>Notes: \*NEORV32 [\[21\]](#page-8-24), OpenSPARC T1 [\[22\]](#page-8-25),
    OpenSPARC T2 [\[23\]](#page-8-26) include chapters meeting the criterion of MAS.
    #NEORV32 [\[21\]](#page-8-24) encompasses chapters describing the SoC implementation.


    # III. DATASET OVERVIEW


    <span id="page-3-0"></span>Table [I](#page-2-0) provides an overview of our proposed
    dataset for architecture specifications. It shows a variety of public architecture
    specifications and user manuals, organized by product types like CPU, SoC, Accelerator,
    Bus and Network, Arithmetic and Crypto, and by levels such as HAS, MAS, and LAS.
    It is important to note that in Table [I,](#page-2-0) we only display a representative
    selection of about 50 architecture specifications. For documents exhibiting a
    high degree of similarity, like additional *Arm Core Technical Reference Manuals*
    and *AMBA Protocol Specifications*, we have omitted them for simplicity. Our collection
    has yielded several notable findings.


    Availability. For the HAS, MAS, or LAS, the architecture specifications related
    to RISC-V are among the most accessible and widely available. Especially for the
    HAS, there are various established standards that exist, catering to different
    kinds of products, such as the CPU [\[16,](#page-8-21) [15\]](#page-8-20), SoC
    [\[35,](#page-8-34) [37,](#page-8-36) [38,](#page-8-37) [39\]](#page-8-38), Accelerator
    [\[43\]](#page-8-42) and Cache Coherence Bus [\[47\]](#page-8-46). An interesting
    observation is that despite the vast number of open-source designs in the RISC-V
    ecosystem, there is a scarcity of available MAS and LAS documents. Many documents,
    such as the Boom Core [\[62\]](#page-8-60) and XiangShan Core [\[63\]](#page-8-61),
    which describe the design of open-source chips, do not fulfill the standards required
    for formal architecture specifications. Therefore, we have chosen not to include
    these in our table. This gap highlights a potential area for improvement in the
    documentation and standardization of open-source chip designs. For the Arm and
    X86 ecosystem, the availability of formal architecture specifications is weaker.
    We can only find some reference manuals intended for programmers or end-users.
    Despite this limitation, upon investigation, it is observed that the contents
    of these manuals partially align with what one would expect in formal architecture
    specifications. Consequently, we have chosen to include these reference manuals
    for conducting research.


    Standard. While writing architecture specifications is a common practice in commercial
    chip manufacturing, we observe the available open specification documents exhibit
    a variety of writing styles and lack unified writing standards. Within individual
    semiconductor companies, there''s a tendency to maintain a consistent format for
    architecture specifications. Yet, the absence of unified writing standards across
    the industry leads to a diversity in the presentation and structuring of these
    documents, especially evident in open-source chip design [\[21,](#page-8-24) [31,](#page-8-30)
    [32,](#page-8-31) [33,](#page-8-32) [34\]](#page-8-33). This variation in writing
    styles and lack of standardized formats in architecture specifications pose additional
    challenges for LLMs in handling and accurately interpreting these documents.


    Length. The length of HAS spans from just over 100 pages to more than 1000 pages.
    The lengths of MAS and LAS can vary widely, with their extent directly correlating
    to the complexity of the chip designs they describe. MASs outline the chip designs,
    and thus are usually shorter than the LASs, ranging from several pages to in excess
    of 100 pages. LASs extend from over 10 pages to hundreds of pages. The longer
    the architecture specifications are, the more contextual information the LLMs
    tend to process. However, current commercial LLM products have a limitation on
    the number of tokens they can handle, which is not infinite. In Section [V,](#page-5-0)
    as we will discuss, the increase in contextual information due to longer architecture
    specifications poses challenges. It can lead to a decrease in the accuracy of
    the LLM''s output. Therefore, the length of the architecture specifications must
    be carefully considered when employing an LLM to manage these documents.


    In the following sections, we will choose portions of them to demonstrate LLM''s
    capabilities in developing architecture specifications. Constrained by the scarcity
    of existing architecture specifications, we consider certain sections of user
    manuals to function as architecture specifications.


    # <span id="page-3-1"></span>IV. LLM GENERATES ARCHITECTURE SPECIFICATION


    # <span id="page-3-2"></span>*A. Motivation*


    Traditionally, writing architecture specifications is a nontrivial, but critical
    step in the process of IC design. Our research indicates that chip architects
    spend a considerable amount of time writing architecture specifications for several
    reasons:


    - Massive Information Organization: Whether for HAS, MAS, or LAS, a significant
    amount of information must be organized. Systematically arranging them is crucial
    for the ensuing RTL coding process. While criteria for writing architecture specifications
    exist in separate companies, the complexity of this task escalates with the increasing
    scale of the designs.

    - Diverse Modules Interconnection: MAS is responsible for establishing the interconnection
    between various hardware modules. This plays a critical role in ensuring the overall
    correctness of the complete product. Despite this, the process of integrating
    these modules often encounters challenges, such as the time-consuming tasks of
    discerning different interfaces and identifying the functions of each module.

    - Complicate Algorithm Implementation: LAS delineates the structures and behaviors
    for each microarchitectural design. According to our observation, LAS is closest
    to the final RTL design among the three types of architecture specifications.
    When dealing with complex microarchitectural algorithms, such as branch prediction,
    writing LAS presents significant challenges, which include accurately defining
    signals and state machines.


    LLMs are promising in solving the above challenges and assisting designers in
    writing architecture specifications. According to Section [II,](#page-1-0) HASs
    set the standards for a variety of products and are inherently determined by human
    decisions. Consequently, they are highly flexible and challenging to generate.
    We thus mainly focus on using LLMs to automatically generate the MAS and LAS.
    We explore two LLM-based applications:


    - 1) LLMs can reduce human effort by writing architecture specifications (Section
    [IV-B\)](#page-4-0).

    - 2) Many open-source chip designs lack comprehensive and formal architecture
    specifications. In situations where these specifications are absent, LLMs could
    reversely convert the RTL code into detailed specification documents. This reverse
    engineering process can bridge the


    ## <span id="page-4-3"></span>**Prompt:**


    From now on, you act as a chip architect writing the architecture specification.
    This specification must encompass detailed descriptions of microarchitectures,
    including aspects such as ports, internal signals, state machines. Subsequently,
    I will assign you a specific module to focus on in your writing.


    Fig. 2: An example prompt to initiate generating LAS. This generation is based
    on the designers'' brief description.


    ## <span id="page-4-4"></span>**Prompt:**


    Please write an architecture specification regarding a perpetual calendar. The
    starting value of Secs, Mins, and Hours are all 0. Both Secs and Mins loop continuously
    from 0 to 59. When Secs=59, Min increases by 1 at the next cycle, and when Min=59
    && Secs=59, Hours increases by 1 at the next cycle. Hours is counted from the
    0-23 cycle.


    Fig. 3: An example prompt to request GPT generating an architecture specification
    for perpetual calendar. The response is shown in Figure [4.](#page-4-1)


    gap in the documentation and enhance understanding and accessibility of the chip
    designs (Section [IV-C\)](#page-4-2).


    In the upcoming sections, we will leverage the widely-used commercial LLM product—GPT-4
    [\[1\]](#page-7-0), to demonstrate how this technology can be effectively exploited
    to enhance the process of writing architecture specifications.


    # <span id="page-4-0"></span>*B. Assist Designers to Write Architecture Specifications*


    Figure [2](#page-4-3) presents an example of a prompt to initiate the generation
    of LAS. This example, referencing [\[64\]](#page-8-62), indicates the specific
    role expected to be performed by the LLMs, and the expected response format. Then
    we employ the prompt in Figure [3](#page-4-4) to direct GPT generating the architecture
    specifications. This particular example, sourced from [\[4\]](#page-8-2), has
    been selected due to its moderate level of complexity, making it suitable for
    testing purposes. Figure [4](#page-4-1) presents the outcomes of GPT-4. The experimental
    results demonstrate that the GPT can accurately understand the questions. These
    specifications, spanning from the high-level functional description to the lowlevel
    state machines, offer in-depth information for subsequent RTL coding. Yet, the
    design of a perpetual calendar is a classical task in the field of circuit design.
    The LLMs have abundant corpus about it. The commercial products, usually include
    internal IPs, which are not used to train the external LLMs like ChatGPT. We believe
    it is non-trivial to generate that kind of architecture specifications. However,
    our experiments at least demonstrate that the LLMs possess the potential to conduct
    the task of generating architecture specifications. The speed of the LLMs in performing
    this task, compared to the experienced chip architects, still has significant
    advantages.


    # <span id="page-4-2"></span>*C. Convert RTL code to Architecture Specifications*


    Figure [5](#page-5-1) showcases the prompt employed for converting the RTL code
    into LAS. Following this prompt, we provide the RTL code to the LLM, which can
    be in the form of textual format or as a source code file. In our example, we
    adopt the Verilog file that is open-sourced by RTLLM [\[65\]](#page-8-63) and
    it is uploaded to our GitHub repo. The response of GPT is shown in Figure [6.](#page-5-2)
    In summary, the generated architecture


    ## <span id="page-4-1"></span>**Response:**


    ## **Architecture Specification for a Perpetual Calendar Module**


    ## **Overview**


    This document outlines the architectural specification for a Perpetual Calendar
    Module (PCM). The PCM is responsible for tracking time in seconds, minutes, and
    hours, with a continuous looping mechanism. The time is represented in a 24-hour
    format. The module initializes with all time values (seconds, minutes, hours)
    at zero and increments according to predefined rules.


    ## **Functional Description**


    #### **Time Units**


    **Seconds (Secs):** This unit ranges from 0 to 59. It resets to 0 after reaching
    59.


    **Minutes (Mins):** This unit also ranges from 0 to 59. It increments by 1 when
    Secs roll over from 59 to 0.


    **Hours:** This unit ranges from 0 to 23. It increments by 1 when both Mins and
    Secs roll over from 59 to 0.


    **Initial State**


    All units (Secs, Mins, Hours) are initialized to 0 at the start.


    **Increment Logic**


    **Secs Increment:** Occurs every cycle. Rolls over to 0 after reaching 59. **Mins
    Increment:** Occurs when Secs roll over from 59 to 0. **Hours Increment:** Occurs
    when both Mins and Secs simultaneously roll over from 59 to 0.


    ## **Microarchitecture**


    #### **Components**


    **Counter Units:** Three independent counters for Secs, Mins, and Hours. **Control
    Logic:** Determines the rollover conditions and triggers the increment of Mins
    and Hours.


    **Clock Signal:** Drives the incrementation of Secs.


    # **Counter Behavior**


    Each counter is a modulo-60 counter for Secs and Mins, and a modulo-24 counter
    for Hours.


    The counters are synchronized with the clock signal.


    **Control Logic**


    Monitors the state of Secs and Mins.


    When Secs reach 59, it sends a pulse to increment Mins at the next cycle. When
    both Secs and Mins are 59, it sends a pulse to increment Hours at the next cycle.


    # **Interface**


    **Input:** Clock signal.


    **Output:** Current time in hours, minutes, and seconds.


    ## **State Machines**


    Seconds Counter (Secs) **State 0-58:** Increment Secs by 1 each clock cycle. **State
    59:** Reset Secs to 0 on the next clock cycle and trigger a Min increment signal.
    Minutes Counter (Mins) **State 0-58 (No Increment Signal):** Hold current value.
    **State 59 (No Increment Signal):** Hold current value.


    **State 0-58 (With Increment Signal):** Increment Mins by 1 on the next clock
    cycle and reset signal.


    **State 59 (With Increment Signal):** Reset Mins to 0 on the next clock cycle,
    reset signal, and trigger an Hour increment signal.


    Hours Counter


    **State 0-22 (No Increment Signal):** Hold current value.


    **State 23 (No Increment Signal):** Hold current value.


    **State 0-22 (With Increment Signal):** Increment Hours by 1 on the next clock
    cycle and reset signal.


    **State 23 (With Increment Signal):** Reset Hours to 0 on the next clock cycle
    and reset signal.


    Fig. 4: A GPT response example to the prompt in Figure [3.](#page-4-4)


    ## <span id="page-5-1"></span>**Prompt:**


    From now on, you act as a chip architect writing the architecture specification.
    This specification must encompass detailed descriptions of microarchitectures,
    including aspects such as ports, internal signals, state machines. Subsequently,
    I will provide you with the RTL code. Please read the RTL code and generate the
    corresponding architecture specification.


    Fig. 5: An example prompt to initiate writing architecture specifications on the
    basis of RTL code. The response is shown in Figure [6.](#page-5-2)


    specifications are slightly shorter than those depicted in Figure [4,](#page-4-1)
    yet they effectively preserve the correct meaning. We conduct experiments to analyze
    this phenomenon. Our findings indicate that the provided RTL code confines the
    scope for the specification generation. Without the RTL code, the LLM may generate
    the architecture specifications on the basis of the previously trained corpus,
    which exhibits diverse explanations for the single circuit design. On the contrary,
    after we offer the LLM with a unique RTL code, its responses are likely to be
    more orientated.


    # <span id="page-5-0"></span>V. LLM REVIEWS ARCHITECTURE SPECIFICATION


    # *A. Motivation*


    The accuracy and precision of architecture specifications directly affect the
    overall quality of chip designs. Ensuring these specifications are correct is
    crucial for the successful development and functionality of the chips. Traditionally,
    the chip company should spend efforts to review architecture specifications. Analogous
    to the reasons mentioned in Section [IV-A,](#page-3-2) reviewing architecture
    specifications also requires significant human efforts. LLMs can also help reviewers
    in these tasks. At least, the reviewers could use these tools to get an overview
    of the specification files and obtain some comments from them. Fortunately, as
    we will demonstrate in Section [V-B](#page-5-3) and Section [V-C,](#page-6-0)
    the LLMs show promise in achieving more profound objectives in the review of architectural
    specifications, going beyond basic overviews to provide detailed and meaningful
    insights.


    # <span id="page-5-3"></span>*B. Defects Category*


    <span id="page-5-4"></span>


    | Type              | Potential Defects                                                                                                                                                                                 |  |  |

    |-------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--|--|

    | Common<br>Case    | • Typographical Error<br>• Inconsistence or Contradiction
    Error<br>• Incomplete or Unclear Error                                                                                                  |  |  |

    | Level<br>Specific | • Combinational Loops Error (LAS)<br>• Uninitialized Register
    Value Error (LAS)<br>• Improvement<br>for<br>Micro-architectural<br>Design<br>(LAS)<br>•
    Improvement for Architectural Design (MAS) |  |  |

    | Level<br>Spanning | • Inconsistence or Contradiction Error<br>(Across Various
    Levels)                                                                                                                                 |  |  |


    TABLE II: Potential Defects Occurred in Different Levels Architecture Specifications


    Initially, we have pinpointed various types of potential defects that may occur
    in architecture specifications. Table [II](#page-5-4)


    #### <span id="page-5-2"></span>**Response:**


    I have received the RTL (Register Transfer Level) code file named "verified\_calendar.v".
    I''ll proceed to read and analyze this file to generate a corresponding architecture
    specification. Let''s start by examining the contents of the file.


    The RTL code in "**verified\_calendar.v**" defines a module named verified\_calendar.
    This module appears to be designed as a simple digital clock. Here''s a brief
    overview of the module and its key components based on the initial part of the
    code:


    # **Module: verified\_calendar**


    ## **Inputs:**


    - **CLK**: Clock input.

    - **RST**: Reset input.

    - **Outputs:**

    - **Hours**: 6-bit output representing hours.

    - **Mins:** 6-bit output representing minutes.

    - **Secs:** 6-bit output representing seconds.


    ## **Internal Registers:**


    • **Hours, Mins, Secs**: 6-bit registers for storing hours, minutes, and seconds.


    ## **Behavioral Description:**


    The module contains three **always** blocks, each triggered on the positive edge
    of **CLK** or **RST**.


    - **Seconds Logic:**

    - On reset (**RST**), seconds (**Secs**) are set to 0.

    - If seconds equal 59, they roll over to 0; otherwise, they increment every clock
    cycle.


    ## **Minutes Logic:**


    - On reset, minutes (**Mins**) are set to 0.

    - Minutes increment when seconds roll over from 59 to 0. If minutes are at 59
    when seconds roll over, they reset to 0.


    **Hours Logic:**


    • Not fully visible in the provided snippet, but likely similar to minutes, incrementing
    as minutes roll over.


    ## **State Machine for Seconds:**


    ## States: **Reset**, **Count**


    Transitions:


    - **Reset** to **Count**: When **RST** de-asserted.

    - **Count** to **Reset**: When **RST** asserted or **Secs** reach 59.

    - Output: Increments **Secs** or resets to 0 based on the state.


    # **State Machine for Minutes:**


    States: **Reset**, **Count**


    Transitions:


    - **Reset** to **Count**: When **RST** de-asserted.

    - **Count** to **Reset**: When **Mins** reach 59 and **Secs** are 59.


    Output: Increments **Mins** or resets to 0 based on the state and **Secs**.


    Fig. 6: A GPT response example to the prompt in Figure [5.](#page-5-1)


    summarizes these defects, categorizing them according to the level of architecture
    specifications.


    In many instances, there are defects that could manifest in all three tiers of
    architecture specifications: HAS, MAS, and LAS. They include the *Typographical
    Error*, *Inconsistence or Contradiction Error*, and *Incomplete or Unclear Error*.
    The *Inconsistence or Contradiction Error* denotes situations within a single
    specification file where either two concepts describing the same object are inconsistent,
    or two related concepts are contradictory. The *Incomplete or Unclear Error* refers
    to instances where certain concepts lack essential information, resulting in sentences
    that are open to ambiguous interpretations.


    Furthermore, the potential defects could also be levelspecific, indicating that
    HAS, MAS, or LAS may each have distinct defects. For example, LAS might include
    details about port connections between various modules. LLMs can be employed to
    scrutinize the LAS for identifying potential *Combinational Loops Error*. Such
    scrutiny, however, may not be applicable to HAS and MAS. In addition to locating
    the writing issues, we also identify some high-level objectives for the review
    of architecture specification, such as the *Improvement for Architectural Design*
    and *Improvement for Micro-architectural Design*. These objectives are aimed at
    leveraging the potential of the review process to enhance both the architectural
    and micro-architectural design aspects. This goes beyond mere error correction,
    focusing on the overall optimization and refinement of the design.


    Additionally, these defects might span across different levels. One example of
    this is the *Inconsistence or Contradiction Error (Across Various Levels)*, that
    the lower-level architecture specifications should not contradict the contents
    established in the higher-level specifications. Note that it is not equivalent
    to the *Inconsistence or Contradiction Error* established for the common case.
    The former indicates the inconsistence or contradiction between different levels
    of architecture specification files, while the latter confines the error in one
    individual specification file.


    # <span id="page-6-0"></span>*C. Architecture Specification Review*


    We exploit the potential defects in architecture specifications as the types of
    feedback (Section [V-B\)](#page-5-3) we aim to receive from the review conducted
    by the LLMs. We have developed two methodologies to guide the LLMs in reviewing
    architecture specifications. The first method involves directly uploading the
    entire specification file and requesting the LLMs to provide feedback section
    by section. The second method entails splitting the specification file into sections,
    uploading each section individually, and then asking the LLMs to conduct a review
    immediately upon the upload of each section. Regarding the first approach, it
    has the advantage of not requiring extra steps to divide the specification file.
    However, we''ve observed that even when instructed to give feedback for each section,
    the LLMs struggle to accurately recognize the boundaries between different sections.
    This often results in the provision of incorrect or misleading indication messages
    by the LLMs. The second approach, which involves splitting the specification file
    into sections and uploading each one separately for review, can effectively alleviate
    this problem by providing clearer boundaries for the LLMs to analyze and respond
    to each section individually. In the subsequent sections, we will use part of
    the architecture specifications, as shown in Table [I,](#page-2-0) to demonstrate
    LLM''s strengths and weaknesses in reviewing architecture specifications.


    *1) Reviewing One File at a Time:* We employ the Amber 2 Core Specification [\[31\]](#page-8-30)
    to conduct the experiments of reviewing architecture specifications. The tested
    file used in our example is from [\[66\]](#page-8-64) and it has been uploaded
    to our


    ## <span id="page-6-2"></span>**Prompt:**


    From now on, you act as a chip architect reviewing the architecture specification.
    For the following document, please find any typo, inconsistence/contradiction,
    incomplete/unclear and propose the corresponding improvement suggestions. The
    responding format should separately list the findings about typo, inconsistence/contradiction,
    incomplete/unclear and the corresponding improvement suggestions.


    Fig. 7: An example prompt to initiate the review of architecture specifications.
    The response is shown in Figure [8.](#page-6-1)


    ## <span id="page-6-1"></span>**Response:**


    I have thoroughly reviewed the Amber Core Specification document. Here are the
    summarized findings:


    1. **Typos and Grammatical Errors:** The document contains some typographical
    and grammatical errors which can be corrected for clarity and professionalism.


    2. **Inconsistencies/Contradictions:** Certain technical specifications and descriptions
    appear to be inconsistent or contradictory, which could lead to confusion in understanding
    the architecture and functionality of the Amber Core.


    3. **Incomplete/Unclear Sections:** Some sections of the document are either incomplete
    or not clearly explained, which may hinder the complete understanding of the core''s
    architecture and operations.


    For a detailed report including specific instances of these issues and improvement
    suggestions, please let me know, and I will provide a comprehensive breakdown.


    Fig. 8: A GPT response example to the prompt in Figure [7.](#page-6-2)


    ## <span id="page-6-3"></span>**Prompt:**


    Good, let''s review Section 1, and please provide specific and comprehensive review
    results. The response format should comply with the previous requirements.


    Fig. 9: An example prompt to review individual sections of the specification document.
    The response is shown in Figure [10.](#page-7-1)


    GitHub repo. Its concise contents enable current LLMs like GPT-4 to review it
    at a time. According to Table [II,](#page-5-4) we develop a prompt to request
    GPT-4 providing feedback on common defects. We use the prompt, as illustrated
    in Figure [7,](#page-6-2) to initiate the process of writing architecture specifications.
    Next, we upload the specification file selected for testing and ask GPT to summarize
    its findings. As illustrated in Figure [8,](#page-6-1) the LLM will then provide
    an overview of its analysis. This information can offer designers a quick glimpse
    into the results of the review. Subsequently, as depicted in Figure [9,](#page-6-3)
    we request GPT to provide us with detailed review results for each individual
    section of the specification document. Figure [10](#page-7-1) displays the in-depth
    review results specifically for Section 1 of the Amber Core Specification. In
    general, we have observed that GPT-4 is adept at correctly identifying typographical
    errors. Furthermore, in more complex cases such as inconsistence or contradiction
    errors, as well as incomplete or unclear errors, GPT-4 is also capable of offering
    constructive advice. It can give the reasons why these types of errors exist and
    suggest ways to improve them. Nonetheless, our research has revealed that GPT-4
    faces challenges in effectively dividing an archi-


    ## <span id="page-7-1"></span>**Response:**


    ## **Section 1 Review of the Amber Core Specification**


    Typos:


    1. "seperate" (in the description of Amber 25) should be "separate."


    2. "Mhz" (in the capability description of Amber 25) should be "MHz."


    3. "that" (in the performance comparison of Amber 25 and Amber 23) should be "than."


    4. "has" (in the paragraph about Linux kernel verification) should be "have."


    5. "pipline" (in the last sentence) should be "pipeline."


    #### Inconsistencies/Contradictions:


    1. The document states that the Amber 23 and Amber 25 cores are 100% software
    compatible, yet they have differing pipeline stages and cache architectures. This
    could potentially lead to different performance characteristics not addressed
    in the compatibility claim.


    ## Incomplete/Unclear Sections:


    1. The description of the Wishbone interface is vague. More detailed specifications
    or references to the interface''s documentation would improve clarity.


    2. The performance metrics (DMIPS per MHz) are provided, but there''s no baseline
    comparison or explanation of what these figures imply in practical terms.


    3. The document mentions that the cores have been verified with a 2.4 Linux kernel
    but does not detail the extent or scope of this verification.


    4. The information about the FPGA synthesis is incomplete. Details about the synthesis
    process, along with any challenges or optimizations, would provide a clearer picture
    of the implementation process.


    ## Improvement Suggestions:


    1. **Typos**: Correct all typographical errors for accuracy and professionalism.


    2. **Inconsistencies/Contradictions:** Clarify the software compatibility claim
    considering the different architectures of Amber 23 and Amber 25. Include any
    limitations or specific scenarios where this compatibility might not hold.


    ## 3. **Incomplete/Unclear Sections:**


    - Expand the description of the Wishbone interface, possibly including a subsection
    dedicated to its architecture and how it interfaces with the Amber cores.

    - Provide a brief explanation or a reference to what DMIPS per MHz represents,
    helping readers unfamiliar with this metric understand the core''s performance.

    - Detail the scope of the Linux kernel verification. Specify what tests were performed,
    any limitations encountered, and how these tests validate the core''s functionality.

    - Elaborate on the FPGA synthesis process, including challenges faced, optimizations
    made, and why specific FPGA boards were chosen for testing.


    Fig. 10: A GPT response example to the prompt in Figure [9.](#page-6-3)


    tecture specification file into distinct sections. This limitation often results
    in the feedback provided for one section possibly originating from other sections
    of the document. To mitigate this problem, we propose to split the complete specification
    file into individual sections, and request GPT to review them sequentially.


    *2) Splitting and Reviewing File Section-by-Section:* Similar to the first method,
    we begin by using prompts, as shown in Figure [7,](#page-6-2) to initiate the
    review of architecture specifications. Next, instead of uploading the entire architecture
    specification file for review, we submit only the file containing the contents
    of the individual section as an attachment. Simultaneously, we provide the prompt,
    as illustrated in Figure [11,](#page-7-2) directing


    #### <span id="page-7-2"></span>**Prompt:**


    Please review the section 1 first, and give me specific and comprehensive reviewing
    results. The responding format should comply with the previous requirements.


    **( Note: attach the divided file that represents Section 1 of the architecture
    specification file)**


    Fig. 11: An example prompt to review the split files. Though the response from
    GPT is similar to the Figure [10,](#page-7-1) the feedback is more likely to concentrate
    on the individual section.


    GPT to review each section. In this strategy, each review task for different sections
    is paired with its corresponding chapter file, ensuring that each section is reviewed
    in isolation and in context. The responses given by the GPT in this review process
    maintain a format similar to the outputs generated by the previous, more holistic
    review method. However, we have observed that the feedbacks are more likely to
    concentrate on each individual section. Furthermore, the GPT appears more effective
    in identifying a broader range of errors, which increases the number of available
    output items. The accurate and abundant review results enable designers to quickly
    but comprehensively locate the defects in their specification documents.


    # *D. Feedbacks Evaluation*


    Not all feedback provided by the LLM is correct or useful. Therefore, it is essential
    to have strategies in place to evaluate the review results of architecture specifications
    effectively. The most intuitive method is to ask the designers to check the outputs
    of the LLM. This approach, compared to directly reviewing the architecture specifications
    themselves, is already more efficient, potentially saving significant time and
    effort. Nonetheless, an alternative approach involves training a new language
    model specifically for evaluating and filtering the review results provided by
    the LLMs. This specialized model could be designed to assess the relevance and
    accuracy of the LLM''s feedback [\[67\]](#page-8-65), thereby streamlining the
    review process and enhancing the reliability of the outcomes. We leave this part
    for future exploration.


    # VI. CONCLUSION


    In this paper, we propose a novel framework for utilizing LLMs to generate and
    review architecture specifications. We defined architecture specifications, categorized
    them into clear levels, and compiled a corresponding dataset. Our approach focuses
    on two primary applications of LLMs: firstly, in generating architecture specifications,
    including assistance in writing and converting RTL code into specifications, and
    secondly, in the review of these specifications. This innovative methodology signifies
    a transformative step in architecture specification development, offering a path
    toward more efficient, accurate, and streamlined processes in chip design.


    # REFERENCES


    <span id="page-7-0"></span>[1] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya,
    F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat *et al.*, "Gpt-4
    technical report," *arXiv preprint arXiv:2303.08774*, 2023.


    - <span id="page-8-0"></span>[2] S. Liu, W. Fang, Y. Lu, Q. Zhang, H. Zhang, and
    Z. Xie, "Rtlcoder: Outperforming gpt-3.5 in design rtl generation with our open-source
    dataset and lightweight solution," *arXiv preprint arXiv:2312.08617*, 2023.

    - <span id="page-8-1"></span>[3] J. Blocklove, S. Garg, R. Karri, and H. Pearce,
    "Chip-chat: Challenges and opportunities in conversational hardware design," *arXiv
    preprint arXiv:2305.13243*, 2023.

    - <span id="page-8-2"></span>[4] Y. Lu, S. Liu, Q. Zhang, and Z. Xie, "Rtllm:
    An open-source benchmark for design rtl generation with large language model,"
    *arXiv preprint arXiv:2308.05345*, 2023.

    - <span id="page-8-3"></span>[5] M. Liu, N. Pinckney, B. Khailany, and H. Ren,
    "Verilogeval: Evaluating large language models for verilog code generation," *arXiv
    preprint arXiv:2309.07544*, 2023.

    - <span id="page-8-4"></span>[6] S. Thakur, B. Ahmad, Z. Fan, H. Pearce, B. Tan,
    R. Karri, B. Dolan-Gavitt, and S. Garg, "Benchmarking large language models for
    automated verilog rtl code generation," in *DATE*, 2023.

    - <span id="page-8-5"></span>[7] S. Thakur, J. Blocklove, H. Pearce, B. Tan, S.
    Garg, and R. Karri, "Autochip: Automating hdl generation using llm feedback,"
    *arXiv preprint arXiv:2311.04887*, 2023.

    - <span id="page-8-6"></span>[8] M. Nair, R. Sadhukhan, and D. Mukhopadhyay, "Generating
    secure hardware using chatgpt resistant to cwes," *Cryptology ePrint Archive*,
    2023.

    - <span id="page-8-7"></span>[9] M. Liu, T.-D. Ene, R. Kirby, C. Cheng, N. Pinckney,
    R. Liang, J. Alben, H. Anand, S. Banerjee, I. Bayraktaroglu *et al.*, "Chipnemo:
    Domainadapted llms for chip design," *arXiv preprint arXiv:2311.00176*, 2023.

    - <span id="page-8-8"></span>[10] Z. He, H. Wu, X. Zhang, X. Yao, S. Zheng, H.
    Zheng, and B. Yu, "Chateda: A large language model powered autonomous agent for
    eda," in *MLCAD Workshop*, 2023.

    - <span id="page-8-9"></span>[11] Y. Fu, Y. Zhang, Z. Yu, S. Li, Z. Ye, C. Li,
    C. Wan, and Y. Lin, "Gpt4aigchip: Towards next-generation ai accelerator design
    automation via large language models," *arXiv preprint arXiv:2309.10730*, 2023.

    - <span id="page-8-10"></span>[12] Z. Yan, Y. Qin, X. S. Hu, and Y. Shi, "On the
    viability of using llms for sw/hw co-design: An example in designing cim dnn accelerators,"
    *arXiv preprint arXiv:2306.06923*, 2023.

    - <span id="page-8-11"></span>[13] R. Kande, H. Pearce, B. Tan, B. Dolan-Gavitt,
    S. Thakur, R. Karri, and J. Rajendran, "Llm-assisted generation of hardware assertions,"
    *arXiv preprint arXiv:2306.14027*, 2023.

    - <span id="page-8-12"></span>[14] B. Ahmad, S. Thakur, B. Tan, R. Karri, and
    H. Pearce, "Fixing hardware security bugs with large language models," *arXiv
    preprint arXiv:2302.01215*, 2023.

    - <span id="page-8-20"></span>[15] *RISC-V ISA Specifications Volume 1, Unprivileged
    Specification*, ["https:]("https://wiki.riscv.org/display/HOME/RISC-V+Technical+Specifications"
    ) [//wiki.riscv.org/display/HOME/RISC-V+Technical+Specifications".]("https://wiki.riscv.org/display/HOME/RISC-V+Technical+Specifications"
    )

    - <span id="page-8-21"></span>[16] *RISC-V ISA Specifications Volume 2, Privileged
    Specification*, ["https:]("https://wiki.riscv.org/display/HOME/RISC-V+Technical+Specifications"
    ) [//wiki.riscv.org/display/HOME/RISC-V+Technical+Specifications".]("https://wiki.riscv.org/display/HOME/RISC-V+Technical+Specifications"
    )

    - <span id="page-8-14"></span>[17] *ARMv8-M Architecture Reference Manual*, ["https://]("https://documentation-service.arm.com/static/5f8efe85f86e16515cdbe596?token="
    ) [documentation-service.arm.com/static/5f8efe85f86e16515cdbe596?]("https://documentation-service.arm.com/static/5f8efe85f86e16515cdbe596?token="
    ) [token=".]("https://documentation-service.arm.com/static/5f8efe85f86e16515cdbe596?token="
    )

    - <span id="page-8-15"></span>[18] *Intel 64 and IA-32 Architectures Software
    Developer''s Manua*, ["https:]("https://cdrdv2.intel.com/v1/dl/getContent/671200"
    ) [//cdrdv2.intel.com/v1/dl/getContent/671200".]("https://cdrdv2.intel.com/v1/dl/getContent/671200"
    )

    - <span id="page-8-22"></span>[19] S. I. Inc and D. L. Weaver, *The SPARC architecture
    manual*, 1994.

    - <span id="page-8-23"></span>[20] *OpenRISC 1000 Architecture Manua*, ["https://openrisc.io/or1k.html".]("https://openrisc.io/or1k.html"
    )

    - <span id="page-8-24"></span>[21] *The NEORV32 RISC-V Processor: Datasheet*,
    ["https://stnolting.github.]("https://stnolting.github.io/neorv32/" ) [io/neorv32/".]("https://stnolting.github.io/neorv32/"
    )

    - <span id="page-8-25"></span>[22] *OpenSPARC T1 Microarchitecture Specification*,
    ["https://www.oracle.com/technetwork/systems/opensparc/]("https://www.oracle.com/technetwork/systems/opensparc/t1-01-opensparct1-micro-arch-1538959.html"
    ) [t1-01-opensparct1-micro-arch-1538959.html".]("https://www.oracle.com/technetwork/systems/opensparc/t1-01-opensparct1-micro-arch-1538959.html"
    )

    - <span id="page-8-26"></span>[23] *OpenSPARC T2 Microarchitecture Specification*,
    ["https://www.oracle.com/technetwork/systems/opensparc/]("https://www.oracle.com/technetwork/systems/opensparc/t2-06-opensparct2-core-microarch-1537749.html"
    ) [t2-06-opensparct2-core-microarch-1537749.html".]("https://www.oracle.com/technetwork/systems/opensparc/t2-06-opensparct2-core-microarch-1537749.html"
    )

    - <span id="page-8-27"></span>[24] *E31 Core Complex Manual*, ["https://d2pn104n81t9m2.cloudfront.net/]("https://d2pn104n81t9m2.cloudfront.net/documentation/")
    [documentation/".]("https://d2pn104n81t9m2.cloudfront.net/documentation/")

    - <span id="page-8-28"></span>[25] *E51 Core Complex Manual*, ["https://d2pn104n81t9m2.cloudfront.net/]("https://d2pn104n81t9m2.cloudfront.net/documentation/")
    [documentation/".]("https://d2pn104n81t9m2.cloudfront.net/documentation/")

    - <span id="page-8-18"></span>[26] *Arm Cortex-A78 Core Technical Reference Manual*,
    ["https://developer.]("https://developer.arm.com/documentation/101430/latest/"
    ) [arm.com/documentation/101430/latest/".]("https://developer.arm.com/documentation/101430/latest/"
    )

    - <span id="page-8-17"></span>[27] *Arm Cortex-X2 Core Technical Reference Manual*,
    ["https://developer.]("https://developer.arm.com/documentation/101803/latest/"
    ) [arm.com/documentation/101803/latest/".]("https://developer.arm.com/documentation/101803/latest/"
    )

    - <span id="page-8-16"></span>[28] *Arm Neoverse N2 Core Technical Reference Manual*,
    ["https://developer.]("https://developer.arm.com/documentation/102099/latest/The-Neoverse-N2--core"
    ) [arm.com/documentation/102099/latest/The-Neoverse-N2--core".]("https://developer.arm.com/documentation/102099/latest/The-Neoverse-N2--core"
    )

    - <span id="page-8-19"></span>[29] *Intel 64 and IA-32 Architectures Optimization
    Reference*, ["https://www.intel.com/content/www/us/en/content-details/671488/]("https://www.intel.com/content/www/us/en/content-details/671488/intel-64-and-ia-32-architectures-optimization-reference-manual.html"
    ) [intel-64-and-ia-32-architectures-optimization-reference-manual.html".]("https://www.intel.com/content/www/us/en/content-details/671488/intel-64-and-ia-32-architectures-optimization-reference-manual.html"
    )

    - <span id="page-8-29"></span>[30] *OpenRISC 1200 IP Core Specification*, ["https://github.com/openrisc/]("https://github.com/openrisc/or1200/blob/master/doc/openrisc1200_spec.pdf")
    [or1200/blob/master/doc/openrisc1200]("https://github.com/openrisc/or1200/blob/master/doc/openrisc1200_spec.pdf")
    spec.pdf".

    - <span id="page-8-30"></span>[31] *Amber ARM-compatible core*, ["https://opencores.org/projects/amber".]("https://opencores.org/projects/amber"
    )

    - <span id="page-8-31"></span>[32] *LXP32, a lightweight, open source and FPGA-friendly
    32-bit CPU IP core*, ["https://opencores.org/projects/lxp32".]("https://opencores.org/projects/lxp32"
    )

    - <span id="page-8-32"></span>[33] *OpenMSP430, Texas Instruments*, ["https://opencores.org/projects/]("https://opencores.org/projects/openmsp430"
    ) [openmsp430".]("https://opencores.org/projects/openmsp430" )

    - <span id="page-8-33"></span>[34] *NEO430, based on the Texas Instruments MSP430(TM)
    ISA*, ["https://]("https://opencores.org/projects/neo430" ) [opencores.org/projects/neo430".]("https://opencores.org/projects/neo430"
    )

    - <span id="page-8-34"></span>[35] *Efficient Trace for RISC-V*, ["https://wiki.riscv.org/display/HOME/]("https://wiki.riscv.org/display/HOME/RISC-V+Technical+Specifications"
    ) [RISC-V+Technical+Specifications".]("https://wiki.riscv.org/display/HOME/RISC-V+Technical+Specifications"
    )

    - <span id="page-8-35"></span>[36] *RISC-V External Debug Support*, ["https://wiki.riscv.org/display/HOME/]("https://wiki.riscv.org/display/HOME/RISC-V+Technical+Specifications"
    ) [RISC-V+Technical+Specifications".]("https://wiki.riscv.org/display/HOME/RISC-V+Technical+Specifications"
    )

    - <span id="page-8-36"></span>[37] *RISC-V IOMMU Architecture Specification*,
    ["https://wiki.riscv.org/]("https://wiki.riscv.org/display/HOME/RISC-V+Technical+Specifications"
    ) [display/HOME/RISC-V+Technical+Specifications".]("https://wiki.riscv.org/display/HOME/RISC-V+Technical+Specifications"
    )

    - <span id="page-8-37"></span>[38] *RISC-V Advanced Interrupt Architecture*, ["https://wiki.riscv.org/display/]("https://wiki.riscv.org/display/HOME/RISC-V+Technical+Specifications"
    ) [HOME/RISC-V+Technical+Specifications".]("https://wiki.riscv.org/display/HOME/RISC-V+Technical+Specifications"
    )

    - <span id="page-8-38"></span>[39] *RISC-V Platform-Level Interrupt Controller
    Specification*, ["https://wiki.]("https://wiki.riscv.org/display/HOME/RISC-V+Technical+Specifications"
    ) [riscv.org/display/HOME/RISC-V+Technical+Specifications".]("https://wiki.riscv.org/display/HOME/RISC-V+Technical+Specifications"
    )

    - <span id="page-8-39"></span>[40] *Freedom E310-G000 Manual*, ["https://www.sifive.com/documentation".]("https://www.sifive.com/documentation"
    )

    - <span id="page-8-40"></span>[41] *Freedom U540-C000 Manual*, ["https://www.sifive.com/documentation".]("https://www.sifive.com/documentation"
    )

    - <span id="page-8-41"></span>[42] *OpenSPARC T2 System-On-Chip (SoC) Microarchitecture
    Specification*, ["https://www.oracle.com/docs/tech/systems/]("https://www.oracle.com/docs/tech/systems/t2-07-opensparct2-socmicroarchvol1.pdf"
    ) [t2-07-opensparct2-socmicroarchvol1.pdf".]("https://www.oracle.com/docs/tech/systems/t2-07-opensparct2-socmicroarchvol1.pdf"
    )

    - <span id="page-8-42"></span>[43] *RISC-V "V" Vector Extension*, ["https://inst.eecs.berkeley.edu/]("https://inst.eecs.berkeley.edu/~cs152/sp20/handouts/sp20/riscv-v-spec.pdf")∼cs152/
    [sp20/handouts/sp20/riscv-v-spec.pdf".]("https://inst.eecs.berkeley.edu/~cs152/sp20/handouts/sp20/riscv-v-spec.pdf")

    - <span id="page-8-43"></span>[44] *Intel Advanced Performance Extensions (Intel
    APX) Architecture Specification*, ["https://www.intel.com/content/www/us/en/developer/articles/]("https://www.intel.com/content/www/us/en/developer/articles/technical/intel-sdm.html"
    ) [technical/intel-sdm.html".]("https://www.intel.com/content/www/us/en/developer/articles/technical/intel-sdm.html"
    )

    - <span id="page-8-44"></span>[45] *Intel Advanced Vector Extensions 10 (Intel
    AVX10) Architecture Specification*, ["https://www.intel.com/content/www/us/en/developer/articles/]("https://www.intel.com/content/www/us/en/developer/articles/technical/intel-sdm.html"
    ) [technical/intel-sdm.html".]("https://www.intel.com/content/www/us/en/developer/articles/technical/intel-sdm.html"
    )

    - <span id="page-8-45"></span>[46] *NVIDIA Deep Learning Accelerator (NVDLA),
    Hardware Architectural Specification*, ["http://nvdla.org/hw/v1/hwarch.html".]("http://nvdla.org/hw/v1/hwarch.html")

    - <span id="page-8-46"></span>[47] *TileLink Specification*, ["https://www.sifive.com/document-file/]("https://www.sifive.com/document-file/tilelink-spec-1.9.3")
    [tilelink-spec-1.9.3".]("https://www.sifive.com/document-file/tilelink-spec-1.9.3")

    - <span id="page-8-47"></span>[48] *AMBA5 CHI Architecture Specification*, ["https://developer.arm.com/]("https://developer.arm.com/documentation/ihi0050/latest/")
    [documentation/ihi0050/latest/".]("https://developer.arm.com/documentation/ihi0050/latest/")

    - <span id="page-8-48"></span>[49] *AMBA5 ACE Protocol Specification (superseded
    by CHI)*, ["https://]("https://developer.arm.com/documentation/ihi0022/hc" ) [developer.arm.com/documentation/ihi0022/hc".]("https://developer.arm.com/documentation/ihi0022/hc"
    )

    - <span id="page-8-49"></span>[50] *AMBA5 AXI Protocol Specification*, ["https://developer.arm.com/]("https://developer.arm.com/documentation/ihi0022/latest")
    [documentation/ihi0022/latest".]("https://developer.arm.com/documentation/ihi0022/latest")

    - <span id="page-8-50"></span>[51] *AMBA4 AXI and ACE Protocol Specification*,
    ["https://developer.arm.]("https://developer.arm.com/documentation/ihi0022/e"
    ) [com/documentation/ihi0022/e".]("https://developer.arm.com/documentation/ihi0022/e"
    )

    - <span id="page-8-51"></span>[52] *10GE MAC Core Specification*, ["https://opencores.org/ocsvn/xge]("https://opencores.org/ocsvn/xge_mac/xge_mac/trunk/doc/xge_mac_spec.pdf"
    ) mac/ xge [mac/trunk/doc/xge]("https://opencores.org/ocsvn/xge_mac/xge_mac/trunk/doc/xge_mac_spec.pdf"
    ) mac spec.pdf".

    - <span id="page-8-52"></span>[53] *Ethernet IP Core Specification*, ["http://www.cprover.org/firmware/doc/]("http://www.cprover.org/firmware/doc/ethoc/eth_speci.pdf"
    ) ethoc/eth [speci.pdf".]("http://www.cprover.org/firmware/doc/ethoc/eth_speci.pdf"
    )

    - <span id="page-8-53"></span>[54] *I2C-Master Core Specification*, ["http://www.cprover.org/firmware/doc/]("http://www.cprover.org/firmware/doc/ethoc/eth_speci.pdf"
    ) ethoc/eth [speci.pdf".]("http://www.cprover.org/firmware/doc/ethoc/eth_speci.pdf"
    )

    - <span id="page-8-54"></span>[55] *UART to Bus Core Specifications*, ["https://opencores.org/usercontent/]("https://opencores.org/usercontent/doc/1330166592"
    ) [doc/1330166592".]("https://opencores.org/usercontent/doc/1330166592" )

    - <span id="page-8-55"></span>[56] *Elliptic Curve Group Core Specification*,
    ["https://opencores.org/projects/]("https://opencores.org/projects/ecg" ) [ecg".]("https://opencores.org/projects/ecg"
    )

    - <span id="page-8-56"></span>[57] *Tate Bilinear Pairing Core Specification*,
    ["https://opencores.org/projects/]("https://opencores.org/projects/pairing" )
    [pairing".]("https://opencores.org/projects/pairing" )

    - <span id="page-8-57"></span>[58] *Tiny Tate Bilinear Pairing Core Specification*,
    ["https://opencores.org/]("https://opencores.org/projects/tiny_tate_bilinear_pairing"
    ) [projects/tiny]("https://opencores.org/projects/tiny_tate_bilinear_pairing"
    ) tate bilinear pairing".

    - <span id="page-8-58"></span>[59] *AES Core Specification*, ["https://opencores.org/usercontent/doc/]("https://opencores.org/usercontent/doc/1354351714"
    ) [1354351714".]("https://opencores.org/usercontent/doc/1354351714" )

    - <span id="page-8-59"></span>[60] *SHA3 Core Specification*, ["https://opencores.org/usercontent/doc/]("https://opencores.org/usercontent/doc/1359445372"
    ) [1359445372".]("https://opencores.org/usercontent/doc/1359445372" )

    - <span id="page-8-13"></span>[61] *RISC-V Specifications*, ["https://riscv.org/technical/specifications/".]("https://riscv.org/technical/specifications/"
    )

    - <span id="page-8-60"></span>[62] C. Celio, D. A. Patterson, and K. Asanovic,
    "The berkeley out-of-order machine (boom): An industry-competitive, synthesizable,
    parameterized risc-v processor," *EECS Department, University of California, Berkeley,
    Tech. Rep. UCB/EECS-2015-167*, 2015.

    - <span id="page-8-61"></span>[63] Y. Xu, Z. Yu, D. Tang, G. Chen, L. Chen, L.
    Gou, Y. Jin, Q. Li, X. Li, Z. Li *et al.*, "Towards developing high performance
    risc-v processors using agile methodology," in *MICRO*, 2022.

    - <span id="page-8-62"></span>[64] J. White, Q. Fu, S. Hays, M. Sandborn, C. Olea,
    H. Gilbert, A. Elnashar, J. Spencer-Smith, and D. C. Schmidt, "A prompt pattern
    catalog to enhance prompt engineering with chatgpt," *arXiv preprint arXiv:2302.11382*,
    2023.

    - <span id="page-8-63"></span>[65] *verified calendar.v in RTLLM*, ["https://github.com/hkust-zhiyao/]("https://github.com/hkust-zhiyao/RTLLM/blob/main/calendar/verified_calendar.v"
    ) [RTLLM/blob/main/calendar/verified]("https://github.com/hkust-zhiyao/RTLLM/blob/main/calendar/verified_calendar.v"
    ) calendar.v".

    - <span id="page-8-64"></span>[66] *Amber 2 Core Specification*, ["https://opencores.org/websvn/filedetails?]("https://opencores.org/websvn/filedetails?repname=amber&path=%2Famber%2Ftrunk%2Fdoc%2Famber-core.pdf"
    ) [repname=amber&path=%2Famber%2Ftrunk%2Fdoc%2Famber-core.]("https://opencores.org/websvn/filedetails?repname=amber&path=%2Famber%2Ftrunk%2Fdoc%2Famber-core.pdf"
    ) [pdf".]("https://opencores.org/websvn/filedetails?repname=amber&path=%2Famber%2Ftrunk%2Fdoc%2Famber-core.pdf"
    )

    - <span id="page-8-65"></span>[67] Y. Chang, X. Wang, J. Wang, Y. Wu, K. Zhu,
    H. Chen, L. Yang, X. Yi, C. Wang, Y. Wang *et al.*, "A survey on evaluation of
    large language models," *arXiv preprint arXiv:2307.03109*, 2023.'
  decisions:
    evaluation_prompt: 'Qualified. Reason: The paper includes multiple sections discussing
      experiments and evaluations, such as "We evaluate our method on…" and "Our experimental
      results indicate…". It also references specific experiments and results, demonstrating
      empirical evaluation.'
    related_work_prompt: 'Qualified. Reason: The paper meaningfully engages with prior
      research by citing a range of studies and references throughout the text, particularly
      in the introduction and dataset overview sections. It discusses the application
      of LLMs in chip design and compares its methods to previous work, indicating
      a thorough engagement with existing literature.'
    novelty_prompt: 'Qualified. Reason: The paper demonstrates novelty by proposing
      a new framework for utilizing large language models (LLMs) in generating and
      reviewing architecture specifications. It introduces structured definitions
      of architecture specifications, creates a dataset, and explores novel applications
      of LLMs in this context. The paper also claims to be the first to focus on this
      topic, as stated in "To the best of our knowledge, there has been no prior design
      automation or LLM research focusing on this important topic.'
    review_only_prompt: '- Disqualified: review paper. Reason: The main body of the
      paper primarily summarizes existing work and explores the use of LLMs in generating
      and reviewing architecture specifications without introducing new contributions.'
- title: "The Landscape of Compute-near-memory and Compute-in-memory: A Research\n\
    \  and Commercial Overview"
  abstract: 'In today''s data-centric world, where data fuels numerous application
    domains,

    with machine learning at the forefront, handling the enormous volume of data

    efficiently in terms of time and energy presents a formidable challenge.

    Conventional computing systems and accelerators are continually being pushed to

    their limits to stay competitive. In this context, computing near-memory (CNM)

    and computing-in-memory (CIM) have emerged as potentially game-changing

    paradigms. This survey introduces the basics of CNM and CIM architectures,

    including their underlying technologies and working principles. We focus

    particularly on CIM and CNM architectures that have either been prototyped or

    commercialized. While surveying the evolving CIM and CNM landscape in academia

    and industry, we discuss the potential benefits in terms of performance,

    energy, and cost, along with the challenges associated with these cutting-edge

    computing paradigms.'
  url: http://arxiv.org/abs/2401.14428v1
  keywords: ''
  document: '# **The Landscape of Compute-near-memory and Compute-in-memory: A Research
    and Commercial Overview**


    [ASIF ALI KHAN,](HTTPS://ORCID.ORG/0000-0002-5130-9855) TU Dresden, Germany


    [JOÃO PAULO C. DE LIMA,](HTTPS://ORCID.ORG/0000-0001-9295-3519) TU Dresden and
    ScaDS.AI, Germany [HAMID FARZANEH,](HTTPS://ORCID.ORG/0000-0002-1780-6217) TU
    Dresden, Germany [JERONIMO CASTRILLON,](HTTPS://ORCID.ORG/0000-0002-5007-445X)
    TU Dresden and ScaDS.AI, Germany


    In today''s data-centric world, where data fuels numerous application domains,
    with machine learning at the forefront, handling the enormous volume of data efficiently
    in terms of time and energy presents a formidable challenge. Conventional computing
    systems and accelerators are continually being pushed to their limits to stay
    competitive. In this context, computing near-memory (CNM) and computing-in-memory
    (CIM) have emerged as potentially game-changing paradigms. This survey introduces
    the basics of CNM and CIM architectures, including their underlying technologies
    and working principles. We focus particularly on CIM and CNM architectures that
    have either been prototyped or commercialized. While surveying the evolving CIM
    and CNM landscape in academia and industry, we discuss the potential benefits
    in terms of performance, energy, and cost, along with the challenges associated
    with these cutting-edge computing paradigms.


    # **1 INTRODUCTION**


    In conventional computing systems, the processor and memory are two independent
    entities connected via communication pathways, known as buses. When the CPU processes
    data, it requires fetching it from memory via the bus, conducting the necessary
    computations, and subsequently storing the results back in memory. This off-chip
    communication becomes a limiting factor for data-intensive workloads due to the
    limited transfer rate and high energy per bit of buses. For example, the data
    transfer between the logic (CPUs and GPUs) and memory chips (DRAM or flash memory)
    requires approximately 10–100 times more energy than the logic operation itself
    [\[1\]](#page-33-0). *Compute-near-memory* (CNM) and *compute-in-memory* [\(CIM\)](#page-39-0)
    concepts address this bottleneck by enabling computations close to where the data
    resides. This is achieved either by implementing CMOS logic on or closer to the
    memory chip, or by leveraging the inherent physical properties of memory devices
    to perform computations in place.


    <span id="page-0-1"></span><span id="page-0-0"></span>The core concept behind
    [CNM](#page-39-1)[/CIM](#page-39-0) is not entirely new. However, the sudden surge
    in these systems can be attributed to two primary factors, namely, the exponential
    increase in the volume of data required for modern applications, and the technological
    readiness. Recent advancements in machine learning, particularly the emergence
    of generative AI and *large language models* [\(LLM\)](#page-41-0), demand the
    processing of terabytes of data, substantial computational resources, and complex
    execution, thus highlighting the limitations of traditional computing systems.
    A recent study revealed that OpenAI utilized over 3600 of NVIDIA''s HGX A100 servers,
    totaling around 29,000 GPUs, to train ChatGPT, resulting in a daily energy consumption
    of 564 MWh [\[2\]](#page-34-0). Projections indicate that by 2027, AI is expected
    to consume between 85 and 124 TWh annually, equivalent to approximately 0.5% of
    the world''s total electricity consumption. It is no surprise that Microsoft has
    announced plans to develop its own nuclear reactors to power their data centers
    [\[3\]](#page-34-1).


    Currently, machine learning applications primarily leverage GPU accelerators like
    A100, H100, GH200, application-specific integrated circuits (e.g., Google''s TPU),
    and dataflow processors as in the case of companies like GraphCore, Cerebras,
    Groq, and SambaNova [\[4\]](#page-34-2). Over the past few


    Authors'' addresses: [Asif Ali Khan,](https://orcid.org/0000-0002-5130-9855) TU
    Dresden, Dresden, Germany, asif\_ali.khan@tu-dresden.de; [João Paulo C. de Lima,](https://orcid.org/0000-0001-9295-3519)
    TU Dresden and ScaDS.AI, Dresden, Germany, joao.lima@tu-dresden.de; [Hamid Farzaneh,](https://orcid.org/0000-0002-1780-6217)
    TU Dresden, Dresden, Germany, hamid.farzaneh@tu-dresden.de; [Jeronimo Castrillon,](https://orcid.org/0000-0002-5007-445X)
    TU Dresden and ScaDS.AI, Dresden, Germany, jeronimo.castrillon@tudresden.de.


    years, [CNM](#page-39-1)[/CIM](#page-39-0) systems have also transcended their
    prototypical stages and successfully entered the market. The timing of these advancements
    in CIM/CNM systems is of paramount importance as it perfectly aligns with the
    AI revolution. As a result, numerous companies have emerged in the last few years
    offering CIM/CNM solutions for various use domains. This surge reflects a competitive
    landscape where these companies are striving to leverage the demand and cater
    to various market segments. All commercially available solutions hold the promise
    of significantly reducing execution time and energy consumption for data-intensive
    workloads.


    This survey explores CNM and CIM architectures, detailing their technologies,
    fundamental concepts, working principles, and the evolving landscape in academia
    and industry. Addressing a broader audience, it provides foundational concepts
    for non-experts while delivering state-of-the-art insights for experts in the
    domain. It also summarizes the impact and challenges associated with adopting
    the novel CIM/CNM computing paradigms. Concretely, our discussion revolves around
    three key aspects:


    - (1) **Key technologies and concepts:** In CNM systems, a specialized CMOS logic
    is integrated into the memory chip. This logic can be either general-purpose,
    as in UPMEM systems [\[5\]](#page-34-3), or domain-specific, as in systems developed
    by Samsung [\[6,](#page-34-4) [7\]](#page-34-5) and SK Hynix [\[8,](#page-34-6)
    [9\]](#page-34-7), integrated within DRAM memory chips. While CNM significantly
    reduces data movement, it does not eliminate it. In contrast, CIM nearly eliminates
    data movement by performing computations within the same devices that store the
    data. A particularly noteworthy operation is the analog dot-product in memory,
    which is of significant importance to the machine learning domain and can be performed
    in constant time. Initially demonstrated in crossbar-configured resistive *non-volatile
    memory* [\(NVM\)](#page-40-0) technologies like *phase change memory* [\(PCM\)](#page-40-1)
    [\[10\]](#page-34-8) and *resistive RAM* [\(RRAM\)](#page-40-2) [\[11\]](#page-34-9),
    this concept has also been shown with SRAM, *magnetic RAM* [\(MRAM\)](#page-40-3)
    [\[12\]](#page-34-10), and *ferroelectric field-effect transistor* [\(FeFET\)](#page-40-4)
    [\[13\]](#page-34-11). While other arithmetic, search and boolean logic operations
    have also been demonstrated using CIM, they have received comparatively less attention.

    - <span id="page-1-5"></span><span id="page-1-4"></span><span id="page-1-3"></span><span
    id="page-1-2"></span><span id="page-1-1"></span><span id="page-1-0"></span>(2)
    **Commercial trends:** As the demand for fast and efficient computing systems
    continues to rise, the in-/near-memory computing market is experiencing rapid
    expansion. In 2022, this market was valued at USD 15.5 billion, with an anticipated
    *compound annual growth rate* [\(CAGR\)](#page-41-1) of 17.5% over the next decade
    [\[14\]](#page-34-12). This growth is underscored by the proliferation of startups
    offering CIM and CNM solutions. Some of these companies have secured hundreds
    of millions of dollars in early funding rounds. While many of these companies
    provide innovative solutions for data-intensive applications (dominated by AI
    inference), there is no clear winner yet. At present, these solutions are predominantly
    based on SRAM technology, although solutions based on resistive NVM and flash
    technologies also exist [\[15\]](#page-34-13). This trend can be attributed to
    the mature tools and design processes for SRAM compared to emerging NVMs. However,
    considering the SRAM''s scalability aspects and its static power consumption,
    it is likely that NVMs, particularly [PCM,](#page-40-1) [RRAM,](#page-40-2) [MRAM,](#page-40-3)
    and [FeFET,](#page-40-4) will progressively replace or complement SRAM as these
    technologies mature.

    - (3) **Challenges:** Although CIM/CNM systems are at the tipping point, they
    are yet to make substantial inroads into the market. The predominant obstacle
    facing these systems is perhaps the absence of a software ecosystem, which renders
    programmability and optimization exceedingly challenging. This is also highlighted
    by a recent Meta article [\[16\]](#page-34-14) stating, *We''ve investigated applying
    processing-in-memory (PIM) to our workloads and determined there are several challenges
    to using these approaches. Perhaps the biggest challenge of PIM is its programmability*.
    Other challenges requiring attention include: addressing reliability concerns
    associated with emerging NVMs (particularly in the CIM context), developing


    novel performance models, profiling and analysis tools for these systems, which
    could be leveraged to exploit their potential effectively.


    The remainder of this paper is structured as follows: Section [2](#page-2-0) explains
    the terminology associated with these domains and provides insights into the conventional
    Von-Neumann computing approach, as well as the emerging data-centric paradigms.
    In Section [3,](#page-4-0) a comprehensive overview of promising memory technologies
    within the context of [CIM](#page-39-0) and [CNM](#page-39-1) systems is provided.
    Section [4](#page-9-0) outlines various common [CIM](#page-39-0) and [CNM](#page-39-1)
    systems including very recent prototype chips from various industries. Lastly,
    Section [5](#page-25-0) presents a comprehensive overview of the commercial landscape
    for these systems (start-ups), discussing their products details, target application
    domain, and funding status. Finally, Section [6](#page-32-0) concludes the paper
    by summarizing our key observations and providing insights and recommendations
    into the future.


    # <span id="page-2-0"></span>**2 TERMINOLOGY AND BACKGROUND**


    This section highlights the bottleneck in the Von Neumann computing model by discussing
    its working mechanism, motivates the need for memory-centric computing, and explains
    the terminology.


    #### <span id="page-2-2"></span>**2.1 Mainstream Von-Neumann Computing**


    As depicted in Figure [1a](#page-2-1), the interaction between memory and the
    processor in the Von Neumann architecture is facilitated through address and data
    buses. However, because CPU performance significantly outpaces memory performance,
    the Von Neumann models are often bottlenecked by the memory. To address this challenge
    and mitigate the impact of larger memory access latencies on the CPU, modern processors
    incorporate a tiered hierarchy of caches. Caches are smaller memory units that,
    while being much smaller compared to main memory and storage, are notably faster.
    The first-level cache (L1) is typically integrated onto the CPU chip and operates
    nearly at CPU speed, enabling single-cycle access. L2 cache is usually shared
    by multiple cores and can vary in location depending on the design goals. Some
    systems even include an L3 cache, usually larger and situated off-chip.


    <span id="page-2-1"></span>![](_page_2_Figure_7.jpeg)


    Fig. 1. (a) Conventional computing system where an operation is performed on data
    in the CPU (b) Memory-centric design where is computed in the proximity of and
    CPU is mainly working as a control unit [\[17\]](#page-34-15).


    <span id="page-3-8"></span>Note that Von Neumann architectures are characterized
    by the sequential execution of instructions. However, in multi-core CPU systems,
    parallel execution of instructions at various levels of granularity, including
    instruction-level, data-level, and thread-level parallelisms, is supported. To
    enhance performance and energy efficiency in resource-constrained systems, specialized
    accelerators are often developed and integrated on the same chip. For instance,
    in application domains such as embedded systems, digital signal processing, and
    networking, *multiprocessor system-on-chip* [\(MPSoC\)](#page-40-5) architectures
    are employed. These integrate multiple processor cores, memory, input/output interfaces,
    and potentially specialized hardware accelerators, enabling parallel execution
    of different tasks to meet specific constraints.


    Although these designs may significantly enhance performance when compared to
    conventional CPU-only systems, the underlying design principle remains CPU-centric
    and follows the Von Neumann model of execution. Consequently, the performance
    improvements, largely resulting from concurrent execution, heavily rely on the
    nature of the application. In cases where an application is memory-bound, i.e.,
    most of the execution time is spent on the memory accesses and not on the actual
    compute operations, the shared data bus is fully occupied and becomes a bottleneck.
    Even for compute-bound applications, where these architectures can yield substantial
    gains in execution time, power consumption remains largely unaffected and might
    even increase due to the complex structure of these systems.


    #### **2.2 Memory-centric computing**


    Unlike conventional computing systems where CPU has a central role and is responsible
    for all computations, most computations in the memory-centric designs are performed
    within or near memory. As depicted in Figure [1b](#page-2-1), the core concept
    revolves around minimizing data transfer on the bus by relocating a substantial
    share of computations closer to the data (memory). The CPU''s primary role becomes
    issuing commands and handling computations that cannot be effectively executed
    in close proximity to the memory.


    The concept of memory-centric computing is not a novel one, but it has experienced
    a significant surge in recent years. Consequently, various terms have emerged,
    often referring to the same idea, and more detailed classifications have been
    introduced in architectural designs. This section aims to clarify the terminology
    surrounding these approaches.


    <span id="page-3-7"></span><span id="page-3-5"></span>*2.2.1 Compute-in-memory.*
    Computing systems can be broadly divided into two categories: *compute-in-memory*
    [\(CIM\)](#page-39-0) systems and *compute-outside-memory* [\(COM\)](#page-39-2)
    systems (see Section [2.1\)](#page-2-2). In the literature, there are different
    names for similar things. These architectures are often named based on (1) the
    *location* of the compute units within the memory hierarchy (near cache or near
    main memory), or (2) based on the underlying paradigm, i.e., whether the memory
    device itself is used to implement computation [\(CIM\)](#page-39-0), or whether
    extra CMOS-logic is added near the memory to perform computations (*compute-near-memory*
    [\(CNM\)](#page-39-1)). Figure [2](#page-4-1) shows an overview of different processor
    and memory system configurations. A compute operation can be a logic operation
    or an arithmetic operation such as addition and multiplication. [CIM](#page-39-0)
    systems are also frequently referred to as *in-memory-computing* [\(IMC\)](#page-39-3),
    *in-memory-processing* [\(IMP\)](#page-39-4), *processing-in-memory* [\(PIM\)](#page-39-5),
    *processing-using-memory* [\(PUM\)](#page-39-6) or *logic-in-memory* [\(LIM\)](#page-39-7)
    systems [\[18\]](#page-34-16). For the purposes of this report, we will use the
    term [CIM.](#page-39-0)


    <span id="page-3-6"></span><span id="page-3-4"></span><span id="page-3-3"></span><span
    id="page-3-2"></span><span id="page-3-1"></span><span id="page-3-0"></span>*2.2.2
    Compute-near-memory.* When operations are computed outside the memory [\(COM\)](#page-39-2)
    using conventional computing cores (Fig [2.](#page-4-1)a), the architecture is
    a conventional Von Neumann system (see Section [2.1\)](#page-2-2). On the other
    hand, if the computations are performed outside the memory but with a dedicated
    logic unit connected to the memory module via a high-bandwidth channel (Fig [2.](#page-4-1)b),
    the system is referred to as a *compute-near-memory* [\(CNM\)](#page-39-1) or
    *near-memory-computing* [\(NMC\)](#page-39-8) or


    <span id="page-4-3"></span><span id="page-4-2"></span>*near-memory-processing*
    [\(NMP\)](#page-39-9), or *processing-near-memory* [\(PNM\)](#page-39-10) system.
    In this report, we will restrict ourselves to calling it [CNM.](#page-39-1)


    <span id="page-4-5"></span>In the [CIM](#page-39-0) category, computations can
    be carried out using memory cells within the memory array, known as CIM-array
    [\(CIM-A\)](#page-39-11) (see Fig [2.](#page-4-1)d). Alternatively, computations
    can occur in the memory peripheral circuitry, termed CIM-peripheral [\(CIM-P\)](#page-39-12)
    (see Fig [2.](#page-4-1)c).


    <span id="page-4-6"></span><span id="page-4-1"></span>![](_page_4_Figure_3.jpeg)


    Fig. 2. High-level overview of systems where computation is performed a) COM (mainstream
    computing): outside of memory system, b) [CNM:](#page-39-1) using a logic connected
    to the memory via the memory high-bandwidth channel, c) [CIM-P:](#page-39-12)
    in the memory peripheral circuitry, and d) [CIM-A:](#page-39-11) using memory
    cells within the memory array.


    In [CIM-A,](#page-39-11) memory cells are often modified to support logic design,
    e.g., in [\[19\]](#page-34-17). Sometimes, it also necessitates changes to the
    periphery to support the modified cells. Therefore, some literature further divides
    the [CIM-A](#page-39-11) designs into basic [CIM-A](#page-39-11) that do not require
    any modifications to the periphery, e.g., [\[20\]](#page-34-18), and hybrid [CIM-A](#page-39-11)
    that requires support from the peripheral circuit. A well-known example of a hybrid
    [CIM-A](#page-39-11) is the MAGIC design [\[19\]](#page-34-17) that requires extending
    the peripheral circuit to write multiple memory rows simultaneously.


    <span id="page-4-10"></span><span id="page-4-7"></span><span id="page-4-4"></span>Typical
    examples of [CIM-P](#page-39-12) architectures are crossbars employing *analog-to-digital
    converter*s [\(ADCs](#page-39-13)) and digital-to-analog converters [\(DACs](#page-39-14))
    to implement *matrix-vector multiplication* [\(MVM\)](#page-41-2) and other logic
    operations [\[21,](#page-34-19) [22\]](#page-34-20). Additionally, [CIM-P](#page-39-12)
    designs employing customized sense amplifiers also exist [\[23\]](#page-34-21).
    Similar to [CIM-A,](#page-39-11) [CIM-P](#page-39-12) can be either basic, as
    in Pinatubo [\[23\]](#page-34-21), requiring no changes to the memory array, or
    hybrid, as seen in ISAAC [\[22\]](#page-34-20). A summary of the terminology''s
    classification is presented in Figure [3.](#page-5-0)


    Both [CIM-A](#page-39-11) and [CIM-P](#page-39-12) can also be used together,
    wherein the memory array calculates partial results that are later post-processed
    or accumulated in the peripheral circuit. In such cases, it is referred to as
    a [CIM](#page-39-0) architecture.


    #### <span id="page-4-0"></span>**3 TECHNOLOGY OVERVIEW**


    <span id="page-4-9"></span><span id="page-4-8"></span>In this section, we present
    an overview of the main memory cells used in various [CNM](#page-39-1) and [CIM](#page-39-0)
    systems, encompassing both volatile *static random-access memory* [\(SRAM\)](#page-40-6),
    *dynamic random-access memory* [\(DRAM\)](#page-39-15)) and non-volatile types
    such as [PCM,](#page-40-1) [MRAM,](#page-40-3) [RRAM](#page-40-2) and [FeFETs](#page-40-4).
    These memory technologies are versatile enough to serve as main memory for data
    storage and support [CNM](#page-39-1) without requiring any modification to the
    memory chips. Additionally, we explore how these memory cells can be leveraged
    for in-memory computation, considering technological aspects such as performance,
    energy consumption, lifetime, CMOS compatibility, and other relevant factors.
    Before going into the individual technologies, let us first explain the different
    components of the memory subsystem.


    <span id="page-5-0"></span>![](_page_5_Figure_1.jpeg)


    Fig. 3. [CIM](#page-39-0) and [CNM](#page-39-1) classification.


    #### **3.1 Memory subsystem**


    The memory subsystem typically consists of a four-level hierarchy, each with its
    own characteristics and access time. The fastest and smallest level is the CPU
    registers, while the slowest one in the hierarchy is storage devices (HDD/SSD).
    The [CNM/](#page-39-1)[CIM](#page-39-0) concepts have been proposed at different
    levels in the memory hierarchy, e.g., in-cache computing [\[24\]](#page-34-22),
    in-DRAM [\[25\]](#page-35-0), in-storage computing [\[26\]](#page-35-1). However,
    [CNM](#page-39-1)[/CIM](#page-39-0) at the main memory level have rightly gained
    more attention than others.


    <span id="page-5-1"></span>![](_page_5_Figure_5.jpeg)


    Fig. 4. Typical [DRAM](#page-39-15) system organization.


    The main memory is also typically organized hierarchically, as shown in Figure
    [4.](#page-5-1) The main components of the hierarchy are memory cells, subarrays,
    banks, and ranks. A cell is the fundamental working unit and serves as a building
    block to form arrays. An array is a 2D grid of cells connected via *word lines*
    and *bitlines*. A word line is a horizontal line that connects all the memory
    cells in a row, while bitlines, on the other hand, connect all cells in a column.
    Memory cells are placed at the intersection of word lines and bitlines. The combination
    of a specific word line and a bit line uniquely identifies each memory cell. When
    a particular memory cell needs to be accessed, the corresponding word line and
    bit line are activated, allowing data to be read from or written into that cell.


    #### **3.2 [DRAM](#page-39-15)**


    [DRAM](#page-39-15) is the most mature and widely used memory technology today.
    A [DRAM](#page-39-15) cell is composed of a transistor and a capacitor. When the
    capacitor is fully charged, it represents the logical value 1, while a discharged
    capacitor represents the logical value 0. To access data from the [DRAM](#page-39-15)
    array, the memory controller brings a particular row into the row buffer by sending
    an *activate* command. A *read* command is then issued to read specific column(s)
    from the row buffer and put them on the bus.


    [DRAM](#page-39-15) has scaled nicely for decades and has been used across application
    domains and systems ranging from HPC to portable devices. However, it is presently
    facing several challenges to remain the dominant technology. The increasing demand
    for higher capacity has put tremendous pressure on the [DRAM](#page-39-15) capacitor
    size to shrink which makes it susceptible to errors. Also, the increase in capacity
    is significantly increasing the refresh power budget.


    <span id="page-6-4"></span><span id="page-6-3"></span>To address the escalating
    demands for higher bandwidth in modern applications, 3D stacked [DRAM](#page-39-15)
    architectures, such as *high bandwidth memory* [\(HBM\)](#page-40-7) (see Section
    [4.1.3\)](#page-12-0), have been proposed. These architectures consist of stacked
    [DRAM](#page-39-15) dies atop a logic layer, interconnected through *through-silicon
    vias* [\(TSVs\)](#page-41-3), resulting in remarkably higher bandwidth. These
    structures are also employed in a series of [CNM](#page-39-1) solutions, where
    the logic layer is used to implement custom logic and perform computations in
    closer proximity to the data [\[27,](#page-35-2) [28\]](#page-35-3).


    From the [CIM](#page-39-0) perspective, the majority of in[-DRAM](#page-39-15)
    implementations rely on charge sharing, wherein multiple rows are activated simultaneously.
    The shared charge is then utilized in a controlled manner to carry out various
    logic and data copy operations [\[25\]](#page-35-0). Moreover, cleverly manipulating
    the memory timing parameters, deviating from standard timings, has also been employed
    to implement different logic operations [\[29\]](#page-35-4).


    #### **3.3 SRAM**


    SRAM is another mature memory technology that provides fast and efficient memory
    accesses. It is commonly used in caches, register files, and other high-speed
    memory applications where speed and low access latency are critical. An [SRAM](#page-40-6)
    cell consists of multiple transistors arranged in a specific configuration to
    hold one bit of data. The most common configuration of an [SRAM](#page-40-6) cell
    is a pair of cross-coupled inverters that are connected in a feedback loop, forming
    a latch.


    <span id="page-6-0"></span>![](_page_6_Figure_8.jpeg)


    (a) A sample 6T [SRAM](#page-40-6) cell [\[30\]](#page-35-5) (b) Metal-oxide RRAM
    cell [\[31\]](#page-35-6) (c) [FeFET](#page-40-4) device [\[32\]](#page-35-7)


    <span id="page-6-5"></span><span id="page-6-2"></span><span id="page-6-1"></span>Fig.
    5. Cell structures of various memory technologies


    To read and store data on the cell, the bitline terminals, *bitline* [\(BL\)](#page-39-16)
    and *bitline bar* [\(BLB\)](#page-39-17) (see Fig. [5a\)](#page-6-0), are precharged
    and discharged, and the wordline(*word line* [\(WL\)](#page-41-4)) is activated
    or deactivated depending on the values reading/writing from/to the cell.


    There have been proposals for in[-SRAM](#page-40-6) computing, especially at the
    last-level cache, which can be considerably slower compared to the L1 cache (e.g.,
    by an order of magnitude). Similar to [DRAM,](#page-39-15) most in[-SRAM](#page-40-6)
    computing architectures also leverage charge sharing in the bitlines. Specifically,
    precharging the bitlines in a controlled manner and activating multiple rows simultaneously
    enables


    performing logic operations [\[24\]](#page-34-22). For bitwise multiplication
    in [SRAM,](#page-40-6) research has demonstrated that the amplitude of the input
    voltage at the [WL](#page-41-4) directly influences the discharge rate of the
    [BLB.](#page-39-17) The voltage discharge on [BLB,](#page-39-17) achieved within
    a specific timeframe, effectively represents a one-bit multiplication of the data
    stored in the [SRAM](#page-40-6) cell.


    #### **3.4 Phase change memory [\(PCM\)](#page-40-1)**


    [PCM](#page-40-1) is resistive memory technology that employs reversible phase
    changes in materials to store data. The earliest demonstration of a 256-bit [PCM](#page-40-1)
    prototype dates back to 1970 [\[33\]](#page-35-8). Today, [PCM](#page-40-1) stands
    as one of the most extensively researched [NVM](#page-40-0) technologies. A [PCM](#page-40-1)
    device comprises a phase-changing material sandwiched between two electrodes (very
    similar to Fig. [5b\)](#page-6-0), which transitions between crystalline (low
    resistance state) and amorphous (high resistance state) phases. These two resistance
    states represent binary logic states, i.e., 1 and 0.


    Typically, [PCM](#page-40-1) requires a relatively high programming current (>200),
    but this can be mitigated to less than 10 by scaling down the device size [\[10,](#page-34-8)
    [34\]](#page-35-9). As [PCM](#page-40-1) stores data based on resistance, it can
    be programmed to encompass more than two resistance states, allowing for multi-level
    cells to represent more than a single bit of information. Nevertheless, relying
    on multiple resistance states for prolonged periods poses challenges, as the device
    resistance tends to drift over time, making it difficult to discern between resistance
    states.


    #### **3.5 Resistive RAM [\(RRAM\)](#page-40-2)**


    [RRAM](#page-40-2) is another class of resistive memory technologies that utilizes
    the resistive switching phenomenon in metal oxide materials to store data [\[11\]](#page-34-9).
    As shown in Fig. [5b,](#page-6-0) a typical [RRAM](#page-40-2) cell comprises
    a top and a bottom electrode with a thin oxide layer sandwiched in between. To
    achieve resistive switching, a high electric field is applied to the [RRAM](#page-40-2)
    cell, leading to the creation of oxygen vacancies within the metal oxide layer.
    This process results in the formation of conductive filaments, causing the device
    state to transition from a high resistance to a low resistance (set) state. To
    revert to the high resistance (reset) state, the device is subjected to , which
    breaks the conductive filament, allowing the oxygen ions to migrate back to the
    bulk. Compared to [PCM,](#page-40-1) [RRAM](#page-40-2) exhibits several advantages,
    including higher write endurance (>1010), faster write operations, larger resistance
    on-off ratios, and improved scalability prospects [\[11\]](#page-34-9). However,
    ReRAM does suffer from inconsistent electrical characteristics, meaning it exhibits
    larger variations in resistance across different devices [\[10\]](#page-34-8).


    #### <span id="page-7-0"></span>**3.6 Magnetic RAM (MRAM)**


    <span id="page-7-3"></span><span id="page-7-2"></span><span id="page-7-1"></span>[MRAM](#page-40-3)
    store data in nano-scale ferromagnetic elements via magnetic orientation [\[35\]](#page-35-10).
    An [MRAM](#page-40-3) cell is a *magnetic tunnel junction* [\(MTJ\)](#page-40-8)
    device composed of two ferromagnetic layers, namely a fixed reference layer and
    a free layer, separated by an insulating layer. The free layer holds the data
    bit, and reading it involves passing an electric current and measuring its resistance.
    For data writing into an [MRAM](#page-40-3) cell, various techniques can be used.
    The most common method is the *spin-transfer-torque* [\(STT\)](#page-40-9), which
    utilizes spin-polarized electric current to change the free layer''s magnetic
    orientation. *Spin-orbit-torque* [\(SOT\)](#page-40-10)[-MRAM,](#page-40-3) on
    the other hand, uses an in-plane current through the heavy metal layer to generate
    a spin current that exerts a torque on the magnetization of the free layer.The
    relative orientations of the free and fixed layers result in different resistance
    states. [MRAM](#page-40-3) exhibits virtually unlimited endurance and acceptable
    access latency. However, it is faced with challenges such as a larger cell size
    and a smaller on/off resistance ratio, limiting an [MRAM](#page-40-3) cell to
    store only one bit of data [\[36\]](#page-35-11).


    CNM/CIM Landscape 9


    <span id="page-8-0"></span>


    | Device                 | SRAM      | DRAM        | RRAM        | PCM         |
    STT-MRAM | FeFET  |

    |------------------------|-----------|-------------|-------------|-------------|----------|--------|

    | Write time             | 1 − 10𝑛𝑠  | > 20𝑛𝑠      | > 10𝑛𝑠      | ∼ 50𝑛𝑠      |
    > 10𝑛𝑠   | ∼ 10𝑛𝑠 |

    | Read time              | 1 − 10𝑛𝑠  | > 20𝑛𝑠      | > 10𝑛𝑠      | > 10𝑛𝑠      |
    > 10𝑛𝑠   | ∼ 10𝑛𝑠 |

    | Drift                  | No        | No          | Weak        | Yes         |
    No       | No     |

    | Write energy (per bit) | 1 − 10𝑓 𝐽 | 10 − 100𝑓 𝐽 | 0.1 − 1𝑝𝐽   | 100𝑝𝐽       |
    ∼ 100𝑓 𝐽 | > 1𝑓 𝐽 |

    | Density                | Low       | Medium      | High        | High        |
    Medium   | High   |

    | Endurance              | > 1016    | > 1016      | > 105 − 108 | > 105 − 108
    | > 1015   | > 1015 |

    | Retention              | Low       | Very Low    | Medium      | long        |
    Medium   | long   |


    Table 1. A comparison of the key features across different mainstream CMOS and
    emerging memristive technologies [\[39\]](#page-35-12).


    ### **3.7 Ferroelectric Field-Effect Transistor (FeFET)**


    <span id="page-8-1"></span>Since the discovery of ferroelectricity in hafnium
    oxide, [FeFETs](#page-40-4) have received considerable attention. [FeFETs](#page-40-4)
    are non-volatile three-terminal devices, offering high / ratios and low read voltage.
    Unlike *metal-oxide-semiconductor* [\(MOS\)](#page-40-11)-FETs, [FeFETs](#page-40-4)
    incorporate a ferroelectric oxide layer in the gate stack, as shown in Figure
    [5c.](#page-6-0) The nonvolatility arises from hysteresis due to the coupling
    between the ferroelectric and CMOS capacitances (C and C ). The three-terminal
    structure of [FeFETs](#page-40-4) enables separate read and write paths. Reading
    involves sensing the drain-source current, while writing involves switching the
    ferroelectric polarization with an appropriate V voltage. Unlike two-terminal
    devices with variable resistance, [FeFETs](#page-40-4) do not require a drain-source
    current during the writing process, leading to low writing energy consumption
    [\[37\]](#page-35-13). There are various [CIM](#page-39-0) architectures exploiting
    different properties of [FeFETs](#page-40-4). For instance, for boolean operations,
    [\[37\]](#page-35-13) proposes precharging the bitlines followed by simultaneous
    activation of the target rows, and using differential sense amplifiers to discern
    the output.


    #### **3.8 Comparison and discussion**


    Table [1](#page-8-0) presents a comparison between mainstream and emerging memory
    devices, discussed in the preceding sections, with respect to performance, reliability,
    and energy consumption, among others. This analysis gives insights into their
    suitability for different application domains. It is clear that no single memory
    device can optimize all metrics. Nonetheless, recent investigations into machine
    learning use cases show that different phases of machine learning tasks demand
    different memory device properties, potentially offering the opportunity to employ
    various devices for various application domains (or tasks within a domain) and
    achieve the best results [\[38\]](#page-35-14).


    [PCM,](#page-40-1) [RRAM,](#page-40-2) [MRAM](#page-40-3) and [FeFET](#page-40-4)
    fall under the category of memristive technologies, where devices can exhibit
    multiple resistance states. This characteristic has been effectively leveraged
    to perform [MVM,](#page-41-2) as depicted in Figure [15a.](#page-17-0) Although
    the analog computation may not be entirely precise, some loss in accuracy is acceptable
    in many application domains, particularly for machine learning applications. Numerous
    [CIM](#page-39-0) architectures have been proposed using this technique to accelerate
    neural network models (see Section [4.2\)](#page-15-0).


    Figure [6](#page-9-1) shows the importance of various device properties for neural
    network training and inference. For training, frequent weight updates within the
    memory are crucial, making memory technologies like [PCM](#page-40-1) and [RRAM,](#page-40-2)
    with limited endurance and expensive write operations, poorly suitable for training
    acceleration. However, in inference, where operations are predominantly read-based
    with minimal writes to the crossbar array, the same technology could outperform
    others by orders of magnitude. Similarly, retention for training is the least
    important but is critical for inference.


    The arguments around Figure [6](#page-9-1) generally hold true for other metrics
    and other application domains. Although machine learning constitutes a significant
    area, it is not the only domain benefiting from the


    <span id="page-9-1"></span>![](_page_9_Figure_1.jpeg)


    Fig. 6. A spider chart showing the importance of different attributes of the NVM
    technologies for the neural network training and inference. More distance from
    the center means more important [\[38\]](#page-35-14).


    [CIM](#page-39-0) paradigm. Many other data-intensive application domains have
    also effectively exploited the [CIM](#page-39-0) paradigm. Figure [7](#page-9-2)
    shows a landscape of [CIM](#page-39-0) applications, emphasizing precision considerations,
    computational complexity and memory access requirements. These applications are
    classified into three categories based on their precision demands. This data pertains
    to 2020. Over the past two years, additional application domains, such as databases,
    bioinformatics, and solving more complex algebraic tasks, have gained significant
    attention as well.


    <span id="page-9-2"></span>![](_page_9_Figure_4.jpeg)


    Fig. 7. The applications landscape for [CIM](#page-39-0) and [CNM](#page-39-1)
    [\[17\]](#page-34-15).


    # <span id="page-9-0"></span>**4 SELECTED ARCHITECTURES**


    In this section, we discuss some prevalent [CIM](#page-39-0) and [CNM](#page-39-1)
    architectures, explaining their programming models and systems integration. It
    is important to highlight that there exist COM accelerators optimized for specific
    domains, achieving throughput similar to [CNM/](#page-39-1)[CIM](#page-39-0) counterparts,
    albeit at the expense of higher energy consumption. These accelerators are beyond
    the scope of this paper.


    #### CNM/CIM Landscape 11


    This section is structured as follows: In Section [4.1,](#page-10-0) we provide
    an overview of [CNM](#page-39-1) systems, starting with academic designs and progressing
    to commercial [CNM](#page-39-1) systems, including both planar 2D and stacked
    DRAM structures. Section [4.2](#page-15-0) follows a similar organization for
    [CIM](#page-39-0) systems employing various technologies. In Section [4.3,](#page-24-0)
    we conduct a comparative analysis of different [CIM](#page-39-0)[/CNM](#page-39-1)
    systems, while Section [5.20](#page-31-0) outlines the key challenges faced by
    these innovative architectures.


    #### <span id="page-10-0"></span>**4.1 CNM architectures**


    <span id="page-10-4"></span>The core principle of compute-near-memory is to perform
    computations in the memory proximity by placing *processing unit*s [\(PUs](#page-40-12))
    on/near the memory chip. The first [CNM](#page-39-1) architecture dates back to
    the 1990s that aimed at integrating compute units with embedded DRAM on the same
    chip to achieve higher bandwidth. However, due to technological limitations and
    costly fabrication processes, even the promising initial [CNM](#page-39-1) proposals
    like IRAM [\[40\]](#page-35-15), DIVA [\[41\]](#page-35-16), and FlexRAM [\[42\]](#page-35-17)
    never commercialized.


    In recent years, due to the advancements in integration and die-stacking technologies,
    [CNM](#page-39-1) has regained interest within both industry and academia. [PUs](#page-40-12)
    are being integrated at different locations within memory devices, including within
    the memory chip as well as outside the memory chip on the module level, i.e.,
    dual in-line memory module (DIMM). A DIMM typically consists of multiple memory
    chips, each consisting of multiple ranks, banks, and subarrays. It is worth noting
    that some researchers also classify [PUs](#page-40-12) integrated at the memory
    controller level as [CNM.](#page-39-1) However, following the classification and
    terminology adopted in this report, we categorize it under the COM class.


    In the following, we explain some of the common [CNM](#page-39-1) architectures.
    We start by examining the planar 2D DRAM-based [CNM](#page-39-1) designs, then
    transition to discussing the 2.5D and 3D DRAM-based [CNM](#page-39-1) systems,
    and ultimately conclude on the NVM-based [CNM](#page-39-1) architectures.


    <span id="page-10-2"></span>*4.1.1 The UPMEM system.* UPMEM is a recent commercial
    near-bank [CNM](#page-39-1) system and is publicly available [\[5\]](#page-34-3).
    Figure [8](#page-10-1) gives a detailed overview of the UPMEM architecture. The
    memory modules are divided into PIM-enabled memory and main memory (conventional).
    The PIM-enabled memory combines co-processors known as *data processing unit*s
    [\(DPUs](#page-39-18)) with conventional DDR4 DRAM on the same die.


    <span id="page-10-3"></span><span id="page-10-1"></span>![](_page_10_Figure_7.jpeg)


    Fig. 8. An overview of the UPMEM architecture [\[43\]](#page-35-18).


    [DPUs](#page-39-18) are 32-bit general-purpose RISC processors, comprising a 64kB
    SRAM-based scratchpad working memory known as WRAM, a 24kB SRAM-based instruction
    memory referred to as IRAM, and a shared main memory named MRAM, based on DRAM
    technology. As shown in the figure (lower left), each DIMM consists of 16 memory
    chips, with each chip housing 8 banks, and each bank containing one [DPU.](#page-39-18)
    The latest UPMEM systems can support up to 20 DIMMs.


    **[DPU-DPU](#page-39-18) communication:** [DPUs](#page-39-18) in UPMEM can have
    up to 24 hardware threads called tasklets.


    <span id="page-11-2"></span>Within the same [DPU,](#page-39-18) tasklets can share
    data through MRAM and WRAM, however, [DPUs](#page-39-18) can not communicate with
    each other directly and must go through the host for any possible data sharing.
    **Progammability:** For programmability, UPMEM offers its own *software development
    kit* [\(SDK\)](#page-40-13) consisting of an UPMEM compiler and runtime libraries.
    [DPU](#page-39-18) programs are written in the C language including specific library
    calls. The runtime library provides functions for data and instruction transfers
    between different memory, e.g., MRAM-IRAM, MRAM-WRAM etc.; executing various functions
    on the [DPUs](#page-39-18); and synchronization (mutex locks, barriers, handshakes,
    and semaphores).


    Although UPMEM claims they have an easily programmable [SDK,](#page-40-13) programming
    the system has several challenges. The programmer is responsible for efficient
    mapping of executions and load-balancing on thousands of [DPUs](#page-39-18),
    managing data transfer, and ensuring coherence of data between CPU and [DPUs](#page-39-18).


    *4.1.2 CNM for [MVM](#page-41-2) in DRAM.* McDRAM [\[44\]](#page-35-19) and MViD
    [\[45\]](#page-35-20) (both involving Samsung Electronics) aimed at accelerating
    machine learning workloads by embedding *multiply-accumulate* [\(MAC\)](#page-40-14)
    units within the DRAM bank. Similar to UPMEM, both McDRAM and MViD utilize 2D
    DRAM (LPDDR in these cases) and incorporate [PUs](#page-40-12) within the memory
    banks to exploit the higher internal memory bandwidth. However, unlike UPMEM,
    these architectures are domain-specific and hence employ fixed functional units
    [\(MACs](#page-40-14)) instead of general-purpose programmable cores.


    <span id="page-11-3"></span><span id="page-11-0"></span>Figure [9](#page-11-0)
    shows the McDRAM architecture along with the three locations (column decoder,
    bitline *sense amplifier* [\(SA\)](#page-41-5)s, and I/O drivers) where MAC units
    were employed and evaluated. Each McDRAM chip consists of 4 banks and each bank
    has four 8-bit MAC units. The multiplication is performed in parallel by multiplying
    rows of the matrix with the input vector.


    <span id="page-11-1"></span>![](_page_11_Figure_5.jpeg)


    Fig. 9. The McDRAM architecture with three possible locations for MAC units [\[44\]](#page-35-19).


    **Programmability:** McDRAM is a fixed-function accelerator and offers a single
    interface function (matmul) that triggers the device driver to configure the control
    registers of the memory controller.It operates in two modes, memory and compute
    modes, determined by a configuration register. In compute mode, McDRAM performs
    [MVM](#page-41-2) tasks. For the management of [MVM](#page-41-2) within compute
    mode, it introduces six novel DRAM commands that leverage existing DRAM I/O signals,
    rendering no modifications to DRAM I/O signals. McDRAM, a fixed-function accelerator,
    employs a single interface function (matmul) triggering the device driver to configure
    memory controller control registers. It operates in two modes, memory and compute,
    determined by a configuration register. In compute mode, McDRAM performs [MVM](#page-41-2)
    introducing six novel DRAM commands for [MVM](#page-41-2) management without modifying
    existing DRAM I/O signals.


    <span id="page-12-1"></span>![](_page_12_Figure_2.jpeg)


    <span id="page-12-3"></span>Fig. 10. The MViD architecture [\[45\]](#page-35-20)
    where each bank has 16 MAC units.


    The MViD architecture depicted in Figure [10](#page-12-1) is similar to the McDRAM
    design and is specifically optimized for edge devices. Much like McDRAM, MViD
    incorporates MAC units within the DRAM I/O drivers to capitalize on the internal
    bandwidth of the DRAM. However, unlike McDRAM, MViD introduces a partitioning
    of memory banks into two categories: [MVM](#page-41-2) banks that are equipped
    for MAC units and two SRAM structures to hold the input and output vectors and
    non[-MVM](#page-41-2) banks (traditional). This division enables concurrent access
    to both types, meaning that multiplication operations in [MVM](#page-41-2) banks
    can occur simultaneously with the CPU accesses to the non[-MVM](#page-41-2) banks.


    <span id="page-12-0"></span>*4.1.3 Samsung''s CNM systems.* Samsung is probably
    ahead of everyone in the race for commercial [CNM](#page-39-1) systems. In the
    following, we discuss two of their recent promising (and complete) solutions.


    **PIM-HBM:** Samsung has recently introduced a [CNM](#page-39-1) architecture
    referred to as Function-in-Memory DRAM (FIMDRAM)[\[6\]](#page-34-4) or PIM-HBM[\[7\]](#page-34-5).
    It incorporates 16 *single-instruction multiple-data* [\(SIMD\)](#page-40-15)
    engines within the memory banks, enabling bank-level parallelism. As reported
    in [\[7\]](#page-34-5), their design does not disrupt crucial elements on the
    memory side, such as the sub-array and bank in conventional DRAM, making its integration
    seamless and straightforward. Importantly, it does not require any modifications
    to contemporary commercial processor components, including DRAM controllers. It
    is designed for host processors to manage PIM operations via standard DRAM interfaces.
    This feature allows for a straightforward substitution of existing JEDEC-compliant
    DRAM with PIM-DRAM across various systems.


    <span id="page-12-2"></span>![](_page_12_Figure_7.jpeg)


    Fig. 11. Samsung''s PIM-HBM (a) HBM die organization (b) Bank coupled with a PIM
    unit (c) PIM unit data path [\[7\]](#page-34-5).


    While Samsung reports that their design is compatible with any DRAM family, they
    have showcased its functionality using the 2.5D high bandwidth memory (HBM) DRAM.
    Figure [11](#page-12-2) provides a <span id="page-13-1"></span>high-level view
    of this architecture. Each bank comprises 16 [SIMD](#page-40-15) *floating-point
    unit*s [\(FPUs](#page-40-16)), with each FPU consisting of a 16-bit floating-point
    adder and a 16-bit floating-point multiplier. Furthermore, each FPU is equipped
    with data registers (GRFs), control and instruction registers (CRF, SRF), and
    an internal control unit. The internal control unit orchestrates operation sequences
    without necessitating modifications to the memory controller. When operating in
    PIM mode, the PIM execution units within all banks simultaneously respond to a
    standard DRAM column (Read or Write) command initiated by the host processor and
    execute a wide SIMD operation with deterministic latency in a lock-step manner.


    <span id="page-13-2"></span>**Programmability:** PIM-HBM comes with an *instruction
    set architecture* [\(ISA\)](#page-40-17), a software stack, and a specific programming
    model. The software stack presents a native execution path that does not require
    any modifications to the input code. The framework takes the high-level representation
    of an application and transforms it into device code. Furthermore, it offers a
    direct execution path that permits direct invocation of various function calls
    using the "PIM custom op". The PIM runtime includes a collection of modules responsible
    for tasks like operations offloading, memory allocation, and execution on the
    FPUs.


    HBM-PIM is a commercial accelerator and, as per, Samsung is already used by companies.
    Here is an excerpt from Samsung''s newsroom:


    *"Xilinx has been collaborating with Samsung Electronics to enable high-performance
    solutions for data center, networking, and real-time signal processing applications
    starting with the Virtex UltraScale+ HBM family, and recently introduced our new
    and exciting Versal HBM series products," said Arun Varadarajan Rajagopal, senior
    director, Product Planning at Xilinx, Inc. "We are delighted to continue this
    collaboration with Samsung as we help to evaluate HBM-PIM systems for their potential
    to achieve major performance and energy-efficiency gains in AI applications."*


    **AxDIMM (by Samsung-Facebook:)** Samsung is also working on the development of
    an FPGAenabled [CNM](#page-39-1) platform named AxDIMM. In collaboration with
    Facebook, this solution has showcased its effectiveness in a personalized recommender
    system. As shown in Figure [12a,](#page-13-0) the [CNM](#page-39-1) architecture
    (RANK) is the same as Samsung''s HBM-PIM, but the controlling unit is FPGA that
    starts the execution, maps computations to the RANK, and gets back the results.
    Like the HBM-PIM, AxDIMM has a complete software stack that allows programming
    the architecture without changing the input code or manually writing code using
    AxDIMM python API.


    <span id="page-13-0"></span>![](_page_13_Figure_6.jpeg)


    (a) Samsung-Facebook AxDIMM hardware module and architecture [\[46\]](#page-35-21)


    (b) AiM architecture [\[9\]](#page-34-7).


    Fig. 12. PUMA tile and core architectures [\[47\]](#page-36-0).


    For this product, Samsung also seems to be in discussion with SAP HANA. Here is
    another excerpt from the newsroom:


    *"SAP has been continuously collaborating with Samsung on their new and emerging
    memory technologies to deliver optimal performance on SAP HANA and help database
    acceleration," said Oliver Rebholz, head of HANA core research & innovation at
    SAP. "Based on performance projections and potential integration scenarios, we
    expect significant performance improvements for in-memory database management
    system (IMDBMS) and higher energy efficiency via disaggregated computing on AXDIMM.
    SAP is looking to continue its collaboration with Samsung in this area"*.


    *4.1.4 SK hynix''s accelerator-in-memory.* SK hynix''s accelerator-in-memory (AiM)
    is another [CNM](#page-39-1) system that targets the machine learning application
    domain [\[8,](#page-34-6) [9\]](#page-34-7). As stated in [\[9\]](#page-34-7),
    "Samsung''s FIMDRAM is near commercialization, but the required HBM technology
    may prevent it from being applied to other applications due to its high cost".
    AiM fundamentally follows a very similar design approach to Samsung''s FIMDRAM
    but utilizes GDDR6 instead.


    Figure [12b](#page-13-0) provides an overview of the AiM architecture. As depicted,
    each bank is equipped with a processing unit (PU) that executes a MAC operation
    using 16 multiplier units and an adder tree. The adder tree can be deactivated
    for operations not requiring additions. Similar to the FIMDRAM design, pairs of
    banks can establish direct communication. For inter-group communication, an internal
    2KB SRAM structure within the periphery facilitates the process.


    Although the programming model is not explicitly explained, the presented set
    of commands in AiM implies an interface enabling interaction with the device for
    various operations. Some of these operations are particularly interesting, such
    as the ability to perform computations within banks of different granularities
    (1, 4, 16) and data movement functions that can be utilized to implement row-cloning
    within DRAM.


    *4.1.5 AxRAM.* AxRAM targets optimizing for the off-chip memory communication
    bottleneck in GPUs by integrating approximate MAC units in the DRAM [\[48\]](#page-36-1).
    The fundamental idea is to exploit the inherent approximability of numerous GPU
    applications and perform approximate calculations directly within the memory banks,
    thereby reducing data movement and energy consumption. AxRAM leverages the concept
    of neural transformation, a technique that accelerates a wide range of applications
    by approximating specific sections of GPU code and transforming them into a neural
    representation composed primarily of [MAC](#page-40-14) and *look-up table* [\(LUT\)](#page-40-18)
    operations for nonlinear function calculation. The multiplications in the MAC
    operations are further approximated with limited iterations of shift-add and LUT
    accesses. These approximate units are connected to the wide data lines that connect
    the DRAM banks to the global I/O, keeping the banks and memory column unchanged.


    <span id="page-14-0"></span>Figure [13](#page-15-1) shows a sample example where
    the GPU code is transformed into MAC and lookup operations. Once such patterns
    are identified and transformed, they are offloaded to the in-DRAM accelerator.
    The new instructions that invoke and configure the in-DRAM accelerators are added
    to the GPU''s ISA and are exposed to the compiler. As for the flow of execution,
    initially, all data is assumed to be in one memory chip. The GPU starts normal
    execution, and for the identified approximate regions, the GPU warps send an initiation
    request to the on-chip memory controller. The additional logic in the memory controller
    first sends an invalid signal to the on-chip caches (to ensure data consistency)
    and subsequently drives the in-DRAM accelerator to perform the computations and
    store the results in the designated location. To check whether the execution is
    completed, the memory controller periodically checks the memory-mapped mode register
    of the DRAM, which is updated by the accelerator. Once the controller detects
    that this register is set, it signals the GPU that execution is finalized, allowing
    the GPU to proceed with precise execution of the subsequent instructions.


    <span id="page-15-1"></span>![](_page_15_Figure_1.jpeg)


    ![](_page_15_Figure_2.jpeg)


    *4.1.6 CNM systems based on 3D-stacked DRAM.* All the [CNM](#page-39-1) architectures
    discussed so far (except FIMDRAM) are based on planar 2D DRAM. However, the resurgence
    in [CNM](#page-39-1) systems is also primarily attributed to HBM and HMC technologies
    that seamlessly combine logic and memory within the same package. There is a series
    of proposals for [CNM](#page-39-1) systems leveraging these technologies. In the
    following, we discuss some of the prominent architectures.


    <span id="page-15-2"></span>**TESSERACT** [\[49\]](#page-36-2) targets accelerating
    graph-based applications. Their design comprises a host processor and an HMC with
    multiple vaults, each housing an out-of-order processor. These processors exclusively
    access their local data partition, while inter-communication is achieved through
    a messagepassing protocol. The host processor, however, can access the complete
    address space of the HMC. To capitalize on the substantial memory bandwidth available,
    they introduce prefetching mechanisms. **TOP-PIM** [\[50\]](#page-36-3) is an
    architecture that proposes an *accelerated processing unit* [\(APU\)](#page-39-19).
    Each APU integrates a GPU and a CPU on the same silicon die. These APUs are linked
    through high-speed serial connections to several 3D-stacked memory modules. APUs
    are general-purpose and support a series of applications ranging from graph processing
    to fluid and structure dynamics. The architecture allows code portability and
    easy programmability.


    **Active memory cube (AMC)** [\[51\]](#page-36-4) is also built upon HMC and proposes
    "lanes" in the HMC vault. Each AMC lane consists of a register file, a computational
    unit, and a load/store unit to support memory accesses. Communication among AMCs
    is only possible via the host processor. AMC also offers a compiler based on OpenMP
    for C/C++ and FORTRAN.


    **Heterogeneous reconfigurable logic (HRL)** [\[52\]](#page-36-5) leverages the
    logic layer in the 3D stacked HMC to implement heterogeneous coarse-grained (CGRAs)
    and fine-grained (FPGAs) logic blocks. The architecture separates routing networks
    for control and data signals, employing specialized units to efficiently handle
    branch operations and non-uniform data layouts commonly found in analytics workloads.


    # <span id="page-15-0"></span>**4.2 [CIM](#page-39-0) architectures**


    Much like [CNM,](#page-39-1) the concept of [CIM](#page-39-0) systems is not entirely
    novel; however, it has gained significant momentum due to breakthroughs in various
    NVM devices over the past decade. Figure [14](#page-16-0) shows a partial landscape
    of [CIM](#page-39-0) systems, along with a corresponding timeline. Most of the
    depicted [CIM](#page-39-0) accelerators originate from academia and are not taped
    out. However, in recent years, several semiconductor industry giants, including
    Intel, Samsung, TSMC, GlobalFoundries, and IBM, have invested in developing their
    own [CIM](#page-39-0) prototypes, mostly focused on the machine learning case.
    IBM,


    #### CNM/CIM Landscape 17


    ![](_page_16_Figure_1.jpeg)


    <span id="page-16-0"></span>in particular, stands out among others when it comes
    to the development of [CIM](#page-39-0) systems for different use cases.


    Fig. 14. A partial timeline of the evolution of [CIM](#page-39-0) systems (data
    until 2018) [\[28\]](#page-35-3). The radius of the circle is proportional to
    the amount of papers published that year.


    In this section, we overview some of the prominent [CIM](#page-39-0) designs from
    academia and industry. However, before going into the details of individual [CIM](#page-39-0)
    designs, we first introduce circuits that are typically used as basic [CIM](#page-39-0)
    primitives in these architectures.


    *4.2.1 CIM primitives.* Each of the [CIM](#page-39-0) architectures discussed
    in the following sections is either based on a crossbar, content-addressable-memory,
    or a boolean and arithmetic logic unit. In the following, we explain all three
    of them.


    **Crossbar:** A crossbar is a [CIM](#page-39-0) configuration in which each input
    connects to every output through cross-points, comprising memory cells and selectors.
    Figure [15a](#page-17-0) shows a technology-independent crossbar configuration.
    As we will see in the following sections, crossbars are particularly useful for
    the machine learning domain as they can compute [MVM](#page-41-2) in constant
    time.


    <span id="page-16-1"></span>**CAM:** *content-addressable-memory* [\(CAM\)](#page-39-20)
    is associative memory that enables parallel searches for a given query (input)
    across all stored content within a CAM array. CAMs are used in pattern matching
    and search operations from various application domains including databases, networking,
    and machine learning [\[53\]](#page-36-6). Figure [15c](#page-17-0) shows a technology-independent
    3 × 3 CAM structure.


    **Boolean and arithmetic logic in CIM:** In this class of [CIM,](#page-39-0) the
    [CIM](#page-39-0) array facilitates a specific set of general operations, such
    as Boolean logic and arithmetic, to be executed using customized peripheral circuits
    integrated within the random-access memory (RAM). The operands need to be stored
    in different rows of an array in a column-aligned fashion where each column represents
    a bit position. For a [CIM](#page-39-0) operation, multiple rows are typically
    activated simultaneously, and the output is sensed and inferred by the peripherical
    circuitry [\[23,](#page-34-21) [54\]](#page-36-7). Figure [15b](#page-17-0) shows
    a technology-independent structure implementing boolean logic.


    <span id="page-16-2"></span>*4.2.2 ISAAC (by Hewlett Packard Enterprise).* In-situ
    analog arithmetic in crossbars (ISAAC) [\[22\]](#page-34-20) is among the first
    [CIM](#page-39-0) accelerators with a complete design targeting *convolutional
    neural network* [\(CNN\)](#page-39-21) in RRAM. As shown in Figure [16,](#page-17-1)
    ISAAC''s architecture consists of multiple interconnected tiles via a concentrated-mesh
    (c-mesh) network. Each tile consists of 12 in-situ multiply-and-accumulate (IMA)
    units, a shift-and-add (S&A) unit, two sigmoid units, one max-pooling unit, an
    embedded DRAM (eDRAM) buffer for input data storage and an output register (OR)
    to accumulate (partial) results. Each


    <span id="page-17-0"></span>![](_page_17_Figure_1.jpeg)


    Fig. 15. Fundamental [CIM](#page-39-0) primitives [\[53\]](#page-36-6).


    <span id="page-17-1"></span>IMA integrates its own input register (IR), output
    register, S&A units, and eight 128 × 128 resistive crossbar arrays, also abbreviated
    XB or XBars, that share analog-to-digital converters (ADCs). Each XBar performs
    analog [MVM](#page-41-2) (see Figure [15a\)](#page-17-0) and is also equipped
    with a digital-to-analog converter (DAC) and an S&H circuitry. Communication within
    a tile is facilitated by a 32-bit inter-tile link.


    ![](_page_17_Figure_4.jpeg)


    Fig. 16. ISAAC architecture hierarchy [\[22\]](#page-34-20).


    The ISAAC design uses dataflow pipelining to optimize IMA unit utilization and
    reduce buffering requirements. Depending on the network''s size, each [CNN](#page-39-21)
    layer is mapped to one or multiple IMAs or tiles. Initially, input data is acquired
    through an I/O connection and stored within a tile''s eDRAM buffer. Before being
    fed to ReRAM XBars within each IMA, the data goes through DACs. Once processed
    by XBars, the generated feature maps are converted back to digital form and forwarded
    to max-pooling and activation units. The outcome of the NN layer is then accumulated
    within the S&A and OR units and subsequently written to a new eDRAM buffer (for
    the following layer). The depth of the pipeline corresponds to the depth of the
    neural network, which presents challenges when training *deep neural network*s
    [\(DNNs](#page-39-22)). Thus, ISAAC is specifically designed for inference and
    is not used for training. ISAAC has no mention of the design tools and programming
    interface.


    <span id="page-17-2"></span>*4.2.3 PUMA (by Hewlett Packard Enterprise).* PUMA
    (programmable ultra-efficient memristorbased accelerator) is a generalization
    of memristive crossbars to accelerate a range of ML inference workloads [\[47\]](#page-36-0).
    PUMA''s microarchitecture techniques exposed via dedicated ISA ensure the efficiency
    of in-memory computing and analog circuitry while providing a high degree of programmability.
    The


    architecture is organized into three hierarchy levels: cores, tiles, and nodes.
    Nodes are connected and communicate via a chip-to-chip network. Each individual
    node consists of tiles that are connected via an on-chip network, where each tile
    comprises cores that communicate via shared memory, as shown in Figure [17a.](#page-18-0)
    A PUMA''s core consists of its own memory and functional units, including the
    XBar array, referred to as the [MVM](#page-41-2) unit (MVMU), see Figure [17b.](#page-18-0)


    <span id="page-18-0"></span>![](_page_18_Figure_2.jpeg)


    Fig. 17. PUMA tile and core architectures [\[47\]](#page-36-0).


    Unlike most other [CIM](#page-39-0) architectures, which are data parallel, PUMA
    is a spatial architecture where distinct instructions are executed by each core
    or tile. Since manually writing code for such architectures is extremely difficult,
    particularly when they have thousands of cores, PUMA has a runtime compiler implemented
    as a C++ library. The compiler takes the high-level input code and extracts a
    dataflow graph from it. The graph is then divided into subgraphs, considering
    the sizes of MVMUs, and hierarchically assigned to MVMUs, cores, and tiles. The
    subgraph execution is carefully scheduled to ensure effective resource utilization
    while avoiding potential deadlocks. Given the constraint of serial read and write
    operations in RRAM, PUMA exclusively supports the ML inference. However, to facilitate
    training, PUMA has been repurposed in a follow-up work named PANTHER [\[55\]](#page-36-8).


    *4.2.4 Pinatubo: Accelerating bulk bitwise logic operation.* Pinatubo is a memristor-based
    architecture that harnesses data-level parallelism to conduct bulk bitwise operations
    [\[23\]](#page-34-21). Unlike the crossbar configurations, it performs computations
    in the digital domain by modifying the [SAs](#page-41-5). The system architecture
    is similar to a typical Von Neumann architecture that has a processor equipped
    with caches and a non-volatile main memory. Pinatubo then exploits the physical
    attributes of the NVM-based main memory and modifies the [SAs](#page-41-5) to
    support it. The main idea is the operands are stored in different rows but the
    same columns in an array, the rows are activated in parallel and the accumulated
    current in the bitline is compared to a reference level in the [SAs](#page-41-5).
    For different logic gates, the memory controller changes the reference levels
    in the [SAs](#page-41-5) to different stats.


    The Pinatubo''s main memory structure is illustrated in Figure [18,](#page-19-0)
    comprising multiple banks divided into banks and mats. For operands within the
    same mat, the modified [SAs](#page-41-5) work out of the box and can perform bitwise
    vector operations. For operations where the data is spread across different mats,
    whether within the same bank or not, additional logic gates are used for execution
    (within the global data line or global I/O). The architecture supports only logic
    operations.


    For programmability, Pinatubo presents a software infrastructure containing both
    the programming model and runtime support components. The programming model offers
    two functions to allocate


    <span id="page-19-0"></span>![](_page_19_Figure_1.jpeg)


    Fig. 18. Pinatubo architecture showing chip, bank and mat [\[23\]](#page-34-21).


    bit-vectors and perform bitwise operations. The runtime support facet encompasses
    adjustments to the C/C++ runtime library and the operating system (OS) and the
    development of a dynamic linked driver library. The runtime library ensures that
    bit-vectors are allocated to separate memory rows while the OS equipped with PIM-aware
    memory management, ensures intelligent invocation of the operations.


    *4.2.5 PRIME.* PRIME [\[21\]](#page-34-19) is another RRAM-based analog [CIM](#page-39-0)
    accelerator. The architecture comprises multiple banks where each bank integrates
    eight subarrays (chips) which are further (logically) split into memory (Mem)
    units, two full function (FF) units, and one buffer. FFs can function conventionally
    as memory or in an NN computation mode, controlled by the PRIME controller. A
    typical FF unit is 256 × 256 RRAM cells, with 6-bit reconfigurable local [SAs](#page-41-5)
    reading their outputs. During computation mode, RRAM resolution is 4-bit *multi-level
    cell* [\(MLC\)](#page-40-19), shifting to *single-level cell* [\(SLC\)](#page-40-20)
    in memory mode. Distinct crossbar arrays are utilized for storing positive and
    negative weights. The input to the mat comes from a 3-bit fixed point signal originating
    from a wordline decoder and driver (WDD). Analog subtraction and sigmoid functions
    within the NN are implemented in the modified column multiplexers within the RRAM
    arrays.


    <span id="page-19-4"></span><span id="page-19-1"></span>![](_page_19_Figure_5.jpeg)


    <span id="page-19-3"></span><span id="page-19-2"></span>Fig. 19. PRIME: Source
    code to execution [\[21\]](#page-34-19).


    The execution of an *neural-network* [\(NN\)](#page-40-21) on PRIME involves three
    stages. Firstly, the [NN](#page-40-21) is mapped onto FF subarrays, and synaptic
    weights are programmed into ReRAM cells. In the optimization stage, depending
    on the [NN](#page-40-21) size, mapping could occur in a single bank or across
    multiple banks. These first two stages are executed by the CPU. Subsequently,
    a series of generated instructions are transmitted to the PRIME controller in
    RRAM banks to perform computations. The presence of latches and OR gates facilitates
    pipelined computation within PRIME.


    As shown in Figure [19,](#page-19-1) PRIME also comes with a compiler and an API,
    exposing device capabilities as function calls. The process from code to execution
    involves programming (coding), compiling (code optimization), and code execution.
    PRIME offers application programming interfaces (APIs) that empower developers
    to map [NN](#page-40-21) topologies onto FFs and configure data paths etc.


    *4.2.6 Pipelayer.* Pipelayer is another RRAM-based accelerator for [CNNs](#page-40-21)
    that supports both training and inference [\[56\]](#page-36-9). The overall architecture
    of PipeLayer, shown in Figure [20](#page-20-0) features RRAM crossbars, the spike
    Driver block to encode inputs as spikes and get rid of DACs, and integration <span
    id="page-20-0"></span>and fire components that eliminate ADCs. In write mode,
    the spike driver updates RRAM array weights with a 4-bit resolution. Within the
    cell, data processing occurs across morphable and memory subarrays, where memory
    subarrays are conventional memory arrays while morphable arrays can be configured
    in both compute and memory modes. Pipelayer leverages these morphable subarrays
    for different purposes in training and inference.


    ![](_page_20_Figure_2.jpeg)


    <span id="page-20-1"></span>Fig. 20. An overview of the Pipelayer architecture
    [\[56\]](#page-36-9).


    PipeLayer allows interactive configuration of the system on a per-layer basis.
    It provides an API that has functions for different operations e.g., bidirectional
    transfer of data between the CPU main memory and PipeLayer, the topology\_set
    function, where the number of compute groups can be specified by the programmer,
    the weight\_load function to load either pre-trained weights during testing or
    initial weights during training into the arrays. Other functions include pipeline
    and mode set functions for the morphable subarrays.


    There are many other RRAM-based analog and digital [CIM](#page-39-0) accelerators.
    Some other common ones that are mostly taped-out and not discussed here include:
    AtomLayer [\[57\]](#page-36-10), RIMAC[\[58\]](#page-36-11), FORM [\[59\]](#page-36-12),
    RRAMs for pattern recognition [\[60\]](#page-36-13), RRAM accelerator for BNNs
    (ISSCC, 65 nm) [\[61\]](#page-36-14), RRAM for edge processors (ISSCC, 55 nm)
    [\[62\]](#page-36-15), analog RRAM with fully parallel MAC and extremely high
    TOPS/W (ISSCC, 130 nm but large array) [\[63\]](#page-36-16).


    *4.2.7 In-DRAM computing.* Ambit [\[25\]](#page-35-0) is a DRAM-based [CIM](#page-39-0)
    accelerator that, unlike all previous systems, leverages the analog capabilities
    of current DRAM technology for executing bulk bitwise operations. Ambit mainly
    comprises two components. First, Ambit-AND-OR implements *triple row activation*
    [\(TRA\)](#page-41-6) in conventional DRAM. Like memristors, the idea is to activate
    three rows in parallel and leverage the *charge-sharing and charge accumulation
    principle*. [TRA](#page-41-6) produces a bitwise majority function. Controlling
    the initial state of one of the three rows enable performing AND and OR operation.
    The second component of Ambit is Ambit-NOT, which uses the inverters in the DRAM
    [SAs](#page-41-5) to implement the logic NOT operation. The basic components are
    then extended to implement other logic operations and accelerate bulk bitwise
    operations in multiple applications. With 8 DRAM banks, Ambit demonstrates a substantial
    improvement in bulk bitwise operation throughput compared to an Intel Skylake
    processor and the NVIDIA GTX 745 GPU.


    A follow-up work on the bulk bitwise logic in DRAM, ComputeDRAM [\[29\]](#page-35-4)
    demonstrated that by deliberately violating timing parameters between activation
    commands, certain existing off-the-shelf DRAM chips can implement the [TRA](#page-41-6)
    operation of Ambit. This indicates that certain real-world off-the-shelf DRAM
    chips, despite not being intended for Ambit operations, can indeed perform in-DRAM
    AND and OR operations. This also suggests that the concepts introduced in Ambit
    might not be too far from practical implementation. If existing DRAM chips can
    perform such operations to some extent, then chips explicitly designed for such
    functions could potentially be even more capable.


    *4.2.8 In-SRAM computing.* Neural Cache [\[64\]](#page-36-17) is an SRAM-based
    [CIM](#page-39-0) accelerator primarily targeting [CNNs](#page-39-21). The core
    operations of Neural Cache are bitwise AND and NOR operations, which are executed
    by simultaneously activating multiple rows (charge sharing). It repurposes the
    cache memory by modifying the peripheral circuitry to support operations such
    as convolution, pooling, quantization, and fully-connected layers, all performed
    at an 8-bit data precision. It is also capable of performing bit-serial operations
    like addition, subtraction, multiplication, comparison, search, and copy for larger
    data, utilizing carry latches linked to [SAs](#page-41-5). A transpose memory
    unit is introduced that facilitates the reorganization of data into bit-serial
    format within the memory when needed.


    IMAC [\[30\]](#page-35-5) is another SRAM-based [CIM](#page-39-0) accelerator
    that uses the precharge circuit to perform multi-bit analog multiplication by
    encoding the bit significance in the pulse width of pre-charge pulse. IMAC also
    requires DAC/ADC converters to facilitate the conversion between digital and analog
    forms. There are many other instances of SRAM-based [CIM](#page-39-0) designs,
    some even **taped-out** [\[65](#page-36-18)[–68\]](#page-36-19).


    *4.2.9 In-MRAM computing.* In NVMs, Magnetic RAM (MRAM) is probably the most mature
    memory technology that is commercially available and is already used in many embedded
    devices (see Section [3.6\)](#page-7-0). Therefore, it has also been intensively
    investigated in the [CIM](#page-39-0) context and computing approaches implementing
    in-MRAM basic boolean logic operations and more complex arithmetic functions have
    been showcased. Like all other technologies, the basic [CIM](#page-39-0) methods
    include bit-cell modification, reference adaptation, and in-memory analog computation.


    <span id="page-21-0"></span>![](_page_21_Figure_5.jpeg)


    Fig. 21. A typical SOT-MRAM subarray architecture for in-place logic [\[69\]](#page-37-0).


    Figure [21](#page-21-0) shows a typical subarray architecture of an in-MRAM [CIM](#page-39-0)
    [\[69\]](#page-37-0). The important difference here compared to already known
    aspects is that it has separate read and write bit and word lines and three reference
    resistance states in the sensing circuity (RAND/RM/ROR). RM is used to perform
    normal memory operations, while RAND and ROR, as the names suggest, are used to
    implement AND and OR operations, respectively.


    Similar to other technologies, the boolean logic is implemented with charge-sharing,
    and MAC is implemented in the analog domain with current accumulation. Some prominent
    MRAM-based [CIM](#page-39-0) designs include analog MACs for [TRA](#page-41-6)
    inference [\[70\]](#page-37-1), MRAM[-CIM](#page-39-0) for BNNs [\[71\]](#page-37-2),
    and MRAM crossbar [\[72\]](#page-37-3).


    *4.2.10 CIM using FeFETs.* FeFeTs have also been shown to implement in-place logic
    gates, addition, and content-addressable memories (CAMs). Notably, these logic
    operations can also be implemented with a single [FeFET](#page-40-4) cell. For
    instance, if one operand is stored in a cell (or a set of cells), the other operand
    can be applied as input to perform logic operation [\[73\]](#page-37-4), akin
    to the working principle of crossbars. Further, solutions proposing activating
    multiple rows and leveraging the bitline''s charge sharing (as in other memory
    technology) have also been presented [\[37\]](#page-35-13).


    FeFETs have received particular interest in CAM designs. CAMs are associative
    memories that can perform parallel searches for a query across all stored contents
    within an array. FeFeTs have been used to implement different types of CAMs for
    exact-search operations, approximate search operations, range-based search operations,
    or a combination of them [\[74,](#page-37-5) [75\]](#page-37-6).


    *4.2.11 Latest industrial chips.* In the previous sections, we have extensively
    discussed a variety of notable [CIM](#page-39-0) and [CNM](#page-39-1) solutions
    employing different technologies. While a few of these systems have been developed
    in collaboration with industry partners and a subset has undergone the tape-out
    process, the majority of these accelerators originate from academia. In this section,
    we specifically present [CIM](#page-39-0) systems originating from the industrial
    sector in the last couple of years. Note that these [CIM](#page-39-0) systems
    also primarily show prototypes showcasing various research outcomes, but they
    indicate their potential realization in the near future.


    **IBM''s PCM-based accelerators:** For more than five years, IBM has been using
    its PCM device to do in-place operations for different use cases. Initially, they
    were working with a reservoir of devices (millions of them) and implementing the
    peripheral circuitry and additional CMOS logic in an FPGA. Their research has
    progressed to consolidate all components onto a single chip, as exemplified by
    HERMES, a core composed of 256×256 PCM array with ADCs, a local digital processing
    unit, and additional peripheries [\[76\]](#page-37-7). The core effectively executes
    a fully parallel 256×256 analog [MVM,](#page-41-2) where each 8T4R unit cell encodes
    a positive/negative weight, with simultaneous handling of 256 8-bit digital inputs/outputs.
    Positive weights are encoded by combining the conductance of two PCM devices,
    while negative weights are represented by the other two PCMs within the unit cell.


    This year, IBM announced a newer 64-core [CIM](#page-39-0) chip designed and fabricated
    in 14-nm CMOS technology integrated with PCM [\[77\]](#page-37-8). The fully integrated
    chip comprises 64 cores, each with a size of 256×256, connected through an on-chip
    communication network. It reportedly achieves an unparalleled maximal throughput
    of 63.1 TOPS at an energy efficiency of 9.76 TOPS/W for 8-bit input/output [MVMs](#page-41-2).


    **Samsung''s MRAM crossbar:** Crossbar-based analog [MVM](#page-41-2) is well-explored
    in RRAM and PCM technologies. However, implementing MRAM-based crossbars is challenging
    due to the inherent low resistance of these devices, which could lead to significant
    power consumption. In 2022, Samsung presented a 64x64 MRAM crossbar array to address
    the low-resistance issue by employing an architecture that uses resistance summation
    (instead of current summation) for analog multiplyaccumulate operations [\[72\]](#page-37-3).
    Compared to the IBM HERMES cores, Samsung''s crossbar is significantly less sophisticates
    and limited in scale.


    <span id="page-23-0"></span>**TSMC''s in-SRAM accelerator:** While other SRAM-based
    [CIM](#page-39-0) chips exist, our focus is on the TSMC macro structure using
    standard 8T cells [\[78\]](#page-37-9) due to its better noise margin, ensuring
    stable activation for multiple rows operations in the [CIM](#page-39-0) mode,
    albeit with approximately 30% increased area.


    ![](_page_23_Figure_3.jpeg)


    Fig. 22. TSMC''s [CIM](#page-39-0) SRAM structure [\[78\]](#page-37-9).


    The proposed design shown in Figure [22](#page-23-0) has a 64×64 SRAM array and
    enables parallel computations of the multiply-and-average (MAV) operations. In
    a single cycle, the MAV computation of 64 4-bit inputs with 16 4-bit weight can
    be completed. The 4-bit input is represented by the number of read word line pulses
    which is precisely controlled by 4-bit digital counters. The 4-bit weight is achieved
    through charge sharing across binary-weighted computation capacitors. Each computation
    capacitor unit is constructed using the inherent capacitor of the [SA](#page-41-5)
    within the 4-bit flash ADC to optimize space and minimize the kick-back effect.
    This 64x64 8T macro is fabricated using 7nm FinFET technology, exhibiting an energy
    efficiency of 351 TOPS/W and a throughput of 372.4 GOPS for 1024 (64x16) 4x4b
    MAV operations.


    **Intel''s SRAM-based analog CIM design:** Intel has recently proposed an SRAM-based
    [CIM](#page-39-0) macro utilizing their 22nm Low-Power FinFET process [\[79\]](#page-37-10).
    Through the implementation of a 1-to-2 ratioed capacitor ladder (C-2C)-based charge
    domain computing scheme, the presented prototype chip (shown in Figure [23\)](#page-24-1)
    achieves the capability to perform up to 2k MAC operations in a single clock cycle,
    alongside achieving a peak power efficiency of 32.2-TOPS/W with 8-bit precision
    for both input activation and weights. The chip also ensures accurate [MVMs](#page-41-2)
    by restricting the computation error of less than 0.5%.


    **Bosch+Fraunhofer and GlobalFoundries+Fraunhofer FeFET based CIM designs:** Fraunhofer
    is also actively working on exploring the manufacturability and scalability aspects
    of FeFET and MRAM devices at both the device and array levels. Together with GlobalFoundries,
    they have demonstrated a


    <span id="page-24-1"></span>![](_page_24_Figure_1.jpeg)


    Fig. 23. Chip level architecture diagram of Intel''s analog [CIM](#page-39-0)
    design [\[79\]](#page-37-10).


    FeFET-based crossbar array for multiply-accumulate (MAC) operation [\[80\]](#page-37-11).
    The array was fabricated at GlobalFoundries with 28nm CMOS technology coupled
    with FeFET. To prevent the accumulation of errors on the bitline, the arrays were
    divided into 8×8 segments.


    In a recent work, Fraunhofer and Robert Bosch demonstrated a [CIM](#page-39-0)
    crossbar using multi-level FeFET cells. In the proposed design, the input is encoded
    into the applied voltage duration and magnitude while the weights are stored in
    the multi-level FeFET cells. The MAC output is the accumulated capacitor voltage
    that depends on the activation time and the number of FeFETs activated. This reportedly
    reduces the impact of variations and the achieved performance of 885.4 TOPS/W
    is also nearly-double compared to existing solutions.


    **HP''s CAM designs:** In a recent work, Hewlett Packard Labs proposed a memristive-based
    analog CAM for tree-based machine learning [\[81\]](#page-37-12). Analog CAMs
    are capable of performing searches based on analog signal levels rather than digital
    data comparison. The proposed design combines analog CAMs with traditional analog
    RAM and accelerates large random forest models with it. Figure [24](#page-25-1)
    shows a high-level overview of the proposed system where the analog CAM can perform
    root-to-leaf evaluation of an entire tree in a single step.


    #### <span id="page-24-0"></span>**4.3 Comparative analysis and discussion**


    The surge in [CIM](#page-39-0) and [CNM](#page-39-1) systems is largely attributed
    to the revolution in data-intensive applications. According to recent research
    by TSMC, traditional SRAM and DRAM technologies have effectively scaled to meet
    capacity and bandwidth demands in the past decades [\[82\]](#page-37-13), but
    their future scalability is uncertain due to reaching inherent technological limits.
    This underscores the pivotal role that NVM will play in the future of computing.


    Especially in edge scenarios such as automotive, augmented reality, and AI, where
    energy efficiency is paramount, NVM technologies are poised to play a pivotal
    role. As energy efficiency increases through specialized hardware, domain-specific
    architectures harnessing these NVMs for [CIM](#page-39-0) and [CNM](#page-39-1)
    solutions are anticipated to experience an unprecedented surge in the coming years.
    A recent article from Intel [\[79\]](#page-37-10) compares the performance of
    conventional digital accelerators with the emerging analog and digital [CIM](#page-39-0)
    and [CNM](#page-39-1) accelerators. Conventional accelerators still achieve higher
    throughput because [CIM](#page-39-0) systems are relatively less optimized, array
    sizes are small, and the peripheral circuitry overhead is non-negligible. Yet,
    they are orders of magnitude better in terms of


    <span id="page-25-1"></span>![](_page_25_Figure_1.jpeg)


    Fig. 24. An overview of the HP [CIM](#page-39-0) system for tree-based learning
    [\[81\]](#page-37-12).


    power consumption. As of the time of writing, the most recent comparison depicted
    in [\[82\]](#page-37-13) shows similar trends.


    ![](_page_25_Figure_4.jpeg)


    <span id="page-25-2"></span>Fig. 25. Performance and power comparison of different
    *outside memory processing* [\(OMP\)](#page-40-22) (we call it [COM](#page-39-2)
    in this report), [NMP](#page-39-9) (ours [CNM\)](#page-39-1) and [IMP](#page-39-4)
    (ours [CIM\)](#page-39-0) [\[83\]](#page-37-14).


    Table [2](#page-26-0) presents a summary and comparison of the architectures discussed
    in this section. For brevity, we only compare important parameters, such as the
    underlying memory technology, available function (boolean logic, arithmetic, etc.),
    evaluation technique (simulation, prototype, analytic), programming model, application
    domain, and technology node.


    #### <span id="page-25-0"></span>**5 COMMERCIAL LANDSCAPE**


    This section overviews CIM and CNM companies/startups, highlighting their products,
    underlying technologies, customers (when known), and tools. As not everything
    about companies is public,


    <span id="page-26-0"></span>


    | Accelerator                                | Year | Technology                     |
    Type | Programming<br>model | Logic unit                   | Implementation                   |
    Domain              |

    |--------------------------------------------|------|--------------------------------|------|----------------------|------------------------------|----------------------------------|---------------------|

    | McDRAM                                     | 2018 | DRAM                           |
    CNM  | Extended ISA         | MAC                          | Hardware                         |
    AI                  |

    | MViD                                       | 2020 | DRAM                           |
    CNM  | Extended ISA         | MAC                          | Hardware                         |
    AI                  |

    | PIM-HBM                                    | 2021 | DRAM                           |
    CNM  | Full<br>software     | FPUs (add, Mul)              | Hardware                         |
    AI                  |

    | AiM                                        | 2022 | (HBM)<br>DRAM<br>(GDDR6)       |
    CNM  | stack<br>API         | MAC                          | Hardware                         |
    AI                  |

    | AxRAM                                      | 2018 | DRAM (GPU                      |
    CNM  | API                  | MAC, LUTs                    | GPGPU-Sim                        |
    AI                  |

    | TESSERACT                                  | 2015 | based system)<br>DRAM<br>(HMC)
    | CNM  | API                  | CPU                          | Simulation                       |
    Graph processing    |

    | TOP-PIM                                    | 2014 | DRAM<br>(HMC)                  |
    CNM  | OpenCL               | CPU+GPU                      | Simulation                       |
    Graph, HPC          |

    | AMC                                        | 2015 | DRAM                           |
    CNM  | OpenMP               | CPU                          | Simulation                       |
    HPC                 |

    | HRL                                        | 2015 | (HMC)<br>DRAM<br>(HMC)         |
    CNM  | MapReduce            | CGRA+FPGA                    | Simulation                       |
    Data analytics      |

    | CIM architectures (Academia/Research Labs) |      |                                |      |                      |                              |                                  |                     |

    | ISAAC                                      | 2016 | RRAM                           |
    CIM  | NA                   | Analog Xbar                  | Analytical                       |
    AI                  |

    | PUMA                                       | 2019 | RRAM                           |
    CIM  | Compiler             | Xbar                         | PUMAsim<br>(arch.<br>simulation)
    | AI                  |

    | Pinatubo                                   | 2016 | RRAM                           |
    CIM  | API, Runtime         | Boolean logic                | In-house simulator               |
    Bitwise Logic       |

    | PRIME                                      | 2016 | RRAM                           |
    CIM  | Compiler+API         | Xbar                         | Analytical                       |
    AI                  |

    | PipeLayer                                  | 2017 | RRAM                           |
    CIM  | API                  | Xbar                         | Analytical                       |
    CNN (train + infer) |

    | AtomLayer                                  | 2018 | RRAM                           |
    CIM  | NA                   | Xbar                         | Analytical                       |
    CNN (train + infer) |

    | RIMAC                                      | 2023 | RRAM                           |
    CIM  | NA                   | Xbar<br>(without<br>DAC/ADC) | In-house simulator               |
    DNN inference       |


    Table 2. A summary of the presented architectures. They are grouped into three
    categories: [CNM,](#page-39-1) [CIM,](#page-39-0) and [CIM](#page-39-0) (prototype
    chips/systems). All presented architectures are either simulation-based or prototype-based
    (no products).


    we only include details that we extract from these companies'' websites or are
    known to us via our network.


    #### **5.1 Axelera**


    <span id="page-26-1"></span>Axelera [\[84\]](#page-37-15) is one of the notable
    Semiconductor startups in Europe. Founded in 2021 and backed by tech giants like
    Bitfury and IMEC, it had already taped out its first CIM chip, Thetis, in December
    2021 (just four months after its founding). Today, it offers a fully integrated
    *system-on-chip* [\(SoC\)](#page-40-23) powered by its Metis AI processing units
    (AIPU).


    About the AI core, as per the company''s website: "Axelera AI has fundamentally
    changed the architecture of "compute-in-place" by introducing an SRAM-based digital
    in-memory computing (D-IMC) engine. In contrast to analog in-memory computing
    approaches, Axelera''s D-IMC design is immune to noise and memory non-idealities
    that affect the precision of the analog matrix-vector operations as well as the
    deterministic nature and repeatability of the matrix-vector multiplication results.
    Our D-IMC supports INT8 activations and weights, but the accumulation maintains
    full precision at INT32, which enables state-of-the-art FP32 iso-accuracy for
    a wide range of applications without the need for retraining".


    Axelera''s latest SoC consists of 4 cores and a RISC-V based control core. For
    programming these systems, Axelera provides an end-to-end integrated framework
    for application development. The high-level framework takes users along the development
    processes without needing to understand the underlying architecture or even the
    machine learning concepts.


    **Funding:** "Axelera AI, the provider of the world''s most powerful and advanced
    solutions for AI at the Edge, announces new investors who have joined their oversubscribed
    Series A round, bringing the total amount raised to \$50 million. In the last
    several months, CDP Venture Capital, Verve Ventures, and Fractionelera have joined
    the round", Axelera AI, May 22, 2023.


    # **5.2 d-Matrix**


    d-Matrix is at the forefront of driving the transformation in data center architecture
    toward digital in-memory computing (DIMC) [\[85\]](#page-37-16). Founded in 2019,
    the company has received substantial support from prominent investors and strategic
    partners, including Playground Global, M12 (Microsoft Venture Fund), SK Hynix,
    Nautilus Venture Partners, Marvell Technology, and Entrada Ventures.


    Leveraging their in-SRAM digital computing techniques, a chipset-based design,
    high-bandwidth BoW interconnects, and a full stack of machine learning and large
    language model tools and software, d-Matrix pioneers best-performing solutions
    for large-scale inference requirements. A full stack framework, compiler, and
    APIs (open-source as per the company''s website but couldn''t find the link).
    Their latest product Jayhawk II can scale up to 150 TOPS/W using 6nm technology
    and can handle LLM models up to 20× more inferences per second for LLM sizing
    to 40B parameters, compared to state-of-the-art GPUs.


    **Funding:** Temasek, Playground Global and Microsoft Corp.


    # **5.3 Gyrfalcon Technology**


    Gyrfalcon Technology [\[86\]](#page-37-17) also leverages CNM to accelerate AI
    on the edge. They offer an AI processing in memory (APiM) architecture that combines
    a large MAC array directly with MRAM memory modules. As of the current date, their
    software stack is not available.


    **Funding:** Private.


    # **5.4 MemComputing**


    MemComputing [\[87\]](#page-37-18), founded in 2016, uses a computational memory
    based on its self-organizing logic gates (SOLG). SOLGs are terminal-agnostic elements
    (memristor or memcapacitor) that implement various logic gates. Their target applications
    comprise industrial computations associated with optimizations, big data analytics,
    and machine learning. MemComputing provides a software stack and offers it as
    a software-as-a-service.


    **Funding:** MemComputing mentions the US Space Force, ENSOS, NASA, Ball Aerospace,
    PSA, US Air Force, Canvass Labs and Defence Innovation Unit as partners.


    ### **5.5 Memverge**


    Memverge [\[88\]](#page-37-19) is not directly doing any CIM or CNM but is relevant
    in the context. Backed by 9 investors including tech giants like Intel, SK hynix,
    the company''s main goal is to provide software designed to accelerate and optimize
    data-intensive applications. Their main target is to consider environments with
    "Endless Memory" and efficiently manage the memory to get more performance.


    **Latest news**: "Samsung, MemVerge, H3 Platform, and XConn, today unveiled a
    2TB Pooled CXL Memory System at Flash Memory Summit. The system addresses performance
    challenges faced by highly distributed AI/ML applications. These challenges include
    issues like spilling memory to slow storage when main memory is full, excessive
    memory copying, I/O to storage, serialization/deserialization, and Out-of-Memory
    errors that can crash an application.", MemVerge, August 8, 2023.


    # **5.6 Mythic**


    Mythic [\[89\]](#page-37-20) offers an analog matrix processor (Mythic AMP) that
    uses their analog compute engine (ACE) based on flash memory array and ADCs. Mythic
    ACE also has a 32b RISC V processor, SIMD vector engine, and a 64KB SRAM along
    with a high-throughput network-on-chip (NoC). Mythic workflow in Figure [26](#page-28-0)
    shows that the software stack takes a trained NN model, optimizes it, and compiles
    it to generate code for Mythic AMP. The optimization suit also transforms NN in
    a way that can be accelerated on the analog CIM system.


    <span id="page-28-0"></span>![](_page_28_Figure_3.jpeg)


    Fig. 26. Mythic AI workflow [\[89\]](#page-37-20).


    **Funding:** The company is supported by many investors: Micron, HP Enterprise,
    SoftBank, Future ventures, Lam Research, Threshold, Catapult, DCVC and UDC ventures.


    # **5.7 NeuroBlad**


    Founded in 2018, NeuroBlade offers the SPU (SQL Processing Unit), the industry''s
    first, proven processor architecture that delivers orders of magnitude improvement
    by departing from Von Neumann model [\[90\]](#page-37-21). Neuroblade is also
    a CNM architecture (more closed to near-storage computing) where they integrate
    custom RISC processors on the DRAM chip (very similar to UPMEM). The SPUs are
    installed as PCI-e cards that can be deployed in data centers. As for the software
    stack, the company offers an SDK along with a set of APIs that hide the complexity
    and programming model for these cores from the end user and also allow optimizing
    for maximum parallelism and efficiency.


    **Funding:** NeuroBlade is funded by Stage one, Grove Ventures, UMC, PSMC Intel
    capitals, Pegratron, Marubeni, Marius Nacht, Corner and MediaTek.


    # **5.8 Rain AI**


    Founded in 2017, Rain AI also focuses on radically cheaper AI computing [\[91\]](#page-37-22).
    The company has no hardware product yet but is aiming to be 100× better than GPU
    using their innovations in radical co-design (by looking at the algorithms and
    the CIM hardware at the same time). They are targeting AI training (along with
    the inference) on the edge with the ultimate goal of putting models the size of
    ChatGPT into chips of the size of a thumbnail. They are transforming the algorithms
    in a way that fundamentally matches the behavior of the analog memristive devices.
    As per the CEO, they have a few tap-outs planned for this year and the product
    (a complete platform) next year and they are working on a software stack for ease
    of use and ease of integration.


    **Funding:** The company is funded by Y combinator S18, Sam Altman (CEO OpenAI),
    Liquid 2 Ventures, Loup Ventures, Airbus Ventures, and Daniel Gross (founder Poineer).


    # **5.9 SEMRON**


    Founded in 2020, Semron [\[92\]](#page-37-23) promises to offer 3D solutions powered
    by analog CIM. At the core of their technology is their innovative CapRAM devices
    which are semiconductor devices that store multi-bit values in their variable
    capacitances (unlike variable resistance states in memristors). Since CapRAM is
    capacitive, the noise in calculations is much lower and the energy efficiency,
    as per their website, is unparalleled. Although Semron has the device technology,
    there are no details of its products, architecture, and software stack.


    **Funding:** As per crunchbase, the company is funded by VentureOut.


    # **5.10 SureCore**


    Surecore [\[93\]](#page-37-24) is working on many low-power products including
    custom application-specific. They also have a product named "CompuRAM" that embeds
    arithmetic capability within the SRAM array to enable low-power AI on the edge.
    Besides working on SRAM-based solutions, in collaboration with Intrinsic, they
    have recently ventured into RRAM technology. No information is provided regarding
    the software stack.


    **Funding:** The company is supported by Capital-E, Finance Yorkshire and Mercia
    Technologies.


    # **5.11 Synthara**


    Synthara is a Zurich-based Semiconductor company that was founded in 2017 [\[94\]](#page-37-25).
    Their latest product, ComputeRAM, integrates SRAM-based CIM macros with proprietary
    elements to accelerate dot products. The solution delivers 50× compute efficiency
    and can be used for AI, digital signal processing, and linear algebra-heavy routines.
    The CIM-powered SRAM array can be operated just like conventional SRAM. ComputeRAM
    is not married to a specific ISA and can work with any host processor. Synthara
    also provides what they call Compiler hooks that can transparently offload any
    input application to their ComputeRAM accelerator, without changing or rewriting
    the code.


    **Funding:** The company is supported by EU funding for research & innovation,
    High-tech Gründerfonds, Intel.ignite, FNSNF, multicoreware, ventureKick and others.


    # **5.12 Syntiant**


    Founded in 2017, Syntiant also leverages DRAM-based [CNM](#page-39-1) and utilizes
    standard CMOS processes to design their neural decision processors (NDPs) that
    perform direct processing of neural network layers from platforms like TensorFlow
    [\[95\]](#page-37-26). Syntiant also mainly targets AI on the edge having applications
    in many domains, including always-on voice, audio, image, and sensor applications.


    Syntiant''s TinyML platform, powered by NDP101, aspires to democratize AI by presenting
    a comprehensive system for those interested in initiating their own model training
    for edge computing. **Funding:** Syntiant is funded by prominent investors including,
    Atlantic Bridge, Rober Bosch Venture Capital, Embark Ventures, DHVC, Intel capitals,
    M12 (Microsoft ventures), and Motorola Solutions.


    # **5.13 TetraMem**


    Founded in 2018, TetraMem is set to offer the industry''s most disruptive CIM
    technology for edge application [\[96\]](#page-37-27). TetraMem is also leveraging
    memristors for analog MAC operations, aiming at inference on the edge. Their systems
    are built upon their patented devices and co-design solutions.


    TetraMem offers (1) Platform as a service (PaaS), a complete hardware and software
    platform designed to integrate into your own system; (2) Software as a service
    (SaaS), to help develop your NN edge application and integrate it into your system.
    Their verified full software stack provides an unmatched experience on actual
    analog in-memory compute silicon; and (3) a neural processing unit (NPU) based
    on memristive technology.


    TetraMem has recently announced a collaboration with Andes Technologies and together
    with their research collaborators have demonstrated a memristive device that can
    have thousands of conductance levels (unmatched) [\[97\]](#page-37-28).


    #### **Funding:** Private.


    # **5.14 EnCharge AI**


    Founded in 2022, EnCharge AI promise to offer an end-to-end scalable architecture
    for AI inference [\[98\]](#page-37-29). They leverage SRAM-based CIM arrays for
    analog MVM operations and combine them with SIMD CNM logic to perform custom element-wise
    operations. The architecture comprises an array of CIM units (CIMUs), an on-chip
    network interconnecting CIMUs, buffers, control circuitry, and off-chip interfaces.
    Each CIMU is equipped with an SRAM-based CIM array featuring ADCs to convert computed
    outputs into digital values. Additionally, CIMUs house SIMD units and FPUs with
    a custom instruction set, along with buffers dedicated to both computation and
    data flow. According to the company''s official website, they offer a software
    platform that fits with standard ML frameworks, such as PyTorch, TensorFlow, and
    ONNX. This also allows the implementation of various ML models and their customizations.
    Specific implementation details about the software stack are not available.


    **Funding:** Encharge AI is funded by AlleyCorp, Scout Ventures, Silicon Catalyst
    Angels, Schams Ventures, E14 Fund, and Alumni Ventures. At their launch in December
    2022, they announced securing \$21.7 Mio. in their series A round.


    ### **5.15 Re(conceive) AI**


    Re(conceive) is another CIM startup founded in 2019 that promises offering "the
    most power AI accelerator" [\[99\]](#page-38-0). As per their website, re(conceive)
    are pioneers in realizing the complete potential of CMOS-based analog in-memory
    AI computing, achieving the utmost efficiency among all known AI accelerators.
    However, no specific details are available on the company''s funding and technology
    (hardware/software).


    ### **5.16 Fractile AI**


    Established in 2022 by a team of Oxford University scientists, Fractile [\[100\]](#page-38-1)
    aims to transform the world by enabling large language models'' (LLM) inference
    at speeds up to 100 times faster than Nvidia''s most recent H100 GPUs. This increase
    in performance primarily arises from in-memory computations. However, the details
    of the technology, both hardware and software, as well as the company''s funding
    particulars, remain undisclosed.


    # **5.17 Untether AI**


    Founded in 2018 [\[101\]](#page-38-2), Untether''s main design integrates RISC-V
    cores on the SRAM chips for processing AI workloads. Their latest product, the
    tsunAImi accelerator card provides a phenomenal 2 POPS of compute power, twice
    the amount of any available product. This compute power translates into over 80,000
    frames per second of ResNet-50 throughput, three times the throughput of any product
    on the market. Untether AI provides an automated SDK for its products. The SDK
    takes a network model implemented in common machine learning frameworks like TensorFlow
    and PyTorch and lowers it into the kernel code that runs on these RISC-V processors.
    It automatically takes care of low-level optimizations, providing extensive visualization,
    a cycle-accurate simulator, and an easily adoptable runtime API.


    <span id="page-31-1"></span>


    | Company           | Use-Case                        | Technology                    |
    Solution                          | Programmability                                        |
    Funding<br>(Mio.<br>\$)<br>(PitchBook) |

    |-------------------|---------------------------------|-------------------------------|-----------------------------------|--------------------------------------------------------|----------------------------------------|

    | Axelera           | AI on the Edge                  | SRAM (digital MAC)            |
    Hardware-SoC                      | SDK provided                                           |
    63.72 (Early stage VC)                 |

    | d-Matrix          | AI inference in dat<br>acenters | SRAM (digital MAC)            |
    Chiplets                          | Open-source<br>Frame<br>work(s)                        |
    161.3 (Early stage VC)                 |

    | Synthara          | AI, DSP, Linear al<br>gebra     | SRAM (dot product)            |
    Accelerator                       | Compiler available                                     |
    3.33 (Grant)                           |

    | Mythic            | AI on the Edge                  | Flash (Analog computing)      |
    Accelerator,<br>pro<br>cessor     | Software<br>stack<br>(does<br>rewriting, opt,
    mapping) | 177.41 (Later stage VC)                |

    | Surecore          | AI on the edge                  | SRAM (CNM)                    |
    Chip                              | No details                                             |
    11.16 (Later stage VC)                 |

    | SEMRON            | AI on the edge                  | Memcapacitor                  |
    3D-Chip (planned)                 | No details                                             |
    1.63 (Seed round)                      |

    | Untether AI       | AI everywhere                   | SRAM+RISC-V (CNM)             |
    Chips, accelerator                | SDK<br>and<br>simulator<br>(Toolkit)                   |
    153.52 (Early stage VC)                |

    | Syntiant          | AI on the edge                  | SRAM+ARM<br>MCUs<br>(CNM)     |
    Processor                         | Available                                              |
    121.43 (Later stage VC)                |

    | Neuroblade        | Analytics                       | DRAM+RISC<br>cores<br>(CNM)   |
    Processor                         | Set of APIs                                            |
    110.43 (Debt - General)                |

    | Rain AI           | LLMs on the edge<br>(Training)  | Memristors                    |
    Processors                        | NA                                                     |
    64.04 (Later stage VC)                 |

    | TetraMem          | Edge applications               | Memristors                    |
    Processors,<br>soft<br>ware stack | HDK, SDK                                               |
    NA                                     |

    | Gyrfalcon<br>Tech | AI on the edge                  | CNM<br>(MACs<br>with<br>MRAM)
    | Chip                              | NA                                                     |
    68.0 (Debt - PPP)                      |

    | UPMEM             | General-purpose                 | DRAM+RISC cores               |
    System                            | APIs                                                   |
    15.5 (Later stage VC)                  |

    | EnCharge AI       | AI inference                    | SRAM (CIM) + SIMD<br>(CNM)    |
    Chip                              | Software available                                     |
    21.7 (Angel - individ)                 |

    | Re(conveive)      | AI inference                    | SRAM (analog CIM)             |
    Chip                              | NA                                                     |
    NA                                     |

    | Fractile          | LLMs inference                  | NA                            |
    Chip                              | NA                                                     |
    NA                                     |


    Table 3. CIM/CNM companies, with their products, technologies, and funding status.


    **Funding:** Untether''s investors include CPPIB, GM Ventures, Intel Capital,
    Radical Ventures, and Tracker Capital.


    # **5.18 UPMEM Technology**


    Founded in 2015, UPMEM is a tech company offering programmable CNM systems for
    data-intensive applications. See more details on the architecture and programmability
    in Section [4.1.1.](#page-10-2)


    **Funding:** The company is funded by Western Digital, Partech, and super nova
    invest.


    ### **5.19 Summary**


    Table [3](#page-31-1) and Figure [27](#page-32-1) summarize the discussion in
    this section and provides a landscape of CIM, CNM companies, their products, technologies,
    and funding status. Please note that this compilation is not exhaustive; it includes
    only companies known to us and those that, based on our understanding, fall within
    the CIM and CNM categories. As Figure [27](#page-32-1) clearly illustrates, the
    current landscape is predominantly characterized by conventional technologies,
    with a notable absence of a comprehensive software ecosystem.


    # <span id="page-31-0"></span>**5.20 Open challenges**


    [CIM](#page-39-0) and [CNM](#page-39-1) systems have already entered the market,
    yet a series of open challenges are expected to become more pronounced as time
    progresses. It will take years to understand how these units will harmonize within
    the overall system architecture and determine their optimal utilization. In the
    following, we briefly discuss the important categories.


    **Materials:** During the era of Moore''s law in computing, the primary focus
    was on refining transistors to be smaller, faster, and more energy-efficient.
    The selection of materials was confined to only


    #### CNM/CIM Landscape 33


    <span id="page-32-1"></span>![](_page_32_Figure_1.jpeg)


    ![](_page_32_Figure_2.jpeg)


    those compatible with manufacturing processes. However, the limitations of these
    materials to scale further are now exposed. As a result, new materials have emerged
    and further research is needed to investigate novel materials (to enable further
    transistor scaling: hopes with carbon nanotube, and novel memory devices).


    **Devices:** Mainstream computing has largely relied on digital logic and binary
    storage. Nonetheless, the emerging wave of computing architectures, particularly
    [CIM](#page-39-0) requires novel multi-state devices allowing both analog and
    digital operations. Existing devices, memristors in particular, do offer such
    properties but have reliability and other associated challenges.


    **Integration:** We have seen various architectures based on various technologies.
    As is evident, there is no on-technology-fits-all solution. Eventually, [CIM](#page-39-0)
    modules based on different technologies need to be integrated into the same to
    get the best out of all these technologies. This poses integration challenges
    that have received little to no attention.


    **Processing systems:** These novel architectures require new tools, algorithms,
    cost models, and software solutions. All of them are crucial to understanding
    these architectures, enabling their design space exploration, and making them
    accessible to a larger audience.


    While every challenge holds significance and demands attention, programmability
    and userfriendliness are the most important ones from the user''s standpoint.
    Following is an excerpt from Facebook''s recent article on their inference accelerator
    that highlights the same.


    *"We''ve investigated applying processing-in-memory (PIM) to our workloads and
    determined there are several challenges to using these approaches. Perhaps the
    biggest challenge of PIM is its programmability".*


    In response to the challenges associated with programmability, we have ourselves
    been working on high-level programming and compilation frameworks for CNM and
    CIM systems [\[102](#page-38-3)[–105\]](#page-38-4). We have developed reusable
    abstractions and demonstrated compilation flows for CIM systems with memristive
    crossbars, CAMs, CIM-logic modules, and for CNM systems like UPMEM and Samsung
    CNM. However, much more cross-layer work is needed to improve automation [\[106\]](#page-38-5),
    in particular for heterogeneous systems integrating several paradigms and technologies.


    #### <span id="page-32-0"></span>**6 CONCLUSIONS**


    This paper overviews the landscape of compute-near-memory (CNM) and compute-in-memory
    (CIM) paradigms. It starts with an explanation of the Von Neumann bottleneck,
    the necessity of novel CIM/CNM paradigms, and the key terminology used in the
    related literature. It offers a comprehensive


    background on major memory technologies and emphasizes the importance of heterogeneous
    systems. The paper overviews prominent CIM and CNM designs from both academia
    and industry. In contrast to other studies in the literature that focus on either
    application domains or memory technologies, this paper concentrates on designs
    that have either successfully transitioned into product offerings or have reached
    a stage where commercialization is a feasible prospect. We explain prevalent CNM
    architectures, including microarchitectural details, associated technologies,
    software frameworks, and the results achieved (usually measured as throughput).
    Subsequently, we survey the landscape of CIM systems, explaining prevailing CIM
    designs that use prominent technologies such as SRAM, DRAM, MRAM, RRAM, PCM, and
    FeFET. We overview CIM chips from industrial giants (research centers), spanning
    from earlier designs like ISAAC and PUMA by Hewlett Packard Enterprise to the
    most cutting-edge chips from IBM, Samsung, TSMC, Intel, Meta (Facebook), Bosch,
    Frauenhofer, and GlobalFoundries. Current trends in industrial research show that
    while conventional SRAM and DRAM technologies are ready to be leveraged in CIM/CNM
    systems, emerging technologies like PCM, RRAM, MRAM, and FeFETs are also poised
    to make partial inroads, particularly for selected operations, such as dot products
    and pattern matching.


    Finally, we describe the landscape of CIM and CNM start-ups, highlighting the
    emergence of numerous new companies in recent years that have introduced innovative
    solutions to cater to the thriving demands of AI and other data-intensive application
    domains. These companies are targeting a diverse range of market segments, spanning
    from power-efficient edge applications (AI at the edge) to high-performance data
    center servers (e.g., for AI training), and many have successfully secured substantial
    funding (hundreds of millions) in their initial funding rounds. The paper shows
    that SRAM technology currently dominates this landscape. However, with active
    research and breakthroughs in emerging NVMs (demonstrated by recent industrial
    chips), it is anticipated that NVMs will play a more prominent role in these paradigms
    in the near future.


    The paper highlights that CIM and CNM technologies (i) harbor significant potential
    to outperform conventional systems, and (ii) have already made inroads into the
    market. However, their true potential remains untapped. This is attributed to
    a number of challenges, including the lack of accurate design space exploration
    tools, programming frameworks, and a comprehensive software ecosystem in general,
    and cost and performance models that can be leveraged to guide static and runtime
    optimizations for these systems.


    CNM and CIM computing is an extremely active field. We believe that we have captured
    a representative snapshot of this field, early in year 2024, and remain excited
    about how technologies, devices, architectures and tools will continue to develop
    moving forward.


    #### **ACKNOWLEDGEMENTS**


    This work was supported by Vsquared Ventures (VSQ). Special thanks to Max Odendahl
    (Venture Partner at VSQ) for his feedback on previous versions of the manuscript.
    This work was also supported by the German Research Council (DFG) through the
    HetCIM project (project number 502388442) in the context of the DFG Priority Program
    on Disruptive Memory Technologies (SPP2377 [https://spp2377.uos.de\)](https://spp2377.uos.de)
    and the German Federal Ministry of Education and Research (BMBF, project number
    01IS18026A-D) by funding the competence center for Big Data and AI ScaDS.AI Dresden/Leipzig
    [\(https://scads.ai\)](https://scads.ai).


    #### **REFERENCES**


    <span id="page-33-0"></span>[1] S. Li, A. O. Glova, X. Hu, P. Gu, D. Niu, K. T.
    Malladi, H. Zheng, B. Brennan, and Y. Xie, "Scope: A stochastic computing engine
    for dram-based in-situ accelerator," in *2018 51st Annual IEEE/ACM International
    Symposium on Microarchitecture (MICRO)*. IEEE, 2018, pp. 696–709.


    - <span id="page-34-0"></span>[2] A. de Vries, "The growing energy footprint of
    artificial intelligence," *Joule*, vol. 9, no. 4, p. 1–4, Oct 2023. [Online].
    Available:<https://doi.org/10.1016/j.joule.2023.09.004>

    - <span id="page-34-1"></span>[3] J. Calma. (2023, September) Microsoft is going
    nuclear to power its ai ambitions. The Verge. [Online]. Available: <https://www.theverge.com/2023/9/26/23889956/microsoft-next-generation-nuclear-energy-smr-job-hiring>

    - <span id="page-34-2"></span>[4] A. Reuther, P. Michaleas, M. Jones, V. Gadepally,
    S. Samsi, and J. Kepner, "Ai and ml accelerator survey and trends," in *2022 IEEE
    High Performance Extreme Computing Conference (HPEC)*. IEEE, 2022, pp. 1–10.

    - <span id="page-34-3"></span>[5] F. Devaux, "The true processing in memory accelerator,"
    in *2019 IEEE Hot Chips 31 Symposium (HCS)*. IEEE Computer Society, 2019, pp.
    1–24.

    - <span id="page-34-4"></span>[6] Y.-C. Kwon, S. H. Lee, J. Lee, S.-H. Kwon, J.
    M. Ryu, J.-P. Son, O. Seongil, H.-S. Yu, H. Lee, S. Y. Kim *et al.*, "25.4 a 20nm
    6gb function-in-memory dram, based on hbm2 with a 1.2 tflops programmable computing
    unit using bank-level parallelism, for machine learning applications," in *2021
    IEEE International Solid-State Circuits Conference (ISSCC)*, vol. 64. IEEE, 2021,
    pp. 350–352.

    - <span id="page-34-5"></span>[7] S. Lee, S.-h. Kang, J. Lee, H. Kim, E. Lee,
    S. Seo, H. Yoon, S. Lee, K. Lim, H. Shin *et al.*, "Hardware architecture and
    software stack for pim based on commercial dram technology: Industrial product,"
    in *2021 ACM/IEEE 48th Annual International Symposium on Computer Architecture
    (ISCA)*. IEEE, 2021, pp. 43–56.

    - <span id="page-34-6"></span>[8] M. He, C. Song, I. Kim, C. Jeong, S. Kim, I.
    Park, M. Thottethodi, and T. Vijaykumar, "Newton: A dram-maker''s accelerator-in-memory
    (aim) architecture for machine learning," in *2020 53rd Annual IEEE/ACM International
    Symposium on Microarchitecture (MICRO)*. IEEE, 2020, pp. 372–385.

    - <span id="page-34-7"></span>[9] S. Lee, K. Kim, S. Oh, J. Park, G. Hong, D.
    Ka, K. Hwang, J. Park, K. Kang, J. Kim *et al.*, "A 1ynm 1.25 v 8gb, 16gb/s/pin
    gddr6-based accelerator-in-memory supporting 1tflops mac operation and various
    activation functions for deep-learning applications," in *2022 IEEE International
    Solid-State Circuits Conference (ISSCC)*, vol. 65. IEEE, 2022, pp. 1–3.

    - <span id="page-34-8"></span>[10] H.-S. Wong and S. Salahuddin, "Memory leads
    the way to better computing," *Nature nanotechnology*, vol. 10, pp. 191–4, 03
    2015.

    - <span id="page-34-9"></span>[11] H.-S. P. Wong, H.-Y. Lee, S. Yu, Y.-S. Chen,
    Y. Wu, P.-S. Chen, B. Lee, F. T. Chen, and M.-J. Tsai, "Metal–oxide rram," *Proceedings
    of the IEEE*, vol. 100, no. 6, pp. 1951–1970, 2012.

    - <span id="page-34-10"></span>[12] W. J. Gallagher and S. S. P. Parkin, "Development
    of the magnetic tunnel junction mram at ibm: From first junctions to a 16-mb mram
    demonstrator chip," *IBM J. Res. Dev.*, vol. 50, no. 1, pp. 5–23, Jan. 2006. [Online].
    Available: <http://dx.doi.org/10.1147/rd.501.0005>

    - <span id="page-34-11"></span>[13] J. Hoffman, X. Pan, J. W. Reiner, F. J. Walker,
    J. P. Han, C. H. Ahn, and T. P. Ma, "Ferroelectric field effect transistors for
    memory applications," *Advanced Materials*, vol. 22, no. 26-27, pp. 2957–2961,
    2010. [Online]. Available: <https://onlinelibrary.wiley.com/doi/abs/10.1002/adma.200904327>

    - <span id="page-34-12"></span>[14] P. M. Research, "Market study on in-memory
    computing: Adoption of fast-processing databases fuels the demand. report pmrrep33026,"
    2022.

    - <span id="page-34-13"></span>[15] P. Radojković, P. Carpenter, P. Esmaili-Dokht,
    R. Cimadomo, H.-P. Charles, S. Abu, and P. Amato, "Processing in memory: the tipping
    point," *White paper: Processing in Memory: the Tipping Point*, 2021.

    - <span id="page-34-14"></span>[16] M. Anderson, B. Chen, S. Chen, S. Deng, J.
    Fix, M. Gschwind, A. Kalaiah, C. Kim, J. Lee, J. Liang *et al.*, "First-generation
    inference accelerator deployment at facebook," *arXiv preprint arXiv:2107.04140*,
    2021.

    - <span id="page-34-15"></span>[17] A. Sebastian, M. Le Gallo, R. Khaddam-Aljameh,
    and E. Eleftheriou, "Memory devices and applications for in-memory computing,"
    *Nature Nanotechnology*, pp. 1–16, 2020.

    - <span id="page-34-16"></span>[18] F. Ottati, G. Turvani, G. Masera, and M. Vacca,
    "Custom memory design for logic-in-memory: Drawbacks and improvements over conventional
    memories," *Electronics*, vol. 10, no. 18, p. 2291, 2021.

    - <span id="page-34-17"></span>[19] S. Kvatinsky, D. Belousov, S. Liman, G. Satat,
    N. Wald, E. G. Friedman, A. Kolodny, and U. C. Weiser, "Magic—memristor-aided
    logic," *IEEE Transactions on Circuits and Systems II: Express Briefs*, vol. 61,
    no. 11, pp. 895–899, 2014.

    - <span id="page-34-18"></span>[20] E. Lehtonen, J. Poikonen, and M. Laiho, *Memristive
    Stateful Logic*, 01 2014, pp. 603–623.

    - <span id="page-34-19"></span>[21] P. Chi et al., "Prime: A novel processing-in-memory
    architecture for neural network computation in reram-based main memory," in *ACM/IEEE
    43rd Annual International Symposium on Computer Architecture (ISCA)*, June 2016,
    pp. 27–39.

    - <span id="page-34-20"></span>[22] A. Shafiee et al., "Isaac: A convolutional
    neural network accelerator with in-situ analog arithmetic in crossbars," *ACM
    SIGARCH Computer Architecture News*, vol. 44, no. 3, pp. 14–26, 2016.

    - <span id="page-34-21"></span>[23] S. Li, C. Xu, Q. Zou, J. Zhao, Y. Lu, and
    Y. Xie, "Pinatubo: A processing-in-memory architecture for bulk bitwise operations
    in emerging non-volatile memories," in *Proceedings of the 53rd Annual Design
    Automation Conference*, 2016, pp. 1–6.

    - <span id="page-34-22"></span>[24] W. A. Simon, Y. M. Qureshi, M. Rios, A. Levisse,
    M. Zapater, and D. Atienza, "Blade: An in-cache computing architecture for edge
    devices," *IEEE Transactions on Computers*, vol. 69, no. 9, pp. 1349–1363, 2020.

    - <span id="page-35-0"></span>[25] V. Seshadri, D. Lee, T. Mullins, H. Hassan,
    A. Boroumand, J. Kim, M. A. Kozuch, O. Mutlu, P. B. Gibbons, and T. C. Mowry,
    "Ambit: In-memory accelerator for bulk bitwise operations using commodity dram
    technology," in *Proceedings of the 50th Annual IEEE/ACM International Symposium
    on Microarchitecture*, 2017, pp. 273–287.

    - <span id="page-35-1"></span>[26] D. Fakhry, M. Abdelsalam, M. W. El-Kharashi,
    and M. Safar, "A review on computational storage devices and near memory computing
    for high performance applications," *Memories-Materials, Devices, Circuits and
    Systems*, p. 100051, 2023.

    - <span id="page-35-2"></span>[27] G. Singh, L. Chelini, S. Corda, A. J. Awan,
    S. Stuijk, R. Jordans, H. Corporaal, and A.-J. Boonstra, "Near-memory computing:
    Past, present, and future," *Microprocessors and Microsystems*, vol. 71, p. 102868,
    2019. [Online]. Available:<https://www.sciencedirect.com/science/article/pii/S0141933119300389>

    - <span id="page-35-3"></span>[28] A. Gebregiorgis, H. A. Du Nguyen, J. Yu, R.
    Bishnoi, M. Taouil, F. Catthoor, and S. Hamdioui, "A survey on memory-centric
    computer architectures," *J. Emerg. Technol. Comput. Syst.*, vol. 18, no. 4, oct
    2022. [Online]. Available: <https://doi.org/10.1145/3544974>

    - <span id="page-35-4"></span>[29] F. Gao, G. Tziantzioulis, and D. Wentzlaff,
    "Computedram: In-memory compute using off-the-shelf drams," in *Proceedings of
    the 52nd annual IEEE/ACM international symposium on microarchitecture*, 2019,
    pp. 100–113.

    - <span id="page-35-5"></span>[30] M. Ali, A. Jaiswal, S. Kodge, A. Agrawal, I.
    Chakraborty, and K. Roy, "Imac: In-memory multi-bit multiplication and accumulation
    in 6t sram array," *IEEE Transactions on Circuits and Systems I: Regular Papers*,
    vol. 67, no. 8, pp. 2521–2531, 2020.

    - <span id="page-35-6"></span>[31] Z.-R. Wang, Y.-T. Su, Y. Li, Y.-X. Zhou, T.-J.
    Chu, K.-C. Chang, T.-C. Chang, T.-M. Tsai, S. M. Sze, and X.-S. Miao, "Functionally
    complete boolean logic in 1t1r resistive random access memory," *IEEE Electron
    Device Letters*, vol. 38, no. 2, pp. 179–182, 2016.

    - <span id="page-35-7"></span>[32] A. Kazemi, F. Müller, M. M. Sharifi, H. Errahmouni,
    G. Gerlach, T. Kämpfe, M. Imani, X. S. Hu, and M. Niemier, *Scientific reports*,
    vol. 12, no. 1, p. 19201, 2022.

    - <span id="page-35-8"></span>[33] R. Neale, D. Nelson, and G. E. Moore, "Nonvolatile
    and reprogrammable, the read-mostly memory is here," *Electronics*, vol. 43, no.
    20, pp. 56–60, 1970.

    - <span id="page-35-9"></span>[34] G. W. Burr, M. J. Brightsky, A. Sebastian,
    H.-Y. Cheng, J.-Y. Wu, S. Kim, N. E. Sosa, N. Papandreou, H.-L. Lung, H. Pozidis
    *et al.*, "Recent progress in phase-change memory technology," *IEEE Journal on
    Emerging and Selected Topics in Circuits and Systems*, vol. 6, no. 2, pp. 146–162,
    2016.

    - <span id="page-35-10"></span>[35] Z. Guo, J. Yin, Y. Bai, D. Zhu, K. Shi, G.
    Wang, K. Cao, and W. Zhao, "Spintronics for energy-efficient computing: An overview
    and outlook," *Proceedings of the IEEE*, vol. 109, no. 8, pp. 1398–1417, 2021.

    - <span id="page-35-11"></span>[36] A. Kent and D. Worledge, "A new spin on magnetic
    memories," *Nature nanotechnology*, vol. 10, pp. 187–91, 03 2015.

    - <span id="page-35-13"></span>[37] D. Reis, M. Niemier, and X. S. Hu, "Computing
    in memory with fefets," in *Proceedings of the international symposium on low
    power electronics and design*, 2018, pp. 1–6.

    - <span id="page-35-14"></span>[38] J. D. Kendall and S. Kumar, "The building
    blocks of a brain-inspired computer," *Applied Physics Reviews*, vol. 7, no. 1,
    2020.

    - <span id="page-35-12"></span>[39] V. Milo, G. Malavena, C. Compagnoni, and D.
    Ielmini, "Memristive and cmos devices for neuromorphic computing," *Materials*,
    vol. 13, p. 166, 01 2020.

    - <span id="page-35-15"></span>[40] D. Patterson, K. Asanovic, A. Brown, R. Fromm,
    J. Golbus, B. Gribstad, K. Keeton, C. Kozyrakis, D. Martin, S. Perissakis, R.
    Thomas, N. Treuhaft, and K. Yelick, "Intelligent ram (iram): the industrial setting,
    applications, and architectures," in *Proceedings International Conference on
    Computer Design VLSI in Computers and Processors*, 1997, pp. 2–7.

    - <span id="page-35-16"></span>[41] J. Draper, J. Chame, M. Hall, C. Steele, T.
    Barrett, J. LaCoss, J. Granacki, J. Shin, C. Chen, C. W. Kang *et al.*, "The architecture
    of the diva processing-in-memory chip," in *Proceedings of the 16th international
    conference on Supercomputing*, 2002, pp. 14–25.

    - <span id="page-35-17"></span>[42] Y. Kang, W. Huang, S.-M. Yoo, D. Keen, Z.
    Ge, V. Lam, P. Pattnaik, and J. Torrellas, "Flexram: Toward an advanced intelligent
    memory system," in *2012 IEEE 30th International Conference on Computer Design
    (ICCD)*. IEEE, 2012, pp. 5–14.

    - <span id="page-35-18"></span>[43] J. Gómez-Luna, I. E. Hajj, I. Fernandez, C.
    Giannoula, G. F. Oliveira, and O. Mutlu, "Benchmarking a new paradigm: An experimental
    analysis of a real processing-in-memory architecture," *arXiv preprint arXiv:2105.03814*,
    2021.

    - <span id="page-35-19"></span>[44] H. Shin, D. Kim, E. Park, S. Park, Y. Park,
    and S. Yoo, "Mcdram: Low latency and energy-efficient matrix computations in dram,"
    *IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems*,
    vol. 37, no. 11, pp. 2613–2622, 2018.

    - <span id="page-35-20"></span>[45] B. Kim, J. Chung, E. Lee, W. Jung, S. Lee,
    J. Choi, J. Park, M. Wi, S. Lee, and J. H. Ahn, "Mvid: Sparse matrix-vector multiplication
    in mobile dram for accelerating recurrent neural networks," *IEEE Transactions
    on Computers*, vol. 69, no. 7, pp. 955–967, 2020.

    - <span id="page-35-21"></span>[46] L. Ke, X. Zhang, J. So, J.-G. Lee, S.-H. Kang,
    S. Lee, S. Han, Y. Cho, J. H. Kim, Y. Kwon *et al.*, "Near-memory processing in
    action: Accelerating personalized recommendation with axdimm," *IEEE Micro*, vol.
    42, no. 1, pp. 116–127, 2021.

    - <span id="page-36-0"></span>[47] A. Ankit et al., "Puma: A programmable ultra-efficient
    memristor-based accelerator for machine learning inference," in *Proceedings of
    the Twenty-Fourth International Conference on Architectural Support for Programming
    Languages and Operating Systems*, 2019, pp. 715–731.

    - <span id="page-36-1"></span>[48] A. Yazdanbakhsh, C. Song, J. Sacks, P. Lotfi-Kamran,
    H. Esmaeilzadeh, and N. S. Kim, "In-dram near-data approximate acceleration for
    gpus," in *Proceedings of the 27th International Conference on Parallel Architectures
    and Compilation Techniques*, 2018, pp. 1–14.

    - <span id="page-36-2"></span>[49] J. Ahn, S. Hong, S. Yoo, O. Mutlu, and K. Choi,
    "A scalable processing-in-memory accelerator for parallel graph processing," in
    *Proceedings of the 42nd Annual International Symposium on Computer Architecture*,
    2015, pp. 105–117.

    - <span id="page-36-3"></span>[50] D. Zhang, N. Jayasena, A. Lyashevsky, J. L.
    Greathouse, L. Xu, and M. Ignatowski, "Top-pim: Throughput-oriented programmable
    processing in memory," in *Proceedings of the 23rd international symposium on
    High-performance parallel and distributed computing*, 2014, pp. 85–98.

    - <span id="page-36-4"></span>[51] R. Nair, S. F. Antao, C. Bertolli, P. Bose,
    J. R. Brunheroto, T. Chen, C.-Y. Cher, C. H. Costa, J. Doi, C. Evangelinos *et
    al.*, "Active memory cube: A processing-in-memory architecture for exascale systems,"
    *IBM Journal of Research and Development*, vol. 59, no. 2/3, pp. 17–1, 2015.

    - <span id="page-36-5"></span>[52] M. Gao and C. Kozyrakis, "Hrl: Efficient and
    flexible reconfigurable logic for near-data processing," in *2016 IEEE International
    Symposium on High Performance Computer Architecture (HPCA)*. Ieee, 2016, pp. 126–137.

    - <span id="page-36-6"></span>[53] D. Reis, A. F. Laguna, M. Niemier, and X. S.
    Hu, "In-memory computing accelerators for emerging learning paradigms," in *Proceedings
    of the 28th Asia and South Pacific Design Automation Conference*, 2023, pp. 606–611.

    - <span id="page-36-7"></span>[54] L. Xie, H. A. Du Nguyen, J. Yu, A. Kaichouhi,
    M. Taouil, M. AlFailakawi, and S. Hamdioui, "Scouting logic: A novel memristor-based
    logic design for resistive computing," in *2017 IEEE Computer Society Annual Symposium
    on VLSI (ISVLSI)*. IEEE, 2017, pp. 176–181.

    - <span id="page-36-8"></span>[55] A. Ankit, I. El Hajj, S. R. Chalamalasetti,
    S. Agarwal, M. Marinella, M. Foltin, J. P. Strachan, D. Milojicic, W.-M. Hwu,
    and K. Roy, "Panther: A programmable architecture for neural network training
    harnessing energy-efficient reram," *IEEE Transactions on Computers*, vol. 69,
    no. 8, pp. 1128–1142, 2020.

    - <span id="page-36-9"></span>[56] L. Song et al., "Pipelayer: A pipelined reram-based
    accelerator for deep learning," in *IEEE International Symposium on High Performance
    Computer Architecture (HPCA)*, Feb 2017, pp. 541–552.

    - <span id="page-36-10"></span>[57] X. Qiao, X. Cao, H. Yang, L. Song, and H.
    Li, "Atomlayer: A universal reram-based cnn accelerator with atomic layer computation,"
    in *Proceedings of the 55th Annual Design Automation Conference*, 2018, pp. 1–6.

    - <span id="page-36-11"></span>[58] P. Chen, M. Wu, Y. Ma, L. Ye, and R. Huang,
    "Rimac: An array-level adc/dac-free reram-based in-memory dnn processor with analog
    cache and computation," in *Proceedings of the 28th Asia and South Pacific Design
    Automation Conference*, 2023, pp. 228–233.

    - <span id="page-36-12"></span>[59] G. Yuan, P. Behnam, Z. Li, A. Shafiee, S.
    Lin, X. Ma, H. Liu, X. Qian, M. N. Bojnordi, Y. Wang *et al.*, "Forms: Fine-grained
    polarized reram-based in-situ computation for mixed-signal dnn accelerator," in
    *2021 ACM/IEEE 48th Annual International Symposium on Computer Architecture (ISCA)*.
    IEEE, 2021, pp. 265–278.

    - <span id="page-36-13"></span>[60] Z. Wang, S. Joshi, S. Savel''Ev, W. Song,
    R. Midya, Y. Li, M. Rao, P. Yan, S. Asapu, Y. Zhuo *et al.*, "Fully memristive
    neural networks for pattern classification with unsupervised learning," *Nature
    Electronics*, vol. 1, no. 2, pp. 137–145, 2018.

    - <span id="page-36-14"></span>[61] W.-H. Chen, K.-X. Li, W.-Y. Lin, K.-H. Hsu,
    P.-Y. Li, C.-H. Yang, C.-X. Xue, E.-Y. Yang, Y.-K. Chen, Y.-S. Chang *et al.*,
    "A 65nm 1mb nonvolatile computing-in-memory reram macro with sub-16ns multiply-and-accumulate
    for binary dnn ai edge processors," in *2018 IEEE International Solid-State Circuits
    Conference-(ISSCC)*. IEEE, 2018, pp. 494–496.

    - <span id="page-36-15"></span>[62] C.-X. Xue, W.-H. Chen, J.-S. Liu, J.-F. Li,
    W.-Y. Lin, W.-E. Lin, J.-H. Wang, W.-C. Wei, T.-W. Chang, T.-C. Chang *et al.*,
    "24.1 a 1mb multibit reram computing-in-memory macro with 14.6 ns parallel mac
    computing time for cnn based ai edge processors," in *2019 IEEE International
    Solid-State Circuits Conference-(ISSCC)*. IEEE, 2019, pp. 388–390.

    - <span id="page-36-16"></span>[63] Q. Liu, B. Gao, P. Yao, D. Wu, J. Chen, Y.
    Pang, W. Zhang, Y. Liao, C.-X. Xue, W.-H. Chen *et al.*, "33.2 a fully integrated
    analog reram based 78.4 tops/w compute-in-memory chip with fully parallel mac
    computing," in *2020 IEEE International Solid-State Circuits Conference-(ISSCC)*.
    IEEE, 2020, pp. 500–502.

    - <span id="page-36-17"></span>[64] C. Eckert, X. Wang, J. Wang, A. Subramaniyan,
    R. Iyer, D. Sylvester, D. Blaaauw, and R. Das, "Neural cache: Bit-serial in-cache
    acceleration of deep neural networks," in *2018 ACM/IEEE 45Th annual international
    symposium on computer architecture (ISCA)*. IEEE, 2018, pp. 383–396.

    - <span id="page-36-18"></span>[65] M. Kang, S. K. Gonugondla, S. Lim, and N.
    R. Shanbhag, "A 19.4-nj/decision, 364-k decisions/s, in-memory random forest multi-class
    inference accelerator," *IEEE Journal of Solid-State Circuits*, vol. 53, no. 7,
    pp. 2126–2135, 2018.

    - [66] H. Valavi, P. J. Ramadge, E. Nestler, and N. Verma, "A 64-tile 2.4-mb in-memory-computing
    cnn accelerator employing charge-domain compute," *IEEE Journal of Solid-State
    Circuits*, vol. 54, no. 6, pp. 1789–1799, 2019.

    - [67] A. Biswas and A. P. Chandrakasan, "Conv-sram: An energy-efficient sram
    with in-memory dot-product computation for low-power convolutional neural networks,"
    *IEEE Journal of Solid-State Circuits*, vol. 54, no. 1, pp. 217–230, 2018.

    - <span id="page-36-19"></span>[68] S. Yin, Z. Jiang, J.-S. Seo, and M. Seok,
    "Xnor-sram: In-memory computing sram macro for binary/ternary deep neural networks,"
    *IEEE Journal of Solid-State Circuits*, vol. 55, no. 6, pp. 1733–1743, 2020.

    - <span id="page-37-0"></span>[69] S. Angizi, Z. He, A. S. Rakin, and D. Fan,
    "Cmp-pim: an energy-efficient comparator-based processing-in-memory neural network
    accelerator," in *Proceedings of the 55th Annual Design Automation Conference*,
    2018, pp. 1–6.

    - <span id="page-37-1"></span>[70] J. Doevenspeck, K. Garello, B. Verhoef, R.
    Degraeve, S. Van Beek, D. Crotti, F. Yasin, S. Couet, G. Jayakumar, I. Papistas
    *et al.*, "Sot-mram based analog in-memory computing for dnn inference," in *2020
    IEEE Symposium on VLSI Technology*. IEEE, 2020, pp. 1–2.

    - <span id="page-37-2"></span>[71] L. Chang, X. Ma, Z. Wang, Y. Zhang, Y. Xie,
    and W. Zhao, "Pxnor-bnn: In/with spin-orbit torque mram preset-xnor operation-based
    binary neural networks," *IEEE Transactions on Very Large Scale Integration (VLSI)
    Systems*, vol. 27, no. 11, pp. 2668–2679, 2019.

    - <span id="page-37-3"></span>[72] S. Jung, H. Lee, S. Myung, H. Kim, S. K. Yoon,
    S.-W. Kwon, Y. Ju, M. Kim, W. Yi, S. Han *et al.*, "A crossbar array of magnetoresistive
    memory devices for in-memory computing," *Nature*, vol. 601, no. 7892, pp. 211–216,
    2022.

    - <span id="page-37-4"></span>[73] E. Breyer, H. Mulaosmanovic, T. Mikolajick,
    and S. Slesazeck, "Reconfigurable nand/nor logic gates in 28 nm hkmg and 22 nm
    fd-soi fefet technology," in *2017 IEEE International Electron Devices Meeting
    (IEDM)*. IEEE, 2017, pp. 28–5.

    - <span id="page-37-5"></span>[74] X. Yin, C. Li, Q. Huang, L. Zhang, M. Niemier,
    X. S. Hu, C. Zhuo, and K. Ni, "Fecam: A universal compact digital and analog content
    addressable memory using ferroelectric," *IEEE Transactions on Electron Devices*,
    vol. 67, no. 7, pp. 2785–2792, 2020.

    - <span id="page-37-6"></span>[75] A. Kazemi, M. M. Sharifi, A. F. Laguna, F.
    Müller, R. Rajaei, R. Olivo, T. Kämpfe, M. Niemier, and X. S. Hu, "In-memory nearest
    neighbor search with fefet multi-bit content-addressable memories," in *2021 Design,
    Automation & Test in Europe Conference & Exhibition (DATE)*. IEEE, 2021, pp. 1084–1089.

    - <span id="page-37-7"></span>[76] R. Khaddam-Aljameh, M. Stanisavljevic, J. F.
    Mas, G. Karunaratne, M. Braendli, F. Liu, A. Singh, S. M. Müller, U. Egger, A.
    Petropoulos *et al.*, "Hermes core–a 14nm cmos and pcm-based in-memory compute
    core using an array of 300ps/lsb linearized cco-based adcs and local digital processing,"
    in *2021 Symposium on VLSI Circuits*. IEEE, 2021, pp. 1–2.

    - <span id="page-37-8"></span>[77] M. Le Gallo, R. Khaddam-Aljameh, M. Stanisavljevic,
    A. Vasilopoulos, B. Kersting, M. Dazzi, G. Karunaratne, M. Brändli, A. Singh,
    S. M. Mueller *et al.*, "A 64-core mixed-signal in-memory compute chip based on
    phase-change memory for deep neural network inference," *Nature Electronics*,
    pp. 1–14, 2023.

    - <span id="page-37-9"></span>[78] Q. Dong, M. E. Sinangil, B. Erbagci, D. Sun,
    W.-S. Khwa, H.-J. Liao, Y. Wang, and J. Chang, "15.3 a 351tops/w and 372.4 gops
    compute-in-memory sram macro in 7nm finfet cmos for machine-learning applications,"
    in *2020 IEEE International Solid-State Circuits Conference-(ISSCC)*. IEEE, 2020,
    pp. 242–244.

    - <span id="page-37-10"></span>[79] H. Wang, R. Liu, R. Dorrance, D. Dasalukunte,
    D. Lake, and B. Carlton, "A charge domain sram compute-in-memory macro with c-2c
    ladder-based 8-bit mac unit in 22-nm finfet process for edge inference," *IEEE
    Journal of Solid-State Circuits*, vol. 58, no. 4, pp. 1037–1050, 2023.

    - <span id="page-37-11"></span>[80] S. De, F. Mueller, N. Laleni, M. Lederer,
    Y. Raffel, S. Mojumder, A. Vardar, S. Abdulazhanov, T. Ali, S. Dünkel *et al.*,
    "Demonstration of multiply-accumulate operation with 28 nm fefet crossbar array,"
    *IEEE Electron Device Letters*, vol. 43, no. 12, pp. 2081–2084, 2022.

    - <span id="page-37-12"></span>[81] G. Pedretti, C. E. Graves, S. Serebryakov,
    R. Mao, X. Sheng, M. Foltin, C. Li, and J. P. Strachan, "Tree-based machine learning
    performed in-memory with memristive analog cam," *Nature communications*, vol.
    12, no. 1, p. 5806, 2021.

    - <span id="page-37-13"></span>[82] K. Akarvardar and H.-S. P. Wong, "Technology
    prospects for data-intensive computing," *Proceedings of the IEEE*, vol. 111,
    no. 1, pp. 92–112, 2023.

    - <span id="page-37-14"></span>[83] C. Zhang, H. Sun, S. Li, Y. Wang, H. Chen,
    and H. Liu, "A survey of memory-centric energy efficient computer architecture,"
    *IEEE Transactions on Parallel and Distributed Systems*, 2023.

    - <span id="page-37-16"></span><span id="page-37-15"></span>[84] "Axelera," [https://www.axelera.ai/digital-in-memory-computing-for-deep-learning-acceleration/.](https://www.axelera.ai/digital-in-memory-computing-for-deep-learning-acceleration/)

    - [85] "d-matrix," [https://www.d-matrix.ai/.](https://www.d-matrix.ai/)

    - <span id="page-37-17"></span>[86] "Gyrfalcon tech," [https://www.gyrfalcontech.ai/about-us/company-overview/.](https://www.gyrfalcontech.ai/about-us/company-overview/)

    - <span id="page-37-18"></span>[87] "Memcpu," [https://www.memcpu.com/.](https://www.memcpu.com/)

    - <span id="page-37-19"></span>[88] "Memverge," [https://memverge.com/company/.](https://memverge.com/company/)

    - <span id="page-37-20"></span>[89] "mythic," [https://mythic.ai/.](https://mythic.ai/)

    - <span id="page-37-21"></span>[90] "Neuroblade," [https://www.neuroblade.com/product/.](https://www.neuroblade.com/product/)

    - <span id="page-37-22"></span>[91] "Rain," [https://rain.ai/about-us/.](https://rain.ai/about-us/)

    - <span id="page-37-23"></span>[92] "Semron," [https://www.semron.ai.](https://www.semron.ai)

    - <span id="page-37-24"></span>[93] "Surecore," [https://www.sure-core.com.](https://www.sure-core.com)

    - <span id="page-37-25"></span>[94] "Synthara," [https://www.synthara.ai.](https://www.synthara.ai)

    - <span id="page-37-26"></span>[95] "Syntiant," [https://www.syntiant.com/.](https://www.syntiant.com/)

    - <span id="page-37-27"></span>[96] "Tetramem," [https://www.tetramem.com.](https://www.tetramem.com)

    - <span id="page-37-28"></span>[97] M. Rao, H. Tang, J. Wu, W. Song, M. Zhang,
    W. Yin, Y. Zhuo, F. Kiani, B. Chen, X. Jiang *et al.*, "Thousands of conductance
    levels in memristors integrated on cmos," *Nature*, vol. 615, no. 7954, pp. 823–829,
    2023.

    - <span id="page-37-29"></span>[98] "Encharge ai," [https://enchargeai.com.](https://enchargeai.com)

    - <span id="page-38-0"></span>[99] "Reconceive," [https://www.re-conceive.com/home.](https://www.re-conceive.com/home)

    - <span id="page-38-1"></span>[100] "Fractile," [https://www.fractile.ai/.](https://www.fractile.ai/)

    - <span id="page-38-2"></span>[101] "Untether," [https://www.untether.ai/.](https://www.untether.ai/)

    - <span id="page-38-3"></span>[102] A. Siemieniuk, L. Chelini, A. A. Khan, J.
    Castrillon, A. Drebes, H. Corporaal, T. Grosser, and M. Kong, "OCC: An automated
    end-to-end machine learning optimizing compiler for computing-in-memory," *IEEE
    Transactions on Computer-Aided Design of Integrated Circuits and Systems (TCAD)*,
    vol. 41, no. 6, pp. 1674–1686, Aug. 2021. [Online]. Available:<https://ieeexplore.ieee.org/document/9502921>

    - [103] A. A. Khan, H. Farzaneh, K. F. Friebel, L. Chelini, and J. Castrillon,
    "Cinm (cinnamon): A compilation infrastructure for heterogeneous compute in-memory
    and compute near-memory paradigms," *arXiv preprint arXiv:2301.07486*, 2022.

    - [104] H. Farzaneh, J. P. C. de Lima, M. Li, A. A. Khan, X. S. Hu, and J. Castrillon,
    "C4cam: A compiler for cam-based in-memory accelerators," *arXiv preprint arXiv:2309.06418*,
    2023.

    - <span id="page-38-4"></span>[105] J. P. C. de Lima, A. A. Khan, H. Farzaneh,
    and J. Castrillon, "Full-stack optimization for cam-only dnn inference," in *Proceedings
    of the 2024 Design, Automation and Test in Europe Conference (DATE)*, ser. DATE''24.
    IEEE, Mar. 2024, pp. 1–6.

    - <span id="page-38-5"></span>[106] J. Ryckaert, M. Niemier, Z. Enciso, M. M.
    Sharifi, X. S. Hu, I. O''Connor, A. Graening, R. Sharma, P. Gupta, J. Castrillon,
    J. P. C. de Lima, A. A. Khan, and H. Farzaneh, "Smoothing disruption across the
    stack: Tales of memory, heterogeneity, and compilers," in *Proceedings of the
    2024 Design, Automation and Test in Europe Conference (DATE)*, ser. DATE''24.
    IEEE, Mar. 2024, pp. 1–6.


    # **ACRONYMS**


    <span id="page-39-22"></span><span id="page-39-21"></span><span id="page-39-20"></span><span
    id="page-39-19"></span><span id="page-39-18"></span><span id="page-39-17"></span><span
    id="page-39-16"></span><span id="page-39-15"></span><span id="page-39-14"></span><span
    id="page-39-13"></span><span id="page-39-12"></span><span id="page-39-11"></span><span
    id="page-39-10"></span><span id="page-39-9"></span><span id="page-39-8"></span><span
    id="page-39-7"></span><span id="page-39-6"></span><span id="page-39-5"></span><span
    id="page-39-4"></span><span id="page-39-3"></span><span id="page-39-2"></span><span
    id="page-39-1"></span><span id="page-39-0"></span>


    | CIM<br>compute-in-memory             | 1  |

    |--------------------------------------|----|

    | IMC<br>in-memory-computing           | 4  |

    | IMP<br>in-memory-processing          | 4  |

    | LIM<br>logic-in-memory               | 4  |

    | PIM<br>processing-in-memory          | 4  |

    | PUM<br>processing-using-memory       | 4  |

    | CNM<br>compute-near-memory           | 4  |

    | NMC<br>near-memory-computing         | 4  |

    | PNM<br>processing-near-memory        | 4  |

    | NMP<br>near-memory-processing        | 4  |

    | ADC<br>analog-to-digital converter   | 5  |

    | APU<br>accelerated processing unit   | 16 |

    | BL<br>bitline                        | 7  |

    | BLB<br>bitline bar                   | 7  |

    | CIM-A<br>CIM-array                   | 5  |

    | CIM-P<br>CIM-peripheral              | 5  |

    | CAM<br>content-addressable-memory    | 17 |

    | CNN<br>convolutional neural network  | 17 |

    | COM<br>compute-outside-memory        | 4  |

    | DAC<br>digital-to-analog converter . | 5  |

    | DNN<br>deep neural network           | 18 |

    | DPU<br>data processing unit          | 11 |

    | DRAM<br>dynamic random-access memory | 5  |


    <span id="page-40-23"></span><span id="page-40-22"></span><span id="page-40-21"></span><span
    id="page-40-20"></span><span id="page-40-19"></span><span id="page-40-18"></span><span
    id="page-40-17"></span><span id="page-40-16"></span><span id="page-40-15"></span><span
    id="page-40-14"></span><span id="page-40-13"></span><span id="page-40-12"></span><span
    id="page-40-11"></span><span id="page-40-10"></span><span id="page-40-9"></span><span
    id="page-40-8"></span><span id="page-40-7"></span><span id="page-40-6"></span><span
    id="page-40-5"></span><span id="page-40-4"></span><span id="page-40-3"></span><span
    id="page-40-2"></span><span id="page-40-1"></span><span id="page-40-0"></span>


    | FeFET<br>ferroelectric field-effect transistor | 2  |

    |------------------------------------------------|----|

    | FPU<br>floating-point unit                     | 13 |

    | HBM<br>high bandwidth memory                   | 7  |

    | ISA<br>instruction set architecture            | 14 |

    | LUT<br>look-up table                           | 15 |

    | MAC<br>multiply-accumulate                     | 12 |

    | MLC<br>multi-level cell                        | 20 |

    | MOS<br>metal-oxide-semiconductor               | 9  |

    | MRAM<br>magnetic RAM                           | 2  |

    | MTJ<br>magnetic tunnel junction                | 8  |

    | MPSoC<br>multiprocessor system-on-chip         | 4  |

    | NN<br>neural-network                           | 20 |

    | NVM<br>non-volatile memory                     | 2  |

    | OMP<br>outside memory processing               | 26 |

    | PCM<br>phase change memory                     | 2  |

    | PU<br>processing unit                          | 11 |

    | RRAM<br>resistive RAM                          | 2  |

    | SDK<br>software development kit                | 12 |

    | SIMD<br>single-instruction multiple-data       | 13 |

    | SLC<br>single-level cell                       | 20 |

    | SoC<br>system-on-chip                          | 26 |

    | SRAM<br>static random-access memory            | 5  |

    | SOT<br>Spin-orbit-torque                       | 8  |

    | STT<br>spin-transfer-torque                    | 8  |


    <span id="page-41-6"></span><span id="page-41-5"></span><span id="page-41-4"></span><span
    id="page-41-3"></span><span id="page-41-2"></span><span id="page-41-1"></span><span
    id="page-41-0"></span>


    | TRA<br>triple row activation        | 21 |

    |-------------------------------------|----|

    | TSVs<br>through-silicon vias        | 7  |

    | WL<br>word line                     | 7  |

    | CAGR<br>compound annual growth rate | 2  |

    | MVM<br>matrix-vector multiplication | 5  |

    | SA<br>sense amplifier               | 12 |

    | LLM<br>large language models        | 1  |'
  decisions:
    evaluation_prompt: 'Disqualified: Chunk 2: Disqualified: no evaluation. Reason:
      The provided section does not contain any form of empirical, experimental, or
      quantitative evaluation. It primarily describes various companies and their
      technologies without presenting any evaluation, results, or comparisons. | Chunk
      3: Disqualified: no evaluation. Reason: The provided section does not contain
      any empirical, experimental, or quantitative evaluation. It only describes a
      company''s claims without providing evidence or results from any evaluation.'
    related_work_prompt: 'Disqualified: Chunk 3: Disqualified: no related work. Reason:
      The section does not engage with prior research, lacks academic citations, and
      does not compare or discuss its method in the context of existing work. | Chunk
      4: Disqualified: no related work. Reason: The section provided does not include
      any meaningful engagement with prior research. It lists companies, technologies,
      and funding details without discussing or comparing these to existing research
      or methodologies. There are no academic citations or discussions of how these
      technologies relate to or build upon previous work.'
    novelty_prompt: 'Disqualified: Chunk 3: Disqualified: no novelty. Reason: The
      section does not present any new ideas, methods, applications, or results. It
      merely describes a company''s claim about its AI accelerator without providing
      specific details or evidence of novelty.'
    review_only_prompt: 'Qualified. Reason: All relevant sections passed.'
- title: Efficient yet Accurate End-to-End SC Accelerator Design
  abstract: 'Providing end-to-end stochastic computing (SC) neural network acceleration

    for state-of-the-art (SOTA) models has become an increasingly challenging task,

    requiring the pursuit of accuracy while maintaining efficiency. It also

    necessitates flexible support for different types and sizes of operations in

    models by end-to-end SC circuits. In this paper, we summarize our recent

    research on end-to-end SC neural network acceleration. We introduce an accurate

    end-to-end SC accelerator based on a deterministic coding and sorting network.

    In addition, we propose an SC-friendly model that combines low-precision data

    paths with high-precision residuals. We introduce approximate computing

    techniques to optimize SC nonlinear adders and provide some new SC designs for

    arithmetic operations required by SOTA models. Overall, our approach allows for

    further significant improvements in circuit efficiency, flexibility, and

    compatibility through circuit design and model co-optimization. The results

    demonstrate that the proposed end-to-end SC architecture achieves accurate and

    efficient neural network acceleration while flexibly accommodating model

    requirements, showcasing the potential of SC in neural network acceleration.'
  url: http://arxiv.org/abs/2401.15332v1
  keywords: ''
  document: "### I. INTRODUCTION\n\nStochastic computing (SC) has emerged as a promising\
    \ alternative to traditional binary computing, offering simplified arithmetic\
    \ operations and improved error resilience [\\[1\\]](#page-4-0)–[\\[5\\]](#page-4-1).\
    \ Both hybrid and end-to-end SC-based neural accelerators have been proposed [\\\
    [1\\]](#page-4-0)–[\\[5\\]](#page-4-1). While hybrid accelerators involve back-and-forth\
    \ conversion between binary and SC representations, leading to high power consumption\
    \ and area overhead, end-to-end SC-based accelerators demonstrate superior power,\
    \ area efficiency, and fault tolerance [\\[3\\]](#page-4-2)–[\\[5\\]](#page-4-1).\
    \ In this context, our research aims to further enhance the capabilities of endto-end\
    \ SC-based accelerators.\n\nExisting SC-based accelerators primarily focus on\
    \ multiplication, accumulation, and activation functions in convolutional networks\
    \ [\\[6\\]](#page-4-3)–[\\[9\\]](#page-4-4). However, these approaches have limitations.\
    \ FSM-based activation modules suffer from accuracy issues, particularly for ReLU\
    \ with larger accumulation widths (Figure [1\\)](#page-0-0). Furthermore, there\
    \ exists a trade-off between inference efficiency and accuracy (Figure [2\\)](#page-0-1),\
    \ where high precision computing enhances accuracy but exponentially increases\
    \ costs, while low precision computing compromises accuracy. Additionally, there\
    \ is a lack of research on SC circuits supporting functions like batch normalization\
    \ (BN), residual connections, gaussian error linear unit (GELU), and softmax for\
    \ state-of-the-art (SOTA) models.\n\nTherefore, in this paper, we will summarize\
    \ our recent efforts on end-to-end SC-based NN accelerators that address these\
    \ limitations to meet the requirements in terms of accuracy, efficiency, flexibility,\
    \ and compatibility, as shown in Table [I.](#page-0-2)\n\n## <span id=\"page-0-3\"\
    ></span>II. ACCURATE END-TO-END SC ACCELERATOR BASED ON DETERMINISTIC THERMOMETER\
    \ CODING\n\nIn this section, we introduce deterministic coding called thermometer\
    \ coding and the corresponding end-to-end SC accelerator design. The proposed\
    \ design achieves exact end-to-end SC NN acceleration.\n\nThis work was supported\
    \ in part by the National Key R&D Program of China (2020YFB2205502), NSFC (62125401)\
    \ and the 111 Project (B18001).\n\n<sup>∗</sup>Corresponding author: {meng.li,\
    \ r.wang}@pku.edu.cn\n\n![](_page_0_Figure_14.jpeg)\n\n<span id=\"page-0-0\"></span>Fig.\
    \ 1. FSM-based design to implement (a) tanh and (b) ReLU. Ideally, the circuit\
    \ output is the same as the exact output, marked by the red line.\n\n<span id=\"\
    page-0-1\"></span>![](_page_0_Figure_16.jpeg)\n\nFig. 2. The trade-off between\
    \ inference accuracy and efficiency (measured by area-delay product, i.e., ADP).\
    \ Here, we fix the weight BSL to 2-bit and sweep the activation BSL.\n\n<span\
    \ id=\"page-0-2\"></span>TABLE I COMPARISON OF DIFFERENT END-TO-END SC ACCELERATORS.\n\
    \n| Design            |      | Accuracy Efficiency | Flexibility             \
    \             | *Compatibility |\n|-------------------|------|---------------------|--------------------------------------|----------------|\n\
    | FSM-based [6]–[9] | Low  | Low                 | Limited for large Conv    \
    \           | Basic CNNs     |\n| Ours [3]–[5]      | High | Low             \
    \    | Limited for variable Conv Basic CNNs |                |\n| Ours [10], [11]\
    \   | High | High                | Flexible                             | DNNs\
    \           |\n| Ours [12]         | High | High                | Flexible   \
    \                          | DNNs+ViT       |\n\n\\*Basic CNNs contain convolution\
    \ and ReLU. DNNs further require residual connection and BN. And transformer models\
    \ further require GeLU and softmax.\n\n### *A. Motivation*\n\nWe refer to the\
    \ accumulation and activation module as the SC non-linear adder. Typical SC Non-linear\
    \ adders employ stochastic coding with FSM to implement different activation functions\
    \ [\\[6\\]](#page-4-3)– [\\[9\\]](#page-4-4). FSM-based designs serially process\
    \ stochastic bitstream inputs, which results in inaccurate outputs (Figure [1\\\
    )](#page-0-0) that do not utilize all of the information in the inputs and have\
    \ random fluctuations in the inputs themselves. Thus, very long bitstreams, e.g.,\
    \ 1024 bits, are used for accuracy and lead to an unacceptable latency, which\
    \ severely affects the hardware efficiency.\n\n### <span id=\"page-0-4\"></span>*B.\
    \ Accurate End-to-End SC Acceleration with Sorting Network*\n\nIn our work, we\
    \ employ the deterministic thermometer coding scheme (Table [II\\)](#page-1-0)\
    \ and the corresponding accurate SC circuit designs to achieve accurate end-to-end\
    \ SC acceleration. With thermometer coding, all the 1s appear at the beginning\
    \ of the bitstream and\n\n![](_page_1_Figure_0.jpeg)\n\n<span id=\"page-1-1\"\
    ></span>Fig. 3. (a) The truth table and circuit of ternary SC multiplier. (b)\
    \ The BSN and the selective interconnect system for accumulation and activation\
    \ function.\n\n<span id=\"page-1-0\"></span>TABLE II THE CORRESPONDING BINARY\
    \ PRECISION AND THE REPRESENTED RANGE FOR THERMOMETER CODING OF DIFFERENT BSL.\n\
    \n| BSL | Binary<br>Precision | Range             | Thermometer Coding       \
    \                                                      |\n|-----|---------------------|-------------------|--------------------------------------------------------------------------------|\n\
    | 2   | -                   | -1, 0, 1          | 00, 10, 11                 \
    \                                                    |\n| 4   | 2            \
    \       | -2, -1, 0, 1, 2   | 0000, 1000, 1100, 1110, 1111                   \
    \                                |\n| 8   | 3                   | -4, -3 · · ·\
    \ 3, 4 | 00000000, 10000000 · · · 11111110, 11111111                         \
    \           |\n| 16  | 4                   | -8, -7 · · · 7, 8 | 0000000000000000,\
    \ 1000000000000000 · · ·<br>1111111111111110, 1111111111111111 |\n\na value x\
    \ is represented with a L-bit sequence as x = αx<sup>q</sup> = α( P<sup>L</sup>−<sup>1</sup>\
    \ <sup>i</sup>=0 x[i] − L 2 ), where x<sup>q</sup> = P<sup>L</sup>−<sup>1</sup>\
    \ <sup>i</sup>=0 x[i] − L 2 is the quantized value of range [− L , L ] and α is\
    \ a scaling factor obtained by training.\n\n2 2 Deterministic coding, in contrast\
    \ to stochastic coding, achieves hardware-efficient and accurate computations\
    \ with shorter bitstreams. By employing a 2-bit ternary bitstream, we can realize\
    \ multiplication with only 5 gates using a deterministic multiplier (Figure [3\\\
    (](#page-1-1)a)).\n\nTo achieve accurate accumulation and activation functions\
    \ simultaneously, we employ the bitonic sorting network (BSN). BSN is a parallel\
    \ sorting network that sorts inputs in thermometer coding, ensuring the output\
    \ is also in thermometer coding. The sorting process, performed by comparators\
    \ constructed with AND and OR gates, follows Batcher's bitonic sorting algorithm\
    \ [\\[13\\]](#page-4-8) (Figure [3\\(](#page-1-1)b)). The number of 1's in the\
    \ sorted bitstream output from BSN corresponds to the sum of 1's in all input\
    \ bitstreams, effectively representing the accumulation result.\n\nBy sorting\
    \ all the bits, the inputs and outputs of the selective interconnect (SI) [\\\
    [14\\]](#page-4-9) are deterministic. Therefore, when the SI selects different\
    \ bits from the BSN directly as outputs based on the selection signals, a deterministic\
    \ input-output correspondence is generated and different activation functions\
    \ are realized. The example in Figure [3\\(](#page-1-1)b) implements the two-step\
    \ activation function shown at the bottom when the SI selects the 3rd and 6th\
    \ bits of the BSN as outputs. We refer interested readers to [\\[3\\]](#page-4-2),\
    \ [\\[4\\]](#page-4-10) for more details.\n\n### *C. Experimental Results*\n\n\
    We prototype the proposed SC accelerator with a 28-nm CMOS process. The chip's\
    \ measured current consumption and energy efficiency in Figure [4](#page-1-2)\
    \ show a peak of 198.9 TOPS/W at 200 MHz and 650 mV. Compared to state-of-the-art\
    \ binary-based NN processors [\\[15\\]](#page-4-11)–[\\[19\\]](#page-4-12), the\
    \ fabricated SC-based NN processor achieves an average\n\n![](_page_1_Figure_10.jpeg)\n\
    \n<span id=\"page-1-2\"></span>Fig. 4. (a) Current and (b) energy efficiency versus\
    \ supply voltage at different working frequencies.\n\n![](_page_1_Figure_12.jpeg)\n\
    \n<span id=\"page-1-3\"></span>Fig. 5. Accuracy loss of the conventional binary\
    \ design and proposed SC design versus bit error rate, at the soft accuracy of\
    \ 98.28%.\n\nenergy efficiency improvement of 10.75× (1.16× ∼ 17.30×). And the\
    \ area efficiency improves by 4.20× (2.09× ∼ 6.76×). We also compare the accuracy\
    \ under varying bit error rates (BER) using a ternary neural network that achieves\
    \ 98.28% accuracy on the MNIST dataset, as shown in Figure [5.](#page-1-3) The\
    \ proposed SC design demonstrates significant fault tolerance, as the average\
    \ reduction of accuracy loss by 70%. It is the first silicon-proven end-to-end\
    \ SC accelerator, to the best of the authors' knowledge.\n\n## III. ACCURATE YET\
    \ EFFICIENT SC WITH HIGH PRECISION RESIDUAL FUSION\n\nThe SC accelerator above\
    \ validated the effectiveness of deterministic thermometer coding and the corresponding\
    \ SC design on the basic small model (MNIST). In this section, we propose SC-friendly\
    \ models as well as new SC circuit blocks to support SOTA model requirements and\
    \ greatly improve the accuracy of the SC accelerators.\n\n### *A. Motivation*\n\
    \nThe SC TNN accelerator in Section [II](#page-0-3) lacks support for batch normalization\
    \ (BN) and residual connections, limiting its accuracy on complex datasets like\
    \ CIFAR10 or CIFAR100. Increasing precision can enhance accuracy but compromises\
    \ hardware efficiency. Figure [2](#page-0-1) demonstrates that increasing BSL\
    \ from 2 to 8 bits improves accuracy at the expense of a 3 to 10 times efficiency\
    \ overhead. Accurate yet efficient SC acceleration is very challenging.\n\n###\
    \ *B. SC-Friendly Low Precision Network*\n\nTo understand the origin of the accuracy\
    \ degradation, we quantize the network weight and activation to low precision\
    \ separately. Table [III](#page-2-0) shows similar accuracy between low precision\
    \ weight quantization and the floating point baseline, while 2b BSL activation\
    \ quantization results in a 10% accuracy drop. Hence, low precision activation\
    \ is the root cause of the accuracy loss due to its limited representation capacity.\
    \ After quantization, the range of activations is\n\n<span id=\"page-2-0\"></span>TABLE\
    \ III NETWORK ACCURACY COMPARISON OF DIFFERENT QUANTIZED NETWORKS ON CIFAR10.\n\
    \n| Network              | Weight/BSL | Act/BSL | Top-1 Accuracy (%) |\n|----------------------|------------|---------|--------------------|\n\
    | baseline             | FP         | FP      | 94.27              |\n| weight\
    \ quantized     | 2          | FP      | 93.98              |\n| activation quantized\
    \ | FP         | 2       | 84.18              |\n| fully quantized      | 2  \
    \        | 2       | 83.51              |\n|                      |          \
    \  |         |                    |\n\n![](_page_2_Figure_2.jpeg)\n\n<span id=\"\
    page-2-1\"></span>Fig. 6. High precision residual helps to achieve better representation\
    \ capability.\n\nreduced to {−1, 0, +1} for 2b BSL encoding, significantly reducing\
    \ the number of possible configurations.\n\nAs a remedy, we add the high-precision\
    \ activation input through residual connections to the result of the low-precision\
    \ convolution (Figure [6\\)](#page-2-1). By increasing the activation range to\
    \ {−8, −7, . . . , 7, 8}, we enhance representation capacity to 17<sup>H</sup>×W×<sup>C</sup>\
    \ . This significantly improves inference accuracy while maintaining efficiency\
    \ by preserving energy-efficient convolution computation.\n\n$$\\text{ReLU}(\\\
    text{BN}(x)) = \\begin{cases} \\gamma(x-\\beta) & x \\ge \\beta \\\\ 0 & x < \\\
    beta \\end{cases} \\tag{1}$$\n\nBesides the high precision residual, another remaining\
    \ question is how to efficiently process BN. And BN(x) = γ(x − β), where γ and\
    \ β are trainable parameters. We propose to fuse BN with the ReLU activation function\
    \ as Equation [1.](#page-2-2) Consequently, we achieve an SC-friendly low precision\
    \ model with high precision residual fusion depicted in Figure [6\\(](#page-2-1)b).\n\
    \n### *C. End-to-End SC Accelerator with High Precision Residual*\n\nCompared\
    \ to the proposed accelerator in Section [II-B,](#page-0-4) the model in Figure\
    \ [6\\(](#page-2-1)b) further requires the implementation of SC circuits for BN\
    \ fusion and residual connection.\n\nThe above fused BN and ReLU function can\
    \ be efficiently and accurately processed in SC, leveraging the selective interconnect\
    \ described in Section [II-B.](#page-0-4) Figure [7](#page-2-3) demonstrates how\
    \ different BN parameters affect the objective function of the SI.\n\n![](_page_2_Figure_11.jpeg)\n\
    \n<span id=\"page-2-3\"></span>Fig. 7. BN-fused activation function with 16b BSL\
    \ output. The blue dots are\n\n![](_page_2_Figure_13.jpeg)\n\n<span id=\"page-2-4\"\
    ></span>Fig. 8. (a) The proposed model optimization helps to achieve much better\
    \ inference accuracy; (b) 16b BSL residual achieves 5.78% accuracy improvement,\
    \ almost the same as floating point residual.\n\n<span id=\"page-2-5\"></span>TABLE\
    \ IV INFERENCE EFFICIENCY AND ACCURACY COMPARISON.\n\n| W-A-R/BSL | Area (um²)\
    \ | ADP (um²·us) | Accuracy (%) |\n|-----------|------------|--------------|--------------|\n\
    | 2-2-2     | 4349.7     | 225.36       | 82.58        |\n| 2-4-4     | 10683.3\
    \    | 687.47       | 92.35        |\n| 2-2-16    | 4406.9     | 228.32      \
    \ | 92.01        |\n\nFor the accumulation of residual and multiplication products,\
    \ the different scaling factors α of residual and convolution results can lead\
    \ to errors in the accumulation operation. The residual re-scaling block is proposed\
    \ to align the α before accumulation. In the re-scaling block, we multiply or\
    \ divide the residual by a factor of 2 <sup>N</sup> (where N is an integer). To\
    \ multiply the residual by 2 <sup>N</sup> , we replicate it 2 N times in the buffer.\
    \ For division by 2 <sup>N</sup> , we select 1 out of 2 bits of the residual per\
    \ cycle and generate the final result after N cycles. To maintain a constant BSL\
    \ for the residual, we append 8 bits of '11110000' (equal to 0) per division cycle.\n\
    \n### <span id=\"page-2-2\"></span>*D. Experimental Results*\n\nFigure [8](#page-2-4)\
    \ demonstrate significant improvement in network accuracy. With the high precision\
    \ residual, network accuracy is improved significantly by 8.69% and 8.12% for\
    \ low precision ResNet18 on CIFAR10 and CIFAR100, respectively. Combined with\
    \ the novel training techniques, network accuracy can be improved in total by\
    \ 9.43% and 15.42%. Compared to baseline accelerators, it achieves a 9.4% accuracy\
    \ improvement with only a 1.3% efficiency overhead compared to the efficient baseline\
    \ and achieves a 3× efficiency improvement with comparable accuracy to the accurate\
    \ baseline design, as shown in Table [IV.](#page-2-5) In this way, the proposed\
    \ method achieves accurate yet efficient SC acceleration.\n\n## IV. FLEXIBLE AND\
    \ EFFICIENT SC ACCELERATOR WITH APPROXIMATE SPATIAL-TEMPORAL BSN\n\nIn this section,\
    \ we greatly improve the flexibility and hardware efficiency of the SC accelerator\
    \ by compressing the BSN.\n\n### *A. Motivation*\n\nBSN accumulates all the input\
    \ in parallel through sorting, so as to generate an accurate output based on all\
    \ the information input. However, it also forces the hardware cost to increase\
    \ super linearly with the accumulation widths (Figure [9\\(](#page-3-0)a)). And\
    \ the BSN has to support the largest accumulation widths among all layers. The\
    \ large BSN, however, leads to very high hardware redundancy at shallow layers\
    \ where the accumulation is always small (Figure [9\\(](#page-3-0)b)). This makes\
    \ our previous design still inefficient for SOTA models.\n\n![](_page_3_Figure_0.jpeg)\n\
    \n**Motivation**\n\n**Why End-to-End SC?**\n\n70.0 75.7 81.2\n\nAxPC +FSM\n\n\
    • Stochastic encoding\n\nMUX +FSM\n\n**FSM-based Non-Linear Adder**\n\n**SI-based\
    \ Non-Linear Adder**\n\n• Deterministic thermometer coding: all 1s appearing at\
    \ the beginning of bitstream ൌ = ሺ∑ ሾሿ ିଵ\n\n• **BSN** accumulates all the input\
    \ bitstream simultaneously through **sorting** • **SI** enables accurate and flexible\n\
    \nୀ െ /2)\n\n**sorted**\n\n• Limited accuracy: random fluctuation\n\nArea Breakdown\n\
    \nAPC +FSM\n\n22.2 15.1 7.9\n\n7.8 9.1 10.9 7.6\n\n• Removing the conversion logic\
    \ • Higher energy/area efficiency • More tolerable to the soft errors\n\n**Challenge:\
    \ Non-Linear Addition** • Non-linear Add = accum + activation • High hardware\
    \ cost: ~90% area\n\n20.4\n\nBSN +SI\n\n**Previous Works**\n\n72.0\n\nMultiplication\
    \ Accumulation Activation\n\nArea Breakdown\n\n17.0 20.4 30.9\n\n64 128 256 Accumulation\
    \ width\n\n73.0 72.0 63.7\n\n10.0 7.6 5.4\n\n<span id=\"page-3-0\"></span>Fig.\
    \ 9. The inefficiency of the BSN design: (a) BSN hardware cost increases significantly\
    \ with the accumulation widths; (b) ADP overhead using a large BSN for small accumulation\
    \ widths.\n\n![](_page_3_Figure_2.jpeg)\n\n<span id=\"page-3-1\"></span>Fig. 10.\
    \ (a) Reducing BSN output BSL has little effect on the accuracy of SI; (b) Parameterized\
    \ BSN design space.\n\n### *B. Approximate Spatial-Temporal Sorting Network*\n\
    \nTo address the inefficiency and inflexibility of BSN, we find a significant\
    \ precision gap between the input and output of SI, as revealed in Figure [6\\\
    (](#page-2-1)b), making the high precision SC input redundant. We reduce the BSN\
    \ output BSL, resulting in a small accuracy loss for the tanh function and negligible\
    \ impact on the ReLU function, as shown in Figure [10\\(](#page-3-1)a).\n\nTo\
    \ further reduce hardware cost, we adopt a progressive sorting and sub-sampling\
    \ approach for the BSN. Figure [10\\(](#page-3-1)b) presents a parameterized BSN\
    \ design space that determines the location, number of sampling times, and method\
    \ of sampling. The parameterized BSN consists of N stages and in the ith stage,\
    \ there are m<sup>i</sup> sub-BSN modules, each taking an input bitstream of li-bit\
    \ BSL. Within each sub-BSN, there is a sub-sampling block that implements truncated\
    \ quantization. It clips out c<sup>i</sup> bits on each end of the BSN while sampling\
    \ 1 bit every s<sup>i</sup> bit from the remaining. Considering the input distribution\
    \ resembles a Gaussian distribution with a small variance due to inputs from a\
    \ large number of multipliers, significant clipping can be performed with negligible\
    \ errors, as illustrated in Figure [11.](#page-3-2)\n\nThanks to the fact that\
    \ the output BSL of the approximate BSN is much shorter than the input, we can\
    \ further fold the accumulation temporally to achieve more flexibility. In this\
    \ case, as shown in Figure [12,](#page-3-3) a large BSN is implemented by multi-cycle\
    \ reuse of a single small BSN circuit. In the proposed spatial-temporal BSN architecture,\
    \ the approximation level of BSN, i.e., the BSL of partial sums, and its corresponding\
    \ reuse can be controlled through control signals. This allows for flexible handling\
    \ of various accumulation widths with different approximate configurations.\n\n\
    ### *C. Experimental Results*\n\nFor the largest convolution in the ResNet18,\
    \ the two proposed approximate BSN reduced the ADP of BSN by 2.8× and 4.1× compared\
    \ to the baseline, as shown in Table [V.](#page-3-4) When handling the four different\
    \ sizes of convolutions in ResNet18, the spatial-temporal\n\n![](_page_3_Figure_10.jpeg)\n\
    \n **Clipping** some bits from both ends of the bitstream with **truncation error\
    \ Scaling** by taking 1bit for every bits with\n\nPartial sums are **distributed\
    \ around 0:** we can\n\n**Yixuan Hu1**, Tengyu Zhang1, Meng Li\\*213, Renjie Wei1,\
    \ Liangzhen Lai5, Yuan Wang13, Runsheng Wang134, Ru Huang134 *<sup>1</sup>School\
    \ of Integrated Circuits, <sup>2</sup>Institute for Artificial Intelligence &\
    \ <sup>4</sup>Institute of Electronic Design Automation, Peking University*\n\n\
    **Observations of BSN**\n\n• **Hardware cost increase super linearly**  with the\
    \ input size (Accumulation width) • Using a single large BSN for different layers\n\
    \nincurs **very high redundancy**\n\n• Exists a **huge precision redundancy**\
    \  between the input and output for SI\n\nHardware redundancy\n\nAccumulation\
    \ width\n\nSub-sampling BSN output as SI input ReLU: sampling has almost **no\
    \ effect** on accuracy\n\n576 1152 2304 4608 <sup>0</sup>\n\n**Spatial-Temporal\
    \ Approximate BSN Architecture**\n\n• Flexible approximation and reuse to **support**\
    \ \n\n**Experimental Results**\n\n• Find 47 pareto optimal in total 3362 design\
    \ • **Flexible** choose suitable configuration\n\n> Total 3362 BSN 128 input non-linear\
    \ adder for ReLU\n\n• Reduce the ADP by 1.9x/1.9x/2.5x for tanh/ sigmoid/ReLU\
    \ with negligible errors\n\n• Spatial / spatial-temporal approximate BSN reduce\
    \ the ADP by 2.8x and **4.1x**\n\nLargest convolution (3x3x512) in ResNet18\n\n\
    **Large Convolution Comparison**\n\n**Neural Network Task Evaluation** • Four\
    \ scaled layers in ResNet18: spatialtemporal approximate BSN reduce the ADP\n\n\
    by **4.1~23.3x (average 8.47x)** • Also reduce the area of datapath by 2.2x\n\n\
    *We propose much more efficient and flexible BSN‐based non‐linear adder design\
    \ for end‐to‐end SC neural network acceleration*\n\n47 pareto optimal\n\n1x103\
    \ 1x104 1x105 1x106\n\n APC+FSM MUX+FSM Baseline BSN+SI Design space Pareto front\
    \ ADP / (um²ꞏns)\n\nCycle9 Final BSN reuse\n\n8 \\* 72b in 8 cycles 576b\n\n576b\
    \ Appr. BSN Cycle2 reuse Cycle8 reuse 72b 72b 72b\n\nSpatial-Temporal 4608to256\
    \ Accumulation in 9 cycles\n\n**256b** Reused 576b BSN\n\n……\n\n**different accumulation\
    \ widths**\n\n**Opportunity from approximation** • BSN output has fewer bits than\
    \ input • Use much smaller BSNs for later stages\n\n**Spatial-Temporal BSN Design**\
    \ • **Fold the accumulation** temporally by reusing a single BSN over **multiple\
    \ cycles**\n\n576b\n\n576b 576b\n\n**4608b**\n\n**Design Space Exploration**\n\
    \n1x10-4\n\n1x10-3\n\nComputation error (MSE)\n\n1x10-2\n\n**Approximate Spatial-Temporal\
    \ Sorting Network**\n\n**Efficient Non-Linear Adder for Stochastic Computing with**\
    \ \n\n*<sup>3</sup>Beijing Advanced Innovation Center for Integrated Circuits*\
    \  \\*meng.li@pku.edu.cn *<sup>5</sup>Meta*\n\nADP / (um²ꞏns)\n\n> 0.03 0.04 0.05\
    \ 0.06 0.07\n\nComputing error\n\n**Why BSN Ineffective?** • Completely **parallel**\
    \ sorting\n\nAccumulation width\n\n1 1/2 1/4 1/6 1/8\n\nbitstreams to **reduce\
    \ costs**\n\n• Sub-sample methods trade off\n\nclip a lot with negligible errors\n\
    \n**quantization error**\n\n**Parameterized Approximation**\n\n• Approximate BSN:\
    \ sorts and sub-sample the\n\n• Root: BSN recursively sorts all input\n\nSub-sample\
    \ ratio\n\n**Sub-Sample the BSN**\n\n576 1152 2304 4608 <sup>215</sup>\n\n tanh\
    \ ReLU\n\n<span id=\"page-3-2\"></span>• Number of sub-samples? • Where to sub-sample?\
    \ Fig. 11. Input distribution of the intermediate sub-sampling blocks in different\
    \ stages of the BSN provides an opportunity to reduce the BSN via clipping.\n\n\
    ![](_page_3_Figure_12.jpeg)\n\n<span id=\"page-3-3\"></span>Fig. 12. Spatial-temporal\
    \ BSN architecture with an example: a 576-bit BSN is reused over 9 cycles for\
    \ 4608b accumulation.\n\n<span id=\"page-3-4\"></span>TABLE V PERFORMANCE OF DIFFERENT\
    \ DESIGNS FOR A 3X3X512 CONVOLUTION.\n\n| Design                        | Area\
    \ (um²) | Delay (ns) | ADP (um²·ns) | MSE       |\n|-------------------------------|------------|------------|--------------|-----------|\n\
    | Baseline BSN                  | 2.95×105   | 4.33       | 1.26×106     | - \
    \        |\n| Spatial Appr. BSN             | 1.32×105   | 3.36       | 4.55×105\
    \     | 3.79×10−7 |\n| Spatial-Temporal<br>Appr. BSN | 8.18×103   | 1.92     \
    \  | 3.06×105*    | 3.79×10−7 |\n\n\\*Spatial-temporal BSN considers 19× area\
    \ to achieve the same throughput.\n\n![](_page_3_Figure_17.jpeg)\n\n<span id=\"\
    page-3-5\"></span>Fig. 13. (a) ADP and (b) MSE comparison on 4 size of layers\
    \ in ResNet18.\n\nBSN needs fewer cycles for smaller convolutions and achieved\
    \ ADP reductions from 8.2× to 23.3× with negligible errors, as shown in Figure\
    \ [13.](#page-3-5) On average, the spatial-temporal BSN reduces the 2.2× area\
    \ of datapath by reducing the average ADP of BSN by 8.5×. This shows that the\
    \ proposed SC design is more flexible and efficient.\n\n### V. SUMMARY AND FUTURE\
    \ WORK\n\nIn this paper, we review our recent works on end-to-end SC neural network\
    \ acceleration. [\\[4\\]](#page-4-10) implemented a parallel fully SCbased TNN\
    \ processor using deterministic thermometer encoding and sorting networks on the\
    \ MNIST, achieving energy efficiency of 198.9 TOPS/W. In addition, [\\[10\\]](#page-4-5)\
    \ propose SC-friendly models with highprecision residual fusion and corresponding\
    \ SC circuits to greatly improve the network accuracy. [\\[11\\]](#page-4-6) further\
    \ proposed a more flexible and efficient spatial-temporal approximate BSN, enabling\
    \ accurate, efficient, and flexible end-to-end SC acceleration. In future work,\
    \ we explore SOTA transformer acceleration based on end-to-end stochastic computing,\
    \ which has been submitted [\\[12\\]](#page-4-7).\n\n### REFERENCES\n\n- <span\
    \ id=\"page-4-0\"></span>[1] W. Romaszkan *et al.*, \"ACOUSTIC: Accelerating Convolutional\
    \ Neural Networks through Or-Unipolar Skipped Stochastic Computing,\" in *Design,\
    \ Automation & Test in Europe Conference & Exhibition (DATE)*, 2020, pp. 768–773.\n\
    - [2] W. Romaszkan *et al.*, \"A 4.4–75-TOPS/W 14-nm Programmable, Performance-\
    \ and Precision-Tunable All-Digital Stochastic Computing Neural Network Inference\
    \ Accelerator,\" *IEEE Solid-State Circuits Letters*, vol. 5, pp. 206–209, 2022.\n\
    - <span id=\"page-4-2\"></span>[3] Y. Zhang *et al.*, \"When sorting network meets\
    \ parallel bitstreams: A fault-tolerant parallel ternary neural network accelerator\
    \ based on stochastic computing,\" in *Design, Automation & Test in Europe Conference\
    \ & Exhibition (DATE)*. IEEE, 2020, pp. 1287–1290.\n- <span id=\"page-4-10\"></span>[4]\
    \ Y. Hu *et al.*, \"A 28-nm 198.9-TOPS/W Fault-Tolerant Stochastic Computing Neural\
    \ Network Processor,\" *IEEE Solid-State Circuits Letters*, vol. 5, pp. 198–201,\
    \ 2022.\n- <span id=\"page-4-1\"></span>[5] Y. Zhang *et al.*, \"Accurate and\
    \ Energy-Efficient Implementation of Non-Linear Adder in Parallel Stochastic Computing\
    \ using Sorting Network,\" in *IEEE International Symposium on Circuits and Systems\
    \ (ISCAS)*, 2020, pp. 1–5.\n- <span id=\"page-4-3\"></span>[6] K. Kim *et al.*,\
    \ \"Dynamic energy-accuracy trade-off using stochastic computing in deep neural\
    \ networks,\" in *Proceedings of the 53rd Annual Design Automation Conference*,\
    \ 2016, pp. 1–6.\n- [7] J. Li *et al.*, \"Towards acceleration of deep convolutional\
    \ neural networks using stochastic computing,\" in *2017 22nd Asia and South Pacific\
    \ Design Automation Conference (ASP-DAC)*. IEEE, 2017, pp. 115–120.\n- [8] Z.\
    \ Li *et al.*, \"HEIF: Highly efficient stochastic computing-based inference framework\
    \ for deep neural networks,\" *IEEE Transactions on Computer-Aided Design of Integrated\
    \ Circuits and Systems*, vol. 38, no. 8, pp. 1543–1556, 2018.\n- <span id=\"page-4-4\"\
    ></span>[9] J. Li *et al.*, \"Hardware-driven nonlinear activation for stochastic\
    \ computing based deep convolutional neural networks,\" in *2017 International\
    \ Joint Conference on Neural Networks (IJCNN)*, 2017, pp. 1230–1236.\n- <span\
    \ id=\"page-4-5\"></span>[10] Y. Hu *et al.*, \"Accurate yet Efficient Stochastic\
    \ Computing Neural Acceleration with High Precision Residual Fusion,\" in *Design,\
    \ Automation & Test in Europe Conference & Exhibition (DATE)*, 2023.\n- <span\
    \ id=\"page-4-6\"></span>[11] Y. Hu *et al.*, \"Efficient Non-Linear Adder for\
    \ Stochastic Computing with Approximate Spatial-Temporal Sorting Network,\" in\
    \ *ACM/IEEE Design Automation Conference (DAC)*, 2023.\n- <span id=\"page-4-7\"\
    ></span>[12] Y. Hu *et al.*, \"ASCEND: Accurate yet Efficient End-to-End Stochastic\
    \ Computing Acceleration of Vision Transformer,\" in *submitted*.\n- <span id=\"\
    page-4-8\"></span>[13] K. E. Batcher, \"Sorting networks and their applications,\"\
    \ in *Proceedings of the April 30–May 2, 1968, spring joint computer conference*,\
    \ 1968, pp. 307–314.\n- <span id=\"page-4-9\"></span>[14] S. Mohajer *et al.*,\
    \ \"Routing magic: Performing computations using routing networks and voting logic\
    \ on unary encoded data,\" in *Proceedings of the 2018 ACM/SIGDA International\
    \ Symposium on Field-Programmable Gate Arrays*, 2018, pp. 77–86.\n- <span id=\"\
    page-4-11\"></span>[15] J. Lee *et al.*, \"UNPU: A 50.6 TOPS/W unified deep neural\
    \ network accelerator with 1b-to-16b fully-variable weight bit-precision,\" in\
    \ *2018 IEEE International Solid-State Circuits Conference-(ISSCC)*. IEEE, 2018,\
    \ pp. 218–220.\n- [16] J. Song *et al.*, \"7.1 An 11.5 TOPS/W 1024-MAC butterfly\
    \ structure dualcore sparsity-aware neural processing unit in 8nm flagship mobile\
    \ SoC,\" in *2019 IEEE International Solid-State Circuits Conference-(ISSCC)*.\
    \ IEEE, 2019, pp. 130–132.\n- [17] C.-H. Lin *et al.*, \"7.1 A 3.4-to-13.3 TOPS/W\
    \ 3.6 TOPS dual-core deeplearning accelerator for versatile AI applications in\
    \ 7nm 5G smartphone SoC,\" in *2020 ieee international solid-state circuits conference-(isscc)*.\
    \ IEEE, 2020, pp. 134–136.\n- [18] F. Tu *et al.*, \"Evolver: A deep learning\
    \ processor with on-device quantization–voltage–frequency tuning,\" *IEEE Journal\
    \ of Solid-State Circuits*, vol. 56, no. 2, pp. 658–673, 2020.\n- <span id=\"\
    page-4-12\"></span>[19] H. Mo *et al.*, \"9.2 A 28nm 12.1 TOPS/W dual-mode CNN\
    \ processor using effective-weight-based convolution and error-compensation-based\
    \ prediction,\" in *2021 IEEE International Solid-State Circuits Conference (ISSCC)*,\
    \ vol. 64. IEEE, 2021, pp. 146–148."
  decisions:
    evaluation_prompt: 'Qualified. Reason: The paper includes multiple sections that
      provide empirical and experimental evaluations, such as "Experimental Results"
      sections, performance tables, and figures demonstrating the evaluation of the
      proposed methods. Additionally, it discusses benchmarks, metrics, and comparisons
      with state-of-the-art methods, fulfilling the criteria for qualification.'
    related_work_prompt: 'Qualified. Reason: The paper meaningfully engages with prior
      research throughout its content. It includes numerous academic citations, discusses
      existing SC-based accelerators, compares its methods to previous work, and provides
      context and discussion for the tools and frameworks it cites. The paper also
      includes a detailed comparison of its proposed methods with prior research in
      various sections, such as the introduction, motivation, and experimental results.'
    novelty_prompt: 'Qualified. Reason: The paper introduces several novel contributions,
      including a deterministic thermometer coding scheme for SC accelerators, SC-friendly
      models with high precision residual fusion, and an approximate spatial-temporal
      sorting network. It also claims to be the first silicon-proven end-to-end SC
      accelerator and presents experimental results demonstrating significant improvements
      in energy efficiency and accuracy.'
    review_only_prompt: '- Disqualified: review paper. Reason: The paper primarily
      summarizes existing work on end-to-end stochastic computing neural network acceleration,
      including previous efforts and contributions, without introducing new contributions.'
- title: "Rethinking the Producer-Consumer Relationship in Modern DRAM-Based\n  Systems"
  abstract: "Generational improvements to commodity DRAM throughout half a century\
    \ have\nlong solidified its prevalence as main memory across the computing industry.\n\
    However, overcoming today's DRAM technology scaling challenges requires new\n\
    solutions driven by both DRAM producers and consumers. In this paper, we\nobserve\
    \ that the separation of concerns between producers and consumers\nspecified by\
    \ industry-wide DRAM standards is becoming a liability to progress\nin addressing\
    \ scaling-related concerns.\n  To understand the problem, we study four key directions\
    \ for overcoming DRAM\nscaling challenges using system-memory cooperation: (i)\
    \ improving memory access\nlatencies; (ii) reducing DRAM refresh overheads; (iii)\
    \ securely defending\nagainst the RowHammer vulnerability; and (iv) addressing\
    \ worsening memory\nerrors. We find that the single most important barrier to\
    \ advancement in all\nfour cases is the consumer's lack of insight into DRAM reliability.\
    \ Based on an\nanalysis of DRAM reliability testing, we recommend revising the\
    \ separation of\nconcerns to incorporate limited information transparency between\
    \ producers and\nconsumers. Finally, we propose adopting this revision in a two-step\
    \ plan,\nstarting with immediate information release through crowdsourcing and\n\
    publication and culminating in widespread modifications to DRAM standards."
  url: http://arxiv.org/abs/2401.16279v1
  keywords: ''
  document: '## 1. Introduction


    Dynamic Random Access Memory (DRAM) [\[1,](#page-13-0) [2\]](#page-13-1) is the
    dominant main memory technology across a broad range of computing systems because
    of its high capacity at low cost [\[3–](#page-13-2)[6\]](#page-13-3). Building
    modern DRAM chips requires specialized design and manufacturing techniques (e.g.,
    custom process nodes [\[7\]](#page-13-4) and bespoke materials [\[8\]](#page-13-5))
    developed across more than half a century of rich history [\[9\]](#page-13-6),
    so the computing industry employs a separation of concerns to explicitly divide
    responsibilities between DRAM producers (e.g., manufacturers) and DRAM consumers
    (e.g., cloud architects, processor and system-on-a-chip designers, memory module
    vendors, etc.). DRAM producers typically develop highly-optimized DRAM chips as
    standalone mass-market components with clearly-specified interfaces, independently
    addressing design concerns related to DRAM technology. DRAM consumers then integrate
    these DRAM chips to develop a broad range of DRAM-based systems. This approach
    enables both parties to specialize their designs and preserve trade secrets while
    working around a common interface.


    In the last decade, however, this long-standing separation of concerns has been
    challenged by worsening DRAM technology scaling difficulties that manifest in
    two problematic ways.


    First, improvements to DRAM access latency and storage capacity are slowing down
    due to DRAM technology scaling challenges [\[10,](#page-13-7) [11\]](#page-13-8).
    To illustrate this problem, we survey 58 publicly-available DRAM chip datasheets[1](#page-0-0)
    in Section [3.1,](#page-3-0) broadly sampling chips from across 19 DRAM manufacturers
    spanning the past five decades. We study the evolution of access latency and storage
    capacity characteristics, showing slowdowns in improvements to both metrics in
    the past ten to twenty years. For example, average annual improvements to the
    data access latency (governed by the timing parameter tRCD) reduced by 69.5% (from
    2.66% to 0.81%) before and after the year 2000, with little to no improvement
    in the past decade. This is consistent with similar surveys conducted by prior
    works [\[12–](#page-13-9)[22\]](#page-13-10).


    Second, conventional approaches to managing scaling challenges used by DRAM producers
    (e.g., in-DRAM mechanisms to mitigate worsening memory errors) no longer suffice
    to hide the underlying problems from DRAM consumers. For example, memory errors
    caused by the RowHammer vulnerability are a serious and growing threat to system-level
    robustness (i.e., security, reliability, and safety) [\[23](#page-13-11)[–27\]](#page-13-12).
    Experimental studies [\[28](#page-13-13)[–32\]](#page-13-14) throughout the past
    decade demonstrate that these errors can be consistently induced across a broad
    range of DRAM chips. In particular, Kim et al. [\[28\]](#page-13-13) show that
    RowHammer errors can be induced much more quickly in modern chips, i.e., with
    only 4.8K memory activations for chips from 2019– 2020, which is 14.4× lower than
    the 69.2K activations required for older DRAM chips from 2010–2013. Section [3](#page-3-1)
    further discuss how DRAM technology design concerns that were previously hidden
    by the separation are now breaking through to become consumer-facing problems
    that impact system-wide performance and robustness.


    We believe that overcoming the DRAM scaling problem requires creative, holistic
    thinking from everyone involved, including both producers and consumers throughout
    industry and academia. Unfortunately, we observe two key limitations with today''s
    separation of concerns that discourage progress toward addressing scaling challenges:


    1. The industry-wide DRAM standards that specify how to implement the separation
    (e.g., JEDEC DDRn [\[33,](#page-13-15) [34\]](#page-13-16), HBMx [\[35,](#page-13-17)
    [36\]](#page-13-18))[2](#page-0-1) do so imperfectly, requiring laborious revision
    to adapt to failures in separation.


    <span id="page-0-0"></span><sup>1</sup>We conduct a best-effort survey of publicly-available
    datasheets. We itemize the specific datasheets we reference in Appendix [B.](#page-24-0)


    <span id="page-0-1"></span><sup>2</sup>DRAM standards specify a DRAM chip''s microarchitecture,
    including its interface, configuration, and performance characteristics as visible
    to DRAM consumers. Section [2](#page-2-0) explains standards in detail.


    2. The existing separation is too strict, which constrains each party''s solution
    space and stifles opportunities to explore new ways to address the scaling challenges.


    These observations stem from a combination of two key sources of evidence.


    First, recent robustness challenges caused by memory errors have already broken
    the separation of concerns established by current DRAM standards. Section [3.2](#page-4-0)
    references two specific instances of this problem, RowHammer [\[25](#page-13-19)[–27,](#page-13-12)
    [29,](#page-13-20) [37\]](#page-13-21) and ondie error correction [\[38](#page-13-22)[–41\]](#page-13-23),
    showing that both cases expose memory errors caused by DRAM technology behavior
    in a way that is undefined by existing DRAM standards. Although recent changes
    to the standards [\[34,](#page-13-16) [42](#page-13-24)[–44\]](#page-13-25) discuss
    these problems and provide limited solutions, undefined chip behavior remains
    a serious and worsening problem for DRAM consumers [\[27,](#page-13-12) [45](#page-13-26)[–48\]](#page-13-27).


    Second, many promising approaches to address DRAM scaling challenges in today''s
    chips [\[16,](#page-13-28) [22,](#page-13-10) [29,](#page-13-20) [49–](#page-13-29)[110\]](#page-15-0)
    rely upon exploiting the benefits of deliberately breaking the separation of concerns.
    These approaches employ system-memory cooperation [\[10,](#page-13-7) [27,](#page-13-12)
    [111–](#page-15-1)[113\]](#page-15-2), demonstrating significant system-level
    benefits from implementing mechanisms outside the DRAM chip to supplement on-chip
    solutions built by DRAM producers. Section [4](#page-5-0) surveys these proposals
    categorized by the particular DRAM scaling challenge they tackle: (1) improving
    memory access latencies (Section [4.1\)](#page-5-1); (2) reducing DRAM refresh
    overheads (Section [4.2\)](#page-6-0); (3) securely defending against the RowHammer
    vulnerability (Section [4.3\)](#page-6-1); and (4) addressing worsening memory
    errors (Section [4.4\)](#page-7-0). Unfortunately, we observe that today''s separation
    of concerns does not support producers and consumers to adopt these methods with
    ease. Instead, doing so requires them to work around DRAM standards, which is
    impractical for the overwhelming majority of consumers due to the risks and costs
    inherent in custom modifications to DRAM chips and unstandardized behavior.


    Based on these observations, we conclude that both the separation of concerns
    and the standards that specify them are outdated for today''s DRAM landscape.
    To rethink the separation of concerns in a modern context, we refer to each of
    the four cases of system-memory cooperation that we study in Section [4.](#page-5-0)
    In each case, we review how prior proposals break the separation of concerns so
    that we can better understand its limitations today.


    We find that the single most important barrier to advancement in all four cases
    is the consumer''s lack of insight into DRAM reliability. For example, Section
    [4.3](#page-6-1) explains how knowing certain properties of memory errors (e.g.,
    correlation with physical chip locations, memory access patterns, and operating
    parameters) is essential for developing secure defenses against the RowHammer
    vulnerability. The existing separation of concerns effectively abstracts details
    of a DRAM chip''s internal operation away from consumers to the extent that consumers
    do not have the necessary context to properly reason about and evaluate how operating
    the chip in a particular way will impact its reliable operation. This encompasses
    operating points both within and outside of manufacturer recommendations;


    in either case, the consumer lacks the context necessary to accurately determine
    how their design decisions outside of the DRAM chip (e.g., in the memory controller)
    affect the DRAM chip''s reliable operation.


    To gain further insight into this problem, we study the general process of memory
    reliability testing in Section [5.](#page-8-0) Our analysis suggests that the
    entire testing process is grounded on knowing relevant properties of a DRAM chip''s
    microarchitectural design, such as the physical organization of cells within the
    storage array and how they encode data at a circuit level. Using this information,
    a DRAM consumer can build models and test methodologies to explore the full design
    space surrounding commodity DRAM chips.


    Based on our analysis, we advocate revising both the separation of concerns and
    the standards that specify them to incorporate limited information transparency
    between DRAM producers and consumers. In particular, explicitly communicating
    basic DRAM design and test characteristics from DRAM producers to consumers empowers
    the consumer with the context to understand how different system-level design
    choices (e.g., optimizations in the processor and memory controller) will affect
    DRAM chip operation. Section [7.1](#page-11-0) identifies information to communicate
    by drawing on examples from research studies, including (1) basic microarchitectural
    properties (e.g., organization of physical rows, sizes of internal storage arrays)
    and (2) best practice guidelines for reliability testing (e.g., test patterns
    for key error mechanisms).[3](#page-1-0) Section [6](#page-9-0) further explains
    how access to this information provides a practical degree of insight into DRAM
    operation for DRAM consumers to work with, without compromising DRAM producers''
    trade secrets or the cost advantages of commodity DRAM.


    We advocate for industry to incorporate information transparency into DRAM standards
    through a two-step approach involving all DRAM stakeholders, including producers
    and consumers.


    Step 1: Early Adoption. Initially, we recommend conceptually revising the separation
    of concerns without yet revising DRAM standards. This step targets DRAM chips
    already in the field, asking both DRAM producers and consumers to voluntarily
    release information they already have at hand. We propose each party to take a
    different approach as follows:


    - Consumers: Contribute to a crowdsourced database of information obtained through
    third-party testing of commodity DRAM chips on the market (e.g., as conducted
    in numerous studies discussed in Section [4\)](#page-5-0).

    - Producers: Publish information (e.g., using datasheet revisions, whitepapers,
    or online resources) about their products, possibly limited to information that
    they already have on hand from past records (i.e., information that requires minimal
    logistical effort to release).


    Through these two avenues, all DRAM stakeholders will benefit from the release
    of information: consumers will gain ac-


    <span id="page-1-0"></span><sup>3</sup>Consumers with access to appropriate testing
    infrastructure can reverseengineer much of this information [\[16,](#page-13-28)
    [22,](#page-13-10) [32,](#page-13-14) [73,](#page-14-0) [74,](#page-14-1) [78,](#page-14-2)
    [114–](#page-15-3)[118\]](#page-15-4). However, existing techniques may not reveal
    all possible necessary information (e.g., due to inaccessible components such
    as remapped rows).


    cess to a wider solution space for addressing DRAM scaling challenges, and producers
    will gain access to insights and new solution directions from consumers'' efforts.
    In particular, these benefits require neither changes to existing DRAM hardware
    or standards (though standardizing the information release could streamline the
    process) nor forced disclosure of sensitive information by either party.


    Step 2: Long-Term Revision. In the long run, we propose revising DRAM standards
    to include industry-standard information, tools, and specifications for a wider
    range of DRAM operating points. In addition to the information requested by Step
    1, we identify two recommendations that our studies show would be beneficial for
    consumers: (1) reliability guarantees for how a chip is expected to behave under
    certain operating conditions (e.g., predictable behavior of faults [\[119\]](#page-15-5));
    (2) disclosure of industry-validated DRAM reliability models and testing strategies
    suitable for commodity DRAM chips (e.g., similar to how JEDEC JEP122 [\[120\]](#page-15-6),
    JESD218 [\[121\]](#page-15-7), and JESD219 [\[122\]](#page-15-8) address Flash-memory-specific
    error mechanisms [\[123–](#page-15-9)[125\]](#page-15-10) such as floating-gate
    data retention [\[126–](#page-15-11)[129\]](#page-15-12) and models for physical
    phenomena such as threshold voltage distributions [\[130–](#page-15-13)[135\]](#page-15-14)).
    Revising DRAM standards in this way will align the standards with a more permissive
    separation of concerns going forward, thereby encouraging cooperation between
    producers and consumers in pursuit of building more robust and higher-performance
    future computing systems.


    We make the following contributions:


    - We make a case to rethink the long-standing separation of concerns between DRAM
    producers and consumers to enable and encourage new solutions for addressing worsening
    DRAM technology scaling challenges.

    - We motivate and support our case by thoroughly reviewing prior work, including
    (1) four case studies that survey system-memory cooperative techniques to address
    DRAM scaling challenges and (2) a historical survey of DRAM chip capacity, latency,
    and energy characteristics based on datasheets. We open-source [\[136\]](#page-15-15)
    our dataset.

    - We provide a new perspective on memory reliability testing from the viewpoint
    of DRAM consumers, identifying access to a DRAM chip''s microarchitectural details
    as both a key challenge and enabler for system-memory cooperative solutions to
    DRAM scaling challenges.

    - We propose a practical plan to encourage new solutions to modern DRAM scaling
    challenges by revising both the separation of concerns and how it is specified
    by DRAM standards today.


    ## <span id="page-2-0"></span>2. DRAM Standards as a Separation of Concerns


    Industry-wide DRAM standards split the responsibilities of building and integrating
    commodity DRAM chips to DRAM producers and consumers, respectively. This section
    reviews how standards achieve this and its implications for DRAM producers and
    consumers.


    ## 2.1. DRAM Standards


    DRAM standards carefully balance the needs of both producers and consumers through
    industry-wide consensus. Today, the


    JEDEC consortium [\[137\]](#page-15-16) maintains a limited set of standards describing
    commodity DRAM chips with different target applications, e.g., general-purpose
    DDRn [\[33,](#page-13-15) [34,](#page-13-16) [138\]](#page-15-17), bandwidthoptimized
    HBMn [\[35,](#page-13-17) [36\]](#page-13-18), mobile-oriented LPDDRn [\[44,](#page-13-25)
    [139\]](#page-15-18), graphics-oriented GDDRn [\[140,](#page-15-19) [141\]](#page-15-20).


    DRAM standards specify all aspects of a DRAM chip''s design that pertain to the
    interface between producers and consumers, including the chip''s access interfaces,
    configuration mechanisms, and performance characteristics. By doing so, standards
    effectively abstract DRAM technology-level concerns (e.g., cellto-cell variation,
    reliability challenges) into predictable usage patterns, thereby reducing a complex
    storage technology into a modular computing component.


    #### 2.2. Advantages of Standardized DRAM


    Standardized DRAM enables the widespread use of highlyoptimized DRAM chips. This
    is because standards scope DRAM technology to a few fixed components (i.e., standardized
    chips) with clearly-defined interfaces that both producers and consumers can optimize
    towards without concerning themselves with the other party''s design challenges.
    This gives each party the freedom to explore different solutions to DRAM design
    challenges based on their design goals. For example, empirical studies of DRAM
    chips [\[23,](#page-13-11) [40,](#page-13-30) [48,](#page-13-27) [118,](#page-15-4)
    [142\]](#page-15-21) show that different DRAM producers mitigate memory errors
    using a broad range of different on-chip error mitigation techniques. In general,
    producers are free to innovate in any way that does not violate the specifications
    established by standards, and consumers can build upon those specifications in
    any way to meet their systems'' needs.


    2.2.1. DRAM Producers'' Trade Secrets. DRAM producers closely guard their innovations
    because trade secrets are a key component of business competitiveness and success
    [\[86,](#page-14-3) [143–](#page-15-22)[145\]](#page-15-23). Producers who build
    standards-compliant chips are competing in a commodity market, so they seek profitability
    though economies of scale [\[146](#page-15-24)[–149\]](#page-15-25). Each producer
    develops and uses home-grown, highly-optimized design, manufacturing, and testing
    processes that amortize costs in high volume production, thereby maximizing per-chip
    profit margins.


    As a result, DRAM producers publish only what information DRAM standards require,
    such as access timing specifications and circuit characteristics needed for chip
    integration. Additional information not specified by standards (e.g., internal
    circuit designs, chip error rates) is kept in-house. Although such details can
    often be inferred through reverse-engineering studies [\[16,](#page-13-28) [23,](#page-13-11)
    [24,](#page-13-31) [28,](#page-13-13) [32,](#page-13-14) [40,](#page-13-30) [74,](#page-14-1)
    [78,](#page-14-2) [92,](#page-14-4) [114,](#page-15-3) [117,](#page-15-26) [150](#page-15-27)[–155\]](#page-15-28)
    and chip teardowns [\[156,](#page-15-29) [157\]](#page-15-30) (discussed further
    in Section [4\)](#page-5-0), producers have no obligation to communicate this
    information to consumers.


    #### <span id="page-2-1"></span>2.3. Using Non-Commodity DRAM Chips


    Consumers can use non-commodity DRAM chips by either (i) privately working with
    DRAM producers to build customized chips or (ii) buying specialized or otherwise
    domain-optimized DRAM chips (e.g., high reliability [\[158,](#page-15-31) [159\]](#page-15-32),
    low latency [\[160\]](#page-15-33)). These chips are generally still compliant
    with JEDEC standards, though they may provide additional unstandardized features.


    Developing and using non-commodity DRAM can benefit industry-wide standards. For
    example, the HBM standard (JESD235 [\[35\]](#page-13-17)) largely grew from private
    industry collaborations between AMD and SK Hynix [\[161\]](#page-15-34). Similarly,
    features pioneered in non-commodity DRAM can be integrated into newer standards.


    Unfortunately, innovating through non-commodity DRAM is a slow and costly process
    because it forgoes the advantages of mainstream DRAM chips. Non-commodity chips
    are typically feasible only for consumers in specific industries (e.g., imaging,
    networking [\[162\]](#page-15-35)) or with significant stake in the global DRAM
    market (e.g., large-scale cloud vendors). Our work takes inspiration from non-commodity
    DRAM to enable all consumers to pursue such innovations, ultimately benefitting
    the DRAM industry as a whole.


    #### <span id="page-3-5"></span>2.4. Creating or Modifying DRAM Standards


    Changes to DRAM standards require participation from all stakeholders throughout
    the DRAM industry, including JEDEC personnel, DRAM producers, and DRAM consumers.
    Therefore, making changes is a slow process that can involve nontechnical elements,
    such as political motivations and business goals [\[163\]](#page-15-36). Major
    changes typically follow one of three different paths. First, a standards committee
    comprising experts from all stakeholders may directly draft a new standard. Second,
    new standards may grow out of the development and use of non-commodity DRAM as
    discussed in Section [2.3.](#page-2-1) Third, existing standards may be updated
    or supplemented by JEDEC committees for special issues, such as LPDDR4X [\[164\]](#page-16-0),
    3Dstacked DRAM [\[165\]](#page-16-1), and RowHammer [\[42,](#page-13-24) [43\]](#page-13-32).


    ## <span id="page-3-1"></span>3. Challenges of DRAM Technology Scaling


    DRAM''s primary competitive advantage is its low cost-percapacity [\[3](#page-13-2)[–6\]](#page-13-3),
    which DRAM producers maintain by continually improving chip storage densities.
    Doing so across successive product generations requires carefully balancing shrinking
    physical feature sizes, optimizing circuit areas, and mitigating worsening memory
    errors [\[38,](#page-13-22) [86,](#page-14-3) [166–](#page-16-2)[170\]](#page-16-3).


    Unfortunately, today''s DRAM faces two key challenges to continued technology
    scaling: (i) the slowdown of generational improvements to storage capacity, access
    latency, and power consumption [\[13,](#page-13-33) [17,](#page-13-34) [171\]](#page-16-4);
    and (ii) the breakdown of conventional approaches to mitigate memory errors. This
    section reviews both challenges in detail.


    #### <span id="page-3-0"></span>3.1. Slowdown of Generational Improvements


    Advancements in DRAM chip storage density have been central to increasing demands
    for memory capacity since the inception of DRAM technology [\[9,](#page-13-6)
    [172\]](#page-16-5). Today''s emerging data-intensive applications and systems
    in domains such as AI, cloud, and HPC continue to demand greater memory capacity
    at an unprecedented scale [\[173–](#page-16-6)[177\]](#page-16-7). Unfortunately,
    technology shrinkage throughout the past two decades has yielded diminishing benefits
    for chip storage capacity, access latency, and refresh overheads because of the
    growing costs and overheads of maintaining reliable chip operation at smaller
    technology node sizes [\[10,](#page-13-7) [11\]](#page-13-8).


    To better understand this slowdown of generational improvements, we survey manufacturer-reported
    DRAM chip capacities, access timings, and current consumption characteristics
    given by 58 publicly-available DRAM chip datasheets from across 19 different DRAM
    manufacturers with datasheet publication dates between 1970 and 2021.[4](#page-3-2)
    The remainder of this section individually analyzes chip capacity, access timings,
    and refresh overheads in the context of our survey.


    3.1.1. Chip Storage Capacity. Figure [1](#page-3-3) shows the time evolution of
    per-chip storage capacity and four key DRAM operating timings (all shown in log
    scale). We observe that storage capacity has grown exponentially over time alongside
    improvements to all four timing parameters (timings are discussed in Section [3.1.2\)](#page-3-4).
    However, storage capacity growth has slowed down markedly since 2010, dropping
    from an exponential growth factor of 0.341 per year for 1970-2000 to 0.278 for
    2000-2020. This is consistent with recent challenges in scaling beyond 16 Gb chip
    densities, and this slowdown is expected to continue going forward [\[11,](#page-13-8)
    [178\]](#page-16-8).


    <span id="page-3-3"></span>![](_page_3_Figure_11.jpeg)


    Figure 1: Semi-log plot showing the evolution of key DRAM access timings (left)
    and per-chip storage capacity (right)<sup>i</sup> across each 5-year period of
    time.


    | i<br>JEDEC-standardized parameters[34] found in DRAM chip datasheets: |            |

    |-----------------------------------------------------------------------|------------|

    | Parameter                                                             | Definition
    |


    | tRCD        | minimum row activation to column operation delay |

    |-------------|--------------------------------------------------|

    | CAS Latency | read operation to data access latency            |

    | tRAS        | minimum row activation to precharge delay        |

    | tRC         | minimum delay between accesses to different rows |

    |             |                                                  |


    <span id="page-3-4"></span>3.1.2. Chip Access Latency. DRAM access latency has
    not significantly improved relative to storage capacity over the last two decades
    [\[12](#page-13-9)[–14,](#page-13-35) [16–](#page-13-28)[21\]](#page-13-36). This
    makes DRAM an increasingly significant system performance bottleneck today, especially
    for workloads with large footprints that are sensitive to DRAM access latency
    [\[12,](#page-13-9) [77,](#page-14-5) [111,](#page-15-1) [112,](#page-15-37) [179](#page-16-9)[–202\]](#page-16-10).
    Therefore, there is significant opportunity for improving overall system performance
    by reducing the memory access latency [\[16,](#page-13-28) [22,](#page-13-10)
    [52,](#page-13-37) [59,](#page-14-6) [74,](#page-14-1) [77,](#page-14-5) [78,](#page-14-2)
    [82,](#page-14-7) [106,](#page-15-38) [109,](#page-15-39) [110,](#page-15-0) [203,](#page-16-11)
    [204\]](#page-16-12). Although conventional latencyhiding techniques (e.g., caching,
    prefetching, multithreading) can potentially help mitigate many of the performance
    concerns, these techniques (1) fundamentally do not change the


    <span id="page-3-2"></span><sup>4</sup>This data encompasses DRAM chips from both
    asynchronous (e.g., page mode, extended data out) and synchronous (e.g., SDRAM,
    DDRn) DRAM chips. Appendix [A](#page-21-0) describes our data collection methodology
    in further detail, and Appendix [B](#page-24-0) provides an overview of our dataset,
    which is publicly available on GitHub [\[136\]](#page-15-15).


    latency of each memory access and (2) fail to work in many cases (e.g., irregular
    memory access patterns, random accesses, huge memory footprints).


    Figure [1](#page-3-3) shows that none of the four timings we study have improved
    significantly in the last two decades. For example, the median tRCD/CAS Latency/tRAS/tRC
    reduced by 2.66/3.11/2.89/2.89% per year on average between 1970 and 2000, but
    only 0.81/0.97/1.33/1.53% between 2000 and 2015.[5](#page-4-1) This is consistent
    with similar studies in prior work [\[12–](#page-13-9)[21\]](#page-13-36).


    <span id="page-4-5"></span>3.1.3. Worsening Refresh Overheads. The circuits that
    DRAM uses to store data are inherently susceptible to a wide range of different
    leakage mechanisms (e.g., capacitor charge leakage), which ultimately cause data
    loss if ignored. To prevent this, DRAM standards mandate periodic refresh operations
    that intermittently restore data values throughout the entire DRAM chip. Unfortunately,
    DRAM refresh incurs significant system performance and power overheads [\[49,](#page-13-29)
    [69,](#page-14-8) [80,](#page-14-9) [82,](#page-14-7) [86,](#page-14-3) [89,](#page-14-10)
    [104,](#page-14-11) [205](#page-16-13)[–207\]](#page-16-14), sacrificing almost
    half of the total memory throughput and consuming almost half of the total DRAM
    power for projected 64 Gb chips [\[80\]](#page-14-9).


    Figure [2](#page-4-2) illustrates the performance overheads of DRAM refresh across
    the different DRAM chips in our datasheet survey. The data shows the refresh penalty,
    [6](#page-4-3) which is defined as the ratio of two key timing parameters used
    to govern refresh operations: tRF C , the duration of each refresh command, and
    tREF I , the time between consecutive refresh commands. The refresh penalty represents
    the average time that a DRAM rank (or bank) is unavailable for access due to refresh
    operations [\[208,](#page-16-15) [210–](#page-16-16)[213\]](#page-16-17).


    <span id="page-4-2"></span>![](_page_4_Figure_4.jpeg)


    Figure 2: Refresh penalty (computed as the ratio between tRF C and tREF I ) for
    DRAM chips of different storage capacities.


    We observe that the refresh penalty worsens from a median of 1.04% for 1 Kib chips
    to 2.05% for 16 Kib chips, then improves to 0.43% for 128 Mib chips, and finally
    worsens to a median of 4.48% (worst-case of 7.56% for DDR5 chips) for 16 Gib chips.This
    non-monotonic trend is due to the relative rates of improvement in DRAM access
    latency and storage capacity: access (and therefore, refresh) latencies stagnated
    around the introduction of 128 Mib chips while capacity improvements did not.
    This data is consistent with both (i) our more detailed


    analysis of DRAM refresh timings in Appendix [A.4](#page-23-0) and (ii) trends
    observed in prior work [\[21,](#page-13-36) [80,](#page-14-9) [94,](#page-14-12)
    [206,](#page-16-19) [214,](#page-16-20) [215\]](#page-16-21), which expect that
    future, higher-capacity DRAM chips will spend an even larger proportion of time
    refreshing unless the DRAM refresh algorithm and its implementation are improved.


    #### <span id="page-4-0"></span>3.2. Breakdown of the Separation of Concerns


    DRAM is susceptible to a variety of error mechanisms that worsen with technology
    scaling and can impact system-wide reliability if not carefully managed. Today''s
    separation of concerns largely puts the burden to address these errors on DRAM
    producers, i.e., to hide them from consumers and provide the illusion of an error-free
    memory chip. Unfortunately, two classes of scaling-related errors are breaking
    through the separation to impact consumers regardless: random, single-bit errors
    and RowHammer errors. This section discusses these errors and how they break the
    separation of concerns.


    <span id="page-4-7"></span>3.2.1. Worsening Reliability. DRAM suffers from a range
    of circuit-level error mechanisms that can impact consumers (e.g., data loss,
    system failure) if mismanaged [\[84,](#page-14-13) [216–](#page-16-22)[221\]](#page-17-0).
    To mitigate these errors on-chip, DRAM producers typically use a combination of
    conservative operating timings (e.g., with added safety margins) and simple error-correcting
    hardware within the DRAM chip (e.g., manufacturing spare rows and columns to replace
    faulty ones [\[33,](#page-13-15) [34,](#page-13-16) [216,](#page-16-22) [222,](#page-17-1)
    [223\]](#page-17-2)).[7](#page-4-4) These low-cost techniques enable DRAM producers
    to provide the illusion of a fully reliable DRAM chip, thereby preserving the
    separation of concerns between producers and consumers.


    In contrast, modern DRAM chips exhibit much higher error rates because technology
    scaling exacerbates the underlying circuit-level error mechanisms that cause errors
    [\[86,](#page-14-3) [142,](#page-15-21) [143,](#page-15-22) [166,](#page-16-2)
    [167,](#page-16-23) [224,](#page-17-3) [225\]](#page-17-4). To combat these errors,
    DRAM producers use stronger error-mitigation mechanisms in modern DRAM chips (e.g.,
    on-die ECC [\[34,](#page-13-16) [38,](#page-13-22) [40,](#page-13-30) [41,](#page-13-23)
    [87,](#page-14-14) [115,](#page-15-40) [119,](#page-15-5) [167,](#page-16-23)
    [224,](#page-17-3) [226](#page-17-5)[–230\]](#page-17-6), post-package repair
    [\[33,](#page-13-15) [34,](#page-13-16) [216,](#page-16-22) [222,](#page-17-1)
    [223\]](#page-17-2), target row refresh [\[23,](#page-13-11) [32,](#page-13-14)
    [45,](#page-13-26) [46\]](#page-13-38), refresh management [\[34,](#page-13-16)
    [47\]](#page-13-39)), which are more expensive and incur higher performance and
    energy overheads.


    Unfortunately, worsening memory reliability remains a serious problem for DRAM
    consumers, especially high-volume consumers for whom even modest chip error rates
    are significant at scale [\[142,](#page-15-21) [221\]](#page-17-0). Although stronger
    in-DRAM error mitigations are effective against growing error rates [\[142,](#page-15-21)
    [224\]](#page-17-3), they introduce new overheads and challenges for consumers.
    For example, neither on-die ECC nor target row refresh correct all errors, and
    the remaining errors (e.g., uncorrectable errors) are difficult for consumers
    to predict and mitigate because their manifestation depends on the particular
    on-die ECC and/or TRR mechanism used by a given chip [\[23,](#page-13-11) [32,](#page-13-14)
    [38,](#page-13-22) [40,](#page-13-30) [41,](#page-13-23) [46,](#page-13-38) [115,](#page-15-40)
    [152,](#page-15-41) [229,](#page-17-7) [231\]](#page-17-8). As a result, DRAM
    consumers face errors that are growing in both magnitude and complexity, making
    reliability a key design concern for continued DRAM scaling.


    <span id="page-4-6"></span>3.2.2. The RowHammer Vulnerability. RowHammer [\[25,](#page-13-19)
    [26,](#page-13-40) [29,](#page-13-20) [37,](#page-13-21) [232–](#page-17-9)[234\]](#page-17-10)
    is a scaling-related read-disturb phenomenon


    <span id="page-4-1"></span><sup>5</sup>We report 2015 instead of 2020 because
    2020 shows a regression in CAS latency due to first-generation DDR5 chips, which
    we believe is not representative because of its immature technology.


    <span id="page-4-3"></span><sup>6</sup>Also referred to as refresh overhead [\[208\]](#page-16-15)
    and refresh duty cycle [\[209\]](#page-16-18).


    <span id="page-4-4"></span><sup>7</sup>Enterprise-class computing providers (e.g.,
    cloud, HPC) may use supplementary error-mitigation mechanisms discussed further
    in Section [4.4.1.](#page-7-1)


    affecting modern DRAM chips in which memory accesses to a given physical memory
    location can induce bit-flips at other locations. Significant work [\[23](#page-13-11)[–26,](#page-13-40)
    [102,](#page-14-15) [235–](#page-17-11)[258\]](#page-17-12) shows that RowHammer
    is a security threat that can be exploited to exfiltrate sensitive data, escalate
    user privileges, or otherwise compromise a system. Because RowHammer is fundamental
    to DRAM circuit designs, it is a problem for all DRAM-based systems and becomes
    significantly worse with continued process technology scaling [\[25,](#page-13-19)
    [26,](#page-13-40) [28,](#page-13-13) [29\]](#page-13-20).


    Modern DRAM partially addresses RowHammer through best practices laid out in DRAM
    specifications [\[34,](#page-13-16) [42,](#page-13-24) [43\]](#page-13-32) and
    RowHammer mitigation mechanisms built into DRAM chips by producers, including
    target row refresh (TRR) [\[23,](#page-13-11) [28,](#page-13-13) [32,](#page-13-14)
    [258–](#page-17-12)[261\]](#page-17-13) and refresh management (RFM) [\[34,](#page-13-16)
    [47\]](#page-13-39). Unfortunately, neither approach suffices to provide fully
    secure solutions against RowHammer [\[23,](#page-13-11) [32,](#page-13-14) [48,](#page-13-27)
    [118,](#page-15-4) [258,](#page-17-12) [262\]](#page-17-14). Therefore, RowHammer
    remains a serious consumer-visible problem that challenges the illusion of a robust
    DRAM chip established by the separation of concerns.


    ## <span id="page-5-0"></span>4. Challenges in Overcoming Scaling Problems


    Developing new solutions to address the DRAM scaling challenges discussed in Section
    [3](#page-3-1) requires creative thinking that today''s separation of concerns
    constrains. This section explores new techniques based on system-memory cooperation
    that holistically address DRAM scaling challenges with the help of hardware and
    software mechanisms at the system level. We review techniques that target each
    of the four key DRAM scaling challenges discussed in Section [3:](#page-3-1) access
    latency, refresh overheads, RowHammer, and worsening reliability. In each case
    study, we survey relevant prior work to understand how and why today''s separation
    of concerns discourages systemmemory cooperative solutions.


    #### <span id="page-5-1"></span>4.1. Study 1: Long DRAM Access Latency


    As Section [3.1.2](#page-3-4) discusses, slow generational improvements to DRAM
    access latency make DRAM a worsening bottleneck for latency-sensitive workloads
    today [\[12,](#page-13-9) [77,](#page-14-5) [111,](#page-15-1) [112,](#page-15-37)
    [179–](#page-16-9)[202\]](#page-16-10). System-memory cooperation promises to
    overcome this problem by directly reducing the DRAM access latency beyond what
    commodity DRAM chips provide.


    4.1.1. Solutions to Reduce the DRAM Access Latency. Prior works have taken two
    major directions to improve the DRAM access latency. First, many works [\[16,](#page-13-28)
    [22,](#page-13-10) [52,](#page-13-37) [59,](#page-14-6) [74,](#page-14-1) [77,](#page-14-5)
    [78,](#page-14-2) [82,](#page-14-7) [106,](#page-15-38) [109,](#page-15-39) [110\]](#page-15-0)
    show that the average DRAM access latency can be shortened by reducing DRAM access
    timings for particular memory locations that can tolerate faster accesses. This
    can be done safely because, although DRAM standards call for constant access timings
    across all memory locations, the minimum viable access timings that the hardware
    can support actually differ across memory locations due to factors such as heterogeneity
    in the circuit design [\[17,](#page-13-34) [78\]](#page-14-2) and manufacturing
    process variation in circuit components [\[16,](#page-13-28) [22,](#page-13-10)
    [52,](#page-13-37) [74,](#page-14-1) [263\]](#page-17-15).


    Exploiting these variations in access timings to reduce the average memory access
    latency provides significant system performance improvement. For example, Chang
    et al. [\[16\]](#page-13-28) experimentally show that exploiting access latency
    variations provides an average 8-core system performance improvement of 13.3%/17.6%/19.5%
    for real DRAM chips from three major DRAM manufacturers. Similarly, Kim et al.
    [\[74\]](#page-14-1) show that exploiting access latency variations induced by
    DRAM sense amplifiers provides an average (maximum) 4-core system performance
    improvement of 4.97% (8.79%) versus using default DRAM access timings based on
    data obtained from 282 commodity LPDDR4 DRAM chips.


    Second, other works [\[55,](#page-14-16) [58,](#page-14-17) [73,](#page-14-0)
    [75,](#page-14-18) [90,](#page-14-19) [91,](#page-14-20) [96–](#page-14-21)[100,](#page-14-22)
    [264–](#page-17-16)[270\]](#page-17-17) show that commodity DRAM can perform massively-parallel
    computations (e.g., at the granularity of an 8 KiB DRAM row) by exploiting the
    underlying analog behavior of DRAM operations (e.g., charge sharing between cells).
    These works show that such computations can significantly improve overall system
    performance and energy-efficiency by both (1) reducing the amount of data transferred
    between the processor and DRAM and (2) exploiting the relatively high throughput
    of rowgranularity operations. For example, Gao et al. [\[55\]](#page-14-16) show
    that in-DRAM 8-bit vector addition is 9.3× more energy-efficient than the same
    computation in the processor, primarily due to avoiding the need for off-chip
    data transfers. Similarly, Olgun et al. [\[90\]](#page-14-19) experimentally demonstrate
    that in-DRAM copy and initialization techniques can improve the performance of
    system-level copy and initialization operations by 12.6× and 14.6×, respectively.


    <span id="page-5-2"></span>4.1.2. Application to Today''s Commodity DRAM Chips.
    Unfortunately, both reducing DRAM access timings and exploiting DRAM''s massively-parallel
    analog behavior are discouraged by today''s separation of concerns. In both cases,
    new DRAM access timings must be determined that ensure new or modified DRAM operations
    can be performed predictably and reliably under all operating conditions.


    To identify new access timings, the majority of prior works [\[13,](#page-13-33)
    [16,](#page-13-28) [17,](#page-13-34) [22,](#page-13-10) [52,](#page-13-37) [55,](#page-14-16)
    [73,](#page-14-0) [75,](#page-14-18) [91,](#page-14-20) [171,](#page-16-4) [263,](#page-17-15)
    [271](#page-17-18)[–273\]](#page-18-0) perform extensive experimental characterization
    studies across many (e.g., hundreds or thousands of) DRAM chips. These studies
    account for three primary sources of variation that affect the access timings
    of a given memory location. First, process variation introduces random variations
    between DRAM chip components (e.g., cells, rows, columns). Second, a manufacturer''s
    particular circuit design introduces structural variation (called design-induced
    variation [\[78\]](#page-14-2)) that deterministically affects access timings
    based on a component''s location in the overall DRAM design (e.g., cells along
    the same bitline [\[74\]](#page-14-1), cells at the borders of internal storage
    arrays [\[78\]](#page-14-2)). Third, the charge level of a DRAM cell varies over
    time due to leakage and the effects of DRAM accesses [\[59,](#page-14-6) [198\]](#page-16-24).
    Experimentally determining the new predictable and reliable access timings requires
    properly accounting for all three sources of variation under all operating conditions.


    For a typical DRAM consumer, determining new access timings using a custom DRAM
    testing methodology is impractical without assistance from DRAM producers. Choosing
    runtime (e.g., data and access patterns) and environmental (e.g., temperature,
    voltage) testing conditions in a meaningful way requires understanding the error
    mechanisms involved in timingrelated errors [\[68\]](#page-14-23), including (but
    not limited to) microarchitectural design details, such as internal substructure
    dimensions (e.g., subarray sizing) [\[74,](#page-14-1) [78\]](#page-14-2), the
    correspondence between logical DRAM bus addresses and physical cell locations
    [\[16,](#page-13-28) [22,](#page-13-10) [67\]](#page-14-24), and the order of
    rows refreshed by each auto-refresh operation [\[198\]](#page-16-24). Therefore,
    consumers who lack trustworthy access to this information are discouraged from
    exploring improvements to the commodity DRAM access latency.


    ### <span id="page-6-0"></span>4.2. Study 2: DRAM Refresh Overheads


    DRAM refresh overheads are a key design concern in modern systems as discussed
    in Section [3.1.3.](#page-4-5) System-memory cooperation can overcome this problem
    by eliminating or otherwise mitigating unnecessary refresh operations, thereby
    improving overall system performance and energy efficiency.


    4.2.1. Solutions to Reduce DRAM Refresh Overheads. Prior works [\[56,](#page-14-25)
    [61,](#page-14-26) [64–](#page-14-27)[70,](#page-14-28) [79,](#page-14-29) [80,](#page-14-9)
    [82,](#page-14-7) [86,](#page-14-3) [89,](#page-14-10) [92,](#page-14-4) [94,](#page-14-12)
    [103,](#page-14-30) [104\]](#page-14-11) address refresh overheads by exploiting
    the fact that most refresh operations are unnecessary.[8](#page-6-2) The standardized
    DRAM refresh algorithm refreshes all cells frequently (i.e., at the worst-case
    rate) to simplify DRAM refresh and guarantee correctness. However, each cell''s
    data retention characteristics vary significantly due to a combination of data-dependence
    [\[65,](#page-14-31) [67,](#page-14-24) [68,](#page-14-23) [92,](#page-14-4) [150\]](#page-15-27)
    and process variation [\[80,](#page-14-9) [86,](#page-14-3) [104,](#page-14-11)
    [143,](#page-15-22) [150,](#page-15-27) [276,](#page-18-1) [277\]](#page-18-2).
    As a result, eliminating unnecessary refresh operations provides significant power
    reduction and performance improvement. For example, Liu et al. [\[80\]](#page-14-9)
    demonstrate an average energyper-access and system performance improvement of
    8.3% and 4.1%, respectively, for 4 Gib chips (49.7% and 107.9% for 64 Gib chips)
    when relaxing the refresh rate at the row granularity. Therefore, reducing refresh
    overheads potentially benefits all DRAM-based systems.


    <span id="page-6-4"></span>4.2.2. Application to Today''s Commodity DRAM Chips.
    Reducing unnecessary refresh operations in commodity DRAM chips comprises two
    key steps. First, the memory controller must reduce the frequency of periodic
    refresh operations. This is achievable (though not necessarily supported to arbitrary
    values) using commodity DRAM chips because the memory controller manages DRAM
    refresh timings. For example, the memory controller might relax the rate at which
    it issues refresh operations to half of the DDRn standard of 3.9 or 7.8 µs, which
    is supported by standards at extended temperature ranges [\[33,](#page-13-15)
    [34,](#page-13-16) [44,](#page-13-25) [138,](#page-15-17) [139\]](#page-15-18),
    or even to over an order of magnitude less often [\[64,](#page-14-27) [80,](#page-14-9)
    [86,](#page-14-3) [103\]](#page-14-30).


    Second, the system must mitigate any errors that may occur within the small number
    of DRAM cells that require frequent refreshing. Doing so requires using either
    (i) additional operations to mitigate data loss (e.g., issuing extra row activations
    to


    those cells'' rows [\[80\]](#page-14-9)) or (ii) supplementary error-mitigation
    mechanisms within processor (e.g., ECC [\[94\]](#page-14-12) and/or bit-repair
    techniques [\[79,](#page-14-29) [86,](#page-14-3) [103\]](#page-14-30)). Although
    both approaches can introduce new performance and energy overheads, the benefits
    of reducing unnecessary refresh operations outweigh the overheads introduced [\[56,](#page-14-25)
    [79,](#page-14-29) [80,](#page-14-9) [86,](#page-14-3) [89,](#page-14-10) [92,](#page-14-4)
    [94,](#page-14-12) [103,](#page-14-30) [104,](#page-14-11) [278\]](#page-18-5).
    For example, Liu et al. [\[80\]](#page-14-9) project that DRAM refresh overheads
    cause a 187.6% increase in the energy-per access and a 63.7% system performance
    degradation for 64 Gib chips. By reducing the overall number of DRAM refresh operations,
    the authors'' proposal (RAIDR) mitigates these overheads by 49.7% and 107.9%,
    respectively.


    Unfortunately, this second step is difficult for the average DRAM consumer because
    it requires a trustworthy method for discriminating DRAM cells'' data retention
    characteristics. These characteristics vary with both the DRAM chip circuit design
    (e.g., random cell-to-cell variations, locations of true and anti-cells [\[115,](#page-15-40)
    [117,](#page-15-26) [150\]](#page-15-27)) and usage characteristics (e.g., operating
    conditions such as voltage and temperature, workload access patterns), so identifying
    which cells to refresh more often requires access to both internal knowledge of
    a given DRAM chip and how the chip''s end use will impact data retention. Prior
    works propose reliability testing [\[65,](#page-14-31) [67,](#page-14-24) [79,](#page-14-29)
    [80,](#page-14-9) [82,](#page-14-7) [92\]](#page-14-4) and monitoring [\[94,](#page-14-12)
    [229,](#page-17-7) [279,](#page-18-6) [280\]](#page-18-7) techniques to work around
    the lack of this knowledge, which the separation of concerns hides from DRAM consumers.
    Ultimately, this means that system-memory cooperation to improve the standardized
    DRAM refresh algorithm are discouraged today.


    #### <span id="page-6-1"></span>4.3. Study 3: RowHammer Mitigation


    Section [3.2.2](#page-4-6) discusses the severity of the RowHammer vulnerability,
    motivating the need for secure defenses beyond those currently implemented. System-memory
    cooperative mechanisms are capable of supplementing these defenses from outside
    of the DRAM chip to provide strong security without requiring changes to DRAM
    chip hardware. Such a solution is attractive for a system designer with interest
    in building a secure system because the designer can guarantee correctness using
    their own methods rather than taking the word of external parties [\[45,](#page-13-26)
    [46\]](#page-13-38).


    <span id="page-6-3"></span>4.3.1. Solutions to Securely Mitigate RowHammer. We
    classify secure RowHammer mitigations into four groups based on categorization
    by recent work [\[107\]](#page-15-42).


    - 1. Access-agnostic mitigation hardens a DRAM chip against RowHammer independently
    of the memory access pattern. This includes increasing the overall DRAM refresh
    rate [\[29,](#page-13-20) [281,](#page-18-8) [282\]](#page-18-9) and memory-wide
    error correction and/or integrity-checking mechanisms such as strong ECC [\[29,](#page-13-20)
    [45,](#page-13-26) [260\]](#page-17-19). These mechanisms are algorithmically
    simple but can introduce significant system hardware, performance, and/or energy-efficiency
    overheads (e.g., a large number of additional refresh operations [\[28,](#page-13-13)
    [29,](#page-13-20) [207\]](#page-16-14)).

    - 2. Proactive mitigations [\[29,](#page-13-20) [57,](#page-14-32) [85,](#page-14-33)
    [107\]](#page-15-42) adjust the DRAM access pattern to prevent the possibility
    of RowHammer errors.

    - 3. Physically isolating mitigations [\[50,](#page-13-41) [76,](#page-14-34)
    [95,](#page-14-35) [102,](#page-14-15) [245,](#page-17-20) [283–](#page-18-10)[287\]](#page-18-11)
    physically separate data such that accesses to one portion


    <span id="page-6-2"></span><sup>8</sup>Latency-hiding techniques (e.g, prefetching,
    memory command scheduling, on-chip caching, etc.) and parallelization of refresh
    and access operations [\[21,](#page-13-36) [211,](#page-16-25) [215,](#page-16-21)
    [274,](#page-18-3) [275\]](#page-18-4) help mitigate performance overheads but
    do not change the total number of refresh operations issued. As a result, such
    techniques do not reduce refresh energy consumption. These techniques are also
    imperfect in many cases where latency-hiding is impractical (e.g., row conflicts
    between refresh and access commands, larger memory footprints than available caching
    resources) [\[208,](#page-16-15) [214,](#page-16-20) [215,](#page-16-21) [274\]](#page-18-3).


    of the data cannot cause RowHammer errors in another.


    4. Reactive mitigations [\[29,](#page-13-20) [34,](#page-13-16) [288–](#page-18-12)[304\]](#page-18-13)
    identify symptoms of an ongoing RowHammer attack (e.g., excessive row activations)
    and issue additional row activation or refresh operations to prevent bit-flips
    from occurring.


    Choosing a secure RowHammer defense for a given system depends on the system''s
    particular threat model and the overheads (e.g., performance, energy, hardware
    area, complexity) it can tolerate. For example, if DRAM is accessible only through
    processor cores (e.g., peripherals are incapable of direct memory access), secure
    defenses may be possible solely through on-chip cache management [\[305\]](#page-18-14).


    <span id="page-7-7"></span>4.3.2. Application to Today''s Commodity DRAM Chips.
    Unfortunately, implementing secure RowHammer defenses is discouraged in today''s
    DRAM chips because the separation of concerns hides the mechanics of how RowHammer
    occurs from DRAM consumers. The defenses discussed throughout Section [4.3.1](#page-6-3)
    all require understanding one or more of a chip''s RowHammer error characteristics,
    which are summarized in Table [1.](#page-7-2) The first is known as HCfirst [\[28,](#page-13-13)
    [118\]](#page-15-4) or RowHammer Threshold [\[29,](#page-13-20) [107,](#page-15-42)
    [306\]](#page-18-15), which describes the worst-case number of RowHammer memory
    accesses required to induce a bit-flip. The second is known as the blast radius
    [\[28,](#page-13-13) [29\]](#page-13-20), which describes how many rows are affected
    by hammering a single row. The third is the DRAM''s internal physical row address
    mapping [\[29,](#page-13-20) [307\]](#page-18-16), which is used to identify the
    locations of victim rows.


    <span id="page-7-2"></span>


    |                      | Required Information |              |             |

    |----------------------|----------------------|--------------|-------------|

    | Strategy             | HCfirst              | Blast Radius | Row Mapping |

    | Access-Agnostic      | ✓                    |              |             |

    | Proactive            | ✓                    | ✓            |             |

    | Physically Isolating | ✓                    | ✓            | ✓           |

    | Reactive             | ✓                    | ✓            | ✓           |


    Table 1: Information needed by each of the four RowHammermitigation strategies.


    All three RowHammer error characteristics vary between DRAM manufacturers, chips,
    and cells based on a combination of random process variation, a chip''s particular
    circuit design (including yield-management techniques such as postmanufacturing
    repair, target row refresh, and error correcting codes), and operating conditions
    such as temperature and voltage [\[28–](#page-13-13)[31,](#page-13-42) [118,](#page-15-4)
    [152,](#page-15-41) [300,](#page-18-17) [308](#page-18-18)[–311\]](#page-18-19).


    Without trustworthy access this information, DRAM consumers are discouraged from
    adopting secure RowHammer defenses. To work around this limitation, proposals
    for secure RowHammer defenses conduct extensive experimental testing to estimate
    RowHammer error characteristics that are needed to design and/or configure their
    proposals. Unfortunately, similar to efforts that improve DRAM access latency
    and refresh timings discussed in Sections [4.2.2](#page-6-4) and [4.1.2,](#page-5-2)
    deploying these methods in practice is impractical for most consumers. These observations
    are consistent with prior works [\[45,](#page-13-26) [46,](#page-13-38) [231\]](#page-17-8)
    that discuss the difficulty in practically determining and relying on this information
    without support from DRAM manufacturers.


    #### <span id="page-7-0"></span>4.4. Study 4: Improving Memory Reliability


    Main memory reliability is a key system design concern because memory errors can
    cause data loss or system failure if mismanaged (discussed further in Section
    [3.2.1\)](#page-4-7). Systemmemory cooperation can supplement the memory chip
    with additional mechanisms to improve its base reliability beyond what producers
    alone can provide.[9](#page-7-3)


    <span id="page-7-1"></span>4.4.1. Solutions to Improve Memory Reliability. Systemmemory
    cooperative solutions that DRAM consumers can implement to improve memory reliability
    identify and/or address memory errors before they impact the system at large.
    Hardware solutions include rank-level ECC [\[51,](#page-13-43) [53,](#page-13-44)
    [54,](#page-13-45) [62,](#page-14-36) [63,](#page-14-37) [71,](#page-14-38) [72,](#page-14-39)
    [81,](#page-14-40) [87,](#page-14-14) [93,](#page-14-41) [101,](#page-14-42) [105,](#page-14-43)
    [108\]](#page-15-43), rank-level ECC scrubbing [\[94,](#page-14-12) [221,](#page-17-0)
    [279,](#page-18-6) [280,](#page-18-7) [315,](#page-18-20) [315–](#page-18-20)[319\]](#page-18-21),
    and bit repair techniques [\[79,](#page-14-29) [86,](#page-14-3) [320](#page-18-22)[–327\]](#page-18-23).
    Software-based approaches include retiring known-bad memory pages [\[49,](#page-13-29)
    [60,](#page-14-44) [83,](#page-14-45) [84,](#page-14-13) [88,](#page-14-46) [103\]](#page-14-30),
    and predicting failures [\[328–](#page-18-24) [333\]](#page-19-0).


    These solutions all enable DRAM consumers to adapt unreliable DRAM chips to systems
    that require reliable main memory at reasonable cost.[10](#page-7-4) For example,
    HOTH [\[321\]](#page-18-25) supplements rank-level ECC with a cache-like hardware
    mechanism to track faulty memory locations, enabling the system to detect and
    correct one additional error for each ECC word (i.e., extend SECDED to 2EC3ED).


    4.4.2. Application to Today''s Commodity DRAM Chips. Unfortunately, exposing memory
    errors outside of the DRAM chip is at best a gray area within the separation of
    concerns between DRAM producers and consumers. Commodity DRAM is designed to work
    for a wide variety of systems at a reasonable (albeit unspecified)[11](#page-7-5)
    error rate. In general, a consumer who is concerned about the remaining errors
    they may encounter must design and build their own solutions (i.e., outside of
    the DRAM chip) to tolerate memory errors.[12](#page-7-6)


    However, these solutions fundamentally rely on understanding how those errors
    might manifest in the first place. Each error-mitigation mechanism targets a particular
    error model, which defines the scope of the errors that it is designed to mitigate.
    As a result, although a given mechanism efficiently mitigates errors within its
    target error model, it may fail to do so if errors no longer fit the model. In
    such cases, a different error-mitigation mechanism (or possibly, a combination
    of multiple mechanisms) may be more suitable.


    For example, a coarse-grained approach such as page retirement [\[49,](#page-13-29)
    [60,](#page-14-44) [83,](#page-14-45) [84,](#page-14-13) [88,](#page-14-46) [103\]](#page-14-30)
    efficiently mitigates a small number


    <span id="page-7-3"></span><sup>9</sup>These mechanisms commonly fall under the
    umbrella of memory reliability, availability and serviceability (RAS) features
    [\[312](#page-18-26)[–314\]](#page-18-27).


    <span id="page-7-4"></span><sup>10</sup>Consumers with exceptional reliability
    needs, such as those targeting extreme or hostile environments (e.g., military,
    automotive, industrial, extraterrestrial), may take more extreme measures (e.g.,
    custom components [\[158,](#page-15-31) [159,](#page-15-32) [334](#page-19-1)[–340\]](#page-19-2),
    redundant resources [\[93,](#page-14-41) [341,](#page-19-3) [342\]](#page-19-4))
    to ensure that memory errors do not compromise their systems.


    <span id="page-7-5"></span><sup>11</sup>Academic works speculate that commodity
    DRAM targets a bit error rate (BER) within the range of 10−<sup>16</sup> − 10−<sup>12</sup>
    [\[86,](#page-14-3) [92,](#page-14-4) [321,](#page-18-25) [343\]](#page-19-5),
    but we are not aware of industry-provided values.


    <span id="page-7-6"></span><sup>12</sup>Even designers who adopt custom DRAM solutions
    that sacrifice the cost advantages of commodity memory (e.g., high-reliability
    DRAM [\[158,](#page-15-31) [159\]](#page-15-32)) may supplement the DRAM chips
    with additional error-mitigation mechanisms outside of the DRAM chip.


    of errors at fixed bit positions. However, page retirement exhibits significant
    capacity and performance overheads at high error rates or when mitigating errors
    that change positions over time [\[83,](#page-14-45) [84,](#page-14-13) [344\]](#page-19-6).
    In contrast, a fine-grained hardware-based approach such as a block error-correcting
    code [\[345–](#page-19-7)[350\]](#page-19-8) can efficiently mitigate a limited
    number of randomly-distributed errors but can fail silently (and even exacerbate
    the number of errors present [\[38,](#page-13-22) [40,](#page-13-30) [115,](#page-15-40)
    [119,](#page-15-5) [226,](#page-17-5) [230,](#page-17-6) [351\]](#page-19-9))
    when its correction capability is exceeded. We conclude that it is essential for
    the system designer to know when and how errors occur in a given memory chip in
    order to make an informed choice of which error-mitigation mechanism to use in
    a particular system.


    Unfortunately, DRAM consumers generally do not have access to definitive error
    models for commodity DRAM chips. Obtaining this information without cooperation
    from producers requires extensive reliability testing (as discussed throughout
    Section [5\)](#page-8-0), guidance from external (possibly untrustworthy) sources,
    or otherwise reasoning about memory errors at a high level (e.g., disregarding
    uninteresting failure modes). As a result, the separation of concerns between
    DRAM producers and consumers effectively discourages consumers from exploring
    the full design space for error-mitigation mechanisms.


    ## <span id="page-8-0"></span>5. DRAM Reliability Testing


    As our case studies in Sections [4](#page-5-0) show, enabling DRAM consumers to
    reason about how a given DRAM operating point way will impact its reliable operation
    is essential for enabling them to adopt system-memory cooperative mechanisms to
    address DRAM scaling challenges. This section formalizes the information that
    a DRAM consumer may need (but does not necessarily have access to today) in order
    to quantitatively reason about DRAM reliability.


    #### 5.1. Information Flow During Testing


    Figure [3](#page-8-1) describes the flow of information necessary for a consumer
    to quantitatively estimate[13](#page-8-2) a DRAM chip''s error characteristics
    ⃝5 starting from basic properties of the chip ⃝1 . In principle, these characteristics
    can comprise any aspect of DRAM reliability that a consumer wants to quantify
    while exploring their system''s design and/or configuration space.


    <span id="page-8-2"></span><sup>13</sup>"Estimate" because, in general, no model
    or experiment is likely to be perfect, including those provided by manufacturers.


    Examples include: (1) worst-case error rates (e.g., bit error rate (BER) or failures
    in time (FIT)) across a given set of operating points; (2) a profile of error-prone
    memory locations; or (3) a list of error-free operating points (e.g., as identified
    in a shmoo analysis [\[352\]](#page-19-10)). The error characteristics can be
    estimated in two different ways: testing or modeling.


    5.1.1. Determination from Testing. First, a consumer may estimate error characteristics
    using measurements from detailed experimental testing ⃝3 across a variety of operating
    conditions. Examples of measured quantities include: aggregate error rates, per-cell
    probabilities of error, and spatial/temporal error distributions. These measurements
    can be made using testing infrastructures ranging from industry-standard largescale
    testing equipment [\[353,](#page-19-11) [354\]](#page-19-12) to home-grown tools
    based on commodity FPGAs [\[16,](#page-13-28) [29,](#page-13-20) [55,](#page-14-16)
    [65,](#page-14-31) [90,](#page-14-19) [171,](#page-16-4) [263,](#page-17-15) [272,](#page-17-21)
    [355](#page-19-13)[–358\]](#page-19-14) or DRAM-based computing systems [\[236,](#page-17-22)
    [262,](#page-17-14) [273,](#page-18-0) [359,](#page-19-15) [360\]](#page-19-16).


    To conduct accurate and rigorous testing, the consumer must use an effective test
    methodology ⃝2 that suits the particular DRAM chip under test. Prior works extensively
    study key aspects of effective test methodologies, including appropriate data
    and access patterns, the effects of enabling/disabling DRAM chip features such
    as target row refresh (TRR) [\[23,](#page-13-11) [28,](#page-13-13) [32,](#page-13-14)
    [48,](#page-13-27) [301\]](#page-18-28) and on-die error correcting codes (on-die
    ECC) [\[38–](#page-13-22)[40,](#page-13-30) [87,](#page-14-14) [115,](#page-15-40)
    [143,](#page-15-22) [166,](#page-16-2) [168,](#page-16-26) [230,](#page-17-6)
    [361–](#page-19-17)[363\]](#page-19-18), and the viability of different DRAM command
    sequences (e.g., sequences that enable in-DRAM row copy operations [\[55,](#page-14-16)
    [90,](#page-14-19) [96,](#page-14-21) [364\]](#page-19-19), true randomnumber
    generation [\[75,](#page-14-18) [91,](#page-14-20) [266,](#page-17-23) [365\]](#page-19-20),
    and physically unclonable functions [\[73,](#page-14-0) [267\]](#page-17-24)).


    In turn, choosing an effective test methodology requires knowing basic properties
    about a DRAM chip''s design and/or error mechanisms ⃝1 . For example, DRAM manufacturer''s
    design choices for the sizes of internal storage arrays (i.e., mats [\[12,](#page-13-9)
    [78,](#page-14-2) [91,](#page-14-20) [366\]](#page-19-21)), charge encoding conventions
    of each cell (i.e., the true- and anti-cell organization [\[117,](#page-15-26)
    [150\]](#page-15-27)), use of ondie reliability-improving mechanisms (e.g., on-die
    ECC, TRR), and organization of row and column addresses all play key roles in
    determining whether a DRAM chip is likely to show errors from key error mechanisms
    (e.g., data retention [\[115,](#page-15-40) [117,](#page-15-26) [150,](#page-15-27)
    [277,](#page-18-2) [367–](#page-19-22)[369\]](#page-19-23), access-latency-related
    failures [\[14,](#page-13-35) [16,](#page-13-28) [22,](#page-13-10) [52,](#page-13-37)
    [74,](#page-14-1) [77,](#page-14-5) [78,](#page-14-2) [91\]](#page-14-20), and
    RowHammer [\[25,](#page-13-19) [26,](#page-13-40) [29,](#page-13-20) [30,](#page-13-46)
    [232,](#page-17-9) [370\]](#page-19-24)).


    5.1.2. Determination from Modeling. Second, the consumer may make predictions
    from analytical or empirical er-


    <span id="page-8-1"></span>![](_page_8_Figure_12.jpeg)


    Figure 3: Flow of information necessary to determine key error characteristics
    for a given DRAM device.


    ror models ⃝4 based on a previous understanding of DRAM errors (e.g., from past
    experiments or scientific studies). Examples of such error models include: analytical
    models based on understanding DRAM failure modes (e.g., sources of runtime faults
    [\[51,](#page-13-43) [60,](#page-14-44) [149,](#page-15-25) [371](#page-19-25)[–373\]](#page-19-26)),
    parametric statistical models that provide useful summary statistics (e.g., lognormal
    distribution of cell data-retention times [\[276,](#page-18-1) [277,](#page-18-2)
    [374](#page-19-27)[–380\]](#page-19-28), exponential distribution of the time-in-state
    of cells susceptible to variable-retention time (VRT) [\[65,](#page-14-31) [94,](#page-14-12)
    [150,](#page-15-27) [166,](#page-16-2) [367,](#page-19-22) [381](#page-19-29)[–389\]](#page-19-30)),
    physics-based simulation models (e.g., TCAD [\[232,](#page-17-9) [374,](#page-19-27)
    [390–](#page-20-0) [392\]](#page-20-1) and SPICE models [\[14,](#page-13-35) [59,](#page-14-6)
    [78,](#page-14-2) [106,](#page-15-38) [109,](#page-15-39) [283,](#page-18-10)
    [393](#page-20-2)[–395\]](#page-20-3)), and empirically-determined curves that
    predict observations well (e.g., single-bit error rates [\[30,](#page-13-46) [65,](#page-14-31)
    [67,](#page-14-24) [92,](#page-14-4) [94,](#page-14-12) [150\]](#page-15-27)).
    Similar to testing, using error models to predict error characteristics ultimately
    relies on understanding the DRAM chip being tested because the accuracy of the
    predictions requires choosing appropriate models and model parameters (e.g., through
    testing ⃝3 or directly from fundamental chip design properties ⃝1 ).


    ### 5.2. Access to Modeling and Testing Information


    Figure [3](#page-8-1) shows that determining a DRAM chip''s error characteristics
    through modeling or testing ultimately relies on understanding the chip''s fundamental
    design properties. This reliance can be implicit (e.g., inherent within a pre-existing
    workflow designed for a specific chip) or explicit (e.g., chosen as part of a
    home-grown testing methodology). Therefore, a consumer must be vigilant of the
    information they (perhaps unknowingly) rely upon at each step of their design
    process concerning commodity DRAM.


    Fortunately, the consumer only needs to be concerned with the information flow
    at the children of a node whose information is already known from a trustworthy
    source. For example, a consumer who wants to identify the locations of error-prone
    cells (i.e., ⃝5 ) using testing need not be concerned with chip design properties
    (i.e., ⃝1 ) if DRAM manufacturers provide appropriate test methodologies (i.e.,
    ⃝2 ) or detailed test results (i.e., ⃝3 ). Unfortunately, to our knowledge, neither
    DRAM standards nor manufacturers provide the information in any of the nodes today,
    much less in a clear, industry-validated manner. Therefore, the consumer lacks
    a base of trustworthy information to build upon. This creates a barrier to entry
    for a consumer who wants to explore optimizations to commodity DRAM by compromising
    the consumer''s ability to make well-informed or effective decisions.


    In general, except for the few major DRAM customers who may be able to secure
    confidentiality agreements,[14](#page-9-1) consumers would need to rely on (possibly
    incorrect or incomplete) inferences or assumptions based on domain knowledge or
    reverseengineering studies (e.g., similar in spirit to [\[16,](#page-13-28) [23,](#page-13-11)
    [24,](#page-13-31) [28,](#page-13-13) [32,](#page-13-14) [40,](#page-13-30) [74,](#page-14-1)
    [78,](#page-14-2) [92,](#page-14-4) [114,](#page-15-3) [117,](#page-15-26) [150–](#page-15-27)[155\]](#page-15-28))
    that are not verified or supported by the DRAM industry.[15](#page-9-2) As a result,
    the need for assumptions


    can discourage practitioners from exploring the full design space even when a
    given design choice is otherwise beneficial. We conclude that the lack of information
    transparency is a serious impediment to adopting new designs for addressing DRAM
    scaling challenges today.


    ## <span id="page-9-0"></span>6. Rethinking Today''s Separation of Concerns


    Our case studies throughout Section [4](#page-5-0) demonstrate that although DRAM
    consumers have explored new and creative ways to address DRAM scaling challenges,
    their solutions face serious practicality concerns because of limited access to
    information about DRAM chip reliability. In this section, we hypothesize that
    the unavailability of this information is caused by a lack of transparency within
    DRAM standards which provide control over, but not insight into, DRAM operations.


    #### 6.1. The Problem of Information Unavailability


    Addressing DRAM scaling challenges fundamentally requires understanding how those
    challenges impact system operation. Therefore, it is unsurprising that reliability
    analysis and testing is central to each of the approaches we survey in our case
    studies. In some cases, solutions explicitly require reliability testing (e.g.,
    identifying and monitoring the physical locations of error-prone cells). Other
    solutions implicitly rely on the results of reliability analysis (e.g., configuring
    RowHammer defenses based on a chip''s degree of vulnerability). Ultimately, deploying
    consumer-driven solutions to DRAM scaling challenges requires some degree of understanding
    of how different (representative) operating conditions impact DRAM and overall
    system reliability.


    Unfortunately, the separation of concerns does not convey this information, which
    discourages consumers from adopting such solutions in practice. For example, recent
    works [\[45,](#page-13-26) [46\]](#page-13-38) discuss the pitfalls of designing
    RowHammer defense mechanisms that rely on knowledge of how RowHammer errors behave
    (e.g., HCfirst, dependence on a chip''s internal cell organization), calling into
    question the practicality of accurately determining these details given an arbitrary
    DRAM chip. Knowing or determining this information is essential to guarantee protection
    against RowHammer. However, determining it without guidance from DRAM manufacturers
    requires trusting in a home-grown testing or reverse-engineering methodology,
    which ultimately relies on knowledge of DRAM chip details that likely needs to
    be assumed or inferred (as discussed in Sections [4.3.2](#page-7-7) and [5\)](#page-8-0).


    As a result, system-memory cooperative solutions to overcome scaling challenges
    largely remain theoretical ideas or proofs-of-concept based on performance and
    reliability characteristics that are assumed, inferred, or reverse-engineered
    from a limited set of observations and DRAM products (e.g., using in-house experimental
    studies [\[13,](#page-13-33) [16,](#page-13-28) [17,](#page-13-34) [22,](#page-13-10)
    [29,](#page-13-20) [52,](#page-13-37) [55,](#page-14-16) [73–](#page-14-0)[75,](#page-14-18)
    [78,](#page-14-2) [91,](#page-14-20) [150,](#page-15-27) [171,](#page-16-4) [263,](#page-17-15)
    [266,](#page-17-23) [267,](#page-17-24) [271–](#page-17-18)[273\]](#page-18-0))
    without DRAM manufacturers'' support.


    Unfortunately, this lack of a trustworthy base of information to build upon can
    discourage even the most enterprising consumers from exploring new designs. Such
    exploration would


    <span id="page-9-1"></span><sup>14</sup>Even under confidentiality, DRAM manufacturers
    may be unwilling to reveal certain proprietary design details (e.g., on-die error
    correction [\[40,](#page-13-30) [224\]](#page-17-3), target row refresh [\[46\]](#page-13-38))
    or provide specifically requested numbers.


    <span id="page-9-2"></span><sup>15</sup>DRAM manufacturers may make assumptions
    during their own testing. However, they have full transparency into their own
    designs (i.e, the root node in the information flow), so they can make the most
    informed decision.


    <span id="page-10-1"></span>


    | Design Property                                                       | Reverse-Engineered
    By                     | Use-Case(s) Relying on Knowing the Property                                                                                                                                                                                                          |

    |-----------------------------------------------------------------------|-------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|

    | Cell charge encoding convention<br>(i.e., true- and anti-cell layout) | Testing
    [92, 115, 117, 150]               | Data-retention error modeling and testing
    for mitigating refresh<br>overheads (e.g., designing worst-case test patterns)
    [68, 117, 150]                                                                                                              |

    | On-die ECC details                                                    | Modeling
    and testing [40,<br>115]         | Improving reliability (e.g., designing ECC
    within the memory<br>controller) [38, 41, 119, 167], mitigating RowHammer [28,
    32, 48, 260]                                                                                                               |

    | Target row refresh (TRR) details                                      | Testing
    [23, 32]                          | Modeling and mitigating RowHammer [23, 32,
    48]                                                                                                                                                                                                       |

    | Mapping between internal and external<br>row addresses                | Testing
    [28, 78, 114, 151,<br>153, 245]   | Mitigating RowHammer [28, 29, 114, 151, 152]                                                                                                                                                                                                         |

    | Row addresses refreshed by each<br>refresh operation                  | Testing
    [32]                              | Mitigating RowHammer [32], improving access
    timings [106, 198]                                                                                                                                                                                       |

    | Substructure organization (e.g., cell<br>array dimensions)            | Modeling
    [78] and<br>testing [16, 74, 78] | Improving DRAM access timings [16, 74, 78]
    and security [287]                                                                                                                                                                                        |

    | Analytical model parameters<br>(e.g., bitline capacitance)            | Modeling
    and testing [150,<br>277]        | Developing and using error models for improving
    overall<br>reliability [376], mitigating refresh overheads (e.g., data-retention
    [277,<br>371, 375] and VRT [383, 384] models), improving access timings [78],<br>and
    mitigating RowHammer [30, 396] |


    #### Table 2: Basic DRAM chip microarchitectural design properties that are typically
    assumed or inferred for experimental studies.


    require weighing any potential benefits (e.g., improved performance, security,
    etc.) against both: (1) risks (e.g., failures in the field) associated with potentially
    operating outside manufacturer recommendations and (2) limitations due to compatibility
    with only a subset of all commodity DRAM products (e.g., only those that have
    been accurately reverse-engineered). These risks and limitations are a serious
    barrier to adoption; therefore, we conclude that the lack of information transparency
    today discourages system designers from exploring alternative designs that have
    been shown to provide tangible benefits.


    #### 6.2. DRAM Standards Lack Transparency


    Historically, DRAM standards have not discussed DRAM chip reliability because
    the separation of concerns assigns DRAM producers (near)[16](#page-10-0) full
    responsibility to address DRAM-related reliability concerns. Therefore, DRAM producers
    are expected to address reliability-related issues, leaving consumers to integrate
    reliable DRAM chips. As technology scaling continues to degrade DRAM chip reliability,
    modern DRAM standards are exposing new reliability-related features, such as on-die
    ECC scrubbing [\[34,](#page-13-16) [119,](#page-15-5) [319\]](#page-18-21), post-package
    repair [\[33,](#page-13-15) [34,](#page-13-16) [216,](#page-16-22) [222,](#page-17-1)
    [223\]](#page-17-2), target row refresh [\[23,](#page-13-11) [32\]](#page-13-14),
    and refresh management [\[34,](#page-13-16) [47\]](#page-13-39). Unfortunately,
    more general reasoning about reliability remains elusive for consumers at large.


    We believe that this state of affairs naturally arises from establishing a a clear
    and explicit interface between producers and consumers. Consequently, ensuring
    that the standards leave enough flexibility for diverse consumer use-cases requires
    careful and explicit attention. This is because the standards are susceptible
    to abstraction inversion [\[397\]](#page-20-5), a design antipattern in which
    a previously agreed-upon interface becomes an obstacle, forcing system designers
    to re-implement basic functionality in terms of the outdated abstraction. A rigid
    interface limits what is and is not possible, potentially requiring unproductive
    reverse-engineering to work around.


    We contend that the difficulty that consumers face today in addressing DRAM scaling
    challenges clearly indicates abstraction inversion: the separation of concerns
    has aged without sufficient attention to flexibility. Although a fixed operating
    point defines a clear interface, we believe that leaving room for (and potentially
    even encouraging) different operating points is essential today.


    #### <span id="page-10-2"></span>6.3. Benefits for Both Producers and Consumers


    Today''s separation of concerns discourages not only consumers from exploring
    new ways to work with commodity DRAM chips but also producers from adopting consumer-driven
    ideas that help address DRAM scaling challenges. In other words, the separation
    of concerns effectively discourages both parties from exploring solutions outside
    their areas of concern. As a result, neither party explores the full design space
    surrounding commodity DRAM chips.


    We believe that rethinking the separation to encourage cooperation stands to benefit
    all aspects of DRAM technology, encompassing both the producers and consumers
    who build and use DRAM, respectively. Producers gain access to a broad base of
    innovation from consumers who prototype solutions (with or without additional
    investment from producers themselves), thereby creating new opportunities for
    producers to make DRAM a more competitive product. Consumers gain access to new
    ways to improve system-level metrics, such as performance and energy efficiency,
    that were previously not practical. Ultimately, both producers and consumers benefit
    from the best possible version of DRAM technology.


    ## 7. DRAM Standards as the Solution


    Separating design concerns between producers and consumers is practical for enabling
    each party to focus on their respective areas of expertise. However, we recommend
    that the separation be crafted in a way that not only enables both parties to
    help address DRAM scaling challenges, but ideally encourages and aids it. To achieve
    both goals, we propose extending DRAM standards in a way that enables consumers
    to make informed


    <span id="page-10-0"></span><sup>16</sup>Consumers with exceptional reliability
    requirements may then choose to supplement DRAM chips with additional error-mitigation
    mechanisms, as discussed in Section [3.2.1.](#page-4-7)


    decisions about how their design choices will affect a DRAM chip''s reliable operation.
    In other words, instead of modifying DRAM designs, we advocate modifying standards
    to facilitate transparency of DRAM reliability characteristics. Armed with this
    information, consumers can freely explore how to best use commodity DRAM chips
    to solve their own design challenges while preserving the separation of concerns
    that allows DRAM designers to focus on building the best possible standardscompliant
    DRAM chips.


    #### <span id="page-11-0"></span>7.1. Choosing Information to Release


    We identify what information to release based on our analysis of information flow
    throughout DRAM reliability testing in Section [5.](#page-8-0) We observe that,
    given the information at any node in Figure [3,](#page-8-1) consumers can self-determine
    the information at each of its child nodes. As a result, access to trustworthy
    information at any node provides consumers with a foundation on which to build
    their own designs. Therefore, we recommend that the DRAM industry release information
    at at least one node, but that producers be free to choose that information based
    on their interests and capabilities. This section examines realistic possibilities
    for communicating information at each node of the flowchart.


    7.1.1. Microarchitectural Design Properties. At the lowest level, DRAM producers
    can communicate basic microarchitectural design properties that enable consumers
    to develop robust test methodologies and error models. This is the most general
    and flexible approach because it places no limitations on what types of studies
    consumers may pursue (e.g., in contrast to providing information that is useful
    for reasoning about select error mechanism(s)). Table [2](#page-10-1) reviews
    example properties used by prior works to build system-level solutions for addressing
    DRAM scaling challenges. For each design property, we list prior works that reverse-engineer
    it and describe use-cases that rely on its knowledge.


    We believe that releasing these properties will minimally (if at all) impact DRAM
    producer''s business interests given that each of the properties can be reverse-engineered
    with existing methods (as shown by Table [2,](#page-10-1) Column 2) and access
    to appropriate tools, as demonstrated by prior studies [\[16,](#page-13-28) [23,](#page-13-11)
    [28,](#page-13-13) [29,](#page-13-20) [32,](#page-13-14) [40,](#page-13-30) [65,](#page-14-31)
    [74,](#page-14-1) [78,](#page-14-2) [92,](#page-14-4) [114](#page-15-3)[–117,](#page-15-26)
    [150–](#page-15-27)[153,](#page-15-45) [277\]](#page-18-2). Releasing this information
    in an official capacity confirms what is already demonstrated publicly through
    experiment, yielding no further information than others already have the capability
    to identify. On the other hand, knowing this information empowers all consumers
    to confidently explore a larger design space, benefiting both designers and producers
    in the long run (as discussed in Section [6.3\)](#page-10-2).


    7.1.2. Test Methodologies. Abstracting beyond microarchitectural details, DRAM
    producers can disclose effective test methodologies for consumers to conduct their
    own reliability studies (e.g., to explore new viable operating points). Providing
    test methodologies absolves (1) producers from needing to disclose chip design
    details; and (2) consumers from needing the DRAM-related expertise to determine
    the test methodologies from those details.[17](#page-11-1) As a limitation, disclosing
    only test methodologies constrains consumers to work with only the particular
    error mechanisms that the methodologies are designed for (e.g., data-retention,
    RowHammer). Table [3](#page-11-2) provides example test parameters that prior
    works generally depend on (e.g., assume or reverse-engineer) to conduct reliability
    testing.


    <span id="page-11-2"></span>


    | Test Parameter              | Description                                                                                                                              |  |

    |-----------------------------|------------------------------------------------------------------------------------------------------------------------------------------|--|

    | Data pattern                | Data pattern that maximizes the chance of<br>errors
    occurring [28, 29, 32, 48, 65, 74, 92, 116,<br>116–118, 150, 245, 262, 356, 398–400]
    |  |

    | Environmental<br>conditions | Temperature and voltage that lead to<br>worst-case
    behavior [31, 75, 118, 150, 218, 277,<br>356, 357, 382, 401]                          |  |

    | Test algorithm              | Sequence of representative and/or worst-case<br>DRAM
    operations to test [22, 29, 32, 48, 73, 75,<br>150, 262, 402]                       |  |


    Table 3: Testing parameters that are typically assumed or inferred during experimental
    studies.


    7.1.3. Test Results and/or Error Models. At the highest level of abstraction,
    DRAM producers can directly disclose test results and/or error models related
    to specific studies useful to consumers. For example, these could take the form
    of parametric error models (e.g., the statistical relationship between operating
    timings and error rates) along with parameter values for each chip; fine-granularity
    error characteristics (e.g., per-column minimum viable access timings); and/or
    specific summary statistics (e.g., HCfirst in studies pertaining to RowHammer).
    In this way, consumers exploring new designs can avoid the need to conduct reliability
    testing to identify the producer-provided information. As a limitation, directly
    releasing test results and/or error models constrains consumers to developing
    solutions only for those design concerns that pertain to the released information.
    Table [4](#page-12-0) provides examples of key test results and error models that
    prior works leverage in order to explore new design points based on commodity
    DRAM chips.


    #### 7.2. Choosing When to Release the Information


    We recommend decoupling the release of information from the requirement to do
    so because modifying DRAM standards is a slow process due to the need for consensus
    among DRAM stakeholders (discussed in Section [2.4\)](#page-3-5). To this end,
    we recommend a practical two-step process with different approaches in the short-
    and long-term.


    7.2.1. Step 1: Immediate Disclosure of Information. We recommend two independent
    approaches to quickly release information in the short-term. First, we recommend
    establishing a publicly-accessible database for researchers and practitioners
    to aggregate information (e.g., reverse-engineered design details) through crowdsourcing.
    We believe this is practical given the significant academic and industry interest
    in addressing


    <span id="page-11-1"></span><sup>17</sup>We believe that interested parties already
    have such expertise, as shown by the fact that many studies [\[16,](#page-13-28)
    [23,](#page-13-11) [28,](#page-13-13) [29,](#page-13-20) [32,](#page-13-14) [40,](#page-13-30)
    [65,](#page-14-31) [74,](#page-14-1) [78,](#page-14-2) [92,](#page-14-4) [114–](#page-15-3)[117,](#page-15-26)
    [150](#page-15-27)[–153,](#page-15-45) [277\]](#page-18-2) establish test methodologies
    through experimentation.


    <span id="page-12-0"></span>


    | Test Result or                          | Description                                          |  |

    |-----------------------------------------|------------------------------------------------------|--|

    | Error Model                             |                                                      |  |

    | Data-retention<br>times                 | Minimum refresh rate required for
    different          |  |

    |                                         | DRAM regions (e.g., rows, cells) [65,
    66, 79, 80,    |  |

    |                                         | 86, 150, 403]                                        |  |

    | Error profile                           | List of cells susceptible to errors
    (e.g., VRT [65,  |  |

    |                                         | 94, 150], latency-related [16, 52,
    73–75])           |  |

    | Error rate<br>summary<br>statistics     | Aggregate error rates (e.g., BER [92,
    115, 150,      |  |

    |                                         | 166, 356], FIT [218, 404, 405]), distribution        |  |

    |                                         | parameters (e.g., copula [384], lognormal
    [276,      |  |

    |                                         | 277, 376], exponential [80, 387])                    |  |

    | RowHammer<br>blast radius               | Maximum number of rows affected by                   |  |

    |                                         | hammering one or more row(s) [28,
    29, 107, 231,      |  |

    |                                         | 300, 396]                                            |  |

    | HCfirst<br>or<br>RowHammer<br>Threshold |                                                      |  |

    |                                         | Minimum number of RowHammer accesses                 |  |

    |                                         | required to induce bit-flips [28,
    29, 107, 118, 306] |  |


    Table 4: Examples of key test results and error models from prior works that study
    and/or optimize commodity DRAM.


    DRAM scaling challenges. Such a database would provide an opportunity for peer
    review of posted information, increasing the likelihood that the information is
    trustworthy. In the long run, we believe such a database would facilitate information
    release from DRAM producers themselves because the producers could simply validate
    database information, if not contribute directly.


    Second, we recommend that commodity DRAM producers individually release information
    for current DRAM chips and those already in the field. For example, producers
    may update chip datasheets to incorporate relevant design properties or make more
    extensive information available online (e.g., similar to how some producers already
    provide compliance documents and functional simulation models on their websites
    [\[406](#page-20-13)[–408\]](#page-20-14)). Releasing the information described
    in Section [7.1](#page-11-0) requires no changes to DRAM designs or standards,
    though modifying standards (e.g., via an addendum, as we suggest in Step 2) would
    help unify the information release across all producers. Regardless, we believe
    it is important to release information in the near term (even if not standardized)
    so that it is available as soon as possible.


    7.2.2. Step 2: Explicit DRAM Reliability Standards. In the long term, we recommend
    modifying DRAM standards to promote (or even require) producers to disclose information
    that can have consumer-visible impact to DRAM reliability. This may include any
    or all of the information discussed throughout this paper; we believe that the
    DRAM stakeholders themselves (i.e., DRAM producers and consumers) must collectively
    determine and standardize which information is the most relevant and useful to
    regulate.


    As a concrete example of how such changes to standards may occur, we reference
    test methodologies [\[121,](#page-15-7) [122\]](#page-15-8) and error models [\[120\]](#page-15-6)
    that JEDEC provides for NAND flash memory endurance [\[123](#page-15-9)[–125\]](#page-15-10),
    including floating-gate data retention [\[126](#page-15-11)[–129\]](#page-15-12)
    and threshold voltage distributions [\[130–](#page-15-13)[133\]](#page-15-47).
    These documents outline standardized best practices for studying and characterizing
    endurance properties of SSD devices. We envision analogous documents released
    for key DRAM error mechanisms (e.g., data-retention, access-timing-related, RowHammer),
    providing a standardized and trustworthy alternative to inferring the same information
    through unofficial channels.


    #### 7.3. Natural Progression Toward Transparency


    As a final note, we anticipate that efforts to overcome DRAM technology scaling
    challenges will naturally bring DRAM producers and consumers closer together in
    pursuit of the best possible solutions. Diversifying consumer needs, increasing
    use of system-memory cooperation [\[10,](#page-13-7) [29,](#page-13-20) [112,](#page-15-37)
    [409,](#page-20-15) [410\]](#page-20-16), and emerging, non-traditional DRAM architectures
    [\[185,](#page-16-28) [200,](#page-16-29) [201,](#page-16-30) [410](#page-20-16)[–417\]](#page-20-17)
    all challenge existing DRAM design and use practices today. As scaling challenges
    continue to worsen, the opportunity costs of maintaining today''s separation of
    concerns will do so as well.


    However, if we are to step ahead of worsening DRAM scaling challenges, we must
    ensure that the standards of the future proactively enable the whole community
    (both industry and academia) to collectively develop creative and effective solutions.
    Although recent changes to DRAM standards such as refresh management [\[34,](#page-13-16)
    [47\]](#page-13-39) and on-die ECC scrubbing [\[34,](#page-13-16) [119,](#page-15-5)
    [319\]](#page-18-21) are increasing signs of cooperation, these changes are reactions
    to long-standing problems. Our work preempts this post hoc approach to system
    design, preemptively forging a path toward cooperative solutions capable of holistically
    address the scaling challenge.


    ## 8. Conclusion


    We show that the separation of concerns between DRAM producers and consumers is
    an impediment to overcoming modern DRAM scaling challenges because it discourages
    exploring the full design space around standardized designs. Our case studies
    that support this observation find that consumers'' lack of insight into DRAM
    reliability is the key factor discouraging more efficient solutions based on system-memory
    cooperation. We then analyze how consumers can obtain this insight through DRAM
    reliability testing, and we introduce a twostep approach to revise the separation
    of concerns to encourage system-memory cooperation. We start with conceptual changes
    to the separation of concerns and build toward modifying the DRAM standards that
    specify the separation. Our work is a call-to-action for more open and flexible
    practices for DRAM design and use, harnessing the synergy between researchers
    and practitioners to fully explore the potential of DRAM technology.


    ## Acknowledgment


    We thank the members of the SAFARI Research Group for their valuable feedback
    and the constructively critical environment that they provide. We specifically
    thank Geraldo F. Oliveira, Jisung Park, Haiyu Mao, Jawad Haj-Yahya, Jeremie S.
    Kim, Hasan Hassan, Joel Lindegger, and Meryem Banu Cavlak for the feedback they
    provided on earlier versions of this paper. We thank external experts who helped
    shape our arguments, including Mattan Erez, Moinuddin Qureshi, Vilas Sridharan,
    Christian Weis, and Tanj Bennett. This work was supported in part by the generous
    gifts provided by our industry partners, including Google, Huawei, Intel, Microsoft,
    and VMware, and support from the ETH Future Computing Laboratory and the Semiconductor
    Research Corporation. A much earlier version of this work was placed on arXiv
    in 2022 [\[418\]](#page-20-18).


    ## <span id="page-13-0"></span>References


    - [1] R. H. Dennard, "Field-Effect Transistor Memory," 1968, US Patent 3,387,286.

    - <span id="page-13-1"></span>[2] R. H. Dennard, F. H. Gaensslen, H.-N. Yu, V.
    L. Rideout, E. Bassous, and A. R. LeBlanc, "Design of Ion-Implanted MOSFET''s
    with Very Small Physical Dimensions," JSSC, 1974.

    - <span id="page-13-2"></span>[3] B. Keeth, R. J. Baker, B. Johnson, and F. Lin,
    DRAM Circuit Design: Fundamental and High-Speed Topics. John Wiley & Sons, 2007.

    - [4] J. Markoff, "IBM''s Robert H. Dennard and the Chip That Changed the World,"
    2019, [https://www.ibm.com/blogs/think/2019/11/ibms-r](https://www.ibm.com/blogs/think/2019/11/ibms-robert-h-dennard-and-the-chip-that-changed-the-world/)
    [obert-h-dennard-and-the-chip-that-changed-the-world/.](https://www.ibm.com/blogs/think/2019/11/ibms-robert-h-dennard-and-the-chip-that-changed-the-world/)

    - [5] Nature Electronics, "Memory Lane," 2018.

    - <span id="page-13-3"></span>[6] "DRAM: The Invention of On-Demand Data," [https://www.ibm.com/](https://www.ibm.com/ibm/history/ibm100/us/en/icons/dram/transform/)
    [ibm/history/ibm100/us/en/icons/dram/transform/,](https://www.ibm.com/ibm/history/ibm100/us/en/icons/dram/transform/)
    2021.

    - <span id="page-13-4"></span>[7] Y.-B. Kim and T. W. Chen, "Assessing Merged
    DRAM/Logic Technology," Integration, 1999.

    - <span id="page-13-5"></span>[8] J. Kittl, K. Opsomer, M. Popovici, N. Menou,
    B. Kaczer, X. P. Wang, C. Adelmann, M. Pawlak, K. Tomida, A. Rothschild et al.,
    "High-K Dielectrics for Future Generation Memory Devices," Microelectronic engineering,
    2009.

    - <span id="page-13-6"></span>[9] IBM, "Icons of Progress: DRAM," [https://www.ibm.com/ibm/history/](https://www.ibm.com/ibm/history/ibm100/us/en/icons/dram/)
    [ibm100/us/en/icons/dram/,](https://www.ibm.com/ibm/history/ibm100/us/en/icons/dram/)
    2023.

    - <span id="page-13-7"></span>[10] O. Mutlu, "Main Memory Scaling: Challenges
    and Solution Directions," in More Than Moore Technologies for Next Generation
    Computer Design. Springer, 2015, pp. 127–153.

    - <span id="page-13-8"></span>[11] IEEE, "More Moore: 2022 Update," International
    Roadmap for Devices and Systems, 2022.

    - <span id="page-13-9"></span>[12] Y. H. Son, O. Seongil, Y. Ro, J. W. Lee, and
    J. H. Ahn, "Reducing Memory Access Latency with Asymmetric DRAM Bank Organizations,"
    in ISCA, 2013.

    - <span id="page-13-33"></span>[13] K. K. Chang, "Understanding and Improving
    Latency of DRAM-Based Memory Systems," Ph.D. dissertation, Carnegie Mellon University,
    2017.

    - <span id="page-13-35"></span>[14] D. Lee, Y. Kim, V. Seshadri, J. Liu, L. Subramanian,
    and O. Mutlu, "Tiered-Latency DRAM: A Low Latency and Low Cost DRAM Architecture,"
    in HPCA, 2013.

    - [15] J. L. Hennessy and D. A. Patterson, Computer Architecture: A Quantitative
    Approach. Elsevier, 2011.

    - <span id="page-13-28"></span>[16] K. K. Chang, A. Kashyap, H. Hassan, S. Ghose,
    K. Hsieh, D. Lee, T. Li, G. Pekhimenko, S. Khan, and O. Mutlu, "Understanding
    Latency Variation in Modern DRAM Chips: Experimental Characterization, Analysis,
    and Optimization," in SIGMETRICS, 2016.

    - <span id="page-13-34"></span>[17] D. Lee, "Reducing DRAM Latency at Low Cost
    by Exploiting Heterogeneity," Ph.D. dissertation, Carnegie Mellon University,
    2016.

    - [18] R. Isaac, "The Remarkable Story of the DRAM Industry," IEEE SSCS News,
    2008.

    - [19] J. Choi, W. Shin, J. Jang, J. Suh, Y. Kwon, Y. Moon, and L.-S. Kim, "Multiple
    Clone Row DRAM: A Low Latency and Area Optimized DRAM," in ISCA, 2015.

    - [20] S. Borkar and A. A. Chien, "The Future of Microprocessors," CACM, 2011.

    - <span id="page-13-36"></span>[21] K. Nguyen, K. Lyu, X. Meng, V. Sridharan,
    and X. Jian, "Nonblocking Memory Refresh," in ISCA, 2018.

    - <span id="page-13-10"></span>[22] D. Lee, Y. Kim, G. Pekhimenko, S. Khan, V.
    Seshadri, K. Chang, and O. Mutlu, "Adaptive-Latency DRAM: Optimizing DRAM Timing
    for the Common-Case," in HPCA, 2015.

    - <span id="page-13-11"></span>[23] P. Frigo, E. Vannacci, H. Hassan, V. van der
    Veen, O. Mutlu, C. Giuffrida, H. Bos, and K. Razavi, "TRRespass: Exploiting the
    Many Sides of Target Row Refresh," in IEEE S&P, 2020.

    - <span id="page-13-31"></span>[24] P. Pessl, D. Gruss, C. Maurice, M. Schwarz,
    and S. Mangard, "DRAMA: Exploiting DRAM Addressing for Cross-CPU Attacks," in
    USENIX Sec., 2016.

    - <span id="page-13-19"></span>[25] O. Mutlu, "The RowHammer Problem and Other
    Issues we may Face as Memory Becomes Denser," in DATE, 2017.

    - <span id="page-13-40"></span>[26] O. Mutlu and J. Kim, "RowHammer: A Retrospective,"
    in TCAD, 2019.

    - <span id="page-13-12"></span>[27] O. Mutlu, A. Olgun, and A. G. Yaglıkcı, "Fundamentally
    Understand- ˘ ing and Solving RowHammer," in ASP-DAC, 2023.

    - <span id="page-13-13"></span>[28] J. S. Kim, M. Patel, A. G. Yaglık ˘ c¸ı, H.
    Hassan, R. Azizi, L. Orosa, and O. Mutlu, "Revisiting RowHammer: An Experimental
    Analysis of Modern Devices and Mitigation Techniques," in ISCA, 2020.

    - <span id="page-13-20"></span>[29] Y. Kim, R. Daly, J. Kim, C. Fallin, J. H.
    Lee, D. Lee, C. Wilkerson, K. Lai, and O. Mutlu, "Flipping Bits in Memory Without
    Accessing Them: An Experimental Study of DRAM Disturbance Errors," in ISCA, 2014.

    - <span id="page-13-46"></span>[30] K. Park, D. Yun, and S. Baeg, "Statistical
    Distributions of Row-Hammering Induced Failures in DDR3 Components," Microelectronics
    Reliability, 2016.

    - <span id="page-13-42"></span>[31] K. Park, C. Lim, D. Yun, and S. Baeg, "Experiments
    and Root Cause Analysis for Active-Precharge Hammering Fault In DDR3 SDRAM Under
    3× Nm Technology," Microelectronics Reliability, 2016.

    - <span id="page-13-14"></span>[32] H. Hassan, Y. C. Tugrul, J. S. Kim, V. Van
    der Veen, K. Razavi, and O. Mutlu, "Uncovering In-DRAM RowHammer Protection Mechanisms:
    A New Methodology, Custom RowHammer Patterns, and Implications," in MICRO, 2021.

    - <span id="page-13-16"></span><span id="page-13-15"></span>[33] JEDEC, DDR4 SDRAM
    Specification, 2012.

    - [34] JEDEC, DDR5 SDRAM Specification, 2020.

    - <span id="page-13-17"></span>[35] JEDEC, "High Bandwidth Memory (HBM) DRAM,"
    JEDEC Standard JESD235D, 2021.

    - <span id="page-13-18"></span>[36] JEDEC, "High Bandwidth Memory DRAM (HBM3),"
    JEDEC Standard JESD238, 2022.

    - <span id="page-13-21"></span>[37] K. Bains, J. Halbert, C. Mozak, T. Schoenborn,
    and Z. Greenfield, "Row Hammer Refresh Command," 2014, US Patent 9,117,544 B2.

    - <span id="page-13-22"></span>[38] Y. H. Son, S. Lee, O. Seongil, S. Kwon, N.
    S. Kim, and J. H. Ahn, "CiDRA: A cache-Inspired DRAM resilience architecture,"
    in HPCA, 2015.

    - [39] N. Kwak, S.-H. Kim, K. H. Lee, C.-K. Baek, M. S. Jang, Y. Joo, S.-H. Lee,
    W. Y. Lee, E. Lee, D. Han, J. Kang, J. H. Lim, J.-B. Park, K.-T. Kim, S. Cho,
    S. W. Han, J. Y. Keh, J. H. Chun, J. Oh, and S. H. Lee, "A 4.8 Gb/s/pin 2Gb LPDDR4
    SDRAM with Sub-100µA Self-Refresh Current for IoT Applications," in ISSCC, 2017.

    - <span id="page-13-30"></span>[40] M. Patel, J. Kim, T. Shahroodi, H. Hassan,
    and O. Mutlu, "Bit-Exact ECC Recovery (BEER): Determining DRAM On-Die ECC Functions
    by Exploiting DRAM Data Retention Characteristics," in MICRO, 2020.

    - <span id="page-13-23"></span>[41] S.-L. Gong, J. Kim, S. Lym, M. Sullivan, H.
    David, and M. Erez, "DUO: Exposing On-Chip Redundancy to Rank-Level ECC for High
    Reliability," in HPCA, 2018.

    - <span id="page-13-24"></span>[42] JEDEC, "System Level RowHammer Mitigation,"
    JEDEC Standard JEP301-1, 2021.

    - <span id="page-13-32"></span>[43] JEDEC, "Near-Term DRAM Level Rowhammer Mitigation,"
    JEDEC Standard JEP300-1, 2021.

    - <span id="page-13-25"></span>[44] JEDEC, "Low Power Double Data Rate 5 (LPDDR5)
    SDRAM Specification," JEDEC Standard JESD209–5A, 2020.

    - <span id="page-13-26"></span>[45] M. K. Qureshi, "Rethinking ECC in the Era
    of Row-Hammer," in DRAMSec, 2021.

    - <span id="page-13-38"></span>[46] S. Saroiu, A. Wolman, and L. Cojocar, "The
    Price of Secrecy: How Hiding Internal DRAM Topologies Hurts Rowhammer Defenses,"
    in IRPS, 2022.

    - <span id="page-13-39"></span>[47] S. Saroiu and A. Wolman, "How to Configure
    Row-Sampling-Based Rowhammer Defenses," DRAMSec, 2022.

    - <span id="page-13-27"></span>[48] P. Jattke, V. van der Veen, P. Frigo, S. Gunter,
    and K. Razavi, "Blacksmith: Scalable Rowhammering in the Frequency Domain," in
    SP, 2022.

    - <span id="page-13-29"></span>[49] S. Baek, S. Cho, and R. Melhem, "Refresh Now
    and Then," in TC, 2014.

    - <span id="page-13-41"></span>[50] F. Brasser, L. Davi, D. Gens, C. Liebchen,
    and A.-R. Sadeghi, "CAn''t Touch This: Software-Only Mitigation Against Rowhammer
    Attacks Targeting Kernel Memory," in USENIX Sec., 2017.

    - <span id="page-13-43"></span>[51] G. C. Cardarilli, P. Marinucci, and A. Salsano,
    "Development of an Evaluation Model for the Design of Fault-Tolerant Solid State
    Mass Memory," in ISCAS, 2000.

    - <span id="page-13-37"></span>[52] K. Chandrasekar, S. Goossens, C. Weis, M.
    Koedam, B. Akesson, N. Wehn, and K. Goossens, "Exploiting Expendable Process-Margins
    in DRAMs for Run-Time Performance Optimization," in DATE, 2014.

    - <span id="page-13-44"></span>[53] L. Chen, Y. Cao, and Z. Zhang, "E3CC: A Memory
    Error Protection Scheme With Novel Address Mapping for Subranked And Low-Power
    Memories," TACO, 2013.

    - <span id="page-13-45"></span>[54] H.-M. Chen, A. Arunkumar, C.-J. Wu, T. Mudge,
    and C. Chakrabarti, "E-ECC: Low Power Erasure And Error Correction Schemes For
    Increasing Reliability Of Commodity DRAM Systems," in Proceedings of the 2015
    International Symposium on Memory Systems, 2015, pp. 60–70.

    - <span id="page-14-16"></span>[55] F. Gao, G. Tziantzioulis, and D. Wentzlaff,
    "ComputeDRAM: In-Memory Compute using Off-the-Shelf DRAMs," in MICRO, 2019.

    - <span id="page-14-25"></span>[56] M. Ghosh and H.-H. S. Lee, "Smart Refresh:
    An Enhanced Memory Controller Design for Reducing Energy in Conventional and 3D
    Die-Stacked DRAMs," in MICRO, 2007.

    - <span id="page-14-32"></span>[57] Z. Greenfield and T. Levy, "Throttling Support
    for Row-Hammer Counters," 2016, U.S. Patent 9,251,885.

    - <span id="page-14-17"></span>[58] N. Hajinazar, G. F. Oliveira, S. Gregorio,
    J. Ferreira, N. M. Ghiasi, M. Patel, M. Alser, S. Ghose, J. G. Luna, and O. Mutlu,
    "SIMDRAM: An End-to-End Framework for Bit-Serial SIMD Computing in DRAM," ASPLOS,
    2021.

    - <span id="page-14-6"></span>[59] H. Hassan, G. Pekhimenko, N. Vijaykumar, V.
    Seshadri, D. Lee, O. Ergin, and O. Mutlu, "ChargeCache: Reducing DRAM Latency
    by Exploiting Row Access Locality," in HPCA, 2016.

    - <span id="page-14-44"></span>[60] A. A. Hwang, I. A. Stefanovici, and B. Schroeder,
    "Cosmic Rays Don''t Strike Twice: Understanding the Nature of DRAM Errors and
    the Implications for System Design," in ASPLOS, 2012.

    - <span id="page-14-26"></span>[61] S. M. Jafri, H. Hassan, A. Hemani, and O.
    Mutlu, "Refresh Triggered Computation: Improving the Energy Efficiency of Convolutional
    Neural Network Accelerators," TACO, 2020.

    - <span id="page-14-36"></span>[62] X. Jian and R. Kumar, "Adaptive Reliability
    Chipkill Correct (ARCC)," in HPCA, 2013.

    - <span id="page-14-37"></span>[63] X. Jian, H. Duwe, J. Sartori, V. Sridharan,
    and R. Kumar, "Low-Power, Low-Storage-Overhead Chipkill Correct via Multi-Line
    Error Correction," in SC, 2013.

    - <span id="page-14-27"></span>[64] Y. Katayama, E. J. Stuckey, S. Morioka, and
    Z. Wu, "Fault-Tolerant Refresh Power Reduction of DRAMs for Quasi-Nonvolatile
    Data Retention," in EFT, 1999.

    - <span id="page-14-31"></span>[65] S. Khan, D. Lee, Y. Kim, A. R. Alameldeen,
    C. Wilkerson, and O. Mutlu, "The Efficacy of Error Mitigation Techniques for DRAM
    Retention Failures: A Comparative Experimental Study," in SIGMETRICS, 2014.

    - <span id="page-14-47"></span>[66] S. Khan, C. Wilkerson, D. Lee, A. R. Alameldeen,
    and O. Mutlu, "A Case for Memory Content-Based Detection and Mitigation of Data-Dependent
    Failures in DRAM," in IEEE CAL, 2016.

    - <span id="page-14-24"></span>[67] S. Khan, D. Lee, and O. Mutlu, "PARBOR: An
    Efficient System-Level Technique to Detect Data-Dependent Failures in DRAM," in
    DSN, 2016.

    - <span id="page-14-23"></span>[68] S. Khan, C. Wilkerson, Z. Wang, A. R. Alameldeen,
    D. Lee, and O. Mutlu, "Detecting and Mitigating Data-Dependent DRAM Failures by
    Exploiting Current Memory Content," in MICRO, 2017.

    - <span id="page-14-8"></span>[69] J. Kim and M. C. Papaefthymiou, "Dynamic Memory
    Design for Low Data-Retention Power," in PATMOS, 2000.

    - <span id="page-14-28"></span>[70] J. Kim and M. C. Papaefthymiou, "Block-Based
    Multiperiod Dynamic Memory Design for Low Data-Retention Power," in TVLSI, 2003.

    - <span id="page-14-38"></span>[71] J. Kim, M. Sullivan, and M. Erez, "Bamboo
    ECC: Strong, Safe, and Flexible Codes For Reliable Computer Memory," in HPCA,
    2015.

    - <span id="page-14-39"></span>[72] J. Kim, M. Sullivan, S.-L. Gong, and M. Erez,
    "Frugal ECC: Efficient And Versatile Memory Error Protection Through Fine-Grained
    Compression," in SC, 2015.

    - <span id="page-14-0"></span>[73] J. S. Kim, M. Patel, H. Hassan, and O. Mutlu,
    "The DRAM Latency PUF: Quickly Evaluating Physical Unclonable Functions by Exploiting
    the Latency-Reliability Tradeoff in Modern Commodity DRAM Devices," in HPCA, 2018.

    - <span id="page-14-1"></span>[74] J. S. Kim, M. Patel, H. Hassan, and O. Mutlu,
    "Solar-DRAM: Reducing DRAM Access Latency by Exploiting the Variation in Local
    Bitlines," in ICCD, 2018.

    - <span id="page-14-18"></span>[75] J. S. Kim, M. Patel, H. Hassan, L. Orosa,
    and O. Mutlu, "D-RaNGe: Using Commodity DRAM Devices to Generate True Random Numbers
    With Low Latency And High Throughput," in HPCA, 2019.

    - <span id="page-14-34"></span>[76] R. K. Konoth, M. Oliverio, A. Tatar, D. Andriesse,
    H. Bos, C. Giuffrida, and K. Razavi, "ZebRAM: Comprehensive and Compatible Software
    Protection Against Rowhammer Attacks," in OSDI, 2018.

    - <span id="page-14-5"></span>[77] S. Koppula, L. Orosa, A. G. Yaglık ˘ c¸ı, R.
    Azizi, T. Shahroodi, K. Kanellopoulos, and O. Mutlu, "EDEN: Enabling Energy-Efficient,
    High-Performance Deep Neural Network Inference Using Approximate DRAM," in MICRO,
    2019.

    - <span id="page-14-2"></span>[78] D. Lee, S. Khan, L. Subramanian, S. Ghose,
    R. Ausavarungnirun, G. Pekhimenko, V. Seshadri, and O. Mutlu, "Design-Induced
    Latency Variation in Modern DRAM Chips: Characterization, Analysis, and Latency
    Reduction Mechanisms," in SIGMETRICS, 2017.

    - <span id="page-14-29"></span>[79] C. H. Lin, D.-Y. Shen, Y.-J. Chen, C.-L. Yang,
    and M. Wang, "SECRET: Selective Error Correction for Refresh Energy Reduction
    in DRAMs," in ICCD, 2012.

    - <span id="page-14-9"></span>[80] J. Liu, B. Jaiyen, R. Veras, and O. Mutlu,
    "RAIDR: Retention-Aware Intelligent DRAM Refresh," in ISCA, 2012.

    - <span id="page-14-40"></span>[81] E. Manzhosov, A. Hastings, M. Pancholi, R.
    Piersma, M. T. I. Ziad, and S. Sethumadhavan, "MUSE: Multi-Use Error Correcting
    Codes," arXiv:2107.09245, 2021.

    - <span id="page-14-7"></span>[82] D. M. Mathew, E. F. Zulian, M. Jung, K. Kraft,
    C. Weis, B. Jacob, and ´ N. Wehn, "Using Run-Time Reverse-Engineering to Optimize
    DRAM Refresh," in MEMSYS, 2017.

    - <span id="page-14-45"></span>[83] Bad Page Offlining, mcelog, 2021, [https://mcelog.org/badpageofflini](https://mcelog.org/badpageofflining.html)
    [ng.html.](https://mcelog.org/badpageofflining.html)

    - <span id="page-14-13"></span>[84] J. Meza, Q. Wu, S. Kumar, and O. Mutlu, "Revisiting
    Memory Errors in Large-Scale Production Data Centers: Analysis and Modeling of
    New Trends from the Field," in DSN, 2015.

    - <span id="page-14-33"></span>[85] O. Mutlu, "RowHammer," [https://people.inf.ethz.ch/omutlu/pub/](https://people.inf.ethz.ch/omutlu/pub/onur-Rowhammer-TopPicksinHardwareEmbeddedSecurity-November-8-2018.pdf)
    [onur-Rowhammer-TopPicksinHardwareEmbeddedSecurity-N](https://people.inf.ethz.ch/omutlu/pub/onur-Rowhammer-TopPicksinHardwareEmbeddedSecurity-November-8-2018.pdf)
    [ovember-8-2018.pdf,](https://people.inf.ethz.ch/omutlu/pub/onur-Rowhammer-TopPicksinHardwareEmbeddedSecurity-November-8-2018.pdf)
    2018, Top Picks in Hardware and Embedded Security.

    - <span id="page-14-3"></span>[86] P. J. Nair, D.-H. Kim, and M. K. Qureshi, "ArchShield:
    Architectural Framework for Assisting DRAM Scaling by Tolerating High Error Rates,"
    in ISCA, 2013.

    - <span id="page-14-14"></span>[87] P. J. Nair, V. Sridharan, and M. K. Qureshi,
    "XED: Exposing On-Die Error Detection Information for Strong Memory Reliability,"
    in ISCA, 2016.

    - <span id="page-14-46"></span>[88] Dynamic Page Retirement, NVIDIA, 2020, [https://docs.nvidia.com/de](https://docs.nvidia.com/deploy/dynamic-page-retirement/index.html)
    [ploy/dynamic-page-retirement/index.html.](https://docs.nvidia.com/deploy/dynamic-page-retirement/index.html)

    - <span id="page-14-10"></span>[89] T. Ohsawa, K. Kai, and K. Murakami, "Optimizing
    the DRAM Refresh Count for Merged DRAM/logic LSIs," in ISLPED, 1998.

    - <span id="page-14-19"></span>[90] A. Olgun, J. G. Luna, K. Kanellopoulos, B.
    Salami, H. Hassan, O. Ergin, and O. Mutlu, "PiDRAM: A Holistic End-to-end FPGA-based
    Framework for Processing-in-DRAM," arXiv:2111.00082, 2021.

    - <span id="page-14-20"></span>[91] A. Olgun, M. Patel, A. G. Yaglık ˘ c¸ı, H.
    Luo, J. S. Kim, N. Bostancı, N. Vijaykumar, O. Ergin, and O. Mutlu, "QUAC-TRNG:
    High-Throughput True Random Number Generation Using Quadruple Row Activation in
    Commodity DRAM Chips," in ISCA, 2021.

    - <span id="page-14-4"></span>[92] M. Patel, J. S. Kim, and O. Mutlu, "The Reach
    Profiler (REAPER): Enabling the Mitigation of DRAM Retention Failures via Profiling
    at Aggressive Conditions," in ISCA, 2017.

    - <span id="page-14-41"></span>[93] A. Patil, V. Nagarajan, R. Balasubramonian,
    and N. Oswald, "Dve:´ Improving DRAM Reliability and Performance On-Demand via
    Coherent Replication," in ISCA, 2021.

    - <span id="page-14-12"></span>[94] M. K. Qureshi, D.-H. Kim, S. Khan, P. J. Nair,
    and O. Mutlu, "AVATAR: A Variable-Retention-Time (VRT) Aware Refresh for DRAM
    Systems," in DSN, 2015.

    - <span id="page-14-35"></span>[95] G. Saileshwar, B. Wang, M. Qureshi, and P.
    J. Nair, "Randomized Row-Swap: Mitigating Row Hammer by Breaking Spatial Correlation
    Between Aggressor and Victim Rows," in ASPLOS, 2022.

    - <span id="page-14-21"></span>[96] V. Seshadri, Y. Kim, C. Fallin, D. Lee, R.
    Ausavarungnirun, G. Pekhimenko, Y. Luo, O. Mutlu, P. B. Gibbons, M. A. Kozuch,
    and T. C. Mowry, "RowClone: Fast and Energy-Efficient In-DRAM Bulk Data Copy and
    Initialization," in MICRO, 2013.

    - [97] V. Seshadri, K. Hsieh, A. Boroum, D. Lee, M. A. Kozuch, O. Mutlu, P. B.
    Gibbons, and T. C. Mowry, "Fast Bulk Bitwise AND and OR in DRAM," IEEE CAL, 2015.

    - [98] V. Seshadri, D. Lee, T. Mullins, H. Hassan, A. Boroumand, J. Kim, M. A.
    Kozuch, O. Mutlu, P. B. Gibbons, and T. C. Mowry, "Buddy-RAM: Improving the Performance
    and Efficiency of Bulk Bitwise Operations Using DRAM," in arXiv, 2016.

    - [99] V. Seshadri, D. Lee, T. Mullins, H. Hassan, A. Boroumand, J. Kim, M. A.
    Kozuch, O. Mutlu, P. B. Gibbons, and T. C. Mowry, "Ambit: In-Memory Accelerator
    for Bulk Bitwise Operations Using Commodity DRAM Technology," in MICRO, 2017.

    - <span id="page-14-22"></span>[100] V. Seshadri and O. Mutlu, "In-DRAM Bulk Bitwise
    Execution Engine," arXiv:1905.09822, 2019.

    - <span id="page-14-42"></span>[101] A. N. Udipi, N. Muralimanohar, R. Balsubramonian,
    A. Davis, and N. P. Jouppi, "LOT-ECC: Localized And Tiered Reliability Mechanisms
    For Commodity Memory Systems," in ISCA, 2012.

    - <span id="page-14-15"></span>[102] V. van der Veen, M. Lindorfer, Y. Fratantonio,
    H. P. Pillai, G. Vigna, C. Kruegel, H. Bos, and K. Razavi, "GuardION: Practical
    Mitigation of DMA-Based Rowhammer Attacks on ARM," in DIMVA, 2018.

    - <span id="page-14-30"></span>[103] R. K. Venkatesan, S. Herr, and E. Rotenberg,
    "Retention-Aware Placement in DRAM (RAPID): Software Methods for Quasi-Non-Volatile
    DRAM," in HPCA, 2006.

    - <span id="page-14-11"></span>[104] J. Wang, X. Dong, and Y. Xie, "ProactiveDRAM:
    A DRAM-Initiated Retention Management Scheme," in ICCD, 2014.

    - <span id="page-14-43"></span>[105] S. Wang, M. N. Bojnordi, X. Guo, and E. Ipek,
    "Content Aware Refresh: Exploiting the Asymmetry of DRAM Retention Errors to Reduce
    the Refresh Frequency of Less Vulnerable Data," TOC, 2018.

    - <span id="page-15-38"></span>[106] Y. Wang, A. Tavakkol, L. Orosa, S. Ghose,
    N. M. Ghiasi, M. Patel, J. S. Kim, H. Hassan, M. Sadrosadati, and O. Mutlu, "Reducing
    DRAM Latency Via Charge-Level-Aware Look-Ahead Partial Restoration," in MICRO,
    2018.

    - <span id="page-15-42"></span>[107] A. G. Yaglık ˘ c¸ı, M. Patel, J. S. Kim,
    R. Azizibarzoki, A. Olgun, L. Orosa, H. Hassan, J. Park, K. Kanellopoullos, T.
    Shahroodi, S. Ghose, and O. Mutlu, "BlockHammer: Preventing RowHammer at Low Cost
    by Blacklisting Rapidly-Accessed DRAM Rows," in HPCA, 2021.

    - <span id="page-15-43"></span>[108] D. H. Yoon and M. Erez, "Virtualized and
    Flexible ECC for Main Memory," in ASPLOS, 2010.

    - <span id="page-15-39"></span>[109] X. Zhang, Y. Zhang, B. R. Childers, and J.
    Yang, "Restore Truncation for Performance Improvement in Future DRAM Systems,"
    in HPCA, 2016.

    - <span id="page-15-0"></span>[110] D. Zhang, G. Panwar, J. B. Kotra, N. DeBardeleben,
    S. Blanchard, and X. Jian, "Quantifying Server Memory Frequency Margin and Using
    it to Improve Performance in HPC Systems," in ISCA, 2021.

    - <span id="page-15-1"></span>[111] O. Mutlu, "Memory Scaling: A Systems Architecture
    Perspective," in IMW, 2013.

    - <span id="page-15-37"></span>[112] O. Mutlu and L. Subramanian, "Research Problems
    and Opportunities in Memory Systems," in SUPERFRI, 2014.

    - <span id="page-15-2"></span>[113] K. Loughlin, S. Saroiu, A. Wolman, and B.
    Kasikci, "Software-Defined Memory Controllers: An Idea Whose Time Has Come," WACI,
    2022.

    - <span id="page-15-3"></span>[114] M. Jung, C. C. Rheinlander, C. Weis, and N.
    Wehn, "Reverse En- ¨ gineering of DRAMs: Row Hammer with Crosshair," in MEMSYS,
    2016.

    - <span id="page-15-40"></span>[115] M. Patel, J. S. Kim, H. Hassan, and O. Mutlu,
    "Understanding and Modeling On-Die Error Correction in Modern DRAM: An Experimental
    Study Using Real Devices," in DSN, 2019.

    - <span id="page-15-46"></span>[116] L. Mukhanov, D. S. Nikolopoulos, and G. Karakonstantis,
    "DStress: Automatic Synthesis of DRAM Reliability Stress Viruses using Genetic
    Algorithms," in MICRO, 2020.

    - <span id="page-15-26"></span>[117] K. Kraft, C. Sudarshan, D. M. Mathew, C.
    Weis, N. Wehn, and M. Jung, "Improving the Error Behavior of DRAM by Exploiting
    its Z-Channel Property," in DATE, 2018.

    - <span id="page-15-4"></span>[118] L. Orosa, A. G. Yaglık ˘ c¸ı, H. Luo, A. Olgun,
    J. Park, H. Hassan, M. Patel, J. S. Kim, and O. Mutlu, "A Deeper Look into RowHammer''s
    Sensitivities: Experiemental Analysis of Real DRAM Chips and Implications on Future
    Attacks and Defenses," in MICRO, 2021.

    - <span id="page-15-5"></span>[119] K. Criss, K. Bains, R. Agarwal, T. Bennett,
    T. Grunzke, J. K. Kim, H. Chung, and M. Jang, "Improving Memory Reliability by
    Bounding DRAM Faults: DDR5 Improved Reliability Features," in MEMSYS, 2020.

    - <span id="page-15-6"></span>[120] JEDEC, JEP122H: Failure Mechanisms and Models
    for Semiconductor Devices, 2016.

    - <span id="page-15-7"></span>[121] JEDEC, JESD218: Solid-State Drive (SSD) Requirements
    and Endurance Test Method, 2010.

    - <span id="page-15-9"></span><span id="page-15-8"></span>[122] JEDEC, JESD219:
    Solid-State Drive (SSD) Endurance Workloads, 2010.

    - [123] Y. Cai, S. Ghose, E. F. Haratsch, Y. Luo, and O. Mutlu, "Error Characterization,
    Mitigation, and Recovery In Flash-Memory-Based Solid-State Drives," Proc. IEEE,
    2017.

    - [124] Y. Cai, E. F. Haratsch, O. Mutlu, and K. Mai, "Error Patterns in MLC NAND
    Flash Memory: Measurement, Characterization, and Analysis," in DATE, 2012.

    - <span id="page-15-10"></span>[125] Y. Cai, S. Ghose, E. F. Haratsch, Y. Luo,
    and O. Mutlu, "Errors in Flash-Memory-Based Solid-State Drives: Analysis, Mitigation,
    and Recovery," Inside Solid State Drives, 2018.

    - <span id="page-15-11"></span>[126] Y. Cai, Y. Luo, E. F. Haratsch, K. Mai, and
    O. Mutlu, "Data Retention in MLC NAND Flash Memory: Characterization, Optimization,
    and Recovery," in HPCA, 2015.

    - [127] Y. Luo, S. Ghose, Y. Cai, E. F. Haratsch, and O. Mutlu, "HeatWatch: Improving
    3D NAND Flash Memory Device Reliability by Exploiting Self-Recovery and Temperature
    Awareness," in HPCA, 2018.

    - [128] Y. Luo, S. Ghose, Y. Cai, E. F. Haratsch, and O. Mutlu, "Improving 3D
    NAND Flash Memory Lifetime by Tolerating Early Retention Loss and Process Variation,"
    SIGMETRICS, 2018.

    - <span id="page-15-12"></span>[129] Y. Cai, G. Yalcin, O. Mutlu, E. F. Haratsch,
    A. Cristal, O. S. Unsal, and K. Mai, "Flash Correct-And-Refresh: Retention-Aware
    Error Management for Increased Flash Memory Lifetime," in ICCD, 2012.

    - <span id="page-15-13"></span>[130] Y. Cai, E. F. Haratsch, O. Mutlu, and K.
    Mai, "Threshold Voltage Distribution in MLC NAND Flash Memory: Characterization,
    Analysis, and Modeling," in DATE, 2013.

    - [131] Y. Cai, O. Mutlu, E. F. Haratsch, and K. Mai, "Program Interference in
    MLC NAND Flash Memory: Characterization, Modeling, and Mitigation," in ICCD, 2013.

    - [132] Y. Cai, Y. Luo, S. Ghose, and O. Mutlu, "Read Disturb Errors in MLC NAND
    Flash Memory: Characterization, Mitigation, and Recovery," in DSN, 2015.

    - <span id="page-15-47"></span>[133] Y. Luo, S. Ghose, Y. Cai, E. F. Haratsch,
    and O. Mutlu, "Enabling Accurate and Practical Online Flash Channel Modeling for
    Modern MLC NAND Flash Memory," in JSAC, 2016.

    - [134] Y. Cai, S. Ghose, Y. Luo, K. Mai, O. Mutlu, and E. F. Haratsch, "Vulnerabilities
    in MLC NAND Flash Memory Programming: Experimental Analysis, Exploits, and Mitigation
    Techniques," in HPCA, 2017.

    - <span id="page-15-14"></span>[135] Y. Cai, G. Yalcin, O. Mutlu, E. F. Haratsch,
    O. Unsal, A. Cristal, and K. Mai, "Neighbor-Cell Assisted Error Correction for
    MLC NAND Flash Memories," in SIGMETRICS, 2014.

    - <span id="page-15-15"></span>[136] "DRAM Datasheet Survey," [https://github.com/CMU-SAFARI/DR](https://github.com/CMU-SAFARI/DRAM-Datasheet-Survey)
    [AM-Datasheet-Survey.](https://github.com/CMU-SAFARI/DRAM-Datasheet-Survey)

    - <span id="page-15-16"></span>[137] JEDEC, "JC-42 Solid State Memories," [https://www.jedec.org/commit](https://www.jedec.org/committees/jc-42)
    [tees/jc-42.](https://www.jedec.org/committees/jc-42)

    - <span id="page-15-17"></span>[138] JEDEC, DDR3 SDRAM Specification, 2008.

    - <span id="page-15-18"></span>[139] JEDEC, "Low Power Double Data Rate 4 (LPDDR4)
    SDRAM Specification," JEDEC Standard JESD209–4B, 2014.

    - <span id="page-15-19"></span>[140] JEDEC, "Graphics Double Data Rate (GDDR5)
    SGRAM Standard," JEDEC Standard JESD212C, 2016.

    - <span id="page-15-20"></span>[141] JEDEC, "Graphics Double Data Rate (GDDR6)
    SGRAM Standard," JEDEC Standard JESD250C, 2021.

    - <span id="page-15-21"></span>[142] M. V. Beigi, Y. Cao, S. Gurumurthi, C. Recchia,
    A. Walton, and V. Sridharan, "A Systematic Study of DDR4 DRAM Faults in the Field,"
    in HPCA, 2023.

    - <span id="page-15-22"></span>[143] S.-L. Gong, J. Kim, and M. Erez, "DRAM Scaling
    Error Evaluation Model Using Various Retention Time," in DSN-W, 2017.

    - [144] B. R. Childers, J. Yang, and Y. Zhang, "Achieving Yield, Density and Performance
    Effective DRAM at Extreme Technology Sizes," in MEMSYS, 2015.

    - <span id="page-15-23"></span>[145] L. Peters, D. Potter, and R. Bowman, Cost
    Effective IC Manufacturing, 1998–1999. Integrated Circuit Engineering Corporation,
    1997.

    - <span id="page-15-24"></span>[146] J. Kang, "A Study of the DRAM Industry,"
    Master''s thesis, Massachusetts Institute of Technology, 2010.

    - [147] T. J. Dell, "A White Paper on the Benefits of Chipkill-Correct ECC for
    PC Server Main Memory," IBM Microelectronics Division, 1997.

    - [148] K. H. Lee, "A Strategic Analysis of the DRAM Industry After the Year 2000,"
    Master''s thesis, Massachusetts Institute of Technology, 2013.

    - <span id="page-15-25"></span>[149] J. A. Croswell, "A Model for Analysis of
    the Effects of Redundancy and Error Correction on DRAM Memory Yield and Reliability,"
    Master''s thesis, MIT, 2000.

    - <span id="page-15-27"></span>[150] J. Liu, B. Jaiyen, Y. Kim, C. Wilkerson,
    and O. Mutlu, "An Experimental Study of Data Retention Behavior in Modern DRAM
    Devices: Implications for Retention Time Profiling Mechanisms," in ISCA, 2013.

    - <span id="page-15-44"></span>[151] A. Barenghi, L. Breveglieri, N. Izzo, and
    G. Pelosi, "Software-Only Reverse Engineering of Physical DRAM Mappings For RowHammer
    Attacks," in IVSW, 2018.

    - <span id="page-15-41"></span>[152] M. Farmani, M. Tehranipoor, and F. Rahman,
    "RHAT: Efficient RowHammer-Aware Test for Modern DRAM Modules," in ETS, 2021.

    - <span id="page-15-45"></span>[153] M. Wang, Z. Zhang, Y. Cheng, and S. Nepal,
    "Dramdig: A Knowledge-Assisted Tool To Uncover DRAM Address Mapping," in DAC,
    2020.

    - [154] Y. Jiang, H. Zhu, H. Shan, X. Guo, X. Zhang, and Y. Jin, "TRRScope: Understanding
    Target Row Refresh Mechanism for Modern DDR Protection," in HOST, 2021.

    - <span id="page-15-28"></span>[155] H. Nam, S. Baek, M. Wi, M. J. Kim, J. Park,
    C. Song, N. S. Kim, and J. H. Ahn, "X-ray: Discovering DRAM Internal Structure
    and Error Characteristics by Issuing Memory Commands," IEEE CAL, 2023.

    - <span id="page-15-29"></span>[156] D. James, "Silicon Chip Teardown to the Atomic
    Scale–Challenges Facing the Reverse Engineering of Semiconductors," Microscopy
    and Microanalysis, 2010.

    - <span id="page-15-30"></span>[157] R. Torrance and D. James, "The State-of-the-Art
    in IC Reverse Engineering," in CHES, 2009.

    - <span id="page-15-31"></span>[158] Rugged Memory, SMART Modular Technologies,
    2021, [https://www.](https://www.smartm.com/product/rugged-memory) [smartm.com/product/rugged-memory.](https://www.smartm.com/product/rugged-memory)

    - <span id="page-15-32"></span>[159] Intelligent Memory, "IM ECC DRAM with Integrated
    Error Correcting Code," 2016, Product Brief.

    - <span id="page-15-33"></span>[160] RLDRAM Memory, Micron Technology, 2021, [https://www.micron.c](https://www.micron.com/products/dram/rldram-memory)
    [om/products/dram/rldram-memory.](https://www.micron.com/products/dram/rldram-memory)

    - <span id="page-15-34"></span>[161] J. Macri, "AMD''s Next Generation GPU and
    High Bandwidth Memory Architecture: FURY," in HCS, 2015.

    - <span id="page-15-35"></span>[162] Intel Corporation, "RLDRAM II and RLDRAM
    3 Features," 2023, [https:](https://www.intel.com/content/www/us/en/docs/programmable/710283/17-0/rldram-ii-and-rldram-3-features.html)
    [//www.intel.com/content/www/us/en/docs/programmable/710283](https://www.intel.com/content/www/us/en/docs/programmable/710283/17-0/rldram-ii-and-rldram-3-features.html)
    [/17-0/rldram-ii-and-rldram-3-features.html.](https://www.intel.com/content/www/us/en/docs/programmable/710283/17-0/rldram-ii-and-rldram-3-features.html)

    - <span id="page-15-36"></span>[163] J. Church, "Strategic Networking in Standard
    Setting Organizations: The Case of JEDEC," Master''s thesis, Cornell University,
    2007.

    - <span id="page-16-0"></span>[164] JEDEC, "Addendum No. 1 to JESD209-4, Low Power
    Double Data Rate 4x (LPDDR4X)," JEDEC Standard JESD209–4–1A, 2021.

    - <span id="page-16-1"></span>[165] JEDEC, "Addendum No. 1 to JESD79-4, 3D Stacked
    DRAM," JEDEC Standard JESD79-4-1B, 2021.

    - <span id="page-16-2"></span>[166] U. Kang, H.-s. Yu, C. Park, H. Zheng, J. Halbert,
    K. Bains, S. Jang, and J. S. Choi, "Co-Architecting Controllers and DRAM to Enhance
    DRAM Process Scaling," in The Memory Forum, 2014.

    - <span id="page-16-23"></span>[167] S. Cha, O. Seongil, H. Shin, S. Hwang, K.
    Park, S. J. Jang, J. S. Choi, G. Y. Jin, Y. H. Son, H. Cho, J. H. Ahn, and N.
    S. Kim, "Defect Analysis and Cost-Effective Resilience Architecture for Future
    DRAM Devices," in HPCA, 2017.

    - <span id="page-16-26"></span>[168] Micron Technology Inc., "ECC Brings Reliability
    and Power Efficiency to Mobile Devices," Micron Technology Inc., Tech. Rep., 2017.

    - [169] S.-K. Park, "Technology Scaling Challenge and Future Prospects of DRAM
    and NAND Flash Memory," in IMW, 2015.

    - <span id="page-16-3"></span>[170] Micron Technologies, Inc., "Quarterly Report
    on Form 10-Q," [https:](https://investors.micron.com/quarterly-results) [//investors.micron.com/quarterly-results,](https://investors.micron.com/quarterly-results)
    2022.

    - <span id="page-16-4"></span>[171] S. Ghose, A. G. Yaglık ˘ c¸ı, R. Gupta, D.
    Lee, K. Kudrolli, W. X. Liu, H. Hassan, K. K. Chang, N. Chatterjee, A. Agrawal,
    M. O''Connor, and O. Mutlu, "What Your DRAM Power Models Are Not Telling You:
    Lessons from a Detailed Experimental Study," SIGMETRICS, 2018.

    - <span id="page-16-5"></span>[172] K. Akarvardar and H.-S. P. Wong, "Technology
    Prospects for Data-Intensive Computing," Proc. IEEE, 2023.

    - <span id="page-16-6"></span>[173] B. Van Essen, R. Pearce, S. Ames, and M. Gokhale,
    "On the Role of NVRAM in Data-Intensive Architectures: An Evaluation," in IPDPS,
    2012.

    - [174] J. Ousterhout, P. Agrawal, D. Erickson, C. Kozyrakis, J. Leverich, D.
    Mazieres, S. Mitra, A. Narayanan, G. Parulkar, M. Rosenblum ` et al., "The Case
    for RAMClouds: Scalable High-Performance Storage Entirely in DRAM," SIGOPS OSR,
    2010.

    - [175] S. Lee, G.-J. Kim, N. Lee, K. Lee, B. Woo, J. Jin, J. Kim, Y. Lee, H.
    Kim, and S. Pae, "Reliability Characterization for Advanced DRAM using HK/MG+
    EUV Process Technology," in IEDM, 2021.

    - [176] X. Sun, H. Wan, Q. Li, C.-L. Yang, T.-W. Kuo, and C. J. Xue, "RM-SSD:
    In-Storage Computing for Large-Scale Recommendation Inference," in HPCA, 2022.

    - <span id="page-16-7"></span>[177] S. Park, H. Kim, K. Kim, J. So, J. Ahn, W.
    Lee, D. Kim, Y. Kim, J. Seok, J. Lee et al., "Scaling of Memory Performance and
    Capacity with CXL Memory Expander," in HCS, 2022.

    - <span id="page-16-8"></span>[178] B. Gervasi, "Will Carbon Nanotube Memory Replace
    DRAM?" IEEE Micro, 2019.

    - <span id="page-16-9"></span>[179] K. Hsieh, S. Khan, N. Vijaykumar, K. K. Chang,
    A. Boroumand, S. Ghose, and O. Mutlu, "Accelerating Pointer Chasing In 3D-Stacked
    Memory: Challenges, Mechanisms, Evaluation," in ICCD, 2016.

    - [180] M. Ferdman, A. Adileh, O. Kocberber, S. Volos, M. Alisafaee, D. Jevdjic,
    C. Kaynak, A. D. Popescu, A. Ailamaki, and B. Falsafi, "Clearing The Clouds: A
    Study Of Emerging Scale-Out Workloads On Modern Hardware," ASPLOS, 2012.

    - [181] A. Gutierrez, R. G. Dreslinski, T. F. Wenisch, T. Mudge, A. Saidi, C.
    Emmons, and N. Paver, "Full-System Analysis And Characterization Of Interactive
    Smartphone Applications," in IISWC, 2011.

    - [182] J. Hestness, S. W. Keckler, and D. A. Wood, "A Comparative Analysis Of
    Microarchitecture Effects On CPU and GPU Memory System Behavior," in IISWC, 2014.

    - [183] Y. Huang, Z. Zha, M. Chen, and L. Zhang, "Moby: A Mobile Benchmark Suite
    For Architectural Simulators," in ISPASS, 2014.

    - [184] Y. Zhu, D. Richins, M. Halpern, and V. J. Reddi, "Microarchitectural Implications
    Of Event-Driven Server-Side Web Applications," in MI-CRO, 2015.

    - <span id="page-16-28"></span>[185] G. F. Oliveira, J. Gomez-Luna, S. Ghose,
    L. Orosa, N. Vijaykumar, ´ I. Fernandez, M. Sadrosadati, and O. Mutlu, "DAMOV:
    A New Methodology and Benchmark Suite for Evaluating Data Movement Bottlenecks,"
    in IEEE Access, 2021.

    - [186] A. Boroumand, S. Ghose, Y. Kim, R. Ausavarungnirun, E. Shiu, R. Thakur,
    D. Kim, A. Kuusela, A. Knies, P. Ranganathan, and O. Mutlu, "Google Workloads
    for Consumer Devices: Mitigating Data Movement Bottlenecks," in ASPLOS, 2018.

    - [187] A. Boroumand, S. Ghose, B. Akin, R. Narayanaswami, G. F. Oliveira, X.
    Ma, E. Shiu, and O. Mutlu, "Google Neural Network Models for Edge Devices: Analyzing
    and Mitigating Machine Learning Inference Bottlenecks," in PACT, 2021.

    - [188] K. Kanellopoulos, N. Vijaykumar, C. Giannoula, R. Azizi, S. Koppula, N.
    M. Ghiasi, T. Shahroodi, J. G. Luna, and O. Mutlu, "SMASH: Co-Designing Software
    Compression and Hardware-Accelerated Indexing for Efficient Sparse Matrix Operations,"
    in MICRO, 2019.

    - [189] M. V. Wilkes, "The Memory Gap and The Future of High Performance Memories,"
    SIGARCH Computer Architecture News, 2001.

    - [190] W. A. Wulf and S. A. McKee, "Hitting the Memory Wall: Implications of
    the Obvious," SIGARCH Computer Architecture News, 1995.

    - [191] O. Mutlu and T. Moscibroda, "Stall-Time Fair Memory Access Scheduling
    for Chip Multiprocessors," in MICRO, 2007.

    - [192] O. Mutlu, J. Stark, C. Wilkerson, and Y. N. Patt, "Runahead Execution:
    An Alternative to Very Large Instruction Windows for Out-of-Order Processors,"
    in HPCA, 2003.

    - [193] S. Kanev, J. P. Darago, K. Hazelwood, P. Ranganathan, T. Moseley, G.-Y.
    Wei, and D. Brooks, "Profiling a Warehouse-scale Computer," in ISCA, 2015.

    - [194] R. Bera, A. V. Nori, O. Mutlu, and S. Subramoney, "DSPatch: Dual Spatial
    Pattern Prefetcher," in MICRO, 2019.

    - [195] R. Bera, K. Kanellopoulos, A. Nori, T. Shahroodi, S. Subramoney, and O.
    Mutlu, "Pythia: A Customizable Hardware Prefetching Framework using Online Reinforcement
    Learning," in MICRO, 2021.

    - [196] X. Liu, D. Roberts, R. Ausavarungnirun, O. Mutlu, and J. Zhao, "Binary
    Star: Coordinated Reliability in Heterogeneous Memory Systems for High Performance
    and Scalability," in MICRO, 2019.

    - [197] S. Ghose, A. Boroumand, J. S. Kim, J. Gomez-Luna, and O. Mutlu, ´ "Processing-in-Memory:
    A Workload-driven Perspective," IBM JRD, 2019.

    - <span id="page-16-24"></span>[198] W. Shin, J. Yang, J. Choi, and L.-S. Kim,
    "NUAT: A Non-Uniform Access Time Memory Controller," in HPCA, 2014.

    - [199] S. Ghose, T. Li, N. Hajinazar, D. S. Cali, and O. Mutlu, "Demystifying
    Complex Workload-DRAM Interactions: An Experimental Study," SIGMETRICS, 2019.

    - <span id="page-16-29"></span>[200] J. Gomez-Luna, J. El Hajj, I. Fernandez,
    and C. Giannoula, "Bench- ´ marking a New Paradigm: Understanding a Modern Processing-in-Memory
    Architecture," in arXiv:2105.03814, 2021.

    - <span id="page-16-30"></span>[201] J. Gomez-Luna, I. El Hajj, I. Fernandez,
    C. Giannoula, G. F. Oliveira, ´ and O. Mutlu, "Benchmarking Memory-Centric Computing
    Systems: Analysis of Real Processing-in-Memory Hardware," in CUT, 2021.

    - <span id="page-16-10"></span>[202] C. Giannoula, I. Fernandez, J. Gomez-Luna,
    N. Koziris, G. Goumas, ´ and O. Mutlu, "Towards Efficient Sparse Matrix Vector
    Multiplication on Real Processing-in-Memory Architectures," SIGMETRICS, 2022.

    - <span id="page-16-11"></span>[203] J. S. Kim, "Improving DRAM Performance, Security,
    and Reliability by Understanding and Exploiting DRAM Timing Parameter Margins,"
    Ph.D. dissertation, Carnegie Mellon University, 2020.

    - <span id="page-16-12"></span>[204] D. Lee, S. Ghose, G. Pekhimenko, S. Khan,
    and O. Mutlu, "Simultaneous Multi-Layer Access: Improving 3D-Stacked Memory Bandwidth
    at Low Cost," TACO, 2016.

    - <span id="page-16-13"></span>[205] J. Laudon, "UltraSPARC T1: Architecture and
    Physical Design of a 32-threaded General Purpose CPU," in ISSCC, 2006.

    - <span id="page-16-19"></span>[206] I. Bhati, Z. Chishti, S.-L. Lu, and B. Jacob,
    "Flexible Auto-Refresh: Enabling Scalable and Energy-Efficient DRAM Refresh Reductions,"
    in ISCA, 2015.

    - <span id="page-16-14"></span>[207] I. Bhati, M.-T. Chang, Z. Chishti, S.-L.
    Lu, and B. Jacob, "DRAM Refresh Mechanisms, Penalties, and Trade-Offs," in TC,
    2016.

    - <span id="page-16-15"></span>[208] T. Zhang, M. Poremba, C. Xu, G. Sun, and
    Y. Xie, "CREAM: A Concurrent-Refresh-Aware DRAM Memory Architecture," in HPCA,
    2014.

    - <span id="page-16-18"></span>[209] P. J. Nair, C.-C. Chou, and M. K. Qureshi,
    "Refresh Pausing in DRAM Memory Systems," in TACO, 2014.

    - <span id="page-16-16"></span>[210] R. Balasubramonian, "A DRAM Refresh Tutorial,"
    [http://utaharch.b](http://utaharch.blogspot.com/2013/11/a-dram-refresh-tutorial.html)
    [logspot.com/2013/11/a-dram-refresh-tutorial.html,](http://utaharch.blogspot.com/2013/11/a-dram-refresh-tutorial.html)
    2013.

    - <span id="page-16-25"></span>[211] J. Stuecheli, D. Kaseridis, H. C. Hunter,
    and L. K. John, "Elastic Refresh: Techniques to Mitigate Refresh Penalties in
    High Density Memory," in MICRO, 2010.

    - <span id="page-16-31"></span>[212] R. Balasubramonian, "Innovations in the Memory
    System," Synthesis Lectures on Computer Architecture, 2019.

    - <span id="page-16-17"></span>[213] W.-K. Cheng, P.-Y. Shen, and X.-L. Li, "Retention-Aware
    DRAM Auto-Refresh Scheme for Energy and Performance Efficiency," Micromachines,
    2019.

    - <span id="page-16-20"></span>[214] P. Nair, C.-C. Chou, and M. K. Qureshi, "A
    Case for Refresh Pausing in DRAM Memory Systems," in HPCA, 2013.

    - <span id="page-16-21"></span>[215] K. K. Chang, D. Lee, Z. Chishti, A. R. Alameldeen,
    C. Wilkerson, Y. Kim, and O. Mutlu, "Improving DRAM Performance by Parallelizing
    Refreshes with Accesses," in HPCA, 2014.

    - <span id="page-16-22"></span>[216] M. Horiguchi and K. Itoh, Nanoscale Memory
    Repair. Springer SBM, 2011.

    - [217] V. Sridharan and D. Liberty, "A Study of DRAM Failures in the Field,"
    in SC, 2012.

    - <span id="page-16-27"></span>[218] B. Schroeder, E. Pinheiro, and W.-D. Weber,
    "DRAM Errors in the Wild: a Large-Scale Field Study," in SIGMETRICS, 2009.

    - [219] T. C. May and M. H. Woods, "Alpha-Particle-Induced Soft Errors in Dynamic
    Memories," TED, 1979.

    - [220] T. J. O''Gorman, J. M. Ross, A. H. Taber, J. F. Ziegler, H. P. Muhlfeld,
    C. J. Montrose, H. W. Curtis, and J. L. Walsh, "Field Testing for Cosmic Ray Soft
    Errors in Semiconductor Memories," IBM JRD, 1996.

    - <span id="page-17-0"></span>[221] V. Sridharan, N. DeBardeleben, S. Blanchard,
    K. B. Ferreira, J. Stearley, J. Shalf, and S. Gurumurthi, "Memory Errors in Modern
    Systems: The Good, the Bad, and the Ugly," in ASPLOS, 2015.

    - <span id="page-17-1"></span>[222] D.-H. Kim and L. S. Milor, "ECC-ASPIRIN: An
    ECC-assisted Post-Package Repair Scheme for Aging Errors in DRAMs," in VTS, 2016.

    - <span id="page-17-2"></span>[223] O. Wada, T. Namekawa, H. Ito, A. Nakayama,
    and S. Fujii, "Post-Packaging Auto Repair Techniques for Fast Row Cycle Embedded
    DRAM," in TEST, 2004.

    - <span id="page-17-3"></span>[224] S. Gurumurthi, K. Lee, M. Jang, V. Sridharan,
    A. Nygren, Y. Ryu, K. Sohn, T. Kim, and H. Chung, "HBM3: Enabling Memory Resilience
    at Scale," IEEE CAL, 2021.

    - <span id="page-17-4"></span>[225] H. Lee, Y. Yoo, S. H. Shin, and S. Kang, "ECMO:
    ECC Architecture Reusing Content-Addressable Memories for Obtaining High Reliability
    in DRAM," TVLSI, 2022.

    - <span id="page-17-5"></span>[226] S. Jeong, S. Kang, and J.-S. Yang, "PAIR:
    Pin-aligned In-DRAM ECC architecture using expandability of Reed-Solomon code,"
    in DAC, 2020.

    - [227] S.-I. Pae, V. Kozhikkottu, D. Somasekar, W. Wu, S. G. Ramasubramanian,
    M. Dadual, H. Cho, and K.-W. Kwon, "Minimal Aliasing Single-Error-Correction Codes
    for DRAM Reliability Improvement," IEEE Access, 2021.

    - [228] Y. Luo, S. Govindan, B. Sharma, M. Santaniello, J. Meza, A. Kansal, J.
    Liu, B. Khessib, K. Vaid, and O. Mutlu, "Characterizing Application Memory Error
    Vulnerability to Optimize Datacenter Cost via Heterogeneous-Reliability Memory,"
    in DSN, 2014.

    - <span id="page-17-7"></span>[229] M. Patel, G. F. de Oliveira, and O. Mutlu,
    "HARP: Practically and Effectively Identifying Uncorrectable Errors in Memory
    Chips That Use On-Die Error-Correcting Codes," in MICRO, 2021.

    - <span id="page-17-6"></span>[230] M. Patel, "Enabling Effective Error Mitigation
    in Memory Chips That Use On-Die Error-Correcting Codes," Ph.D. dissertation, ETH
    Zurich, ¨ 2021.

    - <span id="page-17-8"></span>[231] K. Loughlin, S. Saroiu, A. Wolman, and B.
    Kasikci, "Stop! Hammer Time: Rethinking Our Approach to Rowhammer Mitigations,"
    in HotOS, 2021.

    - <span id="page-17-9"></span>[232] T. Yang and X.-W. Lin, "Trap-Assisted DRAM
    Row Hammer Effect," EDL, 2019.

    - [233] Z. Greenfield, J. B. Halbert, and K. S. Bains, "Method, Apparatus and
    System for Determining a Count of Accesses to a Row of Memory," 2014, US Patent
    US20140085995A1.

    - <span id="page-17-10"></span>[234] O. Mutlu, "Retrospective: Flipping Bits in
    Memory Without Accessing Them: An Experimental Study of DRAM Disturbance Errors,"
    in ISCA@50 25-Year Retrospective: 1996-2020, J. F. Mart´ınez and L. K. John, Eds.,
    2023.

    - <span id="page-17-11"></span>[235] M. Seaborn and T. Dullien, "Exploiting the
    DRAM Rowhammer Bug to Gain Kernel Privileges," Black Hat, 2015.

    - <span id="page-17-22"></span>[236] V. van der Veen, Y. Fratantonio, M. Lindorfer,
    D. Gruss, C. Maurice, G. Vigna, H. Bos, K. Razavi, and C. Giuffrida, "Drammer:
    Deterministic Rowhammer Attacks on Mobile Platforms," in CCS, 2016.

    - [237] D. Gruss, C. Maurice, and S. Mangard, "Rowhammer.js: A Remote Software-Induced
    Fault Attack in Javascript," in DIMVA, 2016.

    - [238] K. Razavi, B. Gras, E. Bosman, B. Preneel, C. Giuffrida, and H. Bos, "Flip
    Feng Shui: Hammering a Needle in the Software Stack," in USENIX Sec., 2016.

    - [239] Y. Xiao, X. Zhang, Y. Zhang, and R. Teodorescu, "One Bit Flips, One Cloud
    Flops: Cross-VM Row Hammer Attacks and Privilege Escalation," in USENIX Sec.,
    2016.

    - [240] E. Bosman, K. Razavi, H. Bos, and C. Giuffrida, "Dedup Est Machina: Memory
    Deduplication as An Advanced Exploitation Vector," in S&P, 2016.

    - [241] S. Bhattacharya and D. Mukhopadhyay, "Curious Case of RowHammer: Flipping
    Secret Exponent Bits using Timing Analysis," in CHES, 2016.

    - [242] R. Qiao and M. Seaborn, "A New Approach for Rowhammer Attacks," in HOST,
    2016.

    - [243] Y. Jang, J. Lee, S. Lee, and T. Kim, "SGX-Bomb: Locking Down the Processor
    via Rowhammer Attack," in SysTEX, 2017.

    - [244] M. T. Aga, Z. B. Aweke, and T. Austin, "When Good Protections go Bad:
    Exploiting anti-DoS Measures to Accelerate Rowhammer Attacks," in HOST, 2017.

    - <span id="page-17-20"></span>[245] A. Tatar, C. Giuffrida, H. Bos, and K. Razavi,
    "Defeating Software Mitigations Against Rowhammer: A Surgical Precision Hammer,"
    in RAID, 2018.

    - [246] D. Gruss, M. Lipp, M. Schwarz, D. Genkin, J. Juffinger, S. O''Connell,
    W. Schoechl, and Y. Yarom, "Another Flip in the Wall of Rowhammer Defenses," in
    IEEE S&P, 2018.

    - [247] M. Lipp, M. Schwarz, L. Raab, L. Lamster, M. T. Aga, C. Maurice, and D.
    Gruss, "Nethammer: Inducing Rowhammer Faults Through Network Requests," in EuroS&PW,
    2020.

    - [248] P. Frigo, C. Giuffrida, H. Bos, and K. Razavi, "Grand Pwning Unit: Accelerating
    Microarchitectural Attacks with the GPU," in S&P, 2018.

    - [249] L. Cojocar, K. Razavi, C. Giuffrida, and H. Bos, "Exploiting Correcting
    Codes: On the Effectiveness of ECC Memory Against Rowhammer Attacks," in S&P,
    2019.

    - [250] S. Ji, Y. Ko, S. Oh, and J. Kim, "Pinpoint Rowhammer: Suppressing Unwanted
    Bit Flips on Rowhammer Attacks," in ASIACCS, 2019.

    - [251] S. Hong, P. Frigo, Y. Kaya, C. Giuffrida, and T. Dumitras¸, "Terminal
    Brain Damage: Exposing the Graceless Degradation in Deep Neural Networks Under
    Hardware Fault Attacks," in USENIX Sec., 2019.

    - [252] A. Kwong, D. Genkin, D. Gruss, and Y. Yarom, "RAMBleed: Reading Bits in
    Memory Without Accessing Them," in S&P, 2020.

    - [253] L. Cojocar, J. Kim, M. Patel, L. Tsai, S. Saroiu, A. Wolman, and O. Mutlu,
    "Are We Susceptible to Rowhammer? An End-to-End Methodology for Cloud Providers,"
    in S&P, 2020.

    - [254] Z. Weissman, T. Tiemann, D. Moghimi, E. Custodio, T. Eisenbarth, and B.
    Sunar, "JackHammer: Efficient Rowhammer on Heterogeneous FPGA–CPU Platforms,"
    in TCHES, 2020.

    - [255] Z. Zhang, Y. Cheng, D. Liu, S. Nepal, Z. Wang, and Y. Yarom, "PTHammer:
    Cross-User-Kernel-Boundary Rowhammer Through Implicit Accesses," in MICRO, 2020.

    - [256] F. Yao, A. S. Rakin, and D. Fan, "Deephammer: Depleting the Intelligence
    of Deep Neural Networks Through Targeted Chain of Bit Flips," in USENIX Sec.,
    2020.

    - [257] A. Kogler, J. Juffinger, S. Qazi, Y. Kim, M. Lipp, N. Boichat, E. Shiu,
    M. Nissler, and D. Gruss, "Half-Double: Hammering From the Next Row Over," in
    USENIX Sec., 2022.

    - <span id="page-17-12"></span>[258] F. de Ridder, P. Frigo, E. Vannacci, H. Bos,
    C. Giuffrida, and K. Razavi, "SMASH: Synchronized Many-sided Rowhammer Attacks
    from JavaScript," in USENIX Sec., 2021.

    - [259] J. Lee, "Green Memory Solution," Investor''s Forum, Samsung Electronics,
    2014.

    - <span id="page-17-19"></span>[260] L. Cojocar, K. Razavi, C. Giuffrida, and
    H. Bos, "Exploiting Correcting Codes: On The Effectiveness Of ECC Memory Against
    Rowhammer Attacks," in S&P, 2019.

    - <span id="page-17-13"></span>[261] Micron Technology Inc., "8Gb: x4, x8, x16
    DDR4 SDRAM Features - Excessive Row Activation," Micron Technology Inc., Tech.
    Rep., 2020.

    - <span id="page-17-14"></span>[262] L. Cojocar, J. Kim, M. Patel, L. Tsai, S.
    Saroiu, A. Wolman, and O. Mutlu, "Are We Susceptible to Rowhammer? An End-to-End
    Methodology for Cloud Providers," in IEEE S&P, 2020.

    - <span id="page-17-15"></span>[263] K. K. Chang, A. G. Yaglık ˘ c¸ı, S. Ghose,
    A. Agrawal, N. Chatterjee, A. Kashyap, D. Lee, M. O''Connor, H. Hassan, and O.
    Mutlu, "Understanding Reduced-Voltage Operation in Modern DRAM Devices: Experimental
    Characterization, Analysis, and Mechanisms," in SIG-METRICS, 2017.

    - <span id="page-17-16"></span>[264] V. Seshadri and O. Mutlu, "Simple Operations
    in Memory to Reduce Data Movement," in Advances in Computers, 2017.

    - [265] V. Seshadri and O. Mutlu, "In-DRAM Bulk Bitwise Execution Engine," Advances
    in Computers, 2020.

    - <span id="page-17-23"></span>[266] B. Talukder, J. Kerns, B. Ray, T. Morris,
    and M. T. Rahman, "Exploiting DRAM Latency Variations for Generating True Random
    Numbers," ICCE, 2019.

    - <span id="page-17-24"></span>[267] B. Talukder, B. Ray, M. Tehranipoor, D. Forte,
    and M. T. Rahman, "LDPUF: Exploiting DRAM Latency Variations to Generate Robust
    Device Signatures," arXiv preprint arXiv:1808.02584, 2018.

    - [268] M. Yue, N. Karimian, W. Yan, N. A. Anagnostopoulos, and F. Tehranipoor,
    "DRAM-Based Authentication Using Deep Convolutional Neural Networks," IEEE Consumer
    Electronics Magazine, 2020.

    - [269] M. S. Hashemian, B. Singh, F. Wolff, D. Weyer, S. Clay, and C. Papachristou,
    "A Robust Authentication Methodology Using Physically Unclonable Functions in
    DRAM Arrays," in DATE, 2015.

    - <span id="page-17-17"></span>[270] A. Schaller, W. Xiong, N. A. Anagnostopoulos,
    M. U. Saleem, S. Gabmeyer, B. Skori ˇ c, S. Katzenbeisser, and J. Szefer, "Decay-Based
    DRAM ´ PUFs in Commodity Devices," TDSC, 2018.

    - <span id="page-17-18"></span>[271] B. B. Talukder, B. Ray, D. Forte, and M.
    T. Rahman, "PreLatPUF: Exploiting DRAM Latency Variations For Generating Robust
    Device Signatures," IEEE Access, 2019.

    - <span id="page-17-21"></span>[272] H. Hassan, N. Vijaykumar, S. Khan, S. Ghose,
    K. Chang, G. Pekhimenko, D. Lee, O. Ergin, and O. Mutlu, "SoftMC: A Flexible and
    Practical Open-Source Infrastructure for Enabling Experimental DRAM Studies,"
    in HPCA, 2017.

    - <span id="page-18-0"></span>[273] H. David, C. Fallin, E. Gorbatov, U. R. Hanebutte,
    and O. Mutlu, "Memory Power Management via Dynamic Voltage/Frequency Scaling,"
    in ICAC, 2011.

    - <span id="page-18-3"></span>[274] X. Pan and F. Mueller, "Hiding DRAM Refresh
    Overhead in Real-Time Cyclic Executives," in RTSS, 2019.

    - <span id="page-18-4"></span>[275] J. Mukundan, H. Hunter, K.-h. Kim, J. Stuecheli,
    and J. F. Mart´ınez, "Understanding and Mitigating Refresh Overheads in High-Density
    DDR4 DRAM Systems," in ISCA, 2013.

    - <span id="page-18-1"></span>[276] T. Hamamoto, S. Sugiura, and S. Sawada, "Well
    Concentration: A Novel Scaling Limitation Factor Derived From DRAM Retention Time
    and Its Modeling," in IEDM, 1995.

    - <span id="page-18-2"></span>[277] T. Hamamoto, S. Sugiura, and S. Sawada, "On
    the Retention Time Distribution of Dynamic Random Access Memory (DRAM)," in TED,
    1998.

    - <span id="page-18-5"></span>[278] D.-T. Nguyen, N.-M. Ho, M.-S. Le, W.-F. Wong,
    and I.-J. Chang, "ZEM: Zero-Cycle Bit-Masking Module for Deep Learning Refresh-Less
    DRAM," IEEE Access, 2021.

    - <span id="page-18-6"></span>[279] Y. Han, Y. Wang, H. Li, and X. Li, "Data-Aware
    DRAM Refresh to Squeeze the Margin of Retention Time in Hybrid Memory Cube," in
    ICCAD, 2014.

    - <span id="page-18-7"></span>[280] H. Choi, D. Hong, J. Lee, and S. Yoo, "Reducing
    DRAM Refresh Power Consumption by Runtime Profiling of Retention Time and Dual-Row
    Activation," Microprocessors and Microsystems, 2020.

    - <span id="page-18-8"></span>[281] Apple Inc., "About the Security Content of
    Mac EFI Security Update 2015-001," [https://support.apple.com/en-us/HT204934,](https://support.apple.com/en-us/HT204934)
    2015.

    - <span id="page-18-9"></span>[282] B. Aichinger, "DDR Memory Errors Caused by
    Row Hammer," in HPEC, 2015.

    - <span id="page-18-10"></span>[283] H. Hassan, M. Patel, J. S. Kim, A. G. Yaglık
    ˘ c¸ı, N. Vijaykumar, N. M. Ghiasi, S. Ghose, and O. Mutlu, "CROW: A Low-Cost
    Substrate for Improving DRAM Performance, Energy Efficiency, and Reliability,"
    in ISCA, 2019.

    - [284] A. Di Dio, K. Koning, H. Bos, and C. Giuffrida, "Copy-on-Flip: Hardening
    ECC Memory Against Rowhammer Attacks," in NDSS, 2023.

    - [285] F. Brasser, L. Davi, D. Gens, C. Liebchen, and A.-R. Sadeghi, "Can''t
    Touch This: Practical and Generic Software-Only Defenses Against Rowhammer Attacks,"
    in CoRR, 2016.

    - [286] A. Saxena, G. Saileshwar, P. J. Nair, and M. Qureshi, "Aqua: Scalable
    RowHammer Mitigation by Quarantining Aggressor Rows at Runtime," in MICRO, 2022.

    - <span id="page-18-11"></span>[287] K. Loughlin, J. Rosenblum, S. Saroiu, A.
    Wolman, D. Skarlatos, and B. Kasikci, "Siloz: Leveraging DRAM Isolation Domains
    to Prevent Inter-VM Rowhammer," in SOSP, 2023.

    - <span id="page-18-12"></span>[288] Z. B. Aweke, S. F. Yitbarek, R. Qiao, R.
    Das, M. Hicks, Y. Oren, and T. Austin, "ANVIL: Software-Based Protection Against
    Next-Generation Rowhammer Attacks," in ASPLOS, 2016.

    - [289] M. Son, H. Park, J. Ahn, and S. Yoo, "Making DRAM Stronger Against Row
    Hammering," in DAC, 2017.

    - [290] S. M. Seyedzadeh, A. K. Jones, and R. Melhem, "Mitigating Wordline Crosstalk
    Using Adaptive Trees of Counters," in ISCA, 2018.

    - [291] J. M. You and J.-S. Yang, "MRLoc : Mitigating Row-Hammering Based on Memory
    Locality," in DAC, 2019.

    - [292] E. Lee, I. Kang, S. Lee, G. Edward Suh, and J. Ho Ahn, "TWiCe: Preventing
    Row-Hammering by Exploiting Time Window Counters," in ISCA, 2019.

    - [293] Y. Park, W. Kwon, E. Lee, T. J. Ham, J. H. Ahn, and J. W. Lee, "Graphene:
    Strong yet Lightweight Row Hammer Protection," in MICRO, 2020.

    - [294] D.-H. Kim, P. J. Nair, and M. K. Qureshi, "Architectural Support for Mitigating
    Row Hammering in DRAM Memories," IEEE CAL, 2014.

    - [295] I. Kang, E. Lee, and J. H. Ahn, "CAT-TWO: Counter-Based Adaptive Tree,
    Time Window Optimized for DRAM Row-Hammer Prevention," IEEE Access, 2020.

    - [296] K. Bains, J. Halbert, C. Mozak, T. Schoenborn, and Z. Greenfield, "Row
    Hammer Refresh Command," 2015, U.S. Patent 9,117,544.

    - [297] K. S. Bains and J. B. Halbert, "Distributed Row Hammer Tracking," 2016,
    U.S. Patent 9,299,400.

    - [298] K. S. Bains and J. B. Halbert, "Row Hammer Monitoring Based on Stored
    Row Hammer Threshold Value," 2016, U.S. Patent 9,384,821.

    - [299] F. Devaux and R. Ayrignac, "Method and Circuit for Protecting a DRAM Memory
    Device from the Row Hammer Effect," 2021, 10,885,966.

    - <span id="page-18-17"></span>[300] A. G. Yaglık ˘ c¸ı, J. S. Kim, F. Devaux,
    and O. Mutlu, "Security Analysis of the Silver Bullet Technique for RowHammer
    Prevention," 2021.

    - <span id="page-18-28"></span>[301] M. Marazzi, P. Jattke, S. Flavien, and K.
    Razavi, "ProTRR: Principled yet Optimal In-DRAM Target Row Refresh," in SP, 2022.

    - [302] Y. Kim, "Architectural Techniques to Enhance DRAM Scaling," Ph.D. dissertation,
    Carnegie Mellon University, 2015.

    - [303] M. Marazzi, F. Solt, P. Jattke, K. Takashi, and K. Razavi, "REGA: Scalable
    Rowhammer Mitigation with Refresh-Generating Activations," in S&P, 2023.

    - <span id="page-18-13"></span>[304] M. Qureshi, A. Rohan, G. Saileshwar, and
    P. J. Nair, "Hydra: Enabling Low-Overhead Mitigation of Row-Hammer at Ultra-Low
    Thresholds via Hybrid Tracking," in ISCA, 2022.

    - <span id="page-18-14"></span>[305] S. Lee, K.-D. Kang, G. Park, N. S. Kim, and
    D. Kim, "NoHammer: Preventing Row Hammer with Last-Level Cache Management," IEEE
    CAL, 2023.

    - <span id="page-18-15"></span>[306] T. Bennett, S. Saroiu, A. Wolman, and L.
    Cojocar, "Panopticon: A Complete In-DRAM Rowhammer Mitigation," in DRAMSec, 2021.

    - <span id="page-18-16"></span>[307] Y. Kim, V. Seshadri, D. Lee, J. Liu, and
    O. Mutlu, "A Case for Exploiting Subarray-Level Parallelism (SALP) in DRAM," in
    ISCA, 2012.

    - <span id="page-18-18"></span>[308] D. Yun, M. Park, C. Lim, and S. Baeg, "Study
    of TID Effects on One Row Hammering using Gamma in DDR4 SDRAMs," in IRPS, 2018.

    - [309] C. Lim, K. Park, and S. Baeg, "Active Precharge Hammering to Monitor Displacement
    Damage using High-Energy Protons in 3x-nm SDRAM," IEEE Trans. Nucl. Sci., 2016.

    - [310] A. Olgun, M. Osseiran, A. G. Yaglık ˘ c¸ı, Y. C. Tugrul, H. Luo, S. Rhyner,
    ˘ B. Salami, J. G. Luna, and O. Mutlu, "An Experimental Analysis of RowHammer
    in HBM2 DRAM Chips," in DSN-S, 2023.

    - <span id="page-18-19"></span>[311] Z. Lang, P. Jattke, M. Marazzi, and K. Razavi,
    "BLASTER: Characterizing the Blast Radius of Rowhammer," in DRAMSec, 2023.

    - <span id="page-18-26"></span>[312] Synopsys, "Reliability, Availability and
    Serviceability (RAS) for Memory Interfaces," Synopsys, Tech. Rep., 2015.

    - [313] T. J. Dell, "System RAS Implications of DRAM Soft Errors," IBM JRD, 2008.

    - <span id="page-18-27"></span>[314] C. Slayman, M. Ma, and S. Lindley, "Impact
    of Error Correction Code and Dynamic Memory Reconfiguration on High-Reliability/Low-Cost
    Server Memory," in IRWS, 2006.

    - <span id="page-18-20"></span>[315] R. Sharifi and Z. Navabi, "Online Profiling
    for Cluster-Specific Variable Rate Refreshing in High-Density DRAM Systems," in
    ETS, 2017.

    - [316] A. R. Alameldeen, I. Wagner, Z. Chishti, W. Wu, C. Wilkerson, and S.-L.
    Lu, "Energy-Efficient Cache Design Using Variable-Strength Error-Correcting Codes,"
    ISCA, 2011.

    - [317] H. Naeimi, C. Augustine, A. Raychowdhury, S.-L. Lu, and J. Tschanz, "STTRAM
    Scaling and Retention Failure," Intel Technology Journal, 2013.

    - [318] M. Awasthi, M. Shevgoor, K. Sudan, B. Rajendran, R. Balasubramonian, and
    V. Srinivasan, "Efficient Scrub Mechanisms for Error-Prone Emerging Memories,"
    in HPCA, 2012.

    - <span id="page-18-21"></span>[319] M. J. M. Rahman, "Utilizing Two Stage Scrubbing
    to Handle Single-Fault Multi-Error Cases in DRAM Systems," Master''s thesis, Iowa
    State University, 2021.

    - <span id="page-18-22"></span>[320] D. Kline, J. Zhang, R. Melhem, and A. K.
    Jones, "Flower and Fame: A Low Overhead Bit-Level Fault-Map and Fault-Tolerance
    Approach for Deeply Scaled Memories," in HPCA, 2020.

    - <span id="page-18-25"></span>[321] S. Longofono, D. Kline Jr, R. Melhem, and
    A. K. Jones, "Predicting and Mitigating Single-Event Upsets in DRAM using HOTH,"
    Microelectronics Reliability, 2021.

    - [322] D. Kline, R. Melhem, and A. K. Jones, "Sustainable Fault Management and
    Error Correction for Next-Generation Main Memories," in IGSC, 2017.

    - [323] S. Schechter, G. H. Loh, K. Strauss, and D. Burger, "Use ECP, Not ECC,
    for Hard Failures in Resistive Memories," ISCA, 2010.

    - [324] P. J. Nair, B. Asgari, and M. K. Qureshi, "SuDoku: Tolerating High-Rate
    of Transient Failures for Enabling Scalable STTRAM," in DSN, 2019.

    - [325] J. Zhang, D. Kline, L. Fang, R. Melhem, and A. K. Jones, "Dynamic Partitioning
    To Mitigate Stuck-At Faults in Emerging Memories," in ICCAD, 2017.

    - [326] H. Wang, "Architecting Memory Systems Upon Highly Scaled Error-Prone Memory
    Technologies," Ph.D. dissertation, Rensselaer Polytechnic Institute, 2017.

    - <span id="page-18-24"></span><span id="page-18-23"></span>[327] D. W. Kim and
    M. Erez, "RelaxFault Memory Repair," in ISCA, 2016.

    - [328] L. Mukhanov, K. Tovletoglou, H. Vandierendonck, D. S. Nikolopoulos, and
    G. Karakonstantis, "Workload-Aware DRAM Error Prediction Using Machine Learning,"
    in IISWC, 2019.

    - [329] E. Baseman, N. DeBardeleben, K. Ferreira, S. Levy, S. Raasch, V. Sridharan,
    T. Siddiqua, and Q. Guan, "Improving DRAM Fault Characterization Through Machine
    Learning," in DSN-W, 2016.

    - [330] I. Giurgiu, J. Szabo, D. Wiesmann, and J. Bird, "Predicting DRAM Reliability
    in the Field with Machine Learning," in Middleware, 2017.

    - [331] Z. Lan, J. Gu, Z. Zheng, R. Thakur, and S. Coghlan, "A Study of Dynamic
    Meta-Learning for Failure Prediction in Large-Scale Systems," PDC, 2010.

    - [332] Y. Liang, Y. Zhang, A. Sivasubramaniam, M. Jette, and R. Sahoo, "Bluegene/L
    Failure Analysis and Prediction Models," in DSN, 2006.

    - <span id="page-19-0"></span>[333] I. Boixaderas, D. Zivanovic, S. More, J. Bartolome,
    D. Vicente, ´ M. Casas, P. M. Carpenter, P. Radojkovic, and E. Ayguad ´ e, "Cost-
    ´ Aware Prediction of Uncorrected DRAM Errors in the Field," in SC, 2020.

    - <span id="page-19-1"></span>[334] G. Agrawal, L. Massengill, and K. Gulati,
    "A Proposed SEU Tolerant Dynamic Random Access Memory (DRAM) Cell," IEEE Trans.
    Nucl. Sci., 1994.

    - [335] Infineon, "Radiation Hardened & High Reliability Memories," [https:](https://www.infineon.com/cms/en/product/memories/radiation-hardened-high-reliability-memories/)
    [//www.infineon.com/cms/en/product/memories/radiation-hardene](https://www.infineon.com/cms/en/product/memories/radiation-hardened-high-reliability-memories/)
    [d-high-reliability-memories/,](https://www.infineon.com/cms/en/product/memories/radiation-hardened-high-reliability-memories/)
    2022.

    - [336] N. C. Lu, "Advanced Cell Structures for Dynamic RAMs," IEEE Circuits and
    Devices Magazine, 1989.

    - [337] S. K. Banerjee, "Two-Transistor DRAM Cell with High Alpha Particle Immunity,"
    1989, US Patent 4,864,374.

    - [338] P. Mazumder, "Design of a Fault-Tolerant Three-Dimensional Dynamic Random-Access
    Memory with On-Chip Error-Correcting Circuit," TOC, 1993.

    - [339] Data Device Corporation, "Rad Hard Memories," [https://www.ddc-w](https://www.ddc-web.com/en/radhard/memories)
    [eb.com/en/radhard/memories,](https://www.ddc-web.com/en/radhard/memories) 2022.

    - <span id="page-19-2"></span>[340] 3D PLUS, "DDR4 SDRAM," [https://www.3d-plus.com/product.php?](https://www.3d-plus.com/product.php?fam=8&prod=43)
    [fam=8&prod=43,](https://www.3d-plus.com/product.php?fam=8&prod=43) 2022.

    - <span id="page-19-3"></span>[341] D. M. Mathew, H. Kattan, C. Weis, J. Henkel,
    N. Wehn, and H. Amrouch, "Thermoelectric Cooling to Survive Commodity DRAMs in
    Harsh Environment Automotive Electronics," IEEE Access, 2021.

    - <span id="page-19-4"></span>[342] K. Kobayashi, "Highly-reliable Integrated
    Circuits for Ground and Space Applications," ASICON, 2017.

    - <span id="page-19-5"></span>[343] J. Kim, M. Sullivan, S. Lym, and M. Erez,
    "All-Inclusive ECC: Thorough End-to-End Protection for Reliable Computer Memory,"
    in ISCA, 2016.

    - <span id="page-19-6"></span>[344] S. Lee, N. S. Kim, and D. Kim, "Exploiting
    OS-Level Memory Offlining for DRAM Power Management," IEEE CAL, 2019.

    - <span id="page-19-7"></span>[345] T. K. Moon, Error Correction Coding: Mathematical
    Methods and Algorithms. John Wiley & Sons, 2005.

    - [346] T. Richardson and R. Urbanke, Modern Coding Theory. Cambridge University
    Press, 2008.

    - [347] R. M. Roth, Introduction to Coding Theory. Cambridge University Press,
    2006.

    - [348] G. C. Clark Jr and J. B. Cain, Error-Correction Coding for Digital Communications.
    Springer SBM, 2013.

    - [349] D. J. Costello and S. Lin, Error Control Coding: Fundamentals and Applications.
    Prentice Hall, 1982.

    - <span id="page-19-8"></span>[350] S. Lin and D. J. Costello, Error Control Coding:
    Fundamentals and Applications. Prentice Hall, 2004.

    - <span id="page-19-9"></span>[351] I. Alam, "Lightweight Opportunistic Memory
    Resilience," Ph.D. dissertation, University of California, Los Angeles, 2021.

    - <span id="page-19-10"></span>[352] K. Baker and J. Van Beers, "Shmoo Plotting:
    The Black Art of IC Testing," IEEE Des Test, 1997.

    - <span id="page-19-11"></span>[353] Advantest, T5833/T5833ES Memory Test System,
    [https://www.advant](https://www.advantest.com/products/memory/t5833.html) [est.com/products/memory/t5833.html,](https://www.advantest.com/products/memory/t5833.html)
    2022.

    - <span id="page-19-12"></span>[354] Teradyne, Magnum EPIC Ultra-high Performance
    Solution for Memory Device Test, [https://www.teradyne.com/products/magnum-epic/,](https://www.teradyne.com/products/magnum-epic/)
    2022.

    - <span id="page-19-13"></span>[355] C.-S. Hou, J.-F. Li, C.-Y. Lo, D.-M. Kwai,
    Y.-F. Chou, and C.-W. Wu, "An FPGA-Based Test Platform for Analyzing Data Retention
    Time Distribution of DRAMs," in VLSI-DAT, 2013.

    - <span id="page-19-35"></span>[356] C. Weis, M. Jung, P. Ehses, C. Santos, P.
    Vivet, S. Goossens, M. Koedam, and N. Wehn, "Retention Time Measurements and Modelling
    of Bit Error Rates of Wide I/O DRAM in MPSoCs," in DATE, 2015.

    - <span id="page-19-36"></span>[357] F. Wang, T. Vogelsang, B. Haukness, and S.
    C. Magee, "DRAM Retention at Cryogenic Temperatures," in IMW, 2018.

    - <span id="page-19-14"></span>[358] R. Ladbury, M. Berg, E. Wilcox, K. LaBel,
    H. Kim, A. Phan, and C. Seidleck, "Use of Commercial FPGA-Based Evaluation Boards
    for Single-Event Testing of DDR2 and DDR3 SDRAMS," IEEE Trans. Nucl. Sci., 2013.

    - <span id="page-19-15"></span>[359] P. Software, "MemTest86 Overview," [https://www.memtest86.com/in](https://www.memtest86.com/index.html)
    [dex.html,](https://www.memtest86.com/index.html) 2019.

    - <span id="page-19-16"></span>[360] P. Francis-Mezger and V. M. Weaver, "A Raspberry
    Pi Operating System for Exploring Advanced Memory System Concepts," in MEMSYS,
    2018.

    - <span id="page-19-17"></span>[361] T.-Y. Oh, H. Chung, J.-Y. Park, K.-W. Lee,
    S. oh, S.-Y. Doo, H.-J. Kim, C. Lee, H.-R. Kim, J.-H. Lee, J.-I. Lee, K.-S. Ha,
    Y. Choi, Y.-C. Cho, Y.-C. Bae, T. Jang, C. Park, K. Park, S. Jang, and J. Choi,
    "A 3.2Gbps/pin 8Gb 1.0V LPDDR4 SDRAM with Integrated ECC Engine for Sub-1V DRAM
    Core Operation," in ISSCC, 2014.

    - [362] T.-Y. Oh, H. Chung, J.-Y. Park, K.-W. Lee, S. Oh, S.-Y. Doo, H.-J. Kim,
    C. Lee, H.-R. Kim, J.-H. Lee, J.-I. Lee, K.-S. Ha, Y. Choi, Y.-C. Cho, Y.-C. Bae,
    T. Jang, C. Park, K. Park, S. Jang, and J. S. Choi, "A 3.2 Gbps/Pin 8 Gbit 1.0
    V LPDDR4 SDRAM with Integrated ECC Engine for Sub-1 V DRAM Core Operation," JSSC,
    2014.

    - <span id="page-19-18"></span>[363] S. Kwon, Y. H. Son, and J. H. Ahn, "Understanding
    DDR4 in Pursuit of In-DRAM ECC," in ISOCC, 2014.

    - <span id="page-19-19"></span>[364] K. K. Chang, P. J. Nair, D. Lee, S. Ghose,
    M. K. Qureshi, and O. Mutlu, "Low-Cost Inter-Linked Subarrays (LISA): Enabling
    Fast Inter-Subarray Data Movement in DRAM," in HPCA, 2016.

    - <span id="page-19-20"></span>[365] F. Bostancı, A. Olgun, L. Orosa, A. G. Yaglık
    ˘ c¸ı, J. S. Kim, H. Hassan, O. Ergin, and O. Mutlu, "DR-STRaNGe: End-to-End System
    Design for DRAM-based True Random Number Generators," HPCA, 2022.

    - <span id="page-19-21"></span>[366] T. Zhang, K. Chen, C. Xu, G. Sun, T. Wang,
    and Y. Xie, "Half-DRAM: A High-Bandwidth and Low-Power DRAM Architecture from
    the Rethinking of Fine-Grained Activation," in ISCA, 2014.

    - <span id="page-19-22"></span>[367] A. Bacchini, M. Rovatti, G. Furano, and M.
    Ottavi, "Characterization of Data Retention Faults in DRAM Devices," in DFT, 2014.

    - [368] A. Weber, A. Birner, and W. Krautschneider, "Data Retention Analysis on
    Individual Cells of 256Mb DRAM in 110nm Technology," in ESSDERC, 2005.

    - <span id="page-19-23"></span>[369] K. Yamaguchi, "Theoretical Study of Deep-Trap-Assisted
    Anomalous Currents in Worst-Bit Cells of Dynamic Random-Access Memories (DRAM''s),"
    TED, 2000.

    - <span id="page-19-24"></span>[370] A. J. Walker, S. Lee, and D. Beery, "On DRAM
    Rowhammer and the Physics of Insecurity," TED, 2021.

    - <span id="page-19-25"></span>[371] A. Das, H. Hassan, and O. Mutlu, "VRL-DRAM:
    Improving DRAM Performance Via Variable Refresh Latency," in DAC, 2018.

    - [372] T. Siddiqua, A. E. Papathanasiou, A. Biswas, S. Gurumurthi, I. Corp, and
    T. Aster, "Analysis and Modeling of Memory Errors From Large-Scale Field Data
    Collection," in SELSE, 2013.

    - <span id="page-19-26"></span>[373] J. Meza, Q. Wu, S. Kumar, and O. Mutlu, "A
    Large-Scale Study of Flash Memory Errors in the Field," in SIGMETRICS, 2015.

    - <span id="page-19-27"></span>[374] S. Jin, J.-H. Yi, J. H. Choi, D. G. Kang,
    Y. J. Park, and H. S. Min, "Prediction of Data Retention Time Distribution of
    DRAM by Physics-Based Statistical Simulation," TED, 2005.

    - <span id="page-19-32"></span>[375] A. Hiraiwa, M. Ogasawara, N. Natsuaki, Y.
    Itoh, and H. Iwai, "Statistical Modeling of Dynamic Random Access Memory Data
    Retention Characteristics," JAP, 1996.

    - <span id="page-19-31"></span>[376] Y. Li, H. Schneider, F. Schnabel, R. Thewes,
    and D. Schmitt-Landsiedel, "DRAM Yield Analysis and Optimization by a Statistical
    Design Approach," in CSI, 2011.

    - [377] A. Hiraiwa, M. Ogasawara, N. Natsuaki, Y. Itoh, and H. Iwai, "Local-Field-Enhancement
    Model of DRAM Retention Failure," in IEDM, 1998.

    - [378] N. Edri, P. Meinerzhagen, A. Teman, A. Burg, and A. Fish, "Silicon-Proven,
    Per-Cell Retention Time Distribution Model for Gain-Cell Based eDRAMs," IEEE TOCS,
    2016.

    - [379] K. Kim and J. Lee, "A New Investigation of Data Retention Time in Truly
    Nanoscaled DRAMs," in EDL, 2009.

    - <span id="page-19-28"></span>[380] W. Kong, P. C. Parries, G. Wang, and S. S.
    Iyer, "Analysis of Retention Time Distribution of Embedded DRAM-A New Method to
    Characterize Across-Chip Threshold Voltage Variation," in ITC, 2008.

    - <span id="page-19-29"></span>[381] D.-H. Kim, S. Cha, and L. S. Milor, "AVERT:
    An Elaborate Model for Simulating Variable Retention Time in DRAMs," Microelectronics
    Reliability, 2015.

    - <span id="page-19-37"></span>[382] D. S. Yaney, C.-Y. Lu, R. A. Kohler, M. J.
    Kelly, and J. T. Nelson, "A Meta-Stable Leakage Phenomenon in DRAM Charge Storage-Variable
    Hold Time," in IEDM, 1987.

    - <span id="page-19-33"></span>[383] P. J. Restle, J. Park, and B. F. Lloyd, "DRAM
    Variable Retention Time," in IEDM, 1992.

    - <span id="page-19-34"></span>[384] C. G. Shirley and W. R. Daasch, "Copula Models
    of Correlation: A DRAM Case Study," in TC, 2014.

    - [385] H. Kim, B. Oh, Y. Son, K. Kim, S.-Y. Cha, J.-G. Jeong, S.-J. Hong, and
    H. Shin, "Characterization of the Variable Retention Time in Dynamic Random Access
    Memory," TED, 2011.

    - [386] H. Kim, B. Oh, Y. Son, K. Kim, S.-Y. Cha, J.-G. Jeong, S.-J. Hong, and
    H. Shin, "Study of Trap Models Related to the Variable Retention Time Phenomenon
    in DRAM," TED, 2011.

    - <span id="page-19-38"></span>[387] N. Kumar, "Detection of Variable Retention
    Time in DRAM," Master''s thesis, Portland State University, Portland, Oregon,
    2014.

    - [388] Y. Mori, K. Ohyu, K. Okonogi, and R. i. Yamada, "The Origin of Variable
    Retention Time in DRAM," in IEDM, 2005.

    - <span id="page-19-30"></span>[389] K. Ohyu, T. Umeda, K. Okonogi, S. Tsukada,
    M. Hidaka, S. Fujieda, and Y. Mochizuki, "Quantitative Identification for the
    Physical Origin of Variable Retention Time: A Vacancy-Oxygen Complex Defect Model,"
    in IEDM, 2006.

    - <span id="page-20-0"></span>[390] Sentaurus Sdevice User''s Manual, Synopsys,
    2018.

    - [391] M. Duan, F. Adam-Lema, B. Cheng, C. Navarro, X. Wang, V. Georgiev, F.
    Gamiz, C. Millar, and A. Asenov, "2D-TCAD Simulation on Retention Time of Z2FET
    for DRAM Application," in SISPAD, 2017.

    - <span id="page-20-1"></span>[392] P. Pfaffli, H. Wong, X. Xu, L. Silvestri,
    X. Lin, T. Yang, R. Tiwari, ¨ S. Mahapatra, S. Motzny, V. Moroz, and T. Ma, "TCAD
    Modeling for Reliability," Microelectronics Reliability, 2018.

    - <span id="page-20-2"></span>[393] H. Luo, T. Shahroodi, H. Hassan, M. Patel,
    A. Giray Yaglık ˘ c¸ı, L. Orosa, J. Park, and O. Mutlu, "CLR-DRAM: A Low-Cost
    DRAM Architecture Enabling Dynamic Capacity-Latency Trade-Off," in ISCA, 2020.

    - [394] H. H. Shin and E.-Y. Chung, "In-DRAM Cache Management for Low Latency
    and Low Power 3D-Stacked DRAMs," Micromachines, 2019.

    - <span id="page-20-3"></span>[395] Y. Wang, L. Orosa, X. Peng, Y. Guo, S. Ghose,
    M. Patel, J. S. Kim, J. G. Luna, M. Sadrosadati, N. M. Ghiasi, and O. Mutlu, "FIGARO:
    Improving System Performance via Fine-Grained In-DRAM Data Relocation and Caching,"
    in MICRO, 2020.

    - <span id="page-20-4"></span>[396] A. J. Walker, S. Lee, and D. Beery, "On DRAM
    Rowhammer and the Physics of Insecurity," TED, 2021.

    - <span id="page-20-6"></span><span id="page-20-5"></span>[397] T. Baker, "Opening
    Up Ada-Tasking," ACM SIGAda Ada Letters, 1990.

    - [398] K. Duganapalli, "Modelling and Test Generation for Crosstalk Faults in
    DSM Chips," Ph.D. dissertation, Universitat Bremen, 2016. ¨

    - [399] L. Cojocar, K. Loughlin, S. Saroiu, B. Kasikci, and A. Wolman, "mFIT:
    A Bump-in-the-Wire Tool for Plug-and-Play Analysis of Rowhammer Susceptibility
    Factors," Microsoft Research, Tech. Rep., 2021.

    - <span id="page-20-7"></span>[400] L. Borucki, G. Schindlbeck, and C. Slayman,
    "Comparison of Accelerated DRAM Soft Error Rates Measured at Component and System
    Level," in IEEE IRPS, 2008.

    - <span id="page-20-8"></span>[401] A. G. Yaglık ˘ c¸ı, H. Luo, A. Olgun, G. F.
    de Oliveira Junior, J. Park, M. Patel, H. Hassan, L. Orosa, J. Kim, and O. Mutlu,
    "Understanding the RowHammer Vulnerability Under Reduced Wordline Voltage: An
    Experimental Study Using Real Devices," in DSN, 2022.

    - <span id="page-20-9"></span>[402] Q. Salman, K. Yoongu, B. Nicolas, S. Eric,
    and N. Mattias, "Half-Double: Next-Row-Over Assisted Rowhammer," 2021.

    - <span id="page-20-10"></span>[403] J. Kim and M. C. Papaefthymiou, "Block-Based
    Multi-Period Refresh For Energy Efficient Dynamic Memory," in IEEE International
    ASIC/SOC Conference, 2001.

    - <span id="page-20-11"></span>[404] S. Levy, K. B. Ferreira, N. DeBardeleben,
    T. Siddiqua, V. Sridharan, and E. Baseman, "Lessons Learned from Memory Errors
    Observed Over the Lifetime of Cielo," in SC, 2018.

    - <span id="page-20-12"></span>[405] F. Wang and V. D. Agrawal, "Soft Error Rates
    with Inertial and Logical Masking," in VLSI, 2009.

    - <span id="page-20-13"></span>[406] Micron Technology, "DRAM," [https://www.micron.com/products/dr](https://www.micron.com/products/dram/)
    [am/.](https://www.micron.com/products/dram/)

    - [407] ISSI, "DDR4 SDRAM," [https://www.issi.com/US/product-dram-ddr4.](https://www.issi.com/US/product-dram-ddr4.shtml)
    [shtml.](https://www.issi.com/US/product-dram-ddr4.shtml)

    - <span id="page-20-14"></span>[408] ISSI, "NT5AD256M16E4-JR," [https://www.nanya.com/en/Product/4](https://www.nanya.com/en/Product/4596/NT5AD256M16E4-JR)
    [596/NT5AD256M16E4-JR.](https://www.nanya.com/en/Product/4596/NT5AD256M16E4-JR)

    - <span id="page-20-15"></span>[409] D. Patterson, T. Anderson, N. Cardwell, R.
    Fromm, K. Keeton, C. Kozyrakis, R. Thomas, and K. Yelick, "A Case for Intelligent
    RAM," IEEE Micro, 1997.

    - <span id="page-20-16"></span>[410] O. Mutlu, S. Ghose, J. Gomez-Luna, and R.
    Ausavarungnirun, "A Modern Primer on Processing in Memory," in arXiv, 2020.

    - [411] F. Devaux, "The True Processing in Memory Accelerator," in HCS, 2019.

    - [412] Y.-C. Kwon, S. H. Lee, J. Lee, S.-H. Kwon, J. M. Ryu, J.-P. Son, O. Seongil,
    H.-S. Yu, H. Lee, S. Y. Kim, Y. Cho, J. G. Kim, J. Choi, H.-S. Shin, J. Kim, B.
    Phuah, H. Kim, M. J. Song, A. Choi, D. Kim, S. Kim, E.-B. Kim, D. Wang, S. Kang,
    Y. Ro, S. Seo, J. Song, J. Youn, K. Sohn, and N. S. Kim, "25.4 A 20nm 6GB Function-In-Memory
    DRAM, Based on HBM2 with a 1.2 TFLOPS Programmable Computing Unit Using Bank-Level
    Parallelism, for Machine Learning Applications," in ISSCC, 2021.

    - [413] M. He, C. Song, I. Kim, C. Jeong, S. Kim, I. Park, M. Thottethodi, and
    T. Vijaykumar, "Newton: A DRAM-Maker''s Accelerator-In-Memory (AiM) Architecture
    for Machine Learning," in MICRO, 2020.

    - [414] D. Niu, S. Li, Y. Wang, W. Han, Z. Zhang, Y. Guan, T. Guan, F. Sun, F.
    Xue, L. Duan, Y. Fang, H. Zheng, X. Jiang, S. Wang, F. Zuo, Y. Wang, B. Yu, Q.
    Ren, and Y. Xie, "184QPS/W 64Mb/mm 2 3D Logic-to-DRAM Hybrid Bonding with Process-Near-Memory
    Engine for Recommendation System," in ISSCC, 2022.

    - [415] J. Ahn, S. Hong, S. Yoo, O. Mutlu, and K. Choi, "A Scalable Processing-In-Memory
    Accelerator for Parallel Graph Processing," ISCA, 2016.

    - [416] S. Lee, K. Kim, S. Oh, J. Park, G. Hong, D. Ka, K. Hwang, J. Park, K.
    Kang, J. Kim et al., "A 1ynm 1.25 V 8Gb, 16Gb/s/pin GDDR6-based Accelerator-in-Memory
    supporting 1TFLOPS MAC Operation and Various Activation Functions for Deep-Learning
    Applications," in ISSCC, 2022.

    - <span id="page-20-17"></span>[417] O. Mutlu, S. Ghose, J. Gomez-Luna, and R.
    Ausavarungnirun, "Pro- ´ cessing Data Where It Makes Sense: Enabling In-Memory
    Computation," Microprocessors and Microsystems, 2019.

    - <span id="page-20-18"></span>[418] M. Patel, T. Shahroodi, A. Manglik, A. G.
    Yaglikci, A. Olgun, H. Luo, and O. Mutlu, "A Case for Transparent Reliability
    in DRAM Systems," arXiv:2204.10378, 2022.


    ## <span id="page-21-0"></span>A. DRAM Trends Survey


    We survey manufacturer-recommended DRAM operating parameters as specified in commodity
    DRAM chip datasheets in order to understand how the parameters have evolved over
    time. We extract values from 58 independent DRAM chip datasheets from across 19
    different DRAM manufacturers with datasheet publishing dates between 1970 and
    2021. Appendix [B](#page-24-0) lists each datasheet and the details of the DRAM
    chip that it corresponds to. We openly release our full dataset on GitHub [\[136\]](#page-15-15),
    which includes all of the raw data used in this paper, including each timing and
    current parameter value, and additional fields (e.g., clock frequencies, package
    pin counts, remaining IDD values) that are not presented here.


    ## <span id="page-21-3"></span>A.1. DRAM Access Timing Trends


    We survey how the following four DRAM timing parameters that are directly related
    to DRAM chip performance evolve.


    - tRCD: time between issuing a row command (i.e., row activation) and a column
    command (e.g., read) to the row.

    - CAS Latency (or tAA): time between issuing an access to a given column address
    and the data being ready to access.

    - tRAS: time between issuing a row command (i.e., row activation) and a precharge
    command.

    - tRC : time between accessing two different rows.


    <span id="page-21-1"></span>![](_page_21_Figure_8.jpeg)


    Figure 4: Evolution of four key DRAM timing parameters (shown in log scale) across
    years (top) and chip capacities (bottom) separated by DRAM standard.


    Figure [4](#page-21-1) shows how key DRAM timing parameters have evolved across
    DRAM chips of different years (top) and capacities (bottom). Timing values are
    shown in log scale to better distinguish small values in newer DRAM chips. Each
    type of marker illustrates DRAM chips of different DRAM standards.


    We make three qualitative observations. First, although all four DRAM timing values
    roughly decrease over time, improvements are relatively stagnant for the last
    two decades (note the logarithmic Y-axis). The bulk of the improvement in timing
    parameter values occurred during the period of asynchronous DRAM, and following
    the introduction of SDRAM and DDRn DRAM chips, little to no improvements have
    been made despite, or possibly as a result of, continual increases in overall
    chip storage density. Second, CAS latency and tRCD converged to roughly the same
    values following the introduction of synchronous DRAM. We hypothesize that this
    is because similar factors affect the latency of these operations, including a
    long command and data communication latency between the external DRAM bus and
    the internal storage array [\[3\]](#page-13-2). Third, the DDR5 data points appear
    to worsen relative to previous DDRn points. However, we believe this might be
    because DDR5 chips are new at the time of writing this article and have not yet
    been fully optimized (e.g., through die revisions and other process improvements).


    To quantify the changes in access timing values, we aggregate the data points
    from Figure [4](#page-21-1) by time, DRAM standard, and chip capacity. Figure
    [5,](#page-21-2) shows the minimum, median, and maximum values (in log scale)
    for each 5-year period (top) and DRAM standard (bottom). The data shows that the
    median tRCD/CAS Latency/tRAS/tRC reduced by 2.66/3.11/2.89/2.89% per year on average
    between 1970 and


    <span id="page-21-2"></span>![](_page_21_Figure_13.jpeg)


    Figure 5: Evolution of the minimum, median, and maximum values of key DRAM timing
    parameters (shown in log scale) for each 5-year period (top) and DRAM standard
    (bottom).


    <span id="page-22-1"></span>![](_page_22_Figure_0.jpeg)


    Figure 6: Evolution of the minimum, median, and maximum values of key DRAM timing
    parameters (shown in log scale) grouped by DRAM chip storage capacity.


    2000 but only 0.81/0.97/1.33/1.53% between 2000 and 2015[18](#page-22-0) for an
    overall decrease of 1.83/2.10/1.99/2.00% between 1970 and 2015.


    Figure [6](#page-22-1) shows the minimum, median, and maximum of the timing parameter
    values (in log scale) grouped by DRAM chip storage capacity.[19](#page-22-2) We
    find that the timings follow similar trends as in Figure [5](#page-21-2) because
    higher-capacity DRAM chips are typically introduced more recently and follow newer
    DRAM standards.


    #### A.2. Current Consumption Trends


    We review the evolution of the following key DRAM current consumption measurements,
    which are standardized by JEDEC and are provided by manufacturers in their datasheets.


    - IDD0: current consumption with continuous row activation and precharge commands
    issued to only one bank.

    - IDD4R: current consumption when issuing back-to-back read operations to all
    banks.

    - IDD5B: current consumption when issuing continuous burst refresh operations.


    Figure [7](#page-22-3) shows how key DRAM current consumption values (in log scale)
    have evolved across DRAM chips of different years (top) and capacities (bottom).
    We use different markers to show data points from chips of different DRAM standards.
    We qualitatively observe that current consumption increased exponentially up until
    approximately the year 2000, which is about the time at which improvements in
    access timings slowed down (as seen in Figure [4\)](#page-21-1). After this point,
    different current consumption measurements diverged as IDD0 values decreased while
    IDD4R and IDD5B stabilized or increased. We explain this behavior by a change
    in the way DRAM chips were refreshed as DRAM capacities continued to increase.
    Earlier DRAM chips refreshed rows using individual row accesses (e.g., RAS-only
    refresh), which result in comparable behavior for access and refresh operations.
    In contrast, newer DRAM chips aggressively refresh multiple rows per refresh operation
    (e.g., burst refresh), which differentiates refresh operations from normal row
    accesses [\[210,](#page-16-16) [212,](#page-16-31) [275\]](#page-18-4).


    We aggregate the current consumption data points from Figure [7](#page-22-3) by
    time and DRAM standard. Figure [8](#page-23-1) shows the minimum, median, and
    maximum values (in log scale) across each 5-year period (top) and DRAM standard
    (bottom). The data shows that the median IDD0/IDD4R/IDD5B increased by 12.22/20.91/26.97%
    per year on average between 1970 and 2000 but decreased by 4.62/1.00/0.13% between
    2000 and 2015[20](#page-22-4)


    #### A.3. Relationship Between Timings and Currents


    Finally, we examine the high-level relationship between the timing parameter and
    current consumption values. We find that the two are generally inversely related,
    which follows from the general principle that faster DRAM chips (i.e., lower


    <span id="page-22-4"></span><sup>20</sup>Similar to Section [A.1,](#page-21-3)
    we omit the 2020 data point because the firstgeneration DDR5 chips exhibit outlying
    data values (e.g., no data reported for IDD5B in the datasheets). for an overall
    increase of 0.96/11.5/17.5% between 1970 and 2015.


    <span id="page-22-3"></span>![](_page_22_Figure_16.jpeg)


    Figure 7: Evolution of key DRAM current consumption values (shown in log scale)
    across years (top) and chip capacities (bottom) separated by DRAM standard.


    <span id="page-22-0"></span><sup>18</sup>We omit the 2020 data point because 2020
    shows a regression in CAS latency due to first-generation DDR5 chips, which we
    believe is not representative because of its immature technology.


    <span id="page-22-2"></span><sup>19</sup>We omit tRCD and tRAS for the 1 Kib chips
    because they do not use a row address strobe (RAS) signal.


    <span id="page-23-1"></span>![](_page_23_Figure_0.jpeg)


    Figure 8: Evolution of the minimum, median, and maximum of key DRAM current consumption
    value (shown in log scale) for each 5-year period (top) and DRAM standard (bottom).


    ![](_page_23_Figure_2.jpeg)


    Figure 9: Relationship between the four timing parameters and IDD4R separated
    by DRAM standard.


    <span id="page-23-2"></span>![](_page_23_Figure_4.jpeg)


    Figure 10: Evolution of tREF I (left y-axis) and tRF C (right y-axis) across DRAM
    chips of increasing storage capacity.


    timing parameters) require more power (i.e., increased current consumption values).
    Figure [9](#page-23-1) illustrates this relationship for the four timing parameters
    studied in Section [A.1](#page-21-3) relative to IDD4R (i.e., the current consumption
    of read operations).


    ### <span id="page-23-0"></span>A.4. DRAM Refresh Timing Trends


    DRAM refresh is governed by two key timing parameters:


    - tREF I (refresh interval): time between consecutive refresh commands sent by
    the memory controller.

    - tRF C : duration of a single refresh command.


    Figure [10](#page-23-2) shows how tREF I (left y-axis) and tRF C (right yaxis)
    evolved across the DRAM chips in our study. We group chips by storage capacity
    because DRAM refresh timings are closely related to capacity: higher-capacity
    chips using the same technology require more time or more refresh operations to
    fully refresh. The error bars show the minimum and maximum values observed across
    all chips for any given chip capacity.


    We make three observations. First, tREF I is shorter for higher-capacity DRAM
    chips (e.g., 62.5 µs for an asynchronous 1 Kib chip versus 3.9 µs for a 16 Gib
    DDR5 chip). This is consistent with the fact that higher-capacity chips require
    more frequent refreshing. Second, tRF C first decreases with chip capacity (e.g.,
    900 ns for an asynchronous 1 Kib chip versus 54 ns for a 32 Mib SDRAM chip) but
    then increases (e.g., to 350 ns for a 16 Gib DDR4 chip). This is because rapid
    improvements in row access times (and therefore refresh timings) initially outpaced
    the increase in storage capacity. However, starting around 512 Mib chip sizes,
    row access times improved much more slowly (as observed in Section [A.1\)](#page-21-3)
    while storage capacity continued to increase. This matches our analysis of the
    refresh penalty in Section [3.1.3.](#page-4-5) Third, the variation in tRF C across
    chips of each capacity (illustrated using the error bars) decreased for higher-capacity
    chips. This is because higher-capacity chips follow more recent DRAM standards
    (i.e., DDRn), which standardize DRAM auto refresh timings. In contrast, older
    DRAM chips were simply refreshed as quickly as their rows could be accessed (e.g.,
    every tRC using RAS-only refresh).


    ## <span id="page-24-0"></span>B. Survey Data Sources


    Table [5](#page-24-1) itemizes the 58 DRAM datasheets used for our survey in Appendix
    [A.](#page-21-0) For each datasheet, we show the DRAM chip manufacturer, model
    number, DRAM standard, year, and capacity. Our full dataset is available online
    [\[136\]](#page-15-15).


    | Year | Manufacturer      | Model Number | Datasheet Source | DRAM Standard |
    Capacity per Chip (Kib) |

    |------|-------------------|--------------|------------------|---------------|-------------------------|

    | 1970 | Intel             | 1103         | [S1]             | Asynchronous  |
    1                       |

    | 1971 | Mostek            | MK4006       | [S2]             | Asynchronous  |
    1                       |

    | 1973 | Mostek            | MK4096       | [S3]             | Asynchronous  |
    4                       |

    | 1976 | Mostek            | MK4027       | [S4]             | PM            |
    4                       |

    | 1976 | Mostek            | MK4116P      | [S5]             | PM            |
    16                      |

    | 1978 | Fairchild         | F4116        | [S6]             | PM            |
    16                      |

    | 1979 | Intel             | 2118         | [S7]             | PM            |
    16                      |

    | 1981 | Mitsubishi        | M5K4164ANP   | [S8]             | PM            |
    64                      |

    | 1982 | Mostek            | MK4564       | [S9]             | PM            |
    64                      |

    | 1984 | NTE               | NTE4164      | [S10]            | PM            |
    64                      |

    | 1984 | Texas Instruments | TMS4416      | [S11]            | PM            |
    64                      |

    | 1985 | Mitsubishi        | M5M4256P     | [S12]            | PM            |
    256                     |

    | 1987 | Samsung           | KM41464A     | [S13]            | PM            |
    256                     |

    | 1987 | Texas Instruments | TMS4464      | [S14]            | PM            |
    256                     |

    | 1989 | Texas Instruments | SMJ4464      | [S15]            | PM            |
    256                     |

    | 1990 | Intel             | 21256        | [S16]            | PM            |
    256                     |

    | 1991 | Mitsubishi        | M5M44100     | [S17]            | FPM           |
    4096                    |

    | 1993 | Mitsubishi        | M5M44256B    | [S18]            | FPM           |
    1024                    |

    | 1993 | Mosel Vitelic     | V404J8       | [S19]            | FPM           |
    8192                    |

    | 1995 | Siemens           | HYB511000BJ  | [S20]            | FPM           |
    1024                    |

    | 1997 | Hyundai           | HY5118164B   | [S21]            | EDO           |
    16384                   |

    | 1997 | Samsung           | KM48S2020CT  | [S22]            | SDRAM         |
    16384                   |

    | 1998 | Micron            | MT48LC4M4A1  | [S23]            | SDRAM         |
    16384                   |

    | 1998 | Mosel Vitelic     | V53C808H     | [S24]            | EDO           |
    8192                    |

    | 1998 | Siemens           | HYB39S16400  | [S25]            | SDRAM         |
    16384                   |

    | 1999 | Samsung           | K4S160822D   | [S26]            | SDRAM         |
    16384                   |

    | 1999 | Samsung           | K4S561632A   | [S27]            | SDRAM         |
    262144                  |

    | 2000 | Amic              | A416316B     | [S28]            | FPM           |
    1024                    |

    | 2000 | ISSI              | IS41LV32256  | [S29]            | EDO           |
    8192                    |

    | 2000 | Samsung           | K4D623237A5  | [S30]            | DDR           |
    65536                   |

    | 2001 | Alliance          | AS4C256K16E0 | [S31]            | EDO           |
    4096                    |

    | 2001 | Alliance          | AS4C4M4FOQ   | [S32]            | FPM           |
    16384                   |

    | 2001 | ISSI              | IS41C4400X   | [S33]            | EDO           |
    16384                   |

    | 2001 | Micron            | MT46V2M32    | [S34]            | DDR           |
    65536                   |

    | 2001 | Micron            | MT46V32M4    | [S35]            | DDR           |
    131072                  |

    | 2001 | Mosel Vitelic     | V58C265164S  | [S36]            | DDR           |
    65536                   |

    | 2001 | TM Tech           | T224160B     | [S37]            | FPM           |
    4096                    |

    | 2003 | Micron            | MT46V64M4    | [S38]            | DDR           |
    262144                  |

    | 2003 | Samsung           | K4S560432E   | [S39]            | SDRAM         |
    262144                  |

    | 2005 | Amic              | A43L0632     | [S40]            | SDRAM         |
    32768                   |

    | 2006 | Elite             | M52S32321A   | [S41]            | SDRAM         |
    32768                   |

    | 2006 | ISSI              | IS42S81600B  | [S42]            | SDRAM         |
    131072                  |

    | 2006 | Samsung           | K4T51043QC   | [S43]            | DDR2          |
    524288                  |

    | 2007 | Micron            | MT47H256M4   | [S44]            | DDR2          |
    1048576                 |

    | 2010 | Samsung           | K4B4G0446A   | [S45]            | DDR3          |
    4194304                 |

    | 2011 | Hynix             | H5TQ4G43MFR  | [S46]            | DDR3          |
    4194304                 |

    | 2011 | Nanya             | NT5CB512M    | [S47]            | DDR3          |
    2097152                 |

    | 2013 | Samsung           | K4B4G0446A   | [S48]            | DDR3          |
    4194304                 |

    | 2015 | Micron            | MT40A2G      | [S49]            | DDR4          |
    8388608                 |

    | 2016 | Hynix             | H5AN4G4NAFR  | [S50]            | DDR4          |
    4194304                 |

    | 2016 | Samsung           | K4A8G165WC   | [S51]            | DDR4          |
    8388608                 |

    | 2017 | Hynix             | H5AN8G4NAFR  | [S52]            | DDR4          |
    8388608                 |

    | 2018 | Micron            | MT40A        | [S53]            | DDR4          |
    16777216                |

    | 2019 | Hynix             | H5AN8G4NCJR  | [S54]            | DDR4          |
    8388608                 |

    | 2019 | Samsung           | K4AAG045WA   | [S55]            | DDR4          |
    16777216                |

    | 2020 | Samsung           | K4AAG085WA   | [S56]            | DDR4          |
    16777216                |

    | 2021 | Hynix             | HMCG66MEB    | [S57]            | DDR5          |
    16777216                |

    | 2021 | Micron            | MT60B1G16    | [S58]            | DDR5          |
    16777216                |


    <span id="page-24-1"></span>Table 5: List of DRAM chip datasheets used in our
    DRAM trends survey.


    ## Survey Sources


    - [S1] Intel, "1103," [http://www.decadecounter.com/vta/pdf/Intel%20Memo](http://www.decadecounter.com/vta/pdf/Intel%20Memory%20Design%20Handbook%20[1973-08].pdf)
    [ry%20Design%20Handbook%20\[1973-08\].pdf,](http://www.decadecounter.com/vta/pdf/Intel%20Memory%20Design%20Handbook%20[1973-08].pdf)
    1970.

    - [S2] Mostek, "MK4006," [https://usermanual.wiki/Pdf/1974MostekIntegrat](https://usermanual.wiki/Pdf/1974MostekIntegratedCircuitGuide.1468557856/view)
    [edCircuitGuide.1468557856/view,](https://usermanual.wiki/Pdf/1974MostekIntegratedCircuitGuide.1468557856/view)
    1971.

    - [S3] Mostek, "MK4096," [https://console5.com/techwiki/images/0/04/MK](https://console5.com/techwiki/images/0/04/MK4096.pdf)
    [4096.pdf,](https://console5.com/techwiki/images/0/04/MK4096.pdf) 1973.

    - [S4] Mostek, "MK4027," [https://console5.com/techwiki/images/d/df/MK](https://console5.com/techwiki/images/d/df/MK4027.pdf)
    [4027.pdf,](https://console5.com/techwiki/images/d/df/MK4027.pdf) 1976.

    - [S5] Mostek, "MK4116P," [https://console5.com/techwiki/images/8/85/MK](https://console5.com/techwiki/images/8/85/MK4116.pdf)
    [4116.pdf,](https://console5.com/techwiki/images/8/85/MK4116.pdf) 1976.

    - [S6] Fairchild, "F4116," [http://minuszerodegrees.net/memory/4116/datas](http://minuszerodegrees.net/memory/4116/datasheet_F4116.pdf)
    heet [F4116.pdf,](http://minuszerodegrees.net/memory/4116/datasheet_F4116.pdf)
    1978.

    - [S7] Intel, "2118," [https://drive.google.com/file/d/0B9rh9tVI0J5mNDkwZ](https://drive.google.com/file/d/0B9rh9tVI0J5mNDkwZGEwM2QtMzYzNC00YjQ4LTg4NjYtOGY2ZGRkMDMxYjFm/view?resourcekey=0-vyWj--_z6lp7BjZ-6epTng)
    [GEwM2QtMzYzNC00YjQ4LTg4NjYtOGY2ZGRkMDMxYjFm/view](https://drive.google.com/file/d/0B9rh9tVI0J5mNDkwZGEwM2QtMzYzNC00YjQ4LTg4NjYtOGY2ZGRkMDMxYjFm/view?resourcekey=0-vyWj--_z6lp7BjZ-6epTng)
    [?resourcekey=0-vyWj--](https://drive.google.com/file/d/0B9rh9tVI0J5mNDkwZGEwM2QtMzYzNC00YjQ4LTg4NjYtOGY2ZGRkMDMxYjFm/view?resourcekey=0-vyWj--_z6lp7BjZ-6epTng)
    z6lp7BjZ-6epTng, 1979.

    - [S8] Mitsubishi, "M5K4164ANP," [https://datasheetspdf.com/pdf-file/1110](https://datasheetspdf.com/pdf-file/1110696/Mitsubishi/M5K4164ANP-15/1)
    [696/Mitsubishi/M5K4164ANP-15/1,](https://datasheetspdf.com/pdf-file/1110696/Mitsubishi/M5K4164ANP-15/1)
    1981.

    - [S9] Mostek, "MK4564," [http://www.minuszerodegrees.net/memory/4164](http://www.minuszerodegrees.net/memory/4164/datasheet_MK4564-15_and_MK4564-20.pdf)
    /datasheet MK4564-15 and [MK4564-20.pdf,](http://www.minuszerodegrees.net/memory/4164/datasheet_MK4564-15_and_MK4564-20.pdf)
    1982.

    - [S10] NTE, "NTE4164," [http://www.farnell.com/datasheets/1905614.pdf,](http://www.farnell.com/datasheets/1905614.pdf)
    1984.

    - [S11] Texas Instruments, "TMS4416," [http://pdf.datasheetcatalog.com/dat](http://pdf.datasheetcatalog.com/datasheets2/81/817426_1.pdf)
    [asheets2/81/817426](http://pdf.datasheetcatalog.com/datasheets2/81/817426_1.pdf)
    1.pdf, 1984.

    - [S12] Mitsubishi, "M5M4256P," [http://bitsavers.trailing-edge.com/comp](http://bitsavers.trailing-edge.com/components/mitsubishi/_dataBooks/1985_Mitsubishi_IC_Memories.pdf)
    [onents/mitsubishi/](http://bitsavers.trailing-edge.com/components/mitsubishi/_dataBooks/1985_Mitsubishi_IC_Memories.pdf)
    dataBooks/1985 Mitsubishi IC Memories.pdf, 1985.

    - [S13] Samsung, "KM41464A," [https://console5.com/techwiki/images/2/24/](https://console5.com/techwiki/images/2/24/KM41464A.pdf)
    [KM41464A.pdf,](https://console5.com/techwiki/images/2/24/KM41464A.pdf) 1987.

    - [S14] Texas Instruments, "TMS4464," [https://www.silicon-ark.co.uk/datas](https://www.silicon-ark.co.uk/datasheets/tms4464-datasheet-texas-instruments.pdf)
    [heets/tms4464-datasheet-texas-instruments.pdf,](https://www.silicon-ark.co.uk/datasheets/tms4464-datasheet-texas-instruments.pdf)
    1987.

    - [S15] Texas Instruments, "SMJ4464," [http://65xx.unet.bz/ds/TMS4464.pdf,](http://65xx.unet.bz/ds/TMS4464.pdf)
    1989.

    - [S16] Intel, "21256," [https://drive.google.com/file/d/0B9rh9tVI0J5mMjU2M](https://drive.google.com/file/d/0B9rh9tVI0J5mMjU2MDJlNzItNWVkYy00NWM0LThmZjEtYTkyYjE5MTQxOGI2/view?resourcekey=0-Q0K9JcVvNlgRngkBon8vAw)
    [DJlNzItNWVkYy00NWM0LThmZjEtYTkyYjE5MTQxOGI2/view?re](https://drive.google.com/file/d/0B9rh9tVI0J5mMjU2MDJlNzItNWVkYy00NWM0LThmZjEtYTkyYjE5MTQxOGI2/view?resourcekey=0-Q0K9JcVvNlgRngkBon8vAw)
    [sourcekey=0-Q0K9JcVvNlgRngkBon8vAw,](https://drive.google.com/file/d/0B9rh9tVI0J5mMjU2MDJlNzItNWVkYy00NWM0LThmZjEtYTkyYjE5MTQxOGI2/view?resourcekey=0-Q0K9JcVvNlgRngkBon8vAw)
    1990.

    - [S17] Mitsubishi, "M5M44100," [https://www.datasheetarchive.com/pdf/d](https://www.datasheetarchive.com/pdf/download.php?id=74e4e0a53cd85e765cc396504a798082be9621&type=O&term=M5M44100)
    [ownload.php?id=74e4e0a53cd85e765cc396504a798082be9621&type](https://www.datasheetarchive.com/pdf/download.php?id=74e4e0a53cd85e765cc396504a798082be9621&type=O&term=M5M44100)
    [=O&term=M5M44100,](https://www.datasheetarchive.com/pdf/download.php?id=74e4e0a53cd85e765cc396504a798082be9621&type=O&term=M5M44100)
    1991.

    - [S18] Mitsubishi, "M5M44256B," [https://datasheetspdf.com/pdf-file/1111](https://datasheetspdf.com/pdf-file/1111257/Mitsubishi/M5M44256BP-10/1)
    [257/Mitsubishi/M5M44256BP-10/1,](https://datasheetspdf.com/pdf-file/1111257/Mitsubishi/M5M44256BP-10/1)
    1993.

    - [S19] Mosel Vitelic, "V404J8," [https://www.datasheetarchive.com/pdf/d](https://www.datasheetarchive.com/pdf/download.php?id=d5e7f23416e86a5950d91ea69b37003889d50e&type=M&term=V404J8SU70)
    [ownload.php?id=d5e7f23416e86a5950d91ea69b37003889d50e&type](https://www.datasheetarchive.com/pdf/download.php?id=d5e7f23416e86a5950d91ea69b37003889d50e&type=M&term=V404J8SU70)
    [=M&term=V404J8SU70,](https://www.datasheetarchive.com/pdf/download.php?id=d5e7f23416e86a5950d91ea69b37003889d50e&type=M&term=V404J8SU70)
    1993.

    - [S20] Siemens, "HYB511000BJ," [https://datasheetspdf.com/pdf-file/381513](https://datasheetspdf.com/pdf-file/381513/Siemens/HYB511000BJ-/1)
    [/Siemens/HYB511000BJ-/1,](https://datasheetspdf.com/pdf-file/381513/Siemens/HYB511000BJ-/1)
    1995.

    - [S21] Hyundai, "HY5118164B," [https://www.datasheetarchive.com/pdf/d](https://www.datasheetarchive.com/pdf/download.php?id=7413fece3e0e5dee9b73ec0bb7f6c53afd4c99&type=P&term=HY5118164B)
    [ownload.php?id=7413fece3e0e5dee9b73ec0bb7f6c53afd4c99&type](https://www.datasheetarchive.com/pdf/download.php?id=7413fece3e0e5dee9b73ec0bb7f6c53afd4c99&type=P&term=HY5118164B)
    [=P&term=HY5118164B,](https://www.datasheetarchive.com/pdf/download.php?id=7413fece3e0e5dee9b73ec0bb7f6c53afd4c99&type=P&term=HY5118164B)
    1997.

    - [S22] Samsung, "KM48S2020CT," [http://www.maxim4u.com/view](http://www.maxim4u.com/view_online.php?id=1777838&file=0390\km48s2020ct-fl_3917068.pdf)
    online. [php?id=1777838&file=0390](http://www.maxim4u.com/view_online.php?id=1777838&file=0390\km48s2020ct-fl_3917068.pdf)\km48s2020ct-fl
    3917068.pdf, 1997.

    - [S23] Micron, "MT48LC4M4A1," [https://www.digchip.com/datasheets/d](https://www.digchip.com/datasheets/download_datasheet.php?id=688351&part-number=MT48LC2M8A1)
    ownload [datasheet.php?id=688351&part-number=MT48LC2M8A1,](https://www.digchip.com/datasheets/download_datasheet.php?id=688351&part-number=MT48LC2M8A1)
    1998.

    - [S24] Mosel Vitelic, "V53C808H," [https://www.digchip.com/datasheets](https://www.digchip.com/datasheets/download_datasheet.php?id=1031590&part-number=V53C808H)
    /download [datasheet.php?id=1031590&part-number=V53C808H,](https://www.digchip.com/datasheets/download_datasheet.php?id=1031590&part-number=V53C808H)
    1998.

    - [S25] Siemens, "HYB39S16400," [https://www.digchip.com/datasheets/dow](https://www.digchip.com/datasheets/download_datasheet.php?id=390213&part-number=HYB39S16160AT-10)
    nload [datasheet.php?id=390213&part-number=HYB39S16160AT-1](https://www.digchip.com/datasheets/download_datasheet.php?id=390213&part-number=HYB39S16160AT-10)
    [0,](https://www.digchip.com/datasheets/download_datasheet.php?id=390213&part-number=HYB39S16160AT-10)
    1998.

    - [S26] Samsung, "K4S160822D," [http://pdf.datasheetcatalog.com/datasheet](http://pdf.datasheetcatalog.com/datasheet/SamsungElectronic/mXtvtzs.pdf)
    [/SamsungElectronic/mXtvtzs.pdf,](http://pdf.datasheetcatalog.com/datasheet/SamsungElectronic/mXtvtzs.pdf)
    1999.

    - [S27] Samsung, "K4S561632A," [https://www.datasheetarchive.com/pdf/d](https://www.datasheetarchive.com/pdf/download.php?id=fd48625bbd5e92da34308233eb404f7635e593&type=M&term=K4S561632A)
    [ownload.php?id=fd48625bbd5e92da34308233eb404f7635e593&type](https://www.datasheetarchive.com/pdf/download.php?id=fd48625bbd5e92da34308233eb404f7635e593&type=M&term=K4S561632A)
    [=M&term=K4S561632A,](https://www.datasheetarchive.com/pdf/download.php?id=fd48625bbd5e92da34308233eb404f7635e593&type=M&term=K4S561632A)
    1999.

    - [S28] Amic, "A416316B," [https://pdf1.alldatasheet.com/datasheet-pdf/vie](https://pdf1.alldatasheet.com/datasheet-pdf/view/55599/AMICC/A416316BS-35.html)
    [w/55599/AMICC/A416316BS-35.html,](https://pdf1.alldatasheet.com/datasheet-pdf/view/55599/AMICC/A416316BS-35.html)
    2000.

    - [S29] ISSI, "IS41LV32256," [https://www.digchip.com/datasheets/downloa](https://www.digchip.com/datasheets/download_datasheet.php?id=442395&part-number=IS41LV32256)
    d [datasheet.php?id=442395&part-number=IS41LV32256,](https://www.digchip.com/datasheets/download_datasheet.php?id=442395&part-number=IS41LV32256)
    2000.

    - [S30] Samsung, "K4D623237A5," [https://www.datasheetarchive.com/pdf/d](https://www.datasheetarchive.com/pdf/download.php?id=1ddb8613b8da9b267a1e546d9442e36e6a9d62&type=M&term=K4D623237A)
    [ownload.php?id=1ddb8613b8da9b267a1e546d9442e36e6a9d62&ty](https://www.datasheetarchive.com/pdf/download.php?id=1ddb8613b8da9b267a1e546d9442e36e6a9d62&type=M&term=K4D623237A)
    [pe=M&term=K4D623237A,](https://www.datasheetarchive.com/pdf/download.php?id=1ddb8613b8da9b267a1e546d9442e36e6a9d62&type=M&term=K4D623237A)
    2000.

    - [S31] Alliance, "AS4C256K16E0," [http://www.dexsilicium.com/Alliance](http://www.dexsilicium.com/Alliance_AS4C256K16E0.pdf)
    A [S4C256K16E0.pdf,](http://www.dexsilicium.com/Alliance_AS4C256K16E0.pdf) 2001.

    - [S32] Alliance, "AS4C4M4FOQ," [https://www.datasheetarchive.com/pdf/d](https://www.datasheetarchive.com/pdf/download.php?id=c12832d7fa384ccc466fa8f0d33d169415452d&type=P&term=409--1%252Ftds%252B0541)
    [ownload.php?id=c12832d7fa384ccc466fa8f0d33d169415452d&type](https://www.datasheetarchive.com/pdf/download.php?id=c12832d7fa384ccc466fa8f0d33d169415452d&type=P&term=409--1%252Ftds%252B0541)
    [=P&term=409--1%252Ftds%252B0541,](https://www.datasheetarchive.com/pdf/download.php?id=c12832d7fa384ccc466fa8f0d33d169415452d&type=P&term=409--1%252Ftds%252B0541)
    2001.

    - [S33] ISSI, "IS41C4400X," [https://datasheetspdf.com/pdf-file/1237264/Inte](https://datasheetspdf.com/pdf-file/1237264/IntegratedSiliconSolution/IS41LV44002/1)
    [gratedSiliconSolution/IS41LV44002/1,](https://datasheetspdf.com/pdf-file/1237264/IntegratedSiliconSolution/IS41LV44002/1)
    2001.

    - [S34] Micron, "MT46V2M32," [https://datasheetspdf.com/pdf-file/534262](https://datasheetspdf.com/pdf-file/534262/MicronTechnology/MT46V2M32/1)
    [/MicronTechnology/MT46V2M32/1,](https://datasheetspdf.com/pdf-file/534262/MicronTechnology/MT46V2M32/1)
    2001.

    - [S35] Micron, "MT46V32M4," [https://www.compel.ru/item-pdf/b6f0ed7c2](https://www.compel.ru/item-pdf/b6f0ed7c2d40f9dc96e3fa571607bc09/ps/micron~mt46v8m16.pdf)
    [d40f9dc96e3fa571607bc09/ps/micron](https://www.compel.ru/item-pdf/b6f0ed7c2d40f9dc96e3fa571607bc09/ps/micron~mt46v8m16.pdf)∼mt46v8m16.pdf,
    2001.

    - [S36] Mosel Vitelic, "V58C265164S," [https://datasheetspdf.com/pdf-file/29](https://datasheetspdf.com/pdf-file/295988/MoselVitelicCorp/V58C265164S/1)
    [5988/MoselVitelicCorp/V58C265164S/1,](https://datasheetspdf.com/pdf-file/295988/MoselVitelicCorp/V58C265164S/1)
    2001.

    - [S37] TM Tech, "T224160B," [https://www.digchip.com/datasheets/downloa](https://www.digchip.com/datasheets/download_datasheet.php?id=945886&part-number=T224160B)
    d [datasheet.php?id=945886&part-number=T224160B,](https://www.digchip.com/datasheets/download_datasheet.php?id=945886&part-number=T224160B)
    2001.

    - [S38] Micron, "MT46V64M4," [https://media-www.micron.com/-/media/cl](https://media-www.micron.com/-/media/client/global/documents/products/data-sheet/dram/ddr1/256mb_ddr.pdf?rev=7d969af24d6d4b74a34e427f350b1c77)
    [ient/global/documents/products/data-sheet/dram/ddr1/256mb](https://media-www.micron.com/-/media/client/global/documents/products/data-sheet/dram/ddr1/256mb_ddr.pdf?rev=7d969af24d6d4b74a34e427f350b1c77)
    ddr [.pdf?rev=7d969af24d6d4b74a34e427f350b1c77,](https://media-www.micron.com/-/media/client/global/documents/products/data-sheet/dram/ddr1/256mb_ddr.pdf?rev=7d969af24d6d4b74a34e427f350b1c77)
    2003.

    - [S39] Samsung, "K4S560432E," [https://ru.datasheetbank.com/datasheet-d](https://ru.datasheetbank.com/datasheet-download/429400/1/Samsung/K4S560432E-UC75)
    [ownload/429400/1/Samsung/K4S560432E-UC75,](https://ru.datasheetbank.com/datasheet-download/429400/1/Samsung/K4S560432E-UC75)
    2003.

    - [S40] Amic, "A43L0632," [https://datasheetspdf.com/pdf-file/672656/AMI](https://datasheetspdf.com/pdf-file/672656/AMICTechnology/A43L0632/1)
    [CTechnology/A43L0632/1,](https://datasheetspdf.com/pdf-file/672656/AMICTechnology/A43L0632/1)
    2005.

    - [S41] Elite, "M52S32321A," [http://www.farnell.com/datasheets/62304.pdf,](http://www.farnell.com/datasheets/62304.pdf)
    2006.

    - [S42] ISSI, "IS42S81600B," [https://datasheetspdf.com/pdf-file/591012/ISSI/](https://datasheetspdf.com/pdf-file/591012/ISSI/IS42S81600B/1)
    [IS42S81600B/1,](https://datasheetspdf.com/pdf-file/591012/ISSI/IS42S81600B/1)
    2006.

    - [S43] Samsung, "K4T51043QC," [https://www.digchip.com/datasheets/dow](https://www.digchip.com/datasheets/download_datasheet.php?id=1088989&part-number=K4T51083QC)
    nload [datasheet.php?id=1088989&part-number=K4T51083QC,](https://www.digchip.com/datasheets/download_datasheet.php?id=1088989&part-number=K4T51083QC)
    2006.

    - [S44] Micron, "MT47H256M4," [https://media-www.micron.com/-/media/](https://media-www.micron.com/-/media/client/global/documents/products/data-sheet/dram/ddr2/1gb_ddr2.pdf?rev=854b480189b84d558d466bc18efe270c)
    [client/global/documents/products/data-sheet/dram/ddr2/1gb](https://media-www.micron.com/-/media/client/global/documents/products/data-sheet/dram/ddr2/1gb_ddr2.pdf?rev=854b480189b84d558d466bc18efe270c)
    ddr2. [pdf?rev=854b480189b84d558d466bc18efe270c,](https://media-www.micron.com/-/media/client/global/documents/products/data-sheet/dram/ddr2/1gb_ddr2.pdf?rev=854b480189b84d558d466bc18efe270c)
    2007.

    - [S45] Samsung, "K4B4G0446A," [https://www.samsung.com/semiconducto](https://www.samsung.com/semiconductor/global.semi/file/resource/2017/11/DS_K4B4G0846D-BC_Rev123-0.pdf)
    [r/global.semi/file/resource/2017/11/DS](https://www.samsung.com/semiconductor/global.semi/file/resource/2017/11/DS_K4B4G0846D-BC_Rev123-0.pdf)
    K4B4G0846D-BC Rev123-0 [.pdf,](https://www.samsung.com/semiconductor/global.semi/file/resource/2017/11/DS_K4B4G0846D-BC_Rev123-0.pdf)
    2010.

    - [S46] Hynix, "H5TQ4G43MFR," [https://pdf1.alldatasheet.com/datasheet-p](https://pdf1.alldatasheet.com/datasheet-pdf/view/533445/HYNIX/H5TQ4G63MFR-H9C.html)
    [df/view/533445/HYNIX/H5TQ4G63MFR-H9C.html,](https://pdf1.alldatasheet.com/datasheet-pdf/view/533445/HYNIX/H5TQ4G63MFR-H9C.html)
    2011.

    - [S47] Nanya, "NT5CB512M," [http://www.sunnyqi.com/upLoad/product/m](http://www.sunnyqi.com/upLoad/product/month_1308/NT5CB256M8GN.pdf)
    onth [1308/NT5CB256M8GN.pdf,](http://www.sunnyqi.com/upLoad/product/month_1308/NT5CB256M8GN.pdf)
    2011.

    - [S48] Samsung, "K4B4G0446A," [https://www.samsung.com/semiconducto](https://www.samsung.com/semiconductor/global.semi/file/resource/2017/11/DS_K4B4G0846D-BC_Rev123-0.pdf)
    [r/global.semi/file/resource/2017/11/DS](https://www.samsung.com/semiconductor/global.semi/file/resource/2017/11/DS_K4B4G0846D-BC_Rev123-0.pdf)
    K4B4G0846D-BC Rev123-0 [.pdf,](https://www.samsung.com/semiconductor/global.semi/file/resource/2017/11/DS_K4B4G0846D-BC_Rev123-0.pdf)
    2013.

    - [S49] Micron, "MT40A2G," [https://www.micron.com/-/media/client/globa](https://www.micron.com/-/media/client/global/documents/products/data-sheet/dram/ddr4/8gb_ddr4_sdram.pdf)
    [l/documents/products/data-sheet/dram/ddr4/8gb](https://www.micron.com/-/media/client/global/documents/products/data-sheet/dram/ddr4/8gb_ddr4_sdram.pdf)
    ddr4 sdram.pdf, 2015.

    - [S50] Hynix, "H5AN4G4NAFR," [https://datasheetspdf.com/pdf-file/130916](https://datasheetspdf.com/pdf-file/1309166/HynixSemiconductor/H5AN4G8NAFR-xxC/1)
    [6/HynixSemiconductor/H5AN4G8NAFR-xxC/1,](https://datasheetspdf.com/pdf-file/1309166/HynixSemiconductor/H5AN4G8NAFR-xxC/1)
    2016.

    - [S51] Samsung, "K4A8G165WC," [https://www.samsung.com/semiconduc](https://www.samsung.com/semiconductor/global.semi/file/resource/2017/12/x16%20only_8G_C_DDR4_Samsung_Spec_Rev1.5_Apr.17.pdf)
    [tor/global.semi/file/resource/2017/12/x16%20only](https://www.samsung.com/semiconductor/global.semi/file/resource/2017/12/x16%20only_8G_C_DDR4_Samsung_Spec_Rev1.5_Apr.17.pdf)
    8G C DDR4 S amsung Spec Rev1.5 [Apr.17.pdf,](https://www.samsung.com/semiconductor/global.semi/file/resource/2017/12/x16%20only_8G_C_DDR4_Samsung_Spec_Rev1.5_Apr.17.pdf)
    2016.

    - [S52] Hynix, "H5AN8G4NAFR," [https://www.digchip.com/datasheets/dow](https://www.digchip.com/datasheets/download_datasheet.php?id=217237&part-number=H5AN8G8NAFR&type=pn2)
    nload [datasheet.php?id=217237&part-number=H5AN8G8NAFR&](https://www.digchip.com/datasheets/download_datasheet.php?id=217237&part-number=H5AN8G8NAFR&type=pn2)
    [type=pn2,](https://www.digchip.com/datasheets/download_datasheet.php?id=217237&part-number=H5AN8G8NAFR&type=pn2)
    2017.

    - [S53] Micron, "MT40A," [https://www.micron.com/-/media/client/global/](https://www.micron.com/-/media/client/global/documents/products/data-sheet/dram/ddr4/16gb_ddr4_sdram.pdf)
    [documents/products/data-sheet/dram/ddr4/16gb](https://www.micron.com/-/media/client/global/documents/products/data-sheet/dram/ddr4/16gb_ddr4_sdram.pdf)
    ddr4 sdram.pdf, 2018.

    - [S54] Hynix, "H5AN8G4NCJR," [http://www.hytic.net/upload/files/2019/10](http://www.hytic.net/upload/files/2019/10/SK%20Hynix%20%20%20-H5AN8G4NCJR.pdf)
    [/SK%20Hynix%20%20%20-H5AN8G4NCJR.pdf,](http://www.hytic.net/upload/files/2019/10/SK%20Hynix%20%20%20-H5AN8G4NCJR.pdf)
    2019.

    - [S55] Samsung, "K4AAG045WA," [https://www.memory-distributor.com/p](https://www.memory-distributor.com/pub/media/downloads/datasheets/K4AAG085WA_BIxx.pdf)
    [ub/media/downloads/datasheets/K4AAG085WA](https://www.memory-distributor.com/pub/media/downloads/datasheets/K4AAG085WA_BIxx.pdf)
    BIxx.pdf, 2019.

    - [S56] Samsung, "K4AAG085WA," [https://www.memory-distributor.com/p](https://www.memory-distributor.com/pub/media/downloads/datasheets/K4AAG085WA_BIxx.pdf)
    [ub/media/downloads/datasheets/K4AAG085WA](https://www.memory-distributor.com/pub/media/downloads/datasheets/K4AAG085WA_BIxx.pdf)
    BIxx.pdf, 2020.

    - [S57] Hynix, "HMCG66MEB," [https://gzhls.at/blob/ldb/b/e/5/8/5bc212f7c9](https://gzhls.at/blob/ldb/b/e/5/8/5bc212f7c92604fd3737505ee4c96014733c.pdf)
    [2604fd3737505ee4c96014733c.pdf,](https://gzhls.at/blob/ldb/b/e/5/8/5bc212f7c92604fd3737505ee4c96014733c.pdf)
    2021.

    - [S58] Micron, "MT60B1G16," [https://media-www.micron.com/-/media/clie](https://media-www.micron.com/-/media/client/global/documents/products/data-sheet/dram/ddr5/16gb_ddr5_sdram_diereva.pdf?rev=c95e4a49184145f18e105cc41e0ee643)
    [nt/global/documents/products/data-sheet/dram/ddr5/16gb](https://media-www.micron.com/-/media/client/global/documents/products/data-sheet/dram/ddr5/16gb_ddr5_sdram_diereva.pdf?rev=c95e4a49184145f18e105cc41e0ee643)
    ddr5 s dram [diereva.pdf?rev=c95e4a49184145f18e105cc41e0ee643,](https://media-www.micron.com/-/media/client/global/documents/products/data-sheet/dram/ddr5/16gb_ddr5_sdram_diereva.pdf?rev=c95e4a49184145f18e105cc41e0ee643)
    2021.'
  decisions:
    evaluation_prompt: 'Disqualified: Chunk 2: Disqualified: no evaluation. Reason:
      The provided section discusses DRAM standards and their implications without
      any mention of empirical, experimental, or quantitative evaluation. | Chunk
      3: Disqualified: no evaluation. Reason: The provided section from the paper
      discusses DRAM standards, advantages, and processes but does not include any
      empirical, experimental, or quantitative evaluation. There are no mentions of
      evaluation, experiments, results, benchmarks, metrics, comparisons, datasets,
      performance tables, figures, or graphs. | Chunk 8: Disqualified: no evaluation.
      Reason: The provided section discusses challenges related to accessing information
      about DRAM chips and the implications of this lack of transparency. However,
      it does not include any empirical, experimental, or quantitative evaluation,
      nor does it reference any evaluation methods or results. | Chunk 11: Disqualified:
      no evaluation. Reason: The provided section does not contain any evidence of
      empirical, experimental, or quantitative evaluation. It discusses conceptual
      changes and a call-to-action but lacks any mention of evaluation, experiments,
      results, or similar evidence. | Chunk 12: Disqualified: no evaluation. Reason:
      The provided section is an acknowledgment and does not contain any form of empirical,
      experimental, or quantitative evaluation. | Chunk 13: Disqualified: no evaluation.
      Reason: The provided section is solely a list of references and does not contain
      any form of empirical, experimental, or quantitative evaluation. | Chunk 18:
      Disqualified: no evaluation. Reason: The provided section is a list of survey
      sources and does not contain any empirical, experimental, or quantitative evaluation.
      There is no mention of evaluation, experiments, results, benchmarks, metrics,
      comparisons, datasets, or performance figures.'
    related_work_prompt: 'Disqualified: Chunk 2: Disqualified: no related work. Reason:
      The provided section does not include any citations or discussion of prior research.
      It focuses solely on explaining DRAM standards without engaging with existing
      academic literature or comparing its content to previous work. | Chunk 11: Disqualified:
      no related work. Reason: The provided section does not contain any citations
      or discussion of prior research, nor does it compare its methods to previous
      work. There is no evidence of meaningful engagement with existing academic literature.
      | Chunk 12: Disqualified: no related work. Reason: The provided section is an
      acknowledgment and does not contain any engagement with prior research or academic
      citations. There is no discussion or comparison of the paper''s methods to previous
      work. | Chunk 13: Disqualified: no related work. Reason: The provided section
      is solely a list of references without any context, discussion, or comparison
      to prior research within the text. There is no indication of meaningful engagement
      with these references elsewhere in the paper. | Chunk 14: Disqualified: no related
      work. Reason: The section provided does not include any meaningful engagement
      with prior research, such as citations or comparisons to previous work. It only
      describes the collection and release of data from DRAM datasheets without discussing
      how this relates to or builds upon existing academic literature. | Chunk 16:
      Disqualified: no related work. Reason: The provided section discusses DRAM refresh
      timing trends and observations without any citations or comparisons to prior
      research. There is no engagement with existing literature or discussion of how
      these findings relate to or differ from previous work. | Chunk 17: Disqualified:
      no related work. Reason: The section provided does not include any meaningful
      engagement with prior research. It lists datasheets used in a survey without
      discussing or comparing the findings to existing literature or previous studies.
      | Chunk 18: Disqualified: no related work. Reason: The section provided lists
      various sources, primarily datasheets, without any context, discussion, or comparison
      to prior research. There is no meaningful engagement with prior academic work
      or explanation of how these sources relate to the paper''s contributions.'
    novelty_prompt: 'Disqualified: Chunk 2: Disqualified: no novelty. Reason: The
      section provided is a review of existing DRAM standards and does not present
      any new ideas, methods, applications, or results. It lacks claims of novelty
      or contribution. | Chunk 3: Disqualified: no novelty. Reason: The provided section
      discusses existing DRAM standards, their advantages, and the process of creating
      or modifying these standards. It does not introduce any new methods, applications,
      or insights, nor does it apply known techniques in a novel context. There are
      no claims of contribution or novelty in the text provided. | Chunk 4: Disqualified:
      no novelty. Reason: The section provided discusses challenges in DRAM technology
      scaling, such as the slowdown of generational improvements and worsening refresh
      overheads, but it does not propose any new methods, insights, or applications.
      It primarily reviews existing challenges and trends without making claims of
      novel contributions. | Chunk 6: Disqualified: no novelty. Reason: The provided
      section of the paper discusses existing solutions and techniques for reducing
      DRAM refresh overheads and mitigating the RowHammer vulnerability. It does not
      present any new methods, algorithms, architectures, datasets, or insights, nor
      does it apply known techniques in a novel context or domain. The section primarily
      summarizes prior work without making clear claims of novel contributions. |
      Chunk 8: Disqualified: no novelty. Reason: The section discusses the challenges
      and limitations faced by consumers in accessing DRAM chip information but does
      not propose any new methods, solutions, or insights to address these challenges.
      It highlights existing issues without contributing novel ideas or approaches.
      | Chunk 12: Disqualified: no novelty. Reason: The provided section is an acknowledgment
      and does not contain any claims of novelty or contributions related to new methods,
      applications, or insights. | Chunk 13: Disqualified: no novelty. Reason: The
      provided section is solely a list of references and does not contain any claims
      or evidence of novelty, such as new methods, applications, or insights. | Chunk
      15: Disqualified: no novelty. Reason: The section provided is a survey of existing
      DRAM timing and current consumption trends over time. It does not propose any
      new methods, algorithms, architectures, datasets, or insights, nor does it apply
      known techniques in a novel context or domain. The content is purely observational
      and descriptive of existing data without any claims of novel contributions.
      | Chunk 16: Disqualified: no novelty. Reason: The section provides an analysis
      of DRAM refresh timing trends but does not propose any new methods, algorithms,
      architectures, datasets, or insights. It does not apply known techniques in
      a novel context or domain, nor does it make any clear claims of contribution
      or novelty. | Chunk 17: Disqualified: no novelty. Reason: The section provided
      is a summary of data sources used in a survey, listing DRAM datasheets and their
      specifications. There is no indication of a new method, application, or insight,
      nor any claims of novelty or contribution in this section. | Chunk 18: Disqualified:
      no novelty. Reason: The provided section is a list of survey sources, which
      does not contain any claims of novelty, new methods, algorithms, applications,
      or insights. It is purely a compilation of existing data sources without any
      indication of novel contributions.'
    review_only_prompt: 'Qualified. Reason: All relevant sections passed.'
