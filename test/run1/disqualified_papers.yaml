papers:
- title: "MX: Enhancing RISC-V's Vector ISA for Ultra-Low Overhead,\n  Energy-Efficient\
    \ Matrix Multiplication"
  abstract: 'Dense Matrix Multiplication (MatMul) is arguably one of the most ubiquitous

    compute-intensive kernels, spanning linear algebra, DSP, graphics, and machine

    learning applications. Thus, MatMul optimization is crucial not only in

    high-performance processors but also in embedded low-power platforms. Several

    Instruction Set Architectures (ISAs) have recently included matrix extensions

    to improve MatMul performance and efficiency at the cost of added matrix

    register files and units. In this paper, we propose Matrix eXtension (MX), a

    lightweight approach that builds upon the open-source RISC-V Vector (RVV) ISA

    to boost MatMul energy efficiency. Instead of adding expensive dedicated

    hardware, MX uses the pre-existing vector register file and functional units to

    create a hybrid vector/matrix engine at a negligible area cost (< 3%), which

    comes from a compact near-FPU tile buffer for higher data reuse, and no clock

    frequency overhead. We implement MX on a compact and highly energy-optimized

    RVV processor and evaluate it in both a Dual- and 64-Core cluster in a 12-nm

    technology node. MX boosts the Dual-Core''s energy efficiency by 10% for a

    double-precision 64x64x64 matrix multiplication with the same FPU utilization

    (~97%) and by 25% on the 64-Core cluster for the same benchmark on 32-bit data,

    with a 56% performance gain.'
  url: http://arxiv.org/abs/2401.04012v1
  keywords: RISC-V, Matrix, Vector, Efficiency
  document: '#### I. INTRODUCTION


    The exponential growth of the computational requirements in Machine Learning (ML)
    and Artificial Intelligence (AI) applications is a major challenge for hardware
    architects. The rise of application-specific accelerators [1] and single instruction,
    multiple data (SIMD) programmable systems [2] demonstrates the need for novel
    architectures able to cope with the rising computational demand. Furthermore,
    AI/ML applications also found their way into edge computing, with benefits such
    as higher privacy, user personalization, and lower power consumption. However,
    edge-AI/ML systems have the additional challenge of balancing large computational
    demands against a very tight power envelope and minimal area footprint. The quest
    for energy efficiency and cost (i.e., area) minimization is even more pressing
    today since ML/AI computation at the edge does not only involve inference but
    also training in the so-called AI on Edge [3].


    Matrix Multiplication (MatMul) is a cornerstone in ML and AI, and essential in
    scientific computing, graphics, and Digital Signal Processing (DSP). The importance
    of MatMul is testified by market-leading companies, such as Google, which developed
    the first Tensor-Processing Unit (TPU) in 2015 to accelerate matrix operations
    [4] and updated it in 2018 with an edgeoriented version achieving 4 TOPS within
    a 2 W power envelope [5]. As with other Domain-Specific Accelerators (DSAs) for
    specific neural-network tasks [6], the TPU is an added resource to which a general-purpose
    processor offloads the workload (for example, through a PCIe interface). This
    brings an area and power overhead that is not affordable in constrained systems
    at the edge, especially when they need to compute non-AI tasks as well. Moreover,
    an excessively specialized accelerator risks becoming useless when the ML/AI algorithm
    evolves.


    Most proprietary Instruction Set Architectures (ISAs) offer dedicated matrix extensions,
    such as Arm''s Scalable Matrix Extension (SME), Intel Advanced Matrix Extension
    (AMX), and IBM Matrix-Multiply Assist (MMA). Unluckily, the microarchitectural
    details of the implementations remain company secrets. So far, the RISC-V open-source
    ISA features only a vector extension (RISC-V Vector (RVV)), even though researchers
    developed multiple unofficial AI/matrix extensions. Still, they add tightly coupled
    matrix units [7] or a new matrix register file [8] used only during matrix operations,
    which add area and power consumption.


    RVV recently showed to be a valid solution to efficiently accelerate MatMul while
    keeping a well-known programming model to handle diverse data-parallel workloads,
    also in the embedded domain [9]. Vector processors execute multiple operations
    with one instruction, amortizing its fetch/decode cost. Moreover, they feature
    a Vector Register File (VRF) to buffer the vector elements, decreasing the accesses
    to memory without changing the computational balance for the architecture [10].
    Even if the VRF helps decrease the power associated with the memory accesses,
    it is an additional block at the bottom of the memory hierarchy, one of the key
    drivers for performance and energy efficiency [11]. Its size can be way larger
    than the one of a scalar register file, and it is usually connected to multiple
    functional units in parallel, which leads to energyhungry interconnects. Hence,
    the VRF access-related energy is usually non-negligible [9], [12].


    With this paper, we present Matrix eXtension (MX), a non-intrusive ISA extension
    to RVV that creates a generalpurpose hybrid matrix/vector architecture with minimal
    area impact and superior energy efficiency. To cut the power consumption, we reduce
    the expensive accesses to/from the


    The first two authors contributed equally to this work.


    <sup>©</sup> 2023 IEEE. Personal use of this material is permitted. Permission
    from IEEE must be obtained for all other uses, in any current or future media,
    including reprinting/republishing this material for advertising or promotional
    purposes, creating new collective works, for resale or redistribution to servers
    or lists, or reuse of any copyrighted component of this work in other works.


    VRF by featuring a software-transparent lightweight accumulator close to the processing
    units. MX does not add a matrix unit to the architecture but re-uses the already
    available processing resources to keep the area and energy overhead at its minimum
    and exploit the energy efficiency savings that come from the reduced VRF accesses.


    To validate MX across multiple domains, we add MX to a constrained embedded Dual-Core
    cluster built upon the opensource energy-optimized RVV-based Spatz [9] vector
    processor and to a scaled-up MemPool architecture [13] with 64 Spatz processors
    and implement both systems in a competitive 12-nm technology. We provide a quantitative
    justification of the energy savings and a detailed power, performance, and area
    (PPA) analysis on matrix multiplications on different data precisions, finding
    that our matrix extension can boost not only energy efficiency but also performance.


    With this paper, we present the following contributions:


    - We define MX, a lightweight and non-intrusive ISA extension based on RVV 1.0
    aimed at supporting memory and computational operations directly on matrices.
    MX reduces the power consumption of the architecture with similar or better performance
    by introducing a near-Floating Point Unit (FPU) tile buffer, a per-vector-element
    broadcast system, and minimal modifications to the Vector Load/Store Unit (VLSU).

    - We provide a theoretical justification of the benefits that the ISA has on the
    power consumption when executing a matrix multiplication kernel, effectively reducing
    the expensive VRF accesses.

    - We implement MX on a constrained Dual-Core and a complex 64-core clusters based
    on the energy-efficient RVV vector processor Spatz, and characterize MX''s impact
    on performance and PPA metrics in a 12-nm technology. For less than 3% area overhead,
    we get a maximum of 56% and 25% performance and energy efficiency gains, respectively.


    #### II. ANALYSIS


    In the following, we discuss the tiling of a General Matrix Multiply (GEMM) problem
    through a multi-level memory hierarchy. When C is a zero matrix, GEMM becomes
    a MatMul.


    $$D\_{M \times N} = A\_{M \times K} \cdot B\_{K \times N} + C\_{M \times N} \tag{l}$$


    For convenience, let us consider a memory hierarchy composed of a memory, a VRF,
    and a near-FPU buffer. The memory connects to the VRF, which is connected to a
    buffer that feeds the FPUs, as reported in Figure 1. The following analysis can
    be easily extended to memory hierarchies with a different number of levels.


    #### *A. The tiling problem*


    The lower level of the hierarchy is usually not large enough to keep the input
    and output matrices all at once. Therefore, the matrices are divided into chunks
    (tiles), and the hardware works on one output tile at a time, and the outer product
    algorithm is often used to maximize parallelism. The number of elements


    ![](_page_1_Figure_12.jpeg)


    Figure 1. The tiling problem over a memory hierarchy composed of three levels,
    ending with the processing elements (FPUs).


    transferred between two consecutive levels of the hierarchy impacts both performance
    and power consumption and depends on how the matrices are tiled. Usually, the
    number of transfers is partially encoded in the *arithmetic intensity*, i.e.,
    the total number of operations divided by the total number of Bytes transferred
    between the memory and the core.


    In the following, we provide equations to fine-grain count how many memory accesses
    happen between each pair of consecutive levels of the hierarchy. Each equation
    contains four terms, which correspond to 1) the elements of matrix A, 2) the elements
    of matrix B, 3) the elements of matrix C (or D) from the upper level to the lower
    one (load/fetch), and 4) the elements of the matrix D from the lower level back
    to the upper one (store/write-back).


    In the most generic scenario, without buffering the output tile in the VRF for
    more than updates, the number of elements moved between the memory and the VRF
    is:


    $$\#E\,lm\_{VRF}^{MEM} = \frac{N}{n}MK + \frac{M}{m}NK + \frac{K}{k}MN + \frac{K}{k}MN
    \qquad (2)$$


    Where the A, B, and D (C) matrices stored in memory have sizes , , , and we tile
    the problem between the memory and the VRF with tiles of size , , .


    For each matrix tile, the number of elements exchanged between the VRF and the
    buffer is:


    $$\#Elim\_{BUF}^{VRF} = \frac{n}{n''}mk + \frac{m}{m''}nk + \frac{k}{k''}mn +
    \frac{k}{k''}mn\tag{3}$$


    Where the tiles stored in the VRF have sizes , , , and we sub-tile the problem
    between the VRF and the buffer with sub-tiles of size ′ ′ , ′ ′ , ′ ′ .


    For each matrix sub-tile, the number of elements exchanged between the buffer
    and the FPUs is:


    $$\#E\,l m\_{FPU}^{BUF} = \frac{n''}{t\_B} m'' k'' + \frac{m''}{t\_A} n'' k''
    + k'' m'' n'' + k'' m'' n'' \qquad (4)$$


    Where the sub-tiles stored in the buffer have sizes ′ ′ , ′ ′ , ′ ′ , and we access
    and elements from tiles A and B, respectively.


    ## *B. Total number of transfers*


    To get the total number of transfers between each pair of hierarchy levels, we
    need to take into account how many output tiles and sub-tiles we calculate throughout
    the program.


    Table I NUMBER OF ACCESSES BETWEEN CONSECUTIVE LEVELS OF THE MEMORY HIERARCHY.


    | Ref.           | Metric                                             | A (↓)                                        |
    B (↓)                                       | C, D (↓)                                                             |
    D (↑)                                                                |

    |----------------|----------------------------------------------------|----------------------------------------------|---------------------------------------------|----------------------------------------------------------------------|----------------------------------------------------------------------|

    | 1)<br>2)<br>3) | #ElmMEM<br>VRF<br>#ElmVRF<br>BUF<br>#ElmBUF<br>FPU | N<br>n
    MK<br>N<br>′ MK<br>n<br>N<br>MK<br>tB | M<br>NK<br>m<br>M<br>m′ NK<br>M<br>NK<br>tA
    | K<br>k MN<br>k<br>K<br>k MN<br>′<br>k<br>k′ k<br>K<br>k MN<br>′<br>k | K<br>k
    MN<br>k<br>K<br>k MN<br>′<br>k<br>k′ k<br>K<br>k MN<br>′<br>k |


    (a) ↓ /↑ indicate transfers to a lower/higher level of the memory.


    *a) Memory and VRF:* In the most generic case, we load the C (first iteration)
    and D (from the second iteration on) tiles from memory before consuming the input
    , -sized tiles, and we store the D tile back to memory after updates. Without
    inter-k-tile buffering in the VRF, we load ( ) output tiles with size from the
    memory to the VRF, and we store back the same amount. If we buffer the output
    tiles until they are completely calculated over the whole K dimension, the formula
    simplifies to . Instead, we load a total of input A and B tiles, with sizes ,
    .


    *b) VRF and buffer:* The ′ ′ output sub-tiles of a tile are fetched for ( ′ )
    ′ ′ times from the VRF before consuming the input ′ ′ , ′ ′ -sized sub-tiles in
    the buffer, and writtenback to the VRF for the same number of times if there is
    no inter-k-tile buffering in the buffer. Instead, the input A and B sub-tiles,
    with sizes ′ ′ , ′ ′ , are loaded times. If we keep into account how many times
    each tile is loaded/stored from/to memory, these formulas become ( ) ( ′ ) ′ ′
    for the sub-tiles fetch, the same amount for the sub-tiles writes-back, and ′
    ′ ′ for the A and B sub-tiles fetch.


    We summarize all the transfers across the hierarchy in Table I.


    #### *C. Optimizations*


    *a) Inter-k-buffering:* If the output tile (sub-tile) is buffered in the VRF (buffer)
    until the whole K (k) dimension is traversed and the whole output tile (sub-tile)
    is ready, we can simplify the equations above. If the buffering happens in the
    VRF, = 1 in Table I Ref. 1), while if it happens in the buffer until the whole
    dimension, ′ = 1, in Table I Ref. 2) (if the buffering only happens until the
    whole dimension is traversed, ′ = 1).


    Inter-k-buffering is ultimately limited by the size of the lower memory level,
    which should be able to host the whole output tile (sub-tile) for the whole computation
    on the () dimension. Therefore, keeping the output tile in the buffer for the
    whole dimension requires that = ′ , = ′ . On the other hand, relaxing this constraint,
    e.g. = ′ , = × ′ , allows for fewer overall transfers between the memory and the
    VRF. In this case, the inter-k-buffering can be done only between the memory and
    the VRF.


    *b) C-tile reset:* When C is a zero matrix, it''s possible to avoid loading it
    from memory and initialize the VRF with zeroes or reset the buffer. If we use
    inter-k-buffering and the zero initialization is applied to the VRF and the buffer,
    the third term of the equations related to the load/fetch of matrix C, D, becomes
    zero in Table I Ref. 1) and 2), respectively.


    ![](_page_2_Figure_10.jpeg)


    Figure 2. Spatz''s VLSU, VRF, and VFU with MX architectural schematic.


    #### III. MX IMPLEMENTATION


    #### *A. ISA Extension*


    We implement MX in Spatz [9], an open-source, RVV-based, highly-optimized compact
    vector processor, targeting minimal area and power overhead. Thus, we do not add
    any dedicated matrix units or software-addressable registers, as shown in Figure
    2. MX adds three configure instructions (msettile[m,n,k]), three memory instructions
    (mld.[a,b], mst.c), and two computation instructions (mx[f]macc). The first three
    instructions set up the sub-tile sizes ′ , ′ , ′ on which the matrix instructions
    operate ( ′ ′ = , ′ ′ ≤ , and is the vector length in elements). We enhance the
    VLSU to enable matrix load and store operations, which are composed of multiple
    unitand non-unit-stride memory operations already supported by Spatz. The new
    memory instructions are introduced to load tiles from matrix *A* and *B* and to
    store the computed tile back to memory, while the two computational instructions
    perform a MatMul between the two ′ ′ and ′ ′ sub-tiles, storing the resulting
    ′ ′ sub-tile in the VRF. Close to the FPUs, we introduce a tiny broadcast block
    consisting only of a register and some multiplexers to broadcast single elements
    from the *A* tile across multiple elements of tile *B*. This block increases the
    data reuse of tile *A* at a minimal cost. Finally, we implement a latch-based
    result tile buffer in the vector functional unit (VFU) to reduce energy consumption
    by minimizing VRF accesses by intermediate result accumulation, limiting the buffer
    size to <sup>1</sup> 8 of the VRF size (i.e., = 256) to maintain low energy and
    area overhead. Since Spatz''s VLSU has four parallel memory ports and the buffer
    is constrained in size, ′ , ′ , ′ ∈ {4, 8}.


    #### *B. MX Benefits*


    Table II summarizes the number of elements transferred between consecutive memory
    hierarchies for a baseline vectoronly MatMul and a MX-ready MatMul. The baseline
    employs a traditional scalar-vector algorithm to load *m* scalar elements from
    the input matrix *A* and an *n*-long vector from matrix *B*. The MX-ready configuration
    loads A tiles with size ′ ′ and *B* tiles with size ′ ′ . While the MX algorithm
    does not further sub-tile the tiles on or (*m''* = *m* and *k''* = *k*), it sub-tiles


    | Config                     | Metric                                             |
    A (↓)                                           | B (↓)                                    |
    C, D (↓)                   | D (↑)                       |

    |----------------------------|----------------------------------------------------|-------------------------------------------------|------------------------------------------|----------------------------|-----------------------------|

    | Baseline(a)<br>Baseline(a) | #ElmMEM<br>VRF<br>#ElmVRF<br>FPU                   |
    N<br>n MK<br>N<br>F MK                          | M<br>NK<br>m<br>MNK                      |
    0<br>KMN                   | MN<br>KMN                   |

    | MX<br>MX<br>MX             | #ElmMEM<br>VRF<br>#ElmVRF<br>BUF<br>#ElmBUF<br>FPU
    | N<br>′ MK<br>B×n<br>N<br>′ MK<br>n<br>N<br>F MK | M<br>m′ NK<br>M<br>m′ NK<br>M<br>NK<br>F
    | 0<br>K<br>′ MN<br>k<br>KMN | MN<br>K<br>′ MN<br>k<br>KMN |


    Table II DATA TRANSFERS: MX-READY VS BASELINE.


    (a) Elements from A are loaded/fetched to/from the scalar register file;


    (b) *F* represents the number of FPUs;


    (c) ↓ /↑ indicate transfers to a lower/higher level of the memory.


    along *n* such that *n* = ×*n''*, where ∈ {2, 4}. In the following, we highlight
    the benefits brought by the MX algorithm.


    *1) Matrix A operands:* In the baseline approach, operands from matrix *A* are
    fetched as scalars from the scalar register file and individually forwarded to
    the vector unit. In contrast, the matrix algorithm retrieves multiple elements
    from *A* in a tiled-vector manner, improving the access pattern and enabling the
    data reuse of the *A* tile by means of the broadcast engine.


    *2) Instruction count:* In the baseline algorithm, each vector instruction is
    amortized over operations. With MX, the total number of instructions fetched and
    decoded is lower, as each mxfmacc instruction is amortized over ′ ′ ′ operations,
    ′ ′ = , and ′ > 1. This boosts the SIMD ratio, i.e., the average number of operations
    per instruction.


    *3) Tile window:* The matrix algorithm exploits the *k* dimension to increase
    the size of the tile window when the dimensions *M* and *N* are limited. This
    is especially beneficial as the SIMD ratio is further improved by allowing each
    core to work on a larger output tile window in a multi-core environment when processing
    matrices with a limited dimension.


    *4) Scalar-vector interactions:* With the baseline algorithm, the scalar core
    must remain active to compute operand addresses and forward scalar operands to
    the Vector Processing Unit (VPU). In contrast, MX pushes the whole computation
    to the vector unit, freeing up the scalar core.


    *5) Performance:* The computing performance is significantly impacted by the number
    of data transfers between the memory and the VRF and the related latency. The
    MX-ready VLSU regularizes the memory accesses, which can reduce conflicts in both
    the interconnect and memory banks.


    *6) Energy:* In the VPU, the power consumption of the VRF normally constitutes
    a non-negligible portion of the overall energy usage. MX''s inexpensive broadcast
    engine and tile buffers enhance data reuse for the tiled matrix *A* and reduce
    the VRF access by a ′ factor. Moreover, the reduced instruction count and more
    regular memory access pattern alleviate the pressure on the instruction and data
    memories, further improving the energy efficiency of the overall system.


    #### IV. EXPERIMENT SETUP AND RESULTS


    # *A. Computing Clusters and Methodology*


    We integrate the baseline and the MX-ready versions of the Spatz VPU into two
    floating-point-capable computing clusters: a 64-bit constrained Dual-Core cluster
    for in-depth analysis of various tile and sub-tile configurations, and a 32-bit
    large-scale 64-Core cluster for performance evaluation in a complex system.


    *1) Dual-Core Cluster:* The Dual-Core cluster is a 64 bit shared-L1-memory cluster,
    implemented with 128 KiB of Tightly Coupled Data Memory (TCDM) across 16 Static
    Random-Access Memory (SRAM) banks. This cluster features 2 Snitch cores, each
    controlling a Spatz instance equipped with 4 double-precision FPUs and 2 KiB VRF
    each, supporting a vector length of 512 bits. The peak achievable performance
    is 16 DP−FLOP/cycle.


    *2) 64-Core MemPool Cluster:* MemPool, a large-scale 32 bit shared-L1-memory cluster,
    scales up to 256 RISC-V cores and includes 1 MiB of L1 TCDM [13]. The cluster
    is hierarchically organized into 4 groups, each containing 16 tiles. A fully connected
    logarithmic crossbar is employed between the cores and memories, achieving non-uniform
    memory access (NUMA) with a maximum latency of 5 cycles. We equip each Spatz instance
    with 4 32-bit FPUs and 2 KiB of VRFs each, supporting a vector length of 512 bits,
    and pair each instance with a scalar Snitch core to form a Core Complex (CC).
    This cluster configuration, labeled MemPool64Spatz4, consists of 64 CCs, one for
    each tile, and achieves a peak performance of 512 SP−FLOP/cycle, as detailed further
    in [9].


    We implement our designs in GlobalFoundries'' 12 nm LP-PLUS FinFET technology
    through Synopsys Fusion Compiler 2022.03 for synthesis and Place-and-Route (PnR).
    We analyze the PPA metrics of the MX-ready clusters at the post-PnR implementation
    stage and compare them to their respective non-MX baseline architectures. We calculate
    power consumption using Synopsys'' PrimeTime 2022.03 under typical operating conditions
    (TT/0.80 V/25 °C), with switching activities obtained from QuestaSim 2021.3 post-layout
    gate-level simulations and back-annotated parasitic information. In the used MatMul
    kernels, all the input and output matrices are kept in the L1 memory and each
    core of the cluster calculates one portion of the output matrix. The kernel executes
    in parallel across the entire cluster, partitioning the matrix equally among multiple
    cores. At the end of each parallel task, the cores are synchronized to ensure
    consistent write-back of the results.


    #### *B. Implementation Area and Frequency*


    The logic area breakdown of the clusters is presented in Table III. For the MX-ready
    Dual-Core cluster, the main area increase originates from the VFU (+5.3%) due
    to the near-FPU tile buffer and is followed by a slight increase in the VLSU (+5.94
    kGE), which is related to supporting matrix loads/stores. The total area overhead
    of MX is negligible, amounting to an increase of 2.5 %. The MemPool64Spatz<sup>4</sup>
    cluster follows the same trend, resulting in a similar 2.89 % area overhead. MX
    does not affect the critical path of the two systems in analysis, which runs through
    Snitch to a TCDM bank. Thus, the MXready dual- and 64-core systems achieve 920
    MHz and 720 MHz in the (SS/0.72 V/125 °C) corner, respectively, with no frequency
    degradation with respect to the baseline clusters.


    Table III LOGIC AREA BREAKDOWN IN 12-NM TECHNOLOGY.


    |         |          | Dual-Core Cluster[kGE] |        | 64-Core Cluster[MGE]
    |       |          |  |  |

    |---------|----------|------------------------|--------|----------------------|-------|----------|--|--|

    |         | Baseline | MX<br>Overhead         |        | Baseline<br>MX       |       |
    Overhead |  |  |

    | Snitch  | 47.82    | 48.01                  | +0.40% | 1.50                 |
    1.47  | -2.04%   |  |  |

    | i-Cache | 149.67   | 149.56                 | -0.07% | 4.96                 |
    4.95  | -0.20%   |  |  |

    | TCDM(a) | 1191.89  | 1192.03                | +0.01% | 20.46                |
    20.48 | +0.09%   |  |  |

    | VRF     | 345.04   | 348.87                 | +1.11% | 9.32                 |
    9.32  | 0.0%     |  |  |

    | VFU     | 1532.11  | 1613.39                | +5.31% | 12.91                |
    13.97 | +8.21%   |  |  |

    | VLSU    | 111.66   | 117.60                 | +5.32% | 2.54                 |
    3.07  | +20.87%  |  |  |

    | Other   | 570.63   | 575.97                 | +0.94% | 7.28                 |
    7.39  | +1.51%   |  |  |

    | Total   | 3948.82  | 4045.43                | +2.45% | 59.70                |
    61.43 | +2.89%   |  |  |


    (a) Including Memory Banks and Interconnect Logic.


    ![](_page_4_Figure_3.jpeg)


    Figure 3. Power breakdown for Dual-Core (Left) and 64-Core clusters (Right) executing
    MatMul. Dual-Core: at *TT@1GHz*, executing non-MX (4 vectors, length 32) and MX-ready
    algorithms (′ = 8, ′ = 4, ′ = 4, = 4). 64- Core: at *TT@910MHz*, executing non-MX
    (8 vectors, length 32) and MX-ready algorithms (′ = 8, ′ = 4, ′ = 8, = 8).


    #### *C. Performance, Power and Energy Efficiency*


    *1) Dual-Core Cluster:* The upper part of Table IV summarizes the kernel information,
    execution performance, and energy efficiency for the Dual-Core cluster when executing
    a 64-bit MatMul across various problem sizes and tile/sub-tile configurations,
    highlighting the rows where the kernel''s tile and sub-tile configurations achieve
    the best energy efficiency. The MX-ready cluster with a sub-tile size of (8, 4,
    4) achieves performance similar to the best-performing execution on the baseline
    cluster with efficiency gains by +10.9 % (16 × 16 × 16), +10.3 % (32 × 32 × 32),
    and +5.2 % (64 × 64 × 64).


    We evaluate the baseline algorithm using two different output tile configurations
    with constant sizes. For small problems (16 × 16 × 16), the output tile size of
    (8, 16, 1) yields higher FPU utilization. As discussed in Section II, although
    (8, 16, 1) has higher arithmetic intensity and fewer transfers between TCDM and
    VRF compared to (4, 32, 1), the latter configuration benefits from a 2× increase
    in SIMD ratio, leading to better performance for larger problem sizes. For the
    MX-ready algorithm, the output tiles with larger and equal ′ sub-tile dimensions
    consistently yield better performance and energy efficiency. This improvement
    is attributed to their higher arithmetic intensity and average SIMD ratio. A similar
    trend is observed for the energy efficiency when increasing the ′ dimension of
    the sub-tile. Due to the higher arithmetic intensity, the power decreases when
    the output tile size changes from (4, 16, 4) to (8, 8, 4). However, the (4, 16,
    4) configuration achieves higher performance thanks to more and shorter matrix
    result stores, which can be interleaved with computational instructions to hide
    latency.


    The left part of Figure 3 presents the power breakdown of the Dual-Core cluster''s
    baseline and MX-ready execution of a 64 × 64 × 64 MatMul, with the most energy-efficient
    tile and sub-tile size in our benchmarks. MX reduces VRF access for the B tile
    and intermediate result storage, leading to a 53.5 % reduction in VRF power consumption.
    Although the sub-tile buffer integration results in a slight 9.4 % power increase
    in VFU, the overall VPU power decreases by 4.1 %. We also observed a power decrease
    across the rest of the cluster components, including the Snitch core, instruction
    caches, and TCDM. This reduction is attributed to the higher SIMD ratio and tiled
    memory request pattern in MX-ready execution, which eliminates the multiple requests
    for scalar operands generated by the Snitch core in the baseline. As a result,
    the total power savings for the Dual-Core cluster achieved through MX amounts
    to 10.4 %.


    *2) 64-Core Cluster:* Our benchmark results for various problem sizes on MemPool64Spatz<sup>4</sup>
    are presented in the bottom section of Table IV. In such a large interconnected
    memory, contentions may occur when memory requests in the same tile access the
    same local bank or the same remote group in the same cycle. This generates stalls
    of the VLSU and increases the access latency. Although such contentions could
    be mitigated by allocating data structures in a local tile''s memory [14], this
    approach is hard to implement for MatMul, which inherently requires an extremely
    global data access pattern.


    MX regular memory accesses alleviate contention and improve VLSU utilization by
    distributing vector element loads/stores across different banks and groups in
    a strided fashion, contrasting with the baseline where vector elements are fetched
    from continuous addresses within the same group by both scalar and vector core.
    This is even more evident with small matrices, where the initial vector load and
    final result store constitute a significant portion of the total runtime due to
    the inability to hide latency. FPU utilization increases from 50.4% to 78.7%,
    leading to a 56% improvement in cluster performance.


    Despite a power consumption increase due to the higher FPU utilization, the MX-ready
    cluster achieves 25 % better energy efficiency. Even though the baseline kernels
    already achieve near-peak utilization for matrix sizes of 128 × 128 × 128 and
    256 × 256 × 256, with the same arithmetic intensity, MX still improves performance
    by 5.6 % and 2.3 %, with energy efficiency gains by 13.4 % and 9.8 %, respectively.
    The right side of Figure 3 presents the MemPool64Spatz4-related power breakdown
    comparison for a 256 × 256 × 256 MatMul. MX reduces the VRF power consumption
    by 60 %, thanks to fewer accesses achieved by buffering intermediate results.
    The VFU power increases by only 6 %, which comes from the sub-tile buffer and
    higher FPU utilization. Overall, MX leads to a 6.9 % cluster power reduction with
    near-peak FPU utilization.


    These analyses on both small- and large-scale vector clusters demonstrate that
    MX significantly improves the energy efficiency by reducing the power consumption
    related to the VRF accesses. MX also pushes the FPU utilization closer to its
    peak with a negligible area overhead. A quantitative comparison of MX against
    [7], [8] is hard since none of them presents area or power results, and the effective
    MatMul speed-up is unclear [7].


    Table IV THE SUMMARY OF KERNEL INFORMATION, EXECUTION PERFORMANCE AND ENERGY EFFICIENCY


    | Config                   | Mtx Size<br>[M, N, K] | Tile Size<br>[m, n, k] |
    Sub-Tile Size<br>[m'', n'', k''] | Mem-VRF<br>Transfers | Arithmetic<br>Intensity<br>[FLOP/B]
    | SIMD Ratio<br>[FLOP/vinsn] | Utilization | Performance<br>@ss freq<br>[GFLOPS]
    | Performance<br>@tt freq<br>[GFLOPS] | Power<br>@tt freq<br>[W] | En. Efficiency<br>@tt
    freq<br>[GFLOPS/W] |

    |--------------------------|-----------------------|------------------------|-------------------------------|----------------------|-------------------------------------|----------------------------|-------------|-------------------------------------|-------------------------------------|--------------------------|------------------------------------------|

    | Dual-Core Cluster(a) (b) |                       |                        |                               |                      |                                     |                            |             |                                     |                                     |                          |                                          |

    | Baseline                 | 64x64x64              | 8,16,1                 |
    -                             | 53248                | 1.23                                |
    16.00                      | 95.9%       | 14.13                               |
    15.34                               | 0.21                     | 71.49                                    |

    | Baseline                 | 64x64x64              | 4,32,1                 |
    -                             | 77824                | 0.84                                |
    32.00                      | 97.8%       | 14.41                               |
    15.65                               | 0.21                     | 73.48                                    |

    | Baseline                 | 32x32x32              | 8,16,1                 |
    -                             | 7168                 | 1.14                                |
    16.00                      | 90.0%       | 13.26                               |
    14.40                               | 0.20                     | 70.95                                    |

    | Baseline                 | 32x32x32              | 4,32,1                 |
    -                             | 10240                | 0.80                                |
    32.00                      | 93.3%       | 13.75                               |
    14.93                               | 0.20                     | 72.87                                    |

    | Baseline                 | 16x16x16              | 8,16,1                 |
    -                             | 1024                 | 1.00                                |
    16.00                      | 70.1%       | 10.33                               |
    11.22                               | 0.16                     | 71.69                                    |

    | Baseline                 | 16x16x16              | 4,32,1                 |
    -                             | 1408                 | 0.73                                |
    32.00                      | 64.7%       | 9.53                                |
    10.35                               | 0.16                     | 66.70                                    |

    | MX-ready                 | 64x64x64              | 4,8,4                  |
    4,4,4                         | 102400               | 0.64                                |
    34.73                      | 94.1%       | 13.86                               |
    15.06                               | 0.21                     | 72.91                                    |

    | MX-ready                 | 64x64x64              | 8,8,4                  |
    8,4,4                         | 69632                | 0.94                                |
    63.22                      | 95.6%       | 14.08                               |
    15.30                               | 0.19                     | 79.15                                    |

    | MX-ready                 | 64x64x64              | 4,16,4                 |
    4,4,4                         | 86016                | 0.76                                |
    36.76                      | 96.4%       | 14.20                               |
    15.42                               | 0.21                     | 75.19                                    |

    | MX-ready                 | 64x64x64              | 8,16,4                 |
    8,4,4                         | 53248                | 1.23                                |
    66.59                      | 97.2%       | 14.32                               |
    15.55                               | 0.19                     | 81.49                                    |

    | MX-ready                 | 32x32x32              | 4,8,4                  |
    4,4,4                         | 13312                | 0.62                                |
    34.29                      | 88.4%       | 13.02                               |
    14.14                               | 0.20                     | 71.90                                    |

    | MX-ready                 | 32x32x32              | 8,8,4                  |
    8,4,4                         | 9216                 | 0.89                                |
    62.48                      | 89.7%       | 13.22                               |
    14.35                               | 0.18                     | 77.68                                    |

    | MX-ready                 | 32x32x32              | 4,16,4                 |
    4,4,4                         | 11264                | 0.73                                |
    36.21                      | 92.7%       | 13.66                               |
    14.83                               | 0.20                     | 74.36                                    |

    | MX-ready                 | 32x32x32              | 8,16,4                 |
    8,4,4                         | 7168                 | 1.14                                |
    65.68                      | 93.5%       | 13.78                               |
    14.96                               | 0.19                     | 80.38                                    |

    | MX-ready                 | 16x16x16              | 4,8,4                  |
    4,4,4                         | 1792                 | 0.57                                |
    33.45                      | 63.1%       | 9.30                                |
    10.10                               | 0.15                     | 67.45                                    |

    | MX-ready                 | 16x16x16              | 8,8,4                  |
    8,4,4                         | 1280                 | 0.80                                |
    61.09                      | 66.1%       | 9.74                                |
    10.58                               | 0.14                     | 75.03                                    |

    | MX-ready                 | 16x16x16              | 4,16,4                 |
    4,4,4                         | 1536                 | 0.67                                |
    35.20                      | 71.6%       | 10.55                               |
    11.46                               | 0.16                     | 72.03                                    |

    | MX-ready                 | 16x16x16              | 8,16,4                 |
    8,4,4                         | 1024                 | 1.00                                |
    64.00                      | 70.3%       | 10.36                               |
    11.25                               | 0.15                     | 75.41                                    |

    | 64-Core Cluster(c)       |                       |                        |                               |                      |                                     |                            |             |                                     |                                     |                          |                                          |

    | Baseline                 | 256x256x256           | 8,32,1                 |
    -                             | 2686976              | 3.12                                |
    32                         | 94.5%       | 372.26                              |
    439.94                              | 1.57                     | 279.86                                   |

    | Baseline                 | 128x128x128           | 8,32,1                 |
    -                             | 344064               | 3.05                                |
    32                         | 90.7%       | 357.34                              |
    422.31                              | 1.57                     | 268.64                                   |

    | Baseline                 | 64x64x64              | 8,8,1                  |
    -                             | 69632                | 1.88                                |
    8                          | 50.4%       | 198.57                              |
    234.68                              | 1.20                     | 194.91                                   |

    | MX-ready                 | 256x256x256           | 8,32,8                 |
    8,4,8                         | 2686976              | 3.12                                |
    137.74                     | 96.7%       | 380.74                              |
    449.97                              | 1.46                     | 307.35                                   |

    | MX-ready                 | 128x128x128           | 8,32,8                 |
    8,4,8                         | 344064               | 3.05                                |
    136.23                     | 95.8%       | 377.27                              |
    445.86                              | 1.46                     | 304.55                                   |

    | MX-ready                 | 64x64x64              | 8,8,8                  |
    8,4,8                         | 69632                | 1.88                                |
    123.43                     | 78.7%       | 309.99                              |
    366.35                              | 1.50                     | 244.24                                   |


    (a) In bold, we highlight the best metrics for both the Baseline and MX-ready
    Dual-Core Cluster execution across various matrix sizes.


    (b) Dual-Core Cluster: Double-Precision operations; ss freq = 920MHz; tt freq
    = 1GHz.


    (c) 64-Core Cluster: Single-Precision operations; ss freq = 770MHz; tt freq =
    910MHz.


    ## V. CONCLUSION


    In this paper, we presented MX, an RVV-based ISA extension to support tiled matrix
    operations for energy-efficient MatMuls. With an embedded-device-friendly and
    extremely low footprint overhead, MX enhances the energy efficiency of MatMul
    by means of a small tile buffer near the FPUs, which minimizes the VRF accesses
    by storing and reusing both input and output matrix tiles. Moreover, MX reduces
    the number of instructions fetched by the scalar core, decreases the interaction
    between the scalar and vector cores, and regularizes the memory access pattern,
    further reducing power consumption. We characterized MX by implementing it on
    two multi-core clusters in a modern 12-nm technology node. With less than 3 %
    area overhead and no impact on the operating frequency, MX significantly boosts
    MatMul''s energy efficiency of a Dual-Core cluster by up to 10.9 %. In a 64-Core
    cluster and 64 × 64 matrices, performance and energy efficiency improve by 56
    % and 25 %, respectively, further pushing the FPU utilization toward the theoretical
    peak.


    ### ACKNOWLEDGMENTS


    This project has received funding from the ISOLDE project, No. 101112274, supported
    by the Chips Joint Undertaking of the European Union''s Horizon Europe''s research
    and innovation program and its members Austria, Czechia, France, Germany, Italy,
    Romania, Spain, Sweden, Switzerland.


    #### REFERENCES


    [1] B. Peccerillo, M. Mannino, A. Mondelli, and S. Bartolini, "A survey on hardware
    accelerators: Taxonomy, trends, challenges, and perspectives," *Journal of Systems
    Architecture*, vol. 129, p. 102561, 2022.


    - [2] H. Amiri and A. Shahbahrami, "Simd programming using Intel vector extensions,"
    *J. of Parallel and Distr. Comp.*, vol. 135, pp. 83–100, 2020.

    - [3] S. Deng, H. Zhao, W. Fang, J. Yin, S. Dustdar, and A. Y. Zomaya, "Edge intelligence:
    The confluence of edge computing and artificial intelligence," *IEEE Internet
    of Things Journal*, vol. 7, no. 8, pp. 7457–7469, 2020.

    - [4] N. Jouppi, C. Young, N. Patil, and D. Patterson, "Motivation for and evaluation
    of the first tensor processing unit," *IEEE Micro*, vol. 38, no. 3, pp. 10–19,
    2018.

    - [5] C. AI, "Edge TPU performance benchmarks," 2020. [Online]. Available: https://coral.ai/docs/edgetpu/benchmarks

    - [6] Y.-H. Chen, T. Krishna, J. S. Emer, and V. Sze, "Eyeriss: An energyefficient
    reconfigurable accelerator for deep convolutional neural networks," *IEEE Journal
    of Solid-State Circuits*, vol. 52, no. 1, pp. 127–138, 2017.

    - [7] V. Verma, T. Tracy II, and M. R. Stan, "EXTREM-EDGE EXtensions To RISC-V
    for Energy-efficient ML inference at the EDGE of IoT," *Sust. Comp.: Informatics
    and Systems*, vol. 35, p. 100742, 2022.

    - [8] T-Head Semiconductor, *RISC-V Matrix Multiplication Extension Specification*,
    T-Head Semiconductor, 2023. [Online]. Available: https:// github.com/T-head-Semi/riscv-matrix-extension-spec/releases/tag/v0.3.0

    - [9] M. Cavalcante, D. Wuthrich, M. Perotti, S. Riedel, and L. Benini, "Spatz:
    A ¨ compact vector processing unit for high-performance and energy-efficient shared-L1
    clusters," in *Proc. of the 41st ICCAD*. San Diego, CA, USA: IEEE/ACM, Oct. 2022.

    - [10] H. T. Kung, "Memory requirements for balanced computer architectures,"
    *SIGARCH Comp. Arch. News*, vol. 14, no. 2, p. 49–54, May 1986.

    - [11] B. Dally, "Hardware for deep learning," in *Hot Chips*, Stanford, CA, USA,
    Aug. 2023.

    - [12] M. Perotti, M. Cavalcante, N. Wistoff, R. Andri, L. Cavigelli, and L. Benini,
    "A ''New Ara'' for Vector Computing: an Open Source Highly Efficient RISC-V V
    1.0 Vector Processor Design," in *Proceedings of the 33rd IEEE Int. Conf. on ASAP*.
    Gothenburg, Sweden: IEEE, Jul. 2022.

    - [13] S. Riedel, M. Cavalcante, R. Andri, and L. Benini, "MemPool: A scalable
    manycore architecture with a low-latency shared L1 memory," *IEEE Transactions
    on Computers*, 2023, early access.

    - [14] M. Bertuletti, Y. Zhang, A. Vanelli-Coralli, and L. Benini, "Efficient
    parallelization of 5G-PUSCH on a scalable RISC-V many-core processor," in *Proc.
    of the 2023 DATE Conf.* Antwerp, Belgium: IEEE, Mar. 2023.'
  decisions:
    evaluation_prompt: 'Qualified. Reason: The paper includes quantitative analysis
      and mentions a detailed power, performance, and area (PPA) analysis, as well
      as a theoretical justification of benefits.'
    related_work_prompt: 'Disqualified: no related work. Reason: Lacks a section labeled
      “Related Work” or equivalent.'
- title: "X-HEEP: An Open-Source, Configurable and Extendible RISC-V\n  Microcontroller\
    \ for the Exploration of Ultra-Low-Power Edge Accelerators"
  abstract: 'The field of edge computing has witnessed remarkable growth owing to
    the

    increasing demand for real-time processing of data in applications. However,

    challenges persist due to limitations in performance and power consumption. To

    overcome these challenges, heterogeneous architectures have emerged that

    combine host processors with specialized accelerators tailored to specific

    applications, leading to improved performance and reduced power consumption.

    However, most of the existing platforms lack the necessary configurability and

    extendability options for integrating custom accelerators. To overcome these

    limitations, we introduce in this paper the eXtendible Heterogeneous

    Energy-Efficient Platform (X-HEEP). X-HEEP is an open-source platform designed

    to natively support the integration of ultra-low-power edge accelerators. It

    provides customization options to match specific application requirements by

    exploring various core types, bus topologies, addressing modes, memory sizes,

    and peripherals. Moreover, the platform prioritizes energy efficiency by

    implementing low-power strategies, such as clock-gating and power-gating. We

    demonstrate the real-world applicability of X-HEEP by providing an integration

    example tailored for healthcare applications that includes a coarse-grained

    reconfigurable array (CGRA) and in-memory computing (IMC) accelerators. The

    resulting design, called HEEPocrates, has been implemented both in field

    programmable gate array (FPGA) on the Xilinx Zynq-7020 chip and in silicon with

    TSMC 65nm low-power CMOS technology. We run a set of healthcare applications

    and measure their energy consumption to demonstrate the alignment of our chip

    with other state-of-the-art microcontrollers commonly adopted in this domain.

    Moreover, we present the energy benefits of 4.9x and 4.8x gained by exploiting

    the integrated CGRA and IMC accelerators compared to running on the host CPU.'
  url: http://arxiv.org/abs/2401.05548v2
  keywords: ''
  document: '# X-HEEP: An Open-Source, Configurable and Extendible RISC-V Microcontroller
    for the Exploration of Ultra-Low-Power Edge Accelerators


    [SIMONE MACHETTI,](HTTPS://ORCID.ORG/0000-0002-2887-5031) Embedded Systems Laboratory
    (ESL), EPFL, Switzerland [PASQUALE DAVIDE SCHIAVONE,](HTTPS://ORCID.ORG/0000-0003-2931-0435)
    Embedded Systems Laboratory (ESL), EPFL, Switzerland [THOMAS CHRISTOPH MÜLLER,](HTTPS://ORCID.ORG/0009-0004-2805-6310)
    Embedded Systems Laboratory (ESL), EPFL, Switzerland [MIGUEL PEÓN-QUIRÓS,](HTTPS://ORCID.ORG/0000-0002-5760-090X)
    EcoCloud, EPFL, Switzerland [DAVID ATIENZA,](HTTPS://ORCID.ORG/0000-0001-9536-4947)
    Embedded Systems Laboratory (ESL), EPFL, Switzerland


    The field of edge computing has witnessed remarkable growth owing to the increasing
    demand for real-time processing of data in applications. However, challenges persist
    due to limitations in the performance and power efficiency of edge-computing devices.
    To overcome these challenges, heterogeneous architectures have emerged that combine
    host processors with specialized accelerators tailored to specific applications,
    leading to improved performance and reduced power consumption. However, most of
    the existing platforms lack configurability and extendability options, necessitating
    extensive modifications of the register transfer level (RTL) code for integrating
    custom accelerators.


    To overcome these limitations, we introduce in this paper the eXtendible Heterogeneous
    Energy-Efficient Platform (X-HEEP). X-HEEP is an open-source platform designed
    to natively support the integration of ultra-low-power edge accelerators. It provides
    customization options to match specific application requirements by exploring
    various core types, bus topologies, and memory addressing modes. It also enables
    a fine-grained configuration of memory banks to match the constraints of the integrated
    accelerators. The platform prioritizes energy efficiency by implementing low-power
    strategies, such as clock-gating and power-gating, and integrating these with
    connected accelerators through dedicated power control interfaces.


    We demonstrate the real-world applicability of X-HEEP by providing an integration
    example tailored for healthcare applications that includes a coarse-grained reconfigurable
    array (CGRA) and in-memory computing (IMC) accelerators. The resulting design,
    called HEEPocrates, has been implemented both in field programmable gate arrays
    (FPGAs) on multiple Xilinx chips, for prototyping and exploration, and in silicon
    with TSMC 65 nm low-power CMOS technology. The fabricated chip can operate from
    0.8 V to 1.2 V, achieving a maximum frequency of 170 MHz and 470 MHz, respectively.
    Its power consumption ranges from 270 µW at 32 kHz and 0.8 V, to 48 mW at 470
    MHz and 1.2 V.


    We run a set of healthcare applications and measure their energy consumption to
    demonstrate the alignment of our chip with other state-of-the-art microcontrollers
    commonly adopted in this domain, showing that HEEPocrates provides a good trade-off
    between acquisition-dominated and processing-dominated applications for energy
    efficiency. Moreover, we present the energy benefits of 4.9 × and 4.8 × gained
    by exploiting the integrated CGRA accelerator and IMC accelerator, respectively,
    compared to running on the host CPU.


    Additional Key Words and Phrases: Ultra-Low Power, Energy Efficiency, Microcontroller,
    Accelerator, Field Programmable Gate Array (FPGA), Course-Grained Reconfigurable
    Array (CGRA), In-Memory Computing (IMC), Tapeout, Silicon Validation.


    Authors'' addresses: [Simone Machetti,](https://orcid.org/0000-0002-2887-5031)
    Embedded Systems Laboratory (ESL), EPFL, Lausanne, Switzerland; [Pasquale Davide
    Schiavone,](https://orcid.org/0000-0003-2931-0435) Embedded Systems Laboratory
    (ESL), EPFL, Lausanne, Switzerland; [Thomas Christoph Müller,](https://orcid.org/0009-0004-2805-6310)
    Embedded Systems Laboratory (ESL), EPFL, Lausanne, Switzerland; [Miguel Peón-Quirós,](https://orcid.org/0000-0002-5760-090X)
    EcoCloud, EPFL, Lausanne, Switzerland; [David Atienza,](https://orcid.org/0000-0001-9536-4947)
    Embedded Systems Laboratory (ESL), EPFL, Lausanne, Switzerland.


    © 2024 Association for Computing Machinery.


    Manuscript submitted to ACM


    Permission to make digital or hard copies of all or part of this work for personal
    or classroom use is granted without fee provided that copies are not made or distributed
    for profit or commercial advantage and that copies bear this notice and the full
    citation on the first page. Copyrights for components of this work owned by others
    than ACM must be honored. Abstracting with credit is permitted. To copy otherwise,
    or republish, to post on servers or to redistribute to lists, requires prior specific
    permission and/or a fee. Request permissions from permissions@acm.org.


    #### ACM Reference Format:


    Simone Machetti, Pasquale Davide Schiavone, Thomas Christoph Müller, Miguel Peón-Quirós,
    and David Atienza. 2024. X-HEEP: An Open-Source, Configurable and Extendible RISC-V
    Microcontroller for the Exploration of Ultra-Low-Power Edge Accelerators. 1, 1
    (March 2024), [21](#page-20-0) pages. <https://doi.org/10.1145/nnnnnnn.nnnnnnn>


    #### 1 INTRODUCTION


    In recent years, the field of edge computing has witnessed remarkable growth and
    adoption in commercial products. This process has been driven by the increasing
    demand for real-time computing solutions, particularly Artificial Intelligence
    (AI) and Machine Learning (ML) algorithms. As data processing at the edge for
    new edge AI computing has become more prevalent, the performance and power consumption
    limitations of edge-computing devices have become increasingly apparent, which
    has posed significant challenges for researchers and engineers.


    Heterogeneous architectures have emerged to overcome these challenges. These architectures
    offer a promising path toward high energy efficiency while maintaining performance
    constraints. Heterogeneous architectures rely on a combination of ultra-low-power
    host processors to run control and communication tasks, and custom accelerators
    tailored to specific application domains, such as artificial intelligence, image
    processing, healthcare, and cryptography, to run computationally demanding tasks.


    Control and communication tasks include accessing external memories, acquiring
    data from analog-to-digital converters (ADCs) or sensors, preparing data for computations,
    and running real-time operating system (RTOS) functions. Meanwhile, computational
    demanding tasks focus on data processing, for example, convolutional and fully
    connected layers in neural networks (NNs), fast-Fourier transforms (FFTs) in temporal
    series, secure-hash algorithms (SHA) in cryptography, etc.


    Each accelerator comes with unique requirements, such as memory size, area, performance,
    and power, to meet the constraints of the target applications. For this reason,
    proper customization of host platforms is imperative. This may include exploring
    different CPUs to trade performance and power, bus topologies and memory hierarchy,
    memory sizes to accommodate the required computational data, peripherals to provide
    the necessary I/O connectivity, power domains and strategies, etc. However, commercial
    platforms limit hardware exploration due to their non-open-source nature. They
    often involve costly licensing models and do not allow for customization. As a
    result, there is a growing preference for open-source platforms as a more attractive
    solution that does not limit exploration and customization, and that gives designers
    digital sovereignty and control over IPs.


    Today, there are an increasing number of open-source projects related to heterogeneous
    systems, thanks to the open RISC-V instruction set architecture (ISA) revolution.
    However, many of such platforms focus only on the CPU part, whereas microcontroller-based
    state-of-the-art projects lack the flexibility and customization options needed
    to fulfill accelerator requirements natively. These limitations include restricted
    configurability for the internal platform''s components (core, memory, bus, etc.)
    to adapt to the application needs, limited support for external accelerator connectivity
    to communicate with the host system, and inadequate built-in power management
    strategies to optimize energy efficiency. Thus, hardware developers need to extensively
    modify the platform to properly align with the target applications on their own
    copy of the platform. This includes forking, modifying, and maintaining the forked
    platform''s repository, leading to high maintenance costs. Therefore, addressing
    the configurability and extendability aspects of these platforms is crucial to
    lowering the adoption barrier of open-source-based edge solutions.


    In this paper, we address the limitations mentioned above by introducing X-HEEP[1](#page-2-0)
    [\[30\]](#page-20-1), an open-source configurable and extendable platform designed
    to support the exploration of ultra-low power edge accelerators. X-HEEP is a streamlined
    configurable host architecture based on RISC-V and built on top of existing IPs
    from relevant open-source projects, such as the PULP project, the OpenHW Group,
    and the OpenTitan project, as extensively verified, validated in silicon, and
    adopted in commercial products. This allows extensive reuse of third-party hardware
    and software extensions and inheriting verification and silicon validation.


    To allow users to explore their custom solutions, X-HEEP can be natively extended
    via the proposed eXtendible Accelerator InterFace (XAIF), which allows the integration
    of a wide range of accelerators with different area, power, and performance constraints.
    Having a complete interface that covers all the edge accelerator performance and
    power requirements will enable extensive reuse of hardware and software IPs, reducing
    costs and mitigating fragmentation. To explore the custom hardware design space,
    users will use X-HEEP as an IP which exposes an interface capable of addressing
    all the edge-computing state-of-the-art requirements for domain-specific applications.
    To enable a high degree of versatility, such exploration can be performed both
    on FPGAs or RTL simulators, as well as SystemC for a mixed high-level and RTL
    simulation environment. Additionally, to offer high degree of optimizations, X-HEEP
    offers internal configurability options through the selection of different (1)
    core types, depending on the target workloads [\[29\]](#page-20-2); (2) bus topology
    and addressing mode, ensuring a perfect match with the bandwidth requirements
    of the integrated accelerators; (3) memory size, depending on the processing data
    and application complexity; and finally (4) peripherals, to provide the needed
    I/O connectivity. This configurability enables designers to tailor the platform
    to specific application requirements and meet area, power, and performance constraints.


    As energy efficiency is a key figure in edge computing devices, X-HEEP implements
    state-of-the-art fine-grained low-power strategies such as clock-gating, power-gating,
    and RAM retention, which are integrated into the XAIF interface to be leveraged
    by the connected accelerators and maximize overall energy efficiency.


    To demonstrate the real-world applicability of X-HEEP, we present an integration
    example specifically tailored for ultra-low-power healthcare applications. These
    applications typically involve long and slow acquisition periods, where data from
    external bio-sensors are stored in memory while the rest of the system is in an
    idle state, followed by intense processing periods to compute pattern extraction
    algorithms based on digital signal processing algorithms, or machine learning
    (ML), and deep learning. Therefore, we extended X-HEEP with a CGRA accelerator
    [\[9\]](#page-19-0) and an IMC accelerator [\[31\]](#page-20-3), both of which
    have been shown to efficiently reduce the overall energy consumption of healthcare
    applications [\[7,](#page-19-1) [24\]](#page-20-4). We configured X-HEEP with
    the RISC-V OpenHW Group CV32E20 core [\[29\]](#page-20-2), 8 banks of 32 KiB on-chip
    SRAM organized in a contiguous addressing mode, and 11 different power domains
    (including the external accelerators) that can be individually switched on and
    off for fine-grained power control.


    The resulting design, called HEEPocrates, has been implemented both in FPGAs on
    the Zynq 7020, Zynq UltraScale+, and Artix 7 chips by Xilinx, for early prototyping,
    verification, and system exploration, as well as in silicon with TSMC 65 nm low-power
    CMOS technology, for silicon validation and profiling performance, power, and
    area figures. The measured performance of the fabricated chip shows that it can
    operate in a wide voltage range, from 0.8 V to 1.2 V, achieving a maximum frequency
    of 170 MHz and 470 MHz, respectively. Its power consumption ranges from 270 µW
    at 32 kHz and 0.8 V, to 48 mW at 470 MHz and 1.2 V.


    To validate our design and compare it with state-of-the-art solutions, we measured
    the energy consumption of the chip in a set of healthcare applications, showing
    that it offers a good trade-off between the computationally hungry


    <span id="page-2-0"></span><sup>1</sup>X-HEEP is freely downloadable at https://github.com/esl-epfl/x-heep
    under a permissive license.


    and acquisition-dominated state-of-the-art microcontrollers commonly adopted in
    this domain. This demonstrates the flexibility of our host platform in adapting
    to the specific needs of integrated accelerators and matching the strict requirements
    of healthcare applications. Furthermore, we present the energy benefits of 4.9
    × and 4.8 × gained by exploiting the integrated CGRA accelerator and the IMC accelerator,
    respectively, compared to running on the host CPU.


    Throughout this work, we will dive deeper into the features of the X-HEEP platform,
    describing its architecture, configurability, and extendability options to build
    versatile and energy-efficient edge applications.


    The following contributions are presented:


    - X-HEEP: A configurable and extendible RISC-V microcontroller to support the
    exploration of ultra-low-power edge accelerators.

    - XAIF: A configurable interface that adapts to the different requirements of
    accelerators in terms of programmability, bandwidth, interrupts, and power modes,
    and that allows their seamless integration into the X-HEEP architecture.

    - HEEPocrates: A real-world integration example, based on TSMC 65 nm low-power
    CMOS technology, that includes a CGRA and an IMC accelerator.

    - An open-source repository with a permissive license with the complete X-HEEP
    platform code and documentation to allow researchers to explore new custom accelerators
    and advance research in this field.


    The remainder of this paper is structured as follows. In Section 2, we conduct
    an in-depth analysis of the most relevant state-of-the-art accelerators and host
    platforms. In Section 3, we provide a qualitative and quantitative description
    of the configurability and extendability features of X-HEEP. In Section 4, we
    present a real-world integration example, called HEEPocrates. In Section 5 we
    describe our experimental setup, while in Section 6 our experimental results.
    Lastly, in Section 7, we offer a comprehensive summary of the main conclusions
    of our work.


    #### 2 STATE-OF-THE-ART


    This section gives an overview of cutting-edge accelerators, analyzing the fundamental
    requirements necessary for their integration into host platforms. Such requirements
    are collected into the XAIF to accommodate all the state-of-the-art accelerators.
    Subsequently, it focuses on host platforms from the state of the art, conducting
    an evaluation of their strengths and limitations in terms of configurability and
    extendability.


    #### 2.1 Edge-computing accelerators


    The extensive array of open-source accelerators includes a diverse range of requirements
    regarding memory capacity, area, performance, and power efficiency. Therefore,
    an analysis of their main features becomes imperative for the design of flexible
    and efficient host platforms.


    We can divide accelerators into three main categories: memories, processors (and
    co-processors), and I/O peripherals. One accelerator can belong to one or more
    categories.


    <span id="page-3-0"></span>2.1.1 Memories. Memory accelerators are a class of
    IPs that feature one or more slave ports to access internal functionality. These
    IPs require the host CPU, or a DMA, to copy the needed computational data from
    the main memory of the platform to their internal data memory, before starting
    the operations. At the end of the computations, an interrupt or status bit could
    be used to synchronize with the host CPU.


    An example is the Keccak accelerator presented in [\[8\]](#page-19-2). This accelerator
    exposes two 32 bit slave ports, one to access the internal register file used
    for control and status operations and one to access the private data memory, which
    stores the processing data.


    Other examples are in-memory or near-memory macros such as the C-SRAM [\[15\]](#page-19-3),
    where the IP is connected via a 32 bit slave port to a host platform that sends
    commands/instructions to the memory through write operations. C-SRAM decodes the
    memory instructions, by concatenating the address and the write data transmitted
    by the CPU, and performs the requested operation.


    <span id="page-4-0"></span>2.1.2 Processors. To improve performance, many accelerators
    feature one or multiple master ports to independently read in parallel the processing
    data and write back the generated results from/to the main memory.


    Some examples are domain-specific accelerators, such as DSP engines [\[16\]](#page-19-4),
    CGRAs [\[9\]](#page-19-0), multi-CPU clusters [\[25\]](#page-20-5), GPUs [\[33\]](#page-20-6),
    etc., and application-specific accelerators for neural networks [\[5\]](#page-19-5),
    FFT [\[32\]](#page-20-7), cryptography [\[8\]](#page-19-2), image processing [\[20\]](#page-20-8),
    etc.


    An example of a domain-specific accelerator is the CGRA presented in [\[9\]](#page-19-0),
    which has two 32 bit slave ports for configuration registers and private instruction
    memory and four 32 bit master ports for reading and writing data from and to the
    main memory, reaching a maximum bandwidth of 128 bit per bus cycle.


    Another example is the PULP cluster [\[25\]](#page-20-5), which features four
    to eight CV32E40P cores [\[12\]](#page-19-6) connected to a shared instruction
    cache and a multi-bank scratchpad memory. The cluster exposes one 32 bit slave
    port for configuration and for pre-loading the memories, and one 64 bit master
    port shared between the cluster DMA, to transfer data in and out of the scratchpad
    memory, and the instruction cache, to fetch program code.


    Examples of application-specific accelerators are Echoes [\[32\]](#page-20-7)
    and Marsellus [\[13\]](#page-19-7). The former is used to speed up FFT execution
    and has eight 32 bit master ports, four allocated for input and four for output,
    reaching a maximum bandwidth of 256 bit per bus cycle. The latter accelerates
    convolution layers and offers nine 32 bit master ports, with a maximum bandwidth
    of 288 bit per bus cycle.


    Co-processors are a sub-category of processors used to implement custom ISA extensions.
    Co-processors are either tightly coupled in the processor pipeline, or integrated
    via a dedicated interface for reusability.


    Their wide application domains include floating-point operations [\[3\]](#page-19-8),
    posit arithmetic [\[18\]](#page-20-9), post-quantum cryptography [\[10\]](#page-19-9),
    integer complex arithmetic [\[34\]](#page-20-10), etc. For example, [\[10\]](#page-19-9)
    proposes a post-quantum cryptography ISA extension and interacts with the coupled
    RISC-V CPU thanks to the CORE-V-XIF [\[6\]](#page-19-10) interface of OpenHW Group.


    All these previous examples illustrate that there is a large choice of possible
    processors and co-processors for edge AI systems today. Therefore, it is required
    to have a fast and scalable exploration and prototyping framework to choose the
    right set of components, co-processors, or domain-specific accelerators that a
    final implementation should have, and then a well-tuned silicon design flow with
    a predefined set of open-source hardware components and peripherals.


    2.1.3 I/O peripherals. These IPs are meant to implement special interfaces to
    communicate with off-chip components or to pre/post-process data during such communications.


    An example can be found in Arnold [\[27\]](#page-20-11), where the embedded FPGA
    (eFPGA) can be used to control an off-chip accelerator, which requires a custom
    interface, as well as to pre-process data coming from peripherals before being
    stored in memory.


    In this case, there is a clear need to ideally have a framework that can target
    both on-chip and off-chip accelerator concepts by enabling a flexible set of interconnect
    standards. This set of standards should be extendable with minimum


    effort from the system designer thanks to an interface that enables a superset
    of interconnection protocols, as we propose in X-HEEP.


    ### 2.2 Host platforms


    In this subsection, we present a comparison of relevant open-source platforms
    that can be used to host edge-computing accelerators, focusing on configurability,
    extendability, and other key features. The limitations of each platform are analyzed
    in detail to motivate the need for a dedicated solution that can fulfill all the
    requirements.


    2.2.1 PULPissimo [\[28\]](#page-20-12). A single-core platform within the PULP
    family, designed to target ultra-low-power edgecomputing applications. Depending
    on performance requirements, designers can configure the platform with the CV32E20
    [\[29\]](#page-20-2) or CV32E40P [\[12\]](#page-19-6) cores. PULPissimo has been
    integrated with various accelerators, including the aforementioned multi-CPU cluster
    [\[25\]](#page-20-5), neural network accelerators [\[13\]](#page-19-7), CGRAs
    [\[9\]](#page-19-0), eFPGAs [\[27\]](#page-20-11), etc. Many silicon prototypes
    have been implemented, which demonstrate best-in-class energy efficiency in a
    wide range of applications.


    However, PULPissimo provides only a generic AXI 32 bit slave and a 64 bit master
    external interfaces that are used to connect the multi-CPU cluster, while the
    other accelerators have been integrated by forking and modifying the original
    RTL code. Such external interfaces may limit accelerators'' bandwidth. Moreover,
    the platform lacks native support for external interrupts/events and power control,
    which is crucial for efficient power management. Lastly, the platform does not
    offer configurability options to select memory size, bus topology, and memory
    addressing mode, or to change the included peripherals, which limit area, bandwidth,
    and power-space exploration.


    2.2.2 Cheshire [\[22\]](#page-20-13). The limitations mentioned above have been
    partially addressed by another PULP-based platform, called Cheshire. Cheshire
    is based on the CVA6 core [\[35\]](#page-20-14) and allows designers to choose
    the number of external slave and master ports to connect their custom accelerators.
    Furthermore, the platform allows for the configuration of the internal last-level
    cache (LLC) size and of the necessary peripherals, providing the flexibility needed
    to target specific application requirements.


    However, Cheshire has been designed for high-performance systems and consumes
    up to 300 mW, making it unsuitable for most ultra-low-power devices, which typically
    operate in the range of tens of mW. Furthermore, Cheshire lacks support for external
    interrupts and power control, which has implications for its overall energy efficiency,
    as the accelerators are usually power-hungry. Lastly, designers do not have the
    option to select the core type, bus topology, and memory addressing mode.


    2.2.3 BlackParrot [\[23\]](#page-20-15). An open-source Linux-capable platform
    designed to accommodate one or multiple customdesigned accelerators. The platform
    showcases a mesh of heterogeneous tiles, offering the flexibility to compose 64
    bit BlackParrot cores, L2 cache slices, I/O, DRAM controllers, and accelerators
    in various configurations.


    However, it does not allow for selecting the core type, bus topology, and memory
    addressing mode. Additionally, the absence of essential peripherals commonly used
    in edge devices, such as I2Cs, GPIOs, timers, DMAs, interrupt controllers, and
    a power manager to implement low-power strategies, restricts the usage of the
    platform for real applications deployed on ultra-low-power edge applications.
    Moreover, the platform''s internal integration of accelerators, as opposed to
    external plug-ins, involves forking and modifying the original RTL code, leading
    to greater effort and higher development costs. Lastly, the 64 bit architecture
    of BlackParrot targets high-performance systems and is unsuitable for ultra-low-power
    edge devices.


    2.2.4 OpenTitan [\[17\]](#page-19-11). OpenTitan is designed for ultra-low-power
    edge-secure applications. It offers a single-core architecture based on the CV32E20
    [\[29\]](#page-20-2) core and an extensive portfolio of peripherals.


    Despite these strengths, OpenTitan does not offer external support for accelerator
    plug-ins, requiring designers to manually modify the RTL code to integrate their
    custom accelerators. Furthermore, the platform lacks configurability for core
    type, bus topology, and memory addressing mode and size. Furthermore, OpenTitan
    does not come equipped with built-in low-power strategies.


    2.2.5 Chipyard [\[1\]](#page-19-12). On the contrary, the Rocket chip generator
    [\[26\]](#page-20-16), which has been subsequently incorporated and expanded into
    the Chipyard platform, offers extensive configuration options. Using the open-source
    Chisel hardware description language, designers can craft their system, providing
    flexibility and customization. The platform offers a wide range of core types,
    including Ariane, CV32E20, Rocket, and BOOM, allowing designers to tailor the
    system''s performance to meet specific application requirements. Additionally,
    the memory size and peripherals can be customized, further enhancing its adaptability.


    However, even though Chipyard enables accelerators to be integrated into the design
    using the Chisel language, the platform does not offer external master and slave
    ports for the connectivity of accelerators. As a result, designers need to invest
    time in becoming familiar with the Chisel language to successfully configure the
    architecture and integrate custom accelerators. Furthermore, Chipyard does not
    provide support for any specific power reduction strategies. Given the critical
    importance of power efficiency in ultra-low-power applications, designers are
    forced to implement power-saving techniques manually to achieve the desired energy
    efficiency level.


    2.2.6 LiteX [\[14\]](#page-19-13) and ESP [\[19\]](#page-20-17). Two other notable
    SoC generators are LiteX and ESP. LiteX serves as a framework thought to explore
    various FPGA-based architectures. On the other hand, ESP is an open-source platform
    designed for heterogeneous SoC design and prototyping on FPGAs. Both platforms
    offer configurable options, allowing designers to customize core type, memory
    size, peripherals, and the number of external master and slave ports, making them
    adaptable to various application requirements.


    However, LiteX and ESP focus on FPGA development only and do not offer support
    for ASIC design flow. Such limitations hinder their applicability in projects
    aimed at silicon implementations and present difficulties in accurately estimating
    the platform energy consumption, crucial when evaluating the impact of integrated
    accelerators. Moreover, they lack built-in support for external interrupts and
    power control, essential for efficient power management.


    2.2.7 X-HEEP. To overcome the limitations mentioned above and cater to the unique
    needs of ultra-low-power edge designers, we present in this paper the X-HEEP platform.
    The proposed platform features a streamlined architecture that operates in conjunction
    with dedicated open-source tools. These tools enable developers to easily customize
    and extend the architecture with their accelerators and interconnection interfaces,
    thus eliminating the need for manual modification of the RTL code. Using X-HEEP,
    designers can achieve the desired level of configurability, extendability, and
    power efficiency, making it an ideal choice for a wide range of ultra-low-power
    edge applications. In addition, it has been developed using SystemVerilog, to
    offer high compatibility with most of the available electronic design automation
    (EDA) tools.


    #### 3 X-HEEP


    In this section, we present a qualitative and quantitative analysis of the key
    features of X-HEEP regarding configurability, extendability, and software support.
    We synthesized X-HEEP with TSMC 65 nm low-power technology and performed our quantitative
    analysis at the nominal voltage, 1.2 V.


    #### 3.1 Architecture


    Figure [1](#page-8-0) shows the X-HEEP architecture and its essential components.
    These include a configurable RISC-V CPU, a configurable bus, a configurable memory,
    two configurable peripheral domains, and a debug unit.


    X-HEEP leverages existing widely adopted open-source IPs to maintain compatibility
    with existing systems and reuse available software routines and hardware extensions.
    Among the wide portfolio of open-source IPs, we selected those that provide permissive
    licenses, to ease the X-HEEP adoption to a wide range of users, and written in
    SystemVerilog, to make the integration in existing systems and EDA tools compatible
    with industrial standards.


    The RISC-V cores have been selected from the OpenHW Group CORE-V family, as extensively
    verified, mature, and implemented in silicon many times; the bus, the memory models,
    the debug unit, and a plethora of IPs from the PULP project, as again adopted
    by several stakeholders and validated in silicon multiple times; and the peripherals
    from the OpenTitan project as documented, verified, and inclusive of hardware-abstraction-layer
    (HAL) functions. Moreover, X-HEEP includes home-made IPs such as a boot ROM, a
    power manager, a fast interrupt controller, and a DMA.


    3.1.1 CPU. The user can choose among the CV32E20, CV32E40X, and CV32E40P as core
    options [\[29\]](#page-20-2), to trade off power and performance. In particular,
    the CV32E20 core is optimized for control-oriented tasks, while the CV32E40P core
    is optimized for processing-oriented tasks. The CV32E40X core offers power consumption
    and performance similar to the CV32E40P core, without featuring the floating-point
    RVF and custom Xpulp ISA extensions. Moreover, it provides an external interface,
    known as CORE-V-XIF [\[6\]](#page-19-10), that allows for the plug-in of custom
    co-processors to extend the RISC-V ISA without the need to modify the RTL code
    of the core.


    3.1.2 Memory. The user can select the memory size and number of memory banks to
    trade off area, power, and storage capacity. Each bank offers a retention state
    aimed at reducing leakage power, of about 42.5 % compared to active leakage, when
    the bank is not accessed for some time but the data needs to be preserved.


    3.1.3 Bus. To maximize compatibility with the other IPs selected from the OpenHW
    Group, PULP platforms, and OpenTitan project, the bus is based on the same open-bus
    interface (OBI) [\[21\]](#page-20-18) protocol.


    The user can choose either a one-at-a-time topology, where only one master at
    a time can access the bus (one decoder), or a fully connected topology (same number
    of decoders as simultaneous masters), where multiple masters can access multiple
    slaves in parallel, to trade off area and bandwidth. When the fully connected
    option is used, the user can further configure the bus to access a variable number
    of banks in a contiguous or interleaved addressing mode. The contiguous mode offers
    limited bandwidth to applications that require multiple masters to access contiguous
    data stored in memory but allows for power-gating or setting in retention mode
    the banks that are not actively used. Vice versa, the interleaved mode offers
    higher bandwidth to applications that access contiguous data in memory, at the
    cost of keeping all the banks active all the time.


    In addition, to connect additional components, the bus also exposes a configurable
    number of slave and master ports to the external XAIF interface to accommodate
    one or multiple accelerators with different bandwidth constraints. Manuscript
    submitted to ACM


    <span id="page-8-0"></span>![](_page_8_Figure_1.jpeg)


    Figure 1. X-HEEP architecture. The various power domains are visually marked using
    different colors. The components in grey are always on. The accelerator and co-processor
    integration are highlighted in red.


    Figures [2](#page-9-0) (a) and (b) show the variation in the area and the bandwidth
    of the X-HEEP bus by adding slave/master ports to the basic bus configuration,
    which connects the CV32E20 core, two memory banks, the debug unit, the two peripheral
    domains, and no external connection. Ports are added in pairs, i.e., for each
    external master port (M), we add an internal slave port (S) for a memory bank
    to avoid limiting bandwidth during memory access.


    Increasing the number of slave/master ports does not lead to any performance improvement
    in the one-at-atime configuration, limited to 32 bit per bus cycle according to
    its architecture. On the contrary, the fully connected configuration maximizes
    bandwidth, which increases linearly with the number of bus ports, at the cost
    of a higher area (and power consumption). The bus in the one-at-a-time configuration
    occupies about 85 % less silicon space compared to the fully connected configuration,
    considering the same number of slave/master ports.


    In overall performance, a 16 × 16 matrix multiplication algorithm on X-HEEP takes
    approximately 34 % fewer clock cycles in the highest performance configuration
    with the CV32E40P core and fully connected bus compared to the lowest power configuration
    with the CV32E40P core and one-at-a-time bus. Furthermore, when using the Xpulp
    extensions and fully connected bus, the CV32E40P can compute matrix multiplication
    algorithms 4 × faster with 32 bit data or up to 16 × faster with 8 bit SIMD extensions
    for the same CPU without extensions, as shown in [\[12\]](#page-19-6).


    3.1.4 Peripheral domain. Figure [2](#page-9-0) (c) shows the area of the IPs located
    in the peripheral domain. This domain includes peripherals that can be removed
    from the design or powered off if not needed to trade off area or power and functionality.
    Manuscript submitted to ACM


    <span id="page-9-0"></span>![](_page_9_Figure_1.jpeg)


    Figure 2. Exploration of different X-HEEP configurations. The used technology
    is TSMC 65 nm low-power CMOS at 1.2 V.


    These include a platform-level interrupt controller (PLIC), a timer, and general-purpose
    I/O peripherals such as a GPIO, I2C, and SPI.


    3.1.5 Always-on peripheral domain. This domain includes IPs that are always powered
    on. To meet our specific needs and requirements, we custom-designed key components
    such as an SoC controller, a boot ROM, a power manager, a fast interrupt controller,
    and a DMA. The domain also includes other peripherals such as a timer, a UART,
    an SPI, and a GPIO.


    The power manager is responsible for implementing low-power strategies, including
    clock-gating, power-gating, and RAM retention. It features a set of configuration
    registers that provide the user with real-time control over the available low-power
    techniques.


    The architecture is divided into several power domains, marked with different
    colors in Figure [1.](#page-8-0) Clock-gating can be applied to the main CPU,
    peripheral domain, and each memory bank, while retention can only be applied to
    memory banks. Additionally, each power domain can be individually power-gated.
    The leakage power consumption of each domain is reported in Figure [2](#page-9-0)
    (d). The system bus, debug unit, and other essential IPs represent about 35 %
    of the Manuscript submitted to ACM


    leakage power of the always-on domain. The remaining 65 % comes from other general-purpose
    peripherals added to enhance versatility, such as a GPIO, SPI, UART, etc.


    The platform can be extended with additional power domains to include user external
    accelerators. This is possible thanks to external power ports, part of the XAIF
    interface, directly connected to the power manager, which can be used to clock-gate,
    power-gate, or set in retention mode external accelerators.


    #### 3.2 Extendible Accelerator InterFace (XAIF)


    The extensive array of domain-specific hardware accelerators encompasses a diverse
    range of requirements, including memory capacity, area, performance, and power
    efficiency. These varied demands are aggregated into the configurable XAIF interface,
    facilitating enhanced connectivity to state-of-the-art accelerators, and agile
    integration into microcontrollers for real-life applications. Such an interface
    gathers all the requirements to extend X-HEEP with domain-specific customizations.
    To the best of the authors'' knowledge, no other open-source platform for edge-computing
    applications exists that provides such a complete extension interface to fulfill
    the requirements of state-of-the-art solutions.


    3.2.1 Memory mapped ports. A configurable number of slave and master ports, utilizing
    the OBI protocol, can be harnessed to connect custom accelerators to the X-HEEP
    bus. Slave ports provide easy access and configuration for memory-like accelerators,
    exemplified in Subsection [2.1.1,](#page-3-0) such as the Keccak [\[8\]](#page-19-2),
    which requires two 32 bit slave ports for control and status operations and data
    memory. In addition, a further peripheral interface connected to the X-HEEP peripheral
    bus is provided for external custom peripherals. This peripheral interface is
    further extended by a FIFO interface to allow easy DMA-peripheral connections.
    This allows the CPU to wait for peripheral transactions to transfer all data to
    the main memory with the support of the system DMA, as implemented in [\[27\]](#page-20-11).
    On the other hand, master ports accommodate the bandwidth requirements of processor-like
    accelerators outlined in Subsection [2.1.2.](#page-4-0) For example, a CGRA [\[9\]](#page-19-0)
    leverages the four 32 bit master ports that are used to independently read and
    write data to and from the main memory.


    3.2.2 Interrupt ports. A configurable number of interrupt lines can be used by
    the custom hardware to rapidly synchronize with the host CPU. Each line is connected
    to the X-HEEP PLIC interrupt controller, which can be controlled via software.
    This functionality allows the host CPU to enter a sleep state during active accelerator
    periods, significantly reducing the overall energy consumption of the running
    application.


    3.2.3 Power control ports. To provide low-power strategy capabilities to custom
    accelerators, a configurable number of power control interfaces is provided. Each
    interface is connected to the X-HEEP power manager to implement different power-saving
    strategies. Each interface includes control signals for power-gating, clock-gating,
    and RAM retention.


    #### 3.3 Tools and software


    We present the tools and software provided by X-HEEP to configure, program, and
    implement user designs.


    3.3.1 Configuration. X-HEEP is configured through SystemVerilog templates, which
    function as a dynamic tool that enables users to automatically customize the RTL
    code of the platform thanks to customizable parameters. This makes the generated
    code readable and easy to maintain and debug.


    3.3.2 Software. X-HEEP includes a HAL to access peripheral functionalities and
    supports FreeRTOS for improved development and efficient resource management.


    3.3.3 Simulation and implementation. X-HEEP offers support for simulation and
    implementation, in FPGA and silicon, based on the FuseSoc build system [\[11\]](#page-19-14).
    FuseSoC supports several EDA tools, such as Verilator, Questasim, Design Compiler,
    Genus, and Vivado, and automatically generates the scripts required to simulate
    or implement user designs. Thanks to it, the user can explore the design both
    at a high level by integrating accelerators described in SystemC, at the RTL level,
    and FPGA, for early prototyping and exploration, as well as in silicon, for final
    validation.


    #### 4 HEEPOCRATES


    In this section, we present an integration example to demonstrate the real-world
    applicability of X-HEEP. This integration effort results in HEEPocrates, a heterogeneous
    architecture designed for ultra-low-power healthcare applications. These applications
    typically involve extended data acquisition periods during which data from external
    biosensors are stored in memory, followed by intensive processing periods to classify
    such data. Therefore, we exploited the XAIF interface to extend X-HEEP with a
    CGRA accelerator [\[9\]](#page-19-0) and an IMC accelerator [\[31\]](#page-20-3),
    both of which have been shown to efficiently reduce the overall energy consumption
    of healthcare applications [\[7,](#page-19-1) [24\]](#page-20-4). Moreover, each
    accelerator is located in a separate power domain that can be individually switched
    on and off for fine-grained power control.


    ### 4.1 Architecture


    Figure [3](#page-12-0) shows the HEEPocrates architecture highlighting how the
    CGRA and IMC accelerators are integrated to minimize power and maximize bandwidth.


    4.1.1 X-HEEP configuration. We configured the X-HEEP host platform with (1) the
    CV32E20 core, which is optimal for running control tasks and offloading performance-intensive
    computations to the external accelerators while preserving low power consumption;
    (2) 8 SRAM banks of 32 KiB in contiguous addressing mode to accommodate variable
    lengths of data acquisitions while power-gating the unused banks on different
    applications; (3) a fully connected bus to provide high-bandwidth capabilities
    to the integrated accelerators; (4) all the available peripherals in place to
    deliver high flexibility; (5) a CGRA and IMC accelerators connected to the external
    XAIF interface.


    4.1.2 CGRA accelerator [\[9\]](#page-19-0). This accelerator offers two slave
    ports, one to access the internal configuration registers, and one for the context
    memory, plus four master ports used to load and store data. The context memory
    stores the kernel''s code executed by the four internal processing elements (PEs).
    Each PE is connected to a dedicated master port to read and write data from/to
    the X-HEEP main memory, independently. This allows a maximum bandwidth of 128
    bit per bus cycle. To synchronize the CGRA and the CPU, the CGRA end-of-computation
    event is connected to the X-HEEP interrupt controller (PLIC) via the XAIF interface.


    The CGRA is divided into two power domains: one for the control logic and the
    datapaths; and one for the context memory. The control logic and datapaths can
    be clock-gated or power-gated, while the context memory can be clock-gated, power-gated,
    or set in retention mode. This dual power domain structure enables clock-gating
    individual domains during short periods of inactivity and power-gating during
    extended non-use periods. Additionally, it offers the flexibility of independently
    setting the context memory in retention mode while clock-gating or power-gating
    the datapaths and control logic to save CGRA configuration time. The XAIF interface
    provides control over the various power modes, enabling the system to dynamically
    adjust its power consumption based on the operational requirements of the CGRA
    accelerator.


    <span id="page-12-0"></span>![](_page_12_Figure_1.jpeg)


    Figure 3. HEEPocrates architecture. Power domains are visually marked using different
    colors. The components highlighted in grey are always on. The accelerator integration
    is highlighted in red.


    4.1.3 IMC accelerator [\[31\]](#page-20-3). This accelerator offers one slave
    port to access its memory array. An internal controller decodes the memory requests
    and facilitates the transition of the accelerator between two modes: memory mode
    and computation mode. In memory mode, the memory space functions as a conventional
    memory bank. In contrast, the computation mode enables the execution of in-memory
    computations, eliminating the need for additional data transfers between the main
    memory and the accelerator.


    As for the CGRA, the IMC accelerator is placed in a separate power domain to save
    power when not used.


    4.1.4 Frequency-locked loop [\[2\]](#page-19-15). We utilized the XAIF interface
    to connect the frequency-locked loop (FLL) responsible for generating the system
    clock from a 32 kHz external source. For real-time configurability, the FLL exposes
    a set of memory-mapped registers that enable the host CPU to adjust the system
    clock frequency during application execution, dynamically. This feature is precious
    during extended data acquisition periods in healthcare applications because it
    allows for reducing the system frequency to the minimum value required for acquiring
    the necessary biosignals, thereby minimizing dynamic power consumption. Lastly,
    the FLL can be also bypassed, allowing the external source to serve as the system
    clock.


    ## 4.2 FPGA implementation


    We implemented HEEPocrates in FPGAs on the Zynq 7020, Zynq UltraScale+, and Artix
    7 chips by Xilinx for early prototyping. This allows for the exploration of different
    X-HEEP configurations and accelerators to optimally tune the architecture for
    the healthcare domain.


    #### 4.3 Silicon implementation


    After FPGA prototyping and exploration, we implemented HEEPocrates in silicon
    with TSMC 65 nm low-power CMOS technology. Figure [4](#page-13-0) shows the 6
    mm<sup>2</sup> layout of HEEPocrates, with the power domains shown in different
    colors.


    For conducting our measurements, we developed a board specifically designed to
    accommodate our chip. HEEPocrates has been tested from 0.8 V to 1.2 V, achieving
    a maximum frequency of 170 MHz and 470 MHz, respectively. Power Manuscript submitted
    to ACM


    ## 14 Machetti et al.


    <span id="page-13-0"></span>![](_page_13_Figure_1.jpeg)


    Figure 4. HEEPocrates layout, silicon photo, and physical chip (on a Swiss 5-cent
    franc coin).


    consumption ranges from 270 µW at 32 kHz and 0.8 V, to 48 mW at 470 MHz and 1.2
    V. Each phase of healthcare applications has been optimized to minimize power
    consumption.


    4.3.1 Acquisition phase. Healthcare applications commonly feature an extended
    acquisition phase due to the lowbandwidth nature of biosignals and the typical
    lengthy data windows. During this phase, samples are gathered from external ADCs
    via SPI, or other I/O peripherals, and stored in memory by the main CPU or the
    DMA. We run this phase at 1 MHz, 0.8 V to minimize power while offering enough
    performance for the acquisition of bio-signals in the order of hundreds of Hertz.
    HEEPocrates consumes 384 µW during acquisition when the complete system is active,
    and the host CPU is clock-gated when not used. However, power can be further optimized
    by switching off the unused memory banks, the peripheral domain, and the external
    accelerators for the entire acquisition period. This enables a reduction in power
    of 19 %, which leads to 310 µW. Furthermore, the CPU can be turned off during
    idle periods, i.e., when not used actively to acquire ADC samples, reaching the
    lowest power level of the system at 1 MHz of 286 µW, with a further reduction
    of 8 %.


    4.3.2 Processing phase. Upon completion of the acquisition phase, we run the processing
    phase at the maximum speed of 170 MHz, 0.8 V to minimize processing time and race
    to sleep. HEEPocrates consumes 8.17 mW during the processing phase when the complete
    system is active and the CPU executes a matrix multiplication. Power can be further
    optimized 6 % by turning off the unused memory banks, the peripheral domain, and
    the external accelerators, with a consumption of about 7.68 mW. During the processing
    phase, the external accelerators can be individually powered on, and computationally
    intensive tasks can be offloaded by the main CPU to reduce the system''s overall
    energy consumption. HEEPocrates consumes 4.01 mW and 1.65 mW when CNN algorithms
    are executed on the CGRA accelerator and IMC accelerator, respectively, at their
    maximum frequency of 60 MHz. The host CPU, the unused memory banks, and the peripheral
    domain are powered off during accelerator activity.


    #### 5 EXPERIMENTAL SETUP


    This section introduces a representative set of different families of microcontrollers
    commonly used in healthcare applications. Subsequently, it describes the biomedical
    applications that are included in our benchmark.


    #### 5.1 Healthcare microcontrollers


    Healthcare applications exhibit significant variability in acquisition and processing
    times, influenced by factors such as the length of sampling windows and the complexity
    of adopted algorithms. To address this variability, a diverse range of microcontrollers
    have been designed, each optimized to minimize power consumption during specific
    phases. The Apollo 3 Blue excels in acquisition phases, prioritizing power efficiency
    through its deep sleep mode, which ensures remarkably low power consumption when
    the system is inactive during idle periods. On the other hand, GAP9 takes the
    lead in processing phases thanks to its higher-performance core, which guarantees
    substantial reductions in processing time. The analysis of these two microcontrollers
    enables covering the entire spectrum of ultra-low-power edge devices, ranging
    from top-tier power efficiency, with Apollo 3 Blue, to top-tier performance, with
    GAP9. Furthermore, the frequent use of both microcontrollers in this domain demonstrates
    their capability to meet the rigorous demands of healthcare applications in terms
    of performance, power, and area. Table [1](#page-15-0) reports the features of
    the selected microcontrollers.


    5.1.1 Apollo 3 Blue. This MCU is part of the Ambiq board and features an ARM Cortex-M4
    core. The code is stored in the on-chip flash memory with zero overhead in instruction
    fetching, while the rest of the data resides either entirely in the SRAM when
    it fits or in both the SRAM and the flash. Unnecessary SRAM banks are turned off
    for the entire duration of the application. Its optimal processing configuration
    is 0.7 V, 48 MHz. However, we exploited the TurboSPOT mode to increase the frequency
    to 96 MHz when required to meet the timing constraints of the benchmark applications.
    Moreover, during idle periods, the system enters its deep sleep mode, consuming
    approximately 6 µA/MHz, where most of the system components are power-gated, with
    only a few power control modules active.


    5.1.2 GAP9. This MCU is part of the GAP9EVK board and features one CV32E40P core,
    known as the fabric controller (FC), and a cluster (CL) with nine CV32E40P cores,
    which can be switched on and off. We execute the benchmark applications exclusively
    on the FC while power-gating the CL and unnecessary SRAM banks for the entire
    duration of the application. The application code and data are stored in the SRAM
    for maximum performance. Its optimal processing configuration is 0.65 V, 240 MHz.
    Furthermore, during idle periods, the system transitions into its sleep mode,
    where the majority of components are power-gated, except for memory banks, which
    enter a retention mode.


    5.1.3 HEEPocrates. The application code and data are completely stored in the
    SRAM, when possible, or in a combination of the SRAM and the off-chip flash, connected
    through the SPI interface. The peripheral domain and the unused memory banks are
    also powered off throughout the entire duration of the application. We execute
    all the benchmark applications on the host CPU while power-gating the external
    accelerators. Moreover, we also accelerate CNN computations on the CGRA and IMC
    accelerators and showcase the energy improvement compared to running on the host
    CPU. We performed each measurement under the optimal operating conditions: 170
    MHz at 0.8 V, for the host CPU; 60 MHz at 0.8 V, for the CGRA and IMC accelerators.
    During idle periods, the host CPU and the external accelerators are power-gated,
    and the system frequency is lowered to 1 MHz to reduce power consumption.


    <span id="page-15-0"></span>


    | MCU           | Board         | Processing element | Voltage | Maximum frequency
    |

    |---------------|---------------|--------------------|---------|-------------------|

    | Apollo 3 Blue | Ambiq         | Cortex-M4          | 0.7 V   | 48 MHz            |

    | GAP9          | Gapuino       | CV32E40P           | 0.65 V  | 240 MHz           |

    | HEEPocrates   | Testing board | CV32E20            | 0.8 V   | 170 MHz           |


    Table 1. Microcontrollers commonly adopted in healthcare applications.


    Table 2. Healthcare applications included in our benchmark.


    <span id="page-15-1"></span>


    | Application           | Acquisition window | Input leads | Sampling rate | Bits
    per sample |

    |-----------------------|--------------------|-------------|---------------|-----------------|

    | Heartbeat classifier  | 15 s               | 3           | 256 Hz        | 16              |

    | Seizure detection CNN | 4 s                | 23          | 256 Hz        | 16              |


    #### 5.2 Healthcare applications


    Table [2](#page-15-1) reports the healthcare applications selected for our benchmark.
    Our selection ensures that we cover the full spectrum of ultra-low-power healthcare
    applications, ranging from acquisition-dominated, with the heartbeat classifier,
    to processing-dominated, with the seizure detection CNN. Moreover, these applications
    showcase computational algorithms of varying complexity, thereby enhancing the
    comprehensiveness of our analysis.


    5.2.1 Heartbeat classifier [\[4\]](#page-19-16). This application is used to detect
    irregular beat patterns for common heart diseases through the analysis of electrocardiogram
    (ECG) signals. The most resource-intensive part of this application lies in the
    initial computation phase, specifically the morphological filtering, which consumes
    over 80 % of the total processing time. Subsequently, the classification stage
    employs random projections. Initially, the algorithm processes a single input
    channel. If an abnormal heartbeat is detected, the analysis extends to the other
    leads for a more precise determination. Our testing scenarios involve input signals
    that all contain abnormal beats to evaluate the complete application pipeline.
    The input signal is derived from three distinct ECG leads, each sampled at 256
    Hz with an accuracy of 16 bit. A 15 s acquisition window produces an input signal
    of 22.5 KiB.


    5.2.2 Seizure detection CNN [\[13\]](#page-19-7). This application is used to
    detect seizures in electroencephalography (EEG) signals. It features a CNN with
    three one-dimensional convolutional layers, each incorporating pooling and ReLU
    layers. 90 % of the processing time is spent in convolutional computations, which
    mainly involve multiply and accumulate (MAC) and shift operations. Following each
    convolution, there is an overflow check and a maximum test for the pooling layer.
    Two fully connected layers end the network. The signal is sampled from 23 leads
    at a rate of 256 Hz with 16 bit accuracy and the acquisition phase lasts 4 s,
    resulting in an input signal size of 46 KiB.


    #### 6 EXPERIMENTAL RESULTS


    In this section, first, we analyze the energy consumption of the proposed host
    platform, HEEPocrates (with the accelerators power-gated), in comparison with
    the selected state-of-the-art microcontrollers that may serve as host platforms.
    Subsequently, we assess the energy efficiency gained from leveraging the HEEPocrates''
    accelerators in comparison to execution on the host CPU.


    <span id="page-16-0"></span>![](_page_16_Figure_1.jpeg)


    Figure 5. Energy consumption of our benchmark running on common healthcare microcontrollers
    and on HEEPocrates at 0.8 V.


    #### 6.1 Host platforms


    Figure [5](#page-16-0) illustrates the measured energy values for each healthcare
    application from our benchmark.


    The heartbeat classifier application exhibits an acquisition-driven nature, characterized
    by extended acquisition windows and a low sampling rate of 256 Hz. This forces
    microcontrollers to spend a significant amount of time in idle states during acquisition.
    In particular, the Apollo 3 Blue stands out for its energy efficiency, attributed
    to its remarkably low sleep mode of only 6 µA/MHz, where most of the system is
    power-gated, with only a few control modules active. On the contrary, GAP9 lacks
    aggressive sleep modes and keeps more modules always on, resulting in considerably
    higher energy consumption. Even during the processing phase, Apollo 3 Blue maintains
    a slight energy advantage over GAP9. This can be attributed to the optimized design
    of its CPU, the ARM Cortex-M4, which is more efficient for the specific operations
    required by this application, including logical and comparison operations, branches,
    as well as load and store instructions [\[4\]](#page-19-16).


    HEEPocrates positions itself in a middle ground during acquisition, offering a
    more robust sleep mode compared to GAP9. However, it does not reach the exceptionally
    low power consumption levels of Apollo 3 Blue due to the absence of aggressive
    sleep strategies for faster wake-up times, which includes in the always-on IPs
    more peripherals as an FLL, a pad controller, bus, a debug unit, and more general-purpose
    peripherals added for enhanced versatility (e.g. SPI, UART, etc.). However, HEEPocrates''
    energy efficiency can be improved by removing the general-purpose peripherals,
    resulting in a 27 % reduction in overall energy consumption. During processing,
    HEEPocrates consumes slightly higher energy compared to the other microcontrollers,
    due to its ultra-low-power CV32E20 core [\[29\]](#page-20-2) that is not optimized
    for performance like GAP9, and due to the higher-power consumption of the active
    part of the chip compared to Apollo-3, sitting HEEPocrates in the middle between
    the two.


    The seizure detection CNN application is processing-dominated due to its computationally
    intense convolutional network, leading microcontrollers to spend the majority
    of their time in the processing phase. GAP9 emerges as the dominant contender
    in this phase, leveraging its high-performance core to achieve reduced processing
    times and efficient transitions to sleep. In contrast, the core of Apollo 3 Blue
    lacks sufficient computational power, resulting in an extended processing phase
    and increased energy consumption. However, during the acquisition phase, Apollo
    3 Blue maintains dominance over GAP9 due to its more efficient sleep mode, resulting
    in lower energy consumption.


    ## <span id="page-17-0"></span>18 Machetti et al.


    ![](_page_17_Figure_1.jpeg)


    Figure 6. Energy consumption of HEEPocrates at 0.8 V running a 16x16 convolution
    (3x3 filter) on the host CPU (at 170 MHz) and the CGRA and IMC accelerators (at
    60 MHz).


    HEEPocrates finds itself positioned between Apollo 3 Blue and GAP9 in both the
    processing and acquisition phases. During acquisition, it offers a more efficient
    low-power mode than GAP9 but does not reach the efficiency levels of Apollo 3
    Blue, for the reasons explained earlier. In the processing phase, the higher performance
    of HEEPocrates allows for faster entry into the sleep state than Apollo 3 Blue
    but lags behind GAP9 due to its higher-frequency core. Notably, similar to the
    previous application, HEEPocrates'' energy efficiency may be enhanced by removing
    general-purpose peripherals from the always-on domain, resulting in an overall
    energy reduction of about 3 %.


    In conclusion, our analysis reveals the energy consumption alignment of HEEPocrates
    with state-of-the-art microcontrollers commonly adopted in healthcare applications.
    The performance and power efficiency of our platform falls between the top-tier
    power efficiency of Apollo 3 Blue and the top-tier performance of GAP9. This underscores
    that HEEPocrates achieves state-of-the-art energy efficiency figures across a
    wide range of real-world application profiles typical of the healthcare domain,
    ranging from acquisition-dominated to processing-dominated scenarios.


    #### 6.2 Accelerators


    In Figure [6,](#page-17-0) we compare the energy consumption of HEEPocrates while
    running a 16×16 convolutional layer with a 3×3 filter on the host CPU, the CGRA
    and IMC accelerators. Our results demonstrate an improvement in energy efficiency
    of approximately 4.9 × and 4.8 × achieved by exploiting the integrated CGRA accelerator
    and the IMC accelerator, respectively, compared to running on the host CPU. This
    improvement is attributed to the higher parallelism of the proposed accelerators,
    which compensates for the increased power consumption resulting from the more
    intense computation.


    #### 7 CONCLUSIONS


    In this paper, we have explored the growth and increasing demand for efficient
    processing solutions in the field of edge computing, particularly in the context
    of new AI/ML applications. Persistent challenges arise from the limitations in
    performance and power consumption of edge devices, which impact overall energy
    efficiency.


    To address these challenges, heterogeneous architectures have emerged, presenting
    a promising solution by combining ultra-low-power host processors with specialized
    accelerators tailored to specific applications or domains.


    However, we have shown the limitations of existing host platforms in exploring
    the design space of acceleratorbased ultra-low power edge AI platforms, as well
    as in providing the configurability and extendability options needed Manuscript
    submitted to ACM


    to integrate the large variety of custom accelerators and interfaces that exist
    nowadays. Consequently, extensive modifications to the RTL code are often required
    to integrate accelerators effectively, leading to high maintenance costs.


    To overcome these limitations, we introduced X-HEEP, an open-source solution designed
    specifically to support the integration and exploration of ultra-low-power edge
    AI/ML accelerators. The platform offers comprehensive customizability and extendability
    options via the proposed XAIF, which gathers all the requirements of state-of-the-art
    domain-specific solutions, as memory-mapped accelerators, including memory, processors,
    and peripherals with DMAsupport, custom ISA co-processor, interrupts, and power
    saving strategies interface, enabling designers to tailor the platform to meet
    the unique requirements of the target applications in performance, power, and
    area.


    X-HEEP provides configuration options to match specific application requirements
    by exploring various core types, bus topologies, and memory addressing modes.
    It also enables a fine-grained configuration of memory banks to match the constraints
    of the integrated accelerators. The platform prioritizes energy efficiency by
    implementing low-power strategies and integrating them with accelerators through
    dedicated power control interfaces. This cohesive integration ensures that all
    system components work together to maximize energy savings.


    To illustrate the practical benefits of X-HEEP, in this work, we presented a real-world
    integration example tailored for healthcare applications, which shows high variability
    among acquisition and processing-dominated application profiles. This example
    featured a CGRA accelerator and an IMC accelerator, both of which have proved
    to effectively reduce the overall energy consumption for this application domain.
    The resulting design, called HEEPocrates, has been implemented both in FPGAs on
    the Zynq 7020, Zynq UltraScale+, and Artix 7 chips by Xilinx, for early prototyping
    and exploration, and in silicon with TSMC 65 nm low-power CMOS technology, for
    silicon validation. The fabricated chip can operate from 0.8 V to 1.2 V, achieving
    a maximum frequency of 170 MHz and 470 MHz, respectively. Its power consumption
    ranges from 270 µW at 32 kHz and 0.8 V, to 48 mW at 470 MHz and 1.2 V.


    To measure the performance and versatility of the proposed design, we analyze
    the execution of an illustrative real-life set of edge AI/ML benchmarks that combines
    ultra-low power healthcare applications from the latest advances in the field,
    showing high variability in the execution profile. Through the execution of our
    benchmark and the measurement of the energy consumption of the chip, we demonstrated
    HEEPocrates'' alignment with other stateof-the-art microcontrollers that are frequently
    employed in healthcare applications. This is achieved thanks to a balanced trade-off
    between fine-grain power domains, to reduce power consumption during acquisition
    phases, and on-demand accelerator capabilities, to speed up the execution of processing
    phases, resulting in a good trade-off between acquisition-dominated and processing-dominated
    applications. These results also showcase the representativeness of the experiments
    that other researchers could perform after integrating their accelerators with
    X-HEEP. Lastly, we proved the energy benefit of 4.9 × and 4.8 × gained by exploiting
    the integrated CGRA accelerator and IMC accelerator, respectively, compared to
    running on the host CPU.


    In conclusion, the introduction of the X-HEEP platform leads to a significant
    step forward in overcoming the challenges faced in the field of edge computing.
    By providing extensive options for customizability and extendability, prioritizing
    energy efficiency, and presenting a practical real-world integration example,
    X-HEEP presents itself as an innovative platform, empowering designers and researchers
    to create efficient heterogeneous edge AI/ML computing systems.


    #### 8 ACKNOWLEDGEMENTS


    We would like to thank the entire X-HEEP team for their great contribution to
    the platform.


    #### REFERENCES


    - <span id="page-19-12"></span>[1] Alon Amid et al. "Chipyard: Integrated Design,
    Simulation, and Implementation Framework for Custom SoCs". In: IEEE Micro 40.4
    (2020), pp. 10–21. doi: [10.1109/MM.2020.2996616.](https://doi.org/10.1109/MM.2020.2996616)

    - <span id="page-19-15"></span>[2] David E Bellasi and Luca Benini. "Smart energy-efficient
    clock synthesizer for duty-cycled sensor socs in 65 nm/28nm cmos". In: IEEE Transactions
    on Circuits and Systems I: Regular Papers 64.9 (2017), pp. 2322–2333.

    - <span id="page-19-8"></span>[3] Andrea Bocco, Yves Durand, and Florent De Dinechin.
    "SMURF: Scalar Multiple-Precision Unum Risc-V Floating-Point Accelerator for Scientific
    Computing". In: Proc. of the ACM Conference for Next Generation Arithmetic. CoNGA''19.
    2019. isbn: 9781450371391. doi: [10.1145/3316279.3316280.](https://doi.org/10.1145/3316279.3316280)

    - <span id="page-19-16"></span>[4] Rubén Braojos, Giovanni Ansaloni, and David
    Atienza. "A Methodology for Embedded Classification of Heartbeats Using Random
    Projections". In: DATE. IEEE, May 2013, pp. 899–904. isbn: 9781467350716. doi:
    [10.7873/DATE.](https://doi.org/10.7873/DATE.2013.189) [2013.189.](https://doi.org/10.7873/DATE.2013.189)

    - <span id="page-19-5"></span>[5] Francesco Conti et al. "A 12.4TOPS/W @ 136GOPS
    AI-IoT System-on-Chip with 16 RISC-V, 2-to-8b Precision-Scalable DNN Acceleration
    and 30%-Boost Adaptive Body Biasing". In: IEEE ISSCC. 2023, pp. 21–23. doi: [10.1109/](https://doi.org/10.1109/ISSCC42615.2023.10067643)
    [ISSCC42615.2023.10067643.](https://doi.org/10.1109/ISSCC42615.2023.10067643)

    - <span id="page-19-10"></span>[6] CORE-V X-Interface. url: [https://github.com/openhwgroup/core-v-xif.](https://github.com/openhwgroup/core-v-xif)

    - <span id="page-19-1"></span>[7] Elisabetta De Giovanni et al. "Modular Design
    and Optimization of Biomedical Applications for Ultralow Power Heterogeneous Platforms".
    In: IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems
    39.11 (2020), pp. 3821–3832. doi: [10.1109/TCAD.2020.3012652.](https://doi.org/10.1109/TCAD.2020.3012652)

    - <span id="page-19-2"></span>[8] Alessandra Dolmeta et al. "Implementation and
    Integration of Keccak Accelerator on RISC-V for CRYSTALS-Kyber". In: Proc. of
    the 20th ACM Int. Conf. on Computing Frontiers. CF ''23. Bologna, Italy, 2023,
    pp. 381–382. doi: [10.1145/3587135.3591432.](https://doi.org/10.1145/3587135.3591432)

    - <span id="page-19-0"></span>[9] Loris Duch et al. "A multi-core reconfigurable
    architecture for ultra-low power bio-signal analysis". In: IEEE BioCAS. 2016,
    pp. 416–419. doi: [10.1109/BioCAS.2016.7833820.](https://doi.org/10.1109/BioCAS.2016.7833820)

    - <span id="page-19-9"></span>[10] Tim Fritzmann, Georg Sigl, and Johanna Sepúlveda.
    "RISQ-V: Tightly Coupled RISC-V Accelerators for Post-Quantum Cryptography". In:
    IACR Transactions on Cryptographic Hardware and Embedded Systems 2020.4 (Aug.
    2020), pp. 239–280. doi: [10.13154/tches.v2020.i4.239-280.](https://doi.org/10.13154/tches.v2020.i4.239-280)

    - <span id="page-19-14"></span>[11] FuseSoC. url: [https://github.com/olofk/fusesoc.](https://github.com/olofk/fusesoc)

    - <span id="page-19-6"></span>[12] Michael Gautschi et al. "Near-threshold RISC-V
    core with DSP extensions for scalable IoT endpoint devices". In: IEEE Transactions
    on Very Large Scale Integration (VLSI) Systems 25.10 (2017), pp. 2700–2713.

    - <span id="page-19-7"></span>[13] Catalina Gómez et al. "Automatic seizure detection
    based on imaged-EEG signals through fully convolutional networks". In: Scientific
    reports 10.1 (2020), pp. 1–13.

    - <span id="page-19-13"></span>[14] Florent Kermarrec et al. LiteX: an open-source
    SoC builder and library based on Migen Python DSL. 2020. arXiv: 2005.02506 [\[cs.AR\]](https://arxiv.org/abs/2005.02506).

    - <span id="page-19-3"></span>[15] Maha Kooli et al. "Towards a Truly Integrated
    Vector Processing Unit for Memory-Bound Applications Based on a Cost-Competitive
    Computational SRAM Design Solution". In: J. Emerg. Technol. Comput. Syst. 18.2
    (2022). issn: 1550-4832. doi: [10.1145/3485823.](https://doi.org/10.1145/3485823)

    - <span id="page-19-4"></span>[16] Kai Li, Wei Yin, and Qiang Liu. "A Portable
    DSP Coprocessor Design Using RISC-V Packed-SIMD Instructions". In: IEEE ISCAS.
    2023, pp. 1–5. doi: [10.1109/ISCAS46773.2023.10181681.](https://doi.org/10.1109/ISCAS46773.2023.10181681)

    - <span id="page-19-11"></span>[17] LowRISC. OpenTitan. url: [https://github.com/lowRISC/opentitan.](https://github.com/lowRISC/opentitan)


    - <span id="page-20-9"></span><span id="page-20-0"></span>[18] David Mallasén,
    Alberto A. del Barrio, and Manuel Prieto-Matias. Big-PERCIVAL: Exploring the Native
    Use of 64-Bit Posit Arithmetic in Scientific Computing. 2023. arXiv: [2305.06946.](https://arxiv.org/abs/2305.06946)

    - <span id="page-20-17"></span>[19] Paolo Mantovani et al. "Agile SoC Development
    with Open ESP : Invited Paper". In: 2020 IEEE/ACM International Conference On
    Computer Aided Design (ICCAD). 2020, pp. 1–9.

    - <span id="page-20-8"></span>[20] Katayoun Neshatpour et al. "Big biomedical
    image processing hardware acceleration: A case study for K-means and image filtering".
    In: IEEE ISCAS. 2016, pp. 1134–1137. doi: [10.1109/ISCAS.2016.7527445.](https://doi.org/10.1109/ISCAS.2016.7527445)

    - <span id="page-20-18"></span>[21] Open Bus Interface Protocol. url: [https://github.com/openhwgroup/obi.](https://github.com/openhwgroup/obi)

    - <span id="page-20-13"></span>[22] Alessandro Ottaviano et al. "Cheshire: A Lightweight,
    Linux-Capable RISC-V Host Platform for Domain-Specific Accelerator Plug-In". In:
    IEEE Transactions on Circuits and Systems II: Express Briefs (2023), pp. 1–1.
    doi: [10.1109/](https://doi.org/10.1109/TCSII.2023.3289186) [TCSII.2023.3289186.](https://doi.org/10.1109/TCSII.2023.3289186)

    - <span id="page-20-15"></span>[23] Daniel Petrisko et al. "BlackParrot: An Agile
    Open-Source RISC-V Multicore for Accelerator SoCs". In: IEEE Micro 40.4 (2020),
    pp. 93–102. doi: [10.1109/MM.2020.2996145.](https://doi.org/10.1109/MM.2020.2996145)

    - <span id="page-20-4"></span>[24] Flavio Ponzina et al. "A Hardware/Software
    Co-Design Vision for Deep Learning at the Edge". In: IEEE Micro 42.6 (July 2022),
    pp. 48–54. doi: [10.1109/MM.2022.3195617.](https://doi.org/10.1109/MM.2022.3195617)

    - <span id="page-20-5"></span>[25] Antonio Pullini et al. "Mr. Wolf: An energy-precision
    scalable parallel ultra low power SoC for IoT edge processing". In: IEEE Journal
    of Solid-State Circuits 54.7 (2019), pp. 1970–1981.

    - <span id="page-20-16"></span>[26] Rocket. url: [https://github.com/chipsalliance/rocket-chip.](https://github.com/chipsalliance/rocket-chip)

    - <span id="page-20-11"></span>[27] Pasquale Davide Schiavone et al. "Arnold:
    An eFPGA-augmented RISC-V SoC for flexible and low-power IoT end nodes". In: IEEE
    Transactions on Very Large Scale Integration (VLSI) Systems 29.4 (2021), pp. 677–690.

    - <span id="page-20-12"></span>[28] Pasquale Davide Schiavone et al. "Quentin:
    an Ultra-Low-Power PULPissimo SoC in 22nm FDX". In: (2018), pp. 1–3. doi: [10.1109/S3S.2018.8640145.](https://doi.org/10.1109/S3S.2018.8640145)

    - <span id="page-20-2"></span>[29] Pasquale Davide Schiavone et al. "Slow and
    steady wins the race? A comparison of ultra-low-power RISC-V cores for Internet-of-Things
    applications". In: Int. Symp. on Power and Timing Modeling, Optimization and Simulation
    (PATMOS). IEEE. 2017, pp. 1–8.

    - <span id="page-20-1"></span>[30] Pasquale Davide Schiavone et al. "X-HEEP: An
    Open-Source, Configurable and Extendible RISC-V Microcontroller". In: Proc. of
    Int. Conf. on Computing Frontiers. CF ''23. New York, NY, USA: ACM, 2023, pp.
    379–380. isbn: 9798400701405. doi: [10.1145/3587135.3591431.](https://doi.org/10.1145/3587135.3591431)

    - <span id="page-20-3"></span>[31] William Andrew Simon et al. "BLADE: An in-cache
    computing architecture for edge devices". In: IEEE Transactions on Computers 69.9
    (2020), pp. 1349–1363.

    - <span id="page-20-7"></span>[32] Mattia Sinigaglia et al. Echoes: a 200 GOPS/W
    Frequency Domain SoC with FFT Processor and I2S DSP for Flexible Data Acquisition
    from Microphone Arrays. 2023. arXiv: [2305.07325.](https://arxiv.org/abs/2305.07325)

    - <span id="page-20-6"></span>[33] Blaise Tine et al. "Vortex: Extending the RISC-V
    ISA for GPGPU and 3D-Graphics". In: IEEE/ACM Int. Symp. on Microarchitecture (MICRO).
    2021, pp. 754–766. isbn: 9781450385572. doi: [10.1145/3466752.3480128.](https://doi.org/10.1145/3466752.3480128)

    - <span id="page-20-10"></span>[34] Y. Varma and M.P. Tull. "Architectural design
    of a complex arithmetic signal processor (CASP)". In: Region 5 Conference: Annual
    Technical and Leadership Workshop. 2004, pp. 69–76. doi: [10.1109/REG5.2004.1300163.](https://doi.org/10.1109/REG5.2004.1300163)

    - <span id="page-20-14"></span>[35] Florian Zaruba and Luca Benini. "The cost
    of application-class processing: Energy and performance analysis of a Linux-ready
    1.7-GHz 64-bit RISC-V core in 22-nm FDSOI technology". In: IEEE Transactions on
    Very Large Scale Integration (VLSI) Systems 27.11 (2019), pp. 2629–2640.'
  decisions:
    evaluation_prompt: 'Qualified. Reason: The paper includes empirical analysis,
      such as measuring energy consumption and comparing the performance of the proposed
      design with state-of-the-art microcontrollers.'
    related_work_prompt: 'Disqualified: no related work. Reason: Lacks a section labeled
      “Related Work” or equivalent.'
- title: Floating Point HUB Adder for RISC-V Sargantana Processor
  abstract: 'HUB format is an emerging technique to improve the hardware and time

    requirement when round to nearest is needed. On the other hand, RISC-V is an

    open-source ISA that many companies currently use in their designs. This paper

    presents a tailored floating point HUB adder implemented in the Sargantana

    RISC-V processor.'
  url: http://arxiv.org/abs/2401.09464v1
  keywords: ''
  document: '# Floating Point HUB Adder for RISC-V Sargantana Processor


    Gerardo Bandera1∗ , Javier Salamero<sup>2</sup> , Miquel Moreto2,3 and Julio Villalba<sup>1</sup>


    <sup>1</sup>Computer Architecture Dept., University of Málaga


    <sup>2</sup>BSC-CNS - Barcelona Supercomputing Center, <sup>3</sup>Computer Architecture
    Dept, UPC


    #### Abstract


    HUB format is an emerging technique to improve the hardware and time requirement
    when round to nearest is needed. On the other hand, RISC-V is a open source ISA
    that an important number of companies are using in their designs currently. In
    this paper we present a tailored floating point HUB adder that has been implemented
    in the Sargantana RISC-V processor.


    ## HUB format background


    In this section we present a basic description of the HUB format, although the
    mathematical foundations and an in-depth analysis of the format can be found in
    [\[1\]](#page-1-0). Now we briefly summarize this new real number representation
    format defined in [\[1\]](#page-1-0) and particularize it for the floating-point
    normalized numbers.


    HUB format is based on shifting the numbers that can be exactly represented under
    conventional formats by adding a bias which equals half unit in the lastplace
    (ULP). This shifting could be also interpreted as appending a hidden least significant
    bit set to one to the conventional number stream (which represents the bias).
    A floating-point HUB number is similar to a regular one but its significand follows
    the HUB format. Thus, the exponent and the sign is the same as convetional. Let
    us define x as a floating-point HUB number, which is represented by the triple
    (Sx, Mx, Ex) such that x = (−1)SxMx2 <sup>E</sup><sup>x</sup> , where the significand
    M<sup>x</sup> is a HUB magnitude. A normalized HUB significand fulfills that 1
    < M<sup>x</sup> < 2. Thus, the normalized HUB significand M<sup>x</sup> is


    <span id="page-0-0"></span>

    $$M\_x = 1 + \left[\sum\_{i=1}^f M\_{x\_i} \cdot 2^{-i}\right] + 2^{-f-1} \qquad
    (1)$$


    where 2 −f−1 is the bias. In this expression we define the representative form
    of the normalized HUB significand as the set of M<sup>x</sup><sup>i</sup> in expression
    [\(1\)](#page-0-0), that is M<sup>x</sup> = (M<sup>x</sup><sup>1</sup> , M<sup>x</sup>−<sup>1</sup>
    , M<sup>x</sup>−<sup>2</sup> , · · ·, M<sup>x</sup>−<sup>f</sup> ) (composed by
    f bits). Taking into account that both the MSB and the LSB are 1 (see expression
    [\(1\)](#page-0-0)), we define the operational form of a normalized HUB significand
    as the following f + 2 bits:


    $$M\_x = 1.M\_{x-1}M\_{x-2} \cdot \cdots \cdot M\_{x-f}1\tag{2}$$


    The representative version is used for storage whereas the operational version
    is required to operate with HUB numbers. We can see that the least significant
    bit (LSB) of the operational form of a nonzero HUB number is always equal to 1,
    and it is implicit in the format (similar situation takes place for the most significant
    bit (MSB) of the significand in the IEEE normalized floating-point numbers). Let
    ILSB denote the implicit LSB of a the operational HUB significand.


    Given a standard floating-point system with a normalized significand, the counterpart
    HUB floating point system has the same precision and accuracy [\[1\]](#page-1-0).


    The most outstanding feature of the HUB format is that round to nearest is performed
    by truncation. In the conventional format round to nearest is carried out by adding
    one to the position of the rounding bit of the final normalized result. Moreover,
    after this operation an overflow can also be produced, which involves a shift
    operation of one bit and an update of the exponent. Thus, a specific hardware
    module is used in conventional. In HUB, this module is not required any more.


    ## FP HUB adder for Sargantana


    The processor used in this work is Sargantana [\[2\]](#page-1-1), a 64 bit in-order
    Linux-capable RISC-V CPU that implements the RV64G ISA (see figure [1\)](#page-1-2).
    For accelerating domain-specific applications, Sargantana uses a Single Instruction
    Multiple Data (SIMD) unit and supports the vector instructions defined in the
    vector extension RVV 0.7.1. In addition, it implements custom application specific
    instructions. The CPU has a 7-stage pipeline that implements register renaming,
    out-oforder write-back, and a non-blocking memory pipeline. It has two first level
    caches: an instruction cache of 16KB, and a non-blocking data cache of 32KB. The
    system also has a 512KB L2 cache outside the CPU.


    Figure [2](#page-1-3) shows the main modules required to carry out the floating
    point addition for conventional and for HUB. Since round to nearest operation
    is carried


    <sup>∗</sup>Corresponding author: <mailto:gbandera@uma.es>


    <span id="page-1-2"></span>![](_page_1_Figure_0.jpeg)


    Figure 1: Architecture of the Sargantana processor


    out by truncation in HUB, the result obtained after normalization module in figure
    [2](#page-1-3) is the final result and not any other operation is required. Thus,
    the module Rounding (crossed out in the figure [2\)](#page-1-3) is not required
    for the HUB implementation in Sargantana.


    <span id="page-1-3"></span>![](_page_1_Figure_3.jpeg)


    Figure 2: Main modules in conventional and HUB FP adders (X-> prevented in HUB)


    For applications requiring conventional addition with round-to-nearest, three
    extra fractional bits are needed: a guard bit, a rounding bit and a sticky bit
    [\[3\]](#page-1-4). Since the round-to-nearest for HUB format is carried out by
    truncation, the rounding bit is not required anymore. Moreover, in conventional
    it is necessary calculate the sticky bit (this bit represents, in some way, the
    bits beyond the rounding bit and is needed for effective subtraction when the
    operands are not aligned). For HUB number, because we know that the LSB of the
    shifted operand is always 1 (that is the ILSB), the sticky bit is always 1 and
    it is not necessary a module to calculate it, as shown in figure [2.](#page-1-3)


    Thus, in spite of having an extra bit in the operational form (the ILSB), this
    extra bit of a HUB number is compensated by the lack of a specific rounding bit..
    Moreover, a guard bit is not necessary and the sticky bit, when required, is always
    1. As consequence, for applications where round to nearest is required, the


    data path of the HUB version has one bit less than that its conventional counterpart


    Our starting point is the FPU adder of the RISC-V Sargantana processor which has
    been modified to meet the HUB format specifications. Unlike the 6 stage original
    FPU adder, our floating point HUB adder has 5 stages. This feature together with
    the absence of denormals and sticky calculation leads to an area reduction of
    25% (3110µm<sup>2</sup> vs. 2332µm<sup>2</sup> ). In the first stage, the smallest
    operand is identified and the difference of exponents is calculated. In the second
    stage the significand of the smallest operand is shifted and the 2-complement
    of the smallest operand is calculated if required. The third state carries out
    the addition of the aligned operands and a possible overflow is detected and corrected.
    In the forth stage the number of leading zeros are calculated for normalization,
    which is carried out in the fifth stage. Notice that, unlike the conventional
    one, a sixth rounding stage is not necessary in HUB since the result of the fifth
    stage is already normalized and rounded to the nearest HUB number.


    In summary, we have integrated a HUB adder in the Sargantana processor, reducing
    the number of stages and allowing the use of this new format in a RISC-V processor.
    As future work, we plan to extend the HUB format to all FP RISC-V arithmetic operations.


    ## References


    - <span id="page-1-0"></span>[1] J. Hormigo and J. Villalba. "New formats for
    computing with real-numbers under round-to-nearest". In: IEEE Transactions on
    Computers 65.7 (July 2016), pp. 2158– 2168. doi: [10.1109/TC.2015.2479623](https://doi.org/10.1109/TC.2015.2479623).

    - <span id="page-1-1"></span>[2] Vıctor Soria-Pardos et al. "Sargantana: A 1 GHz+
    In-Order RISC-V Processor with SIMD Vector Extensions in 22nm FD-SOI". In: 2022
    25th Euromicro Conference on Digital System Design (DSD). IEEE. 2022, pp. 254–261.

    - <span id="page-1-4"></span>[3] M. Ercegovac and T. Lang. Digital Arithmetic.
    1st. Morgan Kaufmann, 2004.'
  decisions:
    evaluation_prompt: 'Disqualified: no evaluation. Reason: The paper lacks experimental,
      empirical, or quantitative analysis.'
- title: A modular architecture for IMU-based data gloves
  abstract: 'The flexibility and range of motion in human hands play a crucial role
    in

    human interaction with the environment and have been studied across different

    fields. Researchers explored various technological solutions for gathering

    information from the hands. These solutions include tracking hand motion

    through cameras or wearable sensors and using wearable sensors to measure the

    position and pressure of contact points. Data gloves can collect both types of

    information by utilizing inertial measurement units, flex sensors, magnetic

    trackers for motion tracking, and force resistors or touch sensors for contact

    measurement. Although there are commercially available data gloves, researchers

    often create custom data gloves to achieve the desired flexibility and control

    over the hardware. However, the existing literature lacks standardization and

    the reuse of previously designed data gloves. As a result, many gloves with

    unclear characteristics exist, which makes replication challenging and

    negatively impacts the reproducibility of studies. This work proposes a

    modular, open hardware and software architecture for creating customized data

    gloves based on IMU technology. We also provide an architecture implementation

    along with an experimental protocol to evaluate device performance.'
  url: http://arxiv.org/abs/2401.13254v1
  keywords: ''
  document: '# A modular architecture for IMU-based data gloves


    Alessandro Carfì [,](https://orcid.org/0000-0001-9208-6910) Mohamad Alame[h](https://orcid.org/0000-0002-6345-8313)
    , Valerio Belcamino [,](https://orcid.org/0000-0002-9264-8191) and Fulvio Mastrogiovann[i](https://orcid.org/0000-0001-5913-1898)


    Department of Informatics, Bioengineering, Robotics, and Systems Engineering,
    University of Genoa, Via Opera Pia 13, 16145 Genoa, Italy alessandro.carfi@dibris.unige.it


    Abstract. The flexibility and range of motion in human hands play a crucial role
    in human interaction with the environment and have been studied across different
    fields. Researchers explored various technological solutions for gathering information
    from the hands. These solutions include tracking hand motion through cameras or
    wearable sensors and using wearable sensors to measure the position and pressure
    of contact points. Data gloves can collect both types of information by utilizing
    inertial measurement units, flex sensors, magnetic trackers for motion tracking,
    and force resistors or touch sensors for contact measurement. Although there are
    commercially available data gloves, researchers often create custom data gloves
    to achieve the desired flexibility and control over the hardware. However, the
    existing literature lacks standardization and the reuse of previously designed
    data gloves. As a result, many gloves with unclear characteristics exist, which
    makes replication challenging and negatively impacts the reproducibility of studies.
    This work proposes a modular, open hardware and software architecture for creating
    customized data gloves based on IMU technology. We also provide an architecture
    implementation along with an experimental protocol to evaluate device performance.


    Keywords: Data Glove · Hand Tracking · Inertial Measuerment Unit.


    ## 1 Introduction


    Human hands'' flexibility and range of motion are fundamental for how humans interact
    with each other and the world. Many research communities have studied human hand
    motion, focusing on the hand''s motion and its interaction with the environment
    [\[1\]](#page-4-0). Hand studies typically include information about hand kinematics
    and sensory input. Hand kinematics are well-described, with minimal variation
    among individuals, except for bone proportions [\[3\]](#page-4-1). On the other
    hand, humans primarily rely on two senses when using their hands: proprioception
    and touch. Proprioception helps determine limb position, while touch provides
    information about forces and points of contact. Depending on the type of study,
    researchers may require information from one or both of the hand senses. As


    #### 2 A. Carfì et al.


    ![](_page_1_Figure_1.jpeg)


    <span id="page-1-0"></span>Fig. 1. The image compares the architecture on the
    left with the implementation on the right. The core module is in blue, while the
    sensory module is in red.


    a result, various technological solutions have been explored. In terms of proprioception,
    tracking the hand''s motion has been achieved through the use of cameras or wearable
    sensors. Instead, for touch, although a few attempts have been made to estimate
    them using cameras [\[5\]](#page-4-2), the position and pressure of contacts are
    primarily measured using wearable sensors. The data glove is a wearable device
    embedding sensors that can collect all the previously defined information [\[4\]](#page-4-3).
    The sensors embedded in a data glove can be adapted to meet the application requirements.
    Inertial Measurement Units (IMU) [\[7\]](#page-4-4), Flex sensors [\[8\]](#page-4-5),
    and magnetic trackers [\[2\]](#page-4-6) are the most common choices for tracking
    motion, while force sensing resistors [\[10\]](#page-4-7) or touch sensors can
    be used to measure contacts [\[9\]](#page-4-8). While various data gloves are
    commercially available [\[6\]](#page-4-9), researchers often opt to construct
    custom data gloves to achieve the desired flexibility and control over the hardware.
    However, the existing literature lacks standardization and the re-use of previously
    designed devices. As a result, many gloves with unclear characteristics have been
    developed, making replication challenging and affecting the reproducibility of
    the studies for which they were created. This work aims to introduce a modular,
    open hardware and software architecture for creating customized data gloves based
    on IMU technology.


    ## 2 Hardware Architecture


    To provide maximum flexibility, a data glove should support a variable IMUs number
    to adapt to the application''s requirements, work without needing external equipment,
    and be easy to repair and reproduce. Our architecture addresses these requirements
    by defining two modules, displayed in Figure [1:](#page-1-0) core and sensory
    modules. The core module includes the MCU for data collection and processing,
    an antenna to send data to a PC, interfaces for connecting other modules, a battery
    for power and an IMU. The sensory module contains an IMU and the communication
    interface with the core module. Each module has a structure that includes the
    basic functionalities of an IMU data glove. However, each component can be expanded
    to incorporate more functionalities while keeping the main structure intact. The
    only technical constraint is the communication interface, which should remain
    fixed to ensure compatibility across


    ![](_page_2_Figure_1.jpeg)


    <span id="page-2-0"></span>Fig. 2. On the left, we have the dataglove worn by
    a human, and on the right, it is mounted on the AR10 Robotic hand.


    different implementations of this architecture. The communication interface has
    two key components: the communication protocol and the physical medium. The two
    most commonly adopted communication protocols for digital sensors are Serial Peripheral
    Interface (SPI) and Inter Integrated Circuit (I2C). SPI is typically used for
    communication between components on the printed circuit board (PCB). It is more
    vulnerable to noise and requires more communication lanes. Instead, I2C allows
    the same physical lane sharing across multiple devices without additional selection
    lanes and is less affected by noise. Therefore, we have chosen I2C as the communication
    protocol for the modules in our hardware architecture. Instead, in the literature,
    solutions for the physical medium often involved soldering cables or using flexible
    PCBs. However, these options make it hard to modify the number of sensors and
    increase the complexity of reproducing and repairing the device. To solve these
    issues, we opted for using a Flexible Flat Cable (FFC) connector as the physical
    interface for the modules and FFC cables to connect them. This solution allows
    easy addition of extra sensory modules or replacement of faulty modules. Additionally,
    FFC cables are flexible and do not restrict hand motion.


    ## 3 Implementation


    The implementation of the two modules can be seen in Figure [1](#page-1-0) while
    the device is pictured in Figure [2.](#page-2-0) The sensory module design includes
    a SparkFun IMU Breakout, which embeds the MPU-9250 from InvenSense, and a custom-designed
    shield to support FFC connection and daisy chaining. Although the I2C protocol
    only requires four lanes (ground, power, SCL, and SDA), we opted for a six-lane
    FFC connector and cable for future expansion. Each I2C lane can connect only two
    sensors since the MPU-9250 only offers two selectable addresses. The two sensory
    modules of each I2C lane can be worn on the proximal and intermediate phalanges
    to monitor the motion of a single finger. Instead, the core module is


    #### 4 A. Carfì et al.


    placed on the back of the hand and consists of a custom PCB to ensure a compact
    design. This PCB includes an ESP32 with a WiFi antenna, 7 FFC connectors, an MPU-9250
    IMU, and an I2C multiplexer. Each finger requires a separate I2C lane for motion
    tracking, so we allocated one FFC connector for each lane. For symmetry and compatibility
    with both right and left hands, we included two connectors in symmetric positions
    for the thumb. Since the ESP32 has only two I2C controllers, but the system requires
    6 I2C lanes (one for each finger and one for the hand back), the design incorporates
    an I2C multiplexer to manage all the communication lanes. Finally, the core module
    also includes a connector for battery power. Our software consists of two modules.
    The first module runs on the ESP32 and collects data from connected sensors, sending
    them via UDP communication. The data includes the sensor''s unique ID, accelerometer
    and gyroscope readings, and orientations in quaternions. The ESP32 uses the I2C
    multiplexer to collect data from each sensor module. The sensor orientation is
    estimated using a data fusion process run by the MPU-9250''s digital motion processor.
    The second module runs on the PC and receives sensory data through UDP communication,
    acting as a driver.


    ## 4 Results


    The purpose of the experimental setup is to demonstrate the general functionalities
    of the data glove. For maximum reproducibility and accuracy, we installed our
    device on an AR10 hand from Active8, mounted on the Baxter manipulator from Rethink
    Robotics, see Figure [2.](#page-2-0) The first test assessed the autonomy and
    acquisition frequencies of the data glove under static conditions. Equipped with
    eleven IMUs and powered by a 220 mAh 3.7V battery, the glove had an average autonomy
    of 62.89 minutes (SD = 4.89) and transmitted data with a frequency of 21.8 Hz
    (SD = 9.47) across six independent tests. In the same static conditions, we measured
    the drifting of the sensors'' estimated orientations over time. The root mean
    square error (RMSE), averaged across all sensors, was 8.91 degrees (SD = 3.89)
    after 30 minutes. We also conducted experiments involving random movements of
    the robot''s hand and arm. Each experiment lasted 45 minutes and was repeated
    five times. The overall RMSE averaged across all sensors and trials was 9.17 degrees
    (STD = 9.30).


    ## 5 Conclusions


    This article presents a modular architecture for an IMU-based dataglove and its
    early implementation. The device, equipped with a small battery, can transmit
    data from eleven sensors at a frequency higher than 20Hz for over an hour. Furthermore,
    tests conducted under unfavourable conditions, without proper calibration or drifting
    compensation, demonstrated a reasonably accurate tracking of motions. The error
    in dynamic conditions is not significantly different from that in stationary conditions,
    as shown in the result sections. This result suggests that most tracking errors
    are due to sensor drifting, which can be compensated for with appropriate software
    solutions. The proposed device represents an initial attempt to provide an easily
    reproducible and modular platform for IMU-based hand tracking. Its extensibility
    offers opportunities for future research to propose new versions or develop more
    accurate tracking software solutions.


    Acknowledgments. This work is supported by the CHIST-ERA (2014-2020) project InDex
    and received funding from the Italian Ministry of Education and Research (MIUR).
    This work has been also made with the Italian government support under the National
    Recovery and Resilience Plan (NRRP), Mission 4, Component 2 Investment 1.5, funded
    from the European Union NextGenerationEU.


    Disclosure of Interests. The authors have no competing interests to declare that
    are relevant to the content of this article.


    ## References


    - <span id="page-4-0"></span>1. Carfì, A., Patten, T., Kuang, Y., Hammoud, A.,
    Alameh, M., Maiettini, E., Weinberg, A.I., Faria, D., Mastrogiovanni, F., Alenyà,
    G., et al.: Hand-object interaction: From human demonstrations to robot manipulation.
    Frontiers in Robotics and AI 8, 714023 (2021)

    - <span id="page-4-6"></span>2. Cazacu, E., van der Grinten, C., Bax, J., Baeten,
    G., Holtkamp, F., Lee, C.: A position sensing glove to aid ankle-foot orthosis
    diagnosis and treatment. Sensors 21(19), 6631 (2021)

    - <span id="page-4-1"></span>3. Cobos, S., Ferre, M., Uran, M.S., Ortego, J.,
    Pena, C.: Efficient human hand kinematics for manipulation tasks. In: 2008 IEEE/RSJ
    International Conference on Intelligent Robots and Systems. pp. 2246–2251. IEEE
    (10 2008)

    - <span id="page-4-3"></span>4. Dipietro, L., Sabatini, A.M., Dario, P.: A survey
    of glove-based systems and their applications. Ieee transactions on systems, man,
    and cybernetics, part c (applications and reviews) 38(4), 461–482 (2008)

    - <span id="page-4-2"></span>5. Grady, P., Tang, C., Brahmbhatt, S., Twigg, C.D.,
    Wan, C., Hays, J., Kemp, C.C.: Pressurevision: Estimating hand pressure from a
    single rgb image. In: European Conference on Computer Vision. pp. 328–345. Springer
    (2022)

    - <span id="page-4-9"></span>6. He, K., Choosri, N.: Commercial data glove selection
    for vr-based hand rehabilitation gaming project. In: 2023 Joint International
    Conference on Digital Arts, Media and Technology with ECTI Northern Section Conference
    on Electrical, Electronics, Computer and Telecommunications Engineering (ECTI
    DAMT & NCON). pp. 177–182. IEEE (2023)

    - <span id="page-4-4"></span>7. Huang, H., Liang, Z., Sun, F., Dong, M., et al.:
    Virtual interaction and manipulation control of a hexacopter through hand gesture
    recognition from a data glove. Robotica 40(12), 4375–4387 (2022)

    - <span id="page-4-5"></span>8. Luo, Y., Chen, X., Li, X., Tian, H., Li, S., Wang,
    L., He, J., Yang, Z., Shao, J.: Heterogeneous strain distribution based programmable
    gated microchannel for ultrasensitive and stable strain sensing. Advanced Materials
    p. 2207141 (2022)

    - <span id="page-4-8"></span>9. Maiolino, P., Mastrogiovanni, F., Cannata, G.,
    et al.: Skinning a robot: Design methodologies for large-scale robot skin. IEEE
    Robotics & Automation Magazine 23(4), 150–159 (2016)

    - <span id="page-4-7"></span>10. Wang, J., Li, B., Li, Z., Zubrycki, I., Granosik,
    G.: Grasping behavior of the human hand during tomato picking. Computers and Electronics
    in Agriculture 180, 105901 (2021)'
  decisions:
    evaluation_prompt: 'Disqualified: no evaluation. Reason: The paper lacks experimental,
      empirical, or quantitative analysis sections or elements.'
- title: "The Landscape of Compute-near-memory and Compute-in-memory: A Research\n\
    \  and Commercial Overview"
  abstract: 'In today''s data-centric world, where data fuels numerous application
    domains,

    with machine learning at the forefront, handling the enormous volume of data

    efficiently in terms of time and energy presents a formidable challenge.

    Conventional computing systems and accelerators are continually being pushed to

    their limits to stay competitive. In this context, computing near-memory (CNM)

    and computing-in-memory (CIM) have emerged as potentially game-changing

    paradigms. This survey introduces the basics of CNM and CIM architectures,

    including their underlying technologies and working principles. We focus

    particularly on CIM and CNM architectures that have either been prototyped or

    commercialized. While surveying the evolving CIM and CNM landscape in academia

    and industry, we discuss the potential benefits in terms of performance,

    energy, and cost, along with the challenges associated with these cutting-edge

    computing paradigms.'
  url: http://arxiv.org/abs/2401.14428v1
  keywords: ''
  document: '# **The Landscape of Compute-near-memory and Compute-in-memory: A Research
    and Commercial Overview**


    [ASIF ALI KHAN,](HTTPS://ORCID.ORG/0000-0002-5130-9855) TU Dresden, Germany


    [JOÃO PAULO C. DE LIMA,](HTTPS://ORCID.ORG/0000-0001-9295-3519) TU Dresden and
    ScaDS.AI, Germany [HAMID FARZANEH,](HTTPS://ORCID.ORG/0000-0002-1780-6217) TU
    Dresden, Germany [JERONIMO CASTRILLON,](HTTPS://ORCID.ORG/0000-0002-5007-445X)
    TU Dresden and ScaDS.AI, Germany


    In today''s data-centric world, where data fuels numerous application domains,
    with machine learning at the forefront, handling the enormous volume of data efficiently
    in terms of time and energy presents a formidable challenge. Conventional computing
    systems and accelerators are continually being pushed to their limits to stay
    competitive. In this context, computing near-memory (CNM) and computing-in-memory
    (CIM) have emerged as potentially game-changing paradigms. This survey introduces
    the basics of CNM and CIM architectures, including their underlying technologies
    and working principles. We focus particularly on CIM and CNM architectures that
    have either been prototyped or commercialized. While surveying the evolving CIM
    and CNM landscape in academia and industry, we discuss the potential benefits
    in terms of performance, energy, and cost, along with the challenges associated
    with these cutting-edge computing paradigms.


    # **1 INTRODUCTION**


    In conventional computing systems, the processor and memory are two independent
    entities connected via communication pathways, known as buses. When the CPU processes
    data, it requires fetching it from memory via the bus, conducting the necessary
    computations, and subsequently storing the results back in memory. This off-chip
    communication becomes a limiting factor for data-intensive workloads due to the
    limited transfer rate and high energy per bit of buses. For example, the data
    transfer between the logic (CPUs and GPUs) and memory chips (DRAM or flash memory)
    requires approximately 10–100 times more energy than the logic operation itself
    [\[1\]](#page-33-0). *Compute-near-memory* (CNM) and *compute-in-memory* [\(CIM\)](#page-39-0)
    concepts address this bottleneck by enabling computations close to where the data
    resides. This is achieved either by implementing CMOS logic on or closer to the
    memory chip, or by leveraging the inherent physical properties of memory devices
    to perform computations in place.


    <span id="page-0-1"></span><span id="page-0-0"></span>The core concept behind
    [CNM](#page-39-1)[/CIM](#page-39-0) is not entirely new. However, the sudden surge
    in these systems can be attributed to two primary factors, namely, the exponential
    increase in the volume of data required for modern applications, and the technological
    readiness. Recent advancements in machine learning, particularly the emergence
    of generative AI and *large language models* [\(LLM\)](#page-41-0), demand the
    processing of terabytes of data, substantial computational resources, and complex
    execution, thus highlighting the limitations of traditional computing systems.
    A recent study revealed that OpenAI utilized over 3600 of NVIDIA''s HGX A100 servers,
    totaling around 29,000 GPUs, to train ChatGPT, resulting in a daily energy consumption
    of 564 MWh [\[2\]](#page-34-0). Projections indicate that by 2027, AI is expected
    to consume between 85 and 124 TWh annually, equivalent to approximately 0.5% of
    the world''s total electricity consumption. It is no surprise that Microsoft has
    announced plans to develop its own nuclear reactors to power their data centers
    [\[3\]](#page-34-1).


    Currently, machine learning applications primarily leverage GPU accelerators like
    A100, H100, GH200, application-specific integrated circuits (e.g., Google''s TPU),
    and dataflow processors as in the case of companies like GraphCore, Cerebras,
    Groq, and SambaNova [\[4\]](#page-34-2). Over the past few


    Authors'' addresses: [Asif Ali Khan,](https://orcid.org/0000-0002-5130-9855) TU
    Dresden, Dresden, Germany, asif\_ali.khan@tu-dresden.de; [João Paulo C. de Lima,](https://orcid.org/0000-0001-9295-3519)
    TU Dresden and ScaDS.AI, Dresden, Germany, joao.lima@tu-dresden.de; [Hamid Farzaneh,](https://orcid.org/0000-0002-1780-6217)
    TU Dresden, Dresden, Germany, hamid.farzaneh@tu-dresden.de; [Jeronimo Castrillon,](https://orcid.org/0000-0002-5007-445X)
    TU Dresden and ScaDS.AI, Dresden, Germany, jeronimo.castrillon@tudresden.de.


    years, [CNM](#page-39-1)[/CIM](#page-39-0) systems have also transcended their
    prototypical stages and successfully entered the market. The timing of these advancements
    in CIM/CNM systems is of paramount importance as it perfectly aligns with the
    AI revolution. As a result, numerous companies have emerged in the last few years
    offering CIM/CNM solutions for various use domains. This surge reflects a competitive
    landscape where these companies are striving to leverage the demand and cater
    to various market segments. All commercially available solutions hold the promise
    of significantly reducing execution time and energy consumption for data-intensive
    workloads.


    This survey explores CNM and CIM architectures, detailing their technologies,
    fundamental concepts, working principles, and the evolving landscape in academia
    and industry. Addressing a broader audience, it provides foundational concepts
    for non-experts while delivering state-of-the-art insights for experts in the
    domain. It also summarizes the impact and challenges associated with adopting
    the novel CIM/CNM computing paradigms. Concretely, our discussion revolves around
    three key aspects:


    - (1) **Key technologies and concepts:** In CNM systems, a specialized CMOS logic
    is integrated into the memory chip. This logic can be either general-purpose,
    as in UPMEM systems [\[5\]](#page-34-3), or domain-specific, as in systems developed
    by Samsung [\[6,](#page-34-4) [7\]](#page-34-5) and SK Hynix [\[8,](#page-34-6)
    [9\]](#page-34-7), integrated within DRAM memory chips. While CNM significantly
    reduces data movement, it does not eliminate it. In contrast, CIM nearly eliminates
    data movement by performing computations within the same devices that store the
    data. A particularly noteworthy operation is the analog dot-product in memory,
    which is of significant importance to the machine learning domain and can be performed
    in constant time. Initially demonstrated in crossbar-configured resistive *non-volatile
    memory* [\(NVM\)](#page-40-0) technologies like *phase change memory* [\(PCM\)](#page-40-1)
    [\[10\]](#page-34-8) and *resistive RAM* [\(RRAM\)](#page-40-2) [\[11\]](#page-34-9),
    this concept has also been shown with SRAM, *magnetic RAM* [\(MRAM\)](#page-40-3)
    [\[12\]](#page-34-10), and *ferroelectric field-effect transistor* [\(FeFET\)](#page-40-4)
    [\[13\]](#page-34-11). While other arithmetic, search and boolean logic operations
    have also been demonstrated using CIM, they have received comparatively less attention.

    - <span id="page-1-5"></span><span id="page-1-4"></span><span id="page-1-3"></span><span
    id="page-1-2"></span><span id="page-1-1"></span><span id="page-1-0"></span>(2)
    **Commercial trends:** As the demand for fast and efficient computing systems
    continues to rise, the in-/near-memory computing market is experiencing rapid
    expansion. In 2022, this market was valued at USD 15.5 billion, with an anticipated
    *compound annual growth rate* [\(CAGR\)](#page-41-1) of 17.5% over the next decade
    [\[14\]](#page-34-12). This growth is underscored by the proliferation of startups
    offering CIM and CNM solutions. Some of these companies have secured hundreds
    of millions of dollars in early funding rounds. While many of these companies
    provide innovative solutions for data-intensive applications (dominated by AI
    inference), there is no clear winner yet. At present, these solutions are predominantly
    based on SRAM technology, although solutions based on resistive NVM and flash
    technologies also exist [\[15\]](#page-34-13). This trend can be attributed to
    the mature tools and design processes for SRAM compared to emerging NVMs. However,
    considering the SRAM''s scalability aspects and its static power consumption,
    it is likely that NVMs, particularly [PCM,](#page-40-1) [RRAM,](#page-40-2) [MRAM,](#page-40-3)
    and [FeFET,](#page-40-4) will progressively replace or complement SRAM as these
    technologies mature.

    - (3) **Challenges:** Although CIM/CNM systems are at the tipping point, they
    are yet to make substantial inroads into the market. The predominant obstacle
    facing these systems is perhaps the absence of a software ecosystem, which renders
    programmability and optimization exceedingly challenging. This is also highlighted
    by a recent Meta article [\[16\]](#page-34-14) stating, *We''ve investigated applying
    processing-in-memory (PIM) to our workloads and determined there are several challenges
    to using these approaches. Perhaps the biggest challenge of PIM is its programmability*.
    Other challenges requiring attention include: addressing reliability concerns
    associated with emerging NVMs (particularly in the CIM context), developing


    novel performance models, profiling and analysis tools for these systems, which
    could be leveraged to exploit their potential effectively.


    The remainder of this paper is structured as follows: Section [2](#page-2-0) explains
    the terminology associated with these domains and provides insights into the conventional
    Von-Neumann computing approach, as well as the emerging data-centric paradigms.
    In Section [3,](#page-4-0) a comprehensive overview of promising memory technologies
    within the context of [CIM](#page-39-0) and [CNM](#page-39-1) systems is provided.
    Section [4](#page-9-0) outlines various common [CIM](#page-39-0) and [CNM](#page-39-1)
    systems including very recent prototype chips from various industries. Lastly,
    Section [5](#page-25-0) presents a comprehensive overview of the commercial landscape
    for these systems (start-ups), discussing their products details, target application
    domain, and funding status. Finally, Section [6](#page-32-0) concludes the paper
    by summarizing our key observations and providing insights and recommendations
    into the future.


    # <span id="page-2-0"></span>**2 TERMINOLOGY AND BACKGROUND**


    This section highlights the bottleneck in the Von Neumann computing model by discussing
    its working mechanism, motivates the need for memory-centric computing, and explains
    the terminology.


    #### <span id="page-2-2"></span>**2.1 Mainstream Von-Neumann Computing**


    As depicted in Figure [1a](#page-2-1), the interaction between memory and the
    processor in the Von Neumann architecture is facilitated through address and data
    buses. However, because CPU performance significantly outpaces memory performance,
    the Von Neumann models are often bottlenecked by the memory. To address this challenge
    and mitigate the impact of larger memory access latencies on the CPU, modern processors
    incorporate a tiered hierarchy of caches. Caches are smaller memory units that,
    while being much smaller compared to main memory and storage, are notably faster.
    The first-level cache (L1) is typically integrated onto the CPU chip and operates
    nearly at CPU speed, enabling single-cycle access. L2 cache is usually shared
    by multiple cores and can vary in location depending on the design goals. Some
    systems even include an L3 cache, usually larger and situated off-chip.


    <span id="page-2-1"></span>![](_page_2_Figure_7.jpeg)


    Fig. 1. (a) Conventional computing system where an operation is performed on data
    in the CPU (b) Memory-centric design where is computed in the proximity of and
    CPU is mainly working as a control unit [\[17\]](#page-34-15).


    <span id="page-3-8"></span>Note that Von Neumann architectures are characterized
    by the sequential execution of instructions. However, in multi-core CPU systems,
    parallel execution of instructions at various levels of granularity, including
    instruction-level, data-level, and thread-level parallelisms, is supported. To
    enhance performance and energy efficiency in resource-constrained systems, specialized
    accelerators are often developed and integrated on the same chip. For instance,
    in application domains such as embedded systems, digital signal processing, and
    networking, *multiprocessor system-on-chip* [\(MPSoC\)](#page-40-5) architectures
    are employed. These integrate multiple processor cores, memory, input/output interfaces,
    and potentially specialized hardware accelerators, enabling parallel execution
    of different tasks to meet specific constraints.


    Although these designs may significantly enhance performance when compared to
    conventional CPU-only systems, the underlying design principle remains CPU-centric
    and follows the Von Neumann model of execution. Consequently, the performance
    improvements, largely resulting from concurrent execution, heavily rely on the
    nature of the application. In cases where an application is memory-bound, i.e.,
    most of the execution time is spent on the memory accesses and not on the actual
    compute operations, the shared data bus is fully occupied and becomes a bottleneck.
    Even for compute-bound applications, where these architectures can yield substantial
    gains in execution time, power consumption remains largely unaffected and might
    even increase due to the complex structure of these systems.


    #### **2.2 Memory-centric computing**


    Unlike conventional computing systems where CPU has a central role and is responsible
    for all computations, most computations in the memory-centric designs are performed
    within or near memory. As depicted in Figure [1b](#page-2-1), the core concept
    revolves around minimizing data transfer on the bus by relocating a substantial
    share of computations closer to the data (memory). The CPU''s primary role becomes
    issuing commands and handling computations that cannot be effectively executed
    in close proximity to the memory.


    The concept of memory-centric computing is not a novel one, but it has experienced
    a significant surge in recent years. Consequently, various terms have emerged,
    often referring to the same idea, and more detailed classifications have been
    introduced in architectural designs. This section aims to clarify the terminology
    surrounding these approaches.


    <span id="page-3-7"></span><span id="page-3-5"></span>*2.2.1 Compute-in-memory.*
    Computing systems can be broadly divided into two categories: *compute-in-memory*
    [\(CIM\)](#page-39-0) systems and *compute-outside-memory* [\(COM\)](#page-39-2)
    systems (see Section [2.1\)](#page-2-2). In the literature, there are different
    names for similar things. These architectures are often named based on (1) the
    *location* of the compute units within the memory hierarchy (near cache or near
    main memory), or (2) based on the underlying paradigm, i.e., whether the memory
    device itself is used to implement computation [\(CIM\)](#page-39-0), or whether
    extra CMOS-logic is added near the memory to perform computations (*compute-near-memory*
    [\(CNM\)](#page-39-1)). Figure [2](#page-4-1) shows an overview of different processor
    and memory system configurations. A compute operation can be a logic operation
    or an arithmetic operation such as addition and multiplication. [CIM](#page-39-0)
    systems are also frequently referred to as *in-memory-computing* [\(IMC\)](#page-39-3),
    *in-memory-processing* [\(IMP\)](#page-39-4), *processing-in-memory* [\(PIM\)](#page-39-5),
    *processing-using-memory* [\(PUM\)](#page-39-6) or *logic-in-memory* [\(LIM\)](#page-39-7)
    systems [\[18\]](#page-34-16). For the purposes of this report, we will use the
    term [CIM.](#page-39-0)


    <span id="page-3-6"></span><span id="page-3-4"></span><span id="page-3-3"></span><span
    id="page-3-2"></span><span id="page-3-1"></span><span id="page-3-0"></span>*2.2.2
    Compute-near-memory.* When operations are computed outside the memory [\(COM\)](#page-39-2)
    using conventional computing cores (Fig [2.](#page-4-1)a), the architecture is
    a conventional Von Neumann system (see Section [2.1\)](#page-2-2). On the other
    hand, if the computations are performed outside the memory but with a dedicated
    logic unit connected to the memory module via a high-bandwidth channel (Fig [2.](#page-4-1)b),
    the system is referred to as a *compute-near-memory* [\(CNM\)](#page-39-1) or
    *near-memory-computing* [\(NMC\)](#page-39-8) or


    <span id="page-4-3"></span><span id="page-4-2"></span>*near-memory-processing*
    [\(NMP\)](#page-39-9), or *processing-near-memory* [\(PNM\)](#page-39-10) system.
    In this report, we will restrict ourselves to calling it [CNM.](#page-39-1)


    <span id="page-4-5"></span>In the [CIM](#page-39-0) category, computations can
    be carried out using memory cells within the memory array, known as CIM-array
    [\(CIM-A\)](#page-39-11) (see Fig [2.](#page-4-1)d). Alternatively, computations
    can occur in the memory peripheral circuitry, termed CIM-peripheral [\(CIM-P\)](#page-39-12)
    (see Fig [2.](#page-4-1)c).


    <span id="page-4-6"></span><span id="page-4-1"></span>![](_page_4_Figure_3.jpeg)


    Fig. 2. High-level overview of systems where computation is performed a) COM (mainstream
    computing): outside of memory system, b) [CNM:](#page-39-1) using a logic connected
    to the memory via the memory high-bandwidth channel, c) [CIM-P:](#page-39-12)
    in the memory peripheral circuitry, and d) [CIM-A:](#page-39-11) using memory
    cells within the memory array.


    In [CIM-A,](#page-39-11) memory cells are often modified to support logic design,
    e.g., in [\[19\]](#page-34-17). Sometimes, it also necessitates changes to the
    periphery to support the modified cells. Therefore, some literature further divides
    the [CIM-A](#page-39-11) designs into basic [CIM-A](#page-39-11) that do not require
    any modifications to the periphery, e.g., [\[20\]](#page-34-18), and hybrid [CIM-A](#page-39-11)
    that requires support from the peripheral circuit. A well-known example of a hybrid
    [CIM-A](#page-39-11) is the MAGIC design [\[19\]](#page-34-17) that requires extending
    the peripheral circuit to write multiple memory rows simultaneously.


    <span id="page-4-10"></span><span id="page-4-7"></span><span id="page-4-4"></span>Typical
    examples of [CIM-P](#page-39-12) architectures are crossbars employing *analog-to-digital
    converter*s [\(ADCs](#page-39-13)) and digital-to-analog converters [\(DACs](#page-39-14))
    to implement *matrix-vector multiplication* [\(MVM\)](#page-41-2) and other logic
    operations [\[21,](#page-34-19) [22\]](#page-34-20). Additionally, [CIM-P](#page-39-12)
    designs employing customized sense amplifiers also exist [\[23\]](#page-34-21).
    Similar to [CIM-A,](#page-39-11) [CIM-P](#page-39-12) can be either basic, as
    in Pinatubo [\[23\]](#page-34-21), requiring no changes to the memory array, or
    hybrid, as seen in ISAAC [\[22\]](#page-34-20). A summary of the terminology''s
    classification is presented in Figure [3.](#page-5-0)


    Both [CIM-A](#page-39-11) and [CIM-P](#page-39-12) can also be used together,
    wherein the memory array calculates partial results that are later post-processed
    or accumulated in the peripheral circuit. In such cases, it is referred to as
    a [CIM](#page-39-0) architecture.


    #### <span id="page-4-0"></span>**3 TECHNOLOGY OVERVIEW**


    <span id="page-4-9"></span><span id="page-4-8"></span>In this section, we present
    an overview of the main memory cells used in various [CNM](#page-39-1) and [CIM](#page-39-0)
    systems, encompassing both volatile *static random-access memory* [\(SRAM\)](#page-40-6),
    *dynamic random-access memory* [\(DRAM\)](#page-39-15)) and non-volatile types
    such as [PCM,](#page-40-1) [MRAM,](#page-40-3) [RRAM](#page-40-2) and [FeFETs](#page-40-4).
    These memory technologies are versatile enough to serve as main memory for data
    storage and support [CNM](#page-39-1) without requiring any modification to the
    memory chips. Additionally, we explore how these memory cells can be leveraged
    for in-memory computation, considering technological aspects such as performance,
    energy consumption, lifetime, CMOS compatibility, and other relevant factors.
    Before going into the individual technologies, let us first explain the different
    components of the memory subsystem.


    <span id="page-5-0"></span>![](_page_5_Figure_1.jpeg)


    Fig. 3. [CIM](#page-39-0) and [CNM](#page-39-1) classification.


    #### **3.1 Memory subsystem**


    The memory subsystem typically consists of a four-level hierarchy, each with its
    own characteristics and access time. The fastest and smallest level is the CPU
    registers, while the slowest one in the hierarchy is storage devices (HDD/SSD).
    The [CNM/](#page-39-1)[CIM](#page-39-0) concepts have been proposed at different
    levels in the memory hierarchy, e.g., in-cache computing [\[24\]](#page-34-22),
    in-DRAM [\[25\]](#page-35-0), in-storage computing [\[26\]](#page-35-1). However,
    [CNM](#page-39-1)[/CIM](#page-39-0) at the main memory level have rightly gained
    more attention than others.


    <span id="page-5-1"></span>![](_page_5_Figure_5.jpeg)


    Fig. 4. Typical [DRAM](#page-39-15) system organization.


    The main memory is also typically organized hierarchically, as shown in Figure
    [4.](#page-5-1) The main components of the hierarchy are memory cells, subarrays,
    banks, and ranks. A cell is the fundamental working unit and serves as a building
    block to form arrays. An array is a 2D grid of cells connected via *word lines*
    and *bitlines*. A word line is a horizontal line that connects all the memory
    cells in a row, while bitlines, on the other hand, connect all cells in a column.
    Memory cells are placed at the intersection of word lines and bitlines. The combination
    of a specific word line and a bit line uniquely identifies each memory cell. When
    a particular memory cell needs to be accessed, the corresponding word line and
    bit line are activated, allowing data to be read from or written into that cell.


    #### **3.2 [DRAM](#page-39-15)**


    [DRAM](#page-39-15) is the most mature and widely used memory technology today.
    A [DRAM](#page-39-15) cell is composed of a transistor and a capacitor. When the
    capacitor is fully charged, it represents the logical value 1, while a discharged
    capacitor represents the logical value 0. To access data from the [DRAM](#page-39-15)
    array, the memory controller brings a particular row into the row buffer by sending
    an *activate* command. A *read* command is then issued to read specific column(s)
    from the row buffer and put them on the bus.


    [DRAM](#page-39-15) has scaled nicely for decades and has been used across application
    domains and systems ranging from HPC to portable devices. However, it is presently
    facing several challenges to remain the dominant technology. The increasing demand
    for higher capacity has put tremendous pressure on the [DRAM](#page-39-15) capacitor
    size to shrink which makes it susceptible to errors. Also, the increase in capacity
    is significantly increasing the refresh power budget.


    <span id="page-6-4"></span><span id="page-6-3"></span>To address the escalating
    demands for higher bandwidth in modern applications, 3D stacked [DRAM](#page-39-15)
    architectures, such as *high bandwidth memory* [\(HBM\)](#page-40-7) (see Section
    [4.1.3\)](#page-12-0), have been proposed. These architectures consist of stacked
    [DRAM](#page-39-15) dies atop a logic layer, interconnected through *through-silicon
    vias* [\(TSVs\)](#page-41-3), resulting in remarkably higher bandwidth. These
    structures are also employed in a series of [CNM](#page-39-1) solutions, where
    the logic layer is used to implement custom logic and perform computations in
    closer proximity to the data [\[27,](#page-35-2) [28\]](#page-35-3).


    From the [CIM](#page-39-0) perspective, the majority of in[-DRAM](#page-39-15)
    implementations rely on charge sharing, wherein multiple rows are activated simultaneously.
    The shared charge is then utilized in a controlled manner to carry out various
    logic and data copy operations [\[25\]](#page-35-0). Moreover, cleverly manipulating
    the memory timing parameters, deviating from standard timings, has also been employed
    to implement different logic operations [\[29\]](#page-35-4).


    #### **3.3 SRAM**


    SRAM is another mature memory technology that provides fast and efficient memory
    accesses. It is commonly used in caches, register files, and other high-speed
    memory applications where speed and low access latency are critical. An [SRAM](#page-40-6)
    cell consists of multiple transistors arranged in a specific configuration to
    hold one bit of data. The most common configuration of an [SRAM](#page-40-6) cell
    is a pair of cross-coupled inverters that are connected in a feedback loop, forming
    a latch.


    <span id="page-6-0"></span>![](_page_6_Figure_8.jpeg)


    (a) A sample 6T [SRAM](#page-40-6) cell [\[30\]](#page-35-5) (b) Metal-oxide RRAM
    cell [\[31\]](#page-35-6) (c) [FeFET](#page-40-4) device [\[32\]](#page-35-7)


    <span id="page-6-5"></span><span id="page-6-2"></span><span id="page-6-1"></span>Fig.
    5. Cell structures of various memory technologies


    To read and store data on the cell, the bitline terminals, *bitline* [\(BL\)](#page-39-16)
    and *bitline bar* [\(BLB\)](#page-39-17) (see Fig. [5a\)](#page-6-0), are precharged
    and discharged, and the wordline(*word line* [\(WL\)](#page-41-4)) is activated
    or deactivated depending on the values reading/writing from/to the cell.


    There have been proposals for in[-SRAM](#page-40-6) computing, especially at the
    last-level cache, which can be considerably slower compared to the L1 cache (e.g.,
    by an order of magnitude). Similar to [DRAM,](#page-39-15) most in[-SRAM](#page-40-6)
    computing architectures also leverage charge sharing in the bitlines. Specifically,
    precharging the bitlines in a controlled manner and activating multiple rows simultaneously
    enables


    performing logic operations [\[24\]](#page-34-22). For bitwise multiplication
    in [SRAM,](#page-40-6) research has demonstrated that the amplitude of the input
    voltage at the [WL](#page-41-4) directly influences the discharge rate of the
    [BLB.](#page-39-17) The voltage discharge on [BLB,](#page-39-17) achieved within
    a specific timeframe, effectively represents a one-bit multiplication of the data
    stored in the [SRAM](#page-40-6) cell.


    #### **3.4 Phase change memory [\(PCM\)](#page-40-1)**


    [PCM](#page-40-1) is resistive memory technology that employs reversible phase
    changes in materials to store data. The earliest demonstration of a 256-bit [PCM](#page-40-1)
    prototype dates back to 1970 [\[33\]](#page-35-8). Today, [PCM](#page-40-1) stands
    as one of the most extensively researched [NVM](#page-40-0) technologies. A [PCM](#page-40-1)
    device comprises a phase-changing material sandwiched between two electrodes (very
    similar to Fig. [5b\)](#page-6-0), which transitions between crystalline (low
    resistance state) and amorphous (high resistance state) phases. These two resistance
    states represent binary logic states, i.e., 1 and 0.


    Typically, [PCM](#page-40-1) requires a relatively high programming current (>200),
    but this can be mitigated to less than 10 by scaling down the device size [\[10,](#page-34-8)
    [34\]](#page-35-9). As [PCM](#page-40-1) stores data based on resistance, it can
    be programmed to encompass more than two resistance states, allowing for multi-level
    cells to represent more than a single bit of information. Nevertheless, relying
    on multiple resistance states for prolonged periods poses challenges, as the device
    resistance tends to drift over time, making it difficult to discern between resistance
    states.


    #### **3.5 Resistive RAM [\(RRAM\)](#page-40-2)**


    [RRAM](#page-40-2) is another class of resistive memory technologies that utilizes
    the resistive switching phenomenon in metal oxide materials to store data [\[11\]](#page-34-9).
    As shown in Fig. [5b,](#page-6-0) a typical [RRAM](#page-40-2) cell comprises
    a top and a bottom electrode with a thin oxide layer sandwiched in between. To
    achieve resistive switching, a high electric field is applied to the [RRAM](#page-40-2)
    cell, leading to the creation of oxygen vacancies within the metal oxide layer.
    This process results in the formation of conductive filaments, causing the device
    state to transition from a high resistance to a low resistance (set) state. To
    revert to the high resistance (reset) state, the device is subjected to , which
    breaks the conductive filament, allowing the oxygen ions to migrate back to the
    bulk. Compared to [PCM,](#page-40-1) [RRAM](#page-40-2) exhibits several advantages,
    including higher write endurance (>1010), faster write operations, larger resistance
    on-off ratios, and improved scalability prospects [\[11\]](#page-34-9). However,
    ReRAM does suffer from inconsistent electrical characteristics, meaning it exhibits
    larger variations in resistance across different devices [\[10\]](#page-34-8).


    #### <span id="page-7-0"></span>**3.6 Magnetic RAM (MRAM)**


    <span id="page-7-3"></span><span id="page-7-2"></span><span id="page-7-1"></span>[MRAM](#page-40-3)
    store data in nano-scale ferromagnetic elements via magnetic orientation [\[35\]](#page-35-10).
    An [MRAM](#page-40-3) cell is a *magnetic tunnel junction* [\(MTJ\)](#page-40-8)
    device composed of two ferromagnetic layers, namely a fixed reference layer and
    a free layer, separated by an insulating layer. The free layer holds the data
    bit, and reading it involves passing an electric current and measuring its resistance.
    For data writing into an [MRAM](#page-40-3) cell, various techniques can be used.
    The most common method is the *spin-transfer-torque* [\(STT\)](#page-40-9), which
    utilizes spin-polarized electric current to change the free layer''s magnetic
    orientation. *Spin-orbit-torque* [\(SOT\)](#page-40-10)[-MRAM,](#page-40-3) on
    the other hand, uses an in-plane current through the heavy metal layer to generate
    a spin current that exerts a torque on the magnetization of the free layer.The
    relative orientations of the free and fixed layers result in different resistance
    states. [MRAM](#page-40-3) exhibits virtually unlimited endurance and acceptable
    access latency. However, it is faced with challenges such as a larger cell size
    and a smaller on/off resistance ratio, limiting an [MRAM](#page-40-3) cell to
    store only one bit of data [\[36\]](#page-35-11).


    CNM/CIM Landscape 9


    <span id="page-8-0"></span>


    | Device                 | SRAM      | DRAM        | RRAM        | PCM         |
    STT-MRAM | FeFET  |

    |------------------------|-----------|-------------|-------------|-------------|----------|--------|

    | Write time             | 1 − 10𝑛𝑠  | > 20𝑛𝑠      | > 10𝑛𝑠      | ∼ 50𝑛𝑠      |
    > 10𝑛𝑠   | ∼ 10𝑛𝑠 |

    | Read time              | 1 − 10𝑛𝑠  | > 20𝑛𝑠      | > 10𝑛𝑠      | > 10𝑛𝑠      |
    > 10𝑛𝑠   | ∼ 10𝑛𝑠 |

    | Drift                  | No        | No          | Weak        | Yes         |
    No       | No     |

    | Write energy (per bit) | 1 − 10𝑓 𝐽 | 10 − 100𝑓 𝐽 | 0.1 − 1𝑝𝐽   | 100𝑝𝐽       |
    ∼ 100𝑓 𝐽 | > 1𝑓 𝐽 |

    | Density                | Low       | Medium      | High        | High        |
    Medium   | High   |

    | Endurance              | > 1016    | > 1016      | > 105 − 108 | > 105 − 108
    | > 1015   | > 1015 |

    | Retention              | Low       | Very Low    | Medium      | long        |
    Medium   | long   |


    Table 1. A comparison of the key features across different mainstream CMOS and
    emerging memristive technologies [\[39\]](#page-35-12).


    ### **3.7 Ferroelectric Field-Effect Transistor (FeFET)**


    <span id="page-8-1"></span>Since the discovery of ferroelectricity in hafnium
    oxide, [FeFETs](#page-40-4) have received considerable attention. [FeFETs](#page-40-4)
    are non-volatile three-terminal devices, offering high / ratios and low read voltage.
    Unlike *metal-oxide-semiconductor* [\(MOS\)](#page-40-11)-FETs, [FeFETs](#page-40-4)
    incorporate a ferroelectric oxide layer in the gate stack, as shown in Figure
    [5c.](#page-6-0) The nonvolatility arises from hysteresis due to the coupling
    between the ferroelectric and CMOS capacitances (C and C ). The three-terminal
    structure of [FeFETs](#page-40-4) enables separate read and write paths. Reading
    involves sensing the drain-source current, while writing involves switching the
    ferroelectric polarization with an appropriate V voltage. Unlike two-terminal
    devices with variable resistance, [FeFETs](#page-40-4) do not require a drain-source
    current during the writing process, leading to low writing energy consumption
    [\[37\]](#page-35-13). There are various [CIM](#page-39-0) architectures exploiting
    different properties of [FeFETs](#page-40-4). For instance, for boolean operations,
    [\[37\]](#page-35-13) proposes precharging the bitlines followed by simultaneous
    activation of the target rows, and using differential sense amplifiers to discern
    the output.


    #### **3.8 Comparison and discussion**


    Table [1](#page-8-0) presents a comparison between mainstream and emerging memory
    devices, discussed in the preceding sections, with respect to performance, reliability,
    and energy consumption, among others. This analysis gives insights into their
    suitability for different application domains. It is clear that no single memory
    device can optimize all metrics. Nonetheless, recent investigations into machine
    learning use cases show that different phases of machine learning tasks demand
    different memory device properties, potentially offering the opportunity to employ
    various devices for various application domains (or tasks within a domain) and
    achieve the best results [\[38\]](#page-35-14).


    [PCM,](#page-40-1) [RRAM,](#page-40-2) [MRAM](#page-40-3) and [FeFET](#page-40-4)
    fall under the category of memristive technologies, where devices can exhibit
    multiple resistance states. This characteristic has been effectively leveraged
    to perform [MVM,](#page-41-2) as depicted in Figure [15a.](#page-17-0) Although
    the analog computation may not be entirely precise, some loss in accuracy is acceptable
    in many application domains, particularly for machine learning applications. Numerous
    [CIM](#page-39-0) architectures have been proposed using this technique to accelerate
    neural network models (see Section [4.2\)](#page-15-0).


    Figure [6](#page-9-1) shows the importance of various device properties for neural
    network training and inference. For training, frequent weight updates within the
    memory are crucial, making memory technologies like [PCM](#page-40-1) and [RRAM,](#page-40-2)
    with limited endurance and expensive write operations, poorly suitable for training
    acceleration. However, in inference, where operations are predominantly read-based
    with minimal writes to the crossbar array, the same technology could outperform
    others by orders of magnitude. Similarly, retention for training is the least
    important but is critical for inference.


    The arguments around Figure [6](#page-9-1) generally hold true for other metrics
    and other application domains. Although machine learning constitutes a significant
    area, it is not the only domain benefiting from the


    <span id="page-9-1"></span>![](_page_9_Figure_1.jpeg)


    Fig. 6. A spider chart showing the importance of different attributes of the NVM
    technologies for the neural network training and inference. More distance from
    the center means more important [\[38\]](#page-35-14).


    [CIM](#page-39-0) paradigm. Many other data-intensive application domains have
    also effectively exploited the [CIM](#page-39-0) paradigm. Figure [7](#page-9-2)
    shows a landscape of [CIM](#page-39-0) applications, emphasizing precision considerations,
    computational complexity and memory access requirements. These applications are
    classified into three categories based on their precision demands. This data pertains
    to 2020. Over the past two years, additional application domains, such as databases,
    bioinformatics, and solving more complex algebraic tasks, have gained significant
    attention as well.


    <span id="page-9-2"></span>![](_page_9_Figure_4.jpeg)


    Fig. 7. The applications landscape for [CIM](#page-39-0) and [CNM](#page-39-1)
    [\[17\]](#page-34-15).


    # <span id="page-9-0"></span>**4 SELECTED ARCHITECTURES**


    In this section, we discuss some prevalent [CIM](#page-39-0) and [CNM](#page-39-1)
    architectures, explaining their programming models and systems integration. It
    is important to highlight that there exist COM accelerators optimized for specific
    domains, achieving throughput similar to [CNM/](#page-39-1)[CIM](#page-39-0) counterparts,
    albeit at the expense of higher energy consumption. These accelerators are beyond
    the scope of this paper.


    #### CNM/CIM Landscape 11


    This section is structured as follows: In Section [4.1,](#page-10-0) we provide
    an overview of [CNM](#page-39-1) systems, starting with academic designs and progressing
    to commercial [CNM](#page-39-1) systems, including both planar 2D and stacked
    DRAM structures. Section [4.2](#page-15-0) follows a similar organization for
    [CIM](#page-39-0) systems employing various technologies. In Section [4.3,](#page-24-0)
    we conduct a comparative analysis of different [CIM](#page-39-0)[/CNM](#page-39-1)
    systems, while Section [5.20](#page-31-0) outlines the key challenges faced by
    these innovative architectures.


    #### <span id="page-10-0"></span>**4.1 CNM architectures**


    <span id="page-10-4"></span>The core principle of compute-near-memory is to perform
    computations in the memory proximity by placing *processing unit*s [\(PUs](#page-40-12))
    on/near the memory chip. The first [CNM](#page-39-1) architecture dates back to
    the 1990s that aimed at integrating compute units with embedded DRAM on the same
    chip to achieve higher bandwidth. However, due to technological limitations and
    costly fabrication processes, even the promising initial [CNM](#page-39-1) proposals
    like IRAM [\[40\]](#page-35-15), DIVA [\[41\]](#page-35-16), and FlexRAM [\[42\]](#page-35-17)
    never commercialized.


    In recent years, due to the advancements in integration and die-stacking technologies,
    [CNM](#page-39-1) has regained interest within both industry and academia. [PUs](#page-40-12)
    are being integrated at different locations within memory devices, including within
    the memory chip as well as outside the memory chip on the module level, i.e.,
    dual in-line memory module (DIMM). A DIMM typically consists of multiple memory
    chips, each consisting of multiple ranks, banks, and subarrays. It is worth noting
    that some researchers also classify [PUs](#page-40-12) integrated at the memory
    controller level as [CNM.](#page-39-1) However, following the classification and
    terminology adopted in this report, we categorize it under the COM class.


    In the following, we explain some of the common [CNM](#page-39-1) architectures.
    We start by examining the planar 2D DRAM-based [CNM](#page-39-1) designs, then
    transition to discussing the 2.5D and 3D DRAM-based [CNM](#page-39-1) systems,
    and ultimately conclude on the NVM-based [CNM](#page-39-1) architectures.


    <span id="page-10-2"></span>*4.1.1 The UPMEM system.* UPMEM is a recent commercial
    near-bank [CNM](#page-39-1) system and is publicly available [\[5\]](#page-34-3).
    Figure [8](#page-10-1) gives a detailed overview of the UPMEM architecture. The
    memory modules are divided into PIM-enabled memory and main memory (conventional).
    The PIM-enabled memory combines co-processors known as *data processing unit*s
    [\(DPUs](#page-39-18)) with conventional DDR4 DRAM on the same die.


    <span id="page-10-3"></span><span id="page-10-1"></span>![](_page_10_Figure_7.jpeg)


    Fig. 8. An overview of the UPMEM architecture [\[43\]](#page-35-18).


    [DPUs](#page-39-18) are 32-bit general-purpose RISC processors, comprising a 64kB
    SRAM-based scratchpad working memory known as WRAM, a 24kB SRAM-based instruction
    memory referred to as IRAM, and a shared main memory named MRAM, based on DRAM
    technology. As shown in the figure (lower left), each DIMM consists of 16 memory
    chips, with each chip housing 8 banks, and each bank containing one [DPU.](#page-39-18)
    The latest UPMEM systems can support up to 20 DIMMs.


    **[DPU-DPU](#page-39-18) communication:** [DPUs](#page-39-18) in UPMEM can have
    up to 24 hardware threads called tasklets.


    <span id="page-11-2"></span>Within the same [DPU,](#page-39-18) tasklets can share
    data through MRAM and WRAM, however, [DPUs](#page-39-18) can not communicate with
    each other directly and must go through the host for any possible data sharing.
    **Progammability:** For programmability, UPMEM offers its own *software development
    kit* [\(SDK\)](#page-40-13) consisting of an UPMEM compiler and runtime libraries.
    [DPU](#page-39-18) programs are written in the C language including specific library
    calls. The runtime library provides functions for data and instruction transfers
    between different memory, e.g., MRAM-IRAM, MRAM-WRAM etc.; executing various functions
    on the [DPUs](#page-39-18); and synchronization (mutex locks, barriers, handshakes,
    and semaphores).


    Although UPMEM claims they have an easily programmable [SDK,](#page-40-13) programming
    the system has several challenges. The programmer is responsible for efficient
    mapping of executions and load-balancing on thousands of [DPUs](#page-39-18),
    managing data transfer, and ensuring coherence of data between CPU and [DPUs](#page-39-18).


    *4.1.2 CNM for [MVM](#page-41-2) in DRAM.* McDRAM [\[44\]](#page-35-19) and MViD
    [\[45\]](#page-35-20) (both involving Samsung Electronics) aimed at accelerating
    machine learning workloads by embedding *multiply-accumulate* [\(MAC\)](#page-40-14)
    units within the DRAM bank. Similar to UPMEM, both McDRAM and MViD utilize 2D
    DRAM (LPDDR in these cases) and incorporate [PUs](#page-40-12) within the memory
    banks to exploit the higher internal memory bandwidth. However, unlike UPMEM,
    these architectures are domain-specific and hence employ fixed functional units
    [\(MACs](#page-40-14)) instead of general-purpose programmable cores.


    <span id="page-11-3"></span><span id="page-11-0"></span>Figure [9](#page-11-0)
    shows the McDRAM architecture along with the three locations (column decoder,
    bitline *sense amplifier* [\(SA\)](#page-41-5)s, and I/O drivers) where MAC units
    were employed and evaluated. Each McDRAM chip consists of 4 banks and each bank
    has four 8-bit MAC units. The multiplication is performed in parallel by multiplying
    rows of the matrix with the input vector.


    <span id="page-11-1"></span>![](_page_11_Figure_5.jpeg)


    Fig. 9. The McDRAM architecture with three possible locations for MAC units [\[44\]](#page-35-19).


    **Programmability:** McDRAM is a fixed-function accelerator and offers a single
    interface function (matmul) that triggers the device driver to configure the control
    registers of the memory controller.It operates in two modes, memory and compute
    modes, determined by a configuration register. In compute mode, McDRAM performs
    [MVM](#page-41-2) tasks. For the management of [MVM](#page-41-2) within compute
    mode, it introduces six novel DRAM commands that leverage existing DRAM I/O signals,
    rendering no modifications to DRAM I/O signals. McDRAM, a fixed-function accelerator,
    employs a single interface function (matmul) triggering the device driver to configure
    memory controller control registers. It operates in two modes, memory and compute,
    determined by a configuration register. In compute mode, McDRAM performs [MVM](#page-41-2)
    introducing six novel DRAM commands for [MVM](#page-41-2) management without modifying
    existing DRAM I/O signals.


    <span id="page-12-1"></span>![](_page_12_Figure_2.jpeg)


    <span id="page-12-3"></span>Fig. 10. The MViD architecture [\[45\]](#page-35-20)
    where each bank has 16 MAC units.


    The MViD architecture depicted in Figure [10](#page-12-1) is similar to the McDRAM
    design and is specifically optimized for edge devices. Much like McDRAM, MViD
    incorporates MAC units within the DRAM I/O drivers to capitalize on the internal
    bandwidth of the DRAM. However, unlike McDRAM, MViD introduces a partitioning
    of memory banks into two categories: [MVM](#page-41-2) banks that are equipped
    for MAC units and two SRAM structures to hold the input and output vectors and
    non[-MVM](#page-41-2) banks (traditional). This division enables concurrent access
    to both types, meaning that multiplication operations in [MVM](#page-41-2) banks
    can occur simultaneously with the CPU accesses to the non[-MVM](#page-41-2) banks.


    <span id="page-12-0"></span>*4.1.3 Samsung''s CNM systems.* Samsung is probably
    ahead of everyone in the race for commercial [CNM](#page-39-1) systems. In the
    following, we discuss two of their recent promising (and complete) solutions.


    **PIM-HBM:** Samsung has recently introduced a [CNM](#page-39-1) architecture
    referred to as Function-in-Memory DRAM (FIMDRAM)[\[6\]](#page-34-4) or PIM-HBM[\[7\]](#page-34-5).
    It incorporates 16 *single-instruction multiple-data* [\(SIMD\)](#page-40-15)
    engines within the memory banks, enabling bank-level parallelism. As reported
    in [\[7\]](#page-34-5), their design does not disrupt crucial elements on the
    memory side, such as the sub-array and bank in conventional DRAM, making its integration
    seamless and straightforward. Importantly, it does not require any modifications
    to contemporary commercial processor components, including DRAM controllers. It
    is designed for host processors to manage PIM operations via standard DRAM interfaces.
    This feature allows for a straightforward substitution of existing JEDEC-compliant
    DRAM with PIM-DRAM across various systems.


    <span id="page-12-2"></span>![](_page_12_Figure_7.jpeg)


    Fig. 11. Samsung''s PIM-HBM (a) HBM die organization (b) Bank coupled with a PIM
    unit (c) PIM unit data path [\[7\]](#page-34-5).


    While Samsung reports that their design is compatible with any DRAM family, they
    have showcased its functionality using the 2.5D high bandwidth memory (HBM) DRAM.
    Figure [11](#page-12-2) provides a <span id="page-13-1"></span>high-level view
    of this architecture. Each bank comprises 16 [SIMD](#page-40-15) *floating-point
    unit*s [\(FPUs](#page-40-16)), with each FPU consisting of a 16-bit floating-point
    adder and a 16-bit floating-point multiplier. Furthermore, each FPU is equipped
    with data registers (GRFs), control and instruction registers (CRF, SRF), and
    an internal control unit. The internal control unit orchestrates operation sequences
    without necessitating modifications to the memory controller. When operating in
    PIM mode, the PIM execution units within all banks simultaneously respond to a
    standard DRAM column (Read or Write) command initiated by the host processor and
    execute a wide SIMD operation with deterministic latency in a lock-step manner.


    <span id="page-13-2"></span>**Programmability:** PIM-HBM comes with an *instruction
    set architecture* [\(ISA\)](#page-40-17), a software stack, and a specific programming
    model. The software stack presents a native execution path that does not require
    any modifications to the input code. The framework takes the high-level representation
    of an application and transforms it into device code. Furthermore, it offers a
    direct execution path that permits direct invocation of various function calls
    using the "PIM custom op". The PIM runtime includes a collection of modules responsible
    for tasks like operations offloading, memory allocation, and execution on the
    FPUs.


    HBM-PIM is a commercial accelerator and, as per, Samsung is already used by companies.
    Here is an excerpt from Samsung''s newsroom:


    *"Xilinx has been collaborating with Samsung Electronics to enable high-performance
    solutions for data center, networking, and real-time signal processing applications
    starting with the Virtex UltraScale+ HBM family, and recently introduced our new
    and exciting Versal HBM series products," said Arun Varadarajan Rajagopal, senior
    director, Product Planning at Xilinx, Inc. "We are delighted to continue this
    collaboration with Samsung as we help to evaluate HBM-PIM systems for their potential
    to achieve major performance and energy-efficiency gains in AI applications."*


    **AxDIMM (by Samsung-Facebook:)** Samsung is also working on the development of
    an FPGAenabled [CNM](#page-39-1) platform named AxDIMM. In collaboration with
    Facebook, this solution has showcased its effectiveness in a personalized recommender
    system. As shown in Figure [12a,](#page-13-0) the [CNM](#page-39-1) architecture
    (RANK) is the same as Samsung''s HBM-PIM, but the controlling unit is FPGA that
    starts the execution, maps computations to the RANK, and gets back the results.
    Like the HBM-PIM, AxDIMM has a complete software stack that allows programming
    the architecture without changing the input code or manually writing code using
    AxDIMM python API.


    <span id="page-13-0"></span>![](_page_13_Figure_6.jpeg)


    (a) Samsung-Facebook AxDIMM hardware module and architecture [\[46\]](#page-35-21)


    (b) AiM architecture [\[9\]](#page-34-7).


    Fig. 12. PUMA tile and core architectures [\[47\]](#page-36-0).


    For this product, Samsung also seems to be in discussion with SAP HANA. Here is
    another excerpt from the newsroom:


    *"SAP has been continuously collaborating with Samsung on their new and emerging
    memory technologies to deliver optimal performance on SAP HANA and help database
    acceleration," said Oliver Rebholz, head of HANA core research & innovation at
    SAP. "Based on performance projections and potential integration scenarios, we
    expect significant performance improvements for in-memory database management
    system (IMDBMS) and higher energy efficiency via disaggregated computing on AXDIMM.
    SAP is looking to continue its collaboration with Samsung in this area"*.


    *4.1.4 SK hynix''s accelerator-in-memory.* SK hynix''s accelerator-in-memory (AiM)
    is another [CNM](#page-39-1) system that targets the machine learning application
    domain [\[8,](#page-34-6) [9\]](#page-34-7). As stated in [\[9\]](#page-34-7),
    "Samsung''s FIMDRAM is near commercialization, but the required HBM technology
    may prevent it from being applied to other applications due to its high cost".
    AiM fundamentally follows a very similar design approach to Samsung''s FIMDRAM
    but utilizes GDDR6 instead.


    Figure [12b](#page-13-0) provides an overview of the AiM architecture. As depicted,
    each bank is equipped with a processing unit (PU) that executes a MAC operation
    using 16 multiplier units and an adder tree. The adder tree can be deactivated
    for operations not requiring additions. Similar to the FIMDRAM design, pairs of
    banks can establish direct communication. For inter-group communication, an internal
    2KB SRAM structure within the periphery facilitates the process.


    Although the programming model is not explicitly explained, the presented set
    of commands in AiM implies an interface enabling interaction with the device for
    various operations. Some of these operations are particularly interesting, such
    as the ability to perform computations within banks of different granularities
    (1, 4, 16) and data movement functions that can be utilized to implement row-cloning
    within DRAM.


    *4.1.5 AxRAM.* AxRAM targets optimizing for the off-chip memory communication
    bottleneck in GPUs by integrating approximate MAC units in the DRAM [\[48\]](#page-36-1).
    The fundamental idea is to exploit the inherent approximability of numerous GPU
    applications and perform approximate calculations directly within the memory banks,
    thereby reducing data movement and energy consumption. AxRAM leverages the concept
    of neural transformation, a technique that accelerates a wide range of applications
    by approximating specific sections of GPU code and transforming them into a neural
    representation composed primarily of [MAC](#page-40-14) and *look-up table* [\(LUT\)](#page-40-18)
    operations for nonlinear function calculation. The multiplications in the MAC
    operations are further approximated with limited iterations of shift-add and LUT
    accesses. These approximate units are connected to the wide data lines that connect
    the DRAM banks to the global I/O, keeping the banks and memory column unchanged.


    <span id="page-14-0"></span>Figure [13](#page-15-1) shows a sample example where
    the GPU code is transformed into MAC and lookup operations. Once such patterns
    are identified and transformed, they are offloaded to the in-DRAM accelerator.
    The new instructions that invoke and configure the in-DRAM accelerators are added
    to the GPU''s ISA and are exposed to the compiler. As for the flow of execution,
    initially, all data is assumed to be in one memory chip. The GPU starts normal
    execution, and for the identified approximate regions, the GPU warps send an initiation
    request to the on-chip memory controller. The additional logic in the memory controller
    first sends an invalid signal to the on-chip caches (to ensure data consistency)
    and subsequently drives the in-DRAM accelerator to perform the computations and
    store the results in the designated location. To check whether the execution is
    completed, the memory controller periodically checks the memory-mapped mode register
    of the DRAM, which is updated by the accelerator. Once the controller detects
    that this register is set, it signals the GPU that execution is finalized, allowing
    the GPU to proceed with precise execution of the subsequent instructions.


    <span id="page-15-1"></span>![](_page_15_Figure_1.jpeg)


    ![](_page_15_Figure_2.jpeg)


    *4.1.6 CNM systems based on 3D-stacked DRAM.* All the [CNM](#page-39-1) architectures
    discussed so far (except FIMDRAM) are based on planar 2D DRAM. However, the resurgence
    in [CNM](#page-39-1) systems is also primarily attributed to HBM and HMC technologies
    that seamlessly combine logic and memory within the same package. There is a series
    of proposals for [CNM](#page-39-1) systems leveraging these technologies. In the
    following, we discuss some of the prominent architectures.


    <span id="page-15-2"></span>**TESSERACT** [\[49\]](#page-36-2) targets accelerating
    graph-based applications. Their design comprises a host processor and an HMC with
    multiple vaults, each housing an out-of-order processor. These processors exclusively
    access their local data partition, while inter-communication is achieved through
    a messagepassing protocol. The host processor, however, can access the complete
    address space of the HMC. To capitalize on the substantial memory bandwidth available,
    they introduce prefetching mechanisms. **TOP-PIM** [\[50\]](#page-36-3) is an
    architecture that proposes an *accelerated processing unit* [\(APU\)](#page-39-19).
    Each APU integrates a GPU and a CPU on the same silicon die. These APUs are linked
    through high-speed serial connections to several 3D-stacked memory modules. APUs
    are general-purpose and support a series of applications ranging from graph processing
    to fluid and structure dynamics. The architecture allows code portability and
    easy programmability.


    **Active memory cube (AMC)** [\[51\]](#page-36-4) is also built upon HMC and proposes
    "lanes" in the HMC vault. Each AMC lane consists of a register file, a computational
    unit, and a load/store unit to support memory accesses. Communication among AMCs
    is only possible via the host processor. AMC also offers a compiler based on OpenMP
    for C/C++ and FORTRAN.


    **Heterogeneous reconfigurable logic (HRL)** [\[52\]](#page-36-5) leverages the
    logic layer in the 3D stacked HMC to implement heterogeneous coarse-grained (CGRAs)
    and fine-grained (FPGAs) logic blocks. The architecture separates routing networks
    for control and data signals, employing specialized units to efficiently handle
    branch operations and non-uniform data layouts commonly found in analytics workloads.


    # <span id="page-15-0"></span>**4.2 [CIM](#page-39-0) architectures**


    Much like [CNM,](#page-39-1) the concept of [CIM](#page-39-0) systems is not entirely
    novel; however, it has gained significant momentum due to breakthroughs in various
    NVM devices over the past decade. Figure [14](#page-16-0) shows a partial landscape
    of [CIM](#page-39-0) systems, along with a corresponding timeline. Most of the
    depicted [CIM](#page-39-0) accelerators originate from academia and are not taped
    out. However, in recent years, several semiconductor industry giants, including
    Intel, Samsung, TSMC, GlobalFoundries, and IBM, have invested in developing their
    own [CIM](#page-39-0) prototypes, mostly focused on the machine learning case.
    IBM,


    #### CNM/CIM Landscape 17


    ![](_page_16_Figure_1.jpeg)


    <span id="page-16-0"></span>in particular, stands out among others when it comes
    to the development of [CIM](#page-39-0) systems for different use cases.


    Fig. 14. A partial timeline of the evolution of [CIM](#page-39-0) systems (data
    until 2018) [\[28\]](#page-35-3). The radius of the circle is proportional to
    the amount of papers published that year.


    In this section, we overview some of the prominent [CIM](#page-39-0) designs from
    academia and industry. However, before going into the details of individual [CIM](#page-39-0)
    designs, we first introduce circuits that are typically used as basic [CIM](#page-39-0)
    primitives in these architectures.


    *4.2.1 CIM primitives.* Each of the [CIM](#page-39-0) architectures discussed
    in the following sections is either based on a crossbar, content-addressable-memory,
    or a boolean and arithmetic logic unit. In the following, we explain all three
    of them.


    **Crossbar:** A crossbar is a [CIM](#page-39-0) configuration in which each input
    connects to every output through cross-points, comprising memory cells and selectors.
    Figure [15a](#page-17-0) shows a technology-independent crossbar configuration.
    As we will see in the following sections, crossbars are particularly useful for
    the machine learning domain as they can compute [MVM](#page-41-2) in constant
    time.


    <span id="page-16-1"></span>**CAM:** *content-addressable-memory* [\(CAM\)](#page-39-20)
    is associative memory that enables parallel searches for a given query (input)
    across all stored content within a CAM array. CAMs are used in pattern matching
    and search operations from various application domains including databases, networking,
    and machine learning [\[53\]](#page-36-6). Figure [15c](#page-17-0) shows a technology-independent
    3 × 3 CAM structure.


    **Boolean and arithmetic logic in CIM:** In this class of [CIM,](#page-39-0) the
    [CIM](#page-39-0) array facilitates a specific set of general operations, such
    as Boolean logic and arithmetic, to be executed using customized peripheral circuits
    integrated within the random-access memory (RAM). The operands need to be stored
    in different rows of an array in a column-aligned fashion where each column represents
    a bit position. For a [CIM](#page-39-0) operation, multiple rows are typically
    activated simultaneously, and the output is sensed and inferred by the peripherical
    circuitry [\[23,](#page-34-21) [54\]](#page-36-7). Figure [15b](#page-17-0) shows
    a technology-independent structure implementing boolean logic.


    <span id="page-16-2"></span>*4.2.2 ISAAC (by Hewlett Packard Enterprise).* In-situ
    analog arithmetic in crossbars (ISAAC) [\[22\]](#page-34-20) is among the first
    [CIM](#page-39-0) accelerators with a complete design targeting *convolutional
    neural network* [\(CNN\)](#page-39-21) in RRAM. As shown in Figure [16,](#page-17-1)
    ISAAC''s architecture consists of multiple interconnected tiles via a concentrated-mesh
    (c-mesh) network. Each tile consists of 12 in-situ multiply-and-accumulate (IMA)
    units, a shift-and-add (S&A) unit, two sigmoid units, one max-pooling unit, an
    embedded DRAM (eDRAM) buffer for input data storage and an output register (OR)
    to accumulate (partial) results. Each


    <span id="page-17-0"></span>![](_page_17_Figure_1.jpeg)


    Fig. 15. Fundamental [CIM](#page-39-0) primitives [\[53\]](#page-36-6).


    <span id="page-17-1"></span>IMA integrates its own input register (IR), output
    register, S&A units, and eight 128 × 128 resistive crossbar arrays, also abbreviated
    XB or XBars, that share analog-to-digital converters (ADCs). Each XBar performs
    analog [MVM](#page-41-2) (see Figure [15a\)](#page-17-0) and is also equipped
    with a digital-to-analog converter (DAC) and an S&H circuitry. Communication within
    a tile is facilitated by a 32-bit inter-tile link.


    ![](_page_17_Figure_4.jpeg)


    Fig. 16. ISAAC architecture hierarchy [\[22\]](#page-34-20).


    The ISAAC design uses dataflow pipelining to optimize IMA unit utilization and
    reduce buffering requirements. Depending on the network''s size, each [CNN](#page-39-21)
    layer is mapped to one or multiple IMAs or tiles. Initially, input data is acquired
    through an I/O connection and stored within a tile''s eDRAM buffer. Before being
    fed to ReRAM XBars within each IMA, the data goes through DACs. Once processed
    by XBars, the generated feature maps are converted back to digital form and forwarded
    to max-pooling and activation units. The outcome of the NN layer is then accumulated
    within the S&A and OR units and subsequently written to a new eDRAM buffer (for
    the following layer). The depth of the pipeline corresponds to the depth of the
    neural network, which presents challenges when training *deep neural network*s
    [\(DNNs](#page-39-22)). Thus, ISAAC is specifically designed for inference and
    is not used for training. ISAAC has no mention of the design tools and programming
    interface.


    <span id="page-17-2"></span>*4.2.3 PUMA (by Hewlett Packard Enterprise).* PUMA
    (programmable ultra-efficient memristorbased accelerator) is a generalization
    of memristive crossbars to accelerate a range of ML inference workloads [\[47\]](#page-36-0).
    PUMA''s microarchitecture techniques exposed via dedicated ISA ensure the efficiency
    of in-memory computing and analog circuitry while providing a high degree of programmability.
    The


    architecture is organized into three hierarchy levels: cores, tiles, and nodes.
    Nodes are connected and communicate via a chip-to-chip network. Each individual
    node consists of tiles that are connected via an on-chip network, where each tile
    comprises cores that communicate via shared memory, as shown in Figure [17a.](#page-18-0)
    A PUMA''s core consists of its own memory and functional units, including the
    XBar array, referred to as the [MVM](#page-41-2) unit (MVMU), see Figure [17b.](#page-18-0)


    <span id="page-18-0"></span>![](_page_18_Figure_2.jpeg)


    Fig. 17. PUMA tile and core architectures [\[47\]](#page-36-0).


    Unlike most other [CIM](#page-39-0) architectures, which are data parallel, PUMA
    is a spatial architecture where distinct instructions are executed by each core
    or tile. Since manually writing code for such architectures is extremely difficult,
    particularly when they have thousands of cores, PUMA has a runtime compiler implemented
    as a C++ library. The compiler takes the high-level input code and extracts a
    dataflow graph from it. The graph is then divided into subgraphs, considering
    the sizes of MVMUs, and hierarchically assigned to MVMUs, cores, and tiles. The
    subgraph execution is carefully scheduled to ensure effective resource utilization
    while avoiding potential deadlocks. Given the constraint of serial read and write
    operations in RRAM, PUMA exclusively supports the ML inference. However, to facilitate
    training, PUMA has been repurposed in a follow-up work named PANTHER [\[55\]](#page-36-8).


    *4.2.4 Pinatubo: Accelerating bulk bitwise logic operation.* Pinatubo is a memristor-based
    architecture that harnesses data-level parallelism to conduct bulk bitwise operations
    [\[23\]](#page-34-21). Unlike the crossbar configurations, it performs computations
    in the digital domain by modifying the [SAs](#page-41-5). The system architecture
    is similar to a typical Von Neumann architecture that has a processor equipped
    with caches and a non-volatile main memory. Pinatubo then exploits the physical
    attributes of the NVM-based main memory and modifies the [SAs](#page-41-5) to
    support it. The main idea is the operands are stored in different rows but the
    same columns in an array, the rows are activated in parallel and the accumulated
    current in the bitline is compared to a reference level in the [SAs](#page-41-5).
    For different logic gates, the memory controller changes the reference levels
    in the [SAs](#page-41-5) to different stats.


    The Pinatubo''s main memory structure is illustrated in Figure [18,](#page-19-0)
    comprising multiple banks divided into banks and mats. For operands within the
    same mat, the modified [SAs](#page-41-5) work out of the box and can perform bitwise
    vector operations. For operations where the data is spread across different mats,
    whether within the same bank or not, additional logic gates are used for execution
    (within the global data line or global I/O). The architecture supports only logic
    operations.


    For programmability, Pinatubo presents a software infrastructure containing both
    the programming model and runtime support components. The programming model offers
    two functions to allocate


    <span id="page-19-0"></span>![](_page_19_Figure_1.jpeg)


    Fig. 18. Pinatubo architecture showing chip, bank and mat [\[23\]](#page-34-21).


    bit-vectors and perform bitwise operations. The runtime support facet encompasses
    adjustments to the C/C++ runtime library and the operating system (OS) and the
    development of a dynamic linked driver library. The runtime library ensures that
    bit-vectors are allocated to separate memory rows while the OS equipped with PIM-aware
    memory management, ensures intelligent invocation of the operations.


    *4.2.5 PRIME.* PRIME [\[21\]](#page-34-19) is another RRAM-based analog [CIM](#page-39-0)
    accelerator. The architecture comprises multiple banks where each bank integrates
    eight subarrays (chips) which are further (logically) split into memory (Mem)
    units, two full function (FF) units, and one buffer. FFs can function conventionally
    as memory or in an NN computation mode, controlled by the PRIME controller. A
    typical FF unit is 256 × 256 RRAM cells, with 6-bit reconfigurable local [SAs](#page-41-5)
    reading their outputs. During computation mode, RRAM resolution is 4-bit *multi-level
    cell* [\(MLC\)](#page-40-19), shifting to *single-level cell* [\(SLC\)](#page-40-20)
    in memory mode. Distinct crossbar arrays are utilized for storing positive and
    negative weights. The input to the mat comes from a 3-bit fixed point signal originating
    from a wordline decoder and driver (WDD). Analog subtraction and sigmoid functions
    within the NN are implemented in the modified column multiplexers within the RRAM
    arrays.


    <span id="page-19-4"></span><span id="page-19-1"></span>![](_page_19_Figure_5.jpeg)


    <span id="page-19-3"></span><span id="page-19-2"></span>Fig. 19. PRIME: Source
    code to execution [\[21\]](#page-34-19).


    The execution of an *neural-network* [\(NN\)](#page-40-21) on PRIME involves three
    stages. Firstly, the [NN](#page-40-21) is mapped onto FF subarrays, and synaptic
    weights are programmed into ReRAM cells. In the optimization stage, depending
    on the [NN](#page-40-21) size, mapping could occur in a single bank or across
    multiple banks. These first two stages are executed by the CPU. Subsequently,
    a series of generated instructions are transmitted to the PRIME controller in
    RRAM banks to perform computations. The presence of latches and OR gates facilitates
    pipelined computation within PRIME.


    As shown in Figure [19,](#page-19-1) PRIME also comes with a compiler and an API,
    exposing device capabilities as function calls. The process from code to execution
    involves programming (coding), compiling (code optimization), and code execution.
    PRIME offers application programming interfaces (APIs) that empower developers
    to map [NN](#page-40-21) topologies onto FFs and configure data paths etc.


    *4.2.6 Pipelayer.* Pipelayer is another RRAM-based accelerator for [CNNs](#page-40-21)
    that supports both training and inference [\[56\]](#page-36-9). The overall architecture
    of PipeLayer, shown in Figure [20](#page-20-0) features RRAM crossbars, the spike
    Driver block to encode inputs as spikes and get rid of DACs, and integration <span
    id="page-20-0"></span>and fire components that eliminate ADCs. In write mode,
    the spike driver updates RRAM array weights with a 4-bit resolution. Within the
    cell, data processing occurs across morphable and memory subarrays, where memory
    subarrays are conventional memory arrays while morphable arrays can be configured
    in both compute and memory modes. Pipelayer leverages these morphable subarrays
    for different purposes in training and inference.


    ![](_page_20_Figure_2.jpeg)


    <span id="page-20-1"></span>Fig. 20. An overview of the Pipelayer architecture
    [\[56\]](#page-36-9).


    PipeLayer allows interactive configuration of the system on a per-layer basis.
    It provides an API that has functions for different operations e.g., bidirectional
    transfer of data between the CPU main memory and PipeLayer, the topology\_set
    function, where the number of compute groups can be specified by the programmer,
    the weight\_load function to load either pre-trained weights during testing or
    initial weights during training into the arrays. Other functions include pipeline
    and mode set functions for the morphable subarrays.


    There are many other RRAM-based analog and digital [CIM](#page-39-0) accelerators.
    Some other common ones that are mostly taped-out and not discussed here include:
    AtomLayer [\[57\]](#page-36-10), RIMAC[\[58\]](#page-36-11), FORM [\[59\]](#page-36-12),
    RRAMs for pattern recognition [\[60\]](#page-36-13), RRAM accelerator for BNNs
    (ISSCC, 65 nm) [\[61\]](#page-36-14), RRAM for edge processors (ISSCC, 55 nm)
    [\[62\]](#page-36-15), analog RRAM with fully parallel MAC and extremely high
    TOPS/W (ISSCC, 130 nm but large array) [\[63\]](#page-36-16).


    *4.2.7 In-DRAM computing.* Ambit [\[25\]](#page-35-0) is a DRAM-based [CIM](#page-39-0)
    accelerator that, unlike all previous systems, leverages the analog capabilities
    of current DRAM technology for executing bulk bitwise operations. Ambit mainly
    comprises two components. First, Ambit-AND-OR implements *triple row activation*
    [\(TRA\)](#page-41-6) in conventional DRAM. Like memristors, the idea is to activate
    three rows in parallel and leverage the *charge-sharing and charge accumulation
    principle*. [TRA](#page-41-6) produces a bitwise majority function. Controlling
    the initial state of one of the three rows enable performing AND and OR operation.
    The second component of Ambit is Ambit-NOT, which uses the inverters in the DRAM
    [SAs](#page-41-5) to implement the logic NOT operation. The basic components are
    then extended to implement other logic operations and accelerate bulk bitwise
    operations in multiple applications. With 8 DRAM banks, Ambit demonstrates a substantial
    improvement in bulk bitwise operation throughput compared to an Intel Skylake
    processor and the NVIDIA GTX 745 GPU.


    A follow-up work on the bulk bitwise logic in DRAM, ComputeDRAM [\[29\]](#page-35-4)
    demonstrated that by deliberately violating timing parameters between activation
    commands, certain existing off-the-shelf DRAM chips can implement the [TRA](#page-41-6)
    operation of Ambit. This indicates that certain real-world off-the-shelf DRAM
    chips, despite not being intended for Ambit operations, can indeed perform in-DRAM
    AND and OR operations. This also suggests that the concepts introduced in Ambit
    might not be too far from practical implementation. If existing DRAM chips can
    perform such operations to some extent, then chips explicitly designed for such
    functions could potentially be even more capable.


    *4.2.8 In-SRAM computing.* Neural Cache [\[64\]](#page-36-17) is an SRAM-based
    [CIM](#page-39-0) accelerator primarily targeting [CNNs](#page-39-21). The core
    operations of Neural Cache are bitwise AND and NOR operations, which are executed
    by simultaneously activating multiple rows (charge sharing). It repurposes the
    cache memory by modifying the peripheral circuitry to support operations such
    as convolution, pooling, quantization, and fully-connected layers, all performed
    at an 8-bit data precision. It is also capable of performing bit-serial operations
    like addition, subtraction, multiplication, comparison, search, and copy for larger
    data, utilizing carry latches linked to [SAs](#page-41-5). A transpose memory
    unit is introduced that facilitates the reorganization of data into bit-serial
    format within the memory when needed.


    IMAC [\[30\]](#page-35-5) is another SRAM-based [CIM](#page-39-0) accelerator
    that uses the precharge circuit to perform multi-bit analog multiplication by
    encoding the bit significance in the pulse width of pre-charge pulse. IMAC also
    requires DAC/ADC converters to facilitate the conversion between digital and analog
    forms. There are many other instances of SRAM-based [CIM](#page-39-0) designs,
    some even **taped-out** [\[65](#page-36-18)[–68\]](#page-36-19).


    *4.2.9 In-MRAM computing.* In NVMs, Magnetic RAM (MRAM) is probably the most mature
    memory technology that is commercially available and is already used in many embedded
    devices (see Section [3.6\)](#page-7-0). Therefore, it has also been intensively
    investigated in the [CIM](#page-39-0) context and computing approaches implementing
    in-MRAM basic boolean logic operations and more complex arithmetic functions have
    been showcased. Like all other technologies, the basic [CIM](#page-39-0) methods
    include bit-cell modification, reference adaptation, and in-memory analog computation.


    <span id="page-21-0"></span>![](_page_21_Figure_5.jpeg)


    Fig. 21. A typical SOT-MRAM subarray architecture for in-place logic [\[69\]](#page-37-0).


    Figure [21](#page-21-0) shows a typical subarray architecture of an in-MRAM [CIM](#page-39-0)
    [\[69\]](#page-37-0). The important difference here compared to already known
    aspects is that it has separate read and write bit and word lines and three reference
    resistance states in the sensing circuity (RAND/RM/ROR). RM is used to perform
    normal memory operations, while RAND and ROR, as the names suggest, are used to
    implement AND and OR operations, respectively.


    Similar to other technologies, the boolean logic is implemented with charge-sharing,
    and MAC is implemented in the analog domain with current accumulation. Some prominent
    MRAM-based [CIM](#page-39-0) designs include analog MACs for [TRA](#page-41-6)
    inference [\[70\]](#page-37-1), MRAM[-CIM](#page-39-0) for BNNs [\[71\]](#page-37-2),
    and MRAM crossbar [\[72\]](#page-37-3).


    *4.2.10 CIM using FeFETs.* FeFeTs have also been shown to implement in-place logic
    gates, addition, and content-addressable memories (CAMs). Notably, these logic
    operations can also be implemented with a single [FeFET](#page-40-4) cell. For
    instance, if one operand is stored in a cell (or a set of cells), the other operand
    can be applied as input to perform logic operation [\[73\]](#page-37-4), akin
    to the working principle of crossbars. Further, solutions proposing activating
    multiple rows and leveraging the bitline''s charge sharing (as in other memory
    technology) have also been presented [\[37\]](#page-35-13).


    FeFETs have received particular interest in CAM designs. CAMs are associative
    memories that can perform parallel searches for a query across all stored contents
    within an array. FeFeTs have been used to implement different types of CAMs for
    exact-search operations, approximate search operations, range-based search operations,
    or a combination of them [\[74,](#page-37-5) [75\]](#page-37-6).


    *4.2.11 Latest industrial chips.* In the previous sections, we have extensively
    discussed a variety of notable [CIM](#page-39-0) and [CNM](#page-39-1) solutions
    employing different technologies. While a few of these systems have been developed
    in collaboration with industry partners and a subset has undergone the tape-out
    process, the majority of these accelerators originate from academia. In this section,
    we specifically present [CIM](#page-39-0) systems originating from the industrial
    sector in the last couple of years. Note that these [CIM](#page-39-0) systems
    also primarily show prototypes showcasing various research outcomes, but they
    indicate their potential realization in the near future.


    **IBM''s PCM-based accelerators:** For more than five years, IBM has been using
    its PCM device to do in-place operations for different use cases. Initially, they
    were working with a reservoir of devices (millions of them) and implementing the
    peripheral circuitry and additional CMOS logic in an FPGA. Their research has
    progressed to consolidate all components onto a single chip, as exemplified by
    HERMES, a core composed of 256×256 PCM array with ADCs, a local digital processing
    unit, and additional peripheries [\[76\]](#page-37-7). The core effectively executes
    a fully parallel 256×256 analog [MVM,](#page-41-2) where each 8T4R unit cell encodes
    a positive/negative weight, with simultaneous handling of 256 8-bit digital inputs/outputs.
    Positive weights are encoded by combining the conductance of two PCM devices,
    while negative weights are represented by the other two PCMs within the unit cell.


    This year, IBM announced a newer 64-core [CIM](#page-39-0) chip designed and fabricated
    in 14-nm CMOS technology integrated with PCM [\[77\]](#page-37-8). The fully integrated
    chip comprises 64 cores, each with a size of 256×256, connected through an on-chip
    communication network. It reportedly achieves an unparalleled maximal throughput
    of 63.1 TOPS at an energy efficiency of 9.76 TOPS/W for 8-bit input/output [MVMs](#page-41-2).


    **Samsung''s MRAM crossbar:** Crossbar-based analog [MVM](#page-41-2) is well-explored
    in RRAM and PCM technologies. However, implementing MRAM-based crossbars is challenging
    due to the inherent low resistance of these devices, which could lead to significant
    power consumption. In 2022, Samsung presented a 64x64 MRAM crossbar array to address
    the low-resistance issue by employing an architecture that uses resistance summation
    (instead of current summation) for analog multiplyaccumulate operations [\[72\]](#page-37-3).
    Compared to the IBM HERMES cores, Samsung''s crossbar is significantly less sophisticates
    and limited in scale.


    <span id="page-23-0"></span>**TSMC''s in-SRAM accelerator:** While other SRAM-based
    [CIM](#page-39-0) chips exist, our focus is on the TSMC macro structure using
    standard 8T cells [\[78\]](#page-37-9) due to its better noise margin, ensuring
    stable activation for multiple rows operations in the [CIM](#page-39-0) mode,
    albeit with approximately 30% increased area.


    ![](_page_23_Figure_3.jpeg)


    Fig. 22. TSMC''s [CIM](#page-39-0) SRAM structure [\[78\]](#page-37-9).


    The proposed design shown in Figure [22](#page-23-0) has a 64×64 SRAM array and
    enables parallel computations of the multiply-and-average (MAV) operations. In
    a single cycle, the MAV computation of 64 4-bit inputs with 16 4-bit weight can
    be completed. The 4-bit input is represented by the number of read word line pulses
    which is precisely controlled by 4-bit digital counters. The 4-bit weight is achieved
    through charge sharing across binary-weighted computation capacitors. Each computation
    capacitor unit is constructed using the inherent capacitor of the [SA](#page-41-5)
    within the 4-bit flash ADC to optimize space and minimize the kick-back effect.
    This 64x64 8T macro is fabricated using 7nm FinFET technology, exhibiting an energy
    efficiency of 351 TOPS/W and a throughput of 372.4 GOPS for 1024 (64x16) 4x4b
    MAV operations.


    **Intel''s SRAM-based analog CIM design:** Intel has recently proposed an SRAM-based
    [CIM](#page-39-0) macro utilizing their 22nm Low-Power FinFET process [\[79\]](#page-37-10).
    Through the implementation of a 1-to-2 ratioed capacitor ladder (C-2C)-based charge
    domain computing scheme, the presented prototype chip (shown in Figure [23\)](#page-24-1)
    achieves the capability to perform up to 2k MAC operations in a single clock cycle,
    alongside achieving a peak power efficiency of 32.2-TOPS/W with 8-bit precision
    for both input activation and weights. The chip also ensures accurate [MVMs](#page-41-2)
    by restricting the computation error of less than 0.5%.


    **Bosch+Fraunhofer and GlobalFoundries+Fraunhofer FeFET based CIM designs:** Fraunhofer
    is also actively working on exploring the manufacturability and scalability aspects
    of FeFET and MRAM devices at both the device and array levels. Together with GlobalFoundries,
    they have demonstrated a


    <span id="page-24-1"></span>![](_page_24_Figure_1.jpeg)


    Fig. 23. Chip level architecture diagram of Intel''s analog [CIM](#page-39-0)
    design [\[79\]](#page-37-10).


    FeFET-based crossbar array for multiply-accumulate (MAC) operation [\[80\]](#page-37-11).
    The array was fabricated at GlobalFoundries with 28nm CMOS technology coupled
    with FeFET. To prevent the accumulation of errors on the bitline, the arrays were
    divided into 8×8 segments.


    In a recent work, Fraunhofer and Robert Bosch demonstrated a [CIM](#page-39-0)
    crossbar using multi-level FeFET cells. In the proposed design, the input is encoded
    into the applied voltage duration and magnitude while the weights are stored in
    the multi-level FeFET cells. The MAC output is the accumulated capacitor voltage
    that depends on the activation time and the number of FeFETs activated. This reportedly
    reduces the impact of variations and the achieved performance of 885.4 TOPS/W
    is also nearly-double compared to existing solutions.


    **HP''s CAM designs:** In a recent work, Hewlett Packard Labs proposed a memristive-based
    analog CAM for tree-based machine learning [\[81\]](#page-37-12). Analog CAMs
    are capable of performing searches based on analog signal levels rather than digital
    data comparison. The proposed design combines analog CAMs with traditional analog
    RAM and accelerates large random forest models with it. Figure [24](#page-25-1)
    shows a high-level overview of the proposed system where the analog CAM can perform
    root-to-leaf evaluation of an entire tree in a single step.


    #### <span id="page-24-0"></span>**4.3 Comparative analysis and discussion**


    The surge in [CIM](#page-39-0) and [CNM](#page-39-1) systems is largely attributed
    to the revolution in data-intensive applications. According to recent research
    by TSMC, traditional SRAM and DRAM technologies have effectively scaled to meet
    capacity and bandwidth demands in the past decades [\[82\]](#page-37-13), but
    their future scalability is uncertain due to reaching inherent technological limits.
    This underscores the pivotal role that NVM will play in the future of computing.


    Especially in edge scenarios such as automotive, augmented reality, and AI, where
    energy efficiency is paramount, NVM technologies are poised to play a pivotal
    role. As energy efficiency increases through specialized hardware, domain-specific
    architectures harnessing these NVMs for [CIM](#page-39-0) and [CNM](#page-39-1)
    solutions are anticipated to experience an unprecedented surge in the coming years.
    A recent article from Intel [\[79\]](#page-37-10) compares the performance of
    conventional digital accelerators with the emerging analog and digital [CIM](#page-39-0)
    and [CNM](#page-39-1) accelerators. Conventional accelerators still achieve higher
    throughput because [CIM](#page-39-0) systems are relatively less optimized, array
    sizes are small, and the peripheral circuitry overhead is non-negligible. Yet,
    they are orders of magnitude better in terms of


    <span id="page-25-1"></span>![](_page_25_Figure_1.jpeg)


    Fig. 24. An overview of the HP [CIM](#page-39-0) system for tree-based learning
    [\[81\]](#page-37-12).


    power consumption. As of the time of writing, the most recent comparison depicted
    in [\[82\]](#page-37-13) shows similar trends.


    ![](_page_25_Figure_4.jpeg)


    <span id="page-25-2"></span>Fig. 25. Performance and power comparison of different
    *outside memory processing* [\(OMP\)](#page-40-22) (we call it [COM](#page-39-2)
    in this report), [NMP](#page-39-9) (ours [CNM\)](#page-39-1) and [IMP](#page-39-4)
    (ours [CIM\)](#page-39-0) [\[83\]](#page-37-14).


    Table [2](#page-26-0) presents a summary and comparison of the architectures discussed
    in this section. For brevity, we only compare important parameters, such as the
    underlying memory technology, available function (boolean logic, arithmetic, etc.),
    evaluation technique (simulation, prototype, analytic), programming model, application
    domain, and technology node.


    #### <span id="page-25-0"></span>**5 COMMERCIAL LANDSCAPE**


    This section overviews CIM and CNM companies/startups, highlighting their products,
    underlying technologies, customers (when known), and tools. As not everything
    about companies is public,


    <span id="page-26-0"></span>


    | Accelerator                                | Year | Technology                     |
    Type | Programming<br>model | Logic unit                   | Implementation                   |
    Domain              |

    |--------------------------------------------|------|--------------------------------|------|----------------------|------------------------------|----------------------------------|---------------------|

    | McDRAM                                     | 2018 | DRAM                           |
    CNM  | Extended ISA         | MAC                          | Hardware                         |
    AI                  |

    | MViD                                       | 2020 | DRAM                           |
    CNM  | Extended ISA         | MAC                          | Hardware                         |
    AI                  |

    | PIM-HBM                                    | 2021 | DRAM                           |
    CNM  | Full<br>software     | FPUs (add, Mul)              | Hardware                         |
    AI                  |

    | AiM                                        | 2022 | (HBM)<br>DRAM<br>(GDDR6)       |
    CNM  | stack<br>API         | MAC                          | Hardware                         |
    AI                  |

    | AxRAM                                      | 2018 | DRAM (GPU                      |
    CNM  | API                  | MAC, LUTs                    | GPGPU-Sim                        |
    AI                  |

    | TESSERACT                                  | 2015 | based system)<br>DRAM<br>(HMC)
    | CNM  | API                  | CPU                          | Simulation                       |
    Graph processing    |

    | TOP-PIM                                    | 2014 | DRAM<br>(HMC)                  |
    CNM  | OpenCL               | CPU+GPU                      | Simulation                       |
    Graph, HPC          |

    | AMC                                        | 2015 | DRAM                           |
    CNM  | OpenMP               | CPU                          | Simulation                       |
    HPC                 |

    | HRL                                        | 2015 | (HMC)<br>DRAM<br>(HMC)         |
    CNM  | MapReduce            | CGRA+FPGA                    | Simulation                       |
    Data analytics      |

    | CIM architectures (Academia/Research Labs) |      |                                |      |                      |                              |                                  |                     |

    | ISAAC                                      | 2016 | RRAM                           |
    CIM  | NA                   | Analog Xbar                  | Analytical                       |
    AI                  |

    | PUMA                                       | 2019 | RRAM                           |
    CIM  | Compiler             | Xbar                         | PUMAsim<br>(arch.<br>simulation)
    | AI                  |

    | Pinatubo                                   | 2016 | RRAM                           |
    CIM  | API, Runtime         | Boolean logic                | In-house simulator               |
    Bitwise Logic       |

    | PRIME                                      | 2016 | RRAM                           |
    CIM  | Compiler+API         | Xbar                         | Analytical                       |
    AI                  |

    | PipeLayer                                  | 2017 | RRAM                           |
    CIM  | API                  | Xbar                         | Analytical                       |
    CNN (train + infer) |

    | AtomLayer                                  | 2018 | RRAM                           |
    CIM  | NA                   | Xbar                         | Analytical                       |
    CNN (train + infer) |

    | RIMAC                                      | 2023 | RRAM                           |
    CIM  | NA                   | Xbar<br>(without<br>DAC/ADC) | In-house simulator               |
    DNN inference       |


    Table 2. A summary of the presented architectures. They are grouped into three
    categories: [CNM,](#page-39-1) [CIM,](#page-39-0) and [CIM](#page-39-0) (prototype
    chips/systems). All presented architectures are either simulation-based or prototype-based
    (no products).


    we only include details that we extract from these companies'' websites or are
    known to us via our network.


    #### **5.1 Axelera**


    <span id="page-26-1"></span>Axelera [\[84\]](#page-37-15) is one of the notable
    Semiconductor startups in Europe. Founded in 2021 and backed by tech giants like
    Bitfury and IMEC, it had already taped out its first CIM chip, Thetis, in December
    2021 (just four months after its founding). Today, it offers a fully integrated
    *system-on-chip* [\(SoC\)](#page-40-23) powered by its Metis AI processing units
    (AIPU).


    About the AI core, as per the company''s website: "Axelera AI has fundamentally
    changed the architecture of "compute-in-place" by introducing an SRAM-based digital
    in-memory computing (D-IMC) engine. In contrast to analog in-memory computing
    approaches, Axelera''s D-IMC design is immune to noise and memory non-idealities
    that affect the precision of the analog matrix-vector operations as well as the
    deterministic nature and repeatability of the matrix-vector multiplication results.
    Our D-IMC supports INT8 activations and weights, but the accumulation maintains
    full precision at INT32, which enables state-of-the-art FP32 iso-accuracy for
    a wide range of applications without the need for retraining".


    Axelera''s latest SoC consists of 4 cores and a RISC-V based control core. For
    programming these systems, Axelera provides an end-to-end integrated framework
    for application development. The high-level framework takes users along the development
    processes without needing to understand the underlying architecture or even the
    machine learning concepts.


    **Funding:** "Axelera AI, the provider of the world''s most powerful and advanced
    solutions for AI at the Edge, announces new investors who have joined their oversubscribed
    Series A round, bringing the total amount raised to \$50 million. In the last
    several months, CDP Venture Capital, Verve Ventures, and Fractionelera have joined
    the round", Axelera AI, May 22, 2023.


    # **5.2 d-Matrix**


    d-Matrix is at the forefront of driving the transformation in data center architecture
    toward digital in-memory computing (DIMC) [\[85\]](#page-37-16). Founded in 2019,
    the company has received substantial support from prominent investors and strategic
    partners, including Playground Global, M12 (Microsoft Venture Fund), SK Hynix,
    Nautilus Venture Partners, Marvell Technology, and Entrada Ventures.


    Leveraging their in-SRAM digital computing techniques, a chipset-based design,
    high-bandwidth BoW interconnects, and a full stack of machine learning and large
    language model tools and software, d-Matrix pioneers best-performing solutions
    for large-scale inference requirements. A full stack framework, compiler, and
    APIs (open-source as per the company''s website but couldn''t find the link).
    Their latest product Jayhawk II can scale up to 150 TOPS/W using 6nm technology
    and can handle LLM models up to 20× more inferences per second for LLM sizing
    to 40B parameters, compared to state-of-the-art GPUs.


    **Funding:** Temasek, Playground Global and Microsoft Corp.


    # **5.3 Gyrfalcon Technology**


    Gyrfalcon Technology [\[86\]](#page-37-17) also leverages CNM to accelerate AI
    on the edge. They offer an AI processing in memory (APiM) architecture that combines
    a large MAC array directly with MRAM memory modules. As of the current date, their
    software stack is not available.


    **Funding:** Private.


    # **5.4 MemComputing**


    MemComputing [\[87\]](#page-37-18), founded in 2016, uses a computational memory
    based on its self-organizing logic gates (SOLG). SOLGs are terminal-agnostic elements
    (memristor or memcapacitor) that implement various logic gates. Their target applications
    comprise industrial computations associated with optimizations, big data analytics,
    and machine learning. MemComputing provides a software stack and offers it as
    a software-as-a-service.


    **Funding:** MemComputing mentions the US Space Force, ENSOS, NASA, Ball Aerospace,
    PSA, US Air Force, Canvass Labs and Defence Innovation Unit as partners.


    ### **5.5 Memverge**


    Memverge [\[88\]](#page-37-19) is not directly doing any CIM or CNM but is relevant
    in the context. Backed by 9 investors including tech giants like Intel, SK hynix,
    the company''s main goal is to provide software designed to accelerate and optimize
    data-intensive applications. Their main target is to consider environments with
    "Endless Memory" and efficiently manage the memory to get more performance.


    **Latest news**: "Samsung, MemVerge, H3 Platform, and XConn, today unveiled a
    2TB Pooled CXL Memory System at Flash Memory Summit. The system addresses performance
    challenges faced by highly distributed AI/ML applications. These challenges include
    issues like spilling memory to slow storage when main memory is full, excessive
    memory copying, I/O to storage, serialization/deserialization, and Out-of-Memory
    errors that can crash an application.", MemVerge, August 8, 2023.


    # **5.6 Mythic**


    Mythic [\[89\]](#page-37-20) offers an analog matrix processor (Mythic AMP) that
    uses their analog compute engine (ACE) based on flash memory array and ADCs. Mythic
    ACE also has a 32b RISC V processor, SIMD vector engine, and a 64KB SRAM along
    with a high-throughput network-on-chip (NoC). Mythic workflow in Figure [26](#page-28-0)
    shows that the software stack takes a trained NN model, optimizes it, and compiles
    it to generate code for Mythic AMP. The optimization suit also transforms NN in
    a way that can be accelerated on the analog CIM system.


    <span id="page-28-0"></span>![](_page_28_Figure_3.jpeg)


    Fig. 26. Mythic AI workflow [\[89\]](#page-37-20).


    **Funding:** The company is supported by many investors: Micron, HP Enterprise,
    SoftBank, Future ventures, Lam Research, Threshold, Catapult, DCVC and UDC ventures.


    # **5.7 NeuroBlad**


    Founded in 2018, NeuroBlade offers the SPU (SQL Processing Unit), the industry''s
    first, proven processor architecture that delivers orders of magnitude improvement
    by departing from Von Neumann model [\[90\]](#page-37-21). Neuroblade is also
    a CNM architecture (more closed to near-storage computing) where they integrate
    custom RISC processors on the DRAM chip (very similar to UPMEM). The SPUs are
    installed as PCI-e cards that can be deployed in data centers. As for the software
    stack, the company offers an SDK along with a set of APIs that hide the complexity
    and programming model for these cores from the end user and also allow optimizing
    for maximum parallelism and efficiency.


    **Funding:** NeuroBlade is funded by Stage one, Grove Ventures, UMC, PSMC Intel
    capitals, Pegratron, Marubeni, Marius Nacht, Corner and MediaTek.


    # **5.8 Rain AI**


    Founded in 2017, Rain AI also focuses on radically cheaper AI computing [\[91\]](#page-37-22).
    The company has no hardware product yet but is aiming to be 100× better than GPU
    using their innovations in radical co-design (by looking at the algorithms and
    the CIM hardware at the same time). They are targeting AI training (along with
    the inference) on the edge with the ultimate goal of putting models the size of
    ChatGPT into chips of the size of a thumbnail. They are transforming the algorithms
    in a way that fundamentally matches the behavior of the analog memristive devices.
    As per the CEO, they have a few tap-outs planned for this year and the product
    (a complete platform) next year and they are working on a software stack for ease
    of use and ease of integration.


    **Funding:** The company is funded by Y combinator S18, Sam Altman (CEO OpenAI),
    Liquid 2 Ventures, Loup Ventures, Airbus Ventures, and Daniel Gross (founder Poineer).


    # **5.9 SEMRON**


    Founded in 2020, Semron [\[92\]](#page-37-23) promises to offer 3D solutions powered
    by analog CIM. At the core of their technology is their innovative CapRAM devices
    which are semiconductor devices that store multi-bit values in their variable
    capacitances (unlike variable resistance states in memristors). Since CapRAM is
    capacitive, the noise in calculations is much lower and the energy efficiency,
    as per their website, is unparalleled. Although Semron has the device technology,
    there are no details of its products, architecture, and software stack.


    **Funding:** As per crunchbase, the company is funded by VentureOut.


    # **5.10 SureCore**


    Surecore [\[93\]](#page-37-24) is working on many low-power products including
    custom application-specific. They also have a product named "CompuRAM" that embeds
    arithmetic capability within the SRAM array to enable low-power AI on the edge.
    Besides working on SRAM-based solutions, in collaboration with Intrinsic, they
    have recently ventured into RRAM technology. No information is provided regarding
    the software stack.


    **Funding:** The company is supported by Capital-E, Finance Yorkshire and Mercia
    Technologies.


    # **5.11 Synthara**


    Synthara is a Zurich-based Semiconductor company that was founded in 2017 [\[94\]](#page-37-25).
    Their latest product, ComputeRAM, integrates SRAM-based CIM macros with proprietary
    elements to accelerate dot products. The solution delivers 50× compute efficiency
    and can be used for AI, digital signal processing, and linear algebra-heavy routines.
    The CIM-powered SRAM array can be operated just like conventional SRAM. ComputeRAM
    is not married to a specific ISA and can work with any host processor. Synthara
    also provides what they call Compiler hooks that can transparently offload any
    input application to their ComputeRAM accelerator, without changing or rewriting
    the code.


    **Funding:** The company is supported by EU funding for research & innovation,
    High-tech Gründerfonds, Intel.ignite, FNSNF, multicoreware, ventureKick and others.


    # **5.12 Syntiant**


    Founded in 2017, Syntiant also leverages DRAM-based [CNM](#page-39-1) and utilizes
    standard CMOS processes to design their neural decision processors (NDPs) that
    perform direct processing of neural network layers from platforms like TensorFlow
    [\[95\]](#page-37-26). Syntiant also mainly targets AI on the edge having applications
    in many domains, including always-on voice, audio, image, and sensor applications.


    Syntiant''s TinyML platform, powered by NDP101, aspires to democratize AI by presenting
    a comprehensive system for those interested in initiating their own model training
    for edge computing. **Funding:** Syntiant is funded by prominent investors including,
    Atlantic Bridge, Rober Bosch Venture Capital, Embark Ventures, DHVC, Intel capitals,
    M12 (Microsoft ventures), and Motorola Solutions.


    # **5.13 TetraMem**


    Founded in 2018, TetraMem is set to offer the industry''s most disruptive CIM
    technology for edge application [\[96\]](#page-37-27). TetraMem is also leveraging
    memristors for analog MAC operations, aiming at inference on the edge. Their systems
    are built upon their patented devices and co-design solutions.


    TetraMem offers (1) Platform as a service (PaaS), a complete hardware and software
    platform designed to integrate into your own system; (2) Software as a service
    (SaaS), to help develop your NN edge application and integrate it into your system.
    Their verified full software stack provides an unmatched experience on actual
    analog in-memory compute silicon; and (3) a neural processing unit (NPU) based
    on memristive technology.


    TetraMem has recently announced a collaboration with Andes Technologies and together
    with their research collaborators have demonstrated a memristive device that can
    have thousands of conductance levels (unmatched) [\[97\]](#page-37-28).


    #### **Funding:** Private.


    # **5.14 EnCharge AI**


    Founded in 2022, EnCharge AI promise to offer an end-to-end scalable architecture
    for AI inference [\[98\]](#page-37-29). They leverage SRAM-based CIM arrays for
    analog MVM operations and combine them with SIMD CNM logic to perform custom element-wise
    operations. The architecture comprises an array of CIM units (CIMUs), an on-chip
    network interconnecting CIMUs, buffers, control circuitry, and off-chip interfaces.
    Each CIMU is equipped with an SRAM-based CIM array featuring ADCs to convert computed
    outputs into digital values. Additionally, CIMUs house SIMD units and FPUs with
    a custom instruction set, along with buffers dedicated to both computation and
    data flow. According to the company''s official website, they offer a software
    platform that fits with standard ML frameworks, such as PyTorch, TensorFlow, and
    ONNX. This also allows the implementation of various ML models and their customizations.
    Specific implementation details about the software stack are not available.


    **Funding:** Encharge AI is funded by AlleyCorp, Scout Ventures, Silicon Catalyst
    Angels, Schams Ventures, E14 Fund, and Alumni Ventures. At their launch in December
    2022, they announced securing \$21.7 Mio. in their series A round.


    ### **5.15 Re(conceive) AI**


    Re(conceive) is another CIM startup founded in 2019 that promises offering "the
    most power AI accelerator" [\[99\]](#page-38-0). As per their website, re(conceive)
    are pioneers in realizing the complete potential of CMOS-based analog in-memory
    AI computing, achieving the utmost efficiency among all known AI accelerators.
    However, no specific details are available on the company''s funding and technology
    (hardware/software).


    ### **5.16 Fractile AI**


    Established in 2022 by a team of Oxford University scientists, Fractile [\[100\]](#page-38-1)
    aims to transform the world by enabling large language models'' (LLM) inference
    at speeds up to 100 times faster than Nvidia''s most recent H100 GPUs. This increase
    in performance primarily arises from in-memory computations. However, the details
    of the technology, both hardware and software, as well as the company''s funding
    particulars, remain undisclosed.


    # **5.17 Untether AI**


    Founded in 2018 [\[101\]](#page-38-2), Untether''s main design integrates RISC-V
    cores on the SRAM chips for processing AI workloads. Their latest product, the
    tsunAImi accelerator card provides a phenomenal 2 POPS of compute power, twice
    the amount of any available product. This compute power translates into over 80,000
    frames per second of ResNet-50 throughput, three times the throughput of any product
    on the market. Untether AI provides an automated SDK for its products. The SDK
    takes a network model implemented in common machine learning frameworks like TensorFlow
    and PyTorch and lowers it into the kernel code that runs on these RISC-V processors.
    It automatically takes care of low-level optimizations, providing extensive visualization,
    a cycle-accurate simulator, and an easily adoptable runtime API.


    <span id="page-31-1"></span>


    | Company           | Use-Case                        | Technology                    |
    Solution                          | Programmability                                        |
    Funding<br>(Mio.<br>\$)<br>(PitchBook) |

    |-------------------|---------------------------------|-------------------------------|-----------------------------------|--------------------------------------------------------|----------------------------------------|

    | Axelera           | AI on the Edge                  | SRAM (digital MAC)            |
    Hardware-SoC                      | SDK provided                                           |
    63.72 (Early stage VC)                 |

    | d-Matrix          | AI inference in dat<br>acenters | SRAM (digital MAC)            |
    Chiplets                          | Open-source<br>Frame<br>work(s)                        |
    161.3 (Early stage VC)                 |

    | Synthara          | AI, DSP, Linear al<br>gebra     | SRAM (dot product)            |
    Accelerator                       | Compiler available                                     |
    3.33 (Grant)                           |

    | Mythic            | AI on the Edge                  | Flash (Analog computing)      |
    Accelerator,<br>pro<br>cessor     | Software<br>stack<br>(does<br>rewriting, opt,
    mapping) | 177.41 (Later stage VC)                |

    | Surecore          | AI on the edge                  | SRAM (CNM)                    |
    Chip                              | No details                                             |
    11.16 (Later stage VC)                 |

    | SEMRON            | AI on the edge                  | Memcapacitor                  |
    3D-Chip (planned)                 | No details                                             |
    1.63 (Seed round)                      |

    | Untether AI       | AI everywhere                   | SRAM+RISC-V (CNM)             |
    Chips, accelerator                | SDK<br>and<br>simulator<br>(Toolkit)                   |
    153.52 (Early stage VC)                |

    | Syntiant          | AI on the edge                  | SRAM+ARM<br>MCUs<br>(CNM)     |
    Processor                         | Available                                              |
    121.43 (Later stage VC)                |

    | Neuroblade        | Analytics                       | DRAM+RISC<br>cores<br>(CNM)   |
    Processor                         | Set of APIs                                            |
    110.43 (Debt - General)                |

    | Rain AI           | LLMs on the edge<br>(Training)  | Memristors                    |
    Processors                        | NA                                                     |
    64.04 (Later stage VC)                 |

    | TetraMem          | Edge applications               | Memristors                    |
    Processors,<br>soft<br>ware stack | HDK, SDK                                               |
    NA                                     |

    | Gyrfalcon<br>Tech | AI on the edge                  | CNM<br>(MACs<br>with<br>MRAM)
    | Chip                              | NA                                                     |
    68.0 (Debt - PPP)                      |

    | UPMEM             | General-purpose                 | DRAM+RISC cores               |
    System                            | APIs                                                   |
    15.5 (Later stage VC)                  |

    | EnCharge AI       | AI inference                    | SRAM (CIM) + SIMD<br>(CNM)    |
    Chip                              | Software available                                     |
    21.7 (Angel - individ)                 |

    | Re(conveive)      | AI inference                    | SRAM (analog CIM)             |
    Chip                              | NA                                                     |
    NA                                     |

    | Fractile          | LLMs inference                  | NA                            |
    Chip                              | NA                                                     |
    NA                                     |


    Table 3. CIM/CNM companies, with their products, technologies, and funding status.


    **Funding:** Untether''s investors include CPPIB, GM Ventures, Intel Capital,
    Radical Ventures, and Tracker Capital.


    # **5.18 UPMEM Technology**


    Founded in 2015, UPMEM is a tech company offering programmable CNM systems for
    data-intensive applications. See more details on the architecture and programmability
    in Section [4.1.1.](#page-10-2)


    **Funding:** The company is funded by Western Digital, Partech, and super nova
    invest.


    ### **5.19 Summary**


    Table [3](#page-31-1) and Figure [27](#page-32-1) summarize the discussion in
    this section and provides a landscape of CIM, CNM companies, their products, technologies,
    and funding status. Please note that this compilation is not exhaustive; it includes
    only companies known to us and those that, based on our understanding, fall within
    the CIM and CNM categories. As Figure [27](#page-32-1) clearly illustrates, the
    current landscape is predominantly characterized by conventional technologies,
    with a notable absence of a comprehensive software ecosystem.


    # <span id="page-31-0"></span>**5.20 Open challenges**


    [CIM](#page-39-0) and [CNM](#page-39-1) systems have already entered the market,
    yet a series of open challenges are expected to become more pronounced as time
    progresses. It will take years to understand how these units will harmonize within
    the overall system architecture and determine their optimal utilization. In the
    following, we briefly discuss the important categories.


    **Materials:** During the era of Moore''s law in computing, the primary focus
    was on refining transistors to be smaller, faster, and more energy-efficient.
    The selection of materials was confined to only


    #### CNM/CIM Landscape 33


    <span id="page-32-1"></span>![](_page_32_Figure_1.jpeg)


    ![](_page_32_Figure_2.jpeg)


    those compatible with manufacturing processes. However, the limitations of these
    materials to scale further are now exposed. As a result, new materials have emerged
    and further research is needed to investigate novel materials (to enable further
    transistor scaling: hopes with carbon nanotube, and novel memory devices).


    **Devices:** Mainstream computing has largely relied on digital logic and binary
    storage. Nonetheless, the emerging wave of computing architectures, particularly
    [CIM](#page-39-0) requires novel multi-state devices allowing both analog and
    digital operations. Existing devices, memristors in particular, do offer such
    properties but have reliability and other associated challenges.


    **Integration:** We have seen various architectures based on various technologies.
    As is evident, there is no on-technology-fits-all solution. Eventually, [CIM](#page-39-0)
    modules based on different technologies need to be integrated into the same to
    get the best out of all these technologies. This poses integration challenges
    that have received little to no attention.


    **Processing systems:** These novel architectures require new tools, algorithms,
    cost models, and software solutions. All of them are crucial to understanding
    these architectures, enabling their design space exploration, and making them
    accessible to a larger audience.


    While every challenge holds significance and demands attention, programmability
    and userfriendliness are the most important ones from the user''s standpoint.
    Following is an excerpt from Facebook''s recent article on their inference accelerator
    that highlights the same.


    *"We''ve investigated applying processing-in-memory (PIM) to our workloads and
    determined there are several challenges to using these approaches. Perhaps the
    biggest challenge of PIM is its programmability".*


    In response to the challenges associated with programmability, we have ourselves
    been working on high-level programming and compilation frameworks for CNM and
    CIM systems [\[102](#page-38-3)[–105\]](#page-38-4). We have developed reusable
    abstractions and demonstrated compilation flows for CIM systems with memristive
    crossbars, CAMs, CIM-logic modules, and for CNM systems like UPMEM and Samsung
    CNM. However, much more cross-layer work is needed to improve automation [\[106\]](#page-38-5),
    in particular for heterogeneous systems integrating several paradigms and technologies.


    #### <span id="page-32-0"></span>**6 CONCLUSIONS**


    This paper overviews the landscape of compute-near-memory (CNM) and compute-in-memory
    (CIM) paradigms. It starts with an explanation of the Von Neumann bottleneck,
    the necessity of novel CIM/CNM paradigms, and the key terminology used in the
    related literature. It offers a comprehensive


    background on major memory technologies and emphasizes the importance of heterogeneous
    systems. The paper overviews prominent CIM and CNM designs from both academia
    and industry. In contrast to other studies in the literature that focus on either
    application domains or memory technologies, this paper concentrates on designs
    that have either successfully transitioned into product offerings or have reached
    a stage where commercialization is a feasible prospect. We explain prevalent CNM
    architectures, including microarchitectural details, associated technologies,
    software frameworks, and the results achieved (usually measured as throughput).
    Subsequently, we survey the landscape of CIM systems, explaining prevailing CIM
    designs that use prominent technologies such as SRAM, DRAM, MRAM, RRAM, PCM, and
    FeFET. We overview CIM chips from industrial giants (research centers), spanning
    from earlier designs like ISAAC and PUMA by Hewlett Packard Enterprise to the
    most cutting-edge chips from IBM, Samsung, TSMC, Intel, Meta (Facebook), Bosch,
    Frauenhofer, and GlobalFoundries. Current trends in industrial research show that
    while conventional SRAM and DRAM technologies are ready to be leveraged in CIM/CNM
    systems, emerging technologies like PCM, RRAM, MRAM, and FeFETs are also poised
    to make partial inroads, particularly for selected operations, such as dot products
    and pattern matching.


    Finally, we describe the landscape of CIM and CNM start-ups, highlighting the
    emergence of numerous new companies in recent years that have introduced innovative
    solutions to cater to the thriving demands of AI and other data-intensive application
    domains. These companies are targeting a diverse range of market segments, spanning
    from power-efficient edge applications (AI at the edge) to high-performance data
    center servers (e.g., for AI training), and many have successfully secured substantial
    funding (hundreds of millions) in their initial funding rounds. The paper shows
    that SRAM technology currently dominates this landscape. However, with active
    research and breakthroughs in emerging NVMs (demonstrated by recent industrial
    chips), it is anticipated that NVMs will play a more prominent role in these paradigms
    in the near future.


    The paper highlights that CIM and CNM technologies (i) harbor significant potential
    to outperform conventional systems, and (ii) have already made inroads into the
    market. However, their true potential remains untapped. This is attributed to
    a number of challenges, including the lack of accurate design space exploration
    tools, programming frameworks, and a comprehensive software ecosystem in general,
    and cost and performance models that can be leveraged to guide static and runtime
    optimizations for these systems.


    CNM and CIM computing is an extremely active field. We believe that we have captured
    a representative snapshot of this field, early in year 2024, and remain excited
    about how technologies, devices, architectures and tools will continue to develop
    moving forward.


    #### **ACKNOWLEDGEMENTS**


    This work was supported by Vsquared Ventures (VSQ). Special thanks to Max Odendahl
    (Venture Partner at VSQ) for his feedback on previous versions of the manuscript.
    This work was also supported by the German Research Council (DFG) through the
    HetCIM project (project number 502388442) in the context of the DFG Priority Program
    on Disruptive Memory Technologies (SPP2377 [https://spp2377.uos.de\)](https://spp2377.uos.de)
    and the German Federal Ministry of Education and Research (BMBF, project number
    01IS18026A-D) by funding the competence center for Big Data and AI ScaDS.AI Dresden/Leipzig
    [\(https://scads.ai\)](https://scads.ai).


    #### **REFERENCES**


    <span id="page-33-0"></span>[1] S. Li, A. O. Glova, X. Hu, P. Gu, D. Niu, K. T.
    Malladi, H. Zheng, B. Brennan, and Y. Xie, "Scope: A stochastic computing engine
    for dram-based in-situ accelerator," in *2018 51st Annual IEEE/ACM International
    Symposium on Microarchitecture (MICRO)*. IEEE, 2018, pp. 696–709.


    - <span id="page-34-0"></span>[2] A. de Vries, "The growing energy footprint of
    artificial intelligence," *Joule*, vol. 9, no. 4, p. 1–4, Oct 2023. [Online].
    Available:<https://doi.org/10.1016/j.joule.2023.09.004>

    - <span id="page-34-1"></span>[3] J. Calma. (2023, September) Microsoft is going
    nuclear to power its ai ambitions. The Verge. [Online]. Available: <https://www.theverge.com/2023/9/26/23889956/microsoft-next-generation-nuclear-energy-smr-job-hiring>

    - <span id="page-34-2"></span>[4] A. Reuther, P. Michaleas, M. Jones, V. Gadepally,
    S. Samsi, and J. Kepner, "Ai and ml accelerator survey and trends," in *2022 IEEE
    High Performance Extreme Computing Conference (HPEC)*. IEEE, 2022, pp. 1–10.

    - <span id="page-34-3"></span>[5] F. Devaux, "The true processing in memory accelerator,"
    in *2019 IEEE Hot Chips 31 Symposium (HCS)*. IEEE Computer Society, 2019, pp.
    1–24.

    - <span id="page-34-4"></span>[6] Y.-C. Kwon, S. H. Lee, J. Lee, S.-H. Kwon, J.
    M. Ryu, J.-P. Son, O. Seongil, H.-S. Yu, H. Lee, S. Y. Kim *et al.*, "25.4 a 20nm
    6gb function-in-memory dram, based on hbm2 with a 1.2 tflops programmable computing
    unit using bank-level parallelism, for machine learning applications," in *2021
    IEEE International Solid-State Circuits Conference (ISSCC)*, vol. 64. IEEE, 2021,
    pp. 350–352.

    - <span id="page-34-5"></span>[7] S. Lee, S.-h. Kang, J. Lee, H. Kim, E. Lee,
    S. Seo, H. Yoon, S. Lee, K. Lim, H. Shin *et al.*, "Hardware architecture and
    software stack for pim based on commercial dram technology: Industrial product,"
    in *2021 ACM/IEEE 48th Annual International Symposium on Computer Architecture
    (ISCA)*. IEEE, 2021, pp. 43–56.

    - <span id="page-34-6"></span>[8] M. He, C. Song, I. Kim, C. Jeong, S. Kim, I.
    Park, M. Thottethodi, and T. Vijaykumar, "Newton: A dram-maker''s accelerator-in-memory
    (aim) architecture for machine learning," in *2020 53rd Annual IEEE/ACM International
    Symposium on Microarchitecture (MICRO)*. IEEE, 2020, pp. 372–385.

    - <span id="page-34-7"></span>[9] S. Lee, K. Kim, S. Oh, J. Park, G. Hong, D.
    Ka, K. Hwang, J. Park, K. Kang, J. Kim *et al.*, "A 1ynm 1.25 v 8gb, 16gb/s/pin
    gddr6-based accelerator-in-memory supporting 1tflops mac operation and various
    activation functions for deep-learning applications," in *2022 IEEE International
    Solid-State Circuits Conference (ISSCC)*, vol. 65. IEEE, 2022, pp. 1–3.

    - <span id="page-34-8"></span>[10] H.-S. Wong and S. Salahuddin, "Memory leads
    the way to better computing," *Nature nanotechnology*, vol. 10, pp. 191–4, 03
    2015.

    - <span id="page-34-9"></span>[11] H.-S. P. Wong, H.-Y. Lee, S. Yu, Y.-S. Chen,
    Y. Wu, P.-S. Chen, B. Lee, F. T. Chen, and M.-J. Tsai, "Metal–oxide rram," *Proceedings
    of the IEEE*, vol. 100, no. 6, pp. 1951–1970, 2012.

    - <span id="page-34-10"></span>[12] W. J. Gallagher and S. S. P. Parkin, "Development
    of the magnetic tunnel junction mram at ibm: From first junctions to a 16-mb mram
    demonstrator chip," *IBM J. Res. Dev.*, vol. 50, no. 1, pp. 5–23, Jan. 2006. [Online].
    Available: <http://dx.doi.org/10.1147/rd.501.0005>

    - <span id="page-34-11"></span>[13] J. Hoffman, X. Pan, J. W. Reiner, F. J. Walker,
    J. P. Han, C. H. Ahn, and T. P. Ma, "Ferroelectric field effect transistors for
    memory applications," *Advanced Materials*, vol. 22, no. 26-27, pp. 2957–2961,
    2010. [Online]. Available: <https://onlinelibrary.wiley.com/doi/abs/10.1002/adma.200904327>

    - <span id="page-34-12"></span>[14] P. M. Research, "Market study on in-memory
    computing: Adoption of fast-processing databases fuels the demand. report pmrrep33026,"
    2022.

    - <span id="page-34-13"></span>[15] P. Radojković, P. Carpenter, P. Esmaili-Dokht,
    R. Cimadomo, H.-P. Charles, S. Abu, and P. Amato, "Processing in memory: the tipping
    point," *White paper: Processing in Memory: the Tipping Point*, 2021.

    - <span id="page-34-14"></span>[16] M. Anderson, B. Chen, S. Chen, S. Deng, J.
    Fix, M. Gschwind, A. Kalaiah, C. Kim, J. Lee, J. Liang *et al.*, "First-generation
    inference accelerator deployment at facebook," *arXiv preprint arXiv:2107.04140*,
    2021.

    - <span id="page-34-15"></span>[17] A. Sebastian, M. Le Gallo, R. Khaddam-Aljameh,
    and E. Eleftheriou, "Memory devices and applications for in-memory computing,"
    *Nature Nanotechnology*, pp. 1–16, 2020.

    - <span id="page-34-16"></span>[18] F. Ottati, G. Turvani, G. Masera, and M. Vacca,
    "Custom memory design for logic-in-memory: Drawbacks and improvements over conventional
    memories," *Electronics*, vol. 10, no. 18, p. 2291, 2021.

    - <span id="page-34-17"></span>[19] S. Kvatinsky, D. Belousov, S. Liman, G. Satat,
    N. Wald, E. G. Friedman, A. Kolodny, and U. C. Weiser, "Magic—memristor-aided
    logic," *IEEE Transactions on Circuits and Systems II: Express Briefs*, vol. 61,
    no. 11, pp. 895–899, 2014.

    - <span id="page-34-18"></span>[20] E. Lehtonen, J. Poikonen, and M. Laiho, *Memristive
    Stateful Logic*, 01 2014, pp. 603–623.

    - <span id="page-34-19"></span>[21] P. Chi et al., "Prime: A novel processing-in-memory
    architecture for neural network computation in reram-based main memory," in *ACM/IEEE
    43rd Annual International Symposium on Computer Architecture (ISCA)*, June 2016,
    pp. 27–39.

    - <span id="page-34-20"></span>[22] A. Shafiee et al., "Isaac: A convolutional
    neural network accelerator with in-situ analog arithmetic in crossbars," *ACM
    SIGARCH Computer Architecture News*, vol. 44, no. 3, pp. 14–26, 2016.

    - <span id="page-34-21"></span>[23] S. Li, C. Xu, Q. Zou, J. Zhao, Y. Lu, and
    Y. Xie, "Pinatubo: A processing-in-memory architecture for bulk bitwise operations
    in emerging non-volatile memories," in *Proceedings of the 53rd Annual Design
    Automation Conference*, 2016, pp. 1–6.

    - <span id="page-34-22"></span>[24] W. A. Simon, Y. M. Qureshi, M. Rios, A. Levisse,
    M. Zapater, and D. Atienza, "Blade: An in-cache computing architecture for edge
    devices," *IEEE Transactions on Computers*, vol. 69, no. 9, pp. 1349–1363, 2020.

    - <span id="page-35-0"></span>[25] V. Seshadri, D. Lee, T. Mullins, H. Hassan,
    A. Boroumand, J. Kim, M. A. Kozuch, O. Mutlu, P. B. Gibbons, and T. C. Mowry,
    "Ambit: In-memory accelerator for bulk bitwise operations using commodity dram
    technology," in *Proceedings of the 50th Annual IEEE/ACM International Symposium
    on Microarchitecture*, 2017, pp. 273–287.

    - <span id="page-35-1"></span>[26] D. Fakhry, M. Abdelsalam, M. W. El-Kharashi,
    and M. Safar, "A review on computational storage devices and near memory computing
    for high performance applications," *Memories-Materials, Devices, Circuits and
    Systems*, p. 100051, 2023.

    - <span id="page-35-2"></span>[27] G. Singh, L. Chelini, S. Corda, A. J. Awan,
    S. Stuijk, R. Jordans, H. Corporaal, and A.-J. Boonstra, "Near-memory computing:
    Past, present, and future," *Microprocessors and Microsystems*, vol. 71, p. 102868,
    2019. [Online]. Available:<https://www.sciencedirect.com/science/article/pii/S0141933119300389>

    - <span id="page-35-3"></span>[28] A. Gebregiorgis, H. A. Du Nguyen, J. Yu, R.
    Bishnoi, M. Taouil, F. Catthoor, and S. Hamdioui, "A survey on memory-centric
    computer architectures," *J. Emerg. Technol. Comput. Syst.*, vol. 18, no. 4, oct
    2022. [Online]. Available: <https://doi.org/10.1145/3544974>

    - <span id="page-35-4"></span>[29] F. Gao, G. Tziantzioulis, and D. Wentzlaff,
    "Computedram: In-memory compute using off-the-shelf drams," in *Proceedings of
    the 52nd annual IEEE/ACM international symposium on microarchitecture*, 2019,
    pp. 100–113.

    - <span id="page-35-5"></span>[30] M. Ali, A. Jaiswal, S. Kodge, A. Agrawal, I.
    Chakraborty, and K. Roy, "Imac: In-memory multi-bit multiplication and accumulation
    in 6t sram array," *IEEE Transactions on Circuits and Systems I: Regular Papers*,
    vol. 67, no. 8, pp. 2521–2531, 2020.

    - <span id="page-35-6"></span>[31] Z.-R. Wang, Y.-T. Su, Y. Li, Y.-X. Zhou, T.-J.
    Chu, K.-C. Chang, T.-C. Chang, T.-M. Tsai, S. M. Sze, and X.-S. Miao, "Functionally
    complete boolean logic in 1t1r resistive random access memory," *IEEE Electron
    Device Letters*, vol. 38, no. 2, pp. 179–182, 2016.

    - <span id="page-35-7"></span>[32] A. Kazemi, F. Müller, M. M. Sharifi, H. Errahmouni,
    G. Gerlach, T. Kämpfe, M. Imani, X. S. Hu, and M. Niemier, *Scientific reports*,
    vol. 12, no. 1, p. 19201, 2022.

    - <span id="page-35-8"></span>[33] R. Neale, D. Nelson, and G. E. Moore, "Nonvolatile
    and reprogrammable, the read-mostly memory is here," *Electronics*, vol. 43, no.
    20, pp. 56–60, 1970.

    - <span id="page-35-9"></span>[34] G. W. Burr, M. J. Brightsky, A. Sebastian,
    H.-Y. Cheng, J.-Y. Wu, S. Kim, N. E. Sosa, N. Papandreou, H.-L. Lung, H. Pozidis
    *et al.*, "Recent progress in phase-change memory technology," *IEEE Journal on
    Emerging and Selected Topics in Circuits and Systems*, vol. 6, no. 2, pp. 146–162,
    2016.

    - <span id="page-35-10"></span>[35] Z. Guo, J. Yin, Y. Bai, D. Zhu, K. Shi, G.
    Wang, K. Cao, and W. Zhao, "Spintronics for energy-efficient computing: An overview
    and outlook," *Proceedings of the IEEE*, vol. 109, no. 8, pp. 1398–1417, 2021.

    - <span id="page-35-11"></span>[36] A. Kent and D. Worledge, "A new spin on magnetic
    memories," *Nature nanotechnology*, vol. 10, pp. 187–91, 03 2015.

    - <span id="page-35-13"></span>[37] D. Reis, M. Niemier, and X. S. Hu, "Computing
    in memory with fefets," in *Proceedings of the international symposium on low
    power electronics and design*, 2018, pp. 1–6.

    - <span id="page-35-14"></span>[38] J. D. Kendall and S. Kumar, "The building
    blocks of a brain-inspired computer," *Applied Physics Reviews*, vol. 7, no. 1,
    2020.

    - <span id="page-35-12"></span>[39] V. Milo, G. Malavena, C. Compagnoni, and D.
    Ielmini, "Memristive and cmos devices for neuromorphic computing," *Materials*,
    vol. 13, p. 166, 01 2020.

    - <span id="page-35-15"></span>[40] D. Patterson, K. Asanovic, A. Brown, R. Fromm,
    J. Golbus, B. Gribstad, K. Keeton, C. Kozyrakis, D. Martin, S. Perissakis, R.
    Thomas, N. Treuhaft, and K. Yelick, "Intelligent ram (iram): the industrial setting,
    applications, and architectures," in *Proceedings International Conference on
    Computer Design VLSI in Computers and Processors*, 1997, pp. 2–7.

    - <span id="page-35-16"></span>[41] J. Draper, J. Chame, M. Hall, C. Steele, T.
    Barrett, J. LaCoss, J. Granacki, J. Shin, C. Chen, C. W. Kang *et al.*, "The architecture
    of the diva processing-in-memory chip," in *Proceedings of the 16th international
    conference on Supercomputing*, 2002, pp. 14–25.

    - <span id="page-35-17"></span>[42] Y. Kang, W. Huang, S.-M. Yoo, D. Keen, Z.
    Ge, V. Lam, P. Pattnaik, and J. Torrellas, "Flexram: Toward an advanced intelligent
    memory system," in *2012 IEEE 30th International Conference on Computer Design
    (ICCD)*. IEEE, 2012, pp. 5–14.

    - <span id="page-35-18"></span>[43] J. Gómez-Luna, I. E. Hajj, I. Fernandez, C.
    Giannoula, G. F. Oliveira, and O. Mutlu, "Benchmarking a new paradigm: An experimental
    analysis of a real processing-in-memory architecture," *arXiv preprint arXiv:2105.03814*,
    2021.

    - <span id="page-35-19"></span>[44] H. Shin, D. Kim, E. Park, S. Park, Y. Park,
    and S. Yoo, "Mcdram: Low latency and energy-efficient matrix computations in dram,"
    *IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems*,
    vol. 37, no. 11, pp. 2613–2622, 2018.

    - <span id="page-35-20"></span>[45] B. Kim, J. Chung, E. Lee, W. Jung, S. Lee,
    J. Choi, J. Park, M. Wi, S. Lee, and J. H. Ahn, "Mvid: Sparse matrix-vector multiplication
    in mobile dram for accelerating recurrent neural networks," *IEEE Transactions
    on Computers*, vol. 69, no. 7, pp. 955–967, 2020.

    - <span id="page-35-21"></span>[46] L. Ke, X. Zhang, J. So, J.-G. Lee, S.-H. Kang,
    S. Lee, S. Han, Y. Cho, J. H. Kim, Y. Kwon *et al.*, "Near-memory processing in
    action: Accelerating personalized recommendation with axdimm," *IEEE Micro*, vol.
    42, no. 1, pp. 116–127, 2021.

    - <span id="page-36-0"></span>[47] A. Ankit et al., "Puma: A programmable ultra-efficient
    memristor-based accelerator for machine learning inference," in *Proceedings of
    the Twenty-Fourth International Conference on Architectural Support for Programming
    Languages and Operating Systems*, 2019, pp. 715–731.

    - <span id="page-36-1"></span>[48] A. Yazdanbakhsh, C. Song, J. Sacks, P. Lotfi-Kamran,
    H. Esmaeilzadeh, and N. S. Kim, "In-dram near-data approximate acceleration for
    gpus," in *Proceedings of the 27th International Conference on Parallel Architectures
    and Compilation Techniques*, 2018, pp. 1–14.

    - <span id="page-36-2"></span>[49] J. Ahn, S. Hong, S. Yoo, O. Mutlu, and K. Choi,
    "A scalable processing-in-memory accelerator for parallel graph processing," in
    *Proceedings of the 42nd Annual International Symposium on Computer Architecture*,
    2015, pp. 105–117.

    - <span id="page-36-3"></span>[50] D. Zhang, N. Jayasena, A. Lyashevsky, J. L.
    Greathouse, L. Xu, and M. Ignatowski, "Top-pim: Throughput-oriented programmable
    processing in memory," in *Proceedings of the 23rd international symposium on
    High-performance parallel and distributed computing*, 2014, pp. 85–98.

    - <span id="page-36-4"></span>[51] R. Nair, S. F. Antao, C. Bertolli, P. Bose,
    J. R. Brunheroto, T. Chen, C.-Y. Cher, C. H. Costa, J. Doi, C. Evangelinos *et
    al.*, "Active memory cube: A processing-in-memory architecture for exascale systems,"
    *IBM Journal of Research and Development*, vol. 59, no. 2/3, pp. 17–1, 2015.

    - <span id="page-36-5"></span>[52] M. Gao and C. Kozyrakis, "Hrl: Efficient and
    flexible reconfigurable logic for near-data processing," in *2016 IEEE International
    Symposium on High Performance Computer Architecture (HPCA)*. Ieee, 2016, pp. 126–137.

    - <span id="page-36-6"></span>[53] D. Reis, A. F. Laguna, M. Niemier, and X. S.
    Hu, "In-memory computing accelerators for emerging learning paradigms," in *Proceedings
    of the 28th Asia and South Pacific Design Automation Conference*, 2023, pp. 606–611.

    - <span id="page-36-7"></span>[54] L. Xie, H. A. Du Nguyen, J. Yu, A. Kaichouhi,
    M. Taouil, M. AlFailakawi, and S. Hamdioui, "Scouting logic: A novel memristor-based
    logic design for resistive computing," in *2017 IEEE Computer Society Annual Symposium
    on VLSI (ISVLSI)*. IEEE, 2017, pp. 176–181.

    - <span id="page-36-8"></span>[55] A. Ankit, I. El Hajj, S. R. Chalamalasetti,
    S. Agarwal, M. Marinella, M. Foltin, J. P. Strachan, D. Milojicic, W.-M. Hwu,
    and K. Roy, "Panther: A programmable architecture for neural network training
    harnessing energy-efficient reram," *IEEE Transactions on Computers*, vol. 69,
    no. 8, pp. 1128–1142, 2020.

    - <span id="page-36-9"></span>[56] L. Song et al., "Pipelayer: A pipelined reram-based
    accelerator for deep learning," in *IEEE International Symposium on High Performance
    Computer Architecture (HPCA)*, Feb 2017, pp. 541–552.

    - <span id="page-36-10"></span>[57] X. Qiao, X. Cao, H. Yang, L. Song, and H.
    Li, "Atomlayer: A universal reram-based cnn accelerator with atomic layer computation,"
    in *Proceedings of the 55th Annual Design Automation Conference*, 2018, pp. 1–6.

    - <span id="page-36-11"></span>[58] P. Chen, M. Wu, Y. Ma, L. Ye, and R. Huang,
    "Rimac: An array-level adc/dac-free reram-based in-memory dnn processor with analog
    cache and computation," in *Proceedings of the 28th Asia and South Pacific Design
    Automation Conference*, 2023, pp. 228–233.

    - <span id="page-36-12"></span>[59] G. Yuan, P. Behnam, Z. Li, A. Shafiee, S.
    Lin, X. Ma, H. Liu, X. Qian, M. N. Bojnordi, Y. Wang *et al.*, "Forms: Fine-grained
    polarized reram-based in-situ computation for mixed-signal dnn accelerator," in
    *2021 ACM/IEEE 48th Annual International Symposium on Computer Architecture (ISCA)*.
    IEEE, 2021, pp. 265–278.

    - <span id="page-36-13"></span>[60] Z. Wang, S. Joshi, S. Savel''Ev, W. Song,
    R. Midya, Y. Li, M. Rao, P. Yan, S. Asapu, Y. Zhuo *et al.*, "Fully memristive
    neural networks for pattern classification with unsupervised learning," *Nature
    Electronics*, vol. 1, no. 2, pp. 137–145, 2018.

    - <span id="page-36-14"></span>[61] W.-H. Chen, K.-X. Li, W.-Y. Lin, K.-H. Hsu,
    P.-Y. Li, C.-H. Yang, C.-X. Xue, E.-Y. Yang, Y.-K. Chen, Y.-S. Chang *et al.*,
    "A 65nm 1mb nonvolatile computing-in-memory reram macro with sub-16ns multiply-and-accumulate
    for binary dnn ai edge processors," in *2018 IEEE International Solid-State Circuits
    Conference-(ISSCC)*. IEEE, 2018, pp. 494–496.

    - <span id="page-36-15"></span>[62] C.-X. Xue, W.-H. Chen, J.-S. Liu, J.-F. Li,
    W.-Y. Lin, W.-E. Lin, J.-H. Wang, W.-C. Wei, T.-W. Chang, T.-C. Chang *et al.*,
    "24.1 a 1mb multibit reram computing-in-memory macro with 14.6 ns parallel mac
    computing time for cnn based ai edge processors," in *2019 IEEE International
    Solid-State Circuits Conference-(ISSCC)*. IEEE, 2019, pp. 388–390.

    - <span id="page-36-16"></span>[63] Q. Liu, B. Gao, P. Yao, D. Wu, J. Chen, Y.
    Pang, W. Zhang, Y. Liao, C.-X. Xue, W.-H. Chen *et al.*, "33.2 a fully integrated
    analog reram based 78.4 tops/w compute-in-memory chip with fully parallel mac
    computing," in *2020 IEEE International Solid-State Circuits Conference-(ISSCC)*.
    IEEE, 2020, pp. 500–502.

    - <span id="page-36-17"></span>[64] C. Eckert, X. Wang, J. Wang, A. Subramaniyan,
    R. Iyer, D. Sylvester, D. Blaaauw, and R. Das, "Neural cache: Bit-serial in-cache
    acceleration of deep neural networks," in *2018 ACM/IEEE 45Th annual international
    symposium on computer architecture (ISCA)*. IEEE, 2018, pp. 383–396.

    - <span id="page-36-18"></span>[65] M. Kang, S. K. Gonugondla, S. Lim, and N.
    R. Shanbhag, "A 19.4-nj/decision, 364-k decisions/s, in-memory random forest multi-class
    inference accelerator," *IEEE Journal of Solid-State Circuits*, vol. 53, no. 7,
    pp. 2126–2135, 2018.

    - [66] H. Valavi, P. J. Ramadge, E. Nestler, and N. Verma, "A 64-tile 2.4-mb in-memory-computing
    cnn accelerator employing charge-domain compute," *IEEE Journal of Solid-State
    Circuits*, vol. 54, no. 6, pp. 1789–1799, 2019.

    - [67] A. Biswas and A. P. Chandrakasan, "Conv-sram: An energy-efficient sram
    with in-memory dot-product computation for low-power convolutional neural networks,"
    *IEEE Journal of Solid-State Circuits*, vol. 54, no. 1, pp. 217–230, 2018.

    - <span id="page-36-19"></span>[68] S. Yin, Z. Jiang, J.-S. Seo, and M. Seok,
    "Xnor-sram: In-memory computing sram macro for binary/ternary deep neural networks,"
    *IEEE Journal of Solid-State Circuits*, vol. 55, no. 6, pp. 1733–1743, 2020.

    - <span id="page-37-0"></span>[69] S. Angizi, Z. He, A. S. Rakin, and D. Fan,
    "Cmp-pim: an energy-efficient comparator-based processing-in-memory neural network
    accelerator," in *Proceedings of the 55th Annual Design Automation Conference*,
    2018, pp. 1–6.

    - <span id="page-37-1"></span>[70] J. Doevenspeck, K. Garello, B. Verhoef, R.
    Degraeve, S. Van Beek, D. Crotti, F. Yasin, S. Couet, G. Jayakumar, I. Papistas
    *et al.*, "Sot-mram based analog in-memory computing for dnn inference," in *2020
    IEEE Symposium on VLSI Technology*. IEEE, 2020, pp. 1–2.

    - <span id="page-37-2"></span>[71] L. Chang, X. Ma, Z. Wang, Y. Zhang, Y. Xie,
    and W. Zhao, "Pxnor-bnn: In/with spin-orbit torque mram preset-xnor operation-based
    binary neural networks," *IEEE Transactions on Very Large Scale Integration (VLSI)
    Systems*, vol. 27, no. 11, pp. 2668–2679, 2019.

    - <span id="page-37-3"></span>[72] S. Jung, H. Lee, S. Myung, H. Kim, S. K. Yoon,
    S.-W. Kwon, Y. Ju, M. Kim, W. Yi, S. Han *et al.*, "A crossbar array of magnetoresistive
    memory devices for in-memory computing," *Nature*, vol. 601, no. 7892, pp. 211–216,
    2022.

    - <span id="page-37-4"></span>[73] E. Breyer, H. Mulaosmanovic, T. Mikolajick,
    and S. Slesazeck, "Reconfigurable nand/nor logic gates in 28 nm hkmg and 22 nm
    fd-soi fefet technology," in *2017 IEEE International Electron Devices Meeting
    (IEDM)*. IEEE, 2017, pp. 28–5.

    - <span id="page-37-5"></span>[74] X. Yin, C. Li, Q. Huang, L. Zhang, M. Niemier,
    X. S. Hu, C. Zhuo, and K. Ni, "Fecam: A universal compact digital and analog content
    addressable memory using ferroelectric," *IEEE Transactions on Electron Devices*,
    vol. 67, no. 7, pp. 2785–2792, 2020.

    - <span id="page-37-6"></span>[75] A. Kazemi, M. M. Sharifi, A. F. Laguna, F.
    Müller, R. Rajaei, R. Olivo, T. Kämpfe, M. Niemier, and X. S. Hu, "In-memory nearest
    neighbor search with fefet multi-bit content-addressable memories," in *2021 Design,
    Automation & Test in Europe Conference & Exhibition (DATE)*. IEEE, 2021, pp. 1084–1089.

    - <span id="page-37-7"></span>[76] R. Khaddam-Aljameh, M. Stanisavljevic, J. F.
    Mas, G. Karunaratne, M. Braendli, F. Liu, A. Singh, S. M. Müller, U. Egger, A.
    Petropoulos *et al.*, "Hermes core–a 14nm cmos and pcm-based in-memory compute
    core using an array of 300ps/lsb linearized cco-based adcs and local digital processing,"
    in *2021 Symposium on VLSI Circuits*. IEEE, 2021, pp. 1–2.

    - <span id="page-37-8"></span>[77] M. Le Gallo, R. Khaddam-Aljameh, M. Stanisavljevic,
    A. Vasilopoulos, B. Kersting, M. Dazzi, G. Karunaratne, M. Brändli, A. Singh,
    S. M. Mueller *et al.*, "A 64-core mixed-signal in-memory compute chip based on
    phase-change memory for deep neural network inference," *Nature Electronics*,
    pp. 1–14, 2023.

    - <span id="page-37-9"></span>[78] Q. Dong, M. E. Sinangil, B. Erbagci, D. Sun,
    W.-S. Khwa, H.-J. Liao, Y. Wang, and J. Chang, "15.3 a 351tops/w and 372.4 gops
    compute-in-memory sram macro in 7nm finfet cmos for machine-learning applications,"
    in *2020 IEEE International Solid-State Circuits Conference-(ISSCC)*. IEEE, 2020,
    pp. 242–244.

    - <span id="page-37-10"></span>[79] H. Wang, R. Liu, R. Dorrance, D. Dasalukunte,
    D. Lake, and B. Carlton, "A charge domain sram compute-in-memory macro with c-2c
    ladder-based 8-bit mac unit in 22-nm finfet process for edge inference," *IEEE
    Journal of Solid-State Circuits*, vol. 58, no. 4, pp. 1037–1050, 2023.

    - <span id="page-37-11"></span>[80] S. De, F. Mueller, N. Laleni, M. Lederer,
    Y. Raffel, S. Mojumder, A. Vardar, S. Abdulazhanov, T. Ali, S. Dünkel *et al.*,
    "Demonstration of multiply-accumulate operation with 28 nm fefet crossbar array,"
    *IEEE Electron Device Letters*, vol. 43, no. 12, pp. 2081–2084, 2022.

    - <span id="page-37-12"></span>[81] G. Pedretti, C. E. Graves, S. Serebryakov,
    R. Mao, X. Sheng, M. Foltin, C. Li, and J. P. Strachan, "Tree-based machine learning
    performed in-memory with memristive analog cam," *Nature communications*, vol.
    12, no. 1, p. 5806, 2021.

    - <span id="page-37-13"></span>[82] K. Akarvardar and H.-S. P. Wong, "Technology
    prospects for data-intensive computing," *Proceedings of the IEEE*, vol. 111,
    no. 1, pp. 92–112, 2023.

    - <span id="page-37-14"></span>[83] C. Zhang, H. Sun, S. Li, Y. Wang, H. Chen,
    and H. Liu, "A survey of memory-centric energy efficient computer architecture,"
    *IEEE Transactions on Parallel and Distributed Systems*, 2023.

    - <span id="page-37-16"></span><span id="page-37-15"></span>[84] "Axelera," [https://www.axelera.ai/digital-in-memory-computing-for-deep-learning-acceleration/.](https://www.axelera.ai/digital-in-memory-computing-for-deep-learning-acceleration/)

    - [85] "d-matrix," [https://www.d-matrix.ai/.](https://www.d-matrix.ai/)

    - <span id="page-37-17"></span>[86] "Gyrfalcon tech," [https://www.gyrfalcontech.ai/about-us/company-overview/.](https://www.gyrfalcontech.ai/about-us/company-overview/)

    - <span id="page-37-18"></span>[87] "Memcpu," [https://www.memcpu.com/.](https://www.memcpu.com/)

    - <span id="page-37-19"></span>[88] "Memverge," [https://memverge.com/company/.](https://memverge.com/company/)

    - <span id="page-37-20"></span>[89] "mythic," [https://mythic.ai/.](https://mythic.ai/)

    - <span id="page-37-21"></span>[90] "Neuroblade," [https://www.neuroblade.com/product/.](https://www.neuroblade.com/product/)

    - <span id="page-37-22"></span>[91] "Rain," [https://rain.ai/about-us/.](https://rain.ai/about-us/)

    - <span id="page-37-23"></span>[92] "Semron," [https://www.semron.ai.](https://www.semron.ai)

    - <span id="page-37-24"></span>[93] "Surecore," [https://www.sure-core.com.](https://www.sure-core.com)

    - <span id="page-37-25"></span>[94] "Synthara," [https://www.synthara.ai.](https://www.synthara.ai)

    - <span id="page-37-26"></span>[95] "Syntiant," [https://www.syntiant.com/.](https://www.syntiant.com/)

    - <span id="page-37-27"></span>[96] "Tetramem," [https://www.tetramem.com.](https://www.tetramem.com)

    - <span id="page-37-28"></span>[97] M. Rao, H. Tang, J. Wu, W. Song, M. Zhang,
    W. Yin, Y. Zhuo, F. Kiani, B. Chen, X. Jiang *et al.*, "Thousands of conductance
    levels in memristors integrated on cmos," *Nature*, vol. 615, no. 7954, pp. 823–829,
    2023.

    - <span id="page-37-29"></span>[98] "Encharge ai," [https://enchargeai.com.](https://enchargeai.com)

    - <span id="page-38-0"></span>[99] "Reconceive," [https://www.re-conceive.com/home.](https://www.re-conceive.com/home)

    - <span id="page-38-1"></span>[100] "Fractile," [https://www.fractile.ai/.](https://www.fractile.ai/)

    - <span id="page-38-2"></span>[101] "Untether," [https://www.untether.ai/.](https://www.untether.ai/)

    - <span id="page-38-3"></span>[102] A. Siemieniuk, L. Chelini, A. A. Khan, J.
    Castrillon, A. Drebes, H. Corporaal, T. Grosser, and M. Kong, "OCC: An automated
    end-to-end machine learning optimizing compiler for computing-in-memory," *IEEE
    Transactions on Computer-Aided Design of Integrated Circuits and Systems (TCAD)*,
    vol. 41, no. 6, pp. 1674–1686, Aug. 2021. [Online]. Available:<https://ieeexplore.ieee.org/document/9502921>

    - [103] A. A. Khan, H. Farzaneh, K. F. Friebel, L. Chelini, and J. Castrillon,
    "Cinm (cinnamon): A compilation infrastructure for heterogeneous compute in-memory
    and compute near-memory paradigms," *arXiv preprint arXiv:2301.07486*, 2022.

    - [104] H. Farzaneh, J. P. C. de Lima, M. Li, A. A. Khan, X. S. Hu, and J. Castrillon,
    "C4cam: A compiler for cam-based in-memory accelerators," *arXiv preprint arXiv:2309.06418*,
    2023.

    - <span id="page-38-4"></span>[105] J. P. C. de Lima, A. A. Khan, H. Farzaneh,
    and J. Castrillon, "Full-stack optimization for cam-only dnn inference," in *Proceedings
    of the 2024 Design, Automation and Test in Europe Conference (DATE)*, ser. DATE''24.
    IEEE, Mar. 2024, pp. 1–6.

    - <span id="page-38-5"></span>[106] J. Ryckaert, M. Niemier, Z. Enciso, M. M.
    Sharifi, X. S. Hu, I. O''Connor, A. Graening, R. Sharma, P. Gupta, J. Castrillon,
    J. P. C. de Lima, A. A. Khan, and H. Farzaneh, "Smoothing disruption across the
    stack: Tales of memory, heterogeneity, and compilers," in *Proceedings of the
    2024 Design, Automation and Test in Europe Conference (DATE)*, ser. DATE''24.
    IEEE, Mar. 2024, pp. 1–6.


    # **ACRONYMS**


    <span id="page-39-22"></span><span id="page-39-21"></span><span id="page-39-20"></span><span
    id="page-39-19"></span><span id="page-39-18"></span><span id="page-39-17"></span><span
    id="page-39-16"></span><span id="page-39-15"></span><span id="page-39-14"></span><span
    id="page-39-13"></span><span id="page-39-12"></span><span id="page-39-11"></span><span
    id="page-39-10"></span><span id="page-39-9"></span><span id="page-39-8"></span><span
    id="page-39-7"></span><span id="page-39-6"></span><span id="page-39-5"></span><span
    id="page-39-4"></span><span id="page-39-3"></span><span id="page-39-2"></span><span
    id="page-39-1"></span><span id="page-39-0"></span>


    | CIM<br>compute-in-memory             | 1  |

    |--------------------------------------|----|

    | IMC<br>in-memory-computing           | 4  |

    | IMP<br>in-memory-processing          | 4  |

    | LIM<br>logic-in-memory               | 4  |

    | PIM<br>processing-in-memory          | 4  |

    | PUM<br>processing-using-memory       | 4  |

    | CNM<br>compute-near-memory           | 4  |

    | NMC<br>near-memory-computing         | 4  |

    | PNM<br>processing-near-memory        | 4  |

    | NMP<br>near-memory-processing        | 4  |

    | ADC<br>analog-to-digital converter   | 5  |

    | APU<br>accelerated processing unit   | 16 |

    | BL<br>bitline                        | 7  |

    | BLB<br>bitline bar                   | 7  |

    | CIM-A<br>CIM-array                   | 5  |

    | CIM-P<br>CIM-peripheral              | 5  |

    | CAM<br>content-addressable-memory    | 17 |

    | CNN<br>convolutional neural network  | 17 |

    | COM<br>compute-outside-memory        | 4  |

    | DAC<br>digital-to-analog converter . | 5  |

    | DNN<br>deep neural network           | 18 |

    | DPU<br>data processing unit          | 11 |

    | DRAM<br>dynamic random-access memory | 5  |


    <span id="page-40-23"></span><span id="page-40-22"></span><span id="page-40-21"></span><span
    id="page-40-20"></span><span id="page-40-19"></span><span id="page-40-18"></span><span
    id="page-40-17"></span><span id="page-40-16"></span><span id="page-40-15"></span><span
    id="page-40-14"></span><span id="page-40-13"></span><span id="page-40-12"></span><span
    id="page-40-11"></span><span id="page-40-10"></span><span id="page-40-9"></span><span
    id="page-40-8"></span><span id="page-40-7"></span><span id="page-40-6"></span><span
    id="page-40-5"></span><span id="page-40-4"></span><span id="page-40-3"></span><span
    id="page-40-2"></span><span id="page-40-1"></span><span id="page-40-0"></span>


    | FeFET<br>ferroelectric field-effect transistor | 2  |

    |------------------------------------------------|----|

    | FPU<br>floating-point unit                     | 13 |

    | HBM<br>high bandwidth memory                   | 7  |

    | ISA<br>instruction set architecture            | 14 |

    | LUT<br>look-up table                           | 15 |

    | MAC<br>multiply-accumulate                     | 12 |

    | MLC<br>multi-level cell                        | 20 |

    | MOS<br>metal-oxide-semiconductor               | 9  |

    | MRAM<br>magnetic RAM                           | 2  |

    | MTJ<br>magnetic tunnel junction                | 8  |

    | MPSoC<br>multiprocessor system-on-chip         | 4  |

    | NN<br>neural-network                           | 20 |

    | NVM<br>non-volatile memory                     | 2  |

    | OMP<br>outside memory processing               | 26 |

    | PCM<br>phase change memory                     | 2  |

    | PU<br>processing unit                          | 11 |

    | RRAM<br>resistive RAM                          | 2  |

    | SDK<br>software development kit                | 12 |

    | SIMD<br>single-instruction multiple-data       | 13 |

    | SLC<br>single-level cell                       | 20 |

    | SoC<br>system-on-chip                          | 26 |

    | SRAM<br>static random-access memory            | 5  |

    | SOT<br>Spin-orbit-torque                       | 8  |

    | STT<br>spin-transfer-torque                    | 8  |


    <span id="page-41-6"></span><span id="page-41-5"></span><span id="page-41-4"></span><span
    id="page-41-3"></span><span id="page-41-2"></span><span id="page-41-1"></span><span
    id="page-41-0"></span>


    | TRA<br>triple row activation        | 21 |

    |-------------------------------------|----|

    | TSVs<br>through-silicon vias        | 7  |

    | WL<br>word line                     | 7  |

    | CAGR<br>compound annual growth rate | 2  |

    | MVM<br>matrix-vector multiplication | 5  |

    | SA<br>sense amplifier               | 12 |

    | LLM<br>large language models        | 1  |'
  decisions:
    evaluation_prompt: 'Disqualified: no evaluation. Reason: The paper lacks experimental,
      empirical, or quantitative analysis sections or elements.'
- title: "A RISC-V SOC for Terahertz IoT Devices: Implementation and design\n  challenges"
  abstract: 'Terahertz (THz) communication is considered a viable approach to augmenting

    the communication capacity of prospective Internet-of-Things (IoT) resulting in

    enhanced spectral efficiency. This study first provides an outline of the

    design challenges encountered in developing THz transceivers. This paper

    introduces advanced approaches and a unique methodology known as Modified

    Pulse-width Modulation (MPWM) to address the issues in the THz domain. In this

    situation involving a transceiver that handles complex modulation schemes, the

    presence of a mixed signal through a high-resolution digital-to-analog

    converter (DAC) in the transmitter greatly contributes to the limitation in

    maintaining linearity at high frequencies. The utilization of Pulse-width

    Modulation-based Digital-to-Analog Converters (PWM-DACs) has garnered

    significant attention among scholars due to its efficiency and affordability.

    However, the converters'' performance is restricted by insufficient conversion

    speed and precision, especially in the context of high-resolution, high-order

    modulation schemes for THz wireless communications. The MPWM framework offers
    a

    multitude of adjustable options, rendering the final MPWM-DAC highly adaptable

    for a diverse array of application scenarios. Comparative performance

    assessments indicate that MPWM-DACs have enhanced conversion speed compared to

    standard PWM-DACs, and they also provide greater accuracy in comparison to

    Pulse-count Modulation DACs (PCM-DACs). The study presents a comprehensive

    examination of the core principles, spectrum characteristics, and evaluation

    metrics, as well as the development and experimental validation of the MPWM

    method. Furthermore, we present a RISC-V System-on-Chip (SoC) that incorporates

    an MPWM-DAC, offering a highly favorable resolution for THz IoT communications.'
  url: http://arxiv.org/abs/2401.14620v1
  keywords: ''
  document: '## I. INTRODUCTION


    The exponential growth in the quantity of Internet of Things (IoT) devices will
    inevitably lead to a substantial surge in wireless network congestion. In [\[1\]](#page-14-0),
    the reason for this is that the next wireless systems must offer increased system
    capacity along with exceptionally dependable and low-latency communication. Additionally,
    they must possess more flexible connectivity features to accommodate the evolving
    requirements of the IoT network. RISC-V is an emerging technology that is increasingly
    being used in low-power IoT applications. The architectural expansions of RISC-V
    and the commercialization of Systemon-Chips (SOCs) employing this architecture
    have contributed to the stability of these extensions. Additionally, the lowered
    manufacturing costs and cheaper prices for end-consumers have facilitated the
    commencement of commercialization. A wide range of additional RISC-V cores [\[2\]](#page-14-1)
    have been produced using this instruction set architecture, and devices based
    on RISC-V are now being developed for specialized applications like the IoT, wearables,
    and embedded systems. The embedded IoT platform''s application processor carries
    out the many functions of the IoT application, such as reading, storing, processing,
    and deciding when to send sensor data. The widespread use of IoT applications
    and their often strict power efficiency needs have led to the development of a
    wide range of application processors. It is conceivable to encounter IoT processors
    with power consumption in the nanowatt range that are specifically engineered
    for battery-less applications. Furthermore, cores that target power consumption
    in the milliwatt and microwatt ranges are also accessible. A method in [\[3\]](#page-16-0)
    for developing ultra-low power software-defined radios and an instruction-set
    extension for the open-source RISC-V ISA have been developed by the authors. With
    this extension, challenging arithmetic operations utilized in the physical-layer
    protocols of IoT communication systems are intended to be accelerated.


    Mobile networks in the future are anticipated to integrate nomadic, dispersed
    base stations that employ unmanned aerial vehicles (UAVs). These networks will
    function as a supplementary element of wireless networks, linking a significant
    number of individuals globally as well as a multitude of stationary and portable
    cyber equipment situated in diverse areas. Simultaneously, the data transmission
    speed that mobile devices can handle is continuously rising due to the implementation
    of advanced networks like 5G and enhancements made to current infrastructures.
    In [\[4\]](#page-16-1), there will be a significant increase in the volume of
    data being transmitted daily. This will lead to congestion in the communication
    between base stations and the main network through traditional backhaul connections.
    It is anticipated that the sub-6 Gigahertz (GHz) and millimeter-wave (mmWave)
    frequencies may not have the capacity to facilitate communication for these consumers.
    Therefore, Terahertz (THz) communication (0.1–10 THz) [\[5\]](#page-16-2) has
    been considered a promising method to address the aforementioned issue because
    of its exceptionally large bandwidth. When building THz transceivers, it is crucial
    to take into account three primary performance aspects in [\[6\]](#page-16-3).
    Operating in the THz frequency range offers a wide and untapped spectrum of frequency
    bandwidth. However, increasing the amount of bandwidth assigned to each user poses
    several architectural challenges.


    - 1) The bandwidth rises when there is a significant difference between the two
    resonance frequencies. The assumption of a linear or uniform phase response is
    no longer genuine, resulting in phase distortion and the introduction of noise
    within the required frequency range. Utilizing high-order bandpass matching circuits
    is advisable when the fractional bandwidth exceeds 20%.

    - 2) The adoption of THz carrier frequencies and the presence of inter-user interference
    (IUI) resulting from the implementation of IoT users would pose significant challenges
    owing to the high path loss characteristics. By leveraging multiplexing benefits
    from the propagation of distinct signal streams across several unique paths in
    different spatial and polarization domains, high diversity or rank order in THz
    channels can be used to boost channel capacity or multi-user service.

    - 3) Transceivers operating at the THz frequency perform modulation and demodulation
    using digital signal processing techniques. Consequently, the digital realm may
    restrict the capacity of the analog baseband, mixed-signal, and RF chain to process
    modulated signals with a wide range of values and a high ratio of peak power to
    average power (PAPR). In addition, the effective application of higher-order modulation
    necessitates the use of data converters with better resolution, long dynamic range
    RF chains, and local oscillators with reduced phase noise, all while taking high
    sensitivity and linearity into account.


    The ongoing challenge discussed in [\[6\]](#page-16-3) is to provide energy-efficient
    resolutions for the DAC/ADC and DSP elements of integrated transmitter and receiver
    chipsets, which need to handle data speeds beyond 50 Gbps. The back-end and mixedsignal
    processing use a costly commercial Arbitrary Waveform Generator (AWG) and a real-time
    oscilloscope. Their objective is to produce potent sub-channelized modulated signals
    externally. Subsequently, these signals are used to provide power to the transmitting
    side, adjust for any discrepancies, convert the modulated radio frequency signal
    into its original form, and retrieve the unprocessed data stream on the receiving
    side. From a practical standpoint, integrating an external AWG and a real-time
    oscilloscope into a front-end is not a power-efficient approach. Expanding a transceiver
    from a single element to a multi-antenna configuration for ultra-high data-rate
    applications in the same region would greatly exacerbate the situation. To be
    more precise, transceivers that use higher-order modulations have a lower RF bandwidth
    for a given data rate. However, they need a much better resolution and a higher
    sampling rate for the DAC and ADC relative to the signal baud rate. Also, in practice,
    the sampling rate of a Nyquist-rate data converter is varied by five to six times
    the baud rate for a better bit error rate. In addition, as the modulation intricacy
    rises, the necessary resolution of the data converter also increases, which becomes
    progressively more difficult to achieve as the data rate increases. The ADC at
    the receiving end and the DAC at the transmit end are the main parts that determine
    the mixed signal. This signal has to meet strict requirements while high-speed,
    high-order modulation is being generated and processed. The contributions of this
    study are summarised as follows:


    - 1) We present a comprehensive overview that covers the prior methodologies and
    obstacles associated with the design of THz IoT devices, including the design
    of multi-antenna arrays and transceivers.

    - 2) By implementing a novel modulation technique called Modified PWM (MPWM),
    DACs are capable of displaying pulse density and edge count that can be adjusted.
    An MPWM module that incorporates an n-bit counter possesses the capacity to accommodate
    (n-1) distinct edge count and pulse density configurations.

    - 3) For the purpose of attaining exceptional outcomes, this approach provides
    DAC designers with enhanced versatility to modify their designs to suit specific
    applications.

    - 4) We present the design of an MPWM-DAC and RISC-V SoC specifically tailored
    for implementation in THz IoT applications.


    Section II presents a comprehensive examination of the design of high-performance
    THz antennas, both on-chip and off-chip, as well as multi-antenna arrays. In Section
    III, we tackle the task of developing multi-antenna transceivers for THz frequencies.
    Section IV provides a detailed explanation and comparison of the concept and spectrum
    of MPWM with other modulation techniques. The performance of MPWM-DAC encompasses
    static error, integral nonlinearity, differential nonlinearity, and dynamic features.
    Section V presents the design and FPGA testing of MPWM-DACs, as well as the design
    of an MPWM-DAC and a RISC-V SoC for THz IoT applications.


    # II. HIGH-THROUGHPUT THZ MULTI-ANTENNA ARRAY DESIGN


    Antennas play a crucial role in connecting the communication system to the surrounding
    environment. On-chip antennas [\[7\]](#page-16-4) offer a prospective option that
    is more advantageous than off-chip antennas for THz communication. An on-chip
    antenna can greatly simplify the matching network needed to connect the antenna
    to the RF circuitry. Simplifying the matching network can significantly enhance
    the system''s performance by decreasing the loss and noise figure of the front
    end. In order to achieve optimal radiation or prolong the battery life of the
    systems, it is crucial to maximize the effectiveness of the antenna. In addition,
    the size of the on-chip antenna is the primary influence in defining the chip
    area. Therefore, it is imperative to minimize its dimensions to reduce manufacturing
    costs. Antenna downsizing can be achieved by utilizing compact arrangements and
    incorporating materials with high permittivity.


    TABLE I: Summary of the state-of-the-art THz antennas on chip.


    | Paper | Technology         | Frequency (GHz) | Efficiency (%) | Directivity
    (dBi) | Gain (dBi) |

    |-------|--------------------|-----------------|----------------|-------------------|------------|

    | [8]   | 0.13 µm SiGe       | 370             | 40             | 8                 |
    4          |

    | [9]   | 50 µm              | 300             | 65             | NA                |
    4.5        |

    | [10]  | 65-nm CMOS         | 260-280         | 15             | 7                 |
    22         |

    | [11]  | 65-nm CMOS         | 438–479         | 32             | 21.4              |
    12.4       |

    | [12]  | 0.18 µm SiGe       | 340             | 74             | NA                |
    7.9        |

    | [13]  | 0.13 µm SiGe       | 235-255         | 75             | NA                |
    7          |

    | [14]  | 130 nm SiGe BiCMOS | 165-175         | 45             | NA                |
    5          |

    | [15]  | 0.13 µm SiGe       | 120             | 50             | NA                |
    6          |


    ## *A. On-chip antennas*


    The rectangle, dipole, bow-tie, and slot antennas are commonly used topologies
    in on-chip design at frequencies above 300 GHz, with a gain above 2 dBi [\[16\]](#page-16-13).
    The patch antenna in [\[8\]](#page-16-5), created using 0.13 µm SiGe technology,
    operates at a frequency of 370 GHz. It achieves a gain of 4 dBi, an efficiency
    of 40%, and a directivity of 8 dBi. The gain enhancement can be due to the increased
    electrical thickness of the dielectric as the frequency increases. Within the
    high gain zone, the return loss has an elevated value. Measuring the antenna above
    100 GHz using on-probe methods gets difficult because the metallic elements of
    the probe station reflect the incoming radiated waves. This leads to a certain
    degree of discrepancy in the measured gain outcomes. In addition, the use of highly
    delicate probe tips during the measurements restricts the capacity to freely alter
    any components installed on the probe station that are essential for facilitating
    the measurement procedure, such as the mobility of the receiving reference antenna.


    The antenna outlined in [\[9\]](#page-16-6) consists of a square patch antenna
    positioned on a silicon substrate, accompanied by a ground plane. The substrate-integrated
    waveguide (SIW) is created by a patch that incorporates two T-shaped slots and
    short-circuited edges using metal vias. This method increases the size of the
    aperture region and reduces losses caused by surface waves and substrate. As a
    result, there is an enhancement in impedance matching, bandwidth, isolation, gain,
    and radiation efficiency. This arrangement minimizes the losses resulting from
    surface waves and the silicon dielectric substrate. The structure may be activated
    by employing two coaxial ports that are linked to the patch from the lower side
    of the silicon substrate. The enhanced aperture area significantly enhances both
    the impedance bandwidth and radiation characteristics within the frequency range
    of 0.28 THz to 0.3 THz. The antenna has a mean gain and efficiency of 4.5 dBi
    and 65%, respectively. Furthermore, it possesses a self-contained configuration
    that exhibits exceptional isolation, surpassing 30 dB between the two ports. The
    on-chip antenna is 800 × 800 × 60 µm<sup>3</sup> in size. In addition, the antenna
    in [\[17\]](#page-16-14) was constructed on a GaAs substrate that had a thickness
    of 0.5µm. The transceiver consists of a Voltage-Controlled Oscillator (VCO), buffer
    amplifier, modulator stage, power amplifier, frequency-tripler, and an on-chip
    antenna. The on-chip antenna utilizes substrate-integrated waveguide (SIW) technology
    and has a 4×4 configuration of slots in both the longitudinal and transverse orientations,
    making use of metamaterial technology. The SIW antenna utilizes a high-pass filter
    to efficiently eliminate undesired harmonics and transmit the desired signal.
    The on-chip antenna has dimensions of 2 × 1 × 0.0006 mm<sup>3</sup> and demonstrates
    a minimum gain of 0.25 decibels isotropic (dBi), an average gain of 1.0 dBi, a
    minimum efficiency of 46.12%, and an estimated efficiency of around 55%. The transceiver
    emits an average output power of -15 dBm within the frequency range of 0.3-0.31
    THz, which makes it highly suitable for near-field imaging applications.


    The antenna presented in [\[10\]](#page-16-7) describes an antenna design that
    integrates an SIW cavity with a wideband dual-slot antenna on a chip. This design
    was specifically created for the use of frequency-modulated continuous-wave (FMCW)
    radars. Specifically, the existence of dual slots induces two resonance modes,
    while the SIW cavity induces supplementary modes that aid in achieving a broad
    bandwidth. The strength of this antenna varies from -1 dB to 0 dB across the frequency
    range of 260-280 GHz. It has an efficiency of 15% for impedance matching, which
    corresponds to a fractional bandwidth. However, due to the substrate''s thinness,
    electric fields are mostly limited to the area between the patch and the ground.
    As a result, both the gain and bandwidth are reduced.


    To create the radiation front end, one can utilize conventional antenna designs,
    such as rectangular patches and dipoles [\[18\]](#page-16-15). Given that the
    substrate thickness usually falls within the range of 250-300 µm, it is possible
    to position the ground layer beneath the substrate to augment the total thickness
    between the antenna and the ground layer. The lens-integrated on-chip antennas
    [\[11\]](#page-16-8), which have been recently described, exhibit superior gain
    and bandwidth performance. The lens is responsible for improving the impedance
    matching with the air and also for collimating the output beam. However, these
    methods necessitate the attachment of a large lens on their rear side. Antennas
    employing dielectric resonators (DR) [\[12\]](#page-16-9) have a bandwidth reduction
    of less than 15%, but make up for it with a higher gain of almost 6 dBi. By employing
    a dielectric material that is a minimum of 400 µm in thickness above the antenna,
    these on-chip DR antennas increase their surface area.


    According to the reference [\[13\]](#page-16-10), using the localized backside
    etching (LBE) method on a group of folded dipole antennas made using 0.13 µm SiGe
    technology results in a gain of more than 7 dBi and an efficiency of over 75%
    within the frequency range of 235-255 GHz. Applying the LBE approach described
    in [\[14\]](#page-16-11) to eliminate the lossy silicon substrate around an on-chip
    antenna patch leads to a notable enhancement in performance. Nevertheless, the
    LBE method requires supplementary protocols and a designated area for etching,
    which must encompass the patch. According to [\[19\]](#page-16-16), the substrate
    surrounding the patch is vulnerable to data loss. In order to address this concern,
    a Lossless Back End (LBE) method is implemented to eradicate the substrate responsible
    for data loss. On the other hand, this approach necessitates supplementary processing
    procedures and an extensive etching area. This technique offers various methods
    for integrating lens antennas onto chips by achieving a sufficient balance in
    bandwidth through the inclusion of a sizable lens. Positioning the ground layer
    beneath the substrate enhances the electrical thickness of the dielectric. A considerable
    improvement in the amplification is observed. The use of a folded dipole antenna
    allows for achieving a maximum gain of 5 dBi, together with a radiation efficiency
    of 45%, over the frequency range of 165-175 GHz.


    An improvement in radiation efficiency and a reduction in the space-wave quality
    factor occurs when the depth of the superstate is a multiple of a quarter-wavelength
    and an odd number. Thus, placing a high-dielectric superstrate over the antenna,
    as explained in [\[20\]](#page-16-17), is an alternate approach to enhance radiation
    efficiency. An elliptical slot antenna is manufactured using 0.13 µm CMOS technology.
    This antenna is placed on the top metal layer, while a ground layer is located
    at the LY metal layer, which is positioned 11 µm below the top metal layer. The
    antenna is attached to a 400 µm thick quartz superstrate. At 90 GHz, the antenna
    has a measured bandwidth of 3.9%, a peak gain of 0.7 dBi, and a peak radiation
    efficiency of 30%. However, the research carried out by [\[15\]](#page-16-12)
    shows that by adding a quartz superstrate and a parasitic patch to a shorted patch
    antenna on a 0.13 µm SiGe substrate, a notable improvement of 6 dB may be achieved.
    The system operates at a frequency of 120 GHz and achieves an efficiency of around
    50%. Additionally, the system has a gain bandwidth of 9 GHz, with a 1-dB increase
    within this bandwidth. Enhancing the antenna''s performance is achieved by utilizing
    a small superstrate, which increases the complexity of the manufacturing process.


    The unit cell of the artificial magnetic conductor (AMC) layer is constructed
    by enclosing pairs of walls, each composed of a perfect electric conductor and
    a perfect magnetic conductor. Through a waveport positioned above the unit cell,
    a planar wave strikes perpendicularly to the unit cell. It is possible to get
    an AMC layer surface reflection phase of zero close to the center frequency by
    carefully changing the sizes of the unit cell elements. Furthermore, to mitigate
    loss in the AMC layer, it is necessary to decrease the magnitude of the reflection
    coefficient. The bandwidth of a unit cell is defined as the range of the reflection
    phase, which varies between -90 and 90 degrees. Therefore, the design of a double-rhomboid
    bow-tie (DRBT) slot antenna with a back-to-back E-shaped slot and an anisotropic
    magnetic conductor (AMC) layer is implemented using a 0.13 µm BiCMOS technology
    as presented in [\[21\]](#page-16-18). The antenna operates in the W-band frequency
    range of 75-110 GHz. The maximum gain observed is -0.58 dBi and the bandwidth
    is 6 GHz.


    ## *B. off-chip antennas*


    Integrating chipsets with off-chip antennas using normal interface techniques
    is limited by strict frequency limitations. Offchip antennas are frequently used
    on Printed Circuit Boards (PCBs) when larger dielectric layers are necessary.
    The reduced manufacturing expenses associated with off-chip antennas enable the
    development of extensive off-chip antenna arrays, resulting in significantly amplified
    levels of radiated power. These antennas offer superior efficiency, gain, and
    bandwidth compared to on-chip antennas. While sacrificing bandwidth, the antenna
    described in [\[22\]](#page-16-19) utilizes numerous parallel leaky-wave structures
    to enhance the gain. The antenna off-chip featured in [\[23\]](#page-16-20) achieves
    a high level of amplification and a broad range of frequencies by combining a
    multi-layered board technology with an L-probe feedline.


    It is crucial to have broadband antenna designs that can be accommodated within
    a limited number of metal layers and thin dielectric layers in order to manufacture
    antennas using commonly accessible rigid or flexible PCBs. The antenna presented
    in [\[18\]](#page-16-15) reveals a stacked patch antenna that is connected to
    an external cavity backplane and operates in the frequency range of 91.5-134 GHz.
    The fractional bandwidth of the object is around 38% and it has a peak gain of
    8.1 dBi. This antenna demonstrates the greatest fractional bandwidth compared
    to off-chip antennas operating over 100 GHz. In addition, the ground and patch
    layers both utilize copper cladding with a thickness of 12 µm. The dielectric
    substrates have a relative permittivity (ϵr) of 2.6 and a loss tangent of 0.003.
    In light of the aforementioned methodologies, the viewpoint about off-chip design
    problems can be succinctly summarized as follows:


    - 1) In order to provide broad frequency compatibility between the input G-S-G
    port and the slot, the feedline is constructed as a transmission line consisting
    of two sections. The 50 Ω impedance part is connected to the 50 Ω G-S-G port,
    while a larger low-impedance piece is connected to the impedance of the slot aperture.
    Undesirable parallel plate modes, characterized by resonance at different frequencies,
    arise when conductor layers are situated both above and below the feedline.

    - 2) In order to enhance the performance of wideband systems, the use of FPC technology
    permits a maximum thickness of 50 µm for the substrate layers Sub1, Sub3, and
    Sub4. Sub2, with a reduced thickness of 25 µm, produces substantial coupling between
    the feedline and slot aperture. The G-S-G probe port is situated within the Gnd2
    layer and is linked to the feedline. The feedline is a grounded co-planar waveguide
    designed to be compatible with the G-S-G probe port.

    - 3) The process of cavity generation entails the utilization of via arrays to
    construct a conductor enclosure surrounding patches. The manufacturing process
    can cause surface waves to pass through even the tiniest gaps between vias. Developing
    a broadband antenna is a significant challenge in this matter. At higher frequencies
    in a wideband antenna, the wavelength becomes shorter, necessitating a greater
    proximity between vias compared to lower frequencies.


    The primary issue associated with off-chip antennas is effectively guiding signals
    from the semiconductor to the antenna structure. The interface, equipped with
    pads on both ends, creates a network that exhibits self-inductance as well as
    capacitance and resistance. Ideally, this combination should operate as a direct
    path for electric current. The capacitors and inductors in this network resonate
    together at high frequencies, establishing a frequency limit called the self-resonance
    frequency (SRF). Regarded as the prevailing method for packaging in [\[24\]](#page-16-21)
    and [\[25\]](#page-16-22), aluminum or gold wirebonds can also be employed to
    provide a connection between the chipset and antennas on the PCB. Standard wires
    typically have a diameter ranging from 10 to 75 µm. In high-power applications,
    many wirebonds may be used in tandem. In the usual wirebonding procedure, the
    wire is aligned at an almost right angle to the pads at one end. The wire becomes
    longer than required, leading to substantial self-inductance. The wirebonds exhibit
    a range of self-inductance values, which can span from a few hundred picohenries
    to a few nanohenries. Consequently, the wirebonds exhibit considerable reactance
    at higher frequencies, leading to a large reduction in their insertion loss and
    insufficient impedance matching.


    ## *C. On-chip and off-chip antenna array*


    As the frequency rises, the dielectric thickness increases as a percentage of
    the wavelength, while each antenna element''s area decreases, increasing the gain
    bandwidth of the on-chip antennas. Following this logical sequence, this section
    explores off-chip and on-chip antenna arrays for multi-antenna systems in cases
    where employing a single-element radiator is impractical. In the THz frequency
    band, a chip with a single antenna element generates maximum power that covers
    a limited distance. Antenna arrays are utilized to enhance the coverage area of
    a communication system by greatly amplifying the total radiated power and enhancing
    the beam''s directionality. The antenna components are spaced at a distance equivalent
    to half the wavelength, enabling a coherent combination of radiated waves and
    limiting interference.


    Using an on-chip antenna array simplifies routing and matching between transceiver
    circuitry and antenna components. As previously indicated, on-chip antenna performance
    improves with higher frequencies, with most arrays operating beyond 150 GHz. The
    authors in [\[26\]](#page-17-0) implement an 8-element slot array with a 10 mm
    Si lens at the bottom in a 65 nm CMOS process. Simulated directivity is 16.6 dB,
    the overall radiation efficiency is 42%, and the impedance bandwidth exceeds 60
    GHz in the 260 GHz range. The eight-element on-chip antenna array in [\[27\]](#page-17-1)
    features a λ/4 thick Quartz superstrate on top. The Quartz superstrate boosts
    gain by 3.1 dB and efficiency from 22% to 45% for each antenna element. This leads
    to an array gain of 11-12 dBi and a 10 GHz bandwidth at 385 GHz frequency. At
    412-416 GHz, a 4 × 4 patch antenna in an oscillator network obtains a peak effective
    isotropic radiated power (EIRP) of 14 dBm in [\[28\]](#page-17-2).


    Enhanced gain and bandwidth performance can be achieved by scaling antenna size
    and accessing thicker substrates in off-chip technologies. The allocation of space
    for an antenna on a PCB board is not a major issue, in contrast to an on-chip
    antenna, where it occupies a substantial portion of the chip. This enables the
    use of larger off-chip antenna arrays, hence improving antenna gain performance
    and transceiver chip EIRP. The diminished amplification and frequency range capabilities
    shown in the following investigations can likely be attributed to the reduced
    thickness of the interposer layer and the scattered connection between the chipset
    and the interposer. The designs in [\[29\]](#page-17-3) are pioneering examples
    of off-chip antenna arrays operating at frequencies above 100 GHz. This work constructs
    two patch arrays on a RO3003 substrate with a dielectric constant (ϵr) of 3. One
    array is fed in series, while the other is supplied corporately. The serial-fed
    antenna has a maximum gain of 6 dBi and a frequency range of 5 GHz centered at
    122 GHz. On the other hand, the corporate-fed antenna has a gain of 5 dBi and
    a bandwidth of 7 GHz.


    To showcase long-distance communication capabilities, the authors in [\[30\]](#page-17-4)
    proposed the use of a 384-element array operating at a frequency of 90 GHz. This
    array would consist of 11 layers of metal stacks on a PCB, and would be linked
    to the chipset by flip-chip interconnects. The array achieved an impressive EIRP
    of 60 dBm within a conservative 20 GHz bandwidth. The authors in [\[31\]](#page-17-5)
    constructed 16 patch antennas on an interposer board positioned between the chipset
    and PCB to facilitate large-scale MIMO applications. This study employed an interposer
    board to establish a connection between narrow signal lines on the chipset and
    wider lines on the board. The bandwidth for return loss was measured to be between
    71 and 84 GHz, while the peak gain was about 5 dBi.


    The on-chip antenna array in [\[32\]](#page-17-6) operates within the frequency
    range of 0.450-0.475 THz. It utilizes two vertically oriented DRs on a silicon
    substrate, employing standard CMOS technology. To reduce energy dissipation, one
    can create a winding pathway in the silicon substrate and enclose it with a metallic
    barrier, thereby reducing substrate loss and surface wave effects. The integration
    of slots and vias results in the antenna adopting a metamaterial structure that
    occupies a very small area. The dimensions of the antenna are 400 x 400 × 135
    µm<sup>3</sup> . The antenna achieves a peak gain of 4.5 decibels relative to
    an isotropic radiator and has a radiation efficiency of 45.7% at a frequency of
    0.4625 THz. A series-fed double-DR on-chip antenna array is a promising candidate
    for THz integrated circuits. By employing CMOS 20µm silicon technology, the authors
    in [\[33\]](#page-17-7) develop an innovative on-chip antenna array designed for
    operation at frequencies ranging from 0.6 to 0.65 THz. This array configuration
    has three vertically aligned layers of Silicon-metal-Silicon. The intermediate
    metal layer functions as a ground plane sandwiched between two silicon layers.
    The uppermost layer consists of a pair of antennas, each equipped with three interconnected
    radiating elements. Radiation elements exhibit the behavior of linked dual rings,
    like a metamaterial. This arrangement increases the effective aperture area of
    the array. The inclusion of metallic via-holes between radiation elements in three
    layers mitigates the effects of surface waves and substrate losses. The antenna
    is operated via microstrip wires that are open-circuited on the rear of the structure.
    The ground-plane layer incorporates slots that facilitate the transmission of
    electromagnetic energy from the lower layer to the radiating components located
    on the upper layer. The dimensions of the antenna array are 0.4× 0.4 × 0.06 mm<sup>3</sup>
    . The on-chip antenna array in [\[33\]](#page-17-7) attains an average radiation
    gain, efficiency, and isolation of 7.62 dBi, 32.67 %, and negative 30 dB, respectively.
    The results confirm the effectiveness of the antenna array for THz-integrated
    circuits.


    In essence, on-chip antennas that operate at frequencies over 150 GHz adhere to
    radiation laws, enabling the creation of a fully integrated communication system.
    On-chip antennas are frequently used at lower THz frequencies due to their ability
    to provide a broad gain bandwidth while minimizing substrate elevation. Although
    there have been improvements in radiation quality, the linkage between the chipset
    and off-chip antenna remains a major obstacle to attaining optimal off-chip radiation
    frequencies. Wirebonds have traditionally been employed to establish connections
    with frequencies above 100 GHz. Furthermore, flip-chip technologies [\[34\]](#page-17-8),
    namely copper-pillar and solder bump, offer an impressive SRF of around 200 GHz.
    This makes them highly suitable for the development of future THz communication
    systems.


    ## III. THZ MULTI-ANTENNA TRANSCEIVERS DESIGN


    The majority of studies in multi-antenna transceiver design have focused on beamforming
    using phased-array solutions. Following the first showcases of single, four, and
    eight-element arrays on a single microchip, the phased-arrays rapidly progressed
    to 16 and 32-element configurations on a single microchip. In the transmit mode
    of a phased array with Nelements presented by the following principle in [\[35\]](#page-17-9),
    the effective isotropic radiated power (EIRP) is defined as the product of the
    total transmit power P<sup>t</sup> and the transmit antenna gains Gt. P<sup>t</sup>
    represents the power emitted per element, while Gt is directly proportional to
    N. The EIRP, on the other hand, is directly proportional to N<sup>2</sup> . When
    operating in receive mode, the gain of the phased array antenna is directly proportional
    to the number of elements, denoted as N. Hence, the link budget, which is directly
    proportional to the product of Pt, Gt, and receiver antenna gain Gr, exhibits
    a cubic dependency on the number of N<sup>3</sup> . This section will discuss
    two designs: high-performance multi-antenna systems and low-complexity MIMO precoding
    designs. These architectures address the obstacles and concerns related to increasing
    bandwidth, modulation order, and transmit power in THz technology.


    ## *A. High-performance multi-antenna systems*


    Due to the reduced dimensions of passive components in the THz frequency range,
    it becomes feasible to consider the implementation of interconnected multi-antenna
    transceiver arrays. All-digital beamforming enables the transmission of several
    beams simultaneously, providing maximum flexibility and data transfer rate. The
    digital beamforming (DBF) technique has three main benefits. By employing digital
    precoding, it becomes possible to get precise resolution in both magnitude and
    phase. Moreover, a DBF array possesses the potential to increase its capacity
    by superimposing several beams to handle various data streams. In addition, the
    complete DBF architecture enables independent beamforming precoding on each subcarrier
    or resource block for multicarrier transmissions. This results in exceptional
    performance across a broad spectrum of frequencies. As the authors presented in
    [\[36\]](#page-17-10), DBF-based millimeter-wave MIMO systems enable multi-user
    access and exhibit exceptional spectrum utilization. The principal constraints
    associated with the development of a DBF-based millimeter-wave MIMO transceiver
    are hardware complexity, financial investment, and power consumption. In order
    to facilitate the further development of millimeter-wave MIMO systems utilizing
    DBF, cost reduction in digital baseband processing will be the primary objective.
    Novel semiconductor manufacturing processes and improved integration techniques
    are assisting in the resolution of these constraints.


    Lately, there has been a concentration on a hybrid beamforming approach that merges
    analog beamforming and digital MIMO coding. This technology offers a significant
    advantage by minimizing the complexity of the digital baseband through the utilization
    of a reduced number of up/down conversion chains in systems that have a high number
    of antennas. Consequently, it has emerged as a feasible choice for both outdoor
    and interior millimeter-wave/terahertz communication. By combining multibeam digital
    baseband processing with analog beamforming, it is possible to simultaneously
    improve both multiplexing and beamforming gain. RF phase shifters are primarily
    used to alter the direction of the main lobe. RF variable gain attenuators and/or
    amplifiers (VGAs) provide interference spatial filtering by aligning the zero
    locations of each beamforming line with the incident angle of interference. The
    number of parallel data streams K determines the minimum required number of RF
    chains NRF in a hybrid design. On the other hand, the beamforming gain is achieved
    by using NRF complex weighting factors that appear at each antenna. In the context
    of sub-array systems, hybrid beamforming have the capability to either receive
    or transmit an entire set of data streams from N antennas when N = NRF or only
    a subset of data streams when partially interconnect RF circuits supply subarrays,
    i.e., NRF < N, per antenna. A comprehensive array fulfills the function of an
    exclusively DBF, upon closer inspection. NRF × N denotes the quantity of signal
    processing paths for the subarray, specifically from the digital baseband to the
    antenna front-end. N<sup>2</sup> signifies the total number of signal processing
    paths for the array. On the contrary, the beamforming gain of the sub-array is
    equivalent to the full array''s NRF /N. Consequently, in hybrid beamforming, the
    objectives of beamforming gain and signal processing complexity are in direct
    opposition. As the authors presented in [\[37\]](#page-17-11), it allows for the
    receiving of two streams by utilizing the Cartesian combining principle. The execution
    of this design for a two-stream reception necessitates the use of eight splitters,
    twenty combiners, and twelve mixers. The presence of several signal routes leads
    to electromagnetic cross-talk, which arises from the frequent cross-overs that
    take place between these paths.


    Phased arrays provide a range of capabilities found in a multi-antenna system,
    such as enhancing capacity and diversity. As the authors presented in [\[38\]](#page-17-12),
    the initial integration of the phased-array system in silicon involved the incorporation
    of local oscillator (LO) phase shifting. The SiGe transceiver has four components
    for both transmitting and receiving signals, as well as circuitry for generating
    and distributing LO frequencies. Additionally, it features a local design that
    allows for phase-shifting in the LO path, enabling beam steering. The main benefit
    of this architecture is that the phase-shifters are positioned at a distance from
    the RF route. Consequently, the implementation of the LO-path phase-shifting scheme
    at a local level allows for the creation of a reliable distribution network that
    can effectively handle THz frequencies and/or a larger number of components. The
    presence of large array sizes can present a notable design obstacle for the LO
    distribution network. Nevertheless, this worry can be mitigated by employing a
    phased-array transceiver that incorporates RF phase shifting. In [\[39\]](#page-17-13),
    the described approach involves circuitry that allows for accurate manipulation
    of beam direction, phase, and amplitude at each individual front end. Additionally,
    independent control of tapering and beam steering is achieved at the array level.
    The integrated circuit is designed using 130-nm SiGe BiCMOS technology. It consists
    of 32 transceiver components and supports simultaneous independent beams in two
    polarizations for both transmission and reception operations.


    ## *B. Low-complexity MIMO precoding Designs*


    Without investigating innovative architectural approaches, it may be unfeasible
    to include a unified ultra-high data-rate wireless transceiver that surpasses
    50 Gbps. This is attributed to the substantial power consumption of the baseband
    units and data converter, which may reach a maximum of 10 watts. Utilizing a multi-antenna
    arrangement, rather than a single-element system will result in an increase in
    the power consumption of the transceiver beyond the frequency of 100 GHz. Streamlining
    the complexity of data converters and back-end digital signal processing (DSP)
    is essential, as it allows for the creation of energyefficient and cost-effective
    high-speed wireless connections that can be accessed by a large number of customers.
    Conventional MIMO systems are solely operational within the baseband domain. However,
    full-digital precoders are impracticable due to the substantial energy consumption
    of high-frequency mixed-signal components and the attendant manufacturing costs.
    The primary reason for this is the considerable number of RF chains that are necessary
    for THz MIMO systems, which frequently comprise several tens to hundreds of antennas.
    An array of signals is processed using a hybrid cascaded RF precoder and baseband
    precoder. The RF precoder employs analog phase shifters to precisely manipulate
    the phase of signals entering and departing the antenna components. This enables
    the creation of multiple beams that align with the dominant THz channel paths.
    The baseband precoder exhibits more adaptability compared to the constant-gain/phase-only
    functionalities of the RF precoder. The substantial size of the antenna in the
    hybrid precoder architecture poses challenges in obtaining an optimum full-digital
    precoder. To develop a hybrid precoder, an optimal full-digital precoder must
    be employed. The complete digital precoder is often derived from the dominant
    singular vectors of a channel matrix in the explicit spatial domain. The computation
    of the singular value decomposition (SVD) for the explicit channel matrix is complex
    because of the abundance of antennas. In addition, receiver implementation of
    hybrid precoder designs is commonplace to reduce the quantity of feedback demanded.
    The execution of SVD ensues as a result. Determining the optimal full-digital
    precoder is the initial step in formulating the hybrid precoder design as a sparse
    optimization problem. One potential solution for the hybrid precoder design is
    to employ simultaneous orthogonal matching pursuit (SOMP) [\[40\]](#page-17-14),
    in which the approach aims to select a suitable combination of analog beamforming
    vectors from a pre-determined set of options. Implementing this approach results
    in near-optimal performance. However, the hybrid precoder using the SOMP technique
    necessitates matrix inversion, resulting in a significant increase in complexity.
    It then computes the matching baseband precoding matrix in order to minimize the
    Euclidean distance between the current outcome and the ideal precoding matrix.
    The authors of [\[41\]](#page-17-15) proposed an alternating minimization (Alt-Min)
    approach to separate the hybrid precoder design into two distinct sub-problems.
    Initially, a temporary digital precoder is derived using the least square solution,
    while the initial analog beamformer is generated with random phases. Following
    this, the associated analog beamformer is modified via the phase extraction approach.
    The process of alternate iterations persists until the criterion stated by the
    user is met. The SOMP-based hybrid precoding can be interpreted as a method for
    sparse reconstruction, whilst the Alt-Min algorithm can be seen as a strategy
    for manifold optimization.


    Although there have been several suggestions for adopting hybrid precoding techniques,
    the VLSI implementation of precoding algorithms has not received substantial attention.
    The authors of the study described in [\[42\]](#page-17-16) established a technique
    known as parallel-index-selection matrix-inverse-bypass simultaneous orthogonal
    matching pursuit (PIS-MIB-SOMP). This approach seeks to avoid the necessity of
    performing a complete matrix inversion in the traditional SOMP algorithm. The
    researchers in [\[43\]](#page-17-17) developed a modified version of the SOMP
    technique called orthogonality-based matching pursuit (OBMP) [\[44\]](#page-17-18).
    OBMP employs a discrete Fourier transform (DFT) codebook to replace the original
    candidate array solutions, hence reducing computing expenses. The authors of [\[45\]](#page-17-19)
    introduced an improved iteration of the orthogonal matching pursuit (OMP) method
    in their paper. The enhanced OMP algorithm integrated a revolutionary least-squares
    approach that employed QR decomposition. The authors also investigated the possible
    advantages of utilizing the Coordinate Rotation Digital Computer (CORDIC) method
    for implementing this algorithm within the framework of Very Large Scale Integration
    (VLSI). However, for these designs to serve as a reference design, an ideal precoding
    matrix is required. This matrix can only be constructed by using the SVD of the
    explicit channel matrix. However, this process significantly increases the complexity
    of the transceivers, making it impractically high.


    Inspired by the approach in [\[46\]](#page-17-20) to providing extensive bandwidths,
    short wavelengths at mm-wave or THz generate a spatial signal space with a high
    number of dimensions. This presents an opportunity to leverage high-dimensional
    MIMO techniques in order to achieve substantial capacity increases. Through the
    integration of a hybrid analog-digital transceiver and the beamspace MIMO communication
    concept, continuous aperture-phased MIMO attains performance that is nearly optimal
    while significantly reducing complexity. The researchers demonstrated in [\[47\]](#page-17-21)
    that beamspace singular value decomposition (SVD) is the most efficient approach
    for obtaining a full-digital precoder with minimal effort. It efficiently reduces
    power consumption for both the base station and user equipment. The authors utilize
    compressed sensing (CS)-based channel estimators to get reduced-dimension beamspace
    channel state information (CSI). This algorithm performs SVD implicitly on the
    reduceddimensional beamspace channel instead of explicitly on the large-dimensional
    spatial domain channel. The total complexity is proportional to the number of
    antennas in the MIMO system, which is considerably higher than the channel sparsity.
    The CS-BHP technique described in [\[47\]](#page-17-21) achieves a considerable
    reduction in complexity compared to the state-of-the-art approach by 99.6% by
    utilizing low-dimensional beamspace CSI. This eliminates the need for matrix inversion
    calculations and matching pursuit rounds, making it more efficient than an ideal
    full-digital precoder. Furthermore, it has a performance decrease of less than
    5%. The suggested design in [\[48\]](#page-17-22) utilizes a two-stage precoding
    approach. The first stage involves using a sparse matrix for precoding in the
    beamspace domain, followed by converting the outcome to the antenna domain using
    an inverse fast Fourier transform. This move is performed to simplify the complexity
    of multi-user (MU) precoding in all-digital base station systems. This approach
    uses OMP to calculate sparse precoding matrices in the beamspace domain. As a
    result, this technique reduces the complexity of precoding compared to standard
    linear antenna-domain precoders that involve dense matrix-vector multiplication.
    The output is preprocessed and then transformed into the antenna domain using
    an inverse fast Fourier transform (IFFT). The authors demonstrate that their methods
    provide a bit error-rate (BER) performance that is similar to that of traditional
    antenna-domain Wiener filter (WF) precoding, but with almost double the complexity.


    ## IV. HIGH-SPEED HIGH-RESOLUTION DAC FOR HIGH ORDER MODULATION


    As described in prior research, a Pulse-width Modulation-based Digital-to-Analog
    Converter (PWM-DAC) consists primarily of a filter and a PWM generator [\[49\]](#page-17-23)
    [\[50\]](#page-17-24). Integrated modules for Microcontroller Units (MCUs) or
    Digital Signal Processors (DSPs) constitute the PWM generator, which is principally
    digital. Frequently, the filter is a simple first-order RC filter. The primary
    benefits of PWM-DACs are their low cost and straightforward implementation, which
    are outcomes of their straightforward architecture. These characteristics have
    attracted significant interest in both academic and industrial spheres. A considerable
    number of semiconductor firms that specialize in DSPs and/or MCUs have published
    technical documents detailing the construction of PWM-DACs [\[51\]](#page-17-25)–[\[53\]](#page-17-26).


    DAC performance is contingent on filter design and modulation technique. The bit
    breadth of the counter within the modulator and the efficacy of the low pass filter
    impose constraints on their resolution. In order to implement a DAC with a resolution
    of n bits, the bit width of the counter must be greater than n. In addition, beyond
    the pass band, the filter must have a low cutoff frequency and/or high attenuation,
    which restricts the speed and precision of these DACs.


    Second-order low-pass Butterworth filters are the most economically viable option
    when it comes to PWM-DAC, PCM-DAC, and FONS-DAC. The normalized cutoff frequency
    of the PWM filter, expressed in the least significant bits (LSB), is approximate
    [\[54\]](#page-17-27) for the worst ripple of R<sup>W</sup> C in this instance.


    $$f\_c T = 0.9^2 \sqrt{R\_{WC}/2^n} \,\text{.}\tag{1}$$


    where f<sup>c</sup> is the cutoff frequency and T = 2<sup>n</sup>/fclk is the
    PWM period (n is the bit width of the counter inside the PWM and PCM circuit,
    and fclk is the working clock). The cutoff frequency of the second-order low-pass
    Butterworth filter for a 12-bit PWM-DAC with a 100 MHz clock rate is 250 Hz, and
    the DAC''s settling time is 0.76 ms; therefore, the conversion rate is less than
    1 kHz, given that the settling time can only represent a negligible portion of
    the conversion period. Moreover, as the resolution (n) of the DAC increased, there
    was a significant decrease in fcT; consequently, achieving a high-resolution PWM-DAC
    operating at a reasonable speed will be more difficult.


    In order to increase the speed, PCM-DAC and first-order noise shaping DAC (FONS-DAC)
    reduce the stringent requirement for fcT through the application of extremely
    frequent pulses. Nevertheless, PCM periods contain a large number of ascending
    and descending edges, which leads to significant static error as a consequence
    of the edges'' nonideal properties.


    The edge induces a maximum error of 2 <sup>11</sup> × ∆W × fclk for a 12-bit PCM-DAC,
    where ∆W represents the pulse width deviation resulting from the delay differential
    between the rising and falling edges. Consequently, a 2048-fold increase in the
    pulse width deviation can significantly compromise the accuracy of the DAC. Additionally,
    PCM-DAC and FONS-DAC exhibit the worst error performance as pulse density increases.


    ## <span id="page-8-0"></span>*A. Principle and spectrum of MPWM*


    ![](_page_8_Figure_1.jpeg)


    Fig. 1: The waveform of unfiltered MPWM digital output.


    <span id="page-8-1"></span>The period of the MPWM signal for an n-bit counter
    may be calculated using the formula T = 2n/fclk, where fclk represents the clock
    frequency. We define two terms: the splitting factor (SF) and the splitting number
    (SN). These terms are related by SN = 2SF . MPWM is divided into SN sub-regions.
    The waveform of MPWM for n = 5 and SF = 1, 2, 4 is displayed in Fig. [1](#page-8-0)
    for a duration of 32 clock periods. The matching spectrum can be seen in Fig.
    [2.](#page-8-1)


    ![](_page_8_Figure_4.jpeg)


    Fig. 2: The spectrum of unfiltered MPWM digital output.


    <span id="page-8-2"></span>![](_page_8_Figure_6.jpeg)


    Fig. 3: The MPWM generation using a 5-bit counter with 2 address bits.


    <span id="page-9-0"></span>Fig. [3](#page-8-2) illustrates the principle of generating
    the MPWM wave. The lower significant bits (SF) of the counter correspond to the
    address, which represents the sub-region inside an MPWM period T. The upper bits,
    namely (5 − SF) bits, indicate the specific "wave" that should be placed in the
    sub-region. The counter''s address bits are sent to an Address Decoder, which
    produces an output of SN bits. When the output of the address decoder is a signal
    of length SN with a binary value of 1 at location SNpos+1. This signal represents
    the location of the "wave" inside the full MPWM signal. The higher (5 − SF) bits
    of the counter are connected to the "WAV decoder", whose output WAV is a signal
    with a length of 2 <sup>5</sup>−SF . Fig. [4](#page-9-0) displays


    | SN pos | Waveform | WAV | Waveform                      |

    |--------|----------|-----|-------------------------------|

    |        | 1 0 0 0  | 000 | 1   0  0  0  0  0  0  0  0  0 |

    | o      |          | 001 | 1 1                           |

    |        | 0 0 1 0  | 010 | 1 1 1                         |

    | 1      |          | 011 | 1 1 1<br>ਜ                    |

    |        | 0 1 0 0  | 100 | । 1<br>1 1 1 1                |

    | 2      |          | 101 | 1 1 1 1 1 1 1                 |

    |        | 0 0 0 1  | 110 | 1 1 1 1 1 1 1                 |

    | ന      |          | 111 | 1 1 1 1 1 1 1 1               |


    Fig. 4: The waveform of the output of Address Decoder and Data Decoder.


    <span id="page-9-1"></span>the waveform that corresponds to the WAV format. The
    operation of the WAV Decoder is governed by the Duty input. During an MPWM period,
    the MPWM creates a waveform as shown in Figure [5](#page-9-1) for a specified
    number of clock cycles called the "Duty". After that, it outputs a bit with a
    value of 0 until the next MPWM period starts. In order to examine the spectrum
    of


    $$\begin{array}{c} \mathsf{SN}\_{\mathsf{P}\mathsf{A}\mathsf{T}} \stackrel{\mathsf{0}\
    \mathsf{0}\ \mathsf{T}}{\mathsf{T}} \stackrel{\mathsf{0}\ \mathsf{0}\ \mathsf{T}}{\longleftarrow}
    \begin{array}{c} \mathsf{WAV} : \mathsf{T} \stackrel{\mathsf{0}\ \mathsf{0}\ \mathsf{0}\
    \mathsf{0}\ \mathsf{0}\ \mathsf{0}\ \mathsf{0}\ \mathsf{0}\ \mathsf{0}\ \mathsf{0}\
    \mathsf{0}\ \mathsf{0}\ \mathsf{0}\ \mathsf{0}\ \mathsf{0}\ \mathsf{0}\ \mathsf{0}\
    \mathsf{0}\ \mathsf{0}\ \mathsf{0}\ \mathsf{0}\ \mathsf{0}\ \mathsf{0}\ \mathsf{0}\
    \mathsf{0}\ \mathsf{0}\ \mathsf{0}\ \mathsf{0}\ \mathsf{0}\ \mathsf{0}\ \mathsf{0}\
    \mathsf{0}\ \mathsf{0}\ \mathsf{0}\ \mathsf{0}\ \mathsf{0}\ \mathsf{0}\ \mathsf{0}\
    \mathsf{0}\ \mathsf{0}\ \mathsf{0}\ \mathsf{0}\ \mathsf{0}\ \mathsf{0}\ \mathsf{0}\
    \mathsf{0}\ \mathsf{0}\ \mathsf{0}\ \mathsf{0}\end{array}$$


    Fig. 5: MPWM waveform in the case of Duty = 19, SNpos = 2, and Data = 4.


    MPWM, we will configure the counter to have a bit width of 5 and establish a unit
    signal as follows:


    $$x\_{unit} = \begin{cases} \\ 0, \end{cases} \quad \text{or} \quad T/32 \le t
    \le (m+1)T/32 \\ \tag{2}$$


    which represents the signal at t = mT /32, the duration is T /32, and the period
    is T. By changing the value of m, xunit(t) can be used to form various MPWM waveforms.
    According to the Fourier series formula of the periodic signal as follows:


    $$x(t) = \sum\_{k=-\infty}^{\infty} a\_k e^{-jk\frac{2\pi}{T}t} dt. \tag{3}$$


    $$a\_k = \frac{1}{T} \int\_T x(t)e^{-jk\frac{2\pi}{T}t}dt. \tag{4}$$


    the Fourier series of xunit(t) is calculated as


    $$a\_k = \begin{cases} 1/32, & k = 0\\ e^{-jk(\pi/32 + m\pi/16)} \sin(k\pi/32)/k\pi,
    & k = 1, \ldots, 31. \end{cases} \tag{5}$$


    By adding xunit(t) with different values of m, we can get MPWM with the splitting
    number SN = 2, 4, and 8, respectively. According to the linear properties of the
    Fourier series, it is easy to calculate the Fourier series of these MPWMs. The
    frequency spectrum of PWM, PCM, FONS, and MPWM are shown in Fig. [6.](#page-10-0)
    The harmonic energy of the MPWM is mainly at SN/T. Fig. [8](#page-10-1) and Fig.
    [7](#page-10-2) illustrate the distinctions among PWM, PCM, FONS, and harmonic
    components. In comparison to typical PWM, MPWM utilizes a low-pass filter with
    a greater cut-off frequency, hence reducing the demands for filter design in DAC
    applications. MPWM exhibits a lower number of edges in the time domain when compared
    to PCM and FONS. This characteristic can effectively decrease the integral nonlinearity
    of the DAC, as discussed in Section III.


    <span id="page-10-0"></span>![](_page_10_Figure_0.jpeg)


    <span id="page-10-2"></span>Fig. 6: The Spectrum comparison of the unfiltered
    digital output.


    ![](_page_10_Figure_2.jpeg)


    <span id="page-10-1"></span>Fig. 7: The waveform comparison of the unfiltered
    digital output.


    |      |      |                | Maximum of Amplitude |              |  |

    |------|------|----------------|----------------------|--------------|--|

    |      |      | Number of Edge | f, Amp,              | f2,Amp2      |  |

    | PWM  |      | 17, 0.575<br>2 |                      | 2/7,0.430    |  |

    | PCM  |      | 25 = 32        | 16/p , 0.602         | 8/7 , 0.891  |  |

    | FONS |      | 25 = 32        | 16/p , 0.602         | 12/p , 0.640 |  |

    | MPWM | SF=1 | 2×2 = 4        | 2,426                | 4/7 , 0.619  |  |

    |      | SF=2 | 2×22 = 8       | 4/7, 0.617           | 8/7, 0.619   |  |

    |      | SF=3 | 2×23 =16       | 8/7, 0.614           | 16/7,0.607   |  |


    Fig. 8: Differences among four waveforms: f<sup>1</sup> and f<sup>2</sup> are
    the frequencies with the maximum and the second largest amplitude. Amplitudes
    have been normalized to their DC value.


    ## *B. Performance of MPWM-DACs*


    *1) Static error:* The DC component of the output of an MPWM-DAC can be expressed
    as in Eq[.6,](#page-11-0) where u<sup>d</sup> is the digital output of MPWM and
    D is the nominal duty (the actual duty is D/2 <sup>n</sup>, where n is the bit
    width of the counter).


    <span id="page-11-0"></span>

    $$u\_d(D) = \frac{1}{T} \int\_{t\_1}^{t\_1 + T} u\_d(t, D) dt. \tag{6}$$


    The static error of an MPWM-DAC is defined as in Eq[.7,](#page-11-1) where u<sup>D</sup>
    is the ideal DC voltage value corresponding to the duty D. ULSB is the voltage
    corresponding to the DAC''s LSB.


    <span id="page-11-1"></span>

    $$e\_s(D) = \frac{\bar{u\_d}(D) - u\_D}{U\_{LSB}} \tag{7}$$


    The primary cause of static error in an MPWM-DAC is the combination of power supply
    error and error resulting from edges. Given that the maximum value of the output
    voltage ud(D) is equal to the power supply voltage Us, the direct current component
    of the MPWM-DAC''s output can have the same level of inaccuracy as the power supply.
    MPWM-DAC shares the same characteristics as PWM-DAC, PCM-DAC, and FONS-DAC in
    this regard.


    As for errors caused by edges, since the rising and falling edges of the MPWM
    output are nonideal, errors can be brought. The nonideal characteristics of edges
    can be modeled using Trapezoid model [\[55\]](#page-17-28) as follows:


    <span id="page-11-2"></span>

    $$e\_{edge} = E(D)(t\_{dr} - t\_{df})f\_{clk} \tag{8}$$


    where E(D) is the number of positive (or negative) edges for duty D, tdr is the
    delay from the half amplitude of the clock signal to that of the MPWM digital
    output for the rising edge, and tdf is the delay for the falling edge.


    For MPWM, the number of edges is


    <span id="page-11-3"></span>

    $$E\_{MPWM} = \begin{cases} D, & D \leqslant SN \\ SN, & SN < D \leqslant (2^n
    - SN) \\ 2^n - D, & D > (2^n - SN) . \end{cases} \tag{9}$$


    For PWM, the maximum number of positive edges is 1, thus has the least edge-caused
    error. For PCM, and FONS, the maximum number of edges is 2 n−1 . When using a
    counter with 12 bits, there are 2048 edges in most PCM (or FONS) periods, which
    can cause severe errors.


    For MPWM, the maximum number of edges is SN=2 SF , where the SF can be chosen
    from 1 to 10 for a 12-bit counter, hence MPWM has fewer edges and thus reduces
    the static error caused by edges proportionally.


    *2) Integral nonlinearity (INL):* The integral nonlinearity (INL) of a DAC is
    the maximum deviation of the actual analog output from the ideal output. The INL
    for low-cost DAC can be calculated as in [\[56\]](#page-17-29):


    $$e\_{INL} = \max\_{D} |\ e\_{edge}(D)\rangle|\tag{10}$$


    For MPWM-DAC, the INL is as follows:


    $$|e\_{INL}(MPWM)| = |\, 2^{n-1}(t\_dr - t\_df)f\_{clk}| \, | \, SF = 1, \ldots,
    n-2. \tag{11}$$


    For PWM-DAC, PCM-DAC, and FONS-DAC, the INL is as follows:


    $$e\_{INL} = \begin{cases} |(t\_d r - t\_d f) f\_{clk}| \, | \, \text{PWM} \\
    |\, 2^{n-1} (t\_d r - t\_d f) f\_{clk}| \, | \, \text{PCM} \, \& \text{FONS} .
    \end{cases} \tag{12}$$


    Since SF ranges from 1 to n − 1, MPWM can have much better INL performance than
    PCM-DAC and FONS-DAC.


    *3) Differential nonlinearity (DNL):* The Differential nonlinearity(DNL) for DAC
    is defined as the maximum voltage deviation of the DAC output between two adjacent
    digital inputs in terms of an ideal output voltage step corresponding to 1 LSB.
    It is calculated as:


    $$\varepsilon\_{DNL} = \frac{\bar{u\_d}(D+1) - \bar{u\_d}(D)}{U\_{LSB}} - 1. \tag{13}$$


    Substitude Eq[.8](#page-11-2) and Eq[.9,](#page-11-3) the DNL for MPWM is


    $$|e\_{DNL}(MPWM)| = |(t\_dr - t\_df)f\_{clk}|\,. \tag{14}$$


    The DNL performance of MPWM is the same as that of PWM-DAC, PCM-DAC, and FONS-DAC.
    To improve the DNL of MPWM, the output stage needs to be as symmetric as possible,
    i.e. having the same characteristics for the rising and falling edges, to reduce
    the | tdr − tdf | item. This symmetry is crucial when the clock frequency is high.
    Due to its operating principle, MPWM-DAC is monotonic.


    <span id="page-12-0"></span>![](_page_12_Figure_0.jpeg)


    Fig. 9: Normalized cutoff frequency vs. DAC resolution, using second order Butterworth
    low pass filter.


    <span id="page-12-1"></span>*4) Dynamic Characteristics:* For dynamic characteristics
    analysis, getting a general analytical computation for the output from MPWM-DAC
    is quite difficult, therefore, we use numerical computation. The ripple of the
    output of an MPWM-DAC with second order Butterworth low pass filter is calculated
    to get normalized cutoff frequency fcT of the filter for different SF in Fig.
    [9.](#page-12-0) The fcT increases with the increase of SF. For instance, when
    designing a 12-bit DAC, fcT is 0.01 for PWM-DAC, and for MPWM-DAC it increases
    to 0.04 and 0.1 when SF equals 3 and 7 respectively. Thus the cutoff frequency
    is 4 and 10 times that of PWM-DAC, and the settling time is 1/4 and 1/10 of that
    of PWM-DAC.


    ## V. FPGA DESIGN AND RISC-V SOC


    ![](_page_12_Figure_4.jpeg)


    Fig. 10: MPWM Circuit.


    # *A. MPWM circuit design*


    The block diagram for an MPWM circuit is shown in Fig[.10,](#page-12-1) where
    the bit width of the counter is n. For an MPWM wave with SF, the output of the
    counter is rearranged according to CR={Cn−SF <sup>−</sup>1, Cn−SF <sup>−</sup>2,
    · · · , C0, Cn−SF , · · · , Cn−2, Cn−1}, where C<sup>0</sup> to Cn−<sup>1</sup>
    are the output of the counter, and C<sup>R</sup> is the result of rearrangement.
    The duty target is input from dutyinput, which ranges from 0 to 2 <sup>n</sup>−<sup>1</sup>
    − 1. Then dutyinput is compared with CR, and MPWM outputs according to Eq. [15.](#page-12-2)


    <span id="page-12-2"></span>

    $$MPWM\\_OUT = \begin{cases} 1, \text{ } duty\\_input \ge C\_r\\ 0, \text{ } duty\\_input
    < C\_r. \end{cases} \tag{15}$$


    ## *B. HR-MPWM design in FPGA*


    To increase the DAC''s resolution, one can increase the bit width of the counter,
    but the clock rate needs to be raised simultaneously, or the DAC''s speed will
    decrease proportionally. In many cases, it is very difficult to raise the clock
    rate due to the process limit or power consumption. To solve this problem, the
    circuit to generate a pulse with a resolution better than one clock cycle can
    be designed. An MPWM with this kind of circuit is named high-resolution MPWM,
    i.e. HR-MPWM. When implemented in Xilinx FPGA, the HR-MPWM circuit is shown in
    Fig. [11,](#page-13-0) where IODELAY E1 is the I/O delay element and can allow
    a fine delay-time t<sup>d</sup> = 1/(32 × 2 × fclkref ). Thus, this circuit can
    provide an additional 6-bit resolution for MPWM. Considering the HR-MPWM in the
    final SoC needs only 4 additional bits, the design in FPGA uses a delay tap out
    of every 4 taps.


    <span id="page-13-0"></span>![](_page_13_Figure_0.jpeg)


    Fig. 11: Block diagram of HR-MPWM in FPGA.


    # *C. SoC Structure with the proposed DAC*


    The MPWM-DAC is integrated into a RISC-V SoC for IoT applications. Its position
    within the SoC level range is shown in Fig. [15,](#page-15-0) where it is located
    alongside its associated auxiliary modules within the "MPWM-DAC".


    Control over the entire system is exerted by an open-source 32-bit RISC-V core.
    For the support of complex vector operations, commonly required in AI acceleration,
    a specialized parallel computing core is strategically deployed. Configuration
    of the proposed MPWM-DAC module is undertaken by the RISC-V core, facilitated
    through a high-speed bus linking them. The responsibility for managing massive
    data transportation falls to DMAs. Wireless communication capabilities are introduced
    via an off-chip Wi-Fi chip, which is connected to a UART port. The system''s memory
    requirements are addressed with on-chip 128KB instruction SRAM and an equal capacity
    for data SRAM. Additionally, the External Memory Interface is employed to access
    further memory, with the capacity to address nearly the entirety of the remaining
    4GB.


    ## <span id="page-13-1"></span>*D. Test in FPGA*


    ![](_page_13_Figure_6.jpeg)


    Fig. 12: DLL design in the RISC-V SoC.


    The prototyping SoC with MPWM-DAC is implemented in an Xilinx ARTIX-7 FPGA (XC7A200TFBG484-3),
    as shown in Fig. [16.](#page-15-1) The second-order Butterworth filter is connected
    to the output of the DAC. The INL and DNL for an MPWM-DAC with 10-bit resolution
    are measured, and compared with PWM-DAC and PCM-DAC as presented in Fig. [13.](#page-14-2)


    These results show that MPWM-DAC has improved the INL performance greatly when
    compared to PCM-DAC, and has a shorter settling time than PWM-DAC in Fig. [14.](#page-14-3)
    When designing an MPWM-DAC, one can get a higher accurate DAC by choosing a smaller
    SF or get a faster DAC by selecting a larger SF.


    ## *E. RISC-V SOC using MPWM-DAC for THz IoT devices*


    In the SoC, the HRMPWM module is a fully customized design, which comprises a
    typical PWM generator and delay elements calibrated by a DLL. The DLL is applied
    to get 16 fine phases, refer to Fig. [12.](#page-13-1) After the DLL is locked,
    vc is generated and applied to the signal path for MPWM, which has the same delay
    element as in DLL. A multiplexer circuit with the same delay in every branch is
    employed to get the required phase. The laboratory experiment for the FPGA test
    is


    <span id="page-14-3"></span><span id="page-14-2"></span>![](_page_14_Figure_0.jpeg)


    Fig. 14: Setting time performance for MPWM.


    illustrated in Fig. [16](#page-15-1) and the layout of the MPWM SoC is shown in
    Fig. [17.](#page-16-23) It is fabricated (taped out in March 2021) in a 180nm
    CMOS process with a size of 3.8mm × 4.7mm. The size of HRMPWM is 0.53mm × 0.27mm.


    ## VI. CONCLUSION


    THz communications are expected to have a crucial impact on the development of
    wireless systems in the sixth generation (6G). This study provides a comprehensive
    analysis of THz IoT devices. In addition, the MPWM modulation mechanism, introduced
    in an MPWM circuit with a n-bit counter, may exhibit (n−1) distinct configurations
    and waveforms. The MPWM-DAC is developed using the MPWM technology, offering superior
    conversion speed compared to PWM-DAC, and higher accuracy than the PCM-DAC. By
    virtue of its versatility, the designer is able to modify their design to suit
    unique applications, resulting in a superior outcome. An implementation using
    a delay line has been developed in order to enhance the resolution of MPWM-DAC.
    The MPWM-DAC is included in a RISC-V SoC and evaluated on an FPGA. The SoC is
    specifically engineered using a 180nm CMOS fabrication technique.


    ## REFERENCES


    - <span id="page-14-0"></span>[1] Y. Xu, Z. Liu, C. Huang, and C. Yuen, "Robust
    resource allocation algorithm for energy-harvesting-based d2d communication underlaying
    uav-assisted networks," *IEEE Internet of Things Journal*, vol. 8, no. 23, pp.
    17 161–17 171, 2021.

    - <span id="page-14-1"></span>[2] H. Jang, K. Han, S. Lee, J.-J. Lee, S.-Y. Lee,
    J.-H. Lee, and W. Lee, "Developing a multicore platform utilizing open risc-v
    cores," *IEEE Access*, vol. 9, pp. 120 010–120 023, 2021.


    <span id="page-15-0"></span>![](_page_15_Figure_0.jpeg)


    <span id="page-15-1"></span>Fig. 15: A brief design architecture of the RISC-V
    SoC for THz IoT devices.


    ![](_page_15_Picture_2.jpeg)


    Fig. 16: Experimental setup with FPGA.


    **ESP8266 Wi-Fi Module**


    <span id="page-16-23"></span>![](_page_16_Figure_0.jpeg)


    CPU1 Fig. 17: Layout of the RISC-V SoC.


    - <span id="page-16-0"></span>[3] H. B. Amor, C. Bernier, and Z. Pˇrikryl, "A
    risc-v isa extension for ultra-low power iot wireless signal processing," *IEEE
    Transactions on Computers*, vol. 71, no. 4, pp. 766–778, 2022.

    - <span id="page-16-1"></span>[4] T. Kurner and S. Priebe, "Towards thz communications
    - status in research, standardization and regulation," ¨ *Journal of Infrared,
    Millimeter, and Terahertz Waves*, vol. 35, pp. 53–62, 2014. [Online]. Available:<https://api.semanticscholar.org/CorpusID:28353094>

    - <span id="page-16-2"></span>[5] H. Sarieddeen, M.-S. Alouini, and T. Y. Al-Naffouri,
    "Terahertz-band ultra-massive spatial modulation mimo," *IEEE Journal on Selected
    Areas in Communications*, vol. 37, no. 9, pp. 2040–2052, 2019.

    - <span id="page-16-3"></span>[6] P. Heydari, "Terahertz integrated circuits and
    systems for high-speed wireless communications: Challenges and design perspectives,"
    *IEEE Open Journal of the Solid-State Circuits Society*, vol. 1, pp. 18–36, 2021.

    - <span id="page-16-4"></span>[7] M. Alibakhshikenari, E. M. Ali, M. Soruri, M.
    Dalarsson, M. Naser-Moghadasi, B. S. Virdee, C. Stefanovic, A. Pietrenko-Dabrowska,
    S. Koziel, S. Szczepanski, and E. Limiti, "A comprehensive survey on antennas
    on-chip based on metamaterial, metasurface, and substrate integrated waveguide
    principles for millimeter-waves and terahertz integrated circuits and systems,"
    *IEEE Access*, vol. 10, pp. 3668–3692, 2022.

    - <span id="page-16-5"></span>[8] H. Jalili and O. Momeni, "A 0.34-thz wideband
    wide-angle 2-d steering phased array in 0.13- µ m sige bicmos," *IEEE Journal
    of Solid-State Circuits*, vol. 54, no. 9, pp. 2449–2461, 2019.

    - <span id="page-16-6"></span>[9] M. Alibakhshikenari, B. S. Virdee, C. H. See,
    R. A. Abd-Alhameed, F. Falcone, and E. Limiti, "High-performance 50µm silicon-based
    on-chip antenna with high port-to-port isolation implemented by metamaterial and
    siw concepts for thz integrated systems," in *2019 Thirteenth International Congress
    on Artificial Materials for Novel Wave Phenomena (Metamaterials)*, 2019, pp. X–023–X–025.

    - <span id="page-16-7"></span>[10] X. Yi, C. Wang, X. Chen, J. Wang, J. Grajal,
    and R. Han, "A 220-to-320-ghz fmcw radar in 65-nm cmos using a frequency-comb
    architecture," *IEEE Journal of Solid-State Circuits*, vol. 56, no. 2, pp. 327–339,
    2021.

    - <span id="page-16-8"></span>[11] H. Jalili and O. Momeni, "A 0.46-thz 25-element
    scalable and wideband radiator array with optimized lens integration in 65-nm
    cmos," *IEEE Journal of Solid-State Circuits*, vol. 55, no. 9, pp. 2387–2400,
    2020.

    - <span id="page-16-9"></span>[12] C.-H. Li and T.-Y. Chiu, "340-ghz low-cost
    and high-gain on-chip higher order mode dielectric resonator antenna for thz applications,"
    *IEEE Transactions on Terahertz Science and Technology*, vol. 7, no. 3, pp. 284–294,
    2017.

    - <span id="page-16-10"></span>[13] K. Schmalz, R. Wang, J. Borngraber, W. Debski,
    W. Winkler, and C. Meliani, "245 ghz sige transmitter with integrated antenna
    and external pll," in ¨ *2013 IEEE MTT-S International Microwave Symposium Digest
    (MTT)*, 2013, pp. 1–3.

    - <span id="page-16-11"></span>[14] W. T. Khan, A. C¸ agrı Ulusoy, G. Dufour,
    M. Kaynak, B. Tillack, J. D. Cressler, and J. Papapolymerou, "A d-band micromachined
    end-fire antenna in ˘ 130-nm sige bicmos technology," *IEEE Transactions on Antennas
    and Propagation*, vol. 63, no. 6, pp. 2449–2459, 2015.

    - <span id="page-16-12"></span>[15] I. Sarkas, J. Hasch, A. Balteanu, and S. P.
    Voinigescu, "A fundamental frequency 120-ghz sige bicmos distance sensor with
    integrated antenna," *IEEE Transactions on Microwave Theory and Techniques*, vol.
    60, no. 3, pp. 795–812, 2012.

    - <span id="page-16-13"></span>[16] Y. Tousi and E. Afshari, "A high-power and
    scalable 2-d phased array for terahertz cmos integrated systems," *IEEE Journal
    of Solid-State Circuits*, vol. 50, no. 2, pp. 597–609, 2015.

    - <span id="page-16-14"></span>[17] M. Alibakhshikenari, B. S. Virdee, C. H. See,
    R. Abd-Alhameed, F. Falcone, and E. Limiti, "A novel 0.3-0.31 thz gaas-based transceiver
    with on-chip slotted metamaterial antenna based on siw technology," in *2019 IEEE
    Asia-Pacific Microwave Conference (APMC)*, 2019, pp. 69–71.

    - <span id="page-16-15"></span>[18] M. H. Maktoomi, S. Saadat, O. Momeni, P. Heydari,
    and H. Aghasi, "Broadband antenna design for terahertz communication systems,"
    *IEEE Access*, vol. 11, pp. 20 897–20 911, 2023.

    - <span id="page-16-16"></span>[19] H. J. Ng and D. Kissinger, "Highly miniaturized
    120-ghz simo and mimo radar sensor with on-chip folded dipole antennas for range
    and angular measurements," *IEEE Transactions on Microwave Theory and Techniques*,
    vol. 66, no. 6, pp. 2592–2603, 2018.

    - <span id="page-16-17"></span>[20] J. M. Edwards and G. M. Rebeiz, "High-efficiency
    elliptical slot antennas with quartz superstrates for silicon rfics," *IEEE Transactions
    on Antennas and Propagation*, vol. 60, no. 11, pp. 5010–5020, 2012.

    - <span id="page-16-18"></span>[21] M. Saad Khan, F. A. Tahir, A. Meredov, A.
    Shamim, and H. M. Cheema, "A w-band ebg-backed double-rhomboid bowtie-slot on-chip
    antenna," *IEEE Antennas and Wireless Propagation Letters*, vol. 18, no. 5, pp.
    1046–1050, 2019.

    - <span id="page-16-19"></span>[22] M. Frank, F. Lurz, R. Weigel, and A. Koelpin,
    "122 ghz low-cost substrate integrated waveguide based leaky-wave antenna design,"
    in *2019 IEEE Radio and Wireless Symposium (RWS)*, 2019, pp. 1–4.

    - <span id="page-16-20"></span>[23] H. H. Bae, T. H. Jang, H. Y. Kim, and C. S.
    Park, "Broadband 120 ghz l-probe differential feed dual-polarized patch antenna
    with soft surface," *IEEE Transactions on Antennas and Propagation*, vol. 69,
    no. 10, pp. 6185–6195, 2021.

    - <span id="page-16-21"></span>[24] A. Simsek, A. S. H. Ahmed, A. A. Farid, U.
    Soylu, and M. J. W. Rodwell, "A 140ghz two-channel cmos transmitter using low-cost
    packaging technologies," in *2020 IEEE Wireless Communications and Networking
    Conference Workshops (WCNCW)*, 2020, pp. 1–3.

    - <span id="page-16-22"></span>[25] A. Simsek, S.-K. Kim, M. Abdelghany, A. S.
    H. Ahmed, A. A. Farid, U. Madhow, and M. J. W. Rodwell, "A 146.7 ghz transceiver
    with 5 gbaud data transmission using a low-cost series-fed patch antenna array
    through wirebonding integration," in *2020 IEEE Radio and Wireless Symposium (RWS)*,
    2020, pp. 68–71.

    - <span id="page-17-0"></span>[26] R. Han and E. Afshari, "A cmos high-power broadband
    260-ghz radiator array for spectroscopy," *IEEE Journal of Solid-State Circuits*,
    vol. 48, no. 12, pp. 3090–3104, 2013.

    - <span id="page-17-1"></span>[27] Y. Yang, O. D. Gurbuz, and G. M. Rebeiz, "An
    eight-element 370–410-ghz phased-array transmitter in 45-nm cmos soi with peak
    eirp of 8–8.5 dbm," *IEEE Transactions on Microwave Theory and Techniques*, vol.
    64, no. 12, pp. 4241–4249, 2016.

    - <span id="page-17-2"></span>[28] H. Saeidi, S. Venkatesh, C. R. Chappidi, T.
    Sharma, C. Zhu, and K. Sengupta, "29.9 a 4×4 distributed multi-layer oscillator
    network for harmonic injection and thz beamforming with 14dbm eirp at 416ghz in
    a lensless 65nm cmos ic," in *2020 IEEE International Solid-State Circuits Conference
    - (ISSCC)*, 2020, pp. 256–258.

    - <span id="page-17-3"></span>[29] P. Herrero and J. Schoebel, "Microstrip patch
    array antenna technology for 122 ghz ism sensing applications," in *2009 German
    Microwave Conference*, 2009, pp. 1–4.

    - <span id="page-17-4"></span>[30] S. Shahramian, M. Holyoak, A. Singh, B. J.
    Farahani, and Y. Baeyens, "A fully integrated scalable w-band phased-array module
    with integrated antennas, self-alignment and self-test," in *2018 IEEE International
    Solid-State Circuits Conference-(ISSCC)*. IEEE, 2018, pp. 74–76.

    - <span id="page-17-5"></span>[31] E. Naviasky, L. Iotti, G. LaCaille, B. Nikolic,
    E. Alon, and A. M. Niknejad, "A 71-to-86-ghz 16-element by 16-beam multi-user
    beamforming integrated ´ receiver sub-array for massive mimo," *IEEE Journal of
    Solid-State Circuits*, vol. 56, no. 12, pp. 3811–3826, 2021.

    - <span id="page-17-6"></span>[32] M. Alibakhshikenari, B. S. Virdee, C. H. See,
    R. A. Abd-Alhameed, F. Falcone, and E. Limiti, "Silicon-based 0.450-0.475 thz
    series-fed double dielectric resonator on-chip antenna array based on metamaterial
    properties for integrated-circuits," in *2019 Thirteenth International Congress
    on Artificial Materials for Novel Wave Phenomena (Metamaterials)*, 2019, pp. X–026–X–028.

    - <span id="page-17-7"></span>[33] M. Alibakhshikenari, B. S. Virdee, C. H. See,
    R. A. Abd-Alhameed, and E. Limiti, "High performance on-chip array antenna based
    on metasurface feeding structure for terahertz integrated circuits," in *2019
    44th International Conference on Infrared, Millimeter, and Terahertz Waves (IRMMW-THz)*,
    2019, pp. 1–2.

    - <span id="page-17-8"></span>[34] R. Beica, "Flip chip market and technology
    trends," in *2013 Eurpoean Microelectronics Packaging Conference (EMPC)*. IEEE,
    2013, pp. 1–4.

    - <span id="page-17-9"></span>[35] W. Shin, B.-H. Ku, O. Inac, Y.-C. Ou, and G.
    M. Rebeiz, "A 108–114 ghz 4 × 4 wafer-scale phased array transmitter with high-efficiency
    on-chip antennas," *IEEE Journal of Solid-State Circuits*, vol. 48, no. 9, pp.
    2041–2055, 2013.

    - <span id="page-17-10"></span>[36] B. Yang, Z. Yu, J. Lan, R. Zhang, J. Zhou,
    and W. Hong, "Digital beamforming-based massive mimo transceiver for 5g millimeter-wave
    communications," *IEEE Transactions on Microwave Theory and Techniques*, vol.
    66, no. 7, pp. 3403–3418, 2018.

    - <span id="page-17-11"></span>[37] S. Mondal, R. Singh, A. I. Hussein, and J.
    Paramesh, "A 25–30 ghz fully-connected hybrid beamforming receiver for mimo communication,"
    *IEEE Journal of Solid-State Circuits*, vol. 53, no. 5, pp. 1275–1287, 2018.

    - <span id="page-17-12"></span>[38] A. Natarajan, A. Komijani, X. Guan, A. Babakhani,
    and A. Hajimiri, "A 77-ghz phased-array transceiver with on-chip antennas in silicon:
    Transmitter and local lo-path phase shifting," *IEEE Journal of Solid-State Circuits*,
    vol. 41, no. 12, pp. 2807–2819, 2006.

    - <span id="page-17-13"></span>[39] B. Sadhu, Y. Tousi, J. Hallin, S. Sahl, S.
    K. Reynolds, O. Renstrom, K. Sjogren, O. Haapalahti, N. Mazor, B. Bokinge, G.
    Weibull, H. Bengtsson, A. Carlinger, E. Westesson, J.-E. Thillberg, L. Rexberg,
    M. Yeck, X. Gu, M. Ferriss, D. Liu, D. Friedman, and A. Valdes-Garcia, "A 28-ghz
    32-element trx phased-array ic with concurrent dual-polarized operation and orthogonal
    phase and gain control for 5g communications," *IEEE Journal of Solid-State Circuits*,
    vol. 52, no. 12, pp. 3373–3391, 2017.

    - <span id="page-17-14"></span>[40] T. T. Cai and L. Wang, "Orthogonal matching
    pursuit for sparse signal recovery with noise," *IEEE Transactions on Information
    Theory*, vol. 57, no. 7, pp. 4680–4688, 2011.

    - <span id="page-17-15"></span>[41] X. Yu, J.-C. Shen, J. Zhang, and K. B. Letaief,
    "Alternating minimization algorithms for hybrid precoding in millimeter wave mimo
    systems," *IEEE Journal of Selected Topics in Signal Processing*, vol. 10, no.
    3, pp. 485–500, 2016.

    - <span id="page-17-16"></span>[42] Y.-Y. Lee, C.-H. Wang, and Y.-H. Huang, "A
    hybrid rf/baseband precoding processor based on parallel-index-selection matrix-inversion-bypass
    simultaneous orthogonal matching pursuit for millimeter wave mimo systems," *IEEE
    Transactions on Signal Processing*, vol. 63, no. 2, pp. 305–317, 2015.

    - <span id="page-17-17"></span>[43] C.-K. Ho, H.-Y. Cheng, and Y.-H. Huang, "Hybrid
    precoding processor for millimeter wave mimo communications," *IEEE Transactions
    on Circuits and Systems II: Express Briefs*, vol. 66, no. 12, pp. 1992–1996, 2019.

    - <span id="page-17-18"></span>[44] W.-L. Hung, C.-H. Chen, C.-C. Liao, C.-R.
    Tsai, and A.-Y. A. Wu, "Low-complexity hybrid precoding algorithm based on orthogonal
    beamforming codebook," in *2015 IEEE Workshop on Signal Processing Systems (SiPS)*,
    2015, pp. 1–5.

    - <span id="page-17-19"></span>[45] K.-T. Chen, Y.-T. Hwang, and Y.-C. Liao, "Vlsi
    design of a high throughput hybrid precoding processor for wireless mimo systems,"
    *IEEE Access*, vol. 7, pp. 85 925–85 936, 2019.

    - <span id="page-17-20"></span>[46] J. Brady, N. Behdad, and A. M. Sayeed, "Beamspace
    mimo for millimeter-wave communications: System architecture, modeling, analysis,
    and measurements," *IEEE Transactions on Antennas and Propagation*, vol. 61, no.
    7, pp. 3814–3827, 2013.

    - <span id="page-17-21"></span>[47] C.-H. Chen, C.-R. Tsai, Y.-H. Liu, W.-L. Hung,
    and A.-Y. Wu, "Compressive sensing (cs) assisted low-complexity beamspace hybrid
    precoding for millimeter-wave mimo systems," *IEEE Transactions on Signal Processing*,
    vol. 65, no. 6, pp. 1412–1424, 2017.

    - <span id="page-17-22"></span>[48] E. Gon¨ ultas¸, S. Taner, A. Gallyas-Sanhueza,
    S. H. Mirfarshbafan, and C. Studer, "Hardware-aware beamspace precoding for all-digital
    mmwave massive ¨ mu-mimo," *IEEE Communications Letters*, vol. 25, no. 11, pp.
    3709–3713, 2021.

    - <span id="page-17-23"></span>[49] D. F. Hoeschele, "Analog-to-digital and digital-to-analog
    conversion techniques," pp. 397–397, 1994.

    - <span id="page-17-24"></span>[50] D. S. The Engineering Staff of Analog Devices,
    "Analog-digital conversion handbook," 1986.

    - <span id="page-17-25"></span>[51] I. Corp., "Using a pwm as a digital-to-analog
    converter," *AN035701-0915*.

    - [52] F. Zhou and W. Xiong, "Using pwm output as a digital-to-analog converter
    on dsp," in *2010 International Conference on System Science, Engineering Design
    and Manufacturing Informatization*, vol. 2. IEEE, 2010, pp. 278–281.

    - <span id="page-17-26"></span>[53] T. Instruments, "Using pwm output as a digital-to-analog
    converter on a tms320c240 dsp," *APPLICATION REPORT: SPRA490*, November 1998.

    - <span id="page-17-27"></span>[54] e. a. Halper, "Digital-to-analog conversion
    by pulse-count modulation methods," *IEEE Transactions on Instrumentation and
    Measurement*, 1996.

    - <span id="page-17-28"></span>[55] B.-B. Corp., *Integrated Circuits Data Book
    Holland*, 1986.

    - <span id="page-17-29"></span>[56] H. Zander, *Datenwundler: AD/LlA-Wandler;
    Schnittstellen der digitalen Signulverarbeitung*. Frankfurter Fachverlag, 1985.'
  decisions:
    evaluation_prompt: 'Disqualified: no evaluation. Reason: The paper lacks experimental,
      empirical, or quantitative analysis sections or elements.'
- title: "Rethinking the Producer-Consumer Relationship in Modern DRAM-Based\n  Systems"
  abstract: "Generational improvements to commodity DRAM throughout half a century\
    \ have\nlong solidified its prevalence as main memory across the computing industry.\n\
    However, overcoming today's DRAM technology scaling challenges requires new\n\
    solutions driven by both DRAM producers and consumers. In this paper, we\nobserve\
    \ that the separation of concerns between producers and consumers\nspecified by\
    \ industry-wide DRAM standards is becoming a liability to progress\nin addressing\
    \ scaling-related concerns.\n  To understand the problem, we study four key directions\
    \ for overcoming DRAM\nscaling challenges using system-memory cooperation: (i)\
    \ improving memory access\nlatencies; (ii) reducing DRAM refresh overheads; (iii)\
    \ securely defending\nagainst the RowHammer vulnerability; and (iv) addressing\
    \ worsening memory\nerrors. We find that the single most important barrier to\
    \ advancement in all\nfour cases is the consumer's lack of insight into DRAM reliability.\
    \ Based on an\nanalysis of DRAM reliability testing, we recommend revising the\
    \ separation of\nconcerns to incorporate limited information transparency between\
    \ producers and\nconsumers. Finally, we propose adopting this revision in a two-step\
    \ plan,\nstarting with immediate information release through crowdsourcing and\n\
    publication and culminating in widespread modifications to DRAM standards."
  url: http://arxiv.org/abs/2401.16279v1
  keywords: ''
  document: '## 1. Introduction


    Dynamic Random Access Memory (DRAM) [\[1,](#page-13-0) [2\]](#page-13-1) is the
    dominant main memory technology across a broad range of computing systems because
    of its high capacity at low cost [\[3–](#page-13-2)[6\]](#page-13-3). Building
    modern DRAM chips requires specialized design and manufacturing techniques (e.g.,
    custom process nodes [\[7\]](#page-13-4) and bespoke materials [\[8\]](#page-13-5))
    developed across more than half a century of rich history [\[9\]](#page-13-6),
    so the computing industry employs a separation of concerns to explicitly divide
    responsibilities between DRAM producers (e.g., manufacturers) and DRAM consumers
    (e.g., cloud architects, processor and system-on-a-chip designers, memory module
    vendors, etc.). DRAM producers typically develop highly-optimized DRAM chips as
    standalone mass-market components with clearly-specified interfaces, independently
    addressing design concerns related to DRAM technology. DRAM consumers then integrate
    these DRAM chips to develop a broad range of DRAM-based systems. This approach
    enables both parties to specialize their designs and preserve trade secrets while
    working around a common interface.


    In the last decade, however, this long-standing separation of concerns has been
    challenged by worsening DRAM technology scaling difficulties that manifest in
    two problematic ways.


    First, improvements to DRAM access latency and storage capacity are slowing down
    due to DRAM technology scaling challenges [\[10,](#page-13-7) [11\]](#page-13-8).
    To illustrate this problem, we survey 58 publicly-available DRAM chip datasheets[1](#page-0-0)
    in Section [3.1,](#page-3-0) broadly sampling chips from across 19 DRAM manufacturers
    spanning the past five decades. We study the evolution of access latency and storage
    capacity characteristics, showing slowdowns in improvements to both metrics in
    the past ten to twenty years. For example, average annual improvements to the
    data access latency (governed by the timing parameter tRCD) reduced by 69.5% (from
    2.66% to 0.81%) before and after the year 2000, with little to no improvement
    in the past decade. This is consistent with similar surveys conducted by prior
    works [\[12–](#page-13-9)[22\]](#page-13-10).


    Second, conventional approaches to managing scaling challenges used by DRAM producers
    (e.g., in-DRAM mechanisms to mitigate worsening memory errors) no longer suffice
    to hide the underlying problems from DRAM consumers. For example, memory errors
    caused by the RowHammer vulnerability are a serious and growing threat to system-level
    robustness (i.e., security, reliability, and safety) [\[23](#page-13-11)[–27\]](#page-13-12).
    Experimental studies [\[28](#page-13-13)[–32\]](#page-13-14) throughout the past
    decade demonstrate that these errors can be consistently induced across a broad
    range of DRAM chips. In particular, Kim et al. [\[28\]](#page-13-13) show that
    RowHammer errors can be induced much more quickly in modern chips, i.e., with
    only 4.8K memory activations for chips from 2019– 2020, which is 14.4× lower than
    the 69.2K activations required for older DRAM chips from 2010–2013. Section [3](#page-3-1)
    further discuss how DRAM technology design concerns that were previously hidden
    by the separation are now breaking through to become consumer-facing problems
    that impact system-wide performance and robustness.


    We believe that overcoming the DRAM scaling problem requires creative, holistic
    thinking from everyone involved, including both producers and consumers throughout
    industry and academia. Unfortunately, we observe two key limitations with today''s
    separation of concerns that discourage progress toward addressing scaling challenges:


    1. The industry-wide DRAM standards that specify how to implement the separation
    (e.g., JEDEC DDRn [\[33,](#page-13-15) [34\]](#page-13-16), HBMx [\[35,](#page-13-17)
    [36\]](#page-13-18))[2](#page-0-1) do so imperfectly, requiring laborious revision
    to adapt to failures in separation.


    <span id="page-0-0"></span><sup>1</sup>We conduct a best-effort survey of publicly-available
    datasheets. We itemize the specific datasheets we reference in Appendix [B.](#page-24-0)


    <span id="page-0-1"></span><sup>2</sup>DRAM standards specify a DRAM chip''s microarchitecture,
    including its interface, configuration, and performance characteristics as visible
    to DRAM consumers. Section [2](#page-2-0) explains standards in detail.


    2. The existing separation is too strict, which constrains each party''s solution
    space and stifles opportunities to explore new ways to address the scaling challenges.


    These observations stem from a combination of two key sources of evidence.


    First, recent robustness challenges caused by memory errors have already broken
    the separation of concerns established by current DRAM standards. Section [3.2](#page-4-0)
    references two specific instances of this problem, RowHammer [\[25](#page-13-19)[–27,](#page-13-12)
    [29,](#page-13-20) [37\]](#page-13-21) and ondie error correction [\[38](#page-13-22)[–41\]](#page-13-23),
    showing that both cases expose memory errors caused by DRAM technology behavior
    in a way that is undefined by existing DRAM standards. Although recent changes
    to the standards [\[34,](#page-13-16) [42](#page-13-24)[–44\]](#page-13-25) discuss
    these problems and provide limited solutions, undefined chip behavior remains
    a serious and worsening problem for DRAM consumers [\[27,](#page-13-12) [45](#page-13-26)[–48\]](#page-13-27).


    Second, many promising approaches to address DRAM scaling challenges in today''s
    chips [\[16,](#page-13-28) [22,](#page-13-10) [29,](#page-13-20) [49–](#page-13-29)[110\]](#page-15-0)
    rely upon exploiting the benefits of deliberately breaking the separation of concerns.
    These approaches employ system-memory cooperation [\[10,](#page-13-7) [27,](#page-13-12)
    [111–](#page-15-1)[113\]](#page-15-2), demonstrating significant system-level
    benefits from implementing mechanisms outside the DRAM chip to supplement on-chip
    solutions built by DRAM producers. Section [4](#page-5-0) surveys these proposals
    categorized by the particular DRAM scaling challenge they tackle: (1) improving
    memory access latencies (Section [4.1\)](#page-5-1); (2) reducing DRAM refresh
    overheads (Section [4.2\)](#page-6-0); (3) securely defending against the RowHammer
    vulnerability (Section [4.3\)](#page-6-1); and (4) addressing worsening memory
    errors (Section [4.4\)](#page-7-0). Unfortunately, we observe that today''s separation
    of concerns does not support producers and consumers to adopt these methods with
    ease. Instead, doing so requires them to work around DRAM standards, which is
    impractical for the overwhelming majority of consumers due to the risks and costs
    inherent in custom modifications to DRAM chips and unstandardized behavior.


    Based on these observations, we conclude that both the separation of concerns
    and the standards that specify them are outdated for today''s DRAM landscape.
    To rethink the separation of concerns in a modern context, we refer to each of
    the four cases of system-memory cooperation that we study in Section [4.](#page-5-0)
    In each case, we review how prior proposals break the separation of concerns so
    that we can better understand its limitations today.


    We find that the single most important barrier to advancement in all four cases
    is the consumer''s lack of insight into DRAM reliability. For example, Section
    [4.3](#page-6-1) explains how knowing certain properties of memory errors (e.g.,
    correlation with physical chip locations, memory access patterns, and operating
    parameters) is essential for developing secure defenses against the RowHammer
    vulnerability. The existing separation of concerns effectively abstracts details
    of a DRAM chip''s internal operation away from consumers to the extent that consumers
    do not have the necessary context to properly reason about and evaluate how operating
    the chip in a particular way will impact its reliable operation. This encompasses
    operating points both within and outside of manufacturer recommendations;


    in either case, the consumer lacks the context necessary to accurately determine
    how their design decisions outside of the DRAM chip (e.g., in the memory controller)
    affect the DRAM chip''s reliable operation.


    To gain further insight into this problem, we study the general process of memory
    reliability testing in Section [5.](#page-8-0) Our analysis suggests that the
    entire testing process is grounded on knowing relevant properties of a DRAM chip''s
    microarchitectural design, such as the physical organization of cells within the
    storage array and how they encode data at a circuit level. Using this information,
    a DRAM consumer can build models and test methodologies to explore the full design
    space surrounding commodity DRAM chips.


    Based on our analysis, we advocate revising both the separation of concerns and
    the standards that specify them to incorporate limited information transparency
    between DRAM producers and consumers. In particular, explicitly communicating
    basic DRAM design and test characteristics from DRAM producers to consumers empowers
    the consumer with the context to understand how different system-level design
    choices (e.g., optimizations in the processor and memory controller) will affect
    DRAM chip operation. Section [7.1](#page-11-0) identifies information to communicate
    by drawing on examples from research studies, including (1) basic microarchitectural
    properties (e.g., organization of physical rows, sizes of internal storage arrays)
    and (2) best practice guidelines for reliability testing (e.g., test patterns
    for key error mechanisms).[3](#page-1-0) Section [6](#page-9-0) further explains
    how access to this information provides a practical degree of insight into DRAM
    operation for DRAM consumers to work with, without compromising DRAM producers''
    trade secrets or the cost advantages of commodity DRAM.


    We advocate for industry to incorporate information transparency into DRAM standards
    through a two-step approach involving all DRAM stakeholders, including producers
    and consumers.


    Step 1: Early Adoption. Initially, we recommend conceptually revising the separation
    of concerns without yet revising DRAM standards. This step targets DRAM chips
    already in the field, asking both DRAM producers and consumers to voluntarily
    release information they already have at hand. We propose each party to take a
    different approach as follows:


    - Consumers: Contribute to a crowdsourced database of information obtained through
    third-party testing of commodity DRAM chips on the market (e.g., as conducted
    in numerous studies discussed in Section [4\)](#page-5-0).

    - Producers: Publish information (e.g., using datasheet revisions, whitepapers,
    or online resources) about their products, possibly limited to information that
    they already have on hand from past records (i.e., information that requires minimal
    logistical effort to release).


    Through these two avenues, all DRAM stakeholders will benefit from the release
    of information: consumers will gain ac-


    <span id="page-1-0"></span><sup>3</sup>Consumers with access to appropriate testing
    infrastructure can reverseengineer much of this information [\[16,](#page-13-28)
    [22,](#page-13-10) [32,](#page-13-14) [73,](#page-14-0) [74,](#page-14-1) [78,](#page-14-2)
    [114–](#page-15-3)[118\]](#page-15-4). However, existing techniques may not reveal
    all possible necessary information (e.g., due to inaccessible components such
    as remapped rows).


    cess to a wider solution space for addressing DRAM scaling challenges, and producers
    will gain access to insights and new solution directions from consumers'' efforts.
    In particular, these benefits require neither changes to existing DRAM hardware
    or standards (though standardizing the information release could streamline the
    process) nor forced disclosure of sensitive information by either party.


    Step 2: Long-Term Revision. In the long run, we propose revising DRAM standards
    to include industry-standard information, tools, and specifications for a wider
    range of DRAM operating points. In addition to the information requested by Step
    1, we identify two recommendations that our studies show would be beneficial for
    consumers: (1) reliability guarantees for how a chip is expected to behave under
    certain operating conditions (e.g., predictable behavior of faults [\[119\]](#page-15-5));
    (2) disclosure of industry-validated DRAM reliability models and testing strategies
    suitable for commodity DRAM chips (e.g., similar to how JEDEC JEP122 [\[120\]](#page-15-6),
    JESD218 [\[121\]](#page-15-7), and JESD219 [\[122\]](#page-15-8) address Flash-memory-specific
    error mechanisms [\[123–](#page-15-9)[125\]](#page-15-10) such as floating-gate
    data retention [\[126–](#page-15-11)[129\]](#page-15-12) and models for physical
    phenomena such as threshold voltage distributions [\[130–](#page-15-13)[135\]](#page-15-14)).
    Revising DRAM standards in this way will align the standards with a more permissive
    separation of concerns going forward, thereby encouraging cooperation between
    producers and consumers in pursuit of building more robust and higher-performance
    future computing systems.


    We make the following contributions:


    - We make a case to rethink the long-standing separation of concerns between DRAM
    producers and consumers to enable and encourage new solutions for addressing worsening
    DRAM technology scaling challenges.

    - We motivate and support our case by thoroughly reviewing prior work, including
    (1) four case studies that survey system-memory cooperative techniques to address
    DRAM scaling challenges and (2) a historical survey of DRAM chip capacity, latency,
    and energy characteristics based on datasheets. We open-source [\[136\]](#page-15-15)
    our dataset.

    - We provide a new perspective on memory reliability testing from the viewpoint
    of DRAM consumers, identifying access to a DRAM chip''s microarchitectural details
    as both a key challenge and enabler for system-memory cooperative solutions to
    DRAM scaling challenges.

    - We propose a practical plan to encourage new solutions to modern DRAM scaling
    challenges by revising both the separation of concerns and how it is specified
    by DRAM standards today.


    ## <span id="page-2-0"></span>2. DRAM Standards as a Separation of Concerns


    Industry-wide DRAM standards split the responsibilities of building and integrating
    commodity DRAM chips to DRAM producers and consumers, respectively. This section
    reviews how standards achieve this and its implications for DRAM producers and
    consumers.


    ## 2.1. DRAM Standards


    DRAM standards carefully balance the needs of both producers and consumers through
    industry-wide consensus. Today, the


    JEDEC consortium [\[137\]](#page-15-16) maintains a limited set of standards describing
    commodity DRAM chips with different target applications, e.g., general-purpose
    DDRn [\[33,](#page-13-15) [34,](#page-13-16) [138\]](#page-15-17), bandwidthoptimized
    HBMn [\[35,](#page-13-17) [36\]](#page-13-18), mobile-oriented LPDDRn [\[44,](#page-13-25)
    [139\]](#page-15-18), graphics-oriented GDDRn [\[140,](#page-15-19) [141\]](#page-15-20).


    DRAM standards specify all aspects of a DRAM chip''s design that pertain to the
    interface between producers and consumers, including the chip''s access interfaces,
    configuration mechanisms, and performance characteristics. By doing so, standards
    effectively abstract DRAM technology-level concerns (e.g., cellto-cell variation,
    reliability challenges) into predictable usage patterns, thereby reducing a complex
    storage technology into a modular computing component.


    #### 2.2. Advantages of Standardized DRAM


    Standardized DRAM enables the widespread use of highlyoptimized DRAM chips. This
    is because standards scope DRAM technology to a few fixed components (i.e., standardized
    chips) with clearly-defined interfaces that both producers and consumers can optimize
    towards without concerning themselves with the other party''s design challenges.
    This gives each party the freedom to explore different solutions to DRAM design
    challenges based on their design goals. For example, empirical studies of DRAM
    chips [\[23,](#page-13-11) [40,](#page-13-30) [48,](#page-13-27) [118,](#page-15-4)
    [142\]](#page-15-21) show that different DRAM producers mitigate memory errors
    using a broad range of different on-chip error mitigation techniques. In general,
    producers are free to innovate in any way that does not violate the specifications
    established by standards, and consumers can build upon those specifications in
    any way to meet their systems'' needs.


    2.2.1. DRAM Producers'' Trade Secrets. DRAM producers closely guard their innovations
    because trade secrets are a key component of business competitiveness and success
    [\[86,](#page-14-3) [143–](#page-15-22)[145\]](#page-15-23). Producers who build
    standards-compliant chips are competing in a commodity market, so they seek profitability
    though economies of scale [\[146](#page-15-24)[–149\]](#page-15-25). Each producer
    develops and uses home-grown, highly-optimized design, manufacturing, and testing
    processes that amortize costs in high volume production, thereby maximizing per-chip
    profit margins.


    As a result, DRAM producers publish only what information DRAM standards require,
    such as access timing specifications and circuit characteristics needed for chip
    integration. Additional information not specified by standards (e.g., internal
    circuit designs, chip error rates) is kept in-house. Although such details can
    often be inferred through reverse-engineering studies [\[16,](#page-13-28) [23,](#page-13-11)
    [24,](#page-13-31) [28,](#page-13-13) [32,](#page-13-14) [40,](#page-13-30) [74,](#page-14-1)
    [78,](#page-14-2) [92,](#page-14-4) [114,](#page-15-3) [117,](#page-15-26) [150](#page-15-27)[–155\]](#page-15-28)
    and chip teardowns [\[156,](#page-15-29) [157\]](#page-15-30) (discussed further
    in Section [4\)](#page-5-0), producers have no obligation to communicate this
    information to consumers.


    #### <span id="page-2-1"></span>2.3. Using Non-Commodity DRAM Chips


    Consumers can use non-commodity DRAM chips by either (i) privately working with
    DRAM producers to build customized chips or (ii) buying specialized or otherwise
    domain-optimized DRAM chips (e.g., high reliability [\[158,](#page-15-31) [159\]](#page-15-32),
    low latency [\[160\]](#page-15-33)). These chips are generally still compliant
    with JEDEC standards, though they may provide additional unstandardized features.


    Developing and using non-commodity DRAM can benefit industry-wide standards. For
    example, the HBM standard (JESD235 [\[35\]](#page-13-17)) largely grew from private
    industry collaborations between AMD and SK Hynix [\[161\]](#page-15-34). Similarly,
    features pioneered in non-commodity DRAM can be integrated into newer standards.


    Unfortunately, innovating through non-commodity DRAM is a slow and costly process
    because it forgoes the advantages of mainstream DRAM chips. Non-commodity chips
    are typically feasible only for consumers in specific industries (e.g., imaging,
    networking [\[162\]](#page-15-35)) or with significant stake in the global DRAM
    market (e.g., large-scale cloud vendors). Our work takes inspiration from non-commodity
    DRAM to enable all consumers to pursue such innovations, ultimately benefitting
    the DRAM industry as a whole.


    #### <span id="page-3-5"></span>2.4. Creating or Modifying DRAM Standards


    Changes to DRAM standards require participation from all stakeholders throughout
    the DRAM industry, including JEDEC personnel, DRAM producers, and DRAM consumers.
    Therefore, making changes is a slow process that can involve nontechnical elements,
    such as political motivations and business goals [\[163\]](#page-15-36). Major
    changes typically follow one of three different paths. First, a standards committee
    comprising experts from all stakeholders may directly draft a new standard. Second,
    new standards may grow out of the development and use of non-commodity DRAM as
    discussed in Section [2.3.](#page-2-1) Third, existing standards may be updated
    or supplemented by JEDEC committees for special issues, such as LPDDR4X [\[164\]](#page-16-0),
    3Dstacked DRAM [\[165\]](#page-16-1), and RowHammer [\[42,](#page-13-24) [43\]](#page-13-32).


    ## <span id="page-3-1"></span>3. Challenges of DRAM Technology Scaling


    DRAM''s primary competitive advantage is its low cost-percapacity [\[3](#page-13-2)[–6\]](#page-13-3),
    which DRAM producers maintain by continually improving chip storage densities.
    Doing so across successive product generations requires carefully balancing shrinking
    physical feature sizes, optimizing circuit areas, and mitigating worsening memory
    errors [\[38,](#page-13-22) [86,](#page-14-3) [166–](#page-16-2)[170\]](#page-16-3).


    Unfortunately, today''s DRAM faces two key challenges to continued technology
    scaling: (i) the slowdown of generational improvements to storage capacity, access
    latency, and power consumption [\[13,](#page-13-33) [17,](#page-13-34) [171\]](#page-16-4);
    and (ii) the breakdown of conventional approaches to mitigate memory errors. This
    section reviews both challenges in detail.


    #### <span id="page-3-0"></span>3.1. Slowdown of Generational Improvements


    Advancements in DRAM chip storage density have been central to increasing demands
    for memory capacity since the inception of DRAM technology [\[9,](#page-13-6)
    [172\]](#page-16-5). Today''s emerging data-intensive applications and systems
    in domains such as AI, cloud, and HPC continue to demand greater memory capacity
    at an unprecedented scale [\[173–](#page-16-6)[177\]](#page-16-7). Unfortunately,
    technology shrinkage throughout the past two decades has yielded diminishing benefits
    for chip storage capacity, access latency, and refresh overheads because of the
    growing costs and overheads of maintaining reliable chip operation at smaller
    technology node sizes [\[10,](#page-13-7) [11\]](#page-13-8).


    To better understand this slowdown of generational improvements, we survey manufacturer-reported
    DRAM chip capacities, access timings, and current consumption characteristics
    given by 58 publicly-available DRAM chip datasheets from across 19 different DRAM
    manufacturers with datasheet publication dates between 1970 and 2021.[4](#page-3-2)
    The remainder of this section individually analyzes chip capacity, access timings,
    and refresh overheads in the context of our survey.


    3.1.1. Chip Storage Capacity. Figure [1](#page-3-3) shows the time evolution of
    per-chip storage capacity and four key DRAM operating timings (all shown in log
    scale). We observe that storage capacity has grown exponentially over time alongside
    improvements to all four timing parameters (timings are discussed in Section [3.1.2\)](#page-3-4).
    However, storage capacity growth has slowed down markedly since 2010, dropping
    from an exponential growth factor of 0.341 per year for 1970-2000 to 0.278 for
    2000-2020. This is consistent with recent challenges in scaling beyond 16 Gb chip
    densities, and this slowdown is expected to continue going forward [\[11,](#page-13-8)
    [178\]](#page-16-8).


    <span id="page-3-3"></span>![](_page_3_Figure_11.jpeg)


    Figure 1: Semi-log plot showing the evolution of key DRAM access timings (left)
    and per-chip storage capacity (right)<sup>i</sup> across each 5-year period of
    time.


    | i<br>JEDEC-standardized parameters[34] found in DRAM chip datasheets: |            |

    |-----------------------------------------------------------------------|------------|

    | Parameter                                                             | Definition
    |


    | tRCD        | minimum row activation to column operation delay |

    |-------------|--------------------------------------------------|

    | CAS Latency | read operation to data access latency            |

    | tRAS        | minimum row activation to precharge delay        |

    | tRC         | minimum delay between accesses to different rows |

    |             |                                                  |


    <span id="page-3-4"></span>3.1.2. Chip Access Latency. DRAM access latency has
    not significantly improved relative to storage capacity over the last two decades
    [\[12](#page-13-9)[–14,](#page-13-35) [16–](#page-13-28)[21\]](#page-13-36). This
    makes DRAM an increasingly significant system performance bottleneck today, especially
    for workloads with large footprints that are sensitive to DRAM access latency
    [\[12,](#page-13-9) [77,](#page-14-5) [111,](#page-15-1) [112,](#page-15-37) [179](#page-16-9)[–202\]](#page-16-10).
    Therefore, there is significant opportunity for improving overall system performance
    by reducing the memory access latency [\[16,](#page-13-28) [22,](#page-13-10)
    [52,](#page-13-37) [59,](#page-14-6) [74,](#page-14-1) [77,](#page-14-5) [78,](#page-14-2)
    [82,](#page-14-7) [106,](#page-15-38) [109,](#page-15-39) [110,](#page-15-0) [203,](#page-16-11)
    [204\]](#page-16-12). Although conventional latencyhiding techniques (e.g., caching,
    prefetching, multithreading) can potentially help mitigate many of the performance
    concerns, these techniques (1) fundamentally do not change the


    <span id="page-3-2"></span><sup>4</sup>This data encompasses DRAM chips from both
    asynchronous (e.g., page mode, extended data out) and synchronous (e.g., SDRAM,
    DDRn) DRAM chips. Appendix [A](#page-21-0) describes our data collection methodology
    in further detail, and Appendix [B](#page-24-0) provides an overview of our dataset,
    which is publicly available on GitHub [\[136\]](#page-15-15).


    latency of each memory access and (2) fail to work in many cases (e.g., irregular
    memory access patterns, random accesses, huge memory footprints).


    Figure [1](#page-3-3) shows that none of the four timings we study have improved
    significantly in the last two decades. For example, the median tRCD/CAS Latency/tRAS/tRC
    reduced by 2.66/3.11/2.89/2.89% per year on average between 1970 and 2000, but
    only 0.81/0.97/1.33/1.53% between 2000 and 2015.[5](#page-4-1) This is consistent
    with similar studies in prior work [\[12–](#page-13-9)[21\]](#page-13-36).


    <span id="page-4-5"></span>3.1.3. Worsening Refresh Overheads. The circuits that
    DRAM uses to store data are inherently susceptible to a wide range of different
    leakage mechanisms (e.g., capacitor charge leakage), which ultimately cause data
    loss if ignored. To prevent this, DRAM standards mandate periodic refresh operations
    that intermittently restore data values throughout the entire DRAM chip. Unfortunately,
    DRAM refresh incurs significant system performance and power overheads [\[49,](#page-13-29)
    [69,](#page-14-8) [80,](#page-14-9) [82,](#page-14-7) [86,](#page-14-3) [89,](#page-14-10)
    [104,](#page-14-11) [205](#page-16-13)[–207\]](#page-16-14), sacrificing almost
    half of the total memory throughput and consuming almost half of the total DRAM
    power for projected 64 Gb chips [\[80\]](#page-14-9).


    Figure [2](#page-4-2) illustrates the performance overheads of DRAM refresh across
    the different DRAM chips in our datasheet survey. The data shows the refresh penalty,
    [6](#page-4-3) which is defined as the ratio of two key timing parameters used
    to govern refresh operations: tRF C , the duration of each refresh command, and
    tREF I , the time between consecutive refresh commands. The refresh penalty represents
    the average time that a DRAM rank (or bank) is unavailable for access due to refresh
    operations [\[208,](#page-16-15) [210–](#page-16-16)[213\]](#page-16-17).


    <span id="page-4-2"></span>![](_page_4_Figure_4.jpeg)


    Figure 2: Refresh penalty (computed as the ratio between tRF C and tREF I ) for
    DRAM chips of different storage capacities.


    We observe that the refresh penalty worsens from a median of 1.04% for 1 Kib chips
    to 2.05% for 16 Kib chips, then improves to 0.43% for 128 Mib chips, and finally
    worsens to a median of 4.48% (worst-case of 7.56% for DDR5 chips) for 16 Gib chips.This
    non-monotonic trend is due to the relative rates of improvement in DRAM access
    latency and storage capacity: access (and therefore, refresh) latencies stagnated
    around the introduction of 128 Mib chips while capacity improvements did not.
    This data is consistent with both (i) our more detailed


    analysis of DRAM refresh timings in Appendix [A.4](#page-23-0) and (ii) trends
    observed in prior work [\[21,](#page-13-36) [80,](#page-14-9) [94,](#page-14-12)
    [206,](#page-16-19) [214,](#page-16-20) [215\]](#page-16-21), which expect that
    future, higher-capacity DRAM chips will spend an even larger proportion of time
    refreshing unless the DRAM refresh algorithm and its implementation are improved.


    #### <span id="page-4-0"></span>3.2. Breakdown of the Separation of Concerns


    DRAM is susceptible to a variety of error mechanisms that worsen with technology
    scaling and can impact system-wide reliability if not carefully managed. Today''s
    separation of concerns largely puts the burden to address these errors on DRAM
    producers, i.e., to hide them from consumers and provide the illusion of an error-free
    memory chip. Unfortunately, two classes of scaling-related errors are breaking
    through the separation to impact consumers regardless: random, single-bit errors
    and RowHammer errors. This section discusses these errors and how they break the
    separation of concerns.


    <span id="page-4-7"></span>3.2.1. Worsening Reliability. DRAM suffers from a range
    of circuit-level error mechanisms that can impact consumers (e.g., data loss,
    system failure) if mismanaged [\[84,](#page-14-13) [216–](#page-16-22)[221\]](#page-17-0).
    To mitigate these errors on-chip, DRAM producers typically use a combination of
    conservative operating timings (e.g., with added safety margins) and simple error-correcting
    hardware within the DRAM chip (e.g., manufacturing spare rows and columns to replace
    faulty ones [\[33,](#page-13-15) [34,](#page-13-16) [216,](#page-16-22) [222,](#page-17-1)
    [223\]](#page-17-2)).[7](#page-4-4) These low-cost techniques enable DRAM producers
    to provide the illusion of a fully reliable DRAM chip, thereby preserving the
    separation of concerns between producers and consumers.


    In contrast, modern DRAM chips exhibit much higher error rates because technology
    scaling exacerbates the underlying circuit-level error mechanisms that cause errors
    [\[86,](#page-14-3) [142,](#page-15-21) [143,](#page-15-22) [166,](#page-16-2)
    [167,](#page-16-23) [224,](#page-17-3) [225\]](#page-17-4). To combat these errors,
    DRAM producers use stronger error-mitigation mechanisms in modern DRAM chips (e.g.,
    on-die ECC [\[34,](#page-13-16) [38,](#page-13-22) [40,](#page-13-30) [41,](#page-13-23)
    [87,](#page-14-14) [115,](#page-15-40) [119,](#page-15-5) [167,](#page-16-23)
    [224,](#page-17-3) [226](#page-17-5)[–230\]](#page-17-6), post-package repair
    [\[33,](#page-13-15) [34,](#page-13-16) [216,](#page-16-22) [222,](#page-17-1)
    [223\]](#page-17-2), target row refresh [\[23,](#page-13-11) [32,](#page-13-14)
    [45,](#page-13-26) [46\]](#page-13-38), refresh management [\[34,](#page-13-16)
    [47\]](#page-13-39)), which are more expensive and incur higher performance and
    energy overheads.


    Unfortunately, worsening memory reliability remains a serious problem for DRAM
    consumers, especially high-volume consumers for whom even modest chip error rates
    are significant at scale [\[142,](#page-15-21) [221\]](#page-17-0). Although stronger
    in-DRAM error mitigations are effective against growing error rates [\[142,](#page-15-21)
    [224\]](#page-17-3), they introduce new overheads and challenges for consumers.
    For example, neither on-die ECC nor target row refresh correct all errors, and
    the remaining errors (e.g., uncorrectable errors) are difficult for consumers
    to predict and mitigate because their manifestation depends on the particular
    on-die ECC and/or TRR mechanism used by a given chip [\[23,](#page-13-11) [32,](#page-13-14)
    [38,](#page-13-22) [40,](#page-13-30) [41,](#page-13-23) [46,](#page-13-38) [115,](#page-15-40)
    [152,](#page-15-41) [229,](#page-17-7) [231\]](#page-17-8). As a result, DRAM
    consumers face errors that are growing in both magnitude and complexity, making
    reliability a key design concern for continued DRAM scaling.


    <span id="page-4-6"></span>3.2.2. The RowHammer Vulnerability. RowHammer [\[25,](#page-13-19)
    [26,](#page-13-40) [29,](#page-13-20) [37,](#page-13-21) [232–](#page-17-9)[234\]](#page-17-10)
    is a scaling-related read-disturb phenomenon


    <span id="page-4-1"></span><sup>5</sup>We report 2015 instead of 2020 because
    2020 shows a regression in CAS latency due to first-generation DDR5 chips, which
    we believe is not representative because of its immature technology.


    <span id="page-4-3"></span><sup>6</sup>Also referred to as refresh overhead [\[208\]](#page-16-15)
    and refresh duty cycle [\[209\]](#page-16-18).


    <span id="page-4-4"></span><sup>7</sup>Enterprise-class computing providers (e.g.,
    cloud, HPC) may use supplementary error-mitigation mechanisms discussed further
    in Section [4.4.1.](#page-7-1)


    affecting modern DRAM chips in which memory accesses to a given physical memory
    location can induce bit-flips at other locations. Significant work [\[23](#page-13-11)[–26,](#page-13-40)
    [102,](#page-14-15) [235–](#page-17-11)[258\]](#page-17-12) shows that RowHammer
    is a security threat that can be exploited to exfiltrate sensitive data, escalate
    user privileges, or otherwise compromise a system. Because RowHammer is fundamental
    to DRAM circuit designs, it is a problem for all DRAM-based systems and becomes
    significantly worse with continued process technology scaling [\[25,](#page-13-19)
    [26,](#page-13-40) [28,](#page-13-13) [29\]](#page-13-20).


    Modern DRAM partially addresses RowHammer through best practices laid out in DRAM
    specifications [\[34,](#page-13-16) [42,](#page-13-24) [43\]](#page-13-32) and
    RowHammer mitigation mechanisms built into DRAM chips by producers, including
    target row refresh (TRR) [\[23,](#page-13-11) [28,](#page-13-13) [32,](#page-13-14)
    [258–](#page-17-12)[261\]](#page-17-13) and refresh management (RFM) [\[34,](#page-13-16)
    [47\]](#page-13-39). Unfortunately, neither approach suffices to provide fully
    secure solutions against RowHammer [\[23,](#page-13-11) [32,](#page-13-14) [48,](#page-13-27)
    [118,](#page-15-4) [258,](#page-17-12) [262\]](#page-17-14). Therefore, RowHammer
    remains a serious consumer-visible problem that challenges the illusion of a robust
    DRAM chip established by the separation of concerns.


    ## <span id="page-5-0"></span>4. Challenges in Overcoming Scaling Problems


    Developing new solutions to address the DRAM scaling challenges discussed in Section
    [3](#page-3-1) requires creative thinking that today''s separation of concerns
    constrains. This section explores new techniques based on system-memory cooperation
    that holistically address DRAM scaling challenges with the help of hardware and
    software mechanisms at the system level. We review techniques that target each
    of the four key DRAM scaling challenges discussed in Section [3:](#page-3-1) access
    latency, refresh overheads, RowHammer, and worsening reliability. In each case
    study, we survey relevant prior work to understand how and why today''s separation
    of concerns discourages systemmemory cooperative solutions.


    #### <span id="page-5-1"></span>4.1. Study 1: Long DRAM Access Latency


    As Section [3.1.2](#page-3-4) discusses, slow generational improvements to DRAM
    access latency make DRAM a worsening bottleneck for latency-sensitive workloads
    today [\[12,](#page-13-9) [77,](#page-14-5) [111,](#page-15-1) [112,](#page-15-37)
    [179–](#page-16-9)[202\]](#page-16-10). System-memory cooperation promises to
    overcome this problem by directly reducing the DRAM access latency beyond what
    commodity DRAM chips provide.


    4.1.1. Solutions to Reduce the DRAM Access Latency. Prior works have taken two
    major directions to improve the DRAM access latency. First, many works [\[16,](#page-13-28)
    [22,](#page-13-10) [52,](#page-13-37) [59,](#page-14-6) [74,](#page-14-1) [77,](#page-14-5)
    [78,](#page-14-2) [82,](#page-14-7) [106,](#page-15-38) [109,](#page-15-39) [110\]](#page-15-0)
    show that the average DRAM access latency can be shortened by reducing DRAM access
    timings for particular memory locations that can tolerate faster accesses. This
    can be done safely because, although DRAM standards call for constant access timings
    across all memory locations, the minimum viable access timings that the hardware
    can support actually differ across memory locations due to factors such as heterogeneity
    in the circuit design [\[17,](#page-13-34) [78\]](#page-14-2) and manufacturing
    process variation in circuit components [\[16,](#page-13-28) [22,](#page-13-10)
    [52,](#page-13-37) [74,](#page-14-1) [263\]](#page-17-15).


    Exploiting these variations in access timings to reduce the average memory access
    latency provides significant system performance improvement. For example, Chang
    et al. [\[16\]](#page-13-28) experimentally show that exploiting access latency
    variations provides an average 8-core system performance improvement of 13.3%/17.6%/19.5%
    for real DRAM chips from three major DRAM manufacturers. Similarly, Kim et al.
    [\[74\]](#page-14-1) show that exploiting access latency variations induced by
    DRAM sense amplifiers provides an average (maximum) 4-core system performance
    improvement of 4.97% (8.79%) versus using default DRAM access timings based on
    data obtained from 282 commodity LPDDR4 DRAM chips.


    Second, other works [\[55,](#page-14-16) [58,](#page-14-17) [73,](#page-14-0)
    [75,](#page-14-18) [90,](#page-14-19) [91,](#page-14-20) [96–](#page-14-21)[100,](#page-14-22)
    [264–](#page-17-16)[270\]](#page-17-17) show that commodity DRAM can perform massively-parallel
    computations (e.g., at the granularity of an 8 KiB DRAM row) by exploiting the
    underlying analog behavior of DRAM operations (e.g., charge sharing between cells).
    These works show that such computations can significantly improve overall system
    performance and energy-efficiency by both (1) reducing the amount of data transferred
    between the processor and DRAM and (2) exploiting the relatively high throughput
    of rowgranularity operations. For example, Gao et al. [\[55\]](#page-14-16) show
    that in-DRAM 8-bit vector addition is 9.3× more energy-efficient than the same
    computation in the processor, primarily due to avoiding the need for off-chip
    data transfers. Similarly, Olgun et al. [\[90\]](#page-14-19) experimentally demonstrate
    that in-DRAM copy and initialization techniques can improve the performance of
    system-level copy and initialization operations by 12.6× and 14.6×, respectively.


    <span id="page-5-2"></span>4.1.2. Application to Today''s Commodity DRAM Chips.
    Unfortunately, both reducing DRAM access timings and exploiting DRAM''s massively-parallel
    analog behavior are discouraged by today''s separation of concerns. In both cases,
    new DRAM access timings must be determined that ensure new or modified DRAM operations
    can be performed predictably and reliably under all operating conditions.


    To identify new access timings, the majority of prior works [\[13,](#page-13-33)
    [16,](#page-13-28) [17,](#page-13-34) [22,](#page-13-10) [52,](#page-13-37) [55,](#page-14-16)
    [73,](#page-14-0) [75,](#page-14-18) [91,](#page-14-20) [171,](#page-16-4) [263,](#page-17-15)
    [271](#page-17-18)[–273\]](#page-18-0) perform extensive experimental characterization
    studies across many (e.g., hundreds or thousands of) DRAM chips. These studies
    account for three primary sources of variation that affect the access timings
    of a given memory location. First, process variation introduces random variations
    between DRAM chip components (e.g., cells, rows, columns). Second, a manufacturer''s
    particular circuit design introduces structural variation (called design-induced
    variation [\[78\]](#page-14-2)) that deterministically affects access timings
    based on a component''s location in the overall DRAM design (e.g., cells along
    the same bitline [\[74\]](#page-14-1), cells at the borders of internal storage
    arrays [\[78\]](#page-14-2)). Third, the charge level of a DRAM cell varies over
    time due to leakage and the effects of DRAM accesses [\[59,](#page-14-6) [198\]](#page-16-24).
    Experimentally determining the new predictable and reliable access timings requires
    properly accounting for all three sources of variation under all operating conditions.


    For a typical DRAM consumer, determining new access timings using a custom DRAM
    testing methodology is impractical without assistance from DRAM producers. Choosing
    runtime (e.g., data and access patterns) and environmental (e.g., temperature,
    voltage) testing conditions in a meaningful way requires understanding the error
    mechanisms involved in timingrelated errors [\[68\]](#page-14-23), including (but
    not limited to) microarchitectural design details, such as internal substructure
    dimensions (e.g., subarray sizing) [\[74,](#page-14-1) [78\]](#page-14-2), the
    correspondence between logical DRAM bus addresses and physical cell locations
    [\[16,](#page-13-28) [22,](#page-13-10) [67\]](#page-14-24), and the order of
    rows refreshed by each auto-refresh operation [\[198\]](#page-16-24). Therefore,
    consumers who lack trustworthy access to this information are discouraged from
    exploring improvements to the commodity DRAM access latency.


    ### <span id="page-6-0"></span>4.2. Study 2: DRAM Refresh Overheads


    DRAM refresh overheads are a key design concern in modern systems as discussed
    in Section [3.1.3.](#page-4-5) System-memory cooperation can overcome this problem
    by eliminating or otherwise mitigating unnecessary refresh operations, thereby
    improving overall system performance and energy efficiency.


    4.2.1. Solutions to Reduce DRAM Refresh Overheads. Prior works [\[56,](#page-14-25)
    [61,](#page-14-26) [64–](#page-14-27)[70,](#page-14-28) [79,](#page-14-29) [80,](#page-14-9)
    [82,](#page-14-7) [86,](#page-14-3) [89,](#page-14-10) [92,](#page-14-4) [94,](#page-14-12)
    [103,](#page-14-30) [104\]](#page-14-11) address refresh overheads by exploiting
    the fact that most refresh operations are unnecessary.[8](#page-6-2) The standardized
    DRAM refresh algorithm refreshes all cells frequently (i.e., at the worst-case
    rate) to simplify DRAM refresh and guarantee correctness. However, each cell''s
    data retention characteristics vary significantly due to a combination of data-dependence
    [\[65,](#page-14-31) [67,](#page-14-24) [68,](#page-14-23) [92,](#page-14-4) [150\]](#page-15-27)
    and process variation [\[80,](#page-14-9) [86,](#page-14-3) [104,](#page-14-11)
    [143,](#page-15-22) [150,](#page-15-27) [276,](#page-18-1) [277\]](#page-18-2).
    As a result, eliminating unnecessary refresh operations provides significant power
    reduction and performance improvement. For example, Liu et al. [\[80\]](#page-14-9)
    demonstrate an average energyper-access and system performance improvement of
    8.3% and 4.1%, respectively, for 4 Gib chips (49.7% and 107.9% for 64 Gib chips)
    when relaxing the refresh rate at the row granularity. Therefore, reducing refresh
    overheads potentially benefits all DRAM-based systems.


    <span id="page-6-4"></span>4.2.2. Application to Today''s Commodity DRAM Chips.
    Reducing unnecessary refresh operations in commodity DRAM chips comprises two
    key steps. First, the memory controller must reduce the frequency of periodic
    refresh operations. This is achievable (though not necessarily supported to arbitrary
    values) using commodity DRAM chips because the memory controller manages DRAM
    refresh timings. For example, the memory controller might relax the rate at which
    it issues refresh operations to half of the DDRn standard of 3.9 or 7.8 µs, which
    is supported by standards at extended temperature ranges [\[33,](#page-13-15)
    [34,](#page-13-16) [44,](#page-13-25) [138,](#page-15-17) [139\]](#page-15-18),
    or even to over an order of magnitude less often [\[64,](#page-14-27) [80,](#page-14-9)
    [86,](#page-14-3) [103\]](#page-14-30).


    Second, the system must mitigate any errors that may occur within the small number
    of DRAM cells that require frequent refreshing. Doing so requires using either
    (i) additional operations to mitigate data loss (e.g., issuing extra row activations
    to


    those cells'' rows [\[80\]](#page-14-9)) or (ii) supplementary error-mitigation
    mechanisms within processor (e.g., ECC [\[94\]](#page-14-12) and/or bit-repair
    techniques [\[79,](#page-14-29) [86,](#page-14-3) [103\]](#page-14-30)). Although
    both approaches can introduce new performance and energy overheads, the benefits
    of reducing unnecessary refresh operations outweigh the overheads introduced [\[56,](#page-14-25)
    [79,](#page-14-29) [80,](#page-14-9) [86,](#page-14-3) [89,](#page-14-10) [92,](#page-14-4)
    [94,](#page-14-12) [103,](#page-14-30) [104,](#page-14-11) [278\]](#page-18-5).
    For example, Liu et al. [\[80\]](#page-14-9) project that DRAM refresh overheads
    cause a 187.6% increase in the energy-per access and a 63.7% system performance
    degradation for 64 Gib chips. By reducing the overall number of DRAM refresh operations,
    the authors'' proposal (RAIDR) mitigates these overheads by 49.7% and 107.9%,
    respectively.


    Unfortunately, this second step is difficult for the average DRAM consumer because
    it requires a trustworthy method for discriminating DRAM cells'' data retention
    characteristics. These characteristics vary with both the DRAM chip circuit design
    (e.g., random cell-to-cell variations, locations of true and anti-cells [\[115,](#page-15-40)
    [117,](#page-15-26) [150\]](#page-15-27)) and usage characteristics (e.g., operating
    conditions such as voltage and temperature, workload access patterns), so identifying
    which cells to refresh more often requires access to both internal knowledge of
    a given DRAM chip and how the chip''s end use will impact data retention. Prior
    works propose reliability testing [\[65,](#page-14-31) [67,](#page-14-24) [79,](#page-14-29)
    [80,](#page-14-9) [82,](#page-14-7) [92\]](#page-14-4) and monitoring [\[94,](#page-14-12)
    [229,](#page-17-7) [279,](#page-18-6) [280\]](#page-18-7) techniques to work around
    the lack of this knowledge, which the separation of concerns hides from DRAM consumers.
    Ultimately, this means that system-memory cooperation to improve the standardized
    DRAM refresh algorithm are discouraged today.


    #### <span id="page-6-1"></span>4.3. Study 3: RowHammer Mitigation


    Section [3.2.2](#page-4-6) discusses the severity of the RowHammer vulnerability,
    motivating the need for secure defenses beyond those currently implemented. System-memory
    cooperative mechanisms are capable of supplementing these defenses from outside
    of the DRAM chip to provide strong security without requiring changes to DRAM
    chip hardware. Such a solution is attractive for a system designer with interest
    in building a secure system because the designer can guarantee correctness using
    their own methods rather than taking the word of external parties [\[45,](#page-13-26)
    [46\]](#page-13-38).


    <span id="page-6-3"></span>4.3.1. Solutions to Securely Mitigate RowHammer. We
    classify secure RowHammer mitigations into four groups based on categorization
    by recent work [\[107\]](#page-15-42).


    - 1. Access-agnostic mitigation hardens a DRAM chip against RowHammer independently
    of the memory access pattern. This includes increasing the overall DRAM refresh
    rate [\[29,](#page-13-20) [281,](#page-18-8) [282\]](#page-18-9) and memory-wide
    error correction and/or integrity-checking mechanisms such as strong ECC [\[29,](#page-13-20)
    [45,](#page-13-26) [260\]](#page-17-19). These mechanisms are algorithmically
    simple but can introduce significant system hardware, performance, and/or energy-efficiency
    overheads (e.g., a large number of additional refresh operations [\[28,](#page-13-13)
    [29,](#page-13-20) [207\]](#page-16-14)).

    - 2. Proactive mitigations [\[29,](#page-13-20) [57,](#page-14-32) [85,](#page-14-33)
    [107\]](#page-15-42) adjust the DRAM access pattern to prevent the possibility
    of RowHammer errors.

    - 3. Physically isolating mitigations [\[50,](#page-13-41) [76,](#page-14-34)
    [95,](#page-14-35) [102,](#page-14-15) [245,](#page-17-20) [283–](#page-18-10)[287\]](#page-18-11)
    physically separate data such that accesses to one portion


    <span id="page-6-2"></span><sup>8</sup>Latency-hiding techniques (e.g, prefetching,
    memory command scheduling, on-chip caching, etc.) and parallelization of refresh
    and access operations [\[21,](#page-13-36) [211,](#page-16-25) [215,](#page-16-21)
    [274,](#page-18-3) [275\]](#page-18-4) help mitigate performance overheads but
    do not change the total number of refresh operations issued. As a result, such
    techniques do not reduce refresh energy consumption. These techniques are also
    imperfect in many cases where latency-hiding is impractical (e.g., row conflicts
    between refresh and access commands, larger memory footprints than available caching
    resources) [\[208,](#page-16-15) [214,](#page-16-20) [215,](#page-16-21) [274\]](#page-18-3).


    of the data cannot cause RowHammer errors in another.


    4. Reactive mitigations [\[29,](#page-13-20) [34,](#page-13-16) [288–](#page-18-12)[304\]](#page-18-13)
    identify symptoms of an ongoing RowHammer attack (e.g., excessive row activations)
    and issue additional row activation or refresh operations to prevent bit-flips
    from occurring.


    Choosing a secure RowHammer defense for a given system depends on the system''s
    particular threat model and the overheads (e.g., performance, energy, hardware
    area, complexity) it can tolerate. For example, if DRAM is accessible only through
    processor cores (e.g., peripherals are incapable of direct memory access), secure
    defenses may be possible solely through on-chip cache management [\[305\]](#page-18-14).


    <span id="page-7-7"></span>4.3.2. Application to Today''s Commodity DRAM Chips.
    Unfortunately, implementing secure RowHammer defenses is discouraged in today''s
    DRAM chips because the separation of concerns hides the mechanics of how RowHammer
    occurs from DRAM consumers. The defenses discussed throughout Section [4.3.1](#page-6-3)
    all require understanding one or more of a chip''s RowHammer error characteristics,
    which are summarized in Table [1.](#page-7-2) The first is known as HCfirst [\[28,](#page-13-13)
    [118\]](#page-15-4) or RowHammer Threshold [\[29,](#page-13-20) [107,](#page-15-42)
    [306\]](#page-18-15), which describes the worst-case number of RowHammer memory
    accesses required to induce a bit-flip. The second is known as the blast radius
    [\[28,](#page-13-13) [29\]](#page-13-20), which describes how many rows are affected
    by hammering a single row. The third is the DRAM''s internal physical row address
    mapping [\[29,](#page-13-20) [307\]](#page-18-16), which is used to identify the
    locations of victim rows.


    <span id="page-7-2"></span>


    |                      | Required Information |              |             |

    |----------------------|----------------------|--------------|-------------|

    | Strategy             | HCfirst              | Blast Radius | Row Mapping |

    | Access-Agnostic      | ✓                    |              |             |

    | Proactive            | ✓                    | ✓            |             |

    | Physically Isolating | ✓                    | ✓            | ✓           |

    | Reactive             | ✓                    | ✓            | ✓           |


    Table 1: Information needed by each of the four RowHammermitigation strategies.


    All three RowHammer error characteristics vary between DRAM manufacturers, chips,
    and cells based on a combination of random process variation, a chip''s particular
    circuit design (including yield-management techniques such as postmanufacturing
    repair, target row refresh, and error correcting codes), and operating conditions
    such as temperature and voltage [\[28–](#page-13-13)[31,](#page-13-42) [118,](#page-15-4)
    [152,](#page-15-41) [300,](#page-18-17) [308](#page-18-18)[–311\]](#page-18-19).


    Without trustworthy access this information, DRAM consumers are discouraged from
    adopting secure RowHammer defenses. To work around this limitation, proposals
    for secure RowHammer defenses conduct extensive experimental testing to estimate
    RowHammer error characteristics that are needed to design and/or configure their
    proposals. Unfortunately, similar to efforts that improve DRAM access latency
    and refresh timings discussed in Sections [4.2.2](#page-6-4) and [4.1.2,](#page-5-2)
    deploying these methods in practice is impractical for most consumers. These observations
    are consistent with prior works [\[45,](#page-13-26) [46,](#page-13-38) [231\]](#page-17-8)
    that discuss the difficulty in practically determining and relying on this information
    without support from DRAM manufacturers.


    #### <span id="page-7-0"></span>4.4. Study 4: Improving Memory Reliability


    Main memory reliability is a key system design concern because memory errors can
    cause data loss or system failure if mismanaged (discussed further in Section
    [3.2.1\)](#page-4-7). Systemmemory cooperation can supplement the memory chip
    with additional mechanisms to improve its base reliability beyond what producers
    alone can provide.[9](#page-7-3)


    <span id="page-7-1"></span>4.4.1. Solutions to Improve Memory Reliability. Systemmemory
    cooperative solutions that DRAM consumers can implement to improve memory reliability
    identify and/or address memory errors before they impact the system at large.
    Hardware solutions include rank-level ECC [\[51,](#page-13-43) [53,](#page-13-44)
    [54,](#page-13-45) [62,](#page-14-36) [63,](#page-14-37) [71,](#page-14-38) [72,](#page-14-39)
    [81,](#page-14-40) [87,](#page-14-14) [93,](#page-14-41) [101,](#page-14-42) [105,](#page-14-43)
    [108\]](#page-15-43), rank-level ECC scrubbing [\[94,](#page-14-12) [221,](#page-17-0)
    [279,](#page-18-6) [280,](#page-18-7) [315,](#page-18-20) [315–](#page-18-20)[319\]](#page-18-21),
    and bit repair techniques [\[79,](#page-14-29) [86,](#page-14-3) [320](#page-18-22)[–327\]](#page-18-23).
    Software-based approaches include retiring known-bad memory pages [\[49,](#page-13-29)
    [60,](#page-14-44) [83,](#page-14-45) [84,](#page-14-13) [88,](#page-14-46) [103\]](#page-14-30),
    and predicting failures [\[328–](#page-18-24) [333\]](#page-19-0).


    These solutions all enable DRAM consumers to adapt unreliable DRAM chips to systems
    that require reliable main memory at reasonable cost.[10](#page-7-4) For example,
    HOTH [\[321\]](#page-18-25) supplements rank-level ECC with a cache-like hardware
    mechanism to track faulty memory locations, enabling the system to detect and
    correct one additional error for each ECC word (i.e., extend SECDED to 2EC3ED).


    4.4.2. Application to Today''s Commodity DRAM Chips. Unfortunately, exposing memory
    errors outside of the DRAM chip is at best a gray area within the separation of
    concerns between DRAM producers and consumers. Commodity DRAM is designed to work
    for a wide variety of systems at a reasonable (albeit unspecified)[11](#page-7-5)
    error rate. In general, a consumer who is concerned about the remaining errors
    they may encounter must design and build their own solutions (i.e., outside of
    the DRAM chip) to tolerate memory errors.[12](#page-7-6)


    However, these solutions fundamentally rely on understanding how those errors
    might manifest in the first place. Each error-mitigation mechanism targets a particular
    error model, which defines the scope of the errors that it is designed to mitigate.
    As a result, although a given mechanism efficiently mitigates errors within its
    target error model, it may fail to do so if errors no longer fit the model. In
    such cases, a different error-mitigation mechanism (or possibly, a combination
    of multiple mechanisms) may be more suitable.


    For example, a coarse-grained approach such as page retirement [\[49,](#page-13-29)
    [60,](#page-14-44) [83,](#page-14-45) [84,](#page-14-13) [88,](#page-14-46) [103\]](#page-14-30)
    efficiently mitigates a small number


    <span id="page-7-3"></span><sup>9</sup>These mechanisms commonly fall under the
    umbrella of memory reliability, availability and serviceability (RAS) features
    [\[312](#page-18-26)[–314\]](#page-18-27).


    <span id="page-7-4"></span><sup>10</sup>Consumers with exceptional reliability
    needs, such as those targeting extreme or hostile environments (e.g., military,
    automotive, industrial, extraterrestrial), may take more extreme measures (e.g.,
    custom components [\[158,](#page-15-31) [159,](#page-15-32) [334](#page-19-1)[–340\]](#page-19-2),
    redundant resources [\[93,](#page-14-41) [341,](#page-19-3) [342\]](#page-19-4))
    to ensure that memory errors do not compromise their systems.


    <span id="page-7-5"></span><sup>11</sup>Academic works speculate that commodity
    DRAM targets a bit error rate (BER) within the range of 10−<sup>16</sup> − 10−<sup>12</sup>
    [\[86,](#page-14-3) [92,](#page-14-4) [321,](#page-18-25) [343\]](#page-19-5),
    but we are not aware of industry-provided values.


    <span id="page-7-6"></span><sup>12</sup>Even designers who adopt custom DRAM solutions
    that sacrifice the cost advantages of commodity memory (e.g., high-reliability
    DRAM [\[158,](#page-15-31) [159\]](#page-15-32)) may supplement the DRAM chips
    with additional error-mitigation mechanisms outside of the DRAM chip.


    of errors at fixed bit positions. However, page retirement exhibits significant
    capacity and performance overheads at high error rates or when mitigating errors
    that change positions over time [\[83,](#page-14-45) [84,](#page-14-13) [344\]](#page-19-6).
    In contrast, a fine-grained hardware-based approach such as a block error-correcting
    code [\[345–](#page-19-7)[350\]](#page-19-8) can efficiently mitigate a limited
    number of randomly-distributed errors but can fail silently (and even exacerbate
    the number of errors present [\[38,](#page-13-22) [40,](#page-13-30) [115,](#page-15-40)
    [119,](#page-15-5) [226,](#page-17-5) [230,](#page-17-6) [351\]](#page-19-9))
    when its correction capability is exceeded. We conclude that it is essential for
    the system designer to know when and how errors occur in a given memory chip in
    order to make an informed choice of which error-mitigation mechanism to use in
    a particular system.


    Unfortunately, DRAM consumers generally do not have access to definitive error
    models for commodity DRAM chips. Obtaining this information without cooperation
    from producers requires extensive reliability testing (as discussed throughout
    Section [5\)](#page-8-0), guidance from external (possibly untrustworthy) sources,
    or otherwise reasoning about memory errors at a high level (e.g., disregarding
    uninteresting failure modes). As a result, the separation of concerns between
    DRAM producers and consumers effectively discourages consumers from exploring
    the full design space for error-mitigation mechanisms.


    ## <span id="page-8-0"></span>5. DRAM Reliability Testing


    As our case studies in Sections [4](#page-5-0) show, enabling DRAM consumers to
    reason about how a given DRAM operating point way will impact its reliable operation
    is essential for enabling them to adopt system-memory cooperative mechanisms to
    address DRAM scaling challenges. This section formalizes the information that
    a DRAM consumer may need (but does not necessarily have access to today) in order
    to quantitatively reason about DRAM reliability.


    #### 5.1. Information Flow During Testing


    Figure [3](#page-8-1) describes the flow of information necessary for a consumer
    to quantitatively estimate[13](#page-8-2) a DRAM chip''s error characteristics
    ⃝5 starting from basic properties of the chip ⃝1 . In principle, these characteristics
    can comprise any aspect of DRAM reliability that a consumer wants to quantify
    while exploring their system''s design and/or configuration space.


    <span id="page-8-2"></span><sup>13</sup>"Estimate" because, in general, no model
    or experiment is likely to be perfect, including those provided by manufacturers.


    Examples include: (1) worst-case error rates (e.g., bit error rate (BER) or failures
    in time (FIT)) across a given set of operating points; (2) a profile of error-prone
    memory locations; or (3) a list of error-free operating points (e.g., as identified
    in a shmoo analysis [\[352\]](#page-19-10)). The error characteristics can be
    estimated in two different ways: testing or modeling.


    5.1.1. Determination from Testing. First, a consumer may estimate error characteristics
    using measurements from detailed experimental testing ⃝3 across a variety of operating
    conditions. Examples of measured quantities include: aggregate error rates, per-cell
    probabilities of error, and spatial/temporal error distributions. These measurements
    can be made using testing infrastructures ranging from industry-standard largescale
    testing equipment [\[353,](#page-19-11) [354\]](#page-19-12) to home-grown tools
    based on commodity FPGAs [\[16,](#page-13-28) [29,](#page-13-20) [55,](#page-14-16)
    [65,](#page-14-31) [90,](#page-14-19) [171,](#page-16-4) [263,](#page-17-15) [272,](#page-17-21)
    [355](#page-19-13)[–358\]](#page-19-14) or DRAM-based computing systems [\[236,](#page-17-22)
    [262,](#page-17-14) [273,](#page-18-0) [359,](#page-19-15) [360\]](#page-19-16).


    To conduct accurate and rigorous testing, the consumer must use an effective test
    methodology ⃝2 that suits the particular DRAM chip under test. Prior works extensively
    study key aspects of effective test methodologies, including appropriate data
    and access patterns, the effects of enabling/disabling DRAM chip features such
    as target row refresh (TRR) [\[23,](#page-13-11) [28,](#page-13-13) [32,](#page-13-14)
    [48,](#page-13-27) [301\]](#page-18-28) and on-die error correcting codes (on-die
    ECC) [\[38–](#page-13-22)[40,](#page-13-30) [87,](#page-14-14) [115,](#page-15-40)
    [143,](#page-15-22) [166,](#page-16-2) [168,](#page-16-26) [230,](#page-17-6)
    [361–](#page-19-17)[363\]](#page-19-18), and the viability of different DRAM command
    sequences (e.g., sequences that enable in-DRAM row copy operations [\[55,](#page-14-16)
    [90,](#page-14-19) [96,](#page-14-21) [364\]](#page-19-19), true randomnumber
    generation [\[75,](#page-14-18) [91,](#page-14-20) [266,](#page-17-23) [365\]](#page-19-20),
    and physically unclonable functions [\[73,](#page-14-0) [267\]](#page-17-24)).


    In turn, choosing an effective test methodology requires knowing basic properties
    about a DRAM chip''s design and/or error mechanisms ⃝1 . For example, DRAM manufacturer''s
    design choices for the sizes of internal storage arrays (i.e., mats [\[12,](#page-13-9)
    [78,](#page-14-2) [91,](#page-14-20) [366\]](#page-19-21)), charge encoding conventions
    of each cell (i.e., the true- and anti-cell organization [\[117,](#page-15-26)
    [150\]](#page-15-27)), use of ondie reliability-improving mechanisms (e.g., on-die
    ECC, TRR), and organization of row and column addresses all play key roles in
    determining whether a DRAM chip is likely to show errors from key error mechanisms
    (e.g., data retention [\[115,](#page-15-40) [117,](#page-15-26) [150,](#page-15-27)
    [277,](#page-18-2) [367–](#page-19-22)[369\]](#page-19-23), access-latency-related
    failures [\[14,](#page-13-35) [16,](#page-13-28) [22,](#page-13-10) [52,](#page-13-37)
    [74,](#page-14-1) [77,](#page-14-5) [78,](#page-14-2) [91\]](#page-14-20), and
    RowHammer [\[25,](#page-13-19) [26,](#page-13-40) [29,](#page-13-20) [30,](#page-13-46)
    [232,](#page-17-9) [370\]](#page-19-24)).


    5.1.2. Determination from Modeling. Second, the consumer may make predictions
    from analytical or empirical er-


    <span id="page-8-1"></span>![](_page_8_Figure_12.jpeg)


    Figure 3: Flow of information necessary to determine key error characteristics
    for a given DRAM device.


    ror models ⃝4 based on a previous understanding of DRAM errors (e.g., from past
    experiments or scientific studies). Examples of such error models include: analytical
    models based on understanding DRAM failure modes (e.g., sources of runtime faults
    [\[51,](#page-13-43) [60,](#page-14-44) [149,](#page-15-25) [371](#page-19-25)[–373\]](#page-19-26)),
    parametric statistical models that provide useful summary statistics (e.g., lognormal
    distribution of cell data-retention times [\[276,](#page-18-1) [277,](#page-18-2)
    [374](#page-19-27)[–380\]](#page-19-28), exponential distribution of the time-in-state
    of cells susceptible to variable-retention time (VRT) [\[65,](#page-14-31) [94,](#page-14-12)
    [150,](#page-15-27) [166,](#page-16-2) [367,](#page-19-22) [381](#page-19-29)[–389\]](#page-19-30)),
    physics-based simulation models (e.g., TCAD [\[232,](#page-17-9) [374,](#page-19-27)
    [390–](#page-20-0) [392\]](#page-20-1) and SPICE models [\[14,](#page-13-35) [59,](#page-14-6)
    [78,](#page-14-2) [106,](#page-15-38) [109,](#page-15-39) [283,](#page-18-10)
    [393](#page-20-2)[–395\]](#page-20-3)), and empirically-determined curves that
    predict observations well (e.g., single-bit error rates [\[30,](#page-13-46) [65,](#page-14-31)
    [67,](#page-14-24) [92,](#page-14-4) [94,](#page-14-12) [150\]](#page-15-27)).
    Similar to testing, using error models to predict error characteristics ultimately
    relies on understanding the DRAM chip being tested because the accuracy of the
    predictions requires choosing appropriate models and model parameters (e.g., through
    testing ⃝3 or directly from fundamental chip design properties ⃝1 ).


    ### 5.2. Access to Modeling and Testing Information


    Figure [3](#page-8-1) shows that determining a DRAM chip''s error characteristics
    through modeling or testing ultimately relies on understanding the chip''s fundamental
    design properties. This reliance can be implicit (e.g., inherent within a pre-existing
    workflow designed for a specific chip) or explicit (e.g., chosen as part of a
    home-grown testing methodology). Therefore, a consumer must be vigilant of the
    information they (perhaps unknowingly) rely upon at each step of their design
    process concerning commodity DRAM.


    Fortunately, the consumer only needs to be concerned with the information flow
    at the children of a node whose information is already known from a trustworthy
    source. For example, a consumer who wants to identify the locations of error-prone
    cells (i.e., ⃝5 ) using testing need not be concerned with chip design properties
    (i.e., ⃝1 ) if DRAM manufacturers provide appropriate test methodologies (i.e.,
    ⃝2 ) or detailed test results (i.e., ⃝3 ). Unfortunately, to our knowledge, neither
    DRAM standards nor manufacturers provide the information in any of the nodes today,
    much less in a clear, industry-validated manner. Therefore, the consumer lacks
    a base of trustworthy information to build upon. This creates a barrier to entry
    for a consumer who wants to explore optimizations to commodity DRAM by compromising
    the consumer''s ability to make well-informed or effective decisions.


    In general, except for the few major DRAM customers who may be able to secure
    confidentiality agreements,[14](#page-9-1) consumers would need to rely on (possibly
    incorrect or incomplete) inferences or assumptions based on domain knowledge or
    reverseengineering studies (e.g., similar in spirit to [\[16,](#page-13-28) [23,](#page-13-11)
    [24,](#page-13-31) [28,](#page-13-13) [32,](#page-13-14) [40,](#page-13-30) [74,](#page-14-1)
    [78,](#page-14-2) [92,](#page-14-4) [114,](#page-15-3) [117,](#page-15-26) [150–](#page-15-27)[155\]](#page-15-28))
    that are not verified or supported by the DRAM industry.[15](#page-9-2) As a result,
    the need for assumptions


    can discourage practitioners from exploring the full design space even when a
    given design choice is otherwise beneficial. We conclude that the lack of information
    transparency is a serious impediment to adopting new designs for addressing DRAM
    scaling challenges today.


    ## <span id="page-9-0"></span>6. Rethinking Today''s Separation of Concerns


    Our case studies throughout Section [4](#page-5-0) demonstrate that although DRAM
    consumers have explored new and creative ways to address DRAM scaling challenges,
    their solutions face serious practicality concerns because of limited access to
    information about DRAM chip reliability. In this section, we hypothesize that
    the unavailability of this information is caused by a lack of transparency within
    DRAM standards which provide control over, but not insight into, DRAM operations.


    #### 6.1. The Problem of Information Unavailability


    Addressing DRAM scaling challenges fundamentally requires understanding how those
    challenges impact system operation. Therefore, it is unsurprising that reliability
    analysis and testing is central to each of the approaches we survey in our case
    studies. In some cases, solutions explicitly require reliability testing (e.g.,
    identifying and monitoring the physical locations of error-prone cells). Other
    solutions implicitly rely on the results of reliability analysis (e.g., configuring
    RowHammer defenses based on a chip''s degree of vulnerability). Ultimately, deploying
    consumer-driven solutions to DRAM scaling challenges requires some degree of understanding
    of how different (representative) operating conditions impact DRAM and overall
    system reliability.


    Unfortunately, the separation of concerns does not convey this information, which
    discourages consumers from adopting such solutions in practice. For example, recent
    works [\[45,](#page-13-26) [46\]](#page-13-38) discuss the pitfalls of designing
    RowHammer defense mechanisms that rely on knowledge of how RowHammer errors behave
    (e.g., HCfirst, dependence on a chip''s internal cell organization), calling into
    question the practicality of accurately determining these details given an arbitrary
    DRAM chip. Knowing or determining this information is essential to guarantee protection
    against RowHammer. However, determining it without guidance from DRAM manufacturers
    requires trusting in a home-grown testing or reverse-engineering methodology,
    which ultimately relies on knowledge of DRAM chip details that likely needs to
    be assumed or inferred (as discussed in Sections [4.3.2](#page-7-7) and [5\)](#page-8-0).


    As a result, system-memory cooperative solutions to overcome scaling challenges
    largely remain theoretical ideas or proofs-of-concept based on performance and
    reliability characteristics that are assumed, inferred, or reverse-engineered
    from a limited set of observations and DRAM products (e.g., using in-house experimental
    studies [\[13,](#page-13-33) [16,](#page-13-28) [17,](#page-13-34) [22,](#page-13-10)
    [29,](#page-13-20) [52,](#page-13-37) [55,](#page-14-16) [73–](#page-14-0)[75,](#page-14-18)
    [78,](#page-14-2) [91,](#page-14-20) [150,](#page-15-27) [171,](#page-16-4) [263,](#page-17-15)
    [266,](#page-17-23) [267,](#page-17-24) [271–](#page-17-18)[273\]](#page-18-0))
    without DRAM manufacturers'' support.


    Unfortunately, this lack of a trustworthy base of information to build upon can
    discourage even the most enterprising consumers from exploring new designs. Such
    exploration would


    <span id="page-9-1"></span><sup>14</sup>Even under confidentiality, DRAM manufacturers
    may be unwilling to reveal certain proprietary design details (e.g., on-die error
    correction [\[40,](#page-13-30) [224\]](#page-17-3), target row refresh [\[46\]](#page-13-38))
    or provide specifically requested numbers.


    <span id="page-9-2"></span><sup>15</sup>DRAM manufacturers may make assumptions
    during their own testing. However, they have full transparency into their own
    designs (i.e, the root node in the information flow), so they can make the most
    informed decision.


    <span id="page-10-1"></span>


    | Design Property                                                       | Reverse-Engineered
    By                     | Use-Case(s) Relying on Knowing the Property                                                                                                                                                                                                          |

    |-----------------------------------------------------------------------|-------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|

    | Cell charge encoding convention<br>(i.e., true- and anti-cell layout) | Testing
    [92, 115, 117, 150]               | Data-retention error modeling and testing
    for mitigating refresh<br>overheads (e.g., designing worst-case test patterns)
    [68, 117, 150]                                                                                                              |

    | On-die ECC details                                                    | Modeling
    and testing [40,<br>115]         | Improving reliability (e.g., designing ECC
    within the memory<br>controller) [38, 41, 119, 167], mitigating RowHammer [28,
    32, 48, 260]                                                                                                               |

    | Target row refresh (TRR) details                                      | Testing
    [23, 32]                          | Modeling and mitigating RowHammer [23, 32,
    48]                                                                                                                                                                                                       |

    | Mapping between internal and external<br>row addresses                | Testing
    [28, 78, 114, 151,<br>153, 245]   | Mitigating RowHammer [28, 29, 114, 151, 152]                                                                                                                                                                                                         |

    | Row addresses refreshed by each<br>refresh operation                  | Testing
    [32]                              | Mitigating RowHammer [32], improving access
    timings [106, 198]                                                                                                                                                                                       |

    | Substructure organization (e.g., cell<br>array dimensions)            | Modeling
    [78] and<br>testing [16, 74, 78] | Improving DRAM access timings [16, 74, 78]
    and security [287]                                                                                                                                                                                        |

    | Analytical model parameters<br>(e.g., bitline capacitance)            | Modeling
    and testing [150,<br>277]        | Developing and using error models for improving
    overall<br>reliability [376], mitigating refresh overheads (e.g., data-retention
    [277,<br>371, 375] and VRT [383, 384] models), improving access timings [78],<br>and
    mitigating RowHammer [30, 396] |


    #### Table 2: Basic DRAM chip microarchitectural design properties that are typically
    assumed or inferred for experimental studies.


    require weighing any potential benefits (e.g., improved performance, security,
    etc.) against both: (1) risks (e.g., failures in the field) associated with potentially
    operating outside manufacturer recommendations and (2) limitations due to compatibility
    with only a subset of all commodity DRAM products (e.g., only those that have
    been accurately reverse-engineered). These risks and limitations are a serious
    barrier to adoption; therefore, we conclude that the lack of information transparency
    today discourages system designers from exploring alternative designs that have
    been shown to provide tangible benefits.


    #### 6.2. DRAM Standards Lack Transparency


    Historically, DRAM standards have not discussed DRAM chip reliability because
    the separation of concerns assigns DRAM producers (near)[16](#page-10-0) full
    responsibility to address DRAM-related reliability concerns. Therefore, DRAM producers
    are expected to address reliability-related issues, leaving consumers to integrate
    reliable DRAM chips. As technology scaling continues to degrade DRAM chip reliability,
    modern DRAM standards are exposing new reliability-related features, such as on-die
    ECC scrubbing [\[34,](#page-13-16) [119,](#page-15-5) [319\]](#page-18-21), post-package
    repair [\[33,](#page-13-15) [34,](#page-13-16) [216,](#page-16-22) [222,](#page-17-1)
    [223\]](#page-17-2), target row refresh [\[23,](#page-13-11) [32\]](#page-13-14),
    and refresh management [\[34,](#page-13-16) [47\]](#page-13-39). Unfortunately,
    more general reasoning about reliability remains elusive for consumers at large.


    We believe that this state of affairs naturally arises from establishing a a clear
    and explicit interface between producers and consumers. Consequently, ensuring
    that the standards leave enough flexibility for diverse consumer use-cases requires
    careful and explicit attention. This is because the standards are susceptible
    to abstraction inversion [\[397\]](#page-20-5), a design antipattern in which
    a previously agreed-upon interface becomes an obstacle, forcing system designers
    to re-implement basic functionality in terms of the outdated abstraction. A rigid
    interface limits what is and is not possible, potentially requiring unproductive
    reverse-engineering to work around.


    We contend that the difficulty that consumers face today in addressing DRAM scaling
    challenges clearly indicates abstraction inversion: the separation of concerns
    has aged without sufficient attention to flexibility. Although a fixed operating
    point defines a clear interface, we believe that leaving room for (and potentially
    even encouraging) different operating points is essential today.


    #### <span id="page-10-2"></span>6.3. Benefits for Both Producers and Consumers


    Today''s separation of concerns discourages not only consumers from exploring
    new ways to work with commodity DRAM chips but also producers from adopting consumer-driven
    ideas that help address DRAM scaling challenges. In other words, the separation
    of concerns effectively discourages both parties from exploring solutions outside
    their areas of concern. As a result, neither party explores the full design space
    surrounding commodity DRAM chips.


    We believe that rethinking the separation to encourage cooperation stands to benefit
    all aspects of DRAM technology, encompassing both the producers and consumers
    who build and use DRAM, respectively. Producers gain access to a broad base of
    innovation from consumers who prototype solutions (with or without additional
    investment from producers themselves), thereby creating new opportunities for
    producers to make DRAM a more competitive product. Consumers gain access to new
    ways to improve system-level metrics, such as performance and energy efficiency,
    that were previously not practical. Ultimately, both producers and consumers benefit
    from the best possible version of DRAM technology.


    ## 7. DRAM Standards as the Solution


    Separating design concerns between producers and consumers is practical for enabling
    each party to focus on their respective areas of expertise. However, we recommend
    that the separation be crafted in a way that not only enables both parties to
    help address DRAM scaling challenges, but ideally encourages and aids it. To achieve
    both goals, we propose extending DRAM standards in a way that enables consumers
    to make informed


    <span id="page-10-0"></span><sup>16</sup>Consumers with exceptional reliability
    requirements may then choose to supplement DRAM chips with additional error-mitigation
    mechanisms, as discussed in Section [3.2.1.](#page-4-7)


    decisions about how their design choices will affect a DRAM chip''s reliable operation.
    In other words, instead of modifying DRAM designs, we advocate modifying standards
    to facilitate transparency of DRAM reliability characteristics. Armed with this
    information, consumers can freely explore how to best use commodity DRAM chips
    to solve their own design challenges while preserving the separation of concerns
    that allows DRAM designers to focus on building the best possible standardscompliant
    DRAM chips.


    #### <span id="page-11-0"></span>7.1. Choosing Information to Release


    We identify what information to release based on our analysis of information flow
    throughout DRAM reliability testing in Section [5.](#page-8-0) We observe that,
    given the information at any node in Figure [3,](#page-8-1) consumers can self-determine
    the information at each of its child nodes. As a result, access to trustworthy
    information at any node provides consumers with a foundation on which to build
    their own designs. Therefore, we recommend that the DRAM industry release information
    at at least one node, but that producers be free to choose that information based
    on their interests and capabilities. This section examines realistic possibilities
    for communicating information at each node of the flowchart.


    7.1.1. Microarchitectural Design Properties. At the lowest level, DRAM producers
    can communicate basic microarchitectural design properties that enable consumers
    to develop robust test methodologies and error models. This is the most general
    and flexible approach because it places no limitations on what types of studies
    consumers may pursue (e.g., in contrast to providing information that is useful
    for reasoning about select error mechanism(s)). Table [2](#page-10-1) reviews
    example properties used by prior works to build system-level solutions for addressing
    DRAM scaling challenges. For each design property, we list prior works that reverse-engineer
    it and describe use-cases that rely on its knowledge.


    We believe that releasing these properties will minimally (if at all) impact DRAM
    producer''s business interests given that each of the properties can be reverse-engineered
    with existing methods (as shown by Table [2,](#page-10-1) Column 2) and access
    to appropriate tools, as demonstrated by prior studies [\[16,](#page-13-28) [23,](#page-13-11)
    [28,](#page-13-13) [29,](#page-13-20) [32,](#page-13-14) [40,](#page-13-30) [65,](#page-14-31)
    [74,](#page-14-1) [78,](#page-14-2) [92,](#page-14-4) [114](#page-15-3)[–117,](#page-15-26)
    [150–](#page-15-27)[153,](#page-15-45) [277\]](#page-18-2). Releasing this information
    in an official capacity confirms what is already demonstrated publicly through
    experiment, yielding no further information than others already have the capability
    to identify. On the other hand, knowing this information empowers all consumers
    to confidently explore a larger design space, benefiting both designers and producers
    in the long run (as discussed in Section [6.3\)](#page-10-2).


    7.1.2. Test Methodologies. Abstracting beyond microarchitectural details, DRAM
    producers can disclose effective test methodologies for consumers to conduct their
    own reliability studies (e.g., to explore new viable operating points). Providing
    test methodologies absolves (1) producers from needing to disclose chip design
    details; and (2) consumers from needing the DRAM-related expertise to determine
    the test methodologies from those details.[17](#page-11-1) As a limitation, disclosing
    only test methodologies constrains consumers to work with only the particular
    error mechanisms that the methodologies are designed for (e.g., data-retention,
    RowHammer). Table [3](#page-11-2) provides example test parameters that prior
    works generally depend on (e.g., assume or reverse-engineer) to conduct reliability
    testing.


    <span id="page-11-2"></span>


    | Test Parameter              | Description                                                                                                                              |  |

    |-----------------------------|------------------------------------------------------------------------------------------------------------------------------------------|--|

    | Data pattern                | Data pattern that maximizes the chance of<br>errors
    occurring [28, 29, 32, 48, 65, 74, 92, 116,<br>116–118, 150, 245, 262, 356, 398–400]
    |  |

    | Environmental<br>conditions | Temperature and voltage that lead to<br>worst-case
    behavior [31, 75, 118, 150, 218, 277,<br>356, 357, 382, 401]                          |  |

    | Test algorithm              | Sequence of representative and/or worst-case<br>DRAM
    operations to test [22, 29, 32, 48, 73, 75,<br>150, 262, 402]                       |  |


    Table 3: Testing parameters that are typically assumed or inferred during experimental
    studies.


    7.1.3. Test Results and/or Error Models. At the highest level of abstraction,
    DRAM producers can directly disclose test results and/or error models related
    to specific studies useful to consumers. For example, these could take the form
    of parametric error models (e.g., the statistical relationship between operating
    timings and error rates) along with parameter values for each chip; fine-granularity
    error characteristics (e.g., per-column minimum viable access timings); and/or
    specific summary statistics (e.g., HCfirst in studies pertaining to RowHammer).
    In this way, consumers exploring new designs can avoid the need to conduct reliability
    testing to identify the producer-provided information. As a limitation, directly
    releasing test results and/or error models constrains consumers to developing
    solutions only for those design concerns that pertain to the released information.
    Table [4](#page-12-0) provides examples of key test results and error models that
    prior works leverage in order to explore new design points based on commodity
    DRAM chips.


    #### 7.2. Choosing When to Release the Information


    We recommend decoupling the release of information from the requirement to do
    so because modifying DRAM standards is a slow process due to the need for consensus
    among DRAM stakeholders (discussed in Section [2.4\)](#page-3-5). To this end,
    we recommend a practical two-step process with different approaches in the short-
    and long-term.


    7.2.1. Step 1: Immediate Disclosure of Information. We recommend two independent
    approaches to quickly release information in the short-term. First, we recommend
    establishing a publicly-accessible database for researchers and practitioners
    to aggregate information (e.g., reverse-engineered design details) through crowdsourcing.
    We believe this is practical given the significant academic and industry interest
    in addressing


    <span id="page-11-1"></span><sup>17</sup>We believe that interested parties already
    have such expertise, as shown by the fact that many studies [\[16,](#page-13-28)
    [23,](#page-13-11) [28,](#page-13-13) [29,](#page-13-20) [32,](#page-13-14) [40,](#page-13-30)
    [65,](#page-14-31) [74,](#page-14-1) [78,](#page-14-2) [92,](#page-14-4) [114–](#page-15-3)[117,](#page-15-26)
    [150](#page-15-27)[–153,](#page-15-45) [277\]](#page-18-2) establish test methodologies
    through experimentation.


    <span id="page-12-0"></span>


    | Test Result or                          | Description                                          |  |

    |-----------------------------------------|------------------------------------------------------|--|

    | Error Model                             |                                                      |  |

    | Data-retention<br>times                 | Minimum refresh rate required for
    different          |  |

    |                                         | DRAM regions (e.g., rows, cells) [65,
    66, 79, 80,    |  |

    |                                         | 86, 150, 403]                                        |  |

    | Error profile                           | List of cells susceptible to errors
    (e.g., VRT [65,  |  |

    |                                         | 94, 150], latency-related [16, 52,
    73–75])           |  |

    | Error rate<br>summary<br>statistics     | Aggregate error rates (e.g., BER [92,
    115, 150,      |  |

    |                                         | 166, 356], FIT [218, 404, 405]), distribution        |  |

    |                                         | parameters (e.g., copula [384], lognormal
    [276,      |  |

    |                                         | 277, 376], exponential [80, 387])                    |  |

    | RowHammer<br>blast radius               | Maximum number of rows affected by                   |  |

    |                                         | hammering one or more row(s) [28,
    29, 107, 231,      |  |

    |                                         | 300, 396]                                            |  |

    | HCfirst<br>or<br>RowHammer<br>Threshold |                                                      |  |

    |                                         | Minimum number of RowHammer accesses                 |  |

    |                                         | required to induce bit-flips [28,
    29, 107, 118, 306] |  |


    Table 4: Examples of key test results and error models from prior works that study
    and/or optimize commodity DRAM.


    DRAM scaling challenges. Such a database would provide an opportunity for peer
    review of posted information, increasing the likelihood that the information is
    trustworthy. In the long run, we believe such a database would facilitate information
    release from DRAM producers themselves because the producers could simply validate
    database information, if not contribute directly.


    Second, we recommend that commodity DRAM producers individually release information
    for current DRAM chips and those already in the field. For example, producers
    may update chip datasheets to incorporate relevant design properties or make more
    extensive information available online (e.g., similar to how some producers already
    provide compliance documents and functional simulation models on their websites
    [\[406](#page-20-13)[–408\]](#page-20-14)). Releasing the information described
    in Section [7.1](#page-11-0) requires no changes to DRAM designs or standards,
    though modifying standards (e.g., via an addendum, as we suggest in Step 2) would
    help unify the information release across all producers. Regardless, we believe
    it is important to release information in the near term (even if not standardized)
    so that it is available as soon as possible.


    7.2.2. Step 2: Explicit DRAM Reliability Standards. In the long term, we recommend
    modifying DRAM standards to promote (or even require) producers to disclose information
    that can have consumer-visible impact to DRAM reliability. This may include any
    or all of the information discussed throughout this paper; we believe that the
    DRAM stakeholders themselves (i.e., DRAM producers and consumers) must collectively
    determine and standardize which information is the most relevant and useful to
    regulate.


    As a concrete example of how such changes to standards may occur, we reference
    test methodologies [\[121,](#page-15-7) [122\]](#page-15-8) and error models [\[120\]](#page-15-6)
    that JEDEC provides for NAND flash memory endurance [\[123](#page-15-9)[–125\]](#page-15-10),
    including floating-gate data retention [\[126](#page-15-11)[–129\]](#page-15-12)
    and threshold voltage distributions [\[130–](#page-15-13)[133\]](#page-15-47).
    These documents outline standardized best practices for studying and characterizing
    endurance properties of SSD devices. We envision analogous documents released
    for key DRAM error mechanisms (e.g., data-retention, access-timing-related, RowHammer),
    providing a standardized and trustworthy alternative to inferring the same information
    through unofficial channels.


    #### 7.3. Natural Progression Toward Transparency


    As a final note, we anticipate that efforts to overcome DRAM technology scaling
    challenges will naturally bring DRAM producers and consumers closer together in
    pursuit of the best possible solutions. Diversifying consumer needs, increasing
    use of system-memory cooperation [\[10,](#page-13-7) [29,](#page-13-20) [112,](#page-15-37)
    [409,](#page-20-15) [410\]](#page-20-16), and emerging, non-traditional DRAM architectures
    [\[185,](#page-16-28) [200,](#page-16-29) [201,](#page-16-30) [410](#page-20-16)[–417\]](#page-20-17)
    all challenge existing DRAM design and use practices today. As scaling challenges
    continue to worsen, the opportunity costs of maintaining today''s separation of
    concerns will do so as well.


    However, if we are to step ahead of worsening DRAM scaling challenges, we must
    ensure that the standards of the future proactively enable the whole community
    (both industry and academia) to collectively develop creative and effective solutions.
    Although recent changes to DRAM standards such as refresh management [\[34,](#page-13-16)
    [47\]](#page-13-39) and on-die ECC scrubbing [\[34,](#page-13-16) [119,](#page-15-5)
    [319\]](#page-18-21) are increasing signs of cooperation, these changes are reactions
    to long-standing problems. Our work preempts this post hoc approach to system
    design, preemptively forging a path toward cooperative solutions capable of holistically
    address the scaling challenge.


    ## 8. Conclusion


    We show that the separation of concerns between DRAM producers and consumers is
    an impediment to overcoming modern DRAM scaling challenges because it discourages
    exploring the full design space around standardized designs. Our case studies
    that support this observation find that consumers'' lack of insight into DRAM
    reliability is the key factor discouraging more efficient solutions based on system-memory
    cooperation. We then analyze how consumers can obtain this insight through DRAM
    reliability testing, and we introduce a twostep approach to revise the separation
    of concerns to encourage system-memory cooperation. We start with conceptual changes
    to the separation of concerns and build toward modifying the DRAM standards that
    specify the separation. Our work is a call-to-action for more open and flexible
    practices for DRAM design and use, harnessing the synergy between researchers
    and practitioners to fully explore the potential of DRAM technology.


    ## Acknowledgment


    We thank the members of the SAFARI Research Group for their valuable feedback
    and the constructively critical environment that they provide. We specifically
    thank Geraldo F. Oliveira, Jisung Park, Haiyu Mao, Jawad Haj-Yahya, Jeremie S.
    Kim, Hasan Hassan, Joel Lindegger, and Meryem Banu Cavlak for the feedback they
    provided on earlier versions of this paper. We thank external experts who helped
    shape our arguments, including Mattan Erez, Moinuddin Qureshi, Vilas Sridharan,
    Christian Weis, and Tanj Bennett. This work was supported in part by the generous
    gifts provided by our industry partners, including Google, Huawei, Intel, Microsoft,
    and VMware, and support from the ETH Future Computing Laboratory and the Semiconductor
    Research Corporation. A much earlier version of this work was placed on arXiv
    in 2022 [\[418\]](#page-20-18).


    ## <span id="page-13-0"></span>References


    - [1] R. H. Dennard, "Field-Effect Transistor Memory," 1968, US Patent 3,387,286.

    - <span id="page-13-1"></span>[2] R. H. Dennard, F. H. Gaensslen, H.-N. Yu, V.
    L. Rideout, E. Bassous, and A. R. LeBlanc, "Design of Ion-Implanted MOSFET''s
    with Very Small Physical Dimensions," JSSC, 1974.

    - <span id="page-13-2"></span>[3] B. Keeth, R. J. Baker, B. Johnson, and F. Lin,
    DRAM Circuit Design: Fundamental and High-Speed Topics. John Wiley & Sons, 2007.

    - [4] J. Markoff, "IBM''s Robert H. Dennard and the Chip That Changed the World,"
    2019, [https://www.ibm.com/blogs/think/2019/11/ibms-r](https://www.ibm.com/blogs/think/2019/11/ibms-robert-h-dennard-and-the-chip-that-changed-the-world/)
    [obert-h-dennard-and-the-chip-that-changed-the-world/.](https://www.ibm.com/blogs/think/2019/11/ibms-robert-h-dennard-and-the-chip-that-changed-the-world/)

    - [5] Nature Electronics, "Memory Lane," 2018.

    - <span id="page-13-3"></span>[6] "DRAM: The Invention of On-Demand Data," [https://www.ibm.com/](https://www.ibm.com/ibm/history/ibm100/us/en/icons/dram/transform/)
    [ibm/history/ibm100/us/en/icons/dram/transform/,](https://www.ibm.com/ibm/history/ibm100/us/en/icons/dram/transform/)
    2021.

    - <span id="page-13-4"></span>[7] Y.-B. Kim and T. W. Chen, "Assessing Merged
    DRAM/Logic Technology," Integration, 1999.

    - <span id="page-13-5"></span>[8] J. Kittl, K. Opsomer, M. Popovici, N. Menou,
    B. Kaczer, X. P. Wang, C. Adelmann, M. Pawlak, K. Tomida, A. Rothschild et al.,
    "High-K Dielectrics for Future Generation Memory Devices," Microelectronic engineering,
    2009.

    - <span id="page-13-6"></span>[9] IBM, "Icons of Progress: DRAM," [https://www.ibm.com/ibm/history/](https://www.ibm.com/ibm/history/ibm100/us/en/icons/dram/)
    [ibm100/us/en/icons/dram/,](https://www.ibm.com/ibm/history/ibm100/us/en/icons/dram/)
    2023.

    - <span id="page-13-7"></span>[10] O. Mutlu, "Main Memory Scaling: Challenges
    and Solution Directions," in More Than Moore Technologies for Next Generation
    Computer Design. Springer, 2015, pp. 127–153.

    - <span id="page-13-8"></span>[11] IEEE, "More Moore: 2022 Update," International
    Roadmap for Devices and Systems, 2022.

    - <span id="page-13-9"></span>[12] Y. H. Son, O. Seongil, Y. Ro, J. W. Lee, and
    J. H. Ahn, "Reducing Memory Access Latency with Asymmetric DRAM Bank Organizations,"
    in ISCA, 2013.

    - <span id="page-13-33"></span>[13] K. K. Chang, "Understanding and Improving
    Latency of DRAM-Based Memory Systems," Ph.D. dissertation, Carnegie Mellon University,
    2017.

    - <span id="page-13-35"></span>[14] D. Lee, Y. Kim, V. Seshadri, J. Liu, L. Subramanian,
    and O. Mutlu, "Tiered-Latency DRAM: A Low Latency and Low Cost DRAM Architecture,"
    in HPCA, 2013.

    - [15] J. L. Hennessy and D. A. Patterson, Computer Architecture: A Quantitative
    Approach. Elsevier, 2011.

    - <span id="page-13-28"></span>[16] K. K. Chang, A. Kashyap, H. Hassan, S. Ghose,
    K. Hsieh, D. Lee, T. Li, G. Pekhimenko, S. Khan, and O. Mutlu, "Understanding
    Latency Variation in Modern DRAM Chips: Experimental Characterization, Analysis,
    and Optimization," in SIGMETRICS, 2016.

    - <span id="page-13-34"></span>[17] D. Lee, "Reducing DRAM Latency at Low Cost
    by Exploiting Heterogeneity," Ph.D. dissertation, Carnegie Mellon University,
    2016.

    - [18] R. Isaac, "The Remarkable Story of the DRAM Industry," IEEE SSCS News,
    2008.

    - [19] J. Choi, W. Shin, J. Jang, J. Suh, Y. Kwon, Y. Moon, and L.-S. Kim, "Multiple
    Clone Row DRAM: A Low Latency and Area Optimized DRAM," in ISCA, 2015.

    - [20] S. Borkar and A. A. Chien, "The Future of Microprocessors," CACM, 2011.

    - <span id="page-13-36"></span>[21] K. Nguyen, K. Lyu, X. Meng, V. Sridharan,
    and X. Jian, "Nonblocking Memory Refresh," in ISCA, 2018.

    - <span id="page-13-10"></span>[22] D. Lee, Y. Kim, G. Pekhimenko, S. Khan, V.
    Seshadri, K. Chang, and O. Mutlu, "Adaptive-Latency DRAM: Optimizing DRAM Timing
    for the Common-Case," in HPCA, 2015.

    - <span id="page-13-11"></span>[23] P. Frigo, E. Vannacci, H. Hassan, V. van der
    Veen, O. Mutlu, C. Giuffrida, H. Bos, and K. Razavi, "TRRespass: Exploiting the
    Many Sides of Target Row Refresh," in IEEE S&P, 2020.

    - <span id="page-13-31"></span>[24] P. Pessl, D. Gruss, C. Maurice, M. Schwarz,
    and S. Mangard, "DRAMA: Exploiting DRAM Addressing for Cross-CPU Attacks," in
    USENIX Sec., 2016.

    - <span id="page-13-19"></span>[25] O. Mutlu, "The RowHammer Problem and Other
    Issues we may Face as Memory Becomes Denser," in DATE, 2017.

    - <span id="page-13-40"></span>[26] O. Mutlu and J. Kim, "RowHammer: A Retrospective,"
    in TCAD, 2019.

    - <span id="page-13-12"></span>[27] O. Mutlu, A. Olgun, and A. G. Yaglıkcı, "Fundamentally
    Understand- ˘ ing and Solving RowHammer," in ASP-DAC, 2023.

    - <span id="page-13-13"></span>[28] J. S. Kim, M. Patel, A. G. Yaglık ˘ c¸ı, H.
    Hassan, R. Azizi, L. Orosa, and O. Mutlu, "Revisiting RowHammer: An Experimental
    Analysis of Modern Devices and Mitigation Techniques," in ISCA, 2020.

    - <span id="page-13-20"></span>[29] Y. Kim, R. Daly, J. Kim, C. Fallin, J. H.
    Lee, D. Lee, C. Wilkerson, K. Lai, and O. Mutlu, "Flipping Bits in Memory Without
    Accessing Them: An Experimental Study of DRAM Disturbance Errors," in ISCA, 2014.

    - <span id="page-13-46"></span>[30] K. Park, D. Yun, and S. Baeg, "Statistical
    Distributions of Row-Hammering Induced Failures in DDR3 Components," Microelectronics
    Reliability, 2016.

    - <span id="page-13-42"></span>[31] K. Park, C. Lim, D. Yun, and S. Baeg, "Experiments
    and Root Cause Analysis for Active-Precharge Hammering Fault In DDR3 SDRAM Under
    3× Nm Technology," Microelectronics Reliability, 2016.

    - <span id="page-13-14"></span>[32] H. Hassan, Y. C. Tugrul, J. S. Kim, V. Van
    der Veen, K. Razavi, and O. Mutlu, "Uncovering In-DRAM RowHammer Protection Mechanisms:
    A New Methodology, Custom RowHammer Patterns, and Implications," in MICRO, 2021.

    - <span id="page-13-16"></span><span id="page-13-15"></span>[33] JEDEC, DDR4 SDRAM
    Specification, 2012.

    - [34] JEDEC, DDR5 SDRAM Specification, 2020.

    - <span id="page-13-17"></span>[35] JEDEC, "High Bandwidth Memory (HBM) DRAM,"
    JEDEC Standard JESD235D, 2021.

    - <span id="page-13-18"></span>[36] JEDEC, "High Bandwidth Memory DRAM (HBM3),"
    JEDEC Standard JESD238, 2022.

    - <span id="page-13-21"></span>[37] K. Bains, J. Halbert, C. Mozak, T. Schoenborn,
    and Z. Greenfield, "Row Hammer Refresh Command," 2014, US Patent 9,117,544 B2.

    - <span id="page-13-22"></span>[38] Y. H. Son, S. Lee, O. Seongil, S. Kwon, N.
    S. Kim, and J. H. Ahn, "CiDRA: A cache-Inspired DRAM resilience architecture,"
    in HPCA, 2015.

    - [39] N. Kwak, S.-H. Kim, K. H. Lee, C.-K. Baek, M. S. Jang, Y. Joo, S.-H. Lee,
    W. Y. Lee, E. Lee, D. Han, J. Kang, J. H. Lim, J.-B. Park, K.-T. Kim, S. Cho,
    S. W. Han, J. Y. Keh, J. H. Chun, J. Oh, and S. H. Lee, "A 4.8 Gb/s/pin 2Gb LPDDR4
    SDRAM with Sub-100µA Self-Refresh Current for IoT Applications," in ISSCC, 2017.

    - <span id="page-13-30"></span>[40] M. Patel, J. Kim, T. Shahroodi, H. Hassan,
    and O. Mutlu, "Bit-Exact ECC Recovery (BEER): Determining DRAM On-Die ECC Functions
    by Exploiting DRAM Data Retention Characteristics," in MICRO, 2020.

    - <span id="page-13-23"></span>[41] S.-L. Gong, J. Kim, S. Lym, M. Sullivan, H.
    David, and M. Erez, "DUO: Exposing On-Chip Redundancy to Rank-Level ECC for High
    Reliability," in HPCA, 2018.

    - <span id="page-13-24"></span>[42] JEDEC, "System Level RowHammer Mitigation,"
    JEDEC Standard JEP301-1, 2021.

    - <span id="page-13-32"></span>[43] JEDEC, "Near-Term DRAM Level Rowhammer Mitigation,"
    JEDEC Standard JEP300-1, 2021.

    - <span id="page-13-25"></span>[44] JEDEC, "Low Power Double Data Rate 5 (LPDDR5)
    SDRAM Specification," JEDEC Standard JESD209–5A, 2020.

    - <span id="page-13-26"></span>[45] M. K. Qureshi, "Rethinking ECC in the Era
    of Row-Hammer," in DRAMSec, 2021.

    - <span id="page-13-38"></span>[46] S. Saroiu, A. Wolman, and L. Cojocar, "The
    Price of Secrecy: How Hiding Internal DRAM Topologies Hurts Rowhammer Defenses,"
    in IRPS, 2022.

    - <span id="page-13-39"></span>[47] S. Saroiu and A. Wolman, "How to Configure
    Row-Sampling-Based Rowhammer Defenses," DRAMSec, 2022.

    - <span id="page-13-27"></span>[48] P. Jattke, V. van der Veen, P. Frigo, S. Gunter,
    and K. Razavi, "Blacksmith: Scalable Rowhammering in the Frequency Domain," in
    SP, 2022.

    - <span id="page-13-29"></span>[49] S. Baek, S. Cho, and R. Melhem, "Refresh Now
    and Then," in TC, 2014.

    - <span id="page-13-41"></span>[50] F. Brasser, L. Davi, D. Gens, C. Liebchen,
    and A.-R. Sadeghi, "CAn''t Touch This: Software-Only Mitigation Against Rowhammer
    Attacks Targeting Kernel Memory," in USENIX Sec., 2017.

    - <span id="page-13-43"></span>[51] G. C. Cardarilli, P. Marinucci, and A. Salsano,
    "Development of an Evaluation Model for the Design of Fault-Tolerant Solid State
    Mass Memory," in ISCAS, 2000.

    - <span id="page-13-37"></span>[52] K. Chandrasekar, S. Goossens, C. Weis, M.
    Koedam, B. Akesson, N. Wehn, and K. Goossens, "Exploiting Expendable Process-Margins
    in DRAMs for Run-Time Performance Optimization," in DATE, 2014.

    - <span id="page-13-44"></span>[53] L. Chen, Y. Cao, and Z. Zhang, "E3CC: A Memory
    Error Protection Scheme With Novel Address Mapping for Subranked And Low-Power
    Memories," TACO, 2013.

    - <span id="page-13-45"></span>[54] H.-M. Chen, A. Arunkumar, C.-J. Wu, T. Mudge,
    and C. Chakrabarti, "E-ECC: Low Power Erasure And Error Correction Schemes For
    Increasing Reliability Of Commodity DRAM Systems," in Proceedings of the 2015
    International Symposium on Memory Systems, 2015, pp. 60–70.

    - <span id="page-14-16"></span>[55] F. Gao, G. Tziantzioulis, and D. Wentzlaff,
    "ComputeDRAM: In-Memory Compute using Off-the-Shelf DRAMs," in MICRO, 2019.

    - <span id="page-14-25"></span>[56] M. Ghosh and H.-H. S. Lee, "Smart Refresh:
    An Enhanced Memory Controller Design for Reducing Energy in Conventional and 3D
    Die-Stacked DRAMs," in MICRO, 2007.

    - <span id="page-14-32"></span>[57] Z. Greenfield and T. Levy, "Throttling Support
    for Row-Hammer Counters," 2016, U.S. Patent 9,251,885.

    - <span id="page-14-17"></span>[58] N. Hajinazar, G. F. Oliveira, S. Gregorio,
    J. Ferreira, N. M. Ghiasi, M. Patel, M. Alser, S. Ghose, J. G. Luna, and O. Mutlu,
    "SIMDRAM: An End-to-End Framework for Bit-Serial SIMD Computing in DRAM," ASPLOS,
    2021.

    - <span id="page-14-6"></span>[59] H. Hassan, G. Pekhimenko, N. Vijaykumar, V.
    Seshadri, D. Lee, O. Ergin, and O. Mutlu, "ChargeCache: Reducing DRAM Latency
    by Exploiting Row Access Locality," in HPCA, 2016.

    - <span id="page-14-44"></span>[60] A. A. Hwang, I. A. Stefanovici, and B. Schroeder,
    "Cosmic Rays Don''t Strike Twice: Understanding the Nature of DRAM Errors and
    the Implications for System Design," in ASPLOS, 2012.

    - <span id="page-14-26"></span>[61] S. M. Jafri, H. Hassan, A. Hemani, and O.
    Mutlu, "Refresh Triggered Computation: Improving the Energy Efficiency of Convolutional
    Neural Network Accelerators," TACO, 2020.

    - <span id="page-14-36"></span>[62] X. Jian and R. Kumar, "Adaptive Reliability
    Chipkill Correct (ARCC)," in HPCA, 2013.

    - <span id="page-14-37"></span>[63] X. Jian, H. Duwe, J. Sartori, V. Sridharan,
    and R. Kumar, "Low-Power, Low-Storage-Overhead Chipkill Correct via Multi-Line
    Error Correction," in SC, 2013.

    - <span id="page-14-27"></span>[64] Y. Katayama, E. J. Stuckey, S. Morioka, and
    Z. Wu, "Fault-Tolerant Refresh Power Reduction of DRAMs for Quasi-Nonvolatile
    Data Retention," in EFT, 1999.

    - <span id="page-14-31"></span>[65] S. Khan, D. Lee, Y. Kim, A. R. Alameldeen,
    C. Wilkerson, and O. Mutlu, "The Efficacy of Error Mitigation Techniques for DRAM
    Retention Failures: A Comparative Experimental Study," in SIGMETRICS, 2014.

    - <span id="page-14-47"></span>[66] S. Khan, C. Wilkerson, D. Lee, A. R. Alameldeen,
    and O. Mutlu, "A Case for Memory Content-Based Detection and Mitigation of Data-Dependent
    Failures in DRAM," in IEEE CAL, 2016.

    - <span id="page-14-24"></span>[67] S. Khan, D. Lee, and O. Mutlu, "PARBOR: An
    Efficient System-Level Technique to Detect Data-Dependent Failures in DRAM," in
    DSN, 2016.

    - <span id="page-14-23"></span>[68] S. Khan, C. Wilkerson, Z. Wang, A. R. Alameldeen,
    D. Lee, and O. Mutlu, "Detecting and Mitigating Data-Dependent DRAM Failures by
    Exploiting Current Memory Content," in MICRO, 2017.

    - <span id="page-14-8"></span>[69] J. Kim and M. C. Papaefthymiou, "Dynamic Memory
    Design for Low Data-Retention Power," in PATMOS, 2000.

    - <span id="page-14-28"></span>[70] J. Kim and M. C. Papaefthymiou, "Block-Based
    Multiperiod Dynamic Memory Design for Low Data-Retention Power," in TVLSI, 2003.

    - <span id="page-14-38"></span>[71] J. Kim, M. Sullivan, and M. Erez, "Bamboo
    ECC: Strong, Safe, and Flexible Codes For Reliable Computer Memory," in HPCA,
    2015.

    - <span id="page-14-39"></span>[72] J. Kim, M. Sullivan, S.-L. Gong, and M. Erez,
    "Frugal ECC: Efficient And Versatile Memory Error Protection Through Fine-Grained
    Compression," in SC, 2015.

    - <span id="page-14-0"></span>[73] J. S. Kim, M. Patel, H. Hassan, and O. Mutlu,
    "The DRAM Latency PUF: Quickly Evaluating Physical Unclonable Functions by Exploiting
    the Latency-Reliability Tradeoff in Modern Commodity DRAM Devices," in HPCA, 2018.

    - <span id="page-14-1"></span>[74] J. S. Kim, M. Patel, H. Hassan, and O. Mutlu,
    "Solar-DRAM: Reducing DRAM Access Latency by Exploiting the Variation in Local
    Bitlines," in ICCD, 2018.

    - <span id="page-14-18"></span>[75] J. S. Kim, M. Patel, H. Hassan, L. Orosa,
    and O. Mutlu, "D-RaNGe: Using Commodity DRAM Devices to Generate True Random Numbers
    With Low Latency And High Throughput," in HPCA, 2019.

    - <span id="page-14-34"></span>[76] R. K. Konoth, M. Oliverio, A. Tatar, D. Andriesse,
    H. Bos, C. Giuffrida, and K. Razavi, "ZebRAM: Comprehensive and Compatible Software
    Protection Against Rowhammer Attacks," in OSDI, 2018.

    - <span id="page-14-5"></span>[77] S. Koppula, L. Orosa, A. G. Yaglık ˘ c¸ı, R.
    Azizi, T. Shahroodi, K. Kanellopoulos, and O. Mutlu, "EDEN: Enabling Energy-Efficient,
    High-Performance Deep Neural Network Inference Using Approximate DRAM," in MICRO,
    2019.

    - <span id="page-14-2"></span>[78] D. Lee, S. Khan, L. Subramanian, S. Ghose,
    R. Ausavarungnirun, G. Pekhimenko, V. Seshadri, and O. Mutlu, "Design-Induced
    Latency Variation in Modern DRAM Chips: Characterization, Analysis, and Latency
    Reduction Mechanisms," in SIGMETRICS, 2017.

    - <span id="page-14-29"></span>[79] C. H. Lin, D.-Y. Shen, Y.-J. Chen, C.-L. Yang,
    and M. Wang, "SECRET: Selective Error Correction for Refresh Energy Reduction
    in DRAMs," in ICCD, 2012.

    - <span id="page-14-9"></span>[80] J. Liu, B. Jaiyen, R. Veras, and O. Mutlu,
    "RAIDR: Retention-Aware Intelligent DRAM Refresh," in ISCA, 2012.

    - <span id="page-14-40"></span>[81] E. Manzhosov, A. Hastings, M. Pancholi, R.
    Piersma, M. T. I. Ziad, and S. Sethumadhavan, "MUSE: Multi-Use Error Correcting
    Codes," arXiv:2107.09245, 2021.

    - <span id="page-14-7"></span>[82] D. M. Mathew, E. F. Zulian, M. Jung, K. Kraft,
    C. Weis, B. Jacob, and ´ N. Wehn, "Using Run-Time Reverse-Engineering to Optimize
    DRAM Refresh," in MEMSYS, 2017.

    - <span id="page-14-45"></span>[83] Bad Page Offlining, mcelog, 2021, [https://mcelog.org/badpageofflini](https://mcelog.org/badpageofflining.html)
    [ng.html.](https://mcelog.org/badpageofflining.html)

    - <span id="page-14-13"></span>[84] J. Meza, Q. Wu, S. Kumar, and O. Mutlu, "Revisiting
    Memory Errors in Large-Scale Production Data Centers: Analysis and Modeling of
    New Trends from the Field," in DSN, 2015.

    - <span id="page-14-33"></span>[85] O. Mutlu, "RowHammer," [https://people.inf.ethz.ch/omutlu/pub/](https://people.inf.ethz.ch/omutlu/pub/onur-Rowhammer-TopPicksinHardwareEmbeddedSecurity-November-8-2018.pdf)
    [onur-Rowhammer-TopPicksinHardwareEmbeddedSecurity-N](https://people.inf.ethz.ch/omutlu/pub/onur-Rowhammer-TopPicksinHardwareEmbeddedSecurity-November-8-2018.pdf)
    [ovember-8-2018.pdf,](https://people.inf.ethz.ch/omutlu/pub/onur-Rowhammer-TopPicksinHardwareEmbeddedSecurity-November-8-2018.pdf)
    2018, Top Picks in Hardware and Embedded Security.

    - <span id="page-14-3"></span>[86] P. J. Nair, D.-H. Kim, and M. K. Qureshi, "ArchShield:
    Architectural Framework for Assisting DRAM Scaling by Tolerating High Error Rates,"
    in ISCA, 2013.

    - <span id="page-14-14"></span>[87] P. J. Nair, V. Sridharan, and M. K. Qureshi,
    "XED: Exposing On-Die Error Detection Information for Strong Memory Reliability,"
    in ISCA, 2016.

    - <span id="page-14-46"></span>[88] Dynamic Page Retirement, NVIDIA, 2020, [https://docs.nvidia.com/de](https://docs.nvidia.com/deploy/dynamic-page-retirement/index.html)
    [ploy/dynamic-page-retirement/index.html.](https://docs.nvidia.com/deploy/dynamic-page-retirement/index.html)

    - <span id="page-14-10"></span>[89] T. Ohsawa, K. Kai, and K. Murakami, "Optimizing
    the DRAM Refresh Count for Merged DRAM/logic LSIs," in ISLPED, 1998.

    - <span id="page-14-19"></span>[90] A. Olgun, J. G. Luna, K. Kanellopoulos, B.
    Salami, H. Hassan, O. Ergin, and O. Mutlu, "PiDRAM: A Holistic End-to-end FPGA-based
    Framework for Processing-in-DRAM," arXiv:2111.00082, 2021.

    - <span id="page-14-20"></span>[91] A. Olgun, M. Patel, A. G. Yaglık ˘ c¸ı, H.
    Luo, J. S. Kim, N. Bostancı, N. Vijaykumar, O. Ergin, and O. Mutlu, "QUAC-TRNG:
    High-Throughput True Random Number Generation Using Quadruple Row Activation in
    Commodity DRAM Chips," in ISCA, 2021.

    - <span id="page-14-4"></span>[92] M. Patel, J. S. Kim, and O. Mutlu, "The Reach
    Profiler (REAPER): Enabling the Mitigation of DRAM Retention Failures via Profiling
    at Aggressive Conditions," in ISCA, 2017.

    - <span id="page-14-41"></span>[93] A. Patil, V. Nagarajan, R. Balasubramonian,
    and N. Oswald, "Dve:´ Improving DRAM Reliability and Performance On-Demand via
    Coherent Replication," in ISCA, 2021.

    - <span id="page-14-12"></span>[94] M. K. Qureshi, D.-H. Kim, S. Khan, P. J. Nair,
    and O. Mutlu, "AVATAR: A Variable-Retention-Time (VRT) Aware Refresh for DRAM
    Systems," in DSN, 2015.

    - <span id="page-14-35"></span>[95] G. Saileshwar, B. Wang, M. Qureshi, and P.
    J. Nair, "Randomized Row-Swap: Mitigating Row Hammer by Breaking Spatial Correlation
    Between Aggressor and Victim Rows," in ASPLOS, 2022.

    - <span id="page-14-21"></span>[96] V. Seshadri, Y. Kim, C. Fallin, D. Lee, R.
    Ausavarungnirun, G. Pekhimenko, Y. Luo, O. Mutlu, P. B. Gibbons, M. A. Kozuch,
    and T. C. Mowry, "RowClone: Fast and Energy-Efficient In-DRAM Bulk Data Copy and
    Initialization," in MICRO, 2013.

    - [97] V. Seshadri, K. Hsieh, A. Boroum, D. Lee, M. A. Kozuch, O. Mutlu, P. B.
    Gibbons, and T. C. Mowry, "Fast Bulk Bitwise AND and OR in DRAM," IEEE CAL, 2015.

    - [98] V. Seshadri, D. Lee, T. Mullins, H. Hassan, A. Boroumand, J. Kim, M. A.
    Kozuch, O. Mutlu, P. B. Gibbons, and T. C. Mowry, "Buddy-RAM: Improving the Performance
    and Efficiency of Bulk Bitwise Operations Using DRAM," in arXiv, 2016.

    - [99] V. Seshadri, D. Lee, T. Mullins, H. Hassan, A. Boroumand, J. Kim, M. A.
    Kozuch, O. Mutlu, P. B. Gibbons, and T. C. Mowry, "Ambit: In-Memory Accelerator
    for Bulk Bitwise Operations Using Commodity DRAM Technology," in MICRO, 2017.

    - <span id="page-14-22"></span>[100] V. Seshadri and O. Mutlu, "In-DRAM Bulk Bitwise
    Execution Engine," arXiv:1905.09822, 2019.

    - <span id="page-14-42"></span>[101] A. N. Udipi, N. Muralimanohar, R. Balsubramonian,
    A. Davis, and N. P. Jouppi, "LOT-ECC: Localized And Tiered Reliability Mechanisms
    For Commodity Memory Systems," in ISCA, 2012.

    - <span id="page-14-15"></span>[102] V. van der Veen, M. Lindorfer, Y. Fratantonio,
    H. P. Pillai, G. Vigna, C. Kruegel, H. Bos, and K. Razavi, "GuardION: Practical
    Mitigation of DMA-Based Rowhammer Attacks on ARM," in DIMVA, 2018.

    - <span id="page-14-30"></span>[103] R. K. Venkatesan, S. Herr, and E. Rotenberg,
    "Retention-Aware Placement in DRAM (RAPID): Software Methods for Quasi-Non-Volatile
    DRAM," in HPCA, 2006.

    - <span id="page-14-11"></span>[104] J. Wang, X. Dong, and Y. Xie, "ProactiveDRAM:
    A DRAM-Initiated Retention Management Scheme," in ICCD, 2014.

    - <span id="page-14-43"></span>[105] S. Wang, M. N. Bojnordi, X. Guo, and E. Ipek,
    "Content Aware Refresh: Exploiting the Asymmetry of DRAM Retention Errors to Reduce
    the Refresh Frequency of Less Vulnerable Data," TOC, 2018.

    - <span id="page-15-38"></span>[106] Y. Wang, A. Tavakkol, L. Orosa, S. Ghose,
    N. M. Ghiasi, M. Patel, J. S. Kim, H. Hassan, M. Sadrosadati, and O. Mutlu, "Reducing
    DRAM Latency Via Charge-Level-Aware Look-Ahead Partial Restoration," in MICRO,
    2018.

    - <span id="page-15-42"></span>[107] A. G. Yaglık ˘ c¸ı, M. Patel, J. S. Kim,
    R. Azizibarzoki, A. Olgun, L. Orosa, H. Hassan, J. Park, K. Kanellopoullos, T.
    Shahroodi, S. Ghose, and O. Mutlu, "BlockHammer: Preventing RowHammer at Low Cost
    by Blacklisting Rapidly-Accessed DRAM Rows," in HPCA, 2021.

    - <span id="page-15-43"></span>[108] D. H. Yoon and M. Erez, "Virtualized and
    Flexible ECC for Main Memory," in ASPLOS, 2010.

    - <span id="page-15-39"></span>[109] X. Zhang, Y. Zhang, B. R. Childers, and J.
    Yang, "Restore Truncation for Performance Improvement in Future DRAM Systems,"
    in HPCA, 2016.

    - <span id="page-15-0"></span>[110] D. Zhang, G. Panwar, J. B. Kotra, N. DeBardeleben,
    S. Blanchard, and X. Jian, "Quantifying Server Memory Frequency Margin and Using
    it to Improve Performance in HPC Systems," in ISCA, 2021.

    - <span id="page-15-1"></span>[111] O. Mutlu, "Memory Scaling: A Systems Architecture
    Perspective," in IMW, 2013.

    - <span id="page-15-37"></span>[112] O. Mutlu and L. Subramanian, "Research Problems
    and Opportunities in Memory Systems," in SUPERFRI, 2014.

    - <span id="page-15-2"></span>[113] K. Loughlin, S. Saroiu, A. Wolman, and B.
    Kasikci, "Software-Defined Memory Controllers: An Idea Whose Time Has Come," WACI,
    2022.

    - <span id="page-15-3"></span>[114] M. Jung, C. C. Rheinlander, C. Weis, and N.
    Wehn, "Reverse En- ¨ gineering of DRAMs: Row Hammer with Crosshair," in MEMSYS,
    2016.

    - <span id="page-15-40"></span>[115] M. Patel, J. S. Kim, H. Hassan, and O. Mutlu,
    "Understanding and Modeling On-Die Error Correction in Modern DRAM: An Experimental
    Study Using Real Devices," in DSN, 2019.

    - <span id="page-15-46"></span>[116] L. Mukhanov, D. S. Nikolopoulos, and G. Karakonstantis,
    "DStress: Automatic Synthesis of DRAM Reliability Stress Viruses using Genetic
    Algorithms," in MICRO, 2020.

    - <span id="page-15-26"></span>[117] K. Kraft, C. Sudarshan, D. M. Mathew, C.
    Weis, N. Wehn, and M. Jung, "Improving the Error Behavior of DRAM by Exploiting
    its Z-Channel Property," in DATE, 2018.

    - <span id="page-15-4"></span>[118] L. Orosa, A. G. Yaglık ˘ c¸ı, H. Luo, A. Olgun,
    J. Park, H. Hassan, M. Patel, J. S. Kim, and O. Mutlu, "A Deeper Look into RowHammer''s
    Sensitivities: Experiemental Analysis of Real DRAM Chips and Implications on Future
    Attacks and Defenses," in MICRO, 2021.

    - <span id="page-15-5"></span>[119] K. Criss, K. Bains, R. Agarwal, T. Bennett,
    T. Grunzke, J. K. Kim, H. Chung, and M. Jang, "Improving Memory Reliability by
    Bounding DRAM Faults: DDR5 Improved Reliability Features," in MEMSYS, 2020.

    - <span id="page-15-6"></span>[120] JEDEC, JEP122H: Failure Mechanisms and Models
    for Semiconductor Devices, 2016.

    - <span id="page-15-7"></span>[121] JEDEC, JESD218: Solid-State Drive (SSD) Requirements
    and Endurance Test Method, 2010.

    - <span id="page-15-9"></span><span id="page-15-8"></span>[122] JEDEC, JESD219:
    Solid-State Drive (SSD) Endurance Workloads, 2010.

    - [123] Y. Cai, S. Ghose, E. F. Haratsch, Y. Luo, and O. Mutlu, "Error Characterization,
    Mitigation, and Recovery In Flash-Memory-Based Solid-State Drives," Proc. IEEE,
    2017.

    - [124] Y. Cai, E. F. Haratsch, O. Mutlu, and K. Mai, "Error Patterns in MLC NAND
    Flash Memory: Measurement, Characterization, and Analysis," in DATE, 2012.

    - <span id="page-15-10"></span>[125] Y. Cai, S. Ghose, E. F. Haratsch, Y. Luo,
    and O. Mutlu, "Errors in Flash-Memory-Based Solid-State Drives: Analysis, Mitigation,
    and Recovery," Inside Solid State Drives, 2018.

    - <span id="page-15-11"></span>[126] Y. Cai, Y. Luo, E. F. Haratsch, K. Mai, and
    O. Mutlu, "Data Retention in MLC NAND Flash Memory: Characterization, Optimization,
    and Recovery," in HPCA, 2015.

    - [127] Y. Luo, S. Ghose, Y. Cai, E. F. Haratsch, and O. Mutlu, "HeatWatch: Improving
    3D NAND Flash Memory Device Reliability by Exploiting Self-Recovery and Temperature
    Awareness," in HPCA, 2018.

    - [128] Y. Luo, S. Ghose, Y. Cai, E. F. Haratsch, and O. Mutlu, "Improving 3D
    NAND Flash Memory Lifetime by Tolerating Early Retention Loss and Process Variation,"
    SIGMETRICS, 2018.

    - <span id="page-15-12"></span>[129] Y. Cai, G. Yalcin, O. Mutlu, E. F. Haratsch,
    A. Cristal, O. S. Unsal, and K. Mai, "Flash Correct-And-Refresh: Retention-Aware
    Error Management for Increased Flash Memory Lifetime," in ICCD, 2012.

    - <span id="page-15-13"></span>[130] Y. Cai, E. F. Haratsch, O. Mutlu, and K.
    Mai, "Threshold Voltage Distribution in MLC NAND Flash Memory: Characterization,
    Analysis, and Modeling," in DATE, 2013.

    - [131] Y. Cai, O. Mutlu, E. F. Haratsch, and K. Mai, "Program Interference in
    MLC NAND Flash Memory: Characterization, Modeling, and Mitigation," in ICCD, 2013.

    - [132] Y. Cai, Y. Luo, S. Ghose, and O. Mutlu, "Read Disturb Errors in MLC NAND
    Flash Memory: Characterization, Mitigation, and Recovery," in DSN, 2015.

    - <span id="page-15-47"></span>[133] Y. Luo, S. Ghose, Y. Cai, E. F. Haratsch,
    and O. Mutlu, "Enabling Accurate and Practical Online Flash Channel Modeling for
    Modern MLC NAND Flash Memory," in JSAC, 2016.

    - [134] Y. Cai, S. Ghose, Y. Luo, K. Mai, O. Mutlu, and E. F. Haratsch, "Vulnerabilities
    in MLC NAND Flash Memory Programming: Experimental Analysis, Exploits, and Mitigation
    Techniques," in HPCA, 2017.

    - <span id="page-15-14"></span>[135] Y. Cai, G. Yalcin, O. Mutlu, E. F. Haratsch,
    O. Unsal, A. Cristal, and K. Mai, "Neighbor-Cell Assisted Error Correction for
    MLC NAND Flash Memories," in SIGMETRICS, 2014.

    - <span id="page-15-15"></span>[136] "DRAM Datasheet Survey," [https://github.com/CMU-SAFARI/DR](https://github.com/CMU-SAFARI/DRAM-Datasheet-Survey)
    [AM-Datasheet-Survey.](https://github.com/CMU-SAFARI/DRAM-Datasheet-Survey)

    - <span id="page-15-16"></span>[137] JEDEC, "JC-42 Solid State Memories," [https://www.jedec.org/commit](https://www.jedec.org/committees/jc-42)
    [tees/jc-42.](https://www.jedec.org/committees/jc-42)

    - <span id="page-15-17"></span>[138] JEDEC, DDR3 SDRAM Specification, 2008.

    - <span id="page-15-18"></span>[139] JEDEC, "Low Power Double Data Rate 4 (LPDDR4)
    SDRAM Specification," JEDEC Standard JESD209–4B, 2014.

    - <span id="page-15-19"></span>[140] JEDEC, "Graphics Double Data Rate (GDDR5)
    SGRAM Standard," JEDEC Standard JESD212C, 2016.

    - <span id="page-15-20"></span>[141] JEDEC, "Graphics Double Data Rate (GDDR6)
    SGRAM Standard," JEDEC Standard JESD250C, 2021.

    - <span id="page-15-21"></span>[142] M. V. Beigi, Y. Cao, S. Gurumurthi, C. Recchia,
    A. Walton, and V. Sridharan, "A Systematic Study of DDR4 DRAM Faults in the Field,"
    in HPCA, 2023.

    - <span id="page-15-22"></span>[143] S.-L. Gong, J. Kim, and M. Erez, "DRAM Scaling
    Error Evaluation Model Using Various Retention Time," in DSN-W, 2017.

    - [144] B. R. Childers, J. Yang, and Y. Zhang, "Achieving Yield, Density and Performance
    Effective DRAM at Extreme Technology Sizes," in MEMSYS, 2015.

    - <span id="page-15-23"></span>[145] L. Peters, D. Potter, and R. Bowman, Cost
    Effective IC Manufacturing, 1998–1999. Integrated Circuit Engineering Corporation,
    1997.

    - <span id="page-15-24"></span>[146] J. Kang, "A Study of the DRAM Industry,"
    Master''s thesis, Massachusetts Institute of Technology, 2010.

    - [147] T. J. Dell, "A White Paper on the Benefits of Chipkill-Correct ECC for
    PC Server Main Memory," IBM Microelectronics Division, 1997.

    - [148] K. H. Lee, "A Strategic Analysis of the DRAM Industry After the Year 2000,"
    Master''s thesis, Massachusetts Institute of Technology, 2013.

    - <span id="page-15-25"></span>[149] J. A. Croswell, "A Model for Analysis of
    the Effects of Redundancy and Error Correction on DRAM Memory Yield and Reliability,"
    Master''s thesis, MIT, 2000.

    - <span id="page-15-27"></span>[150] J. Liu, B. Jaiyen, Y. Kim, C. Wilkerson,
    and O. Mutlu, "An Experimental Study of Data Retention Behavior in Modern DRAM
    Devices: Implications for Retention Time Profiling Mechanisms," in ISCA, 2013.

    - <span id="page-15-44"></span>[151] A. Barenghi, L. Breveglieri, N. Izzo, and
    G. Pelosi, "Software-Only Reverse Engineering of Physical DRAM Mappings For RowHammer
    Attacks," in IVSW, 2018.

    - <span id="page-15-41"></span>[152] M. Farmani, M. Tehranipoor, and F. Rahman,
    "RHAT: Efficient RowHammer-Aware Test for Modern DRAM Modules," in ETS, 2021.

    - <span id="page-15-45"></span>[153] M. Wang, Z. Zhang, Y. Cheng, and S. Nepal,
    "Dramdig: A Knowledge-Assisted Tool To Uncover DRAM Address Mapping," in DAC,
    2020.

    - [154] Y. Jiang, H. Zhu, H. Shan, X. Guo, X. Zhang, and Y. Jin, "TRRScope: Understanding
    Target Row Refresh Mechanism for Modern DDR Protection," in HOST, 2021.

    - <span id="page-15-28"></span>[155] H. Nam, S. Baek, M. Wi, M. J. Kim, J. Park,
    C. Song, N. S. Kim, and J. H. Ahn, "X-ray: Discovering DRAM Internal Structure
    and Error Characteristics by Issuing Memory Commands," IEEE CAL, 2023.

    - <span id="page-15-29"></span>[156] D. James, "Silicon Chip Teardown to the Atomic
    Scale–Challenges Facing the Reverse Engineering of Semiconductors," Microscopy
    and Microanalysis, 2010.

    - <span id="page-15-30"></span>[157] R. Torrance and D. James, "The State-of-the-Art
    in IC Reverse Engineering," in CHES, 2009.

    - <span id="page-15-31"></span>[158] Rugged Memory, SMART Modular Technologies,
    2021, [https://www.](https://www.smartm.com/product/rugged-memory) [smartm.com/product/rugged-memory.](https://www.smartm.com/product/rugged-memory)

    - <span id="page-15-32"></span>[159] Intelligent Memory, "IM ECC DRAM with Integrated
    Error Correcting Code," 2016, Product Brief.

    - <span id="page-15-33"></span>[160] RLDRAM Memory, Micron Technology, 2021, [https://www.micron.c](https://www.micron.com/products/dram/rldram-memory)
    [om/products/dram/rldram-memory.](https://www.micron.com/products/dram/rldram-memory)

    - <span id="page-15-34"></span>[161] J. Macri, "AMD''s Next Generation GPU and
    High Bandwidth Memory Architecture: FURY," in HCS, 2015.

    - <span id="page-15-35"></span>[162] Intel Corporation, "RLDRAM II and RLDRAM
    3 Features," 2023, [https:](https://www.intel.com/content/www/us/en/docs/programmable/710283/17-0/rldram-ii-and-rldram-3-features.html)
    [//www.intel.com/content/www/us/en/docs/programmable/710283](https://www.intel.com/content/www/us/en/docs/programmable/710283/17-0/rldram-ii-and-rldram-3-features.html)
    [/17-0/rldram-ii-and-rldram-3-features.html.](https://www.intel.com/content/www/us/en/docs/programmable/710283/17-0/rldram-ii-and-rldram-3-features.html)

    - <span id="page-15-36"></span>[163] J. Church, "Strategic Networking in Standard
    Setting Organizations: The Case of JEDEC," Master''s thesis, Cornell University,
    2007.

    - <span id="page-16-0"></span>[164] JEDEC, "Addendum No. 1 to JESD209-4, Low Power
    Double Data Rate 4x (LPDDR4X)," JEDEC Standard JESD209–4–1A, 2021.

    - <span id="page-16-1"></span>[165] JEDEC, "Addendum No. 1 to JESD79-4, 3D Stacked
    DRAM," JEDEC Standard JESD79-4-1B, 2021.

    - <span id="page-16-2"></span>[166] U. Kang, H.-s. Yu, C. Park, H. Zheng, J. Halbert,
    K. Bains, S. Jang, and J. S. Choi, "Co-Architecting Controllers and DRAM to Enhance
    DRAM Process Scaling," in The Memory Forum, 2014.

    - <span id="page-16-23"></span>[167] S. Cha, O. Seongil, H. Shin, S. Hwang, K.
    Park, S. J. Jang, J. S. Choi, G. Y. Jin, Y. H. Son, H. Cho, J. H. Ahn, and N.
    S. Kim, "Defect Analysis and Cost-Effective Resilience Architecture for Future
    DRAM Devices," in HPCA, 2017.

    - <span id="page-16-26"></span>[168] Micron Technology Inc., "ECC Brings Reliability
    and Power Efficiency to Mobile Devices," Micron Technology Inc., Tech. Rep., 2017.

    - [169] S.-K. Park, "Technology Scaling Challenge and Future Prospects of DRAM
    and NAND Flash Memory," in IMW, 2015.

    - <span id="page-16-3"></span>[170] Micron Technologies, Inc., "Quarterly Report
    on Form 10-Q," [https:](https://investors.micron.com/quarterly-results) [//investors.micron.com/quarterly-results,](https://investors.micron.com/quarterly-results)
    2022.

    - <span id="page-16-4"></span>[171] S. Ghose, A. G. Yaglık ˘ c¸ı, R. Gupta, D.
    Lee, K. Kudrolli, W. X. Liu, H. Hassan, K. K. Chang, N. Chatterjee, A. Agrawal,
    M. O''Connor, and O. Mutlu, "What Your DRAM Power Models Are Not Telling You:
    Lessons from a Detailed Experimental Study," SIGMETRICS, 2018.

    - <span id="page-16-5"></span>[172] K. Akarvardar and H.-S. P. Wong, "Technology
    Prospects for Data-Intensive Computing," Proc. IEEE, 2023.

    - <span id="page-16-6"></span>[173] B. Van Essen, R. Pearce, S. Ames, and M. Gokhale,
    "On the Role of NVRAM in Data-Intensive Architectures: An Evaluation," in IPDPS,
    2012.

    - [174] J. Ousterhout, P. Agrawal, D. Erickson, C. Kozyrakis, J. Leverich, D.
    Mazieres, S. Mitra, A. Narayanan, G. Parulkar, M. Rosenblum ` et al., "The Case
    for RAMClouds: Scalable High-Performance Storage Entirely in DRAM," SIGOPS OSR,
    2010.

    - [175] S. Lee, G.-J. Kim, N. Lee, K. Lee, B. Woo, J. Jin, J. Kim, Y. Lee, H.
    Kim, and S. Pae, "Reliability Characterization for Advanced DRAM using HK/MG+
    EUV Process Technology," in IEDM, 2021.

    - [176] X. Sun, H. Wan, Q. Li, C.-L. Yang, T.-W. Kuo, and C. J. Xue, "RM-SSD:
    In-Storage Computing for Large-Scale Recommendation Inference," in HPCA, 2022.

    - <span id="page-16-7"></span>[177] S. Park, H. Kim, K. Kim, J. So, J. Ahn, W.
    Lee, D. Kim, Y. Kim, J. Seok, J. Lee et al., "Scaling of Memory Performance and
    Capacity with CXL Memory Expander," in HCS, 2022.

    - <span id="page-16-8"></span>[178] B. Gervasi, "Will Carbon Nanotube Memory Replace
    DRAM?" IEEE Micro, 2019.

    - <span id="page-16-9"></span>[179] K. Hsieh, S. Khan, N. Vijaykumar, K. K. Chang,
    A. Boroumand, S. Ghose, and O. Mutlu, "Accelerating Pointer Chasing In 3D-Stacked
    Memory: Challenges, Mechanisms, Evaluation," in ICCD, 2016.

    - [180] M. Ferdman, A. Adileh, O. Kocberber, S. Volos, M. Alisafaee, D. Jevdjic,
    C. Kaynak, A. D. Popescu, A. Ailamaki, and B. Falsafi, "Clearing The Clouds: A
    Study Of Emerging Scale-Out Workloads On Modern Hardware," ASPLOS, 2012.

    - [181] A. Gutierrez, R. G. Dreslinski, T. F. Wenisch, T. Mudge, A. Saidi, C.
    Emmons, and N. Paver, "Full-System Analysis And Characterization Of Interactive
    Smartphone Applications," in IISWC, 2011.

    - [182] J. Hestness, S. W. Keckler, and D. A. Wood, "A Comparative Analysis Of
    Microarchitecture Effects On CPU and GPU Memory System Behavior," in IISWC, 2014.

    - [183] Y. Huang, Z. Zha, M. Chen, and L. Zhang, "Moby: A Mobile Benchmark Suite
    For Architectural Simulators," in ISPASS, 2014.

    - [184] Y. Zhu, D. Richins, M. Halpern, and V. J. Reddi, "Microarchitectural Implications
    Of Event-Driven Server-Side Web Applications," in MI-CRO, 2015.

    - <span id="page-16-28"></span>[185] G. F. Oliveira, J. Gomez-Luna, S. Ghose,
    L. Orosa, N. Vijaykumar, ´ I. Fernandez, M. Sadrosadati, and O. Mutlu, "DAMOV:
    A New Methodology and Benchmark Suite for Evaluating Data Movement Bottlenecks,"
    in IEEE Access, 2021.

    - [186] A. Boroumand, S. Ghose, Y. Kim, R. Ausavarungnirun, E. Shiu, R. Thakur,
    D. Kim, A. Kuusela, A. Knies, P. Ranganathan, and O. Mutlu, "Google Workloads
    for Consumer Devices: Mitigating Data Movement Bottlenecks," in ASPLOS, 2018.

    - [187] A. Boroumand, S. Ghose, B. Akin, R. Narayanaswami, G. F. Oliveira, X.
    Ma, E. Shiu, and O. Mutlu, "Google Neural Network Models for Edge Devices: Analyzing
    and Mitigating Machine Learning Inference Bottlenecks," in PACT, 2021.

    - [188] K. Kanellopoulos, N. Vijaykumar, C. Giannoula, R. Azizi, S. Koppula, N.
    M. Ghiasi, T. Shahroodi, J. G. Luna, and O. Mutlu, "SMASH: Co-Designing Software
    Compression and Hardware-Accelerated Indexing for Efficient Sparse Matrix Operations,"
    in MICRO, 2019.

    - [189] M. V. Wilkes, "The Memory Gap and The Future of High Performance Memories,"
    SIGARCH Computer Architecture News, 2001.

    - [190] W. A. Wulf and S. A. McKee, "Hitting the Memory Wall: Implications of
    the Obvious," SIGARCH Computer Architecture News, 1995.

    - [191] O. Mutlu and T. Moscibroda, "Stall-Time Fair Memory Access Scheduling
    for Chip Multiprocessors," in MICRO, 2007.

    - [192] O. Mutlu, J. Stark, C. Wilkerson, and Y. N. Patt, "Runahead Execution:
    An Alternative to Very Large Instruction Windows for Out-of-Order Processors,"
    in HPCA, 2003.

    - [193] S. Kanev, J. P. Darago, K. Hazelwood, P. Ranganathan, T. Moseley, G.-Y.
    Wei, and D. Brooks, "Profiling a Warehouse-scale Computer," in ISCA, 2015.

    - [194] R. Bera, A. V. Nori, O. Mutlu, and S. Subramoney, "DSPatch: Dual Spatial
    Pattern Prefetcher," in MICRO, 2019.

    - [195] R. Bera, K. Kanellopoulos, A. Nori, T. Shahroodi, S. Subramoney, and O.
    Mutlu, "Pythia: A Customizable Hardware Prefetching Framework using Online Reinforcement
    Learning," in MICRO, 2021.

    - [196] X. Liu, D. Roberts, R. Ausavarungnirun, O. Mutlu, and J. Zhao, "Binary
    Star: Coordinated Reliability in Heterogeneous Memory Systems for High Performance
    and Scalability," in MICRO, 2019.

    - [197] S. Ghose, A. Boroumand, J. S. Kim, J. Gomez-Luna, and O. Mutlu, ´ "Processing-in-Memory:
    A Workload-driven Perspective," IBM JRD, 2019.

    - <span id="page-16-24"></span>[198] W. Shin, J. Yang, J. Choi, and L.-S. Kim,
    "NUAT: A Non-Uniform Access Time Memory Controller," in HPCA, 2014.

    - [199] S. Ghose, T. Li, N. Hajinazar, D. S. Cali, and O. Mutlu, "Demystifying
    Complex Workload-DRAM Interactions: An Experimental Study," SIGMETRICS, 2019.

    - <span id="page-16-29"></span>[200] J. Gomez-Luna, J. El Hajj, I. Fernandez,
    and C. Giannoula, "Bench- ´ marking a New Paradigm: Understanding a Modern Processing-in-Memory
    Architecture," in arXiv:2105.03814, 2021.

    - <span id="page-16-30"></span>[201] J. Gomez-Luna, I. El Hajj, I. Fernandez,
    C. Giannoula, G. F. Oliveira, ´ and O. Mutlu, "Benchmarking Memory-Centric Computing
    Systems: Analysis of Real Processing-in-Memory Hardware," in CUT, 2021.

    - <span id="page-16-10"></span>[202] C. Giannoula, I. Fernandez, J. Gomez-Luna,
    N. Koziris, G. Goumas, ´ and O. Mutlu, "Towards Efficient Sparse Matrix Vector
    Multiplication on Real Processing-in-Memory Architectures," SIGMETRICS, 2022.

    - <span id="page-16-11"></span>[203] J. S. Kim, "Improving DRAM Performance, Security,
    and Reliability by Understanding and Exploiting DRAM Timing Parameter Margins,"
    Ph.D. dissertation, Carnegie Mellon University, 2020.

    - <span id="page-16-12"></span>[204] D. Lee, S. Ghose, G. Pekhimenko, S. Khan,
    and O. Mutlu, "Simultaneous Multi-Layer Access: Improving 3D-Stacked Memory Bandwidth
    at Low Cost," TACO, 2016.

    - <span id="page-16-13"></span>[205] J. Laudon, "UltraSPARC T1: Architecture and
    Physical Design of a 32-threaded General Purpose CPU," in ISSCC, 2006.

    - <span id="page-16-19"></span>[206] I. Bhati, Z. Chishti, S.-L. Lu, and B. Jacob,
    "Flexible Auto-Refresh: Enabling Scalable and Energy-Efficient DRAM Refresh Reductions,"
    in ISCA, 2015.

    - <span id="page-16-14"></span>[207] I. Bhati, M.-T. Chang, Z. Chishti, S.-L.
    Lu, and B. Jacob, "DRAM Refresh Mechanisms, Penalties, and Trade-Offs," in TC,
    2016.

    - <span id="page-16-15"></span>[208] T. Zhang, M. Poremba, C. Xu, G. Sun, and
    Y. Xie, "CREAM: A Concurrent-Refresh-Aware DRAM Memory Architecture," in HPCA,
    2014.

    - <span id="page-16-18"></span>[209] P. J. Nair, C.-C. Chou, and M. K. Qureshi,
    "Refresh Pausing in DRAM Memory Systems," in TACO, 2014.

    - <span id="page-16-16"></span>[210] R. Balasubramonian, "A DRAM Refresh Tutorial,"
    [http://utaharch.b](http://utaharch.blogspot.com/2013/11/a-dram-refresh-tutorial.html)
    [logspot.com/2013/11/a-dram-refresh-tutorial.html,](http://utaharch.blogspot.com/2013/11/a-dram-refresh-tutorial.html)
    2013.

    - <span id="page-16-25"></span>[211] J. Stuecheli, D. Kaseridis, H. C. Hunter,
    and L. K. John, "Elastic Refresh: Techniques to Mitigate Refresh Penalties in
    High Density Memory," in MICRO, 2010.

    - <span id="page-16-31"></span>[212] R. Balasubramonian, "Innovations in the Memory
    System," Synthesis Lectures on Computer Architecture, 2019.

    - <span id="page-16-17"></span>[213] W.-K. Cheng, P.-Y. Shen, and X.-L. Li, "Retention-Aware
    DRAM Auto-Refresh Scheme for Energy and Performance Efficiency," Micromachines,
    2019.

    - <span id="page-16-20"></span>[214] P. Nair, C.-C. Chou, and M. K. Qureshi, "A
    Case for Refresh Pausing in DRAM Memory Systems," in HPCA, 2013.

    - <span id="page-16-21"></span>[215] K. K. Chang, D. Lee, Z. Chishti, A. R. Alameldeen,
    C. Wilkerson, Y. Kim, and O. Mutlu, "Improving DRAM Performance by Parallelizing
    Refreshes with Accesses," in HPCA, 2014.

    - <span id="page-16-22"></span>[216] M. Horiguchi and K. Itoh, Nanoscale Memory
    Repair. Springer SBM, 2011.

    - [217] V. Sridharan and D. Liberty, "A Study of DRAM Failures in the Field,"
    in SC, 2012.

    - <span id="page-16-27"></span>[218] B. Schroeder, E. Pinheiro, and W.-D. Weber,
    "DRAM Errors in the Wild: a Large-Scale Field Study," in SIGMETRICS, 2009.

    - [219] T. C. May and M. H. Woods, "Alpha-Particle-Induced Soft Errors in Dynamic
    Memories," TED, 1979.

    - [220] T. J. O''Gorman, J. M. Ross, A. H. Taber, J. F. Ziegler, H. P. Muhlfeld,
    C. J. Montrose, H. W. Curtis, and J. L. Walsh, "Field Testing for Cosmic Ray Soft
    Errors in Semiconductor Memories," IBM JRD, 1996.

    - <span id="page-17-0"></span>[221] V. Sridharan, N. DeBardeleben, S. Blanchard,
    K. B. Ferreira, J. Stearley, J. Shalf, and S. Gurumurthi, "Memory Errors in Modern
    Systems: The Good, the Bad, and the Ugly," in ASPLOS, 2015.

    - <span id="page-17-1"></span>[222] D.-H. Kim and L. S. Milor, "ECC-ASPIRIN: An
    ECC-assisted Post-Package Repair Scheme for Aging Errors in DRAMs," in VTS, 2016.

    - <span id="page-17-2"></span>[223] O. Wada, T. Namekawa, H. Ito, A. Nakayama,
    and S. Fujii, "Post-Packaging Auto Repair Techniques for Fast Row Cycle Embedded
    DRAM," in TEST, 2004.

    - <span id="page-17-3"></span>[224] S. Gurumurthi, K. Lee, M. Jang, V. Sridharan,
    A. Nygren, Y. Ryu, K. Sohn, T. Kim, and H. Chung, "HBM3: Enabling Memory Resilience
    at Scale," IEEE CAL, 2021.

    - <span id="page-17-4"></span>[225] H. Lee, Y. Yoo, S. H. Shin, and S. Kang, "ECMO:
    ECC Architecture Reusing Content-Addressable Memories for Obtaining High Reliability
    in DRAM," TVLSI, 2022.

    - <span id="page-17-5"></span>[226] S. Jeong, S. Kang, and J.-S. Yang, "PAIR:
    Pin-aligned In-DRAM ECC architecture using expandability of Reed-Solomon code,"
    in DAC, 2020.

    - [227] S.-I. Pae, V. Kozhikkottu, D. Somasekar, W. Wu, S. G. Ramasubramanian,
    M. Dadual, H. Cho, and K.-W. Kwon, "Minimal Aliasing Single-Error-Correction Codes
    for DRAM Reliability Improvement," IEEE Access, 2021.

    - [228] Y. Luo, S. Govindan, B. Sharma, M. Santaniello, J. Meza, A. Kansal, J.
    Liu, B. Khessib, K. Vaid, and O. Mutlu, "Characterizing Application Memory Error
    Vulnerability to Optimize Datacenter Cost via Heterogeneous-Reliability Memory,"
    in DSN, 2014.

    - <span id="page-17-7"></span>[229] M. Patel, G. F. de Oliveira, and O. Mutlu,
    "HARP: Practically and Effectively Identifying Uncorrectable Errors in Memory
    Chips That Use On-Die Error-Correcting Codes," in MICRO, 2021.

    - <span id="page-17-6"></span>[230] M. Patel, "Enabling Effective Error Mitigation
    in Memory Chips That Use On-Die Error-Correcting Codes," Ph.D. dissertation, ETH
    Zurich, ¨ 2021.

    - <span id="page-17-8"></span>[231] K. Loughlin, S. Saroiu, A. Wolman, and B.
    Kasikci, "Stop! Hammer Time: Rethinking Our Approach to Rowhammer Mitigations,"
    in HotOS, 2021.

    - <span id="page-17-9"></span>[232] T. Yang and X.-W. Lin, "Trap-Assisted DRAM
    Row Hammer Effect," EDL, 2019.

    - [233] Z. Greenfield, J. B. Halbert, and K. S. Bains, "Method, Apparatus and
    System for Determining a Count of Accesses to a Row of Memory," 2014, US Patent
    US20140085995A1.

    - <span id="page-17-10"></span>[234] O. Mutlu, "Retrospective: Flipping Bits in
    Memory Without Accessing Them: An Experimental Study of DRAM Disturbance Errors,"
    in ISCA@50 25-Year Retrospective: 1996-2020, J. F. Mart´ınez and L. K. John, Eds.,
    2023.

    - <span id="page-17-11"></span>[235] M. Seaborn and T. Dullien, "Exploiting the
    DRAM Rowhammer Bug to Gain Kernel Privileges," Black Hat, 2015.

    - <span id="page-17-22"></span>[236] V. van der Veen, Y. Fratantonio, M. Lindorfer,
    D. Gruss, C. Maurice, G. Vigna, H. Bos, K. Razavi, and C. Giuffrida, "Drammer:
    Deterministic Rowhammer Attacks on Mobile Platforms," in CCS, 2016.

    - [237] D. Gruss, C. Maurice, and S. Mangard, "Rowhammer.js: A Remote Software-Induced
    Fault Attack in Javascript," in DIMVA, 2016.

    - [238] K. Razavi, B. Gras, E. Bosman, B. Preneel, C. Giuffrida, and H. Bos, "Flip
    Feng Shui: Hammering a Needle in the Software Stack," in USENIX Sec., 2016.

    - [239] Y. Xiao, X. Zhang, Y. Zhang, and R. Teodorescu, "One Bit Flips, One Cloud
    Flops: Cross-VM Row Hammer Attacks and Privilege Escalation," in USENIX Sec.,
    2016.

    - [240] E. Bosman, K. Razavi, H. Bos, and C. Giuffrida, "Dedup Est Machina: Memory
    Deduplication as An Advanced Exploitation Vector," in S&P, 2016.

    - [241] S. Bhattacharya and D. Mukhopadhyay, "Curious Case of RowHammer: Flipping
    Secret Exponent Bits using Timing Analysis," in CHES, 2016.

    - [242] R. Qiao and M. Seaborn, "A New Approach for Rowhammer Attacks," in HOST,
    2016.

    - [243] Y. Jang, J. Lee, S. Lee, and T. Kim, "SGX-Bomb: Locking Down the Processor
    via Rowhammer Attack," in SysTEX, 2017.

    - [244] M. T. Aga, Z. B. Aweke, and T. Austin, "When Good Protections go Bad:
    Exploiting anti-DoS Measures to Accelerate Rowhammer Attacks," in HOST, 2017.

    - <span id="page-17-20"></span>[245] A. Tatar, C. Giuffrida, H. Bos, and K. Razavi,
    "Defeating Software Mitigations Against Rowhammer: A Surgical Precision Hammer,"
    in RAID, 2018.

    - [246] D. Gruss, M. Lipp, M. Schwarz, D. Genkin, J. Juffinger, S. O''Connell,
    W. Schoechl, and Y. Yarom, "Another Flip in the Wall of Rowhammer Defenses," in
    IEEE S&P, 2018.

    - [247] M. Lipp, M. Schwarz, L. Raab, L. Lamster, M. T. Aga, C. Maurice, and D.
    Gruss, "Nethammer: Inducing Rowhammer Faults Through Network Requests," in EuroS&PW,
    2020.

    - [248] P. Frigo, C. Giuffrida, H. Bos, and K. Razavi, "Grand Pwning Unit: Accelerating
    Microarchitectural Attacks with the GPU," in S&P, 2018.

    - [249] L. Cojocar, K. Razavi, C. Giuffrida, and H. Bos, "Exploiting Correcting
    Codes: On the Effectiveness of ECC Memory Against Rowhammer Attacks," in S&P,
    2019.

    - [250] S. Ji, Y. Ko, S. Oh, and J. Kim, "Pinpoint Rowhammer: Suppressing Unwanted
    Bit Flips on Rowhammer Attacks," in ASIACCS, 2019.

    - [251] S. Hong, P. Frigo, Y. Kaya, C. Giuffrida, and T. Dumitras¸, "Terminal
    Brain Damage: Exposing the Graceless Degradation in Deep Neural Networks Under
    Hardware Fault Attacks," in USENIX Sec., 2019.

    - [252] A. Kwong, D. Genkin, D. Gruss, and Y. Yarom, "RAMBleed: Reading Bits in
    Memory Without Accessing Them," in S&P, 2020.

    - [253] L. Cojocar, J. Kim, M. Patel, L. Tsai, S. Saroiu, A. Wolman, and O. Mutlu,
    "Are We Susceptible to Rowhammer? An End-to-End Methodology for Cloud Providers,"
    in S&P, 2020.

    - [254] Z. Weissman, T. Tiemann, D. Moghimi, E. Custodio, T. Eisenbarth, and B.
    Sunar, "JackHammer: Efficient Rowhammer on Heterogeneous FPGA–CPU Platforms,"
    in TCHES, 2020.

    - [255] Z. Zhang, Y. Cheng, D. Liu, S. Nepal, Z. Wang, and Y. Yarom, "PTHammer:
    Cross-User-Kernel-Boundary Rowhammer Through Implicit Accesses," in MICRO, 2020.

    - [256] F. Yao, A. S. Rakin, and D. Fan, "Deephammer: Depleting the Intelligence
    of Deep Neural Networks Through Targeted Chain of Bit Flips," in USENIX Sec.,
    2020.

    - [257] A. Kogler, J. Juffinger, S. Qazi, Y. Kim, M. Lipp, N. Boichat, E. Shiu,
    M. Nissler, and D. Gruss, "Half-Double: Hammering From the Next Row Over," in
    USENIX Sec., 2022.

    - <span id="page-17-12"></span>[258] F. de Ridder, P. Frigo, E. Vannacci, H. Bos,
    C. Giuffrida, and K. Razavi, "SMASH: Synchronized Many-sided Rowhammer Attacks
    from JavaScript," in USENIX Sec., 2021.

    - [259] J. Lee, "Green Memory Solution," Investor''s Forum, Samsung Electronics,
    2014.

    - <span id="page-17-19"></span>[260] L. Cojocar, K. Razavi, C. Giuffrida, and
    H. Bos, "Exploiting Correcting Codes: On The Effectiveness Of ECC Memory Against
    Rowhammer Attacks," in S&P, 2019.

    - <span id="page-17-13"></span>[261] Micron Technology Inc., "8Gb: x4, x8, x16
    DDR4 SDRAM Features - Excessive Row Activation," Micron Technology Inc., Tech.
    Rep., 2020.

    - <span id="page-17-14"></span>[262] L. Cojocar, J. Kim, M. Patel, L. Tsai, S.
    Saroiu, A. Wolman, and O. Mutlu, "Are We Susceptible to Rowhammer? An End-to-End
    Methodology for Cloud Providers," in IEEE S&P, 2020.

    - <span id="page-17-15"></span>[263] K. K. Chang, A. G. Yaglık ˘ c¸ı, S. Ghose,
    A. Agrawal, N. Chatterjee, A. Kashyap, D. Lee, M. O''Connor, H. Hassan, and O.
    Mutlu, "Understanding Reduced-Voltage Operation in Modern DRAM Devices: Experimental
    Characterization, Analysis, and Mechanisms," in SIG-METRICS, 2017.

    - <span id="page-17-16"></span>[264] V. Seshadri and O. Mutlu, "Simple Operations
    in Memory to Reduce Data Movement," in Advances in Computers, 2017.

    - [265] V. Seshadri and O. Mutlu, "In-DRAM Bulk Bitwise Execution Engine," Advances
    in Computers, 2020.

    - <span id="page-17-23"></span>[266] B. Talukder, J. Kerns, B. Ray, T. Morris,
    and M. T. Rahman, "Exploiting DRAM Latency Variations for Generating True Random
    Numbers," ICCE, 2019.

    - <span id="page-17-24"></span>[267] B. Talukder, B. Ray, M. Tehranipoor, D. Forte,
    and M. T. Rahman, "LDPUF: Exploiting DRAM Latency Variations to Generate Robust
    Device Signatures," arXiv preprint arXiv:1808.02584, 2018.

    - [268] M. Yue, N. Karimian, W. Yan, N. A. Anagnostopoulos, and F. Tehranipoor,
    "DRAM-Based Authentication Using Deep Convolutional Neural Networks," IEEE Consumer
    Electronics Magazine, 2020.

    - [269] M. S. Hashemian, B. Singh, F. Wolff, D. Weyer, S. Clay, and C. Papachristou,
    "A Robust Authentication Methodology Using Physically Unclonable Functions in
    DRAM Arrays," in DATE, 2015.

    - <span id="page-17-17"></span>[270] A. Schaller, W. Xiong, N. A. Anagnostopoulos,
    M. U. Saleem, S. Gabmeyer, B. Skori ˇ c, S. Katzenbeisser, and J. Szefer, "Decay-Based
    DRAM ´ PUFs in Commodity Devices," TDSC, 2018.

    - <span id="page-17-18"></span>[271] B. B. Talukder, B. Ray, D. Forte, and M.
    T. Rahman, "PreLatPUF: Exploiting DRAM Latency Variations For Generating Robust
    Device Signatures," IEEE Access, 2019.

    - <span id="page-17-21"></span>[272] H. Hassan, N. Vijaykumar, S. Khan, S. Ghose,
    K. Chang, G. Pekhimenko, D. Lee, O. Ergin, and O. Mutlu, "SoftMC: A Flexible and
    Practical Open-Source Infrastructure for Enabling Experimental DRAM Studies,"
    in HPCA, 2017.

    - <span id="page-18-0"></span>[273] H. David, C. Fallin, E. Gorbatov, U. R. Hanebutte,
    and O. Mutlu, "Memory Power Management via Dynamic Voltage/Frequency Scaling,"
    in ICAC, 2011.

    - <span id="page-18-3"></span>[274] X. Pan and F. Mueller, "Hiding DRAM Refresh
    Overhead in Real-Time Cyclic Executives," in RTSS, 2019.

    - <span id="page-18-4"></span>[275] J. Mukundan, H. Hunter, K.-h. Kim, J. Stuecheli,
    and J. F. Mart´ınez, "Understanding and Mitigating Refresh Overheads in High-Density
    DDR4 DRAM Systems," in ISCA, 2013.

    - <span id="page-18-1"></span>[276] T. Hamamoto, S. Sugiura, and S. Sawada, "Well
    Concentration: A Novel Scaling Limitation Factor Derived From DRAM Retention Time
    and Its Modeling," in IEDM, 1995.

    - <span id="page-18-2"></span>[277] T. Hamamoto, S. Sugiura, and S. Sawada, "On
    the Retention Time Distribution of Dynamic Random Access Memory (DRAM)," in TED,
    1998.

    - <span id="page-18-5"></span>[278] D.-T. Nguyen, N.-M. Ho, M.-S. Le, W.-F. Wong,
    and I.-J. Chang, "ZEM: Zero-Cycle Bit-Masking Module for Deep Learning Refresh-Less
    DRAM," IEEE Access, 2021.

    - <span id="page-18-6"></span>[279] Y. Han, Y. Wang, H. Li, and X. Li, "Data-Aware
    DRAM Refresh to Squeeze the Margin of Retention Time in Hybrid Memory Cube," in
    ICCAD, 2014.

    - <span id="page-18-7"></span>[280] H. Choi, D. Hong, J. Lee, and S. Yoo, "Reducing
    DRAM Refresh Power Consumption by Runtime Profiling of Retention Time and Dual-Row
    Activation," Microprocessors and Microsystems, 2020.

    - <span id="page-18-8"></span>[281] Apple Inc., "About the Security Content of
    Mac EFI Security Update 2015-001," [https://support.apple.com/en-us/HT204934,](https://support.apple.com/en-us/HT204934)
    2015.

    - <span id="page-18-9"></span>[282] B. Aichinger, "DDR Memory Errors Caused by
    Row Hammer," in HPEC, 2015.

    - <span id="page-18-10"></span>[283] H. Hassan, M. Patel, J. S. Kim, A. G. Yaglık
    ˘ c¸ı, N. Vijaykumar, N. M. Ghiasi, S. Ghose, and O. Mutlu, "CROW: A Low-Cost
    Substrate for Improving DRAM Performance, Energy Efficiency, and Reliability,"
    in ISCA, 2019.

    - [284] A. Di Dio, K. Koning, H. Bos, and C. Giuffrida, "Copy-on-Flip: Hardening
    ECC Memory Against Rowhammer Attacks," in NDSS, 2023.

    - [285] F. Brasser, L. Davi, D. Gens, C. Liebchen, and A.-R. Sadeghi, "Can''t
    Touch This: Practical and Generic Software-Only Defenses Against Rowhammer Attacks,"
    in CoRR, 2016.

    - [286] A. Saxena, G. Saileshwar, P. J. Nair, and M. Qureshi, "Aqua: Scalable
    RowHammer Mitigation by Quarantining Aggressor Rows at Runtime," in MICRO, 2022.

    - <span id="page-18-11"></span>[287] K. Loughlin, J. Rosenblum, S. Saroiu, A.
    Wolman, D. Skarlatos, and B. Kasikci, "Siloz: Leveraging DRAM Isolation Domains
    to Prevent Inter-VM Rowhammer," in SOSP, 2023.

    - <span id="page-18-12"></span>[288] Z. B. Aweke, S. F. Yitbarek, R. Qiao, R.
    Das, M. Hicks, Y. Oren, and T. Austin, "ANVIL: Software-Based Protection Against
    Next-Generation Rowhammer Attacks," in ASPLOS, 2016.

    - [289] M. Son, H. Park, J. Ahn, and S. Yoo, "Making DRAM Stronger Against Row
    Hammering," in DAC, 2017.

    - [290] S. M. Seyedzadeh, A. K. Jones, and R. Melhem, "Mitigating Wordline Crosstalk
    Using Adaptive Trees of Counters," in ISCA, 2018.

    - [291] J. M. You and J.-S. Yang, "MRLoc : Mitigating Row-Hammering Based on Memory
    Locality," in DAC, 2019.

    - [292] E. Lee, I. Kang, S. Lee, G. Edward Suh, and J. Ho Ahn, "TWiCe: Preventing
    Row-Hammering by Exploiting Time Window Counters," in ISCA, 2019.

    - [293] Y. Park, W. Kwon, E. Lee, T. J. Ham, J. H. Ahn, and J. W. Lee, "Graphene:
    Strong yet Lightweight Row Hammer Protection," in MICRO, 2020.

    - [294] D.-H. Kim, P. J. Nair, and M. K. Qureshi, "Architectural Support for Mitigating
    Row Hammering in DRAM Memories," IEEE CAL, 2014.

    - [295] I. Kang, E. Lee, and J. H. Ahn, "CAT-TWO: Counter-Based Adaptive Tree,
    Time Window Optimized for DRAM Row-Hammer Prevention," IEEE Access, 2020.

    - [296] K. Bains, J. Halbert, C. Mozak, T. Schoenborn, and Z. Greenfield, "Row
    Hammer Refresh Command," 2015, U.S. Patent 9,117,544.

    - [297] K. S. Bains and J. B. Halbert, "Distributed Row Hammer Tracking," 2016,
    U.S. Patent 9,299,400.

    - [298] K. S. Bains and J. B. Halbert, "Row Hammer Monitoring Based on Stored
    Row Hammer Threshold Value," 2016, U.S. Patent 9,384,821.

    - [299] F. Devaux and R. Ayrignac, "Method and Circuit for Protecting a DRAM Memory
    Device from the Row Hammer Effect," 2021, 10,885,966.

    - <span id="page-18-17"></span>[300] A. G. Yaglık ˘ c¸ı, J. S. Kim, F. Devaux,
    and O. Mutlu, "Security Analysis of the Silver Bullet Technique for RowHammer
    Prevention," 2021.

    - <span id="page-18-28"></span>[301] M. Marazzi, P. Jattke, S. Flavien, and K.
    Razavi, "ProTRR: Principled yet Optimal In-DRAM Target Row Refresh," in SP, 2022.

    - [302] Y. Kim, "Architectural Techniques to Enhance DRAM Scaling," Ph.D. dissertation,
    Carnegie Mellon University, 2015.

    - [303] M. Marazzi, F. Solt, P. Jattke, K. Takashi, and K. Razavi, "REGA: Scalable
    Rowhammer Mitigation with Refresh-Generating Activations," in S&P, 2023.

    - <span id="page-18-13"></span>[304] M. Qureshi, A. Rohan, G. Saileshwar, and
    P. J. Nair, "Hydra: Enabling Low-Overhead Mitigation of Row-Hammer at Ultra-Low
    Thresholds via Hybrid Tracking," in ISCA, 2022.

    - <span id="page-18-14"></span>[305] S. Lee, K.-D. Kang, G. Park, N. S. Kim, and
    D. Kim, "NoHammer: Preventing Row Hammer with Last-Level Cache Management," IEEE
    CAL, 2023.

    - <span id="page-18-15"></span>[306] T. Bennett, S. Saroiu, A. Wolman, and L.
    Cojocar, "Panopticon: A Complete In-DRAM Rowhammer Mitigation," in DRAMSec, 2021.

    - <span id="page-18-16"></span>[307] Y. Kim, V. Seshadri, D. Lee, J. Liu, and
    O. Mutlu, "A Case for Exploiting Subarray-Level Parallelism (SALP) in DRAM," in
    ISCA, 2012.

    - <span id="page-18-18"></span>[308] D. Yun, M. Park, C. Lim, and S. Baeg, "Study
    of TID Effects on One Row Hammering using Gamma in DDR4 SDRAMs," in IRPS, 2018.

    - [309] C. Lim, K. Park, and S. Baeg, "Active Precharge Hammering to Monitor Displacement
    Damage using High-Energy Protons in 3x-nm SDRAM," IEEE Trans. Nucl. Sci., 2016.

    - [310] A. Olgun, M. Osseiran, A. G. Yaglık ˘ c¸ı, Y. C. Tugrul, H. Luo, S. Rhyner,
    ˘ B. Salami, J. G. Luna, and O. Mutlu, "An Experimental Analysis of RowHammer
    in HBM2 DRAM Chips," in DSN-S, 2023.

    - <span id="page-18-19"></span>[311] Z. Lang, P. Jattke, M. Marazzi, and K. Razavi,
    "BLASTER: Characterizing the Blast Radius of Rowhammer," in DRAMSec, 2023.

    - <span id="page-18-26"></span>[312] Synopsys, "Reliability, Availability and
    Serviceability (RAS) for Memory Interfaces," Synopsys, Tech. Rep., 2015.

    - [313] T. J. Dell, "System RAS Implications of DRAM Soft Errors," IBM JRD, 2008.

    - <span id="page-18-27"></span>[314] C. Slayman, M. Ma, and S. Lindley, "Impact
    of Error Correction Code and Dynamic Memory Reconfiguration on High-Reliability/Low-Cost
    Server Memory," in IRWS, 2006.

    - <span id="page-18-20"></span>[315] R. Sharifi and Z. Navabi, "Online Profiling
    for Cluster-Specific Variable Rate Refreshing in High-Density DRAM Systems," in
    ETS, 2017.

    - [316] A. R. Alameldeen, I. Wagner, Z. Chishti, W. Wu, C. Wilkerson, and S.-L.
    Lu, "Energy-Efficient Cache Design Using Variable-Strength Error-Correcting Codes,"
    ISCA, 2011.

    - [317] H. Naeimi, C. Augustine, A. Raychowdhury, S.-L. Lu, and J. Tschanz, "STTRAM
    Scaling and Retention Failure," Intel Technology Journal, 2013.

    - [318] M. Awasthi, M. Shevgoor, K. Sudan, B. Rajendran, R. Balasubramonian, and
    V. Srinivasan, "Efficient Scrub Mechanisms for Error-Prone Emerging Memories,"
    in HPCA, 2012.

    - <span id="page-18-21"></span>[319] M. J. M. Rahman, "Utilizing Two Stage Scrubbing
    to Handle Single-Fault Multi-Error Cases in DRAM Systems," Master''s thesis, Iowa
    State University, 2021.

    - <span id="page-18-22"></span>[320] D. Kline, J. Zhang, R. Melhem, and A. K.
    Jones, "Flower and Fame: A Low Overhead Bit-Level Fault-Map and Fault-Tolerance
    Approach for Deeply Scaled Memories," in HPCA, 2020.

    - <span id="page-18-25"></span>[321] S. Longofono, D. Kline Jr, R. Melhem, and
    A. K. Jones, "Predicting and Mitigating Single-Event Upsets in DRAM using HOTH,"
    Microelectronics Reliability, 2021.

    - [322] D. Kline, R. Melhem, and A. K. Jones, "Sustainable Fault Management and
    Error Correction for Next-Generation Main Memories," in IGSC, 2017.

    - [323] S. Schechter, G. H. Loh, K. Strauss, and D. Burger, "Use ECP, Not ECC,
    for Hard Failures in Resistive Memories," ISCA, 2010.

    - [324] P. J. Nair, B. Asgari, and M. K. Qureshi, "SuDoku: Tolerating High-Rate
    of Transient Failures for Enabling Scalable STTRAM," in DSN, 2019.

    - [325] J. Zhang, D. Kline, L. Fang, R. Melhem, and A. K. Jones, "Dynamic Partitioning
    To Mitigate Stuck-At Faults in Emerging Memories," in ICCAD, 2017.

    - [326] H. Wang, "Architecting Memory Systems Upon Highly Scaled Error-Prone Memory
    Technologies," Ph.D. dissertation, Rensselaer Polytechnic Institute, 2017.

    - <span id="page-18-24"></span><span id="page-18-23"></span>[327] D. W. Kim and
    M. Erez, "RelaxFault Memory Repair," in ISCA, 2016.

    - [328] L. Mukhanov, K. Tovletoglou, H. Vandierendonck, D. S. Nikolopoulos, and
    G. Karakonstantis, "Workload-Aware DRAM Error Prediction Using Machine Learning,"
    in IISWC, 2019.

    - [329] E. Baseman, N. DeBardeleben, K. Ferreira, S. Levy, S. Raasch, V. Sridharan,
    T. Siddiqua, and Q. Guan, "Improving DRAM Fault Characterization Through Machine
    Learning," in DSN-W, 2016.

    - [330] I. Giurgiu, J. Szabo, D. Wiesmann, and J. Bird, "Predicting DRAM Reliability
    in the Field with Machine Learning," in Middleware, 2017.

    - [331] Z. Lan, J. Gu, Z. Zheng, R. Thakur, and S. Coghlan, "A Study of Dynamic
    Meta-Learning for Failure Prediction in Large-Scale Systems," PDC, 2010.

    - [332] Y. Liang, Y. Zhang, A. Sivasubramaniam, M. Jette, and R. Sahoo, "Bluegene/L
    Failure Analysis and Prediction Models," in DSN, 2006.

    - <span id="page-19-0"></span>[333] I. Boixaderas, D. Zivanovic, S. More, J. Bartolome,
    D. Vicente, ´ M. Casas, P. M. Carpenter, P. Radojkovic, and E. Ayguad ´ e, "Cost-
    ´ Aware Prediction of Uncorrected DRAM Errors in the Field," in SC, 2020.

    - <span id="page-19-1"></span>[334] G. Agrawal, L. Massengill, and K. Gulati,
    "A Proposed SEU Tolerant Dynamic Random Access Memory (DRAM) Cell," IEEE Trans.
    Nucl. Sci., 1994.

    - [335] Infineon, "Radiation Hardened & High Reliability Memories," [https:](https://www.infineon.com/cms/en/product/memories/radiation-hardened-high-reliability-memories/)
    [//www.infineon.com/cms/en/product/memories/radiation-hardene](https://www.infineon.com/cms/en/product/memories/radiation-hardened-high-reliability-memories/)
    [d-high-reliability-memories/,](https://www.infineon.com/cms/en/product/memories/radiation-hardened-high-reliability-memories/)
    2022.

    - [336] N. C. Lu, "Advanced Cell Structures for Dynamic RAMs," IEEE Circuits and
    Devices Magazine, 1989.

    - [337] S. K. Banerjee, "Two-Transistor DRAM Cell with High Alpha Particle Immunity,"
    1989, US Patent 4,864,374.

    - [338] P. Mazumder, "Design of a Fault-Tolerant Three-Dimensional Dynamic Random-Access
    Memory with On-Chip Error-Correcting Circuit," TOC, 1993.

    - [339] Data Device Corporation, "Rad Hard Memories," [https://www.ddc-w](https://www.ddc-web.com/en/radhard/memories)
    [eb.com/en/radhard/memories,](https://www.ddc-web.com/en/radhard/memories) 2022.

    - <span id="page-19-2"></span>[340] 3D PLUS, "DDR4 SDRAM," [https://www.3d-plus.com/product.php?](https://www.3d-plus.com/product.php?fam=8&prod=43)
    [fam=8&prod=43,](https://www.3d-plus.com/product.php?fam=8&prod=43) 2022.

    - <span id="page-19-3"></span>[341] D. M. Mathew, H. Kattan, C. Weis, J. Henkel,
    N. Wehn, and H. Amrouch, "Thermoelectric Cooling to Survive Commodity DRAMs in
    Harsh Environment Automotive Electronics," IEEE Access, 2021.

    - <span id="page-19-4"></span>[342] K. Kobayashi, "Highly-reliable Integrated
    Circuits for Ground and Space Applications," ASICON, 2017.

    - <span id="page-19-5"></span>[343] J. Kim, M. Sullivan, S. Lym, and M. Erez,
    "All-Inclusive ECC: Thorough End-to-End Protection for Reliable Computer Memory,"
    in ISCA, 2016.

    - <span id="page-19-6"></span>[344] S. Lee, N. S. Kim, and D. Kim, "Exploiting
    OS-Level Memory Offlining for DRAM Power Management," IEEE CAL, 2019.

    - <span id="page-19-7"></span>[345] T. K. Moon, Error Correction Coding: Mathematical
    Methods and Algorithms. John Wiley & Sons, 2005.

    - [346] T. Richardson and R. Urbanke, Modern Coding Theory. Cambridge University
    Press, 2008.

    - [347] R. M. Roth, Introduction to Coding Theory. Cambridge University Press,
    2006.

    - [348] G. C. Clark Jr and J. B. Cain, Error-Correction Coding for Digital Communications.
    Springer SBM, 2013.

    - [349] D. J. Costello and S. Lin, Error Control Coding: Fundamentals and Applications.
    Prentice Hall, 1982.

    - <span id="page-19-8"></span>[350] S. Lin and D. J. Costello, Error Control Coding:
    Fundamentals and Applications. Prentice Hall, 2004.

    - <span id="page-19-9"></span>[351] I. Alam, "Lightweight Opportunistic Memory
    Resilience," Ph.D. dissertation, University of California, Los Angeles, 2021.

    - <span id="page-19-10"></span>[352] K. Baker and J. Van Beers, "Shmoo Plotting:
    The Black Art of IC Testing," IEEE Des Test, 1997.

    - <span id="page-19-11"></span>[353] Advantest, T5833/T5833ES Memory Test System,
    [https://www.advant](https://www.advantest.com/products/memory/t5833.html) [est.com/products/memory/t5833.html,](https://www.advantest.com/products/memory/t5833.html)
    2022.

    - <span id="page-19-12"></span>[354] Teradyne, Magnum EPIC Ultra-high Performance
    Solution for Memory Device Test, [https://www.teradyne.com/products/magnum-epic/,](https://www.teradyne.com/products/magnum-epic/)
    2022.

    - <span id="page-19-13"></span>[355] C.-S. Hou, J.-F. Li, C.-Y. Lo, D.-M. Kwai,
    Y.-F. Chou, and C.-W. Wu, "An FPGA-Based Test Platform for Analyzing Data Retention
    Time Distribution of DRAMs," in VLSI-DAT, 2013.

    - <span id="page-19-35"></span>[356] C. Weis, M. Jung, P. Ehses, C. Santos, P.
    Vivet, S. Goossens, M. Koedam, and N. Wehn, "Retention Time Measurements and Modelling
    of Bit Error Rates of Wide I/O DRAM in MPSoCs," in DATE, 2015.

    - <span id="page-19-36"></span>[357] F. Wang, T. Vogelsang, B. Haukness, and S.
    C. Magee, "DRAM Retention at Cryogenic Temperatures," in IMW, 2018.

    - <span id="page-19-14"></span>[358] R. Ladbury, M. Berg, E. Wilcox, K. LaBel,
    H. Kim, A. Phan, and C. Seidleck, "Use of Commercial FPGA-Based Evaluation Boards
    for Single-Event Testing of DDR2 and DDR3 SDRAMS," IEEE Trans. Nucl. Sci., 2013.

    - <span id="page-19-15"></span>[359] P. Software, "MemTest86 Overview," [https://www.memtest86.com/in](https://www.memtest86.com/index.html)
    [dex.html,](https://www.memtest86.com/index.html) 2019.

    - <span id="page-19-16"></span>[360] P. Francis-Mezger and V. M. Weaver, "A Raspberry
    Pi Operating System for Exploring Advanced Memory System Concepts," in MEMSYS,
    2018.

    - <span id="page-19-17"></span>[361] T.-Y. Oh, H. Chung, J.-Y. Park, K.-W. Lee,
    S. oh, S.-Y. Doo, H.-J. Kim, C. Lee, H.-R. Kim, J.-H. Lee, J.-I. Lee, K.-S. Ha,
    Y. Choi, Y.-C. Cho, Y.-C. Bae, T. Jang, C. Park, K. Park, S. Jang, and J. Choi,
    "A 3.2Gbps/pin 8Gb 1.0V LPDDR4 SDRAM with Integrated ECC Engine for Sub-1V DRAM
    Core Operation," in ISSCC, 2014.

    - [362] T.-Y. Oh, H. Chung, J.-Y. Park, K.-W. Lee, S. Oh, S.-Y. Doo, H.-J. Kim,
    C. Lee, H.-R. Kim, J.-H. Lee, J.-I. Lee, K.-S. Ha, Y. Choi, Y.-C. Cho, Y.-C. Bae,
    T. Jang, C. Park, K. Park, S. Jang, and J. S. Choi, "A 3.2 Gbps/Pin 8 Gbit 1.0
    V LPDDR4 SDRAM with Integrated ECC Engine for Sub-1 V DRAM Core Operation," JSSC,
    2014.

    - <span id="page-19-18"></span>[363] S. Kwon, Y. H. Son, and J. H. Ahn, "Understanding
    DDR4 in Pursuit of In-DRAM ECC," in ISOCC, 2014.

    - <span id="page-19-19"></span>[364] K. K. Chang, P. J. Nair, D. Lee, S. Ghose,
    M. K. Qureshi, and O. Mutlu, "Low-Cost Inter-Linked Subarrays (LISA): Enabling
    Fast Inter-Subarray Data Movement in DRAM," in HPCA, 2016.

    - <span id="page-19-20"></span>[365] F. Bostancı, A. Olgun, L. Orosa, A. G. Yaglık
    ˘ c¸ı, J. S. Kim, H. Hassan, O. Ergin, and O. Mutlu, "DR-STRaNGe: End-to-End System
    Design for DRAM-based True Random Number Generators," HPCA, 2022.

    - <span id="page-19-21"></span>[366] T. Zhang, K. Chen, C. Xu, G. Sun, T. Wang,
    and Y. Xie, "Half-DRAM: A High-Bandwidth and Low-Power DRAM Architecture from
    the Rethinking of Fine-Grained Activation," in ISCA, 2014.

    - <span id="page-19-22"></span>[367] A. Bacchini, M. Rovatti, G. Furano, and M.
    Ottavi, "Characterization of Data Retention Faults in DRAM Devices," in DFT, 2014.

    - [368] A. Weber, A. Birner, and W. Krautschneider, "Data Retention Analysis on
    Individual Cells of 256Mb DRAM in 110nm Technology," in ESSDERC, 2005.

    - <span id="page-19-23"></span>[369] K. Yamaguchi, "Theoretical Study of Deep-Trap-Assisted
    Anomalous Currents in Worst-Bit Cells of Dynamic Random-Access Memories (DRAM''s),"
    TED, 2000.

    - <span id="page-19-24"></span>[370] A. J. Walker, S. Lee, and D. Beery, "On DRAM
    Rowhammer and the Physics of Insecurity," TED, 2021.

    - <span id="page-19-25"></span>[371] A. Das, H. Hassan, and O. Mutlu, "VRL-DRAM:
    Improving DRAM Performance Via Variable Refresh Latency," in DAC, 2018.

    - [372] T. Siddiqua, A. E. Papathanasiou, A. Biswas, S. Gurumurthi, I. Corp, and
    T. Aster, "Analysis and Modeling of Memory Errors From Large-Scale Field Data
    Collection," in SELSE, 2013.

    - <span id="page-19-26"></span>[373] J. Meza, Q. Wu, S. Kumar, and O. Mutlu, "A
    Large-Scale Study of Flash Memory Errors in the Field," in SIGMETRICS, 2015.

    - <span id="page-19-27"></span>[374] S. Jin, J.-H. Yi, J. H. Choi, D. G. Kang,
    Y. J. Park, and H. S. Min, "Prediction of Data Retention Time Distribution of
    DRAM by Physics-Based Statistical Simulation," TED, 2005.

    - <span id="page-19-32"></span>[375] A. Hiraiwa, M. Ogasawara, N. Natsuaki, Y.
    Itoh, and H. Iwai, "Statistical Modeling of Dynamic Random Access Memory Data
    Retention Characteristics," JAP, 1996.

    - <span id="page-19-31"></span>[376] Y. Li, H. Schneider, F. Schnabel, R. Thewes,
    and D. Schmitt-Landsiedel, "DRAM Yield Analysis and Optimization by a Statistical
    Design Approach," in CSI, 2011.

    - [377] A. Hiraiwa, M. Ogasawara, N. Natsuaki, Y. Itoh, and H. Iwai, "Local-Field-Enhancement
    Model of DRAM Retention Failure," in IEDM, 1998.

    - [378] N. Edri, P. Meinerzhagen, A. Teman, A. Burg, and A. Fish, "Silicon-Proven,
    Per-Cell Retention Time Distribution Model for Gain-Cell Based eDRAMs," IEEE TOCS,
    2016.

    - [379] K. Kim and J. Lee, "A New Investigation of Data Retention Time in Truly
    Nanoscaled DRAMs," in EDL, 2009.

    - <span id="page-19-28"></span>[380] W. Kong, P. C. Parries, G. Wang, and S. S.
    Iyer, "Analysis of Retention Time Distribution of Embedded DRAM-A New Method to
    Characterize Across-Chip Threshold Voltage Variation," in ITC, 2008.

    - <span id="page-19-29"></span>[381] D.-H. Kim, S. Cha, and L. S. Milor, "AVERT:
    An Elaborate Model for Simulating Variable Retention Time in DRAMs," Microelectronics
    Reliability, 2015.

    - <span id="page-19-37"></span>[382] D. S. Yaney, C.-Y. Lu, R. A. Kohler, M. J.
    Kelly, and J. T. Nelson, "A Meta-Stable Leakage Phenomenon in DRAM Charge Storage-Variable
    Hold Time," in IEDM, 1987.

    - <span id="page-19-33"></span>[383] P. J. Restle, J. Park, and B. F. Lloyd, "DRAM
    Variable Retention Time," in IEDM, 1992.

    - <span id="page-19-34"></span>[384] C. G. Shirley and W. R. Daasch, "Copula Models
    of Correlation: A DRAM Case Study," in TC, 2014.

    - [385] H. Kim, B. Oh, Y. Son, K. Kim, S.-Y. Cha, J.-G. Jeong, S.-J. Hong, and
    H. Shin, "Characterization of the Variable Retention Time in Dynamic Random Access
    Memory," TED, 2011.

    - [386] H. Kim, B. Oh, Y. Son, K. Kim, S.-Y. Cha, J.-G. Jeong, S.-J. Hong, and
    H. Shin, "Study of Trap Models Related to the Variable Retention Time Phenomenon
    in DRAM," TED, 2011.

    - <span id="page-19-38"></span>[387] N. Kumar, "Detection of Variable Retention
    Time in DRAM," Master''s thesis, Portland State University, Portland, Oregon,
    2014.

    - [388] Y. Mori, K. Ohyu, K. Okonogi, and R. i. Yamada, "The Origin of Variable
    Retention Time in DRAM," in IEDM, 2005.

    - <span id="page-19-30"></span>[389] K. Ohyu, T. Umeda, K. Okonogi, S. Tsukada,
    M. Hidaka, S. Fujieda, and Y. Mochizuki, "Quantitative Identification for the
    Physical Origin of Variable Retention Time: A Vacancy-Oxygen Complex Defect Model,"
    in IEDM, 2006.

    - <span id="page-20-0"></span>[390] Sentaurus Sdevice User''s Manual, Synopsys,
    2018.

    - [391] M. Duan, F. Adam-Lema, B. Cheng, C. Navarro, X. Wang, V. Georgiev, F.
    Gamiz, C. Millar, and A. Asenov, "2D-TCAD Simulation on Retention Time of Z2FET
    for DRAM Application," in SISPAD, 2017.

    - <span id="page-20-1"></span>[392] P. Pfaffli, H. Wong, X. Xu, L. Silvestri,
    X. Lin, T. Yang, R. Tiwari, ¨ S. Mahapatra, S. Motzny, V. Moroz, and T. Ma, "TCAD
    Modeling for Reliability," Microelectronics Reliability, 2018.

    - <span id="page-20-2"></span>[393] H. Luo, T. Shahroodi, H. Hassan, M. Patel,
    A. Giray Yaglık ˘ c¸ı, L. Orosa, J. Park, and O. Mutlu, "CLR-DRAM: A Low-Cost
    DRAM Architecture Enabling Dynamic Capacity-Latency Trade-Off," in ISCA, 2020.

    - [394] H. H. Shin and E.-Y. Chung, "In-DRAM Cache Management for Low Latency
    and Low Power 3D-Stacked DRAMs," Micromachines, 2019.

    - <span id="page-20-3"></span>[395] Y. Wang, L. Orosa, X. Peng, Y. Guo, S. Ghose,
    M. Patel, J. S. Kim, J. G. Luna, M. Sadrosadati, N. M. Ghiasi, and O. Mutlu, "FIGARO:
    Improving System Performance via Fine-Grained In-DRAM Data Relocation and Caching,"
    in MICRO, 2020.

    - <span id="page-20-4"></span>[396] A. J. Walker, S. Lee, and D. Beery, "On DRAM
    Rowhammer and the Physics of Insecurity," TED, 2021.

    - <span id="page-20-6"></span><span id="page-20-5"></span>[397] T. Baker, "Opening
    Up Ada-Tasking," ACM SIGAda Ada Letters, 1990.

    - [398] K. Duganapalli, "Modelling and Test Generation for Crosstalk Faults in
    DSM Chips," Ph.D. dissertation, Universitat Bremen, 2016. ¨

    - [399] L. Cojocar, K. Loughlin, S. Saroiu, B. Kasikci, and A. Wolman, "mFIT:
    A Bump-in-the-Wire Tool for Plug-and-Play Analysis of Rowhammer Susceptibility
    Factors," Microsoft Research, Tech. Rep., 2021.

    - <span id="page-20-7"></span>[400] L. Borucki, G. Schindlbeck, and C. Slayman,
    "Comparison of Accelerated DRAM Soft Error Rates Measured at Component and System
    Level," in IEEE IRPS, 2008.

    - <span id="page-20-8"></span>[401] A. G. Yaglık ˘ c¸ı, H. Luo, A. Olgun, G. F.
    de Oliveira Junior, J. Park, M. Patel, H. Hassan, L. Orosa, J. Kim, and O. Mutlu,
    "Understanding the RowHammer Vulnerability Under Reduced Wordline Voltage: An
    Experimental Study Using Real Devices," in DSN, 2022.

    - <span id="page-20-9"></span>[402] Q. Salman, K. Yoongu, B. Nicolas, S. Eric,
    and N. Mattias, "Half-Double: Next-Row-Over Assisted Rowhammer," 2021.

    - <span id="page-20-10"></span>[403] J. Kim and M. C. Papaefthymiou, "Block-Based
    Multi-Period Refresh For Energy Efficient Dynamic Memory," in IEEE International
    ASIC/SOC Conference, 2001.

    - <span id="page-20-11"></span>[404] S. Levy, K. B. Ferreira, N. DeBardeleben,
    T. Siddiqua, V. Sridharan, and E. Baseman, "Lessons Learned from Memory Errors
    Observed Over the Lifetime of Cielo," in SC, 2018.

    - <span id="page-20-12"></span>[405] F. Wang and V. D. Agrawal, "Soft Error Rates
    with Inertial and Logical Masking," in VLSI, 2009.

    - <span id="page-20-13"></span>[406] Micron Technology, "DRAM," [https://www.micron.com/products/dr](https://www.micron.com/products/dram/)
    [am/.](https://www.micron.com/products/dram/)

    - [407] ISSI, "DDR4 SDRAM," [https://www.issi.com/US/product-dram-ddr4.](https://www.issi.com/US/product-dram-ddr4.shtml)
    [shtml.](https://www.issi.com/US/product-dram-ddr4.shtml)

    - <span id="page-20-14"></span>[408] ISSI, "NT5AD256M16E4-JR," [https://www.nanya.com/en/Product/4](https://www.nanya.com/en/Product/4596/NT5AD256M16E4-JR)
    [596/NT5AD256M16E4-JR.](https://www.nanya.com/en/Product/4596/NT5AD256M16E4-JR)

    - <span id="page-20-15"></span>[409] D. Patterson, T. Anderson, N. Cardwell, R.
    Fromm, K. Keeton, C. Kozyrakis, R. Thomas, and K. Yelick, "A Case for Intelligent
    RAM," IEEE Micro, 1997.

    - <span id="page-20-16"></span>[410] O. Mutlu, S. Ghose, J. Gomez-Luna, and R.
    Ausavarungnirun, "A Modern Primer on Processing in Memory," in arXiv, 2020.

    - [411] F. Devaux, "The True Processing in Memory Accelerator," in HCS, 2019.

    - [412] Y.-C. Kwon, S. H. Lee, J. Lee, S.-H. Kwon, J. M. Ryu, J.-P. Son, O. Seongil,
    H.-S. Yu, H. Lee, S. Y. Kim, Y. Cho, J. G. Kim, J. Choi, H.-S. Shin, J. Kim, B.
    Phuah, H. Kim, M. J. Song, A. Choi, D. Kim, S. Kim, E.-B. Kim, D. Wang, S. Kang,
    Y. Ro, S. Seo, J. Song, J. Youn, K. Sohn, and N. S. Kim, "25.4 A 20nm 6GB Function-In-Memory
    DRAM, Based on HBM2 with a 1.2 TFLOPS Programmable Computing Unit Using Bank-Level
    Parallelism, for Machine Learning Applications," in ISSCC, 2021.

    - [413] M. He, C. Song, I. Kim, C. Jeong, S. Kim, I. Park, M. Thottethodi, and
    T. Vijaykumar, "Newton: A DRAM-Maker''s Accelerator-In-Memory (AiM) Architecture
    for Machine Learning," in MICRO, 2020.

    - [414] D. Niu, S. Li, Y. Wang, W. Han, Z. Zhang, Y. Guan, T. Guan, F. Sun, F.
    Xue, L. Duan, Y. Fang, H. Zheng, X. Jiang, S. Wang, F. Zuo, Y. Wang, B. Yu, Q.
    Ren, and Y. Xie, "184QPS/W 64Mb/mm 2 3D Logic-to-DRAM Hybrid Bonding with Process-Near-Memory
    Engine for Recommendation System," in ISSCC, 2022.

    - [415] J. Ahn, S. Hong, S. Yoo, O. Mutlu, and K. Choi, "A Scalable Processing-In-Memory
    Accelerator for Parallel Graph Processing," ISCA, 2016.

    - [416] S. Lee, K. Kim, S. Oh, J. Park, G. Hong, D. Ka, K. Hwang, J. Park, K.
    Kang, J. Kim et al., "A 1ynm 1.25 V 8Gb, 16Gb/s/pin GDDR6-based Accelerator-in-Memory
    supporting 1TFLOPS MAC Operation and Various Activation Functions for Deep-Learning
    Applications," in ISSCC, 2022.

    - <span id="page-20-17"></span>[417] O. Mutlu, S. Ghose, J. Gomez-Luna, and R.
    Ausavarungnirun, "Pro- ´ cessing Data Where It Makes Sense: Enabling In-Memory
    Computation," Microprocessors and Microsystems, 2019.

    - <span id="page-20-18"></span>[418] M. Patel, T. Shahroodi, A. Manglik, A. G.
    Yaglikci, A. Olgun, H. Luo, and O. Mutlu, "A Case for Transparent Reliability
    in DRAM Systems," arXiv:2204.10378, 2022.


    ## <span id="page-21-0"></span>A. DRAM Trends Survey


    We survey manufacturer-recommended DRAM operating parameters as specified in commodity
    DRAM chip datasheets in order to understand how the parameters have evolved over
    time. We extract values from 58 independent DRAM chip datasheets from across 19
    different DRAM manufacturers with datasheet publishing dates between 1970 and
    2021. Appendix [B](#page-24-0) lists each datasheet and the details of the DRAM
    chip that it corresponds to. We openly release our full dataset on GitHub [\[136\]](#page-15-15),
    which includes all of the raw data used in this paper, including each timing and
    current parameter value, and additional fields (e.g., clock frequencies, package
    pin counts, remaining IDD values) that are not presented here.


    ## <span id="page-21-3"></span>A.1. DRAM Access Timing Trends


    We survey how the following four DRAM timing parameters that are directly related
    to DRAM chip performance evolve.


    - tRCD: time between issuing a row command (i.e., row activation) and a column
    command (e.g., read) to the row.

    - CAS Latency (or tAA): time between issuing an access to a given column address
    and the data being ready to access.

    - tRAS: time between issuing a row command (i.e., row activation) and a precharge
    command.

    - tRC : time between accessing two different rows.


    <span id="page-21-1"></span>![](_page_21_Figure_8.jpeg)


    Figure 4: Evolution of four key DRAM timing parameters (shown in log scale) across
    years (top) and chip capacities (bottom) separated by DRAM standard.


    Figure [4](#page-21-1) shows how key DRAM timing parameters have evolved across
    DRAM chips of different years (top) and capacities (bottom). Timing values are
    shown in log scale to better distinguish small values in newer DRAM chips. Each
    type of marker illustrates DRAM chips of different DRAM standards.


    We make three qualitative observations. First, although all four DRAM timing values
    roughly decrease over time, improvements are relatively stagnant for the last
    two decades (note the logarithmic Y-axis). The bulk of the improvement in timing
    parameter values occurred during the period of asynchronous DRAM, and following
    the introduction of SDRAM and DDRn DRAM chips, little to no improvements have
    been made despite, or possibly as a result of, continual increases in overall
    chip storage density. Second, CAS latency and tRCD converged to roughly the same
    values following the introduction of synchronous DRAM. We hypothesize that this
    is because similar factors affect the latency of these operations, including a
    long command and data communication latency between the external DRAM bus and
    the internal storage array [\[3\]](#page-13-2). Third, the DDR5 data points appear
    to worsen relative to previous DDRn points. However, we believe this might be
    because DDR5 chips are new at the time of writing this article and have not yet
    been fully optimized (e.g., through die revisions and other process improvements).


    To quantify the changes in access timing values, we aggregate the data points
    from Figure [4](#page-21-1) by time, DRAM standard, and chip capacity. Figure
    [5,](#page-21-2) shows the minimum, median, and maximum values (in log scale)
    for each 5-year period (top) and DRAM standard (bottom). The data shows that the
    median tRCD/CAS Latency/tRAS/tRC reduced by 2.66/3.11/2.89/2.89% per year on average
    between 1970 and


    <span id="page-21-2"></span>![](_page_21_Figure_13.jpeg)


    Figure 5: Evolution of the minimum, median, and maximum values of key DRAM timing
    parameters (shown in log scale) for each 5-year period (top) and DRAM standard
    (bottom).


    <span id="page-22-1"></span>![](_page_22_Figure_0.jpeg)


    Figure 6: Evolution of the minimum, median, and maximum values of key DRAM timing
    parameters (shown in log scale) grouped by DRAM chip storage capacity.


    2000 but only 0.81/0.97/1.33/1.53% between 2000 and 2015[18](#page-22-0) for an
    overall decrease of 1.83/2.10/1.99/2.00% between 1970 and 2015.


    Figure [6](#page-22-1) shows the minimum, median, and maximum of the timing parameter
    values (in log scale) grouped by DRAM chip storage capacity.[19](#page-22-2) We
    find that the timings follow similar trends as in Figure [5](#page-21-2) because
    higher-capacity DRAM chips are typically introduced more recently and follow newer
    DRAM standards.


    #### A.2. Current Consumption Trends


    We review the evolution of the following key DRAM current consumption measurements,
    which are standardized by JEDEC and are provided by manufacturers in their datasheets.


    - IDD0: current consumption with continuous row activation and precharge commands
    issued to only one bank.

    - IDD4R: current consumption when issuing back-to-back read operations to all
    banks.

    - IDD5B: current consumption when issuing continuous burst refresh operations.


    Figure [7](#page-22-3) shows how key DRAM current consumption values (in log scale)
    have evolved across DRAM chips of different years (top) and capacities (bottom).
    We use different markers to show data points from chips of different DRAM standards.
    We qualitatively observe that current consumption increased exponentially up until
    approximately the year 2000, which is about the time at which improvements in
    access timings slowed down (as seen in Figure [4\)](#page-21-1). After this point,
    different current consumption measurements diverged as IDD0 values decreased while
    IDD4R and IDD5B stabilized or increased. We explain this behavior by a change
    in the way DRAM chips were refreshed as DRAM capacities continued to increase.
    Earlier DRAM chips refreshed rows using individual row accesses (e.g., RAS-only
    refresh), which result in comparable behavior for access and refresh operations.
    In contrast, newer DRAM chips aggressively refresh multiple rows per refresh operation
    (e.g., burst refresh), which differentiates refresh operations from normal row
    accesses [\[210,](#page-16-16) [212,](#page-16-31) [275\]](#page-18-4).


    We aggregate the current consumption data points from Figure [7](#page-22-3) by
    time and DRAM standard. Figure [8](#page-23-1) shows the minimum, median, and
    maximum values (in log scale) across each 5-year period (top) and DRAM standard
    (bottom). The data shows that the median IDD0/IDD4R/IDD5B increased by 12.22/20.91/26.97%
    per year on average between 1970 and 2000 but decreased by 4.62/1.00/0.13% between
    2000 and 2015[20](#page-22-4)


    #### A.3. Relationship Between Timings and Currents


    Finally, we examine the high-level relationship between the timing parameter and
    current consumption values. We find that the two are generally inversely related,
    which follows from the general principle that faster DRAM chips (i.e., lower


    <span id="page-22-4"></span><sup>20</sup>Similar to Section [A.1,](#page-21-3)
    we omit the 2020 data point because the firstgeneration DDR5 chips exhibit outlying
    data values (e.g., no data reported for IDD5B in the datasheets). for an overall
    increase of 0.96/11.5/17.5% between 1970 and 2015.


    <span id="page-22-3"></span>![](_page_22_Figure_16.jpeg)


    Figure 7: Evolution of key DRAM current consumption values (shown in log scale)
    across years (top) and chip capacities (bottom) separated by DRAM standard.


    <span id="page-22-0"></span><sup>18</sup>We omit the 2020 data point because 2020
    shows a regression in CAS latency due to first-generation DDR5 chips, which we
    believe is not representative because of its immature technology.


    <span id="page-22-2"></span><sup>19</sup>We omit tRCD and tRAS for the 1 Kib chips
    because they do not use a row address strobe (RAS) signal.


    <span id="page-23-1"></span>![](_page_23_Figure_0.jpeg)


    Figure 8: Evolution of the minimum, median, and maximum of key DRAM current consumption
    value (shown in log scale) for each 5-year period (top) and DRAM standard (bottom).


    ![](_page_23_Figure_2.jpeg)


    Figure 9: Relationship between the four timing parameters and IDD4R separated
    by DRAM standard.


    <span id="page-23-2"></span>![](_page_23_Figure_4.jpeg)


    Figure 10: Evolution of tREF I (left y-axis) and tRF C (right y-axis) across DRAM
    chips of increasing storage capacity.


    timing parameters) require more power (i.e., increased current consumption values).
    Figure [9](#page-23-1) illustrates this relationship for the four timing parameters
    studied in Section [A.1](#page-21-3) relative to IDD4R (i.e., the current consumption
    of read operations).


    ### <span id="page-23-0"></span>A.4. DRAM Refresh Timing Trends


    DRAM refresh is governed by two key timing parameters:


    - tREF I (refresh interval): time between consecutive refresh commands sent by
    the memory controller.

    - tRF C : duration of a single refresh command.


    Figure [10](#page-23-2) shows how tREF I (left y-axis) and tRF C (right yaxis)
    evolved across the DRAM chips in our study. We group chips by storage capacity
    because DRAM refresh timings are closely related to capacity: higher-capacity
    chips using the same technology require more time or more refresh operations to
    fully refresh. The error bars show the minimum and maximum values observed across
    all chips for any given chip capacity.


    We make three observations. First, tREF I is shorter for higher-capacity DRAM
    chips (e.g., 62.5 µs for an asynchronous 1 Kib chip versus 3.9 µs for a 16 Gib
    DDR5 chip). This is consistent with the fact that higher-capacity chips require
    more frequent refreshing. Second, tRF C first decreases with chip capacity (e.g.,
    900 ns for an asynchronous 1 Kib chip versus 54 ns for a 32 Mib SDRAM chip) but
    then increases (e.g., to 350 ns for a 16 Gib DDR4 chip). This is because rapid
    improvements in row access times (and therefore refresh timings) initially outpaced
    the increase in storage capacity. However, starting around 512 Mib chip sizes,
    row access times improved much more slowly (as observed in Section [A.1\)](#page-21-3)
    while storage capacity continued to increase. This matches our analysis of the
    refresh penalty in Section [3.1.3.](#page-4-5) Third, the variation in tRF C across
    chips of each capacity (illustrated using the error bars) decreased for higher-capacity
    chips. This is because higher-capacity chips follow more recent DRAM standards
    (i.e., DDRn), which standardize DRAM auto refresh timings. In contrast, older
    DRAM chips were simply refreshed as quickly as their rows could be accessed (e.g.,
    every tRC using RAS-only refresh).


    ## <span id="page-24-0"></span>B. Survey Data Sources


    Table [5](#page-24-1) itemizes the 58 DRAM datasheets used for our survey in Appendix
    [A.](#page-21-0) For each datasheet, we show the DRAM chip manufacturer, model
    number, DRAM standard, year, and capacity. Our full dataset is available online
    [\[136\]](#page-15-15).


    | Year | Manufacturer      | Model Number | Datasheet Source | DRAM Standard |
    Capacity per Chip (Kib) |

    |------|-------------------|--------------|------------------|---------------|-------------------------|

    | 1970 | Intel             | 1103         | [S1]             | Asynchronous  |
    1                       |

    | 1971 | Mostek            | MK4006       | [S2]             | Asynchronous  |
    1                       |

    | 1973 | Mostek            | MK4096       | [S3]             | Asynchronous  |
    4                       |

    | 1976 | Mostek            | MK4027       | [S4]             | PM            |
    4                       |

    | 1976 | Mostek            | MK4116P      | [S5]             | PM            |
    16                      |

    | 1978 | Fairchild         | F4116        | [S6]             | PM            |
    16                      |

    | 1979 | Intel             | 2118         | [S7]             | PM            |
    16                      |

    | 1981 | Mitsubishi        | M5K4164ANP   | [S8]             | PM            |
    64                      |

    | 1982 | Mostek            | MK4564       | [S9]             | PM            |
    64                      |

    | 1984 | NTE               | NTE4164      | [S10]            | PM            |
    64                      |

    | 1984 | Texas Instruments | TMS4416      | [S11]            | PM            |
    64                      |

    | 1985 | Mitsubishi        | M5M4256P     | [S12]            | PM            |
    256                     |

    | 1987 | Samsung           | KM41464A     | [S13]            | PM            |
    256                     |

    | 1987 | Texas Instruments | TMS4464      | [S14]            | PM            |
    256                     |

    | 1989 | Texas Instruments | SMJ4464      | [S15]            | PM            |
    256                     |

    | 1990 | Intel             | 21256        | [S16]            | PM            |
    256                     |

    | 1991 | Mitsubishi        | M5M44100     | [S17]            | FPM           |
    4096                    |

    | 1993 | Mitsubishi        | M5M44256B    | [S18]            | FPM           |
    1024                    |

    | 1993 | Mosel Vitelic     | V404J8       | [S19]            | FPM           |
    8192                    |

    | 1995 | Siemens           | HYB511000BJ  | [S20]            | FPM           |
    1024                    |

    | 1997 | Hyundai           | HY5118164B   | [S21]            | EDO           |
    16384                   |

    | 1997 | Samsung           | KM48S2020CT  | [S22]            | SDRAM         |
    16384                   |

    | 1998 | Micron            | MT48LC4M4A1  | [S23]            | SDRAM         |
    16384                   |

    | 1998 | Mosel Vitelic     | V53C808H     | [S24]            | EDO           |
    8192                    |

    | 1998 | Siemens           | HYB39S16400  | [S25]            | SDRAM         |
    16384                   |

    | 1999 | Samsung           | K4S160822D   | [S26]            | SDRAM         |
    16384                   |

    | 1999 | Samsung           | K4S561632A   | [S27]            | SDRAM         |
    262144                  |

    | 2000 | Amic              | A416316B     | [S28]            | FPM           |
    1024                    |

    | 2000 | ISSI              | IS41LV32256  | [S29]            | EDO           |
    8192                    |

    | 2000 | Samsung           | K4D623237A5  | [S30]            | DDR           |
    65536                   |

    | 2001 | Alliance          | AS4C256K16E0 | [S31]            | EDO           |
    4096                    |

    | 2001 | Alliance          | AS4C4M4FOQ   | [S32]            | FPM           |
    16384                   |

    | 2001 | ISSI              | IS41C4400X   | [S33]            | EDO           |
    16384                   |

    | 2001 | Micron            | MT46V2M32    | [S34]            | DDR           |
    65536                   |

    | 2001 | Micron            | MT46V32M4    | [S35]            | DDR           |
    131072                  |

    | 2001 | Mosel Vitelic     | V58C265164S  | [S36]            | DDR           |
    65536                   |

    | 2001 | TM Tech           | T224160B     | [S37]            | FPM           |
    4096                    |

    | 2003 | Micron            | MT46V64M4    | [S38]            | DDR           |
    262144                  |

    | 2003 | Samsung           | K4S560432E   | [S39]            | SDRAM         |
    262144                  |

    | 2005 | Amic              | A43L0632     | [S40]            | SDRAM         |
    32768                   |

    | 2006 | Elite             | M52S32321A   | [S41]            | SDRAM         |
    32768                   |

    | 2006 | ISSI              | IS42S81600B  | [S42]            | SDRAM         |
    131072                  |

    | 2006 | Samsung           | K4T51043QC   | [S43]            | DDR2          |
    524288                  |

    | 2007 | Micron            | MT47H256M4   | [S44]            | DDR2          |
    1048576                 |

    | 2010 | Samsung           | K4B4G0446A   | [S45]            | DDR3          |
    4194304                 |

    | 2011 | Hynix             | H5TQ4G43MFR  | [S46]            | DDR3          |
    4194304                 |

    | 2011 | Nanya             | NT5CB512M    | [S47]            | DDR3          |
    2097152                 |

    | 2013 | Samsung           | K4B4G0446A   | [S48]            | DDR3          |
    4194304                 |

    | 2015 | Micron            | MT40A2G      | [S49]            | DDR4          |
    8388608                 |

    | 2016 | Hynix             | H5AN4G4NAFR  | [S50]            | DDR4          |
    4194304                 |

    | 2016 | Samsung           | K4A8G165WC   | [S51]            | DDR4          |
    8388608                 |

    | 2017 | Hynix             | H5AN8G4NAFR  | [S52]            | DDR4          |
    8388608                 |

    | 2018 | Micron            | MT40A        | [S53]            | DDR4          |
    16777216                |

    | 2019 | Hynix             | H5AN8G4NCJR  | [S54]            | DDR4          |
    8388608                 |

    | 2019 | Samsung           | K4AAG045WA   | [S55]            | DDR4          |
    16777216                |

    | 2020 | Samsung           | K4AAG085WA   | [S56]            | DDR4          |
    16777216                |

    | 2021 | Hynix             | HMCG66MEB    | [S57]            | DDR5          |
    16777216                |

    | 2021 | Micron            | MT60B1G16    | [S58]            | DDR5          |
    16777216                |


    <span id="page-24-1"></span>Table 5: List of DRAM chip datasheets used in our
    DRAM trends survey.


    ## Survey Sources


    - [S1] Intel, "1103," [http://www.decadecounter.com/vta/pdf/Intel%20Memo](http://www.decadecounter.com/vta/pdf/Intel%20Memory%20Design%20Handbook%20[1973-08].pdf)
    [ry%20Design%20Handbook%20\[1973-08\].pdf,](http://www.decadecounter.com/vta/pdf/Intel%20Memory%20Design%20Handbook%20[1973-08].pdf)
    1970.

    - [S2] Mostek, "MK4006," [https://usermanual.wiki/Pdf/1974MostekIntegrat](https://usermanual.wiki/Pdf/1974MostekIntegratedCircuitGuide.1468557856/view)
    [edCircuitGuide.1468557856/view,](https://usermanual.wiki/Pdf/1974MostekIntegratedCircuitGuide.1468557856/view)
    1971.

    - [S3] Mostek, "MK4096," [https://console5.com/techwiki/images/0/04/MK](https://console5.com/techwiki/images/0/04/MK4096.pdf)
    [4096.pdf,](https://console5.com/techwiki/images/0/04/MK4096.pdf) 1973.

    - [S4] Mostek, "MK4027," [https://console5.com/techwiki/images/d/df/MK](https://console5.com/techwiki/images/d/df/MK4027.pdf)
    [4027.pdf,](https://console5.com/techwiki/images/d/df/MK4027.pdf) 1976.

    - [S5] Mostek, "MK4116P," [https://console5.com/techwiki/images/8/85/MK](https://console5.com/techwiki/images/8/85/MK4116.pdf)
    [4116.pdf,](https://console5.com/techwiki/images/8/85/MK4116.pdf) 1976.

    - [S6] Fairchild, "F4116," [http://minuszerodegrees.net/memory/4116/datas](http://minuszerodegrees.net/memory/4116/datasheet_F4116.pdf)
    heet [F4116.pdf,](http://minuszerodegrees.net/memory/4116/datasheet_F4116.pdf)
    1978.

    - [S7] Intel, "2118," [https://drive.google.com/file/d/0B9rh9tVI0J5mNDkwZ](https://drive.google.com/file/d/0B9rh9tVI0J5mNDkwZGEwM2QtMzYzNC00YjQ4LTg4NjYtOGY2ZGRkMDMxYjFm/view?resourcekey=0-vyWj--_z6lp7BjZ-6epTng)
    [GEwM2QtMzYzNC00YjQ4LTg4NjYtOGY2ZGRkMDMxYjFm/view](https://drive.google.com/file/d/0B9rh9tVI0J5mNDkwZGEwM2QtMzYzNC00YjQ4LTg4NjYtOGY2ZGRkMDMxYjFm/view?resourcekey=0-vyWj--_z6lp7BjZ-6epTng)
    [?resourcekey=0-vyWj--](https://drive.google.com/file/d/0B9rh9tVI0J5mNDkwZGEwM2QtMzYzNC00YjQ4LTg4NjYtOGY2ZGRkMDMxYjFm/view?resourcekey=0-vyWj--_z6lp7BjZ-6epTng)
    z6lp7BjZ-6epTng, 1979.

    - [S8] Mitsubishi, "M5K4164ANP," [https://datasheetspdf.com/pdf-file/1110](https://datasheetspdf.com/pdf-file/1110696/Mitsubishi/M5K4164ANP-15/1)
    [696/Mitsubishi/M5K4164ANP-15/1,](https://datasheetspdf.com/pdf-file/1110696/Mitsubishi/M5K4164ANP-15/1)
    1981.

    - [S9] Mostek, "MK4564," [http://www.minuszerodegrees.net/memory/4164](http://www.minuszerodegrees.net/memory/4164/datasheet_MK4564-15_and_MK4564-20.pdf)
    /datasheet MK4564-15 and [MK4564-20.pdf,](http://www.minuszerodegrees.net/memory/4164/datasheet_MK4564-15_and_MK4564-20.pdf)
    1982.

    - [S10] NTE, "NTE4164," [http://www.farnell.com/datasheets/1905614.pdf,](http://www.farnell.com/datasheets/1905614.pdf)
    1984.

    - [S11] Texas Instruments, "TMS4416," [http://pdf.datasheetcatalog.com/dat](http://pdf.datasheetcatalog.com/datasheets2/81/817426_1.pdf)
    [asheets2/81/817426](http://pdf.datasheetcatalog.com/datasheets2/81/817426_1.pdf)
    1.pdf, 1984.

    - [S12] Mitsubishi, "M5M4256P," [http://bitsavers.trailing-edge.com/comp](http://bitsavers.trailing-edge.com/components/mitsubishi/_dataBooks/1985_Mitsubishi_IC_Memories.pdf)
    [onents/mitsubishi/](http://bitsavers.trailing-edge.com/components/mitsubishi/_dataBooks/1985_Mitsubishi_IC_Memories.pdf)
    dataBooks/1985 Mitsubishi IC Memories.pdf, 1985.

    - [S13] Samsung, "KM41464A," [https://console5.com/techwiki/images/2/24/](https://console5.com/techwiki/images/2/24/KM41464A.pdf)
    [KM41464A.pdf,](https://console5.com/techwiki/images/2/24/KM41464A.pdf) 1987.

    - [S14] Texas Instruments, "TMS4464," [https://www.silicon-ark.co.uk/datas](https://www.silicon-ark.co.uk/datasheets/tms4464-datasheet-texas-instruments.pdf)
    [heets/tms4464-datasheet-texas-instruments.pdf,](https://www.silicon-ark.co.uk/datasheets/tms4464-datasheet-texas-instruments.pdf)
    1987.

    - [S15] Texas Instruments, "SMJ4464," [http://65xx.unet.bz/ds/TMS4464.pdf,](http://65xx.unet.bz/ds/TMS4464.pdf)
    1989.

    - [S16] Intel, "21256," [https://drive.google.com/file/d/0B9rh9tVI0J5mMjU2M](https://drive.google.com/file/d/0B9rh9tVI0J5mMjU2MDJlNzItNWVkYy00NWM0LThmZjEtYTkyYjE5MTQxOGI2/view?resourcekey=0-Q0K9JcVvNlgRngkBon8vAw)
    [DJlNzItNWVkYy00NWM0LThmZjEtYTkyYjE5MTQxOGI2/view?re](https://drive.google.com/file/d/0B9rh9tVI0J5mMjU2MDJlNzItNWVkYy00NWM0LThmZjEtYTkyYjE5MTQxOGI2/view?resourcekey=0-Q0K9JcVvNlgRngkBon8vAw)
    [sourcekey=0-Q0K9JcVvNlgRngkBon8vAw,](https://drive.google.com/file/d/0B9rh9tVI0J5mMjU2MDJlNzItNWVkYy00NWM0LThmZjEtYTkyYjE5MTQxOGI2/view?resourcekey=0-Q0K9JcVvNlgRngkBon8vAw)
    1990.

    - [S17] Mitsubishi, "M5M44100," [https://www.datasheetarchive.com/pdf/d](https://www.datasheetarchive.com/pdf/download.php?id=74e4e0a53cd85e765cc396504a798082be9621&type=O&term=M5M44100)
    [ownload.php?id=74e4e0a53cd85e765cc396504a798082be9621&type](https://www.datasheetarchive.com/pdf/download.php?id=74e4e0a53cd85e765cc396504a798082be9621&type=O&term=M5M44100)
    [=O&term=M5M44100,](https://www.datasheetarchive.com/pdf/download.php?id=74e4e0a53cd85e765cc396504a798082be9621&type=O&term=M5M44100)
    1991.

    - [S18] Mitsubishi, "M5M44256B," [https://datasheetspdf.com/pdf-file/1111](https://datasheetspdf.com/pdf-file/1111257/Mitsubishi/M5M44256BP-10/1)
    [257/Mitsubishi/M5M44256BP-10/1,](https://datasheetspdf.com/pdf-file/1111257/Mitsubishi/M5M44256BP-10/1)
    1993.

    - [S19] Mosel Vitelic, "V404J8," [https://www.datasheetarchive.com/pdf/d](https://www.datasheetarchive.com/pdf/download.php?id=d5e7f23416e86a5950d91ea69b37003889d50e&type=M&term=V404J8SU70)
    [ownload.php?id=d5e7f23416e86a5950d91ea69b37003889d50e&type](https://www.datasheetarchive.com/pdf/download.php?id=d5e7f23416e86a5950d91ea69b37003889d50e&type=M&term=V404J8SU70)
    [=M&term=V404J8SU70,](https://www.datasheetarchive.com/pdf/download.php?id=d5e7f23416e86a5950d91ea69b37003889d50e&type=M&term=V404J8SU70)
    1993.

    - [S20] Siemens, "HYB511000BJ," [https://datasheetspdf.com/pdf-file/381513](https://datasheetspdf.com/pdf-file/381513/Siemens/HYB511000BJ-/1)
    [/Siemens/HYB511000BJ-/1,](https://datasheetspdf.com/pdf-file/381513/Siemens/HYB511000BJ-/1)
    1995.

    - [S21] Hyundai, "HY5118164B," [https://www.datasheetarchive.com/pdf/d](https://www.datasheetarchive.com/pdf/download.php?id=7413fece3e0e5dee9b73ec0bb7f6c53afd4c99&type=P&term=HY5118164B)
    [ownload.php?id=7413fece3e0e5dee9b73ec0bb7f6c53afd4c99&type](https://www.datasheetarchive.com/pdf/download.php?id=7413fece3e0e5dee9b73ec0bb7f6c53afd4c99&type=P&term=HY5118164B)
    [=P&term=HY5118164B,](https://www.datasheetarchive.com/pdf/download.php?id=7413fece3e0e5dee9b73ec0bb7f6c53afd4c99&type=P&term=HY5118164B)
    1997.

    - [S22] Samsung, "KM48S2020CT," [http://www.maxim4u.com/view](http://www.maxim4u.com/view_online.php?id=1777838&file=0390\km48s2020ct-fl_3917068.pdf)
    online. [php?id=1777838&file=0390](http://www.maxim4u.com/view_online.php?id=1777838&file=0390\km48s2020ct-fl_3917068.pdf)\km48s2020ct-fl
    3917068.pdf, 1997.

    - [S23] Micron, "MT48LC4M4A1," [https://www.digchip.com/datasheets/d](https://www.digchip.com/datasheets/download_datasheet.php?id=688351&part-number=MT48LC2M8A1)
    ownload [datasheet.php?id=688351&part-number=MT48LC2M8A1,](https://www.digchip.com/datasheets/download_datasheet.php?id=688351&part-number=MT48LC2M8A1)
    1998.

    - [S24] Mosel Vitelic, "V53C808H," [https://www.digchip.com/datasheets](https://www.digchip.com/datasheets/download_datasheet.php?id=1031590&part-number=V53C808H)
    /download [datasheet.php?id=1031590&part-number=V53C808H,](https://www.digchip.com/datasheets/download_datasheet.php?id=1031590&part-number=V53C808H)
    1998.

    - [S25] Siemens, "HYB39S16400," [https://www.digchip.com/datasheets/dow](https://www.digchip.com/datasheets/download_datasheet.php?id=390213&part-number=HYB39S16160AT-10)
    nload [datasheet.php?id=390213&part-number=HYB39S16160AT-1](https://www.digchip.com/datasheets/download_datasheet.php?id=390213&part-number=HYB39S16160AT-10)
    [0,](https://www.digchip.com/datasheets/download_datasheet.php?id=390213&part-number=HYB39S16160AT-10)
    1998.

    - [S26] Samsung, "K4S160822D," [http://pdf.datasheetcatalog.com/datasheet](http://pdf.datasheetcatalog.com/datasheet/SamsungElectronic/mXtvtzs.pdf)
    [/SamsungElectronic/mXtvtzs.pdf,](http://pdf.datasheetcatalog.com/datasheet/SamsungElectronic/mXtvtzs.pdf)
    1999.

    - [S27] Samsung, "K4S561632A," [https://www.datasheetarchive.com/pdf/d](https://www.datasheetarchive.com/pdf/download.php?id=fd48625bbd5e92da34308233eb404f7635e593&type=M&term=K4S561632A)
    [ownload.php?id=fd48625bbd5e92da34308233eb404f7635e593&type](https://www.datasheetarchive.com/pdf/download.php?id=fd48625bbd5e92da34308233eb404f7635e593&type=M&term=K4S561632A)
    [=M&term=K4S561632A,](https://www.datasheetarchive.com/pdf/download.php?id=fd48625bbd5e92da34308233eb404f7635e593&type=M&term=K4S561632A)
    1999.

    - [S28] Amic, "A416316B," [https://pdf1.alldatasheet.com/datasheet-pdf/vie](https://pdf1.alldatasheet.com/datasheet-pdf/view/55599/AMICC/A416316BS-35.html)
    [w/55599/AMICC/A416316BS-35.html,](https://pdf1.alldatasheet.com/datasheet-pdf/view/55599/AMICC/A416316BS-35.html)
    2000.

    - [S29] ISSI, "IS41LV32256," [https://www.digchip.com/datasheets/downloa](https://www.digchip.com/datasheets/download_datasheet.php?id=442395&part-number=IS41LV32256)
    d [datasheet.php?id=442395&part-number=IS41LV32256,](https://www.digchip.com/datasheets/download_datasheet.php?id=442395&part-number=IS41LV32256)
    2000.

    - [S30] Samsung, "K4D623237A5," [https://www.datasheetarchive.com/pdf/d](https://www.datasheetarchive.com/pdf/download.php?id=1ddb8613b8da9b267a1e546d9442e36e6a9d62&type=M&term=K4D623237A)
    [ownload.php?id=1ddb8613b8da9b267a1e546d9442e36e6a9d62&ty](https://www.datasheetarchive.com/pdf/download.php?id=1ddb8613b8da9b267a1e546d9442e36e6a9d62&type=M&term=K4D623237A)
    [pe=M&term=K4D623237A,](https://www.datasheetarchive.com/pdf/download.php?id=1ddb8613b8da9b267a1e546d9442e36e6a9d62&type=M&term=K4D623237A)
    2000.

    - [S31] Alliance, "AS4C256K16E0," [http://www.dexsilicium.com/Alliance](http://www.dexsilicium.com/Alliance_AS4C256K16E0.pdf)
    A [S4C256K16E0.pdf,](http://www.dexsilicium.com/Alliance_AS4C256K16E0.pdf) 2001.

    - [S32] Alliance, "AS4C4M4FOQ," [https://www.datasheetarchive.com/pdf/d](https://www.datasheetarchive.com/pdf/download.php?id=c12832d7fa384ccc466fa8f0d33d169415452d&type=P&term=409--1%252Ftds%252B0541)
    [ownload.php?id=c12832d7fa384ccc466fa8f0d33d169415452d&type](https://www.datasheetarchive.com/pdf/download.php?id=c12832d7fa384ccc466fa8f0d33d169415452d&type=P&term=409--1%252Ftds%252B0541)
    [=P&term=409--1%252Ftds%252B0541,](https://www.datasheetarchive.com/pdf/download.php?id=c12832d7fa384ccc466fa8f0d33d169415452d&type=P&term=409--1%252Ftds%252B0541)
    2001.

    - [S33] ISSI, "IS41C4400X," [https://datasheetspdf.com/pdf-file/1237264/Inte](https://datasheetspdf.com/pdf-file/1237264/IntegratedSiliconSolution/IS41LV44002/1)
    [gratedSiliconSolution/IS41LV44002/1,](https://datasheetspdf.com/pdf-file/1237264/IntegratedSiliconSolution/IS41LV44002/1)
    2001.

    - [S34] Micron, "MT46V2M32," [https://datasheetspdf.com/pdf-file/534262](https://datasheetspdf.com/pdf-file/534262/MicronTechnology/MT46V2M32/1)
    [/MicronTechnology/MT46V2M32/1,](https://datasheetspdf.com/pdf-file/534262/MicronTechnology/MT46V2M32/1)
    2001.

    - [S35] Micron, "MT46V32M4," [https://www.compel.ru/item-pdf/b6f0ed7c2](https://www.compel.ru/item-pdf/b6f0ed7c2d40f9dc96e3fa571607bc09/ps/micron~mt46v8m16.pdf)
    [d40f9dc96e3fa571607bc09/ps/micron](https://www.compel.ru/item-pdf/b6f0ed7c2d40f9dc96e3fa571607bc09/ps/micron~mt46v8m16.pdf)∼mt46v8m16.pdf,
    2001.

    - [S36] Mosel Vitelic, "V58C265164S," [https://datasheetspdf.com/pdf-file/29](https://datasheetspdf.com/pdf-file/295988/MoselVitelicCorp/V58C265164S/1)
    [5988/MoselVitelicCorp/V58C265164S/1,](https://datasheetspdf.com/pdf-file/295988/MoselVitelicCorp/V58C265164S/1)
    2001.

    - [S37] TM Tech, "T224160B," [https://www.digchip.com/datasheets/downloa](https://www.digchip.com/datasheets/download_datasheet.php?id=945886&part-number=T224160B)
    d [datasheet.php?id=945886&part-number=T224160B,](https://www.digchip.com/datasheets/download_datasheet.php?id=945886&part-number=T224160B)
    2001.

    - [S38] Micron, "MT46V64M4," [https://media-www.micron.com/-/media/cl](https://media-www.micron.com/-/media/client/global/documents/products/data-sheet/dram/ddr1/256mb_ddr.pdf?rev=7d969af24d6d4b74a34e427f350b1c77)
    [ient/global/documents/products/data-sheet/dram/ddr1/256mb](https://media-www.micron.com/-/media/client/global/documents/products/data-sheet/dram/ddr1/256mb_ddr.pdf?rev=7d969af24d6d4b74a34e427f350b1c77)
    ddr [.pdf?rev=7d969af24d6d4b74a34e427f350b1c77,](https://media-www.micron.com/-/media/client/global/documents/products/data-sheet/dram/ddr1/256mb_ddr.pdf?rev=7d969af24d6d4b74a34e427f350b1c77)
    2003.

    - [S39] Samsung, "K4S560432E," [https://ru.datasheetbank.com/datasheet-d](https://ru.datasheetbank.com/datasheet-download/429400/1/Samsung/K4S560432E-UC75)
    [ownload/429400/1/Samsung/K4S560432E-UC75,](https://ru.datasheetbank.com/datasheet-download/429400/1/Samsung/K4S560432E-UC75)
    2003.

    - [S40] Amic, "A43L0632," [https://datasheetspdf.com/pdf-file/672656/AMI](https://datasheetspdf.com/pdf-file/672656/AMICTechnology/A43L0632/1)
    [CTechnology/A43L0632/1,](https://datasheetspdf.com/pdf-file/672656/AMICTechnology/A43L0632/1)
    2005.

    - [S41] Elite, "M52S32321A," [http://www.farnell.com/datasheets/62304.pdf,](http://www.farnell.com/datasheets/62304.pdf)
    2006.

    - [S42] ISSI, "IS42S81600B," [https://datasheetspdf.com/pdf-file/591012/ISSI/](https://datasheetspdf.com/pdf-file/591012/ISSI/IS42S81600B/1)
    [IS42S81600B/1,](https://datasheetspdf.com/pdf-file/591012/ISSI/IS42S81600B/1)
    2006.

    - [S43] Samsung, "K4T51043QC," [https://www.digchip.com/datasheets/dow](https://www.digchip.com/datasheets/download_datasheet.php?id=1088989&part-number=K4T51083QC)
    nload [datasheet.php?id=1088989&part-number=K4T51083QC,](https://www.digchip.com/datasheets/download_datasheet.php?id=1088989&part-number=K4T51083QC)
    2006.

    - [S44] Micron, "MT47H256M4," [https://media-www.micron.com/-/media/](https://media-www.micron.com/-/media/client/global/documents/products/data-sheet/dram/ddr2/1gb_ddr2.pdf?rev=854b480189b84d558d466bc18efe270c)
    [client/global/documents/products/data-sheet/dram/ddr2/1gb](https://media-www.micron.com/-/media/client/global/documents/products/data-sheet/dram/ddr2/1gb_ddr2.pdf?rev=854b480189b84d558d466bc18efe270c)
    ddr2. [pdf?rev=854b480189b84d558d466bc18efe270c,](https://media-www.micron.com/-/media/client/global/documents/products/data-sheet/dram/ddr2/1gb_ddr2.pdf?rev=854b480189b84d558d466bc18efe270c)
    2007.

    - [S45] Samsung, "K4B4G0446A," [https://www.samsung.com/semiconducto](https://www.samsung.com/semiconductor/global.semi/file/resource/2017/11/DS_K4B4G0846D-BC_Rev123-0.pdf)
    [r/global.semi/file/resource/2017/11/DS](https://www.samsung.com/semiconductor/global.semi/file/resource/2017/11/DS_K4B4G0846D-BC_Rev123-0.pdf)
    K4B4G0846D-BC Rev123-0 [.pdf,](https://www.samsung.com/semiconductor/global.semi/file/resource/2017/11/DS_K4B4G0846D-BC_Rev123-0.pdf)
    2010.

    - [S46] Hynix, "H5TQ4G43MFR," [https://pdf1.alldatasheet.com/datasheet-p](https://pdf1.alldatasheet.com/datasheet-pdf/view/533445/HYNIX/H5TQ4G63MFR-H9C.html)
    [df/view/533445/HYNIX/H5TQ4G63MFR-H9C.html,](https://pdf1.alldatasheet.com/datasheet-pdf/view/533445/HYNIX/H5TQ4G63MFR-H9C.html)
    2011.

    - [S47] Nanya, "NT5CB512M," [http://www.sunnyqi.com/upLoad/product/m](http://www.sunnyqi.com/upLoad/product/month_1308/NT5CB256M8GN.pdf)
    onth [1308/NT5CB256M8GN.pdf,](http://www.sunnyqi.com/upLoad/product/month_1308/NT5CB256M8GN.pdf)
    2011.

    - [S48] Samsung, "K4B4G0446A," [https://www.samsung.com/semiconducto](https://www.samsung.com/semiconductor/global.semi/file/resource/2017/11/DS_K4B4G0846D-BC_Rev123-0.pdf)
    [r/global.semi/file/resource/2017/11/DS](https://www.samsung.com/semiconductor/global.semi/file/resource/2017/11/DS_K4B4G0846D-BC_Rev123-0.pdf)
    K4B4G0846D-BC Rev123-0 [.pdf,](https://www.samsung.com/semiconductor/global.semi/file/resource/2017/11/DS_K4B4G0846D-BC_Rev123-0.pdf)
    2013.

    - [S49] Micron, "MT40A2G," [https://www.micron.com/-/media/client/globa](https://www.micron.com/-/media/client/global/documents/products/data-sheet/dram/ddr4/8gb_ddr4_sdram.pdf)
    [l/documents/products/data-sheet/dram/ddr4/8gb](https://www.micron.com/-/media/client/global/documents/products/data-sheet/dram/ddr4/8gb_ddr4_sdram.pdf)
    ddr4 sdram.pdf, 2015.

    - [S50] Hynix, "H5AN4G4NAFR," [https://datasheetspdf.com/pdf-file/130916](https://datasheetspdf.com/pdf-file/1309166/HynixSemiconductor/H5AN4G8NAFR-xxC/1)
    [6/HynixSemiconductor/H5AN4G8NAFR-xxC/1,](https://datasheetspdf.com/pdf-file/1309166/HynixSemiconductor/H5AN4G8NAFR-xxC/1)
    2016.

    - [S51] Samsung, "K4A8G165WC," [https://www.samsung.com/semiconduc](https://www.samsung.com/semiconductor/global.semi/file/resource/2017/12/x16%20only_8G_C_DDR4_Samsung_Spec_Rev1.5_Apr.17.pdf)
    [tor/global.semi/file/resource/2017/12/x16%20only](https://www.samsung.com/semiconductor/global.semi/file/resource/2017/12/x16%20only_8G_C_DDR4_Samsung_Spec_Rev1.5_Apr.17.pdf)
    8G C DDR4 S amsung Spec Rev1.5 [Apr.17.pdf,](https://www.samsung.com/semiconductor/global.semi/file/resource/2017/12/x16%20only_8G_C_DDR4_Samsung_Spec_Rev1.5_Apr.17.pdf)
    2016.

    - [S52] Hynix, "H5AN8G4NAFR," [https://www.digchip.com/datasheets/dow](https://www.digchip.com/datasheets/download_datasheet.php?id=217237&part-number=H5AN8G8NAFR&type=pn2)
    nload [datasheet.php?id=217237&part-number=H5AN8G8NAFR&](https://www.digchip.com/datasheets/download_datasheet.php?id=217237&part-number=H5AN8G8NAFR&type=pn2)
    [type=pn2,](https://www.digchip.com/datasheets/download_datasheet.php?id=217237&part-number=H5AN8G8NAFR&type=pn2)
    2017.

    - [S53] Micron, "MT40A," [https://www.micron.com/-/media/client/global/](https://www.micron.com/-/media/client/global/documents/products/data-sheet/dram/ddr4/16gb_ddr4_sdram.pdf)
    [documents/products/data-sheet/dram/ddr4/16gb](https://www.micron.com/-/media/client/global/documents/products/data-sheet/dram/ddr4/16gb_ddr4_sdram.pdf)
    ddr4 sdram.pdf, 2018.

    - [S54] Hynix, "H5AN8G4NCJR," [http://www.hytic.net/upload/files/2019/10](http://www.hytic.net/upload/files/2019/10/SK%20Hynix%20%20%20-H5AN8G4NCJR.pdf)
    [/SK%20Hynix%20%20%20-H5AN8G4NCJR.pdf,](http://www.hytic.net/upload/files/2019/10/SK%20Hynix%20%20%20-H5AN8G4NCJR.pdf)
    2019.

    - [S55] Samsung, "K4AAG045WA," [https://www.memory-distributor.com/p](https://www.memory-distributor.com/pub/media/downloads/datasheets/K4AAG085WA_BIxx.pdf)
    [ub/media/downloads/datasheets/K4AAG085WA](https://www.memory-distributor.com/pub/media/downloads/datasheets/K4AAG085WA_BIxx.pdf)
    BIxx.pdf, 2019.

    - [S56] Samsung, "K4AAG085WA," [https://www.memory-distributor.com/p](https://www.memory-distributor.com/pub/media/downloads/datasheets/K4AAG085WA_BIxx.pdf)
    [ub/media/downloads/datasheets/K4AAG085WA](https://www.memory-distributor.com/pub/media/downloads/datasheets/K4AAG085WA_BIxx.pdf)
    BIxx.pdf, 2020.

    - [S57] Hynix, "HMCG66MEB," [https://gzhls.at/blob/ldb/b/e/5/8/5bc212f7c9](https://gzhls.at/blob/ldb/b/e/5/8/5bc212f7c92604fd3737505ee4c96014733c.pdf)
    [2604fd3737505ee4c96014733c.pdf,](https://gzhls.at/blob/ldb/b/e/5/8/5bc212f7c92604fd3737505ee4c96014733c.pdf)
    2021.

    - [S58] Micron, "MT60B1G16," [https://media-www.micron.com/-/media/clie](https://media-www.micron.com/-/media/client/global/documents/products/data-sheet/dram/ddr5/16gb_ddr5_sdram_diereva.pdf?rev=c95e4a49184145f18e105cc41e0ee643)
    [nt/global/documents/products/data-sheet/dram/ddr5/16gb](https://media-www.micron.com/-/media/client/global/documents/products/data-sheet/dram/ddr5/16gb_ddr5_sdram_diereva.pdf?rev=c95e4a49184145f18e105cc41e0ee643)
    ddr5 s dram [diereva.pdf?rev=c95e4a49184145f18e105cc41e0ee643,](https://media-www.micron.com/-/media/client/global/documents/products/data-sheet/dram/ddr5/16gb_ddr5_sdram_diereva.pdf?rev=c95e4a49184145f18e105cc41e0ee643)
    2021.'
  decisions:
    evaluation_prompt: 'Disqualified: no evaluation. Reason: The paper lacks sections
      or elements indicating experimental, empirical, or quantitative analysis.'
- title: "WideSA: A High Array Utilization Mapping Scheme for Uniform Recurrences\n\
    \  on the Versal ACAP Architecture"
  abstract: "The Versal Adaptive Compute Acceleration Platform (ACAP) is a new\narchitecture\
    \ that combines AI Engines (AIEs) with reconfigurable fabric. This\narchitecture\
    \ offers significant acceleration potential for uniform recurrences\nin various\
    \ domains, such as deep learning, high-performance computation, and\nsignal processing.\
    \ However, efficiently mapping these computations onto the\nVersal ACAP architecture\
    \ while achieving high utilization of AIEs poses a\nchallenge.\n  To address this\
    \ issue, we propose a mapping scheme called \\fname, which aims\nto accelerate\
    \ uniform recurrences on the Versal ACAP architecture by leveraging\nthe features\
    \ of both the hardware and the computations. Considering the array\narchitecture\
    \ of AIEs, our approach utilizes space-time transformations based on\nthe polyhedral\
    \ model to generate legally optimized systolic array mappings.\nConcurrently,\
    \ we have developed a routing-aware PLIO assignment algorithm\ntailored for communication\
    \ on the AIE array, and the algorithm aims at\nsuccessful compilation while maximizing\
    \ array utilization. Furthermore, we\nintroduce an automatic mapping framework.\
    \ This framework is designed to\ngenerate the corresponding executable code for\
    \ uniform recurrences, which\nencompasses the AIE kernel program, programmable\
    \ logic bitstreams, and the host\nprogram. The experimental results validate the\
    \ effectiveness of our mapping\nscheme. Specifically, when applying our scheme\
    \ to matrix multiplication\ncomputations on the VCK5000 board, we achieve a throughput\
    \ of 4.15TOPS on float\ndata type, which is 1.11$\\times$ higher compared to the\
    \ state-of-the-art\naccelerator on the Versal ACAP architecture."
  url: http://arxiv.org/abs/2401.16792v1
  keywords: Mapping, Re-configurable Array Architecture, Versal ACAP
  document: '#### I. INTRODUCTION


    Modern heterogeneous FPGA architectures, like AMD/Xilinx Versal Adaptive Compute
    Acceleration Platform (ACAP) [\[1\]](#page-5-0), combine AI Engines (AIEs) with
    programmable logic (PL) to boost applications in the AI and intelligent signal
    processing domains. In these domains, uniform recurrences [\[2\]](#page-5-1),
    which comprise nested loops with uniform dependencies, are prevalent types of
    computations. Regrettably, there is currently a lack of established development
    methodologies for efficiently mapping large-scale uniform recurrences onto the
    Versal ACAP architecture with high utilization of AI Engines.


    The ACAP architecture comprise an array of several hundred AIE cores, such as
    8×50 in the VC1902 architecture [\[3\]](#page-5-2), interconnected through a mesh
    network-on-chip (NoC). Each AIE core consists of vector processing and load/store
    units, functioning as a very-long-instruction-word (VLIW) [\[4\]](#page-5-3) processor
    to deliver high-performance vectorized computations. To facilitate communication
    among the AIE cores, the NoC is utilized for inter-core communication, enabling
    efficient data transfers between cores. Moreover, neighboring cores utilize shared
    buffers, providing higher bandwidth for data exchange. When it comes to data transfer
    to and from the AIEs, there are hundreds of I/O ports available, supporting terabytes
    of bandwidth.


    As ACAP demonstrates a remarkable capacity for intense computation, developing
    acceleration designs on the architecture has become an urgent trend in recent
    times. However, current efforts have not succeeded in achieving high utilization
    of the AIE array. For example, Vitis-AI [\[5\]](#page-5-4) introduces the DPU
    [\[6\]](#page-5-5) for the VC1902 architecture, but only accomplishes a 64% AIE
    utilization. There are several ongoing challenges associated with developing designs
    with high array utilization on the Versal ACAP architecture:


    - Increased programming complexity: Higher AIE utilization results in more cores
    that need to be programmed with certain intrinsics. In some situations, different
    cores execute different programs, necessitating significant human effort to develop
    such accelerators.

    - Increased placement and routing difficulty: Mapping computations onto the Versal
    ACAP architecture with high utilization of AIEs often necessitates careful placement
    and routing of AIEs and data communications. From the perspective of AIE compilation,
    attaining high AIE utilization typically results in difficulties in placing cores
    and buffers, as well as routing streaming communications on the NoC. For example,
    CHARM [\[7\]](#page-5-6) struggles to compile large designs on Vitis 2022.1.

    - Extended compilation time: The default compilation tools provided by AMD/Xilinx
    Vitis employ ILP algorithms to find placement and routing solutions. Consequently,
    a larger number of cores results in a longer time to find a legal solution.


    To address these challenges, we propose WideSA, a high array utilization mapping
    scheme for uniform recurrences on the Versal ACAP architecture. By leveraging
    the AIE array architecture, we apply space-time transformation and loop nest transformation
    using the polyhedral model, generating systolic-like mappings on the AIE array.
    On one hand, systolic designs assign similar workloads to different cores, enabling
    us to reuse a single core program and thereby reduce human


    <span id="page-1-0"></span>![](_page_1_Figure_0.jpeg)


    Fig. 1: Versal ACAP Architecture


    effort. On the other hand, systolic designs regularize both the placement and
    communication of cores, simplifying the placement and routing process. Additionally,
    we designed a routingaware PLIO assignment algorithm to improve the success rate
    of compilation. We also developed an automatic framework to generate the corresponding
    code for heterogeneous backends, including AIEs, PL, and host. In the evaluation
    section, we demonstrate the effectiveness of WideSA by successfully implementing
    executable acceleration systems for various uniform recurrences, accommodating
    different data types. Our approach achieves high throughput with high utilization
    of AIEs.


    We summarize our contributions as follows:


    - We propose a mapping scheme, based on the polyhedral model, for uniform recurrences
    that generates a systolic design on ACAP with high AIE utilization.

    - We design a routing-aware PLIO assignment algorithm that takes into account
    the characteristics of systolic mappings and the AIE architecture, thereby facilitating
    an efficient compilation process.

    - We develop an automatic framework that generates corresponding code for heterogeneous
    backends based on the mapping results.

    - We achieve high throughput across different computations and data types, outperforming
    state-of-the-art methods.


    #### II. BACKGROUND


    #### *A. Versal ACAP Architecture and Workflow*


    *1) Hardware Features:* AMD/Xilinx has developed the Versal ACAP architecture
    to cater to the increasing demands of next-generation wireless processing and
    machine learning applications. Figure [1](#page-1-0) illustrates the detailed
    architecture of VCK5000, an evaluation kit for Versal ACAP, comprising the CPU,
    PL, and AIE components. The AIE array on VCK5000 consists of 8 × 50 AIE cores,
    with each core capable of generating 128 MACs of int8 data type every cycle at
    a frequency of 1 GHz or higher. Moreover, the AIE cores operate in single-instruction-multiple-data
    (SIMD) mode using


    <span id="page-1-1"></span>TABLE I: Different Data Communication Bandwidth on
    the Versal ACAP Architecture


    | Methods        | Frequency | Bitwidth | Channels | Total      |

    |----------------|-----------|----------|----------|------------|

    | AIE DMA        | 1.25 GHz  | 256 bits | 400      | 15.6 TB/s  |

    | AIE NoC Stream | 1.25 GHz  | 32 bits  | 400      | 1.95 TB/s  |

    | PLIO-PL        | 1.25 GHz  | 128 bits | 78       | 1.52 TB/s  |

    | GMIO-DRAM      | 1.25 GHz  | 64 bits  | 16       | 0.125 TB/s |

    | PL-DRAM        | 0.50 GHz  | -        | 4        | 0.100 TB/s |


    a VLIW pattern, enabling acceleration of a large number of vectorized computations.


    In Figure [1,](#page-1-0) we identify five data transfer methods, including those
    within the AIE array and among the AIE, PL, and DRAM components. These methods
    are referred to as AIE DMA, AIE NoC stream, PLIO-PL, PL-DRAM, and GMIO-DRAM interfaces.
    We profile these data transfer methods on VCK5000 and present the results in Table
    [I.](#page-1-1) Within the AIE array, each AIE core has direct memory access (DMA)
    ports connected to four neighboring local buffers with a width of 256 bits. Using
    the AIE DMA method, a total data transfer rate of up to 15.6 TB/s can be achieved.
    Furthermore, each AIE core is linked to the NoC through a stream interface with
    a width of 32 bits. The data transfer bandwidth through the AIE NoC stream method
    reaches a maximum of 2 TB/s, which is lower compared to the DMA method. The PLIO
    ports, responsible for data communication between the PL and AIE array, can provide
    a maximum bandwidth of 1.52 TB/s. Based on the profiling results, utilizing the
    AIE DMA method for data transfer proves beneficial in overcoming communication
    bottlenecks, aligning with the dataflow in systolic array designs. In terms of
    data communication with DRAM, the bandwidth is approximately 0.1 TB/s, significantly
    lower than the on-chip data transfer methods. This observation inspires us to
    exploit data locality within computations to enhance overall performance.


    *2) Software Programming Model:* AMD/Xilinx offers a development tool for AIEs
    and Versal ACAP integrated into Vitis. The programming model [\[8\]](#page-5-7)
    designed for AIEs consists of two levels: a graph program across the AIE array
    with each node representing an AIE kernel program. The graph program represents
    the dataflow information among AIE kernels and between the AIE and I/O ports.
    The compiler in Vitis transforms the dataflow graph into a subnetwork of physical
    AIE cores, determines the placement of buffers, and configures NoC stream routing.
    Since placement and routing are NP-hard problems, the compiler employs ILP solvers
    to process these two phases. However, as the design scale increases and AIE utilization
    becomes high, finding a legal solution efficiently becomes challenging for the
    solvers [\[9\]](#page-5-8). To address this, incorporating constraints for placement
    and routing helps alleviate the congestions and accelerates the solvers in finding
    solutions. The systolic design scheme provides a regular pattern for placement
    and routing, which is suitable for constructing these constraints.


    ## *B. Uniform Recurrences and Systolic Array Mapping*


    Uniform recurrences refer to computations that consist of nested loops, where
    all dependencies are uniform. These types of computations are commonly found in
    AI and signal processing applications, such as matrix multiplication, 2D convolution,
    FIR filtering, and so on. Several prior works [\[10\]](#page-5-9)– [\[12\]](#page-5-10)
    have focused on generating systolic array designs for uniform recurrences on FPGAs,
    employing the polyhedral model for loop transformations to explore successful
    mappings. The polyhedral model [\[13\]](#page-5-11), [\[14\]](#page-5-12) serves
    as a compilation framework for loop transformation, encompassing space-time transformation,
    latency hiding, SIMD vectorization, fusion, and more. A legal combination of these
    transformations represents a schedule within the polyhedral model, and the goal
    of systolic design mapping is to find the optimal schedule.


    An AIE kernel handles more computations compared to a PE in typical systolic arrays.
    Additionally, specific hardware features of the AIE array differ from those of
    common systolic arrays. As a result, the mapping problem on the Versal ACAP architecture
    is not a straightforward systolic array mapping. Consequently, it is necessary
    to model corresponding transformations and constraints within the polyhedral model,
    an area that has not yet been extensively researched.


    ## III. SYSTOLIC MAPPING SCHEME ON ACAP


    #### *A. Kernel Scope Demarcation*


    According to the programming model of AIEs, it is necessary to demarcate the scope
    of codes mapped to execute on a single AIE core and the outer loop nests to be
    mapped to the AIE array. This demarcation allows us to decompose the mapping problem
    into graph-level mapping and kernel-level mapping, which are independent of each
    other after selecting tiling factors.


    Polygonal tiling [\[15\]](#page-5-13), [\[16\]](#page-5-14), an effective solution
    for workload partitioning in uniform polyhedral domains, plays a crucial role
    in determining the innermost and outer loop nests for tiling. We illustrate the
    tiling process using the MM example with (N0, M0, K0) as the tiling factors, involving
    loop re-indexing, tiling, and rewriting, as depicted in Figure [2.](#page-2-0)
    Building on prior works, we consider the specific features of the AIE array when
    performing the demarcation.


    #### *B. Systolic Mapping Generation*


    To generate systolic array designs on the AIE array following kernel scope demarcation,
    we utilize the polyhedral


    <span id="page-2-0"></span>![](_page_2_Figure_9.jpeg)


    Fig. 2: Kernel Scope Demarcation


    <span id="page-2-1"></span>![](_page_2_Figure_11.jpeg)


    Fig. 3: Polyhedral Model-Based Systolic Mapping


    model, drawing inspiration from AutoSA [\[11\]](#page-5-15), to facilitate loop
    transformations. To be specific, we employ four types of transformation techniques,
    as depicted in Figure [3.](#page-2-1)


    *1) Space-time Transformation:* The first step involves performing space-time
    transformation to map the graph-level loop nests to a systolic array design. We
    identify loops in the outermost loop band with dependence distances no greater
    than one and consider them as candidate space loops. Subsequently, we enumerate
    all possible combinations of space loops from the candidate pool. The selected
    space loops are then permuted in the outermost position, while the loops below
    them are designated as time loops. Due to the constraints imposed by the hardware
    shape of the AIE array, the mapper generates only 1D and 2D systolic arrays. This
    step results in the generation of multiple systolic arrays, each with a unique
    schedule. As shown in Figure [3,](#page-2-1) we choose loops i and j as the space
    loops (on dark gray background) and loop k as the time loop (on light gray background)
    in the MM example.


    *2) Array Partition:* To accommodate the limited number of AIEs in the horizontal
    and vertical directions of the AIE array, array partitioning becomes necessary
    when mapping a large array. In order to achieve this, we apply tiling to the outermost
    permutable loop that contains the space loops. In Figure [3,](#page-2-1) we illustrate
    an example where we tile the outermost loop band in the MM example using the tiling
    factors (N1, M1, K1). The point loops originating from the original loops are
    retained as the space loops. This results in a 2D systolic array with dimensions
    of N<sup>1</sup> × M<sup>1</sup> (on dark gray background).


    *3) Latency Hiding:* Latency hiding plays a crucial role in mitigating the pipeline
    stalls caused by loop-carried dependencies in computational statements. In the
    case of the MM example, the accumulate operations in the statement introduce loop-carried
    dependence within the loop, resulting in long latency in the systolic chain. To
    address this issue, we identify parallel loops in the polyhedral model schedules,
    applies tiling to these loops, and permutes the point loops to the innermost position.
    As an illustration, loops i and j are identified as parallel loops in the MM example.
    We extract them using the tiling factors (N2, M2) and permute the point loops
    to the innermost position. Since there are no loop-carried dependencies on the
    innermost loop, the latency of design reduce as the chain length shortened.


    <span id="page-3-0"></span>![](_page_3_Figure_0.jpeg)


    Fig. 4: Communication Methods for PLIO Ports Utilization Reduction


    *4) Multiple Threading:* As AIE cores execute concurrently, the AIE array inherently
    supports multiple threading. Leveraging this characteristic, utilizing multiple
    AIEs to execute the same instructions but different indexing can significantly
    enhance overall performance. We identify parallelizable loops in the time loops
    that do not have data dependence. In the MM example, the loop k is identified
    as a parallelizable loop. We can apply tiling to this loop using the factors K2.
    The point loop is permuted to the innermost position and completely unrolled to
    generate multiple threads of AIEs.


    #### *C. Placement and Routing Constraints Construction*


    The systolic design generated in the previous section represents an abstract mapping
    scheme. Consequently, it is essential to use the space loops as input and generate
    an actual mapped graph for AIE array that considers placement and routing constraints.
    The mapped graph consists of nodes, representing AIE cores and input/output ports,
    and edges, which connect the ports of the nodes. The placement and routing constraints
    involve assigning coordinates to the AIE cores, buffers, and input/output ports,
    as well as determining the routing paths for the edges. In the subsequent subsections,
    we introduce the graph builder and routing-aware PLIO assignment, which are responsible
    for constructing the mapped graph and generating the associated placement and
    routing constraints, respectively.


    *1) Graph Builder:* To construct the mapped graph, we iterate through all coordinates
    in the space loops and create a node for each pair of coordinates in the 2D systolic
    array, representing an AIE core. Next, we identify the data communications between
    AIE cores based on the dependencies within the space loops. Following the definitions
    in AutoSA [\[11\]](#page-5-15), there are three types of data dependences:


    - Read dependence: Transfer the read-only data.

    - Flow dependence: Transfer the intermediate data.

    - Output dependence: Transfer the output-only data.


    Based on these data dependences and the space loops, we define the I/O ports and
    edge directions. Since AIEs do not support intermediate results between different
    iterations, we treat flow dependences as input dependencies when constructing
    I/O ports. The polyhedral model for the array access to matrix A in the MM recurrences
    is {i, j, k} → {i, j+1, k}, and when loops j, k are the space loops, the direction
    is (1, 0). We connect the input ports from the corresponding nodes with a constant
    and non-zero distant direction.


    As for the output ports, the boundary input ports, and the zero distant direction
    ports, we create PLIO ports as the other end of the connection edge. To adhere
    to the limitation on the number of PLIO ports, we utilize packet-switch communications
    and broadcast communications to reduce the number of used ports, as depicted in
    Figure [4.](#page-3-0)


    *2) Routing-Aware PLIO Assignment:* Once we have the mapped graph, we search for
    available cores on the AIE array to place the AIE kernels. To facilitate efficient
    communication between neighboring cores, we assign the buffers of ports connecting
    these cores to the shared buffer of the cores, forming part of the placement constraints.
    These constraints enable the transformation of the kernels'' placement into a
    regular duplicate pattern of a single kernel.


    Aside from facilitating neighboring communication, it is necessary to construct
    paths between PLIO ports and AIE cores for data input and output. Considering
    the mesh structure of the NoC on the AIE array, and given that PLIOs are always
    located in Row 0, we can compute the routing congestion by counting the horizontal
    data transfer numbers. For instance, we compute the congestion for the *west*
    direction as follows:


    $$\operatorname{Cong}\_{i}^{\text{west}} = \sum\_{p \in \text{PL.IOs}, x \in \text{AIEs}}
    W\_{i}[p][x],$$


    $$W\_i[p][x] = \begin{cases} 1 & (p\_{\text{col}} < i \text{ and } x\_{\text{col}}
    > i \text{ and } (x, p) \in \text{Edges}) \text{ or } \\ & (p\_{\text{col}} >
    i \text{ and } x\_{\text{col}} < i \text{ and } (p, x) \in \text{Edges}) \\\ 0
    & \text{Otherwise} \end{cases}$$


    where pcol and xcol represent the column coordinates of PLIO p and AIE x, respectively.


    The computation of the congestion for the *east* direction is symmetrical.


    Consequently, the routing challenges essentially transform into issues of PLIO
    assignment. We formulate the assignment of PLIO ports as a satisfiability problem
    subject to routing resource constraints. We check if there exists a set of values
    for PLIOs that satisfies the following constraints:


    $$\forall i \in \text{Columns}, \mathsf{Cong}\_i^{\text{west}} \le \mathsf{RC}\_{\text{west}},
    \quad \mathsf{Cong}\_i^{\text{east}} \le \mathsf{RC}\_{\text{east}}$$


    where RC*west* and RC*east* denote the available routing resources in the AIE
    array.


    To seek the feasible assignment of PLIO ports, we employ a heuristic greedy algorithm
    outlined in Algorithm [1.](#page-4-0) In this algorithm, we initialize the placement
    of the PLIO ports by calculating the median value of the row numbers of the connected
    AIE cores. If the initially computed placement coordinate is not available, we
    search for the nearest available coordinate instead. This heuristic greedy algorithm
    balances the routing congestion among the PLIO ports. By considering the connectivity
    with the AIE cores, it generates an optimal placement for the PLIO ports, ensuring
    successful routing on the NoC. The algorithm takes into account the availability
    of coordinates and selects the most suitable placement to minimize congestion.


    By generating these constraints for the placement and routing of AIE kernels,
    buffers, and PLIO ports, we can


    <span id="page-4-0"></span>Algorithm 1 Routing-Aware PLIO Assignment Algortihm


    | Algorithm 1 Routing-Aware PLIO Assignment Algortihm         |

    |-------------------------------------------------------------|

    | Require: Numbers of PLIO ports N, AIE cores X               |

    | Ensure: Initialized PLIO assignment set P                   |

    | 1: Initialization available placement sets A as all columns |

    | that have PLIO ports.                                       |

    | 2: for i ← 1 to N do                                        |

    | S = [], num = 0<br>3:                                       |

    | for x ∈ X do<br>4:                                          |

    | if (p, x) ∈ Edges then<br>5:                                |

    | S.append(xcol)<br>6:                                        |

    | num+ = 1<br>7:                                              |

    | end if<br>8:                                                |

    | end for<br>9:                                               |

    | Sort S to find the median: sort(S, S + num)<br>10:          |

    | P[i] = find nearest(A, S[num/2])<br>11:                     |

    | remove(A, P[i])<br>12:                                      |

    | 13: end for                                                 |

    | 14: return<br>P                                             |


    significantly simplify the task for the AIE compiler. These constraints provide
    valuable information and guidelines for the compilers to optimize the placement
    and routing process, ultimately leading to a high utilization of the AIE array.


    #### IV. AUTOMATIC MAPPING FRAMEWORK


    To facilitate the computation of uniform recurrence, we have developed an automatic
    mapping framework that implements the full functional modules on the Versal ACAP
    architecture, as shown in Figure [5.](#page-4-1)


    <span id="page-4-1"></span>![](_page_4_Figure_5.jpeg)


    Fig. 5: Overview of WideSA Automatic Framework


    Specifically, we introduce a kernel-level mapper, a DMA module constructor, and
    a host program generator, which work in conjunction with the kernel scope and
    graph mapper described in the previous section.


    The kernel-level mapper and optimizer transform the C++ program into a program
    with AIE intrinsics, leveraging the capabilities of the AIE vector processor to
    exploit parallelism and optimize performance. Moreover, we design the architecture
    of efficient DMA modules, which serve as the buffers of AIEs on the PL, in the
    DMA module constructor. This architecture is tailored to the characteristics of
    both the hardware and the computations involved. In addition, we engineer a host
    program generator to generate a controller program that oversees global scheduling.


    TABLE II: Evaluation Benchmarks


    <span id="page-4-2"></span>


    | Benchmarks | Dimension    | Problem Size          | Data Types |

    |------------|--------------|-----------------------|------------|

    |            |              | [8192, 8192, 8192]    | Float      |

    |            |              | [10240, 10240, 10240] | Int8       |

    | MM         | [i, j, k]    | [9600, 9600, 9600]    | Int16      |

    |            |              | [8192, 8192, 8192]    | Int32      |

    | 2D-Conv    |              | [10240, 10240, 4, 4]  | Float      |

    |            | [h, w, p, q] | [10240, 10240, 8, 8]  | Int8       |

    |            |              | [10240, 10240, 4, 4]  | Int16      |

    |            |              | [10240, 10240, 4, 4]  | Int32      |

    |            |              | [8192, 8192]          | Cfloat     |

    | 2D-FFT     | [row, col]   | [8192, 8192]          | Cint16     |

    |            |              | [1048576, 15]         | Float      |

    | FIR Filter |              | [1048576, 15]         | Int8       |

    |            | [n, taps]    | [1048576, 15]         | Int16      |

    |            |              | [1048576, 15]         | Cfloat     |


    #### V. EVALUATION


    #### *A. Benchmark and Experimental Setup*


    In this section, we select four representative uniform recurrences with various
    data types as benchmarks to evaluate the performance of WideSA. The selected benchmarks
    include matrix multiplication (MM), 2D convolution (2D-Conv), 2D Fast Fourier
    Transformation (2D-FFT), and FIR filter [\[17\]](#page-5-16). The problem sizes
    and corresponding data types are provided in Table [II.](#page-4-2) Here, Cfloat
    refers to the complex float data type and Cint16 refers to the complex 16-bit
    integer data type. All the experiments are conducted on VCK5000 with 250 MHz on
    PL and 1.25 GHz on AIE. AMD/Xilinx Vitis 2022.1 is used as the compilation backend
    tool.


    ### *B. Full System Performance*


    We conducted a comparison of the throughput between WideSA and other state-of-the-art
    AIE designs for the same problem size. For the MM benchmark, we successfully compiled
    the CHARM code [\[7\]](#page-5-6) for the target VCK5000 with AMD/Xilinx Vitis
    2022.1, incorporating placement and routing constraints, as the baseline. As for
    the 2D-Conv benchmark, we selected the released 8-PEs version of Vitis-AI DPU
    [\[5\]](#page-5-4) which only supports Int8 data type, utilizing 256 AIEs running
    at 1.33 GHz and the PL at 350 MHz, as the baseline. Furthermore, we used the open-source
    designs from the Vitis DSP Library [\[18\]](#page-5-17) as the baselines for the
    2D-FFT and FIR filter benchmarks.


    The results presented in Table [III](#page-5-18) demonstrate that WideSA achieves
    significantly higher throughput with high utilization of AIEs. Additionally, we
    computed the AIE efficiency by considering the throughput and the number of used
    AIEs. The results indicate that WideSA maintains similar efficiency to [\[7\]](#page-5-6)
    for MM, as both approaches exhibit AIE utilization over 95%. When compared to
    the baselines with lower AIE utilizations, WideSA trades AIE efficiency (TOPS/#AIEs)
    for a high overall performance (TOPS) and is bounded by memory bandwidth.


    Moreover, we conducted a comparison of the performance and energy efficiency of
    MM using WideSA and PL-only designs on the VCK5000 target, which has 1968 DSP58
    IPs at total. For the PL-only designs, we utilize AutoSA [\[11\]](#page-5-15)
    as the systolic array generator. The results presented in Table [IV](#page-5-19)


    TABLE III: Comparison of Throughput and AIE Efficiency on Benchmarks


    <span id="page-5-18"></span>


    | Method   | Metric     |       |       | MM    |       | 2D-Conv |       |       |
    2D-FFT |        | FIR Filter |       |       |       |        |

    |----------|------------|-------|-------|-------|-------|---------|-------|-------|--------|--------|------------|-------|-------|-------|--------|

    |          | Data type  | Float | Int8  | Int16 | Int32 | Float   | Int8  | Int16
    | Int32  | Cfloat | Cint16     | Float | Int8  | Int16 | Cfloat |

    |          | #AIEs      | 384   | 384   | 384   | 384   | -       | 256   | -     |
    -      | 10     | 10         | 10    | 10    | 10    | 10     |

    | Baseline | TOPS       | 3.73  | 29.78 | 7.82  | 3.72  | -       | 31.40 | -     |
    -      | 0.04   | 0.13       | 0.15  | 2.56  | 0.62  | 0.15   |

    |          | TOPS/#AIEs | 0.010 | 0.077 | 0.020 | 0.010 | -       | 0.123 | -     |
    -      | 0.004  | 0.013      | 0.015 | 0.256 | 0.062 | 0.015  |

    |          | #AIES      | 400   | 400   | 400   | 400   | 400     | 400   | 400   |
    400    | 320    | 320        | 256   | 256   | 256   | 256    |

    | WideSA   | TOPS       | 4.15  | 32.49 | 8.10  | 3.92  | 4.50    | 36.02 | 10.35
    | 4.48   | 1.10   | 3.83       | 2.92  | 39.3  | 9.47  | 2.89   |

    |          | TOPS/#AIEs | 0.010 | 0.081 | 0.020 | 0.010 | 0.011   | 0.090 | 0.025
    | 0.011  | 0.003  | 0.012      | 0.012 | 0.100 | 0.037 | 0.011  |


    <span id="page-5-19"></span>TABLE IV: MM Performance Comparison between PL-only
    and WideSA Design


    |              |       |       | PL-only |       | WideSA |       |       |       |  |

    |--------------|-------|-------|---------|-------|--------|-------|-------|-------|--|

    | Data Type    | Float | Int8  | Int16   | Int32 | Float  | Int8  | Int16 | Int32
    |  |

    | DSPs         | 1536  | 1528  | 1516    | 1536  | 152    | 60    | 67    | 65    |  |

    | #AIEs        | 0     | 0     | 0       | 0     | 400    | 400   | 400   | 400   |  |

    | TOPS         | 0.59  | 5.77  | 2.16    | 0.60  | 4.15   | 32.49 | 8.10  | 3.92  |  |

    | Power (W)    | 19.5  | 18.8  | 18.6    | 19.5  | 55.8   | 54.4  | 54.9  | 55.6  |  |

    | TOPS/W       | 0.03  | 0.31  | 0.12    | 0.03  | 0.07   | 0.60  | 0.15  | 0.07  |  |

    | Norm. TOPS/W | 1.00x | 1.00x | 1.00x   | 1.00x | 2.25x  | 1.94x | 1.29x | 2.25x
    |  |


    demonstrate that our approach achieves up to 2.25× higher energy efficiency compared
    to the PL-only designs.


    #### *C. Scalability of WideSA on MM examples*


    We evaluate the scalability of WideSA while increasing AIE utilization and analyze
    how various factors influence performance. The results, presented in Figure [6,](#page-5-20)
    show a significant increase in throughput as the number of AIEs increases. In
    addition, the AIE efficiency results demonstrate that our approach scales effectively
    from small-scale to largescale designs. However, when the number exceeds 200,
    the efficiency of a single AIE core decreases due to the memorybound condition
    caused by the number of PLIOs and the size of the PL buffer. The increase in PLIO
    numbers and buffer sizes leads to increased throughput, suggesting that enhancing
    the bandwidth between different fabrics of ACAP can improve performance. This
    indicates that managing the resources and data flow between different components
    of the ACAP is crucial for achieving better performance.


    #### VI. CONCLUSION


    In this paper, we present a high array utilization mapping scheme for uniform
    recurrences on the Versal ACAP architecture. Additionally, we propose several
    optimizations aimed at enhancing overall performance within an automatic mapping
    framework. Through extensive evaluations using typical benchmarks and diverse
    data types, we assess the efficiency of the WideSA framework. In the future work,
    we aim to integrate WideSA into the MLIR-AIE workflow and develop an end-toend
    compilation tool that incorporates automatic design space exploration.


    #### ACKNOWLEDGEMENT


    This work was partly supported by the National Natural Science Foundation of China
    (Grant No. 62090021) and the National Key R&D Program of China (Grant No. 2022YFB4500500).


    <span id="page-5-20"></span>![](_page_5_Figure_11.jpeg)


    Fig. 6: Throughput Evaluation of Different AIE Numbers, PLIO Numbers, and PL Buffer
    Sizes


    #### REFERENCES


    - <span id="page-5-0"></span>[1] AMD/Xilinx. Versal Adaptive Compute Acceleration
    Platform.

    - <span id="page-5-1"></span>[2] R. M. Karp *et al.*, "The organization of computations
    for uniform recurrence equations," *J. ACM*, 1967.

    - <span id="page-5-2"></span>[3] S. Ahmad *et al.*, "Xilinx first 7nm device:
    Versal AI Core (VC1902)," in *HCS*, 2019.

    - <span id="page-5-3"></span>[4] J. A. Fisher, "Very long instruction word architectures
    and the ELI-512," in *ISCA*, 1983.

    - <span id="page-5-4"></span>[5] Vitis AI Library User Guide. [Online]. Available:
    [https://xilinx.github.](https://xilinx.github.io/Vitis-AI/) [io/Vitis-AI/](https://xilinx.github.io/Vitis-AI/)

    - <span id="page-5-5"></span>[6] X. Jia *et al.*, "XVDPU: A high performance CNN
    accelerator on the Versal platform powered by the AI Engine," in *FPL*, 2022.

    - <span id="page-5-6"></span>[7] J. Zhuang *et al.*, "CHARM: Composing heterogeneous
    accelerators for matrix multiply on Versal ACAP architecture," in *FPGA*, 2023.

    - <span id="page-5-7"></span>[8] AMD/Xilinx. AI Engine Kernel and Graph Programming
    Guide.

    - <span id="page-5-8"></span>[9] W. Cook *et al.*, "An exact rational mixed-integer
    programming solver," in *IPCO*, 2011.

    - <span id="page-5-9"></span>[10] J. Cong *et al.*, "PolySA: Polyhedral-based
    systolic array autocompilation," in *ICCAD*, 2018.

    - <span id="page-5-15"></span>[11] J. Wang *et al.*, "AutoSA: A polyhedral compiler
    for high-performance systolic arrays on FPGA," in *FPGA*, 2021.

    - <span id="page-5-10"></span>[12] Y.-H. Lai *et al.*, "SuSy: A programming model
    for productive construction of high-performance systolic arrays on FPGAs," in
    *ICCAD*, 2020.

    - <span id="page-5-11"></span>[13] M.-W. Benabderrahmane *et al.*, "The polyhedral
    model is more widely applicable than you think," in *CC*, 2010.

    - <span id="page-5-12"></span>[14] U. Bondhugula, "Compiling affine loop nests
    for distributed-memory parallel architectures," in *SC*, 2013.

    - <span id="page-5-13"></span>[15] R. Andonov *et al.*, "Optimal semi-oblique
    tiling," *IEEE TPDS*, 2003.

    - <span id="page-5-14"></span>[16] C. Rossetti *et al.*, "Algebraic tiling," in
    *IMPACT*, 2023.

    - <span id="page-5-16"></span>[17] K. K. Parhi, "VLSI digital signal processing
    systems: Design and implementation," 2007.

    - <span id="page-5-17"></span>[18] AMD/Xilinx. Vitis DSP Library for digital signal
    processing.'
  decisions:
    evaluation_prompt: 'Qualified. Reason: The paper includes an evaluation section
      and presents quantitative analysis with tables and descriptions of results.'
    related_work_prompt: 'Disqualified: no related work. Reason: Lacks a section labeled
      “Related Work” or equivalent.'
- title: "Optimization of a Line Detection Algorithm for Autonomous Vehicles on a\n\
    \  RISC-V with Accelerator"
  abstract: 'In recent years, autonomous vehicles have attracted the attention of
    many

    research groups, both in academia and business, including researchers from

    leading companies such as Google, Uber and Tesla. This type of vehicles are

    equipped with systems that are subject to very strict requirements, essentially

    aimed at performing safe operations -- both for potential passengers and

    pedestrians -- as well as carrying out the processing needed for decision

    making in real time. In many instances, general-purpose processors alone cannot

    ensure that these safety, reliability and real-time requirements are met, so it

    is common to implement heterogeneous systems by including accelerators. This

    paper explores the acceleration of a line detection application in the

    autonomous car environment using a heterogeneous system consisting of a

    general-purpose RISC-V core and a domain-specific accelerator. In particular,

    the application is analyzed to identify the most computationally intensive

    parts of the code and it is adapted accordingly for more efficient processing.

    Furthermore, the code is executed on the aforementioned hardware platform to

    verify that the execution effectively meets the existing requirements in

    autonomous vehicles, experiencing a 3.7x speedup with respect to running

    without accelerator.'
  url: http://arxiv.org/abs/2402.00496v1
  keywords: ''
  document: '# Optimization of a Line Detection Algorithm for Autonomous Vehicles
    on a RISC-V with Accelerator


    Optimizacion de un Algoritmo de Detecci ´ on de L ´ ´ıneas para Veh´ıculos Autonomos
    en un ´ RISC-V con Acelerador


    Mar´ıa Jose Belda ´ 1 [,](https://orcid.org/0000-0002-2870-7679) Katzalin Olcoz<sup>1</sup>
    [,](https://orcid.org/0000-0002-1821-124X) Fernando Castro<sup>1</sup> [,](https://orcid.org/0000-0002-2773-3023)
    and Francisco Tirado<sup>1</sup>


    > <sup>1</sup>*Complutense University of Madrid, Madrid 28040, Espana˜* {mbelda,katzalin,fcastror,ptirado}@ucm.es


    # Abstract


    In recent years, autonomous vehicles have attracted the attention of many research
    groups, both in academia and business, including researchers from leading companies
    such as Google, Uber and Tesla. This type of vehicles are equipped with systems
    that are subject to very strict requirements, essentially aimed at performing
    safe operations –both for potential passengers and pedestrians– as well as carrying
    out the processing needed for decision making in real time. In many instances,
    general-purpose processors alone cannot ensure that these safety, reliability
    and real-time requirements are met, so it is common to implement heterogeneous
    systems by including accelerators. This paper explores the acceleration of a line
    detection application in the autonomous car environment using a heterogeneous
    system consisting of a general-purpose RISC-V core and a domain-specific accelerator.
    In particular, the application is analyzed to identify the most computationally
    intensive parts of the code and it is adapted accordingly for more efficient processing.
    Furthermore, the code is executed on the aforementioned hardware platform to verify
    that the execution effectively meets the existing requirements in autonomous vehicles,
    experiencing a 3.7x speedup with respect to running without accelerator.


    Keywords: Autonomous vehicles, Firesim, Image processing, Matrix accelerator,
    RISC-V


    # Resumen


    En los ultimos a ´ nos los veh ˜ ´ıculos autonomos est ´ an´ atrayendo la atencion
    de muchos grupos de investi- ´ gacion, tanto del ´ ambito acad ´ emico como del
    em- ´ presarial, entre los que se incluyen investigadores pertenecientes a empresas
    punteras como Google, Uber o Tesla. Los sistemas de los que estan dota- ´ dos
    este tipo de veh´ıculos estan sometidos a requisitos ´ muy estrictos relacionados
    esencialmente con la realizacion de operaciones seguras, tanto para los poten-
    ´ ciales pasajeros como para los peatones, as´ı como con


    que el procesamiento necesario para la toma de decisiones se realice en tiempo
    real. En muchas ocasiones, los procesadores de proposito general no pueden por
    ´ s´ı solos garantizar el cumplimiento de estos requisitos de seguridad, fiabilidad
    y tiempo real, por lo que es comun implementar sistemas heterog ´ eneos mediante
    ´ la inclusion de aceleradores. En este art ´ ´ıculo se explora la aceleracion
    de una aplicaci ´ on de detecci ´ on de ´ l´ıneas en el entorno de veh´ıculos
    autonomos utilizando ´ para ello un sistema heterogeneo formado por un core ´
    RISC-V de proposito general y un acelerador de do- ´ minio espec´ıfico. En particular,
    se analiza dicha aplicacion para identificar las partes del c ´ odigo m ´ as cos-
    ´ tosas computacionalmente y se adapta el codigo para ´ un procesamiento mas eficiente.
    Adem ´ as, se ejecuta ´ dicho codigo en la mencionada plataforma hardware ´ y
    se comprueba que su procesamiento efectivamente cumple con los requisitos presentes
    en los veh´ıculos autonomos, experimentando una reducci ´ on de 3.7x en ´ su tiempo
    de ejecucion con respecto a su ejecuci ´ on sin ´ acelerador.


    Palabras claves: Veh´ıculos autonomos, Firesim, ´ Procesamiento de imagenes, Acelerador
    de matrices, ´ RISC-V


    # 1 Introduction


    In the technological era in which we live, we every day strive to make all the
    usual tasks as automatic as possible in order to gain free time. In addition,
    we try to achieve scenarios that are impossible right now, such as smarter power
    grids, fully autonomous vehicles or smart cities. This is why the Internet of
    Things (IoT) arises, as we need new technologies to design these systems. Most
    of them are on-board systems, so they need to get a trade-off between power consumption
    and delivered performance. In particular, in this work we focus on autonomous
    vehicles.


    Autonomous driving systems aim to enable vehicles to drive on the road without
    human intervention [\[1,](#page-10-0) [2,](#page-10-1) [3\]](#page-10-2). Therefore,
    these systems must guarantee the safety and integrity of the vehicle, for which
    they must take a series of decisions in real time, including


    moving the steering wheel to ensure that the correct trajectory is followed, detecting
    obstacles in the path (pedestrians, animals, objects...), activating the braking
    mechanism when necessary and others. For this purpose, it is essential that the
    vehicle has a camera that records images of the route and processes them in real
    time to ensure the correct and safe operation of the vehicle. This image processing
    requires considerable computing power, but at the same time, when talking about
    on-board systems, it is essential to keep energy consumption at low levels so
    the vehicle does not loose autonomy [\[4\]](#page-10-3).


    For these reasons, autonomous vehicles require onboard automatic systems to process
    the recorded images that allow certain operations such as line and edge detection.
    Currently, the most widely used algorithms for this type of processing require
    high performance and their basic kernel is matrix and vector multiplication. It
    is therefore highly desirable that this type of algorithms could be executed in
    one of the many domain specific accelerators that have emerged in recent years.


    In this paper we propose to accelerate a line detection application employed in
    autonomous cars by using different heterogeneous systems made up of a general-purpose
    RISC-V core working at low frequency and a domain-specific accelerator. For this
    purpose, the application is deeply analyzed in order to identify the computationally
    intensive parts of the code and adapted consequently for a more efficient processing.
    As it will be explained in Section 3, the hardware platform used in this work
    includes, on the one hand, a general-purpose BOOM processor, which is an out-of-order
    RISC-V core [\[5\]](#page-10-4), and on the other hand, the Gemmini [\[6\]](#page-10-5)
    accelerator, specifically designed for matrix multiplication. This platform was
    chosen because the RISC-V architecture, in addition to being open source, allows
    the integration of accelerators and their potential adaptation in a very simple
    way. Furthermore, the RISC-V instruction set architecture (ISA) is highly modular,
    allowing to choose exactly the functionalities needed, which is especially useful
    in IoT environments.


    This paper leverages two image processing algorithms: 1) the Canny algorithm for
    edge detection of an image, and 2) the Hough transform, oriented to find imperfect
    instances of objects within a certain class of shapes by means of a voting procedure.
    In Section 4 we perform a detailed analysis of both algorithms codes, in order
    to identify the computational load of the different functions included in these
    programs, as well as the available parallelism. Moreover, we schedule some functions
    to run on the accelerator, while the rest of the algorithm is executed on the
    processor, aimed to optimize the total execution time and consequently to meet
    the strict requirements of performance, consumption and safety imposed by autonomous
    vehicles. The experimental evaluation carried out in


    Section 5 reports a speedup of 3.7x when executing these algorithms with respect
    to the baseline where no accelerator is employed. Finally, Section 6 concludes
    the paper.


    # 2 Basic notions and state of the art


    In this section we explain some basic notions related to autonomous vehicles.
    We also provide details on the RISC-V-based development environment that we employ,
    including the tools used that make it possible the evaluation of the proposal
    presented in this paper.


    ## 2.1 Autonomous vehicles


    Autonomous vehicles are equipped with several sensors, as shown in Fig. [1,](#page-2-0)
    including video cameras, which are responsible for obtaining the data that serve
    as input to the processing system. The purpose of this data processing is to recognize
    the environment which the vehicle is driving through, and as a result, to make
    the appropriate decisions at any time, so as to ensure that the vehicle can reach
    its destination efficiently and safely. In this aspect, autonomous vehicles have
    levels of driving automation from 0 (No automation) to 5 (Full automation), as
    explained in [\[7\]](#page-11-0). In the first levels, from 0 to 2, the vehicle
    has very little capacity to act (in level 2 it can only perform steering and acceleration)
    and all the responsibility lies on the driver. In contrast, the automatic system
    monitors the driving environment in levels 3 to 5, being this last one the ideal
    scenario in which the vehicle is completely autonomous, even not providing controls
    for the driver. So, there is a gap between levels 2 and 3. Between these levels
    there is also a technological gap, since generating hardware and software capable
    of monitoring the environment in real time becomes significantly difficult. However,
    this gap is progressively disappearing and this work aims to contribute to this.


    Notably, certain safety decisions are related to the correct recognition of the
    trajectory to be followed by the vehicle, based on the images recorded by the
    camera. In addition to allowing the car to follow the correct route, this functionality
    also involves restricting the likelihood of an accident. For this purpose, computer
    vision algorithms are commonly used in these processing systems [\[9,](#page-11-1)
    [10\]](#page-11-2) and, in particular, quite approaches use Canny algorithm to
    detect edges combined with the Hough transform to detect road lines [\[11,](#page-11-3)
    [12\]](#page-11-4). Therefore, in this paper we focus on improving the performance
    of these algorithms which are the basis of lane detection. The problem with these
    algorithms is their very high computational cost. In addition to this, there is
    a need for data processing to be performed in real time so that the vehicle could
    react with immediacy to changing situations that may occur during the journey.
    It is also highly desirable that the energy consumption associated with such processing


    ![](_page_2_Figure_0.jpeg)


    <span id="page-2-0"></span>Figure 1: Integrated sensors on an autonomous vehicle
    [\[8\]](#page-11-5).


    was as low as possible, so that the vehicle''s autonomy was not affected.


    Autonomous driving systems are essentially composed of three classes of sub-systems
    [\[1,](#page-10-0) [2\]](#page-10-1): *scene recognition*, *route planning* and
    *vehicle control*, consisting of a set of algorithms each. In particular, as shown
    in Fig. [2,](#page-2-1) *scene recognition*, the class in which this article falls,
    comprises three essential tasks, namely 1) *localization*, which precisely establishes
    the vehicle''s location, 2) *object detection*, which identifies objects of interest
    in the vehicle''s environment (such as other vehicles, pedestrians or road signs,
    with the aforementioned objective of avoiding accidents and also traffic violations),
    and 3) *object tracking*, which, since the object detection algorithm is carried
    out on each frame of the image, is responsible for relating its results to other
    frames in order to predict the trajectories of moving objects. These three tasks
    account for a very high percentage of the total computation time required [\[1\]](#page-10-0)
    and therefore constitute bottlenecks that significantly limit the ability of conventional
    processors to satisfy the existing restrictions in the design of this type of
    systems. For this reason, it is being proposed to incorporate some type of accelerator
    to the on-board processing systems that helps the processor to fulfill the strict
    time limits in which it must operate.


    ## 2.2 RISC-V-based development environment


    In order to carry out the implementation and evaluation of our proposal, which
    will be explained in the following section, a series of software tools have been
    used, as detailed next:


    ![](_page_2_Figure_7.jpeg)


    <span id="page-2-1"></span>Figure 2: Schematic of the subsystems of an autonomous
    vehicle.


    #### 2.2.1 Chipyard.


    Chipyard [\[13\]](#page-11-6) is an environment for the design and evaluation
    of hardware systems that consists of a set of tools and libraries designed to
    provide an integration path between open-source tools and commercial tools for
    the development of Systems on Chip (SoC). The environment provides a range of
    components for design construction as well as for compilation and simulation.
    Among these components there are several RISC-V cores and accelerators, including
    the BOOM core and Gemmini accelerator that make up the heterogeneous system chosen
    in this paper and that will be detailed in Section [3.](#page-3-0) The simulation
    of the complete system accelerated with FPGA is one of the types of simulation
    supported by Chipyard, using the FireSim tool described below.


    #### 2.2.2 FireSim.


    FireSim [\[14\]](#page-11-7) is a hardware simulation platform that runs on Amazon
    cloud services and automatically deploys the FPGA services in the cloud when needed.
    In particular, the user can generate the RTL of an own design and run it on these
    FPGAs, obtaining the same results as if the circuit was physically deployed.


    #### 2.2.3 Amazon Web Services.


    Amazon Web Services [\[15\]](#page-11-8) is a cloud services platform that offers
    from training courses in new technologies –such as artificial intelligence or
    IoT– to infrastructure services –such as storage or cloud computing. We focus
    on cloud computing because it offers a wide range of hardware platforms, including
    EC2 F1 instances that correspond to FPGAs, giving us the versatility we need to
    synthesize designs and to simulate the execution of applications on them.


    # <span id="page-3-0"></span>3 Platform design


    The platform employed in our experiments features a general-purpose processor
    equipped with an accelerator –implemented as a systolic array architecture– for
    matrix multiplication. Both components have been developed by the Computer Architecture
    group at Berkeley University [\[6\]](#page-10-5). The accelerator communicates
    with the processor through the RoCC (Rocket Co-Processor) interface, which allows
    the accelerator to receive the specific instructions that the processor sends,
    as shown in Fig. [3.](#page-3-1) In the following two sections we describe the
    processors and the accelerator used.


    ![](_page_3_Figure_6.jpeg)


    <span id="page-3-1"></span>Figure 3: Architecture of our heterogeneous platform
    [\[6\]](#page-10-5).


    ## 3.1 Processors


    As Fig. [3](#page-3-1) illustrates, our system features a core plus an accelerator.
    In our experiments we opted to employ either the Rocket or the BOOM (Berkeley
    Out-of-Order Machine) processor. Both of them are written in Chisel and implement
    the RV64GC instruction set. Also, they


    are easily parameterizable and can be synthesized. Notably, the cores are configured
    by using the Rocket Chip SoC generator [\[16\]](#page-11-9).


    The main differences between both cores lie in the pipeline characteristics: while
    the Rocket core features an in-order 5-stage pipeline, the BOOM core is equipped
    with a deeper out-of-order pipeline, which is inspired by those of MIPS R10000
    and Alpha 212645 [\[5\]](#page-10-4). Consequently, the BOOM core is expected
    to deliver higher performance when executing our line detection algorithm. However,
    this comes at the expense of higher energy consumption than that of the Rocket
    core. Therefore, we experiment with both processors in order to check if the speedup
    reported by the BOOM core is significant enough to cancel out the energy constraints.


    ## 3.2 The Gemmini Accelerator


    The Gemmini matrix multiplication accelerator relies on a 2D systolic array architecture,
    as shown in Fig. [3,](#page-3-1) to perform matrix multiplications in an efficient
    fashion. In addition to this systolic array, it also features a scratchpad memory
    with multiple banks and an accumulator, which has more bits than that of the systolic
    array. Besides, the implementation allows to choose, at compile time, between
    two specific calculation mechanisms: output-stationary or weight-stationary.


    Customized instructions –out of RISC-V standard– are available for the Gemmini
    accelerator, so that it is equipped with its own instruction queues that make
    it possible to execute concurrently with the processor. The Gemmini programming
    model can be broken down into three different levels. In the high-level we can
    run Open Neural Network Exchange (ONNX) models, being the accelerator itself in
    charge of mapping the ONNX kernel to the accelerator by means of dynamic dispatch.
    In the mid-level we use a handtuned library including C macros to perform data
    transfers between the main memory and the accelerator''s scratchpad memory, which
    should be explicitly defined, as well as to automate the calculation of the block
    size used to split a matrix and to perform the full multiplication in a transparent
    way for users. Among available functions we highlight the following: *tiled matmul*,
    to run a tiled matrix multiplication with hardcoded tiling factors; *tiled conv*,
    to apply a convolution with hardcoded tiling factors; *tiled matmul auto*, to
    run a tiled matrix multiplication with automatically calculated tiling factors;
    *gemmini mvin*, to move data from the main memory to the scratchpad and *gemmini
    mvout*, to move data from the scratchpad to the main memory. Finally, at the low-level,
    we can write our own mid-level kernels with low-level assembly instructions.


    # <span id="page-4-2"></span>4 Adapting image processing algorithms


    As stated previously, the aim of this work is to accelerate image processing algorithms
    employed to guide autonomous vehicles. Notably, we focus on those algorithms targeted
    to detect road lines from road images. In this section we first introduce the
    basic algorithms used (the Canny algorithm and the Hough transform). Then, we
    show the full algorithm that we have employed in this work as starting point for
    line detection and, finally, we propose some changes to this algorithm oriented
    to improve its efficiency and performance without impacting on accuracy.


    ## 4.1 Canny Algorithm


    Among the edge detection methods developed to date, the Canny algorithm is one
    of the methods more strictly defined that provides a satisfactory and reliable
    detection. Thus, it has become one of the most popular algorithms targeting edge
    detection.


    This algorithm relies on calculus of variations, which allows to find an analytical
    function to approximate the real curve (i.e., the road lines) as accurately as
    possible. The procedure followed by the Canny algorithm [\[17\]](#page-11-10)
    can be broken down into 5 stages as shown next:


    - 1. Noise reduction: applying the Gauss filter for image smoothing.

    - 2. To find the intensity gradient of the image.

    - 3. Magnitude threshold to the gradient: applying a threshold to the gradient
    for discarding edge false positives.

    - 4. Double threshold: applying again a threshold to the gradient for highlighting
    the potential edges.

    - 5. Hysteresis: removing weak or disconnected edges.


    Algorithm [1](#page-4-0) shows the pseudo-code we employed to apply the Canny
    algorithm, broken down into the 5 stages aforementioned. Essentially, it includes
    multiplications of consecutive matrices and conditions checking in order to detect
    edge points.


    ## 4.2 Hough Transform


    The Hough transform is a technique of features extraction which is employed in
    multiple fields involving image processing, as computer vision or image digital
    processing. The goal of the algorithm is to find imperfect objects among certain
    classes of objects by means of a voting procedure. This procedure lies in creating
    a space with the values assigned to each pixel, so that the resulting local maximums
    in the so called accumulator space are the possible detected objects.


    <span id="page-4-0"></span>Algorithm 1 Canny algorithm summarized pseudocode.


    - 1: float *NR* ← mask ∗ image ▷ Stage 1: Noise reduction

    - 2: float *G<sup>x</sup>* ← mask ∗NR ▷ Stage 2: Gradient intensity

    - 3: float *G<sup>y</sup>* ← mask ∗NR

    - 4: float *G* ← q *G*<sup>2</sup> *<sup>x</sup>* +*G*<sup>2</sup> *y*

    - 5: float φ ← arctan(|*Gy*|/|*Gx*|)

    - 6: if φ[∗] ≥ threshold<sup>φ</sup> then ▷ Stage 3: Gradient threshold

    - 7: float φ ∈ {0,45,90,135}

    - 8: end if

    - 9: if φ[∗] ≥ threshold<sup>φ</sup> && *G*[∗] ≥ threshold*<sup>G</sup>* then
    ▷ Stage 4: Double threshold

    - 10: int edge[∗] ← 1

    - 11: end if

    - 12: if *G*[∗] ≥ threshold*<sup>G</sup>* && edge[∗] == 1 then ▷ Stage 5: Hysteresis

    - 13: int image out[∗] ← 255


    14: end if


    Generally, the classical Hough transform was only applied to the detection of
    straight lines, but in recent years it has been modified and currently it is employed
    for the detection of arbitrary curves, as ellipses or circles.


    Algorithm [2](#page-4-1) illustrates the code we employed to apply the Hough transform
    [\[18\]](#page-11-11). In this code, for each edge point previously detected with
    the Canny algorithm, the Hough transform draws a set of straight lines going through
    that point, recording the amount of lines going through each image pixel. Hence,
    those points with more lines going through them will correspond to a line in the
    original image.


    <span id="page-4-1"></span>Algorithm 2 Hough transform summarized pseudocode.


    1: For each edge point (*i*, *j*) 2: if image[*i* ∗*width*+ *j*] ≥ 250 then 3:
    θ ← 0 4: while θ ≤ 180 do 5: float ρ ← *j* ∗ cosθ +*i* ∗ sinθ 6: accumulators[(ρ
    +*c*<sup>ρ</sup> ) ∗ 180+θ]++ 7: θ++ 8: end while 9: end if


    ## 4.3 Line Detection


    Once we have described the two previous algorithms, we now employ a combination
    of both as well as another specific code targeted to detect with higher accuracy
    the lines that demarcate lanes in conventional ways. For this purpose, given a
    certain input image, we first apply the Canny algorithm and then the Hough


    transform, so that we can apply a function (*Get lines coordinates*) to detect
    lines in the resulting image. In Algorithm [3](#page-5-0) we show the code of
    the mentioned function, which involves a search of local maximums in the preprocessed
    image and the generation of a straight line going through closest maximums.


    <span id="page-5-0"></span>Algorithm 3 Get lines coordinates algorithm summarized
    pseudo-code.


    - 1: For each image point (ρ,θ)

    - 2: if accumulators[∗] ≥ threshold then ▷ It is a local maximum

    - 3: *max* ← accumulators[\*]

    - 4: if accumulators[neighbourhood(\*)] ≥ *max* then ▷ We check its neighborhood

    - 5: *max* ← accumulators[neighbourhood(\*)]

    - 6: end if

    - 7: end if

    - 8: lines.add(*x*1, *y*1, *x*2, *y*2) ▷ We save the two points that demarcate
    the straight line


    ## 4.4 Delivering higher performance


    In the previous sections we have described the original code of the algorithms,
    which indeed deploys many floating point variables. Therefore, it is advisable
    to replace them by integer variables without any loss in accuracy. We effectively
    made these changes in the code and we verified that no accuracy loss occurs when
    detecting lines in an image. Fig. [4](#page-6-0) shows the original image with
    detected lines highlighted in red. The analytical results corresponding to the
    lines detected with the original algorithm and with the simplified one do match,
    and also the second algorithm has performed slightly faster. Details on these
    modifications can be found in [\[19\]](#page-11-12).


    Apart from this change, we also performed a profiling of the full code divided
    into three stages: 1) original image loading, 2) lines detection and 3) generation
    of an output image with the detected lines. Accordingly to the results obtained,
    we opted for not generating an output image (that is not needed by our system)
    due to the high cost associated, as shown in Table [1.](#page-5-1) In doing so,
    we are able to reduce the execution time by 4.2x as we can derive from data reported
    in Table [2.](#page-5-2) It is worth noting that the time values illustrated in
    the mentioned tables are approximate since the profiling was not performed on
    the target platform, but on an Intel i7 processor running Linux. However, in order
    to report time values as accurate as possible, the algorithms were run several
    times so that the tables show the average values obtained. According to numbers
    from Table [2,](#page-5-2) our attention is drawn to the line detection phase
    since it accounts for almost 70% of the execution time.


    In addition, we have performed another specific profiling of the stages of the
    line detection algorithm in


    <span id="page-5-1"></span>


    | Table 1: Phased profiling of the full code. |  |  |

    |---------------------------------------------|--|--|

    |                                             |  |  |


    |                  | Time(µs) | % over total |

    |------------------|----------|--------------|

    | Image load       | 43803    | 7,32%        |

    | Line detection   | 98171    | 16,42%       |

    | Image generation | 456030   | 76,26%       |

    | Total            | 598004   |              |


    <span id="page-5-2"></span>Table 2: Phased *profiling* of the full code excluding
    the generation of the output image.


    |                | Time(µs) | % over total |

    |----------------|----------|--------------|

    | Image load     | 43485    | 30,58%       |

    | Line detection | 98714    | 69,42%       |

    | Total          | 142199   |              |


    order to know in which parts of the processing the acceleration efforts should
    be focused. Table [3](#page-5-3) illustrates that the most time-consuming part
    is the application of the Canny algorithm, which accounts for more than 87% of
    the total execution time. Therefore, we will focus on accelerating this stage
    of image processing.


    <span id="page-5-3"></span>Table 3: Phased *profiling* of the line detection algorithm.


    |                 | Time(µs) | % over total |

    |-----------------|----------|--------------|

    | Canny algorithm | 90265    | 87,64%       |

    | Hough transform | 12275    | 11,92%       |

    | Get coordinates | 459      | 0,45%        |

    | Total           | 102999   |              |


    # 5 Experimental results


    In this section, we first describe the hardware platforms as well as the workloads
    employed in our experiments, and then we detail the results obtained.


    ## 5.1 Platforms generated


    All the components used in the designs generated are written in Scala, so it is
    easy to modify their main features such as number of registers or number of Re-Order
    Buffer (ROB) entries. Notably, we generate several designs: while all of them
    include one (or more) Rocket or BOOM cores, they may include or not the Gemmini
    accelerator.


    Apart from the cores, for the sake of fairness the remaining components in the
    different designs generated (such as memory, clock frequency or buses) are the
    same in all of them. Hence, all designs have an L2 –shared in multicore platforms–
    4MB size. In order to optimize the design to fit into smaller FPGAs, the option
    MCRams is enabled in the FireSim platform configuration for all designs. This
    option allows the FPGA simulation tool (Golden Gate [\[20\]](#page-11-13)) to
    simulate the RAM via serialized accesses with a decoupled model [\[14\]](#page-11-7).


    ![](_page_6_Picture_0.jpeg)


    Figure 4: Original image with detected lines highlighted in red.


    Platforms including the Gemmini accelerator can only be designed to work at 50MHz
    while the remaining ones can reach 80MHz. Thus, the later have been designed both
    at 50 and 80 MHz for a fair comparison against designs equipped with Gemmini.
    Notably, the platforms generated are:


    1. *Platform* 1: Rocket single core.


    This architecture includes a single *Big* Rocket core. There are four different
    sizes for the core, namely *Big, Medium, Small* and *Tiny*, with different features
    such as the size of L1-cache. The *Big* Rocket is the only one providing Floating
    Point Unit. It also has by default the parameters shown in Table [4.](#page-7-0)
    More information on the details of the configuration can be found in [\[19\]](#page-11-12).


    2. *Platform* 2: Rocket dual core.


    This is the same configuration as Platform 1 but it includes two *Big* Rocket
    cores. This dual configuration also has the option MTModels enabled in the FireSim
    platform configuration, so that each core is simulated with a separate thread
    of execution on a shared underlying physical implementation [\[14\]](#page-11-7).


    3. *Platform* 3: Heterogeneous Rocket single core + Gemmini Accelerator.


    This architecture is made up by a *Big* Rocket core and a Gemmini matrix multiplication
    accelerator, which has been designed with default options: 16x16 8-bit systolic
    array, both dataflows supported (output-stationary and weight-stationary), float
    data type supported, a set of accumulator registers with 64B of total capacity,
    a 256KB scratchpad with 4 banks, a small TLB with 4 entries and a bus width of
    128 bits.


    <span id="page-6-0"></span>4. *Platform* 4: BOOM Single core.


    This architecture includes a single *Large* BOOM core. There are different macros
    for defining BOOM cores of *Giga, Mega, Large, Medium* and *Small* sizes. The
    main differences between the one that we are using and the rest is the number
    of entries in the ROB and some L1-cache parameters. Thus, in the configuration
    *WithN-LargeBooms* the value of notable parameters are shown in Table [4.](#page-7-0)
    More information on the details of the configuration can be found in [\[19\]](#page-11-12).
    The Large size was chosen because it is just big enough to provide the required
    performance with minimum power consumption.


    5. *Platform* 5: BOOM dual core.


    This is the same configuration as Platform 4 but it includes two *Large* BOOM
    cores, with the MT-Models option enabled.


    6. *Platform* 6: Heterogeneous BOOM single core + Gemmini Accelerator.


    This architecture is made up by a *Large* BOOM core and a Gemmini matrix multiplication
    accelerator, which has been designed with the default options explained earlier.


    # 5.2 Workloads generated


    Different workloads were designed for running on the platforms described in the
    previous section. They are the following:


    1. *Workload* 1: Multithreaded application on top of Linux buildroot distribution.


    In this workload, a multithreaded application (with each thread computing the
    addition of 2


    |                  |             | Big Rocket | Large Boom |

    |------------------|-------------|------------|------------|

    | I&D Cache        | Size        | 16KB       | 32KB       |

    |                  | Sets        | 64         | 64         |

    |                  | Ways        | 4          | 8          |

    |                  | Prefetching | no         | disabled   |

    | TLB              | Sets        | 1          | 1          |

    |                  | Ways        | 32         | 512        |

    | BTB Entries      |             | 28         | 28         |

    | ROB Entries      |             | no         | 96         |

    | FPU              |             | yes        | yes        |

    | Branch predictor |             |            |            |

    | entries          |             | no         | 128        |


    <span id="page-7-0"></span>Table 4: Platform configuration options.


    long arrays, as explained in [\[19\]](#page-11-12)) is executed on top of Linux.
    It has been specifically designed to fully exploit the parallel features of the
    platforms, so that it can be used to evaluate the maximum performance obtainable
    in the different multicore designs. This value will serve as an upper bound when
    we evaluate the performance achieved by our target application.


    <span id="page-7-1"></span>2. *Workload* 2: Line detection algorithm on top of
    Linux buildroot distribution.


    In this workload, the modified version of the line detection application explained
    in Section [4](#page-4-2) is executed on top of Linux.


    <span id="page-7-2"></span>3. *Workload* 3: Line detection algorithm for baremetal
    platforms with Gemmini.


    In this workload, in addition to the modifications in Section 4, we have modified
    the line detection algorithm to add matrix multiplications. In the original version,
    this algorithm multiplies some mask values to a pixel neighborhood manually by
    writing the corresponding scalar multiplications. We have rewritten these multiplications
    in a matrix form, obtaining a 5x5 matrix for the mask and a 5x5 neighborhood matrix
    for each pixel. As for the platform, the differences with respect to the previous
    workload are that this platform includes a Gemmini accelerator for matrix multiplication
    and the fact that no operating system is available for this platform. Thus, matrix
    multiplications in the code have to be replaced by calls to a Gemmini multiplication.
    As previously explained, some C macros are provided with the designs that make
    it possible to easily programming the accelerator. First, data need to be moved
    from the main memory to the scratchpad memory in Gemmini, then the multiplication
    is performed in tiles and finally the results are transferred back to the main
    memory. We will use the *tiled matmul auto* function that receives the dimensions
    of both matrices as input parameters and automatically splits the multiplication
    in


    blocks of suitable size for the systolic array and memory, thus performing the
    whole multiplication. Finally, system calls not available outside Linux were removed
    from the code and their functionality was implemented in an equivalent way.


    # 5.3 Experiments


    In this section we show the results obtained from the execution of the workloads
    on the different platforms designed. The metrics measured are clock cycles and
    instructions retired provided by the performance counters of the target platforms.


    #### 5.3.1 Experiment 1: Execution of a multithreaded application on single core
    and dual core platforms both with Rocket and BOOM cores.


    The goal of this experiment is to verify the maximum performance attainable in
    the different platforms by using a massively parallel application. Therefore we
    employ *Workload 1*, configured with as many independent threads as the number
    of cores in the system, i.e., 1 or 2 depending on the specific platform.


    The target platforms in this case include both single and dual core processors
    (either Rocket or BOOM, running at 80MHz) that correspond to the *Platforms* 1,
    2, 4 and 5 previously described.


    The results of the experiment are shown in Table [5,](#page-8-0) both for a simulation
    in which the main loop is executed once (column labelled *N times = 1*) and 8
    times (column *N times = 8*). The number of clock cycles for the experiment with
    8 iterations is 8 times the one of the single iteration experiment. Besides, speedup
    of the dual core version with respect to the single core is very close to 2x for
    both Rocket and BOOM. Finally, comparing the performance of the different cores,
    BOOM achieves almost 2.2x higher performance than Rocket, so that a single BOOM
    core outperforms a dual core Rocket running at the same frequency for this highly
    parallel application.


    Thus, it has been verified that multithreaded applications are being correctly
    simulated in the multicore


    |                   | N times = 1 | N times=8 |  |

    |-------------------|-------------|-----------|--|

    |                   | Cycles      |           |  |

    | Rocket singlecore | 2.01×109    | 1.59×1010 |  |

    | BOOM singlecore   | 9.17×108    | 7.31×109  |  |

    | Rocket dualcore   | 9.97×108    | 7.99×109  |  |

    | BOOM dualcore     | 4.53×108    | 3.66×109  |  |

    | Speedup BOOM      |             |           |  |

    | vs Rocket         | 2.19x       | 2.18x     |  |

    | Speedup Rocket    |             |           |  |

    | dual vs single    | 2.02x       | 1.99x     |  |

    | Speedup BOOM      |             |           |  |

    | dual vs single    | 2.02x       | 1.99x     |  |


    <span id="page-8-0"></span>Table 5: Cycles when executing Workload 1 on Platforms
    1, 2, 4 and 5.


    platforms, achieving the expected speedup. Furthermore, the comparison between
    both types of cores has been established.


    #### 5.3.2 Experiment 2: Execution of the line detection application on Rocket
    and BOOM single cores.


    This second experiment involves simulating the execution of the line detection
    application (*workload* [2\)](#page-7-1) on the Rocket and BOOM single core platforms
    employed in the previous experiment (Platforms 1 and 4), also running at 80MHz.
    In Table [6](#page-9-0) we report the number of clock cycles and instructions
    retired corresponding to each of the different parts of the line detection algorithm,
    as well as the average cycles per instructions (CPI) value. In addition, we calculate
    the actual time from the cycles and clock frequency, resulting in times of around
    half second. In particular, for the Rocket core we obtain a total execution time
    of 0.648s and for the Boom core 0.327s. As shown, the CPI for the Hough transform
    is higher than 3 in both Rocket and BOOM platforms. Moreover, its execution on
    the BOOM processor almost matches the time reported on the Rocket platform, as
    the multiple data dependencies in the code make out-of-order capabilities useless.


    On the other hand, the Canny and the GetCoordinates algorithms exhibit lower CPI
    numbers in both platforms, achieving a speedup of 2x when executing on the Boom
    processor with respect to Rocket, due to the greater instruction level parallelism
    that can be extracted from both algorithms. Recall that the Canny algorithm is
    the most relevant part of the line detection application, consuming close to 90%
    of the total execution time (as shown in Table [3\)](#page-5-3). In conclusion,
    using the BOOM core for the execution of the workload is interesting in terms
    of the global speedup achieved.


    #### 5.3.3 Experiment 3: Execution of the line detection application on heterogeneous
    platforms with a Rocket or BOOM single core and a Gemmini matrix multiplication
    accelerator.


    This experiment consists on simulating the execution of the modified line detection
    application (*workload* [3\)](#page-7-2) on the heterogeneous single core platforms
    made up by a Rocket or BOOM processor plus a Gemmini matrix multiplication accelerator
    running at 50MHz.


    Table [7](#page-9-1) shows first the results obtained in the simulation of Workload
    3 (line detection application for bare metal) on a Rocket single core (used as
    baseline for computing speedups) and a BOOM single core, both running at 50MHz.
    As the first row shows, BOOM is 41% faster than Rocket. The execution results
    from the previous section, that is, those corresponding to Workload 2 (line detection
    application for Linux) on Rocket and BOOM single core at 80MHz are also compared
    to the baseline execution, achieving speedups of 2.09x and 3.76x respectively.
    It is worth noting that although the code of Workloads 2 and 3 does not exactly
    match, it performs the same functionality. Finally, the results from the simulation
    of Workload 3 on heterogeneous platforms in which matrix multiplications are performed
    using the Gemmini accelerator are also recap in Table [7.](#page-9-1) According
    to them, speedups of 2.36x and 3.7x are reported for Rocket and BOOM based platforms
    respectively, with respect to the baseline. Although these speedups can be considered
    as significant, they are far from the maximum values attainable by the accelerator.
    The reason is that the size of the matrices employed is smaller than that of the
    systolic array, which indeed is not fully utilized.


    Furthermore, in the graph shown in Fig. [5](#page-10-6) we can see the time corresponding
    to all the single core and heterogeneous experiments. The first thing we notice
    is that the out-of-order execution of the Boom core is beneficial for the Canny
    algorithm, leaving the Rocket core as the slowest by far at both 50 and 80MHz.
    Furthermore, we see how the combination of the cores with the Gemmini accelerator
    at 50MHz gives us a similar time to the same cores without accelerator at 80MHz,
    which gives us a great benefit in terms of consumption by running at a lower clock
    frequency which should be taken into account in the field of autonomous vehicles,
    as it would provide greater autonomy. In addition, we note that the shortest time
    is under half a second, in particular 300ms, and we achieve it with the combination
    of the Boom core and the Gemmini accelerator at a clock frequency of 50MHz. Thus,
    a vehicle travelling at 50km/h could run the algorithm every 4 metres approximately
    and if necessary, options such as mounting several systems in parallel or slightly
    increasing the clock frequency for faster processing could be explored.


    In conclusion, for this application with small matrices, both platforms based
    on the BOOM core deliver similar performance (speedup of around 3.7x with re-


    <span id="page-9-0"></span>


    |                        |             | Cycles   | Instructions | CPI   | Time(ms)
    |

    |------------------------|-------------|----------|--------------|-------|----------|

    | Rocket singlecore      | Canny       | 2.18×109 | 9.06×108     | 2.40  | 648,38   |

    |                        | Hough       | 3.32×108 | 9.35×107     | 3.55  | 98,86    |

    |                        | Coordinates | 6.49×106 | 3.47×106     | 1.87  | 1,93     |

    | Boom singlecore        | Canny       | 1.08×109 | 9.06×108     | 1.19  | 327,10   |

    |                        | Hough       | 3.16×108 | 9.35×107     | 3.38  | 96,07    |

    |                        | Coordinates | 3.2×106  | 3.47×106     | 0.92  | 0,97     |

    | Speedup Boom vs Rocket | Canny       | 2.02x    | 1.00x        | 2.02x | 1.98x    |

    |                        | Hough       | 1.05x    | 1.00x        | 1.05x | 1.03x    |

    |                        | Coordinates | 2.03x    | 1.00x        | 2.03x | 1.99x    |


    Table 6: Cycles, instructions retired and CPI when executing Workload 2 on Platforms
    1 and 4 at 80MHz.


    <span id="page-9-1"></span>Table 7: Speedup results when executing Workload 2
    on Platforms 1 and 4 at 80MHz, and Workload 3 on Platforms 3, 4, 6 at 50MHz, with
    respect to execution of Workload 3 on Platform 1 at 50 MHz.


    |                         | Speedup vs Rocket singlecore 50MHz |       |             |       |  |

    |-------------------------|------------------------------------|-------|-------------|-------|--|

    |                         | Canny                              | Hough | Coordinates
    | Total |  |

    | Boom singlecore 50MHz   | 1.44x                              | 1.04x | 1.85x       |
    1.41x |  |

    | Rocket singlecore 80MHz | 2.26x                              | 0.98x | 1.07x       |
    2.09x |  |

    | Boom singlecore 80MHz   | 4.57x                              | 1.03x | 2.18x       |
    3.76x |  |

    | Rocket + Gemmini 50MHz  | 2.54x                              | 1.16x | 1.03x       |
    2.36x |  |

    | Boom + Gemmini 50MHz    | 4.43x                              | 1.07x | 1.98x       |
    3.70x |  |


    spect to the Rocket baseline), being the BOOM single core at 80MHz slightly faster
    than the BOOM + Gemmini at 50MHz. Even in this non favourable scenario, the accelerator
    allows to report high performance working at a lower frequency, being more power
    efficient than the single core platform running at higher frequency.


    # 6 Conclusions and future work


    In this paper we have explored the acceleration of a line detection algorithm
    in the autonomous car environment using a heterogeneous system consisting of a
    general-purpose RISC-V core and a domain-specific accelerator. In particular,
    we analyzed the application to identify the most computationally intensive parts
    of the code and adapted it accordingly for more efficient processing.


    The first conclusion we extract from this work is that RISC-V architecture provides
    a hw-sw ecosystem that is well suited for IoT in general and autonomous vehicle
    systems in particular, due to its versatility and modularity, which allows to
    generate platforms adapted to different scenarios. In fact, in this work, we designed
    six different platforms covering a wide spectrum of alternatives: on one side
    single and dual core homogeneous systems, and on the other side heterogeneous
    platforms with a single core plus a matrix multiplication accelerator –all of
    them including high performance BOOM cores or more efficient Rocket cores.


    Also, a multithreaded application with high data parallelism has been designed
    to analyze the performance of the homogeneous platforms built. Thus, it has been


    verified that multithreaded applications are being correctly simulated in the
    multicore platforms, achieving the expected speedup. Furthermore, the comparison
    between both types of cores determined that a single BOOM core is up to 2.19 times
    faster than a Rocket one.


    Finally, the original application of line detection has been modified in order
    to decrease its execution time without losing accuracy, and it has also been adapted
    for bare metal and Gemmini execution. We simulated the application on all designed
    platforms. BOOM-based platforms reported the best performance numbers, achieving
    speedups of 3.7x with respect to the baseline (a single Rocket core running at
    50MHz), and being the single BOOM core running at 80MHz slightly faster than the
    BOOM + Gemmini platform at 50MHz. As previously stated, even working at a lower
    frequency the accelerator allows to report high performance, being more power
    efficient than the single core counterpart working at a higher frequency. It is
    worth noting that our goal in this work was to explore how an domain-specific
    accelerator was able to accelerate the baseline execution (just using a conventional
    single core) in applications belonging to autonomous vehicles environment.


    As future work, other applications which involve multiplication of big matrices
    can be adapted to heterogeneous platforms in order to implement more of the functionalities
    required for autonomous vehicles. Moreover, Gemmini is expected to achieve much
    higher speedups for inference using neural networks, as shown in [\[6\]](#page-10-5),
    so exploring this issue constitutes an interesting avenue for future work.


    ![](_page_10_Figure_0.jpeg)


    <span id="page-10-6"></span>Figure 5: Time results when executing Workload 2 on
    Platforms 1 and 4 at 50MHz and 80MHz, and Workload 3 on Platforms 3 and 6 at 50MHz.


    ## Competing interests


    The authors have declared that no competing interests exist.


    #### Funding


    The present work has been funded by the Comunidad de Madrid through project S2018/TCS-4423
    and by the Ministry of Science, Innovation and Universities through project RTI2018-093684-B-I00.


    #### Authors'' contribution


    MJB wrote the programs, conducted the experiments, analyzed the results and wrote
    the manuscript; KO and FC conceived the idea, analyzed the results and wrote the
    manuscript; FT revised the manuscript. All authors read and approved the final
    manuscript.


    # References


    <span id="page-10-0"></span>[1] S.-C. Lin *et al.*, "The architectural implications
    of autonomous driving: Constraints and acceleration," in *Proceedings of the Twenty-Third
    International Conference on Architectural Support for Programming*


    *Languages and Operating Systems*, ASPLOS ''18, p. 751–766, 2018.


    - <span id="page-10-1"></span>[2] S. Kato, E. Takeuchi, Y. Ishiguro, Y. Ninomiya,
    K. Takeda, and T. Hamada, "An open approach to autonomous vehicles," *IEEE Micro*,
    vol. 35, pp. 60–68, 11 2015.

    - <span id="page-10-2"></span>[3] P. Bose, A. J. Vega, S. V. Adve, V. S. Adve,
    and V. J. Reddi, "Secure and resilient socs for autonomous vehicles," in *Proceedings
    of the 3rd International Workshop on Domain Specific System Architecture (DOSSA)*,
    pp. 1–6, 2021.

    - <span id="page-10-3"></span>[4] B. Yu *et al.*, "Building the computing system
    for autonomous micromobility vehicles: Design constraints and architectural optimizations,"
    in *Proceedings of 53rd Annual IEEE/ACM International Symposium on Microarchitecture
    (MICRO)*, pp. 1067–1081, 2020.

    - <span id="page-10-4"></span>[5] J. Zhao, B. Korpan, A. Gonzalez, and K. Asanovic,
    "Sonicboom: The 3rd generation berkeley out-of-order machine," in *Proceedings
    of the 4th Workshop on Computer Architecture Research with RISC-V (CARRV)*, pp.
    1–7, 2020.

    - <span id="page-10-5"></span>[6] H. Genc *et al.*, "Gemmini: Enabling systematic
    deeplearning architecture evaluation via full-stack integration," in *Proceedings
    of the 58th Annual Design Automation Conference (DAC)*, pp. 769–774, 2021.

    - <span id="page-11-0"></span>[7] "The 6 levels of vehicle autonomy explained."
    Available at: [https://www.synopsys.com/](https://www.synopsys.com/automotive/autonomous-driving-levels.html)
    [automotive/autonomous-driving-levels.](https://www.synopsys.com/automotive/autonomous-driving-levels.html)
    [html](https://www.synopsys.com/automotive/autonomous-driving-levels.html). Accessed
    on 2022-09-07.

    - <span id="page-11-5"></span>[8] O. Vermesan *et al.*, *IoT technologies for
    connected and automated driving applications. Internet of Things - The Call of
    the Edge*, pp. 306–332. River Publishers, Oct. 2020.

    - <span id="page-11-1"></span>[9] R. Coppola and M. Morisio, "Connected car: technologies,
    issues, future trends," *ACM Computing Surveys (CSUR)*, vol. 49, no. 3, pp. 1–36,
    2016.

    - <span id="page-11-2"></span>[10] T. Rateke *et al.*, "Passive vision region-based
    road detection: A literature review," *ACM Computing Surveys (CSUR)*, vol. 52,
    no. 2, pp. 1–34, 2019.

    - <span id="page-11-3"></span>[11] F. Bounini, D. Gingras, V. Lapointe, and H.
    Pollart, "Autonomous vehicle and real time road lanes detection and tracking,"
    in *IEEE Vehicle Power and Propulsion Conference (VPPC)*, pp. 1–6, 2015.

    - <span id="page-11-4"></span>[12] G. Zhang, N. Zheng, C. Cui, Y. Yan, and Z.
    Yuan, "An efficient road detection method in noisy urban environment," in *Proceedings
    of IEEE Intelligent Vehicles Symposium*, pp. 556 – 561, 2009.

    - <span id="page-11-6"></span>[13] A. Amid *et al.*, "Chipyard: Integrated design,
    simulation, and implementation framework for custom socs," *IEEE Micro*, vol.
    40, no. 4, pp. 10–21, 2020.

    - <span id="page-11-7"></span>[14] S. Karandikar *et al.*, "Firesim: Fpga-accelerated
    cycleexact scale-out system simulation in the public cloud,"


    in *Proceedings of ACM/IEEE 45th Annual International Symposium on Computer Architecture
    (ISCA)*, pp. 29–42, 2018.


    - <span id="page-11-8"></span>[15] "Amazon web services (aws)." Available at:
    [https:](https://aws.amazon.com/es) [//aws.amazon.com/es](https://aws.amazon.com/es),
    2021. Accessed on 2022-09- 07.

    - <span id="page-11-9"></span>[16] K. Asanovic *et al.*, "The rocket chip generator.
    eecs department," *University of California, Berkeley, Tech. Rep. UCB/EECS-2016-17*,
    vol. 4, 2016.

    - <span id="page-11-10"></span>[17] J. F. Canny, "Finding edges and lines in images,"
    *Theory of Computing Systems - Mathematical Systems Theory*, p. 16, 1983.

    - <span id="page-11-11"></span>[18] R. O. Duda and P. E. Hart, "Use of the hough
    transformation to detect lines and curves in pictures," *Communications of the
    ACM*, vol. 15, no. 1, p. 11–15, 1972.

    - <span id="page-11-12"></span>[19] M. J. Belda, "Image processing in autonomous
    vehicles on a risc-v with accelerator," *Master Thesis, UCM*, 2022.

    - <span id="page-11-13"></span>[20] A. Magyar *et al.*, "Golden gate: Bridging
    the resourceefficiency gap between asics and fpga prototypes," in *Proceedings
    of IEEE/ACM International Conference on Computer-Aided Design (ICCAD)*, pp. 1–8,
    2019.


    Citation: M.J. Belda, K. Olcoz, F. Castro and F. Tirado. *Optimization of a line
    detection algorithm for autonomous vehicles on a RISC-V with accelerator*. Journal
    of Computer Science & Technology, vol. xx, no. x, pp. x–x, 202x.


    ✔


    ✕


    DOI: 10.24215/16666038.18.e01


    ✗


    ✖


    Received: August x, 2022 Accepted: xxx. Copyright: This article is distributed
    under the terms of the Creative Commons License CC-BY-NC.'
  decisions:
    evaluation_prompt: 'Qualified. Reason: The paper includes an "experimental evaluation"
      section reporting a speedup of 3.7x, indicating quantitative analysis.'
    related_work_prompt: 'Disqualified: no related work. Reason: Lacks a section labeled
      “Related Work” or equivalent.'
