papers:
- title: A Statically and Dynamically Scalable Soft GPGPU
  abstract: "Current soft processor architectures for FPGAs do not utilize the potential\n\
    of the massive parallelism available. FPGAs now support many thousands of\nembedded\
    \ floating point operators, and have similar computational densities to\nGPGPUs.\
    \ Several soft GPGPU or SIMT processors have been published, but the\nreported\
    \ large areas and modest Fmax makes their widespread use unlikely for\ncommercial\
    \ designs. In this paper we take an alternative approach, building the\nsoft GPU\
    \ microarchitecture around the FPGA resource mix available. We\ndemonstrate a\
    \ statically scalable soft GPGPU processor (where both parameters\nand feature\
    \ set can be determined at configuration time) that always closes\ntiming at the\
    \ peak speed of the slowest embedded component in the FPGA (DSP or\nhard memory),\
    \ with a completely unconstrained compile into a current Intel\nAgilex FPGA. We\
    \ also show dynamic scalability, where a subset of the thread\nspace can be specified\
    \ on an instruction-by-instruction basis.\n  For one example core type, we show\
    \ a logic range -- depending on the\nconfiguration -- of 4k to 10k ALMs, along\
    \ with 24 to 32 DSP Blocks, and 50 to\n250 M20K memories. All of these instances\
    \ close timing at 771 MHz, a\nperformance level limited only by the DSP Blocks.\
    \ We describe our methodology\nfor reliably achieving this clock rate by matching\
    \ the processor pipeline\nstructure to the physical structure of the FPGA fabric.\
    \ We also benchmark\nseveral algorithms across a range of data sizes, and compare\
    \ to a commercial\nsoft RISC processor."
  url: http://arxiv.org/abs/2401.04261v1
  keywords: ''
  document: '# A Statically and Dynamically Scalable Soft GPGPU


    Martin Langhammer Intel Corporation & Imperial College London London, UK martin.langhammer@intel.com


    ## ABSTRACT


    Current soft processor architectures for FPGAs do not utilize the potential of
    the massive parallelism available. FPGAs now support many thousands of embedded
    floating point operators, and have similar computational densities to GPGPUs.
    Several soft GPGPU or SIMT processors have been published, but the reported large
    areas and modest Fmax makes their widespread use unlikely for commercial designs.
    In this paper we take an alternative approach, building the soft GPU microarchitecture
    around the FPGA resource mix available. We demonstrate a statically scalable soft
    GPGPU processor (where both parameters and feature set can be determined at configuration
    time) that always closes timing at the peak speed of the slowest embedded component
    in the FPGA (DSP or hard memory), with a completely unconstrained compile into
    a current Intel Agilex FPGA. We also show dynamic scalability, where a subset
    of the thread space can be specified on an instruction-by-instruction basis.


    For one example core type, we show a logic range – depending on the configuration
    – of 4k to 10k ALMs, along with 24 to 32 DSP Blocks, and 50 to 250 M20K memories.
    All of these instances close timing at 771 MHz, a performance level limited only
    by the DSP Blocks. We describe our methodology for reliably achieving this clock
    rate by matching the processor pipeline structure to the physical structure of
    the FPGA fabric. We also benchmark several algorithms across a range of data sizes,
    and compare to a commercial soft RISC processor.


    ## 1 INTRODUCTION


    FPGAs are capable platforms, with multiple thousands of embedded memories as well
    as DSP Blocks, many of which now support IEEE 754 floating point numerics. In
    addition, there is a significant amount of high performance IP available for FPGAs,
    e.g. FFTs [\[2,](#page-10-0) [8\]](#page-10-1) and error correction such as Reed-Solomon
    codecs [\[3\]](#page-10-2). High performance systems can readily be assembled
    using a combination of original design and these IP Blocks. The value of the FPGA
    is integration: although each individual IP or function is lower performance than
    ASIC, this is offset by the flexibility. However, modifying IP - even your own
    - requires significant effort. FPGA hardware compile times (synthesis, place and
    route) can take hours, and timing closure can be a significant unknown. Implementing
    (and modifying) a complex subset of a system by a pure software approach, where
    the result of the compile or assembly is essentially instantly available, and
    loaded onto an already placed and routed processor, may be very attractive.


    Soft RISC cores (Nios [\[9\]](#page-10-3) and MicroBlaze [\[4\]](#page-10-4))
    for FPGA have been used for over two decades, and allow the inclusion of complex
    control flow, or the offload of ancillary functions. Although these RISC processors
    are very flexible, they also have a rather low


    George A. Constantinides Imperial College London London, UK g.constantinides@imperial.ac.uk


    performance. Parallel processor architectures may offer better performance, and
    SIMT (GPGPU) processors may be able to efficiently use the large number of memory
    and DSP Blocks distributed across the FPGA device. There have been a number of
    soft SIMT FPGA architectures published [\[14,](#page-10-5) [15,](#page-10-6) [17,](#page-10-7)
    [18,](#page-10-8) [24,](#page-10-9) [25,](#page-10-10) [29\]](#page-10-11), but
    these are often very large (50K-300K LUTs), and typically have a low clock frequency
    (30MHz-100MHz). Other types of parallel processors are also known for FPGA [\[22,](#page-10-12)
    [30,](#page-10-13) [31\]](#page-10-14) (and commercialized [\[5\]](#page-10-15)),
    but the Fmax is relatively low at ∼150MHz.


    A different approach has been taken by Xilinx (now AMD) in the Versal devices,
    with arrays of AI Engines, a hardened VLIW processor. This motivates us to consider
    whether we can combine the flexibility of a soft processor (where any number can
    be instantiated into the soft fabric), but with the performance of an ASIC implementation
    (in this case, running at the speed of the embedded hardened features).


    Our design, which we call the eGPU (for embeddedGPU), is both statically and dynamically
    scalable, features which make it particularly useful and performant for FPGA applications.
    Static scalability is the ability to parameterize the thread space, shared memory
    space, integer ALU functions, as well as major processor features (such as predicates).
    Dynamic scalability allows us to operate on a defined subset of the thread space,
    and change this on an instruction by instruction basis, without any dead time.
    We will see that this can greatly reduce the number of cycles required in some
    portions of the program, such as during a vector reduction (which is a common
    kernel of GPGPU applications).


    We make the following contributions:


    - Describe a novel parameterized SIMT processor for FPGA, with a wide range of
    user defined instructions, as well as architectural trade-offs (such as predicates).

    - Demonstrate that a soft processor can consistently close timing at a level limited
    only by the embedded features such as DSP and memory, and do so with a completely
    unconstrained compile.

    - Compare the absolute and normalized (by resource cost) results of a soft GPGPU
    with a soft RISC processor, and show that the SIMT architecture is better in the
    general case, and significantly better when using dedicated hardware extensions.


    ## 2 BACKGROUND


    Our goal for this project was to architect and implement a compact, high performance
    SIMT processor, that can be used for commercial FPGA system designs. We can use
    current and prior FPGA processors both to understand the limitations of previous
    projects, and to validate some of our design choices. The axes of comparison to
    other work include memory systems, complexity (such as workload balancing), and
    trade-offs between hard and soft implementation.


    #### Table 1: Resource Comparison


    <span id="page-1-0"></span>


    | Architecture<br>Config. |          | LUTs | DSP  | FMax | PPA | Device     |  |

    |-------------------------|----------|------|------|------|-----|------------|--|

    | FGPU [15]               | 2CUx8PE  | 57K  | 48   | 250  | 36  | Zynq-7000  |  |

    | DO-GPU [29]             | 4CUx8PE  | 360K | 1344 | 208  | 133 | Stratix 10 |  |

    | FlexGrip [17]           | 1SMx16PE | 114K | 300  | 100  | 175 | Virtex-6   |  |

    | eGPU                    | 1SMx16SP | 5K   | 24   | 771  | 1   | Agilex     |  |


    Many of the previously published GPGPUs [\[15,](#page-10-6) [17,](#page-10-7)
    [24,](#page-10-9) [29\]](#page-10-11) are SIMT processors which were compiled
    to an FPGA, whereas eGPU was designed for FPGA. The eGPU has an power-performancearea
    (PPA) metric which is one or two orders of magnitude (OOM) smaller than some of
    the earlier soft GPGPUs. Comparisons between high-performance processor designs
    are complex and multidimensional. For example, some existing soft GPUs have more
    complex memory systems, including caches and dynamic workload balancing. This
    does come with a cost, with a typical order of magnitude resource difference,
    as can be seen in Table [1,](#page-1-0) where we compare configurations of the
    other soft GPGPUs that are closest in computational structure to eGPU (PEs are
    roughly the same as SPs). Despite the much deeper pipelines (e.g. FlexGrip [\[17\]](#page-10-7)
    has a 21 deep pipeline, FGPU has a 18 deep pipeline [\[15\]](#page-10-6)), they
    also run at a considerably slower clock frequency. Although they are implemented
    in older FPGA technology (FlexGrip is in Virtex-6 at 100MHz), this does not fully
    explain the performance level, as there are soft processors that run at 450MHz
    in those devices [\[20\]](#page-10-16) [\[21\]](#page-10-17). In the benchmarking
    section we will also see that the benchmarks also run slower than expected on
    the earlier GPGPUs based on the difference in clock frequency.


    Instead, we validate eGPU against existing soft RISC processors [\[9\]](#page-10-3),
    which are extensively used in real applications. We will normalize the benchmark
    results based on cost i.e. FPGA resources consumed. The eGPU, being a parallel
    processor (with essentially 16 smaller multi-threaded processors) will naturally
    be larger; to be effective and usable, it must have a clear advantage in both
    absolute performance and normalized efficiency over the RISC processors.


    eGPU uses a single local data memory, which is configurable in size, and does
    not support a cache. Larger datasets need to be externally managed. Like the eGPU,
    the Xilinx AI Engines [\[13\]](#page-10-18), which are organized as hard VLIW
    hard processor arrays, have only a single local data memory per CPU, the loading
    and unloading of which has to be managed externally. Algorithms with larger amounts
    of data (such as 4K FFTs) need to be split across multiple AI Engines [\[6\]](#page-10-19).
    The eGPU has a greater memory flexibility, as we are able to configure a larger
    shared memory instance (we show examples with up to 128KB in this paper). The
    AI Engines give us an example of a commercial FPGA parallel processor, where using
    multiple simpler processors have been found to have an advantage over using complex
    memory systems.


    ## 3 ARCHITECTURE DESCRIPTION


    The architecture of the eGPU is based on an earlier proof-of-concept design [\[28\]](#page-10-20).
    Our new design adds significant scalability - thread and register space, shared
    memory size, instruction set support, as well as optional predicates for thread
    divergence. Figure [1](#page-2-0) shows the top level architecture of the eGPU.
    The streaming multi-processor (SM) contains 16 parallel scalar processors (SP),
    although only 8 are shown in the figure for clarity. An optional dot-product core
    and special function unit (SFU) reciprocal square root can be attached. We target
    the Intel Agilex [\[23\]](#page-10-21) family of FPGAs in this work. The eGPU
    has a very short pipeline (8 stages) compared to other GPUs; therefore, hazards
    are hidden for most programs. Consequently, we do not provide hardware support
    for tracking hazards in the current version, which in turn gives us an efficient
    and fast processor.


    Two types of embedded memories are now supported, simple dual port (DP) and the
    emulated quad port (QP) blocks [\[12\]](#page-10-22). One of the largest performance
    limitations of the earlier eGPU architecture was memory bandwidth. The QP memory
    will double the write bandwidth, while at the same time reducing the number of
    embedded memory blocks required (the 20K-bit M20K blocks) by half. The trade-off
    is that in QP mode, the memory speed is reduced from 1 GHz to 600 MHz, which then
    becomes the critical path in the processor. Resource, Fmax, and benchmark results
    are all described later in this paper.


    ## 3.1 Dynamic Scalability


    Most GPGPUs support thread divergence by predicates (threadspecific conditionals)
    but these have a potential significant performance impact, as all threads are
    run, whether or not they are written back. In addition to predicates, the eGPU
    sequencer supports an instruction by instruction specification of a subset of
    the thread space, where only the indicated threads are run. If the program can
    be constructed such that the data of interest can be written to the threads that
    can be isolated by the dynamic thread allocation, then a large number of processing
    cycles can be skipped. This is particularly noticeable in programs with many multi-cycle
    instructions, such as reads and writes to shared memory. This will have a direct
    impact on the benchmark performance (number of cycles).


    We define a wavefront as the maximum number of operations that can be run per
    clock cycle; with 16 SPs we have a wavefront width of 16. The thread block depth
    (alternately, the wavefront depth) is the number of wavefronts per instruction,
    which is the initialized thread size / 16. We feel these terms allow us to describe
    our dynamic thread scalability more concisely.


    The eGPU can be configured, on a cycle by cycle basis, to act as a standard SIMT
    processor, a multi-threaded CPU, or a single threaded MCU. While the number of
    clock cycles to execute all the threads for an operation instruction (e.g. FP
    or INT) is dependent on the depth of the thread block, loads and stores are multi-cycle
    (because of the limited number of ports to shared memory). The impact of dynamically
    adjusting the width of certain instructions (e.g. reduction, where the writeback
    data can be orders of magnitude less than the read data) can be seen in the benchmark
    section later in this paper.


    The upper 4-bit field in the instruction word (IW) allows the wavefront width
    and depth to be coded for that instruction. Perhaps the most common case will
    be using only the first SP, or even the first thread in the first SP; many GPU
    applications will have vector reduction kernels, where a reduction result(s) may
    end up in the leftmost SP. If we can operate on this SP exclusively for a certain
    subset of time during the execution of the program, we A Statically and Dynamically
    Scalable Soft GPGPU


    <span id="page-2-0"></span>![](_page_2_Figure_1.jpeg)


    Figure 1: eGPU SM Top Level Architecture


    can save significant processing time, and power. The coding of the personality
    is described in the Instruction Set section.


    Hence, we can change the scale of the SIMT span by reducing the wavefront width
    and/or depth. The eGPU can act as a multithreaded CPU if we set the wavefront
    width to one, and if we also set the thread depth to one, each instruction will
    only act on thread 0 of the first SP - this SP can then be used like a MCU. We
    will use these modes to good effect in our benchmarks later in this paper.


    <span id="page-2-1"></span>![](_page_2_Figure_5.jpeg)


    ## 3.2 Predicates


    The eGPU optionally - by user configuration - supports predicates, which enable
    thread divergence. Conditionals can be applied to each thread individually using
    a condition instruction (see Table [2\)](#page-3-0).


    As with many aspects of eGPU, the number and type of conditions can be selected
    at compile time. Although these will have only a minimal impact on area, the additional
    wireload may impact performance because of the large number of individual predicate
    stacks. There is one predicate stack per initialized thread, so there may be thousands
    of stacks per eGPU instance.


    Some algorithms, such as the bitonic sort benchmark in this paper, require predicates.
    On the other hand, many of the signal processing applications that we expect that
    the eGPU will be used for (such as FFTs and matrix decomposition) do not use data
    dependent decisions. These do not need predicates, and can be programmed using
    only loop constructs, which are supported in the eGPU sequencer. For this reason,
    the presence and complexity of predication is a parameter of our design, especially
    considering the large potential cost of the feature.


    Figure [2](#page-2-1) shows the structure of a single predicate block. Each SP
    has a separate block, which are comprised of multiple predicate stacks. Each thread
    has a unique predicate stack. Multiple nested levels of conditional operations
    (IF/ELSE/END IF) are supported


    Figure 2: One Predicate Block


    per stack, with the maximum supported depth of nesting being parameterized.


    The incremental cost of adding one level of nesting is trivial, as the control
    logic of each predicate stack is the dominant user of logic here. The wavefront
    value (for example, in our base eGPU configuration of 512 threads with 16 SPs,
    there will be 32 wavefronts - i.e. 32 threads per SP) enables the correct predicate
    stack for the current thread. If the condition instruction (IF) condition is true
    for that thread, a ''1'' will be set at the top of the predicate stack, and the
    rest of the stack pushed down. An ELSE instruction will invert the top of the
    stack, and an END IF will pop the stack and return to the previous nesting level.


    The eGPU is configured at compile time for a maximum number of threads; if the
    run time configuration of threads is less than this, there is no issue as only
    the selected threads will trigger the operation of the predicate block.


    <span id="page-3-1"></span>


    | [43:40]  |        |      | [39:34] [33:32] [31:27] | [26:22] | [21:17] | [16:1]    |

    |----------|--------|------|-------------------------|---------|---------|-----------|

    | Variable | Opcode | Typ. | RD                      | RA      | RB      | Immediate
    |


    Figure 3: Instruction Word


    The conditional value will only be applied to the current predicate block, and
    all others ignored in that clock cycle. The current thread activation thread\_active
    signal will be muxed from all the predicate blocks, selected by the current wavefront.
    The thread\_active signal is used to pass or zero the write\_enable signals to
    either the register files or shared memory, whichever is the destination for that
    instruction.


    ## 4 INSTRUCTION SET


    Table [2](#page-3-0) shows most of the instruction set for the eGPU. There are
    a total of 61 instructions, including 18 conditional cases (we omit the FP conditional
    instructions here for brevity). Usually, only a subset of instructions are included
    (by the user defined configuration of the eGPU). The 18 conditional cases depend
    on predicates being included in the parameters - as predicates typically increase
    soft logic cost by 50% they are only used when the expected class of applications
    need them. Many of the intended applications, such as FFT, matrix multiplication
    and decomposition, do not, and the required loops can be handled with the dedicated
    loop instructions. Some instructions can support multiple TYPES, such as signed
    (INT32) and unsigned (UINT32) formats for integer instructions.


    The integer ALU uses a large proportion of the soft logic (≈100 ALMs to ≈400 ALMs),
    so selecting only the required precision (16 bit or 32-bit) and feature subset
    can reduce the cost of the eGPU substantially. Extension instructions are also
    optional. We will use the dot product instruction for some of the benchmarks in
    this paper; if used, it can make significant difference to the performance of
    some functions. We can also add elementary functions (currently we support only
    reciprocal square root), which are required for algorithms such as matrix decomposition.
    In contrast, the FP instructions are almost completely contained inside the DSP
    Block, with only the FP Max() and Min() instructions having a potential impact
    on area or performance.


    Figure [3](#page-3-1) shows an instruction word, here shown in a 43-bit form.
    As the number of registers per thread changes, the three register field widths
    also change; the displayed word is for a 32 registers per thread configuration,
    which requires 5 bits to encode the register number. The 2-bit representation
    field encodes whether the number is unsigned integer, signed integer, or FP32.
    The four most significant bits encode the processing type, which allow the wavefront
    depth and the width of the wavefront to be changed on an instruction by instruction
    basis.


    Writing these results into shared memory using subset write can be 16x faster
    than using the generic write. An instruction, whether used for a full or a partial
    thread space, is almost identical, with only the four instruction type bits used
    to control the subset of the thread space. Table [3](#page-3-2) shows how the
    upper 4 bits of the IW control the width and depth of the thread space.


    | Table 2: Instruction Set |  |

    |--------------------------|--|

    |--------------------------|--|


    <span id="page-3-0"></span>


    | Group              | Instruction            |                         |  |  |  |  |

    |--------------------|------------------------|-------------------------|--|--|--|--|

    |                    | ADD.TYPE Rd,Ra,Rb      | Rd = Ra + Rb            |  |  |  |  |

    |                    | SUB.TYPE Rd,Ra,Rb      | Rd = Ra - Rb            |  |  |  |  |

    | Integer Arithmetic | NEG.TYPE Rd,Ra         | Rd = -Ra                |  |  |  |  |

    |                    | ABS.TYPE Rd,Ra         | Rd = absolute(Ra)       |  |  |  |  |

    |                    | MUL16LO.TYPE Rd,Ra,Rb  | Rd = Ra * Rb            |  |  |  |  |

    |                    | MUL16HI.TYPE Rd,Ra,Rb  | Rd = (Ra * Rb)»16       |  |  |  |  |

    | Integer Multiply   | MUL24.LO.TYPE Rd,Ra,Rb | Rd = Ra * Rb            |  |  |  |  |

    |                    | MUL24.HI.TYPE Rd,Ra,Rb | Rd = (Ra * Rb)»24       |  |  |  |  |

    |                    | AND Rd,Ra,Rb           | Rd = Ra & Rb            |  |  |  |  |

    |                    | OR Rd,Ra,Rb            | Rd = Ra ∥ Rb            |  |  |  |  |

    |                    | XOR Rd,Ra,Rb           | Rd = Ra ⊕ Rb            |  |  |  |  |

    | Integer Logic      | NOT Rd,Ra              | Rd = !Ra                |  |  |  |  |

    |                    | cNOT Rd,Ra             | Rd = (Ra == 0)?1:0      |  |  |  |  |

    |                    | BVS Rd,Ra              | Rd = bit_reverse(Ra)    |  |  |  |  |

    |                    | SHL.TYPE Rd,Ra,Rb      | Rd = Ra ≪ Rb            |  |  |  |  |

    | Integer Shift      | SHR.TYPE Rd,Ra,Rb      | Rd = Ra ≫ Rb            |  |  |  |  |

    |                    | POP Rd,Ra              | Rd = unary(Ra)          |  |  |  |  |

    | Integer Other      | MAX.TYPE Rd,Ra,Rb      | Rd = (Ra>Rb)?Ra:Rb      |  |  |  |  |

    |                    | MIN.TYPE Rd,Ra,Rb      | Rd = (Ra<Rb)?Ra:Rb      |  |  |  |  |

    |                    | ADD.FP32 Rd,Ra,Rb      | Rd = Ra + Rb            |  |  |  |  |

    |                    | SUB.FP32 Rd,Ra,Rb      | Rd = Ra - Rb            |  |  |  |  |

    |                    | NEG.FP32 Rd,Ra         | Rd = -Ra                |  |  |  |  |

    | FP ALU             | ABS.FP32 Rd,Ra         | Rd = absolute(Ra)       |  |  |  |  |

    |                    | MUL.FP32 Rd,Ra,Rb      | Rd = Ra*Rb              |  |  |  |  |

    |                    | MAX.FP32 Rd,Ra,Rb      | Rd = (Ra>Rb)?Ra:Rb      |  |  |  |  |

    |                    | MIN.FP32 Rd,Ra,Rb      | Rd = (Ra<Rb)?Ra:Rb      |  |  |  |  |

    |                    | eq                     | 𝑅𝑎 == 𝑅𝑏                |  |  |  |  |

    |                    | ne                     | 𝑅𝑎 ≠ 𝑅𝑏                 |  |  |  |  |

    | Int Compare        | lt (INT), lo (UINT)    | 𝑅𝑎 < 𝑅𝑏                 |  |  |  |  |

    |                    | le (INT), ls (UINT)    | 𝑅𝑎 ≤ 𝑅𝑏                 |  |  |  |  |

    |                    | gt (INT), hi (UINT)    | 𝑅𝑎 > 𝑅𝑏                 |  |  |  |  |

    |                    | ge (INT), hs (UINT)    | 𝑅𝑎 ≥ 𝑅𝑏                 |  |  |  |  |

    | Memory             | LOD Rd (Ra)+offset     | Read from Shared        |  |  |  |  |

    |                    | STO Rd (Ra)+offset     | Write to Shared         |  |  |  |  |

    | Immediate          | LOD Rd #Imm            | Rd = Imm                |  |  |  |  |

    | Thread             | TDx Rd                 | Rd = Thread IDx         |  |  |  |  |

    |                    | TDy Rd                 | Rd = Thread IDy         |  |  |  |  |

    |                    | DOT Rd,Ra,Rb           | Dot Product ⟨𝑅𝑎, 𝑅𝑏⟩    |  |  |  |  |

    | Extension          | SUM Rd,Ra,Rb           | Reduction ⟨𝑅𝑎, 𝑅𝑏⟩<br>√ |  |  |  |  |

    |                    | INVSQR Rd,Ra           | 𝑅𝑑 = 1/<br>𝑅𝑎           |  |  |  |  |

    |                    | JMP address            | Jump to Address         |  |  |  |  |

    |                    | JSR address            | Subroutine Address      |  |  |  |  |

    |                    | RTS                    | Return from Subroutine  |  |  |  |  |

    | Control            | LOOP address           | Jump and Dec Loop Ctr   |  |  |  |  |

    |                    | INIT loops             | Set Loop Ctr            |  |  |  |  |

    |                    | STOP                   | Stop and Set Flag       |  |  |  |  |

    |                    | IF.cc                  | if cc true              |  |  |  |  |

    | Conditional        | ELSE                   | if cc false             |  |  |  |  |

    |                    | ENDIF                  | clear cc                |  |  |  |  |


    Table 3: Thread Space Control


    <span id="page-3-2"></span>


    | Coding | Width [4:3]             | Depth [2:1]          |  |  |  |

    |--------|-------------------------|----------------------|--|--|--|

    | "00"   | All (16 SPs)            | Wavefront 0 only     |  |  |  |

    | "01"   | 1/4 width (first 4 SPs) | all wavefronts       |  |  |  |

    | "10"   | SP0 only                | first 1/2 wavefronts |  |  |  |

    | "11"   | Undefined               | first 1/4 wavefronts |  |  |  |


    ## 5 RESULTS


    We compiled a number of different eGPU instances, using both DP and QP memory
    versions. We used Quartus Pro V22.4 and targeted an Intel Agilex AGIB027R29A1E1V
    device [\[7\]](#page-10-23). All of our results are reported for a single compilation
    attempt (we did not use seed sweeps).


    The DP memory results are tabulated in Table [4.](#page-6-0) We define three categories
    - small, medium, and large - to show the effects of different thread space, shared
    memory, and ALU features, as well as the impact of supporting predicates. The
    base eGPU architecture is the same for all instances: one SM with 16 SPs, a two
    read port register memory, and a four read and one write port shared memory. We
    configured all of these cases to use 512 threads, but with varying numbers of
    registers per thread. QP memory results are shown in Table [5,](#page-6-1) the
    main architectural change being the two write port shared memory.


    The ''small'' category uses a 16-bit ALU, which will likely only be used for address
    generation. The minimum specification supports only a single bit shift, as well
    as a 16-bit adder/subtractor, and arithmetic logic (AND/OR/XOR) operations. The
    memory requirements for the SPs is reduced by providing 16 registers per thread.
    The ''large'' category implements 64 registers per thread, and larger shared memory
    sizes with up to 128KB. The integer ALU supports the full set of integer instructions
    defined in the previous section. We also include a ''medium'' category for further
    examples. Many other combinations of parameters and features sets are possible
    as well.


    ## 5.1 Impact of Register and Shared Memory Size


    Both the thread registers and the shared memories are implemented using M20K memories,
    which can be configured into either DP (one read port and one write port active
    simultaneously) or QP memories (two read ports and two write ports active simultaneously).
    The natural datapath size of the eGPU is 32-bits, defined by the native IEEE 754
    single precision (FP32) floating point DSPs which will be doing the majority of
    the calculations. In DP mode, a M20K can be configured as a 512x32-bit memory.
    Port restrictions mean that in QP mode the M20K is a 2048x8-bit block, which requires
    a larger minimum thread register space to take advantage of the extra ports.


    In DP mode thread registers are implemented in two dual port memories, providing
    two read ports and one write port per clock cycle. In our most common eGPU configuration
    (with 16 registers per thread), a 512 thread machine will require two M20Ks per
    SP, or 32 M20Ks for thread registers in total, which is also the minimum size.
    Both the number of registers per thread and the number of total threads are parameterized,
    but the number of M20Ks will increase accordingly. In QP mode, the 8-bit data
    port width means that there is no point in using less than 2 thread registers
    per SP, although we will use half the number of M20K blocks compared to the DP
    version when we configure at least these number of registers.


    The shared memory is implemented as a four read port, one write port per memory
    in DP mode. The smallest possible shared memory is 512 words (2KB), which would
    require four M20Ks. This is very small, and unlikely to be useful, as the shared
    memory size would only be as large as the register space in a single SP. A more
    realistic shared memory size would be 2 words (8KB), which would require 16 M20Ks;
    the total memory usage for a small eGPU instance, including registers, would therefore
    be 48 M20Ks. The shared memory is set by parameter, and significantly larger sizes
    are possible without frequency impact. For example, a 64KB shared memory needs
    128 M20Ks, and a 128KB shared memory 256 M20Ks, which is a small fraction of the
    memories on the device. In QP


    mode, the number of M20Ks is halved, and the number of write ports doubled to
    two.


    ## 5.2 Integer ALU Architecture and Resources


    Unlike the floating point arithmetic, which can be mapped directly to a DSP Block,
    the simpler integer operations need to be implemented in soft logic. We will see
    that up to half of the soft logic and registers in an eGPU is required for the
    integer ALU. Table [6](#page-6-2) shows the resources, split by operation type,
    for a wide range of integer ALUs.


    The smallest reasonable integer ALU is a 16 bit version with single bit shifts,
    which consumes 90 ALMs and 136 registers, most of which are used for the 5 stage
    pipeline. Here we have a signed adder/subtractor, as well as logic functions (in
    this case, only AND, OR, and XOR are supported). The more typical full 16-bit
    ALU implementation supports signed and unsigned arithmetic, a more complete set
    of logic operations (AND/OR/XOR/NOT/cNOT/BVS), full 16-bit left and right shifts,
    population count, as well as max/min functions. The resource cost is approximately
    double that of the minimum ALU. The 5 stage pipeline 32-bit version again doubles
    the logic, as might be expected, but the number of registers triples, as individual
    functions (specifically the adder/subtractor and shifters) are themselves pipelined
    to ensure that the ALU always exceeds 800MHz. This contrasts with the 16-bit ALU,
    where the pipelining is used to improve the placement of the entire ALU, rather
    than improving the performance of any individual function. There is also a 4 stage
    pipeline version of the 32-bit integer ALU, which is about the size of the 16-bit
    full function ALU. This returns a lower performance (typically 700 MHz), and is
    used in order to save logic for the QP version of the eGPU (which has a lower
    target speed of 600MHz). The individual resource counts in Table [6](#page-6-2)
    may not accurately reflect the impact of each function to the overall ALU size,
    as synthesis may combine aspects of some functions together.


    ## 5.3 Predicate Resources


    In Table [4](#page-6-0) and [5](#page-6-1) the area impact of predicate support
    is clearly visible, increasing the soft logic resources by about 50%. While each
    predicate stack (including its control) is very small, each thread has a unique
    stack. The base predicate area consists of only a thread comparator (which checks
    that the SP currently executing the thread that the predicate circuit is associated
    with), an instruction decode (IF/ELSE/ENDIF), and the single bit-wide predicate
    stack. This may only be 5 ALMs per thread, but if a typical eGPU contains 1 threads,
    the predicate circuitry can quickly grow to be as large as the rest of the soft
    logic. Increasing the stack depth will have only a minimal impact on area, as
    each additional level consists of only a two input mux and a register.


    ## 5.4 Instruction Fetch, Decode, and Control


    This section will always have a modest footprint, requiring 200 to 250 ALMs, and
    a handful of M20Ks to store the instruction words. The instruction decoder takes
    about 40 ALMs, and the thread generator around 25 ALMs. A single M20K can store
    512 40-bit instruction words; the benchmarks we analyse later in this paper range
    from


    30 instructions (32 element reduction) to 250 instructions (256 element bitonic
    sort), so a multi-tenancy of programs would only need several M20Ks.


    Increasing the IW to 43 or 46 bits (which is required to support a 32 and 64 registers
    per thread, respectively), adds only a single M20K per 2 instructions, as the
    M20K containing the upper bits would be configured in 8 format. In any case, the
    number of M20Ks needed for program storage is small compared to the thread registers
    and shared memory. For example, a 1 word program space would require three M20Ks,
    and a 4 program space nine M20Ks.


    ## 5.5 Calculating Resources and Setting Performance


    Although the eGPU has a parameterized pipeline depth between the SPs and shared
    memory, it can achieve the target performance (771MHz and 600MHz respectively)
    using the minimum depth of 8 stages. The parameterized pipelining can be used
    for future applications with larger shared memories, or when the shared memories
    are placed elsewhere on the device, and not located near the SP array. We also
    report the slowest path outside the embedded (M20K and DSP) resources (see Table
    [4](#page-6-0) and [5\)](#page-6-1). If needed, there are also additional pipelining
    parameters inside the SP for the paths both to and from the FP and Integer ALUs.
    We will show in the next section how additional pipelining may not improve Fmax
    as the eGPU has been designed to fit into an Agilex sector in the base configuration.


    We can see that the SP overhead (mux and control) is ≈150 ALMs, the integer ALU
    ranges from ≈100 ALMs to ≈400 ALMs, and the predicates, if used, start from ≈
    150ALMs. A single SP will therefore be as small as 250 ALMs, and can be as large
    as 650 ALMs; this translates into a small eGPU core (16 SPs) requiring 4 ALMs,
    and over 10 ALMs for fully featured example.


    The number of M20Ks for the register memory for the DP eGPU can be calculated
    as threads×registers/256; for the shared memory the number of blocks is 2 × size().
    The number of M20K blocks required for the QP eGPU are half of this, except that
    there is a minimum size (threads × registers\_per\_thread/16 > 2047) for the number
    of registers, in which case the QP eGPU will need the same number of register
    blocks as the DP version.


    ## 5.6 FPGA Sector Resources and Impact


    It is most beneficial to select eGPU parameters around the available FPGA resources
    and their on-chip organization. The Intel Agilex devices are arranged in sectors,
    the most common of which contains about 16400 ALMs, 240 M20K memories, and 160
    DSP Blocks. Although we are not limited to a single sector (additional pipelining
    may be required to maintain performance across sector boundaries), this ratio
    of resources provides a good guide how to parameterize a eGPU instance. In particular,
    creating too large a register or memory space will be inefficient, as the ALMs
    between the M20K columns will likely be unreachable by other designs in the FPGA.
    Likewise, there is no point in specifying a small register space or shared memory,
    as the M20Ks between the logic structures may not be accessible by other functions.
    Further analysis is provided in the following section where we demonstrate that
    by selecting parameters in this way, the eGPU consistently achieves the reported


    performance levels by matching its architecture with the sector structure.


    ## 6 REPEATABLE HIGH PERFORMANCE


    This section provides the required information to make our design process repeatable
    for those wishing to achieve high performance in their own designs. It is therefore
    necessarily ''close to metal'' in abstraction. Although the details are specific
    to Intel FPGAs, we believe the same approaches are valid for all other FPGAs as
    well.


    The eGPU is designed to give consistent performance, which will always be limited
    by the slowest embedded (DSP or M20K memory) resource in that configuration. The
    clock network in Agilex is specified at 1GHz, which is the absolute limit of performance
    for any design. The M20K memories in DP mode also achieve 1GHz, but only 600 MHz
    in QP mode. The DSP Blocks can run at 771 MHz when implementing a FP32 multiply-add
    datapath with a 4 stage pipeline [\[11\]](#page-10-24). We are therefore limited
    to a maximum speed of 771MHz, unless we use QP memory, in which case the maximum
    frequency drops to 600MHz. The lower performance of the QP memory, however, will
    allow us to support a higher density storage, and the doubled write bandwidth
    may offer an overall higher throughput for some applications. We will examine
    some of these trade-offs in the benchmarking section.


    Using the sector architecture effectively enables the eGPU performance and efficiency.
    Sector resources are arranged in columns, each approximately 41 rows high (several
    columns are shorter because of non-user accessible device features). Achieving
    a 1GHz speed for soft logic does not require logic to be immediately adjacent
    to each other, as there are different horizontal and vertical wire lengths - too
    much pipelining can negatively impact performance as much as too little. More
    important is using the minimal number of wire resources per connection. In the
    Agilex devices, there is a constant 4 columns of logic between each column of
    either DSP or M20K. In a sector we will have 40 columns of logic, 4 columns of
    DSP, and 6 columns of M20K. There is little point in saving logic or memory if
    it is not accessible by other portions of design.


    As we have shown in the previous section, the results are deterministic and repeatable,
    in both area and performance. Ideally, the resource use would be balanced to realize
    the maximum efficiency from the device.


    To map eGPU to the device, we first sketched out a LUT level construct of an SP,
    and adjusted it so that the number of logic levels would align with the sector
    column ratios described above. Paths directly between M20K memories (which implement
    the thread registers in each SP) and the DSP Blocks had to fit into a 4 column
    group of LABs, and longer pure logic paths (e.g. the integer ALUs) were organized
    so that the total area did not spill over into a M20K or DSP column that might
    be used by another SP.


    We can see from the results (Table [6\)](#page-6-2) that a 16-bit Integer ALU
    is in the range of 100-200 ALMs and the 32-bit version requires 200- 400 ALMs.
    If predicates are used, they will cost an additional 125-250 ALMs per SP, depending
    on the defined thread space. The remaining 150 ALMs per SP are used for the data
    muxing and alignment shown in Figure [1.](#page-2-0) We were able to implement
    a small eGPU (the first example in Table [4\)](#page-6-0) that was able to close
    timing over 771 MHz with no dedicated soft logic registers (i.e. registers that
    were not


    <span id="page-6-0"></span>


    | Scale  | ALU<br>Precision | Shift<br>Precision | Threads | Reg.<br>Thread |
    Shared<br>Memory | Predicate<br>Levels | ALM   | Registers | DSP | M20K | Freq<br>(MHz)
    | SP<br>(ALM/Reg.) |

    |--------|------------------|--------------------|---------|----------------|------------------|---------------------|-------|-----------|-----|------|---------------|------------------|

    | Small  | 16               | 1                  | 512     | 16             |
    8KB              | 0                   | 4243  | 13635     | 24  | 50   | 1018/771      |
    224/707          |

    | Small  | 16               | 16                 | 512     | 16             |
    32KB             | 5                   | 7518  | 18992     | 24  | 98   | 898/771       |
    413/979          |

    | Medium | 16               | 16                 | 512     | 32             |
    32KB             | 5                   | 7579  | 19155     | 24  | 131  | 883/771       |
    426/1043         |

    | Medium | 32               | 16                 | 512     | 32             |
    32KB             | 5                   | 9754  | 25425     | 24  | 131  | 902/771       |
    461/1277         |

    | Large  | 32               | 16                 | 512     | 64             |
    32KB             | 8                   | 10127 | 26040     | 32  | 195  | 860/771       |
    575/1505         |

    | Large  | 32               | 32                 | 512     | 64             |
    64KB             | 16                  | 10697 | 26618     | 32  | 259  | 841/771       |
    600/1476         |


    #### Table 4: Fitting Results - DP Memory


    #### Table 5: Fitting Results - QP Memory


    <span id="page-6-1"></span>


    | Scale  | ALU       | Shift     | Threads | Regs./ | Shared | Predicate | ALM   |
    Registers | DSP | M20K | Freq    | SP         |

    |--------|-----------|-----------|---------|--------|--------|-----------|-------|-----------|-----|------|---------|------------|

    |        | Precision | Precision |         | Thread | Memory | Levels    |       |           |     |      |
    (MHz)   | (ALM/Reg.) |

    | Small  | 32        | 1         | 512     | 64     | 32KB   | 0         | 5468  |
    14487     | 24  | 98   | 840/600 | 287/830    |

    | Medium | 32        | 32        | 1024    | 32     | 64KB   | 0         | 7057  |
    16722     | 32  | 131  | 763/600 | 396/1016   |

    | Large  | 32        | 32        | 1024    | 32     | 64KB   | 16        | 11314
    | 25050     | 32  | 131  | 763/600 | 685/1601   |

    | Large  | 32        | 32        | 1024    | 32     | 128KB  | 10        | 10174
    | 23094     | 32  | 195  | 714/600 | 556/1391   |


    Table 6: Fitting Results - Integer ALU


    <span id="page-6-2"></span>


    | Prec. | Type  | ALM | Registers | Add/<br>Sub | Logic | SHL | SHR | Pop |

    |-------|-------|-----|-----------|-------------|-------|-----|-----|-----|

    | 16    | Min   | 90  | 136       | 3           | 9     | -   | -   | -   |

    | 16    | Small | 134 | 207       | 9           | 10    | 20  | 23  | -   |

    | 16    | Full  | 199 | 269       | 9           | 18    | 20  | 23  | 11  |

    | 32    | Min   | 208 | 406       | 5           | 27    | 28  | 28  | -   |

    | 32    | Full  | 394 | 704       | 27          | 36    | 50  | 53  | 27  |


    directly connected with a logic function, such as a mux), but for the generic
    parameterized case, we added a single additional pipeline stage register between
    the thread registers and the functional units, and also one level in the write-back
    path between the functional units and the thread registers. For all of the examples
    in Table [4](#page-6-0) there are also single pipeline stages to and from the
    shared memory. We parameterized the pipeline depth for all of these stages, along
    with the appropriate balancing delays for the data and control paths into the
    write paths of the thread registers, but found that these were not needed to be
    increased beyond one pipeline stage for any of the reported examples.


    Figure [4](#page-7-0) shows the unconstrained placement of the largest instance
    of Table [4.](#page-6-0) The shared memory and 8 out of the 16 SPs have been color
    coded for identification. The shared memory creates a spine in the middle of the
    core, with 8 SPs placed on either side of it. For purposes of illustration we
    have colored a subset of SPs: three random SPs and the left of the spine, and
    five contiguous ones on the right. Three things are evident with all SPs: (a)
    the majority of the logic is in one contiguous block, (b) there is a separate
    contiguous structure (the predicate block) placed some distance away, and (c),
    the SP straddles a columns of DSP Blocks. All of the instances of Table [4](#page-6-0)
    and [5](#page-6-1) display this pattern, including the shared memory spine.


    Figure [5](#page-7-1) shows one of the SPs in greater detail (this SP is the one
    marked by the black boxes in Figure [4\)](#page-7-0). The largest component is
    the integer ALU. The operators (adder/subtractor, shifters, arithmetic logic,
    etc.) are in the 4 columns to the right of the two DSP blocks (the DSP Block for
    the floating point operators is adjacent to the integer multiplier). To the left
    of the DSP Blocks is largely pipelining logic - of the 5 pipeline stages in the
    ALU, only one is used for pipelining the operators - the rest is used to break
    up the paths between the thread register memories and the ALUs. We examined all
    of the SP placements, and the placement of the M20Ks for the register memories
    (8 M20Ks for this instance) was in one of three layouts: (a) a contiguous single
    column (b) most of the registers in one column, with a smaller number in the next
    column further away from the integer ALU, and (c) equally split between two columns
    on either side of the integer ALU. In all of these cases, the pipeline wrapper
    around the ALU was usually grouped together, and essentially separate from the
    actual operators. Rather than having to be in a specific location relative to
    the M20Ks and operators, the ability to split up a bus so that it can be mapped
    to the same number of wire hops is what was important. Fewer pipeline stages would
    have introduced a two stage routing path, which would have likely become the critical
    path in the eGPU. (In the QP memory version, we can remove one of the pipeline
    stages as the M20K becomes the slowest component at 600 MHz, and we can see that
    the removal of some of the pipeline path reduce the non-memory path performance
    to just over 700 MHz). On the other hand, more than a 5 stage integer ALU could
    potentially decrease performance as it could spread out the placement of the SP.


    The predicate circuitry is placed in another contiguous block, but well away from
    the SP core it is associated with. From Figure [4](#page-7-0) we can see that
    the majority of the other predicates have a similar relationship with their respective
    SP. All of these have been automatically placed by Quartus. This is possible because
    the interface to and from the predicate block is very narrow, with only a single
    bit (thread\_active signal) returned. The signals to the block are relatively
    few: a thread index (typically 5 to 8 bits wide), a 3-bit decoded instruction
    signal (IF/ELSE/ENDIF), and a single bit valid condition code. Although there
    are many possible conditions from many different instructions, these can be decoded
    into a single


    <span id="page-7-0"></span>![](_page_7_Figure_1.jpeg)


    Figure 4: eGPU Placement


    valid condition bit in the main SP body. These narrow busses give us flexibility
    to wrap multiple pipes around the relatively simple (consisting largely of a chained
    registers organized in individual stacks) predicate blocks, which makes it possible
    for the tool to place them almost completely independently of the main datapaths.


    To create repeatable high performance designs, we need to understand both the
    structure, and the position of embedded features to each other. Here we are using
    integer ALUs which range in size by four times, our logic and memory density is
    very high, but our performance always exceeds that of the slowest embedded feature.
    It is possible to build a completely different type of CPU (or indeed any other
    type of core) and achieve this type of performance via a push button flow, but
    the architecture of the FPGA needs to be considered at every stage of the IP architecture
    phase.


    ## 7 BENCHMARKS


    We ran a number of workloads of different types to evaluate absolute and relative
    performance of the eGPU for varying data sizes that we might expect for embedded
    applications. We also profiled all the workloads to examine the efficiency of
    the eGPU. For continuity we selected many of the same benchmarks as used by Flexgrip
    [\[16\]](#page-10-25). We chose vector reduction, matrix transpose, and matrix-matrix
    multiply (MMM), as these would be common building blocks for many GPGPU applications.
    Bitonic sort [\[19\]](#page-10-26) is a sorting algorithm suited for parallel
    processing. Instead of the simpler autocorrelation, we used the FFT, as we felt
    this would be more representative of the


    <span id="page-7-1"></span>![](_page_7_Figure_7.jpeg)


    Figure 5: Single SP Placement


    workloads expected for the eGPU. All benchmarks were written in assembly code
    (we have not written our compiler yet).


    We report the comparison to FlexGrip only for the MMM, as the larger dataset size
    would be less affected by any overheads for setup and data transfer. We see that
    there is a significant performance advantage in favor of eGPU in cycle time alone.
    We ran all benchmarks, except the FFT, for which there are no reported FlexGrip
    results. FlexGrip underperforms eGPU by a factor of ≈31x, averaged over all benchmarks.
    We did not compare against DO-GPU (which is the latest iteration of FGPU), as
    DO-GPU normalized size is 50x-100x greater than eGPU.


    Our reported measurements are all based on core performance: we start the clock
    once the data has been loaded into the shared memory, and stop the clock once
    the final result has been written back to the shared memory. The most likely use
    of the eGPU is to apply multiple algorithms to the same data - .. there is no
    loading and unloading of data between different algorithms. For completeness,
    we also ran all of our benchmarks taking into account the time to load and unload
    the data over the 32-bit wide data bus. The performance impact was only 4.7%,
    averaged over all benchmarks.


    Clock frequency was 771 MHz for eGPU (including where the Dot Product operator
    is used), and 600 MHz for eGPU-QP variant. We compare both cycle counts and elapsed
    time for the eGPUs with the two shared memory architectures, and also the impact
    of the optional Dot Product core for reduction and MMM benchmarks. We then normalize
    the performance (time), by the resource cost, which we calculated on the basis
    of ALMs and DSP Blocks. We estimate that the effective cost of a DSP block is
    100 ALMs, which we calculate as follows: we start with the ALM count of the pure
    soft logic implementation of a FP32 multiply and adder (approximately 650 ALMs
    [\[10\]](#page-10-27)), and add 50% area to this number for DSP Block overhead
    (a DSP Block contains considerable additional features). We then divide by 10
    for an approximate soft logic to hard logic


    <span id="page-8-1"></span><span id="page-8-0"></span>


    |                                         |                                             |
    Vector Reduction               |                |                  | Matrix Transpose   |                      |                      |
    Matrix x Matrix          |                             |                                       |              |                            |                                             |

    |-----------------------------------------|---------------------------------------------|--------------------------------|----------------|------------------|--------------------|----------------------|----------------------|--------------------------|-----------------------------|---------------------------------------|--------------|----------------------------|---------------------------------------------|

    | Dimension                               | Metric                                      |
    Nios                           | eGPU           | eGPU             | eGPU               |
    Nios                 | eGPU                 | eGPU                     | Nios                        |
    FlexGrip                              | eGPU         | eGPU                       |
    eGPU                                        |

    |                                         |                                             |                                |
    DP             | QP               | Dot                |                      |
    DP                   | QP                       |                             |                                       |
    DP           | QP                         | Dot                                         |

    |                                         | Cycles                                      |
    459                            | 168            | 160              | 62                 |
    21809                | 1720                 | 1208                     | 1.45M                       |
    2.14M                                 | 111546       | 103354                     |
    19800                                       |

    |                                         | Time(us)                                    |
    1.32                           | 0.22           | 0.27             | 0.08               |
    62.85                | 2.23                 | 2.01                     | 4179                        |
    21400                                 | 144.7        | 172.3                      |
    25.7                                        |

    | 32                                      | Ratio(cycles)                               |
    2.73                           | 1.0            | 0.95             | 0.37               |
    12.68                | 1.0                  | 0.7                      | 13.03                       |
    19.2                                  | 1.0          | 0.93                       |
    0.18                                        |

    |                                         | Ratio(time)                                 |
    6.01                           | 1.0            | 1.23             | 0.37               |
    28.18                | 1.0                  | 0.9                      | 28.97                       |
    147.9                                 | 1.0          | 1.19                       |
    0.18                                        |

    |                                         | Normalized                                  |
    1.14                           | 1.0            | 1.4              | 0.45               |
    5.33                 | 1.0                  | 1.02                     | 5.48                        |
    -                                     | 1.0          | 1.35                       |
    0.21                                        |

    |                                         | Cycles                                      |
    1803                           | 202            | 194              | 94                 |
    86609                | 5529                 | 3481                     | 11.6M                       |
    16.6M                                 | 451066       | 418671                     |
    84425                                       |

    |                                         | Time(us)                                    |
    5.20                           | 0.26           | 0.32             | 0.12               |
    249.6                | 7.17                 | 5.80                     | 33383                       |
    166000                                | 585.0        | 697.8                      |
    109.5                                       |

    | 64                                      | Ratio(cycles)                               |
    8.93                           | 1.0            | 0.96             | 0.47               |
    15.66                | 1.0                  | 0.63                     | 25.7                        |
    36.8                                  | 1.0          | 0.93                       |
    0.19                                        |

    |                                         | Ratio(time)                                 |
    19.98                          | 1.0            | 1.23             | 0.47               |
    34.81                | 1.0                  | 0.81                     | 57.1                        |
    284                                   | 1.0          | 1.19                       |
    0.19                                        |

    |                                         | Normalized                                  |
    3.78                           | 1.0            | 1.4              | 0.60               |
    6.59                 | 1.0                  | 0.92                     | 10.80                       |
    -                                     | 1.0          | 1.35                       |
    0.23                                        |

    |                                         | Cycles                                      |
    3595                           | 216            | 208              | 101                |
    345233               | 20481                | 12649                    | 92.5M                       |
    441.2M                                | 2342356      | 2212136                    |
    886452                                      |

    |                                         | Time(us)                                    |
    10.36                          | 0.28           | 0.35             | 0.13               |
    994.91               | 26.56                | 21.08                    | 266491                      |
    4412.1                                | 3038.1       | 3686.9                     |
    1149.7                                      |

    | 128                                     | Ratio(cycles)                               |
    16.64                          | 1.0            | 0.96             | 0.47               |
    16.86                | 1.0                  | 0.62                     | 39.47                       |
    188.3                                 | 1.0          | 0.94                       |
    0.38                                        |

    |                                         | Ratio(time)                                 |
    37.00                          | 1.0            | 1.23             | 0.47               |
    37.45                | 1.0                  | 0.79                     | 87.71                       |
    1452                                  | 1.0          | 1.21                       |
    0.38                                        |

    |                                         | Normalized                                  |
    7.00                           | 1.0            | 1.4              | 0.60               |
    7.09                 | 1.0                  | 0.90                     | 1659                        |
    -                                     | 1.0          | 1.37                       |
    0.46                                        |

    |                                         |                                             |                                |                |                  |                    |                      |                      |                          |                             |                                       |              |                            |                                             |

    | Bitonic 64<br>Bitonic 32<br>Bitonic 128 | Bitonic 256<br>Bitonic 32QP<br>Bitonic
    64QP | Bitonic 128QP<br>Bitonic 256QP | FFT32<br>FFT64 | FFT128<br>FFT256 | FFT32QP<br>FFT64QP
    | FFT128QP<br>FFT256QP | Reduce32<br>Reduce64 | Reduce32Dot<br>Reduce128 | Reduce64Dot<br>Reduce128Dot
    | MMM 64x64<br>MMM 32x32<br>MMM 128x128 | MMM32x32QP   | MMM64x64QP<br>MMM128x128QP
    | MMM32x32Dot<br>MMM64x64Dot<br>MMM128x128Dot |

    | FP OP                                   | INT OP                                      |                                |
    IMM OP         |                  | Branch             | Load                 |                      |
    Save                     | Predicate                   |                                       |
    Thread Setup |                            | NOP                                         |


    Table 7: Vector and Matrix Benchmarks


    Figure 6: Benchmark Profiling (Y-Axis shows proportion of instructions executed
    by type).


    scaling factor (earlier work [\[26\]](#page-10-28) suggested a higher ratio in
    the general case, but recent work [\[27\]](#page-10-29) described more efficient
    ways of mapping arithmetic, especially multipliers, to FPGAs). We report normalized
    cost (considering both elapsed time and resources, with eGPU-DP as the baseline).


    As a comparison, we ran all of the benchmarks on Nios IIe [\[1\]](#page-10-30),
    which is a mature RISC processor for Intel FPGAs. The configuration we used consumed
    1100 ALMs (plus 3 DSP Blocks, giving a normalized cost of 1400), and closed timing
    at 347 MHz. We did not profile the Nios code, but analyzed the efficiency of operation
    (CPI). Most of the benchmarks retired an instruction every 1.7 clock cycles, except
    for the matrix-matrix multiplies and FFT, which required about 3 clocks, because
    of the way that 32×32 multipliers


    were implemented. (For simplicity, we replaced the FP32 arithmetic with INT32
    for the Nios examples).


    For the vector and matrix benchmarks, we chose an eGPU configuration with 32 registers
    per thread, with a 32 bit ALU, and a 128KB shared memory. This configuration has
    an equivalent cost (see Table [4](#page-6-0) and [5\)](#page-6-1) of 7400, 8400,
    and 9000 ALMs for the eGPU-DP, eGPU-QP, and eGPU-Dot variants respectively. Depending
    on the configuration, eGPU is 5× to 6× larger than Nios (but also more than twice
    the operating frequency). We would therefore expect (or at least hope) that eGPU
    would give an OOM performance increase over Nios.


    We can deduce the mechanism of the matrix transpose benchmarks from Table [7](#page-8-0)
    directly. For a given × matrix, we know that the eGPU will need 2 cycles to write
    the transposed elements


    to shared memory and 1/4th of those cycles to initially read them into the SP
    threads. We can see that the number of cycles clocked is marginally larger than
    this; these are largely used for the integer instructions needed to generate the
    transposed write addresses. We expect that the eGPU-QP will require about 40%
    fewer cycles, being able to write two transposed elements per clock, which indeed
    is the case.


    The vector reduction needs inter-SP communication, which go through the shared
    memory, which is the performance bottleneck in the eGPU. Table [7](#page-8-0)
    shows the impact of memory accesses on reduction performance. The actual floating
    point operations are a relatively small (≈10%) component of the reduction, with
    the majority of the cycles used by the memory operations. If we are using the
    dot product operator, there are even fewer FP operations required, and most of
    the time is spent waiting (NOPs) for the dot product to write back to the SP.
    All final vector reductions end up in the first SP, and we can use the multi-threaded
    CPU or MCU eGPU dynamic scaling personalities to write these values to the shared
    memory.


    The MMMs are much more complex. Although the algorithm itself is very simple,
    consisting only of a three level loop, the standard GPU implementation requires
    a vector reduction. While the cycle count increases as expected (∼ 4×) from 32×32
    to 64×64, there is an unexpected jump from 64×64 to 128×128, which is particularly
    evident in the eGPU-Dot case. Analysis of the code shows that while we are able
    to store the entire matrix (or at least a majority of the matrix) in the SP registers
    (there are 16384 total registers across the 16 SPs in the configuration we have
    chosen here) for the 32×32 and 64×64 cases, we need to keep reloading portions
    of the matrix in the 128×128 case, which can also be seen in the profile stack
    in Figure [6.](#page-8-1) Of course, there is always the option of increasing
    the maximum thread space or registers per thread (through parameterization) if
    the expected workloads were larger matrices. Compared to the vector reduction
    (where we profile a single vector), the thread initialization and integer operations
    are amortized away as we operate on many vectors. The NOPs also disappear as the
    the thread depth increases here.


    The bitonic sort benchmark requires a wider mix of instructions. Predicates are
    required, which increases the effective cost of the eGPU core by about 50%. The
    smaller sorts require many NOPs, which progressively reduce as the number of wavefronts
    increase for the larger datasets. The nature of the bitonic sort tends to use
    many subroutine calls, which we can see here in the relatively large number of
    branch operations. Again, the memory operations take the majority of all cycles,
    as each pass of the sort requires a redistribution of the data among the SPs.
    While the eGPU-QP version requires fewer clock cycles because of the increased
    write bandwidth, the normalized cost of the QP version is higher, largely because
    of the lower clock frequency.


    A similar pattern of instruction distribution is seen in the FFT. Increasing wavefront
    depth for larger datasets reduces NOPs significantly. The number of FP instructions
    (which are doing the actual FFT calculations) is relatively small, at about 10%.
    The largest proportion of operations are once again the memory accesses, especially
    in the write to shared memory; using the QP version of the eGPU results in a 20%
    to 30% decrease in total cycles. The normalized cost of the two eGPU versions,
    however, is approximately the


    Table 8: Bitonic Sort and FFT Benchmarks


    |     |               |        | Bitonic Sort |       | FFT    |      |            |  |  |

    |-----|---------------|--------|--------------|-------|--------|------|------------|--|--|

    | Dim | Metric        |        | eGPU         | eGPU  |        | eGPU | eGPU<br>QP
    |  |  |

    |     |               | Nios   | DP           | QP    | Nios   | DP   |            |  |  |

    |     | Cycles        | 8457   | 1742         | 1543  | 9165   | 876  | 714        |  |  |

    |     | Time(us)      | 24.37  | 2.25         | 2.51  | 26.41  | 1.14 | 1.19       |  |  |

    | 32  | Ratio(cycles) | 4.89   | 1.0          | 0.86  | 10.46  | 1.0  | 0.82       |  |  |

    |     | Ratio(time)   | 10.8   | 1.0          | 1.1   | 23.16  | 1.0  | 1.04       |  |  |

    |     | Normalized    | 1.24   | 1.0          | 1.24  | 4.38   | 1.0  | 1.18       |  |  |

    |     | Cycles        | 20687  | 3728         | 3054  | 20848  | 1695 | 1312       |  |  |

    |     | Time(us)      | 59.6   | 4.83         | 5.09  | 60.08  | 2.20 | 2.19       |  |  |

    | 64  | Ratio(cycles) | 5.54   | 1.0          | 0.82  | 12.30  | 1.0  | 0.82       |  |  |

    |     | Ratio(time)   | 12.3   | 1.0          | 1.05  | 27.31  | 1.0  | 1.01       |  |  |

    |     | Normalized    | 1.42   | 1.0          | 1.18  | 5.17   | 1.0  | 1.13       |  |  |

    |     | Cycles        | 49741  | 8326         | 6536  | 46667  | 3463 | 2558       |  |  |

    |     | Time(us)      | 143.3  | 10.8         | 10.9  | 134.49 | 4.29 | 4.26       |  |  |

    | 128 | Ratio(cycles) | 5.97   | 1.0          | 0.79  | 13.48  | 1.0  | 0.74       |  |  |

    |     | Ratio(time)   | 13.2   | 1.0          | 1.01  | 31.35  | 1.0  | 0.95       |  |  |

    |     | Normalized    | 1.48   | 1.0          | 1.13  | 5.93   | 1.0  | 1.08       |  |  |

    |     | Cycles        | 149271 | 16578        | 11974 | 103636 | 6813 | 4736       |  |  |

    |     | Time(us)      | 430.2  | 21.5         | 19.9  | 298.66 | 8.84 | 7.89       |  |  |

    | 256 | Ratio(cycles) | 9.0    | 1.0          | 0.72  | 15.21  | 1.0  | 0.70       |  |  |

    |     | Ratio(time)   | 20.0   | 1.0          | 0.93  | 33.79  | 1.0  | 0.89       |  |  |

    |     | Normalized    | 2.24   | 1.0          | 1.05  | 6.39   | 1.0  | 1.01       |  |  |


    same, with the high clock frequency of the base version offsetting the higher
    memory bandwidth of the QP version. These results also point to a better optimization
    for the FFT: by using a higher radix FFT, there will be correspondingly fewer
    passes through the shared memory. (We have a extensive flexibility in specifying
    the register and thread parameters, we can easily support much higher radices,
    which will require much larger register spaces).


    Comparing against Nios, we can see that the eGPU performs very well. We see at
    least an OOM performance difference based on time, and in almost all cases on
    a cycle basis as well. This tells us that eGPU is a more efficient architecture
    than a RISC processor, and is a viable candidate for a soft accelerator core.


    ## 8 CONCLUSIONS


    We have demonstrated a GPGPU that consistently beats 770 MHz for a wide range
    of parameters, and described the design approach required to reach such frequencies.
    We are able to swap in and out features as well as change the precision of the
    integer ALU to optimize for area and resource balancing in the FPGA.


    For the eGPU to be useful in an actual system design, it must offer an improvement
    over known methods. We compare the eGPU to a mature commercial soft CPU (Nios)
    over a number of benchmarks. The eGPU is much better on a cycle by cycle or elasped
    time basis in all cases we tried (typically by one to two OOM), and is still better
    on an area normalized basis. When we add the dot product core which can be used
    directly by the eGPU in a regular GPGPU context - the advantage can increase again
    by several times. A soft GPU therefore can offer a valid implementation option
    for many types of algorithms. This does not mean that a GPGPU will replace the
    RISC, anymore than a discrete GPGPU will replace a discrete RISC, only that we
    have shown that the soft GPGPU can now be considered for commercial designs, rather
    than just being of academic interest. The eGPU only uses 1%-2% of a current mid-range
    device, making it a cost effective option to implement complex algorithms in a
    larger FPGA system design, even if multiple cores are required.


    A Statically and Dynamically Scalable Soft GPGPU


    ## REFERENCES


    - <span id="page-10-30"></span>[1] 2016. Nios II Classic Processor Reference GuideNios
    II Classic Processor Reference Guide. [https://www.intel.com/content/www/us/en/docs/programmable/683620/](https://www.intel.com/content/www/us/en/docs/programmable/683620/current/overview-67435.html)
    [current/overview-67435.html.](https://www.intel.com/content/www/us/en/docs/programmable/683620/current/overview-67435.html)

    - <span id="page-10-0"></span>[2] 2017. FFT IP Core: User Guide. [https://www.intel.co.uk/content/www/uk/en/](https://www.intel.co.uk/content/www/uk/en/products/details/fpga/intellectual-property/dsp/fft.html)
    [products/details/fpga/intellectual-property/dsp/fft.html.](https://www.intel.co.uk/content/www/uk/en/products/details/fpga/intellectual-property/dsp/fft.html)

    - <span id="page-10-2"></span>[3] 2017. High-speed Reed-Solomon IP Core User Guide.
    [https://www.intel.com/](https://www.intel.com/content/www/us/en/docs/programmable/683120/17-1/about-the-high-speed-reed-solomon-ip-core.html)
    [content/www/us/en/docs/programmable/683120/17-1/about-the-high-speed](https://www.intel.com/content/www/us/en/docs/programmable/683120/17-1/about-the-high-speed-reed-solomon-ip-core.html)[reed-solomon-ip-core.html.](https://www.intel.com/content/www/us/en/docs/programmable/683120/17-1/about-the-high-speed-reed-solomon-ip-core.html)

    - <span id="page-10-4"></span>[4] 2018. Microblaze Processor Reference Guide.
    [https://docs.xilinx.com/v/u/2018.2-](https://docs.xilinx.com/v/u/2018.2-English/ug984-vivado-microblaze-ref)
    [English/ug984-vivado-microblaze-ref.](https://docs.xilinx.com/v/u/2018.2-English/ug984-vivado-microblaze-ref)

    - <span id="page-10-15"></span>[5] 2020. HB0919 Handbook CoreVectorBlox. [https://www.microsemi.com/existing](https://www.microsemi.com/existing-parts/parts/152678)[parts/parts/152678.](https://www.microsemi.com/existing-parts/parts/152678)

    - <span id="page-10-19"></span>[6] 2021. Block-by-Block Configurable Fast Fourier
    Transform Implementation on AI Engine (XAPP1356). [https://docs.xilinx.com/r/en-US/xapp1356-fft-ai-engine/FFT](https://docs.xilinx.com/r/en-US/xapp1356-fft-ai-engine/FFT-on-Multiple-AI-Engines)[on-Multiple-AI-Engines.](https://docs.xilinx.com/r/en-US/xapp1356-fft-ai-engine/FFT-on-Multiple-AI-Engines)

    - <span id="page-10-23"></span>[7] 2021. Intel Agilex7 FPGAs and SoCs F-Series:
    Product Table. [https://www.intel.](https://www.intel.com/content/dam/www/programmable/us/en/pdfs/literature/pt/intel-agilex-f-series-product-table.pdf)
    [com/content/dam/www/programmable/us/en/pdfs/literature/pt/intel-agilex-f](https://www.intel.com/content/dam/www/programmable/us/en/pdfs/literature/pt/intel-agilex-f-series-product-table.pdf)[series-product-table.pdf.](https://www.intel.com/content/dam/www/programmable/us/en/pdfs/literature/pt/intel-agilex-f-series-product-table.pdf)

    - <span id="page-10-1"></span>[8] 2022. Fast Fourier Transform v9.1. [https://www.xilinx.com/content/dam/xilinx/](https://www.xilinx.com/content/dam/xilinx/support/documents/ip_documentation/xfft/v9_1/pg109-xfft.pdf)
    [support/documents/ip\\_documentation/xfft/v9\\_1/pg109-xfft.pdf.](https://www.xilinx.com/content/dam/xilinx/support/documents/ip_documentation/xfft/v9_1/pg109-xfft.pdf)

    - <span id="page-10-3"></span>[9] 2022. Nios V Processor Reference Manual. [https://www.intel.com/content/www/](https://www.intel.com/content/www/us/en/products/details/fpga/nios-processor/v.html)
    [us/en/products/details/fpga/nios-processor/v.html.](https://www.intel.com/content/www/us/en/products/details/fpga/nios-processor/v.html)

    - <span id="page-10-27"></span>[10] 2023. Floating-Point IP Cores User Guide.
    [https://www.intel.com/content/www/](https://www.intel.com/content/www/us/en/docs/programmable/683750/23-1/about-floating-point-ip-cores.html)
    [us/en/docs/programmable/683750/23-1/about-floating-point-ip-cores.html.](https://www.intel.com/content/www/us/en/docs/programmable/683750/23-1/about-floating-point-ip-cores.html)

    - <span id="page-10-24"></span>[11] 2023. Intel Agilex 7 Variable Precision DSP
    Blocks. [https://www.intel.com/](https://www.intel.com/content/www/us/en/docs/programmable/683037/23-3/variable-precision-dsp-blocks-overview.html)
    [content/www/us/en/docs/programmable/683037/23-3/variable-precision-dsp](https://www.intel.com/content/www/us/en/docs/programmable/683037/23-3/variable-precision-dsp-blocks-overview.html)[blocks-overview.html.](https://www.intel.com/content/www/us/en/docs/programmable/683037/23-3/variable-precision-dsp-blocks-overview.html)

    - <span id="page-10-22"></span>[12] 2023. Intel Agilex7 Embedded Memory User Guide.
    [https://www.intel.com/](https://www.intel.com/content/www/us/en/docs/programmable/683241/23-2/embedded-memory-overview.html)
    [content/www/us/en/docs/programmable/683241/23-2/embedded-memory](https://www.intel.com/content/www/us/en/docs/programmable/683241/23-2/embedded-memory-overview.html)[overview.html.](https://www.intel.com/content/www/us/en/docs/programmable/683241/23-2/embedded-memory-overview.html)

    - <span id="page-10-18"></span>[13] 2023. Versal Adaptive SoC AI Engine Architecture
    Manual (AM009). [https:](https://docs.xilinx.com/v/u/en-US/wp506-ai-engine) [//docs.xilinx.com/v/u/en-US/wp506-ai-engine.](https://docs.xilinx.com/v/u/en-US/wp506-ai-engine)

    - <span id="page-10-5"></span>[14] Abdullah Al-Dujaili, Florian Deragisch, Andrei
    Hagiescu, and Weng-Fai Wong. 2012. Guppy: A GPU-like soft-core processor. In 2012
    International Conference on Field-Programmable Technology. 57–60.<https://doi.org/10.1109/FPT.2012.6412112>

    - <span id="page-10-6"></span>[15] Muhammed Al Kadi, Benedikt Janssen, and Michael
    Huebner. 2016. FGPU: An SIMT-Architecture for FPGAs. In Proceedings of the 2016
    ACM/SIGDA International Symposium on Field-Programmable Gate Arrays (Monterey,
    California, USA) (FPGA ''16). Association for Computing Machinery, New York, NY,
    USA, 254–263.<https://doi.org/10.1145/2847263.2847273>

    - <span id="page-10-25"></span>[16] Kevin Andryc. 2018. An Architecture Evaluation
    and Implementaiton of a Soft GPGPU for FPGAs. (2018).<https://doi.org/10.7275/12722172>

    - <span id="page-10-7"></span>[17] Kevin Andryc, Murtaza Merchant, and Russell
    Tessier. 2013. FlexGrip: A soft GPGPU for FPGAs. In 2013 International Conference
    on Field-Programmable Technology (FPT). 230–237.<https://doi.org/10.1109/FPT.2013.6718358>

    - <span id="page-10-8"></span>[18] Raghuraman Balasubramanian, Vinay Gangadhar,
    Ziliang Guo, Chen-Han Ho, Cherin Joseph, Jaikrishnan Menon, Mario Paulo Drumond,
    Robin Paul, Sharath Prasad, Pradip Valathol, and Karthikeyan Sankaralingam. 2015.
    Enabling GPGPU Low-Level Hardware Explorations with MIAOW: An Open-Source RTL
    Implementation of a GPGPU. ACM Trans. Archit. Code Optim. 12, 2, Article 21 (jun
    2015), 25 pages.<https://doi.org/10.1145/2764908>

    - <span id="page-10-26"></span>[19] K. E. Batcher. 1968. Sorting Networks and
    Their Applications. In Proceedings of the April 30–May 2, 1968, Spring Joint Computer
    Conference (Atlantic City, New Jersey) (AFIPS ''68 (Spring)). Association for
    Computing Machinery, New York,


    NY, USA, 307–314.<https://doi.org/10.1145/1468075.1468121>


    - <span id="page-10-16"></span>[20] Hui Yan Cheah, Fredrik Brosser, Suhaib A.
    Fahmy, and Douglas L. Maskell. 2014. The IDEA DSP Block-Based Soft Processor for
    FPGAs. ACM Trans. Reconfigurable Technol. Syst. 7, 3, Article 19 (sep 2014), 23
    pages.<https://doi.org/10.1145/2629443>

    - <span id="page-10-17"></span>[21] Hui Yan Cheah, Suhaib A. Fahmy, and Nachiket
    Kapre. 2014. Analysis and optimization of a deeply pipelined FPGA soft processor.
    In 2014 International Conference on Field-Programmable Technology (FPT). 235–238.
    [https://doi.org/10.](https://doi.org/10.1109/FPT.2014.7082783) [1109/FPT.2014.7082783](https://doi.org/10.1109/FPT.2014.7082783)

    - <span id="page-10-12"></span>[22] Christopher Han-Yu Chou, Aaron Severance,
    Alex D. Brant, Zhiduo Liu, Saurabh Sant, and Guy G. Lemieux. 2011. VEGAS: soft
    vector processor with scratchpad memory. In Proceedings of the ACM/SIGDA 19th
    International Symposium on Field Programmable Gate Arrays, FPGA 2011, Monterey,
    California, USA, February 27, March 1, 2011, John Wawrzynek and Katherine Compton
    (Eds.). ACM, 15–24. <https://doi.org/10.1145/1950413.1950420>

    - <span id="page-10-21"></span>[23] Jeffrey Chromczak, Mark Wheeler, Charles Chiasson,
    Dana How, Martin Langhammer, Tim Vanderhoek, Grace Zgheib, and Ilya Ganusov. 2020.
    Architectural Enhancements in Intel® Agilex™ FPGAs. In FPGA ''20: The 2020 ACM/SIGDA
    International Symposium on Field-Programmable Gate Arrays, Seaside, CA, USA, February
    23-25, 2020, Stephen Neuendorffer and Lesley Shannon (Eds.). ACM, 140–149.<https://doi.org/10.1145/3373087.3375308>

    - <span id="page-10-9"></span>[24] Pedro Duarte, Pedro Tomas, and Gabriel Falcao.
    2017. SCRATCH: An End-to-End Application-Aware Soft-GPGPU Architecture and Trimming
    Tool. In Proceedings of the 50th Annual IEEE/ACM International Symposium on Microarchitecture
    (Cambridge, Massachusetts) (MICRO-50 ''17). Association for Computing Machinery,
    New York, NY, USA, 165–177.<https://doi.org/10.1145/3123939.3123953>

    - <span id="page-10-10"></span>[25] Jeffrey Kingyens and J. Gregory Steffan. 2010.
    A GPU-inspired soft processor for high-throughput acceleration. In 2010 IEEE International
    Symposium on Parallel and Distributed Processing, Workshops and Phd Forum (IPDPSW).
    1–8. [https:](https://doi.org/10.1109/IPDPSW.2010.5470679) [//doi.org/10.1109/IPDPSW.2010.5470679](https://doi.org/10.1109/IPDPSW.2010.5470679)

    - <span id="page-10-28"></span>[26] Ian Kuon and Jonathan Rose. 2006. Measuring
    the gap between FPGAs and ASICs. In Proceedings of the ACM/SIGDA 14th International
    Symposium on Field Programmable Gate Arrays, FPGA 2006, Monterey, California,
    USA, February 22- 24, 2006, Steven J. E. Wilton and André DeHon (Eds.). ACM, 21–30.
    [https:](https://doi.org/10.1145/1117201.1117205) [//doi.org/10.1145/1117201.1117205](https://doi.org/10.1145/1117201.1117205)

    - <span id="page-10-29"></span>[27] Martin Langhammer and Gregg Baeckler. 2018.
    High Density and Performance Multiplication for FPGA. In 25th IEEE Symposium on
    Computer Arithmetic, ARITH 2018, Amherst, MA, USA, June 25-27, 2018. IEEE, 5–12.
    [https://doi.org/10.1109/](https://doi.org/10.1109/ARITH.2018.8464695) [ARITH.2018.8464695](https://doi.org/10.1109/ARITH.2018.8464695)

    - <span id="page-10-20"></span>[28] Martin Langhammer and George A. Constantinides.
    2023. eGPU: A 750 MHz Class Soft GPGPU for FPGA. In 2023 33rd International Conference
    on Field-Programmable Logic and Applications (FPL). 277–282. [https://doi.org/10.1109/](https://doi.org/10.1109/FPL60245.2023.00047)
    [FPL60245.2023.00047](https://doi.org/10.1109/FPL60245.2023.00047)

    - <span id="page-10-11"></span>[29] Rui Ma, Jia-Ching Hsu, Tian Tan, Eriko Nurvitadhi,
    Rajesh Vivekanandham, Aravind Dasu, Martin Langhammer, and Derek Chiou. 2021.
    DO-GPU: Domain Optimizable Soft GPUs. In 2021 31st International Conference on
    Field-Programmable Logic and Applications (FPL). 140–144. [https://doi.org/10.1109/FPL53798.2021.](https://doi.org/10.1109/FPL53798.2021.00031)
    [00031](https://doi.org/10.1109/FPL53798.2021.00031)

    - <span id="page-10-13"></span>[30] Aaron Severance and Guy Lemieux. 2012. VENICE:
    A compact vector processor for FPGA applications. In 2012 International Conference
    on Field-Programmable Technology, FPT 2012, Seoul, Korea (South), December 10-12,
    2012. IEEE, 261–268. <https://doi.org/10.1109/FPT.2012.6412146>

    - <span id="page-10-14"></span>[31] Aaron Severance and Guy G. F. Lemieux. 2013.
    Embedded supercomputing in FPGAs with the VectorBlox MXP Matrix Processor. In
    Proceedings of the International Conference on Hardware/Software Codesign and
    System Synthesis, CODES+ISSS 2013, Montreal, QC, Canada, September 29 - October
    4, 2013. IEEE, 6:1–6:10.<https://doi.org/10.1109/CODES-ISSS.2013.6658993>'
  decisions:
    evaluation_prompt: 'Qualified. Reason: The paper includes benchmarking and a table
      comparing resource usage and performance metrics, indicating experimental and
      quantitative analysis.'
    related_work_prompt: 'Qualified. Reason: The paper cites several research papers,
      explains and compares to cited work, and includes a section that engages with
      prior research.'
    novelty_prompt: 'Qualified. Reason: The paper introduces a novel parameterized
      SIMT processor for FPGA, demonstrates consistent timing closure, and compares
      its performance to existing processors, indicating novelty.'
    review_only_prompt: 'Qualified. Reason: The paper introduces new contributions
      by describing a novel parameterized SIMT processor for FPGA and demonstrating
      its performance, rather than summarizing existing work.'
- title: "A Composable Dynamic Sparse Dataflow Architecture for Efficient\n  Event-based\
    \ Vision Processing on FPGA"
  abstract: 'Event-based vision represents a paradigm shift in how vision information
    is

    captured and processed. By only responding to dynamic intensity changes in the

    scene, event-based sensing produces far less data than conventional frame-based

    cameras, promising to springboard a new generation of high-speed, low-power

    machines for edge intelligence. However, processing such dynamically sparse

    input originated from event cameras efficiently in real time, particularly with

    complex deep neural networks (DNN), remains a formidable challenge. Existing

    solutions that employ GPUs and other frame-based DNN accelerators often

    struggle to efficiently process the dynamically sparse event data, missing the

    opportunities to improve processing efficiency with sparse data. To address

    this, we propose ESDA, a composable dynamic sparse dataflow architecture that

    allows customized DNN accelerators to be constructed rapidly on FPGAs for

    event-based vision tasks. ESDA is a modular system that is composed of a set of

    parametrizable modules for each network layer type. These modules share a

    uniform sparse token-feature interface and can be connected easily to compose

    an all-on-chip dataflow accelerator on FPGA for each network model. To fully

    exploit the intrinsic sparsity in event data, ESDA incorporates the use of

    submanifold sparse convolutions that largely enhance the activation sparsity

    throughout the layers while simplifying hardware implementation. Finally, a

    network architecture and hardware implementation co-optimizing framework that

    allows tradeoffs between accuracy and performance is also presented.

    Experimental results demonstrate that when compared with existing GPU and

    hardware-accelerated solutions, ESDA achieves substantial speedup and

    improvement in energy efficiency across different applications, and it allows

    much wider design space for real-world deployments.'
  url: http://arxiv.org/abs/2401.05626v1
  keywords: ''
  document: '# A Composable Dynamic Sparse Dataflow Architecture for Efficient Event-based
    Vision Processing on FPGA


    [Yizhao Gao](https://orcid.org/0000-0001-5673-3746) yzgao@eee.hku.hk University
    of Hong Kong Hong Kong


    [Yuhao Ding](https://orcid.org/0009-0009-4819-9251) yhding@eee.hku.hk University
    of Hong Kong Hong Kong


    ### ABSTRACT


    Event-based vision represents a paradigm shift in how vision information is captured
    and processed. By only responding to dynamic intensity changes in the scene, event-based
    sensing produces far less data than conventional frame-based cameras, promising
    to springboard a new generation of high-speed, low-power machines for edge intelligence.
    However, processing such dynamically sparse input originated from event cameras
    efficiently in real time, particularly with complex deep neural networks (DNN),
    remains a formidable challenge. Existing solutions that employ GPUs and other
    frame-based DNN accelerators often struggle to efficiently process the dynamically
    sparse event data, missing the opportunities to improve processing efficiency
    with sparse data. To address this, we propose ESDA, a composable dynamic sparse
    dataflow architecture that allows customized DNN accelerators to be constructed
    rapidly on FPGAs for event-based vision tasks. ESDA is a modular system that is
    composed of a set of parametrizable modules for each network layer type. These
    modules share a uniform sparse token-feature interface and can be connected easily
    to compose an all-on-chip dataflow accelerator on FPGA for each network model.
    To fully exploit the intrinsic sparsity in event data, ESDA incorporates the use
    of submanifold sparse convolutions that largely enhance the activation sparsity
    throughout the layers while simplifying hardware implementation. Finally, a network
    architecture and hardware implementation co-optimizing framework that allows tradeoffs
    between accuracy and performance is also presented. Experimental results demonstrate
    that when compared with existing GPU and hardware-accelerated solutions, ESDA
    achieves substantial speedup and improvement in energy efficiency across different
    applications, and it allows much wider design space for real-world deployments.


    FPGA ''24, March 3–5, 2024, Monterey, CA, USA


    © 2024 Copyright held by the owner/author(s).


    ACM ISBN 979-8-4007-0418-5/24/03.


    <https://doi.org/10.1145/3626202.3637558>


    [Baoheng Zhang](https://orcid.org/0000-0003-1685-656X) bhzhang@eee.hku.hk University
    of Hong Kong Hong Kong


    [Hayden Kwok-Hay So](https://orcid.org/0000-0002-6514-0237) hso@eee.hku.hk University
    of Hong Kong Hong Kong


    ### CCS CONCEPTS


    • Computer systems organization → Reconfigurable computing; Neural networks; Data
    flow architectures; • Computing methodologies → Object recognition.


    ### KEYWORDS


    event-based vision, event camera, sparse DNN accelerator, submanifold sparse convolution,
    dataflow accelerator, FPGA


    #### ACM Reference Format:


    Yizhao Gao, Baoheng Zhang, Yuhao Ding, and Hayden Kwok-Hay So. 2024. A Composable
    Dynamic Sparse Dataflow Architecture for Efficient Eventbased Vision Processing
    on FPGA. In Proceedings of the 2024 ACM/SIGDA International Symposium on Field
    Programmable Gate Arrays (FPGA ''24), March 3–5, 2024, Monterey, CA, USA. ACM,
    New York, NY, USA, [12](#page-11-0) pages. <https://doi.org/10.1145/3626202.3637558>


    ### 1 INTRODUCTION


    Event-based vision systems are bio-inspired systems that represent a paradigm
    shift from conventional frame-based vision systems [\[16\]](#page-10-0). Instead
    of capturing the entire field of view as dense matrices of pixel values like in
    a conventional camera, an event camera only produces a spiking event when a substantial
    change in light intensity local to each pixel is detected. Despite the fundamentally
    different way of sensing, substantial progress has been made in recent years to
    perform a wide range of vision tasks such as object classification, object detection,
    and tracking utilizing deep neural network models [\[49\]](#page-11-1).


    However, deploying event-based vision systems with deep learning models in near-sensor
    or edge-computing contexts remains a great challenge. On the one hand, off-the-shelf
    GPU platforms and dense DNN accelerators that are optimized to operate on dense
    input from conventional cameras can rarely exploit the sparsity in event-based
    inputs. As a result, if blindly employed to accelerate event camera output, these
    systems struggle to deliver the promised energy efficiency and performance benefits
    due to a large amount of wasted computation. On the other hand, some sparse DNN
    accelerators have previously been proposed to address the need for event vision
    processing [\[1,](#page-10-1) [31\]](#page-10-2). Yet, these designs primarily
    concentrate on optimizing sparse computation at the matrix multiplication or single
    layer level, often through a reusable processing engine for various layers. Such
    an approach may lead to considerable latency and energy overhead, mainly due to
    the recurrent


    Permission to make digital or hard copies of part or all of this work for personal
    or classroom use is granted without fee provided that copies are not made or distributed
    for profit or commercial advantage and that copies bear this notice and the full
    citation on the first page. Copyrights for third-party components of this work
    must be honored. For all other uses, contact the owner/author(s).


    input/output operations involving weights and intermediate activations. Consequently,
    these designs are suboptimal for real-time fast-reacting systems intended to exploit
    the low latency sensing and high temporal resolution benefits of event cameras.


    To address these challenges, we introduce ESDA, a system based on FPGA that can
    efficiently accelerate Event-based vision processing with a composable dynamic
    Sparse Dataflow Architecture. ESDA is a modular system that allows parametrizable
    modules for each network layer to be connected together to form a customized all-on-chip
    system on FPGA. Employing the all-on-chip dataflow architecture is a key design
    choice towards low latency and power consumption by minimizing the off-chip communication
    of weight and activation. In contrast to prior dataflow architectures that handle
    dense frame-based input, ESDA exclusively streams the spatially sparse features
    from the event-based input across the different dataflow modules by using a unified
    sparse token-feature interface. Through the straightforward cascading of dataflow
    modules corresponding with the model definition, a sparse accelerator can be rapidly
    constructed to speed up the application. Furthermore, to take full advantage of
    the sparse dataflow approach, ESDA also integrates submanifold sparse convolution
    [\[23\]](#page-10-3) at the algorithmic level. Submanifold convolution can effectively
    maintain high spatial sparsity of feature activations of the models by constraining
    the spatial activation locations, which also helps to derive simpler hardware
    implementation.


    To facilitate design space exploration, we also develop a network model and hardware
    architecture co-optimization framework that leverages the sparsity statistics
    from a given event-based vision task or dataset to perform accuracy and hardware
    efficiency tradeoffs. Given the capacity of today''s FPGA edge device, the search
    framework can identify many efficient models that fit on-chip while achieving
    good performance and accuracy. To this end, the main contribution of this work
    can be summarized as:


    - We proposed a novel composable sparse dataflow architecture for DNN inference
    in event-based vision applications that effectively harness the inherent sparsity
    in event camera data.

    - We implemented ESDA on embedded FPGA platforms and established a set of sparsity-aware
    co-optimization flow, enabling the customization of DNN accelerators for each
    specific application or dataset.

    - We showcased a substantial end-to-end hardware speedup and energy-efficiency
    improvement through extensive evaluation on multiple event-based datasets.


    In the next section, background and related work about neuromorphic vision systems
    and processing will first be provided. The detailed design of ESDA will be discussed
    in Section [3.](#page-2-0) Experiment results are shown in Section [4](#page-6-0)
    and we will conclude in Section [5.](#page-9-0) The artifact associated with this
    work is open-sourced and available at [\[17\]](#page-10-4).


    ### 2 BACKGROUND AND RELATED WORK


    ### 2.1 Event-based Vision


    Event cameras, also known as Neuromorphic Cameras or Dynamic Vision Sensors, are
    designed to mimic the functioning of the human retina. Instead of capturing full
    frames, they detect and report light intensity changes as events asynchronously
    with high temporal resolution. Events are usually encoded in Address Event Representation
    (AER) format. Typically each event can be written as [, , , ] where , points to
    the location of the event, is the timestamp and is the polarity of changes. Commercial
    event cameras nowadays can support 720 × 1280 spatial resolution with 1 µs temporal
    resolution [\[14\]](#page-10-5). Compared with traditional frame-based cameras,
    event cameras have several advantages, including: (1) high temporal resolution
    allowing capturing fast events that conventional cameras may miss; (2) high dynamic
    range enabling them to capture scenes with varying lighting conditions; (3) high
    efficiency in terms of data processing and transmission. As an emerging technology,
    event-based vision has shown great potential in areas like robotics, autonomous
    vehicles, high-speed tracking, scientific imaging, low-power embedded systems,
    etc [\[16\]](#page-10-0).


    Figure [1](#page-2-1) shows a sample from event-based dataset DvsGesture [\[2\]](#page-10-6)
    to illustrate the working principle of the event camera. In this sample, a man
    is rotating his arm counter-clockwise. As the event camera exclusively responds
    to changes in scene intensity, it captures only the movements of the person as
    a sequence of events. The upper figure shows the scatter plots of events in 3D
    spatial-temporal space. The below four images show the 2D feature representations
    constructed from events in a short time interval. It''s a common practice to use
    these 2D representations to serve as the DNN input. Prior research works have
    introduced many different 2D representation algorithms like event histogram [\[36\]](#page-11-2),
    time-surface [\[26,](#page-10-7) [44\]](#page-11-3), learnable MLP kernels [\[21\]](#page-10-8),
    and etc. It''s worth noting that ESDA can seamlessly integrate with different
    2D representation algorithms. As long as the resulting feature is spatially sparse,
    it can benefit from the sparse architecture of ESDA.


    # 2.2 Hardware Acceleration of Event-based Vision


    From a system perspective, fully unlocking the potential of the event-based approaches
    cannot be achieved without considering efficient hardware processing. Previous
    efforts have mainly been devoted to two directions. On the one hand, due to the
    similar neuromorphic working principle, many research works have integrated event
    cameras with bio-inspired neuromorphic computing devices. Typically, these works
    seek to accelerate Spiking Neural Networks (SNNs) on customized hardware such
    as TruthNorth [\[38\]](#page-11-4), Loihi [\[10\]](#page-10-9), SpiNNaker [\[15\]](#page-10-10),
    and etc. However, training SNN for some complex event-based vision tasks itself
    can also be challenging. Moreover, despite deployment on neuromorphic hardware
    like TrueNorth, the latency for network inference remains substantial, e.g. 105
    ms on DVS Gesture Recognition.


    On the other hand, some domain-specific architectures for eventbased vision tasks
    have been proposed, such as optical flow calculation [\[3,](#page-10-11) [32,](#page-10-12)
    [33\]](#page-10-13), object tracking [\[18,](#page-10-14) [19,](#page-10-15) [29,](#page-10-16)
    [30\]](#page-10-17), feature recognition [\[25,](#page-10-18) [34\]](#page-11-5),
    etc. In recent years, the event-based vision has seen significant advancements
    through integrating deep neural networks (DNNs) in its processing pipeline [\[49\]](#page-11-1).
    However, accelerators originally designed for frame-based DNN models struggle
    to efficiently process event-based inputs with the required efficiency. Many sparse
    DNN accelerators have primarily emphasized element-wise weight sparsity or activation
    sparsity induced by the


    <span id="page-2-1"></span>![](_page_2_Figure_2.jpeg)


    Events in 3D spatial-temporal space


    Figure 1: Working principle of event camera. The event camera only captures the
    light intensity change as spiking events in AER format ([x, y, ±1, timestamp]).
    This figure shows a recording sample from DvsGesture dataset where a man is rotating
    his left arm counter-clockwise. Due to the dynamic nature of event cameras, only
    the motions of the man are captured. For vision tasks like object recognition,
    a certain amount of events are usually grouped to form a 2D representation as
    DNN input.


    ReLU [\[7,](#page-10-19) [22,](#page-10-20) [42,](#page-11-6) [50\]](#page-11-7).
    These approaches differ significantly from the intrinsic spatial sparsity in event
    data. While some accelerators can leverage spatial sparsity, they often target
    at point cloud [\[13,](#page-10-21) [28\]](#page-10-22) or dynamic spatial pruning
    networks [\[9,](#page-10-23) [20\]](#page-10-24), which are also different in
    terms of the source and pattern of sparsity.


    Thus, there is a growing need for specialized accelerators to effectively leverage
    the unique spatial sparsity characteristics of eventbased data to unlock its advantages
    in real-time energy-efficient DNN inference. Prior work like NullHop [\[1,](#page-10-1)
    [31\]](#page-10-2) has proposed an architecture that uses a binary bitmap to represent
    layer-wise sparse activation to skip the computation of zeros. Other solutions
    like PFF [\[40\]](#page-11-8) have also explored treating the event-based input
    sparsity as a bit-level sparsity by using Binary Neural Networks. In this work,
    we aim to achieve low-latency and energy-efficient processing of event-based DNN
    models by proposing an all-on-chip sparse dataflow architecture on FPGA. The framework
    is also highly composable, allowing the customization of accelerators for different
    event-based models and datasets.


    ### <span id="page-2-0"></span>3 METHODOLOGY


    ### 3.1 Design Principle of Sparse Dataflow


    ESDA is built upon distinct sparse dataflow modules that share a unified token-feature
    interface. The framework enables seamless customization of a sparse accelerator
    by cascading various modules aligned with the model''s architecture. By definition,
    ESDA only streams spatially sparse features along with their coordinate tokens
    through the dataflow modules. Eqn. [1](#page-2-2) shows the design principle for
    an ESDA dataflow module using a unified sparse token-feature interface:


    <span id="page-2-2"></span>

    $$\begin{aligned} \text{token\_out}\left[i\right] &= f\_{\overline{T}}(\text{token\_in}\left[j\right],
    \dots, \text{token\_in}\left[j-N\right]) \\ \text{feature}\_{\text{out}}\left[i\right]
    &= f\_{\overline{F}}(\text{feature}\_{\text{in}}\left[j\right], \dots, \text{feature}\_{\text{in}}\left[j-N\right])
    \\ \text{s.t. } & \quad \text{rawel}(\text{token\_out}\left[i\right]) > \text{rawel}(\text{token\_out}\left[i-1\right])
    \end{aligned} \quad (1)$$


    where each [] (written in [., ., .]) marks the spatial location of the current
    feature vector []. The flag . denotes whether it''s the end of the stream. The
    order, calculated as


    <span id="page-2-3"></span>![](_page_2_Figure_12.jpeg)


    Figure 2: Overall architecture of an ESDA accelerator.


    ( ∗ ℎ + ), refers to the left-to-right, top-to-bottom streaming order of spatial
    locations, which also corresponds to the 1D memory layout order of a 2D dense
    matrix. In other words, the module itself should maintain the correct output order
    so that different modules can be pipelined.


    The functionality of each dataflow module is separated into: (1) to resolve the
    new output token based on previous input token information; (2) to compute the
    output features at new output token locations by using some temporal neighboring
    features. Depending on the algorithm requirement, different numbers of temporal
    neighboring data might be needed, specified as in Eqn. [1.](#page-2-2) Each module
    in ESDA is designed as a flexible hardware template, where many hardware design
    parameters like bitwidth, parallel factor, buffer size, etc., can be specified
    at compile time.


    Figure [2](#page-2-3) shows an illustrative accelerator diagram for the eventbased
    object classification task. The processing system, or CPU, is responsible for
    event streaming and 2D representation construction. While the accelerator on the
    programmable logic reads the sparse input features following the left-to-right,
    top-to-bottom ordering to align with our dataflow requirement. A binary bitmap
    is used to mark the non-zero locations and generate the token stream. The tokenized
    sparse feature then propagates through each convolution block. Finally, the accelerator
    returns the classification outputs back to the processing system.


    <span id="page-3-1"></span>![](_page_3_Figure_1.jpeg)


    ![](_page_3_Figure_2.jpeg)


    (a) Standard vs. Submanifold Sparse Convolution when stride is 1


    (b) Standard vs. Submanifold Sparse Convolution when stride is 2


    Figure 3: Compare standard convolution with submanifold sparse convolution. The
    gray/green locations in the figure mean non-zero pixels. (a) When stride = 1,
    the input and output location of submanifold convolution is restricted to be identical.
    In the "x" location, standard convolution leads to a valid non-zero output while
    submanifold convolution does not. (b) When stride > 1 (2 in the figure), an output
    location is non-zero if the corresponding input × grid contains non-zeros.


    In the following section, we will first discuss how ESDA incorporates submanifold
    sparse convolution, which is used to enhance the model''s sparsity while simplifying
    the dataflow design ( , and in Eqn. [1\)](#page-2-2). In Section [3.3,](#page-3-0)
    we will show the detailed microarchitecture of some commonly used dataflow modules.


    # 3.2 Sparsify the Algorithm: Submanifold Sparse Convolution


    The input of a DNN model for event-based vision tasks is typically a spatially
    sparse 2D feature/representation generated by a certain amount of events. Although
    input features can be highly sparse (e.g. <%1 in ASL-DVS dataset), using standard
    convolution can easily leads to much denser intermediate features because of the
    "dilation" effect of convolution. Figure [3](#page-3-1) shows the example of sparse
    input features convoluted with a 3 × 3 kernel. The output feature of standard
    convolution is much denser than the input feature, which can diminish the opportunities
    brought by the sparsity.


    To address this, ESDA incorporates submanifold sparse convolution [\[23\]](#page-10-3)
    to better enhance the spatial sparsity of a model. Figure [3](#page-3-1) illustrates
    the operation of submanifold sparse convolution. When the convolution stride is
    1, the submanifold sparse convolution


    enforces input and output non-zero locations to be identical. This means that
    for positions that are originally zeros, e.g. "x" in the Figure [3.](#page-3-1)a,
    the output remains zeros. While for those non-zeros output locations, e.g. "y"
    in the figure, the feature is calculated by using the similar weighted sum algorithm
    as standard convolution. In this way, submanifold convolution prevents the "dilation"
    effect of standard convolution by restricting the possible non-zero output locations.


    When convolution stride is larger than 1, an output location of sparse convolution
    is non-zero if the corresponding input × stride grid contains any non-zeros. For
    instance, the "x" in Figure [3.](#page-3-1)b remains zero because its corresponding
    2×2 grid of the inputs are all zeros. This is in contrast to standard convolution
    which generates non-zero outputs whenever the convolution kernel meets any nonzeros.
    Thus, sparser output features can be expected by using sparse convolution if the
    stride size is smaller than the kernel size, which is a common configuration.


    Submanifold sparse convolution also comes with a simpler hardware implementation.
    For example, in the stride-1 case where the input-output sparse locations are
    the same, the input tokens can simply be queued in a FIFO and reused for the output.
    Consequently, the abstract module function can be written as:


    $$\begin{aligned} \text{token}\_{\text{out}}[i] &= \text{token}\_{\text{in}}[i]
    \\ \text{feature}\_{\text{out}}[i] &= f\_F(\text{feature}\_{\text{in}}[i], \dots,
    \text{feature}\_{\text{in}}[i-N]) \end{aligned} \tag{2}$$


    In this way, the hardware can co-iterate input/output tokens and features, while
    saving the additional logic to generate potential output locations during runtime.


    # <span id="page-3-0"></span>3.3 Sparsify the Architecture: Composable Sparse
    Dataflow Modules


    <span id="page-3-2"></span>![](_page_3_Figure_16.jpeg)


    Figure 4: Convolution 1 × 1 Module.


    3.3.1 1 × 1 Convolution Module. 1 × 1 convolution, also known as point-wise convolution,
    only operates locally by iterating each sparse location and updating its features.
    Benefiting from this property, the input and output tokens can simply be relayed
    using a token register. Figure [4](#page-3-2) shows the hardware diagram of the
    1 × 1 convolution module. The feature vector at the current location [., .] is
    first read into an input feature buffer and then multiplied with the weight matrix
    in the static buffer. Since the convolution layers are spatially mapped on the
    accelerator, the weights will be synthesized into Read-Only Memory using Block
    RAM (BRAM) and initialized during bitstream download. The computation of the output
    feature vector can be implemented as a Matrix-Vector multiplication unit as illustrated
    in Figure [4.](#page-3-2) The size of the PE array, which can also be regarded
    as the parallel factor (PF) on channel


    dimension, is exposed as a hardware template parameter. These parallel factors
    can also affect the BRAM usage by partitioning the weights buffer to match the
    bandwidth requirement for the MAC array. Note that the batch normalization layer
    can be folded into the convolution layer, and the activation function is omitted
    from the diagram for simplicity.


    <span id="page-4-0"></span>![](_page_4_Figure_3.jpeg)


    Figure 5: Convolution 3 × 3 Module.


    3.3.2 × Convolution Module ( > 1). Different from 1 × 1 convolution that directly
    operates on non-zero positions locally, when the convolution kernel size is larger
    than 1, the computation is the weighted summation of neighboring features. In
    ESDA, × convolution is implemented by cascading a Sparse Line Buffer module and
    a × convolution computation module as shown in Figure [5.](#page-4-0) The Sparse
    Line Buffer designs can be slightly different for different convolution strides
    (usually 1 or 2), while the computation module remains the same. As shown in Figure
    [3,](#page-3-1) the weighted sum of spatial kernels can also be sparse (e.g.,
    "y" only requires ∗ 4 + ∗ 7). To leverage this type of sparsity, an additional
    kernel offset stream is used to denote the sparse neighboring offset within the
    × kernel.


    <span id="page-4-1"></span>![](_page_4_Figure_6.jpeg)


    Figure 6: Depthwise Convolution 3 × 3 Computation Module.


    3.3.3 × Convolution Computation Module ( > 1). Figure [6](#page-4-1) shows the
    diagram of the 3 × 3 depthwise convolution module, which is widely used in many
    mobile networks [\[43,](#page-11-9) [45,](#page-11-10) [46\]](#page-11-11). For
    each non-zero output location marked by the current token register, the weighted
    sum of sparse neighboring features is controlled by the additional kernel offset
    stream. The kernel offset marks the relative spatial locations of the current
    input features in the computation window, which also serves as the index of the
    weights buffer. After performing the weighted sum for all non-zero kernel offsets,
    the final results will be streamed out as the output features. Other types of
    convolution, such as full convolution, only differ slightly in the PE organization,
    while having identical token interfaces and sparse weighted sum operations. Like
    the 1 × 1 convolution module, the hardware template also incorporates the channel
    dimension parallel factor as a configurable variable.


    <span id="page-4-2"></span>![](_page_4_Figure_9.jpeg)


    Figure 7: Sparse Line Buffer Module (stride=1).


    3.3.4 Sparse Line Buffer with Stride Equal to 1. The Sparse Line Buffer (SLB)
    mainly comprises a rows data buffer ( being the kernel size), a token FIFO, and
    a small bitmap to record current nonzero spatial locations in the buffer. When
    stride = 1, submanifold convolution enforces the input and output non-zero locations
    to be identical. As a result, a simple token FIFO is used to set up a queue for
    the output tokens. Due to the deterministic streaming order of the sparse sliding
    window algorithm (left-to-right and top-to-bottom as discussed in Eqn. [1\)](#page-2-2),
    we can simply use the token FIFO''s head and tail tokens to control the read and
    write operations of SLB.


    Specifically, the FIFO''s head token should represent the next spatial location
    for weighted sum computation, while the FIFO''s tail token indicates the most
    recently received input location. The offset between the head and tail token can
    be used to determine whether the buffer already contains all the data required
    for the next output (center at head token). If the current head token is ℎ, the
    required input window would be [ℎ. −, ℎ. +] × [ℎ.−, ℎ.+], where = ( −1)/2 is the
    kernel radius. Since our dataflow adheres to the top-to-bottom, left-to-right
    ordering, when the tail token points to a location with ravel order larger than
    [ℎ. + , ℎ. + ], it signifies that all the required data are already in the buffer.
    The head token is valid to be sent out, and the corresponding features are streamed
    for computation.


    In terms of the ready signal, new tokens and features are only allowed to be written
    into the SLB if the input still lies in the currently buffered rows/region. When
    the new input is beyond the buffered rows, SLB will first output the buffered
    tokens before accepting new inputs. This approach is inherently free from deadlock
    as the arrival of a new token from subsequent rows indicates that tokens from
    preceding rows (≤ . − − 1) are all valid for output. This is because the ravel
    order of should be larger than [ℎ. + , ℎ. + ] when ℎ. ≤ . − − 1. Those valid tokens
    will be popped out one by one, and buffers will be cleaned up to allow new input
    data to be buffered. In summary, the control signals ready and valid can then
    be written as:


    <span id="page-4-3"></span>

    $$\begin{aligned} \sigma &= \left( (t.x - h.x \ge u) \land (t.y - h.y \ge u) \right)
    \lor (t.y - h.y \ge u + 1) \\ r &= (t.y - h.y \le u) \\ u &= (k - 1) / 2 \end{aligned}
    \tag{3}$$


    where ℎ, stands for head and tail tokens. is the radius of the kernel. refers
    to kernel size.


    As mentioned before, the kernel offset stream is used to denote each non-zero
    location within the × window. This information is obtained by querying a bitmap
    that marks whether a spatial location in the buffer is currently non-zero or not.
    Figure [7](#page-4-2) shows an example diagram of an SLB for 3 × 3 convolution
    with stride equal to 1. The snapshot shows the kernel window center at the head
    token, where the features on kernel offset 2, 4, 6 are non-zeros. In this case,
    the features at 2, 4, 6 will be outputted along with the kernel offsets for downstream
    weighted sum computation.


    <span id="page-5-1"></span>![](_page_5_Figure_2.jpeg)


    Figure 8: Sparse Line Buffer Module (stride=2).


    3.3.5 Sparse Line Buffer with Stride Equal to 2. When using the stride = 2 in
    a convolution layer, the output feature map will be downsampled. As illustrated
    in Figure [3b](#page-3-1), the sparse convolution will recalculate the non-zero
    coordinates (tokens). For each 2 × 2 input stride grid, if it contains any non-zero
    pixels, it will generate a valid non-zero output. In this case, the Sparse Line
    Buffer module with stride equal to 2 (SLB s2) adopts two token FIFOs to buffer
    tokens from even and odd rows, respectively. A Token Merge unit is positioned
    at the head of the two FIFOs to calculate the new output tokens, which can be
    written as:


    <span id="page-5-0"></span>

    $$\begin{aligned} h\_e e2 &= \{h\_e.x/2, \ h\_e.y/2, \ h\_e.e.end\} \\ h\_o o2
    &= \{h\_o.x/2, \ h\_o.y/2, \ h\_o.e.end\} \\ \text{token}\_{\text{out}} &= (ravel(h\_e2)
    > ravel(h\_o2)) \text{ ? } h\_o o2 : h\_e e2 \end{aligned} \tag{4}$$


    where the ℎ\_ and ℎ\_ are the head tokens from odd and even token FIFOs, respectively.
    When a new output token is generated, the tokens in the corresponding 2 × 2 grid
    will all be popped out from FIFOs.


    Similar to the case with stride being 1, the tail token can help determine whether
    the new output token already sees sufficient data in the buffer. The control logic
    is similar to Eqn. [3,](#page-4-3) with the head token ℎ being the tokenout in
    Eqn. [4.](#page-5-0) Figure [8](#page-5-1) demonstrated an example diagram of
    an SLB s2 module for 3 × 3 convolution, where the dark blue and green positions
    within the 2 × 2 stride grid signify a valid output. The token merge units will
    pop the dark blue and green tokens from both token FIFOs and stream out the new
    token. The SLB s2 module then outputs the corresponding features and kernel offsets
    (1, 4, 8) for downstream computation.


    In typical Deep Neural Network (DNN) model designs, the kernel stride values are
    commonly set to either 1 or 2. If using an even larger stride size ( > 2), the
    SLB can be designed similarly by splitting number of token FIFO channels.


    3.3.6 Pooling and FC Module. For the object classification task, a model usually
    uses a global pooling layer followed by a fully connected layer to generate predictions.
    In the sparse dataflow of ESDA, the global pooling layer will iterate through
    all the non-zero


    <span id="page-5-2"></span>![](_page_5_Figure_11.jpeg)


    Figure 9: Pooling and FC Module.


    tokens until it encounters a token with the . = flag. The features are aggregated
    into a single vector based on the pooling algorithm (maximum or average). Finally,
    the pooling results are fed into a linear classifier layer to compute the final
    prediction. Figure [9](#page-5-2) shows an example diagram of a global average
    pooling layer combined with a fully connected classifier.


    <span id="page-5-3"></span>![](_page_5_Figure_14.jpeg)


    Figure 10: Chaining a Residual Block.


    3.3.7 Chaining Modules into a Block. Many DNN models are built upon convolution
    blocks with residual connections [\[24\]](#page-10-25). Thanks to the autonomous
    sparse iteration capabilities of all the modules in ESDA, building convolution
    blocks by chaining the dataflow modules is straightforward. Figure [10](#page-5-3)
    shows an example of building an inverted residual block MBConv with stride being
    1. MBConv is commonly employed in efficient models like MobileNetV2 [\[43\]](#page-11-9),
    EfficientNet [\[46\]](#page-11-11), MnasNet [\[45\]](#page-11-10), etc. Although
    ESDA mainly uses MBConv blocks in the following experiments, similar approaches
    can be applied to other models like ResNet [\[24\]](#page-10-25), ShuffleNet [\[48\]](#page-11-12),
    etc. Figure [10](#page-5-3) illustrates a convolution block incorporating an identity
    shortcut, with all layer strides set to 1. Leveraging the principles of submanifold
    convolution, the input and output tokens remain identical across all the layers
    within this block. Thus, the input feature stream can simply be forked into two
    separate streams. One stream goes to the residual functional layers, while the
    other is used for the identity shortcut. The features in the shortcut branch are
    buffered in a FIFO and will be merged at the output of the last 1 × 1 convolution
    layer.


    # 3.4 Customize Accelerator with Algorithm/Hardware Co-optimization


    The highly parameterized and composable sparse dataflow modules in ESDA empower
    us to customize all-on-chip accelerators


    for different models or applications by spatially mapping distinct network components
    on FPGA. Moreover, this flexibility allows us to trade off between accuracy, resource
    consumption, and performance. However, it''s essential to recognize that the design
    space can be vast for this problem. To address this challenge, ESDA incorporates
    an algorithm/hardware co-optimization flow to automate this process.


    3.4.1 Sparsity-awared Hardware Optimization. In a dataflow accelerator, the throughput
    is usually bounded by the slowest module in the pipeline. Thus, finding a load-balanced
    hardware resource configuration for different modules is critical. To approach
    the optimal configuration, ESDA adopts a static optimization strategy that hinges
    on application-wise sparsity statistics. More precisely, for each application/dataset,
    we collect the spatial and kernel sparsity ratios ( and ) from all the samples
    in the dataset. The degree of spatial sparsity within the feature activation reflects
    the total number of iterations in each dataflow module, while the kernel sparsity
    indicates the averaged number of weighted sum operations occurring within convolution
    layers. Using the sparsity statistics, we can estimate the average latency and
    resource utilization for each dataflow module. Take the depthwise 3 × 3 convolution
    as an example. The latency, the number of BRAM, and the DSP consumption can be
    estimated as:


    $$\begin{aligned} lat &= (H \times W \times \text{S}\_{\text{s}}) \times (9 \times
    \text{S}\_{\text{k}}) \times (C/PF) \\ bram &= \lceil (B \times 9 \times C) /
    16 \text{K} / PF \rceil \times PF \\ ds \, p &= PF \end{aligned} \tag{5}$$


    where , is the input height and width in the current layer, is the number of channels,
    and is the channel parallel factor. is the bitwidth of the quantized weight and
    we assume one BRAM can store 16 Kb data. The ( × × ) term reflects the averaged
    number of iterations on the spatial dimension of this layer and (9 × ) × (/ )
    estimates the averaged cycles needed for computing an output. The constant weight
    buffer is also partitioned to satisfy the on-chip bandwidth requirement determined
    by the parallelism factor .


    Using the similar approaches, we can formulate the latency and resource utilization
    for the remaining modules. Since all the layers are pipelined, we can estimate
    the theoretical throughput by determining the maximum latency across all layers.
    Thus, the overall hardware optimization problem can be written as:


    <span id="page-6-2"></span>

    $$\begin{aligned} \text{min} & \quad \text{lat} \\ \text{s.t.} & \quad \text{lat}\_{\bar{t}}
    \le \text{lat}, \quad \text{i in all layers} \\ & \sum\_{\bar{t}} r\_{\bar{t}\bar{f}}
    \le R\_{\bar{f}}, \quad \text{i in all layers, j in [BRAM, DSP]} \end{aligned}
    \tag{6}$$


    where means the type resource consumed for layer . is the total resource of each
    kind available. The current framework mainly concerns BRAM and DSP resources on
    FPGA, and the proposed optimization program will be solved by an open-sourced
    mixed integer programming solver.


    3.4.2 Searching for Efficient Models. For an all-on-chip accelerator, the weights
    of all the layers should be placed on-chip using BRAM. In application scenarios
    with tight hardware resource constraints, deploying larger models like ResNet
    [\[24\]](#page-10-25) might be infeasible. Many


    <span id="page-6-1"></span>![](_page_6_Figure_10.jpeg)


    Figure 11: Optimization flow of ESDA.


    previous research works have explored integrating Neural Architecture Search into
    the deployment process to facilitate the model performances while satisfying the
    hardware constraints.


    Inspired by these concepts, we develop a simple two-step greedy search algorithm
    to identify efficient models for deployment. Initially, our software randomly
    samples a pool of model architectures built upon the MBConv convolution block.
    The search space encompasses variations in: (1) the number of blocks; (2) the
    stride of the block; (3) the number of channels in each layer. The models are
    sampled within a coarse-defined model size range according to the available on-chip
    buffer size. Also, the total down-sampling ratio of each model remains fixed.
    These sampled models, along with the sparsity statistic, are fed into the hardware
    optimizer, which searches for the optimal hardware mapping of each model and estimates
    its performance. Running the hardware optimizer for each model typically takes
    only several seconds/minutes to complete. This means that for each dataset, we
    can effortlessly sample hundreds of models.


    Once all the hardware performance profiles for all the models are collected, the
    software selects the top-k highest throughput models for training and finally
    chooses the model with the best accuracy. This approach allows us to identify
    models that fit the application constraints while balancing between accuracy and
    hardware performance. Note that the current simple model search algorithm does
    not leverage any sophisticated reinforcement learning or other meta-learning algorithms.
    Nevertheless, the possibility of integrating other Neural Architecture Search
    algorithms presents an exciting direction for future exploration. Figure [11](#page-6-1)
    provides a summary of the co-optimization process in ESDA.


    ### <span id="page-6-0"></span>4 EVALUATIONS


    ### 4.1 Experiment Setup


    We evaluated ESDA on five different event-based gestures or object recognition
    datasets: DvsGesture [\[2\]](#page-10-6), RoShamBo17 [\[35\]](#page-11-13), ASL-DVS
    [\[5\]](#page-10-26), N-MNIST and N-Caltech101 [\[41\]](#page-11-14). DvsGesture
    contains 10 classes of human body gestures captured by DVS128 camera and RoShamBo17
    records hand gestures playing rock-scissors-paper games. ASL-DVS is a large-scale
    event-based American sign language letters recognition dataset recorded by DAVIS240C
    event camera. N-Caltech101 and N-MNIST datasets slightly differ from previous
    datasets that were directly shot in real scenes. Instead,


    <span id="page-7-0"></span>![](_page_7_Figure_2.jpeg)


    Figure 12: Compare the spatial sparsity between standard and submanifold convolution
    on different event-based vision datasets. The x-axis of each subplot is the spatial
    resolution of the feature activation. Accuracy results of two different models
    in floating point precision are also written in the legends.


    both are recaptured from static image-based Caltech101 [\[12\]](#page-10-27) and
    MNIST [\[27\]](#page-10-28) datasets by using an actuated pan-tilt event-camera
    platform. All the datasets are preprocessed with a similar method that first clips
    event recordings with a fixed time interval and then generates histogram representation
    with two channels (positive and negative events). Except for the RoShamBo17 dataset,
    we directly use their preprocessed histogram format.


    For hardware experiments, we implement ESDA on an embedded FPGA board ZCU102 with
    a Zynq UltraScale+ MPSoC device (XCZU9EG) using Vitis HLS and Vivado Design Flow.
    End-to-end hardware performances are measured with on-board testing after synthesis,
    place, and route. Models are trained using PyTorch and MinkowskiEngine [\[8\]](#page-10-29)
    sparse DNN library. Weights and activations are quantized into 8 bits by using
    HAWQ-V3 [\[47\]](#page-11-15) framework in our training pipeline. The hardware
    optimization program in Eqn. [6](#page-6-2) is solved by a mixed-integer geometric
    programming solver developed in AGNA [\[11\]](#page-10-30) with SCIP [\[4\]](#page-10-31)
    and GPkit [\[6\]](#page-10-32) optimization frameworks.


    ### 4.2 Performance of Submanifold Convolution


    Figure [12](#page-7-0) shows the spatial statistics from different datasets. The
    sparsity ratios are measured on different feature resolutions throughout the network.
    For example, the N-MNIST dataset has the input resolution being 34 × 34. Through
    the downsample stages of the network, the resolution will change to 17×17, 8×8,
    and finally 4×4. We can see that submanifold sparse convolution can significantly
    improve sparsity compared to standard convolution. Even though all the input data
    are very sparse (1.1% to 23.1%), the dilation effect (shown in Figure [3\)](#page-3-1)
    of standard convolution can easily lead to very dense features throughout the
    network. In ASL-DVS dataset, using submanifold sparse convolution can lead to
    as much as 3.4× sparser features than standard convolution.


    To compare the accuracy performances of two different types of convolution, we
    trained MobileNetV2 models with both convolution methods on N-Caltech101, DvsGesture,
    and ASL-DVS. Since N-MNIST and RoShamBo17 have smaller input resolutions, we use
    a customized network architecture instead. The accuracy results (written in the
    legends of Figure [12\)](#page-7-0) show that for most datasets, submanifold sparse
    convolution achieves similar or higher accuracy compared with standard convolution.
    In summary, submanifold sparse convolution showcases its ability to effectively
    leverage


    the intrinsic spatial sparsity present in event-based datasets while having similar
    accuracy performances compared with standard convolution, making it a highly promising
    candidate for hardware acceleration.


    ### 4.3 Compare ESDA with Dense Dataflow


    To demonstrate the effectiveness of the sparse dataflow modules, we individually
    synthesized all the MBConv convolution blocks of the MobileNetV2 network and evaluated
    their performance across various input sparsity ratios. The hardware configuration
    of each block aligns with the overall optimization result of the entire MobileNetV2
    architecture. To evaluate the overhead for supporting dynamic sparse control logic,
    we implement a dense baseline with a similar sliding-window-based dataflow that
    uses a standard line buffer design. The dense baseline eliminates all token stream
    interfaces and dynamic logic components, maintaining identical parallel factors,
    bitwidths, and other design parameters in alignment with the sparse implementations.


    Figure [13](#page-8-0) presents the speedup ratio of ESDA modules over the dense
    baseline using C/RTL co-simulation latency. The inputs consist of randomly generated
    data with varying spatial sparsity ratios, ranging from 10% to 90%. The results
    demonstrate that dynamic sparse dataflow architecture can effectively leverage
    sparsity in general. With an input non-zero ratio of 10%, the sparse designs achieve
    significant speedups of 4.5 − 11× compared to the dense baselines. Moreover, the
    speedup shows a nearly linear increase as sparsity grows.


    However, it is worth noting that the overhead of supporting dynamic sparse computation
    does exist. Specifically, when the input is nearly dense (greater than 70% NZ),
    certain blocks like blk\_0 to blk\_5 exhibit slower performance than the dense
    baseline. These blocks typically have large spatial resolutions and smaller channels,
    making them more susceptible to overhead. Fortunately, it turns out that these
    larger resolution layers in the early stages of a network typically have higher
    sparsity (usually below 40% NZ) as shown in Figure [12.](#page-7-0) As a result,
    despite potential slowdowns with very dense input, these modules can still attain
    notable speedup with highly sparse inputs.


    <span id="page-8-0"></span>![](_page_8_Figure_2.jpeg)


    Figure 13: Speedup of ESDA sparse dataflow architecture over sliding-window-based
    dense dataflow architecture with different input sparsity ratios (randomly generated).
    The sparse and dense designs use identical hardware configurations like Parallel
    Factor, Bitwidth, etc. blk\_x refers to different convolution blocks in MobileNetV2.


    <span id="page-8-1"></span>![](_page_8_Figure_4.jpeg)


    Figure 14: Compare ESDA with an embedded GPU platform (NVIDIA Jetson Xavier NX).
    For GPU performances, we tested both dense DNN implementation using PyTorch (baseline)
    and submanifold sparse DNN implementation using MinkowskiEngine. And we compared
    both MobileNetV2 and the customized models searched by ESDA.


    ### 4.4 Compare ESDA with Embedded GPU


    Figure [14](#page-8-1) shows the comparison between ESDA and an embedded GPU platform
    (NVIDIA Jetson Xavier NX) on three event-based datasets: N-Caltech101, DvsGesture,
    and ASL-DVS. For GPU implementation, we tested the performance of (1) dense DNN
    implementation using PyTorch and (2) submanifold sparse DNN implementation using
    MinkowskiEngine [\[8\]](#page-10-29) library. In addition, we evaluated the MobileNetV2
    (with width-multiplier being 0.5) as well as the customized ESDA-Net (the models
    searched by using ESDA optimization flow for each dataset). The latency and throughput
    performances of GPU use batch size 1 and 128 respectively.


    In terms of latency performance, ESDA exhibits a substantial speedup over both
    dense and sparse GPU implementations for all the model-dataset combinations. In
    comparison to the dense GPU baseline, ESDA delivers 3.3 − 23.0× speedup on MobileNetV2
    and 9.4 − 54.8× speedup on customized models. An interesting observation is that
    the latency performance of sparse GPU implementation lags behind the dense GPU
    baseline. This discrepancy primarily comes from the way that GPUs handle sparsity
    in submanifold convolutions. It stores the sparse input coordinates in a hash
    table and breaks the original computation into a fine-grained sparse gatherscatter
    flow for each spatial kernel (marked as k0-k8 in Figure [3\)](#page-3-1). In scenarios
    with relatively small batch sizes, this additional overhead can have an adverse
    impact on performance.


    As for throughput performances, ESDA surpasses the dense GPU baseline in most
    cases, with the exception being MobileNetV2 on N-Caltech101. The smaller speedup
    ratio is attributed to the fact that current ESDA is mainly optimized for batch-1
    low-latency inference. This design choice is driven by its primary focus on near-sensor
    or edge systems, where latency performance is vital. Nevertheless, our customized
    models searched by the co-optimization flow also demonstrate better speedup performances
    than MobileNetV2, which highlights the advantages and effectiveness of the framework.
    Last but not least, ESDA demonstrates an average of 5.8× and 3.3× improvement
    of energy efficiency (measured in Joules per inference) when compared with GPU
    dense and sparse implementations, respectively, across the benchmarks. This underscores
    the low-power capability of our sparse architecture for event-based vision DNN
    inference.


    ### 4.5 Compare ESDA with Other Related Works


    Table [1](#page-9-1) provides the detailed hardware implementation results of
    ESDA and other related works on different datasets. The hardware performances
    of ESDA were measured on a ZCU102 FPGA board with the programmable logic (PL)
    running at 187 MHz. The power was measured during runtime using the built-in power
    monitor controlled by the PMBus. For N-Caltech101, DvsGesture, and ASL-DVS datasets,
    we implement two models: MobileNetV2 (with the width-multiplier being 0.5) and
    the customized models. Note that the customized models are all named EDSA-Net
    in Table [1](#page-9-1) for brevity but the model for each dataset is different.


    In general, ESDA demonstrates superior performance in terms of overall system
    efficiency and effectiveness. Across all the datasets, ESDA achieves ultra-low
    latency (0.15 ms to 7.12 ms), significantly boosting the real-time performance
    of event-based DNN inference.


    <span id="page-9-1"></span>


    |                | Dataset      | Resolution | Modela      | Bit<br>width | Acc<br>(%)
    | Latency<br>(ms) | Throughput<br>(fps) | Power<br>(W) | Energy<br>Efficiency<br>(mJ/inf)
    | Implementation<br>Detailsb     |      |      |      |

    |----------------|--------------|------------|-------------|--------------|------------|-----------------|---------------------|--------------|----------------------------------|--------------------------------|------|------|------|

    |                |              |            |             |              |            |                 |                     |              |                                  |
    DSP                            | BRAM | FF   | LUT  |

    | ESDA<br>(Ours) | N-Caltech101 | 180 × 240  | ESDA-Net    | 8            | 72.4       |
    3.09            | 323                 | 1.81         | 5.61                             |
    1792                           | 1278 | 115K | 154K |

    |                |              |            | MobileNetV2 | 8            | 71.6       |
    7.12            | 140                 | 2.10         | 14.96                            |
    1992                           | 1600 | 198K | 207K |

    |                | DvsGesture   | 128 × 128  | ESDA-Net    | 8            | 92.5       |
    0.66            | 1526                | 1.58         | 1.03                             |
    1532                           | 848  | 97K  | 128K |

    |                |              |            | MobileNetV2 | 8            | 93.9       |
    1.19            | 839                 | 1.73         | 2.06                             |
    1636                           | 1134 | 104K | 140K |

    |                | ASL-DVS      | 180 × 240  | ESDA-Net    | 8            | 99.5       |
    0.71            | 1406                | 1.60         | 1.14                             |
    1494                           | 917  | 97K  | 131K |

    |                |              |            | MobileNetV2 | 8            | 99.3       |
    1.08            | 927                 | 1.75         | 1.88                             |
    1416                           | 1069 | 108K | 144K |

    |                | N-MNIST      | 34 × 34    | ESDA-Net    | 8            | 98.9       |
    0.15            | 6657                | 1.55         | 0.23                             |
    1525                           | 978  | 93K  | 121K |

    |                | RoShamBo17   | 64 × 64    | ESDA-Net    | 8            | 99.6       |
    0.98            | 1016                | 1.40         | 1.38                             |
    1282                           | 765  | 72K  | 95K  |

    | NullHop        | RoShamBo17   | 64 × 64    | RoshamboNet | 16           | 99.3       |
    10              | 160                 | 0.27         | 1.69                             |
    657                            | 802  | 139K | 266K |

    | PPF            | -            | 60 × 40    | PFF-BNN     | 1            | 87         |
    7.71            | -                   | -            | -                                |
    12                             | 223  | 16K  | 42K  |

    | Asynet         | N-Caltech101 | 180 × 240  | VGG         | FP32         | 74.5       |
    80.4            | -                   | -            | -                                |
    CPU                            |      |      |      |

    | TrueNorth      | DvsGesture   | 64 × 64    | SNN         | Tenary       | 94.6       |
    105             | -                   | 0.18         | 18.7                             |
    Samsung 28 nm LPP CMOS process |      |      |      |

    | Loihi          | DvsGesture   | 32 × 32    | SNN         | 9            | 90.5       |
    11.43           | -                   | -            | -                                |
    Intel 14 nm process            |      |      |      |


    #### Table 1: System Performance


    <sup>a</sup> ESDA-Net refers to the customized network for each dataset. <sup>b</sup>
    If written in 4 columns, the designs are implemented on FPGA.


    As mentioned before, the current implementation of ESDA only supports batch size
    1 inference, and the throughput is calculated using 1/latency. Nevertheless, ESDA
    still achieves over 1000 fps throughput for most datasets except N-Caltech101.
    The performance of N-Caltech101 dataset is slightly lower than the others because
    it has a larger spatial resolution and denser input. On the other hand, ESDA also
    showcases notable low power and high energy efficiency features. The power consumption
    of the FPGA fabric ranges from 1.4 W to 2.1 W across different datasets, with
    the average energy per inference spanning from 0.23 mJ to 14.96 mJ. In summary,
    ESDA provides a promising solution for low-power DNN acceleration on edge event-based
    vision systems.


    ESDA also demonstrates better system performances compared to other prior FPGA
    accelerators, NullHop [\[1,](#page-10-1) [31\]](#page-10-2) and PPF [\[40\]](#page-11-8).
    Both are FPGA DNN accelerators that have been applied to event-based vision tasks.
    On the RoShamBo17 dataset, ESDA achieves 10.2× speedup in latency and 6.4× speedup
    in throughput while also having better accuracy performance. In terms of power
    efficiency, although NullHop exhibits lower power consumption, which may be attributed
    to its lower clock frequency (60 MHz), ESDA outperforms it in terms of energy
    efficiency (1.38 mJ/inf vs 1.69 mJ/inf). PPF is a binary neural network FPGA accelerator
    for event-based pedestrian detection. Since PPF did not release their datasets,
    we can not compare with them directly. With similar input resolution on the RoShamBo17
    dataset, ESDA has significant latency speedup over PPF (0.98 ms vs 7.71 ms).


    Asynet [\[39\]](#page-11-16) is an asynchronous event-based submanifold sparse
    convolution network implemented on CPU. Despite the attractive asynchronous feature
    of Asynet, the overhead of asynchronous computation can be high, which involves
    updating the kernel maps and feature vectors incrementally for each event. On
    the N-Caltech101 dataset, ESDA achieves 26× speedup compared with Asynet with
    only around 2% lower accuracy. When implementing Asynet on customized hardware,
    one can expect a significant amount of random memory access for fetching and updating
    non-zero features. Otherwise, the accelerator shall buffer all the intermediate
    features on-chip, which can easily overwhelm the available resources.


    We also compare ESDA with neuromorphic hardware solutions. TrueNorth [\[38\]](#page-11-4)
    and Loihi [\[10\]](#page-10-9) are well-known SNN chips, and they both have been
    used to accelerate event-based gesture recognition on the DvsGesture dataset [\[2,](#page-10-6)
    [37\]](#page-11-17). In terms of latency, ESDA achieves 160× and 17.4× speedup
    over TrueNorth and Loihi, respectively. At the same time, ESDA also exhibits 18×
    better energy efficiency compared with TrueNorth. In essence, ESDA attains rapid
    and energy-efficient DNN acceleration for event-based vision without relying on
    traditional neuromorphic techniques, which paves the way to future efficient event-based
    systems.


    ### <span id="page-9-0"></span>5 CONCLUSION


    In this work, we have presented ESDA, a novel composable dynamic sparse dataflow
    architecture for efficient event-based vision DNN inference. ESDA confronts the
    challenge of low-latency and low-power processing of event-based data by focusing
    on its inherent sparsity. This is made possible by synergizing algorithmic advancements
    through submanifold sparse convolution with novel architectural solutions realized
    through the sparse dataflow design. The flexibility and programmability of the
    ESDA are also supercharged by the highly modular dataflow design as well as the
    sparsity-aware design optimization flow for FPGA platforms. Extensive experiments
    show the ESDA achieves up to 54.8× speedup than the embedded GPU as well as 10.2×
    speedup compared to NullHop. In addition, ESDA also shows 160× speedup and 17.4×
    better energy efficiency compared to SNN hardware platform TrueNorth. The ability
    to effectively exploit sparsity in ESDA opens up new possibilities for real-time,
    energy-efficient DNN inference for event-based vision applications.


    ### ACKNOWLEDGMENTS


    This work was supported in part by the Research Grants Council (RGC) of Hong Kong
    under the Research Impact Fund project R7003- 21 and the Theme-based Research
    Scheme (TRS) Project T45-701- 22-R. This work was supported by AI Chip Center
    for Emerging Smart Systems (ACCESS), sponsored by InnoHK funding, Hong Kong SAR.


    ### REFERENCES


    - <span id="page-10-1"></span>[1] Alessandro Aimar, Hesham Mostafa, Enrico Calabrese,
    Antonio Rios-Navarro, Ricardo Tapiador-Morales, Iulia-Alexandra Lungu, Moritz
    B. Milde, Federico Corradi, Alejandro Linares-Barranco, Shih-Chii Liu, and Tobi
    Delbruck. 2019. NullHop: A Flexible Convolutional Neural Network Accelerator Based
    on Sparse Representations of Feature Maps. IEEE Transactions on Neural Networks
    and Learning Systems 30, 3 (2019), 644–656. [https://doi.org/10.1109/TNNLS.2018.](https://doi.org/10.1109/TNNLS.2018.2852335)
    [2852335](https://doi.org/10.1109/TNNLS.2018.2852335)

    - <span id="page-10-6"></span>[2] Arnon Amir, Brian Taba, David Berg, Timothy
    Melano, Jeffrey McKinstry, Carmelo Di Nolfo, Tapan Nayak, Alexander Andreopoulos,
    Guillaume Garreau, Marcela Mendoza, et al. 2017. A low power, fully event-based
    gesture recognition system. In Proceedings of the IEEE conference on computer
    vision and pattern recognition. 7243–7252.

    - <span id="page-10-11"></span>[3] Myo Tun Aung, Rodney Teo, and Garrick Orchard.
    2018. Event-based Plane-fitting Optical Flow for Dynamic Vision Sensors in FPGA.
    In 2018 IEEE International Symposium on Circuits and Systems (ISCAS). 1–5. [https://doi.org/10.1109/ISCAS.](https://doi.org/10.1109/ISCAS.2018.8351588)
    [2018.8351588](https://doi.org/10.1109/ISCAS.2018.8351588)

    - <span id="page-10-31"></span>[4] Ksenia Bestuzheva, Mathieu Besançon, Wei-Kun
    Chen, Antonia Chmiela, Tim Donkiewicz, Jasper van Doornmalen, Leon Eifler, Oliver
    Gaul, Gerald Gamrath, Ambros Gleixner, Leona Gottwald, Christoph Graczyk, Katrin
    Halbig, Alexander Hoen, Christopher Hojny, Rolf van der Hulst, Thorsten Koch,
    Marco Lübbecke, Stephen J. Maher, Frederic Matter, Erik Mühmer, Benjamin Müller,
    Marc E. Pfetsch, Daniel Rehfeldt, Steffan Schlein, Franziska Schlösser, Felipe
    Serrano, Yuji Shinano, Boro Sofranac, Mark Turner, Stefan Vigerske, Fabian Wegscheider,
    Philipp Wellner, Dieter Weninger, and Jakob Witzig. 2021. The SCIP Optimization
    Suite 8.0. Technical Report. Optimization Online. [http://www.optimization](http://www.optimization-online.org/DB_HTML/2021/12/8728.html)[online.org/DB\\_HTML/2021/12/8728.html](http://www.optimization-online.org/DB_HTML/2021/12/8728.html)

    - <span id="page-10-26"></span>[5] Yin Bi, Aaron Chadha, Alhabib Abbas, Eirina
    Bourtsoulatze, and Yiannis Andreopoulos. 2019. Graph-based object classification
    for neuromorphic vision sensing. In Proceedings of the IEEE/CVF international
    conference on computer vision. 491–501.

    - <span id="page-10-32"></span>[6] Edward Burnell, Nicole B Damen, and Warren
    Hoburg. 2020. GPkit: A Human-Centered Approach to Convex Optimization in Engineering
    Design. In Proceedings of the 2020 CHI Conference on Human Factors in Computing
    Systems. <https://doi.org/10.1145/3313831.3376412>

    - <span id="page-10-19"></span>[7] Yu-Hsin Chen, Tien-Ju Yang, Joel Emer, and
    Vivienne Sze. 2019. Eyeriss v2: A Flexible Accelerator for Emerging Deep Neural
    Networks on Mobile Devices. IEEE Journal on Emerging and Selected Topics in Circuits
    and Systems 9, 2 (2019), 292–308.<https://doi.org/10.1109/JETCAS.2019.2910232>

    - <span id="page-10-29"></span>[8] Christopher Choy, JunYoung Gwak, and Silvio
    Savarese. 2019. 4d spatio-temporal convnets: Minkowski convolutional neural networks.
    In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition.
    3075–3084.

    - <span id="page-10-23"></span>[9] Steven Colleman, Thomas Verelst, Linyan Mei,
    Tinne Tuytelaars, and Marian Verhelst. 2021. Processor Architecture Optimization
    for Spatially Dynamic Neural Networks. In 2021 IFIP/IEEE 29th International Conference
    on Very Large Scale Integration (VLSI-SoC). 1–6.<https://doi.org/10.1109/VLSI-SoC53125.2021.9607013>

    - <span id="page-10-9"></span>[10] Mike Davies, Narayan Srinivasa, Tsung-Han Lin,
    Gautham Chinya, Yongqiang Cao, Sri Harsha Choday, Georgios Dimou, Prasad Joshi,
    Nabil Imam, Shweta Jain, Yuyun Liao, Chit-Kwan Lin, Andrew Lines, Ruokun Liu,
    Deepak Mathaikutty, Steven McCoy, Arnab Paul, Jonathan Tse, Guruguhanathan Venkataramanan,
    Yi-Hsin Weng, Andreas Wild, Yoonseok Yang, and Hong Wang. 2018. Loihi: A Neuromorphic
    Manycore Processor with On-Chip Learning. IEEE Micro 38, 1 (2018), 82–99.<https://doi.org/10.1109/MM.2018.112130359>

    - <span id="page-10-30"></span>[11] Yuhao Ding, Jiajun Wu, Yizhao Gao, Maolin
    Wang, and Hayden Kwok-Hay So. 2023. Model-Platform Optimized Deep Neural Network
    Accelerator Generation through Mixed-Integer Geometric Programming. In 2023 IEEE
    31st Annual International Symposium on Field-Programmable Custom Computing Machines
    (FCCM). 83–93.<https://doi.org/10.1109/FCCM57271.2023.00018>

    - <span id="page-10-27"></span>[12] Li Fei-Fei, R. Fergus, and P. Perona. 2004.
    Learning Generative Visual Models from Few Training Examples: An Incremental Bayesian
    Approach Tested on 101 Object Categories. In 2004 Conference on Computer Vision
    and Pattern Recognition Workshop. 178–178.<https://doi.org/10.1109/CVPR.2004.383>

    - <span id="page-10-21"></span>[13] Yu Feng, Gunnar Hammonds, Yiming Gan, and
    Yuhao Zhu. 2022. Crescent: Taming Memory Irregularities for Accelerating Deep
    Point Cloud Analytics. In Proceedings of the 49th Annual International Symposium
    on Computer Architecture (New York, New York) (ISCA ''22). Association for Computing
    Machinery, New York, NY, USA, 962–977.<https://doi.org/10.1145/3470496.3527395>

    - <span id="page-10-5"></span>[14] Thomas Finateu, Atsumi Niwa, Daniel Matolin,
    Koya Tsuchimoto, Andrea Mascheroni, Etienne Reynaud, Pooria Mostafalu, Frederick
    Brady, Ludovic Chotard, Florian LeGoff, Hirotsugu Takahashi, Hayato Wakabayashi,
    Yusuke Oike, and Christoph Posch. 2020. 5.10 A 1280×720 Back-Illuminated Stacked
    Temporal Contrast Event-Based Vision Sensor with 4.86µm Pixels, 1.066GEPS Readout,
    Programmable Event-Rate Controller and Compressive Data-Formatting Pipeline. In
    2020 IEEE International Solid- State Circuits Conference - (ISSCC). 112– 114.<https://doi.org/10.1109/ISSCC19947.2020.9063149>

    - <span id="page-10-10"></span>[15] Steve B. Furber, Francesco Galluppi, Steve
    Temple, and Luis A. Plana. 2014. The SpiNNaker Project. Proc. IEEE 102, 5 (2014),
    652–665. [https://doi.org/10.1109/](https://doi.org/10.1109/JPROC.2014.2304638)


    [JPROC.2014.2304638](https://doi.org/10.1109/JPROC.2014.2304638)


    - <span id="page-10-0"></span>[16] G. Gallego, T. Delbruck, G. Orchard, C. Bartolozzi,
    B. Taba, A. Censi, S. Leutenegger, A. J. Davison, J. Conradt, K. Daniilidis, and
    D. Scaramuzza. 2022. Event-Based Vision: A Survey. IEEE Transactions on Pattern
    Analysis and Machine Intelligence 44, 01 (jan 2022), 154–180.<https://doi.org/10.1109/TPAMI.2020.3008413>

    - <span id="page-10-4"></span>[17] Yizhao Gao, Yuhao Ding, Baoheng Zhang, and
    Hayden Kwok Hay So. 2023. ESDA: A Composable Dynamic Sparse Dataflow Architecture
    for Efficient Event-based Vision Processing on FPGA. (12 2023).<https://doi.org/10.25442/hku.24784221.v1>

    - <span id="page-10-14"></span>[18] Yizhao Gao, Song Wang, and Hayden Kwok-Hay
    So. 2022. REMOT: A Hardware-Software Architecture for Attention-Guided Multi-Object
    Tracking with Dynamic Vision Sensors on FPGAs. In Proceedings of the 2022 ACM/SIGDA
    International Symposium on Field-Programmable Gate Arrays (Virtual Event, USA)
    (FPGA ''22). Association for Computing Machinery, New York, NY, USA, 158–168.
    [https:](https://doi.org/10.1145/3490422.3502365) [//doi.org/10.1145/3490422.3502365](https://doi.org/10.1145/3490422.3502365)

    - <span id="page-10-15"></span>[19] Yizhao Gao, Song Wang, and Hayden Kwok-Hay
    So. 2023. A Reconfigurable Architecture for Real-Time Event-Based Multi-Object
    Tracking. ACM Trans. Reconfigurable Technol. Syst. (apr 2023).<https://doi.org/10.1145/3593587>
    Just Accepted.

    - <span id="page-10-24"></span>[20] Yizhao Gao, Baoheng Zhang, Xiaojuan Qi, and
    Hayden Kwok-Hay So. 2023. DPACS: Hardware Accelerated Dynamic Neural Network Pruning
    through Algorithm-Architecture Co-Design. In Proceedings of the 28th ACM International
    Conference on Architectural Support for Programming Languages and Operating Systems,
    Volume 2 (Vancouver, BC, Canada) (ASPLOS 2023). Association for Computing Machinery,
    New York, NY, USA, 237–251. [https://doi.org/10.1145/3575693.](https://doi.org/10.1145/3575693.3575728)
    [3575728](https://doi.org/10.1145/3575693.3575728)

    - <span id="page-10-8"></span>[21] Daniel Gehrig, Antonio Loquercio, Konstantinos
    G Derpanis, and Davide Scaramuzza. 2019. End-to-end learning of representations
    for asynchronous eventbased data. In Proceedings of the IEEE/CVF International
    Conference on Computer Vision. 5633–5643.

    - <span id="page-10-20"></span>[22] Ashish Gondimalla, Noah Chesnut, Mithuna Thottethodi,
    and T. N. Vijaykumar. 2019. SparTen: A Sparse Tensor Accelerator for Convolutional
    Neural Networks. In Proceedings of the 52nd Annual IEEE/ACM International Symposium
    on Microarchitecture (Columbus, OH, USA) (MICRO ''52). Association for Computing
    Machinery, New York, NY, USA, 151–165.<https://doi.org/10.1145/3352460.3358291>

    - <span id="page-10-3"></span>[23] Benjamin Graham, Martin Engelcke, and Laurens
    Van Der Maaten. 2018. 3d semantic segmentation with submanifold sparse convolutional
    networks. In Proceedings of the IEEE conference on computer vision and pattern
    recognition. 9224–9232.

    - <span id="page-10-25"></span>[24] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and
    Jian Sun. 2016. Deep residual learning for image recognition. In Proceedings of
    the IEEE conference on computer vision and pattern recognition. 770–778.

    - <span id="page-10-18"></span>[25] Michael Hofstätter, Martin Litzenberger, Daniel
    Matolin, and Christoph Posch. 2011. Hardware-accelerated address-event processing
    for high-speed visual object recognition. In 2011 18th IEEE International Conference
    on Electronics, Circuits, and Systems. 89–92.<https://doi.org/10.1109/ICECS.2011.6122221>

    - <span id="page-10-7"></span>[26] Xavier Lagorce, Garrick Orchard, Francesco
    Galluppi, Bertram E. Shi, and Ryad B. Benosman. 2017. HOTS: A Hierarchy of Event-Based
    Time-Surfaces for Pattern Recognition. IEEE Transactions on Pattern Analysis and
    Machine Intelligence 39, 7 (2017), 1346–1359.<https://doi.org/10.1109/TPAMI.2016.2574707>

    - <span id="page-10-28"></span>[27] Yann LeCun, Corinna Cortes, and CJ Burges.
    2010. MNIST handwritten digit database. ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist
    2 (2010).

    - <span id="page-10-22"></span>[28] Yujun Lin, Zhekai Zhang, Haotian Tang, Hanrui
    Wang, and Song Han. 2021. PointAcc: Efficient Point Cloud Accelerator. In MICRO-54:
    54th Annual IEEE/ACM International Symposium on Microarchitecture (Virtual Event,
    Greece) (MICRO ''21). Association for Computing Machinery, New York, NY, USA,
    449–461. [https:](https://doi.org/10.1145/3466752.3480084) [//doi.org/10.1145/3466752.3480084](https://doi.org/10.1145/3466752.3480084)

    - <span id="page-10-16"></span>[29] A. Linares-Barranco, F. Gómez-Rodríguez, V.
    Villanueva, L. Longinotti, and T. Delbrück. 2015. A USB3.0 FPGA event-based filtering
    and tracking framework for dynamic vision sensors. In 2015 IEEE International
    Symposium on Circuits and Systems (ISCAS). 2417–2420.<https://doi.org/10.1109/ISCAS.2015.7169172>

    - <span id="page-10-17"></span>[30] Alejandro Linares-Barranco, Fernando Perez-Peña,
    Diederik Paul Moeys, Francisco Gomez-Rodriguez, Gabriel Jimenez-Moreno, Shih-Chii
    Liu, and Tobi Delbruck. 2019. Low Latency Event-Based Filtering and Feature Extraction
    for Dynamic Vision Sensors in Real-Time FPGA Applications. IEEE Access 7 (2019),
    134926–134942.<https://doi.org/10.1109/ACCESS.2019.2941282>

    - <span id="page-10-2"></span>[31] Alejandro Linares-Barranco, Antonio Rios-Navarro,
    Salvador Canas-Moreno, Enrique Piñero Fuentes, Ricardo Tapiador-Morales, and Tobi
    Delbruck. 2021. Dynamic Vision Sensor Integration on FPGA-Based CNN Accelerators
    for High-Speed Visual Classification. In International Conference on Neuromorphic
    Systems 2021 (Knoxville, TN, USA) (ICONS 2021). Association for Computing Machinery,
    New York, NY, USA, Article 21, 7 pages.<https://doi.org/10.1145/3477145.3477167>

    - <span id="page-10-12"></span>[32] Min Liu and Tobi Delbruck. 2017. Block-matching
    optical flow for dynamic vision sensors: Algorithm and FPGA implementation. In
    2017 IEEE International Symposium on Circuits and Systems (ISCAS). 1–4. [https://doi.org/10.1109/ISCAS.](https://doi.org/10.1109/ISCAS.2017.8050295)
    [2017.8050295](https://doi.org/10.1109/ISCAS.2017.8050295)

    - <span id="page-10-13"></span>[33] Min Liu and Tobi Delbruck. 2022. EDFLOW: Event
    Driven Optical Flow Camera With Keypoint Detection and Adaptive Block Matching.
    IEEE Transactions on Circuits and Systems for Video Technology 32, 9 (2022), 5776–5789.
    [https://doi.](https://doi.org/10.1109/TCSVT.2022.3156653)


    <span id="page-11-0"></span>[org/10.1109/TCSVT.2022.3156653](https://doi.org/10.1109/TCSVT.2022.3156653)


    - <span id="page-11-5"></span>[34] Min Liu, Wei-Tse Kao, and Tobi Delbruck. 2019.
    Live demonstration: A realtime event-based fast corner detection demo based on
    fpga. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
    Recognition Workshops. 0–0.

    - <span id="page-11-13"></span>[35] Iulia-Alexandra Lungu, Federico Corradi, and
    Tobi Delbrück. 2017. Live demonstration: Convolutional neural network driven by
    dynamic vision sensor playing RoShamBo. In 2017 IEEE International Symposium on
    Circuits and Systems (ISCAS). 1–1.<https://doi.org/10.1109/ISCAS.2017.8050403>

    - <span id="page-11-2"></span>[36] Ana I Maqueda, Antonio Loquercio, Guillermo
    Gallego, Narciso García, and Davide Scaramuzza. 2018. Event-based vision meets
    deep learning on steering prediction for self-driving cars. In Proceedings of
    the IEEE conference on computer vision and pattern recognition. 5419–5427.

    - <span id="page-11-17"></span>[37] Riccardo Massa, Alberto Marchisio, Maurizio
    Martina, and Muhammad Shafique. 2020. An efficient spiking neural network for
    recognizing gestures with a dvs camera on the loihi neuromorphic processor. In
    2020 International Joint Conference on Neural Networks (IJCNN). IEEE, 1–9.

    - <span id="page-11-4"></span>[38] Paul A Merolla, John V Arthur, Rodrigo Alvarez-Icaza,
    Andrew S Cassidy, Jun Sawada, Filipp Akopyan, Bryan L Jackson, Nabil Imam, Chen
    Guo, Yutaka Nakamura, et al. 2014. A million spiking-neuron integrated circuit
    with a scalable communication network and interface. Science 345, 6197 (2014),
    668–673.

    - <span id="page-11-16"></span>[39] Nico Messikommer, Daniel Gehrig, Antonio Loquercio,
    and Davide Scaramuzza. 2020. Event-based Asynchronous Sparse Convolutional Networks.
    European Conference on Computer Vision. (ECCV). [http://rpg.ifi.uzh.ch/docs/ECCV20\\_](http://rpg.ifi.uzh.ch/docs/ECCV20_Messikommer.pdf)
    [Messikommer.pdf](http://rpg.ifi.uzh.ch/docs/ECCV20_Messikommer.pdf)

    - <span id="page-11-8"></span>[40] Fernando Cladera Ojeda, Anthony Bisulco, Daniel
    Kepple, Volkan Isler, and Daniel D. Lee. 2020. On-Device Event Filtering with
    Binary Neural Networks for Pedestrian Detection Using Neuromorphic Vision Sensors.
    In 2020 IEEE International Conference on Image Processing (ICIP). 3084–3088. [https:](https://doi.org/10.1109/ICIP40778.2020.9191148)
    [//doi.org/10.1109/ICIP40778.2020.9191148](https://doi.org/10.1109/ICIP40778.2020.9191148)

    - <span id="page-11-14"></span>[41] Garrick Orchard, Ajinkya Jayawant, Gregory
    K Cohen, and Nitish Thakor. 2015. Converting static image datasets to spiking
    neuromorphic datasets using saccades. Frontiers in neuroscience 9 (2015), 437.

    - <span id="page-11-6"></span>[42] Angshuman Parashar, Minsoo Rhu, Anurag Mukkara,
    Antonio Puglielli, Rangharajan Venkatesan, Brucek Khailany, Joel Emer, Stephen
    W. Keckler, and


    William J. Dally. 2017. SCNN: An Accelerator for Compressed-Sparse Convolutional
    Neural Networks. In Proceedings of the 44th Annual International Symposium on
    Computer Architecture (Toronto, ON, Canada) (ISCA ''17). Association for Computing
    Machinery, New York, NY, USA, 27–40. [https://doi.org/10.](https://doi.org/10.1145/3079856.3080254)
    [1145/3079856.3080254](https://doi.org/10.1145/3079856.3080254)


    - <span id="page-11-9"></span>[43] Mark Sandler, Andrew Howard, Menglong Zhu,
    Andrey Zhmoginov, and Liang-Chieh Chen. 2018. Mobilenetv2: Inverted residuals
    and linear bottlenecks. In Proceedings of the IEEE conference on computer vision
    and pattern recognition. 4510–4520.

    - <span id="page-11-3"></span>[44] Amos Sironi, Manuele Brambilla, Nicolas Bourdis,
    Xavier Lagorce, and Ryad Benosman. 2018. HATS: Histograms of averaged time surfaces
    for robust eventbased object classification. In Proceedings of the IEEE conference
    on computer vision and pattern recognition. 1731–1740.

    - <span id="page-11-10"></span>[45] Mingxing Tan, Bo Chen, Ruoming Pang, Vijay
    Vasudevan, Mark Sandler, Andrew Howard, and Quoc V Le. 2019. Mnasnet: Platform-aware
    neural architecture search for mobile. In Proceedings of the IEEE/CVF conference
    on computer vision and pattern recognition. 2820–2828.

    - <span id="page-11-11"></span>[46] Mingxing Tan and Quoc Le. 2019. Efficientnet:
    Rethinking model scaling for convolutional neural networks. In International conference
    on machine learning. PMLR, 6105–6114.

    - <span id="page-11-15"></span>[47] Zhewei Yao, Zhen Dong, Zhangcheng Zheng, Amir
    Gholami, Jiali Yu, Eric Tan, Leyuan Wang, Qijing Huang, Yida Wang, Michael Mahoney,
    et al. 2021. Hawq-v3: Dyadic neural network quantization. In International Conference
    on Machine Learning. PMLR, 11875–11886.

    - <span id="page-11-12"></span>[48] Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and
    Jian Sun. 2018. Shufflenet: An extremely efficient convolutional neural network
    for mobile devices. In Proceedings of the IEEE conference on computer vision and
    pattern recognition. 6848–6856.

    - <span id="page-11-1"></span>[49] Xu Zheng, Yexin Liu, Yunfan Lu, Tongyan Hua,
    Tianbo Pan, Weiming Zhang, Dacheng Tao, and Lin Wang. 2023. Deep learning for
    event-based vision: A comprehensive survey and benchmarks. arXiv preprint arXiv:2302.08890
    (2023).

    - <span id="page-11-7"></span>[50] Xuda Zhou, Zidong Du, Qi Guo, Shaoli Liu, Chengsi
    Liu, Chao Wang, Xuehai Zhou, Ling Li, Tianshi Chen, and Yunji Chen. 2018. Cambricon-S:
    Addressing Irregularity in Sparse Neural Networks through A Cooperative Software/Hardware
    Approach. In 2018 51st Annual IEEE/ACM International Symposium on Microarchitecture
    (MICRO). 15–28.<https://doi.org/10.1109/MICRO.2018.00011>'
  decisions:
    evaluation_prompt: 'Qualified. Reason: The paper mentions "Experimental results
      demonstrate that when compared with existing GPU and hardware-accelerated solutions,
      ESDA achieves substantial speedup and improvement in energy efficiency across
      different applications," indicating the presence of experimental evaluation.'
    related_work_prompt: 'Qualified. Reason: The paper includes citations and engages
      with prior research, explaining and comparing to cited work.'
    novelty_prompt: 'Qualified. Reason: The paper proposes a novel composable dynamic
      sparse dataflow architecture (ESDA) for event-based vision processing on FPGA,
      which is a new method and application in this domain.'
    review_only_prompt: 'Qualified. Reason: The paper introduces a new architecture
      (ESDA) for event-based vision processing, presenting novel contributions rather
      than summarizing existing work.'
- title: "Accelerating Boolean Constraint Propagation for Efficient SAT-Solving on\n\
    \  FPGAs"
  abstract: 'We present a hardware-accelerated SAT solver targeting processor/Field

    Programmable Gate Arrays (FPGA) SoCs. Our solution accelerates the most

    expensive subroutine of the Davis-Putnam-Logemann-Loveland (DPLL) algorithm,

    Boolean Constraint Propagation (BCP) through fine-grained FPGA parallelism.

    Unlike prior state-of-the-art solutions, our solver eliminates costly clause

    look-up operations by assigning clauses directly to clause processors on the

    FPGA and dividing large formulas into smaller partitions manageable by FPGA.

    Partitions are hot-swapped during runtime as required and the supported formula

    size is limited only by available external memory, not on-chip FPGA memory. We

    evaluate our solver on a Xilinx Zynq platform with results showing quicker

    execution time across various formula sizes, subject to formula partitioning

    strategy. Compared to prior state-of-the-art, we achieve 1.7x and 1.1x speed up

    on BCP for 2 representative benchmarks and up to 6x total speedup over

    software-only implementation.'
  url: http://arxiv.org/abs/2401.07429v2
  keywords: ''
  document: '# Accelerating Boolean Constraint Propagation for Efficient SAT-Solving
    on FPGAs


    Hari Govindasamy Carleton University Ottawa, Canada hari@sce.carleton.ca


    Babak Esfandiari Carleton University Ottawa, Canada babak@sce.carleton.ca


    Paulo Garcia Chulalongkorn University Bangkok, Thailand paulo.g@chula.ac.th


    #### ABSTRACT


    We present a hardware-accelerated SAT solver targeting processor/Field Programmable
    Gate Arrays (FPGA) SoCs. Our solution accelerates the most expensive subroutine
    of the Davis-Putnam-Logemann-Loveland (DPLL) algorithm, Boolean Constraint Propagation
    (BCP) through fine-grained FPGA parallelism. Unlike prior state-of-the-art solutions,
    our solver eliminates costly clause lookup operations by assigning clauses directly
    to clause processors on the FPGA and dividing large formulas into smaller partitions
    manageable by FPGA. Partitions are hot-swapped during runtime as required and
    the supported formula size is limited only by available external memory, not on-chip
    FPGA memory.


    We evaluate our solver on a Xilinx Zynq platform with results showing quicker
    execution time across various formula sizes, subject to formula partitioning strategy.
    Compared to prior state-of-theart, we achieve 1.7x and 1.1x speed up on BCP for
    2 representative benchmarks and up to 6x total speedup over software-only implementation.


    ## CCS CONCEPTS


    • Computer systems organization → Robotic autonomy; • Networks → Cyber-physical
    networks; • Applied computing → Industry and manufacturing; • Hardware → Hardware
    accelerators; Application specific processors; • Theory of computation → Equational
    logic and rewriting.


    ### KEYWORDS


    FPGA, SAT, acceleration, embedded, boolean, satisfiability


    #### ACM Reference Format:


    Hari Govindasamy, Babak Esfandiari, and Paulo Garcia. 2024. Accelerating Boolean
    Constraint Propagation for Efficient SAT-Solving on FPGAs. In Great Lakes Symposium
    on VLSI 2024 (GLSVLSI ''24), June 12–14, 2024, Clearwater, FL, USA. ACM, New York,
    NY, USA, [5](#page-4-0) pages. [https://doi.org/10.1145/](https://doi.org/10.1145/3649476.3658808)
    [3649476.3658808](https://doi.org/10.1145/3649476.3658808)


    #### 1 INTRODUCTION


    The Boolean Satisfiability problem (SAT) is a fundamental problem in computer
    science, the first NP-Complete problem [\[7\]](#page-4-1). SAT solvers have become
    the backbone of several engineering domains, as any


    GLSVLSI ''24, June 12–14, 2024, Clearwater, FL, USA


    © 2024 Copyright held by the owner/author(s).


    ACM ISBN 979-8-4007-0605-9/24/06.


    <https://doi.org/10.1145/3649476.3658808>


    NP-Complete problem can be encoded as instance of SAT [\[2,](#page-4-2) [7\]](#page-4-1).
    SAT solvers determine whether a given boolean formula is satisfiable by identifying
    an assignment to the formulas'' free variables that evaluate the formula to true.
    The formula is unsatisfiable otherwise. Most SAT solvers target CNF-SAT, a subset
    of SAT that determines the satisfiability of formulas encoded in Conjunctive Normal
    Form (CNF) . Formulas in CNF are conjunctions of clauses, where each clause is
    a disjunction of one or more literals (a variable or its negation).


    With the advent of modern Systems-on-Chip (SoC) comprised of both hard embedded
    processors and configurable FPGA fabric offering myriad implementation opportunities
    [\[17\]](#page-4-3), deployed from the embedded to the high performance computing
    domain [\[1\]](#page-4-4), accelerating SAT-solving through hardware is an attractive
    approach. We present a novel architecture for hardware-accelerated SAT-solving
    that outperforms state of the art solutions, released in open-source form for
    the Xilinx Zynq platform. Specifically, this article offers the following contributions:


    - We describe a methodology to map and runtime-manage clauses across a processor
    and connected FPGA, making efficient use of FPGA resources and avoiding recurring
    performance pitfalls.

    - We describe the implementation of an open-source prototype system, deployed
    on a Xilinx Zynq chip, identifying how the hardware architecture effects the aforementioned
    strategy.

    - We evaluate our design against the state of the art using two representative
    benchmarks, showing speed-ups of 1.7x and 1.1x, respective, and overall a 6x improvement
    over vanilla software execution.


    Section [2](#page-0-0) describes necessary background knowledge on a particular
    SAT-solving algorithm required to understand the remainder of this paper. Section
    [3](#page-1-0) presents an overview of historical solutions and state of the art,
    directly compared against in this paper. Section [4](#page-2-0) presents our contribution,
    evaluated in Section [5,](#page-3-0) with concluding remarks and suggestions for
    future work described in Section [6.](#page-4-5)


    ### <span id="page-0-0"></span>2 BACKGROUND: DPLL AND BCP


    SAT solvers are categorized into complete and incomplete solvers[\[15\]](#page-4-6).
    Complete solvers evaluate every possible variable assignment, ending on the first
    satisfying assignment or after exhausting the search space. A formula is unsatisfiable
    if the complete solver concludes without finding a satisfying assignment. Most
    incomplete solvers use Stochastic Local Search (SLS) to greedily search for a
    satisfying assignment in the formula''s variable assignment search space[\[16\]](#page-4-7).
    While typically quicker than complete solvers, incomplete solvers do not guarantee
    results as they tend to get stuck in local maxima or skip satisfying assignments.
    Since they don''t explore the solution


    Permission to make digital or hard copies of part or all of this work for personal
    or classroom use is granted without fee provided that copies are not made or distributed
    for profit or commercial advantage and that copies bear this notice and the full
    citation on the first page. Copyrights for third-party components of this work
    must be honored. For all other uses, contact the owner/author(s).


    GLSVLSI ''24, June 12–14, 2024, Clearwater, FL, USA Govindasamy, Esfandiari, and
    Garcia


    <span id="page-1-1"></span>![](_page_1_Figure_2.jpeg)


    Figure 1: Interface between processor and FPGA-based BCP Coprocessor. DPLL''s
    BCP is accelerated through fine-grained parallelization across Clause Processors.


    space exhaustively they can never conclude that a formula is unsatisfiable. Davis-Putnam-Logemann-Loveland
    (DPLL) and DPLL-based algorithms are the most predominant complete solvers [\[16\]](#page-4-7).
    DPLL performs two primary operations: 1) decision and 2) Boolean Constraint Propagation
    (BCP). DPLL-based algorithms follow DPLL''s core structure, and propose improved
    decision heuristics, learning and BCP mechanisms. During decision, DPLL heuristically
    picks and assigns truth values to free variables. BCP subsequently propagates
    the effect of the decision using the unit implication rule[\[5\]](#page-4-8).
    The unit implication rule identifies unit clauses where all but one of its literals
    are false. Unit clauses can only be satisfied by assigning the variable to true
    if the literal is positive or to false on negative literals. The resulting assignment
    is known as an implication. BCP repeatedly applies this rule until all clauses
    are satisfied (formula is therefore satisfiable) or at least one clause evaluates
    false (conflict). On conflicts, DPLL backtracks by retracting and/or inverting
    assignments from earlier decisions. BCP is expensive, accounting for 80-90% of
    DPLL''s CPU time, rendering it a prime candidate for hardware acceleration [\[5,](#page-4-8)
    [18\]](#page-4-9). BCP coprocessors accelerate DPLL by implementing specialized
    BCP processing engines on FPGA. These run alongside a General Purpose Processor
    (GPP) that performs the remaining DPLL operations: decision heuristics and backtracking.
    Using this architecture, the BCP coprocessor is first configured with the clauses,
    and then waits to evaluate decisions from the GPP. Any DPLL-based software solver
    can integrate with a BCP-coprocessor by replacing software BCP with the hardware
    accelerated BCP-coprocessor [\[18,](#page-4-9) [19\]](#page-4-10). FPGA-based
    BCP coprocessors are either instance-specific or application-specific. Instancespecific
    solver are built to solve a single SAT instance and designed by translating an
    input formula into its equivalent logical circuit. However, to solve new instances,
    the FPGA requires a complete rebuild (synthesis and FPGA programming may take
    up to several hours). Although these solvers can be significantly quicker than
    their software counterparts, their performance becomes notably slower when build
    times are included. For instance, Ivan et al''s best result against the hole7
    benchmark achieves a 6.66x speedup against MiniSAT[\[11,](#page-4-11) [12\]](#page-4-12);
    however, when build times are included, compilation alone takes 50 seconds, whereas
    MiniSAT finishes in under 0.064 seconds [\[10\]](#page-4-13). Application-specific
    solvers eliminate the need to rebuild the FPGA by instantiating general-purpose
    processing units capable of tackling any SAT instance (given that it fits in hardware).
    The BCP coprocessor is configured with the target problem by simply overwriting
    FPGA memory.


    #### <span id="page-1-0"></span>3 STATE OF THE ART


    Algorithmic techniques for efficient SAT solving have been extensively researched,
    and the literature contains several surveys that describe the history and state
    of the art of the problem ([\[8\]](#page-4-14), [\[13\]](#page-4-15)). Techniques
    aimed at accelerating the execution of a particular SAT solving algorithm include
    software parallelization [\[9\]](#page-4-16), deployment on specialized GPUs [\[14\]](#page-4-17),
    and even acceleration through machinelearning approaches [\[20\]](#page-4-18).


    Our approach sits within FPGA-based acceleration, which began roughly 3 decades
    ago [\[6\]](#page-4-19), with a few prominent results at the turn of the century
    ([\[21\]](#page-4-20), [\[3\]](#page-4-21)). However, it was not until significant
    advances in FPGA performance occurred in the last decade, and the rise of SoC
    platforms combining FPGA fabric with hard processors, that FPGA-based SAT acceleration
    matured. The most notable architectures were proposed by Davis et al [\[5\]](#page-4-8)
    and Thong et al [\[18,](#page-4-9) [19\]](#page-4-10): both exploring the use
    of FPGA to implement BCP coprocessors, keeping the remainder of DPLL in software.


    Davis et al calculate implications in parallel by using several inference engines
    (IE), each assigned a list of clauses (partitions) [\[5\]](#page-4-8). For every
    decision/implication, the clause containing the assignment variable is first retrieved
    before calculating implications. Implications are forwarded to a conflict detector
    that ensures that two or more IEs have not implied opposing values for the same
    variable. Implications are then sent to the processor and queued up for propagation.


    To keep clause retrieval time low, a variable only occurs once in each IEs partition
    (i.e clauses within the same IE share no common variables). This limits the number
    of clauses affected by a decision to one, thereby also limiting implications per
    IE to one, constraining the effected performance. While some strategies to increase
    this limit have been proposed [\[4\]](#page-4-22), they remain unexplored.


    Thong et al. propose a concurrent BCP coprocessor comprising multiple sequential
    processing engines (PE) [\[19\]](#page-4-10). Identifying that Davis et al.''s
    clause lookup is slower than direct access [\[18\]](#page-4-9), they develop a
    clause storage and encoding scheme that efficiently links clauses with shared
    variables. The processor sends decisions to the FPGA and starts BCP execution
    at a single PE. Using the linked list, the PE traverses every clause containing
    the decision variable and calculates implications, which are then added to a local
    queue and propagated. The running PE triggers BCP execution in another PE when
    it arrives at a link to a clause that is located elsewhere. The coprocessor supports
    multithreaded software execution, hiding communication and software latency by
    keeping the coprocessor busy while software threads make decisions when possible.


    Davis et al. and Thong et al. have laid a strong foundation in developing application-specific
    FPGA-based BCP coprocessors; we extend their work and propose a solution that
    processes clauses in parallel without the need for clause lookup.


    Accelerating Boolean Constraint Propagation for Efficient SAT-Solving on FPGAs
    GLSVLSI ''24, June 12–14, 2024, Clearwater, FL, USA


    #### <span id="page-2-0"></span>4 THE SAT SOLVER ARCHITECTURE


    We present a BCP coprocessor that works alongside vanilla DPLL (and should, in
    theory, work seamlessly with any DPLL-based solver). Like Thong et al., we forgo
    clause lookup and allow clauses to share variables within the same partition.
    However, we still achieve Davis et al.''s high degree of parallelism by placing
    clauses directly in clause processors (explained in Section [4.1\)](#page-2-1).


    SAT instances larger than the available number of Clause Processors (CPs) are
    partitioned, stored in external memory (i.e., software) and hot-swapped into the
    BCP coprocessors as required during runtime. Solvable instance size is limited
    only by the GPP''s RAM, not on-chip FPGA memory. We deploy our solution on the
    Zynq chip, and available here[1](#page-2-2) for use. To our knowledge, this is
    the first open-source hardware-accelerated SAT solver.


    #### <span id="page-2-1"></span>4.1 The BCP accelerator architecture


    Figure [1](#page-1-1) illustrates our approach, comprising a GPP and an FPGA accelerated
    BCP coprocessor. The GPP executes DPLL''s remaining elements (decisions, backtrack,
    etc.), partitions large SAT instances (explained in Section [4.2\)](#page-2-3)
    and swaps partitions into hardware as required. Its default state is idle, awaiting
    instructions to execute. Once a decision is received, the systems loops until
    all unit clauses are exhausted. The BCP coprocessor, depicted in Figure [1,](#page-1-1)
    comprises a control unit (1), an array of clause processors (2) and an implication
    selector (3). The central control unit communicates directly with the GPP and
    each CP. Based on the received GPP command, it loads clauses into CPs, broadcasts
    decisions, or clears assignments during backtrack. At its core, the BCP coprocessor
    consists of an array of CPs that calculate decision and implication results in
    parallel. CPs store clauses as an array of literals maintain a local copy of each
    literal''s respective variable assignment. Partitions are hot-swapped into FPGA
    by overwriting a CPs array of literals with the literals of the new clause. Variable
    assignments are updated during decisions and BCP, and cleared during backtrack.
    Finally, the implication selector chooses a single implication to propagate when
    multiple implications arise as a result of BCP. Rather than using an explicit
    implication conflict detector, as done by Davis et al [\[5\]](#page-4-8), we propagate
    the chosen implication, and identify conflicts during evaluation.


    #### <span id="page-2-3"></span>4.2 Formulae partitioning


    SAT instances contain an arbitrary number of variables and clauses. The problem
    size solvable on FPGA is limited by its available Configurable Logic Block (CLB)
    and memory, and requires large problems be partitioned into smaller manageable
    sizes. Partitions are stored in the GPP, and swapped into FPGA during run time
    by overwriting CPs clauses. BCP is performed individually on each partition, and
    implications are relayed back to the GPP. Implications are subsequently propagated
    to other partitions. We aim to make partitions as large as possible, limited by
    the coprocessor''s clause and variable threshold. Consider Equation [1,](#page-2-4)
    composed of four clauses, and an instance of our coprocessor that supports two
    clauses and three variables. Equation [2](#page-2-5) and [3](#page-2-6) outline
    the two possible ways to partition Equation [1.](#page-2-4) Equation [2](#page-2-5)
    describes a scenario where the partitions


    <span id="page-2-7"></span>![](_page_2_Figure_10.jpeg)


    Figure 2: (a) Davis et al. store the formula directly on FPGA. Clauses within
    partitions contain no shared variables, and partitions are mapped directly to
    Implication Engines. (b) Thong et al. store formula directly on FPGA. Clauses
    are linked to other clauses with shared variables and are processed sequentially.
    (c) Formula stored in external memory ("software" view). Clauses in partitions
    mapped directly to Clause Processors, and hot-swapped as required.


    reach the clause limit, while the Equation [3](#page-2-6) reaches the variable
    limit.


    <span id="page-2-4"></span>

    $$f = (\neg a \lor b \lor \neg c) \land (a \lor \neg b \lor \neg c) \land (\neg
    d \lor e \lor f) \land (d \lor e \lor f) \tag{1}$$


    <span id="page-2-5"></span>

    $$\begin{aligned} \{ \begin{aligned} \{ \begin{aligned} \{ \begin{array}{l} (\neg
    a \lor b \lor \neg c) \land (a \lor \neg b \lor \neg c) \end{array} \} \end{aligned}
    \} \end{aligned} \} $$


    <span id="page-2-6"></span>

    $$

    \begin{aligned}

    \{\neg variation\\_2 &= \{\{ (\neg a \lor b \lor \neg c) \}, \{ (a \lor \neg b
    \lor \neg c) \} \}, \\

    \{ (\neg d \lor e \lor f) \}, &\{ (d \lor e \lor f) \} \end{aligned}

    \begin{aligned}

    \{ \neg variation\\_2 \}, \end{aligned}

    $$


    Results (refer to Section [5\)](#page-3-0) indicate that partitioning is a bottleneck
    in our approach. Performance improvement is dictated by the amount of required
    partition swapping and the number of unused CPs (occurs when the number of clauses
    in a partition is less than the available number of CPs). Thus, performance improvement
    is observed with certain partition assignments, while others lead to performance
    degradation. System performance can be improved by developing a more effective
    partitioning algorithm, but beyond the scope of this paper and reserved for future
    work.


    #### 4.3 Execution


    Each clause processor is only associated with a single clause; thus, no clause
    look-up or traversal is required to retrieve the affected clause for processing.
    All clauses on the FPGA are processed in parallel as soon as a decision is received.
    Consider Equation [1''](#page-2-4)s


    <span id="page-2-2"></span><sup>1</sup>[https://github.com/harigovind1998/FPGA\\_BCP\\_acceleration](https://github.com/harigovind1998/FPGA_BCP_acceleration)


    GLSVLSI ''24, June 12–14, 2024, Clearwater, FL, USA Govindasamy, Esfandiari, and
    Garcia


    <span id="page-3-1"></span>


    | Step                 |      | 0    | 1                              | 2                       |
    3                   | 4    |

    |----------------------|------|------|--------------------------------|-------------------------|---------------------|------|

    | Our<br>Approach      | CP   | Rx   | Process<br>Decision  Clause 1  | Done                    |                     |      |

    |                      | CP 2 | Rx   | Process<br>Decision  Clause 2  | Done                    |                     |      |

    | Davis et<br>al.      | IE 1 | Rx   | Retrieve<br>Decision Clause 1  | Process<br>Clause
    1     | Done                |      |

    |                      | IE 2 | Rx   | Retrieve<br>Decision  Clause 2 | Process<br>Clause
    2     | Done                |      |

    | Thong et PE 1<br>al. |      | Rx   | Process<br>Decision  Clause 1  | Traverse
    to<br>Clause 2 | Process<br>Clause 2 | Done |

    |                      | PE 2 | ldle | ldle                           | ldle                    |
    ldle                | ldle |


    Figure 3: Execution steps of each described approach.


    mapping of partitions to hardware as presented in Figure [2.](#page-2-7) Figure
    [3](#page-3-1) summarizes the execution stages for Davis et al.''s, Thong et al.''s
    and our approach for the theoretical execution for a decision of variable . In
    our approach, clauses 1 and 2 are processed by Clause Processor 1 and 2 in parallel
    once the decision is received. Since clauses 3 and 4 do not contain variable ,
    Partition 2 remains in external memory and is not processed. Though Davis et al.
    also process clause 1 and 2 in parallel, each Implication Engine first performs
    a clause look-up to retrieve the affected clause. Results of the decision on the
    affected clause are then calculated. Thong et al.''s approach starts BCP on Processing
    Engine 1. After clause 1 is processed Processing Engine 1 traverses to clause
    2. In the manner, clauses in a partition are processed sequentially. Execution
    concludes after computing Partitions 1''s final element, clause 2. Processing
    Engine 2 remains idle for the entire duration as clauses in partition 2 do not
    contain variable .


    #### 4.4 Processor-FPGA interface


    The BCP coprocessor implements the Advanced eXtensible Interface 4-Lite (AXI-Lite)
    IP interface, acting as a subordinate to a processor (AXI master). Using AXI,
    the processor writes directly to the coprocessor''s registers to send instructions
    and data, and continues polling for status updates and new implication until the
    coprocessor completes.


    Status changes dictate DPLL''s flow, either allowing the search to continue assigning
    additional variables, or triggers backtracking on conflicts. A copy of all the
    implications are saved on the processor to avoid re-assigning implied variables,
    and further propagated to the remaining partitions.


    #### <span id="page-3-0"></span>5 EXPERIMENTS AND RESULTS


    On a Xilinx Zynq chip with total capacity of 14400 LUTs and 28800 FF, our solution
    supports 224 parallel Clause Processors and 63 variables. We achieve a clock frequency
    of 106.66 MHz, utilizing 647 LUTRAM of on-chip memory, 13151 LUTs, and 11059 FFs.


    Related work calculates throughput (in BPCs performed per second), assuming full
    data availability: i.e., not taking into account software execution and communication/data
    transfer latency. Whilst this is a useful metric to assess hardware performance
    in isolation (and we report equivalent results in Table [1\)](#page-3-2), it does
    not accurately depict system performance; to do so, we break down


    <span id="page-3-2"></span>


    |               | Millions of BCP/s |                     |            |  |  |

    |---------------|-------------------|---------------------|------------|--|--|

    | SAT Instance  | Davis et al [5]   | Thong et al<br>[19] | Our Design |  |  |

    | bmc-galileo-8 | 40                | 102                 | 175        |  |  |

    | bmc-ibm-12    | 33                | 150                 | 169        |  |  |


    Table 1: Comparison of BCP engine throughput (BCPs/s) with related work. Results
    reflect maximum theoretical throughput, achieved only data is fully available
    to BCP engines.


    <span id="page-3-3"></span>![](_page_3_Figure_13.jpeg)


    Figure 4: Breakdown of the total execution time across constituent components.


    <span id="page-3-4"></span>


    |         |       | Variables  |           |           |           |  |  |  |

    |---------|-------|------------|-----------|-----------|-----------|--|--|--|

    |         |       | 63         | 126       | 252       | 630       |  |  |  |

    | Clauses | 224   | 362M BCP/s | 17K BCP/s |           | NA        |  |  |  |

    |         |       | 2.2x       | 0.17x     | NA        |           |  |  |  |

    |         | 448   | 702K BCP/s | 21K BCP/s | 13K BCP/s | NA        |  |  |  |

    |         |       | 1.6x       | 0.21x     | 0.08x     |           |  |  |  |

    |         | 2240  | 441K BCP/s | 22K BCP/s | 16K BCP/s | 12K BCP/s |  |  |  |

    |         |       | 1.91x      | 1.26x     | 0.61x     | 0.10x     |  |  |  |

    |         | 22400 | 313K BCP/s | 20K BCP/s | 16K BCP/s | 14K BCP/s |  |  |  |

    |         |       | 6.32x      | 5.04x     | 4.86x     | 3.31x     |  |  |  |


    Table 2: Varied clause/variable sizes and their impact on the relative speedup
    of hardware/software and the effective throughput of BCP engines.


    the full execution in Figure [4](#page-3-3) and evaluate speedup over vanilla
    software implementation, evaluating combinations of clause and variable sizes,
    with speedup depicted in Table [2](#page-3-4) for meaningful combinations. For
    each combination, we also depict real throughput, in the form of BCPs/s averaged
    over total execution time (63 variables and 224 clauses is the theoretical upper
    bound, without the need for hot swapping). To evaluate the different effects of
    clause/variable sizes on execution, we fix one and vary the other, measuring total
    execution time: results are depicted in Figures [5](#page-4-23) and [6.](#page-4-24)


    <span id="page-4-0"></span>Accelerating Boolean Constraint Propagation for Efficient
    SAT-Solving on FPGAs GLSVLSI ''24, June 12–14, 2024, Clearwater, FL, USA


    <span id="page-4-23"></span>![](_page_4_Figure_1.jpeg)


    ![](_page_4_Figure_2.jpeg)


    <span id="page-4-24"></span>![](_page_4_Figure_3.jpeg)


    Figure 6: Effect of increasing variables size on total execution time, for 22400
    clauses.


    #### <span id="page-4-5"></span>6 CONCLUSIONS


    We described a SAT-solver hardware-accelerated architecture that outperforms state
    of the art by hot-swapping clause assignment at runtime, making efficient use
    of FPGA resources. Our solution prototype, on a Xilinx Zynq chip, is available
    in open-source. Practitioners may use the presented solution in their designs,
    whenever a problem is encoded in SAT form and performance is critical.


    An important open question remains: our performance is constrained by how clauses
    are partitioned. A partitioning scheme that minimizes the distribution of variables
    among clauses will minimize runtime swapping, resulting in improved execution.
    However, how to best partition a formula to achieve this is not yet known. Future
    work must formulate this challenge as an optimization problem, and methods for
    its efficient solution must be devised. Once that is achieved, they can be applied
    (offline) prior to deployment on our architecture.


    #### ACKNOWLEDGMENTS


    We acknowledge the support of the Natural Sciences and Engineering Research Council
    of Canada (NSERC).


    #### REFERENCES


    - <span id="page-4-4"></span>[1] Rabie Ben Atitallah and Karim MA Ali. 2017. FPGA-Centric
    High Performance Embedded Computing: Challenges and Trends. In 2017 Euromicro
    Conference on Digital System Design (DSD). IEEE, 390–395.

    - <span id="page-4-2"></span>[2] Stephen A Cook. 2023. The complexity of theorem-proving
    procedures. In Logic, Automata, and Computational Complexity: The Works of Stephen
    A. Cook. 143–152.

    - <span id="page-4-21"></span>[3] Andreas Dandalis and Viktor K Prasanna. 2002.
    Run-time performance optimization of an FPGA-based deduction engine for SAT solvers.
    ACM Transactions on Design Automation of Electronic Systems (TODAES) 7, 4 (2002),
    547–562.

    - <span id="page-4-22"></span>[4] John D Davis, Zhangxi Tan, Fang Yu, and Lintao
    Zhang. 2008. Designing an efficient hardware implication accelerator for SAT solving.
    In International Conference on Theory and Applications of Satisfiability Testing.
    Springer, 48–62.

    - <span id="page-4-8"></span>[5] John D. Davis, Zhangxi Tan, Fang Yu, and Lintao
    Zhang. 2008. A practical reconfigurable hardware accelerator for boolean satisfiability
    solvers. In 2008 45th ACM/IEEE Design Automation Conference. 780–785. [https://doi.org/10.1145/](https://doi.org/10.1145/1391469.1391669)
    [1391469.1391669](https://doi.org/10.1145/1391469.1391669)

    - <span id="page-4-19"></span>[6] Amir H Farrahi and Majid Sarrafzadeh. 1994.
    FPGA technology mapping for power minimization. In International Workshop on Field
    Programmable Logic and Applications. Springer, 66–77.

    - <span id="page-4-1"></span>[7] Michael R. Garey and David S. Johnson. 1990.
    Computers and Intractability; A Guide to the Theory of NP-Completeness. W. H.
    Freeman & Co., USA.

    - <span id="page-4-14"></span>[8] Weiwei Gong and Xu Zhou. 2017. A survey of SAT
    solver. In AIP Conference Proceedings, Vol. 1836. AIP Publishing.

    - <span id="page-4-16"></span>[9] Youssef Hamadi, Said Jabbour, and Lakhdar Sais.
    2010. ManySAT: a parallel SAT solver. Journal on Satisfiability, Boolean Modeling
    and Computation 6, 4 (2010), 245–262.

    - <span id="page-4-13"></span>[10] Anping He, Lvying Yu, Haitao Zhang, Lian Li,
    and Jinzhao Wu. 2018. A FPGA Based SAT Solver with High Random and Concurrent
    Strategies. In 2018 IEEE International Conference on Software Quality, Reliability
    and Security Companion (QRS-C). 221–228.<https://doi.org/10.1109/QRS-C.2018.00049>

    - <span id="page-4-11"></span>[11] Teodor Ivan and El Mostapha Aboulhamid. 2013.
    An Efficient Hardware Implementation of a SAT Problem Solver on FPGA. In 2013
    Euromicro Conference on Digital System Design. 209–216.<https://doi.org/10.1109/DSD.2013.31>

    - <span id="page-4-12"></span>[12] Teodor Ivan and El Mostapha Aboulhamid. 2013.
    Exploring limits of parallelism in FPGA-based Boolean satisfiability. In 2013
    2nd Mediterranean Conference on Embedded Computing (MECO). 62–65.<https://doi.org/10.1109/MECO.2013.6601319>

    - <span id="page-4-15"></span>[13] Ruben Martins, Vasco Manquinho, and Inês Lynce.
    2012. An overview of parallel SAT solving. Constraints 17 (2012), 304–347.

    - <span id="page-4-17"></span>[14] Muhammad Osama, Anton Wijs, and Armin Biere.
    2021. SAT solving with GPU accelerated inprocessing. In International Conference
    on Tools and Algorithms for the Construction and Analysis of Systems. Springer,
    133–151.

    - <span id="page-4-6"></span>[15] I. Skliarova and A.B. Ferrari. 2004. A software/reconfigurable
    hardware SAT solver. IEEE Transactions on Very Large Scale Integration (VLSI)
    Systems 12, 4 (2004), 408–419.<https://doi.org/10.1109/TVLSI.2004.825859>

    - <span id="page-4-7"></span>[16] Ali Asgar Sohanghpurwala, Mohamed W. Hassan,
    and Peter Athanas. 2017. Hardware accelerated SAT solvers: A survey. J. Parallel
    and Distrib. Comput. 106 (2017), 170–184.<https://doi.org/10.1016/j.jpdc.2016.12.014>

    - <span id="page-4-3"></span>[17] Robert Stewart, Bernard Berthomieu, Paulo Garcia,
    Idris Ibrahim, Greg Michaelson, and Andrew Wallace. 2019. Verifying parallel dataflow
    transformations with model checking and its application to FPGAs. Journal of Systems
    Architecture 101 (2019), 101657.

    - <span id="page-4-9"></span>[18] Jason Thong and Nicola Nicolici. 2013. FPGA
    acceleration of enhanced boolean constraint propagation for SAT solvers. In 2013
    IEEE/ACM International Conference on Computer-Aided Design (ICCAD). 234–241. [https://doi.org/10.1109/](https://doi.org/10.1109/ICCAD.2013.6691124)
    [ICCAD.2013.6691124](https://doi.org/10.1109/ICCAD.2013.6691124)

    - <span id="page-4-10"></span>[19] Jason Thong and Nicola Nicolici. 2015. SAT
    solving using FPGA-based heterogeneous computing. In 2015 IEEE/ACM International
    Conference on Computer-Aided Design (ICCAD). 232–239.<https://doi.org/10.1109/ICCAD.2015.7372575>

    - <span id="page-4-18"></span>[20] Haoze Wu. 2017. Improving SAT-solving with
    machine learning. In Proceedings of the 2017 ACM SIGCSE Technical Symposium on
    Computer Science Education. 787–788.

    - <span id="page-4-20"></span>[21] Peixin Zhong, Margaret Martonosi, and Pranav
    Ashar. 2000. FPGA-based SAT solver architecture with near-zero synthesis and layout
    overhead. IEE Proceedings-Computers and Digital Techniques 147, 3 (2000), 135–141.'
  decisions:
    evaluation_prompt: 'Qualified. Reason: The paper includes evaluation results showing
      speed-ups and comparisons with state-of-the-art solutions.'
    related_work_prompt: 'Qualified. Reason: The paper includes a section that describes
      historical solutions and state of the art, and it compares its contributions
      against prior work.'
    novelty_prompt: 'Qualified. Reason: The paper presents a novel architecture for
      hardware-accelerated SAT-solving, specifically targeting FPGA platforms, and
      claims improvements over state-of-the-art solutions, indicating novelty.'
    review_only_prompt: 'Qualified. Reason: The paper introduces a novel architecture
      for hardware-accelerated SAT-solving and presents new contributions, rather
      than summarizing existing work.'
- title: "DeMM: A Decoupled Matrix Multiplication Engine Supporting Relaxed\n  Structured\
    \ Sparsity"
  abstract: 'Deep Learning (DL) has achieved unprecedented success in various application

    domains. Meanwhile, model pruning has emerged as a viable solution to reduce

    the footprint of DL models in mobile applications, without compromising their

    accuracy. To enable the matrix engines built for dense DL models to also handle

    their pruned counterparts, pruned DL models follow a fine-grained structured

    sparsity pattern of 1:4, or 2:4, whereby in each group of four contiguous

    values, at least one, or two, respectively, must be non-zero. Structured

    sparsity has recently also moved to coarser (relaxed) cases of N:128, or N:256,

    for small values of N, targeting a wider range of sparsity (10%-90%) for the DL

    models. In this work, we design an accelerator that operates, by construction,

    on wide blocks with relaxed structured sparsity. In contrast to the

    conventional systolic array archetype, the new engine decouples the memory part

    of the systolic array from the multiply-add units. The memory block comprises
    1

    write and N read ports, with the number of read ports being equal to the number

    of non-zero elements per row. The multiply-add units connect directly to each

    read port and complete the multiplication in a row-wise product-first order.

    More importantly, simple reconfiguration facilitates more dense patterns. The

    experimental evaluation demonstrates substantial latency improvements over

    current state-of-the-art systolic array engines built for fine-grained and

    relaxed structured sparsity.'
  url: http://arxiv.org/abs/2401.08179v1
  keywords: Structured sparsity, Matrix-multiplication engine, Machine learning accelerator,
    Systolic computation
  document: '## I. INTRODUCTION


    The acceleration of DL models, for both training and inference, relies primarily
    on equivalent matrix multiplications that inherently map to systolic arrays. To
    reduce memory storage and computation cost, the weights of DL models are pruned,
    thereby leading to sparse models [\[1\]](#page-3-0), [\[2\]](#page-3-1). The derived
    zero weights are not stored and the corresponding computation is skipped. When
    sparsification occurs during training, the possible accuracy loss is ameliorated
    by allowing the model to adapt to the removal of certain weights.


    The achieved sparsity can either be *unstructured* [\[3\]](#page-3-2), or *structured*
    [\[4\]](#page-3-3), [\[5\]](#page-3-4). In unstructured sparsity, there is no
    constraint on the locations of the zeros, as shown in Fig. [1\(](#page-0-0)a).
    In this case, together with the non-zero elements, multiple metadata indexes are
    also required to identify the original position of each non-zero element.


    On the contrary, in structured sparsity, there is an upper limit on the number
    of non-zero elements that may be present within a block of consecutive elements
    (other forms of structured sparsity are also possible). For instance, in Fig.
    [1\(](#page-0-0)b), for every 4 elements in each row, there is up to one non-zero
    element. Such structured sparsity simplifies both the indexing required to identify
    the position of each non-zero element inside each block, and the hardware needed
    to operate on such sparse data. This simplicity is of paramount importance to
    *lightweight* engines (the focus of this work) found in mobile and embedded applications.


    Christodoulos Peltekis, Vasileios Titopoulos and Giorgos Dimitrakopoulos are with
    the Department of Electrical and Computer Engineering, Democritus University of
    Thrace, Xanthi, Greece, (e-mail: cpeltekis@ee.duth.gr, vtitopou@ee.duth.gr, dimitrak@ee.duth.gr)


    Chrysostomos Nicopoulos is with the Department of Electrical and Computer Engineering
    at the University of Cyprus, Nicosia, Cyprus (e-mail: nicopoulos@ucy.ac.cy).


    ![](_page_0_Figure_13.jpeg)


    <span id="page-0-0"></span>Fig. 1. Examples of (a) unstructured sparsity; (b)
    structured block sparsity of 1:4 (i.e., up to 1 non-zero element in every 4 consecutive
    elements); and (c) relaxed structured sparsity 4:16, and the corresponding packed
    representation of the non-zero elements. A blue square indicates a non-zero element.


    In most practical applications [\[4\]](#page-3-3), [\[6\]](#page-3-5), [\[7\]](#page-3-6),
    blocks are small and *fine-grained* N : M sparsity patterns of 1:2, 1:4 or 2:4
    are supported, where each block of M elements may contain up to N non-zero elements.
    Nevertheless, while fine-grained structured sparsity promises high performance
    and low storage overhead, it may also lead to less accurate ML models [\[2\]](#page-3-1),
    [\[8\]](#page-3-7). This possible weakness is attributed to the constraints imposed
    during the fine-grained sparsification, where a fixed amount of non-zero elements
    is required for all consecutive small blocks.


    To increase the flexibility during model training, sparsity could refer to much
    coarser blocks [\[9\]](#page-3-8), [\[10\]](#page-3-9). For instance, a *relaxed*
    (coarser) structured sparsity of 8:128 allows the presence of at most 8 non-zero
    elements in every 128 consecutive elements. Fig. [1\(](#page-0-0)c) shows an example
    of 4:16 relaxed structured sparsity, together with the packed representation of
    each row, which contains the non-zero element values and their corresponding column
    indexes. Moving to coarser blocks complicates the operation of the corresponding
    hardware modules – e.g., systolic arrays – that operate optimally on well-structured
    data with small block sizes.


    To effectively reconcile these two conflicting attributes of relaxed (coarser)
    sparsity vs. hardware complexity, this work proposes a novel matrix-multiplication
    engine that supports *relaxed* structured sparsity patterns, while still employing
    a simple and *decoupled* hardware organization. Unlike conventional systolic arrays
    that colocate the Multiply-Accumulate (MAC) and storage units within each tile,
    the proposed Decoupled Matrix-Multiplication (DeMM) engine decouples the two.
    It essentially re-organizes the (dispersed) memory portion of a traditional systolic
    array into a regular standardcell memory structure with multiple read ports. This
    transformation enables the support of relaxed structured sparsity and maintains
    the required regularities in the data flow and the physical layout.


    Overall, the proposed DeMM engine provides a two-fold benefit.


    This work was supported by a research grant from Codasip, a provider of customizable
    RISC-V IP and Codasip Studio design toolset, to DUTh.


    ![](_page_1_Figure_1.jpeg)


    <span id="page-1-0"></span>Fig. 2. Read and multiply operations are sufficient
    to perform matrix multiplication when the sparse matrix contains at most one non-zero
    element per row.


    First, it enables support for relaxed structured sparsity patterns that combine
    hardware simplicity (similar to handling fine-grained sparsity) with additional
    flexibility during DL model pruning [\[2\]](#page-3-1), [\[8\]](#page-3-7). Secondly,
    through appropriate reconfiguration, the DeMM engine can also support denser sparsity,
    which allows for the tackling of more common fine-grained structured sparsity
    patterns [\[4\]](#page-3-3).


    The experimental results demonstrate substantial improvements in overall execution
    latency over state-of-the-art matrix engines built to support fine-grained [\[7\]](#page-3-6)
    and relaxed structured sparsity [\[9\]](#page-3-8), [\[11\]](#page-3-10) when
    executing structured-sparse CNN models. It should be noted that, even though said
    approaches – including the proposed DeMM engine – are effective for the low-sparsity
    levels of DNNs (i.e., 10%-90%), they are not as efficient in high-sparsity levels
    of above 95%. At such high-sparsity levels, other accelerator architectures perform
    better. Examples include architectures following a non-systolic dataflow-like
    organization [\[12\]](#page-3-11), [\[13\]](#page-3-12) and ones that optimize
    memory traffic by operating either near-memory [\[14\]](#page-3-13), [\[15\]](#page-3-14),
    or in-memory [\[16\]](#page-3-15).


    ## II. SIMPLIFYING SPARSE×DENSE MATRIX MULTIPLICATION


    The proposed DeMM engine employs a row-wise approach [\[17\]](#page-3-16) in computing
    the matrix product A × B. Matrix A follows a relaxed structured sparsity template
    and B is dense. The product of the multiplication is produced row-by-row, as follows:


    $$C[i,:] = \sum\_{k} A[i,k]B[k,:] \tag{1}$$


    All the non-zero elements in a single row of matrix A should be multiplied in
    parallel with the corresponding rows of matrix B, where the row index of matrix
    B is determined by the column index of the non-zero value in matrix A.


    ## *A. The proposed DeMM engine*


    To achieve the desired parallelism, we *decouple* the *storage* (as used in a
    systolic array) from the *multiply-add* units and treat each portion separately.
    Matrix B is assumed to be pre-loaded in the storage area of DeMM. The pre-loading
    resembles the pre-loading operation of the input- (or weight-) stationary dataflow
    applied in systolic arrays. In each cycle, another row of B is written into the
    memory block, using the one available write port. Subsequently, multiplication
    is computed row-by-row, by feeding the engine with all the non-zero elements of
    each row of stuctured-sparse matrix A.


    Let us initially assume the trivial case where each row of A consists of at most
    one non-zero element. This element is passed to the engine, together with its
    column index, one after the other. For each {value, column index} pair, multiplication
    is performed in two serial steps, as depicted in Fig. [2.](#page-1-0) First, the
    column index is treated as a row address to the memory that stores matrix B. This
    address allows us to read out all elements of the corresponding row of B. In the
    second


    ![](_page_1_Figure_12.jpeg)


    <span id="page-1-1"></span>Fig. 3. Multiplying a sparse matrix with at most two
    non-zero elements per row requires two separate memory read ports and two rows
    of multipliers. The products of each port are then independently added in parallel
    to form the final result of the output row.


    ![](_page_1_Figure_14.jpeg)


    <span id="page-1-2"></span>Fig. 4. The overall organization of a DeMM engine that
    supports relaxed structured sparsity of 4:64 using a memory block of four read
    ports and four multipliers and one add-reduction unit per output element. The
    example assumes that 64 outputs (columns) are computed in parallel.


    step, the read elements are all multiplied in parallel with the value of the non-zero
    element of A. Repeating these two steps for all rows of matrix A would complete
    the multiplication. The hardware engine required in this trivial case of a single
    non-zero element per row is just a memory block with 1 read port and a row of
    multipliers.


    To support more than one non-zero element per row, one must simply increase the
    number of read ports in the memory block and, correspondingly, the number of multipliers
    per read port. Adders are also needed for the final addition. Fig. [3](#page-1-1)
    illustrates an example of the operation of the proposed engine when operating
    with a row sparsity of two non-zero elements per row. In this case, the pairs
    of nonzero elements of each row of matrix A are sent in parallel to the multiplication
    engine, one after the other. Each non-zero element is forwarded to its dedicated
    read port. The column index of each nonzero element selects (reads) the corresponding
    row of matrix B, and the value of each non-zero element is multiplied in parallel
    with all the elements of the selected row. The two products generated at each
    read port are added to finalize the result for this output row.


    In the general case, matrix A follows an N:M row-sparsity pattern, where M is
    much larger than N, e.g., N=8 and M=128, or M=256. Hence, the proposed DeMM engine
    consists of a regular memory of N read and 1 write port. Each read port of DeMM''s
    memory block outputs one data item per column, as selected by the column index
    address (''col idx'') , which points to a row of matrix B. Fig. [4](#page-1-2)
    depicts a complete DeMM engine supporting 4:64 structured sparsity (N=4 and M=64).
    Each read port is associated with a multiplier and the products of all read ports
    are reduced to one sum. Summation at the bottom of Fig. [4](#page-1-2) is implemented
    as a pipelined multi-operand adder


    ![](_page_2_Figure_1.jpeg)


    <span id="page-2-0"></span>Fig. 5. The overall architecture of the DeMM engine
    that supports an N:M relaxed structured sparsity and can be reconfigured for all
    kN:M denser variants.


    of logarithmic depth.


    ## *B. Supporting denser structured sparsity*


    To support denser structured sparsity, e.g., kN:M, in a reconfigurable manner,
    DeMM should be able to read more than N non-zero elements from *the same* M rows
    of matrix B. Since the memory block of each DeMM engine consists of N read ports,
    it means that reading the kN non-zero elements of the same row of A requires time-sharing
    of the N read ports for k× more cycles. To enable this sharing, every read port
    is associated with a k-to-1 multiplexer. Note that, irrespective of the exact
    structured sparsity pattern supported, the memory of the DeMM engine is pre-loaded
    with the same M rows of matrix B. The value of k just determines how many times
    this block would be read before completing the computation for a row of A.


    The overall organization of the DeMM engine is depicted in Fig. [5.](#page-2-0)
    The value chosen for k reflects the reconfiguration properties of the proposed
    engine. For instance, by selecting k=4 for the engine depicted in Fig. [4,](#page-1-2)
    which operates by default on 4:64 relaxed structured sparsity (N=4 and M=64),
    it means that all denser (and more fine-grained) structured sparsities can also
    be supported by the proposed design, e.g., 4:32 (as 8:64), 4:16 (as 16:64). Moving
    to an even denser scenario, such as 4:8, implemented in DeMM as a 32:64 pattern,
    would need larger multiplexers at each read port, i.e., k=8.


    To identify the various design options, we define the DeMM(N, M, C, k) configuration
    as the one that operates on a structured-sparse matrix A of row size M (this is
    also the number of rows in matrix B) and a matrix B with C columns. Structured
    sparsity can be as relaxed as N:M, or as dense as kN:M. The corresponding hardware
    block requires N ×C multipliers, C N-to-1 reduction trees, and an M × C memory
    block of N read ports.


    ## III. EVALUATION


    The effectiveness of the proposed decoupled matrix engine is evaluated by running
    inference in state-of-the-art CNN applications [\[18\]](#page-3-17),


    [\[19\]](#page-3-18) with inputs from ImageNet. In the first set of experiments,
    we consider highly sparse CNN models derived from unstructured pruning. As reported
    in [\[2\]](#page-3-1), [\[8\]](#page-3-7), unstructured pruning can achieve higher
    model compaction with better performance, as compared to structured pruning. In
    particular, we employ ResNet50 [\[18\]](#page-3-17), pruned with RigL [\[3\]](#page-3-2)
    at 95% sparsity that roughly matches the relaxed sparsity of 8:128 targeted by
    this work. Any rows exceeding the sparsity of 8:128 are computed in multiple consecutive
    cycles. For completeness, in the second set of experiments, we also include scenarios
    with fine-grained structured sparsity of 1:2, 1:4 and 1:8, derived with Tensorflow
    for ResNet50 [\[18\]](#page-3-17) and ConvNeXt [\[19\]](#page-3-18).


    ## *A. Relaxed structured sparsity*


    For relaxed row sparsity of 8:128, we compare DeMM with three state-of-the-art
    architectures: (a) VEGETA [\[9\]](#page-3-8), which is able to support such sparsity
    degrees in a structured form of 1:16 blocks, operating with a weight-stationary
    dataflow [\[20\]](#page-3-19); (b) a version of S2TA [\[7\]](#page-3-6) configured
    to support block density 1:16 using output stationarity; and (c) SPOTS [\[11\]](#page-3-10),
    which skips groups of weights and input data that consist of only zero elements,
    following an outputstationary dataflow. Specifically, we compare DeMM(8,128,64,8)
    following an input-stationary dataflow to VEGETA-S-4-2 [\[9\]](#page-3-8), to
    S2TA-4×16×4 8×4 [\[7\]](#page-3-6), and to SPOTS [\[11\]](#page-3-10), with *all*
    designs under evaluation having *the same amount of computational resources of
    512 multiply-add units*. The evaluated VEGETA and S2TA designs have array sizes
    of 32 × 16 PEs, while SPOTS has an array size of 128 × 4 that can be reconfigured
    as four 32×4 blocks operating in parallel. The configuration selected is the one
    that offers the best performance depending on the size of the input matrices.


    The obtained results are summarized in Fig. [6,](#page-2-1) which shows the execution
    latencies of all CNN layers of ResNet50 [\[18\]](#page-3-17). DeMM''s performance
    in the first layers is not the best, but it substantially outperforms the other
    three designs in the later layers. This behavior is the combined result of DeMM''s
    engine architecture and the size of the stationary matrices in each case [\[20\]](#page-3-19).
    DeMM leads to an *overall* (across all CNN layers) latency improvement of 18%,
    54% and 67%, as compared to S2TA [\[7\]](#page-3-6), VEGETA [\[9\]](#page-3-8),
    and SPOTS [\[11\]](#page-3-10), respectively.


    ## *B. Hardware complexity*


    All four evaluated designs were implemented in SystemVerilog and synthesized using
    the Cadence digital implementation flow and a 28 nm standard-cell library. The
    designs operate on 16-bit integer quantized inputs and weights, while the accumulations
    are performed with 32 bits. A block density of 8:128 is used, which is DeMM''s
    primary target. However, the instance of DeMM that is evaluated can also support
    more fine-grained patterns, down to the equivalent of 1:2. The equivalent density
    for S2TA and VEGETA is 1:16. All designs under evaluation operate at a clock frequency
    of 500 MHz at 0.88 V.


    Fig. [7\(](#page-3-20)a) compares the hardware area of the four architectures.
    Compared to S2TA and VEGETA, the DeMM engine requires 2.7% and 10.4%, respectively,
    lower area, which is a testament to the


    ![](_page_2_Figure_18.jpeg)


    <span id="page-2-1"></span>Fig. 6. The execution latencies of all CNN layers of
    ResNet50 [\[18\]](#page-3-17), when using the proposed DeMM(8,128, 64, 8) design,
    the S2TA-4×16×4 8×4 design [\[7\]](#page-3-6), the VEGETA-S-4-2 design [\[9\]](#page-3-8),
    and the SPOTS design [\[11\]](#page-3-10). All three architectures have equal
    amount of computational resources.


    ![](_page_3_Figure_1.jpeg)


    <span id="page-3-20"></span>Fig. 7. Hardware (a) area, and (b) power consumption
    comparisons between DeMM(8,128,64,8), S2TA-4×16×4 8×4 [\[7\]](#page-3-6), VEGETA-S-4-2
    [\[9\]](#page-3-8) and SPOTS [\[11\]](#page-3-10). DeMM and SPOTS use a block
    density of 8:128; S2TA and VEGETA use the equivalent 1:16.


    simplicity of its organization. DeMM is slightly larger than SPOTS (the overhead
    is less than 10%), due to the additional multiplexing logic required for supporting
    reconfigurability and multi-porting. Each additional read port added to the 128×64
    standard-cell-based memory block used in this DeMM setup costs 16% more area.


    In terms of power consumption, DeMM demonstrates significantly better behavior
    than the other designs. As shown in Fig. [7\(](#page-3-20)b), DeMM consumes 36.4%,
    45.8% and 56.1% lower power than SPOTS, S2TA, and VEGETA, respectively. This substantial
    power reduction is mainly attributed to the minimization of data movement in pipeline
    registers. Both S2TA and VEGETA have an input data demand of a multiple-of-M inputs
    and C weights, while DeMM has a much lower input data demand of C inputs and kN
    weights, for a kN:M structured sparsity, along with their addresses. The extra
    hardware cost of VEGETA relative to S2TA stems from its reconfiguration-rich organization
    that is also offered by the DeMM engine. SPOTS has a low multiplexing overhead
    that reduces its area requirements. However, its deeply pipelined operation increases
    its power consumption relative to DeMM.


    ## *C. Fine-grained structured sparsity*


    Furthermore, we also compare DeMM to VEGETA and S2TA in use-cases that better
    fit these two architectures, i.e., in scenarios with fine-grained block densities.
    The designs are evaluated using ResNet50 [\[18\]](#page-3-17) and ConvNeXt [\[19\]](#page-3-18).
    The selected workloads are pruned to fine-grained structured block sparsities
    of 1:2, 1:4 and 1:8 to ensure optimal conditions for both VEGETA and S2TA, even
    if this choice is not the best option for DeMM, which inherently supports a wide
    range of sparsity formats. SPOTS is omitted in this comparison, since – under
    such fine-grained structured sparsity – it is very difficult to find contiguous
    groups of zero data, as required by SPOTS. Consequently, SPOTS exhibits significantly
    higher latencies.


    The results are shown in Fig. [8.](#page-3-21) DeMM engine *outperforms both*
    S2TA and VEGETA architectures in terms of overall latency. Specifically, for block
    sparsity 1:8, DeMM achieves average latency improvements of 29% and 39%, as compared
    to S2TA and VEGETA, respectively. For block sparsity 1:4, DeMM''s respective improvement
    in average latency is 19% and 12%. Finally, for block sparsity 1:2, DeMM still
    yields 14% and 5% average latency improvements, as compared to S2TA and VEGETA,
    respectively.


    ## IV. CONCLUSIONS


    This paper proposes DeMM, a matrix-multiplication engine that natively supports
    *relaxed* structured sparsity patterns, without sacrificing the simplicity and
    regularity of the hardware organization. Contrary to conventional systolic arrays,
    the DeMM design employs a disaggregated micro-architecture that decouples the
    memory elements from the MAC units. Fine-grained sparsity patterns are also supported


    ![](_page_3_Figure_10.jpeg)


    <span id="page-3-21"></span>Fig. 8. Overall execution latencies of ResNet50 [\[18\]](#page-3-17)
    and ConvNeXt [\[19\]](#page-3-18) for configurations with block densities of 1:8,
    1:4 and 1:2.


    with minimal reconfiguration. The experimental evaluation demonstrates substantial
    improvements in the overall execution latency of modern CNN workloads, as compared
    to two existing state-ofthe-art architectures [\[9\]](#page-3-8), [\[7\]](#page-3-6),
    [\[11\]](#page-3-10) for structured sparse workloads. Most importantly, these
    performance improvements are achieved with markedly lower power consumption.


    ## REFERENCES


    - <span id="page-3-0"></span>[1] S. Han *et al.*, "Deep compression: Compressing
    deep neural networks with pruning, trained quantization and huffman coding," in
    *Inter. Conf. on Learning Representations (ICLR)*, 2016.

    - <span id="page-3-1"></span>[2] T. Hoefler *et al.*, "Sparsity in deep learning:
    Pruning and growth for efficient inference and training in neural networks," *The
    Journal of Machine Learning Research*, vol. 22, no. 1, pp. 10 882–11 005, 2021.

    - <span id="page-3-2"></span>[3] U. Evci *et al.*, "Rigging the lottery: Making
    all tickets winners," in *Inter. Conf. on Machine Learning*, Jul. 2020, pp. 2943–2952.

    - <span id="page-3-3"></span>[4] A. Mishra *et al.*, "Accelerating sparse deep
    neural networks," *arXiv preprint arXiv:2104.08378*, 2021.

    - <span id="page-3-4"></span>[5] A. Zhou *et al.*, "Learning N:M fine-grained
    structured sparse neural networks from scratch," in *Inter. Conf. on Learning
    Representations (ICLR)*, May 2021.

    - <span id="page-3-5"></span>[6] Z.-G. Liu *et al.*, "Systolic tensor array: An
    efficient structured-sparse gemm accelerator for mobile CNN inference," *IEEE
    Computer Architecture Letters*, vol. 19, no. 1, pp. 34–37, 2020.

    - <span id="page-3-6"></span>[7] ——, "S2TA: Exploiting structured sparsity for
    energy-efficient mobile CNN acceleration," in *IEEE Inter. Symp. on High-Performance
    Computer Architecture (HPCA)*, Apr. 2022, pp. 573–586.

    - <span id="page-3-7"></span>[8] M. Zhu *et al.*, "Sparse tensor core: Algorithm
    and hardware co-design for vector-wise sparse neural networks on modern gpus,"
    in *IEEE/ACM Inter. Symp. on Microarchitecture (MICRO)*, Oct. 2019, pp. 359–371.

    - <span id="page-3-8"></span>[9] G. Jeong *et al.*, "Vegeta: Vertically-integrated
    extensions for sparse/dense gemm tile acceleration on cpus," in *IEEE Inter. Symp.
    on High-Performance Computer Architecture (HPCA)*, Feb. 2023, pp. 259–272.

    - <span id="page-3-9"></span>[10] S. Muralidharan, "Uniform sparsity in deep neural
    networks," *Proc. of Machine Learning and Systems*, vol. 5, 2023.

    - <span id="page-3-10"></span>[11] M. Soltaniyeh *et al.*, "An accelerator for
    sparse convolutional neural networks leveraging systolic general matrix-matrix
    multiplication," *ACM Trans. on Arch. and Code Opt. (TACO)*, vol. 19, no. 3, pp.
    1–26, 2022.

    - <span id="page-3-11"></span>[12] O. Hsu *et al.*, "The sparse abstract machine,"
    in *ACM Inter. Conf. on Architectural Support for Programming Languages and Operating
    Systems (ASPLOS)*, 2023, pp. 710–726.

    - <span id="page-3-12"></span>[13] G. Zhang *et al.*, "Gamma: Leveraging gustavson''s
    algorithm to accelerate sparse matrix multiplication," in *ACM Inter. Conf. on
    Architectural Support for Programming Languages and Operating Systems (ASPLOS)*,
    2021, pp. 687–701.

    - <span id="page-3-13"></span>[14] G. Gerogiannis *et al.*, "Spade: A flexible
    and scalable accelerator for spmm and sddmm," in *Inter. Symp. on Computer Arch.
    (ISCA)*, 2023.

    - <span id="page-3-14"></span>[15] S. Feng *et al.*, "MeNDA: a near-memory multi-way
    merge solution for sparse transposition and dataflows," in *Inter. Symp. on Comp.
    Arch. (ISCA)*, 2022, pp. 245–258.

    - <span id="page-3-15"></span>[16] Z. Li *et al.*, "RRAM-DNN: An RRAM and model-compression
    empowered all-weights-on-chip DNN accelerator," *IEEE Journal of Solid-State Circuits*,
    vol. 56, no. 4, pp. 1105–1115, 2020.

    - <span id="page-3-16"></span>[17] N. Srivastava *et al.*, "Matraptor: A sparse-sparse
    matrix multiplication accelerator based on row-wise product," in *IEEE/ACM Inter.
    Symp. on Microarchitecture (MICRO)*, Oct. 2020, pp. 766–780.

    - <span id="page-3-17"></span>[18] K. He *et al.*, "Deep residual learning for
    image recognition," in *IEEE CVPR*, 2016, pp. 770–778.

    - <span id="page-3-18"></span>[19] Z. Liu *et al.*, "A ConvNet for the 2020s,"
    in *IEEE Conf. on Comp. Vision and Pattern Recognition (CVPR)*, 2022, pp. 11 976–11
    986.

    - <span id="page-3-19"></span>[20] A. Samajdar *et al.*, "Scale-sim: Systolic
    CNN accelerator simulator," *arXiv preprint arXiv:1811.02883*, 2018.'
  decisions:
    evaluation_prompt: 'Qualified. Reason: The paper mentions "experimental results
      demonstrate substantial improvements" indicating empirical evaluation.'
    related_work_prompt: 'Qualified. Reason: The paper engages with prior research
      by citing multiple research papers, explaining and comparing to cited work,
      and includes a section discussing related concepts and advancements.'
    novelty_prompt: 'Qualified. Reason: The paper proposes a novel matrix-multiplication
      engine (DeMM) that supports relaxed structured sparsity patterns with a decoupled
      hardware organization, offering new insights and methods.'
    review_only_prompt: 'Qualified. Reason: The paper introduces a novel matrix-multiplication
      engine (DeMM) and discusses its implementation and benefits, indicating new
      contributions rather than summarizing existing work.'
- title: "Energy-adaptive Buffering for Efficient, Responsive, and Persistent\n  Batteryless\
    \ Systems"
  abstract: "Batteryless energy harvesting systems enable a wide array of new sensing,\n\
    computation, and communication platforms untethered by power delivery or\nbattery\
    \ maintenance demands. Energy harvesters charge a buffer capacitor from\nan unreliable\
    \ environmental source until enough energy is stored to guarantee a\nburst of\
    \ operation despite changes in power input. Current platforms use a\nfixed-size\
    \ buffer chosen at design time to meet constraints on charge time or\napplication\
    \ longevity, but static energy buffers are a poor fit for the highly\nvolatile\
    \ power sources found in real-world deployments: fixed buffers waste\nenergy both\
    \ as heat when they reach capacity during a power surplus and as\nleakage when\
    \ they fail to charge the system during a power deficit.\n  To maximize batteryless\
    \ system performance in the face of highly dynamic\ninput power, we propose REACT:\
    \ a responsive buffering circuit which varies\ntotal capacitance according to\
    \ net input power. REACT uses a variable capacitor\nbank to expand capacitance\
    \ to capture incoming energy during a power surplus\nand reconfigures internal\
    \ capacitors to reclaim additional energy from each\ncapacitor as power input\
    \ falls. Compared to fixed-capacity systems, REACT\ncaptures more energy, maximizes\
    \ usable energy, and efficiently decouples system\nvoltage from stored charge\
    \ -- enabling low-power and high-performance designs\npreviously limited by ambient\
    \ power. Our evaluation on real-world platforms\nshows that REACT eliminates the\
    \ tradeoff between responsiveness, efficiency,\nand longevity, increasing the\
    \ energy available for useful work by an average\n25.6% over static buffers optimized\
    \ for reactivity and capacity, improving\nevent responsiveness by an average 7.7x\
    \ without sacrificing capacity, and\nenabling programmer directed longevity guarantees."
  url: http://arxiv.org/abs/2401.08806v1
  keywords: ''
  document: '# Energy-adaptive Buffering for Efficient, Responsive, and Persistent
    Batteryless Systems


    Harrison Williams Virginia Tech hrwill@vt.edu


    Matthew Hicks Virginia Tech mdhicks2@vt.edu


    ## Abstract


    Batteryless energy harvesting systems enable a wide array of new sensing, computation,
    and communication platforms untethered by power delivery or battery maintenance
    demands. Energy harvesters charge a buffer capacitor from an unreliable environmental
    source until enough energy is stored to guarantee a burst of operation despite
    changes in power input. Current platforms use a fixed-size buffer chosen at design
    time to meet constraints on charge time or application longevity, but static energy
    buffers are a poor fit for the highly volatile power sources found in real-world
    deployments: fixed buffers waste energy both as heat when they reach capacity
    during a power surplus and as leakage when they fail to charge the system during
    a power deficit.


    To maximize batteryless system performance in the face of highly dynamic input
    power, we propose REACT: a responsive buffering circuit which varies total capacitance
    according to net input power. REACT uses a variable capacitor bank to expand capacitance
    to capture incoming energy during a power surplus and reconfigures internal capacitors
    to reclaim additional energy from each capacitor as power input falls. Compared
    to fixed-capacity systems, REACT captures more energy, maximizes usable energy,
    and efficiently decouples system voltage from stored charge—enabling low-power
    and high-performance designs previously limited by ambient power. Our evaluation
    on real-world platforms shows that REACT eliminates the tradeoff between responsiveness,
    efficiency, and longevity, increasing the energy available for useful work by
    an average 25.6% over static buffers optimized for reactivity and capacity, improving
    event responsiveness by an average 7.7 without sacrificing capacity, and enabling
    programmer directed longevity guarantees.


    ## <span id="page-0-0"></span>1 Introduction


    Ever-shrinking computing and sensing hardware has pushed mobile Internet-of-Things
    (IoT) type devices beyond the limits of the batteries powering them. A typical
    low cost/power microcontroller [\[19\]](#page-11-0) drains a 1 <sup>3</sup> battery
    nearly 14x its size in just over 8 weeks of active operation [\[36\]](#page-12-0),
    rendering the system useless without a potentially costly replacement effort.
    Cost, maintenance, and safety concerns make batteries further incompatible with
    massive-scale (one million devices per square kilometer [\[5\]](#page-11-1)) and
    deeply-deployed (infrastructure [\[2\]](#page-11-2), healthcare [\[34\]](#page-12-1))
    applications. IoT engineers are turning to batteryless energy harvesting platforms


    to power low-cost, perpetual systems capable of driving a ubiquitous computing
    revolution. Increasingly efficient energy harvesting circuits enable batteryless
    systems across a range of IoT use cases including feature-rich batteryless temperature
    sensors 500x smaller than a grain of rice [\[47\]](#page-12-2) and batteryless
    flow-meters [\[13\]](#page-11-3) supporting deep-sea drilling or geothermal plants
    for decades without maintenance.


    The energy harvesting design model both enables new deployments previously limited
    by batteries and places new demands on system developers. Harvested energy is
    highly unreliable: sensitive environmental factors such as shadows over a photovoltaic
    cell or shifts in the orientation of a rectenna produce rapid, outsized changes
    in the energy scavenged by the harvester. Energy harvesters mitigate this unreliability
    by charging an energy buffer to a given enable voltage, which the system periodically
    discharges to supply a useful quantum of work despite potential power loss.


    Buffer capacity is a key design element of any batteryless system. Past work [\[7\]](#page-11-4)
    explores the tradeoff between buffer sizes: small buffers are highly reactive—charging
    rapidly and quickly enabling the system to address time-sensitive events—but sacrifice
    longevity because they rapidly discharge during operation, guaranteeing only a
    short burst of uninterrupted operation. Large buffers store more energy at a given
    voltage, improving longevity by supporting a longer or more energy-intensive burst
    of operation at the cost of reactivity because they require more energy to enable
    the system at all. Matching buffer size to projected energy demand is critical
    to ensuring the system is both reactive enough to address incoming events/deadlines
    (e.g., periodic sensor readings) and long-lived enough to support uninterruptible
    operations (e.g., radio transmissions). Designers choose the minimum size necessary
    to power all atomic operations on the device, maximizing reactivity given a required
    level of longevity.


    In this work, we explore static energy buffer efficiency as a third metric for
    buffer performance and find that it varies dramatically with net energy input
    rather than simple energy demand. Small buffers reach capacity quickly if power
    input exceeds instantaneous demand—burning off hard-won energy as heat to prevent
    overvoltage. Large buffers capture all incoming power, but enable slowly and lose
    more harvested energy to leakage below the minimum system voltage. The volatile
    nature of harvested power means that fixed-size buffers experience both problems
    over the course of their


    deployment, discharging energy during a power surplus and losing large portions
    of energy to leakage during a deficit.


    To make the most of incoming energy in all circumstances, we propose REACT[1](#page-1-0)
    : a dynamic energy buffering system that varies its capacitance following changes
    in net power. REACT maximizes system responsiveness and efficiency using a small
    static buffer capacitor, quickly enabling the system to monitor events or do other
    low-power work under even low input power. If input power rises beyond the current
    system demand and the static buffer approaches capacity, REACT connects additional
    capacitor banks to absorb the surplus, yielding the capacity benefits of large
    buffers without the responsiveness penalty. When net power is negative, these
    capacitors hold the system voltage up and extend operation beyond what is possible
    using the small static capacitor.


    While expanding buffer size to follow net power input ensures the system can capture
    all incoming energy, increasing capacitance also increases the amount of unusable
    charge stored on the capacitor banks—charge which could power useful work if it
    were on a smaller capacitor and therefore available at a higher voltage. As supply
    voltage falls and approaches a minimum threshold, REACT reclaims this otherwise-unavailable
    energy by reconfiguring capacitor banks into series, shifting the same amount
    of charge onto a smaller equivalent capacitance in order to boost the voltage
    at the buffer output and ensure the system continues operating for as long as
    possible. REACT effectively eliminates the design tradeoff between reactivity
    and capacity by tuning buffer size within an arbitrarily large capacitance range,
    only adding capacity when the buffer is already near full. REACT''s charge reclamation
    techniques maximize efficiency by moving charge out of large capacitor banks onto
    smaller ones when net input power is negative, ensuring all energy is available
    for useful work.


    We integrate a hardware prototype of REACT into a full energy harvesting platform
    to evaluate it against previous work, operating under different input power conditions
    and with different power consumption profiles. Our results indicate that REACT
    provides the "best of both worlds" of both smalland large-buffer systems, rapidly
    reaching the operational voltage under any power conditions while also expanding
    as necessary to capture all available energy and provide software longevity guarantees
    as needed. Maximizing buffer capacity and reclaiming charge using REACT''s reconfigurable
    capacitor banks eliminates the efficiency penalties associated with both small
    and large static capacitor buffers, increasing the portion of harvested energy
    used for application code by an average 39% over an equally-reactive static buffer
    and 19% over an equal-capacity one. Compared to prior work exploring dynamic capacitance
    for batteryless systems [\[49\]](#page-12-3), REACT improves performance by an
    average 26% owing to


    its efficient charge management structure. This paper makes the following technical
    contributions:


    - We evaluate the power dynamics of common batteryless systems in real deployments
    and explore how common-case volatility introduces significant energy waste in
    static or demand-driven buffers (§ [2\)](#page-1-1).

    - We design REACT, a dynamic buffer system which varies its capacitance according
    to system needs driven by net input power (§ [3\)](#page-3-0). REACT''s configurable
    arrays combine the responsiveness of small buffers with the longevity and capacity
    of large ones, enables energy reclamation to make the most of harvested power,
    and avoids the pitfalls of energy waste inherent in other dynamic capacitance
    designs (§ [3.3\)](#page-4-0).

    - We integrate REACT into a batteryless system and evaluate its effect on reactivity,
    longevity, and efficiency under a variety of power conditions and workloads (§
    [5\)](#page-8-0). Our evaluation indicates that REACT eliminates the responsiveness-longevity
    tradeoff inherent in static buffer design while increasing overall system efficiency
    compared to any static system.


    ## <span id="page-1-1"></span>2 Background and Related Work


    Scaling sensing, computation, and communication down to smart dust [\[23\]](#page-11-5)
    dimensions requires harvesting power ondemand rather than packaging energy with
    each device using a battery. Many batteryless systems use photovoltaic [\[9,](#page-11-6)
    [47\]](#page-12-2) or RF power [\[40\]](#page-12-4), while other use-cases are
    better suited for sources such as vibration [\[41\]](#page-12-5), fluid flow [\[13\]](#page-11-3),
    or heat gradients [\[27\]](#page-11-7). Commonalities between ambient power sources
    have inspired researchers to develop general-purpose batteryless systems, regardless
    of the actual power source: ambient power is unpredictable, dynamic, and often
    scarce relative to the active power consumption of the system.


    Batteryless systems isolate sensing and actuation components from volatile power
    using a buffer capacitor. The harvester charges the capacitor to a pre-defined
    enable voltage, after which the system turns on and begins consuming power. Because
    many environmental sources cannot consistently power continuous execution, systems
    operate intermittently—draining the capacitor in short bursts of operation punctuated
    by long recharge periods. This generalpurpose intermittent operation model has
    enabled researchers to abstract away the behavior of incoming power and focus
    on developing correct and efficient techniques for working under intermittent
    power [\[17,](#page-11-8) [26,](#page-11-9) [28,](#page-11-10) [29,](#page-11-11)
    [44,](#page-12-6) [46\]](#page-12-7).


    ## 2.1 Choosing Buffer Capacity


    Buffer size determines system behavior in several important ways. Supercapacitors
    provide inexpensive and small-formfactor bulk capacitance [\[24\]](#page-11-12),
    enabling designers to choose a capacitor according to performance rather than
    cost or size concerns. Two metrics motivate past work: reactivity refers


    <span id="page-1-0"></span><sup>1</sup>Reconfigurable, Energy-Adaptive CapaciTors


    <span id="page-2-0"></span>![](_page_2_Figure_0.jpeg)


    Figure 1. Static buffer operation on a simulated solar harvester. Highlighted
    blocks indicate the system is running.


    to the system''s ability to rapidly charge to its enable voltage and begin operation.
    High reactivity ensures a system is online to execute periodic tasks or address
    unpredictable input events. Longevity refers to the energy available for an uninterrupted
    period of work with no additional power input; long-lived systems support high-power
    and long-running uninterruptible operations and reduce the overhead incurred with
    state recovery after a power loss.


    2.1.1 Reactivity and Longevity: A batteryless system''s reactivity and longevity
    depend primarily on the charge and discharge rate of the buffer capacitor. We
    illustrate the tradeoff using a simulated solar harvester with a 22% efficient,
    5 cm<sup>2</sup> panel, based on a pedestrian trace from the EnHANTs solar dataset
    [\[12\]](#page-11-13). The system runs from 3.6V down to 1.8V and draws 1.5 mA
    in active mode, representative of a typical deployment [\[25\]](#page-11-14).
    Figure [1](#page-2-0) illustrates the reactivity-longevity tradeoff inherent in
    static buffer systems at two design extremes, using a 1 mF and 300 mF capacitor.
    The 1 mF system charges rapidly and is therefore highly reactive, reaching the
    enable voltage over 8 sooner than the 300 mF version. However, the smaller capacitor
    also discharges quickly—the mean length of an uninterrupted power cycle using
    the 1 mF capacitor is 10 seconds versus 880 seconds for the 300 mF capacitor,
    indicating the 300 mF system is far longer-lived once charged. The relative importance
    of reactivity and longevity depends on the use case, but often changes over time
    for a complex system—complicating design further.


    2.1.2 Power Volatility and Energy Efficiency: Buffer capacity is also a major
    driver of end-to-end energy efficiency: using the 300 mF capacitor our system
    is operational for 49% of the overall power trace, compared to only 27% for the
    1 mF platform. This stems from the high volatility of incoming power—82% of the
    total energy input is collected during


    short-duration power spikes when harvested power rises above 10 mW, despite the
    system spending 77% of its time at input powers below 3 mW. A large buffer captures
    this excess energy to use later while the smaller buffer quickly reaches capacity
    and discharges energy as heat to avoid overvoltage.


    Large buffers, however, are not always more efficient: the energy used to charge
    the capacitor to the operational voltage cannot power useful work, and is eventually
    lost to leakage while the system is off. When power input is low, this "cold-start"
    energy represents a significant portion of total harvested energy. For the system
    described above powered by a solar panel at night [\[12\]](#page-11-13), the 1
    mF buffer enables a duty cycle of 5.7% versus only 3.3% using a 10 mF buffer.
    This low power environment highlights another risk of oversized buffers: the system
    using the 300 mF capacitor never reaches the enable voltage and so never begins
    operation.


    Improvements in harvester efficiency and low-power chip design are closing the
    gap between harvester output and chip power consumption. Power is increasingly
    limited by volatile environmental factors rather than scarcity induced by low
    efficiency; the result is that energy harvesters experience periods of both energy
    scarcity and surplus. Rapidly changing power conditions place opposing demands
    on batteryless systems, which must remain responsive with low input power, provide
    longevity for long-running operations, and maximize efficiency by avoiding energy
    waste.


    ## 2.2 Power-Responsive Performance Scaling


    One solution to volatile energy input is modulating throughput according to incoming
    power, increasing execution rate when power is plentiful and decreasing it to
    maintain availability when power is scarce. Limiting net input power to the buffer
    by matching power consumption with input enables systems to use small buffer capacitors
    without reaching the buffer capacity, ensuring no power is wasted with an over-full
    buffer. Past work realizes power-responsive scaling using heterogeneous architectures
    [\[10\]](#page-11-15) or by adapting the rate and accuracy of software execution
    [\[1,](#page-11-16) [4,](#page-11-17) [30,](#page-12-8) [48\]](#page-12-9).


    Unfortunately, we find the assumptions underlying powerperformance scaling often
    do not apply to batteryless systems. Increasing energy consumption by accelerating
    execution only improves systems which have useful work to do exactly when input
    power is high, but many batteryless systems focus on periodic sensing and actuation
    deadlines which do not correlate with ambient power supply. Further, resource-constrained
    platforms may have few on-chip operations which can be delayed until power is
    plentiful; when these operations do exist, they are often not amenable to scaling
    (e.g., transmitting data to a base station may be delayed but always requires
    a fixed-cost radio operation). Flexible batteryless systems must capture energy
    and use it on demand rather than fit operation to unreliable power input.


    ## 2.3 Multiplexed Energy Storage


    Rather than match power consumption to incoming supply, systems may charge multiple
    static buffers according to projected demand. Capybara [\[7\]](#page-11-4) switches
    capacitance using an array of heterogeneous buffers: programmers set capacitance
    modes throughout the program, using a smaller capacitor to maximize responsiveness
    for low-power or interruptible tasks and switching to a larger capacitor for high-power
    atomic operations. UFoP and Flicker [\[15,](#page-11-18) [16\]](#page-11-19) assign
    each peripheral on the system a separate buffer and charging priority, enabling
    responsive low-power operation while waiting to collect sufficient energy for
    high-power tasks. These systems increase overall energy capacity by directing
    excess power to capacitors not currently in use.


    Static arrays increase capacity without reducing responsiveness, but waste energy
    when charge is stored on unused buffers. Reserving energy in secondary capacitors
    1) requires error-prone [\[30\]](#page-12-8) speculation about future energy supply
    and demand to decide charging priorities, which can change between when energy
    is harvested and when it needs to be used; and 2), wastes energy as leakage when
    secondary buffers are only partially charged, failing to enable associated systems
    and keeping energy from higher-priority work. To minimize programmer speculation,
    decouple tasks which compete for buffered energy, and minimize leakage, energy
    must be fungible: the buffer must be capable of directing all harvested energy
    to any part of the system on demand.


    ## 2.4 Unified Dynamic Buffering


    Past work has also explored varying the behavior of a single unified buffer to
    capture the fungibility requirement described above. Dewdrop [\[6\]](#page-11-20)
    varies the enable voltage to draw from a single capacitor according to projected
    needs (e.g., begin operation at 2.2V instead of 3.6V)—providing complete energy
    fungibility—but still suffers from the reactivitylongevity tradeoff of capacitor
    size. Morphy [\[49\]](#page-12-3) replaces static buffers using a set of capacitors
    in a unified switching network; software can connect and disconnect arbitrary
    sets of capacitors in series or parallel to produce different equivalent capacitances.
    Morphy addresses two challenges facing batteryless systems: fast charging from
    low-voltage sources by operating the capacitor network as a charge pump, and charge
    isolation to allocate set amounts of energy to tasks and prevent buggy code from
    discharging the entire buffer by isolating parts of the capacitor array.


    We evaluate REACT alongside Morphy because the Morphy architecture can also be
    used to target the reactivity and longevity challenges discussed in § [1.](#page-0-0)
    By starting with a small equivalent capacitance, the system rapidly reaches an
    operational voltage. If the buffer as configured reaches capacity, software can
    gradually reconfigure the capacitor network to increase capacitance. Charge flows
    between the capacitors to equalize the voltage on the capacitor array and reduce


    <span id="page-3-1"></span>![](_page_3_Figure_6.jpeg)


    Figure 2. REACT diagram and signal flow between components.


    the output voltage of the network, enabling the bank to harvest more energy without
    reaching capacity. However, this current flow between capacitors to equalize voltage
    during reconfiguration dissipates a significant amount of the energy stored in
    the network (we explore these energy dynamics in detail in § [3.3.1\)](#page-4-1).
    Our evaluation in § [5](#page-8-0) shows that this internal power dissipation
    reduces common-case end-to-end performance to below that of systems using appropriatelysized
    static capacitors, making this approach impractical for energy-constrained devices.
    An energy-focused approach prioritizing minimal power dissipation is key to developing
    intermittent systems that can simultaneously maximize reactivity, longevity, and
    overall efficiency.


    ## <span id="page-3-0"></span>3 Design


    An intelligent energy buffering strategy is key to effective and efficient batteryless
    systems. Three performance objectives, informed by the advantages and limitations
    of prior approaches, drive REACT''s design:


    - Minimize charge time: Rapidly reaching the operational voltage, even when buffered
    energy cannot support complex operation, maximizes reactivity and enables systems
    to reason about power or sensor events from within low-power sleep modes.

    - Maximize capacity: System-wide longevity and efficiency require buffering large
    amounts of incoming energy when power supply exceeds demand, either to power long-running
    uninterruptible operations or support future power demand when supply is low.

    - Maximize energy fungibility: Unpredictable power demand patterns mean that energy
    cannot be preprovisioned to specific operations at harvest-time; systems need
    the ability to draw all harvested energy from the buffer and direct it as needed.


    ## 3.1 REACT Overview


    REACT buffers energy using a fabric of reconfigurable capacitor banks that software
    adjusts as needed. Figure [2](#page-3-1) shows a


    high-level overview of REACT''s hardware design. We design REACT''s hardware component
    as a drop-in replacement for a typical buffer between the harvester and the rest
    of the system, while the buffer management software requires no code modification
    or programmer input. The only system prerequisite is a set of digital I/O pins
    to configure capacitor banks and receive voltage monitoring information.


    ## 3.2 Cold-start Operation and the Last-level Buffer


    From a cold start (() = 0), REACT minimizes overall capacitance in order to rapidly
    charge to the operational voltage and enable the system with minimum energy input
    (high reactivity). The minimum capacitance is set by the smallest quantum of useful
    work available on the system (minimum required longevity), such as a short-lived
    software operation or an initialization routine that puts the system into a lowpower
    responsive sleep mode. REACT provides this rapid charge time using a small static
    buffer referred to hereafter as the last-level buffer. Additional capacitor banks
    are connected using normally-open switches and only contribute to overall capacitance
    when configured to do so in software, after the system is able to reason about
    buffered energy.


    The last-level buffer sets the minimum capacitance at power-on when all other
    banks are disconnected. This enables simple tuning of the energy input required
    to enable the system (reactivity) and the guaranteed energy level when the system
    does begin work (minimum longevity). It also smooths voltage fluctuations induced
    by capacitor bank switching (§ [3.3.4\)](#page-6-0). Finally, the last-level buffer
    serves as the combination point between the different capacitor banks and the
    rest of the system. Although energy may be stored in multiple banks of varying
    capacity at different voltages, combining it at the last-level buffer simplifies
    system design by presenting harvested power as a unified pool of energy which
    the system taps as needed (i.e., harvested energy is fungible).


    3.2.1 Monitoring Buffered Energy. Despite mutual isolation, bank voltages tends
    to equalize: the last-level buffer pulls energy from the highest-voltage bank
    first, and current flows from the harvester to the lowest-voltage bank first.
    This enables REACT to measure only the voltage on the lastlevel buffer as a surrogate
    for remaining energy capacity. If voltage rises beyond an upper threshold—the
    buffer is near capacity—REACT''s voltage instrumentation hardware signals the
    software component running on the microcontroller to increase capacitance using
    the configurable banks. Voltage falling below a lower threshold indicates the
    buffer is running out of energy and that REACT should reconfigure banks to extract
    additional energy and extend operation. RE-ACT''s instrumentation only needs to
    signal three discrete states—near capacity, near undervoltage, and OK—so two low-power
    comparators is sufficient for energy estimation.


    <span id="page-4-3"></span>![](_page_4_Figure_5.jpeg)


    Figure 3. REACT capacitor banks in different bank sizes and configurations. Arrows
    indicate charging current path.


    <span id="page-4-2"></span>![](_page_4_Figure_7.jpeg)


    Figure 4. Structure of the unified approach presented by Yang et al. [\[49\]](#page-12-3).
    Arrows indicate charging current path.


    ## <span id="page-4-0"></span>3.3 Dynamic Capacitor Banks


    The last-level buffer on its own enables high reactivity and minimizes cold-start
    energy below the operational minimum, maximizing efficiency during power starvation.
    However, when net power into the buffer is positive—such as during a period of
    high input power or low workload—the small last-level buffer rapidly reaches capacity.
    REACT provides the energy capacity required to both maximize efficiency and support
    long-running operation by connecting configurable capacitor banks when the last-level
    buffer reaches capacity, as shown in Figure [2.](#page-3-1)


    <span id="page-4-1"></span>3.3.1 Capacitor Organization. Careful management of
    the connections between each capacitor is key to maximizing energy efficiency
    while also presenting a valid operational voltage for the computational backend.
    Morphy [\[49\]](#page-12-3) presents one approach: by connecting a set of equally-sized
    capacitors through switches similar to a charge pump, overall buffer capacitance
    can be varied across a wide range of capacitance


    <span id="page-5-0"></span>![](_page_5_Figure_0.jpeg)


    Figure 5. Dissipative current flow in a fully-unified buffer during reconfiguration.
    Energy is dissipated by current spikes after capacitors at different voltages
    are placed in parallel.


    values. Different switch configurations produce intermediate buffer sizes between
    the extremes shown in Figure [4;](#page-4-2) gradually stepping through these
    configurations smoothly varies capacitance through software control.


    A fully interconnected array enables a wide range of equivalent, but introduces
    significant waste through dissipative heating when the charged capacitor array
    is reconfigured. Figure [5](#page-5-0) illustrates how energy is lost when charged
    capacitors are connected in a new configuration. Before reconfiguration, the energy
    contained in the system is = 1 2 (/4) 2 ; when a capacitor is taken out of series
    and placed in parallel with the remaining capacitors to increase equivalent capacitance
    to 4/3, current flows to the lowervoltage newly-parallel capacitor to equalize
    output voltage. The final output voltage is 3 /8, and the remaining energy is
    = 1 2 (4/3) (3 /8) 2 . The portion of energy conserved is / = 0.75—i.e., 25% of
    buffered energy is dissipated by current in the switches during reconfiguration.
    Larger arrays are increasingly inefficient: the same scenario with an 8-capacitor
    array wastes 56.25% of its buffered energy transitioning from an 8-parallel to
    a 7-series-1-parallel configuration. Similar waste occurs when reducing equivalent
    capacitance by placing capacitors in series.[2](#page-5-1) Our evaluation in §
    [5.5](#page-10-0) indicates that the energy loss caused by switching often outweighs
    any advantage from dynamic behavior, causing the fully-connected approach to underperform
    even static buffers.


    3.3.2 Bank Isolation. The switching loss discussed above stems from charge flowing
    between capacitors within the power network as they switch into different configurations.
    REACT eliminates unnecessary current flow by organizing capacitors into independent,
    mutually isolated banks as shown in Figure [2.](#page-3-1) Figure [3](#page-4-3)
    illustrates in detail two example capacitor banks in each possible configuration:
    capacitors within a bank can only be arranged in either full-series (low


    capacitance) or full-parallel (high capacitance) so that no current flows between
    capacitors within a bank. Isolation diodes on the input and output of each bank
    prevent current between banks: when a charged parallel-configured bank is reconfigured
    into series (reducing its capacitance and boosting its output voltage), isolation
    diodes prevent it from charging other banks in the array. Similarly, banks re-configured
    into parallel cannot draw current from anywhere except the energy harvester. Isolation
    reduces the number of potential capacitor configurations compared to a fully-connected
    network, but dramatically increases energy efficiency.


    REACT''s isolation diodes direct the flow of current: intermediate capacitor arrays
    are only charged directly from the energy harvester and only discharge to the
    last-level buffer. This also means that all current from the harvester flows through
    two diodes before reaching the system, so minimizing power dissipation in the
    diodes is essential to maintaining overall system efficiency. To maximize charging
    efficiency, we design REACT using ideal diode circuits incorporating a comparator
    and pass transistor, rather than typical PN or Schottky diodes. Active ideal diodes
    are far more efficient at typical currents for batteryless systems: the circuit
    we use [\[21\]](#page-11-21) dissipates 0.02% of the power dissipated in a typical
    Schottky diode [\[42\]](#page-12-10) at a supply current of 1 mA.


    3.3.3 Bank Reconfiguration. The range of buffer sizes depends on the number of
    capacitor banks and the number of capacitors in each bank. REACT''s capacitor
    banks are effectively connected in parallel, so the overall capacitance is the
    sum of each bank''s contribution. Each REACT bank containing identical capacitors
    of capacitance may be configured to contribute no capacitance (disconnected),
    series capacitance /, or parallel capacitance .


    REACT must increment buffer capacitance in small steps in order to keep voltage
    within the operational range while capturing all incoming power. A large increase
    in capacitance pulls output voltage down and introduces cold-start energy loss
    if net power input is low; for extreme cases, the system may run out of energy
    and cease execution while the new capacitance charges even if incoming power would
    be sufficient to power operation. REACT first connects banks in the series configuration
    to contribute a small capacitance and avoid large jumps in overall buffer size.
    If the buffer continues to charge and reaches the upper voltage limit ℎℎ, REACT
    further expands capacitance by toggling double-pole-doublethrow bank switches
    to configure the capacitors in parallel. Expanding the buffer by reconfiguring
    charged capacitors rather than adding new ones reduces the time the system is
    cut off from input power while current flows exclusively to the new capacitance,
    because it is already charged toℎℎ/. Because no current flows between capacitors
    or banks, bank reconfiguration changes capacitance seen on the common rail without
    dissipative loss. REACT uses break-before-make switches to ensure no short-circuit
    current flows during


    <span id="page-5-1"></span><sup>2</sup>Charge pumps avoid this waste by never
    connecting capacitors at different potentials in parallel; in this use case, however,
    parallel capacitance is always necessary to smooth voltage fluctuations during
    switching and keep the output voltage within the computational backend''s acceptable
    range.


    switching; incoming current flows directly to the last-level buffer during the
    momentary open-circuit in the bank.


    <span id="page-6-0"></span>3.3.4 Charge Reclamation. Reconfiguring a bank from
    series to parallel allows REACT to efficiently increase capacitance without dropping
    output voltage. When voltage on the last-level buffer appraches the threshold
    value , indicating net power is leaving the buffer, REACT needs to reduce equivalent
    capacitance to boost voltage and keep the backend running. REACT accomplishes
    this by transitioning charged -capacitor banks from the parallel to the series
    configuration, reducing equivalent capacitance from to / and boosting output voltage
    from to . This boosts voltage on the last-level buffer and extracts more energy
    from the capacitor bank than would otherwise be available once voltage falls below
    .


    The remaining energy unavailable after the parallel→series transition depends
    on the number of -size capacitors in the bank. Before switching, the cold-start
    energy stored on the parallel-mode bank is = 1 2 2 . Switching to the series configuration
    conserves stored energy: = 1 2 ( /) () <sup>2</sup> = , but boosts voltage to
    enable the digital system to continue extracting energy. If net power remains
    negative, the system eventually drains the series-configuration bank down to .
    This is energetically equivalent to draining the parallel-configuration bank to
    /, leaving = 1 2 (/) <sup>2</sup> = 1 2 2 / unusable; the overall result is that
    REACT reduces energy loss by a factor of <sup>2</sup> when reducing system capacitance
    compared to simply disconnecting the capacitor.


    3.3.5 Bank Size Constraints. Increasing the number of capacitors in a bank improves
    efficiency by reclaiming more energy when switching a bank from parallel to series.
    However, it also introduces voltage spikes when the bank output voltage is temporarily
    multiplied by , increasing dissipative loss as current flows from the bank to
    the lastlevel buffer. Because REACT measures overall energy at the last-level
    buffer, the software component may interpret this voltage spike as a buffer-full
    signal and incorrectly add capacitance despite low buffered energy. In extreme
    cases, the voltage spike may exceed component absolute limits.


    The size of the last-level buffer constrains the number and size of each capacitor
    in a bank in order to keep voltage below REACT''s buffer-full threshold during
    a parallel→series transition. A larger contains more energy and thus pulls voltage
    higher when switched from parallel to series. Equation [1](#page-6-1) gives the
    last-level buffer voltage after switching a bank to series at a trigger voltage
    :


    <span id="page-6-1"></span>

    $$V\_{\text{new}} = \frac{(NV\_{low})(C\_{unit}/N)}{C\_{last} + C\_{unit}/N} +
    \frac{V\_{low} \* C\_{last}}{C\_{last} + C\_{unit}/N} \tag{1}$$


    Constraining < ℎℎ and solving for yields the absolute limit for (Equation [2\)](#page-6-2).
    Note that is only constrained if the parallel→series transition at produces


    <span id="page-6-3"></span>


    | Bank                    | 0   | 1   | 2   | 3   | 4   | 5    |

    |-------------------------|-----|-----|-----|-----|-----|------|

    | Capacitor Size (𝜇𝐹<br>) | 770 | 220 | 440 | 880 | 880 | 5000 |

    | Capacitor Count         | 1   | 3   | 3   | 3   | 3   | 2    |


    Table 1. Bank size and configurations for our REACT test implementation. Bank
    0 is the last-level buffer.


    a voltage above ℎℎ:


    <span id="page-6-2"></span>

    $$C\_{unit} < \frac{NC\_{last}(V\_{high} - V\_{low})}{NV\_{low} - V\_{high}} \tag{2}$$


    ## 3.4 REACT Software Interface


    REACT''s standalone hardware design means that the software component running
    on the target microcontroller is minimal. The software subsystem monitors for
    incoming over- or under-voltage signals from REACT''s voltage instrumentation
    and maintains a state machine for each capacitor bank. Each capacitor bank is
    disconnected at startup; on an overvoltage signal from REACT''s hardware, the
    software directs REACT to connect a new capacitor bank in the series configuration.
    A second overvoltage signal[3](#page-7-0) causes REACT to reconfigure the newly-connected
    bank to parallel; on the next overvoltage signal, REACT connects a second capacitor
    bank, and so on. REACT similarly steps capacitor banks in the opposite direction
    when an undervoltage signal arrives.


    <span id="page-6-4"></span>3.4.1 Software-Directed Longevity. REACT''s software
    component requires no active programmer intervention or code changes aside from
    setting voltage thresholds, initializing each bank state machine, and setting
    the order to connect and disconnect banks. Software does not need to know the
    details (, ) of each bank, although this information with the state of each bank
    gives a coarse idea of the current buffered energy. Because REACT only changes
    capacitance when the bank is near-full or near-empty, capacitance level is an
    effective surrogate for stored energy. Application code can use this feedback
    to set longevity guarantees through RE-ACT''s software interface. In preparation
    for a long-running or high-energy atomic operation, software sets a minimum capacitance
    level corresponding to the amount of energy required and then enters a deep-sleep
    mode keeping REACT''s capacitor polling time active. As the system charges, REACT
    eventually accumulates enough energy to reach the minimum capacitance level—indicating
    that enough energy is stored to complete the planned operation, and pulling the
    system out of its deep-sleep with enough energy to complete execution regardless
    of future power conditions.


    ## 4 Implementation


    We explore REACT''s impact on overall efficiency, reactivity, and longevity using
    a hardware prototype integrated into a real batteryless platform. All files for
    REACT, the baseline systems, and the energy harvesting emulator will be open-sourced
    upon publication. Our testbed is based on the MSP430FR5994 [\[22\]](#page-11-22),
    a popular microcontroller for energy harvesters [\[28,](#page-11-10) [44,](#page-12-6)
    [45\]](#page-12-11). For each buffer configuration we evaluate, an intermediate
    circuit power gates the MSP430 to begin operation once the buffer is charged to
    3.3V and disconnects it when the buffer voltage reaches 1.8V.


    Our REACT implementation has a range of 770 -18.03 using a set of 5 dynamic banks,
    in addition to the lastlevel buffer, detailed in Table [1.](#page-6-3) We implement
    the capacitors in banks 0-4 using combinations of 220 capacitors with max leakage
    current of 28 at their rated voltage of 6.3V [\[32\]](#page-12-12). Bank 5 uses
    supercapacitors with approximately 0.15 leakage current at 5.5V [\[33\]](#page-12-13).


    ## 4.1 Baseline Systems


    We evaluate REACT against three fixed-size buffers spanning our implementation''s
    capacitance range—770 F, 10 mF, and 18 mF—to ensure the realized improvement is
    a result of energy-adaptive behavior rather than simply different buffer capacity.
    To compare REACT''s capacitor architecture to prior work on dynamic energy buffers,
    we also implement and evaluate Morphy [\[49\]](#page-12-3) for a similar capacitance
    range. Our Morphy implementation uses eight 2 capacitors with leakage current
    of approximately 25.2 at 6.3V [\[35\]](#page-12-14) (i.e., slightly lower leakage
    than the capacitors in REACT).


    Morphy uses a secondary microcontroller powered by a battery or backup capacitor
    to control the capacitor array; we use a second MSP430FR5994 powered through USB,
    corresponding to Morphy''s battery-powered design. Accordingly, we expect our
    results to slightly overestimate Morphy''s performance in the fully-batteryless
    case as the system does not have to power the Morphy controller or charge a backup
    capacitor in our implementation. Seven of the eight capacitors in the array are
    available to reconfigure, with one task capacitor kept in parallel to smooth voltage
    fluctuations from switching. We evaluate the same subset of eleven possible configurations
    for the remaining seven capacitors as is done in the original Morphy work, resulting
    in a capacitance range for our Morphy implementation of 250 -16 .


    ## 4.2 Computational Backend


    To explore how REACT affects performance across a range of system demands—focusing
    on diverse reactivity and longevity requirements—we implement four software benchmarks:


    - Sense and Compute (SC): Exit a deep-sleep mode once every five seconds second
    to sample and digitally filter readings from a low-power microphone [\[11\]](#page-11-23).
    This benchmark represents systems which value high reactivity and can accept low
    persistence; individual atomic measurements are low-energy, but the system must
    be online to take the measurements.

    - Radio Transmission (RT): Send buffered data over radio [\[18,](#page-11-24)
    [31\]](#page-12-15) to a base station. Data transmission is an example of an application
    with high persistence requirements (radio transmissions are atomic and energyintensive)
    and low reactivity requirements (transmitting data may be delayed until energy
    is available).

    - Packet Forwarding (PF): Listen for and retransmit unpredictable incoming data
    over the radio. Timely packet forwarding demands both high persistence and reactivity
    to successfully receive and retransmit data.


    We emulate the power consumption of the necessary peripherals for each benchmark
    by toggling a resistor connected to a digital output on the MSP430, with values
    for each benchmark chosen to match the relevant peripheral. The reactivity-focused
    benchmarks (SC and PF) have deadlines that may arrive while the system is off;
    we use a secondary MSP430 to deliver these events. A deployed system may use remanence-based
    timekeepers [\[8\]](#page-11-25) to track internal deadlines despite power failures
    for the SC benchmark, while incoming packets as in the PF benchmark would arrive
    from other systems. Although we evaluate each benchmark in isolation, full systems
    are likely to exercise combinations of each requirement—one platform should support
    all reactivity, persistence, and efficiency requirements.


    ## <span id="page-7-1"></span>4.3 Energy Harvesting Frontend


    Energy harvesting volatility makes repeatable experimentation with batteryless
    devices difficult; uncontrollable environmental changes often have an outsized
    effect on energy input and obfuscate differences in actual system performance.
    We make our experiments repeatable and consistent using a programmable power frontend
    inspired by the Ekho [\[14\]](#page-11-26) record-and-replay platform. The power
    controller supplies the energy buffer using a high-drive Digital-to-Analog Converter
    (DAC), measures the load voltage and input current using a sense resistor, and
    tunes the DAC to supply a programmed power level. We evaluate REACT emulating
    both solar (5 <sup>2</sup> , 22% efficient cell [\[43\]](#page-12-16)) and RF
    energy (915 MHz dipole antenna [\[39\]](#page-12-17)). We also emulate the load-dependent
    performance of a commercial RF-to-DC converter [\[37\]](#page-12-18) and solar
    panel management chip [\[20\]](#page-11-27).


    <span id="page-7-0"></span><sup>3</sup>REACT polls the over/undervoltage signals
    using an internal timer rather than edge-sensitive interrupts to handle cases
    such as a high enough power input that the capacitance step does not pull supply
    voltage below .


    ## <span id="page-8-0"></span>5 Evaluation


    We evaluate REACT alongside the baseline buffers running each benchmark under
    three RF and two solar traces from publicly available repositories [\[3,](#page-11-28)
    [12\]](#page-11-13), representative of power dynamics for small energy harvesting
    systems. We record the RF traces in an active office environment using a commercial
    harvester and transmitter [\[37,](#page-12-18) [38\]](#page-12-19) and use solar
    irradiance traces from the Enhants mobile irradiance dataset [\[12\]](#page-11-13);
    Table [3](#page-9-0) gives a short summary of each trace. These traces show the
    power variability common for IoT-scale harvesters: environmental changes (e.g.,
    ambient RF levels, time of day) affect average input power, while short-term changes
    such as orientation cause instantaneous variation even if the environment is unchanged.
    We apply each trace using the power replay system described in § [4.3;](#page-7-1)
    once the trace is complete, we let the system run until it drains the buffer capacitor.


    ## 5.1 Characterization and Overhead


    Figure [6](#page-9-1) illustrates REACT''s behavior through the last-level buffer
    voltage when varying capacitance; the inset focuses on REACT''s voltage output
    as it expands to capture energy (also shown is the voltage of the comparable Morphy
    array). From a cold start REACT only charges the last-level buffer—rapidly reaching
    the enable voltage and then the upper voltage threshold (3.5V). REACT then adds
    a seriesconfigured capacitor bank to capture excess incoming energy. Voltage drops
    as the system temporarily operates exclusively from the last-level buffer while
    harvested energy goes towards charging the new capacitance. As power input falls,
    REACT''s output voltage falls below the upper threshold voltage—indicating REACT
    is operating at an efficient capacitance point. At ≈ 450 the last-level buffer
    is discharged to the lower threshold and REACT begins switching banks into series
    mode to boost their output voltage and charge the last-level buffer, visible in
    Figure [6](#page-9-1) as five voltage spikes corresponding to each capacitor bank—sustaining
    operation until no more energy is available at ≈ 500.


    We characterize REACT''s software overhead by running the DE benchmark on continuous
    power for 5 minutes with and without REACT''s software component, which periodically
    interrupts execution to measure the capacitor bank. At a sample rate of 10 Hz,
    REACT adds a 1.8% penalty to software-heavy applications. We measure REACT''s
    power overhead by comparing the execution time of systems running the DE benchmark
    using REACT and the 770 buffer after charging each to their enable voltage. Based
    on this approach we estimate that our implementation of REACT introduces a 68
    power draw, or <sup>∼</sup>14 per bank.


    ## 5.2 REACT Minimizes System Latency


    Table [4](#page-9-2) details the time it takes each system to begin operation,
    across power traces and energy buffers (charge time


    is software-invariant and constant across benchmarks). Latency is driven by both
    capacitor size and environment—the 10 buffer is <sup>∼</sup>13 larger than the
    770 buffer and takes on average 7 longer to activate the system across our traces.
    High-capacity static buffers incur a larger latency penalty even if mean power
    input is high if much of that power is contained in a short-term spike later in
    the trace (e.g., for the Solar Campus trace), but these dynamics are generally
    impossible to predict at design time. By exclusively charging the last-level buffer
    while the rest of the system is off, REACT matches the latency of the smallest
    static buffer—an average of 7.7 faster than the equivalent-capacity 17 mF buffer,
    which risks failing to start at all. Morphy further reduces system latency because
    its smallest configuration is smaller than REACT''s last-level buffer (250 vs
    770 ), although the limited reduction in average latency compared to the reduction
    in capacitance (Morphy realizes an average 20% reduction in latency over REACT
    using a 68% smaller capacitance) suggests that further reducing capacitance yields
    diminishing latency returns in realistic energy environments.


    Minimizing latency improves reactivity-bound applications such as the SC and PF
    benchmarks; this effect is visible in Table [2](#page-9-3) as the 770 buffer outperforms
    larger static versions in the SC benchmark for relatively low-power traces (RF
    Mobile/Obstructed). REACT inherits the latency advantage due to the small last-level
    buffer, similarly improving performance on each power trace. Morphy realizes a
    similar performance improvement over the static systems, but ultimately underperforms
    REACT as a result of inefficient capacitor switching (§ [5.5\)](#page-10-0). Small
    static buffers enable lowlatency operation, but at the cost of energy capacity.
    As power input increases, the latency penalty of large buffers fades and their
    increased capacity enables them to operate for longer—resulting in higher performance
    for larger static buffers under high-power traces (RF Cart, Solar Campus). Smaller
    buffers, in turn, become less efficient as they must burn more incoming energy
    off as waste heat.


    ## 5.3 REACT Maximizes Energy Capacity


    Figure [6](#page-9-1) illustrates the system-level effects of the capacitylatency
    tradeoff, and how REACT avoids this tradeoff through energy-adaptive buffering.
    The small 770 buffer charges rapidly, but reaches capacity and discharges energy
    when it does not have work to match incoming power (illustrated by clipping at
    3.6V on the 770 F line). The 10 mF buffer sacrifices latency for capacity—starting
    operation 21x later than the smaller buffer, but avoiding overvoltage. Morphy
    begins execution early with a small capacitance, but its lossy switching mechanism
    means it does not overall outperform the 770 buffer. In contrast, REACT achieves
    low latency, high efficiency, and high capacity by efficiently expanding capacitance
    as necessary after enabling the system.


    Tables [2](#page-9-3) and [5](#page-9-4) show that high capacity is valuable when
    average input power exceeds output power (e.g., DE and SC


    <span id="page-9-3"></span>


    |            | Data Encrypt |      |      |        | Sense and Compute |      |     |     |        |
    Radio Transmit |      |      |      |        |       |

    |------------|--------------|------|------|--------|-------------------|------|-----|-----|--------|----------------|------|------|------|--------|-------|

    | Buffer     | 770𝜇         | 10m  | 17m  | Morphy | REACT             | 770𝜇
    | 10m | 17m | Morphy | REACT          | 770𝜇 | 10m  | 17m  | Morphy | REACT |

    | RF Cart    | 1275         | 1574 | 1831 | 1745   | 1711              | 50   |
    81  | 104 | 77     | 83             | 22   | 53   | 56   | 38     | 48    |

    | RF Obs.    | 666          | 472  | 0    | 357    | 576               | 44   |
    28  | 0   | 39     | 49             | 4    | 6    | 0    | 0      | 3     |

    | RF Mob.    | 810          | 1004 | 645  | 801    | 1038              | 52   |
    50  | 40  | 53     | 84             | 4    | 13   | 12   | 4      | 15    |

    | Sol. Camp. | 6666         | 7290 | 7936 | 8194   | 9756              | 330  |
    353 | 367 | 398    | 439            | 1376 | 1457 | 1542 | 1059   | 1426  |

    | Sol. Comm. | 2168         | 2186 | 2554 | 2399   | 2232              | 88   |
    110 | 130 | 133    | 154            | 8    | 40   | 48   | 31     | 34    |

    | Mean       | 2317         | 2505 | 2593 | 2699   | 3063              | 113  |
    124 | 128 | 140    | 162            | 283  | 314  | 332  | 226    | 313   |


    Table 2. Performance on the DE, SC, and RT benchmarks, across traces and energy
    buffers.


    <span id="page-9-0"></span>


    |                                                                       | Trace          |
    Time (s) | Avg. Pow. (mW) | Power CV* |  |  |  |  |

    |-----------------------------------------------------------------------|----------------|----------|----------------|-----------|--|--|--|--|

    |                                                                       | RF Cart        |
    313      | 2.12           | 103%      |  |  |  |  |

    |                                                                       | RF Obstruction
    | 313      | 0.227          | 61%       |  |  |  |  |

    |                                                                       | RF Mobile      |
    318      | 0.5            | 166%      |  |  |  |  |

    |                                                                       | Solar
    Campus   | 3609     | 5.18           | 207%      |  |  |  |  |

    |                                                                       | Solar
    Commute  | 6030     | 0.148          | 333%      |  |  |  |  |

    | Table 3. Details of each power trace. *CV = Coefficient of Variation. |                |          |                |           |  |  |  |  |


    <span id="page-9-2"></span>


    | Buffer     | 770 𝜇𝐹 | 10 mF  | 17 mF  | Morphy | REACT |

    |------------|--------|--------|--------|--------|-------|

    | RF Cart    | 6.65   | 17.73  | 31.27  | 5.51   | 6.65  |

    | RF Obs.    | 14.58  | 223.07 | -      | 6.50   | 16    |

    | RF Mob.    | 6.90   | 148.10 | 239.88 | 5.65   | 6.38  |

    | Sol. Camp. | 42.11  | 737.39 | 741.42 | 35.59  | 41.26 |

    | Sol. Comm. | 119.60 | 196.30 | 213.00 | 108.10 | 130.6 |

    | Mean       | 37.97  | 264.92 | 306.39 | 32.27  | 40.18 |


    Table 4. System latency (seconds) across traces and energy buffers. - indicates
    system never begins operation.


    <span id="page-9-1"></span>![](_page_9_Figure_6.jpeg)


    Figure 6. Buffer voltage and on-time for the SC benchmark under RF Mobile power.
    Solid bars indicate when the system is operating.


    benchmarks executed under the RF Cart trace), or when peak power demand is uncontrollable
    and uncorrelated with input (e.g., the PF benchmark executed on Solar Campus,
    where both power supply and demand are concentrated in short bursts). In both
    cases, high-capacity systems store excess


    <span id="page-9-4"></span>


    | Buffer     | 770 uF |     |     | 10 mF | 17 mF |     | Morphy |     | REACT
    |     |

    |------------|--------|-----|-----|-------|-------|-----|--------|-----|-------|-----|

    | Packets    | Rx     | Tx  | Rx  | Tx    | Rx    | Tx  | Rx     | Tx  | Rx    |
    Tx  |

    | RF Cart    | 22     | 10  | 49  | 49    | 48    | 48  | 55     | 22  | 53    |
    52  |

    | RF Obs.    | 4      | 4   | 4   | 4     | 0     | 0   | 2      | 0   | 3     |
    0   |

    | RF Mob.    | 11     | 4   | 14  | 13    | 9     | 9   | 19     | 0   | 38    |
    5   |

    | Sol. Camp. | 163    | 163 | 240 | 240   | 196   | 196 | 206    | 204 | 284   |
    277 |

    | Sol. Comm. | 72     | 8   | 35  | 35    | 33    | 33  | 85     | 14  | 84    |
    63  |

    | Mean       | 54     | 38  | 68  | 68    | 57    | 57  | 73     | 48  | 92    |
    80  |


    Table 5. Packets successfully received and retransmitted during the Packet Forwarding
    benchmark.


    energy to continue operation even if future power input falls or demand rises.
    REACT efficiently expands to capture all incoming energy during periods of high
    net input power, matching or beating the performance of the 10 mF and 17 mF systems
    when they outperform the small 770 buffer.


    ## 5.4 REACT Provides Flexible, Efficient Longevity


    We evaluate REACT''s software-directed longevity guarantees (§ [3.4.1\)](#page-6-4)
    on the longevity-bound RT and PF benchmarks. We compare REACT to the 770 buffer,
    which cannot sustain a full transmission without additional input power. Running
    the RT benchmark under the RF Cart trace isolates this limitation as the 770 static
    buffer significantly underperforms the other buffers despite never reaching capacity:
    instead, it wastes power on doomed-to-fail transmissions when incoming power cannot
    make up for the deficit. We augment the RT benchmark code for our REACT implementation
    to include a minimum capacitance level for REACT, below which the system waits
    to gather more energy in a lowpower sleep mode. Leveraging REACT''s variable capacitance
    allows software to buffer energy to guarantee completion, more than doubling the
    number of successful transmissions and ultimately outperforming even the larger
    buffers.


    We use the same approach to execute the RT benchmark on our Morphy implementation.
    Similar to REACT, Morphy varies capacitance to keep supply voltage within an acceptable
    level for the application microcontroller while also waiting to gather enough
    energy to power a full transmission. Morphy''s underperformance compared to both
    REACT and the static buffers is a result of Morphy''s capacitor network design—as
    Morphy reconfigures the capacitor array to increase capacitance, stored energy
    is dissipated as current


    flows between capacitors in the network. This energy dissipation dramatically
    reduces Morphy''s end-to-end performance, particularly in systems where Morphy
    must switch capacitance to ensure success (i.e., the RT and PF benchmarks). REACT''s
    isolated capacitor banks eliminate this problem by restricting current flow during
    switching; the energy savings are reflected in the end-to-end performance, where
    REACT completes on average 38% more transmissions than Morphy.


    5.4.1 Fungible Energy Storage. A unified buffer means that energy is fungible,
    and REACT is flexible: software can re-define or ignore previous longevity requirements
    if conditions change or a higher-priority task arrives. The PF benchmark (Table
    [5\)](#page-9-4) shows the value of energy fungibility using two tasks with distinct
    reactivity and longevity requirements. Receiving an incoming packet requires a
    moderate level of longevity, but is uncontrollable and has a strict reactivity
    requirement (the system can only receive a packet exactly when it arrives). Re-transmission
    requires more energy and thus more longevity, but has no deadline. Software must
    effectively split energy between a controllable high-power task and an uncontrollable
    lower-power task.


    As in the RT benchmark we use the minimum-capacitance approach to set separate
    longevity levels for each task, using a similar approach for our Morphy implementation.
    When the system has no packets to transmit, it waits in a deep-sleep until receiving
    an incoming packet. If REACT contains sufficient energy when the packet arrives,
    it receives and buffers the packet to later send. REACT then begins charging for
    the transmit task, forwarding the buffered packet once enough energy is available.
    If another packet is received while RE-ACT is charging for the transmit task,
    however, software disregards the transmit-associated longevity requirement to
    execute the receive task if sufficient energy is available.


    Table [5](#page-9-4) shows that REACT outperforms all static buffer designs on
    the PF benchmark by efficiently addressing the requirements of both tasks, resulting
    in a mean performance improvement of 54%. REACT''s maximal reactivity enables
    it to turn on earlier and begin receiving and buffering packets to send during
    later periods of high power, while its high capacity enables it to make the most
    of incoming energy during those high periods. Software-level longevity guarantees
    both ensure the system only begins receive/transmit operations when enough energy
    is available to complete them, and that software can effectively allocate energy
    to incoming events as needed. Although Morphy enables the same software-level
    control of energy allocation, the energy dissipated when switching capacitors
    in the interconnected array means that Morphy''s overall performance on the PF
    benchmark is below that of the best performing static buffer.


    ## <span id="page-10-0"></span>5.5 REACT Improves End-to-End System Efficiency


    Optimizing buffer behavior maximizes the amount of harvested energy available
    to the end system for useful work.


    <span id="page-10-1"></span>![](_page_10_Figure_6.jpeg)


    Figure 7. Average buffer performance quantified by figures of merit across power
    traces for each benchmark, normalized to REACT.


    Figure [7](#page-10-1) illustrates the aggregate performance of REACT compared
    to the baseline systems across the benchmarks and power traces we evaluate; we
    find that REACT improves performance over the equally-reactive 770 buffer by an
    average of 39.1%, over the equal-capacity 17 mF buffer by 19.3%, and over the
    next-best-efficient 10 mF buffer by 18.8%. Compared to Morphy, REACT improves
    aggregate performance by 26.2%—demonstrating the necessity of REACT''s bank isolation
    approach and boosting performance where prior dynamic capacitance systems underperform
    static approaches. Extreme cases where the system is always operating in an energy
    surplus or deficit—such as the low-power SC benchmark under the high-power RF
    Cart trace—the extra power consumption from REACT''s hardware causes it to underperform
    suitable static buffers because REACT''s flexibility is unnecessary. In the common
    case, however, volatile power conditions expose the latency, longevity, and efficiency-related
    shortcomings of static buffer designs and expose the value of REACT''s efficient
    variable-capacitance approach.


    ## 6 Conclusion


    Energy harvesting systems operate on unreliable and volatile power, but use fixed-size
    buffers which waste energy and functionally limit systems when allocated capacity
    is a poor fit for short-term power dynamics. REACT stores incoming energy in a
    fabric of reconfigurable capacitor banks, varying equivalent capacitance according
    to current energy supply and demand dynamics—adding capacitance to capture surplus
    power and reclaiming energy from excess capacitance. REACT''s energy-adaptive
    approach maximizes reactivity and capacity to ensure all incoming energy is captured
    and efficiently delivered to sensing, computing, and communication devices. Our
    hardware evaluation on real-world power


    traces shows that REACT reduces system latency by an average of 7.7x compared
    to an equivalent-sized static buffer and improves throughput by an average of
    25.6% over any static buffer system, while incorporating software direction allows
    REACT to provide flexible and fungible task longevity guarantees. Compared to
    state-of-the-art switched capacitor systems, REACT''s efficient switching architecture
    improves performance by an average of 26.2%.


    REACT''s runtime-configurable buffering technique eliminates the tradeoff between
    system latency and longevity, and affords designers greater control over how batteryless
    devices respond to incoming power. Our results indicate that energy-responsive
    reconfiguration of hardware is an effective approach to both maximizing energy
    efficiency and system functionality, opening the door for future work leveraging
    energy-adaptive hardware and reconfiguration.


    ## References


    - <span id="page-11-16"></span>[1] Saad Ahmed, Qurat ul Ain, Junaid Haroon Siddiqui,
    Luca Mottola, and Muhammad Hamad Alizai. Intermittent computing with dynamic voltage
    and frequency scaling. In Proceedings of the 2020 International Conference on
    Embedded Wireless Systems and Networks, EWSN ''20, page 97–107, USA, 2020. Junction
    Publishing.

    - <span id="page-11-2"></span>[2] Miran Alhaideri, Michael Rushanan, Denis Foo
    Kune, and Kevin Fu. The moo and cement shoes: Future directions of a practical
    sensecontrol-actuate application, September 2013. Presented at First International
    Workshop on the Swarm at the Edge of the Cloud (SEC''13 @ ESWeek), Montreal.

    - <span id="page-11-28"></span>[3] Anon. Rf traces, October 2022. [https://anonymous.4open.science/r/](https://anonymous.4open.science/r/rf_traces-4B3E/README.md)
    [rf\\_traces-4B3E/README.md](https://anonymous.4open.science/r/rf_traces-4B3E/README.md).

    - <span id="page-11-17"></span>[4] Abu Bakar, Alexander G. Ross, Kasim Sinan Yildirim,
    and Josiah Hester. Rehash: A flexible, developer focused, heuristic adaptation
    platform for intermittently powered computing. Proc. ACM Interact. Mob. Wearable
    Ubiquitous Technol., 5(3), sep 2021.

    - <span id="page-11-1"></span>[5] James Blackman. What is mmtc in 5g nr, and how
    does it impact nb-iot and lte-m, October 2019. https://enterpriseiotinsights.com/20191016/channels/fundamentals/whatis-mmtc-in-5g-nr-and-how-does-it-impact-nb-iot-and-lte-m.

    - <span id="page-11-20"></span>[6] Michael Buettner, Ben Greenstein, and David
    Wetherall. Dewdrop: An energy-aware runtime for computational rfid. In Proceedings
    of the 8th USENIX Conference on Networked Systems Design and Implementation, NSDI''11,
    page 197–210, USA, 2011. USENIX Association.

    - <span id="page-11-4"></span>[7] Alexei Colin, Emily Ruppel, and Brandon Lucia.
    A reconfigurable energy storage architecture for energy-harvesting devices. In
    Proceedings of the Twenty-Third International Conference on Architectural Support
    for Programming Languages and Operating Systems, ASPLOS ''18, page 767–781, New
    York, NY, USA, 2018. Association for Computing Machinery.

    - <span id="page-11-25"></span>[8] Jasper de Winkel, Carlo Delle Donne, Kasim
    Sinan Yildirim, Przemysław Pawełczak, and Josiah Hester. Reliable timekeeping
    for intermittent computing. In Proceedings of the Twenty-Fifth International Conference
    on Architectural Support for Programming Languages and Operating Systems, ASPLOS
    ''20, page 53–67, New York, NY, USA, 2020. Association for Computing Machinery.

    - <span id="page-11-6"></span>[9] Jasper de Winkel, Vito Kortbeek, Josiah Hester,
    and Przemysław Pawełczak. Battery-free game boy. Proc. ACM Interact. Mob. Wearable
    Ubiquitous Technol., 4(3), sep 2020.

    - <span id="page-11-15"></span>[10] H. Desai and B. Lucia. A power-aware heterogeneous
    architecture scaling model for energy-harvesting computers. IEEE Computer Architecture
    Letters, 19(1):68–71, 2020.

    - <span id="page-11-23"></span>[11] Knowles Electronics. SPU0414HR5H-SB, December
    2012. [https://www.mouser.com/datasheet/2/218/knowles\\_01232019\\_](https://www.mouser.com/datasheet/2/218/knowles_01232019_SPU0414HR5H_SB-1891952.pdf)
    [SPU0414HR5H\\_SB-1891952.pdf](https://www.mouser.com/datasheet/2/218/knowles_01232019_SPU0414HR5H_SB-1891952.pdf).

    - <span id="page-11-13"></span>[12] M. Gorlatova, A. Wallwater, and G. Zussman.
    Networking low-power energy harvesting devices: Measurements and algorithms. In
    2011 Proceedings IEEE INFOCOM, pages 1602–1610, 2011.

    - <span id="page-11-3"></span>[13] Wang Song Hao and Ronald Garcia. Development
    of a digital and battery-free smart flowmeter. Energies, 7(6):3695–3709, 2014.

    - <span id="page-11-26"></span>[14] Josiah Hester, Timothy Scott, and Jacob Sorber.
    Ekho: Realistic and repeatable experimentation for tiny energy-harvesting sensors.
    In Proceedings of the 12th ACM Conference on Embedded Network Sensor Systems,
    SenSys ''14, page 330–331, New York, NY, USA, 2014. Association for Computing
    Machinery.

    - <span id="page-11-18"></span>[15] Josiah Hester, Lanny Sitanayah, and Jacob
    Sorber. Tragedy of the coulombs: Federating energy storage for tiny, intermittently-powered
    sensors. In ACM Conference on Embedded Networked Sensor Systems, SenSys, pages
    5–16, 2015.

    - <span id="page-11-19"></span>[16] Josiah Hester and Jacob Sorber. Flicker: Rapid
    prototyping for the batteryless internet-of-things. In Proceedings of the 15th
    ACM Conference on Embedded Network Sensor Systems, SenSys ''17, New York, NY,
    USA, 2017. Association for Computing Machinery.

    - <span id="page-11-8"></span>[17] Matthew Hicks. Clank: Architectural support
    for intermittent computation. In International Symposium on Computer Architecture,
    ISCA, pages 228–240, 2017.

    - <span id="page-11-24"></span>[18] Fraunhofer IIS. RFicient Basic, Ultra-Low-Power
    WakeUp Receiver, January 2019. [https://www.iis.fraunhofer.de/content/dam/iis/en/doc/](https://www.iis.fraunhofer.de/content/dam/iis/en/doc/il/ics/ic-design/Datenblaetter/Factsheet_WakeUp_v4.pdf)
    [il/ics/ic-design/Datenblaetter/Factsheet\\_WakeUp\\_v4.pdf](https://www.iis.fraunhofer.de/content/dam/iis/en/doc/il/ics/ic-design/Datenblaetter/Factsheet_WakeUp_v4.pdf).

    - <span id="page-11-0"></span>[19] Texas Instruments. Msp430g2x52, msp430g2x12
    mixed signal microcontroller datasheet (rev. g), May 2013. [https://www.ti.com/lit/ds/](https://www.ti.com/lit/ds/symlink/msp430g2252.pdf)
    [symlink/msp430g2252.pdf](https://www.ti.com/lit/ds/symlink/msp430g2252.pdf).

    - <span id="page-11-27"></span>[20] Texas Instruments. bq25570 nano power boost
    charger and buck converter for energy harvester powered applications, March 2019.
    <https://www.ti.com/lit/ds/symlink/bq25570.pdf>.

    - <span id="page-11-21"></span>[21] Texas Instruments. Lm66100 5.5-v, 1.5-a 79-milliohm,
    low iq ideal diode with input polarity protection, June 2019. [https://www.ti.com/](https://www.ti.com/lit/ds/symlink/lm66100.pdf)
    [lit/ds/symlink/lm66100.pdf](https://www.ti.com/lit/ds/symlink/lm66100.pdf).

    - <span id="page-11-22"></span>[22] Texas Instruments. MSP430FR599x, MSP430FR596x
    Mixed-Signal Microcontrollers, January 2021. [https://www.ti.com/lit/ds/symlink/](https://www.ti.com/lit/ds/symlink/msp430fr5994.pdf)
    [msp430fr5994.pdf](https://www.ti.com/lit/ds/symlink/msp430fr5994.pdf).

    - <span id="page-11-5"></span>[23] Joseph Kahn, Randy Katz, and Kristofer Pister.
    Next Century Challenges: Mobile Networking for "Smart Dust". In Conference on
    Mobile Computing and Networking (MobiCom), 1999.

    - <span id="page-11-12"></span>[24] Kemet. Supercapacitors fm series, July 2020.
    [https://www.mouser.](https://www.mouser.com/datasheet/2/212/1/KEM_S6012_FM-1103835.pdf)
    [com/datasheet/2/212/1/KEM\\_S6012\\_FM-1103835.pdf](https://www.mouser.com/datasheet/2/212/1/KEM_S6012_FM-1103835.pdf).

    - <span id="page-11-14"></span>[25] Silicon Labs. EFM32 Gecko Family EFM32WG Data
    Sheet, December 2021. [https://www.silabs.com/documents/public/data-sheets/](https://www.silabs.com/documents/public/data-sheets/efm32wg-datasheet.pdf)
    [efm32wg-datasheet.pdf](https://www.silabs.com/documents/public/data-sheets/efm32wg-datasheet.pdf).

    - <span id="page-11-9"></span>[26] Brandon Lucia and Benjamin Ransford. A simpler,
    safer programming and execution model for intermittent systems. In Conference
    on Programming Language Design and Implementation, PLDI, pages 575–585, 2015.

    - <span id="page-11-7"></span>[27] K. Ma, Y. Zheng, S. Li, K. Swaminathan, X.
    Li, Y. Liu, J. Sampson, Y. Xie, and V. Narayanan. Architecture exploration for
    ambient energy harvesting nonvolatile processors. In IEEE International Symposium
    on High Performance Computer Architecture, HPCA, pages 526–537, Feb 2015.

    - <span id="page-11-10"></span>[28] Kiwan Maeng, Alexei Colin, and Brandon Lucia.
    Alpaca: Intermittent execution without checkpoints. In International Conference
    on Object-Oriented Programming, Systems, Languages, and Applications, OOPSLA,
    pages 96:1–96:30, October 2017.

    - <span id="page-11-11"></span>[29] Kiwan Maeng and Brandon Lucia. Adaptive dynamic
    checkpointing for safe efficient intermittent computing. In USENIX Conference
    on Operating Systems Design and Implementation, OSDI, pages 129–144, November
    2018.

    - <span id="page-12-8"></span>[30] Kiwan Maeng and Brandon Lucia. Adaptive low-overhead
    scheduling for periodic and reactive intermittent execution. In Proceedings of
    the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation,
    PLDI 2020, page 1005–1021, New York, NY, USA, 2020. Association for Computing
    Machinery.

    - <span id="page-12-15"></span>[31] Microsemi. ZL70251 Ultra-Low-Power Sub-GHz
    RF Transceiver, March 2018. [https://www.microsemi.com/document-portal/doc\\_view/](https://www.microsemi.com/document-portal/doc_view/132900-zl70251-datasheet)
    [132900-zl70251-datasheet](https://www.microsemi.com/document-portal/doc_view/132900-zl70251-datasheet).

    - <span id="page-12-12"></span>[32] Murata. GRM31CR60J227ME11L Chip Monolithic
    Ceramic Capacitor for General. [https://search.murata.co.jp/Ceramy/image/img/A01X/](https://search.murata.co.jp/Ceramy/image/img/A01X/G101/ENG/GRM31CR60J227ME11-01.pdf)
    [G101/ENG/GRM31CR60J227ME11-01.pdf](https://search.murata.co.jp/Ceramy/image/img/A01X/G101/ENG/GRM31CR60J227ME11-01.pdf).

    - <span id="page-12-13"></span>[33] Murata. Supercapacitors FM Series, July 2020.
    [https://www.mouser.](https://www.mouser.com/datasheet/2/212/1/KEM_S6012_FM-1103835.pdf)
    [com/datasheet/2/212/1/KEM\\_S6012\\_FM-1103835.pdf](https://www.mouser.com/datasheet/2/212/1/KEM_S6012_FM-1103835.pdf).

    - <span id="page-12-1"></span>[34] Phillip Nadeua, Dina El-Damaj, Deal Glettig,
    Yong Lin Kong, Stacy Mo, Cody Cleveland, Lucas Booth, Niclas Roxhed, Robert Langer,
    Anantha P. Chandrakasan, and Giovanni Traverso. Prolonged energy harvesting for
    ingestible devices. Nature Biomedical Engineering, 1(0022), Feb 2017.

    - <span id="page-12-14"></span>[35] Nichicon. ALUMINUM ELECTROLYTIC CAPACITORS.
    [https://www.](https://www.nichicon.co.jp/english/products/pdfs/e-kl.pdf) [nichicon.co.jp/english/products/pdfs/e-kl.pdf](https://www.nichicon.co.jp/english/products/pdfs/e-kl.pdf).

    - <span id="page-12-0"></span>[36] Panasonic. Panasonic coin type lithium batteries,
    August 2005. [https://](https://datasheet.octopart.com/CR1616-Panasonic-datasheet-9751741.pdf)
    [datasheet.octopart.com/CR1616-Panasonic-datasheet-9751741.pdf](https://datasheet.octopart.com/CR1616-Panasonic-datasheet-9751741.pdf).

    - <span id="page-12-18"></span>[37] Powercast. P2110B 915 MHz RF Powerharvester
    Receiver, December 2016. [https://www.powercastco.com/wp-content/uploads/2016/12/](https://www.powercastco.com/wp-content/uploads/2016/12/P2110B-Datasheet-Rev-3.pdf)
    [P2110B-Datasheet-Rev-3.pdf](https://www.powercastco.com/wp-content/uploads/2016/12/P2110B-Datasheet-Rev-3.pdf).

    - <span id="page-12-19"></span>[38] Powercast. TX91501B – 915 MHz Powercaster
    Transmitter, October 2019. [https://www.powercastco.com/wp-content/uploads/2019/10/](https://www.powercastco.com/wp-content/uploads/2019/10/User-Manual-TX-915-01B-Rev-A-1.pdf)
    [User-Manual-TX-915-01B-Rev-A-1.pdf](https://www.powercastco.com/wp-content/uploads/2019/10/User-Manual-TX-915-01B-Rev-A-1.pdf).

    - <span id="page-12-17"></span>[39] Powercast. 915 mhz dipole antenna datasheet,
    November 2020. [https://www.powercastco.com/wp-content/uploads/2020/11/](https://www.powercastco.com/wp-content/uploads/2020/11/DA-915-01-Antenna-Datasheet_new_web.pdf)
    [DA-915-01-Antenna-Datasheet\\_new\\_web.pdf](https://www.powercastco.com/wp-content/uploads/2020/11/DA-915-01-Antenna-Datasheet_new_web.pdf).

    - <span id="page-12-4"></span>[40] Benjamin Ransford, Jacob Sorber, and Kevin
    Fu. Mementos: System Support for Long-Running Computation on RFID-Scale Devices.
    In Architectural Support for Programming Languages and Operating Systems (ASPLOS),
    2011.

    - <span id="page-12-5"></span>[41] Henry Sodano, Gyuhae Park, and Daniel Inman.
    Estimation of Electric Charge Output for Piezoelectric Energy Harvesting. In Strain,
    Volume 40, 2004.

    - <span id="page-12-10"></span>[42] ST. Small signal schottky diode, October 2001.
    [https://www.st.](https://www.st.com/content/ccc/resource/technical/document/datasheet/group1/11/76/e4/a3/df/07/49/14/CD00000767/files/CD00000767.pdf/jcr:content/translations/en.CD00000767.pdf)
    [com/content/ccc/resource/technical/document/datasheet/group1/](https://www.st.com/content/ccc/resource/technical/document/datasheet/group1/11/76/e4/a3/df/07/49/14/CD00000767/files/CD00000767.pdf/jcr:content/translations/en.CD00000767.pdf)
    [11/76/e4/a3/df/07/49/14/CD00000767/files/CD00000767.pdf/jcr:](https://www.st.com/content/ccc/resource/technical/document/datasheet/group1/11/76/e4/a3/df/07/49/14/CD00000767/files/CD00000767.pdf/jcr:content/translations/en.CD00000767.pdf)
    [content/translations/en.CD00000767.pdf](https://www.st.com/content/ccc/resource/technical/document/datasheet/group1/11/76/e4/a3/df/07/49/14/CD00000767/files/CD00000767.pdf/jcr:content/translations/en.CD00000767.pdf).

    - <span id="page-12-16"></span>[43] Voltaic. Voltaic systems p121 r1g, April 2020.
    [https://voltaicsystems.](https://voltaicsystems.com/content/Voltaic Systems P121
    R1G.pdf) [com/content/VoltaicSystemsP121R1G.pdf](https://voltaicsystems.com/content/Voltaic
    Systems P121 R1G.pdf).

    - <span id="page-12-6"></span>[44] Harrison Williams, Xun Jian, and Matthew Hicks.
    Forget failure: Exploiting sram data remanence for low-overhead intermittent computation.
    In Proceedings of the Twenty-Fifth International Conference on Architectural Support
    for Programming Languages and Operating Systems, ASPLOS ''20, page 69–84, New
    York, NY, USA, 2020. Association for Computing Machinery.

    - <span id="page-12-11"></span>[45] Harrison Williams, Michael Moukarzel, and
    Matthew Hicks. Failure sentinels: Ubiquitous just-in-time intermittent computation
    via low-cost hardware support for voltage monitoring. In International Symposium
    on Computer Architecture, ISCA, pages 665–678, 2021.

    - <span id="page-12-7"></span>[46] Joel Van Der Woude and Matthew Hicks. Intermittent
    computation without hardware support or programmer intervention. In USENIX Symposium
    on Operating Systems Design and Implementation, OSDI, pages 17–32, November 2016.

    - <span id="page-12-2"></span>[47] X. Wu, I. Lee, Q. Dong, K. Yang, D. Kim, J.
    Wang, Y. Peng, Y. Zhang, M. Saliganc, M. Yasuda, K. Kumeno, F. Ohno, S. Miyoshi,
    M. Kawaminami, D. Sylvester, and D. Blaauw. A 0.04mm316nw wireless and batteryless
    sensor system with integrated cortex-m0+ processor and optical communication for
    cellular temperature measurement. In 2018 IEEE Symposium on VLSI Circuits, pages
    191–192, 2018.

    - <span id="page-12-9"></span>[48] Fan Yang, Ashok Samraj Thangarajan, Wouter
    Joosen, Christophe Huygens, Danny Hughes, Gowri Sankar Ramachandran, and Bhaskar
    Krishnamachari. Astar: Sustainable battery free energy harvesting for heterogeneous
    platforms and dynamic environments. In Proceedings of the 2019 International Conference
    on Embedded Wireless Systems and Networks, EWSN ''19, page 71–82, USA, 2019. Junction
    Publishing.

    - <span id="page-12-3"></span>[49] Fan Yang, Ashok Samraj Thangarajan, Sam Michiels,
    Wouter Joosen, and Danny Hughes. Morphy: Software defined charge storage for the
    iot. In Proceedings of the 19th ACM Conference on Embedded Networked Sensor Systems,
    SenSys ''21, page 248–260, New York, NY, USA, 2021. Association for Computing
    Machinery.'
  decisions:
    evaluation_prompt: 'Qualified. Reason: The paper includes an evaluation section
      and mentions quantitative improvements such as increasing energy available for
      useful work by an average 25.6% and improving event responsiveness by an average
      7.7.'
    related_work_prompt: 'Qualified. Reason: The paper includes citations to prior
      research and discusses past work on buffer sizes, indicating meaningful engagement
      with existing literature.'
    novelty_prompt: 'Qualified. Reason: The paper proposes a new responsive buffering
      circuit called REACT, which offers a novel method for varying capacitance in
      response to net input power, providing new insights and improvements over existing
      static buffer systems.'
    review_only_prompt: Qualified
- title: "LRSCwait: Enabling Scalable and Efficient Synchronization in Manycore\n\
    \  Systems through Polling-Free and Retry-Free Operation"
  abstract: 'Extensive polling in shared-memory manycore systems can lead to contention,

    decreased throughput, and poor energy efficiency. Both lock implementations and

    the general-purpose atomic operation, load-reserved/store-conditional (LRSC),

    cause polling due to serialization and retries. To alleviate this overhead, we

    propose LRwait and SCwait, a synchronization pair that eliminates polling by

    allowing contending cores to sleep while waiting for previous cores to finish

    their atomic access. As a scalable implementation of LRwait, we present

    Colibri, a distributed and scalable approach to managing LRwait reservations.

    Through extensive benchmarking on an open-source RISC-V platform with 256

    cores, we demonstrate that Colibri outperforms current synchronization

    approaches for various concurrent algorithms with high and low contention

    regarding throughput, fairness, and energy efficiency. With an area overhead of

    only 6%, Colibri outperforms LRSC-based implementations by a factor of 6.5x in

    terms of throughput and 7.1x in terms of energy efficiency.'
  url: http://arxiv.org/abs/2401.09359v1
  keywords: atomics, synchronization, manycore, RISC-V
  document: '## I. INTRODUCTION


    Manycore systems are becoming increasingly popular due to the growing demand for
    computing power. However, the parallel execution of tasks introduces synchronization
    and atomicity issues that can lead to race conditions and unpredictable results.
    To ensure exclusive access to critical sections (CSs), atomic operations and locks
    can be used. However, locks also block cores that try to acquire them when they
    are not free, leading to busy waiting and polling. Polling, or constantly checking
    a shared resource for changes, can become an issue in concurrent algorithms. It
    leads to high core utilization and reduces overall system performance and energy
    efficiency as the cores compete for shared resources [\[1\]](#page-5-0). In the
    worst case, it can lead to livelocks or starvation, where cores are blocked from
    making progress because others continuously block them.


    Non-blocking algorithms avoid locks by updating atomic variables directly with
    atomic read–modify–write (RMW) operations. Specific arithmetic operations, like
    *add, and, or*, are often supported through specialized instructions. However,
    most concurrent algorithms require more complex modifications of atomic variables,
    such as conditional updates. For generic RMW operations, the compare-and-swap
    (CAS) operations or loadreserved/store-conditional (LRSC) pair are typical primitives
    designed to ensure that the operation is *atomic*, i.e., without interference
    from other cores [\[2\]](#page-5-1). For example, RISC-V''s loadreserved (LR)
    instruction loads a value from memory and


    places a reservation. The core can perform operations with the loaded value and
    store the result back conditionally with a store-conditional (SC). The latter
    instruction will only succeed if the reservation is still valid, meaning the memory
    location was not modified in the meantime. If the SC succeeds, the RMW sequence
    appears atomically. However, cores that fail an SC must retry the LRSC sequence
    pair until it succeeds. Variables outside CSs can also cause polling, where cores
    wait for changes in shared variables, leading to inefficiencies in core communication,
    like producer/consumer interactions.


    To eliminate retries and polling, we propose a novel, generalpurpose atomic RMW
    instruction pair called LRwait and SCwait. They extend the standard RISC-V LRSC
    pair by moving the linearization point, the point where the atomic operations
    of different cores get ordered, from the SC to the LRwait. The LRwait and SCwait
    are used in the same way as the LRSC pair. However, instead of returning the memory
    value immediately, the LRwait instruction only responds to one core at a time
    to set it up for a successful SCwait. This prevents failing SCs and retry loops.
    Furthermore, LRSCwait allows implementing polling-free locks. To eliminate polling
    even for non-atomic variables, we propose the Mwait instruction, which enables
    cores to sleep until a specific memory address changes its value.


    While cache-based systems often rely on the coherency protocol to implement such
    behavior, manycore accelerators scaling to hundreds of cores often rely on software-managed,
    multi-banked scratchpad memories (SPMs). Examples include commercial chips like
    GAP9 [\[3\]](#page-5-2) and RC64 [\[4\]](#page-5-3), as well as largescale research
    prototypes like MemPool [\[5\]](#page-5-4). While LRSCwait can be applied to cache
    and cache-less systems, in this work, we focus on cache-less, SPM-based manycore
    systems since they pose the design challenge of the memory controllers having
    to keep track of outstanding LRwait instructions to send their responses at the
    right time. However, duplicating large hardware queues for each bank is costly
    and scales poorly.


    As a scalable implementation of the proposed instructions, we present *Colibri*.
    Its concept is similar to linked-list-based software queues. It does not allocate
    a full array of entries for each queue but just a head and tail pointer per queue
    as illustrated in [Fig. 1.](#page-2-0) Each core is equipped with a queue node
    that can be linked to any queue. For Colibri, this means that instead of equipping
    each memory controller with a


    <sup>© 2024</sup> IEEE. Personal use of this material is permitted. Permission
    from IEEE must be obtained for all other uses, in any current or future media,
    including reprinting/republishing this material for advertising or promotional
    purposes, creating new collective works, for resale or redistribution to servers
    or lists, or reuse of any copyrighted component of this work in other works.


    hardware queue that can hold an entry for each core, each memory controller is
    extended with a parameterizable number of head and tail registers to form linked
    lists. Each core is then equipped with one hardware queue node, and when issuing
    an LRwait, the core inserts itself in the corresponding queue. We implemented
    Colibri on the open-source, manycore MemPool system, consisting of 256 cores sharing
    1 MiB of L1 memory [\[5\]](#page-5-4). Colibri provides a scalable solution that
    can be easily integrated into existing RISC-V systems. The LRSCwait solution can
    be used as a drop-in replacement for LRSC or as a powerful extension, making it
    a desirable option for highperformance computing systems. We evaluate the performance
    of Colibri against various hardware and software approaches. The results indicate
    that Colibri outperforms other approaches in all experiments, with a throughput
    increase of up to 6.5 times in high-contention situations and a 13% increase in
    low-contention scenarios. Additionally, Colibri reduces polling, allowing other
    applications to be unaffected by concurrent atomic accesses. Our key contributions
    are the following:


    - The LRwait extension consisting of three novel instructions (LRwait, SCwait,
    and Mwait), which enable atomic access and monitoring memory locations with a
    minimal amount of polling [\(Section III\)](#page-1-0).

    - A scalable implementation for LRwait named Colibri leveraging a distributed
    reservation queue [\(Section IV\)](#page-3-0).

    - An implementation and evaluation of Colibri on the MemPool platform that outperforms
    other approaches in throughput, fairness, polling, and energy per atomic access.
    Colibri scales linearly on the MemPool platform by introducing an area overhead
    of just 6% while being 8.8x more energy efficient than locks [\(Section V\)](#page-4-0).


    ## II. RELATED WORK


    A common approach to mitigate polling is using a backoff after a failed atomic
    access [\[2\]](#page-5-1). Existing backoff schemes, such as exponential backoff,
    where each failed attempt increases the backoff time, can reduce the overhead
    on shared resources but still make the cores busy-waiting and performing sub-optimally.


    The Mellor-Crummey, Scott (MCS) lock [\[6\]](#page-5-5) relies on a software queue
    for contending cores to enqueue in and spin on their respective node in the queue.
    This guarantees that each core spins on a unique location to mitigate contention
    on the lock variable itself. This approach works well in cache-based systems since
    each core can spin on its own L0 cache. However, in this work, we focus on systems
    with software-managed memories.


    While software approaches to locks are general and platform agnostic, their performance
    can not keep up with hardware locks. A study of two software locks and four hardware
    locks shows that hardware locks consistently outperform the software approaches
    by 25%-94%. However, hardware locks such as Hardlocks [\[7\]](#page-5-6) do not
    scale well, as the locks are managed by a centralized locking unit accessible
    to all cores. Accessing this unit quickly becomes the bottleneck in large systems.
    Furthermore, the number of locks is fixed at implementation time. Similarly, Glaser
    et al. present a synchronization unit where each core has a private direct connection
    to each hardware lock [\[8\]](#page-5-7). While this solves the contention issue,
    it prevents scaling beyond a few tens of cores. GLock suffers from a similar scalability
    issue [\[9\]](#page-5-8). It is based on a dedicated on-chip network consisting
    of lock managers and local controllers that synchronize to acquire a lock. Monchiero
    et al. propose a synchronizationoperation buffer implemented as a hardware queue
    in the memory controller to resolve the lock accesses [\[10\]](#page-5-9). However,
    this approach only implements locks and has a hardware cost that is proportional
    to the number of cores. Furthermore, each memory controller would require such
    a buffer to manage locks.


    While locks are a common solution for protecting critical sections, their blocking
    nature often limits performance. Lockfree algorithms, on the other hand, allow
    for much more concurrency. They often rely on instructions like CAS or the LRSC
    pair. This section focuses on the latter, specifically, RISC-V''s implementation.
    For example, the ATUN is a unit that can be placed in an Advanced eXtensible Interface
    (AXI) bus to support LRSC instructions to the downstream memory [\[11\]](#page-5-10).
    The table allows a reservation for every core, thus implementing a non-blocking
    version of LRSC. Furthermore, each bank would require its own ATUN adapter in
    a multi-banked system, introducing significant hardware overhead in large manycore
    systems. The Rocket chip features a similar implementation [\[12\]](#page-5-11).
    However, the number of reservations is limited.


    MemPool implements a lightweight version of LRSC by only providing a single reservation
    slot per memory bank [\[5\]](#page-5-4). However, this sacrifices the non-blocking
    property of the LRSC pair. The GRVI multiprocessor, on the other hand, modifies
    the granularity at which LRSCs operate by locking the complete memory bank [\[13\]](#page-5-12).
    This reduces the hardware overhead to one bit per core per bank, albeit the approach
    is still affected by retries due to spuriously failing SC operations.


    All those solutions implement the standard RISC-V LRSC instruction, leveraging
    the freedom of the official specification to achieve different trade-offs. However,
    none of them solve the polling and retry issue of failing SC operations. On the
    contrary, they sometimes worsen it. The Request-Store-Forward (RSF) synchronization
    model proposed by Liu et al. is similar to LRwait [\[14\]](#page-5-13). Synchronization
    requests are stored in a hardwaremanaged memory and handled in order by a synchronization
    controller. However, this approach leads to a high memory footprint, and the hardware
    needs to be replicated for each memory bank. Furthermore, it is infeasible for
    software-managed memories as the synchronization controller will interfere with
    the allocated data when adding the queue to the memory.


    Our LRwait approach and the efficient implementation through Colibri scale well
    to hundreds of cores and banks while completely eliminating polling without sacrificing
    granularity.


    ## III. LRWAIT AND SCWAIT


    <span id="page-1-0"></span>RISC-V defines the load-reserved/store-conditional
    (LRSC) instructions to implement generic, atomic RMW operations. The LR instruction
    reads a value from memory and places a reservation, which remains valid until
    the specified memory address is changed. The core can then modify the value and
    write the result back with an SC instruction. The latter will succeed only if
    the reservation is still valid. If the SC fails,


    the LRSC sequence has to be retried. The linearization point between contending
    cores is thus at the SC.


    LRwait eliminates the wasteful retry loop by moving the linearization point to
    the LRwait instruction, i.e., atomic accesses of competing cores are ordered at
    the LRwait instruction. Instead of immediately returning the value, the memory
    controller withholds the response such that only one core gets a response at a
    time, guaranteeing it to be the only core issuing an SCwait to the same address.
    The LRSCwait and LRSC instructions share similar semantics. The SCwait stores
    a value conditionally and returns a success or failure code analogous to the SC.
    Likewise, the LRwait matches the LR instruction, but its response is delayed.
    The sequence of an atomic RMW operation with LRSCwait is the following:


    - 1) The core issues the LRwait and waits for the response.

    - 2) The memory buffers the request until it is the next outstanding LRSCwait
    pair to that address.

    - 3) Once the LRwait is the next in line, the memory serves the request with the
    current memory value and monitors it. A store to the same address clears the reservation.

    - 4) The core modifies the value and writes it with an SCwait.

    - 5) The memory accepts the value if a valid reservation still exists and issues
    the response.


    While the memory guarantees that only one core proceeds with an LRSCwait pair,
    it cannot eliminate the possibility of another core overwriting the atomic variable,
    leading to a failing SCwait. One constraint of the LRSCwait instruction pair is
    that every LRwait must eventually be followed by an SCwait. While RISC-V does
    not have this constraint for LRSC, our extension requires the matching SCwait
    to yield the queue of outstanding LRwait instructions and allow progress on the
    atomic variable. Albeit LRSCwait can be used as a drop-in replacement for LRSC,
    it removes the lock-free progress guarantee that the LRSC instructions have. Since
    only one core can issue an SCwait, a malicious core could block the resource indefinitely
    and obstruct progress. However, LRSCwait still gives strong progress guarantees
    under the following constraints:


    *a) Mutual exclusion:* Just as the LRSC pair, the SCwait only succeeds if a valid
    reservation is present, meaning there was no write between the LRwait and the
    SCwait, which guarantees mutual exclusion and, therefore, atomicity.


    *b) Deadlock freedom:* To prevent circular dependencies between cores, every core
    must have at most one outstanding LRwait operation. RISC-V does not impose this
    requirement on LRSC. However, only the innermost LRSC pair is guaranteed to progress.
    Therefore, this requirement for deadlock freedom is a requirement for livelock
    freedom already. Furthermore, each core''s LRwait must eventually be followed
    by an SCwait to close the CS. We impose the same constraints as the RISC-V standard
    to allow only a finite and limited set of instructions between LRwait and SCwait.


    *c) Starvation freedom:* Starvation freedom guarantees that all cores eventually
    make progress. LRSC only guarantees that one core makes progress because an unlucky
    core could always lose the SC to a faster core. In our work, this scenario is
    prevented by handling the LRSCwait pairs in order, thus


    ![](_page_2_Figure_11.jpeg)


    <span id="page-2-0"></span>Fig. 1. Difference between LRSC architecture with a
    reservation table, LRSCwait with a reservation queue, and Colibri with a linked-list-like
    structure.


    enabling all cores to eventually execute the LRSCwait pair and, therefore, guaranteeing
    starvation freedom.


    Overall, while the blocking nature of the LRSCwait makes a core''s progress depend
    on other cores correctly executing and leaving the LRSCwait blocks, these constraints
    can easily be adhered to in bare-metal systems, which are fully under the programmer''s
    control. LRSCwait can provide very strong progress guarantees, enabling each core
    to progress. However, hardware failure or software bugs can become blocking.


    ## <span id="page-2-1"></span>*A. Ideal Hardware Implementation*


    A straightforward hardware implementation of LRSCwait requires tracking all outstanding
    reservations in order to ensure fairness and starvation freedom. As shown in [Fig.
    1,](#page-2-0) this can be achieved by an LRSCwait adapter placed in front of
    each memory bank, consisting of (i) a queue-like data structure of capacity n,
    where n is the number of cores in the system, and (ii) some additional logic to
    monitor memory accesses and invalidate reservations when the target address is
    overwritten. The overhead of this implementation in a system with m memory banks
    is O(n log<sup>2</sup> (n)m), where log<sup>2</sup> (n) represents identifier
    size per core. Assuming that m scales linearly with the number of cores, this
    implementation''s overhead scales quadratically with the system size: O(n 2 ),
    a non-negligible hardware complexity.


    ## *B. Optimized Hardware Implementation*


    To reduce the hardware complexity, we can decrease the queue''s capacity by assuming
    that only a subset of cores can access a specific address simultaneously. Our
    implementation supports a parametrizable number of reservation slots q. The case
    with q = n falls back to the ideal LRSCwait pair described in [Section](#page-2-1)
    III-A. We call this implementation *LRSCwaitideal*. If q < n, we trade hardware
    overhead with performance. In these implementations, *LRSCwaitq*, cores executing
    an LRwait to a full queue will fail immediately.


    ## *C. Mwait*


    To allow efficient monitoring and notification of a memory location from a core
    in the system, we introduce *Mwait*. Mwait is derived from LRwait, but without
    a matching SCwait. Instead, the reservation placed by Mwait is used to identify
    the core that needs to be notified of a change. For instance, a core may monitor
    a queue and be woken up when an element is pushed onto the queue. Our experiments
    show that Mwait provides a simple and efficient mechanism for monitoring memory


    locations, allowing cores to be woken up only when necessary. To handle the possibility
    that the change we wish to observe has already occurred, we provide Mwait with
    an expected value. If the memory location already differs from the expected value
    when Mwait is served, the core is immediately notified.


    ## IV. COLIBRI


    <span id="page-3-0"></span>Colibri implements a distributed queue, similar to
    a linked list, shown in [Fig. 1.](#page-2-0) It alleviates the huge hardware overhead
    of the hardware queues at each memory controller, replacing it with a dedicated
    head and tail node per queue and a simple controller. On top of that, each core
    requires its own hardware node, called *queue node (Qnode)*, to enqueue itself.
    Since each core can only be in one queue, one Qnode per core is enough. Therefore,
    Colibri only requires O(n + 2m) nodes and scales linearly with the system size.


    Since the queue is distributed across Qnodes and the head/tail nodes next to the
    memory banks, updating the queue becomes more complex. In comparison to the ideal
    LRwait, an enqueue operation from an LRwait, or a dequeue operation by an SCwait,
    does not happen in one place and a single cycle.


    We present a simple example of the construction and deconstruction of the queue
    in [Fig. 2](#page-3-1) with a single memory and two cores contending for the same
    address. Both cores have their own Qnodes, and the memory has a head and tail
    node. We call the cores *A* and *B* for simplicity.


    *a) LRwait:* Core A issues an LRwait request to the memory (1). Since the queue
    is initially empty, the head and tail nodes are set to A, and a reservation to
    the specified location is set up. The memory then sends the value A (2). During
    or after the described events, B''s LRwait request arrives at the memory (3).
    When the B''s LRwait request arrives at the memory, the controller appends B at
    the tail of the queue and then adds it as the successor to A. This is done by
    sending a so-called *SuccessorUpdate* to A (4). This SuccessorUpdate writes to
    A''s Qnode to make it point to B. In this final state shown in the top half of
    [Fig. 2,](#page-3-1) A and B form a queue with A at the head of the queue. At
    this point, A can issue an SCwait while B is sleeping, waiting for a response.


    *b) SCwait:* Core A finishes its LRSCwait pair by issuing an SCwait with the modified
    value (5). Immediately after an SCwait passes the Qnode, it sends a *WakeUpRequest*
    to the memory containing its successor, i.e., B (6). On arrival of the SCwait
    request at the memory, the head node and reservation


    ![](_page_3_Figure_7.jpeg)


    <span id="page-3-1"></span>Fig. 2. LRwait and SCwait sequence in Colibri with
    two cores and one queue.


    are checked. If everything is valid, the head node is temporarily invalidated
    to prevent a future SCwait from the same core from succeeding without reservation,
    and the SCwait is written to memory. The WakeUpRequest sets the head node to the
    successor node and triggers an LRwait response with the latest memory value written
    by A, i.e., for B (7). Core B is now free to issue an SCwait. Finally, the head
    and tail nodes point to B since B is the only core in the queue.


    This sequence can be generalized to more cores. Qnodes accept SuccessorUpdates
    even when the core is asleep, allowing the queue to be enlarged independent of
    the cores'' state.


    ## *A. Correctness of Colibri*


    *1) LRwait:* When an LRwait enqueues a node, it must update the tail to point
    to the newly enqueued node and append it to the previous tail node if it existed.
    If not, the enqueue operation inherently becomes atomic. Otherwise, to update
    the predecessor, the memory controller sends a SuccessorUpdate to the previous
    tail and overwrites the tail node atomically. Since we can only have one LRwait
    per core and SuccessorUpdates are only sent when overwriting a tail node, only
    a single SuccessorUpdate will ever be in flight to a Qnode, guaranteeing no lost
    links in the queue. If the SuccessorUpdate arrives after the core issued an SCwait,
    it will immediately bounce back as a WakeUpRequest. If the next LRwait arrives
    while the SuccessorUpdate is still in flight, the tail will be updated again,
    and the SuccessorUpdate will be sent to the next core. While a glance at the Qnodes
    might reveal broken links momentarily, the links only have to be made when a core
    issues its SCwait, which requires an LRwait response from the memory controller
    since memory transactions are ordered, this will always happen after the SuccessorUpdate.


    *2) SCwait:* If a core issuing an SCwait is the only one in the queue, i.e., the
    head and tail are equal, dequeuing itself by clearing the head and tail is trivial.
    Otherwise, the SCwait will invalidate the head node while leaving the value unchanged.
    A core would need to overwrite the head node to reach an inconsistent queue from
    this stage. This is only allowed for an LRwait reaching an empty queue or a WakeUpRequest
    arriving at the memory after invalidating the head node by an SCwait. A WakeUpRequest
    can only be triggered by an SCwait passing the Qnode, which can only be sent by
    a core at the head of the queue since the other cores are still waiting for their
    LRwait response. Thus, the WakeUpRequest arriving at the memory node guarantees
    that the queue is in a consistent state again.


    ## *B. Extending Colibri with Mwait*


    A core can issue an Mwait request to enqueue into Colibri''s queue to monitor
    a memory location. The memory controller then waits for a write to the monitored
    location, just like for LRwait''s reservation. After a write, the memory controller
    triggers a response to the Mwait instruction. For Mwait, the head node is sleeping
    as well in contrast to LRSCwait where the head is free to issue an SCwait. The
    Mwait response makes the Qnode dispatch the WakeUpReq for its successor, which
    then bounces to the memory controller, where the next Mwait response is released.
    In contrast to LRSCwait, the whole reservation queue is woken up without any interference
    from the cores.


    <span id="page-4-1"></span>TABLE I AREA OF A M E M P O O L\_T I L E WITH DIFFERENT
    LRSCWAIT DESIGNS.


    | Architecture            | Parameters    | Area[kGE] | Area[%] |

    |-------------------------|---------------|-----------|---------|

    | MemPool tile            | none          | 691       | 100.0   |

    | with LRSCwait1          | 1 queue slot  | 790       | 116.4   |

    | with LRSCwait8          | 8 queue slots | 865       | 127.4   |

    | with Colibri with MWait | 1 address     | 732       | 105.9   |

    | with Colibri with MWait | 2 addresses   | 750       | 108.5   |

    | with Colibri with MWait | 4 addresses   | 761       | 110.1   |

    | with Colibri with MWait | 8 addresses   | 802       | 116.3   |


    ![](_page_4_Figure_2.jpeg)


    <span id="page-4-2"></span>Fig. 3. Throughput of different LRSCwait implementations
    and standard RISC-V atomics at varying contention.


    ## V. RESULTS


    <span id="page-4-0"></span>We implement and evaluate various LRSCwait variations
    and Colibri in MemPool, an open-source, 256-core RISC-V system with 1024 SPM banks
    [\[5\]](#page-5-4). All our results are taken from cycle-accurate register-transfer
    level (RTL) simulation. Physical implementation results come from implementing
    MemPool in GlobalFoundries'' 22FDX fully depleted silicon-on-insulator (FD-SOI)
    technology. Power consumption is evaluated in typical conditions (TT/0.80 V/25
    °C), with switching activities from a post-layout gate-level simulation running
    at 600 MHz.


    The area overhead of different implementations is shown in [Table I.](#page-4-1)
    Even optimized implementations of LRSCwait quickly grow in size, while LRSCwaitideal
    is physically infeasible for a system of MemPool''s scale. Colibri, on the other
    hand, grows linearly and allows up to eight queues per memory controller with
    a similar area overhead to LRSCwait<sup>1</sup> of 16%.


    ## *A. Benchmarking*


    *a) Histogram:* We implement a concurrent histogram benchmark to evaluate Colibri''s
    performance at different levels of contentions. The application atomically increments
    a parametrizable number of bins. The fewer bins, the higher the contention. We
    increment a bin with different atomic operations and compare their performance
    as updates per clock cycle.


    The throughput of different LRSCwait implementations is shown in [Fig. 3.](#page-4-2)
    LRSCwaitideal outperforms all implementations across the whole spectrum of contention.
    The optimized implementations show similar performance at low contention but achieve
    much lower performance when the contention is higher than their number of reservations.
    Finally, Colibri achieves nearideal performance across all contentions. The slight
    performance


    ![](_page_4_Figure_10.jpeg)


    <span id="page-4-3"></span>Fig. 4. Throughput of different lock implementations
    compared to generic RMW atomics at varying contention.


    penalty comes from the extra roundtrips of Colibri''s node update messages. Colibri
    outperforms the LRSC-based implementation by a factor of 6.5× at high contention
    and 13% at low contention. For completeness, we also show the throughput of an
    *Atomic Add* implementation, which is designed specifically to increment a memory
    location atomically and represents the plot''s roofline. However, most concurrent
    algorithms need more complex atomic RMW operations than an increment, where programmers
    have to resort to locks of generic RMW atomics like LRSCwait.


    [Fig. 4](#page-4-3) compares Colibri to various lock-based implementations. Colibri,
    LRSC, and Atomic Add locks are spin locks with a backoff of 128 cycles, while
    Mwait lock implements an MCS lock, where Mwait is used to avoid polling. Colibri
    outperforms all other approaches for any contention. We observe that the LRSC
    and AMO-lock approaches perform worst at high contention due to their heavy polling
    and retry traffic, while waiting-based approaches perform average. At low contention,
    the waiting-based approaches perform worst because of their management overhead,
    while the other atomics tend to Colibri.


    *b) Interference:* We showed that LRSCwait can significantly improve the throughput
    of atomic operations across all levels of contention. On top of this increase
    in performance, eliminating the need to retry failed operations and polling also
    reduces traffic and frees up resources for cores not executing atomics. Cores
    working on computation experience less negative interference from the constant
    polling of atomics. To measure this effect, we partitioned the 256 cores of MemPool
    to either work on a matrix multiplication or to execute atomic operations. We
    measure the execution time of the matrix multiplication compared to an execution
    time without any interference. [Figure 5](#page-5-14) shows the relative performance
    for various types of atomic operations and distributions of working cores. Our
    Colibri implementation has a negligible impact on the worker cores, even at high
    contention and with a poller-to-worker ratio of 252:4. The retries of the LRSC
    operations, on the other hand, significantly impact the workers'' performance,
    despite a backoff of 128 cycles. At the same ratio of poller-to-workers, the LRSC
    implementation slows the workers down to 26%.


    *c) Queue:* To evaluate Colibri on a commonly used concurrent algorithm, we implement
    an MCS queue with LRSC and LRSCwait, as well as a lock-based queue using atomic
    adds. Concurrent queues are widely used for task scheduling


    ![](_page_5_Figure_0.jpeg)


    <span id="page-5-14"></span>Fig. 5. Matrix multiplication performance with interference
    from atomics. The poller-to-worker ratio is annotated in the figure with poller:worker.


    ![](_page_5_Figure_2.jpeg)


    <span id="page-5-15"></span>Fig. 6. Queue operations throughput with different
    atomics.


    or producer/consumer pipelines. [Figure 6](#page-5-15) shows the number of queue
    operations for a range of cores accessing a single queue. Colibri performs best
    and can sustain a high performance even at 256 cores. It outperforms the LRSC
    and lock-based approaches by 1.54× and 1.48× times with eight cores before both
    implementations drop in performance due to excessive retries and polling. At 64
    cores, Colibri is 9× faster. The shaded areas show each implementation''s slowest
    and fastest core performance range. It illustrates how Colibri results in a very
    balanced and fair workload distribution, while LRSC can have very big variations.


    *d) Energy efficiency:* [Table II](#page-5-16) shows the energy per operation
    for atomic accesses to the histogram at the highest contention. Comparing Colibri
    to the Atomic Add, which represents an ideal atomic update, we can see how energyefficient
    Colibri is for a generic RMW operation that consists of an LRwait, add, and SCwait
    operation. Compared to the LRSC or lock-based implementation, we observe the large
    benefit of the reduction in polling and retry traffic for improving energy efficiency
    by a factor of 7.1× and 8.8×.


    ## VI. CONCLUSION


    In this work, we propose the LRwait and Mwait synchronization primitives and their
    implementation, Colibri, which demonstrate a novel and effective solution for
    the LRSC synchronization problem in cache-less manycore systems. Colibri offers
    superior performance and scalability compared to existing hardware and software
    approaches, reduces polling, and improves throughput in a fair manner. Our experiments
    show that Colibri outperforms other implementations in both high and low contention
    scenarios by up to 6.5× and improved


    TABLE II AREA RESULTS FOR A M E M P O O L\_T I L E FOR IDEAL LRWAIT.


    <span id="page-5-16"></span>


    | Atomic access   | Backoff | Power (mW) | Energy (pJ/OP) | ∆     |

    |-----------------|---------|------------|----------------|-------|

    | Atomic Add      | 0       | 175        | 29             | −77%  |

    | Colibri         | 0       | 169        | 124            | ±0%   |

    | LRSC            | 128     | 186        | 884            | +613% |

    | Atomic Add lock | 128     | 188        | 1092           | +780% |


    energy efficiency by up to 8.8×. The polling and retries of LRSC-based solutions
    can lead to performance degradation of unrelated workers by up to 4×, while Colibri
    can operate even at high contention without impacting other cores. Additionally,
    Colibri can be easily integrated into existing RISC-V systems with a small hardware
    overhead and can be used as a drop-in replacement for LRSC or as an extension.


    ## ACKNOWLEDGMENT


    This work is funded in part by the COREnext project supported by the EU Horizon
    Europe research and innovation programme under grant agreement No. 101092598.


    ## REFERENCES


    - <span id="page-5-0"></span>[1] T. E. Anderson, "The performance of spin lock
    alternatives for sharedmemory multiprocessors," *IEEE Trans. Parallel Distrib.
    Syst.*, vol. 1, no. 1, pp. 6–16, 1990.

    - <span id="page-5-1"></span>[2] M. Herlihy, N. Shavit, V. Luchangco, and M. Spear,
    *The Art of Multiprocessor Programming*, 2nd ed., S. R. Merken, Ed. Cambridge,
    MA, USA: Morgan Kaufmann Publishers Inc., 2020.

    - <span id="page-5-2"></span>[3] GreenWaves Technologies SAS, "GAP9 next generation
    processor for hearables and smart sensors," GreenWaves Technologies SAS, Tech.
    Rep., 2021. [Online]. Available: [https://greenwaves-technologies](https://greenwaves-technologies.com/wp-content/uploads/2022/06/Product-Brief-GAP9-Sensors-General-V1_14.pdf).com/
    [wp-content/uploads/2022/06/Product-Brief-GAP9-Sensors-General-V1](https://greenwaves-technologies.com/wp-content/uploads/2022/06/Product-Brief-GAP9-Sensors-General-V1_14.pdf)
    14.[pdf](https://greenwaves-technologies.com/wp-content/uploads/2022/06/Product-Brief-GAP9-Sensors-General-V1_14.pdf)

    - <span id="page-5-3"></span>[4] R. Ginosar, P. Aviely, T. Israeli, and H. Meirov,
    "RC64: High performance rad-hard manycore," in *IEEE Aerosp. Conf. Proc.* IEEE,
    Jun. 2016, pp. 2074–2082.

    - <span id="page-5-4"></span>[5] S. Riedel, M. Cavalcante, R. Andri, and L. Benini,
    "MemPool: A scalable manycore architecture with a low-latency shared L1 memory,"
    *IEEE Trans. Comput.*, vol. 72, no. 12, pp. 3561–3575, 2023.

    - <span id="page-5-5"></span>[6] J. M. Mellor-Crummey and M. L. Scott, "Algorithms
    for scalable synchronization on shared-memory multiprocessors," *ACM Trans. Comput.
    Syst.*, vol. 9, no. 1, pp. 21–65, Feb. 1991.

    - <span id="page-5-6"></span>[7] T. B. Strøm, J. Sparsø, and M. Schoeberl, "Hardlock:
    Real-time multicore locking," *J. Syst. Archit.*, vol. 97, pp. 467–476, 2019.

    - <span id="page-5-7"></span>[8] F. Glaser, G. Tagliavini, D. Rossi, G. Haugou,
    Q. Huang, and L. Benini, "Energy-efficient hardware-accelerated synchronization
    for shared-L1 memory multiprocessor clusters," *IEEE Trans. Parallel Distrib.
    Syst.*, vol. 32, no. 3, pp. 633–648, Mar. 2021.

    - <span id="page-5-8"></span>[9] J. L. Abellan, J. Fern ´ andez, and M. E. Acacio,
    "Design of an efficient ´ communication infrastructure for highly contended locks
    in many-core cmps," *J. Parallel Distrib. Comput.*, vol. 73, no. 7, pp. 972–985,
    2013.

    - <span id="page-5-9"></span>[10] M. Monchiero, G. Palermo, C. Silvano, and O.
    Villa, "An efficient synchronization technique for multiprocessor systems on-chip,"
    *ACM SIGARCH Comput. Archit. News*, vol. 34, no. 1, pp. 33–40, Mar. 2006.

    - <span id="page-5-10"></span>[11] A. Kurth, S. Riedel, F. Zaruba, T. Hoefler,
    and L. Benini, "ATUNs: Modular and scalable support for atomic operations in a
    shared memory multiprocessor," in *ACM/IEEE Des. Autom. Conf.*, vol. 57. San Francisco,
    CA, USA: IEEE, Jul. 2020, pp. 902–907.

    - <span id="page-5-11"></span>[12] K. Asanovic´ *et al.*, "The rocket chip generator,"
    EECS Department, University of California, Berkeley, Tech. Rep. UCB/EECS-2016-17,
    Apr. 2016. [Online]. Available: http://www2.eecs.berkeley.[edu/Pubs/TechRpts/](http://www2.eecs.berkeley.edu/Pubs/TechRpts/2016/EECS-2016-17.html)
    [2016/EECS-2016-17](http://www2.eecs.berkeley.edu/Pubs/TechRpts/2016/EECS-2016-17.html).html

    - <span id="page-5-12"></span>[13] J. Gray, "Implementation of LR/SC on the GRVI
    multiprocessor," 2016. [Online]. Available: [https://groups](https://groups.google.com/a/groups.riscv.org/g/hw-dev/c/Mt9Q94f_l2w?pli=1).google.com/a/groups.riscv.org/g/hw[dev/c/Mt9Q94f](https://groups.google.com/a/groups.riscv.org/g/hw-dev/c/Mt9Q94f_l2w?pli=1)
    l2w?pli=1

    - <span id="page-5-13"></span>[14] S. Liu and J. L. Gaudiot, "Synchronization
    mechanisms on modern multicore architectures," in *Proc. 12th Asia-Pacific Conf.
    Adv. Comput. Syst. Archit.* Seoul, Korea: Springer Verlag, 2007, pp. 290–303.'
  decisions:
    evaluation_prompt: 'Qualified. Reason: The paper includes an evaluation of Colibri''s
      performance against various approaches, with results indicating improved throughput
      and reduced polling.'
    related_work_prompt: 'Qualified. Reason: The paper includes a "Related Work" section
      that engages with prior research.'
    novelty_prompt: 'Qualified. Reason: The paper proposes a novel atomic RMW instruction
      pair (LRwait and SCwait) and a scalable implementation called Colibri, offering
      new methods and insights.'
    review_only_prompt: 'Qualified. Reason: The paper introduces new contributions,
      specifically the LRwait extension and its implementation, rather than summarizing
      existing work.'
- title: Analyzing and Improving Hardware Modeling of Accel-Sim
  abstract: "GPU architectures have become popular for executing general-purpose programs.\n\
    Their many-core architecture supports a large number of threads that run\nconcurrently\
    \ to hide the latency among dependent instructions. In modern GPU\narchitectures,\
    \ each SM/core is typically composed of several sub-cores, where\neach sub-core\
    \ has its own independent pipeline.\n  Simulators are a key tool for investigating\
    \ novel concepts in computer\narchitecture. They must be performance-accurate\
    \ and have a proper model related\nto the target hardware to explore the different\
    \ bottlenecks properly.\n  This paper presents a wide analysis of different parts\
    \ of Accel-sim, a\npopular GPGPU simulator, and some improvements of its model.\
    \ First, we focus on\nthe front-end and developed a more realistic model. Then,\
    \ we analyze the way\nthe result bus works and develop a more realistic one. Next,\
    \ we describe the\ncurrent memory pipeline model and propose a model for a more\
    \ cost-effective\ndesign. Finally, we discuss other areas of improvement of the\
    \ simulator."
  url: http://arxiv.org/abs/2401.10082v1
  keywords: ''
  document: '# Analyzing and Improving Hardware Modeling of Accel-Sim


    [Rodrigo Huerta](https://orcid.org/0000-0003-0052-7710) rodrigo.huerta.ganan@upc.edu
    Universitat Politècnica de Catalunya Barcelona, Spain


    [Mojtaba Abaie Shoushtary](https://orcid.org/0000-0003-2377-6939) mojtaba.abaie@upc.edu
    Universitat Politècnica de Catalunya Barcelona, Spain


    [Antonio González](https://orcid.org/0000-0002-0009-0996) antonio@ac.upc.edu Universitat
    Politècnica de Catalunya Barcelona, Spain


    ## ABSTRACT


    GPU architectures have become popular for executing generalpurpose programs. Their
    many-core architecture supports a large number of threads that run concurrently
    to hide the latency among dependent instructions. In modern GPU architectures,
    each SM/core is typically composed of several sub-cores, where each sub-core has
    its own independent pipeline.


    Simulators are a key tool for investigating novel concepts in computer architecture.
    They must be performance-accurate and have a proper model related to the target
    hardware to explore the different bottlenecks properly.


    This paper presents a wide analysis of different parts of Accelsim, a popular
    GPGPU simulator, and some improvements of its model. First, we focus on the front-end
    and developed a more realistic model. Then, we analyze the way the result bus
    works and develop a more realistic one. Next, we describe the current memory pipeline
    model and propose a model for a more cost-effective design. Finally, we discuss
    other areas of improvement of the simulator.


    ### CCS CONCEPTS


    • Computing methodologies → Modeling methodologies; Simulation evaluation; • Computer
    systems organization → Parallel architectures.


    #### KEYWORDS


    GPU, GPGPU, microarchitecture, sub-core, front-end, memory pipeline, result bus,
    simulation, GPGPU-Sim, Accel-sim


    #### ACM Reference Format:


    Rodrigo Huerta, Mojtaba Abaie Shoushtary, and Antonio González. 2023. Analyzing
    and Improving Hardware Modeling of Accel-Sim. In 1st Workshop on Computer Architecture
    Modeling and Simulation (CAMS 2023), October 28, 2023, Toronto, Canada. ACM, New
    York, NY, USA, [6](#page-5-0) pages. [https://doi.org/](https://doi.org/10.1145/3589236.3589244)
    [10.1145/3589236.3589244](https://doi.org/10.1145/3589236.3589244)


    ### 1 INTRODUCTION


    GPU architectures have become popular for executing generalpurpose programs [\[4\]](#page-5-1)
    in addition to graphics workloads. These architectures have many cores, also known
    as Streaming Multiprocessor (SM) or Compute Units in Nvidia and AMD terminology
    respectively, that share an L2 cache. GPUs'' programming model


    CAMS 2023, October 28, 2023, Toronto, Canada


    <https://doi.org/10.1145/3589236.3589244>


    is based on having a vast amount of threads that are arranged into Cooperative
    Thread Arrays (CTA). Each CTA is mapped onto an SM. Threads in a CTA can easily
    synchronize and share data through a configurable scratchpad memory inside each
    SM, typically called Shared Memory. Once a kernel (a task executed in a GPU) is
    launched, CTAs are assigned to SMs. Threads in a CTA are grouped into sets (typically
    of 32 or 64 threads each) referred to as warps (also known as wavefronts). All
    threads in a warp execute in parallel in a lockstep mode, known as SIMT (single
    instruction multiple threads) execution mode. In modern architectures, each core
    is normally subdivided into different sub-cores (usually 4) [\[13\]](#page-5-2),
    [\[14\]](#page-5-3), [\[15\]](#page-5-4), [\[16\]](#page-5-5), [\[17\]](#page-5-6)
    and the warps of each CTA are distributed among them. Each sub-core has its own
    independent L0 instruction cache and pipeline.


    In [Figure 1,](#page-1-0) we can see an image illustrating the most important
    parts of an SM of modern GPU architectures.


    The first stage of a typical GPU pipeline is the Fetch, where a round-robin scheduler
    selects a warp with empty Instruction Buffer slots to start a fetch request of
    a few (e.g., two) consecutive instructions from the L0 instruction cache. When
    the request is completed, the instructions are decoded in the Decode stage and
    placed into the Instruction Buffer of the corresponding warp.


    In the issue stage, a warp among all the eligible ones is selected to issue its
    oldest instruction. An example of a widespread issue policy in the literature
    is Greedy Then Oldest (GTO) [\[18\]](#page-5-7). A warp is eligible to be scheduled
    if it has at least one instruction in the Instruction Buffer, and the oldest instruction
    does not depend on previously executed instructions pending to be finished. In
    this stage, instructions check its dependencies before being issued. A well-known
    approach for handling dependencies is using Scoreboards. However, other alternatives,
    such as a hardware-compiler co-designed approach, are used by some modern GPU
    architectures.


    Once an instruction is issued, it is placed in a Collector Unit (CU), and waits
    until all its source register operands are retrieved. Each sub-core register file
    has multiple banks with a few (e.g., two) ports per bank, allowing for multiple
    accesses in a single cycle at low cost. An arbiter deals with the possible conflicts
    among several petitions to the same bank. When all source operands of an instruction
    are in the CU, the instruction goes to the dispatch stage, where it is dispatched
    to the proper execution unit (e.g., memory, single-precision, special function)
    whose latencies differ depending on the type of unit and instruction. Once the
    instruction reaches the write-back stage, it writes its result in the register
    file.


    In order to explore new ideas, simulators have been widely used in computer architecture,
    and GPGPUs are no exception. NVIDIA has exposed part of the process of creating
    their warehouse simulator, NVIDIA Architectural Simulator (NVArchSim or NVAS)
    [\[21\]](#page-5-8). As for the academic side, we have different alternatives.
    One of them is MGPUSim [\[20\]](#page-5-9), which is a multi-gpu simulator modeling


    Permission to make digital or hard copies of all or part of this work for personal
    or classroom use is granted without fee provided that copies are not made or distributed
    for profit or commercial advantage and that copies bear this notice and the full
    citation on the first page. Copyrights for components of this work owned by others
    than the author(s) must be honored. Abstracting with credit is permitted. To copy
    otherwise, or republish, to post on servers or to redistribute to lists, requires
    prior specific permission and/or a fee. Request permissions from permissions@acm.org.


    <sup>©</sup> 2023 Copyright held by the owner/author(s). Publication rights licensed
    to ACM. ACM ISBN 979-8-4007-0776-6/23/02. . . \$15.00


    <span id="page-1-0"></span>![](_page_1_Figure_1.jpeg)


    Figure 1: SM architecture.


    the AMD GCN 3 micro-architecture and supporting virtual memory. An alternative
    simulator is the Accel-Sim [\[9\]](#page-5-10) framework, a cycleaccurate state-of-the-art
    trace-driven simulator supporting CUDA applications trying to resemble NVIDIA
    modern architectures.


    This work focuses on improving the modeling of three main components inside the
    SM and sub-cores in the Accel-Sim framework: the front-end, the result and the
    memory pipeline.


    The rest of this paper is organized as follows. In [section 2](#page-1-1) we present
    the different enhancements done to the simulator to have a more realistic model.
    In [section 3,](#page-3-0) we describe the evaluation methodology that is later
    used in [section 4](#page-3-1) to analyze the effects of the proposed modifications.
    Then, we continue discussing in [section 5](#page-4-0) other improvements that
    we plan to investigate in the future. Finally, we conclude in [section 6.](#page-5-11)


    #### <span id="page-1-1"></span>2 IMPROVEMENTS TO ACCEL-SIM


    This section discusses the SM modeling problems we found in the simulator and
    how we solved them. First, we start talking about the front-end in [subsection
    2.1.](#page-1-2) Then, we focus on the result bus modeling in [subsection 2.2.](#page-2-0)
    Finally, we improve the memory execution pipeline model in the [subsection 2.3.](#page-2-1)


    #### <span id="page-1-2"></span>2.1 Front-end


    The latest architectures of NVIDIA have a design of SMs with four sub-cores or
    processing blocks in NVIDIA terminology [\[13\]](#page-5-2), [\[14\]](#page-5-3),
    [\[15\]](#page-5-4), [\[16\]](#page-5-5), [\[17\]](#page-5-6). Each sub-core has
    a decode and a fetch unit that accesses each sub-core''s private L0 instruction
    cache. In the upper


    memory level, there is a L1 instruction cache shared by the four sub-cores. As
    is reported by Zhe Jia et al. [\[8\]](#page-5-12) [\[7\]](#page-5-13) and Barnes
    et al. [\[3\]](#page-5-14), warps are distributed between sub-cores with the following
    formula: sub-core\_id = warp\_id%4.


    In [Figure 2,](#page-1-3) we can see a block diagram of what is modeled in Accel-Sim.
    As it can be seen, there are no private L0 instruction caches per subcore, and
    the split among sub-cores starts after the decode stage, meaning there is only
    one fetch and decode unit for the whole SM instead of four. In spite of that,
    the pipeline can be kept fully occupied because the fetch and decode stages are
    done four times per cycle, but this model does not reflect modern architectures,
    and it requires a much more costly solution, since the shared instruction cache
    must be able to serve four different access per cycle. Furthermore, there are
    cases where the fetching and decoding of the instruction is happening in just
    one cycle, which is incorrect.


    <span id="page-1-3"></span>![](_page_1_Figure_13.jpeg)


    Figure 2: Current Accel-Sim front-end.


    We have modified the front-end model of Accel-Sim to model the architecture depicted
    in [Figure 3,](#page-2-2) which is based on the sub-core architecture of modern
    GPUs. As it can be seen, there is an L0 instruction cache, a fetch unit and a
    decode unit private to each subcore. Besides, there is a round-robin priority
    arbiter for handling the requests from different sub-cores to the L1 instruction
    cache.


    Another improvement over current Accel-Sim is regarding the way instructions are
    packed and stored in the intruction cache. The current simulator fetches two instructions
    per warp four times per cycle. The simulator is trace-driven and it packs together
    instructions that may belong to different cache lines and instructions that do
    not have consecutive addresses because branches (e.g., instruction at PC 0x100
    is packed with instruction at PC 0x460). Furthermore, instructions belonging to
    different kernel codes have the same address, which makes the instruction cache
    to wrongly compute many accesses as hits when they should be misses. For instance,
    the first two instructions of each kernel are assumed to have the same memory
    addresses whereas in reality they are different. In our model, we have fixed this
    issues, by mapping instructions to cache lines in a correct manner, and giving
    different addresses to instructions of different kernels.


    In short, we have developed a more accurate/realistic model of the front-end by
    adding private L0 instructions caches, fetch and decode units per sub-cores, and
    have implemented a correct and realistic mapping of instructions to cache lines.


    Analyzing and Improving Hardware Modeling of Accel-Sim CAMS 2023, October 28,
    2023, Toronto, Canada


    <span id="page-2-2"></span>![](_page_2_Figure_1.jpeg)


    Figure 3: Proposed front-end.


    ### <span id="page-2-0"></span>2.2 Result bus


    During the dispatch of instructions with fixed execution latencies to the different
    execution pipelines, the availability of result buses to write-back the register
    file is checked. The result bus modeling in the original simulator is just searching
    for a free result bus with the instruction latency after being dispatched to the
    execution unit. However, it does not consider conflicts of register file banks
    at write-backs of fixed latency operations. So, we have added support to detect
    these conflicts and only allow the same number of instructions that go to the
    same register file bank finishing in a given cycle as the number of register bank
    ports is in the architecture.


    It is remarkable that during this modeling of the result bus, we assume two register
    file ports as the latest architectures of NVIDIA are designed with two register
    file ports per register file bank as discussed by Zhe Jia et al. [\[8\]](#page-5-12)
    [\[7\]](#page-5-13) instead of only one. Moreover, each port can be used for a
    write-back or a read. So, up to two instructions with the same destination register
    file bank being executed in different execution units can be scheduled to finish
    in the same cycle.


    #### <span id="page-2-1"></span>2.3 Memory execution pipeline


    Modern GPUs based on sub-core partitioning have a memory pipeline in each sub-core,
    which accesses shared memory structures for the whole SM (L1 data cache, shared
    memory, texture cache, constant


    memory) [\[13\]](#page-5-2), [\[14\]](#page-5-3), [\[15\]](#page-5-4), [\[16\]](#page-5-5),
    [\[17\]](#page-5-6). Even though the behavior is unrevealed, it has the sense
    that each of these memory pipelines is in charge of calculating the memory addresses
    and coalescing before sending the requests to the memory structures.


    The Accel-Sim choice is to have a single memory pipeline for the whole SM instead
    of a dedicated unit per sub-core. This creates a problem because there is a single
    dispatch latch for the whole SM instead of one per sub-core. Instructions are
    maintained in this latch until all the requests have been sent to the desired
    memory structure. Satisfying all the requests of an instruction can lead to many
    cycle stalls depending on the degree of achieved coalescing and bank conflicts
    of the instructions requests. It prevents other memory instructions from progressing
    and stops other instructions from being issued (even from different sub-cores).
    As memory instructions are not dispatched, they are held in operand collector
    units (a limited resource inside the sub-core), so any instruction is prevented
    from being issued because there is no space in the operand collection stage. Moreover,
    address calculation, coalescing, and request selection are designed to be done
    in the same cycle, greatly increasing hardware requirements. As it is constrained
    to do all these tasks in a single cycle, the more hardware-hungry part is the
    coalescing. The reason is that each thread needs to know which threads have the
    same address, so the total number of address comparators is 32,<sup>2</sup> =
    496, which is huge. Furthermore, there is a single write-back latch for the whole
    SM, which may create some contention in case there are different memory structures
    with ready accesses simultaneously for different sub-cores. This became worse
    if the instruction in the write-back latch could not progress because register
    bank ports were already occupied. In [Figure 4,](#page-2-3) we can see a graphical
    representation of the baseline simulator model for the memory pipeline.


    <span id="page-2-3"></span>![](_page_2_Figure_11.jpeg)


    Figure 4: Memory execution pipeline of Accel-Sim.


    Regarding our model, we have incorporated a Memory pipeline unit in each sub-core
    that accesses the memory structures shared for the whole SM. The details of this
    unit of commercial GPUs are unknown; therefore, we have addressed an aggressive
    performance design that is reasonable, which might differ from commercial designs.
    We have modeled a pipeline inside this unit that splits the memory address calculation
    and coalescing in different cycles. An instruction is maintained in the address
    latch until all the memory addresses have been processed for being coalesced,
    but the instruction just remains in the dispatch latch one cycle unless the address
    latch is not empty. The number of cycles needed for coalescing will depend on
    the requests required for each warp. For example, if all the accesses go to the
    same cache block, it will need just a single cycle. Concerning the coalescing
    hardware, as it dedicates one cycle for each thread, the number of address comparators
    is just 32. Once an address has been processed, it is stored in the request buffer.
    All the memory requests stored in that buffer are sent to the round-robin priority
    arbiter between sub-cores. This means that if there is a memory structure or a
    bank from L1D unused, it will be used for that request, even if that is not the
    first one generated by the coalescing unit. This differs from the current model
    from Accel-Sim, which only allows requests to progress in order of generation.
    Finally, we have a write-back latch for each sub-core with an arbiter to prioritize
    between different memory structures. Moreover, we include a write-back arbiter
    to give priority between sub-cores for popping accesses from the different memory
    structures. A depiction of this model can be found in [Figure 5.](#page-3-2)


    <span id="page-3-2"></span>![](_page_3_Figure_2.jpeg)


    Figure 5: Proposed memory execution pipeline.


    Table 1: GPU specification


    <span id="page-3-3"></span>


    | Parameter                              | Value    |

    |----------------------------------------|----------|

    | Clock                                  | 1605 𝑀𝐻𝑧 |

    | SP/INT/SFU/Tensor Units per sub-core   | 1/1/1/1  |

    | Warps per SM                           | 32       |

    | Warp Width                             | 32       |

    | Number of registers per SM             | 65536    |

    | Issue Scheduler policy                 | GTO      |

    | Number of SMs                          | 40       |

    | Sub-cores per SM                       | 4        |

    | Number of Collector Units per sub-core | 2        |

    | L1 instruction cache size              | 32 KB    |

    | L1 data cache size                     | 32 KB    |

    | Shared memory size                     | 64 KB    |

    | L2 cache size                          | 4 MB     |

    | Memory Partitions                      | 16       |

    | Fixes                                  |          |

    | L0 instruction cache size              | 16 KB    |

    | Max. Num. requests and replies of L0I  | 1        |

    | Register file ports per bank           | 2        |


    #### <span id="page-3-0"></span>3 EVALUATION METHODOLOGY


    To evaluate how these improvements in the SM have affected the simulation, we
    have measured the speed-up and the Absolute Variation in Cycles (AVC) over the
    baseline simulator. We have used 42 benchmarks belonging to Rodinia 3.1 [\[6\]](#page-5-15),
    Deepbench[\[11\]](#page-5-16), Parboil [\[19\]](#page-5-17), Pannotia [\[5\]](#page-5-18),
    and ISPASS-2009 [\[2\]](#page-5-19) suites simulated under completion.


    We have configured the Accel-Sim [\[9\]](#page-5-10) simulation infrastructure
    with the trace execution mode to reassemble an NVIDIA RTX 2070 Super. The main
    configuration parameters in [Table 1.](#page-3-3) Moreover, we have extended the
    simulator to include the different fixes explained in [section 2.](#page-1-1)


    #### <span id="page-3-1"></span>4 RESULTS


    In this section, we analyze the impact of the different incorporated fixes.


    Even these changes report a tiny change in performance, just a 0.25% speed-up
    and a 3.67% of AVC on average, there are significant changes if we look in detail
    at some benchmarks. The reason why the average speed-up is smaller than the AVC
    is because there are benchmarks that are gaining performance, and others are losing
    it. At the same time, AVC measures the absolute difference between the original
    version and the proposed one.


    In [Figure 6,](#page-4-1) we can see the impact of the different improvements
    in some significant benchmarks. The all configuration may have less impact than
    separate fixes because they create differences in cycles in opposite directions.
    Nevertheless, some benchmarks such as gemm-train are affected in the same direction,
    so including all the changes produces an effect of 12%.


    In this figure, we can see that the result bus modeling is the change that is
    affecting the less.


    Analyzing and Improving Hardware Modeling of Accel-Sim CAMS 2023, October 28,
    2023, Toronto, Canada


    <span id="page-4-1"></span>![](_page_4_Figure_2.jpeg)


    Figure 6: Absolute Variation of Cycles of the most significant applications.


    However, the changes in the memory pipeline are more noticeable in more benchmarks,
    where the most significant case is fw reaching a 23% of AVC regarding the memory
    pipeline and 21% including all the changes.


    About the modifications affecting the front-end, it is affecting a lot dwt2d (17%)
    and lud (12.36%). The main reason behind these variations in performance is the
    different enhancements regarding instruction caches. To illustrate it, we can
    see in [Figure 7](#page-4-2) the miss rate increment factor of the first level
    instruction cache.


    <span id="page-4-2"></span>![](_page_4_Figure_6.jpeg)


    Figure 7: Miss ratio.


    If we compare gemm-train against dwt2d and lud, it requires more than 27.4 and
    7.8 cycles respectively to complete the execution. This is why even though the
    miss rate increment factor is more significant in gemm-train, it has less effect
    than in shorter benchmarks.


    #### <span id="page-4-0"></span>5 FUTURE WORK


    Apart from the improvements in modeling that we have presented in this paper,
    others will be necessary to improve the simulator and open the door to new research
    areas in GPUs. We will start discussing what can be done inside the GPU cores,
    and then we will move outside the SM.


    #### 5.1 Inside SM


    First of all, as reported by Mishkin et al. [\[10\]](#page-5-20), the simulator
    is not handling WAR dependencies correctly. Even though the issue of instructions
    is in-order, the dispatch from collector units to the execution pipelines is out-of-order.
    It could be the case that two instructions of the same warp are in collector units
    simultaneously, and the younger one with WAR hazard is dispatched earlier than
    the older one. Besides, this problem is not frequent, as reported by Mishkin et
    al. [\[10\]](#page-5-20), hardware must support these cases not to incur errors.
    We believe that commercial GPUs are not suffering from WAR hazards, but as discussed
    by Zhe Jia et al. [\[8\]](#page-5-12) [\[7\]](#page-5-13), the management of dependencies
    of modern GPUs seems that is detecting instruction dependencies without scoreboards.
    However, we lack detailed documentation on how the hardware supports it.


    Regarding the Operand Collection stage and the register file of GPUs, it is known
    that the latest architectures of NVIDIA are designed with two register file ports
    per register file bank as discussed by Zhe Jia et al. [\[8\]](#page-5-12) [\[7\]](#page-5-13)
    instead of only one. In Accel-Sim, they have modeled this feature focusing on
    the throughput of reading operands. However, the way that is handled is by repeating
    twice the Operand Collection stage per cycle. This means that the allocation of
    collector units, dispatch of ready collector units, and arbitration of reads are
    repeated twice per cycle. Moreover, it is assumed that each collector unit has
    unlimited ports for reading the operands, which would mean having a huge crossbar
    to support the worst case. It was one in previous architectures, but this parameter
    is unknown in the current ones.


    Additionally, the register file caching system for instruction operands is not
    present. Instead, the number of register banks is increased, which may be effective
    for having an equivalent contention to commercial hardware. Due to this lack of
    modeling, it is difficult to know if novel proposals such as Malekeh [\[1\]](#page-5-21)
    are beating industry designs.


    Furthermore, the tracking usage of registers across the execution is uncompleted.
    The tracer tool only captures the usage of regular registers, but registers such
    as predication, uniform, and uniform predicate are not detected. Furthermore,
    some instructions, such as tensor ones, use two registers for some operands even
    though only one of them is captured or indicated in the binary, so investigating
    this conduct will grant the community more opportunities to investigate problems
    in operand collection.


    Finally, as the state-of-the-art mode of the simulator is tracedriven, it focuses
    on capturing executed instructions. In addition, control flow instructions like
    BMOV use special registers that can have dependencies with general purpose registers.
    Therefore, the current model is not enough to correctly analyze many applications''
    control flow behavior. This complicates analyzing and creating new micro-architectural
    and compiler proposals regarding this topic.


    #### 5.2 Ouside SM


    Outside of the SM, other topics can be improved in this simulation tool. First
    of all, the NOC between SMs and memory partitions is plain in the simulator without
    taking into account the hierarchy of TPCs and GPCs, which seems to be the approach
    followed by the industry, as can be seen in this patent [\[12\]](#page-5-22).
    This hierarchy modeling will allow us to analyze new opportunities, including
    the <span id="page-5-0"></span>new Hopper architecture features such as thread-block
    cluster and distributed shared memory between SM inside a GPC [\[17\]](#page-5-6).
    Finally, adding virtual memory and multi-GPU support will be great for analyzing
    new kinds of trendy workloads.


    #### <span id="page-5-11"></span>6 CONCLUSION


    In this paper, we propose different improvements to the Accel-Sim framework with
    the purpose of having a more real simulator. We have focused on the front-end
    modeling (supporting sub-core split and better modeling of instruction caches).
    Also, we have a better design of the result bus. Then, we explored how the baseline
    simulator represents the memory pipeline and how we think it should be modeled
    to have a more feasible approach. Finally, we have compared all these modifications
    against the baseline simulator to show how they affect the performance of benchmarks.


    #### ACKNOWLEDGMENTS


    This work has been supported by the CoCoUnit ERC Advanced Grant of the EU''s Horizon
    2020 program (grant No 833057), the Spanish State Research Agency (MCIN/AEI) under
    grant PID2020- 113172RB-I00, and the ICREA Academia program.


    #### REFERENCES


    - <span id="page-5-21"></span>[1] Mojtaba Abaie Shoushtary, Jose Maria Arnau,
    Jordi Tubella Murgadas, and Antonio Gonzalez. 2023. Lightweight Register File
    Caching in Collector Units for GPUs. Proceedings of the 15th Workshop on General
    Purpose Processing Using GPU (feb 2023), 27–33.<https://doi.org/10.1145/3589236.3589245>

    - <span id="page-5-19"></span>[2] Ali Bakhoda, George L. Yuan, Wilson W. L. Fung,
    Henry Wong, and Tor M. Aamodt. 2009. Analyzing CUDA workloads using a detailed
    GPU simulator. In 2009 IEEE International Symposium on Performance Analysis of
    Systems and Software. 163–174.<https://doi.org/10.1109/ISPASS.2009.4919648>

    - <span id="page-5-14"></span>[3] Aaron Barnes, Fangjia Shen, and Timothy G. Rogers.
    2023. Mitigating GPU Core Partitioning Performance Effects. In 2023 IEEE International
    Symposium on High-Performance Computer Architecture (HPCA). 530–542. [https://doi.org/10.](https://doi.org/10.1109/HPCA56546.2023.10070957)
    [1109/HPCA56546.2023.10070957](https://doi.org/10.1109/HPCA56546.2023.10070957)

    - <span id="page-5-1"></span>[4] Martin Burtscher, Rupesh Nasre, and Keshav Pingali.
    2012. A quantitative study of irregular programs on GPUs. In Proceedings - 2012
    IEEE International Symposium on Workload Characterization, IISWC 2012. 141–151.
    [https://doi.org/10.1109/](https://doi.org/10.1109/IISWC.2012.6402918) [IISWC.2012.6402918](https://doi.org/10.1109/IISWC.2012.6402918)

    - <span id="page-5-18"></span>[5] Shuai Che, Bradford M. Beckmann, Steven K. Reinhardt,
    and Kevin Skadron. 2013. Pannotia: Understanding irregular GPGPU graph applications.
    In 2013 IEEE International Symposium on Workload Characterization (IISWC). 185–195.
    <https://doi.org/10.1109/IISWC.2013.6704684>

    - <span id="page-5-15"></span>[6] Shuai Che, Michael Boyer, Jiayuan Meng, David
    Tarjan, Jeremy W. Sheaffer, Sang Ha Lee, and Kevin Skadron. 2009. Rodinia: A benchmark
    suite for heterogeneous computing. In Proceedings of the 2009 IEEE International
    Symposium on Workload Characterization, IISWC 2009. 44–54. [https://doi.org/10.1109/IISWC.](https://doi.org/10.1109/IISWC.2009.5306797)
    [2009.5306797](https://doi.org/10.1109/IISWC.2009.5306797)

    - <span id="page-5-13"></span>[7] Zhe Jia, Marco Maggioni, Jeffrey Smith, and
    Daniele Paolo Scarpazza. 2019. Dissecting the NVidia Turing T4 GPU via Microbenchmarking
    Technical Report. (2019).

    - <span id="page-5-12"></span>[8] Zhe Jia, Marco Maggioni, Benjamin Staiger, and
    Daniele Paolo Scarpazza. 2018. Dissecting the NVIDIA Volta GPU Architecture via
    Microbenchmarking. CoRR abs/1804.06826 (2018). arXiv[:1804.06826](https://arxiv.org/abs/1804.06826)<http://arxiv.org/abs/1804.06826>

    - <span id="page-5-10"></span>[9] Mahmoud Khairy, Zhesheng Shen, Tor M. Aamodt,
    and Timothy G. Rogers. 2020. Accel-Sim: An Extensible Simulation Framework for
    Validated GPU Modeling. In 2020 ACM/IEEE 47th Annual International Symposium on
    Computer Architecture (ISCA). 473–486.<https://doi.org/10.1109/ISCA45697.2020.00047>

    - <span id="page-5-20"></span>[10] Michael Mishkin. 2016. Write-after-Read Hazard
    Prevention in GPGPUsim. (2016).

    - <span id="page-5-16"></span>[11] S. Narang and G. Diamos. 2016. GitHub - baidu-research/DeepBench:
    Benchmarking Deep Learning operations on different hardware. [https://github.com/baidu](https://github.com/baidu-research/DeepBench)[research/DeepBench](https://github.com/baidu-research/DeepBench)

    - <span id="page-5-22"></span><span id="page-5-2"></span>[12] NVIDIA. 2010. Consolidated
    crossbar that supports a multitude of traffic types. [13] NVIDIA. 2017. NVIDIA
    Tesla V100 GPU architecture the world''s most advanced data center GPU. Technical
    Report. NVIDIA.

    - <span id="page-5-3"></span>[14] NVIDIA. 2018. NVIDIA TURING GPU architecture
    Graphics Reinvented NVIDIA Turing GPU Architecture. Technical Report.

    - <span id="page-5-4"></span>[15] NVIDIA. 2020. NVIDIA AMPERE GA102 GPU architecture
    Second-Generation RTX NVIDIA Ampere GA102 GPU Architecture. Technical Report.

    - <span id="page-5-5"></span>[16] NVIDIA. 2022. NVIDIA ADA GPU architecture. Technical
    Report.

    - <span id="page-5-6"></span>[17] NVIDIA. 2022. NVIDIA H100 Tensor Core GPU Architecture.
    Technical Report.

    - <span id="page-5-7"></span>[18] Timothy G. Rogers, Mike Oconnor, and Tor M.
    Aamodt. 2012. Cache-conscious wavefront scheduling. In Proceedings - 2012 IEEE/ACM
    45th International Symposium on Microarchitecture, MICRO 2012. IEEE Computer Society,
    72–83. [https:](https://doi.org/10.1109/MICRO.2012.16) [//doi.org/10.1109/MICRO.2012.16](https://doi.org/10.1109/MICRO.2012.16)

    - <span id="page-5-17"></span>[19] J.A. Stratton, C. Rodrigues, I.J. Sung, N.
    Obeid, L.W. Chang, N. Anssari, G.D. Liu, and W.W. Hwu. 2012. Parboil: A Revised
    Benchmark Suite for Scientific and Commercial Throughput Computing. Center for
    Reliable and High-Performance Computing (2012).

    - <span id="page-5-9"></span>[20] Yifan Sun, Trinayan Baruah, Saiful A. Mojumder,
    Shi Dong, Xiang Gong, Shane Treadway, Yuhui Bao, Spencer Hance, Carter McCardwell,
    Vincent Zhao, Harrison Barclay, Amir Kavyan Ziabari, Zhongliang Chen, Rafael Ubal,
    José L. Abellán, John Kim, Ajay Joshi, and David Kaeli. 2019. MGPUSim: Enabling
    Multi-GPU Performance Modeling and Optimization. In 2019 ACM/IEEE 46th Annual
    International Symposium on Computer Architecture (ISCA). 197–209.

    - <span id="page-5-8"></span>[21] Oreste Villa, Daniel Lustig, Zi Yan, Evgeny
    Bolotin, Yaosheng Fu, Niladrish Chatterjee, Nan Jiang, and David Nellans. 2021.
    Need for Speed: Experiences Building a Trustworthy System-Level GPU Simulator.
    In 2021 IEEE International Symposium on High-Performance Computer Architecture
    (HPCA). 868–880. [https:](https://doi.org/10.1109/HPCA51647.2021.00077) [//doi.org/10.1109/HPCA51647.2021.00077](https://doi.org/10.1109/HPCA51647.2021.00077)'
  decisions:
    evaluation_prompt: 'Qualified. Reason: The paper includes sections on evaluation
      methodology and analysis of proposed modifications, indicating empirical analysis.'
    related_work_prompt: 'Qualified. Reason: The paper cites prior research and discusses
      improvements to existing models, indicating meaningful engagement with related
      work.'
    novelty_prompt: 'Qualified. Reason: The paper presents improvements to the Accel-Sim
      simulator, offering new models for the front-end, result bus, and memory pipeline,
      indicating novelty.'
    review_only_prompt: 'Qualified. Reason: The paper introduces new contributions
      by analyzing and improving the hardware modeling of Accel-Sim, rather than just
      summarizing existing work.'
- title: "Study on the Particle Sorting Performance for Reactor Monte Carlo\n  Neutron\
    \ Transport on Apple Unified Memory GPUs"
  abstract: 'In simulation of nuclear reactor physics using the Monte Carlo neutron

    transport method on GPUs, the sorting of particles plays a significant role in

    performance of calculation. Traditionally, CPUs and GPUs are separated devices

    connected at low data transfer rate and high data transfer latency. Emerging

    computing chips tend to integrate CPUs and GPUs. One example is the Apple

    silicon chips with unified memory. Such unified memory chips have opened doors

    for new strategies of collaboration between CPUs and GPUs for Monte Carlo

    neutron transport. Sorting particle on CPU and transport on GPU is an example

    of such new strategy, which has been suffering the high CPU-GPU data transfer

    latency on the traditional devices with separated CPU and GPU. The finding is

    that for the Apple M2 max chip, sorting on CPU leads to better performance per

    power than sorting on GPU for the ExaSMR whole core benchmark problems and the

    HTR-10 high temperature gas reactor fuel pebble problem. The partially sorted

    particle order has been identified to contribute to the higher performance with

    CPU sort than GPU. The in-house code using both CPU and GPU achieves 7.5 times

    power efficiency that of OpenMC on CPU for ExaSMR whole core benchmark with

    depleted fuel, and 150 times for HTR-10 fuel pebble benchmark with depleted

    fuel.'
  url: http://arxiv.org/abs/2401.11455v2
  keywords: sorting, Monte Carlo, neutron transport, GPU, apple, unified memory
  document: '### I. INTRODUCTION


    Being the method with the highest fidelity, the Monte Carlo method has been adopted
    as a verification tool to other methods such as discrete ordinates and the method
    of characteristics. Because of its heavy computation burden, the Monte Carlo method
    has not been considered as the everyday reactor simulation tool. The great performance
    improvement on GPUs demonstrated in recent studies makes the adoption of Monte
    Carlo method as a routine practice more practical. Table [I](#page-0-0) summarizes
    some recent work.


    As discovered by Hamilton [\[1\]](#page-5-0), particle sorting is important for
    achieving high neutron transport performance by increasing the coherence in execution
    paths between GPU threads. Joo [\[2\]](#page-5-1) further elaborates the particle
    sorting strategies. In previous study, most codes such as Pragma [\[2\]](#page-5-1),
    Shift [\[1\]](#page-5-0) and MagiC [\[3\]](#page-5-2) (possibly) use GPUs for
    particle sorting, and OpenMC [\[4\]](#page-5-3) possibly uses CPUs for particle
    sorting. The Warp [\[5\]](#page-5-4) code seems sorting particles on CPUs too.


    <span id="page-0-0"></span>TABLE I SUMMARY OF CONTINUOUS ENERGY MONTE CARLO NEUTRON
    TRANSPORT CODE WITH GPU SUPPORT


    | Code       | Developer                  | Sorting on CPUs or GPUs |

    |------------|----------------------------|-------------------------|

    | Warp [5]   | Univ. California, Berkeley | CPUs                    |

    | Pragma [2] | Seoul National Univ.       | GPUs                    |

    | Shift [1]  | Oak Ridge National Lab.    | GPUs                    |

    | OpenMC [4] | Argonne National Lab.      | CPUs (Possibly)         |

    | MagiC [3]  | Univ. South China          | GPU (Possibly)          |

    | In-house   | In-house                   | CPUs and GPUs           |

    |            |                            |                         |


    As indicated in Figure [1,](#page-0-1) from chips for personal entertainment such
    as Sony Playstation 5 [\[6\]](#page-5-5) to chips for high performance computation
    such as AMD [\[7\]](#page-5-6) and Nvidia [\[8\]](#page-5-7) have merged CPU and
    GPU chips. Some chips such as PS5 and MI300A have unified memory with the CPU,
    GPU and memory connected with the high speed bus called infinity fabric.


    ![](_page_0_Figure_11.jpeg)


    <span id="page-0-1"></span>Fig. 1. A snapshot of the design of some recent merged
    CPU and GPU chips


    This work proposes to use Apple unified memory computing devices to study the
    collaboration between CPUs and GPUs in Monte Carlo neutron transport methods.
    This collaboration is previously uneconomic because of the low data transfer rate
    and high data transfer latency between CPUs and GPUs on computing devices with
    separated CPUs and GPUs. There are previous work study the Apple silicon for Monte
    Carlo methods in areas such as: CPU performance study [\[9\]](#page-5-8), multicore
    CPU work balance [\[10\]](#page-5-9), and cross section lookup on GPU [\[11\]](#page-5-10).


    The contributions are summarized as followed.


    • Discussion about programming for Apple M2 Max chip


    - Study of the sorting performance on CPU-GPU for partially sorted data

    - Verification of in-house merged CPU-GPU code with VERA pincell and assembly
    benchmark problems

    - Comparison of CPU and GPU sorting strategies on the simulation power efficiency
    for ExaSMR whole core and HTR-10 fuel pebble benchmark problems


    # II. DEVELOPMENT ON APPLE SILICON AS A UNIFIED MEMORY DEVICE


    The Apple silicon chips are system-on-chips (SoCs), where a cluster of more powerful
    performance CPU cores, and a cluster of less powerful efficiency cores, and a
    cluster of GPU cores are integrated on the same silicon die. All CPU and GPU clusters
    have its private L2 cache, and these clusters are sharing an L3 cache named as
    the System Level Cache (SLC).


    ### *A. Apple M2 Max Chip*


    In this work, the Apple M2 Max chip is studied and Figure [2](#page-1-0) gives
    a snapshot [\[12\]](#page-5-11) and an illustration of the chip components. There
    are four memory chips surrounding the SoC in the center. The memory type is LPDDR5,
    which offers an interface of 512 bit with a bandwidth of 400 GB/s. In most gaming
    GPUs, GDDR6 and GDDR6X are the most common types, and in workstation GPUs, HBM2
    and HBM3 are the most common types. Usually, LPDDR5 is used for power efficient
    mobile devices, and LPDDR5 has higher latency. The way of Apple''s use of LPDDR5
    is unusual, and it has a tight connection with the SoC. The result of this tight
    packaging is the lower latency than the usual LPDDR5 packaging while keeping the
    power consumption at low level.


    The SoC includes 8 performance CPU cores sharing 32 MB L2 cache and 4 efficiency
    CPU cores sharing 4 MB L2 cache. The L2 cache is much larger than Intel, AMD and
    many ARM based CPUs. There are 38 GPU cores sharing an unknown size of L2 cache.
    Moreover, there is a system level cache (SLC) of 48 MB for all CPU cores and GPU
    cores.


    What makes the Apple SoC unique is that the CPU and GPU are sharing the same memories
    and there is a single SLC for both CPU and GPU. Such a design enables closer collaboration
    between CPUs and GPUs. Table [II](#page-1-1) illustrates some of the difference
    between Apple SoC and systems with discrete GPUs. The close connection between
    CPU and GPU in Apple SoC enables collaborated CPU-GPU algorithms with frequent
    communication between CPU and GPU.


    <span id="page-1-1"></span>TABLE II COMPARISON OF APPLE SOC AND SYSTEMS OF CPU
    WITH DISCRETE GPU


    |                    | Apple SoC  | Discrete GPU |

    |--------------------|------------|--------------|

    | CPU-GPU bus        | in-silicon | PCI-E        |

    | Memory type        | sharing    | host/device  |

    | GPU memory latency | low        | high         |


    ![](_page_1_Picture_11.jpeg)


    Fig. 2. A snapshot [\[12\]](#page-5-11) (left) and a sketch of the design (right)
    of Apple M2 Max chip. I-Cache stands for instruction cache, and D-Cache stands
    for data cache. Avalanche and Blizzard are architecture design code names.


    # <span id="page-1-0"></span>*B. Objective-C/Swift Programming Languages and Frameworks*


    The operating systems MacOS for laptops and workstations, and iPadOS for tablets,
    and iOS for mobile phones, and watchOS for watches, and tvOS for home media stations,
    and visionOS for the recently released space computing goggles are delivered with
    user interfaces with distinguished styles. The basic design of such user interfaces
    is dated back to the 1980s, where C++ has not yet been prevailing. Another object-oriented
    language Objective-C [\[13\]](#page-5-12) inspired from the SmallTalk [\[14\]](#page-5-13)
    is adopted by Apple to develop the user interfaces.


    Later, in the last decade, the Swift [\[15\]](#page-5-14) language is further
    proposed for meeting the demand of software developers for an easier to use languages.
    Applications developed in Objective-C or Swift are integrated with system frameworks
    such as Cocoa [\[16\]](#page-5-15) for user interfaces and Metal [\[17\]](#page-5-16)
    for 3D graphics. Figure [3](#page-2-0) illustrates the layers of applications,
    frameworks and OS kernel.


    At the lowest level, Apple computing devices run the Darwin OS kernel [\[18\]](#page-5-17),
    which is different from Linux. Same as Linux, Darwin implements the Portable Operating
    Sys-


    ![](_page_2_Figure_0.jpeg)


    <span id="page-2-0"></span>Fig. 3. A sketch of application development in Objective-C
    & Swift programming language on Apple devices.


    tem Interface (POSIX) [\[19\]](#page-5-18). So migration of lower level applications
    between Linux and Darwin is much easier than that between Linux and Windows, where
    the POSIX has not been completely implemented on Windows. As a side notice, Windows
    has provided the Windows Subsystem for Linux (WSL) [\[20\]](#page-5-19) to provide
    an embedded Linux environment, in order to execute Linux application on Windows.


    ### *C. Metal Shading Language & Framework*


    At the beginning, Apple did not design its own programming languages for GPUs.
    Instead, OpenGL [\[21\]](#page-5-20) and OpenCL [\[22\]](#page-6-0) are adopted,
    which are open standards conceived by many vendors.


    However, as the Apple GPUs get more powerful, the OpenGL and OpenCL have been
    not able to keep the pace of increased hardware features provided by Apple chips.
    So, the Metal Shading Language(MSL) [\[17\]](#page-5-16) has been proposed.


    Applications written in MSL rely on toolchains provided by the Metal framework
    for compilation and execution. Although both MSL and CUDA [\[23\]](#page-6-1)
    C++ are based on C++, there are differences in the code building stages. Figure
    [4](#page-2-1) illustrates the major difference.


    In CUDA, the host code running on CPU and device code running on GPU are combined
    in the same CUDA C++ source code, while in Metal, the host code in Objective-C
    or Swift and device code in Metal are separated. Also, in CUDA the CPU and GPU
    binaries are packed in a single executable, while in Metal, the CPU executable
    will load Metal GPU code in runtime.


    ![](_page_2_Figure_8.jpeg)


    <span id="page-2-1"></span>Fig. 4. A sketch of CPU-GPU program compilation scheme
    on Nvidia and Apple GPU devices.


    ### *D. Apple GPU Programming Patterns*


    Programming with the Metal framework on Apple GPU begins with the creation of
    command queue. Then, create command buffers to submit tasks to GPUs. Each task
    may contain multiple stages. Each stage creates a command encoder, and each GPU
    kernel function binds to a command encoder. After all commands in the buffer are
    encoded, the buffer is committed, so that the GPU starts to execute the commands
    as soon as possible. Figure [5](#page-2-2) illustrates this programming pattern.


    ![](_page_2_Figure_12.jpeg)


    <span id="page-2-2"></span>Fig. 5. Programming patterns for Apple GPU.


    ### III. SORTING ALGORITHMS


    <span id="page-2-4"></span>In this section, the CPU and GPU sorting algorithms
    are discussed and studied on Apple chips.


    ### *A. Summary of Sorting Algorithms on CPU & GPU*


    There are two sorting codes on CPU, which are the C++ standard library (stdlib)
    utility and Intel TBB [\[24\]](#page-6-2) library. The C++ stdlib adopts the Introsort
    algorithm and runs on single thread. The average, best and worse case time complexity
    is O(n log n), where n is the number of elements to sort. The Intel TBB library
    adopts the Quicksort algorithm and supports multi-thread devices. The Quicksort
    algorithm has the same complexity as Introsort, except that the worse case time
    complexity is O(n 2 ). As a side notice, Introsort is a combination of the three
    algorthims: Quicksort, Heapsort, and Insertion sort.


    Because there are no sorting utilities shipped with the Metal framework, an in-house
    code has been implemented using the Bitonic sorting algorithm. The average, best
    and worse case time complexity is O(n log<sup>2</sup> n). The Bitonic algorithm
    requires the data size to be power of 2. Figure [III](#page-2-3) compares the
    CPU and GPU sorting algorithms.


    TABLE III SORTING ALGORITHMS ON CPU & GPU


    <span id="page-2-3"></span>


    | Device | Library                     | Algorithm | Time complexity |

    |--------|-----------------------------|-----------|-----------------|

    | CPU    | C++ Stdlib (single thread ) | Introsort | O(n log n)      |

    | CPU    | Intel TBB (multi-thread)    | Quicksort | O(n log n)      |

    | GPU    | In-house                    | Bitonic   | O(n log2 n)     |


    The time complexity is only a guidance, and the next two subsections propose two
    experiments to illustrate the performance on Apple chips.


    ### *B. Performance of Sorting on Apple Chip*


    *1) Random Integers:* The first experiment studies the sorting algorithms on an
    array of integers randomly sampled. If there are n integers, then each integer
    is sampled using a uniform distribution between 0 and n − 1. Figure [6](#page-3-0)
    compares the time cost for sorting integer arrays with size from 2 9 to 2 24 .


    ![](_page_3_Figure_2.jpeg)


    <span id="page-3-0"></span>Fig. 6. Comparison of time cost for sorting integer
    arrays with size from 2 9 to 2 24


    On Log-Log scale, the plot of time cost versus data size appears as straight lines.
    On GPU, this ''straight line'' appearance does not extend well below 10<sup>5</sup>
    . This is because of the GPU execution overhead. Notice that the time measured
    is purely the GPU execution cost, not including the GPU kernel launch cost.


    *2) Partially Sorted Integers:* It worths notice that the performance of sorting
    is limited by memory bandwidth. So, for partial sorted data, since there are less
    data move operations than fully random data, some algorithms may perform better.


    To test the performance of sorting of partially sorted integers, it begins with
    an array of fully sorted integers. If the are n integers, then the array is 0,
    1, 2, . . . n − 1. Next, define a ratio of swap r, and randomly swap ⌊nr⌋ pairs
    of integers in the array, with the pair indices randomly sampled. Here, ⌊nr⌋ takes
    the max integer less or equal to nr. Figure [7](#page-3-1) shows the time cost
    for integer arrays of size 2 <sup>23</sup> with ratio of swap r from 10<sup>−</sup><sup>7</sup>
    to 1. When r = 10<sup>−</sup><sup>7</sup> , there are no swaps, so the ratio of
    swap is essentially 0.


    When the number of swaps varies, the GPU Bitonic algorithm performance keeps nearly
    the same, but the CPU algorithms drastically varies. When there are less than
    10<sup>−</sup><sup>5</sup> of elements are swapped, CPU performs better than GPU.


    ### *C. Sorting Strategy for Monte Carlo Neutron Transport*


    The particle sorting algorithm is important for accelerating Monte Carlo neutron
    transport on GPU. Hamilton [\[1\]](#page-5-0), Joo [\[2\]](#page-5-1), and Tramm
    [\[4\]](#page-5-3) have good summaries, and Liu [\[25\]](#page-6-3) discusses
    the sorting algorithms on Apple computing devices.


    ![](_page_3_Figure_10.jpeg)


    <span id="page-3-1"></span>Fig. 7. Comparison of time cost for sorting integer
    arrays with size 2 <sup>23</sup> and ratio of swaps from 0 to 1.


    ### IV. REACTOR SIMULATION BENCHMARKS


    The previous discussion of sorting algorithm on integer arrays is limited, and
    the results may not reflect the situation of reactor physics simulation. In this
    section, the VERA pincell and assembly problems [\[26\]](#page-6-4) are simulated
    to verify the correctness of the program. Then the ExaSMR [\[27\]](#page-6-5)
    whole core and HTR-10 [\[28\]](#page-6-6) fuel pebble benchmark problems are simulated
    to study the performance.


    ### *A. Simulation Configuration*


    The in-house code on GPU uses 32-bit floating point number since Apple GPUs only
    support 32-bit floating point numbers. Instead, OpenMC uses 64-bit floating point
    numbers.


    The cross sections are prepared in an optimal set of 13 temperatures for the kernel
    reconstruction Doppler broadening method, which is suitable for neutron transport
    in continuously variable media [\[9\]](#page-5-8). For OpenMC, cross sections
    at the same set of temperatures are used, and the 2-point interpolation Doppler
    broadening method is used.


    The in-house code tallies flux of a 23-group structure and the power. OpenMC code
    tallies nothing. Table [IV](#page-3-2) summarizes the simulation configuration.


    TABLE IV SIMULATION CONFIGURATION FOR NEUTRON TRANSPORT


    <span id="page-3-2"></span>


    |                                                                                     |
    OpenMC Code                                                                                                                                     |  |

    |-------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------|--|

    |                                                                                     |
    64-bit (double)                                                                                                                                 |  |

    |                                                                                     |
    turn off                                                                                                                                        |  |

    |                                                                                     |
    turn off                                                                                                                                        |  |

    |                                                                                     |
    turn off                                                                                                                                        |  |

    | Cross section temperatures (K)<br>300, 304.252, 338.681, 412.408, 530.512, 705.793,
    |                                                                                                                                                 |  |

    | 951.89, 1283.538, 1704.703, 2189.43, 2653.095,                                      |                                                                                                                                                 |  |

    | 2950, 3000                                                                          |                                                                                                                                                 |  |

    |                                                                                     |
    2-point linear interpolation                                                                                                                    |  |

    |                                                                                     |
    None                                                                                                                                            |  |

    |                                                                                     |
    ENDF/B-VIII.0                                                                                                                                   |  |

    |                                                                                     |
    In-house Code<br>32-bit (single)<br>turned off<br>turned off<br>turned off<br>kernel
    reconstruction<br>23-group flux and power<br>ENDF/B-VIII.0 |  |


    # *B. Verification: VERA Pincell & Assembly Benchmark Problem*


    In order to verify simulation on Apple GPU, the VERA pincell and assembly benchmark
    problems are studied. Table [V](#page-4-0) compares K-effective values between
    in-house code on Apple M2 Max CPU+GPU and OpenMC code on Apple M2 Max CPU. The
    probability table, thermal scattering, and resonance scattering are not considered.
    There are 1,048,576 particles per cycle with 100 inactive cycles and 200 total
    cycles.


    <span id="page-4-0"></span>TABLE V K-EFFECTIVE OF VERA PINCELL ASSEMBLY BENCHMARK
    PROBLEMS


    |    | In-house     | OpenMC       |    | In-house    | OpenMC      |

    |----|--------------|--------------|----|-------------|-------------|

    |    | CPU+GPU      | CPU only     |    | CPU+GPU     | CPU only    |

    | 1A | 1.18705 (8)  | 1.18805 (8)  | 2E | 1.06910 (7) | 1.06995 (9) |

    | 1B | 1.18190 (9)  | 1.18290 (10) | 2F | 0.97484 (8) | 0.97557 (8) |

    | 1C | 1.17186 (9)  | 1.17257 (9)  | 2G | 0.84713 (6) | 0.84804 (9) |

    | 1D | 1.16345 (10) | 1.16405 (9)  | 2H | 0.78723 (7) | 0.78799 (8) |

    | 1E | 0.77405 (7)  | 0.77529 (6)  | 2I | 1.18092 (8) | 1.18178 (8) |

    | 2A | 1.18315 (8)  | 1.18391 (8)  | 2J | 0.97392 (8) | 0.97481 (8) |

    | 2B | 1.18398 (8)  | 1.18471 (8)  | 2K | 1.02330 (8) | 1.02385 (8) |

    | 2C | 1.17466 (8)  | 1.17532 (9)  | 2L | 1.02126 (7) | 1.02146 (9) |

    | 2D | 1.16689 (8)  | 1.16772 (8)  | 2M | 0.94233 (6) | 0.94209 (9) |


    The GPU code underestimates the K-effective within 100 pcm, and the using of single
    precision floating point numbers play an important role in this discrepancy.


    # *C. Performance Study: ExaSMR Whole Core Benchmark Problem*


    Next, the influence of the sorting on the performance of whole core nuclear reactor
    simulation has been studied with the ExaSMR benchmark problems. Table [VI](#page-4-1)
    summarizes these problems. There are two versions, one contains fresh fuel with
    only 7 nuclides in fuel, and the other one contains depleted fuel with 245 nuclides
    in fuel.


    TABLE VI SUMMARY OF EXASMR WHOLE CORE BENCHMARK SIMULATION


    <span id="page-4-1"></span>


    |                                   | Fresh fuel  | Depleted fuel                   |

    |-----------------------------------|-------------|---------------------------------|

    | Number of nuclides                | 76          | 283                             |

    | Number of nuclides in fuel        | 7           | 245                             |

    | Number of cycles                  |             | 350                             |

    | Number of inactive cycles         |             | 100                             |

    | OpenMC particles per cycle        |             | 20)<br>1,048,576 (2             |

    | In-house code particles per cycle |             | 23)<br>8,388,608 (2             |

    | OpenMC tally                      |             | None                            |

    | In-house code tally               |             | fission power + 23-group fluxes
    |

    | K-effective OpenMC CPU only       | 1.00656 (6) | 1.00660 (5)                     |

    | K-effective In-house CPU+GPU      | 1.00587 (2) | 1.00586 (2)                     |


    The simulation performance is summarized in Table [VII.](#page-4-2) The sorting
    on CPU performs better than sorting on GPU. This attributes to the partially sorted
    order in the particles as studied in Section [III.](#page-2-4) For the fresh fuel
    problem, the in-house code with GPU transport achieves about 3.0 times power efficiency
    that of OpenMC, and about 7.5 times for the depleted fuel problem. The power efficiency
    has been visualized in Figure [8.](#page-4-3)


    <span id="page-4-2"></span>TABLE VII PERFORMANCE OF SORTING FOR EXASMR WHOLE CORE
    BENCHMARK PROBLEMS


    | In-house<br>In-house<br>OpenMC                                 |

    |----------------------------------------------------------------|

    |                                                                |

    | sorting on CPU<br>sorting on GPU                               |

    | active cycles<br>active cycles<br>active cycles                |

    | (particles/s/Watt)<br>(particles/s/Watt)<br>(particles/s/Watt) |

    | Fresh fuel<br>4.5E3<br>3.7E3<br>1.5E3                          |

    | Depleted fuel<br>3.0E3<br>2.5E3<br>4.0E2                       |


    ![](_page_4_Figure_12.jpeg)


    <span id="page-4-3"></span>Fig. 8. Comparison of simulation efficiency in particle
    per second per Watt for the ExaSMR whole core benchmark problem.


    ### *D. Performance Study: Pebble Fuel from HTR-10 Test Reactor*


    In order to verify the influence of sorting algorithms on the performance of simulation
    of emerging high temperature gas reactors, the fuel pebble benchmark problem of
    the HTR-10 test reactor has been studied. High temperature gas reactors have distinguished
    design from the light water reactors, and the previous study of the performance
    with ExaSMR may not apply to HTR-10. The definition of the HTR-10 pebble benchmark
    problem and the simulation configuration and calculated K-effective is summarized
    in Table [VIII.](#page-5-21) The code simulation configuration follows Table [IV.](#page-3-2)
    The material for the HTR-10 pebble problem is given in Appendix [A.](#page-6-7)
    As indicated in Table [IV,](#page-3-2) ENDF/B-VIII.0 is used.


    The simulation performance of both in-house code using CPU and GPU sorting and
    the OpenMC code on CPU is summarized in Table [IX.](#page-5-22) The in-house code
    agrees with OpenMC in terms of K-effective within in 30 pcm (consider 3 standard
    deviations). The K-effective is about 500 pcm lower than the reference [\[29\]](#page-6-8),
    which is possibly attributed to the ignorance of unresolved resonance (URR).


    Same as the ExaSMR whole benchmark, the CPU sorting algorithms perform better
    than GPU sorting on the basis of performance per power. And the in-house code
    is about 270 times more power efficient than OpenMC on CPU for pebble with fresh
    fuel, and 150 times for pebble with depleted fuel.


    TABLE VIII SUMMARY OF HTR-10 BENCHMARK SIMULATION


    <span id="page-5-21"></span>


    |                                   | HTR-10 fuel pebble              |                  |

    |-----------------------------------|---------------------------------|------------------|

    |                                   | Fresh fuel                      | Depleted
    fuel    |

    | Pebble/fuel region radius (cm)    | 3.0/2.5                         |                  |

    | TRISO particles in fuel region    | 8,335                           |                  |

    | TRISO fuel/buffer/PyC1/SiC/PyC2   | 0.025/0.034/0.038/0.0415/0.0455 |                  |

    | layers outer radius (cm)          |                                 |                  |

    | Pebble boundary condition         | Reflected                       |                  |

    | Number of nuclides                | 10                              | 248              |

    | Number of nuclides in fuel        | 5                               | 243              |

    | Temperature                       | 300K                            |                  |

    | OpenMC particles per cycle        | 18)<br>262,144 (2               | 16)<br>65,536
    (2 |

    | In-house code particles per cycle | 1,048,576 (2                    | 20)              |

    | OpenMC tally                      | None                            |                  |

    | In-house code tally               | None                            |                  |

    | K-effective OpenMC CPU only       | 1.68732 (9)                     | 1.68714
    (21)     |

    | K-effective In-house CPU+GPU      | 1.68728 (5)                     | 1.68726
    (5)      |

    | Reference [29]*                   | 1.70534 (13)                    | N/A              |


    \* Assume room temperature with unresolved resonance considered


    <span id="page-5-22"></span>TABLE IX PERFORMANCE OF SORTING FOR HTR-10 BENCHMARK
    PROBLEMS


    |                                          | In-house<br>sorting<br>on CPU | In-house<br>sorting<br>on
    GPU<br>(particles/s/Watt) | OpenMC |

    |------------------------------------------|-------------------------------|-----------------------------------------------------|--------|

    | HTR-10 fuel pebble<br>with fresh fuel    | 1.7E3                         | 1.3E3                                               |
    6.4    |

    | HTR-10 fuel pebble<br>with depleted fuel | 8.6E2                         | 7.7E2                                               |
    5.8    |


    ### V. CONCLUSIONS


    In this work, the influence of particle sorting algorithms on the VERA pin and
    assembly, ExaSMR whole core, and HTR-10 fuel pebble benchmark problems have been
    studied with the Apple unified memory chips with merged CPU and GPU. First, it
    has reviewed the programming details on Apple silicon chips. Second, it has demonstrated
    that with partially sorted data, sorting on Apple M2 Max CPU can outperform GPU.
    Third, it has verified the correctness of the in-house CPU-GPU code with VERA
    pincell and assembly benchmarks. Fourth, it has given evidence that the CPU sort
    is more efficient in power for the ExaSMR whole core benchmark than GPU sort,
    and the in-house CPU-GPU code achieve 3.0 and 7.5 times power efficiency that
    of OpenMC CPU code for the case of fresh and depleted fuel. And finally, it has
    shown that the CPU sort is more efficient in performance per power than GPU sort
    for the HTR-10 fuel pebble benchmark problem, and the in-house CPU-GPU code achieves
    270 and 150 times power efficiency that of OpenMC CPU code for the case of fresh
    and depleted fuel. In the future, when unified memory chips with merged CPU and
    GPU are prevailing, CPU and GPU collaboration methods might be considered for
    Monte Carlo reactor neutron transport method with better power efficiency.


    ### ACKNOWLEDGMENT


    Computing technologies from New Compute Laboratory are used to produce parts of
    the data in this article. New Compute Laboratory & its information providers endeavor
    to ensure the accuracy & reliability of the information provided, but do not guarantee
    completeness or reliability, or that it is up-to-date & accepts no liability (whether
    in tort or contract or otherwise) for any loss or damage, whether direct or indirect,
    arising from errors, inaccuracies or omissions or the information being upto-date.
    Any information provided is at the user''s risk.


    ### REFERENCES


    - <span id="page-5-0"></span>[1] S. Hamilton and T. Evans, "Continuous-energy
    Monte Carlo neutron transport on GPUs in the Shift code," Annuals of Nuclear Energy,
    vol. 128, pp. 236-247, 2019.

    - <span id="page-5-1"></span>[2] N. Choi and H. Joo, "Domain decomposition for
    GPU-based continuous energy Monte Carlo power reactor calculation," Nuclear Engineering
    and Technology, vol. 52, issue 11, pp. 2667-2677, 2020.

    - <span id="page-5-2"></span>[3] K. Gao, Z. Chen, A. Sun and T. Yu, "The research
    and application of GPU-based Monte Carlo Simulation in reactor calculation," Proceedings
    of RPNM2023, Jul. 26-29 Lanzhou China, 2023.

    - <span id="page-5-3"></span>[4] J. Tramm, P. Romano, J. Doerfert, A. Lund, P.
    Shriwise, A. Siegel, and et. al., "Toward Portable GPU Acceleration of the OpenMC
    Monte Carlo Particle Transport Code," Proceedings of PHYSOR2022, May 15- 20 Pittsburg
    USA. 2022.

    - <span id="page-5-4"></span>[5] R. Bergmman, J. Vujic, "Algorithmic choices in
    WARP - A framework ´ for continuous energy Monte Carlo neutron transport in general
    3D geometries on GPUs," Annuals of Nuclear Energy, vol. 77, pp. 176– 193, 2015.

    - <span id="page-5-5"></span>[6] Sony Playstation 5, https://www.playstation.com/en-us/ps5/
    (Last retrieved: Jan. 21, 2024)

    - <span id="page-5-6"></span>[7] AMD Instinct™ MI300A Accelerators, https://www.amd.com/en/products/accelerators/instinct/mi300/mi300a.html
    (Last retrieved: Jan. 21, 2024)

    - <span id="page-5-7"></span>[8] NVIDIA Grace Hopper Superchip, https://www.nvidia.com/en-us/datacenter/grace-hopper-superchip/
    (Last retrieved: Jan. 21, 2024)

    - <span id="page-5-8"></span>[9] C. Liu, "Doppler broadening using discrete cosine
    transform and kernel reconstruction for spatially variable media," Annuals of
    Nuclear Energy, vol. 174, pp. 109150, 2012.

    - <span id="page-5-9"></span>[10] P. Romano, J. Tramm, P. Shriwise, "Impact of
    Asymmetric Multicore Processors on Monte Carlo Particle Transport Code Performance,"
    Proceedings of M&C 2023 (394), Aug. 13-17 Niagara Falls Canada, 2023.

    - <span id="page-5-10"></span>[11] J. Tramm, K. Yoshii, P. Romano, "Power at Your
    Fingertips: Assessing the Performance of a Monte Carlo Neutron Transport Mini-App
    on Consumer Laptop GPUs," Proceedings of M&C 2023 (433), Aug. 13-17 Niagara Falls
    Canada, 2023.

    - <span id="page-5-11"></span>[12] Apple unveils M2 Pro and M2 Max: nextgeneration
    chips for next-level workflows, https://www.apple.com/newsroom/images/product/mac/standard/Apple-M2-chips-M2-Max-230117.zip
    (Last retrieved: Jan. 21, 2024)

    - <span id="page-5-12"></span>[13] About Objective-C, https://developer.apple.com/library/archive/
    documentation/Cocoa/Conceptual/ProgrammingWithObjectiveC/Introduction/ Introduction.html
    (Last retrieved: Jan. 21, 2024)

    - <span id="page-5-13"></span>[14] GNU Smalltalk, https://www.gnu.org/software/smalltalk/
    (Last retrieved: Jan. 21, 2024)

    - <span id="page-5-14"></span>[15] Swift: the powerful programming language that''s
    also easy to learn, https://developer.apple.com/swift/ (Last retrieved: Jan. 21,
    2024)

    - <span id="page-5-15"></span>[16] What Is Cocoa?, https://developer.apple.com/library/archive/
    documentation/Cocoa/Conceptual/CocoaFundamentals/WhatIsCocoa/ WhatIsCocoa.html
    (Last retrieved: Jan. 21, 2024)

    - <span id="page-5-16"></span>[17] Accelerate graphics and much more with Metal,
    https://developer.apple.com/metal/ (Last retrieved: Jan. 21, 2024)

    - <span id="page-5-17"></span>[18] Kernel Architecture Overview, https://developer.apple.com/library/archive/
    documentation/Darwin/Conceptual/KernelProgramming/Architecture/ Architecture.html
    (Last retrieved: Jan. 21, 2024)

    - <span id="page-5-18"></span>[19] POSIX (The Portable Operating System Interface),
    https://www.gnu.org/software/libc/manual/html node/POSIX.html (Last retrieved:
    Mar. 14, 2024)

    - <span id="page-5-19"></span>[20] How to install Linux on Windows with WSL, https://learn.microsoft.com/en-us/windows/wsl/install
    (Last retrieved: Mar. 14, 2024)

    - <span id="page-5-20"></span>[21] OpenGL: The Industry''s Foundation for High
    Performance Graphics, https://www.opengl.org (Last retrieved: Jan. 21, 2024)

    - <span id="page-6-0"></span>[22] OpenCL: Open Standard for Parallel Programming
    of Heterogeneous Systems, https://www.khronos.org/opencl/ (Last retrieved: Jan.
    21, 2024)

    - <span id="page-6-1"></span>[23] CUDA Toolkit, https://developer.nvidia.com/cuda-toolkit
    (Last retrieved: Jan. 21, 2024)

    - <span id="page-6-2"></span>[24] Intel® oneAPI Threading Building Blocks, https://www.intel.com/content/www/us/en/developer/tools/oneapi/onetbb.html
    (Last retrieved: Mar. 14, 2024)

    - <span id="page-6-3"></span>[25] C. Liu, "Monte Carlo neutron transport using
    low power mobile GPU devices", Arxiv, https://arxiv.org/abs/2208.06296, 2022

    - <span id="page-6-4"></span>[26] B. Godfrey, "VERA core physics benchmark progression
    problem specifications, revision 4," CASL technical report CASL-U-2012-0131- 004,
    2014.

    - <span id="page-6-5"></span>[27] E. Merzari, S. Hamilton, T. Evans, M. Min and
    et. al., "Exascale Multiphysics Nuclear Reactor Simulations for Advanced Designs,"
    Proceedings of SC23, Nov. 12-17 Denver USA, https://doi.org/10.1145/3581784.3627038,
    2023

    - <span id="page-6-6"></span>[28] International Handbook of Reactor Physics Experiments,
    "Evaluation of the Initial Critical Configuration of the HTR-10 Pebble-Bed Reactor,"
    HTR10-GCR-RESR-001, NEA/NSC/DOC(2006)1, Rev. 0., 2006

    - <span id="page-6-8"></span>[29] Y. Cheng, C. Hao and F. Li, "Uncertainty quantification
    of fuel pebble model and its effect on the uncertainty propagation of nuclear
    data in pebble bed HTR," Annuals of Nuclear Energy, vol. 139, pp. 107286, 2020.


    ### <span id="page-6-7"></span>APPENDIX


    The definitions of materials used in the HTR-10 pebble benchmark are summarized
    in Table [X.](#page-6-9)


    <span id="page-6-9"></span>


    | TABLE X                                                    |  |

    |------------------------------------------------------------|--|

    | DEFINITIONS OF MATERIALS IN THE HTR-10 PEBBLE PROBLEM WITH |  |

    | FRESH FUEL                                                 |  |


    | Material               | Nuclide | Atomic density (1024cm−3<br>) |

    |------------------------|---------|-------------------------------|

    | Pebble Carbon matrix   | B-10    | 2.49298E-8                    |

    |                        | B-11    | 1.00345E-7                    |

    |                        | C-12    | 8.57768E-2                    |

    |                        | C-13    | 9.60880E-4                    |

    | Fuel kernel            | B-10    | 4.06384E-7                    |

    |                        | B-11    | 1.63575E-6                    |

    |                        | O-16    | 4.64720E-2                    |

    |                        | U-235   | 3.99198E-3                    |

    |                        | U-238   | 1.92441E-2                    |

    | Buffer                 | B-10    | 1.58513E-8                    |

    |                        | B-11    | 6.38035E-8                    |

    |                        | C-12    | 5.45401E-2                    |

    |                        | C-13    | 6.10964E-4                    |

    | Pyrolytic Carbon (PyC) | B-10    | 2.73795E-8                    |

    | inner and outer        | B-11    | 1.10206E-7                    |

    |                        | C-12    | 9.42057E-2                    |

    |                        | C-13    | 1.05530E-3                    |

    | Silicon Carbide (SiC)  | C-12    | 4.72306E-2                    |

    |                        | C-13    | 5.29082E-4                    |

    |                        | Si-28   | 4.40486E-2                    |

    |                        | Si-29   | 2.23666E-3                    |

    |                        | Si-30   | 1.47442E-3                    |'
  decisions:
    evaluation_prompt: 'Qualified. Reason: The paper includes tables, discusses verification
      and comparison of methods, and mentions evaluation of sorting performance and
      simulation power efficiency.'
    related_work_prompt: 'Qualified. Reason: The paper engages with prior research
      by citing multiple studies, discussing their contributions, and comparing different
      approaches in the context of the Monte Carlo method and GPU performance.'
    novelty_prompt: 'Qualified. Reason: The paper proposes using Apple unified memory
      computing devices for Monte Carlo neutron transport methods, which is a new
      application of existing techniques.'
    review_only_prompt: 'Qualified. Reason: The paper introduces new contributions
      related to the use of Apple unified memory computing devices for Monte Carlo
      neutron transport methods, rather than primarily summarizing existing work.'
- title: "An Irredundant and Compressed Data Layout to Optimize Bandwidth\n  Utilization\
    \ of FPGA Accelerators"
  abstract: 'Memory bandwidth is known to be a performance bottleneck for FPGA

    accelerators, especially when they deal with large multi-dimensional data-sets.

    A large body of work focuses on reducing of off-chip transfers, but few authors

    try to improve the efficiency of transfers. This paper addresses the later

    issue by proposing (i) a compiler-based approach to accelerator''s data layout

    to maximize contiguous access to off-chip memory, and (ii) data packing and

    runtime compression techniques that take advantage of this layout to further

    improve memory performance. We show that our approach can decrease the I/O

    cycles up to $7\times$ compared to un-optimized memory accesses.'
  url: http://arxiv.org/abs/2401.12071v1
  keywords: ''
  document: "Corentin Ferry Univ Rennes, CNRS, Inria, IRISA Rennes, France cferry@mail.colostate.edu\n\
    \nSteven Derrien Univ Rennes, CNRS, Inria, IRISA Rennes, France Steven.Derrien@irisa.fr\n\
    \n#### ABSTRACT\n\nMemory bandwidth is known to be a performance bottleneck for\
    \ FPGA accelerators, especially when they deal with large multidimensional data-sets.\
    \ A large body of work focuses on reducing of off-chip transfers, but few authors\
    \ try to improve the efficiency of transfers. This paper addresses the later issue\
    \ by proposing (i) a compiler-based approach to accelerator's data layout to maximize\
    \ contiguous access to off-chip memory, and (ii) data packing and runtime compression\
    \ techniques that take advantage of this layout to further improve memory performance.\
    \ We show that our approach can decrease the I/O cycles up to 7× compared to un-optimized\
    \ memory accesses.\n\n#### KEYWORDS\n\nmemory access, redundancy, data packing,\
    \ padding, arbitrary precision, memory allocation\n\n#### 1 INTRODUCTION\n\nFPGA\
    \ accelerators have gained significant popularity in recent years, despite their\
    \ inherent programming complexity. High-Level Synthesis tools play a pivotal role\
    \ in reducing the design challenges associated with FPGA acceleration, and facilitate\
    \ their adoption in new application domains (e.g. machine learning).\n\nFPGA accelerator\
    \ boards offer massive computational capabilities, but their performance is often\
    \ hindered by an under-performing memory system, which becomes a performance bottleneck\
    \ [\\[16\\]](#page-10-0). This is especially true for accelerators that target\
    \ compute intensive kernels operating on large data-sets. This issue is generally\
    \ addressed through program transformations that increase temporal reuse, trading\
    \ off-chip memory transfers for on-chip storage resource. However, this approach\
    \ does not primarily seek to optimize bandwidth usage (i.e. total amount of data\
    \ transferred), leaving room for further improvement.\n\nAnother approach consists\
    \ in improving the effectiveness of the memory subsystem by reorganizing access\
    \ patterns and data layout in order to exploit FPGA-specific constraints[\\[17\\\
    ]](#page-10-1). One classical way of doing so is by exploiting large burst-based\
    \ transfers, which requires contiguous data in memory. This is however not easy\
    \ when dealing with the multi-dimensional data-sets found in\n\nNicolas Derumigny\
    \ Colorado State University, Univ. Grenoble Alpes, Inria, CNRS, Grenoble INP,\
    \ LIG 38000 Grenoble, France nicolas.derumigny@inria.fr\n\nSanjay Rajopadhye Colorado\
    \ State University Fort Collins, CO, USA First.Last@colostate.edu\n\nmany applications,\
    \ since the usual row-major and/or column major layouts only guarantee contiguity\
    \ of data in only one dimension.\n\nThe problem becomes even more difficult when\
    \ considering custom data formats (fixed/floating point) whose bitwidth do not\
    \ correspond to the native memory bus interface. In such cases, the designer is\
    \ left with two choices : padding the format to fit the bus width or deal with\
    \ misaligned access, both choices incurring a loss of effective bandwidth.\n\n\
    Nevertheless, the ability to design application-specific hardware also brings\
    \ opportunities to improve bandwidth efficiency. For example, the fact that successive\
    \ and/or ajacent values are numerically close (typical case in physical simulation)\
    \ makes runtime compression a viable strategy to increase the effectiveness of\
    \ memory transfers.\n\nIn this paper, we present an automatic HLS optimisation\
    \ flow that combines contiguity, data packing and compression to maximize the\
    \ utilization of bandwidth with custom data types. More precisely, our contributions\
    \ are the following:\n\n- an algorithm to automatically derive (i) burst-friendly\
    \ data layouts, and (ii) accelerator-specific access patterns that maximize contiguity\
    \ while enabling data packing and compression,\n- an automated code generation\
    \ framework implementing the algorithm that generates synthesizable hardware,\n\
    - an evaluation of our approach on FPGA accelerators generated using the code\
    \ generator that shows a up to 7× decrease in I/O cycles.\n\nThis paper is organized\
    \ as follows: Section [2](#page-0-0) presents the concepts and core optimization\
    \ techniques our work is relying on; Section [3](#page-3-0) describes the memory\
    \ layout transformation, and Section [4](#page-5-0) explains how we automatically\
    \ apply it to FPGA accelerators. Finally, Section [5](#page-7-0) validates our\
    \ approach and discusses it on a series of benchmarks.\n\n#### <span id=\"page-0-0\"\
    ></span>2 BACKGROUND\n\n#### <span id=\"page-0-1\"></span>2.1 Locality optimizations\n\
    \nAs manually optimizing an HLS design at the source level is a tedious process,\
    \ automated approaches are now routinely used for HLS/FPGA targets in order to\
    \ exploit parallelism and locality at\n\n<span id=\"page-1-0\"></span>![](_page_1_Figure_1.jpeg)\n\
    \nFigure 1: Domain of the Jacobi stencil divided into tiles of size 6 × 6. Each\
    \ tile contains 18 (,) points corresponding to 18 computations of , s.\n\n<span\
    \ id=\"page-1-3\"></span>![](_page_1_Figure_3.jpeg)\n\nFigure 2: Compiler flow\
    \ (our contributions in green)\n\nmultiple levels, which are often implemented\
    \ as source-to-source compilers [\\[5,](#page-10-2) [14,](#page-10-3) [15,](#page-10-4)\
    \ [28\\]](#page-10-5). Due to the inherent regularity of HLS-valid code, loop\
    \ transformations engines such as PolyOpt/HLS [\\[21\\]](#page-10-6) and POLSCA\
    \ [\\[29\\]](#page-10-7) excel at this task. However, such transformations are\
    \ calibrated to improve computation time of benchmarks and do not seek to change\
    \ the memory layout to enforce memory access contiguity.\n\nIndeed, to get the\
    \ best runtime performance from an FPGA accelerator, it is necessary to limit\
    \ its off-chip memory accesses as much as possible. Only a fraction of large data\
    \ sets can fit the limited size of on-chip memory; programs operating on large\
    \ data sets must therefore be transformed to work on smaller workloads at a time.\
    \ Loop tiling does this: it breaks large spaces into smaller subproblems called\
    \ tiles, where the on-chip memory and parallelism requirements of each tile match\
    \ those available on the chip.\n\nAn accelerator for a tiled program processes\
    \ the domain tile by tile. To execute a tile, the accelerator needs to retrieve\
    \ intermediate results from previously executed tiles. These intermediate results\
    \ are located outside of the accelerator, in off-chip memory, and need to be copied\
    \ into on-chip memory.\n\nThe amount of on-chip memory needed to run a tile is\
    \ directly influenced by the tile's shape and size. In addition, when the tile\
    \ size increases, the overall off-chip memory access is reduced, thus improving\
    \ the overall arithmetic Intensity. Selection of the best tile\n\n<span id=\"\
    page-1-1\"></span>![](_page_1_Figure_9.jpeg)\n\nFigure 3: Inter-tile communication\
    \ pattern for the Jacobi stencil: red arrows indicate data input into the tile\
    \ shown in the center, and blue arrows indicate data output from this tile.\n\n\
    shape and size is outside the scope of this work, and mainly depends on the performance\
    \ / area trade-off desired by the designer.\n\n#### <span id=\"page-1-2\"></span>2.2\
    \ Illustrative example: 1D Jacobi stencil\n\nTo illustrate the flow proposed in\
    \ this paper, we propose a Jacobi-1D stencil as running example. This kernel updates\
    \ a one-dimensional sequence of values, and computes each point as a weighted\
    \ average of it and its neighbors:\n\n$$c\\_{t+1,i} = \\frac{1}{3} \\left( c\\\
    _{t,i-1} + c\\_{t,i} + c\\_{t,i+1} \\right)^2$$\n\nA C implementation of this\
    \ stencil is provided in the Poly-Bench/C suite as the following code:\n\n```\n\
    1 for (t = 0; t < _PB_TSTEPS ; t ++) {\n2 for (i = 1; i < _PB_N - 1; i ++)\n3\
    \ B[i] = 0.33 * (A[i -1] + A[i] + A[i +1]) ;\n4 for (i = 1; i < _PB_N - 1; i ++)\n\
    5 A[i] = 0.33 * (B[i -1] + B[i] + B[i +1]) ;\n6 }\n```\nThis stencil operates\
    \ over a two-dimensional iteration domain × where each point has a coordinate\
    \ (,). Because such a domain may be arbitrarily large, the whole dataset may not\
    \ fit into FPGA on-chip memory, and needs to be optimized before it can be mapped\
    \ to the FPGA. As stated in the previous subsection, this naive implementation\
    \ of Jacobi-1D cannot fit on-chip for gigabytescale problem sizes, thus requiring\
    \ tiling. For the sake of simplicity of the illustration, we have chosen small,\
    \ diamond-shaped tiles, illustrated in Figure [1.](#page-1-0)\n\nFor this tiling\
    \ scheme, intermediate results to be retrieved come from the tiles located below\
    \ the tile to execute; in Figure [3,](#page-1-1) these tiles are designated as\
    \ the source of incoming arrows into the tile to execute. Likewise, the outgoing\
    \ arrows show those intermediate results that will be used by other neighboring\
    \ tiles. All of these data\n\n<span id=\"page-2-0\"></span>![](_page_2_Figure_1.jpeg)\n\
    \nFigure 4: Macro-pipeline structure: read-execute-write. Our contribution focuses\
    \ on the read and write stages.\n\ntransfers are the ones this work seeks to optimize;\
    \ improvements of the compute engine fall out of the scope of this paper.\n\n\
    #### 2.3 Deriving parallel accelerators using HLS\n\nThe optimizations mentioned\
    \ section [2.1](#page-0-1) are only a part of all those optimizations that need\
    \ to be applied to get the best performance. One also needs to extract parallelism\
    \ to maximize utilization of operators on the FPGA, and create a macro-pipeline\
    \ to maximize the compute throughput.\n\nLoop tiling naturally yields a \"read-execute-writeback\"\
    \ macropipeline structure as illustrated in Figure [4:](#page-2-0) because tiles\
    \ can be executed atomically, all I/O operations can happen before and after execution.\
    \ HLS tools such as Vitis HLS support such macropipelines through manual code\
    \ annotation, but through a restricted set of conditions of the pipeline (i.e.\
    \ absence of cyclic dependency between stages). Moreover, automated macro-pipelining\
    \ further increase pressure on the on-chip memory usage, as buffers used for inter-stage\
    \ communication are either implemented using FIFOs or duplicated. While standard\
    \ coding techniques would pass the complete tile data buffer across pipeline stages,\
    \ this widely inefficient in hardware as communication only requires a subset\
    \ of the actual tile data. On one tile of our Jacobi-1D example from subsection\
    \ [2.2,](#page-1-2) the required data to be communicated is represented in blue\
    \ in Figure [6.](#page-3-1)\n\nUsually, the goal of the execute stage is to take\
    \ advantage of massive operation-level parallelism (thanks to loop pipelining\
    \ and unrolling), that have already been extensively addressed [\\[4,](#page-10-8)\
    \ [21,](#page-10-6) [22,](#page-10-9) [25\\]](#page-10-10). In this paper, we\
    \ only seek to optimize transfer times and memory bandwidth usage; optimisation\
    \ of the complete design including crafting and balancing of a coarse-grain pipeline\
    \ are not evaluated.\n\nIn applications with low operational intensity, the limited\
    \ offchip memory bandwidth turns the read and write stages into performance bottlenecks,\
    \ even with aggressive tiling transformations. In most cases, this is due to a\
    \ poor utilization of the off-chip memory interface, where only a fraction of\
    \ the peak bandwidth is effectively used due to inefficient access patterns. As\
    \ a matter of fact, approaching the peak memory bandwidth requires that almost\
    \ all access to external memory consist of large transfers over contiguous memory\
    \ locations (called memory burst).\n\nHLS tools can infer burst memory accesses\
    \ depending on the target interface. In the case of a shared bus (e.g. AXI, PCIe),\
    \ which is commonly found for off-chip accesses, a burst access may occur\n\n\
    <span id=\"page-2-1\"></span>![](_page_2_Figure_10.jpeg)\n\nFigure 5: Data packing\
    \ and compression reduce storage and transfer redundancy at the expense of address\
    \ alignment and, for compression, predictability of addresses.\n\nif the bus supports\
    \ it and the compiler recognizes access to a series of consecutive addresses.\
    \ Tools such as Vitis HLS 2022.2 exploit this using with either a call to a HLS-specific\
    \ memcpy routine, or through some form or pattern matching in the source code.\
    \ In burst mode, no cycle is spent stalling for a new value after a one-shot initialization\
    \ latency, which yields full utilization of the available bandwidth.\n\nThe goal\
    \ of this work is to propose a source level compiler optimisation to (i) reorganize\
    \ data in memory to enable contiguous burst access and (ii) further improve bandwidth\
    \ utilization through packing and compression. Our optimization pass is meant\
    \ to be integrated within an HLS polyhedral compilation flow, as illustrated in\
    \ Figure [2;](#page-1-3) aiming at sitting between the locality optimization phase\
    \ (tiling) and the HLS synthesis stage. In fact, our approach does not replace\
    \ locality optimizations, it complements them.\n\n#### 2.4 Padding vs packing\n\
    \nIn order to maximize the utilization of bandwidth, every bit of data transmitted\
    \ must be useful. However, with domain-specific data types (e.g., custom fixed\
    \ point), unused bits must usually be transmitted due to memory alignment requirements.\
    \ In the following, we explain how data contiguity can be leveraged to two ways:\
    \ packing data to reduce the unused bits transmitted; and compressing data to\
    \ further save bandwidth.\n\nMost memories are byte-addressable and most processor\
    \ architectures also require aligned accesses at word boundaries, usually at 32\
    \ or 64 bits. Although FPGA accelerators can operate on arbitraryprecision data\
    \ types, off-chip data transfers must abide by the addressing requirements of\
    \ the external memory. They therefore need to pad the incoming and outgoing data:\
    \ in practice, for a 17-bit access, 32 bits of data will be transferred, 15 bits\
    \ of them being wasted in padding.\n\nNote that padding is necessary to enable\
    \ random accesses to data: it provides the guarantee that a given memory cell\
    \ contains only the requested data and no manipulation needs to be done to extract\
    \ it. Data accessed in a contiguous manner does not need this guarantee and may\
    \ overlap multiple adjacent cells, as simple wire manipulations on the FPGA will\
    \ give back the original data.\n\nData packing, as illustrated in Figure [5,](#page-2-1)\
    \ consists in avoiding padding the data so that words are adjacent at the bit\
    \ level in memory. Figure [5](#page-2-1) shows buffer structure for unpacked and\
    \ packed data of 17 bits in 32-bit words. Unpacked data has aligned addresses,\
    \ but requires extra storage and transfers unused data; packed data has unaligned\
    \ addresses but saves storage and avoids some redundant bits from being transmitted.\
    \ It becomes however impossible to randomly seek in a packed stream due to misalignment\
    \ without additional data processing, but by definition such random seeks do not\
    \ happen with contiguous accesses.\n\nIn our approach, we leverage contiguous\
    \ accesses to (i) avoid the adverse effects of packing induced misalignment and\
    \ (ii) to maximize bandwidth utilization by not padding data.\n\n#### <span id=\"\
    page-3-4\"></span>2.5 Runtime data compression\n\nPacking data saves bandwidth\
    \ by eliminating the padding bits, and is applied independently of the data itself.\
    \ However, further optimisation is possible by exploiting properties of the data\
    \ (e.g., correlation between integers in an array) for compression. When this\
    \ technique is applied right before / after off-chip communications, the design\
    \ benefits from a reduction of I/O cycles (as the amount of data transferred is\
    \ reduced) without increase of the computation subsystem as the latency of the\
    \ compression module can be hidden by the pipelining structure\n\nCompression\
    \ is easy to apply to contiguous streams of data, but is not to data where indexed\
    \ or random accesses are necessary. We must exhibit access contiguity, as it is\
    \ in general impossible to seek within a compressed block without decompressing\
    \ more data than needed. Figure [5](#page-2-1) shows that the position of data\
    \ within a compressed block is unpredictable.\n\nOur approach performs runtime\
    \ compression and, to maximize its efficiency, creates data blocks with a contiguous\
    \ access guarantee to ensure every decompressed piece of data is used.\n\nIn general,\
    \ the compression algorithm is domain-specific, e.g., ADPCM for voice [\\[6\\\
    ]](#page-10-11) or JPEG for images [\\[24\\]](#page-10-12). For FPGA implementations,\
    \ the choice of the algorithm is also driven by its throughput: compression and\
    \ decompression must be able to sustain the input and output throughput not to\
    \ become the bottleneck. We choose to illustrate the idea with a simple differential\
    \ compression algorithm which encodes a sequence 0<sup>1</sup> . . . of -bit words\
    \ as follows:\n\n- Encode <sup>0</sup> as is.\n- For 1 ⩽ ⩽ :\n\t- (1) Compute\
    \ Δ = − −1,\n\t- (2) Let be the number of leading zeroes of Δ if Δ ⩾ 0, or leading\
    \ ones if Δ < 0,\n\t- (3) Encode <sup>−</sup> using ⌊1 <sup>+</sup> log<sup>2</sup>\
    \ ()⌋ bits, followed by the sign bit of Δ,\n\t- (4) Encode the − ( + 1) lowest\
    \ bits of Δ.\n\nThis technique is especially effective when the distribution of\
    \ the transferred data is not spread, typically on benchmarks based on the computation\
    \ of the average such as our Jacobi-1D example from subsection [2.2.](#page-1-2)\n\
    \n#### <span id=\"page-3-0\"></span>3 MEMORY LAYOUT OPTIMIZATION\n\nThis work\
    \ seeks to minimize the I/O cycles for an accelerator to transmit and retrieve\
    \ its data into and from global memory. Locality\n\n<span id=\"page-3-1\"></span>![](_page_3_Figure_17.jpeg)\n\
    \nFigure 6: MARS: Groups of points within a tile which data is contiguous in global\
    \ memory. In blue, the MARS produced by the center tile (O1 to O4); in red, the\
    \ MARS consumed by that same tile (I1 to I7).\n\noptimizations having already\
    \ been applied, we do not want to store or retrieve fewer values from memory,\
    \ but rather make better accesses to memory, from the bandwidth utilization, and\
    \ therefore I/O cycles, standpoints.\n\nTo this aim, we build a contiguous, irredundant\
    \ and compressed data layout. This section details the steps taken: first, we\
    \ analyze the program and extract sets of on-chip data as contiguous data blocks;\
    \ second, we lay out these data blocks to obtain further contiguous accesses;\
    \ third, we compress and pack the data blocks together to save even more bandwidth.\n\
    \n#### <span id=\"page-3-3\"></span>3.1 Extracting Contiguous Data Blocks\n\n\
    The first step in our method consists in analyzing a program's behavior with respect\
    \ to memory, to determine which data can/should be grouped together as contiguous\
    \ blocks. The sought groups of data honor two properties:\n\n- Atomicity: If any\
    \ data in the group is needed for an instance of the accelerator's execution flow\
    \ (a tile), then the entire group is also needed for the same tile.\n- Irredundancy:\
    \ No data is retrieved or stored more than once into memory throughout the execution\
    \ of a single tile.\n\nThese groups of data are determined by using the analysis\
    \ technique from Ferry et al. [\\[9\\]](#page-10-13) within a polyhedral compiler.\
    \ This analysis yields sets of on-chip memory addresses, such that all the data\
    \ from these on-chip cells will be allocated a contiguous block of data in off-chip\
    \ memory.\n\n<span id=\"page-3-2\"></span>Example. Applying the MARS analysis\
    \ from [\\[9\\]](#page-10-13) to the Jacobi stencil of Section [2.2](#page-1-2)\
    \ gives the sets of addresses corresponding to the points illustrated in Figure\
    \ [6:](#page-3-1)\n\n• For the input of each tile, seven contiguous blocks of\
    \ data labeled I1 to I7 are to be taken, across three different producer tiles.\n\
    \n• For the output, each tile will produce four contiguous blocks of data labeled\
    \ O1 to O4.\n\nThere is a correspondence between output blocks (MARS) O from a\
    \ tile and input blocks I from other tiles: each O corresponds to one I in several\
    \ other tiles.\n\nWithout any further information, the result of MARS analysis\
    \ would make the accelerator require seven input and four output burst accesses.\
    \ This number could potentially be reduced. If I1, I2 and I3 were adjacent in\
    \ memory, it would be possible to make a single access instead of three, and likewise\
    \ for I5, I6 and I7. The total number of input accesses would go down to just\
    \ three.\n\nIn order to reduce the number of accesses to the above, we have to\
    \ show that it is actually achievable: the blocks I1 through I7 are read by multiple\
    \ tiles, and coalescing opportunities for one tile may be incompatible with another\
    \ tile's coalescing opportunities.\n\nThe next subsection formalizes this example\
    \ into an optimization problem seeking to minimize the number of accesses.\n\n\
    ## <span id=\"page-4-1\"></span>3.2 Enabling Coalesced Accesses across Contiguous\
    \ Data Blocks\n\nFrom the polyhedral analysis of the previous subsection, we have\
    \ determined sets of on-chip data to be grouped as contiguous blocks of data,\
    \ called MARS. How these blocks are laid out in memory is important for access\
    \ performance: if multiple MARS happen to be accessed in a row and they are adjacent\
    \ in memory, the accesses to these MARS can be coalesced into a single access\
    \ and better utilize bandwidth.\n\nThis section explains how the \"outer layout\"\
    \ of the MARS is determined so as to maximize the coalescing opportunities.\n\n\
    <span id=\"page-4-0\"></span>3.2.1 Properties of the layout. The goal of this\
    \ work is to minimize the number of I/O cycles, and therefore the data layout\
    \ must exhibit contiguity (for both reading and writing). However, that contiguity\
    \ must not come at the price of an increase in I/O volume. To model this constraint,\
    \ we apply two hypotheses.\n\nContiguous tile-level allocation. We are looking\
    \ for a layout of MARS in memory, and know that compression will be applied to\
    \ them. Due to the size and position of compressed blocks being unpredictable,\
    \ it is not feasible to interleave MARS from multiple tiles in memory. Therefore,\
    \ we allocate each tile a contiguous block of memory for its MARS output.\n\n\
    This allocation has two consequences: the write side can be done entirely contiguously,\
    \ and we only have to optimize contiguity at the read side.\n\nIrredundancy of\
    \ storage. Under the previous hypothesis, we want to maximize the coalescing opportunities\
    \ between MARS accesses for the read side only. While it is possible to obtain\
    \ this contiguity by replicating the MARS in multiple layouts, one per consumer,\
    \ doing so would defeat the goal to save I/O cycles. We therefore choose to store\
    \ each MARS only once in memory (irredundant storage).\n\nThe goal is now to find\
    \ a single layout for the MARS produced by each tile, that exhibits as much read-side\
    \ coalescing opportunities as possible. We obtain it throgh an optimization problem\
    \ that is defined in the next subsections.\n\n3.2.2 Example. In the example of\
    \ Section [3.1,](#page-3-2) it appeared that the number of burst accesses could\
    \ go from 7 to 3. Let us show there actually exists a layout achieving these 3\
    \ bursts.\n\nFigure [6](#page-3-1) shows the correspondence between input and\
    \ output MARS:\n\n- I1, I2 and I3 come from the southwest tile, corresponding\
    \ to its O2, O3 and O4 blocks. We would like these three MARS to be contiguous,\
    \ regardless of which relative order, to make a single burst.\n- I4 comes from\
    \ the south tile, corresponding to its O2 block.\n- I5, I6 and I7 come from the\
    \ southeast tile, corresponding to its O1, O2 and O3 blocks. We would also like\
    \ them to be contiguous.\n\nWe do not make any hypothesis on the relative location\
    \ of data from the southwest tile, the south tile and the southeast tile. This\
    \ makes it impossible to obtain fewer than 3 burst accesses.\n\nThe information\
    \ we have at this point can be used as the constraints and objective of an optimization\
    \ problem: we want to maximize the number of contiguities in the layout among\
    \ those desired, under the irredundancy constraint. We provide a solver with the\
    \ following problem:\n\n- Maximize the contiguities among the desired ones: make\
    \ MARS O2, O3 and O4 contiguous in any order, and make MARS O1, O2 and O3 also\
    \ contiguous in any order.\n- Per the hypothesis of Section [3.2.1,](#page-4-0)\
    \ we want a layout of MARS O1, O2, O3 and O4.\n- There can be no fewer read bursts\
    \ than 3.\n\nThe solver returns the following layout of the output MARS for each\
    \ tile: O1, O3, O2, O4.\n\nLooking from the consumers, I1, I2 and I3 (resp. southwest\
    \ O2, O3 and O4) are contiguous; I5, I6 and I7 (southeast O1, O2 and O3) are also\
    \ contiguous. We can therefore coalesce, for each tile, the reads I1, I2 and I3\
    \ into a single burst, and I5, I6 and I7 into another burst, achieving the three\
    \ sought input bursts.\n\n3.2.3 General case. In this section, we lay out the\
    \ blocks of data from the MARS analysis to maximize the coalescing opportunities\
    \ between them.\n\nWith the allocation choice of Sec. [3.2.1,](#page-4-0) writes\
    \ are guaranteed to be done without discontiguity. We therefore lay out the MARS\
    \ to make the read side as contiguous as possible. In other words, we need to\
    \ lay out the MARS in memory so that as many MARS as possible can be read as a\
    \ coalesced burst.\n\nWe propose to model this problem as an Integer Linear Programming\
    \ optimization problem as described in Algorithm [1.](#page-5-1) Intuitively,\
    \ if a pair of MARS is needed by a consumer tile and the two MARS are next to\
    \ each other in memory, then a coalesced access for the two (a \"contiguity\"\
    ) is issued. We therefore seek to maximize the number of such contiguities.\n\n\
    The solution to this optimization problem, given by the solver is an ordered list\
    \ of the MARS produced by each tile, that allows the minimal number of transactions\
    \ to read all MARS input of a tile.\n\nThe layout created in this section honors\
    \ the irredundancy property of the MARS (see Section [3.1\\)](#page-3-3), but\
    \ does not yet take full advantage of their atomicity: the fact that the MARS\
    \ are contiguous\n\nAlgorithm 1: Optimizing the MARS layout\n\nInput: = ( ) :\
    \ = 1, ..., = list of MARS, P = list of producer tiles from which MARS are needed,\
    \ Result: = ordered list of MARS Optimization Variables: let , ∈ {0, 1} = successor\
    \ variables: , = 1 encodes that MARS is immediately before MARS in memory. let\
    \ ∈ {1, . . . , } be a permutation; is the position where MARS will be in the\
    \ final layout. Problem Constants: let ,, be equal to 1 if and (, ∈ [[1, ]]) from\
    \ tile ∈ P are consumed together, 0 otherwise. Maximize #{contiguities}: Í ∈P\
    \ Í =1 Í =1 ≠ ,, , subject to: • ∀ : , = 0 (a MARS is not its own predecessor)\
    \ • ∀ : Í , ⩽ 1 (a MARS has at most 1 precedecessor) • ∀ : Í , ⩽ 1 (a MARS has\
    \ at most 1 successor) • Í Í , = − 1 (number of successor relations) • ∀ : 0 ⩽\
    \ ⩽ − 1 (permutation of length ) • ∀, : ( − = 1) ⇔ (, = 1) (definition of successor)\
    \ • ∀, : ( ≠ ) ⇒ | − | ⩾ 1 (MARS have ≠ positions)\n\nreturn = ( (1) , ..., (\
    \ ) ) (-ordered list of MARS)\n\n<span id=\"page-5-2\"></span>![](_page_5_Figure_4.jpeg)\n\
    \nFigure 7: MARS data shown without compression, with compression (inside the\
    \ MARS) and with MARS compression and packing. Packing the compressed MARS preserves\
    \ the contiguity of coalesced accesses.\n\nblocks of data makes them ideal candidates\
    \ for data packing and compression. This is what we perform in the next subsection.\n\
    \n#### 3.3 Contiguity-Preserving Block Compression\n\nSo far, our approach has\
    \ given a layout of data in memory enabling coalesced accesses to contiguous blocks\
    \ of data produced and consumed by an accelerator. These blocks have an atomicity\
    \ property that we can further exploit to save bandwidth, by applying data packing\
    \ and compression, as illustrated in Figure [7.](#page-5-2)\n\n3.3.1 Combining\
    \ compression and packing. Compressed blocks of data must be considered atomic\
    \ in the sense that no random accesses into them are possible. This atomicity\
    \ property is borne by the MARS, as each MARS data block is entirely used when\
    \ it is accessed, i.e. there are no partial accesses to a MARS.\n\nCompressing\
    \ the MARS reduces the size of the data and therefore saves bandwidth and storage\
    \ space; however, it can also break the contiguity brought by the layout of Section\
    \ [3.2](#page-4-1) as illustrated by Figure [7.](#page-5-2) To preserve it, we\
    \ also apply packing to the compressed MARS, making them immediately adjacent\
    \ to each other in memory. Packing compressed MARS also spares the accelerators\
    \ from unused reads due to padding.\n\n3.3.2 Need to preserve metadata. As the\
    \ size of compressed blocks depends on their data, it is impossible to know the\
    \ exact size of each access. However, the size of a burst access must be known\
    \ prior to the request being issued; additionally, using an estimation of the\
    \ size or an over-approximation would result in unused input data or additional\
    \ requests to fill in missing data.\n\nIn order to be able to exactly fetch the\
    \ right size, it is necessary to keep track of the size of each compressed MARS.\
    \ Moreover, the packing of compressed MARS means that the start of a compressed\
    \ block may be improperly aligned. It is also therefore necessary to keep track\
    \ of the alignment of each MARS for proper decompression. In our implementation,\
    \ bookkeeping is done using on-chip markers that are filled in after each MARS\
    \ is compressed. Details are in Section [4.2.2.](#page-6-0)\n\n<span id=\"page-5-1\"\
    ></span>Packing will cause unused input data to enter; however, its size is bounded\
    \ to one aligned word at the beginning and one aligned word at the end of each\
    \ transaction. This input redundancy is notably independent of the size of the\
    \ MARS.\n\n#### <span id=\"page-5-0\"></span>4 INTEGRATION INTO HLS DESIGN FLOW\n\
    \nIn this section, we show how we transform an HLS accelerator description in\
    \ order to optimize its off-chip memory accesses for bandwidth utilization.\n\n\
    The off-chip data layout and compression proposed in Section [3](#page-3-0) can\
    \ be automatically implemented around the existing description of a tile in HLS.\
    \ The result is a sequence of steps:\n\n- Read MARS layout data and non-MARS input\
    \ data from off-chip memory into on-chip FIFOs,\n- Decompress the input data into\
    \ FIFOs,\n- Dispatch MARS data into on-chip buffers with an allocation suitable\
    \ for computation,\n- Perform the computations onto on-chip buffers,\n- Collect\
    \ MARS output data from the on-chip buffers into FIFOs,\n- Compress the collected\
    \ data,\n- Write back the results into MARS layout in off-chip memory.\n\nThe\
    \ next subsections explain how the complex data structures describing the MARS\
    \ are turned into two simple decompression/dispatch and collect/compression steps.\n\
    \n#### 4.1 From MARS to Collect/Dispatch Functions\n\nThe input and output data\
    \ of each tile is respectively copied into and out of on-chip buffers before the\
    \ tile execution takes place and after it has fully completed. This is the step\
    \ where the data goes from a contiguous layout to a non-contiguous layout (suitable\
    \ for execution) and vice-versa.\n\nImplementing these dispatch and collect steps\
    \ requires to describe each MARS so that the data contained in it is placed into,\
    \ or taken from, the right location in on-chip memory. Before dispatch and after\
    \ collect, the data is located into FIFOs in the contiguous layout.\n\n```\n1\
    \ // MARS Dispatch\n2 for ( int i =0; i <33; ++i) {\n3 // take on - chip address\
    \ from ROM\n4 struct mars_transfert mt = FPGA_MARS_IN_TBL [i];\n5 switch (mt.\
    \ array ) {\n6 // on - chip random write\n7 case MARS_DATA_ENUM ::A: {\n8 marsToMem_A\
    \ (mt.dim0 , mt. dim1 ) ;\n9 break ;\n10 }\n11 case MARS_DATA_ENUM ::B: {\n12\
    \ marsToMem_B (mt.dim0 , mt. dim1 ) ;\n13 break ;\n14 }\n15 }\n16 }\n```\nFigure\
    \ 8: Structure of the MARS dispatch implementation (off-chip to on-chip layout)\n\
    \nMARS can have arbitrary complex shapes, and cannot in general be described using\
    \ simple loops. However, it is possible to fully unroll these loops and obtain\
    \ a list of on-chip addresses for each MARS. Such unrolled lists are placed into\
    \ read-only memories on chip. Iterating through these ROMs as in Figure [8](#page-6-1)\
    \ gives the corresponding addresses. The size of these ROMs is notably only dependent\
    \ on the tile size, and not on the problem size or data type.\n\n#### 4.2 Automatic\
    \ compression\n\nWhen the data is in the contiguous layout in the form of MARS,\
    \ it can be seamlessly compressed and decompressed, and the compressed MARS can\
    \ be packed to preserve contiguity. We explain here the compression, packing and\
    \ decompression steps, along with how the compression metadata is taken care of.\n\
    \n4.2.1 Compressing Data and Packing MARS. The compression step is relatively\
    \ straightforward: the compression module takes its input from the collect step\
    \ FIFO, and generates a compressed stream of data from it. The layout of the data\
    \ in this FIFO is not altered by the compression step. Likewise, the decompression\
    \ step takes a stream of compressed words and decompresses it into a FIFO, which\
    \ is then used by the MARS dispatch step. MARS packing is transparently implemented\
    \ by the compression step: because MARS are provided in a contiguous manner from\
    \ the collect step, the first word of each MARS will be immediately adjacent to\
    \ the last word of the previous MARS in the compressed data stream.\n\nOur compressor,\
    \ which algorithm is given in Section [2.5,](#page-3-4) is pipelined with an initiation\
    \ interval of 1 cycle, despite a loop-carried dependence.\n\nThe difficult part\
    \ to implement is decompression: because not all MARS from a given tile are decompressed,\
    \ we need to be able to seek at the start of a particular MARS. This ability is\
    \ given by metadata described in the next paragraph.\n\n<span id=\"page-6-0\"\
    ></span>4.2.2 Metadata management. The consequence of MARS compression is that\
    \ their size is unknown a priori. To preserve the contiguity of the layout from\
    \ Section [3.2,](#page-4-1) we must avoid padding the compressed MARS to preserve\
    \ alignment, Therefore, the compressed\n\nMARS are packed and immediately adjacent\
    \ to each other in memory.\n\nTo keep track of the position of each MARS, we use\
    \ a data structure with two pieces of information: a coarse-grain position indicating\
    \ how far (in aligned words) to seek, and a fine-grain position marker that specifies\
    \ which bit is the first of the said MARS.\n\nBecause the length of MARS is known\
    \ at compile time and constant across tiles, the position of the markers within\
    \ the uncompressed stream is also constant. Therefore, like the MARS descriptions,\
    \ the positions of markers (i.e. start of each MARS) within the uncompressed stream\
    \ are put into a ROM:\n\n```\n1 # define NB_MARKERS 3\n```\n\n```\n2 # define\
    \ MARKERS {62 , 63 , 64}\n```\nThe markers for the compressed stream are maintained\
    \ within an on-chip cache, which size is specified at synthesis time via a macro:\n\
    \n```\n1 struct compressed_marker < NB_MARS_POS_BITS ,\n       LOG_BUS_WIDTH >\
    \ markers [ COMPRESSION_METADATA_SIZE ][\n       NB_MARKERS ];\n```\nThe allocation\
    \ within this cache is done from the host: registers are used to specify whether\
    \ a tile's MARS are compressed, whether its dependences are, and where the markers\
    \ for its dependences are located. This location depends on the size of the space;\
    \ for the Jacobi stencil, the formula is:\n\n```\n1 unsigned compressionMetadataAllocation\
    \ (\n```\n\n```\n2 int tsteps , int n, int M1 , int M2 , int k1 , int k2) {\n\
    ```\n\n```\n3 return (k2) + M2DEC_FORMULA + M2 * (( k1 - 1) & 0x01) ;\n4 }\n```\n\
    It should be noted that the markers structure is persistent between runs. It is\
    \ updated by the MARS write step and used by the MARS read step. This update prevents\
    \ the current HLS tools from constructing a macro-pipeline (e.g. using the HLS\
    \ DATAFLOW pragma) unless the structure is in a separate module.\n\n#### 4.3 Host/FPGA\
    \ dispatching of tiles\n\nThe FPGA accelerator must have a simple control structure\
    \ to exhibit as much parallelism as possible. Therefore, only full tiles are executed\
    \ on FPGA. Full tiles also all have the same volume of I/O, regardless of their\
    \ position in the iteration space.\n\nPartial tiles, i.e. those that contain space\
    \ boundaries, are run on the host CPU, using the original program's allocation.\
    \ To permit this, data computed on FPGA is taken back from MARS into the original\
    \ program's memory, and MARS are created back from partial tiles results. It can\
    \ be demonstrated that no FPGA tiles need any missing MARS data from partial tiles,\
    \ and therefore there is no issue in writing part of the MARS for these tiles.\n\
    \nThe operations performed to execute a partial tile (on the host) are:\n\n- Read\
    \ MARS from neighboring full tiles that were executed on FPGA, remap their data\
    \ to its original location,\n- Execute the tile's iterations using the original\
    \ allocation,\n- Write back MARS by copying data from the original allocation,\
    \ skipping cells that would be in MARS yet have no producer iteration.\n\nThe\
    \ control flow necessary for compression would significantly lengthen the execution\
    \ of host tiles. Therefore, only tiles which producers and consumers are all executed\
    \ on FPGA will use compression.\n\n#### <span id=\"page-7-0\"></span>5 EVALUATION\n\
    \nWe evaluate our approach with respect to the following questions:\n\n- Compile-time\
    \ performance: How much time does it take to compute the MARS layout?\n- Design\
    \ quality: How does using MARS affect the FPGA accelerator's area consumption?\n\
    - Runtime performance: How much I/O cycles do compressed MARS save with respect\
    \ to a non-MARS memory layout?\n- Applicability: How does the data type, tile\
    \ size and problem size affect the compression ratio?\n\n#### 5.1 Protocol and\
    \ benchmarks\n\n5.1.1 Benchmarks. We have selected the following applications\
    \ from the PolyBench/C suite[\\[20\\]](#page-10-14):\n\n- jacobi-1d: Jacobi 1D\
    \ stencil, as used in the running example;\n- jacobi-2d: Two-dimensional version\
    \ of the Jacobi stencil, exhibiting few and simple MARS;\n- seidel-2d: More complex\
    \ benchmark exhibiting a higher number of MARS with more complex shapes.\n\nLayout\
    \ determination was done using the Gurobi solver (version 10.0.3 build v10.0.3rc0\
    \ (linux64)).\n\nThe data types used are fixed-point numbers (18 bits, 24 bits,\
    \ 28 bits) and floating-point numbers (float, double). We also ran simulations\
    \ with a 12-bit fixed-point data type without synthesizing it, Vitis HLS being\
    \ unable to infer bursts from that data type.\n\nThe chosen applications provide\
    \ a non-MARS data layout in their original code. Because FPGA developers usually\
    \ try to seek burst accesses where possible, we have created two access patterns\
    \ on the non-MARS layout to compare against that try to exhibit bursts:\n\n- A\
    \ minimal access pattern, fetching and storing the exact I/O footprint of the\
    \ tile, letting the HLS tool infer bursts where possible.\n- A rectangular bounding\
    \ box of the accessed data like done in PolyOpt/HLS [\\[21\\]](#page-10-6), which\
    \ description is simple enough to infer only burst accesses.\n\nBoth access patterns\
    \ are generated using a polyhedral code generator available in ISL [\\[27\\]](#page-10-15).\n\
    \n5.1.2 Hardware platform. We used a Xilinx ZCU104 evaluation board, equipped\
    \ with a xczu7ev MPSoC. We ran Pynq 3.0.1 with Linux 5.15 and synthesis was done\
    \ using the Vitis/Vivado suite version 2022.2.2 All benchmarks, are running at\
    \ a clock frequency of 187 MHz and communicate with the off-chip DDR using one\
    \ non cache-coherent AXI HP port.\n\n5.1.3 Protocol. Each benchmark is run for\
    \ each data type, each space size and tile size. Part of the computation is done\
    \ on the host: incomplete tiles are executed on a single thread on the Cortex-A53\n\
    \nCPU of the MPSoC. Transfer cycles are measured only for the FPGA tiles and do\
    \ not account for the host.\n\nCycle measurements are gathered using an on-FPGA\
    \ counter and the area measurements are extracted from Vivado place and route\
    \ reports.\n\nTable [1](#page-8-0) shows the characteristics of each benchmark,\
    \ in terms of number of MARS, and number of bursts after coalescing optimization\
    \ of Sec. [3.2.](#page-4-1)\n\n#### 5.2 Results and discussion\n\n5.2.1 Compile-time\
    \ performance. Table [2](#page-8-1) shows the time it took for each benchmark\
    \ to be run through the layout determination and code generation framework. The\
    \ compilation process does not take more than a few seconds to execute for the\
    \ benchmarks we selected, starting from the polyhedral representation of the program\
    \ to the end of HLS code generation. Notably, the layout determination ILP problem\
    \ only depends on the number of MARS and is independent of the tile size.\n\n\
    5.2.2 Design quality. Figure [9](#page-8-2) shows the total area occupied by our\
    \ benchmarks, with respect to the different memory allocation baselines. One tile\
    \ size per benchmark is considered.\n\nMARS introduces extra control logic and\
    \ extra I/O functions that the other baselines do not have. It is therefore normal\
    \ to observe area increases with this baseline. The most significant increases\
    \ in Figure [9a](#page-8-2) are for jacobi1d; in this benchmark, on-chip arrays\
    \ are implemented in logic instead of Block RAM. Figure [9b](#page-8-2) shows\
    \ little DSP and BRAM consumption by this benchmark compared to others. FIFOs\
    \ holding all the MARS are implemented only on the MARS baseline, and require\
    \ extra BRAMs. The extra DSP blocks for MARS baselines come from the address computations\
    \ that are performed inside the I/O units; the size of the space is passed as\
    \ a parameter instead of being a constant, requiring true multipliers.\n\nFigure\
    \ [9a](#page-8-2) shows that the data width causes the logic area to increase\
    \ with it. This increase is more sensible in jacobi1d where the on-chip arrays\
    \ are implemented in logic instead of Block RAM, which effect is also visible\
    \ in Figure [9b.](#page-8-2)\n\n5.2.3 Runtime performance. Figure [10](#page-8-3)\
    \ shows the transfer time relative to compressed MARS for each data type and each\
    \ benchmark.\n\nImpact of dimensionality. For the 2d examples that have threedimensional\
    \ iteration spaces, using MARS layout is already profitable versus the non-MARS\
    \ layouts; most of the gains are due to contiguity more than compression. On the\
    \ one-dimensional Jacobi example, the gains are on the contrary more due to compression:\
    \ the data being one-dimensional, non-MARS layouts are already contiguous. For\
    \ small tile sizes like 6 × 6, the gains are marginal if any: the number of compressed\
    \ elements is too small to exhibit large gains from compression.\n\nEffect of\
    \ data type. On the jacobi-1d benchmark, the choice of a 200 × 200 tile size shows\
    \ a more significant benefit in using compressed MARS for fixed-point data types\
    \ than floating-point. This is explained with the better compression ratio: when\
    \ modeling continuous spaces like those used on the Jacobi stencils, neighboring\
    \ fixed-point values will have more higher bits in common than\n\n| Benchmark\
    \ | Tile Sizes               | #MARS In | #MARS Out | Read bursts | Write bursts\
    \ |\n|-----------|--------------------------|----------|-----------|-------------|--------------|\n\
    | jacobi-1d | 6 × 6 64 × 64, 200 × 200 | 7        | 4         | 3           |\
    \ 1            |\n| jacobi-2d | 4 × 5 × 7, 10 × 10 × 10  | 28       | 13     \
    \   | 10          | 1            |\n| seidel-2d | 4 × 10 × 10              | 33\
    \       | 13        | 10          | 1            |\n\n<span id=\"page-8-0\"></span>Table\
    \ 1: Characteristics of the selected benchmarks. The number of bursts per tile\
    \ accounts for layout-induced access coalescing and is independent of tile and\
    \ problem size.\n\n<span id=\"page-8-2\"></span>![](_page_8_Figure_3.jpeg)\n\n\
    Figure 9: Area statistics for the benchmarks\n\n<span id=\"page-8-3\"></span>![](_page_8_Figure_5.jpeg)\n\
    \nFigure 10: Transfer time relative to compressed MARS (lower is better).\n\n\
    <span id=\"page-8-1\"></span>\n\n| Benchmark | Tile Size    | Compile Time (s)\
    \ |\n|-----------|--------------|------------------|\n| jacobi-1d | 6 × 6    \
    \    | 0.76             |\n| jacobi-1d | 64 × 64      | 0.68             |\n|\
    \ jacobi-1d | 200 × 200    | 1.02             |\n| jacobi-2d | 4 × 5 × 7    |\
    \ 5.57             |\n| jacobi-2d | 10 × 10 × 10 | 5.09             |\n| seidel-2d\
    \ | 4 × 10 × 10  | 3.21             |\n\n![](_page_8_Figure_8.jpeg)\n\nfloating-point\
    \ data where neighboring values mostly only share the exponent.\n\n5.2.4 Applicablity.\
    \ Figure [11](#page-9-0) shows the compression rate for each data type and tile\
    \ size for the jacobi1d benchmark. Two ratios are shown: the true ratio which\
    \ accounts only for the bit savings due to compression, and a ratio with padding\
    \ that accounts for the savings due to not padding the data. The ratio with padding\
    \ is the one our accelerators really benefit from, because the data is not packed\
    \ in memory except in compressed MARS form.\n\n<span id=\"page-9-0\"></span>![](_page_9_Figure_0.jpeg)\n\
    \nFigure 11: Compression ratio vs. data type and tile size for jacobi1d\n\nOverall,\
    \ compressing the data for the selected benchmarks is almost always profitable,\
    \ possibly largely as the compression ratio goes up to 5.09:1 for 200 × 200 tiles\
    \ and 18-bit type.\n\nWe can observe that large tiles (64 × 64, 200 × 200) exhibit\
    \ closer compression ratios than smaller tiles (6 × 6). This discrepancy can be\
    \ explained by the compressed chunks being too small to benefit from the data's\
    \ low entropy; for the smallest data type and tile size, compressing data is even\
    \ worse than not compressing.\n\n#### 6 RELATED WORK\n\nThis work comes as part\
    \ of a global effort to relieve memoryboundness of high-performance accelerators.\
    \ In this section, we study other techniques used to relieve the memory wall,\
    \ some of which may not apply to compilers due to not being automatable or breaking\
    \ program semantics.\n\n#### 6.1 Data Compression\n\nData compression saves bandwidth\
    \ without requiring to modify the program's algorithm. It is therefore suitable\
    \ for many bandwidthbound problems.\n\n6.1.1 Compression techniques. Data compression\
    \ in FPGA accelerators is already a necessity for some intrinsically memory-bound\
    \ applications such as deep convolutional networks, as no locality optimization\
    \ can bring further bandwidth savings. We here focus on two kinds of compression:\
    \ lossless and lossy.\n\nLossless compression. Lossless compression guarantees\
    \ that the decompressed data is exactly the same as the data before it was compressed.\
    \ This property makes it possible to do seamless, inline compression and decompression\
    \ as is done for MARS. This is commonly performed in deep neural network accelerators\
    \ [\\[1,](#page-10-16) [11\\]](#page-10-17)\n\nSparse encoding can be considered\
    \ a form of lossless compression, and is also commonly found in machine learning\
    \ applications [\\[8,](#page-10-18) [13\\]](#page-10-19). Sparse data structures\
    \ often require indirections, which make them unsuitable for use in polyhedral\
    \ compiler flows unless the sparse structure is immutable [\\[12\\]](#page-10-20).\n\
    \nLossy compression. It is possible to save more storage and bandwidth by using\
    \ lossy compression. Some applications in machine\n\nlearning can afford a loss\
    \ of precision without degrading the quality of the result, e.g. using JPEG-compressed\
    \ images [\\[18\\]](#page-10-21) as inputs. However, automatic compression alters\
    \ the data and cannot be automatically inserted by a compiler unless the user\
    \ explicitly requests it.\n\n6.1.2 Dynamic data compression. In this work, we\
    \ automate the compression and decompression of data and it is transparent to\
    \ the computation engine on FPGA. Other works [\\[19,](#page-10-22) [23\\]](#page-10-23)\
    \ perform dynamic, demand-driven compression without prior knowledge of the data\
    \ to be handled. Thanks to the static control flow of polyhedral codes, all the\
    \ data flow is statically known and it is not necessary to maintain a cache policy.\n\
    \n#### 6.2 Memory access optimization\n\nThe layout we propose in this work optimizes\
    \ memory accesses by exhibiting contiguity using polyhedral analysis. In this\
    \ section, we go through other polyhedral memory access optimizations, and explain\
    \ other non-polyhedral ways it is possible to improve memory accesses.\n\n6.2.1\
    \ Polyhedral-based optimizations. Using the polyhedral model and loop tiling to\
    \ capture the data flow is the subject of a number of works, proposing different\
    \ breakups of the dataflow. Datharthri et al. [\\[7\\]](#page-10-24) and Bondhugula\
    \ [\\[2\\]](#page-10-25) propose decompositions of the inter-tile communications\
    \ to minimze MPI communications. This work also seeks to optimize the passing\
    \ of intermediate results, but the data allocation is not statically determined\
    \ like in this work.\n\nA MARS-like decomposition of the inter-tile data flow\
    \ into coarse-grain blocks for MPI has been proposed by Zhao et al. [\\[30\\]](#page-10-26);\
    \ our work achieves irredundancy which requires a finer-grain modeling than the\
    \ one proposed by [\\[30\\]](#page-10-26).\n\n6.2.2 Domain-specific optimizations.\
    \ Memory access optimizations such as a change of data layout or access pattern\
    \ can also be specific to each problem. We show here two cases of domain-specific\
    \ optimizations.\n\nData blocking. Data blocking (or tiling) is memory layout\
    \ transformation that chunks multi-dimensional arrays into contiguous blocks.\
    \ Similar to loop tiling, data blocking allows to coalesce accesses to entire\
    \ regions of the input or output data.\n\nData blocking can be efficient when\
    \ the memory footprint of one iteration of an accelerator corresponds to a data\
    \ tile. Although it has been used to optimize machine learning accelerators [\\\
    [26\\]](#page-10-27), it may break spatial locality and degrade performance of\
    \ accesses that cross tile boundaries.\n\nData blocking can be combined with loop\
    \ tiling and polyhedral analysis to coalesce inter-tile accesses. Ferry et al.\
    \ [\\[10\\]](#page-10-28) seeks to exhibit the largest possible contiguous units\
    \ spanning multiple tiles.\n\nStencil optimization. Stencil computations have\
    \ regular and statically known memory access patterns. Domain-specific optimizers\
    \ like SODA [\\[3\\]](#page-10-29) derive an optimized FPGA architecture and memory\
    \ layout specific to each stencil.\n\n#### 7 CONCLUSION\n\nThis work gives a twofold\
    \ contribution: a compression-friendly, contiguous data layout, and an automated\
    \ adaptation of FPGA accelerators to use this layout thanks to polyhedral compilation\
    \ tools. Thanks to the compression and contiguity, we can automatically reduce\
    \ the number of I/O cycles spent by the accelerator.\n\n### REFERENCES\n\n- <span\
    \ id=\"page-10-16\"></span>[1] Thea Aarrestad, Vladimir Loncar, Nicolò Ghielmetti,\
    \ Maurizio Pierini, Sioni Summers, Jennifer Ngadiuba, Christoffer Petersson, Hampus\
    \ Linander, Yutaro Iiyama, Giuseppe Di Guglielmo, Javier Duarte, Philip Harris,\
    \ Dylan Rankin, Sergo Jindariani, Kevin Pedro, Nhan Tran, Mia Liu, Edward Kreinar,\
    \ Zhenbin Wu, and Duc Hoang. 2021. Fast convolutional neural networks on FPGAs\
    \ with hls4ml. Machine Learning: Science and Technology 2, 4 (jul 2021), 045015.\
    \ <https://doi.org/10.1088/2632-2153/ac0ea1>\n- <span id=\"page-10-25\"></span>[2]\
    \ Uday Bondhugula. 2013. Compiling Affine Loop Nests for Distributed-Memory Parallel\
    \ Architectures. In Proceedings of the International Conference on High Performance\
    \ Computing, Networking, Storage and Analysis. ACM. [https://doi.](https://doi.org/10.1145/2503210.2503289)\
    \ [org/10.1145/2503210.2503289](https://doi.org/10.1145/2503210.2503289)\n- <span\
    \ id=\"page-10-29\"></span>[3] Yuze Chi, Jason Cong, Peng Wei, and Peipei Zhou.\
    \ 2018. SODA: Stencil with optimized dataflow architecture. In 2018 IEEE/ACM International\
    \ Conference on Computer-Aided Design (ICCAD). IEEE, 1–8. [https://doi.org/10.1145/3240765.](https://doi.org/10.1145/3240765.3240850)\
    \ [3240850](https://doi.org/10.1145/3240765.3240850)\n- <span id=\"page-10-8\"\
    ></span>[4] Young-kyu Choi and Jason Cong. 2018. HLS-based optimization and design\
    \ space exploration for applications with variable loop bounds. In 2018 IEEE/ACM\
    \ International Conference on Computer-Aided Design (ICCAD). IEEE, 1–8.\n- <span\
    \ id=\"page-10-2\"></span>[5] Jason Cong, Muhuan Huang, Peichen Pan, Yuxin Wang,\
    \ and Peng Zhang. 2016. Source-to-source optimization for HLS. FPGAs for Software\
    \ Programmers (2016), 137–163.\n- <span id=\"page-10-11\"></span>[6] P Cummiskey,\
    \ Nikil S. Jayant, and James L. Flanagan. 1973. Adaptive Quantization in Differential\
    \ PCM Coding of Speech. Bell System Technical Journal 52 (09 1973). <https://doi.org/10.1002/j.1538-7305.1973.tb02007.x>\n\
    - <span id=\"page-10-24\"></span>[7] Roshan Dathathri, Chandan Reddy, Thejas Ramashekar,\
    \ and Uday Bondhugula. 2013. Generating Efficient Data Movement Code for Heterogeneous\
    \ Architectures with Distributed-Memory. In Proceedings of the 22nd International\
    \ Conference on Parallel Architectures and Compilation Techniques. IEEE. [https:](https://doi.org/10.1109/PACT.2013.6618833)\
    \ [//doi.org/10.1109/PACT.2013.6618833](https://doi.org/10.1109/PACT.2013.6618833)\n\
    - <span id=\"page-10-18\"></span>[8] Yixiao Du, Yuwei Hu, Zhongchun Zhou, and\
    \ Zhiru Zhang. 2022. High-Performance Sparse Linear Algebra on HBM-Equipped FPGAs\
    \ Using HLS. In Proceedings of the 2022 ACM/SIGDA International Symposium on Field-Programmable\
    \ Gate Arrays. ACM.<https://doi.org/10.1145/3490422.3502368>\n- <span id=\"page-10-13\"\
    ></span>[9] Corentin Ferry, Steven Derrien, and Sanjay Rajopadhye. 2023. Maximal\
    \ Atomic irRedundant Sets: a Usage-based Dataflow Partitioning Algorithm. In 13th\
    \ International Workshop on Polyhedral Compilation Techniques (IMPACT'23).\n-\
    \ <span id=\"page-10-28\"></span>[10] Corentin Ferry, Tomofumi Yuki, Steven Derrien,\
    \ and Sanjay Rajopadhye. 2022. Increasing FPGA Accelerators Memory Bandwidth with\
    \ a Burst-Friendly Memory Layout. IEEE Transactions on Computer-Aided Design of\
    \ Integrated Circuits and Systems (2022), 1–1.<https://doi.org/10.1109/tcad.2022.3201494>\n\
    - <span id=\"page-10-17\"></span>[11] Yijin Guan, Ningyi Xu, Chen Zhang, Zhihang\
    \ Yuan, and Jason Cong. 2017. Using Data Compression for Optimizing FPGA-Based\
    \ Convolutional Neural Network Accelerators. In Lecture Notes in Computer Science.\
    \ Springer International Publishing, 14–26. [https://doi.org/10.1007/978-3-319-67952-5\\\
    \\_2](https://doi.org/10.1007/978-3-319-67952-5_2)\n- <span id=\"page-10-20\"\
    ></span>[12] Marcos Horro, Louis-Noël Pouchet, Gabriel Rodríguez, and Juan Touriño.\
    \ 2023. Custom High-Performance Vector Code Generation for Data-Specific Sparse\
    \ Computations. In Proceedings of the International Conference on Parallel Architectures\
    \ and Compilation Techniques (Chicago, Illinois) (PACT '22). Association for Computing\
    \ Machinery, New York, NY, USA, 160–171. [https://doi.org/10.1145/](https://doi.org/10.1145/3559009.3569668)\
    \ [3559009.3569668](https://doi.org/10.1145/3559009.3569668)\n- <span id=\"page-10-19\"\
    ></span>[13] Shiqing Li, Di Liu, and Weichen Liu. 2023. Efficient FPGA-based Sparse\
    \ Matrix-Vector Multiplication with Data Reuse-aware Compression. IEEE Transactions\
    \ on Computer-Aided Design of Integrated Circuits and Systems (2023), 1–1. [https:](https://doi.org/10.1109/tcad.2023.3281715)\
    \ [//doi.org/10.1109/tcad.2023.3281715](https://doi.org/10.1109/tcad.2023.3281715)\n\
    - <span id=\"page-10-3\"></span>[14] Junyi Liu, Samuel Bayliss, and George A Constantinides.\
    \ 2015. Offline synthesis of online dependence testing: Parametric loop pipelining\
    \ for HLS. In 2015 IEEE 23rd Annual International Symposium on Field-Programmable\
    \ Custom Computing Machines. IEEE, 159–162.\n- <span id=\"page-10-4\"></span>[15]\
    \ Junyi Liu, John Wickerson, Samuel Bayliss, and George A Constantinides. 2017.\
    \ Polyhedral-based dynamic loop pipelining for high-level synthesis. IEEE Transactions\
    \ on Computer-Aided Design of Integrated Circuits and Systems 37, 9 (2017), 1802–1815.\n\
    - <span id=\"page-10-0\"></span>[16] Michael Lo, Young-kyu Choi, Weikang Qiao,\
    \ Mau-Chung Frank Chang, and Jason Cong. 2023. HMLib: Efficient Data Transfer\
    \ for HLS Using Host Memory. In Proceedings of the 2023 ACM/SIGDA International\
    \ Symposium on Field Programmable Gate Arrays. 50–50.\n- <span id=\"page-10-1\"\
    ></span>[17] Florian Mayer, Julian Brandner, and Michael Philippsen. 2023. Employing\
    \ Polyhedral Methods to Reduce Data Movement in FPGA Stencil Codes. In International\
    \ Workshop on Languages and Compilers for Parallel Computing. Springer, 47–63.\n\
    - <span id=\"page-10-21\"></span>[18] Hiroki Nakahara, Zhiqiang Que, and Wayne\
    \ Luk. 2020. High-Throughput Convolutional Neural Network on an FPGA by Customized\
    \ JPEG Compression. In 2020 IEEE 28th Annual International Symposium on Field-Programmable\
    \ Custom Computing Machines (FCCM). 1–9.<https://doi.org/10.1109/FCCM48280.2020.00010>\n\
    - <span id=\"page-10-22\"></span>[19] O. Ozturk, M. Kandemir, and M.J. Irwin.\
    \ 2009. Using Data Compression for Increasing Memory System Utilization. IEEE\
    \ Transactions on Computer-Aided Design of Integrated Circuits and Systems 28,\
    \ 6 (jun 2009), 901–914. [https:](https://doi.org/10.1109/tcad.2009.2017430) [//doi.org/10.1109/tcad.2009.2017430](https://doi.org/10.1109/tcad.2009.2017430)\n\
    - <span id=\"page-10-14\"></span>[20] Louis-Noël Pouchet and Tomofumi Yuki. 2016.\
    \ PolyBench/C 4.2.1. [http:](http://polybench.sf.net) [//polybench.sf.net](http://polybench.sf.net)\n\
    - <span id=\"page-10-6\"></span>[21] Louis-Noel Pouchet, Peng Zhang, P. Sadayappan,\
    \ and Jason Cong. 2013. Polyhedral-based data reuse optimization for configurable\
    \ computing. In Proceedings of the ACM/SIGDA international symposium on Field\
    \ programmable gate arrays - FPGA '13. ACM Press.<https://doi.org/10.1145/2435264.2435273>\n\
    - <span id=\"page-10-9\"></span>[22] Tiago Santos and João MP Cardoso. 2020. Automatic\
    \ selection and insertion of hls directives via a source-to-source compiler. In\
    \ 2020 International Conference on Field-Programmable Technology (ICFPT). IEEE,\
    \ 227–232.\n- <span id=\"page-10-23\"></span>[23] Somayeh Sardashti, Andre Seznec,\
    \ and David A. Wood. 2016. Yet Another Compressed Cache: A Low-Cost Yet Effective\
    \ Compressed Cache. ACM Trans. Archit. Code Optim. 13, 3, Article 27 (Sept. 2016),\
    \ 25 pages. [https://doi.org/10.](https://doi.org/10.1145/2976740) [1145/2976740](https://doi.org/10.1145/2976740)\n\
    - <span id=\"page-10-12\"></span>[24] A. Skodras, C. Christopoulos, and T. Ebrahimi.\
    \ 2001. The JPEG 2000 still image compression standard. IEEE Signal Processing\
    \ Magazine 18, 5 (2001), 36–58. <https://doi.org/10.1109/79.952804>\n- <span id=\"\
    page-10-10\"></span>[25] Atefeh Sohrabizadeh, Cody Hao Yu, Min Gao, and Jason\
    \ Cong. 2022. AutoDSE: Enabling software programmers to design efficient FPGA\
    \ accelerators. ACM Transactions on Design Automation of Electronic Systems (TODAES)\
    \ 27, 4 (2022), 1–27.\n- <span id=\"page-10-27\"></span>[26] Teng Tian, Xi Jin,\
    \ Letian Zhao, Xiaotian Wang, Jie Wang, and Wei Wu. 2020. Exploration of Memory\
    \ Access Optimization for FPGA-based 3D CNN Accelerator. In 2020 Design, Automation\
    \ & Test in Europe Conference & Exhibition (DATE). 1650–1655.<https://doi.org/10.23919/DATE48585.2020.9116376>\n\
    - <span id=\"page-10-15\"></span>[27] Sven Verdoolaege. 2010. isl: An Integer\
    \ Set Library for the Polyhedral Model. In Mathematical Software – ICMS 2010,\
    \ Komei Fukuda, Joris van der Hoeven, Michael Joswig, and Nobuki Takayama (Eds.).\
    \ Springer Berlin Heidelberg, Berlin, Heidelberg, 299–302.\n- <span id=\"page-10-5\"\
    ></span>[28] Hanchen Ye, Cong Hao, Jianyi Cheng, Hyunmin Jeong, Jack Huang, Stephen\
    \ Neuendorffer, and Deming Chen. 2022. Scalehls: A new scalable high-level synthesis\
    \ framework on multi-level intermediate representation. In 2022 IEEE International\
    \ Symposium on High-Performance Computer Architecture (HPCA). IEEE, 741–755.\n\
    - <span id=\"page-10-7\"></span>[29] Ruizhe Zhao, Jianyi Cheng, Wayne Luk, and\
    \ George A Constantinides. 2022. POLSCA: Polyhedral High-Level Synthesis with\
    \ Compiler Transformations. In 2022 32nd International Conference on Field-Programmable\
    \ Logic and Applications (FPL). IEEE, 235–242.\n- <span id=\"page-10-26\"></span>[30]\
    \ Tuowen Zhao, Mary Hall, Hans Johansen, and Samuel Williams. 2021. Improving\
    \ communication by optimizing on-node data movement with data layout. In Proceedings\
    \ of the 26th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming.\
    \ ACM.<https://doi.org/10.1145/3437801.3441598>"
  decisions:
    evaluation_prompt: 'Qualified. Reason: The paper includes an evaluation of the
      approach on FPGA accelerators, showing a decrease in I/O cycles, and discusses
      results on a series of benchmarks.'
    related_work_prompt: 'Qualified. Reason: The paper engages with prior research
      by citing multiple research papers and discussing related concepts and techniques.'
    novelty_prompt: 'Qualified. Reason: The paper proposes a new compiler-based approach
      to optimize memory bandwidth for FPGA accelerators, including new algorithms
      and an automated code generation framework.'
    review_only_prompt: 'Qualified. Reason: The paper introduces new contributions,
      including a compiler-based approach and an automated code generation framework,
      rather than primarily summarizing existing work.'
- title: "ACS: Concurrent Kernel Execution on Irregular, Input-Dependent\n  Computational\
    \ Graphs"
  abstract: 'GPUs are widely used to accelerate many important classes of workloads
    today.

    However, we observe that several important emerging classes of workloads,

    including simulation engines for deep reinforcement learning and dynamic neural

    networks, are unable to fully utilize the massive parallelism that GPUs offer.

    These applications tend to have kernels that are small in size, i.e., have few

    thread blocks that do not saturate compute resources. Executing independent

    kernels concurrently is a promising approach to improve parallelism and

    utilization. However, this inter-kernel concurrency is difficult to leverage in

    such workloads with existing approaches: First, the inter-kernel dependencies

    and computational graph are input-dependent and vary each time the application

    is executed. Second, the computational graphs tend to be irregular, requiring

    fine-grain scheduling and synchronization; thus incurring significant

    synchronization overheads if kernel execution is parallelized. In this work, we

    propose ACS, a framework that enables lightweight detection of inter-kernel

    dependencies and low overhead kernel scheduling at runtime. The key idea behind

    ACS is to perform inter-kernel dependency checks for a small window of kernels

    at runtime, similar to out-of order instruction scheduling. This enables

    concurrent execution of kernels in applications whose computational graphs are

    input dependent and require fine-grained scheduling. We propose ACS-SW, a

    software-only open-source implementation of ACS and ACS-HW, a hardware-software

    cooperative implementation. ACS-HW further reduces synchronization overheads by

    reducing communication between the CPU and GPU. We evaluate ACS for deep RL

    simulation and dynamic DNNs on both real hardware and a GPU simulator. We

    demonstrate speedups of up to 2.19x (1.56x on average) by improving GPU

    utilization with concurrent kernel execution.'
  url: http://arxiv.org/abs/2401.12377v1
  keywords: ''
  document: '#### I. INTRODUCTION


    Graphics Processing Units (GPUs) today are commonly used to accelerate a diverse
    set of applications, such as deep neural network (DNN) processing, scientific
    computing, graphics, and cryptography. The massive parallelism offered by GPUs
    enables efficient computations on large amounts of data concurrently. However,
    we observe that certain important classes of applications, such as simulation
    engines for deep reinforcement learning (RL) [1]–[5] and dynamic neural networks
    [6]–[19], are unable to fully utilize the significant compute capability that
    GPUs offer. This underutilization is because these applications comprise a large
    number of small kernels, i.e., kernels with few thread blocks that are unable
    to fully saturate the GPU cores. To understand the challenges in alleviating this
    underutilization, we evaluate two important classes of applications and introduce
    their properties.


    Simulation Engines for Deep RL. With reinforcement learning (RL) an agent (for
    example, a robot) learns to perform tasks such as robotic locomotion, manipulation,
    and navigation [20], [21] by trial and error from interactions with the environment.
    Deep RL training involves using a DNN to learn policies that optimize for rewards
    from data collected by interacting with a simulation environment. By leveraging
    the benefits of DNNs, deep RL has recently gained widespread application for many
    challenging and important tasks [20], [22]–[28]. Despite leveraging GPUs, a significant
    fraction of the deep RL runtime is the data collection phase (up to 70% of the
    runtime), where physics simulations are used to generate training data. We observe
    that these physics simulations heavily underutilize the GPU, only achieving an
    occupancy of 34% on average. The underutilization is caused by kernels that contain
    a small number of thread blocks that cannot fully utilize the GPU. Programming
    larger kernels is impractical as each instance simulates a different scenario,
    and large kernels would lead to thread divergence.


    Dynamic DNNs. Several recent types of DNNs [6], [10], [12], [29] have emerged
    as a promising approach to reduce inference latencies in resource-constrained
    devices by reconfiguring/specializing the architecture based on the input to the
    DNN. For example, InstaNAS [10] configures the network architecture at runtime
    based on the input image. Our evaluations demonstrate that, while these architectures
    require significantly fewer FLOPs and lower inference latencies, there is still
    significant underutilization of GPU resources (achieving an occupancy of only
    39% on average). Similar to the simulation engines, we find that this underutilization
    is caused by small kernels that do not fully utilize the GPU cores.


    GPU kernels from such applications are typically executed *serially*, and thus
    the utilization is determined by the size (i.e., the number of threads and thread
    blocks) of the kernel. However, we observe that many kernels are independent and
    thus can be executed concurrently. By concurrently executing independent kernels,
    we can effectively improve GPU utilization and thus performance. Existing GPU
    architectures allow for concurrent execution of kernels by using multiple command
    queues [30] which are abstracted in software (such as CUDA Stream [31]), allowing
    the programmer to identify and launch independent kernels in parallel. However,
    enabling concurrent kernel execution for these applications is still a challenging
    task for two major reasons.


    Challenge 1: Input-dependent computational graphs. For these applications, the
    computational graph (i.e. the kernels to be executed and their dependencies) is
    only resolved at runtime based on the input, and each input or set of inputs leads
    to a different computational graph. This means that identifying independent kernels
    to launch in parallel requires performing inter-kernel dependency checks at runtime.
    These workloads have short running kernels that significantly exacerbate the scheduling
    and dependency checking overheads, making this a challenging problem to solve.
    Frameworks such as CUDA Graph [32] and AMD ATMI [33] allow programmers to define
    the inter-kernel dependency information and construct a directed acyclic graph
    (DAG) of kernels. These frameworks enable concurrent kernel execution. However,
    when inter-kernel dependencies vary by input, we must incur the significant latency
    of constructing the dependency graph and scheduling independent kernels, every
    time the application is executed, significantly increasing run time (§ II-D and
    § VI).


    Challenge 2: Irregular inter-kernel dependencies require fine-grain scheduling.
    The computational graph for a given input tends to be highly irregular. In other
    words, the kernels cannot be easily partitioned into independent streams and fine-grain
    scheduling is required to expose inter-kernel parallelism. Thus, parallel execution
    of kernels requires frequent synchronization to ensure correctness, leading to
    significant synchronization overheads from communicating with the CPU and from
    kernel launches (§ II-D).


    To address these challenges, our goal in this work is to enable kernel concurrency
    with *(i)* lightweight scheduling and dependency checking of kernels that can
    be performed at runtime and *(ii)* low overhead synchronization for scheduling
    and kernel launch. To this end, we propose ACS, a new framework for Automatic
    Concurrent Scheduling with two implementations: *(i)* ACS-SW, a software-only
    mechanism to enable lightweight kernel scheduling at runtime and *(ii)* ACS-HW:
    a hardware-software mechanism to further reduce synchronization overheads for
    efficient kernel concurrency.


    The key idea of ACS is to perform dependency checks between sequentially launched
    kernels within a fixed window at runtime, similar to out-of-order instruction
    scheduling. We refer to this window as the *scheduling window*. When a kernel
    is inserted into the scheduling window, the kernels that it is dependent on are
    identified. As kernels complete execution, kernels in the scheduling window are
    marked ready based on the identified dependencies. Ready kernels can then be concurrently
    launched as they have no more dependencies. Since at any given time, only a small
    set of kernels are scheduled and tracked (instead of the entire computational
    graph), this approach enables efficient kernel parallelization and scheduling
    at runtime. To perform dependency checks between kernels, ACS leverages annotations
    from the application that specify the memory address ranges that are read/written
    by each kernel. This metadata is then used to identify inter-kernel dependencies
    at runtime when kernels are inserted into the scheduling window. Compared to prior
    approaches (§ III-A), this method alleviates the significant kernel scheduling
    and dependency-check overheads for kernel parallelization.


    ACS-SW implements the above out-of-order runtime kernel scheduling in software
    as an application runtime system using CUDA streams. ACS-SW however still incurs
    synchronization overheads from communication with the CPU and kernel launch. On
    the other hand, ACS-HW implements the out-oforder kernel scheduler in the GPU
    hardware and can alleviate the synchronization overheads. We propose an efficient
    implementation of ACS-HW that reduces synchronization and kernel overheads by
    reducing communication with the CPU.


    Prior works such as task superscalar [34], carbon [35], TDM [36] and ADM [37]
    propose similar out-of-order scheduling to leverage irregular parallelism between
    tasks in CPU multiprocessors. However, the major challenge in CPUs is the latency
    of runtime dependence checking. The primary bottleneck with GPUs is the latency
    for launch/signal completion of kernels rather than dependence checking (§ IV-D).
    ACS addresses this challenge and provides an efficient approach to enable out-of-order
    kernel scheduling in GPUs.


    We demonstrate the effectiveness of ACS in improving GPU utilization and thus
    performance for physics simulation workloads, a range of dynamic neural networks,
    as well as static neural networks with small kernels. We demonstrate an average
    speedup of up to 1.87× using our softwareonly approach and up to 2.19× from the
    hardware-software implementation. The major contributions of this work are:


    - We identify and characterize GPU underutilization as a result of small GPU kernels
    in applications with inputdependent irregular computational graphs, e.g., deepRL
    and dynamic DNNs.

    - We introduce ACS, a runtime mechanism that improves GPU utilization by enabling
    concurrent execution of GPU kernels with a lightweight dependency tracking and
    scheduling framework.

    - We will provide an open-source software-only implementation of ACS that can
    be used on real hardware to enable low overhead GPU kernel concurrency.

    - We evaluate the effectiveness of ACS-SW and ACS-HW on a range of important GPU
    applications and demonstrate significant speedups and improved GPU utilization.


    #### II. MOTIVATION


    #### *A. Baseline GPU architecture*


    Figure 1 shows an overview of the hardware model in modern GPU architectures [38].
    The host communicates with the command processor (CP) of the GPU via a virtual
    memory region which is memory mapped to the GPU, accessible by the command processor.
    This enables communication between the CPU and GPU through entries in the command
    queue. The CPU transmits kernel launch packets to the GPU by writing them to the
    user mode command queue. The CP is responsible for decoding and dispatching the
    kernels in these command queues for execution. The CP accesses the command queue
    and schedules the kernels at the head for execution. This ensures that the kernels
    are dispatched for launch from these queues in order.


    ![](_page_1_Figure_15.jpeg)


    Fig. 1: Scheduling kernels from multiple streamsw


    #### *B. Case Study 1: Simulation Engines for Deep RL*


    Deep reinforcement learning (RL) has widely gained attention as a promising approach
    to learning control policies in robotics and dynamical systems for tasks such
    as locomotion on legged robots [1], [26], [28], dexterous hand manipulation [21],
    autonomous driving [20], [24], and drone control [22], [23], [25]. Deep RL involves
    training a DNN to learn policies that maximize the reward, based on the actions
    that the agent (e.g., four-legged robot) performs in a given environment. This
    training process requires data from the agent interacting with a physics simulator.
    Typically, each training step requires data from thousands of physics simulations.
    Recent works [1]–[5], [39] accelerate this data generation phase by leveraging
    GPUs. GPUs can accelerate data generation by performing multiple simulations simultaneously
    and also parallelizing within a single simulation. Hence this makes them an appropriate
    candidate workload for GPU execution. Despite GPU acceleration, the simulation/data
    generation phase is still the predominant computation in deep RL—taking about
    30−70% of training time depending on the complexity of the simulated environment.
    Thus accelerating simulation engines is critical for deep RL performance.


    To evaluate the efficiency of physics simulations, we analyzed a set of physics
    simulations with different environments on a GPU (parameters in § V) with the
    widely used Brax [1] framework. We evaluate the utilization of the GPU by measuring
    achieved occupancy (average ratio of active warps to the maximum supported), depicted
    in Fig. 2. We find that as much as 65% of the GPU cores are underutilized on average
    (on both GPUs). To evaluate the cause of this underutilization, we analyze the
    number of kernel launches required to generate one batch of training data in Fig.
    3. We also present the average number of CTAs per kernel in Fig. 4 and depict
    the distribution of kernel sizes observed for the ant environment in Fig. 5. We
    observe that physics simulations in our evaluations generate a large number of
    *small* kernels that have few threads and CTAs. This is a fundamental problem
    because the simulation engine cannot be efficiently mapped into large kernels
    as the different threads will likely diverge in the execution path. This is because
    each thread typically simulates a different scenario in the environment. Thus
    the application is instead programmed as a large number of short-running kernels.
    This phenomenon has also been observed by recent works [39], [40].


    ![](_page_2_Figure_3.jpeg)


    Fig. 2: Simulation engines: Achieved occupancy.


    ![](_page_2_Figure_5.jpeg)


    **0-9 10-49 50-199 200+** Fig. 5: Kernel size distribution for the ant environment
    power budgets as direct deployment of large neural network architectures on these
    devices leads to high-inference times. Automated DNN architecture design (neural
    architecture search) is a promising approach to generate faster neural network
    architectures while retaining or improving accuracy [41]–[44]. These optimized
    architectures tend to have irregular elaborate connections between convolution
    operations. Fig. 6a depicts an example DNN with irregular structure. Additionally,
    an emerging trend in recent research [29] shows that *dynamic inference models*
    [6]–[8], [10], [13]–[19], [45]–[48] are very promising to significantly reduce
    inference latency and FLOPs. With these dynamic inference models, the path of
    execution through the network is determined by the *input*. Thus, the computational
    graph is not known ahead of time. For example, Fig. 6b shows an example CNN model
    with different paths of execution based on the input [10].


    Similar to § II-B, we evaluate the efficiency of these workloads on a GPU (an
    NVIDIA RTX 3060 and an NVIDIA RTX 4090) and depict the resulting utilization in
    Fig. 7 (evaluation and workload settings are in § V). We find that the total achieved
    occupancy is around 39% in the InstaNAS-A [10] workload for both GPUs. Similar
    to the simulation engines, we root cause this underutilization to the existence
    of a large number of small kernels, as depicted in Fig. 8, where a large fraction
    of the kernels have fewer than 200 CTAs. Thus, these small kernels are unable
    to fully utilize the GPU. In these workloads, the small kernels are due to convolution
    layers that were optimized for fewer FLOPs with smaller filters. avg 3x3 max 3x3


    ![](_page_2_Figure_8.jpeg)


    # *C. Case Study 2: DNNs with dynamic irregular graphs*


    Recent research has extensively investigated specialized DNNs for edge devices
    with limited compute resources and


    #### *D. Key Observations*


    While small-sized kernels lead to underutilization, we observe that there are
    typically many kernels that can be executed *concurrently*. Thus we can improve
    GPU utilization and reduce


    ![](_page_3_Figure_0.jpeg)


    Fig. 8: Kernel size distribution (in CTAs) for InstaNAS-A [10]


    runtimes by identifying independent kernels and scheduling them for concurrent
    execution. However, this is a challenging task for these classes of applications
    for the following reasons.


    (1) Input-dependent kernel dependencies. The computational graph, and hence, the
    dependencies between kernels are only determined at *runtime* for each input.
    For example, with the instance-aware dynamic DNNs [6]–[8], [10] described in §
    II-C, for the classification inference task, the computational graph is different
    for each image. As a result, the determination of kernel dependencies and scheduling
    of kernels for the entire computational graph needs to be done for *each input*.
    This adds significant latencies to the runtime.


    CUDA Graphs [32] and AMD ATMI [33] are software frameworks that allow developers
    to specify dependencies between different kernels as edges of a directed acyclic
    graph (DAG). The challenge with this approach is that the DAG needs to be constructed
    in full (with dependencies, kernel launches, and barriers determined) before the
    application is executed on the GPU, *for each input*. This process adds high latency
    in compiling the complete dependency information. We perform an experiment to
    measure the DAG construction and launch time on Brax [1] simulation engine (§
    V) compared to the program execution time, shown in Fig. 9. We observe that the
    time taken to construct the graph is exceedingly high (average of 47% of overall
    execution time).


    ![](_page_3_Figure_5.jpeg)


    Fig. 9: DAG construction time as % of execution time


    Similarly, recent works for DNNs [50]–[52] perform kernel scheduling, fusion,
    or parallelization for better GPU utilization. These works, for example, partition
    the computational graph into independent sub-graphs that are scheduled into multiple
    streams. However, this scheduling and partitioning is too time-consuming to be
    done for each input at runtime and thus cannot be applied to these classes of
    workloads.


    (2) Irregular kernel dependencies. These classes of applications have *irregular*
    computational graphs that are challenging to easily partition into CUDA streams
    (§ II-C). Popular deep learning frameworks [53], [54] use a single stream by default.
    The stream abstraction works best if the entire graph can be partitioned into
    independent streams of kernels. However, these graphs with irregular dependencies
    would require finegrained scheduling and heavy use of synchronization (e.g., cudaDeviceSynchronize
    and cudaStreamSynchronize) when parallelizing using CUDA streams. This synchronization
    may lead to large overheads as it requires communication between the GPU and CPU.
    Fig. 10 depicts the different overheads when CUDA streams are used for fine-grained
    scheduling with irregular graphs: kernel launch overheads 1 , CPU execution overheads
    2 and the synchronization overheads 3 . Based on our profiling, the synchronization
    and launch overheads vary between 5-20us. CUDA Graphs [32] and ATMI [33] can eliminate
    the synchronization and kernel launch overhead. However, for input-dependent graphs,
    as demonstrated in (1), this benefit is lost due to DAG construction overheads.


    ![](_page_3_Figure_10.jpeg)


    Fig. 10: Kernel launch and synchronization overheads


    # III. APPROACH


    Our goal in this work is to design a framework that enables efficient concurrent
    execution of GPU kernels *(i)* whose computational graph may only be known at
    runtime, *(ii)* without incurring significant synchronization overheads. To this
    end, we introduce ACS, a new framework that concurrently schedules independent
    kernels with a lightweight runtime mechanism.


    #### *A. Prior Mechanisms*


    We consider the baseline GPU architecture as described in § II-A. The GPU runtime
    can launch kernels into different streams. These streams are mapped to one of
    the command queues in the device-mapped memory of the GPU. The command processor
    schedules kernels at the head of these queues concurrently, thus enabling concurrent
    kernel execution. However, neither the command processor nor the kernel launch
    packets in the command queues have information on inter-kernel data dependencies.
    Kernels in different queues are assumed to be independent of each other and all
    kernels within the same queue are executed in order. Hence, in order to leverage
    parallelism in kernel executions, the task of checking inter-kernel dependencies
    and determining the kernels which can execute concurrently (and thus scheduling
    into different queues) *has to be done by the host application.* However, this
    is a problem, as this adds significant dependency-checking/scheduling latency
    to the run time. It also requires communication with the host (through a synchronization
    routine) to be performed each time a kernel completes execution, adding to the
    overhead. Several prior works describe approaches to efficiently schedule kernels
    into multiple streams. Fig. 11 depicts approaches to scheduling a computational
    graph (Fig. 11a). Fig. 11b is the baseline approach used by many existing frameworks
    [53], [54], where a single CUDA stream is used to execute all kernels serially.
    This approach leads to underutilization (§ II-C). Fig. 11c shows prior works [50],
    [51] that use the computational graph to identify independent kernels and the
    *entire graph* is scheduled ahead of time into multiple CUDA streams. However,
    this fine-grained scheduling and synchronization leads to large overheads.


    ![](_page_4_Figure_1.jpeg)


    ![](_page_4_Figure_2.jpeg)


    One way to avoid using a device-level synchronization (like cudaDeviceSynchronize)
    and enable asynchronous execution of kernels without communication with the CPU
    is to use events provided by the CUDA stream management API. Events serve as signaling
    mechanisms to indicate the occurrence of specific operations in a stream. This
    allows synchronization between kernels across streams through the cudaStreamWaitEvent
    API, facilitating asynchronous kernel execution without blocking the host. By
    strategically placing events and using cudaStreamWaitEvent, it is possible to
    orchestrate the order in which kernels are executed on the GPU without communication
    with host. However, this approach still requires deriving dependencies between
    all kernels beforehand, and thus incurs significant scheduling overhead.


    Another set of approaches [51], [52], [55], define static dependencies between
    kernels as a DAG, which is then scheduled with DAG frameworks (CUDA Graph [32]/ATMI
    [33]). These approaches cannot be applied to input-dependent computation graphs,
    as constructing the entire computational graph is too time-consuming to be done
    at runtime. To convey the DAG information, ATMI sends barrier packets [56] along
    with kernel launch packets to the command queue. A barrier packet [57] is a 64-byte
    data packet that contains id information about a kernel and a set of kernels that
    depend on it. This packet can be inserted into the command queue by the device
    runtime. The barrier packet blocks the launching of dependent kernels until the
    independent kernel completes execution. The barrier packet however does not contain
    any information regarding the current status of the executing kernels in the GPU
    and thus cannot perform any additional runtime reordering of kernels. It simply
    follows the dependencies already specified by the DAG. While it is possible to
    devise a framework that dynamically launches barrier packets and launch commands
    onto the GPU command queue in memory, this would require hardware support and
    would still incur synchronization overheads with the CPU. Our approach is specifically
    designed to mitigate this scheduling cost by avoiding direct communication from
    the GPU to the CPU, thereby reducing potential overheads.


    Persistent threads (PT) eliminate the scheduling and launch overheads but are
    only effective when all kernels are homogeneous [58] . CUDA dynamic parallelism
    [59] (CDP) or AMD''s device enqueue [60] (DE) enables parent kernels to launch
    child kernels, only allowing data dependencies between one parent and its children.
    These workloads however involve kernels that depend on multiple kernels, and it
    is an open problem how to use CDP for these types of dependencies.


    K6 K5 K4 K3 K2 K1 N We summarize different approaches for parallel kernel scheduling
    in Table I, in terms of applicability (whether inputdependent irregular workloads
    can be effectively mapped), synchronization/launch overheads and preparation overhead
    (resolving dependencies, constructing, and scheduling the computational graph).


    | Method                  |   | Applicability Sync+Launch Preparation<br>Overhead
    | Overhead |

    |-------------------------|---|---------------------------------------------------|----------|

    | Multi-Stream [50], [51] | ✓ | x                                                 |
    ✓        |

    | DAG [32], [33], [52]    | ✓ | ✓                                                 |
    x        |

    | PT [58], [61], [62]     | x | ✓                                                 |
    ✓        |

    | CDP [59] / DE [60]      | x | x                                                 |
    ✓        |

    | ACS-SW (Our approach)   | ✓ | x                                                 |
    ✓        |

    | ACS-HW (Our approach)   | ✓ | ✓                                                 |
    ✓        |


    TABLE I: Comparison of ACS to other scheduling frameworks


    #### *B. Key Idea of ACS*


    With ACS, the key idea is to instead perform the dependence checking and scheduling
    within a small window of kernels at *runtime* similar to out-of-order instruction
    scheduling. We perform this scheduling over a single command queue (or a single
    initialized stream). Fig. 12a depicts out-of-order kernel dispatch with ACS. Fig.
    12b shows the corresponding high-level hardware modifications for ACS. A fixed
    number of kernels in the original stream (scheduling window 1 ) are evaluated
    for dependencies. When a kernel completes execution, we evaluate which kernels
    within the scheduling window are now ready for execution 2 . All such kernels
    are marked ready and can be scheduled concurrently.


    ![](_page_4_Figure_12.jpeg)


    (a) Out-of-order kernel dispatch from the scheduling window (b) CP scheduling
    kernels in out of order manner


    Fig. 12: ACS: Runtime out-of-order kernel scheduling


    We propose two implementations of ACS: ACS-SW, a SWonly approach and ACS-HW, a
    hardware-software cooperative mechanism, which we describe in the following sections.
    ACS-SW emulates the out-of-order kernel scheduling mechanism by scheduling independent
    kernels into multiple streams and can be implemented with purely software changes,
    however the hardware support in ACS-HW is more efficient as it also alleviates
    synchronization overheads.


    #### *C. Design Overview*


    To design ACS to perform the runtime kernel scheduling as depicted in Fig. 12a,
    we need *(i)* a mechanism to determine inter-kernel dependencies in the scheduling
    window; *(ii)* to identify kernels that are ready for execution; and *(iii)* alleviate
    synchronization and kernel launch overheads.


    Determining inter-kernel dependencies. In order to determine dependencies between
    kernels, the application adds additional metadata to each kernel invocation. This
    metadata defines the range of global memory addresses that are written to and
    read from by each kernel. This metadata is provided to ACS by using a kernel wrapper
    (described in § IV-B) and can be defined by the programmer, library-writer, or
    compilation tools. By checking for overlaps between read segments and write segments,
    we determine dependencies between kernels. The kernel wrapper defines the pointers
    to the read and write data segments (start\_addr) along with the size of the segments
    (Fig. 13). The actual virtual addresses associated with the pointers are resolved
    just before kernel launch in order to perform the dependence checks (§ IV-A).
    We refer to these memory ranges as read\_segments and write\_segments.


    ![](_page_5_Figure_3.jpeg)


    Fig. 13: Memory regions written to/accessed by the kernel


    Tracking kernel state at runtime. Fig. 14 depicts the scheduling window ( 1 ),
    with the additional state required for scheduling. The kernels in the window can
    be ready, pending, or executing ( 3 ). Kernels in the scheduling window become
    ready for launch (ready) when the kernels it is dependent on (referred to as *upstream*
    kernels 2 ) complete execution. For each kernel in the scheduling window, we track
    a list of the corresponding upstream kernels. The upstream kernels are determined
    using the above dependency checks when inserting into the scheduling window. When
    the upstream list is empty, the kernel is marked ready for execution. After each
    kernel completes execution, the upstream list is updated for all kernels in the
    scheduling window. For ACS-SW, these checks are performed in the software runtime
    system (§ IV-B), and for ACS-HW, we implement them in hardware (§ IV-C).


    ![](_page_5_Figure_6.jpeg)


    Fig. 14: Kernels in the scheduling window with their state and corresponding upstream
    kernels (i.e., dependencies)


    Eliminating CPU synchronization overheads. In order to eliminate synchronization
    and kernel launch overheads resulting from communication between the CPU and GPU,
    we implement the scheduling window in the GPU hardware in ACS-HW. We design an
    efficient implementation of ACS-HW that reduces communication with the CPU. The
    management of the scheduling window is done entirely in hardware, including the
    determination of ready kernels. Similarly, once a kernel completes execution,
    the scheduling window is updated without requiring synchronization with the CPU.


    #### *D. Mechanism Walkthrough*


    start\_addr size Fig. 15 depicts a high level walkthrough of ACS. For each GPU
    kernel invoked by the application 1 , the read and write segments are resolved
    (detailed in § IV-A). All invoked kernels along with the corresponding read/write
    segments are entered into the input FIFO queue to await scheduling 2 . Kernels
    are then added to the fixed size scheduling window in a FIFO manner 3 . When the
    kernel enters the scheduling window 4 , the write segments of the current kernel
    are compared against read and write segments of all kernels in the scheduling
    window. The kernels with overlap are added to the corresponding upstream kernel
    list and are marked pending. When an executing kernel completes execution, all
    corresponding upstream kernel lists are updated. Any kernel that has an empty
    list is marked ready for the scheduler to launch. input FIFO queue <sup>2</sup>


    ![](_page_5_Figure_12.jpeg)


    Fig. 15: High level overview of ACS


    #### IV. DETAILED DESIGN


    #### *A. ACS Kernel Wrappers*


    In order to perform runtime dependency checks, the application defines the read/write
    segments for each kernel. These segments are defined using a kernel wrapper, ACS\_wrapper
    (defined in Fig. 16). Since virtual addresses can only be resolved at runtime,
    the programmer instead defines a function get\_addresses which populates the \_\_read\_segments\_\_
    and \_\_write\_segments\_\_ lists (lines 6 and 7 in Fig. 16). The get\_addresses
    function takes the kernel''s launch arguments as the input arguments (lines 12
    to 15). These arguments are then used to compute the read/write segments.


    scheduling window 1 K2 none ready K1 none executing Just before kernel launch,
    the CUDA runtime calls the get\_addresses function. At this point, the \_\_read\_segments\_\_
    and \_\_write\_segments\_\_ lists are populated with the resolved virtual addresses.
    In our implementation of ACS-SW, since the CUDA drivers are closed-source, we
    implement an intermediate user-level kernel launch function that calls the get\_addresses
    function instead. Fig. 17 depicts an example implementation of the get\_addresses
    function. ACS assumes that the programmer or the kernel library provider has knowledge
    of the memory regions accessed by the kernel from the kernel function prototype.
    For a wide range of commonly used kernels, such as matrix multiplication, convolution,
    addition, etc., which operate on data stored as contiguous regions in memory,
    this task is straightforward. Additionally, the get\_address function can be obtained
    using a static binary analysis tool like GPUOcelot [63]. However, in situations
    where it is not possible to determine the range of memory accessed by the kernel
    (for example, indirect memory accesses), our approach assumes that the entire
    GPU memory may be accessed by the kernel.


    ```

    1 struct ACE_wrapper {

    2 //list of read,write segments defined as

    3 //[{start_adr1,size1},{start_adr2,size2}..]

    4 list __read_segments__;

    5 list __write_segments__;

    6 // function which gets called at kernel

    7 // launch to populate read,write segments

    8 void get_addresses(

    9 dim3 blocks, dim3 threads, ...

    10 );

    11 // function declaration of the kernel

    12 static __global__ void kernel(...);

    13 };

    ```

    Fig. 16: The ACS\_wrapper definition


    ```

    1 // get address function for matrix multiply

    2 // input matrices: input1 (mxn), input2(nxk)

    3 // output matrix: output(mxk)

    4 void ACE_wrapper::get_addresses(

    5 dim3 blocks, dim3 threads,

    6 int* input1, int* input2, int* output1,

    7 int m, int n, int k) {

    8 // input1 reads m*n elements

    9 // input2 reads n*k elements

    10 __read_segments__ = {

    11 {(void*)input1, m*n*sizeof(int)},

    12 {(void*)input2, n*k*sizeof(int)}

    13 };

    14 // output reads m*k elements

    15 __write_segments__ = {

    16 {(void*)output, m*k*sizeof(int)},

    17 };

    18 }

    ```

    Fig. 17: Example: get\_addresses function


    ### *B. ACS-SW Design*


    ACS-SW is implemented as a user-level runtime that is called by the application.
    The functionalities of ACS-SW are performed by multiple independent threads that
    are launched simultaneously. The ACS-SW runtime performs two major tasks: *(i)*
    implementing and maintaining the scheduling window (window module); and *(ii)*
    scheduling kernels ready for execution (scheduling module).


    *1) The window module:* The window module is implemented as a separate thread
    that manages the input FIFO queue and the scheduling window. All the functionalities
    of the scheduling window, dependency tracking, and state management are performed
    in software within this module. This module is called in two ways: First, when
    a kernel is invoked by the application thread, this module is called and the kernel
    is inserted into the input queue. Second, the scheduler module (implemented as
    a separate thread(s)) calls the window module when a kernel completes execution.
    At this point, the state of upstream lists is updated and the kernel is removed
    from the scheduling window. The window module constantly polls the input queue
    and the scheduling window. When there is a vacancy in the scheduling window and
    a pending kernel in the input queue, the kernel is moved into the scheduling window.
    At this point, the window module performs the necessary dependency checks and
    bookkeeping. Algorithm 1 describes how the dependency check is performed.


    |  |  | Algorithm 1 Dependency check algorithm |  |  |

    |--|--|----------------------------------------|--|--|

    |--|--|----------------------------------------|--|--|


    | Input: rslist1, wslist1, wslist2<br>▷ RW segments of scheduling window   |

    |--------------------------------------------------------------------------|

    | kernel, w-segment of kernel in inputFIFO                                 |

    | Output: is dependent<br>▷                                                |

    | 1: is dependent = false<br>▷ initial state of is dependent               |

    | S<br>2: rwslist1 ← wslist1<br>rslist1<br>▷ Read+Write segments           |

    | 3: for each segment1 in rwslist1 do<br>▷ Test for every pair of segments |

    | for each ws2 in wslist2 do<br>4:                                         |

    | ▷ get start and end virtual memory addresses                             |

    | start1 ← segment1.start<br>5:                                            |

    | end1 ← segment1.start + segment1.size<br>6:                              |

    | start2 ← ws2.start<br>7:                                                 |

    | 8:<br>end2 ← ws2.start + ws2.size                                        |

    | ▷ check overlaps between start and end addresses                         |

    | 9:<br>if start1 < end2 and end1 > start2 then                            |

    | is dependent = true<br>▷<br>10:                                          |

    | 11:<br>end if                                                            |

    | 12:<br>end for each                                                      |

    | 13: end for each                                                         |


    *2) The scheduler module:* This module schedules and launches ready kernels for
    execution. This module is implemented as a configurable fixed number of threads,
    each of which launches kernels into an independent CUDA stream for concurrent
    execution, as depicted in Fig. 18. Each stream contains only one kernel at any
    given time. Threads with empty streams poll the scheduling window for a ready
    kernel 1 , which is then launched in its CUDA stream 2 . The thread then waits
    for the kernel to complete execution using the StreamSync primitive 3 . Once the
    kernel completes execution, the thread calls the window module as described above.
    This algorithm is described in Algorithm 2. Thread 1 Thread 2


    ![](_page_6_Figure_12.jpeg)


    Fig. 18: ACS-SW: The scheduler module


    |    | Algorithm 2 The scheduler module in software |                                      |

    |----|----------------------------------------------|--------------------------------------|

    |    | Input: SchedulingWindow SW, stream id        |                                      |

    |    | 1: while notstop() do                        | ▷ poll for kernels until
    stop signal |

    | 2: | ACQUIRE LOCK(SW)                             |                                      |

    | 3: | if SW.ready.exists( )then                    | ▷ check ready kernels                |

    | 4: | kernel ← SW.ready.pop()                      | ▷ get ready kernel                   |

    | 5: | end if                                       |                                      |

    | 6: | RELEASE LOCK(SW)                             |                                      |

    | 7: | LAUNCH(kernel, stream id)                    | ▷ launch kernel                      |

    | 8: | STREAM SYNC(stream id)                       | ▷ wait for completion                |

    |    | 9: end while                                 |                                      |


    #### *C. ACS-HW Design*


    While ACS-SW enables concurrent execution of kernels and can be fully realized
    in software, it still incurs overheads from *(i)* synchronization with the CPU
    when a kernel completes execution, i.e., the StreamSync primitive that blocks
    the scheduler module thread; and *(ii)* the kernel launch overhead when the scheduler
    module launches a kernel in the CPU. ACS-HW is designed to alleviate these overheads
    with hardware support for kernel scheduling in the GPU.


    Fig. 19 depicts an overview of ACS-HW. ACS-HW comprises a software runtime system
    similar to ACS-SW that maintains an input FIFO queue containing the kernels that
    were invoked by the application 1 . The scheduling window and its management are
    however implemented in hardware on the GPU side 2 . The input queue is essentially
    implemented as a CUDA stream that dispatches kernels to the GPU. In addition to
    the input FIFO queue, the software runtime also maintains a list of kernels in
    the GPU''s scheduling window, which we call the scheduled\_list 3 . To avoid frequent
    synchronization between the CPU and GPU, we allow this list to be stale. Before
    a kernel is inserted into the scheduling window, the software runtime performs
    dependency checks with the scheduled\_list to determine the upstream kernels.
    Note that since the scheduled\_list may be stale, this upstream list needs to
    be further updated before insertion into the scheduling window (discussed below).


    ![](_page_7_Figure_2.jpeg)


    Fig. 19: ACS-HW: Design overview


    The hardware component 4 consists of two modules: *(i)* the scheduling window
    and *(ii)* the upstream load module.


    The hardware scheduling window structure is depicted in Fig. 20 and comprises
    a fixed number of slots (N) 1 . Each slot contains an 8-bit kernel identifier
    and (N-1) 8-bit upstream kernel identifiers that are implemented with SRAM 2 .
    Each slot of the SRAM module is implemented as a single bank of SRAM, contaning
    N-1 fully associated units to store upstream kernel identifiers. These upstream
    identifiers are used to determine when a kernel is ready. An additional two bits
    are used to identify the state of each kernel (i.e., ready, pending, and executing).
    When a kernel completes execution, the upstream identifiers are updated and the
    corresponding state of each kernel is updated. The completed kernel is also removed
    from the scheduling window. Any kernels that are now ready are then dispatched
    to the GPU''s kernel dispatch unit for execution 3 .


    The upstream load module is responsible for refining the upstream list provided
    by the CPU which may be stale in two ways. It may contain kernels that have (1)
    already completed execution and (2) may miss long-running kernels that are still
    executing. The first case is handled by the upstream module by checking against
    a list of kernels in the scheduling window 4 . The second case is avoided by ensuring
    that the scheduled\_list (of size M) in the CPU never misses kernels that are
    still executing. The upstream load module


    ![](_page_7_Figure_7.jpeg)


    Fig. 20: HW scheduling window and upstream load module tracks the oldest scheduled
    kernel 5 . If the number of newer kernels exceeds M (size of the scheduled\_list),
    this module blocks the insertion of more kernels from the CPU 6 .


    #### *D. ACS Overheads*


    (1) Hardware area overhead. ACS-HW introduces the hardware scheduling window which
    contains N slots, where N is the size of the scheduling window. Each slot contains
    N kernel ids of upstream data of 8 bytes each and 2 bits for status. Assuming
    a scheduling window of length N = 32, we require 1KB of SRAM for the scheduling
    module (for the entire GPU). The upstream module keeps track of the oldest executing
    kernel with an 8-bit


    GPU K1 K3 window <sup>2</sup> 4 load module (2) Storage overheads. The read and
    write segments that are saved as metadata in the input FIFO and the scheduled\_list
    by the software runtime in the CPU require memory storage. Each read and write
    segment requires 48 bits to hold the start addresses and the size.


    > (3) Mechanism latencies. ACS-HW requires updating all upstream kernels in each
    slot of the scheduling window every time a kernel completes execution. ACS-HW
    updates each slot in N-1 cycles (where N is the size of the scheduling window).
    Additionally, ACS-HW requires N cycles to insert a kernel ID with its upstream
    kernel IDs into the scheduling window. For a scheduling window of size 64, this
    operation adds 64 cycles (about 50-100ns) overhead to dispatch a ready kernel
    for launch. Thus, ACS-HW adds negligible runtime to the application compared to
    the baseline kernel launch overhead (in the order of a few microseconds).


    > (4) Dependency checking overheads To determine the list of upstream kernels,
    the CPU checks for overlaps between the write segments of the kernel in the input
    queue and the read-write segments of the kernels in the scheduled\_list. As the
    scheduled\_list can fit completely into the cache (4KB), dependency-checking is
    compute-bound and dependent on the number of read and write segments. Table II
    presents the time required to do dependence checking. For a processor with P execution
    units, effective utilization requires dependency checks to be performed in no
    more than T /P, where T is the task execution time [34], [36]. We estimate T /P
    to be around 4us, which is much more than the dependency check latency.


    #### V. METHODOLOGY


    We evaluate ACS-SW on a real hardware setup with an Intel Core i7 11700K CPU (Table
    III) and an NVIDIA RTX3060


    | Window<br>size | Number of<br>RW-segments | Dependency<br>check time |

    |----------------|--------------------------|--------------------------|

    | 16             | 6<br>10                  | 410ns<br>700ns           |

    | 32             | 6<br>10                  | 510ns<br>1640ns          |


    TABLE II: Dependency checking overhead analysis


    GPU (Table IV). We model ACS-HW on GPUs using the Accel-Sim simulator [64], configured
    with parameters of RTX3070 (Table V). We use AccelWattch [64] to model GPU power.
    We choose a scheduling window size of 32.


    | CPU 3.6GHz, OOO 4-wide dispatch window, 32 entry LSQ    |

    |---------------------------------------------------------|

    | L1D + L1I Cache 32KB, 4 way LRU, 1 cycle; 64 Byte line; |

    | L2 Cache 256KB, 8 way LRU, 4 cycle; 64 Byte line;       |

    | L3 Cache 1MB, 16 way LRU, 20 cycle; 64 Byte line;       |

    | DRAM 2-channel; 16-bank; open-row policy, 4GB DDR4      |

    | TABLE III: CPU system configuration                     |


    | Shader core 28 SMs, 1.3GHz; 2 schedulers per SM             |

    |-------------------------------------------------------------|

    | SM Resources 32768 Registers, 32KB Shared memory, 128KB L1D |

    | DRAM 2-channel; 16-bank; open-row policy, 12GB DDR4         |

    | TABLE IV: GPU system configuration                          |


    | Shader core 46 SMs, 1.4GHz; 4 schedulers per SM             |

    |-------------------------------------------------------------|

    | SM Resources 32768 Registers, 32KB Shared memory, 128KB L1D |

    | DRAM 2-channel; 16-bank; open-row policy, 16GB DDR4         |

    | TABLE V: Simulated GPU configuration                        |


    Workloads. We evaluate ACS using:


    (1) Deep RL physics simulations. Brax [1] is a GPU accelerated simulation engine
    for control tasks in reinforcement learning. We evaluate ACS with the Ant (ant),
    Grasp (grasp), Humanoid (human), Cheetah (ct), and Walker2d (w2d) simulation environments.
    These environments are Mu-JoCo [65] simulations for training RL agents to perform
    a specific task. For example, ant contains a 3d robot (the agent) with one torso
    and 4 legs, each with a knee joint, and the goal is to move in a particular direction
    by controlling its legs.


    (2) Dynamic DNNs. We evaluate our approach for 3 dynamic DNN workloads: InstaNAS
    [10] (I-NAS) is a dynamic CNN for image classification. We evaluate our approach
    using the InstaNAS-A architecture on the CIFAR10 dataset. Dynamic routing [12]
    (DR) is a DNN trained for semantic segmentation of images. We evaluate our approach
    on the Dynamic-A 16 layer architecture using the Cityscapes dataset [66]. Conditional
    Convolution [46] (CC) is a mixtureof-experts CNN model for image classification
    where the weights of the convolutions are computed at runtime. We evaluate the
    version of Conditional Convolution with 4 experts that uses an efficientnet b4
    [67] network as the backbone. All three dynamic DNNs are designed for a batch
    size of 1 and the input image defines the DNN architecture. We use Pytorch [54]
    implementations.


    (3) Static DNNs. CNN architectures optimized for low inference latency using neural
    architecture search (NAS): NASNet [41] (NASNet), AmoebaNet [42] (Amoeba), SqueezeNet
    [68] (Squeeze), and RandomWire [44] (RW). These CNNs have highly irregular structures
    with many small kernels. We evaluate ACS with a batch size of 1 on CIFAR10.


    #### VI. EVALUATION


    We evaluate ACS using three designs: *(i)* Baseline: cuDNN implementation (for
    DNNs) and a jax implementation [1] (for deep RL simulation), both using CUDA streams.
    *(ii)* ACS-SW: Our software-only mechanism is evaluated on real hardware. *(iii)*
    ACS-SW-Sim: Our software-only mechanism evaluated on the GPU simulator. We also
    include these results to compare against ACS-HW. *(iv)* ACS-HW: Our hardware-software
    cooperative mechanism evaluated on the GPU simulator. *(v)* CUDAGraph: Framework
    where the inter-kernel dependencies are prepared on the CPU as a directed acyclic
    graph and sent to the GPU ahead of time. We only present ACS-SW results for the
    deep RL workloads as the dynamic and static DNNs heavily use CuDNN libraries that
    do not currently allow modifications to make use of different CUDA streams. We
    instead model the same effect with ACS-SW-Sim.


    #### *A. Deep RL Physics Simulations*


    Fig. 21 depicts the runtimes for the generation of a single batch of training
    data from different simulation environments using ACS-SW, normalized to the baseline
    approach.


    ![](_page_8_Figure_14.jpeg)


    **ant human ct w2d grasp gmean** Fig. 21: Deep RL physics simulations: Normalized
    Speedup


    Fig. 22 depicts the runtimes for ACS-SW-Sim and ACS-HW normalized to the baseline
    implementation. We make two observations. First, ACS-SW-Sim provides similar speedups
    as in real hardware compared to the baseline implementation (up to 1.79× and 1.66×
    on average). Second, ACS-HW is able to further improve performance compared to
    the software-only approach by alleviating the synchronization and kernel launch
    overheads. We observe a slowdown with CUDAGraph due to the significant latency
    of constructing the kernel dependency graph and sending the information to the
    GPU.


    ![](_page_8_Figure_17.jpeg)


    Fig. 22: Deep RL physics simulations: Normalized speedup


    The end-to-end speedup in training tasks (simulation + learning algorithm) as
    observed is shown in Fig. 23. We observe a mean speedup of 1.42× on ACS-HW, and
    1.30× on ACS-SW.


    ![](_page_8_Figure_20.jpeg)


    In Fig. 24, we depict the achieved occupancy for the three configurations. Achieved
    occupancy is calculated as the number of active warps divided by the maximum number
    of active warps supported by the GPU averaged over all clock cycles. We observe
    that the ACS is able to significantly increase the achieved occupancy and thus
    the utilization.


    ![](_page_9_Figure_1.jpeg)


    Fig. 24: Deep RL physics simulations: Achieved occupancy


    #### *B. Inference on Dynamic DNNs*


    Fig. 25 depicts speedup over the baseline for the dynamic DNNs described in §
    V. We observe that ACS is able to provide speedups of up to 1.39× on dynamic DNN
    workloads with ACS-HW and on average 1.05× with ACS-SW and 1.3× with ACS-HW. I-NAS
    suffers a slowdown with ACS-SW because this workload has significant kernel launch
    overheads when parallelized but are hidden in the baseline case where the kernels
    are simply launched serially into a single stream without synchronization. We
    observe that CUDAGraph exhibits a significant slowdown due to the overhead incurred
    during the construction and communication of the DAG dependencies.


    Fig. 26 depicts the corresponding achieved occupancy. We find that the ACS configurations
    are able to significantly improve utilization, leading to performance improvements.


    ![](_page_9_Figure_6.jpeg)


    *C. Inference on Static DNNs*


    While our approach is designed for applications with dynamic computational graphs,
    we also evaluate its effectiveness in improving the concurrency of static DNNs.
    We depict the speedups obtained normalized to the baseline in Fig. 27. We observe
    an average speedup of 1.31× with ACS-HW, and a speedup of 1.16× with ACS-SW. Fig.
    28 depicts the corresponding achieved occupancy. We find that ACS leads to higher
    GPU utilization, leading to performance improvements. As expected, we observe
    that CUDAGraph exhibits similar execution times as ACS-HW for static graphs. This
    is because the task graph needs to be constructed only once.


    #### *D. Sensitivity Analysis*


    Fig. 29 compares the speedups obtained on using scheduling window sizes of 16
    and 32 for ACS-HW over baseline. We observe that the Brax simulations have higher
    performance


    ![](_page_9_Figure_11.jpeg)


    (4.5% on average) with a window size of 32 compared to 16. However, the window
    size has less of an impact on the DNNs. This is because the simulation engines
    have more inter-kernel parallelism that is exposed with a larger scheduling window.


    ![](_page_9_Figure_13.jpeg)


    *E. Comparison with Persistent Thread Frameworks*


    Persistent threads (PT) [58], [61], [69], [70] are used to efficiently schedule
    multiple tasks with dynamically determined dependencies. These tasks are executed
    using threads of a *single kernel*. Thus, it assumes all tasks are *homogeneous*,
    requiring the same number of registers and shared memory. PT frameworks which
    allow heterogeneous kernels are nontrivial and would be inefficient as the persistent
    kernel must be configured to use the maximum registers/scratchpad used by any
    kernel [58]. We use the persistent thread framework implementation from juggler
    [61] and adapted it to handle heterogeneous kernels. We were only able to implement
    a section of a rigid body simulator (used for finding contacts between pairs of
    rigid bodies). This routine invokes a different kernel (with different register
    usages) for different pairs of geometries. We implement these kernels as tasks
    of our PT framework and find that it is 1.35× slower than baseline. This slowdown
    is due to inefficient use of registers/scratchpad by the kernel that leads to
    lower parallelism.


    #### VII. RELATED WORK


    In this work, we (i) observe that input-dependent interkernel dependencies and
    small kernels are a significant performance bottleneck in a range of important
    applications such as simulation engines in deep RL and dynamic neural networks;
    and (ii) propose both a software-only and hardware-software cooperative mechanism
    to enable concurrent execution of kernels with statically unknown inter-kernel
    dependencies. In this section, we describe prior work that aim to improve GPU
    utilization and kernel concurrency.


    Leveraging concurrent streams in DL workloads. Mainstream deep learning frameworks
    like Tensorflow [53] and Pytorch [54] launch GPU kernels into a single CUDA stream
    that executes them sequentially. Recent works [50], [52], [71] propose software
    techniques to enable concurrent execution of GPU kernels using multiple streams
    with static scheduling and stream assignment before application execution. Inter-operator
    scheduling [50] partitions a computation graph into sections of kernels that can
    execute in parallel. Out-of-order backprop [52] observes that gradient computation
    can be parallelized using CUDA streams into weight gradients and the output gradient
    computation during backpropagation. However, these works are only applicable to
    DL workloads whose computation graph is static and known ahead of time, often
    requiring significant compilation times. Furthermore, these approaches incur high
    synchronization overheads.


    Task-based programming frameworks in CPUs. Taskbased frameworks [72]–[74] enable
    programmers to describe a program as multiple tasks which are scheduled for execution
    in multiprocessor architectures [75]. Works such as task superscalar [34], carbon
    [35], TDM [36] and ADM [37] propose out-of-order scheduling of tasks to efficiently
    leverage irregular parallelism in multiprocessors. The major bottleneck in outof-order
    scheduling of tasks dynamically for multiprocessors is the long latency required
    to do dependence checks. Thus, prior work [34]–[37] propose hardware accelerators
    to address the long latency dependence checking needed at runtime. However, with
    GPUs, the primary bottleneck is the long latency to launch/signal completion of
    kernels instead, requiring a different approach to enable out-of-order scheduling.


    Programmer annotations Prior works leverage programmer-specified annotations as
    hints to the compiler to extract parallelism. DeNovo [76] uses programmer annotations
    that encode the data read and written to by each method/function. This information
    is used at compile time to determine independent tasks that can be scheduled.
    Some frameworks [74], [77]–[80] allow programmers to annotate the array regions
    accessed by each task as a compile time directive. In ACS, we use a similar approach
    of programmer annotations to help determine parallelism at runtime to enable out-of-order
    kernel scheduling.


    Software techniques to improve GPU utilization with concurrent kernel execution.
    CUDA Graphs [32] and AMD ATMI [33], [81], [82] are frameworks that allow users
    to define dependencies between kernels as a directed-acyclicgraph (DAG) prior
    to execution. This approach eliminates synchronization and kernel launch overheads
    due to communication with the CPU. Nimble [51] identifies independent GPU kernels
    prior to execution and concurrently schedules independent kernels using CUDA streams.
    This approach uses CUDA Graphs [32] to reduce synchronization and kernel launch
    overheads. Irregular graphs are also seen in solving sparse linear equations for
    CFD simulations [83] and hyperplane sweep routines [84], where DAG frameworks
    have been shown to be effective.We quantitatively compared ACS against a CUDA
    graph implementation in § VI. None of these approaches is applicable to dynamic
    input-dependent computational graphs, as caching dependency information and constructing
    CUDA Graphs incur non-trivial latencies ( § II-D).


    Hardware support for concurrent kernels. Wireframe [85] proposes merging multiple
    kernels into a single large kernel and performs CTA scheduling with data dependency
    checks between CTAs. Blockmaestro [86] enables concurrently running kernels by
    identifying dependencies between their CTAs. These approaches however perform
    dependence checks by tracing and extracting the memory loads and stores performed
    by each thread block of every kernel. Similar to the software approaches, these
    approaches are designed for static computational graphs. The proposed scheduling
    and dependency check techniques would be too time-consuming for runtime scheduling.
    GPU dynamic parallelism [59], [87]–[89] enables launching kernels from the device
    itself and allows data dependencies between a single parent and multiple child
    kernels. However, Dynamic-NN and RL simulation workloads contain kernels that
    depend on multiple kernels, making it difficult to apply GPU dynamic parallelism.


    Compilers, runtime systems for dynamic neural networks. Prior software [11], [90]–[95]
    and hardware approaches [96] aim to optimize CPU-GPU communication overheads,
    launch overheads, and blocking synchronization calls for dynamic computational
    graphs. These approaches introduce techniques such as dynamic batching and kernel
    fusion. However, these works are orthogonal to our approach. Prior works [97],
    [98] have proposed software frameworks for CPU-GPU systems that provide simplified
    and convenient abstractions to interface with GPU runtime APIs. These frameworks
    encapsulate runtime-level code, simplifying code development for programmers in
    single and multi-GPU environments. However, these works do not specifically focus
    on input-dependent dynamic computation. Instead their goal is to provide simpler
    abstractions for programming GPU tasks and expressing dataflow dependencies between
    them. Efficient GPU sharing techniques, such as Kernelet [99], GPUPool [100] introduce
    runtime systems to enable concurrent kernel execution by scheduling kernels from
    different processes which have different memory and compute usage intensities.
    However, while these works increase overall GPU utilization by kernels launched
    from different processes, they do not leverage the parallelism between kernels
    of a single application.


    # VIII. CONCLUSION


    We introduce ACS, the first framework that enables automatic concurrent kernel
    execution with low overhead runtime scheduling and dependency checks. The key
    idea behind ACS is to dynamically schedule a small window of kernels by identifying
    which kernel(s) within the window is ready for execution. ACS leverages kernel
    annotations to automatically identify kernel dependencies at runtime. We implement
    ACS as both a software framework and a hardware-software mechanism that is able
    to further reduce synchronization overheads from CPU-GPU communication. We demonstrate
    that ACS can improve the performance of important emerging classes of workloads,
    such as RL simulations and dynamic DNNs, whose kernel dependencies are irregular
    and vary with input.


    #### REFERENCES


    - [1] C. D. Freeman, E. Frey, A. Raichuk, S. Girgin, I. Mordatch, and O. Bachem,
    "Brax - a differentiable physics engine for large scale rigid body simulation,"
    *ArXiv*, vol. abs/2106.13281, 2021.

    - [2] V. Makoviychuk, L. Wawrzyniak, Y. Guo, M. Lu, K. Storey, M. Macklin, D.
    Hoeller, N. Rudin, A. Allshire, A. Handa, and G. State, "Isaac gym: High performance
    gpu-based physics simulation for robot learning," *ArXiv*, vol. abs/2108.10470,
    2021.

    - [3] B. Shacklett, E. Wijmans, A. Petrenko, M. Savva, D. Batra, V. Koltun, and
    K. Fatahalian, "Large batch simulation for deep reinforcement learning," *ArXiv*,
    vol. abs/2103.07013, 2021.

    - [4] S. Dalton and I. Frosio, "Accelerating reinforcement learning through gpu
    atari emulation," *arXiv: Learning*, 2020.

    - [5] A. Petrenko, Z. Huang, T. Kumar, G. Sukhatme, and V. Koltun, "Sample factory:
    Egocentric 3d control from pixels at 100000 fps with asynchronous reinforcement
    learning," in *ICML*, 2020.

    - [6] K. Yuan, Q. Li, S. Guo, D. Chen, A. Zhou, F. Yu, and Z. Liu, "Differentiable
    dynamic wirings for neural networks," *2021 IEEE/CVF International Conference
    on Computer Vision (ICCV)*, pp. 317–326, 2021.

    - [7] L. Liu and J. Deng, "Dynamic deep neural networks: Optimizing accuracy-efficiency
    trade-offs by selective execution," in *AAAI*, 2018.

    - [8] Z. Yuan, B. Wu, Z. Liang, S. Zhao, W. Bi, and G. Sun, "S2dnas: Transforming
    static cnn model for dynamic inference via neural architecture search," *ArXiv*,
    vol. abs/1911.07033, 2020.

    - [9] Y. Han, G. Huang, S. Song, L. Yang, H. Wang, and Y. Wang, "Dynamic neural
    networks: A survey," *IEEE Transactions on Pattern Analysis and Machine Intelligence*,
    vol. 44, pp. 7436–7456, 2022.

    - [10] A. Cheng, C. H. Lin, D.-C. Juan, W. Wei, and M. Sun, "Instanas: Instance-aware
    neural architecture search," in *AAAI*, 2020.

    - [11] J. Wei, G. Gibson, V. Vasudevan, and E. Xing, "Dynamic scheduling for dynamic
    control flow in deep learning systems," *URL http://www. cs. cmu. edu/jinlianw/papers/dynamic
    scheduling nips18 sysml. pdf*, 2018.

    - [12] S. Cai, Y. Shu, and W. Wang, "Dynamic routing networks," *2021 IEEE Winter
    Conference on Applications of Computer Vision (WACV)*, pp. 3587–3596, 2021.

    - [13] H. Wang, S. Li, S.-C. Su, Z. Qin, and X. Li, "Rdi-net: Relational dynamic
    inference networks," *2021 IEEE/CVF International Conference on Computer Vision
    (ICCV)*, pp. 4601–4610, 2021.

    - [14] P. Singh and V. P. Namboodiri, "Skipconv: skip convolution for computationally
    efficient deep cnns," in *2020 International Joint Conference on Neural Networks
    (IJCNN)*, pp. 1–8, IEEE, 2020.

    - [15] S. Teerapittayanon, B. McDanel, and H. T. Kung, "Branchynet: Fast inference
    via early exiting from deep neural networks," *2016 23rd International Conference
    on Pattern Recognition (ICPR)*, pp. 2464– 2469, 2016.

    - [16] Z. Wu, T. Nagarajan, A. Kumar, S. Rennie, L. S. Davis, K. Grauman, and
    R. Feris, "Blockdrop: Dynamic inference paths in residual networks," in *CVPR*,
    2018.

    - [17] A. Veit and S. J. Belongie, "Convolutional networks with adaptive inference
    graphs," *International Journal of Computer Vision*, vol. 128, pp. 730–741, 2019.

    - [18] Y. Li, Y. Chen, X. Dai, D. Chen, M. Liu, L. Yuan, Z. Liu, L. Zhang, and
    N. Vasconcelos, "Micronet: Improving image recognition with extremely low flops,"
    *2021 IEEE/CVF International Conference on Computer Vision (ICCV)*, pp. 458–467,
    2021.

    - [19] W. Xia, H. Yin, X. Dai, and N. K. Jha, "Fully dynamic inference with deep
    neural networks," *IEEE Transactions on Emerging Topics in Computing*, vol. 10,
    pp. 962–972, 2022.

    - [20] A. Kendall, J. Hawke, D. Janz, P. Mazur, D. Reda, J. M. Allen, V.-D. Lam,
    A. Bewley, and A. Shah, "Learning to drive in a day," *2019 International Conference
    on Robotics and Automation (ICRA)*, pp. 8248–8254, 2019.

    - [21] T. Chen, J. Xu, and P. Agrawal, "A system for general in-hand object re-orientation,"
    in *Conference on Robot Learning*, pp. 297–307, PMLR, 2022.

    - [22] J. Panerati, H. Zheng, S. Zhou, J. Xu, A. Prorok, A. P. S. U. of Toronto
    Institute for A Studies, V. I. for Artificial Intelligence, and U. of Cambridge,
    "Learning to fly—a gym environment with pybullet physics for reinforcement learning
    of multi-agent quadcopter control," *2021 IEEE/RSJ International Conference on
    Intelligent Robots and Systems (IROS)*, pp. 7512–7519, 2021.

    - [23] L. Bartolomei, L. Teixeira, and M. Chli, "Semantic-aware active perception
    for uavs using deep reinforcement learning," in *2021 IEEE/RSJ International Conference
    on Intelligent Robots and Systems (IROS)*, pp. 3101–3108, 2021.

    - [24] J. Chen, S. E. Li, and M. Tomizuka, "Interpretable end-to-end urban autonomous
    driving with latent deep reinforcement learning," *arXiv preprint arXiv:2001.08726*,
    2020.

    - [25] S. Krishnan, B. Boroujerdian, W. Fu, A. Faust, and V. J. Reddi, "Air learning:
    a deep reinforcement learning gym for autonomous aerial robot visual navigation,"
    *Mach. Learn.*, vol. 110, pp. 2501–2540, 2021.

    - [26] Z. Xie, X. Da, B. Babich, A. Garg, and M. van de Panne, "Glide: Generalizable
    quadrupedal locomotion in diverse environments with a centroidal model," *arXiv
    preprint arXiv:2104.09771*, 2021.

    - [27] Z. Si and W. Yuan, "Taxim: An example-based simulation model for gelsight
    tactile sensors," *IEEE Robotics and Automation Letters*, vol. 7, no. 2, pp. 2361–2368,
    2022.

    - [28] N. Rudin, D. Hoeller, P. Reist, and M. Hutter, "Learning to walk in minutes
    using massively parallel deep reinforcement learning," *ArXiv*, vol. abs/2109.11978,
    2021.

    - [29] S. Rajbhandari, C. Li, Z. Yao, M. Zhang, R. Y. Aminabadi, A. A. Awan, J.
    Rasley, and Y. He, "Deepspeed-moe: Advancing mixture-ofexperts inference and training
    to power next-generation ai scale," in *ICML*, 2022.

    - [30] "Nvidia inc, hyperq." https://developer.download.nvidia.com/compute/ DevZone/C/html
    x64/6 Advanced/simpleHyperQ/doc/HyperQ.pdf. Accessed: 2023-07-21.

    - [31] "Nvidia inc, cuda programming guide." https://docs.nvidia.com/cuda/ cuda-c-programming-guide/index.html#streams.
    Accessed: 2022-11- 21.

    - [32] "Nvidia inc, getting started with cuda graphs." https://developer.nvidia.
    com/blog/cuda-graphs/. Accessed: 2020-09-30.

    - [33] "Radeon open compute, atmi (asynchronous task and memory interface)." https://github.com/RadeonOpenCompute/atmi.
    Accessed: 2022- 09-30.

    - [34] Y. Etsion, F. Cabarcas, A. Rico, A. Ramirez, R. M. Badia, E. Ayguade, J.
    Labarta, and M. Valero, "Task superscalar: An out-of-order task pipeline," in
    *2010 43rd Annual IEEE/ACM International Symposium on Microarchitecture*, pp.
    89–100, IEEE, 2010.

    - [35] S. Kumar, C. J. Hughes, and A. D. Nguyen, "Carbon: architectural support
    for fine-grained parallelism on chip multiprocessors," in *International Symposium
    on Computer Architecture*, 2007.

    - [36] E. Castillo, L. Alvarez, M. Moreto, M. Casas, E. Vallejo, J. L. Bosque,
    ´ R. Beivide, and M. Valero, "Architectural support for task dependence management
    with flexible software scheduling," *2018 IEEE International Symposium on High
    Performance Computer Architecture (HPCA)*, pp. 283–295, 2018.

    - [37] D. Sanchez, R. M. Yoo, and C. E. Kozyrakis, "Flexible architectural ´ support
    for fine-grain scheduling," in *ASPLOS XV*, 2010.

    - [38] S. Puthoor, X. Tang, J. Gross, and B. M. Beckmann, "Oversubscribed command
    queues in gpus," *Proceedings of the 11th Workshop on General Purpose GPUs*, 2018.

    - [39] J. Gleeson, D. Snider, Y. Yang, M. Gabel, E. de Lara, and G. Pekhimenko,
    "Optimizing data collection in deep reinforcement learning," *ArXiv*, vol. abs/2207.07736,
    2022.

    - [40] J. Gleeson, S. Krishnan, M. Gabel, V. J. Reddi, E. de Lara, and G. Pekhimenko,
    "Rl-scope: Cross-stack profiling for deep reinforcement learning workloads," *ArXiv*,
    vol. abs/2102.04285, 2021.

    - [41] B. Zoph, V. Vasudevan, J. Shlens, and Q. V. Le, "Learning transferable
    architectures for scalable image recognition," *2018 IEEE/CVF Conference on Computer
    Vision and Pattern Recognition*, pp. 8697–8710, 2018.

    - [42] E. Real, A. Aggarwal, Y. Huang, and Q. V. Le, "Regularized evolution for
    image classifier architecture search," in *AAAI*, 2019.

    - [43] H. Liu, K. Simonyan, and Y. Yang, "Darts: Differentiable architecture search,"
    *arXiv preprint arXiv:1806.09055*, 2018.

    - [44] S. Xie, A. Kirillov, R. B. Girshick, and K. He, "Exploring randomly wired
    neural networks for image recognition," *2019 IEEE/CVF International Conference
    on Computer Vision (ICCV)*, pp. 1284–1293, 2019.

    - [45] H. Bai, F. Zhou, L. Hong, N. Ye, S.-H. G. Chan, and Z. Li, "Nasood: Neural
    architecture search for out-of-distribution generalization," *2021 IEEE/CVF International
    Conference on Computer Vision (ICCV)*, pp. 8300–8309, 2021.

    - [46] B. Yang, G. Bender, Q. V. Le, and J. Ngiam, "Condconv: Conditionally parameterized
    convolutions for efficient inference," in *NeurIPS*, 2019.

    - [47] Z. You, S. Feng, D. Su, and D. Yu, "Speechmoe: Scaling to large acoustic
    models with dynamic routing mixture of experts," *arXiv preprint arXiv:2105.03036*,
    2021.

    - [48] N. M. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. V. Le, G. E. Hinton,
    and J. Dean, "Outrageously large neural networks: The sparsely-gated mixture-of-experts
    layer," *ArXiv*, vol. abs/1701.06538, 2017.

    - [49] M. Sandler, A. G. Howard, M. Zhu, A. Zhmoginov, and L.-C. Chen, "Mobilenetv2:
    Inverted residuals and linear bottlenecks," *2018 IEEE/CVF Conference on Computer
    Vision and Pattern Recognition*, pp. 4510–4520, 2018.

    - [50] Y. Ding, L. Zhu, Z. Jia, G. Pekhimenko, and S. Han, "Ios: Inter-operator
    scheduler for cnn acceleration," *ArXiv*, vol. abs/2011.01302, 2021.

    - [51] W. Kwon, G.-I. Yu, E. Jeong, and B.-G. Chun, "Nimble: Lightweight and parallel
    gpu task scheduling for deep learning," in *NeurIPS*, 2020.

    - [52] H. Oh, J. Lee, H. Kim, and J. Seo, "Out-of-order backprop: an effective
    scheduling technique for deep learning," *Proceedings of the Seventeenth European
    Conference on Computer Systems*, 2022.

    - [53] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin, S.
    Ghemawat, G. Irving, M. Isard, M. Kudlur, J. Levenberg, R. Monga, S. Moore, D.
    G. Murray, B. Steiner, P. A. Tucker, V. Vasudevan, P. Warden, M. Wicke, Y. Yu,
    and X. Zhang, "Tensorflow: A system for large-scale machine learning," *ArXiv*,
    vol. abs/1605.08695, 2016.

    - [54] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen,
    Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. Kopf, ¨ E. Yang, Z. DeVito,
    M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala,
    "Pytorch: An imperative style, highperformance deep learning library," *ArXiv*,
    vol. abs/1912.01703, 2019.

    - [55] T. Chen, M. Li, Y. Li, M. Lin, N. Wang, M. Wang, T. Xiao, B. Xu, C. Zhang,
    and Z. Zhang, "Mxnet: A flexible and efficient machine learning library for heterogeneous
    distributed systems," *ArXiv*, vol. abs/1512.01274, 2015.

    - [56] S. Puthoor, A. M. Aji, S. Che, M. Daga, W. Wu, B. M. Beckmann, and G. P.
    Rodgers, "Implementing directed acyclic graphs with the heterogeneous system architecture,"
    *Proceedings of the 9th Annual Workshop on General Purpose Processing using Graphics
    Processing Unit*, 2016.

    - [57] HSA Foundation, "Hsa standard," 2017. http://hsafoundation.com/ standards/,
    Last accessed on 2023-02-14.

    - [58] Y. Chen, B. Brock, S. D. Porumbescu, A. Bulucc, K. A. Yelick, and J. D.
    Owens, "Atos: A task-parallel gpu dynamic scheduling framework for dynamic irregular
    computations," *ArXiv*, vol. abs/2112.00132, 2021.

    - [59] "Nvidia inc, cuda dynamic parallelism." https://developer.nvidia.com/ blog/cuda-dynamic-parallelism-api-principles/.
    Accessed: 2022-09-30.

    - [60] "Amd inc, rocm device enqueue." https://sep5.readthedocs.io/en/latest/
    Programming Guides/Opencl-programming-guide.html#device-sideenqueue. Accessed:
    2022-09-30.

    - [61] M. E. Belviranli, S. Lee, J. S. Vetter, and L. N. Bhuyan, "Juggler: a dependence-aware
    task-based execution framework for gpus," *Proceedings of the 23rd ACM SIGPLAN
    Symposium on Principles and Practice of Parallel Programming*, 2018.

    - [62] M. Steinberger, M. Kenzel, P. Boechat, B. Kerbl, M. Dokter, and D. Schmalstieg,
    "Whippletree: task-based scheduling of dynamic workloads on the gpu," *ACM Trans.
    Graph.*, vol. 33, pp. 228:1–228:11, 2014.

    - [63] N. Farooqui, A. Kerr, G. Diamos, S. Yalamanchili, and K. Schwan, "A framework
    for dynamically instrumenting gpu compute applications within gpu ocelot," in
    *Proceedings of the Fourth Workshop on General Purpose Processing on Graphics
    Processing Units*, pp. 1–9, 2011.

    - [64] M. Khairy, Z. Shen, T. M. Aamodt, and T. G. Rogers, "Accel-sim: An extensible
    simulation framework for validated gpu modeling," *2020 ACM/IEEE 47th Annual International
    Symposium on Computer Architecture (ISCA)*, pp. 473–486, 2020.

    - [65] E. Todorov, T. Erez, and Y. Tassa, "Mujoco: A physics engine for model-based
    control," in *2012 IEEE/RSJ international conference on intelligent robots and
    systems*, pp. 5026–5033, IEEE, 2012.

    - [66] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson, U.
    Franke, S. Roth, and B. Schiele, "The cityscapes dataset for semantic urban scene
    understanding," in *Proceedings of the IEEE conference on computer vision and
    pattern recognition*, pp. 3213–3223, 2016.

    - [67] M. Tan and Q. Le, "Efficientnet: Rethinking model scaling for convolutional
    neural networks," in *International conference on machine learning*, pp. 6105–6114,
    PMLR, 2019.

    - [68] F. N. Iandola, M. W. Moskewicz, K. Ashraf, S. Han, W. J. Dally, and K.
    Keutzer, "Squeezenet: Alexnet-level accuracy with 50x fewer parameters and ¡1mb
    model size," *ArXiv*, vol. abs/1602.07360, 2016.

    - [69] K. Gupta, J. A. Stuart, and J. D. Owens, "A study of persistent threads
    style gpu programming for gpgpu workloads," *2012 Innovative Parallel Computing
    (InPar)*, pp. 1–14, 2012.

    - [70] T. Aila and S. Laine, "Understanding the efficiency of ray traversal on
    gpus," *Proceedings of the Conference on High Performance Graphics 2009*, 2009.

    - [71] H. Zhu, A. Phanishayee, and G. Pekhimenko, "Daydream: Accurately estimating
    the efficacy of optimizations for DNN training," in *2020 USENIX Annual Technical
    Conference (USENIX ATC 20)*, pp. 337– 352, USENIX Association, July 2020.

    - [72] J. Reinders, M. J. Voss, P. Reble, and R. Asenjo-Plaza, "++ for heterogeneous
    programming: oneapi (dpc++ and onetbb)," in *C++ for Heterogeneous Programming:
    oneAPI (DPC++ and oneTBB)*, 2020.

    - [73] R. D. Blumofe, C. F. Joerg, B. C. Kuszmaul, C. E. Leiserson, K. H. Randall,
    and Y. Zhou, "Cilk: an efficient multithreaded runtime system," in *PPOPP ''95*,
    1995.

    - [74] L. Dagum and R. Menon, "Openmp: an industry standard api for shared-memory
    programming," in *OpenMP: an industry standard API for shared-memory programming*,
    1998.

    - [75] A. Ram´ırez, F. Cabarcas, B. H. H. Juurlink, M. Alvarez-Mesa, F. Sanchez,
    A. Azevedo, C. Meenderinck, C. B. Ciobanu, S. Isaza, and ´ G. Gaydadjiev, "The
    sarc architecture," *IEEE Micro*, vol. 30, pp. 16–29, 2010.

    - [76] B. Choi, R. Komuravelli, H. Sung, R. Smolinski, N. Honarmand, S. V. Adve,
    V. S. Adve, N. P. Carter, and C.-T. Chou, "Denovo: Rethinking the memory hierarchy
    for disciplined parallelism," *2011 International Conference on Parallel Architectures
    and Compilation Techniques*, pp. 155–166, 2011.

    - [77] J. Planas, R. M. Badia, E. Ayguade, and J. Labarta, "Hierarchical task-
    ´ based programming with starss," *The International Journal of High Performance
    Computing Applications*, vol. 23, pp. 284 – 299, 2009.

    - [78] A. Pop and A. Cohen, "Openstream: Expressiveness and data-flow compilation
    of openmp streaming programs," *ACM Trans. Archit. Code Optim.*, vol. 9, pp. 53:1–53:25,
    2012.

    - [79] G. Gupta and G. S. Sohi, "Dataflow execution of sequential imperative programs
    on multicore architectures," in *Proceedings of the 44th annual IEEE/ACM international
    symposium on Microarchitecture*, pp. 59–70, 2011.

    - [80] M. D. Allen, S. Sridharan, and G. S. Sohi, "Serialization sets: a dynamic
    dependence-based parallel execution model," in *Proceedings of the 14th ACM SIGPLAN
    symposium on Principles and practice of parallel programming*, pp. 85–96, 2009.

    - [81] AMD Research, "Dagee," 2017. https://github.com/AMDResearch/ DAGEE.git,
    Last accessed on 2023-02-14.

    - [82] AMD Research, "Hipgraph," 2017. https://github.com/HipGraph/, Last accessed
    on 2023-02-14.

    - [83] A. E. Helal, A. M. Aji, M. L. Chu, B. M. Beckmann, and W. chun Feng, "Adaptive
    task aggregation for high-performance sparse solvers on gpus," *2019 28th International
    Conference on Parallel Architectures and Compilation Techniques (PACT)*, pp. 324–336,
    2019.

    - [84] A. M. Kaushik, A. M. Aji, M. A. Hassaan, N. Chalmers, N. Wolfe, S. Moe,
    S. Puthoor, and B. M. Beckmann, "Optimizing hyperplane sweep operations using
    asynchronous multi-grain gpu tasks," *2019 IEEE International Symposium on Workload
    Characterization (IISWC)*, pp. 59–69, 2019.

    - [85] A. Abdolrashidi, D. Tripathy, M. E. Belviranli, L. N. Bhuyan, and D. Wong,
    "Wireframe: Supporting data-dependent parallelism through dependency graph execution
    in gpus," *2017 50th Annual IEEE/ACM International Symposium on Microarchitecture
    (MICRO)*, pp. 600–611, 2017.

    - [86] A. Abdolrashidi, H. A. Esfeden, A. Jahanshahi, K. Singh, N. B. Abu-Ghazaleh,
    and D. Wong, "Blockmaestro: Enabling programmertransparent task-based execution
    in gpu systems," *2021 ACM/IEEE 48th Annual International Symposium on Computer
    Architecture (ISCA)*, pp. 333–346, 2021.

    - [87] G. Chen and X. Shen, "Free launch: Optimizing gpu dynamic kernel launches
    through thread reuse," *2015 48th Annual IEEE/ACM International Symposium on Microarchitecture
    (MICRO)*, pp. 407–419, 2015.

    - [88] I. E. Hajj, J. Gomez-Luna, C. Li, L.-W. Chang, D. S. Milojicic, and ´ W.
    mei W. Hwu, "Klap: Kernel launch aggregation and promotion for optimizing dynamic
    parallelism," *2016 49th Annual IEEE/ACM International Symposium on Microarchitecture
    (MICRO)*, pp. 1–12, 2016.

    - [89] J. Wang, N. Rubin, A. Sidelnik, and S. Yalamanchili, "Dynamic thread block
    launch: A lightweight execution mechanism to support irregular applications on
    gpus," *2015 ACM/IEEE 42nd Annual International Symposium on Computer Architecture
    (ISCA)*, pp. 528–540, 2015.

    - [90] P. Fegade, T. Chen, P. Gibbons, and T. Mowry, "Cortex: A compiler for recursive
    deep learning models," *Proceedings of Machine Learning and Systems*, vol. 3,
    pp. 38–54, 2021.

    - [91] E. Jeong, S. Cho, G.-I. Yu, J. S. Jeong, D.-J. Shin, and B.-G. Chun, "{JANUS}:
    fast and flexible deep learning via symbolic graph execution of imperative programs,"
    in *16th USENIX Symposium on Networked Systems Design and Implementation (NSDI
    19)*, pp. 453– 468, 2019.

    - [92] S. Xu, H. Zhang, G. Neubig, W. Dai, J. K. Kim, Z. Deng, Q. Ho, G. Yang,
    and E. P. Xing, "Cavs: An efficient runtime system for dynamic neural networks,"
    in *2018 USENIX Annual Technical Conference (USENIX ATC 18)*, pp. 937–950, 2018.

    - [93] H. Shen, J. Roesch, Z. Chen, W. Chen, Y. Wu, M. Li, V. Sharma, Z. Tatlock,
    and Y. Wang, "Nimble: Efficiently compiling dynamic neural networks for model
    inference," *ArXiv*, vol. abs/2006.03031, 2021.

    - [94] E. Jeong, J. S. Jeong, S. Kim, G.-I. Yu, and B.-G. Chun, "Improving the
    expressiveness of deep learning frameworks with recursion," in *Proceedings of
    the Thirteenth EuroSys Conference*, pp. 1–13, 2018.

    - [95] M. Looks, M. Herreshoff, D. S. Hutchins, and P. Norvig, "Deep learning
    with dynamic computation graphs," *ArXiv*, vol. abs/1702.02181, 2017.

    - [96] F. Khorasani, H. A. Esfeden, N. B. Abu-Ghazaleh, and V. Sarkar, "In-register
    parameter caching for dynamic neural nets with virtual persistent processor specialization,"
    *2018 51st Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)*,
    pp. 377–389, 2018.

    - [97] C. J. Rossbach, J. Currey, M. Silberstein, B. Ray, and E. Witchel, "Ptask:
    operating system abstractions to manage gpus as compute devices," in *Proceedings
    of the Twenty-Third ACM Symposium on Operating Systems Principles*, pp. 233–248,
    2011.

    - [98] C. J. Rossbach, Y. Yu, J. Currey, J.-P. Martin, and D. Fetterly, "Dandelion:
    a compiler and runtime for heterogeneous systems," in *Proceedings of the Twenty-Fourth
    ACM Symposium on Operating Systems Principles*, pp. 49–68, 2013.

    - [99] J. Zhong and B. He, "Kernelet: High-throughput gpu kernel executions with
    dynamic slicing and scheduling," *IEEE Transactions on Parallel and Distributed
    Systems*, vol. 25, no. 6, pp. 1522–1532, 2013.

    - [100] X. Tan, *GPUPool: A Holistic Approach to Fine-Grained GPU Sharing in the
    Cloud*. PhD thesis, University of Toronto (Canada), 2021.'
  decisions:
    evaluation_prompt: 'Qualified. Reason: The paper includes evaluations of GPU utilization
      and mentions phrases like "Our evaluations demonstrate" and "We demonstrate
      the effectiveness of ACS," indicating experimental analysis.'
    related_work_prompt: 'Qualified. Reason: The paper engages with prior research
      by citing numerous research papers, explaining and comparing to cited work,
      and includes a section discussing related work.'
    novelty_prompt: Qualified.
    review_only_prompt: Qualified.
- title: "SpecLLM: Exploring Generation and Review of VLSI Design Specification\n\
    \  with Large Language Model"
  abstract: 'The development of architecture specifications is an initial and fundamental

    stage of the integrated circuit (IC) design process. Traditionally,

    architecture specifications are crafted by experienced chip architects, a

    process that is not only time-consuming but also error-prone. Mistakes in these

    specifications may significantly affect subsequent stages of chip design.

    Despite the presence of advanced electronic design automation (EDA) tools,

    effective solutions to these specification-related challenges remain scarce.

    Since writing architecture specifications is naturally a natural language

    processing (NLP) task, this paper pioneers the automation of architecture

    specification development with the advanced capabilities of large language

    models (LLMs). Leveraging our definition and dataset, we explore the

    application of LLMs in two key aspects of architecture specification

    development: (1) Generating architecture specifications, which includes both

    writing specifications from scratch and converting RTL code into detailed

    specifications. (2) Reviewing existing architecture specifications. We got

    promising results indicating that LLMs may revolutionize how these critical

    specification documents are developed in IC design nowadays. By reducing the

    effort required, LLMs open up new possibilities for efficiency and accuracy in

    this crucial aspect of chip design.'
  url: http://arxiv.org/abs/2401.13266v1
  keywords: ''
  document: '# I. INTRODUCTION


    Developing architecture specifications is a critical initial step in the process
    of IC design. It lays the foundational framework and guidelines necessary for
    the subsequent stages of design and development. Traditionally, the task of writing
    and reviewing architecture specifications is undertaken by skilled chip architects.
    This process, while expertise-driven, tends to be time-consuming and can be susceptible
    to human errors, soliciting a more automated methodology to enhance efficiency
    and accuracy.


    In recent years, LLMs such as ChatGPT [\[1\]](#page-7-0) have showcased remarkable
    capabilities in the field of artificial intelligence, with a wide range of applications
    from question answering to content creation. The growth of chip computing power
    will endow LLMs with greater capabilities. Consequently, researchers have started
    to investigate the potential of LLMs in augmenting the process of chip design,
    reversely enhancing


    <span id="page-0-0"></span>![](_page_0_Figure_11.jpeg)


    Fig. 1: The overall structure of this paper. We first propose basic definitions
    and an organized dataset dedicated to architecture specifications. Leveraging
    them, we explore the use of LLMs in the generation and review of architecture
    specifications.


    the computing power of the chips themselves. For instance, recent studies [\[2,](#page-8-0)
    [3,](#page-8-1) [4,](#page-8-2) [5,](#page-8-3) [6,](#page-8-4) [7,](#page-8-5)
    [8,](#page-8-6) [9\]](#page-8-7) have utilized LLMs to generate RTL code like
    Verilog. Other works develop LLM-based solution to control EDA tools [\[9,](#page-8-7)
    [10\]](#page-8-8), design AI accelerator architectures [\[11,](#page-8-9) [12\]](#page-8-10),
    hardware security assertion generation [\[13\]](#page-8-11), fix security bugs
    [\[14\]](#page-8-12), etc. These research efforts imply a promising future for
    LLMs in chip design. In this paper, we conduct a pioneering investigation in the
    potential and practicality of LLMs in processing specifications. To the best of
    our knowledge, there has been no prior design automation or LLM research focusing
    on this important topic.


    This paper focuses on employing LLMs to address the challenges inherent in the
    traditional management of architecture specifications. Figure [1](#page-0-0) highlights
    the overall structure of this paper. Considering the absence of a formal definition
    or categorization of architecture specifications, we start with investigating
    existing architecture specifications across a diverse range of products. Then
    we categorize architecture specifications into three levels: Highest-level Architecture
    Specification (HAS), Middle-level Architecture Specification (MAS), and Lowest-level
    Architecture Specification (LAS). HAS is defined as the specification that establishes
    standards applicable across


    <sup>1</sup>The dataset will be available at https://github.com/hkust-zhiyao/SpecLLM


    a range of products. It is at the highest abstraction level. MAS represents the
    specification that outlines the high-level architecture of a single-chip design.
    LAS is the detailed specification focused on the microarchitecture design of an
    individual chip.


    Building upon our definition, we have assembled a comprehensive dataset that includes
    46 public architecture specifications from various types of products. For the
    HAS, MAS, or LAS, the architecture specifications related to RISC-V are among
    the most accessible and widely available. In contrast, architecture specifications
    for other instruction sets are relatively scarce. We have also observed that current
    architecture specifications suffer from a lack of unified writing standards, and
    the length of these specifications may exceed the processing capacity of LLMs.
    Both of them complicate the process of handling architecture specifications using
    LLMs.


    Based on the basic formulation, we investigate the application of LLMs in both
    generation and reviewing architecture specifications. Regarding specification
    generation, we suggest two potential approaches. The first is to simplify the
    process of writing architecture specifications for designers, making it more efficient
    and less error-prone. The second approach is applicable when architecture specifications
    are absent for an already implemented chip. In such scenario, we can transform
    the RTL code back into architecture specifications, essentially reconstructing
    the original design documentation from the implemented code. According to our
    experiment, when generating specifications for simple logic circuits, the majority
    of the human tasks could be done by LLMs. We are optimistic that, even for more
    complex logic circuits, it is promising for LLMs to progressively take over a
    large portion of the work.


    We also demonstrate that the LLMs can efficiently review the architecture specifications.
    We first propose our definition of various types of defects in architecture specification
    document. Building on this, we utilized these defects as the target responses
    and developed specific prompts, seeking for LLM''s review. As for the public architecture
    specifications, our experiments have demonstrated that the LLMs could provide
    valuable feedback for enhancing these documents. Moving forward, we plan to extend
    our research to include the evaluation of the review results generated by the
    LLMs, streamlining the review process and enhancing the reliability of the outcomes.


    In summary, we formulate the task of adopting LLMs in the development of architecture
    specifications. We provide fundamental definitions and explore applications of
    LLMs in the realm of architecture specification development, including the basic
    definitions of architecture specifications and various tasks that LLMs can perform
    in this context. Our key contributions are summarized below:


    - We provide structured definitions of architecture specifications, facilitating
    efficient utilization of LLMs in developing architecture specifications. (Section
    [II\)](#page-1-0)

    - We generate a dataset of design specifications by methodically collecting and
    systematically organizing architecture specification documents from a variety
    of online sources. This paves the way for exploring LLMs in the development of
    architecture specifications. (Section [III\)](#page-3-0)

    - We explore the use of LLMs as tools for generating architecture specifications,
    including assisting designers in writing these specifications and converting RTL
    code into comprehensive specifications. Our findings suggest that LLMs hold considerable
    promise in efficiently generating architecture specifications. (Section [IV\)](#page-3-1)

    - We explore the use of LLMs in the review of architecture specifications. We
    identify various potential defects that may arise in these specifications. Based
    on these identified defects, we have crafted specific processes and prompts to
    guide the LLMs in their review. Our experimental results indicate that LLMs are
    capable of providing valuable feedback for improving these documents in aspects
    of accuracy. (Section [V\)](#page-5-0)


    # <span id="page-1-0"></span>II. OUR DEFINITION ON ARCHITECTURE SPECIFICATION


    We define the architecture specification as the document that describes the chip
    architecture prior to RTL coding. Writing architecture specifications for the
    target chip is usually the starting point of the IC design flow. The term architecture
    specification is specifically chosen to emphasize that the document captures the
    architectural aspects of a chip, distinguishing it from general specifications.
    We categorize the architecture specifications into three levels.


    - The Highest-level Architecture Specification (HAS) establishes standards applicable
    to a range of products. A notable example is the RISC-V specifications [\[61\]](#page-8-13).
    It defines the ISA specifications (e.g., instruction formats, register usages)
    and Non-ISA specifications (e.g., trace). Designing specific RISC-V chips should
    comply with these specifications.

    - The Middle-level Architecture Specification (MAS) outlines the high-level architecture
    of a single product. These specifications encompass the essential information
    required to profile the chip design. For example, one MAS for a RISC-V CPU may
    include an overview of the microarchitectures (e.g., block diagram) and their
    primary parameters (e.g., cache size).

    - The Lowest-level Architecture Specification (LAS) details the microarchitecture
    design of a single product. Unlike HAS and MAS, LAS should give the implementation
    details for each microarchitecture, which may involve the ports, internal signals,
    pipelines, and the associated descriptions. By reading them, the designers are
    expected to write the corresponding RTL code correctly.


    The architecture specifications are distinct from the user manuals. We define
    the user manuals as the documents that succinctly outline the chip design post-production.
    In contrast to architecture specifications, which primarily cater to chip designers,
    user manuals are typically crafted for the end users and programmers. Nonetheless,
    we note that certain contents in the user manuals exhibit similarities to the
    HAS or MAS. For example, as described in Section [III,](#page-3-0) we identify
    the *ARMv8-M Architecture Reference Manual* [\[17\]](#page-8-14), *Intel 64 and
    IA-32 Architectures Software Developer''s Manual* [\[18\]](#page-8-15) as HAS;
    and a series of *Arm Core Technical Reference Manual*s [\[28,](#page-8-16) [27,](#page-8-17)
    [26\]](#page-8-18), *Intel 64 and IA-32 Architectures Optimization Reference*
    [\[29\]](#page-8-19) as MAS.


    <span id="page-2-0"></span>


    | Type             | Highest-level Architecture<br>Specifications (HAS)                                                                                                                                                                                                                                                                                                                    |
    Middle-level Architecture<br>Specifications (MAS)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |
    Lowest-level Architecture<br>Specifications (LAS)                                                                                                                                                                                                                                                                                                                                                                                                  |

    |------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|

    | CPU              | • RISC-V ISA Specifications, Unpriv<br>ileged Specification
    [15]<br>• RISC-V ISA Specifications, Privi<br>leged Specification [16]<br>• ARMv8-M
    Architecture Reference<br>Manual [17]<br>• Intel 64 and IA-32 Architectures<br>Software
    Developer''s Manual [18]<br>• The SPARC Architecture Manual,<br>Version 9 [19]<br>•
    OpenRISC 1000 Architecture Man<br>ual [20] | • *The NEORV32 RISC-V Processor:<br>Datasheet
    [21]<br>• *OpenSPARC T1 Microarchitecture<br>Specification [22]<br>• *OpenSPARC
    T2 Core Microarchi<br>tecture Specification [23]<br>• E31 Core Complex Manual
    [24]<br>• E51 Core Complex Manual [25]<br>• Arm<br>Cortex-A78<br>Core<br>Technical<br>Reference
    Manual [26]<br>• Arm Cortex-X2 Core Technical Ref<br>erence Manual [27]<br>• Arm
    Neoverse-N2 Core Technical<br>Reference Manual [28]<br>• Intel 64 and IA-32 Architectures<br>Optimization
    Reference [29]<br>• OpenRISC 1200 IP Core Specifica<br>tion [30] | • The<br>NEORV32<br>RISC-V<br>Processor:<br>Datasheet
    [21]<br>• OpenSPARC<br>T1<br>Microarchitecture<br>Specification [22]<br>• OpenSPARC
    T2 Core Microarchitecture<br>Specification [23]<br>• Amber 2 Core Specification
    [31]<br>• LXP32, a lightweight open source 32-bit<br>CPU core, Technical Reference
    Manual<br>[32]<br>• OpenMSP430, Texas Instruments [33]<br>• NEO430,<br>based<br>on<br>the<br>Texas<br>Instruments
    MSP430(TM) ISA [34] |

    | SoC              | • Efficient Trace for RISC-V [35]<br>• RISC-V<br>External<br>Debug<br>Support<br>[36]<br>•
    RISC-V<br>IOMMU<br>Architecture<br>Specification [37]<br>• RISC-V<br>Advanced<br>Interrupt<br>Architecture
    [38]<br>• RISC-V<br>Platform-Level<br>Interrupt<br>Controller Specification [39]                                                                                |
    • Freedom E310-G000 Manual [40]<br>• Freedom U540-C000 Manual [41]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |
    • #The<br>NEORV32<br>RISC-V<br>Processor:<br>Datasheet [21]<br>• OpenSPARC T2
    System-On-Chip (SoC)<br>Microarchitecture Specification [42]                                                                                                                                                                                                                                                                                                         |

    | Accelerator      | • RISC-V "V" Vector Extension [43]<br>• Intel Advanced Performance
    Exten<br>sions (Intel APX) Architecture Speci<br>fication [44]<br>• Intel Advanced
    Vector Extensions<br>10 (Intel AVX10) Architecture Speci<br>fication [45]                                                                                                                                       |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
    • NVIDIA<br>Deep<br>Learning<br>Accelerator<br>(NVDLA), Hardware Architectural
    Spec<br>ification [46]                                                                                                                                                                                                                                                                                                                                              |

    | Bus &<br>Network | • TileLink Specification [47]<br>• AMBA5 CHI Architecture
    Specifi<br>cation [48]<br>• AMBA5 ACE Protocol Specification<br>(superseded by
    CHI) [49]<br>• AMBA5 AXI Protocol Specification<br>[50]<br>• AMBA4 AXI and ACE
    Protocol<br>Specification [51]                                                                                                                |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
    • 10GE MAC Core Specification [52]<br>• Ethernet IP Core Specification [53]<br>•
    I2C-Master Core Specification [54]<br>• UART to Bus Core Specifications [55]                                                                                                                                                                                                                                                                                      |

    | Arithmetic       |                                                                                                                                                                                                                                                                                                                                                                       |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
    • Elliptic Curve Group Core Specification<br>[56]<br>• Tate Bilinear Pairing Core
    Specification<br>[57]<br>• Tiny Tate Bilinear Pairing Core Specifi<br>cation [58]                                                                                                                                                                                                                                                                                |

    | Crypto           |                                                                                                                                                                                                                                                                                                                                                                       |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
    • AES Core Specification [59]<br>• SHA3 Core Specification [60]                                                                                                                                                                                                                                                                                                                                                                                    |


    | TABLE I: Our proposed dataset for architecture specifications, including approximately
    46 specification documents. |  |  |  |

    |--------------------------------------------------------------------------------------------------------------------|--|--|--|

    |                                                                                                                    |  |  |  |


    <sup>⋆</sup>Notes: \*NEORV32 [\[21\]](#page-8-24), OpenSPARC T1 [\[22\]](#page-8-25),
    OpenSPARC T2 [\[23\]](#page-8-26) include chapters meeting the criterion of MAS.
    #NEORV32 [\[21\]](#page-8-24) encompasses chapters describing the SoC implementation.


    # III. DATASET OVERVIEW


    <span id="page-3-0"></span>Table [I](#page-2-0) provides an overview of our proposed
    dataset for architecture specifications. It shows a variety of public architecture
    specifications and user manuals, organized by product types like CPU, SoC, Accelerator,
    Bus and Network, Arithmetic and Crypto, and by levels such as HAS, MAS, and LAS.
    It is important to note that in Table [I,](#page-2-0) we only display a representative
    selection of about 50 architecture specifications. For documents exhibiting a
    high degree of similarity, like additional *Arm Core Technical Reference Manuals*
    and *AMBA Protocol Specifications*, we have omitted them for simplicity. Our collection
    has yielded several notable findings.


    Availability. For the HAS, MAS, or LAS, the architecture specifications related
    to RISC-V are among the most accessible and widely available. Especially for the
    HAS, there are various established standards that exist, catering to different
    kinds of products, such as the CPU [\[16,](#page-8-21) [15\]](#page-8-20), SoC
    [\[35,](#page-8-34) [37,](#page-8-36) [38,](#page-8-37) [39\]](#page-8-38), Accelerator
    [\[43\]](#page-8-42) and Cache Coherence Bus [\[47\]](#page-8-46). An interesting
    observation is that despite the vast number of open-source designs in the RISC-V
    ecosystem, there is a scarcity of available MAS and LAS documents. Many documents,
    such as the Boom Core [\[62\]](#page-8-60) and XiangShan Core [\[63\]](#page-8-61),
    which describe the design of open-source chips, do not fulfill the standards required
    for formal architecture specifications. Therefore, we have chosen not to include
    these in our table. This gap highlights a potential area for improvement in the
    documentation and standardization of open-source chip designs. For the Arm and
    X86 ecosystem, the availability of formal architecture specifications is weaker.
    We can only find some reference manuals intended for programmers or end-users.
    Despite this limitation, upon investigation, it is observed that the contents
    of these manuals partially align with what one would expect in formal architecture
    specifications. Consequently, we have chosen to include these reference manuals
    for conducting research.


    Standard. While writing architecture specifications is a common practice in commercial
    chip manufacturing, we observe the available open specification documents exhibit
    a variety of writing styles and lack unified writing standards. Within individual
    semiconductor companies, there''s a tendency to maintain a consistent format for
    architecture specifications. Yet, the absence of unified writing standards across
    the industry leads to a diversity in the presentation and structuring of these
    documents, especially evident in open-source chip design [\[21,](#page-8-24) [31,](#page-8-30)
    [32,](#page-8-31) [33,](#page-8-32) [34\]](#page-8-33). This variation in writing
    styles and lack of standardized formats in architecture specifications pose additional
    challenges for LLMs in handling and accurately interpreting these documents.


    Length. The length of HAS spans from just over 100 pages to more than 1000 pages.
    The lengths of MAS and LAS can vary widely, with their extent directly correlating
    to the complexity of the chip designs they describe. MASs outline the chip designs,
    and thus are usually shorter than the LASs, ranging from several pages to in excess
    of 100 pages. LASs extend from over 10 pages to hundreds of pages. The longer
    the architecture specifications are, the more contextual information the LLMs
    tend to process. However, current commercial LLM products have a limitation on
    the number of tokens they can handle, which is not infinite. In Section [V,](#page-5-0)
    as we will discuss, the increase in contextual information due to longer architecture
    specifications poses challenges. It can lead to a decrease in the accuracy of
    the LLM''s output. Therefore, the length of the architecture specifications must
    be carefully considered when employing an LLM to manage these documents.


    In the following sections, we will choose portions of them to demonstrate LLM''s
    capabilities in developing architecture specifications. Constrained by the scarcity
    of existing architecture specifications, we consider certain sections of user
    manuals to function as architecture specifications.


    # <span id="page-3-1"></span>IV. LLM GENERATES ARCHITECTURE SPECIFICATION


    # <span id="page-3-2"></span>*A. Motivation*


    Traditionally, writing architecture specifications is a nontrivial, but critical
    step in the process of IC design. Our research indicates that chip architects
    spend a considerable amount of time writing architecture specifications for several
    reasons:


    - Massive Information Organization: Whether for HAS, MAS, or LAS, a significant
    amount of information must be organized. Systematically arranging them is crucial
    for the ensuing RTL coding process. While criteria for writing architecture specifications
    exist in separate companies, the complexity of this task escalates with the increasing
    scale of the designs.

    - Diverse Modules Interconnection: MAS is responsible for establishing the interconnection
    between various hardware modules. This plays a critical role in ensuring the overall
    correctness of the complete product. Despite this, the process of integrating
    these modules often encounters challenges, such as the time-consuming tasks of
    discerning different interfaces and identifying the functions of each module.

    - Complicate Algorithm Implementation: LAS delineates the structures and behaviors
    for each microarchitectural design. According to our observation, LAS is closest
    to the final RTL design among the three types of architecture specifications.
    When dealing with complex microarchitectural algorithms, such as branch prediction,
    writing LAS presents significant challenges, which include accurately defining
    signals and state machines.


    LLMs are promising in solving the above challenges and assisting designers in
    writing architecture specifications. According to Section [II,](#page-1-0) HASs
    set the standards for a variety of products and are inherently determined by human
    decisions. Consequently, they are highly flexible and challenging to generate.
    We thus mainly focus on using LLMs to automatically generate the MAS and LAS.
    We explore two LLM-based applications:


    - 1) LLMs can reduce human effort by writing architecture specifications (Section
    [IV-B\)](#page-4-0).

    - 2) Many open-source chip designs lack comprehensive and formal architecture
    specifications. In situations where these specifications are absent, LLMs could
    reversely convert the RTL code into detailed specification documents. This reverse
    engineering process can bridge the


    ## <span id="page-4-3"></span>**Prompt:**


    From now on, you act as a chip architect writing the architecture specification.
    This specification must encompass detailed descriptions of microarchitectures,
    including aspects such as ports, internal signals, state machines. Subsequently,
    I will assign you a specific module to focus on in your writing.


    Fig. 2: An example prompt to initiate generating LAS. This generation is based
    on the designers'' brief description.


    ## <span id="page-4-4"></span>**Prompt:**


    Please write an architecture specification regarding a perpetual calendar. The
    starting value of Secs, Mins, and Hours are all 0. Both Secs and Mins loop continuously
    from 0 to 59. When Secs=59, Min increases by 1 at the next cycle, and when Min=59
    && Secs=59, Hours increases by 1 at the next cycle. Hours is counted from the
    0-23 cycle.


    Fig. 3: An example prompt to request GPT generating an architecture specification
    for perpetual calendar. The response is shown in Figure [4.](#page-4-1)


    gap in the documentation and enhance understanding and accessibility of the chip
    designs (Section [IV-C\)](#page-4-2).


    In the upcoming sections, we will leverage the widely-used commercial LLM product—GPT-4
    [\[1\]](#page-7-0), to demonstrate how this technology can be effectively exploited
    to enhance the process of writing architecture specifications.


    # <span id="page-4-0"></span>*B. Assist Designers to Write Architecture Specifications*


    Figure [2](#page-4-3) presents an example of a prompt to initiate the generation
    of LAS. This example, referencing [\[64\]](#page-8-62), indicates the specific
    role expected to be performed by the LLMs, and the expected response format. Then
    we employ the prompt in Figure [3](#page-4-4) to direct GPT generating the architecture
    specifications. This particular example, sourced from [\[4\]](#page-8-2), has
    been selected due to its moderate level of complexity, making it suitable for
    testing purposes. Figure [4](#page-4-1) presents the outcomes of GPT-4. The experimental
    results demonstrate that the GPT can accurately understand the questions. These
    specifications, spanning from the high-level functional description to the lowlevel
    state machines, offer in-depth information for subsequent RTL coding. Yet, the
    design of a perpetual calendar is a classical task in the field of circuit design.
    The LLMs have abundant corpus about it. The commercial products, usually include
    internal IPs, which are not used to train the external LLMs like ChatGPT. We believe
    it is non-trivial to generate that kind of architecture specifications. However,
    our experiments at least demonstrate that the LLMs possess the potential to conduct
    the task of generating architecture specifications. The speed of the LLMs in performing
    this task, compared to the experienced chip architects, still has significant
    advantages.


    # <span id="page-4-2"></span>*C. Convert RTL code to Architecture Specifications*


    Figure [5](#page-5-1) showcases the prompt employed for converting the RTL code
    into LAS. Following this prompt, we provide the RTL code to the LLM, which can
    be in the form of textual format or as a source code file. In our example, we
    adopt the Verilog file that is open-sourced by RTLLM [\[65\]](#page-8-63) and
    it is uploaded to our GitHub repo. The response of GPT is shown in Figure [6.](#page-5-2)
    In summary, the generated architecture


    ## <span id="page-4-1"></span>**Response:**


    ## **Architecture Specification for a Perpetual Calendar Module**


    ## **Overview**


    This document outlines the architectural specification for a Perpetual Calendar
    Module (PCM). The PCM is responsible for tracking time in seconds, minutes, and
    hours, with a continuous looping mechanism. The time is represented in a 24-hour
    format. The module initializes with all time values (seconds, minutes, hours)
    at zero and increments according to predefined rules.


    ## **Functional Description**


    #### **Time Units**


    **Seconds (Secs):** This unit ranges from 0 to 59. It resets to 0 after reaching
    59.


    **Minutes (Mins):** This unit also ranges from 0 to 59. It increments by 1 when
    Secs roll over from 59 to 0.


    **Hours:** This unit ranges from 0 to 23. It increments by 1 when both Mins and
    Secs roll over from 59 to 0.


    **Initial State**


    All units (Secs, Mins, Hours) are initialized to 0 at the start.


    **Increment Logic**


    **Secs Increment:** Occurs every cycle. Rolls over to 0 after reaching 59. **Mins
    Increment:** Occurs when Secs roll over from 59 to 0. **Hours Increment:** Occurs
    when both Mins and Secs simultaneously roll over from 59 to 0.


    ## **Microarchitecture**


    #### **Components**


    **Counter Units:** Three independent counters for Secs, Mins, and Hours. **Control
    Logic:** Determines the rollover conditions and triggers the increment of Mins
    and Hours.


    **Clock Signal:** Drives the incrementation of Secs.


    # **Counter Behavior**


    Each counter is a modulo-60 counter for Secs and Mins, and a modulo-24 counter
    for Hours.


    The counters are synchronized with the clock signal.


    **Control Logic**


    Monitors the state of Secs and Mins.


    When Secs reach 59, it sends a pulse to increment Mins at the next cycle. When
    both Secs and Mins are 59, it sends a pulse to increment Hours at the next cycle.


    # **Interface**


    **Input:** Clock signal.


    **Output:** Current time in hours, minutes, and seconds.


    ## **State Machines**


    Seconds Counter (Secs) **State 0-58:** Increment Secs by 1 each clock cycle. **State
    59:** Reset Secs to 0 on the next clock cycle and trigger a Min increment signal.
    Minutes Counter (Mins) **State 0-58 (No Increment Signal):** Hold current value.
    **State 59 (No Increment Signal):** Hold current value.


    **State 0-58 (With Increment Signal):** Increment Mins by 1 on the next clock
    cycle and reset signal.


    **State 59 (With Increment Signal):** Reset Mins to 0 on the next clock cycle,
    reset signal, and trigger an Hour increment signal.


    Hours Counter


    **State 0-22 (No Increment Signal):** Hold current value.


    **State 23 (No Increment Signal):** Hold current value.


    **State 0-22 (With Increment Signal):** Increment Hours by 1 on the next clock
    cycle and reset signal.


    **State 23 (With Increment Signal):** Reset Hours to 0 on the next clock cycle
    and reset signal.


    Fig. 4: A GPT response example to the prompt in Figure [3.](#page-4-4)


    ## <span id="page-5-1"></span>**Prompt:**


    From now on, you act as a chip architect writing the architecture specification.
    This specification must encompass detailed descriptions of microarchitectures,
    including aspects such as ports, internal signals, state machines. Subsequently,
    I will provide you with the RTL code. Please read the RTL code and generate the
    corresponding architecture specification.


    Fig. 5: An example prompt to initiate writing architecture specifications on the
    basis of RTL code. The response is shown in Figure [6.](#page-5-2)


    specifications are slightly shorter than those depicted in Figure [4,](#page-4-1)
    yet they effectively preserve the correct meaning. We conduct experiments to analyze
    this phenomenon. Our findings indicate that the provided RTL code confines the
    scope for the specification generation. Without the RTL code, the LLM may generate
    the architecture specifications on the basis of the previously trained corpus,
    which exhibits diverse explanations for the single circuit design. On the contrary,
    after we offer the LLM with a unique RTL code, its responses are likely to be
    more orientated.


    # <span id="page-5-0"></span>V. LLM REVIEWS ARCHITECTURE SPECIFICATION


    # *A. Motivation*


    The accuracy and precision of architecture specifications directly affect the
    overall quality of chip designs. Ensuring these specifications are correct is
    crucial for the successful development and functionality of the chips. Traditionally,
    the chip company should spend efforts to review architecture specifications. Analogous
    to the reasons mentioned in Section [IV-A,](#page-3-2) reviewing architecture
    specifications also requires significant human efforts. LLMs can also help reviewers
    in these tasks. At least, the reviewers could use these tools to get an overview
    of the specification files and obtain some comments from them. Fortunately, as
    we will demonstrate in Section [V-B](#page-5-3) and Section [V-C,](#page-6-0)
    the LLMs show promise in achieving more profound objectives in the review of architectural
    specifications, going beyond basic overviews to provide detailed and meaningful
    insights.


    # <span id="page-5-3"></span>*B. Defects Category*


    <span id="page-5-4"></span>


    | Type              | Potential Defects                                                                                                                                                                                 |  |  |

    |-------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--|--|

    | Common<br>Case    | • Typographical Error<br>• Inconsistence or Contradiction
    Error<br>• Incomplete or Unclear Error                                                                                                  |  |  |

    | Level<br>Specific | • Combinational Loops Error (LAS)<br>• Uninitialized Register
    Value Error (LAS)<br>• Improvement<br>for<br>Micro-architectural<br>Design<br>(LAS)<br>•
    Improvement for Architectural Design (MAS) |  |  |

    | Level<br>Spanning | • Inconsistence or Contradiction Error<br>(Across Various
    Levels)                                                                                                                                 |  |  |


    TABLE II: Potential Defects Occurred in Different Levels Architecture Specifications


    Initially, we have pinpointed various types of potential defects that may occur
    in architecture specifications. Table [II](#page-5-4)


    #### <span id="page-5-2"></span>**Response:**


    I have received the RTL (Register Transfer Level) code file named "verified\_calendar.v".
    I''ll proceed to read and analyze this file to generate a corresponding architecture
    specification. Let''s start by examining the contents of the file.


    The RTL code in "**verified\_calendar.v**" defines a module named verified\_calendar.
    This module appears to be designed as a simple digital clock. Here''s a brief
    overview of the module and its key components based on the initial part of the
    code:


    # **Module: verified\_calendar**


    ## **Inputs:**


    - **CLK**: Clock input.

    - **RST**: Reset input.

    - **Outputs:**

    - **Hours**: 6-bit output representing hours.

    - **Mins:** 6-bit output representing minutes.

    - **Secs:** 6-bit output representing seconds.


    ## **Internal Registers:**


    • **Hours, Mins, Secs**: 6-bit registers for storing hours, minutes, and seconds.


    ## **Behavioral Description:**


    The module contains three **always** blocks, each triggered on the positive edge
    of **CLK** or **RST**.


    - **Seconds Logic:**

    - On reset (**RST**), seconds (**Secs**) are set to 0.

    - If seconds equal 59, they roll over to 0; otherwise, they increment every clock
    cycle.


    ## **Minutes Logic:**


    - On reset, minutes (**Mins**) are set to 0.

    - Minutes increment when seconds roll over from 59 to 0. If minutes are at 59
    when seconds roll over, they reset to 0.


    **Hours Logic:**


    • Not fully visible in the provided snippet, but likely similar to minutes, incrementing
    as minutes roll over.


    ## **State Machine for Seconds:**


    ## States: **Reset**, **Count**


    Transitions:


    - **Reset** to **Count**: When **RST** de-asserted.

    - **Count** to **Reset**: When **RST** asserted or **Secs** reach 59.

    - Output: Increments **Secs** or resets to 0 based on the state.


    # **State Machine for Minutes:**


    States: **Reset**, **Count**


    Transitions:


    - **Reset** to **Count**: When **RST** de-asserted.

    - **Count** to **Reset**: When **Mins** reach 59 and **Secs** are 59.


    Output: Increments **Mins** or resets to 0 based on the state and **Secs**.


    Fig. 6: A GPT response example to the prompt in Figure [5.](#page-5-1)


    summarizes these defects, categorizing them according to the level of architecture
    specifications.


    In many instances, there are defects that could manifest in all three tiers of
    architecture specifications: HAS, MAS, and LAS. They include the *Typographical
    Error*, *Inconsistence or Contradiction Error*, and *Incomplete or Unclear Error*.
    The *Inconsistence or Contradiction Error* denotes situations within a single
    specification file where either two concepts describing the same object are inconsistent,
    or two related concepts are contradictory. The *Incomplete or Unclear Error* refers
    to instances where certain concepts lack essential information, resulting in sentences
    that are open to ambiguous interpretations.


    Furthermore, the potential defects could also be levelspecific, indicating that
    HAS, MAS, or LAS may each have distinct defects. For example, LAS might include
    details about port connections between various modules. LLMs can be employed to
    scrutinize the LAS for identifying potential *Combinational Loops Error*. Such
    scrutiny, however, may not be applicable to HAS and MAS. In addition to locating
    the writing issues, we also identify some high-level objectives for the review
    of architecture specification, such as the *Improvement for Architectural Design*
    and *Improvement for Micro-architectural Design*. These objectives are aimed at
    leveraging the potential of the review process to enhance both the architectural
    and micro-architectural design aspects. This goes beyond mere error correction,
    focusing on the overall optimization and refinement of the design.


    Additionally, these defects might span across different levels. One example of
    this is the *Inconsistence or Contradiction Error (Across Various Levels)*, that
    the lower-level architecture specifications should not contradict the contents
    established in the higher-level specifications. Note that it is not equivalent
    to the *Inconsistence or Contradiction Error* established for the common case.
    The former indicates the inconsistence or contradiction between different levels
    of architecture specification files, while the latter confines the error in one
    individual specification file.


    # <span id="page-6-0"></span>*C. Architecture Specification Review*


    We exploit the potential defects in architecture specifications as the types of
    feedback (Section [V-B\)](#page-5-3) we aim to receive from the review conducted
    by the LLMs. We have developed two methodologies to guide the LLMs in reviewing
    architecture specifications. The first method involves directly uploading the
    entire specification file and requesting the LLMs to provide feedback section
    by section. The second method entails splitting the specification file into sections,
    uploading each section individually, and then asking the LLMs to conduct a review
    immediately upon the upload of each section. Regarding the first approach, it
    has the advantage of not requiring extra steps to divide the specification file.
    However, we''ve observed that even when instructed to give feedback for each section,
    the LLMs struggle to accurately recognize the boundaries between different sections.
    This often results in the provision of incorrect or misleading indication messages
    by the LLMs. The second approach, which involves splitting the specification file
    into sections and uploading each one separately for review, can effectively alleviate
    this problem by providing clearer boundaries for the LLMs to analyze and respond
    to each section individually. In the subsequent sections, we will use part of
    the architecture specifications, as shown in Table [I,](#page-2-0) to demonstrate
    LLM''s strengths and weaknesses in reviewing architecture specifications.


    *1) Reviewing One File at a Time:* We employ the Amber 2 Core Specification [\[31\]](#page-8-30)
    to conduct the experiments of reviewing architecture specifications. The tested
    file used in our example is from [\[66\]](#page-8-64) and it has been uploaded
    to our


    ## <span id="page-6-2"></span>**Prompt:**


    From now on, you act as a chip architect reviewing the architecture specification.
    For the following document, please find any typo, inconsistence/contradiction,
    incomplete/unclear and propose the corresponding improvement suggestions. The
    responding format should separately list the findings about typo, inconsistence/contradiction,
    incomplete/unclear and the corresponding improvement suggestions.


    Fig. 7: An example prompt to initiate the review of architecture specifications.
    The response is shown in Figure [8.](#page-6-1)


    ## <span id="page-6-1"></span>**Response:**


    I have thoroughly reviewed the Amber Core Specification document. Here are the
    summarized findings:


    1. **Typos and Grammatical Errors:** The document contains some typographical
    and grammatical errors which can be corrected for clarity and professionalism.


    2. **Inconsistencies/Contradictions:** Certain technical specifications and descriptions
    appear to be inconsistent or contradictory, which could lead to confusion in understanding
    the architecture and functionality of the Amber Core.


    3. **Incomplete/Unclear Sections:** Some sections of the document are either incomplete
    or not clearly explained, which may hinder the complete understanding of the core''s
    architecture and operations.


    For a detailed report including specific instances of these issues and improvement
    suggestions, please let me know, and I will provide a comprehensive breakdown.


    Fig. 8: A GPT response example to the prompt in Figure [7.](#page-6-2)


    ## <span id="page-6-3"></span>**Prompt:**


    Good, let''s review Section 1, and please provide specific and comprehensive review
    results. The response format should comply with the previous requirements.


    Fig. 9: An example prompt to review individual sections of the specification document.
    The response is shown in Figure [10.](#page-7-1)


    GitHub repo. Its concise contents enable current LLMs like GPT-4 to review it
    at a time. According to Table [II,](#page-5-4) we develop a prompt to request
    GPT-4 providing feedback on common defects. We use the prompt, as illustrated
    in Figure [7,](#page-6-2) to initiate the process of writing architecture specifications.
    Next, we upload the specification file selected for testing and ask GPT to summarize
    its findings. As illustrated in Figure [8,](#page-6-1) the LLM will then provide
    an overview of its analysis. This information can offer designers a quick glimpse
    into the results of the review. Subsequently, as depicted in Figure [9,](#page-6-3)
    we request GPT to provide us with detailed review results for each individual
    section of the specification document. Figure [10](#page-7-1) displays the in-depth
    review results specifically for Section 1 of the Amber Core Specification. In
    general, we have observed that GPT-4 is adept at correctly identifying typographical
    errors. Furthermore, in more complex cases such as inconsistence or contradiction
    errors, as well as incomplete or unclear errors, GPT-4 is also capable of offering
    constructive advice. It can give the reasons why these types of errors exist and
    suggest ways to improve them. Nonetheless, our research has revealed that GPT-4
    faces challenges in effectively dividing an archi-


    ## <span id="page-7-1"></span>**Response:**


    ## **Section 1 Review of the Amber Core Specification**


    Typos:


    1. "seperate" (in the description of Amber 25) should be "separate."


    2. "Mhz" (in the capability description of Amber 25) should be "MHz."


    3. "that" (in the performance comparison of Amber 25 and Amber 23) should be "than."


    4. "has" (in the paragraph about Linux kernel verification) should be "have."


    5. "pipline" (in the last sentence) should be "pipeline."


    #### Inconsistencies/Contradictions:


    1. The document states that the Amber 23 and Amber 25 cores are 100% software
    compatible, yet they have differing pipeline stages and cache architectures. This
    could potentially lead to different performance characteristics not addressed
    in the compatibility claim.


    ## Incomplete/Unclear Sections:


    1. The description of the Wishbone interface is vague. More detailed specifications
    or references to the interface''s documentation would improve clarity.


    2. The performance metrics (DMIPS per MHz) are provided, but there''s no baseline
    comparison or explanation of what these figures imply in practical terms.


    3. The document mentions that the cores have been verified with a 2.4 Linux kernel
    but does not detail the extent or scope of this verification.


    4. The information about the FPGA synthesis is incomplete. Details about the synthesis
    process, along with any challenges or optimizations, would provide a clearer picture
    of the implementation process.


    ## Improvement Suggestions:


    1. **Typos**: Correct all typographical errors for accuracy and professionalism.


    2. **Inconsistencies/Contradictions:** Clarify the software compatibility claim
    considering the different architectures of Amber 23 and Amber 25. Include any
    limitations or specific scenarios where this compatibility might not hold.


    ## 3. **Incomplete/Unclear Sections:**


    - Expand the description of the Wishbone interface, possibly including a subsection
    dedicated to its architecture and how it interfaces with the Amber cores.

    - Provide a brief explanation or a reference to what DMIPS per MHz represents,
    helping readers unfamiliar with this metric understand the core''s performance.

    - Detail the scope of the Linux kernel verification. Specify what tests were performed,
    any limitations encountered, and how these tests validate the core''s functionality.

    - Elaborate on the FPGA synthesis process, including challenges faced, optimizations
    made, and why specific FPGA boards were chosen for testing.


    Fig. 10: A GPT response example to the prompt in Figure [9.](#page-6-3)


    tecture specification file into distinct sections. This limitation often results
    in the feedback provided for one section possibly originating from other sections
    of the document. To mitigate this problem, we propose to split the complete specification
    file into individual sections, and request GPT to review them sequentially.


    *2) Splitting and Reviewing File Section-by-Section:* Similar to the first method,
    we begin by using prompts, as shown in Figure [7,](#page-6-2) to initiate the
    review of architecture specifications. Next, instead of uploading the entire architecture
    specification file for review, we submit only the file containing the contents
    of the individual section as an attachment. Simultaneously, we provide the prompt,
    as illustrated in Figure [11,](#page-7-2) directing


    #### <span id="page-7-2"></span>**Prompt:**


    Please review the section 1 first, and give me specific and comprehensive reviewing
    results. The responding format should comply with the previous requirements.


    **( Note: attach the divided file that represents Section 1 of the architecture
    specification file)**


    Fig. 11: An example prompt to review the split files. Though the response from
    GPT is similar to the Figure [10,](#page-7-1) the feedback is more likely to concentrate
    on the individual section.


    GPT to review each section. In this strategy, each review task for different sections
    is paired with its corresponding chapter file, ensuring that each section is reviewed
    in isolation and in context. The responses given by the GPT in this review process
    maintain a format similar to the outputs generated by the previous, more holistic
    review method. However, we have observed that the feedbacks are more likely to
    concentrate on each individual section. Furthermore, the GPT appears more effective
    in identifying a broader range of errors, which increases the number of available
    output items. The accurate and abundant review results enable designers to quickly
    but comprehensively locate the defects in their specification documents.


    # *D. Feedbacks Evaluation*


    Not all feedback provided by the LLM is correct or useful. Therefore, it is essential
    to have strategies in place to evaluate the review results of architecture specifications
    effectively. The most intuitive method is to ask the designers to check the outputs
    of the LLM. This approach, compared to directly reviewing the architecture specifications
    themselves, is already more efficient, potentially saving significant time and
    effort. Nonetheless, an alternative approach involves training a new language
    model specifically for evaluating and filtering the review results provided by
    the LLMs. This specialized model could be designed to assess the relevance and
    accuracy of the LLM''s feedback [\[67\]](#page-8-65), thereby streamlining the
    review process and enhancing the reliability of the outcomes. We leave this part
    for future exploration.


    # VI. CONCLUSION


    In this paper, we propose a novel framework for utilizing LLMs to generate and
    review architecture specifications. We defined architecture specifications, categorized
    them into clear levels, and compiled a corresponding dataset. Our approach focuses
    on two primary applications of LLMs: firstly, in generating architecture specifications,
    including assistance in writing and converting RTL code into specifications, and
    secondly, in the review of these specifications. This innovative methodology signifies
    a transformative step in architecture specification development, offering a path
    toward more efficient, accurate, and streamlined processes in chip design.


    # REFERENCES


    <span id="page-7-0"></span>[1] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya,
    F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat *et al.*, "Gpt-4
    technical report," *arXiv preprint arXiv:2303.08774*, 2023.


    - <span id="page-8-0"></span>[2] S. Liu, W. Fang, Y. Lu, Q. Zhang, H. Zhang, and
    Z. Xie, "Rtlcoder: Outperforming gpt-3.5 in design rtl generation with our open-source
    dataset and lightweight solution," *arXiv preprint arXiv:2312.08617*, 2023.

    - <span id="page-8-1"></span>[3] J. Blocklove, S. Garg, R. Karri, and H. Pearce,
    "Chip-chat: Challenges and opportunities in conversational hardware design," *arXiv
    preprint arXiv:2305.13243*, 2023.

    - <span id="page-8-2"></span>[4] Y. Lu, S. Liu, Q. Zhang, and Z. Xie, "Rtllm:
    An open-source benchmark for design rtl generation with large language model,"
    *arXiv preprint arXiv:2308.05345*, 2023.

    - <span id="page-8-3"></span>[5] M. Liu, N. Pinckney, B. Khailany, and H. Ren,
    "Verilogeval: Evaluating large language models for verilog code generation," *arXiv
    preprint arXiv:2309.07544*, 2023.

    - <span id="page-8-4"></span>[6] S. Thakur, B. Ahmad, Z. Fan, H. Pearce, B. Tan,
    R. Karri, B. Dolan-Gavitt, and S. Garg, "Benchmarking large language models for
    automated verilog rtl code generation," in *DATE*, 2023.

    - <span id="page-8-5"></span>[7] S. Thakur, J. Blocklove, H. Pearce, B. Tan, S.
    Garg, and R. Karri, "Autochip: Automating hdl generation using llm feedback,"
    *arXiv preprint arXiv:2311.04887*, 2023.

    - <span id="page-8-6"></span>[8] M. Nair, R. Sadhukhan, and D. Mukhopadhyay, "Generating
    secure hardware using chatgpt resistant to cwes," *Cryptology ePrint Archive*,
    2023.

    - <span id="page-8-7"></span>[9] M. Liu, T.-D. Ene, R. Kirby, C. Cheng, N. Pinckney,
    R. Liang, J. Alben, H. Anand, S. Banerjee, I. Bayraktaroglu *et al.*, "Chipnemo:
    Domainadapted llms for chip design," *arXiv preprint arXiv:2311.00176*, 2023.

    - <span id="page-8-8"></span>[10] Z. He, H. Wu, X. Zhang, X. Yao, S. Zheng, H.
    Zheng, and B. Yu, "Chateda: A large language model powered autonomous agent for
    eda," in *MLCAD Workshop*, 2023.

    - <span id="page-8-9"></span>[11] Y. Fu, Y. Zhang, Z. Yu, S. Li, Z. Ye, C. Li,
    C. Wan, and Y. Lin, "Gpt4aigchip: Towards next-generation ai accelerator design
    automation via large language models," *arXiv preprint arXiv:2309.10730*, 2023.

    - <span id="page-8-10"></span>[12] Z. Yan, Y. Qin, X. S. Hu, and Y. Shi, "On the
    viability of using llms for sw/hw co-design: An example in designing cim dnn accelerators,"
    *arXiv preprint arXiv:2306.06923*, 2023.

    - <span id="page-8-11"></span>[13] R. Kande, H. Pearce, B. Tan, B. Dolan-Gavitt,
    S. Thakur, R. Karri, and J. Rajendran, "Llm-assisted generation of hardware assertions,"
    *arXiv preprint arXiv:2306.14027*, 2023.

    - <span id="page-8-12"></span>[14] B. Ahmad, S. Thakur, B. Tan, R. Karri, and
    H. Pearce, "Fixing hardware security bugs with large language models," *arXiv
    preprint arXiv:2302.01215*, 2023.

    - <span id="page-8-20"></span>[15] *RISC-V ISA Specifications Volume 1, Unprivileged
    Specification*, ["https:]("https://wiki.riscv.org/display/HOME/RISC-V+Technical+Specifications"
    ) [//wiki.riscv.org/display/HOME/RISC-V+Technical+Specifications".]("https://wiki.riscv.org/display/HOME/RISC-V+Technical+Specifications"
    )

    - <span id="page-8-21"></span>[16] *RISC-V ISA Specifications Volume 2, Privileged
    Specification*, ["https:]("https://wiki.riscv.org/display/HOME/RISC-V+Technical+Specifications"
    ) [//wiki.riscv.org/display/HOME/RISC-V+Technical+Specifications".]("https://wiki.riscv.org/display/HOME/RISC-V+Technical+Specifications"
    )

    - <span id="page-8-14"></span>[17] *ARMv8-M Architecture Reference Manual*, ["https://]("https://documentation-service.arm.com/static/5f8efe85f86e16515cdbe596?token="
    ) [documentation-service.arm.com/static/5f8efe85f86e16515cdbe596?]("https://documentation-service.arm.com/static/5f8efe85f86e16515cdbe596?token="
    ) [token=".]("https://documentation-service.arm.com/static/5f8efe85f86e16515cdbe596?token="
    )

    - <span id="page-8-15"></span>[18] *Intel 64 and IA-32 Architectures Software
    Developer''s Manua*, ["https:]("https://cdrdv2.intel.com/v1/dl/getContent/671200"
    ) [//cdrdv2.intel.com/v1/dl/getContent/671200".]("https://cdrdv2.intel.com/v1/dl/getContent/671200"
    )

    - <span id="page-8-22"></span>[19] S. I. Inc and D. L. Weaver, *The SPARC architecture
    manual*, 1994.

    - <span id="page-8-23"></span>[20] *OpenRISC 1000 Architecture Manua*, ["https://openrisc.io/or1k.html".]("https://openrisc.io/or1k.html"
    )

    - <span id="page-8-24"></span>[21] *The NEORV32 RISC-V Processor: Datasheet*,
    ["https://stnolting.github.]("https://stnolting.github.io/neorv32/" ) [io/neorv32/".]("https://stnolting.github.io/neorv32/"
    )

    - <span id="page-8-25"></span>[22] *OpenSPARC T1 Microarchitecture Specification*,
    ["https://www.oracle.com/technetwork/systems/opensparc/]("https://www.oracle.com/technetwork/systems/opensparc/t1-01-opensparct1-micro-arch-1538959.html"
    ) [t1-01-opensparct1-micro-arch-1538959.html".]("https://www.oracle.com/technetwork/systems/opensparc/t1-01-opensparct1-micro-arch-1538959.html"
    )

    - <span id="page-8-26"></span>[23] *OpenSPARC T2 Microarchitecture Specification*,
    ["https://www.oracle.com/technetwork/systems/opensparc/]("https://www.oracle.com/technetwork/systems/opensparc/t2-06-opensparct2-core-microarch-1537749.html"
    ) [t2-06-opensparct2-core-microarch-1537749.html".]("https://www.oracle.com/technetwork/systems/opensparc/t2-06-opensparct2-core-microarch-1537749.html"
    )

    - <span id="page-8-27"></span>[24] *E31 Core Complex Manual*, ["https://d2pn104n81t9m2.cloudfront.net/]("https://d2pn104n81t9m2.cloudfront.net/documentation/")
    [documentation/".]("https://d2pn104n81t9m2.cloudfront.net/documentation/")

    - <span id="page-8-28"></span>[25] *E51 Core Complex Manual*, ["https://d2pn104n81t9m2.cloudfront.net/]("https://d2pn104n81t9m2.cloudfront.net/documentation/")
    [documentation/".]("https://d2pn104n81t9m2.cloudfront.net/documentation/")

    - <span id="page-8-18"></span>[26] *Arm Cortex-A78 Core Technical Reference Manual*,
    ["https://developer.]("https://developer.arm.com/documentation/101430/latest/"
    ) [arm.com/documentation/101430/latest/".]("https://developer.arm.com/documentation/101430/latest/"
    )

    - <span id="page-8-17"></span>[27] *Arm Cortex-X2 Core Technical Reference Manual*,
    ["https://developer.]("https://developer.arm.com/documentation/101803/latest/"
    ) [arm.com/documentation/101803/latest/".]("https://developer.arm.com/documentation/101803/latest/"
    )

    - <span id="page-8-16"></span>[28] *Arm Neoverse N2 Core Technical Reference Manual*,
    ["https://developer.]("https://developer.arm.com/documentation/102099/latest/The-Neoverse-N2--core"
    ) [arm.com/documentation/102099/latest/The-Neoverse-N2--core".]("https://developer.arm.com/documentation/102099/latest/The-Neoverse-N2--core"
    )

    - <span id="page-8-19"></span>[29] *Intel 64 and IA-32 Architectures Optimization
    Reference*, ["https://www.intel.com/content/www/us/en/content-details/671488/]("https://www.intel.com/content/www/us/en/content-details/671488/intel-64-and-ia-32-architectures-optimization-reference-manual.html"
    ) [intel-64-and-ia-32-architectures-optimization-reference-manual.html".]("https://www.intel.com/content/www/us/en/content-details/671488/intel-64-and-ia-32-architectures-optimization-reference-manual.html"
    )

    - <span id="page-8-29"></span>[30] *OpenRISC 1200 IP Core Specification*, ["https://github.com/openrisc/]("https://github.com/openrisc/or1200/blob/master/doc/openrisc1200_spec.pdf")
    [or1200/blob/master/doc/openrisc1200]("https://github.com/openrisc/or1200/blob/master/doc/openrisc1200_spec.pdf")
    spec.pdf".

    - <span id="page-8-30"></span>[31] *Amber ARM-compatible core*, ["https://opencores.org/projects/amber".]("https://opencores.org/projects/amber"
    )

    - <span id="page-8-31"></span>[32] *LXP32, a lightweight, open source and FPGA-friendly
    32-bit CPU IP core*, ["https://opencores.org/projects/lxp32".]("https://opencores.org/projects/lxp32"
    )

    - <span id="page-8-32"></span>[33] *OpenMSP430, Texas Instruments*, ["https://opencores.org/projects/]("https://opencores.org/projects/openmsp430"
    ) [openmsp430".]("https://opencores.org/projects/openmsp430" )

    - <span id="page-8-33"></span>[34] *NEO430, based on the Texas Instruments MSP430(TM)
    ISA*, ["https://]("https://opencores.org/projects/neo430" ) [opencores.org/projects/neo430".]("https://opencores.org/projects/neo430"
    )

    - <span id="page-8-34"></span>[35] *Efficient Trace for RISC-V*, ["https://wiki.riscv.org/display/HOME/]("https://wiki.riscv.org/display/HOME/RISC-V+Technical+Specifications"
    ) [RISC-V+Technical+Specifications".]("https://wiki.riscv.org/display/HOME/RISC-V+Technical+Specifications"
    )

    - <span id="page-8-35"></span>[36] *RISC-V External Debug Support*, ["https://wiki.riscv.org/display/HOME/]("https://wiki.riscv.org/display/HOME/RISC-V+Technical+Specifications"
    ) [RISC-V+Technical+Specifications".]("https://wiki.riscv.org/display/HOME/RISC-V+Technical+Specifications"
    )

    - <span id="page-8-36"></span>[37] *RISC-V IOMMU Architecture Specification*,
    ["https://wiki.riscv.org/]("https://wiki.riscv.org/display/HOME/RISC-V+Technical+Specifications"
    ) [display/HOME/RISC-V+Technical+Specifications".]("https://wiki.riscv.org/display/HOME/RISC-V+Technical+Specifications"
    )

    - <span id="page-8-37"></span>[38] *RISC-V Advanced Interrupt Architecture*, ["https://wiki.riscv.org/display/]("https://wiki.riscv.org/display/HOME/RISC-V+Technical+Specifications"
    ) [HOME/RISC-V+Technical+Specifications".]("https://wiki.riscv.org/display/HOME/RISC-V+Technical+Specifications"
    )

    - <span id="page-8-38"></span>[39] *RISC-V Platform-Level Interrupt Controller
    Specification*, ["https://wiki.]("https://wiki.riscv.org/display/HOME/RISC-V+Technical+Specifications"
    ) [riscv.org/display/HOME/RISC-V+Technical+Specifications".]("https://wiki.riscv.org/display/HOME/RISC-V+Technical+Specifications"
    )

    - <span id="page-8-39"></span>[40] *Freedom E310-G000 Manual*, ["https://www.sifive.com/documentation".]("https://www.sifive.com/documentation"
    )

    - <span id="page-8-40"></span>[41] *Freedom U540-C000 Manual*, ["https://www.sifive.com/documentation".]("https://www.sifive.com/documentation"
    )

    - <span id="page-8-41"></span>[42] *OpenSPARC T2 System-On-Chip (SoC) Microarchitecture
    Specification*, ["https://www.oracle.com/docs/tech/systems/]("https://www.oracle.com/docs/tech/systems/t2-07-opensparct2-socmicroarchvol1.pdf"
    ) [t2-07-opensparct2-socmicroarchvol1.pdf".]("https://www.oracle.com/docs/tech/systems/t2-07-opensparct2-socmicroarchvol1.pdf"
    )

    - <span id="page-8-42"></span>[43] *RISC-V "V" Vector Extension*, ["https://inst.eecs.berkeley.edu/]("https://inst.eecs.berkeley.edu/~cs152/sp20/handouts/sp20/riscv-v-spec.pdf")∼cs152/
    [sp20/handouts/sp20/riscv-v-spec.pdf".]("https://inst.eecs.berkeley.edu/~cs152/sp20/handouts/sp20/riscv-v-spec.pdf")

    - <span id="page-8-43"></span>[44] *Intel Advanced Performance Extensions (Intel
    APX) Architecture Specification*, ["https://www.intel.com/content/www/us/en/developer/articles/]("https://www.intel.com/content/www/us/en/developer/articles/technical/intel-sdm.html"
    ) [technical/intel-sdm.html".]("https://www.intel.com/content/www/us/en/developer/articles/technical/intel-sdm.html"
    )

    - <span id="page-8-44"></span>[45] *Intel Advanced Vector Extensions 10 (Intel
    AVX10) Architecture Specification*, ["https://www.intel.com/content/www/us/en/developer/articles/]("https://www.intel.com/content/www/us/en/developer/articles/technical/intel-sdm.html"
    ) [technical/intel-sdm.html".]("https://www.intel.com/content/www/us/en/developer/articles/technical/intel-sdm.html"
    )

    - <span id="page-8-45"></span>[46] *NVIDIA Deep Learning Accelerator (NVDLA),
    Hardware Architectural Specification*, ["http://nvdla.org/hw/v1/hwarch.html".]("http://nvdla.org/hw/v1/hwarch.html")

    - <span id="page-8-46"></span>[47] *TileLink Specification*, ["https://www.sifive.com/document-file/]("https://www.sifive.com/document-file/tilelink-spec-1.9.3")
    [tilelink-spec-1.9.3".]("https://www.sifive.com/document-file/tilelink-spec-1.9.3")

    - <span id="page-8-47"></span>[48] *AMBA5 CHI Architecture Specification*, ["https://developer.arm.com/]("https://developer.arm.com/documentation/ihi0050/latest/")
    [documentation/ihi0050/latest/".]("https://developer.arm.com/documentation/ihi0050/latest/")

    - <span id="page-8-48"></span>[49] *AMBA5 ACE Protocol Specification (superseded
    by CHI)*, ["https://]("https://developer.arm.com/documentation/ihi0022/hc" ) [developer.arm.com/documentation/ihi0022/hc".]("https://developer.arm.com/documentation/ihi0022/hc"
    )

    - <span id="page-8-49"></span>[50] *AMBA5 AXI Protocol Specification*, ["https://developer.arm.com/]("https://developer.arm.com/documentation/ihi0022/latest")
    [documentation/ihi0022/latest".]("https://developer.arm.com/documentation/ihi0022/latest")

    - <span id="page-8-50"></span>[51] *AMBA4 AXI and ACE Protocol Specification*,
    ["https://developer.arm.]("https://developer.arm.com/documentation/ihi0022/e"
    ) [com/documentation/ihi0022/e".]("https://developer.arm.com/documentation/ihi0022/e"
    )

    - <span id="page-8-51"></span>[52] *10GE MAC Core Specification*, ["https://opencores.org/ocsvn/xge]("https://opencores.org/ocsvn/xge_mac/xge_mac/trunk/doc/xge_mac_spec.pdf"
    ) mac/ xge [mac/trunk/doc/xge]("https://opencores.org/ocsvn/xge_mac/xge_mac/trunk/doc/xge_mac_spec.pdf"
    ) mac spec.pdf".

    - <span id="page-8-52"></span>[53] *Ethernet IP Core Specification*, ["http://www.cprover.org/firmware/doc/]("http://www.cprover.org/firmware/doc/ethoc/eth_speci.pdf"
    ) ethoc/eth [speci.pdf".]("http://www.cprover.org/firmware/doc/ethoc/eth_speci.pdf"
    )

    - <span id="page-8-53"></span>[54] *I2C-Master Core Specification*, ["http://www.cprover.org/firmware/doc/]("http://www.cprover.org/firmware/doc/ethoc/eth_speci.pdf"
    ) ethoc/eth [speci.pdf".]("http://www.cprover.org/firmware/doc/ethoc/eth_speci.pdf"
    )

    - <span id="page-8-54"></span>[55] *UART to Bus Core Specifications*, ["https://opencores.org/usercontent/]("https://opencores.org/usercontent/doc/1330166592"
    ) [doc/1330166592".]("https://opencores.org/usercontent/doc/1330166592" )

    - <span id="page-8-55"></span>[56] *Elliptic Curve Group Core Specification*,
    ["https://opencores.org/projects/]("https://opencores.org/projects/ecg" ) [ecg".]("https://opencores.org/projects/ecg"
    )

    - <span id="page-8-56"></span>[57] *Tate Bilinear Pairing Core Specification*,
    ["https://opencores.org/projects/]("https://opencores.org/projects/pairing" )
    [pairing".]("https://opencores.org/projects/pairing" )

    - <span id="page-8-57"></span>[58] *Tiny Tate Bilinear Pairing Core Specification*,
    ["https://opencores.org/]("https://opencores.org/projects/tiny_tate_bilinear_pairing"
    ) [projects/tiny]("https://opencores.org/projects/tiny_tate_bilinear_pairing"
    ) tate bilinear pairing".

    - <span id="page-8-58"></span>[59] *AES Core Specification*, ["https://opencores.org/usercontent/doc/]("https://opencores.org/usercontent/doc/1354351714"
    ) [1354351714".]("https://opencores.org/usercontent/doc/1354351714" )

    - <span id="page-8-59"></span>[60] *SHA3 Core Specification*, ["https://opencores.org/usercontent/doc/]("https://opencores.org/usercontent/doc/1359445372"
    ) [1359445372".]("https://opencores.org/usercontent/doc/1359445372" )

    - <span id="page-8-13"></span>[61] *RISC-V Specifications*, ["https://riscv.org/technical/specifications/".]("https://riscv.org/technical/specifications/"
    )

    - <span id="page-8-60"></span>[62] C. Celio, D. A. Patterson, and K. Asanovic,
    "The berkeley out-of-order machine (boom): An industry-competitive, synthesizable,
    parameterized risc-v processor," *EECS Department, University of California, Berkeley,
    Tech. Rep. UCB/EECS-2015-167*, 2015.

    - <span id="page-8-61"></span>[63] Y. Xu, Z. Yu, D. Tang, G. Chen, L. Chen, L.
    Gou, Y. Jin, Q. Li, X. Li, Z. Li *et al.*, "Towards developing high performance
    risc-v processors using agile methodology," in *MICRO*, 2022.

    - <span id="page-8-62"></span>[64] J. White, Q. Fu, S. Hays, M. Sandborn, C. Olea,
    H. Gilbert, A. Elnashar, J. Spencer-Smith, and D. C. Schmidt, "A prompt pattern
    catalog to enhance prompt engineering with chatgpt," *arXiv preprint arXiv:2302.11382*,
    2023.

    - <span id="page-8-63"></span>[65] *verified calendar.v in RTLLM*, ["https://github.com/hkust-zhiyao/]("https://github.com/hkust-zhiyao/RTLLM/blob/main/calendar/verified_calendar.v"
    ) [RTLLM/blob/main/calendar/verified]("https://github.com/hkust-zhiyao/RTLLM/blob/main/calendar/verified_calendar.v"
    ) calendar.v".

    - <span id="page-8-64"></span>[66] *Amber 2 Core Specification*, ["https://opencores.org/websvn/filedetails?]("https://opencores.org/websvn/filedetails?repname=amber&path=%2Famber%2Ftrunk%2Fdoc%2Famber-core.pdf"
    ) [repname=amber&path=%2Famber%2Ftrunk%2Fdoc%2Famber-core.]("https://opencores.org/websvn/filedetails?repname=amber&path=%2Famber%2Ftrunk%2Fdoc%2Famber-core.pdf"
    ) [pdf".]("https://opencores.org/websvn/filedetails?repname=amber&path=%2Famber%2Ftrunk%2Fdoc%2Famber-core.pdf"
    )

    - <span id="page-8-65"></span>[67] Y. Chang, X. Wang, J. Wang, Y. Wu, K. Zhu,
    H. Chen, L. Yang, X. Yi, C. Wang, Y. Wang *et al.*, "A survey on evaluation of
    large language models," *arXiv preprint arXiv:2307.03109*, 2023.'
  decisions:
    evaluation_prompt: 'Qualified. Reason: The paper includes experimental results
      and evaluation of LLMs in generating and reviewing architecture specifications.'
    related_work_prompt: 'Qualified. Reason: The paper cites multiple research papers,
      explains and compares to cited work, and includes a section labeled “Related
      Work” or equivalent.'
    novelty_prompt: 'Qualified. Reason: The paper proposes new definitions, a dataset,
      and explores novel applications of LLMs in architecture specification development
      and review.'
    review_only_prompt: 'Qualified. Reason: The paper introduces new contributions
      by proposing definitions, datasets, and exploring applications of LLMs in architecture
      specification development and review.'
- title: "HOPE: Holistic STT-RAM Architecture Exploration Framework for Future\n \
    \ Cross-Platform Analysis"
  abstract: 'Spin Transfer Torque Random Access Memory (STT-RAM) is an emerging

    Non-Volatile Memory (NVM) technology that has garnered attention to overcome

    the drawbacks of conventional CMOS-based technologies. However, such

    technologies must be evaluated before deployment under real workloads and

    architecture. But there is a lack of available open-source STT-RAM-based system

    evaluation framework, which hampers research and experimentation and impacts

    the adoption of STT- RAM in a system. This paper proposes a novel, extendable

    STT-RAM memory controller design integrated inside the gem5 simulator. Our

    framework enables understanding various aspects of STT-RAM, i.e., power, delay,

    clock cycles, energy, and system throughput. We will open-source our HOPE

    framework, which will fuel research and aid in accelerating the development of

    future system architectures based on STT-RAM. It will also facilitate the user

    for further tool enhancement.'
  url: http://arxiv.org/abs/2401.14888v1
  keywords: '* Non-volatile memory, STT-RAM, Power Estimation, gem5, Emerging Technologies'
  document: "Date of publication xxxx 00, 0000, date of current version xxxx 00, 0000.\
    \ *Digital Object Identifier 10.1109/ACCESS.XXXX.DOI*\n\n# **HOPE: Holistic STT-RAM\
    \ Architecture Exploration Framework for Future Cross-Platform Analysis**\n\n\
    **SAEED SEYEDFARAJI (Graduate Student Member, IEEE), MARKUS BICHL, ASAD AFTAB\
    \ (Graduate Student Member, IEEE),and SEMEEN REHMAN(Member, IEEE).<sup>1</sup>\
    \ ,**\n\n<sup>1</sup>Faculty of Electrical Engineering and Information Technology,\
    \ Vienna University of Technology (TU-Wien), 1040 Vienna, Austria Corresponding\
    \ author: Saeed Seyedfaraji (e-mail: saeed.seyedfaraji@tuwien.ac.at).\n\n**ABSTRACT**\
    \ Spin Transfer Torque Random Access Memory (STT-RAM) is an emerging Non-Volatile\
    \ Memory (NVM) technology that has garnered attention to overcome the drawbacks\
    \ of conventional CMOS-based technologies. However, such technologies must be\
    \ evaluated before deployment under real workloads and architecture. But there\
    \ is a lack of available open-source STT-RAM-based system evaluation framework,\
    \ which hampers research and experimentation and impacts the adoption of STT-RAM\
    \ in a system. This paper proposes a novel, extendable STT-RAM memory controller\
    \ design integrated inside the gem5 simulator. Our framework enables understanding\
    \ various aspects of STT-RAM, i.e., power, delay, clock cycles, energy, and system\
    \ throughput. We will open-source our HOPE framework, which will fuel research\
    \ and aid in accelerating the development of future system architectures based\
    \ on STT-RAM. It will also facilitate the user for further tool enhancement.\n\
    \n**INDEX TERMS** Non-volatile memory, STT-RAM, Power Estimation, gem5, Emerging\
    \ Technologies\n\n## **I. INTRODUCTION**\n\nSTT-RAM boastsseveral compelling featuresincluding\
    \ nonvolatility, high density, soft error reliability, CMOS compatibility, high\
    \ endurance, and scalability [1]–[4]. According to the International Roadmap for\
    \ Devices and Systems (IRDS) [5], STT-RAM emerges as the most promising memory\
    \ option to replace conventional memory technologies. Table 1 presents a comprehensive\
    \ comparison of various design elements related to memory technologies, such as\
    \ endurance, associated read/write energy, latency, and compatibility with CMOS\
    \ technology size. It is worth noting that the listed suppliers are not exclusive\
    \ options, as alternative providers may also offer each type of memory technology.\
    \ Considering the decreasing technology node size over time due to Denard's law,\
    \ it is important to note that the suggested compatibility in the table is based\
    \ on the findings from the literature review of [1]–[4], [6]–[18]. Therefore,\
    \ it is plausible that each memory technology could potentially be fabricated\
    \ with other CMOS sizes.\n\nComparatively, the write and read energy of Dynamic\
    \ Random Access Memory (DRAM) Static Random Access Memory (SRAM) per operation\
    \ is lower than that of NVMs. Nevertheless, both DRAM and SRAM, as volatile memory\
    \ types, rely on a continuous power supply to retain stored\n\ndata. DRAM, commonly\
    \ employed as the main memory in computers, consumes more energy due to its constant\
    \ need for refreshing to preserve data integrity. Similarly, SRAM, another volatile\
    \ memory technology, also requires a consistent power supply for data retention.\
    \ On the other hand, NVMs like Flash memory can maintain data even when power\
    \ is off, resulting in lower energy consumption compared to DRAM and SRAM. Therefore,\
    \ NVM became an attractive alternative for main memories because of lower energy\
    \ consumption. Phase Change Memory (PCM), and STT-RAM [19] are some of the common\
    \ NVMs proposed to replace DRAM as main memory. They have also been explored for\
    \ building larger on-chip caches because of their high density. Amongst all, STT-RAM\
    \ has matured a lot in terms of its on-chip computation, and high energy efficiency\
    \ [20].\n\nHowever, the utilization of STT-RAM in widespread industrial applications\
    \ is hindered by several limitations. Notably, challenges such as write operation\
    \ delays and high write energy consumption. To overcome these issues, different\
    \ approaches have been explored at various levels of abstraction, including circuit-level\
    \ approaches [21]–[25], architecture-level approaches [10], [26], [27], and methods\
    \ proposed from an application perspective [28], [29].\n\n![](_page_1_Picture_0.jpeg)\n\
    \n|                         | STT-RAM          | PCMRAM                  | RRAM\
    \              | Fe-FET               | FLASH           | SRAM            | DRAM\
    \              |\n|-------------------------|------------------|-------------------------|-------------------|----------------------|-----------------|-----------------|-------------------|\n\
    | Non-volatility          | +                | +                       | +   \
    \              | +                    | +               | -               | -\
    \                 |\n| Data Retention (years)  | 10               | 10       \
    \               | 10                | 10                   | 10              |\
    \ -               | -                 |\n| Cell Endurance (cycles) | 1016    \
    \         | 1012                    | 1010              | 1012               \
    \  | 106             | 1016            | 1015              |\n| Cell Size (F 2)\
    \         | 6-20             | 4-8                     | 4                 | 4-8\
    \                  | 4-6             | >100            | 4-12              |\n\
    | Technology node (nm)    | 45               | 65                      | 40  \
    \              | 5                    | 15              | 10              | 32\
    \                |\n| Read Latency (ns)       | 2-20             | 20-50     \
    \              | <10               | 10                   | 25x103          |\
    \ <5              | 2                 |\n| Write Latency (ns)      | 5-20    \
    \         | 30                      | 5                 | 10                 \
    \  | 500x103         | <5              | 5                 |\n| Erase Latency\
    \ (ns)      | 5-20             | 30                      | 10                |\
    \ 10                   | 2 (ms)          | <5              | 5               \
    \  |\n| Write Energy (pJ)       | 0.1-2.5          | 18                      |\
    \ 0.1               | 1                    | 0.1 - 1         | <0.1          \
    \  | <0.1              |\n| Erase Energy (pJ)       | 1                | 18  \
    \                    | 0.1               | 1                    | 1000       \
    \     | <1              | <1                |\n| Suppliers               | Toshiba,\
    \ Hitachi | Samsung, Intel, WD, IBM | Panasonic, Micron | Globalfoundries, FMC\
    \ | Micron, Samsung | Qualcomm, Intel | Samsung, SK Hynix |\n\nTABLE 1: Comprehensive\
    \ comparison of NVM technologies [1]–[4], [6]–[18]\n\n# *A. NEED FOR STT-RAM BASED\
    \ SYSTEM EVALUATION FRAMEWORKS*\n\nResearchers have made significant contributions\
    \ to enhancing comparison metrics within their respective levels of abstraction\
    \ [1], [2], [4], [6]–[11], [13], [14], [24], [25]. However, it is crucial to note\
    \ that the current architectural perspective findings are derived from a behavioral\
    \ model of the circuit, which may not offer precise and detailed outcomes comparable\
    \ to those from a real computing system. This approach falls short in addressing\
    \ the need for a comprehensive system exploration framework. Hence, the associated\
    \ research challenge is *how to design a holistic system evaluation framework\
    \ that can be used to evaluate the impact of incorporating STT-RAM memories in\
    \ current systems, while accurately modeling the scaling, energy consumption and\
    \ performance characteristics of these devices and enabling architectural design\
    \ space exploration.*\n\nA number of simulation environments are available for\
    \ research and development of system-level exploration of computer architectures,\
    \ such as gem5 and ZSIM [30]. However, gem5 is widely used due to its ability\
    \ to emulate the full-system mode and help in the exploration of systemlevel metrics,\
    \ with different instruction set architectures (ISAs) such as Alpha, ARM, SPARC,\
    \ MIPS, RISC-V, and x86 ISAs), and various timing and CPU modes [31], [32]. ZSIM,\
    \ as an alternative simulation software, does not offer full-system simulation\
    \ capabilities, but also does not rely on event-driven execution and is therefore\
    \ faster. As this work targets the integration of STT-RAM into a complete system,\
    \ also showing capabilities of executing an operating system on top of STT-RAM,\
    \ gem5 is the selected choice. gem5 showed fast enough simulation speed for benchmark\
    \ applications on top of an operating system.\n\n# *B. ANALYZING STT-RAM IMPACT\
    \ ON DIFFERENT APPLICATIONS*\n\nIn order to verify the framework design along\
    \ with its advantages, a case study i.e., investigating STT-RAM from the perspective\
    \ of reducing the energy consumption of High Performance Computing (HPC) applications,\
    \ and its characteristics i.e., power, area, latency, etc., is carried out. Until\
    \ now an ideal platform for system-level evaluation is the gem5 simulator, as\
    \ it provides methods for generating system environments with easily exchangeable\
    \ separated components such as the memory controller, DRAM inter-\n\n## face,\
    \ and NVM interface.\n\n# *C. NOVEL CONTRIBUTION*\n\nIn order to meet the requirement\
    \ of designing a holistic system exploration framework, this paper introduces\
    \ an innovative memory interface utilizing STT-RAM, making it a notable contribution.\
    \ The interface has been created and seamlessly integrated into the gem5 simulator,\
    \ establishing a connection with the included memory controller.\n\n*The novel\
    \ contributions of this paper are:*\n\n- *•* We propose HOPE which is an STT-RAM\
    \ modeling and simulation framework integrated into the full system simulator\
    \ gem5.\n- *•* We leverage the recently implemented NVM interface in gem5 to integrate\
    \ HOPE with existing gem5 memory interfaces. This is in contrast to prior approaches\
    \ that rely on external patches (like NVMain), which become less maintainable\
    \ over time, thus stymying further development. Our proposed framework introduces\
    \ a third memory interface tailored specifically for STT-RAM. This extension offers\
    \ highly detailed results comparable to the existing DRAM implementation within\
    \ gem5. Fortunately, integrating our framework into gem5 requires only minimal\
    \ changes to gem5 files, as all functionality is implemented in new files that\
    \ can be added seamlessly. Our implementation can be used identically to the existing\
    \ memory interfaces and can potentially be integrated into the official gem5 repository\
    \ by its core maintainers. Such integration would be the ideal outcome for our\
    \ work, ensuring ongoing compatibility with gem5.\n- *•* We also extend the power\
    \ model in gem5 DRAM-Power to support our proposed STT-RAM model.\n- *•* We evaluate\
    \ HOPE using HPC applications from the SPEC CPU 2017 benchmark suite on our event-driven\
    \ gem5 simulator and successfully extract evaluation metrics from both the application\
    \ and circuit perspectives.\n- *•* We will also open-source our framework to enable\
    \ and accelerate the development of future system architectures based on STT-RAM.\n\
    \nThe rest of the paper is organized as follows: Section II presents the various\
    \ state-of-the-art works w.r.t. different types of memory controllers implemented\
    \ inside gem5 and their drawbacks. In section III, the STT-RAM integration\n\n\
    with the memory controller inside gem5 is discussed in detail. Section IV provides\
    \ the evaluation metrics, results, and comparison. Finally, we conclude this paper\
    \ in section V.\n\n#### **II. BACKGROUND AND RELATED WORKS**\n\nSTT-RAM has been\
    \ recently exploited as an alternative to conventional on-chip memories because\
    \ of its low energy consumption, high-speed access rate, scalability, and boundless\
    \ endurance. However, several fundamental barriers, i.e., reliability issues due\
    \ to Read/Write failure, Process Variation (PV) effects leading to stochastic\
    \ switching time, should be considered before its vast industrial adaptation.\n\
    \n# *A. OPERATION PRINCIPLES AND STT-RAM STRUCTURE*\n\nThe most common structure\
    \ of the STT-RAM cell includes an MTJ cell for data storage in series with an\
    \ access transistor (1T1MTJ). MTJ cells include an oxide barrier sandwiched between\
    \ two ferromagnetic layers called Rotation Layer (RL) and Fixed Layer (FL). The\
    \ magnetization orientation between these two layers will result in two different\
    \ states (i.e., parallel(P) and anti-parallel(AP)). These states are interpreted\
    \ as an indicator of logic one and logic zero (see Fig. 1). The concept of Read\
    \ and Write operation in the cell is explained in detail as follows:\n\n#### 1)\
    \ Write Operation\n\nIn order to write the intended information into the MTJ cell,\
    \ a write current should be applied through the memory cell. A successful writing\
    \ operation demands a minimum barrier exploiting the energy of the MTJ cell (*Eb*).\
    \ If the required energy is supplied, then the state of the memory could be changed\
    \ based on the current direction through the memory cell (see Fig. 1). In a coherent\
    \ STT-RAM model, the required current to fulfill the required minimum barrier\
    \ exploiting energy could be expressed as:\n\n$$I\\_c = I\\_{c0}(1 - \\frac{1}{\\\
    Delta\\_{\\text{max}}} \\ln(f\\_0 t\\_p))\\tag{1}$$\n\n$$I\\_{c0} = \\frac{8aeM\\\
    _s t}{\\eta h \\pi d^2} H\\_k \\tag{2}$$\n\nwhere, *Ic*<sup>0</sup> : critical\
    \ write current of the STT-RAM model (at 273*◦* Kelvin). This parameter is related\
    \ to the physical\n\n![](_page_2_Figure_12.jpeg)\n\nFIGURE 1: Schematic representation\
    \ of magnetic orientation and energy barrier between two Magnetic Tunnel Junction\
    \ (MTJ) states [33]\n\nproperty of the MTJ cell, such as: *Ms*: Material saturation\
    \ magnetization; *f*0: Attempt frequency which is typically *∼*1 ns, *Hk*: Effective\
    \ magnetic anisotropy field, *α*: Damping factor, *η*: Spin polarization, *tp*:\
    \ Operating pulse-width (inverse of frequency), which provides an access frequency\
    \ of 1 GHz; *t, d*: physical dimensions of the MTJ cell; ∆: Thermal stability\
    \ factor, and it can be expressed as:\n\n$$\n\\Delta = \\frac{(M\\_s H\\_k \\\
    text{td})}{2k\\_B T} \\tag{3}\n$$\n\nwhere, *kBT* : Describes the ambient energy\
    \ in the system due to random thermal fluctuations [33].\n\n#### 2) Read Operation\n\
    \n*R<sup>P</sup>* Read operation in MTJ cell is due to the current *Iread* passage\
    \ through the cell. *Iread* must be less than *Icritical* and must not lead to\
    \ change in the cell. The MTJ exhibits resistance value based on its two layers'\
    \ magnetization (0*◦* or 180*◦* in Fig. 1). Therefore, by sensing, and passing\
    \ the *Iread* and measuring the resistance, we could identify the state of the\
    \ cell. Moreover, the parameter Tunnel Magnetoresistance Ratio (TMR) is described\
    \ as the difference between these two resistance states and can be expressed as\
    \ *TMR* = (*RAP <sup>−</sup>R<sup>P</sup>* ) , where *RAP* is the resistance in\
    \ anti-parallel and *R<sup>P</sup>* is the resistance in parallel state. TMR is\
    \ directly related to the cell's read latency, which means a higher TMR enables\
    \ a faster and more precise read operation [33].\n\n#### *B. RELATED WORK*\n\n\
    Recent studies show NVM as the main memory element via a simulation in gem5, as\
    \ gem5 was isolated from an NVM interface till October 2020. The lack of an NVM\
    \ interface made it necessary for NVM research to use external tools such as NVSim\
    \ [34], NVMain [35], and NVMain 2.0 [36]. To maintain compatibility, these NVM\
    \ simulators must be developed simultaneously with the gem5 simulator. Especially\
    \ NVMain offers a patch to be applied to the gem5 simulator, which connects the\
    \ NVMain tool to the gem5 simulator. This patch directly modifies the gem5 source\
    \ code and needs to be updated to match the latest gem5 releases. The result of\
    \ the patch is a co-simulator where the gem5 simulator is in constant interaction\
    \ with NVMain. Small changes in the gem5 simulator can directly require a change\
    \ in the NVM simulation tools and the needed patches for integration into a co-simulator.\
    \ The official patch for integrating NVMain into the gem5 was last updated in\
    \ December 2016 and is incompatible with recent releases of the gem5. Furthermore,\
    \ these state-of-theart STT-RAM gem5 tool flow lacks available open-source models,\
    \ which has hampered research and experimentation, impacting the adoption of STT-RAM\
    \ in the current systems.\n\nTherefore, in this manuscript, we propose a holistic\
    \ gem5 based framework that will be open-sourced to fuel research and development\
    \ in this area and further enhancement in the framework.\n\n![](_page_3_Figure_2.jpeg)\n\
    \nFIGURE 2: The STTDDR4 Interface integration into the gem5 standard componentslibrary.\
    \ Components in blue are modified or new to gem5, and components in white are\
    \ unmodified gem5 components.\n\nThe work presented in [37] explores some architecturelevel\
    \ modifications of STT-RAM structure aiming to provide NVM-based row buffers and\
    \ reports a 67% energy improvement exploiting their approaches compared with state-ofthe-art\
    \ techniques. Moreover, authors in [29] explore the possibility of using STT-RAM\
    \ to substitute the DRAM in main memory and evaluate their approach based on the\
    \ SPEC CPU2006 dataset to be compared with DRAM-based memories. This study has\
    \ been carried out on a tracebased cycle-accurate simulator. In [38], an STT-RAM-based\
    \ memory has been proposed based on a 9F2-cell on the circuit level. The exploited\
    \ MTJ model in this work requires a low switching current to change the state\
    \ from logicone to logic-zero. The application-level analysis has been estimated\
    \ based on an HPC SPEC CPU 2017 benchmark for latency improvement. The aforementioned\
    \ techniques either perform only circuit-level simulations (NVSim) which is typically\
    \ time-consuming due to detailed hardware simulations, while other approaches\
    \ that evaluate applications at the system level are not available open-source\
    \ (to the best of our knowledge), thus obstructing the adoption of the STT-RAM\
    \ model in the systems. *Our novel proposed HOPE framework has a fully integrated\
    \ architectural model of STT-RAM in the gem5 simulator using a memory controller\
    \ for exploiting system-level characteristics. This is in contrast to prior approaches\
    \ that rely on external patches (like NVMain). HOPE is an event-driven gem5 simulator\
    \ that facilitates system-level evaluation and enablesthe extraction of comparative\
    \ metrics across all layers of the system hierarchy.*\n\n## **III. THE HOPE FRAMEWORK**\n\
    \n#### *A. HOPE FRAMEWORK OVERVIEW*\n\nThe novel proposed STT-RAM interface in\
    \ this manuscript is an additional memory interface to the gem5. Therefore, in\
    \ order to satisfy the compatibility of the STT-RAM interface with the gem5's\
    \ MemCtrl component, there is a\n\nneed for some modifications to the MemCtrl\
    \ component. This tailoring has no effect on the existing functionality of connecting\
    \ DRAM or NVM memories.\n\nIn the system configuration of gem5, the MemCtrl component\
    \ offers a single port to connect a main memory instance to the MemCtrl, historically\
    \ called *DRAM*, used for all types of memories. We introduced the STT-RAM interface\
    \ to the gem5 as an alternative choice to the DRAMInterface and NVMInterface components\
    \ as shown in Fig. 2. The interface is implemented in C++ (Component functional\
    \ description), wrapped by a Python parameter configuration (part of Component\
    \ definitions). The Python wrapper defines and inherits parameters that are mandatory\
    \ for the component to work. The functionality of the STTDDR4Interface is placed\
    \ within its C++ class. The component STT\\_1333\\_4x16 is the test device, populated\
    \ with parameters from the EMD4E001G16G2 datasheet [39]. Our proposed framework\
    \ enables access to modify the memory via its integrated interface and fetch the\
    \ output data in our component definition. Thus, we integrated all the functionality\
    \ into the STT-RAM memory controller through the provided interface to configure\
    \ and evaluate the system analysis. This provides an edge over the state-of-the-art\
    \ proposed methods that are based on the co-simulation of gem5 and other simulation\
    \ tools (e.g., NVSIM, NVMAIN, etc.).\n\n#### *B. HOPE STT-RAM POWER MODEL*\n\n\
    The implemented STTDDR4 memory interface implementation represents a state machine\
    \ consisting of states for idle, storing, activating, power up, and power down\
    \ stages, as shown in Fig. 3. The MemCtrl instance of the simulated system and\
    \ the memory logic help in the transition of the state in the state machine. As\
    \ soon as the system is started, the very first state of the system is the PWR\\\
    _IDLE state. This is the state from where the transition to activate state PWR\\\
    _ACT, or activate with store state PWR\\_ACT\\_ST\n\n![](_page_4_Figure_1.jpeg)\n\
    \nFIGURE 3: Power state machine of STTDDR4 integration to gem5\n\ncan be achieved\
    \ using an ACT or ACT\\_ST command. The ACT\\_ST command is introduced to gem5\
    \ which is responsible for the STT-RAM-specific handling of data in the volatile\
    \ page buffer.\n\nThe main memory can be exploited using gem5 standard commands\
    \ i.e., for a bank activation ACT (activate), RD (read), WR (write), REF (refresh\
    \ all banks), SREF (Self-Refresh), PRE (explicit pre-charge of a single bank),\
    \ etc. In this manuscript, an additional command is been introduced i.e., ACT\\\
    _ST (activate with the store). In the EMD4E001G16G2 datasheet, [39], the introduced\
    \ ACT\\_ST command is known as ACT\\*. The ACT\\* command includes a store procedure\
    \ for the specific bank accessed. The implementation of the ACT\\_ST command to\
    \ gem5 also includes a new event, the *actStoreEvent*, to enable the transition\
    \ to the new power state PWR\\_ACT\\_ST. Moreover, the automatic execution of\
    \ REF commands needed for data persistence in DRAM is not included in STTDDR4,\
    \ as refreshes are not mandatory in STT-RAM devices as per the EMD4E001G16G2 devices.\n\
    \nAs we know that method calls are responsible for MemCtrl interaction with the\
    \ memory interface during simulation\n\n# **Algorithm 1** Select between ACT and\
    \ ACT\\_ST command on rank:bank:row bank.storingState *←* PERSISTENT bank.lastRow\
    \ *←* 0\n\n```\nprocedure ACTIVATEBANK(rank, bank, row)\n  cmd ← ACT\n  if bank.lastRow\
    \ ̸= row then\n     if bank.storingState = BUFFER then\n        cmd ← ACT _ST\n\
    \        cmdDelay ← cmdDelay + tST\n     end if\n  end if\n  cmdList.push_back(cmd,\
    \ bank, delay) \n  bank.lastRow ← row\n  bank.storingState ← BU FFER\n  process\
    \ cmd in drampower\n```\n#### **Algorithm 2** DRAM Power Extension nOfActsBanks[banks]\
    \ *←* zeros(banks) nOfStoresBanks[banks] *←* zeros(banks) **procedure** EVALUATECOMMANDS(*cmdList*)\
    \ **for all** *cmd ∈ cmdList* **do if** *cmd.type* = *ACT* **then** *handleAct*(*cmd.bank*)\
    \ **else if** *cmd.type* = *ACT*\\_*ST* **then** *handleActSt*(*cmd.bank*) **else\
    \ if** <other command types> **then** <handle commands> **end if end for end procedure\
    \ procedure** HANDLEACTST(*bank*) **if** isPrecharged(bank) **then** *nOf ActsBanks*[*bank*]\
    \ + +; *nOf StoresBanks*[*bank*] + +; **end if end procedure procedure** POWER\\\
    _CALC() *calc*(*sum*(*nOf StoresBanks*) *∗ tST, idd*0 *− idd*3*n*) **for all**\
    \ *bank ∈ banks* **do** *calc*(*nOf StoresBanks*[*bank*] *∗ tST, idd*0 *− ione*)\
    \ **end for end procedure**\n\n**procedure** CALC(*cycles*, *current*) **return**\
    \ (cycles\\*clkPeriod\\*current\\*voltage) **end procedure**\n\ne.g., while reading\
    \ or writing data, the MemCtrl initiates a burst access to the memory device where\
    \ MemCtrl provides data for which rank, bank and row the burst access needs to\
    \ be executed. This rank, bank, and row information are handed over to the bank\
    \ activation method, as shown in algorithm 1. The EMD4E001G16G2 device includes\
    \ functionality for automatic storing of page memory data to the persistent memory\
    \ array when the page memory data would become lost otherwise. The MemCtrl does\
    \ not offer functionality for differentiation of the storing states in STT-RAM.\n\
    \nTherefore, the STTDDR4Interface got extended with functionalities to track the\
    \ storing state of the data in the page buffer of each bank. The storing state\
    \ of each bank supports the states BUFFER and PERSISTENT. All the banks during\
    \ startup are initialized with PERSISTENT, which indicates the page buffer data\
    \ to be saved to the persistent memory array. Moreover, the state BUFFER proves\
    \ the bank to be not saved to the bank's persistent memory array. Also, the last\
    \ row accessed will be saved in each bank and updated during each bank activation.\
    \ In order to change the storing state of a bank, or all banks, to PERSISTENT,\
    \ a store operation needs to be performed. This can be triggered by an ACT\\_ST\
    \ command, a REF command, or an SREF command. Within a REF or SREF execution,\
    \ store operations on all banks in storing state BUFFER will be performed. When\
    \ there are no banks in storing state BUFFER, the commands REF and SREF are not\
    \ effective.\n\nThe selection between the ACT or ACT\\_ST command is done in the\
    \ activated bank method with a sequence of procedure calls as Algorithm 1. The\
    \ requested row is compared\n\nto the last accessed row of the particular bank.\
    \ When the last accessed data is still the working data, which means the last\
    \ access row and requested row are identical, there is no need for a store operation.\
    \ In this case, a normal ACT command will be simulated. If the requested row differs\
    \ from the last accessed row, and the bank is in the storing state \"BUFFER\"\
    , an ACT\\_ST command will be pushed to simulate. The difference in simulating\
    \ ACT or ACT\\_ST is implemented in a higher delay for the additional store operation,\
    \ which is known as the store time (tST). This store procedure call impacts the\
    \ energy consumption that can be calculated in the power library. The bank's storing\
    \ state can be changed to \"PERSISTENT\" by performing a store operation on the\
    \ particular bank, or on all banks. The ACT command is simulated as with DRAM\
    \ memory in gem5: The ACT command is saved in a list of pending commands which\
    \ is handed over to the modified DRAMPower tool [40], which is part of gem5 and\
    \ performs energy calculations from the gem5 inputs.\n\n**Extensions to DRAMPower\
    \ Model:** To be able to include the ACT\\_ST command, DRAMPower got extended\
    \ by the command and energy calculation. The extensions to DRAMPower are presented\
    \ in Algorithm 2. These extensions include functionality for counting the number\
    \ of store procedures during runtime and calculating the resulting store energy\
    \ and power.\n\nMoreover, the energy calculation in gem5 is not updated on every\
    \ new command execution, but on specific simulation phases: suspension, end of\
    \ a refresh (REF) command, triggered from the gem5 system configuration script\
    \ or by the internal command line of the full system simulation using the m5 utility\
    \ (the gem5 utility used in full system disk images).\n\nFurthermore, the gem5\
    \ statistic output has been modified to include the store energy per rank and\
    \ power state time per rank in the simulation results. In this section, we present\
    \ our HOPE framework for an all-integrated STT-RAM with a gem5 simulator using\
    \ the memory controller. This helps in exploiting system-level meta-heuristics\
    \ that include power consumption, memory utilization, heat patterns, etc.\n\n\
    #### *C. HOPE CONFIGURATION*\n\nThe gem5 being an instruction-level simulator\
    \ enables the integration of different types of memories with a memory controller.\
    \ The memory controller is a component that enables an option of choosing memory\
    \ for system-level analysis. The memory controller has evolved a lot in the\n\n\
    TABLE 2: The Configuration of the Memory cell\n\n| Paramters             | Configuration\
    \                    |\n|-----------------------|----------------------------------|\n\
    | Memory                | 1Gbit x16                        |\n| Organization \
    \         | 8-banks (2-banks per Bank Group) |\n| Latency               | 667MHz\
    \ (1333MT/s)                |\n| Access Time           | 225ps               \
    \             |\n| Supply Voltage - Min: | 1.14 V                           |\n\
    | Supply Voltage - Max: | 1.26 V                           |\n\npast few years.\
    \ Recently, in May 2020 the gem5 introduced a new memory controller (MemCtrl)\
    \ component revision and introduced an NVM interface (NVMInterface) class to the\
    \ gem5, officially integrated into version 20.1 of the gem5 simulator. This NVM\
    \ interface is designed very generic in terms of its functionality and parameters\
    \ to be taken into consideration. The NVMInterface class offers three timing parameters:\
    \ tREAD, tWRITE, and tSEND.\n\nThere is also an already existing DRAM interface\
    \ (DRAMInterface) class. This class of the gem5 contains detailed logic on DRAM\
    \ timing and power state simulation and offers various timing, e.g. tRCD, tCL,\
    \ tRAS, and tREFI, and energy parameters, e.g. IDD0, IDD4R, and VDD. But there\
    \ is no such logic for calculating NVM energy and power consumption. Also, there\
    \ are no energy parameters available for NVMInterface.\n\nThus, to overcome such\
    \ shortcomings, HOPE provides another detailed memory interface targeting STT-RAM.\
    \ This memory targets real-world STT-RAM devices which are designed as STT on\
    \ top of DDR4. Therefore, this interface is named as STTDDR4Interface. This offers\
    \ a high level of detail timing and energy parameters, combined with a power state\
    \ and energy calculation logic. Fig. 2 depicts the detailed flow of our HOPE framework\
    \ within the extended and modified gem5 simulator.\n\nThe simulated system is\
    \ configured using the fs.py system configuration script. Using this script, a\
    \ System instance is set up according to the input values of the HOPE framework.\
    \ We use an X86 architecture-based system. The CPU we defined is the TimingCPU,\
    \ which offers detailed memory timings during simulation. The CPU is also equipped\
    \ with L1 and L2 caches. The system uses a Linux kernel and a disk image with\
    \ an Ubuntu operating system and workloads installed. Within gem5, communication\
    \ between system blocks is done via ports, also as real systems do. The system\
    \ block is connected to the Membus block. The memory bus selected is by default,\
    \ the SystemXBar. All CPU interactions to the main memory are forwarded by the\
    \ Membus to the memory controller. MemCtrl got modified to support STT-RAM connected\
    \ through the memory port.\n\nFig. 2, shows the proposed architecture with the\
    \ STT\\_1333\\_4x16 which is a class created for interfacing STT-RAM with the\
    \ memory controller. It has multiple parameters e.g., tCK, tST, tRCD, IDD0, IDD2N,\
    \ etc that has been extracted from the datasheet of the aforementioned device.\n\
    \nThe tCK is the clock period, depending on the device operating clock frequency\
    \ (fCK) (e.g., fCK = 667MHz results in 1.5ns tCK (=1/fCK)), tST is a special timing\
    \ parameter for STT-RAM and refers to the storing time of the memory (indicates\
    \ the time needed for storing the data from the row address buffer to the persistent\
    \ STT memory). The tST is a newly introduced parameter to gem5 unique to STT,\
    \ which was a missing timing parameter for gem5. The address buffer acts like\
    \ a cache, and the data placed in the cache needs to be written to the main STT\
    \ memory\n\nTABLE 3: Experimental systems configuration for STT-RAM and DRAM\n\
    \n| System elements | Processor                                 | L1 Instr.<br>cache\
    \ | L1 Data<br>cache | L2 cache         | Main Memory                        \
    \                                                            | Clock Speed | Row\
    \ Buffer<br>Size | Device Size | Channel<br>Cappacity | tRCDmin | tRCmin | tRASmin\
    \ | tFAWmin | tRPmin |\n|-----------------|-------------------------------------------|--------------------|------------------|------------------|------------------------------------------------------------------------------------------------|-------------|--------------------|-------------|----------------------|---------|--------|---------|---------|--------|\n\
    | STT-RAM         | 64-bit x86<br>single core,<br>timing 3GHz | Private,<br>32kB\
    \   | Private,<br>32kB | Shared,<br>256kB | 1 channel,<br>2 rank per channel,<br>4\
    \ chips per rank,<br>EMD4E001G16G2,<br>1Gbit x16, 1333MHz | 667MHz      | 256B\
    \               | 128MiB      | 1GiB                 | 135ns   | 44.5ns | 32ns\
    \    | 15ns    | 12.5ns |\n| DRAM            | \"                            \
    \             | \"                  | \"                | \"                |\
    \ 1 channel,<br>2 rank per channel,<br>8 chips per rank,<br>MT40A1G8SA,<br>1Gbit\
    \ x8, 2400MHz     | 1200MHz     | 1kB                | 1GiB        | 16GiB   \
    \             | 12.5ns  | 190ns  | 143ns   | 240ns   | 7.5ns  |\n\nduring the\
    \ Store operation. Therefore, tST is the time needed to process data moving from\
    \ the row address buffer to the STT persistent memory array. Researchers in the\
    \ field could optimize different metrics to minimize this value and evaluate the\
    \ performance of so-called in-memory processing approaches. The parameters such\
    \ as tRCD, IDD0, IDD2N, etc., are standardized DDR4 parameters.\n\nMoreover, this\
    \ interface makes it possible to simulate systems using the latest STT-RAM devices\
    \ including power states and energy consumption as it was never possible before\
    \ in a stand-alone gem5 environment. The integration carried out on the interface\
    \ is based on the parameters offered by the STT-RAM EMD4E001G16G2 from Everspin\
    \ Technologies [39]. These device parameters are shown in Table 2.\n\nAs per the\
    \ physical characteristics of STT-RAM, there are deviations to the DDR4 specification\
    \ for DRAM especially the *Refresh* command, which is mandatory to be issued in\
    \ a time interval tREFI on DRAM, is no longer used in STT-RAM. Therefore, tREFI\
    \ got removed for STT memory. Moreover, the STT-RAM also has a store time parameter\
    \ tST. The store operation of delay tST, is used to move recently written data\
    \ from the page buffer to the persistent memory array.\n\nSome other deviations\
    \ specific to the test devices (simulating the EMD4E001G16G2 device) include the\
    \ memory size that in the case of STT-RAM is a 1 Gbit device, whereas the DDR4\
    \ specification for DRAM only allows devices of 2, 4, 8 and 16 GBit. Furthermore,\
    \ there is also a limit of 667 Mhz for the clock frequency, while the DDR4 Specification\
    \ for DRAM allows 800, 933, 1067, and 1200MHz.\n\n### *D. EVALUATION SETUP CONFIGURATION*\n\
    \nFig. 4 presents a comprehensive overview of the HOPE framework setup configuration\
    \ steps, highlighting its key contributions depicted in blue. gem5 full system\
    \ simulations require a disk image prepared with an operating system and a kernel\
    \ compatible with the chosen operating system. The HOPE framework uses a 1 modified\
    \ Packer SPEC CPU 2017 setup script from the gem5 resources repository for generating\
    \ a disk image containing the Ubuntu operating system and the SPEC CPU 2017 benchmark\
    \ suite for X86 architecture. The 2 benchmark installation is then followed finalized\
    \ by mounting the disk image on the host system. Each benchmark from the disk\
    \ image has been run once for finalizing the benchmark installation, this includes\
    \ com-\n\n![](_page_6_Figure_10.jpeg)\n\nFIGURE 4: Overview of HOPE framework\n\
    \npiling, training, and running the benchmark. In 3 the gem5 full system simulation\
    \ including the HOPE extensions and modifications is run. Therefore the created\
    \ disk image is used. Each simulation runs a selected workload from the prepared\
    \ disk image, includes the creation of checkpoints after the operating system\
    \ boot, and the output of the 4 detailed statistics after the gem5 simulation\
    \ is completed. The gem5 full system simulation includes the introduced STT-RAM\
    \ extension and modifications to DRAMPower to allow detailed energy calculation\
    \ for our STT-RAM device. 5 shows the provided McPAT template file, modified to\
    \ support the extended outputs of gem5. Using the system configuration, simulation\
    \ statistics, and McPAT template, the 6 the GEM5ToMcPAT [41] tool is used to generate\
    \ an input file for later use with McPAT. HOPE includes the enhanced 7 cMcPAT\
    \ power, area, and timing modeling framework. cMcPAT [41] is capable of calculating\
    \ the power parameters of 9. Using the statistics output of gem5, and the power\
    \ model of cMcPAT, the script \"print\\_energy\" [41] is calculating the total\
    \ energy consumption of the simulated environment. The results also combine the\
    \ detailed output of gem5, especially, the instructions count.\n\nThe simulated\
    \ system is configured using the gem5 stdlib (gem5 standard library) based on\
    \ an X86 configuration. Table 3 lists the detailed configuration of the processor,\
    \ cache, and memory for both experimental systems using STT-RAM and DRAM. We selected\
    \ an existing STT-RAM device for simulation and paired it with a widely used DRAM\
    \ device to facilitate a functional comparison. It's\n\nimportant to note that\
    \ our chosen STT-RAM device does not align with JEDEC's JESD79-4A DDR4 standard,\
    \ which limits our ability to select a DDR4-based DRAM device with nearly identical\
    \ parameters.\n\nMoreover, Fig. 5 shows the system configuration used for benchmarks\
    \ using STT-RAM. gem5 offers a general full system default configuration script\
    \ (fs.py), which we used in this work to reflect the system architecture of our\
    \ simulated system. We used the gem5 full system emulation mode to reflect real-world\
    \ systems in the best way provided.\n\nThe operating system selected is an Ubuntu\
    \ 18.04 configured for gem5 and set up with a SPEC CPU 2017 benchmark suite [42]\
    \ instance. The kernel used is the linux kernel version 4.19.83. The gem5 configuration\
    \ script handles the creation of memory controllers and memory devices. The count\
    \ of memory ranks and banks is set in the memory device configuration. The parameters\
    \ used for the STT-RAM device configuration in gem5 are sourced from the EMD4E001G16G2\
    \ datasheet [39]. We performed SPEC CPU 2017 benchmarks on our simulated systems\
    \ using 2 checkpoints per benchmark to be able to perform detailed simulations\
    \ using the TimingCPU from gem5.\n\nThe first checkpoint has been saved after\
    \ the OS boot is finished. The second checkpoint has been saved after the first\
    \ 4.5 billion instructions of benchmark application to ensure the initialization\
    \ phase has been finished and the checkpoint defines a direct jump into the main\
    \ benchmark algorithm. Both checkpoints were performed in a fastforward method\
    \ using gem5's AtomicSimpleCPU. The main simulation run was done from the second\
    \ checkpoint for a total of 2 Billion instructions. This procedure has been performed\
    \ for all of the benchmark applications included in SPEC CPU 2017.\n\n## **IV.\
    \ RESULTS**\n\nIn our research endeavour, our primary focus revolved around conducting\
    \ simulations utilizing the parameters of real-world devices. Specifically, we\
    \ honed in on a selected STT-RAM device, which holds the distinction of being\
    \ DDR4-compatible, albeit with certain deviations from the official standard.\
    \ As we delved into the simulations, we\n\n![](_page_7_Figure_8.jpeg)\n\nFIGURE\
    \ 5: Architecture of the STT-RAM simulated system.\n\nobserved that these deviations\
    \ had a tangible impact on our results, which we duly documented.\n\nOur findings\
    \ shed light on the applicability of HOPE as a potent tool for evaluating the\
    \ feasibility of incorporating STT-RAM main memories into practical systems. By\
    \ harnessing the capabilities of HOPE, we were able to gain insightful glimpses\
    \ into the performance and energy efficiency of the SPEC CPU 2017 benchmarks when\
    \ paired with STT-RAM. This analytical approach not only offers a valuable lens\
    \ to understand the potential of STT-RAM but also opens up new vistas of exploration\
    \ and optimization in memory technology research.\n\nThe gem5 simulator provides\
    \ users with multiple avenues for creating system configurations through its gem5\
    \ standard library of components and functions, including CPUs, memory, boards,\
    \ cache hierarchies, and more. This comprehensive collection of building blocks\
    \ is commonly referred to as the gem5 standard library. For instance, if you wish\
    \ to modify the CPU architecture, you can simply select the corresponding board\
    \ (e.g., X86Board or ArmBoard) and fine-tune the memory configurations accordingly.\
    \ This flexible approach can be applied to alter any available system component.\n\
    \nAlternatively, there is an option to configure a system within gem5 without\
    \ utilizing a predefined board. Instead, you can manually establish connections\
    \ between a CPU and a selected memory device using a memory bus. Furthermore,\
    \ you have the flexibility to augment the CPU with either a straightforward single-level\
    \ cache or a more intricate cache hierarchy to suit your needs.\n\nOur proposed\
    \ framework takes a distinct approach by leveraging an existing system configuration\
    \ known as fs.py. This configuration can be effortlessly modified via command-line\
    \ inputs, enabling rapid adjustments to the system configuration with a single\
    \ bash script edit. Different fs.py configurations are available for various system\
    \ architectures, such as *configs/example/riscv/fs*\\_*linux.py* for RISC-V or\
    \ *configs/example/arm/fs*\\_*bigLITTLE.py* for ARM.\n\nIn our research endeavors,\
    \ we conducted extensive simulations employing both fs.py and custom gem5 system\
    \ configuration scripts. This comprehensive approach allowed us to thoroughly\
    \ assess and analyze our simulations.\n\n## *A. STT-RAM STATE TIME DISTRIBUTION*\n\
    \nAs depicted in Fig. 8(b), the power state times for the IDLE state exhibit variations\
    \ in the case of STT-RAM, contingent upon the specific workload. Notably, STT-RAM\
    \ distinguishes itself by eschewing the need for periodic refreshes to maintain\
    \ data states, leading to a complete absence of time spent in the REF power state.\
    \ Conversely, the bank activation (ACT) time, also illustrated in Fig. 8(b), demonstrates\
    \ only a minor variation within the STT-RAMbased system. This effect can be attributed\
    \ to the relatively prolonged delays observed in the bank activation process,\
    \ especially concerning store state (ACT\\_ST) for STT-RAM,\n\nas visually demonstrated\
    \ in Fig. 8(b). Such insights garnered from our analyses provide valuable perspectives\
    \ on the dynamic behaviour and efficiency of STT-RAM, imparting crucial knowledge\
    \ for potential real-world implementations and optimizations in-memory technologies.\n\
    \n# *B. STT-RAM PERFORMANCE EVALUATION*\n\nFig. 6(a) show the total amount of\
    \ read and write requests which are generally higher in the STT-RAM devices. High\
    \ read and write requests are explained by Fig. 8(a) which shows the hit rate\
    \ for read operations. The hit rates for STT-RAM depend on the application complexity\
    \ level. Fig. 8(c) shows the average read and write bandwidth with a simulated\
    \ STT-RAM. Also, the average latency of STT-RAM, shown in Fig. 6(b), for each\
    \ application highly depends on the hit ratio. As described in our STTDDR4 power\
    \ state machine description, a high alteration of accessed rows negatively affects\
    \ the energy and latency of our simulated STT-RAM device. A high alteration of\
    \ accessed rows further has an impact on store operations.\n\n# *C. STT-RAM POWER\
    \ AND ENERGY BREAKDOWN*\n\nIn Fig. 6 (c and d), we present a comprehensive view\
    \ of the power and energy breakdown for our simulated systems, offering valuable\
    \ insights into their performance characteristics. The shown parameters in d represent\
    \ the accumulated energy for different commands issued to the memory device. As\
    \ shown in Fig. 3, the memory devices move through different states during runtime.\
    \ The parameter \"Activation energy\" therefore shows the total energy consumption\
    \ for all ACT commands initiated. \"Store energy\" accumulates the energy of all\
    \ store operations during the evaluation. In the case of an ACT\\_ST operation,\
    \ the energy accumulated separately for the activation and the store energy results.\
    \ The parameter \"Refresh energy\" is associated to the REF command, while the\
    \ \"RD/WR energy\" is the accumulated energy during the processing of the read\
    \ and write burst accesses.\n\nParticularly noteworthy is the substantial count\
    \ of store operations, which, in conjunction with the notably high IDD0 current\
    \ of 437mA, prominently influences the calculated store energy. As evident from\
    \ the results, STT-RAM stands out for its lack of refresh energy requirements.\
    \ However, it should be noted that the stored energy demands contribute to an\
    \ overall increase in the total energy breakdown. These findings shed light on\
    \ the contrasting energy consumption patterns of STT-RAM compared to conventional\
    \ memory technologies, signifying the potential for more energy-efficient computing\
    \ paradigms. The comprehensive understanding gained from these power and energy\
    \ analyses is essential for devising strategies to optimize memory architectures,\
    \ thus fostering advancements in the realm of energy-efficient computing systems.\
    \ The presented energy parameters in d are not a full view of all calculated energy\
    \ parameters within gem5, but an excerpt of significant values. The full list\
    \ of energy parameters is extended by\n\nparameters for interface energy, standby\
    \ energy, self-refresh energy, power-down and power-up energy.\n\n# *D. DRAM METRICS*\n\
    \nTo maintain the extendibility and versatility of our framework, we have thoughtfully\
    \ retained the interface to the DRAM. This strategic decision allows our framework\
    \ to adapt effortlessly to various memory technologies, rendering it highly versatile\
    \ for a wide array of computing scenarios. In this section, we present an in-depth\
    \ analysis of the extracted data concerning the state time distribution, power\
    \ consumption, energy usage, and latency breakdown for the same applications from\
    \ SPEC 2017, with the DRAM serving as the primary memory. This comprehensive investigation\
    \ is instrumental in understanding the behavior and performance characteristics\
    \ of our framework when interfacing with DRAM.\n\nFig. 9(b) illustrates the state\
    \ time distribution of these applications when utilizing DRAM. Additionally, Fig.\
    \ 7(a) showcases the framework's memory requests. Analyzing memory requests offers\
    \ deeper insights into the applications´ memory access patterns, shedding light\
    \ on potential areas for improvement in terms of data locality and cache utilization.\
    \ Fig. 9(c) provides insight into bandwidth usage with DRAM as the memory. Bandwidth\
    \ utilization is a critical metric for assessing memory system efficiency and\
    \ identifying potential bottlenecks that may impact application performance. Furthermore,\
    \ Fig. 9(a) reveals DRAM row hits, and Fig. 7(b) presents latency per application.\
    \ Finally, Fig. 7(c and d) exhibits DRAM average power and energy usage while\
    \ running the SPEC 2017 applications. These detailed analyses offer valuable insights\
    \ into our framework's performance and its potential for adaptation to future\
    \ memory technologies and diverse computing environments.\n\n## *E. COMPARING\
    \ STT-RAM AND DRAM RESULTS*\n\nBased on our evaluation of SPEC 2017 benchmarks,\
    \ it becomes evident that STT-RAM is not yet ready to replace DRAM-based main\
    \ memories for many applications due to its higher store latency and energy consumption.\
    \ STT-RAM needs storing from the page buffer to the persistent memory array whereas\
    \ DRAM does not need this. Due to the overhead for storing data in the STT persistent\
    \ memory array within the ACT-ST state, the delays are significantly higher than\
    \ in DRAM. Each store takes 380ns extra, in all cases of ACT-ST state. Furthermore,\
    \ STT-RAM is running at 1333 MHz, whereas DRAM is running at 2400 MHz. The impact\
    \ is especially pronounced in applications with high write-to-read ratios like\
    \ Ibm\\_s (see Fig. 6a). Before STT-RAM can be a feasible alternative to DRAM's\
    \ main memories, further technology, and architectural optimizations are necessary\
    \ to reduce the store latency and energy requirements. Fortunately, with the availability\
    \ of HOPE, we now have a systematic means to evaluate and optimize STT-RAM at\
    \ the system level. HOPE presents an invaluable opportunity to drive STT-RAM's\
    \ progress by allowing us to\n\n![](_page_9_Figure_2.jpeg)\n\nexplore and analyze\
    \ various architectural possibilities with greater precision.\n\n## **V. CONCLUSION**\n\
    \nWe presented an extension to the open-source full-system simulator gem5 for\
    \ enabling detailed evaluation of STT-RAM devices in an accurate manner. We have\
    \ shown our implemented power state machine, memory commands, power calculation,\
    \ and statistics output. We have also shown the results of an STT-RAM-based system\
    \ configured using real-world device parameters and compared the resulting metrics\
    \ to a DRAM-based system. The STT-RAM-specific characteristics of required store\
    \ operations and deviations to the DDR4 standard for DRAM have been discussed\
    \ based on the comparison of the simulation results. Our HOPE implementation is\
    \ easily configurable for other STT-RAM devices, by adding timing values, currents,\
    \ and voltages. We will open-source our HOPE framework to fuel research and accelerate\
    \ the development of future system architectures based on STT-RAM.\n\n## **VI.\
    \ ACKNOWLEDGMENT**\n\nThe authors acknowledge TU Wien Bibliothek for financial\
    \ support through its Open Access Funding Programme.\n\n#### **REFERENCES**\n\n\
    - [1] Y. Wang, C. Zhang, H. Yu, and W. Zhang, \"Design of low power 3d hybrid\
    \ memory by non-volatile cbram-crossbar with block-level data-retention,\" in\
    \ Proceedings of the 2012 ACM/IEEE international symposium on Low power electronics\
    \ and design, 2012, pp. 197–202.\n- [2] Y. Shin, \"Non-volatile memory technologies\
    \ for beyond 2010,\" in Digest of Technical Papers. 2005 Symposium on VLSI Circuits,\
    \ 2005. IEEE, 2005, pp. 156–159.\n- [3] B. Gervasi, \"Will carbon nanotube memory\
    \ replace dram?\" IEEE Micro, vol. 39, no. 2, pp. 45–51, 2019.\n- [4] J. Lamb,\
    \ S. Gibbons, R. Trichur, Y. Jiang, K. Mangelson, K. Kremer, and D. Janzen, \"\
    Advancements in microelectronics-grade carbon nanotube materials for nram® device\
    \ manufacture and analysis of carbon nanotube mass in end user devices.\"\n- [5]\
    \ \"Irds 2022 beyond cmos and emerging materials integration,\" Online, 2022,\
    \ accessed on: November 7, 2023. [Online]. Available: https://irds.ieee.org/editions/2022\n\
    - [6] D. Jana, S. Roy, R. Panja, M. Dutta, S. Z. Rahaman, R. Mahapatra, and S.\
    \ Maikap, \"Conductive-bridging random access memory: challenges and opportunity\
    \ for 3d architecture,\" Nanoscale research letters, vol. 10, pp. 1–23, 2015.\n\
    - [7] S. Mittal and J. S. Vetter, \"A survey of software techniques for using\
    \ non-volatile memories for storage and main memory systems,\" IEEE Transactions\
    \ on Parallel and Distributed Systems, vol. 27, no. 5, pp. 1537– 1550, 2015.\n\
    - [8] J. S. Meena, S. M. Sze, U. Chand, and T.-Y. Tseng, \"Overview of emerging\
    \ nonvolatile memory technologies,\" Nanoscale research letters, vol. 9, pp. 1–33,\
    \ 2014.\n- [9] T. Mikolajick, C. Dehm, W. Hartner, I. Kasko, M. Kastner, N. Nagel,\
    \ M. Moert, and C. Mazure, \"Feram technology for high density applications,\"\
    \ Microelectronics Reliability, vol. 41, no. 7, pp. 947–950, 2001.\n- [10] M.\
    \ Imani, S. Patil, and T. Rosing, \"Low power data-aware stt-ram based hybrid\
    \ cache architecture,\" in 2016 17th international symposium on quality electronic\
    \ design (isqed). IEEE, 2016, pp. 88–94.\n- [11] S. Jeloka, Z. Wang, R. Xie, S.\
    \ Khanna, S. Bartling, D. Sylvester, and D. Blaauw, \"Energy efficient adiabatic\
    \ fram with 0.99 pj/bit write for iot applications,\" in 2018 IEEE symposium on\
    \ VLSI circuits. IEEE, 2018, pp. 85–86.\n- [12] M. Moore et al., \"International\
    \ roadmap for devices and systems,\" Accessed: Jan, 2020.\n- [13] I. Yoon, A.\
    \ Anwar, T. Rakshit, and A. Raychowdhury, \"Transfer and online reinforcement\
    \ learning in stt-mram based embedded systems for autonomous drones,\" in 2019\
    \ Design, Automation & Test in Europe Conference & Exhibition (DATE). IEEE, 2019,\
    \ pp. 1489–1494.\n- [14] B. Narasimham, V. Chaudhary, M. Smith, L. Tsau, D. Ball,\
    \ and B. Bhuva, \"Scaling trends in the soft error rate of srams from planar to\
    \ 5-nm finfet,\" in 2021 IEEE International Reliability Physics Symposium (IRPS).\
    \ IEEE, 2021, pp. 1–5.\n- [15] J. Wang, N. Xiu, J. Wu, Y. Chen, Y. Sun, H. Yang,\
    \ V. Narayanan, S. George, and X. Li, \"An 8t/cell fefet-based nonvolatile sram\
    \ with improved density and sub-fj backup and restore energy,\" in 2022 IEEE International\
    \ Symposium on Circuits and Systems (ISCAS), 2022, pp. 3408–3412.\n- [16] J. Y.\
    \ Kim, M.-J. Choi, and H. W. Jang, \"Ferroelectric field effect transistors: Progress\
    \ and perspective,\" APL Materials, vol. 9, no. 2, p. 021102, 02 2021.\n- [17]\
    \ S. Yu, Q. Wang, Y. Zhang, P. Yang, X. Luo, H. Liu, C. Chen, Q. Li, and S. Liu,\
    \ \"Multistate capability improvement of beol compatible fefet by introducing\
    \ an al2o3 interlayer,\" IEEE Transactions on Electron Devices, vol. 70, no. 11,\
    \ pp. 5632–5637, 2023.\n- [18] J. Y. Park, D.-H. Choe, D. H. Lee, G. T. Yu, K.\
    \ Yang, S. H. Kim, G. H. Park, S.-G. Nam, H. J. Lee, S. Jo, B. J. Kuh, D. Ha,\
    \ Y. Kim, J. Heo, and M. H. Park, \"Revival of ferroelectric memories based on\
    \ emerging fluoritestructured ferroelectrics,\" Advanced Materials, vol. 35, no.\
    \ 43, p. 2204904, 2023.\n- [19] S. Seyedfaraji, J. T. Daryani, M. M. S. Aly, and\
    \ S. Rehman, \"Extent: Enabling approximation-oriented energy efficient stt-ram\
    \ write circuit,\" IEEE Access, vol. 10, pp. 82 144–82 155, 2022.\n- [20] S. M.\
    \ Nair, R. Bishnoi, A. Vijayan, and M. B. Tahoori, \"Dynamic faults based hardware\
    \ trojan design in stt-mram,\" in 2020 Design, Automation & Test in Europe Conference\
    \ & Exhibition (DATE). IEEE, 2020, pp. 933–938.\n- [21] R. Bishnoi, M. Ebrahimi,\
    \ F. Oboril, and M. B. Tahoori, \"Improving write performance for stt-mram,\"\
    \ IEEE Transactions on Magnetics, vol. 52, no. 8, pp. 1–11, 2016.\n- [22] S. Swami\
    \ and K. Mohanram, \"Reliable nonvolatile memories: Techniques and measures,\"\
    \ IEEE Design & Test, vol. 34, no. 3, pp. 31–41, 2017.\n- [23] S. Seyedfaraji,\
    \ A. M. Hajisadeghi, J. Talafy, and H. R. Zarandi, \"Dysco: Dynamic stepper current\
    \ injector to improve write performance in stt-ram memories,\" Microprocessors\
    \ and Microsystems, vol. 73, p. 102963, 2020.\n- [24] E. Garzon, R. De Rose, F.\
    \ Crupi, L. Trojman, G. Finocchio, M. Carpentieri, and M. Lanuzza, \"Assessment\
    \ of stt-mrams based on double-barrier mtjs for cache applications by means of\
    \ a device-to-system level simulation framework,\" Integration, vol. 71, pp. 56–69,\
    \ 2020.\n- [25] R. Saha, Y. P. Pundir, and P. K. Pal, \"Design of an area and\
    \ energyefficient last-level cache memory using stt-mram,\" Journal of Magnetism\
    \ and Magnetic Materials, vol. 529, p. 167882, 2021.\n- [26] E. Cheshmikhani,\
    \ H. Farbeh, and H. Asadi, \"3rset: Read disturbance rate reduction in stt-mram\
    \ caches by selective tag comparison,\" IEEE Transactions on Computers, vol. 71,\
    \ no. 6, pp. 1305–1319, 2021.\n- [27] ——, \"Robin: Incremental oblique interleaved\
    \ ecc for reliability improvement in stt-mram caches,\" in Proceedings of the\
    \ 24th Asia and South Pacific Design Automation Conference, 2019, pp. 173–178.\n\
    - [28] N. Mahdavi, F. Razaghian, and H. Farbeh, \"Data block manipulation for\
    \ error rate reduction in stt-mram based main memory,\" The Journal of Supercomputing,\
    \ vol. 78, no. 11, pp. 13 342–13 372, 2022.\n- [29] E. Kültürsay, M. Kandemir,\
    \ A. Sivasubramaniam, and O. Mutlu, \"Evaluating stt-ram as an energy-efficient\
    \ main memory alternative,\" in 2013 IEEE International Symposium on Performance\
    \ Analysis of Systems and Software (ISPASS), 2013, pp. 256–267.\n- [30] D. Sanchez\
    \ and C. Kozyrakis, \"Zsim: Fast and accurate microarchitectural simulation of\
    \ thousand-core systems,\" ACM SIGARCH Computer architecture news, vol. 41, no.\
    \ 3, pp. 475–486, 2013.\n- [31] N. Binkert, B. Beckmann, G. Black, S. K. Reinhardt,\
    \ A. Saidi, A. Basu, J. Hestness, D. R. Hower, T. Krishna, S. Sardashti et al.,\
    \ \"The gem5 simulator,\" ACM SIGARCH computer architecture news, vol. 39, no.\
    \ 2, pp. 1–7, 2011.\n- [32] A. Hansson, N. Agarwal, A. Kolli, T. Wenisch, and\
    \ A. N. Udipi, \"Simulating dram controllers for future system architecture exploration,\"\
    \ in 2014 IEEE International Symposium on Performance Analysis of Systems and\
    \ Software (ISPASS). IEEE, 2014, pp. 201–210.\n- [33] A. Gebregiorgis, L. Wu,\
    \ C. Münch, S. Rao, M. B. Tahoori, and S. Hamdioui, \"Special session: Stt-mrams:\
    \ Technology, design and test,\" in 2022 IEEE 40th VLSI Test Symposium (VTS).\
    \ IEEE, 2022, pp. 1–10.\n- [34] X. Dong, C. Xu, Y. Xie, and N. P. Jouppi, \"Nvsim:\
    \ A circuit-level performance, energy, and area model for emerging nonvolatile\
    \ memory,\" IEEE Transactions on Computer-Aided Design of Integrated Circuits\
    \ and Systems, vol. 31, no. 7, pp. 994–1007, 2012.\n- [35] M. Poremba and Y. Xie,\
    \ \"Nvmain: An architectural-level main memory simulator for emerging non-volatile\
    \ memories,\" in 2012 IEEE Computer Society Annual Symposium on VLSI. IEEE, 2012,\
    \ pp. 392–397.\n- [36] M. Poremba, T. Zhang, and Y. Xie, \"Nvmain 2.0: A user-friendly\
    \ memory simulator to model (non-) volatile memory systems,\" IEEE Computer Architecture\
    \ Letters, vol. 14, no. 2, pp. 140–143, 2015.\n- [37] J. L. NMeza, Justin and\
    \ O. Mutlu., \"Evaluating row buffer locality in future non-volatile main memories,\"\
    \ in arXiv preprint arXiv:1812.06377. arXiv, 2018.\n- [38] S. Chung, K.-M. Rho,\
    \ S.-D. Kim, H.-J. Suh, D.-J. Kim, H.-J. Kim, S.-H. Lee, J.-H. Park, H.-M. Hwang,\
    \ S.-M. Hwang, J.-Y. Lee, Y.-B. An, J.-U. Yi, Y.-H. Seo, D.-H. Jung, M.-S. Lee,\
    \ S.-H. Cho, J.-N. Kim, G.-J. Park, G. Jin, A. Driskill-Smith, V. Nikitin, A.\
    \ Ong, X. Tang, Y. Kim, J.-S. Rho, S.-K. Park, S.-W. Chung, J.-G. Jeong, and S.-J.\
    \ Hong, \"Fully integrated 54nm stt-ram with the smallest bit cell dimension for\
    \ high density memory pplication,\" in 2010 International Electron Devices Meeting,\
    \ 2010, pp. 12.7.1–12.7.4.\n- [39] \"1 gb non-volatile st-ddr4 spin-transfer torque\
    \ mram.\" [Online]. Available: https://shorturl.at/bgsMR\n- [40] K. Chandrasekar,\
    \ C. Weis, Y. Li, B. Akesson, N. Wehn, and K. Goossens, \"Drampower: Open-source\
    \ dram power & energy estimation tool,\" [URL:http://www.](http://www/) drampower.\
    \ info, vol. 22, 2012.\n- [41] A. Brokalakis, N. Tampouratzis, A. Nikitakis, I.\
    \ Papaefstathiou, S. Andrianakis, D. Pau, E. Plebani, M. Paracchini, M. Marcon,\
    \ I. Sourdis, P. R. Geethakumari, M. C. Palacios, M. A. Anton, and A. Szasz, \"\
    Cossim: An open-source integrated solution to address the simulator gap for systems\
    \ of systems,\" in 2018 21st Euromicro Conference on Digital System Design (DSD),\
    \ 2018, pp. 115–120.\n- [42] [Online]. Available: [https://www.S](http://www.spec.org/cpu2017)PEC.o[rg/cpu2017](http://www.spec.org/cpu2017)\n\
    \n![](_page_11_Picture_11.jpeg)\n\nSAEED SEYEDFARAJI is a Graduate Student Member,\
    \ IEEE, and holds a B.Sc. degree from Isfahan University of Technology in Isfahan,\
    \ Iran, and an M.Sc. degree from Amirkabir University of Technology (Tehran Polytechnique)\
    \ in Tehran, Iran. Currently, he is pursuing a Ph.D. in computer engineering at\
    \ the Technische Universität Wien (TU Wien), Austria, where he also serves as\
    \ a University Assistant. His research interests encompass emerging non-volatile\
    \ memory\n\ntechnologies, in-memory processing, the integration of intelligence\
    \ into hardware, and system-on-chip design. Notably, he received the Design Automation\
    \ Conference 2020 Young Fellow (DAC YF 2020) Prize and was a part of the Best\
    \ Team at DAC YF 2020.\n\n![](_page_11_Picture_14.jpeg)\n\nMARKUS BICHL is currently\
    \ with the Technische Universität Wien (TU Wien), Faculty of Electrical Engineering\
    \ and Information Technology (ETIT) as a student. He started his studies with\
    \ Technische Universität Wien (TU Wien), Faculty of Informatics, Bachelore's Programme\
    \ of Computer Science in 2016. He is pursuing his Master's degree in Electrical\
    \ Engineering in the Master's Programme Embedded Systems. His main research interests\
    \ include Emerging\n\nMemory Technologies, low-power computing, FPGA development,\
    \ ASIC design, and cyber-physical systems. Besides his studies, he is working\
    \ on industry-leading electrical powertrains for the automotive industry, with\
    \ hundreds of thousands of units already produced. His passion is to work further\
    \ on Embedded Systems topics and gain a professional career in research.\n\n![](_page_11_Picture_17.jpeg)\n\
    \nASAD AFTAB is a Graduate Student Member, IEEE, and received the B.S. degree\
    \ in Computer Systems engineering from University of Engineering and Technology\
    \ (UET), Peshawar, in 2017 and the M.S. degree in Electrical (Telecommunication\
    \ and Computer Networks) engineering from the National University of Sciences\
    \ and Technology (NUST), Islamabad, in 2021. He is currently pursuing a Ph.D.\
    \ in Electrical engineering at the Technische Universität Wien (TU\n\nWien), Austria.\
    \ His research interests encompass designing both hardware and software-based\
    \ sustainable security techniques for autonomous CPS, which includes researching\
    \ suitable ML algorithms for defence, analyzing various adversarial attacks, and\
    \ exploring innovative defence methods to enhance the resilience of machine learning\
    \ algorithms.\n\n![](_page_11_Picture_20.jpeg)\n\nSEMEEN REHMAN is currently with\
    \ the Technische Universität Wien (TU Wien), as an Assistant Professor. In October\
    \ 2020, she received her Habilitation Degree in the area of Embedded Systems from\
    \ the Faculty of Electrical Engineering and Information Technology, TU Wien. She\
    \ has co-authored one book, multiple book chapters, and more than 60+ publications\
    \ in premier journals and conferences. Her main research interests include dependable\
    \ systems and energy-efficient\n\nembedded system, approximate computing, security,\
    \ IoT/CPS. She has received the CODES+ISSS 2011 and 2015 Best Paper Awards, DATE\
    \ 2017 Best Paper Award Nomination, HiPEAC Paper Awards, DAC Richard Newton Young\
    \ Student Fellow Award, and Research Student Award at the KIT. She served as the\
    \ Topic Track Chair and co-chair at the DATE and ISVLSI conferences from 2020\
    \ and 2023, and has served as the TPC of multiple premier conferences on design\
    \ automation and embedded systems."
  decisions:
    evaluation_prompt: 'Qualified. Reason: The paper includes a table comparing various
      memory technologies, indicating quantitative analysis.'
    related_work_prompt: 'Qualified. Reason: The paper cites numerous research papers,
      explains and compares to cited work, and includes a section discussing the need
      for STT-RAM based system evaluation frameworks, engaging with prior research
      meaningfully.'
    novelty_prompt: 'Qualified. Reason: The paper proposes a novel, extendable STT-RAM
      memory controller design integrated inside the gem5 simulator, which is a new
      method and framework.'
    review_only_prompt: Qualified.
- title: Efficient yet Accurate End-to-End SC Accelerator Design
  abstract: 'Providing end-to-end stochastic computing (SC) neural network acceleration

    for state-of-the-art (SOTA) models has become an increasingly challenging task,

    requiring the pursuit of accuracy while maintaining efficiency. It also

    necessitates flexible support for different types and sizes of operations in

    models by end-to-end SC circuits. In this paper, we summarize our recent

    research on end-to-end SC neural network acceleration. We introduce an accurate

    end-to-end SC accelerator based on a deterministic coding and sorting network.

    In addition, we propose an SC-friendly model that combines low-precision data

    paths with high-precision residuals. We introduce approximate computing

    techniques to optimize SC nonlinear adders and provide some new SC designs for

    arithmetic operations required by SOTA models. Overall, our approach allows for

    further significant improvements in circuit efficiency, flexibility, and

    compatibility through circuit design and model co-optimization. The results

    demonstrate that the proposed end-to-end SC architecture achieves accurate and

    efficient neural network acceleration while flexibly accommodating model

    requirements, showcasing the potential of SC in neural network acceleration.'
  url: http://arxiv.org/abs/2401.15332v1
  keywords: ''
  document: "### I. INTRODUCTION\n\nStochastic computing (SC) has emerged as a promising\
    \ alternative to traditional binary computing, offering simplified arithmetic\
    \ operations and improved error resilience [\\[1\\]](#page-4-0)–[\\[5\\]](#page-4-1).\
    \ Both hybrid and end-to-end SC-based neural accelerators have been proposed [\\\
    [1\\]](#page-4-0)–[\\[5\\]](#page-4-1). While hybrid accelerators involve back-and-forth\
    \ conversion between binary and SC representations, leading to high power consumption\
    \ and area overhead, end-to-end SC-based accelerators demonstrate superior power,\
    \ area efficiency, and fault tolerance [\\[3\\]](#page-4-2)–[\\[5\\]](#page-4-1).\
    \ In this context, our research aims to further enhance the capabilities of endto-end\
    \ SC-based accelerators.\n\nExisting SC-based accelerators primarily focus on\
    \ multiplication, accumulation, and activation functions in convolutional networks\
    \ [\\[6\\]](#page-4-3)–[\\[9\\]](#page-4-4). However, these approaches have limitations.\
    \ FSM-based activation modules suffer from accuracy issues, particularly for ReLU\
    \ with larger accumulation widths (Figure [1\\)](#page-0-0). Furthermore, there\
    \ exists a trade-off between inference efficiency and accuracy (Figure [2\\)](#page-0-1),\
    \ where high precision computing enhances accuracy but exponentially increases\
    \ costs, while low precision computing compromises accuracy. Additionally, there\
    \ is a lack of research on SC circuits supporting functions like batch normalization\
    \ (BN), residual connections, gaussian error linear unit (GELU), and softmax for\
    \ state-of-the-art (SOTA) models.\n\nTherefore, in this paper, we will summarize\
    \ our recent efforts on end-to-end SC-based NN accelerators that address these\
    \ limitations to meet the requirements in terms of accuracy, efficiency, flexibility,\
    \ and compatibility, as shown in Table [I.](#page-0-2)\n\n## <span id=\"page-0-3\"\
    ></span>II. ACCURATE END-TO-END SC ACCELERATOR BASED ON DETERMINISTIC THERMOMETER\
    \ CODING\n\nIn this section, we introduce deterministic coding called thermometer\
    \ coding and the corresponding end-to-end SC accelerator design. The proposed\
    \ design achieves exact end-to-end SC NN acceleration.\n\nThis work was supported\
    \ in part by the National Key R&D Program of China (2020YFB2205502), NSFC (62125401)\
    \ and the 111 Project (B18001).\n\n<sup>∗</sup>Corresponding author: {meng.li,\
    \ r.wang}@pku.edu.cn\n\n![](_page_0_Figure_14.jpeg)\n\n<span id=\"page-0-0\"></span>Fig.\
    \ 1. FSM-based design to implement (a) tanh and (b) ReLU. Ideally, the circuit\
    \ output is the same as the exact output, marked by the red line.\n\n<span id=\"\
    page-0-1\"></span>![](_page_0_Figure_16.jpeg)\n\nFig. 2. The trade-off between\
    \ inference accuracy and efficiency (measured by area-delay product, i.e., ADP).\
    \ Here, we fix the weight BSL to 2-bit and sweep the activation BSL.\n\n<span\
    \ id=\"page-0-2\"></span>TABLE I COMPARISON OF DIFFERENT END-TO-END SC ACCELERATORS.\n\
    \n| Design            |      | Accuracy Efficiency | Flexibility             \
    \             | *Compatibility |\n|-------------------|------|---------------------|--------------------------------------|----------------|\n\
    | FSM-based [6]–[9] | Low  | Low                 | Limited for large Conv    \
    \           | Basic CNNs     |\n| Ours [3]–[5]      | High | Low             \
    \    | Limited for variable Conv Basic CNNs |                |\n| Ours [10], [11]\
    \   | High | High                | Flexible                             | DNNs\
    \           |\n| Ours [12]         | High | High                | Flexible   \
    \                          | DNNs+ViT       |\n\n\\*Basic CNNs contain convolution\
    \ and ReLU. DNNs further require residual connection and BN. And transformer models\
    \ further require GeLU and softmax.\n\n### *A. Motivation*\n\nWe refer to the\
    \ accumulation and activation module as the SC non-linear adder. Typical SC Non-linear\
    \ adders employ stochastic coding with FSM to implement different activation functions\
    \ [\\[6\\]](#page-4-3)– [\\[9\\]](#page-4-4). FSM-based designs serially process\
    \ stochastic bitstream inputs, which results in inaccurate outputs (Figure [1\\\
    )](#page-0-0) that do not utilize all of the information in the inputs and have\
    \ random fluctuations in the inputs themselves. Thus, very long bitstreams, e.g.,\
    \ 1024 bits, are used for accuracy and lead to an unacceptable latency, which\
    \ severely affects the hardware efficiency.\n\n### <span id=\"page-0-4\"></span>*B.\
    \ Accurate End-to-End SC Acceleration with Sorting Network*\n\nIn our work, we\
    \ employ the deterministic thermometer coding scheme (Table [II\\)](#page-1-0)\
    \ and the corresponding accurate SC circuit designs to achieve accurate end-to-end\
    \ SC acceleration. With thermometer coding, all the 1s appear at the beginning\
    \ of the bitstream and\n\n![](_page_1_Figure_0.jpeg)\n\n<span id=\"page-1-1\"\
    ></span>Fig. 3. (a) The truth table and circuit of ternary SC multiplier. (b)\
    \ The BSN and the selective interconnect system for accumulation and activation\
    \ function.\n\n<span id=\"page-1-0\"></span>TABLE II THE CORRESPONDING BINARY\
    \ PRECISION AND THE REPRESENTED RANGE FOR THERMOMETER CODING OF DIFFERENT BSL.\n\
    \n| BSL | Binary<br>Precision | Range             | Thermometer Coding       \
    \                                                      |\n|-----|---------------------|-------------------|--------------------------------------------------------------------------------|\n\
    | 2   | -                   | -1, 0, 1          | 00, 10, 11                 \
    \                                                    |\n| 4   | 2            \
    \       | -2, -1, 0, 1, 2   | 0000, 1000, 1100, 1110, 1111                   \
    \                                |\n| 8   | 3                   | -4, -3 · · ·\
    \ 3, 4 | 00000000, 10000000 · · · 11111110, 11111111                         \
    \           |\n| 16  | 4                   | -8, -7 · · · 7, 8 | 0000000000000000,\
    \ 1000000000000000 · · ·<br>1111111111111110, 1111111111111111 |\n\na value x\
    \ is represented with a L-bit sequence as x = αx<sup>q</sup> = α( P<sup>L</sup>−<sup>1</sup>\
    \ <sup>i</sup>=0 x[i] − L 2 ), where x<sup>q</sup> = P<sup>L</sup>−<sup>1</sup>\
    \ <sup>i</sup>=0 x[i] − L 2 is the quantized value of range [− L , L ] and α is\
    \ a scaling factor obtained by training.\n\n2 2 Deterministic coding, in contrast\
    \ to stochastic coding, achieves hardware-efficient and accurate computations\
    \ with shorter bitstreams. By employing a 2-bit ternary bitstream, we can realize\
    \ multiplication with only 5 gates using a deterministic multiplier (Figure [3\\\
    (](#page-1-1)a)).\n\nTo achieve accurate accumulation and activation functions\
    \ simultaneously, we employ the bitonic sorting network (BSN). BSN is a parallel\
    \ sorting network that sorts inputs in thermometer coding, ensuring the output\
    \ is also in thermometer coding. The sorting process, performed by comparators\
    \ constructed with AND and OR gates, follows Batcher's bitonic sorting algorithm\
    \ [\\[13\\]](#page-4-8) (Figure [3\\(](#page-1-1)b)). The number of 1's in the\
    \ sorted bitstream output from BSN corresponds to the sum of 1's in all input\
    \ bitstreams, effectively representing the accumulation result.\n\nBy sorting\
    \ all the bits, the inputs and outputs of the selective interconnect (SI) [\\\
    [14\\]](#page-4-9) are deterministic. Therefore, when the SI selects different\
    \ bits from the BSN directly as outputs based on the selection signals, a deterministic\
    \ input-output correspondence is generated and different activation functions\
    \ are realized. The example in Figure [3\\(](#page-1-1)b) implements the two-step\
    \ activation function shown at the bottom when the SI selects the 3rd and 6th\
    \ bits of the BSN as outputs. We refer interested readers to [\\[3\\]](#page-4-2),\
    \ [\\[4\\]](#page-4-10) for more details.\n\n### *C. Experimental Results*\n\n\
    We prototype the proposed SC accelerator with a 28-nm CMOS process. The chip's\
    \ measured current consumption and energy efficiency in Figure [4](#page-1-2)\
    \ show a peak of 198.9 TOPS/W at 200 MHz and 650 mV. Compared to state-of-the-art\
    \ binary-based NN processors [\\[15\\]](#page-4-11)–[\\[19\\]](#page-4-12), the\
    \ fabricated SC-based NN processor achieves an average\n\n![](_page_1_Figure_10.jpeg)\n\
    \n<span id=\"page-1-2\"></span>Fig. 4. (a) Current and (b) energy efficiency versus\
    \ supply voltage at different working frequencies.\n\n![](_page_1_Figure_12.jpeg)\n\
    \n<span id=\"page-1-3\"></span>Fig. 5. Accuracy loss of the conventional binary\
    \ design and proposed SC design versus bit error rate, at the soft accuracy of\
    \ 98.28%.\n\nenergy efficiency improvement of 10.75× (1.16× ∼ 17.30×). And the\
    \ area efficiency improves by 4.20× (2.09× ∼ 6.76×). We also compare the accuracy\
    \ under varying bit error rates (BER) using a ternary neural network that achieves\
    \ 98.28% accuracy on the MNIST dataset, as shown in Figure [5.](#page-1-3) The\
    \ proposed SC design demonstrates significant fault tolerance, as the average\
    \ reduction of accuracy loss by 70%. It is the first silicon-proven end-to-end\
    \ SC accelerator, to the best of the authors' knowledge.\n\n## III. ACCURATE YET\
    \ EFFICIENT SC WITH HIGH PRECISION RESIDUAL FUSION\n\nThe SC accelerator above\
    \ validated the effectiveness of deterministic thermometer coding and the corresponding\
    \ SC design on the basic small model (MNIST). In this section, we propose SC-friendly\
    \ models as well as new SC circuit blocks to support SOTA model requirements and\
    \ greatly improve the accuracy of the SC accelerators.\n\n### *A. Motivation*\n\
    \nThe SC TNN accelerator in Section [II](#page-0-3) lacks support for batch normalization\
    \ (BN) and residual connections, limiting its accuracy on complex datasets like\
    \ CIFAR10 or CIFAR100. Increasing precision can enhance accuracy but compromises\
    \ hardware efficiency. Figure [2](#page-0-1) demonstrates that increasing BSL\
    \ from 2 to 8 bits improves accuracy at the expense of a 3 to 10 times efficiency\
    \ overhead. Accurate yet efficient SC acceleration is very challenging.\n\n###\
    \ *B. SC-Friendly Low Precision Network*\n\nTo understand the origin of the accuracy\
    \ degradation, we quantize the network weight and activation to low precision\
    \ separately. Table [III](#page-2-0) shows similar accuracy between low precision\
    \ weight quantization and the floating point baseline, while 2b BSL activation\
    \ quantization results in a 10% accuracy drop. Hence, low precision activation\
    \ is the root cause of the accuracy loss due to its limited representation capacity.\
    \ After quantization, the range of activations is\n\n<span id=\"page-2-0\"></span>TABLE\
    \ III NETWORK ACCURACY COMPARISON OF DIFFERENT QUANTIZED NETWORKS ON CIFAR10.\n\
    \n| Network              | Weight/BSL | Act/BSL | Top-1 Accuracy (%) |\n|----------------------|------------|---------|--------------------|\n\
    | baseline             | FP         | FP      | 94.27              |\n| weight\
    \ quantized     | 2          | FP      | 93.98              |\n| activation quantized\
    \ | FP         | 2       | 84.18              |\n| fully quantized      | 2  \
    \        | 2       | 83.51              |\n|                      |          \
    \  |         |                    |\n\n![](_page_2_Figure_2.jpeg)\n\n<span id=\"\
    page-2-1\"></span>Fig. 6. High precision residual helps to achieve better representation\
    \ capability.\n\nreduced to {−1, 0, +1} for 2b BSL encoding, significantly reducing\
    \ the number of possible configurations.\n\nAs a remedy, we add the high-precision\
    \ activation input through residual connections to the result of the low-precision\
    \ convolution (Figure [6\\)](#page-2-1). By increasing the activation range to\
    \ {−8, −7, . . . , 7, 8}, we enhance representation capacity to 17<sup>H</sup>×W×<sup>C</sup>\
    \ . This significantly improves inference accuracy while maintaining efficiency\
    \ by preserving energy-efficient convolution computation.\n\n$$\\text{ReLU}(\\\
    text{BN}(x)) = \\begin{cases} \\gamma(x-\\beta) & x \\ge \\beta \\\\ 0 & x < \\\
    beta \\end{cases} \\tag{1}$$\n\nBesides the high precision residual, another remaining\
    \ question is how to efficiently process BN. And BN(x) = γ(x − β), where γ and\
    \ β are trainable parameters. We propose to fuse BN with the ReLU activation function\
    \ as Equation [1.](#page-2-2) Consequently, we achieve an SC-friendly low precision\
    \ model with high precision residual fusion depicted in Figure [6\\(](#page-2-1)b).\n\
    \n### *C. End-to-End SC Accelerator with High Precision Residual*\n\nCompared\
    \ to the proposed accelerator in Section [II-B,](#page-0-4) the model in Figure\
    \ [6\\(](#page-2-1)b) further requires the implementation of SC circuits for BN\
    \ fusion and residual connection.\n\nThe above fused BN and ReLU function can\
    \ be efficiently and accurately processed in SC, leveraging the selective interconnect\
    \ described in Section [II-B.](#page-0-4) Figure [7](#page-2-3) demonstrates how\
    \ different BN parameters affect the objective function of the SI.\n\n![](_page_2_Figure_11.jpeg)\n\
    \n<span id=\"page-2-3\"></span>Fig. 7. BN-fused activation function with 16b BSL\
    \ output. The blue dots are\n\n![](_page_2_Figure_13.jpeg)\n\n<span id=\"page-2-4\"\
    ></span>Fig. 8. (a) The proposed model optimization helps to achieve much better\
    \ inference accuracy; (b) 16b BSL residual achieves 5.78% accuracy improvement,\
    \ almost the same as floating point residual.\n\n<span id=\"page-2-5\"></span>TABLE\
    \ IV INFERENCE EFFICIENCY AND ACCURACY COMPARISON.\n\n| W-A-R/BSL | Area (um²)\
    \ | ADP (um²·us) | Accuracy (%) |\n|-----------|------------|--------------|--------------|\n\
    | 2-2-2     | 4349.7     | 225.36       | 82.58        |\n| 2-4-4     | 10683.3\
    \    | 687.47       | 92.35        |\n| 2-2-16    | 4406.9     | 228.32      \
    \ | 92.01        |\n\nFor the accumulation of residual and multiplication products,\
    \ the different scaling factors α of residual and convolution results can lead\
    \ to errors in the accumulation operation. The residual re-scaling block is proposed\
    \ to align the α before accumulation. In the re-scaling block, we multiply or\
    \ divide the residual by a factor of 2 <sup>N</sup> (where N is an integer). To\
    \ multiply the residual by 2 <sup>N</sup> , we replicate it 2 N times in the buffer.\
    \ For division by 2 <sup>N</sup> , we select 1 out of 2 bits of the residual per\
    \ cycle and generate the final result after N cycles. To maintain a constant BSL\
    \ for the residual, we append 8 bits of '11110000' (equal to 0) per division cycle.\n\
    \n### <span id=\"page-2-2\"></span>*D. Experimental Results*\n\nFigure [8](#page-2-4)\
    \ demonstrate significant improvement in network accuracy. With the high precision\
    \ residual, network accuracy is improved significantly by 8.69% and 8.12% for\
    \ low precision ResNet18 on CIFAR10 and CIFAR100, respectively. Combined with\
    \ the novel training techniques, network accuracy can be improved in total by\
    \ 9.43% and 15.42%. Compared to baseline accelerators, it achieves a 9.4% accuracy\
    \ improvement with only a 1.3% efficiency overhead compared to the efficient baseline\
    \ and achieves a 3× efficiency improvement with comparable accuracy to the accurate\
    \ baseline design, as shown in Table [IV.](#page-2-5) In this way, the proposed\
    \ method achieves accurate yet efficient SC acceleration.\n\n## IV. FLEXIBLE AND\
    \ EFFICIENT SC ACCELERATOR WITH APPROXIMATE SPATIAL-TEMPORAL BSN\n\nIn this section,\
    \ we greatly improve the flexibility and hardware efficiency of the SC accelerator\
    \ by compressing the BSN.\n\n### *A. Motivation*\n\nBSN accumulates all the input\
    \ in parallel through sorting, so as to generate an accurate output based on all\
    \ the information input. However, it also forces the hardware cost to increase\
    \ super linearly with the accumulation widths (Figure [9\\(](#page-3-0)a)). And\
    \ the BSN has to support the largest accumulation widths among all layers. The\
    \ large BSN, however, leads to very high hardware redundancy at shallow layers\
    \ where the accumulation is always small (Figure [9\\(](#page-3-0)b)). This makes\
    \ our previous design still inefficient for SOTA models.\n\n![](_page_3_Figure_0.jpeg)\n\
    \n**Motivation**\n\n**Why End-to-End SC?**\n\n70.0 75.7 81.2\n\nAxPC +FSM\n\n\
    • Stochastic encoding\n\nMUX +FSM\n\n**FSM-based Non-Linear Adder**\n\n**SI-based\
    \ Non-Linear Adder**\n\n• Deterministic thermometer coding: all 1s appearing at\
    \ the beginning of bitstream ൌ = ሺ∑ ሾሿ ିଵ\n\n• **BSN** accumulates all the input\
    \ bitstream simultaneously through **sorting** • **SI** enables accurate and flexible\n\
    \nୀ െ /2)\n\n**sorted**\n\n• Limited accuracy: random fluctuation\n\nArea Breakdown\n\
    \nAPC +FSM\n\n22.2 15.1 7.9\n\n7.8 9.1 10.9 7.6\n\n• Removing the conversion logic\
    \ • Higher energy/area efficiency • More tolerable to the soft errors\n\n**Challenge:\
    \ Non-Linear Addition** • Non-linear Add = accum + activation • High hardware\
    \ cost: ~90% area\n\n20.4\n\nBSN +SI\n\n**Previous Works**\n\n72.0\n\nMultiplication\
    \ Accumulation Activation\n\nArea Breakdown\n\n17.0 20.4 30.9\n\n64 128 256 Accumulation\
    \ width\n\n73.0 72.0 63.7\n\n10.0 7.6 5.4\n\n<span id=\"page-3-0\"></span>Fig.\
    \ 9. The inefficiency of the BSN design: (a) BSN hardware cost increases significantly\
    \ with the accumulation widths; (b) ADP overhead using a large BSN for small accumulation\
    \ widths.\n\n![](_page_3_Figure_2.jpeg)\n\n<span id=\"page-3-1\"></span>Fig. 10.\
    \ (a) Reducing BSN output BSL has little effect on the accuracy of SI; (b) Parameterized\
    \ BSN design space.\n\n### *B. Approximate Spatial-Temporal Sorting Network*\n\
    \nTo address the inefficiency and inflexibility of BSN, we find a significant\
    \ precision gap between the input and output of SI, as revealed in Figure [6\\\
    (](#page-2-1)b), making the high precision SC input redundant. We reduce the BSN\
    \ output BSL, resulting in a small accuracy loss for the tanh function and negligible\
    \ impact on the ReLU function, as shown in Figure [10\\(](#page-3-1)a).\n\nTo\
    \ further reduce hardware cost, we adopt a progressive sorting and sub-sampling\
    \ approach for the BSN. Figure [10\\(](#page-3-1)b) presents a parameterized BSN\
    \ design space that determines the location, number of sampling times, and method\
    \ of sampling. The parameterized BSN consists of N stages and in the ith stage,\
    \ there are m<sup>i</sup> sub-BSN modules, each taking an input bitstream of li-bit\
    \ BSL. Within each sub-BSN, there is a sub-sampling block that implements truncated\
    \ quantization. It clips out c<sup>i</sup> bits on each end of the BSN while sampling\
    \ 1 bit every s<sup>i</sup> bit from the remaining. Considering the input distribution\
    \ resembles a Gaussian distribution with a small variance due to inputs from a\
    \ large number of multipliers, significant clipping can be performed with negligible\
    \ errors, as illustrated in Figure [11.](#page-3-2)\n\nThanks to the fact that\
    \ the output BSL of the approximate BSN is much shorter than the input, we can\
    \ further fold the accumulation temporally to achieve more flexibility. In this\
    \ case, as shown in Figure [12,](#page-3-3) a large BSN is implemented by multi-cycle\
    \ reuse of a single small BSN circuit. In the proposed spatial-temporal BSN architecture,\
    \ the approximation level of BSN, i.e., the BSL of partial sums, and its corresponding\
    \ reuse can be controlled through control signals. This allows for flexible handling\
    \ of various accumulation widths with different approximate configurations.\n\n\
    ### *C. Experimental Results*\n\nFor the largest convolution in the ResNet18,\
    \ the two proposed approximate BSN reduced the ADP of BSN by 2.8× and 4.1× compared\
    \ to the baseline, as shown in Table [V.](#page-3-4) When handling the four different\
    \ sizes of convolutions in ResNet18, the spatial-temporal\n\n![](_page_3_Figure_10.jpeg)\n\
    \n **Clipping** some bits from both ends of the bitstream with **truncation error\
    \ Scaling** by taking 1bit for every bits with\n\nPartial sums are **distributed\
    \ around 0:** we can\n\n**Yixuan Hu1**, Tengyu Zhang1, Meng Li\\*213, Renjie Wei1,\
    \ Liangzhen Lai5, Yuan Wang13, Runsheng Wang134, Ru Huang134 *<sup>1</sup>School\
    \ of Integrated Circuits, <sup>2</sup>Institute for Artificial Intelligence &\
    \ <sup>4</sup>Institute of Electronic Design Automation, Peking University*\n\n\
    **Observations of BSN**\n\n• **Hardware cost increase super linearly**  with the\
    \ input size (Accumulation width) • Using a single large BSN for different layers\n\
    \nincurs **very high redundancy**\n\n• Exists a **huge precision redundancy**\
    \  between the input and output for SI\n\nHardware redundancy\n\nAccumulation\
    \ width\n\nSub-sampling BSN output as SI input ReLU: sampling has almost **no\
    \ effect** on accuracy\n\n576 1152 2304 4608 <sup>0</sup>\n\n**Spatial-Temporal\
    \ Approximate BSN Architecture**\n\n• Flexible approximation and reuse to **support**\
    \ \n\n**Experimental Results**\n\n• Find 47 pareto optimal in total 3362 design\
    \ • **Flexible** choose suitable configuration\n\n> Total 3362 BSN 128 input non-linear\
    \ adder for ReLU\n\n• Reduce the ADP by 1.9x/1.9x/2.5x for tanh/ sigmoid/ReLU\
    \ with negligible errors\n\n• Spatial / spatial-temporal approximate BSN reduce\
    \ the ADP by 2.8x and **4.1x**\n\nLargest convolution (3x3x512) in ResNet18\n\n\
    **Large Convolution Comparison**\n\n**Neural Network Task Evaluation** • Four\
    \ scaled layers in ResNet18: spatialtemporal approximate BSN reduce the ADP\n\n\
    by **4.1~23.3x (average 8.47x)** • Also reduce the area of datapath by 2.2x\n\n\
    *We propose much more efficient and flexible BSN‐based non‐linear adder design\
    \ for end‐to‐end SC neural network acceleration*\n\n47 pareto optimal\n\n1x103\
    \ 1x104 1x105 1x106\n\n APC+FSM MUX+FSM Baseline BSN+SI Design space Pareto front\
    \ ADP / (um²ꞏns)\n\nCycle9 Final BSN reuse\n\n8 \\* 72b in 8 cycles 576b\n\n576b\
    \ Appr. BSN Cycle2 reuse Cycle8 reuse 72b 72b 72b\n\nSpatial-Temporal 4608to256\
    \ Accumulation in 9 cycles\n\n**256b** Reused 576b BSN\n\n……\n\n**different accumulation\
    \ widths**\n\n**Opportunity from approximation** • BSN output has fewer bits than\
    \ input • Use much smaller BSNs for later stages\n\n**Spatial-Temporal BSN Design**\
    \ • **Fold the accumulation** temporally by reusing a single BSN over **multiple\
    \ cycles**\n\n576b\n\n576b 576b\n\n**4608b**\n\n**Design Space Exploration**\n\
    \n1x10-4\n\n1x10-3\n\nComputation error (MSE)\n\n1x10-2\n\n**Approximate Spatial-Temporal\
    \ Sorting Network**\n\n**Efficient Non-Linear Adder for Stochastic Computing with**\
    \ \n\n*<sup>3</sup>Beijing Advanced Innovation Center for Integrated Circuits*\
    \  \\*meng.li@pku.edu.cn *<sup>5</sup>Meta*\n\nADP / (um²ꞏns)\n\n> 0.03 0.04 0.05\
    \ 0.06 0.07\n\nComputing error\n\n**Why BSN Ineffective?** • Completely **parallel**\
    \ sorting\n\nAccumulation width\n\n1 1/2 1/4 1/6 1/8\n\nbitstreams to **reduce\
    \ costs**\n\n• Sub-sample methods trade off\n\nclip a lot with negligible errors\n\
    \n**quantization error**\n\n**Parameterized Approximation**\n\n• Approximate BSN:\
    \ sorts and sub-sample the\n\n• Root: BSN recursively sorts all input\n\nSub-sample\
    \ ratio\n\n**Sub-Sample the BSN**\n\n576 1152 2304 4608 <sup>215</sup>\n\n tanh\
    \ ReLU\n\n<span id=\"page-3-2\"></span>• Number of sub-samples? • Where to sub-sample?\
    \ Fig. 11. Input distribution of the intermediate sub-sampling blocks in different\
    \ stages of the BSN provides an opportunity to reduce the BSN via clipping.\n\n\
    ![](_page_3_Figure_12.jpeg)\n\n<span id=\"page-3-3\"></span>Fig. 12. Spatial-temporal\
    \ BSN architecture with an example: a 576-bit BSN is reused over 9 cycles for\
    \ 4608b accumulation.\n\n<span id=\"page-3-4\"></span>TABLE V PERFORMANCE OF DIFFERENT\
    \ DESIGNS FOR A 3X3X512 CONVOLUTION.\n\n| Design                        | Area\
    \ (um²) | Delay (ns) | ADP (um²·ns) | MSE       |\n|-------------------------------|------------|------------|--------------|-----------|\n\
    | Baseline BSN                  | 2.95×105   | 4.33       | 1.26×106     | - \
    \        |\n| Spatial Appr. BSN             | 1.32×105   | 3.36       | 4.55×105\
    \     | 3.79×10−7 |\n| Spatial-Temporal<br>Appr. BSN | 8.18×103   | 1.92     \
    \  | 3.06×105*    | 3.79×10−7 |\n\n\\*Spatial-temporal BSN considers 19× area\
    \ to achieve the same throughput.\n\n![](_page_3_Figure_17.jpeg)\n\n<span id=\"\
    page-3-5\"></span>Fig. 13. (a) ADP and (b) MSE comparison on 4 size of layers\
    \ in ResNet18.\n\nBSN needs fewer cycles for smaller convolutions and achieved\
    \ ADP reductions from 8.2× to 23.3× with negligible errors, as shown in Figure\
    \ [13.](#page-3-5) On average, the spatial-temporal BSN reduces the 2.2× area\
    \ of datapath by reducing the average ADP of BSN by 8.5×. This shows that the\
    \ proposed SC design is more flexible and efficient.\n\n### V. SUMMARY AND FUTURE\
    \ WORK\n\nIn this paper, we review our recent works on end-to-end SC neural network\
    \ acceleration. [\\[4\\]](#page-4-10) implemented a parallel fully SCbased TNN\
    \ processor using deterministic thermometer encoding and sorting networks on the\
    \ MNIST, achieving energy efficiency of 198.9 TOPS/W. In addition, [\\[10\\]](#page-4-5)\
    \ propose SC-friendly models with highprecision residual fusion and corresponding\
    \ SC circuits to greatly improve the network accuracy. [\\[11\\]](#page-4-6) further\
    \ proposed a more flexible and efficient spatial-temporal approximate BSN, enabling\
    \ accurate, efficient, and flexible end-to-end SC acceleration. In future work,\
    \ we explore SOTA transformer acceleration based on end-to-end stochastic computing,\
    \ which has been submitted [\\[12\\]](#page-4-7).\n\n### REFERENCES\n\n- <span\
    \ id=\"page-4-0\"></span>[1] W. Romaszkan *et al.*, \"ACOUSTIC: Accelerating Convolutional\
    \ Neural Networks through Or-Unipolar Skipped Stochastic Computing,\" in *Design,\
    \ Automation & Test in Europe Conference & Exhibition (DATE)*, 2020, pp. 768–773.\n\
    - [2] W. Romaszkan *et al.*, \"A 4.4–75-TOPS/W 14-nm Programmable, Performance-\
    \ and Precision-Tunable All-Digital Stochastic Computing Neural Network Inference\
    \ Accelerator,\" *IEEE Solid-State Circuits Letters*, vol. 5, pp. 206–209, 2022.\n\
    - <span id=\"page-4-2\"></span>[3] Y. Zhang *et al.*, \"When sorting network meets\
    \ parallel bitstreams: A fault-tolerant parallel ternary neural network accelerator\
    \ based on stochastic computing,\" in *Design, Automation & Test in Europe Conference\
    \ & Exhibition (DATE)*. IEEE, 2020, pp. 1287–1290.\n- <span id=\"page-4-10\"></span>[4]\
    \ Y. Hu *et al.*, \"A 28-nm 198.9-TOPS/W Fault-Tolerant Stochastic Computing Neural\
    \ Network Processor,\" *IEEE Solid-State Circuits Letters*, vol. 5, pp. 198–201,\
    \ 2022.\n- <span id=\"page-4-1\"></span>[5] Y. Zhang *et al.*, \"Accurate and\
    \ Energy-Efficient Implementation of Non-Linear Adder in Parallel Stochastic Computing\
    \ using Sorting Network,\" in *IEEE International Symposium on Circuits and Systems\
    \ (ISCAS)*, 2020, pp. 1–5.\n- <span id=\"page-4-3\"></span>[6] K. Kim *et al.*,\
    \ \"Dynamic energy-accuracy trade-off using stochastic computing in deep neural\
    \ networks,\" in *Proceedings of the 53rd Annual Design Automation Conference*,\
    \ 2016, pp. 1–6.\n- [7] J. Li *et al.*, \"Towards acceleration of deep convolutional\
    \ neural networks using stochastic computing,\" in *2017 22nd Asia and South Pacific\
    \ Design Automation Conference (ASP-DAC)*. IEEE, 2017, pp. 115–120.\n- [8] Z.\
    \ Li *et al.*, \"HEIF: Highly efficient stochastic computing-based inference framework\
    \ for deep neural networks,\" *IEEE Transactions on Computer-Aided Design of Integrated\
    \ Circuits and Systems*, vol. 38, no. 8, pp. 1543–1556, 2018.\n- <span id=\"page-4-4\"\
    ></span>[9] J. Li *et al.*, \"Hardware-driven nonlinear activation for stochastic\
    \ computing based deep convolutional neural networks,\" in *2017 International\
    \ Joint Conference on Neural Networks (IJCNN)*, 2017, pp. 1230–1236.\n- <span\
    \ id=\"page-4-5\"></span>[10] Y. Hu *et al.*, \"Accurate yet Efficient Stochastic\
    \ Computing Neural Acceleration with High Precision Residual Fusion,\" in *Design,\
    \ Automation & Test in Europe Conference & Exhibition (DATE)*, 2023.\n- <span\
    \ id=\"page-4-6\"></span>[11] Y. Hu *et al.*, \"Efficient Non-Linear Adder for\
    \ Stochastic Computing with Approximate Spatial-Temporal Sorting Network,\" in\
    \ *ACM/IEEE Design Automation Conference (DAC)*, 2023.\n- <span id=\"page-4-7\"\
    ></span>[12] Y. Hu *et al.*, \"ASCEND: Accurate yet Efficient End-to-End Stochastic\
    \ Computing Acceleration of Vision Transformer,\" in *submitted*.\n- <span id=\"\
    page-4-8\"></span>[13] K. E. Batcher, \"Sorting networks and their applications,\"\
    \ in *Proceedings of the April 30–May 2, 1968, spring joint computer conference*,\
    \ 1968, pp. 307–314.\n- <span id=\"page-4-9\"></span>[14] S. Mohajer *et al.*,\
    \ \"Routing magic: Performing computations using routing networks and voting logic\
    \ on unary encoded data,\" in *Proceedings of the 2018 ACM/SIGDA International\
    \ Symposium on Field-Programmable Gate Arrays*, 2018, pp. 77–86.\n- <span id=\"\
    page-4-11\"></span>[15] J. Lee *et al.*, \"UNPU: A 50.6 TOPS/W unified deep neural\
    \ network accelerator with 1b-to-16b fully-variable weight bit-precision,\" in\
    \ *2018 IEEE International Solid-State Circuits Conference-(ISSCC)*. IEEE, 2018,\
    \ pp. 218–220.\n- [16] J. Song *et al.*, \"7.1 An 11.5 TOPS/W 1024-MAC butterfly\
    \ structure dualcore sparsity-aware neural processing unit in 8nm flagship mobile\
    \ SoC,\" in *2019 IEEE International Solid-State Circuits Conference-(ISSCC)*.\
    \ IEEE, 2019, pp. 130–132.\n- [17] C.-H. Lin *et al.*, \"7.1 A 3.4-to-13.3 TOPS/W\
    \ 3.6 TOPS dual-core deeplearning accelerator for versatile AI applications in\
    \ 7nm 5G smartphone SoC,\" in *2020 ieee international solid-state circuits conference-(isscc)*.\
    \ IEEE, 2020, pp. 134–136.\n- [18] F. Tu *et al.*, \"Evolver: A deep learning\
    \ processor with on-device quantization–voltage–frequency tuning,\" *IEEE Journal\
    \ of Solid-State Circuits*, vol. 56, no. 2, pp. 658–673, 2020.\n- <span id=\"\
    page-4-12\"></span>[19] H. Mo *et al.*, \"9.2 A 28nm 12.1 TOPS/W dual-mode CNN\
    \ processor using effective-weight-based convolution and error-compensation-based\
    \ prediction,\" in *2021 IEEE International Solid-State Circuits Conference (ISSCC)*,\
    \ vol. 64. IEEE, 2021, pp. 146–148."
  decisions:
    evaluation_prompt: 'Qualified. Reason: The paper includes experimental results,
      tables, and figures demonstrating quantitative analysis.'
    related_work_prompt: 'Qualified. Reason: The paper cites multiple research papers,
      explains and compares to cited work, and includes a section that discusses related
      work.'
    novelty_prompt: 'Qualified. Reason: The paper introduces a new deterministic thermometer
      coding scheme and corresponding end-to-end SC accelerator design, addressing
      limitations in existing SC-based accelerators.'
    review_only_prompt: Qualified.
- title: 'TOP: Towards Open & Predictable Heterogeneous SoCs'
  abstract: 'Ensuring predictability in modern real-time Systems-on-Chip (SoCs) is
    an

    increasingly critical concern for many application domains such as automotive,

    robotics, and industrial automation. An effective approach involves the

    modeling and development of hardware components, such as interconnects and

    shared memory resources, to evaluate or enforce their deterministic behavior.

    Unfortunately, these IPs are often closed-source, and these studies are limited

    to the single modules that must later be integrated with third-party IPs in

    more complex SoCs, hindering the precision and scope of modeling and

    compromising the overall predictability. With the coming-of-age of open-source

    instruction set architectures (RISC-V) and hardware, major opportunities for

    changing this status quo are emerging. This study introduces an innovative

    methodology for modeling and analyzing State-of-the-Art (SoA) open-source SoCs

    for low-power cyber-physical systems. Our approach models and analyzes the

    entire set of open-source IPs within these SoCs and then provides a

    comprehensive analysis of the entire architecture. We validate this methodology

    on a sample heterogenous low-power RISC-V architecture through RTL simulation

    and FPGA implementation, minimizing pessimism in bounding the service time of

    transactions crossing the architecture between 28% and 1%, which is

    considerably lower when compared to similar SoA works.'
  url: http://arxiv.org/abs/2401.15639v2
  keywords: '*—Heterogeneous SoC, Cyber-Physical-Systems, Timing Predictable Architectures,
    Open-Source Hardware.'
  document: "# TOP: Towards Open & Predictable Heterogeneous SoCs\n\nLuca Valente,\
    \ Francesco Restuccia, Davide Rossi, *Member, IEEE* Ryan Kastner, *Fellow, IEEE*\
    \ Luca Benini, *Fellow, IEEE*\n\n**Abstract**—Ensuring predictability in modern\
    \ real-time Systems-on-Chip (SoCs) is an increasingly critical concern for many\
    \ application domains such as automotive, robotics, and industrial automation.\
    \ An effective approach involves the modeling and development of hardware components,\
    \ such as interconnects and shared memory resources, to evaluate or enforce their\
    \ deterministic behavior. Unfortunately, these IPs are often closed-source, and\
    \ these studies are limited to the single modules that must later be integrated\
    \ with third-party IPs in more complex SoCs, hindering the precision and scope\
    \ of modeling and compromising the overall predictability. With the coming-of-age\
    \ of open-source instruction set architectures (RISC-V) and hardware, major opportunities\
    \ for changing this status quo are emerging. This study introduces an innovative\
    \ methodology for modeling and analyzing State-of-the-Art (SoA) open-source SoCs\
    \ for low-power cyber-physical systems. Our approach models and analyzes the entire\
    \ set of open-source IPs within these SoCs and then provides a comprehensive analysis\
    \ of the entire architecture. We validate this methodology on a sample heterogenous\
    \ low-power RISC-V architecture through RTL simulation and FPGA implementation,\
    \ minimizing pessimism in bounding the service time of transactions crossing the\
    \ architecture between 28% and 1%, which is considerably lower when compared to\
    \ similar SoA works.\n\n✦\n\n**Index Terms**—Heterogeneous SoC, Cyber-Physical-Systems,\
    \ Timing Predictable Architectures, Open-Source Hardware.\n\n# **1 INTRODUCTION**\n\
    \nThe exponential growth of cyber-physical systems (CPS) (e.g., self-driving cars,\
    \ autonomous robots, ...) and related applications has been fueled by the increase\
    \ in computational capabilities of heterogeneous low-power Systems-on-Chip (SoCs).\
    \ These SoCs are complex computing platforms composed of a set of different hardware\
    \ computing units (e.g., CPUs, hardware accelerators), each tailored to a specific\
    \ target application, sharing a set of resources (memory, sensors) through interconnects\
    \ [\\[1\\]](#page-12-0)–[\\[5\\]](#page-12-1). While integrating multiple computing\
    \ units on the same platform has enabled efficient scale-up of computational capabilities,\
    \ it also poses significant challenges when it comes to assessing their *timing\
    \ predictability*, which is a requirement for CPSs dealing with real-time and\
    \ safety-critical applications: the primary challenge arises from resource contentions\
    \ that emerge when multiple active agents within the SoC must access the same\
    \ shared resources [\\[1\\]](#page-12-0)–[\\[7\\]](#page-12-2).\n\nNumerous research\
    \ efforts have focused on enhancing the timing predictability of heterogeneous\
    \ Systems-on-Chip (SoCs). This includes safely upper bounding execution times\
    \ for data transfers [\\[8\\]](#page-13-0)–[\\[10\\]](#page-13-1) or the deadline\
    \ miss ratio for critical tasks [\\[1\\]](#page-12-0)–[\\[3\\]](#page-12-3), with\
    \ the smallest possible pessimism. These efforts have predominantly focused on\
    \ modeling and analyzing commercial DDR protocols [\\[8\\]](#page-13-0), memory\
    \ IPs [\\[11\\]](#page-13-2), and memory controllers [\\[12\\]](#page-13-3), but\
    \ also predictable interconnects [\\[1\\]](#page-12-0), [\\[4\\]](#page-12-4)\
    \ and on-chip communication protocols [\\[13\\]](#page-13-4). Regrettably, despite\
    \ their value, these studies are scattered, with each one focusing on only one\
    \ of these resources at a time, resulting in being overly pessimistic [\\[5\\\
    ]](#page-12-1).\n\nModeling and analysis of communication protocols are done speculatively\
    \ on abstract models, thus reducing their real-world applicability. Recent works\
    \ for modeling and analysis of IPs (memories, memory controllers, interconnect,\
    \ etc.) have to address the unavailability of cycle-accurate RTL descriptions.\
    \ Many of these IPs are either entirely closed-source [\\[8\\]](#page-13-0) or\
    \ provide loosely-timed behavioral models [\\[5\\]](#page-12-1), [\\[12\\]](#page-13-3)\
    \ or just µarchitectural descriptions [\\[1\\]](#page-12-0), [\\[3\\]](#page-12-3),\
    \ [\\[4\\]](#page-12-4). In essence, the fragmented and proprietary nature of\
    \ commercial and research IPs restricts studies to the particular IP, greatly\
    \ reducing the accuracy achievable through systemlevel analysis. For example,\
    \ Restuccia et al. in [\\[9\\]](#page-13-5) bound the access times of multiple\
    \ initiators on FPGA reading and writing from/to the shared DDR memory. The proposed\
    \ upper bounds' pessimism is between 50% and 90%: even though they finely modeled\
    \ and analyzed the proprietary interconnect, the authors did not have access to\
    \ its RTL nor to the memory controller and IP. The same applies to Ditty [\\[10\\\
    ]](#page-13-1), which is a predictable cache coherence mechanism. In Ditty, even\
    \ though the caches' timing is finely modeled, the overall execution time can\
    \ be up to 3× bigger than the theoretical upper bounds, as the authors did not\
    \ model other components. Another example is AXI-ICRT [\\[1\\]](#page-12-0), an\
    \ advanced AXI interconnect with a sophisticated scheduler which allows transaction\
    \ prioritization based on importance. While proposing a highly advanced interconnect\
    \ with a\n\n<sup>•</sup> *Luca Valente, Luca Benini, and Davide Rossi are with\
    \ the Department of Electrical, Electronic and Information Engineering, University\
    \ of Bologna, 40136 Bologna, Italy. Luca Benini is also with the Integrated Systems\
    \ Laboratory (IIS), ETH Z ¨urich, 8092 Z ¨urich, Switzerland.*\n\n<sup>•</sup>\
    \ *Francesco Restuccia and Ryan Kastner are with the Computer Science and Engineering,\
    \ University of California at San Diego, San Diego, CA 92093 USA.*\n\n*This work\
    \ was supported by Technology Innovation Institute, Secure Systems Research Center,\
    \ Abu Dhabi, UAE, PO Box: 9639, by the Spoke 1 on Future HPC of the Italian Research\
    \ Center on High-Performance Computing, Big Data and Quantum Computing (ICSC)\
    \ funded by MUR Mission 4 - Next Generation EU, and by the European Project EuroHPC\
    \ JU The European Pilot (g.a. 101034126), and by KDT TRISTAN project (g.a.101095947).*\n\
    \n<span id=\"page-1-0\"></span>![](_page_1_Figure_0.jpeg)\n\nFig. 1: Proposed\
    \ methodology.\n\ntightly coupled model, the authors do not extend the model to\
    \ the other components of the SoC, even when assessing the deadline miss ratio\
    \ and benchmarking the architecture.\n\nThe emergence of open-source hardware\
    \ creates a major opportunity for building accurate end-to-end models for realtime\
    \ analysis of cutting-edge heterogeneous low-power SoCs [\\[14\\]](#page-13-6)–[\\\
    [16\\]](#page-13-7): the openness of the IPs allows for cycle-accurate analysis\
    \ of the whole architecture from the interconnects to the shared resources. Yet,\
    \ investigations and successful demonstrations in this direction are still scarce,\
    \ primarily because open hardware has only very recently reached the maturity\
    \ and completeness levels required to build full heterogeneous SoCs [\\[17\\]](#page-13-8).\
    \ In this context, this is the first work to bridge the gap between open-source\
    \ hardware and timing analysis, demonstrating a methodology that successfully\
    \ exploits the availability of the source code to provide finegrained upper bounds\
    \ of the system-level data transfers. We leverage a set of open-source IPs from\
    \ the PULP family, one of the most popular open-hardware platforms proposed by\
    \ the research community [\\[14\\]](#page-13-6), [\\[18\\]](#page-13-9).\n\nFigure\
    \ [1](#page-1-0) shows the proposed methodology, highlighting the novel contributions\
    \ in yellow. It consists of (i) a model for standalone IPs composing modern heterogeneous\
    \ lowpower SoCs, (ii) a static analysis of the RTL code of such components, and\
    \ (iii) a compositional mathematical analysis of the whole system to upper bound\
    \ the response time of the interactions between managers (initiators) and shared\
    \ subordinates (targets), considering the maximum interference generated by the\
    \ interfering managers. Figure [1](#page-1-0) highlights the differences between\
    \ the proposed methodology and previous studies also based on a static and compositional\
    \ approach [\\[5\\]](#page-12-1), [\\[7\\]](#page-12-2), [\\[9\\]](#page-13-5).\
    \ While previous works typically focus on one IP at a time [\\[9\\]](#page-13-5),\
    \ or rely on loosely-timed models [\\[5\\]](#page-12-1), thereby limiting the\
    \ overall accuracy, our approach is the first to model and analyze all the IPs\
    \ directly from the RTL source code to build a holistic system-level analysis.\
    \ This limits the proposed upper bounds' pessimism between 28% and just 1%, in\
    \ isolation and under interference, which is considerably lower when compared\
    \ to similar SoA works for closedsource or loosely-timed platforms [\\[1\\]](#page-12-0)–[\\\
    [4\\]](#page-12-4), [\\[8\\]](#page-13-0), [\\[10\\]](#page-13-1), as better detailed\
    \ in Section [7.](#page-11-0) We demonstrate our methodology on a completely open-source\
    \ prototype of a heterogeneous lowpower open-source SoC for embedded systems composed\
    \ of a Linux-capable host core, a parallel accelerator, a set of IOs, and on-chip\
    \ and off-chip memories. the system-level analysis of the architecture. Finally,\
    \ Section 2-banks 512KiB SPM\n\nThe manuscript is organized as follows: Section\
    \ [2](#page-1-1) presents the target open-source RISC-V-based SoC architecture,\
    \ and Section [3](#page-2-0) discusses the model we apply to its different components.\
    \ Section [4](#page-3-0) analyzes the components to specialize the generic model\
    \ to each of them, and Section [5](#page-7-0) provides\n\n<span id=\"page-1-2\"\
    ></span>![](_page_1_Figure_6.jpeg)\n\nFig. 2: Sample architecture.\n\n[6](#page-9-0)\
    \ validates the results with cycle-accurate experiments (on simulation and FPGA),\
    \ Section [7](#page-11-0) compares this work with the SoA. Section [8](#page-12-5)\
    \ concludes the manuscript.\n\n# <span id=\"page-1-1\"></span>**2 ARCHITECTURE**\n\
    \nFig. [2](#page-1-2) shows the architectural template we target. It also shows\
    \ the four classes of hardware modules we identify in the architecture under analysis,\
    \ namely (i) *controllers*, (ii) the main *crossbar*, (iii) *bridges*, and (iv)\
    \ *peripherals*, which we model in the next Section. The architecture leverages\
    \ a set of fully open-source PULP IPs [\\[18\\]](#page-13-9). It is based on Cheshire\
    \ [\\[15\\]](#page-13-10), an open-source host platform consisting of an RV64\
    \ Linuxcapable CPU, a set of commodity IOs (SPI, SDIO, UART, ...), and an AXI-based\
    \ crossbar with a configurable number of subordinate and manager ports for easy\
    \ integration of accelerators and resources. Our platform includes a parallel\
    \ accelerator and a low-power lightweight HyperBUS memory controller [\\[19\\\
    ]](#page-13-11), connected to the crossbar.\n\nThe host CPU is CVA6 [\\[20\\]](#page-13-12),\
    \ which is a six stages, singleissue, in-order, 64-bit Linux-capable RISC-V core,\
    \ supporting the RV64GC ISA variant, SV39 virtual memory with a dedicated Memory\
    \ Management Unit (MMU), three levels of privilege (Machine, Supervisor, User),\
    \ and PMP [\\[21\\]](#page-13-13). CVA6 features private L1 instruction and caches,\
    \ operating in parallel, with the latter being able to issue multiple transactions.\
    \ When needed, CVA6 can offload computationintensive tasks to the parallel hardware\
    \ accelerator, the socalled PULP cluster [\\[22\\]](#page-13-14). It is built\
    \ around 8 CV32E4-based cores [\\[23\\]](#page-13-15) sharing 16×8 kB SRAM banks,\
    \ composing a 128 kB L1 Scratchpad Memory (SPM). The cluster features a DMA to\
    \ perform data transfers between the private L1SPM and the main memory: data movement\
    \ is performed via softwareprogrammed DMA transfers. Once the data are available\
    \ inside the L1SPM, the accelerator starts the computation.\n\nCVA6 and the cluster\
    \ are the managers of the systems connected to the main AXI crossbar [\\[24\\\
    ]](#page-13-16), which routes their requests to the desired subordinates according\
    \ to the memory map. A manager can access any subordinate in the system. The main\
    \ subordinates of the systems are, respectively, (i) the on-chip SRAM memory,\
    \ (ii) the IO subsystem, and (iii) the off-chip main memory with a tightly coupled\
    \ Last Level Cache (LLC). The on-chip memory is used for low-latency, high-bandwidth\
    \ data storage. The APB subsystem is used to communicate with off-chip sensors\
    \ or memories through the\n\ncommodity IOs. The off-chip main memory is where\
    \ the code and the shared data are stored. Differently from high-end embedded\
    \ systems relying on relatively power-hungry and expensive DDR3/4/5 memories,\
    \ the platform under analysis adopts HyperRAMs as off-chip main memory, which\
    \ are fully-digital low-power small-area DRAMs with less than 14 IO pins and that\
    \ provide enough capacity to boot Linux [\\[16\\]](#page-13-7) and bandwidth for\
    \ IoT applications [\\[19\\]](#page-13-11), [\\[25\\]](#page-13-17).\n\n# <span\
    \ id=\"page-2-0\"></span>**3 MODEL**\n\nThis section presents the model we construct\
    \ for the different components of our SoC. Our aim is to propose a general model\
    \ that describes the characteristics of the components and that can be re-targeted\
    \ to different IPs and novel architectures, regardless of the number of integrated\
    \ controllers and peripherals. This work is also an effort to provide base support\
    \ to stimulate further studies in predictability improvements and analysis for\
    \ open hardware architectures.\n\n#### **3.1 Communication model**\n\nWe identify\
    \ four classes of hardware modules in the architecture under analysis, shown in\
    \ Fig. [2,](#page-1-2) namely (i) *controllers*, (ii) the main *crossbar*, (iii)\
    \ *bridges*, and (iv) *peripherals*. As the AXI standard is the main communication\
    \ standard used to implement non-coherent on-chip communications [\\[24\\]](#page-13-16),\
    \ we discuss here its main features. It defines a managersubordinate interface\
    \ enabling simultaneous, bi-directional data exchange and multiple outstanding\
    \ transactions. Fig. [3](#page-2-1) shows the AXI channel architecture and information\
    \ flow. Bus transactions are initiated by a *controller* (exporting a manager\
    \ interface), submitting a transaction request to read/write data to/from a subordinate\
    \ interface through AR or AW channels, respectively. A request describes the starting\
    \ target address and a *burst length*. After the request phase, in case of a read,\
    \ data are transmitted through the R channel. In case of a write, data are provided\
    \ by the *controller* to the target *peripheral* through the W channel. Upon completing\
    \ a write transaction, the *peripheral* also sends a beat on the B channel to\
    \ acknowledge the transaction's completion. For multiple in-flight write transactions,\
    \ the standard enforces strict in-order access to the W channel: the data on the\
    \ W channel must be propagated in the same order as the AW channel requests. Even\
    \ though the standard does not require it, many commercial and open-source platforms\
    \ apply the same policy for reads, typically to limit the system's overall complexity,\
    \ as reported in their documentation [\\[26\\]](#page-13-18), [\\[27\\]](#page-13-19).\n\
    \n#### **3.2 Controller model**\n\n*Controllers* have an active role on the bus.\
    \ Each *controller* exports an AXI manager interface, through which it initiates\
    \ requests for bus transactions directed to the *peripherals*. A generic *controller*\
    \ C<sup>i</sup> can be described through two parameters: the maximum number of\
    \ outstanding read/write transactions that it can issue in parallel, denoted with\
    \ ϕ C<sup>i</sup> R/W , and their relative burst length β<sup>i</sup> . While\
    \ our model and analysis can be applied to a generic architecture, the system\
    \ under analysis features as *controllers* a CVA6 core [\\[20\\]](#page-13-12)\
    \ and a cluster accelerator [\\[22\\]](#page-13-14) (see Section [2\\)](#page-1-1).\
    \ Bus transactions issued by the cluster interfere with those issued by CVA6\n\
    \n<span id=\"page-2-1\"></span>\n\n| READ TRANSACTION                        \
    \ |                     |  |  |  |  |\n|------------------------------------------|---------------------|--|--|--|--|\n\
    | 1 AR (ADDR+ LEN)                         | SUBORDINATE         |  |  |  |  |\n\
    | 2 R (DATA + COMPL.)                      | INTERFACE           |  |  |  |  |\n\
    | WRITE TRANSACTION<br>1<br>AW (ADDR+ LEN) |                     |  |  |  |  |\n\
    | W (DATA)<br>2                            | SUBORDINATE         |  |  |  |  |\n\
    |                                          | INTERFACE           |  |  |  |  |\n\
    |                                          | 3<br>B (COMPLETION) |  |  |  |  |\n\
    \nFig. 3: AXI Channel architecture\n\nand vice-versa. CVA6 is assumed to compute\
    \ a critical periodic workload, running on top of a Real-time Operating System\
    \ (RTOS). The PULP cluster executes computationintensive tasks and issues bus\
    \ transactions through its DMA. Contention internal to the PULP cluster has been\
    \ profiled in detail in [\\[28\\]](#page-13-20). However, our analysis provides\
    \ the worstcase data transfer time in accessing the shared *peripherals* to support\
    \ the safe scheduling and execution of critical tasks within their deadline. We\
    \ specifically focus on interference in accessing the shared resources. Modeling\
    \ the internal effects of *controllers*, such as pipeline stalls in the core or\
    \ contention within the accelerator, is beyond the scope of this work.\n\n####\
    \ <span id=\"page-2-2\"></span>**3.3 Peripheral model**\n\n*Peripherals* export\
    \ a *subordinate* interface through which they receive and serve the bus transactions.\
    \ The *peripherals* deployed in the system are heterogeneous. Nonetheless, our\
    \ model offers a set of parameters representative of a generic peripheral, and\
    \ it is not tied to a specific communication protocol. It works as the baseline\
    \ for the analysis of any *peripheral* deployed in the system under analysis.\
    \ The generic *peripheral* P<sup>j</sup> is characterized with two sets of parameters:\
    \ (i) the maximum number of supported outstanding reads (χ P<sup>j</sup> <sup>R</sup>\
    \ ) and write (χ P<sup>j</sup> <sup>W</sup> ) transactions; (ii) the maximum number\
    \ of cycles incurred from the reception of the request to its completion, for\
    \ a read (d P<sup>j</sup> <sup>R</sup> ) and a write (d P<sup>j</sup> <sup>W</sup>\
    \ ) transaction in isolation. d P<sup>j</sup> <sup>R</sup> and d P<sup>j</sup>\
    \ <sup>W</sup> are composed of two contributions: (i) the *data time*, defined\
    \ as the time required for the *peripheral* to send or receive one word of data\
    \ (tDATA) multiplied by the burst length of the transaction in service (βi) and\
    \ (ii) the *control overhead* tCTRL, defined as the maximum time elapsing between\
    \ accepting the request and the availability of the first word of data (reads)\
    \ or availability to receive data (writes). From the previous considerations,\
    \ d P<sup>j</sup> R/W = t P<sup>j</sup> CTRL + t P<sup>j</sup> DATA · β. We define\
    \ two extra parameters ρ <sup>P</sup><sup>j</sup> and θ P<sup>j</sup> . The first\
    \ indicates the level of pipelining in serving multiple transactions. ρ <sup>P</sup><sup>j</sup>\
    \ = 1 means that each stage of P<sup>j</sup> does not stall the previous, and\
    \ transactions are served in a pipelined fashion, while ρ <sup>P</sup><sup>j</sup>\
    \ = 0 indicates that no pipeline is implemented. θ <sup>P</sup><sup>j</sup> =\
    \ 0 indicates that read and write transactions interfere with each other. θ <sup>P</sup><sup>j</sup>\
    \ = 1 indicates that read and write transactions can be handled in parallel by\
    \ P<sup>j</sup> .\n\n#### <span id=\"page-2-3\"></span>**3.4 Main crossbar model**\n\
    \nWe provide here the model of the main *crossbar*, the routing component enabling\
    \ communication among *controller*s and *peripheral*s. Each *controller* has its\
    \ manager port connected to a subordinate port of the *crossbar*. Each *peripheral*\
    \ has its subordinate port connected to a manager port of the *crossbar*. We model\
    \ the *crossbar* R<sup>0</sup> with two sets of parameters: (i) the\n\nmaximum\
    \ amount of outstanding read and write transactions that a subordinate port can\
    \ accept (χ R<sup>0</sup> <sup>R</sup> and χ R<sup>0</sup> <sup>W</sup> , respectively);\
    \ and (ii) the maximum overall latency introduced by R<sup>0</sup> on each read\
    \ (d R<sup>0</sup> <sup>R</sup> ) and write transaction (d R<sup>0</sup> <sup>W</sup>\
    \ ). d R<sup>0</sup> R and d R<sup>0</sup> <sup>W</sup> are composed of two contributions:\
    \ (i) the overall delay introduced by the *crossbar* on a transaction in isolation\
    \ (tPROP); (ii) the maximum time a request is delayed at the arbitration stage\
    \ due to the contention generated by interfering transactions (t R<sup>0</sup>\
    \ CON). From the previous considerations, the propagation latency is modeled as\
    \ d R<sup>0</sup> R/W = t R<sup>0</sup> PROP + t R<sup>0</sup> CON. Such parameters\
    \ depend on the arbitration policies and routing mechanisms, as we investigate\
    \ in detail in Section [4.](#page-3-0)\n\n#### <span id=\"page-3-1\"></span>**3.5\
    \ Bridge model**\n\nBridges export a single manager interface and a single subordinate\
    \ interface. They perform protocol/clock conversion between a *controller* and\
    \ the *crossbar*. Bridges require a certain number of clock cycles to be crossed\
    \ but do not limit the number of in-flight transactions and do not create any\
    \ contention. We model the bridges with two parameters: the overall maximum delay\
    \ introduced over a whole transaction for (a) read (d Q<sup>j</sup> <sup>R</sup>\
    \ ) and (b) write (d Q<sup>j</sup> <sup>W</sup> ) transactions.\n\n# <span id=\"\
    page-3-0\"></span>**4 ANALYSIS OF THE HARDWARE MODULES**\n\nThis Section aims\
    \ to analyze the worst-case behavior of the *peripherals*, *bridges*, and the\
    \ *crossbar* present in the platform under analysis. Our approach is compositional\
    \ – in this Section, we analyze each hardware component separately, specializing\
    \ in the generic models introduced in Section [3,](#page-2-0) and bounding the\
    \ service times at the IP level in isolation. In the next Section, we provide\
    \ an overall worst-case analysis at the system level, in isolation and under interference.\
    \ We define t P<sup>j</sup> CK as the period period of the clock fed to P<sup>j</sup>\
    \ .\n\n#### <span id=\"page-3-4\"></span>**4.1 AXI CDC FIFO queues**\n\nAXI CDC\
    \ FIFOs are leveraged to perform clock-domain crossing between two AXI-based devices.\
    \ The generic AXI CDC FIFO F<sup>i</sup> is a *bridge*: we apply here the model\
    \ presented in Section [3.5.](#page-3-1) It exports a manager interface and a\
    \ subordinate interface. It is composed of five independent CDC FIFOs, each serving\
    \ as a buffer for an AXI channel, having depth D<sup>i</sup> CDC (design parameter\
    \ for the IP under analysis).\n\n#### *4.1.1 RTL IP structure*\n\nFigure [4](#page-4-0)\
    \ shows the block diagram of a CDC FIFO in the platform under analysis. They are\
    \ structured following established clock domain crossing (CDC) principles [\\\
    [24\\]](#page-13-16). The design is split into two parts, the transmitter (TX)\
    \ and the receiver (RX), having different clock domains. TX and RX interface through\
    \ asynchronous signals, namely a counter for data synchronization (synchronized\
    \ with two-stage Flip-Flops (FFs)) and the payload data signal.\n\n#### *4.1.2\
    \ Delays analysis*\n\nAs mentioned earlier, CDC FIFOs are *bridges*: we apply\
    \ the model presented in Section [3.5.](#page-3-1) The CDC FIFO under analysis\
    \ behaves as follows: TX samples the payload data into an FF. In the following\
    \ cycle, the TX counter is updated. The TX\n\ncrossing the CDC FIFO introduces\
    \ a fixed delay of one clock cycle of the TX domain (t CK) and four clock cycles\
    \ of the RX domain (t RX CK). This means that the delay in crossing the CDC FIFO\
    \ is equal to tCDC(t TX CK, tRX CK) = t TX CK + 4 · t RX CK.We leverage this baseline\
    \ delay to build the overall latency introduced by F<sup>i</sup> , interposed\
    \ between a manager (clocked at t C CK) and a subordinate (clocked at t P CK).\n\
    \n*Read transaction:* A read transaction AR<sup>k</sup> is composed of two phases:\
    \ (i) the address propagation phase and (ii) the data phase. This means that F<sup>i</sup>\
    \ is crossed twice to complete ARk: during phase (i), the manager is on the TX\
    \ side, propagating the request. In phase (ii), the subordinate is on the TX side,\
    \ propagating the data. Hence, the propagation latency is tCDC(t C CK, t<sup>P</sup>\
    \ CK) in phase (i) and tCDC(t P CK, t<sup>C</sup> CK) in phase (ii). Adding them\
    \ together, the propagation latency introduced by F<sup>i</sup> on AR<sup>k</sup>\
    \ is equal to:\n\n<span id=\"page-3-2\"></span>\n$$d\\_R^{\\rm CDC} = t\\_{\\\
    rm CDC}(t\\_{\\rm CK}^C, t\\_{\\rm CK}^P) + t\\_{\\rm CDC}(t\\_{\\rm CK}^P, t\\\
    _{\\rm CK}^C) = 5(t\\_{\\rm CK}^C + t\\_{\\rm CK}^P) \\tag{1}$$\n\n*Write transaction:*\
    \ A write transaction is composed of three phases: (i) an address phase (manager\
    \ on the TX side), (ii) a data phase (manager on the TX side), and (iii) a write\
    \ response phase (subordinate on the TX side). Phases (i) and (ii) happen in parallel\
    \ (see [\\[29\\]](#page-13-21) p. 45). Thus, tCDC(t C CK, t<sup>P</sup> CK) is\
    \ incurred for phases (i) and (ii), and tCDC(t P CK, t<sup>C</sup> CK) for phase\
    \ (iii). The delay introduced by F<sup>i</sup> on AW<sup>k</sup> is equal to the\
    \ delay introduced in Equation [1,](#page-3-2) d CDC <sup>W</sup> = d CDC <sup>R</sup>\
    \ .\n\n#### <span id=\"page-3-3\"></span>**4.2 AXI SRAM scratchpad memory (SPM)**\n\
    \nThe AXI SPM is a high-speed, low-latency memory component used for temporary\
    \ data storage – a block design representation is reported in Figure [5.](#page-4-0)\
    \ The SPM memory is a *peripheral*: we apply here the model presented in Section\
    \ [3.3.](#page-2-2)\n\n#### *4.2.1 RTL IP structure*\n\nThe first stage of the\
    \ SPM architecture is represented by a protocol converter (AXI-SRAM-Interface),\
    \ translating the read and write AXI channels into SRAM-compatible transactions.\
    \ Following the converter, an internal demux directs the SRAM transactions to\
    \ the desired SRAM bank, where the data is stored. Each SRAM bank provides two\
    \ independent SRAM ports, one for reads and one for writes, as from the specification\
    \ of industry-standard SRAM resources [\\[30\\]](#page-13-22).\n\n*The AXI-SRAM-Interface*\
    \ is structured in two submodules, independently managing read and write transactions.\
    \ The first stage of each submodule is a FIFO queue (of depth DSPM FIFO) buffering\
    \ the AXI AW or AR channel, respectively. Each submodule features the logic for\
    \ protocol translation, consisting of (i) saving transaction metadata (starting\
    \ address and length) and (ii) producing the output SRAM requests. For writes,\
    \ the incoming data on the W channel are directly propagated towards the banks.\
    \ The logic operating the protocol conversion generates the address for each W\
    \ beat. For reads, the data coming from the SRAM banks are directly driven on\
    \ the R channel. The logic keeps compliance with the AXI standard, adding the\
    \ last signal or generating write responses when required. *The demux* is fully\
    \ combinatorial\n\n<span id=\"page-4-0\"></span>![](_page_4_Figure_1.jpeg)\n\n\
    Fig. 4: CDC FIFO block diagram.\n\nFig. 5: AXI SPM block diagram\n\n0 1 N-1 ...\n\
    \nDual-Port SRAM Interface\n\nBank 1\n\nAR FIFO AW FIFO ADDR & REQ CTRL ADDR GEN\
    \ & CTRL\n\nAR R AW B W\n\nSRAM Interface\n\n0 1 N-1 ... Bank 0\n\nDual-Port SRAM\
    \ Interface\n\nCONVERTER\n\nDEMUXING\n\nBANKS\n\n![](_page_4_Figure_4.jpeg)\n\n\
    Fig. 6: IO subsystem block diagram.\n\nand selects the target bank according to\
    \ the request's address. *The SRAM banks* are technology-specific macros instantiated\
    \ at design time. Each SRAM bank's port exports an enable signal, an address signal,\
    \ and a signal to determine if a transaction is a read or a write. The SRAM interface\
    \ expects simultaneous propagation of data and commands for writes; for reads,\
    \ the data are sent the cycle following the command.\n\n#### *4.2.2 Delays and\
    \ parallelism analysis*\n\n*AXI-SRAM-Interface:* the FIFOs in the converter are\
    \ only in charge of data buffering – each FIFO introduces a fixed delay of one\
    \ clock cycle (t SPM CK ). After the FIFOs, the control logic requires at most\
    \ one clock cycle (t SPM CK ) to set up the propagation of a burst transaction\
    \ – the direct connection over the W and R channels makes the data streaming in\
    \ a pipeline fashion, adding no further latency. At the end of a write transaction,\
    \ the converter takes two clock cycles (2t SPM CK ) to generate the write response:\
    \ one to acknowledge that the last W beat has been accepted and one to provide\
    \ the B response. The same applies to reads, to generate the AXI last signal.\
    \ Summing up the contributions, the control latency introduced by the AXI-SRAM-Interface\
    \ to each transaction is upper bound by 4t SPM CK for both reads and writes.\n\
    \n*Demux:* The demultiplexing is combinatorial: it connects the transaction to\
    \ the SRAM bank in one clock cycle (t SPM CK ).\n\n*Banks:* As by the definition\
    \ of the SRAM interface [\\[30\\]](#page-13-22), an SRAM bank serves one transaction\
    \ per clock cycle, which makes t SPM DATA,R/W = t SPM CK . For write transactions,\
    \ the protocol guarantees that the SRAM bank samples the data in parallel with\
    \ the request (in the same clock cycle). For read transactions, the data are served\
    \ the clock cycle after the bank samples the request. So, it contributes to t\
    \ SPM CTRL,R with one clock cycle (t SPM CK ). Summing up the contributions, the\
    \ service time of the SPM in isolation is upper bound by:\n\n$$t\\_{\\rm{CTRL},W}^{\\\
    rm{SPM}} = 5 \\cdot t\\_{\\rm{CK}}^{\\rm{SPM}}; t\\_{\\rm{CTRL},R}^{\\rm{SPM}}\
    \ = 6 \\cdot t\\_{\\rm{CK}}^{\\rm{SPM}}; t\\_{\\rm{DATA},R/W}^{\\rm{SPM}} = t\\\
    _{\\rm{CK}}^{\\rm{SPM}}; \\tag{2}$$\n\nConsider now the parallelism supported\
    \ by the SPM. The maximum number of accepted outstanding transactions at the SPM\
    \ χ SPM <sup>R</sup> is defined by the depth DSPM FIFO of the input buffers implemented\
    \ in the AXI-SRAM-Interface. Thus,\n\n$$\n\\chi\\_R^{\\text{SPM}} = \\chi\\_W^{\\\
    text{SPM}} = D\\_{\\text{FIFO}}^{\\text{SPM}} \\tag{3}\n$$\n\nThe *SPM* module\
    \ under analysis is aggressively pipelined, operations are executed in one clock\
    \ cycle, and no stall sources are present in the design. Also, as mentioned earlier,\
    \ read and write transactions do not interfere with each other. From the previous\
    \ considerations, ρ SPM = 1 and θ SPM = 1.\n\n#### <span id=\"page-4-1\"></span>**4.3\
    \ IO Subsystem**\n\nSRAM Interface\n\nBank B-1\n\n0 1 N-1 ...\n\nInterface\n\n\
    The IO subsystem is the *peripheral* in charge of writing/reading data to/from\
    \ the off-chip I/Os. We apply here the model presented in Section [3.3.](#page-2-2)\
    \ It is composed of a set of memory-mapped peripheral registers that are accessed\
    \ through a demux and that manage the datapaths issuing the transactions on the\
    \ I/O interfaces (e.g., SPI, I2C, etc.).\n\n#### *4.3.1 RTL IP structure*\n\n\
    Figure [6](#page-4-0) shows the block diagram of the IO subsystem. It is composed\
    \ of an AXI-REG-Interface, a demux, and a set of registers. The first stage of\
    \ the *AXI-REG-Interface* is composed of two FIFOs (of depth DIO FIFO), buffering\
    \ read and write transactions, respectively. After the FIFOs, a round-robin arbiter\
    \ manages read and write transactions, allowing only one at a time to pass to\
    \ the protocol conversion. Since the IO subsystem is meant for low-power reads\
    \ and writes, registers' transactions share the same set of signals for reads\
    \ and writes and are limited to single-word accesses. For such a reason, the IO\
    \ subsystem does not support burst transactions (requests having β<sup>i</sup>\
    \ > 1 are suppressed). *The demux* stage decodes the request and directs it to\
    \ the proper register destination, where it is finally served as a register read\
    \ or write.\n\n#### *4.3.2 Delays and parallelism analysis*\n\nThe IO subsystem\
    \ is a *peripheral*, thus, we apply the model proposed in Section [3.5.](#page-3-1)\
    \ Considering the maximum service delays, overall, the IO subsystem is composed\
    \ of four stages: (i) the FIFOs, (ii) the protocol conversion, (iii) demultiplexing,\
    \ and (iv) target register access. The first three stages, contributing to the\
    \ control overhead, introduce a fixed delay of one clock cycle (t IO CK) each\
    \ for a total of 3 · t IO CK clock cycles. Consider now stage (iv). In the case\
    \ of a write, the request and the corresponding data are propagated in parallel\
    \ in one clock cycle. In the case of a read, the register provides the data in\
    \ the clock cycle following the request – t IO CTRL requires one extra clock cycle.\
    \ Summing all the contributions, the service time of the I/O subsystem is upper\
    \ bounded by:\n\n$$t\\_{\\rm{CTRL,W}}^{IO} = 3 \\cdot t\\_{\\rm{CK}}^{IO}; \\\
    quad t\\_{\\rm{CTRL,R}}^{IO} = 4 \\cdot t\\_{\\rm{CK}}^{IO}; \\quad t\\_{\\rm{DATA,W/R}}^{IO}\
    \ = t\\_{\\rm{CK}}^{IO} \\tag{4}$$\n\nConsider now the parallelism. Similarly\
    \ to the SPM module, the IO subsystem is capable of buffering up to DIO FIFO of\
    \ each type in its input FIFO queues. Thus, the maximum number of outstanding\
    \ transactions supported by the IO subsystem is equal to:\n\n$$\n\\chi\\_W^{\\\
    rm IO} = \\chi\\_R^{\\rm IO} = D\\_{\\rm FIFO}^{\\rm IO} \\tag{5}\n$$\n\nThe IO\
    \ subsystem serves read and write transactions one at a time, and no pipelining\
    \ is implemented among the different stages. This means that ρ IO = 0 and θ SPM\
    \ = 0.\n\n<span id=\"page-5-0\"></span>![](_page_5_Figure_0.jpeg)\n\nFig. 7: Block\
    \ diagrams of the components of the main memory subsystem. (a) LLC block diagram,\
    \ (b) Transaction control flow diagram, (c) Memory controller block diagram.\n\
    \n#### <span id=\"page-5-1\"></span>**4.4 The main memory subsystem**\n\nThe main\
    \ memory subsystem is a *peripheral*: we apply here the model presented in Section\
    \ [3.3.](#page-2-2) It is composed of three macro submodules: (i) the *AXI Last-level\
    \ Cache (LLC)*; (ii) the *HyperRAM memory controller (HMC)*; and (iii) the *HyperRAM\
    \ memory (HRAM)*. It is based on HyperRAM memories leveraging the HyperBUS protocol\
    \ [\\[25\\]](#page-13-17). HyperRAMs are optimized for low-overhead data storage\
    \ while offering up to 3.2Gbps bandwidth. HyperRAMs expose a low pin count, a\
    \ fully digital 8-bit double-data-rate (DDR) interface used for commands and data.\
    \ HyperRAMs serve transactions in order, one at a time, as required by the protocol\
    \ [\\[25\\]](#page-13-17). While a pure in-order strategy is simpler than those\
    \ deployed by high-end commercial memory controllers, it is important to note\
    \ that these controllers are typically complex closedsource IPs, making detailed\
    \ analysis extremely challenging. Notably, our analysis is the first to explore\
    \ this level of detail. Furthermore, the memory subsystem under analysis has shown\
    \ to be effective in tape-outs of Linux-capable chips [\\[16\\]](#page-13-7).\
    \ We model the service times of a single transaction in case of an LLC hit and\
    \ miss. By doing so, we provide upper bounds that can be leveraged by future studies\
    \ focusing on LLC interference between different *controllers* at the application\
    \ level. For example, advanced cache management studies for real-time applications\
    \ (e.g., cache coloring) could leverage the upper bounds provided here to bound\
    \ overall task execution times.\n\n#### *4.4.1 RTL IP structure*\n\n*The AXI Last-Level\
    \ Cache* is the interface of the memory subsystem with the platform. The LLC under\
    \ analysis has configurable cache line length, defined as LWLLC. Figure [7\\(](#page-5-0)a)\
    \ shows the LLC's block diagram, composed of 5 pipelined units: (i) burst splitter,\
    \ (ii) hit-miss detection, (iii) eviction/refill, (iv) data read/write, and (v)\
    \ data ways. Figure [7\\(](#page-5-0)b) shows how these units cooperate to serve\
    \ the requests. The burst splitter buffers and splits the incoming AXI requests\
    \ into multiple sub-requests that have the same length of the cache line, and\
    \ it calculates the tags of the sub-transactions. A βi-word AXI burst request\
    \ is split internally into ⌈ βi LWLLC ⌉ requests of length LWLLC. The tags are\
    \ the input to the hitmiss detection unit, which analyzes them to determine if\
    \ any sub-request will be a (a) hit or (b) miss. In case (a), the transaction\
    \ is directed to the read/write unit: if it is a (a.i) read, the read response\
    \ is generated and immediately sent through the AXI subordinate port, completing\
    \ the transaction. In the case of a (a.ii) write, the locally cached value is\
    \ updated, and a write response is generated and sent back to the\n\nAXI interface\
    \ to complete the transaction. In case (b), the transaction is submitted to the\
    \ eviction/refill unit. Refill is performed on every miss and consists of issuing\
    \ a read to the memory controller to fetch the missing data and update the data\
    \ way. Eviction is performed when a cache set is full to free the necessary spot\
    \ before a refill. A Least Recently Used (LRU) algorithm is used in the module\
    \ under analysis.\n\n*The HyperRAM memory controller* [\\[31\\]](#page-13-23)\
    \ is depicted in Figure [7\\(](#page-5-0)c). It consists of two tightly coupled\
    \ modules working in two separated frequency domains: (i) the AXI *front-end*\
    \ and (ii) the *back-end* PHY controller. The front-end handles and converts the\
    \ AXI transactions into data packets for the PHY controller; it runs at the same\
    \ clock as the LLC (t HMC CK ). The back-end features a Finite State Machine (FSM)\
    \ to send/receive the data packets and keep compliance with the HyperBUS protocol\
    \ timings and data flow; it runs at the same clock as the HyperRAMs (t HRAM CK\
    \ ). The back-end handles two off-chip HyperRAMs in parallel, configured with\
    \ interleaved addresses. As each HyperRAM arranges data as 16-bit words, the word\
    \ size of the back-end is DWHYPER = 32 bits.\n\nThe first stage of the front-end\
    \ is composed of two FIFOs buffering incoming AXI read and write requests. Then,\
    \ a serializer solves conflicts among reads and writes, allowing only one AW or\
    \ AR request at a time. Following, three modules translate between AXI and the\
    \ back-end protocol: (i) AXTOPHY, translating the AXI AW or AR requests into commands\
    \ for the back-end; (ii) PHYTOR converting the data words from the back-end into\
    \ AXI read beats for the AXI interface; and (iii) WTOPHY, converting AXI W data\
    \ beats into data words and generating write response at the end of the transaction.\
    \ Three CDC FIFOs are deployed between the AXTOPHY, WTOPHY, and PHYTOR and the\
    \ back-end. The back-end deploys an internal FSM arranging the requests coming\
    \ from the front-end into 48-bit vector requests, as required in the HyperBUS\
    \ protocol, and propagating the data packets to/from the two physical HyperRAM\
    \ memories through two *transceivers* (TRX).\n\n*The HyperRAM memory* is an off-chip\
    \ memory IP [\\[25\\]](#page-13-17). It is provided with a cycle-accurate model,\
    \ fundamental for our analysis purposes [\\[32\\]](#page-13-24). Each HyperRAM\
    \ is organized as an array of 16-bit words and supports one outstanding burst\
    \ transaction, up to 1kB long. As two HyperRAM are interleaved, the overall burst\
    \ can be up to 2kB long [\\[19\\]](#page-13-11).\n\n#### *4.4.2 Delays and parallelism\
    \ analysis*\n\nWe now bound the worst-case service time of the main memory subsystem,\
    \ analyzing its components one at a time. Starting with the LLC, we follow the\
    \ control flow diagram\n\nreported in Figure [7\\(](#page-5-0)b) to guide the\
    \ explanation. The LLC collects the requests incoming to the main memory. Three\
    \ scenarios can happen: (i) LLC cache hit, (ii) LLC cache miss with refill, and\
    \ (iii) LLC cache miss with eviction and refill.\n\nIn case (i), the LLC directly\
    \ manages the request, and no commands are submitted to the HMC. The request proceeds\
    \ through the LLC splitter, hit/miss unit, read/write unit, and data way stages.\
    \ By design, each stage of the LLC requires a fixed number of clock cycles. The\
    \ burst splitter executes in one clock cycle (t LLC CK ). The hit/miss detection\
    \ stage takes two clock cycles (2t LLC CK ): one for tag checking and one to propagate\
    \ the request to the read/write unit or the evict/refill unit. The read/write\
    \ unit requires one clock cycle (t LLC CK ) to route the transaction to the data\
    \ ways. The data ways accept the incoming request in one clock cycle (t LLC CK\
    \ ) to then access the internal SRAM macros (same as the SPM, Section [4.2\\)](#page-3-3).\
    \ The internal SRAM takes one clock cycle to provide the read data (t LLC CK ),\
    \ but no further latency is required on writes. Once it gets the response, the\
    \ read/write unit routes the read channel to the AX interface, whereas it takes\
    \ one clock cycle (t LLC CK ) to generate the write B response at the end. Thus,\
    \ read/write unit and data ways take together three clock cycles (3t LLC CK ).\
    \ Summing up the contributions, the service time in case of a hit is upper bound\
    \ by:\n\n$$t\\_{\\rm{CTRL},\\rm{R}/\\rm{W}}^{\\rm{MS-HIT}} = 6 \\cdot t\\_{\\\
    rm{CK}}^{\\rm{LLC}}; \\quad t\\_{\\rm{DATA},\\rm{R}/\\rm{W}}^{\\rm{MS-HIT}} =\
    \ t\\_{\\rm{CK}}^{\\rm{LLC}}; \\tag{6}$$\n\nConsider now cases (ii) and (iii):\
    \ the eviction and refill stage is also involved, and a read (for refill) and,\
    \ optionally, a write (for eviction) is issued to the main memory. Eviction and\
    \ refill are run in parallel. Each operation performs two steps, each taking one\
    \ clock cycle: (a) generating a transaction for the main memory and (b) generating\
    \ a transaction for the data way. Thus, summing the latency introduced by the\
    \ eviction and refill stage (2t LLC CK ) with the ones from the other stages,\
    \ the LLC's contribution to the overall control time in case of a miss is upper\
    \ bound by:\n\n$$t\\_{\\text{CTRL,R/W}}^{\\text{LLC-MISS}} = t\\_{\\text{CTRL,R/W}}^{\\\
    text{MS-HTT}} + 2t\\_{\\text{CK}}^{\\text{LLC}} \\tag{7}$$\n\nConsider now the\
    \ delay introduced by the HMC on a generic request. Later, we will use it to bound\
    \ the service time for the batch of transactions issued by the LLC. As described\
    \ earlier, the HMC is composed of (a) the frontend, (b) the CDC FIFOs, and (c)\
    \ the back-end. Consider (a): each one of the front-end's submodules takes one\
    \ clock cycle to sample and process the transaction, except for the serializer,\
    \ which takes two. As transactions pass through 4 modules (FIFOs, serializer,\
    \ AXITOPHY, and either WTOPHY or PHYTOR), the overall delay contribution of the\
    \ front-end is equal to 5t HMC CK . Consider now (b): these are the CDC FIFOs\
    \ composing the AXI CDC FIFOs introduced in Section [4.1.](#page-3-4) For writes,\
    \ the transmitter (TX) is the front-end, sending data to the back-end from the\
    \ AXTOPHY and the WTOPHY. As both transfers happen in parallel, the delay introduced\
    \ by the CDC on a write is upper bound by tCDC(t HMC CK , tHRAM CK ). For reads,\
    \ first, the front-end transmits (TX) the AXTOPHY request, and then the back-end\
    \ transmits the data beats: the delay introduced by the CDC on a read is upper\
    \ bound by tCDC(t HMC CK , tHRAM CK ) + tCDC(t HRAM CK , tHMC CK ). Consider now\
    \ (c): the back-end's FSM parses the incoming request into a HyperRAM command\
    \ in one cycle (t HRAM CK ). Following this, an extra cycle is required for the\
    \ data to cross the back-end. Summing up the contributions just described, the\
    \ control time of the HMC on a generic transaction is upper bound by:\n\n$$\\\
    begin{aligned} t\\_{\\rm{CIRL},\\rm{R}}^{\\rm{HMC}} &= 5 \\cdot t\\_{\\rm{CK}}^{\\\
    rm{HMC}} + t\\_{\\rm{CIRC}} (t\\_{\\rm{CK}}^{\\rm{HMC}}, t\\_{\\rm{CK}}^{\\rm{HRM}})\
    \ + t\\_{\\rm{CIRC}} (t\\_{\\rm{CK}}^{\\rm{HRM}}, t\\_{\\rm{CK}}^{\\rm{HMC}})\
    \ + 2 \\cdot t\\_{\\rm{CK}}^{\\rm{HRM}} \\\\ t\\_{\\rm{CIRL},\\rm{W}}^{\\rm{HMC}}\
    \ &= 5 \\cdot t\\_{\\rm{CK}}^{\\rm{HMC}} + t\\_{\\rm{CDC}} (t\\_{\\rm{CK}}^{\\\
    rm{HMC}}, t\\_{\\rm{CK}}^{\\rm{HRM}}) + 2 \\cdot t\\_{\\rm{CK}}^{\\rm{HRM}} \\\
    end{aligned} \\tag{8}$$\n\nConsider now the delays introduced by the HyperRAM\
    \ memories on a generic request. The control overhead time to access the HyperRAM\
    \ memory is defined by the HyperBUS protocol [\\[25\\]](#page-13-17). First, the\
    \ 48-bit HyperRAM command vector is sent over the two memories in 3 · t HRAM CK\
    \ clock cycles, as the HyperBUS command bus is 16 bits. Following, the HyperBUS\
    \ provides a fixed latency for the maximum time to access the first data word,\
    \ accounting for refresh effects and crossing row boundaries. The specifications\
    \ [\\[33\\]](#page-13-25) bound such a delay between 7 and 16 clock cycles. In\
    \ our case, this is set to 12 · t HRAM CK . Thus, the total control latency of\
    \ the HyperRAM memory is upper bound by:\n\n$$t\\_{\\text{CTRL,R/W}}^{\\text{HRAM}}\
    \ = 15 \\cdot t\\_{\\text{CK}}^{\\text{HRAM}} \\tag{9}$$\n\nAt this point, data\
    \ are ready to be propagated. As the AXI domain and the HyperRAM have different\
    \ data widths, the number of cycles to send/receive an AXI word is:\n\n$$t\\_{\\\
    text{DATA,R}/\\text{W}}^{\\text{HRAM}} = DW\\_{\\text{HYPER}} \\cdot \\lceil \\\
    frac{DW\\_{\\text{AXI}}}{DW\\_{\\text{HYPER}}} \\rceil \\cdot t\\_{\\text{CK}}^{\\\
    text{HRAM}} \\tag{10}$$\n\nWe now have all the elements to bound the overall service\
    \ time of the whole main memory subsystem in case of a miss (ii) with refill and\
    \ (iii) eviction and refill. First, we bound the service time to serve a refill\
    \ (read) request. A βi-long transaction is split by the LLC into ⌈βi/LWLLC⌉ subtransactions\
    \ to the memory, each LWLLC-long. Therefore, by multiplying the control time of\
    \ each sub-transaction (t HMC CTRL,R+ t HRAM CTRL,R) by the number of transactions\
    \ issued (⌈ βi LWLLC ⌉), we bound the control time introduced by the memory controller\
    \ and the off-chip memories. To this, we sum the control time of the LLC in case\
    \ of a miss (t MS-MISS CTRL,W/R) and obtain the whole control overhead. The same\
    \ reasoning applies to the data time: the total number of values requested by\
    \ the LLC to the memory will be equal to LWLLC · ⌈ <sup>β</sup><sup>i</sup> LWLLC\
    \ ⌉ and the overall time spent reading LWLLC · ⌈ <sup>β</sup><sup>i</sup> LWLLC\
    \ ⌉t HRAM DATA,R/W. It follows that the time to serve one word is LWLLC βi · ⌈\
    \ <sup>β</sup><sup>i</sup> LWLLC ⌉ · t HRAM DATA,R/W. Summing it with the data\
    \ time of the LLC (t MS-HIT DATA,R/W), we obtain the following upper bounds for\
    \ case (ii):\n\n<span id=\"page-6-0\"></span>\n$$\\begin{split} t\\_{\\text{CTRL,R/W}}^{\\\
    text{MS-MESS-REF}} &= t\\_{\\text{CTRL,R}}^{\\text{LLC-MESS}} + \\left[ \\frac{\\\
    beta\\_i}{LW\\_{\\text{LLC}}} \\right] \\cdot (t\\_{\\text{CTRL,R}}^{\\text{HDAC}}\
    \ + t\\_{\\text{CTRL,R}}^{\\text{HERAM}}); \\\\ t\\_{\\text{DATA,R/W}}^{\\text{MS-MESS-REF}}\
    \ &= t\\_{\\text{DATA,R/W}}^{\\text{MS-HIT}} + \\frac{LW\\_{\\text{LLC}}}{\\beta\\\
    _i} \\cdot \\left[ \\frac{\\beta\\_i}{LW\\_{\\text{LLC}}} \\right] \\cdot t\\\
    _{\\text{DATA,R}}^{\\text{HRAM}}; \\end{split} \\tag{11}$$\n\nIf the eviction\
    \ is also required, ⌈ βi LWLLC ⌉ extra write transactions of length β<sup>i</sup>\
    \ are performed to save the evicted data. Following the same reasoning as earlier,\
    \ this batch of transactions will introduce ⌈ βi LWLLC ⌉(t HMC CTRL,W + t HRAM\
    \ CTRL,W) clock cycles to the control time and LWLLC βi · ⌈ <sup>β</sup><sup>i</sup>\
    \ LWLLC ⌉ ·t HRAM DATA,W to the data time. We sum these numbers to eq. [11](#page-6-0)\
    \ to upper bound the overall control and data time as follows:\n\n$$t\\_{\\text{CTIL,W/R}}^{\\\
    text{MS-MIS-REF-EV}} = t\\_{\\text{CTIL,W/R}}^{\\text{MS-MIS-REF}} + \\left[\\\
    frac{\\beta\\_i}{LW\\_{\\text{LLC}}}\\right] (t\\_{\\text{CTIL,W}}^{\\text{HMCC}}\
    \ + t\\_{\\text{CTIL,W}}^{\\text{HBM}});$$\n\n$$t\\_{\\text{DATA,W/R}}^{\\text{MS-MIS-REF-EV}}\
    \ = t\\_{\\text{DATA,W/R}}^{\\text{MS-MIS-REF}} + \\frac{LW\\_{\\text{LLC}}}{\\\
    beta\\_i} \\cdot \\left[\\frac{\\beta\\_i}{LW\\_{\\text{LLC}}}\\right] \\cdot\
    \ t\\_{\\text{DATA,W}}^{\\text{HBM}};$$\n\nConsider now the parallelism of the\
    \ main memory subsystem. This is defined by the LLC, which acts as an interface\
    \ with the rest of the platform, buffering up to DLLC FIFO read and write transactions.\
    \ This means that the maximum number of supported outstanding transactions is\
    \ as follows:\n\n$$\n\\chi\\_R^{MS} = \\chi\\_W^{MS} = D\\_{\\rm FIFO}^{\\rm LLC}\
    \ \\tag{13}\n$$\n\nThe LLC is pipelined: in the case all the enqueued accesses\
    \ are hits, there is no stalling. However, the memory controller handles only\
    \ one transaction at a time, stalling the preceding ones, and only serves one\
    \ read or one write at a time. Hence, as soon as one access is a miss, ρ MS =\
    \ 0 and θ MS = 0.\n\n#### <span id=\"page-7-2\"></span>**4.5 AXI host crossbar**\n\
    \nThe AXI host crossbar under analysis is a consolidated AXI crossbar already\
    \ validated in multiple silicon tapeouts [\\[16\\]](#page-13-7), [\\[15\\]](#page-13-10),\
    \ [\\[24\\]](#page-13-16). We apply here the generic model for the *crossbar*\
    \ proposed in Section [3.4.](#page-2-3) The crossbar is referred as R0.\n\n####\
    \ *4.5.1 RTL IP structure*\n\nAs detailed in Figure [8,](#page-7-1) the crossbar\
    \ exports a set of input subordinate ports (S) and output manager ports (M). Each\
    \ S port is connected to a demultiplexer, which routes the incoming AW and AR\
    \ requests and W data to the proper destination. Each M port is connected to a\
    \ multiplexer, which (i) arbitrates AW and AR requests directed to the same *peripheral*,\
    \ (ii) connects the selected W channel from the *controller* to the *peripheral*,\
    \ and (iii) routes back the R read data and B write responses. The crossbar under\
    \ analysis can be configured for a fully combinatorial (i.e., decoding and routing\
    \ operations in one clock cycle) or pipelined structure with up to three pipeline\
    \ stages. In the platform under analysis, it is configured to be fully combinatorial.\
    \ ble of granting one AW and one AR request for each clock CUT\n\n#### *4.5.2\
    \ Delays and parallelism analysis*\n\nTo analyze the maximum propagation delays\
    \ introduced by the crossbar, we upper bound the overall latency on a transaction\
    \ by combining the delays introduced on each AXI channel. We provide two upper\
    \ bounds, one for transactions in isolation (i.e., t R<sup>0</sup> PROP,R/W as\
    \ defined in Section [3\\)](#page-2-0) and the other for transactions under contention\
    \ (i.e., t R<sup>0</sup> PROP,R/W+t R<sup>0</sup> CON,R/W as defined in Section\
    \ [3\\)](#page-2-0). We will use both of them in our architectural analysis reported\
    \ in Section [5.](#page-7-0)\n\n*Maximum delays in isolation:* Thanks to the combinatorial\
    \ structure, it is guaranteed by design that a request for a transaction, a data\
    \ word, or a write response crosses the crossbar in one clock cycle (t R<sup>0</sup>\
    \ CK). Consider a whole AXI transaction. For a read transaction, the crossbar\
    \ is crossed twice: on the AR and R AXI channels, respectively. For each AXI write\
    \ transaction, the crossbar is crossed two times: the first time is crossed by\
    \ the AW and W beats (propagated in parallel), and the second time by the B response.\
    \ Thus, the propagation delays in isolation are equal to:\n\n<span id=\"page-7-3\"\
    ></span>\n$$t\\_{\\text{PROP}\\text{R}/\\text{W}}^{R\\_0} = 2 \\cdot t\\_{\\text{CK}}^{R\\\
    _0};\\tag{14}$$\n\n*Maximum delays under contention:* Under contention, multiple\
    \ *controllers* connected to the crossbar can attempt to concurrently send requests\
    \ to the same *peripheral*, generating interference. The arbiters deploy a round-robin\
    \ scheme capa-\n\n<span id=\"page-7-1\"></span>![](_page_7_Figure_12.jpeg)\n\n\
    Fig. 8: AXI Crossbar block diagram\n\ncycle. In the worst-case scenario, the request\
    \ under analysis loses the round-robin and is served last, experiencing a delay\
    \ of MR<sup>0</sup> − 1 clock cycles (with MR<sup>0</sup> the number of *controller*\
    \ capable of interfering with the request under analysis). From the previous considerations,\
    \ the maximum propagation time introduced by the crossbar is upper bound by:\n\
    \n<span id=\"page-7-4\"></span>\n$$t\\_{\\text{CON,R}}^{R\\_0} = t\\_{\\text{CON,W}}^{R\\\
    _0} = M\\_{R\\_0} - 1 \\tag{15}$$\n\nConsider now the parallelism. Concerning\
    \ reads, the crossbar does not keep track of the inflight transactions. To route\
    \ the responses back, it appends information to the AXI ID. Doing so does not\
    \ limit the maximum number of outstanding transactions. The behavior is different\
    \ for writes: AXI enforces a strict in-order execution of write transactions (see\
    \ [\\[29\\]](#page-13-21) p. 98). This requires the crossbar to implement a table\
    \ to know the order of granted transactions. The maximum number of outstanding\
    \ write transactions per S port is limited by the depth of such tables, refereed\
    \ as D R<sup>0</sup> TAB. From the previous consideration: χ R<sup>0</sup> <sup>W</sup>\
    \ = D R<sup>0</sup> TAB. In the architecture under analysis, χ R<sup>0</sup> <sup>W</sup>\
    \ is set to be bigger than the parallelism supported by the *peripherals* so that\
    \ the crossbar does not limit the overall parallelism of the system.\n\n# <span\
    \ id=\"page-7-0\"></span>**5 SYSTEM-LEVEL WORST-CASE RESPONSE TIME ANALYSIS**\n\
    \nThis section introduces our system-level end-to-end analysis to upper bound\
    \ the overall response times of read and write transactions issued by a generic\
    \ *controller* and directed to a generic *peripheral*, considering the maximum\
    \ interference generated by the other *controllers* in the system. Our approach\
    \ is static [\\[34\\]](#page-13-26) and compositional [\\[35\\]](#page-13-27).\
    \ Specifically, we leverage the component-level static analysis introduced in\
    \ Section [4](#page-3-0) to then compose, step-by-step, the system-level worst-case\
    \ service time of transactions traversing the whole architecture.\n\nWe make an\
    \ assumption aligned with the SoA [\\[3\\]](#page-12-3), [\\[4\\]](#page-12-4),\
    \ [\\[8\\]](#page-13-0), [\\[11\\]](#page-13-2), [\\[12\\]](#page-13-3), [\\[36\\\
    ]](#page-13-28) to ensure independence among *peripherals* while not compromising\
    \ the generality of the analysis. It is assumed that multiple outstanding transactions\
    \ of the same type (either read or write) issued by the same *controller* target\
    \ the same *peripheral*: before issuing a transaction targeting a *peripheral*\
    \ P<sup>j</sup> , a *controller* completes the pending transactions of the same\
    \ type targeting a different *peripheral* Pz. Without such an assumption, due\
    \ to the strict ordering imposed by the AXI standard [\\[29\\]](#page-13-21) on\
    \ the W channel, and the structure of some *peripherals* generating interference\
    \ between reads and writes (i.e., ρ <sup>P</sup><sup>j</sup> = 0), transactions\
    \ issued by C<sup>k</sup> and directed to P<sup>j</sup> might interfere with transactions\
    \ issued by C<sup>i</sup> and directed to Pz, if C<sup>i</sup> also issues in\
    \ parallel transactions\n\nto P<sup>j</sup> , and vice-versa. This assumption\
    \ allows us to relax our analysis, removing such pathological cases. It is worth\
    \ noticing that it does not enforce any relationship between read and write transactions.\
    \ Such an assumption can either be enforced at the software level or at the hardware\
    \ level. The results of our analysis can be extended to such corner cases if required.\
    \ We leave this exploration for future works.\n\nThe first step of the analysis\
    \ is to bound the overall response time of a transaction in isolation (Lemma [1\\\
    )](#page-8-0). Secondly, we bound the maximum number of transactions that can\
    \ interfere with a transaction under analysis, either of the same type (e.g.,\
    \ reads interfering with a read, Lemma [2\\)](#page-8-1) or of a different type\
    \ (e.g., write interfering with a read, and vice versa, Lemma [3\\)](#page-8-2).\
    \ Lemma [4](#page-8-3) bounds the maximum temporal delay each interfering transaction\
    \ can delay a transaction under analysis. Finally, Theorem [1](#page-9-1) combines\
    \ the results of all the lemmas to upper bound the overall worstcase response\
    \ time of a transaction under analysis under interference. We report the lemmas\
    \ in a general form. AXi,j can represent either a read or write transaction issued\
    \ by the generic *controller* C<sup>i</sup> and directed to the generic *peripheral*\
    \ P<sup>j</sup> . The *crossbar* is referred to as R0. To make our analysis general,\
    \ we assume that Ψ<sup>j</sup> = [C0, ..., CM−1] is the generic set of interfering\
    \ *controllers* capable of interfering with C<sup>i</sup> issuing transactions\
    \ to P<sup>j</sup> and that that a generic set of *bridges* Θ<sup>i</sup> = {Q0,\
    \ ..., Qq−1} can be present between each *controller* C<sup>i</sup> and the crossbar\
    \ R0. The cardinality of Ψ<sup>j</sup> is referred to as \f Ψ<sup>j</sup> \f and\
    \ corresponds to the number of *controllers* interfering with AXi,j .\n\n<span\
    \ id=\"page-8-0\"></span>**Lemma 1.** *The response time in isolation of* AXi,j\
    \ *is upper bounded by:*\n\n<span id=\"page-8-4\"></span>\n$$d\\_{i,j}^{X} = d\\\
    _{\\mathbb{R}\\mathcal{W}}^{P\\_j} + \\sum\\_{Q\\_l \\in \\Theta\\_{i,j}} d\\\
    _{\\mathbb{R}\\mathcal{W}}^{Q\\_l} + d\\_{\\mathbb{R}\\mathcal{W}}^{R\\_0} \\\
    tag{16}$$\n\n*Proof.* Section [4](#page-3-0) upper bounds the worst-case delays\
    \ in isolation introduced by each component in the platform. According to their\
    \ definition, such delays account for all of the phases of the transaction. The\
    \ components in the platform are independent of each other. Thus, the delay introduced\
    \ by each traversed component is independent of the behavior of the other components.\
    \ It derives that the overall delay incurred in traversing the set of components\
    \ between C<sup>i</sup> and P<sup>j</sup> is upper bounded by the sum of the worst-case\
    \ delays introduced by all of the components in the set. Summing up the maximum\
    \ delay introduced by the target *peripheral* P<sup>j</sup> (d P<sup>j</sup> R/W),\
    \ by the set of traversed *bridges* Θ<sup>i</sup> , and by the *crossbar* R<sup>0</sup>\
    \ (d R<sup>0</sup> R/W), the lemma derives.\n\n<span id=\"page-8-1\"></span>**Lemma\
    \ 2.** *The maximum number of transactions of the same type that can interfere\
    \ with* AXi,j *is upper bounded by:*\n\n$$S\\_{i,j}^X = \\min\\left(\\sum\\_{C\\\
    _y \\in \\Psi\\_j} \\phi\\_X^{C\\_y}, \\chi\\_X^{P\\_j} + \\mid \\Psi\\_j \\mid\
    \ \\right) \\tag{17}$$\n\n*Proof.* The min in the formula has two components.\
    \ As from the AXI standard definition, an interfering *controller* C<sup>k</sup>\
    \ cannot have more than ϕ C<sup>k</sup> <sup>X</sup> pending outstanding transactions.\
    \ This means that summing up the maximum number of outstanding transactions for\
    \ each interfering *controller* in Ψ<sup>j</sup> provides an upper bound on the\
    \ number of transactions of the same type interfering with AXi,j – the left member\n\
    \nof the min derives. From our *peripheral* analysis reported in Section [4,](#page-3-0)\
    \ P<sup>j</sup> and R<sup>0</sup> can limit the maximum amount of transactions\
    \ accepted by the system: P<sup>j</sup> accepts overall at most χ P<sup>j</sup>\
    \ R/W transactions – when such a limit is reached, any further incoming transaction\
    \ directed to P<sup>j</sup> is stalled. After P<sup>j</sup> serves a transaction,\
    \ R<sup>0</sup> restarts forwarding transactions to the *peripheral* following\
    \ a round-robin scheme (see Section [4\\)](#page-3-0). In the worst-case scenario,\
    \ C<sup>i</sup> loses the round-robin arbitration against all of the \f Ψ<sup>j</sup>\
    \ interfering *controllers* in Ψ<sup>j</sup> , each ready to submit an interfering\
    \ request. Summing up the contributions, also χ P<sup>j</sup> <sup>R</sup> + \f\
    \ Ψ<sup>j</sup> \f upper bounds the maximum number of transactions interfering\
    \ with AXi,j – the right member of the min derives. Both of the bounds are valid\
    \ – the minimum between them is an upper bound providing the least pessimism –\
    \ Lemma [2](#page-8-1) derives.\n\n<span id=\"page-8-2\"></span>**Lemma 3.** *The\
    \ maximum number of transactions of a different type (represented here as Y, i.e.,\
    \ write transactions interfering with a read under analysis, and vice versa) interfering\
    \ with* AXi,j *is upper bounded by:*\n\n$$U\\_{i,j}^{Y} = (S\\_{i,j}^{X} + 1)\
    \ \\cdot (1 - \\theta^{P\\_j}) \\tag{18}$$\n\n*Proof.* According to Section [4.5,](#page-7-2)\
    \ R<sup>0</sup> manages transactions of different types independently – thus,\
    \ no interference of this type is generated at the R<sup>0</sup> level. From Section\
    \ [3,](#page-2-0) θ <sup>P</sup><sup>j</sup> = 1 represents the case in which\
    \ the *peripheral* is capable of serving read and write transactions in parallel\
    \ (e.g., the SPM *peripheral*, Section [4.2\\)](#page-3-3). Thus, no interference\
    \ is generated among them – the second equation derives. From Section [3,](#page-2-0)\
    \ θ <sup>P</sup><sup>j</sup> = 0 represents the case in which P<sup>j</sup> does\
    \ not feature parallelism in serving read and write transactions (i.e., also write\
    \ transactions interfere with reads, e.g., main memory subsystem, Section [4.4\\\
    )](#page-5-1). Considering lemma [2,](#page-8-1) at most S X i,j transactions\
    \ of the same type can interfere with AXi,j . With θ <sup>P</sup><sup>j</sup>\
    \ = 0, and assuming a round-robin scheme arbitrating between reads and writes\
    \ at the *peripheral* level, each one of the S X i,j interfering transaction of\
    \ the same type can be preceded by a transaction of the opposite type, which can,\
    \ therefore, create interference. The same applies to AXi,j , which can lose the\
    \ arbitration at the *peripheral* level as well. Summing up the contribution,\
    \ it follows that S X i,j + 1 can overall interfere with AXi,j – the first equation\
    \ derives.\n\n<span id=\"page-8-3\"></span>**Lemma 4.** *The maximum time delay\
    \ that a transaction of any kind* AXk,j *issued by the generic interfering* controller\
    \ C<sup>k</sup> *can cause on* AXi,j *is upper bounded by:*\n\n<span id=\"page-8-5\"\
    ></span>\n$$\n\\Delta\\_{k,j} = d\\_{\\text{R\\\\$}\\mathcal{W}}^{R\\_0} + (1\
    \ - \\rho^{P\\_j}) \\cdot t\\_{\\text{CTRL,R\\\\$}\\mathcal{W}}^{P\\_j} + t\\\
    _{\\text{DATT},\\text{R\\\\$}\\mathcal{W}}^{P\\_j} \\cdot \\beta\\_k \\tag{19}\n\
    $$\n\n*Proof.* In traversing the path between C<sup>k</sup> and P<sup>j</sup>\
    \ , AXk,j shares a portion of the path with AXi,j , i.e., the target *peripheral*\
    \ P<sup>j</sup> and the crossbar R<sup>0</sup> – no *bridges* from Θ<sup>k</sup>\
    \ belongs to the shared path, thus the delay propagation of AXk,j do not contribute\
    \ in delaying AXk,j . Considering the delay generated by AXk,j at R0, this is\
    \ upper bounded by d R<sup>0</sup> R/W in Section [3.4.](#page-2-3) As from Section\
    \ [3.3,](#page-2-2) t P<sup>j</sup> CTRL,R/W + t P<sup>j</sup> DATA,R/W · β<sup>k</sup>\
    \ is the maximum service time of P<sup>j</sup> for the transaction AXk,j and upper\
    \ bounds the maximum temporal delay that AXk,j can cause on AXi,j at P<sup>j</sup>\
    \ . As from the definition of an interfering transaction, AXk,j is served by P<sup>j</sup>\
    \ before AXi,j . As defined by the model in Section [3.3,](#page-2-2) when ρ <sup>P</sup><sup>j</sup>\
    \ = 1, the *peripheral* works in a pipeline fashion. This means that\n\nfor ρ\
    \ <sup>P</sup><sup>j</sup> = 1, the control time t P<sup>j</sup> CTRL,R/W of an\
    \ interfering transaction is pipelined and executed in parallel with the transaction\
    \ under analysis. Differently, when ρ <sup>P</sup><sup>j</sup> = 0, no pipeline\
    \ is implemented, and the control time of the interfering transaction can partially\
    \ or totally interfere with the transaction under analysis. From the previous\
    \ considerations, the contribution (1 − ρ <sup>P</sup><sup>j</sup> )·t P<sup>j</sup>\
    \ CTRL,R/W + t P<sup>j</sup> DATA,R/W · β<sup>k</sup> derives. Summing up the\
    \ contributions, the lemma follows.\n\n<span id=\"page-9-1\"></span>**Theorem\
    \ 1.** *The overall response time of* AXi,j *under the interference generated\
    \ by the other* controllers *in the system is upper bounded by:*\n\n<span id=\"\
    page-9-3\"></span>\n$$H\\_{i,j}^X = d\\_{i,j}^X + (S\\_{i,j}^X + U\\_{i,j}^Y)\
    \ \\cdot \\Delta\\_{k,j} \\tag{20}$$\n\n*Proof.* Summing up the contribution in\
    \ isolation for AXi,j (Lemma [1\\)](#page-8-0) with the sum of the maximum number\
    \ of interfering transactions of the same type (Lemma [2\\)](#page-8-1) and of\
    \ a different type (Lemma [3\\)](#page-8-2) multiplied by the maximum delay generated\
    \ by each interfering transaction (Lemma [4\\)](#page-8-3), Theorem [1](#page-9-1)\
    \ derives.\n\nThe results presented in this Section represent analytical upper\
    \ bounds derived through static code analysis and the formulation of mathematical\
    \ proofs. Section [6](#page-9-0) will validate them through a comprehensive set\
    \ of cycle-accurate experiments and measurements.\n\n# <span id=\"page-9-0\"></span>**6\
    \ EXPERIMENTAL VALIDATION**\n\nThis Section describes the experimental campaign\
    \ we conducted to validate the methodology and models. The aim of the experimental\
    \ campaign is to assess that the results presented in the previous Sections correctly\
    \ upper bound the maximum delays and response times at the component level and\
    \ the architectural level. We follow a hierarchical approach: at first, Section\
    \ [6.1](#page-9-2) aims to validate the results at the component level we proposed\
    \ in Section [4.](#page-3-0) Following, in Section [6.2,](#page-10-0) we experimentally\
    \ validate the system-level analysis we proposed in Section [5.](#page-7-0) The\
    \ experiments are conducted in a simulated environment (leveraging the Siemens\
    \ QuestaSIM simulator) and by deploying the design on an FPGA platform. In the\
    \ simulated experiments, we deploy custom AXI managers for *ad-hoc* traffic generation\
    \ and cycle-accurate performance monitors. The generic custom manager represents\
    \ a generic configurable *controller* C<sup>i</sup> issuing requests for transactions\
    \ – we will refer to that as GC<sup>i</sup> . In the FPGA, we leverage CVA6 and\
    \ the PULP cluster to generate the traffic with synthetic software benchmarks\
    \ and rely on their performance-monitoring registers to collect the measurements.\
    \ The experimental designs are deployed on the AMD-Xilinx VCU118, using the Vitis\
    \ 2022.1 toolchain. Similar State-of-the-Art works upper bounding the execution\
    \ time of a single transaction leverage synthetic benchmarks to measure the worst-case\
    \ access times since generic applications fail to do so [\\[8\\]](#page-13-0)–[\\\
    [10\\]](#page-13-1). For this reason, we concentrate on synthetic benchmarks at\
    \ the IP and the system level.\n\n#### <span id=\"page-9-2\"></span>**6.1 Component-level\
    \ hardware modules**\n\n#### *6.1.1 Delays analysis*\n\nThis subsection presents\
    \ the tests run to measure the worstcase access latency time in isolation for\
    \ the *peripherals* (d P<sup>j</sup> R/W ), 10\n\nthe *crossbar* (d R<sup>0</sup>\
    \ R/W ) and the *bridges* (d Q<sup>j</sup> R/W ) from Section [4.](#page-3-0)\
    \ We connect the generic controller CG<sup>i</sup> to the IP under analysis for\
    \ these experiments. We let CG<sup>i</sup> issue 100'000 transactions, one at\
    \ a time, with random burst length (βi). We monitor the service times and then\
    \ pick the longest ones for different β<sup>i</sup> .\n\nFigure [9](#page-10-1)\
    \ compares the maximum measured experimental delays with the upper bound proposed\
    \ in Section [4.](#page-3-0) Figure [9\\(](#page-10-1)a) reports the maximum service\
    \ time of the main memory subsystem in case of a miss as a function of the burst\
    \ length of the transaction under analysis, either when (i) only a refill is necessary\
    \ and (ii) both refill and eviction are necessary, compared with the bounds proposed\
    \ in Section [4.4.](#page-5-1) The measured service times are lower than the bounds.\
    \ The pessimism is between 3% and 10.1%; the larger β<sup>i</sup> , the higher\
    \ the pessimism. Higher pessimism on longer transactions is due to the internal\
    \ splitting at the LLC. As from our analysis, the memory subsystem is not fully\
    \ pipelined (ρMS = 0). However, in practice, the control and data phases of consecutive\
    \ sub-transactions might be partially served in parallel by the LLC and the memory\
    \ controller. This means that the longer the transaction, the higher the number\
    \ of sub-transactions and their overlap, and the lower the service time compared\
    \ to our model. Thus, the pessimism increases. Figure [9\\(](#page-10-1)b) reports\
    \ the measured results on the main memory subsystem but in case of a hit, compared\
    \ with the bounds proposed in Section [4.4.](#page-5-1) As we consider an LLC\
    \ hit, the access to the HyperRAM is not performed: this test analyzes the service\
    \ time of the LLC. Our bounds are always upper bounds for the maximum measured\
    \ results. The trend here is reversed w.r.t. Figure [9\\(](#page-10-1)a) – as\
    \ β<sup>i</sup> increases, the relative pessimism decreases from 7.7% down to\
    \ 0.4%. In this case, the source of the pessimism comes only from the control\
    \ time, which does not depend on β<sup>i</sup> , while there is no pessimism on\
    \ the data time. Hence, this pessimism gets amortized as the burst length and\
    \ the overall service time increase. We conduct the same experimental campaign\
    \ also on the AXI SPM – the measured results, compared with the bounds proposed\
    \ in Section [4.2,](#page-3-3) are reported in Figure [9\\(](#page-10-1)c). The\
    \ trends are similar to the ones reported in Figure [9\\(](#page-10-1)b) for LLC\
    \ hits – the pessimism of our analysis is limited to 1 and 2 clock cycles for\
    \ reads and writes on the control time, respectively. As in the case of the LLC\
    \ HITs, the upper bound on the control overhead gets amortized for longer transactions,\
    \ and the pessimism reduces from 8.8% to 0.5%.\n\nFigure [9\\(](#page-10-1)d)\
    \ reports the maximum measured latency to cross an AXI CDC FIFO as a function\
    \ of the manager clock period (the subordinate clock period is fixed to 30 ns)\
    \ and compared with the bounds proposed in Section [4.1.](#page-3-4) The results\
    \ are independent of the length of the transaction. To stimulate the highest variability,\
    \ the phases of the clocks are randomly selected on a uniform distribution. The\
    \ first bar reports the crossing delays from the manager to the subordinate side,\
    \ corresponding to the delays introduced on the AW, W, and AR AXI channels. The\
    \ second bar reports the crossing delays from the subordinate to the manager side,\
    \ corresponding to the overall delays on the AXI R and B channels. The third bar\
    \ shows the overall delay on a complete transaction, corresponding to the sum\
    \ of the two previously introduced contributions (see Section [4.1\\)](#page-3-4).\
    \ The pessimism of our bounds is, at most, one clock cycle of the slowest clock\
    \ between manager and subordinate.\n\n<span id=\"page-10-1\"></span>![](_page_10_Figure_0.jpeg)\n\
    \nFig. 9: Services time in isolation.\n\nFigure [9\\(](#page-10-1)e) reports the\
    \ measured propagation delays introduced by the crossbar over an entire write\
    \ and read transaction, compared with the bounds of Section [4.5,](#page-7-2)\
    \ varying the number of *controllers*. As explained in Section [4.5,](#page-7-2)\
    \ the propagation delay is the sum of the propagation latency without interference\
    \ (eq. [14\\)](#page-7-3) and the additional contention latency (eq. [15\\)](#page-7-4),\
    \ which depends on the number of *controllers*. Thanks to the simplicity of the\
    \ arbitration operated by the crossbar (pure round-robin), our proposed bounds\
    \ exactly match the measurements. We conducted the experimental campaign also\
    \ on the IO subsystem. We measured the maximum service time and compared it with\
    \ the upper bounds of Section [4.3,](#page-4-1) which we do not show for space\
    \ reasons: such IP supports only single-word transactions. Our upper bounds exceed\
    \ the maximum measured service time with pessimism of down to 2 clock cycles,\
    \ with service times of 4 (write) and 5 (read) clock cycles.\n\n#### *6.1.2 Parallelism*\n\
    \nWe also demonstrate our analysis of parallelism of the *peripherals* (χ P<sup>j</sup>\
    \ R/W ) and the *crossbar* (χ R<sup>0</sup> R/W ) analyzed in Section [4.](#page-3-0)\
    \ To do so, we configured CG<sup>i</sup> to issue unlimited outstanding transactions\
    \ to the *peripheral* under test. In parallel, we monitor the maximum number of\
    \ accepted outstanding transactions. Our measurements match our analysis: the\
    \ maximum number of outstanding transactions is defined by the maximum parallelism\
    \ accepted at the input stage of the peripherals and the crossbar.\n\n#### <span\
    \ id=\"page-10-0\"></span>**6.2 System-level experiments**\n\nWhile the previous\
    \ experiments focused on the evaluation at the IP level, this set of experiments\
    \ aims to evaluate the system-level bounds proposed in Section [5.](#page-7-0)\
    \ We first validate our analysis in simulation. We developed a System Verilog\
    \ testbench with two configurable AXI synthetic *controllers* CG<sup>i</sup> connected\
    \ to the target architecture (see Figure [2\\)](#page-1-2) stimulating overload\
    \ conditions to highlight worst-case scenarios. We also validate our results on\
    \ FPGA, generating traffic with CVA6 and the PULP cluster.\n\nAt first, we evaluate\
    \ the results in isolation *at the system level* as a function of the burst length,\
    \ leveraging the same strategy used for the previous experiments. Namely, these\
    \ tests are meant to validate Lemma [1](#page-8-0) (eq. [16\\)](#page-8-4). To\
    \ measure the service time at the system level in isolation, we let one GC<sup>i</sup>\
    \ issue 100'000 transactions, one at a time, with different β<sup>i</sup> , while\
    \ the other GC<sup>k</sup> is inactive. We monitor the service times and then\
    \ pick the longest ones for each\n\nβi . Figures [10](#page-11-1) (a) and (b)\
    \ report the maximum measured system-level response times in isolation for completing\
    \ a transaction issued by the generic *controller* GC<sup>i</sup> and directed\
    \ to (a) the main memory subsystem (case of cache miss, causing either refill\
    \ or both refill and eviction) and (b) to the SPM memory, compared with the bounds\
    \ proposed in Lemma [1.](#page-8-0) The measured service times are smaller than\
    \ the bounds in all the tested scenarios. The measure and the trends reported\
    \ in Figure [10\\(](#page-11-1)a) are aligned with the ones found at the IP level\
    \ and reported in Figure [9\\(](#page-10-1)a). This is because the overhead introduced\
    \ by the crossbar (in isolation) and the CDC FIFOs is negligible compared to the\
    \ memory subsystem's service time. Figure [10\\(](#page-11-1)b) shows a trend\
    \ aligned with the results at the IP-level reported in Figure [9\\(](#page-10-1)c):\
    \ the lower β<sup>i</sup> , the higher the pessimism. It is worth mentioning that\
    \ the analysis shows higher pessimism at the system level than at the IP level.\
    \ This is due to the extra pessimism from the crossbar and the CDC, which is nevertheless\
    \ amortized on longer transactions, down to 1.9%.\n\nWe now analyze the results\
    \ under maximum interference, to verify the results of Lemma [2](#page-8-1) and\
    \ [3](#page-8-2) and Theorem [1.](#page-9-1) For these tests, the execution of\
    \ GC<sup>i</sup> (100'000 transactions, one at a time) receives interference by\
    \ *controller* GCk. β<sup>k</sup> is fixed and equal to β<sup>i</sup> , while\
    \ we vary the amount of outstanding transactions GC<sup>k</sup> can issue (ϕ CG<sup>k</sup>\
    \ R/W ). Figures [10](#page-11-1) (c), (d), and (e) report the maximum measured\
    \ systemlevel response times for completing a transaction issued by the generic\
    \ *controller* GC<sup>i</sup> and directed to (c) the main memory with an LLC\
    \ miss considering β<sup>i</sup> = 16, (d) the SPM memory, considering β<sup>i</sup>\
    \ = 16, and (e) the SPM memory, considering β<sup>i</sup> = 256, and compare them\
    \ with the upper bounds proposed in equation [20.](#page-9-3) Figures [10](#page-11-1)\
    \ (c), (d), and (e) verify the results of Lemma [2](#page-8-1) and [3:](#page-8-2)\
    \ when ϕ CG<sup>k</sup> R/W > χMS R/W (two bars on the right), the total service\
    \ time is defined by the parallelism of the peripheral itself – as expected, after\
    \ saturating the number of interfering transactions accepted by the peripheral,\
    \ the measured results are the same regardless of the increase of ϕ CG<sup>k</sup>\
    \ R/W . Differently, when ϕ CG<sup>k</sup> R/W ≤ χMS R/W , a reduced value of\
    \ ϕ CG<sup>k</sup> R/W corresponds to lower interference and response times. Figure\
    \ [10\\(](#page-11-1)c) refers to the case of an LLC miss under interference when\
    \ β<sup>k</sup> = 16. The results confirm the safeness of our analysis, which\
    \ correctly upper bounds the overall response times with a pessimism around 15%,\
    \ which is slightly higher than the pessimism of a transaction in isolation at\
    \ the system level. As explained in the previous subsection, when multiple transactions\
    \ are enqueued, the memory subsystem can partially serve their data and control\n\
    \n12\n\n<span id=\"page-11-1\"></span>![](_page_11_Figure_1.jpeg)\n\nFig. 10:\
    \ Services times under interference.\n\nphases in parallel. However, our model\
    \ only allows ρMS = 1 or ρMS = 0, i.e., either the *peripheral* is fully pipelined\
    \ or not pipelined at all. Since ρMS = 0, the pessimism is slightly higher when\
    \ more transactions are enqueued (and partially served in parallel) as equation\
    \ [19](#page-8-5) counts the service time of a transaction fully when ρMS = 0.\
    \ Varying β<sup>k</sup> of GC<sup>k</sup> gives comparable results – we do not\
    \ report such results for briefness and lack of space. We provide two charts for\
    \ the SPM, in Figure [10\\(](#page-11-1)d) and Figure [10\\(](#page-11-1)e). The\
    \ comparison of the two charts highlights how the interfering transactions' length\
    \ impacts the analysis's pessimism, ranging between 19.7% for β = 16 to 1% for\
    \ β = 256. The trend here is aligned with the service time at the system level\
    \ in isolation: the pessimism comes from the control times of SPM and propagation\
    \ latency of the crossbar and the CDC, which are amortized as the data time increases\
    \ with βk.\n\n#### **6.3 Discussion**\n\nIn this Section, we validated the analysis\
    \ of Sections [4](#page-3-0) and [5](#page-7-0) through an extensive set of tests.\
    \ We demonstrated how the proposed approach enables detailed explanations of the\
    \ analysis's pessimism and facilitates iterative refinement. This allows us to\
    \ derive upper bounds that are safe yet not overly pessimistic, particularly when\
    \ compared to similar stateof-the-art works based on closed-source or loosely-timed\
    \ IPs. Nevertheless, while the methodology is promising, the resulting analysis\
    \ may seem limited in comparison to other works that model more sophisticated\
    \ closed-source IPs. Here, we discuss the limitations of our analysis, focusing\
    \ on its dependence on the underlying characteristics of the available open-source\
    \ hardware.\n\nIt is noteworthy how the analysis leverages the roundrobin policy\
    \ of the main interconnect and the in-order nature of *peripherals* in Lemmas\
    \ [2](#page-8-1) and [3.](#page-8-2) The absence of internal reordering allows\
    \ to derive the number of transactions preceding the one under interference directly\
    \ from the arbitration policy. As long as the *peripherals* serve the transactions\
    \ in order, extending the analysis to support other arbitration policies is expected\
    \ to require minimal effort. Instead, supporting *peripherals* with internal transaction\
    \ reordering can lead to *timing anomalies* [\\[7\\]](#page-12-2) and make the\
    \ proposed model unsafe, as previously demonstrated in [\\[5\\]](#page-12-1).\
    \ Our analysis focuses on the available *peripherals* within the target architecture,\
    \ as out-of-order *peripherals* are not available open-source to us. We envision\
    \ expanding the\n\nanalysis to match higher-performance platforms as opensource\
    \ hardware evolves.\n\nLastly, it is important to note that the analysis bounds\
    \ only a single transaction issued by C<sup>i</sup> – this limitation is not imposed\
    \ on the interfering controllers. Lemma [2](#page-8-1) does not consider C<sup>i</sup>\
    \ to have more pending transactions, except for the ones already accepted by P<sup>j</sup>\
    \ . In other words, Lemma [2](#page-8-1) assumes that there is not a queue of\
    \ transactions buffered in the *bridges* between C<sup>i</sup> and R0, which could\
    \ exist when P<sup>j</sup> is full. We could potentially extend the model to define\
    \ a batch of enqueued transactions and then modify Lemma [2](#page-8-1) to analyze\
    \ this scenario. Such an extension would further build upon the proposed model\
    \ and analysis, which is limited to bound the access time of a single transaction.\n\
    \n# <span id=\"page-11-0\"></span>**7 RELATED WORK**\n\nIn this Section, we provide\
    \ a thorough comparison with previous works focusing on enhancing the timing predictability\
    \ of digital circuits. Traditionally, the majority of these works leverage commercial\
    \ off-the-shelf devices [\\[34\\]](#page-13-26), [\\[38\\]](#page-13-29) or predictable\
    \ architectures modeled with a mix of cycleaccurate and behavioral simulators\
    \ [\\[39\\]](#page-13-30). Also, they focus on bounding the execution times for\
    \ predefined specific software tasks rather than the individual transaction service\
    \ times [\\[7\\]](#page-12-2), [\\[38\\]](#page-13-29)–[\\[40\\]](#page-13-31).\
    \ Furthermore, they build the models from dynamic experiments rather than from\
    \ static analysis, largely due to the dearth of detailed hardware specifications\
    \ [\\[35\\]](#page-13-27), limiting the generality of their approach. More recent\
    \ works advocate for static modeling and analysis of protocols [\\[8\\]](#page-13-0),\
    \ [\\[13\\]](#page-13-4), interconnect [\\[1\\]](#page-12-0), [\\[3\\]](#page-12-3),\
    \ [\\[9\\]](#page-13-5), and shared memory resources [\\[5\\]](#page-12-1), [\\\
    [10\\]](#page-13-1) to provide more generic and comprehensive models. While their\
    \ value is undeniable, due to the unavailability of the source RTL, each one focuses\
    \ on only one of these resources, resulting in a significant penalty to the pessimism\
    \ of the upper bounds [\\[5\\]](#page-12-1). Our work breaks from this convention,\
    \ presenting a holistic static model of an entire open-source architecture rigorously\
    \ validated through RTL cycle-accurate simulation and FPGA emulation. As Table\
    \ [1](#page-12-6) shows, this is the first work to analyze and model the open-source\
    \ siliconproven RTL of all the IPs composing a whole SoC to build the least pessimistic\
    \ upper bounds for data transfers within the architecture when compared to similar\
    \ SoA works.\n\nBiondi et al. [\\[13\\]](#page-13-4) developed a model of the\
    \ memory-access regulation mechanisms in the ARM MPAM and provided detailed instantiations\
    \ of such mechanisms, which they\n\n<span id=\"page-12-6\"></span>\n\n|  |  |\
    \  |  |  |  |  |  |  |  |  |  |  |  |  | TABLE 1: Comparison with State-of-the-Art\
    \ works for predictability. IC = Interconnect. DMR = Deadline miss ratio. |  |\
    \  |  |  |  |\n|--|--|--|--|--|--|--|--|--|--|--|--|--|--|--|-------------------------------------------------------------------------------------------------------------------|--|--|--|--|--|\n\
    |--|--|--|--|--|--|--|--|--|--|--|--|--|--|--|-------------------------------------------------------------------------------------------------------------------|--|--|--|--|--|\n\
    \n|                       | Target                             | Analysis on \
    \                   | Pessimism   | Technology  | Open RTL |\n|-----------------------|------------------------------------|--------------------------------|-------------|-------------|----------|\n\
    | Biondi et. al. [13]   | ARM MPAM Protocol                  | Protocol specs\
    \ (Model)         | No HW       | ✗           | ✗        |\n| Hassan et. al. [8]\
    \    | JEDEC DDR3 Protocol                | Protocol specs (Model)         | 0%\
    \ − 200%   | ✗           | ✗        |\n| Abdelhalim et.al. [5] | Whole mem. hier.\
    \                   | IPs & System (C++ Model)       | 16% − 50%   | ✗       \
    \    | ✗        |\n| BlueScale [3]         | Hier. mem. IC                   \
    \   | IC uArch (Black-box)           | DMR         | FPGA        | ✗        |\n\
    | AXI-RT-IC [1]         | AXI SoC IC                         | IC uArch (Black-box)\
    \           | DMR         | FPGA        | ✗        |\n| Restuccia et. al. [9]\
    \ | AXI Hier. mem. IC                  | IC uArch (Black-box)           | 50%\
    \ − 90%   | FPGA        | ✗        |\n| AXI-REALM [37]        | AXI traffic regulator\
    \              | No analysis                    | No model    | FPGA & ASIC |\
    \ ✓        |\n| Ditty [10]            | Cache coher. mechanism             | IP\
    \ (Fine-grained RTL)          | 100% − 200% | FPGA        | ✓        |\n| This\
    \ Work             | SoC IC, peripherals & system-level | IP & System (Fine-grained\
    \ RTL) | 1% − 28%    | FPGA & ASIC | ✓        |\n\nthen evaluated with IBM CPLEX,\
    \ a decision optimization software for solving complex optimization models. While\
    \ elegant, this approach is not validated on hardware and, therefore, is limited\
    \ in terms of applicability and precision. A more practical and adopted approach\
    \ is the one proposed by Hassan and Pellizzoni [\\[8\\]](#page-13-0). The authors\
    \ develop a finegrained model of the JEDEC DDR3 protocol, validated with MCsim\
    \ [\\[12\\]](#page-13-3), a cycle-accurate C++ memory controller simulator. Unfortunately,\
    \ not having access to the RTL prevents fine-grained modeling and analysis and\
    \ mandates over-provisioning, strongly impacting the overall pessimism of the\
    \ system, which can be as high as 200%. Abdelhalim et al. in [\\[5\\]](#page-12-1)\
    \ present a study bounding the access times of memory requests traversing the\
    \ entire memory hierarchy and propose µarchitectural modifications to the arbiters\
    \ in such hierarchy. Their modifications result in very low pessimism (down to\
    \ 16%) on synthetic and real-world benchmarks. However, the results are validated\
    \ on C++ models of the cores, interconnect, and memory controllers, not RTL code\
    \ targeting silicon implementation.\n\nMore recently, different researchers proposed\
    \ models of hardware IPs that they could validate through cycle-accurate experiments\
    \ [\\[1\\]](#page-12-0), [\\[4\\]](#page-12-4), [\\[9\\]](#page-13-5). In [\\\
    [9\\]](#page-13-5), Restuccia et al. focused on upper bounding the response times\
    \ of AXI bus transactions on FPGA SoCs through the modeling and analysis of generic\
    \ hierarchical interconnects arbitrating the accesses of multiple hardware accelerators\
    \ towards a shared DDR memory. In this work, the interconnect under analysis is\
    \ a proprietary Xilinx IP, which had to be treated as a black box. Also, due to\
    \ the unavailability of the RTL code, the authors did not model the other IPs\
    \ composing the target platform, limiting the precision of the proposed upper\
    \ bounds, which achieve a pessimism between 50% and 90%. Jiang et al. modeled,\
    \ analyzed, and developed AXI-ICRT [\\[1\\]](#page-12-0) and Bluescale [\\[3\\\
    ]](#page-12-3), two sophisticated interconnects providing predictability features\
    \ and coming with a comprehensive model. However, the model and analysis proposed\
    \ in AXI-ICRT [\\[1\\]](#page-12-0), and Bluescale [\\[3\\]](#page-12-3) are not\
    \ as fine-grained as ours: the authors do not provide upper bounds of the access\
    \ times but rather focus on the deadline miss ratio given a fixed workload for\
    \ the different controllers in the system. Moreover, the authors do not provide\
    \ the RTL of such solutions. AXI-REALM [\\[37\\]](#page-13-32) proposes completely\
    \ open-source IPs supporting predictable communications. However, it misses a\
    \ holistic model and analysis. In Ditty [\\[10\\]](#page-13-1), researchers propose\
    \ an open-source predictable directory-based cache coherence mechanism for multicore\
    \ safety-critical systems that guarantees a worst-case latency (WCL) on data accesses\
    \ with almost cycle-accurate precision. However, Ditty's model only covers the\
    \ coherency protocol latency and the core subsystem, overlooking systemlevel analysis\
    \ and achieving very pessimistic boundaries. In this landscape, it emerges clearly\
    \ that our work is the first one covering both modeling and analysis of the interconnects\
    \ and the shared memory resources, with an in-depth analysis of silicon-proven\
    \ open-source RTL IPs and achieving the lowest pessimism when compared to similar\
    \ SoA works.\n\n# <span id=\"page-12-5\"></span>**8 CONCLUSIONS**\n\nIn conclusion,\
    \ this is the first work to bridge the gap between open-source hardware and predictability\
    \ modeling and analysis. It presented (i) a fine-grained model and analysis for\
    \ the typical building blocks composing modern heterogeneous low-power SoCs directly\
    \ based on the source RTL, and (ii) a full mathematical analysis to upper bound\
    \ data transfer execution times. Namely, we demonstrated a methodology that successfully\
    \ exploits the availability of the source code to provide safe, but not overly\
    \ pessimistic, upper bounds for the execution times of data transfers when compared\
    \ to similar SoA works based on closed-source IPs.\n\nAs discussed in Section\
    \ [6,](#page-9-0) after this thorough evaluation, we envision extending our results\
    \ to other popular opensource IPs and different arbitration policies. To hopefully\
    \ stimulate novel research contributions, we open-source a guide to replicate\
    \ the results shown in Section [6](#page-9-0) at [https:](https://github.com/pulp-platform/soc_model_rt_analysis)\
    \ [//github.com/pulp-platform/soc](https://github.com/pulp-platform/soc_model_rt_analysis)\
    \ model rt analysis, comprehensive of the simulated environment and the software\
    \ benchmarks to run on a sophisticated Cheshire-based SoC targeting automotive\
    \ applications.\n\n# **REFERENCES**\n\n- <span id=\"page-12-0\"></span>[1] Z.\
    \ Jiang *et al.*, \"AXI-ICRT RT : Towards a Real-Time AXI-Interconnect for Highly\
    \ Integrated SoCs,\" *IEEE Transactions on Computers*, vol. 72, no. 3, pp. 786–799,\
    \ 2022.\n- [2] A. Biondi *et al.*, \"SPHERE: A multi-SoC architecture for nextgeneration\
    \ cyber-physical systems based on heterogeneous platforms,\" *IEEE Access*, vol.\
    \ 9, pp. 75 446–75 459, 2021.\n- <span id=\"page-12-3\"></span>[3] Z. Jiang *et\
    \ al.*, \"BlueScale: a scalable memory architecture for predictable real-time\
    \ computing on highly integrated SoCs,\" in *Proceedings of the 59th ACM/IEEE\
    \ Design Automation Conference*, 2022, pp. 1261–1266.\n- <span id=\"page-12-4\"\
    ></span>[4] F. Restuccia *et al.*, \"AXI HyperConnect: A Predictable, Hypervisorlevel\
    \ Interconnect for Hardware Accelerators in FPGA SoC,\" in *2020 57th ACM/IEEE\
    \ Design Automation Conference (DAC)*, 2020.\n- <span id=\"page-12-1\"></span>[5]\
    \ S. Abdelhalim *et al.*, \"A Tight Holistic Memory Latency Bound Through Coordinated\
    \ Management of Memory Resources,\" in *35th Euromicro Conference on Real-Time\
    \ Systems (ECRTS 2023)*, vol. 262, 2023, pp. 17:1–17:25.\n- [6] G. Fernandez *et\
    \ al.*, \"Contention in multicore hardware shared resources: Understanding of\
    \ the state of the art,\" in *Proceedings of the 14th International Workshop on\
    \ Worst-Case Execution Time Analysis (WCET 2014)*, 2014, pp. 31–42.\n- <span id=\"\
    page-12-2\"></span>[7] S. Hahn, M. Jacobs, and J. Reineke, \"Enabling Compositionality\
    \ for Multicore Timing Analysis,\" in *Proceedings of the 24th International Conference\
    \ on Real-Time Networks and Systems*. Association for Computing Machinery, 2016,\
    \ p. 299–308.\n- <span id=\"page-13-0\"></span>[8] M. Hassan and R. Pellizzoni,\
    \ \"Bounding DRAM Interference in COTS Heterogeneous MPSoCs for Mixed Criticality\
    \ Systems,\" *IEEE Transactions on Computer-Aided Design of Integrated Circuits\
    \ and Systems*, vol. 37, no. 11, pp. 2323–2336, 2018.\n- <span id=\"page-13-5\"\
    ></span>[9] F. Restuccia *et al.*, \"Bounding Memory Access Times in Multi-Accelerator\
    \ Architectures on FPGA SoCs,\" *IEEE Transactions on Computers*, vol. 72, no.\
    \ 1, pp. 154–167, 2022.\n- <span id=\"page-13-1\"></span>[10] Z. Wu, M. Bekmyrza,\
    \ N. Kapre, and H. Patel, \"Ditty: Directorybased Cache Coherence for Multicore\
    \ Safety-critical Systems,\" in *2023 Design, Automation & Test in Europe Conference\
    \ & Exhibition (DATE)*. IEEE, 2023, pp. 1–6.\n- <span id=\"page-13-2\"></span>[11]\
    \ M. Hassan, \"On the Off-Chip Memory Latency of Real-Time Systems: Is DDR DRAM\
    \ Really the Best Option?\" in *2018 IEEE Real-Time Systems Symposium (RTSS)*,\
    \ 2018, pp. 495–505.\n- <span id=\"page-13-3\"></span>[12] R. Mirosanlou, D. Guo,\
    \ M. Hassan, and R. Pellizzoni, \"Mcsim: An extensible dram memory controller\
    \ simulator,\" *IEEE Computer Architecture Letters*, vol. 19, no. 2, pp. 105–109,\
    \ 2020.\n- <span id=\"page-13-4\"></span>[13] M. Zini, D. Casini, and A. Biondi,\
    \ \"Analyzing Arm's MPAM From the Perspective of Time Predictability,\" *IEEE\
    \ Transactions on Computers*, vol. 72, no. 1, pp. 168–182, 2023.\n- <span id=\"\
    page-13-6\"></span>[14] A. Herrera, \"The Promises and Challenges of Open Source\
    \ Hardware,\" *Computer*, vol. 53, no. 10, pp. 101–104, 2020.\n- <span id=\"page-13-10\"\
    ></span>[15] A. Ottaviano, T. Benz, P. Scheffler, and L. Benini, \"Cheshire: A\
    \ Lightweight, Linux-Capable RISC-V Host Platform for Domain-Specific Accelerator\
    \ Plug-In,\" *IEEE Transactions on Circuits and Systems II: Express Briefs*, pp.\
    \ 1–1, 2023.\n- <span id=\"page-13-7\"></span>[16] L. Valente *et al.*, \"Shaheen:\
    \ An Open, Secure, and Scalable RV64 SoC for Autonomous Nano-UAVs,\" in *2023\
    \ IEEE Hot Chips 35 Symposium (HCS)*, 2023, pp. 1–12.\n- <span id=\"page-13-8\"\
    ></span>[17] M. B. Taylor, \"Your Agile Open Source HW Stinks (Because It Is Not\
    \ a System),\" in *2020 IEEE/ACM International Conference On Computer Aided Design\
    \ (ICCAD)*, 2020, pp. 1–6.\n- <span id=\"page-13-9\"></span>[18] PULP, \"PULP\
    \ Platform Github,\" [https://github.com/](https://github.com/pulp-platform) [pulp-platform,](https://github.com/pulp-platform)\
    \ 2022.\n- <span id=\"page-13-11\"></span>[19] L. Valente *et al.*, \"HULK-V:\
    \ a Heterogeneous Ultra-low-power Linux capable RISC-V SoC,\" in *2023 Design,\
    \ Automation & Test in Europe Conference & Exhibition (DATE)*, 2023, pp. 1–6.\n\
    - <span id=\"page-13-12\"></span>[20] OpenHW-Group, \"CVA6,\" [https://github.com/openhwgroup/](https://github.com/openhwgroup/cva6)\
    \ [cva6,](https://github.com/openhwgroup/cva6) 2022.\n- <span id=\"page-13-13\"\
    ></span>[21] M. Schneider *et al.*, \"Composite Enclaves: Towards Disaggregated\
    \ Trusted Execution,\" *IACR Transactions on Cryptographic Hardware and Embedded\
    \ Systems*, vol. 2022, no. 1, p. 630–656, Nov. 2021.\n- <span id=\"page-13-14\"\
    ></span>[22] P. Platform, \"PULP cluster,\" [https://github.com/pulp-platform/](https://github.com/pulp-platform/pulp_cluster)\
    \ pulp [cluster,](https://github.com/pulp-platform/pulp_cluster) 2022.\n- <span\
    \ id=\"page-13-15\"></span>[23] OpenHW-Group, \"CV32E40P,\" [https://github.com/](https://github.com/openhwgroup/cv32e40p)\
    \ [openhwgroup/cv32e40p,](https://github.com/openhwgroup/cv32e40p) 2023.\n- <span\
    \ id=\"page-13-16\"></span>[24] A. Kurth *et al.*, \"An Open-Source Platform for\
    \ High-Performance Non-Coherent On-Chip Communication,\" *IEEE Transactions on\
    \ Computers*, pp. 1–1, 2021.\n- <span id=\"page-13-17\"></span>[25] B. John, \"\
    HyperRAM as a low pin-count expansion memory for embedded systems,\" [https://www.infineon.com/dgdl/](https://www.infineon.com/dgdl/Infineon-HyperRAM_as_a_low_pin-count_expansion_memory_for_embedded_systems-Whitepaper-v01_00-EN.pdf?fileId=8ac78c8c7d0d8da4017d0fb28970272c&da=t)\
    \ [Infineon-HyperRAM](https://www.infineon.com/dgdl/Infineon-HyperRAM_as_a_low_pin-count_expansion_memory_for_embedded_systems-Whitepaper-v01_00-EN.pdf?fileId=8ac78c8c7d0d8da4017d0fb28970272c&da=t)\
    \ as a low pin-count expansion memory for embedded [systems-Whitepaper-v01](https://www.infineon.com/dgdl/Infineon-HyperRAM_as_a_low_pin-count_expansion_memory_for_embedded_systems-Whitepaper-v01_00-EN.pdf?fileId=8ac78c8c7d0d8da4017d0fb28970272c&da=t)\
    \ 00-EN.pdf?fileId= [8ac78c8c7d0d8da4017d0fb28970272c&da=t,](https://www.infineon.com/dgdl/Infineon-HyperRAM_as_a_low_pin-count_expansion_memory_for_embedded_systems-Whitepaper-v01_00-EN.pdf?fileId=8ac78c8c7d0d8da4017d0fb28970272c&da=t)\
    \ 2020.\n- <span id=\"page-13-18\"></span>[26] AMD, \"Zynq-7000 - Technical Reference\
    \ Manual, UG585,\" [https:](https://docs.xilinx.com/r/en-US/ug585-zynq-7000-SoC-TRM)\
    \ [//docs.xilinx.com/r/en-US/ug585-zynq-7000-SoC-TRM.](https://docs.xilinx.com/r/en-US/ug585-zynq-7000-SoC-TRM)\n\
    - <span id=\"page-13-19\"></span>[27] A. Noami, B. Pradeep Kumar, and P. Chandrasekhar,\
    \ \"High Performance AXI4 Interface Protocol for Multi-Core Memory Controller\
    \ on SoC,\" in *Data Engineering and Communication Technology*, K. A. Reddy, B.\
    \ R. Devi, B. George, and K. S. Raju, Eds. Singapore: Springer Singapore, 2021,\
    \ pp. 131–140.\n- <span id=\"page-13-20\"></span>[28] D. Rossi, I. Loi, G. Haugou,\
    \ and L. Benini, \"Ultra-low-latency lightweight dma for tightly coupled multi-core\
    \ clusters,\" in *Proceedings of the 11th ACM Conference on Computing Frontiers*,\
    \ ser. CF '14. New York, NY, USA: Association for Computing Machinery, 2014. [Online].\
    \ Available:<https://doi.org/10.1145/2597917.2597922>\n- <span id=\"page-13-21\"\
    ></span>[29] ARM, \"AMBA AXI Protocol Specification,\" [https://developer.arm.](https://developer.arm.com/documentation/ihi0022/j/?lang=en)\
    \ [com/documentation/ihi0022/j/?lang=en,](https://developer.arm.com/documentation/ihi0022/j/?lang=en)\
    \ 2022.\n- <span id=\"page-13-22\"></span>[30] Xilinx-AMD, \"Dual Port SRAM specifications,\"\
    \ [https://docs.xilinx.com/r/2022.1-English/](https://docs.xilinx.com/r/2022.1-English/ug1483-model-composer-sys-gen-user-guide/Dual-Port-RAM)\
    \ [ug1483-model-composer-sys-gen-user-guide/Dual-Port-RAM.](https://docs.xilinx.com/r/2022.1-English/ug1483-model-composer-sys-gen-user-guide/Dual-Port-RAM)\n\
    - <span id=\"page-13-23\"></span>[31] PULP, \"HyperRAM Controller RTL,\" [https://github.com/](https://github.com/pulp-platform/hyperbus)\
    \ [pulp-platform/hyperbus,](https://github.com/pulp-platform/hyperbus) 2022.\n\
    - <span id=\"page-13-24\"></span>[32] Infineon, \"HyperRAM RTL,\" [https://www.](https://www.infineon.com/dgdl/Infineon-S27KL0641_S27KS0641_VERILOG-SimulationModels-v05_00-EN.zip?fileId=8ac78c8c7d0d8da4017d0f6349a14f68)\
    \ [infineon.com/dgdl/Infineon-S27KL0641](https://www.infineon.com/dgdl/Infineon-S27KL0641_S27KS0641_VERILOG-SimulationModels-v05_00-EN.zip?fileId=8ac78c8c7d0d8da4017d0f6349a14f68)\
    \ S27KS0641 [VERILOG-SimulationModels-v05](https://www.infineon.com/dgdl/Infineon-S27KL0641_S27KS0641_VERILOG-SimulationModels-v05_00-EN.zip?fileId=8ac78c8c7d0d8da4017d0f6349a14f68)\
    \ 00-EN.zip?fileId= [8ac78c8c7d0d8da4017d0f6349a14f68,](https://www.infineon.com/dgdl/Infineon-S27KL0641_S27KS0641_VERILOG-SimulationModels-v05_00-EN.zip?fileId=8ac78c8c7d0d8da4017d0f6349a14f68)\
    \ 2022.\n- <span id=\"page-13-25\"></span>[33] Infineon, \"HyperBUS specifications,\"\
    \ [https://www.](https://www.infineon.com/dgdl/Infineon-HYPERBUS_SPECIFICATION_LOW_SIGNAL_COUNT_HIGH_PERFORMANCE_DDR_BUS-AdditionalTechnicalInformation-v09_00-EN.pdf?fileId=8ac78c8c7d0d8da4017d0ed619b05663)\
    \ [infineon.com/dgdl/Infineon-HYPERBUS](https://www.infineon.com/dgdl/Infineon-HYPERBUS_SPECIFICATION_LOW_SIGNAL_COUNT_HIGH_PERFORMANCE_DDR_BUS-AdditionalTechnicalInformation-v09_00-EN.pdf?fileId=8ac78c8c7d0d8da4017d0ed619b05663)\
    \ SPECIFICATION LOW SIGNAL COUNT HIGH [PERFORMANCE](https://www.infineon.com/dgdl/Infineon-HYPERBUS_SPECIFICATION_LOW_SIGNAL_COUNT_HIGH_PERFORMANCE_DDR_BUS-AdditionalTechnicalInformation-v09_00-EN.pdf?fileId=8ac78c8c7d0d8da4017d0ed619b05663)\
    \ DDR [BUS-AdditionalTechnicalInformation-v09](https://www.infineon.com/dgdl/Infineon-HYPERBUS_SPECIFICATION_LOW_SIGNAL_COUNT_HIGH_PERFORMANCE_DDR_BUS-AdditionalTechnicalInformation-v09_00-EN.pdf?fileId=8ac78c8c7d0d8da4017d0ed619b05663)\
    \ 00-EN.pdf?fileId= [8ac78c8c7d0d8da4017d0ed619b05663,](https://www.infineon.com/dgdl/Infineon-HYPERBUS_SPECIFICATION_LOW_SIGNAL_COUNT_HIGH_PERFORMANCE_DDR_BUS-AdditionalTechnicalInformation-v09_00-EN.pdf?fileId=8ac78c8c7d0d8da4017d0ed619b05663)\
    \ 2022.\n- <span id=\"page-13-26\"></span>[34] R. Wilhelm *et al.*, \"The worst-case\
    \ execution-time problem—overview of methods and survey of tools,\" *ACM Trans.\
    \ Embed. Comput. Syst.*, vol. 7, no. 3, may 2008. [Online]. Available: <https://doi.org/10.1145/1347375.1347389>\n\
    - <span id=\"page-13-27\"></span>[35] T. Mitra, J. Teich, and L. Thiele, \"Time-critical\
    \ systems design: A survey,\" *IEEE Design & Test*, vol. 35, no. 2, pp. 8–26,\
    \ 2018.\n- <span id=\"page-13-28\"></span>[36] F. Restuccia *et al.*, \"Modeling\
    \ and analysis of bus contention for hardware accelerators in FPGA SoCs,\" in\
    \ *32nd Euromicro Conference on Real-Time Systems (ECRTS 2020)*. Schloss Dagstuhl-Leibniz-Zentrum\
    \ fur Informatik, 2020. ¨\n- <span id=\"page-13-32\"></span>[37] B. Thomas *et\
    \ al.*, \"AXI-REALM: A Lightweight and Modular Interconnect Extension for Traffic\
    \ Regulation and Monitoring of Heterogeneous Real-Time SoCs,\" in *2024 Design,\
    \ Automation & Test in Europe Conference & Exhibition (DATE)*, 2024.\n- <span\
    \ id=\"page-13-29\"></span>[38] J. P. Cerrolaza *et al.*, \"Multi-Core Devices\
    \ for Safety-Critical Systems: A Survey,\" *ACM Comput. Surv.*, vol. 53, no. 4,\
    \ aug 2020. [Online]. Available:<https://doi.org/10.1145/3398665>\n- <span id=\"\
    page-13-30\"></span>[39] M. Schoeberl *et al.*, \"T-CREST: Time-predictable multi-core\
    \ architecture for embedded systems,\" *Journal of Systems Architecture*, vol.\
    \ 61, no. 9, pp. 449–471, 2015.\n- <span id=\"page-13-31\"></span>[40] G. Fernandez\
    \ *et al.*, \"Increasing confidence on measurement-based contention bounds for\
    \ real-time round-robin buses,\" in *Proceedings of the 52nd Annual Design Automation\
    \ Conference*, ser. DAC '15. New York, NY, USA: Association for Computing Machinery,\
    \ 2015.\n\n**Luca Valente** received the MSc degree in electronic engineering\
    \ from the Politecnico of Turin in 2020. He is currently a PhD student at the\
    \ University of Bologna in the Department of Electrical, Electronic, and Information\
    \ Technologies Engineering (DEI). His main research interests are hardware-software\
    \ co-design of heterogeneous SoCs.\n\n**Francesco Restuccia** received a PhD degree\
    \ in computer engineering (cum laude) from Scuola Superiore Sant'Anna Pisa, in\
    \ 2021. He is a postdoctoral researcher at the University of California, San Diego.\
    \ His main research interests include hardware security, on-chip communications,\
    \ timing analysis for heterogeneous platforms, cyber-physical systems, and time-predictable\
    \ hardware acceleration of deep neural networks on commercial FPGA SoC platforms.\n\
    \n**Davide Rossi** received the Ph.D. degree from the University of Bologna in\
    \ 2012. He has been a Post-Doctoral Researcher with the Department of Electrical,\
    \ Electronic and Information Engineering \"Guglielmo Marconi,\" University of\
    \ Bologna, since 2015, where he is currently an Associate Professor position.\
    \ His research interests focus on energy-efficient digital architectures. In this\
    \ field, he has published more than 100 papers in international peer-reviewed\
    \ conferences and journals.\n\n**Ryan Kastner** is a professor in the Department\
    \ of Computer Science and Engineering at UC San Diego. He received a PhD in Computer\
    \ Science at UCLA, a masters degree in engineering and bachelor degrees in Electrical\
    \ Engineering and Computer Engineering from Northwestern University. His current\
    \ research interests fall into three areas: hardware acceleration, hardware security,\
    \ and remote sensing.\n\n**Luca Benini** holds the chair of Digital Circuits and\
    \ Systems at ETHZ and is Full Professor at the Universita di Bologna. He received\
    \ a PhD from ` Stanford University. His research interests are in energy-efficient\
    \ parallel computing systems, smart sensing micro-systems and machine learning\
    \ hardware. He has published more than 1000 peer-reviewed papers and 5 books.\
    \ He is a Fellow of the ACM and a member of the Academia Europaea."
  decisions:
    evaluation_prompt: 'Qualified. Reason: The paper includes validation through RTL
      simulation and FPGA implementation, and mentions minimizing pessimism in bounding
      service time, indicating empirical analysis.'
    related_work_prompt: 'Qualified. Reason: The paper engages with prior research
      by citing numerous research papers, explaining and comparing to cited work,
      and includes a section that discusses related work.'
    novelty_prompt: 'Qualified. Reason: The paper introduces an innovative methodology
      for modeling and analyzing open-source SoCs, claims novelty in bridging the
      gap between open-source hardware and timing analysis, and provides new insights
      into reducing pessimism in bounding service time.'
    review_only_prompt: 'Qualified. Reason: The paper introduces a new methodology
      for modeling and analyzing open-source SoCs, providing novel contributions rather
      than summarizing existing work.'
- title: "Green Adaptation of Real-Time Web Services for Industrial CPS within a\n\
    \  Cloud Environment"
  abstract: 'Managing energy efficiency under timing constraints is an interesting
    and big

    challenge. This work proposes an accurate power model in data centers for

    time-constrained servers in Cloud computing. This model, as opposed to previous

    approaches, does not only consider the workload assigned to the processing

    element, but also incorporates the need of considering the static power

    consumption and, even more interestingly, its dependency with temperature. The

    proposed model has been used in a multi-objective optimization environment in

    which the Dynamic Voltage and Frequency Scaling (DVFS) and workload assignment

    have been efficiently optimized.'
  url: http://arxiv.org/abs/2401.16387v1
  keywords: Adaptive Systems, Cyber Physical Systems, Cloud Computing, Real-Time Systems,
    Energy efficiency, Industrial-based Services, Multi-Objective Optimization, Parallel
    Computing.
  document: '#### I. INTRODUCTION


    B OTH *Cyber Physical Systems* (CPSs) and *Cyber Physical Society* [\[1\]](#page-7-0)
    combine computing and networking power with physical components, enabling innovation
    in a wide range of domains related to future-generation sensor networks (e.g.,
    robotics, avionics, transportation, manufacturing processes, energy, smart homes
    and vehicles, medical implants, healthcare, etc). The design and implementation
    of CPS involve the consideration of multiple aspects like energy and tight realtime
    constraints. Because of that, real-time scheduling for CPS brings new research
    issues in the scope of real-time systems [\[2\]](#page-7-1).


    Managing energy efficiency under timing constraints is a big challenge. Most modern
    micro-controllers already provide support for various energy saving modes (e.g.,
    Intel Xeon and AMD Opteron). A common way of reducing dynamic power is to use
    the technique called *Dynamic Voltage and Frequency Scaling* (DVFS), which changes
    the processor voltage and the clock frequency simultaneously, reducing the energy
    consumption. Decreasing the processor voltage and frequency will slow down the
    performance of the processor. If the execution performance is not a hard constraint,
    then, decreasing both processor voltage and frequency allows to reduce the dynamic
    power consumption of the processor.


    Copyright (c) 2009 IEEE. Personal use of this material is permitted. However,
    permission to use this material for any other purposes must be obtained from the
    IEEE by sending a request to pubs-permissions@ieee.org.


    Nowadays, new embedded devices are collaborating in distributed environments.
    In this new scenario, tasks and resources are widely distributed and then, real-time
    applications become more complex and more relevant. A cloud datacenter usually
    contains a large group of servers connected through the Internet, and a scheduler
    has to make an efficiently use of the resources of the cloud to execute jobs.
    Since many applications require *Quality of Service* (QoS), power consumption
    in data centers must be minimized, satisfying the *Service Level Agreement* (SLA)
    constraints. Consequently, novel approaches that base their optimizations on accurate
    power models must be devised, performing an optimized setting of the parameters
    of the server (frequency, voltage, workload allocation, etc) while accomplishing
    with time requeriments and a wide range of real-time constraints.


    DVFS-based solutions for distributed real-time environments identify two main
    dimensions of the problem: *(i)* taskto-*Central-Processing-Unit* (CPU) allocation
    and *(ii)* run-time voltage scaling on individual CPUs. In CPS, physical factors
    (e.g., the network topology of CPS may dynamically change due to physical environments)
    are not entirely predictable and may lead to problems such as missed task deadlines,
    that can impact dramatically on economic loss for individuals or for the industry.
    Moreover, a critical task deadline missed could trigger a disaster (e.g., humans
    life loss, natural disasters, or huge economic loss).


    In this paper, we propose a method for solving such CPS problems by introducing
    new adaptive real-time scheduling algorithms in distributed computing infrastructures
    that also consider energy efficiency. This scheme requires to know *a priori*
    the processing and timing constraints of the set of tasks, and must be supported
    by reservation-based real-time operating systems.


    The remainder of this paper is organized as follows: after a brief summary of
    the previous works in this field (Section [II\)](#page-0-0), a real-time scheduling
    algorithm for CPS is sketched (Section [III\)](#page-1-0). Following, the devised
    power model is presented (Section [IV\)](#page-3-0), and the optimization of the
    algorithm developed is profusely described (Section [V\)](#page-4-0). Experimental
    results can be found in Section [VI.](#page-5-0) Finally, some conclusions are
    drawn (Section [VII\)](#page-7-2).


    #### II. RELATED WORK


    <span id="page-0-0"></span>The energy-efficient scheduling problem in real-time
    systems consists in minimizing the energy consumption while ensuring that all
    the real-time tasks meet their deadlines. The work presented in [\[3\]](#page-7-3)
    is based on the observation that a


    M. Teresa Higuera-Toledano, Jose L. Risco-Mart ´ ´ın, and Jose L. Ayala are ´
    with the Department of Computer Architecture and Automation, Complutense University
    of Madrid, C/Prof. Jose Garc ´ ´ıa Santesmases 9, 28040 Madrid, Spain, email:
    mthiguer,jlrisco,jlayalar@ucm.es


    Patricia Arroba is with the Department of Electronic Engineering, Technical University
    of Madrid, Avda. Complutense 30, 28040, Madrid, Spain, email: parroba@die.upm.es


    significant percentage of time spent in idle mode is due to the accumulation of
    small gaps between tasks. Whether the gap between the activation of two periodic
    tasks is less than transition-time from idle to deep-sleep, the processor is not
    able to transition to the deep-sleep state even though there is no useful work
    to be done, and continues in the idle energy state all the time.


    There are extensive research works on energy-aware realtime scheduling by using
    DVFS (e.g., [\[4\]](#page-7-4)). Different works using this technique within a
    real-time context, considered an offline scheduling algorithm and a set of a periodic
    jobs on an ideal processor. Each job is characterized by its release time, deadline,
    and execution CPU cycles, and all jobs have the same power consumption function.


    Several papers have also proposed DVFS-based solutions for real-time multi-processor
    systems. As the complexity of CPS increases, *Chip Multicore Processors* (CMP)
    and parallel tasks scheduled in a real-time way are needed. The fact that the
    processing cores share a common voltage level makes the CMP energy-efficiency
    problem different from multi-processor platforms. The work presented in [\[5\]](#page-7-5)
    provides a simple, elegant and effective solution on energy-efficient real-time
    scheduling on CMP. This solution addresses fixed-priority scheduling of periodic
    real-time tasks having a deadline equal to their period. Note that this problem
    is NP-hard.


    The load balancing in CMP is particularly important because the main contributor
    to the overall energy consumption in the system is the core with the maximum load.
    This fact is given by the global voltage/frequency constraint. Considering a CMP
    system with a workload perfectly balanced across the processors, the *Earliest
    Deadline First* (EDF) scheduling minimizes the total energy consumption. This
    is not the case of *Rate Monotonic Scheduling* (RMS) where load-balancing does
    not always result in lowering energy consumption [\[5\]](#page-7-5).


    In mixed-criticality systems, varying degrees of assurance must be provided to
    functionalities of varying importances. As shown in [\[6\]](#page-7-6) there is
    a conflict between safety and energy minimization because critical tasks must
    meet their deadlines even whether exceeding their expected *Worts Case Execution
    Time* (WCET). This work integrates continuous DVFS with the EDF with *Virtual
    Deadlines* (EDF-VD) scheduling for mixedcriticality systems [\[7\]](#page-7-7)
    and shows that speeding up the system to handle overrun is beneficial for minimizing
    the expected energy consumption of the system.


    Generally, large-scale distributed applications require realtime responses to
    meet soft deadlines. Hence, the middleware coordinates resource allocation in
    order to provide services accomplishing with SLA requirements. In [\[8\]](#page-7-8),
    we can find a scheduling algorithm based on DVFS for clusters, which develops
    a green SLA-based mechanism to reduce energy consumption by increasing the scheduling
    makespans. In [\[9\]](#page-7-9), we can find an energy-aware resource allocation
    for Cloud computing with negotiated QoS. However, similarly to the solution presented
    in [\[8\]](#page-7-8), this method sacrifices system performance.


    The work presented in [\[10\]](#page-7-10) proposes a priority-based scheduler,
    which satisfies the minimum resource requirement of a job by selecting a *Virtual
    Machine* (VM) according to both the SLA level and the W<sup>i</sup> parameter
    that is described as W<sup>i</sup> = P<sup>i</sup> × R<sup>i</sup> , where P<sup>i</sup>
    is the unit power cost of V M<sup>i</sup> , and R<sup>i</sup> defines the resources
    used by the V M<sup>i</sup> .


    The location of nodes in CPS affects the effective release time and deadline of
    real-time tasks, which may be different depending on the node location and the
    migration delay time among the network nodes. Because of that, traditional realtime
    scheduling algorithms have to be modified to include the location node and the
    spatial factors. The work presented in [\[11\]](#page-7-11) proposes a CPS scheduling
    algorithm, where the servicing node (i.e., the CPU) needs to move to serviced
    (i.e., the executed Job) node for real-time services.


    The power modeling technique proposed in [\[12\]](#page-7-12) is most relevant
    for us. A correlation between the total system''s power consumption and the component
    utilization is observed, defining a four-dimensional linear weighted power model
    for the total power consumed (i.e., P = c0+c1PCP U +c2Pcache+ c3PDRAM + c4Pdisk).
    Our work follows a similar approach but also incorporates the contribution of
    the static power consumption, its dependency with temperature, and the effect
    of applying DVFS techniques.


    Static power consumption has a high impact on energy, due to the temperature-dependent
    leakage currents. In this manner, novel optimizations may be devised by quantitatively
    understanding the power-thermal trade-offs of a system, thus developing analytical
    models.


    Finally, Rafique et al. [\[13\]](#page-7-13) makes a description of the complexity
    of the power management and allocation challenge. Authors demonstrate that achieving
    an optimal allocation depends on many factors as the server''s maximum and minimum
    frequencies, the job''s arrival rate, and consequently, the relationship between
    power and frequency. They conduct a set of experiments that provides significant
    savings in terms of energy in both homogeneous and heterogeneous clusters. However,
    our work presented in this paper outperforms these savings by exploiting a multi-objective
    optimization strategy to help to minimize the servers'' power for time-constrained
    Cloud applications.


    #### <span id="page-1-0"></span>III. THE INDUSTRIAL SERVICES EXECUTION MODEL


    CPS comprise a large number of sensors and actuators, and computing units that
    exchange different types of data, some of these interactions have real-time constraints.
    Real-time system abstraction and hybrid system modeling and control are among
    the CPS research challenges. The hybrid system model of CPS requires the design
    and integration of both the physical and computational (i.e., cyber) elements.
    While physical elements behave in continuous real-time, computational elements
    change according to discrete logic. This fact requires to merge continuous-time
    based systems with event-triggered logical systems, and also we must address the
    dimensional scale (i.e., from on-chip level to the cloud). Moreover, the interaction
    with physical world introduces uncertainty in CPS because of randomness in the
    environment, errors in physical devices, and security attacks.


    Control and scheduling co-design is a well-known area in the embedded real-time
    systems'' community. However, since CPS are typically networked control systems,
    the tradeoff between the effects of the network must be included in the real-time
    schedulability, that results in a non-periodic control approach. In this work,
    we study how to guarantee the overall system stability with minimum computational
    resource and power usage. System properties and requirements (e.g., the control
    laws, real-time and power constraints) must be captured and supported by data
    abstractions encapsulated in components.


    #### *A. Task characterization*


    Typically CPS''s are composed of hard real-time tasks and feedback control tasks.
    Whereas real-time tasks present time constraints (i.e., deadlines) that must always
    be satisfied, feedback control tasks are characterized by their *Quality of Control*
    (QoC), which needs to be optimized. A typical approach to the above scheduling
    problem is to translate the QoC requirements into time constraints and then, to
    apply traditional real-time scheduling techniques [\[14\]](#page-7-14). Real-time
    systems are structured as a set of schedulable tasks, where parameters used for
    the scheduling (e.g., execution time, deadline, or period) are a priori known
    and clearly defined. However, this solution is very conservative and consequently
    it is not efficient for CPS.


    An alternative solution is the given in [\[15\]](#page-7-15), that deals with
    this problem using a *multi-layered* scheme based on mixed-critical real-time
    systems: *(i)* for real-time tasks it uses triggering patterns (i.e., uses arrival
    curves), which allow a more general characterization regarding the classical real-time
    task models (i.e., *periodic* or *sporadic*), and *(ii)* for control tasks, it
    is based on three QoC-oriented metrics. Mixedcritical real-time systems literature
    focuses on tasks with different criticality levels and certification issues[1](#page-2-0)
    , providing heterogeneous timing guarantees for tasks of varying criticality levels.


    As an example, in the *Unmanned Aerial Vehicles* (UAVs), functionalities can be
    categorized as safety-critical tasks (e.g., like flight control and trajectory
    computation) or missioncritical tasks (e.g., object tracking for surveillance
    purposes). Note that the system is still safe although mission-critical functionalities
    can be lost. This makes the design parameters for safety-critical tasks (e.g.,
    WCET) much more pessimistic than those for mission-critical tasks. However, in
    CPS, tasks are not characterized by criticality levels, but by their criticality
    types.


    There has been considerable research on schedule synthesis for control applications.
    However, these works are particularly centered on control/scheduling co-design
    for optimized QoC, and only deal with control tasks. On the other hand, CPS focus
    on mixed task sets comprising of feedback control tasks and hard real-time tasks,
    which requires a joint schedule synthesis.


    #### *B. The task model*


    In CPS, tasks may be classified according to their criticality types (e.g., deadline-critical
    real-time tasks and QoC-critical


    <span id="page-2-0"></span><sup>1</sup>When there are tasks with different safety
    requirements into the same real-time platform, it is called mixed-criticality
    system.


    feedback control tasks). While the system must satisfy always the deadlines of
    real-time tasks, particularly for those that are critical, only the QoC parameters
    for control tasks need to be optimized. In order to do that, we require stochastic
    hybrid systems to identify the interaction between continuous dynamical physical
    models and discrete state machines, and the CPS architecture must follow the new
    paradigm *"globally virtual, locally physical"*.


    We consider a set of independent tasks, (i.e., Σ) which are executed remotely
    in a set of physical servers m. We define our real-time problem as a pair P =
    (Σ, S) where S is a scheduling solution and Σ = τ1, ..., τ<sup>n</sup> is a set
    of n tasks with different timing characteristics (i.e., strict, flexible, and
    firm) as shows Figure [1.](#page-2-1)


    ![](_page_2_Figure_12.jpeg)


    <span id="page-2-1"></span>Fig. 1. An overrun in response time (i.e., a deadline
    miss) has a different value function depending on its possible consequences


    Each task τ<sup>i</sup> is a possibly infinite sequence of jobs (i.e., demands
    for processing time), each one with an associated deadline. Jobs of the same task
    must be executed sequentially and in First-In-First-Out (FIFO) order. If the timing
    characteristics of the task τ<sup>i</sup> are soft or firm, the jobs may be not
    identical.


    CPS requires jointly scheduling hard real-time, soft realtime or best-effort,
    and control-feedback tasks. Due to the stringent stability requirements, we classify
    control tasks as firm deadline. While a hard deadline cannot be missed, soft deadlines
    may be occasionally missed and it does not harm the system safety. Similarly,
    firm deadlines can be missed but there is an upper limit on the number of misses
    within a given time interval. However, as we aim to optimize the QoC, we must
    minimize the number of deadline misses to avoid QoC degradation. The characterization
    of each type of task is fundamentally different as follows.


    *1) Hard real-time tasks:* A real-time system is considered *hard* if an overrun
    in a task response time leads to potential loss of life and/or big financial damage.
    The system is considered to be safety critical or high integrity, and is often
    mission critical. We consider a real-time task as a tuple τ<sup>i</sup> = (R<sup>i</sup>
    , C<sup>i</sup> , T<sup>i</sup> , Di) where:


    - Ri is the first release time of the task (i.e. the *phase* of the task),

    - Ci is the WCET,

    - Ti is the activation period (i.e., minimum inter-release time), and


    D<sup>i</sup> is the relative deadline (r<sup>i</sup> ≤ D<sup>i</sup> ≤ Ti). The
    absolute deadline is the relative deadline plus the current arrival time.


    We compute the CPU utilization factor of τ<sup>i</sup> as U<sup>i</sup> = C<sup>i</sup>
    Ti .


    *2) Soft real-time tasks:* For *soft* real-time tasks, deadline overruns are tolerable
    but not desired (i.e., there are not catastrophic consequences of missing one
    or more deadlines). There is a cost function associated with these systems, which
    is often related to QoS. Hence, we consider a stochastic task model based on the
    one presented in [\[16\]](#page-7-16). Then, we represent each soft-real-time
    task using a tuple τ<sup>i</sup> = (r<sup>i</sup> , s<sup>i</sup> , a<sup>i</sup>
    , di) where:


    - ri is the release time of the task,

    - si is the service time, which follows an exponential distribution of average
    µ −1 (i.e., µ is the number of serviced jobs of τ<sup>i</sup> per unit time),

    - ai is the arrival time; tasks arrive according to a renewal process with exponential
    distribution of average λ −1 , and

    - di is the absolute deadline; the relative deadline is D<sup>i</sup> = d<sup>i</sup>
    − a<sup>i</sup> , D<sup>i</sup> distributed on [0, D].


    We compute the response time of τ<sup>i</sup> as ρ<sup>i</sup> = c<sup>i</sup>
    − a<sup>i</sup> , where c<sup>i</sup> is the completion time (i.e., c<sup>i</sup>
    = a<sup>i</sup> + si). The average CPU utilization factor is given by Υ<sup>i</sup>
    = µi λi .


    *3) Feedback control tasks:* For a *firm* real-time task the computation is obsolete
    whether the job is not finished on time. In this case, the cost function may be
    interpreted as loss of value associated to QoC. This is the case of the feedback
    control task in CPS. For this kind of task we can consider D<sup>i</sup> ≥ Ti)
    to guarantee that the controlled physical tasks are still stable in the worst
    case scenario. However, this sacrifices the system performance and also may result
    unstable under physical perturbations.


    In most cases, feedback control systems become unstable with too many missed control
    cycles. Therefore, a critical question is how to determine T<sup>i</sup> to ensure
    both schedulability and robustness of the physical system. Considering a simple
    proportional-gain feedback controller, which is fixed for each control task, in
    order to determine T<sup>i</sup> , we can find the minimum T<sup>i</sup> ∈ (T1,
    T2, . . . , Tn) under the following constraints:


    $$0 \le \quad \sum\_{i} \frac{C\_i}{T\_i} \quad \le p \tag{1}$$


    $$C\_i \le \quad \quad T\_i \qquad \le D\_i \tag{2}$$


    where p < 1 is a priori known. However, some controller parameters may need to
    be adjusted when the task period is changed. Alternatively, we can use a multiple-versions
    approach or a predictive model with a quadratic optimization computed iteratively
    for each job. However, very often, probabilistic guarantees are sufficient (e.g.,
    t out of k deadlines have to be met).


    Permitting skips in periodic tasks increases the system flexibility [\[17\]](#page-7-17).
    The maximum number of skipped jobs for each task can be controlled by a specific
    parameter S<sup>i</sup> associated with the task, which gives the minimum distance
    between two consecutive jobs skips (e.g., if (S<sup>i</sup> = 3) the task can
    skip one job every three). This parameter can be considered as a QoC metric (i.e.,
    the higher S, the better QoC).


    When S<sup>i</sup> = ∞ no skips are allowed, meaning that τ<sup>i</sup> is a real-time
    hard periodic task. We then consider a control task as a tuple τ<sup>i</sup> =
    (R<sup>i</sup> , C<sup>i</sup> , T<sup>i</sup> , D<sup>i</sup> , Si) where T<sup>i</sup>
    = D<sup>i</sup> .


    ## *C. The parallel scheduling*


    For each of the above described tasks τ<sup>i</sup> ∈ Σ, we consider a set of
    independent subtasks τ<sup>i</sup> = τi,1, ..., τi,q, where τi,j denotes the subtasks
    j of task τ<sup>i</sup> . Therefore, e<sup>i</sup> ≥ 0, is the energy consumption
    rate of the task τ<sup>i</sup> per time unit:


    $$e\_i \quad = \bigcup\_{j=1}^{|\tau\_i|} e\_{i,j} \tag{3}$$


    The scheduling allocates each τi,j subtask in a set of m physical servers, taken
    into account the critical timing characteristics of each task τ<sup>i</sup> and
    the minimal energy consumption of the task set Σ.


    The performance criteria generally used in systems when the model task does not
    have explicit deadlines, is to minimize the task delay (i.e., the response time
    of all tasks). However, when there are explicit deadlines, we must ensure that
    critical tasks fulfill their deadline and minimize the fraction of non-critical
    tasks that do not meet their timing requirements.


    We can consider lateness constraints of the form α(x) ≤ β, where α(x) is the fraction
    of jobs that miss their deadline by more than x time units. Here, missing a deadline
    by x time units is considered as a failure.


    - For firm deadlines, we require that α(0) ≤ β (i.e., the fraction of tasks missing
    their deadliness were limited to β). Note that this has a different meaning for
    the S parameter, which is the minimal distance between the consecutive misses
    of the task τ<sup>i</sup> . Hence, we consider a τ<sup>i</sup> missing whether
    one or more subtasks τi,j of a job miss the deadline.

    - For hard real-time tasks, we establish α(0) ≤ 0 (i.e., we do not tolerate any
    deadline missed), while for each control task τ<sup>i</sup> , α(0) ≤ Si−1 S<sup>i</sup>
    .

    - For soft real-time tasks, we generalize, α(xi) ≤ β<sup>i</sup> , for a set of
    time values x1, ..., x<sup>p</sup> and constraint specifications β1, ..., βp,
    where 1 ≤ i ≤ p, which allows to take into account the stochastic nature of task
    arrivals and service time of soft real-time tasks.


    #### IV. POWER AND ENERGY MODEL


    <span id="page-3-0"></span>Traditionally in electronic systems, dynamic consumption
    has been the major contributor to the power budget. In contrast, when scaling
    technology below 100nm, static consumption reaches the 30−59% of the total power,
    thus becoming much more significant [\[18\]](#page-7-18). Moreover, the exponential
    impact of temperature on leakage currents intensifies this effect. Thus, modeling
    leakage will allow the exploitation of the trade-offs between leakage and temperature
    at the server level when taking decisions on resource configuration and selection.


    Therefore, the impact of static consumption must be considered, taking into account
    its correlation with temperature. This section presents our leakage-aware static
    power model. We validate this model using real data gathered from real machines
    of our case study (e.g., Intel Xeon and AMD Opteron).


    ## *A. Leakage power*


    Equation [\(4\)](#page-4-1) shows the impact of leakage on the currents in a MOS
    device. Rabaey demonstrates in his work that, when VDS > 100mV , the second exponential
    may be considered negligible [\[19\]](#page-7-19). Consequently, the previous
    equation may be revised as in [\(5\)](#page-4-1), also regrouping technological
    parameters together obtaining the formula presented in equation [\(6\)](#page-4-1).


    <span id="page-4-1"></span>

    $$I\_{leak} = \left. I\_s \cdot e^{\frac{V\_{GS} - V\_{TH}}{nkT/q}} \cdot \left(1
    - e^{\frac{Vds}{kT/q}}\right) \right. \tag{4}$$


    $$I\_{leak} = \begin{array}{c c} I\_s \cdot e^{\frac{V\_{GS} - V\_{TH}}{nkT/q}}
    \\ \dots \end{array} \tag{5}$$


    $$I\_{leak} = -B \cdot T^2 \cdot e^{\frac{\cdot \cdot \omega\_s \cdot \cdot \mu
    \cdot H}{nkT/q}} \tag{6}$$


    The leakage power consumption for the physical machine m ∈ {1, . . . , M} presented
    in Equation [\(8\)](#page-4-2) can be inferred from the expression in [\(7\)](#page-4-2).
    Then, the expansion of the mathematical expression in its Taylor 3rd order series
    provides Equation [\(9\)](#page-4-2), where Bm, C<sup>m</sup> and D<sup>m</sup>
    represent the technological constants of the server.


    <span id="page-4-2"></span>

    $$P\_{leak,m} = \ \ I\_{leak,m} \cdot V\_{DD,m} \tag{7}$$


    $$\begin{array}{rcl}P\_{leak,m} & = & B\_m \cdot T\_m^2 \cdot e^{\frac{V\_{GS}
    - V\_{TH}}{nkT/q}} \\ P\_{leak,m} & = & B\_m \cdot T\_m^2 \cdot V\_{DD,m} \end{array}
    \tag{8}$$


    $$+\quad C\_m \cdot T\_h \stackrel{\cdots}{\cdot} V\_{DD,m}^2 + D\_m \cdot V\_{DD,m}^3
    \qquad (9)$$


    #### *B. Dealing with DVFS*


    The main contributors to energy consumption in nowadays servers are CPU and memory
    devices. Despite DVFS is easily found in CPUs, there are still few memories with
    these capabilities. However, memory consumption in some cases (memoryintensive
    applications) is very significant compared to the CPU consumption and, because
    of this, it was considered important enough to be studied independently.


    Equation [10](#page-4-3) provides the consumption of a physical server that has
    k ∈ {1 . . . K} DVFS modes, while memory remains at a constant voltage. This expression
    takes into account the impact of temperature on the static power contribution.
    We define E<sup>m</sup> as the contribution of other server resources operating
    at constant values of frequency and voltage.


    <span id="page-4-3"></span>

    $$\begin{aligned} P\_{leak,mk} &= -B\_m \cdot T\_{CPU,m}^2 \cdot V\_{DD,mk} \\
    &+ \quad C\_m \cdot T\_{CPU,m} \cdot V\_{DD,mk}^2 + D\_m \cdot V\_{DD,mk}^3 \\
    &+ \quad E\_m + G\_m \cdot T\_{MEM,m}^2 + H\_m \cdot T\_{MEM,m} \\ \end{aligned}$$


    In order to measure temperature-dependent leakage we must understand also the
    dynamic contribution of the server''s power consumption. To maintain constant
    conditions, we use *lookbusy* [2](#page-4-4) , which is a synthetic application
    that stresses the CPU during specifics periods of time. *Lookbusy* is able to
    stress, not only the cores but also the hardware threads of the CPU at a precise
    utilization, having no impact on memory or disk devices. Synthetic workloads help
    us to maintain the utilization rate constant (in terms of instructions per cycle),


    <span id="page-4-4"></span><sup>2</sup>http://www.devin.com/lookbusy/


    thus revealing the leakage contribution due to temperature variations. The formulation
    of the dynamic power consumption is shown in Equation [11.](#page-4-5)


    <span id="page-4-5"></span>

    $$P\_{CPU,dyn,imk} = \ = A\_m \cdot V\_{DD,mk}^2 \cdot f\_{mk} \cdot u\_{CPU,imk}
    \tag{11}$$


    where A<sup>m</sup> defines the technological constant of the physical machine
    m and fmk and VDD,mk are respectively the frequency and the supply voltage at
    the k DVFS mode of the CPU. uCP U,imk represents the CPU utilization and it is
    correlated with the number of CPU cycles.


    #### *C. Energy model*


    So far, the power model is derived as in [\(12\)](#page-4-6).


    <span id="page-4-6"></span>

    $$\begin{array}{rcl} P\_{tot,mk} & = & A\_m \cdot V\_{DD,mk}^2 \cdot f\_{mk} \cdot
    \sum\_i u\_{CPU,imk} \\ & & + & B\_m \cdot T\_{CPU,m}^2 \cdot V\_{DD,mk} \\ &
    & + & C\_m \cdot T\_{CPU,m} \cdot V\_{DD,mk}^2 \\ & & + & D\_m \cdot V\_{DD,mk}^3
    + E\_m \end{array}$$


    The corresponding energy model can be easily obtained taking into account that
    E = P × t, being P the power model in [\(12\)](#page-4-6) and t, the execution
    time. Thus, the total energy consumed per host is described as the summation of
    the following equations:


    <span id="page-4-7"></span>

    $$\begin{array}{rcl} E\_{CPU,dyn,mk} & = & A\_m \cdot V\_{DD,m}^2 \cdot CPI \\
    & & \cdot \sum\_i u\_{CPU,imk} \cdot n\_{CPU,imk} \\ E\_{leak,mk} & = & [ \\ &
    & B\_m \cdot T\_{CPU,m}^2 \cdot V\_{DD,m} \\ & & + & C\_m \cdot T\_{CPU,m} \cdot
    V\_{DD,m}^2 + D\_m \cdot V\_{DD,m}^3 \\ & + & E\_m + G\_m \cdot T\_{MEM,m}^2 +
    H\_m \cdot T\_{MEM,m} \\ & & & 1 \cdot CPI \cdot \sum \end{array}$$


    where


    • CP I is the number of cycles per instruction


    fmk · X i


    ] ·


    • nCP U,imk is the number of CPU instructions of each task i assigned to be executed
    in a specific server m and DVFS mode k.


    nCP U,imk (14)


    The summation of both the instructions to execute and the resources used by the
    workload hosted on the server are needed in order to get the execution time of
    all tasks executed in parallel considering the resources offered by each server,
    as seen in [\(14\)](#page-4-7).


    <span id="page-4-8"></span>

    $$E\_{tot} = \sum\_{mk} \left( E\_{CPU, dyn, mk} + E\_{leak, mk} \right) \qquad
    (15)$$


    #### V. MULTI-OBJECTIVE OPTIMIZATION ALGORITHM


    <span id="page-4-0"></span>In this work, we aim for a workload allocation in a
    cloud that allows to optimize energy consumption. In addition, the benefits offered
    by virtualization are exploited, allowing to allocate the tasks in a more versatile
    way. The proposed system is defined as a cluster of machines of a cloud facility.


    The proposed solution considers server heterogeneity, so the technological parameters
    will vary from one architecture to another, resulting in a different energy consumption.
    Since the resultant power model is non-linear and there exists a large set of
    constraints, the problem is tackled as a multi-objective optimization:


    <span id="page-5-1"></span>

    $$\begin{array}{rcl} \text{Minimize} \\ \mathbf{y} = \mathbf{f}(\mathbf{x}) &=&
    [\boldsymbol{\lambda}, (1 + \boldsymbol{\lambda}) \cdot E\_{tot}(\mathbf{x})]
    \\ \text{Subject to} \\ \mathbf{x} &=& (x\_1, x\_2, \dots, x\_n) \in \mathbf{X}
    \end{array} \tag{16}$$


    where x is the vector of n decision variables, f is the vector of 2 objectives
    function, λ is the number of constraints not satisfied, Etot is the total energy,
    and X is the feasible region in the decision space. Using λ as shown in Equation
    [16,](#page-5-1) unfeasible solutions are also allowed, but only when no other
    alternatives are found. In this particular case, Etot is measured using [\(15\)](#page-4-8),
    whereas λ is computed as a penalization over the control and soft tasks that are
    delivered after the deadline (see Figure [1\)](#page-2-1).


    Using this formulation, we are able to obtain optimal energy savings, realistic
    with the current technology. To provide an efficient assignment in data centers
    it is necessary to consider both the energy consumption and the resource needs
    of each task of the workload.


    A task τ<sup>i</sup> can be split in different subtasks τi,j in order to achieve
    energy savings. Therefore, a task τ<sup>i</sup> can be executed using a specific
    amount of resources of one or more servers defined by uCP U,imk. The utilization
    percentage of the resources assigned to a task determines its execution time (i.e.,
    C<sup>i</sup> or si). In summary, the proposed multi-objective formulation, once
    solved, decides the following aspects:


    - Operating server set, indicating which hosts are active according to the operating
    conditions of each physical machine.

    - Best assignment for the various tasks of the workload, distributing each CPU
    instruction and memory requirements according to the minimum energy consumption
    of the applications in the computing infrastructure. For control tasks, S = 2
    must be fulfilled. However, a penalty is added to λ when one control task is aborted,
    even when S is being satisfied.

    - Percentage of resources used by every task in each host where it is allocated,
    achieving best energy consumption.


    #### *A. The solver*


    Evolutionary algorithms have been used to run the proposed multi-objective formulation.
    In this work, we use the Nondominated Sorting Genetic Algorithm II (NSGA-II) [\[20\]](#page-7-20),
    which has become a standard approach to solve this kind of problems [\[21\]](#page-7-21).
    The chromosome encoding is shown in Figure [2.](#page-5-2)


    In this case, each gene represents a decision variable. Because many decision
    variables are integer, the chromosome uses


    <span id="page-5-2"></span>


    |  | DV F S1 | · · ·<br>DV F SM | nCP U,11 | · · · | nCP U,NM |

    |--|---------|------------------|----------|-------|----------|

    |--|---------|------------------|----------|-------|----------|


    Fig. 2. Chromosome encoding


    integer encoding. Decision variables like nCP U,imk are scaled to the integer
    interval 0 ≤ nCP U,imk ≤ 100, and transformed to its real value (i.e., multiplying
    the percentage by the total number of instructions in the multi-objective function
    for evaluation).


    NSGA-II is always executed with an initial random population of 100 chromosomes.
    After that, the algorithm evolves the population applying (1) the NSGA-II standard
    tournament operator, (2) a single point crossover operator with probability of
    0.9 as recommended in [\[20\]](#page-7-20), (3) a integer flip mutation operator
    (with probability of 1/number of decision variables as also recommended in [\[20\]](#page-7-20),
    and (4) the multi-objective evaluation. Steps (1) to (4) are applied for a variable
    number of iterations or generations, which depend on the time that the parameter
    λ becomes 0 (usually 25000 iterations have been enough).


    #### VI. RESULTS


    <span id="page-5-0"></span>Tests have been conducted gathering real data from
    a Fujitsu RX300 S6 server based on an Intel Xeon E5620 processor and a SunFire
    V20z Dual Core AMD Opteron 270, both operating at the set of frequencies fmi given
    in Table [I.](#page-5-3) Total power consumption and CPU temperature have been
    collected via the *Intelligent Platform Management Interface* (IPMI) during the
    execution of *lookbusy* at different utilization levels ranging from 0% to 100%,
    where a 65% of these levels were used to fit the energy model and the remaining
    35% for validation. We used MATLAB to fit our data, obtaining the constants and
    validation errors shown in Table [II.](#page-6-0)


    <span id="page-5-3"></span>TABLE I INTEL XEON E5620 AND SUNFIRE V20Z DUAL CORE
    AMD OPTERON 270 FREQUENCIES


    | Platform    | fm1  | fm2  | fm3  | fm4  | fm5  | fm6  |

    |-------------|------|------|------|------|------|------|

    | Intel (GHz) | 1.73 | 1.86 | 2.13 | 2.26 | 2.39 | 2.40 |

    | AMD (GHz)   | 1.0  | 1.8  | 2.0  |      |      |      |


    The efficiency of the power supplies affects the calculation of these constants
    for different temperatures. In consequence, negative constants appear due to the
    fact that only CPU and memory have been characterized in this work because of
    their dominant contribution. In order to adapt the problem to more generic Cloud
    computing environments, our model constants can be calculated for data obtained
    during the execution of the workload in virtual machines. In that experimental
    approach, both the power model and the multi-objective optimization formulations
    would still be valid.


    Once the model proposed in section [IV](#page-3-0) for both Intel Xeon and AMD
    Opteron servers have been validated, we have proceeded with the analysis of results.
    The considered performance parameters are the temperature of both CPU and memory,
    as well as the frequency and voltage of the DVFS modes available to the CPU in
    each physical machine. These


    <span id="page-6-0"></span>


    | Server | A       | B1     | B2      | C1      | C2     | D      | E       |
    F       | G1       | G2      | H1      | H2      | Error  | Temp. range |

    |--------|---------|--------|---------|---------|--------|--------|---------|---------|----------|---------|---------|---------|--------|-------------|

    | Intel  | 14.3505 | 0.1110 | -       | -0.0011 | -      | 0.3347 | -40700  |
    64.9494 | 275.702  | -       | -0.4644 | -       | 11.28% | 293-309K    |

    | AMD    | 11.2390 | 1.9857 | -6.1703 | -0.0002 | 0.0132 | 426.51 | -5.3506 |
    25.1461 | -444.480 | 464.076 | 0.6977  | -0.7636 | 9.12%  | 293-312K    |


    variables modify independently the dynamic and static consumption of servers in
    each architecture, so different behaviors for Intel and AMD have been found. Table
    [III](#page-6-1) shows the set of tasks used for the optimization.


    TABLE III PROFILE OF TASKS ALLOCATED


    <span id="page-6-1"></span>


    | Task Id | Type | # Ins     | Period | Deadline | # Jobs |

    |---------|------|-----------|--------|----------|--------|

    | 0       | REAL | 7740796   | 114.20 | 0.021    | 131    |

    | 1       | CTRL | 5594832   | 114.21 | 0.015    | 115    |

    | 2       | REAL | 4138643   | 137.12 | 0.011    | 112    |

    | 3       | CTRL | 98156923  | 124.66 | 0.267    | 95     |

    | 4       | REAL | 739437676 | 124.76 | 2.01     | 118    |

    | 5       | SOFT | 2591877   | 124.86 | 0.007    | 103    |

    | 6       | SOFT | 3093531   | 124.85 | 0.008    | 112    |

    | 7       | SOFT | 5447445   | 105.76 | 0.015    | 115    |

    | 8       | SOFT | 5722568   | 152.21 | 0.016    | 99     |


    These tasks have been adapted from the TUDelft workloads archive[3](#page-6-2)
    . The task set consists of a number of deadline-critical tasks τhrt = {τ0, τ2,
    τ4}, a number of QoC-critical control tasks τ<sup>c</sup> = {τ1, τ3}, and a number
    soft real-time tasks τsrt = {τ5, τ6, τ7, τ8}. We assume that all tasks are independent
    from each other. However, due to the interference from other tasks, each task
    τ<sup>i</sup> experiences a response time or delay R<sup>i</sup> . Periods and
    deadlines are given in seconds. Each real-time tasks τhrt is bounded to one single
    host. Only control τ<sup>c</sup> and soft tasks τsrt are allowed to loss their
    deadline, increasing the λ parameter in the multi-objective function. Control
    tasks are configured with S = 2.


    NSGA-II has been executed with the minimum frequency in all the CPUs (labeled
    in the results as DVFS-MIN), the maximum frequency (labeled as DVFS-MAX) and a
    range of 5 possible DVFS modes (from 1 to 5). This algorithm has been compared
    with a more traditional approach, the EDF-VD algorithm. The overall goal is to
    design a priority assignment technique with the following objectives:


    - All the real-time tasks τhrt meet their deadlines Dhrt in the WCET

    - The overall QoC of all the control tasks τ<sup>c</sup> and QoS of all the soft
    real-time tasks τsrt is maximized.

    - The overall energy is minimized.


    <span id="page-6-2"></span>Figure [3](#page-6-3) depicts the three obtained Pareto
    fronts for the Intel architecture. Both objectives have been normalized to the
    worst value in all the Intel and AMD optimizations (1 unit of energy = 95.6 KJ).
    As can be seen, the DVFS-MAX Intel framework is able to allocate all the tasks
    in Table [III](#page-6-1) without penalizations (labeled as full feasibility).
    Using DVFS-MIN, the algorithm was not able to allocate all the required tasks,
    having to break some soft timing constraints (labeled as partial feasibility).
    As can be seen, there is at least one DVFS-VAR


    ![](_page_6_Figure_13.jpeg)


    <span id="page-6-3"></span>Fig. 3. Pareto front obtained with NSGA-II after optimizing
    the allocation of tasks over the Intel architecture.


    ![](_page_6_Figure_15.jpeg)


    <span id="page-6-4"></span>Fig. 4. Pareto front obtained with NSGA-II after optimizing
    the allocation of tasks over the AMD architecture.


    configuration able to execute all the tasks without penalization and with less
    energy than DVFS-MAX and close to DVFS-MIN. Table [IV](#page-7-22) shows the DVFS
    modes selected by the DVFS-VAR solution with full feasibility.


    Similarly, Figure [4](#page-6-4) shows the three obtained non-dominated fronts
    for the AMD architecture. As with the Intel scenario, the algorithm was not able
    to execute all the REAL, CTRL and SOFT tasks without penalization using the minimum
    DVFS mode (DVFS-MIN), although all the REAL tasks were properly executed. However,
    we found a completely feasible solution in DVFS-VAR (feasibility=0), consuming
    less energy than DVFS-MAX. Table [IV](#page-7-22) shows the DVFS models selected
    by the multi-objective algorithm in the DVFS-VAR AMD optimization.


    EDF was able to schedule all the tasks in both cases, but using the maximum DVFS
    mode and thus consuming more energy than the proposed algorithm.


    <span id="page-7-22"></span>TABLE IV DVFS MODES OBTAINED BY NSGA-II PARETO FRONT
    IN THE DVFS-VAR OPTIMIZATION


    | Platform | CPU 1 | CPU 2 | CPU 3 | CPU 4 | CPU 5 | CPU 6 |

    |----------|-------|-------|-------|-------|-------|-------|

    | Intel    | 2     | 2     | 2     | 2     | 2     | 5     |

    | AMD      | 5     | 1     | 5     |       |       |       |


    As a result, the best DVFS configuration that can execute all the demanded services
    given in Table [III](#page-6-1) has been found without penalizations, obtaining
    a high diversity in terms of energy consumption.


    #### VII. CONCLUSIONS


    <span id="page-7-2"></span>CPS and Mobile Cloud Computing have collided with the
    lack of accurate power models for the energy-efficient provisioning of their devised
    infrastructures, and the real-time management of the computing facilities. In
    this paper, we have presented a reservation-based scheme aiming to jointly schedule
    deadline-critical, QoS non-critical, and QoC tasks. The work proposed in this
    paper has made substantial contributions in the area of power modeling of high-performance
    servers for Cloud computing services under timing constraints, which is an interesting
    and big challenge.


    We have proposed an accurate power model in data centers for time constrained
    servers in Cloud computing, which does not only consider the workload assigned
    to the processing element, but also incorporates the need of considering the static
    power consumption and its dependency with temperature.


    The proposed model has been used in a multi-objective optimization environment
    in which the DVFS and workload assignment have been efficiently optimized in a
    realistic scenario composed of Fujitsu RX300 S6 servers based on an Intel Xeon
    E5620 and SunFire V20z Dual Core AMD Opteron 270. Results show that the proposed
    multi-objective optimization framework is able to find the best DVFS configuration
    that can execute all the given demanded services without penalizations. In addition,
    the set of non-dominated solutions found presents a high diversity in terms of
    energy consumption.


    The obtained results open a motivating research line that could enable intensely
    sought Green computing paradigm with hard timing constraints. Future work envisages
    to extend the scheduling model to integrate the concept of criticality levels.


    #### ACKNOWLEDGMENT


    This work is supported by the Spanish Ministry of Economy and Competitivity under
    research grants TIN2013-40968-P and TIN2014-54806-R.


    #### REFERENCES


    - <span id="page-7-0"></span>[1] E. A. Lee, "CPS Foundations," in *Proceedings
    of the 47th Design Automation Conference*, ser. DAC ''10. New York, NY, USA: ACM,
    2010, pp. 737–742.

    - <span id="page-7-1"></span>[2] A. Banerjee, K. K. Venkatasubramanian, T. Mukherjee,
    and S. K. S. Gupta, "Ensuring safety, security, and sustainability of mission-critical
    cyber-physical systems." *Proceedings of the IEEE*, vol. 100, no. 1, pp. 283–299,
    2012. [Online]. Available: [http://dblp.uni-trier.de/db/journals/](http://dblp.uni-trier.de/db/journals/pieee/pieee100.html)
    [pieee/pieee100.html](http://dblp.uni-trier.de/db/journals/pieee/pieee100.html)

    - <span id="page-7-3"></span>[3] A. Rowe, K. Lakshmanan, H. Zhu, and R. Rajkumar,
    "Rate-harmonized scheduling for saving energy," in *Proceedings of the 2008 Real-Time
    Systems Symposium*, ser. RTSS ''08. Washington, DC, USA: IEEE Computer Society,
    2008, pp. 113–122. [Online]. Available: <http://dx.doi.org/10.1109/RTSS.2008.50>

    - <span id="page-7-4"></span>[4] J. Chen and C. Kuo, "Energy-efficient scheduling
    for real-time systems on dynamic voltage scaling (DVS) platforms," in *13th IEEE
    International Conference on Embedded and Real-Time Computing Systems and Applications
    (RTCSA 2007), 21-24 August 2007, Daegu, Korea*, 2007, pp. 28–38.

    - <span id="page-7-5"></span>[5] J. Kim, H. Kim, K. Lakshmanan, and R. R. Rajkumar,
    "Parallel scheduling for cyber-physical systems: Analysis and case study on a
    self-driving car," in *Proceedings of the ACM/IEEE 4th International Conference
    on Cyber-Physical Systems*, ser. ICCPS ''13. New York, NY, USA: ACM, 2013, pp.
    31–40. [Online]. Available: <http://doi.acm.org/10.1145/2502524.2502530>

    - <span id="page-7-6"></span>[6] P. Huang, P. Kumar, G. Giannopoulou, and L. Thiele,
    "Energy efficient dvfs scheduling for mixed-criticality systems," in *Proceedings
    of the 14th International Conference on Embedded Software*, ser. EMSOFT ''14.
    New York, NY, USA: ACM, 2014, pp. 11:1–11:10. [Online]. Available:<http://doi.acm.org/10.1145/2656045.2656057>

    - <span id="page-7-7"></span>[7] S. K. Baruah, V. Bonifaci, G. D''Angelo, H. Li,
    A. Marchetti-Spaccamela, S. van der Ster, and L. Stougie, "The preemptive uniprocessor
    scheduling of mixed-criticality implicit-deadline sporadic task systems." in *ECRTS*,
    R. Davis, Ed. IEEE Computer Society, 2012, pp. 145–154.

    - <span id="page-7-8"></span>[8] L. Wang, G. von Laszewski, J. Dayal, and F. Wang,
    "Towards energy aware scheduling for precedence constrained parallel tasks in
    a cluster with DVFS," in *Cluster, Cloud and Grid Computing (CCGrid), 2010 10th
    IEEE/ACM International Conference on*, May 2010, pp. 368–377.

    - <span id="page-7-9"></span>[9] A. Beloglazov, J. Abawajy, and R. Buyya, "Energy-aware
    resource allocation heuristics for efficient management of data centers for cloud
    computing," *Future Gener. Comput. Syst.*, vol. 28, no. 5, pp. 755–768, May 2012.
    [Online]. Available: [http://dx.doi.org/10.1016/j.future.2011.](http://dx.doi.org/10.1016/j.future.2011.04.017)
    [04.017](http://dx.doi.org/10.1016/j.future.2011.04.017)

    - <span id="page-7-10"></span>[10] C. Wu, R. Chang, and H. Chan, "A green energy-efficient
    scheduling algorithm using the DVFS technique for cloud datacenters," *Future
    Generation Comp. Syst.*, vol. 37, pp. 141–147, 2014.

    - <span id="page-7-11"></span>[11] S. Park, J. Kim, and G. Fox, "Effective real-time
    scheduling algorithm for cyber physical systems society," *Future Generation Comp.
    Syst.*, vol. 32, pp. 253–259, 2014.

    - <span id="page-7-12"></span>[12] C. Mobius, W. Dargie, and A. Schill, "Power
    consumption estimation models for processors, virtual machines, and servers,"
    *Parallel and Distributed Systems, IEEE Transactions on*, vol. 25, no. 6, pp.
    1600– 1614, June 2014.

    - <span id="page-7-13"></span>[13] M. M. Rafique and et al., "Power management
    for heterogeneous clusters: An experimental study," in *IGCC*, Washington, DC,
    USA, 2011, pp. 1–8.

    - <span id="page-7-14"></span>[14] R. I. Davis and A. Burns, "A survey of hard
    real-time scheduling for multiprocessor systems," *ACM Comput. Surv.*, vol. 43,
    no. 4, pp. 35:1–35:44, Oct. 2011. [Online]. Available: [http://doi.acm.org/10.1145/](http://doi.acm.org/10.1145/1978802.1978814)
    [1978802.1978814](http://doi.acm.org/10.1145/1978802.1978814)

    - <span id="page-7-15"></span>[15] R. Schneider, D. Goswami, A. Masrur, M. Becker,
    and S. Chakraborty, "Multi-layered scheduling of mixed-criticality cyber-physical
    systems," *Journal of Systems Architecture (JSA)*, vol. 59, no. 10-D, 2013.

    - <span id="page-7-16"></span>[16] L. Abeni, N. Manica, and L. Palopoli, "Efficient
    and robust probabilistic guarantees for real-time tasks," *J. Syst. Softw.*, vol.
    85, no. 5, pp. 1147–1156, May 2012. [Online]. Available: [http:](http://dx.doi.org/10.1016/j.jss.2011.12.042)
    [//dx.doi.org/10.1016/j.jss.2011.12.042](http://dx.doi.org/10.1016/j.jss.2011.12.042)

    - <span id="page-7-17"></span>[17] T. Chantem, X. Hu, and M. Lemmon, "Generalized
    elastic scheduling for real-time tasks," *Computers, IEEE Transactions on*, vol.
    58, no. 4, pp. 480–495, 2009.

    - <span id="page-7-18"></span>[18] S. Narendra and A. Chandrakasan, *Leakage in
    Nanometer CMOS Technologies*, ser. Integrated Circuits and Systems. Springer,
    2010.

    - <span id="page-7-19"></span>[19] J. Rabaey, *Low Power Design Essentials*, ser.
    Engineering (Springer-11647). Springer, 2009.

    - <span id="page-7-20"></span>[20] K. Deb, A. Pratap, S. Agarwal, and T. Meyarivan,
    "A Fast and Elitist Multiobjective Genetic Algorithm: NSGA-II," *IEEE Transactions
    on Evolutionary Computation*, vol. 6, no. 2, pp. 182–197, 2002.

    - <span id="page-7-21"></span>[21] A. Sayyad and H. Ammar, "Pareto-optimal search-based
    software engineering (posbse): A literature survey," in *Realizing Artificial
    Intelligence Synergies in Software Engineering (RAISE), 2013 2nd International
    Workshop on*, May 2013, pp. 21–27.'
  decisions:
    evaluation_prompt: 'Qualified. Reason: The paper mentions "Experimental results
      can be found in Section VI," indicating the presence of experimental or empirical
      analysis.'
    related_work_prompt: 'Qualified. Reason: The paper includes a "Related Work" section
      that engages with prior research by citing and discussing various works related
      to energy-efficient scheduling in real-time systems.'
    novelty_prompt: 'Qualified. Reason: The paper proposes a new adaptive real-time
      scheduling algorithm for CPS in distributed computing infrastructures, which
      considers energy efficiency, indicating novelty.'
    review_only_prompt: 'Qualified. Reason: The paper introduces new adaptive real-time
      scheduling algorithms and presents novel approaches, indicating new contributions
      beyond summarizing existing work.'
- title: "A Scalable RISC-V Vector Processor Enabling Efficient Multi-Precision\n\
    \  DNN Inference"
  abstract: 'RISC-V processors encounter substantial challenges in deploying

    multi-precision deep neural networks (DNNs) due to their restricted precision

    support, constrained throughput, and suboptimal dataflow design. To tackle

    these challenges, a scalable RISC-V vector (RVV) processor, namely SPEED, is

    proposed to enable efficient multi-precision DNN inference by innovations from

    customized instructions, hardware architecture, and dataflow mapping. Firstly,

    dedicated customized RISC-V instructions are proposed based on RVV extensions,

    providing SPEED with fine-grained control over processing precision ranging

    from 4 to 16 bits. Secondly, a parameterized multi-precision systolic array

    unit is incorporated within the scalable module to enhance parallel processing

    capability and data reuse opportunities. Finally, a mixed multi-precision

    dataflow strategy, compatible with different convolution kernels and data

    precision, is proposed to effectively improve data utilization and

    computational efficiency. We perform synthesis of SPEED in TSMC 28nm

    technology. The experimental results demonstrate that SPEED achieves a peak

    throughput of 287.41 GOPS and an energy efficiency of 1335.79 GOPS/W at 4-bit

    precision condition, respectively. Moreover, when compared to the pioneer

    open-source vector processor Ara, SPEED provides an area efficiency improvement

    of 2.04$\times$ and 1.63$\times$ under 16-bit and 8-bit precision conditions,

    respectively, which shows SPEED''s significant potential for efficient

    multi-precision DNN inference.'
  url: http://arxiv.org/abs/2401.16872v2
  keywords: ''
  document: '## I. INTRODUCTION


    RISC-V processors [\[1\]](#page-4-0)–[\[11\]](#page-4-1) are distinguished by
    the concise open-source RISC-V instruction set architecture (ISA) [\[12\]](#page-4-2),
    which defines a fundamental set of instructions while offering opportunities to
    incorporate application-specific custom instructions. This unique feature enables
    RISC-V processors as a promising solution ranging from energy-efficient embedded
    systems [\[10\]](#page-4-3) to high-throughput servers [\[1\]](#page-4-0). On
    these RISC-V hardware platforms, deep neural networks (DNNs) exhibit substantial
    deployment demands [\[13\]](#page-4-4)–[\[15\]](#page-4-5) but face the extensive
    storage and computational costs. Aiming to reduce resource overhead and improve
    inference speed, multiprecision quantized DNNs [\[16\]](#page-4-6)–[\[18\]](#page-4-7)
    have emerged as an efficient choice for deployment. However, deploying multiprecision
    quantized DNNs also incurs a series of significant challenges on prior RISC-V
    processors.


    The emergence of the RISC-V Vector (RVV) extension instruction set has made RVV
    processors a promising choice for deploying multi-precision DNNs. These RVV processors
    [\[5\]](#page-4-8)–[\[9\]](#page-4-9) excel in throughput enhancement through
    their parallel processing capabilities and minimize instruction overhead by employing
    a minimal number of configuration-setting instructions to define data precision.
    Notably, Ara [\[6\]](#page-4-10) stands out as the pioneer open-source vector
    processor compatible with the standard RVV 1.0 ISA. It achieves an exceptional
    throughput improvement of up to 380× in comparison to a scalar core [\[19\]](#page-4-11),
    rendering it highly efficient for tasks such as DNN inference. However, the potential
    performance improvement of the Ara is constrained due to limited support for low-bitwidth
    (e.g., 4-bit) operations, which are widely used in quantized DNNs to reduce computational
    complexity and accelerate inference processing with little loss in accuracy [\[20\]](#page-4-12).
    Furthermore, Ara''s parallelism is constrained by the number of scalable modules.
    When handling intensive computational tasks, increasing the number of scalable
    modules can cause excessive hardware consumption. Meanwhile, inefficient dataflow
    of Ara can lead to increased off-chip data movement and underutilization of on-chip
    memory, thereby lowering computational efficiency. In summary, deploying multi-precision
    quantized DNNs on prior RISC-V processors still struggles with (1) limited precision
    support, (2) constrained throughput, and (3) inefficient dataflow mapping.


    To address the issues above, we propose SPEED, a scalable RVV processor enabling
    efficient DNN inference across 4, 8, and 16 bits through customized instructions
    based on the standard RVV ISA. Moreover, SPEED significantly improves data throughput
    and computational efficiency by increasing the data-level parallelism of scalable
    modules and utilizing a mixed multi-precision dataflow strategy tailored for various
    convolution kernels. Specifically, the contributions of this work can be summarized
    in the three following aspects:


    - 1) Customized instructions: Dedicated customized RISC-V instructions are proposed
    based on RVV extensions, providing SPEED with fine-grained control over processing
    precision and dataflow strategy, while supporting multi-precision computations
    ranging from 4 to 16 bits.

    - 2) Hardware architecture: A scalable RVV processor is developed, namely SPEED,
    for efficient multi-precision DNN inference by enhancing parallel processing capability
    of scalable modules. Compared to Ara [\[6\]](#page-4-10), SPEED improves an area
    efficiency by 2.04× and 1.63× under 16-bit and 8-bit precision conditions, respectively.

    - 3) Dataflow mapping: A mixed dataflow strategy embracing feature map-first (FF)
    and channel-first (CF) strategies is proposed, aiming to achieve high computational


    efficiency of convolution kernels across various kernel sizes and data precisions.
    The mixed dataflow strategy improves area efficiency of SPEED by 1.87∼3.53× over
    Ara [\[6\]](#page-4-10) under various convolution kernels.


    ![](_page_1_Figure_2.jpeg)


    Fig. 1. Customized instructions and overall architecture of SPEED.


    ## <span id="page-1-0"></span>II. THE PROPOSED SPEED ARCHITECTURE


    SPEED is a scalable RVV processor augmented with customized instructions enabling
    high-throughput multi-precision DNN inference, which is built on the RVV v1.0
    ISA. Additionally, the RVV processor is tightly coupled to a RISC-V scalar core
    for programmable instructions and an external memory for fetching necessary data.
    In this section, SPEED will be described from three perspectives: customized instructions,
    hardware architecture, and dataflow mapping.


    ## *A. Customized Instructions*


    To facilitate efficient inference of multi-precision DNNs, several customized
    instructions are proposed to enhance the computational efficiency of DNN operations
    compared to the standard RVV extensions. Specifically, the customized instructions
    mainly contain the configuration-setting (VSACFG), load (VSALD), and arithmetic
    (VSAM), as depicted in Fig. [1.](#page-1-0)


    VSACFG serves as a vector configuration-setting instruction that effectively provides
    the necessary information for subsequent instructions, such as data precision
    (4-∼16-bit) and dataflow strategy (FF/CF strategy). This information is encoded
    within the *zimm9* and *uimm5* encoding spaces, as shown in Fig. [1,](#page-1-0)
    to prepare for processing of subsequent RVV and customized instructions.


    VSALD is a customized load instruction responsible for loading data from the base
    address of external memory and storing it into vector register files (VRFs) at
    the specified destination address, aiming to maximize data utilization. In contrast
    to the ordered allocation operation of the standard RVV load instruction VLE,
    the loaded data from the external memory are broadcast to each lane for improving
    data reuse.


    VSAM is a customized arithmetic instruction that exploits data-level parallelism
    in efficiency, thereby enhancing computational efficiency. As shown in Fig. [1,](#page-1-0)
    VSAM requests data from the base addresses *vs1* and *vs2* where they locate in
    the VRFs, respectively. The performed results of above data are stored at the
    accumulation address *Acc Addr* within the VRFs.


    ## <span id="page-1-1"></span>*B. Hardware Architecture*


    The overall hardware architecture of SPEED is shown in Fig. [1.](#page-1-0) To
    realize efficient multi-precision DNN inference, ➊ vector instruction decode unit
    (VIDU) is developed to decode customized instructions as well as the standard
    RVV instruction set. Furthermore, ➋ vector load unit (VLDU) is designed to distribute
    data through broadcast or ordered allocation, enabling our design to meet the
    diverse computation requirements of mixed dataflow strategy. Scalable modules
    for vector processors, namely ➌ lane, serve as the main computational components
    of the proposed processor, which consists of lane sequencer, VRFs, systolic array
    unit (SAU) and arithmetic logic unit (ALU).


    To enhance the processor''s parallel processing capability and fully exploit data
    reuse opportunities, a highly flexible and parameterized multi-precision ➍ SAU
    is presented as the main computing unit of SPEED, which is composed of three components:
    operand requester, queues, and systolic array core (SA Core). The operand requester
    consists of an address generator and a request arbiter, enabling efficient data
    access by concurrently generating addresses and prioritizing data requests. The
    queue is responsible for buffering the data involved in the computation, including
    inputs, weights, accumulation results, and outputs. The SA Core is a reconfigurable
    twodimensional array of processing elements (PEs) determined by the parameters
    T ILE R and T ILE C, which can be flexibly adjusted for different computation
    requirements. For convolution operations, SA Core architecture employs three levels
    of parallelism: within each PE on input channel dimension, across the PE array
    within each lane on output channel dimension and on the height dimension of feature
    map. Moreover, each PE consists of sixteen 4-bit multipliers that can be dynamically
    combined to perform multiply-accumulate operation (MAC) with 16-bit precision,
    four sets of MACs at 8-bit precision, or sixteen sets of MACs at 4-bit precision.


    ## *C. Dataflow Mapping*


    To enhance SPEED''s computational efficiency, an optimized multi-precision dataflow
    strategy, combined with FF and CF strategies, is developed to flexibly handle
    DNN layers with varying kernel sizes. Specifically, the proposed FF strategy is
    suitable for convolutional layers with large kernel size, while the CF strategy
    is better suited for those with small kernel size. Detailed descriptions are provided
    as follows.


    To unify multi-precision data representation, all inputs and weights are preprocessed
    into multiple elements along the input channel dimension. Specifically, every
    adjacent 1, 4, and 16 operands are combined into a unified element under 16 bit,
    8-bit, and 4-bit precision modes, respectively. Both the FF and CF strategies
    support continuous multi-stage convolution computations. To clearly show how these
    strategies work, we select the first and second stages as examples of broadcasting
    the inputs and allocating the weights to the lanes, as shown in Fig. [2.](#page-2-0)
    Here, the size of weight kernel is 3 × 3, and the parameter T ILE H is set to
    4, which is determined by the parallelism of the SAU and kernel size.


    ![](_page_2_Figure_1.jpeg)


    <span id="page-2-0"></span>Fig. 2. Examples on how CF strategy and FF strategy
    work with multiprecision elements. Note that the 4-bit data is operated in the
    same way as 16-bit and 8-bit.


    For example, as shown on the left of Fig. [2](#page-2-0) (a), 16-bit input (X16)
    elements are pre-fetched to the VRF during two-stage convolution operations. In
    FF strategy, to facilitate parallel computing and reduce off-chip data movement,
    we pre-fetch 4×4 elements on a single input channel of inputs. These prefetched
    elements are requested from the VRFs and calculated in the SAU during the two-stage
    convolution operations, and the request process is as depicted in the right of
    Fig. [2](#page-2-0) (a). When the first stage is completed, the results are stored
    to the VRFs, and the elements in the overlapping areas of the blue and red boxes
    are reused. However, multi-stage convolution results occupy a large portion of
    the storage space in VRFs, and extra time is wasted in transferring the partial
    results between stages. In order to reduce the memory footprint of VRFs and avoid
    output transfer latency, the CF strategy is proposed to pre-fetch elements along
    the input channel dimension. The two-stage convolution operations pre-fetch elements
    from the two input channels of inputs. The first stage and the second stage respectively
    request the first and the second input channels of the pre-fetched elements for
    computations, and the results of the two stages accumulate inside the SAU.


    Fig. [2](#page-2-0) (b) and (c) illustrate how to the pre-fetch and the request
    of 16-bit weight (W16) and 8-bit weight (W8) elements in a two-stage convolution
    operations. In W16+CF mode, we prefetch 2 weights along the output channel dimension
    to enhance parallel computing within a single lane, where the number of weights
    is determined by T ILE C. The elements at the same position in the first channel
    of both weights are simultaneously requested by the SAU and participate in the
    convolution computations for the first stage. After the first stage, the second
    channel elements of the weights participate in the computations. In W8+FF mode,
    as discussed in Sec. [II-B,](#page-1-1) unified W8 element has a parallelism of
    4 along the input channel dimension, thereby enhancing computational efficiency.
    Weights are reused in the second stage to minimize off-chip data movement when
    the computations in the first stage are completed. In summary, the FF strategy
    takes advantage of calculating larger convolution kernels due to its high data
    reuse. The CF strategy is suitable for smaller convolution kernels with low reuse
    requirements, as it reduces the memory footprint of partial results and avoids
    additional output transfer consumption.


    ## III. EXPERIMENTAL RESULTS


    ## *A. Experimental Setup*


    To highlight the effectiveness of our design, we select Ara [\[6\]](#page-4-10)
    as a baseline design for comparison, which is the first opensource implementation
    of RISC-V vector processor. Several popular DNNs are implemented as benchmarks
    for evaluation, including VGG16 [\[21\]](#page-4-13), ResNet18 [\[22\]](#page-4-14),
    GoogLeNet [\[23\]](#page-4-15), and SqueezeNet [\[24\]](#page-4-16). Note that
    the evaluated metric is area efficiency (GOPS/mm<sup>2</sup> ), measured across
    the convolutional layers in the DNN model using cycle-accurate simulation with
    QuestaSim, consistent with the experimental method in [\[6\]](#page-4-10). To
    evaluate the impact of our architectural modifications at the hardware level,
    we synthesize both SPEED and Ara using Synopsys Design Compiler 2014.09 on the
    TSMC 28 nm process. For a fair comparison, we use 4 lanes and a vector length
    of 4096 bits for both SPEED and Ara. And we set both T ILE R and T ILE C of SAU
    in each SPEED''s lane to 4 for these experiments. Finally, we conduct a comprehensive
    comparison between our design and Ara to demonstrate the effectiveness of SPEED
    in multi-precision DNN deployments.


    ![](_page_2_Figure_9.jpeg)


    <span id="page-2-1"></span>Fig. 3. Layer-wise area efficiency breakdown of GoogLeNet
    on SPEED under 16-bit precision. Our mixed dataflow strategy surpasses the FF-only
    and CFonly strategies by 1.88× and 1.38×, respectively.


    ## *B. Model Evaluation*


    To evaluate the impact of convolution kernel size on different dataflow strategies,
    a layer-wise evaluation of GoogleNet, which employs diverse convolution kernel
    sizes, is conducted on SPEED using various strategies under the 16-bit precision
    condition. The mixed strategy dynamically selects the FF-only or CF-only strategy
    with the best performance in each layer to further improve area efficiency. As
    shown in Fig. [3,](#page-2-1) the mixed dataflow strategy achieves a area efficiency
    improvement of 1.88× and 1.38× in comparison with the FF-only and CFonly dataflow
    strategies, respectively. Meanwhile, compared with Ara, the FF-only and CF-only
    dataflow strategies have a area efficiency improvement of 1.87× and 2.55×, respectively,
    and the mixed dataflow strategy achieves a 3.53× increase in area efficiency.
    To more clearly illustrate the composition of the mixed strategy in the evaluation
    of GoogleNet, Fig. [3](#page-2-1) presents a layer-wise breakdown of the mixed
    strategy and annotates the specific strategy used. The results indicate that the
    CF-only strategy is better suited for conv1x1, while the FFonly strategy is suitable
    for other convolution kernels, where convKxK denotes a convolutional operator
    with a kernel size of K. Meanwhile, it shows that with larger convolution kernel
    sizes, the area efficiency improves. Therefore, employing the mixed dataflow strategy
    can significantly enhance SPEED''s performance for DNN deployment.


    ![](_page_3_Figure_2.jpeg)


    <span id="page-3-0"></span>Fig. 4. Average area efficiency under multi-precision
    DNN benchmarks, SPEED outperforms Ara by 2.77× and 6.39× at 16-bit and 8-bit precision,
    respectively.


    As illustrated in Fig. [4,](#page-3-0) we conduct a comprehensive area efficiency
    evaluation of SPEED with the mixed dataflow strategy using multiple popular DNNs
    across various precisions. SPEED achieves 2.77× and 6.39× higher area efficiency
    over Ara on average under 16-bit and 8-bit precision conditions, respectively.
    Moreover, SPEED enables efficient 4 bit inference with an average area efficiency
    of up to 94.6 GOPS/mm<sup>2</sup> , surpassing the best of Ara by 12.78× on these
    DNN benchmarks.


    ![](_page_3_Figure_5.jpeg)


    <span id="page-3-1"></span>Fig. 5. Area Breakdown of (a) SPEED and (b) a single
    lane. SAU occupies only 26% of the area in a single lane while achieving significant
    computational performance.


    ## *C. Analysis of Synthesized Results*


    Fig. [5](#page-3-1) (a) illustrates the area breakdown of SPEED with a total area
    of 1.10 mm<sup>2</sup> . It reveals that the majority of SPEED''s area, up to
    90%, is occupied by the lanes. Moreover, Fig. [5](#page-3-1) (b) shows that the
    area of a lane is primarily consumed by four components: OP Queues (25%), OP Requester
    (17%), VRFs (18%) and SAU (26%). Notably, SAU serves as a key component of SPEED,
    enabling a 12.78× area efficiency improvement in DNN inference performance on
    SPEED compared to Ara. Despite the substantial performance boost, SAU accounts
    for only 26% of the lane area, which corresponds to about 24% of the total area
    of SPEED.


    TABLE I SYNTHESIZED RESULTS OF ARA AND SPEED


    <span id="page-3-2"></span>


    |                                             | Ara [6]           | SPEED (Ours)         |

    |---------------------------------------------|-------------------|----------------------|

    | ISA                                         | RV64GCV1.0        | RV64GCV1.0           |

    | Frequency                                   | 500 MHz @ 0.9 V   | 500 MHz @
    0.9 V      |

    | Chip Area (mm2<br>)                         | 0.44              | 1.10                 |

    | Int. Formats (bit)                          | 8, 16, 32, and 64 | 4, 8, 16,
    32, and 64 |

    | Power (mW)                                  | 61.14             | 215.16               |

    | Peak Int. Throughput<br>(GOPS)              | 6.82 (16b)        | 34.89 (16b)          |

    |                                             | 22.95 (8b)        | 93.65 (8b)           |

    |                                             | -                 | 287.41 (4b)          |

    | Peak Int. Area Efficiency<br>(GOPS/mm2<br>) | 15.51 (16b)       | 31.72 (16b)          |

    |                                             | 52.16 (8b)        | 85.13 (8b)           |

    |                                             | -                 | 261.28 (4b)          |

    | Peak Int. Energy Efficiency<br>(GOPS/W)     | 111.61 (16b)      | 162.15 (16b)         |

    |                                             | 373.68 (8b)       | 435.25 (8b)          |

    |                                             | -                 | 1335.79 (4b)         |

    |                                             |                   |                      |


    Table [I](#page-3-2) provides a comprehensive comparison between SPEED and Ara
    [\[6\]](#page-4-10), showing the peak throughput results of Ara and SPEED across
    various precision conditions through evaluating each convolutional layer in all
    DNN benchmarks. Based on the results in Table [I,](#page-3-2) SPEED shows 5.12×
    and 4.14× throughput improvement under 16-bit and 8-bit precision condition, respectively.
    In addition, SPEED enables efficient 16-bit and 8-bit inference that surpasses
    the best of Ara by 2.04× and 1.63×, respectively, on area efficiency. In terms
    of energy efficiency, SPEED also demonstrates enhancements of 1.45× for 16-bit
    and 1.16× for 8-bit cases, respectively.


    ## IV. CONCLUSION


    In this paper, a scalable RISC-V vector processor, namely SPEED, is proposed to
    enable efficient multi-precision DNN inference. SPEED develops several customized
    RISC-V vector instructions to efficiently support DNN operations ranging from
    4-bit to 16-bit precision. It also enhances the parallel processing capabilities
    of scalable modules to boost throughput. Moreover, a mixed dataflow strategy is
    presented that significantly improves the computational efficiency for various
    convolution kernels. Experimental results show that SPEED achieves 2.04× and 1.63×
    higher area efficiency over the pioneer open-source vector processor, Ara, under
    16-bit and 8-bit precision conditions, respectively, demonstrating its good potential
    for multi-precision DNN inference.


    ## ACKNOWLEDGMENT


    This work was supported in part by the National Key R&D Program of China under
    Grant 2022YFB4400604, in part by the National Natural Science Foundation of China
    under Grant 62174084 and 62341408, and in part by the AI & AI for Science Project
    of Nanjing University.


    ## REFERENCES


    - <span id="page-4-0"></span>[1] Y. Xu *et al.*, "Towards Developing High Performance
    RISC-V Processors Using Agile Methodology," in *2022 55th IEEE/ACM International
    Symposium on Microarchitecture (MICRO)*, 2022, pp. 1178–1199.

    - [2] D. Rossi *et al.*, "Vega: A Ten-Core SoC for IoT Endnodes With DNN Acceleration
    and Cognitive Wake-Up From MRAM-Based State-Retentive Sleep Mode," *IEEE Journal
    of Solid-State Circuits (JSSC)*, vol. 57, no. 1, pp. 127–139, 2022.

    - [3] A. Garofalo *et al.*, "Dustin: A 16-Cores Parallel Ultra-Low-Power Cluster
    With 2b-to-32b Fully Flexible Bit-Precision and Vector Lockstep Execution Mode,"
    *IEEE Transactions on Circuits and Systems I: Regular Papers (TCAS-I)*, vol. 70,
    no. 6, pp. 2450–2463, 2023.

    - [4] A. Garofalo *et al.*, "DARKSIDE: A Heterogeneous RISC-V Compute Cluster
    for Extreme-Edge On-Chip DNN Inference and Training," *IEEE Open Journal of the
    Solid-State Circuits Society (OJSSCS)*, vol. 2, pp. 231–243, 2022.

    - <span id="page-4-8"></span>[5] M. Perotti *et al.*, "Yun: An Open-Source, 64-Bit
    RISC-V-Based Vector Processor With Multi-Precision Integer and Floating-Point
    Support in 65-nm CMOS," *IEEE Transactions on Circuits and Systems II: Express
    Briefs (TCAS-II)*, vol. 70, no. 10, pp. 3732–3736, 2023.

    - <span id="page-4-10"></span>[6] M. Perotti *et al.*, "A "New Ara" for Vector
    Computing: An Open Source Highly Efficient RISC-V V 1.0 Vector Processor Design,"
    in *2022 IEEE 33rd International Conference on Application-specific Systems, Architectures
    and Processors (ASAP)*, 2022, pp. 43–51.

    - [7] M. Cavalcante *et al.*, "Ara: A 1-GHz+ Scalable and Energy-Efficient RISC-V
    Vector Processor With Multiprecision Floating-Point Support in 22-nm FD-SOI,"
    *IEEE Transactions on Very Large Scale Integration Systems (TVLSI)*, vol. 28,
    no. 2, pp. 530–543, 2020.

    - [8] M. Askarihemmat *et al.*, "Quark: An Integer RISC-V Vector Processor for
    Sub-Byte Quantized DNN Inference," in *2023 IEEE International Symposium on Circuits
    and Systems (ISCAS)*, 2023, pp. 1–5.

    - <span id="page-4-9"></span>[9] T. Dupuis *et al.*, "Sparq: A Custom RISC-V Vector
    Processor for Efficient Sub-Byte Quantized Inference," in *2023 21st IEEE Interregional
    NEWCAS Conference (NEWCAS)*, 2023, pp. 1–5.

    - <span id="page-4-3"></span>[10] Z. He *et al.*, "Agile Hardware and Software
    Co-design for RISC-V-based Multi-precision Deep Learning Microprocessor," in *2023
    28th Asia and South Pacific Design Automation Conference (ASP-DAC)*, 2023, pp.
    490–495.

    - <span id="page-4-1"></span>[11] L. Huang *et al.*, "A Precision-Scalable RISC-V
    DNN Processor with On-Device Learning Capability at the Extreme Edge," *arXiv
    preprint arXiv:2309.08186*, 2023.

    - <span id="page-4-2"></span>[12] A. Waterman, "Design of the RISC-V Instruction
    Set Architecture," Ph.D. dissertation, EECS Department, University of California,
    Berkeley, Jan 2016. [Online]. Available: [http://www2.eecs.berkeley.edu/](http://www2.eecs.berkeley.edu/Pubs/TechRpts/2016/EECS-2016-1.html)
    [Pubs/TechRpts/2016/EECS-2016-1.html](http://www2.eecs.berkeley.edu/Pubs/TechRpts/2016/EECS-2016-1.html)

    - <span id="page-4-4"></span>[13] M. Askarihemmat *et al.*, "BARVINN: Arbitrary
    Precision DNN Accelerator Controlled by a RISC-V CPU," in *2023 28th Asia and
    South Pacific Design Automation Conference (ASP-DAC)*, 2023, pp. 483–489.

    - [14] X. Wu *et al.*, "A Flexible and Efficient FPGA Accelerator for Various
    Large-scale and Lightweight CNNs," *IEEE Transactions on Circuits and Systems
    I: Regular Papers (TCAS-I)*, vol. 69, no. 3, pp. 1185–1198, 2022.

    - <span id="page-4-5"></span>[15] C. Fang *et al.*, "Accelerating 3D Convolutional
    Neural Networks Using 3D Fast Fourier Transform," in *2021 IEEE International
    Symposium on Circuits and Systems (ISCAS)*, 2021, pp. 1–5.

    - <span id="page-4-6"></span>[16] R. Ding *et al.*, "Quantized Deep Neural Networks
    for Energy Efficient Hardware-based Inference," in *2018 23rd Asia and South Pacific
    Design Automation Conference (ASP-DAC)*, 2018, pp. 1–8.

    - [17] J. Tian *et al.*, "BEBERT: Efficient and Robust Binary Ensemble BERT,"
    in *2023 IEEE International Conference on Acoustics, Speech and Signal Processing
    (ICASSP)*, 2023, pp. 1–5.

    - <span id="page-4-7"></span>[18] A. Zhou *et al.*, "Explicit Loss-Error-Aware
    Quantization for Low-Bit Deep Neural Networks," in *2018 IEEE/CVF Conference on
    Computer Vision and Pattern Recognition (CVPR)*, 2018, pp. 9426–9435.

    - <span id="page-4-11"></span>[19] F. Zaruba *et al.*, "The Cost of Application-Class
    Processing: Energy and Performance Analysis of a Linux-Ready 1.7-GHz 64-Bit RISC-V
    Core in 22-nm FDSOI Technology," *IEEE Transactions on Very Large Scale Integration
    Systems (TVLSI)*, vol. 27, no. 11, pp. 2629–2640, Nov 2019.

    - <span id="page-4-12"></span>[20] K. Li *et al.*, "A Precision-Scalable Energy-Efficient
    Bit-Split-and-Combination Vector Systolic Accelerator for NAS-Optimized DNNs on
    Edge," in *2022 Design, Automation & Test in Europe Conference & Exhibition (DATE)*,
    2022, pp. 730–735.

    - <span id="page-4-13"></span>[21] S. Liu *et al.*, "Very deep convolutional neural
    network based image classification using small training sample size," in *2015
    3rd IAPR Asian Conference on Pattern Recognition (ACPR)*, 2015, pp. 730–734.

    - <span id="page-4-14"></span>[22] K. He *et al.*, "Deep Residual Learning for
    Image Recognition," in *2016 IEEE Conference on Computer Vision and Pattern Recognition
    (CVPR)*, 2016, pp. 770–778.

    - <span id="page-4-15"></span>[23] C. Szegedy *et al.*, "Going Deeper with Convolutions,"
    in *2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*, 2015,
    pp. 1–9.

    - <span id="page-4-16"></span>[24] F. N. Iandola *et al.*, "SqueezeNet: AlexNet-level
    Accuracy with 50x Fewer Parameters and <0.5MB Model Size," *CoRR*, 2016. [Online].
    Available: <http://arxiv.org/abs/1602.07360>'
  decisions:
    evaluation_prompt: 'Qualified. Reason: The paper includes quantitative analysis
      and mentions improvements in area efficiency and computational efficiency, indicating
      evaluation.'
    related_work_prompt: 'Qualified. Reason: The paper cites numerous research papers,
      explains and compares to cited work, and includes a section discussing related
      work.'
    novelty_prompt: 'Qualified. Reason: The paper proposes a new scalable RVV processor
      called SPEED, introduces customized instructions, and presents a new dataflow
      strategy, all of which contribute to novel methods for efficient DNN inference.'
    review_only_prompt: Qualified.
- title: "SAL-PIM: A Subarray-level Processing-in-Memory Architecture with\n  LUT-based\
    \ Linear Interpolation for Transformer-based Text Generation"
  abstract: 'Text generation is a compelling sub-field of natural language processing,

    aiming to generate human-readable text from input words. In particular, the

    decoder-only generative models, such as generative pre-trained transformer

    (GPT), are widely used for text generation, with two major computational

    stages: summarization and generation. Unlike the summarization stage, which can

    process the input tokens in parallel, the generation stage is difficult to

    accelerate due to its sequential generation of output tokens through iteration.

    Moreover, each iteration requires reading a whole model with little data reuse

    opportunity. Therefore, the workload of transformer-based text generation is

    severely memory-bound, making the external memory bandwidth system bottleneck.

    In this paper, we proposed a subarray-level processing-in-memory architecture

    named SAL-PIM, HBM-based PIM architecture for the end-to-end acceleration of

    transformer-based text generation. The SAL-PIM architecture includes three

    architectural features. First, the SAL-PIM architecture utilizes higher

    internal bandwidth by integrating multiple subarray-level arithmetic logic

    units with optimized data mapping schemes. Second, the SAL-PIM architecture

    adopts LUT-based linear interpolation to perform complex non-linear functions

    in PIM. Third, the SAL-PIM architecture accelerates end-to-end inference on PIM

    in text generation. Furthermore, to validate the SAL-PIM architecture, we built

    cycle-accurate simulator and implemented the SAL-PIM''s logic units in 28-nm

    CMOS technology. As a result, when the input size is from 32 to 128 and the

    output size is from 1 to 256, SAL-PIM achieves a maximum of 4.72 times speedup

    and an average of 1.83 times speedup for the text generation based on the GPT-2

    medium model compared to the server-level GPU.'
  url: http://arxiv.org/abs/2401.17005v1
  keywords: '*—Processing-in-memory, DRAM, Transformer, Text generation.'
  document: '# SAL-PIM: A Subarray-level Processing-in-Memory Architecture with LUT-based
    Linear Interpolation for Transformer-based Text Generation


    Wontak Han, *Student Member, IEEE,* Hyunjun Cho, *Student Member, IEEE,* Donghyuk
    Kim, *Student Member, IEEE,* and Joo-Young Kim, *Senior Member, IEEE*


    **Abstract**—Text generation is a compelling sub-field of natural language processing,
    aiming to generate human-readable text from input words. Although many deep learning
    models have been proposed, the recent emergence of transformer-based large language
    models advances its academic research and industry development, showing remarkable
    qualitative results in text generation. In particular, the decoder-only generative
    models, such as generative pre-trained transformer (GPT), are widely used for
    text generation, with two major computational stages: summarization and generation.
    Unlike the summarization stage, which can process the input tokens in parallel,
    the generation stage is difficult to accelerate due to its sequential generation
    of output tokens through iteration. Moreover, each iteration requires reading
    a whole model with little data reuse opportunity. Therefore, the workload of transformer-based
    text generation is severely memory-bound, making the external memory bandwidth
    system bottleneck. In this paper, we propose a subarray-level processing-in-memory
    (PIM) architecture named SAL-PIM, the first HBM-based PIM architecture for the
    end-to-end acceleration of transformer-based text generation. With optimized data
    mapping schemes for different operations, SAL-PIM utilizes higher internal bandwidth
    by integrating multiple subarray-level arithmetic logic units (S-ALUs) next to
    memory subarrays. To minimize the area overhead for S-ALU, it uses shared MACs
    leveraging slow clock frequency of commands for the same bank. In addition, a
    few subarrays in the bank are used as look-up tables (LUTs) to handle non-linear
    functions in PIM, supporting multiple addressing to select sections for linear
    interpolation. Lastly, the channel-level arithmetic logic unit (C-ALU) is added
    in the buffer die of HBM to perform the accumulation and reduce-sum operations
    of data across multiple banks, completing end-to-end inference on PIM. To validate
    the SAL-PIM architecture, we built a cycle-accurate simulator based on Ramulator.
    We also implemented the SAL-PIM''s logic units in 28-nm CMOS technology and scaled
    the results to DRAM technology to verify its feasibility. We measured the end-to-end
    latency of SAL-PIM when it runs various text generation workloads on the GPT-2
    medium model (with 345 million parameters), in which the input and output token
    numbers vary from 32 to 128 and from 1 to 256, respectively. As a result, with
    4.81% area overhead, SAL-PIM achieves up to 4.72× speedup (1.83× on average) over
    the Nvidia Titan RTX GPU running FasterTransformer Framework.


    ✦


    **Index Terms**—Processing-in-memory, DRAM, Transformer, Text generation.


    # **1 INTRODUCTION**


    D EEP learning technology has made significant progress on various cognitive tasks,
    but the vast adoption also reveals its shortcomings, such as limited generalizability
    and lack of interpretability. Witnessing the performance saturation of early models
    such as multi-layer perceptron (MLP), convolutional neural network (CNN), and
    recurrent neural network (RNN), one notable recent innovation in deep learning
    architecture is transformer [\[1\]](#page-11-0). It has two good properties towards
    artificial general intelligence over conventional models. First, the performance
    of transformer models continues to grow with their model sizes and training data.
    Second, transformers can be pre-trained with tons of unlabeled data either through
    unsupervised or selfsupervised learning and can be fine-tuned quickly for each


    *E-mail:* {*11tak, jooyoung1203*}*@kaist.ac.kr*


    application. With the above characteristics, transformerbased models quickly become
    mainstream in natural language processing (NLP) as well as other applications
    such as image classification [\[2\]](#page-12-0) and object detection [\[3\]](#page-12-1),
    achieving higher accuracy than other deep-learning models.


    Text generation is one of the most popular applications in natural language processing,
    in which its task is to generate human-readable and plausible sentences from input
    words called tokens. Text generation is highly demanded in the conversational
    AI industry, including chatbots [\[4\]](#page-12-2) and article writing [\[5\]](#page-12-3).
    It is also essential for automatic code generation within the context of no-code
    development paradigm [\[6\]](#page-12-4). Among many transformer models, a decoderonly
    transformer model from OpenAI named generative pretrained transformer (GPT) [\[7\]](#page-12-5),
    [\[8\]](#page-12-6) achieves notable performance in text generation. GPT models
    are pretrained on massive internet data with enormous parameter numbers, raising
    the quality of text generation close to the human level.


    The text generation process consists of summarization


    <sup>•</sup> *The authors are with the Department of Electrical Engineering, Korea
    Advanced Institute of Science and Technology (KAIST), Daejeon 34141, South Korea.*


    *Manuscript received April 19, 2005; revised August 26, 2015.*


    ![](_page_1_Figure_0.jpeg)


    <span id="page-1-0"></span>Fig. 1. The execution time by input and output size
    for the GPT-2 medium model on a GPU.


    and generation stages. In the summarization stage, all input tokens are computed
    simultaneously to generate a single output token that will be the input token
    of the generation stage. On the other hand, the generation stage sequentially
    generates output tokens through iterations (i.e., an output token per iteration)
    because the output token becomes the input token of the next iteration. Figure
    [1](#page-1-0) shows the execution time of the text generation based on the GPT-2
    medium model running on Nvidia Titan RTX GPU, where the input size (i.e., the
    number of input tokens) and output size (i.e., the number of output tokens) vary.
    As the graph shows, an increase in the output size linearly increases the total
    execution time, while an increase in the input size has little impact. This result
    is because the GPU can efficiently handle a batch of input tokens, but it cannot
    perform parallel processing in the generation stage due to its sequential nature.
    In addition, the GPT operations in each iteration of the generation stage are
    mostly memory-bounded, with a large model size. Furthermore, the size of transformer
    models has a trend of exponential increase (e.g., max 1.5 billion parameters in
    GPT-2 [\[7\]](#page-12-5) and 175 billion parameters in GPT-3 [\[8\]](#page-12-6)).
    It is easily concluded that the performance of text generation depends on the
    number of iterations in the generation stage and the system''s effective memory
    bandwidth.


    Processing-in-memory (PIM) is a promising solution for the memory bandwidth problem.
    By performing operations near memory, PIM can utilize higher memory bandwidth,
    hence improving performance significantly for memorybound operations. Accordingly,
    many PIM works have been proposed to accelerate memory-bound applications [\[9\]](#page-12-7),
    [\[10\]](#page-12-8), [\[11\]](#page-12-9), [\[12\]](#page-12-10), [\[13\]](#page-12-11),
    [\[14\]](#page-12-12), [\[15\]](#page-12-13), [\[16\]](#page-12-14). Most studies
    have focused on accelerating the bit-wise operation and general matrixvector multiplication
    (GEMV). It is reasonable because GEMV is a typical memory-bound operation and
    occupies a large portion of operations. However, the transformerbased models also
    include additional non-linear functions, which require complex operations that
    are hard to compute using multiplication and addition. The non-linear functions
    are compute-bound operations than memory-bound operations, and these functions
    are not negligible in the entire execution time.


    All things considered, there are two ways to achieve higher speedup in PIM. First,
    the higher bandwidth for memory-bound operation enables higher speedup. GEMV operations
    for larger weights and biases require higher bandwidth. Second, acceleration for
    non-linear functions also significantly improves overall performance. Not only
    do non-linear functions take up a large part, but if PIM accelerates GEMV operation,
    the proportion of non-linear functions in total execution time increases further.
    Moreover, supporting non-linear functions in PIM enables accelerating end-to-end
    model inference. End-to-end acceleration in PIM removes data movement for the
    intermediate data and prevents the overhead caused by switching between PIM operation
    and host (generally CPU or GPU) operations [\[12\]](#page-12-10), [\[17\]](#page-12-15).


    The acceleration of transformer-decoder-based generative models in PIM faces several
    challenges due to its characteristics. First, previous bank-level PIMs have limited
    bandwidth, and the triple-row activation scheme induces a high latency. It is
    necessary to use higher bandwidth with low latency to cope with the number of
    parameters that continue to grow. Second, integrating hardware units in PIM for
    all necessary operations of the model is challenging. PIM cannot use sufficient
    metal and area because it is implemented in DRAM technology. Therefore, PIM should
    support complex operations with limited hardware units. Lastly, data movement
    is time-consuming, and it is a critical overhead for the entire execution time
    in PIM. Performing operations simultaneously at the bank-level causes data movement
    between banks. Furthermore, data movement between PIM and the host also affects
    overall execution time. Accordingly, supporting end-to-end model inference in
    PIM can eliminate wasteful data movement.


    To address the above challenges, we propose SAL-PIM, a subarray-level PIM architecture
    for accelerating the endto-end transformer-decoder-based generative model in PIM.
    We make the following contributions.


    - We present the SAL-PIM architecture that includes subarray-level ALUs (S-ALUs),
    bank-level units, and channel-level ALUs (C-ALUs). Two types of ALUs (S-ALU and
    C-ALU) are integrated into the subarray-level and the channel-level, respectively.
    S-ALU utilizes higher bandwidth than bank-level PIM to compute memorybound operation,
    and C-ALU supports accumulation and reduce-sum operation for multiple banks, eliminating
    data movement between banks.

    - We propose acceleration for non-linear functions using a look-up table (LUT)-based
    linear interpolation in PIM. It enables the computation of complex functions using
    S-ALUs without additional hardware. Moreover, we optimized a few subarrays, named
    LUT-embedded subarray, in the bank for LUT operations in DRAM.

    - We present a mapping scheme across subarray, bank, and channel for the SAL-PIM
    architecture. The mapping method enables higher utilization for subarray-level
    computation and removes data reshaping operations, such as transpose operation,
    using two input feeding methods and two accumulation directions.

    - We evaluated the SAL-PIM architecture using a simulator based on the latest
    DRAM simulator [\[18\]](#page-12-16). SAL-PIM achieves a maximum 4.72× speedup
    and an average 1.83× speedup compared to GPU in text generation (input size 32
    to 128 and output size 1 to 256) with the GPT-2 medium model. Furthermore, in
    order to verify the feasibility of PIM, we implemented units of the SAL-PIM


    ![](_page_2_Figure_1.jpeg)


    <span id="page-2-0"></span>Fig. 2. GPT structure and text generation process.


    architecture in 28-nm logic technology and scaled to 20 nm DRAM technology.


    # <span id="page-2-2"></span>**2 BACKGROUND**


    #### **2.1 Workload Analysis of GPT-based Text Generation**


    Text generation''s task is generating sentences from given input words. Figure
    [2](#page-2-0) shows the text generation process and structure of GPT. As aforementioned,
    text generation consists of two stages: summarization and generation. The summarization
    stage simultaneously computes input tokens (*"Hello, my name"*), as shown in Figure
    [2.](#page-2-0) Each token is embedded in a vector, so input tokens are embedded
    in a matrix. Therefore, general matrix-matrix multiplications (GEMMs) are mainly
    performed in the summarization stage, and the operations are compute-bounded due
    to the parameters being reused for vectors in the embedded matrix. At the end
    of the summarization stage, GPT makes one output token (*''is''*). Unlike the
    summarization stage, the generation stage''s input is only the former output token
    (*''is''*). The one input token is embedded into one vector. Thus, the overall
    operation in the generation stage consists of GEMVs which are memory-bound operations
    because the input is one vector. Furthermore, the generation stage sequentially
    generates output tokens, and the model''s parameters should be loaded repeatedly
    for each token generation. Correspondingly, the higher memory bandwidth improves
    overall inference speed.


    GPT is one of the most famous transformer-decoderbased generative models. As shown
    in Figure [2,](#page-2-0) GPT comprises an embedding layer and decoder layers.
    In the embedding layer, input tokens are converted to vectors through the embedding
    table and added with positional vectors by the token''s position. After passing
    through the embedding layer, the vector (generation stage) or matrix (summarization
    stage) goes to the decoder layers. The decoder layer consists of a multi-head
    attention (MHA), a feed-forward network (FFN), layerNorms, and residual operations.


    #### *2.1.1 Multi-Head Attention (MHA)*


    An MHA is the most time-consuming layer for GPT in text generation. In the MHA,
    query (*Q*), key (*K*), and value (*V*)


    ![](_page_2_Figure_10.jpeg)


    <span id="page-2-1"></span>Fig. 3. The breakdown of execution time for the GPT-2
    medium model on a GPU.


    are derived from the input vectors. Generated *K* and *V* are concatenated with
    *Ks* and *Vs*, which are generated in the former token generations, respectively.
    Accordingly, the number of *K* and *V* increases as token generation progresses.
    The MHA consists of three steps. First, *Q*, *K*, and *V* are computed from the
    input vectors through multiplication with weights and addition with biases. The
    generated *Q*, *K*, and *V* are separated by heads and calculated between each
    other from the same head. Second, *Q* is multiplied by *K* T , and the result
    is called the score. The score is masked by the token position and generates an
    attention score (*S*) through softmax. Then *S* is multiplied by *V*, and the
    results are concatenated for all heads. *Q*×*K* T and *S*×*V* are GEMV in each
    head, but due to the transpose, directions of the matrixvector multiplication
    are different. Lastly, the concatenated vectors are computed by a fully-connected
    layer, then added with residual input.


    #### *2.1.2 Feed-Forward Network (FFN)*


    An FFN is composed of two fully-connected layers and an activation function. The
    fully-connected layers perform multiplication with weight and addition with bias.
    Its intermediate vector size is four times larger than the input vector of the
    decoder layer, so the weight and bias are larger than the fully-connected layer
    of MHA. Moreover, GELU, which consists of complex functions (*tanh* and *sqrt*),
    is usually used as an activation function. These complex functions are generally
    done with series calculations, which are timeconsuming. Therefore, the FFN occupies
    a large part of the execution time of the decoder layer due to the large GEMV
    and complex operations in the activation function.


    #### *2.1.3 Layer Normalization (layerNorm)*


    The remained parts of the decoder layer are two layerNorms. The layerNorm is a
    non-linear function like the softmax in the MHA and GELU in the FFN. Firstly,
    average and standard deviation are computed position-wise for the input vector.
    Then, each data of the input vector is subtracted from the average and divided
    by the standard deviation. The result goes to a fully-connected operation to generate
    an output vector.


    Figure [3](#page-2-1) shows the breakdown of execution time for the GPT-2 medium
    model on Nvidia Titan RTX GPU. The ratio


    TABLE 1 DRAM-based Processing-in-Memory


    <span id="page-3-0"></span>


    | Type                  | Bandwidth | Latency | Operation              | Example                                    |

    |-----------------------|-----------|---------|------------------------|--------------------------------------------|

    | Triple-row Activation | Very high | High    | Bit-wise operation     | Ambit[9],
    ComputeDRAM[10],<br>TransPIM[11] |

    | Bank-level PIM        | Medium    | Low     | ALUs                   | FIM[12,
    20], AiM[13, 21]                   |

    | Subarray-level PIM    | High      | Low     | Logic gates or<br>ALUs | DRISA[14],
    Fulcrum[15], SAL-PIM            |


    of the MHA and FFN is 50.26% and 29.36%, respectively, which accounts for the
    most significant portion. Therefore, a higher speedup can be expected when the
    MHA and FFN are accelerated with higher bandwidth, so previous studies aimed to
    accelerate GEMV, the main operation of the MHA and FFN, showed notable improvement.
    However, as shown in Figure [3,](#page-2-1) the non-linear functions are also
    important in the total execution time. Softmax in the MHA, GELU in the FFN, and
    layerNorm are the non-linear function, which is a compute-bound operation, occupying
    23.45%. In addition, if the MHA and FFN are accelerated, the portion of non-linear
    functions is further increased. As a result, accelerating the non-linear functions
    enable higher speedup than only accelerating GEMV.


    ## **2.2 DRAM-based Processing-in-Memory**


    Conventional systems using the original DRAM have limited memory bandwidth by
    a memory standard called JEDEC [\[19\]](#page-12-17). In machine learning applications,
    the limitation remarkably affects the overall latency because most operations
    are memory-bound operations, such as multiplication and addition, on a tremendous
    number of data. PIM enables leveraging higher bandwidth thanks to performing computation
    nearby memory cells. Accordingly, PIM has been actively studied to exploit the
    higher bandwidth. [\[9\]](#page-12-7), [\[10\]](#page-12-8), [\[11\]](#page-12-9),
    [\[12\]](#page-12-10), [\[13\]](#page-12-11), [\[14\]](#page-12-12), [\[15\]](#page-12-13),
    [\[16\]](#page-12-14).


    Table [1](#page-3-0) summarizes DRAM-based PIMs. PIM architecture has two types.
    The first type computes data using existing memory operations, such as activation,
    read, and precharge, without additional ALU. For example, Ambit [\[9\]](#page-12-7)
    and ComputeDRAM [\[10\]](#page-12-8) perform the bit-wise functions by sharing
    bit-line with triple-row activation. These accomplish impressive speedup using
    enormous bandwidth. TransPIM [\[11\]](#page-12-9) also adopts triple-row activation
    with subarray-level adder tree, accelerating transformer models. However, it is
    challenging to fully utilize bandwidth on complex operations for a small-size
    vector, and the latency is long because row activation is repeated in DRAM. Moreover,
    in order to compute complex functions, a Taylor series-like approach [\[11\]](#page-12-9)
    is applied, which requires a lot of additions and multiplications.


    The second type computes data using ALUs in the memory. FIM [\[12\]](#page-12-10),
    [\[20\]](#page-12-18) and AiM [\[13\]](#page-12-11), [\[21\]](#page-12-19) perform
    MAC operations in the bank, and these are optimized to compute GEMV. However,
    the number of parameters in the model is increasing, and the higher bandwidth
    makes PIM expect a high speedup. Therefore, bank-level parallelism is insufficient
    and should be extended to the subarray-level. There are also previously proposed
    subarray-level PIM studies [\[14\]](#page-12-12), [\[15\]](#page-12-13). DRISA
    [\[14\]](#page-12-12) implements logic gates or adders


    ![](_page_3_Figure_7.jpeg)


    <span id="page-3-1"></span>Fig. 4. Linear interpolation with a look-up table for
    GELU.


    in subarray and achieves higher bandwidth. Nevertheless, it is not appropriate
    for computing non-linear functions. Fulcrum [\[15\]](#page-12-13) puts an ALU
    in a subarray, which is flexible with the operation by subarray. Although Fulcrum
    has flexibility, which can perform various operations in PIM, it is more critical
    for PIM to target transformer-based models to optimize memory-bound operations,
    such as GEMV. As a result, a PIM structure that can perform memory-bound operations
    with high bandwidth and accelerate non-linear function is required.


    ## **2.3 Linear Interpolation**


    Linear interpolation is one of the approximation methods to compute complex functions
    with limited hardware. Nonlinear functions in GPT have complex functions, such
    as *exp*, *tanh*, and *sqrt*. Hence, to accelerate the non-linear functions in
    PIM, a method of computing these with limited hardware is required because PIM
    has limited area and power to implement computing units. Figure [4](#page-3-1)
    shows linear interpolation for GELU. The range of input data is divided into sections,
    and an LUT stores pre-calculated slopes (W) and intercepts (B) for each section.
    Then, when the input comes in, find the section that the input belongs to through
    decoding. The slope and intercept of the corresponding section are multiplied
    and added with input, respectively. Thus, the complex functions can be calculated
    with only one multiplication and addition. In other words, the compute-bound operation
    is projected into the memory-bound operation with LUT, so PIM is appropriate with
    linear interpolation. Naturally, if the number of sections is insufficient, there
    is the possibility of accuracy loss. So, we measured accuracy loss by linear interpolation
    with the GPT-2 medium model on text generation, and the accuracy was kept when
    the number of sections was larger than 32.


    Some previous works use linear interpolation. MVP [\[22\]](#page-12-20) uses dynamic
    linear interpolation, which dynamically sets the section size, to calculate various
    types of activation functions. MVP has additional memory to store LUT, but PIM
    can use DRAM cells as LUT. Also, there is PIM-based linear interpolation. AiM
    [\[21\]](#page-12-19) applies linear interpolation to the results of adder trees.
    However, it only computes the activation function. Therefore, to accelerate the
    end-to-end inference of the model, a PIM structure that can calculate linear interpolation
    with DRAM cell-based LUT is needed.


    # <span id="page-3-2"></span>**3 SAL-PIM ARCHITECTURE**


    SAL-PIM supports three architectural features to address challenges when accelerating
    the execution of GPT endto-end. First, it exploits subarray-level parallelism
    for


    ![](_page_4_Figure_0.jpeg)


    <span id="page-4-0"></span>Fig. 5. Overall architecture of SAL-PIM.


    subarray-level ALUs. The memory-bound operations are computed with a higher bandwidth
    than the previous banklevel PIM. Second, the SAL-PIM architecture adopts LUTbased
    linear interpolation, thus minimizing circuit overhead for the complex non-linear
    functions required. It further eliminates the LUT overhead by reusing the existing
    DRAM cells. Third, the SAL-PIM architecture includes channellevel ALUs, which
    perform operations for all banks in the same channel to support the entire GPT
    operations in memory. Accordingly, data movement between the host and PIM is minimized.


    #### **3.1 Overall Architecture**


    The overall architecture of SAL-PIM is based on HBM2, as shown in Figure [5.](#page-4-0)
    The SAL-PIM architecture is composed of four DRAM dies and a buffer die, which
    are connected through silicon vias (TSVs). The DRAM die has a hierarchical structure
    of channels, banks, and subarrays. Each channel consists of 16 banks, which are
    connected to data buses shared with the other banks in the same channel. Hence,
    the original HBM2 cannot access multiple banks simultaneously. In contrast, the
    SAL-PIM architecture can access multiple banks simultaneously by integrating computation
    logic units near each bank. Each bank includes subarrays with 512 rows [\[23\]](#page-12-21).
    Each subarray reads data to a bit-line sense amplifier (BLSA) connected to the
    local bitlines (LBLs) for the column address. Then, the LBLs are connected to
    global bit-lines (GBLs), which are used as the path for subarray-level ALUs. In
    addition, the SAL-PIM architecture includes channel-level ALUs and interconnection,
    which connects channels, on the buffer die.


    The SAL-PIM architecture includes three types of logic units: subarray-level ALU
    (S-ALU), bank-level unit, and channel-level ALU (C-ALU).


    • S-ALU is responsible for principle operations, such as element-wise addition,
    element-wise multiplication, MAC, and max, with subarray-level parallelism. In
    order to exploit subarray-level parallelism, the SAL-PIM architecture employs
    previous research [\[23\]](#page-12-21), utilizing multiple subarrays. Multiple
    subarrays in the bank are activated simultaneously by using a BLSA as a cache
    for each subarray. Ideally, simultaneously operating all of the subarrays can
    utilize the maximum bandwidth. However, integrating S-ALU into all subarrays causes
    impractical area overhead, so the subarrays in the bank are grouped to use the
    S-ALU. For example, if the number of S-ALU is 4 in a bank, the subarray group
    consists of 15 subarrays without LUT-embedded subarray. The S-ALU is connected
    to the GBLs to receive weight from memory. Therefore, when multiple subarrays
    are activated, these are floated and transfer data to each S-ALU, while GBLs generally
    operate as a single connected data path. Consequently, the SAL-PIM architecture
    enables utilizing higher internal bandwidth.


    In addition, linear interpolation for the non-linear functions is also performed
    by multiplication and addition using S-ALU. As aforementioned in Section [2,](#page-2-2)
    the nonlinear functions are compute-bound which occupies a large portion of the
    total execution time. The SAL-PIM architecture uses LUT-based linear interpolation
    to reduce the computation intensity and accelerate non-linear functions. In the
    SAL-PIM architecture, the S-ALU computes linear interpolation by multiplying slope
    and adding intercept while subarrays are used as LUT. Thus, the acceleration of
    non-linear functions is achieved in the SAL-PIM architecture without additional
    hardware.


    - A bank-level unit supports feeding input data for S-ALU and generating select
    signals for LUT-embedded subarray. MHA in GPT needs two types of matrix multiplication
    (*Q*×*K* T and *S*×*V*), so the SAL-PIM architecture has two directional input
    feeding schemes for eliminating transpose operation. The bank-level unit selects
    whether to broadcast the same input for all MACs in S-ALUs or different inputs
    for each MAC in S-ALUs. Also, decoding units in the bank-level units generate
    select signals for LUT-embedded subarray. When the LUT-embedded subarrays operate
    like conventional memory, the decoding units decode addresses. In contrast, when
    they act as LUT, the decoding units decode data from a register in the bank-level
    unit.

    - C-ALU performs accumulation and reduce-sum operations for multiple banks. To
    maximize the utilization of S-ALUs, All banks in SAL-PIM operate in parallel.
    Hence, the SAL-PIM architecture needs to merge each bank''s output, so C-ALU merges
    the outputs of banks and broadcasts the results to all banks in the same channel.
    C-ALU minimizes data movement between each bank in the same channel.


    As a result, by using these three types of units, SAL-PIM supports all operations
    of the transformer-decoderbased generative model, including non-linear functions,
    fully-connected operations, and multi-head operations. The detailed circuit of
    each unit is described in Section [4.](#page-6-0)


    ## **3.2 Data Mapping**


    SAL-PIM adopts flexible data mapping schemes across various layers of GPT to run
    end-to-end inference efficiently. The data mapping of SAL-PIM considers the hardware
    architecture and dataflows, maximizing the utilization of increased bandwidth.
    Moreover, the flexible data mapping minimizes the data movement that occurs when
    the outputs are distributed back to the subarrays for the next layer as inputs.
    As mentioned in Section [2,](#page-2-2) the decoder layer of GPT consists of FFN,
    MHA, and layerNorms. These are decomposed into three types of computations (i.e.,
    non-linear function, matrix-vector operation, and multi-head operation), where
    each has a different dataflow. In addition, the SAL-PIM ar-


    ![](_page_5_Figure_0.jpeg)


    <span id="page-5-0"></span>Fig. 6. Data mapping schemes for SAL-PIM. (a) Non-linear
    function. (b) Matrix-vector operation. (c) Multi-head operation accumulated by
    subarrays. (d) Multi-head operation accumulated by banks.


    chitecture supports three levels of hierarchy that can operate in parallel (i.e.,
    channels, banks, and subarrays), each of which has different physical characteristics.
    The higher level hierarchically, the further away from the memory cell, the higher
    the cost of moving data. For example, data movement between channels is more time-consuming
    because data moves between them through interconnection on the buffer die. Thus,
    each channel should be mapped with independent weight, which does not require
    accumulation. For the banks, the output can be merged in C-ALU, so the weight
    is mapped to utilize parallelism supremely. The subarrays are tightly coupled,
    allowing faster and wider data movement among the subarrays. Thus, SAL-PIM can
    adopt data mapping, which performs the accumulation of partial sums among the
    subarrays.


    Data mapping schemes of the SAL-PIM architecture reflect three considerations.
    First, SAL-PIM supports the data mapping scheme that minimizes data movement within
    and between the computations. Since the weight data of GPT is large and must be
    tiled, the additional data movement is incurred by partial sums among the tiles,
    which are generated within each computation. Furthermore, data movement between
    the computations is incurred since the output of each computation is used as the
    input of the following computation. Second, SAL-PIM''s data mapping also maximizes
    bandwidth utilization for memory-bound operations. Most operations in the generation
    stage have no reuse of weight at all, which can be executed faster with higher
    bandwidth. The extended bandwidth from subarraylevel parallelism is fully utilized
    through data mapping. Third, SAL-PIM supports the data mapping scheme that eliminates
    any data movement of intermediate data for the two featured operations of MHA,
    such as concatenation and transposition. Concatenation needs frequent data movement
    for the concatenated matrices because it is performed by reading all of the data
    and concatenating them. The data mapping removes the data movement by sequentially
    mapping them to banks. Also, transposition is time-consuming due to reshaping
    extensive data and requires additional buffers. The SAL-PIM architecture eliminates
    transpose operation using data mapping and input feeding schemes.


    Figure [6](#page-5-0) illustrates the data mapping schemes for SAL-PIM. The parameters
    associated with the data mapping in SAL-PIM are PCh, PBa, and PSub, and each refers
    to parallelism by channels, banks, and S-ALUs, respectively.


    First, the data mapping scheme reduces the data movement within and between computations.
    As shown in Figure [6\(](#page-5-0)b), [6\(](#page-5-0)c), and [6\(](#page-5-0)d),
    the SAL-PIM architecture is mapped column of matrix or head of the multi-head
    operation to PCh, minimizing data movement for partial sums within the computation.
    Furthermore, data movement caused by the accumulation of partial sums is reduced
    between banks in the same channel using C-ALU. In addition, the output of computations
    leads to the input of the following computations in GPT. In particular, the non-linear
    functions are in the middle of MHA and FFN, so these functions are connected to
    the other computations directly. As shown in Figure [6\(](#page-5-0)a), in the
    case of the non-linear function, two data mapping schemes exist to minimize data
    movement when the non-linear function continues to other computations. For example,
    if the matrix-vector operation is followed by the non-linear function, the input
    vector is tiled in banks and duplicated in channels. On the contrary, if the multi-head
    operation is followed, the input vector is tiled both banks and channels. The
    mapping schemes for non-linear functions support the same tiling shapes as the
    other computations input, so data movements are eliminated. Therefore, the flexible
    data mapping schemes enable the non-linear function''s output to seamlessly be
    used as input for the following computation.


    In addition, the data mapping scheme maximizes bandwidth utilization. In the case
    of the non-linear function, the number of computations is small due to linear
    interpolation. So, SAL-PIM cannot utilize the bandwidth fully, and then the data
    mapping scheme aims to minimize data movement. On the other hand, in the case
    of matrix-vector and multihead operations, which are memory-bounded. Therefore,
    these operations utilize all three types of parallelism, as shown in Figures [6\(](#page-5-0)b),
    [6\(](#page-5-0)c), and [6\(](#page-5-0)d). For the matrix-vector operation, the
    rows of the matrix are mapped according to PCh and PSub, and the columns are divided
    into PBa. In contrast, for the multi-head operation, the heads, which are independent
    of other heads, are mapped on PCh, and PBa and PSub divide the row or column of
    the matrix. Both cases need accumulation between banks, and SAL-PIM uses C-ALUs
    to simplify bank-level accumulation without data movement between banks.


    Lastly, the data mapping scheme enables two operations in MHA for text generation:
    concatenation and transposition. The multi-head operation concatenates *Ks* and
    *Vs* in text generation in order to project the previous word in the generation
    of the next word. Therefore, the hardware must enable supporting concatenation
    of *K* and *V*, and in SAL-PIM, concatenation is performed by mapping the bank
    sequentially, as shown in Figure [6\(](#page-5-0)c) and [6\(](#page-5-0)d). Furthermore,
    in the multi-head operation, two types of matrix multiplication are needed (*Q*×*K*
    T and *S*×*V*). To accelerate these, PIM should support transpose operation for
    matrix, which is time-consuming and requires an additional buffer. In contrast,
    SAL-PIM supports two accumulation directions and two input feeding methods, which
    eliminates the need for transpose operations in PIM (Figure [6\(](#page-5-0)c)
    and [6\(](#page-5-0)d)).


    #### *3.2.1 Dataflow for GPT in SAL-PIM*


    These data mapping schemes reduce data movement and enable acceleration of the
    overall computation of the depicted GPT in Figure 2 using PIM. The input tokens
    of GPT generate an output token through the embedding layer, positional encoding,
    layerNorm, MHA, FFN, and residual addition. Firstly, the input token is translated
    into a vector through the embedding operation. The embedding operation simply
    finds the vector corresponding to the dictionary, which can be performed by reading
    the vector at the corresponding address in DRAM. The vector is divided into banks
    and added with a position vector duplicated in all channels. Then, the vector
    performs layerNorm. layerNorm subtracts the mean and divides the vector by the
    standard deviation, followed by the GEMV operation. The mean and standard deviation
    are obtained through reduction operation in the S-ALU and C-ALU and linear interpolation
    to compute the reciprocal square root. The mean and standard deviation are broadcasted
    to all banks and subtracted and multiplied, respectively. The calculated vector
    performs GEMV operation within its channel, and then the computed vector is broadcasted
    to all channels for further computations.


    Moving on to the MHA, it consists of *Q*, *K*, *V* generation, *Q*×*K* T , softmax,
    *S*×*V*, and a GEMV operation. Firstly, *Q*, *K*, and *V* generations perform
    GEMV operation three times each. At this stage, the heads are divided on each
    channel, allowing independent operations for each head. *Q* is duplicated to all
    banks, and *K* and *V* values are sequentially mapped to the banks and concatenated.
    Then, *Q*×*K* T is performed as shown in Figure 6(d), and the results are stored
    back in each bank. The computed data is linearly interpolated for exponential
    calculation and summed in the C-ALU for softmax operation. The sum is broadcasted
    to all banks, where it performs linear interpolation for the reciprocal operation
    and is multiplied to compute *S*. Finally, *S* is multiplied by *V* in each bank
    as depicted in Figure 6(c), and the output is accumulated across all banks in
    C-ALU. Then, the GEMV operation is performed to complete MHA.


    The output of the MHA is reshaped into a single channel, and layerNorm is applied
    again after residual addition. The resulting vector is then broadcasted across
    all channels, similar to the previous stage. The FFN stage consists of two GEMV
    operations and an activation function(GELU). The FFN stage consists of two GEMV
    operations and an activation function called GELU. The two GEMV operations are
    performed as shown in Figure 6(b), and the GELU activation is applied using linear
    interpolation. The resulting vector from the FFN stage undergoes another residual
    addition. All decoder layers in GPT are configured in the same manner, so SAL-PIM
    iterates these operations. As a result, SAL-PIM can minimize data movement and
    maximize parallelism in the GPT inference.


    # <span id="page-6-0"></span>**4 IN-MEMORY CIRCUIT DESIGN**


    SAL-PIM has three compute units (S-ALUs, Bank-level units, and C-ALUs) and optimized
    subarrays (LUTembedded subarrays). This chapter describes detailed circuits and
    operation flow.


    ![](_page_6_Figure_6.jpeg)


    S-ALU **-** Regs


    Bank-level Register


    **>>**


    <span id="page-6-1"></span>Fig. 7. Circuit design of S-ALU.


    **Max**


    *32,768 rows = 64 subarrays*


    ## **4.1 Subarray-level ALU (S-ALU)**


    As aforementioned in Section [3,](#page-3-2) S-ALU performs principal arithmetic
    operations of SAL-PIM with subarray-level parallelism. Figure [7](#page-6-1) shows
    the circuit of S-ALU. S-ALU consists of 16-bit fixed-point MAC units, 16×32-bit
    registers, and right shifters. The S-ALU''s operation proceeds in three steps.
    First, read data from memory to the bank-level unit via GBLs, and the data is
    used as input for S-ALUs. Second, S-ALU calculates the data read from memory and
    the input broadcasted from the bank-level unit. The results are stored and accumulated
    in S-ALU''s registers. Lastly, the data of the S-ALU''s registers are written
    in the memory. When writing the data from S-ALU''s registers, the results stored
    in the registers are 32-bit precision, but the GBL has half of the precision.
    Therefore, the results are shifted and truncated by fraction bit using shifters.
    Then, the tri-state buffer connects the register''s output and the GBLs to write
    the result to the memory.


    As shown in the table of Figure [7,](#page-6-1) S-ALU''s MAC supports four operations:
    element-wise addition and multiplication, MAC, and max. The max operation is used
    for linear interpolation of exponential in softmax. The exponential function has
    a wide range of data; thus, it is computed using linear interpolation after subtracting
    by the maximum. The MAC units enable the computation of three types of operands
    from memory (GBL), a bank-level register in the bank-level unit, and registers
    in S-ALU. The primary operand of S-ALU is data from memory, and GBL is connected
    during a single operation to receive data from all subarrays. Therefore, one S-ALU
    is performed at a time to compute linear interpolation using LUT-embedded subarrays.
    On the contrary, when S-ALUs compute simultaneously in the case of the matrix-vector
    operation or the multi-head operation, the control signal cuts off the GBLs connected
    with other S-ALUs. So the S-ALUs operate individually read memory at a time.


    In general, most transformer-based models use 32-bit floating-point precision.
    However, in the previous work [\[24\]](#page-12-22), the transformer models are
    sufficient with 8-bit precision. Furthermore, the high bit precision of the register
    enables minimizing the data loss of accumulation. In order to verify accuracy
    when fixed-point precision is used, we evaluated the accuracy of the lambada dataset
    [\[25\]](#page-12-23) using


    ![](_page_7_Figure_0.jpeg)


    <span id="page-7-0"></span>Fig. 8. Circuit of LUT-embedded subarray and bank-level
    unit.


    16-bit fixed-point precision. As a result, the accuracy only dropped about 2.8%
    on the GPT-2 medium model without other quantization schemes.


    The subarray-level parallelism enables the PIM architecture to achieve outstanding
    performance, but it has a drawback for the area overhead. As PSub increases, the
    area overhead increases proportionally. So, SAL-PIM uses shared MACs in S-ALU,
    which leverage faster computation units than memory read. Several recent studies,
    including [\[26\]](#page-12-24), [\[27\]](#page-12-25), have utilized MAC units
    operating at a frequency of 1GHz in logic technology. Moreover, in the recent
    PIM research [\[28\]](#page-12-26) uses 1GHz ALU by utilizing faster transistors
    in DRAM peripheral area. On the other hand, ALUs in recent PIM works mostly can
    be only utilized at 250MHz. In HBM2, the clock frequency is 1GHz, and HBM2 reads
    data on tccds = 2tck (500MHz) for maximum bandwidth with bank interleaving. However,
    in PIM, the all-bank mode operates more slowly than the bank-interleaving case
    because the same banks are consecutively accessed. Accordingly, the memory reads
    or writes data on tccdl = 4tck (250MHz), whereas the frequency limitation of ALU
    is higher than the memory read or write speed. Therefore, by computing part of
    the data several times while memory moves to the next address, the total number
    of MACs is reduced, and the area is optimized. For example, 16×16-bit inputs are
    read from memory at one read, and the 16×16-bit MACs are required. However, 8×16-bit
    MACs can compute the inputs in two computations thanks to a faster speed (500MHz).
    We implemented shared MAC using a standard cell library to verify its feasibility,
    achieving a clock frequency of over 800MHz. Considering the 22% performance degradation
    between DRAM and logic technology [\[29\]](#page-12-27), it is confirmed that
    the shared MACs are feasible. As a result, when we implemented the S-ALU, the
    8 MACs on 500MHz were about 30% smaller than the 16 MACs on 250MHz.


    #### **4.2 LUT-embedded Subarray**


    SAL-PIM uses LUT-based linear interpolation to compute complex functions. However,
    the original DRAM is unsuitable for performing LUT operations. In the DRAM, a
    subarray is divided into several MATs [\[30\]](#page-12-28). All MATs of the subarray
    are operated simultaneously, and a columnselect signal selects which column in
    the MAT is connected to the GBLs. However, LUT needs different addresses for the
    various data. Suppose the original subarray is used as


    ![](_page_7_Figure_6.jpeg)


    <span id="page-7-1"></span>Fig. 9. Operation flow of LUT-embedded subarray.


    LUT. In that case, it is necessary to repeatedly approach the subarray as many
    times as the number of data to find the corresponding value in LUT. These are
    inefficient tasks for DRAM, and these are time-consuming due to a lot of activation
    and precharge. Therefore, an LUT-embedded subarray is optimized for LUT operation
    in DRAM.


    The circuit of the LUT-embedded subarray is shown in Figure [8.](#page-7-0) The
    LUT-embedded subarray is almost identical to the original subarray except for
    the path to the columnselect signal. The LUT-embedded subarray has different column-select
    signal paths for each MAT, and each MAT receives a different column-select signal.
    The column-select signals are generated from a column address or data in the bank-level
    register. However, although MATs operate independently on column addresses, it
    is not a sufficient solution if one row cannot store all of LUT due to the many
    sections for linear interpolation. Accordingly, the LUT-embedded subarrays are
    activated together, and an LUT selector determines which LUT-embedded subarray
    is connected with GBLs. As a result, a number of LUT-embedded subarrays in the
    bank and the number of column addresses enable using many sections for linear
    interpolation.


    Figure [9](#page-7-1) shows the operation flow of the LUT-embedded subarray. The
    timing parameter is the same as the parameters in the original DRAM. The operation
    flow consists of four parts. First, activate all rows for source, destination,
    slope (W), and intercept (B). Thanks to existing research [\[23\]](#page-12-21),
    it is possible to activate simultaneously for different subarrays. Second, read
    source data from the memory to the bank-level register. The data is used for generating
    columnselect signals and LUT-select signals. Third, S-ALU multiplies the slopes
    and adds intercepts for linear interpolation. The data read from the LUT-embedded
    subarray moves through GBL and is computed in an S-ALU. Lastly, the data of register
    in S-ALU, results of linear interpolation, is written to memory cells. At the
    end of the operation, all subarrays are precharged. As shown in Figure [9,](#page-7-1)
    the activation and precharge commands are issued once at the start and the end,
    respectively. Consequently, the LUT-embedded subarray enables linear interpolation
    for the entire data in bank-level register at a time, which can be up to 16 times
    faster than the original DRAM, which performs linear interpolation done one by
    one.


    #### **4.3 Bank-level Unit**


    The bank-level unit is responsible for generating select signals to perform LUT-embedded
    subarrays and broadcasting input data to S-ALUs. The bank-level unit consists
    of a banklevel register and decoding units. The bank-level register


    ![](_page_8_Figure_0.jpeg)


    <span id="page-8-0"></span>Fig. 10. Circuit design of channel-level ALU.


    broadcasts the input to S-ALUs in the same bank with two methods. The first is
    each data of the bank-level register goes to each MAC of S-ALU. It is for element-wise
    computations. The other method is only single data of the bank-level register
    is broadcasted to all of the MACs in the bank. It is used in MAC operation and
    enables each MAC to accumulate the output.


    Furthermore, data in the bank-level register is used as addresses of LUT-embedded
    subarray. Figure [8](#page-7-0) also shows the circuit of the bank-level unit.
    The decoding units in the bank-level unit have two types: column decoder and subsel
    decoder. Each is responsible for generating column-select signals and LUT-select
    signals, respectively. Making the select signals through decoding in an appropriate
    bit position for fixed-point bit precision is possible. The bit position means
    linear interpolation region. If the slope and intercept are generated in intervals
    from -4 to 4, the decoding units decode addresses based on the interval. Therefore,
    the right shifters select the bit position since each function''s proper linear
    interpolation range differs.


    #### **4.4 Channel-level ALU (C-ALU)**


    C-ALU supports merging operations for multiple banks in the same channel. Because
    the SAL-PIM architecture operates multiple banks simultaneously, the merging procedures
    for banks should be performed. When the data is moved from PIM to the host, the
    energy caused by data movement is consumed, and PIM should wait for the host''s
    work. In addition, since the summed result is used as input for the following
    operation, the result returns immediately from the host to the PIM and is broadcasted
    to the banks. These operations, such as the accumulation of banks and reduce-sum
    operation, are performed in C-ALU, which is implemented in the buffer die of HBM2
    for each channel. At the buffer die, although there is no bandwidth, it is possible
    to eliminate data movement by performing simple addition. Likewise, if bank-level
    adder trees exist, PIM can reduce the execution time for merging operations. However,
    since accumulations by banks are small in GEMV, it is wasteful to implement it
    at the bank, considering the area overhead.


    Figure [10](#page-8-0) illustrates the circuit of C-ALU, which consists of two
    channel vector registers (16×16-bit), two scalar registers (16-bit), and configurable
    adders. The configurable adders have sixteen adders, which act as two types of
    modules, either an accumulator or an adder tree, depending on the command. In
    the case of accumulation, the configurable adders accumulate memory output to
    the channel vector register. The accumulated data is broadcasted to memory or
    summed into one data. In the case of reduce-sum operation, the configurable adders
    operate as an adder tree to compute summation about the channel vector register.
    The summed result is stored in the channel scalar register. The


    TABLE 2 SAL-PIM configuration


    <span id="page-8-1"></span>


    | HBM2 Configuration | Channels/die = 8 (pch/die = 16), Banks/channel = 32 (Banks/pch
    = 16),<br>Subarrays/bank = 64, Row/subarray = 512,<br>Row Size = 1KB, MAT Size
    = 512x512, DQ Size = 128-bit/channel |  |  |  |  |

    |--------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--|--|--|--|

    |                    | Timing parameters(ns) BL = 4, tRC = 45, tRCD = 16, tRAS
    = 29, tCL = 16, tRRD = 2, tCCDS = 2, tCCDL = 4                                                                             |  |  |  |  |

    | LUT-subarray       | Number of LUT-subarray (Slope & Intercept) = 4,<br>Number
    of Sections for Linear Interpolation = 64                                                                                |  |  |  |  |

    | S-ALU              | PSub = 4, MACs/S-ALU = 8, Register Size = 16×32-bit                                                                                                                                |  |  |  |  |

    | Bank-level Unit    | Column Decoder = 16×5 to 32 decoders,<br>Sub-sel Decoder
    = 16×1 to 2 decoders,<br>Bank-level register = 16×16-bit                                                                  |  |  |  |  |

    | C-ALU              | Channel-vector register = 16×16-bit<br>Channel scalar register
    = 16-bit<br>Flexible adders = 16×16-bit adders                                                                      |  |  |  |  |


    channel scalar register''s data is broadcasted to memory for layerNorm or softmax.


    # **5 EVALUATION**


    #### **5.1 Methodology & Configuration**


    We evaluated the SAL-PIM architecture on the simulator modified from the latest
    DRAM simulator [\[18\]](#page-12-16). The SAL-PIM is based on conventional HBM2
    [\[19\]](#page-12-17), and the configuration with timing parameters [\[31\]](#page-12-29)
    is shown in Table [2.](#page-8-1) As shown in Table [2,](#page-8-1) we evaluated
    the SAL-PIM architecture of PSub=1, 2, and 4, so the maximum bandwidth is four
    times larger than bank-level PIM when PSub is 4. In addition, the SAL-PIM architecture
    applied linear interpolation with 64 sections on GELU, *exp*, *sqrt*, and reciprocal
    operations. As aforementioned, when the number of sections is larger than 32,
    it has no accuracy drop by linear interpolation.


    The transformer-decoder-based generative model we used for the evaluation is the
    GPT-2 medium model (with 345 million parameters) [\[7\]](#page-12-5), which computes
    the vector with a size of 1,024 and has 24 decoder layers. The model''s overall
    structure is the same as in Figure [2](#page-2-0) of Section [2.](#page-2-2) We
    compared the SAL-PIM architecture to the serverlevel GPU, Nvidia Titan RTX [\[32\]](#page-12-30).
    The GPU used 24GB GDDR6 memory with 1.77GHz clock frequency. The maximum available
    bandwidth is 672GB/s, 2.63× larger than the maximum bandwidth of HBM2. The GPU
    executed the GPT-2 medium model from FasterTransformer Framework [\[33\]](#page-12-31)
    for comparison baseline.


    ## **5.2 Area & Power**


    Firstly, we implemented units of the SAL-PIM architecture to verify its feasibility.
    These units are implemented using standard cells of the TSMC 28-nm technology
    with Design Compiler. Since the previous HBM2-PIM [\[20\]](#page-12-18) was fabricated
    on the Samsung 20-nm DRAM technology, we scaled the implemented 28-nm area to
    the 20-nm area on DRAM technology. Prior research [\[11\]](#page-12-9), [\[14\]](#page-12-12),
    [\[15\]](#page-12-13), [\[29\]](#page-12-27) has shown that the area difference
    between DRAM technology and CMOS logic technology is approximately 1.8 times.
    To be conservative, we further increased this area overhead by 3.6 times, doubling
    the initial scaling factor. Additionally, we measured the power consumption of
    the units using the PrimePower tool [\[34\]](#page-12-32).


    ![](_page_9_Figure_0.jpeg)


    <span id="page-9-1"></span>Fig. 11. Speedup of SAL-PIM compared to GPU for text
    generation by input and output sizes.


    TABLE 3 Summarization of area and power consumption of units in the SAL-PIM architecture


    <span id="page-9-0"></span>


    |                 | Area/Unit | Area/Channel         | Power Consumption  |

    |-----------------|-----------|----------------------|--------------------|

    | HBM2 8GB        | -         | 53.15mm2             | 60W (power budget) |

    | S-ALU           | 18,744um2 | 2.40mm2 (128 S-ALUs) | 5.298mW            |

    | Bank-level Unit | 4,847um2  | 0.16mm2 (32 banks)   | 0.926mW            |

    | C-ALU           | 19,126um2 | 0.02mm2 (1 C-ALUs)   | 2.749mW            |


    ![](_page_9_Figure_4.jpeg)


    Table [3](#page-9-0) summarizes the area and power consumption of each unit. Accordingly,
    When the SAL-PIM has PSub=4, the number of S-ALU is 64 in a channel, and the area
    overhead is 4.81% compared to conventional HBM2 [\[11\]](#page-12-9). The overhead
    is far below than 25% threshold mentioned in previous work [\[13\]](#page-12-11).
    Accordingly, the area overhead is acceptable, and SAL-PIM does not need to sacrifice
    memory capacity. Furthermore, we checked the power consumption of SAL-PIM architecture.
    When the power consumption is maximum, all of S-ALU performs MAC operations simultaneously,
    it is only 9.04% of the total power budget of HBM2 [\[31\]](#page-12-29). The
    results indicate that the SAL-PIM architecture is feasible with the permissible
    area and power overheads.


    #### **5.3 Comparison with GPU**


    We evaluated the performance of the SAL-PIM architecture for text generation by
    various input and output sizes. As aforementioned in Section [2,](#page-2-2) when
    the input size is large, the number of the input vectors in the summarization
    stage is increased, and when the output size is large, the number of token generations
    in the generation stage is increased. Figure [11](#page-9-1) shows the speedup
    compared to the GPU by input and output sizes. The input lengths are 32, 64, and
    128 tokens, and the output length is from 1 to 256 tokens. The input and output
    sizes were determined by the typical ranges of user requests based on previous
    work [\[35\]](#page-12-33). In addition, because the SAL-PIM architecture supports
    GPT end-to-end, the overhead from data movement is considered on the simulator.
    Therefore, comparing the SAL-PIM architecture using the software simulator with
    GPU is reliable.


    As shown in Figure [11,](#page-9-1) in the case of increasing the input size,
    The speedup decreases. The decrement in performance is because the increase in
    summarization means an increase in operations that can be performed in parallel,
    and the high-performance GPU performs operations much faster.


    However, SAL-PIM operates on a slower frequency, and the number of ALU is far
    less than the GPU.


    <span id="page-9-2"></span>Fig. 12. Speedup of SAL-PIM for GEMV compared to bank-level
    PIM.


    On the contrary, in the case of increasing the output size, the speedup tends
    to grow for the same input size. The improvement is because the time to execute
    the model once for one input vector is much shorter than that of the GPU. Hence,
    the longer the generation stage, the higher the speedup of the SAL-PIM. As a result,
    when PSub is 4, the maximum speedup of SAL-PIM is 4.72× compared to the GPU when
    the input size is 32, and the output size is 128. Then, the average speedup is
    1.83× compared to the GPU. This result indicates the maximum speedup lower than
    the gain obtained from subarray-level parallelism. This means means that even
    though SAL-PIM supports memory-bound operation of generation stage much faster
    than the GPU, the summarization stage and compute-bound operations degrade overall
    performance. However, this performance increase in the generation stage is meaningful
    because the output tokens tend to be much longer in general workloads of text
    generation application.


    ## **5.4 Comparison with Bank-level PIM**


    SAL-PIM architecture has a much higher maximum bandwidth compared to Bank-level
    PIM, so we evaluated the SAL-PIM architecture compared to the bank-level PIM architecture
    to verify whether this high bandwidth is being fully utilized is necessary. The
    bank-level PIM architecture is based on Newton [\[13\]](#page-12-11), which has
    multipliers and adder trees in each bank. We modified the SAL-PIM simulator to
    the Bank-level PIM simulator with the same configuration as Table [2.](#page-8-1)
    Figure [12](#page-9-2) shows the SAL-PIM''s speedup for GEMV operation compared
    to the bank-level PIM.


    ![](_page_10_Figure_0.jpeg)


    <span id="page-10-0"></span>Fig. 13. Execution time comparison for the LUT operation
    between LUTembedded subarray and two other linear interpolation cases.


    The maximum bandwidth of SAL-PIM is 4× larger than bank-level PIM because of PSub=4.
    However, in the case of the small size of the input and output vector, the minimum
    speedup is only 1.75×, as shown in Figure [12.](#page-9-2) The degradation is
    because accumulation is needed for the SAL-PIM by mapping scheme, but bank-level
    PIM does not require the bank-level data movement. Hence, with larger input and
    output vector sizes, the portion of data movement is smaller, and the speedup
    is closer to maximum gain. In the GPT-2 medium model, the vector length is only
    1,024, but the latest transformer-decoder-based generative model [\[8\]](#page-12-6)
    has a longer vector length of up to 12,288. Therefore, acceleration through subarray-level
    parallelism is required for a higher performance increase for the large-size model.


    # **6 ADDITIONAL ANALYSIS**


    ## **6.1 LUT-embedded subarray**


    The SAL-PIM architecture uses the linear interpolation method to compute the non-linear
    functions. Furthermore, SAL-PIM has the LUT-embedded subarray to optimize LUT
    operations on DRAM. However, the LUT operation can be performed through other
    methods in the original DRAM subarrays. So. we evaluated the LUT-embedded subarray
    between the other two cases. Case 1 is *Scan*, which scans all regions of LUT
    storing slope and intercept. For example, when the number of sections is 64, the
    slopes and intercepts are stored in 128 addresses. In that case, starting from
    the beginning, read to the end and find the slope and intercept corresponding
    to the section. Case 2 is *Select*. This is the case that the LUT decodes the
    corresponding address from the first data to the last data to which the LUT is
    applied and finds the slopes and intercepts sequentially.


    Figure [13](#page-10-0) shows the Execution time comparison of the LUT-embedded
    subarray and two linear interpolation cases without the LUT-embedded subarray.
    Case 1 shows the worst execution time because the size of the bank-level register
    is limited. Furthermore, in this case, the number of sections is only 64. For
    a large number of sections, Case 1 is worsened due to the large region to scan.
    In Case 2, the LUT operation is performed on only one data at a time in a bank.
    Therefore, In the case of large vectors, more speedup can be expected for the
    LUT-embedded subarray. Accordingly, the LUT-embedded subarray shows a 3.57× speedup
    in a case with a vector size of 16,384, as shown in Figure [13.](#page-10-0)


    ![](_page_10_Figure_7.jpeg)


    <span id="page-10-1"></span>Fig. 14. The execution time and average bandwidth
    by subarray-level parallelism on text generation.


    ![](_page_10_Figure_9.jpeg)


    <span id="page-10-2"></span>Fig. 15. The power consumption for subarray-level
    parallelism in 32 token generations of the GPT-2 medium model.


    #### **6.2 Subarray-level Parallelism**


    The subarray-level parallelism enables to use of an enormous bandwidth maximum
    of 8TB/s when PSub is 4. However, if the higher bandwidth is not fully used, there
    is no advantage for the area and power overheads caused by S-ALUs. Therefore,
    we evaluated the SAL-PIM architecture on various PSub. Figure [14](#page-10-1)
    shows the execution time and average bandwidth by the number of subarray-level
    parallelisms on text generation.


    The SAL-PIM architecture uses subarray-level parallelism for matrix-vector and
    multi-head operations, occupying about 60% of the total execution time in text
    generation applications. Hence, the SAL-PIM architecture achieves a 2.11× speedup
    when PSub is 4 compared to when PSub is 1, as shown in Figure [14,](#page-10-1)
    showing utilizes higher bandwidth efficiently for memory-bound operations. Similarly,
    the average bandwidth of the case of PSub is 4 is also about two times larger
    than the case of PSub is 1. Furthermore, considering there are larger models than
    the GPT-2 medium model, the higher bandwidth can be used efficiently to accelerate
    memory-bound operations.


    Subarray-level parallelism offers increased bandwidth but consumes more energy.
    Operating multiple banks and subarrays simultaneously result in higher energy
    consumption compared to conventional DRAM operations. However, it provides advantages
    for energy due to the shorter data movement distances. Figure [15](#page-10-2)
    illustrates the power consumption of SAL-PIM for various values of PSub.


    In our evaluation, we conducted 32 token generations using the GPT-2 medium model.
    The energy consumption was assigned as follows [\[31\]](#page-12-29): eact = 909pJ,
    epre−gsa = 1.51pJ/bit, epost−gsa = 1.17pJ/bit, and eio = 0.80pJ/bit. We assumed
    that 26% of the total HBM poewr budget is allocated for refresh operations [\[36\]](#page-12-34).
    As depicted in Figure [15,](#page-10-2) when PSub is 1 or 2, the power consumption
    remains significantly lower than the power budget, while for PSub is 4, it exceeds
    the power budget by 24.0%. Despite this, considering the high power usage expected
    for computations in CPUs or GPUs, SAL-PIM can still be considered sufficiently
    power-efficient for transformer-decoder-based text generation. Furthermore, This
    result assumes that the ALUs are always operating. Therefore, there is room for
    further power savings through increasing bulk capacitance and optimization techniques
    such as clock gating or power gating.


    ## **6.3 Future Work**


    While SAL-PIM presents a novel architecture for efficient end-to-end text generation
    within the PIM paradigm, leveraging its inherent high bandwidth, further investigation
    is necessary to address two key challenges. These areas represent valuable future
    directions for optimizing SAL-PIM''s performance and scalability.


    The first challenge concerns the performance bottleneck by the summarization stage.
    The compute-bound nature of summarization significantly impedes SAL-PIM''s overall
    speedup, diminishing the potential gains from its high bandwidth advantage. To
    mitigate this, future research should explore mapping and operation strategies
    that offload the summarization stage to dedicated accelerators like GPUs or NPUs
    while reserving PIM for the generation stage. This heterogeneous execution scheme
    could potentially unlock the full performance potential of SAL-PIM.


    The second challenge stems from the ever-growing size of modern LLMs, which often
    exceed the capacity of single PIM units. Consequently, SAL-PIM requires a new
    approach to accommodate these expansive models. Two promising avenues for future
    research lie in exploiting parallelism: Intra-PIM parallelism through pipelined
    weight write, read, and computation operations and Inter-PIM parallelism via synchronization
    and workload distribution across multiple PIM instances. Additionally, recent
    research [\[37\]](#page-12-35) has proposed leveraging the inherent bias in output
    tokens to selectively load weights, minimizing memory footprint. Therefore, sparsity-aware
    data mapping and handling is another intriguing direction for PIM research.


    By addressing these challenges and actively pursuing the outlined future work,
    SAL-PIM holds immense potential as a groundbreaking architecture for efficient
    and scalable endto-end text generation within the PIM domain.


    # **7 RELATED WORKS**


    #### **7.1 Accelerators for Transformer models**


    There are previous works for accelerating transformer models. SpAtten [\[26\]](#page-12-24)
    accelerates the attention through sparsity handling using cascade token and head
    pruning. Also, ELSA [\[27\]](#page-12-25) suggests the architecture with hardware-software
    co-design for self-attention. These achieve higher performance improvements than
    GPU but only accelerate attention in the model. DFX [\[35\]](#page-12-33) proposes
    the architecture for accelerating end-to-end text generation on multi-FPGA. It
    was actually implemented and verified using FPGAs and showed a higher performance
    increase in the generation stage compared to GPU. However, DFX uses many HBMs
    in FPGAs, so SAL-PIM is a promising solution that uses high bandwidth with fewer
    HBMs.


    ## **7.2 PIM using Look-up-table**


    There are a few architectures PIM using LUT. Lacc [\[38\]](#page-12-36), pluto
    [\[39\]](#page-12-37), and LT-PIM [\[40\]](#page-12-38) use DRAM cells as LUT
    and suggest novel circuit designs for LUT operation. Accordingly, these show notable
    performance for massive computations. However, there is a challenge to calculate
    using only the LUT. In order to scan cases in LUT, the latency increases exponentially
    in the case of higher bit-precision. In addition, most machine learning uses a
    higher bit precision of 16 bit or more. Therefore, SAL-PIM uses LUT-based linear
    interpolation to support higher bit precision. AiM [\[21\]](#page-12-19) also
    utilizes LUT-based linear interpolation to support various activation functions.
    However, it is limited to supporting activation functions and is only applied
    to the result data of a bank-level adder tree. On the other hand, SAL-PIM supports
    a wide range of non-linear functions and utilizes LUT-embedded subarrays to apply
    LUT-based linear interpolation to multiple data at once.


    # **8 CONCLUSION**


    With the emergence of the transformer, the performance of machine learning has
    increased faster in several applications. However, as larger pretrained transformer-based
    models have been developed, the overall execution time has slowed. To address
    this issue, We propose the SAL-PIM architecture, the first PIM architecture to
    accelerate the endto-end transformer-decoder-based generative model. SAL-PIM uses
    S-ALU to accelerate memory-bound operation by utilizing subarray-level parallelism
    with the optimized mapping scheme. Moreover, it optimizes the area overhead of
    S-ALU by sharing MAC units leveraging the slow clock frequency of commands for
    the same bank. C-ALU is integrated on the buffer die in HBM2 and performs accumulation
    and reduce-sum operations for multiple banks. Furthermore, SAL-PIM adopts linear
    interpolation to compute complex non-linear functions to support end-to-end inference.
    Then, a LUT-embedded subarray is suggested to optimize LUT operation in DRAM.


    We have implemented the proposed logic units of SAL-PIM architecture in a 28-nm
    technology to verify its feasibility. Also, we built the SAL-PIM simulator based
    on Ramulator to evaluate the architecture. As a result, when the input size is
    from 32 to 128 and the output size is from 1 to 256, SAL-PIM achieves a maximum
    of 4.72× speedup and an average of 1.83× speedup for the text generation based
    on the GPT-2 medium model compared to the GPU.


    # **REFERENCES**


    <span id="page-11-0"></span>[1] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit,
    L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin, "Attention is all you need,"
    *Advances in neural information processing systems*, vol. 30, 2017.


    - <span id="page-12-0"></span>[2] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D.
    Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S.
    Gelly *et al.*, "An image is worth 16x16 words: Transformers for image recognition
    at scale," *arXiv preprint arXiv:2010.11929*, 2020.

    - <span id="page-12-1"></span>[3] N. Carion, F. Massa, G. Synnaeve, N. Usunier,
    A. Kirillov, and S. Zagoruyko, "End-to-end object detection with transformers,"
    in *Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28,
    2020, Proceedings, Part I*, 2020, pp. 213–229.

    - <span id="page-12-2"></span>[4] J. Schulman, B. Zoph, C. Kim, J. Hilton, J.
    Menick, J. Weng, J. Uribe, L. Fedus, L. Metz, M. Pokorny *et al.*, "Chatgpt: Optimizing
    language models for dialogue," 2022.

    - <span id="page-12-3"></span>[5] "A robot wrote this entire article. are you
    scared yet, human? gpt-3," Sep 2020. [Online]. Available: [https://www.theguardian.](https://www.theguardian.com/commentisfree/2020/sep/08/robot-wrote-this-article-gpt-3)
    [com/commentisfree/2020/sep/08/robot-wrote-this-article-gpt-3](https://www.theguardian.com/commentisfree/2020/sep/08/robot-wrote-this-article-gpt-3)

    - <span id="page-12-4"></span>[6] R. Sanchis, O. Garc ´ ´ıa-Perales, F. Fraile,
    and R. Poler, "Low-code as enabler of digital transformation in manufacturing
    industry," *Applied Sciences*, vol. 10, no. 1, p. 12, 2019.

    - <span id="page-12-5"></span>[7] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei,
    I. Sutskever *et al.*, "Language models are unsupervised multitask learners,"
    *OpenAI blog*, vol. 1, no. 8, p. 9, 2019.

    - <span id="page-12-6"></span>[8] T. Brown, B. Mann, N. Ryder, M. Subbiah, J.
    D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell *et al.*,
    "Language models are few-shot learners," *Advances in neural information processing
    systems*, vol. 33, pp. 1877–1901, 2020.

    - <span id="page-12-7"></span>[9] V. Seshadri, D. Lee, T. Mullins, H. Hassan,
    A. Boroumand, J. Kim, M. A. Kozuch, O. Mutlu, P. B. Gibbons, and T. C. Mowry,
    "Ambit: In-memory accelerator for bulk bitwise operations using commodity dram
    technology," in *2017 50th Annual IEEE/ACM International Symposium on Microarchitecture
    (MICRO)*. IEEE, 2017, pp. 273–287.

    - <span id="page-12-8"></span>[10] F. Gao, G. Tziantzioulis, and D. Wentzlaff,
    "Computedram: Inmemory compute using off-the-shelf drams," in *Proceedings of
    the 52nd annual IEEE/ACM international symposium on microarchitecture*, 2019,
    pp. 100–113.

    - <span id="page-12-9"></span>[11] M. Zhou, W. Xu, J. Kang, and T. Rosing, "Transpim:
    A memorybased acceleration via software-hardware co-design for transformer," in
    *2022 IEEE International Symposium on High-Performance Computer Architecture (HPCA)*.
    IEEE, 2022, pp. 1071–1085.

    - <span id="page-12-10"></span>[12] S. Lee, S.-h. Kang, J. Lee, H. Kim, E. Lee,
    S. Seo, H. Yoon, S. Lee, K. Lim, H. Shin *et al.*, "Hardware architecture and
    software stack for pim based on commercial dram technology: Industrial product,"
    in *2021 ACM/IEEE 48th Annual International Symposium on Computer Architecture
    (ISCA)*. IEEE, 2021, pp. 43–56.

    - <span id="page-12-11"></span>[13] M. He, C. Song, I. Kim, C. Jeong, S. Kim,
    I. Park, M. Thottethodi, and T. Vijaykumar, "Newton: A dram-maker''s accelerator-inmemory
    (aim) architecture for machine learning," in *2020 53rd Annual IEEE/ACM International
    Symposium on Microarchitecture (MICRO)*. IEEE, 2020, pp. 372–385.

    - <span id="page-12-12"></span>[14] S. Li, D. Niu, K. T. Malladi, H. Zheng, B.
    Brennan, and Y. Xie, "Drisa: A dram-based reconfigurable in-situ accelerator,"
    in *2017 50th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)*.
    IEEE, 2017, pp. 288–301.

    - <span id="page-12-13"></span>[15] M. Lenjani, P. Gonzalez, E. Sadredini, S.
    Li, Y. Xie, A. Akel, S. Eilert, M. R. Stan, and K. Skadron, "Fulcrum: a simplified
    control and access mechanism toward flexible and practical insitu accelerators,"
    in *2020 IEEE International Symposium on High Performance Computer Architecture
    (HPCA)*. IEEE, 2020, pp. 556– 569.

    - <span id="page-12-14"></span>[16] D. Kim, C. Yu, S. Xie, Y. Chen, J.-Y. Kim,
    B. Kim, J. Kulkarni, and T. T.-H. Kim, "An overview of processing-in-memory circuits
    for artificial intelligence and machine learning," *IEEE Journal on Emerging and
    Selected Topics in Circuits and Systems*, 2022.

    - <span id="page-12-15"></span>[17] F. Devaux, "The true processing in memory
    accelerator," in *2019 IEEE Hot Chips 31 Symposium (HCS)*. IEEE Computer Society,
    2019, pp. 1–24.

    - <span id="page-12-16"></span>[18] Y. Kim, W. Yang, and O. Mutlu, "Ramulator:
    A fast and extensible dram simulator," *IEEE Computer architecture letters*, vol.
    15, no. 1, pp. 45–49, 2015.

    - <span id="page-12-17"></span>[19] J. Standard, "High bandwidth memory (hbm)
    dram," *Jesd235*, vol. 16, 2013.

    - <span id="page-12-18"></span>[20] Y.-C. Kwon, S. H. Lee, J. Lee, S.-H. Kwon,
    J. M. Ryu, J.-P. Son, O. Seongil, H.-S. Yu, H. Lee, S. Y. Kim *et al.*, "25.4
    a 20nm 6gb function-in-memory dram, based on hbm2 with a 1.2 tflops programmable
    computing unit using bank-level parallelism, for machine learning applications,"
    in *2021 IEEE International Solid-State Circuits Conference (ISSCC)*, vol. 64.
    IEEE, 2021, pp. 350–352.

    - <span id="page-12-19"></span>[21] S. Lee, K. Kim, S. Oh, J. Park, G. Hong, D.
    Ka, K. Hwang, J. Park, K. Kang, J. Kim *et al.*, "A 1ynm 1.25 v 8gb, 16gb/s/pin
    gddr6-


    based accelerator-in-memory supporting 1tflops mac operation and various activation
    functions for deep-learning applications," in *2022 IEEE International Solid-State
    Circuits Conference (ISSCC)*, vol. 65. IEEE, 2022, pp. 1–3.


    - <span id="page-12-20"></span>[22] J.-H. Kim, S. Lee, S. Moon, S. Yoo, and J.-Y.
    Kim, "19.2 a 409.6 gops and 204.8 gflops mixed-precision vector processor system
    for general-purpose machine learning acceleration," in *2022 IEEE Asian Solid-State
    Circuits Conference (A-SSCC)*. IEEE, 2022.

    - <span id="page-12-21"></span>[23] Y. Kim, V. Seshadri, D. Lee, J. Liu, and O.
    Mutlu, "A case for exploiting subarray-level parallelism (salp) in dram," in *2012
    39th Annual International Symposium on Computer Architecture (ISCA)*. IEEE, 2012,
    pp. 368–379.

    - <span id="page-12-22"></span>[24] A. H. Zadeh, I. Edo, O. M. Awad, and A. Moshovos,
    "Gobo: Quantizing attention-based nlp models for low latency and energy efficient
    inference," in *2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture
    (MICRO)*. IEEE, 2020, pp. 811– 824.

    - <span id="page-12-23"></span>[25] D. Paperno, G. Kruszewski, A. Lazaridou, Q.
    N. Pham, R. Bernardi, S. Pezzelle, M. Baroni, G. Boleda, and R. Fernandez, ´ "The
    lambada dataset: Word prediction requiring a broad discourse context," *arXiv
    preprint arXiv:1606.06031*, 2016.

    - <span id="page-12-24"></span>[26] H. Wang, Z. Zhang, and S. Han, "Spatten: Efficient
    sparse attention architecture with cascade token and head pruning," in *2021 IEEE
    International Symposium on High-Performance Computer Architecture (HPCA)*. IEEE,
    2021, pp. 97–110.

    - <span id="page-12-25"></span>[27] T. J. Ham, Y. Lee, S. H. Seo, S. Kim, H. Choi,
    S. J. Jung, and J. W. Lee, "Elsa: Hardware-software co-design for efficient, lightweight
    selfattention mechanism in neural networks," in *2021 ACM/IEEE 48th Annual International
    Symposium on Computer Architecture (ISCA)*. IEEE, 2021, pp. 692–705.

    - <span id="page-12-26"></span>[28] S. Cho, H. Choi, E. Park, H. Shin, and S.
    Yoo, "Mcdram v2: In-dynamic random access memory systolic array accelerator to
    address the large model problem in deep neural networks on the edge," *IEEE Access*,
    vol. 8, pp. 135 223–135 243, 2020.

    - <span id="page-12-27"></span>[29] Y.-B. Kim and T. W. Chen, "Assessing merged
    dram/logic technology," *Integration, the VLSI Journal*, vol. 2, no. 27, pp. 179–194,
    1999.

    - <span id="page-12-28"></span>[30] V. Seshadri and O. Mutlu, "In-dram bulk bitwise
    execution engine," *arXiv preprint arXiv:1905.09822*, 2019.

    - <span id="page-12-29"></span>[31] M. O''Connor, N. Chatterjee, D. Lee, J. Wilson,
    A. Agrawal, S. W. Keckler, and W. J. Dally, "Fine-grained dram: Energy-efficient
    dram for extreme bandwidth systems," in *Proceedings of the 50th Annual IEEE/ACM
    International Symposium on Microarchitecture*, 2017, pp. 41–54.

    - <span id="page-12-30"></span>[32] Nvidia titan rtx. [Online]. Available: [https:](https://www.nvidia.com/content/dam/en-zz/Solutions/titan/documents/titan-rtx-for-creators-us-nvidia-1011126-r6-web.pdf)
    [//www.nvidia.com/content/dam/en-zz/Solutions/titan/](https://www.nvidia.com/content/dam/en-zz/Solutions/titan/documents/titan-rtx-for-creators-us-nvidia-1011126-r6-web.pdf)

    - <span id="page-12-31"></span>[documents/titan-rtx-for-creators-us-nvidia-1011126-r6-web.pdf](https://www.nvidia.com/content/dam/en-zz/Solutions/titan/documents/titan-rtx-for-creators-us-nvidia-1011126-r6-web.pdf)
    [33] Nvidia fastertransformer. [Online]. Available: [https://github.](https://github.com/NVIDIA/FasterTransformer)
    [com/NVIDIA/FasterTransformer](https://github.com/NVIDIA/FasterTransformer)

    - <span id="page-12-32"></span>[34] Primepower. [Online]. Available: [https://www.synopsys.com/](https://www.synopsys.com/implementation-and-signoff/signoff/primepower.html)
    [implementation-and-signoff/signoff/primepower.html](https://www.synopsys.com/implementation-and-signoff/signoff/primepower.html)

    - <span id="page-12-33"></span>[35] S. Hong, S. Moon, J. Kim, S. Lee, M. Kim,
    D. Lee, and J.-Y. Kim, "Dfx: A low-latency multi-fpga appliance for accelerating
    transformer-based text generation," in *2022 55th IEEE/ACM International Symposium
    on Microarchitecture (MICRO)*. IEEE, 2022, pp. 616–630.

    - <span id="page-12-34"></span>[36] H. H. Shin, Y. M. Park, D. Choi, B. J. Kim,
    D.-H. Cho, and E.- Y. Chung, "Extreme: Exploiting page table for reducing refresh
    power of 3d-stacked dram memory," *IEEE Transactions on Computers*, vol. 67, no.
    1, pp. 32–44, 2017.

    - <span id="page-12-35"></span>[37] K. Alizadeh, I. Mirzadeh, D. Belenko, K. Khatamifard,
    M. Cho, C. C. Del Mundo, M. Rastegari, and M. Farajtabar, "Llm in a flash: Efficient
    large language model inference with limited memory," *arXiv preprint arXiv:2312.11514*,
    2023.

    - <span id="page-12-36"></span>[38] Q. Deng, Y. Zhang, M. Zhang, and J. Yang,
    "Lacc: Exploiting lookup table-based fast and accurate vector multiplication in
    dram-based cnn accelerator," in *Proceedings of the 56th Annual Design Automation
    Conference 2019*, 2019, pp. 1–6.

    - <span id="page-12-37"></span>[39] J. D. Ferreira, G. Falcao, J. Gomez-Luna,
    M. Alser, L. Orosa, ´ M. Sadrosadati, J. S. Kim, G. F. Oliveira, T. Shahroodi,
    A. Nori *et al.*, "Pluto: Enabling massively parallel computation in dram via
    lookup tables," in *2022 55th IEEE/ACM International Symposium on Microarchitecture
    (MICRO)*. IEEE, 2022, pp. 900–919.

    - <span id="page-12-38"></span>[40] R. Zhou, S. Tabrizchi, A. Roohi, and S. Angizi,
    "Lt-pim: An lut-based processing-in-dram architecture with rowhammer selftracking,"
    *IEEE Computer Architecture Letters*, 2022.


    ![](_page_13_Picture_0.jpeg)


    **Wontak Han** (Graduate Student Member, IEEE) received the B.S. degree in Electronic
    Engineering from Hanyang University, Seoul, South Korea, in 2021, and the M.S.
    degree in electrical engineering from Korea Advanced Institute of Science and
    Technology (KAIST), Daejeon, South Korea, in 2023. He is currently an engineer
    in Samsung Electronics DRAM Design Team.


    His research interests include energy-efficient processing-in/near-memory architecture,
    and


    deep-learning network and database management system (DBMS) accelerators.


    ![](_page_13_Picture_4.jpeg)


    **Hyunjun Cho** (Graduate Student Member, IEEE) received the B.S. degree in Electrical
    Engineering from Korea Advanced Institute of Science and Technology (KAIST), Daejeon,
    South Korea, in 2023, where he is currently pursuing the M.S. degree.


    His research interests include energy-efficient processing-in/near memory architecture,
    design of neural processing units (NPUs), and the design of hardware architectures
    that preserve security.


    ![](_page_13_Picture_7.jpeg)


    **Donghyuk Kim** (Graduate Student Member, IEEE) received the B.S. degree in electrical
    and computer engineering from University of Washington, Seattle, USA, in 2020.
    He received the M.S. degree in electrical and computer engineering from Korea
    Advanced Institute of Science and Technology (KAIST), Daejeon, South Korea, in
    2022, where he is currently pursuing the Ph.D. degree.


    His research interests include energy-efficient processing-in-memory architecture
    for machine


    learning and database, and deep neural network accelerators.


    ![](_page_13_Picture_11.jpeg)


    **Joo-Young Kim** (Senior Member, IEEE) received the B.S., M.S., and Ph. D. degrees
    in Electrical Engineering from KAIST, Daejeon, South Korea, in 2005, 2007, and
    2010, respectively. He is currently an Associate Professor in the School of Electrical
    Engineering at KAIST. He is also the Director of AI Semiconductor Systems Research
    Center. His research interests span various aspects of hardware design, including
    VLSI design, computer architecture, FPGA, domain-specific accelerators, hard-


    ware/software co-design, and agile hardware development. Before joining KAIST,
    he was a Senior Hardware Engineering Lead at Microsoft Azure, Redmond, WA, USA,
    working on hardware acceleration for the hyper-scale big data analytics platform
    named Azure Data Lake. He was also one of the initial members of Catapult project
    at Microsoft Research, Redmond, WA, USA, where he deployed a fabric of FPGA accelerators
    in datacenters to accelerate critical cloud services, such as machine learning,
    data storage, and networking.'
  decisions:
    evaluation_prompt: 'Qualified. Reason: The paper includes experimental validation
      with a cycle-accurate simulator and provides quantitative results such as speedup
      comparisons over a GPU.'
    related_work_prompt: 'Qualified. Reason: The paper engages with prior research
      by citing multiple research papers, explaining and comparing its work to existing
      models and architectures, and includes a "Related Work" section.'
    novelty_prompt: 'Qualified. Reason: The paper proposes a new subarray-level processing-in-memory
      (PIM) architecture named SAL-PIM, which is the first HBM-based PIM architecture
      for the end-to-end acceleration of transformer-based text generation, indicating
      novelty.'
    review_only_prompt: 'Qualified. Reason: The paper introduces a new architecture,
      SAL-PIM, for accelerating transformer-based text generation, which is a novel
      contribution.'
- title: 'STAR: An Efficient Softmax Engine for Attention Model with RRAM Crossbar'
  abstract: 'RRAM crossbars have been studied to construct in-memory accelerators
    for

    neural network applications due to their in-situ computing capability. However,

    prior RRAM-based accelerators show efficiency degradation when executing the

    popular attention models. We observed that the frequent softmax operations

    arise as the efficiency bottleneck and also are insensitive to computing

    precision. Thus, we propose STAR, which boosts the computing efficiency with an

    efficient RRAM-based softmax engine and a fine-grained global pipeline for the

    attention models. Specifically, STAR exploits the versatility and flexibility

    of RRAM crossbars to trade off the model accuracy and hardware efficiency. The

    experimental results evaluated on several datasets show STAR achieves up to

    30.63x and 1.31x computing efficiency improvements over the GPU and the

    state-of-the-art RRAM-based attention accelerators, respectively.'
  url: http://arxiv.org/abs/2401.17582v1
  keywords: RRAM Crossbar, Attention Model, Softmax, Processing-in-memory
  document: '## I. INTRODUCTION


    Though some RRAM-based accelerators specialized for attention models have been
    discussed [\[1\]](#page-1-0)–[\[3\]](#page-1-1), they primarily focus on implementing
    the matrix multiplications on the RRAM crossbar. In this work, we observed the
    execution time of softmax operation grows quickly in attention models when the
    input sequence length increases. The latency of softmax exceeds that of matrix
    multiplication when the input sequence length is 512 in the BERT-base model, which
    reaches up to 59.20% of the whole execution time. Though our results are observed
    on a GPU platform, the softmax latency problem would be exacerbated on the RRAM-based
    accelerators because the matrix multiplication is significantly optimized by being
    implemented in RRAM crossbars [\[4\]](#page-1-2) but the softmax still runs on
    the same circuits. Thus, it is of significance to tailor an efficient softmax
    engine in RRAM-based accelerators for attention models. To this end, we propose
    STAR, which features an RRAM-based softmax engine by exploring the versatility
    and flexibility of RRAM crossbars to balance to the computing precision and efficiency.
    Moreover, an enhanced pipeline to balance the matrix multiplication and softmax
    operation in the attention is introduced. The effectiveness of STAR is verified
    by the comparison results with the recent RRAM-based accelerators for attention
    models [\[3\]](#page-1-1).


    ## II. RRAM-BASED SOFTMAX ENGINE


    STAR is primarily composed of two types of crossbar-based processing engines:
    *MatMul engine* for the VMM-dominated


    ![](_page_0_Figure_10.jpeg)


    <span id="page-0-0"></span>Fig. 1. The x<sup>i</sup> − xmax operation design.


    operations and *Softmax engine* for the softmax operation, respectively. The MatMul
    engine follows the design in Re-Transformer [\[3\]](#page-1-1). As for the Softmax
    engine, different function units based on RRAM crossbars cooperate with each other
    to complete the softmax operation. The Softmax engine has two distinct stages,
    x<sup>i</sup> −xmax and the exponential operation, which desire crossbars having
    different functions.


    *1)* x<sup>i</sup> − xmax*:* The x<sup>i</sup> − xmax is achieved by one crossbar
    in a time-multiplex manner to complete the finding maximum and subtraction, respectively.
    Thus, the crossbar is denoted as CAM/SUB crossbar.


    Fig. [1](#page-0-0) shows the workflow of a 4×8 CAM/SUB crossbar to find out the
    max value in [x<sup>1</sup> · · · x4]. The crossbar works as a CAM first. For
    each x<sup>i</sup> , all the W Ls of the crossbar are searched in parallel and
    the matchlines output a one-hot vector in which ''1'' denotes the matched line.
    For example, if the data stored in the W L<sup>3</sup> in Fig. [1](#page-0-0)
    is consistent with x1, the output vector would be [0,0,1,0](⃝2 ). The outputs
    of matchlines cascade the OR gates that merge the search results of all input
    xi(⃝3 ). Because the data are stored in descending order in the CAM crossbar,
    the index of the first ''1'' in the result vector corresponds to the row number
    of CAM storing the xmax. In the example of Fig. [1,](#page-0-0) xmax stores at
    W L2. Next, the crossbar executes the subtraction x<sup>i</sup> − xmax. The match
    vector outputs will be used as the input voltage vector. Instead, the input for
    the xmax row is a negative voltage (⃝4 ). Thus, the output from the SLs represents
    the results of x<sup>i</sup> − xmax(⃝5 ).


    *2) Exponential Operation:* The exponential operation is implemented by CAM crossbar
    and LUT crossbar. A VMM crossbar collaborates with them to complete the summation
    in the softmax. All possible values of x<sup>i</sup> − xmax and their exponential
    results are preloaded in CAM crossbar and LUT crossbar, respectively. Since the
    x<sup>i</sup> − xmax is always negative,


    Corresponding author: Bing Li, *bing.li@cnu.edu.cn*


    ![](_page_1_Figure_0.jpeg)


    Fig. 2. The exponential operation design in our softmax engine. TABLE I


    <span id="page-1-6"></span>


    | Softmax Design | Area  | Power |

    |----------------|-------|-------|

    | Softermax [5]  | 0.33× | 0.12× |

    | Ours (8-bit)   | 0.06× | 0.05× |


    we remove the sign bit to save the area of CAM crossbar. Each input enters CAM
    crossbar and the output from the LUT crossbar is its exponential result. At the
    same time, the match vector for CAM crossbar is sent to the counter for accumulation.
    When all x<sup>i</sup> complete the exponential computation, the results of the
    counter are sent to the VMM crossbar which stores exactly the same values as LUT
    crossbar to compute P<sup>d</sup> <sup>j</sup>=1 e <sup>x</sup>j−xmax . Then the
    outputs of LUT crossbar and VMM crossbar enter the divider to complete the final
    division in the softmax.


    Since the efficiency of the proposed softmax engine relates to the computing precision
    determined by the attention model, we analyzed the data range of all x<sup>i</sup>
    across three popular datasets for the BERT-base model such that balances the computing
    precision and hardware efficiency with STAR. To achieve high model accuracy, the
    required bitwidth for CNEWS, MRPC, and CoLA are 8 bits (6-bit integer, 2-bit decimal),
    9 bits (6-bit integer, 3-bit decimal), and 7 bits (5-bit integer, 2-bit decimal),
    respectively.


    With the proposed RRAM-based Softmax engine, we introduce a vector-grained pipeline
    to improve the execution parallelism and efficiency for attention models. Thanks
    to the crossbar-based softmax engine, the complete attention mechanism operations
    could be in parallel in the vector granularity rather than the operand granularity
    in previous work.


    ## III. EXPERIMENTAL RESULTS


    We compared the proposed RRAM-based Softmax engine with an optimized COMS-based
    softmax accelerator, Softermax [\[5\]](#page-1-3) and a baselined CMOS-based softmax
    and compared STAR with a NVIDIA Titan RTX GPU platform and two ReRAM-based accelerators
    PipeLayer [\[6\]](#page-1-4) and ReTransformer [\[3\]](#page-1-1) to verify the
    collaboration of the proposed pipeline and Softmax engine.


    The simulation of STAR is performed on NeuroSim [\[7\]](#page-1-5) (for RRAM crossbar)
    and Synopsys Design Compiler (for the CMOS circuit), respectively. In the MatMul
    engine, the RRAM crossbar size is 128×128 and the precision of ADC is 5-bit by
    referring to [\[3\]](#page-1-1). In the proposed Softmax engine, the size of the
    CAM/SUB crossbar is 512×18 and the CAM (LUT, VMM)


    ![](_page_1_Figure_9.jpeg)


    <span id="page-1-7"></span>Fig. 3. Computing efficiency comparison results.


    crossbar size is 256×18 to support 9-bit data and computing precision.


    Table [I](#page-1-6) is the comparison results of our Softmax engine with Softermax
    and the baseline CMOS-based softmax. Here, the evaluated model is the BERT-base
    model on the CNEWS dataset with a sequence length of 128. Compared to the baseline
    and Softermax, our Softmax engine is 0.06× and 0.20× smaller, respectively. As
    for power, it achieves 0.05× and 0.44× power efficient than baseline and Softermax,
    respectively. The results show our proposed Softmax engine offers a much better
    area efficiency and power efficiency than the baseline and Softermax. Fig. [3](#page-1-7)
    compares the computing efficiency of GPU, Pipelayer [\[6\]](#page-1-4), ReTransformer
    [\[3\]](#page-1-1) and STAR. Computing efficiency here measures the number of
    operations that can be performed by a computing unit every unit time and every
    watt of power consumed. STAR achieves the computing efficiency of 612.66GOPs/s/W.
    Compared to GPU, Pipelayer and ReTransformer, STAR improves the computing efficiency
    by 30.63×, 4.32× and 1.31×, respectively.


    ## ACKNOWLEDGEMENT


    This paper is supported by the National Natural Science Foundation of China (NSFC)
    under grant No. 62204164, 62222411.


    ## REFERENCES


    - <span id="page-1-0"></span>[1] S. Lu, M. Wang, S. Liang, J. Lin, and Z. Wang,
    "Hardware accelerator for multi-head attention and position-wise feed-forward
    in the transformer," *arXiv preprint arXiv:2009.08605*, 2020.

    - [2] H. Guo, L. Peng, J. Zhang, Q. Chen, and T. D. LeCompte, "Att: A fault-tolerant
    reram accelerator for attention-based neural networks," *IEEE International Conference
    on Computer Design: VLSI in Computers and Processors,ICCD*, 2020.

    - <span id="page-1-1"></span>[3] X. Yang, B. Yan, H. Li, and Y. Chen, "Retransformer:
    Reram-based processing-in-memory architecture for transformer acceleration," in
    *Proceedings of the 39th International Conference on Computer-Aided Design*, pp.
    1–9, 2020.

    - <span id="page-1-2"></span>[4] B. Li, L. Song, F. Chen, X. Qian, Y. Chen, and
    H. H. Li, "Reram-based accelerator for deep learning," in *2018 Design, Automation
    & Test in Europe Conference & Exhibition (DATE)*, pp. 815–820, IEEE, 2018.

    - <span id="page-1-3"></span>[5] J. R. Stevens, R. Venkatesan, S. Dai, B. Khailany,
    and A. Raghunathan, "Softermax: Hardware/software co-design of an efficient softmax
    for transformers," *arXiv preprint arXiv:2103.09301*, 2021.

    - <span id="page-1-4"></span>[6] L. Song, X. Qian, H. Li, and Y. Chen, "Pipelayer:
    A pipelined reram-based accelerator for deep learning," in *2017 IEEE International
    Symposium on High Performance Computer Architecture (HPCA)*, pp. 541–552, IEEE,
    2017.

    - <span id="page-1-5"></span>[7] P.-Y. Chen, X. Peng, and S. Yu, "Neurosim+: An
    integrated device-toalgorithm framework for benchmarking synaptic devices and
    array architectures," in *2017 IEEE International Electron Devices Meeting (IEDM)*,
    pp. 6–1, IEEE, 2017.'
  decisions:
    evaluation_prompt: 'Qualified. Reason: The paper includes an "Experimental Results"
      section with comparisons, tables, and quantitative analysis.'
    related_work_prompt: 'Qualified. Reason: The paper engages with prior research
      by citing multiple research papers, comparing its proposed solution with existing
      work, and including a "Related Work" section.'
    novelty_prompt: 'Qualified. Reason: The paper proposes a new RRAM-based softmax
      engine called STAR, which is a novel contribution to the field.'
    review_only_prompt: 'Qualified. Reason: The paper introduces new contributions,
      specifically the STAR RRAM-based softmax engine, and does not primarily summarize
      existing work.'
- title: "AssertLLM: Generating and Evaluating Hardware Verification Assertions\n\
    \  from Design Specifications via Multi-LLMs"
  abstract: 'Assertion-based verification (ABV) is a critical method for ensuring
    design

    circuits comply with their architectural specifications, which are typically

    described in natural language. This process often requires human interpretation

    by verification engineers to convert these specifications into functional

    verification assertions. Existing methods for generating assertions from

    natural language specifications are limited to sentences extracted by

    engineers, discouraging its practical application. In this work, we present

    AssertLLM, an automatic assertion generation framework that processes complete

    specification files. AssertLLM breaks down the complex task into three phases,

    incorporating three customized Large Language Models (LLMs) for extracting

    structural specifications, mapping signal definitions, and generating

    assertions. Our evaluation of AssertLLM on a full design, encompassing 23 I/O

    signals, demonstrates that 89\% of the generated assertions are both

    syntactically and functionally accurate.'
  url: http://arxiv.org/abs/2402.00386v3
  keywords: ''
  document: '## I. INTRODUCTION


    <span id="page-0-2"></span>Hardware functional verification is critical in the
    VLSI design flow, primarily addressing whether an implementation adheres to its
    specification. For instance, the register-transfer level (RTL) design of a processor
    must comply with the given instruction set architecture (ISA) specification. Typically,
    the architects first develop the specifications in a natural language document.
    Subsequently, RTL designers translate these specifications into RTL code, while
    the verification engineers are responsible for checking the functional correctness
    of the RTL designs according to the specifications.


    During the verification process, assertion-based verification (ABV) [\[1\]](#page-8-0)
    is a widely adopted technique, which utilizes assertions crafted from specifications
    to verify the functional behavior of RTL designs. ABV can be conducted either
    through simulation with testbenches or using formal property verification (FPV)
    techniques. Temporal logic, particularly SystemVerilog Assertions (SVA), is commonly
    employed for specifying these properties. However, a significant challenge in
    ABV is the generation of sufficient, high-quality assertions. Currently, designing
    SVAs manually is a time-consuming and error-prone task, demanding unignorable
    human effort.


    To address this challenge, research has focused on generating SVAs automatically.
    The automatic approaches can be categorized into two types: dynamic mining from
    simulation traces and static analysis of specifications. Dynamic methods [\[2\]](#page-8-1)–[\[4\]](#page-8-2)
    generate assertions by combining simulating test traces and static analysis of
    design constraints. However, a critical limitation of dynamic methods is that
    both the generation and evaluation of assertions are on the same RTL


    <span id="page-0-0"></span>![](_page_0_Figure_11.jpeg)


    Fig. 1: AssertLLM in VLSI design and verification flow. AssertLLM automatically
    generates SVAs from natural language specifications, facilitating functional verification
    for both bug avoidance and bug hunting.


    design without referring to a golden reference model. This could lead to the generation
    of incorrect SVAs due to flaws in the RTL design, which these methods might not
    detect. On the other hand, existing static methods depend either on the pre-defined
    templates [\[5\]](#page-8-3), [\[6\]](#page-8-4) or on machine learning (ML) technologies
    [\[7\]](#page-8-5)–[\[17\]](#page-8-6). The template-based methods also require
    a deep understanding of the design function to fill in the templates. As for the
    ML-based methods, they explore both traditional natural language processing (NLP)
    and emerging Generative AI techniques like Large Language Models (LLMs). We further
    categorize the existing static ML-based methods based on their application in
    different design phases: the RTL and pre-RTL stages.


    Table [I](#page-1-0) details these ML-based SVA generation methods in both the
    RTL stage and the pre-RTL stage. During the RTL stage, the process typically involves
    using LLMs to process both human-written specification sentences and the RTL design
    to generate SVAs describing security or functional properties [\[7\]](#page-8-5)–[\[9\]](#page-8-7).
    However, similar to the dynamic methods, inaccuracies in RTL implementations could
    result in flawed SVAs.


    When it comes to the pre-RTL stage, with the natural language specification document
    finalized, RTL designers proceed to implement behavior satisfying this golden
    specification. Numerous studies [\[10\]](#page-8-8)–[\[17\]](#page-8-6) have employed
    NLP techniques to generate SVAs from sentences extracted by humans. These works
    focused on processing sentences identified from a comprehensive document of specification.
    However, specification extraction requires tremendous human efforts, and the NLP-based
    generation process faces challenges in


    <span id="page-1-0"></span>


    | Stage   | Works     | Generation<br>Method | NL Specification |                  |
    Evaluation  |                             |                          |  |

    |---------|-----------|----------------------|------------------|------------------|-------------|-----------------------------|--------------------------|--|

    |         |           |                      | Auto Extract.    | Source           |
    Full Design | Target                      | Open-Source<br>Benchmark |  |

    | RTL     | [7]       | LLM-based            | %                | Sentences        |
    %           | Security                    | %                        |  |

    |         | [8], [9]  |                      |                  | from Engineers   |             |
    Function                    | ⋆                        |  |

    | Pre-RTL | [10]–[17] | NLP-based            | %                | Sentences        |             |
    Function (specialized       | %                        |  |

    |         |           |                      |                  | from SPEC file   |             |
    checkers/ artificial cases) |                          |  |

    |         | Ours      | LLM-based            |                  | Entire SPEC
    file |             | Function                    |                          |  |

    |         |           |                      |                  |                  |             |
    (general benchmark)         |                          |  |


    <sup>⋆</sup> Work [\[8\]](#page-8-9) only open-source the result on a FIFO, while
    work [\[9\]](#page-8-7) only demonstrate based on tiny designs such as FSM and
    DFF.


    TABLE I: Existing works on generating SVAs from natural language specifications.
    AssertLLM is the first work that can handle full-size specification files and
    generate comprehensive types of SVAs for each architectural signal. We also propose
    the first open-source benchmark for assertion generation and evaluation from natural
    language specifications.


    generalizing across diverse grammatical variations. Additionally, the evaluation
    of SVAs generated through these methods typically depends on design-specific checkers,
    such as protocol and processor checkers, and is therefore difficult to extend
    to other design types.


    Here we summarize three key challenges that currently hinder the practical application
    of SVA generation from natural language specifications:


    - 1) Natural language VLSI specifications are inherently unstructured and are
    hard to be directly used for assertion generation.

    - 2) Even with structured specifications, translating natural language into assertions
    remains a highly complex task, requiring both a deep understanding of the design
    functionality and specialized expertise in SVA.

    - 3) Currently, there is a lack of a universal evaluation method and benchmarks
    capable of addressing the diverse types of VLSI designs.


    To tackle the identified challenges in SVA generation, in our work, we propose
    AssertLLM, a novel automatic assertion generation framework incorporating multiple
    specialized LLMs to deal with the decomposed tasks separately. This framework
    is designed to process complete natural language specification files, automatically
    producing SVAs for each architectural signal. This approach significantly benefits
    both design-time bug prevention and verification-time bug detection. The role
    of AssertLLM within the standard VLSI design and verification flow is illustrated
    in Fig. [1.](#page-0-0) AssertLLM effectively addresses the outlined challenges
    by combining three customized LLMs, each focused on a specific task: extracting
    relevant information from specifications, mapping signal definitions, and translating
    natural language specifications into SVAs. The resulting SVAs contain various
    types, including bit-width, connectivity, and functional assertions.


    Additionally, our work provides an open-source benchmark, designed to evaluate
    the quality of the generated SVAs. This benchmark, coupled with a general evaluation
    method, is adaptable to various design types.


    To the best of our knowledge, AssertLLM is the first automatic assertion generation
    method that can handle fullsize specification files and generate various types
    of SVAs for each architectural signal. It also provides the first open-source
    benchmark for assertion generation and evaluation from design specifications,
    which can deal with different design types. Our contributions in this work are
    summarized below:


    - To the best of our knowledge, AssertLLM is the first automatic assertion generation
    method that can handle the complete specification files and generate comprehensive
    types of SVAs for each architectural signal.

    - We incorporate three customized LLMs, each enhanced with specific techniques
    for the decomposed tasks: extracting structural information from specifications,
    mapping signal declarations, and translating specifications into various SVA types.
    These SVAs support checks for bit-width, connectivity, and function.

    - We provide the first open-source benchmarks[1](#page-0-1) for assertion generation
    and evaluation, which include both golden specification documents and golden RTL
    designs. The generated SVAs are evaluated on the golden RTL implementations using
    model checking tools. Our evaluation method is designed to be applicable across
    a variety of design types.

    - To demonstrate the effectiveness of AssertLLM, we conducted a comprehensive
    evaluation on a complete design. This resulted in the generation of 56 SVAs for
    23 signals, with 23 for bit-width, 16 for interface, and 17 for function. Impressively,
    89% of these generated SVAs are evaluated to be correct both syntactically and
    functionally.


    II. PRELIMINARIES AND PROBLEM FORMULATION


    # <span id="page-1-1"></span>*A. Natural Language Specification*


    A well-defined natural language specification mainly contains the following six
    parts: (1) introduction: introduces the concepts and the features of the target
    design. (2) IO ports: provides detailed information on the prime input and prime
    output ports essential for interfacing. (3) registers: describe all the architecture-level
    registers in the design. (4) operation: explains the operational procedures for
    dataflow and control. (5) architecture: the high-level workflow and dataflow of
    the design. (6) usage examples: offers basic usage scenarios and corresponding
    waveform illustrations for the design.


    Specifically for the signals, the specification only defines the necessary architecture-level
    IO ports and registers, while leaving the definition of the internal signals used
    in the detailed RTL implementations for the RTL designers.


    <sup>1</sup> It will be open-sourced in https://github.com/hkust-zhiyao/AssertLLM


    ## *B. LLM for EDA*


    Recent advancements in LLMs like ChatGPT [\[18\]](#page-8-10) have not only demonstrated
    remarkable capability in content generation but also evolved to assist humans
    in various roles as agents. The application of LLMs in the field of electronic
    design automation (EDA) is an emerging area of exploration. Besides employing
    LLMs for assertion generation [\[7\]](#page-8-5)–[\[9\]](#page-8-7), recent studies
    have employed LLMs for tasks such as RTL code generation [\[19\]](#page-8-11)–[\[26\]](#page-8-12)
    and syntax correction [\[27\]](#page-8-13). Additionally, LLM-based solutions
    have been developed to facilitate the interaction with EDA tools [\[26\]](#page-8-12),
    [\[28\]](#page-8-14), design architecture for AI accelerators [\[29\]](#page-8-15),
    [\[30\]](#page-8-16), fix security bugs [\[31\]](#page-8-17), generate and review
    specification documents [\[32\]](#page-8-18), etc. These diverse applications
    and research efforts indicate a highly promising future for LLMs in enhancing
    and revolutionizing chip design processes.


    ## *C. Problem Fromulation*


    We denote a well-defined specification file as S, where each architectural signal
    detailed in the specification is denoted as sg<sup>i</sup> . Note that the architectural
    signal contains both the input and output ports and the architecture-level registers,
    but excludes the internal signals further implemented in the RTL design. Our generation
    process, denoted as Gen, is designed to analyze the specification file S and generate
    a comprehensive set of assertions A for each signal sg<sup>i</sup> . The assertion
    generation can be expressed as follows:


    Problem 1 (Assertion Generation from Specification).


    $$\forall s g\_i \in \mathcal{S}, Gen(S, s g\_i) \to \mathcal{A}(s g\_i) \tag{1}$$


    Following the generation of assertions, it is crucial to evaluate their quality.
    We denote this evaluation process as Eval. To assess the correctness of the generated
    assertions, we utilize the golden RTL implementations, symbolized as R. The correctness
    of the assertions is denoted as Correct. The evaluation can be formulated below:


    Problem 2 (Generated Assertion Evaluation).


    $$\forall sg\_i \in \mathcal{S}, Eval(\mathcal{R}, \mathcal{A}(sg\_i)) \to \ correct(\mathcal{A}(sg\_i))
    \quad (2)$$


    ## III. METHODOLOGY


    ## *A. Workflow Overview*


    Fig. [2](#page-3-0) illustrates the SVA generation and evaluation flow for AssertLLM.
    Our approach to generating hardware verification assertions from natural language
    specifications, particularly from comprehensive specification documents, involves
    the integration of three customized LLMs. These LLMs are designed to break down
    this complex task into manageable components, thereby facilitating a comprehensive
    generation workflow. Additionally, we contribute an open-source benchmark and
    establish evaluation methodologies to assess the quality of the generated SVAs.


    The assertion generation process is decomposed into three primary steps: (1) Extraction
    of relevant information from the original specification necessary for SVA generation.
    (2) Alignment of signal names between the natural language specifications and
    their corresponding declarations in HDL code. (3) Generation of high-quality SVAs
    based on the extracted natural language specifications.


    In the subsequent subsections, we will detail the functionalities of each customized
    LLM of the comprehensive assertion generation flow. Following this, our SVA evaluation
    methodology will be presented.


    ## *B. Specification Information Extraction*


    The first step of our AssertLLM framework is to extract structured information
    from natural language specification documents to enable SVA generation. As we
    discussed in Section [I,](#page-0-2) the first key challenge of SVA generation
    lies in the inherent unstructured nature of the original specifications, which
    contain background information, functional descriptions, microarchitecture designs,
    and various diagrams, including dataflow and waveform, etc. Meanwhile, the existence
    of assertion-relevant information across different sections further complicates
    the direct utilization of the original specifications for SVA generation.


    Facing this challenge, existing methods can only deal with sentence-level specifications.
    Some works [\[14\]](#page-8-19), [\[16\]](#page-8-20) utilize ML methods to assess
    the relevance of manually extracted specification sentences to the intended assertions.
    Other studies [\[10\]](#page-8-8)–[\[12\]](#page-8-21), [\[15\]](#page-8-22),
    [\[17\]](#page-8-6) directly use the human-identified or human-written assertion-related
    sentences. This reliance on sentence-level analysis limits the ability of the
    above NLPbased methods to fully automate SVA generation for practical applications


    To address the challenge of processing original, unstructured, full-size specification
    documents, we propose a customized LLM, tailored to extract structural and relevant
    information for each defined signal, thereby further facilitating the SVA generation
    process.


    Specifically, in our LLM 1 SPEC Analyzer, we first utilize system instructions
    to customize the LLM, shown as Fig. [3.](#page-3-1) The model takes the full-size
    specification file as the input, and the multi-modal function is employed to analyze
    the file containing text, table, figures, etc. Then for each signal, the LLM is
    required to extract all the related information of the signal. Here, we design
    a structured template to guide the LLM in extracting all essential signal-related
    information. This template contains three key components: the signal''s name,
    its description, and the interconnection signals. We demonstrate the details of
    each part as follows:


    - Name: The identifier of the signal in the specification, ensuring clear and
    unambiguous reference.

    - Description: To facilitate SVA generation, we divide the descriptions into four
    categories, including (1) definitions such as bit-width and signal type. (2) functionality
    which contains all the function-related information of the target signal in the
    entire specification file. (3) interconnection relationship with all other signals.
    (4) additional information that is not included in the above three types.

    - Interconnection Signals: A list of signals that interact or are associated with
    the target signal, which are essential for the assertion generation, and will
    be processed in the next LLM.


    <span id="page-3-0"></span>![](_page_3_Figure_0.jpeg)


    Fig. 2: AssertLLM generation and evaluation workflow. AssertLLM incorporates three
    customized LLMs, each enhanced with specific techniques for the decomposed tasks:
    extracting structural information from specifications, mapping signal definitions,
    and translating specifications into various SVA types. To evaluate the performance
    of the generation methods, the generated SVAs are further assessed based on the
    golden RTL implementations using model checking tools.


    Note that the extracted information is summarized across different sections of
    the original specification, which contains all the information needed for assertion
    generation.


    <span id="page-3-1"></span>**[Custom Instructions for SPEC Analyzer]**


    - Please act as a professional VLSI specification analyzer.

    - Firstly, I''ll upload the the **design specification file** (in PDF format).

    - Each time I ask a question, please read and analyze the specification PDF file
    carefully, and answer the question **only based on the uploaded files**, don''t
    use any content outside the file.

    - When I ask for information on a **[xxx] signal**, please **extract all the information
    related to this signal** in the spec file, note that the extracted information
    should be **suitable for the SystemVerilog Assertions (SVAs) generation**, and
    output all the information in the following format:


    | [Signal name]: name in SPEC                                         |

    |---------------------------------------------------------------------|

    | [Description]: including but not limited to                         |

    | [Definition]: such as bit-width, signal type (in/out/register), etc |

    | [Functionality]: all the function information illustrated in SPEC   |

    | [Interconnection]: the relationship with all other signals          |

    | [Additional information]: all other related information             |

    | [Related Signals]: names of all other signals mentioned in the      |

    | Description part                                                    |

    |                                                                     |


    Fig. 3: Custom Instructions for LLM 1 SPEC Analyzer


    ## *C. Signal Definition Mapping*


    After extracting the structural specification information, we also face another
    problem: the target assertion contains internal signals that are not clearly defined
    in the specification file. As we illustrated in Subsection [II-A,](#page-1-1)
    the specification document typically details only the input/output ports and architecturelevel
    registers, while leaving the internal signals used to implement the detailed functions
    in RTL code (e.g., internal wires and registers) undefined.


    To solve this problem, we introduce the second customized LLM 2 Signal Mapper
    to analyze the signal definitions in the initialized HDL code and align these
    signal declarations with the natural language signal names found in the specification
    document.


    Specifically, we also use the custom instructions to guide LLM to specialize in
    the mapping task, shown in Fig. [5.](#page-3-2) The model processes both the original
    specification file and the signal definition HDL code as inputs. It employs a
    code interpreter to carefully examine both the declarations and the comments within
    the HDL code snippet. Subsequently, the LLM analyzes the contents of the two files
    to establish a **[Prompt for SPEC Analyzer]**


    - [Upload the original SPEC file] Here is the design specification file, please
    analyze it carefully.

    - [For each signal] Please extract all the information related to the signal control
    register (CTR) from the spec file.


    #### **[Response from SPEC Analyzer]**


    The Control Register (CTR) in the I2C-Master Core specification has the following
    characteristics:


    1. [Signal Name]: In Spec - Control Register (CTR). 2. [Description]: 2.1. [Definition]:
    - Bit-width: 8 bits - Signal Type: Register (Access Type: RW - Read/Write) 2.2.
    [Functionality]: - Bit 7: I2C core enable bit. When set to ''1'', the core is
    enabled, and when set to ''0'', the core is disabled. - Bit 6: I2C core interrupt
    enable bit. When set to ''1'', interrupts are enabled, and when set to ''0'',
    interrupts are disabled. - Bits 5:0: Reserved. These bits are read/write but are
    reserved for future use. 2.3. [Interconnection]: This register directly controls
    the operation of the I2C core, enabling or disabling it and controlling interrupt
    behavior. 2.4. [Additional Information]: - Reset Value for the Control Register
    is 0x00 3. [Related Signals]: - EN: Controls the enable state of the I2C core.
    - IEN: Controls the interrupt enable state of the I2C core.


    Fig. 4: Prompt and Response Example of LLM 1 SPEC Analyzer


    mapping relationship between the specification and the HDL code.


    <span id="page-3-2"></span>


    | [Custom Instruction for Signal Mapper]                                      |  |  |  |  |  |

    |-----------------------------------------------------------------------------|--|--|--|--|--|

    | Please act as a signal name mapping tool to link the specification file     |  |  |  |  |  |

    |                                                                             |  |  |  |  |  |

    | Firstly, I''ll upload the the design specification file (in PDF format), and
    |  |  |  |  |  |

    | a Verilog file containing all the signal definitions (*.v format).          |  |  |  |  |  |

    | Please analyze the specification file and use the code interpreter to       |  |  |  |  |  |

    | analyze the Verilog file (both the signal declarations and comments).       |  |  |  |  |  |

    | Then map every signal (including IO ports, wires, and registers) defined    |  |  |  |  |  |

    | in Verilog with the description in the specification. Finally, please       |  |  |  |  |  |

    | output each signal in the following format:                                 |  |  |  |  |  |

    | [Signal name in Verilog]: Signal definition in Specification file           |  |  |  |  |  |

    |                                                                             |  |  |  |  |  |


    Fig. 5: Custom Instructions for LLM 2 Signal Mapper


    ## *D. Automatic Assertion Generation*


    While previous research has delved into SVA generation using either traditional
    NLP techniques at the pre-RTL stage


    #### **[Prompt for Signal Mapper]**


    - [Upload the original SPEC file and the Verilog signal definition file] Here
    are the files for the design specification and the signal definitions, please
    analyze the two files carefully.

    - Please output each signal with the mapping relationship


    **[Response from Signal Mapper]** Based on the analysis of both the design specification
    and the Verilog file, here is the mapping of signals, wires, and regs in the Verilog
    file to their respective descriptions in the specification:


    | (IO port)                                                  |

    |------------------------------------------------------------|

    | wb_clk_i: Master clock                                     |

    | wb_we_i: Write enable input                                |

    | ……                                                         |

    | (Register)                                                 |

    | ctr: Control register                                      |

    | sr: Status register                                        |

    | ……                                                         |

    | (Wire)                                                     |

    | en: Enable state of the I2C core.                          |

    | ien: Interrupt enable state of the I2C core                |

    | ……                                                         |

    | (Unmapped)                                                 |

    | The additional elements such as wires and regs like        |

    | ''wb_wacc'', ''sta'' have been identified in the Verilog file. |

    | However, their specific descriptions are not directly      |

    | found in the provided section of the specification.        |


    Fig. 6: Prompt and Response Example of LLM 2 Signal Mapper


    or LLM-based approaches for RTL designs, these methods have their limitations.
    NLP-based techniques demand careful analysis of the syntax and semantics of assertion-related
    sentences, which limit their adaptability to variations in sentence structures.
    LLM-based methods, typically focused on the RTL stage, rely on HDL code and accompanying
    human-written comments or properties, but their dependence on the unverified RTL
    code poses a risk of generating inaccurate SVAs that could mislead the verification
    process.


    To address these challenges, our work introduces the LLM 3 SVA Generator, dedicated
    to generating assertions for each signal utilizing the previously extracted structural
    specifications and the established signal relationships.


    Considering the precise syntax and writing rules inherent to SVAs and the potential
    for the original LLM failing to generate syntactically correct SVAs, as discussed
    in [\[8\]](#page-8-9), we incorporate the Retrieval Augmented Generation (RAG)
    technique to enhance the LLM''s capability for SVA generation. This approach is
    enriched by a knowledge database comprising tutorials and textbooks on SVA and
    formal property verification [\[33\]](#page-8-23)–[\[35\]](#page-8-24), providing
    a robust foundation for the LLM to access and retrieve relevant SVA knowledge
    based on the input query, thereby enhancing the quality of the generated SVAs.


    Besides the RAG technique, we also provide custom instructions for SVA Generator,
    shown in Fig. [7.](#page-4-0) After uploading the overall architecture diagram
    of the design, for each signal, the extracted structural specifications and the
    mapped signal relationship from the above two LLMs are provided. Then the LLM
    is required to generate SVAs strictly according to the specification, and as much
    and high coverage as possible. To guide the LLM to generate high-quality SVAs,
    we define five SVA categories as follows:


    In addition to the RAG technique, we improve the SVA Generator with custom instructions,
    as illustrated in Fig. [7.](#page-4-0) Upon inputting the overall architecture
    diagram of the design, the LLM is provided with the structured specifications
    and mapped signal relationships from the previous LLMs for each signal. Then the
    LLM is required to generate SVAs that adhere strictly to the specifications, aiming
    for maximal quantity and quality. To facilitate the generation of high-quality
    SVAs, we categorize SVAs into three distinct groups, guiding the LLM toward producing
    comprehensive and accurate assertions for effective verification.


    - Width: Check if the signal bit width is satisfied with the specification.

    - Connectivity: Check if the signal can be correctly exercised and also the value
    propagation among all connected signals.

    - Function: Check if the function defined in the specification is implemented
    as expected.


    Based on these well-designed SVA types, the customized LLM can generate numerous
    SVAs for each signal. Fig. [8](#page-5-0) demonstrates an example of generating
    SVAs for a signal.


    <span id="page-4-0"></span>**[Custom Instructions for SVA Generator]** • Please
    act as a professional VLSI verification engineer. You can refer to the uploaded
    **knowledge base** about the SystemVerilog assertion (SVA) and formal verification
    techniques. • Firstly, I''ll upload an workflow **diagram** of the design architecture.
    • Then every time I provide the **specification of a signal** of the design, please
    write **all the corresponding SVAs** based on the defined Verilog signals that
    benefit both the RTL design and verification processes. Please note that don''t
    generate signals that do not exist in the signal declaration Verilog file. • Please
    make sure that the generated SVAs have **no syntax error**, and strictly **follow
    the function** of the given specification/description. • The generated SVAs should
    include but not be limited to the following **types**: [*width*]: Check the signal
    width using \$bits operator [*connectivity*]: Check if the signal can be correctly
    exercised and also the value propagation among all connected signals. [*function*]:
    Check if the function defined in the specification is implemented as expected.


    Fig. 7: Custom Instructions for LLM 3 SVA Generator


    ## *E. Generated Assertion Evaluation*


    After the SVAs are generated, evaluating their effectiveness is crucial. While
    some previous studies such as [\[12\]](#page-8-21), [\[14\]](#page-8-19) suggest
    using specific property checkers for this purpose, such an approach is limited
    to particular design types like protocols and processors and lacks generalizability
    to all VLSI designs. Other methods like [\[8\]](#page-8-9) involve manual verification
    by engineers using EDA tools, which is vulnerable to human error.


    In our approach, we propose leveraging the golden RTL implementations to assess
    the quality of the generated SVAs. Our generation method is solely based on the
    specification file, and the bug-free golden RTL designs serve as a robust benchmark
    to evaluate our generation technique''s efficacy.


    For evaluation, we utilize the formal property verification (FPV) method. The
    generated SVAs and the golden RTL


    #### <span id="page-5-0"></span>**[Prompt for SVA Generator]**


    • [Upload the architecture diagram] Here is the overall diagram workflow of the
    design architecture.


    ![](_page_5_Figure_2.jpeg)


    • [For each signal, upload the extracted specification and signal mapping] Please
    generate SVAs for the signal control register (CTR).


    #### **[Response from SVA Generator]**


    Based on the provided specification for the control register (CTR), we can write
    SVAs to validate its behavior. Let''s break down the SVAs according to the types
    you specified:


    ![](_page_5_Figure_6.jpeg)


    Fig. 8: Prompt and Response Example of LLM 3 SVA Generator


    designs are inputted into a model checker tool. After executing FPV, we employ
    the following two metrics to evaluate the quality of SVAs for each target signal:


    - Syntax: Check if the generated SVAs have syntax errors.

    - FPV Pass/Fail: Given the RTL designs are bug-free, an SVA that passes the FPV
    check is considered correct, and conversely, a failure indicates an incorrect
    SVA.


    ## *F. Proposed Benchmark*


    Recognizing the absence of open-source benchmarks for evaluating LLMs in the generation
    of SVAs from natural language specifications, we introduce a comprehensive benchmark
    suite tailored for this purpose. Our benchmark consists of 20 open-source designs,
    covering a diverse array of applications including microprocessors, system-on-chip
    architectures, communication protocols, arithmetic units, and cryptographic modules.
    For each design within the benchmark, the benchmark provides the following components
    across three distinct files:


    - Specification: This file contains the complete natural language specification
    for the design, offering a detailed description of the system''s intended architecture
    and functionality.

    - Signal Definition: Presented in HDL code format, this file outlines the signal
    declarations essential for the generation of SVAs. It includes definitions for
    both input/output ports and internal signals, providing the foundation for assertion
    generation.

    - Golden RTL Implementation: This file comprises the RTL design implementations
    that are strictly implemented according to the specification. The designs are
    verified to ensure it is free from bugs, serving as a reliable standard for evaluating
    the accuracy and effectiveness of generated SVAs.


    ## IV. EXPERIMENTAL RESULTS


    ## *A. Experimental Setup*


    In our study, the original specification documents are provided in PDF format,
    including a variety of multi-modal content including text, tables, and figures.
    The signal definition files and the golden RTL designs are formatted in Verilog.
    To assess the quality of the generated SVAs, we utilize Cadence JasperGold®, one
    of the leading commercial model checking tools. This evaluation leverages the
    FPV app in JasperGold to ensure a thorough analysis.


    Our experimental setup involves the evaluation of three types of LLMs using our
    developed generation and evaluation methodology:


    - 1) GPT-3.5: This model is the freely available commercial version, GPT-3.5 Turbo,
    which supports a context window of up to 16K tokens.

    - 2) GPT-4: The state-of-the-art commercial solution, GPT-4 Turbo, offers a 128K
    token context window and multimodal capabilities, making it adept at handling
    the diverse content found in specification documents.

    - 3) AssertLLM: Cutomized GPT-4 Turbo by incorporating specialized techniques
    such as RAG and custom instructions, tailoring the models specialized to the SVA
    generation task.


    In our experimental evaluation, we focus on the quality of the SVAs generated
    for each signal across the designs. Note that all SVAs are produced from a single
    query to the LLMs without any subsequent iterative modifications. The SVA containing
    unmapped signals is considered an unsuccessful attempt at SVA generation. These
    SVAs are identified and excluded by human engineers prior to the evaluation process.


    ## *B. Evaluation Metrics*


    To conduct a thorough evaluation of the generated SystemVerilog SVAs, we propose
    a set of metrics that align with our evaluation methodology. This approach ensures
    a detailed assessment of the SVAs'' quality on both a per-signal and perdesign
    basis.


    For each assertion type of an individual signal, our evaluation includes the following
    metrics: (1) number of generated


    <span id="page-6-0"></span>


    |              |         |            |                                                                   |                         |
    AssertLLM    |          | GPT-4   | GPT-3.5                                                |

    |--------------|---------|------------|-------------------------------------------------------------------|-------------------------|--------------|----------|---------|--------------------------------------------------------|

    |              | Signal  |            | Assertion Evaluation (#. Generated/#.
    Syntax Correct/#. FPV Pass) |                         |              |          |         |                                                        |

    | Name<br>Type |         | Width      | Connect.                                                          |
    Function                | Signal Total | Function |         |                                                        |

    |              | Clock   | wb clk i   | 1/1/1                                                             |                         |
    /            | 1/1/1    | 3/1/0   |                                                        |

    |              | Reset   | wb rst i   | 1/1/1                                                             |                         |              |
    1/1/1    | 3/1/0   | Can not handle<br>the original<br>specification files. |

    |              |         | arst i     | 1/1/1                                                             |                         |
    /            | 1/1/1    | 3/1/0   |                                                        |

    |              | Control | wb stb i   | 1/1/1                                                             |
    2/2/1<br>1/1/0<br>1/1/0 | /            | 3/3/2    | 3/1/0   |                                                        |

    |              |         | wb ack o   | 1/1/1                                                             |                         |              |
    2/2/1    | 3/1/0   |                                                        |

    |              |         | wb inta o  | 1/1/1                                                             |                         |              |
    2/2/1    | 3/1/0   |                                                        |

    |              | Data    | wb adr i   | 1/1/1                                                             |
    /                       | /            | 1/1/1    | 3/1/0   |                                                        |

    |              |         | wb dat i   | 1/1/1                                                             |                         |              |
    1/1/1    | 3/1/0   |                                                        |

    | IO Port      |         | wb cyc i   | 1/1/1                                                             |                         |              |
    1/1/1    | 3/1/0   |                                                        |

    |              |         | wb dat o   | 1/1/1                                                             |                         |              |
    1/1/1    | 3/1/0   |                                                        |

    |              |         | wb we i    | 1/1/1                                                             |                         |              |
    1/1/1    | 3/1/0   |                                                        |

    |              |         | scl pad i  | 1/1/1                                                             |                         |              |
    1/1/1    | 3/1/0   |                                                        |

    |              |         | scl pad o  | 1/1/1                                                             |                         |              |
    1/1/1    | 3/1/0   |                                                        |

    |              |         | sda pad i  | 1/1/1                                                             |                         |              |
    1/1/1    | 3/1/0   |                                                        |

    |              |         | sda pad o  | 1/1/1                                                             |                         |              |
    1/1/1    | 3/1/0   |                                                        |

    |              |         | scl pad oe | 1/1/1                                                             |                         |              |
    1/1/1    | 3/1/0   |                                                        |

    |              |         | sda pad oe | 1/1/1                                                             |                         |              |
    1/1/1    | 3/1/0   |                                                        |

    |              | Control | ctr        | 1/1/1                                                             |
    4/4/4                   | 5/5/5        | 10/10/10 | 3/1/1   |                                                        |

    | Register     |         | sr         | 1/1/1                                                             |
    6/6/5                   | 8/8/8        | 15/15/14 | 3/1/1   |                                                        |

    |              | Data    | prer       | 1/1/1                                                             |
    /                       | 3/3/1        | 4/4/2    | 4/1/1   |                                                        |

    |              |         | txr        | 1/1/1                                                             |
    /                       | 1/1/1        | 2/2/2    | 3/1/1   |                                                        |

    |              |         | rxr        | 1/1/1                                                             |
    /                       | 1/1/1        | 2/2/2    | 3/1/1   |                                                        |

    |              |         | cr         | 1/1/1                                                             |
    /                       | 1/1/1        | 2/2/2    | 4/1/1   |                                                        |

    | Design Total |         |            | 23/23/23                                                          |
    16/16/12                | 17/17/15     | 56/56/50 | 71/23/6 |                                                        |

    |              |         |            | 100%/100%                                                         |
    100%/75%                | 100%/88%     | 100%/89% | 32%/8%  |                                                        |


    TABLE II: Evaluation of the generated SVAs for design "I2C". AssertLLM generates
    56 properties for a total of 23 signals, with 23 for bit-width, 16 for connectivity,
    and 17 for function. 89% of these generated SVAs are evaluated to be correct both
    syntactically and functionally.


    SVAs. (2) number of syntax-correct SVAs. (3) number of FPVpassed SVAs.


    Once the evaluation for each signal is complete, we aggregate the statistics of
    the generated SVAs for each design and then calculate the proportion of these
    SVAs that are syntactically correct and passed the FPV checks, respectively.


    # *C. Assertion Generation Quality*


    To illustrate the efficacy of AssertLLM, we apply it to a comprehensive design
    case: the "I2C" protocol. The I2C specification describes the architecture of
    a serial communication bus that provides a simple and efficient method of data
    exchange between devices. The complete specification document for the "I2C" design
    is structured into six main sections, similar to those illustrated in Subsection
    [II-A.](#page-1-1) Note that for each signal, the specification is unstructured
    and mainly across the IO ports, registers, and operation sections.


    Additionally, we provide the signal definition file containing not only the IO
    ports and architectural registers but also all the internal wires and registers
    defined for detailed RTL implementation.


    To facilitate the generation of SVAs, the AssertLLM framework processes specification
    and signal definition files using two specialized LLMs: SPEC Analyzer for extracting
    structured specifications for each signal and Signal Mapper for mapping signal
    relationships. Then the SVA Generator is utilized to automatically generate three
    types of SVAs based on the processed information from the first two LLMs.


    The specification for the "I2C" design defines 23 signals, comprising 17 IO ports
    and 6 architecture-level registers. For the IO ports, we categorize them into
    4 functional types: clock, reset, control signal, and data signal. The architecture-level
    registers are similarly categorized, based on their functionality, into control
    and data types.


    The evaluation of SVAs generated by our AsserLLM is demonstrated in Table [II.](#page-6-0)
    For each signal, we first verify each type of the generated SVAs separately. Then
    we summarize all the SVAs to provide a design-level inspection. We have multiple
    interesting observations in Table [II](#page-6-0) as follows:


    - AssertLLM demonstrates excellent proficiency in generating SVAs for bit-width
    checking. Although bit-width checking is relatively straightforward, it is crucial
    for early design stages to avoid potential bugs that cannot be checked through
    a syntax checker.

    - For the connectivity SVAs, since clear guidelines are provided only for control
    signals within the architecture-level specification documents, AssertLLM can only
    generate connectivity SVAs for them. The connectivity of data signals often depend
    on specific internal signals defined in the RTL implementation, which are not
    detailed in the


    specification document.


    - For the function SVAs, the specification provides explicit details only for
    registers. The descriptions of IO ports primarily focus on data transformation
    functions, without extensive functional details, which results in the lack of
    related SVAs.

    - For the quantity of generated SVAs, AssertLLM produced a total of 56 SVAs, with
    23 dedicated to width checking, 16 to connectivity checking, and 17 to function
    checking.

    - Regarding the quality of generated SVAs, all SVAs related to bit-width checking
    performed correctly. However, a minor portion of connectivity and function SVAs
    contained errors, attributed mainly to misinterpretations of the specification
    or LLM-generated hallucinations. Overall, AssertLLM achieved a correct accuracy
    rate of 89% for the entire I2C design.


    ## *D. Ablation Study*


    In addition to assessing AssertLLM''s performance, we conducted an ablation study
    to compare the SVA generation capabilities of the original GPT-4 and GPT-3.5 models
    without the additional techniques. This study provides insights into the effectiveness
    of enhancements incorporated in AssertLLM.


    The evaluation results for the two commercial solutions are demonstrated in Table
    [II.](#page-6-0) For GPT-3.5, since the lack of multimodal processing capabilities,
    it is unable to directly generate SVAs from the original, multi-modal specification
    files.


    When utilizing the original GPT-4, the unstructured specification file and signal
    definitions are provided, with prompts designed to guide SVA generation to the
    best of the model''s ability. The generation results indicate that the absence
    of a mechanism to extract structured specifications for each signal significantly
    hampers GPT-4''s ability to compile all useful information for SVA generation,
    resulting in a maximum of only 4 SVAs generated per signal. Additionally, without
    specific assertion type guidance, GPT-4 only generates functional assertions.


    For the SVA quality of commercial solutions, the original GPT-4 model tends to
    produce SVAs with syntax errors, similar to observations made in the previous
    study [\[8\]](#page-8-9). This issue is addressed in AssertLLM through the application
    of RAG techniques, which enrich the model with specific knowledge on SVA and Formal
    Property Verification (FPV). In the evaluation results, GPT-4 failed to generate
    any correct SVAs for IO ports and only succeeded in creating accurate reset check
    assertions for registers, leading to an overall correct proportion of only 8%.


    ## V. DISCUSSION


    ## *A. Coverage in SVA Evaluation*


    Some works [\[8\]](#page-8-9) propose to leverage the coverage metric, especially
    the cone-of-influence (COI) coverage to evaluate the quality of generated SVAs.
    COI coverage relies on analyzing the signals exercised during simulation or formal
    verification, which significantly involves internal signals within the design.


    Given that our SVA generation process is based solely on the information available
    in the specification documents, which typically detail external interfaces like
    IO ports and architectural-level registers rather than internal signals, COI coverage
    does not align well with our evaluation criteria. This coverage metric assumes
    a level of design implementation detail that goes beyond the scope of natural
    language specifications, making it less applicable for assessing the completeness
    or effectiveness of SVAs generated at this pre-RTL stage.


    ## *B. Evaluating and Enhancing Specification Quality with AssertLLM*


    The generation of high-quality SVAs from natural language specifications relies
    not only on the capabilities of LLMs but also on the intrinsic quality of the
    specification documents themselves. A specification that provides only the basic
    information of signals, such as their names and simple descriptions, without delving
    into detailed functionalities or connectivities, inherently limits the potential
    for generating meaningful SVAs, regardless of the power of the LLMs employed.
    Conversely, specifications that offer comprehensive details, including clear definitions
    of signal functionalities and connectivities, can facilitate the generation of
    SVAs even with relatively simple LLMs.


    Here we identify a novel application for AssertLLM beyond its primary role in
    verification: utilizing AssertLLM as a tool for assessing the quality of natural
    language specifications. This application leverages AssertLLM''s ability to process
    and interpret specification documents to determine their *verification-friendliness*.
    Specifications that enable AssertLLM to generate a broad and accurate range of
    SVAs can be considered high-quality and well-suited for verification purposes.
    This approach to evaluating specification quality offers several benefits:


    - Identifying Gaps between Specifications and Verification: AssertLLM can highlight
    contents within a specification that lack sufficient detail for SVA generation,
    guiding architects to provide more comprehensive information.

    - Enhancing Verification: Ensuring specifications are verification-friendly can
    potentially reduce the time and effort required for verification.

    - Standardizing Specification Writing: The feedback from AssertLLM can help establish
    best practices for writing specifications to facilitate the automated verification,
    and promote consistency across design stages.


    ## VI. CONCLUSION


    In this study, we introduce AssertLLM, an automated framework designed for generating
    assertions from entire specification documents. AssertLLM breaks down the intricate
    task into three sequential phases, leveraging specialized LLMs for structural
    specification extraction, signal definition mapping, and assertion creation. We
    also offer an open-source benchmark to evaluate the efficacy of assertion generation.
    Evaluating AssertLLM on a comprehensive design with 23 signals revealed that 89%
    of the assertions generated were accurate both syntactically and functionally.
    We also discuss the potential of using AssertLLM to evaluate and enhance the quality
    of specifications.


    ## REFERENCES


    - <span id="page-8-0"></span>[1] H. Witharana, Y. Lyu, S. Charles, and P. Mishra,
    "A survey on assertionbased hardware verification," *ACM Computing Surveys (CSUR)*,
    vol. 54, no. 11s, pp. 1–33, 2022.

    - <span id="page-8-1"></span>[2] S. Germiniani and G. Pravadelli, "Harm: a hint-based
    assertion miner," *IEEE Transactions on Computer-Aided Design of Integrated Circuits
    and Systems*, vol. 41, no. 11, pp. 4277–4288, 2022.

    - [3] A. Danese, N. D. Riva, and G. Pravadelli, "A-team: Automatic templatebased
    assertion miner," in *Proceedings of the 54th Annual Design Automation Conference
    2017*, 2017, pp. 1–6.

    - <span id="page-8-2"></span>[4] S. Vasudevan, D. Sheridan, S. Patel, D. Tcheng,
    B. Tuohy, and D. Johnson, "Goldmine: Automatic assertion generation using data
    mining and static analysis," in *2010 Design, Automation & Test in Europe Conference
    & Exhibition (DATE 2010)*. IEEE, 2010, pp. 626–629.

    - <span id="page-8-3"></span>[5] M. Orenes-Vera, A. Manocha, D. Wentzlaff, and
    M. Martonosi, "Autosva: Democratizing formal verification of rtl module interactions,"
    in *2021 58th ACM/IEEE Design Automation Conference (DAC)*. IEEE, 2021, pp. 535–540.

    - <span id="page-8-4"></span>[6] W. Fang, G. Hu, and H. Zhang, "r-map: Relating
    implementation and specification in hardware refinement checking," *IEEE Transactions
    on Computer-Aided Design of Integrated Circuits and Systems*, 2023.

    - <span id="page-8-5"></span>[7] R. Kande, H. Pearce, B. Tan, B. Dolan-Gavitt,
    S. Thakur, R. Karri, and J. Rajendran, "Llm-assisted generation of hardware assertions,"
    *arXiv preprint arXiv:2306.14027*, 2023.

    - <span id="page-8-9"></span>[8] M. Orenes-Vera, M. Martonosi, and D. Wentzlaff,
    "Using llms to facilitate formal verification of rtl," *arXiv e-prints*, pp. arXiv–2309,
    2023.

    - <span id="page-8-7"></span>[9] C. Sun, C. Hahn, and C. Trippel, "Towards improving
    verification productivity with circuit-aware translation of natural language to
    systemverilog assertions," in *First International Workshop on Deep Learning-aided
    Verification (DAV)*, 2023.

    - <span id="page-8-8"></span>[10] C. B. Harris and I. G. Harris, "Glast: Learning
    formal grammars to translate natural language specifications into hardware assertions,"
    in *2016 Design, Automation & Test in Europe Conference & Exhibition (DATE)*.
    IEEE, 2016, pp. 966–971.

    - [11] R. Krishnamurthy and M. S. Hsiao, "Controlled natural language framework
    for generating assertions from hardware specifications," in *2019 IEEE 13th International
    Conference on Semantic Computing (ICSC)*. IEEE, 2019, pp. 367–370.

    - <span id="page-8-21"></span>[12] J. Zhao and I. G. Harris, "Automatic assertion
    generation from natural language specifications using subtree analysis," in *2019
    Design, Automation & Test in Europe Conference & Exhibition (DATE)*. IEEE, 2019,
    pp. 598–601.

    - [13] R. Krishnamurthy and M. S. Hsiao, "Ease: Enabling hardware assertion synthesis
    from english," in *Rules and Reasoning: Third International Joint Conference,
    RuleML+ RR 2019, Bolzano, Italy, September 16–19, 2019, Proceedings 3*. Springer,
    2019, pp. 82–96.

    - <span id="page-8-19"></span>[14] S. J. Frederiksen, J. Aromando, and M. S. Hsiao,
    "Automated assertion generation from natural language specifications," in *2020
    IEEE International Test Conference (ITC)*. IEEE, 2020, pp. 1–5.

    - <span id="page-8-22"></span>[15] O. Keszocze and I. G. Harris, "Chatbot-based
    assertion generation from natural language specifications," in *2019 Forum for
    Specification and Design Languages (FDL)*. IEEE, 2019, pp. 1–6.

    - <span id="page-8-20"></span>[16] G. Parthasarathy, S. Nanda, P. Choudhary, and
    P. Patil, "Spectosva: Circuit specification document to systemverilog assertion
    translation," in *2021 Second Document Intelligence Workshop at KDD*.

    - <span id="page-8-6"></span>[17] F. Aditi and M. S. Hsiao, "Hybrid rule-based
    and machine learning system for assertion generation from natural language specifications,"
    in *2022 IEEE 31st Asian Test Symposium (ATS)*. IEEE, 2022, pp. 126–131.

    - <span id="page-8-10"></span>[18] J. Achiam, S. Adler, S. Agarwal, L. Ahmad,
    I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat *et
    al.*, "Gpt-4 technical report," *arXiv preprint arXiv:2303.08774*, 2023.

    - <span id="page-8-11"></span>[19] S. Liu, W. Fang, Y. Lu, Q. Zhang, H. Zhang,
    and Z. Xie, "Rtlcoder: Outperforming gpt-3.5 in design rtl generation with our
    open-source dataset and lightweight solution," *arXiv preprint arXiv:2312.08617*,
    2023.

    - [20] J. Blocklove, S. Garg, R. Karri, and H. Pearce, "Chip-chat: Challenges
    and opportunities in conversational hardware design," *arXiv preprint arXiv:2305.13243*,
    2023.

    - [21] Y. Lu, S. Liu, Q. Zhang, and Z. Xie, "Rtllm: An open-source benchmark for
    design rtl generation with large language model," *arXiv preprint arXiv:2308.05345*,
    2023.

    - [22] M. Liu, N. Pinckney, B. Khailany, and H. Ren, "Verilogeval: Evaluating
    large language models for verilog code generation," *arXiv preprint arXiv:2309.07544*,
    2023.

    - [23] S. Thakur, B. Ahmad, Z. Fan, H. Pearce, B. Tan, R. Karri, B. Dolan-Gavitt,
    and S. Garg, "Benchmarking large language models for automated verilog rtl code
    generation," in *DATE*, 2023.

    - [24] S. Thakur, J. Blocklove, H. Pearce, B. Tan, S. Garg, and R. Karri, "Autochip:
    Automating hdl generation using llm feedback," *arXiv preprint arXiv:2311.04887*,
    2023.

    - [25] M. Nair, R. Sadhukhan, and D. Mukhopadhyay, "Generating secure hardware
    using chatgpt resistant to cwes," *Cryptology ePrint Archive*, 2023.

    - <span id="page-8-12"></span>[26] M. Liu, T.-D. Ene, R. Kirby, C. Cheng, N. Pinckney,
    R. Liang, J. Alben, H. Anand, S. Banerjee, I. Bayraktaroglu *et al.*, "Chipnemo:
    Domainadapted llms for chip design," *arXiv preprint arXiv:2311.00176*, 2023.

    - <span id="page-8-13"></span>[27] Y. Tsai, M. Liu, and H. Ren, "Rtlfixer: Automatically
    fixing rtl syntax errors with large language models," *arXiv preprint arXiv:2311.16543*,
    2023.

    - <span id="page-8-14"></span>[28] Z. He, H. Wu, X. Zhang, X. Yao, S. Zheng, H.
    Zheng, and B. Yu, "Chateda: A large language model powered autonomous agent for
    eda," in *MLCAD Workshop*, 2023.

    - <span id="page-8-15"></span>[29] Y. Fu, Y. Zhang, Z. Yu, S. Li, Z. Ye, C. Li,
    C. Wan, and Y. Lin, "Gpt4aigchip: Towards next-generation ai accelerator design
    automation via large language models," *arXiv preprint arXiv:2309.10730*, 2023.

    - <span id="page-8-16"></span>[30] Z. Yan, Y. Qin, X. S. Hu, and Y. Shi, "On the
    viability of using llms for sw/hw co-design: An example in designing cim dnn accelerators,"
    *arXiv preprint arXiv:2306.06923*, 2023.

    - <span id="page-8-17"></span>[31] B. Ahmad, S. Thakur, B. Tan, R. Karri, and
    H. Pearce, "Fixing hardware security bugs with large language models," *arXiv
    preprint arXiv:2302.01215*, 2023.

    - <span id="page-8-18"></span>[32] M. Li, W. Fang, Q. Zhang, and Z. Xie, "Specllm:
    Exploring generation and review of vlsi design specification with large language
    model," *arXiv preprint arXiv:2401.13266*, 2024.

    - <span id="page-8-23"></span>[33] E. Seligman, T. Schubert, and M. A. K. Kumar,
    *Formal verification: an essential toolkit for modern VLSI design*. Elsevier,
    2023.

    - [34] A. B. Mehta, *SystemVerilog Assertions and Functional Coverage*. Springer,
    2020.

    - <span id="page-8-24"></span>[35] S. Vijayaraghavan and M. Ramanathan, *A practical
    guide for SystemVerilog assertions*. Springer Science & Business Media, 2005.'
  decisions:
    evaluation_prompt: 'Qualified. Reason: The paper includes a table detailing evaluation
      methods and mentions an open-source benchmark for assertion generation and evaluation.'
    related_work_prompt: 'Qualified. Reason: The paper engages with prior research
      by citing multiple works, explaining and comparing different methods, and discussing
      existing challenges and solutions in the field.'
    novelty_prompt: 'Qualified. Reason: The paper proposes a novel framework, AssertLLM,
      for automatic assertion generation, claims to be the first to handle full-size
      specification files, and introduces the first open-source benchmark for this
      purpose.'
    review_only_prompt: 'Qualified. Reason: The paper introduces a novel framework,
      AssertLLM, for automatic assertion generation, addressing specific challenges
      in SVA generation and providing new contributions.'
- title: 'Reuse Detector: Improving the Management of STT-RAM SLLCs'
  abstract: 'Various constraints of Static Random Access Memory (SRAM) are leading
    to

    consider new memory technologies as candidates for building on-chip shared

    last-level caches (SLLCs). Spin-Transfer Torque RAM (STT-RAM) is currently

    postulated as the prime contender due to its better energy efficiency, smaller

    die footprint and higher scalability. However, STT-RAM also exhibits some

    drawbacks, like slow and energy-hungry write operations, that need to be

    mitigated. In this work we address these shortcomings by leveraging a new

    management mechanism for STT-RAM SLLCs. This approach is based on the previous

    observation that the stream of references arriving at the SLLC of a Chip

    MultiProcessor (CMP) exhibits reuse locality, i.e., those blocks referenced

    several times manifest high probability of forthcoming reuse. In this paper, we

    employ a cache management mechanism that selects the contents of the SLLC aimed

    to exploit reuse locality instead of temporal locality. Specifically, our

    proposal consists in the inclusion of a Reuse Detector between private cache

    levels and the STT-RAM SLLC to detect blocks that do not exhibit reuse, in

    order to avoid their insertion in the SLLC, hence reducing the number of write

    operations and the energy consumption in the STT-RAM. Our evaluation reveals

    that our scheme reports on average, energy reductions in the SLLC in the range

    of 37-30\%, additional energy savings in the main memory in the range of 6-8\%

    and performance improvements of 3\% up to 14\% (16-core) compared to an STT-RAM

    SLLC baseline where no reuse detector is employed. More importantly, our

    approach outperforms DASCA, the state-of-the-art STT-RAM SLLC management,

    reporting SLLC energy savings in the range of 4-11\% higher than those of

    DASCA, delivering higher performance in the range of 1.5-14\%, and additional

    improvements in DRAM energy consumption in the range of 2-9\% higher than

    DASCA.'
  url: http://arxiv.org/abs/2402.00533v1
  keywords: ''
  document: '# 1. INTRODUCTION


    In the last years chip multiprocessors have become majority on many off-the-shelf
    systems, such as high performance servers, desktop systems, mobile devices and
    embedded systems. In all of them the designers usually include a multilevel memory
    hierarchy, where the shared last-level cache (SLLC) plays an important role in
    terms of cost, performance and energy consumption. As for the cost, the SLLC generally
    occupies a chip area similar or even bigger than that of cores. Regarding performance
    and energy consumption, the SLLC is the last resource before accessing the main
    memory, which delivers higher energy consumption and lower performance as it is
    located outside


    the chip.


    The technologies currently employed in building SLLCs are mainly SRAM or embedded
    DRAM. However, they both reveal as power-hungry, especially for the large sizes
    required as the number of cores increases. One way of mitigating this problem
    is to employ emerging non-volatile memory technologies. Among them, Spin-Transfer
    Torque RAM (STT-RAM) is clearly the prime contender. STT-RAM removes almost all
    the static power consumption and, compared to SRAM, provides higher density and
    therefore much higher capacity within the same budget. Moreover, it delivers higher
    read efficiency in terms of latency and energy. Nevertheless, some obstacles restrict
    the adoptio n of STT-RAM as last-level cache for the next generation


    2 RODRIGUEZ-RODRIGUEZ ET AL


    of CMPs: its write operation is slower and requires more energy than an SRAM cache.
    These constraints may lead to a performance drop and even to almost cancel the
    energy savings derived from the minimal static power consumption of STT-RAM.


    In addition, previous research states that conventional SLLC designs are inefficient
    since they waste most storage space [\[1,](#page-20-0) [2\]](#page-20-1). This
    is due to the fact that SLLC management policies often lead to store *dead blocks*,
    i.e., blocks that will not be referenced again before eviction. Indeed, it is
    frequent that blocks were dead the very first time they enter into the SLLC. This
    is mainly because the cache levels closer to the processor exploit most of the
    *temporal locality*, which therefore becomes largely filtered before accessing
    the SLLC. With the goal of avoiding this effect and hence increasing the hit rate,
    various mechanisms that modify the SLLC insertion and replacement policies have
    been proposed recently.


    This work addresses the shortcomings aforementioned by focusing on improving the
    efficiency, in terms of both performance and energy, of a non-inclusive and nonexclusive
    STT-RAM SLLC in a chip multiprocessor system. Notably, we present a new mechanism
    of content selection for last-level caches that benefits from the *reuse locality*
    that SLLC references exhibit [\[3,](#page-20-2) [4\]](#page-20-3). This locality
    lies in the following principle: when a block is referenced twice in the last
    level cache (i.e. it is reused), this block has a good chance of being referenced
    again in the near future. Our approach pursues to insert in the SLLC only those
    blocks that exhibit reuse at that level. For this purpose, we propose to include
    a new hardware resource between the SLLC and the private cache levels –referred
    to as *Reuse Detector*– which determines for each block evicted from the private
    cache levels if it has been reused or not at the SLLC. If the block is determined
    to having been reused, it is inserted (or it updates) in the SLLC. Otherwise,
    the block *bypasses* the SLLC and is sent directly to main memory.


    Our proposal is evaluated in quad, eight and 16-core systems running multiprogrammed
    workloads, and our experimental results reveal that the reuse detector avoids
    the insertion of low-utility blocks in the STT-RAM SLLC, making it easier to retain
    most of reused blocks. This enables us to reduce the amount of the slow and energyhungry
    writes to the STT-RAM SLLC, which translates into energy consumption reduction
    and system performance improvement, outperforming other recent approaches.


    The rest of the paper is organized as follows: Section 2 motivates our work and
    explains some necessary background. Section 3 presents our proposal to improve
    the STT-RAM LLC management. Sections 4 and 5 detail the experimental framework
    used and the obtained results, respectively. Section 6 recaps some related work
    and finally, Section 7 concludes the paper.


    # 2. BACKGROUND AND MOTIVATION


    In this section we first motivate the need of a new SLLC management scheme by
    describing the main limitations of


    <span id="page-1-1"></span><span id="page-1-0"></span>![](_page_1_Figure_8.jpeg)


    FIGURE 1. (a) STT-RAM memory cell structure. (b) STT-RAM equivalent circuit.


    SRAM technology and conventional management. Then we briefly describe the DASCA
    scheme, which is the closest approach to our work and the state-of-the-art STT-RAM
    SLLC management scheme [\[1\]](#page-20-0).


    # 2.1. SRAM and STT-RAM technologies


    As stated above, various emerging technologies are currently considered to replace
    SRAM as the building-block for SLLCs, being STT-RAM the best placed to overcome
    SRAM constraints, such as energy consumption and read operation latency.


    The main difference between STT-RAM and SRAM is that the information carrier of
    the former is a Magnetic Tunnel Junction (MTJ) instead of electric charges. An
    MTJ contains two ferromagnetic layers (denoted as free and fixed layers) and one
    tunnel barrier layer, see Figure [1\(a\).](#page-1-0) The fixed layer has a fixed
    magnetic direction while the free layer can change its magnetic direction by passing
    a current through the MTJ. If the two ferromagnetic layers have different directions,
    the MTJ resistance is high, indicating a "1" state; if the two layers have the
    same direction, the MTJ resistance is low, indicating a "0" state. A read operation
    to an MTJ is performed by applying a small voltage difference between two electrodes
    of the MTJ and sensing the current flow (see Figure [1\(b\),](#page-1-1) where
    the STT-RAM cell is represented as a variable resistor). A write operation is
    performed by applying a large voltage difference between two electrodes for a
    given duration called write pulse width.


    Table [1](#page-2-0) shows the key features of an 1-bank 1MB LLC implemented with
    SRAM and STT-RAM 22 nm technology, modeled with CACTI 6.5 [\[5\]](#page-21-0)
    and NVSim [\[6\]](#page-21-1), respectively. As shown, an STT-RAM cache exhibits
    smaller die footprint and better efficiency in read operations than an SRAM cache.
    More importantly, an STT-RAM cache consumes almost two orders of magnitude less
    static power compared to SRAM. Conversely, the STT-RAM cache exhibits a significant
    drawback that needs to be mitigated: the poor write performance both in terms
    of latency and energy consumption.


    # 2.2. SLLC management techniques


    Regardless of implementation technology, last-level caches usually suffer from
    the same problem: they keep data


    | Parameter          | SRAM   | STT-RAM | Ratio SRAM/STT-RAM |

    |--------------------|--------|---------|--------------------|

    | Area (mm2<br>)     | 0.94   | 0.35    | 2.68               |

    | Read Latency (ns)  | 8.75   | 5.61    | 1.56               |

    | Write Latency (ns) | 8.75   | 16.5    | 0.53               |

    | Read Energy (nJ)   | 0.56   | 0.32    | 1.75               |

    | Write Energy (nJ)  | 0.56   | 1.31    | 0.43               |

    | Leakage Power (mW) | 190.58 | 3.09    | 61.67              |


    <span id="page-2-0"></span>TABLE 1. Area, latency and energy consumption for 22
    nm SRAM and STT-RAM 1MB caches.


    assuming that recently referenced lines are likely to appear in the near future
    (*temporal locality*). Nevertheless, various recent studies point out that the
    reference stream entered in the SLLC does not usually exhibit temporal locality.
    Notably, in [\[3\]](#page-20-2) the authors observe that this reference stream
    exhibits *reuse locality* instead of temporal locality. Essentially, that term
    describes the property that the second reference to a line is a good indicator
    of forthcoming reuse and also that recently reused lines are more valuable than
    other lines reused longer.


    The studies carried out in [\[3,](#page-20-2) [4,](#page-20-3) [7\]](#page-21-2)
    demonstrate, considering a large amount of multiprogrammed workloads and different
    multiprocessor configurations, two important aspects: first, most lines in the
    SLLC are dead (they will not receive any further hits during their lifetime in
    the SLLC) and second, most SLLC hits come from a small subset of lines. We have
    performed our own analysis about the behavior of the blocks evicted from the SLLC,
    in terms of the amount of accesses they receive before eviction, in the scenario
    with the configuration detailed in Table [2,](#page-7-0) but, as a starting point,
    employing just one core. Figure [2](#page-2-1) illustrates this behavior, grouping
    the blocks into three different categories: no reuse, just one reuse or more than
    one reuse (multiple reuse).


    ![](_page_2_Figure_5.jpeg)


    <span id="page-2-1"></span>FIGURE 2. Breakdown of blocks replaced from the LLC
    according to the number of accesses they receive before eviction.


    As shown, our experimental results confirm that most lines in the LLC are dead.
    Notably, around 70% of the blocks do not receive any further access since the
    moment they enter the LLC. Only around 5% of the blocks just receives one further
    hit (i.e. one reuse) and around 25% exhibit more than one reuse.


    Consequently, getting blocks with just one use (the block fill, so no reuse) to
    bypass the LLC when they are evicted from the previous level caches, and just
    storing blocks with reuse (at least two LLC accesses), should allow to hold the
    small fraction of blocks with multiple reuses, increasing the LLC hit rate and
    improving system performance.


    Furthermore, Figure [3](#page-2-2) shows that most LLC hits are to blocks having
    multiple reuses, which together with the aforementioned fact that most blocks
    inserted in the LLC do not experience any reuse, highly justify the idea of a
    content selector based on reuse detection between private caches and LLC.


    ![](_page_2_Figure_11.jpeg)


    <span id="page-2-2"></span>FIGURE 3. Breakdown of block hits at the LLC according
    to the number of accesses they have received before read.


    # 2.3. DASCA scheme


    In [\[1\]](#page-20-0), the authors propose *Dead Write Prediction Assisted STT-RAM
    Cache Architecture* (DASCA) to predict and bypass dead writes (writes to data
    in last level caches not referenced again during the lifetime of corresponding
    cache blocks) for write energy reduction. In this work dead writes are classified
    into three categories: dead-on-arrival fills, dead-value fills and closing writes,
    as a theoretical model for redundant write elimination. On top of that they also
    present a dead write predictor based on a state-of-the-art dead block predictor
    [\[2\]](#page-20-1). Thus, DASCA bypasses a write operation to the SLLC only if
    it is predicted not to incur extra cache misses.


    Notably, DASCA adds a specific field to each line at the private levels to store
    the PC (program counter) of the instructions writing a block, being this PC only
    updated upon write operations. Also, a PC-signature table (prediction table) is
    included in the design in order to make the prediction about dead writes (this
    table is updated according to the mechanism shown in the Table 2 of the paper
    itself [\[1\]](#page-20-0)). Specifically, the mechanism samples a few cache sets
    and keeps track of PC information only for those sets. Predictions are made via
    the predictor table, made up of saturating counters similar to those used in a
    bimodal branch predictor, being the counters indexed by the signatures stored
    in the sampler entries. Thus, this PC-based predictor correlates dead blocks with
    addresses of memory instructions (signatures), so that different signatures are
    used depending on the kind of dead write predicted.


    # 3. DESIGN


    In this section we first describe the baseline system we start from. Then we describe
    in detail the proposed design built on top of that.


    # 3.1. Baseline system


    The memory hierarchy used in the baseline multi-core system includes two private
    levels (L1 and L2) and a lastlevel cache shared among all the cores (SLLC). All
    caches are write-back, write-allocate and LRU. L1 and L2 are inclusive while the
    SLLC is non inclusive.


    The baseline management of this memory hierarchy is as follows: When a block is
    requested to Main Memory (MM), it is copied to the private cache levels of the
    requester core, but not to the SLLC. During its lifetime at the private levels
    of the core, the block can be requested by other cores, in which case a copy will
    be sent from a private L2 cache to the L1-L2 caches of the requesting core, as
    dictated by the directory-based coherency mechanism (please refer to Section [4](#page-6-0)
    for more details on the coherency mechanism).


    When a block is evicted from an L2 cache, the SLLC is checked: In case the block
    is not present there (either because it has not been inserted yet or because it
    has already been inserted and evicted by the SLLC replacement mechanism), it is
    inserted in the SLLC, otherwise if the block is already in the SLLC, the block
    is updated or just discarded, depending respectively on whether the block is dirty
    or clean. Thus, in our hierarchy, SLLC insertions never come from MM but from
    an L2, in a similar way to an exclusive policy. Note however that our mechanism
    differs from an exclusive policy in that, as a result of a hit in the SLLC, the
    block is copied to the private cache levels of the requester core, but maintained
    in the SLLC.


    # 3.2. The Reuse Detector


    As explained earlier, several works have demonstrated that a notable percentage
    of the blocks inserted/updated in the SLLC are in fact useless, as they are dead-blocks
    (i.e. blocks which will not be referenced any more during their lifetime in the
    SLLC) [\[1,](#page-20-0) [2\]](#page-20-1). These useless blocks are harmful,
    as they evict other blocks which could potentially be useful in the future, and
    moreover, they increase the amount of writes to the SLLC, which in the context
    of NVMs (Non-Volatile Memories) is far from convenient, as explained in previous
    sections.


    In this paper we leverage the technique for reducing the amount of dead-blocks
    inserted/updated in the SLLC [\[7\]](#page-21-2) to improve the efficiency of
    a STT-RAM SLLC. In [\[7\]](#page-21-2), the authors present a proposal that, in
    an exclusive memory hierarchy, reduces the amount of blocks inserted in a conventional
    SLLC by around 90%. We apply this technique to a different context, i.e., to a
    non-inclusive STT-RAM SLLC design within a memory hierarchy where L1-L2 are inclusive.
    The exclusion policy employed in [\[7\]](#page-21-2) implies that, upon a SLLC
    hit, the block is copied to the private cache levels and removed from the SLLC.
    In our case, the block is inserted in the SLLC at the end of its usage in the
    private caches and remains in the SLLC until eviction. For our purpose, we include
    an intermediate element between each private L2 cache and the SLLC (Figure [4\)](#page-3-0).
    A block evicted from the private L2 caches is targeted to the corresponding element,
    which we denote as Reuse Detector (RD), instead


    of accessing directly to the SLLC as it would do in the baseline system. The RD
    decides whether to send the block to the SLLC or not (i.e. to bypass the shared
    last-level cache), by means of a prediction about the future usefulness of the
    block. We must highlight that, being the RD out of the block request path to the
    SLLC, it does not impact the SLLC hit or miss latencies.


    ![](_page_3_Figure_9.jpeg)


    <span id="page-3-0"></span>FIGURE 4. Placement of a Reuse Detector between each
    private L2 level and STT-RAM SLLC.


    For accomplishing the RD prediction, we apply Albericio''s concept of reuse locality
    [\[3,](#page-20-2) [4\]](#page-20-3). As such, if the evicted block from the L2
    has never been requested to the SLLC since the time it entered the cache hierarchy
    (i.e. it has never been reused at the SLLC), the block is predicted as a deadblock
    and thus it bypasses the SLLC, directly updating MM (if the block is dirty) or
    being just discarded (if it is clean). Otherwise, if the block has been reused
    (i.e. it has been requested to the SLLC at least once since the time it entered
    the cache hierarchy) and thus it is predicted as a non-deadblock, it is inserted/updated
    in the SLLC.


    The RD consists of a FIFO buffer and some management logic. The buffer stores
    the tag addresses of the blocks evicted by the private levels in order to maintain
    their reuse state. Moreover, an extra bit, called *reuse bit*, is added to each
    cache line in the private levels. This bit distinguishes if the block was inserted
    at the private cache levels from the SLLC or from another private cache level
    (*reuse* bit is 1), or main memory (*reuse* bit is 0). In the following sections,
    we analyze in detail the Reuse Detector operation and implementation.


    # *3.2.1. Reuse Detector operation*


    As we said in a previous section, our proposal aims to reduce the amount of writes
    to the STT-RAM SLLC and to improve the management of SLLC blocks, which translate
    into system performance improvement and energy consumption reduction on the system.


    Figure [5](#page-4-0) shows a flow diagram of a block request from a


    core to its associated private caches. If the request hits in L1 or L2 the reuse
    bit is untouched, and the block is copied in L1 if it was not there (inclusive
    policy). Otherwise, the request is forwarded to the SLLC. If the access hits in
    the SLLC, the block is provided to the core and copied in the private levels with
    the reuse bit set. If the access misses in the SLLC but the coherency mechanism
    informs that the block is present in another private cache, the block is provided
    by that cache. In this case, the access is recognized as a reuse, so the reuse
    bits are also set. Finally, if no copy of the block is present in the cache hierarchy,
    it is requested to MM and copied in L1-L2 with the reuse bit unset.


    ![](_page_4_Figure_2.jpeg)


    <span id="page-4-0"></span>FIGURE 5. Block request and reuse bit management.


    Figure [6](#page-4-1) shows a flow diagram of a block eviction from an L2 cache
    (if required, the corresponding L1 cache is invalidated). When a block is evicted
    from a last private level cache, its reuse bit is checked. If the reuse bit is
    set, it means that the block was inserted into the private caches either from
    the SLLC or from another private cache after checking the SLLC and the coherency
    mechanism. In any case, the block is considered as having been reused, and it
    should be inserted in the SLLC (if not present yet) or just updated (if the block
    is dirty but it is already present in the SLLC). Note that if the block is clean
    and already present in the SLLC, it can just be discarded. If the reuse bit is
    unset (i.e. the block was brought into the private caches directly from main memory)
    but the block''s tag is found in the RD buffer, the block is also considered as
    having been reused, and thus it is handled as in the previous situation. Finally,
    if the reuse bit is unset and its tag is not present in the RD buffer, it means
    that the block is considered as not having been reused yet. Based again on Albericio''s
    observations [\[3,](#page-20-2) [4\]](#page-20-3), the block should bypass the
    SLLC, as it is predicted as a dead-block, and it should be sent to MM (if the
    block is dirty) or just discarded (if it is clean). Note that in all cases the
    coherency mechanism must be updated.


    ![](_page_4_Figure_5.jpeg)


    <span id="page-4-1"></span>FIGURE 6. Block eviction from a private cache and SLLC
    insertion.


    # *3.2.2. Example*


    For the sake of clarifying the Reuse Detector operation, in this subsection we
    provide a straightforward example illustrating the flow of five memory blocks
    (A, B, C, D and E) through the different cache levels under a given access pattern.
    In this example, we consider a dual-core system (*Core*<sup>0</sup> and *Core*1)
    with private first level caches (*L*1<sup>0</sup> and *L*11), a shared second
    level cache (SLLC), and the corresponding Reuse Detectors between both cache levels.
    In the example we assume a simplified configuration where: 1) direct-mapped L1s,
    2-way set associative RDs and 4-way set associative SLLC are considered; 2) all
    memory blocks map to the same L1 frame and to the same RD and SLLC set; and 3)
    initially, all caches and RDs are empty. Next, we detail the access sequence of
    our example and show the contents of the memory hierarchy after each access in
    Figure [7.](#page-5-0) Note that we specify as a subindex the dirty bit followed
    by the reuse bit (*Xd*,*<sup>r</sup>* ) for each block *X* in the private cache
    levels, and only the dirty bit (*Xd*) for each block *X* in the SLLC.


    1. *Core*<sup>0</sup> requests a word within block A for reading: The access misses
    in *L*10, it is forwarded to the SLLC, and given that the access to SLLC also
    misses and the block is not present in any other private cache, it is forwarded
    to MM. According to Figure [5,](#page-4-0) block A is copied to *L*1<sup>0</sup>
    with its reuse bit unset, and the requested word is provided to *Core*0.


    ![](_page_5_Figure_2.jpeg)


    <span id="page-5-0"></span>FIGURE 7. Example of the Reuse Detector operation.


    - 2. *Core*<sup>1</sup> requests a word within block A for reading: The access
    misses in *L*1<sup>1</sup> and SLLC. However, the coherency mechanism informs
    that the block is at *L*10, so the request is forwarded to that cache. According
    to Figure [5,](#page-4-0) the block is copied to *L*1<sup>1</sup> and both reuse
    bits are set, as we recognize this access as an SLLC reuse.

    - 3. *Core*<sup>1</sup> requests a word within block B for reading: The access
    misses in *L*1<sup>1</sup> and SLLC, and the block is not present in any other
    private cache, so the request is forwarded to MM. According to Figure [5,](#page-4-0)
    block B is copied to *L*1<sup>1</sup> (replacing block A) with its reuse bit unset,
    and the requested word is provided to *Core*1. According to Figure [6,](#page-4-1)
    given that block A had its reuse bit set, it is inserted into the SLLC.

    - 4. *Core*<sup>1</sup> requests a word within block C for reading: Block C is
    inserted in *L*1<sup>1</sup> and block B is replaced. As the reuse bit of block
    B was unset and its tag was not in *RD*1, according to Figure [6](#page-4-1) the
    tag is stored in *RD*<sup>1</sup> and, given that the block is clean, it is not
    inserted in the SLLC but just discarded.

    - 5. *Core*<sup>1</sup> requests a word within block B for reading: This


    access is handled analogously to the previous access.


    - 6. *Core*<sup>1</sup> requests a word within block D for reading: Block D is
    inserted in *L*1<sup>1</sup> and block B is replaced. As the reuse bit of block
    B was unset but its tag was present in *RD*1, according to Figure [6](#page-4-1)
    block B is inserted in the SLLC.

    - 7. *Core*<sup>0</sup> writes to a word within block A: The access hits in *L*10.
    The dirty bit for the block is set.

    - 8. *Core*<sup>0</sup> requests a word within block E for reading: Block E is
    inserted in *L*1<sup>0</sup> and block A is replaced. As the dirty bit of block
    A is set and A is already present in the SLLC, the block is updated at this level.


    # <span id="page-5-1"></span>*3.2.3. Implementation details*


    Although a typical set-associative design could be used for the RD implementation,
    where a whole block tag, a validity bit and some information related with the
    replacement policy is included for each line, as in [\[7\]](#page-21-2) we use
    two techniques aimed at reducing the required space: sectoring and compression.
    A sector is a set of consecutive memory blocks aligned to the sector size. Storing
    sector tags in


    the RD allows to merge in a single line of the RD the information related with
    several blocks. Note that for each entry it is necessary to store a presence bit.
    For example, with a sector comprising 4-blocks, each line is made up of a tag
    derived from the sector address, a validity bit, some bits storing the replacement
    state and 4 presence bits.


    The compression of the tags is achieved based on the following process: let *t*
    be the amount of bits of the full tag and *c* the amount of bits of the compressed
    tag, being t>c. We first divide the full tag into several pieces, each of size
    *c* (the last piece is filled with *0s* if necessary). Then, we *xor* all the
    pieces, obtaining the compressed tag. Note that each compressed tag is shared
    among various sectors, thus false positives are possible where non-reused blocks
    are delivered to the SLLC. This situation does not cause a functional problem,
    but it may degrade system performance, so the value of *c* must be chosen carefully.


    As for the storage overhead of the RD implementation, i.e., the total amount of
    extra bits required compared to the baseline, we need the following hardware:
    The RD has 1024 sets and 16 ways (our simulations reveal that this value provides
    similar performance to that of higher associativity values), and a sector size
    of 2 blocks. Each RD entry requires 14 bits (10 for the compressed tag, 2 for
    the block presence, 1 for the replacement policy and 1 validity bit) as Figure
    [8](#page-6-1) illustrates. Given that the amount of entries in the RD is 8K,
    the total extra storage required per core is 14 KB, which represents a negligible
    1.3% of an 1MB last level cache.


    | CompressedTag | S1 S1   SpL   V |                                                     |  |

    |---------------|-----------------|-----------------------------------------------------|--|

    | Bits 13-4     |                 | Bit 3 Bit 2 Bit 2 Bit 1 - Bit 1 - Bit 1 -
    Bit 0 - / |  |


    <span id="page-6-1"></span>FIGURE 8. Reuse Detector entry.


    Finally, as RD replacement policy we use a 1-bit FIFO. Again based on our simulations,
    this scheme delivers a similar performance as other policies that would require
    more storage. In a FIFO policy, age information is updated only when a new address
    is inserted, and not during subsequent hits. This approach is fully consistent
    with the main RD buffer goal of detecting the first reuse of a block.


    # 3.3. Reuse Detector vs DASCA


    In this section we briefly discuss the main differences between the RD approach
    and the DASCA scheme, which will be further extended and analyzed in the evaluation
    section. As for the operation of both approaches, note that the DASCA mechanism
    tries to predict dead writes based on a PC-based predictor. For this purpose,
    the PC signature of each block that accesses the SLLC must be recorded. Conversely,
    the RD scheme tries to predict dead-blocks based on their reuse state. Our prediction
    is based on the addresses of the accessed data instead of the instruction addresses
    used in DASCA. Also, in our approach we directly store the mentioned addresses
    while in the DASCA scheme the authors employ a PC-signatures table which is trained
    by an auxiliary cache that works in parallel with the conventional cache.


    Focusing on the specific implementation of the DASCA scheme for the evaluation
    of this proposal, it is worthy to note that our approach employs a memory hierarchy
    where L1 and L2 are inclusive while the SLLC (L3) is non inclusive, whereas the
    original DASCA scheme is evaluated in [\[1\]](#page-20-0) employing a memory hierarchy
    with just two cache levels and assuming non-inclusive caches by default, although
    the authors also propose a bypassing scheme that supports both inclusive and non-inclusive
    caches. Therefore, and looking for a fair comparison between RD and DASCA, we
    implement DASCA using exactly the same three-level non-inclusive non-exclusive
    hierarchy employed in the RD approach. As a result of that, the only high-level
    change with respect to the original version of DASCA lies in that one of the three
    possible cases they use to classify the dead writes ("dead-value fills", blocks
    that receive a writeback request from lower-level caches right after the block
    is filled, but before any read operation, i.e., the filled block data are overwritten
    before being read), can not exist since they are removed by the inclusion mechanism
    we employ in our approach. Notably, this is due to the fact that, in our configuration,
    all the insertions in the SLLC are motivated by an L2 block eviction, and, as
    L1 and L2 are inclusive, these evicted blocks are only located in the SLLC after
    the eviction from L2. If after that the processor generates a write request on
    one of these blocks, a write hit occurs, and consequently the block is copied
    to the private cache levels and therefore this block in the SLLC can not be written
    again in the SLLC before being read. Hence, we consider this evaluation as fair,
    since this way we are evaluating DASCA under the same conditions as our approach,
    so that we are not giving an advantage to our RD by the fact that it directly
    avoids the "dead-value fills" with the inclusiveness management.


    # <span id="page-6-0"></span>4. EXPERIMENTAL FRAMEWORK


    For our experiments we use the *gem5* simulator [\[8\]](#page-21-3) and we employ
    the *ruby* memory model, specifically the MOESI\_CMP-directory coherence policy
    provided by the simulator. It is worth noting that we focus on a MOESI policy
    since protocols with owned state (e.g. MOESI and MOSI) are able to reduce the
    number of writes to the LLC, as demonstrated by Chang et.al. [\[9\]](#page-21-4).
    We modify the coherence protocol, encoding the proposed reuse detector. We simulate
    both a single and a multi-core scenario. For the sake of a better accuracy in
    both execution modes, an O3 processor type (detailed mode of simulation) was used.


    The main features of both the processor and the memory hierarchy are shown in
    Table [2.](#page-7-0) The network used is a crossbar modeled with Garnet [\[10\]](#page-21-5),
    a detailed interconnection network model inside gem5. As explained above, for
    the evaluation of our proposed RDs we implement them in the cache hierarchy modifying
    the coherence protocol. For modeling the DRAM main memory we use DRAMSIM2 [\[11\]](#page-21-6).
    We adapt the LLC read and 8 RODRIGUEZ-RODRIGUEZ ET AL


    | Architecture         | x86                                                                                         |

    |----------------------|---------------------------------------------------------------------------------------------|

    | CPUS                 | 1/4/8, 2GHz                                                                                 |

    | Pipeline             | 8 Fetch, 8 Decode, 8 Rename, 8 Issue/Execute/Writeback,
    8 Commit                            |

    | Registers            | Integer (256), Floating Point (256)                                                         |

    | Buffers              | Reorder Buffer (192), Instruction Queue (64)                                                |

    | Branch Predictor     | TournamentBP                                                                                |

    | Functional Units     | IntALU=6, IntMulDiv=2, FPALU=4, FPMultDiv=2, SIMD-Unit=4,
    RdWrPort=4, IprPort=1             |

    | Private Cache L1 D/I | 32 KB, 8 ways, LRU replacement, Block Size 64B, Access
    Latency 2 cycles, SRAM               |

    | Private Cache L2 D/I | 256 KB, 16 ways, LRU replacement, Block Size 64B, Access
    Latency 5 cycles, SRAM             |

    | Interconnection      | Crossbar network, modeled using Garnet, latency 3 cycles                                    |

    | Shared Cache L3      | 1 bank/1MB/core, 16 ways, LRU replacement, Block Size
    64B, R/W Latency 6/17 cycles, STT-RAM |

    | DRAM                 | 2 Ranks, 8 Banks, 4kB Page Size, DDR3 1066MHz                                               |

    | DRAM Bus             | 2 channels with a 8 bus of 8 bits                                                           |


    <span id="page-7-0"></span>TABLE 2. CPU and Memory Hierarchy specification.


    | High<br>Medium                    |                                    | Low                                |  |

    |-----------------------------------|------------------------------------|------------------------------------|--|

    | lbm, mcf, libquantum, bwaves,     | bzip2, soplex, gcc, wrf, astar,    | gromacs,
    calculix, h264ref, tonto, |  |

    | milc, cactusADM, zeusmp, leslie3d | hmmer, xalancbmk, gobmk, perlbench | omnetpp,
    namd, sphinx3, GemsFDTD   |  |


    <span id="page-7-1"></span>TABLE 3. Benchmark characterization according to the
    number of LLC writes per Kinstruction (WPKI).


    write latencies according to the STT-RAM target. Both latencies and energy consumption
    values are obtained from NVSim [\[6\]](#page-21-1) for a 1MB (1 bank) cache and
    are illustrated in Table [1.](#page-2-0) For scaling the LLC to larger sizes,
    we multiply the leakage power by the number of cores.


    Our experiments make use of the SPEC CPU2006 benchmark suite [\[12\]](#page-21-7).
    When we evaluate our proposal in a single core scenario (LLC 1MB size) we employ
    *reference* inputs and simulate 1 billion instructions from the checkpoint determined
    using PinPoints [\[13\]](#page-21-8). Note that results from 4 out of 29 benchmarks
    are not considered in the evaluation section due to experimental framework constraints.
    We also report results of 28 multiprogrammed mixes employing SPEC CPU2006 programs
    in 4, 8 and 16- CMP systems with 4, 8 and 16MB SLLC sizes, respectively. In all
    the cases, we fast forward 100M instructions, warm up caches for 200M instructions
    and then report results for at least 500M instructions per core.


    For selecting the aforementioned multiprogrammed mixes, we employ the following
    methodology: we execute each benchmark alone, using an LLC of 1MB and without
    any reuse detector, and we measure the amount of LLC writes that it generates.
    We then obtain for each benchmark the *number of writes to LLC per 1000 instructions*ratio


    (WPKI). Based on these values, we include each benchmark into the *high*, *medium*
    or *low* category. Specifically, the *high* category includes benchmarks with
    a WPKI higher than 8, the *medium* one those with a WPKI satisfying 1 <*WPKI*
    < 8 and finally, in the *low* category we include the programs with a WPKI lower
    than 1. Table [3](#page-7-1) shows this classification. Based on this classification,
    and as detailed in Section [5,](#page-9-0) we build some mixes made up of programs
    with high WPKI, some with medium WPKI, some with low WPKI, and some combining
    applications from different WPKI categories trying to fill most of the combinations
    high-medium, high-low, medium-low and high-medium-low. Tables [4,](#page-7-2)
    [5](#page-8-0) and [6](#page-8-1) show the built mixes for the 4-core, 8-core
    and 16-core CMP systems, respectively, where for each mix the applications are
    sorted first by decreasing WPKI categories (from the high category to the low
    one) and then within each of the categories they are also sorted alphabetically.
    We employ the symbol ";" to separate applications from different categories and
    also the parenthesis to indicate how many instances of an application (when employ
    more than one) we are using in each mix.


    Energy model: The DRAM energy is obtained directly from the simulator. For computing
    the LLC energy we employ a model that includes both dynamic and static


    | Mixes    | Applications                         | Mixes    | Applications                           |

    |----------|--------------------------------------|----------|----------------------------------------|

    | mix.H0   | cactusADM, leslie3d, libquantum, mcf | mix.H1   | bwaves, cactusADM,
    leslie3d, milc      |

    | mix.H2   | libquantum, mcf, milc, zeusmp        | mix.H3   | bwaves, cactusADM,
    lbm, leslie3d       |

    | mix.M0   | bzip2, gobmk, soplex, xalancbmk      | mix.M1   | gcc, perlbench,
    wrf, xalancbmk         |

    | mix.M2   | gcc, gobmk, hmmer, soplex            | mix.M3   | astar, gobmk, perlbench,
    wrf           |

    | mix.L0   | calculix, GemsFDTD, namd, sphinx3    | mix.L1   | gromacs, h264ref,
    omnetpp, tonto       |

    | mix.L2   | calculix, GemsFDTD, omnetpp, sphinx3 | mix.L3   | gromacs, h264ref,
    namd, tonto          |

    | mix.HM0  | milc, zeusmp; astar, gcc             | mix.HM1  | lbm, leslie3d,
    libquantum; gobmk       |

    | mix.HM2  | milc, zeusmp; gcc, gobmk             | mix.HM3  | bwaves, mcf; soplex,
    xalancbmk         |

    | mix.HL0  | bwaves, cactusADM, leslie3d; omnetpp | mix.HL1  | bwaves, lbm, libquantum;
    omnetpp       |

    | mix.HL2  | lbm, leslie3d; gromacs, namd         | mix.HL3  | leslie3d, milc;
    GemsFDTD, omnetpp      |

    | mix.ML0  | perlbench; gromacs, sphinx3, tonto   | mix.ML1  | hmmer, wrf; gromacs,h264ref            |

    | mix.ML2  | perlbench, wrf; GemsFDTD, namd       | mix.ML3  | soplex, xalancbmk;
    sphinx3, tonto      |

    | mix.HML0 | mcf; hmmer; h264ref, omnetpp         | mix.HML1 | milc; hmmer; GemsFDTD,
    h264ref         |

    | mix.HML2 | milc; bzip2, wrf; GemsFDTD           | mix.HML3 | bwaves, leslie3d;
    xalancbmk; GemsFDTD, |


    <span id="page-7-2"></span>TABLE 4. SPEC 2006 multiprogrammed mixes for the 4-core
    CMP.


    | Mixes    | Applications                                                             |

    |----------|--------------------------------------------------------------------------|

    | mix.H0   | bwaves, cactusADM, lbm, leslie3d, libquantum, mcf, milc, zeusmp          |

    | mix.H1   | bwaves, cactusADM(2), leslie3d(2), libquantum, mcf, milc                 |

    | mix.H2   | bwaves, cactusADM, leslie3d, libquantum, mcf, milc(2), zeusmp            |

    | mix.H3   | cactusADM, lbm, leslie3d, libquantum(2), milc, zeusmp(2)                 |

    | mix.M0   | astar, gcc, gobmk, hmmer, perlbench, soplex, wrf, xalancbmk              |

    | mix.M1   | bzip2, gcc, perlbench(2), soplex, wrf, xalancbmk(2),                     |

    | mix.M2   | gcc, gobmk(2), hmmer, perlbench, soplex(2), xalancbmk                    |

    | mix.M3   | astar, gobmk(2), hmmer, perlbench, soplex, wrf(2)                        |

    | mix.L0   | calculix, GemsFDTD, gromacs, h264ref, namd, omnetpp, sphinx3, tonto      |

    | mix.L1   | GemsFDTD, gromacs, h264ref(2), namd, omnetpp(2), tonto                   |

    | mix.L2   | gromacs(2), h264ref(2), namd, omnetpp, tonto(2)                          |

    | mix.L3   | calculix, GemsFDTD(2), gromacs, namd, omnetpp, tonto(2)                  |

    | mix.HM0  | cactusADM, leslie3d, libquantum, mcf; bzip2, gobmk, soplex, xalancbmk    |

    | mix.HM1  | lbm, mcf, milc, zeusmp; gcc, hmmer, perlbench, wrf                       |

    | mix.HM2  | bwaves, cactusADM, libquantum, milc; astar, gobmk, perlbench, soplex     |

    | mix.HM3  | cactusADM, lbm, leslie3d, zeusmp; gcc, gobmk, soplex, wrf                |

    | mix.HL0  | cactusADM, leslie3d, libquantum, mcf; calculix, GemsFDTD, namd, sphinx3  |

    | mix.HL1  | lbm, mcf, milc, zeusmp; gromacs, h264ref, omnetpp, tonto                 |

    | mix.HL2  | bwaves, lbm, libquantum, milc; GemsFDTD, h264ref, sphinx3, tonto         |

    | mix.HL3  | cactusADM, lbm, leslie3d, zeusmp; calculix, gromacs, namd, omnetpp       |

    | mix.ML0  | bzip2, gobmk, soplex, xalancbmk; calculix, GemsFDTD, namd, sphinx3       |

    | mix.ML1  | astar, hmmer, perlbench, wrf; h264ref, gromacs, omnetpp, tonto           |

    | mix.ML2  | gcc, gobmk, perlbench, xalancbmk; GemsFDTD, gromacs, omnetpp, sphinx3    |

    | mix.ML3  | bzip2, hmmer, soplex, wrf; calculix, h264ref, namd, tonto                |

    | mix.HML0 | bwaves, cactusADM, leslie3d; gobmk, soplex, xalancbmk; GemsFDTD,
    sphinx3 |

    | mix.HML1 | lbm, libquantum, mcf; astar, perlbench; calculix, h264ref, namd          |

    | mix.HML2 | cactusADM, milc, zeusmp; hmmer, wrf, xalancbmk; h264ref, tonto           |

    | mix.HML3 | leslie3d, mcf, milc; perlbench, soplex; gromacs, namd, omnetpp           |


    <span id="page-8-0"></span>TABLE 5. SPEC 2006 multiprogrammed mixes for the 8-core
    CMP.


    | Mixes    | Applications                                                                                                                              |

    |----------|-------------------------------------------------------------------------------------------------------------------------------------------|

    | mix.H0   | bwaves(2), cactusADM(3), lbm, leslie3d(3), libquantum(2), mcf(2),
    milc(2), zeusmp                                                         |

    | mix.H1   | bwaves(2), cactusADM, lbm(2), libquantum(2), mcf(3), milc(4), zeusmp(2)                                                                   |

    | mix.H2   | bwaves, cactusADM, lbm(3), leslie3d(4), libquantum(3), mcf(2), milc,
    zeusmp                                                               |

    | mix.H3   | bwaves(3), cactusADM(2), lbm(2), leslie3d(2), libquantum, mcf, milc(2),
    zeusmp(3)                                                         |

    | mix.HM0  | bwaves, cactusADM, lbm, leslie3d, libquantum, mcf, milc, zeusmp;
    astar, gcc, gobmk, hmmer, perlbench, soplex, wrf, xalancbmk              |

    | mix.HM1  | bwaves, cactusADM(2), leslie3d(2), libquantum, mcf, milc; bzip2,
    gcc, perlbench(2) soplex, wrf, xalancbmk(2)                              |

    | mix.HM2  | bwaves, cactusADM, libquantum, leslie3d, mcf, milc(2), zeusmp; gcc,
    gobmk(2), hmmer, perlbench, soplex(2), xalancbmk                      |

    | mix.HM3  | bwaves, cactusADM, lbm, leslie3d, libquantum, mcf, milc, zeusmp;
    astar, gobmk(2), hmmer, perlbench, soplex, wrf(2)                        |

    | mix.M0   | astar(2), bzip2(2), gcc, gobmk(3), hmmer(2), perlbench(2), soplex,
    wrf(2), xalancbmk                                                      |

    | mix.M1   | astar, bzip2(2), gcc, gobmk, hmmer, perlbench, soplex(2), wrf(3),
    xalancbmk(4)                                                            |

    | mix.M2   | astar, bzip2, gcc, gobmk(3), hmmer(3), perlbench(2), soplex(4), xalancbmk                                                                 |

    | mix.M3   | astar(2), bzip2, gcc(3), perlbench, soplex(2), wrf(4), xalancbmk(3)                                                                       |

    | mix.L0   | calculix(2), GemsFDTD(2), gromacs(2), h264ref(2), namd(2), omnetpp(2),
    sphinx3(2), tonto(2)                                               |

    | mix.L1   | calculix, GemsFDTD(3), gromacs(3), h264ref, namd(2), omnetpp(2),
    sphinx3(3), tonto                                                        |

    | mix.L2   | calculix(2), GemsFDTD(3), gromacs(3), h264ref(3), namd(3), tonto(2)                                                                       |

    | mix.L3   | calculix(4), gromacs(2), h264ref(2), omnetpp(4), sphinx3(2), tonto(2)                                                                     |

    | mix.HL0  | cactusADM, lbm, leslie3d, libquantum, mcf(2), milc, zeusmp; calculix,
    GemsFDTD, gromacs, h264ref, namd, omnetpp, sphinx3, tonto           |

    | mix.HL1  | bwaves, lbm(2), libquantum, mcf, milc(2), zeusmp; GemsFDTD, gromacs,
    h264ref(2), omnetpp, sphinx3, tonto(2)                               |

    | mix.HL2  | cactusADM, lbm(2), leslie3d, mcf, milc, zeusmp(2); calculix, gromacs(2),
    h264ref, namd, omnetpp(2), tonto                                 |

    | mix.HL3  | bwaves, cactusADM, lbm, leslie3d, libquantum(2), mcf, milc; calculix,
    GemsFDTD(2), h264ref, namd, sphinx3(2), tonto                       |

    | mix.ML0  | astar, bzip2, gobmk, hmmer, perlbench, soplex, wrf, xalancbmk; calculix,
    GemsFDTD, gromacs, h264ref, namd, omnetpp, sphinx3, tonto        |

    | mix.ML1  | astar(2), gcc, perlbench(2), soplex(2), xalancbmk(2); GemsFDTD(3),
    h264ref(2), tonto(2)                                                   |

    | mix.ML2  | astar, bzip2, hmmer(2), perlbench, soplex, wrf(2); calculix, gromacs,
    h264ref(2), namd, omnetpp, tonto(2)                                 |

    | mix.ML3  | bzip2, gcc(2), gobmk(2), perlbench, soplex, xalancbmk(2); calculix,
    GemsFDTD, gromacs, namd, omnetpp, sphinx3(2)                          |

    | mix.HML0 | bwaves, cactusADM, lbm, leslie3d, libquantum, mcf; astar, gobmk,
    perlbench, soplex, xalancbmk; calculix, GemsFDTD, h264ref, namd, sphinx3 |

    | mix.HML1 | cactusADM, leslie3d, mcf, milc(2), zeusmp; hmmer, perlbench, soplex,
    wrf, xalancbmk; gromacs, h264ref, namd, omnetpp, tonto               |

    | mix.HML2 | lbm, leslie3d, libquantum, mcf(2), milc; astar, perlbench(2), soplex;
    calculix, gromacs, h264ref, namd(2), omnetpp                        |

    | mix.HML3 | bwaves, cactusADM(2), leslie3d, milc, zeusmp; gobmk, hmmer, soplex,
    wrf, xalancbmk(2); GemsFDTD, h264ref, sphinx3, tonto                  |


    <span id="page-8-1"></span>TABLE 6. SPEC 2006 multiprogrammed mixes for the 16-core
    CMP.


    contributions. The static component is calculated using NVSim [\[6\]](#page-21-1),
    which reports the leakage number for 1MB LLC. Thus, we multiply that number by
    the execution time and the number of cores to obtain the total static energy.
    In the case of the dynamic component, we again use NVSim for determining the dynamic
    energy consumption per access to the LLC. Then, we compute the dynamic energy
    consumption as follows:


    $$\begin{aligned} \text{Dynamic Energy} &= H\_{LLC} \ast H E\_{LLC} + W\_{LLC}
    \ast W E\_{LLC} + \\ &M\_{LLC} \ast M E\_{LLC} \end{aligned} \tag{1}$$


    where H*LLC*, W*LLC* and M*LLC* denote the number of hits, writes and misses in
    the LLC respectively, and HE*LLC*, WE*LLC* and ME*LLC* correspond to the energy
    consumption of a hit, a write and a miss in the LLC respectively.


    # <span id="page-9-0"></span>5. EVALUATION


    This section compares how well RD and DASCA behave when managing an STT-RAM LLC,
    both in terms of performance and energy consumption of LLC and main memory. Single,
    four and eight-core systems are discussed in Sections [5.1,](#page-9-1) [5.2,](#page-11-0)
    and [5.3,](#page-15-0) respectively. Finally, in Section [5.4,](#page-17-0) we
    show the results derived from the evaluation in a 16-core CMP and we analyze the
    performance of both approaches as the number of cores augments.


    # <span id="page-9-1"></span>5.1. Evaluation in a single-core scenario


    First, we show the number of writes to the LLC that each evaluated proposal involves
    as well as the performance delivered. Then, we focus on the involved energy consumption
    in both the STT-RAM and the main memory according to the model detailed in Section
    [4.](#page-6-0) Finally, we discuss the obtained results. All the graphs shown
    in this section report individual data for each benchmark, adding at the right
    end the arithmetic mean considering all data (labeled as *AVG*) or the geometric
    mean (labeled as *GMEAN*) in the case of the performance metric, and also the
    arithmetic mean of the eight most write-intensive benchmarks according to Table
    [3](#page-7-1) or the corresponding geometric mean for the performance metric
    (labeled as *HIGH* in both cases).


    # *5.1.1. Write filtering*


    Figure [9](#page-10-0) illustrates the number of writes to the STT-RAM LLC generated
    by the DASCA scheme and our proposal (using a RD of 8K entries) normalized to
    a baseline system without any write reduction/filtering scheme.


    As shown, our proposal significantly outperforms DASCA. Notably, in 20 out of
    25 benchmarks evaluated the Reuse Detector exhibits higher ability in cutting
    the write traffic to the STT-RAM LLC. Overall, the block bypassing decisions commanded
    by RD reduce the number of LLC writes in the baseline system around 65% whereas
    DASCA just achieves a 52% reduction. In addition, if we zoom just in the 8 programs
    with highest WPKI numbers (those labeled as *high* in Table [3\)](#page-7-1),
    RD reduces the number of LLC writes by around 80% with respect to the baseline,
    while DASCA cuts the write traffic by 66%.


    # *5.1.2. Performance*


    Apart from the goal of decreasing the STT-RAM LLC energy consumption (quantified
    later in this section), it is clear that energy efficiency should not come at
    the expense of a performance drop. Thus, to further evaluate the benefits of RD,
    Figure [10](#page-10-1) shows the performance (IPC) delivered.


    Overall our scheme performs moderately better than DASCA: RD delivers 1.9% performance
    improvement compared to the baseline while DASCA just improves IPC by 0.3%. If
    we focus on the write-intensive applications RD clearly outperforms DASCA, achieving
    performance improvements of 5% and 1.4%, respectively. This reveals, as we will
    confirm later in the multi-core environment, that


    our approach works especially well for those applications for which the number
    of writes performed to the LLC is high.


    # *5.1.3. Energy savings*


    Figure [11](#page-10-2) shows the total energy savings (adding both the dynamic
    and the static components) in the LLC. Overall, our proposal reports 34.5% energy
    reduction compared to the baseline while DASCA reports 29.5%. Considering only
    the write-intensive programs, the numbers are 60% and 49%, respectively. If we
    split the total energy savings with respect to the baseline into the dynamic and
    static parts, our proposal achieves 50% of reduction in the dynamic part considering
    all the applications (68% for the *high* programs), while DASCA obtains 42% (57%
    for the *high* benchmarks). As for the static part RD is able to obtain 2% energy
    savings (around 5% for the *high programs*) while DASCA just achieves 0.3% (1.4%
    for the write-intensive applications). Note that avoiding LLC writes reduces dynamic
    energy, whereas increasing performance translates into static energy savings.
    It is also worth noting that, as Figure [12](#page-11-1) illustrates, the dynamic
    energy consumption in the LLC of the baseline system is, for most of the applications
    evaluated, significantly higher than the static contribution.


    Finally, we have also explored the impact on the energy consumption in the DRAM
    main memory. For the sake of simplicity, we do not show the results for all the
    applications. However, as expected, the DRAM energy reduction follows the trend
    of performance improvement. Overall, our proposal manages to reduce the DRAM energy
    consumption by 2% (4.7% for the write-intensive programs) with respect to the
    baseline while DASCA just improves the memory energy consumption by 0.2% (1.1%
    for the *high* applications).


    # *5.1.4. Discussion*


    If we zoom into specific benchmarks, there are some special cases that deserve
    further detail to get a deeper insight. Note that globally, the relative trend
    shown in the amount of writes to the LLC between our approach and DASCA for each
    benchmark, is mainly held in the energy consumption differences, although modulated
    with the relative performance numbers. However, there are some few exceptions
    such as *namd*, *GemsFDTD* or *omnetpp*, where RD is able to reduce the amount
    of LLC writes significantly more than DASCA but the energy savings and also the
    performance improvements obtained by both techniques are almost the same (and
    quite low compared to the baseline) in all these three cases. The reason is that
    these programs are three of the four benchmarks that exhibit the lowest values
    of WPKI, so although the write reductions that RD achieves in relative terms compared
    to DASCA is significant for these applications, the corresponding reduction in
    absolute values are very modest, and therefore the impact on the energy is almost
    negligible.


    Also, in other applications such as *mcf*, *cactusADM* or *hmmer*, our approach
    is able to report IPC numbers significantly higher than in DASCA, while both techniques


    ![](_page_10_Figure_0.jpeg)


    FIGURE 9. Number of writes to the STT-RAM LLC normalized to the baseline: SPEC
    CPU2006 suite.


    <span id="page-10-0"></span>![](_page_10_Figure_2.jpeg)


    <span id="page-10-1"></span>FIGURE 10. Performance (Instructions per Cycle) normalized
    to the baseline: SPEC CPU2006 suite.


    exhibit quite similar write reduction capabilities. In order to explain that,
    first note that there are many different aspects involved in the system performance
    delivered. Among others, one key aspect is that reducing the amount of writes
    to the LLC is not sufficient in itself to guarantee performance improvements:
    although the main goals when bypassing blocks from the LLC to main memory are
    both to save energy and improve performance by increasing the hit rate in the
    LLC, obviously the bypassing may fail in the sense that a bypassed block could
    be referenced again soon, leading to a LLC miss and even a performance drop with
    respect to the case where bypassing is not carried out. Thus, for all these


    three benchmarks, the experimental data reveal that with our proposal the amount
    of hits in the LLC clearly outperforms both the baseline and the DASCA mechanism.
    Notably, the amount of LLC hits experienced in the *cactusADM* and *mcf* programs
    are 7.23x and 2x the values obtained in the baseline, while DASCA obtains 1.89x
    and 0.89x, respectively. Also, the amount of misses in the LLC is lower than that
    of the baseline and DASCA, with values ranging between 0.77-0.87x those obtained
    in the baseline. Considering all the evaluated benchmarks, RD is able to improve
    the amount of hits around 31% with respect to the baseline (106% if we only consider
    the write-intensive


    ![](_page_10_Figure_6.jpeg)


    <span id="page-10-2"></span>FIGURE 11. Energy consumption in the STT-RAM LLC normalized
    to the baseline: SPEC CPU2006 suite.


    ![](_page_11_Figure_1.jpeg)


    <span id="page-11-1"></span>FIGURE 12. Breakdown of energy consumption in the
    LLC into the static and dynamic contributions for the baseline in the singlecore
    system.


    applications) while DASCA experiments only 5% increment when considering all the
    benchmarks and 31% for the *high* applications.


    At a first glance, the behavior of the *libquantum* application may seem somehow
    strange: Neither RD nor DASCA are able to significantly reduce the amount of writes
    to the LLC, but however this benchmark running under RD reports a performance
    improvement of 7% with respect to the baseline while the performance remains largely
    unchanged under DASCA. In addition, and as one would expect since the number of
    bypasses is low, the number of hits in the LLC is practically the same in the
    three cases. The reason to explain the performance improvement lies in the LLC
    bank contention due to the write activity: this application is by far the most
    stalled one due to write contention. Thus, although the write reduction is very
    limited with our scheme, it is enough to reduce stalls with respect to the baseline
    by around 8%, which in absolute numbers implies various millions of these kind
    of situations avoided, which leads to the performance improvement obtained.


    Conversely, although other benchmarks such as *gromacs*, *calculix* or *wrf* exhibit
    moderate LLC writes reduction with RD and DASCA, they all perform worse than in
    the baseline. For these three programs the amount of hits experienced in the LLC
    is, in RD and DASCA, lower than in the baseline, which suggests that the bypassing
    performed is not efficient for these benchmarks. Recall that the energy savings
    achieved in the LLC as a consequence of the reduction in the number of writes
    performed in this cache level may be partially offset with the performance drop
    derived from the increment in the amount of LLC misses, as in these three programs
    occurs. Note also that, although the write operations are outside the critical
    path, the performance improvement derived from avoiding the long write operations
    may be mitigated if bank contention exists between the writes effectively performed.


    # <span id="page-11-0"></span>5.2. Evaluation in a 4-core CMP system


    In this section we extend the previous single-core analysis to a more up-to-date
    environment: a multi-core scenario where the LLC is shared among different cores.
    For this purpose, we measure again the number of writes to the SLLC, the performance
    and the energy consumption in both the STT-RAM SLLC and the DRAM main memory for
    RD and DASCA and report results normalized to the baseline. However, due to the
    inherent non-determinism that all simulators exhibit (especially in multi-core
    environments, where the number of instructions executed across different schemes
    are not stable owing to the random interleaving among memory accesses of different
    programs) and for the sake of higher accuracy, we employ in this scenario, as
    well as in the 8 and 16-core CMP systems, the arithmetic mean of the number of
    writes and energy consumption (per application) but *divided by the total number
    of instructions executed*. Note that, conversely, in the single-core scenario
    both kind of metrics match, since all the benchmarks execute the same amount of
    instructions (1B) in all the runs.


    We employ 28 mixes made up of applications from the SPEC CPU2006 suite chosen
    accordingly to the WPKI categories illustrated in Table [3.](#page-7-1) First,
    we randomly compose three groups of 4 mixes made up of applications belonging
    to just one WPKI category (mixes referred to as H*i*, M*i* and L*i* for high,
    medium and low WPKI respectively). Then, we build other 16 mixes merging applications
    with WPKI corresponding to different categories and trying to construct them in
    a balanced and homogeneous fashion. Again, the workload name encodes the WPKI
    categories of the applications. For example, HL2 is the third mix we build consisting
    of applications with high WPKI (2 in this case) and applications with low WPKI
    (other two). The detailed mixes are illustrated in Table [4.](#page-7-2) Most
    graphs in this section report individual results for each mix, the arithmetic
    mean (AVG) considering all the mixes, just the 4 H*i* mixes (HIGH), the 4 H*i*
    and the 4 HM*i* mixes together (H+HM), the 4 H*i*, the 4 HM*i* and the 4 HML*i*
    mixes together (H+HM+HML) and all the mixes including at least a high program
    (SomeH). Again, in the case of the performance metric we employ the geometric
    mean instead of the arithmetic one.


    # *5.2.1. Write filtering*


    Figure [13](#page-12-0) illustrates the number of writes to the STT-RAM SLLC generated
    by using DASCA and an 8K-entry RD per core normalized to a baseline STT-RAM without
    any write reduction mechanism.


    The experimental results reveal that RD exhibits a significantly greater ability
    to decrease the amount of writes to the SLLC than DASCA. Notably, in 25 out of
    the 28 mixes evaluated RD outperforms DASCA. Overall, the number of writes in
    the baseline system gets reduced to 37% by using RD, in contrast with DASCA which
    only achieves a 48%. As for the write-intensive mixes the RD and DASCA makes around
    40% and 46% of the writes the baseline performs, respectively.


    ![](_page_12_Figure_0.jpeg)


    <span id="page-12-0"></span>FIGURE 13. Number of writes to the STT-RAM SLLC normalized
    to the baseline in the 4-core CMP system.


    # <span id="page-12-1"></span>*5.2.2. Performance*


    In order to evaluate performance when executing multiprogrammed workloads, we
    analyze the *Instruction Throughput* (IT) and the *Weighted Speedup* (WS) metrics.
    The IT metric is defined as the sum of all instructions committed per cycle in
    the entire chip (∑ *n i*=1 *IPC<sup>i</sup>* , being *n* the number of threads),
    while the WS is defined as the slowdown experienced by each application in a mix,
    compared to its run under the same configuration when no other application is
    running on other cores (∑ *n i*=1 (*IPCshared i* /*IPCalone i* )). For the sake
    of simplicity and since in our context the WS does not constitute a metric as
    significant as IT, we do not show the WS results obtained. Anyway, these results
    follow an analogous trend to those obtained when we evaluate the instruction throughput.
    Figure [14](#page-13-0) illustrates the IT that each evaluated policy delivers
    normalized to the baseline.


    As shown, RD moderately outperforms DASCA. This is a key contribution of RD, since
    our approach, managing to reduce the amount of writes to the SLLC to a greater
    extent than DASCA, is also able to deliver higher performance (which also allows
    to report higher energy savings in both the SLLC and the main memory as shown
    later). The data reveal that, overall, RD improves performance by around 3% compared
    to the baseline, while DASCA just improves it by around 1.2%. Moreover, we can
    observe that, in almost all of the 28 mixes evaluated (except mainly those mixes
    made up of benchmarks with a reduced WPKI, those labeled as *low*, where the performance
    of both techniques essentially matches that of the baseline), our technique performs
    better. Zooming into particular mixes, the results reveal that RD performs especially
    better than DASCA in those mixes made up of write-intensive applications. Thus,
    our approach reports a performance improvement of more than 7% when considering
    just the H*<sup>i</sup>* mixes while DASCA just reports 1.7% IT improvement with
    respect to the baseline. Also, RD delivers significantly higher performance than
    DASCA and the baseline for those mixes which contain any application with high
    WPKI.


    # *5.2.3. Energy savings*


    Figure [15](#page-13-1) illustrates the energy savings in the SLLC. As in the
    single-core scenario, the graph follows a similar relative trend between our approach
    and DASCA to that observed in the write reduction numbers (Figure [13\)](#page-12-0),
    just slightly modulated with the performance numbers since, as shown in Figure
    [16,](#page-13-2) the dynamic contribution to the energy consumption in the SLLC
    is higher than the static part (except in the mixes made up of applications with
    low WPKI only), so that the ability to reduce the amount of writes to the SLLC
    (dynamic contribution) impacts the total energy consumption more than the ability
    to improve performance, which mainly affects the static contribution. Overall,
    our proposal reports around 37% energy reduction in the STT-RAM SLLC compared
    to the baseline while DASCA reduces it by around 31%. If we zoom into the write-intensive
    mixes, both RD and DASCA are able to save around 45% and 39% of SLLC energy consumption,
    respectively. If we break the SLLC energy numbers down into the static and dynamic
    contributions, our results reveal that, overall, RD is able to reduce –considering
    all mixes– the static energy part by around 2.7% with respect to the baseline
    (around 6% for the write-intensive mixes) while DASCA reduces the static contribution
    by 1.2% (1.7% for the *high* mixes). In addition, our approach reports dynamic
    energy savings of around 50% (51% for the *high* mixes) while DASCA numbers are
    42% (46% for the *high* mixes).


    Also, we explore the energy savings obtained in the DRAM main memory, where the
    leakage contribution has far greater significance than in the STT-RAM SLLC, so
    that the trends obtained essentially follow those of the IT graph, but inverted
    (higher performance translates into lower DRAM energy consumption). Figure [17](#page-14-0)
    illustrates that RD manages to additionally reduce the energy consumption of the
    main memory by around 6.2% on average compared to the baseline (8.3% for the write-intensive
    mixes), while DASCA barely reaches a 3.6% energy reduction (around 2% for the
    *high* mixes), mainly due to the higher performance improvement that our proposal
    exhibits.


    # *5.2.4. Discussion*


    For the sake of clarity, we next explain where the performance improvements of
    our technique come from. First, as Figure [13](#page-12-0) illustrated earlier,
    the write reductions to the SLLC that RD achieves are greater than those


    ![](_page_13_Figure_1.jpeg)


    <span id="page-13-0"></span>FIGURE 14. Instruction throughput normalized to the
    baseline in the 4-core CMP system.


    ![](_page_13_Figure_3.jpeg)


    <span id="page-13-1"></span>FIGURE 15. Energy consumption in the STT-RAM SLLC
    normalized to the baseline in the 4-core CMP system.


    ![](_page_13_Figure_5.jpeg)


    <span id="page-13-2"></span>FIGURE 16. Breakdown of energy consumption in the
    SLLC into the static and dynamic contributions for the baseline in the 4-core
    CMP system.


    of DASCA. Second, and more importantly, as Figure [14](#page-13-0) reveals, the
    bypasses dictated by RD translate into more performance than that of DASCA. As
    in the single-core scenario, the rationale behind that is related with the hit
    rate experimented in the SLLC with both schemes. Figure [18](#page-14-1) illustrates
    the number of hits in the SLLC per kilo instruction that each mix experiments
    normalized to the baseline.


    The results reveal that in most of the mixes evaluated the amount of hits in the
    SLLC is higher under our approach than using DASCA. Again, this is especially
    evident for the case of the mixes including write-intensive applications such
    as H1, H3 and HL2 where the number of hits is 2.87x, 2.45x and 1.37x those of
    the baseline, respectively. This is the key to explain our performance improvements:
    the efficient management of the SLLC contents by exploiting the reuse locality.
    In addition, there are other factors that also contribute to the throughput gain
    such as less write operations to the SLLC, less main memory accesses, and increased
    row buffer hit rates. In order to perform a deeper comparison between RD and DASCA,
    Table [7](#page-14-2) recaps the average values of different metrics involved
    in the performance delivered by RD and DASCA, normalized to those of the baseline.
    As shown, our scheme improves DASCA and the baseline (especially in the data from
    writeintensive mixes) in all the metrics considered.


    As in the single-core scenario, next we zoom into particular mixes that need further
    detail to get a better understanding. First, in some mixes such as H0, HM3 or
    HML0 we can observe that the DASCA scheme is able to reduce the amount of writes
    to the SLLC and also the energy consumption in the STT-RAM more than our scheme
    does (Figures [13](#page-12-0) and [15\)](#page-13-1). Conversely, the RD manages
    to deliver more throughput than DASCA (Figure [14\)](#page-13-0). However, these
    performance improvements our approach achieves are not enough to offset the higher
    energy savings in the SLLC that the DASCA scheme reports for these mixes as a
    consequence of the lower number of writes to the STT-


    ![](_page_14_Figure_0.jpeg)


    FIGURE 17. Energy consumption in the DRAM normalized to the baseline in the 4-core
    CMP system.


    <span id="page-14-0"></span>![](_page_14_Figure_2.jpeg)


    <span id="page-14-1"></span>FIGURE 18. Number of STT-RAM SLLC hits per kilo instruction
    normalized to the baseline in the 4-core CMP system.


    # RAM.


    Second, data for mix L2 reveal that the RD is able to reduce the amount of writes
    to the SLLC much more than DASCA with respect to the baseline (81% vs. 48%). However,
    this great difference translates into just 22% of energy savings in RD vs. 13%
    of DASCA. As shown, the difference between both policies has been significantly
    reduced due to the low contribution of the dynamic energy to the total energy
    consumption in the SLLC that this mix exhibits, as Figure [16](#page-13-2) illustrates.


    # *5.2.5. Sensitivity to Reuse Detector size*


    The RD size is a key design aspect of our proposal. In order to evaluate its impact
    we show in Figure [19](#page-15-1) the amount of writes to the SLLC, the Instruction
    Throughput, and the energy consumption in both the SLLC and the main memory for
    different RD sizes per core, namely 8K, 16K, 32K and 64K entries.


    As shown, the major impact is observed on the capability to reduce the number
    of writes in the SLLC, ranging from an average reduction of 63% with respect to
    the baseline when an 8K-entry RD per core is employed (60% for the write-intensive
    mixes) to a reduction of around 51% for a 64K-entry RD per core (50% for the *high
    mixes*). Note that maybe these data might appear contradictory at first sight.
    However, they are not: As the size of RD increases, it also augments the probability
    that a block finds its tag in the RD, so the probability of bypassing decreases,
    leading to minor reduction of writes to the SLLC. We can also observe a moderate
    impact on the average energy consumed in the SLLC, with values in the range 63-69%
    as the size of RD gets increased: again, note that these numbers follow a similar
    trend to that exhibited by the amount of writes. Finally, the impact over the
    performance and the energy consumption of the main memory is much reduced, falling
    the average IT variations into a small range of 1% (4% for the write-intensive
    mixes) and the average DRAM energy variations into a range of 1.5% (5% for the
    write-intensive mixes).


    | ❳<br>❳❳❳❳<br>Metrics | SLLC      | SLLC      | Row buffer Read | DRAM      |
    DRAM      | Bank contention in |

    |----------------------|-----------|-----------|-----------------|-----------|-----------|--------------------|

    | ❳❳❳❳❳<br>Policies    | Misses    | Hits      | Hit Rate        | reads     |
    Writes    | SLLC               |

    | DASCA (All/High)     | 1.01/1.05 | 1.04/1.10 | 1.03/1.00       | 1.01/1.05 |
    1.04/1.06 | 0.45/0.16          |

    | RD (All/High)        | 0.94/0.95 | 1.20/1.83 | 1.05/1.01       | 0.98/0.98 |
    0.96/0.94 | 0.29/0.08          |


    <span id="page-14-2"></span>TABLE 7. Average values of different metrics normalized
    to the baseline in the 4-core CMP system.


    # 16 RODRIGUEZ-RODRIGUEZ ET AL


    ![](_page_15_Figure_2.jpeg)


    <span id="page-15-1"></span>FIGURE 19. Writes to SLLC, IT and energy consumption
    in both SLLC and main memory normalized to the baseline for different RD sizes
    per core in the 4-core CMP system.


    # *5.2.6. Overhead analysis*


    In Section [3.2.3](#page-5-1) we outlined that an 8K-entry RD for a 1MB LLC requires
    an extra storage of 14KB, which represents a 1.37% overhead with respect to the
    LLC size. In this section we previously noted that for the 4-CMP system under
    evaluation (4MB SLLC) we employ an 8K-entry RD per core. The reason is that we
    are maintaining for each evaluated system the 1.37% overhead with respect the
    SLLC size. Therefore, in the 8-CMP evaluated later, we also employ an 8K-entry
    RD per core. Hence, the total extra storage (overhead) of RD is 56KB and 112KB
    for the 4- CMP and 8-CMP systems respectively, representing in all the cases a
    1.37% overhead with respect to the SLLC size.


    # *5.2.7. RD in a two-level cache hierarchy*


    We have evaluated the operation of our proposal in a threelevel cache hierarchy
    since most current processors employ this configuration. Furthermore, two private
    levels are more likely to filter better the temporal locality than using just
    one private level. However, for a fair comparison, we have also evaluated our
    proposal and the DASCA scheme in a configuration with just two cache levels. Notably,
    we reproduce the same configuration (4-CMP) used by the authors in [\[1\]](#page-20-0)
    when presenting the DASCA technique (32 KB IL1 and DL1 as private caches and a
    1MB per core shared L2 cache). Table [8](#page-16-0) illustrates the main results.


    As shown, RD maintains higher capability than DASCA (around 11% higher) in reducing
    the amount of writes to the SLLC. However, as expected, the amount of writes avoided
    (and also the hits experienced in the SLLC) is significantly lower than that exhibited
    in an scenario with 3 cache levels. Recall that this is due to the fact that with
    two cache levels only, most temporal locality has not been filtered, so that the
    reuse locality can not be fully exploitable. Also, as a consequence of this lower
    capability in cutting the write traffic to the SLLC, the energy savings achieved
    in the shared L2 are significantly lower than those obtained with three cache
    levels, although RD still reports better numbers than DASCA. Finally, RD again
    improves the Instruction Throughput to a greater extent than DASCA, and consequently
    also delivers higher energy savings in the main memory. Note that we have also
    evaluated 28 mixes in this configuration following the same criteria explained
    earlier, but they are not exactly the same as in the threelevel cache hierarchy
    experiments since the WPKI values that the benchmarks exhibit do not match those
    of the threelevel configuration and therefore some programs changed the category
    (high, medium or low) in which they were classified.


    # <span id="page-15-0"></span>5.3. Evaluation in an 8-core CMP system


    In this section we illustrate and analyze the main results obtained when using
    RD and DASCA in an 8-core CMP system with an 8MB SLLC. Like in the previous section,
    in this scenario we create 28 mixes following the same criteria as in a 4-CMP
    system. The mixes evaluated are shown in Table [5.](#page-8-0) Given that a detailed
    analysis of the 8-core system would show similar results as the 4-core scenario,
    in this section we will not zoom into details but will only describe the average
    results and main trends.


    # *5.3.1. Write filtering*


    Figure [20](#page-16-1) illustrates the number of writes to the STT-RAM SLLC generated
    with DASCA and with RD (assuming an 8K-entry RD per core). Both schemes are normalized
    to a baseline STT-RAM without any content selection mechanism.


    Similarly to the results for the 4-core scenario, the experimental results reveal
    that RD just performs 46% of the writes made in the baseline scheme, whereas DASCA
    produces 56% of the writes that the baseline did. For the write-intensive mixes,
    RD and DASCA reduce the amount of writes compared to the baseline in 44% and 35%
    respectively.


    # *5.3.2. Performance*


    As we did in Section [5.2.2,](#page-12-1) we employ the *Instruction Throughput*
    (IT) to evaluate the performance when executing multiprogrammed workloads. Figure
    [21](#page-16-2) illustrates the IT that each evaluated policy delivers normalized
    to the baseline.


    Similarly to the results obtained for a 4-core CMP system, RD outperforms DASCA
    in the 8-core scenario. Moreover, in the 8-core scenario, higher performance improvements
    are achieved in both schemes over the baseline. The


    | ❳<br>❳❳❳❳<br>Metrics  | Writes    | Instr.     | Energy consumpt. | Energy consumpt.
    | SLLC      |

    |-----------------------|-----------|------------|------------------|------------------|-----------|

    | ❳❳❳❳<br>Policies<br>❳ | SLLC      | Throughput | SLLC             | DRAM             |
    Hits      |

    | DASCA (All/High)      | 0.70/0.79 | 1.01/1.03  | 0.81/0.86        | 0.97/0.96        |
    0.98/1.00 |

    | RD (All/High)         | 0.59/0.76 | 1.03/1.04  | 0.75/0.83        | 0.95/0.95        |
    0.98/1.00 |


    TABLE 8. Average values of different metrics normalized to the baseline in a 4-core
    CMP system with two cache levels.


    <span id="page-16-0"></span>![](_page_16_Figure_3.jpeg)


    <span id="page-16-1"></span>FIGURE 20. Number of writes to the STT-RAM SLLC normalized
    to the baseline in the 8-core CMP system.


    results reveal that RD improves performance by around 7% compared to the baseline,
    while DASCA improves it by around 4%. As for write-intensive mixes, RD improves
    the baseline by 20% and DASCA by 11%. As shown in Figure [21,](#page-16-2) RD
    significantly overcomes DASCA and the baseline scheme in those mixes which contain
    any application with high WPKI.


    # *5.3.3. Energy savings*


    Figure [22](#page-17-1) illustrates the energy savings in the shared LLC. In general,
    the results in the 8-core scenario follow the trend observed for the 4-core environment.
    Specifically, RD reports around 32.5% energy reduction in the STT-RAM SLLC compared
    to the baseline while DASCA reduces energy by around 27%. In the case of write-intensive
    mixes, both RD and DASCA reduce the SLLC energy consumption by 34% and 27.5%,
    respectively. Analyzing the static and dynamic contributions on the SLLC energy
    consumption, overall, RD is able to reduce –for all mixes– the static energy part
    by around 6% with respect to the baseline (around 15% for the write-intensive
    mixes) while DASCA reduces the static contribution by 3.6% (9.5% for the *high*
    mixes). In addition, our approach reports dynamic energy savings of around 43%
    (36% for the *high* mixes) while DASCA numbers are 36% (30% for the *high* mixes).
    Note that mixes made up of applications with low WPKI exhibit the lowest energy
    savings across the board. This is consistent with the modest write reduction they
    report and especially with the high contribution of the static part to the total
    SLLC energy consumption that they exhibit, as Figure [23](#page-17-2) shows.


    Figure [24](#page-18-0) illustrates the energy savings obtained in the DRAM main
    memory, where it is shown that RD reduces the energy consumption of the main memory
    by around 6% on average compared to the baseline (3% for the write-intensive mixes),
    while DASCA reaches a 2.8% energy reduction and actually wastes more energy, around
    6%, for the *high* mixes.


    ![](_page_16_Figure_10.jpeg)


    <span id="page-16-2"></span>FIGURE 21. Instruction throughput normalized to the
    baseline in the 8-core CMP system.


    # 18 RODRIGUEZ-RODRIGUEZ ET AL


    ![](_page_17_Figure_2.jpeg)


    <span id="page-17-1"></span>FIGURE 22. Energy consumption in the STT-RAM SLLC
    normalized to the baseline in the 8-core CMP system.


    ![](_page_17_Figure_4.jpeg)


    <span id="page-17-2"></span>FIGURE 23. Breakdown of energy consumption in the
    SLLC into the static and dynamic contributions for the baseline in the 8-core
    CMP system.


    This energy waste may look surprising, given that DASCA is able to reduce the
    number of writes with respect to the baseline by 35% and to deliver a performance
    improvement higher than 10%. However, this can be explained by the fact that DASCA
    suffers a very significant increase in the amount of SLLC misses, which translates
    into high values of DRAM accesses (as shown in Table [9\)](#page-18-1).


    # *5.3.4. Discussion*


    As in the 4-core configuration, in this section we explain the reasons for the
    higher performance improvement achieved in our technique (RD) against DASCA in
    the 8-core scenario.


    As we already reasoned in the previous section, the better performance of RD is
    due to several factors, being the most important one the high efficiency achieved
    from the reuse locality exploitation. For demonstrating that fact, Figure [25](#page-18-2)
    shows the number of hits in the SLLC per kilo instruction that each mix experiments
    normalized to the baseline. As the figure shows, our approach achieves in most
    mixes a higher or much higher number of hits than DASCA, which confirms that RD
    uses a more efficient policy than DASCA.


    In addition to the hit rate improvement, there are other metrics that also justify
    achieving a better performance, such as SLLC misses, DRAM reads and writes, row
    buffer read hit rate and bank contention in the SLLC. All these metrics are shown
    in Table [9,](#page-18-1) for both RD and DASCA and also for both all and write-intensive
    mixes. Note that the RD beats DASCA in all the metrics considered.


    # *5.3.5. Sensitivity to Reuse Detector size*


    Given that the RD size is a determining factor in our proposal, and as done in
    the 4-CMP system, in Figure [26](#page-18-3) we show the amount of writes to the
    SLLC, the Instruction Throughput, and the energy consumption in both the SLLC
    and the main memory for different RD sizes per core, namely 8K, 16K, 32K and 64K
    entries.


    The trends are very similar to those observed in the 4 core scenario. Notably,
    a significant impact is observed on the capability to reduce the number of writes
    in the SLLC, especially for the All mixes, whereas a moderate (or even negligible
    in some cases) impact is seen on the average energy consumed in that cache level
    or main memory and performance of the overall system.


    # <span id="page-17-0"></span>5.4. RD performance in multi-core scenarios


    In this section we briefly inspect the main hints about the performance of RD
    when we augment the number of cores. So far, we have evaluated the RD in systems
    with one, four and eight cores. In order to further explore the RD behavior we
    have also evaluated our proposal in a system with 16 cores, employing the mixes
    shown in Table [6.](#page-8-1)


    In Table [10](#page-19-0) we recap the main numbers derived from RD and DASCA
    evaluation across the different configurations (for the sake of simplicity, we
    show just the numbers considering all applications or mixes, not those corresponding
    to the write-intensive ones). Notably, we illustrate the average LLC write reduction
    capability, energy savings in the LLC, the performance delivered and the energy
    savings in the DRAM.


    As shown, the write reduction capability in percentage terms gets reduced with
    the number of cores for both RD and DASCA. However, despite this decrease in the
    write filtering numbers, the most important consequence derived from this aspect,
    i.e. the net energy savings in the LLC, essentially remains in the range of 30-37%
    for RD, slightly decreasing


    ![](_page_18_Figure_0.jpeg)


    FIGURE 24. Energy consumption in the DRAM normalized to the baseline in the 8-core
    CMP system.


    <span id="page-18-0"></span>![](_page_18_Figure_2.jpeg)


    ![](_page_18_Figure_3.jpeg)


    <span id="page-18-2"></span>![](_page_18_Figure_4.jpeg)


    <span id="page-18-3"></span>FIGURE 26. Writes to SLLC, IT and energy consumption
    in both SLLC and main memory normalized to the baseline for different RD sizes
    per core in the 8-core CMP system.


    with the number of cores as a consequence of the increment in the number of writes,
    but keeping the benefit with respect to DASCA largely unchanged. Moreover, the
    performance improvement increases as the number of cores augments for both RD
    and DASCA, reporting significant numbers especially for the 8-core and 16-core
    system. Indeed, the


    | ❳<br>❳❳❳❳<br>Metrics | SLLC      | SLLC      | Row buffer Read | DRAM      |
    DRAM      | Bank contention in |

    |----------------------|-----------|-----------|-----------------|-----------|-----------|--------------------|

    | ❳❳❳❳❳<br>Policies    | Misses    | Hits      | Hit Rate        | reads     |
    Writes    | SLLC               |

    | DASCA (All/High)     | 1.08/1.30 | 0.92/0.84 | 1.00/0.99       | 1.08/1.30 |
    1.09/1.21 | 0.40/0.13          |

    | RD (All/High)        | 0.98/1.00 | 1.05/1.01 | 1.02/1.04       | 1.00/1.06 |
    1.02/1.05 | 0.24/0.07          |


    REUSE DETECTOR: IMPROVING THE MANAGEMENT OF STT-RAM SLLCS 19


    <span id="page-18-1"></span>TABLE 9. Average values of different metrics normalized
    to the baseline in the 8-core CMP system.


    20 RODRIGUEZ-RODRIGUEZ ET AL


    | ❵<br>❵❵❵❵<br>Metrics<br>❵❵❵<br>Scenario | LLC Write<br>reduction (%)<br>(DASCA/RD)
    | LLC Energy<br>Savings (%)<br>(DASCA/RD) | Performance<br>improvement (%)<br>(DASCA/RD)
    | DRAM Energy<br>Savings (%)<br>(DASCA/RD) |

    |-----------------------------------------|------------------------------------------|-----------------------------------------|----------------------------------------------|------------------------------------------|

    | Single core                             | 51.8 / 65.2                              |
    29.5 / 34.5                             | 0.3 / 1.9                                    |
    0.2 / 2.0                                |

    | 4-core CMP                              | 51.6 / 62.7                              |
    30.8 / 37.0                             | 1.2 / 2.9                                    |
    3.6 / 6.2                                |

    | 8-core CMP                              | 43.8 / 54.0                              |
    27.3 / 32.5                             | 3.7 / 6.7                                    |
    2.8 / 6.0                                |

    | 16-core CMP                             | 38.6 / 46.7                              |
    26.0 / 30.5                             | 10.2 / 14.5                                  |
    4.0 / 7.9                                |


    <span id="page-19-0"></span>TABLE 10. Average values of different metrics normalized
    to the baseline across different configurations.


    performance improvement that RD reports with respect to the DASCA scheme also
    increases with the number of cores (which is especially significant in the case
    of write-intensive mixes for the 16-CMP system, where RD is able to increase the
    performance of the baseline by around 38%, whereas DASCA only achieves 24% of
    improvement). This is due to the fact that the difference in LLC misses reported
    by both approaches increases as the number of cores augments (notably, these numbers
    are 1%, 6%, 10% and 13% for 1, 4, 8 and 16-core systems respectively, always reporting
    RD the best value) and also the difference in other factors as the number of hits
    in the LLC, the number of DRAM reads and writes or the row buffer hit rate also
    increase, benefiting RD more with the number of cores. Finally, and derived from
    the higher performance delivered, the same trend is observed in the energy savings
    experienced in the main memory (where it is worthy to note that, in the 16-core
    scenario, RD manages to reduce the energy consumption of the baseline by more
    than 15% for the write-intensive mixes, while DASCA hardly achieves 8%). Overall
    we can conclude that, as the number of cores increases, RD is able to significantly
    increase the performance delivered (and also to increase the energy savings achieved
    in the main memory) at the expense of a slight decrease in the LLC energy savings
    obtained.


    # 6. RELATED WORK


    To address the problems of energy consumption and performance of STT-RAM SLLCs,
    in the last years different researchers have proposed solutions aiming to reduce
    either the amount of writes or the per-write energy.


    A great body of work mainly tries to cut the write traffic to the STT-RAM: In
    [\[14\]](#page-21-9) the authors propose an obstructionaware cache management
    policy called OAP. OAP monitors the cache to periodically detect LLC-obstruction
    processes, and manage the cache accesses from different processes, so that when
    an LLC-obstruction is detected the data is forwarded to the next cache level or
    Main Memory as appropriate. In [\[15\]](#page-21-10) two techniques are proposed
    to reduce the number of writes to a last level (L2) STT-RAM cache and also save
    energy. The first one adds a small cache between L1 and L2 –called write-cache
    (WC)– which is mutually exclusive with L2 and stores only the dirty lines evicted
    from L1. On a cache access, both L2 and WC are accessed in parallel. The write
    misses are allocated in WC and the load misses are allocated in L2. WC reduces
    the number of L2 writes by absorbing most of the L1 writebacks. Other authors
    propose a coding scheme for STT-RAM last level cache based on the concept of value
    locality. They reduce switching probability in cache by swapping common patterns
    with limited weight codes to make writes less often as well as more uniform [\[16\]](#page-21-11).
    Other techniques [\[17\]](#page-21-12) rely on the observation that on average,
    a large fraction of bytes and words written to the L2 cache are only zero-valued
    data. Based on this, this technique adds additional "all-zero-data" flags in the
    tag arrays at the granularity of a single byte and a single word. Before any cache
    write, the data value is checked. If the all-zero bytes or words are detected,
    the corresponding flags are set and only the non-zero bytes or words are written.
    During a cache read operation, only the non-zero bytes or words are read and then
    the actual data are constructed by combining the information from the allzero
    flags. Another proposal [\[18\]](#page-21-13) logically divides the STT-RAM cache
    line into multiple partial lines. In L1 cache, a history bit is kept for each
    partial line to track which partial lines have changed. Using this information,
    when a dirty L1 block is written to last level cache, only those partial lines
    which have been changed are written. Other authors propose techniques for mitigating
    the write pressure caused due to prefetching in STT-RAM based LLC [\[19\]](#page-21-14).
    One of these techniques prioritizes different types of LLC requests such as load,
    store, prefetch, or write back, etc. based on their criticality. The critical
    requests are assigned a high priority and hence, they are served earlier. In multicore
    systems, the excessive requests generated from a cache-intensive program may block
    those generated from a cache-unintensive program which may lead to its starvation.
    To address this, they propose another technique which prioritizes the requests
    from a cache-unintensive program, so that they are served promptly. Also, authors
    in [\[9\]](#page-21-4) analyze the cache coherence protocols impact in the number
    of write to a LLC based on STT-RAM, showing that the protocols with a owned state
    (MOESI and MOSI) reduce the number of writes to LLC.


    Another body of work mainly deals with performance of STT-RAM caches: In [\[20\]](#page-21-15)
    a cache revive technique to calculate retention time is proposed. Some cache blocks
    retain data even after completion of retention time. The retention time is chosen
    so that it will minimize the number of unrefreshed cache blocks. Other authors
    propose the use of STT-RAM to design combinational logic, register files and on-chip
    storage (I/D L1 caches, TLBs and L2 cache) [\[21\]](#page-21-16). Also, to hide
    the write latency of STT-RAM, they propose subbank buffering which allows the
    writes to complete locally within each sub-bank, while the reads from


    other locations within the array can complete unobstructed. They show that by
    carefully designing the pipeline, the STT-RAM based design can significantly reduce
    the leakage power, while also maintaining the performance level close to the CMOS
    design. Also, an STT-RAM cache design for lower level caches where different cache
    ways are designed with different retention periods is proposed in [\[22\]](#page-21-17).
    For example, in a 16-way cache, way 0 is designed with a fast STT-RAM design with
    low retention period and the remaining 15 ways are designed with a slow STT-RAM
    design which has higher retention period. Their technique uses hardware to detect
    whether a block is read or write intensive. The write intensive blocks are primarily
    allocated to way 0, while the read intensive blocks are allocated to the other
    ways. Also, to avoid refreshing dying blocks in way 0, their technique uses data
    migration to move such blocks to banks with higher retention period. Finally,
    a write-buffer design to address the long write latency of last level (L2) STT-RAM
    cache is proposed in [\[23\]](#page-21-18). The L2 may receive a request from
    both L1 and the write buffer. Since read latency of STT-RAM is smaller than the
    write latency and also reads are performance-critical, the buffer uses a readpreemptive
    management policy, which ensures that a read request receives higher priority
    than a write request. The authors also propose a hybrid SRAM and STT-RAM cache
    design which aims to move the most write-intensive blocks to SRAM.


    It is worth mentioning that we compare our RD scheme just against DASCA due to
    three reasons: 1) DASCA, which is a more recent proposal than all the aforementioned
    works, is the closest work to ours in the sense that both schemes try to reduce
    the energy consumption of an STT-RAM LLC by bypassing write operations predicted
    to be dead (or not showing reuse), 2) some of the mentioned approaches are already
    evaluated in the DASCA paper, such as [\[14\]](#page-21-9), being clearly outperformed
    by the DASCA mechanism, and 3) some other schemes, such as [\[16,](#page-21-11)
    [17,](#page-21-12) [18,](#page-21-13) [19,](#page-21-14) [20,](#page-21-15) [22,](#page-21-17)
    [23\]](#page-21-18), although addressing the same problem, are completely orthogonal
    to our approach. Thus, the RD could be built on top of them, making a direct comparison
    meaningless. As an example, in [\[16\]](#page-21-11) the authors address the STT-RAM
    write energy problem at a circuit level, trying to reduce the number of writes
    and also to balance the wear among the cells. The proposal relies in identifying
    the most frequent values stored in the LLC (value locality) and encoding these
    patterns to reduce the number of writes. Thus, unlike our proposal, this approach
    operates at a bit-level.


    # 7. CONCLUSIONS


    In this paper we have addressed the main constraints of conventional SRAM last-level
    caches: power-hungry operation and inefficient management. In order to overcome
    these drawbacks we propose to employ a STT-RAM SLLC where its contents are selected
    according to a *Reuse Detector* which exploits the reuse locality of the stream
    of references arriving at the SLLC. The Reuse Detector is a hardware component
    that tracks block reuse and determines,


    according to its predicted future utility, if they must be inserted in the SLLC
    or bypassed to the main memory. The Reuse Detector succeeds in managing the STT-RAM
    SLLC contents in two complementary ways. First, it is able to bypass to main memory
    a significant fraction of the blocks coming to the SLLC, thus decreasing the amount
    of the energy-hungry writes to be performed. Second, it increases significantly
    the SLLC hit rate, which leads to moderate performance improvements. In addition,
    the energy consumption in the main memory is also reduced. This way, our approach
    is able to outperform other strategies also oriented to decrease the energy consumption
    in STT-RAM SLLCs, such as the DASCA scheme. Although DASCA exhibits slightly lower
    ability to cut the write operations to the SLLCs, this technique, which predicts
    if a block will not be reused again instead of predicting if a block is going
    to be reused as ours, achieves lower accuracy in the prediction, hence also significantly
    lower hit rates at this cache level and therefore much lower performance improvements.
    Overall RD reports on average energy reductions in the SLLC in the range of 37-30%,
    additional energy savings in the main memory in the range of 6-8% and performance
    improvements of 3% (quadcore), 7% (eight-core) and 14% (16-core) compared to an
    STT-RAM SLLC baseline where no reuse detector is employed. More importantly, our
    approach outperforms DASCA, the state-of-the-art STT-RAM SLLC management, reporting
    –depending on the specific scenario and the kind of applications used– SLLC energy
    savings in the range of 4-11% higher than those of DASCA, delivering higher performance
    in the range of 1.5-14%, and an additional improvement in DRAM energy consumption
    in the range of 2-9% higher than DASCA.


    # ACKNOWLEDGEMENTS


    This work has been supported in part by the Spanish government through the research
    contracts TIN2012-32180, TIN2015-65277-R, TIN2015-65316-P and by the HIPEAC-4
    European Network of Excellence. It has been also supported by a grant scholarship
    from the University of Costa Rica and the Costa Rican Ministry of Science and
    Technology MICIT and CONICIT.


    # <span id="page-20-0"></span>REFERENCES


    - [1] Ahn, J., Yoo, S., and Choi, K. (2014) Dasca: Dead write prediction assisted
    stt-ram cache architecture. *IEEE 20th International Symposium on High Performance
    Computer Architecture (HPCA), 2014*, pp. 25–36. IEEE.

    - <span id="page-20-1"></span>[2] Khan, S. M., Tian, Y., and Jimenez, D. A. (2010)
    Sampling dead block prediction for last-level caches. *Proceedings of the 2010
    43rd Annual IEEE/ACM International Symposium on Microarchitecture*, Washington,
    DC, USA MICRO ''43, pp. 175–186. IEEE Computer Society.

    - <span id="page-20-2"></span>[3] Albericio, J., Ibáñez, P., Viñals, V., and Llabería,
    J. M. (2013) Exploiting reuse locality on inclusive shared last-level caches.
    *ACM Trans. Archit. Code Optim.*, 9, 38:1–38:19.

    - <span id="page-20-3"></span>[4] Albericio, J., Ibáñez, P., Viñals, V., and Llabería,
    J. M. (2013) The reuse cache: Downsizing the shared last-level cache.


    *Proceedings of the 46th Annual IEEE/ACM International Symposium on Microarchitecture*,
    New York, NY, USA MICRO-46, pp. 310–321. ACM.


    - <span id="page-21-1"></span><span id="page-21-0"></span>[5] (2013). http://www.hpl.hp.com/research/cacti/.

    - [6] Dong, X. et al. (2012) NVSim: A circuit-level performance, energy, and area
    model for emerging nonvolatile memory. *IEEE Transaction on Compter-Aided Design
    of Integrated Circuits and Systems*, 31, 994–1007.

    - <span id="page-21-2"></span>[7] Díaz, J., Monreal, T., Viñals, V., Ibáñez, P.,
    and Llabería, J. M. (2015) Selección de contenidos basada en reuso para caches
    compartidas en exclusión. *Proceedings of the XXVI Jornadas de Paralelismo* JP-2015,
    pp. 433–442.

    - <span id="page-21-4"></span><span id="page-21-3"></span>[8] Binkert, N. et al.
    (2011) The gem5 simulator. *ACM SIGARCH Computer Architecture News*, 39, 1.

    - [9] Chang, M.-T., Lu, S.-L., and Jacob, B. (2014) Impact of cache coherence
    protocols on the power consumption of stt-rambased llc. *The Memory Forum Workshop*.

    - <span id="page-21-5"></span>[10] Agarwal, N., Krishna, T., Peh, L.-S., and Jha,
    N. K. (2009) Garnet: A detailed on-chip network model inside a fullsystem simulator.
    *Performance Analysis of Systems and Software, 2009. ISPASS 2009. IEEE International
    Symposium on*, pp. 33–42. IEEE.

    - <span id="page-21-6"></span>[11] Rosenfeld, P., Cooper-Balis, E., and Jacob,
    B. (2011) Dramsim2: A cycle accurate memory system simulator. *Computer Architecture
    Letters*, 10, 16 –19.

    - <span id="page-21-8"></span><span id="page-21-7"></span>[12] (2013). http://www.spec.org/cpu2006/.

    - [13] Patil, H., Cohn, R. S., Charney, M., Kapoor, R., Sun, A., and Karunanidhi,
    A. (2004) Pinpointing representative portions of large intel® itanium® programs
    with dynamic instrumentation. *MICRO*, pp. 81–92. IEEE Computer Society.

    - <span id="page-21-9"></span>[14] Wang, J., Dong, X., and Xie, Y. (2013) Oap:
    an obstructionaware cache management policy for stt-ram last-level caches. *Proceedings
    of the Conference on Design, Automation and Test in Europe*, pp. 847–852. EDA
    Consortium.

    - <span id="page-21-10"></span>[15] Rasquinha, M. (2011) An energy efficient cache
    design using spin torque transfer (STT) RAM. Master of science in the school of
    electrical and computer engineering Georgia Institute of Technology.

    - <span id="page-21-11"></span>[16] Yazdanshenas, S., Ranjbar Pirbast, M., Fazeli,
    M., and Patooghy, A. (2014) Coding last level stt-ram cache for high endurance
    and low power. *IEEE Computer Architecture Letters*, 13, 73–76.

    - <span id="page-21-12"></span>[17] Jung, J., Nakata, Y., Yoshimoto, M., and Kawaguchi,
    H. (2013) Energy-efficient spin-transfer torque ram cache exploiting additional
    all-zero-data flags. *Quality Electronic Design (ISQED), 2013 14th International
    Symposium on*, pp. 216–222. IEEE.

    - <span id="page-21-13"></span>[18] Park, S. P., Gupta, S., Mojumder, N., Raghunathan,
    A., and Roy, K. (2012) Future cache design using stt mrams for improved energy
    efficiency: devices, circuits and architecture. *Proceedings of the 49th Annual
    Design Automation Conference*, pp. 492–497. ACM.

    - <span id="page-21-14"></span>[19] Mao, M., Li, H. H., Jones, A. K., and Chen,
    Y. (2013) Coordinating prefetching and stt-ram based last-level cache management
    for multicore systems. *Proceedings of the 23rd ACM international conference on
    Great lakes symposium on VLSI*, pp. 55–60. ACM.

    - <span id="page-21-15"></span>[20] Jog, A., Mishra, A. K., Xu, C., Xie, Y., Narayanan,
    V., Iyer, R., and Das, C. R. (2012) Cache revive: architecting volatile stt-ram
    caches for enhanced performance in cmps. *Proceedings of the 49th Annual Design
    Automation Conference*, pp. 243–252. ACM.

    - <span id="page-21-16"></span>[21] Guo, X., Ipek, E., and Soyata, T. (2010) Resistive
    computation: avoiding the power wall with low-leakage, stt-mram based computing.
    *ACM SIGARCH Computer Architecture News*, pp. 371–382. ACM.

    - <span id="page-21-17"></span>[22] Sun, Z., Bi, X., Li, H. H., Wong, W.-F., Ong,
    Z.-L., Zhu, X., and Wu, W. (2011) Multi retention level stt-ram cache designs
    with a dynamic refresh scheme. *Proceedings of the 44th Annual IEEE/ACM International
    Symposium on Microarchitecture*, pp. 329–338. ACM.

    - <span id="page-21-18"></span>[23] Sun, G., Dong, X., Xie, Y., Li, J., and Chen,
    Y. (2009) A novel architecture of the 3d stacked mram l2 cache for cmps. *High
    Performance Computer Architecture, 2009. HPCA 2009. IEEE 15th International Symposium
    on*, pp. 239–249. IEEE.


    This figure "CeldaSTTRAM.jpeg" is available in "jpeg" format from:


    This figure "CeldaSTTRAM.png" is available in "png" format from:


    This figure "STTRAMCircuit.jpeg" is available in "jpeg" format from:


    This figure "STTRAMCircuit.png" is available in "png" format from:'
  decisions:
    evaluation_prompt: 'Qualified. Reason: The paper includes an experimental framework
      and results section, as well as tables showing quantitative data.'
    related_work_prompt: 'Qualified. Reason: The paper includes a "Related Work" section
      and engages with prior research by citing and comparing to other works.'
    novelty_prompt: 'Qualified. Reason: The paper proposes a new mechanism for content
      selection in last-level caches, introducing a "Reuse Detector" to improve efficiency,
      which indicates novelty.'
    review_only_prompt: Qualified.
- title: "Cocco: Hardware-Mapping Co-Exploration towards Memory\n  Capacity-Communication\
    \ Optimization"
  abstract: "Memory is a critical design consideration in current data-intensive DNN\n\
    accelerators, as it profoundly determines energy consumption, bandwidth\nrequirements,\
    \ and area costs. As DNN structures become more complex, a larger\non-chip memory\
    \ capacity is required to reduce data movement overhead, but at\nthe expense of\
    \ silicon costs. Some previous works have proposed memory-oriented\noptimizations,\
    \ such as different data reuse and layer fusion schemes. However,\nthese methods\
    \ are not general and potent enough to cope with various graph\nstructures.\n\
    \  In this paper, we explore the intrinsic connection between network structures\n\
    and memory features to optimize both hardware and mapping. First, we introduce\n\
    a graph-level execution scheme with a corresponding dataflow and memory\nmanagement\
    \ method. This scheme enables the execution of arbitrary graph\npatterns with\
    \ high data reuse and low hardware overhead. Subsequently, we\npropose Cocco,\
    \ a hardware-mapping co-exploration framework leveraging\ngraph-level features\
    \ of networks. It aims to minimize communication overhead,\nsuch as energy consumption\
    \ and bandwidth requirements, with a smaller memory\ncapacity. We formulate the\
    \ graph-partition scheduling and memory configuration\nsearch as an optimization\
    \ problem and employ a genetic-based method to achieve\nefficient co-exploration\
    \ for large and irregular networks. Experiments\ndemonstrate that Cocco obtains\
    \ lower external memory access, lower bandwidth\nrequirements, and more stable\
    \ optimization for graph partition compared to the\ngreedy algorithm and dynamic\
    \ programming introduced in prior works. Cocco also\nreduces the costs by 1.89%\
    \ to 50.33% using co-exploration compared to other\ntypical methods."
  url: http://arxiv.org/abs/2402.00629v1
  keywords: ''
  document: '# Cocco: Hardware-Mapping Co-Exploration towards Memory Capacity-Communication
    Optimization


    Zhanhong Tan tanzh19@mails.tsinghua.edu.cn IIIS, Tsinghua University Beijing,
    China


    Zijian Zhu zhuzj23@mails.tsinghua.edu.cn IIIS, Tsinghua University Beijing, China


    Kaisheng Ma<sup>∗</sup> kaisheng@mail.tsinghua.edu.cn IIIS, Tsinghua University
    Beijing, China


    # Abstract


    Memory is a critical design consideration in current dataintensive DNN accelerators,
    as it profoundly determines energy consumption, bandwidth requirements, and area
    costs. As DNN structures become more complex, a larger on-chip memory capacity
    is required to reduce data movement overhead, but at the expense of silicon costs.
    Some previous works have proposed memory-oriented optimizations, such as different
    data reuse and layer fusion schemes. However, these methods are not general and
    potent enough to cope with various graph structures.


    In this paper, we explore the intrinsic connection between network structures
    and memory features to optimize both hardware and mapping. First, we introduce
    a graph-level execution scheme with a corresponding dataflow and memory management
    method. This scheme enables the execution of arbitrary graph patterns with high
    data reuse and low hardware overhead. Subsequently, we propose Cocco, a hardware-mapping
    co-exploration framework leveraging graph-level features of networks. It aims
    to minimize communication overhead, such as energy consumption and bandwidth requirements,
    with a smaller memory capacity. We formulate the graph-partition scheduling and
    memory configuration search as an optimization problem and employ a genetic-based
    method to achieve efficient co-exploration for large and irregular networks. Experiments
    demonstrate that Cocco obtains lower external memory access, lower bandwidth requirements,
    and more stable optimization for graph partition compared to the greedy algorithm
    and dynamic programming introduced in prior works. Cocco also reduces the costs
    by 1.89% to 50.33% using co-exploration compared to other typical methods.


    ASPLOS ''24, April 27-May 1, 2024, La Jolla, CA, USA © 2024 Copyright held by
    the owner/author(s). ACM ISBN 979-8-4007-0372-0/24/04. <https://doi.org/10.1145/3617232.3624865>


    CCS Concepts: • Hardware → Design reuse and communication-based design; On-chip
    resource management; • Computer systems organization → Parallel architectures;
    • Software and its engineering → Compilers.


    Keywords: Design space exploration, Memory, Graph analysis, Subgraph, Genetic
    algorithm, Deep learning accelerator


    #### ACM Reference Format:


    Zhanhong Tan, Zijian Zhu, and Kaisheng Ma. 2024. Cocco: Hardware-Mapping Co-Exploration
    towards Memory Capacity-Communication Optimization. In 29th ACM International
    Conference on Architectural Support for Programming Languages and Operating Systems,
    Volume 1 (ASPLOS ''24), April 27-May 1, 2024, La Jolla, CA, USA. ACM, New York,
    NY, USA, [16](#page-15-0) pages. <https://doi.org/10.1145/3617232.3624865>


    ## <span id="page-0-0"></span>1 Introduction


    The evolution of neural network topology has driven the remarkable progress of
    artificial intelligence from the early single-layer perceptron (SLP) [\[45,](#page-14-0)
    [54\]](#page-14-1) and multi-layer perceptron (MLP) [\[17,](#page-13-0) [22,](#page-13-1)
    [39\]](#page-13-2) to modern DNNs with plain [\[36,](#page-13-3) [57\]](#page-14-2)/inception
    [\[59\]](#page-14-3)/residual [\[20,](#page-13-4) [55\]](#page-14-4) structures
    based on manual design, and even irregular structures using neural architecture
    search (NAS) [\[53,](#page-14-5) [75\]](#page-15-1) or random network generation
    [\[68\]](#page-14-6). These technological innovations have resulted in increasingly
    complex computation graphs, which pose challenges for efficient memory design
    and deployment.


    Memory design is crucial in the accelerator system, as it performs data preparation
    at the start of each processing stage according to the scheduling scheme, determining
    energy consumption, bandwidth requirements, and area costs. Figure [1](#page-1-0)
    shows the trade-off between the on-chip memory size and the external memory access
    in DNN accelerators. A smaller on-chip buffer (left side) saves area but requires
    more data reloading. A larger buffer (right side) can reduce external memory access
    and save energy and bandwidth but at the cost of increasing the memory overhead.
    An excessively large SRAM may not be feasible due to the high silicon area cost,
    typically ranging from 1 to 2 mm<sup>2</sup> /MB in 12nm, and the high energy
    overhead, dozens of times that of a MAC operation for a large SRAM.


    Therefore, the key problem is: between the two extremes in Figure [1,](#page-1-0)
    how to find an appropriate memory configuration with efficient workload mapping
    and data management, especially under the growing complexity of neural network
    architectures.


    <sup>∗</sup>Corresponding author.


    Permission to make digital or hard copies of part or all of this work for personal
    or classroom use is granted without fee provided that copies are not made or distributed
    for profit or commercial advantage and that copies bear this notice and the full
    citation on the first page. Copyrights for thirdparty components of this work
    must be honored. For all other uses, contact the owner/author(s).


    ASPLOS ''24, April 27-May 1, 2024, La Jolla, CA, USA Zhanhong Tan, Zijian Zhu,
    and Kaisheng Ma


    <span id="page-1-0"></span>![](_page_1_Figure_1.jpeg)


    Figure 1. The effect of different memory capacities for a computation graph. Intermediate
    results can be buffered in the on-chip memory if it is large enough. The on-chip
    memory of small capacity can only buffer two nodes (marked in the red dotted box),
    and the larger memory can cover a larger subgraph (right side).


    The critical status of memory design has attracted extensive research. Most previous
    studies focus on simple layer-level optimization (the left one of Figure [1\)](#page-1-0)
    by applying loop transformation techniques such as tiling and reordering to fit
    the memory size and reuse the on-chip data [\[23,](#page-13-5) [43,](#page-14-7)
    [44,](#page-14-8) [61,](#page-14-9) [70\]](#page-15-2). In addition, several works
    also guide the memory capacity and hierarchy design using designspace exploration
    [\[12,](#page-12-0) [32,](#page-13-6) [37,](#page-13-7) [66,](#page-14-10) [67\]](#page-14-11).
    However, these layerlevel optimizations are confined to the limited intra-layer
    reuse, which is insufficient for memory-intensive networks. A subgraph-level scheme
    (e.g., the middle one and the right one of Figure [1\)](#page-1-0) provides a
    larger optimization space via inter-layer reuse [\[3,](#page-12-1) [4,](#page-12-2)
    [38,](#page-13-8) [73\]](#page-15-3) to reduce the I/O overhead. Therefore, this
    paper aims to leverage the subgraph-level computing flow to optimize the memory
    capacity and external communication for networks with any topology.


    However, there are three primary challenges to fully exploit the subgraph-level
    optimization.


    First, we need a general execution flow for any sub-graph. Due to the various
    kernel sizes and strides, a parent node in a subgraph may have unbalanced data
    requirements from its consumers, which makes it difficult to determine the tensor
    tiling scheme and the memory allocation for each node (layer). In the traditional
    single-layer execution, we usually divide a large tensor into loop tiles, which
    are processed through a series of regular computing steps. Similarly, we want
    the sub-graph execution to be a series of elementary computing steps with a simple
    control flow.


    Second, we require a suitable memory management method for the subgraph execution.
    Due to complicated dependency among nodes in a subgraph, careful management is
    needed to reuse overlapping and inter-layer intermediate data.


    Solving these two challenges contributes to a basic hardware execution model compatible
    with subgraph-level optimization. However, we also encounter the third challenge:
    how to partition a model into subgraphs and how much memory to allocate. The optimization
    space is huge, so we need to devise a search method with high sampling efficiency
    to find a proper subgraph partition and memory configuration result.


    In this paper, we first introduce a complete graph-level scheme for memory. In
    particular, it contains a consumptioncentric flow that enables the execution of
    arbitrary subgraphs with low memory footprints (for challenge 1). Accordingly,
    we provide an explicit memory dataflow and the corresponding memory management
    scheme for effective data reuse (for challenge 2). Building on the graph-level
    memory scheme, we propose Cocco, a hardware-mapping co-exploration framework,
    to establish a connection between model features and the memory configuration
    (for challenge 3).


    Cocco aims to find a combination of on-chip buffers and the corresponding graph-level
    scheduling for lower memory and communication overhead. In particular, we develop
    a genetic-based algorithm to efficiently explore the search space of graph partitions
    and the associated memory configuration for a series of neural networks.


    In summary, this work makes the following contributions:


    - Subgraph execution scheme. We first introduce a consumption-centric flow to
    determine a low-cost execution sequence by throttling and aligning the dataflow.

    - Efficient dataflow and memory management for subgraph data reuse. We propose
    a memory management scheme featuring multiple reconfigurable regions and the corresponding
    dataflow to support arbitrary subgraph execution with full data reuse.

    - Hardware-mapping co-exploration framework. Based on the subgraph execution scheme
    and memory dataflow, we propose Cocco, a genetic-based framework combining the
    graph-level partition and memory design-space exploration together. Cocco achieves
    1.89% to 50.33% lower costs (lower communication with a smaller size) using co-exploration
    in contrast to other methods.


    ## 2 Background and Motivation


    #### 2.1 Design of Neural Network Accelerators


    The DNN accelerator unit is the most basic execution unit in a computing system,
    on top of which, we can scale it out to many-core, many-socket, and many-drawer
    systems [\[24,](#page-13-9) [40,](#page-13-10) [48,](#page-14-12) [60\]](#page-14-13).
    An accelerator unit usually employs a processing element (PE) array on a sophisticated
    interconnection network to enable efficient tensor-level computation. Each PE
    typically contains local scratchpads and ALUs to process basic data packets. The
    global buffer and the weight buffer store activations and weights, and they are
    generally


    <span id="page-2-0"></span>![](_page_2_Figure_1.jpeg)


    *\* Those designs only support INT8 precision for tensor, we scale to FP16 performance
    by a factor of 0.5. \*\* For most designs fabricated under 12nm (or close to)
    process, we align all areas to 12nm. The SRAM area is estimated as 1.2mm<sup>2</sup>/MB.*


    Figure 2. Left: performance v.s. memory capacity of several industrial NPUs. Right:
    a summary of SRAM area ratio in these accelerators.


    located next to the PE array to serve as the data interface and manage data between
    the PE array and the external memory (e.g., DRAM or other cores). Due to the limited
    capacity of the global buffer, the compiler has to partition the network execution
    into a series of elementary workloads that are scheduled along the parallel spatial
    resources and the temporal dimension [\[18,](#page-13-11) [61,](#page-14-9) [72\]](#page-15-4).
    The capacity of the global buffer usually dominates the external memory access
    and bandwidth requirements, significantly impacting system performance. If the
    global memory is larger, it is more likely to buffer more intermediate data and
    avoid data being evicted to DRAM. As shown in Figure [1,](#page-1-0) a larger
    buffer expands the scope of elementary workloads from a single layer to a larger
    subgraph, reducing the communication overhead.


    However, choosing an appropriate memory specification is always a challenge. In
    Figure [2,](#page-2-0) we surveyed 16 popular industrial neural network processors
    with various memory/performance/area characteristics, where nine of them target
    the training domain [\[6,](#page-12-3) [11,](#page-12-4) [24,](#page-13-9) [34,](#page-13-12)
    [35,](#page-13-13) [40,](#page-13-10) [41,](#page-13-14) [48,](#page-14-12) [60,](#page-14-13)
    [63,](#page-14-14) [69\]](#page-14-15) and seven target model inference [\[1,](#page-12-5)
    [7,](#page-12-6) [8,](#page-12-7) [26–](#page-13-15)[28,](#page-13-16) [49,](#page-14-16)
    [65\]](#page-14-17). According to the survey, we can observe several trends as
    follows:


    - 1. Memory occupies a significant portion of the silicon footprint on an NPU
    chip, ranging from 4% to 79% of the area, with capacities from 2.5MB to 896MB.

    - 2. Figure [2](#page-2-0) Left shows a trend of diminishing marginal benefit
    of memory capacity. This is because there is a critical capacity to meet the data
    reuse and bandwidth requirement at the beginning, and the increments become negligible
    with higher memory capacity.

    - 3. We can infer that there is a saturated capacity equivalent to the ideal unlimited
    memory, especially for the inference design. For example, Hanguang [\[26\]](#page-13-15)
    is a special SRAM-only inference system without DDR, and the 394MB buffers are
    large enough to hold the intermediate data in their scenarios.


    <span id="page-2-1"></span>![](_page_2_Figure_10.jpeg)


    Figure 3. Evaluations on subgraphs fusing different number of layers (denoted
    as L=1,3,5). Y-axis is in the log domain. The 2TOPS NPU accelerator is configured
    with a 1MB global buffer and a 1.125MB weight buffer. The bandwidth requirement
    of weights is from the prefetch of the next subgraph, while that of activations
    is from the inputs and outputs of each subgraph.


    This survey implies a design trade-off between memory capacity and performance
    based on workloads and commercial considerations. Motivated by the observations
    above, this paper aims to provide several memory design considerations and study
    the connection between workload features and memory capacity in an NPU accelerator.


    #### 2.2 Workload Deployment


    A neural network is usually executed in a DNN accelerator with layer or graph
    granularities based on the buffer capacity and dataflow.


    2.2.1 Layer-level Assignment. This manner assigns tasks layer by layer. Most previous
    studies employ a tiling-based layer-wise execution manner [\[10,](#page-12-8)
    [21,](#page-13-17) [30,](#page-13-18) [37,](#page-13-7) [50,](#page-14-18) [61\]](#page-14-9),
    which elaborates the tiling sizes of tensors to fit in the accelerator buffers
    and maintain performance. A proper tiling scheme should overlap the data loading
    latency with the computing time of each tile and try to reduce the repeated access
    of local weight buffers. Tiles of data are transferred between the external memory
    and the global buffer, and PEs subsequently fetch data from the global to their
    local buffers. Given the larger bit-width of partial sums (e.g., 24bit partial
    sums v.s. 8bit inputs in Simba), the output-centric tiling scheme is more commonly
    used to calculate the final results before writing back to the global buffer [\[61\]](#page-14-9).


    2.2.2 Graph-level Assignment. Unlike the layer-level assignment that restrains
    from leveraging inter-layer reuse, a graph-level assignment processes several
    layers of a neural network as a whole. To demonstrate the effectiveness of the
    layer-level assignment, we evaluate four networks on a 2TOPS accelerator model,
    as shown in Figure [3.](#page-2-1) The results show that fusing layers into subgraphs
    significantly reduces external memory access by 42.3% ∼ 74.7% and average bandwidth
    requirements by 26.8% ∼ 67.8%. However, the improvements of larger subgraphs are
    marginal, indicating that there is an optimal trade-off between inter-layer


    reuse and subgraph size, which determines the memory requirement. For example,
    executing three-layer subgraphs reduces external memory access by 53.7% in ResNet50,
    while executing five-layer subgraphs only further reduces it by 13.6%.


    Several works have studied inter-layer reuse and graph partition. However, they
    have several limitations in terms of performance and flexibility. LCP [\[42\]](#page-14-19)
    groups similar layers into a cluster and executes them as a whole, which makes
    it challenging to generalize into an arbitrary graph. Fused-CNN [\[4\]](#page-12-2)
    and SR-CNN [\[38\]](#page-13-8) fuse large contiguous layers for plain networks
    using manually-designed strategies. Irregular-NN [\[73\]](#page-15-3) attempts
    to execute a complex subgraph using a DP-based algorithm, but the constrained
    search space limits the exploration.


    To overcome these challenges, we propose an end-to-end framework that automatically
    optimizes the graph partition and memory configuration for any neural network.
    Our framework consists of two main components: a graph-level dataflow and a hardware-mapping
    co-exploration algorithm. We first introduce the graph-level dataflow and its
    hardware implementation. Then, we present Cocco, an efficient algorithm that explores
    the trade-offs among memory configurations and graph partition schemes based on
    workload features.


    ## <span id="page-3-1"></span>3 The Proposed Graph-Level Scheme


    To execute layers on an NPU core in a graph-level manner, we need an effective
    approach to reuse intermediate data and decide the memory allocation. This section
    presents our comprehensive scheme for subgraph execution, which addresses the
    first two challenges mentioned in Section [1.](#page-0-0) First, we describe a
    multi-layer execution flow that minimizes the memory footprint by a friendly tiling
    approach (for challenge 1). Second, we explain how to implement this flow on a
    real NPU using an efficient data reuse pattern (for challenge 2). The consistent
    target is to reduce the memory footprint and be friendly to implementation.


    #### 3.1 Subgraph execution scheme


    It is common practice for the layer-level scheduling to partition the output tensor
    into several tiles as layer-level elementary operations [\[56,](#page-14-20) [61,](#page-14-9)
    [72,](#page-15-4) [74\]](#page-15-5), simplifying the scheduling and instruction
    generation. Likewise, our high-level idea is also to generate a series of explicit
    subgraph-level elementary operations. However, we need to address the challenges
    of various kernel sizes and strides in different paths to prevent unbalanced data
    production and unnecessary memory.


    A model''s subgraph consists of multiple layers (nodes) with dependencies. Section
    [4](#page-5-0) provides detailed information on subgraph partition. In Figure
    [4\(](#page-3-0)a), we present a straightforward production-centric scheme for
    executing a subgraph


    with different kernel sizes in two branches, deriving tile sizes of the subsequent
    layers based on the predetermined input tile sizes. For example, we can produce
    a 1 × 1 tile of Node(0) and a 2 × 2 tile of Node(2) with a given 5 × 5 feature
    map of input Node(-1). In this case, these intermediate results only reduce to
    1 × 1 in Node(3), limited by the smallest input of Node(0), so the remaining results
    of Node(2) can not be consumed immediately. As shown in Figure [4,](#page-3-0)
    three extra data of Node(2) along with sixteen extra source data of Node(1) take
    up extra memory space. There are more redundant cached data when the subgraph
    becomes larger and more complicated. Disadvantages of this manner are attributed
    to the production-centric idea that consumes all related activations from the
    producers at once.


    To avoid the memory overhead of storing unused data, we propose a consumption-centric
    scheme in Figure [4\(](#page-3-0)b), where results of each node are produced on
    demand based on consumer(s) (i.e., output node(s)). For example, given a 1 × 1
    tile of Node(3), we derive the 1 × 1 tile size for Node(2), which subsequently
    decides a 3 × 3 tile for Node(1).


    The backward-derivation for each producer node is nontrivial because of diverse
    kernel sizes and strides in different paths. Therefore, we propose a three-stage
    flow to determine the behavior of each node, as illustrated in Figure [5.](#page-4-0)
    The highlevel idea is to let output nodes drive the whole execution and match
    the data consumption and production in each subgraph-level elementary operation.


    The stage-1 is similar to the traditional single-layer scheduling, where the tile
    size is optimized for higher computation utilization. In order to hold a larger
    subgraph, the tile size


    <span id="page-3-0"></span>![](_page_3_Figure_14.jpeg)


    Figure 4. A conceptual comparison between two manners to process a subgraph. The
    node marked with a negative number represents the input node. The corresponding
    subgraph is shown in the upper right, where × / refers to the convolution kernel
    size ( ) and stride ().


    Cocco: Hardware-Mapping Co-Exploration towards Memory ... ASPLOS ''24, April 27-May
    1, 2024, La Jolla, CA, USA


    <span id="page-4-0"></span>![](_page_4_Figure_1.jpeg)


    Figure 5. The flow to determine the execution scheme of a subgraph (i.e., the
    computed tile size of each node, the tile offset, and the processing sequence
    of nodes). For simplicity, we discuss the 1D-CONV in this example and it is similar
    in the 2D-CONV case.


    tends to be smaller. In the 1D-CONV example, we set the tile size to be 2 for
    output nodes.


    The stage-2 aims to determine the data update offset Δ and the memory allocation
    size for each node based on the consumer(s), processing in the reverse topological
    order. We use the least common multiply (LCM) operation to determine Δ () of producers
    for aligning different input offset requirements (Δ () () ) from consumers. Hence,
    one producer update may correspond to multiple updates of a consumer. For example,
    Δ (−2) = lcm{Δ (0) (0) , Δ (1) (1) } = 4 = 2Δ (1) (1) , one update of Node(-2)
    corresponds to two updates of Node(1). As for the tile size deduction, (Δ () /
    () ) is to derive the required input tile size (,) for output node [1](#page-4-1)
    , where Δ () / () is the consumer offset (updated data) per producer update. The
    maximum result (,) of all outputs is the tile size () of input node . In this
    example, (−2) = max{<sup>0</sup> (2), <sup>1</sup> (4)} = 6 and (−1) = max{<sup>1</sup>
    (2), <sup>2</sup> (2)} = 4.


    As mentioned above, since we use LCM to align production and consumption, one
    producer update may correspond to multiple updates of a consumer. In the stage-3,
    we use \_ to represent the number of memory update per subgraph elementary operation.
    The generated result of the example in Figure [5](#page-4-0) is shown in Figure
    [6.](#page-4-2) \_ of Node(-1), Node(1), and Node(2) are two, where the second
    updates are highlighted in red boxes. Note that the {\_(−2) , . . . , \_(2) }
    solution is not unique, but the unique co-prime one {1, 2, 1, 2, 2} corresponds
    to the minimal elementary operation.


    <span id="page-4-2"></span>![](_page_4_Figure_8.jpeg)


    Figure 6. The memory snapshot during two subgraph elementary operations based
    on the execution scheme of Figure [5](#page-4-0) example. The allocated memory
    size and update offset correspond to and Δ, respectively (the [:] notation denotes
    data ranging from index to ). The arrows denote the data dependency according
    to the node relation in the subgraph.


    The proposed flow is based on a general directed acyclic computation graph and
    is not limited to specific layer features. In this way, we can determine the execution
    scheme for any complex irregular network like NasNet [\[75\]](#page-15-1) and
    RandWire [\[68\]](#page-14-6).


    #### 3.2 Memory Management for the subgraph execution


    Up to now, we have inferred the execution scheme for subgraphs, and the remaining
    challenge is how to implement it on hardware efficiently. Figure [7](#page-5-1)
    shows the memory allocation and update scheme for the subgraph execution. Before
    computing a subgraph, the compiler determines logical blocks for input, intermediate,
    and output nodes, where the block sizes depend on the tile sizes derived from
    the execution flow.


    For convenient management, we introduce two types of memory regions: MAIN and
    SIDE. The MAIN region stores the source data for PE (i.e., the tile of <sup>0</sup>
    × <sup>0</sup> × in Figure [7\)](#page-5-1). The SIDE region reserves the horizontally
    overlapping data[2](#page-4-3) . Considering no reuse requirement for some output
    nodes, we only need a MAIN region to buffer the results of the current tile. Except
    for the input nodes (negative numbers) loading data from DRAM, the other nodes
    update data locally based on the computed results of the input node(s).


    In detail, the update scheme leverages the collaboration between the MAIN region
    and the SIDE region to achieve full reuse across sliding tiles (we consider kernel
    size > stride). As shown in Figure [7,](#page-5-1) when the convolution windows
    slide across the feature maps, the vertical overlap data (e.g., column = 5) are
    reused locally in the MAIN region. In contrast, the horizontally overlapping data
    (e.g., the first row of = 6 ∼ 8) are loaded from the SIDE region (path ①). Only
    a subset of data is replaced by the newly calculated results


    <span id="page-4-1"></span><sup>1</sup>For example, assume node is a convolution
    layer with kernel size () and stride () , then ( ) = () + ( − 1) × () .


    <span id="page-4-3"></span><sup>2</sup>We assume the column is the inner loop
    while the row is the outer loop.


    (marked in green). Besides, the bottom horizontal slices write new data to the
    SIDE region for the next row loop (path ②).


    The extra hardware overhead for the proposed memory scheme is slight. Figure [8](#page-5-2)
    presents our 12nm NPU core for the subgraph processing, with a buffer region manager
    to logically partition the global buffer to support contiguous layer processing.
    The buffer region manager is a 2-depth register file, where determines the maximum
    subgraph size, and each entry pair indicates the start and the end address for
    each region. The area overhead is quite small, and in our test chip, the area
    ratio is only 0.18% with = 64 and 272-byte size (17-bit address for the 1MB 64bit-width
    global buffer).


    In summary, our high-level idea is to divide the buffer into logical blocks for
    different layers and try to reuse data for sliding convolution windows. The memory
    management approach can be compatible with an accelerator as long as it supports
    the data movement inside the on-chip memory and flexible data assignment for computing.
    Coupled with our subgraph execution scheme introduced before, intermediate outputs
    in the subgraph can avoid being recomputed. Only those layers required by other
    subgraphs are written back to DRAM for further reuse.


    # <span id="page-5-0"></span>4 Memory Communication-Capacity Co-Exploration


    <span id="page-5-1"></span>**A subgraph example**


    The aforementioned hardware model enables arbitrary subgraph execution, but there
    is always limited buffer capacity


    **Data update diagram**


    ![](_page_5_Figure_6.jpeg)


    Figure 7. Memory allocation and data update scheme in the global buffer for full
    data reuse. The data layout used in our implementation is NWHC8c (aligned to 8
    channels), which can be changed in another design. <sup>0</sup> and <sup>0</sup>
    are the height and width of an input tile; is the input channel size; is the global
    width-dimension index of the input tensor; and <sup>0</sup> is the width-dimension
    index of an input tile.


    <span id="page-5-2"></span>![](_page_5_Figure_9.jpeg)


    Figure 8. Hardware implementation with the buffer region manager in our 12nm NPU
    as a demonstration. The layout is an NPU core extracted from part of our in-house
    chip.


    in hardware. Therefore, we need to partition the whole computation graph into
    a series of subgraphs that fit the memory. Below, we move up to the optimization
    for graph partition and memory design-space exploration for challenge 3.


    #### 4.1 Problem Formulation


    4.1.1 Graph-Level Partition. Formally, a DNN model can be represented as a computation
    graph = ( , ), where is the vertex set consisting of all the layers in a DNN model,
    and is the edge set that defines the structure of DNN. In particular, an edge
    (, ) ∈ represents that the output of layer is an input of layer .


    We aim to find a partition scheme : → N that assigns each layer to a subgraph,
    where layer ∈ is computed in the ()-th subgraph. A valid partition scheme should
    satisfy that any layer is computed before use. Therefore, for any (, ) ∈ , we
    have () ≤ (). Moreover, any subgraph should be connected in , otherwise meaningless.


    We cast the partition exploration as an optimization problem. The objective is
    to find a valid partition scheme that minimizes the total cost:


    <span id="page-5-4"></span>

    $$\sum\_{i} Cost\_{\mathcal{M}}(\{v \in V \mid P(v) = i\}),\tag{1}$$


    where is a cost function of a given subgraph based on a target metric (e.g., external
    memory access (EMA) and energy). For each subgraph, the EMA cost contains the
    loading of weights and input activations and the storage of output activations[3](#page-5-3)
    . The energy cost includes the overhead of EMA, on-chip buffers, and computation
    units.


    4.1.2 Design-Space Exploration (DSE). Our work further extends the optimization
    to combine with the memory design-space exploration. In this paper, we focus on
    the global buffer and the weight buffer, given that they dominate


    <span id="page-5-3"></span><sup>3</sup>The nodes that are required to write-back
    to DRAM can be the model output layer or the layers required by the future subgraph.


    the overhead of energy and area in an NPU core. As illustrated in Figure [1,](#page-1-0)
    a larger buffer capacity can take in more layers inside a subgraph, reducing communication
    costs but compromising the silicon area. To co-explore the hardware configuration
    and mapping, we construct an objective function by a linear combination of the
    hardware and mapping costs:


    <span id="page-6-0"></span>

    $$\text{BUF\\_SIZE} + \alpha \cdot \sum\_{i} Cost\_{\mathcal{M}}(\{v \in V \mid
    P(v) = i\}),\qquad \text{(2)}$$


    where is a preference hyper-parameter to adjust the proportion between two costs.


    #### 4.2 Baseline Methods


    Several optimization methods that exist today can perform graph-level partition.
    However, most of them fail to directly co-explore hardware and partition. Below,
    we list four typical methods as our baselines and sketch their features.


    4.2.1 Enumeration-based Algorithm. Fused-CNN [\[4\]](#page-12-2) applies a straightforward
    way to enumerate all possible partition schemes and return the best one. Jangda
    et al. [\[25\]](#page-13-19) proposed state compression dynamic programming to
    speed up the enumeration-based algorithm. We migrate their methods as our baseline
    and further improve them by only recording one subgraph in the state to reduce
    the time complexity.


    Nonetheless, there are still exponential states in the improved implementation.
    Let be the number of nodes in a graph, and the enumeration-based method may explore
    up to (2 <sup>2</sup> ) states for irregular networks. Consequently, the search
    is hard to complete within a reasonable search time for large-scale networks,
    not to mention the co-exploration with DSE.


    4.2.2 Greedy Algorithm. Halide [\[47\]](#page-14-21) employs a greedy algorithm
    to perform function grouping, which can be applied to the graph-level partition.
    Specifically, it first assigns each layer into a single-layer subgraph. Then it
    iteratively fuses a pair of subgraphs contributing the greatest benefit until
    all benefits are negative.


    Therefore, this algorithm tends to be trapped at the local minimum. Moreover,
    since the fusion decision rules are based on a given hardware, the greedy method
    cannot co-explore with DSE.


    4.2.3 Dynamic Programming (DP)-based Algorithm. For the irregular network scheduling
    problem, Zheng et al. [\[73\]](#page-15-3) proposed a DP-based algorithm. They
    arrange the layers based on their depth and perform DP in a sequential manner.


    This method is restricted to assigning layers that are contiguous in the depth
    order into a subgraph, hence the exploration is confined to constrained search
    space. It is unlikely to find the global optimum, especially for non-plain network


    structures. In addition, since the state transition of DP depends on the predefined
    buffer size, it is also tough to carry out co-exploration.


    4.2.4 Simulated Annealing (SA). SA [\[33\]](#page-13-20) is a popular optimization
    algorithm that samples a point and updates it iteratively to improve. It adopts
    the new sample points with a probability affected by the performance difference
    and a hyper-parameter named temperature. We employ the customized mutation operations
    (described in Section [4.4.3\)](#page-7-0) to update the sample points and implement
    an SA-based algorithm as a baseline.


    SA is an alternative optimization method for our framework with compatible operators,
    but it is not stable as the genetic algorithm in a range of benchmarks, which
    will be shown in later experiments.


    #### 4.3 Genetic Algorithm


    Previous research shows competitive performance of the Genetic Algorithm (GA)
    in several scheduling optimization problems [\[30,](#page-13-18) [31\]](#page-13-21).
    We summarize several benefits of GA for our hardware-mapping co-exploration problem:


    - 1. White-box property: We can track and tune its optimization process conveniently.
    Therefore, it is easy and intuitive to understand.

    - 2. Complete search space: It has the potential to explore the complete search
    space by customized mutation and crossover operations.

    - 3. Avoid local optima: In contrast to the greedy algorithm, GA can naturally
    jump out of the local minimum benefiting from the diversity of the population.

    - 4. Flexible initialization: We can use the results of other optimization algorithms
    to initialize GA and use GA to finetune the result.

    - 5. Co-exploration: Through the proposed GA operations and genome encoding, it
    can further support partition-DSE co-exploration.


    We encode each candidate solution (partition scheme and the corresponding memory
    configuration for our problem) as a genome, and the population contains a set
    of genomes. The GA goes through a series of generations to obtain a lower cost.
    It performs the crossover and mutation operations on the population in each generation.
    Specifically, a crossover operation blends two genomes selected from the population
    to generate one offspring while a mutation operation modifies a genome randomly.
    At the end of each generation, the evaluation environment evaluates the fitness
    of each genome, and the population in the new generation is selected based on
    the fitness results.


    #### 4.4 Cocco Optimization Framework


    Cocco is a GA-based optimization framework that enables the co-exploration of
    memory configuration and graph-level partition, as shown in Figure [10.](#page-7-1)
    The core of Cocco is a


    <span id="page-7-2"></span>![](_page_7_Figure_2.jpeg)


    Figure 9. Illustration of crossover and mutation operations in Cocco.


    series of operations that explore a complete search space. We build a genetic
    algorithm based on these customized operations. Fed with the neural network structure
    and DSE requirements, Cocco goes through several steps to get the optimization
    results. The execution model described in Section [3](#page-3-1) is embedded in
    the evaluation environment. In the following, we introduce the five stages of
    Cocco.


    4.4.1 Initialization. The first step in Cocco is to generate the initial population,
    where each genome contains a partition scheme of the computation graph and a memory
    configuration for DSE. For the DSE part, every genome selects a capacity value
    in a given range following a uniform distribution. There are two options in Cocco
    to initialize the partition scheme of each genome. The first option is random
    initialization. Precisely, we determine the () for each layer ∈ in topological
    order, and each () is selected randomly within the valid range. The other option
    is to initialize the partition scheme from other optimization algorithms.


    4.4.2 Crossover. We designed a customized crossover operation to inherit and blend
    the features of two parents selected from the population. Specifically, each hardware
    configuration (i.e., memory capacity) in the offspring is the average of its parents
    and then rounds to the nearest candidate value. For the partition scheme, we assign
    layers to subgraphs in topological order. Each undecided layer chooses one parent
    randomly to reproduce the corresponding subgraph. If the reproduced subgraph contains
    layers that have been decided, we split out a new one excluding those layers,
    or merge it with one of the subgraphs to which the decided layers belong.


    As shown in Figure [9\(](#page-7-2)b), layer 1 and layer 3 select Dad as the parent
    to reproduce the subgraphs {1, 2} and {3, 4}, respectively. Next, layer 5 selects
    Mom as its parent, so it


    <span id="page-7-1"></span>![](_page_7_Figure_8.jpeg)


    Figure 10. Cocco framework overview.


    intends to reproduce subgraph {4, 5, 6}. However, since we have already decided
    on layer 4 in subgraph {3, 4}, there are two alternatives: creating a new subgraph
    {5, 6} (Child-1) or merging with subgraph {3, 4} to obtain {3, 4, 5, 6} (Child-2).


    <span id="page-7-0"></span>4.4.3 Mutation. Four mutation operations are customized
    for the optimization flow to explore the search space extensively. We guarantee
    the validity of genomes after each mutation in the implementation. At the bottom
    of Figure [9,](#page-7-2) we show a node-level operation (modify-node) and two
    subgraph-level ones (split-subgraph and merge-subgraph):


    - modify-node (Figure [9\(](#page-7-2)c)): Modify the assignment of a randomly
    selected node : from → () to → ′ (), where ′ () can be an existed subgraph or
    a new one.

    - split-subgraph (Figure [9\(](#page-7-2)d)): Split a randomly selected subgraph
    into two or more subgraphs.

    - merge-subgraph (Figure [9\(](#page-7-2)e)): Merge two randomly selected subgraphs
    into one subgraph.

    - mutation-DSE (not shown): Modify the memory configuration to a random one within
    the range. The new values are sampled based on a normal distribution, where the
    average is the original value, and the variance is a hyper-parameter.


    4.4.4 Evaluation. Since GA tries to maximize the fitness of the genomes, we set
    fitness to be the opposite of the cost (e.g., Formula [1](#page-5-4) and [2\)](#page-6-0).
    To evaluate the fitness of each genome in the population, we use our simulator
    (introduced in the next section) to extract the execution costs of subgraphs (e.g.,
    EMA and energy).


    During the evaluation, the simulator decodes the subgraph and hardware configuration
    of each genome and calculates the fitness by aggregating the cost of each subgraph.
    Particularly, when a large subgraph exceeds the buffer capacity, we perform the
    split-subgraph operation to ensure genome validity. This kind of in-situ tuning
    can increase the number of valid samples during the optimization operations and
    thus, improve the sample efficiency.


    4.4.5 Selection. At the end of each generation, Cocco performs the tournament
    selection. Specifically, it holds multiple tournaments among a few randomly selected
    genomes, and the winners (the genome with the best fitness) of these tournaments
    form the population of a new generation. This operation facilitates superior fitness
    in the new generation. The number of genomes in each tournament is decided by
    a hyper-parameter. The new generation subsequently starts from the crossover step
    again.


    ## 5 Experiments


    In the evaluations, we first present the superiority of Cocco for the graph partition;
    and then demonstrate its outstanding stability and sample efficiency of the co-exploration
    for the hardware optimization, followed by additional discussions about the results
    under different configurations.


    #### 5.1 Methodology


    5.1.1 Evaluated Models. In the following evaluations, we consider three types
    of model structures: plain (VGG16 [\[57\]](#page-14-2)), multi-branch (ResNet50,
    ResNet152 [\[20\]](#page-13-4), GoogleNet [\[59\]](#page-14-3), Transformer [\[64\]](#page-14-22),
    and GPT [\[52\]](#page-14-23)), and irregular structure (RandWire-A/B [\[68\]](#page-14-6)
    and NasNet [\[75\]](#page-15-1)). RandWire-A/B are


    generated based on the small and regular regime configurations introduced in the
    paper [\[68\]](#page-14-6). FC layers are transformed to 1×1 CONV while pooling
    and element-wise layers are analyzed as depth-wise CONV without weights. The scalar
    operations (e.g., activation function) are hidden in the pipeline (e.g., the post-process
    module following PE in Simba [\[56\]](#page-14-20)) and their overhead can be
    ignored.


    5.1.2 Accelerator Platform. As shown at the top of Figure [10,](#page-7-1) we
    consider a SIMBA-like hierarchical accelerator with a global buffer, a weight
    buffer, and a 4×4 PE array in each core used in several previous works [\[56,](#page-14-20)
    [61,](#page-14-9) [71\]](#page-15-6). Each PE contains an 8×8 MAC array to process
    a sub-tile from the global buffer. In particular, we model the execution flow
    based on the scheme described in Section [3.](#page-3-1) The parallelism of two
    dimensions of the PE array can be dynamically configured by the mapper results
    to ensure high utilization. We schedule subgraphs in topological order and prefetch
    weights of the next subgraph during the current computing. We also extend our
    platform to support fundamental multi-core studies by interconnecting cores with
    a crossbar. They share weights to release the burden of each core.


    The arithmetic and memory overhead is extracted in a 12nm library based on the
    synthesized RTL implementations (SRAM based on the ARM memory compiler) with 1GHz.
    The DRAM energy is set as 12.5pJ/bit [\[70\]](#page-15-2). The extra footprint
    of the plug-in design is mainly a 272-Byte register file to store the head and
    end logical region addresses of maximal 64 nodes, which is negligible. Based on
    off-the-shelf evaluators Timeloop [\[50\]](#page-14-18) and MAESTRO [\[37\]](#page-13-7)
    for spatial accelerators, we developed a modified simulator that supports the
    evaluation of latency and energy. It employs the consumption-centric scheme to
    determine the tile size of each layer, and the memory access in the model is free
    from padding data. The latency per subgraph depends on the maximum of the calculation
    and external communication cycles. We allocate 16GB/s DRAM bandwidth per accelerator
    core for loading weights and input activations and writing back data for subsequent
    subgraphs. The off-chip communication consists of weight loading of each layer
    and the inputs and outputs of each subgraph. As described in Section [3,](#page-3-1)
    our subgraph execution scheme avoids recomputing of intermediate outputs.


    5.1.3 Baselines. Three optimization baselines for graph partition are the greedy
    algorithm used in Halide [\[47\]](#page-14-21), dynamic programming (DP) used
    in Irregular-NN [\[73\]](#page-15-3) , and the enumeration-based method as a reference.


    For the DSE studies, we compare Cocco with simulated annealing (SA) [\[33\]](#page-13-20)
    to demonstrate the better stability of GA. These two methods are both the co-optimization
    scheme that optimizes partition and hardware settings at the same time. In contrast
    to co-optimization, the two-step scheme is another method for design-space exploration.
    Specifically, we


    <span id="page-9-0"></span>![](_page_9_Figure_2.jpeg)


    Figure 11. The evaluation results for graph partition using the EMA-opt configuration
    (EMA as the optimization metric). The enumeration-based method is deterministic,
    which figures out the optimal solution as a reference in the first four models.
    It cannot complete for large-scale models (Transformer, GPT, RandWire-A, and RandWire-B)
    in a reasonable time because of the exponential search space.


    use random search (RS) or grid search (GS) to sample memory capacity candidates
    and then explore the corresponding partition schemes. During the search, we evaluate
    5,000 samples for each capacity candidate and keep the best candidate as the output.
    As for the sampling method, RS randomly samples memory capacity candidates while
    GS uses a coarser granularity to enumerate the candidates.


    #### 5.2 Graph Partition Evaluations


    We start by presenting the partition performance on the single-core hardware with
    a 1MB global buffer and a 1.125MB weight buffer. The number of samples in Cocco
    is set to be 400,000. We evaluate the external memory access (EMA) and bandwidth
    requirements of eight models shown in Figure [11,](#page-9-0) where the results
    are normalized to the Halide baseline. This experiment aims to validate the effectiveness
    of our Cocco framework in graph partition. For networks with simpler structures,
    Cocco can find out the optimal solutions same as the enumeration-based results.
    For large-scale irregular networks (Transformer, GPT, RandWire-A, and RandWire-B),
    the enumeration-based method cannot complete in a reasonable time, while Cocco
    provides better solutions than Halide and DP. A better subgraph partition strategy
    helps to ease the communication burden, thus reducing the EMA cost and bandwidth
    requirements.


    #### 5.3 Hardware-Mapping Co-Exploration


    After learning the superiority of Cocco for the graph partition, we further co-explore
    the memory configuration and graph partition mapping as the core study of this
    work. Three categories of exploration methods are used, including the fixed hardware
    scheme, the two-step scheme as baselines, and the proposed co-optimization scheme.
    We set three fixed memory configurations with Small capacity, Medium capacity,
    and Large capacity, followed by a partition-only procedure. The two-step scheme
    is implemented with decoupled steps for capacity search (RS or GS) and partition
    (GA). The cooptimization methods include the proposed Cocco and an SA-based one
    as the comparison. All methods sample up to


    <span id="page-9-2"></span>Table 1. Hardware-mapping co-exploration for separate
    buffer. In this table, A refers to the global buffer, and W refers to the weight
    buffer. We evaluate the cost using Formula [2](#page-6-0) (the lower cost, the
    better), where the metric is energy. We use RandWire-A as RandWire in the following
    experiments.


    | Optimization |        | ResNet50      |                   |        | GoogleNet
    |                   |        |  |

    |--------------|--------|---------------|-------------------|--------|-----------|-------------------|--------|--|

    |              |        |               | Size (A) Size (W) | Cost   |           |
    Size (A) Size (W) | Cost   |  |

    |              | Buf(S) | 512KB         | 576KB             | 1.04E7 | 512KB     |
    576KB             | 4.07E6 |  |

    | Fixed<br>HW  |        | Buf(M) 1024KB | 1152KB            | 1.07E7 | 1024KB    |
    1152KB            | 5.06E6 |  |

    |              | Buf(L) | 2048KB        | 2304KB            | 1.24E7 | 2048KB    |
    2304KB            | 7.18E6 |  |

    |              | RS+GA  | 448KB         | 864KB             | 1.04E7 | 384KB     |
    432KB             | 3.88E6 |  |

    | Two-Step     | GS+GA  | 128KB         | 864KB             | 1.07E7 | 128KB     |
    144KB             | 3.80E6 |  |

    |              | SA     | 256KB         | 360KB             | 1.06E7 | 192KB     |
    144KB             | 3.78E6 |  |

    | Co-Opt       | Cocco  | 704KB         | 864KB             | 1.04E7 | 192KB     |
    432KB             | 3.75E6 |  |

    |              |        |               |                   |        |           |                   |        |  |

    |              |        |               | RandWire          |        |           |
    NasNet            |        |  |

    | Optimization |        |               | Size (A) Size (W) | Cost   |           |
    Size (A) Size (W) | Cost   |  |

    |              | Buf(S) | 512KB         | 576KB             | 3.23E6 | 512KB     |
    576KB             | 6.14E7 |  |

    | Fixed        |        | Buf(M) 1024KB | 1152KB            | 3.92E6 | 1024KB    |
    1152KB            | 5.83E7 |  |

    | HW           | Buf(L) | 2048KB        | 2304KB            | 6.00E6 | 2048KB    |
    2304KB            | 5.66E7 |  |

    |              | RS+GA  | 448KB         | 792KB             | 3.31E6 | 1152KB    |
    2016KB            | 5.60E7 |  |

    | Two-Step     | GS+GA  | 128KB         | 144KB             | 3.02E6 | 2048KB    |
    2304KB            | 5.66E7 |  |

    | Co-Opt       | SA     | 192KB         | 144KB             | 3.00E6 | 2048KB    |
    1872KB            | 5.61E7 |  |


    50,000 points. The energy-capacity co-optimization is used in the following evaluations.


    5.3.1 DSE analysis using separate and shared buffer. We first perform the hardware-mapping
    co-exploration to determine the suitable memory configuration (except for the
    fixed-HW scheme) with = 0.002[4](#page-9-1) and then solely execute the partition-only
    Cocco to obtain the final cost. In particular, we also compared the results using
    two memory designs: separate buffer and shared buffer. For the separate buffer
    design, activations and weights are stored in different buffers while they share
    the same space in the shared buffer design. The memory capacity candidates for
    the global buffer


    <span id="page-9-1"></span><sup>4</sup>The energy and the capacity units are pJ
    and Byte, respectively.


    Cocco: Hardware-Mapping Co-Exploration towards Memory ... ASPLOS ''24, April 27-May
    1, 2024, La Jolla, CA, USA


    <span id="page-10-0"></span>Table 2. Hardware-mapping co-exploration for shared
    buffer. We evaluate the cost using Formula [2](#page-6-0) (the lower cost, the
    better), where the metric is energy.


    | Optimization |        |        | ResNet50 | GoogleNet |        |  |

    |--------------|--------|--------|----------|-----------|--------|--|

    |              |        | Size   | Cost     | Size      | Cost   |  |

    | Fixed<br>HW  | Buf(S) | 576KB  | 1.01E7   | 576KB     | 3.66E6 |  |

    |              | Buf(M) | 1152KB | 1.00E7   | 1152KB    | 4.04E6 |  |

    |              | Buf(L) | 2304KB | 1.04E7   | 2304KB    | 5.09E6 |  |

    |              | RS+GA  | 1280KB | 0.98E7   | 640KB     | 3.65E6 |  |

    | Two-Step     | GS+GA  | 1344KB | 0.98E7   | 512KB     | 3.65E6 |  |

    |              | SA     | 896KB  | 1.00E7   | 192KB     | 3.75E6 |  |

    | Co-Opt       | Cocco  | 1344KB | 0.98E7   | 384KB     | 3.60E6 |  |

    |              |        |        |          |           |        |  |

    |              |        |        | RandWire | NasNet    |        |  |

    | Optimization |        | Size   | Cost     | Size      | Cost   |  |

    |              | Buf(S) | 576KB  | 2.83E6   | 576KB     | 6.36E7 |  |

    | Fixed        | Buf(M) | 1152KB | 3.03E6   | 1152KB    | 5.73E7 |  |

    | HW           | Buf(L) | 2304KB | 3.90E6   | 2304KB    | 5.51E7 |  |

    |              | RS+GA  | 320KB  | 2.85E6   | 2560KB    | 5.47E7 |  |

    | Two-Step     | GS+GA  | 832KB  | 2.86E6   | 3072KB    | 5.42E7 |  |

    | Co-Opt       | SA     | 256KB  | 2.92E6   | 1728KB    | 5.56E7 |  |


    (for activations) range from 128KB to 2048KB with a 64KB interval, while that
    for the weight buffer range from 144KB to 2304KB with a 72KB interval. The exploration
    range of the shared buffer is from 128KB to 3072KB with an interval of 64KB.


    The evaluation using separate buffers is shown in Table [1,](#page-9-2) where
    Cocco achieves better optimization with up to 1.89% (compared to SA in ResNet50)
    to 50.33% (compared to Fixed-HW(L) in RandWire) lower cost compared to various
    baselines across four models. The two-step scheme fails to combine the information
    between different sizes, so it is generally worse than the co-optimization method.


    The capacity results also reflect the inherent capacity preference of different
    models. The data amount in GoogleNet and RandWire is relatively smaller, and thus
    their capacity requirements are lower. In contrast, the data amount in NasNet
    is larger, so a high capacity is preferred.


    As shown in Table [2,](#page-10-0) the evaluation of the shared buffer setting
    shows a similar trend. Furthermore, we can observe that most of the cost results
    of the shared buffer are lower than those using the separate configuration. Although
    the shared buffer design requires additional control flows, it indeed improves
    efficiency than the separate buffer design.


    5.3.2 Sample efficiency analysis. We next study the sample efficiency of the two-step
    and the co-optimization scheme in Figure [12.](#page-10-1) We record the cost
    trends of the first 50,000 samples on ResNet50, GoogleNet, and RandWire during
    the exploration. Overall, Cocco shows a consistent convergence trend on these
    three networks. And it converges faster and


    <span id="page-10-1"></span>![](_page_10_Figure_9.jpeg)


    Figure 12. The convergence curve of Cocco compared with other baselines in the
    hardware-mapping co-explorations. The optimization method requiring fewer samples
    in (d) has higher sample efficiency.


    <span id="page-10-2"></span>![](_page_10_Figure_11.jpeg)


    Figure 13. The visualization of sample points distribution during optimization.
    The slope of the red dashed line denotes the preference between energy and capacity
    cost. The point on the line with a lower intercept has a smaller cost.


    achieves lower costs compared to other baselines, exhibiting a higher sample efficiency.
    The two-step methods perform graph-partition separately under different capacities,
    so they fail to utilize the partition information between capacities. Particularly,
    the GS method uses a deterministic search direction (search from large to small
    capacity in this experiment), so the convergence time depends on the optimal capacity.
    Since GoogleNet and RandWire require relatively small buffers, GS takes a considerable
    number of samples to converge.


    5.3.3 Optimization procedure analysis. We next study how the distribution of sample
    points changes during the optimization procedure of Cocco. While searching for
    20 generations with 500 genomes each, we divided them into ten groups with different
    colors in Figure [13.](#page-10-2) The results show that the distribution moves
    towards a lower intercept


    <span id="page-11-0"></span>![](_page_11_Figure_1.jpeg)


    Figure 14. The trade-off between energy and memory capacity. The optimization
    target is to minimize the cost defined in Formula [2,](#page-6-0) where the metric
    is energy. Energy results of each model are normalized to the first (= 0.0005)
    results.


    <span id="page-11-1"></span>Table 3. Multi-core and batch evaluation using the
    energycapacity co-opt configuration. Size denotes the shared buffer size in each
    core.


    | Core# Batch |             |            | ResNet50     |            | GoogleNet           |                 |              |  |

    |-------------|-------------|------------|--------------|------------|---------------------|-----------------|--------------|--|

    |             |             | Energy(mJ) | Lat.(ms)     |            | Size(KB)
    Energy(mJ) | Lat.(ms)        | Size(KB)     |  |

    | 1           | 1           | 4.21       | 4.59         | 1344       | 1.61                |
    2.05            | 384          |  |

    |             | 2           | 6.32       | 8.98         | 1728       | 2.18                |
    3.91            | 896          |  |

    |             | 8           | 11.88      | 35.93        | 2880       | 5.64                |
    15.53           | 1472         |  |

    |             | 1           | 4.38       | 2.48         | 768        | 1.66                |
    1.04            | 192          |  |

    | 2           | 2           | 6.46       | 4.78         | 1088       | 2.34                |
    1.99            | 384          |  |

    |             | 8           | 13.01      | 19.12        | 1664       | 5.84                |
    7.97            | 960          |  |

    |             | 1           | 4.29       | 1.39         | 448        | 1.34                |
    0.54            | 192          |  |

    | 4           | 2           | 6.58       | 2.68         | 640        | 2.20                |
    1.07            | 192          |  |

    |             | 8           | 11.50      | 10.71        | 1664       | 6.24                |
    4.30            | 448          |  |

    |             |             |            |              |            |                     |                 |              |  |

    |             |             |            | RandWire     |            |                     |
    NasNet          |              |  |

    |             | Core# Batch | Energy(mJ) | Lat.(ms)     |            | Size(KB)
    Energy(mJ) | Lat.(ms)        | Size(KB)     |  |

    |             | 1           | 1.26       | 1.47         | 384        | 28.57               |
    49.92           | 2624         |  |

    | 1           | 2           | 2.25       | 2.74         | 704        | 47.68               |
    99.87           | 3072         |  |

    |             | 8           | 8.66       | 10.85        | 1664       | 133.03              |
    396.90          | 3072         |  |

    |             | 1           | 1.41       | 0.95         | 192        | 29.18               |
    24.93           | 1728         |  |

    | 2           | 2           | 2.37       | 1.80         | 384        | 48.80               |
    49.73           | 2624         |  |

    |             | 8           | 8.39       | 7.16         | 1280       | 153.25              |
    227.19          | 3072         |  |

    |             | 1           | 1.39       | 0.71         | 192        | 28.00               |
    14.56           | 960          |  |

    | 4           | 2           | 2.91       | 1.40<br>5.55 | 192<br>960 | 45.03               |
    28.58<br>133.38 | 1664<br>2816 |  |


    of the -slope line and gets more centralized in the later generations during the
    optimization process of Cocco.


    #### 5.4 Sensitivity Study about Cocco framework


    5.4.1 Study of in the cost function. The results shown in Figure [14](#page-11-0)
    demonstrate the effectiveness of in adjusting the preference between the memory
    capacity and the given metric (energy is used here). The optimization trades the
    memory capacity for lower energy cost with the increase of . In addition, a larger
    memory capacity indeed contributes to lower energy, but the yields show differences
    because of their various model-inherent graph and layer patterns. For example,
    NasNet is more memory-intensive and more structure-complex than the other three
    models, so it requires a larger memory capacity for less energy consumption.


    5.4.2 Study of performance v.s. memory capacity. Figure [2](#page-2-0) shows that
    the increase of capacity is sub-linear with


    performance. To study this observation, we scale our model to the multi-core version
    and share weights of a subgraph across cores. Different cores only buffer a subset
    of weights and transfer the data between cores, similar to BSD in Tangram [\[18\]](#page-13-11)
    or data-rotation in NN-Baton [\[61\]](#page-14-9). The overhead of the interconnection
    crossbar is extracted from the implemented Arteries IP [\[5\]](#page-12-9).


    An accelerator with more cores can cover a larger subgraph but bring more core-to-core
    overhead. As shown in Table [3,](#page-11-1) in most cases, energy increases from
    the single-core to dual-core configuration because of the communication overhead.
    Moreover, profiting from the data-sharing mechanism, the required memory of each
    core drops with the increase of core number.


    5.4.3 Batch size study. For the batch size evaluation shown in Table [3,](#page-11-1)
    the latency with a larger batch size principally presents a sub-linear increase,
    which benefits from the lower bandwidth requirement of weights via the inter-sample
    data reuse. In addition, such data reuse amortizes the energy burden per batch
    processing. And owing to the better weight reuse in multi-batch processing, a
    larger batch size does not require a proportional capacity.


    ## 6 Related Works


    #### 6.1 Intra-layer Optimization


    Prior works focus on the data reuse for intra-layer assignments, like output-stationary
    in ShiDianNao [\[14\]](#page-13-22) and Envision [\[46\]](#page-14-24), weight-stationary
    in NeuFlow [\[15\]](#page-13-23) and Nvdla [\[49\]](#page-14-16), input-stationary
    in SCNN [\[51\]](#page-14-25), and row-stationary in Eyeriss [\[13\]](#page-12-10).
    Based on these primitive dataflow patterns, extensive studies explored the optimal
    tiling and reordering schemes via brute-force, feedback-based, and constraint
    optimization approaches [\[23,](#page-13-5) [30,](#page-13-18) [50\]](#page-14-18).
    These works focus on layer-level optimization, missing the graph information at
    a higher level. The efficiency of tile updates depends on the memory architecture.
    Simba [\[56,](#page-14-20) [74\]](#page-15-5) and NN-Baton [\[61\]](#page-14-9)
    view each tile as an independent workload so that the tile size has a prominent
    impact on memory access due to halo regions. Motivated by traditional vision processors,
    Ascend [\[40\]](#page-13-10) and DRQ [\[58\]](#page-14-26) employ line buffers
    to achieve data reuse in the row direction, but the line buffer cannot well support
    the 2D-tiling reuse in both row and column directions.


    #### 6.2 Inter-layer Optimization


    Intra-layer scheduling is sub-optimal, which is limited by the data reuse within
    a layer. Therefore, Fused-CNN [\[4\]](#page-12-2), SR-CNN [\[38\]](#page-13-8),
    and LCP [\[42\]](#page-14-19) introduce layer fusion method that cache intermediate
    data on-chip to reduce data transfer overhead using handcrafted or heuristic methods
    for fusion partition. Although Irregular-NN [\[73\]](#page-15-3) suggests a customized-DP
    algorithm, the exploration space is constrained because the layers in an assignment
    need to be successive in a specific


    order. A recent work named DNNFuser [\[29\]](#page-13-24) employs an RLbased method,
    but their formulation towards 1D layer-fusion is hard to handle complex irregular
    networks. Tangram [\[18\]](#page-13-11) and Atomic [\[72\]](#page-15-4) schedule
    DNN workloads on a multi-core (scalable) accelerator, but they focus on executing
    a single layer on each core at a time rather than processing multiple layers with
    local data reuse. Also, some previous works [\[2,](#page-12-11) [19,](#page-13-25)
    [62\]](#page-14-27) tackle the workload placement problem for multiple devides
    without discussing the downstream execution on each device.


    Cocco proposes an automatic framework for inter-layer scheduling with a comprehensive
    memory scheme. It focuses on the fundamental core-level temporal execution that
    can be potentially scaled up to the multi-core or multi-device scenario with a
    spatial parallelism mechanism.


    #### 6.3 Design-Space Exploration for Memory


    Memory design exploration methods lie primarily on two sides: analysis-driven
    and search-driven. For the analysisdriven method, Chen et al. [\[12\]](#page-12-0)
    leverage red-blue pebble models to derive the proper memory capacity representations.
    Subsequently, Cai et al. [\[9\]](#page-12-12) propose Olympus, which generalizes
    a framework to a batch of successive layers and also fills up with more scheduling
    and data reuse techniques. However, they are difficult to represent a subgraph
    with complex inter-layer connections. As for the search-driven method, Xiao et
    al. [\[67\]](#page-14-11), Kwon et al. [\[37\]](#page-13-7), and Feng et al. [\[16\]](#page-13-26)
    explore the memory configuration for the layer-level assignment using the brute-force
    search, while Kao et al. [\[32\]](#page-13-6) employ a genetic algorithm to improve
    the efficiency. These works principally focus on the layer-level information,
    while in comparison, Cocco exploits graph-level features for the better optimization.


    ## 7 Conclusion


    While layer-level scheduling is widely studied to improve memory efficiency, graph-level
    optimization remains relatively unexplored. This paper proposed a graph-level
    dataflow with the corresponding memory management scheme that enables flexible
    graph partitions with high memory utilization. On top of it, we propose Cocco,
    a framework to provide a recommended memory configuration with graph-level scheduling
    strategies. Cocco shows outstanding graph partition ability compared to the greedy
    algorithm and DP employed in previous works and enables efficient graph-level
    hardware-mapping co-exploration. This paper helps to provide an implementation
    philosophy for the accelerator memory and better deployment for it.


    ## Acknowledgments


    This research was partially supported by National Key R&D Program of China (2022YFB2804103),
    Tsinghua University Dushi Program, and Tsinghua University Talent Program. We
    would like to appreciate all the anonymous reviewers for their valuable feedback.


    ## References


    - <span id="page-12-5"></span>[1] Dennis Abts, Jonathan Ross, Jonathan Sparling,
    Mark Wong-VanHaren, Max Baker, Tom Hawkins, Andrew Bell, John Thompson, Temesghen
    Kahsai, Garrin Kimmell, Jennifer Hwang, Rebekah Leslie-Hurd, Michael Bye, E. R.
    Creswick, Matthew Boyd, Mahitha Venigalla, Evan Laforge, Jon Purdy, Purushotham
    Kamath, Dinesh Maheshwari, Michael Beidler, Geert Rosseel, Omar Ahmad, Gleb Gagarin,
    Richard Czekalski, Ashay Rane, Sahil Parmar, Jeff Werner, Jim Sproch, Adrian Macias,
    and Brian Kurtz. 2020. Think Fast: A Tensor Streaming Processor (TSP) for Accelerating
    Deep Learning Workloads. In Proceedings of the 47th ACM/IEEE Annual International
    Symposium on Computer Architecture (ISCA). IEEE, Valencia, Spain, 145–158.

    - <span id="page-12-11"></span>[2] Ravichandra Addanki, Shaileshh Bojja Venkatakrishnan,
    Shreyan Gupta, Hongzi Mao, and Mohammad Alizadeh. 2019. Learning Generalizable
    Device Placement Algorithms for Distributed Machine Learning. In Advances in Neural
    Information Processing Systems (NeurIPS), Hanna M. Wallach, Hugo Larochelle, Alina
    Beygelzimer, Florence d''Alché-Buc, Emily B. Fox, and Roman Garnett (Eds.). OpenReview.net,
    Vancouver, BC, Canada, 3983–3993.

    - <span id="page-12-1"></span>[3] Byung Hoon Ahn, Jinwon Lee, Jamie Menjay Lin,
    Hsin-Pai Cheng, Jilei Hou, and Hadi Esmaeilzadeh. 2020. Ordering Chaos: Memory-Aware
    Scheduling of Irregularly Wired Neural Networks for Edge Devices. In Proceedings
    of Machine Learning and Systems (MLSys), Inderjit S. Dhillon, Dimitris S. Papailiopoulos,
    and Vivienne Sze (Eds.). mlsys.org, Austin, TX, USA, 1–14.

    - <span id="page-12-2"></span>[4] Manoj Alwani, Han Chen, Michael Ferdman, and
    Peter A. Milder. 2016. Fused-layer CNN accelerators. In Proceedings of the 49th
    IEEE/ACM International Symposium on Microarchitecture (MICRO). IEEE Computer Society,
    Taipei, Taiwan, 22:1–22:12.

    - <span id="page-12-9"></span>[5] Arteries. 2022. Arteries IP Homepage. <https://www.arteris.com>.

    - <span id="page-12-3"></span>[6] Ljubisa Bajic and Jasmina Vasiljevic. 2020.
    Compute substrate for Software 2.0. In Proceedings of the IEEE Hot Chips 32 Symposium
    (HCS). IEEE, Palo Alto, CA, USA, 1–31.

    - <span id="page-12-6"></span>[7] Pete Bannon, Ganesh Venkataramanan, Debjit Das
    Sarma, and Emil Talpes. 2019. Computer and Redundancy Solution for the Full Self-Driving
    Computer. In Proceedings of the IEEE Hot Chips 31 Symposium (HCS). IEEE, Cupertino,
    CA, USA, 1–22.

    - <span id="page-12-7"></span>[8] John Burgess. 2019. RTX ON - The NVIDIA TURING
    GPU. In Proceedings of the IEEE Hot Chips 31 Symposium (HCS). IEEE, Cupertino,
    CA, USA, 1–27.

    - <span id="page-12-12"></span>[9] Xuyi Cai, Ying Wang, Kaijie Tu, Chengsi Gao,
    and Lei Zhang. 2022. Olympus: Reaching Memory-Optimality on DNN Processors. IEEE
    Transactions on Computers (TC) 71, 8 (2022), 1939–1951.

    - <span id="page-12-8"></span>[10] Prasanth Chatarasi, Hyoukjun Kwon, Angshuman
    Parashar, Michael Pellauer, Tushar Krishna, and Vivek Sarkar. 2022. Marvel: A
    Data-Centric Approach for Mapping Deep Learning Operators on Spatial Accelerators.
    ACM Transactions on Architecture and Code Optimization 19, 1 (2022), 6:1–6:26.

    - <span id="page-12-4"></span>[11] Karam Chatha. 2021. Qualcomm® Cloud Al-100:
    12TOPS/W Scalable, High Performance and Low Latency Deep Learning Inference Accelerator.
    In Proceedings of the IEEE Hot Chips 33 Symposium (HCS). IEEE, Palo Alto, CA,
    USA, 1–19.

    - <span id="page-12-0"></span>[12] Xiaoming Chen, Yinhe Han, and Yu Wang. 2020.
    Communication Lower Bound in Convolution Accelerators. In Proceedings of the IEEE
    International Symposium on High Performance Computer Architecture (HPCA). IEEE,
    San Diego, CA, USA, 529–541.

    - <span id="page-12-10"></span>[13] Yu-Hsin Chen, Joel S. Emer, and Vivienne Sze.
    2016. Eyeriss: A Spatial Architecture for Energy-Efficient Dataflow for Convolutional
    Neural Networks. In Proceedings of the ACM/IEEE Annual International Symposium
    on Computer Architecture (ISCA). IEEE Computer Society, Seoul,


    South Korea, 367–379.


    - <span id="page-13-22"></span>[14] Zidong Du, Robert Fasthuber, Tianshi Chen,
    Paolo Ienne, Ling Li, Tao Luo, Xiaobing Feng, Yunji Chen, and Olivier Temam. 2015.
    ShiDianNao: shifting vision processing closer to the sensor. In Proceedings of
    the ACM/IEEE Annual International Symposium on Computer Architecture (ISCA). ACM,
    Portland, OR, USA, 92–104.

    - <span id="page-13-23"></span>[15] Clément Farabet, Berin Martini, B. Corda,
    Polina Akselrod, Eugenio Culurciello, and Yann LeCun. 2011. NeuFlow: A runtime
    reconfigurable dataflow processor for vision. In Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition (CVPR) Workshops. IEEE Computer Society,
    Colorado Springs, CO, USA, 109–116.

    - <span id="page-13-26"></span>[16] Kaijie Feng, Xiaoya Fan, Jianfeng An, Xiping
    Wang, Kaiyue Di, Jiangfei Li, Minghao Lu, and Chuxi Li. 2021. ERDSE: efficient
    reinforcement learning based design space exploration method for CNN accelerator
    on resource limited platform. Graphics and Visual Computing 4 (2021), 1–11.

    - <span id="page-13-0"></span>[17] Ken-ichi Funahashi. 1989. On the approximate
    realization of continuous mappings by neural networks. Neural Networks 2, 3 (1989),
    183–192.

    - <span id="page-13-11"></span>[18] Mingyu Gao, Xuan Yang, Jing Pu, Mark Horowitz,
    and Christos Kozyrakis. 2019. TANGRAM: Optimized Coarse-Grained Dataflow for Scalable
    NN Accelerators. In Proceedings of the International Conference on Architectural
    Support for Programming Languages and Operating Systems (ASPLOS). ACM, Providence,
    RI, USA, 807–820.

    - <span id="page-13-25"></span>[19] Yuanxiang Gao, Li Chen, and Baochun Li. 2018.
    Spotlight: Optimizing Device Placement for Training Deep Neural Networks. In Proceedings
    of the 35th International Conference on Machine Learning (ICML) (Proceedings of
    Machine Learning Research, Vol. 80), Jennifer G. Dy and Andreas Krause (Eds.).
    PMLR, Stockholm, Sweden, 1662–1670.

    - <span id="page-13-4"></span>[20] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and
    Sun Jian. 2016. Deep Residual Learning for Image Recognition. In Proceedings of
    the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE Computer
    Society, Las Vegas, NV, USA, 770–778.

    - <span id="page-13-17"></span>[21] Kartik Hegde, Po-An Tsai, Sitao Huang, Vikas
    Chandra, Angshuman Parashar, and Christopher W. Fletcher. 2021. Mind mappings:
    enabling efficient algorithm-accelerator mapping space search. In Proceedings
    of the 26th ACM International Conference on Architectural Support for Programming
    Languages and Operating Systems (ASPLOS), Tim Sherwood, Emery D. Berger, and Christos
    Kozyrakis (Eds.). ACM, Virtual Event, USA, 943–958.

    - <span id="page-13-1"></span>[22] Kurt Hornik, Maxwell B. Stinchcombe, and Halbert
    White. 1989. Multilayer feedforward networks are universal approximators. Neural
    Networks 2, 5 (1989), 359–366.

    - <span id="page-13-5"></span>[23] Qijing Huang, Aravind Kalaiah, Minwoo Kang,
    James Demmel, Grace Dinh, John Wawrzynek, Thomas Norell, and Yakun Sophia Shao.
    2021. CoSA: Scheduling by Constrained Optimization for Spatial Accelerators. In
    Proceedings of the ACM/IEEE Annual International Symposium on Computer Architecture
    (ISCA). IEEE, Valencia, Spain, 554–566.

    - <span id="page-13-9"></span>[24] Drago Ignjatovic, Daniel W. Bailey, and Ljubisa
    Bajic. 2022. The Wormhole AI Training Processor. In Proceedings of the IEEE International
    Solid-State Circuits Conference (ISSCC). IEEE, San Francisco, CA, USA, 356–358.

    - <span id="page-13-19"></span>[25] Abhinav Jangda and Uday Bondhugula. 2018.
    An effective fusion and tile size model for optimizing image processing pipelines.
    In Proceedings of the 23rd ACM SIGPLAN Symposium on Principles and Practice of
    Parallel Programming (PPoPP), Andreas Krall and Thomas R. Gross (Eds.). ACM, Vienna,
    Austria, 261–275.

    - <span id="page-13-15"></span>[26] Yang Jiao, Liang Han, Rong Jin, Yi-Jung Su,
    Chiente Ho, Li Yin, Yun Li, Long Chen, Zhen Chen, Lu Liu, Zhuyu He, Yu Yan, Jun
    He, Jun Mao, Xiaotao Zai, Xuejun Wu, Yongquan Zhou, Mingqiu Gu, Guocai Zhu, Rong
    Zhong, Wenyuan Lee, Ping Chen, Yiping Chen, Weiliang Li, Deyu Xiao, Qing Yan,
    Mingyuan Zhuang, Jiejun Chen, Yun Tian, Yingzi Lin, Wei Wu, Hao Li, and Zesheng
    Dou. 2020. A 12nm Programmable Convolution-Efficient Neural-Processing-Unit Chip


    Achieving 825TOPS. In Proceedings of the IEEE International Solid-State Circuits
    Conference (ISSCC). IEEE, San Francisco, CA, USA, 136–140.


    - [27] Yang Jiao, Liang Han, and Xin Long. 2020. Hanguang 800 NPU The Ultimate
    AI Inference Solution for Data Centers. In Proceedings of the IEEE Hot Chips 32
    Symposium (HCS). IEEE, Palo Alto, CA, USA, 1–29.

    - <span id="page-13-16"></span>[28] Norman P. Jouppi, Doe Hyun Yoon, Matthew Ashcraft,
    Mark Gottscho, Thomas B. Jablin, George Kurian, James Laudon, Sheng Li, Peter
    C. Ma, Xiaoyu Ma, Thomas Norrie, Nishant Patil, Sushma Prasad, Cliff Young, Zongwei
    Zhou, and David A. Patterson. 2021. Ten Lessons From Three Generations Shaped
    Google''s TPUv4i : Industrial Product. In Proceedings of the 48th ACM/IEEE Annual
    International Symposium on Computer Architecture (ISCA). IEEE, Valencia, Spain,
    1–14.

    - <span id="page-13-24"></span>[29] Sheng-Chun Kao, Xiaoyu Huang, and Tushar Krishna.
    2022. DNNFuser: Generative Pre-Trained Transformer as a Generalized Mapper for
    Layer Fusion in DNN Accelerators. arXiv preprint arXiv:2201.11218 abs/2201.11218
    (2022), 1–8.

    - <span id="page-13-18"></span>[30] Sheng-Chun Kao and Tushar Krishna. 2020. GAMMA:
    Automating the HW Mapping of DNN Models on Accelerators via Genetic Algorithm.
    In Proceedings of the IEEE/ACM International Conference On Computer Aided Design
    (ICCAD). IEEE, San Diego, CA, USA, 44:1–44:9.

    - <span id="page-13-21"></span>[31] Sheng-Chun Kao and Tushar Krishna. 2022. MAGMA:
    An Optimization Framework for Mapping Multiple DNNs on Multiple Accelerator Cores.
    In IEEE International Symposium on High-Performance Computer Architecture, (HPCA).
    IEEE, Seoul, South Korea, 814–830.

    - <span id="page-13-6"></span>[32] Sheng-Chun Kao, Michael Pellauer, Angshuman
    Parashar, and Tushar Krishna. 2022. DiGamma: Domain-aware Genetic Algorithm for
    HW-Mapping Co-optimization for DNN Accelerators. In Proceedings of the Design,
    Automation & Test in Europe Conference & Exhibition (DATE), Cristiana Bolchini,
    Ingrid Verbauwhede, and Ioana Vatajelu (Eds.). IEEE, Antwerp, Belgium, 232–237.

    - <span id="page-13-20"></span>[33] Scott Kirkpatrick, D. Gelatt Jr., and Mario
    P. Vecchi. 1983. Optimization by Simmulated Annealing. Sci. 220, 4598 (1983),
    671–680.

    - <span id="page-13-12"></span>[34] Simon Knowles. 2017. Scalable Silicon Compute.
    In Workshop on Deep Learning At Supercomputer Scale, NIPS. OpenReview.net, Long
    Beach, CA, USA, 1–22.

    - <span id="page-13-13"></span>[35] Simon Knowles. 2021. Graphcore. In Proceedings
    of the IEEE Hot Chips 33 Symposium (HCS). IEEE, Palo Alto, CA, USA, 1–25.

    - <span id="page-13-3"></span>[36] Alex Krizhevsky, Ilya Sutskever, and Geoffrey
    E. Hinton. 2012. ImageNet Classification with Deep Convolutional Neural Networks.
    In Proceedings of the 26th Annual Conference on Neural Information Processing
    Systems (NIPS). Curran Associates, Inc., Lake Tahoe, Nevada, United States, 1106–1114.

    - <span id="page-13-7"></span>[37] Hyoukjun Kwon, Prasanth Chatarasi, Michael
    Pellauer, Angshuman Parashar, Vivek Sarkar, and Tushar Krishna. 2019. Understanding
    Reuse, Performance, and Hardware Cost of DNN Dataflow: A Data-Centric Approach.
    In Proceedings of the IEEE/ACM International Symposium on Microarchitecture (MICRO).
    ACM, Columbus, OH, USA, 754–768.

    - <span id="page-13-8"></span>[38] Juhyoung Lee, Dongjoo Shin, Jinsu Lee, Jinmook
    Lee, Sanghoon Kang, and Hoi-Jun Yoo. 2019. A Full HD 60 fps CNN Super Resolution
    Processor with Selective Caching based Layer Fusion for Mobile Devices. In Proceedings
    of the Symposium on VLSI Circuits. IEEE, Kyoto, Japan, 302–303.

    - <span id="page-13-2"></span>[39] Grzegorz Lewicki and Giuseppe Marino. 2004.
    Approximation of functions of finite variation by superpositions of a Sigmoidal
    function. Appl. Math. Lett. 17, 10 (2004), 1147–1152.

    - <span id="page-13-10"></span>[40] Heng Liao, Jiajin Tu, Jing Xia, Hu Liu, Xiping
    Zhou, Honghui Yuan, and Yuxing Hu. 2021. Ascend: a Scalable and Unified Architecture
    for Ubiquitous Deep Neural Network Computing : Industry Track Paper. In Proceedings
    of the IEEE International Symposium on High-Performance Computer Architecture,
    HPCA. IEEE, Seoul, South Korea, 789–801.

    - <span id="page-13-14"></span>[41] Heng Liao, Jiajin Tu, Jing Xia, and Xiping
    Zhou. 2019. DaVinci: A Scalable Architecture for Neural Network Computing. In
    Proceedings


    Cocco: Hardware-Mapping Co-Exploration towards Memory ... ASPLOS ''24, April 27-May
    1, 2024, La Jolla, CA, USA


    of the IEEE Hot Chips 31 Symposium (HCS). IEEE, Cupertino, CA, USA, 1–44.


    - <span id="page-14-19"></span>[42] Xinhan Lin, Shouyi Yin, Fengbin Tu, Leibo
    Liu, Xiangyu Li, and Shaojun Wei. 2018. LCP: a layer clusters paralleling mapping
    method for accelerating inception and residual networks on FPGA. In Proceedings
    of the 55th Annual Design Automation Conference (DAC). ACM, San Francisco, CA,
    USA, 16:1–16:6.

    - <span id="page-14-7"></span>[43] Wenyan Lu, Guihai Yan, Jiajun Li, Shijun Gong,
    Yinhe Han, and Xiaowei Li. 2017. FlexFlow: A Flexible Dataflow Accelerator Architecture
    for Convolutional Neural Networks. In Proceedings of the IEEE International Symposium
    on High Performance Computer Architecture (HPCA). IEEE Computer Society, Austin,
    TX, USA, 553–564.

    - <span id="page-14-8"></span>[44] Yufei Ma, Yu Cao, Sarma B. K. Vrudhula, and
    Jae-sun Seo. 2017. Optimizing Loop Operation and Dataflow in FPGA Acceleration
    of Deep Convolutional Neural Networks. In Proceedings of the ACM/SIGDA International
    Symposium on Field-Programmable Gate Arrays (FPGA). ACM, Monterey, CA, USA, 45–54.

    - <span id="page-14-0"></span>[45] Marvin Minsky and Seymour Papert. 1987. Perceptrons
    - an introduction to computational geometry. MIT Press, .

    - <span id="page-14-24"></span>[46] Bert Moons, Roel Uytterhoeven, Wim Dehaene,
    and Marian Verhelst. 2017. Envision: A 0.26-to-10TOPS/W subword-parallel dynamicvoltage-accuracy-frequency-scalable
    Convolutional Neural Network processor in 28nm FDSOI. In Proceedings of the IEEE
    International Solid-State Circuits Conference (ISSCC). IEEE, San Francisco, CA,
    USA, 246–247.

    - <span id="page-14-21"></span>[47] Ravi Teja Mullapudi, Andrew Adams, Dillon
    Sharlet, Jonathan Ragan-Kelley, and Kayvon Fatahalian. 2016. Automatically scheduling
    halide image processing pipelines. ACM Trans. Graph. 35, 4 (2016), 83:1– 83:11.

    - <span id="page-14-12"></span>[48] Thomas Norrie, Nishant Patil, Doe Hyun Yoon,
    George Kurian, Sheng Li, James Laudon, Cliff Young, Norman P. Jouppi, and David
    A. Patterson. 2020. Google''s Training Chips Revealed: TPUv2 and TPUv3. In Proceedings
    of the IEEE Hot Chips 32 Symposium (HCS). IEEE, Palo Alto, CA, USA, 1–70.

    - <span id="page-14-16"></span>[49] NVIDIA. 2018. THE NVIDIA DEEP LEARNING ACCELERATOR.
    In Proceedings of the IEEE Hot Chips 30 Symposium (HCS). IEEE, Cupertino, CA,
    USA, 1–18.

    - <span id="page-14-18"></span>[50] Angshuman Parashar, Priyanka Raina, Yakun
    Sophia Shao, Yu-Hsin Chen, Victor A. Ying, Anurag Mukkara, Rangharajan Venkatesan,
    Brucek Khailany, Stephen W. Keckler, and Joel S. Emer. 2019. Timeloop: A Systematic
    Approach to DNN Accelerator Evaluation. In Proceedings of the IEEE International
    Symposium on Performance Analysis of Systems and Software (ISPASS). IEEE, Madison,
    WI, USA, 304–315.

    - <span id="page-14-25"></span>[51] Angshuman Parashar, Minsoo Rhu, Anurag Mukkara,
    Antonio Puglielli, Rangharajan Venkatesan, Brucek Khailany, Joel S. Emer, Stephen
    W. Keckler, and William J. Dally. 2017. SCNN: An Accelerator for Compressed-sparse
    Convolutional Neural Networks. In Proceedings of the 44th Annual International
    Symposium on Computer Architecture (ISCA). ACM, Toronto, ON, Canada, 27–40.

    - <span id="page-14-23"></span>[52] Alec Radford and Karthik Narasimhan. 2018.
    Improving Language Understanding by Generative Pre-Training. In Preprint. OpenAI,
    , 1– 12.

    - <span id="page-14-5"></span>[53] Esteban Real, Alok Aggarwal, Yanping Huang,
    and Quoc V. Le. 2019. Regularized Evolution for Image Classifier Architecture
    Search. In Proceedings of the 33rd Conference on Artificial Intelligence (AAAI).
    AAAI Press, Honolulu, Hawaii, USA, 4780–4789.

    - <span id="page-14-1"></span>[54] Frank Rosenblatt. 1957. The perceptron, a perceiving
    and recognizing automaton Project Para. Cornell Aeronautical Laboratory, .

    - <span id="page-14-4"></span>[55] Mark Sandler, Andrew G. Howard, Menglong Zhu,
    Andrey Zhmoginov, and Liang-Chieh Chen. 2018. MobileNetV2: Inverted Residuals
    and Linear Bottlenecks. In Proceedings of the IEEE Conference on Computer Vision
    and Pattern Recognition (CVPR). Computer Vision Foundation / IEEE Computer Society,
    Salt Lake City, UT, USA, 4510–4520.

    - <span id="page-14-20"></span>[56] Yakun Sophia Shao, Jason Clemons, Rangharajan
    Venkatesan, Brian Zimmer, Matthew Fojtik, Nan Jiang, Ben Keller, Alicia Klinefelter,
    Nathaniel Pinckney, Priyanka Raina, Stephen G. Tell, Yanqing Zhang, William J.
    Dally, Joel Emer, C. Thomas Gray, Brucek Khailany, and Stephen W. Keckler. 2019.
    Simba: Scaling Deep-Learning Inference with Multi-Chip-Module-Based Architecture.
    In Proceedings of the IEEE/ACM International Symposium on Microarchitecture (MICRO).
    ACM, Columbus, OH, USA, 14–27.

    - <span id="page-14-2"></span>[57] Karen Simonyan and Andrew Zisserman. 2015.
    Very Deep Convolutional Networks for Large-Scale Image Recognition. In Proceedings
    of the International Conference on Learning Representations (ICLR). Computational
    and Biological Learning Society, San Diego, CA, USA, 1–14.

    - <span id="page-14-26"></span>[58] Zhuoran Song, Bangqi Fu, Feiyang Wu, Zhaoming
    Jiang, Li Jiang, Naifeng Jing, and Xiaoyao Liang. 2020. DRQ: Dynamic Region-based
    Quantization for Deep Neural Network Acceleration. In Proceedings of the 47th
    ACM/IEEE Annual International Symposium on Computer Architecture (ISCA). IEEE,
    Valencia, Spain, 1010–1021.

    - <span id="page-14-3"></span>[59] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre
    Sermanet, Scott E. Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke,
    and Andrew Rabinovich. 2015. Going deeper with convolutions. In Proceedings of
    the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE Computer
    Society, Boston, MA, USA, 1–9.

    - <span id="page-14-13"></span>[60] Emil Talpes, Douglas Williams, and Debjit
    Das Sarma. 2022. DOJO: The Microarchitecture of Tesla''s Exa-Scale Computer. In
    Proceedings of the IEEE Hot Chips 34 Symposium (HCS). IEEE, Cupertino, CA, USA,
    1–28.

    - <span id="page-14-9"></span>[61] Zhanhong Tan, Hongyu Cai, Runpei Dong, and
    Kaisheng Ma. 2021. NN-Baton: DNN Workload Orchestration and Chiplet Granularity
    Exploration for Multichip Accelerators. In Proceedings of the IEEE Annual International
    Symposium on Computer Architecture (ISCA). IEEE, Valencia, Spain, 1013–1026.

    - <span id="page-14-27"></span>[62] Jakub Tarnawski, Amar Phanishayee, Nikhil
    R. Devanur, Divya Mahajan, and Fanny Nina Paravecino. 2020. Efficient Algorithms
    for Device Placement of DNN Graph Operators. In Advances in Neural Information
    Processing Systems (NeurIPS), Hugo Larochelle, Marc''Aurelio Ranzato, Raia Hadsell,
    Maria-Florina Balcan, and Hsuan-Tien Lin (Eds.). Open-Review.net, Virtual, 1–13.

    - <span id="page-14-14"></span>[63] Tenstorrent. 2021. Grayskull. <https://tenstorrent.com/grayskull/>.

    - <span id="page-14-22"></span>[64] Ashish Vaswani, Noam Shazeer, Niki Parmar,
    Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin.
    2017. Attention is All you Need. In Advances in Neural Information Processing
    Systems (NIPS), Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach,
    Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (Eds.). OpenReview.net, Long
    Beach, CA, USA, 5998–6008.

    - <span id="page-14-17"></span>[65] Ofri Wechsler, Michael Behar, and Bharat Daga.
    2019. Spring Hill (NNP-I 1000) Intel''s Data Center Inference Chip. In Proceedings
    of the IEEE Hot Chips 31 Symposium (HCS). IEEE, Cupertino, CA, USA, 1–12.

    - <span id="page-14-10"></span>[66] Jian Weng, Sihao Liu, Vidushi Dadu, Zhengrong
    Wang, Preyas Shah, and Tony Nowatzki. 2020. DSAGEN: Synthesizing Programmable
    Spatial Accelerators. In Proceedings of the 47th ACM/IEEE Annual International
    Symposium on Computer Architecture (ISCA). IEEE, Valencia, Spain, 268–281.

    - <span id="page-14-11"></span>[67] Qingcheng Xiao, Size Zheng, Bingzhe Wu, Pengcheng
    Xu, Xuehai Qian, and Yun Liang. 2021. HASCO: Towards Agile HArdware and Software
    CO-design for Tensor Computation. In Proceedings of the 48th ACM/IEEE Annual International
    Symposium on Computer Architecture (ISCA). IEEE, Valencia, Spain, 1055–1068.

    - <span id="page-14-6"></span>[68] Saining Xie, Alexander Kirillov, Ross B. Girshick,
    and Kaiming He. 2019. Exploring Randomly Wired Neural Networks for Image Recognition.
    In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV).
    IEEE, Seoul, South Korea, 1284–1293.

    - <span id="page-14-15"></span>[69] Andrew Yang. 2019. Deep Learning Training
    At Scale Spring Crest Deep Learning Accelerator (Intel® Nervana™ NNP-T). In Proceedings
    of the IEEE Hot Chips 31 Symposium (HCS). IEEE, Cupertino, CA, USA,


    <span id="page-15-0"></span>1–20.


    - <span id="page-15-2"></span>[70] Xuan Yang, Mingyu Gao, Qiaoyi Liu, Jeff Setter,
    Jing Pu, Ankita Nayak, Steven Bell, Kaidi Cao, Heonjae Ha, Priyanka Raina, Christos
    Kozyrakis, and Mark Horowitz. 2020. Interstellar: Using Halide''s Scheduling Language
    to Analyze DNN Accelerators. In Proceedings of the International Conference on
    Architectural Support for Programming Languages and Operating Systems (ASPLOS).
    ACM, Lausanne, Switzerland, 369–383.

    - <span id="page-15-6"></span>[71] Size Zheng, Renze Chen, Anjiang Wei, Yicheng
    Jin, Qin Han, Liqiang Lu, Bingyang Wu, Xiuhong Li, Shengen Yan, and Yun Liang.
    2022. AMOS: enabling automatic mapping for tensor computations on spatial accelerators
    with hardware abstraction. In Proceedings of the 49th Annual International Symposium
    on Computer Architecture (ISCA). ACM, New York, New York, USA, 874–887.

    - <span id="page-15-4"></span>[72] Shixuan Zheng, Xianjue Zhang, Leibo Liu, Shaojun
    Wei, and Shouyi Yin. 2022. Atomic Dataflow based Graph-Level Workload Orchestration
    for Scalable DNN Accelerators. In Proceedings of the IEEE International Symposium
    on High-Performance Computer Architecture (HPCA). IEEE, Seoul, South Korea, 475–489.

    - <span id="page-15-3"></span>[73] Shixuan Zheng, Xianjue Zhang, Daoli Ou, Shibin
    Tang, Leibo Liu, Shaojun Wei, and Shouyi Yin. 2020. Efficient Scheduling of Irregular
    Network Structures on CNN Accelerators. IEEE Transactions on Computer-Aided Design
    of Integrated Circuits and Systems (TCAD) 39, 11 (2020), 3408–3419.

    - <span id="page-15-5"></span>[74] Brian Zimmer, Rangharajan Venkatesan, Yakun
    Sophia Shao, Jason Clemons, Matthew Fojtik, Nan Jiang, Ben Keller, Alicia Klinefelter,
    Nathaniel Ross Pinckney, Priyanka Raina, Stephen G. Tell, Yanqing Zhang, William
    J. Dally, Joel S. Emer, C. Thomas Gray, Stephen W. Keckler, and Brucek Khailany.
    2019. A 0.11 pJ/Op, 0.32-128 TOPS, Scalable Multi-Chip-Module-based Deep Neural
    Network Accelerator with Ground-Reference Signaling in 16nm. In Proceedings of
    the IEEE Symposium on VLSI Circuits (VLSI). IEEE, Kyoto, Japan, 300.

    - <span id="page-15-1"></span>[75] Barret Zoph, Vijay Vasudevan, Jonathon Shlens,
    and Quoc V. Le. 2018. Learning Transferable Architectures for Scalable Image Recognition.
    In IEEE Conference on Computer Vision and Pattern Recognition, (CVPR). Computer
    Vision Foundation / IEEE Computer Society, Salt Lake City, UT, USA, 8697–8710.'
  decisions:
    evaluation_prompt: 'Qualified. Reason: The paper mentions experiments and provides
      quantitative analysis, such as reducing costs by specific percentages and comparing
      results with other methods.'
    related_work_prompt: 'Qualified. Reason: The paper engages with prior research
      by citing numerous research papers, explaining and comparing to cited work,
      and includes a section discussing related work.'
    novelty_prompt: 'Qualified. Reason: The paper introduces a new hardware-mapping
      co-exploration framework called Cocco, which leverages graph-level features
      to optimize memory capacity and communication, demonstrating novelty.'
    review_only_prompt: 'Qualified. Reason: The paper introduces new contributions,
      specifically a hardware-mapping co-exploration framework called Cocco, and does
      not primarily summarize existing work.'
