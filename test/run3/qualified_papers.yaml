papers:
- title: A modular architecture for IMU-based data gloves
  abstract: ''
  keywords: ''
  document: '# A modular architecture for IMU-based data gloves


    Alessandro Carfì [,](https://orcid.org/0000-0001-9208-6910) Mohamad Alame[h](https://orcid.org/0000-0002-6345-8313)
    , Valerio Belcamino [,](https://orcid.org/0000-0002-9264-8191) and Fulvio Mastrogiovann[i](https://orcid.org/0000-0001-5913-1898)


    Department of Informatics, Bioengineering, Robotics, and Systems Engineering,
    University of Genoa, Via Opera Pia 13, 16145 Genoa, Italy alessandro.carfi@dibris.unige.it


    Abstract. The flexibility and range of motion in human hands play a crucial role
    in human interaction with the environment and have been studied across different
    fields. Researchers explored various technological solutions for gathering information
    from the hands. These solutions include tracking hand motion through cameras or
    wearable sensors and using wearable sensors to measure the position and pressure
    of contact points. Data gloves can collect both types of information by utilizing
    inertial measurement units, flex sensors, magnetic trackers for motion tracking,
    and force resistors or touch sensors for contact measurement. Although there are
    commercially available data gloves, researchers often create custom data gloves
    to achieve the desired flexibility and control over the hardware. However, the
    existing literature lacks standardization and the reuse of previously designed
    data gloves. As a result, many gloves with unclear characteristics exist, which
    makes replication challenging and negatively impacts the reproducibility of studies.
    This work proposes a modular, open hardware and software architecture for creating
    customized data gloves based on IMU technology. We also provide an architecture
    implementation along with an experimental protocol to evaluate device performance.


    Keywords: Data Glove · Hand Tracking · Inertial Measuerment Unit.


    ## 1 Introduction


    Human hands'' flexibility and range of motion are fundamental for how humans interact
    with each other and the world. Many research communities have studied human hand
    motion, focusing on the hand''s motion and its interaction with the environment
    [\[1\]](#page-4-0). Hand studies typically include information about hand kinematics
    and sensory input. Hand kinematics are well-described, with minimal variation
    among individuals, except for bone proportions [\[3\]](#page-4-1). On the other
    hand, humans primarily rely on two senses when using their hands: proprioception
    and touch. Proprioception helps determine limb position, while touch provides
    information about forces and points of contact. Depending on the type of study,
    researchers may require information from one or both of the hand senses. As


    #### 2 A. Carfì et al.


    ![](_page_1_Figure_1.jpeg)


    <span id="page-1-0"></span>Fig. 1. The image compares the architecture on the
    left with the implementation on the right. The core module is in blue, while the
    sensory module is in red.


    a result, various technological solutions have been explored. In terms of proprioception,
    tracking the hand''s motion has been achieved through the use of cameras or wearable
    sensors. Instead, for touch, although a few attempts have been made to estimate
    them using cameras [\[5\]](#page-4-2), the position and pressure of contacts are
    primarily measured using wearable sensors. The data glove is a wearable device
    embedding sensors that can collect all the previously defined information [\[4\]](#page-4-3).
    The sensors embedded in a data glove can be adapted to meet the application requirements.
    Inertial Measurement Units (IMU) [\[7\]](#page-4-4), Flex sensors [\[8\]](#page-4-5),
    and magnetic trackers [\[2\]](#page-4-6) are the most common choices for tracking
    motion, while force sensing resistors [\[10\]](#page-4-7) or touch sensors can
    be used to measure contacts [\[9\]](#page-4-8). While various data gloves are
    commercially available [\[6\]](#page-4-9), researchers often opt to construct
    custom data gloves to achieve the desired flexibility and control over the hardware.
    However, the existing literature lacks standardization and the re-use of previously
    designed devices. As a result, many gloves with unclear characteristics have been
    developed, making replication challenging and affecting the reproducibility of
    the studies for which they were created. This work aims to introduce a modular,
    open hardware and software architecture for creating customized data gloves based
    on IMU technology.


    ## 2 Hardware Architecture


    To provide maximum flexibility, a data glove should support a variable IMUs number
    to adapt to the application''s requirements, work without needing external equipment,
    and be easy to repair and reproduce. Our architecture addresses these requirements
    by defining two modules, displayed in Figure [1:](#page-1-0) core and sensory
    modules. The core module includes the MCU for data collection and processing,
    an antenna to send data to a PC, interfaces for connecting other modules, a battery
    for power and an IMU. The sensory module contains an IMU and the communication
    interface with the core module. Each module has a structure that includes the
    basic functionalities of an IMU data glove. However, each component can be expanded
    to incorporate more functionalities while keeping the main structure intact. The
    only technical constraint is the communication interface, which should remain
    fixed to ensure compatibility across


    ![](_page_2_Figure_1.jpeg)


    <span id="page-2-0"></span>Fig. 2. On the left, we have the dataglove worn by
    a human, and on the right, it is mounted on the AR10 Robotic hand.


    different implementations of this architecture. The communication interface has
    two key components: the communication protocol and the physical medium. The two
    most commonly adopted communication protocols for digital sensors are Serial Peripheral
    Interface (SPI) and Inter Integrated Circuit (I2C). SPI is typically used for
    communication between components on the printed circuit board (PCB). It is more
    vulnerable to noise and requires more communication lanes. Instead, I2C allows
    the same physical lane sharing across multiple devices without additional selection
    lanes and is less affected by noise. Therefore, we have chosen I2C as the communication
    protocol for the modules in our hardware architecture. Instead, in the literature,
    solutions for the physical medium often involved soldering cables or using flexible
    PCBs. However, these options make it hard to modify the number of sensors and
    increase the complexity of reproducing and repairing the device. To solve these
    issues, we opted for using a Flexible Flat Cable (FFC) connector as the physical
    interface for the modules and FFC cables to connect them. This solution allows
    easy addition of extra sensory modules or replacement of faulty modules. Additionally,
    FFC cables are flexible and do not restrict hand motion.


    ## 3 Implementation


    The implementation of the two modules can be seen in Figure [1](#page-1-0) while
    the device is pictured in Figure [2.](#page-2-0) The sensory module design includes
    a SparkFun IMU Breakout, which embeds the MPU-9250 from InvenSense, and a custom-designed
    shield to support FFC connection and daisy chaining. Although the I2C protocol
    only requires four lanes (ground, power, SCL, and SDA), we opted for a six-lane
    FFC connector and cable for future expansion. Each I2C lane can connect only two
    sensors since the MPU-9250 only offers two selectable addresses. The two sensory
    modules of each I2C lane can be worn on the proximal and intermediate phalanges
    to monitor the motion of a single finger. Instead, the core module is


    #### 4 A. Carfì et al.


    placed on the back of the hand and consists of a custom PCB to ensure a compact
    design. This PCB includes an ESP32 with a WiFi antenna, 7 FFC connectors, an MPU-9250
    IMU, and an I2C multiplexer. Each finger requires a separate I2C lane for motion
    tracking, so we allocated one FFC connector for each lane. For symmetry and compatibility
    with both right and left hands, we included two connectors in symmetric positions
    for the thumb. Since the ESP32 has only two I2C controllers, but the system requires
    6 I2C lanes (one for each finger and one for the hand back), the design incorporates
    an I2C multiplexer to manage all the communication lanes. Finally, the core module
    also includes a connector for battery power. Our software consists of two modules.
    The first module runs on the ESP32 and collects data from connected sensors, sending
    them via UDP communication. The data includes the sensor''s unique ID, accelerometer
    and gyroscope readings, and orientations in quaternions. The ESP32 uses the I2C
    multiplexer to collect data from each sensor module. The sensor orientation is
    estimated using a data fusion process run by the MPU-9250''s digital motion processor.
    The second module runs on the PC and receives sensory data through UDP communication,
    acting as a driver.


    ## 4 Results


    The purpose of the experimental setup is to demonstrate the general functionalities
    of the data glove. For maximum reproducibility and accuracy, we installed our
    device on an AR10 hand from Active8, mounted on the Baxter manipulator from Rethink
    Robotics, see Figure [2.](#page-2-0) The first test assessed the autonomy and
    acquisition frequencies of the data glove under static conditions. Equipped with
    eleven IMUs and powered by a 220 mAh 3.7V battery, the glove had an average autonomy
    of 62.89 minutes (SD = 4.89) and transmitted data with a frequency of 21.8 Hz
    (SD = 9.47) across six independent tests. In the same static conditions, we measured
    the drifting of the sensors'' estimated orientations over time. The root mean
    square error (RMSE), averaged across all sensors, was 8.91 degrees (SD = 3.89)
    after 30 minutes. We also conducted experiments involving random movements of
    the robot''s hand and arm. Each experiment lasted 45 minutes and was repeated
    five times. The overall RMSE averaged across all sensors and trials was 9.17 degrees
    (STD = 9.30).


    ## 5 Conclusions


    This article presents a modular architecture for an IMU-based dataglove and its
    early implementation. The device, equipped with a small battery, can transmit
    data from eleven sensors at a frequency higher than 20Hz for over an hour. Furthermore,
    tests conducted under unfavourable conditions, without proper calibration or drifting
    compensation, demonstrated a reasonably accurate tracking of motions. The error
    in dynamic conditions is not significantly different from that in stationary conditions,
    as shown in the result sections. This result suggests that most tracking errors
    are due to sensor drifting, which can be compensated for with appropriate software
    solutions. The proposed device represents an initial attempt to provide an easily
    reproducible and modular platform for IMU-based hand tracking. Its extensibility
    offers opportunities for future research to propose new versions or develop more
    accurate tracking software solutions.


    Acknowledgments. This work is supported by the CHIST-ERA (2014-2020) project InDex
    and received funding from the Italian Ministry of Education and Research (MIUR).
    This work has been also made with the Italian government support under the National
    Recovery and Resilience Plan (NRRP), Mission 4, Component 2 Investment 1.5, funded
    from the European Union NextGenerationEU.


    Disclosure of Interests. The authors have no competing interests to declare that
    are relevant to the content of this article.


    ## References


    - <span id="page-4-0"></span>1. Carfì, A., Patten, T., Kuang, Y., Hammoud, A.,
    Alameh, M., Maiettini, E., Weinberg, A.I., Faria, D., Mastrogiovanni, F., Alenyà,
    G., et al.: Hand-object interaction: From human demonstrations to robot manipulation.
    Frontiers in Robotics and AI 8, 714023 (2021)

    - <span id="page-4-6"></span>2. Cazacu, E., van der Grinten, C., Bax, J., Baeten,
    G., Holtkamp, F., Lee, C.: A position sensing glove to aid ankle-foot orthosis
    diagnosis and treatment. Sensors 21(19), 6631 (2021)

    - <span id="page-4-1"></span>3. Cobos, S., Ferre, M., Uran, M.S., Ortego, J.,
    Pena, C.: Efficient human hand kinematics for manipulation tasks. In: 2008 IEEE/RSJ
    International Conference on Intelligent Robots and Systems. pp. 2246–2251. IEEE
    (10 2008)

    - <span id="page-4-3"></span>4. Dipietro, L., Sabatini, A.M., Dario, P.: A survey
    of glove-based systems and their applications. Ieee transactions on systems, man,
    and cybernetics, part c (applications and reviews) 38(4), 461–482 (2008)

    - <span id="page-4-2"></span>5. Grady, P., Tang, C., Brahmbhatt, S., Twigg, C.D.,
    Wan, C., Hays, J., Kemp, C.C.: Pressurevision: Estimating hand pressure from a
    single rgb image. In: European Conference on Computer Vision. pp. 328–345. Springer
    (2022)

    - <span id="page-4-9"></span>6. He, K., Choosri, N.: Commercial data glove selection
    for vr-based hand rehabilitation gaming project. In: 2023 Joint International
    Conference on Digital Arts, Media and Technology with ECTI Northern Section Conference
    on Electrical, Electronics, Computer and Telecommunications Engineering (ECTI
    DAMT & NCON). pp. 177–182. IEEE (2023)

    - <span id="page-4-4"></span>7. Huang, H., Liang, Z., Sun, F., Dong, M., et al.:
    Virtual interaction and manipulation control of a hexacopter through hand gesture
    recognition from a data glove. Robotica 40(12), 4375–4387 (2022)

    - <span id="page-4-5"></span>8. Luo, Y., Chen, X., Li, X., Tian, H., Li, S., Wang,
    L., He, J., Yang, Z., Shao, J.: Heterogeneous strain distribution based programmable
    gated microchannel for ultrasensitive and stable strain sensing. Advanced Materials
    p. 2207141 (2022)

    - <span id="page-4-8"></span>9. Maiolino, P., Mastrogiovanni, F., Cannata, G.,
    et al.: Skinning a robot: Design methodologies for large-scale robot skin. IEEE
    Robotics & Automation Magazine 23(4), 150–159 (2016)

    - <span id="page-4-7"></span>10. Wang, J., Li, B., Li, Z., Zubrycki, I., Granosik,
    G.: Grasping behavior of the human hand during tomato picking. Computers and Electronics
    in Agriculture 180, 105901 (2021)'
  decisions:
    evaluation_prompt: 'Qualified. Reason: The paper includes a "Results" section
      that provides structured evaluation through experiments, presenting quantifiable
      outcomes such as autonomy, acquisition frequencies, and root mean square error
      (RMSE) measurements.'
    related_work_prompt: 'Qualified. Reason: The paper meaningfully engages with prior
      research by citing various studies related to hand kinematics, sensor technologies,
      and data gloves. It discusses the existing literature''s lack of standardization
      and compares its proposed modular architecture to previous solutions, highlighting
      the novelty and advantages of their approach.'
    novelty_prompt: 'Qualified. Reason: The paper proposes a modular, open hardware
      and software architecture for creating customized data gloves based on IMU technology,
      which is a novel contribution. The authors clearly state their contribution
      by introducing a new architecture that addresses the lack of standardization
      and reusability in existing data glove designs. Additionally, the paper provides
      an implementation and experimental protocol, further supporting its novelty.'
    review_only_prompt: 'Qualified. Reason: The paper proposes a modular, open hardware
      and software architecture for creating customized data gloves based on IMU technology.
      It includes an architecture implementation, an experimental protocol to evaluate
      device performance, and presents original experiments and results, indicating
      novel contributions.'
- title: 'WideSA: A High Array Utilization Mapping Scheme for Uniform Recurrences
    on ACAP'
  abstract: ''
  keywords: Mapping, Re-configurable Array Architecture, Versal ACAP
  document: '#### I. INTRODUCTION


    Modern heterogeneous FPGA architectures, like AMD/Xilinx Versal Adaptive Compute
    Acceleration Platform (ACAP) [\[1\]](#page-5-0), combine AI Engines (AIEs) with
    programmable logic (PL) to boost applications in the AI and intelligent signal
    processing domains. In these domains, uniform recurrences [\[2\]](#page-5-1),
    which comprise nested loops with uniform dependencies, are prevalent types of
    computations. Regrettably, there is currently a lack of established development
    methodologies for efficiently mapping large-scale uniform recurrences onto the
    Versal ACAP architecture with high utilization of AI Engines.


    The ACAP architecture comprise an array of several hundred AIE cores, such as
    8×50 in the VC1902 architecture [\[3\]](#page-5-2), interconnected through a mesh
    network-on-chip (NoC). Each AIE core consists of vector processing and load/store
    units, functioning as a very-long-instruction-word (VLIW) [\[4\]](#page-5-3) processor
    to deliver high-performance vectorized computations. To facilitate communication
    among the AIE cores, the NoC is utilized for inter-core communication, enabling
    efficient data transfers between cores. Moreover, neighboring cores utilize shared
    buffers, providing higher bandwidth for data exchange. When it comes to data transfer
    to and from the AIEs, there are hundreds of I/O ports available, supporting terabytes
    of bandwidth.


    As ACAP demonstrates a remarkable capacity for intense computation, developing
    acceleration designs on the architecture has become an urgent trend in recent
    times. However, current efforts have not succeeded in achieving high utilization
    of the AIE array. For example, Vitis-AI [\[5\]](#page-5-4) introduces the DPU
    [\[6\]](#page-5-5) for the VC1902 architecture, but only accomplishes a 64% AIE
    utilization. There are several ongoing challenges associated with developing designs
    with high array utilization on the Versal ACAP architecture:


    - Increased programming complexity: Higher AIE utilization results in more cores
    that need to be programmed with certain intrinsics. In some situations, different
    cores execute different programs, necessitating significant human effort to develop
    such accelerators.

    - Increased placement and routing difficulty: Mapping computations onto the Versal
    ACAP architecture with high utilization of AIEs often necessitates careful placement
    and routing of AIEs and data communications. From the perspective of AIE compilation,
    attaining high AIE utilization typically results in difficulties in placing cores
    and buffers, as well as routing streaming communications on the NoC. For example,
    CHARM [\[7\]](#page-5-6) struggles to compile large designs on Vitis 2022.1.

    - Extended compilation time: The default compilation tools provided by AMD/Xilinx
    Vitis employ ILP algorithms to find placement and routing solutions. Consequently,
    a larger number of cores results in a longer time to find a legal solution.


    To address these challenges, we propose WideSA, a high array utilization mapping
    scheme for uniform recurrences on the Versal ACAP architecture. By leveraging
    the AIE array architecture, we apply space-time transformation and loop nest transformation
    using the polyhedral model, generating systolic-like mappings on the AIE array.
    On one hand, systolic designs assign similar workloads to different cores, enabling
    us to reuse a single core program and thereby reduce human


    <span id="page-1-0"></span>![](_page_1_Figure_0.jpeg)


    Fig. 1: Versal ACAP Architecture


    effort. On the other hand, systolic designs regularize both the placement and
    communication of cores, simplifying the placement and routing process. Additionally,
    we designed a routingaware PLIO assignment algorithm to improve the success rate
    of compilation. We also developed an automatic framework to generate the corresponding
    code for heterogeneous backends, including AIEs, PL, and host. In the evaluation
    section, we demonstrate the effectiveness of WideSA by successfully implementing
    executable acceleration systems for various uniform recurrences, accommodating
    different data types. Our approach achieves high throughput with high utilization
    of AIEs.


    We summarize our contributions as follows:


    - We propose a mapping scheme, based on the polyhedral model, for uniform recurrences
    that generates a systolic design on ACAP with high AIE utilization.

    - We design a routing-aware PLIO assignment algorithm that takes into account
    the characteristics of systolic mappings and the AIE architecture, thereby facilitating
    an efficient compilation process.

    - We develop an automatic framework that generates corresponding code for heterogeneous
    backends based on the mapping results.

    - We achieve high throughput across different computations and data types, outperforming
    state-of-the-art methods.


    #### II. BACKGROUND


    #### *A. Versal ACAP Architecture and Workflow*


    *1) Hardware Features:* AMD/Xilinx has developed the Versal ACAP architecture
    to cater to the increasing demands of next-generation wireless processing and
    machine learning applications. Figure [1](#page-1-0) illustrates the detailed
    architecture of VCK5000, an evaluation kit for Versal ACAP, comprising the CPU,
    PL, and AIE components. The AIE array on VCK5000 consists of 8 × 50 AIE cores,
    with each core capable of generating 128 MACs of int8 data type every cycle at
    a frequency of 1 GHz or higher. Moreover, the AIE cores operate in single-instruction-multiple-data
    (SIMD) mode using


    <span id="page-1-1"></span>TABLE I: Different Data Communication Bandwidth on
    the Versal ACAP Architecture


    | Methods        | Frequency | Bitwidth | Channels | Total      |

    |----------------|-----------|----------|----------|------------|

    | AIE DMA        | 1.25 GHz  | 256 bits | 400      | 15.6 TB/s  |

    | AIE NoC Stream | 1.25 GHz  | 32 bits  | 400      | 1.95 TB/s  |

    | PLIO-PL        | 1.25 GHz  | 128 bits | 78       | 1.52 TB/s  |

    | GMIO-DRAM      | 1.25 GHz  | 64 bits  | 16       | 0.125 TB/s |

    | PL-DRAM        | 0.50 GHz  | -        | 4        | 0.100 TB/s |


    a VLIW pattern, enabling acceleration of a large number of vectorized computations.


    In Figure [1,](#page-1-0) we identify five data transfer methods, including those
    within the AIE array and among the AIE, PL, and DRAM components. These methods
    are referred to as AIE DMA, AIE NoC stream, PLIO-PL, PL-DRAM, and GMIO-DRAM interfaces.
    We profile these data transfer methods on VCK5000 and present the results in Table
    [I.](#page-1-1) Within the AIE array, each AIE core has direct memory access (DMA)
    ports connected to four neighboring local buffers with a width of 256 bits. Using
    the AIE DMA method, a total data transfer rate of up to 15.6 TB/s can be achieved.
    Furthermore, each AIE core is linked to the NoC through a stream interface with
    a width of 32 bits. The data transfer bandwidth through the AIE NoC stream method
    reaches a maximum of 2 TB/s, which is lower compared to the DMA method. The PLIO
    ports, responsible for data communication between the PL and AIE array, can provide
    a maximum bandwidth of 1.52 TB/s. Based on the profiling results, utilizing the
    AIE DMA method for data transfer proves beneficial in overcoming communication
    bottlenecks, aligning with the dataflow in systolic array designs. In terms of
    data communication with DRAM, the bandwidth is approximately 0.1 TB/s, significantly
    lower than the on-chip data transfer methods. This observation inspires us to
    exploit data locality within computations to enhance overall performance.


    *2) Software Programming Model:* AMD/Xilinx offers a development tool for AIEs
    and Versal ACAP integrated into Vitis. The programming model [\[8\]](#page-5-7)
    designed for AIEs consists of two levels: a graph program across the AIE array
    with each node representing an AIE kernel program. The graph program represents
    the dataflow information among AIE kernels and between the AIE and I/O ports.
    The compiler in Vitis transforms the dataflow graph into a subnetwork of physical
    AIE cores, determines the placement of buffers, and configures NoC stream routing.
    Since placement and routing are NP-hard problems, the compiler employs ILP solvers
    to process these two phases. However, as the design scale increases and AIE utilization
    becomes high, finding a legal solution efficiently becomes challenging for the
    solvers [\[9\]](#page-5-8). To address this, incorporating constraints for placement
    and routing helps alleviate the congestions and accelerates the solvers in finding
    solutions. The systolic design scheme provides a regular pattern for placement
    and routing, which is suitable for constructing these constraints.


    ## *B. Uniform Recurrences and Systolic Array Mapping*


    Uniform recurrences refer to computations that consist of nested loops, where
    all dependencies are uniform. These types of computations are commonly found in
    AI and signal processing applications, such as matrix multiplication, 2D convolution,
    FIR filtering, and so on. Several prior works [\[10\]](#page-5-9)– [\[12\]](#page-5-10)
    have focused on generating systolic array designs for uniform recurrences on FPGAs,
    employing the polyhedral model for loop transformations to explore successful
    mappings. The polyhedral model [\[13\]](#page-5-11), [\[14\]](#page-5-12) serves
    as a compilation framework for loop transformation, encompassing space-time transformation,
    latency hiding, SIMD vectorization, fusion, and more. A legal combination of these
    transformations represents a schedule within the polyhedral model, and the goal
    of systolic design mapping is to find the optimal schedule.


    An AIE kernel handles more computations compared to a PE in typical systolic arrays.
    Additionally, specific hardware features of the AIE array differ from those of
    common systolic arrays. As a result, the mapping problem on the Versal ACAP architecture
    is not a straightforward systolic array mapping. Consequently, it is necessary
    to model corresponding transformations and constraints within the polyhedral model,
    an area that has not yet been extensively researched.


    ## III. SYSTOLIC MAPPING SCHEME ON ACAP


    #### *A. Kernel Scope Demarcation*


    According to the programming model of AIEs, it is necessary to demarcate the scope
    of codes mapped to execute on a single AIE core and the outer loop nests to be
    mapped to the AIE array. This demarcation allows us to decompose the mapping problem
    into graph-level mapping and kernel-level mapping, which are independent of each
    other after selecting tiling factors.


    Polygonal tiling [\[15\]](#page-5-13), [\[16\]](#page-5-14), an effective solution
    for workload partitioning in uniform polyhedral domains, plays a crucial role
    in determining the innermost and outer loop nests for tiling. We illustrate the
    tiling process using the MM example with (N0, M0, K0) as the tiling factors, involving
    loop re-indexing, tiling, and rewriting, as depicted in Figure [2.](#page-2-0)
    Building on prior works, we consider the specific features of the AIE array when
    performing the demarcation.


    #### *B. Systolic Mapping Generation*


    To generate systolic array designs on the AIE array following kernel scope demarcation,
    we utilize the polyhedral


    <span id="page-2-0"></span>![](_page_2_Figure_9.jpeg)


    Fig. 2: Kernel Scope Demarcation


    <span id="page-2-1"></span>![](_page_2_Figure_11.jpeg)


    Fig. 3: Polyhedral Model-Based Systolic Mapping


    model, drawing inspiration from AutoSA [\[11\]](#page-5-15), to facilitate loop
    transformations. To be specific, we employ four types of transformation techniques,
    as depicted in Figure [3.](#page-2-1)


    *1) Space-time Transformation:* The first step involves performing space-time
    transformation to map the graph-level loop nests to a systolic array design. We
    identify loops in the outermost loop band with dependence distances no greater
    than one and consider them as candidate space loops. Subsequently, we enumerate
    all possible combinations of space loops from the candidate pool. The selected
    space loops are then permuted in the outermost position, while the loops below
    them are designated as time loops. Due to the constraints imposed by the hardware
    shape of the AIE array, the mapper generates only 1D and 2D systolic arrays. This
    step results in the generation of multiple systolic arrays, each with a unique
    schedule. As shown in Figure [3,](#page-2-1) we choose loops i and j as the space
    loops (on dark gray background) and loop k as the time loop (on light gray background)
    in the MM example.


    *2) Array Partition:* To accommodate the limited number of AIEs in the horizontal
    and vertical directions of the AIE array, array partitioning becomes necessary
    when mapping a large array. In order to achieve this, we apply tiling to the outermost
    permutable loop that contains the space loops. In Figure [3,](#page-2-1) we illustrate
    an example where we tile the outermost loop band in the MM example using the tiling
    factors (N1, M1, K1). The point loops originating from the original loops are
    retained as the space loops. This results in a 2D systolic array with dimensions
    of N<sup>1</sup> × M<sup>1</sup> (on dark gray background).


    *3) Latency Hiding:* Latency hiding plays a crucial role in mitigating the pipeline
    stalls caused by loop-carried dependencies in computational statements. In the
    case of the MM example, the accumulate operations in the statement introduce loop-carried
    dependence within the loop, resulting in long latency in the systolic chain. To
    address this issue, we identify parallel loops in the polyhedral model schedules,
    applies tiling to these loops, and permutes the point loops to the innermost position.
    As an illustration, loops i and j are identified as parallel loops in the MM example.
    We extract them using the tiling factors (N2, M2) and permute the point loops
    to the innermost position. Since there are no loop-carried dependencies on the
    innermost loop, the latency of design reduce as the chain length shortened.


    <span id="page-3-0"></span>![](_page_3_Figure_0.jpeg)


    Fig. 4: Communication Methods for PLIO Ports Utilization Reduction


    *4) Multiple Threading:* As AIE cores execute concurrently, the AIE array inherently
    supports multiple threading. Leveraging this characteristic, utilizing multiple
    AIEs to execute the same instructions but different indexing can significantly
    enhance overall performance. We identify parallelizable loops in the time loops
    that do not have data dependence. In the MM example, the loop k is identified
    as a parallelizable loop. We can apply tiling to this loop using the factors K2.
    The point loop is permuted to the innermost position and completely unrolled to
    generate multiple threads of AIEs.


    #### *C. Placement and Routing Constraints Construction*


    The systolic design generated in the previous section represents an abstract mapping
    scheme. Consequently, it is essential to use the space loops as input and generate
    an actual mapped graph for AIE array that considers placement and routing constraints.
    The mapped graph consists of nodes, representing AIE cores and input/output ports,
    and edges, which connect the ports of the nodes. The placement and routing constraints
    involve assigning coordinates to the AIE cores, buffers, and input/output ports,
    as well as determining the routing paths for the edges. In the subsequent subsections,
    we introduce the graph builder and routing-aware PLIO assignment, which are responsible
    for constructing the mapped graph and generating the associated placement and
    routing constraints, respectively.


    *1) Graph Builder:* To construct the mapped graph, we iterate through all coordinates
    in the space loops and create a node for each pair of coordinates in the 2D systolic
    array, representing an AIE core. Next, we identify the data communications between
    AIE cores based on the dependencies within the space loops. Following the definitions
    in AutoSA [\[11\]](#page-5-15), there are three types of data dependences:


    - Read dependence: Transfer the read-only data.

    - Flow dependence: Transfer the intermediate data.

    - Output dependence: Transfer the output-only data.


    Based on these data dependences and the space loops, we define the I/O ports and
    edge directions. Since AIEs do not support intermediate results between different
    iterations, we treat flow dependences as input dependencies when constructing
    I/O ports. The polyhedral model for the array access to matrix A in the MM recurrences
    is {i, j, k} → {i, j+1, k}, and when loops j, k are the space loops, the direction
    is (1, 0). We connect the input ports from the corresponding nodes with a constant
    and non-zero distant direction.


    As for the output ports, the boundary input ports, and the zero distant direction
    ports, we create PLIO ports as the other end of the connection edge. To adhere
    to the limitation on the number of PLIO ports, we utilize packet-switch communications
    and broadcast communications to reduce the number of used ports, as depicted in
    Figure [4.](#page-3-0)


    *2) Routing-Aware PLIO Assignment:* Once we have the mapped graph, we search for
    available cores on the AIE array to place the AIE kernels. To facilitate efficient
    communication between neighboring cores, we assign the buffers of ports connecting
    these cores to the shared buffer of the cores, forming part of the placement constraints.
    These constraints enable the transformation of the kernels'' placement into a
    regular duplicate pattern of a single kernel.


    Aside from facilitating neighboring communication, it is necessary to construct
    paths between PLIO ports and AIE cores for data input and output. Considering
    the mesh structure of the NoC on the AIE array, and given that PLIOs are always
    located in Row 0, we can compute the routing congestion by counting the horizontal
    data transfer numbers. For instance, we compute the congestion for the *west*
    direction as follows:


    $$\operatorname{Cong}\_{i}^{\text{west}} = \sum\_{p \in \text{PL.IOs}, x \in \text{AIEs}}
    W\_{i}[p][x],$$


    $$W\_i[p][x] = \begin{cases} 1 & (p\_{\text{col}} < i \text{ and } x\_{\text{col}}
    > i \text{ and } (x, p) \in \text{Edges}) \text{ or } \\ & (p\_{\text{col}} >
    i \text{ and } x\_{\text{col}} < i \text{ and } (p, x) \in \text{Edges}) \\\ 0
    & \text{Otherwise} \end{cases}$$


    where pcol and xcol represent the column coordinates of PLIO p and AIE x, respectively.


    The computation of the congestion for the *east* direction is symmetrical.


    Consequently, the routing challenges essentially transform into issues of PLIO
    assignment. We formulate the assignment of PLIO ports as a satisfiability problem
    subject to routing resource constraints. We check if there exists a set of values
    for PLIOs that satisfies the following constraints:


    $$\forall i \in \text{Columns}, \mathsf{Cong}\_i^{\text{west}} \le \mathsf{RC}\_{\text{west}},
    \quad \mathsf{Cong}\_i^{\text{east}} \le \mathsf{RC}\_{\text{east}}$$


    where RC*west* and RC*east* denote the available routing resources in the AIE
    array.


    To seek the feasible assignment of PLIO ports, we employ a heuristic greedy algorithm
    outlined in Algorithm [1.](#page-4-0) In this algorithm, we initialize the placement
    of the PLIO ports by calculating the median value of the row numbers of the connected
    AIE cores. If the initially computed placement coordinate is not available, we
    search for the nearest available coordinate instead. This heuristic greedy algorithm
    balances the routing congestion among the PLIO ports. By considering the connectivity
    with the AIE cores, it generates an optimal placement for the PLIO ports, ensuring
    successful routing on the NoC. The algorithm takes into account the availability
    of coordinates and selects the most suitable placement to minimize congestion.


    By generating these constraints for the placement and routing of AIE kernels,
    buffers, and PLIO ports, we can


    <span id="page-4-0"></span>Algorithm 1 Routing-Aware PLIO Assignment Algortihm


    | Algorithm 1 Routing-Aware PLIO Assignment Algortihm         |

    |-------------------------------------------------------------|

    | Require: Numbers of PLIO ports N, AIE cores X               |

    | Ensure: Initialized PLIO assignment set P                   |

    | 1: Initialization available placement sets A as all columns |

    | that have PLIO ports.                                       |

    | 2: for i ← 1 to N do                                        |

    | S = [], num = 0<br>3:                                       |

    | for x ∈ X do<br>4:                                          |

    | if (p, x) ∈ Edges then<br>5:                                |

    | S.append(xcol)<br>6:                                        |

    | num+ = 1<br>7:                                              |

    | end if<br>8:                                                |

    | end for<br>9:                                               |

    | Sort S to find the median: sort(S, S + num)<br>10:          |

    | P[i] = find nearest(A, S[num/2])<br>11:                     |

    | remove(A, P[i])<br>12:                                      |

    | 13: end for                                                 |

    | 14: return<br>P                                             |


    significantly simplify the task for the AIE compiler. These constraints provide
    valuable information and guidelines for the compilers to optimize the placement
    and routing process, ultimately leading to a high utilization of the AIE array.


    #### IV. AUTOMATIC MAPPING FRAMEWORK


    To facilitate the computation of uniform recurrence, we have developed an automatic
    mapping framework that implements the full functional modules on the Versal ACAP
    architecture, as shown in Figure [5.](#page-4-1)


    <span id="page-4-1"></span>![](_page_4_Figure_5.jpeg)


    Fig. 5: Overview of WideSA Automatic Framework


    Specifically, we introduce a kernel-level mapper, a DMA module constructor, and
    a host program generator, which work in conjunction with the kernel scope and
    graph mapper described in the previous section.


    The kernel-level mapper and optimizer transform the C++ program into a program
    with AIE intrinsics, leveraging the capabilities of the AIE vector processor to
    exploit parallelism and optimize performance. Moreover, we design the architecture
    of efficient DMA modules, which serve as the buffers of AIEs on the PL, in the
    DMA module constructor. This architecture is tailored to the characteristics of
    both the hardware and the computations involved. In addition, we engineer a host
    program generator to generate a controller program that oversees global scheduling.


    TABLE II: Evaluation Benchmarks


    <span id="page-4-2"></span>


    | Benchmarks | Dimension    | Problem Size          | Data Types |

    |------------|--------------|-----------------------|------------|

    |            |              | [8192, 8192, 8192]    | Float      |

    |            |              | [10240, 10240, 10240] | Int8       |

    | MM         | [i, j, k]    | [9600, 9600, 9600]    | Int16      |

    |            |              | [8192, 8192, 8192]    | Int32      |

    | 2D-Conv    |              | [10240, 10240, 4, 4]  | Float      |

    |            | [h, w, p, q] | [10240, 10240, 8, 8]  | Int8       |

    |            |              | [10240, 10240, 4, 4]  | Int16      |

    |            |              | [10240, 10240, 4, 4]  | Int32      |

    |            |              | [8192, 8192]          | Cfloat     |

    | 2D-FFT     | [row, col]   | [8192, 8192]          | Cint16     |

    |            |              | [1048576, 15]         | Float      |

    | FIR Filter |              | [1048576, 15]         | Int8       |

    |            | [n, taps]    | [1048576, 15]         | Int16      |

    |            |              | [1048576, 15]         | Cfloat     |


    #### V. EVALUATION


    #### *A. Benchmark and Experimental Setup*


    In this section, we select four representative uniform recurrences with various
    data types as benchmarks to evaluate the performance of WideSA. The selected benchmarks
    include matrix multiplication (MM), 2D convolution (2D-Conv), 2D Fast Fourier
    Transformation (2D-FFT), and FIR filter [\[17\]](#page-5-16). The problem sizes
    and corresponding data types are provided in Table [II.](#page-4-2) Here, Cfloat
    refers to the complex float data type and Cint16 refers to the complex 16-bit
    integer data type. All the experiments are conducted on VCK5000 with 250 MHz on
    PL and 1.25 GHz on AIE. AMD/Xilinx Vitis 2022.1 is used as the compilation backend
    tool.


    ### *B. Full System Performance*


    We conducted a comparison of the throughput between WideSA and other state-of-the-art
    AIE designs for the same problem size. For the MM benchmark, we successfully compiled
    the CHARM code [\[7\]](#page-5-6) for the target VCK5000 with AMD/Xilinx Vitis
    2022.1, incorporating placement and routing constraints, as the baseline. As for
    the 2D-Conv benchmark, we selected the released 8-PEs version of Vitis-AI DPU
    [\[5\]](#page-5-4) which only supports Int8 data type, utilizing 256 AIEs running
    at 1.33 GHz and the PL at 350 MHz, as the baseline. Furthermore, we used the open-source
    designs from the Vitis DSP Library [\[18\]](#page-5-17) as the baselines for the
    2D-FFT and FIR filter benchmarks.


    The results presented in Table [III](#page-5-18) demonstrate that WideSA achieves
    significantly higher throughput with high utilization of AIEs. Additionally, we
    computed the AIE efficiency by considering the throughput and the number of used
    AIEs. The results indicate that WideSA maintains similar efficiency to [\[7\]](#page-5-6)
    for MM, as both approaches exhibit AIE utilization over 95%. When compared to
    the baselines with lower AIE utilizations, WideSA trades AIE efficiency (TOPS/#AIEs)
    for a high overall performance (TOPS) and is bounded by memory bandwidth.


    Moreover, we conducted a comparison of the performance and energy efficiency of
    MM using WideSA and PL-only designs on the VCK5000 target, which has 1968 DSP58
    IPs at total. For the PL-only designs, we utilize AutoSA [\[11\]](#page-5-15)
    as the systolic array generator. The results presented in Table [IV](#page-5-19)


    TABLE III: Comparison of Throughput and AIE Efficiency on Benchmarks


    <span id="page-5-18"></span>


    | Method   | Metric     |       |       | MM    |       | 2D-Conv |       |       |
    2D-FFT |        | FIR Filter |       |       |       |        |

    |----------|------------|-------|-------|-------|-------|---------|-------|-------|--------|--------|------------|-------|-------|-------|--------|

    |          | Data type  | Float | Int8  | Int16 | Int32 | Float   | Int8  | Int16
    | Int32  | Cfloat | Cint16     | Float | Int8  | Int16 | Cfloat |

    |          | #AIEs      | 384   | 384   | 384   | 384   | -       | 256   | -     |
    -      | 10     | 10         | 10    | 10    | 10    | 10     |

    | Baseline | TOPS       | 3.73  | 29.78 | 7.82  | 3.72  | -       | 31.40 | -     |
    -      | 0.04   | 0.13       | 0.15  | 2.56  | 0.62  | 0.15   |

    |          | TOPS/#AIEs | 0.010 | 0.077 | 0.020 | 0.010 | -       | 0.123 | -     |
    -      | 0.004  | 0.013      | 0.015 | 0.256 | 0.062 | 0.015  |

    |          | #AIES      | 400   | 400   | 400   | 400   | 400     | 400   | 400   |
    400    | 320    | 320        | 256   | 256   | 256   | 256    |

    | WideSA   | TOPS       | 4.15  | 32.49 | 8.10  | 3.92  | 4.50    | 36.02 | 10.35
    | 4.48   | 1.10   | 3.83       | 2.92  | 39.3  | 9.47  | 2.89   |

    |          | TOPS/#AIEs | 0.010 | 0.081 | 0.020 | 0.010 | 0.011   | 0.090 | 0.025
    | 0.011  | 0.003  | 0.012      | 0.012 | 0.100 | 0.037 | 0.011  |


    <span id="page-5-19"></span>TABLE IV: MM Performance Comparison between PL-only
    and WideSA Design


    |              |       |       | PL-only |       | WideSA |       |       |       |  |

    |--------------|-------|-------|---------|-------|--------|-------|-------|-------|--|

    | Data Type    | Float | Int8  | Int16   | Int32 | Float  | Int8  | Int16 | Int32
    |  |

    | DSPs         | 1536  | 1528  | 1516    | 1536  | 152    | 60    | 67    | 65    |  |

    | #AIEs        | 0     | 0     | 0       | 0     | 400    | 400   | 400   | 400   |  |

    | TOPS         | 0.59  | 5.77  | 2.16    | 0.60  | 4.15   | 32.49 | 8.10  | 3.92  |  |

    | Power (W)    | 19.5  | 18.8  | 18.6    | 19.5  | 55.8   | 54.4  | 54.9  | 55.6  |  |

    | TOPS/W       | 0.03  | 0.31  | 0.12    | 0.03  | 0.07   | 0.60  | 0.15  | 0.07  |  |

    | Norm. TOPS/W | 1.00x | 1.00x | 1.00x   | 1.00x | 2.25x  | 1.94x | 1.29x | 2.25x
    |  |


    demonstrate that our approach achieves up to 2.25× higher energy efficiency compared
    to the PL-only designs.


    #### *C. Scalability of WideSA on MM examples*


    We evaluate the scalability of WideSA while increasing AIE utilization and analyze
    how various factors influence performance. The results, presented in Figure [6,](#page-5-20)
    show a significant increase in throughput as the number of AIEs increases. In
    addition, the AIE efficiency results demonstrate that our approach scales effectively
    from small-scale to largescale designs. However, when the number exceeds 200,
    the efficiency of a single AIE core decreases due to the memorybound condition
    caused by the number of PLIOs and the size of the PL buffer. The increase in PLIO
    numbers and buffer sizes leads to increased throughput, suggesting that enhancing
    the bandwidth between different fabrics of ACAP can improve performance. This
    indicates that managing the resources and data flow between different components
    of the ACAP is crucial for achieving better performance.


    #### VI. CONCLUSION


    In this paper, we present a high array utilization mapping scheme for uniform
    recurrences on the Versal ACAP architecture. Additionally, we propose several
    optimizations aimed at enhancing overall performance within an automatic mapping
    framework. Through extensive evaluations using typical benchmarks and diverse
    data types, we assess the efficiency of the WideSA framework. In the future work,
    we aim to integrate WideSA into the MLIR-AIE workflow and develop an end-toend
    compilation tool that incorporates automatic design space exploration.


    #### ACKNOWLEDGEMENT


    This work was partly supported by the National Natural Science Foundation of China
    (Grant No. 62090021) and the National Key R&D Program of China (Grant No. 2022YFB4500500).


    <span id="page-5-20"></span>![](_page_5_Figure_11.jpeg)


    Fig. 6: Throughput Evaluation of Different AIE Numbers, PLIO Numbers, and PL Buffer
    Sizes


    #### REFERENCES


    - <span id="page-5-0"></span>[1] AMD/Xilinx. Versal Adaptive Compute Acceleration
    Platform.

    - <span id="page-5-1"></span>[2] R. M. Karp *et al.*, "The organization of computations
    for uniform recurrence equations," *J. ACM*, 1967.

    - <span id="page-5-2"></span>[3] S. Ahmad *et al.*, "Xilinx first 7nm device:
    Versal AI Core (VC1902)," in *HCS*, 2019.

    - <span id="page-5-3"></span>[4] J. A. Fisher, "Very long instruction word architectures
    and the ELI-512," in *ISCA*, 1983.

    - <span id="page-5-4"></span>[5] Vitis AI Library User Guide. [Online]. Available:
    [https://xilinx.github.](https://xilinx.github.io/Vitis-AI/) [io/Vitis-AI/](https://xilinx.github.io/Vitis-AI/)

    - <span id="page-5-5"></span>[6] X. Jia *et al.*, "XVDPU: A high performance CNN
    accelerator on the Versal platform powered by the AI Engine," in *FPL*, 2022.

    - <span id="page-5-6"></span>[7] J. Zhuang *et al.*, "CHARM: Composing heterogeneous
    accelerators for matrix multiply on Versal ACAP architecture," in *FPGA*, 2023.

    - <span id="page-5-7"></span>[8] AMD/Xilinx. AI Engine Kernel and Graph Programming
    Guide.

    - <span id="page-5-8"></span>[9] W. Cook *et al.*, "An exact rational mixed-integer
    programming solver," in *IPCO*, 2011.

    - <span id="page-5-9"></span>[10] J. Cong *et al.*, "PolySA: Polyhedral-based
    systolic array autocompilation," in *ICCAD*, 2018.

    - <span id="page-5-15"></span>[11] J. Wang *et al.*, "AutoSA: A polyhedral compiler
    for high-performance systolic arrays on FPGA," in *FPGA*, 2021.

    - <span id="page-5-10"></span>[12] Y.-H. Lai *et al.*, "SuSy: A programming model
    for productive construction of high-performance systolic arrays on FPGAs," in
    *ICCAD*, 2020.

    - <span id="page-5-11"></span>[13] M.-W. Benabderrahmane *et al.*, "The polyhedral
    model is more widely applicable than you think," in *CC*, 2010.

    - <span id="page-5-12"></span>[14] U. Bondhugula, "Compiling affine loop nests
    for distributed-memory parallel architectures," in *SC*, 2013.

    - <span id="page-5-13"></span>[15] R. Andonov *et al.*, "Optimal semi-oblique
    tiling," *IEEE TPDS*, 2003.

    - <span id="page-5-14"></span>[16] C. Rossetti *et al.*, "Algebraic tiling," in
    *IMPACT*, 2023.

    - <span id="page-5-16"></span>[17] K. K. Parhi, "VLSI digital signal processing
    systems: Design and implementation," 2007.

    - <span id="page-5-17"></span>[18] AMD/Xilinx. Vitis DSP Library for digital signal
    processing.'
  decisions:
    evaluation_prompt: 'Qualified. Reason: The paper contains a dedicated "Evaluation"
      section, presents tables and figures with quantifiable outcomes, and discusses
      benchmarks and experimental setups, demonstrating structured evaluation.'
    related_work_prompt: '- Qualified. Reason: The paper meaningfully engages with
      prior research throughout its content. It includes numerous academic citations,
      discusses previous work in the context of its proposed methods, and compares
      its approach to existing solutions. The paper references prior research in sections
      such as the Introduction, Background, and Evaluation, demonstrating a thorough
      engagement with existing literature.'
    novelty_prompt: 'Qualified. Reason: The paper proposes a novel mapping scheme
      called WideSA for high array utilization of uniform recurrences on the Versal
      ACAP architecture. It introduces a routing-aware PLIO assignment algorithm and
      an automatic framework for generating code for heterogeneous backends. The paper
      clearly states its contributions and demonstrates novelty in the proposed methods
      and their applications.'
    review_only_prompt: 'Qualified. Reason: The paper introduces WideSA, a novel high
      array utilization mapping scheme for uniform recurrences on the Versal ACAP
      architecture. It proposes new methods, including a routing-aware PLIO assignment
      algorithm and an automatic framework for code generation, and presents original
      experiments demonstrating the effectiveness of these contributions.'
- title: 'MX: Enhancing RISC-V''s Vector ISA for Ultra-Low Overhead, Energy-Efficient
    Matrix Multiplication'
  abstract: ''
  keywords: RISC-V, Matrix, Vector, Efficiency
  document: '#### I. INTRODUCTION


    The exponential growth of the computational requirements in Machine Learning (ML)
    and Artificial Intelligence (AI) applications is a major challenge for hardware
    architects. The rise of application-specific accelerators [1] and single instruction,
    multiple data (SIMD) programmable systems [2] demonstrates the need for novel
    architectures able to cope with the rising computational demand. Furthermore,
    AI/ML applications also found their way into edge computing, with benefits such
    as higher privacy, user personalization, and lower power consumption. However,
    edge-AI/ML systems have the additional challenge of balancing large computational
    demands against a very tight power envelope and minimal area footprint. The quest
    for energy efficiency and cost (i.e., area) minimization is even more pressing
    today since ML/AI computation at the edge does not only involve inference but
    also training in the so-called AI on Edge [3].


    Matrix Multiplication (MatMul) is a cornerstone in ML and AI, and essential in
    scientific computing, graphics, and Digital Signal Processing (DSP). The importance
    of MatMul is testified by market-leading companies, such as Google, which developed
    the first Tensor-Processing Unit (TPU) in 2015 to accelerate matrix operations
    [4] and updated it in 2018 with an edgeoriented version achieving 4 TOPS within
    a 2 W power envelope [5]. As with other Domain-Specific Accelerators (DSAs) for
    specific neural-network tasks [6], the TPU is an added resource to which a general-purpose
    processor offloads the workload (for example, through a PCIe interface). This
    brings an area and power overhead that is not affordable in constrained systems
    at the edge, especially when they need to compute non-AI tasks as well. Moreover,
    an excessively specialized accelerator risks becoming useless when the ML/AI algorithm
    evolves.


    Most proprietary Instruction Set Architectures (ISAs) offer dedicated matrix extensions,
    such as Arm''s Scalable Matrix Extension (SME), Intel Advanced Matrix Extension
    (AMX), and IBM Matrix-Multiply Assist (MMA). Unluckily, the microarchitectural
    details of the implementations remain company secrets. So far, the RISC-V open-source
    ISA features only a vector extension (RISC-V Vector (RVV)), even though researchers
    developed multiple unofficial AI/matrix extensions. Still, they add tightly coupled
    matrix units [7] or a new matrix register file [8] used only during matrix operations,
    which add area and power consumption.


    RVV recently showed to be a valid solution to efficiently accelerate MatMul while
    keeping a well-known programming model to handle diverse data-parallel workloads,
    also in the embedded domain [9]. Vector processors execute multiple operations
    with one instruction, amortizing its fetch/decode cost. Moreover, they feature
    a Vector Register File (VRF) to buffer the vector elements, decreasing the accesses
    to memory without changing the computational balance for the architecture [10].
    Even if the VRF helps decrease the power associated with the memory accesses,
    it is an additional block at the bottom of the memory hierarchy, one of the key
    drivers for performance and energy efficiency [11]. Its size can be way larger
    than the one of a scalar register file, and it is usually connected to multiple
    functional units in parallel, which leads to energyhungry interconnects. Hence,
    the VRF access-related energy is usually non-negligible [9], [12].


    With this paper, we present Matrix eXtension (MX), a non-intrusive ISA extension
    to RVV that creates a generalpurpose hybrid matrix/vector architecture with minimal
    area impact and superior energy efficiency. To cut the power consumption, we reduce
    the expensive accesses to/from the


    The first two authors contributed equally to this work.


    <sup>©</sup> 2023 IEEE. Personal use of this material is permitted. Permission
    from IEEE must be obtained for all other uses, in any current or future media,
    including reprinting/republishing this material for advertising or promotional
    purposes, creating new collective works, for resale or redistribution to servers
    or lists, or reuse of any copyrighted component of this work in other works.


    VRF by featuring a software-transparent lightweight accumulator close to the processing
    units. MX does not add a matrix unit to the architecture but re-uses the already
    available processing resources to keep the area and energy overhead at its minimum
    and exploit the energy efficiency savings that come from the reduced VRF accesses.


    To validate MX across multiple domains, we add MX to a constrained embedded Dual-Core
    cluster built upon the opensource energy-optimized RVV-based Spatz [9] vector
    processor and to a scaled-up MemPool architecture [13] with 64 Spatz processors
    and implement both systems in a competitive 12-nm technology. We provide a quantitative
    justification of the energy savings and a detailed power, performance, and area
    (PPA) analysis on matrix multiplications on different data precisions, finding
    that our matrix extension can boost not only energy efficiency but also performance.


    With this paper, we present the following contributions:


    - We define MX, a lightweight and non-intrusive ISA extension based on RVV 1.0
    aimed at supporting memory and computational operations directly on matrices.
    MX reduces the power consumption of the architecture with similar or better performance
    by introducing a near-Floating Point Unit (FPU) tile buffer, a per-vector-element
    broadcast system, and minimal modifications to the Vector Load/Store Unit (VLSU).

    - We provide a theoretical justification of the benefits that the ISA has on the
    power consumption when executing a matrix multiplication kernel, effectively reducing
    the expensive VRF accesses.

    - We implement MX on a constrained Dual-Core and a complex 64-core clusters based
    on the energy-efficient RVV vector processor Spatz, and characterize MX''s impact
    on performance and PPA metrics in a 12-nm technology. For less than 3% area overhead,
    we get a maximum of 56% and 25% performance and energy efficiency gains, respectively.


    #### II. ANALYSIS


    In the following, we discuss the tiling of a General Matrix Multiply (GEMM) problem
    through a multi-level memory hierarchy. When C is a zero matrix, GEMM becomes
    a MatMul.


    $$D\_{M \times N} = A\_{M \times K} \cdot B\_{K \times N} + C\_{M \times N} \tag{l}$$


    For convenience, let us consider a memory hierarchy composed of a memory, a VRF,
    and a near-FPU buffer. The memory connects to the VRF, which is connected to a
    buffer that feeds the FPUs, as reported in Figure 1. The following analysis can
    be easily extended to memory hierarchies with a different number of levels.


    #### *A. The tiling problem*


    The lower level of the hierarchy is usually not large enough to keep the input
    and output matrices all at once. Therefore, the matrices are divided into chunks
    (tiles), and the hardware works on one output tile at a time, and the outer product
    algorithm is often used to maximize parallelism. The number of elements


    ![](_page_1_Figure_12.jpeg)


    Figure 1. The tiling problem over a memory hierarchy composed of three levels,
    ending with the processing elements (FPUs).


    transferred between two consecutive levels of the hierarchy impacts both performance
    and power consumption and depends on how the matrices are tiled. Usually, the
    number of transfers is partially encoded in the *arithmetic intensity*, i.e.,
    the total number of operations divided by the total number of Bytes transferred
    between the memory and the core.


    In the following, we provide equations to fine-grain count how many memory accesses
    happen between each pair of consecutive levels of the hierarchy. Each equation
    contains four terms, which correspond to 1) the elements of matrix A, 2) the elements
    of matrix B, 3) the elements of matrix C (or D) from the upper level to the lower
    one (load/fetch), and 4) the elements of the matrix D from the lower level back
    to the upper one (store/write-back).


    In the most generic scenario, without buffering the output tile in the VRF for
    more than updates, the number of elements moved between the memory and the VRF
    is:


    $$\#E\,lm\_{VRF}^{MEM} = \frac{N}{n}MK + \frac{M}{m}NK + \frac{K}{k}MN + \frac{K}{k}MN
    \qquad (2)$$


    Where the A, B, and D (C) matrices stored in memory have sizes , , , and we tile
    the problem between the memory and the VRF with tiles of size , , .


    For each matrix tile, the number of elements exchanged between the VRF and the
    buffer is:


    $$\#Elim\_{BUF}^{VRF} = \frac{n}{n''}mk + \frac{m}{m''}nk + \frac{k}{k''}mn +
    \frac{k}{k''}mn\tag{3}$$


    Where the tiles stored in the VRF have sizes , , , and we sub-tile the problem
    between the VRF and the buffer with sub-tiles of size ′ ′ , ′ ′ , ′ ′ .


    For each matrix sub-tile, the number of elements exchanged between the buffer
    and the FPUs is:


    $$\#E\,l m\_{FPU}^{BUF} = \frac{n''}{t\_B} m'' k'' + \frac{m''}{t\_A} n'' k''
    + k'' m'' n'' + k'' m'' n'' \qquad (4)$$


    Where the sub-tiles stored in the buffer have sizes ′ ′ , ′ ′ , ′ ′ , and we access
    and elements from tiles A and B, respectively.


    ## *B. Total number of transfers*


    To get the total number of transfers between each pair of hierarchy levels, we
    need to take into account how many output tiles and sub-tiles we calculate throughout
    the program.


    Table I NUMBER OF ACCESSES BETWEEN CONSECUTIVE LEVELS OF THE MEMORY HIERARCHY.


    | Ref.           | Metric                                             | A (↓)                                        |
    B (↓)                                       | C, D (↓)                                                             |
    D (↑)                                                                |

    |----------------|----------------------------------------------------|----------------------------------------------|---------------------------------------------|----------------------------------------------------------------------|----------------------------------------------------------------------|

    | 1)<br>2)<br>3) | #ElmMEM<br>VRF<br>#ElmVRF<br>BUF<br>#ElmBUF<br>FPU | N<br>n
    MK<br>N<br>′ MK<br>n<br>N<br>MK<br>tB | M<br>NK<br>m<br>M<br>m′ NK<br>M<br>NK<br>tA
    | K<br>k MN<br>k<br>K<br>k MN<br>′<br>k<br>k′ k<br>K<br>k MN<br>′<br>k | K<br>k
    MN<br>k<br>K<br>k MN<br>′<br>k<br>k′ k<br>K<br>k MN<br>′<br>k |


    (a) ↓ /↑ indicate transfers to a lower/higher level of the memory.


    *a) Memory and VRF:* In the most generic case, we load the C (first iteration)
    and D (from the second iteration on) tiles from memory before consuming the input
    , -sized tiles, and we store the D tile back to memory after updates. Without
    inter-k-tile buffering in the VRF, we load ( ) output tiles with size from the
    memory to the VRF, and we store back the same amount. If we buffer the output
    tiles until they are completely calculated over the whole K dimension, the formula
    simplifies to . Instead, we load a total of input A and B tiles, with sizes ,
    .


    *b) VRF and buffer:* The ′ ′ output sub-tiles of a tile are fetched for ( ′ )
    ′ ′ times from the VRF before consuming the input ′ ′ , ′ ′ -sized sub-tiles in
    the buffer, and writtenback to the VRF for the same number of times if there is
    no inter-k-tile buffering in the buffer. Instead, the input A and B sub-tiles,
    with sizes ′ ′ , ′ ′ , are loaded times. If we keep into account how many times
    each tile is loaded/stored from/to memory, these formulas become ( ) ( ′ ) ′ ′
    for the sub-tiles fetch, the same amount for the sub-tiles writes-back, and ′
    ′ ′ for the A and B sub-tiles fetch.


    We summarize all the transfers across the hierarchy in Table I.


    #### *C. Optimizations*


    *a) Inter-k-buffering:* If the output tile (sub-tile) is buffered in the VRF (buffer)
    until the whole K (k) dimension is traversed and the whole output tile (sub-tile)
    is ready, we can simplify the equations above. If the buffering happens in the
    VRF, = 1 in Table I Ref. 1), while if it happens in the buffer until the whole
    dimension, ′ = 1, in Table I Ref. 2) (if the buffering only happens until the
    whole dimension is traversed, ′ = 1).


    Inter-k-buffering is ultimately limited by the size of the lower memory level,
    which should be able to host the whole output tile (sub-tile) for the whole computation
    on the () dimension. Therefore, keeping the output tile in the buffer for the
    whole dimension requires that = ′ , = ′ . On the other hand, relaxing this constraint,
    e.g. = ′ , = × ′ , allows for fewer overall transfers between the memory and the
    VRF. In this case, the inter-k-buffering can be done only between the memory and
    the VRF.


    *b) C-tile reset:* When C is a zero matrix, it''s possible to avoid loading it
    from memory and initialize the VRF with zeroes or reset the buffer. If we use
    inter-k-buffering and the zero initialization is applied to the VRF and the buffer,
    the third term of the equations related to the load/fetch of matrix C, D, becomes
    zero in Table I Ref. 1) and 2), respectively.


    ![](_page_2_Figure_10.jpeg)


    Figure 2. Spatz''s VLSU, VRF, and VFU with MX architectural schematic.


    #### III. MX IMPLEMENTATION


    #### *A. ISA Extension*


    We implement MX in Spatz [9], an open-source, RVV-based, highly-optimized compact
    vector processor, targeting minimal area and power overhead. Thus, we do not add
    any dedicated matrix units or software-addressable registers, as shown in Figure
    2. MX adds three configure instructions (msettile[m,n,k]), three memory instructions
    (mld.[a,b], mst.c), and two computation instructions (mx[f]macc). The first three
    instructions set up the sub-tile sizes ′ , ′ , ′ on which the matrix instructions
    operate ( ′ ′ = , ′ ′ ≤ , and is the vector length in elements). We enhance the
    VLSU to enable matrix load and store operations, which are composed of multiple
    unitand non-unit-stride memory operations already supported by Spatz. The new
    memory instructions are introduced to load tiles from matrix *A* and *B* and to
    store the computed tile back to memory, while the two computational instructions
    perform a MatMul between the two ′ ′ and ′ ′ sub-tiles, storing the resulting
    ′ ′ sub-tile in the VRF. Close to the FPUs, we introduce a tiny broadcast block
    consisting only of a register and some multiplexers to broadcast single elements
    from the *A* tile across multiple elements of tile *B*. This block increases the
    data reuse of tile *A* at a minimal cost. Finally, we implement a latch-based
    result tile buffer in the vector functional unit (VFU) to reduce energy consumption
    by minimizing VRF accesses by intermediate result accumulation, limiting the buffer
    size to <sup>1</sup> 8 of the VRF size (i.e., = 256) to maintain low energy and
    area overhead. Since Spatz''s VLSU has four parallel memory ports and the buffer
    is constrained in size, ′ , ′ , ′ ∈ {4, 8}.


    #### *B. MX Benefits*


    Table II summarizes the number of elements transferred between consecutive memory
    hierarchies for a baseline vectoronly MatMul and a MX-ready MatMul. The baseline
    employs a traditional scalar-vector algorithm to load *m* scalar elements from
    the input matrix *A* and an *n*-long vector from matrix *B*. The MX-ready configuration
    loads A tiles with size ′ ′ and *B* tiles with size ′ ′ . While the MX algorithm
    does not further sub-tile the tiles on or (*m''* = *m* and *k''* = *k*), it sub-tiles


    | Config                     | Metric                                             |
    A (↓)                                           | B (↓)                                    |
    C, D (↓)                   | D (↑)                       |

    |----------------------------|----------------------------------------------------|-------------------------------------------------|------------------------------------------|----------------------------|-----------------------------|

    | Baseline(a)<br>Baseline(a) | #ElmMEM<br>VRF<br>#ElmVRF<br>FPU                   |
    N<br>n MK<br>N<br>F MK                          | M<br>NK<br>m<br>MNK                      |
    0<br>KMN                   | MN<br>KMN                   |

    | MX<br>MX<br>MX             | #ElmMEM<br>VRF<br>#ElmVRF<br>BUF<br>#ElmBUF<br>FPU
    | N<br>′ MK<br>B×n<br>N<br>′ MK<br>n<br>N<br>F MK | M<br>m′ NK<br>M<br>m′ NK<br>M<br>NK<br>F
    | 0<br>K<br>′ MN<br>k<br>KMN | MN<br>K<br>′ MN<br>k<br>KMN |


    Table II DATA TRANSFERS: MX-READY VS BASELINE.


    (a) Elements from A are loaded/fetched to/from the scalar register file;


    (b) *F* represents the number of FPUs;


    (c) ↓ /↑ indicate transfers to a lower/higher level of the memory.


    along *n* such that *n* = ×*n''*, where ∈ {2, 4}. In the following, we highlight
    the benefits brought by the MX algorithm.


    *1) Matrix A operands:* In the baseline approach, operands from matrix *A* are
    fetched as scalars from the scalar register file and individually forwarded to
    the vector unit. In contrast, the matrix algorithm retrieves multiple elements
    from *A* in a tiled-vector manner, improving the access pattern and enabling the
    data reuse of the *A* tile by means of the broadcast engine.


    *2) Instruction count:* In the baseline algorithm, each vector instruction is
    amortized over operations. With MX, the total number of instructions fetched and
    decoded is lower, as each mxfmacc instruction is amortized over ′ ′ ′ operations,
    ′ ′ = , and ′ > 1. This boosts the SIMD ratio, i.e., the average number of operations
    per instruction.


    *3) Tile window:* The matrix algorithm exploits the *k* dimension to increase
    the size of the tile window when the dimensions *M* and *N* are limited. This
    is especially beneficial as the SIMD ratio is further improved by allowing each
    core to work on a larger output tile window in a multi-core environment when processing
    matrices with a limited dimension.


    *4) Scalar-vector interactions:* With the baseline algorithm, the scalar core
    must remain active to compute operand addresses and forward scalar operands to
    the Vector Processing Unit (VPU). In contrast, MX pushes the whole computation
    to the vector unit, freeing up the scalar core.


    *5) Performance:* The computing performance is significantly impacted by the number
    of data transfers between the memory and the VRF and the related latency. The
    MX-ready VLSU regularizes the memory accesses, which can reduce conflicts in both
    the interconnect and memory banks.


    *6) Energy:* In the VPU, the power consumption of the VRF normally constitutes
    a non-negligible portion of the overall energy usage. MX''s inexpensive broadcast
    engine and tile buffers enhance data reuse for the tiled matrix *A* and reduce
    the VRF access by a ′ factor. Moreover, the reduced instruction count and more
    regular memory access pattern alleviate the pressure on the instruction and data
    memories, further improving the energy efficiency of the overall system.


    #### IV. EXPERIMENT SETUP AND RESULTS


    # *A. Computing Clusters and Methodology*


    We integrate the baseline and the MX-ready versions of the Spatz VPU into two
    floating-point-capable computing clusters: a 64-bit constrained Dual-Core cluster
    for in-depth analysis of various tile and sub-tile configurations, and a 32-bit
    large-scale 64-Core cluster for performance evaluation in a complex system.


    *1) Dual-Core Cluster:* The Dual-Core cluster is a 64 bit shared-L1-memory cluster,
    implemented with 128 KiB of Tightly Coupled Data Memory (TCDM) across 16 Static
    Random-Access Memory (SRAM) banks. This cluster features 2 Snitch cores, each
    controlling a Spatz instance equipped with 4 double-precision FPUs and 2 KiB VRF
    each, supporting a vector length of 512 bits. The peak achievable performance
    is 16 DP−FLOP/cycle.


    *2) 64-Core MemPool Cluster:* MemPool, a large-scale 32 bit shared-L1-memory cluster,
    scales up to 256 RISC-V cores and includes 1 MiB of L1 TCDM [13]. The cluster
    is hierarchically organized into 4 groups, each containing 16 tiles. A fully connected
    logarithmic crossbar is employed between the cores and memories, achieving non-uniform
    memory access (NUMA) with a maximum latency of 5 cycles. We equip each Spatz instance
    with 4 32-bit FPUs and 2 KiB of VRFs each, supporting a vector length of 512 bits,
    and pair each instance with a scalar Snitch core to form a Core Complex (CC).
    This cluster configuration, labeled MemPool64Spatz4, consists of 64 CCs, one for
    each tile, and achieves a peak performance of 512 SP−FLOP/cycle, as detailed further
    in [9].


    We implement our designs in GlobalFoundries'' 12 nm LP-PLUS FinFET technology
    through Synopsys Fusion Compiler 2022.03 for synthesis and Place-and-Route (PnR).
    We analyze the PPA metrics of the MX-ready clusters at the post-PnR implementation
    stage and compare them to their respective non-MX baseline architectures. We calculate
    power consumption using Synopsys'' PrimeTime 2022.03 under typical operating conditions
    (TT/0.80 V/25 °C), with switching activities obtained from QuestaSim 2021.3 post-layout
    gate-level simulations and back-annotated parasitic information. In the used MatMul
    kernels, all the input and output matrices are kept in the L1 memory and each
    core of the cluster calculates one portion of the output matrix. The kernel executes
    in parallel across the entire cluster, partitioning the matrix equally among multiple
    cores. At the end of each parallel task, the cores are synchronized to ensure
    consistent write-back of the results.


    #### *B. Implementation Area and Frequency*


    The logic area breakdown of the clusters is presented in Table III. For the MX-ready
    Dual-Core cluster, the main area increase originates from the VFU (+5.3%) due
    to the near-FPU tile buffer and is followed by a slight increase in the VLSU (+5.94
    kGE), which is related to supporting matrix loads/stores. The total area overhead
    of MX is negligible, amounting to an increase of 2.5 %. The MemPool64Spatz<sup>4</sup>
    cluster follows the same trend, resulting in a similar 2.89 % area overhead. MX
    does not affect the critical path of the two systems in analysis, which runs through
    Snitch to a TCDM bank. Thus, the MXready dual- and 64-core systems achieve 920
    MHz and 720 MHz in the (SS/0.72 V/125 °C) corner, respectively, with no frequency
    degradation with respect to the baseline clusters.


    Table III LOGIC AREA BREAKDOWN IN 12-NM TECHNOLOGY.


    |         |          | Dual-Core Cluster[kGE] |        | 64-Core Cluster[MGE]
    |       |          |  |  |

    |---------|----------|------------------------|--------|----------------------|-------|----------|--|--|

    |         | Baseline | MX<br>Overhead         |        | Baseline<br>MX       |       |
    Overhead |  |  |

    | Snitch  | 47.82    | 48.01                  | +0.40% | 1.50                 |
    1.47  | -2.04%   |  |  |

    | i-Cache | 149.67   | 149.56                 | -0.07% | 4.96                 |
    4.95  | -0.20%   |  |  |

    | TCDM(a) | 1191.89  | 1192.03                | +0.01% | 20.46                |
    20.48 | +0.09%   |  |  |

    | VRF     | 345.04   | 348.87                 | +1.11% | 9.32                 |
    9.32  | 0.0%     |  |  |

    | VFU     | 1532.11  | 1613.39                | +5.31% | 12.91                |
    13.97 | +8.21%   |  |  |

    | VLSU    | 111.66   | 117.60                 | +5.32% | 2.54                 |
    3.07  | +20.87%  |  |  |

    | Other   | 570.63   | 575.97                 | +0.94% | 7.28                 |
    7.39  | +1.51%   |  |  |

    | Total   | 3948.82  | 4045.43                | +2.45% | 59.70                |
    61.43 | +2.89%   |  |  |


    (a) Including Memory Banks and Interconnect Logic.


    ![](_page_4_Figure_3.jpeg)


    Figure 3. Power breakdown for Dual-Core (Left) and 64-Core clusters (Right) executing
    MatMul. Dual-Core: at *TT@1GHz*, executing non-MX (4 vectors, length 32) and MX-ready
    algorithms (′ = 8, ′ = 4, ′ = 4, = 4). 64- Core: at *TT@910MHz*, executing non-MX
    (8 vectors, length 32) and MX-ready algorithms (′ = 8, ′ = 4, ′ = 8, = 8).


    #### *C. Performance, Power and Energy Efficiency*


    *1) Dual-Core Cluster:* The upper part of Table IV summarizes the kernel information,
    execution performance, and energy efficiency for the Dual-Core cluster when executing
    a 64-bit MatMul across various problem sizes and tile/sub-tile configurations,
    highlighting the rows where the kernel''s tile and sub-tile configurations achieve
    the best energy efficiency. The MX-ready cluster with a sub-tile size of (8, 4,
    4) achieves performance similar to the best-performing execution on the baseline
    cluster with efficiency gains by +10.9 % (16 × 16 × 16), +10.3 % (32 × 32 × 32),
    and +5.2 % (64 × 64 × 64).


    We evaluate the baseline algorithm using two different output tile configurations
    with constant sizes. For small problems (16 × 16 × 16), the output tile size of
    (8, 16, 1) yields higher FPU utilization. As discussed in Section II, although
    (8, 16, 1) has higher arithmetic intensity and fewer transfers between TCDM and
    VRF compared to (4, 32, 1), the latter configuration benefits from a 2× increase
    in SIMD ratio, leading to better performance for larger problem sizes. For the
    MX-ready algorithm, the output tiles with larger and equal ′ sub-tile dimensions
    consistently yield better performance and energy efficiency. This improvement
    is attributed to their higher arithmetic intensity and average SIMD ratio. A similar
    trend is observed for the energy efficiency when increasing the ′ dimension of
    the sub-tile. Due to the higher arithmetic intensity, the power decreases when
    the output tile size changes from (4, 16, 4) to (8, 8, 4). However, the (4, 16,
    4) configuration achieves higher performance thanks to more and shorter matrix
    result stores, which can be interleaved with computational instructions to hide
    latency.


    The left part of Figure 3 presents the power breakdown of the Dual-Core cluster''s
    baseline and MX-ready execution of a 64 × 64 × 64 MatMul, with the most energy-efficient
    tile and sub-tile size in our benchmarks. MX reduces VRF access for the B tile
    and intermediate result storage, leading to a 53.5 % reduction in VRF power consumption.
    Although the sub-tile buffer integration results in a slight 9.4 % power increase
    in VFU, the overall VPU power decreases by 4.1 %. We also observed a power decrease
    across the rest of the cluster components, including the Snitch core, instruction
    caches, and TCDM. This reduction is attributed to the higher SIMD ratio and tiled
    memory request pattern in MX-ready execution, which eliminates the multiple requests
    for scalar operands generated by the Snitch core in the baseline. As a result,
    the total power savings for the Dual-Core cluster achieved through MX amounts
    to 10.4 %.


    *2) 64-Core Cluster:* Our benchmark results for various problem sizes on MemPool64Spatz<sup>4</sup>
    are presented in the bottom section of Table IV. In such a large interconnected
    memory, contentions may occur when memory requests in the same tile access the
    same local bank or the same remote group in the same cycle. This generates stalls
    of the VLSU and increases the access latency. Although such contentions could
    be mitigated by allocating data structures in a local tile''s memory [14], this
    approach is hard to implement for MatMul, which inherently requires an extremely
    global data access pattern.


    MX regular memory accesses alleviate contention and improve VLSU utilization by
    distributing vector element loads/stores across different banks and groups in
    a strided fashion, contrasting with the baseline where vector elements are fetched
    from continuous addresses within the same group by both scalar and vector core.
    This is even more evident with small matrices, where the initial vector load and
    final result store constitute a significant portion of the total runtime due to
    the inability to hide latency. FPU utilization increases from 50.4% to 78.7%,
    leading to a 56% improvement in cluster performance.


    Despite a power consumption increase due to the higher FPU utilization, the MX-ready
    cluster achieves 25 % better energy efficiency. Even though the baseline kernels
    already achieve near-peak utilization for matrix sizes of 128 × 128 × 128 and
    256 × 256 × 256, with the same arithmetic intensity, MX still improves performance
    by 5.6 % and 2.3 %, with energy efficiency gains by 13.4 % and 9.8 %, respectively.
    The right side of Figure 3 presents the MemPool64Spatz4-related power breakdown
    comparison for a 256 × 256 × 256 MatMul. MX reduces the VRF power consumption
    by 60 %, thanks to fewer accesses achieved by buffering intermediate results.
    The VFU power increases by only 6 %, which comes from the sub-tile buffer and
    higher FPU utilization. Overall, MX leads to a 6.9 % cluster power reduction with
    near-peak FPU utilization.


    These analyses on both small- and large-scale vector clusters demonstrate that
    MX significantly improves the energy efficiency by reducing the power consumption
    related to the VRF accesses. MX also pushes the FPU utilization closer to its
    peak with a negligible area overhead. A quantitative comparison of MX against
    [7], [8] is hard since none of them presents area or power results, and the effective
    MatMul speed-up is unclear [7].


    Table IV THE SUMMARY OF KERNEL INFORMATION, EXECUTION PERFORMANCE AND ENERGY EFFICIENCY


    | Config                   | Mtx Size<br>[M, N, K] | Tile Size<br>[m, n, k] |
    Sub-Tile Size<br>[m'', n'', k''] | Mem-VRF<br>Transfers | Arithmetic<br>Intensity<br>[FLOP/B]
    | SIMD Ratio<br>[FLOP/vinsn] | Utilization | Performance<br>@ss freq<br>[GFLOPS]
    | Performance<br>@tt freq<br>[GFLOPS] | Power<br>@tt freq<br>[W] | En. Efficiency<br>@tt
    freq<br>[GFLOPS/W] |

    |--------------------------|-----------------------|------------------------|-------------------------------|----------------------|-------------------------------------|----------------------------|-------------|-------------------------------------|-------------------------------------|--------------------------|------------------------------------------|

    | Dual-Core Cluster(a) (b) |                       |                        |                               |                      |                                     |                            |             |                                     |                                     |                          |                                          |

    | Baseline                 | 64x64x64              | 8,16,1                 |
    -                             | 53248                | 1.23                                |
    16.00                      | 95.9%       | 14.13                               |
    15.34                               | 0.21                     | 71.49                                    |

    | Baseline                 | 64x64x64              | 4,32,1                 |
    -                             | 77824                | 0.84                                |
    32.00                      | 97.8%       | 14.41                               |
    15.65                               | 0.21                     | 73.48                                    |

    | Baseline                 | 32x32x32              | 8,16,1                 |
    -                             | 7168                 | 1.14                                |
    16.00                      | 90.0%       | 13.26                               |
    14.40                               | 0.20                     | 70.95                                    |

    | Baseline                 | 32x32x32              | 4,32,1                 |
    -                             | 10240                | 0.80                                |
    32.00                      | 93.3%       | 13.75                               |
    14.93                               | 0.20                     | 72.87                                    |

    | Baseline                 | 16x16x16              | 8,16,1                 |
    -                             | 1024                 | 1.00                                |
    16.00                      | 70.1%       | 10.33                               |
    11.22                               | 0.16                     | 71.69                                    |

    | Baseline                 | 16x16x16              | 4,32,1                 |
    -                             | 1408                 | 0.73                                |
    32.00                      | 64.7%       | 9.53                                |
    10.35                               | 0.16                     | 66.70                                    |

    | MX-ready                 | 64x64x64              | 4,8,4                  |
    4,4,4                         | 102400               | 0.64                                |
    34.73                      | 94.1%       | 13.86                               |
    15.06                               | 0.21                     | 72.91                                    |

    | MX-ready                 | 64x64x64              | 8,8,4                  |
    8,4,4                         | 69632                | 0.94                                |
    63.22                      | 95.6%       | 14.08                               |
    15.30                               | 0.19                     | 79.15                                    |

    | MX-ready                 | 64x64x64              | 4,16,4                 |
    4,4,4                         | 86016                | 0.76                                |
    36.76                      | 96.4%       | 14.20                               |
    15.42                               | 0.21                     | 75.19                                    |

    | MX-ready                 | 64x64x64              | 8,16,4                 |
    8,4,4                         | 53248                | 1.23                                |
    66.59                      | 97.2%       | 14.32                               |
    15.55                               | 0.19                     | 81.49                                    |

    | MX-ready                 | 32x32x32              | 4,8,4                  |
    4,4,4                         | 13312                | 0.62                                |
    34.29                      | 88.4%       | 13.02                               |
    14.14                               | 0.20                     | 71.90                                    |

    | MX-ready                 | 32x32x32              | 8,8,4                  |
    8,4,4                         | 9216                 | 0.89                                |
    62.48                      | 89.7%       | 13.22                               |
    14.35                               | 0.18                     | 77.68                                    |

    | MX-ready                 | 32x32x32              | 4,16,4                 |
    4,4,4                         | 11264                | 0.73                                |
    36.21                      | 92.7%       | 13.66                               |
    14.83                               | 0.20                     | 74.36                                    |

    | MX-ready                 | 32x32x32              | 8,16,4                 |
    8,4,4                         | 7168                 | 1.14                                |
    65.68                      | 93.5%       | 13.78                               |
    14.96                               | 0.19                     | 80.38                                    |

    | MX-ready                 | 16x16x16              | 4,8,4                  |
    4,4,4                         | 1792                 | 0.57                                |
    33.45                      | 63.1%       | 9.30                                |
    10.10                               | 0.15                     | 67.45                                    |

    | MX-ready                 | 16x16x16              | 8,8,4                  |
    8,4,4                         | 1280                 | 0.80                                |
    61.09                      | 66.1%       | 9.74                                |
    10.58                               | 0.14                     | 75.03                                    |

    | MX-ready                 | 16x16x16              | 4,16,4                 |
    4,4,4                         | 1536                 | 0.67                                |
    35.20                      | 71.6%       | 10.55                               |
    11.46                               | 0.16                     | 72.03                                    |

    | MX-ready                 | 16x16x16              | 8,16,4                 |
    8,4,4                         | 1024                 | 1.00                                |
    64.00                      | 70.3%       | 10.36                               |
    11.25                               | 0.15                     | 75.41                                    |

    | 64-Core Cluster(c)       |                       |                        |                               |                      |                                     |                            |             |                                     |                                     |                          |                                          |

    | Baseline                 | 256x256x256           | 8,32,1                 |
    -                             | 2686976              | 3.12                                |
    32                         | 94.5%       | 372.26                              |
    439.94                              | 1.57                     | 279.86                                   |

    | Baseline                 | 128x128x128           | 8,32,1                 |
    -                             | 344064               | 3.05                                |
    32                         | 90.7%       | 357.34                              |
    422.31                              | 1.57                     | 268.64                                   |

    | Baseline                 | 64x64x64              | 8,8,1                  |
    -                             | 69632                | 1.88                                |
    8                          | 50.4%       | 198.57                              |
    234.68                              | 1.20                     | 194.91                                   |

    | MX-ready                 | 256x256x256           | 8,32,8                 |
    8,4,8                         | 2686976              | 3.12                                |
    137.74                     | 96.7%       | 380.74                              |
    449.97                              | 1.46                     | 307.35                                   |

    | MX-ready                 | 128x128x128           | 8,32,8                 |
    8,4,8                         | 344064               | 3.05                                |
    136.23                     | 95.8%       | 377.27                              |
    445.86                              | 1.46                     | 304.55                                   |

    | MX-ready                 | 64x64x64              | 8,8,8                  |
    8,4,8                         | 69632                | 1.88                                |
    123.43                     | 78.7%       | 309.99                              |
    366.35                              | 1.50                     | 244.24                                   |


    (a) In bold, we highlight the best metrics for both the Baseline and MX-ready
    Dual-Core Cluster execution across various matrix sizes.


    (b) Dual-Core Cluster: Double-Precision operations; ss freq = 920MHz; tt freq
    = 1GHz.


    (c) 64-Core Cluster: Single-Precision operations; ss freq = 770MHz; tt freq =
    910MHz.


    ## V. CONCLUSION


    In this paper, we presented MX, an RVV-based ISA extension to support tiled matrix
    operations for energy-efficient MatMuls. With an embedded-device-friendly and
    extremely low footprint overhead, MX enhances the energy efficiency of MatMul
    by means of a small tile buffer near the FPUs, which minimizes the VRF accesses
    by storing and reusing both input and output matrix tiles. Moreover, MX reduces
    the number of instructions fetched by the scalar core, decreases the interaction
    between the scalar and vector cores, and regularizes the memory access pattern,
    further reducing power consumption. We characterized MX by implementing it on
    two multi-core clusters in a modern 12-nm technology node. With less than 3 %
    area overhead and no impact on the operating frequency, MX significantly boosts
    MatMul''s energy efficiency of a Dual-Core cluster by up to 10.9 %. In a 64-Core
    cluster and 64 × 64 matrices, performance and energy efficiency improve by 56
    % and 25 %, respectively, further pushing the FPU utilization toward the theoretical
    peak.


    ### ACKNOWLEDGMENTS


    This project has received funding from the ISOLDE project, No. 101112274, supported
    by the Chips Joint Undertaking of the European Union''s Horizon Europe''s research
    and innovation program and its members Austria, Czechia, France, Germany, Italy,
    Romania, Spain, Sweden, Switzerland.


    #### REFERENCES


    [1] B. Peccerillo, M. Mannino, A. Mondelli, and S. Bartolini, "A survey on hardware
    accelerators: Taxonomy, trends, challenges, and perspectives," *Journal of Systems
    Architecture*, vol. 129, p. 102561, 2022.


    - [2] H. Amiri and A. Shahbahrami, "Simd programming using Intel vector extensions,"
    *J. of Parallel and Distr. Comp.*, vol. 135, pp. 83–100, 2020.

    - [3] S. Deng, H. Zhao, W. Fang, J. Yin, S. Dustdar, and A. Y. Zomaya, "Edge intelligence:
    The confluence of edge computing and artificial intelligence," *IEEE Internet
    of Things Journal*, vol. 7, no. 8, pp. 7457–7469, 2020.

    - [4] N. Jouppi, C. Young, N. Patil, and D. Patterson, "Motivation for and evaluation
    of the first tensor processing unit," *IEEE Micro*, vol. 38, no. 3, pp. 10–19,
    2018.

    - [5] C. AI, "Edge TPU performance benchmarks," 2020. [Online]. Available: https://coral.ai/docs/edgetpu/benchmarks

    - [6] Y.-H. Chen, T. Krishna, J. S. Emer, and V. Sze, "Eyeriss: An energyefficient
    reconfigurable accelerator for deep convolutional neural networks," *IEEE Journal
    of Solid-State Circuits*, vol. 52, no. 1, pp. 127–138, 2017.

    - [7] V. Verma, T. Tracy II, and M. R. Stan, "EXTREM-EDGE EXtensions To RISC-V
    for Energy-efficient ML inference at the EDGE of IoT," *Sust. Comp.: Informatics
    and Systems*, vol. 35, p. 100742, 2022.

    - [8] T-Head Semiconductor, *RISC-V Matrix Multiplication Extension Specification*,
    T-Head Semiconductor, 2023. [Online]. Available: https:// github.com/T-head-Semi/riscv-matrix-extension-spec/releases/tag/v0.3.0

    - [9] M. Cavalcante, D. Wuthrich, M. Perotti, S. Riedel, and L. Benini, "Spatz:
    A ¨ compact vector processing unit for high-performance and energy-efficient shared-L1
    clusters," in *Proc. of the 41st ICCAD*. San Diego, CA, USA: IEEE/ACM, Oct. 2022.

    - [10] H. T. Kung, "Memory requirements for balanced computer architectures,"
    *SIGARCH Comp. Arch. News*, vol. 14, no. 2, p. 49–54, May 1986.

    - [11] B. Dally, "Hardware for deep learning," in *Hot Chips*, Stanford, CA, USA,
    Aug. 2023.

    - [12] M. Perotti, M. Cavalcante, N. Wistoff, R. Andri, L. Cavigelli, and L. Benini,
    "A ''New Ara'' for Vector Computing: an Open Source Highly Efficient RISC-V V
    1.0 Vector Processor Design," in *Proceedings of the 33rd IEEE Int. Conf. on ASAP*.
    Gothenburg, Sweden: IEEE, Jul. 2022.

    - [13] S. Riedel, M. Cavalcante, R. Andri, and L. Benini, "MemPool: A scalable
    manycore architecture with a low-latency shared L1 memory," *IEEE Transactions
    on Computers*, 2023, early access.

    - [14] M. Bertuletti, Y. Zhang, A. Vanelli-Coralli, and L. Benini, "Efficient
    parallelization of 5G-PUSCH on a scalable RISC-V many-core processor," in *Proc.
    of the 2023 DATE Conf.* Antwerp, Belgium: IEEE, Mar. 2023.'
  decisions:
    evaluation_prompt: 'Qualified. Reason: The paper contains structured evaluation
      sections, including "Experiment Setup and Results," with detailed quantitative
      analysis, performance metrics, and empirical results presented in tables and
      figures.'
    related_work_prompt: '- Qualified. Reason: The paper meaningfully engages with
      prior research by citing relevant works throughout the introduction and other
      sections. It discusses the context of existing technologies and methods, compares
      its proposed method to previous work, and provides a comprehensive list of references,
      indicating a thorough engagement with the academic literature.'
    novelty_prompt: 'Qualified. Reason: The paper introduces Matrix eXtension (MX),
      a novel ISA extension for the RISC-V Vector (RVV) architecture. MX is designed
      to enhance energy efficiency and performance in matrix multiplication tasks
      by minimizing VRF accesses and utilizing existing processing resources. The
      paper claims novelty in several areas, including the introduction of a lightweight
      and non-intrusive ISA extension, a theoretical justification of its benefits,
      and its implementation on multi-core clusters with detailed performance and
      energy efficiency analysis. These contributions demonstrate clear novelty in
      method, application, and results.'
    review_only_prompt: 'Qualified. Reason: The paper introduces a novel ISA extension
      called Matrix eXtension (MX) for the RISC-V Vector (RVV) architecture, which
      enhances energy efficiency and performance for matrix operations. It provides
      original contributions, including the definition of MX, theoretical justifications,
      implementation details, and experimental results demonstrating the benefits
      of MX in terms of power, performance, and area metrics.'
- title: LLM-Based Human-Robot Collaboration Framework for Manipulation Tasks
  abstract: ''
  keywords: ''
  document: '## 1. Introduction


    As robotics technology advances, the potential for robots to assist with domestic
    chores becomes increasingly promising. With the ability to understand and process
    natural language, these robots become more adaptable and flexible to accommodate
    a wide range of user instructions[1]. However, the previous works with LLM-based
    control sometimes show a relatively low accuracy for high intelligence task decisionmaking[1].
    Our work introduces the idea of "LLM-Based task planning with human-robot collaboration",
    which is a novel approach to enhance human supervision in LLMbased autonomy. The
    contributions of this paper are summarized as follows: (1) Our LLM converts high-level
    language commands into sequences of executable motion functions, enabling adaptability
    to various user instructions. (2) Additionally, teleoperation and DMP are utilized
    for motion demonstration which allows the robot to learn from human guidance and
    potentially improves task feasibility and generalizability. (3) Furthermore, the
    robot is empowered with environmental perception through YOLO-based perception
    module for targeted tasks. The position of objects will be registered once recognized
    and update with the real-time position. Combining these elements, the proposed
    approach opens new possibilities for seamless human-robot collaboration in household
    tasks, making robots more practical and adaptable.


    <span id="page-0-0"></span>![](_page_0_Figure_12.jpeg)


    Fig. 1. Framework of LLM-based task planning with enhanced HRC.


    ## 2. LLM-based human-robot collaboration framework


    The system diagram is illustrated in Fig. [1.](#page-0-0) The system consists
    of three main components: the user, LLM, and the robot, which forms an interactive
    loop. Additionally, we introduce a skilled teleoperator as an assistant to enhance
    the overall system''s generalizability and feasibility.


    ## 2.1 LLM-based task planning


    In our approach, we build our model based on LLM (GPT-2) and train it using a
    text corpus following previous work done by other researchers[2], enabling LLM
    to provide accurate function predictions in response to specific instructions.
    Subsequently, we integrate the perceived target position information and motion
    functions obtained from LLM into a prepared code template, enabling the robot
    to execute the corresponding tasks effectively. To efficiently manage task execution,
    we adopt a hierarchical approach in our work-treating long-horizon tasks, short-horizon
    tasks, and motion functions as three layers. For long-horizon tasks which include
    motion functions of more than 10, we consider them first-layer tasks. In such
    cases, these tasks are separated into multiple short-horizon tasks through LLM.
    However, short-horizon tasks which involve less than 10 motion functions, are
    treated as the second layer task. When LLM receives commands from these second-layer
    tasks, it directly returns the functions necessary to accomplish the designated
    tasks.


    <sup>∗</sup>Corresponding author email: zhu@robo.mein.nagoya-u.ac.jp


    This work has been submitted to the IEEE for possible publication. Copyright may
    be transferred without notice, after which this version may no longer be accessible


    | Tasks                             | Objects Num                         |    |
    Fins | SIR  | Bxec | TESIB |

    |-----------------------------------|-------------------------------------|----|------|------|------|-------|

    | Catch                             | bottle,<br>cup                      | 10
    | 4    | 0.80 | 1.00 | 1.00  |

    | *Catch                            | bowl                                | -  |
    4    | 0.00 | 1.00 | 0.00  |

    | Put/<br>Place                     | bowl,<br>cabinet,<br>cup,<br>bottle | 10
    | 7    | 0.80 | 1.00 | 1.00  |

    | Open                              | cabinet                             | ഗ  |
    7    | 0.60 | 1.00 | 1.00  |

    | Clean<br>the top<br>of<br>cabinet | cabinet                             | ળ  |
    17   | 0.40 | 0.80 | 0.80  |


    ## Fig. 2. Experiment data.


    On the other hand, the first-layer tasks can be separated into multiple short-horizon
    tasks through LLM. Subsequently, we process each short-horizon task following
    its specific procedure to divide them into motion functions as mentioned. Finally,
    motion functions are organized by following a planned task sequence to construct
    the complete long-horizon task. This hierarchical task handling allows for a more
    organized and effective execution of both short and long-horizon tasks, contributing
    to our system''s overall efficiency and accuracy.


    ## 2.2 DMP-based task correction


    To enhance the generalizability of LLM-based autonomy, We propose to integrate
    DMP-based task correction with human teleoperation-driven demonstrations. Dynamic
    Movement Primitives (DMP) is a generic approach for trajectory modeling in an
    attractor landscape based on differential dynamical systems [3]. In this paper,
    we leverage our previously developed teleoperation system[4], [5] which can intuitively
    control the robot motion through a VR device, and also utilize DMP to record trajectories
    obtained from manual teleoperation. These trajectories can then be reproduced
    to complement any deficiencies in the LLM-based autonomy, particularly in failed
    function sequence generation or function sequence impracticality.


    For instance, when we issue the command "catch the bowl," the default motion function
    for bowl grasping could be inadequate to complete the task. To address this issue,
    we switch to the DMP-based teleoperation mode and provide instructions for the
    desired action. The robot can then accurately reproduce the trajectory using DMP.
    This approach will be continually developed to manage a wider range of long-horizon
    tasks, with the ultimate goal of creating an effective Human-Robot Collaboration
    (HRC) system. This system will strategically take advantage of both human flexibility,
    in terms of adaptability and problem-solving skills, and robot autonomy, in terms
    of precision and efficiency.


    ## 3. Experiment and Result


    We conducted multiple experiments by providing "catch", "put", "open" and long-horizon
    tasks-"clean the top of the cabinet" for several objects to assess their success
    rates (SR), executability (Exec), and feasibility (FSB). The indicator Num means
    the number of trials, and Fns shows the motion functions used in completing the
    task. Additionally, Exec is defined as if the task is executable in the environment,
    and FSB represents if the motion is feasible to reach the goal. The experimental
    results are presented in Fig. 2. In the case of the indicator "Exec" showing 0.80
    in the "clean the top of the cabinet" task, the reasonable explanation is the
    randomness of LLM, which has a low probability of generating incorrect responses
    (*Exec* = 0.2). As for the FBS of 0.00 in the "catch the bowl" task, this outcome
    can be attributed to the task being impossible to complete due to the default
    motion function being unsuitable for the target object''s shape. In such cases,
    the DMP-based task correction is used to make necessary demonstrations.


    ## 4. Conclusion


    In this work, we have successfully proposed a LLM-based task-planning method.
    An interface is built to integrate the LLM, perception pipeline, teleoperation
    system, and DMPbased task correction. The results show that the robot can execute
    the command from the user with a considerable success rate for short-horizon tasks
    like "catch", "put", or "open". Especially, for the task with 0.00 FBS, such as
    "catch the bowl", DMP-based correction is introduced to improve it. However, for
    long-horizon tasks, it shows a relatively low success rate. The reason could be
    the error accumulating with motion. The future work includes the improvement of
    DMPbased task correction and fine-tuning teleoperation which can complement the
    error from hardware to improve the success rate and feasibility.


    ## 5. Acknowledgement


    This work was supported in part by JST Trilateral AI Research, Japan, under Grant
    JPMJCR20G8; and in part by JSPS KAKENHI under Grant JP22K14222; and in part by
    NCGG under Chojuiryou Kenkyukaihatsuhi Nos. 19–5, 21- 21.


    ## References


    - [1] I. Singh, V. Blukis, A. Mousavian, A. Goyal, D. Xu, J. Tremblay, D. Fox,
    J. Thomason, and A. Garg, "Progprompt: Generating situated robot task plans using
    large language models," in *2023 IEEE International Conference on Robotics and
    Automation (ICRA)*, 2023, pp. 11 523–11 530.

    - [2] G. Chalvatzaki, A. Younes, D. Nandha, A. T. Le, L. F. R. Ribeiro, and I.
    Gurevych, "Learning to reason over scene graphs: A case study of finetuning GPT-2
    into a robot language model for grounded task planning," *CoRR*, vol. abs/2305.07716,
    2023. [Online]. Available: <https://doi.org/10.48550/arXiv.2305.07716>

    - [3] R. Wang, Y. Wu, W. L. Chan, and K. P. Tee, "Dynamic movement primitives
    plus: For enhanced reproduction quality and efficient trajectory modification
    using truncated kernels and local biases," in *2016 IEEE/RSJ International Conference
    on Intelligent Robots and Systems (IROS)*, 2016, pp. 3765–3771.

    - [4] J. Nakanishi, S. Itadera, T. Aoyama, and Y. Hasegawa, "Towards the development
    of an intuitive teleoperation system for human support robot using a vr device,"
    *Advanced Robotics*, vol. 34, no. 19, pp. 1239– 1253, 2020.

    - [5] Y. Zhu, B. Jiang, Q. Chen, T. Aoyama, and Y. Hasegawa, "A shared control
    framework for enhanced grasping performance in teleoperation," *IEEE Access*,
    vol. 11, pp. 69 204–69 215, 2023.'
  decisions:
    evaluation_prompt: '- Qualified. Reason: The paper contains a section titled "Experiment
      and Result" where multiple experiments are conducted to assess success rates,
      executability, and feasibility. It includes a table presenting quantifiable
      outcomes, fulfilling the criteria for structured evaluation.'
    related_work_prompt: 'Qualified. Reason: The paper meaningfully engages with prior
      research by citing and discussing previous works in the context of LLM-based
      task planning and DMP-based task correction. It references specific studies
      in the introduction and throughout the text, explaining how the current work
      builds upon and differentiates from these prior efforts.'
    novelty_prompt: 'Qualified. Reason: The paper introduces a novel approach to enhance
      human supervision in LLM-based autonomy through "LLM-Based task planning with
      human-robot collaboration." It proposes a new method for converting high-level
      language commands into executable motion functions, integrates teleoperation
      and DMP for motion demonstration, and uses a YOLO-based perception module for
      environmental perception. These elements collectively contribute to the novelty
      of the paper by opening new possibilities for seamless human-robot collaboration
      in household tasks.'
    review_only_prompt: 'Qualified. Reason: The paper introduces a novel approach
      to LLM-based task planning with human-robot collaboration, proposes new methods
      such as integrating teleoperation and DMP for motion demonstration, and presents
      original experiments and results.'
- title: 'Cocco: Hardware-Mapping Co-Exploration towards Memory Capacity-Communication
    Optimization'
  abstract: 'Memory is a critical design consideration in current dataintensive DNN
    accelerators, as it profoundly determines energy consumption, bandwidth requirements,
    and area costs. As DNN structures become more complex, a larger on-chip memory
    capacity is required to reduce data movement overhead, but at the expense of silicon
    costs. Some previous works have proposed memory-oriented optimizations, such as
    different data reuse and layer fusion schemes. However, these methods are not
    general and potent enough to cope with various graph structures.

    In this paper, we explore the intrinsic connection between network structures
    and memory features to optimize both hardware and mapping. First, we introduce
    a graph-level execution scheme with a corresponding dataflow and memory management
    method. This scheme enables the execution of arbitrary graph patterns with high
    data reuse and low hardware overhead. Subsequently, we propose Cocco, a hardware-mapping
    co-exploration framework leveraging graph-level features of networks. It aims
    to minimize communication overhead, such as energy consumption and bandwidth requirements,
    with a smaller memory capacity. We formulate the graph-partition scheduling and
    memory configuration search as an optimization problem and employ a genetic-based
    method to achieve efficient co-exploration for large and irregular networks. Experiments
    demonstrate that Cocco obtains lower external memory access, lower bandwidth requirements,
    and more stable optimization for graph partition compared to the greedy algorithm
    and dynamic programming introduced in prior works. Cocco also reduces the costs
    by 1.89% to 50.33% using co-exploration compared to other typical methods.

    ASPLOS ''24, April 27-May 1, 2024, La Jolla, CA, USA © 2024 Copyright held by
    the owner/author(s). ACM ISBN 979-8-4007-0372-0/24/04. <https://doi.org/10.1145/3617232.3624865>

    CCS Concepts: • Hardware → Design reuse and communication-based design; On-chip
    resource management; • Computer systems organization → Parallel architectures;
    • Software and its engineering → Compilers.

    Keywords: Design space exploration, Memory, Graph analysis, Subgraph, Genetic
    algorithm, Deep learning accelerator'
  keywords: ''
  document: '# Cocco: Hardware-Mapping Co-Exploration towards Memory Capacity-Communication
    Optimization


    Zhanhong Tan tanzh19@mails.tsinghua.edu.cn IIIS, Tsinghua University Beijing,
    China


    Zijian Zhu zhuzj23@mails.tsinghua.edu.cn IIIS, Tsinghua University Beijing, China


    Kaisheng Ma<sup>∗</sup> kaisheng@mail.tsinghua.edu.cn IIIS, Tsinghua University
    Beijing, China


    # Abstract


    Memory is a critical design consideration in current dataintensive DNN accelerators,
    as it profoundly determines energy consumption, bandwidth requirements, and area
    costs. As DNN structures become more complex, a larger on-chip memory capacity
    is required to reduce data movement overhead, but at the expense of silicon costs.
    Some previous works have proposed memory-oriented optimizations, such as different
    data reuse and layer fusion schemes. However, these methods are not general and
    potent enough to cope with various graph structures.


    In this paper, we explore the intrinsic connection between network structures
    and memory features to optimize both hardware and mapping. First, we introduce
    a graph-level execution scheme with a corresponding dataflow and memory management
    method. This scheme enables the execution of arbitrary graph patterns with high
    data reuse and low hardware overhead. Subsequently, we propose Cocco, a hardware-mapping
    co-exploration framework leveraging graph-level features of networks. It aims
    to minimize communication overhead, such as energy consumption and bandwidth requirements,
    with a smaller memory capacity. We formulate the graph-partition scheduling and
    memory configuration search as an optimization problem and employ a genetic-based
    method to achieve efficient co-exploration for large and irregular networks. Experiments
    demonstrate that Cocco obtains lower external memory access, lower bandwidth requirements,
    and more stable optimization for graph partition compared to the greedy algorithm
    and dynamic programming introduced in prior works. Cocco also reduces the costs
    by 1.89% to 50.33% using co-exploration compared to other typical methods.


    ASPLOS ''24, April 27-May 1, 2024, La Jolla, CA, USA © 2024 Copyright held by
    the owner/author(s). ACM ISBN 979-8-4007-0372-0/24/04. <https://doi.org/10.1145/3617232.3624865>


    CCS Concepts: • Hardware → Design reuse and communication-based design; On-chip
    resource management; • Computer systems organization → Parallel architectures;
    • Software and its engineering → Compilers.


    Keywords: Design space exploration, Memory, Graph analysis, Subgraph, Genetic
    algorithm, Deep learning accelerator


    #### ACM Reference Format:


    Zhanhong Tan, Zijian Zhu, and Kaisheng Ma. 2024. Cocco: Hardware-Mapping Co-Exploration
    towards Memory Capacity-Communication Optimization. In 29th ACM International
    Conference on Architectural Support for Programming Languages and Operating Systems,
    Volume 1 (ASPLOS ''24), April 27-May 1, 2024, La Jolla, CA, USA. ACM, New York,
    NY, USA, [16](#page-15-0) pages. <https://doi.org/10.1145/3617232.3624865>


    ## <span id="page-0-0"></span>1 Introduction


    The evolution of neural network topology has driven the remarkable progress of
    artificial intelligence from the early single-layer perceptron (SLP) [\[45,](#page-14-0)
    [54\]](#page-14-1) and multi-layer perceptron (MLP) [\[17,](#page-13-0) [22,](#page-13-1)
    [39\]](#page-13-2) to modern DNNs with plain [\[36,](#page-13-3) [57\]](#page-14-2)/inception
    [\[59\]](#page-14-3)/residual [\[20,](#page-13-4) [55\]](#page-14-4) structures
    based on manual design, and even irregular structures using neural architecture
    search (NAS) [\[53,](#page-14-5) [75\]](#page-15-1) or random network generation
    [\[68\]](#page-14-6). These technological innovations have resulted in increasingly
    complex computation graphs, which pose challenges for efficient memory design
    and deployment.


    Memory design is crucial in the accelerator system, as it performs data preparation
    at the start of each processing stage according to the scheduling scheme, determining
    energy consumption, bandwidth requirements, and area costs. Figure [1](#page-1-0)
    shows the trade-off between the on-chip memory size and the external memory access
    in DNN accelerators. A smaller on-chip buffer (left side) saves area but requires
    more data reloading. A larger buffer (right side) can reduce external memory access
    and save energy and bandwidth but at the cost of increasing the memory overhead.
    An excessively large SRAM may not be feasible due to the high silicon area cost,
    typically ranging from 1 to 2 mm<sup>2</sup> /MB in 12nm, and the high energy
    overhead, dozens of times that of a MAC operation for a large SRAM.


    Therefore, the key problem is: between the two extremes in Figure [1,](#page-1-0)
    how to find an appropriate memory configuration with efficient workload mapping
    and data management, especially under the growing complexity of neural network
    architectures.


    <sup>∗</sup>Corresponding author.


    Permission to make digital or hard copies of part or all of this work for personal
    or classroom use is granted without fee provided that copies are not made or distributed
    for profit or commercial advantage and that copies bear this notice and the full
    citation on the first page. Copyrights for thirdparty components of this work
    must be honored. For all other uses, contact the owner/author(s).


    ASPLOS ''24, April 27-May 1, 2024, La Jolla, CA, USA Zhanhong Tan, Zijian Zhu,
    and Kaisheng Ma


    <span id="page-1-0"></span>![](_page_1_Figure_1.jpeg)


    Figure 1. The effect of different memory capacities for a computation graph. Intermediate
    results can be buffered in the on-chip memory if it is large enough. The on-chip
    memory of small capacity can only buffer two nodes (marked in the red dotted box),
    and the larger memory can cover a larger subgraph (right side).


    The critical status of memory design has attracted extensive research. Most previous
    studies focus on simple layer-level optimization (the left one of Figure [1\)](#page-1-0)
    by applying loop transformation techniques such as tiling and reordering to fit
    the memory size and reuse the on-chip data [\[23,](#page-13-5) [43,](#page-14-7)
    [44,](#page-14-8) [61,](#page-14-9) [70\]](#page-15-2). In addition, several works
    also guide the memory capacity and hierarchy design using designspace exploration
    [\[12,](#page-12-0) [32,](#page-13-6) [37,](#page-13-7) [66,](#page-14-10) [67\]](#page-14-11).
    However, these layerlevel optimizations are confined to the limited intra-layer
    reuse, which is insufficient for memory-intensive networks. A subgraph-level scheme
    (e.g., the middle one and the right one of Figure [1\)](#page-1-0) provides a
    larger optimization space via inter-layer reuse [\[3,](#page-12-1) [4,](#page-12-2)
    [38,](#page-13-8) [73\]](#page-15-3) to reduce the I/O overhead. Therefore, this
    paper aims to leverage the subgraph-level computing flow to optimize the memory
    capacity and external communication for networks with any topology.


    However, there are three primary challenges to fully exploit the subgraph-level
    optimization.


    First, we need a general execution flow for any sub-graph. Due to the various
    kernel sizes and strides, a parent node in a subgraph may have unbalanced data
    requirements from its consumers, which makes it difficult to determine the tensor
    tiling scheme and the memory allocation for each node (layer). In the traditional
    single-layer execution, we usually divide a large tensor into loop tiles, which
    are processed through a series of regular computing steps. Similarly, we want
    the sub-graph execution to be a series of elementary computing steps with a simple
    control flow.


    Second, we require a suitable memory management method for the subgraph execution.
    Due to complicated dependency among nodes in a subgraph, careful management is
    needed to reuse overlapping and inter-layer intermediate data.


    Solving these two challenges contributes to a basic hardware execution model compatible
    with subgraph-level optimization. However, we also encounter the third challenge:
    how to partition a model into subgraphs and how much memory to allocate. The optimization
    space is huge, so we need to devise a search method with high sampling efficiency
    to find a proper subgraph partition and memory configuration result.


    In this paper, we first introduce a complete graph-level scheme for memory. In
    particular, it contains a consumptioncentric flow that enables the execution of
    arbitrary subgraphs with low memory footprints (for challenge 1). Accordingly,
    we provide an explicit memory dataflow and the corresponding memory management
    scheme for effective data reuse (for challenge 2). Building on the graph-level
    memory scheme, we propose Cocco, a hardware-mapping co-exploration framework,
    to establish a connection between model features and the memory configuration
    (for challenge 3).


    Cocco aims to find a combination of on-chip buffers and the corresponding graph-level
    scheduling for lower memory and communication overhead. In particular, we develop
    a genetic-based algorithm to efficiently explore the search space of graph partitions
    and the associated memory configuration for a series of neural networks.


    In summary, this work makes the following contributions:


    - Subgraph execution scheme. We first introduce a consumption-centric flow to
    determine a low-cost execution sequence by throttling and aligning the dataflow.

    - Efficient dataflow and memory management for subgraph data reuse. We propose
    a memory management scheme featuring multiple reconfigurable regions and the corresponding
    dataflow to support arbitrary subgraph execution with full data reuse.

    - Hardware-mapping co-exploration framework. Based on the subgraph execution scheme
    and memory dataflow, we propose Cocco, a genetic-based framework combining the
    graph-level partition and memory design-space exploration together. Cocco achieves
    1.89% to 50.33% lower costs (lower communication with a smaller size) using co-exploration
    in contrast to other methods.


    ## 2 Background and Motivation


    #### 2.1 Design of Neural Network Accelerators


    The DNN accelerator unit is the most basic execution unit in a computing system,
    on top of which, we can scale it out to many-core, many-socket, and many-drawer
    systems [\[24,](#page-13-9) [40,](#page-13-10) [48,](#page-14-12) [60\]](#page-14-13).
    An accelerator unit usually employs a processing element (PE) array on a sophisticated
    interconnection network to enable efficient tensor-level computation. Each PE
    typically contains local scratchpads and ALUs to process basic data packets. The
    global buffer and the weight buffer store activations and weights, and they are
    generally


    <span id="page-2-0"></span>![](_page_2_Figure_1.jpeg)


    *\* Those designs only support INT8 precision for tensor, we scale to FP16 performance
    by a factor of 0.5. \*\* For most designs fabricated under 12nm (or close to)
    process, we align all areas to 12nm. The SRAM area is estimated as 1.2mm<sup>2</sup>/MB.*


    Figure 2. Left: performance v.s. memory capacity of several industrial NPUs. Right:
    a summary of SRAM area ratio in these accelerators.


    located next to the PE array to serve as the data interface and manage data between
    the PE array and the external memory (e.g., DRAM or other cores). Due to the limited
    capacity of the global buffer, the compiler has to partition the network execution
    into a series of elementary workloads that are scheduled along the parallel spatial
    resources and the temporal dimension [\[18,](#page-13-11) [61,](#page-14-9) [72\]](#page-15-4).
    The capacity of the global buffer usually dominates the external memory access
    and bandwidth requirements, significantly impacting system performance. If the
    global memory is larger, it is more likely to buffer more intermediate data and
    avoid data being evicted to DRAM. As shown in Figure [1,](#page-1-0) a larger
    buffer expands the scope of elementary workloads from a single layer to a larger
    subgraph, reducing the communication overhead.


    However, choosing an appropriate memory specification is always a challenge. In
    Figure [2,](#page-2-0) we surveyed 16 popular industrial neural network processors
    with various memory/performance/area characteristics, where nine of them target
    the training domain [\[6,](#page-12-3) [11,](#page-12-4) [24,](#page-13-9) [34,](#page-13-12)
    [35,](#page-13-13) [40,](#page-13-10) [41,](#page-13-14) [48,](#page-14-12) [60,](#page-14-13)
    [63,](#page-14-14) [69\]](#page-14-15) and seven target model inference [\[1,](#page-12-5)
    [7,](#page-12-6) [8,](#page-12-7) [26–](#page-13-15)[28,](#page-13-16) [49,](#page-14-16)
    [65\]](#page-14-17). According to the survey, we can observe several trends as
    follows:


    - 1. Memory occupies a significant portion of the silicon footprint on an NPU
    chip, ranging from 4% to 79% of the area, with capacities from 2.5MB to 896MB.

    - 2. Figure [2](#page-2-0) Left shows a trend of diminishing marginal benefit
    of memory capacity. This is because there is a critical capacity to meet the data
    reuse and bandwidth requirement at the beginning, and the increments become negligible
    with higher memory capacity.

    - 3. We can infer that there is a saturated capacity equivalent to the ideal unlimited
    memory, especially for the inference design. For example, Hanguang [\[26\]](#page-13-15)
    is a special SRAM-only inference system without DDR, and the 394MB buffers are
    large enough to hold the intermediate data in their scenarios.


    <span id="page-2-1"></span>![](_page_2_Figure_10.jpeg)


    Figure 3. Evaluations on subgraphs fusing different number of layers (denoted
    as L=1,3,5). Y-axis is in the log domain. The 2TOPS NPU accelerator is configured
    with a 1MB global buffer and a 1.125MB weight buffer. The bandwidth requirement
    of weights is from the prefetch of the next subgraph, while that of activations
    is from the inputs and outputs of each subgraph.


    This survey implies a design trade-off between memory capacity and performance
    based on workloads and commercial considerations. Motivated by the observations
    above, this paper aims to provide several memory design considerations and study
    the connection between workload features and memory capacity in an NPU accelerator.


    #### 2.2 Workload Deployment


    A neural network is usually executed in a DNN accelerator with layer or graph
    granularities based on the buffer capacity and dataflow.


    2.2.1 Layer-level Assignment. This manner assigns tasks layer by layer. Most previous
    studies employ a tiling-based layer-wise execution manner [\[10,](#page-12-8)
    [21,](#page-13-17) [30,](#page-13-18) [37,](#page-13-7) [50,](#page-14-18) [61\]](#page-14-9),
    which elaborates the tiling sizes of tensors to fit in the accelerator buffers
    and maintain performance. A proper tiling scheme should overlap the data loading
    latency with the computing time of each tile and try to reduce the repeated access
    of local weight buffers. Tiles of data are transferred between the external memory
    and the global buffer, and PEs subsequently fetch data from the global to their
    local buffers. Given the larger bit-width of partial sums (e.g., 24bit partial
    sums v.s. 8bit inputs in Simba), the output-centric tiling scheme is more commonly
    used to calculate the final results before writing back to the global buffer [\[61\]](#page-14-9).


    2.2.2 Graph-level Assignment. Unlike the layer-level assignment that restrains
    from leveraging inter-layer reuse, a graph-level assignment processes several
    layers of a neural network as a whole. To demonstrate the effectiveness of the
    layer-level assignment, we evaluate four networks on a 2TOPS accelerator model,
    as shown in Figure [3.](#page-2-1) The results show that fusing layers into subgraphs
    significantly reduces external memory access by 42.3% ∼ 74.7% and average bandwidth
    requirements by 26.8% ∼ 67.8%. However, the improvements of larger subgraphs are
    marginal, indicating that there is an optimal trade-off between inter-layer


    reuse and subgraph size, which determines the memory requirement. For example,
    executing three-layer subgraphs reduces external memory access by 53.7% in ResNet50,
    while executing five-layer subgraphs only further reduces it by 13.6%.


    Several works have studied inter-layer reuse and graph partition. However, they
    have several limitations in terms of performance and flexibility. LCP [\[42\]](#page-14-19)
    groups similar layers into a cluster and executes them as a whole, which makes
    it challenging to generalize into an arbitrary graph. Fused-CNN [\[4\]](#page-12-2)
    and SR-CNN [\[38\]](#page-13-8) fuse large contiguous layers for plain networks
    using manually-designed strategies. Irregular-NN [\[73\]](#page-15-3) attempts
    to execute a complex subgraph using a DP-based algorithm, but the constrained
    search space limits the exploration.


    To overcome these challenges, we propose an end-to-end framework that automatically
    optimizes the graph partition and memory configuration for any neural network.
    Our framework consists of two main components: a graph-level dataflow and a hardware-mapping
    co-exploration algorithm. We first introduce the graph-level dataflow and its
    hardware implementation. Then, we present Cocco, an efficient algorithm that explores
    the trade-offs among memory configurations and graph partition schemes based on
    workload features.


    ## <span id="page-3-1"></span>3 The Proposed Graph-Level Scheme


    To execute layers on an NPU core in a graph-level manner, we need an effective
    approach to reuse intermediate data and decide the memory allocation. This section
    presents our comprehensive scheme for subgraph execution, which addresses the
    first two challenges mentioned in Section [1.](#page-0-0) First, we describe a
    multi-layer execution flow that minimizes the memory footprint by a friendly tiling
    approach (for challenge 1). Second, we explain how to implement this flow on a
    real NPU using an efficient data reuse pattern (for challenge 2). The consistent
    target is to reduce the memory footprint and be friendly to implementation.


    #### 3.1 Subgraph execution scheme


    It is common practice for the layer-level scheduling to partition the output tensor
    into several tiles as layer-level elementary operations [\[56,](#page-14-20) [61,](#page-14-9)
    [72,](#page-15-4) [74\]](#page-15-5), simplifying the scheduling and instruction
    generation. Likewise, our high-level idea is also to generate a series of explicit
    subgraph-level elementary operations. However, we need to address the challenges
    of various kernel sizes and strides in different paths to prevent unbalanced data
    production and unnecessary memory.


    A model''s subgraph consists of multiple layers (nodes) with dependencies. Section
    [4](#page-5-0) provides detailed information on subgraph partition. In Figure
    [4\(](#page-3-0)a), we present a straightforward production-centric scheme for
    executing a subgraph


    with different kernel sizes in two branches, deriving tile sizes of the subsequent
    layers based on the predetermined input tile sizes. For example, we can produce
    a 1 × 1 tile of Node(0) and a 2 × 2 tile of Node(2) with a given 5 × 5 feature
    map of input Node(-1). In this case, these intermediate results only reduce to
    1 × 1 in Node(3), limited by the smallest input of Node(0), so the remaining results
    of Node(2) can not be consumed immediately. As shown in Figure [4,](#page-3-0)
    three extra data of Node(2) along with sixteen extra source data of Node(1) take
    up extra memory space. There are more redundant cached data when the subgraph
    becomes larger and more complicated. Disadvantages of this manner are attributed
    to the production-centric idea that consumes all related activations from the
    producers at once.


    To avoid the memory overhead of storing unused data, we propose a consumption-centric
    scheme in Figure [4\(](#page-3-0)b), where results of each node are produced on
    demand based on consumer(s) (i.e., output node(s)). For example, given a 1 × 1
    tile of Node(3), we derive the 1 × 1 tile size for Node(2), which subsequently
    decides a 3 × 3 tile for Node(1).


    The backward-derivation for each producer node is nontrivial because of diverse
    kernel sizes and strides in different paths. Therefore, we propose a three-stage
    flow to determine the behavior of each node, as illustrated in Figure [5.](#page-4-0)
    The highlevel idea is to let output nodes drive the whole execution and match
    the data consumption and production in each subgraph-level elementary operation.


    The stage-1 is similar to the traditional single-layer scheduling, where the tile
    size is optimized for higher computation utilization. In order to hold a larger
    subgraph, the tile size


    <span id="page-3-0"></span>![](_page_3_Figure_14.jpeg)


    Figure 4. A conceptual comparison between two manners to process a subgraph. The
    node marked with a negative number represents the input node. The corresponding
    subgraph is shown in the upper right, where × / refers to the convolution kernel
    size ( ) and stride ().


    Cocco: Hardware-Mapping Co-Exploration towards Memory ... ASPLOS ''24, April 27-May
    1, 2024, La Jolla, CA, USA


    <span id="page-4-0"></span>![](_page_4_Figure_1.jpeg)


    Figure 5. The flow to determine the execution scheme of a subgraph (i.e., the
    computed tile size of each node, the tile offset, and the processing sequence
    of nodes). For simplicity, we discuss the 1D-CONV in this example and it is similar
    in the 2D-CONV case.


    tends to be smaller. In the 1D-CONV example, we set the tile size to be 2 for
    output nodes.


    The stage-2 aims to determine the data update offset Δ and the memory allocation
    size for each node based on the consumer(s), processing in the reverse topological
    order. We use the least common multiply (LCM) operation to determine Δ () of producers
    for aligning different input offset requirements (Δ () () ) from consumers. Hence,
    one producer update may correspond to multiple updates of a consumer. For example,
    Δ (−2) = lcm{Δ (0) (0) , Δ (1) (1) } = 4 = 2Δ (1) (1) , one update of Node(-2)
    corresponds to two updates of Node(1). As for the tile size deduction, (Δ () /
    () ) is to derive the required input tile size (,) for output node [1](#page-4-1)
    , where Δ () / () is the consumer offset (updated data) per producer update. The
    maximum result (,) of all outputs is the tile size () of input node . In this
    example, (−2) = max{<sup>0</sup> (2), <sup>1</sup> (4)} = 6 and (−1) = max{<sup>1</sup>
    (2), <sup>2</sup> (2)} = 4.


    As mentioned above, since we use LCM to align production and consumption, one
    producer update may correspond to multiple updates of a consumer. In the stage-3,
    we use \_ to represent the number of memory update per subgraph elementary operation.
    The generated result of the example in Figure [5](#page-4-0) is shown in Figure
    [6.](#page-4-2) \_ of Node(-1), Node(1), and Node(2) are two, where the second
    updates are highlighted in red boxes. Note that the {\_(−2) , . . . , \_(2) }
    solution is not unique, but the unique co-prime one {1, 2, 1, 2, 2} corresponds
    to the minimal elementary operation.


    <span id="page-4-2"></span>![](_page_4_Figure_8.jpeg)


    Figure 6. The memory snapshot during two subgraph elementary operations based
    on the execution scheme of Figure [5](#page-4-0) example. The allocated memory
    size and update offset correspond to and Δ, respectively (the [:] notation denotes
    data ranging from index to ). The arrows denote the data dependency according
    to the node relation in the subgraph.


    The proposed flow is based on a general directed acyclic computation graph and
    is not limited to specific layer features. In this way, we can determine the execution
    scheme for any complex irregular network like NasNet [\[75\]](#page-15-1) and
    RandWire [\[68\]](#page-14-6).


    #### 3.2 Memory Management for the subgraph execution


    Up to now, we have inferred the execution scheme for subgraphs, and the remaining
    challenge is how to implement it on hardware efficiently. Figure [7](#page-5-1)
    shows the memory allocation and update scheme for the subgraph execution. Before
    computing a subgraph, the compiler determines logical blocks for input, intermediate,
    and output nodes, where the block sizes depend on the tile sizes derived from
    the execution flow.


    For convenient management, we introduce two types of memory regions: MAIN and
    SIDE. The MAIN region stores the source data for PE (i.e., the tile of <sup>0</sup>
    × <sup>0</sup> × in Figure [7\)](#page-5-1). The SIDE region reserves the horizontally
    overlapping data[2](#page-4-3) . Considering no reuse requirement for some output
    nodes, we only need a MAIN region to buffer the results of the current tile. Except
    for the input nodes (negative numbers) loading data from DRAM, the other nodes
    update data locally based on the computed results of the input node(s).


    In detail, the update scheme leverages the collaboration between the MAIN region
    and the SIDE region to achieve full reuse across sliding tiles (we consider kernel
    size > stride). As shown in Figure [7,](#page-5-1) when the convolution windows
    slide across the feature maps, the vertical overlap data (e.g., column = 5) are
    reused locally in the MAIN region. In contrast, the horizontally overlapping data
    (e.g., the first row of = 6 ∼ 8) are loaded from the SIDE region (path ①). Only
    a subset of data is replaced by the newly calculated results


    <span id="page-4-1"></span><sup>1</sup>For example, assume node is a convolution
    layer with kernel size () and stride () , then ( ) = () + ( − 1) × () .


    <span id="page-4-3"></span><sup>2</sup>We assume the column is the inner loop
    while the row is the outer loop.


    (marked in green). Besides, the bottom horizontal slices write new data to the
    SIDE region for the next row loop (path ②).


    The extra hardware overhead for the proposed memory scheme is slight. Figure [8](#page-5-2)
    presents our 12nm NPU core for the subgraph processing, with a buffer region manager
    to logically partition the global buffer to support contiguous layer processing.
    The buffer region manager is a 2-depth register file, where determines the maximum
    subgraph size, and each entry pair indicates the start and the end address for
    each region. The area overhead is quite small, and in our test chip, the area
    ratio is only 0.18% with = 64 and 272-byte size (17-bit address for the 1MB 64bit-width
    global buffer).


    In summary, our high-level idea is to divide the buffer into logical blocks for
    different layers and try to reuse data for sliding convolution windows. The memory
    management approach can be compatible with an accelerator as long as it supports
    the data movement inside the on-chip memory and flexible data assignment for computing.
    Coupled with our subgraph execution scheme introduced before, intermediate outputs
    in the subgraph can avoid being recomputed. Only those layers required by other
    subgraphs are written back to DRAM for further reuse.


    # <span id="page-5-0"></span>4 Memory Communication-Capacity Co-Exploration


    <span id="page-5-1"></span>**A subgraph example**


    The aforementioned hardware model enables arbitrary subgraph execution, but there
    is always limited buffer capacity


    **Data update diagram**


    ![](_page_5_Figure_6.jpeg)


    Figure 7. Memory allocation and data update scheme in the global buffer for full
    data reuse. The data layout used in our implementation is NWHC8c (aligned to 8
    channels), which can be changed in another design. <sup>0</sup> and <sup>0</sup>
    are the height and width of an input tile; is the input channel size; is the global
    width-dimension index of the input tensor; and <sup>0</sup> is the width-dimension
    index of an input tile.


    <span id="page-5-2"></span>![](_page_5_Figure_9.jpeg)


    Figure 8. Hardware implementation with the buffer region manager in our 12nm NPU
    as a demonstration. The layout is an NPU core extracted from part of our in-house
    chip.


    in hardware. Therefore, we need to partition the whole computation graph into
    a series of subgraphs that fit the memory. Below, we move up to the optimization
    for graph partition and memory design-space exploration for challenge 3.


    #### 4.1 Problem Formulation


    4.1.1 Graph-Level Partition. Formally, a DNN model can be represented as a computation
    graph = ( , ), where is the vertex set consisting of all the layers in a DNN model,
    and is the edge set that defines the structure of DNN. In particular, an edge
    (, ) ∈ represents that the output of layer is an input of layer .


    We aim to find a partition scheme : → N that assigns each layer to a subgraph,
    where layer ∈ is computed in the ()-th subgraph. A valid partition scheme should
    satisfy that any layer is computed before use. Therefore, for any (, ) ∈ , we
    have () ≤ (). Moreover, any subgraph should be connected in , otherwise meaningless.


    We cast the partition exploration as an optimization problem. The objective is
    to find a valid partition scheme that minimizes the total cost:


    <span id="page-5-4"></span>

    $$\sum\_{i} Cost\_{\mathcal{M}}(\{v \in V \mid P(v) = i\}),\tag{1}$$


    where is a cost function of a given subgraph based on a target metric (e.g., external
    memory access (EMA) and energy). For each subgraph, the EMA cost contains the
    loading of weights and input activations and the storage of output activations[3](#page-5-3)
    . The energy cost includes the overhead of EMA, on-chip buffers, and computation
    units.


    4.1.2 Design-Space Exploration (DSE). Our work further extends the optimization
    to combine with the memory design-space exploration. In this paper, we focus on
    the global buffer and the weight buffer, given that they dominate


    <span id="page-5-3"></span><sup>3</sup>The nodes that are required to write-back
    to DRAM can be the model output layer or the layers required by the future subgraph.


    the overhead of energy and area in an NPU core. As illustrated in Figure [1,](#page-1-0)
    a larger buffer capacity can take in more layers inside a subgraph, reducing communication
    costs but compromising the silicon area. To co-explore the hardware configuration
    and mapping, we construct an objective function by a linear combination of the
    hardware and mapping costs:


    <span id="page-6-0"></span>

    $$\text{BUF\\_SIZE} + \alpha \cdot \sum\_{i} Cost\_{\mathcal{M}}(\{v \in V \mid
    P(v) = i\}),\qquad \text{(2)}$$


    where is a preference hyper-parameter to adjust the proportion between two costs.


    #### 4.2 Baseline Methods


    Several optimization methods that exist today can perform graph-level partition.
    However, most of them fail to directly co-explore hardware and partition. Below,
    we list four typical methods as our baselines and sketch their features.


    4.2.1 Enumeration-based Algorithm. Fused-CNN [\[4\]](#page-12-2) applies a straightforward
    way to enumerate all possible partition schemes and return the best one. Jangda
    et al. [\[25\]](#page-13-19) proposed state compression dynamic programming to
    speed up the enumeration-based algorithm. We migrate their methods as our baseline
    and further improve them by only recording one subgraph in the state to reduce
    the time complexity.


    Nonetheless, there are still exponential states in the improved implementation.
    Let be the number of nodes in a graph, and the enumeration-based method may explore
    up to (2 <sup>2</sup> ) states for irregular networks. Consequently, the search
    is hard to complete within a reasonable search time for large-scale networks,
    not to mention the co-exploration with DSE.


    4.2.2 Greedy Algorithm. Halide [\[47\]](#page-14-21) employs a greedy algorithm
    to perform function grouping, which can be applied to the graph-level partition.
    Specifically, it first assigns each layer into a single-layer subgraph. Then it
    iteratively fuses a pair of subgraphs contributing the greatest benefit until
    all benefits are negative.


    Therefore, this algorithm tends to be trapped at the local minimum. Moreover,
    since the fusion decision rules are based on a given hardware, the greedy method
    cannot co-explore with DSE.


    4.2.3 Dynamic Programming (DP)-based Algorithm. For the irregular network scheduling
    problem, Zheng et al. [\[73\]](#page-15-3) proposed a DP-based algorithm. They
    arrange the layers based on their depth and perform DP in a sequential manner.


    This method is restricted to assigning layers that are contiguous in the depth
    order into a subgraph, hence the exploration is confined to constrained search
    space. It is unlikely to find the global optimum, especially for non-plain network


    structures. In addition, since the state transition of DP depends on the predefined
    buffer size, it is also tough to carry out co-exploration.


    4.2.4 Simulated Annealing (SA). SA [\[33\]](#page-13-20) is a popular optimization
    algorithm that samples a point and updates it iteratively to improve. It adopts
    the new sample points with a probability affected by the performance difference
    and a hyper-parameter named temperature. We employ the customized mutation operations
    (described in Section [4.4.3\)](#page-7-0) to update the sample points and implement
    an SA-based algorithm as a baseline.


    SA is an alternative optimization method for our framework with compatible operators,
    but it is not stable as the genetic algorithm in a range of benchmarks, which
    will be shown in later experiments.


    #### 4.3 Genetic Algorithm


    Previous research shows competitive performance of the Genetic Algorithm (GA)
    in several scheduling optimization problems [\[30,](#page-13-18) [31\]](#page-13-21).
    We summarize several benefits of GA for our hardware-mapping co-exploration problem:


    - 1. White-box property: We can track and tune its optimization process conveniently.
    Therefore, it is easy and intuitive to understand.

    - 2. Complete search space: It has the potential to explore the complete search
    space by customized mutation and crossover operations.

    - 3. Avoid local optima: In contrast to the greedy algorithm, GA can naturally
    jump out of the local minimum benefiting from the diversity of the population.

    - 4. Flexible initialization: We can use the results of other optimization algorithms
    to initialize GA and use GA to finetune the result.

    - 5. Co-exploration: Through the proposed GA operations and genome encoding, it
    can further support partition-DSE co-exploration.


    We encode each candidate solution (partition scheme and the corresponding memory
    configuration for our problem) as a genome, and the population contains a set
    of genomes. The GA goes through a series of generations to obtain a lower cost.
    It performs the crossover and mutation operations on the population in each generation.
    Specifically, a crossover operation blends two genomes selected from the population
    to generate one offspring while a mutation operation modifies a genome randomly.
    At the end of each generation, the evaluation environment evaluates the fitness
    of each genome, and the population in the new generation is selected based on
    the fitness results.


    #### 4.4 Cocco Optimization Framework


    Cocco is a GA-based optimization framework that enables the co-exploration of
    memory configuration and graph-level partition, as shown in Figure [10.](#page-7-1)
    The core of Cocco is a


    <span id="page-7-2"></span>![](_page_7_Figure_2.jpeg)


    Figure 9. Illustration of crossover and mutation operations in Cocco.


    series of operations that explore a complete search space. We build a genetic
    algorithm based on these customized operations. Fed with the neural network structure
    and DSE requirements, Cocco goes through several steps to get the optimization
    results. The execution model described in Section [3](#page-3-1) is embedded in
    the evaluation environment. In the following, we introduce the five stages of
    Cocco.


    4.4.1 Initialization. The first step in Cocco is to generate the initial population,
    where each genome contains a partition scheme of the computation graph and a memory
    configuration for DSE. For the DSE part, every genome selects a capacity value
    in a given range following a uniform distribution. There are two options in Cocco
    to initialize the partition scheme of each genome. The first option is random
    initialization. Precisely, we determine the () for each layer ∈ in topological
    order, and each () is selected randomly within the valid range. The other option
    is to initialize the partition scheme from other optimization algorithms.


    4.4.2 Crossover. We designed a customized crossover operation to inherit and blend
    the features of two parents selected from the population. Specifically, each hardware
    configuration (i.e., memory capacity) in the offspring is the average of its parents
    and then rounds to the nearest candidate value. For the partition scheme, we assign
    layers to subgraphs in topological order. Each undecided layer chooses one parent
    randomly to reproduce the corresponding subgraph. If the reproduced subgraph contains
    layers that have been decided, we split out a new one excluding those layers,
    or merge it with one of the subgraphs to which the decided layers belong.


    As shown in Figure [9\(](#page-7-2)b), layer 1 and layer 3 select Dad as the parent
    to reproduce the subgraphs {1, 2} and {3, 4}, respectively. Next, layer 5 selects
    Mom as its parent, so it


    <span id="page-7-1"></span>![](_page_7_Figure_8.jpeg)


    Figure 10. Cocco framework overview.


    intends to reproduce subgraph {4, 5, 6}. However, since we have already decided
    on layer 4 in subgraph {3, 4}, there are two alternatives: creating a new subgraph
    {5, 6} (Child-1) or merging with subgraph {3, 4} to obtain {3, 4, 5, 6} (Child-2).


    <span id="page-7-0"></span>4.4.3 Mutation. Four mutation operations are customized
    for the optimization flow to explore the search space extensively. We guarantee
    the validity of genomes after each mutation in the implementation. At the bottom
    of Figure [9,](#page-7-2) we show a node-level operation (modify-node) and two
    subgraph-level ones (split-subgraph and merge-subgraph):


    - modify-node (Figure [9\(](#page-7-2)c)): Modify the assignment of a randomly
    selected node : from → () to → ′ (), where ′ () can be an existed subgraph or
    a new one.

    - split-subgraph (Figure [9\(](#page-7-2)d)): Split a randomly selected subgraph
    into two or more subgraphs.

    - merge-subgraph (Figure [9\(](#page-7-2)e)): Merge two randomly selected subgraphs
    into one subgraph.

    - mutation-DSE (not shown): Modify the memory configuration to a random one within
    the range. The new values are sampled based on a normal distribution, where the
    average is the original value, and the variance is a hyper-parameter.


    4.4.4 Evaluation. Since GA tries to maximize the fitness of the genomes, we set
    fitness to be the opposite of the cost (e.g., Formula [1](#page-5-4) and [2\)](#page-6-0).
    To evaluate the fitness of each genome in the population, we use our simulator
    (introduced in the next section) to extract the execution costs of subgraphs (e.g.,
    EMA and energy).


    During the evaluation, the simulator decodes the subgraph and hardware configuration
    of each genome and calculates the fitness by aggregating the cost of each subgraph.
    Particularly, when a large subgraph exceeds the buffer capacity, we perform the
    split-subgraph operation to ensure genome validity. This kind of in-situ tuning
    can increase the number of valid samples during the optimization operations and
    thus, improve the sample efficiency.


    4.4.5 Selection. At the end of each generation, Cocco performs the tournament
    selection. Specifically, it holds multiple tournaments among a few randomly selected
    genomes, and the winners (the genome with the best fitness) of these tournaments
    form the population of a new generation. This operation facilitates superior fitness
    in the new generation. The number of genomes in each tournament is decided by
    a hyper-parameter. The new generation subsequently starts from the crossover step
    again.


    ## 5 Experiments


    In the evaluations, we first present the superiority of Cocco for the graph partition;
    and then demonstrate its outstanding stability and sample efficiency of the co-exploration
    for the hardware optimization, followed by additional discussions about the results
    under different configurations.


    #### 5.1 Methodology


    5.1.1 Evaluated Models. In the following evaluations, we consider three types
    of model structures: plain (VGG16 [\[57\]](#page-14-2)), multi-branch (ResNet50,
    ResNet152 [\[20\]](#page-13-4), GoogleNet [\[59\]](#page-14-3), Transformer [\[64\]](#page-14-22),
    and GPT [\[52\]](#page-14-23)), and irregular structure (RandWire-A/B [\[68\]](#page-14-6)
    and NasNet [\[75\]](#page-15-1)). RandWire-A/B are


    generated based on the small and regular regime configurations introduced in the
    paper [\[68\]](#page-14-6). FC layers are transformed to 1×1 CONV while pooling
    and element-wise layers are analyzed as depth-wise CONV without weights. The scalar
    operations (e.g., activation function) are hidden in the pipeline (e.g., the post-process
    module following PE in Simba [\[56\]](#page-14-20)) and their overhead can be
    ignored.


    5.1.2 Accelerator Platform. As shown at the top of Figure [10,](#page-7-1) we
    consider a SIMBA-like hierarchical accelerator with a global buffer, a weight
    buffer, and a 4×4 PE array in each core used in several previous works [\[56,](#page-14-20)
    [61,](#page-14-9) [71\]](#page-15-6). Each PE contains an 8×8 MAC array to process
    a sub-tile from the global buffer. In particular, we model the execution flow
    based on the scheme described in Section [3.](#page-3-1) The parallelism of two
    dimensions of the PE array can be dynamically configured by the mapper results
    to ensure high utilization. We schedule subgraphs in topological order and prefetch
    weights of the next subgraph during the current computing. We also extend our
    platform to support fundamental multi-core studies by interconnecting cores with
    a crossbar. They share weights to release the burden of each core.


    The arithmetic and memory overhead is extracted in a 12nm library based on the
    synthesized RTL implementations (SRAM based on the ARM memory compiler) with 1GHz.
    The DRAM energy is set as 12.5pJ/bit [\[70\]](#page-15-2). The extra footprint
    of the plug-in design is mainly a 272-Byte register file to store the head and
    end logical region addresses of maximal 64 nodes, which is negligible. Based on
    off-the-shelf evaluators Timeloop [\[50\]](#page-14-18) and MAESTRO [\[37\]](#page-13-7)
    for spatial accelerators, we developed a modified simulator that supports the
    evaluation of latency and energy. It employs the consumption-centric scheme to
    determine the tile size of each layer, and the memory access in the model is free
    from padding data. The latency per subgraph depends on the maximum of the calculation
    and external communication cycles. We allocate 16GB/s DRAM bandwidth per accelerator
    core for loading weights and input activations and writing back data for subsequent
    subgraphs. The off-chip communication consists of weight loading of each layer
    and the inputs and outputs of each subgraph. As described in Section [3,](#page-3-1)
    our subgraph execution scheme avoids recomputing of intermediate outputs.


    5.1.3 Baselines. Three optimization baselines for graph partition are the greedy
    algorithm used in Halide [\[47\]](#page-14-21), dynamic programming (DP) used
    in Irregular-NN [\[73\]](#page-15-3) , and the enumeration-based method as a reference.


    For the DSE studies, we compare Cocco with simulated annealing (SA) [\[33\]](#page-13-20)
    to demonstrate the better stability of GA. These two methods are both the co-optimization
    scheme that optimizes partition and hardware settings at the same time. In contrast
    to co-optimization, the two-step scheme is another method for design-space exploration.
    Specifically, we


    <span id="page-9-0"></span>![](_page_9_Figure_2.jpeg)


    Figure 11. The evaluation results for graph partition using the EMA-opt configuration
    (EMA as the optimization metric). The enumeration-based method is deterministic,
    which figures out the optimal solution as a reference in the first four models.
    It cannot complete for large-scale models (Transformer, GPT, RandWire-A, and RandWire-B)
    in a reasonable time because of the exponential search space.


    use random search (RS) or grid search (GS) to sample memory capacity candidates
    and then explore the corresponding partition schemes. During the search, we evaluate
    5,000 samples for each capacity candidate and keep the best candidate as the output.
    As for the sampling method, RS randomly samples memory capacity candidates while
    GS uses a coarser granularity to enumerate the candidates.


    #### 5.2 Graph Partition Evaluations


    We start by presenting the partition performance on the single-core hardware with
    a 1MB global buffer and a 1.125MB weight buffer. The number of samples in Cocco
    is set to be 400,000. We evaluate the external memory access (EMA) and bandwidth
    requirements of eight models shown in Figure [11,](#page-9-0) where the results
    are normalized to the Halide baseline. This experiment aims to validate the effectiveness
    of our Cocco framework in graph partition. For networks with simpler structures,
    Cocco can find out the optimal solutions same as the enumeration-based results.
    For large-scale irregular networks (Transformer, GPT, RandWire-A, and RandWire-B),
    the enumeration-based method cannot complete in a reasonable time, while Cocco
    provides better solutions than Halide and DP. A better subgraph partition strategy
    helps to ease the communication burden, thus reducing the EMA cost and bandwidth
    requirements.


    #### 5.3 Hardware-Mapping Co-Exploration


    After learning the superiority of Cocco for the graph partition, we further co-explore
    the memory configuration and graph partition mapping as the core study of this
    work. Three categories of exploration methods are used, including the fixed hardware
    scheme, the two-step scheme as baselines, and the proposed co-optimization scheme.
    We set three fixed memory configurations with Small capacity, Medium capacity,
    and Large capacity, followed by a partition-only procedure. The two-step scheme
    is implemented with decoupled steps for capacity search (RS or GS) and partition
    (GA). The cooptimization methods include the proposed Cocco and an SA-based one
    as the comparison. All methods sample up to


    <span id="page-9-2"></span>Table 1. Hardware-mapping co-exploration for separate
    buffer. In this table, A refers to the global buffer, and W refers to the weight
    buffer. We evaluate the cost using Formula [2](#page-6-0) (the lower cost, the
    better), where the metric is energy. We use RandWire-A as RandWire in the following
    experiments.


    | Optimization |        | ResNet50      |                   |        | GoogleNet
    |                   |        |  |

    |--------------|--------|---------------|-------------------|--------|-----------|-------------------|--------|--|

    |              |        |               | Size (A) Size (W) | Cost   |           |
    Size (A) Size (W) | Cost   |  |

    |              | Buf(S) | 512KB         | 576KB             | 1.04E7 | 512KB     |
    576KB             | 4.07E6 |  |

    | Fixed<br>HW  |        | Buf(M) 1024KB | 1152KB            | 1.07E7 | 1024KB    |
    1152KB            | 5.06E6 |  |

    |              | Buf(L) | 2048KB        | 2304KB            | 1.24E7 | 2048KB    |
    2304KB            | 7.18E6 |  |

    |              | RS+GA  | 448KB         | 864KB             | 1.04E7 | 384KB     |
    432KB             | 3.88E6 |  |

    | Two-Step     | GS+GA  | 128KB         | 864KB             | 1.07E7 | 128KB     |
    144KB             | 3.80E6 |  |

    |              | SA     | 256KB         | 360KB             | 1.06E7 | 192KB     |
    144KB             | 3.78E6 |  |

    | Co-Opt       | Cocco  | 704KB         | 864KB             | 1.04E7 | 192KB     |
    432KB             | 3.75E6 |  |

    |              |        |               |                   |        |           |                   |        |  |

    |              |        |               | RandWire          |        |           |
    NasNet            |        |  |

    | Optimization |        |               | Size (A) Size (W) | Cost   |           |
    Size (A) Size (W) | Cost   |  |

    |              | Buf(S) | 512KB         | 576KB             | 3.23E6 | 512KB     |
    576KB             | 6.14E7 |  |

    | Fixed        |        | Buf(M) 1024KB | 1152KB            | 3.92E6 | 1024KB    |
    1152KB            | 5.83E7 |  |

    | HW           | Buf(L) | 2048KB        | 2304KB            | 6.00E6 | 2048KB    |
    2304KB            | 5.66E7 |  |

    |              | RS+GA  | 448KB         | 792KB             | 3.31E6 | 1152KB    |
    2016KB            | 5.60E7 |  |

    | Two-Step     | GS+GA  | 128KB         | 144KB             | 3.02E6 | 2048KB    |
    2304KB            | 5.66E7 |  |

    | Co-Opt       | SA     | 192KB         | 144KB             | 3.00E6 | 2048KB    |
    1872KB            | 5.61E7 |  |


    50,000 points. The energy-capacity co-optimization is used in the following evaluations.


    5.3.1 DSE analysis using separate and shared buffer. We first perform the hardware-mapping
    co-exploration to determine the suitable memory configuration (except for the
    fixed-HW scheme) with = 0.002[4](#page-9-1) and then solely execute the partition-only
    Cocco to obtain the final cost. In particular, we also compared the results using
    two memory designs: separate buffer and shared buffer. For the separate buffer
    design, activations and weights are stored in different buffers while they share
    the same space in the shared buffer design. The memory capacity candidates for
    the global buffer


    <span id="page-9-1"></span><sup>4</sup>The energy and the capacity units are pJ
    and Byte, respectively.


    Cocco: Hardware-Mapping Co-Exploration towards Memory ... ASPLOS ''24, April 27-May
    1, 2024, La Jolla, CA, USA


    <span id="page-10-0"></span>Table 2. Hardware-mapping co-exploration for shared
    buffer. We evaluate the cost using Formula [2](#page-6-0) (the lower cost, the
    better), where the metric is energy.


    | Optimization |        |        | ResNet50 | GoogleNet |        |  |

    |--------------|--------|--------|----------|-----------|--------|--|

    |              |        | Size   | Cost     | Size      | Cost   |  |

    | Fixed<br>HW  | Buf(S) | 576KB  | 1.01E7   | 576KB     | 3.66E6 |  |

    |              | Buf(M) | 1152KB | 1.00E7   | 1152KB    | 4.04E6 |  |

    |              | Buf(L) | 2304KB | 1.04E7   | 2304KB    | 5.09E6 |  |

    |              | RS+GA  | 1280KB | 0.98E7   | 640KB     | 3.65E6 |  |

    | Two-Step     | GS+GA  | 1344KB | 0.98E7   | 512KB     | 3.65E6 |  |

    |              | SA     | 896KB  | 1.00E7   | 192KB     | 3.75E6 |  |

    | Co-Opt       | Cocco  | 1344KB | 0.98E7   | 384KB     | 3.60E6 |  |

    |              |        |        |          |           |        |  |

    |              |        |        | RandWire | NasNet    |        |  |

    | Optimization |        | Size   | Cost     | Size      | Cost   |  |

    |              | Buf(S) | 576KB  | 2.83E6   | 576KB     | 6.36E7 |  |

    | Fixed        | Buf(M) | 1152KB | 3.03E6   | 1152KB    | 5.73E7 |  |

    | HW           | Buf(L) | 2304KB | 3.90E6   | 2304KB    | 5.51E7 |  |

    |              | RS+GA  | 320KB  | 2.85E6   | 2560KB    | 5.47E7 |  |

    | Two-Step     | GS+GA  | 832KB  | 2.86E6   | 3072KB    | 5.42E7 |  |

    | Co-Opt       | SA     | 256KB  | 2.92E6   | 1728KB    | 5.56E7 |  |


    (for activations) range from 128KB to 2048KB with a 64KB interval, while that
    for the weight buffer range from 144KB to 2304KB with a 72KB interval. The exploration
    range of the shared buffer is from 128KB to 3072KB with an interval of 64KB.


    The evaluation using separate buffers is shown in Table [1,](#page-9-2) where
    Cocco achieves better optimization with up to 1.89% (compared to SA in ResNet50)
    to 50.33% (compared to Fixed-HW(L) in RandWire) lower cost compared to various
    baselines across four models. The two-step scheme fails to combine the information
    between different sizes, so it is generally worse than the co-optimization method.


    The capacity results also reflect the inherent capacity preference of different
    models. The data amount in GoogleNet and RandWire is relatively smaller, and thus
    their capacity requirements are lower. In contrast, the data amount in NasNet
    is larger, so a high capacity is preferred.


    As shown in Table [2,](#page-10-0) the evaluation of the shared buffer setting
    shows a similar trend. Furthermore, we can observe that most of the cost results
    of the shared buffer are lower than those using the separate configuration. Although
    the shared buffer design requires additional control flows, it indeed improves
    efficiency than the separate buffer design.


    5.3.2 Sample efficiency analysis. We next study the sample efficiency of the two-step
    and the co-optimization scheme in Figure [12.](#page-10-1) We record the cost
    trends of the first 50,000 samples on ResNet50, GoogleNet, and RandWire during
    the exploration. Overall, Cocco shows a consistent convergence trend on these
    three networks. And it converges faster and


    <span id="page-10-1"></span>![](_page_10_Figure_9.jpeg)


    Figure 12. The convergence curve of Cocco compared with other baselines in the
    hardware-mapping co-explorations. The optimization method requiring fewer samples
    in (d) has higher sample efficiency.


    <span id="page-10-2"></span>![](_page_10_Figure_11.jpeg)


    Figure 13. The visualization of sample points distribution during optimization.
    The slope of the red dashed line denotes the preference between energy and capacity
    cost. The point on the line with a lower intercept has a smaller cost.


    achieves lower costs compared to other baselines, exhibiting a higher sample efficiency.
    The two-step methods perform graph-partition separately under different capacities,
    so they fail to utilize the partition information between capacities. Particularly,
    the GS method uses a deterministic search direction (search from large to small
    capacity in this experiment), so the convergence time depends on the optimal capacity.
    Since GoogleNet and RandWire require relatively small buffers, GS takes a considerable
    number of samples to converge.


    5.3.3 Optimization procedure analysis. We next study how the distribution of sample
    points changes during the optimization procedure of Cocco. While searching for
    20 generations with 500 genomes each, we divided them into ten groups with different
    colors in Figure [13.](#page-10-2) The results show that the distribution moves
    towards a lower intercept


    <span id="page-11-0"></span>![](_page_11_Figure_1.jpeg)


    Figure 14. The trade-off between energy and memory capacity. The optimization
    target is to minimize the cost defined in Formula [2,](#page-6-0) where the metric
    is energy. Energy results of each model are normalized to the first (= 0.0005)
    results.


    <span id="page-11-1"></span>Table 3. Multi-core and batch evaluation using the
    energycapacity co-opt configuration. Size denotes the shared buffer size in each
    core.


    | Core# Batch |             |            | ResNet50     |            | GoogleNet           |                 |              |  |

    |-------------|-------------|------------|--------------|------------|---------------------|-----------------|--------------|--|

    |             |             | Energy(mJ) | Lat.(ms)     |            | Size(KB)
    Energy(mJ) | Lat.(ms)        | Size(KB)     |  |

    | 1           | 1           | 4.21       | 4.59         | 1344       | 1.61                |
    2.05            | 384          |  |

    |             | 2           | 6.32       | 8.98         | 1728       | 2.18                |
    3.91            | 896          |  |

    |             | 8           | 11.88      | 35.93        | 2880       | 5.64                |
    15.53           | 1472         |  |

    |             | 1           | 4.38       | 2.48         | 768        | 1.66                |
    1.04            | 192          |  |

    | 2           | 2           | 6.46       | 4.78         | 1088       | 2.34                |
    1.99            | 384          |  |

    |             | 8           | 13.01      | 19.12        | 1664       | 5.84                |
    7.97            | 960          |  |

    |             | 1           | 4.29       | 1.39         | 448        | 1.34                |
    0.54            | 192          |  |

    | 4           | 2           | 6.58       | 2.68         | 640        | 2.20                |
    1.07            | 192          |  |

    |             | 8           | 11.50      | 10.71        | 1664       | 6.24                |
    4.30            | 448          |  |

    |             |             |            |              |            |                     |                 |              |  |

    |             |             |            | RandWire     |            |                     |
    NasNet          |              |  |

    |             | Core# Batch | Energy(mJ) | Lat.(ms)     |            | Size(KB)
    Energy(mJ) | Lat.(ms)        | Size(KB)     |  |

    |             | 1           | 1.26       | 1.47         | 384        | 28.57               |
    49.92           | 2624         |  |

    | 1           | 2           | 2.25       | 2.74         | 704        | 47.68               |
    99.87           | 3072         |  |

    |             | 8           | 8.66       | 10.85        | 1664       | 133.03              |
    396.90          | 3072         |  |

    |             | 1           | 1.41       | 0.95         | 192        | 29.18               |
    24.93           | 1728         |  |

    | 2           | 2           | 2.37       | 1.80         | 384        | 48.80               |
    49.73           | 2624         |  |

    |             | 8           | 8.39       | 7.16         | 1280       | 153.25              |
    227.19          | 3072         |  |

    |             | 1           | 1.39       | 0.71         | 192        | 28.00               |
    14.56           | 960          |  |

    | 4           | 2           | 2.91       | 1.40<br>5.55 | 192<br>960 | 45.03               |
    28.58<br>133.38 | 1664<br>2816 |  |


    of the -slope line and gets more centralized in the later generations during the
    optimization process of Cocco.


    #### 5.4 Sensitivity Study about Cocco framework


    5.4.1 Study of in the cost function. The results shown in Figure [14](#page-11-0)
    demonstrate the effectiveness of in adjusting the preference between the memory
    capacity and the given metric (energy is used here). The optimization trades the
    memory capacity for lower energy cost with the increase of . In addition, a larger
    memory capacity indeed contributes to lower energy, but the yields show differences
    because of their various model-inherent graph and layer patterns. For example,
    NasNet is more memory-intensive and more structure-complex than the other three
    models, so it requires a larger memory capacity for less energy consumption.


    5.4.2 Study of performance v.s. memory capacity. Figure [2](#page-2-0) shows that
    the increase of capacity is sub-linear with


    performance. To study this observation, we scale our model to the multi-core version
    and share weights of a subgraph across cores. Different cores only buffer a subset
    of weights and transfer the data between cores, similar to BSD in Tangram [\[18\]](#page-13-11)
    or data-rotation in NN-Baton [\[61\]](#page-14-9). The overhead of the interconnection
    crossbar is extracted from the implemented Arteries IP [\[5\]](#page-12-9).


    An accelerator with more cores can cover a larger subgraph but bring more core-to-core
    overhead. As shown in Table [3,](#page-11-1) in most cases, energy increases from
    the single-core to dual-core configuration because of the communication overhead.
    Moreover, profiting from the data-sharing mechanism, the required memory of each
    core drops with the increase of core number.


    5.4.3 Batch size study. For the batch size evaluation shown in Table [3,](#page-11-1)
    the latency with a larger batch size principally presents a sub-linear increase,
    which benefits from the lower bandwidth requirement of weights via the inter-sample
    data reuse. In addition, such data reuse amortizes the energy burden per batch
    processing. And owing to the better weight reuse in multi-batch processing, a
    larger batch size does not require a proportional capacity.


    ## 6 Related Works


    #### 6.1 Intra-layer Optimization


    Prior works focus on the data reuse for intra-layer assignments, like output-stationary
    in ShiDianNao [\[14\]](#page-13-22) and Envision [\[46\]](#page-14-24), weight-stationary
    in NeuFlow [\[15\]](#page-13-23) and Nvdla [\[49\]](#page-14-16), input-stationary
    in SCNN [\[51\]](#page-14-25), and row-stationary in Eyeriss [\[13\]](#page-12-10).
    Based on these primitive dataflow patterns, extensive studies explored the optimal
    tiling and reordering schemes via brute-force, feedback-based, and constraint
    optimization approaches [\[23,](#page-13-5) [30,](#page-13-18) [50\]](#page-14-18).
    These works focus on layer-level optimization, missing the graph information at
    a higher level. The efficiency of tile updates depends on the memory architecture.
    Simba [\[56,](#page-14-20) [74\]](#page-15-5) and NN-Baton [\[61\]](#page-14-9)
    view each tile as an independent workload so that the tile size has a prominent
    impact on memory access due to halo regions. Motivated by traditional vision processors,
    Ascend [\[40\]](#page-13-10) and DRQ [\[58\]](#page-14-26) employ line buffers
    to achieve data reuse in the row direction, but the line buffer cannot well support
    the 2D-tiling reuse in both row and column directions.


    #### 6.2 Inter-layer Optimization


    Intra-layer scheduling is sub-optimal, which is limited by the data reuse within
    a layer. Therefore, Fused-CNN [\[4\]](#page-12-2), SR-CNN [\[38\]](#page-13-8),
    and LCP [\[42\]](#page-14-19) introduce layer fusion method that cache intermediate
    data on-chip to reduce data transfer overhead using handcrafted or heuristic methods
    for fusion partition. Although Irregular-NN [\[73\]](#page-15-3) suggests a customized-DP
    algorithm, the exploration space is constrained because the layers in an assignment
    need to be successive in a specific


    order. A recent work named DNNFuser [\[29\]](#page-13-24) employs an RLbased method,
    but their formulation towards 1D layer-fusion is hard to handle complex irregular
    networks. Tangram [\[18\]](#page-13-11) and Atomic [\[72\]](#page-15-4) schedule
    DNN workloads on a multi-core (scalable) accelerator, but they focus on executing
    a single layer on each core at a time rather than processing multiple layers with
    local data reuse. Also, some previous works [\[2,](#page-12-11) [19,](#page-13-25)
    [62\]](#page-14-27) tackle the workload placement problem for multiple devides
    without discussing the downstream execution on each device.


    Cocco proposes an automatic framework for inter-layer scheduling with a comprehensive
    memory scheme. It focuses on the fundamental core-level temporal execution that
    can be potentially scaled up to the multi-core or multi-device scenario with a
    spatial parallelism mechanism.


    #### 6.3 Design-Space Exploration for Memory


    Memory design exploration methods lie primarily on two sides: analysis-driven
    and search-driven. For the analysisdriven method, Chen et al. [\[12\]](#page-12-0)
    leverage red-blue pebble models to derive the proper memory capacity representations.
    Subsequently, Cai et al. [\[9\]](#page-12-12) propose Olympus, which generalizes
    a framework to a batch of successive layers and also fills up with more scheduling
    and data reuse techniques. However, they are difficult to represent a subgraph
    with complex inter-layer connections. As for the search-driven method, Xiao et
    al. [\[67\]](#page-14-11), Kwon et al. [\[37\]](#page-13-7), and Feng et al. [\[16\]](#page-13-26)
    explore the memory configuration for the layer-level assignment using the brute-force
    search, while Kao et al. [\[32\]](#page-13-6) employ a genetic algorithm to improve
    the efficiency. These works principally focus on the layer-level information,
    while in comparison, Cocco exploits graph-level features for the better optimization.


    ## 7 Conclusion


    While layer-level scheduling is widely studied to improve memory efficiency, graph-level
    optimization remains relatively unexplored. This paper proposed a graph-level
    dataflow with the corresponding memory management scheme that enables flexible
    graph partitions with high memory utilization. On top of it, we propose Cocco,
    a framework to provide a recommended memory configuration with graph-level scheduling
    strategies. Cocco shows outstanding graph partition ability compared to the greedy
    algorithm and DP employed in previous works and enables efficient graph-level
    hardware-mapping co-exploration. This paper helps to provide an implementation
    philosophy for the accelerator memory and better deployment for it.


    ## Acknowledgments


    This research was partially supported by National Key R&D Program of China (2022YFB2804103),
    Tsinghua University Dushi Program, and Tsinghua University Talent Program. We
    would like to appreciate all the anonymous reviewers for their valuable feedback.


    ## References


    - <span id="page-12-5"></span>[1] Dennis Abts, Jonathan Ross, Jonathan Sparling,
    Mark Wong-VanHaren, Max Baker, Tom Hawkins, Andrew Bell, John Thompson, Temesghen
    Kahsai, Garrin Kimmell, Jennifer Hwang, Rebekah Leslie-Hurd, Michael Bye, E. R.
    Creswick, Matthew Boyd, Mahitha Venigalla, Evan Laforge, Jon Purdy, Purushotham
    Kamath, Dinesh Maheshwari, Michael Beidler, Geert Rosseel, Omar Ahmad, Gleb Gagarin,
    Richard Czekalski, Ashay Rane, Sahil Parmar, Jeff Werner, Jim Sproch, Adrian Macias,
    and Brian Kurtz. 2020. Think Fast: A Tensor Streaming Processor (TSP) for Accelerating
    Deep Learning Workloads. In Proceedings of the 47th ACM/IEEE Annual International
    Symposium on Computer Architecture (ISCA). IEEE, Valencia, Spain, 145–158.

    - <span id="page-12-11"></span>[2] Ravichandra Addanki, Shaileshh Bojja Venkatakrishnan,
    Shreyan Gupta, Hongzi Mao, and Mohammad Alizadeh. 2019. Learning Generalizable
    Device Placement Algorithms for Distributed Machine Learning. In Advances in Neural
    Information Processing Systems (NeurIPS), Hanna M. Wallach, Hugo Larochelle, Alina
    Beygelzimer, Florence d''Alché-Buc, Emily B. Fox, and Roman Garnett (Eds.). OpenReview.net,
    Vancouver, BC, Canada, 3983–3993.

    - <span id="page-12-1"></span>[3] Byung Hoon Ahn, Jinwon Lee, Jamie Menjay Lin,
    Hsin-Pai Cheng, Jilei Hou, and Hadi Esmaeilzadeh. 2020. Ordering Chaos: Memory-Aware
    Scheduling of Irregularly Wired Neural Networks for Edge Devices. In Proceedings
    of Machine Learning and Systems (MLSys), Inderjit S. Dhillon, Dimitris S. Papailiopoulos,
    and Vivienne Sze (Eds.). mlsys.org, Austin, TX, USA, 1–14.

    - <span id="page-12-2"></span>[4] Manoj Alwani, Han Chen, Michael Ferdman, and
    Peter A. Milder. 2016. Fused-layer CNN accelerators. In Proceedings of the 49th
    IEEE/ACM International Symposium on Microarchitecture (MICRO). IEEE Computer Society,
    Taipei, Taiwan, 22:1–22:12.

    - <span id="page-12-9"></span>[5] Arteries. 2022. Arteries IP Homepage. <https://www.arteris.com>.

    - <span id="page-12-3"></span>[6] Ljubisa Bajic and Jasmina Vasiljevic. 2020.
    Compute substrate for Software 2.0. In Proceedings of the IEEE Hot Chips 32 Symposium
    (HCS). IEEE, Palo Alto, CA, USA, 1–31.

    - <span id="page-12-6"></span>[7] Pete Bannon, Ganesh Venkataramanan, Debjit Das
    Sarma, and Emil Talpes. 2019. Computer and Redundancy Solution for the Full Self-Driving
    Computer. In Proceedings of the IEEE Hot Chips 31 Symposium (HCS). IEEE, Cupertino,
    CA, USA, 1–22.

    - <span id="page-12-7"></span>[8] John Burgess. 2019. RTX ON - The NVIDIA TURING
    GPU. In Proceedings of the IEEE Hot Chips 31 Symposium (HCS). IEEE, Cupertino,
    CA, USA, 1–27.

    - <span id="page-12-12"></span>[9] Xuyi Cai, Ying Wang, Kaijie Tu, Chengsi Gao,
    and Lei Zhang. 2022. Olympus: Reaching Memory-Optimality on DNN Processors. IEEE
    Transactions on Computers (TC) 71, 8 (2022), 1939–1951.

    - <span id="page-12-8"></span>[10] Prasanth Chatarasi, Hyoukjun Kwon, Angshuman
    Parashar, Michael Pellauer, Tushar Krishna, and Vivek Sarkar. 2022. Marvel: A
    Data-Centric Approach for Mapping Deep Learning Operators on Spatial Accelerators.
    ACM Transactions on Architecture and Code Optimization 19, 1 (2022), 6:1–6:26.

    - <span id="page-12-4"></span>[11] Karam Chatha. 2021. Qualcomm® Cloud Al-100:
    12TOPS/W Scalable, High Performance and Low Latency Deep Learning Inference Accelerator.
    In Proceedings of the IEEE Hot Chips 33 Symposium (HCS). IEEE, Palo Alto, CA,
    USA, 1–19.

    - <span id="page-12-0"></span>[12] Xiaoming Chen, Yinhe Han, and Yu Wang. 2020.
    Communication Lower Bound in Convolution Accelerators. In Proceedings of the IEEE
    International Symposium on High Performance Computer Architecture (HPCA). IEEE,
    San Diego, CA, USA, 529–541.

    - <span id="page-12-10"></span>[13] Yu-Hsin Chen, Joel S. Emer, and Vivienne Sze.
    2016. Eyeriss: A Spatial Architecture for Energy-Efficient Dataflow for Convolutional
    Neural Networks. In Proceedings of the ACM/IEEE Annual International Symposium
    on Computer Architecture (ISCA). IEEE Computer Society, Seoul,


    South Korea, 367–379.


    - <span id="page-13-22"></span>[14] Zidong Du, Robert Fasthuber, Tianshi Chen,
    Paolo Ienne, Ling Li, Tao Luo, Xiaobing Feng, Yunji Chen, and Olivier Temam. 2015.
    ShiDianNao: shifting vision processing closer to the sensor. In Proceedings of
    the ACM/IEEE Annual International Symposium on Computer Architecture (ISCA). ACM,
    Portland, OR, USA, 92–104.

    - <span id="page-13-23"></span>[15] Clément Farabet, Berin Martini, B. Corda,
    Polina Akselrod, Eugenio Culurciello, and Yann LeCun. 2011. NeuFlow: A runtime
    reconfigurable dataflow processor for vision. In Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition (CVPR) Workshops. IEEE Computer Society,
    Colorado Springs, CO, USA, 109–116.

    - <span id="page-13-26"></span>[16] Kaijie Feng, Xiaoya Fan, Jianfeng An, Xiping
    Wang, Kaiyue Di, Jiangfei Li, Minghao Lu, and Chuxi Li. 2021. ERDSE: efficient
    reinforcement learning based design space exploration method for CNN accelerator
    on resource limited platform. Graphics and Visual Computing 4 (2021), 1–11.

    - <span id="page-13-0"></span>[17] Ken-ichi Funahashi. 1989. On the approximate
    realization of continuous mappings by neural networks. Neural Networks 2, 3 (1989),
    183–192.

    - <span id="page-13-11"></span>[18] Mingyu Gao, Xuan Yang, Jing Pu, Mark Horowitz,
    and Christos Kozyrakis. 2019. TANGRAM: Optimized Coarse-Grained Dataflow for Scalable
    NN Accelerators. In Proceedings of the International Conference on Architectural
    Support for Programming Languages and Operating Systems (ASPLOS). ACM, Providence,
    RI, USA, 807–820.

    - <span id="page-13-25"></span>[19] Yuanxiang Gao, Li Chen, and Baochun Li. 2018.
    Spotlight: Optimizing Device Placement for Training Deep Neural Networks. In Proceedings
    of the 35th International Conference on Machine Learning (ICML) (Proceedings of
    Machine Learning Research, Vol. 80), Jennifer G. Dy and Andreas Krause (Eds.).
    PMLR, Stockholm, Sweden, 1662–1670.

    - <span id="page-13-4"></span>[20] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and
    Sun Jian. 2016. Deep Residual Learning for Image Recognition. In Proceedings of
    the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE Computer
    Society, Las Vegas, NV, USA, 770–778.

    - <span id="page-13-17"></span>[21] Kartik Hegde, Po-An Tsai, Sitao Huang, Vikas
    Chandra, Angshuman Parashar, and Christopher W. Fletcher. 2021. Mind mappings:
    enabling efficient algorithm-accelerator mapping space search. In Proceedings
    of the 26th ACM International Conference on Architectural Support for Programming
    Languages and Operating Systems (ASPLOS), Tim Sherwood, Emery D. Berger, and Christos
    Kozyrakis (Eds.). ACM, Virtual Event, USA, 943–958.

    - <span id="page-13-1"></span>[22] Kurt Hornik, Maxwell B. Stinchcombe, and Halbert
    White. 1989. Multilayer feedforward networks are universal approximators. Neural
    Networks 2, 5 (1989), 359–366.

    - <span id="page-13-5"></span>[23] Qijing Huang, Aravind Kalaiah, Minwoo Kang,
    James Demmel, Grace Dinh, John Wawrzynek, Thomas Norell, and Yakun Sophia Shao.
    2021. CoSA: Scheduling by Constrained Optimization for Spatial Accelerators. In
    Proceedings of the ACM/IEEE Annual International Symposium on Computer Architecture
    (ISCA). IEEE, Valencia, Spain, 554–566.

    - <span id="page-13-9"></span>[24] Drago Ignjatovic, Daniel W. Bailey, and Ljubisa
    Bajic. 2022. The Wormhole AI Training Processor. In Proceedings of the IEEE International
    Solid-State Circuits Conference (ISSCC). IEEE, San Francisco, CA, USA, 356–358.

    - <span id="page-13-19"></span>[25] Abhinav Jangda and Uday Bondhugula. 2018.
    An effective fusion and tile size model for optimizing image processing pipelines.
    In Proceedings of the 23rd ACM SIGPLAN Symposium on Principles and Practice of
    Parallel Programming (PPoPP), Andreas Krall and Thomas R. Gross (Eds.). ACM, Vienna,
    Austria, 261–275.

    - <span id="page-13-15"></span>[26] Yang Jiao, Liang Han, Rong Jin, Yi-Jung Su,
    Chiente Ho, Li Yin, Yun Li, Long Chen, Zhen Chen, Lu Liu, Zhuyu He, Yu Yan, Jun
    He, Jun Mao, Xiaotao Zai, Xuejun Wu, Yongquan Zhou, Mingqiu Gu, Guocai Zhu, Rong
    Zhong, Wenyuan Lee, Ping Chen, Yiping Chen, Weiliang Li, Deyu Xiao, Qing Yan,
    Mingyuan Zhuang, Jiejun Chen, Yun Tian, Yingzi Lin, Wei Wu, Hao Li, and Zesheng
    Dou. 2020. A 12nm Programmable Convolution-Efficient Neural-Processing-Unit Chip


    Achieving 825TOPS. In Proceedings of the IEEE International Solid-State Circuits
    Conference (ISSCC). IEEE, San Francisco, CA, USA, 136–140.


    - [27] Yang Jiao, Liang Han, and Xin Long. 2020. Hanguang 800 NPU The Ultimate
    AI Inference Solution for Data Centers. In Proceedings of the IEEE Hot Chips 32
    Symposium (HCS). IEEE, Palo Alto, CA, USA, 1–29.

    - <span id="page-13-16"></span>[28] Norman P. Jouppi, Doe Hyun Yoon, Matthew Ashcraft,
    Mark Gottscho, Thomas B. Jablin, George Kurian, James Laudon, Sheng Li, Peter
    C. Ma, Xiaoyu Ma, Thomas Norrie, Nishant Patil, Sushma Prasad, Cliff Young, Zongwei
    Zhou, and David A. Patterson. 2021. Ten Lessons From Three Generations Shaped
    Google''s TPUv4i : Industrial Product. In Proceedings of the 48th ACM/IEEE Annual
    International Symposium on Computer Architecture (ISCA). IEEE, Valencia, Spain,
    1–14.

    - <span id="page-13-24"></span>[29] Sheng-Chun Kao, Xiaoyu Huang, and Tushar Krishna.
    2022. DNNFuser: Generative Pre-Trained Transformer as a Generalized Mapper for
    Layer Fusion in DNN Accelerators. arXiv preprint arXiv:2201.11218 abs/2201.11218
    (2022), 1–8.

    - <span id="page-13-18"></span>[30] Sheng-Chun Kao and Tushar Krishna. 2020. GAMMA:
    Automating the HW Mapping of DNN Models on Accelerators via Genetic Algorithm.
    In Proceedings of the IEEE/ACM International Conference On Computer Aided Design
    (ICCAD). IEEE, San Diego, CA, USA, 44:1–44:9.

    - <span id="page-13-21"></span>[31] Sheng-Chun Kao and Tushar Krishna. 2022. MAGMA:
    An Optimization Framework for Mapping Multiple DNNs on Multiple Accelerator Cores.
    In IEEE International Symposium on High-Performance Computer Architecture, (HPCA).
    IEEE, Seoul, South Korea, 814–830.

    - <span id="page-13-6"></span>[32] Sheng-Chun Kao, Michael Pellauer, Angshuman
    Parashar, and Tushar Krishna. 2022. DiGamma: Domain-aware Genetic Algorithm for
    HW-Mapping Co-optimization for DNN Accelerators. In Proceedings of the Design,
    Automation & Test in Europe Conference & Exhibition (DATE), Cristiana Bolchini,
    Ingrid Verbauwhede, and Ioana Vatajelu (Eds.). IEEE, Antwerp, Belgium, 232–237.

    - <span id="page-13-20"></span>[33] Scott Kirkpatrick, D. Gelatt Jr., and Mario
    P. Vecchi. 1983. Optimization by Simmulated Annealing. Sci. 220, 4598 (1983),
    671–680.

    - <span id="page-13-12"></span>[34] Simon Knowles. 2017. Scalable Silicon Compute.
    In Workshop on Deep Learning At Supercomputer Scale, NIPS. OpenReview.net, Long
    Beach, CA, USA, 1–22.

    - <span id="page-13-13"></span>[35] Simon Knowles. 2021. Graphcore. In Proceedings
    of the IEEE Hot Chips 33 Symposium (HCS). IEEE, Palo Alto, CA, USA, 1–25.

    - <span id="page-13-3"></span>[36] Alex Krizhevsky, Ilya Sutskever, and Geoffrey
    E. Hinton. 2012. ImageNet Classification with Deep Convolutional Neural Networks.
    In Proceedings of the 26th Annual Conference on Neural Information Processing
    Systems (NIPS). Curran Associates, Inc., Lake Tahoe, Nevada, United States, 1106–1114.

    - <span id="page-13-7"></span>[37] Hyoukjun Kwon, Prasanth Chatarasi, Michael
    Pellauer, Angshuman Parashar, Vivek Sarkar, and Tushar Krishna. 2019. Understanding
    Reuse, Performance, and Hardware Cost of DNN Dataflow: A Data-Centric Approach.
    In Proceedings of the IEEE/ACM International Symposium on Microarchitecture (MICRO).
    ACM, Columbus, OH, USA, 754–768.

    - <span id="page-13-8"></span>[38] Juhyoung Lee, Dongjoo Shin, Jinsu Lee, Jinmook
    Lee, Sanghoon Kang, and Hoi-Jun Yoo. 2019. A Full HD 60 fps CNN Super Resolution
    Processor with Selective Caching based Layer Fusion for Mobile Devices. In Proceedings
    of the Symposium on VLSI Circuits. IEEE, Kyoto, Japan, 302–303.

    - <span id="page-13-2"></span>[39] Grzegorz Lewicki and Giuseppe Marino. 2004.
    Approximation of functions of finite variation by superpositions of a Sigmoidal
    function. Appl. Math. Lett. 17, 10 (2004), 1147–1152.

    - <span id="page-13-10"></span>[40] Heng Liao, Jiajin Tu, Jing Xia, Hu Liu, Xiping
    Zhou, Honghui Yuan, and Yuxing Hu. 2021. Ascend: a Scalable and Unified Architecture
    for Ubiquitous Deep Neural Network Computing : Industry Track Paper. In Proceedings
    of the IEEE International Symposium on High-Performance Computer Architecture,
    HPCA. IEEE, Seoul, South Korea, 789–801.

    - <span id="page-13-14"></span>[41] Heng Liao, Jiajin Tu, Jing Xia, and Xiping
    Zhou. 2019. DaVinci: A Scalable Architecture for Neural Network Computing. In
    Proceedings


    Cocco: Hardware-Mapping Co-Exploration towards Memory ... ASPLOS ''24, April 27-May
    1, 2024, La Jolla, CA, USA


    of the IEEE Hot Chips 31 Symposium (HCS). IEEE, Cupertino, CA, USA, 1–44.


    - <span id="page-14-19"></span>[42] Xinhan Lin, Shouyi Yin, Fengbin Tu, Leibo
    Liu, Xiangyu Li, and Shaojun Wei. 2018. LCP: a layer clusters paralleling mapping
    method for accelerating inception and residual networks on FPGA. In Proceedings
    of the 55th Annual Design Automation Conference (DAC). ACM, San Francisco, CA,
    USA, 16:1–16:6.

    - <span id="page-14-7"></span>[43] Wenyan Lu, Guihai Yan, Jiajun Li, Shijun Gong,
    Yinhe Han, and Xiaowei Li. 2017. FlexFlow: A Flexible Dataflow Accelerator Architecture
    for Convolutional Neural Networks. In Proceedings of the IEEE International Symposium
    on High Performance Computer Architecture (HPCA). IEEE Computer Society, Austin,
    TX, USA, 553–564.

    - <span id="page-14-8"></span>[44] Yufei Ma, Yu Cao, Sarma B. K. Vrudhula, and
    Jae-sun Seo. 2017. Optimizing Loop Operation and Dataflow in FPGA Acceleration
    of Deep Convolutional Neural Networks. In Proceedings of the ACM/SIGDA International
    Symposium on Field-Programmable Gate Arrays (FPGA). ACM, Monterey, CA, USA, 45–54.

    - <span id="page-14-0"></span>[45] Marvin Minsky and Seymour Papert. 1987. Perceptrons
    - an introduction to computational geometry. MIT Press, .

    - <span id="page-14-24"></span>[46] Bert Moons, Roel Uytterhoeven, Wim Dehaene,
    and Marian Verhelst. 2017. Envision: A 0.26-to-10TOPS/W subword-parallel dynamicvoltage-accuracy-frequency-scalable
    Convolutional Neural Network processor in 28nm FDSOI. In Proceedings of the IEEE
    International Solid-State Circuits Conference (ISSCC). IEEE, San Francisco, CA,
    USA, 246–247.

    - <span id="page-14-21"></span>[47] Ravi Teja Mullapudi, Andrew Adams, Dillon
    Sharlet, Jonathan Ragan-Kelley, and Kayvon Fatahalian. 2016. Automatically scheduling
    halide image processing pipelines. ACM Trans. Graph. 35, 4 (2016), 83:1– 83:11.

    - <span id="page-14-12"></span>[48] Thomas Norrie, Nishant Patil, Doe Hyun Yoon,
    George Kurian, Sheng Li, James Laudon, Cliff Young, Norman P. Jouppi, and David
    A. Patterson. 2020. Google''s Training Chips Revealed: TPUv2 and TPUv3. In Proceedings
    of the IEEE Hot Chips 32 Symposium (HCS). IEEE, Palo Alto, CA, USA, 1–70.

    - <span id="page-14-16"></span>[49] NVIDIA. 2018. THE NVIDIA DEEP LEARNING ACCELERATOR.
    In Proceedings of the IEEE Hot Chips 30 Symposium (HCS). IEEE, Cupertino, CA,
    USA, 1–18.

    - <span id="page-14-18"></span>[50] Angshuman Parashar, Priyanka Raina, Yakun
    Sophia Shao, Yu-Hsin Chen, Victor A. Ying, Anurag Mukkara, Rangharajan Venkatesan,
    Brucek Khailany, Stephen W. Keckler, and Joel S. Emer. 2019. Timeloop: A Systematic
    Approach to DNN Accelerator Evaluation. In Proceedings of the IEEE International
    Symposium on Performance Analysis of Systems and Software (ISPASS). IEEE, Madison,
    WI, USA, 304–315.

    - <span id="page-14-25"></span>[51] Angshuman Parashar, Minsoo Rhu, Anurag Mukkara,
    Antonio Puglielli, Rangharajan Venkatesan, Brucek Khailany, Joel S. Emer, Stephen
    W. Keckler, and William J. Dally. 2017. SCNN: An Accelerator for Compressed-sparse
    Convolutional Neural Networks. In Proceedings of the 44th Annual International
    Symposium on Computer Architecture (ISCA). ACM, Toronto, ON, Canada, 27–40.

    - <span id="page-14-23"></span>[52] Alec Radford and Karthik Narasimhan. 2018.
    Improving Language Understanding by Generative Pre-Training. In Preprint. OpenAI,
    , 1– 12.

    - <span id="page-14-5"></span>[53] Esteban Real, Alok Aggarwal, Yanping Huang,
    and Quoc V. Le. 2019. Regularized Evolution for Image Classifier Architecture
    Search. In Proceedings of the 33rd Conference on Artificial Intelligence (AAAI).
    AAAI Press, Honolulu, Hawaii, USA, 4780–4789.

    - <span id="page-14-1"></span>[54] Frank Rosenblatt. 1957. The perceptron, a perceiving
    and recognizing automaton Project Para. Cornell Aeronautical Laboratory, .

    - <span id="page-14-4"></span>[55] Mark Sandler, Andrew G. Howard, Menglong Zhu,
    Andrey Zhmoginov, and Liang-Chieh Chen. 2018. MobileNetV2: Inverted Residuals
    and Linear Bottlenecks. In Proceedings of the IEEE Conference on Computer Vision
    and Pattern Recognition (CVPR). Computer Vision Foundation / IEEE Computer Society,
    Salt Lake City, UT, USA, 4510–4520.

    - <span id="page-14-20"></span>[56] Yakun Sophia Shao, Jason Clemons, Rangharajan
    Venkatesan, Brian Zimmer, Matthew Fojtik, Nan Jiang, Ben Keller, Alicia Klinefelter,
    Nathaniel Pinckney, Priyanka Raina, Stephen G. Tell, Yanqing Zhang, William J.
    Dally, Joel Emer, C. Thomas Gray, Brucek Khailany, and Stephen W. Keckler. 2019.
    Simba: Scaling Deep-Learning Inference with Multi-Chip-Module-Based Architecture.
    In Proceedings of the IEEE/ACM International Symposium on Microarchitecture (MICRO).
    ACM, Columbus, OH, USA, 14–27.

    - <span id="page-14-2"></span>[57] Karen Simonyan and Andrew Zisserman. 2015.
    Very Deep Convolutional Networks for Large-Scale Image Recognition. In Proceedings
    of the International Conference on Learning Representations (ICLR). Computational
    and Biological Learning Society, San Diego, CA, USA, 1–14.

    - <span id="page-14-26"></span>[58] Zhuoran Song, Bangqi Fu, Feiyang Wu, Zhaoming
    Jiang, Li Jiang, Naifeng Jing, and Xiaoyao Liang. 2020. DRQ: Dynamic Region-based
    Quantization for Deep Neural Network Acceleration. In Proceedings of the 47th
    ACM/IEEE Annual International Symposium on Computer Architecture (ISCA). IEEE,
    Valencia, Spain, 1010–1021.

    - <span id="page-14-3"></span>[59] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre
    Sermanet, Scott E. Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke,
    and Andrew Rabinovich. 2015. Going deeper with convolutions. In Proceedings of
    the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE Computer
    Society, Boston, MA, USA, 1–9.

    - <span id="page-14-13"></span>[60] Emil Talpes, Douglas Williams, and Debjit
    Das Sarma. 2022. DOJO: The Microarchitecture of Tesla''s Exa-Scale Computer. In
    Proceedings of the IEEE Hot Chips 34 Symposium (HCS). IEEE, Cupertino, CA, USA,
    1–28.

    - <span id="page-14-9"></span>[61] Zhanhong Tan, Hongyu Cai, Runpei Dong, and
    Kaisheng Ma. 2021. NN-Baton: DNN Workload Orchestration and Chiplet Granularity
    Exploration for Multichip Accelerators. In Proceedings of the IEEE Annual International
    Symposium on Computer Architecture (ISCA). IEEE, Valencia, Spain, 1013–1026.

    - <span id="page-14-27"></span>[62] Jakub Tarnawski, Amar Phanishayee, Nikhil
    R. Devanur, Divya Mahajan, and Fanny Nina Paravecino. 2020. Efficient Algorithms
    for Device Placement of DNN Graph Operators. In Advances in Neural Information
    Processing Systems (NeurIPS), Hugo Larochelle, Marc''Aurelio Ranzato, Raia Hadsell,
    Maria-Florina Balcan, and Hsuan-Tien Lin (Eds.). Open-Review.net, Virtual, 1–13.

    - <span id="page-14-14"></span>[63] Tenstorrent. 2021. Grayskull. <https://tenstorrent.com/grayskull/>.

    - <span id="page-14-22"></span>[64] Ashish Vaswani, Noam Shazeer, Niki Parmar,
    Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin.
    2017. Attention is All you Need. In Advances in Neural Information Processing
    Systems (NIPS), Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach,
    Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (Eds.). OpenReview.net, Long
    Beach, CA, USA, 5998–6008.

    - <span id="page-14-17"></span>[65] Ofri Wechsler, Michael Behar, and Bharat Daga.
    2019. Spring Hill (NNP-I 1000) Intel''s Data Center Inference Chip. In Proceedings
    of the IEEE Hot Chips 31 Symposium (HCS). IEEE, Cupertino, CA, USA, 1–12.

    - <span id="page-14-10"></span>[66] Jian Weng, Sihao Liu, Vidushi Dadu, Zhengrong
    Wang, Preyas Shah, and Tony Nowatzki. 2020. DSAGEN: Synthesizing Programmable
    Spatial Accelerators. In Proceedings of the 47th ACM/IEEE Annual International
    Symposium on Computer Architecture (ISCA). IEEE, Valencia, Spain, 268–281.

    - <span id="page-14-11"></span>[67] Qingcheng Xiao, Size Zheng, Bingzhe Wu, Pengcheng
    Xu, Xuehai Qian, and Yun Liang. 2021. HASCO: Towards Agile HArdware and Software
    CO-design for Tensor Computation. In Proceedings of the 48th ACM/IEEE Annual International
    Symposium on Computer Architecture (ISCA). IEEE, Valencia, Spain, 1055–1068.

    - <span id="page-14-6"></span>[68] Saining Xie, Alexander Kirillov, Ross B. Girshick,
    and Kaiming He. 2019. Exploring Randomly Wired Neural Networks for Image Recognition.
    In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV).
    IEEE, Seoul, South Korea, 1284–1293.

    - <span id="page-14-15"></span>[69] Andrew Yang. 2019. Deep Learning Training
    At Scale Spring Crest Deep Learning Accelerator (Intel® Nervana™ NNP-T). In Proceedings
    of the IEEE Hot Chips 31 Symposium (HCS). IEEE, Cupertino, CA, USA,


    <span id="page-15-0"></span>1–20.


    - <span id="page-15-2"></span>[70] Xuan Yang, Mingyu Gao, Qiaoyi Liu, Jeff Setter,
    Jing Pu, Ankita Nayak, Steven Bell, Kaidi Cao, Heonjae Ha, Priyanka Raina, Christos
    Kozyrakis, and Mark Horowitz. 2020. Interstellar: Using Halide''s Scheduling Language
    to Analyze DNN Accelerators. In Proceedings of the International Conference on
    Architectural Support for Programming Languages and Operating Systems (ASPLOS).
    ACM, Lausanne, Switzerland, 369–383.

    - <span id="page-15-6"></span>[71] Size Zheng, Renze Chen, Anjiang Wei, Yicheng
    Jin, Qin Han, Liqiang Lu, Bingyang Wu, Xiuhong Li, Shengen Yan, and Yun Liang.
    2022. AMOS: enabling automatic mapping for tensor computations on spatial accelerators
    with hardware abstraction. In Proceedings of the 49th Annual International Symposium
    on Computer Architecture (ISCA). ACM, New York, New York, USA, 874–887.

    - <span id="page-15-4"></span>[72] Shixuan Zheng, Xianjue Zhang, Leibo Liu, Shaojun
    Wei, and Shouyi Yin. 2022. Atomic Dataflow based Graph-Level Workload Orchestration
    for Scalable DNN Accelerators. In Proceedings of the IEEE International Symposium
    on High-Performance Computer Architecture (HPCA). IEEE, Seoul, South Korea, 475–489.

    - <span id="page-15-3"></span>[73] Shixuan Zheng, Xianjue Zhang, Daoli Ou, Shibin
    Tang, Leibo Liu, Shaojun Wei, and Shouyi Yin. 2020. Efficient Scheduling of Irregular
    Network Structures on CNN Accelerators. IEEE Transactions on Computer-Aided Design
    of Integrated Circuits and Systems (TCAD) 39, 11 (2020), 3408–3419.

    - <span id="page-15-5"></span>[74] Brian Zimmer, Rangharajan Venkatesan, Yakun
    Sophia Shao, Jason Clemons, Matthew Fojtik, Nan Jiang, Ben Keller, Alicia Klinefelter,
    Nathaniel Ross Pinckney, Priyanka Raina, Stephen G. Tell, Yanqing Zhang, William
    J. Dally, Joel S. Emer, C. Thomas Gray, Stephen W. Keckler, and Brucek Khailany.
    2019. A 0.11 pJ/Op, 0.32-128 TOPS, Scalable Multi-Chip-Module-based Deep Neural
    Network Accelerator with Ground-Reference Signaling in 16nm. In Proceedings of
    the IEEE Symposium on VLSI Circuits (VLSI). IEEE, Kyoto, Japan, 300.

    - <span id="page-15-1"></span>[75] Barret Zoph, Vijay Vasudevan, Jonathon Shlens,
    and Quoc V. Le. 2018. Learning Transferable Architectures for Scalable Image Recognition.
    In IEEE Conference on Computer Vision and Pattern Recognition, (CVPR). Computer
    Vision Foundation / IEEE Computer Society, Salt Lake City, UT, USA, 8697–8710.'
  decisions:
    evaluation_prompt: 'Qualified. Reason: The paper contains multiple sections and
      phrases indicating structured evaluation, including "Experiments demonstrate
      that Cocco obtains lower external memory access, lower bandwidth requirements,
      and more stable optimization for graph partition compared to the greedy algorithm
      and dynamic programming introduced in prior works." Additionally, there are
      tables and figures presenting quantifiable outcomes, such as Table 1 and Figure
      11, which show evaluation results.'
    related_work_prompt: '- Qualified. Reason: The paper meaningfully engages with
      prior research throughout its text, including in the Introduction, Background,
      and Related Works sections. It provides numerous academic citations, discusses
      previous methods, and compares its proposed method to existing work, demonstrating
      a thorough engagement with the academic literature.'
    novelty_prompt: 'Qualified. Reason: The paper introduces Cocco, a novel hardware-mapping
      co-exploration framework that leverages graph-level features of networks to
      optimize memory capacity and communication. It proposes a new subgraph execution
      scheme, an efficient dataflow and memory management method, and a genetic-based
      algorithm for hardware-mapping co-exploration. These contributions demonstrate
      novelty in method, application, and results.'
    review_only_prompt: 'Qualified. Reason: The paper introduces a novel hardware-mapping
      co-exploration framework called Cocco, which proposes new methods for optimizing
      memory capacity and communication in DNN accelerators. It includes original
      contributions such as a graph-level execution scheme, a genetic-based optimization
      framework, and experimental results demonstrating its effectiveness compared
      to existing methods.'
- title: Accelerating Boolean Constraint Propagation for Efficient SAT-Solving on
    FPGAs
  abstract: 'We present a hardware-accelerated SAT solver targeting processor/Field
    Programmable Gate Arrays (FPGA) SoCs. Our solution accelerates the most expensive
    subroutine of the Davis-Putnam-Logemann-Loveland (DPLL) algorithm, Boolean Constraint
    Propagation (BCP) through fine-grained FPGA parallelism. Unlike prior state-of-the-art
    solutions, our solver eliminates costly clause lookup operations by assigning
    clauses directly to clause processors on the FPGA and dividing large formulas
    into smaller partitions manageable by FPGA. Partitions are hot-swapped during
    runtime as required and the supported formula size is limited only by available
    external memory, not on-chip FPGA memory.

    We evaluate our solver on a Xilinx Zynq platform with results showing quicker
    execution time across various formula sizes, subject to formula partitioning strategy.
    Compared to prior state-of-theart, we achieve 1.7x and 1.1x speed up on BCP for
    2 representative benchmarks and up to 6x total speedup over software-only implementation.'
  keywords: FPGA, SAT, acceleration, embedded, boolean, satisfiability
  document: '# Accelerating Boolean Constraint Propagation for Efficient SAT-Solving
    on FPGAs


    Hari Govindasamy Carleton University Ottawa, Canada hari@sce.carleton.ca


    Babak Esfandiari Carleton University Ottawa, Canada babak@sce.carleton.ca


    Paulo Garcia Chulalongkorn University Bangkok, Thailand paulo.g@chula.ac.th


    #### ABSTRACT


    We present a hardware-accelerated SAT solver targeting processor/Field Programmable
    Gate Arrays (FPGA) SoCs. Our solution accelerates the most expensive subroutine
    of the Davis-Putnam-Logemann-Loveland (DPLL) algorithm, Boolean Constraint Propagation
    (BCP) through fine-grained FPGA parallelism. Unlike prior state-of-the-art solutions,
    our solver eliminates costly clause lookup operations by assigning clauses directly
    to clause processors on the FPGA and dividing large formulas into smaller partitions
    manageable by FPGA. Partitions are hot-swapped during runtime as required and
    the supported formula size is limited only by available external memory, not on-chip
    FPGA memory.


    We evaluate our solver on a Xilinx Zynq platform with results showing quicker
    execution time across various formula sizes, subject to formula partitioning strategy.
    Compared to prior state-of-theart, we achieve 1.7x and 1.1x speed up on BCP for
    2 representative benchmarks and up to 6x total speedup over software-only implementation.


    ## CCS CONCEPTS


    • Computer systems organization → Robotic autonomy; • Networks → Cyber-physical
    networks; • Applied computing → Industry and manufacturing; • Hardware → Hardware
    accelerators; Application specific processors; • Theory of computation → Equational
    logic and rewriting.


    ### KEYWORDS


    FPGA, SAT, acceleration, embedded, boolean, satisfiability


    #### ACM Reference Format:


    Hari Govindasamy, Babak Esfandiari, and Paulo Garcia. 2024. Accelerating Boolean
    Constraint Propagation for Efficient SAT-Solving on FPGAs. In Great Lakes Symposium
    on VLSI 2024 (GLSVLSI ''24), June 12–14, 2024, Clearwater, FL, USA. ACM, New York,
    NY, USA, [5](#page-4-0) pages. [https://doi.org/10.1145/](https://doi.org/10.1145/3649476.3658808)
    [3649476.3658808](https://doi.org/10.1145/3649476.3658808)


    #### 1 INTRODUCTION


    The Boolean Satisfiability problem (SAT) is a fundamental problem in computer
    science, the first NP-Complete problem [\[7\]](#page-4-1). SAT solvers have become
    the backbone of several engineering domains, as any


    GLSVLSI ''24, June 12–14, 2024, Clearwater, FL, USA


    © 2024 Copyright held by the owner/author(s).


    ACM ISBN 979-8-4007-0605-9/24/06.


    <https://doi.org/10.1145/3649476.3658808>


    NP-Complete problem can be encoded as instance of SAT [\[2,](#page-4-2) [7\]](#page-4-1).
    SAT solvers determine whether a given boolean formula is satisfiable by identifying
    an assignment to the formulas'' free variables that evaluate the formula to true.
    The formula is unsatisfiable otherwise. Most SAT solvers target CNF-SAT, a subset
    of SAT that determines the satisfiability of formulas encoded in Conjunctive Normal
    Form (CNF) . Formulas in CNF are conjunctions of clauses, where each clause is
    a disjunction of one or more literals (a variable or its negation).


    With the advent of modern Systems-on-Chip (SoC) comprised of both hard embedded
    processors and configurable FPGA fabric offering myriad implementation opportunities
    [\[17\]](#page-4-3), deployed from the embedded to the high performance computing
    domain [\[1\]](#page-4-4), accelerating SAT-solving through hardware is an attractive
    approach. We present a novel architecture for hardware-accelerated SAT-solving
    that outperforms state of the art solutions, released in open-source form for
    the Xilinx Zynq platform. Specifically, this article offers the following contributions:


    - We describe a methodology to map and runtime-manage clauses across a processor
    and connected FPGA, making efficient use of FPGA resources and avoiding recurring
    performance pitfalls.

    - We describe the implementation of an open-source prototype system, deployed
    on a Xilinx Zynq chip, identifying how the hardware architecture effects the aforementioned
    strategy.

    - We evaluate our design against the state of the art using two representative
    benchmarks, showing speed-ups of 1.7x and 1.1x, respective, and overall a 6x improvement
    over vanilla software execution.


    Section [2](#page-0-0) describes necessary background knowledge on a particular
    SAT-solving algorithm required to understand the remainder of this paper. Section
    [3](#page-1-0) presents an overview of historical solutions and state of the art,
    directly compared against in this paper. Section [4](#page-2-0) presents our contribution,
    evaluated in Section [5,](#page-3-0) with concluding remarks and suggestions for
    future work described in Section [6.](#page-4-5)


    ### <span id="page-0-0"></span>2 BACKGROUND: DPLL AND BCP


    SAT solvers are categorized into complete and incomplete solvers[\[15\]](#page-4-6).
    Complete solvers evaluate every possible variable assignment, ending on the first
    satisfying assignment or after exhausting the search space. A formula is unsatisfiable
    if the complete solver concludes without finding a satisfying assignment. Most
    incomplete solvers use Stochastic Local Search (SLS) to greedily search for a
    satisfying assignment in the formula''s variable assignment search space[\[16\]](#page-4-7).
    While typically quicker than complete solvers, incomplete solvers do not guarantee
    results as they tend to get stuck in local maxima or skip satisfying assignments.
    Since they don''t explore the solution


    Permission to make digital or hard copies of part or all of this work for personal
    or classroom use is granted without fee provided that copies are not made or distributed
    for profit or commercial advantage and that copies bear this notice and the full
    citation on the first page. Copyrights for third-party components of this work
    must be honored. For all other uses, contact the owner/author(s).


    GLSVLSI ''24, June 12–14, 2024, Clearwater, FL, USA Govindasamy, Esfandiari, and
    Garcia


    <span id="page-1-1"></span>![](_page_1_Figure_2.jpeg)


    Figure 1: Interface between processor and FPGA-based BCP Coprocessor. DPLL''s
    BCP is accelerated through fine-grained parallelization across Clause Processors.


    space exhaustively they can never conclude that a formula is unsatisfiable. Davis-Putnam-Logemann-Loveland
    (DPLL) and DPLL-based algorithms are the most predominant complete solvers [\[16\]](#page-4-7).
    DPLL performs two primary operations: 1) decision and 2) Boolean Constraint Propagation
    (BCP). DPLL-based algorithms follow DPLL''s core structure, and propose improved
    decision heuristics, learning and BCP mechanisms. During decision, DPLL heuristically
    picks and assigns truth values to free variables. BCP subsequently propagates
    the effect of the decision using the unit implication rule[\[5\]](#page-4-8).
    The unit implication rule identifies unit clauses where all but one of its literals
    are false. Unit clauses can only be satisfied by assigning the variable to true
    if the literal is positive or to false on negative literals. The resulting assignment
    is known as an implication. BCP repeatedly applies this rule until all clauses
    are satisfied (formula is therefore satisfiable) or at least one clause evaluates
    false (conflict). On conflicts, DPLL backtracks by retracting and/or inverting
    assignments from earlier decisions. BCP is expensive, accounting for 80-90% of
    DPLL''s CPU time, rendering it a prime candidate for hardware acceleration [\[5,](#page-4-8)
    [18\]](#page-4-9). BCP coprocessors accelerate DPLL by implementing specialized
    BCP processing engines on FPGA. These run alongside a General Purpose Processor
    (GPP) that performs the remaining DPLL operations: decision heuristics and backtracking.
    Using this architecture, the BCP coprocessor is first configured with the clauses,
    and then waits to evaluate decisions from the GPP. Any DPLL-based software solver
    can integrate with a BCP-coprocessor by replacing software BCP with the hardware
    accelerated BCP-coprocessor [\[18,](#page-4-9) [19\]](#page-4-10). FPGA-based
    BCP coprocessors are either instance-specific or application-specific. Instancespecific
    solver are built to solve a single SAT instance and designed by translating an
    input formula into its equivalent logical circuit. However, to solve new instances,
    the FPGA requires a complete rebuild (synthesis and FPGA programming may take
    up to several hours). Although these solvers can be significantly quicker than
    their software counterparts, their performance becomes notably slower when build
    times are included. For instance, Ivan et al''s best result against the hole7
    benchmark achieves a 6.66x speedup against MiniSAT[\[11,](#page-4-11) [12\]](#page-4-12);
    however, when build times are included, compilation alone takes 50 seconds, whereas
    MiniSAT finishes in under 0.064 seconds [\[10\]](#page-4-13). Application-specific
    solvers eliminate the need to rebuild the FPGA by instantiating general-purpose
    processing units capable of tackling any SAT instance (given that it fits in hardware).
    The BCP coprocessor is configured with the target problem by simply overwriting
    FPGA memory.


    #### <span id="page-1-0"></span>3 STATE OF THE ART


    Algorithmic techniques for efficient SAT solving have been extensively researched,
    and the literature contains several surveys that describe the history and state
    of the art of the problem ([\[8\]](#page-4-14), [\[13\]](#page-4-15)). Techniques
    aimed at accelerating the execution of a particular SAT solving algorithm include
    software parallelization [\[9\]](#page-4-16), deployment on specialized GPUs [\[14\]](#page-4-17),
    and even acceleration through machinelearning approaches [\[20\]](#page-4-18).


    Our approach sits within FPGA-based acceleration, which began roughly 3 decades
    ago [\[6\]](#page-4-19), with a few prominent results at the turn of the century
    ([\[21\]](#page-4-20), [\[3\]](#page-4-21)). However, it was not until significant
    advances in FPGA performance occurred in the last decade, and the rise of SoC
    platforms combining FPGA fabric with hard processors, that FPGA-based SAT acceleration
    matured. The most notable architectures were proposed by Davis et al [\[5\]](#page-4-8)
    and Thong et al [\[18,](#page-4-9) [19\]](#page-4-10): both exploring the use
    of FPGA to implement BCP coprocessors, keeping the remainder of DPLL in software.


    Davis et al calculate implications in parallel by using several inference engines
    (IE), each assigned a list of clauses (partitions) [\[5\]](#page-4-8). For every
    decision/implication, the clause containing the assignment variable is first retrieved
    before calculating implications. Implications are forwarded to a conflict detector
    that ensures that two or more IEs have not implied opposing values for the same
    variable. Implications are then sent to the processor and queued up for propagation.


    To keep clause retrieval time low, a variable only occurs once in each IEs partition
    (i.e clauses within the same IE share no common variables). This limits the number
    of clauses affected by a decision to one, thereby also limiting implications per
    IE to one, constraining the effected performance. While some strategies to increase
    this limit have been proposed [\[4\]](#page-4-22), they remain unexplored.


    Thong et al. propose a concurrent BCP coprocessor comprising multiple sequential
    processing engines (PE) [\[19\]](#page-4-10). Identifying that Davis et al.''s
    clause lookup is slower than direct access [\[18\]](#page-4-9), they develop a
    clause storage and encoding scheme that efficiently links clauses with shared
    variables. The processor sends decisions to the FPGA and starts BCP execution
    at a single PE. Using the linked list, the PE traverses every clause containing
    the decision variable and calculates implications, which are then added to a local
    queue and propagated. The running PE triggers BCP execution in another PE when
    it arrives at a link to a clause that is located elsewhere. The coprocessor supports
    multithreaded software execution, hiding communication and software latency by
    keeping the coprocessor busy while software threads make decisions when possible.


    Davis et al. and Thong et al. have laid a strong foundation in developing application-specific
    FPGA-based BCP coprocessors; we extend their work and propose a solution that
    processes clauses in parallel without the need for clause lookup.


    Accelerating Boolean Constraint Propagation for Efficient SAT-Solving on FPGAs
    GLSVLSI ''24, June 12–14, 2024, Clearwater, FL, USA


    #### <span id="page-2-0"></span>4 THE SAT SOLVER ARCHITECTURE


    We present a BCP coprocessor that works alongside vanilla DPLL (and should, in
    theory, work seamlessly with any DPLL-based solver). Like Thong et al., we forgo
    clause lookup and allow clauses to share variables within the same partition.
    However, we still achieve Davis et al.''s high degree of parallelism by placing
    clauses directly in clause processors (explained in Section [4.1\)](#page-2-1).


    SAT instances larger than the available number of Clause Processors (CPs) are
    partitioned, stored in external memory (i.e., software) and hot-swapped into the
    BCP coprocessors as required during runtime. Solvable instance size is limited
    only by the GPP''s RAM, not on-chip FPGA memory. We deploy our solution on the
    Zynq chip, and available here[1](#page-2-2) for use. To our knowledge, this is
    the first open-source hardware-accelerated SAT solver.


    #### <span id="page-2-1"></span>4.1 The BCP accelerator architecture


    Figure [1](#page-1-1) illustrates our approach, comprising a GPP and an FPGA accelerated
    BCP coprocessor. The GPP executes DPLL''s remaining elements (decisions, backtrack,
    etc.), partitions large SAT instances (explained in Section [4.2\)](#page-2-3)
    and swaps partitions into hardware as required. Its default state is idle, awaiting
    instructions to execute. Once a decision is received, the systems loops until
    all unit clauses are exhausted. The BCP coprocessor, depicted in Figure [1,](#page-1-1)
    comprises a control unit (1), an array of clause processors (2) and an implication
    selector (3). The central control unit communicates directly with the GPP and
    each CP. Based on the received GPP command, it loads clauses into CPs, broadcasts
    decisions, or clears assignments during backtrack. At its core, the BCP coprocessor
    consists of an array of CPs that calculate decision and implication results in
    parallel. CPs store clauses as an array of literals maintain a local copy of each
    literal''s respective variable assignment. Partitions are hot-swapped into FPGA
    by overwriting a CPs array of literals with the literals of the new clause. Variable
    assignments are updated during decisions and BCP, and cleared during backtrack.
    Finally, the implication selector chooses a single implication to propagate when
    multiple implications arise as a result of BCP. Rather than using an explicit
    implication conflict detector, as done by Davis et al [\[5\]](#page-4-8), we propagate
    the chosen implication, and identify conflicts during evaluation.


    #### <span id="page-2-3"></span>4.2 Formulae partitioning


    SAT instances contain an arbitrary number of variables and clauses. The problem
    size solvable on FPGA is limited by its available Configurable Logic Block (CLB)
    and memory, and requires large problems be partitioned into smaller manageable
    sizes. Partitions are stored in the GPP, and swapped into FPGA during run time
    by overwriting CPs clauses. BCP is performed individually on each partition, and
    implications are relayed back to the GPP. Implications are subsequently propagated
    to other partitions. We aim to make partitions as large as possible, limited by
    the coprocessor''s clause and variable threshold. Consider Equation [1,](#page-2-4)
    composed of four clauses, and an instance of our coprocessor that supports two
    clauses and three variables. Equation [2](#page-2-5) and [3](#page-2-6) outline
    the two possible ways to partition Equation [1.](#page-2-4) Equation [2](#page-2-5)
    describes a scenario where the partitions


    <span id="page-2-7"></span>![](_page_2_Figure_10.jpeg)


    Figure 2: (a) Davis et al. store the formula directly on FPGA. Clauses within
    partitions contain no shared variables, and partitions are mapped directly to
    Implication Engines. (b) Thong et al. store formula directly on FPGA. Clauses
    are linked to other clauses with shared variables and are processed sequentially.
    (c) Formula stored in external memory ("software" view). Clauses in partitions
    mapped directly to Clause Processors, and hot-swapped as required.


    reach the clause limit, while the Equation [3](#page-2-6) reaches the variable
    limit.


    <span id="page-2-4"></span>

    $$f = (\neg a \lor b \lor \neg c) \land (a \lor \neg b \lor \neg c) \land (\neg
    d \lor e \lor f) \land (d \lor e \lor f) \tag{1}$$


    <span id="page-2-5"></span>

    $$\begin{aligned} \{ \begin{aligned} \{ \begin{aligned} \{ \begin{array}{l} (\neg
    a \lor b \lor \neg c) \land (a \lor \neg b \lor \neg c) \end{array} \} \end{aligned}
    \} \end{aligned} \} $$


    <span id="page-2-6"></span>

    $$

    \begin{aligned}

    \{\neg variation\\_2 &= \{\{ (\neg a \lor b \lor \neg c) \}, \{ (a \lor \neg b
    \lor \neg c) \} \}, \\

    \{ (\neg d \lor e \lor f) \}, &\{ (d \lor e \lor f) \} \end{aligned}

    \begin{aligned}

    \{ \neg variation\\_2 \}, \end{aligned}

    $$


    Results (refer to Section [5\)](#page-3-0) indicate that partitioning is a bottleneck
    in our approach. Performance improvement is dictated by the amount of required
    partition swapping and the number of unused CPs (occurs when the number of clauses
    in a partition is less than the available number of CPs). Thus, performance improvement
    is observed with certain partition assignments, while others lead to performance
    degradation. System performance can be improved by developing a more effective
    partitioning algorithm, but beyond the scope of this paper and reserved for future
    work.


    #### 4.3 Execution


    Each clause processor is only associated with a single clause; thus, no clause
    look-up or traversal is required to retrieve the affected clause for processing.
    All clauses on the FPGA are processed in parallel as soon as a decision is received.
    Consider Equation [1''](#page-2-4)s


    <span id="page-2-2"></span><sup>1</sup>[https://github.com/harigovind1998/FPGA\\_BCP\\_acceleration](https://github.com/harigovind1998/FPGA_BCP_acceleration)


    GLSVLSI ''24, June 12–14, 2024, Clearwater, FL, USA Govindasamy, Esfandiari, and
    Garcia


    <span id="page-3-1"></span>


    | Step                 |      | 0    | 1                              | 2                       |
    3                   | 4    |

    |----------------------|------|------|--------------------------------|-------------------------|---------------------|------|

    | Our<br>Approach      | CP   | Rx   | Process<br>Decision  Clause 1  | Done                    |                     |      |

    |                      | CP 2 | Rx   | Process<br>Decision  Clause 2  | Done                    |                     |      |

    | Davis et<br>al.      | IE 1 | Rx   | Retrieve<br>Decision Clause 1  | Process<br>Clause
    1     | Done                |      |

    |                      | IE 2 | Rx   | Retrieve<br>Decision  Clause 2 | Process<br>Clause
    2     | Done                |      |

    | Thong et PE 1<br>al. |      | Rx   | Process<br>Decision  Clause 1  | Traverse
    to<br>Clause 2 | Process<br>Clause 2 | Done |

    |                      | PE 2 | ldle | ldle                           | ldle                    |
    ldle                | ldle |


    Figure 3: Execution steps of each described approach.


    mapping of partitions to hardware as presented in Figure [2.](#page-2-7) Figure
    [3](#page-3-1) summarizes the execution stages for Davis et al.''s, Thong et al.''s
    and our approach for the theoretical execution for a decision of variable . In
    our approach, clauses 1 and 2 are processed by Clause Processor 1 and 2 in parallel
    once the decision is received. Since clauses 3 and 4 do not contain variable ,
    Partition 2 remains in external memory and is not processed. Though Davis et al.
    also process clause 1 and 2 in parallel, each Implication Engine first performs
    a clause look-up to retrieve the affected clause. Results of the decision on the
    affected clause are then calculated. Thong et al.''s approach starts BCP on Processing
    Engine 1. After clause 1 is processed Processing Engine 1 traverses to clause
    2. In the manner, clauses in a partition are processed sequentially. Execution
    concludes after computing Partitions 1''s final element, clause 2. Processing
    Engine 2 remains idle for the entire duration as clauses in partition 2 do not
    contain variable .


    #### 4.4 Processor-FPGA interface


    The BCP coprocessor implements the Advanced eXtensible Interface 4-Lite (AXI-Lite)
    IP interface, acting as a subordinate to a processor (AXI master). Using AXI,
    the processor writes directly to the coprocessor''s registers to send instructions
    and data, and continues polling for status updates and new implication until the
    coprocessor completes.


    Status changes dictate DPLL''s flow, either allowing the search to continue assigning
    additional variables, or triggers backtracking on conflicts. A copy of all the
    implications are saved on the processor to avoid re-assigning implied variables,
    and further propagated to the remaining partitions.


    #### <span id="page-3-0"></span>5 EXPERIMENTS AND RESULTS


    On a Xilinx Zynq chip with total capacity of 14400 LUTs and 28800 FF, our solution
    supports 224 parallel Clause Processors and 63 variables. We achieve a clock frequency
    of 106.66 MHz, utilizing 647 LUTRAM of on-chip memory, 13151 LUTs, and 11059 FFs.


    Related work calculates throughput (in BPCs performed per second), assuming full
    data availability: i.e., not taking into account software execution and communication/data
    transfer latency. Whilst this is a useful metric to assess hardware performance
    in isolation (and we report equivalent results in Table [1\)](#page-3-2), it does
    not accurately depict system performance; to do so, we break down


    <span id="page-3-2"></span>


    |               | Millions of BCP/s |                     |            |  |  |

    |---------------|-------------------|---------------------|------------|--|--|

    | SAT Instance  | Davis et al [5]   | Thong et al<br>[19] | Our Design |  |  |

    | bmc-galileo-8 | 40                | 102                 | 175        |  |  |

    | bmc-ibm-12    | 33                | 150                 | 169        |  |  |


    Table 1: Comparison of BCP engine throughput (BCPs/s) with related work. Results
    reflect maximum theoretical throughput, achieved only data is fully available
    to BCP engines.


    <span id="page-3-3"></span>![](_page_3_Figure_13.jpeg)


    Figure 4: Breakdown of the total execution time across constituent components.


    <span id="page-3-4"></span>


    |         |       | Variables  |           |           |           |  |  |  |

    |---------|-------|------------|-----------|-----------|-----------|--|--|--|

    |         |       | 63         | 126       | 252       | 630       |  |  |  |

    | Clauses | 224   | 362M BCP/s | 17K BCP/s |           | NA        |  |  |  |

    |         |       | 2.2x       | 0.17x     | NA        |           |  |  |  |

    |         | 448   | 702K BCP/s | 21K BCP/s | 13K BCP/s | NA        |  |  |  |

    |         |       | 1.6x       | 0.21x     | 0.08x     |           |  |  |  |

    |         | 2240  | 441K BCP/s | 22K BCP/s | 16K BCP/s | 12K BCP/s |  |  |  |

    |         |       | 1.91x      | 1.26x     | 0.61x     | 0.10x     |  |  |  |

    |         | 22400 | 313K BCP/s | 20K BCP/s | 16K BCP/s | 14K BCP/s |  |  |  |

    |         |       | 6.32x      | 5.04x     | 4.86x     | 3.31x     |  |  |  |


    Table 2: Varied clause/variable sizes and their impact on the relative speedup
    of hardware/software and the effective throughput of BCP engines.


    the full execution in Figure [4](#page-3-3) and evaluate speedup over vanilla
    software implementation, evaluating combinations of clause and variable sizes,
    with speedup depicted in Table [2](#page-3-4) for meaningful combinations. For
    each combination, we also depict real throughput, in the form of BCPs/s averaged
    over total execution time (63 variables and 224 clauses is the theoretical upper
    bound, without the need for hot swapping). To evaluate the different effects of
    clause/variable sizes on execution, we fix one and vary the other, measuring total
    execution time: results are depicted in Figures [5](#page-4-23) and [6.](#page-4-24)


    <span id="page-4-0"></span>Accelerating Boolean Constraint Propagation for Efficient
    SAT-Solving on FPGAs GLSVLSI ''24, June 12–14, 2024, Clearwater, FL, USA


    <span id="page-4-23"></span>![](_page_4_Figure_1.jpeg)


    ![](_page_4_Figure_2.jpeg)


    <span id="page-4-24"></span>![](_page_4_Figure_3.jpeg)


    Figure 6: Effect of increasing variables size on total execution time, for 22400
    clauses.


    #### <span id="page-4-5"></span>6 CONCLUSIONS


    We described a SAT-solver hardware-accelerated architecture that outperforms state
    of the art by hot-swapping clause assignment at runtime, making efficient use
    of FPGA resources. Our solution prototype, on a Xilinx Zynq chip, is available
    in open-source. Practitioners may use the presented solution in their designs,
    whenever a problem is encoded in SAT form and performance is critical.


    An important open question remains: our performance is constrained by how clauses
    are partitioned. A partitioning scheme that minimizes the distribution of variables
    among clauses will minimize runtime swapping, resulting in improved execution.
    However, how to best partition a formula to achieve this is not yet known. Future
    work must formulate this challenge as an optimization problem, and methods for
    its efficient solution must be devised. Once that is achieved, they can be applied
    (offline) prior to deployment on our architecture.


    #### ACKNOWLEDGMENTS


    We acknowledge the support of the Natural Sciences and Engineering Research Council
    of Canada (NSERC).


    #### REFERENCES


    - <span id="page-4-4"></span>[1] Rabie Ben Atitallah and Karim MA Ali. 2017. FPGA-Centric
    High Performance Embedded Computing: Challenges and Trends. In 2017 Euromicro
    Conference on Digital System Design (DSD). IEEE, 390–395.

    - <span id="page-4-2"></span>[2] Stephen A Cook. 2023. The complexity of theorem-proving
    procedures. In Logic, Automata, and Computational Complexity: The Works of Stephen
    A. Cook. 143–152.

    - <span id="page-4-21"></span>[3] Andreas Dandalis and Viktor K Prasanna. 2002.
    Run-time performance optimization of an FPGA-based deduction engine for SAT solvers.
    ACM Transactions on Design Automation of Electronic Systems (TODAES) 7, 4 (2002),
    547–562.

    - <span id="page-4-22"></span>[4] John D Davis, Zhangxi Tan, Fang Yu, and Lintao
    Zhang. 2008. Designing an efficient hardware implication accelerator for SAT solving.
    In International Conference on Theory and Applications of Satisfiability Testing.
    Springer, 48–62.

    - <span id="page-4-8"></span>[5] John D. Davis, Zhangxi Tan, Fang Yu, and Lintao
    Zhang. 2008. A practical reconfigurable hardware accelerator for boolean satisfiability
    solvers. In 2008 45th ACM/IEEE Design Automation Conference. 780–785. [https://doi.org/10.1145/](https://doi.org/10.1145/1391469.1391669)
    [1391469.1391669](https://doi.org/10.1145/1391469.1391669)

    - <span id="page-4-19"></span>[6] Amir H Farrahi and Majid Sarrafzadeh. 1994.
    FPGA technology mapping for power minimization. In International Workshop on Field
    Programmable Logic and Applications. Springer, 66–77.

    - <span id="page-4-1"></span>[7] Michael R. Garey and David S. Johnson. 1990.
    Computers and Intractability; A Guide to the Theory of NP-Completeness. W. H.
    Freeman & Co., USA.

    - <span id="page-4-14"></span>[8] Weiwei Gong and Xu Zhou. 2017. A survey of SAT
    solver. In AIP Conference Proceedings, Vol. 1836. AIP Publishing.

    - <span id="page-4-16"></span>[9] Youssef Hamadi, Said Jabbour, and Lakhdar Sais.
    2010. ManySAT: a parallel SAT solver. Journal on Satisfiability, Boolean Modeling
    and Computation 6, 4 (2010), 245–262.

    - <span id="page-4-13"></span>[10] Anping He, Lvying Yu, Haitao Zhang, Lian Li,
    and Jinzhao Wu. 2018. A FPGA Based SAT Solver with High Random and Concurrent
    Strategies. In 2018 IEEE International Conference on Software Quality, Reliability
    and Security Companion (QRS-C). 221–228.<https://doi.org/10.1109/QRS-C.2018.00049>

    - <span id="page-4-11"></span>[11] Teodor Ivan and El Mostapha Aboulhamid. 2013.
    An Efficient Hardware Implementation of a SAT Problem Solver on FPGA. In 2013
    Euromicro Conference on Digital System Design. 209–216.<https://doi.org/10.1109/DSD.2013.31>

    - <span id="page-4-12"></span>[12] Teodor Ivan and El Mostapha Aboulhamid. 2013.
    Exploring limits of parallelism in FPGA-based Boolean satisfiability. In 2013
    2nd Mediterranean Conference on Embedded Computing (MECO). 62–65.<https://doi.org/10.1109/MECO.2013.6601319>

    - <span id="page-4-15"></span>[13] Ruben Martins, Vasco Manquinho, and Inês Lynce.
    2012. An overview of parallel SAT solving. Constraints 17 (2012), 304–347.

    - <span id="page-4-17"></span>[14] Muhammad Osama, Anton Wijs, and Armin Biere.
    2021. SAT solving with GPU accelerated inprocessing. In International Conference
    on Tools and Algorithms for the Construction and Analysis of Systems. Springer,
    133–151.

    - <span id="page-4-6"></span>[15] I. Skliarova and A.B. Ferrari. 2004. A software/reconfigurable
    hardware SAT solver. IEEE Transactions on Very Large Scale Integration (VLSI)
    Systems 12, 4 (2004), 408–419.<https://doi.org/10.1109/TVLSI.2004.825859>

    - <span id="page-4-7"></span>[16] Ali Asgar Sohanghpurwala, Mohamed W. Hassan,
    and Peter Athanas. 2017. Hardware accelerated SAT solvers: A survey. J. Parallel
    and Distrib. Comput. 106 (2017), 170–184.<https://doi.org/10.1016/j.jpdc.2016.12.014>

    - <span id="page-4-3"></span>[17] Robert Stewart, Bernard Berthomieu, Paulo Garcia,
    Idris Ibrahim, Greg Michaelson, and Andrew Wallace. 2019. Verifying parallel dataflow
    transformations with model checking and its application to FPGAs. Journal of Systems
    Architecture 101 (2019), 101657.

    - <span id="page-4-9"></span>[18] Jason Thong and Nicola Nicolici. 2013. FPGA
    acceleration of enhanced boolean constraint propagation for SAT solvers. In 2013
    IEEE/ACM International Conference on Computer-Aided Design (ICCAD). 234–241. [https://doi.org/10.1109/](https://doi.org/10.1109/ICCAD.2013.6691124)
    [ICCAD.2013.6691124](https://doi.org/10.1109/ICCAD.2013.6691124)

    - <span id="page-4-10"></span>[19] Jason Thong and Nicola Nicolici. 2015. SAT
    solving using FPGA-based heterogeneous computing. In 2015 IEEE/ACM International
    Conference on Computer-Aided Design (ICCAD). 232–239.<https://doi.org/10.1109/ICCAD.2015.7372575>

    - <span id="page-4-18"></span>[20] Haoze Wu. 2017. Improving SAT-solving with
    machine learning. In Proceedings of the 2017 ACM SIGCSE Technical Symposium on
    Computer Science Education. 787–788.

    - <span id="page-4-20"></span>[21] Peixin Zhong, Margaret Martonosi, and Pranav
    Ashar. 2000. FPGA-based SAT solver architecture with near-zero synthesis and layout
    overhead. IEE Proceedings-Computers and Digital Techniques 147, 3 (2000), 135–141.'
  decisions:
    evaluation_prompt: 'Qualified. Reason: The paper includes a dedicated section
      titled "Experiments and Results," which provides structured evaluation through
      benchmarks, metrics, and performance measurements. It presents tables and figures
      with quantifiable outcomes, such as speedup comparisons and throughput measurements,
      demonstrating empirical analysis of the proposed method.'
    related_work_prompt: 'Qualified. Reason: The paper meaningfully engages with prior
      research throughout its content. It includes a detailed discussion of previous
      work in the "State of the Art" section, comparing the proposed method with existing
      solutions by Davis et al. and Thong et al. Additionally, the paper provides
      citations and context in the introduction and background sections, demonstrating
      an understanding of the existing literature and how the new approach builds
      upon it.'
    novelty_prompt: 'Qualified. Reason: The paper proposes a novel architecture for
      hardware-accelerated SAT-solving that eliminates costly clause lookup operations
      by assigning clauses directly to clause processors on the FPGA and dividing
      large formulas into smaller partitions. It claims to outperform state-of-the-art
      solutions and is released in open-source form for the Xilinx Zynq platform.
      The paper makes clear claims of contribution, such as "Unlike prior state-of-the-art
      solutions, our solver eliminates costly clause lookup operations" and "To the
      best of our knowledge, this is the first open-source hardware-accelerated SAT
      solver.'
    review_only_prompt: 'Qualified. Reason: The paper presents a novel architecture
      for hardware-accelerated SAT-solving on FPGAs, introduces a new methodology
      for mapping and managing clauses, and provides original experimental results
      demonstrating performance improvements over existing solutions.'
- title: '**HOPE: Holistic STT-RAM Architecture Exploration Framework for Future Cross-Platform
    Analysis**'
  abstract: ''
  keywords: '* Non-volatile memory, STT-RAM, Power Estimation, gem5, Emerging Technologies'
  document: "Date of publication xxxx 00, 0000, date of current version xxxx 00, 0000.\
    \ *Digital Object Identifier 10.1109/ACCESS.XXXX.DOI*\n\n# **HOPE: Holistic STT-RAM\
    \ Architecture Exploration Framework for Future Cross-Platform Analysis**\n\n\
    **SAEED SEYEDFARAJI (Graduate Student Member, IEEE), MARKUS BICHL, ASAD AFTAB\
    \ (Graduate Student Member, IEEE),and SEMEEN REHMAN(Member, IEEE).<sup>1</sup>\
    \ ,**\n\n<sup>1</sup>Faculty of Electrical Engineering and Information Technology,\
    \ Vienna University of Technology (TU-Wien), 1040 Vienna, Austria Corresponding\
    \ author: Saeed Seyedfaraji (e-mail: saeed.seyedfaraji@tuwien.ac.at).\n\n**ABSTRACT**\
    \ Spin Transfer Torque Random Access Memory (STT-RAM) is an emerging Non-Volatile\
    \ Memory (NVM) technology that has garnered attention to overcome the drawbacks\
    \ of conventional CMOS-based technologies. However, such technologies must be\
    \ evaluated before deployment under real workloads and architecture. But there\
    \ is a lack of available open-source STT-RAM-based system evaluation framework,\
    \ which hampers research and experimentation and impacts the adoption of STT-RAM\
    \ in a system. This paper proposes a novel, extendable STT-RAM memory controller\
    \ design integrated inside the gem5 simulator. Our framework enables understanding\
    \ various aspects of STT-RAM, i.e., power, delay, clock cycles, energy, and system\
    \ throughput. We will open-source our HOPE framework, which will fuel research\
    \ and aid in accelerating the development of future system architectures based\
    \ on STT-RAM. It will also facilitate the user for further tool enhancement.\n\
    \n**INDEX TERMS** Non-volatile memory, STT-RAM, Power Estimation, gem5, Emerging\
    \ Technologies\n\n## **I. INTRODUCTION**\n\nSTT-RAM boastsseveral compelling featuresincluding\
    \ nonvolatility, high density, soft error reliability, CMOS compatibility, high\
    \ endurance, and scalability [1]–[4]. According to the International Roadmap for\
    \ Devices and Systems (IRDS) [5], STT-RAM emerges as the most promising memory\
    \ option to replace conventional memory technologies. Table 1 presents a comprehensive\
    \ comparison of various design elements related to memory technologies, such as\
    \ endurance, associated read/write energy, latency, and compatibility with CMOS\
    \ technology size. It is worth noting that the listed suppliers are not exclusive\
    \ options, as alternative providers may also offer each type of memory technology.\
    \ Considering the decreasing technology node size over time due to Denard's law,\
    \ it is important to note that the suggested compatibility in the table is based\
    \ on the findings from the literature review of [1]–[4], [6]–[18]. Therefore,\
    \ it is plausible that each memory technology could potentially be fabricated\
    \ with other CMOS sizes.\n\nComparatively, the write and read energy of Dynamic\
    \ Random Access Memory (DRAM) Static Random Access Memory (SRAM) per operation\
    \ is lower than that of NVMs. Nevertheless, both DRAM and SRAM, as volatile memory\
    \ types, rely on a continuous power supply to retain stored\n\ndata. DRAM, commonly\
    \ employed as the main memory in computers, consumes more energy due to its constant\
    \ need for refreshing to preserve data integrity. Similarly, SRAM, another volatile\
    \ memory technology, also requires a consistent power supply for data retention.\
    \ On the other hand, NVMs like Flash memory can maintain data even when power\
    \ is off, resulting in lower energy consumption compared to DRAM and SRAM. Therefore,\
    \ NVM became an attractive alternative for main memories because of lower energy\
    \ consumption. Phase Change Memory (PCM), and STT-RAM [19] are some of the common\
    \ NVMs proposed to replace DRAM as main memory. They have also been explored for\
    \ building larger on-chip caches because of their high density. Amongst all, STT-RAM\
    \ has matured a lot in terms of its on-chip computation, and high energy efficiency\
    \ [20].\n\nHowever, the utilization of STT-RAM in widespread industrial applications\
    \ is hindered by several limitations. Notably, challenges such as write operation\
    \ delays and high write energy consumption. To overcome these issues, different\
    \ approaches have been explored at various levels of abstraction, including circuit-level\
    \ approaches [21]–[25], architecture-level approaches [10], [26], [27], and methods\
    \ proposed from an application perspective [28], [29].\n\n![](_page_1_Picture_0.jpeg)\n\
    \n|                         | STT-RAM          | PCMRAM                  | RRAM\
    \              | Fe-FET               | FLASH           | SRAM            | DRAM\
    \              |\n|-------------------------|------------------|-------------------------|-------------------|----------------------|-----------------|-----------------|-------------------|\n\
    | Non-volatility          | +                | +                       | +   \
    \              | +                    | +               | -               | -\
    \                 |\n| Data Retention (years)  | 10               | 10       \
    \               | 10                | 10                   | 10              |\
    \ -               | -                 |\n| Cell Endurance (cycles) | 1016    \
    \         | 1012                    | 1010              | 1012               \
    \  | 106             | 1016            | 1015              |\n| Cell Size (F 2)\
    \         | 6-20             | 4-8                     | 4                 | 4-8\
    \                  | 4-6             | >100            | 4-12              |\n\
    | Technology node (nm)    | 45               | 65                      | 40  \
    \              | 5                    | 15              | 10              | 32\
    \                |\n| Read Latency (ns)       | 2-20             | 20-50     \
    \              | <10               | 10                   | 25x103          |\
    \ <5              | 2                 |\n| Write Latency (ns)      | 5-20    \
    \         | 30                      | 5                 | 10                 \
    \  | 500x103         | <5              | 5                 |\n| Erase Latency\
    \ (ns)      | 5-20             | 30                      | 10                |\
    \ 10                   | 2 (ms)          | <5              | 5               \
    \  |\n| Write Energy (pJ)       | 0.1-2.5          | 18                      |\
    \ 0.1               | 1                    | 0.1 - 1         | <0.1          \
    \  | <0.1              |\n| Erase Energy (pJ)       | 1                | 18  \
    \                    | 0.1               | 1                    | 1000       \
    \     | <1              | <1                |\n| Suppliers               | Toshiba,\
    \ Hitachi | Samsung, Intel, WD, IBM | Panasonic, Micron | Globalfoundries, FMC\
    \ | Micron, Samsung | Qualcomm, Intel | Samsung, SK Hynix |\n\nTABLE 1: Comprehensive\
    \ comparison of NVM technologies [1]–[4], [6]–[18]\n\n# *A. NEED FOR STT-RAM BASED\
    \ SYSTEM EVALUATION FRAMEWORKS*\n\nResearchers have made significant contributions\
    \ to enhancing comparison metrics within their respective levels of abstraction\
    \ [1], [2], [4], [6]–[11], [13], [14], [24], [25]. However, it is crucial to note\
    \ that the current architectural perspective findings are derived from a behavioral\
    \ model of the circuit, which may not offer precise and detailed outcomes comparable\
    \ to those from a real computing system. This approach falls short in addressing\
    \ the need for a comprehensive system exploration framework. Hence, the associated\
    \ research challenge is *how to design a holistic system evaluation framework\
    \ that can be used to evaluate the impact of incorporating STT-RAM memories in\
    \ current systems, while accurately modeling the scaling, energy consumption and\
    \ performance characteristics of these devices and enabling architectural design\
    \ space exploration.*\n\nA number of simulation environments are available for\
    \ research and development of system-level exploration of computer architectures,\
    \ such as gem5 and ZSIM [30]. However, gem5 is widely used due to its ability\
    \ to emulate the full-system mode and help in the exploration of systemlevel metrics,\
    \ with different instruction set architectures (ISAs) such as Alpha, ARM, SPARC,\
    \ MIPS, RISC-V, and x86 ISAs), and various timing and CPU modes [31], [32]. ZSIM,\
    \ as an alternative simulation software, does not offer full-system simulation\
    \ capabilities, but also does not rely on event-driven execution and is therefore\
    \ faster. As this work targets the integration of STT-RAM into a complete system,\
    \ also showing capabilities of executing an operating system on top of STT-RAM,\
    \ gem5 is the selected choice. gem5 showed fast enough simulation speed for benchmark\
    \ applications on top of an operating system.\n\n# *B. ANALYZING STT-RAM IMPACT\
    \ ON DIFFERENT APPLICATIONS*\n\nIn order to verify the framework design along\
    \ with its advantages, a case study i.e., investigating STT-RAM from the perspective\
    \ of reducing the energy consumption of High Performance Computing (HPC) applications,\
    \ and its characteristics i.e., power, area, latency, etc., is carried out. Until\
    \ now an ideal platform for system-level evaluation is the gem5 simulator, as\
    \ it provides methods for generating system environments with easily exchangeable\
    \ separated components such as the memory controller, DRAM inter-\n\n## face,\
    \ and NVM interface.\n\n# *C. NOVEL CONTRIBUTION*\n\nIn order to meet the requirement\
    \ of designing a holistic system exploration framework, this paper introduces\
    \ an innovative memory interface utilizing STT-RAM, making it a notable contribution.\
    \ The interface has been created and seamlessly integrated into the gem5 simulator,\
    \ establishing a connection with the included memory controller.\n\n*The novel\
    \ contributions of this paper are:*\n\n- *•* We propose HOPE which is an STT-RAM\
    \ modeling and simulation framework integrated into the full system simulator\
    \ gem5.\n- *•* We leverage the recently implemented NVM interface in gem5 to integrate\
    \ HOPE with existing gem5 memory interfaces. This is in contrast to prior approaches\
    \ that rely on external patches (like NVMain), which become less maintainable\
    \ over time, thus stymying further development. Our proposed framework introduces\
    \ a third memory interface tailored specifically for STT-RAM. This extension offers\
    \ highly detailed results comparable to the existing DRAM implementation within\
    \ gem5. Fortunately, integrating our framework into gem5 requires only minimal\
    \ changes to gem5 files, as all functionality is implemented in new files that\
    \ can be added seamlessly. Our implementation can be used identically to the existing\
    \ memory interfaces and can potentially be integrated into the official gem5 repository\
    \ by its core maintainers. Such integration would be the ideal outcome for our\
    \ work, ensuring ongoing compatibility with gem5.\n- *•* We also extend the power\
    \ model in gem5 DRAM-Power to support our proposed STT-RAM model.\n- *•* We evaluate\
    \ HOPE using HPC applications from the SPEC CPU 2017 benchmark suite on our event-driven\
    \ gem5 simulator and successfully extract evaluation metrics from both the application\
    \ and circuit perspectives.\n- *•* We will also open-source our framework to enable\
    \ and accelerate the development of future system architectures based on STT-RAM.\n\
    \nThe rest of the paper is organized as follows: Section II presents the various\
    \ state-of-the-art works w.r.t. different types of memory controllers implemented\
    \ inside gem5 and their drawbacks. In section III, the STT-RAM integration\n\n\
    with the memory controller inside gem5 is discussed in detail. Section IV provides\
    \ the evaluation metrics, results, and comparison. Finally, we conclude this paper\
    \ in section V.\n\n#### **II. BACKGROUND AND RELATED WORKS**\n\nSTT-RAM has been\
    \ recently exploited as an alternative to conventional on-chip memories because\
    \ of its low energy consumption, high-speed access rate, scalability, and boundless\
    \ endurance. However, several fundamental barriers, i.e., reliability issues due\
    \ to Read/Write failure, Process Variation (PV) effects leading to stochastic\
    \ switching time, should be considered before its vast industrial adaptation.\n\
    \n# *A. OPERATION PRINCIPLES AND STT-RAM STRUCTURE*\n\nThe most common structure\
    \ of the STT-RAM cell includes an MTJ cell for data storage in series with an\
    \ access transistor (1T1MTJ). MTJ cells include an oxide barrier sandwiched between\
    \ two ferromagnetic layers called Rotation Layer (RL) and Fixed Layer (FL). The\
    \ magnetization orientation between these two layers will result in two different\
    \ states (i.e., parallel(P) and anti-parallel(AP)). These states are interpreted\
    \ as an indicator of logic one and logic zero (see Fig. 1). The concept of Read\
    \ and Write operation in the cell is explained in detail as follows:\n\n#### 1)\
    \ Write Operation\n\nIn order to write the intended information into the MTJ cell,\
    \ a write current should be applied through the memory cell. A successful writing\
    \ operation demands a minimum barrier exploiting the energy of the MTJ cell (*Eb*).\
    \ If the required energy is supplied, then the state of the memory could be changed\
    \ based on the current direction through the memory cell (see Fig. 1). In a coherent\
    \ STT-RAM model, the required current to fulfill the required minimum barrier\
    \ exploiting energy could be expressed as:\n\n$$I\\_c = I\\_{c0}(1 - \\frac{1}{\\\
    Delta\\_{\\text{max}}} \\ln(f\\_0 t\\_p))\\tag{1}$$\n\n$$I\\_{c0} = \\frac{8aeM\\\
    _s t}{\\eta h \\pi d^2} H\\_k \\tag{2}$$\n\nwhere, *Ic*<sup>0</sup> : critical\
    \ write current of the STT-RAM model (at 273*◦* Kelvin). This parameter is related\
    \ to the physical\n\n![](_page_2_Figure_12.jpeg)\n\nFIGURE 1: Schematic representation\
    \ of magnetic orientation and energy barrier between two Magnetic Tunnel Junction\
    \ (MTJ) states [33]\n\nproperty of the MTJ cell, such as: *Ms*: Material saturation\
    \ magnetization; *f*0: Attempt frequency which is typically *∼*1 ns, *Hk*: Effective\
    \ magnetic anisotropy field, *α*: Damping factor, *η*: Spin polarization, *tp*:\
    \ Operating pulse-width (inverse of frequency), which provides an access frequency\
    \ of 1 GHz; *t, d*: physical dimensions of the MTJ cell; ∆: Thermal stability\
    \ factor, and it can be expressed as:\n\n$$\n\\Delta = \\frac{(M\\_s H\\_k \\\
    text{td})}{2k\\_B T} \\tag{3}\n$$\n\nwhere, *kBT* : Describes the ambient energy\
    \ in the system due to random thermal fluctuations [33].\n\n#### 2) Read Operation\n\
    \n*R<sup>P</sup>* Read operation in MTJ cell is due to the current *Iread* passage\
    \ through the cell. *Iread* must be less than *Icritical* and must not lead to\
    \ change in the cell. The MTJ exhibits resistance value based on its two layers'\
    \ magnetization (0*◦* or 180*◦* in Fig. 1). Therefore, by sensing, and passing\
    \ the *Iread* and measuring the resistance, we could identify the state of the\
    \ cell. Moreover, the parameter Tunnel Magnetoresistance Ratio (TMR) is described\
    \ as the difference between these two resistance states and can be expressed as\
    \ *TMR* = (*RAP <sup>−</sup>R<sup>P</sup>* ) , where *RAP* is the resistance in\
    \ anti-parallel and *R<sup>P</sup>* is the resistance in parallel state. TMR is\
    \ directly related to the cell's read latency, which means a higher TMR enables\
    \ a faster and more precise read operation [33].\n\n#### *B. RELATED WORK*\n\n\
    Recent studies show NVM as the main memory element via a simulation in gem5, as\
    \ gem5 was isolated from an NVM interface till October 2020. The lack of an NVM\
    \ interface made it necessary for NVM research to use external tools such as NVSim\
    \ [34], NVMain [35], and NVMain 2.0 [36]. To maintain compatibility, these NVM\
    \ simulators must be developed simultaneously with the gem5 simulator. Especially\
    \ NVMain offers a patch to be applied to the gem5 simulator, which connects the\
    \ NVMain tool to the gem5 simulator. This patch directly modifies the gem5 source\
    \ code and needs to be updated to match the latest gem5 releases. The result of\
    \ the patch is a co-simulator where the gem5 simulator is in constant interaction\
    \ with NVMain. Small changes in the gem5 simulator can directly require a change\
    \ in the NVM simulation tools and the needed patches for integration into a co-simulator.\
    \ The official patch for integrating NVMain into the gem5 was last updated in\
    \ December 2016 and is incompatible with recent releases of the gem5. Furthermore,\
    \ these state-of-theart STT-RAM gem5 tool flow lacks available open-source models,\
    \ which has hampered research and experimentation, impacting the adoption of STT-RAM\
    \ in the current systems.\n\nTherefore, in this manuscript, we propose a holistic\
    \ gem5 based framework that will be open-sourced to fuel research and development\
    \ in this area and further enhancement in the framework.\n\n![](_page_3_Figure_2.jpeg)\n\
    \nFIGURE 2: The STTDDR4 Interface integration into the gem5 standard componentslibrary.\
    \ Components in blue are modified or new to gem5, and components in white are\
    \ unmodified gem5 components.\n\nThe work presented in [37] explores some architecturelevel\
    \ modifications of STT-RAM structure aiming to provide NVM-based row buffers and\
    \ reports a 67% energy improvement exploiting their approaches compared with state-ofthe-art\
    \ techniques. Moreover, authors in [29] explore the possibility of using STT-RAM\
    \ to substitute the DRAM in main memory and evaluate their approach based on the\
    \ SPEC CPU2006 dataset to be compared with DRAM-based memories. This study has\
    \ been carried out on a tracebased cycle-accurate simulator. In [38], an STT-RAM-based\
    \ memory has been proposed based on a 9F2-cell on the circuit level. The exploited\
    \ MTJ model in this work requires a low switching current to change the state\
    \ from logicone to logic-zero. The application-level analysis has been estimated\
    \ based on an HPC SPEC CPU 2017 benchmark for latency improvement. The aforementioned\
    \ techniques either perform only circuit-level simulations (NVSim) which is typically\
    \ time-consuming due to detailed hardware simulations, while other approaches\
    \ that evaluate applications at the system level are not available open-source\
    \ (to the best of our knowledge), thus obstructing the adoption of the STT-RAM\
    \ model in the systems. *Our novel proposed HOPE framework has a fully integrated\
    \ architectural model of STT-RAM in the gem5 simulator using a memory controller\
    \ for exploiting system-level characteristics. This is in contrast to prior approaches\
    \ that rely on external patches (like NVMain). HOPE is an event-driven gem5 simulator\
    \ that facilitates system-level evaluation and enablesthe extraction of comparative\
    \ metrics across all layers of the system hierarchy.*\n\n## **III. THE HOPE FRAMEWORK**\n\
    \n#### *A. HOPE FRAMEWORK OVERVIEW*\n\nThe novel proposed STT-RAM interface in\
    \ this manuscript is an additional memory interface to the gem5. Therefore, in\
    \ order to satisfy the compatibility of the STT-RAM interface with the gem5's\
    \ MemCtrl component, there is a\n\nneed for some modifications to the MemCtrl\
    \ component. This tailoring has no effect on the existing functionality of connecting\
    \ DRAM or NVM memories.\n\nIn the system configuration of gem5, the MemCtrl component\
    \ offers a single port to connect a main memory instance to the MemCtrl, historically\
    \ called *DRAM*, used for all types of memories. We introduced the STT-RAM interface\
    \ to the gem5 as an alternative choice to the DRAMInterface and NVMInterface components\
    \ as shown in Fig. 2. The interface is implemented in C++ (Component functional\
    \ description), wrapped by a Python parameter configuration (part of Component\
    \ definitions). The Python wrapper defines and inherits parameters that are mandatory\
    \ for the component to work. The functionality of the STTDDR4Interface is placed\
    \ within its C++ class. The component STT\\_1333\\_4x16 is the test device, populated\
    \ with parameters from the EMD4E001G16G2 datasheet [39]. Our proposed framework\
    \ enables access to modify the memory via its integrated interface and fetch the\
    \ output data in our component definition. Thus, we integrated all the functionality\
    \ into the STT-RAM memory controller through the provided interface to configure\
    \ and evaluate the system analysis. This provides an edge over the state-of-the-art\
    \ proposed methods that are based on the co-simulation of gem5 and other simulation\
    \ tools (e.g., NVSIM, NVMAIN, etc.).\n\n#### *B. HOPE STT-RAM POWER MODEL*\n\n\
    The implemented STTDDR4 memory interface implementation represents a state machine\
    \ consisting of states for idle, storing, activating, power up, and power down\
    \ stages, as shown in Fig. 3. The MemCtrl instance of the simulated system and\
    \ the memory logic help in the transition of the state in the state machine. As\
    \ soon as the system is started, the very first state of the system is the PWR\\\
    _IDLE state. This is the state from where the transition to activate state PWR\\\
    _ACT, or activate with store state PWR\\_ACT\\_ST\n\n![](_page_4_Figure_1.jpeg)\n\
    \nFIGURE 3: Power state machine of STTDDR4 integration to gem5\n\ncan be achieved\
    \ using an ACT or ACT\\_ST command. The ACT\\_ST command is introduced to gem5\
    \ which is responsible for the STT-RAM-specific handling of data in the volatile\
    \ page buffer.\n\nThe main memory can be exploited using gem5 standard commands\
    \ i.e., for a bank activation ACT (activate), RD (read), WR (write), REF (refresh\
    \ all banks), SREF (Self-Refresh), PRE (explicit pre-charge of a single bank),\
    \ etc. In this manuscript, an additional command is been introduced i.e., ACT\\\
    _ST (activate with the store). In the EMD4E001G16G2 datasheet, [39], the introduced\
    \ ACT\\_ST command is known as ACT\\*. The ACT\\* command includes a store procedure\
    \ for the specific bank accessed. The implementation of the ACT\\_ST command to\
    \ gem5 also includes a new event, the *actStoreEvent*, to enable the transition\
    \ to the new power state PWR\\_ACT\\_ST. Moreover, the automatic execution of\
    \ REF commands needed for data persistence in DRAM is not included in STTDDR4,\
    \ as refreshes are not mandatory in STT-RAM devices as per the EMD4E001G16G2 devices.\n\
    \nAs we know that method calls are responsible for MemCtrl interaction with the\
    \ memory interface during simulation\n\n# **Algorithm 1** Select between ACT and\
    \ ACT\\_ST command on rank:bank:row bank.storingState *←* PERSISTENT bank.lastRow\
    \ *←* 0\n\n```\nprocedure ACTIVATEBANK(rank, bank, row)\n  cmd ← ACT\n  if bank.lastRow\
    \ ̸= row then\n     if bank.storingState = BUFFER then\n        cmd ← ACT _ST\n\
    \        cmdDelay ← cmdDelay + tST\n     end if\n  end if\n  cmdList.push_back(cmd,\
    \ bank, delay) \n  bank.lastRow ← row\n  bank.storingState ← BU FFER\n  process\
    \ cmd in drampower\n```\n#### **Algorithm 2** DRAM Power Extension nOfActsBanks[banks]\
    \ *←* zeros(banks) nOfStoresBanks[banks] *←* zeros(banks) **procedure** EVALUATECOMMANDS(*cmdList*)\
    \ **for all** *cmd ∈ cmdList* **do if** *cmd.type* = *ACT* **then** *handleAct*(*cmd.bank*)\
    \ **else if** *cmd.type* = *ACT*\\_*ST* **then** *handleActSt*(*cmd.bank*) **else\
    \ if** <other command types> **then** <handle commands> **end if end for end procedure\
    \ procedure** HANDLEACTST(*bank*) **if** isPrecharged(bank) **then** *nOf ActsBanks*[*bank*]\
    \ + +; *nOf StoresBanks*[*bank*] + +; **end if end procedure procedure** POWER\\\
    _CALC() *calc*(*sum*(*nOf StoresBanks*) *∗ tST, idd*0 *− idd*3*n*) **for all**\
    \ *bank ∈ banks* **do** *calc*(*nOf StoresBanks*[*bank*] *∗ tST, idd*0 *− ione*)\
    \ **end for end procedure**\n\n**procedure** CALC(*cycles*, *current*) **return**\
    \ (cycles\\*clkPeriod\\*current\\*voltage) **end procedure**\n\ne.g., while reading\
    \ or writing data, the MemCtrl initiates a burst access to the memory device where\
    \ MemCtrl provides data for which rank, bank and row the burst access needs to\
    \ be executed. This rank, bank, and row information are handed over to the bank\
    \ activation method, as shown in algorithm 1. The EMD4E001G16G2 device includes\
    \ functionality for automatic storing of page memory data to the persistent memory\
    \ array when the page memory data would become lost otherwise. The MemCtrl does\
    \ not offer functionality for differentiation of the storing states in STT-RAM.\n\
    \nTherefore, the STTDDR4Interface got extended with functionalities to track the\
    \ storing state of the data in the page buffer of each bank. The storing state\
    \ of each bank supports the states BUFFER and PERSISTENT. All the banks during\
    \ startup are initialized with PERSISTENT, which indicates the page buffer data\
    \ to be saved to the persistent memory array. Moreover, the state BUFFER proves\
    \ the bank to be not saved to the bank's persistent memory array. Also, the last\
    \ row accessed will be saved in each bank and updated during each bank activation.\
    \ In order to change the storing state of a bank, or all banks, to PERSISTENT,\
    \ a store operation needs to be performed. This can be triggered by an ACT\\_ST\
    \ command, a REF command, or an SREF command. Within a REF or SREF execution,\
    \ store operations on all banks in storing state BUFFER will be performed. When\
    \ there are no banks in storing state BUFFER, the commands REF and SREF are not\
    \ effective.\n\nThe selection between the ACT or ACT\\_ST command is done in the\
    \ activated bank method with a sequence of procedure calls as Algorithm 1. The\
    \ requested row is compared\n\nto the last accessed row of the particular bank.\
    \ When the last accessed data is still the working data, which means the last\
    \ access row and requested row are identical, there is no need for a store operation.\
    \ In this case, a normal ACT command will be simulated. If the requested row differs\
    \ from the last accessed row, and the bank is in the storing state \"BUFFER\"\
    , an ACT\\_ST command will be pushed to simulate. The difference in simulating\
    \ ACT or ACT\\_ST is implemented in a higher delay for the additional store operation,\
    \ which is known as the store time (tST). This store procedure call impacts the\
    \ energy consumption that can be calculated in the power library. The bank's storing\
    \ state can be changed to \"PERSISTENT\" by performing a store operation on the\
    \ particular bank, or on all banks. The ACT command is simulated as with DRAM\
    \ memory in gem5: The ACT command is saved in a list of pending commands which\
    \ is handed over to the modified DRAMPower tool [40], which is part of gem5 and\
    \ performs energy calculations from the gem5 inputs.\n\n**Extensions to DRAMPower\
    \ Model:** To be able to include the ACT\\_ST command, DRAMPower got extended\
    \ by the command and energy calculation. The extensions to DRAMPower are presented\
    \ in Algorithm 2. These extensions include functionality for counting the number\
    \ of store procedures during runtime and calculating the resulting store energy\
    \ and power.\n\nMoreover, the energy calculation in gem5 is not updated on every\
    \ new command execution, but on specific simulation phases: suspension, end of\
    \ a refresh (REF) command, triggered from the gem5 system configuration script\
    \ or by the internal command line of the full system simulation using the m5 utility\
    \ (the gem5 utility used in full system disk images).\n\nFurthermore, the gem5\
    \ statistic output has been modified to include the store energy per rank and\
    \ power state time per rank in the simulation results. In this section, we present\
    \ our HOPE framework for an all-integrated STT-RAM with a gem5 simulator using\
    \ the memory controller. This helps in exploiting system-level meta-heuristics\
    \ that include power consumption, memory utilization, heat patterns, etc.\n\n\
    #### *C. HOPE CONFIGURATION*\n\nThe gem5 being an instruction-level simulator\
    \ enables the integration of different types of memories with a memory controller.\
    \ The memory controller is a component that enables an option of choosing memory\
    \ for system-level analysis. The memory controller has evolved a lot in the\n\n\
    TABLE 2: The Configuration of the Memory cell\n\n| Paramters             | Configuration\
    \                    |\n|-----------------------|----------------------------------|\n\
    | Memory                | 1Gbit x16                        |\n| Organization \
    \         | 8-banks (2-banks per Bank Group) |\n| Latency               | 667MHz\
    \ (1333MT/s)                |\n| Access Time           | 225ps               \
    \             |\n| Supply Voltage - Min: | 1.14 V                           |\n\
    | Supply Voltage - Max: | 1.26 V                           |\n\npast few years.\
    \ Recently, in May 2020 the gem5 introduced a new memory controller (MemCtrl)\
    \ component revision and introduced an NVM interface (NVMInterface) class to the\
    \ gem5, officially integrated into version 20.1 of the gem5 simulator. This NVM\
    \ interface is designed very generic in terms of its functionality and parameters\
    \ to be taken into consideration. The NVMInterface class offers three timing parameters:\
    \ tREAD, tWRITE, and tSEND.\n\nThere is also an already existing DRAM interface\
    \ (DRAMInterface) class. This class of the gem5 contains detailed logic on DRAM\
    \ timing and power state simulation and offers various timing, e.g. tRCD, tCL,\
    \ tRAS, and tREFI, and energy parameters, e.g. IDD0, IDD4R, and VDD. But there\
    \ is no such logic for calculating NVM energy and power consumption. Also, there\
    \ are no energy parameters available for NVMInterface.\n\nThus, to overcome such\
    \ shortcomings, HOPE provides another detailed memory interface targeting STT-RAM.\
    \ This memory targets real-world STT-RAM devices which are designed as STT on\
    \ top of DDR4. Therefore, this interface is named as STTDDR4Interface. This offers\
    \ a high level of detail timing and energy parameters, combined with a power state\
    \ and energy calculation logic. Fig. 2 depicts the detailed flow of our HOPE framework\
    \ within the extended and modified gem5 simulator.\n\nThe simulated system is\
    \ configured using the fs.py system configuration script. Using this script, a\
    \ System instance is set up according to the input values of the HOPE framework.\
    \ We use an X86 architecture-based system. The CPU we defined is the TimingCPU,\
    \ which offers detailed memory timings during simulation. The CPU is also equipped\
    \ with L1 and L2 caches. The system uses a Linux kernel and a disk image with\
    \ an Ubuntu operating system and workloads installed. Within gem5, communication\
    \ between system blocks is done via ports, also as real systems do. The system\
    \ block is connected to the Membus block. The memory bus selected is by default,\
    \ the SystemXBar. All CPU interactions to the main memory are forwarded by the\
    \ Membus to the memory controller. MemCtrl got modified to support STT-RAM connected\
    \ through the memory port.\n\nFig. 2, shows the proposed architecture with the\
    \ STT\\_1333\\_4x16 which is a class created for interfacing STT-RAM with the\
    \ memory controller. It has multiple parameters e.g., tCK, tST, tRCD, IDD0, IDD2N,\
    \ etc that has been extracted from the datasheet of the aforementioned device.\n\
    \nThe tCK is the clock period, depending on the device operating clock frequency\
    \ (fCK) (e.g., fCK = 667MHz results in 1.5ns tCK (=1/fCK)), tST is a special timing\
    \ parameter for STT-RAM and refers to the storing time of the memory (indicates\
    \ the time needed for storing the data from the row address buffer to the persistent\
    \ STT memory). The tST is a newly introduced parameter to gem5 unique to STT,\
    \ which was a missing timing parameter for gem5. The address buffer acts like\
    \ a cache, and the data placed in the cache needs to be written to the main STT\
    \ memory\n\nTABLE 3: Experimental systems configuration for STT-RAM and DRAM\n\
    \n| System elements | Processor                                 | L1 Instr.<br>cache\
    \ | L1 Data<br>cache | L2 cache         | Main Memory                        \
    \                                                            | Clock Speed | Row\
    \ Buffer<br>Size | Device Size | Channel<br>Cappacity | tRCDmin | tRCmin | tRASmin\
    \ | tFAWmin | tRPmin |\n|-----------------|-------------------------------------------|--------------------|------------------|------------------|------------------------------------------------------------------------------------------------|-------------|--------------------|-------------|----------------------|---------|--------|---------|---------|--------|\n\
    | STT-RAM         | 64-bit x86<br>single core,<br>timing 3GHz | Private,<br>32kB\
    \   | Private,<br>32kB | Shared,<br>256kB | 1 channel,<br>2 rank per channel,<br>4\
    \ chips per rank,<br>EMD4E001G16G2,<br>1Gbit x16, 1333MHz | 667MHz      | 256B\
    \               | 128MiB      | 1GiB                 | 135ns   | 44.5ns | 32ns\
    \    | 15ns    | 12.5ns |\n| DRAM            | \"                            \
    \             | \"                  | \"                | \"                |\
    \ 1 channel,<br>2 rank per channel,<br>8 chips per rank,<br>MT40A1G8SA,<br>1Gbit\
    \ x8, 2400MHz     | 1200MHz     | 1kB                | 1GiB        | 16GiB   \
    \             | 12.5ns  | 190ns  | 143ns   | 240ns   | 7.5ns  |\n\nduring the\
    \ Store operation. Therefore, tST is the time needed to process data moving from\
    \ the row address buffer to the STT persistent memory array. Researchers in the\
    \ field could optimize different metrics to minimize this value and evaluate the\
    \ performance of so-called in-memory processing approaches. The parameters such\
    \ as tRCD, IDD0, IDD2N, etc., are standardized DDR4 parameters.\n\nMoreover, this\
    \ interface makes it possible to simulate systems using the latest STT-RAM devices\
    \ including power states and energy consumption as it was never possible before\
    \ in a stand-alone gem5 environment. The integration carried out on the interface\
    \ is based on the parameters offered by the STT-RAM EMD4E001G16G2 from Everspin\
    \ Technologies [39]. These device parameters are shown in Table 2.\n\nAs per the\
    \ physical characteristics of STT-RAM, there are deviations to the DDR4 specification\
    \ for DRAM especially the *Refresh* command, which is mandatory to be issued in\
    \ a time interval tREFI on DRAM, is no longer used in STT-RAM. Therefore, tREFI\
    \ got removed for STT memory. Moreover, the STT-RAM also has a store time parameter\
    \ tST. The store operation of delay tST, is used to move recently written data\
    \ from the page buffer to the persistent memory array.\n\nSome other deviations\
    \ specific to the test devices (simulating the EMD4E001G16G2 device) include the\
    \ memory size that in the case of STT-RAM is a 1 Gbit device, whereas the DDR4\
    \ specification for DRAM only allows devices of 2, 4, 8 and 16 GBit. Furthermore,\
    \ there is also a limit of 667 Mhz for the clock frequency, while the DDR4 Specification\
    \ for DRAM allows 800, 933, 1067, and 1200MHz.\n\n### *D. EVALUATION SETUP CONFIGURATION*\n\
    \nFig. 4 presents a comprehensive overview of the HOPE framework setup configuration\
    \ steps, highlighting its key contributions depicted in blue. gem5 full system\
    \ simulations require a disk image prepared with an operating system and a kernel\
    \ compatible with the chosen operating system. The HOPE framework uses a 1 modified\
    \ Packer SPEC CPU 2017 setup script from the gem5 resources repository for generating\
    \ a disk image containing the Ubuntu operating system and the SPEC CPU 2017 benchmark\
    \ suite for X86 architecture. The 2 benchmark installation is then followed finalized\
    \ by mounting the disk image on the host system. Each benchmark from the disk\
    \ image has been run once for finalizing the benchmark installation, this includes\
    \ com-\n\n![](_page_6_Figure_10.jpeg)\n\nFIGURE 4: Overview of HOPE framework\n\
    \npiling, training, and running the benchmark. In 3 the gem5 full system simulation\
    \ including the HOPE extensions and modifications is run. Therefore the created\
    \ disk image is used. Each simulation runs a selected workload from the prepared\
    \ disk image, includes the creation of checkpoints after the operating system\
    \ boot, and the output of the 4 detailed statistics after the gem5 simulation\
    \ is completed. The gem5 full system simulation includes the introduced STT-RAM\
    \ extension and modifications to DRAMPower to allow detailed energy calculation\
    \ for our STT-RAM device. 5 shows the provided McPAT template file, modified to\
    \ support the extended outputs of gem5. Using the system configuration, simulation\
    \ statistics, and McPAT template, the 6 the GEM5ToMcPAT [41] tool is used to generate\
    \ an input file for later use with McPAT. HOPE includes the enhanced 7 cMcPAT\
    \ power, area, and timing modeling framework. cMcPAT [41] is capable of calculating\
    \ the power parameters of 9. Using the statistics output of gem5, and the power\
    \ model of cMcPAT, the script \"print\\_energy\" [41] is calculating the total\
    \ energy consumption of the simulated environment. The results also combine the\
    \ detailed output of gem5, especially, the instructions count.\n\nThe simulated\
    \ system is configured using the gem5 stdlib (gem5 standard library) based on\
    \ an X86 configuration. Table 3 lists the detailed configuration of the processor,\
    \ cache, and memory for both experimental systems using STT-RAM and DRAM. We selected\
    \ an existing STT-RAM device for simulation and paired it with a widely used DRAM\
    \ device to facilitate a functional comparison. It's\n\nimportant to note that\
    \ our chosen STT-RAM device does not align with JEDEC's JESD79-4A DDR4 standard,\
    \ which limits our ability to select a DDR4-based DRAM device with nearly identical\
    \ parameters.\n\nMoreover, Fig. 5 shows the system configuration used for benchmarks\
    \ using STT-RAM. gem5 offers a general full system default configuration script\
    \ (fs.py), which we used in this work to reflect the system architecture of our\
    \ simulated system. We used the gem5 full system emulation mode to reflect real-world\
    \ systems in the best way provided.\n\nThe operating system selected is an Ubuntu\
    \ 18.04 configured for gem5 and set up with a SPEC CPU 2017 benchmark suite [42]\
    \ instance. The kernel used is the linux kernel version 4.19.83. The gem5 configuration\
    \ script handles the creation of memory controllers and memory devices. The count\
    \ of memory ranks and banks is set in the memory device configuration. The parameters\
    \ used for the STT-RAM device configuration in gem5 are sourced from the EMD4E001G16G2\
    \ datasheet [39]. We performed SPEC CPU 2017 benchmarks on our simulated systems\
    \ using 2 checkpoints per benchmark to be able to perform detailed simulations\
    \ using the TimingCPU from gem5.\n\nThe first checkpoint has been saved after\
    \ the OS boot is finished. The second checkpoint has been saved after the first\
    \ 4.5 billion instructions of benchmark application to ensure the initialization\
    \ phase has been finished and the checkpoint defines a direct jump into the main\
    \ benchmark algorithm. Both checkpoints were performed in a fastforward method\
    \ using gem5's AtomicSimpleCPU. The main simulation run was done from the second\
    \ checkpoint for a total of 2 Billion instructions. This procedure has been performed\
    \ for all of the benchmark applications included in SPEC CPU 2017.\n\n## **IV.\
    \ RESULTS**\n\nIn our research endeavour, our primary focus revolved around conducting\
    \ simulations utilizing the parameters of real-world devices. Specifically, we\
    \ honed in on a selected STT-RAM device, which holds the distinction of being\
    \ DDR4-compatible, albeit with certain deviations from the official standard.\
    \ As we delved into the simulations, we\n\n![](_page_7_Figure_8.jpeg)\n\nFIGURE\
    \ 5: Architecture of the STT-RAM simulated system.\n\nobserved that these deviations\
    \ had a tangible impact on our results, which we duly documented.\n\nOur findings\
    \ shed light on the applicability of HOPE as a potent tool for evaluating the\
    \ feasibility of incorporating STT-RAM main memories into practical systems. By\
    \ harnessing the capabilities of HOPE, we were able to gain insightful glimpses\
    \ into the performance and energy efficiency of the SPEC CPU 2017 benchmarks when\
    \ paired with STT-RAM. This analytical approach not only offers a valuable lens\
    \ to understand the potential of STT-RAM but also opens up new vistas of exploration\
    \ and optimization in memory technology research.\n\nThe gem5 simulator provides\
    \ users with multiple avenues for creating system configurations through its gem5\
    \ standard library of components and functions, including CPUs, memory, boards,\
    \ cache hierarchies, and more. This comprehensive collection of building blocks\
    \ is commonly referred to as the gem5 standard library. For instance, if you wish\
    \ to modify the CPU architecture, you can simply select the corresponding board\
    \ (e.g., X86Board or ArmBoard) and fine-tune the memory configurations accordingly.\
    \ This flexible approach can be applied to alter any available system component.\n\
    \nAlternatively, there is an option to configure a system within gem5 without\
    \ utilizing a predefined board. Instead, you can manually establish connections\
    \ between a CPU and a selected memory device using a memory bus. Furthermore,\
    \ you have the flexibility to augment the CPU with either a straightforward single-level\
    \ cache or a more intricate cache hierarchy to suit your needs.\n\nOur proposed\
    \ framework takes a distinct approach by leveraging an existing system configuration\
    \ known as fs.py. This configuration can be effortlessly modified via command-line\
    \ inputs, enabling rapid adjustments to the system configuration with a single\
    \ bash script edit. Different fs.py configurations are available for various system\
    \ architectures, such as *configs/example/riscv/fs*\\_*linux.py* for RISC-V or\
    \ *configs/example/arm/fs*\\_*bigLITTLE.py* for ARM.\n\nIn our research endeavors,\
    \ we conducted extensive simulations employing both fs.py and custom gem5 system\
    \ configuration scripts. This comprehensive approach allowed us to thoroughly\
    \ assess and analyze our simulations.\n\n## *A. STT-RAM STATE TIME DISTRIBUTION*\n\
    \nAs depicted in Fig. 8(b), the power state times for the IDLE state exhibit variations\
    \ in the case of STT-RAM, contingent upon the specific workload. Notably, STT-RAM\
    \ distinguishes itself by eschewing the need for periodic refreshes to maintain\
    \ data states, leading to a complete absence of time spent in the REF power state.\
    \ Conversely, the bank activation (ACT) time, also illustrated in Fig. 8(b), demonstrates\
    \ only a minor variation within the STT-RAMbased system. This effect can be attributed\
    \ to the relatively prolonged delays observed in the bank activation process,\
    \ especially concerning store state (ACT\\_ST) for STT-RAM,\n\nas visually demonstrated\
    \ in Fig. 8(b). Such insights garnered from our analyses provide valuable perspectives\
    \ on the dynamic behaviour and efficiency of STT-RAM, imparting crucial knowledge\
    \ for potential real-world implementations and optimizations in-memory technologies.\n\
    \n# *B. STT-RAM PERFORMANCE EVALUATION*\n\nFig. 6(a) show the total amount of\
    \ read and write requests which are generally higher in the STT-RAM devices. High\
    \ read and write requests are explained by Fig. 8(a) which shows the hit rate\
    \ for read operations. The hit rates for STT-RAM depend on the application complexity\
    \ level. Fig. 8(c) shows the average read and write bandwidth with a simulated\
    \ STT-RAM. Also, the average latency of STT-RAM, shown in Fig. 6(b), for each\
    \ application highly depends on the hit ratio. As described in our STTDDR4 power\
    \ state machine description, a high alteration of accessed rows negatively affects\
    \ the energy and latency of our simulated STT-RAM device. A high alteration of\
    \ accessed rows further has an impact on store operations.\n\n# *C. STT-RAM POWER\
    \ AND ENERGY BREAKDOWN*\n\nIn Fig. 6 (c and d), we present a comprehensive view\
    \ of the power and energy breakdown for our simulated systems, offering valuable\
    \ insights into their performance characteristics. The shown parameters in d represent\
    \ the accumulated energy for different commands issued to the memory device. As\
    \ shown in Fig. 3, the memory devices move through different states during runtime.\
    \ The parameter \"Activation energy\" therefore shows the total energy consumption\
    \ for all ACT commands initiated. \"Store energy\" accumulates the energy of all\
    \ store operations during the evaluation. In the case of an ACT\\_ST operation,\
    \ the energy accumulated separately for the activation and the store energy results.\
    \ The parameter \"Refresh energy\" is associated to the REF command, while the\
    \ \"RD/WR energy\" is the accumulated energy during the processing of the read\
    \ and write burst accesses.\n\nParticularly noteworthy is the substantial count\
    \ of store operations, which, in conjunction with the notably high IDD0 current\
    \ of 437mA, prominently influences the calculated store energy. As evident from\
    \ the results, STT-RAM stands out for its lack of refresh energy requirements.\
    \ However, it should be noted that the stored energy demands contribute to an\
    \ overall increase in the total energy breakdown. These findings shed light on\
    \ the contrasting energy consumption patterns of STT-RAM compared to conventional\
    \ memory technologies, signifying the potential for more energy-efficient computing\
    \ paradigms. The comprehensive understanding gained from these power and energy\
    \ analyses is essential for devising strategies to optimize memory architectures,\
    \ thus fostering advancements in the realm of energy-efficient computing systems.\
    \ The presented energy parameters in d are not a full view of all calculated energy\
    \ parameters within gem5, but an excerpt of significant values. The full list\
    \ of energy parameters is extended by\n\nparameters for interface energy, standby\
    \ energy, self-refresh energy, power-down and power-up energy.\n\n# *D. DRAM METRICS*\n\
    \nTo maintain the extendibility and versatility of our framework, we have thoughtfully\
    \ retained the interface to the DRAM. This strategic decision allows our framework\
    \ to adapt effortlessly to various memory technologies, rendering it highly versatile\
    \ for a wide array of computing scenarios. In this section, we present an in-depth\
    \ analysis of the extracted data concerning the state time distribution, power\
    \ consumption, energy usage, and latency breakdown for the same applications from\
    \ SPEC 2017, with the DRAM serving as the primary memory. This comprehensive investigation\
    \ is instrumental in understanding the behavior and performance characteristics\
    \ of our framework when interfacing with DRAM.\n\nFig. 9(b) illustrates the state\
    \ time distribution of these applications when utilizing DRAM. Additionally, Fig.\
    \ 7(a) showcases the framework's memory requests. Analyzing memory requests offers\
    \ deeper insights into the applications´ memory access patterns, shedding light\
    \ on potential areas for improvement in terms of data locality and cache utilization.\
    \ Fig. 9(c) provides insight into bandwidth usage with DRAM as the memory. Bandwidth\
    \ utilization is a critical metric for assessing memory system efficiency and\
    \ identifying potential bottlenecks that may impact application performance. Furthermore,\
    \ Fig. 9(a) reveals DRAM row hits, and Fig. 7(b) presents latency per application.\
    \ Finally, Fig. 7(c and d) exhibits DRAM average power and energy usage while\
    \ running the SPEC 2017 applications. These detailed analyses offer valuable insights\
    \ into our framework's performance and its potential for adaptation to future\
    \ memory technologies and diverse computing environments.\n\n## *E. COMPARING\
    \ STT-RAM AND DRAM RESULTS*\n\nBased on our evaluation of SPEC 2017 benchmarks,\
    \ it becomes evident that STT-RAM is not yet ready to replace DRAM-based main\
    \ memories for many applications due to its higher store latency and energy consumption.\
    \ STT-RAM needs storing from the page buffer to the persistent memory array whereas\
    \ DRAM does not need this. Due to the overhead for storing data in the STT persistent\
    \ memory array within the ACT-ST state, the delays are significantly higher than\
    \ in DRAM. Each store takes 380ns extra, in all cases of ACT-ST state. Furthermore,\
    \ STT-RAM is running at 1333 MHz, whereas DRAM is running at 2400 MHz. The impact\
    \ is especially pronounced in applications with high write-to-read ratios like\
    \ Ibm\\_s (see Fig. 6a). Before STT-RAM can be a feasible alternative to DRAM's\
    \ main memories, further technology, and architectural optimizations are necessary\
    \ to reduce the store latency and energy requirements. Fortunately, with the availability\
    \ of HOPE, we now have a systematic means to evaluate and optimize STT-RAM at\
    \ the system level. HOPE presents an invaluable opportunity to drive STT-RAM's\
    \ progress by allowing us to\n\n![](_page_9_Figure_2.jpeg)\n\nexplore and analyze\
    \ various architectural possibilities with greater precision.\n\n## **V. CONCLUSION**\n\
    \nWe presented an extension to the open-source full-system simulator gem5 for\
    \ enabling detailed evaluation of STT-RAM devices in an accurate manner. We have\
    \ shown our implemented power state machine, memory commands, power calculation,\
    \ and statistics output. We have also shown the results of an STT-RAM-based system\
    \ configured using real-world device parameters and compared the resulting metrics\
    \ to a DRAM-based system. The STT-RAM-specific characteristics of required store\
    \ operations and deviations to the DDR4 standard for DRAM have been discussed\
    \ based on the comparison of the simulation results. Our HOPE implementation is\
    \ easily configurable for other STT-RAM devices, by adding timing values, currents,\
    \ and voltages. We will open-source our HOPE framework to fuel research and accelerate\
    \ the development of future system architectures based on STT-RAM.\n\n## **VI.\
    \ ACKNOWLEDGMENT**\n\nThe authors acknowledge TU Wien Bibliothek for financial\
    \ support through its Open Access Funding Programme.\n\n#### **REFERENCES**\n\n\
    - [1] Y. Wang, C. Zhang, H. Yu, and W. Zhang, \"Design of low power 3d hybrid\
    \ memory by non-volatile cbram-crossbar with block-level data-retention,\" in\
    \ Proceedings of the 2012 ACM/IEEE international symposium on Low power electronics\
    \ and design, 2012, pp. 197–202.\n- [2] Y. Shin, \"Non-volatile memory technologies\
    \ for beyond 2010,\" in Digest of Technical Papers. 2005 Symposium on VLSI Circuits,\
    \ 2005. IEEE, 2005, pp. 156–159.\n- [3] B. Gervasi, \"Will carbon nanotube memory\
    \ replace dram?\" IEEE Micro, vol. 39, no. 2, pp. 45–51, 2019.\n- [4] J. Lamb,\
    \ S. Gibbons, R. Trichur, Y. Jiang, K. Mangelson, K. Kremer, and D. Janzen, \"\
    Advancements in microelectronics-grade carbon nanotube materials for nram® device\
    \ manufacture and analysis of carbon nanotube mass in end user devices.\"\n- [5]\
    \ \"Irds 2022 beyond cmos and emerging materials integration,\" Online, 2022,\
    \ accessed on: November 7, 2023. [Online]. Available: https://irds.ieee.org/editions/2022\n\
    - [6] D. Jana, S. Roy, R. Panja, M. Dutta, S. Z. Rahaman, R. Mahapatra, and S.\
    \ Maikap, \"Conductive-bridging random access memory: challenges and opportunity\
    \ for 3d architecture,\" Nanoscale research letters, vol. 10, pp. 1–23, 2015.\n\
    - [7] S. Mittal and J. S. Vetter, \"A survey of software techniques for using\
    \ non-volatile memories for storage and main memory systems,\" IEEE Transactions\
    \ on Parallel and Distributed Systems, vol. 27, no. 5, pp. 1537– 1550, 2015.\n\
    - [8] J. S. Meena, S. M. Sze, U. Chand, and T.-Y. Tseng, \"Overview of emerging\
    \ nonvolatile memory technologies,\" Nanoscale research letters, vol. 9, pp. 1–33,\
    \ 2014.\n- [9] T. Mikolajick, C. Dehm, W. Hartner, I. Kasko, M. Kastner, N. Nagel,\
    \ M. Moert, and C. Mazure, \"Feram technology for high density applications,\"\
    \ Microelectronics Reliability, vol. 41, no. 7, pp. 947–950, 2001.\n- [10] M.\
    \ Imani, S. Patil, and T. Rosing, \"Low power data-aware stt-ram based hybrid\
    \ cache architecture,\" in 2016 17th international symposium on quality electronic\
    \ design (isqed). IEEE, 2016, pp. 88–94.\n- [11] S. Jeloka, Z. Wang, R. Xie, S.\
    \ Khanna, S. Bartling, D. Sylvester, and D. Blaauw, \"Energy efficient adiabatic\
    \ fram with 0.99 pj/bit write for iot applications,\" in 2018 IEEE symposium on\
    \ VLSI circuits. IEEE, 2018, pp. 85–86.\n- [12] M. Moore et al., \"International\
    \ roadmap for devices and systems,\" Accessed: Jan, 2020.\n- [13] I. Yoon, A.\
    \ Anwar, T. Rakshit, and A. Raychowdhury, \"Transfer and online reinforcement\
    \ learning in stt-mram based embedded systems for autonomous drones,\" in 2019\
    \ Design, Automation & Test in Europe Conference & Exhibition (DATE). IEEE, 2019,\
    \ pp. 1489–1494.\n- [14] B. Narasimham, V. Chaudhary, M. Smith, L. Tsau, D. Ball,\
    \ and B. Bhuva, \"Scaling trends in the soft error rate of srams from planar to\
    \ 5-nm finfet,\" in 2021 IEEE International Reliability Physics Symposium (IRPS).\
    \ IEEE, 2021, pp. 1–5.\n- [15] J. Wang, N. Xiu, J. Wu, Y. Chen, Y. Sun, H. Yang,\
    \ V. Narayanan, S. George, and X. Li, \"An 8t/cell fefet-based nonvolatile sram\
    \ with improved density and sub-fj backup and restore energy,\" in 2022 IEEE International\
    \ Symposium on Circuits and Systems (ISCAS), 2022, pp. 3408–3412.\n- [16] J. Y.\
    \ Kim, M.-J. Choi, and H. W. Jang, \"Ferroelectric field effect transistors: Progress\
    \ and perspective,\" APL Materials, vol. 9, no. 2, p. 021102, 02 2021.\n- [17]\
    \ S. Yu, Q. Wang, Y. Zhang, P. Yang, X. Luo, H. Liu, C. Chen, Q. Li, and S. Liu,\
    \ \"Multistate capability improvement of beol compatible fefet by introducing\
    \ an al2o3 interlayer,\" IEEE Transactions on Electron Devices, vol. 70, no. 11,\
    \ pp. 5632–5637, 2023.\n- [18] J. Y. Park, D.-H. Choe, D. H. Lee, G. T. Yu, K.\
    \ Yang, S. H. Kim, G. H. Park, S.-G. Nam, H. J. Lee, S. Jo, B. J. Kuh, D. Ha,\
    \ Y. Kim, J. Heo, and M. H. Park, \"Revival of ferroelectric memories based on\
    \ emerging fluoritestructured ferroelectrics,\" Advanced Materials, vol. 35, no.\
    \ 43, p. 2204904, 2023.\n- [19] S. Seyedfaraji, J. T. Daryani, M. M. S. Aly, and\
    \ S. Rehman, \"Extent: Enabling approximation-oriented energy efficient stt-ram\
    \ write circuit,\" IEEE Access, vol. 10, pp. 82 144–82 155, 2022.\n- [20] S. M.\
    \ Nair, R. Bishnoi, A. Vijayan, and M. B. Tahoori, \"Dynamic faults based hardware\
    \ trojan design in stt-mram,\" in 2020 Design, Automation & Test in Europe Conference\
    \ & Exhibition (DATE). IEEE, 2020, pp. 933–938.\n- [21] R. Bishnoi, M. Ebrahimi,\
    \ F. Oboril, and M. B. Tahoori, \"Improving write performance for stt-mram,\"\
    \ IEEE Transactions on Magnetics, vol. 52, no. 8, pp. 1–11, 2016.\n- [22] S. Swami\
    \ and K. Mohanram, \"Reliable nonvolatile memories: Techniques and measures,\"\
    \ IEEE Design & Test, vol. 34, no. 3, pp. 31–41, 2017.\n- [23] S. Seyedfaraji,\
    \ A. M. Hajisadeghi, J. Talafy, and H. R. Zarandi, \"Dysco: Dynamic stepper current\
    \ injector to improve write performance in stt-ram memories,\" Microprocessors\
    \ and Microsystems, vol. 73, p. 102963, 2020.\n- [24] E. Garzon, R. De Rose, F.\
    \ Crupi, L. Trojman, G. Finocchio, M. Carpentieri, and M. Lanuzza, \"Assessment\
    \ of stt-mrams based on double-barrier mtjs for cache applications by means of\
    \ a device-to-system level simulation framework,\" Integration, vol. 71, pp. 56–69,\
    \ 2020.\n- [25] R. Saha, Y. P. Pundir, and P. K. Pal, \"Design of an area and\
    \ energyefficient last-level cache memory using stt-mram,\" Journal of Magnetism\
    \ and Magnetic Materials, vol. 529, p. 167882, 2021.\n- [26] E. Cheshmikhani,\
    \ H. Farbeh, and H. Asadi, \"3rset: Read disturbance rate reduction in stt-mram\
    \ caches by selective tag comparison,\" IEEE Transactions on Computers, vol. 71,\
    \ no. 6, pp. 1305–1319, 2021.\n- [27] ——, \"Robin: Incremental oblique interleaved\
    \ ecc for reliability improvement in stt-mram caches,\" in Proceedings of the\
    \ 24th Asia and South Pacific Design Automation Conference, 2019, pp. 173–178.\n\
    - [28] N. Mahdavi, F. Razaghian, and H. Farbeh, \"Data block manipulation for\
    \ error rate reduction in stt-mram based main memory,\" The Journal of Supercomputing,\
    \ vol. 78, no. 11, pp. 13 342–13 372, 2022.\n- [29] E. Kültürsay, M. Kandemir,\
    \ A. Sivasubramaniam, and O. Mutlu, \"Evaluating stt-ram as an energy-efficient\
    \ main memory alternative,\" in 2013 IEEE International Symposium on Performance\
    \ Analysis of Systems and Software (ISPASS), 2013, pp. 256–267.\n- [30] D. Sanchez\
    \ and C. Kozyrakis, \"Zsim: Fast and accurate microarchitectural simulation of\
    \ thousand-core systems,\" ACM SIGARCH Computer architecture news, vol. 41, no.\
    \ 3, pp. 475–486, 2013.\n- [31] N. Binkert, B. Beckmann, G. Black, S. K. Reinhardt,\
    \ A. Saidi, A. Basu, J. Hestness, D. R. Hower, T. Krishna, S. Sardashti et al.,\
    \ \"The gem5 simulator,\" ACM SIGARCH computer architecture news, vol. 39, no.\
    \ 2, pp. 1–7, 2011.\n- [32] A. Hansson, N. Agarwal, A. Kolli, T. Wenisch, and\
    \ A. N. Udipi, \"Simulating dram controllers for future system architecture exploration,\"\
    \ in 2014 IEEE International Symposium on Performance Analysis of Systems and\
    \ Software (ISPASS). IEEE, 2014, pp. 201–210.\n- [33] A. Gebregiorgis, L. Wu,\
    \ C. Münch, S. Rao, M. B. Tahoori, and S. Hamdioui, \"Special session: Stt-mrams:\
    \ Technology, design and test,\" in 2022 IEEE 40th VLSI Test Symposium (VTS).\
    \ IEEE, 2022, pp. 1–10.\n- [34] X. Dong, C. Xu, Y. Xie, and N. P. Jouppi, \"Nvsim:\
    \ A circuit-level performance, energy, and area model for emerging nonvolatile\
    \ memory,\" IEEE Transactions on Computer-Aided Design of Integrated Circuits\
    \ and Systems, vol. 31, no. 7, pp. 994–1007, 2012.\n- [35] M. Poremba and Y. Xie,\
    \ \"Nvmain: An architectural-level main memory simulator for emerging non-volatile\
    \ memories,\" in 2012 IEEE Computer Society Annual Symposium on VLSI. IEEE, 2012,\
    \ pp. 392–397.\n- [36] M. Poremba, T. Zhang, and Y. Xie, \"Nvmain 2.0: A user-friendly\
    \ memory simulator to model (non-) volatile memory systems,\" IEEE Computer Architecture\
    \ Letters, vol. 14, no. 2, pp. 140–143, 2015.\n- [37] J. L. NMeza, Justin and\
    \ O. Mutlu., \"Evaluating row buffer locality in future non-volatile main memories,\"\
    \ in arXiv preprint arXiv:1812.06377. arXiv, 2018.\n- [38] S. Chung, K.-M. Rho,\
    \ S.-D. Kim, H.-J. Suh, D.-J. Kim, H.-J. Kim, S.-H. Lee, J.-H. Park, H.-M. Hwang,\
    \ S.-M. Hwang, J.-Y. Lee, Y.-B. An, J.-U. Yi, Y.-H. Seo, D.-H. Jung, M.-S. Lee,\
    \ S.-H. Cho, J.-N. Kim, G.-J. Park, G. Jin, A. Driskill-Smith, V. Nikitin, A.\
    \ Ong, X. Tang, Y. Kim, J.-S. Rho, S.-K. Park, S.-W. Chung, J.-G. Jeong, and S.-J.\
    \ Hong, \"Fully integrated 54nm stt-ram with the smallest bit cell dimension for\
    \ high density memory pplication,\" in 2010 International Electron Devices Meeting,\
    \ 2010, pp. 12.7.1–12.7.4.\n- [39] \"1 gb non-volatile st-ddr4 spin-transfer torque\
    \ mram.\" [Online]. Available: https://shorturl.at/bgsMR\n- [40] K. Chandrasekar,\
    \ C. Weis, Y. Li, B. Akesson, N. Wehn, and K. Goossens, \"Drampower: Open-source\
    \ dram power & energy estimation tool,\" [URL:http://www.](http://www/) drampower.\
    \ info, vol. 22, 2012.\n- [41] A. Brokalakis, N. Tampouratzis, A. Nikitakis, I.\
    \ Papaefstathiou, S. Andrianakis, D. Pau, E. Plebani, M. Paracchini, M. Marcon,\
    \ I. Sourdis, P. R. Geethakumari, M. C. Palacios, M. A. Anton, and A. Szasz, \"\
    Cossim: An open-source integrated solution to address the simulator gap for systems\
    \ of systems,\" in 2018 21st Euromicro Conference on Digital System Design (DSD),\
    \ 2018, pp. 115–120.\n- [42] [Online]. Available: [https://www.S](http://www.spec.org/cpu2017)PEC.o[rg/cpu2017](http://www.spec.org/cpu2017)\n\
    \n![](_page_11_Picture_11.jpeg)\n\nSAEED SEYEDFARAJI is a Graduate Student Member,\
    \ IEEE, and holds a B.Sc. degree from Isfahan University of Technology in Isfahan,\
    \ Iran, and an M.Sc. degree from Amirkabir University of Technology (Tehran Polytechnique)\
    \ in Tehran, Iran. Currently, he is pursuing a Ph.D. in computer engineering at\
    \ the Technische Universität Wien (TU Wien), Austria, where he also serves as\
    \ a University Assistant. His research interests encompass emerging non-volatile\
    \ memory\n\ntechnologies, in-memory processing, the integration of intelligence\
    \ into hardware, and system-on-chip design. Notably, he received the Design Automation\
    \ Conference 2020 Young Fellow (DAC YF 2020) Prize and was a part of the Best\
    \ Team at DAC YF 2020.\n\n![](_page_11_Picture_14.jpeg)\n\nMARKUS BICHL is currently\
    \ with the Technische Universität Wien (TU Wien), Faculty of Electrical Engineering\
    \ and Information Technology (ETIT) as a student. He started his studies with\
    \ Technische Universität Wien (TU Wien), Faculty of Informatics, Bachelore's Programme\
    \ of Computer Science in 2016. He is pursuing his Master's degree in Electrical\
    \ Engineering in the Master's Programme Embedded Systems. His main research interests\
    \ include Emerging\n\nMemory Technologies, low-power computing, FPGA development,\
    \ ASIC design, and cyber-physical systems. Besides his studies, he is working\
    \ on industry-leading electrical powertrains for the automotive industry, with\
    \ hundreds of thousands of units already produced. His passion is to work further\
    \ on Embedded Systems topics and gain a professional career in research.\n\n![](_page_11_Picture_17.jpeg)\n\
    \nASAD AFTAB is a Graduate Student Member, IEEE, and received the B.S. degree\
    \ in Computer Systems engineering from University of Engineering and Technology\
    \ (UET), Peshawar, in 2017 and the M.S. degree in Electrical (Telecommunication\
    \ and Computer Networks) engineering from the National University of Sciences\
    \ and Technology (NUST), Islamabad, in 2021. He is currently pursuing a Ph.D.\
    \ in Electrical engineering at the Technische Universität Wien (TU\n\nWien), Austria.\
    \ His research interests encompass designing both hardware and software-based\
    \ sustainable security techniques for autonomous CPS, which includes researching\
    \ suitable ML algorithms for defence, analyzing various adversarial attacks, and\
    \ exploring innovative defence methods to enhance the resilience of machine learning\
    \ algorithms.\n\n![](_page_11_Picture_20.jpeg)\n\nSEMEEN REHMAN is currently with\
    \ the Technische Universität Wien (TU Wien), as an Assistant Professor. In October\
    \ 2020, she received her Habilitation Degree in the area of Embedded Systems from\
    \ the Faculty of Electrical Engineering and Information Technology, TU Wien. She\
    \ has co-authored one book, multiple book chapters, and more than 60+ publications\
    \ in premier journals and conferences. Her main research interests include dependable\
    \ systems and energy-efficient\n\nembedded system, approximate computing, security,\
    \ IoT/CPS. She has received the CODES+ISSS 2011 and 2015 Best Paper Awards, DATE\
    \ 2017 Best Paper Award Nomination, HiPEAC Paper Awards, DAC Richard Newton Young\
    \ Student Fellow Award, and Research Student Award at the KIT. She served as the\
    \ Topic Track Chair and co-chair at the DATE and ISVLSI conferences from 2020\
    \ and 2023, and has served as the TPC of multiple premier conferences on design\
    \ automation and embedded systems."
  decisions:
    evaluation_prompt: 'Qualified. Reason: The paper includes a section titled "IV.
      RESULTS" that discusses the evaluation of the proposed framework using simulations.
      It mentions the use of SPEC CPU 2017 benchmarks, provides details on the evaluation
      setup, and presents results with figures and tables that show performance and
      energy metrics.'
    related_work_prompt: '- Qualified. Reason: The paper meaningfully engages with
      prior research throughout its text. It includes a comprehensive set of academic
      citations, discusses the context and limitations of existing tools and frameworks,
      and compares its proposed method to previous work, particularly in the "Background
      and Related Works" section.'
    novelty_prompt: 'Qualified. Reason: The paper proposes a novel, extendable STT-RAM
      memory controller design integrated inside the gem5 simulator, which is a new
      method for evaluating STT-RAM systems. The authors claim novelty by stating,
      "We propose HOPE which is an STT-RAM modeling and simulation framework integrated
      into the full system simulator gem5," and they highlight the framework''s unique
      integration approach and its potential to fuel further research and development.
      Additionally, the paper makes clear claims of contribution, such as the introduction
      of a third memory interface tailored specifically for STT-RAM, and the extension
      of the power model in gem5 to support the proposed STT-RAM model.'
    review_only_prompt: 'Qualified. Reason: The paper proposes a novel STT-RAM memory
      controller design integrated into the gem5 simulator, introduces the HOPE framework,
      and presents original contributions such as new methods, experiments, and evaluations.
      It is not merely a summary of existing work.'
- title: 'TOP: Towards Open & Predictable Heterogeneous SoCs'
  abstract: ''
  keywords: '*—Heterogeneous SoC, Cyber-Physical-Systems, Timing Predictable Architectures,
    Open-Source Hardware.'
  document: "# TOP: Towards Open & Predictable Heterogeneous SoCs\n\nLuca Valente,\
    \ Francesco Restuccia, Davide Rossi, *Member, IEEE* Ryan Kastner, *Fellow, IEEE*\
    \ Luca Benini, *Fellow, IEEE*\n\n**Abstract**—Ensuring predictability in modern\
    \ real-time Systems-on-Chip (SoCs) is an increasingly critical concern for many\
    \ application domains such as automotive, robotics, and industrial automation.\
    \ An effective approach involves the modeling and development of hardware components,\
    \ such as interconnects and shared memory resources, to evaluate or enforce their\
    \ deterministic behavior. Unfortunately, these IPs are often closed-source, and\
    \ these studies are limited to the single modules that must later be integrated\
    \ with third-party IPs in more complex SoCs, hindering the precision and scope\
    \ of modeling and compromising the overall predictability. With the coming-of-age\
    \ of open-source instruction set architectures (RISC-V) and hardware, major opportunities\
    \ for changing this status quo are emerging. This study introduces an innovative\
    \ methodology for modeling and analyzing State-of-the-Art (SoA) open-source SoCs\
    \ for low-power cyber-physical systems. Our approach models and analyzes the entire\
    \ set of open-source IPs within these SoCs and then provides a comprehensive analysis\
    \ of the entire architecture. We validate this methodology on a sample heterogenous\
    \ low-power RISC-V architecture through RTL simulation and FPGA implementation,\
    \ minimizing pessimism in bounding the service time of transactions crossing the\
    \ architecture between 28% and 1%, which is considerably lower when compared to\
    \ similar SoA works.\n\n✦\n\n**Index Terms**—Heterogeneous SoC, Cyber-Physical-Systems,\
    \ Timing Predictable Architectures, Open-Source Hardware.\n\n# **1 INTRODUCTION**\n\
    \nThe exponential growth of cyber-physical systems (CPS) (e.g., self-driving cars,\
    \ autonomous robots, ...) and related applications has been fueled by the increase\
    \ in computational capabilities of heterogeneous low-power Systems-on-Chip (SoCs).\
    \ These SoCs are complex computing platforms composed of a set of different hardware\
    \ computing units (e.g., CPUs, hardware accelerators), each tailored to a specific\
    \ target application, sharing a set of resources (memory, sensors) through interconnects\
    \ [\\[1\\]](#page-12-0)–[\\[5\\]](#page-12-1). While integrating multiple computing\
    \ units on the same platform has enabled efficient scale-up of computational capabilities,\
    \ it also poses significant challenges when it comes to assessing their *timing\
    \ predictability*, which is a requirement for CPSs dealing with real-time and\
    \ safety-critical applications: the primary challenge arises from resource contentions\
    \ that emerge when multiple active agents within the SoC must access the same\
    \ shared resources [\\[1\\]](#page-12-0)–[\\[7\\]](#page-12-2).\n\nNumerous research\
    \ efforts have focused on enhancing the timing predictability of heterogeneous\
    \ Systems-on-Chip (SoCs). This includes safely upper bounding execution times\
    \ for data transfers [\\[8\\]](#page-13-0)–[\\[10\\]](#page-13-1) or the deadline\
    \ miss ratio for critical tasks [\\[1\\]](#page-12-0)–[\\[3\\]](#page-12-3), with\
    \ the smallest possible pessimism. These efforts have predominantly focused on\
    \ modeling and analyzing commercial DDR protocols [\\[8\\]](#page-13-0), memory\
    \ IPs [\\[11\\]](#page-13-2), and memory controllers [\\[12\\]](#page-13-3), but\
    \ also predictable interconnects [\\[1\\]](#page-12-0), [\\[4\\]](#page-12-4)\
    \ and on-chip communication protocols [\\[13\\]](#page-13-4). Regrettably, despite\
    \ their value, these studies are scattered, with each one focusing on only one\
    \ of these resources at a time, resulting in being overly pessimistic [\\[5\\\
    ]](#page-12-1).\n\nModeling and analysis of communication protocols are done speculatively\
    \ on abstract models, thus reducing their real-world applicability. Recent works\
    \ for modeling and analysis of IPs (memories, memory controllers, interconnect,\
    \ etc.) have to address the unavailability of cycle-accurate RTL descriptions.\
    \ Many of these IPs are either entirely closed-source [\\[8\\]](#page-13-0) or\
    \ provide loosely-timed behavioral models [\\[5\\]](#page-12-1), [\\[12\\]](#page-13-3)\
    \ or just µarchitectural descriptions [\\[1\\]](#page-12-0), [\\[3\\]](#page-12-3),\
    \ [\\[4\\]](#page-12-4). In essence, the fragmented and proprietary nature of\
    \ commercial and research IPs restricts studies to the particular IP, greatly\
    \ reducing the accuracy achievable through systemlevel analysis. For example,\
    \ Restuccia et al. in [\\[9\\]](#page-13-5) bound the access times of multiple\
    \ initiators on FPGA reading and writing from/to the shared DDR memory. The proposed\
    \ upper bounds' pessimism is between 50% and 90%: even though they finely modeled\
    \ and analyzed the proprietary interconnect, the authors did not have access to\
    \ its RTL nor to the memory controller and IP. The same applies to Ditty [\\[10\\\
    ]](#page-13-1), which is a predictable cache coherence mechanism. In Ditty, even\
    \ though the caches' timing is finely modeled, the overall execution time can\
    \ be up to 3× bigger than the theoretical upper bounds, as the authors did not\
    \ model other components. Another example is AXI-ICRT [\\[1\\]](#page-12-0), an\
    \ advanced AXI interconnect with a sophisticated scheduler which allows transaction\
    \ prioritization based on importance. While proposing a highly advanced interconnect\
    \ with a\n\n<sup>•</sup> *Luca Valente, Luca Benini, and Davide Rossi are with\
    \ the Department of Electrical, Electronic and Information Engineering, University\
    \ of Bologna, 40136 Bologna, Italy. Luca Benini is also with the Integrated Systems\
    \ Laboratory (IIS), ETH Z ¨urich, 8092 Z ¨urich, Switzerland.*\n\n<sup>•</sup>\
    \ *Francesco Restuccia and Ryan Kastner are with the Computer Science and Engineering,\
    \ University of California at San Diego, San Diego, CA 92093 USA.*\n\n*This work\
    \ was supported by Technology Innovation Institute, Secure Systems Research Center,\
    \ Abu Dhabi, UAE, PO Box: 9639, by the Spoke 1 on Future HPC of the Italian Research\
    \ Center on High-Performance Computing, Big Data and Quantum Computing (ICSC)\
    \ funded by MUR Mission 4 - Next Generation EU, and by the European Project EuroHPC\
    \ JU The European Pilot (g.a. 101034126), and by KDT TRISTAN project (g.a.101095947).*\n\
    \n<span id=\"page-1-0\"></span>![](_page_1_Figure_0.jpeg)\n\nFig. 1: Proposed\
    \ methodology.\n\ntightly coupled model, the authors do not extend the model to\
    \ the other components of the SoC, even when assessing the deadline miss ratio\
    \ and benchmarking the architecture.\n\nThe emergence of open-source hardware\
    \ creates a major opportunity for building accurate end-to-end models for realtime\
    \ analysis of cutting-edge heterogeneous low-power SoCs [\\[14\\]](#page-13-6)–[\\\
    [16\\]](#page-13-7): the openness of the IPs allows for cycle-accurate analysis\
    \ of the whole architecture from the interconnects to the shared resources. Yet,\
    \ investigations and successful demonstrations in this direction are still scarce,\
    \ primarily because open hardware has only very recently reached the maturity\
    \ and completeness levels required to build full heterogeneous SoCs [\\[17\\]](#page-13-8).\
    \ In this context, this is the first work to bridge the gap between open-source\
    \ hardware and timing analysis, demonstrating a methodology that successfully\
    \ exploits the availability of the source code to provide finegrained upper bounds\
    \ of the system-level data transfers. We leverage a set of open-source IPs from\
    \ the PULP family, one of the most popular open-hardware platforms proposed by\
    \ the research community [\\[14\\]](#page-13-6), [\\[18\\]](#page-13-9).\n\nFigure\
    \ [1](#page-1-0) shows the proposed methodology, highlighting the novel contributions\
    \ in yellow. It consists of (i) a model for standalone IPs composing modern heterogeneous\
    \ lowpower SoCs, (ii) a static analysis of the RTL code of such components, and\
    \ (iii) a compositional mathematical analysis of the whole system to upper bound\
    \ the response time of the interactions between managers (initiators) and shared\
    \ subordinates (targets), considering the maximum interference generated by the\
    \ interfering managers. Figure [1](#page-1-0) highlights the differences between\
    \ the proposed methodology and previous studies also based on a static and compositional\
    \ approach [\\[5\\]](#page-12-1), [\\[7\\]](#page-12-2), [\\[9\\]](#page-13-5).\
    \ While previous works typically focus on one IP at a time [\\[9\\]](#page-13-5),\
    \ or rely on loosely-timed models [\\[5\\]](#page-12-1), thereby limiting the\
    \ overall accuracy, our approach is the first to model and analyze all the IPs\
    \ directly from the RTL source code to build a holistic system-level analysis.\
    \ This limits the proposed upper bounds' pessimism between 28% and just 1%, in\
    \ isolation and under interference, which is considerably lower when compared\
    \ to similar SoA works for closedsource or loosely-timed platforms [\\[1\\]](#page-12-0)–[\\\
    [4\\]](#page-12-4), [\\[8\\]](#page-13-0), [\\[10\\]](#page-13-1), as better detailed\
    \ in Section [7.](#page-11-0) We demonstrate our methodology on a completely open-source\
    \ prototype of a heterogeneous lowpower open-source SoC for embedded systems composed\
    \ of a Linux-capable host core, a parallel accelerator, a set of IOs, and on-chip\
    \ and off-chip memories. the system-level analysis of the architecture. Finally,\
    \ Section 2-banks 512KiB SPM\n\nThe manuscript is organized as follows: Section\
    \ [2](#page-1-1) presents the target open-source RISC-V-based SoC architecture,\
    \ and Section [3](#page-2-0) discusses the model we apply to its different components.\
    \ Section [4](#page-3-0) analyzes the components to specialize the generic model\
    \ to each of them, and Section [5](#page-7-0) provides\n\n<span id=\"page-1-2\"\
    ></span>![](_page_1_Figure_6.jpeg)\n\nFig. 2: Sample architecture.\n\n[6](#page-9-0)\
    \ validates the results with cycle-accurate experiments (on simulation and FPGA),\
    \ Section [7](#page-11-0) compares this work with the SoA. Section [8](#page-12-5)\
    \ concludes the manuscript.\n\n# <span id=\"page-1-1\"></span>**2 ARCHITECTURE**\n\
    \nFig. [2](#page-1-2) shows the architectural template we target. It also shows\
    \ the four classes of hardware modules we identify in the architecture under analysis,\
    \ namely (i) *controllers*, (ii) the main *crossbar*, (iii) *bridges*, and (iv)\
    \ *peripherals*, which we model in the next Section. The architecture leverages\
    \ a set of fully open-source PULP IPs [\\[18\\]](#page-13-9). It is based on Cheshire\
    \ [\\[15\\]](#page-13-10), an open-source host platform consisting of an RV64\
    \ Linuxcapable CPU, a set of commodity IOs (SPI, SDIO, UART, ...), and an AXI-based\
    \ crossbar with a configurable number of subordinate and manager ports for easy\
    \ integration of accelerators and resources. Our platform includes a parallel\
    \ accelerator and a low-power lightweight HyperBUS memory controller [\\[19\\\
    ]](#page-13-11), connected to the crossbar.\n\nThe host CPU is CVA6 [\\[20\\]](#page-13-12),\
    \ which is a six stages, singleissue, in-order, 64-bit Linux-capable RISC-V core,\
    \ supporting the RV64GC ISA variant, SV39 virtual memory with a dedicated Memory\
    \ Management Unit (MMU), three levels of privilege (Machine, Supervisor, User),\
    \ and PMP [\\[21\\]](#page-13-13). CVA6 features private L1 instruction and caches,\
    \ operating in parallel, with the latter being able to issue multiple transactions.\
    \ When needed, CVA6 can offload computationintensive tasks to the parallel hardware\
    \ accelerator, the socalled PULP cluster [\\[22\\]](#page-13-14). It is built\
    \ around 8 CV32E4-based cores [\\[23\\]](#page-13-15) sharing 16×8 kB SRAM banks,\
    \ composing a 128 kB L1 Scratchpad Memory (SPM). The cluster features a DMA to\
    \ perform data transfers between the private L1SPM and the main memory: data movement\
    \ is performed via softwareprogrammed DMA transfers. Once the data are available\
    \ inside the L1SPM, the accelerator starts the computation.\n\nCVA6 and the cluster\
    \ are the managers of the systems connected to the main AXI crossbar [\\[24\\\
    ]](#page-13-16), which routes their requests to the desired subordinates according\
    \ to the memory map. A manager can access any subordinate in the system. The main\
    \ subordinates of the systems are, respectively, (i) the on-chip SRAM memory,\
    \ (ii) the IO subsystem, and (iii) the off-chip main memory with a tightly coupled\
    \ Last Level Cache (LLC). The on-chip memory is used for low-latency, high-bandwidth\
    \ data storage. The APB subsystem is used to communicate with off-chip sensors\
    \ or memories through the\n\ncommodity IOs. The off-chip main memory is where\
    \ the code and the shared data are stored. Differently from high-end embedded\
    \ systems relying on relatively power-hungry and expensive DDR3/4/5 memories,\
    \ the platform under analysis adopts HyperRAMs as off-chip main memory, which\
    \ are fully-digital low-power small-area DRAMs with less than 14 IO pins and that\
    \ provide enough capacity to boot Linux [\\[16\\]](#page-13-7) and bandwidth for\
    \ IoT applications [\\[19\\]](#page-13-11), [\\[25\\]](#page-13-17).\n\n# <span\
    \ id=\"page-2-0\"></span>**3 MODEL**\n\nThis section presents the model we construct\
    \ for the different components of our SoC. Our aim is to propose a general model\
    \ that describes the characteristics of the components and that can be re-targeted\
    \ to different IPs and novel architectures, regardless of the number of integrated\
    \ controllers and peripherals. This work is also an effort to provide base support\
    \ to stimulate further studies in predictability improvements and analysis for\
    \ open hardware architectures.\n\n#### **3.1 Communication model**\n\nWe identify\
    \ four classes of hardware modules in the architecture under analysis, shown in\
    \ Fig. [2,](#page-1-2) namely (i) *controllers*, (ii) the main *crossbar*, (iii)\
    \ *bridges*, and (iv) *peripherals*. As the AXI standard is the main communication\
    \ standard used to implement non-coherent on-chip communications [\\[24\\]](#page-13-16),\
    \ we discuss here its main features. It defines a managersubordinate interface\
    \ enabling simultaneous, bi-directional data exchange and multiple outstanding\
    \ transactions. Fig. [3](#page-2-1) shows the AXI channel architecture and information\
    \ flow. Bus transactions are initiated by a *controller* (exporting a manager\
    \ interface), submitting a transaction request to read/write data to/from a subordinate\
    \ interface through AR or AW channels, respectively. A request describes the starting\
    \ target address and a *burst length*. After the request phase, in case of a read,\
    \ data are transmitted through the R channel. In case of a write, data are provided\
    \ by the *controller* to the target *peripheral* through the W channel. Upon completing\
    \ a write transaction, the *peripheral* also sends a beat on the B channel to\
    \ acknowledge the transaction's completion. For multiple in-flight write transactions,\
    \ the standard enforces strict in-order access to the W channel: the data on the\
    \ W channel must be propagated in the same order as the AW channel requests. Even\
    \ though the standard does not require it, many commercial and open-source platforms\
    \ apply the same policy for reads, typically to limit the system's overall complexity,\
    \ as reported in their documentation [\\[26\\]](#page-13-18), [\\[27\\]](#page-13-19).\n\
    \n#### **3.2 Controller model**\n\n*Controllers* have an active role on the bus.\
    \ Each *controller* exports an AXI manager interface, through which it initiates\
    \ requests for bus transactions directed to the *peripherals*. A generic *controller*\
    \ C<sup>i</sup> can be described through two parameters: the maximum number of\
    \ outstanding read/write transactions that it can issue in parallel, denoted with\
    \ ϕ C<sup>i</sup> R/W , and their relative burst length β<sup>i</sup> . While\
    \ our model and analysis can be applied to a generic architecture, the system\
    \ under analysis features as *controllers* a CVA6 core [\\[20\\]](#page-13-12)\
    \ and a cluster accelerator [\\[22\\]](#page-13-14) (see Section [2\\)](#page-1-1).\
    \ Bus transactions issued by the cluster interfere with those issued by CVA6\n\
    \n<span id=\"page-2-1\"></span>\n\n| READ TRANSACTION                        \
    \ |                     |  |  |  |  |\n|------------------------------------------|---------------------|--|--|--|--|\n\
    | 1 AR (ADDR+ LEN)                         | SUBORDINATE         |  |  |  |  |\n\
    | 2 R (DATA + COMPL.)                      | INTERFACE           |  |  |  |  |\n\
    | WRITE TRANSACTION<br>1<br>AW (ADDR+ LEN) |                     |  |  |  |  |\n\
    | W (DATA)<br>2                            | SUBORDINATE         |  |  |  |  |\n\
    |                                          | INTERFACE           |  |  |  |  |\n\
    |                                          | 3<br>B (COMPLETION) |  |  |  |  |\n\
    \nFig. 3: AXI Channel architecture\n\nand vice-versa. CVA6 is assumed to compute\
    \ a critical periodic workload, running on top of a Real-time Operating System\
    \ (RTOS). The PULP cluster executes computationintensive tasks and issues bus\
    \ transactions through its DMA. Contention internal to the PULP cluster has been\
    \ profiled in detail in [\\[28\\]](#page-13-20). However, our analysis provides\
    \ the worstcase data transfer time in accessing the shared *peripherals* to support\
    \ the safe scheduling and execution of critical tasks within their deadline. We\
    \ specifically focus on interference in accessing the shared resources. Modeling\
    \ the internal effects of *controllers*, such as pipeline stalls in the core or\
    \ contention within the accelerator, is beyond the scope of this work.\n\n####\
    \ <span id=\"page-2-2\"></span>**3.3 Peripheral model**\n\n*Peripherals* export\
    \ a *subordinate* interface through which they receive and serve the bus transactions.\
    \ The *peripherals* deployed in the system are heterogeneous. Nonetheless, our\
    \ model offers a set of parameters representative of a generic peripheral, and\
    \ it is not tied to a specific communication protocol. It works as the baseline\
    \ for the analysis of any *peripheral* deployed in the system under analysis.\
    \ The generic *peripheral* P<sup>j</sup> is characterized with two sets of parameters:\
    \ (i) the maximum number of supported outstanding reads (χ P<sup>j</sup> <sup>R</sup>\
    \ ) and write (χ P<sup>j</sup> <sup>W</sup> ) transactions; (ii) the maximum number\
    \ of cycles incurred from the reception of the request to its completion, for\
    \ a read (d P<sup>j</sup> <sup>R</sup> ) and a write (d P<sup>j</sup> <sup>W</sup>\
    \ ) transaction in isolation. d P<sup>j</sup> <sup>R</sup> and d P<sup>j</sup>\
    \ <sup>W</sup> are composed of two contributions: (i) the *data time*, defined\
    \ as the time required for the *peripheral* to send or receive one word of data\
    \ (tDATA) multiplied by the burst length of the transaction in service (βi) and\
    \ (ii) the *control overhead* tCTRL, defined as the maximum time elapsing between\
    \ accepting the request and the availability of the first word of data (reads)\
    \ or availability to receive data (writes). From the previous considerations,\
    \ d P<sup>j</sup> R/W = t P<sup>j</sup> CTRL + t P<sup>j</sup> DATA · β. We define\
    \ two extra parameters ρ <sup>P</sup><sup>j</sup> and θ P<sup>j</sup> . The first\
    \ indicates the level of pipelining in serving multiple transactions. ρ <sup>P</sup><sup>j</sup>\
    \ = 1 means that each stage of P<sup>j</sup> does not stall the previous, and\
    \ transactions are served in a pipelined fashion, while ρ <sup>P</sup><sup>j</sup>\
    \ = 0 indicates that no pipeline is implemented. θ <sup>P</sup><sup>j</sup> =\
    \ 0 indicates that read and write transactions interfere with each other. θ <sup>P</sup><sup>j</sup>\
    \ = 1 indicates that read and write transactions can be handled in parallel by\
    \ P<sup>j</sup> .\n\n#### <span id=\"page-2-3\"></span>**3.4 Main crossbar model**\n\
    \nWe provide here the model of the main *crossbar*, the routing component enabling\
    \ communication among *controller*s and *peripheral*s. Each *controller* has its\
    \ manager port connected to a subordinate port of the *crossbar*. Each *peripheral*\
    \ has its subordinate port connected to a manager port of the *crossbar*. We model\
    \ the *crossbar* R<sup>0</sup> with two sets of parameters: (i) the\n\nmaximum\
    \ amount of outstanding read and write transactions that a subordinate port can\
    \ accept (χ R<sup>0</sup> <sup>R</sup> and χ R<sup>0</sup> <sup>W</sup> , respectively);\
    \ and (ii) the maximum overall latency introduced by R<sup>0</sup> on each read\
    \ (d R<sup>0</sup> <sup>R</sup> ) and write transaction (d R<sup>0</sup> <sup>W</sup>\
    \ ). d R<sup>0</sup> R and d R<sup>0</sup> <sup>W</sup> are composed of two contributions:\
    \ (i) the overall delay introduced by the *crossbar* on a transaction in isolation\
    \ (tPROP); (ii) the maximum time a request is delayed at the arbitration stage\
    \ due to the contention generated by interfering transactions (t R<sup>0</sup>\
    \ CON). From the previous considerations, the propagation latency is modeled as\
    \ d R<sup>0</sup> R/W = t R<sup>0</sup> PROP + t R<sup>0</sup> CON. Such parameters\
    \ depend on the arbitration policies and routing mechanisms, as we investigate\
    \ in detail in Section [4.](#page-3-0)\n\n#### <span id=\"page-3-1\"></span>**3.5\
    \ Bridge model**\n\nBridges export a single manager interface and a single subordinate\
    \ interface. They perform protocol/clock conversion between a *controller* and\
    \ the *crossbar*. Bridges require a certain number of clock cycles to be crossed\
    \ but do not limit the number of in-flight transactions and do not create any\
    \ contention. We model the bridges with two parameters: the overall maximum delay\
    \ introduced over a whole transaction for (a) read (d Q<sup>j</sup> <sup>R</sup>\
    \ ) and (b) write (d Q<sup>j</sup> <sup>W</sup> ) transactions.\n\n# <span id=\"\
    page-3-0\"></span>**4 ANALYSIS OF THE HARDWARE MODULES**\n\nThis Section aims\
    \ to analyze the worst-case behavior of the *peripherals*, *bridges*, and the\
    \ *crossbar* present in the platform under analysis. Our approach is compositional\
    \ – in this Section, we analyze each hardware component separately, specializing\
    \ in the generic models introduced in Section [3,](#page-2-0) and bounding the\
    \ service times at the IP level in isolation. In the next Section, we provide\
    \ an overall worst-case analysis at the system level, in isolation and under interference.\
    \ We define t P<sup>j</sup> CK as the period period of the clock fed to P<sup>j</sup>\
    \ .\n\n#### <span id=\"page-3-4\"></span>**4.1 AXI CDC FIFO queues**\n\nAXI CDC\
    \ FIFOs are leveraged to perform clock-domain crossing between two AXI-based devices.\
    \ The generic AXI CDC FIFO F<sup>i</sup> is a *bridge*: we apply here the model\
    \ presented in Section [3.5.](#page-3-1) It exports a manager interface and a\
    \ subordinate interface. It is composed of five independent CDC FIFOs, each serving\
    \ as a buffer for an AXI channel, having depth D<sup>i</sup> CDC (design parameter\
    \ for the IP under analysis).\n\n#### *4.1.1 RTL IP structure*\n\nFigure [4](#page-4-0)\
    \ shows the block diagram of a CDC FIFO in the platform under analysis. They are\
    \ structured following established clock domain crossing (CDC) principles [\\\
    [24\\]](#page-13-16). The design is split into two parts, the transmitter (TX)\
    \ and the receiver (RX), having different clock domains. TX and RX interface through\
    \ asynchronous signals, namely a counter for data synchronization (synchronized\
    \ with two-stage Flip-Flops (FFs)) and the payload data signal.\n\n#### *4.1.2\
    \ Delays analysis*\n\nAs mentioned earlier, CDC FIFOs are *bridges*: we apply\
    \ the model presented in Section [3.5.](#page-3-1) The CDC FIFO under analysis\
    \ behaves as follows: TX samples the payload data into an FF. In the following\
    \ cycle, the TX counter is updated. The TX\n\ncrossing the CDC FIFO introduces\
    \ a fixed delay of one clock cycle of the TX domain (t CK) and four clock cycles\
    \ of the RX domain (t RX CK). This means that the delay in crossing the CDC FIFO\
    \ is equal to tCDC(t TX CK, tRX CK) = t TX CK + 4 · t RX CK.We leverage this baseline\
    \ delay to build the overall latency introduced by F<sup>i</sup> , interposed\
    \ between a manager (clocked at t C CK) and a subordinate (clocked at t P CK).\n\
    \n*Read transaction:* A read transaction AR<sup>k</sup> is composed of two phases:\
    \ (i) the address propagation phase and (ii) the data phase. This means that F<sup>i</sup>\
    \ is crossed twice to complete ARk: during phase (i), the manager is on the TX\
    \ side, propagating the request. In phase (ii), the subordinate is on the TX side,\
    \ propagating the data. Hence, the propagation latency is tCDC(t C CK, t<sup>P</sup>\
    \ CK) in phase (i) and tCDC(t P CK, t<sup>C</sup> CK) in phase (ii). Adding them\
    \ together, the propagation latency introduced by F<sup>i</sup> on AR<sup>k</sup>\
    \ is equal to:\n\n<span id=\"page-3-2\"></span>\n$$d\\_R^{\\rm CDC} = t\\_{\\\
    rm CDC}(t\\_{\\rm CK}^C, t\\_{\\rm CK}^P) + t\\_{\\rm CDC}(t\\_{\\rm CK}^P, t\\\
    _{\\rm CK}^C) = 5(t\\_{\\rm CK}^C + t\\_{\\rm CK}^P) \\tag{1}$$\n\n*Write transaction:*\
    \ A write transaction is composed of three phases: (i) an address phase (manager\
    \ on the TX side), (ii) a data phase (manager on the TX side), and (iii) a write\
    \ response phase (subordinate on the TX side). Phases (i) and (ii) happen in parallel\
    \ (see [\\[29\\]](#page-13-21) p. 45). Thus, tCDC(t C CK, t<sup>P</sup> CK) is\
    \ incurred for phases (i) and (ii), and tCDC(t P CK, t<sup>C</sup> CK) for phase\
    \ (iii). The delay introduced by F<sup>i</sup> on AW<sup>k</sup> is equal to the\
    \ delay introduced in Equation [1,](#page-3-2) d CDC <sup>W</sup> = d CDC <sup>R</sup>\
    \ .\n\n#### <span id=\"page-3-3\"></span>**4.2 AXI SRAM scratchpad memory (SPM)**\n\
    \nThe AXI SPM is a high-speed, low-latency memory component used for temporary\
    \ data storage – a block design representation is reported in Figure [5.](#page-4-0)\
    \ The SPM memory is a *peripheral*: we apply here the model presented in Section\
    \ [3.3.](#page-2-2)\n\n#### *4.2.1 RTL IP structure*\n\nThe first stage of the\
    \ SPM architecture is represented by a protocol converter (AXI-SRAM-Interface),\
    \ translating the read and write AXI channels into SRAM-compatible transactions.\
    \ Following the converter, an internal demux directs the SRAM transactions to\
    \ the desired SRAM bank, where the data is stored. Each SRAM bank provides two\
    \ independent SRAM ports, one for reads and one for writes, as from the specification\
    \ of industry-standard SRAM resources [\\[30\\]](#page-13-22).\n\n*The AXI-SRAM-Interface*\
    \ is structured in two submodules, independently managing read and write transactions.\
    \ The first stage of each submodule is a FIFO queue (of depth DSPM FIFO) buffering\
    \ the AXI AW or AR channel, respectively. Each submodule features the logic for\
    \ protocol translation, consisting of (i) saving transaction metadata (starting\
    \ address and length) and (ii) producing the output SRAM requests. For writes,\
    \ the incoming data on the W channel are directly propagated towards the banks.\
    \ The logic operating the protocol conversion generates the address for each W\
    \ beat. For reads, the data coming from the SRAM banks are directly driven on\
    \ the R channel. The logic keeps compliance with the AXI standard, adding the\
    \ last signal or generating write responses when required. *The demux* is fully\
    \ combinatorial\n\n<span id=\"page-4-0\"></span>![](_page_4_Figure_1.jpeg)\n\n\
    Fig. 4: CDC FIFO block diagram.\n\nFig. 5: AXI SPM block diagram\n\n0 1 N-1 ...\n\
    \nDual-Port SRAM Interface\n\nBank 1\n\nAR FIFO AW FIFO ADDR & REQ CTRL ADDR GEN\
    \ & CTRL\n\nAR R AW B W\n\nSRAM Interface\n\n0 1 N-1 ... Bank 0\n\nDual-Port SRAM\
    \ Interface\n\nCONVERTER\n\nDEMUXING\n\nBANKS\n\n![](_page_4_Figure_4.jpeg)\n\n\
    Fig. 6: IO subsystem block diagram.\n\nand selects the target bank according to\
    \ the request's address. *The SRAM banks* are technology-specific macros instantiated\
    \ at design time. Each SRAM bank's port exports an enable signal, an address signal,\
    \ and a signal to determine if a transaction is a read or a write. The SRAM interface\
    \ expects simultaneous propagation of data and commands for writes; for reads,\
    \ the data are sent the cycle following the command.\n\n#### *4.2.2 Delays and\
    \ parallelism analysis*\n\n*AXI-SRAM-Interface:* the FIFOs in the converter are\
    \ only in charge of data buffering – each FIFO introduces a fixed delay of one\
    \ clock cycle (t SPM CK ). After the FIFOs, the control logic requires at most\
    \ one clock cycle (t SPM CK ) to set up the propagation of a burst transaction\
    \ – the direct connection over the W and R channels makes the data streaming in\
    \ a pipeline fashion, adding no further latency. At the end of a write transaction,\
    \ the converter takes two clock cycles (2t SPM CK ) to generate the write response:\
    \ one to acknowledge that the last W beat has been accepted and one to provide\
    \ the B response. The same applies to reads, to generate the AXI last signal.\
    \ Summing up the contributions, the control latency introduced by the AXI-SRAM-Interface\
    \ to each transaction is upper bound by 4t SPM CK for both reads and writes.\n\
    \n*Demux:* The demultiplexing is combinatorial: it connects the transaction to\
    \ the SRAM bank in one clock cycle (t SPM CK ).\n\n*Banks:* As by the definition\
    \ of the SRAM interface [\\[30\\]](#page-13-22), an SRAM bank serves one transaction\
    \ per clock cycle, which makes t SPM DATA,R/W = t SPM CK . For write transactions,\
    \ the protocol guarantees that the SRAM bank samples the data in parallel with\
    \ the request (in the same clock cycle). For read transactions, the data are served\
    \ the clock cycle after the bank samples the request. So, it contributes to t\
    \ SPM CTRL,R with one clock cycle (t SPM CK ). Summing up the contributions, the\
    \ service time of the SPM in isolation is upper bound by:\n\n$$t\\_{\\rm{CTRL},W}^{\\\
    rm{SPM}} = 5 \\cdot t\\_{\\rm{CK}}^{\\rm{SPM}}; t\\_{\\rm{CTRL},R}^{\\rm{SPM}}\
    \ = 6 \\cdot t\\_{\\rm{CK}}^{\\rm{SPM}}; t\\_{\\rm{DATA},R/W}^{\\rm{SPM}} = t\\\
    _{\\rm{CK}}^{\\rm{SPM}}; \\tag{2}$$\n\nConsider now the parallelism supported\
    \ by the SPM. The maximum number of accepted outstanding transactions at the SPM\
    \ χ SPM <sup>R</sup> is defined by the depth DSPM FIFO of the input buffers implemented\
    \ in the AXI-SRAM-Interface. Thus,\n\n$$\n\\chi\\_R^{\\text{SPM}} = \\chi\\_W^{\\\
    text{SPM}} = D\\_{\\text{FIFO}}^{\\text{SPM}} \\tag{3}\n$$\n\nThe *SPM* module\
    \ under analysis is aggressively pipelined, operations are executed in one clock\
    \ cycle, and no stall sources are present in the design. Also, as mentioned earlier,\
    \ read and write transactions do not interfere with each other. From the previous\
    \ considerations, ρ SPM = 1 and θ SPM = 1.\n\n#### <span id=\"page-4-1\"></span>**4.3\
    \ IO Subsystem**\n\nSRAM Interface\n\nBank B-1\n\n0 1 N-1 ...\n\nInterface\n\n\
    The IO subsystem is the *peripheral* in charge of writing/reading data to/from\
    \ the off-chip I/Os. We apply here the model presented in Section [3.3.](#page-2-2)\
    \ It is composed of a set of memory-mapped peripheral registers that are accessed\
    \ through a demux and that manage the datapaths issuing the transactions on the\
    \ I/O interfaces (e.g., SPI, I2C, etc.).\n\n#### *4.3.1 RTL IP structure*\n\n\
    Figure [6](#page-4-0) shows the block diagram of the IO subsystem. It is composed\
    \ of an AXI-REG-Interface, a demux, and a set of registers. The first stage of\
    \ the *AXI-REG-Interface* is composed of two FIFOs (of depth DIO FIFO), buffering\
    \ read and write transactions, respectively. After the FIFOs, a round-robin arbiter\
    \ manages read and write transactions, allowing only one at a time to pass to\
    \ the protocol conversion. Since the IO subsystem is meant for low-power reads\
    \ and writes, registers' transactions share the same set of signals for reads\
    \ and writes and are limited to single-word accesses. For such a reason, the IO\
    \ subsystem does not support burst transactions (requests having β<sup>i</sup>\
    \ > 1 are suppressed). *The demux* stage decodes the request and directs it to\
    \ the proper register destination, where it is finally served as a register read\
    \ or write.\n\n#### *4.3.2 Delays and parallelism analysis*\n\nThe IO subsystem\
    \ is a *peripheral*, thus, we apply the model proposed in Section [3.5.](#page-3-1)\
    \ Considering the maximum service delays, overall, the IO subsystem is composed\
    \ of four stages: (i) the FIFOs, (ii) the protocol conversion, (iii) demultiplexing,\
    \ and (iv) target register access. The first three stages, contributing to the\
    \ control overhead, introduce a fixed delay of one clock cycle (t IO CK) each\
    \ for a total of 3 · t IO CK clock cycles. Consider now stage (iv). In the case\
    \ of a write, the request and the corresponding data are propagated in parallel\
    \ in one clock cycle. In the case of a read, the register provides the data in\
    \ the clock cycle following the request – t IO CTRL requires one extra clock cycle.\
    \ Summing all the contributions, the service time of the I/O subsystem is upper\
    \ bounded by:\n\n$$t\\_{\\rm{CTRL,W}}^{IO} = 3 \\cdot t\\_{\\rm{CK}}^{IO}; \\\
    quad t\\_{\\rm{CTRL,R}}^{IO} = 4 \\cdot t\\_{\\rm{CK}}^{IO}; \\quad t\\_{\\rm{DATA,W/R}}^{IO}\
    \ = t\\_{\\rm{CK}}^{IO} \\tag{4}$$\n\nConsider now the parallelism. Similarly\
    \ to the SPM module, the IO subsystem is capable of buffering up to DIO FIFO of\
    \ each type in its input FIFO queues. Thus, the maximum number of outstanding\
    \ transactions supported by the IO subsystem is equal to:\n\n$$\n\\chi\\_W^{\\\
    rm IO} = \\chi\\_R^{\\rm IO} = D\\_{\\rm FIFO}^{\\rm IO} \\tag{5}\n$$\n\nThe IO\
    \ subsystem serves read and write transactions one at a time, and no pipelining\
    \ is implemented among the different stages. This means that ρ IO = 0 and θ SPM\
    \ = 0.\n\n<span id=\"page-5-0\"></span>![](_page_5_Figure_0.jpeg)\n\nFig. 7: Block\
    \ diagrams of the components of the main memory subsystem. (a) LLC block diagram,\
    \ (b) Transaction control flow diagram, (c) Memory controller block diagram.\n\
    \n#### <span id=\"page-5-1\"></span>**4.4 The main memory subsystem**\n\nThe main\
    \ memory subsystem is a *peripheral*: we apply here the model presented in Section\
    \ [3.3.](#page-2-2) It is composed of three macro submodules: (i) the *AXI Last-level\
    \ Cache (LLC)*; (ii) the *HyperRAM memory controller (HMC)*; and (iii) the *HyperRAM\
    \ memory (HRAM)*. It is based on HyperRAM memories leveraging the HyperBUS protocol\
    \ [\\[25\\]](#page-13-17). HyperRAMs are optimized for low-overhead data storage\
    \ while offering up to 3.2Gbps bandwidth. HyperRAMs expose a low pin count, a\
    \ fully digital 8-bit double-data-rate (DDR) interface used for commands and data.\
    \ HyperRAMs serve transactions in order, one at a time, as required by the protocol\
    \ [\\[25\\]](#page-13-17). While a pure in-order strategy is simpler than those\
    \ deployed by high-end commercial memory controllers, it is important to note\
    \ that these controllers are typically complex closedsource IPs, making detailed\
    \ analysis extremely challenging. Notably, our analysis is the first to explore\
    \ this level of detail. Furthermore, the memory subsystem under analysis has shown\
    \ to be effective in tape-outs of Linux-capable chips [\\[16\\]](#page-13-7).\
    \ We model the service times of a single transaction in case of an LLC hit and\
    \ miss. By doing so, we provide upper bounds that can be leveraged by future studies\
    \ focusing on LLC interference between different *controllers* at the application\
    \ level. For example, advanced cache management studies for real-time applications\
    \ (e.g., cache coloring) could leverage the upper bounds provided here to bound\
    \ overall task execution times.\n\n#### *4.4.1 RTL IP structure*\n\n*The AXI Last-Level\
    \ Cache* is the interface of the memory subsystem with the platform. The LLC under\
    \ analysis has configurable cache line length, defined as LWLLC. Figure [7\\(](#page-5-0)a)\
    \ shows the LLC's block diagram, composed of 5 pipelined units: (i) burst splitter,\
    \ (ii) hit-miss detection, (iii) eviction/refill, (iv) data read/write, and (v)\
    \ data ways. Figure [7\\(](#page-5-0)b) shows how these units cooperate to serve\
    \ the requests. The burst splitter buffers and splits the incoming AXI requests\
    \ into multiple sub-requests that have the same length of the cache line, and\
    \ it calculates the tags of the sub-transactions. A βi-word AXI burst request\
    \ is split internally into ⌈ βi LWLLC ⌉ requests of length LWLLC. The tags are\
    \ the input to the hitmiss detection unit, which analyzes them to determine if\
    \ any sub-request will be a (a) hit or (b) miss. In case (a), the transaction\
    \ is directed to the read/write unit: if it is a (a.i) read, the read response\
    \ is generated and immediately sent through the AXI subordinate port, completing\
    \ the transaction. In the case of a (a.ii) write, the locally cached value is\
    \ updated, and a write response is generated and sent back to the\n\nAXI interface\
    \ to complete the transaction. In case (b), the transaction is submitted to the\
    \ eviction/refill unit. Refill is performed on every miss and consists of issuing\
    \ a read to the memory controller to fetch the missing data and update the data\
    \ way. Eviction is performed when a cache set is full to free the necessary spot\
    \ before a refill. A Least Recently Used (LRU) algorithm is used in the module\
    \ under analysis.\n\n*The HyperRAM memory controller* [\\[31\\]](#page-13-23)\
    \ is depicted in Figure [7\\(](#page-5-0)c). It consists of two tightly coupled\
    \ modules working in two separated frequency domains: (i) the AXI *front-end*\
    \ and (ii) the *back-end* PHY controller. The front-end handles and converts the\
    \ AXI transactions into data packets for the PHY controller; it runs at the same\
    \ clock as the LLC (t HMC CK ). The back-end features a Finite State Machine (FSM)\
    \ to send/receive the data packets and keep compliance with the HyperBUS protocol\
    \ timings and data flow; it runs at the same clock as the HyperRAMs (t HRAM CK\
    \ ). The back-end handles two off-chip HyperRAMs in parallel, configured with\
    \ interleaved addresses. As each HyperRAM arranges data as 16-bit words, the word\
    \ size of the back-end is DWHYPER = 32 bits.\n\nThe first stage of the front-end\
    \ is composed of two FIFOs buffering incoming AXI read and write requests. Then,\
    \ a serializer solves conflicts among reads and writes, allowing only one AW or\
    \ AR request at a time. Following, three modules translate between AXI and the\
    \ back-end protocol: (i) AXTOPHY, translating the AXI AW or AR requests into commands\
    \ for the back-end; (ii) PHYTOR converting the data words from the back-end into\
    \ AXI read beats for the AXI interface; and (iii) WTOPHY, converting AXI W data\
    \ beats into data words and generating write response at the end of the transaction.\
    \ Three CDC FIFOs are deployed between the AXTOPHY, WTOPHY, and PHYTOR and the\
    \ back-end. The back-end deploys an internal FSM arranging the requests coming\
    \ from the front-end into 48-bit vector requests, as required in the HyperBUS\
    \ protocol, and propagating the data packets to/from the two physical HyperRAM\
    \ memories through two *transceivers* (TRX).\n\n*The HyperRAM memory* is an off-chip\
    \ memory IP [\\[25\\]](#page-13-17). It is provided with a cycle-accurate model,\
    \ fundamental for our analysis purposes [\\[32\\]](#page-13-24). Each HyperRAM\
    \ is organized as an array of 16-bit words and supports one outstanding burst\
    \ transaction, up to 1kB long. As two HyperRAM are interleaved, the overall burst\
    \ can be up to 2kB long [\\[19\\]](#page-13-11).\n\n#### *4.4.2 Delays and parallelism\
    \ analysis*\n\nWe now bound the worst-case service time of the main memory subsystem,\
    \ analyzing its components one at a time. Starting with the LLC, we follow the\
    \ control flow diagram\n\nreported in Figure [7\\(](#page-5-0)b) to guide the\
    \ explanation. The LLC collects the requests incoming to the main memory. Three\
    \ scenarios can happen: (i) LLC cache hit, (ii) LLC cache miss with refill, and\
    \ (iii) LLC cache miss with eviction and refill.\n\nIn case (i), the LLC directly\
    \ manages the request, and no commands are submitted to the HMC. The request proceeds\
    \ through the LLC splitter, hit/miss unit, read/write unit, and data way stages.\
    \ By design, each stage of the LLC requires a fixed number of clock cycles. The\
    \ burst splitter executes in one clock cycle (t LLC CK ). The hit/miss detection\
    \ stage takes two clock cycles (2t LLC CK ): one for tag checking and one to propagate\
    \ the request to the read/write unit or the evict/refill unit. The read/write\
    \ unit requires one clock cycle (t LLC CK ) to route the transaction to the data\
    \ ways. The data ways accept the incoming request in one clock cycle (t LLC CK\
    \ ) to then access the internal SRAM macros (same as the SPM, Section [4.2\\)](#page-3-3).\
    \ The internal SRAM takes one clock cycle to provide the read data (t LLC CK ),\
    \ but no further latency is required on writes. Once it gets the response, the\
    \ read/write unit routes the read channel to the AX interface, whereas it takes\
    \ one clock cycle (t LLC CK ) to generate the write B response at the end. Thus,\
    \ read/write unit and data ways take together three clock cycles (3t LLC CK ).\
    \ Summing up the contributions, the service time in case of a hit is upper bound\
    \ by:\n\n$$t\\_{\\rm{CTRL},\\rm{R}/\\rm{W}}^{\\rm{MS-HIT}} = 6 \\cdot t\\_{\\\
    rm{CK}}^{\\rm{LLC}}; \\quad t\\_{\\rm{DATA},\\rm{R}/\\rm{W}}^{\\rm{MS-HIT}} =\
    \ t\\_{\\rm{CK}}^{\\rm{LLC}}; \\tag{6}$$\n\nConsider now cases (ii) and (iii):\
    \ the eviction and refill stage is also involved, and a read (for refill) and,\
    \ optionally, a write (for eviction) is issued to the main memory. Eviction and\
    \ refill are run in parallel. Each operation performs two steps, each taking one\
    \ clock cycle: (a) generating a transaction for the main memory and (b) generating\
    \ a transaction for the data way. Thus, summing the latency introduced by the\
    \ eviction and refill stage (2t LLC CK ) with the ones from the other stages,\
    \ the LLC's contribution to the overall control time in case of a miss is upper\
    \ bound by:\n\n$$t\\_{\\text{CTRL,R/W}}^{\\text{LLC-MISS}} = t\\_{\\text{CTRL,R/W}}^{\\\
    text{MS-HTT}} + 2t\\_{\\text{CK}}^{\\text{LLC}} \\tag{7}$$\n\nConsider now the\
    \ delay introduced by the HMC on a generic request. Later, we will use it to bound\
    \ the service time for the batch of transactions issued by the LLC. As described\
    \ earlier, the HMC is composed of (a) the frontend, (b) the CDC FIFOs, and (c)\
    \ the back-end. Consider (a): each one of the front-end's submodules takes one\
    \ clock cycle to sample and process the transaction, except for the serializer,\
    \ which takes two. As transactions pass through 4 modules (FIFOs, serializer,\
    \ AXITOPHY, and either WTOPHY or PHYTOR), the overall delay contribution of the\
    \ front-end is equal to 5t HMC CK . Consider now (b): these are the CDC FIFOs\
    \ composing the AXI CDC FIFOs introduced in Section [4.1.](#page-3-4) For writes,\
    \ the transmitter (TX) is the front-end, sending data to the back-end from the\
    \ AXTOPHY and the WTOPHY. As both transfers happen in parallel, the delay introduced\
    \ by the CDC on a write is upper bound by tCDC(t HMC CK , tHRAM CK ). For reads,\
    \ first, the front-end transmits (TX) the AXTOPHY request, and then the back-end\
    \ transmits the data beats: the delay introduced by the CDC on a read is upper\
    \ bound by tCDC(t HMC CK , tHRAM CK ) + tCDC(t HRAM CK , tHMC CK ). Consider now\
    \ (c): the back-end's FSM parses the incoming request into a HyperRAM command\
    \ in one cycle (t HRAM CK ). Following this, an extra cycle is required for the\
    \ data to cross the back-end. Summing up the contributions just described, the\
    \ control time of the HMC on a generic transaction is upper bound by:\n\n$$\\\
    begin{aligned} t\\_{\\rm{CIRL},\\rm{R}}^{\\rm{HMC}} &= 5 \\cdot t\\_{\\rm{CK}}^{\\\
    rm{HMC}} + t\\_{\\rm{CIRC}} (t\\_{\\rm{CK}}^{\\rm{HMC}}, t\\_{\\rm{CK}}^{\\rm{HRM}})\
    \ + t\\_{\\rm{CIRC}} (t\\_{\\rm{CK}}^{\\rm{HRM}}, t\\_{\\rm{CK}}^{\\rm{HMC}})\
    \ + 2 \\cdot t\\_{\\rm{CK}}^{\\rm{HRM}} \\\\ t\\_{\\rm{CIRL},\\rm{W}}^{\\rm{HMC}}\
    \ &= 5 \\cdot t\\_{\\rm{CK}}^{\\rm{HMC}} + t\\_{\\rm{CDC}} (t\\_{\\rm{CK}}^{\\\
    rm{HMC}}, t\\_{\\rm{CK}}^{\\rm{HRM}}) + 2 \\cdot t\\_{\\rm{CK}}^{\\rm{HRM}} \\\
    end{aligned} \\tag{8}$$\n\nConsider now the delays introduced by the HyperRAM\
    \ memories on a generic request. The control overhead time to access the HyperRAM\
    \ memory is defined by the HyperBUS protocol [\\[25\\]](#page-13-17). First, the\
    \ 48-bit HyperRAM command vector is sent over the two memories in 3 · t HRAM CK\
    \ clock cycles, as the HyperBUS command bus is 16 bits. Following, the HyperBUS\
    \ provides a fixed latency for the maximum time to access the first data word,\
    \ accounting for refresh effects and crossing row boundaries. The specifications\
    \ [\\[33\\]](#page-13-25) bound such a delay between 7 and 16 clock cycles. In\
    \ our case, this is set to 12 · t HRAM CK . Thus, the total control latency of\
    \ the HyperRAM memory is upper bound by:\n\n$$t\\_{\\text{CTRL,R/W}}^{\\text{HRAM}}\
    \ = 15 \\cdot t\\_{\\text{CK}}^{\\text{HRAM}} \\tag{9}$$\n\nAt this point, data\
    \ are ready to be propagated. As the AXI domain and the HyperRAM have different\
    \ data widths, the number of cycles to send/receive an AXI word is:\n\n$$t\\_{\\\
    text{DATA,R}/\\text{W}}^{\\text{HRAM}} = DW\\_{\\text{HYPER}} \\cdot \\lceil \\\
    frac{DW\\_{\\text{AXI}}}{DW\\_{\\text{HYPER}}} \\rceil \\cdot t\\_{\\text{CK}}^{\\\
    text{HRAM}} \\tag{10}$$\n\nWe now have all the elements to bound the overall service\
    \ time of the whole main memory subsystem in case of a miss (ii) with refill and\
    \ (iii) eviction and refill. First, we bound the service time to serve a refill\
    \ (read) request. A βi-long transaction is split by the LLC into ⌈βi/LWLLC⌉ subtransactions\
    \ to the memory, each LWLLC-long. Therefore, by multiplying the control time of\
    \ each sub-transaction (t HMC CTRL,R+ t HRAM CTRL,R) by the number of transactions\
    \ issued (⌈ βi LWLLC ⌉), we bound the control time introduced by the memory controller\
    \ and the off-chip memories. To this, we sum the control time of the LLC in case\
    \ of a miss (t MS-MISS CTRL,W/R) and obtain the whole control overhead. The same\
    \ reasoning applies to the data time: the total number of values requested by\
    \ the LLC to the memory will be equal to LWLLC · ⌈ <sup>β</sup><sup>i</sup> LWLLC\
    \ ⌉ and the overall time spent reading LWLLC · ⌈ <sup>β</sup><sup>i</sup> LWLLC\
    \ ⌉t HRAM DATA,R/W. It follows that the time to serve one word is LWLLC βi · ⌈\
    \ <sup>β</sup><sup>i</sup> LWLLC ⌉ · t HRAM DATA,R/W. Summing it with the data\
    \ time of the LLC (t MS-HIT DATA,R/W), we obtain the following upper bounds for\
    \ case (ii):\n\n<span id=\"page-6-0\"></span>\n$$\\begin{split} t\\_{\\text{CTRL,R/W}}^{\\\
    text{MS-MESS-REF}} &= t\\_{\\text{CTRL,R}}^{\\text{LLC-MESS}} + \\left[ \\frac{\\\
    beta\\_i}{LW\\_{\\text{LLC}}} \\right] \\cdot (t\\_{\\text{CTRL,R}}^{\\text{HDAC}}\
    \ + t\\_{\\text{CTRL,R}}^{\\text{HERAM}}); \\\\ t\\_{\\text{DATA,R/W}}^{\\text{MS-MESS-REF}}\
    \ &= t\\_{\\text{DATA,R/W}}^{\\text{MS-HIT}} + \\frac{LW\\_{\\text{LLC}}}{\\beta\\\
    _i} \\cdot \\left[ \\frac{\\beta\\_i}{LW\\_{\\text{LLC}}} \\right] \\cdot t\\\
    _{\\text{DATA,R}}^{\\text{HRAM}}; \\end{split} \\tag{11}$$\n\nIf the eviction\
    \ is also required, ⌈ βi LWLLC ⌉ extra write transactions of length β<sup>i</sup>\
    \ are performed to save the evicted data. Following the same reasoning as earlier,\
    \ this batch of transactions will introduce ⌈ βi LWLLC ⌉(t HMC CTRL,W + t HRAM\
    \ CTRL,W) clock cycles to the control time and LWLLC βi · ⌈ <sup>β</sup><sup>i</sup>\
    \ LWLLC ⌉ ·t HRAM DATA,W to the data time. We sum these numbers to eq. [11](#page-6-0)\
    \ to upper bound the overall control and data time as follows:\n\n$$t\\_{\\text{CTIL,W/R}}^{\\\
    text{MS-MIS-REF-EV}} = t\\_{\\text{CTIL,W/R}}^{\\text{MS-MIS-REF}} + \\left[\\\
    frac{\\beta\\_i}{LW\\_{\\text{LLC}}}\\right] (t\\_{\\text{CTIL,W}}^{\\text{HMCC}}\
    \ + t\\_{\\text{CTIL,W}}^{\\text{HBM}});$$\n\n$$t\\_{\\text{DATA,W/R}}^{\\text{MS-MIS-REF-EV}}\
    \ = t\\_{\\text{DATA,W/R}}^{\\text{MS-MIS-REF}} + \\frac{LW\\_{\\text{LLC}}}{\\\
    beta\\_i} \\cdot \\left[\\frac{\\beta\\_i}{LW\\_{\\text{LLC}}}\\right] \\cdot\
    \ t\\_{\\text{DATA,W}}^{\\text{HBM}};$$\n\nConsider now the parallelism of the\
    \ main memory subsystem. This is defined by the LLC, which acts as an interface\
    \ with the rest of the platform, buffering up to DLLC FIFO read and write transactions.\
    \ This means that the maximum number of supported outstanding transactions is\
    \ as follows:\n\n$$\n\\chi\\_R^{MS} = \\chi\\_W^{MS} = D\\_{\\rm FIFO}^{\\rm LLC}\
    \ \\tag{13}\n$$\n\nThe LLC is pipelined: in the case all the enqueued accesses\
    \ are hits, there is no stalling. However, the memory controller handles only\
    \ one transaction at a time, stalling the preceding ones, and only serves one\
    \ read or one write at a time. Hence, as soon as one access is a miss, ρ MS =\
    \ 0 and θ MS = 0.\n\n#### <span id=\"page-7-2\"></span>**4.5 AXI host crossbar**\n\
    \nThe AXI host crossbar under analysis is a consolidated AXI crossbar already\
    \ validated in multiple silicon tapeouts [\\[16\\]](#page-13-7), [\\[15\\]](#page-13-10),\
    \ [\\[24\\]](#page-13-16). We apply here the generic model for the *crossbar*\
    \ proposed in Section [3.4.](#page-2-3) The crossbar is referred as R0.\n\n####\
    \ *4.5.1 RTL IP structure*\n\nAs detailed in Figure [8,](#page-7-1) the crossbar\
    \ exports a set of input subordinate ports (S) and output manager ports (M). Each\
    \ S port is connected to a demultiplexer, which routes the incoming AW and AR\
    \ requests and W data to the proper destination. Each M port is connected to a\
    \ multiplexer, which (i) arbitrates AW and AR requests directed to the same *peripheral*,\
    \ (ii) connects the selected W channel from the *controller* to the *peripheral*,\
    \ and (iii) routes back the R read data and B write responses. The crossbar under\
    \ analysis can be configured for a fully combinatorial (i.e., decoding and routing\
    \ operations in one clock cycle) or pipelined structure with up to three pipeline\
    \ stages. In the platform under analysis, it is configured to be fully combinatorial.\
    \ ble of granting one AW and one AR request for each clock CUT\n\n#### *4.5.2\
    \ Delays and parallelism analysis*\n\nTo analyze the maximum propagation delays\
    \ introduced by the crossbar, we upper bound the overall latency on a transaction\
    \ by combining the delays introduced on each AXI channel. We provide two upper\
    \ bounds, one for transactions in isolation (i.e., t R<sup>0</sup> PROP,R/W as\
    \ defined in Section [3\\)](#page-2-0) and the other for transactions under contention\
    \ (i.e., t R<sup>0</sup> PROP,R/W+t R<sup>0</sup> CON,R/W as defined in Section\
    \ [3\\)](#page-2-0). We will use both of them in our architectural analysis reported\
    \ in Section [5.](#page-7-0)\n\n*Maximum delays in isolation:* Thanks to the combinatorial\
    \ structure, it is guaranteed by design that a request for a transaction, a data\
    \ word, or a write response crosses the crossbar in one clock cycle (t R<sup>0</sup>\
    \ CK). Consider a whole AXI transaction. For a read transaction, the crossbar\
    \ is crossed twice: on the AR and R AXI channels, respectively. For each AXI write\
    \ transaction, the crossbar is crossed two times: the first time is crossed by\
    \ the AW and W beats (propagated in parallel), and the second time by the B response.\
    \ Thus, the propagation delays in isolation are equal to:\n\n<span id=\"page-7-3\"\
    ></span>\n$$t\\_{\\text{PROP}\\text{R}/\\text{W}}^{R\\_0} = 2 \\cdot t\\_{\\text{CK}}^{R\\\
    _0};\\tag{14}$$\n\n*Maximum delays under contention:* Under contention, multiple\
    \ *controllers* connected to the crossbar can attempt to concurrently send requests\
    \ to the same *peripheral*, generating interference. The arbiters deploy a round-robin\
    \ scheme capa-\n\n<span id=\"page-7-1\"></span>![](_page_7_Figure_12.jpeg)\n\n\
    Fig. 8: AXI Crossbar block diagram\n\ncycle. In the worst-case scenario, the request\
    \ under analysis loses the round-robin and is served last, experiencing a delay\
    \ of MR<sup>0</sup> − 1 clock cycles (with MR<sup>0</sup> the number of *controller*\
    \ capable of interfering with the request under analysis). From the previous considerations,\
    \ the maximum propagation time introduced by the crossbar is upper bound by:\n\
    \n<span id=\"page-7-4\"></span>\n$$t\\_{\\text{CON,R}}^{R\\_0} = t\\_{\\text{CON,W}}^{R\\\
    _0} = M\\_{R\\_0} - 1 \\tag{15}$$\n\nConsider now the parallelism. Concerning\
    \ reads, the crossbar does not keep track of the inflight transactions. To route\
    \ the responses back, it appends information to the AXI ID. Doing so does not\
    \ limit the maximum number of outstanding transactions. The behavior is different\
    \ for writes: AXI enforces a strict in-order execution of write transactions (see\
    \ [\\[29\\]](#page-13-21) p. 98). This requires the crossbar to implement a table\
    \ to know the order of granted transactions. The maximum number of outstanding\
    \ write transactions per S port is limited by the depth of such tables, refereed\
    \ as D R<sup>0</sup> TAB. From the previous consideration: χ R<sup>0</sup> <sup>W</sup>\
    \ = D R<sup>0</sup> TAB. In the architecture under analysis, χ R<sup>0</sup> <sup>W</sup>\
    \ is set to be bigger than the parallelism supported by the *peripherals* so that\
    \ the crossbar does not limit the overall parallelism of the system.\n\n# <span\
    \ id=\"page-7-0\"></span>**5 SYSTEM-LEVEL WORST-CASE RESPONSE TIME ANALYSIS**\n\
    \nThis section introduces our system-level end-to-end analysis to upper bound\
    \ the overall response times of read and write transactions issued by a generic\
    \ *controller* and directed to a generic *peripheral*, considering the maximum\
    \ interference generated by the other *controllers* in the system. Our approach\
    \ is static [\\[34\\]](#page-13-26) and compositional [\\[35\\]](#page-13-27).\
    \ Specifically, we leverage the component-level static analysis introduced in\
    \ Section [4](#page-3-0) to then compose, step-by-step, the system-level worst-case\
    \ service time of transactions traversing the whole architecture.\n\nWe make an\
    \ assumption aligned with the SoA [\\[3\\]](#page-12-3), [\\[4\\]](#page-12-4),\
    \ [\\[8\\]](#page-13-0), [\\[11\\]](#page-13-2), [\\[12\\]](#page-13-3), [\\[36\\\
    ]](#page-13-28) to ensure independence among *peripherals* while not compromising\
    \ the generality of the analysis. It is assumed that multiple outstanding transactions\
    \ of the same type (either read or write) issued by the same *controller* target\
    \ the same *peripheral*: before issuing a transaction targeting a *peripheral*\
    \ P<sup>j</sup> , a *controller* completes the pending transactions of the same\
    \ type targeting a different *peripheral* Pz. Without such an assumption, due\
    \ to the strict ordering imposed by the AXI standard [\\[29\\]](#page-13-21) on\
    \ the W channel, and the structure of some *peripherals* generating interference\
    \ between reads and writes (i.e., ρ <sup>P</sup><sup>j</sup> = 0), transactions\
    \ issued by C<sup>k</sup> and directed to P<sup>j</sup> might interfere with transactions\
    \ issued by C<sup>i</sup> and directed to Pz, if C<sup>i</sup> also issues in\
    \ parallel transactions\n\nto P<sup>j</sup> , and vice-versa. This assumption\
    \ allows us to relax our analysis, removing such pathological cases. It is worth\
    \ noticing that it does not enforce any relationship between read and write transactions.\
    \ Such an assumption can either be enforced at the software level or at the hardware\
    \ level. The results of our analysis can be extended to such corner cases if required.\
    \ We leave this exploration for future works.\n\nThe first step of the analysis\
    \ is to bound the overall response time of a transaction in isolation (Lemma [1\\\
    )](#page-8-0). Secondly, we bound the maximum number of transactions that can\
    \ interfere with a transaction under analysis, either of the same type (e.g.,\
    \ reads interfering with a read, Lemma [2\\)](#page-8-1) or of a different type\
    \ (e.g., write interfering with a read, and vice versa, Lemma [3\\)](#page-8-2).\
    \ Lemma [4](#page-8-3) bounds the maximum temporal delay each interfering transaction\
    \ can delay a transaction under analysis. Finally, Theorem [1](#page-9-1) combines\
    \ the results of all the lemmas to upper bound the overall worstcase response\
    \ time of a transaction under analysis under interference. We report the lemmas\
    \ in a general form. AXi,j can represent either a read or write transaction issued\
    \ by the generic *controller* C<sup>i</sup> and directed to the generic *peripheral*\
    \ P<sup>j</sup> . The *crossbar* is referred to as R0. To make our analysis general,\
    \ we assume that Ψ<sup>j</sup> = [C0, ..., CM−1] is the generic set of interfering\
    \ *controllers* capable of interfering with C<sup>i</sup> issuing transactions\
    \ to P<sup>j</sup> and that that a generic set of *bridges* Θ<sup>i</sup> = {Q0,\
    \ ..., Qq−1} can be present between each *controller* C<sup>i</sup> and the crossbar\
    \ R0. The cardinality of Ψ<sup>j</sup> is referred to as \f Ψ<sup>j</sup> \f and\
    \ corresponds to the number of *controllers* interfering with AXi,j .\n\n<span\
    \ id=\"page-8-0\"></span>**Lemma 1.** *The response time in isolation of* AXi,j\
    \ *is upper bounded by:*\n\n<span id=\"page-8-4\"></span>\n$$d\\_{i,j}^{X} = d\\\
    _{\\mathbb{R}\\mathcal{W}}^{P\\_j} + \\sum\\_{Q\\_l \\in \\Theta\\_{i,j}} d\\\
    _{\\mathbb{R}\\mathcal{W}}^{Q\\_l} + d\\_{\\mathbb{R}\\mathcal{W}}^{R\\_0} \\\
    tag{16}$$\n\n*Proof.* Section [4](#page-3-0) upper bounds the worst-case delays\
    \ in isolation introduced by each component in the platform. According to their\
    \ definition, such delays account for all of the phases of the transaction. The\
    \ components in the platform are independent of each other. Thus, the delay introduced\
    \ by each traversed component is independent of the behavior of the other components.\
    \ It derives that the overall delay incurred in traversing the set of components\
    \ between C<sup>i</sup> and P<sup>j</sup> is upper bounded by the sum of the worst-case\
    \ delays introduced by all of the components in the set. Summing up the maximum\
    \ delay introduced by the target *peripheral* P<sup>j</sup> (d P<sup>j</sup> R/W),\
    \ by the set of traversed *bridges* Θ<sup>i</sup> , and by the *crossbar* R<sup>0</sup>\
    \ (d R<sup>0</sup> R/W), the lemma derives.\n\n<span id=\"page-8-1\"></span>**Lemma\
    \ 2.** *The maximum number of transactions of the same type that can interfere\
    \ with* AXi,j *is upper bounded by:*\n\n$$S\\_{i,j}^X = \\min\\left(\\sum\\_{C\\\
    _y \\in \\Psi\\_j} \\phi\\_X^{C\\_y}, \\chi\\_X^{P\\_j} + \\mid \\Psi\\_j \\mid\
    \ \\right) \\tag{17}$$\n\n*Proof.* The min in the formula has two components.\
    \ As from the AXI standard definition, an interfering *controller* C<sup>k</sup>\
    \ cannot have more than ϕ C<sup>k</sup> <sup>X</sup> pending outstanding transactions.\
    \ This means that summing up the maximum number of outstanding transactions for\
    \ each interfering *controller* in Ψ<sup>j</sup> provides an upper bound on the\
    \ number of transactions of the same type interfering with AXi,j – the left member\n\
    \nof the min derives. From our *peripheral* analysis reported in Section [4,](#page-3-0)\
    \ P<sup>j</sup> and R<sup>0</sup> can limit the maximum amount of transactions\
    \ accepted by the system: P<sup>j</sup> accepts overall at most χ P<sup>j</sup>\
    \ R/W transactions – when such a limit is reached, any further incoming transaction\
    \ directed to P<sup>j</sup> is stalled. After P<sup>j</sup> serves a transaction,\
    \ R<sup>0</sup> restarts forwarding transactions to the *peripheral* following\
    \ a round-robin scheme (see Section [4\\)](#page-3-0). In the worst-case scenario,\
    \ C<sup>i</sup> loses the round-robin arbitration against all of the \f Ψ<sup>j</sup>\
    \ interfering *controllers* in Ψ<sup>j</sup> , each ready to submit an interfering\
    \ request. Summing up the contributions, also χ P<sup>j</sup> <sup>R</sup> + \f\
    \ Ψ<sup>j</sup> \f upper bounds the maximum number of transactions interfering\
    \ with AXi,j – the right member of the min derives. Both of the bounds are valid\
    \ – the minimum between them is an upper bound providing the least pessimism –\
    \ Lemma [2](#page-8-1) derives.\n\n<span id=\"page-8-2\"></span>**Lemma 3.** *The\
    \ maximum number of transactions of a different type (represented here as Y, i.e.,\
    \ write transactions interfering with a read under analysis, and vice versa) interfering\
    \ with* AXi,j *is upper bounded by:*\n\n$$U\\_{i,j}^{Y} = (S\\_{i,j}^{X} + 1)\
    \ \\cdot (1 - \\theta^{P\\_j}) \\tag{18}$$\n\n*Proof.* According to Section [4.5,](#page-7-2)\
    \ R<sup>0</sup> manages transactions of different types independently – thus,\
    \ no interference of this type is generated at the R<sup>0</sup> level. From Section\
    \ [3,](#page-2-0) θ <sup>P</sup><sup>j</sup> = 1 represents the case in which\
    \ the *peripheral* is capable of serving read and write transactions in parallel\
    \ (e.g., the SPM *peripheral*, Section [4.2\\)](#page-3-3). Thus, no interference\
    \ is generated among them – the second equation derives. From Section [3,](#page-2-0)\
    \ θ <sup>P</sup><sup>j</sup> = 0 represents the case in which P<sup>j</sup> does\
    \ not feature parallelism in serving read and write transactions (i.e., also write\
    \ transactions interfere with reads, e.g., main memory subsystem, Section [4.4\\\
    )](#page-5-1). Considering lemma [2,](#page-8-1) at most S X i,j transactions\
    \ of the same type can interfere with AXi,j . With θ <sup>P</sup><sup>j</sup>\
    \ = 0, and assuming a round-robin scheme arbitrating between reads and writes\
    \ at the *peripheral* level, each one of the S X i,j interfering transaction of\
    \ the same type can be preceded by a transaction of the opposite type, which can,\
    \ therefore, create interference. The same applies to AXi,j , which can lose the\
    \ arbitration at the *peripheral* level as well. Summing up the contribution,\
    \ it follows that S X i,j + 1 can overall interfere with AXi,j – the first equation\
    \ derives.\n\n<span id=\"page-8-3\"></span>**Lemma 4.** *The maximum time delay\
    \ that a transaction of any kind* AXk,j *issued by the generic interfering* controller\
    \ C<sup>k</sup> *can cause on* AXi,j *is upper bounded by:*\n\n<span id=\"page-8-5\"\
    ></span>\n$$\n\\Delta\\_{k,j} = d\\_{\\text{R\\\\$}\\mathcal{W}}^{R\\_0} + (1\
    \ - \\rho^{P\\_j}) \\cdot t\\_{\\text{CTRL,R\\\\$}\\mathcal{W}}^{P\\_j} + t\\\
    _{\\text{DATT},\\text{R\\\\$}\\mathcal{W}}^{P\\_j} \\cdot \\beta\\_k \\tag{19}\n\
    $$\n\n*Proof.* In traversing the path between C<sup>k</sup> and P<sup>j</sup>\
    \ , AXk,j shares a portion of the path with AXi,j , i.e., the target *peripheral*\
    \ P<sup>j</sup> and the crossbar R<sup>0</sup> – no *bridges* from Θ<sup>k</sup>\
    \ belongs to the shared path, thus the delay propagation of AXk,j do not contribute\
    \ in delaying AXk,j . Considering the delay generated by AXk,j at R0, this is\
    \ upper bounded by d R<sup>0</sup> R/W in Section [3.4.](#page-2-3) As from Section\
    \ [3.3,](#page-2-2) t P<sup>j</sup> CTRL,R/W + t P<sup>j</sup> DATA,R/W · β<sup>k</sup>\
    \ is the maximum service time of P<sup>j</sup> for the transaction AXk,j and upper\
    \ bounds the maximum temporal delay that AXk,j can cause on AXi,j at P<sup>j</sup>\
    \ . As from the definition of an interfering transaction, AXk,j is served by P<sup>j</sup>\
    \ before AXi,j . As defined by the model in Section [3.3,](#page-2-2) when ρ <sup>P</sup><sup>j</sup>\
    \ = 1, the *peripheral* works in a pipeline fashion. This means that\n\nfor ρ\
    \ <sup>P</sup><sup>j</sup> = 1, the control time t P<sup>j</sup> CTRL,R/W of an\
    \ interfering transaction is pipelined and executed in parallel with the transaction\
    \ under analysis. Differently, when ρ <sup>P</sup><sup>j</sup> = 0, no pipeline\
    \ is implemented, and the control time of the interfering transaction can partially\
    \ or totally interfere with the transaction under analysis. From the previous\
    \ considerations, the contribution (1 − ρ <sup>P</sup><sup>j</sup> )·t P<sup>j</sup>\
    \ CTRL,R/W + t P<sup>j</sup> DATA,R/W · β<sup>k</sup> derives. Summing up the\
    \ contributions, the lemma follows.\n\n<span id=\"page-9-1\"></span>**Theorem\
    \ 1.** *The overall response time of* AXi,j *under the interference generated\
    \ by the other* controllers *in the system is upper bounded by:*\n\n<span id=\"\
    page-9-3\"></span>\n$$H\\_{i,j}^X = d\\_{i,j}^X + (S\\_{i,j}^X + U\\_{i,j}^Y)\
    \ \\cdot \\Delta\\_{k,j} \\tag{20}$$\n\n*Proof.* Summing up the contribution in\
    \ isolation for AXi,j (Lemma [1\\)](#page-8-0) with the sum of the maximum number\
    \ of interfering transactions of the same type (Lemma [2\\)](#page-8-1) and of\
    \ a different type (Lemma [3\\)](#page-8-2) multiplied by the maximum delay generated\
    \ by each interfering transaction (Lemma [4\\)](#page-8-3), Theorem [1](#page-9-1)\
    \ derives.\n\nThe results presented in this Section represent analytical upper\
    \ bounds derived through static code analysis and the formulation of mathematical\
    \ proofs. Section [6](#page-9-0) will validate them through a comprehensive set\
    \ of cycle-accurate experiments and measurements.\n\n# <span id=\"page-9-0\"></span>**6\
    \ EXPERIMENTAL VALIDATION**\n\nThis Section describes the experimental campaign\
    \ we conducted to validate the methodology and models. The aim of the experimental\
    \ campaign is to assess that the results presented in the previous Sections correctly\
    \ upper bound the maximum delays and response times at the component level and\
    \ the architectural level. We follow a hierarchical approach: at first, Section\
    \ [6.1](#page-9-2) aims to validate the results at the component level we proposed\
    \ in Section [4.](#page-3-0) Following, in Section [6.2,](#page-10-0) we experimentally\
    \ validate the system-level analysis we proposed in Section [5.](#page-7-0) The\
    \ experiments are conducted in a simulated environment (leveraging the Siemens\
    \ QuestaSIM simulator) and by deploying the design on an FPGA platform. In the\
    \ simulated experiments, we deploy custom AXI managers for *ad-hoc* traffic generation\
    \ and cycle-accurate performance monitors. The generic custom manager represents\
    \ a generic configurable *controller* C<sup>i</sup> issuing requests for transactions\
    \ – we will refer to that as GC<sup>i</sup> . In the FPGA, we leverage CVA6 and\
    \ the PULP cluster to generate the traffic with synthetic software benchmarks\
    \ and rely on their performance-monitoring registers to collect the measurements.\
    \ The experimental designs are deployed on the AMD-Xilinx VCU118, using the Vitis\
    \ 2022.1 toolchain. Similar State-of-the-Art works upper bounding the execution\
    \ time of a single transaction leverage synthetic benchmarks to measure the worst-case\
    \ access times since generic applications fail to do so [\\[8\\]](#page-13-0)–[\\\
    [10\\]](#page-13-1). For this reason, we concentrate on synthetic benchmarks at\
    \ the IP and the system level.\n\n#### <span id=\"page-9-2\"></span>**6.1 Component-level\
    \ hardware modules**\n\n#### *6.1.1 Delays analysis*\n\nThis subsection presents\
    \ the tests run to measure the worstcase access latency time in isolation for\
    \ the *peripherals* (d P<sup>j</sup> R/W ), 10\n\nthe *crossbar* (d R<sup>0</sup>\
    \ R/W ) and the *bridges* (d Q<sup>j</sup> R/W ) from Section [4.](#page-3-0)\
    \ We connect the generic controller CG<sup>i</sup> to the IP under analysis for\
    \ these experiments. We let CG<sup>i</sup> issue 100'000 transactions, one at\
    \ a time, with random burst length (βi). We monitor the service times and then\
    \ pick the longest ones for different β<sup>i</sup> .\n\nFigure [9](#page-10-1)\
    \ compares the maximum measured experimental delays with the upper bound proposed\
    \ in Section [4.](#page-3-0) Figure [9\\(](#page-10-1)a) reports the maximum service\
    \ time of the main memory subsystem in case of a miss as a function of the burst\
    \ length of the transaction under analysis, either when (i) only a refill is necessary\
    \ and (ii) both refill and eviction are necessary, compared with the bounds proposed\
    \ in Section [4.4.](#page-5-1) The measured service times are lower than the bounds.\
    \ The pessimism is between 3% and 10.1%; the larger β<sup>i</sup> , the higher\
    \ the pessimism. Higher pessimism on longer transactions is due to the internal\
    \ splitting at the LLC. As from our analysis, the memory subsystem is not fully\
    \ pipelined (ρMS = 0). However, in practice, the control and data phases of consecutive\
    \ sub-transactions might be partially served in parallel by the LLC and the memory\
    \ controller. This means that the longer the transaction, the higher the number\
    \ of sub-transactions and their overlap, and the lower the service time compared\
    \ to our model. Thus, the pessimism increases. Figure [9\\(](#page-10-1)b) reports\
    \ the measured results on the main memory subsystem but in case of a hit, compared\
    \ with the bounds proposed in Section [4.4.](#page-5-1) As we consider an LLC\
    \ hit, the access to the HyperRAM is not performed: this test analyzes the service\
    \ time of the LLC. Our bounds are always upper bounds for the maximum measured\
    \ results. The trend here is reversed w.r.t. Figure [9\\(](#page-10-1)a) – as\
    \ β<sup>i</sup> increases, the relative pessimism decreases from 7.7% down to\
    \ 0.4%. In this case, the source of the pessimism comes only from the control\
    \ time, which does not depend on β<sup>i</sup> , while there is no pessimism on\
    \ the data time. Hence, this pessimism gets amortized as the burst length and\
    \ the overall service time increase. We conduct the same experimental campaign\
    \ also on the AXI SPM – the measured results, compared with the bounds proposed\
    \ in Section [4.2,](#page-3-3) are reported in Figure [9\\(](#page-10-1)c). The\
    \ trends are similar to the ones reported in Figure [9\\(](#page-10-1)b) for LLC\
    \ hits – the pessimism of our analysis is limited to 1 and 2 clock cycles for\
    \ reads and writes on the control time, respectively. As in the case of the LLC\
    \ HITs, the upper bound on the control overhead gets amortized for longer transactions,\
    \ and the pessimism reduces from 8.8% to 0.5%.\n\nFigure [9\\(](#page-10-1)d)\
    \ reports the maximum measured latency to cross an AXI CDC FIFO as a function\
    \ of the manager clock period (the subordinate clock period is fixed to 30 ns)\
    \ and compared with the bounds proposed in Section [4.1.](#page-3-4) The results\
    \ are independent of the length of the transaction. To stimulate the highest variability,\
    \ the phases of the clocks are randomly selected on a uniform distribution. The\
    \ first bar reports the crossing delays from the manager to the subordinate side,\
    \ corresponding to the delays introduced on the AW, W, and AR AXI channels. The\
    \ second bar reports the crossing delays from the subordinate to the manager side,\
    \ corresponding to the overall delays on the AXI R and B channels. The third bar\
    \ shows the overall delay on a complete transaction, corresponding to the sum\
    \ of the two previously introduced contributions (see Section [4.1\\)](#page-3-4).\
    \ The pessimism of our bounds is, at most, one clock cycle of the slowest clock\
    \ between manager and subordinate.\n\n<span id=\"page-10-1\"></span>![](_page_10_Figure_0.jpeg)\n\
    \nFig. 9: Services time in isolation.\n\nFigure [9\\(](#page-10-1)e) reports the\
    \ measured propagation delays introduced by the crossbar over an entire write\
    \ and read transaction, compared with the bounds of Section [4.5,](#page-7-2)\
    \ varying the number of *controllers*. As explained in Section [4.5,](#page-7-2)\
    \ the propagation delay is the sum of the propagation latency without interference\
    \ (eq. [14\\)](#page-7-3) and the additional contention latency (eq. [15\\)](#page-7-4),\
    \ which depends on the number of *controllers*. Thanks to the simplicity of the\
    \ arbitration operated by the crossbar (pure round-robin), our proposed bounds\
    \ exactly match the measurements. We conducted the experimental campaign also\
    \ on the IO subsystem. We measured the maximum service time and compared it with\
    \ the upper bounds of Section [4.3,](#page-4-1) which we do not show for space\
    \ reasons: such IP supports only single-word transactions. Our upper bounds exceed\
    \ the maximum measured service time with pessimism of down to 2 clock cycles,\
    \ with service times of 4 (write) and 5 (read) clock cycles.\n\n#### *6.1.2 Parallelism*\n\
    \nWe also demonstrate our analysis of parallelism of the *peripherals* (χ P<sup>j</sup>\
    \ R/W ) and the *crossbar* (χ R<sup>0</sup> R/W ) analyzed in Section [4.](#page-3-0)\
    \ To do so, we configured CG<sup>i</sup> to issue unlimited outstanding transactions\
    \ to the *peripheral* under test. In parallel, we monitor the maximum number of\
    \ accepted outstanding transactions. Our measurements match our analysis: the\
    \ maximum number of outstanding transactions is defined by the maximum parallelism\
    \ accepted at the input stage of the peripherals and the crossbar.\n\n#### <span\
    \ id=\"page-10-0\"></span>**6.2 System-level experiments**\n\nWhile the previous\
    \ experiments focused on the evaluation at the IP level, this set of experiments\
    \ aims to evaluate the system-level bounds proposed in Section [5.](#page-7-0)\
    \ We first validate our analysis in simulation. We developed a System Verilog\
    \ testbench with two configurable AXI synthetic *controllers* CG<sup>i</sup> connected\
    \ to the target architecture (see Figure [2\\)](#page-1-2) stimulating overload\
    \ conditions to highlight worst-case scenarios. We also validate our results on\
    \ FPGA, generating traffic with CVA6 and the PULP cluster.\n\nAt first, we evaluate\
    \ the results in isolation *at the system level* as a function of the burst length,\
    \ leveraging the same strategy used for the previous experiments. Namely, these\
    \ tests are meant to validate Lemma [1](#page-8-0) (eq. [16\\)](#page-8-4). To\
    \ measure the service time at the system level in isolation, we let one GC<sup>i</sup>\
    \ issue 100'000 transactions, one at a time, with different β<sup>i</sup> , while\
    \ the other GC<sup>k</sup> is inactive. We monitor the service times and then\
    \ pick the longest ones for each\n\nβi . Figures [10](#page-11-1) (a) and (b)\
    \ report the maximum measured system-level response times in isolation for completing\
    \ a transaction issued by the generic *controller* GC<sup>i</sup> and directed\
    \ to (a) the main memory subsystem (case of cache miss, causing either refill\
    \ or both refill and eviction) and (b) to the SPM memory, compared with the bounds\
    \ proposed in Lemma [1.](#page-8-0) The measured service times are smaller than\
    \ the bounds in all the tested scenarios. The measure and the trends reported\
    \ in Figure [10\\(](#page-11-1)a) are aligned with the ones found at the IP level\
    \ and reported in Figure [9\\(](#page-10-1)a). This is because the overhead introduced\
    \ by the crossbar (in isolation) and the CDC FIFOs is negligible compared to the\
    \ memory subsystem's service time. Figure [10\\(](#page-11-1)b) shows a trend\
    \ aligned with the results at the IP-level reported in Figure [9\\(](#page-10-1)c):\
    \ the lower β<sup>i</sup> , the higher the pessimism. It is worth mentioning that\
    \ the analysis shows higher pessimism at the system level than at the IP level.\
    \ This is due to the extra pessimism from the crossbar and the CDC, which is nevertheless\
    \ amortized on longer transactions, down to 1.9%.\n\nWe now analyze the results\
    \ under maximum interference, to verify the results of Lemma [2](#page-8-1) and\
    \ [3](#page-8-2) and Theorem [1.](#page-9-1) For these tests, the execution of\
    \ GC<sup>i</sup> (100'000 transactions, one at a time) receives interference by\
    \ *controller* GCk. β<sup>k</sup> is fixed and equal to β<sup>i</sup> , while\
    \ we vary the amount of outstanding transactions GC<sup>k</sup> can issue (ϕ CG<sup>k</sup>\
    \ R/W ). Figures [10](#page-11-1) (c), (d), and (e) report the maximum measured\
    \ systemlevel response times for completing a transaction issued by the generic\
    \ *controller* GC<sup>i</sup> and directed to (c) the main memory with an LLC\
    \ miss considering β<sup>i</sup> = 16, (d) the SPM memory, considering β<sup>i</sup>\
    \ = 16, and (e) the SPM memory, considering β<sup>i</sup> = 256, and compare them\
    \ with the upper bounds proposed in equation [20.](#page-9-3) Figures [10](#page-11-1)\
    \ (c), (d), and (e) verify the results of Lemma [2](#page-8-1) and [3:](#page-8-2)\
    \ when ϕ CG<sup>k</sup> R/W > χMS R/W (two bars on the right), the total service\
    \ time is defined by the parallelism of the peripheral itself – as expected, after\
    \ saturating the number of interfering transactions accepted by the peripheral,\
    \ the measured results are the same regardless of the increase of ϕ CG<sup>k</sup>\
    \ R/W . Differently, when ϕ CG<sup>k</sup> R/W ≤ χMS R/W , a reduced value of\
    \ ϕ CG<sup>k</sup> R/W corresponds to lower interference and response times. Figure\
    \ [10\\(](#page-11-1)c) refers to the case of an LLC miss under interference when\
    \ β<sup>k</sup> = 16. The results confirm the safeness of our analysis, which\
    \ correctly upper bounds the overall response times with a pessimism around 15%,\
    \ which is slightly higher than the pessimism of a transaction in isolation at\
    \ the system level. As explained in the previous subsection, when multiple transactions\
    \ are enqueued, the memory subsystem can partially serve their data and control\n\
    \n12\n\n<span id=\"page-11-1\"></span>![](_page_11_Figure_1.jpeg)\n\nFig. 10:\
    \ Services times under interference.\n\nphases in parallel. However, our model\
    \ only allows ρMS = 1 or ρMS = 0, i.e., either the *peripheral* is fully pipelined\
    \ or not pipelined at all. Since ρMS = 0, the pessimism is slightly higher when\
    \ more transactions are enqueued (and partially served in parallel) as equation\
    \ [19](#page-8-5) counts the service time of a transaction fully when ρMS = 0.\
    \ Varying β<sup>k</sup> of GC<sup>k</sup> gives comparable results – we do not\
    \ report such results for briefness and lack of space. We provide two charts for\
    \ the SPM, in Figure [10\\(](#page-11-1)d) and Figure [10\\(](#page-11-1)e). The\
    \ comparison of the two charts highlights how the interfering transactions' length\
    \ impacts the analysis's pessimism, ranging between 19.7% for β = 16 to 1% for\
    \ β = 256. The trend here is aligned with the service time at the system level\
    \ in isolation: the pessimism comes from the control times of SPM and propagation\
    \ latency of the crossbar and the CDC, which are amortized as the data time increases\
    \ with βk.\n\n#### **6.3 Discussion**\n\nIn this Section, we validated the analysis\
    \ of Sections [4](#page-3-0) and [5](#page-7-0) through an extensive set of tests.\
    \ We demonstrated how the proposed approach enables detailed explanations of the\
    \ analysis's pessimism and facilitates iterative refinement. This allows us to\
    \ derive upper bounds that are safe yet not overly pessimistic, particularly when\
    \ compared to similar stateof-the-art works based on closed-source or loosely-timed\
    \ IPs. Nevertheless, while the methodology is promising, the resulting analysis\
    \ may seem limited in comparison to other works that model more sophisticated\
    \ closed-source IPs. Here, we discuss the limitations of our analysis, focusing\
    \ on its dependence on the underlying characteristics of the available open-source\
    \ hardware.\n\nIt is noteworthy how the analysis leverages the roundrobin policy\
    \ of the main interconnect and the in-order nature of *peripherals* in Lemmas\
    \ [2](#page-8-1) and [3.](#page-8-2) The absence of internal reordering allows\
    \ to derive the number of transactions preceding the one under interference directly\
    \ from the arbitration policy. As long as the *peripherals* serve the transactions\
    \ in order, extending the analysis to support other arbitration policies is expected\
    \ to require minimal effort. Instead, supporting *peripherals* with internal transaction\
    \ reordering can lead to *timing anomalies* [\\[7\\]](#page-12-2) and make the\
    \ proposed model unsafe, as previously demonstrated in [\\[5\\]](#page-12-1).\
    \ Our analysis focuses on the available *peripherals* within the target architecture,\
    \ as out-of-order *peripherals* are not available open-source to us. We envision\
    \ expanding the\n\nanalysis to match higher-performance platforms as opensource\
    \ hardware evolves.\n\nLastly, it is important to note that the analysis bounds\
    \ only a single transaction issued by C<sup>i</sup> – this limitation is not imposed\
    \ on the interfering controllers. Lemma [2](#page-8-1) does not consider C<sup>i</sup>\
    \ to have more pending transactions, except for the ones already accepted by P<sup>j</sup>\
    \ . In other words, Lemma [2](#page-8-1) assumes that there is not a queue of\
    \ transactions buffered in the *bridges* between C<sup>i</sup> and R0, which could\
    \ exist when P<sup>j</sup> is full. We could potentially extend the model to define\
    \ a batch of enqueued transactions and then modify Lemma [2](#page-8-1) to analyze\
    \ this scenario. Such an extension would further build upon the proposed model\
    \ and analysis, which is limited to bound the access time of a single transaction.\n\
    \n# <span id=\"page-11-0\"></span>**7 RELATED WORK**\n\nIn this Section, we provide\
    \ a thorough comparison with previous works focusing on enhancing the timing predictability\
    \ of digital circuits. Traditionally, the majority of these works leverage commercial\
    \ off-the-shelf devices [\\[34\\]](#page-13-26), [\\[38\\]](#page-13-29) or predictable\
    \ architectures modeled with a mix of cycleaccurate and behavioral simulators\
    \ [\\[39\\]](#page-13-30). Also, they focus on bounding the execution times for\
    \ predefined specific software tasks rather than the individual transaction service\
    \ times [\\[7\\]](#page-12-2), [\\[38\\]](#page-13-29)–[\\[40\\]](#page-13-31).\
    \ Furthermore, they build the models from dynamic experiments rather than from\
    \ static analysis, largely due to the dearth of detailed hardware specifications\
    \ [\\[35\\]](#page-13-27), limiting the generality of their approach. More recent\
    \ works advocate for static modeling and analysis of protocols [\\[8\\]](#page-13-0),\
    \ [\\[13\\]](#page-13-4), interconnect [\\[1\\]](#page-12-0), [\\[3\\]](#page-12-3),\
    \ [\\[9\\]](#page-13-5), and shared memory resources [\\[5\\]](#page-12-1), [\\\
    [10\\]](#page-13-1) to provide more generic and comprehensive models. While their\
    \ value is undeniable, due to the unavailability of the source RTL, each one focuses\
    \ on only one of these resources, resulting in a significant penalty to the pessimism\
    \ of the upper bounds [\\[5\\]](#page-12-1). Our work breaks from this convention,\
    \ presenting a holistic static model of an entire open-source architecture rigorously\
    \ validated through RTL cycle-accurate simulation and FPGA emulation. As Table\
    \ [1](#page-12-6) shows, this is the first work to analyze and model the open-source\
    \ siliconproven RTL of all the IPs composing a whole SoC to build the least pessimistic\
    \ upper bounds for data transfers within the architecture when compared to similar\
    \ SoA works.\n\nBiondi et al. [\\[13\\]](#page-13-4) developed a model of the\
    \ memory-access regulation mechanisms in the ARM MPAM and provided detailed instantiations\
    \ of such mechanisms, which they\n\n<span id=\"page-12-6\"></span>\n\n|  |  |\
    \  |  |  |  |  |  |  |  |  |  |  |  |  | TABLE 1: Comparison with State-of-the-Art\
    \ works for predictability. IC = Interconnect. DMR = Deadline miss ratio. |  |\
    \  |  |  |  |\n|--|--|--|--|--|--|--|--|--|--|--|--|--|--|--|-------------------------------------------------------------------------------------------------------------------|--|--|--|--|--|\n\
    |--|--|--|--|--|--|--|--|--|--|--|--|--|--|--|-------------------------------------------------------------------------------------------------------------------|--|--|--|--|--|\n\
    \n|                       | Target                             | Analysis on \
    \                   | Pessimism   | Technology  | Open RTL |\n|-----------------------|------------------------------------|--------------------------------|-------------|-------------|----------|\n\
    | Biondi et. al. [13]   | ARM MPAM Protocol                  | Protocol specs\
    \ (Model)         | No HW       | ✗           | ✗        |\n| Hassan et. al. [8]\
    \    | JEDEC DDR3 Protocol                | Protocol specs (Model)         | 0%\
    \ − 200%   | ✗           | ✗        |\n| Abdelhalim et.al. [5] | Whole mem. hier.\
    \                   | IPs & System (C++ Model)       | 16% − 50%   | ✗       \
    \    | ✗        |\n| BlueScale [3]         | Hier. mem. IC                   \
    \   | IC uArch (Black-box)           | DMR         | FPGA        | ✗        |\n\
    | AXI-RT-IC [1]         | AXI SoC IC                         | IC uArch (Black-box)\
    \           | DMR         | FPGA        | ✗        |\n| Restuccia et. al. [9]\
    \ | AXI Hier. mem. IC                  | IC uArch (Black-box)           | 50%\
    \ − 90%   | FPGA        | ✗        |\n| AXI-REALM [37]        | AXI traffic regulator\
    \              | No analysis                    | No model    | FPGA & ASIC |\
    \ ✓        |\n| Ditty [10]            | Cache coher. mechanism             | IP\
    \ (Fine-grained RTL)          | 100% − 200% | FPGA        | ✓        |\n| This\
    \ Work             | SoC IC, peripherals & system-level | IP & System (Fine-grained\
    \ RTL) | 1% − 28%    | FPGA & ASIC | ✓        |\n\nthen evaluated with IBM CPLEX,\
    \ a decision optimization software for solving complex optimization models. While\
    \ elegant, this approach is not validated on hardware and, therefore, is limited\
    \ in terms of applicability and precision. A more practical and adopted approach\
    \ is the one proposed by Hassan and Pellizzoni [\\[8\\]](#page-13-0). The authors\
    \ develop a finegrained model of the JEDEC DDR3 protocol, validated with MCsim\
    \ [\\[12\\]](#page-13-3), a cycle-accurate C++ memory controller simulator. Unfortunately,\
    \ not having access to the RTL prevents fine-grained modeling and analysis and\
    \ mandates over-provisioning, strongly impacting the overall pessimism of the\
    \ system, which can be as high as 200%. Abdelhalim et al. in [\\[5\\]](#page-12-1)\
    \ present a study bounding the access times of memory requests traversing the\
    \ entire memory hierarchy and propose µarchitectural modifications to the arbiters\
    \ in such hierarchy. Their modifications result in very low pessimism (down to\
    \ 16%) on synthetic and real-world benchmarks. However, the results are validated\
    \ on C++ models of the cores, interconnect, and memory controllers, not RTL code\
    \ targeting silicon implementation.\n\nMore recently, different researchers proposed\
    \ models of hardware IPs that they could validate through cycle-accurate experiments\
    \ [\\[1\\]](#page-12-0), [\\[4\\]](#page-12-4), [\\[9\\]](#page-13-5). In [\\\
    [9\\]](#page-13-5), Restuccia et al. focused on upper bounding the response times\
    \ of AXI bus transactions on FPGA SoCs through the modeling and analysis of generic\
    \ hierarchical interconnects arbitrating the accesses of multiple hardware accelerators\
    \ towards a shared DDR memory. In this work, the interconnect under analysis is\
    \ a proprietary Xilinx IP, which had to be treated as a black box. Also, due to\
    \ the unavailability of the RTL code, the authors did not model the other IPs\
    \ composing the target platform, limiting the precision of the proposed upper\
    \ bounds, which achieve a pessimism between 50% and 90%. Jiang et al. modeled,\
    \ analyzed, and developed AXI-ICRT [\\[1\\]](#page-12-0) and Bluescale [\\[3\\\
    ]](#page-12-3), two sophisticated interconnects providing predictability features\
    \ and coming with a comprehensive model. However, the model and analysis proposed\
    \ in AXI-ICRT [\\[1\\]](#page-12-0), and Bluescale [\\[3\\]](#page-12-3) are not\
    \ as fine-grained as ours: the authors do not provide upper bounds of the access\
    \ times but rather focus on the deadline miss ratio given a fixed workload for\
    \ the different controllers in the system. Moreover, the authors do not provide\
    \ the RTL of such solutions. AXI-REALM [\\[37\\]](#page-13-32) proposes completely\
    \ open-source IPs supporting predictable communications. However, it misses a\
    \ holistic model and analysis. In Ditty [\\[10\\]](#page-13-1), researchers propose\
    \ an open-source predictable directory-based cache coherence mechanism for multicore\
    \ safety-critical systems that guarantees a worst-case latency (WCL) on data accesses\
    \ with almost cycle-accurate precision. However, Ditty's model only covers the\
    \ coherency protocol latency and the core subsystem, overlooking systemlevel analysis\
    \ and achieving very pessimistic boundaries. In this landscape, it emerges clearly\
    \ that our work is the first one covering both modeling and analysis of the interconnects\
    \ and the shared memory resources, with an in-depth analysis of silicon-proven\
    \ open-source RTL IPs and achieving the lowest pessimism when compared to similar\
    \ SoA works.\n\n# <span id=\"page-12-5\"></span>**8 CONCLUSIONS**\n\nIn conclusion,\
    \ this is the first work to bridge the gap between open-source hardware and predictability\
    \ modeling and analysis. It presented (i) a fine-grained model and analysis for\
    \ the typical building blocks composing modern heterogeneous low-power SoCs directly\
    \ based on the source RTL, and (ii) a full mathematical analysis to upper bound\
    \ data transfer execution times. Namely, we demonstrated a methodology that successfully\
    \ exploits the availability of the source code to provide safe, but not overly\
    \ pessimistic, upper bounds for the execution times of data transfers when compared\
    \ to similar SoA works based on closed-source IPs.\n\nAs discussed in Section\
    \ [6,](#page-9-0) after this thorough evaluation, we envision extending our results\
    \ to other popular opensource IPs and different arbitration policies. To hopefully\
    \ stimulate novel research contributions, we open-source a guide to replicate\
    \ the results shown in Section [6](#page-9-0) at [https:](https://github.com/pulp-platform/soc_model_rt_analysis)\
    \ [//github.com/pulp-platform/soc](https://github.com/pulp-platform/soc_model_rt_analysis)\
    \ model rt analysis, comprehensive of the simulated environment and the software\
    \ benchmarks to run on a sophisticated Cheshire-based SoC targeting automotive\
    \ applications.\n\n# **REFERENCES**\n\n- <span id=\"page-12-0\"></span>[1] Z.\
    \ Jiang *et al.*, \"AXI-ICRT RT : Towards a Real-Time AXI-Interconnect for Highly\
    \ Integrated SoCs,\" *IEEE Transactions on Computers*, vol. 72, no. 3, pp. 786–799,\
    \ 2022.\n- [2] A. Biondi *et al.*, \"SPHERE: A multi-SoC architecture for nextgeneration\
    \ cyber-physical systems based on heterogeneous platforms,\" *IEEE Access*, vol.\
    \ 9, pp. 75 446–75 459, 2021.\n- <span id=\"page-12-3\"></span>[3] Z. Jiang *et\
    \ al.*, \"BlueScale: a scalable memory architecture for predictable real-time\
    \ computing on highly integrated SoCs,\" in *Proceedings of the 59th ACM/IEEE\
    \ Design Automation Conference*, 2022, pp. 1261–1266.\n- <span id=\"page-12-4\"\
    ></span>[4] F. Restuccia *et al.*, \"AXI HyperConnect: A Predictable, Hypervisorlevel\
    \ Interconnect for Hardware Accelerators in FPGA SoC,\" in *2020 57th ACM/IEEE\
    \ Design Automation Conference (DAC)*, 2020.\n- <span id=\"page-12-1\"></span>[5]\
    \ S. Abdelhalim *et al.*, \"A Tight Holistic Memory Latency Bound Through Coordinated\
    \ Management of Memory Resources,\" in *35th Euromicro Conference on Real-Time\
    \ Systems (ECRTS 2023)*, vol. 262, 2023, pp. 17:1–17:25.\n- [6] G. Fernandez *et\
    \ al.*, \"Contention in multicore hardware shared resources: Understanding of\
    \ the state of the art,\" in *Proceedings of the 14th International Workshop on\
    \ Worst-Case Execution Time Analysis (WCET 2014)*, 2014, pp. 31–42.\n- <span id=\"\
    page-12-2\"></span>[7] S. Hahn, M. Jacobs, and J. Reineke, \"Enabling Compositionality\
    \ for Multicore Timing Analysis,\" in *Proceedings of the 24th International Conference\
    \ on Real-Time Networks and Systems*. Association for Computing Machinery, 2016,\
    \ p. 299–308.\n- <span id=\"page-13-0\"></span>[8] M. Hassan and R. Pellizzoni,\
    \ \"Bounding DRAM Interference in COTS Heterogeneous MPSoCs for Mixed Criticality\
    \ Systems,\" *IEEE Transactions on Computer-Aided Design of Integrated Circuits\
    \ and Systems*, vol. 37, no. 11, pp. 2323–2336, 2018.\n- <span id=\"page-13-5\"\
    ></span>[9] F. Restuccia *et al.*, \"Bounding Memory Access Times in Multi-Accelerator\
    \ Architectures on FPGA SoCs,\" *IEEE Transactions on Computers*, vol. 72, no.\
    \ 1, pp. 154–167, 2022.\n- <span id=\"page-13-1\"></span>[10] Z. Wu, M. Bekmyrza,\
    \ N. Kapre, and H. Patel, \"Ditty: Directorybased Cache Coherence for Multicore\
    \ Safety-critical Systems,\" in *2023 Design, Automation & Test in Europe Conference\
    \ & Exhibition (DATE)*. IEEE, 2023, pp. 1–6.\n- <span id=\"page-13-2\"></span>[11]\
    \ M. Hassan, \"On the Off-Chip Memory Latency of Real-Time Systems: Is DDR DRAM\
    \ Really the Best Option?\" in *2018 IEEE Real-Time Systems Symposium (RTSS)*,\
    \ 2018, pp. 495–505.\n- <span id=\"page-13-3\"></span>[12] R. Mirosanlou, D. Guo,\
    \ M. Hassan, and R. Pellizzoni, \"Mcsim: An extensible dram memory controller\
    \ simulator,\" *IEEE Computer Architecture Letters*, vol. 19, no. 2, pp. 105–109,\
    \ 2020.\n- <span id=\"page-13-4\"></span>[13] M. Zini, D. Casini, and A. Biondi,\
    \ \"Analyzing Arm's MPAM From the Perspective of Time Predictability,\" *IEEE\
    \ Transactions on Computers*, vol. 72, no. 1, pp. 168–182, 2023.\n- <span id=\"\
    page-13-6\"></span>[14] A. Herrera, \"The Promises and Challenges of Open Source\
    \ Hardware,\" *Computer*, vol. 53, no. 10, pp. 101–104, 2020.\n- <span id=\"page-13-10\"\
    ></span>[15] A. Ottaviano, T. Benz, P. Scheffler, and L. Benini, \"Cheshire: A\
    \ Lightweight, Linux-Capable RISC-V Host Platform for Domain-Specific Accelerator\
    \ Plug-In,\" *IEEE Transactions on Circuits and Systems II: Express Briefs*, pp.\
    \ 1–1, 2023.\n- <span id=\"page-13-7\"></span>[16] L. Valente *et al.*, \"Shaheen:\
    \ An Open, Secure, and Scalable RV64 SoC for Autonomous Nano-UAVs,\" in *2023\
    \ IEEE Hot Chips 35 Symposium (HCS)*, 2023, pp. 1–12.\n- <span id=\"page-13-8\"\
    ></span>[17] M. B. Taylor, \"Your Agile Open Source HW Stinks (Because It Is Not\
    \ a System),\" in *2020 IEEE/ACM International Conference On Computer Aided Design\
    \ (ICCAD)*, 2020, pp. 1–6.\n- <span id=\"page-13-9\"></span>[18] PULP, \"PULP\
    \ Platform Github,\" [https://github.com/](https://github.com/pulp-platform) [pulp-platform,](https://github.com/pulp-platform)\
    \ 2022.\n- <span id=\"page-13-11\"></span>[19] L. Valente *et al.*, \"HULK-V:\
    \ a Heterogeneous Ultra-low-power Linux capable RISC-V SoC,\" in *2023 Design,\
    \ Automation & Test in Europe Conference & Exhibition (DATE)*, 2023, pp. 1–6.\n\
    - <span id=\"page-13-12\"></span>[20] OpenHW-Group, \"CVA6,\" [https://github.com/openhwgroup/](https://github.com/openhwgroup/cva6)\
    \ [cva6,](https://github.com/openhwgroup/cva6) 2022.\n- <span id=\"page-13-13\"\
    ></span>[21] M. Schneider *et al.*, \"Composite Enclaves: Towards Disaggregated\
    \ Trusted Execution,\" *IACR Transactions on Cryptographic Hardware and Embedded\
    \ Systems*, vol. 2022, no. 1, p. 630–656, Nov. 2021.\n- <span id=\"page-13-14\"\
    ></span>[22] P. Platform, \"PULP cluster,\" [https://github.com/pulp-platform/](https://github.com/pulp-platform/pulp_cluster)\
    \ pulp [cluster,](https://github.com/pulp-platform/pulp_cluster) 2022.\n- <span\
    \ id=\"page-13-15\"></span>[23] OpenHW-Group, \"CV32E40P,\" [https://github.com/](https://github.com/openhwgroup/cv32e40p)\
    \ [openhwgroup/cv32e40p,](https://github.com/openhwgroup/cv32e40p) 2023.\n- <span\
    \ id=\"page-13-16\"></span>[24] A. Kurth *et al.*, \"An Open-Source Platform for\
    \ High-Performance Non-Coherent On-Chip Communication,\" *IEEE Transactions on\
    \ Computers*, pp. 1–1, 2021.\n- <span id=\"page-13-17\"></span>[25] B. John, \"\
    HyperRAM as a low pin-count expansion memory for embedded systems,\" [https://www.infineon.com/dgdl/](https://www.infineon.com/dgdl/Infineon-HyperRAM_as_a_low_pin-count_expansion_memory_for_embedded_systems-Whitepaper-v01_00-EN.pdf?fileId=8ac78c8c7d0d8da4017d0fb28970272c&da=t)\
    \ [Infineon-HyperRAM](https://www.infineon.com/dgdl/Infineon-HyperRAM_as_a_low_pin-count_expansion_memory_for_embedded_systems-Whitepaper-v01_00-EN.pdf?fileId=8ac78c8c7d0d8da4017d0fb28970272c&da=t)\
    \ as a low pin-count expansion memory for embedded [systems-Whitepaper-v01](https://www.infineon.com/dgdl/Infineon-HyperRAM_as_a_low_pin-count_expansion_memory_for_embedded_systems-Whitepaper-v01_00-EN.pdf?fileId=8ac78c8c7d0d8da4017d0fb28970272c&da=t)\
    \ 00-EN.pdf?fileId= [8ac78c8c7d0d8da4017d0fb28970272c&da=t,](https://www.infineon.com/dgdl/Infineon-HyperRAM_as_a_low_pin-count_expansion_memory_for_embedded_systems-Whitepaper-v01_00-EN.pdf?fileId=8ac78c8c7d0d8da4017d0fb28970272c&da=t)\
    \ 2020.\n- <span id=\"page-13-18\"></span>[26] AMD, \"Zynq-7000 - Technical Reference\
    \ Manual, UG585,\" [https:](https://docs.xilinx.com/r/en-US/ug585-zynq-7000-SoC-TRM)\
    \ [//docs.xilinx.com/r/en-US/ug585-zynq-7000-SoC-TRM.](https://docs.xilinx.com/r/en-US/ug585-zynq-7000-SoC-TRM)\n\
    - <span id=\"page-13-19\"></span>[27] A. Noami, B. Pradeep Kumar, and P. Chandrasekhar,\
    \ \"High Performance AXI4 Interface Protocol for Multi-Core Memory Controller\
    \ on SoC,\" in *Data Engineering and Communication Technology*, K. A. Reddy, B.\
    \ R. Devi, B. George, and K. S. Raju, Eds. Singapore: Springer Singapore, 2021,\
    \ pp. 131–140.\n- <span id=\"page-13-20\"></span>[28] D. Rossi, I. Loi, G. Haugou,\
    \ and L. Benini, \"Ultra-low-latency lightweight dma for tightly coupled multi-core\
    \ clusters,\" in *Proceedings of the 11th ACM Conference on Computing Frontiers*,\
    \ ser. CF '14. New York, NY, USA: Association for Computing Machinery, 2014. [Online].\
    \ Available:<https://doi.org/10.1145/2597917.2597922>\n- <span id=\"page-13-21\"\
    ></span>[29] ARM, \"AMBA AXI Protocol Specification,\" [https://developer.arm.](https://developer.arm.com/documentation/ihi0022/j/?lang=en)\
    \ [com/documentation/ihi0022/j/?lang=en,](https://developer.arm.com/documentation/ihi0022/j/?lang=en)\
    \ 2022.\n- <span id=\"page-13-22\"></span>[30] Xilinx-AMD, \"Dual Port SRAM specifications,\"\
    \ [https://docs.xilinx.com/r/2022.1-English/](https://docs.xilinx.com/r/2022.1-English/ug1483-model-composer-sys-gen-user-guide/Dual-Port-RAM)\
    \ [ug1483-model-composer-sys-gen-user-guide/Dual-Port-RAM.](https://docs.xilinx.com/r/2022.1-English/ug1483-model-composer-sys-gen-user-guide/Dual-Port-RAM)\n\
    - <span id=\"page-13-23\"></span>[31] PULP, \"HyperRAM Controller RTL,\" [https://github.com/](https://github.com/pulp-platform/hyperbus)\
    \ [pulp-platform/hyperbus,](https://github.com/pulp-platform/hyperbus) 2022.\n\
    - <span id=\"page-13-24\"></span>[32] Infineon, \"HyperRAM RTL,\" [https://www.](https://www.infineon.com/dgdl/Infineon-S27KL0641_S27KS0641_VERILOG-SimulationModels-v05_00-EN.zip?fileId=8ac78c8c7d0d8da4017d0f6349a14f68)\
    \ [infineon.com/dgdl/Infineon-S27KL0641](https://www.infineon.com/dgdl/Infineon-S27KL0641_S27KS0641_VERILOG-SimulationModels-v05_00-EN.zip?fileId=8ac78c8c7d0d8da4017d0f6349a14f68)\
    \ S27KS0641 [VERILOG-SimulationModels-v05](https://www.infineon.com/dgdl/Infineon-S27KL0641_S27KS0641_VERILOG-SimulationModels-v05_00-EN.zip?fileId=8ac78c8c7d0d8da4017d0f6349a14f68)\
    \ 00-EN.zip?fileId= [8ac78c8c7d0d8da4017d0f6349a14f68,](https://www.infineon.com/dgdl/Infineon-S27KL0641_S27KS0641_VERILOG-SimulationModels-v05_00-EN.zip?fileId=8ac78c8c7d0d8da4017d0f6349a14f68)\
    \ 2022.\n- <span id=\"page-13-25\"></span>[33] Infineon, \"HyperBUS specifications,\"\
    \ [https://www.](https://www.infineon.com/dgdl/Infineon-HYPERBUS_SPECIFICATION_LOW_SIGNAL_COUNT_HIGH_PERFORMANCE_DDR_BUS-AdditionalTechnicalInformation-v09_00-EN.pdf?fileId=8ac78c8c7d0d8da4017d0ed619b05663)\
    \ [infineon.com/dgdl/Infineon-HYPERBUS](https://www.infineon.com/dgdl/Infineon-HYPERBUS_SPECIFICATION_LOW_SIGNAL_COUNT_HIGH_PERFORMANCE_DDR_BUS-AdditionalTechnicalInformation-v09_00-EN.pdf?fileId=8ac78c8c7d0d8da4017d0ed619b05663)\
    \ SPECIFICATION LOW SIGNAL COUNT HIGH [PERFORMANCE](https://www.infineon.com/dgdl/Infineon-HYPERBUS_SPECIFICATION_LOW_SIGNAL_COUNT_HIGH_PERFORMANCE_DDR_BUS-AdditionalTechnicalInformation-v09_00-EN.pdf?fileId=8ac78c8c7d0d8da4017d0ed619b05663)\
    \ DDR [BUS-AdditionalTechnicalInformation-v09](https://www.infineon.com/dgdl/Infineon-HYPERBUS_SPECIFICATION_LOW_SIGNAL_COUNT_HIGH_PERFORMANCE_DDR_BUS-AdditionalTechnicalInformation-v09_00-EN.pdf?fileId=8ac78c8c7d0d8da4017d0ed619b05663)\
    \ 00-EN.pdf?fileId= [8ac78c8c7d0d8da4017d0ed619b05663,](https://www.infineon.com/dgdl/Infineon-HYPERBUS_SPECIFICATION_LOW_SIGNAL_COUNT_HIGH_PERFORMANCE_DDR_BUS-AdditionalTechnicalInformation-v09_00-EN.pdf?fileId=8ac78c8c7d0d8da4017d0ed619b05663)\
    \ 2022.\n- <span id=\"page-13-26\"></span>[34] R. Wilhelm *et al.*, \"The worst-case\
    \ execution-time problem—overview of methods and survey of tools,\" *ACM Trans.\
    \ Embed. Comput. Syst.*, vol. 7, no. 3, may 2008. [Online]. Available: <https://doi.org/10.1145/1347375.1347389>\n\
    - <span id=\"page-13-27\"></span>[35] T. Mitra, J. Teich, and L. Thiele, \"Time-critical\
    \ systems design: A survey,\" *IEEE Design & Test*, vol. 35, no. 2, pp. 8–26,\
    \ 2018.\n- <span id=\"page-13-28\"></span>[36] F. Restuccia *et al.*, \"Modeling\
    \ and analysis of bus contention for hardware accelerators in FPGA SoCs,\" in\
    \ *32nd Euromicro Conference on Real-Time Systems (ECRTS 2020)*. Schloss Dagstuhl-Leibniz-Zentrum\
    \ fur Informatik, 2020. ¨\n- <span id=\"page-13-32\"></span>[37] B. Thomas *et\
    \ al.*, \"AXI-REALM: A Lightweight and Modular Interconnect Extension for Traffic\
    \ Regulation and Monitoring of Heterogeneous Real-Time SoCs,\" in *2024 Design,\
    \ Automation & Test in Europe Conference & Exhibition (DATE)*, 2024.\n- <span\
    \ id=\"page-13-29\"></span>[38] J. P. Cerrolaza *et al.*, \"Multi-Core Devices\
    \ for Safety-Critical Systems: A Survey,\" *ACM Comput. Surv.*, vol. 53, no. 4,\
    \ aug 2020. [Online]. Available:<https://doi.org/10.1145/3398665>\n- <span id=\"\
    page-13-30\"></span>[39] M. Schoeberl *et al.*, \"T-CREST: Time-predictable multi-core\
    \ architecture for embedded systems,\" *Journal of Systems Architecture*, vol.\
    \ 61, no. 9, pp. 449–471, 2015.\n- <span id=\"page-13-31\"></span>[40] G. Fernandez\
    \ *et al.*, \"Increasing confidence on measurement-based contention bounds for\
    \ real-time round-robin buses,\" in *Proceedings of the 52nd Annual Design Automation\
    \ Conference*, ser. DAC '15. New York, NY, USA: Association for Computing Machinery,\
    \ 2015.\n\n**Luca Valente** received the MSc degree in electronic engineering\
    \ from the Politecnico of Turin in 2020. He is currently a PhD student at the\
    \ University of Bologna in the Department of Electrical, Electronic, and Information\
    \ Technologies Engineering (DEI). His main research interests are hardware-software\
    \ co-design of heterogeneous SoCs.\n\n**Francesco Restuccia** received a PhD degree\
    \ in computer engineering (cum laude) from Scuola Superiore Sant'Anna Pisa, in\
    \ 2021. He is a postdoctoral researcher at the University of California, San Diego.\
    \ His main research interests include hardware security, on-chip communications,\
    \ timing analysis for heterogeneous platforms, cyber-physical systems, and time-predictable\
    \ hardware acceleration of deep neural networks on commercial FPGA SoC platforms.\n\
    \n**Davide Rossi** received the Ph.D. degree from the University of Bologna in\
    \ 2012. He has been a Post-Doctoral Researcher with the Department of Electrical,\
    \ Electronic and Information Engineering \"Guglielmo Marconi,\" University of\
    \ Bologna, since 2015, where he is currently an Associate Professor position.\
    \ His research interests focus on energy-efficient digital architectures. In this\
    \ field, he has published more than 100 papers in international peer-reviewed\
    \ conferences and journals.\n\n**Ryan Kastner** is a professor in the Department\
    \ of Computer Science and Engineering at UC San Diego. He received a PhD in Computer\
    \ Science at UCLA, a masters degree in engineering and bachelor degrees in Electrical\
    \ Engineering and Computer Engineering from Northwestern University. His current\
    \ research interests fall into three areas: hardware acceleration, hardware security,\
    \ and remote sensing.\n\n**Luca Benini** holds the chair of Digital Circuits and\
    \ Systems at ETHZ and is Full Professor at the Universita di Bologna. He received\
    \ a PhD from ` Stanford University. His research interests are in energy-efficient\
    \ parallel computing systems, smart sensing micro-systems and machine learning\
    \ hardware. He has published more than 1000 peer-reviewed papers and 5 books.\
    \ He is a Fellow of the ACM and a member of the Academia Europaea."
  decisions:
    evaluation_prompt: 'Qualified. Reason: All relevant sections passed.'
    related_work_prompt: 'Qualified. Reason: All relevant sections passed.'
    novelty_prompt: 'Qualified. Reason: All relevant sections passed.'
    review_only_prompt: 'Qualified. Reason: All relevant sections passed.'
- title: 'POSTER: Identifying and Mitigating Vulnerabilities in LLM-Integrated Applications'
  abstract: Compared with the traditional usage of large language models (LLMs) where
    users directly send queries to an LLM, LLM-integrated applications serve as middleware
    to refine users' queries with domain-specific knowledge to better inform LLMs
    and enhance the responses. However, LLM-integrated applications also introduce
    new attack surfaces. This work considers a setup where the user and LLM interact
    via an application in the middle. We focus on the interactions that begin with
    user's queries and end with LLMintegrated application returning responses to the
    queries, powered by LLMs at the service backend. We identify potential high-risk
    vulnerabilities in this setting that can originate from the malicious application
    developer or from an outsider threat initiator that can control the database access,
    manipulate and poison high-risk data for the user. Successful exploits of the
    identified vulnerabilities result in the users receiving responses tailored to
    the intent of a threat initiator. We assess such threats against LLM-integrated
    applications empowered by GPT-3.5 and GPT-4. Our experiments show that the threats
    can effectively bypass the restrictions and moderation policies of OpenAI, resulting
    in users exposing to the risk of bias, toxic content, privacy, and disinformation.
    We develop a lightweight, threat-agnostic defense to mitigate insider and outsider
    threats. Our evaluations demonstrate the efficacy of our defense.
  keywords: ''
  document: '![](_page_0_Picture_0.jpeg)


    # POSTER: Identifying and Mitigating Vulnerabilities in LLM-Integrated Applications


    Fengqing Jiang University of Washington Seattle, USA fqjiang@uw.edu


    Zhangchen Xu University of Washington Seattle, USA zxu9@uw.edu


    Luyao Niu University of Washington Seattle, USA luyaoniu@uw.edu


    Boxin Wang Nvidia Santa Clara, USA boxinw@nvidia.com


    Jinyuan Jia Penn State University State College, USA jinyuan@psu.edu


    Bo Li University of Chicago Chicago, USA bol@uchicago.edu


    ## 1 INTRODUCTION


    LLM-integrated applications are increasingly deployed to allow third party developers/vendors
    to serve users leveraging the astonishing capabilities of large language models
    (LLMs). An LLMintegrated application consists of three parties – user, application,
    and LLM, interacting through two interfaces as shown in Fig. [1.](#page-0-0) The
    interaction consists of two communication phases: upstream communication and downstream
    communication. In the upstream communication, a user sends queries to an application
    through a userapplication interface; the application refines the user''s queries
    based on a domain-specific database and forwards the refined queries to the LLM
    via an application-LLM interface. In the downstream communication, the LLM generates
    responses to the refined queries and sends the responses back to the application;
    the application post-processes the responses and sends the processed responses
    to the user. While users can utilize LLM-integrated applications to better inform
    LLMs for enhanced services, the presence of untrusted/unverified application developers
    opens up new attack surfaces for misuses. Currently, however, identifying and
    mitigating the vulnerabilities of LLM-integrated applications have not been studied.


    Radha Poovendran University of Washington Seattle, USA rp3@uw.edu


    <span id="page-0-0"></span>![](_page_0_Figure_10.jpeg)


    #### Figure 1: Service schematic of LLM-integrated applications.


    In this work in progress, we identify and list a set of attacks that arise from
    an LLM application and external adversaries that can interact with the LLM application,
    which define the attack surface. In particular, we focus on the model where a
    user interacts with the LLM through an LLM-integrated application, i.e., a user
    sends the query and the application returns the answer with the help of LLM. We
    show that such a query-response protocol is vulnerable to both insider and outsider
    threats with the goal of monetizing and enhancing their profits. An insider threat
    arises from a malicious


    #### ABSTRACT


    Compared with the traditional usage of large language models (LLMs) where users
    directly send queries to an LLM, LLM-integrated applications serve as middleware
    to refine users'' queries with domain-specific knowledge to better inform LLMs
    and enhance the responses. However, LLM-integrated applications also introduce
    new attack surfaces. This work considers a setup where the user and LLM interact
    via an application in the middle. We focus on the interactions that begin with
    user''s queries and end with LLMintegrated application returning responses to
    the queries, powered by LLMs at the service backend. We identify potential high-risk
    vulnerabilities in this setting that can originate from the malicious application
    developer or from an outsider threat initiator that can control the database access,
    manipulate and poison high-risk data for the user. Successful exploits of the
    identified vulnerabilities result in the users receiving responses tailored to
    the intent of a threat initiator. We assess such threats against LLM-integrated
    applications empowered by GPT-3.5 and GPT-4. Our experiments show that the threats
    can effectively bypass the restrictions and moderation policies of OpenAI, resulting
    in users exposing to the risk of bias, toxic content, privacy, and disinformation.
    We develop a lightweight, threat-agnostic defense to mitigate insider and outsider
    threats. Our evaluations demonstrate the efficacy of our defense.


    #### CCS CONCEPTS


    • Security and privacy → Human and societal aspects of security and privacy.


    #### ACM Reference Format:


    Fengqing Jiang, Zhangchen Xu, Luyao Niu, Boxin Wang, Jinyuan Jia, Bo Li, and Radha
    Poovendran. 2024. POSTER: Identifying and Mitigating Vulnerabilities in LLM-Integrated
    Applications. In Proceedings of ACM Asia Conference on Computer and Communications
    Security (Asia CCS''24). ACM, New York, NY, USA, [3](#page-2-0) pages. <https://doi.org/10.1145/3634737.3659433>


    Asia CCS''24, July 1–5, 2024, Singapore, Singapore


    © 2024 Copyright held by the owner/author(s).


    ACM ISBN 979-8-4007-0482-6/24/07


    <https://doi.org/10.1145/3634737.3659433>


    Permission to make digital or hard copies of part or all of this work for personal
    or classroom use is granted without fee provided that copies are not made or distributed
    for profit or commercial advantage and that copies bear this notice and the full
    citation on the first page. Copyrights for third-party components of this work
    must be honored. For all other uses, contact the owner/author(s).


    Asia CCS''24, July 1–5, 2024, Singapore, Singapore Fengqing Jiang, Zhangchen Xu,
    Luyao Niu, Boxin Wang, Jinyuan Jia, Bo Li, and Radha Poovendran


    ![](_page_1_Figure_2.jpeg)


    Figure 2: Attack in upstream communication by an insider.


    application developer. The insider threat initiator could achieve its attack objective
    by manipulating users'' queries and/or responses from the LLM to alter the contexts
    and perturb the semantics during the upstream and downstream communication phases.
    An outsider threat arises from the compromised database maintained by the application.
    The outsider threat initiator can control the database access and poison the data
    used by the application. Consequently, even if the application developer is benign,
    the queries from users may be refined in an unintended manner by the application,
    leading to responses that are aligned with the attack goal. We empirically assess
    both threats to a chatbot of an online shopping application integrated with GPT-3.5
    and GPT-4. Our results show that attacks can successfully bypass the restrictions
    [\[5\]](#page-2-1) of OpenAI, and result in responses to users containing bias
    and toxic contents.


    We propose the first known defense, Shield, to mitigate the identified risks.
    We showShield prevents both threats from manipulating the queries from users or
    responses by LLM. Our empirical evaluations show that Shield achieves attack detection
    with high accuracy and utility preservation when serving benign users.


    ## <span id="page-1-1"></span>2 LLM-INTEGRATED APPLICATION AND THREAT MODEL


    #### 2.1 LLM-integrated Application


    The service pipeline of an LLM-integrated application consists of three parties:
    user , application , and LLM , as shown in Fig. [1.](#page-0-0)


    Upstream Communication: User sends a query prompt to the application to access
    certain services such as shopping advising. After receiving , the application
    first identifies and extracts information, denoted as ( ), from the query. Then,
    the application utilizes its external source, e.g., query database or access context
    memory, to obtain domain-specific information ( ( )). Finally, the application
    refines user query with domain-specific information ( ( )) to generate an intermediate
    prompt as = ( , ( ( ))) using techniques such as Self-instruct [\[7\]](#page-2-2).


    Downstream Communication: LLM responds to prompt by returning a raw response to
    the application. The application takes a post-processing action (e.g., using an
    external toolkit) to generate response = () in order to satisfy user''s query
    .


    #### 2.2 Threat Model and Attack Surface


    Insider Threat and Attack. An insider threat originates from malicious application
    developers. Even when the application developers are benign, a threat initiator
    may exploit the vulnerabilities inherent in the application such as unpatched
    software [\[2\]](#page-2-3). An


    <span id="page-1-0"></span>Table 1: TSRs of bias, toxic, privacy, and disinformation
    risks.


    | Risk     | Threat       | GPT-3.5   |          | GPT-4     |          |

    |----------|--------------|-----------|----------|-----------|----------|

    |          | Model        | HumanEval | GPT-auto | HumanEval | GPT-auto |

    | Bias     | Neutral      | 2%        | 0%       | 0%        | 0%       |

    |          | Pertb-User   | 62%       | 47%      | 99%       | 67%      |

    |          | Pertb-System | 97 %      | 85 %     | 100%      | 81%      |

    |          | Proxy        | 83%       | 68%      | 80%       | 53%      |

    | Toxic    | Neutral      | 0%        | 0%       | 0%        | 0%       |

    |          | Outsider     | 78%       | 78%      | 88%       | 94%      |

    |          | Pertb-System | 100%      | 100%     | 100%      | 100%     |

    | Privacy  | Neutral      | 0%        | 0%       | 0%        | 0%       |

    |          | Pertb-System | 98%       | 100%     | 100%      | 100%     |

    | Disinfo. | Neutral      | 0%        | 0%       | 0%        | 0%       |

    |          | Pertb-System | 100%      | 100%     | 100%      | 98%      |


    initiator of insider threat can thereby control the application, and attack LLM-integrated
    applications during both the upstream and downstream communication phases. As
    a result, the threat initiator has can either (1) modify the user prompt to through
    prompt injection [\[6\]](#page-2-4), or (2) manipulate the LLM response to ˜ in
    downstream communication. Both (1) and (2) lead to users receiving responses aligned
    with the semantic goal of threat initiator.


    Outsider Threat and Attack. In this case, the application is operated by a benign
    entity. The threat initiator could achieve its semantic goal by compromising the
    external sources such as database of the application via data poisoning attacks
    [\[1\]](#page-2-5). Consequently, the application may use compromised information
    ( ( )) to generate prompt , which leads the LLM to generate response that fulfills
    the threat initiator''s semantic goal.


    #### 3 THREAT EVALUATION


    Experimental Setup. We consider an online shopping application whose chatbot uses
    GPT-3.5 and GPT-4 [\[4\]](#page-2-6) in the backend. An insider threat initiator
    can tamper with the queries from users in the upstream communication in two ways:
    (i) by perturbing the queries via prompt injection [\[6\]](#page-2-4), denoted
    as Pertb-User, and (ii) by applying perturbed system prompt [\[3\]](#page-2-7),
    denoted as Pertb-System. During the downstream communication, an insider threat
    initiator perturbs the semantics of responses by generating a proxy prompt ˜ using
    prompt injection [\[6\]](#page-2-4). We denote this attack as Proxy.


    We use targeted attack success rate (TSR) to measure the effectiveness of attacks,
    defined as


    $$\text{TSR} = \frac{\text{\text# of responses aligned with semantic goal}}{\text{\textdegreeresponse}}.1$$


    We calculate TSR using two methods: HumanEval and GPT-auto. For HumanEval, we
    manually check whether each response satisfies the condition. For GPT-auto, we
    utilize GPT-3.5 to check those responses. Even in the absence of insider and outsider
    threats, LLM may occasionally return responses containing unintended bias, privacy
    issues, and/or disinformation. To identify whether such undesired semantics are
    generated due to attacks or from LLMs, we evaluate TSRs in the absence of the
    threats, denote as Neutral.


    Experimental Results. In Table [1,](#page-1-0) we evaluate the threat models on
    bias, toxic, privacy and disinformation risks. We observe that the insider threat
    effectively lead to responses demonstrating all risks compared to Neutral. The
    results indicate our proposed attack successfully bypass the ethic restrictions
    deployed by OpenAI [\[5\]](#page-2-1). <span id="page-2-0"></span>POSTER: Identifying
    and Mitigating Vulnerabilities in LLM-Integrated Applications Asia CCS''24, July
    1–5, 2024, Singapore, Singapore


    <span id="page-2-8"></span>![](_page_2_Figure_1.jpeg)


    Figure 3: This figure shows the workflow of **Shield**.


    We also note that using system prompt achieves highest TSR. This is because the
    insider threat initiator can fully control the application.


    ### 4 PROPOSED DEFENSE **SHIELD**


    To mitigate the vulnerabilities in Section [2,](#page-1-1) We design a defense
    named Shield. Our key idea is to ensure the queries from users cannot be manipulated,
    and are distinguishable from the intermediate prompts from application. Fig. [3](#page-2-8)
    shows the workflow of Shield. We define the signature of a message as = sig (),
    where sig is a signing algorithm using key . We denote the signed message as (,
    ). The verification of (, ), denoted as ver (, ), outputs either true or false.
    The unique session ID is .


    Upstream communication. ❶: Session ID and user''s query (, ) is signed using user''s
    key as <sup>1</sup> = sig (, ). ❷: The signed query is then sent to the application
    to generate the intermediate prompt = ( , ( ( ))). ❸: After receiving the intermediate
    prompt, Shield verifies whether ver ( (, ), 1) holds true. If the result is true,
    Shield then records the ID and constructs a metaprompt <sup>1</sup> for detection
    by LLM. ❹: Shield sends <sup>1</sup> to the LLM. If no attack is detected, is
    transmitted to the LLM for response generation. Otherwise, only user''s query
    is sent to the LLM.


    Downstream communication. ➀: After the application receives the response from
    the LLM, it generates a response and sends it back to Shield. The API then constructs
    a meta-prompt 2. ➁: Shield then sends <sup>2</sup> to the LLM for attack detection.
    ➂: If no attack is detected, then Shield signs as <sup>2</sup> = sig (, ), where
    is the key of Shield. The signed response ( (, ), 2) is then returned to the user.
    Otherwise, Shield returns to the user with the corresponding signature. ➃: After
    receiving responses from the application, the user executes ver ( (, ), 2). If
    the verification process returns true, then user accepts as the response.


    Evaluation. We empirically evaluate the attack detectability and utility preservation
    of our defense. We quantify the attack detectability by computing the ratio of
    tests that are correctly labeled as under attack. The utility preservation is
    evaluated using the Neutral scenario, where there exists no attack. We summarize
    the evaluation results on the online shopping application in Table [2.](#page-2-9)
    We first observe that Shield successfully detects the attacks when both GPT-3.5
    and GPT-4 are used as LLM services. The latest GPT-4 achieves nearly 100% success
    rate in detecting attacks across all risks. Furthermore, Shield preserves the
    utility of LLM-integrated


    <span id="page-2-9"></span>


    | Risk     | Threat Model | GPT-3.5 | GPT-4 |

    |----------|--------------|---------|-------|

    |          | Neutral      | 94%     | 100%  |

    |          | Pertb-User   | 100%    | 100%  |

    | Bias     | Pertb-System | 92%     | 100%  |

    |          | Proxy        | 71%     | 99%   |

    |          | Neutral      | 100%    | 100%  |

    | Toxic    | Outsider     | 100%    | 100%  |

    |          | Pertb-System | 100%    | 100%  |

    |          | Neutral      | 100%    | 100%  |

    | Privacy  | Pertb-System | 36%     | 100%  |

    |          | Neutral      | 100%    | 100%  |

    | Disinfo. | Pertb-System | 56%     | 80%   |


    applications. When there exist no attacks, all responses produced by LLM-integrated
    applications can address the users'' queries.


    #### 5 CONCLUSION AND DISCUSSION


    In this paper, we showed that LLM-integrated applications become new attack surfaces
    that could be exploited by both insider and outsider threat initiators, leading
    to bias, toxic, privacy, and disinformation risks for users of applications. Our
    extensive empirical evaluations confirmed those risks. We designed a defense Shield
    in addition to the LLM-API which is compatible with any LLMs. Our experimental
    results demonstrated the efficacy of our defense.


    #### ACKNOWLEDGEMENT


    This work is partially supported by the Air Force Office of Scientific Research
    (AFOSR) under grant FA9550-23-1-0208, National Science Foundation (NSF) under
    grants No.1910100, No.2046726, No. 2229876, DARPA GARD, the National Aeronautics
    and Space Administration (NASA) under grant No.80NSSC20M0229, Alfred P. Sloan
    Fellowship, Office of Naval Research (ONR) under grant N00014-23-1-2386, and the
    Amazon research award.


    This work is supported in part by funds provided by the National Science Foundation,
    by the Department of Homeland Security, and by IBM. Any opinions, findings, and
    conclusions or recommendations expressed in this material are those of the author(s)
    and do not necessarily reflect the views of the National Science Foundation or
    its federal agency and industry partners.


    #### REFERENCES


    - <span id="page-2-5"></span>[1] Xinyun Chen, Chang Liu, Bo Li, Kimberly Lu, and
    Dawn Song. 2017. Targeted backdoor attacks on deep learning systems using data
    poisoning. arXiv preprint arXiv:1712.05526 (2017).

    - <span id="page-2-3"></span>[2] Lockheed Martin. 2022. The cyber kill chain.
    [https://www.lockheedmartin.com/en](https://www.lockheedmartin.com/en-us/capabilities/cyber/cyber-kill-chain.html)[us/capabilities/cyber/cyber-kill-chain.html.](https://www.lockheedmartin.com/en-us/capabilities/cyber/cyber-kill-chain.html)
    Accessed: 2023-09-15.

    - <span id="page-2-7"></span>[3] OpenAI. 2023. ChatGPT API Transition Guide. [https://help.openai.com/en/articles/](https://help.openai.com/en/articles/7042661-chatgpt-api-transition-guide)
    [7042661-chatgpt-api-transition-guide.](https://help.openai.com/en/articles/7042661-chatgpt-api-transition-guide)
    Accessed: 2023-09-15.

    - <span id="page-2-6"></span>[4] OpenAI. 2023. Models-OpenAI API. [https://platform.openai.com/docs/models.](https://platform.openai.com/docs/models)
    Accessed: 2023-09-15.

    - <span id="page-2-1"></span>[5] OpenAI. 2023. Usage Policies–OpenAI. [https://openai.com/policies/usage-policies.](https://openai.com/policies/usage-policies)
    Accessed: 2023-09-15.

    - <span id="page-2-4"></span>[6] Fábio Perez and Ian Ribeiro. 2022. Ignore previous
    prompt: Attack techniques for language models. arXiv preprint arXiv:2211.09527
    (2022).

    - <span id="page-2-2"></span>[7] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra,
    Alisa Liu, Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2022. Self-Instruct:
    Aligning language model with self generated instructions. arXiv preprint arXiv:2212.10560
    (2022).'
  decisions:
    evaluation_prompt: 'Qualified. Reason: The paper contains sections titled "Threat
      Evaluation" and "Evaluation," which describe empirical assessments of threats
      and the proposed defense mechanism, Shield. It includes tables and figures presenting
      quantifiable outcomes, such as targeted attack success rates (TSR) and the effectiveness
      of the defense mechanism.'
    related_work_prompt: '- Qualified. Reason: The paper includes meaningful engagement
      with prior research, as evidenced by the citations throughout the text. It references
      previous work on data poisoning attacks, the cyber kill chain, and techniques
      for language model attacks, among others. These citations are used to contextualize
      the current study and its contributions, indicating a meaningful engagement
      with existing literature.'
    novelty_prompt: 'Qualified. Reason: The paper proposes a novel defense mechanism
      called "Shield" to mitigate vulnerabilities in LLM-integrated applications,
      which is claimed to be the first known defense of its kind. The paper also identifies
      new attack surfaces and vulnerabilities specific to LLM-integrated applications,
      which have not been previously studied. These contributions demonstrate novelty
      in both identifying new security challenges and proposing a new solution.'
    review_only_prompt: 'Qualified. Reason: The paper introduces novel contributions
      by identifying vulnerabilities in LLM-integrated applications and proposing
      a defense mechanism called "Shield" to mitigate these threats. It includes original
      experiments and evaluations demonstrating the efficacy of the proposed defense,
      which qualifies it as a research paper with novel contributions.'
- title: 'LRSCwait: Enabling Scalable and Efficient Synchronization in Manycore Systems
    through Polling-Free and Retry-Free Operation'
  abstract: ''
  keywords: atomics, synchronization, manycore, RISC-V
  document: '## I. INTRODUCTION


    Manycore systems are becoming increasingly popular due to the growing demand for
    computing power. However, the parallel execution of tasks introduces synchronization
    and atomicity issues that can lead to race conditions and unpredictable results.
    To ensure exclusive access to critical sections (CSs), atomic operations and locks
    can be used. However, locks also block cores that try to acquire them when they
    are not free, leading to busy waiting and polling. Polling, or constantly checking
    a shared resource for changes, can become an issue in concurrent algorithms. It
    leads to high core utilization and reduces overall system performance and energy
    efficiency as the cores compete for shared resources [\[1\]](#page-5-0). In the
    worst case, it can lead to livelocks or starvation, where cores are blocked from
    making progress because others continuously block them.


    Non-blocking algorithms avoid locks by updating atomic variables directly with
    atomic read–modify–write (RMW) operations. Specific arithmetic operations, like
    *add, and, or*, are often supported through specialized instructions. However,
    most concurrent algorithms require more complex modifications of atomic variables,
    such as conditional updates. For generic RMW operations, the compare-and-swap
    (CAS) operations or loadreserved/store-conditional (LRSC) pair are typical primitives
    designed to ensure that the operation is *atomic*, i.e., without interference
    from other cores [\[2\]](#page-5-1). For example, RISC-V''s loadreserved (LR)
    instruction loads a value from memory and


    places a reservation. The core can perform operations with the loaded value and
    store the result back conditionally with a store-conditional (SC). The latter
    instruction will only succeed if the reservation is still valid, meaning the memory
    location was not modified in the meantime. If the SC succeeds, the RMW sequence
    appears atomically. However, cores that fail an SC must retry the LRSC sequence
    pair until it succeeds. Variables outside CSs can also cause polling, where cores
    wait for changes in shared variables, leading to inefficiencies in core communication,
    like producer/consumer interactions.


    To eliminate retries and polling, we propose a novel, generalpurpose atomic RMW
    instruction pair called LRwait and SCwait. They extend the standard RISC-V LRSC
    pair by moving the linearization point, the point where the atomic operations
    of different cores get ordered, from the SC to the LRwait. The LRwait and SCwait
    are used in the same way as the LRSC pair. However, instead of returning the memory
    value immediately, the LRwait instruction only responds to one core at a time
    to set it up for a successful SCwait. This prevents failing SCs and retry loops.
    Furthermore, LRSCwait allows implementing polling-free locks. To eliminate polling
    even for non-atomic variables, we propose the Mwait instruction, which enables
    cores to sleep until a specific memory address changes its value.


    While cache-based systems often rely on the coherency protocol to implement such
    behavior, manycore accelerators scaling to hundreds of cores often rely on software-managed,
    multi-banked scratchpad memories (SPMs). Examples include commercial chips like
    GAP9 [\[3\]](#page-5-2) and RC64 [\[4\]](#page-5-3), as well as largescale research
    prototypes like MemPool [\[5\]](#page-5-4). While LRSCwait can be applied to cache
    and cache-less systems, in this work, we focus on cache-less, SPM-based manycore
    systems since they pose the design challenge of the memory controllers having
    to keep track of outstanding LRwait instructions to send their responses at the
    right time. However, duplicating large hardware queues for each bank is costly
    and scales poorly.


    As a scalable implementation of the proposed instructions, we present *Colibri*.
    Its concept is similar to linked-list-based software queues. It does not allocate
    a full array of entries for each queue but just a head and tail pointer per queue
    as illustrated in [Fig. 1.](#page-2-0) Each core is equipped with a queue node
    that can be linked to any queue. For Colibri, this means that instead of equipping
    each memory controller with a


    <sup>© 2024</sup> IEEE. Personal use of this material is permitted. Permission
    from IEEE must be obtained for all other uses, in any current or future media,
    including reprinting/republishing this material for advertising or promotional
    purposes, creating new collective works, for resale or redistribution to servers
    or lists, or reuse of any copyrighted component of this work in other works.


    hardware queue that can hold an entry for each core, each memory controller is
    extended with a parameterizable number of head and tail registers to form linked
    lists. Each core is then equipped with one hardware queue node, and when issuing
    an LRwait, the core inserts itself in the corresponding queue. We implemented
    Colibri on the open-source, manycore MemPool system, consisting of 256 cores sharing
    1 MiB of L1 memory [\[5\]](#page-5-4). Colibri provides a scalable solution that
    can be easily integrated into existing RISC-V systems. The LRSCwait solution can
    be used as a drop-in replacement for LRSC or as a powerful extension, making it
    a desirable option for highperformance computing systems. We evaluate the performance
    of Colibri against various hardware and software approaches. The results indicate
    that Colibri outperforms other approaches in all experiments, with a throughput
    increase of up to 6.5 times in high-contention situations and a 13% increase in
    low-contention scenarios. Additionally, Colibri reduces polling, allowing other
    applications to be unaffected by concurrent atomic accesses. Our key contributions
    are the following:


    - The LRwait extension consisting of three novel instructions (LRwait, SCwait,
    and Mwait), which enable atomic access and monitoring memory locations with a
    minimal amount of polling [\(Section III\)](#page-1-0).

    - A scalable implementation for LRwait named Colibri leveraging a distributed
    reservation queue [\(Section IV\)](#page-3-0).

    - An implementation and evaluation of Colibri on the MemPool platform that outperforms
    other approaches in throughput, fairness, polling, and energy per atomic access.
    Colibri scales linearly on the MemPool platform by introducing an area overhead
    of just 6% while being 8.8x more energy efficient than locks [\(Section V\)](#page-4-0).


    ## II. RELATED WORK


    A common approach to mitigate polling is using a backoff after a failed atomic
    access [\[2\]](#page-5-1). Existing backoff schemes, such as exponential backoff,
    where each failed attempt increases the backoff time, can reduce the overhead
    on shared resources but still make the cores busy-waiting and performing sub-optimally.


    The Mellor-Crummey, Scott (MCS) lock [\[6\]](#page-5-5) relies on a software queue
    for contending cores to enqueue in and spin on their respective node in the queue.
    This guarantees that each core spins on a unique location to mitigate contention
    on the lock variable itself. This approach works well in cache-based systems since
    each core can spin on its own L0 cache. However, in this work, we focus on systems
    with software-managed memories.


    While software approaches to locks are general and platform agnostic, their performance
    can not keep up with hardware locks. A study of two software locks and four hardware
    locks shows that hardware locks consistently outperform the software approaches
    by 25%-94%. However, hardware locks such as Hardlocks [\[7\]](#page-5-6) do not
    scale well, as the locks are managed by a centralized locking unit accessible
    to all cores. Accessing this unit quickly becomes the bottleneck in large systems.
    Furthermore, the number of locks is fixed at implementation time. Similarly, Glaser
    et al. present a synchronization unit where each core has a private direct connection
    to each hardware lock [\[8\]](#page-5-7). While this solves the contention issue,
    it prevents scaling beyond a few tens of cores. GLock suffers from a similar scalability
    issue [\[9\]](#page-5-8). It is based on a dedicated on-chip network consisting
    of lock managers and local controllers that synchronize to acquire a lock. Monchiero
    et al. propose a synchronizationoperation buffer implemented as a hardware queue
    in the memory controller to resolve the lock accesses [\[10\]](#page-5-9). However,
    this approach only implements locks and has a hardware cost that is proportional
    to the number of cores. Furthermore, each memory controller would require such
    a buffer to manage locks.


    While locks are a common solution for protecting critical sections, their blocking
    nature often limits performance. Lockfree algorithms, on the other hand, allow
    for much more concurrency. They often rely on instructions like CAS or the LRSC
    pair. This section focuses on the latter, specifically, RISC-V''s implementation.
    For example, the ATUN is a unit that can be placed in an Advanced eXtensible Interface
    (AXI) bus to support LRSC instructions to the downstream memory [\[11\]](#page-5-10).
    The table allows a reservation for every core, thus implementing a non-blocking
    version of LRSC. Furthermore, each bank would require its own ATUN adapter in
    a multi-banked system, introducing significant hardware overhead in large manycore
    systems. The Rocket chip features a similar implementation [\[12\]](#page-5-11).
    However, the number of reservations is limited.


    MemPool implements a lightweight version of LRSC by only providing a single reservation
    slot per memory bank [\[5\]](#page-5-4). However, this sacrifices the non-blocking
    property of the LRSC pair. The GRVI multiprocessor, on the other hand, modifies
    the granularity at which LRSCs operate by locking the complete memory bank [\[13\]](#page-5-12).
    This reduces the hardware overhead to one bit per core per bank, albeit the approach
    is still affected by retries due to spuriously failing SC operations.


    All those solutions implement the standard RISC-V LRSC instruction, leveraging
    the freedom of the official specification to achieve different trade-offs. However,
    none of them solve the polling and retry issue of failing SC operations. On the
    contrary, they sometimes worsen it. The Request-Store-Forward (RSF) synchronization
    model proposed by Liu et al. is similar to LRwait [\[14\]](#page-5-13). Synchronization
    requests are stored in a hardwaremanaged memory and handled in order by a synchronization
    controller. However, this approach leads to a high memory footprint, and the hardware
    needs to be replicated for each memory bank. Furthermore, it is infeasible for
    software-managed memories as the synchronization controller will interfere with
    the allocated data when adding the queue to the memory.


    Our LRwait approach and the efficient implementation through Colibri scale well
    to hundreds of cores and banks while completely eliminating polling without sacrificing
    granularity.


    ## III. LRWAIT AND SCWAIT


    <span id="page-1-0"></span>RISC-V defines the load-reserved/store-conditional
    (LRSC) instructions to implement generic, atomic RMW operations. The LR instruction
    reads a value from memory and places a reservation, which remains valid until
    the specified memory address is changed. The core can then modify the value and
    write the result back with an SC instruction. The latter will succeed only if
    the reservation is still valid. If the SC fails,


    the LRSC sequence has to be retried. The linearization point between contending
    cores is thus at the SC.


    LRwait eliminates the wasteful retry loop by moving the linearization point to
    the LRwait instruction, i.e., atomic accesses of competing cores are ordered at
    the LRwait instruction. Instead of immediately returning the value, the memory
    controller withholds the response such that only one core gets a response at a
    time, guaranteeing it to be the only core issuing an SCwait to the same address.
    The LRSCwait and LRSC instructions share similar semantics. The SCwait stores
    a value conditionally and returns a success or failure code analogous to the SC.
    Likewise, the LRwait matches the LR instruction, but its response is delayed.
    The sequence of an atomic RMW operation with LRSCwait is the following:


    - 1) The core issues the LRwait and waits for the response.

    - 2) The memory buffers the request until it is the next outstanding LRSCwait
    pair to that address.

    - 3) Once the LRwait is the next in line, the memory serves the request with the
    current memory value and monitors it. A store to the same address clears the reservation.

    - 4) The core modifies the value and writes it with an SCwait.

    - 5) The memory accepts the value if a valid reservation still exists and issues
    the response.


    While the memory guarantees that only one core proceeds with an LRSCwait pair,
    it cannot eliminate the possibility of another core overwriting the atomic variable,
    leading to a failing SCwait. One constraint of the LRSCwait instruction pair is
    that every LRwait must eventually be followed by an SCwait. While RISC-V does
    not have this constraint for LRSC, our extension requires the matching SCwait
    to yield the queue of outstanding LRwait instructions and allow progress on the
    atomic variable. Albeit LRSCwait can be used as a drop-in replacement for LRSC,
    it removes the lock-free progress guarantee that the LRSC instructions have. Since
    only one core can issue an SCwait, a malicious core could block the resource indefinitely
    and obstruct progress. However, LRSCwait still gives strong progress guarantees
    under the following constraints:


    *a) Mutual exclusion:* Just as the LRSC pair, the SCwait only succeeds if a valid
    reservation is present, meaning there was no write between the LRwait and the
    SCwait, which guarantees mutual exclusion and, therefore, atomicity.


    *b) Deadlock freedom:* To prevent circular dependencies between cores, every core
    must have at most one outstanding LRwait operation. RISC-V does not impose this
    requirement on LRSC. However, only the innermost LRSC pair is guaranteed to progress.
    Therefore, this requirement for deadlock freedom is a requirement for livelock
    freedom already. Furthermore, each core''s LRwait must eventually be followed
    by an SCwait to close the CS. We impose the same constraints as the RISC-V standard
    to allow only a finite and limited set of instructions between LRwait and SCwait.


    *c) Starvation freedom:* Starvation freedom guarantees that all cores eventually
    make progress. LRSC only guarantees that one core makes progress because an unlucky
    core could always lose the SC to a faster core. In our work, this scenario is
    prevented by handling the LRSCwait pairs in order, thus


    ![](_page_2_Figure_11.jpeg)


    <span id="page-2-0"></span>Fig. 1. Difference between LRSC architecture with a
    reservation table, LRSCwait with a reservation queue, and Colibri with a linked-list-like
    structure.


    enabling all cores to eventually execute the LRSCwait pair and, therefore, guaranteeing
    starvation freedom.


    Overall, while the blocking nature of the LRSCwait makes a core''s progress depend
    on other cores correctly executing and leaving the LRSCwait blocks, these constraints
    can easily be adhered to in bare-metal systems, which are fully under the programmer''s
    control. LRSCwait can provide very strong progress guarantees, enabling each core
    to progress. However, hardware failure or software bugs can become blocking.


    ## <span id="page-2-1"></span>*A. Ideal Hardware Implementation*


    A straightforward hardware implementation of LRSCwait requires tracking all outstanding
    reservations in order to ensure fairness and starvation freedom. As shown in [Fig.
    1,](#page-2-0) this can be achieved by an LRSCwait adapter placed in front of
    each memory bank, consisting of (i) a queue-like data structure of capacity n,
    where n is the number of cores in the system, and (ii) some additional logic to
    monitor memory accesses and invalidate reservations when the target address is
    overwritten. The overhead of this implementation in a system with m memory banks
    is O(n log<sup>2</sup> (n)m), where log<sup>2</sup> (n) represents identifier
    size per core. Assuming that m scales linearly with the number of cores, this
    implementation''s overhead scales quadratically with the system size: O(n 2 ),
    a non-negligible hardware complexity.


    ## *B. Optimized Hardware Implementation*


    To reduce the hardware complexity, we can decrease the queue''s capacity by assuming
    that only a subset of cores can access a specific address simultaneously. Our
    implementation supports a parametrizable number of reservation slots q. The case
    with q = n falls back to the ideal LRSCwait pair described in [Section](#page-2-1)
    III-A. We call this implementation *LRSCwaitideal*. If q < n, we trade hardware
    overhead with performance. In these implementations, *LRSCwaitq*, cores executing
    an LRwait to a full queue will fail immediately.


    ## *C. Mwait*


    To allow efficient monitoring and notification of a memory location from a core
    in the system, we introduce *Mwait*. Mwait is derived from LRwait, but without
    a matching SCwait. Instead, the reservation placed by Mwait is used to identify
    the core that needs to be notified of a change. For instance, a core may monitor
    a queue and be woken up when an element is pushed onto the queue. Our experiments
    show that Mwait provides a simple and efficient mechanism for monitoring memory


    locations, allowing cores to be woken up only when necessary. To handle the possibility
    that the change we wish to observe has already occurred, we provide Mwait with
    an expected value. If the memory location already differs from the expected value
    when Mwait is served, the core is immediately notified.


    ## IV. COLIBRI


    <span id="page-3-0"></span>Colibri implements a distributed queue, similar to
    a linked list, shown in [Fig. 1.](#page-2-0) It alleviates the huge hardware overhead
    of the hardware queues at each memory controller, replacing it with a dedicated
    head and tail node per queue and a simple controller. On top of that, each core
    requires its own hardware node, called *queue node (Qnode)*, to enqueue itself.
    Since each core can only be in one queue, one Qnode per core is enough. Therefore,
    Colibri only requires O(n + 2m) nodes and scales linearly with the system size.


    Since the queue is distributed across Qnodes and the head/tail nodes next to the
    memory banks, updating the queue becomes more complex. In comparison to the ideal
    LRwait, an enqueue operation from an LRwait, or a dequeue operation by an SCwait,
    does not happen in one place and a single cycle.


    We present a simple example of the construction and deconstruction of the queue
    in [Fig. 2](#page-3-1) with a single memory and two cores contending for the same
    address. Both cores have their own Qnodes, and the memory has a head and tail
    node. We call the cores *A* and *B* for simplicity.


    *a) LRwait:* Core A issues an LRwait request to the memory (1). Since the queue
    is initially empty, the head and tail nodes are set to A, and a reservation to
    the specified location is set up. The memory then sends the value A (2). During
    or after the described events, B''s LRwait request arrives at the memory (3).
    When the B''s LRwait request arrives at the memory, the controller appends B at
    the tail of the queue and then adds it as the successor to A. This is done by
    sending a so-called *SuccessorUpdate* to A (4). This SuccessorUpdate writes to
    A''s Qnode to make it point to B. In this final state shown in the top half of
    [Fig. 2,](#page-3-1) A and B form a queue with A at the head of the queue. At
    this point, A can issue an SCwait while B is sleeping, waiting for a response.


    *b) SCwait:* Core A finishes its LRSCwait pair by issuing an SCwait with the modified
    value (5). Immediately after an SCwait passes the Qnode, it sends a *WakeUpRequest*
    to the memory containing its successor, i.e., B (6). On arrival of the SCwait
    request at the memory, the head node and reservation


    ![](_page_3_Figure_7.jpeg)


    <span id="page-3-1"></span>Fig. 2. LRwait and SCwait sequence in Colibri with
    two cores and one queue.


    are checked. If everything is valid, the head node is temporarily invalidated
    to prevent a future SCwait from the same core from succeeding without reservation,
    and the SCwait is written to memory. The WakeUpRequest sets the head node to the
    successor node and triggers an LRwait response with the latest memory value written
    by A, i.e., for B (7). Core B is now free to issue an SCwait. Finally, the head
    and tail nodes point to B since B is the only core in the queue.


    This sequence can be generalized to more cores. Qnodes accept SuccessorUpdates
    even when the core is asleep, allowing the queue to be enlarged independent of
    the cores'' state.


    ## *A. Correctness of Colibri*


    *1) LRwait:* When an LRwait enqueues a node, it must update the tail to point
    to the newly enqueued node and append it to the previous tail node if it existed.
    If not, the enqueue operation inherently becomes atomic. Otherwise, to update
    the predecessor, the memory controller sends a SuccessorUpdate to the previous
    tail and overwrites the tail node atomically. Since we can only have one LRwait
    per core and SuccessorUpdates are only sent when overwriting a tail node, only
    a single SuccessorUpdate will ever be in flight to a Qnode, guaranteeing no lost
    links in the queue. If the SuccessorUpdate arrives after the core issued an SCwait,
    it will immediately bounce back as a WakeUpRequest. If the next LRwait arrives
    while the SuccessorUpdate is still in flight, the tail will be updated again,
    and the SuccessorUpdate will be sent to the next core. While a glance at the Qnodes
    might reveal broken links momentarily, the links only have to be made when a core
    issues its SCwait, which requires an LRwait response from the memory controller
    since memory transactions are ordered, this will always happen after the SuccessorUpdate.


    *2) SCwait:* If a core issuing an SCwait is the only one in the queue, i.e., the
    head and tail are equal, dequeuing itself by clearing the head and tail is trivial.
    Otherwise, the SCwait will invalidate the head node while leaving the value unchanged.
    A core would need to overwrite the head node to reach an inconsistent queue from
    this stage. This is only allowed for an LRwait reaching an empty queue or a WakeUpRequest
    arriving at the memory after invalidating the head node by an SCwait. A WakeUpRequest
    can only be triggered by an SCwait passing the Qnode, which can only be sent by
    a core at the head of the queue since the other cores are still waiting for their
    LRwait response. Thus, the WakeUpRequest arriving at the memory node guarantees
    that the queue is in a consistent state again.


    ## *B. Extending Colibri with Mwait*


    A core can issue an Mwait request to enqueue into Colibri''s queue to monitor
    a memory location. The memory controller then waits for a write to the monitored
    location, just like for LRwait''s reservation. After a write, the memory controller
    triggers a response to the Mwait instruction. For Mwait, the head node is sleeping
    as well in contrast to LRSCwait where the head is free to issue an SCwait. The
    Mwait response makes the Qnode dispatch the WakeUpReq for its successor, which
    then bounces to the memory controller, where the next Mwait response is released.
    In contrast to LRSCwait, the whole reservation queue is woken up without any interference
    from the cores.


    <span id="page-4-1"></span>TABLE I AREA OF A M E M P O O L\_T I L E WITH DIFFERENT
    LRSCWAIT DESIGNS.


    | Architecture            | Parameters    | Area[kGE] | Area[%] |

    |-------------------------|---------------|-----------|---------|

    | MemPool tile            | none          | 691       | 100.0   |

    | with LRSCwait1          | 1 queue slot  | 790       | 116.4   |

    | with LRSCwait8          | 8 queue slots | 865       | 127.4   |

    | with Colibri with MWait | 1 address     | 732       | 105.9   |

    | with Colibri with MWait | 2 addresses   | 750       | 108.5   |

    | with Colibri with MWait | 4 addresses   | 761       | 110.1   |

    | with Colibri with MWait | 8 addresses   | 802       | 116.3   |


    ![](_page_4_Figure_2.jpeg)


    <span id="page-4-2"></span>Fig. 3. Throughput of different LRSCwait implementations
    and standard RISC-V atomics at varying contention.


    ## V. RESULTS


    <span id="page-4-0"></span>We implement and evaluate various LRSCwait variations
    and Colibri in MemPool, an open-source, 256-core RISC-V system with 1024 SPM banks
    [\[5\]](#page-5-4). All our results are taken from cycle-accurate register-transfer
    level (RTL) simulation. Physical implementation results come from implementing
    MemPool in GlobalFoundries'' 22FDX fully depleted silicon-on-insulator (FD-SOI)
    technology. Power consumption is evaluated in typical conditions (TT/0.80 V/25
    °C), with switching activities from a post-layout gate-level simulation running
    at 600 MHz.


    The area overhead of different implementations is shown in [Table I.](#page-4-1)
    Even optimized implementations of LRSCwait quickly grow in size, while LRSCwaitideal
    is physically infeasible for a system of MemPool''s scale. Colibri, on the other
    hand, grows linearly and allows up to eight queues per memory controller with
    a similar area overhead to LRSCwait<sup>1</sup> of 16%.


    ## *A. Benchmarking*


    *a) Histogram:* We implement a concurrent histogram benchmark to evaluate Colibri''s
    performance at different levels of contentions. The application atomically increments
    a parametrizable number of bins. The fewer bins, the higher the contention. We
    increment a bin with different atomic operations and compare their performance
    as updates per clock cycle.


    The throughput of different LRSCwait implementations is shown in [Fig. 3.](#page-4-2)
    LRSCwaitideal outperforms all implementations across the whole spectrum of contention.
    The optimized implementations show similar performance at low contention but achieve
    much lower performance when the contention is higher than their number of reservations.
    Finally, Colibri achieves nearideal performance across all contentions. The slight
    performance


    ![](_page_4_Figure_10.jpeg)


    <span id="page-4-3"></span>Fig. 4. Throughput of different lock implementations
    compared to generic RMW atomics at varying contention.


    penalty comes from the extra roundtrips of Colibri''s node update messages. Colibri
    outperforms the LRSC-based implementation by a factor of 6.5× at high contention
    and 13% at low contention. For completeness, we also show the throughput of an
    *Atomic Add* implementation, which is designed specifically to increment a memory
    location atomically and represents the plot''s roofline. However, most concurrent
    algorithms need more complex atomic RMW operations than an increment, where programmers
    have to resort to locks of generic RMW atomics like LRSCwait.


    [Fig. 4](#page-4-3) compares Colibri to various lock-based implementations. Colibri,
    LRSC, and Atomic Add locks are spin locks with a backoff of 128 cycles, while
    Mwait lock implements an MCS lock, where Mwait is used to avoid polling. Colibri
    outperforms all other approaches for any contention. We observe that the LRSC
    and AMO-lock approaches perform worst at high contention due to their heavy polling
    and retry traffic, while waiting-based approaches perform average. At low contention,
    the waiting-based approaches perform worst because of their management overhead,
    while the other atomics tend to Colibri.


    *b) Interference:* We showed that LRSCwait can significantly improve the throughput
    of atomic operations across all levels of contention. On top of this increase
    in performance, eliminating the need to retry failed operations and polling also
    reduces traffic and frees up resources for cores not executing atomics. Cores
    working on computation experience less negative interference from the constant
    polling of atomics. To measure this effect, we partitioned the 256 cores of MemPool
    to either work on a matrix multiplication or to execute atomic operations. We
    measure the execution time of the matrix multiplication compared to an execution
    time without any interference. [Figure 5](#page-5-14) shows the relative performance
    for various types of atomic operations and distributions of working cores. Our
    Colibri implementation has a negligible impact on the worker cores, even at high
    contention and with a poller-to-worker ratio of 252:4. The retries of the LRSC
    operations, on the other hand, significantly impact the workers'' performance,
    despite a backoff of 128 cycles. At the same ratio of poller-to-workers, the LRSC
    implementation slows the workers down to 26%.


    *c) Queue:* To evaluate Colibri on a commonly used concurrent algorithm, we implement
    an MCS queue with LRSC and LRSCwait, as well as a lock-based queue using atomic
    adds. Concurrent queues are widely used for task scheduling


    ![](_page_5_Figure_0.jpeg)


    <span id="page-5-14"></span>Fig. 5. Matrix multiplication performance with interference
    from atomics. The poller-to-worker ratio is annotated in the figure with poller:worker.


    ![](_page_5_Figure_2.jpeg)


    <span id="page-5-15"></span>Fig. 6. Queue operations throughput with different
    atomics.


    or producer/consumer pipelines. [Figure 6](#page-5-15) shows the number of queue
    operations for a range of cores accessing a single queue. Colibri performs best
    and can sustain a high performance even at 256 cores. It outperforms the LRSC
    and lock-based approaches by 1.54× and 1.48× times with eight cores before both
    implementations drop in performance due to excessive retries and polling. At 64
    cores, Colibri is 9× faster. The shaded areas show each implementation''s slowest
    and fastest core performance range. It illustrates how Colibri results in a very
    balanced and fair workload distribution, while LRSC can have very big variations.


    *d) Energy efficiency:* [Table II](#page-5-16) shows the energy per operation
    for atomic accesses to the histogram at the highest contention. Comparing Colibri
    to the Atomic Add, which represents an ideal atomic update, we can see how energyefficient
    Colibri is for a generic RMW operation that consists of an LRwait, add, and SCwait
    operation. Compared to the LRSC or lock-based implementation, we observe the large
    benefit of the reduction in polling and retry traffic for improving energy efficiency
    by a factor of 7.1× and 8.8×.


    ## VI. CONCLUSION


    In this work, we propose the LRwait and Mwait synchronization primitives and their
    implementation, Colibri, which demonstrate a novel and effective solution for
    the LRSC synchronization problem in cache-less manycore systems. Colibri offers
    superior performance and scalability compared to existing hardware and software
    approaches, reduces polling, and improves throughput in a fair manner. Our experiments
    show that Colibri outperforms other implementations in both high and low contention
    scenarios by up to 6.5× and improved


    TABLE II AREA RESULTS FOR A M E M P O O L\_T I L E FOR IDEAL LRWAIT.


    <span id="page-5-16"></span>


    | Atomic access   | Backoff | Power (mW) | Energy (pJ/OP) | ∆     |

    |-----------------|---------|------------|----------------|-------|

    | Atomic Add      | 0       | 175        | 29             | −77%  |

    | Colibri         | 0       | 169        | 124            | ±0%   |

    | LRSC            | 128     | 186        | 884            | +613% |

    | Atomic Add lock | 128     | 188        | 1092           | +780% |


    energy efficiency by up to 8.8×. The polling and retries of LRSC-based solutions
    can lead to performance degradation of unrelated workers by up to 4×, while Colibri
    can operate even at high contention without impacting other cores. Additionally,
    Colibri can be easily integrated into existing RISC-V systems with a small hardware
    overhead and can be used as a drop-in replacement for LRSC or as an extension.


    ## ACKNOWLEDGMENT


    This work is funded in part by the COREnext project supported by the EU Horizon
    Europe research and innovation programme under grant agreement No. 101092598.


    ## REFERENCES


    - <span id="page-5-0"></span>[1] T. E. Anderson, "The performance of spin lock
    alternatives for sharedmemory multiprocessors," *IEEE Trans. Parallel Distrib.
    Syst.*, vol. 1, no. 1, pp. 6–16, 1990.

    - <span id="page-5-1"></span>[2] M. Herlihy, N. Shavit, V. Luchangco, and M. Spear,
    *The Art of Multiprocessor Programming*, 2nd ed., S. R. Merken, Ed. Cambridge,
    MA, USA: Morgan Kaufmann Publishers Inc., 2020.

    - <span id="page-5-2"></span>[3] GreenWaves Technologies SAS, "GAP9 next generation
    processor for hearables and smart sensors," GreenWaves Technologies SAS, Tech.
    Rep., 2021. [Online]. Available: [https://greenwaves-technologies](https://greenwaves-technologies.com/wp-content/uploads/2022/06/Product-Brief-GAP9-Sensors-General-V1_14.pdf).com/
    [wp-content/uploads/2022/06/Product-Brief-GAP9-Sensors-General-V1](https://greenwaves-technologies.com/wp-content/uploads/2022/06/Product-Brief-GAP9-Sensors-General-V1_14.pdf)
    14.[pdf](https://greenwaves-technologies.com/wp-content/uploads/2022/06/Product-Brief-GAP9-Sensors-General-V1_14.pdf)

    - <span id="page-5-3"></span>[4] R. Ginosar, P. Aviely, T. Israeli, and H. Meirov,
    "RC64: High performance rad-hard manycore," in *IEEE Aerosp. Conf. Proc.* IEEE,
    Jun. 2016, pp. 2074–2082.

    - <span id="page-5-4"></span>[5] S. Riedel, M. Cavalcante, R. Andri, and L. Benini,
    "MemPool: A scalable manycore architecture with a low-latency shared L1 memory,"
    *IEEE Trans. Comput.*, vol. 72, no. 12, pp. 3561–3575, 2023.

    - <span id="page-5-5"></span>[6] J. M. Mellor-Crummey and M. L. Scott, "Algorithms
    for scalable synchronization on shared-memory multiprocessors," *ACM Trans. Comput.
    Syst.*, vol. 9, no. 1, pp. 21–65, Feb. 1991.

    - <span id="page-5-6"></span>[7] T. B. Strøm, J. Sparsø, and M. Schoeberl, "Hardlock:
    Real-time multicore locking," *J. Syst. Archit.*, vol. 97, pp. 467–476, 2019.

    - <span id="page-5-7"></span>[8] F. Glaser, G. Tagliavini, D. Rossi, G. Haugou,
    Q. Huang, and L. Benini, "Energy-efficient hardware-accelerated synchronization
    for shared-L1 memory multiprocessor clusters," *IEEE Trans. Parallel Distrib.
    Syst.*, vol. 32, no. 3, pp. 633–648, Mar. 2021.

    - <span id="page-5-8"></span>[9] J. L. Abellan, J. Fern ´ andez, and M. E. Acacio,
    "Design of an efficient ´ communication infrastructure for highly contended locks
    in many-core cmps," *J. Parallel Distrib. Comput.*, vol. 73, no. 7, pp. 972–985,
    2013.

    - <span id="page-5-9"></span>[10] M. Monchiero, G. Palermo, C. Silvano, and O.
    Villa, "An efficient synchronization technique for multiprocessor systems on-chip,"
    *ACM SIGARCH Comput. Archit. News*, vol. 34, no. 1, pp. 33–40, Mar. 2006.

    - <span id="page-5-10"></span>[11] A. Kurth, S. Riedel, F. Zaruba, T. Hoefler,
    and L. Benini, "ATUNs: Modular and scalable support for atomic operations in a
    shared memory multiprocessor," in *ACM/IEEE Des. Autom. Conf.*, vol. 57. San Francisco,
    CA, USA: IEEE, Jul. 2020, pp. 902–907.

    - <span id="page-5-11"></span>[12] K. Asanovic´ *et al.*, "The rocket chip generator,"
    EECS Department, University of California, Berkeley, Tech. Rep. UCB/EECS-2016-17,
    Apr. 2016. [Online]. Available: http://www2.eecs.berkeley.[edu/Pubs/TechRpts/](http://www2.eecs.berkeley.edu/Pubs/TechRpts/2016/EECS-2016-17.html)
    [2016/EECS-2016-17](http://www2.eecs.berkeley.edu/Pubs/TechRpts/2016/EECS-2016-17.html).html

    - <span id="page-5-12"></span>[13] J. Gray, "Implementation of LR/SC on the GRVI
    multiprocessor," 2016. [Online]. Available: [https://groups](https://groups.google.com/a/groups.riscv.org/g/hw-dev/c/Mt9Q94f_l2w?pli=1).google.com/a/groups.riscv.org/g/hw[dev/c/Mt9Q94f](https://groups.google.com/a/groups.riscv.org/g/hw-dev/c/Mt9Q94f_l2w?pli=1)
    l2w?pli=1

    - <span id="page-5-13"></span>[14] S. Liu and J. L. Gaudiot, "Synchronization
    mechanisms on modern multicore architectures," in *Proc. 12th Asia-Pacific Conf.
    Adv. Comput. Syst. Archit.* Seoul, Korea: Springer Verlag, 2007, pp. 290–303.'
  decisions:
    evaluation_prompt: 'Qualified. Reason: The paper includes a dedicated "RESULTS"
      section with structured evaluation, including benchmarking, throughput comparisons,
      and energy efficiency analysis. It presents tables and figures showing quantifiable
      outcomes, such as throughput and energy per operation, and discusses performance
      improvements and comparisons with other methods.'
    related_work_prompt: '- Qualified. Reason: The paper meaningfully engages with
      prior research throughout its content, particularly in the "Related Work" section,
      where it discusses various existing approaches to synchronization in manycore
      systems, compares them to the proposed solution, and cites relevant academic
      sources. Additionally, the introduction and other sections reference prior work
      to provide context and justify the need for the new methods proposed in the
      paper.'
    novelty_prompt: 'Qualified. Reason: The paper proposes a novel set of instructions
      (LRwait, SCwait, and Mwait) and a scalable implementation called Colibri, which
      addresses synchronization issues in manycore systems. The paper clearly states
      its contributions, including the introduction of new methods and architectures,
      and demonstrates novelty by improving performance and scalability compared to
      existing solutions.'
    review_only_prompt: 'Qualified. Reason: The paper introduces novel contributions,
      including the LRwait and SCwait synchronization primitives, the Colibri implementation,
      and the Mwait instruction. It presents new methods, experiments, and results
      that demonstrate improvements over existing approaches, indicating original
      research and contributions.'
- title: Dimensions for Designing LLM-based Writing Support
  abstract: ''
  keywords: ''
  document: '# Dimensions for Designing LLM-based Writing Support


    Frederic Gmeiner HCI Institute Carnegie Mellon University Pittsburgh, PA, USA
    gmeiner@cmu.edu


    ### 1 INTRODUCTION


    Advances in large language models (LLMs) have enabled a myriad of unprecedented
    capabilities over the past few years: computers can write code, translate between
    languages, and generate conversations. Among the potential use cases, writing
    has been a domain that continually fascinated researchers. Nowadays, as LLMs move
    from research labs to the real world, there is a growing interest in exploring
    the capabilities of these systems to support writing tasks across domains such
    as fiction writing [\[3,](#page-1-0) [11\]](#page-1-1), scientific writing [\[7\]](#page-1-2),
    poetry [\[4\]](#page-1-3), and theatre scripts and screenplays [\[8\]](#page-1-4).


    As HCI researchers and designers, we have been "playing with" GPT to understand
    its capabilities as a design material to support writing. A common pitfall when
    designing AI experiences is the tendency to envision "holy grail use cases": places
    that require nearperfect AI performance for delivering high-quality outputs that
    match human intelligence or creativity [\[5,](#page-1-5) [10\]](#page-1-6). For
    instance, our initial experiments mostly focused on prompting the model to come
    up with novel content: writing stories, podcast scripts, characters, and plots.
    Similar to others [\[8,](#page-1-4) [11\]](#page-1-1), we were soon disappointed
    by how generic and bland the outputs were. However, over time we learned to lower
    our expectations and focus on low hanging fruit: less complex writing tasks where
    average quality LLM output could be useful. These included recommending word associations,
    listing things (e.g., places, names, objects) as inspiration, or simply reformatting
    writing (e.g., creating an outline based on prose).


    In our experience, there are three key considerations when designing LLM experiences
    for writing support: LLM capabilities, task complexity and output quality. We
    propose that these dimensions are likely to complement common taxonomy dimensions
    coming from human-centered and socio-technical perspectives, such as feasibility,
    value co-creation, usability, etc. [\[2\]](#page-1-7). In this position paper,
    we argue that a taxonomy of writing assistants capturing these dimensions holistically
    could scaffold the process of designing experiences that writers find valuable.
    The remainder of this paper details each dimension and how these could inform
    the exploration of LLM''s design space.


    ## 2 LLM CAPABILITIES AS DESIGN MATERIAL


    Gaining an understanding of AI''s capabilities and limitations is a major challenge
    for designers and end users who do not have a technical background [\[10\]](#page-1-6).
    One of our goals throughout our experiments with chatGPT was to gain a better
    understanding of what it can do, and what it can do reasonably well. At a high
    level, LLMs are "trained to predict the most likely next word given a textual
    prompt" [\[1\]](#page-1-8). However, this high-level description does not capture
    the wide range of things LLMs can do.


    We found that explicating distinct LLM capabilities was a good starting place
    to understand LLMs as a design material. We first


    Nur Yildirim HCI Institute Carnegie Mellon University Pittsburgh, PA, USA yildirim@cmu.edu


    <span id="page-0-0"></span>Table 1: Non-exhaustive list of LLM capabilities and
    example writing tasks where they might be useful.


    | LLM Capabilities     | Writing Tasks                            |

    |----------------------|------------------------------------------|

    | Text summarization   | Reviewing, Reflection                    |

    | Paraphrasing         | Refining, reviewing                      |

    | Elaboration          | Detailing, scene setting                 |

    | Dialog generation    | Writing scripts, screenplay              |

    | Story seeding        | Unblocking                               |

    | Sentence completion  | Detailing plots, dialog, etc.            |

    | Rewriting in a tone  | Reviewing, characters'' speech            |

    | Rewriting in a style | Reviewing, conveying setting, time, mood |

    | Listing              | Detailing places, characters, etc.       |

    | Formatting           | Prose to outline                         |

    | Keyword association  | Inspiration, ideation                    |


    reviewed prior literature and looked for emerging UX patterns for prompting [\[3,](#page-1-0)
    [11\]](#page-1-1). We then curated a subset of example prompts demonstrating LLM
    capabilities, such as text summarization, paraphrasing, dialog generation, elaboration,
    story seeding, sentence completion, rewriting in a tone, rewriting in a style,
    etc. (Table [1\)](#page-0-0). As we played around with each capability, we tried
    to assess the quality of outputs – however, this was dependent on the use case
    context. For example, we prompted chatGPT to "write a rap battle between Harry
    Potter and Lord Voldemort" or "write the lyrics of a song where Rousseau and Voltaire
    argue on the nature of mankind". While these prompts resulted in reasonably well
    outputs (i.e., coherence and rhyme), we did not find them particularly useful.
    We wondered whether we could produce a podcast script instead, which could be
    immediately useful. However, this seemed to be a complex writing task. Our trials
    resulted in generic scripts that professional podcasters would not find useful.
    In search of a target user group, we thought the scripts could provide value for
    content creators on Youtube who review products. A draft script for product review
    based on a few bullet points might be better than having no script.


    Through these experiments, we became aware of the interplay between the task complexity
    and output quality for a given LLM capability. In the next section, we detail
    how these dimensions are likely to impact the end user experience for writing
    support.


    ### 3 TASK COMPLEXITY AND OUTPUT QUALITY


    Writing tasks vary vastly: some are trivial, some are more challenging. Some tasks
    are tedious, while others have a great impact on writers'' enjoyment and ownership
    [\[11\]](#page-1-1). We asked two questions as we tried to gain a better sense
    of writing tasks:


    - (1) How complex is this writing task for a user to do? Does it require a high
    level of expertise or a deep contextual understanding?

    - (2) What is the level of quality for LLM outputs to be perceived as useful?
    Does the task require particular qualities to be acceptable? (e.g., factuality,
    consistency, etc.)


    <span id="page-1-10"></span>Figure 1: Task Complexity-Output Quality matrix for
    LLM capabilities. Our exploration revealed that writing tasks where average quality
    outputs are acceptable, provide a rich design space for LLM experiences.


    ![](_page_1_Figure_3.jpeg)


    Average *AccepWable OXWpXW QXaliW\* E[cellent


    We started to map the writing tasks we have been thinking about based on the task
    complexity and required output quality. Prior work delineated writing tasks based
    on the writing goals for specific writing processes (i.e., planning, translation,
    reviewing) and noted that each part of the writing process might impose different
    levels of constraints [\[6\]](#page-1-9). In our case, we focused on the quality
    of output as a key dimension that makes or breaks the usefulness of LLMs for writing
    support.


    We realized that our initial explorations mostly focused on complex writing tasks
    that required high quality outputs: things such as generating original story plots,
    podcast content, characters, or styles. These tasks were part and parcel of the
    writing process where writer expertise and involvement were high (Figure [1\)](#page-1-10).
    On the other hand, we found many writing tasks that were relatively less complex.
    Things such as asking for inspirational keywords, word or sentence completion,
    listing names or places, and rewriting sentences to be longer or more concise
    – these were places where vague, unrelated, or even inconsistent outputs could
    be useful. For example, we prompted chatGPT to play a word association game that
    could help us describe a mood and feel in a story. We found the listed words useful
    even though we did not necessarily use them, as they sensitized us to better think
    and reflect on our story.


    We view the task complexity-output quality mapping as a valuable perspective to
    navigate LLM''s design space. Can researchers, designers, and technologists think
    of low complexity writing tasks where average quality LLM outputs could be useful?
    What are the ways LLMs can support high complexity writing tasks without


    providing original, factual, or consistent outputs? Similar to others [\[1\]](#page-1-8),
    we suspect that more contained and tedious writing tasks lend themselves better
    for LLM support and AI-assisted writing in general.


    From a machine learning perspective, the classification of LLM capabilities within
    a complexity-output quality matrix could also serve as guidance for model developers
    for improving the support of specific writing tasks – for example, when part of
    a reinforcement learning from human feedback approach. Furthermore, for developing
    writing assistive tools that can adapt to users'' expertise and context, a complexity-output
    quality matrix might help determine which capabilities best support different
    users'' needs and expectations.


    ### 4 PROMPTS FOR WORKSHOP DISCUSSION


    Below, we highlight two open questions as starting points for workshop discussion:


    - What dimensions should a taxonomy of writing assistants capture? Our exploration
    focused on dimensions that are critical for finding use cases for LLM-based writing
    support. However, this is a partial perspective; there are many critical dimensions
    for designing LLM experiences, including factuality, bias, stereotyping, and homogenization.
    Recent work that proposed a taxonomy of LLM risks [\[9\]](#page-1-11) could provide
    a starting point for further dimensions to consider.

    - How to assess the success of LLM-based writing support? Evaluating writing assistants
    is an open research area. The workshop could expand on dimensions that could be
    used to assess LLM experiences.


    ### REFERENCES


    - <span id="page-1-8"></span>[1] 2022. Wordcraft Writers Workshop.<https://g.co/research/wordcraft>

    - <span id="page-1-7"></span>[2] Robert P Bostrom and J Stephen Heinen. 1977.
    MIS problems and failures: A socio-technical perspective. Part I: The causes.
    MIS quarterly (1977), 17–32.

    - <span id="page-1-0"></span>[3] Alex Calderwood, Vivian Qiu, Katy Ilonka Gero,
    and Lydia B Chilton. 2020. How Novelists Use Generative Language Models: An Exploratory
    User Study.. In HAI-GEN+ user2agent@ IUI.

    - <span id="page-1-3"></span>[4] Tuhin Chakrabarty, Vishakh Padmakumar, and He
    He. 2022. Help me write a poem: Instruction Tuning as a Vehicle for Collaborative
    Poetry Writing. arXiv preprint arXiv:2210.13669 (2022).

    - <span id="page-1-5"></span>[5] Graham Dove, Kim Halskov, Jodi Forlizzi, and
    John Zimmerman. 2017. UX design innovation: Challenges for working with machine
    learning as a design material. In Proceedings of the 2017 chi conference on human
    factors in computing systems. 278–288.

    - <span id="page-1-9"></span>[6] Katy Gero, Alex Calderwood, Charlotte Li, and
    Lydia Chilton. 2022. A design space for writing support tools using a cognitive
    process model of writing. In Proceedings of the First Workshop on Intelligent
    and Interactive Writing Assistants (In2Writing 2022). 11–24.

    - <span id="page-1-2"></span>[7] Katy Ilonka Gero, Vivian Liu, and Lydia Chilton.
    2022. Sparks: Inspiration for science writing using language models. In Designing
    Interactive Systems Conference. 1002–1019.

    - <span id="page-1-4"></span>[8] Piotr Mirowski, Kory W Mathewson, Jaylen Pittman,
    and Richard Evans. [n. d.]. Co-writing screenplays and theatre scripts alongside
    language models using Dramatron. ([n. d.]).

    - <span id="page-1-11"></span>[9] Laura Weidinger, Jonathan Uesato, Maribeth Rauh,
    Conor Griffin, Po-Sen Huang, John Mellor, Amelia Glaese, Myra Cheng, Borja Balle,
    Atoosa Kasirzadeh, et al. 2022. Taxonomy of risks posed by language models. In
    2022 ACM Conference on Fairness, Accountability, and Transparency. 214–229.

    - <span id="page-1-6"></span>[10] Qian Yang, Aaron Steinfeld, Carolyn Rosé, and
    John Zimmerman. 2020. Reexamining whether, why, and how human-ai interaction is
    uniquely difficult to design. In Proceedings of the 2020 chi conference on human
    factors in computing systems. 1–13.

    - <span id="page-1-1"></span>[11] Ann Yuan, Andy Coenen, Emily Reif, and Daphne
    Ippolito. 2022. Wordcraft: story writing with large language models. In 27th International
    Conference on Intelligent User Interfaces. 841–852.'
  decisions:
    evaluation_prompt: 'Qualified. Reason: The paper includes structured evaluation
      through experiments with chatGPT, assessing the quality of outputs for various
      writing tasks. It discusses the interplay between task complexity and output
      quality, and presents a Task Complexity-Output Quality matrix (Figure 1) as
      part of the evaluation. Additionally, Table 1 lists LLM capabilities and example
      writing tasks, indicating a structured approach to understanding LLM performance
      in writing support.'
    related_work_prompt: 'Qualified. Reason: The paper meaningfully engages with prior
      research throughout the text, including in the Introduction and other sections.
      It includes numerous academic citations that provide context and discussion,
      compares its methods to previous work, and discusses prior research in relation
      to designing LLM-based writing support.'
    novelty_prompt: 'Qualified. Reason: The paper proposes a novel taxonomy for designing
      LLM-based writing support by introducing dimensions such as LLM capabilities,
      task complexity, and output quality. It also suggests a complexity-output quality
      matrix as a tool for navigating the design space of LLMs, which could guide
      model developers and assistive tool designers. These contributions indicate
      novelty in the context of designing writing support systems using LLMs.'
    review_only_prompt: 'Qualified. Reason: The paper proposes a new taxonomy for
      designing LLM-based writing support, introduces original experiments with LLM
      capabilities, and discusses novel dimensions for writing assistants, indicating
      novel contributions beyond merely summarizing existing work.'
- title: 'KG4Diagnosis: A Hierarchical Multi-Agent LLM Framework with Knowledge Graph
    Enhancement for Medical Diagnosis'
  abstract: 'Integrating Large Language Models (LLMs) in healthcare diagnosis demands
    systematic frameworks that can handle complex medical scenarios while maintaining
    specialized expertise. We present KG4Diagnosis, a novel hierarchical multi-agent
    framework that combines LLMs with automated knowledge graph construction, encompassing
    362 common diseases across medical specialties. Our framework mirrors real-world
    medical systems through a two-tier architecture: a general practitioner (GP) agent
    for initial assessment and triage, coordinating with specialized agents for in-depth
    diagnosis in specific domains. The core innovation lies in our end-to-end knowledge
    graph generation methodology, incorporating: (1) semantic-driven entity and relation
    extraction optimized for medical terminology, (2) multi-dimensional decision relationship
    reconstruction from unstructured medical texts, and (3) human-guided reasoning
    for knowledge expansion. KG4Diagnosis serves as an extensible foundation for specialized
    medical diagnosis systems, with capabilities to incorporate new diseases and medical
    knowledge. The framework''s modular design enables seamless integration of domain-specific
    enhancements, making it valuable for developing targeted medical diagnosis systems.
    We provide architectural guidelines and protocols to facilitate adoption across
    medical contexts.'
  keywords: ''
  document: '## Introduction


    Knowledge graphs (KGs) have emerged as transformative tools across numerous domains,
    showcasing their ability to organize complex datasets and support advanced reasoning
    and decision-making. In finance, KGs play a pivotal role in risk assessment and
    fraud detection by linking disparate financial datasets to uncover hidden patterns
    and relationships. For example, the application of KGs in detecting fraudulent
    related party transactions enables financial institutions to model complex interdependencies
    between entities, improving accuracy in identifying fraudulent activities (Zhang,
    Li, and Wang 2023). Similarly, in education, KGs enhance personalized learning
    by structuring knowledge from vast academic resources to recommend tailored learning
    paths. A notable implementation includes the use of KGs to integrate data from
    curriculum design, student assessments, and teaching resources, creating adaptive
    systems that improve student engagement and outcomes. In manufacturing, knowledge
    graphs (KGs) enable automation and optimization of processes by integrating heterogeneous
    data sources. A recent study highlighted their role in Reconfigurable Manufacturing
    Systems (RMS), where semantic models and KGs support automated asset capability
    matching and reconfiguration solutions. This approach demonstrated significant
    improvements in efficiency, cost reduction, and productivity by leveraging structured
    knowledge for dynamic decision-making in manufacturing systems (Mo et al. 2024).
    Du et al. constructed highly efficient manufacturing knowledge graphs using multi-feature
    fusion technology, which has been successfully used in automobile manufacturing
    (Du et al. 2022).


    In the medical domain, KGs (Abdulla, Mukherjee, and Ranganathan 2023; Alam, Giglou,
    and Malik 2023; Wu et al. 2024) serve as crucial infrastructure for organizing
    diverse healthcare data and supporting clinical decisionmaking. However, constructing
    and reasoning over medical KGs (Abdulla, Mukherjee, and Ranganathan 2023; Al Khatib
    et al. 2024), particularly from unstructured and multimodal data, presents significant
    challenges that existing approaches have not fully addressed.


    Current methods for medical KG construction span traditional rule-based systems
    to advanced AI models. Rulebased and ontology-driven approaches using SNOMED-CT
    (Chang and Mostafa 2021) and UMLS (Amos et al. 2020) offer reliability but lack
    scalability and struggle with unstructured data. While Large Language Models (LLMs)
    like GPT (OpenAI 2022, 2023; Touvron et al. 2023; Garc´ıa-Ferrero et al. 2024)
    and MedPaLM (Qian et al. 2024) show promise in generating structured knowledge
    from unstructured data, they face challenges with hallucination and accuracy (Huang
    et al. 2023; Tonmoy et al. 2024; Guo et al. 2024). Hybrid approaches incorporating
    Graph Neural Networks (GNNs) attempt to balance symbolic reasoning with deep learning
    but remain computationally complex and dependent on well-structured inputs (Zhang
    2021; Zhang et al. 2024; Shuifa et al. 2023).


    For diagnosis and treatment, medical KGs provide a critical foundation for identifying
    patterns and relationships within patient data, medical literature, and clinical
    guidelines (Li et al. 2020). In diagnosis, KGs help map symp-


    <sup>\*</sup>Corresponding author.


    Copyright © 2025, Association for the Advancement of Artificial Intelligence (www.aaai.org).
    All rights reserved.


    toms to potential conditions, identify relevant tests, and prioritize differential
    diagnoses (Tang et al. 2023). In treatment, KGs assist in recommending personalized
    treatment plans based on patient-specific factors such as comorbidities, drug
    interactions, and genetic markers (Bonner et al. 2022). These processes enhance
    clinical decision-making by offering structured, evidence-based recommendations.


    To address limitations in current methods and enhance the overall clinical workflow,
    we propose KG4Diagnosis, a novel end-to-end framework for the construction, diagnosis,
    treatment and reasoning of automated medical knowledge graphs. Our framework uniquely
    integrates a hierarchical multi-agent architecture, mirroring real-world medical
    systems: a general practitioner (GP) agent conducts the initial assessment and
    triage before coordinating with specialized agents for domain-specific analysis.
    This approach combines the broad capabilities of LLMs with the precision of specialized
    medical knowledge, ensuring accurate diagnosis, personalized treatment suggestions,
    and enhanced clinical decision-making.


    The framework innovatively incorporates advanced techniques for semantic entity
    extraction, decision-making reconstruction, and scalable knowledge expansion,
    specifically designed to handle unstructured and multimodal medical data. By bridging
    the gap between traditional KG approaches and modern AI capabilities, KG4Diagnosis
    aims to enable more robust and adaptable healthcare decision support systems.


    In this paper, we make the following key contributions:


    - We propose KG4Diagnosis, a novel hierarchical multiagent framework that mirrors
    real-world medical systems, consisting of a GP agent for initial assessment and
    specialized agents for domain-specific diagnosis across 362 common diseases.

    - We develop an innovative end-to-end knowledge graph construction pipeline incorporating
    three key components: semantic-driven entity extraction, multidimensional decision
    relationship reconstruction, and human-guided reasoning for knowledge expansion.

    - We implement robust mechanisms to address LLM hallucination challenges in medical
    diagnosis through multiagent verification and knowledge graph constraints, validated
    using comprehensive benchmarks.

    - We demonstrate the framework''s practical value through real-world healthcare
    scenarios.

    - We provide a modular and extensible architecture that supports the seamless
    integration of new medical domains and knowledge, with detailed implementation
    protocols for widespread adoption in various medical contexts.


    ## Methodology


    ### System Architecture Overview


    KG4Diagnosis is designed as a hierarchical multi-agent framework that integrates
    LLMs with automated knowledge graph construction for medical diagnosis (see Figure
    1). The system architecture consists of two primary components: a knowledge graph
    construction pipeline that processes and structures medical knowledge and a Camel-based
    multiagent system that enables hierarchical medical decisionmaking. This design
    mirrors real-world medical practices, where general practitioners collaborate
    with specialists to provide comprehensive patient care (see Figure 2).


    ### Knowledge Graph Construction Pipeline


    This framework implements a three-stage process for the automated construction
    of a medical knowledge graph. Initially, medical documents are segmented into
    data chunks that adhere to the contextual constraints of the knowledge graph.
    Subsequently, a semantic-driven entity and relationship extraction module is employed
    to extract entities and relationships from these data chunks. This process leverages
    BioBERT, a model specifically designed for the biomedical domain, which ensures
    the precise extraction of medical entities and the identification of relationships
    between them. In the following stage, based on the extracted entities and relationships,
    a knowledge graph is constructed, thereby facilitating its automatic generation.
    We also enhance the medical knowledge graph by using LLMs to identify broader,
    context-aware entities and relations, complementing BioBERT''s domain-specific
    extractions.


    In the last stage, the expansion and validation of the knowledge graph will be
    facilitated through expert evaluation. Medical experts will manually validate
    the relationships that have been constructed, and the verified knowledge will
    be used to train large-scale models to facilitate future knowledge expansion.


    The details of each part of the construction pipeline are as follows:


    Stage 1: Data Chunking and Segmentation In the first stage, medical documents
    are segmented into data chunks based on contextual constraints. Let D = {d1, d2,
    . . . , dn} represent a set of medical documents. Each document d<sup>i</sup>
    is segmented into m data chunks:


    $$C\_i = \{c\_{i1}, c\_{i2}, \dots, c\_{im}\}$$


    These chunks cij are generated using context-based segmentation rules. The segmentation
    process can be mathematically represented as:


    $$f\_{\text{seg}}(d\_i) \to C\_i$$


    where fseg is a function that maps a document d<sup>i</sup> to a set of data chunks
    C<sup>i</sup> .


    Stage 2: Semantic-driven Entity and Relationship Extraction The pipeline leverages
    BioBERT''s contextual embeddings along with medical ontologies, such as SNOMED-CT
    and UMLS, to extract entities and relationships from the segmented data chunks.
    The process of extraction can be represented as follows:


    • *Entity Extraction:* The set of extracted entities E is defined as:


    $$E = \{e\_1, e\_2, \dots, e\_n\}$$


    where E represents the set of medical entities, such as diseases, drugs, symptoms,
    etc.


    ![](_page_2_Figure_0.jpeg)


    Figure 1: An overview of the KG4Diagnosis framework. The system includes the following
    components: (1) input medical text is segmented into chunks and processed through
    entity extraction and relation extraction modules; (2) extracted entities and
    relations are stored in dedicated databases; (3) these databases are utilized
    to construct the medical KG; (4) the medical KG is integrated with LLMs and MAS
    to enhance diagnostic reasoning; (5) diagnostic responses are delivered to user
    endpoints, supported by human-guided reasoning. The framework highlights a structured
    approach to medical text processing, accurate knowledge graph construction, and
    collaborative reasoning for advanced diagnostic outcomes.


    **Diagnosis Example**


    ![](_page_2_Figure_3.jpeg)


    Figure 2: An example of a diagnostic conversation illustrating interactions between
    a patient, a doctor, and an AI medical assistant. The patient describes symptoms,
    the doctor asks clarifying questions, and the AI provides explanations and suggestions.
    This dialogue highlights the collaborative diagnostic process and how AI systems
    can assist in providing personalized medical advice.


    • *Relationship Extraction:* The set of relationships R between entities e<sup>i</sup>
    and e<sup>j</sup> is represented as:


    $$R = \{ (e\_i, r, e\_j) \mid e\_i, e\_j \in E \}$$


    where r denotes the relationship between entities e<sup>i</sup> and


    e<sup>j</sup> that are extracted from the medical text.


    In this stage, BioBERT captures the semantic meaning of the medical text and maps
    it to standardized medical ontologies, ensuring accurate entity and relationship
    extraction.


    Stage 3: Knowledge Graph Construction Once entities and relationships are extracted,
    a knowledge graph is constructed. A knowledge graph can be represented as a graph
    G, where:


    $$G = (V, E)$$


    Here, the set of nodes V = {e1, e2, . . . , ek} represents the medical entities,
    and the set of edges E = {r1, r2, . . . , rl} represents the relationships between
    these entities.


    The construction of the knowledge graph is based on the extracted entities and
    relationships. Thus, the knowledge graph can be represented as:


    $$G = (V, E) \quad \text{where} \quad V = E \text{ and } E = R$$


    The nodes represent entities, and the edges represent relationships.


    Stage 4: LLM-Augmented Knowledge Graph We utilize LLMs to enhance the medical
    knowledge graph by identifying entities and relations that extend beyond BioBERT''s
    extraction capabilities. While BioBERT excels in precise, domain-specific extractions
    within the biomedical field, LLMs contribute broader, context-aware semantic extractions,
    especially from complex or ambiguous medical texts. The enriched entities and
    relations are stored in dedicated databases and integrated into the knowledge
    graph, which is then optimized for reasoning with LLMs. This enhanced knowledge
    graph supports advanced diagnostic workflows by enabling more robust reasoning
    and decision-making through the synergistic capabilities of multi-agent systems
    and LLM-driven diagnostic reasoning.


    Stage 5: Human-Guided Reasoning In this final stage, expert validation is crucial
    in ensuring the quality and accuracy of the constructed relationships and entities
    in the knowledge graph. The expert validation process involves active learning
    and reinforcement learning techniques to expand the graph with verified and reliable
    information.


    - *Expert Validation of Relationships:* Medical experts manually review the extracted
    relationships R between entities to validate their clinical relevance. If a relationship
    (e<sup>i</sup> , r, e<sup>j</sup> ) is confirmed to be accurate, it is retained
    in the knowledge graph. If a relationship is deemed invalid or uncertain, it is
    either corrected or removed.

    - *Graph Expansion with Expert-Verified Relationships:* After validation, the
    knowledge graph is expanded by incorporating new, expert-verified entities and
    relationships. The validated graph is enriched with these confirmed connections,
    improving the graph''s reliability and comprehensiveness.


    Gexpanded = G ∪ Validated Entities and Relationships


    where Gexpanded represents the expanded knowledge graph that includes both previously
    extracted and expertverified entities and relationships.


    Through this expert-guided validation and expansion process, the knowledge graph
    evolves into a robust and reliable resource for medical research and clinical
    decision-making.


    ### Hierarchical Multi-Agent Framework for Medical Diagnosis


    To address the complexity of medical diagnostic reasoning, we developed a hierarchical
    multi-agent framework that processes user queries for diagnosis. This framework
    integrates a General Practitioner Large Language Model (GP-LLM) and multiple domain-specific
    Consultant Large Language Models (Consultant-LLMs). The diagnostic process is
    mathematically modelled as follows:


    GP-LLM: Primary Diagnostic Agent The GP-LLM serves as the initial interface for
    analyzing user queries. Let the user query be denoted by q ∈ Q, where Q is the
    set of all possible user queries. The diagnostic confidence for a query q producing
    a preliminary diagnosis x is defined as:


    $$P\_{\rm GP}(x \mid q) = f\_{\rm GP}(q) \tag{1}$$


    where PGP(x | q) ∈ [0, 1] is the confidence assigned by the GP-LLM to the diagnosis
    x, and fGP represents the probabilistic diagnostic function based on a broad-spectrum
    knowledge base.


    The GP-LLM initiates a referral when:


    $$P\_{\text{GP}}(x \mid q) < \tau \quad \text{or} \quad x \in X\_s \tag{2}$$


    Here:


    • τ is the confidence threshold for referral (set to 0.7).


    • X<sup>s</sup> ⊂ X is the subset of diagnoses requiring specialized expertise.


    The output of the GP-LLM is expressed as:


    $$\text{Output}\_{\text{GP}} = \begin{cases} \text{Reflex1 to Consulant-L.LM},
    & \text{if } P\_{\text{GP}}(x \mid q) < \tau \text{ or } x \in X\_{\text{s}},\\
    \text{Diagnosis: } x, & \text{otherwise.} \end{cases} \tag{3}$$


    Consultant-LLMs: Specialized Diagnostic Agents Each Consultant-LLM is optimized
    for a specific medical domain, such as rheumatology. Let Agent<sup>i</sup> represent
    the i th Consultant-LLM, where i = 1, 2, . . . , n and n = 4 (cardiology, neurology,
    endocrinology and rheumatology) in this framework. The confidence function for
    Agent<sup>i</sup> diagnosing a condition y from query q is defined as:


    $$P\_{\text{Agent}\_i}(y \mid q) = f\_{\text{Agent}\_i}(q) \tag{4}$$


    where PAgent<sup>i</sup> (y | q) ∈ [0, 1] and fAgent<sup>i</sup> is the probabilistic
    diagnostic function based on domain-specific training datasets and clinical guidelines.


    For cases requiring collaborative reasoning between multiple agents, the final
    diagnosis confidence is computed as:


    $$P\_{\text{final}}(z \mid q) = \sum\_{i=1}^{n} w\_i P\_{\text{Agent}\_i}(z \mid
    q) \tag{5}$$


    where w<sup>i</sup> represents the weight assigned to Agent<sup>i</sup> ''s contribution,
    normalized such that P<sup>n</sup> <sup>i</sup>=1 w<sup>i</sup> = 1.


    Inter-Agent Communication Protocol The referral and communication processes ensure
    the seamless transfer of cases and collaborative refinement. Let T(A, B, q) denote
    the transfer of the user query q from agent A to agent B. The transfer function
    is modeled as:


    $$T(A, B, q) = \phi(q), \quad \phi: Q \to Q'' \tag{6}$$


    where ϕ transforms q into a format compatible with the receiving agent B. Feedback
    to the GP-LLM updates its knowledge base KGP as follows:


    $$K\_{\rm GP}^{(t+1)} = K\_{\rm GP}^{(t)} + \Delta K \tag{7}$$


    where ∆K is the incremental knowledge derived from Consultant-LLMs.


    Referral Decision Threshold The referral decision is mathematically defined as:


    $$\text{Referal} = \begin{cases} 1, & \text{if } P\_{\text{GP}}(x \mid q) < \tau
    \text{ or } x \in X\_s \\ 0, & \text{otherwise.} \end{cases} \quad (8)$$


    Here:


    - Referral = 1 indicates escalation to a Consultant-LLM.

    - Referral = 0 implies retention of the query within the GP-LLM.


    Advanced Diagnosis with Multi-Agent Collaboration For complex queries requiring
    input from multiple Consultant-LLMs, the final diagnosis confidence is calculated
    as:


    $$P\_{\text{final}}(z \mid q) = \frac{1}{n} \sum\_{i=1}^{n} P\_{\text{Agent}\_i}(z
    \mid q), \quad z \in Z \qquad (9)$$


    where Z ⊂ X represents the space of complex diagnoses requiring multi-domain expertise.


    Summary This modelling formalizes the diagnostic reasoning within the hierarchical
    multi-agent framework. The confidence functions PGP(x | q), PAgent<sup>i</sup>
    (y | q), and Pfinal(z | q) define the probabilistic outputs of the GP-LLM, individual
    Consultant-LLMs, and the collaborative multi-agent system, respectively. The confidence
    threshold (τ = 0.7) ensures accurate and efficient escalation to specialized diagnostic
    agents when necessary.


    ### Future Training and Evaluation Work


    The system''s training approach encompasses a comprehensive coverage of 362 common
    diseases across multiple medical specialties, representing a significant scope
    in medical diagnosis. The training process is strategically designed to be multi-faceted,
    combining general medical knowledge with specialized domain expertise. For each
    disease category, we implement targeted fine-tuning protocols for the respective
    specialist agents, ensuring deep domain-specific knowledge while maintaining coherent
    integration within the broader framework.


    The example of the knowledge graph presented by Figure 3, 4 showcases two advanced
    obesity medications (Ozempic and Wegovy), demonstrating how our framework effectively
    simulates real-world clinical consultations. The full structure of the knowledge
    graph resulting, as illustrated in Figure 5 demonstrates the complex interconnections
    between different entities of disease, symptoms, and diagnostic patterns. The
    visualization reveals the hierarchical nature of medical knowledge organization,
    with clear pathways from general diagnostic patterns to specialized medical domains.
    This structure enables efficient knowledge navigation and supports the system''s
    hierarchical decision-making processes.


    Our continuous learning mechanism enhances the initial training through dynamic
    agent interactions and feedback loops. This approach allows the system to evolve
    and refine its diagnostic capabilities over time, adapting to new medical insights
    and patterns identified through agent collaboration. The framework, implemented
    using PyTorch for neural network components and Neo4j for knowledge graph management,
    currently encompasses all 362 diseases in its knowledge base, with structured
    pathways for knowledge expansion.


    Given the framework''s comprehensive scope and innovative approach to medical
    diagnosis, a comprehensive benchmark is currently being developed to evaluate
    performance across multiple dimensions, including diagnostic accuracy, hallucination
    prevention, and multi-agent coordination efficiency. This benchmark will provide
    standardized metrics


    ![](_page_4_Figure_9.jpeg)


    Figure 3: Example 1 illustrates the complexity of obesity, highlighting its core
    condition along with related factors such as patient status and bariatric surgery.
    It also depicts associated drug and BMI categorization, emphasizing the interconnectedness
    of these elements in understanding obesity as a multifaceted health condition.


    for assessing medical AI systems and will be made publicly available through our
    GitHub repository upon completion. The forthcoming benchmark aims to establish
    new standards for evaluating hierarchical multi-agent systems in medical applications,
    facilitating future research and development in this critical domain.


    ## Discussion


    The development and evaluation of KG4Diagnosis, encompassing 362 common diseases
    across multiple medical specialties, reveals significant insights into integrating
    hierarchical multi-agent systems with medical knowledge graphs for healthcare
    applications. Our comprehensive framework demonstrates both promising capabilities
    and important challenges that warrant further investigation.


    ### Technical Achievements and Innovations


    The combination of automated knowledge graph construction with hierarchical multi-agent
    architecture shows encouraging results in addressing key challenges in medical
    AI systems. Our framework''s ability to maintain diagnostic accuracy while preventing
    hallucination represents a significant advancement over traditional single-agent
    approaches. Particularly noteworthy is the effectiveness of our semanticdriven
    entity extraction and relationship reconstruction modules in handling complex
    medical terminology and relationships, achieving higher precision compared to
    conventional


    ![](_page_5_Figure_0.jpeg)


    Figure 4: Example 2 illustrates the expertise of the knowledge graph in the field
    of obesity. This knowledge graph highlights how certain drugs, such as Ozempic,
    not only aid in weight management but also reduce cardiovascular risk. Connections
    between obesity, Type 2 Diabetes, and cardiovascular diseases are depicted, showing
    their shared symptoms, treatments, and comorbidities. The graph underscores the
    multifaceted role of medications in addressing complex health conditions.


    methods.


    The hierarchical multi-agent structure, implemented through the MAS, proves especially
    valuable in managing complex medical cases. The GP agent''s ability to effectively
    triage cases and coordinate with specialist agents mirrors real-world medical
    practices, potentially reducing the computational overhead associated with full
    specialist consultation for every case. Furthermore, our approach to hallucination
    prevention through multiple validation layers, with the knowledge graph serving
    as an effective constraint system, significantly reduces incorrect diagnoses compared
    to standalone LLM implementations.


    ### System Adaptability and Scalability


    The resulting knowledge graph structure demonstrates the complex interconnections
    between different disease entities, symptoms, and diagnostic patterns. This comprehensive
    coverage supports efficient knowledge navigation and hierarchical decision-making
    processes. The modularity of our framework shows particular strength in incorporating
    new medical domains and knowledge, making it well-suited for the dynamic nature
    of medical knowledge.


    However, scalability analysis reveals important considerations. While the hierarchical
    structure efficiently manages computational resources through its tiered decision-making
    process, the system faces increasing complexity in coordinating multiple specialist
    agents as the number of medical domains expands. This highlights the need for
    more sophisticated coordination mechanisms in future iterations.


    ### Limitations and Challenges


    The system''s performance can be influenced by the quality and comprehensiveness
    of the underlying knowledge graph, particularly in rare or complex medical conditions.
    Challenges remain in handling edge cases where medical knowledge is rapidly evolving
    or when dealing with rare disease combinations not well-represented in the training
    data.


    Future research will involve conducting experiments on the state-of-the-art MedQA
    dataset to validate the superiority of our framework. MEDQA can be used to perform
    benchmark tests, thus evaluating the reproducibility of the perfect functioning
    of LLM. Meanwhile, this will allow us to benchmark our framework against other
    prominent models, such as ESM-1b, Med-PaLM, and BioGPT. By evaluating performance
    in MedQA, we aim not only to demonstrate the competitive advantages of our system
    but also to identify areas for further improvement. Additionally, the system''s
    heavy reliance on high-quality medical data for both knowledge graph construction
    and agent training presents challenges for deployment in regions with limited
    medical data resources. While our framework shows strong performance in well-documented
    medical conditions, its effectiveness in handling rare diseases or unusual symptom
    combinations requires further investigation.


    ## Related Work


    Rule-Based and Ontology-Driven Approaches: Recent advances in the construction
    and reasoning of medical KG have spawned various methodological approaches (Lu
    et al. 2024; Li et al. 2020; Peng et al. 2023), each offering unique advantages
    while facing distinct challenges. Traditional approaches to medical KG construction
    primarily rely on rulebased systems and ontology-driven techniques. While these
    methods excel in producing interpretable outputs and maintaining structural consistency
    through established medical ontologies, they face significant limitations in scalability
    and processing unstructured data (Abdulla, Mukherjee, and Ranganathan 2023).


    Deep Learning and Pre-Trained Models: The emergence of deep learning methods,
    particularly pre-trained language models such as BERT and BioBERT (Masoumi et
    al. 2024), has substantially improved information extraction capabilities in clinical
    texts. However, these models often struggle with domain-specific nuances and require
    considerable computational resources (Alsentzer et al. 2019).LLMs represent an
    advancement in processing unstructured medical data. While recent studies demonstrate
    their potential in generating structured knowledge and understanding complex medical
    relationships, challenges persist regarding hallucination and validation (Brown
    et al. 2020). The Med-HALT benchmark and contrastive decoding techniques have
    emerged as promising approaches to address these concerns (Liu et al. 2023). Furthermore,
    integrating LLMs with multi-agent systems (MAS) has shown particular promise in
    medical applications (Singhal et al. 2023b).


    Hybrid Symbolic-Neural Approaches: Hybrid approaches combining symbolic reasoning
    with neural architectures have gained traction for their ability to bal-


    ![](_page_6_Figure_0.jpeg)


    Figure 5: A visualization of the KG4Diagnosis full medical knowledge graph. Nodes
    represent different medical concepts, such as actions, symptoms, categories, and
    conditions, as indicated by the color legend. Edges signify relationships between
    these concepts, enabling structured representation and advanced diagnostic reasoning.
    The densely connected central region highlights the core interactions between
    treatments, symptoms, and diagnostics, while peripheral nodes provide additional
    contextual details. This hierarchical structure integrates medical data to facilitate
    multi-agent collaboration and human-guided reasoning.


    ance interpretability with adaptability. These systems integrate knowledge-driven
    reasoning with data-driven learning, though they require well-curated inputs and
    face computational scalability challenges (Wu, Zhang, and Lin 2023). Recent innovations
    in multimodal integration have expanded KG capabilities to incorporate diverse
    data types, including clinical notes, medical imaging, and laboratory results,
    although standardization and fusion challenges remain (Zhou et al. 2022).


    Advancements in Medical LLMs: Recent advancements in medical LLMs have significantly
    enhanced the field of natural language understanding in healthcare. Models like
    ESM-1b (Rives et al. 2021), originally developed for protein representation, have
    shown promise in biomedical applications, leveraging evolutionary scale modeling
    to analyze biological sequences with high accuracy. Med-PaLM (Singhal et al. 2023a),,
    on the other hand, represents a specialized adaptation of general-purpose LLMs
    for clinical use, focusing on answering medical questions and reasoning within
    structured datasets. Similarly, MediTron (Bosselut et al. 2024) and BioGPT (Luo
    et al. 2022) have been designed to extract biomedical knowledge, with MediTron
    excelling in multimodal data integration and BioGPT being fine-tuned specifically
    on biomedical literature for entity and relation extraction tasks. The recent
    development of GPT-4 medprompt (Nori et al. 2023) further pushes the boundaries
    of medical LLMs by integrating domain-specific prompts to guide reasoning, improving
    contextual accuracy and reducing hallucination in medical applications.


    Hierarchical Multi-Agent Architectures: The emergence of hierarchical multi-agent
    architectures represents a particularly promising direction. Pandey et al. (Pandey,
    Amod, and Kumar 2024) demonstrate that such architectures can effectively mirror
    real-world medical systems, with general-purpose agents handling initial assessment
    and specialized agents managing domain-specific diagnoses. This approach not only
    improves diagnostic accuracy but also enhances system scalability and reliability.


    Despite these advancements, the field continues to grapple with several critical
    challenges. The processing of unstructured medical data remains a significant
    hurdle, requiring more sophisticated approaches for accurate information extraction
    and structuring (Avula et al. 2022). The prevention and detection of LLM hallucinations
    in medical contexts demands continued innovation in verification mechanisms and
    validation protocols (Huang et al. 2023). Additionally, the integration of multimodal
    medical information presents ongoing challenges in data standardization and fusion.
    The coordination of multiple specialized agents within medical systems requires
    further refinement of communication protocols and decision-making frameworks.
    Furthermore, the development of comprehensive and standardized evaluation protocols
    for medical KG systems remains an active area of research, which is essential
    for ensuring the reliability and effectiveness of these systems in clinical applications.
    These interconnected challenges present opportunities for innovative solutions
    that combine the strengths of various approaches while addressing their individual
    limitations.


    ## Conclusion


    This paper presents KG4Diagnosis, a novel hierarchical multi-agent framework that
    integrates automated knowledge graph construction with specialized LLMs for medical
    diagnosis. Our implementation, covering 362 common diseases, demonstrates the
    effectiveness of combining knowledge graphs with a hierarchical multi-agent architecture
    to address critical challenges in medical AI systems. The framework''s innovations
    lie in its three-stage knowledge graph construction pipeline and hierarchical-based
    agent structure, where semantic-driven processing and human-guided reasoning create
    a robust knowledge foundation, while the multi-tiered agent architecture mirrors
    real-world medical practices. The system demonstrates significant advantages in
    preventing hallucination through multiple validation layers and managing computational
    resources through targeted specialist consultation. Although our current implementation
    shows promising results, we are developing comprehensive benchmarks to provide
    standardized evaluation metrics for the community. This work not only contributes
    a practical solution for current medical AI challenges but also establishes a
    foundation for future developments in hierarchical multi-agent systems for healthcare
    applications, potentially improving healthcare delivery and patient outcomes.


    ## References


    Abdulla, K.; Mukherjee, S.; and Ranganathan, P. 2023. Integrating Multimodal Data
    for Enhancing Knowledge Graphs: Current Challenges and Opportunities. *Journal
    of Big Data*, 10(1): 1–15.


    Al Khatib, H. S.; Neupane, S.; Kumar Manchukonda, H.; Golilarz, N. A.; Mittal,
    S.; Amirlatifi, A.; and Rahimi, S. 2024. Patient-centric Knowledge Graphs: A Survey
    of Current Methods, Challenges, and Applications. *Frontiers in Artificial Intelligence*,
    7: 1388479.


    Alam, F.; Giglou, H. B.; and Malik, K. M. 2023. Automated Clinical Knowledge Graph
    Generation Framework for Evidence-based Medicine. *Expert Systems with Applications*,
    233: 120964.


    Alsentzer, E.; Murphy, J.; Boag, W.; Weng, W.; Jin, D.; Naumann, T.; and McDermott,
    M. 2019. Publicly Available Clinical BERT Embeddings. arXiv:1901.08746.


    Amos, L.; Anderson, D.; Brody, S.; Ripple, A.; and Humphreys, B. L. 2020. UMLS
    Users and Uses: A Current Overview. *Journal of the American Medical Informatics
    Association*, 27(10): 1606–1611.


    Avula, R.; et al. 2022. Data-Driven Decision-Making in Healthcare Through Advanced
    Data Mining Techniques: A Survey on Applications and Limitations. *International
    Journal of Applied Machine Learning and Computational Intelligence*, 12(4): 64–85.


    Bonner, S.; Barrett, I. P.; Ye, C.; Swiers, R.; Engkvist, O.; Bender, A.; Hoyt,
    C. T.; and Hamilton, W. L. 2022. A Review of Biomedical Datasets Relating to Drug
    Discovery: A Knowledge Graph Perspective. *Briefings in Bioinformatics*, 23(6):
    bbac404.


    Bosselut, A.; Chen, Z.; Romanou, A.; Bonnet, A.; Hernandez-Cano, A.; Alkhamissi,
    B.; Matoba, K.; Salvi, F.; ´ Pagliardini, M.; Fan, S.; et al. 2024. MEDITRON:
    Open Medical Foundation Models Adapted for Clinical Practice.


    Brown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J.; Dhariwal, P.; Neelakantan,
    A.; Shyam, P.; Sastry, G.; and Askell, A. 2020. Language Models Are Few-Shot Learners.
    arXiv:2005.14165.


    Chang, E.; and Mostafa, J. 2021. The Use of SNOMED CT, 2013-2020: A Literature
    Review. *Journal of the American Medical Informatics Association*, 28(9): 2017–2026.


    Du, K.; Yang, B.; Wang, S.; Chang, Y.; Li, S.; and Yi, G. 2022. Relation extraction
    for manufacturing knowledge graphs based on feature fusion of attention mechanism
    and graph convolution network. *Knowledge-Based Systems*, 255: 109703.


    Garc´ıa-Ferrero, I.; Agerri, R.; Salazar, A. A.; Cabrio, E.; de la Iglesia, I.;
    Lavelli, A.; Magnini, B.; Molinet, B.; Ramirez-Romero, J.; Rigau, G.; et al. 2024.
    Medical mT5: An Open-Source Multilingual Text-to-Text LLM for the Medical Domain.
    *arXiv preprint arXiv:2404.07613*.


    Guo, T.; Chen, X.; Wang, Y.; Chang, R.; Pei, S.; Chawla, N. V.; Wiest, O.; and
    Zhang, X. 2024. Large Language Model Based Multi-Agents: A Survey of Progress
    and Challenges. *arXiv preprint arXiv:2402.01680*.


    Huang, L.; Yu, W.; Ma, W.; Zhong, W.; Feng, Z.; Wang, H.; Chen, Q.; Peng, W.;
    Feng, X.; Qin, B.; et al. 2023. A Survey on Hallucination in Large Language Models:
    Principles, Taxonomy, Challenges, and Open Questions. *ACM Transactions on Information
    Systems*.


    Li, L.; Wang, P.; Yan, J.; Wang, Y.; Li, S.; Jiang, J.; Sun, Z.; Tang, B.; Chang,
    T.-H.; Wang, S.; et al. 2020. Real-World Data Medical Knowledge Graph: Construction
    and Applications. *Artificial Intelligence in Medicine*, 103: 101817.


    Liu, Y.; Ott, M.; Goyal, N.; Du, J.; Joshi, M.; Chen, D.; Levy, O.; Lewis, M.;
    Zettlemoyer, L.; and Stoyanov, V. 2023. Med-HALT: Evaluating Hallucinations in
    Medical LLMs. arXiv:2410.15702.


    Lu, Z.; Afridi, I.; Kang, H. J.; Ruchkin, I.; and Zheng, X. 2024. Surveying Neuro-Symbolic
    Approaches for Reliable Artificial Intelligence of Things. *Journal of Reliable
    Intelligent Environments*, 10(3): 257–279.


    Luo, R.; Sun, L.; Xia, Y.; Qin, T.; Zhang, S.; Poon, H.; and Liu, T.-Y. 2022.
    BioGPT: Generative Pre-Trained Transformer for Biomedical Text Generation and
    Mining. *Briefings in Bioinformatics*, 23(6): bbac409.


    Masoumi, S.; Amirkhani, H.; Sadeghian, N.; and Shahraz, S. 2024. Natural Language
    Processing (NLP) to Facilitate Abstract Review in Medical Research: The Application
    of BioBERT to Exploring the 20-Year Use of NLP in Medical Research. *Systematic
    Reviews*, 13(1): 107.


    Mo, F.; Chaplin, J. C.; Sanderson, D.; Mart´ınez-Arellano, G.; and Ratchev, S.
    2024. Semantic models and knowledge graphs as manufacturing system reconfiguration
    enablers. *Robotics and Computer-Integrated Manufacturing*, 86: 102625.


    Nori, H.; Lee, Y. T.; Zhang, S.; Carignan, D.; Edgar, R.; Fusi, N.; King, N.;
    Larson, J.; Li, Y.; Liu, W.; et al. 2023. Can Generalist Foundation Models Outcompete
    Special-Purpose Tuning? Case Study in Medicine. *arXiv preprint arXiv:2311.16452*.


    OpenAI. 2022. ChatGPT: Optimizing Language Models for Dialogue. OpenAI Technical
    Blog.


    OpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774.


    Pandey, H. G.; Amod, A.; and Kumar, S. 2024. Advancing Healthcare Automation:
    Multi-Agent System for Medical Necessity Justification. In Demner-Fushman, D.;
    Ananiadou, S.; Miwa, M.; Roberts, K.; and Tsujii, J., eds., *Proceedings of the
    23rd Workshop on Biomedical Natural Language Processing*, 39–49. Bangkok, Thailand:
    Association for Computational Linguistics.


    Peng, C.; Xia, F.; Naseriparsa, M.; and Osborne, F. 2023. Knowledge Graphs: Opportunities
    and Challenges. *Artificial Intelligence Review*, 56(11): 13071–13102.


    Qian, J.; Jin, Z.; Zhang, Q.; Cai, G.; and Liu, B. 2024. A Liver Cancer Question-Answering
    System Based on Next-Generation Intelligence and the Large Model Med-PaLM 2. *International
    Journal of Computer Science and Information Technology*, 2(1): 28–35.


    Rives, A.; Meier, J.; Sercu, T.; Goyal, S.; Lin, Z.; Liu, J.; Guo, D.; Ott, M.;
    Zitnick, C. L.; Ma, J.; et al. 2021. Biological Structure and Function Emerge
    from Scaling Unsupervised Learning to 250 Million Protein Sequences. *Proceedings
    of the National Academy of Sciences*, 118(15): e2016239118.


    Shuifa, S.; Xiaolong, L.; Weisheng, L.; Dajiang, L.; Sihui, L.; Liu, Y.; and Yirong,
    W. 2023. Review of Graph Neural Networks Applied to Knowledge Graph Reasoning.
    *Journal of Frontiers of Computer Science & Technology*, 17(1): 27.


    Singhal, K.; Azizi, S.; Tu, T.; Mahdavi, S. S.; Wei, J.; Chung, H. W.; Scales,
    N.; Tanwani, A.; Cole-Lewis, H.; Pfohl, S.; et al. 2023a. Large Language Models
    Encode Clinical Knowledge. *Nature*, 620(7972): 172–180.


    Singhal, K.; Tu, T.; Gottweis, J.; Sayres, R.; and Wulczyn, E. 2023b. Towards
    Expert-Level Medical Question Answering with Large Language Models. *Nature Medicine*,
    29(1): 50– 58.


    Tang, X.; Chi, G.; Cui, L.; Ip, A. W.; Yung, K. L.; and Xie, X. 2023. Exploring
    Research on the Construction and Application of Knowledge Graphs for Aircraft
    Fault Diagnosis. *Sensors*, 23(11): 5295.


    Tonmoy, S.; Zaman, S.; Jain, V.; Rani, A.; Rawte, V.; Chadha, A.; and Das, A.
    2024. A Comprehensive Survey of Hallucination Mitigation Techniques in Large Language
    Models. *arXiv preprint arXiv:2401.01313*.


    Touvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux, M.-A.; Lacroix, T.;
    Roziere, B.; Goyal, N.; Hambro, E.; ` Azhar, F.; et al. 2023. LLaMA: Open and
    Efficient Foundation Language Models. *arXiv preprint arXiv:2302.13971*.


    Wu, J.; Zhu, J.; Qi, Y.; Chen, J.; Xu, M.; Menolascina, F.; and Grau, V. 2024.
    Medical Graph RAG: Towards Safe Medical Large Language Model via Graph Retrieval-Augmented
    Generation. *arXiv preprint arXiv:2408.04187*.


    Wu, Z.; Zhang, Y.; and Lin, X. 2023. Scalability Challenges in Medical Knowledge
    Graph Construction. *IEEE Transactions on Medical Informatics*, 14(2): 120–132.


    Zhang, J.; Zan, H.; Wu, S.; Zhang, K.; and Huo, J. 2024. Adaptive Graph Neural
    Network with Incremental Learning Mechanism for Knowledge Graph Reasoning. *Electronics*,
    13(14): 2778.


    Zhang, Y. 2021. Knowledge Reasoning with Graph Neural Networks. *Georgia Institute
    of Technology: Atlanta, GA, USA*.


    Zhang, Y.; Li, X.; and Wang, J. 2023. Knowledge Graph for Fraud Detection: Case
    of Fraudulent Related Party Transactions. In *Advances in Knowledge Discovery
    and Data Mining*, 182–194. Springer.


    Zhou, J.; Cui, G.; Zhang, Z.; Yang, C.; Liu, Z.; Wang, L.; Li, C.; and Sun, M.
    2022. Graph Neural Networks: A Review of Methods and Applications. *AI Open*,
    1(1): 1–12.'
  decisions:
    evaluation_prompt: 'Qualified. Reason: The paper includes structured evaluation
      elements such as a discussion of benchmarks, performance metrics, and a comprehensive
      framework for evaluation. It mentions the use of comprehensive benchmarks to
      validate the framework''s performance and discusses future evaluation work,
      indicating a structured approach to empirical analysis.'
    related_work_prompt: '- Qualified. Reason: The paper meaningfully engages with
      prior research throughout its text, including the Introduction, Methodology,
      and Related Work sections. It provides numerous academic citations, discusses
      existing methods, and compares its proposed framework to previous work in the
      field.'
    novelty_prompt: 'Qualified. Reason: The paper proposes KG4Diagnosis, a novel hierarchical
      multi-agent framework for constructing and reasoning over medical knowledge
      graphs. It introduces a new method that integrates a general practitioner agent
      with specialized agents, and incorporates advanced techniques for semantic entity
      extraction and decision-making reconstruction. The paper also claims novelty
      in addressing LLM hallucination challenges and provides a modular architecture
      for integrating new medical domains. These elements demonstrate clear contributions
      and novelty.'
    review_only_prompt: 'Qualified. Reason: The paper proposes KG4Diagnosis, a novel
      hierarchical multi-agent framework for medical diagnosis, which includes original
      contributions such as a new end-to-end knowledge graph construction pipeline,
      innovative techniques for semantic entity extraction, and a multi-agent system
      for medical decision-making. It also provides detailed implementation protocols
      and demonstrates the framework''s practical value through real-world scenarios,
      indicating novel contributions beyond a mere survey or review.'
- title: A Statically and Dynamically Scalable Soft GPGPU
  abstract: 'Current soft processor architectures for FPGAs do not utilize the potential
    of the massive parallelism available. FPGAs now support many thousands of embedded
    floating point operators, and have similar computational densities to GPGPUs.
    Several soft GPGPU or SIMT processors have been published, but the reported large
    areas and modest Fmax makes their widespread use unlikely for commercial designs.
    In this paper we take an alternative approach, building the soft GPU microarchitecture
    around the FPGA resource mix available. We demonstrate a statically scalable soft
    GPGPU processor (where both parameters and feature set can be determined at configuration
    time) that always closes timing at the peak speed of the slowest embedded component
    in the FPGA (DSP or hard memory), with a completely unconstrained compile into
    a current Intel Agilex FPGA. We also show dynamic scalability, where a subset
    of the thread space can be specified on an instruction-by-instruction basis.

    For one example core type, we show a logic range – depending on the configuration
    – of 4k to 10k ALMs, along with 24 to 32 DSP Blocks, and 50 to 250 M20K memories.
    All of these instances close timing at 771 MHz, a performance level limited only
    by the DSP Blocks. We describe our methodology for reliably achieving this clock
    rate by matching the processor pipeline structure to the physical structure of
    the FPGA fabric. We also benchmark several algorithms across a range of data sizes,
    and compare to a commercial soft RISC processor.'
  keywords: ''
  document: '# A Statically and Dynamically Scalable Soft GPGPU


    Martin Langhammer Intel Corporation & Imperial College London London, UK martin.langhammer@intel.com


    ## ABSTRACT


    Current soft processor architectures for FPGAs do not utilize the potential of
    the massive parallelism available. FPGAs now support many thousands of embedded
    floating point operators, and have similar computational densities to GPGPUs.
    Several soft GPGPU or SIMT processors have been published, but the reported large
    areas and modest Fmax makes their widespread use unlikely for commercial designs.
    In this paper we take an alternative approach, building the soft GPU microarchitecture
    around the FPGA resource mix available. We demonstrate a statically scalable soft
    GPGPU processor (where both parameters and feature set can be determined at configuration
    time) that always closes timing at the peak speed of the slowest embedded component
    in the FPGA (DSP or hard memory), with a completely unconstrained compile into
    a current Intel Agilex FPGA. We also show dynamic scalability, where a subset
    of the thread space can be specified on an instruction-by-instruction basis.


    For one example core type, we show a logic range – depending on the configuration
    – of 4k to 10k ALMs, along with 24 to 32 DSP Blocks, and 50 to 250 M20K memories.
    All of these instances close timing at 771 MHz, a performance level limited only
    by the DSP Blocks. We describe our methodology for reliably achieving this clock
    rate by matching the processor pipeline structure to the physical structure of
    the FPGA fabric. We also benchmark several algorithms across a range of data sizes,
    and compare to a commercial soft RISC processor.


    ## 1 INTRODUCTION


    FPGAs are capable platforms, with multiple thousands of embedded memories as well
    as DSP Blocks, many of which now support IEEE 754 floating point numerics. In
    addition, there is a significant amount of high performance IP available for FPGAs,
    e.g. FFTs [\[2,](#page-10-0) [8\]](#page-10-1) and error correction such as Reed-Solomon
    codecs [\[3\]](#page-10-2). High performance systems can readily be assembled
    using a combination of original design and these IP Blocks. The value of the FPGA
    is integration: although each individual IP or function is lower performance than
    ASIC, this is offset by the flexibility. However, modifying IP - even your own
    - requires significant effort. FPGA hardware compile times (synthesis, place and
    route) can take hours, and timing closure can be a significant unknown. Implementing
    (and modifying) a complex subset of a system by a pure software approach, where
    the result of the compile or assembly is essentially instantly available, and
    loaded onto an already placed and routed processor, may be very attractive.


    Soft RISC cores (Nios [\[9\]](#page-10-3) and MicroBlaze [\[4\]](#page-10-4))
    for FPGA have been used for over two decades, and allow the inclusion of complex
    control flow, or the offload of ancillary functions. Although these RISC processors
    are very flexible, they also have a rather low


    George A. Constantinides Imperial College London London, UK g.constantinides@imperial.ac.uk


    performance. Parallel processor architectures may offer better performance, and
    SIMT (GPGPU) processors may be able to efficiently use the large number of memory
    and DSP Blocks distributed across the FPGA device. There have been a number of
    soft SIMT FPGA architectures published [\[14,](#page-10-5) [15,](#page-10-6) [17,](#page-10-7)
    [18,](#page-10-8) [24,](#page-10-9) [25,](#page-10-10) [29\]](#page-10-11), but
    these are often very large (50K-300K LUTs), and typically have a low clock frequency
    (30MHz-100MHz). Other types of parallel processors are also known for FPGA [\[22,](#page-10-12)
    [30,](#page-10-13) [31\]](#page-10-14) (and commercialized [\[5\]](#page-10-15)),
    but the Fmax is relatively low at ∼150MHz.


    A different approach has been taken by Xilinx (now AMD) in the Versal devices,
    with arrays of AI Engines, a hardened VLIW processor. This motivates us to consider
    whether we can combine the flexibility of a soft processor (where any number can
    be instantiated into the soft fabric), but with the performance of an ASIC implementation
    (in this case, running at the speed of the embedded hardened features).


    Our design, which we call the eGPU (for embeddedGPU), is both statically and dynamically
    scalable, features which make it particularly useful and performant for FPGA applications.
    Static scalability is the ability to parameterize the thread space, shared memory
    space, integer ALU functions, as well as major processor features (such as predicates).
    Dynamic scalability allows us to operate on a defined subset of the thread space,
    and change this on an instruction by instruction basis, without any dead time.
    We will see that this can greatly reduce the number of cycles required in some
    portions of the program, such as during a vector reduction (which is a common
    kernel of GPGPU applications).


    We make the following contributions:


    - Describe a novel parameterized SIMT processor for FPGA, with a wide range of
    user defined instructions, as well as architectural trade-offs (such as predicates).

    - Demonstrate that a soft processor can consistently close timing at a level limited
    only by the embedded features such as DSP and memory, and do so with a completely
    unconstrained compile.

    - Compare the absolute and normalized (by resource cost) results of a soft GPGPU
    with a soft RISC processor, and show that the SIMT architecture is better in the
    general case, and significantly better when using dedicated hardware extensions.


    ## 2 BACKGROUND


    Our goal for this project was to architect and implement a compact, high performance
    SIMT processor, that can be used for commercial FPGA system designs. We can use
    current and prior FPGA processors both to understand the limitations of previous
    projects, and to validate some of our design choices. The axes of comparison to
    other work include memory systems, complexity (such as workload balancing), and
    trade-offs between hard and soft implementation.


    #### Table 1: Resource Comparison


    <span id="page-1-0"></span>


    | Architecture<br>Config. |          | LUTs | DSP  | FMax | PPA | Device     |  |

    |-------------------------|----------|------|------|------|-----|------------|--|

    | FGPU [15]               | 2CUx8PE  | 57K  | 48   | 250  | 36  | Zynq-7000  |  |

    | DO-GPU [29]             | 4CUx8PE  | 360K | 1344 | 208  | 133 | Stratix 10 |  |

    | FlexGrip [17]           | 1SMx16PE | 114K | 300  | 100  | 175 | Virtex-6   |  |

    | eGPU                    | 1SMx16SP | 5K   | 24   | 771  | 1   | Agilex     |  |


    Many of the previously published GPGPUs [\[15,](#page-10-6) [17,](#page-10-7)
    [24,](#page-10-9) [29\]](#page-10-11) are SIMT processors which were compiled
    to an FPGA, whereas eGPU was designed for FPGA. The eGPU has an power-performancearea
    (PPA) metric which is one or two orders of magnitude (OOM) smaller than some of
    the earlier soft GPGPUs. Comparisons between high-performance processor designs
    are complex and multidimensional. For example, some existing soft GPUs have more
    complex memory systems, including caches and dynamic workload balancing. This
    does come with a cost, with a typical order of magnitude resource difference,
    as can be seen in Table [1,](#page-1-0) where we compare configurations of the
    other soft GPGPUs that are closest in computational structure to eGPU (PEs are
    roughly the same as SPs). Despite the much deeper pipelines (e.g. FlexGrip [\[17\]](#page-10-7)
    has a 21 deep pipeline, FGPU has a 18 deep pipeline [\[15\]](#page-10-6)), they
    also run at a considerably slower clock frequency. Although they are implemented
    in older FPGA technology (FlexGrip is in Virtex-6 at 100MHz), this does not fully
    explain the performance level, as there are soft processors that run at 450MHz
    in those devices [\[20\]](#page-10-16) [\[21\]](#page-10-17). In the benchmarking
    section we will also see that the benchmarks also run slower than expected on
    the earlier GPGPUs based on the difference in clock frequency.


    Instead, we validate eGPU against existing soft RISC processors [\[9\]](#page-10-3),
    which are extensively used in real applications. We will normalize the benchmark
    results based on cost i.e. FPGA resources consumed. The eGPU, being a parallel
    processor (with essentially 16 smaller multi-threaded processors) will naturally
    be larger; to be effective and usable, it must have a clear advantage in both
    absolute performance and normalized efficiency over the RISC processors.


    eGPU uses a single local data memory, which is configurable in size, and does
    not support a cache. Larger datasets need to be externally managed. Like the eGPU,
    the Xilinx AI Engines [\[13\]](#page-10-18), which are organized as hard VLIW
    hard processor arrays, have only a single local data memory per CPU, the loading
    and unloading of which has to be managed externally. Algorithms with larger amounts
    of data (such as 4K FFTs) need to be split across multiple AI Engines [\[6\]](#page-10-19).
    The eGPU has a greater memory flexibility, as we are able to configure a larger
    shared memory instance (we show examples with up to 128KB in this paper). The
    AI Engines give us an example of a commercial FPGA parallel processor, where using
    multiple simpler processors have been found to have an advantage over using complex
    memory systems.


    ## 3 ARCHITECTURE DESCRIPTION


    The architecture of the eGPU is based on an earlier proof-of-concept design [\[28\]](#page-10-20).
    Our new design adds significant scalability - thread and register space, shared
    memory size, instruction set support, as well as optional predicates for thread
    divergence. Figure [1](#page-2-0) shows the top level architecture of the eGPU.
    The streaming multi-processor (SM) contains 16 parallel scalar processors (SP),
    although only 8 are shown in the figure for clarity. An optional dot-product core
    and special function unit (SFU) reciprocal square root can be attached. We target
    the Intel Agilex [\[23\]](#page-10-21) family of FPGAs in this work. The eGPU
    has a very short pipeline (8 stages) compared to other GPUs; therefore, hazards
    are hidden for most programs. Consequently, we do not provide hardware support
    for tracking hazards in the current version, which in turn gives us an efficient
    and fast processor.


    Two types of embedded memories are now supported, simple dual port (DP) and the
    emulated quad port (QP) blocks [\[12\]](#page-10-22). One of the largest performance
    limitations of the earlier eGPU architecture was memory bandwidth. The QP memory
    will double the write bandwidth, while at the same time reducing the number of
    embedded memory blocks required (the 20K-bit M20K blocks) by half. The trade-off
    is that in QP mode, the memory speed is reduced from 1 GHz to 600 MHz, which then
    becomes the critical path in the processor. Resource, Fmax, and benchmark results
    are all described later in this paper.


    ## 3.1 Dynamic Scalability


    Most GPGPUs support thread divergence by predicates (threadspecific conditionals)
    but these have a potential significant performance impact, as all threads are
    run, whether or not they are written back. In addition to predicates, the eGPU
    sequencer supports an instruction by instruction specification of a subset of
    the thread space, where only the indicated threads are run. If the program can
    be constructed such that the data of interest can be written to the threads that
    can be isolated by the dynamic thread allocation, then a large number of processing
    cycles can be skipped. This is particularly noticeable in programs with many multi-cycle
    instructions, such as reads and writes to shared memory. This will have a direct
    impact on the benchmark performance (number of cycles).


    We define a wavefront as the maximum number of operations that can be run per
    clock cycle; with 16 SPs we have a wavefront width of 16. The thread block depth
    (alternately, the wavefront depth) is the number of wavefronts per instruction,
    which is the initialized thread size / 16. We feel these terms allow us to describe
    our dynamic thread scalability more concisely.


    The eGPU can be configured, on a cycle by cycle basis, to act as a standard SIMT
    processor, a multi-threaded CPU, or a single threaded MCU. While the number of
    clock cycles to execute all the threads for an operation instruction (e.g. FP
    or INT) is dependent on the depth of the thread block, loads and stores are multi-cycle
    (because of the limited number of ports to shared memory). The impact of dynamically
    adjusting the width of certain instructions (e.g. reduction, where the writeback
    data can be orders of magnitude less than the read data) can be seen in the benchmark
    section later in this paper.


    The upper 4-bit field in the instruction word (IW) allows the wavefront width
    and depth to be coded for that instruction. Perhaps the most common case will
    be using only the first SP, or even the first thread in the first SP; many GPU
    applications will have vector reduction kernels, where a reduction result(s) may
    end up in the leftmost SP. If we can operate on this SP exclusively for a certain
    subset of time during the execution of the program, we A Statically and Dynamically
    Scalable Soft GPGPU


    <span id="page-2-0"></span>![](_page_2_Figure_1.jpeg)


    Figure 1: eGPU SM Top Level Architecture


    can save significant processing time, and power. The coding of the personality
    is described in the Instruction Set section.


    Hence, we can change the scale of the SIMT span by reducing the wavefront width
    and/or depth. The eGPU can act as a multithreaded CPU if we set the wavefront
    width to one, and if we also set the thread depth to one, each instruction will
    only act on thread 0 of the first SP - this SP can then be used like a MCU. We
    will use these modes to good effect in our benchmarks later in this paper.


    <span id="page-2-1"></span>![](_page_2_Figure_5.jpeg)


    ## 3.2 Predicates


    The eGPU optionally - by user configuration - supports predicates, which enable
    thread divergence. Conditionals can be applied to each thread individually using
    a condition instruction (see Table [2\)](#page-3-0).


    As with many aspects of eGPU, the number and type of conditions can be selected
    at compile time. Although these will have only a minimal impact on area, the additional
    wireload may impact performance because of the large number of individual predicate
    stacks. There is one predicate stack per initialized thread, so there may be thousands
    of stacks per eGPU instance.


    Some algorithms, such as the bitonic sort benchmark in this paper, require predicates.
    On the other hand, many of the signal processing applications that we expect that
    the eGPU will be used for (such as FFTs and matrix decomposition) do not use data
    dependent decisions. These do not need predicates, and can be programmed using
    only loop constructs, which are supported in the eGPU sequencer. For this reason,
    the presence and complexity of predication is a parameter of our design, especially
    considering the large potential cost of the feature.


    Figure [2](#page-2-1) shows the structure of a single predicate block. Each SP
    has a separate block, which are comprised of multiple predicate stacks. Each thread
    has a unique predicate stack. Multiple nested levels of conditional operations
    (IF/ELSE/END IF) are supported


    Figure 2: One Predicate Block


    per stack, with the maximum supported depth of nesting being parameterized.


    The incremental cost of adding one level of nesting is trivial, as the control
    logic of each predicate stack is the dominant user of logic here. The wavefront
    value (for example, in our base eGPU configuration of 512 threads with 16 SPs,
    there will be 32 wavefronts - i.e. 32 threads per SP) enables the correct predicate
    stack for the current thread. If the condition instruction (IF) condition is true
    for that thread, a ''1'' will be set at the top of the predicate stack, and the
    rest of the stack pushed down. An ELSE instruction will invert the top of the
    stack, and an END IF will pop the stack and return to the previous nesting level.


    The eGPU is configured at compile time for a maximum number of threads; if the
    run time configuration of threads is less than this, there is no issue as only
    the selected threads will trigger the operation of the predicate block.


    <span id="page-3-1"></span>


    | [43:40]  |        |      | [39:34] [33:32] [31:27] | [26:22] | [21:17] | [16:1]    |

    |----------|--------|------|-------------------------|---------|---------|-----------|

    | Variable | Opcode | Typ. | RD                      | RA      | RB      | Immediate
    |


    Figure 3: Instruction Word


    The conditional value will only be applied to the current predicate block, and
    all others ignored in that clock cycle. The current thread activation thread\_active
    signal will be muxed from all the predicate blocks, selected by the current wavefront.
    The thread\_active signal is used to pass or zero the write\_enable signals to
    either the register files or shared memory, whichever is the destination for that
    instruction.


    ## 4 INSTRUCTION SET


    Table [2](#page-3-0) shows most of the instruction set for the eGPU. There are
    a total of 61 instructions, including 18 conditional cases (we omit the FP conditional
    instructions here for brevity). Usually, only a subset of instructions are included
    (by the user defined configuration of the eGPU). The 18 conditional cases depend
    on predicates being included in the parameters - as predicates typically increase
    soft logic cost by 50% they are only used when the expected class of applications
    need them. Many of the intended applications, such as FFT, matrix multiplication
    and decomposition, do not, and the required loops can be handled with the dedicated
    loop instructions. Some instructions can support multiple TYPES, such as signed
    (INT32) and unsigned (UINT32) formats for integer instructions.


    The integer ALU uses a large proportion of the soft logic (≈100 ALMs to ≈400 ALMs),
    so selecting only the required precision (16 bit or 32-bit) and feature subset
    can reduce the cost of the eGPU substantially. Extension instructions are also
    optional. We will use the dot product instruction for some of the benchmarks in
    this paper; if used, it can make significant difference to the performance of
    some functions. We can also add elementary functions (currently we support only
    reciprocal square root), which are required for algorithms such as matrix decomposition.
    In contrast, the FP instructions are almost completely contained inside the DSP
    Block, with only the FP Max() and Min() instructions having a potential impact
    on area or performance.


    Figure [3](#page-3-1) shows an instruction word, here shown in a 43-bit form.
    As the number of registers per thread changes, the three register field widths
    also change; the displayed word is for a 32 registers per thread configuration,
    which requires 5 bits to encode the register number. The 2-bit representation
    field encodes whether the number is unsigned integer, signed integer, or FP32.
    The four most significant bits encode the processing type, which allow the wavefront
    depth and the width of the wavefront to be changed on an instruction by instruction
    basis.


    Writing these results into shared memory using subset write can be 16x faster
    than using the generic write. An instruction, whether used for a full or a partial
    thread space, is almost identical, with only the four instruction type bits used
    to control the subset of the thread space. Table [3](#page-3-2) shows how the
    upper 4 bits of the IW control the width and depth of the thread space.


    | Table 2: Instruction Set |  |

    |--------------------------|--|

    |--------------------------|--|


    <span id="page-3-0"></span>


    | Group              | Instruction            |                         |  |  |  |  |

    |--------------------|------------------------|-------------------------|--|--|--|--|

    |                    | ADD.TYPE Rd,Ra,Rb      | Rd = Ra + Rb            |  |  |  |  |

    |                    | SUB.TYPE Rd,Ra,Rb      | Rd = Ra - Rb            |  |  |  |  |

    | Integer Arithmetic | NEG.TYPE Rd,Ra         | Rd = -Ra                |  |  |  |  |

    |                    | ABS.TYPE Rd,Ra         | Rd = absolute(Ra)       |  |  |  |  |

    |                    | MUL16LO.TYPE Rd,Ra,Rb  | Rd = Ra * Rb            |  |  |  |  |

    |                    | MUL16HI.TYPE Rd,Ra,Rb  | Rd = (Ra * Rb)»16       |  |  |  |  |

    | Integer Multiply   | MUL24.LO.TYPE Rd,Ra,Rb | Rd = Ra * Rb            |  |  |  |  |

    |                    | MUL24.HI.TYPE Rd,Ra,Rb | Rd = (Ra * Rb)»24       |  |  |  |  |

    |                    | AND Rd,Ra,Rb           | Rd = Ra & Rb            |  |  |  |  |

    |                    | OR Rd,Ra,Rb            | Rd = Ra ∥ Rb            |  |  |  |  |

    |                    | XOR Rd,Ra,Rb           | Rd = Ra ⊕ Rb            |  |  |  |  |

    | Integer Logic      | NOT Rd,Ra              | Rd = !Ra                |  |  |  |  |

    |                    | cNOT Rd,Ra             | Rd = (Ra == 0)?1:0      |  |  |  |  |

    |                    | BVS Rd,Ra              | Rd = bit_reverse(Ra)    |  |  |  |  |

    |                    | SHL.TYPE Rd,Ra,Rb      | Rd = Ra ≪ Rb            |  |  |  |  |

    | Integer Shift      | SHR.TYPE Rd,Ra,Rb      | Rd = Ra ≫ Rb            |  |  |  |  |

    |                    | POP Rd,Ra              | Rd = unary(Ra)          |  |  |  |  |

    | Integer Other      | MAX.TYPE Rd,Ra,Rb      | Rd = (Ra>Rb)?Ra:Rb      |  |  |  |  |

    |                    | MIN.TYPE Rd,Ra,Rb      | Rd = (Ra<Rb)?Ra:Rb      |  |  |  |  |

    |                    | ADD.FP32 Rd,Ra,Rb      | Rd = Ra + Rb            |  |  |  |  |

    |                    | SUB.FP32 Rd,Ra,Rb      | Rd = Ra - Rb            |  |  |  |  |

    |                    | NEG.FP32 Rd,Ra         | Rd = -Ra                |  |  |  |  |

    | FP ALU             | ABS.FP32 Rd,Ra         | Rd = absolute(Ra)       |  |  |  |  |

    |                    | MUL.FP32 Rd,Ra,Rb      | Rd = Ra*Rb              |  |  |  |  |

    |                    | MAX.FP32 Rd,Ra,Rb      | Rd = (Ra>Rb)?Ra:Rb      |  |  |  |  |

    |                    | MIN.FP32 Rd,Ra,Rb      | Rd = (Ra<Rb)?Ra:Rb      |  |  |  |  |

    |                    | eq                     | 𝑅𝑎 == 𝑅𝑏                |  |  |  |  |

    |                    | ne                     | 𝑅𝑎 ≠ 𝑅𝑏                 |  |  |  |  |

    | Int Compare        | lt (INT), lo (UINT)    | 𝑅𝑎 < 𝑅𝑏                 |  |  |  |  |

    |                    | le (INT), ls (UINT)    | 𝑅𝑎 ≤ 𝑅𝑏                 |  |  |  |  |

    |                    | gt (INT), hi (UINT)    | 𝑅𝑎 > 𝑅𝑏                 |  |  |  |  |

    |                    | ge (INT), hs (UINT)    | 𝑅𝑎 ≥ 𝑅𝑏                 |  |  |  |  |

    | Memory             | LOD Rd (Ra)+offset     | Read from Shared        |  |  |  |  |

    |                    | STO Rd (Ra)+offset     | Write to Shared         |  |  |  |  |

    | Immediate          | LOD Rd #Imm            | Rd = Imm                |  |  |  |  |

    | Thread             | TDx Rd                 | Rd = Thread IDx         |  |  |  |  |

    |                    | TDy Rd                 | Rd = Thread IDy         |  |  |  |  |

    |                    | DOT Rd,Ra,Rb           | Dot Product ⟨𝑅𝑎, 𝑅𝑏⟩    |  |  |  |  |

    | Extension          | SUM Rd,Ra,Rb           | Reduction ⟨𝑅𝑎, 𝑅𝑏⟩<br>√ |  |  |  |  |

    |                    | INVSQR Rd,Ra           | 𝑅𝑑 = 1/<br>𝑅𝑎           |  |  |  |  |

    |                    | JMP address            | Jump to Address         |  |  |  |  |

    |                    | JSR address            | Subroutine Address      |  |  |  |  |

    |                    | RTS                    | Return from Subroutine  |  |  |  |  |

    | Control            | LOOP address           | Jump and Dec Loop Ctr   |  |  |  |  |

    |                    | INIT loops             | Set Loop Ctr            |  |  |  |  |

    |                    | STOP                   | Stop and Set Flag       |  |  |  |  |

    |                    | IF.cc                  | if cc true              |  |  |  |  |

    | Conditional        | ELSE                   | if cc false             |  |  |  |  |

    |                    | ENDIF                  | clear cc                |  |  |  |  |


    Table 3: Thread Space Control


    <span id="page-3-2"></span>


    | Coding | Width [4:3]             | Depth [2:1]          |  |  |  |

    |--------|-------------------------|----------------------|--|--|--|

    | "00"   | All (16 SPs)            | Wavefront 0 only     |  |  |  |

    | "01"   | 1/4 width (first 4 SPs) | all wavefronts       |  |  |  |

    | "10"   | SP0 only                | first 1/2 wavefronts |  |  |  |

    | "11"   | Undefined               | first 1/4 wavefronts |  |  |  |


    ## 5 RESULTS


    We compiled a number of different eGPU instances, using both DP and QP memory
    versions. We used Quartus Pro V22.4 and targeted an Intel Agilex AGIB027R29A1E1V
    device [\[7\]](#page-10-23). All of our results are reported for a single compilation
    attempt (we did not use seed sweeps).


    The DP memory results are tabulated in Table [4.](#page-6-0) We define three categories
    - small, medium, and large - to show the effects of different thread space, shared
    memory, and ALU features, as well as the impact of supporting predicates. The
    base eGPU architecture is the same for all instances: one SM with 16 SPs, a two
    read port register memory, and a four read and one write port shared memory. We
    configured all of these cases to use 512 threads, but with varying numbers of
    registers per thread. QP memory results are shown in Table [5,](#page-6-1) the
    main architectural change being the two write port shared memory.


    The ''small'' category uses a 16-bit ALU, which will likely only be used for address
    generation. The minimum specification supports only a single bit shift, as well
    as a 16-bit adder/subtractor, and arithmetic logic (AND/OR/XOR) operations. The
    memory requirements for the SPs is reduced by providing 16 registers per thread.
    The ''large'' category implements 64 registers per thread, and larger shared memory
    sizes with up to 128KB. The integer ALU supports the full set of integer instructions
    defined in the previous section. We also include a ''medium'' category for further
    examples. Many other combinations of parameters and features sets are possible
    as well.


    ## 5.1 Impact of Register and Shared Memory Size


    Both the thread registers and the shared memories are implemented using M20K memories,
    which can be configured into either DP (one read port and one write port active
    simultaneously) or QP memories (two read ports and two write ports active simultaneously).
    The natural datapath size of the eGPU is 32-bits, defined by the native IEEE 754
    single precision (FP32) floating point DSPs which will be doing the majority of
    the calculations. In DP mode, a M20K can be configured as a 512x32-bit memory.
    Port restrictions mean that in QP mode the M20K is a 2048x8-bit block, which requires
    a larger minimum thread register space to take advantage of the extra ports.


    In DP mode thread registers are implemented in two dual port memories, providing
    two read ports and one write port per clock cycle. In our most common eGPU configuration
    (with 16 registers per thread), a 512 thread machine will require two M20Ks per
    SP, or 32 M20Ks for thread registers in total, which is also the minimum size.
    Both the number of registers per thread and the number of total threads are parameterized,
    but the number of M20Ks will increase accordingly. In QP mode, the 8-bit data
    port width means that there is no point in using less than 2 thread registers
    per SP, although we will use half the number of M20K blocks compared to the DP
    version when we configure at least these number of registers.


    The shared memory is implemented as a four read port, one write port per memory
    in DP mode. The smallest possible shared memory is 512 words (2KB), which would
    require four M20Ks. This is very small, and unlikely to be useful, as the shared
    memory size would only be as large as the register space in a single SP. A more
    realistic shared memory size would be 2 words (8KB), which would require 16 M20Ks;
    the total memory usage for a small eGPU instance, including registers, would therefore
    be 48 M20Ks. The shared memory is set by parameter, and significantly larger sizes
    are possible without frequency impact. For example, a 64KB shared memory needs
    128 M20Ks, and a 128KB shared memory 256 M20Ks, which is a small fraction of the
    memories on the device. In QP


    mode, the number of M20Ks is halved, and the number of write ports doubled to
    two.


    ## 5.2 Integer ALU Architecture and Resources


    Unlike the floating point arithmetic, which can be mapped directly to a DSP Block,
    the simpler integer operations need to be implemented in soft logic. We will see
    that up to half of the soft logic and registers in an eGPU is required for the
    integer ALU. Table [6](#page-6-2) shows the resources, split by operation type,
    for a wide range of integer ALUs.


    The smallest reasonable integer ALU is a 16 bit version with single bit shifts,
    which consumes 90 ALMs and 136 registers, most of which are used for the 5 stage
    pipeline. Here we have a signed adder/subtractor, as well as logic functions (in
    this case, only AND, OR, and XOR are supported). The more typical full 16-bit
    ALU implementation supports signed and unsigned arithmetic, a more complete set
    of logic operations (AND/OR/XOR/NOT/cNOT/BVS), full 16-bit left and right shifts,
    population count, as well as max/min functions. The resource cost is approximately
    double that of the minimum ALU. The 5 stage pipeline 32-bit version again doubles
    the logic, as might be expected, but the number of registers triples, as individual
    functions (specifically the adder/subtractor and shifters) are themselves pipelined
    to ensure that the ALU always exceeds 800MHz. This contrasts with the 16-bit ALU,
    where the pipelining is used to improve the placement of the entire ALU, rather
    than improving the performance of any individual function. There is also a 4 stage
    pipeline version of the 32-bit integer ALU, which is about the size of the 16-bit
    full function ALU. This returns a lower performance (typically 700 MHz), and is
    used in order to save logic for the QP version of the eGPU (which has a lower
    target speed of 600MHz). The individual resource counts in Table [6](#page-6-2)
    may not accurately reflect the impact of each function to the overall ALU size,
    as synthesis may combine aspects of some functions together.


    ## 5.3 Predicate Resources


    In Table [4](#page-6-0) and [5](#page-6-1) the area impact of predicate support
    is clearly visible, increasing the soft logic resources by about 50%. While each
    predicate stack (including its control) is very small, each thread has a unique
    stack. The base predicate area consists of only a thread comparator (which checks
    that the SP currently executing the thread that the predicate circuit is associated
    with), an instruction decode (IF/ELSE/ENDIF), and the single bit-wide predicate
    stack. This may only be 5 ALMs per thread, but if a typical eGPU contains 1 threads,
    the predicate circuitry can quickly grow to be as large as the rest of the soft
    logic. Increasing the stack depth will have only a minimal impact on area, as
    each additional level consists of only a two input mux and a register.


    ## 5.4 Instruction Fetch, Decode, and Control


    This section will always have a modest footprint, requiring 200 to 250 ALMs, and
    a handful of M20Ks to store the instruction words. The instruction decoder takes
    about 40 ALMs, and the thread generator around 25 ALMs. A single M20K can store
    512 40-bit instruction words; the benchmarks we analyse later in this paper range
    from


    30 instructions (32 element reduction) to 250 instructions (256 element bitonic
    sort), so a multi-tenancy of programs would only need several M20Ks.


    Increasing the IW to 43 or 46 bits (which is required to support a 32 and 64 registers
    per thread, respectively), adds only a single M20K per 2 instructions, as the
    M20K containing the upper bits would be configured in 8 format. In any case, the
    number of M20Ks needed for program storage is small compared to the thread registers
    and shared memory. For example, a 1 word program space would require three M20Ks,
    and a 4 program space nine M20Ks.


    ## 5.5 Calculating Resources and Setting Performance


    Although the eGPU has a parameterized pipeline depth between the SPs and shared
    memory, it can achieve the target performance (771MHz and 600MHz respectively)
    using the minimum depth of 8 stages. The parameterized pipelining can be used
    for future applications with larger shared memories, or when the shared memories
    are placed elsewhere on the device, and not located near the SP array. We also
    report the slowest path outside the embedded (M20K and DSP) resources (see Table
    [4](#page-6-0) and [5\)](#page-6-1). If needed, there are also additional pipelining
    parameters inside the SP for the paths both to and from the FP and Integer ALUs.
    We will show in the next section how additional pipelining may not improve Fmax
    as the eGPU has been designed to fit into an Agilex sector in the base configuration.


    We can see that the SP overhead (mux and control) is ≈150 ALMs, the integer ALU
    ranges from ≈100 ALMs to ≈400 ALMs, and the predicates, if used, start from ≈
    150ALMs. A single SP will therefore be as small as 250 ALMs, and can be as large
    as 650 ALMs; this translates into a small eGPU core (16 SPs) requiring 4 ALMs,
    and over 10 ALMs for fully featured example.


    The number of M20Ks for the register memory for the DP eGPU can be calculated
    as threads×registers/256; for the shared memory the number of blocks is 2 × size().
    The number of M20K blocks required for the QP eGPU are half of this, except that
    there is a minimum size (threads × registers\_per\_thread/16 > 2047) for the number
    of registers, in which case the QP eGPU will need the same number of register
    blocks as the DP version.


    ## 5.6 FPGA Sector Resources and Impact


    It is most beneficial to select eGPU parameters around the available FPGA resources
    and their on-chip organization. The Intel Agilex devices are arranged in sectors,
    the most common of which contains about 16400 ALMs, 240 M20K memories, and 160
    DSP Blocks. Although we are not limited to a single sector (additional pipelining
    may be required to maintain performance across sector boundaries), this ratio
    of resources provides a good guide how to parameterize a eGPU instance. In particular,
    creating too large a register or memory space will be inefficient, as the ALMs
    between the M20K columns will likely be unreachable by other designs in the FPGA.
    Likewise, there is no point in specifying a small register space or shared memory,
    as the M20Ks between the logic structures may not be accessible by other functions.
    Further analysis is provided in the following section where we demonstrate that
    by selecting parameters in this way, the eGPU consistently achieves the reported


    performance levels by matching its architecture with the sector structure.


    ## 6 REPEATABLE HIGH PERFORMANCE


    This section provides the required information to make our design process repeatable
    for those wishing to achieve high performance in their own designs. It is therefore
    necessarily ''close to metal'' in abstraction. Although the details are specific
    to Intel FPGAs, we believe the same approaches are valid for all other FPGAs as
    well.


    The eGPU is designed to give consistent performance, which will always be limited
    by the slowest embedded (DSP or M20K memory) resource in that configuration. The
    clock network in Agilex is specified at 1GHz, which is the absolute limit of performance
    for any design. The M20K memories in DP mode also achieve 1GHz, but only 600 MHz
    in QP mode. The DSP Blocks can run at 771 MHz when implementing a FP32 multiply-add
    datapath with a 4 stage pipeline [\[11\]](#page-10-24). We are therefore limited
    to a maximum speed of 771MHz, unless we use QP memory, in which case the maximum
    frequency drops to 600MHz. The lower performance of the QP memory, however, will
    allow us to support a higher density storage, and the doubled write bandwidth
    may offer an overall higher throughput for some applications. We will examine
    some of these trade-offs in the benchmarking section.


    Using the sector architecture effectively enables the eGPU performance and efficiency.
    Sector resources are arranged in columns, each approximately 41 rows high (several
    columns are shorter because of non-user accessible device features). Achieving
    a 1GHz speed for soft logic does not require logic to be immediately adjacent
    to each other, as there are different horizontal and vertical wire lengths - too
    much pipelining can negatively impact performance as much as too little. More
    important is using the minimal number of wire resources per connection. In the
    Agilex devices, there is a constant 4 columns of logic between each column of
    either DSP or M20K. In a sector we will have 40 columns of logic, 4 columns of
    DSP, and 6 columns of M20K. There is little point in saving logic or memory if
    it is not accessible by other portions of design.


    As we have shown in the previous section, the results are deterministic and repeatable,
    in both area and performance. Ideally, the resource use would be balanced to realize
    the maximum efficiency from the device.


    To map eGPU to the device, we first sketched out a LUT level construct of an SP,
    and adjusted it so that the number of logic levels would align with the sector
    column ratios described above. Paths directly between M20K memories (which implement
    the thread registers in each SP) and the DSP Blocks had to fit into a 4 column
    group of LABs, and longer pure logic paths (e.g. the integer ALUs) were organized
    so that the total area did not spill over into a M20K or DSP column that might
    be used by another SP.


    We can see from the results (Table [6\)](#page-6-2) that a 16-bit Integer ALU
    is in the range of 100-200 ALMs and the 32-bit version requires 200- 400 ALMs.
    If predicates are used, they will cost an additional 125-250 ALMs per SP, depending
    on the defined thread space. The remaining 150 ALMs per SP are used for the data
    muxing and alignment shown in Figure [1.](#page-2-0) We were able to implement
    a small eGPU (the first example in Table [4\)](#page-6-0) that was able to close
    timing over 771 MHz with no dedicated soft logic registers (i.e. registers that
    were not


    <span id="page-6-0"></span>


    | Scale  | ALU<br>Precision | Shift<br>Precision | Threads | Reg.<br>Thread |
    Shared<br>Memory | Predicate<br>Levels | ALM   | Registers | DSP | M20K | Freq<br>(MHz)
    | SP<br>(ALM/Reg.) |

    |--------|------------------|--------------------|---------|----------------|------------------|---------------------|-------|-----------|-----|------|---------------|------------------|

    | Small  | 16               | 1                  | 512     | 16             |
    8KB              | 0                   | 4243  | 13635     | 24  | 50   | 1018/771      |
    224/707          |

    | Small  | 16               | 16                 | 512     | 16             |
    32KB             | 5                   | 7518  | 18992     | 24  | 98   | 898/771       |
    413/979          |

    | Medium | 16               | 16                 | 512     | 32             |
    32KB             | 5                   | 7579  | 19155     | 24  | 131  | 883/771       |
    426/1043         |

    | Medium | 32               | 16                 | 512     | 32             |
    32KB             | 5                   | 9754  | 25425     | 24  | 131  | 902/771       |
    461/1277         |

    | Large  | 32               | 16                 | 512     | 64             |
    32KB             | 8                   | 10127 | 26040     | 32  | 195  | 860/771       |
    575/1505         |

    | Large  | 32               | 32                 | 512     | 64             |
    64KB             | 16                  | 10697 | 26618     | 32  | 259  | 841/771       |
    600/1476         |


    #### Table 4: Fitting Results - DP Memory


    #### Table 5: Fitting Results - QP Memory


    <span id="page-6-1"></span>


    | Scale  | ALU       | Shift     | Threads | Regs./ | Shared | Predicate | ALM   |
    Registers | DSP | M20K | Freq    | SP         |

    |--------|-----------|-----------|---------|--------|--------|-----------|-------|-----------|-----|------|---------|------------|

    |        | Precision | Precision |         | Thread | Memory | Levels    |       |           |     |      |
    (MHz)   | (ALM/Reg.) |

    | Small  | 32        | 1         | 512     | 64     | 32KB   | 0         | 5468  |
    14487     | 24  | 98   | 840/600 | 287/830    |

    | Medium | 32        | 32        | 1024    | 32     | 64KB   | 0         | 7057  |
    16722     | 32  | 131  | 763/600 | 396/1016   |

    | Large  | 32        | 32        | 1024    | 32     | 64KB   | 16        | 11314
    | 25050     | 32  | 131  | 763/600 | 685/1601   |

    | Large  | 32        | 32        | 1024    | 32     | 128KB  | 10        | 10174
    | 23094     | 32  | 195  | 714/600 | 556/1391   |


    Table 6: Fitting Results - Integer ALU


    <span id="page-6-2"></span>


    | Prec. | Type  | ALM | Registers | Add/<br>Sub | Logic | SHL | SHR | Pop |

    |-------|-------|-----|-----------|-------------|-------|-----|-----|-----|

    | 16    | Min   | 90  | 136       | 3           | 9     | -   | -   | -   |

    | 16    | Small | 134 | 207       | 9           | 10    | 20  | 23  | -   |

    | 16    | Full  | 199 | 269       | 9           | 18    | 20  | 23  | 11  |

    | 32    | Min   | 208 | 406       | 5           | 27    | 28  | 28  | -   |

    | 32    | Full  | 394 | 704       | 27          | 36    | 50  | 53  | 27  |


    directly connected with a logic function, such as a mux), but for the generic
    parameterized case, we added a single additional pipeline stage register between
    the thread registers and the functional units, and also one level in the write-back
    path between the functional units and the thread registers. For all of the examples
    in Table [4](#page-6-0) there are also single pipeline stages to and from the
    shared memory. We parameterized the pipeline depth for all of these stages, along
    with the appropriate balancing delays for the data and control paths into the
    write paths of the thread registers, but found that these were not needed to be
    increased beyond one pipeline stage for any of the reported examples.


    Figure [4](#page-7-0) shows the unconstrained placement of the largest instance
    of Table [4.](#page-6-0) The shared memory and 8 out of the 16 SPs have been color
    coded for identification. The shared memory creates a spine in the middle of the
    core, with 8 SPs placed on either side of it. For purposes of illustration we
    have colored a subset of SPs: three random SPs and the left of the spine, and
    five contiguous ones on the right. Three things are evident with all SPs: (a)
    the majority of the logic is in one contiguous block, (b) there is a separate
    contiguous structure (the predicate block) placed some distance away, and (c),
    the SP straddles a columns of DSP Blocks. All of the instances of Table [4](#page-6-0)
    and [5](#page-6-1) display this pattern, including the shared memory spine.


    Figure [5](#page-7-1) shows one of the SPs in greater detail (this SP is the one
    marked by the black boxes in Figure [4\)](#page-7-0). The largest component is
    the integer ALU. The operators (adder/subtractor, shifters, arithmetic logic,
    etc.) are in the 4 columns to the right of the two DSP blocks (the DSP Block for
    the floating point operators is adjacent to the integer multiplier). To the left
    of the DSP Blocks is largely pipelining logic - of the 5 pipeline stages in the
    ALU, only one is used for pipelining the operators - the rest is used to break
    up the paths between the thread register memories and the ALUs. We examined all
    of the SP placements, and the placement of the M20Ks for the register memories
    (8 M20Ks for this instance) was in one of three layouts: (a) a contiguous single
    column (b) most of the registers in one column, with a smaller number in the next
    column further away from the integer ALU, and (c) equally split between two columns
    on either side of the integer ALU. In all of these cases, the pipeline wrapper
    around the ALU was usually grouped together, and essentially separate from the
    actual operators. Rather than having to be in a specific location relative to
    the M20Ks and operators, the ability to split up a bus so that it can be mapped
    to the same number of wire hops is what was important. Fewer pipeline stages would
    have introduced a two stage routing path, which would have likely become the critical
    path in the eGPU. (In the QP memory version, we can remove one of the pipeline
    stages as the M20K becomes the slowest component at 600 MHz, and we can see that
    the removal of some of the pipeline path reduce the non-memory path performance
    to just over 700 MHz). On the other hand, more than a 5 stage integer ALU could
    potentially decrease performance as it could spread out the placement of the SP.


    The predicate circuitry is placed in another contiguous block, but well away from
    the SP core it is associated with. From Figure [4](#page-7-0) we can see that
    the majority of the other predicates have a similar relationship with their respective
    SP. All of these have been automatically placed by Quartus. This is possible because
    the interface to and from the predicate block is very narrow, with only a single
    bit (thread\_active signal) returned. The signals to the block are relatively
    few: a thread index (typically 5 to 8 bits wide), a 3-bit decoded instruction
    signal (IF/ELSE/ENDIF), and a single bit valid condition code. Although there
    are many possible conditions from many different instructions, these can be decoded
    into a single


    <span id="page-7-0"></span>![](_page_7_Figure_1.jpeg)


    Figure 4: eGPU Placement


    valid condition bit in the main SP body. These narrow busses give us flexibility
    to wrap multiple pipes around the relatively simple (consisting largely of a chained
    registers organized in individual stacks) predicate blocks, which makes it possible
    for the tool to place them almost completely independently of the main datapaths.


    To create repeatable high performance designs, we need to understand both the
    structure, and the position of embedded features to each other. Here we are using
    integer ALUs which range in size by four times, our logic and memory density is
    very high, but our performance always exceeds that of the slowest embedded feature.
    It is possible to build a completely different type of CPU (or indeed any other
    type of core) and achieve this type of performance via a push button flow, but
    the architecture of the FPGA needs to be considered at every stage of the IP architecture
    phase.


    ## 7 BENCHMARKS


    We ran a number of workloads of different types to evaluate absolute and relative
    performance of the eGPU for varying data sizes that we might expect for embedded
    applications. We also profiled all the workloads to examine the efficiency of
    the eGPU. For continuity we selected many of the same benchmarks as used by Flexgrip
    [\[16\]](#page-10-25). We chose vector reduction, matrix transpose, and matrix-matrix
    multiply (MMM), as these would be common building blocks for many GPGPU applications.
    Bitonic sort [\[19\]](#page-10-26) is a sorting algorithm suited for parallel
    processing. Instead of the simpler autocorrelation, we used the FFT, as we felt
    this would be more representative of the


    <span id="page-7-1"></span>![](_page_7_Figure_7.jpeg)


    Figure 5: Single SP Placement


    workloads expected for the eGPU. All benchmarks were written in assembly code
    (we have not written our compiler yet).


    We report the comparison to FlexGrip only for the MMM, as the larger dataset size
    would be less affected by any overheads for setup and data transfer. We see that
    there is a significant performance advantage in favor of eGPU in cycle time alone.
    We ran all benchmarks, except the FFT, for which there are no reported FlexGrip
    results. FlexGrip underperforms eGPU by a factor of ≈31x, averaged over all benchmarks.
    We did not compare against DO-GPU (which is the latest iteration of FGPU), as
    DO-GPU normalized size is 50x-100x greater than eGPU.


    Our reported measurements are all based on core performance: we start the clock
    once the data has been loaded into the shared memory, and stop the clock once
    the final result has been written back to the shared memory. The most likely use
    of the eGPU is to apply multiple algorithms to the same data - .. there is no
    loading and unloading of data between different algorithms. For completeness,
    we also ran all of our benchmarks taking into account the time to load and unload
    the data over the 32-bit wide data bus. The performance impact was only 4.7%,
    averaged over all benchmarks.


    Clock frequency was 771 MHz for eGPU (including where the Dot Product operator
    is used), and 600 MHz for eGPU-QP variant. We compare both cycle counts and elapsed
    time for the eGPUs with the two shared memory architectures, and also the impact
    of the optional Dot Product core for reduction and MMM benchmarks. We then normalize
    the performance (time), by the resource cost, which we calculated on the basis
    of ALMs and DSP Blocks. We estimate that the effective cost of a DSP block is
    100 ALMs, which we calculate as follows: we start with the ALM count of the pure
    soft logic implementation of a FP32 multiply and adder (approximately 650 ALMs
    [\[10\]](#page-10-27)), and add 50% area to this number for DSP Block overhead
    (a DSP Block contains considerable additional features). We then divide by 10
    for an approximate soft logic to hard logic


    <span id="page-8-1"></span><span id="page-8-0"></span>


    |                                         |                                             |
    Vector Reduction               |                |                  | Matrix Transpose   |                      |                      |
    Matrix x Matrix          |                             |                                       |              |                            |                                             |

    |-----------------------------------------|---------------------------------------------|--------------------------------|----------------|------------------|--------------------|----------------------|----------------------|--------------------------|-----------------------------|---------------------------------------|--------------|----------------------------|---------------------------------------------|

    | Dimension                               | Metric                                      |
    Nios                           | eGPU           | eGPU             | eGPU               |
    Nios                 | eGPU                 | eGPU                     | Nios                        |
    FlexGrip                              | eGPU         | eGPU                       |
    eGPU                                        |

    |                                         |                                             |                                |
    DP             | QP               | Dot                |                      |
    DP                   | QP                       |                             |                                       |
    DP           | QP                         | Dot                                         |

    |                                         | Cycles                                      |
    459                            | 168            | 160              | 62                 |
    21809                | 1720                 | 1208                     | 1.45M                       |
    2.14M                                 | 111546       | 103354                     |
    19800                                       |

    |                                         | Time(us)                                    |
    1.32                           | 0.22           | 0.27             | 0.08               |
    62.85                | 2.23                 | 2.01                     | 4179                        |
    21400                                 | 144.7        | 172.3                      |
    25.7                                        |

    | 32                                      | Ratio(cycles)                               |
    2.73                           | 1.0            | 0.95             | 0.37               |
    12.68                | 1.0                  | 0.7                      | 13.03                       |
    19.2                                  | 1.0          | 0.93                       |
    0.18                                        |

    |                                         | Ratio(time)                                 |
    6.01                           | 1.0            | 1.23             | 0.37               |
    28.18                | 1.0                  | 0.9                      | 28.97                       |
    147.9                                 | 1.0          | 1.19                       |
    0.18                                        |

    |                                         | Normalized                                  |
    1.14                           | 1.0            | 1.4              | 0.45               |
    5.33                 | 1.0                  | 1.02                     | 5.48                        |
    -                                     | 1.0          | 1.35                       |
    0.21                                        |

    |                                         | Cycles                                      |
    1803                           | 202            | 194              | 94                 |
    86609                | 5529                 | 3481                     | 11.6M                       |
    16.6M                                 | 451066       | 418671                     |
    84425                                       |

    |                                         | Time(us)                                    |
    5.20                           | 0.26           | 0.32             | 0.12               |
    249.6                | 7.17                 | 5.80                     | 33383                       |
    166000                                | 585.0        | 697.8                      |
    109.5                                       |

    | 64                                      | Ratio(cycles)                               |
    8.93                           | 1.0            | 0.96             | 0.47               |
    15.66                | 1.0                  | 0.63                     | 25.7                        |
    36.8                                  | 1.0          | 0.93                       |
    0.19                                        |

    |                                         | Ratio(time)                                 |
    19.98                          | 1.0            | 1.23             | 0.47               |
    34.81                | 1.0                  | 0.81                     | 57.1                        |
    284                                   | 1.0          | 1.19                       |
    0.19                                        |

    |                                         | Normalized                                  |
    3.78                           | 1.0            | 1.4              | 0.60               |
    6.59                 | 1.0                  | 0.92                     | 10.80                       |
    -                                     | 1.0          | 1.35                       |
    0.23                                        |

    |                                         | Cycles                                      |
    3595                           | 216            | 208              | 101                |
    345233               | 20481                | 12649                    | 92.5M                       |
    441.2M                                | 2342356      | 2212136                    |
    886452                                      |

    |                                         | Time(us)                                    |
    10.36                          | 0.28           | 0.35             | 0.13               |
    994.91               | 26.56                | 21.08                    | 266491                      |
    4412.1                                | 3038.1       | 3686.9                     |
    1149.7                                      |

    | 128                                     | Ratio(cycles)                               |
    16.64                          | 1.0            | 0.96             | 0.47               |
    16.86                | 1.0                  | 0.62                     | 39.47                       |
    188.3                                 | 1.0          | 0.94                       |
    0.38                                        |

    |                                         | Ratio(time)                                 |
    37.00                          | 1.0            | 1.23             | 0.47               |
    37.45                | 1.0                  | 0.79                     | 87.71                       |
    1452                                  | 1.0          | 1.21                       |
    0.38                                        |

    |                                         | Normalized                                  |
    7.00                           | 1.0            | 1.4              | 0.60               |
    7.09                 | 1.0                  | 0.90                     | 1659                        |
    -                                     | 1.0          | 1.37                       |
    0.46                                        |

    |                                         |                                             |                                |                |                  |                    |                      |                      |                          |                             |                                       |              |                            |                                             |

    | Bitonic 64<br>Bitonic 32<br>Bitonic 128 | Bitonic 256<br>Bitonic 32QP<br>Bitonic
    64QP | Bitonic 128QP<br>Bitonic 256QP | FFT32<br>FFT64 | FFT128<br>FFT256 | FFT32QP<br>FFT64QP
    | FFT128QP<br>FFT256QP | Reduce32<br>Reduce64 | Reduce32Dot<br>Reduce128 | Reduce64Dot<br>Reduce128Dot
    | MMM 64x64<br>MMM 32x32<br>MMM 128x128 | MMM32x32QP   | MMM64x64QP<br>MMM128x128QP
    | MMM32x32Dot<br>MMM64x64Dot<br>MMM128x128Dot |

    | FP OP                                   | INT OP                                      |                                |
    IMM OP         |                  | Branch             | Load                 |                      |
    Save                     | Predicate                   |                                       |
    Thread Setup |                            | NOP                                         |


    Table 7: Vector and Matrix Benchmarks


    Figure 6: Benchmark Profiling (Y-Axis shows proportion of instructions executed
    by type).


    scaling factor (earlier work [\[26\]](#page-10-28) suggested a higher ratio in
    the general case, but recent work [\[27\]](#page-10-29) described more efficient
    ways of mapping arithmetic, especially multipliers, to FPGAs). We report normalized
    cost (considering both elapsed time and resources, with eGPU-DP as the baseline).


    As a comparison, we ran all of the benchmarks on Nios IIe [\[1\]](#page-10-30),
    which is a mature RISC processor for Intel FPGAs. The configuration we used consumed
    1100 ALMs (plus 3 DSP Blocks, giving a normalized cost of 1400), and closed timing
    at 347 MHz. We did not profile the Nios code, but analyzed the efficiency of operation
    (CPI). Most of the benchmarks retired an instruction every 1.7 clock cycles, except
    for the matrix-matrix multiplies and FFT, which required about 3 clocks, because
    of the way that 32×32 multipliers


    were implemented. (For simplicity, we replaced the FP32 arithmetic with INT32
    for the Nios examples).


    For the vector and matrix benchmarks, we chose an eGPU configuration with 32 registers
    per thread, with a 32 bit ALU, and a 128KB shared memory. This configuration has
    an equivalent cost (see Table [4](#page-6-0) and [5\)](#page-6-1) of 7400, 8400,
    and 9000 ALMs for the eGPU-DP, eGPU-QP, and eGPU-Dot variants respectively. Depending
    on the configuration, eGPU is 5× to 6× larger than Nios (but also more than twice
    the operating frequency). We would therefore expect (or at least hope) that eGPU
    would give an OOM performance increase over Nios.


    We can deduce the mechanism of the matrix transpose benchmarks from Table [7](#page-8-0)
    directly. For a given × matrix, we know that the eGPU will need 2 cycles to write
    the transposed elements


    to shared memory and 1/4th of those cycles to initially read them into the SP
    threads. We can see that the number of cycles clocked is marginally larger than
    this; these are largely used for the integer instructions needed to generate the
    transposed write addresses. We expect that the eGPU-QP will require about 40%
    fewer cycles, being able to write two transposed elements per clock, which indeed
    is the case.


    The vector reduction needs inter-SP communication, which go through the shared
    memory, which is the performance bottleneck in the eGPU. Table [7](#page-8-0)
    shows the impact of memory accesses on reduction performance. The actual floating
    point operations are a relatively small (≈10%) component of the reduction, with
    the majority of the cycles used by the memory operations. If we are using the
    dot product operator, there are even fewer FP operations required, and most of
    the time is spent waiting (NOPs) for the dot product to write back to the SP.
    All final vector reductions end up in the first SP, and we can use the multi-threaded
    CPU or MCU eGPU dynamic scaling personalities to write these values to the shared
    memory.


    The MMMs are much more complex. Although the algorithm itself is very simple,
    consisting only of a three level loop, the standard GPU implementation requires
    a vector reduction. While the cycle count increases as expected (∼ 4×) from 32×32
    to 64×64, there is an unexpected jump from 64×64 to 128×128, which is particularly
    evident in the eGPU-Dot case. Analysis of the code shows that while we are able
    to store the entire matrix (or at least a majority of the matrix) in the SP registers
    (there are 16384 total registers across the 16 SPs in the configuration we have
    chosen here) for the 32×32 and 64×64 cases, we need to keep reloading portions
    of the matrix in the 128×128 case, which can also be seen in the profile stack
    in Figure [6.](#page-8-1) Of course, there is always the option of increasing
    the maximum thread space or registers per thread (through parameterization) if
    the expected workloads were larger matrices. Compared to the vector reduction
    (where we profile a single vector), the thread initialization and integer operations
    are amortized away as we operate on many vectors. The NOPs also disappear as the
    the thread depth increases here.


    The bitonic sort benchmark requires a wider mix of instructions. Predicates are
    required, which increases the effective cost of the eGPU core by about 50%. The
    smaller sorts require many NOPs, which progressively reduce as the number of wavefronts
    increase for the larger datasets. The nature of the bitonic sort tends to use
    many subroutine calls, which we can see here in the relatively large number of
    branch operations. Again, the memory operations take the majority of all cycles,
    as each pass of the sort requires a redistribution of the data among the SPs.
    While the eGPU-QP version requires fewer clock cycles because of the increased
    write bandwidth, the normalized cost of the QP version is higher, largely because
    of the lower clock frequency.


    A similar pattern of instruction distribution is seen in the FFT. Increasing wavefront
    depth for larger datasets reduces NOPs significantly. The number of FP instructions
    (which are doing the actual FFT calculations) is relatively small, at about 10%.
    The largest proportion of operations are once again the memory accesses, especially
    in the write to shared memory; using the QP version of the eGPU results in a 20%
    to 30% decrease in total cycles. The normalized cost of the two eGPU versions,
    however, is approximately the


    Table 8: Bitonic Sort and FFT Benchmarks


    |     |               |        | Bitonic Sort |       | FFT    |      |            |  |  |

    |-----|---------------|--------|--------------|-------|--------|------|------------|--|--|

    | Dim | Metric        |        | eGPU         | eGPU  |        | eGPU | eGPU<br>QP
    |  |  |

    |     |               | Nios   | DP           | QP    | Nios   | DP   |            |  |  |

    |     | Cycles        | 8457   | 1742         | 1543  | 9165   | 876  | 714        |  |  |

    |     | Time(us)      | 24.37  | 2.25         | 2.51  | 26.41  | 1.14 | 1.19       |  |  |

    | 32  | Ratio(cycles) | 4.89   | 1.0          | 0.86  | 10.46  | 1.0  | 0.82       |  |  |

    |     | Ratio(time)   | 10.8   | 1.0          | 1.1   | 23.16  | 1.0  | 1.04       |  |  |

    |     | Normalized    | 1.24   | 1.0          | 1.24  | 4.38   | 1.0  | 1.18       |  |  |

    |     | Cycles        | 20687  | 3728         | 3054  | 20848  | 1695 | 1312       |  |  |

    |     | Time(us)      | 59.6   | 4.83         | 5.09  | 60.08  | 2.20 | 2.19       |  |  |

    | 64  | Ratio(cycles) | 5.54   | 1.0          | 0.82  | 12.30  | 1.0  | 0.82       |  |  |

    |     | Ratio(time)   | 12.3   | 1.0          | 1.05  | 27.31  | 1.0  | 1.01       |  |  |

    |     | Normalized    | 1.42   | 1.0          | 1.18  | 5.17   | 1.0  | 1.13       |  |  |

    |     | Cycles        | 49741  | 8326         | 6536  | 46667  | 3463 | 2558       |  |  |

    |     | Time(us)      | 143.3  | 10.8         | 10.9  | 134.49 | 4.29 | 4.26       |  |  |

    | 128 | Ratio(cycles) | 5.97   | 1.0          | 0.79  | 13.48  | 1.0  | 0.74       |  |  |

    |     | Ratio(time)   | 13.2   | 1.0          | 1.01  | 31.35  | 1.0  | 0.95       |  |  |

    |     | Normalized    | 1.48   | 1.0          | 1.13  | 5.93   | 1.0  | 1.08       |  |  |

    |     | Cycles        | 149271 | 16578        | 11974 | 103636 | 6813 | 4736       |  |  |

    |     | Time(us)      | 430.2  | 21.5         | 19.9  | 298.66 | 8.84 | 7.89       |  |  |

    | 256 | Ratio(cycles) | 9.0    | 1.0          | 0.72  | 15.21  | 1.0  | 0.70       |  |  |

    |     | Ratio(time)   | 20.0   | 1.0          | 0.93  | 33.79  | 1.0  | 0.89       |  |  |

    |     | Normalized    | 2.24   | 1.0          | 1.05  | 6.39   | 1.0  | 1.01       |  |  |


    same, with the high clock frequency of the base version offsetting the higher
    memory bandwidth of the QP version. These results also point to a better optimization
    for the FFT: by using a higher radix FFT, there will be correspondingly fewer
    passes through the shared memory. (We have a extensive flexibility in specifying
    the register and thread parameters, we can easily support much higher radices,
    which will require much larger register spaces).


    Comparing against Nios, we can see that the eGPU performs very well. We see at
    least an OOM performance difference based on time, and in almost all cases on
    a cycle basis as well. This tells us that eGPU is a more efficient architecture
    than a RISC processor, and is a viable candidate for a soft accelerator core.


    ## 8 CONCLUSIONS


    We have demonstrated a GPGPU that consistently beats 770 MHz for a wide range
    of parameters, and described the design approach required to reach such frequencies.
    We are able to swap in and out features as well as change the precision of the
    integer ALU to optimize for area and resource balancing in the FPGA.


    For the eGPU to be useful in an actual system design, it must offer an improvement
    over known methods. We compare the eGPU to a mature commercial soft CPU (Nios)
    over a number of benchmarks. The eGPU is much better on a cycle by cycle or elasped
    time basis in all cases we tried (typically by one to two OOM), and is still better
    on an area normalized basis. When we add the dot product core which can be used
    directly by the eGPU in a regular GPGPU context - the advantage can increase again
    by several times. A soft GPU therefore can offer a valid implementation option
    for many types of algorithms. This does not mean that a GPGPU will replace the
    RISC, anymore than a discrete GPGPU will replace a discrete RISC, only that we
    have shown that the soft GPGPU can now be considered for commercial designs, rather
    than just being of academic interest. The eGPU only uses 1%-2% of a current mid-range
    device, making it a cost effective option to implement complex algorithms in a
    larger FPGA system design, even if multiple cores are required.


    A Statically and Dynamically Scalable Soft GPGPU


    ## REFERENCES


    - <span id="page-10-30"></span>[1] 2016. Nios II Classic Processor Reference GuideNios
    II Classic Processor Reference Guide. [https://www.intel.com/content/www/us/en/docs/programmable/683620/](https://www.intel.com/content/www/us/en/docs/programmable/683620/current/overview-67435.html)
    [current/overview-67435.html.](https://www.intel.com/content/www/us/en/docs/programmable/683620/current/overview-67435.html)

    - <span id="page-10-0"></span>[2] 2017. FFT IP Core: User Guide. [https://www.intel.co.uk/content/www/uk/en/](https://www.intel.co.uk/content/www/uk/en/products/details/fpga/intellectual-property/dsp/fft.html)
    [products/details/fpga/intellectual-property/dsp/fft.html.](https://www.intel.co.uk/content/www/uk/en/products/details/fpga/intellectual-property/dsp/fft.html)

    - <span id="page-10-2"></span>[3] 2017. High-speed Reed-Solomon IP Core User Guide.
    [https://www.intel.com/](https://www.intel.com/content/www/us/en/docs/programmable/683120/17-1/about-the-high-speed-reed-solomon-ip-core.html)
    [content/www/us/en/docs/programmable/683120/17-1/about-the-high-speed](https://www.intel.com/content/www/us/en/docs/programmable/683120/17-1/about-the-high-speed-reed-solomon-ip-core.html)[reed-solomon-ip-core.html.](https://www.intel.com/content/www/us/en/docs/programmable/683120/17-1/about-the-high-speed-reed-solomon-ip-core.html)

    - <span id="page-10-4"></span>[4] 2018. Microblaze Processor Reference Guide.
    [https://docs.xilinx.com/v/u/2018.2-](https://docs.xilinx.com/v/u/2018.2-English/ug984-vivado-microblaze-ref)
    [English/ug984-vivado-microblaze-ref.](https://docs.xilinx.com/v/u/2018.2-English/ug984-vivado-microblaze-ref)

    - <span id="page-10-15"></span>[5] 2020. HB0919 Handbook CoreVectorBlox. [https://www.microsemi.com/existing](https://www.microsemi.com/existing-parts/parts/152678)[parts/parts/152678.](https://www.microsemi.com/existing-parts/parts/152678)

    - <span id="page-10-19"></span>[6] 2021. Block-by-Block Configurable Fast Fourier
    Transform Implementation on AI Engine (XAPP1356). [https://docs.xilinx.com/r/en-US/xapp1356-fft-ai-engine/FFT](https://docs.xilinx.com/r/en-US/xapp1356-fft-ai-engine/FFT-on-Multiple-AI-Engines)[on-Multiple-AI-Engines.](https://docs.xilinx.com/r/en-US/xapp1356-fft-ai-engine/FFT-on-Multiple-AI-Engines)

    - <span id="page-10-23"></span>[7] 2021. Intel Agilex7 FPGAs and SoCs F-Series:
    Product Table. [https://www.intel.](https://www.intel.com/content/dam/www/programmable/us/en/pdfs/literature/pt/intel-agilex-f-series-product-table.pdf)
    [com/content/dam/www/programmable/us/en/pdfs/literature/pt/intel-agilex-f](https://www.intel.com/content/dam/www/programmable/us/en/pdfs/literature/pt/intel-agilex-f-series-product-table.pdf)[series-product-table.pdf.](https://www.intel.com/content/dam/www/programmable/us/en/pdfs/literature/pt/intel-agilex-f-series-product-table.pdf)

    - <span id="page-10-1"></span>[8] 2022. Fast Fourier Transform v9.1. [https://www.xilinx.com/content/dam/xilinx/](https://www.xilinx.com/content/dam/xilinx/support/documents/ip_documentation/xfft/v9_1/pg109-xfft.pdf)
    [support/documents/ip\\_documentation/xfft/v9\\_1/pg109-xfft.pdf.](https://www.xilinx.com/content/dam/xilinx/support/documents/ip_documentation/xfft/v9_1/pg109-xfft.pdf)

    - <span id="page-10-3"></span>[9] 2022. Nios V Processor Reference Manual. [https://www.intel.com/content/www/](https://www.intel.com/content/www/us/en/products/details/fpga/nios-processor/v.html)
    [us/en/products/details/fpga/nios-processor/v.html.](https://www.intel.com/content/www/us/en/products/details/fpga/nios-processor/v.html)

    - <span id="page-10-27"></span>[10] 2023. Floating-Point IP Cores User Guide.
    [https://www.intel.com/content/www/](https://www.intel.com/content/www/us/en/docs/programmable/683750/23-1/about-floating-point-ip-cores.html)
    [us/en/docs/programmable/683750/23-1/about-floating-point-ip-cores.html.](https://www.intel.com/content/www/us/en/docs/programmable/683750/23-1/about-floating-point-ip-cores.html)

    - <span id="page-10-24"></span>[11] 2023. Intel Agilex 7 Variable Precision DSP
    Blocks. [https://www.intel.com/](https://www.intel.com/content/www/us/en/docs/programmable/683037/23-3/variable-precision-dsp-blocks-overview.html)
    [content/www/us/en/docs/programmable/683037/23-3/variable-precision-dsp](https://www.intel.com/content/www/us/en/docs/programmable/683037/23-3/variable-precision-dsp-blocks-overview.html)[blocks-overview.html.](https://www.intel.com/content/www/us/en/docs/programmable/683037/23-3/variable-precision-dsp-blocks-overview.html)

    - <span id="page-10-22"></span>[12] 2023. Intel Agilex7 Embedded Memory User Guide.
    [https://www.intel.com/](https://www.intel.com/content/www/us/en/docs/programmable/683241/23-2/embedded-memory-overview.html)
    [content/www/us/en/docs/programmable/683241/23-2/embedded-memory](https://www.intel.com/content/www/us/en/docs/programmable/683241/23-2/embedded-memory-overview.html)[overview.html.](https://www.intel.com/content/www/us/en/docs/programmable/683241/23-2/embedded-memory-overview.html)

    - <span id="page-10-18"></span>[13] 2023. Versal Adaptive SoC AI Engine Architecture
    Manual (AM009). [https:](https://docs.xilinx.com/v/u/en-US/wp506-ai-engine) [//docs.xilinx.com/v/u/en-US/wp506-ai-engine.](https://docs.xilinx.com/v/u/en-US/wp506-ai-engine)

    - <span id="page-10-5"></span>[14] Abdullah Al-Dujaili, Florian Deragisch, Andrei
    Hagiescu, and Weng-Fai Wong. 2012. Guppy: A GPU-like soft-core processor. In 2012
    International Conference on Field-Programmable Technology. 57–60.<https://doi.org/10.1109/FPT.2012.6412112>

    - <span id="page-10-6"></span>[15] Muhammed Al Kadi, Benedikt Janssen, and Michael
    Huebner. 2016. FGPU: An SIMT-Architecture for FPGAs. In Proceedings of the 2016
    ACM/SIGDA International Symposium on Field-Programmable Gate Arrays (Monterey,
    California, USA) (FPGA ''16). Association for Computing Machinery, New York, NY,
    USA, 254–263.<https://doi.org/10.1145/2847263.2847273>

    - <span id="page-10-25"></span>[16] Kevin Andryc. 2018. An Architecture Evaluation
    and Implementaiton of a Soft GPGPU for FPGAs. (2018).<https://doi.org/10.7275/12722172>

    - <span id="page-10-7"></span>[17] Kevin Andryc, Murtaza Merchant, and Russell
    Tessier. 2013. FlexGrip: A soft GPGPU for FPGAs. In 2013 International Conference
    on Field-Programmable Technology (FPT). 230–237.<https://doi.org/10.1109/FPT.2013.6718358>

    - <span id="page-10-8"></span>[18] Raghuraman Balasubramanian, Vinay Gangadhar,
    Ziliang Guo, Chen-Han Ho, Cherin Joseph, Jaikrishnan Menon, Mario Paulo Drumond,
    Robin Paul, Sharath Prasad, Pradip Valathol, and Karthikeyan Sankaralingam. 2015.
    Enabling GPGPU Low-Level Hardware Explorations with MIAOW: An Open-Source RTL
    Implementation of a GPGPU. ACM Trans. Archit. Code Optim. 12, 2, Article 21 (jun
    2015), 25 pages.<https://doi.org/10.1145/2764908>

    - <span id="page-10-26"></span>[19] K. E. Batcher. 1968. Sorting Networks and
    Their Applications. In Proceedings of the April 30–May 2, 1968, Spring Joint Computer
    Conference (Atlantic City, New Jersey) (AFIPS ''68 (Spring)). Association for
    Computing Machinery, New York,


    NY, USA, 307–314.<https://doi.org/10.1145/1468075.1468121>


    - <span id="page-10-16"></span>[20] Hui Yan Cheah, Fredrik Brosser, Suhaib A.
    Fahmy, and Douglas L. Maskell. 2014. The IDEA DSP Block-Based Soft Processor for
    FPGAs. ACM Trans. Reconfigurable Technol. Syst. 7, 3, Article 19 (sep 2014), 23
    pages.<https://doi.org/10.1145/2629443>

    - <span id="page-10-17"></span>[21] Hui Yan Cheah, Suhaib A. Fahmy, and Nachiket
    Kapre. 2014. Analysis and optimization of a deeply pipelined FPGA soft processor.
    In 2014 International Conference on Field-Programmable Technology (FPT). 235–238.
    [https://doi.org/10.](https://doi.org/10.1109/FPT.2014.7082783) [1109/FPT.2014.7082783](https://doi.org/10.1109/FPT.2014.7082783)

    - <span id="page-10-12"></span>[22] Christopher Han-Yu Chou, Aaron Severance,
    Alex D. Brant, Zhiduo Liu, Saurabh Sant, and Guy G. Lemieux. 2011. VEGAS: soft
    vector processor with scratchpad memory. In Proceedings of the ACM/SIGDA 19th
    International Symposium on Field Programmable Gate Arrays, FPGA 2011, Monterey,
    California, USA, February 27, March 1, 2011, John Wawrzynek and Katherine Compton
    (Eds.). ACM, 15–24. <https://doi.org/10.1145/1950413.1950420>

    - <span id="page-10-21"></span>[23] Jeffrey Chromczak, Mark Wheeler, Charles Chiasson,
    Dana How, Martin Langhammer, Tim Vanderhoek, Grace Zgheib, and Ilya Ganusov. 2020.
    Architectural Enhancements in Intel® Agilex™ FPGAs. In FPGA ''20: The 2020 ACM/SIGDA
    International Symposium on Field-Programmable Gate Arrays, Seaside, CA, USA, February
    23-25, 2020, Stephen Neuendorffer and Lesley Shannon (Eds.). ACM, 140–149.<https://doi.org/10.1145/3373087.3375308>

    - <span id="page-10-9"></span>[24] Pedro Duarte, Pedro Tomas, and Gabriel Falcao.
    2017. SCRATCH: An End-to-End Application-Aware Soft-GPGPU Architecture and Trimming
    Tool. In Proceedings of the 50th Annual IEEE/ACM International Symposium on Microarchitecture
    (Cambridge, Massachusetts) (MICRO-50 ''17). Association for Computing Machinery,
    New York, NY, USA, 165–177.<https://doi.org/10.1145/3123939.3123953>

    - <span id="page-10-10"></span>[25] Jeffrey Kingyens and J. Gregory Steffan. 2010.
    A GPU-inspired soft processor for high-throughput acceleration. In 2010 IEEE International
    Symposium on Parallel and Distributed Processing, Workshops and Phd Forum (IPDPSW).
    1–8. [https:](https://doi.org/10.1109/IPDPSW.2010.5470679) [//doi.org/10.1109/IPDPSW.2010.5470679](https://doi.org/10.1109/IPDPSW.2010.5470679)

    - <span id="page-10-28"></span>[26] Ian Kuon and Jonathan Rose. 2006. Measuring
    the gap between FPGAs and ASICs. In Proceedings of the ACM/SIGDA 14th International
    Symposium on Field Programmable Gate Arrays, FPGA 2006, Monterey, California,
    USA, February 22- 24, 2006, Steven J. E. Wilton and André DeHon (Eds.). ACM, 21–30.
    [https:](https://doi.org/10.1145/1117201.1117205) [//doi.org/10.1145/1117201.1117205](https://doi.org/10.1145/1117201.1117205)

    - <span id="page-10-29"></span>[27] Martin Langhammer and Gregg Baeckler. 2018.
    High Density and Performance Multiplication for FPGA. In 25th IEEE Symposium on
    Computer Arithmetic, ARITH 2018, Amherst, MA, USA, June 25-27, 2018. IEEE, 5–12.
    [https://doi.org/10.1109/](https://doi.org/10.1109/ARITH.2018.8464695) [ARITH.2018.8464695](https://doi.org/10.1109/ARITH.2018.8464695)

    - <span id="page-10-20"></span>[28] Martin Langhammer and George A. Constantinides.
    2023. eGPU: A 750 MHz Class Soft GPGPU for FPGA. In 2023 33rd International Conference
    on Field-Programmable Logic and Applications (FPL). 277–282. [https://doi.org/10.1109/](https://doi.org/10.1109/FPL60245.2023.00047)
    [FPL60245.2023.00047](https://doi.org/10.1109/FPL60245.2023.00047)

    - <span id="page-10-11"></span>[29] Rui Ma, Jia-Ching Hsu, Tian Tan, Eriko Nurvitadhi,
    Rajesh Vivekanandham, Aravind Dasu, Martin Langhammer, and Derek Chiou. 2021.
    DO-GPU: Domain Optimizable Soft GPUs. In 2021 31st International Conference on
    Field-Programmable Logic and Applications (FPL). 140–144. [https://doi.org/10.1109/FPL53798.2021.](https://doi.org/10.1109/FPL53798.2021.00031)
    [00031](https://doi.org/10.1109/FPL53798.2021.00031)

    - <span id="page-10-13"></span>[30] Aaron Severance and Guy Lemieux. 2012. VENICE:
    A compact vector processor for FPGA applications. In 2012 International Conference
    on Field-Programmable Technology, FPT 2012, Seoul, Korea (South), December 10-12,
    2012. IEEE, 261–268. <https://doi.org/10.1109/FPT.2012.6412146>

    - <span id="page-10-14"></span>[31] Aaron Severance and Guy G. F. Lemieux. 2013.
    Embedded supercomputing in FPGAs with the VectorBlox MXP Matrix Processor. In
    Proceedings of the International Conference on Hardware/Software Codesign and
    System Synthesis, CODES+ISSS 2013, Montreal, QC, Canada, September 29 - October
    4, 2013. IEEE, 6:1–6:10.<https://doi.org/10.1109/CODES-ISSS.2013.6658993>'
  decisions:
    evaluation_prompt: 'Qualified. Reason: The paper contains multiple sections dedicated
      to evaluation, including "RESULTS" and "BENCHMARKS", and provides empirical
      data through tables and figures. It discusses benchmarking against other processors,
      uses metrics for performance assessment, and includes phrases indicating structured
      evaluation, such as "We benchmark several algorithms" and "We ran a number of
      workloads to evaluate absolute and relative performance.'
    related_work_prompt: '- Qualified. Reason: The paper meaningfully engages with
      prior research by discussing and comparing its approach to existing soft GPGPU
      architectures and processors. It includes a substantial number of citations
      and references to previous work throughout the text, particularly in the Introduction
      and Background sections, where it compares its method to previous designs and
      discusses the limitations and advantages of its approach relative to existing
      solutions.'
    novelty_prompt: 'Qualified. Reason: The paper proposes a novel parameterized SIMT
      processor for FPGA, introduces the concept of both static and dynamic scalability
      in a soft GPGPU context, and demonstrates a soft processor that consistently
      closes timing at high performance levels. It also claims to offer improvements
      over existing methods and provides a detailed comparison with other architectures,
      highlighting its unique contributions and performance advantages.'
    review_only_prompt: 'Qualified. Reason: The paper introduces a novel parameterized
      SIMT processor for FPGA, demonstrates dynamic and static scalability, and provides
      original experiments and benchmarks comparing the proposed architecture with
      existing solutions. It clearly makes novel contributions to the field.'
