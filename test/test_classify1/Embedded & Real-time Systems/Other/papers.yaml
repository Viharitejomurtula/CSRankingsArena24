papers:
- title: "X-HEEP: An Open-Source, Configurable and Extendible RISC-V\n  Microcontroller\
    \ for the Exploration of Ultra-Low-Power Edge Accelerators"
  abstract: 'The field of edge computing has witnessed remarkable growth owing to
    the

    increasing demand for real-time processing of data in applications. However,

    challenges persist due to limitations in performance and power consumption. To

    overcome these challenges, heterogeneous architectures have emerged that

    combine host processors with specialized accelerators tailored to specific

    applications, leading to improved performance and reduced power consumption.

    However, most of the existing platforms lack the necessary configurability and

    extendability options for integrating custom accelerators. To overcome these

    limitations, we introduce in this paper the eXtendible Heterogeneous

    Energy-Efficient Platform (X-HEEP). X-HEEP is an open-source platform designed

    to natively support the integration of ultra-low-power edge accelerators. It

    provides customization options to match specific application requirements by

    exploring various core types, bus topologies, addressing modes, memory sizes,

    and peripherals. Moreover, the platform prioritizes energy efficiency by

    implementing low-power strategies, such as clock-gating and power-gating. We

    demonstrate the real-world applicability of X-HEEP by providing an integration

    example tailored for healthcare applications that includes a coarse-grained

    reconfigurable array (CGRA) and in-memory computing (IMC) accelerators. The

    resulting design, called HEEPocrates, has been implemented both in field

    programmable gate array (FPGA) on the Xilinx Zynq-7020 chip and in silicon with

    TSMC 65nm low-power CMOS technology. We run a set of healthcare applications

    and measure their energy consumption to demonstrate the alignment of our chip

    with other state-of-the-art microcontrollers commonly adopted in this domain.

    Moreover, we present the energy benefits of 4.9x and 4.8x gained by exploiting

    the integrated CGRA and IMC accelerators compared to running on the host CPU.'
  url: http://arxiv.org/abs/2401.05548v2
  keywords: ''
  document: '# X-HEEP: An Open-Source, Configurable and Extendible RISC-V Microcontroller
    for the Exploration of Ultra-Low-Power Edge Accelerators


    [SIMONE MACHETTI,](HTTPS://ORCID.ORG/0000-0002-2887-5031) Embedded Systems Laboratory
    (ESL), EPFL, Switzerland [PASQUALE DAVIDE SCHIAVONE,](HTTPS://ORCID.ORG/0000-0003-2931-0435)
    Embedded Systems Laboratory (ESL), EPFL, Switzerland [THOMAS CHRISTOPH MÜLLER,](HTTPS://ORCID.ORG/0009-0004-2805-6310)
    Embedded Systems Laboratory (ESL), EPFL, Switzerland [MIGUEL PEÓN-QUIRÓS,](HTTPS://ORCID.ORG/0000-0002-5760-090X)
    EcoCloud, EPFL, Switzerland [DAVID ATIENZA,](HTTPS://ORCID.ORG/0000-0001-9536-4947)
    Embedded Systems Laboratory (ESL), EPFL, Switzerland


    The field of edge computing has witnessed remarkable growth owing to the increasing
    demand for real-time processing of data in applications. However, challenges persist
    due to limitations in the performance and power efficiency of edge-computing devices.
    To overcome these challenges, heterogeneous architectures have emerged that combine
    host processors with specialized accelerators tailored to specific applications,
    leading to improved performance and reduced power consumption. However, most of
    the existing platforms lack configurability and extendability options, necessitating
    extensive modifications of the register transfer level (RTL) code for integrating
    custom accelerators.


    To overcome these limitations, we introduce in this paper the eXtendible Heterogeneous
    Energy-Efficient Platform (X-HEEP). X-HEEP is an open-source platform designed
    to natively support the integration of ultra-low-power edge accelerators. It provides
    customization options to match specific application requirements by exploring
    various core types, bus topologies, and memory addressing modes. It also enables
    a fine-grained configuration of memory banks to match the constraints of the integrated
    accelerators. The platform prioritizes energy efficiency by implementing low-power
    strategies, such as clock-gating and power-gating, and integrating these with
    connected accelerators through dedicated power control interfaces.


    We demonstrate the real-world applicability of X-HEEP by providing an integration
    example tailored for healthcare applications that includes a coarse-grained reconfigurable
    array (CGRA) and in-memory computing (IMC) accelerators. The resulting design,
    called HEEPocrates, has been implemented both in field programmable gate arrays
    (FPGAs) on multiple Xilinx chips, for prototyping and exploration, and in silicon
    with TSMC 65 nm low-power CMOS technology. The fabricated chip can operate from
    0.8 V to 1.2 V, achieving a maximum frequency of 170 MHz and 470 MHz, respectively.
    Its power consumption ranges from 270 µW at 32 kHz and 0.8 V, to 48 mW at 470
    MHz and 1.2 V.


    We run a set of healthcare applications and measure their energy consumption to
    demonstrate the alignment of our chip with other state-of-the-art microcontrollers
    commonly adopted in this domain, showing that HEEPocrates provides a good trade-off
    between acquisition-dominated and processing-dominated applications for energy
    efficiency. Moreover, we present the energy benefits of 4.9 × and 4.8 × gained
    by exploiting the integrated CGRA accelerator and IMC accelerator, respectively,
    compared to running on the host CPU.


    Additional Key Words and Phrases: Ultra-Low Power, Energy Efficiency, Microcontroller,
    Accelerator, Field Programmable Gate Array (FPGA), Course-Grained Reconfigurable
    Array (CGRA), In-Memory Computing (IMC), Tapeout, Silicon Validation.


    Authors'' addresses: [Simone Machetti,](https://orcid.org/0000-0002-2887-5031)
    Embedded Systems Laboratory (ESL), EPFL, Lausanne, Switzerland; [Pasquale Davide
    Schiavone,](https://orcid.org/0000-0003-2931-0435) Embedded Systems Laboratory
    (ESL), EPFL, Lausanne, Switzerland; [Thomas Christoph Müller,](https://orcid.org/0009-0004-2805-6310)
    Embedded Systems Laboratory (ESL), EPFL, Lausanne, Switzerland; [Miguel Peón-Quirós,](https://orcid.org/0000-0002-5760-090X)
    EcoCloud, EPFL, Lausanne, Switzerland; [David Atienza,](https://orcid.org/0000-0001-9536-4947)
    Embedded Systems Laboratory (ESL), EPFL, Lausanne, Switzerland.


    © 2024 Association for Computing Machinery.


    Manuscript submitted to ACM


    Permission to make digital or hard copies of all or part of this work for personal
    or classroom use is granted without fee provided that copies are not made or distributed
    for profit or commercial advantage and that copies bear this notice and the full
    citation on the first page. Copyrights for components of this work owned by others
    than ACM must be honored. Abstracting with credit is permitted. To copy otherwise,
    or republish, to post on servers or to redistribute to lists, requires prior specific
    permission and/or a fee. Request permissions from permissions@acm.org.


    #### ACM Reference Format:


    Simone Machetti, Pasquale Davide Schiavone, Thomas Christoph Müller, Miguel Peón-Quirós,
    and David Atienza. 2024. X-HEEP: An Open-Source, Configurable and Extendible RISC-V
    Microcontroller for the Exploration of Ultra-Low-Power Edge Accelerators. 1, 1
    (March 2024), [21](#page-20-0) pages. <https://doi.org/10.1145/nnnnnnn.nnnnnnn>


    #### 1 INTRODUCTION


    In recent years, the field of edge computing has witnessed remarkable growth and
    adoption in commercial products. This process has been driven by the increasing
    demand for real-time computing solutions, particularly Artificial Intelligence
    (AI) and Machine Learning (ML) algorithms. As data processing at the edge for
    new edge AI computing has become more prevalent, the performance and power consumption
    limitations of edge-computing devices have become increasingly apparent, which
    has posed significant challenges for researchers and engineers.


    Heterogeneous architectures have emerged to overcome these challenges. These architectures
    offer a promising path toward high energy efficiency while maintaining performance
    constraints. Heterogeneous architectures rely on a combination of ultra-low-power
    host processors to run control and communication tasks, and custom accelerators
    tailored to specific application domains, such as artificial intelligence, image
    processing, healthcare, and cryptography, to run computationally demanding tasks.


    Control and communication tasks include accessing external memories, acquiring
    data from analog-to-digital converters (ADCs) or sensors, preparing data for computations,
    and running real-time operating system (RTOS) functions. Meanwhile, computational
    demanding tasks focus on data processing, for example, convolutional and fully
    connected layers in neural networks (NNs), fast-Fourier transforms (FFTs) in temporal
    series, secure-hash algorithms (SHA) in cryptography, etc.


    Each accelerator comes with unique requirements, such as memory size, area, performance,
    and power, to meet the constraints of the target applications. For this reason,
    proper customization of host platforms is imperative. This may include exploring
    different CPUs to trade performance and power, bus topologies and memory hierarchy,
    memory sizes to accommodate the required computational data, peripherals to provide
    the necessary I/O connectivity, power domains and strategies, etc. However, commercial
    platforms limit hardware exploration due to their non-open-source nature. They
    often involve costly licensing models and do not allow for customization. As a
    result, there is a growing preference for open-source platforms as a more attractive
    solution that does not limit exploration and customization, and that gives designers
    digital sovereignty and control over IPs.


    Today, there are an increasing number of open-source projects related to heterogeneous
    systems, thanks to the open RISC-V instruction set architecture (ISA) revolution.
    However, many of such platforms focus only on the CPU part, whereas microcontroller-based
    state-of-the-art projects lack the flexibility and customization options needed
    to fulfill accelerator requirements natively. These limitations include restricted
    configurability for the internal platform''s components (core, memory, bus, etc.)
    to adapt to the application needs, limited support for external accelerator connectivity
    to communicate with the host system, and inadequate built-in power management
    strategies to optimize energy efficiency. Thus, hardware developers need to extensively
    modify the platform to properly align with the target applications on their own
    copy of the platform. This includes forking, modifying, and maintaining the forked
    platform''s repository, leading to high maintenance costs. Therefore, addressing
    the configurability and extendability aspects of these platforms is crucial to
    lowering the adoption barrier of open-source-based edge solutions.


    In this paper, we address the limitations mentioned above by introducing X-HEEP[1](#page-2-0)
    [\[30\]](#page-20-1), an open-source configurable and extendable platform designed
    to support the exploration of ultra-low power edge accelerators. X-HEEP is a streamlined
    configurable host architecture based on RISC-V and built on top of existing IPs
    from relevant open-source projects, such as the PULP project, the OpenHW Group,
    and the OpenTitan project, as extensively verified, validated in silicon, and
    adopted in commercial products. This allows extensive reuse of third-party hardware
    and software extensions and inheriting verification and silicon validation.


    To allow users to explore their custom solutions, X-HEEP can be natively extended
    via the proposed eXtendible Accelerator InterFace (XAIF), which allows the integration
    of a wide range of accelerators with different area, power, and performance constraints.
    Having a complete interface that covers all the edge accelerator performance and
    power requirements will enable extensive reuse of hardware and software IPs, reducing
    costs and mitigating fragmentation. To explore the custom hardware design space,
    users will use X-HEEP as an IP which exposes an interface capable of addressing
    all the edge-computing state-of-the-art requirements for domain-specific applications.
    To enable a high degree of versatility, such exploration can be performed both
    on FPGAs or RTL simulators, as well as SystemC for a mixed high-level and RTL
    simulation environment. Additionally, to offer high degree of optimizations, X-HEEP
    offers internal configurability options through the selection of different (1)
    core types, depending on the target workloads [\[29\]](#page-20-2); (2) bus topology
    and addressing mode, ensuring a perfect match with the bandwidth requirements
    of the integrated accelerators; (3) memory size, depending on the processing data
    and application complexity; and finally (4) peripherals, to provide the needed
    I/O connectivity. This configurability enables designers to tailor the platform
    to specific application requirements and meet area, power, and performance constraints.


    As energy efficiency is a key figure in edge computing devices, X-HEEP implements
    state-of-the-art fine-grained low-power strategies such as clock-gating, power-gating,
    and RAM retention, which are integrated into the XAIF interface to be leveraged
    by the connected accelerators and maximize overall energy efficiency.


    To demonstrate the real-world applicability of X-HEEP, we present an integration
    example specifically tailored for ultra-low-power healthcare applications. These
    applications typically involve long and slow acquisition periods, where data from
    external bio-sensors are stored in memory while the rest of the system is in an
    idle state, followed by intense processing periods to compute pattern extraction
    algorithms based on digital signal processing algorithms, or machine learning
    (ML), and deep learning. Therefore, we extended X-HEEP with a CGRA accelerator
    [\[9\]](#page-19-0) and an IMC accelerator [\[31\]](#page-20-3), both of which
    have been shown to efficiently reduce the overall energy consumption of healthcare
    applications [\[7,](#page-19-1) [24\]](#page-20-4). We configured X-HEEP with
    the RISC-V OpenHW Group CV32E20 core [\[29\]](#page-20-2), 8 banks of 32 KiB on-chip
    SRAM organized in a contiguous addressing mode, and 11 different power domains
    (including the external accelerators) that can be individually switched on and
    off for fine-grained power control.


    The resulting design, called HEEPocrates, has been implemented both in FPGAs on
    the Zynq 7020, Zynq UltraScale+, and Artix 7 chips by Xilinx, for early prototyping,
    verification, and system exploration, as well as in silicon with TSMC 65 nm low-power
    CMOS technology, for silicon validation and profiling performance, power, and
    area figures. The measured performance of the fabricated chip shows that it can
    operate in a wide voltage range, from 0.8 V to 1.2 V, achieving a maximum frequency
    of 170 MHz and 470 MHz, respectively. Its power consumption ranges from 270 µW
    at 32 kHz and 0.8 V, to 48 mW at 470 MHz and 1.2 V.


    To validate our design and compare it with state-of-the-art solutions, we measured
    the energy consumption of the chip in a set of healthcare applications, showing
    that it offers a good trade-off between the computationally hungry


    <span id="page-2-0"></span><sup>1</sup>X-HEEP is freely downloadable at https://github.com/esl-epfl/x-heep
    under a permissive license.


    and acquisition-dominated state-of-the-art microcontrollers commonly adopted in
    this domain. This demonstrates the flexibility of our host platform in adapting
    to the specific needs of integrated accelerators and matching the strict requirements
    of healthcare applications. Furthermore, we present the energy benefits of 4.9
    × and 4.8 × gained by exploiting the integrated CGRA accelerator and the IMC accelerator,
    respectively, compared to running on the host CPU.


    Throughout this work, we will dive deeper into the features of the X-HEEP platform,
    describing its architecture, configurability, and extendability options to build
    versatile and energy-efficient edge applications.


    The following contributions are presented:


    - X-HEEP: A configurable and extendible RISC-V microcontroller to support the
    exploration of ultra-low-power edge accelerators.

    - XAIF: A configurable interface that adapts to the different requirements of
    accelerators in terms of programmability, bandwidth, interrupts, and power modes,
    and that allows their seamless integration into the X-HEEP architecture.

    - HEEPocrates: A real-world integration example, based on TSMC 65 nm low-power
    CMOS technology, that includes a CGRA and an IMC accelerator.

    - An open-source repository with a permissive license with the complete X-HEEP
    platform code and documentation to allow researchers to explore new custom accelerators
    and advance research in this field.


    The remainder of this paper is structured as follows. In Section 2, we conduct
    an in-depth analysis of the most relevant state-of-the-art accelerators and host
    platforms. In Section 3, we provide a qualitative and quantitative description
    of the configurability and extendability features of X-HEEP. In Section 4, we
    present a real-world integration example, called HEEPocrates. In Section 5 we
    describe our experimental setup, while in Section 6 our experimental results.
    Lastly, in Section 7, we offer a comprehensive summary of the main conclusions
    of our work.


    #### 2 STATE-OF-THE-ART


    This section gives an overview of cutting-edge accelerators, analyzing the fundamental
    requirements necessary for their integration into host platforms. Such requirements
    are collected into the XAIF to accommodate all the state-of-the-art accelerators.
    Subsequently, it focuses on host platforms from the state of the art, conducting
    an evaluation of their strengths and limitations in terms of configurability and
    extendability.


    #### 2.1 Edge-computing accelerators


    The extensive array of open-source accelerators includes a diverse range of requirements
    regarding memory capacity, area, performance, and power efficiency. Therefore,
    an analysis of their main features becomes imperative for the design of flexible
    and efficient host platforms.


    We can divide accelerators into three main categories: memories, processors (and
    co-processors), and I/O peripherals. One accelerator can belong to one or more
    categories.


    <span id="page-3-0"></span>2.1.1 Memories. Memory accelerators are a class of
    IPs that feature one or more slave ports to access internal functionality. These
    IPs require the host CPU, or a DMA, to copy the needed computational data from
    the main memory of the platform to their internal data memory, before starting
    the operations. At the end of the computations, an interrupt or status bit could
    be used to synchronize with the host CPU.


    An example is the Keccak accelerator presented in [\[8\]](#page-19-2). This accelerator
    exposes two 32 bit slave ports, one to access the internal register file used
    for control and status operations and one to access the private data memory, which
    stores the processing data.


    Other examples are in-memory or near-memory macros such as the C-SRAM [\[15\]](#page-19-3),
    where the IP is connected via a 32 bit slave port to a host platform that sends
    commands/instructions to the memory through write operations. C-SRAM decodes the
    memory instructions, by concatenating the address and the write data transmitted
    by the CPU, and performs the requested operation.


    <span id="page-4-0"></span>2.1.2 Processors. To improve performance, many accelerators
    feature one or multiple master ports to independently read in parallel the processing
    data and write back the generated results from/to the main memory.


    Some examples are domain-specific accelerators, such as DSP engines [\[16\]](#page-19-4),
    CGRAs [\[9\]](#page-19-0), multi-CPU clusters [\[25\]](#page-20-5), GPUs [\[33\]](#page-20-6),
    etc., and application-specific accelerators for neural networks [\[5\]](#page-19-5),
    FFT [\[32\]](#page-20-7), cryptography [\[8\]](#page-19-2), image processing [\[20\]](#page-20-8),
    etc.


    An example of a domain-specific accelerator is the CGRA presented in [\[9\]](#page-19-0),
    which has two 32 bit slave ports for configuration registers and private instruction
    memory and four 32 bit master ports for reading and writing data from and to the
    main memory, reaching a maximum bandwidth of 128 bit per bus cycle.


    Another example is the PULP cluster [\[25\]](#page-20-5), which features four
    to eight CV32E40P cores [\[12\]](#page-19-6) connected to a shared instruction
    cache and a multi-bank scratchpad memory. The cluster exposes one 32 bit slave
    port for configuration and for pre-loading the memories, and one 64 bit master
    port shared between the cluster DMA, to transfer data in and out of the scratchpad
    memory, and the instruction cache, to fetch program code.


    Examples of application-specific accelerators are Echoes [\[32\]](#page-20-7)
    and Marsellus [\[13\]](#page-19-7). The former is used to speed up FFT execution
    and has eight 32 bit master ports, four allocated for input and four for output,
    reaching a maximum bandwidth of 256 bit per bus cycle. The latter accelerates
    convolution layers and offers nine 32 bit master ports, with a maximum bandwidth
    of 288 bit per bus cycle.


    Co-processors are a sub-category of processors used to implement custom ISA extensions.
    Co-processors are either tightly coupled in the processor pipeline, or integrated
    via a dedicated interface for reusability.


    Their wide application domains include floating-point operations [\[3\]](#page-19-8),
    posit arithmetic [\[18\]](#page-20-9), post-quantum cryptography [\[10\]](#page-19-9),
    integer complex arithmetic [\[34\]](#page-20-10), etc. For example, [\[10\]](#page-19-9)
    proposes a post-quantum cryptography ISA extension and interacts with the coupled
    RISC-V CPU thanks to the CORE-V-XIF [\[6\]](#page-19-10) interface of OpenHW Group.


    All these previous examples illustrate that there is a large choice of possible
    processors and co-processors for edge AI systems today. Therefore, it is required
    to have a fast and scalable exploration and prototyping framework to choose the
    right set of components, co-processors, or domain-specific accelerators that a
    final implementation should have, and then a well-tuned silicon design flow with
    a predefined set of open-source hardware components and peripherals.


    2.1.3 I/O peripherals. These IPs are meant to implement special interfaces to
    communicate with off-chip components or to pre/post-process data during such communications.


    An example can be found in Arnold [\[27\]](#page-20-11), where the embedded FPGA
    (eFPGA) can be used to control an off-chip accelerator, which requires a custom
    interface, as well as to pre-process data coming from peripherals before being
    stored in memory.


    In this case, there is a clear need to ideally have a framework that can target
    both on-chip and off-chip accelerator concepts by enabling a flexible set of interconnect
    standards. This set of standards should be extendable with minimum


    effort from the system designer thanks to an interface that enables a superset
    of interconnection protocols, as we propose in X-HEEP.


    ### 2.2 Host platforms


    In this subsection, we present a comparison of relevant open-source platforms
    that can be used to host edge-computing accelerators, focusing on configurability,
    extendability, and other key features. The limitations of each platform are analyzed
    in detail to motivate the need for a dedicated solution that can fulfill all the
    requirements.


    2.2.1 PULPissimo [\[28\]](#page-20-12). A single-core platform within the PULP
    family, designed to target ultra-low-power edgecomputing applications. Depending
    on performance requirements, designers can configure the platform with the CV32E20
    [\[29\]](#page-20-2) or CV32E40P [\[12\]](#page-19-6) cores. PULPissimo has been
    integrated with various accelerators, including the aforementioned multi-CPU cluster
    [\[25\]](#page-20-5), neural network accelerators [\[13\]](#page-19-7), CGRAs
    [\[9\]](#page-19-0), eFPGAs [\[27\]](#page-20-11), etc. Many silicon prototypes
    have been implemented, which demonstrate best-in-class energy efficiency in a
    wide range of applications.


    However, PULPissimo provides only a generic AXI 32 bit slave and a 64 bit master
    external interfaces that are used to connect the multi-CPU cluster, while the
    other accelerators have been integrated by forking and modifying the original
    RTL code. Such external interfaces may limit accelerators'' bandwidth. Moreover,
    the platform lacks native support for external interrupts/events and power control,
    which is crucial for efficient power management. Lastly, the platform does not
    offer configurability options to select memory size, bus topology, and memory
    addressing mode, or to change the included peripherals, which limit area, bandwidth,
    and power-space exploration.


    2.2.2 Cheshire [\[22\]](#page-20-13). The limitations mentioned above have been
    partially addressed by another PULP-based platform, called Cheshire. Cheshire
    is based on the CVA6 core [\[35\]](#page-20-14) and allows designers to choose
    the number of external slave and master ports to connect their custom accelerators.
    Furthermore, the platform allows for the configuration of the internal last-level
    cache (LLC) size and of the necessary peripherals, providing the flexibility needed
    to target specific application requirements.


    However, Cheshire has been designed for high-performance systems and consumes
    up to 300 mW, making it unsuitable for most ultra-low-power devices, which typically
    operate in the range of tens of mW. Furthermore, Cheshire lacks support for external
    interrupts and power control, which has implications for its overall energy efficiency,
    as the accelerators are usually power-hungry. Lastly, designers do not have the
    option to select the core type, bus topology, and memory addressing mode.


    2.2.3 BlackParrot [\[23\]](#page-20-15). An open-source Linux-capable platform
    designed to accommodate one or multiple customdesigned accelerators. The platform
    showcases a mesh of heterogeneous tiles, offering the flexibility to compose 64
    bit BlackParrot cores, L2 cache slices, I/O, DRAM controllers, and accelerators
    in various configurations.


    However, it does not allow for selecting the core type, bus topology, and memory
    addressing mode. Additionally, the absence of essential peripherals commonly used
    in edge devices, such as I2Cs, GPIOs, timers, DMAs, interrupt controllers, and
    a power manager to implement low-power strategies, restricts the usage of the
    platform for real applications deployed on ultra-low-power edge applications.
    Moreover, the platform''s internal integration of accelerators, as opposed to
    external plug-ins, involves forking and modifying the original RTL code, leading
    to greater effort and higher development costs. Lastly, the 64 bit architecture
    of BlackParrot targets high-performance systems and is unsuitable for ultra-low-power
    edge devices.


    2.2.4 OpenTitan [\[17\]](#page-19-11). OpenTitan is designed for ultra-low-power
    edge-secure applications. It offers a single-core architecture based on the CV32E20
    [\[29\]](#page-20-2) core and an extensive portfolio of peripherals.


    Despite these strengths, OpenTitan does not offer external support for accelerator
    plug-ins, requiring designers to manually modify the RTL code to integrate their
    custom accelerators. Furthermore, the platform lacks configurability for core
    type, bus topology, and memory addressing mode and size. Furthermore, OpenTitan
    does not come equipped with built-in low-power strategies.


    2.2.5 Chipyard [\[1\]](#page-19-12). On the contrary, the Rocket chip generator
    [\[26\]](#page-20-16), which has been subsequently incorporated and expanded into
    the Chipyard platform, offers extensive configuration options. Using the open-source
    Chisel hardware description language, designers can craft their system, providing
    flexibility and customization. The platform offers a wide range of core types,
    including Ariane, CV32E20, Rocket, and BOOM, allowing designers to tailor the
    system''s performance to meet specific application requirements. Additionally,
    the memory size and peripherals can be customized, further enhancing its adaptability.


    However, even though Chipyard enables accelerators to be integrated into the design
    using the Chisel language, the platform does not offer external master and slave
    ports for the connectivity of accelerators. As a result, designers need to invest
    time in becoming familiar with the Chisel language to successfully configure the
    architecture and integrate custom accelerators. Furthermore, Chipyard does not
    provide support for any specific power reduction strategies. Given the critical
    importance of power efficiency in ultra-low-power applications, designers are
    forced to implement power-saving techniques manually to achieve the desired energy
    efficiency level.


    2.2.6 LiteX [\[14\]](#page-19-13) and ESP [\[19\]](#page-20-17). Two other notable
    SoC generators are LiteX and ESP. LiteX serves as a framework thought to explore
    various FPGA-based architectures. On the other hand, ESP is an open-source platform
    designed for heterogeneous SoC design and prototyping on FPGAs. Both platforms
    offer configurable options, allowing designers to customize core type, memory
    size, peripherals, and the number of external master and slave ports, making them
    adaptable to various application requirements.


    However, LiteX and ESP focus on FPGA development only and do not offer support
    for ASIC design flow. Such limitations hinder their applicability in projects
    aimed at silicon implementations and present difficulties in accurately estimating
    the platform energy consumption, crucial when evaluating the impact of integrated
    accelerators. Moreover, they lack built-in support for external interrupts and
    power control, essential for efficient power management.


    2.2.7 X-HEEP. To overcome the limitations mentioned above and cater to the unique
    needs of ultra-low-power edge designers, we present in this paper the X-HEEP platform.
    The proposed platform features a streamlined architecture that operates in conjunction
    with dedicated open-source tools. These tools enable developers to easily customize
    and extend the architecture with their accelerators and interconnection interfaces,
    thus eliminating the need for manual modification of the RTL code. Using X-HEEP,
    designers can achieve the desired level of configurability, extendability, and
    power efficiency, making it an ideal choice for a wide range of ultra-low-power
    edge applications. In addition, it has been developed using SystemVerilog, to
    offer high compatibility with most of the available electronic design automation
    (EDA) tools.


    #### 3 X-HEEP


    In this section, we present a qualitative and quantitative analysis of the key
    features of X-HEEP regarding configurability, extendability, and software support.
    We synthesized X-HEEP with TSMC 65 nm low-power technology and performed our quantitative
    analysis at the nominal voltage, 1.2 V.


    #### 3.1 Architecture


    Figure [1](#page-8-0) shows the X-HEEP architecture and its essential components.
    These include a configurable RISC-V CPU, a configurable bus, a configurable memory,
    two configurable peripheral domains, and a debug unit.


    X-HEEP leverages existing widely adopted open-source IPs to maintain compatibility
    with existing systems and reuse available software routines and hardware extensions.
    Among the wide portfolio of open-source IPs, we selected those that provide permissive
    licenses, to ease the X-HEEP adoption to a wide range of users, and written in
    SystemVerilog, to make the integration in existing systems and EDA tools compatible
    with industrial standards.


    The RISC-V cores have been selected from the OpenHW Group CORE-V family, as extensively
    verified, mature, and implemented in silicon many times; the bus, the memory models,
    the debug unit, and a plethora of IPs from the PULP project, as again adopted
    by several stakeholders and validated in silicon multiple times; and the peripherals
    from the OpenTitan project as documented, verified, and inclusive of hardware-abstraction-layer
    (HAL) functions. Moreover, X-HEEP includes home-made IPs such as a boot ROM, a
    power manager, a fast interrupt controller, and a DMA.


    3.1.1 CPU. The user can choose among the CV32E20, CV32E40X, and CV32E40P as core
    options [\[29\]](#page-20-2), to trade off power and performance. In particular,
    the CV32E20 core is optimized for control-oriented tasks, while the CV32E40P core
    is optimized for processing-oriented tasks. The CV32E40X core offers power consumption
    and performance similar to the CV32E40P core, without featuring the floating-point
    RVF and custom Xpulp ISA extensions. Moreover, it provides an external interface,
    known as CORE-V-XIF [\[6\]](#page-19-10), that allows for the plug-in of custom
    co-processors to extend the RISC-V ISA without the need to modify the RTL code
    of the core.


    3.1.2 Memory. The user can select the memory size and number of memory banks to
    trade off area, power, and storage capacity. Each bank offers a retention state
    aimed at reducing leakage power, of about 42.5 % compared to active leakage, when
    the bank is not accessed for some time but the data needs to be preserved.


    3.1.3 Bus. To maximize compatibility with the other IPs selected from the OpenHW
    Group, PULP platforms, and OpenTitan project, the bus is based on the same open-bus
    interface (OBI) [\[21\]](#page-20-18) protocol.


    The user can choose either a one-at-a-time topology, where only one master at
    a time can access the bus (one decoder), or a fully connected topology (same number
    of decoders as simultaneous masters), where multiple masters can access multiple
    slaves in parallel, to trade off area and bandwidth. When the fully connected
    option is used, the user can further configure the bus to access a variable number
    of banks in a contiguous or interleaved addressing mode. The contiguous mode offers
    limited bandwidth to applications that require multiple masters to access contiguous
    data stored in memory but allows for power-gating or setting in retention mode
    the banks that are not actively used. Vice versa, the interleaved mode offers
    higher bandwidth to applications that access contiguous data in memory, at the
    cost of keeping all the banks active all the time.


    In addition, to connect additional components, the bus also exposes a configurable
    number of slave and master ports to the external XAIF interface to accommodate
    one or multiple accelerators with different bandwidth constraints. Manuscript
    submitted to ACM


    <span id="page-8-0"></span>![](_page_8_Figure_1.jpeg)


    Figure 1. X-HEEP architecture. The various power domains are visually marked using
    different colors. The components in grey are always on. The accelerator and co-processor
    integration are highlighted in red.


    Figures [2](#page-9-0) (a) and (b) show the variation in the area and the bandwidth
    of the X-HEEP bus by adding slave/master ports to the basic bus configuration,
    which connects the CV32E20 core, two memory banks, the debug unit, the two peripheral
    domains, and no external connection. Ports are added in pairs, i.e., for each
    external master port (M), we add an internal slave port (S) for a memory bank
    to avoid limiting bandwidth during memory access.


    Increasing the number of slave/master ports does not lead to any performance improvement
    in the one-at-atime configuration, limited to 32 bit per bus cycle according to
    its architecture. On the contrary, the fully connected configuration maximizes
    bandwidth, which increases linearly with the number of bus ports, at the cost
    of a higher area (and power consumption). The bus in the one-at-a-time configuration
    occupies about 85 % less silicon space compared to the fully connected configuration,
    considering the same number of slave/master ports.


    In overall performance, a 16 × 16 matrix multiplication algorithm on X-HEEP takes
    approximately 34 % fewer clock cycles in the highest performance configuration
    with the CV32E40P core and fully connected bus compared to the lowest power configuration
    with the CV32E40P core and one-at-a-time bus. Furthermore, when using the Xpulp
    extensions and fully connected bus, the CV32E40P can compute matrix multiplication
    algorithms 4 × faster with 32 bit data or up to 16 × faster with 8 bit SIMD extensions
    for the same CPU without extensions, as shown in [\[12\]](#page-19-6).


    3.1.4 Peripheral domain. Figure [2](#page-9-0) (c) shows the area of the IPs located
    in the peripheral domain. This domain includes peripherals that can be removed
    from the design or powered off if not needed to trade off area or power and functionality.
    Manuscript submitted to ACM


    <span id="page-9-0"></span>![](_page_9_Figure_1.jpeg)


    Figure 2. Exploration of different X-HEEP configurations. The used technology
    is TSMC 65 nm low-power CMOS at 1.2 V.


    These include a platform-level interrupt controller (PLIC), a timer, and general-purpose
    I/O peripherals such as a GPIO, I2C, and SPI.


    3.1.5 Always-on peripheral domain. This domain includes IPs that are always powered
    on. To meet our specific needs and requirements, we custom-designed key components
    such as an SoC controller, a boot ROM, a power manager, a fast interrupt controller,
    and a DMA. The domain also includes other peripherals such as a timer, a UART,
    an SPI, and a GPIO.


    The power manager is responsible for implementing low-power strategies, including
    clock-gating, power-gating, and RAM retention. It features a set of configuration
    registers that provide the user with real-time control over the available low-power
    techniques.


    The architecture is divided into several power domains, marked with different
    colors in Figure [1.](#page-8-0) Clock-gating can be applied to the main CPU,
    peripheral domain, and each memory bank, while retention can only be applied to
    memory banks. Additionally, each power domain can be individually power-gated.
    The leakage power consumption of each domain is reported in Figure [2](#page-9-0)
    (d). The system bus, debug unit, and other essential IPs represent about 35 %
    of the Manuscript submitted to ACM


    leakage power of the always-on domain. The remaining 65 % comes from other general-purpose
    peripherals added to enhance versatility, such as a GPIO, SPI, UART, etc.


    The platform can be extended with additional power domains to include user external
    accelerators. This is possible thanks to external power ports, part of the XAIF
    interface, directly connected to the power manager, which can be used to clock-gate,
    power-gate, or set in retention mode external accelerators.


    #### 3.2 Extendible Accelerator InterFace (XAIF)


    The extensive array of domain-specific hardware accelerators encompasses a diverse
    range of requirements, including memory capacity, area, performance, and power
    efficiency. These varied demands are aggregated into the configurable XAIF interface,
    facilitating enhanced connectivity to state-of-the-art accelerators, and agile
    integration into microcontrollers for real-life applications. Such an interface
    gathers all the requirements to extend X-HEEP with domain-specific customizations.
    To the best of the authors'' knowledge, no other open-source platform for edge-computing
    applications exists that provides such a complete extension interface to fulfill
    the requirements of state-of-the-art solutions.


    3.2.1 Memory mapped ports. A configurable number of slave and master ports, utilizing
    the OBI protocol, can be harnessed to connect custom accelerators to the X-HEEP
    bus. Slave ports provide easy access and configuration for memory-like accelerators,
    exemplified in Subsection [2.1.1,](#page-3-0) such as the Keccak [\[8\]](#page-19-2),
    which requires two 32 bit slave ports for control and status operations and data
    memory. In addition, a further peripheral interface connected to the X-HEEP peripheral
    bus is provided for external custom peripherals. This peripheral interface is
    further extended by a FIFO interface to allow easy DMA-peripheral connections.
    This allows the CPU to wait for peripheral transactions to transfer all data to
    the main memory with the support of the system DMA, as implemented in [\[27\]](#page-20-11).
    On the other hand, master ports accommodate the bandwidth requirements of processor-like
    accelerators outlined in Subsection [2.1.2.](#page-4-0) For example, a CGRA [\[9\]](#page-19-0)
    leverages the four 32 bit master ports that are used to independently read and
    write data to and from the main memory.


    3.2.2 Interrupt ports. A configurable number of interrupt lines can be used by
    the custom hardware to rapidly synchronize with the host CPU. Each line is connected
    to the X-HEEP PLIC interrupt controller, which can be controlled via software.
    This functionality allows the host CPU to enter a sleep state during active accelerator
    periods, significantly reducing the overall energy consumption of the running
    application.


    3.2.3 Power control ports. To provide low-power strategy capabilities to custom
    accelerators, a configurable number of power control interfaces is provided. Each
    interface is connected to the X-HEEP power manager to implement different power-saving
    strategies. Each interface includes control signals for power-gating, clock-gating,
    and RAM retention.


    #### 3.3 Tools and software


    We present the tools and software provided by X-HEEP to configure, program, and
    implement user designs.


    3.3.1 Configuration. X-HEEP is configured through SystemVerilog templates, which
    function as a dynamic tool that enables users to automatically customize the RTL
    code of the platform thanks to customizable parameters. This makes the generated
    code readable and easy to maintain and debug.


    3.3.2 Software. X-HEEP includes a HAL to access peripheral functionalities and
    supports FreeRTOS for improved development and efficient resource management.


    3.3.3 Simulation and implementation. X-HEEP offers support for simulation and
    implementation, in FPGA and silicon, based on the FuseSoc build system [\[11\]](#page-19-14).
    FuseSoC supports several EDA tools, such as Verilator, Questasim, Design Compiler,
    Genus, and Vivado, and automatically generates the scripts required to simulate
    or implement user designs. Thanks to it, the user can explore the design both
    at a high level by integrating accelerators described in SystemC, at the RTL level,
    and FPGA, for early prototyping and exploration, as well as in silicon, for final
    validation.


    #### 4 HEEPOCRATES


    In this section, we present an integration example to demonstrate the real-world
    applicability of X-HEEP. This integration effort results in HEEPocrates, a heterogeneous
    architecture designed for ultra-low-power healthcare applications. These applications
    typically involve extended data acquisition periods during which data from external
    biosensors are stored in memory, followed by intensive processing periods to classify
    such data. Therefore, we exploited the XAIF interface to extend X-HEEP with a
    CGRA accelerator [\[9\]](#page-19-0) and an IMC accelerator [\[31\]](#page-20-3),
    both of which have been shown to efficiently reduce the overall energy consumption
    of healthcare applications [\[7,](#page-19-1) [24\]](#page-20-4). Moreover, each
    accelerator is located in a separate power domain that can be individually switched
    on and off for fine-grained power control.


    ### 4.1 Architecture


    Figure [3](#page-12-0) shows the HEEPocrates architecture highlighting how the
    CGRA and IMC accelerators are integrated to minimize power and maximize bandwidth.


    4.1.1 X-HEEP configuration. We configured the X-HEEP host platform with (1) the
    CV32E20 core, which is optimal for running control tasks and offloading performance-intensive
    computations to the external accelerators while preserving low power consumption;
    (2) 8 SRAM banks of 32 KiB in contiguous addressing mode to accommodate variable
    lengths of data acquisitions while power-gating the unused banks on different
    applications; (3) a fully connected bus to provide high-bandwidth capabilities
    to the integrated accelerators; (4) all the available peripherals in place to
    deliver high flexibility; (5) a CGRA and IMC accelerators connected to the external
    XAIF interface.


    4.1.2 CGRA accelerator [\[9\]](#page-19-0). This accelerator offers two slave
    ports, one to access the internal configuration registers, and one for the context
    memory, plus four master ports used to load and store data. The context memory
    stores the kernel''s code executed by the four internal processing elements (PEs).
    Each PE is connected to a dedicated master port to read and write data from/to
    the X-HEEP main memory, independently. This allows a maximum bandwidth of 128
    bit per bus cycle. To synchronize the CGRA and the CPU, the CGRA end-of-computation
    event is connected to the X-HEEP interrupt controller (PLIC) via the XAIF interface.


    The CGRA is divided into two power domains: one for the control logic and the
    datapaths; and one for the context memory. The control logic and datapaths can
    be clock-gated or power-gated, while the context memory can be clock-gated, power-gated,
    or set in retention mode. This dual power domain structure enables clock-gating
    individual domains during short periods of inactivity and power-gating during
    extended non-use periods. Additionally, it offers the flexibility of independently
    setting the context memory in retention mode while clock-gating or power-gating
    the datapaths and control logic to save CGRA configuration time. The XAIF interface
    provides control over the various power modes, enabling the system to dynamically
    adjust its power consumption based on the operational requirements of the CGRA
    accelerator.


    <span id="page-12-0"></span>![](_page_12_Figure_1.jpeg)


    Figure 3. HEEPocrates architecture. Power domains are visually marked using different
    colors. The components highlighted in grey are always on. The accelerator integration
    is highlighted in red.


    4.1.3 IMC accelerator [\[31\]](#page-20-3). This accelerator offers one slave
    port to access its memory array. An internal controller decodes the memory requests
    and facilitates the transition of the accelerator between two modes: memory mode
    and computation mode. In memory mode, the memory space functions as a conventional
    memory bank. In contrast, the computation mode enables the execution of in-memory
    computations, eliminating the need for additional data transfers between the main
    memory and the accelerator.


    As for the CGRA, the IMC accelerator is placed in a separate power domain to save
    power when not used.


    4.1.4 Frequency-locked loop [\[2\]](#page-19-15). We utilized the XAIF interface
    to connect the frequency-locked loop (FLL) responsible for generating the system
    clock from a 32 kHz external source. For real-time configurability, the FLL exposes
    a set of memory-mapped registers that enable the host CPU to adjust the system
    clock frequency during application execution, dynamically. This feature is precious
    during extended data acquisition periods in healthcare applications because it
    allows for reducing the system frequency to the minimum value required for acquiring
    the necessary biosignals, thereby minimizing dynamic power consumption. Lastly,
    the FLL can be also bypassed, allowing the external source to serve as the system
    clock.


    ## 4.2 FPGA implementation


    We implemented HEEPocrates in FPGAs on the Zynq 7020, Zynq UltraScale+, and Artix
    7 chips by Xilinx for early prototyping. This allows for the exploration of different
    X-HEEP configurations and accelerators to optimally tune the architecture for
    the healthcare domain.


    #### 4.3 Silicon implementation


    After FPGA prototyping and exploration, we implemented HEEPocrates in silicon
    with TSMC 65 nm low-power CMOS technology. Figure [4](#page-13-0) shows the 6
    mm<sup>2</sup> layout of HEEPocrates, with the power domains shown in different
    colors.


    For conducting our measurements, we developed a board specifically designed to
    accommodate our chip. HEEPocrates has been tested from 0.8 V to 1.2 V, achieving
    a maximum frequency of 170 MHz and 470 MHz, respectively. Power Manuscript submitted
    to ACM


    ## 14 Machetti et al.


    <span id="page-13-0"></span>![](_page_13_Figure_1.jpeg)


    Figure 4. HEEPocrates layout, silicon photo, and physical chip (on a Swiss 5-cent
    franc coin).


    consumption ranges from 270 µW at 32 kHz and 0.8 V, to 48 mW at 470 MHz and 1.2
    V. Each phase of healthcare applications has been optimized to minimize power
    consumption.


    4.3.1 Acquisition phase. Healthcare applications commonly feature an extended
    acquisition phase due to the lowbandwidth nature of biosignals and the typical
    lengthy data windows. During this phase, samples are gathered from external ADCs
    via SPI, or other I/O peripherals, and stored in memory by the main CPU or the
    DMA. We run this phase at 1 MHz, 0.8 V to minimize power while offering enough
    performance for the acquisition of bio-signals in the order of hundreds of Hertz.
    HEEPocrates consumes 384 µW during acquisition when the complete system is active,
    and the host CPU is clock-gated when not used. However, power can be further optimized
    by switching off the unused memory banks, the peripheral domain, and the external
    accelerators for the entire acquisition period. This enables a reduction in power
    of 19 %, which leads to 310 µW. Furthermore, the CPU can be turned off during
    idle periods, i.e., when not used actively to acquire ADC samples, reaching the
    lowest power level of the system at 1 MHz of 286 µW, with a further reduction
    of 8 %.


    4.3.2 Processing phase. Upon completion of the acquisition phase, we run the processing
    phase at the maximum speed of 170 MHz, 0.8 V to minimize processing time and race
    to sleep. HEEPocrates consumes 8.17 mW during the processing phase when the complete
    system is active and the CPU executes a matrix multiplication. Power can be further
    optimized 6 % by turning off the unused memory banks, the peripheral domain, and
    the external accelerators, with a consumption of about 7.68 mW. During the processing
    phase, the external accelerators can be individually powered on, and computationally
    intensive tasks can be offloaded by the main CPU to reduce the system''s overall
    energy consumption. HEEPocrates consumes 4.01 mW and 1.65 mW when CNN algorithms
    are executed on the CGRA accelerator and IMC accelerator, respectively, at their
    maximum frequency of 60 MHz. The host CPU, the unused memory banks, and the peripheral
    domain are powered off during accelerator activity.


    #### 5 EXPERIMENTAL SETUP


    This section introduces a representative set of different families of microcontrollers
    commonly used in healthcare applications. Subsequently, it describes the biomedical
    applications that are included in our benchmark.


    #### 5.1 Healthcare microcontrollers


    Healthcare applications exhibit significant variability in acquisition and processing
    times, influenced by factors such as the length of sampling windows and the complexity
    of adopted algorithms. To address this variability, a diverse range of microcontrollers
    have been designed, each optimized to minimize power consumption during specific
    phases. The Apollo 3 Blue excels in acquisition phases, prioritizing power efficiency
    through its deep sleep mode, which ensures remarkably low power consumption when
    the system is inactive during idle periods. On the other hand, GAP9 takes the
    lead in processing phases thanks to its higher-performance core, which guarantees
    substantial reductions in processing time. The analysis of these two microcontrollers
    enables covering the entire spectrum of ultra-low-power edge devices, ranging
    from top-tier power efficiency, with Apollo 3 Blue, to top-tier performance, with
    GAP9. Furthermore, the frequent use of both microcontrollers in this domain demonstrates
    their capability to meet the rigorous demands of healthcare applications in terms
    of performance, power, and area. Table [1](#page-15-0) reports the features of
    the selected microcontrollers.


    5.1.1 Apollo 3 Blue. This MCU is part of the Ambiq board and features an ARM Cortex-M4
    core. The code is stored in the on-chip flash memory with zero overhead in instruction
    fetching, while the rest of the data resides either entirely in the SRAM when
    it fits or in both the SRAM and the flash. Unnecessary SRAM banks are turned off
    for the entire duration of the application. Its optimal processing configuration
    is 0.7 V, 48 MHz. However, we exploited the TurboSPOT mode to increase the frequency
    to 96 MHz when required to meet the timing constraints of the benchmark applications.
    Moreover, during idle periods, the system enters its deep sleep mode, consuming
    approximately 6 µA/MHz, where most of the system components are power-gated, with
    only a few power control modules active.


    5.1.2 GAP9. This MCU is part of the GAP9EVK board and features one CV32E40P core,
    known as the fabric controller (FC), and a cluster (CL) with nine CV32E40P cores,
    which can be switched on and off. We execute the benchmark applications exclusively
    on the FC while power-gating the CL and unnecessary SRAM banks for the entire
    duration of the application. The application code and data are stored in the SRAM
    for maximum performance. Its optimal processing configuration is 0.65 V, 240 MHz.
    Furthermore, during idle periods, the system transitions into its sleep mode,
    where the majority of components are power-gated, except for memory banks, which
    enter a retention mode.


    5.1.3 HEEPocrates. The application code and data are completely stored in the
    SRAM, when possible, or in a combination of the SRAM and the off-chip flash, connected
    through the SPI interface. The peripheral domain and the unused memory banks are
    also powered off throughout the entire duration of the application. We execute
    all the benchmark applications on the host CPU while power-gating the external
    accelerators. Moreover, we also accelerate CNN computations on the CGRA and IMC
    accelerators and showcase the energy improvement compared to running on the host
    CPU. We performed each measurement under the optimal operating conditions: 170
    MHz at 0.8 V, for the host CPU; 60 MHz at 0.8 V, for the CGRA and IMC accelerators.
    During idle periods, the host CPU and the external accelerators are power-gated,
    and the system frequency is lowered to 1 MHz to reduce power consumption.


    <span id="page-15-0"></span>


    | MCU           | Board         | Processing element | Voltage | Maximum frequency
    |

    |---------------|---------------|--------------------|---------|-------------------|

    | Apollo 3 Blue | Ambiq         | Cortex-M4          | 0.7 V   | 48 MHz            |

    | GAP9          | Gapuino       | CV32E40P           | 0.65 V  | 240 MHz           |

    | HEEPocrates   | Testing board | CV32E20            | 0.8 V   | 170 MHz           |


    Table 1. Microcontrollers commonly adopted in healthcare applications.


    Table 2. Healthcare applications included in our benchmark.


    <span id="page-15-1"></span>


    | Application           | Acquisition window | Input leads | Sampling rate | Bits
    per sample |

    |-----------------------|--------------------|-------------|---------------|-----------------|

    | Heartbeat classifier  | 15 s               | 3           | 256 Hz        | 16              |

    | Seizure detection CNN | 4 s                | 23          | 256 Hz        | 16              |


    #### 5.2 Healthcare applications


    Table [2](#page-15-1) reports the healthcare applications selected for our benchmark.
    Our selection ensures that we cover the full spectrum of ultra-low-power healthcare
    applications, ranging from acquisition-dominated, with the heartbeat classifier,
    to processing-dominated, with the seizure detection CNN. Moreover, these applications
    showcase computational algorithms of varying complexity, thereby enhancing the
    comprehensiveness of our analysis.


    5.2.1 Heartbeat classifier [\[4\]](#page-19-16). This application is used to detect
    irregular beat patterns for common heart diseases through the analysis of electrocardiogram
    (ECG) signals. The most resource-intensive part of this application lies in the
    initial computation phase, specifically the morphological filtering, which consumes
    over 80 % of the total processing time. Subsequently, the classification stage
    employs random projections. Initially, the algorithm processes a single input
    channel. If an abnormal heartbeat is detected, the analysis extends to the other
    leads for a more precise determination. Our testing scenarios involve input signals
    that all contain abnormal beats to evaluate the complete application pipeline.
    The input signal is derived from three distinct ECG leads, each sampled at 256
    Hz with an accuracy of 16 bit. A 15 s acquisition window produces an input signal
    of 22.5 KiB.


    5.2.2 Seizure detection CNN [\[13\]](#page-19-7). This application is used to
    detect seizures in electroencephalography (EEG) signals. It features a CNN with
    three one-dimensional convolutional layers, each incorporating pooling and ReLU
    layers. 90 % of the processing time is spent in convolutional computations, which
    mainly involve multiply and accumulate (MAC) and shift operations. Following each
    convolution, there is an overflow check and a maximum test for the pooling layer.
    Two fully connected layers end the network. The signal is sampled from 23 leads
    at a rate of 256 Hz with 16 bit accuracy and the acquisition phase lasts 4 s,
    resulting in an input signal size of 46 KiB.


    #### 6 EXPERIMENTAL RESULTS


    In this section, first, we analyze the energy consumption of the proposed host
    platform, HEEPocrates (with the accelerators power-gated), in comparison with
    the selected state-of-the-art microcontrollers that may serve as host platforms.
    Subsequently, we assess the energy efficiency gained from leveraging the HEEPocrates''
    accelerators in comparison to execution on the host CPU.


    <span id="page-16-0"></span>![](_page_16_Figure_1.jpeg)


    Figure 5. Energy consumption of our benchmark running on common healthcare microcontrollers
    and on HEEPocrates at 0.8 V.


    #### 6.1 Host platforms


    Figure [5](#page-16-0) illustrates the measured energy values for each healthcare
    application from our benchmark.


    The heartbeat classifier application exhibits an acquisition-driven nature, characterized
    by extended acquisition windows and a low sampling rate of 256 Hz. This forces
    microcontrollers to spend a significant amount of time in idle states during acquisition.
    In particular, the Apollo 3 Blue stands out for its energy efficiency, attributed
    to its remarkably low sleep mode of only 6 µA/MHz, where most of the system is
    power-gated, with only a few control modules active. On the contrary, GAP9 lacks
    aggressive sleep modes and keeps more modules always on, resulting in considerably
    higher energy consumption. Even during the processing phase, Apollo 3 Blue maintains
    a slight energy advantage over GAP9. This can be attributed to the optimized design
    of its CPU, the ARM Cortex-M4, which is more efficient for the specific operations
    required by this application, including logical and comparison operations, branches,
    as well as load and store instructions [\[4\]](#page-19-16).


    HEEPocrates positions itself in a middle ground during acquisition, offering a
    more robust sleep mode compared to GAP9. However, it does not reach the exceptionally
    low power consumption levels of Apollo 3 Blue due to the absence of aggressive
    sleep strategies for faster wake-up times, which includes in the always-on IPs
    more peripherals as an FLL, a pad controller, bus, a debug unit, and more general-purpose
    peripherals added for enhanced versatility (e.g. SPI, UART, etc.). However, HEEPocrates''
    energy efficiency can be improved by removing the general-purpose peripherals,
    resulting in a 27 % reduction in overall energy consumption. During processing,
    HEEPocrates consumes slightly higher energy compared to the other microcontrollers,
    due to its ultra-low-power CV32E20 core [\[29\]](#page-20-2) that is not optimized
    for performance like GAP9, and due to the higher-power consumption of the active
    part of the chip compared to Apollo-3, sitting HEEPocrates in the middle between
    the two.


    The seizure detection CNN application is processing-dominated due to its computationally
    intense convolutional network, leading microcontrollers to spend the majority
    of their time in the processing phase. GAP9 emerges as the dominant contender
    in this phase, leveraging its high-performance core to achieve reduced processing
    times and efficient transitions to sleep. In contrast, the core of Apollo 3 Blue
    lacks sufficient computational power, resulting in an extended processing phase
    and increased energy consumption. However, during the acquisition phase, Apollo
    3 Blue maintains dominance over GAP9 due to its more efficient sleep mode, resulting
    in lower energy consumption.


    ## <span id="page-17-0"></span>18 Machetti et al.


    ![](_page_17_Figure_1.jpeg)


    Figure 6. Energy consumption of HEEPocrates at 0.8 V running a 16x16 convolution
    (3x3 filter) on the host CPU (at 170 MHz) and the CGRA and IMC accelerators (at
    60 MHz).


    HEEPocrates finds itself positioned between Apollo 3 Blue and GAP9 in both the
    processing and acquisition phases. During acquisition, it offers a more efficient
    low-power mode than GAP9 but does not reach the efficiency levels of Apollo 3
    Blue, for the reasons explained earlier. In the processing phase, the higher performance
    of HEEPocrates allows for faster entry into the sleep state than Apollo 3 Blue
    but lags behind GAP9 due to its higher-frequency core. Notably, similar to the
    previous application, HEEPocrates'' energy efficiency may be enhanced by removing
    general-purpose peripherals from the always-on domain, resulting in an overall
    energy reduction of about 3 %.


    In conclusion, our analysis reveals the energy consumption alignment of HEEPocrates
    with state-of-the-art microcontrollers commonly adopted in healthcare applications.
    The performance and power efficiency of our platform falls between the top-tier
    power efficiency of Apollo 3 Blue and the top-tier performance of GAP9. This underscores
    that HEEPocrates achieves state-of-the-art energy efficiency figures across a
    wide range of real-world application profiles typical of the healthcare domain,
    ranging from acquisition-dominated to processing-dominated scenarios.


    #### 6.2 Accelerators


    In Figure [6,](#page-17-0) we compare the energy consumption of HEEPocrates while
    running a 16×16 convolutional layer with a 3×3 filter on the host CPU, the CGRA
    and IMC accelerators. Our results demonstrate an improvement in energy efficiency
    of approximately 4.9 × and 4.8 × achieved by exploiting the integrated CGRA accelerator
    and the IMC accelerator, respectively, compared to running on the host CPU. This
    improvement is attributed to the higher parallelism of the proposed accelerators,
    which compensates for the increased power consumption resulting from the more
    intense computation.


    #### 7 CONCLUSIONS


    In this paper, we have explored the growth and increasing demand for efficient
    processing solutions in the field of edge computing, particularly in the context
    of new AI/ML applications. Persistent challenges arise from the limitations in
    performance and power consumption of edge devices, which impact overall energy
    efficiency.


    To address these challenges, heterogeneous architectures have emerged, presenting
    a promising solution by combining ultra-low-power host processors with specialized
    accelerators tailored to specific applications or domains.


    However, we have shown the limitations of existing host platforms in exploring
    the design space of acceleratorbased ultra-low power edge AI platforms, as well
    as in providing the configurability and extendability options needed Manuscript
    submitted to ACM


    to integrate the large variety of custom accelerators and interfaces that exist
    nowadays. Consequently, extensive modifications to the RTL code are often required
    to integrate accelerators effectively, leading to high maintenance costs.


    To overcome these limitations, we introduced X-HEEP, an open-source solution designed
    specifically to support the integration and exploration of ultra-low-power edge
    AI/ML accelerators. The platform offers comprehensive customizability and extendability
    options via the proposed XAIF, which gathers all the requirements of state-of-the-art
    domain-specific solutions, as memory-mapped accelerators, including memory, processors,
    and peripherals with DMAsupport, custom ISA co-processor, interrupts, and power
    saving strategies interface, enabling designers to tailor the platform to meet
    the unique requirements of the target applications in performance, power, and
    area.


    X-HEEP provides configuration options to match specific application requirements
    by exploring various core types, bus topologies, and memory addressing modes.
    It also enables a fine-grained configuration of memory banks to match the constraints
    of the integrated accelerators. The platform prioritizes energy efficiency by
    implementing low-power strategies and integrating them with accelerators through
    dedicated power control interfaces. This cohesive integration ensures that all
    system components work together to maximize energy savings.


    To illustrate the practical benefits of X-HEEP, in this work, we presented a real-world
    integration example tailored for healthcare applications, which shows high variability
    among acquisition and processing-dominated application profiles. This example
    featured a CGRA accelerator and an IMC accelerator, both of which have proved
    to effectively reduce the overall energy consumption for this application domain.
    The resulting design, called HEEPocrates, has been implemented both in FPGAs on
    the Zynq 7020, Zynq UltraScale+, and Artix 7 chips by Xilinx, for early prototyping
    and exploration, and in silicon with TSMC 65 nm low-power CMOS technology, for
    silicon validation. The fabricated chip can operate from 0.8 V to 1.2 V, achieving
    a maximum frequency of 170 MHz and 470 MHz, respectively. Its power consumption
    ranges from 270 µW at 32 kHz and 0.8 V, to 48 mW at 470 MHz and 1.2 V.


    To measure the performance and versatility of the proposed design, we analyze
    the execution of an illustrative real-life set of edge AI/ML benchmarks that combines
    ultra-low power healthcare applications from the latest advances in the field,
    showing high variability in the execution profile. Through the execution of our
    benchmark and the measurement of the energy consumption of the chip, we demonstrated
    HEEPocrates'' alignment with other stateof-the-art microcontrollers that are frequently
    employed in healthcare applications. This is achieved thanks to a balanced trade-off
    between fine-grain power domains, to reduce power consumption during acquisition
    phases, and on-demand accelerator capabilities, to speed up the execution of processing
    phases, resulting in a good trade-off between acquisition-dominated and processing-dominated
    applications. These results also showcase the representativeness of the experiments
    that other researchers could perform after integrating their accelerators with
    X-HEEP. Lastly, we proved the energy benefit of 4.9 × and 4.8 × gained by exploiting
    the integrated CGRA accelerator and IMC accelerator, respectively, compared to
    running on the host CPU.


    In conclusion, the introduction of the X-HEEP platform leads to a significant
    step forward in overcoming the challenges faced in the field of edge computing.
    By providing extensive options for customizability and extendability, prioritizing
    energy efficiency, and presenting a practical real-world integration example,
    X-HEEP presents itself as an innovative platform, empowering designers and researchers
    to create efficient heterogeneous edge AI/ML computing systems.


    #### 8 ACKNOWLEDGEMENTS


    We would like to thank the entire X-HEEP team for their great contribution to
    the platform.


    #### REFERENCES


    - <span id="page-19-12"></span>[1] Alon Amid et al. "Chipyard: Integrated Design,
    Simulation, and Implementation Framework for Custom SoCs". In: IEEE Micro 40.4
    (2020), pp. 10–21. doi: [10.1109/MM.2020.2996616.](https://doi.org/10.1109/MM.2020.2996616)

    - <span id="page-19-15"></span>[2] David E Bellasi and Luca Benini. "Smart energy-efficient
    clock synthesizer for duty-cycled sensor socs in 65 nm/28nm cmos". In: IEEE Transactions
    on Circuits and Systems I: Regular Papers 64.9 (2017), pp. 2322–2333.

    - <span id="page-19-8"></span>[3] Andrea Bocco, Yves Durand, and Florent De Dinechin.
    "SMURF: Scalar Multiple-Precision Unum Risc-V Floating-Point Accelerator for Scientific
    Computing". In: Proc. of the ACM Conference for Next Generation Arithmetic. CoNGA''19.
    2019. isbn: 9781450371391. doi: [10.1145/3316279.3316280.](https://doi.org/10.1145/3316279.3316280)

    - <span id="page-19-16"></span>[4] Rubén Braojos, Giovanni Ansaloni, and David
    Atienza. "A Methodology for Embedded Classification of Heartbeats Using Random
    Projections". In: DATE. IEEE, May 2013, pp. 899–904. isbn: 9781467350716. doi:
    [10.7873/DATE.](https://doi.org/10.7873/DATE.2013.189) [2013.189.](https://doi.org/10.7873/DATE.2013.189)

    - <span id="page-19-5"></span>[5] Francesco Conti et al. "A 12.4TOPS/W @ 136GOPS
    AI-IoT System-on-Chip with 16 RISC-V, 2-to-8b Precision-Scalable DNN Acceleration
    and 30%-Boost Adaptive Body Biasing". In: IEEE ISSCC. 2023, pp. 21–23. doi: [10.1109/](https://doi.org/10.1109/ISSCC42615.2023.10067643)
    [ISSCC42615.2023.10067643.](https://doi.org/10.1109/ISSCC42615.2023.10067643)

    - <span id="page-19-10"></span>[6] CORE-V X-Interface. url: [https://github.com/openhwgroup/core-v-xif.](https://github.com/openhwgroup/core-v-xif)

    - <span id="page-19-1"></span>[7] Elisabetta De Giovanni et al. "Modular Design
    and Optimization of Biomedical Applications for Ultralow Power Heterogeneous Platforms".
    In: IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems
    39.11 (2020), pp. 3821–3832. doi: [10.1109/TCAD.2020.3012652.](https://doi.org/10.1109/TCAD.2020.3012652)

    - <span id="page-19-2"></span>[8] Alessandra Dolmeta et al. "Implementation and
    Integration of Keccak Accelerator on RISC-V for CRYSTALS-Kyber". In: Proc. of
    the 20th ACM Int. Conf. on Computing Frontiers. CF ''23. Bologna, Italy, 2023,
    pp. 381–382. doi: [10.1145/3587135.3591432.](https://doi.org/10.1145/3587135.3591432)

    - <span id="page-19-0"></span>[9] Loris Duch et al. "A multi-core reconfigurable
    architecture for ultra-low power bio-signal analysis". In: IEEE BioCAS. 2016,
    pp. 416–419. doi: [10.1109/BioCAS.2016.7833820.](https://doi.org/10.1109/BioCAS.2016.7833820)

    - <span id="page-19-9"></span>[10] Tim Fritzmann, Georg Sigl, and Johanna Sepúlveda.
    "RISQ-V: Tightly Coupled RISC-V Accelerators for Post-Quantum Cryptography". In:
    IACR Transactions on Cryptographic Hardware and Embedded Systems 2020.4 (Aug.
    2020), pp. 239–280. doi: [10.13154/tches.v2020.i4.239-280.](https://doi.org/10.13154/tches.v2020.i4.239-280)

    - <span id="page-19-14"></span>[11] FuseSoC. url: [https://github.com/olofk/fusesoc.](https://github.com/olofk/fusesoc)

    - <span id="page-19-6"></span>[12] Michael Gautschi et al. "Near-threshold RISC-V
    core with DSP extensions for scalable IoT endpoint devices". In: IEEE Transactions
    on Very Large Scale Integration (VLSI) Systems 25.10 (2017), pp. 2700–2713.

    - <span id="page-19-7"></span>[13] Catalina Gómez et al. "Automatic seizure detection
    based on imaged-EEG signals through fully convolutional networks". In: Scientific
    reports 10.1 (2020), pp. 1–13.

    - <span id="page-19-13"></span>[14] Florent Kermarrec et al. LiteX: an open-source
    SoC builder and library based on Migen Python DSL. 2020. arXiv: 2005.02506 [\[cs.AR\]](https://arxiv.org/abs/2005.02506).

    - <span id="page-19-3"></span>[15] Maha Kooli et al. "Towards a Truly Integrated
    Vector Processing Unit for Memory-Bound Applications Based on a Cost-Competitive
    Computational SRAM Design Solution". In: J. Emerg. Technol. Comput. Syst. 18.2
    (2022). issn: 1550-4832. doi: [10.1145/3485823.](https://doi.org/10.1145/3485823)

    - <span id="page-19-4"></span>[16] Kai Li, Wei Yin, and Qiang Liu. "A Portable
    DSP Coprocessor Design Using RISC-V Packed-SIMD Instructions". In: IEEE ISCAS.
    2023, pp. 1–5. doi: [10.1109/ISCAS46773.2023.10181681.](https://doi.org/10.1109/ISCAS46773.2023.10181681)

    - <span id="page-19-11"></span>[17] LowRISC. OpenTitan. url: [https://github.com/lowRISC/opentitan.](https://github.com/lowRISC/opentitan)


    - <span id="page-20-9"></span><span id="page-20-0"></span>[18] David Mallasén,
    Alberto A. del Barrio, and Manuel Prieto-Matias. Big-PERCIVAL: Exploring the Native
    Use of 64-Bit Posit Arithmetic in Scientific Computing. 2023. arXiv: [2305.06946.](https://arxiv.org/abs/2305.06946)

    - <span id="page-20-17"></span>[19] Paolo Mantovani et al. "Agile SoC Development
    with Open ESP : Invited Paper". In: 2020 IEEE/ACM International Conference On
    Computer Aided Design (ICCAD). 2020, pp. 1–9.

    - <span id="page-20-8"></span>[20] Katayoun Neshatpour et al. "Big biomedical
    image processing hardware acceleration: A case study for K-means and image filtering".
    In: IEEE ISCAS. 2016, pp. 1134–1137. doi: [10.1109/ISCAS.2016.7527445.](https://doi.org/10.1109/ISCAS.2016.7527445)

    - <span id="page-20-18"></span>[21] Open Bus Interface Protocol. url: [https://github.com/openhwgroup/obi.](https://github.com/openhwgroup/obi)

    - <span id="page-20-13"></span>[22] Alessandro Ottaviano et al. "Cheshire: A Lightweight,
    Linux-Capable RISC-V Host Platform for Domain-Specific Accelerator Plug-In". In:
    IEEE Transactions on Circuits and Systems II: Express Briefs (2023), pp. 1–1.
    doi: [10.1109/](https://doi.org/10.1109/TCSII.2023.3289186) [TCSII.2023.3289186.](https://doi.org/10.1109/TCSII.2023.3289186)

    - <span id="page-20-15"></span>[23] Daniel Petrisko et al. "BlackParrot: An Agile
    Open-Source RISC-V Multicore for Accelerator SoCs". In: IEEE Micro 40.4 (2020),
    pp. 93–102. doi: [10.1109/MM.2020.2996145.](https://doi.org/10.1109/MM.2020.2996145)

    - <span id="page-20-4"></span>[24] Flavio Ponzina et al. "A Hardware/Software
    Co-Design Vision for Deep Learning at the Edge". In: IEEE Micro 42.6 (July 2022),
    pp. 48–54. doi: [10.1109/MM.2022.3195617.](https://doi.org/10.1109/MM.2022.3195617)

    - <span id="page-20-5"></span>[25] Antonio Pullini et al. "Mr. Wolf: An energy-precision
    scalable parallel ultra low power SoC for IoT edge processing". In: IEEE Journal
    of Solid-State Circuits 54.7 (2019), pp. 1970–1981.

    - <span id="page-20-16"></span>[26] Rocket. url: [https://github.com/chipsalliance/rocket-chip.](https://github.com/chipsalliance/rocket-chip)

    - <span id="page-20-11"></span>[27] Pasquale Davide Schiavone et al. "Arnold:
    An eFPGA-augmented RISC-V SoC for flexible and low-power IoT end nodes". In: IEEE
    Transactions on Very Large Scale Integration (VLSI) Systems 29.4 (2021), pp. 677–690.

    - <span id="page-20-12"></span>[28] Pasquale Davide Schiavone et al. "Quentin:
    an Ultra-Low-Power PULPissimo SoC in 22nm FDX". In: (2018), pp. 1–3. doi: [10.1109/S3S.2018.8640145.](https://doi.org/10.1109/S3S.2018.8640145)

    - <span id="page-20-2"></span>[29] Pasquale Davide Schiavone et al. "Slow and
    steady wins the race? A comparison of ultra-low-power RISC-V cores for Internet-of-Things
    applications". In: Int. Symp. on Power and Timing Modeling, Optimization and Simulation
    (PATMOS). IEEE. 2017, pp. 1–8.

    - <span id="page-20-1"></span>[30] Pasquale Davide Schiavone et al. "X-HEEP: An
    Open-Source, Configurable and Extendible RISC-V Microcontroller". In: Proc. of
    Int. Conf. on Computing Frontiers. CF ''23. New York, NY, USA: ACM, 2023, pp.
    379–380. isbn: 9798400701405. doi: [10.1145/3587135.3591431.](https://doi.org/10.1145/3587135.3591431)

    - <span id="page-20-3"></span>[31] William Andrew Simon et al. "BLADE: An in-cache
    computing architecture for edge devices". In: IEEE Transactions on Computers 69.9
    (2020), pp. 1349–1363.

    - <span id="page-20-7"></span>[32] Mattia Sinigaglia et al. Echoes: a 200 GOPS/W
    Frequency Domain SoC with FFT Processor and I2S DSP for Flexible Data Acquisition
    from Microphone Arrays. 2023. arXiv: [2305.07325.](https://arxiv.org/abs/2305.07325)

    - <span id="page-20-6"></span>[33] Blaise Tine et al. "Vortex: Extending the RISC-V
    ISA for GPGPU and 3D-Graphics". In: IEEE/ACM Int. Symp. on Microarchitecture (MICRO).
    2021, pp. 754–766. isbn: 9781450385572. doi: [10.1145/3466752.3480128.](https://doi.org/10.1145/3466752.3480128)

    - <span id="page-20-10"></span>[34] Y. Varma and M.P. Tull. "Architectural design
    of a complex arithmetic signal processor (CASP)". In: Region 5 Conference: Annual
    Technical and Leadership Workshop. 2004, pp. 69–76. doi: [10.1109/REG5.2004.1300163.](https://doi.org/10.1109/REG5.2004.1300163)

    - <span id="page-20-14"></span>[35] Florian Zaruba and Luca Benini. "The cost
    of application-class processing: Energy and performance analysis of a Linux-ready
    1.7-GHz 64-bit RISC-V core in 22-nm FDSOI technology". In: IEEE Transactions on
    Very Large Scale Integration (VLSI) Systems 27.11 (2019), pp. 2629–2640.'
  decisions:
    evaluation_prompt: 'Qualified. Reason: The paper provides evidence of evaluation
      through multiple sections discussing experimental results, performance measurements,
      and comparisons with state-of-the-art microcontrollers. It includes phrases
      like "We run a set of healthcare applications and measure their energy consumption,"
      and presents performance tables and figures, such as Figure 5 and Figure 6,
      which show energy consumption comparisons.'
    related_work_prompt: 'Qualified. Reason: The paper meaningfully engages with prior
      research throughout the text. It includes an extensive "State-of-the-Art" section
      that discusses existing accelerators and host platforms, comparing their strengths
      and limitations. The paper also references numerous prior works in the introduction
      and other sections, providing context and comparison for the proposed X-HEEP
      platform.'
    novelty_prompt: 'Qualified. Reason: The paper introduces X-HEEP, a novel open-source,
      configurable, and extendible RISC-V microcontroller platform designed for the
      exploration of ultra-low-power edge accelerators. It claims novelty by proposing
      a new platform (X-HEEP) and a new interface (XAIF) for integrating accelerators,
      which are tailored to meet specific application requirements. The paper also
      presents a real-world integration example, HEEPocrates, demonstrating the platform''s
      applicability in healthcare applications. These contributions indicate clear
      novelty in terms of new methods, architectures, and applications.'
    review_only_prompt: '- Qualified. Reason: The paper introduces a new platform,
      X-HEEP, and provides detailed information about its architecture, features,
      and real-world applications, demonstrating new contributions rather than merely
      summarizing existing work.'
  topics:
    main_topic: Embedded & Real-time Systems
    main_topic_reasoning: The paper focuses on the development of X-HEEP, an open-source
      platform designed for ultra-low-power edge computing, which includes microcontroller
      implementations and custom accelerators specifically tailored for embedded systems
      in real-time applications such as healthcare.
    secondary_topic: Computer Architecture
    secondary_topic_reasoning: The paper discusses the architectural design of the
      platform and the integration of heterogeneous components and accelerators, making
      it relevant to topics related to computer architecture.
    main_topic_sub: Other
    secondary_topic_sub: IoT, mobile, and embedded architecture
- title: "Energy-adaptive Buffering for Efficient, Responsive, and Persistent\n  Batteryless\
    \ Systems"
  abstract: "Batteryless energy harvesting systems enable a wide array of new sensing,\n\
    computation, and communication platforms untethered by power delivery or\nbattery\
    \ maintenance demands. Energy harvesters charge a buffer capacitor from\nan unreliable\
    \ environmental source until enough energy is stored to guarantee a\nburst of\
    \ operation despite changes in power input. Current platforms use a\nfixed-size\
    \ buffer chosen at design time to meet constraints on charge time or\napplication\
    \ longevity, but static energy buffers are a poor fit for the highly\nvolatile\
    \ power sources found in real-world deployments: fixed buffers waste\nenergy both\
    \ as heat when they reach capacity during a power surplus and as\nleakage when\
    \ they fail to charge the system during a power deficit.\n  To maximize batteryless\
    \ system performance in the face of highly dynamic\ninput power, we propose REACT:\
    \ a responsive buffering circuit which varies\ntotal capacitance according to\
    \ net input power. REACT uses a variable capacitor\nbank to expand capacitance\
    \ to capture incoming energy during a power surplus\nand reconfigures internal\
    \ capacitors to reclaim additional energy from each\ncapacitor as power input\
    \ falls. Compared to fixed-capacity systems, REACT\ncaptures more energy, maximizes\
    \ usable energy, and efficiently decouples system\nvoltage from stored charge\
    \ -- enabling low-power and high-performance designs\npreviously limited by ambient\
    \ power. Our evaluation on real-world platforms\nshows that REACT eliminates the\
    \ tradeoff between responsiveness, efficiency,\nand longevity, increasing the\
    \ energy available for useful work by an average\n25.6% over static buffers optimized\
    \ for reactivity and capacity, improving\nevent responsiveness by an average 7.7x\
    \ without sacrificing capacity, and\nenabling programmer directed longevity guarantees."
  url: http://arxiv.org/abs/2401.08806v1
  keywords: ''
  document: '# Energy-adaptive Buffering for Efficient, Responsive, and Persistent
    Batteryless Systems


    Harrison Williams Virginia Tech hrwill@vt.edu


    Matthew Hicks Virginia Tech mdhicks2@vt.edu


    ## Abstract


    Batteryless energy harvesting systems enable a wide array of new sensing, computation,
    and communication platforms untethered by power delivery or battery maintenance
    demands. Energy harvesters charge a buffer capacitor from an unreliable environmental
    source until enough energy is stored to guarantee a burst of operation despite
    changes in power input. Current platforms use a fixed-size buffer chosen at design
    time to meet constraints on charge time or application longevity, but static energy
    buffers are a poor fit for the highly volatile power sources found in real-world
    deployments: fixed buffers waste energy both as heat when they reach capacity
    during a power surplus and as leakage when they fail to charge the system during
    a power deficit.


    To maximize batteryless system performance in the face of highly dynamic input
    power, we propose REACT: a responsive buffering circuit which varies total capacitance
    according to net input power. REACT uses a variable capacitor bank to expand capacitance
    to capture incoming energy during a power surplus and reconfigures internal capacitors
    to reclaim additional energy from each capacitor as power input falls. Compared
    to fixed-capacity systems, REACT captures more energy, maximizes usable energy,
    and efficiently decouples system voltage from stored charge—enabling low-power
    and high-performance designs previously limited by ambient power. Our evaluation
    on real-world platforms shows that REACT eliminates the tradeoff between responsiveness,
    efficiency, and longevity, increasing the energy available for useful work by
    an average 25.6% over static buffers optimized for reactivity and capacity, improving
    event responsiveness by an average 7.7 without sacrificing capacity, and enabling
    programmer directed longevity guarantees.


    ## <span id="page-0-0"></span>1 Introduction


    Ever-shrinking computing and sensing hardware has pushed mobile Internet-of-Things
    (IoT) type devices beyond the limits of the batteries powering them. A typical
    low cost/power microcontroller [\[19\]](#page-11-0) drains a 1 <sup>3</sup> battery
    nearly 14x its size in just over 8 weeks of active operation [\[36\]](#page-12-0),
    rendering the system useless without a potentially costly replacement effort.
    Cost, maintenance, and safety concerns make batteries further incompatible with
    massive-scale (one million devices per square kilometer [\[5\]](#page-11-1)) and
    deeply-deployed (infrastructure [\[2\]](#page-11-2), healthcare [\[34\]](#page-12-1))
    applications. IoT engineers are turning to batteryless energy harvesting platforms


    to power low-cost, perpetual systems capable of driving a ubiquitous computing
    revolution. Increasingly efficient energy harvesting circuits enable batteryless
    systems across a range of IoT use cases including feature-rich batteryless temperature
    sensors 500x smaller than a grain of rice [\[47\]](#page-12-2) and batteryless
    flow-meters [\[13\]](#page-11-3) supporting deep-sea drilling or geothermal plants
    for decades without maintenance.


    The energy harvesting design model both enables new deployments previously limited
    by batteries and places new demands on system developers. Harvested energy is
    highly unreliable: sensitive environmental factors such as shadows over a photovoltaic
    cell or shifts in the orientation of a rectenna produce rapid, outsized changes
    in the energy scavenged by the harvester. Energy harvesters mitigate this unreliability
    by charging an energy buffer to a given enable voltage, which the system periodically
    discharges to supply a useful quantum of work despite potential power loss.


    Buffer capacity is a key design element of any batteryless system. Past work [\[7\]](#page-11-4)
    explores the tradeoff between buffer sizes: small buffers are highly reactive—charging
    rapidly and quickly enabling the system to address time-sensitive events—but sacrifice
    longevity because they rapidly discharge during operation, guaranteeing only a
    short burst of uninterrupted operation. Large buffers store more energy at a given
    voltage, improving longevity by supporting a longer or more energy-intensive burst
    of operation at the cost of reactivity because they require more energy to enable
    the system at all. Matching buffer size to projected energy demand is critical
    to ensuring the system is both reactive enough to address incoming events/deadlines
    (e.g., periodic sensor readings) and long-lived enough to support uninterruptible
    operations (e.g., radio transmissions). Designers choose the minimum size necessary
    to power all atomic operations on the device, maximizing reactivity given a required
    level of longevity.


    In this work, we explore static energy buffer efficiency as a third metric for
    buffer performance and find that it varies dramatically with net energy input
    rather than simple energy demand. Small buffers reach capacity quickly if power
    input exceeds instantaneous demand—burning off hard-won energy as heat to prevent
    overvoltage. Large buffers capture all incoming power, but enable slowly and lose
    more harvested energy to leakage below the minimum system voltage. The volatile
    nature of harvested power means that fixed-size buffers experience both problems
    over the course of their


    deployment, discharging energy during a power surplus and losing large portions
    of energy to leakage during a deficit.


    To make the most of incoming energy in all circumstances, we propose REACT[1](#page-1-0)
    : a dynamic energy buffering system that varies its capacitance following changes
    in net power. REACT maximizes system responsiveness and efficiency using a small
    static buffer capacitor, quickly enabling the system to monitor events or do other
    low-power work under even low input power. If input power rises beyond the current
    system demand and the static buffer approaches capacity, REACT connects additional
    capacitor banks to absorb the surplus, yielding the capacity benefits of large
    buffers without the responsiveness penalty. When net power is negative, these
    capacitors hold the system voltage up and extend operation beyond what is possible
    using the small static capacitor.


    While expanding buffer size to follow net power input ensures the system can capture
    all incoming energy, increasing capacitance also increases the amount of unusable
    charge stored on the capacitor banks—charge which could power useful work if it
    were on a smaller capacitor and therefore available at a higher voltage. As supply
    voltage falls and approaches a minimum threshold, REACT reclaims this otherwise-unavailable
    energy by reconfiguring capacitor banks into series, shifting the same amount
    of charge onto a smaller equivalent capacitance in order to boost the voltage
    at the buffer output and ensure the system continues operating for as long as
    possible. REACT effectively eliminates the design tradeoff between reactivity
    and capacity by tuning buffer size within an arbitrarily large capacitance range,
    only adding capacity when the buffer is already near full. REACT''s charge reclamation
    techniques maximize efficiency by moving charge out of large capacitor banks onto
    smaller ones when net input power is negative, ensuring all energy is available
    for useful work.


    We integrate a hardware prototype of REACT into a full energy harvesting platform
    to evaluate it against previous work, operating under different input power conditions
    and with different power consumption profiles. Our results indicate that REACT
    provides the "best of both worlds" of both smalland large-buffer systems, rapidly
    reaching the operational voltage under any power conditions while also expanding
    as necessary to capture all available energy and provide software longevity guarantees
    as needed. Maximizing buffer capacity and reclaiming charge using REACT''s reconfigurable
    capacitor banks eliminates the efficiency penalties associated with both small
    and large static capacitor buffers, increasing the portion of harvested energy
    used for application code by an average 39% over an equally-reactive static buffer
    and 19% over an equal-capacity one. Compared to prior work exploring dynamic capacitance
    for batteryless systems [\[49\]](#page-12-3), REACT improves performance by an
    average 26% owing to


    its efficient charge management structure. This paper makes the following technical
    contributions:


    - We evaluate the power dynamics of common batteryless systems in real deployments
    and explore how common-case volatility introduces significant energy waste in
    static or demand-driven buffers (§ [2\)](#page-1-1).

    - We design REACT, a dynamic buffer system which varies its capacitance according
    to system needs driven by net input power (§ [3\)](#page-3-0). REACT''s configurable
    arrays combine the responsiveness of small buffers with the longevity and capacity
    of large ones, enables energy reclamation to make the most of harvested power,
    and avoids the pitfalls of energy waste inherent in other dynamic capacitance
    designs (§ [3.3\)](#page-4-0).

    - We integrate REACT into a batteryless system and evaluate its effect on reactivity,
    longevity, and efficiency under a variety of power conditions and workloads (§
    [5\)](#page-8-0). Our evaluation indicates that REACT eliminates the responsiveness-longevity
    tradeoff inherent in static buffer design while increasing overall system efficiency
    compared to any static system.


    ## <span id="page-1-1"></span>2 Background and Related Work


    Scaling sensing, computation, and communication down to smart dust [\[23\]](#page-11-5)
    dimensions requires harvesting power ondemand rather than packaging energy with
    each device using a battery. Many batteryless systems use photovoltaic [\[9,](#page-11-6)
    [47\]](#page-12-2) or RF power [\[40\]](#page-12-4), while other use-cases are
    better suited for sources such as vibration [\[41\]](#page-12-5), fluid flow [\[13\]](#page-11-3),
    or heat gradients [\[27\]](#page-11-7). Commonalities between ambient power sources
    have inspired researchers to develop general-purpose batteryless systems, regardless
    of the actual power source: ambient power is unpredictable, dynamic, and often
    scarce relative to the active power consumption of the system.


    Batteryless systems isolate sensing and actuation components from volatile power
    using a buffer capacitor. The harvester charges the capacitor to a pre-defined
    enable voltage, after which the system turns on and begins consuming power. Because
    many environmental sources cannot consistently power continuous execution, systems
    operate intermittently—draining the capacitor in short bursts of operation punctuated
    by long recharge periods. This generalpurpose intermittent operation model has
    enabled researchers to abstract away the behavior of incoming power and focus
    on developing correct and efficient techniques for working under intermittent
    power [\[17,](#page-11-8) [26,](#page-11-9) [28,](#page-11-10) [29,](#page-11-11)
    [44,](#page-12-6) [46\]](#page-12-7).


    ## 2.1 Choosing Buffer Capacity


    Buffer size determines system behavior in several important ways. Supercapacitors
    provide inexpensive and small-formfactor bulk capacitance [\[24\]](#page-11-12),
    enabling designers to choose a capacitor according to performance rather than
    cost or size concerns. Two metrics motivate past work: reactivity refers


    <span id="page-1-0"></span><sup>1</sup>Reconfigurable, Energy-Adaptive CapaciTors


    <span id="page-2-0"></span>![](_page_2_Figure_0.jpeg)


    Figure 1. Static buffer operation on a simulated solar harvester. Highlighted
    blocks indicate the system is running.


    to the system''s ability to rapidly charge to its enable voltage and begin operation.
    High reactivity ensures a system is online to execute periodic tasks or address
    unpredictable input events. Longevity refers to the energy available for an uninterrupted
    period of work with no additional power input; long-lived systems support high-power
    and long-running uninterruptible operations and reduce the overhead incurred with
    state recovery after a power loss.


    2.1.1 Reactivity and Longevity: A batteryless system''s reactivity and longevity
    depend primarily on the charge and discharge rate of the buffer capacitor. We
    illustrate the tradeoff using a simulated solar harvester with a 22% efficient,
    5 cm<sup>2</sup> panel, based on a pedestrian trace from the EnHANTs solar dataset
    [\[12\]](#page-11-13). The system runs from 3.6V down to 1.8V and draws 1.5 mA
    in active mode, representative of a typical deployment [\[25\]](#page-11-14).
    Figure [1](#page-2-0) illustrates the reactivity-longevity tradeoff inherent in
    static buffer systems at two design extremes, using a 1 mF and 300 mF capacitor.
    The 1 mF system charges rapidly and is therefore highly reactive, reaching the
    enable voltage over 8 sooner than the 300 mF version. However, the smaller capacitor
    also discharges quickly—the mean length of an uninterrupted power cycle using
    the 1 mF capacitor is 10 seconds versus 880 seconds for the 300 mF capacitor,
    indicating the 300 mF system is far longer-lived once charged. The relative importance
    of reactivity and longevity depends on the use case, but often changes over time
    for a complex system—complicating design further.


    2.1.2 Power Volatility and Energy Efficiency: Buffer capacity is also a major
    driver of end-to-end energy efficiency: using the 300 mF capacitor our system
    is operational for 49% of the overall power trace, compared to only 27% for the
    1 mF platform. This stems from the high volatility of incoming power—82% of the
    total energy input is collected during


    short-duration power spikes when harvested power rises above 10 mW, despite the
    system spending 77% of its time at input powers below 3 mW. A large buffer captures
    this excess energy to use later while the smaller buffer quickly reaches capacity
    and discharges energy as heat to avoid overvoltage.


    Large buffers, however, are not always more efficient: the energy used to charge
    the capacitor to the operational voltage cannot power useful work, and is eventually
    lost to leakage while the system is off. When power input is low, this "cold-start"
    energy represents a significant portion of total harvested energy. For the system
    described above powered by a solar panel at night [\[12\]](#page-11-13), the 1
    mF buffer enables a duty cycle of 5.7% versus only 3.3% using a 10 mF buffer.
    This low power environment highlights another risk of oversized buffers: the system
    using the 300 mF capacitor never reaches the enable voltage and so never begins
    operation.


    Improvements in harvester efficiency and low-power chip design are closing the
    gap between harvester output and chip power consumption. Power is increasingly
    limited by volatile environmental factors rather than scarcity induced by low
    efficiency; the result is that energy harvesters experience periods of both energy
    scarcity and surplus. Rapidly changing power conditions place opposing demands
    on batteryless systems, which must remain responsive with low input power, provide
    longevity for long-running operations, and maximize efficiency by avoiding energy
    waste.


    ## 2.2 Power-Responsive Performance Scaling


    One solution to volatile energy input is modulating throughput according to incoming
    power, increasing execution rate when power is plentiful and decreasing it to
    maintain availability when power is scarce. Limiting net input power to the buffer
    by matching power consumption with input enables systems to use small buffer capacitors
    without reaching the buffer capacity, ensuring no power is wasted with an over-full
    buffer. Past work realizes power-responsive scaling using heterogeneous architectures
    [\[10\]](#page-11-15) or by adapting the rate and accuracy of software execution
    [\[1,](#page-11-16) [4,](#page-11-17) [30,](#page-12-8) [48\]](#page-12-9).


    Unfortunately, we find the assumptions underlying powerperformance scaling often
    do not apply to batteryless systems. Increasing energy consumption by accelerating
    execution only improves systems which have useful work to do exactly when input
    power is high, but many batteryless systems focus on periodic sensing and actuation
    deadlines which do not correlate with ambient power supply. Further, resource-constrained
    platforms may have few on-chip operations which can be delayed until power is
    plentiful; when these operations do exist, they are often not amenable to scaling
    (e.g., transmitting data to a base station may be delayed but always requires
    a fixed-cost radio operation). Flexible batteryless systems must capture energy
    and use it on demand rather than fit operation to unreliable power input.


    ## 2.3 Multiplexed Energy Storage


    Rather than match power consumption to incoming supply, systems may charge multiple
    static buffers according to projected demand. Capybara [\[7\]](#page-11-4) switches
    capacitance using an array of heterogeneous buffers: programmers set capacitance
    modes throughout the program, using a smaller capacitor to maximize responsiveness
    for low-power or interruptible tasks and switching to a larger capacitor for high-power
    atomic operations. UFoP and Flicker [\[15,](#page-11-18) [16\]](#page-11-19) assign
    each peripheral on the system a separate buffer and charging priority, enabling
    responsive low-power operation while waiting to collect sufficient energy for
    high-power tasks. These systems increase overall energy capacity by directing
    excess power to capacitors not currently in use.


    Static arrays increase capacity without reducing responsiveness, but waste energy
    when charge is stored on unused buffers. Reserving energy in secondary capacitors
    1) requires error-prone [\[30\]](#page-12-8) speculation about future energy supply
    and demand to decide charging priorities, which can change between when energy
    is harvested and when it needs to be used; and 2), wastes energy as leakage when
    secondary buffers are only partially charged, failing to enable associated systems
    and keeping energy from higher-priority work. To minimize programmer speculation,
    decouple tasks which compete for buffered energy, and minimize leakage, energy
    must be fungible: the buffer must be capable of directing all harvested energy
    to any part of the system on demand.


    ## 2.4 Unified Dynamic Buffering


    Past work has also explored varying the behavior of a single unified buffer to
    capture the fungibility requirement described above. Dewdrop [\[6\]](#page-11-20)
    varies the enable voltage to draw from a single capacitor according to projected
    needs (e.g., begin operation at 2.2V instead of 3.6V)—providing complete energy
    fungibility—but still suffers from the reactivitylongevity tradeoff of capacitor
    size. Morphy [\[49\]](#page-12-3) replaces static buffers using a set of capacitors
    in a unified switching network; software can connect and disconnect arbitrary
    sets of capacitors in series or parallel to produce different equivalent capacitances.
    Morphy addresses two challenges facing batteryless systems: fast charging from
    low-voltage sources by operating the capacitor network as a charge pump, and charge
    isolation to allocate set amounts of energy to tasks and prevent buggy code from
    discharging the entire buffer by isolating parts of the capacitor array.


    We evaluate REACT alongside Morphy because the Morphy architecture can also be
    used to target the reactivity and longevity challenges discussed in § [1.](#page-0-0)
    By starting with a small equivalent capacitance, the system rapidly reaches an
    operational voltage. If the buffer as configured reaches capacity, software can
    gradually reconfigure the capacitor network to increase capacitance. Charge flows
    between the capacitors to equalize the voltage on the capacitor array and reduce


    <span id="page-3-1"></span>![](_page_3_Figure_6.jpeg)


    Figure 2. REACT diagram and signal flow between components.


    the output voltage of the network, enabling the bank to harvest more energy without
    reaching capacity. However, this current flow between capacitors to equalize voltage
    during reconfiguration dissipates a significant amount of the energy stored in
    the network (we explore these energy dynamics in detail in § [3.3.1\)](#page-4-1).
    Our evaluation in § [5](#page-8-0) shows that this internal power dissipation
    reduces common-case end-to-end performance to below that of systems using appropriatelysized
    static capacitors, making this approach impractical for energy-constrained devices.
    An energy-focused approach prioritizing minimal power dissipation is key to developing
    intermittent systems that can simultaneously maximize reactivity, longevity, and
    overall efficiency.


    ## <span id="page-3-0"></span>3 Design


    An intelligent energy buffering strategy is key to effective and efficient batteryless
    systems. Three performance objectives, informed by the advantages and limitations
    of prior approaches, drive REACT''s design:


    - Minimize charge time: Rapidly reaching the operational voltage, even when buffered
    energy cannot support complex operation, maximizes reactivity and enables systems
    to reason about power or sensor events from within low-power sleep modes.

    - Maximize capacity: System-wide longevity and efficiency require buffering large
    amounts of incoming energy when power supply exceeds demand, either to power long-running
    uninterruptible operations or support future power demand when supply is low.

    - Maximize energy fungibility: Unpredictable power demand patterns mean that energy
    cannot be preprovisioned to specific operations at harvest-time; systems need
    the ability to draw all harvested energy from the buffer and direct it as needed.


    ## 3.1 REACT Overview


    REACT buffers energy using a fabric of reconfigurable capacitor banks that software
    adjusts as needed. Figure [2](#page-3-1) shows a


    high-level overview of REACT''s hardware design. We design REACT''s hardware component
    as a drop-in replacement for a typical buffer between the harvester and the rest
    of the system, while the buffer management software requires no code modification
    or programmer input. The only system prerequisite is a set of digital I/O pins
    to configure capacitor banks and receive voltage monitoring information.


    ## 3.2 Cold-start Operation and the Last-level Buffer


    From a cold start (() = 0), REACT minimizes overall capacitance in order to rapidly
    charge to the operational voltage and enable the system with minimum energy input
    (high reactivity). The minimum capacitance is set by the smallest quantum of useful
    work available on the system (minimum required longevity), such as a short-lived
    software operation or an initialization routine that puts the system into a lowpower
    responsive sleep mode. REACT provides this rapid charge time using a small static
    buffer referred to hereafter as the last-level buffer. Additional capacitor banks
    are connected using normally-open switches and only contribute to overall capacitance
    when configured to do so in software, after the system is able to reason about
    buffered energy.


    The last-level buffer sets the minimum capacitance at power-on when all other
    banks are disconnected. This enables simple tuning of the energy input required
    to enable the system (reactivity) and the guaranteed energy level when the system
    does begin work (minimum longevity). It also smooths voltage fluctuations induced
    by capacitor bank switching (§ [3.3.4\)](#page-6-0). Finally, the last-level buffer
    serves as the combination point between the different capacitor banks and the
    rest of the system. Although energy may be stored in multiple banks of varying
    capacity at different voltages, combining it at the last-level buffer simplifies
    system design by presenting harvested power as a unified pool of energy which
    the system taps as needed (i.e., harvested energy is fungible).


    3.2.1 Monitoring Buffered Energy. Despite mutual isolation, bank voltages tends
    to equalize: the last-level buffer pulls energy from the highest-voltage bank
    first, and current flows from the harvester to the lowest-voltage bank first.
    This enables REACT to measure only the voltage on the lastlevel buffer as a surrogate
    for remaining energy capacity. If voltage rises beyond an upper threshold—the
    buffer is near capacity—REACT''s voltage instrumentation hardware signals the
    software component running on the microcontroller to increase capacitance using
    the configurable banks. Voltage falling below a lower threshold indicates the
    buffer is running out of energy and that REACT should reconfigure banks to extract
    additional energy and extend operation. RE-ACT''s instrumentation only needs to
    signal three discrete states—near capacity, near undervoltage, and OK—so two low-power
    comparators is sufficient for energy estimation.


    <span id="page-4-3"></span>![](_page_4_Figure_5.jpeg)


    Figure 3. REACT capacitor banks in different bank sizes and configurations. Arrows
    indicate charging current path.


    <span id="page-4-2"></span>![](_page_4_Figure_7.jpeg)


    Figure 4. Structure of the unified approach presented by Yang et al. [\[49\]](#page-12-3).
    Arrows indicate charging current path.


    ## <span id="page-4-0"></span>3.3 Dynamic Capacitor Banks


    The last-level buffer on its own enables high reactivity and minimizes cold-start
    energy below the operational minimum, maximizing efficiency during power starvation.
    However, when net power into the buffer is positive—such as during a period of
    high input power or low workload—the small last-level buffer rapidly reaches capacity.
    REACT provides the energy capacity required to both maximize efficiency and support
    long-running operation by connecting configurable capacitor banks when the last-level
    buffer reaches capacity, as shown in Figure [2.](#page-3-1)


    <span id="page-4-1"></span>3.3.1 Capacitor Organization. Careful management of
    the connections between each capacitor is key to maximizing energy efficiency
    while also presenting a valid operational voltage for the computational backend.
    Morphy [\[49\]](#page-12-3) presents one approach: by connecting a set of equally-sized
    capacitors through switches similar to a charge pump, overall buffer capacitance
    can be varied across a wide range of capacitance


    <span id="page-5-0"></span>![](_page_5_Figure_0.jpeg)


    Figure 5. Dissipative current flow in a fully-unified buffer during reconfiguration.
    Energy is dissipated by current spikes after capacitors at different voltages
    are placed in parallel.


    values. Different switch configurations produce intermediate buffer sizes between
    the extremes shown in Figure [4;](#page-4-2) gradually stepping through these
    configurations smoothly varies capacitance through software control.


    A fully interconnected array enables a wide range of equivalent, but introduces
    significant waste through dissipative heating when the charged capacitor array
    is reconfigured. Figure [5](#page-5-0) illustrates how energy is lost when charged
    capacitors are connected in a new configuration. Before reconfiguration, the energy
    contained in the system is = 1 2 (/4) 2 ; when a capacitor is taken out of series
    and placed in parallel with the remaining capacitors to increase equivalent capacitance
    to 4/3, current flows to the lowervoltage newly-parallel capacitor to equalize
    output voltage. The final output voltage is 3 /8, and the remaining energy is
    = 1 2 (4/3) (3 /8) 2 . The portion of energy conserved is / = 0.75—i.e., 25% of
    buffered energy is dissipated by current in the switches during reconfiguration.
    Larger arrays are increasingly inefficient: the same scenario with an 8-capacitor
    array wastes 56.25% of its buffered energy transitioning from an 8-parallel to
    a 7-series-1-parallel configuration. Similar waste occurs when reducing equivalent
    capacitance by placing capacitors in series.[2](#page-5-1) Our evaluation in §
    [5.5](#page-10-0) indicates that the energy loss caused by switching often outweighs
    any advantage from dynamic behavior, causing the fully-connected approach to underperform
    even static buffers.


    3.3.2 Bank Isolation. The switching loss discussed above stems from charge flowing
    between capacitors within the power network as they switch into different configurations.
    REACT eliminates unnecessary current flow by organizing capacitors into independent,
    mutually isolated banks as shown in Figure [2.](#page-3-1) Figure [3](#page-4-3)
    illustrates in detail two example capacitor banks in each possible configuration:
    capacitors within a bank can only be arranged in either full-series (low


    capacitance) or full-parallel (high capacitance) so that no current flows between
    capacitors within a bank. Isolation diodes on the input and output of each bank
    prevent current between banks: when a charged parallel-configured bank is reconfigured
    into series (reducing its capacitance and boosting its output voltage), isolation
    diodes prevent it from charging other banks in the array. Similarly, banks re-configured
    into parallel cannot draw current from anywhere except the energy harvester. Isolation
    reduces the number of potential capacitor configurations compared to a fully-connected
    network, but dramatically increases energy efficiency.


    REACT''s isolation diodes direct the flow of current: intermediate capacitor arrays
    are only charged directly from the energy harvester and only discharge to the
    last-level buffer. This also means that all current from the harvester flows through
    two diodes before reaching the system, so minimizing power dissipation in the
    diodes is essential to maintaining overall system efficiency. To maximize charging
    efficiency, we design REACT using ideal diode circuits incorporating a comparator
    and pass transistor, rather than typical PN or Schottky diodes. Active ideal diodes
    are far more efficient at typical currents for batteryless systems: the circuit
    we use [\[21\]](#page-11-21) dissipates 0.02% of the power dissipated in a typical
    Schottky diode [\[42\]](#page-12-10) at a supply current of 1 mA.


    3.3.3 Bank Reconfiguration. The range of buffer sizes depends on the number of
    capacitor banks and the number of capacitors in each bank. REACT''s capacitor
    banks are effectively connected in parallel, so the overall capacitance is the
    sum of each bank''s contribution. Each REACT bank containing identical capacitors
    of capacitance may be configured to contribute no capacitance (disconnected),
    series capacitance /, or parallel capacitance .


    REACT must increment buffer capacitance in small steps in order to keep voltage
    within the operational range while capturing all incoming power. A large increase
    in capacitance pulls output voltage down and introduces cold-start energy loss
    if net power input is low; for extreme cases, the system may run out of energy
    and cease execution while the new capacitance charges even if incoming power would
    be sufficient to power operation. REACT first connects banks in the series configuration
    to contribute a small capacitance and avoid large jumps in overall buffer size.
    If the buffer continues to charge and reaches the upper voltage limit ℎℎ, REACT
    further expands capacitance by toggling double-pole-doublethrow bank switches
    to configure the capacitors in parallel. Expanding the buffer by reconfiguring
    charged capacitors rather than adding new ones reduces the time the system is
    cut off from input power while current flows exclusively to the new capacitance,
    because it is already charged toℎℎ/. Because no current flows between capacitors
    or banks, bank reconfiguration changes capacitance seen on the common rail without
    dissipative loss. REACT uses break-before-make switches to ensure no short-circuit
    current flows during


    <span id="page-5-1"></span><sup>2</sup>Charge pumps avoid this waste by never
    connecting capacitors at different potentials in parallel; in this use case, however,
    parallel capacitance is always necessary to smooth voltage fluctuations during
    switching and keep the output voltage within the computational backend''s acceptable
    range.


    switching; incoming current flows directly to the last-level buffer during the
    momentary open-circuit in the bank.


    <span id="page-6-0"></span>3.3.4 Charge Reclamation. Reconfiguring a bank from
    series to parallel allows REACT to efficiently increase capacitance without dropping
    output voltage. When voltage on the last-level buffer appraches the threshold
    value , indicating net power is leaving the buffer, REACT needs to reduce equivalent
    capacitance to boost voltage and keep the backend running. REACT accomplishes
    this by transitioning charged -capacitor banks from the parallel to the series
    configuration, reducing equivalent capacitance from to / and boosting output voltage
    from to . This boosts voltage on the last-level buffer and extracts more energy
    from the capacitor bank than would otherwise be available once voltage falls below
    .


    The remaining energy unavailable after the parallel→series transition depends
    on the number of -size capacitors in the bank. Before switching, the cold-start
    energy stored on the parallel-mode bank is = 1 2 2 . Switching to the series configuration
    conserves stored energy: = 1 2 ( /) () <sup>2</sup> = , but boosts voltage to
    enable the digital system to continue extracting energy. If net power remains
    negative, the system eventually drains the series-configuration bank down to .
    This is energetically equivalent to draining the parallel-configuration bank to
    /, leaving = 1 2 (/) <sup>2</sup> = 1 2 2 / unusable; the overall result is that
    REACT reduces energy loss by a factor of <sup>2</sup> when reducing system capacitance
    compared to simply disconnecting the capacitor.


    3.3.5 Bank Size Constraints. Increasing the number of capacitors in a bank improves
    efficiency by reclaiming more energy when switching a bank from parallel to series.
    However, it also introduces voltage spikes when the bank output voltage is temporarily
    multiplied by , increasing dissipative loss as current flows from the bank to
    the lastlevel buffer. Because REACT measures overall energy at the last-level
    buffer, the software component may interpret this voltage spike as a buffer-full
    signal and incorrectly add capacitance despite low buffered energy. In extreme
    cases, the voltage spike may exceed component absolute limits.


    The size of the last-level buffer constrains the number and size of each capacitor
    in a bank in order to keep voltage below REACT''s buffer-full threshold during
    a parallel→series transition. A larger contains more energy and thus pulls voltage
    higher when switched from parallel to series. Equation [1](#page-6-1) gives the
    last-level buffer voltage after switching a bank to series at a trigger voltage
    :


    <span id="page-6-1"></span>

    $$V\_{\text{new}} = \frac{(NV\_{low})(C\_{unit}/N)}{C\_{last} + C\_{unit}/N} +
    \frac{V\_{low} \* C\_{last}}{C\_{last} + C\_{unit}/N} \tag{1}$$


    Constraining < ℎℎ and solving for yields the absolute limit for (Equation [2\)](#page-6-2).
    Note that is only constrained if the parallel→series transition at produces


    <span id="page-6-3"></span>


    | Bank                    | 0   | 1   | 2   | 3   | 4   | 5    |

    |-------------------------|-----|-----|-----|-----|-----|------|

    | Capacitor Size (𝜇𝐹<br>) | 770 | 220 | 440 | 880 | 880 | 5000 |

    | Capacitor Count         | 1   | 3   | 3   | 3   | 3   | 2    |


    Table 1. Bank size and configurations for our REACT test implementation. Bank
    0 is the last-level buffer.


    a voltage above ℎℎ:


    <span id="page-6-2"></span>

    $$C\_{unit} < \frac{NC\_{last}(V\_{high} - V\_{low})}{NV\_{low} - V\_{high}} \tag{2}$$


    ## 3.4 REACT Software Interface


    REACT''s standalone hardware design means that the software component running
    on the target microcontroller is minimal. The software subsystem monitors for
    incoming over- or under-voltage signals from REACT''s voltage instrumentation
    and maintains a state machine for each capacitor bank. Each capacitor bank is
    disconnected at startup; on an overvoltage signal from REACT''s hardware, the
    software directs REACT to connect a new capacitor bank in the series configuration.
    A second overvoltage signal[3](#page-7-0) causes REACT to reconfigure the newly-connected
    bank to parallel; on the next overvoltage signal, REACT connects a second capacitor
    bank, and so on. REACT similarly steps capacitor banks in the opposite direction
    when an undervoltage signal arrives.


    <span id="page-6-4"></span>3.4.1 Software-Directed Longevity. REACT''s software
    component requires no active programmer intervention or code changes aside from
    setting voltage thresholds, initializing each bank state machine, and setting
    the order to connect and disconnect banks. Software does not need to know the
    details (, ) of each bank, although this information with the state of each bank
    gives a coarse idea of the current buffered energy. Because REACT only changes
    capacitance when the bank is near-full or near-empty, capacitance level is an
    effective surrogate for stored energy. Application code can use this feedback
    to set longevity guarantees through RE-ACT''s software interface. In preparation
    for a long-running or high-energy atomic operation, software sets a minimum capacitance
    level corresponding to the amount of energy required and then enters a deep-sleep
    mode keeping REACT''s capacitor polling time active. As the system charges, REACT
    eventually accumulates enough energy to reach the minimum capacitance level—indicating
    that enough energy is stored to complete the planned operation, and pulling the
    system out of its deep-sleep with enough energy to complete execution regardless
    of future power conditions.


    ## 4 Implementation


    We explore REACT''s impact on overall efficiency, reactivity, and longevity using
    a hardware prototype integrated into a real batteryless platform. All files for
    REACT, the baseline systems, and the energy harvesting emulator will be open-sourced
    upon publication. Our testbed is based on the MSP430FR5994 [\[22\]](#page-11-22),
    a popular microcontroller for energy harvesters [\[28,](#page-11-10) [44,](#page-12-6)
    [45\]](#page-12-11). For each buffer configuration we evaluate, an intermediate
    circuit power gates the MSP430 to begin operation once the buffer is charged to
    3.3V and disconnects it when the buffer voltage reaches 1.8V.


    Our REACT implementation has a range of 770 -18.03 using a set of 5 dynamic banks,
    in addition to the lastlevel buffer, detailed in Table [1.](#page-6-3) We implement
    the capacitors in banks 0-4 using combinations of 220 capacitors with max leakage
    current of 28 at their rated voltage of 6.3V [\[32\]](#page-12-12). Bank 5 uses
    supercapacitors with approximately 0.15 leakage current at 5.5V [\[33\]](#page-12-13).


    ## 4.1 Baseline Systems


    We evaluate REACT against three fixed-size buffers spanning our implementation''s
    capacitance range—770 F, 10 mF, and 18 mF—to ensure the realized improvement is
    a result of energy-adaptive behavior rather than simply different buffer capacity.
    To compare REACT''s capacitor architecture to prior work on dynamic energy buffers,
    we also implement and evaluate Morphy [\[49\]](#page-12-3) for a similar capacitance
    range. Our Morphy implementation uses eight 2 capacitors with leakage current
    of approximately 25.2 at 6.3V [\[35\]](#page-12-14) (i.e., slightly lower leakage
    than the capacitors in REACT).


    Morphy uses a secondary microcontroller powered by a battery or backup capacitor
    to control the capacitor array; we use a second MSP430FR5994 powered through USB,
    corresponding to Morphy''s battery-powered design. Accordingly, we expect our
    results to slightly overestimate Morphy''s performance in the fully-batteryless
    case as the system does not have to power the Morphy controller or charge a backup
    capacitor in our implementation. Seven of the eight capacitors in the array are
    available to reconfigure, with one task capacitor kept in parallel to smooth voltage
    fluctuations from switching. We evaluate the same subset of eleven possible configurations
    for the remaining seven capacitors as is done in the original Morphy work, resulting
    in a capacitance range for our Morphy implementation of 250 -16 .


    ## 4.2 Computational Backend


    To explore how REACT affects performance across a range of system demands—focusing
    on diverse reactivity and longevity requirements—we implement four software benchmarks:


    - Sense and Compute (SC): Exit a deep-sleep mode once every five seconds second
    to sample and digitally filter readings from a low-power microphone [\[11\]](#page-11-23).
    This benchmark represents systems which value high reactivity and can accept low
    persistence; individual atomic measurements are low-energy, but the system must
    be online to take the measurements.

    - Radio Transmission (RT): Send buffered data over radio [\[18,](#page-11-24)
    [31\]](#page-12-15) to a base station. Data transmission is an example of an application
    with high persistence requirements (radio transmissions are atomic and energyintensive)
    and low reactivity requirements (transmitting data may be delayed until energy
    is available).

    - Packet Forwarding (PF): Listen for and retransmit unpredictable incoming data
    over the radio. Timely packet forwarding demands both high persistence and reactivity
    to successfully receive and retransmit data.


    We emulate the power consumption of the necessary peripherals for each benchmark
    by toggling a resistor connected to a digital output on the MSP430, with values
    for each benchmark chosen to match the relevant peripheral. The reactivity-focused
    benchmarks (SC and PF) have deadlines that may arrive while the system is off;
    we use a secondary MSP430 to deliver these events. A deployed system may use remanence-based
    timekeepers [\[8\]](#page-11-25) to track internal deadlines despite power failures
    for the SC benchmark, while incoming packets as in the PF benchmark would arrive
    from other systems. Although we evaluate each benchmark in isolation, full systems
    are likely to exercise combinations of each requirement—one platform should support
    all reactivity, persistence, and efficiency requirements.


    ## <span id="page-7-1"></span>4.3 Energy Harvesting Frontend


    Energy harvesting volatility makes repeatable experimentation with batteryless
    devices difficult; uncontrollable environmental changes often have an outsized
    effect on energy input and obfuscate differences in actual system performance.
    We make our experiments repeatable and consistent using a programmable power frontend
    inspired by the Ekho [\[14\]](#page-11-26) record-and-replay platform. The power
    controller supplies the energy buffer using a high-drive Digital-to-Analog Converter
    (DAC), measures the load voltage and input current using a sense resistor, and
    tunes the DAC to supply a programmed power level. We evaluate REACT emulating
    both solar (5 <sup>2</sup> , 22% efficient cell [\[43\]](#page-12-16)) and RF
    energy (915 MHz dipole antenna [\[39\]](#page-12-17)). We also emulate the load-dependent
    performance of a commercial RF-to-DC converter [\[37\]](#page-12-18) and solar
    panel management chip [\[20\]](#page-11-27).


    <span id="page-7-0"></span><sup>3</sup>REACT polls the over/undervoltage signals
    using an internal timer rather than edge-sensitive interrupts to handle cases
    such as a high enough power input that the capacitance step does not pull supply
    voltage below .


    ## <span id="page-8-0"></span>5 Evaluation


    We evaluate REACT alongside the baseline buffers running each benchmark under
    three RF and two solar traces from publicly available repositories [\[3,](#page-11-28)
    [12\]](#page-11-13), representative of power dynamics for small energy harvesting
    systems. We record the RF traces in an active office environment using a commercial
    harvester and transmitter [\[37,](#page-12-18) [38\]](#page-12-19) and use solar
    irradiance traces from the Enhants mobile irradiance dataset [\[12\]](#page-11-13);
    Table [3](#page-9-0) gives a short summary of each trace. These traces show the
    power variability common for IoT-scale harvesters: environmental changes (e.g.,
    ambient RF levels, time of day) affect average input power, while short-term changes
    such as orientation cause instantaneous variation even if the environment is unchanged.
    We apply each trace using the power replay system described in § [4.3;](#page-7-1)
    once the trace is complete, we let the system run until it drains the buffer capacitor.


    ## 5.1 Characterization and Overhead


    Figure [6](#page-9-1) illustrates REACT''s behavior through the last-level buffer
    voltage when varying capacitance; the inset focuses on REACT''s voltage output
    as it expands to capture energy (also shown is the voltage of the comparable Morphy
    array). From a cold start REACT only charges the last-level buffer—rapidly reaching
    the enable voltage and then the upper voltage threshold (3.5V). REACT then adds
    a seriesconfigured capacitor bank to capture excess incoming energy. Voltage drops
    as the system temporarily operates exclusively from the last-level buffer while
    harvested energy goes towards charging the new capacitance. As power input falls,
    REACT''s output voltage falls below the upper threshold voltage—indicating REACT
    is operating at an efficient capacitance point. At ≈ 450 the last-level buffer
    is discharged to the lower threshold and REACT begins switching banks into series
    mode to boost their output voltage and charge the last-level buffer, visible in
    Figure [6](#page-9-1) as five voltage spikes corresponding to each capacitor bank—sustaining
    operation until no more energy is available at ≈ 500.


    We characterize REACT''s software overhead by running the DE benchmark on continuous
    power for 5 minutes with and without REACT''s software component, which periodically
    interrupts execution to measure the capacitor bank. At a sample rate of 10 Hz,
    REACT adds a 1.8% penalty to software-heavy applications. We measure REACT''s
    power overhead by comparing the execution time of systems running the DE benchmark
    using REACT and the 770 buffer after charging each to their enable voltage. Based
    on this approach we estimate that our implementation of REACT introduces a 68
    power draw, or <sup>∼</sup>14 per bank.


    ## 5.2 REACT Minimizes System Latency


    Table [4](#page-9-2) details the time it takes each system to begin operation,
    across power traces and energy buffers (charge time


    is software-invariant and constant across benchmarks). Latency is driven by both
    capacitor size and environment—the 10 buffer is <sup>∼</sup>13 larger than the
    770 buffer and takes on average 7 longer to activate the system across our traces.
    High-capacity static buffers incur a larger latency penalty even if mean power
    input is high if much of that power is contained in a short-term spike later in
    the trace (e.g., for the Solar Campus trace), but these dynamics are generally
    impossible to predict at design time. By exclusively charging the last-level buffer
    while the rest of the system is off, REACT matches the latency of the smallest
    static buffer—an average of 7.7 faster than the equivalent-capacity 17 mF buffer,
    which risks failing to start at all. Morphy further reduces system latency because
    its smallest configuration is smaller than REACT''s last-level buffer (250 vs
    770 ), although the limited reduction in average latency compared to the reduction
    in capacitance (Morphy realizes an average 20% reduction in latency over REACT
    using a 68% smaller capacitance) suggests that further reducing capacitance yields
    diminishing latency returns in realistic energy environments.


    Minimizing latency improves reactivity-bound applications such as the SC and PF
    benchmarks; this effect is visible in Table [2](#page-9-3) as the 770 buffer outperforms
    larger static versions in the SC benchmark for relatively low-power traces (RF
    Mobile/Obstructed). REACT inherits the latency advantage due to the small last-level
    buffer, similarly improving performance on each power trace. Morphy realizes a
    similar performance improvement over the static systems, but ultimately underperforms
    REACT as a result of inefficient capacitor switching (§ [5.5\)](#page-10-0). Small
    static buffers enable lowlatency operation, but at the cost of energy capacity.
    As power input increases, the latency penalty of large buffers fades and their
    increased capacity enables them to operate for longer—resulting in higher performance
    for larger static buffers under high-power traces (RF Cart, Solar Campus). Smaller
    buffers, in turn, become less efficient as they must burn more incoming energy
    off as waste heat.


    ## 5.3 REACT Maximizes Energy Capacity


    Figure [6](#page-9-1) illustrates the system-level effects of the capacitylatency
    tradeoff, and how REACT avoids this tradeoff through energy-adaptive buffering.
    The small 770 buffer charges rapidly, but reaches capacity and discharges energy
    when it does not have work to match incoming power (illustrated by clipping at
    3.6V on the 770 F line). The 10 mF buffer sacrifices latency for capacity—starting
    operation 21x later than the smaller buffer, but avoiding overvoltage. Morphy
    begins execution early with a small capacitance, but its lossy switching mechanism
    means it does not overall outperform the 770 buffer. In contrast, REACT achieves
    low latency, high efficiency, and high capacity by efficiently expanding capacitance
    as necessary after enabling the system.


    Tables [2](#page-9-3) and [5](#page-9-4) show that high capacity is valuable when
    average input power exceeds output power (e.g., DE and SC


    <span id="page-9-3"></span>


    |            | Data Encrypt |      |      |        | Sense and Compute |      |     |     |        |
    Radio Transmit |      |      |      |        |       |

    |------------|--------------|------|------|--------|-------------------|------|-----|-----|--------|----------------|------|------|------|--------|-------|

    | Buffer     | 770𝜇         | 10m  | 17m  | Morphy | REACT             | 770𝜇
    | 10m | 17m | Morphy | REACT          | 770𝜇 | 10m  | 17m  | Morphy | REACT |

    | RF Cart    | 1275         | 1574 | 1831 | 1745   | 1711              | 50   |
    81  | 104 | 77     | 83             | 22   | 53   | 56   | 38     | 48    |

    | RF Obs.    | 666          | 472  | 0    | 357    | 576               | 44   |
    28  | 0   | 39     | 49             | 4    | 6    | 0    | 0      | 3     |

    | RF Mob.    | 810          | 1004 | 645  | 801    | 1038              | 52   |
    50  | 40  | 53     | 84             | 4    | 13   | 12   | 4      | 15    |

    | Sol. Camp. | 6666         | 7290 | 7936 | 8194   | 9756              | 330  |
    353 | 367 | 398    | 439            | 1376 | 1457 | 1542 | 1059   | 1426  |

    | Sol. Comm. | 2168         | 2186 | 2554 | 2399   | 2232              | 88   |
    110 | 130 | 133    | 154            | 8    | 40   | 48   | 31     | 34    |

    | Mean       | 2317         | 2505 | 2593 | 2699   | 3063              | 113  |
    124 | 128 | 140    | 162            | 283  | 314  | 332  | 226    | 313   |


    Table 2. Performance on the DE, SC, and RT benchmarks, across traces and energy
    buffers.


    <span id="page-9-0"></span>


    |                                                                       | Trace          |
    Time (s) | Avg. Pow. (mW) | Power CV* |  |  |  |  |

    |-----------------------------------------------------------------------|----------------|----------|----------------|-----------|--|--|--|--|

    |                                                                       | RF Cart        |
    313      | 2.12           | 103%      |  |  |  |  |

    |                                                                       | RF Obstruction
    | 313      | 0.227          | 61%       |  |  |  |  |

    |                                                                       | RF Mobile      |
    318      | 0.5            | 166%      |  |  |  |  |

    |                                                                       | Solar
    Campus   | 3609     | 5.18           | 207%      |  |  |  |  |

    |                                                                       | Solar
    Commute  | 6030     | 0.148          | 333%      |  |  |  |  |

    | Table 3. Details of each power trace. *CV = Coefficient of Variation. |                |          |                |           |  |  |  |  |


    <span id="page-9-2"></span>


    | Buffer     | 770 𝜇𝐹 | 10 mF  | 17 mF  | Morphy | REACT |

    |------------|--------|--------|--------|--------|-------|

    | RF Cart    | 6.65   | 17.73  | 31.27  | 5.51   | 6.65  |

    | RF Obs.    | 14.58  | 223.07 | -      | 6.50   | 16    |

    | RF Mob.    | 6.90   | 148.10 | 239.88 | 5.65   | 6.38  |

    | Sol. Camp. | 42.11  | 737.39 | 741.42 | 35.59  | 41.26 |

    | Sol. Comm. | 119.60 | 196.30 | 213.00 | 108.10 | 130.6 |

    | Mean       | 37.97  | 264.92 | 306.39 | 32.27  | 40.18 |


    Table 4. System latency (seconds) across traces and energy buffers. - indicates
    system never begins operation.


    <span id="page-9-1"></span>![](_page_9_Figure_6.jpeg)


    Figure 6. Buffer voltage and on-time for the SC benchmark under RF Mobile power.
    Solid bars indicate when the system is operating.


    benchmarks executed under the RF Cart trace), or when peak power demand is uncontrollable
    and uncorrelated with input (e.g., the PF benchmark executed on Solar Campus,
    where both power supply and demand are concentrated in short bursts). In both
    cases, high-capacity systems store excess


    <span id="page-9-4"></span>


    | Buffer     | 770 uF |     |     | 10 mF | 17 mF |     | Morphy |     | REACT
    |     |

    |------------|--------|-----|-----|-------|-------|-----|--------|-----|-------|-----|

    | Packets    | Rx     | Tx  | Rx  | Tx    | Rx    | Tx  | Rx     | Tx  | Rx    |
    Tx  |

    | RF Cart    | 22     | 10  | 49  | 49    | 48    | 48  | 55     | 22  | 53    |
    52  |

    | RF Obs.    | 4      | 4   | 4   | 4     | 0     | 0   | 2      | 0   | 3     |
    0   |

    | RF Mob.    | 11     | 4   | 14  | 13    | 9     | 9   | 19     | 0   | 38    |
    5   |

    | Sol. Camp. | 163    | 163 | 240 | 240   | 196   | 196 | 206    | 204 | 284   |
    277 |

    | Sol. Comm. | 72     | 8   | 35  | 35    | 33    | 33  | 85     | 14  | 84    |
    63  |

    | Mean       | 54     | 38  | 68  | 68    | 57    | 57  | 73     | 48  | 92    |
    80  |


    Table 5. Packets successfully received and retransmitted during the Packet Forwarding
    benchmark.


    energy to continue operation even if future power input falls or demand rises.
    REACT efficiently expands to capture all incoming energy during periods of high
    net input power, matching or beating the performance of the 10 mF and 17 mF systems
    when they outperform the small 770 buffer.


    ## 5.4 REACT Provides Flexible, Efficient Longevity


    We evaluate REACT''s software-directed longevity guarantees (§ [3.4.1\)](#page-6-4)
    on the longevity-bound RT and PF benchmarks. We compare REACT to the 770 buffer,
    which cannot sustain a full transmission without additional input power. Running
    the RT benchmark under the RF Cart trace isolates this limitation as the 770 static
    buffer significantly underperforms the other buffers despite never reaching capacity:
    instead, it wastes power on doomed-to-fail transmissions when incoming power cannot
    make up for the deficit. We augment the RT benchmark code for our REACT implementation
    to include a minimum capacitance level for REACT, below which the system waits
    to gather more energy in a lowpower sleep mode. Leveraging REACT''s variable capacitance
    allows software to buffer energy to guarantee completion, more than doubling the
    number of successful transmissions and ultimately outperforming even the larger
    buffers.


    We use the same approach to execute the RT benchmark on our Morphy implementation.
    Similar to REACT, Morphy varies capacitance to keep supply voltage within an acceptable
    level for the application microcontroller while also waiting to gather enough
    energy to power a full transmission. Morphy''s underperformance compared to both
    REACT and the static buffers is a result of Morphy''s capacitor network design—as
    Morphy reconfigures the capacitor array to increase capacitance, stored energy
    is dissipated as current


    flows between capacitors in the network. This energy dissipation dramatically
    reduces Morphy''s end-to-end performance, particularly in systems where Morphy
    must switch capacitance to ensure success (i.e., the RT and PF benchmarks). REACT''s
    isolated capacitor banks eliminate this problem by restricting current flow during
    switching; the energy savings are reflected in the end-to-end performance, where
    REACT completes on average 38% more transmissions than Morphy.


    5.4.1 Fungible Energy Storage. A unified buffer means that energy is fungible,
    and REACT is flexible: software can re-define or ignore previous longevity requirements
    if conditions change or a higher-priority task arrives. The PF benchmark (Table
    [5\)](#page-9-4) shows the value of energy fungibility using two tasks with distinct
    reactivity and longevity requirements. Receiving an incoming packet requires a
    moderate level of longevity, but is uncontrollable and has a strict reactivity
    requirement (the system can only receive a packet exactly when it arrives). Re-transmission
    requires more energy and thus more longevity, but has no deadline. Software must
    effectively split energy between a controllable high-power task and an uncontrollable
    lower-power task.


    As in the RT benchmark we use the minimum-capacitance approach to set separate
    longevity levels for each task, using a similar approach for our Morphy implementation.
    When the system has no packets to transmit, it waits in a deep-sleep until receiving
    an incoming packet. If REACT contains sufficient energy when the packet arrives,
    it receives and buffers the packet to later send. REACT then begins charging for
    the transmit task, forwarding the buffered packet once enough energy is available.
    If another packet is received while RE-ACT is charging for the transmit task,
    however, software disregards the transmit-associated longevity requirement to
    execute the receive task if sufficient energy is available.


    Table [5](#page-9-4) shows that REACT outperforms all static buffer designs on
    the PF benchmark by efficiently addressing the requirements of both tasks, resulting
    in a mean performance improvement of 54%. REACT''s maximal reactivity enables
    it to turn on earlier and begin receiving and buffering packets to send during
    later periods of high power, while its high capacity enables it to make the most
    of incoming energy during those high periods. Software-level longevity guarantees
    both ensure the system only begins receive/transmit operations when enough energy
    is available to complete them, and that software can effectively allocate energy
    to incoming events as needed. Although Morphy enables the same software-level
    control of energy allocation, the energy dissipated when switching capacitors
    in the interconnected array means that Morphy''s overall performance on the PF
    benchmark is below that of the best performing static buffer.


    ## <span id="page-10-0"></span>5.5 REACT Improves End-to-End System Efficiency


    Optimizing buffer behavior maximizes the amount of harvested energy available
    to the end system for useful work.


    <span id="page-10-1"></span>![](_page_10_Figure_6.jpeg)


    Figure 7. Average buffer performance quantified by figures of merit across power
    traces for each benchmark, normalized to REACT.


    Figure [7](#page-10-1) illustrates the aggregate performance of REACT compared
    to the baseline systems across the benchmarks and power traces we evaluate; we
    find that REACT improves performance over the equally-reactive 770 buffer by an
    average of 39.1%, over the equal-capacity 17 mF buffer by 19.3%, and over the
    next-best-efficient 10 mF buffer by 18.8%. Compared to Morphy, REACT improves
    aggregate performance by 26.2%—demonstrating the necessity of REACT''s bank isolation
    approach and boosting performance where prior dynamic capacitance systems underperform
    static approaches. Extreme cases where the system is always operating in an energy
    surplus or deficit—such as the low-power SC benchmark under the high-power RF
    Cart trace—the extra power consumption from REACT''s hardware causes it to underperform
    suitable static buffers because REACT''s flexibility is unnecessary. In the common
    case, however, volatile power conditions expose the latency, longevity, and efficiency-related
    shortcomings of static buffer designs and expose the value of REACT''s efficient
    variable-capacitance approach.


    ## 6 Conclusion


    Energy harvesting systems operate on unreliable and volatile power, but use fixed-size
    buffers which waste energy and functionally limit systems when allocated capacity
    is a poor fit for short-term power dynamics. REACT stores incoming energy in a
    fabric of reconfigurable capacitor banks, varying equivalent capacitance according
    to current energy supply and demand dynamics—adding capacitance to capture surplus
    power and reclaiming energy from excess capacitance. REACT''s energy-adaptive
    approach maximizes reactivity and capacity to ensure all incoming energy is captured
    and efficiently delivered to sensing, computing, and communication devices. Our
    hardware evaluation on real-world power


    traces shows that REACT reduces system latency by an average of 7.7x compared
    to an equivalent-sized static buffer and improves throughput by an average of
    25.6% over any static buffer system, while incorporating software direction allows
    REACT to provide flexible and fungible task longevity guarantees. Compared to
    state-of-the-art switched capacitor systems, REACT''s efficient switching architecture
    improves performance by an average of 26.2%.


    REACT''s runtime-configurable buffering technique eliminates the tradeoff between
    system latency and longevity, and affords designers greater control over how batteryless
    devices respond to incoming power. Our results indicate that energy-responsive
    reconfiguration of hardware is an effective approach to both maximizing energy
    efficiency and system functionality, opening the door for future work leveraging
    energy-adaptive hardware and reconfiguration.


    ## References


    - <span id="page-11-16"></span>[1] Saad Ahmed, Qurat ul Ain, Junaid Haroon Siddiqui,
    Luca Mottola, and Muhammad Hamad Alizai. Intermittent computing with dynamic voltage
    and frequency scaling. In Proceedings of the 2020 International Conference on
    Embedded Wireless Systems and Networks, EWSN ''20, page 97–107, USA, 2020. Junction
    Publishing.

    - <span id="page-11-2"></span>[2] Miran Alhaideri, Michael Rushanan, Denis Foo
    Kune, and Kevin Fu. The moo and cement shoes: Future directions of a practical
    sensecontrol-actuate application, September 2013. Presented at First International
    Workshop on the Swarm at the Edge of the Cloud (SEC''13 @ ESWeek), Montreal.

    - <span id="page-11-28"></span>[3] Anon. Rf traces, October 2022. [https://anonymous.4open.science/r/](https://anonymous.4open.science/r/rf_traces-4B3E/README.md)
    [rf\\_traces-4B3E/README.md](https://anonymous.4open.science/r/rf_traces-4B3E/README.md).

    - <span id="page-11-17"></span>[4] Abu Bakar, Alexander G. Ross, Kasim Sinan Yildirim,
    and Josiah Hester. Rehash: A flexible, developer focused, heuristic adaptation
    platform for intermittently powered computing. Proc. ACM Interact. Mob. Wearable
    Ubiquitous Technol., 5(3), sep 2021.

    - <span id="page-11-1"></span>[5] James Blackman. What is mmtc in 5g nr, and how
    does it impact nb-iot and lte-m, October 2019. https://enterpriseiotinsights.com/20191016/channels/fundamentals/whatis-mmtc-in-5g-nr-and-how-does-it-impact-nb-iot-and-lte-m.

    - <span id="page-11-20"></span>[6] Michael Buettner, Ben Greenstein, and David
    Wetherall. Dewdrop: An energy-aware runtime for computational rfid. In Proceedings
    of the 8th USENIX Conference on Networked Systems Design and Implementation, NSDI''11,
    page 197–210, USA, 2011. USENIX Association.

    - <span id="page-11-4"></span>[7] Alexei Colin, Emily Ruppel, and Brandon Lucia.
    A reconfigurable energy storage architecture for energy-harvesting devices. In
    Proceedings of the Twenty-Third International Conference on Architectural Support
    for Programming Languages and Operating Systems, ASPLOS ''18, page 767–781, New
    York, NY, USA, 2018. Association for Computing Machinery.

    - <span id="page-11-25"></span>[8] Jasper de Winkel, Carlo Delle Donne, Kasim
    Sinan Yildirim, Przemysław Pawełczak, and Josiah Hester. Reliable timekeeping
    for intermittent computing. In Proceedings of the Twenty-Fifth International Conference
    on Architectural Support for Programming Languages and Operating Systems, ASPLOS
    ''20, page 53–67, New York, NY, USA, 2020. Association for Computing Machinery.

    - <span id="page-11-6"></span>[9] Jasper de Winkel, Vito Kortbeek, Josiah Hester,
    and Przemysław Pawełczak. Battery-free game boy. Proc. ACM Interact. Mob. Wearable
    Ubiquitous Technol., 4(3), sep 2020.

    - <span id="page-11-15"></span>[10] H. Desai and B. Lucia. A power-aware heterogeneous
    architecture scaling model for energy-harvesting computers. IEEE Computer Architecture
    Letters, 19(1):68–71, 2020.

    - <span id="page-11-23"></span>[11] Knowles Electronics. SPU0414HR5H-SB, December
    2012. [https://www.mouser.com/datasheet/2/218/knowles\\_01232019\\_](https://www.mouser.com/datasheet/2/218/knowles_01232019_SPU0414HR5H_SB-1891952.pdf)
    [SPU0414HR5H\\_SB-1891952.pdf](https://www.mouser.com/datasheet/2/218/knowles_01232019_SPU0414HR5H_SB-1891952.pdf).

    - <span id="page-11-13"></span>[12] M. Gorlatova, A. Wallwater, and G. Zussman.
    Networking low-power energy harvesting devices: Measurements and algorithms. In
    2011 Proceedings IEEE INFOCOM, pages 1602–1610, 2011.

    - <span id="page-11-3"></span>[13] Wang Song Hao and Ronald Garcia. Development
    of a digital and battery-free smart flowmeter. Energies, 7(6):3695–3709, 2014.

    - <span id="page-11-26"></span>[14] Josiah Hester, Timothy Scott, and Jacob Sorber.
    Ekho: Realistic and repeatable experimentation for tiny energy-harvesting sensors.
    In Proceedings of the 12th ACM Conference on Embedded Network Sensor Systems,
    SenSys ''14, page 330–331, New York, NY, USA, 2014. Association for Computing
    Machinery.

    - <span id="page-11-18"></span>[15] Josiah Hester, Lanny Sitanayah, and Jacob
    Sorber. Tragedy of the coulombs: Federating energy storage for tiny, intermittently-powered
    sensors. In ACM Conference on Embedded Networked Sensor Systems, SenSys, pages
    5–16, 2015.

    - <span id="page-11-19"></span>[16] Josiah Hester and Jacob Sorber. Flicker: Rapid
    prototyping for the batteryless internet-of-things. In Proceedings of the 15th
    ACM Conference on Embedded Network Sensor Systems, SenSys ''17, New York, NY,
    USA, 2017. Association for Computing Machinery.

    - <span id="page-11-8"></span>[17] Matthew Hicks. Clank: Architectural support
    for intermittent computation. In International Symposium on Computer Architecture,
    ISCA, pages 228–240, 2017.

    - <span id="page-11-24"></span>[18] Fraunhofer IIS. RFicient Basic, Ultra-Low-Power
    WakeUp Receiver, January 2019. [https://www.iis.fraunhofer.de/content/dam/iis/en/doc/](https://www.iis.fraunhofer.de/content/dam/iis/en/doc/il/ics/ic-design/Datenblaetter/Factsheet_WakeUp_v4.pdf)
    [il/ics/ic-design/Datenblaetter/Factsheet\\_WakeUp\\_v4.pdf](https://www.iis.fraunhofer.de/content/dam/iis/en/doc/il/ics/ic-design/Datenblaetter/Factsheet_WakeUp_v4.pdf).

    - <span id="page-11-0"></span>[19] Texas Instruments. Msp430g2x52, msp430g2x12
    mixed signal microcontroller datasheet (rev. g), May 2013. [https://www.ti.com/lit/ds/](https://www.ti.com/lit/ds/symlink/msp430g2252.pdf)
    [symlink/msp430g2252.pdf](https://www.ti.com/lit/ds/symlink/msp430g2252.pdf).

    - <span id="page-11-27"></span>[20] Texas Instruments. bq25570 nano power boost
    charger and buck converter for energy harvester powered applications, March 2019.
    <https://www.ti.com/lit/ds/symlink/bq25570.pdf>.

    - <span id="page-11-21"></span>[21] Texas Instruments. Lm66100 5.5-v, 1.5-a 79-milliohm,
    low iq ideal diode with input polarity protection, June 2019. [https://www.ti.com/](https://www.ti.com/lit/ds/symlink/lm66100.pdf)
    [lit/ds/symlink/lm66100.pdf](https://www.ti.com/lit/ds/symlink/lm66100.pdf).

    - <span id="page-11-22"></span>[22] Texas Instruments. MSP430FR599x, MSP430FR596x
    Mixed-Signal Microcontrollers, January 2021. [https://www.ti.com/lit/ds/symlink/](https://www.ti.com/lit/ds/symlink/msp430fr5994.pdf)
    [msp430fr5994.pdf](https://www.ti.com/lit/ds/symlink/msp430fr5994.pdf).

    - <span id="page-11-5"></span>[23] Joseph Kahn, Randy Katz, and Kristofer Pister.
    Next Century Challenges: Mobile Networking for "Smart Dust". In Conference on
    Mobile Computing and Networking (MobiCom), 1999.

    - <span id="page-11-12"></span>[24] Kemet. Supercapacitors fm series, July 2020.
    [https://www.mouser.](https://www.mouser.com/datasheet/2/212/1/KEM_S6012_FM-1103835.pdf)
    [com/datasheet/2/212/1/KEM\\_S6012\\_FM-1103835.pdf](https://www.mouser.com/datasheet/2/212/1/KEM_S6012_FM-1103835.pdf).

    - <span id="page-11-14"></span>[25] Silicon Labs. EFM32 Gecko Family EFM32WG Data
    Sheet, December 2021. [https://www.silabs.com/documents/public/data-sheets/](https://www.silabs.com/documents/public/data-sheets/efm32wg-datasheet.pdf)
    [efm32wg-datasheet.pdf](https://www.silabs.com/documents/public/data-sheets/efm32wg-datasheet.pdf).

    - <span id="page-11-9"></span>[26] Brandon Lucia and Benjamin Ransford. A simpler,
    safer programming and execution model for intermittent systems. In Conference
    on Programming Language Design and Implementation, PLDI, pages 575–585, 2015.

    - <span id="page-11-7"></span>[27] K. Ma, Y. Zheng, S. Li, K. Swaminathan, X.
    Li, Y. Liu, J. Sampson, Y. Xie, and V. Narayanan. Architecture exploration for
    ambient energy harvesting nonvolatile processors. In IEEE International Symposium
    on High Performance Computer Architecture, HPCA, pages 526–537, Feb 2015.

    - <span id="page-11-10"></span>[28] Kiwan Maeng, Alexei Colin, and Brandon Lucia.
    Alpaca: Intermittent execution without checkpoints. In International Conference
    on Object-Oriented Programming, Systems, Languages, and Applications, OOPSLA,
    pages 96:1–96:30, October 2017.

    - <span id="page-11-11"></span>[29] Kiwan Maeng and Brandon Lucia. Adaptive dynamic
    checkpointing for safe efficient intermittent computing. In USENIX Conference
    on Operating Systems Design and Implementation, OSDI, pages 129–144, November
    2018.

    - <span id="page-12-8"></span>[30] Kiwan Maeng and Brandon Lucia. Adaptive low-overhead
    scheduling for periodic and reactive intermittent execution. In Proceedings of
    the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation,
    PLDI 2020, page 1005–1021, New York, NY, USA, 2020. Association for Computing
    Machinery.

    - <span id="page-12-15"></span>[31] Microsemi. ZL70251 Ultra-Low-Power Sub-GHz
    RF Transceiver, March 2018. [https://www.microsemi.com/document-portal/doc\\_view/](https://www.microsemi.com/document-portal/doc_view/132900-zl70251-datasheet)
    [132900-zl70251-datasheet](https://www.microsemi.com/document-portal/doc_view/132900-zl70251-datasheet).

    - <span id="page-12-12"></span>[32] Murata. GRM31CR60J227ME11L Chip Monolithic
    Ceramic Capacitor for General. [https://search.murata.co.jp/Ceramy/image/img/A01X/](https://search.murata.co.jp/Ceramy/image/img/A01X/G101/ENG/GRM31CR60J227ME11-01.pdf)
    [G101/ENG/GRM31CR60J227ME11-01.pdf](https://search.murata.co.jp/Ceramy/image/img/A01X/G101/ENG/GRM31CR60J227ME11-01.pdf).

    - <span id="page-12-13"></span>[33] Murata. Supercapacitors FM Series, July 2020.
    [https://www.mouser.](https://www.mouser.com/datasheet/2/212/1/KEM_S6012_FM-1103835.pdf)
    [com/datasheet/2/212/1/KEM\\_S6012\\_FM-1103835.pdf](https://www.mouser.com/datasheet/2/212/1/KEM_S6012_FM-1103835.pdf).

    - <span id="page-12-1"></span>[34] Phillip Nadeua, Dina El-Damaj, Deal Glettig,
    Yong Lin Kong, Stacy Mo, Cody Cleveland, Lucas Booth, Niclas Roxhed, Robert Langer,
    Anantha P. Chandrakasan, and Giovanni Traverso. Prolonged energy harvesting for
    ingestible devices. Nature Biomedical Engineering, 1(0022), Feb 2017.

    - <span id="page-12-14"></span>[35] Nichicon. ALUMINUM ELECTROLYTIC CAPACITORS.
    [https://www.](https://www.nichicon.co.jp/english/products/pdfs/e-kl.pdf) [nichicon.co.jp/english/products/pdfs/e-kl.pdf](https://www.nichicon.co.jp/english/products/pdfs/e-kl.pdf).

    - <span id="page-12-0"></span>[36] Panasonic. Panasonic coin type lithium batteries,
    August 2005. [https://](https://datasheet.octopart.com/CR1616-Panasonic-datasheet-9751741.pdf)
    [datasheet.octopart.com/CR1616-Panasonic-datasheet-9751741.pdf](https://datasheet.octopart.com/CR1616-Panasonic-datasheet-9751741.pdf).

    - <span id="page-12-18"></span>[37] Powercast. P2110B 915 MHz RF Powerharvester
    Receiver, December 2016. [https://www.powercastco.com/wp-content/uploads/2016/12/](https://www.powercastco.com/wp-content/uploads/2016/12/P2110B-Datasheet-Rev-3.pdf)
    [P2110B-Datasheet-Rev-3.pdf](https://www.powercastco.com/wp-content/uploads/2016/12/P2110B-Datasheet-Rev-3.pdf).

    - <span id="page-12-19"></span>[38] Powercast. TX91501B – 915 MHz Powercaster
    Transmitter, October 2019. [https://www.powercastco.com/wp-content/uploads/2019/10/](https://www.powercastco.com/wp-content/uploads/2019/10/User-Manual-TX-915-01B-Rev-A-1.pdf)
    [User-Manual-TX-915-01B-Rev-A-1.pdf](https://www.powercastco.com/wp-content/uploads/2019/10/User-Manual-TX-915-01B-Rev-A-1.pdf).

    - <span id="page-12-17"></span>[39] Powercast. 915 mhz dipole antenna datasheet,
    November 2020. [https://www.powercastco.com/wp-content/uploads/2020/11/](https://www.powercastco.com/wp-content/uploads/2020/11/DA-915-01-Antenna-Datasheet_new_web.pdf)
    [DA-915-01-Antenna-Datasheet\\_new\\_web.pdf](https://www.powercastco.com/wp-content/uploads/2020/11/DA-915-01-Antenna-Datasheet_new_web.pdf).

    - <span id="page-12-4"></span>[40] Benjamin Ransford, Jacob Sorber, and Kevin
    Fu. Mementos: System Support for Long-Running Computation on RFID-Scale Devices.
    In Architectural Support for Programming Languages and Operating Systems (ASPLOS),
    2011.

    - <span id="page-12-5"></span>[41] Henry Sodano, Gyuhae Park, and Daniel Inman.
    Estimation of Electric Charge Output for Piezoelectric Energy Harvesting. In Strain,
    Volume 40, 2004.

    - <span id="page-12-10"></span>[42] ST. Small signal schottky diode, October 2001.
    [https://www.st.](https://www.st.com/content/ccc/resource/technical/document/datasheet/group1/11/76/e4/a3/df/07/49/14/CD00000767/files/CD00000767.pdf/jcr:content/translations/en.CD00000767.pdf)
    [com/content/ccc/resource/technical/document/datasheet/group1/](https://www.st.com/content/ccc/resource/technical/document/datasheet/group1/11/76/e4/a3/df/07/49/14/CD00000767/files/CD00000767.pdf/jcr:content/translations/en.CD00000767.pdf)
    [11/76/e4/a3/df/07/49/14/CD00000767/files/CD00000767.pdf/jcr:](https://www.st.com/content/ccc/resource/technical/document/datasheet/group1/11/76/e4/a3/df/07/49/14/CD00000767/files/CD00000767.pdf/jcr:content/translations/en.CD00000767.pdf)
    [content/translations/en.CD00000767.pdf](https://www.st.com/content/ccc/resource/technical/document/datasheet/group1/11/76/e4/a3/df/07/49/14/CD00000767/files/CD00000767.pdf/jcr:content/translations/en.CD00000767.pdf).

    - <span id="page-12-16"></span>[43] Voltaic. Voltaic systems p121 r1g, April 2020.
    [https://voltaicsystems.](https://voltaicsystems.com/content/Voltaic Systems P121
    R1G.pdf) [com/content/VoltaicSystemsP121R1G.pdf](https://voltaicsystems.com/content/Voltaic
    Systems P121 R1G.pdf).

    - <span id="page-12-6"></span>[44] Harrison Williams, Xun Jian, and Matthew Hicks.
    Forget failure: Exploiting sram data remanence for low-overhead intermittent computation.
    In Proceedings of the Twenty-Fifth International Conference on Architectural Support
    for Programming Languages and Operating Systems, ASPLOS ''20, page 69–84, New
    York, NY, USA, 2020. Association for Computing Machinery.

    - <span id="page-12-11"></span>[45] Harrison Williams, Michael Moukarzel, and
    Matthew Hicks. Failure sentinels: Ubiquitous just-in-time intermittent computation
    via low-cost hardware support for voltage monitoring. In International Symposium
    on Computer Architecture, ISCA, pages 665–678, 2021.

    - <span id="page-12-7"></span>[46] Joel Van Der Woude and Matthew Hicks. Intermittent
    computation without hardware support or programmer intervention. In USENIX Symposium
    on Operating Systems Design and Implementation, OSDI, pages 17–32, November 2016.

    - <span id="page-12-2"></span>[47] X. Wu, I. Lee, Q. Dong, K. Yang, D. Kim, J.
    Wang, Y. Peng, Y. Zhang, M. Saliganc, M. Yasuda, K. Kumeno, F. Ohno, S. Miyoshi,
    M. Kawaminami, D. Sylvester, and D. Blaauw. A 0.04mm316nw wireless and batteryless
    sensor system with integrated cortex-m0+ processor and optical communication for
    cellular temperature measurement. In 2018 IEEE Symposium on VLSI Circuits, pages
    191–192, 2018.

    - <span id="page-12-9"></span>[48] Fan Yang, Ashok Samraj Thangarajan, Wouter
    Joosen, Christophe Huygens, Danny Hughes, Gowri Sankar Ramachandran, and Bhaskar
    Krishnamachari. Astar: Sustainable battery free energy harvesting for heterogeneous
    platforms and dynamic environments. In Proceedings of the 2019 International Conference
    on Embedded Wireless Systems and Networks, EWSN ''19, page 71–82, USA, 2019. Junction
    Publishing.

    - <span id="page-12-3"></span>[49] Fan Yang, Ashok Samraj Thangarajan, Sam Michiels,
    Wouter Joosen, and Danny Hughes. Morphy: Software defined charge storage for the
    iot. In Proceedings of the 19th ACM Conference on Embedded Networked Sensor Systems,
    SenSys ''21, page 248–260, New York, NY, USA, 2021. Association for Computing
    Machinery.'
  decisions:
    evaluation_prompt: 'Qualified. Reason: The paper includes an evaluation section
      that discusses the performance of the proposed REACT system on real-world platforms.
      It provides evidence of empirical evaluation through performance metrics, comparisons
      with baseline systems, and results from experiments conducted under various
      power conditions. The paper also includes performance tables and figures that
      illustrate the results of these evaluations.'
    related_work_prompt: '- Qualified. Reason: The paper meaningfully engages with
      prior research throughout its text. It includes a "Background and Related Work"
      section that discusses previous work on energy harvesting systems, buffer capacity,
      power-responsive performance scaling, multiplexed energy storage, and unified
      dynamic buffering. The paper also compares its proposed method, REACT, to existing
      solutions like Morphy, and cites numerous academic sources to provide context
      and support for its claims.'
    novelty_prompt: 'Qualified. Reason: The paper proposes a novel method called REACT,
      which is a responsive buffering circuit that dynamically varies capacitance
      to optimize energy capture and usage in batteryless systems. It claims to improve
      upon existing static and dynamic buffering systems by eliminating the tradeoff
      between responsiveness and capacity, and it provides empirical evidence of its
      performance benefits over previous approaches. The paper clearly states its
      contributions, including the design and evaluation of REACT, making it a novel
      contribution to the field.'
    review_only_prompt: 'Qualified. Reason: The paper introduces a new contribution,
      REACT, a dynamic energy buffering system for batteryless systems, and evaluates
      its performance against existing systems, rather than primarily summarizing
      existing work.'
  topics:
    main_topic: Embedded & Real-time Systems
    main_topic_reasoning: The paper focuses on energy-adaptive buffering solutions
      for batteryless energy harvesting systems, which are crucial in embedded and
      real-time applications where efficient energy management is key.
    secondary_topic: Computer Networks
    secondary_topic_reasoning: While not the primary focus, the energy adaptability
      and performance improvements discussed can be beneficial in networked environments
      where such systems are deployed.
    main_topic_sub: Other
    secondary_topic_sub: Other
- title: 'TOP: Towards Open & Predictable Heterogeneous SoCs'
  abstract: 'Ensuring predictability in modern real-time Systems-on-Chip (SoCs) is
    an

    increasingly critical concern for many application domains such as automotive,

    robotics, and industrial automation. An effective approach involves the

    modeling and development of hardware components, such as interconnects and

    shared memory resources, to evaluate or enforce their deterministic behavior.

    Unfortunately, these IPs are often closed-source, and these studies are limited

    to the single modules that must later be integrated with third-party IPs in

    more complex SoCs, hindering the precision and scope of modeling and

    compromising the overall predictability. With the coming-of-age of open-source

    instruction set architectures (RISC-V) and hardware, major opportunities for

    changing this status quo are emerging. This study introduces an innovative

    methodology for modeling and analyzing State-of-the-Art (SoA) open-source SoCs

    for low-power cyber-physical systems. Our approach models and analyzes the

    entire set of open-source IPs within these SoCs and then provides a

    comprehensive analysis of the entire architecture. We validate this methodology

    on a sample heterogenous low-power RISC-V architecture through RTL simulation

    and FPGA implementation, minimizing pessimism in bounding the service time of

    transactions crossing the architecture between 28% and 1%, which is

    considerably lower when compared to similar SoA works.'
  url: http://arxiv.org/abs/2401.15639v2
  keywords: '*—Heterogeneous SoC, Cyber-Physical-Systems, Timing Predictable Architectures,
    Open-Source Hardware.'
  document: "# TOP: Towards Open & Predictable Heterogeneous SoCs\n\nLuca Valente,\
    \ Francesco Restuccia, Davide Rossi, *Member, IEEE* Ryan Kastner, *Fellow, IEEE*\
    \ Luca Benini, *Fellow, IEEE*\n\n**Abstract**—Ensuring predictability in modern\
    \ real-time Systems-on-Chip (SoCs) is an increasingly critical concern for many\
    \ application domains such as automotive, robotics, and industrial automation.\
    \ An effective approach involves the modeling and development of hardware components,\
    \ such as interconnects and shared memory resources, to evaluate or enforce their\
    \ deterministic behavior. Unfortunately, these IPs are often closed-source, and\
    \ these studies are limited to the single modules that must later be integrated\
    \ with third-party IPs in more complex SoCs, hindering the precision and scope\
    \ of modeling and compromising the overall predictability. With the coming-of-age\
    \ of open-source instruction set architectures (RISC-V) and hardware, major opportunities\
    \ for changing this status quo are emerging. This study introduces an innovative\
    \ methodology for modeling and analyzing State-of-the-Art (SoA) open-source SoCs\
    \ for low-power cyber-physical systems. Our approach models and analyzes the entire\
    \ set of open-source IPs within these SoCs and then provides a comprehensive analysis\
    \ of the entire architecture. We validate this methodology on a sample heterogenous\
    \ low-power RISC-V architecture through RTL simulation and FPGA implementation,\
    \ minimizing pessimism in bounding the service time of transactions crossing the\
    \ architecture between 28% and 1%, which is considerably lower when compared to\
    \ similar SoA works.\n\n✦\n\n**Index Terms**—Heterogeneous SoC, Cyber-Physical-Systems,\
    \ Timing Predictable Architectures, Open-Source Hardware.\n\n# **1 INTRODUCTION**\n\
    \nThe exponential growth of cyber-physical systems (CPS) (e.g., self-driving cars,\
    \ autonomous robots, ...) and related applications has been fueled by the increase\
    \ in computational capabilities of heterogeneous low-power Systems-on-Chip (SoCs).\
    \ These SoCs are complex computing platforms composed of a set of different hardware\
    \ computing units (e.g., CPUs, hardware accelerators), each tailored to a specific\
    \ target application, sharing a set of resources (memory, sensors) through interconnects\
    \ [\\[1\\]](#page-12-0)–[\\[5\\]](#page-12-1). While integrating multiple computing\
    \ units on the same platform has enabled efficient scale-up of computational capabilities,\
    \ it also poses significant challenges when it comes to assessing their *timing\
    \ predictability*, which is a requirement for CPSs dealing with real-time and\
    \ safety-critical applications: the primary challenge arises from resource contentions\
    \ that emerge when multiple active agents within the SoC must access the same\
    \ shared resources [\\[1\\]](#page-12-0)–[\\[7\\]](#page-12-2).\n\nNumerous research\
    \ efforts have focused on enhancing the timing predictability of heterogeneous\
    \ Systems-on-Chip (SoCs). This includes safely upper bounding execution times\
    \ for data transfers [\\[8\\]](#page-13-0)–[\\[10\\]](#page-13-1) or the deadline\
    \ miss ratio for critical tasks [\\[1\\]](#page-12-0)–[\\[3\\]](#page-12-3), with\
    \ the smallest possible pessimism. These efforts have predominantly focused on\
    \ modeling and analyzing commercial DDR protocols [\\[8\\]](#page-13-0), memory\
    \ IPs [\\[11\\]](#page-13-2), and memory controllers [\\[12\\]](#page-13-3), but\
    \ also predictable interconnects [\\[1\\]](#page-12-0), [\\[4\\]](#page-12-4)\
    \ and on-chip communication protocols [\\[13\\]](#page-13-4). Regrettably, despite\
    \ their value, these studies are scattered, with each one focusing on only one\
    \ of these resources at a time, resulting in being overly pessimistic [\\[5\\\
    ]](#page-12-1).\n\nModeling and analysis of communication protocols are done speculatively\
    \ on abstract models, thus reducing their real-world applicability. Recent works\
    \ for modeling and analysis of IPs (memories, memory controllers, interconnect,\
    \ etc.) have to address the unavailability of cycle-accurate RTL descriptions.\
    \ Many of these IPs are either entirely closed-source [\\[8\\]](#page-13-0) or\
    \ provide loosely-timed behavioral models [\\[5\\]](#page-12-1), [\\[12\\]](#page-13-3)\
    \ or just µarchitectural descriptions [\\[1\\]](#page-12-0), [\\[3\\]](#page-12-3),\
    \ [\\[4\\]](#page-12-4). In essence, the fragmented and proprietary nature of\
    \ commercial and research IPs restricts studies to the particular IP, greatly\
    \ reducing the accuracy achievable through systemlevel analysis. For example,\
    \ Restuccia et al. in [\\[9\\]](#page-13-5) bound the access times of multiple\
    \ initiators on FPGA reading and writing from/to the shared DDR memory. The proposed\
    \ upper bounds' pessimism is between 50% and 90%: even though they finely modeled\
    \ and analyzed the proprietary interconnect, the authors did not have access to\
    \ its RTL nor to the memory controller and IP. The same applies to Ditty [\\[10\\\
    ]](#page-13-1), which is a predictable cache coherence mechanism. In Ditty, even\
    \ though the caches' timing is finely modeled, the overall execution time can\
    \ be up to 3× bigger than the theoretical upper bounds, as the authors did not\
    \ model other components. Another example is AXI-ICRT [\\[1\\]](#page-12-0), an\
    \ advanced AXI interconnect with a sophisticated scheduler which allows transaction\
    \ prioritization based on importance. While proposing a highly advanced interconnect\
    \ with a\n\n<sup>•</sup> *Luca Valente, Luca Benini, and Davide Rossi are with\
    \ the Department of Electrical, Electronic and Information Engineering, University\
    \ of Bologna, 40136 Bologna, Italy. Luca Benini is also with the Integrated Systems\
    \ Laboratory (IIS), ETH Z ¨urich, 8092 Z ¨urich, Switzerland.*\n\n<sup>•</sup>\
    \ *Francesco Restuccia and Ryan Kastner are with the Computer Science and Engineering,\
    \ University of California at San Diego, San Diego, CA 92093 USA.*\n\n*This work\
    \ was supported by Technology Innovation Institute, Secure Systems Research Center,\
    \ Abu Dhabi, UAE, PO Box: 9639, by the Spoke 1 on Future HPC of the Italian Research\
    \ Center on High-Performance Computing, Big Data and Quantum Computing (ICSC)\
    \ funded by MUR Mission 4 - Next Generation EU, and by the European Project EuroHPC\
    \ JU The European Pilot (g.a. 101034126), and by KDT TRISTAN project (g.a.101095947).*\n\
    \n<span id=\"page-1-0\"></span>![](_page_1_Figure_0.jpeg)\n\nFig. 1: Proposed\
    \ methodology.\n\ntightly coupled model, the authors do not extend the model to\
    \ the other components of the SoC, even when assessing the deadline miss ratio\
    \ and benchmarking the architecture.\n\nThe emergence of open-source hardware\
    \ creates a major opportunity for building accurate end-to-end models for realtime\
    \ analysis of cutting-edge heterogeneous low-power SoCs [\\[14\\]](#page-13-6)–[\\\
    [16\\]](#page-13-7): the openness of the IPs allows for cycle-accurate analysis\
    \ of the whole architecture from the interconnects to the shared resources. Yet,\
    \ investigations and successful demonstrations in this direction are still scarce,\
    \ primarily because open hardware has only very recently reached the maturity\
    \ and completeness levels required to build full heterogeneous SoCs [\\[17\\]](#page-13-8).\
    \ In this context, this is the first work to bridge the gap between open-source\
    \ hardware and timing analysis, demonstrating a methodology that successfully\
    \ exploits the availability of the source code to provide finegrained upper bounds\
    \ of the system-level data transfers. We leverage a set of open-source IPs from\
    \ the PULP family, one of the most popular open-hardware platforms proposed by\
    \ the research community [\\[14\\]](#page-13-6), [\\[18\\]](#page-13-9).\n\nFigure\
    \ [1](#page-1-0) shows the proposed methodology, highlighting the novel contributions\
    \ in yellow. It consists of (i) a model for standalone IPs composing modern heterogeneous\
    \ lowpower SoCs, (ii) a static analysis of the RTL code of such components, and\
    \ (iii) a compositional mathematical analysis of the whole system to upper bound\
    \ the response time of the interactions between managers (initiators) and shared\
    \ subordinates (targets), considering the maximum interference generated by the\
    \ interfering managers. Figure [1](#page-1-0) highlights the differences between\
    \ the proposed methodology and previous studies also based on a static and compositional\
    \ approach [\\[5\\]](#page-12-1), [\\[7\\]](#page-12-2), [\\[9\\]](#page-13-5).\
    \ While previous works typically focus on one IP at a time [\\[9\\]](#page-13-5),\
    \ or rely on loosely-timed models [\\[5\\]](#page-12-1), thereby limiting the\
    \ overall accuracy, our approach is the first to model and analyze all the IPs\
    \ directly from the RTL source code to build a holistic system-level analysis.\
    \ This limits the proposed upper bounds' pessimism between 28% and just 1%, in\
    \ isolation and under interference, which is considerably lower when compared\
    \ to similar SoA works for closedsource or loosely-timed platforms [\\[1\\]](#page-12-0)–[\\\
    [4\\]](#page-12-4), [\\[8\\]](#page-13-0), [\\[10\\]](#page-13-1), as better detailed\
    \ in Section [7.](#page-11-0) We demonstrate our methodology on a completely open-source\
    \ prototype of a heterogeneous lowpower open-source SoC for embedded systems composed\
    \ of a Linux-capable host core, a parallel accelerator, a set of IOs, and on-chip\
    \ and off-chip memories. the system-level analysis of the architecture. Finally,\
    \ Section 2-banks 512KiB SPM\n\nThe manuscript is organized as follows: Section\
    \ [2](#page-1-1) presents the target open-source RISC-V-based SoC architecture,\
    \ and Section [3](#page-2-0) discusses the model we apply to its different components.\
    \ Section [4](#page-3-0) analyzes the components to specialize the generic model\
    \ to each of them, and Section [5](#page-7-0) provides\n\n<span id=\"page-1-2\"\
    ></span>![](_page_1_Figure_6.jpeg)\n\nFig. 2: Sample architecture.\n\n[6](#page-9-0)\
    \ validates the results with cycle-accurate experiments (on simulation and FPGA),\
    \ Section [7](#page-11-0) compares this work with the SoA. Section [8](#page-12-5)\
    \ concludes the manuscript.\n\n# <span id=\"page-1-1\"></span>**2 ARCHITECTURE**\n\
    \nFig. [2](#page-1-2) shows the architectural template we target. It also shows\
    \ the four classes of hardware modules we identify in the architecture under analysis,\
    \ namely (i) *controllers*, (ii) the main *crossbar*, (iii) *bridges*, and (iv)\
    \ *peripherals*, which we model in the next Section. The architecture leverages\
    \ a set of fully open-source PULP IPs [\\[18\\]](#page-13-9). It is based on Cheshire\
    \ [\\[15\\]](#page-13-10), an open-source host platform consisting of an RV64\
    \ Linuxcapable CPU, a set of commodity IOs (SPI, SDIO, UART, ...), and an AXI-based\
    \ crossbar with a configurable number of subordinate and manager ports for easy\
    \ integration of accelerators and resources. Our platform includes a parallel\
    \ accelerator and a low-power lightweight HyperBUS memory controller [\\[19\\\
    ]](#page-13-11), connected to the crossbar.\n\nThe host CPU is CVA6 [\\[20\\]](#page-13-12),\
    \ which is a six stages, singleissue, in-order, 64-bit Linux-capable RISC-V core,\
    \ supporting the RV64GC ISA variant, SV39 virtual memory with a dedicated Memory\
    \ Management Unit (MMU), three levels of privilege (Machine, Supervisor, User),\
    \ and PMP [\\[21\\]](#page-13-13). CVA6 features private L1 instruction and caches,\
    \ operating in parallel, with the latter being able to issue multiple transactions.\
    \ When needed, CVA6 can offload computationintensive tasks to the parallel hardware\
    \ accelerator, the socalled PULP cluster [\\[22\\]](#page-13-14). It is built\
    \ around 8 CV32E4-based cores [\\[23\\]](#page-13-15) sharing 16×8 kB SRAM banks,\
    \ composing a 128 kB L1 Scratchpad Memory (SPM). The cluster features a DMA to\
    \ perform data transfers between the private L1SPM and the main memory: data movement\
    \ is performed via softwareprogrammed DMA transfers. Once the data are available\
    \ inside the L1SPM, the accelerator starts the computation.\n\nCVA6 and the cluster\
    \ are the managers of the systems connected to the main AXI crossbar [\\[24\\\
    ]](#page-13-16), which routes their requests to the desired subordinates according\
    \ to the memory map. A manager can access any subordinate in the system. The main\
    \ subordinates of the systems are, respectively, (i) the on-chip SRAM memory,\
    \ (ii) the IO subsystem, and (iii) the off-chip main memory with a tightly coupled\
    \ Last Level Cache (LLC). The on-chip memory is used for low-latency, high-bandwidth\
    \ data storage. The APB subsystem is used to communicate with off-chip sensors\
    \ or memories through the\n\ncommodity IOs. The off-chip main memory is where\
    \ the code and the shared data are stored. Differently from high-end embedded\
    \ systems relying on relatively power-hungry and expensive DDR3/4/5 memories,\
    \ the platform under analysis adopts HyperRAMs as off-chip main memory, which\
    \ are fully-digital low-power small-area DRAMs with less than 14 IO pins and that\
    \ provide enough capacity to boot Linux [\\[16\\]](#page-13-7) and bandwidth for\
    \ IoT applications [\\[19\\]](#page-13-11), [\\[25\\]](#page-13-17).\n\n# <span\
    \ id=\"page-2-0\"></span>**3 MODEL**\n\nThis section presents the model we construct\
    \ for the different components of our SoC. Our aim is to propose a general model\
    \ that describes the characteristics of the components and that can be re-targeted\
    \ to different IPs and novel architectures, regardless of the number of integrated\
    \ controllers and peripherals. This work is also an effort to provide base support\
    \ to stimulate further studies in predictability improvements and analysis for\
    \ open hardware architectures.\n\n#### **3.1 Communication model**\n\nWe identify\
    \ four classes of hardware modules in the architecture under analysis, shown in\
    \ Fig. [2,](#page-1-2) namely (i) *controllers*, (ii) the main *crossbar*, (iii)\
    \ *bridges*, and (iv) *peripherals*. As the AXI standard is the main communication\
    \ standard used to implement non-coherent on-chip communications [\\[24\\]](#page-13-16),\
    \ we discuss here its main features. It defines a managersubordinate interface\
    \ enabling simultaneous, bi-directional data exchange and multiple outstanding\
    \ transactions. Fig. [3](#page-2-1) shows the AXI channel architecture and information\
    \ flow. Bus transactions are initiated by a *controller* (exporting a manager\
    \ interface), submitting a transaction request to read/write data to/from a subordinate\
    \ interface through AR or AW channels, respectively. A request describes the starting\
    \ target address and a *burst length*. After the request phase, in case of a read,\
    \ data are transmitted through the R channel. In case of a write, data are provided\
    \ by the *controller* to the target *peripheral* through the W channel. Upon completing\
    \ a write transaction, the *peripheral* also sends a beat on the B channel to\
    \ acknowledge the transaction's completion. For multiple in-flight write transactions,\
    \ the standard enforces strict in-order access to the W channel: the data on the\
    \ W channel must be propagated in the same order as the AW channel requests. Even\
    \ though the standard does not require it, many commercial and open-source platforms\
    \ apply the same policy for reads, typically to limit the system's overall complexity,\
    \ as reported in their documentation [\\[26\\]](#page-13-18), [\\[27\\]](#page-13-19).\n\
    \n#### **3.2 Controller model**\n\n*Controllers* have an active role on the bus.\
    \ Each *controller* exports an AXI manager interface, through which it initiates\
    \ requests for bus transactions directed to the *peripherals*. A generic *controller*\
    \ C<sup>i</sup> can be described through two parameters: the maximum number of\
    \ outstanding read/write transactions that it can issue in parallel, denoted with\
    \ ϕ C<sup>i</sup> R/W , and their relative burst length β<sup>i</sup> . While\
    \ our model and analysis can be applied to a generic architecture, the system\
    \ under analysis features as *controllers* a CVA6 core [\\[20\\]](#page-13-12)\
    \ and a cluster accelerator [\\[22\\]](#page-13-14) (see Section [2\\)](#page-1-1).\
    \ Bus transactions issued by the cluster interfere with those issued by CVA6\n\
    \n<span id=\"page-2-1\"></span>\n\n| READ TRANSACTION                        \
    \ |                     |  |  |  |  |\n|------------------------------------------|---------------------|--|--|--|--|\n\
    | 1 AR (ADDR+ LEN)                         | SUBORDINATE         |  |  |  |  |\n\
    | 2 R (DATA + COMPL.)                      | INTERFACE           |  |  |  |  |\n\
    | WRITE TRANSACTION<br>1<br>AW (ADDR+ LEN) |                     |  |  |  |  |\n\
    | W (DATA)<br>2                            | SUBORDINATE         |  |  |  |  |\n\
    |                                          | INTERFACE           |  |  |  |  |\n\
    |                                          | 3<br>B (COMPLETION) |  |  |  |  |\n\
    \nFig. 3: AXI Channel architecture\n\nand vice-versa. CVA6 is assumed to compute\
    \ a critical periodic workload, running on top of a Real-time Operating System\
    \ (RTOS). The PULP cluster executes computationintensive tasks and issues bus\
    \ transactions through its DMA. Contention internal to the PULP cluster has been\
    \ profiled in detail in [\\[28\\]](#page-13-20). However, our analysis provides\
    \ the worstcase data transfer time in accessing the shared *peripherals* to support\
    \ the safe scheduling and execution of critical tasks within their deadline. We\
    \ specifically focus on interference in accessing the shared resources. Modeling\
    \ the internal effects of *controllers*, such as pipeline stalls in the core or\
    \ contention within the accelerator, is beyond the scope of this work.\n\n####\
    \ <span id=\"page-2-2\"></span>**3.3 Peripheral model**\n\n*Peripherals* export\
    \ a *subordinate* interface through which they receive and serve the bus transactions.\
    \ The *peripherals* deployed in the system are heterogeneous. Nonetheless, our\
    \ model offers a set of parameters representative of a generic peripheral, and\
    \ it is not tied to a specific communication protocol. It works as the baseline\
    \ for the analysis of any *peripheral* deployed in the system under analysis.\
    \ The generic *peripheral* P<sup>j</sup> is characterized with two sets of parameters:\
    \ (i) the maximum number of supported outstanding reads (χ P<sup>j</sup> <sup>R</sup>\
    \ ) and write (χ P<sup>j</sup> <sup>W</sup> ) transactions; (ii) the maximum number\
    \ of cycles incurred from the reception of the request to its completion, for\
    \ a read (d P<sup>j</sup> <sup>R</sup> ) and a write (d P<sup>j</sup> <sup>W</sup>\
    \ ) transaction in isolation. d P<sup>j</sup> <sup>R</sup> and d P<sup>j</sup>\
    \ <sup>W</sup> are composed of two contributions: (i) the *data time*, defined\
    \ as the time required for the *peripheral* to send or receive one word of data\
    \ (tDATA) multiplied by the burst length of the transaction in service (βi) and\
    \ (ii) the *control overhead* tCTRL, defined as the maximum time elapsing between\
    \ accepting the request and the availability of the first word of data (reads)\
    \ or availability to receive data (writes). From the previous considerations,\
    \ d P<sup>j</sup> R/W = t P<sup>j</sup> CTRL + t P<sup>j</sup> DATA · β. We define\
    \ two extra parameters ρ <sup>P</sup><sup>j</sup> and θ P<sup>j</sup> . The first\
    \ indicates the level of pipelining in serving multiple transactions. ρ <sup>P</sup><sup>j</sup>\
    \ = 1 means that each stage of P<sup>j</sup> does not stall the previous, and\
    \ transactions are served in a pipelined fashion, while ρ <sup>P</sup><sup>j</sup>\
    \ = 0 indicates that no pipeline is implemented. θ <sup>P</sup><sup>j</sup> =\
    \ 0 indicates that read and write transactions interfere with each other. θ <sup>P</sup><sup>j</sup>\
    \ = 1 indicates that read and write transactions can be handled in parallel by\
    \ P<sup>j</sup> .\n\n#### <span id=\"page-2-3\"></span>**3.4 Main crossbar model**\n\
    \nWe provide here the model of the main *crossbar*, the routing component enabling\
    \ communication among *controller*s and *peripheral*s. Each *controller* has its\
    \ manager port connected to a subordinate port of the *crossbar*. Each *peripheral*\
    \ has its subordinate port connected to a manager port of the *crossbar*. We model\
    \ the *crossbar* R<sup>0</sup> with two sets of parameters: (i) the\n\nmaximum\
    \ amount of outstanding read and write transactions that a subordinate port can\
    \ accept (χ R<sup>0</sup> <sup>R</sup> and χ R<sup>0</sup> <sup>W</sup> , respectively);\
    \ and (ii) the maximum overall latency introduced by R<sup>0</sup> on each read\
    \ (d R<sup>0</sup> <sup>R</sup> ) and write transaction (d R<sup>0</sup> <sup>W</sup>\
    \ ). d R<sup>0</sup> R and d R<sup>0</sup> <sup>W</sup> are composed of two contributions:\
    \ (i) the overall delay introduced by the *crossbar* on a transaction in isolation\
    \ (tPROP); (ii) the maximum time a request is delayed at the arbitration stage\
    \ due to the contention generated by interfering transactions (t R<sup>0</sup>\
    \ CON). From the previous considerations, the propagation latency is modeled as\
    \ d R<sup>0</sup> R/W = t R<sup>0</sup> PROP + t R<sup>0</sup> CON. Such parameters\
    \ depend on the arbitration policies and routing mechanisms, as we investigate\
    \ in detail in Section [4.](#page-3-0)\n\n#### <span id=\"page-3-1\"></span>**3.5\
    \ Bridge model**\n\nBridges export a single manager interface and a single subordinate\
    \ interface. They perform protocol/clock conversion between a *controller* and\
    \ the *crossbar*. Bridges require a certain number of clock cycles to be crossed\
    \ but do not limit the number of in-flight transactions and do not create any\
    \ contention. We model the bridges with two parameters: the overall maximum delay\
    \ introduced over a whole transaction for (a) read (d Q<sup>j</sup> <sup>R</sup>\
    \ ) and (b) write (d Q<sup>j</sup> <sup>W</sup> ) transactions.\n\n# <span id=\"\
    page-3-0\"></span>**4 ANALYSIS OF THE HARDWARE MODULES**\n\nThis Section aims\
    \ to analyze the worst-case behavior of the *peripherals*, *bridges*, and the\
    \ *crossbar* present in the platform under analysis. Our approach is compositional\
    \ – in this Section, we analyze each hardware component separately, specializing\
    \ in the generic models introduced in Section [3,](#page-2-0) and bounding the\
    \ service times at the IP level in isolation. In the next Section, we provide\
    \ an overall worst-case analysis at the system level, in isolation and under interference.\
    \ We define t P<sup>j</sup> CK as the period period of the clock fed to P<sup>j</sup>\
    \ .\n\n#### <span id=\"page-3-4\"></span>**4.1 AXI CDC FIFO queues**\n\nAXI CDC\
    \ FIFOs are leveraged to perform clock-domain crossing between two AXI-based devices.\
    \ The generic AXI CDC FIFO F<sup>i</sup> is a *bridge*: we apply here the model\
    \ presented in Section [3.5.](#page-3-1) It exports a manager interface and a\
    \ subordinate interface. It is composed of five independent CDC FIFOs, each serving\
    \ as a buffer for an AXI channel, having depth D<sup>i</sup> CDC (design parameter\
    \ for the IP under analysis).\n\n#### *4.1.1 RTL IP structure*\n\nFigure [4](#page-4-0)\
    \ shows the block diagram of a CDC FIFO in the platform under analysis. They are\
    \ structured following established clock domain crossing (CDC) principles [\\\
    [24\\]](#page-13-16). The design is split into two parts, the transmitter (TX)\
    \ and the receiver (RX), having different clock domains. TX and RX interface through\
    \ asynchronous signals, namely a counter for data synchronization (synchronized\
    \ with two-stage Flip-Flops (FFs)) and the payload data signal.\n\n#### *4.1.2\
    \ Delays analysis*\n\nAs mentioned earlier, CDC FIFOs are *bridges*: we apply\
    \ the model presented in Section [3.5.](#page-3-1) The CDC FIFO under analysis\
    \ behaves as follows: TX samples the payload data into an FF. In the following\
    \ cycle, the TX counter is updated. The TX\n\ncrossing the CDC FIFO introduces\
    \ a fixed delay of one clock cycle of the TX domain (t CK) and four clock cycles\
    \ of the RX domain (t RX CK). This means that the delay in crossing the CDC FIFO\
    \ is equal to tCDC(t TX CK, tRX CK) = t TX CK + 4 · t RX CK.We leverage this baseline\
    \ delay to build the overall latency introduced by F<sup>i</sup> , interposed\
    \ between a manager (clocked at t C CK) and a subordinate (clocked at t P CK).\n\
    \n*Read transaction:* A read transaction AR<sup>k</sup> is composed of two phases:\
    \ (i) the address propagation phase and (ii) the data phase. This means that F<sup>i</sup>\
    \ is crossed twice to complete ARk: during phase (i), the manager is on the TX\
    \ side, propagating the request. In phase (ii), the subordinate is on the TX side,\
    \ propagating the data. Hence, the propagation latency is tCDC(t C CK, t<sup>P</sup>\
    \ CK) in phase (i) and tCDC(t P CK, t<sup>C</sup> CK) in phase (ii). Adding them\
    \ together, the propagation latency introduced by F<sup>i</sup> on AR<sup>k</sup>\
    \ is equal to:\n\n<span id=\"page-3-2\"></span>\n$$d\\_R^{\\rm CDC} = t\\_{\\\
    rm CDC}(t\\_{\\rm CK}^C, t\\_{\\rm CK}^P) + t\\_{\\rm CDC}(t\\_{\\rm CK}^P, t\\\
    _{\\rm CK}^C) = 5(t\\_{\\rm CK}^C + t\\_{\\rm CK}^P) \\tag{1}$$\n\n*Write transaction:*\
    \ A write transaction is composed of three phases: (i) an address phase (manager\
    \ on the TX side), (ii) a data phase (manager on the TX side), and (iii) a write\
    \ response phase (subordinate on the TX side). Phases (i) and (ii) happen in parallel\
    \ (see [\\[29\\]](#page-13-21) p. 45). Thus, tCDC(t C CK, t<sup>P</sup> CK) is\
    \ incurred for phases (i) and (ii), and tCDC(t P CK, t<sup>C</sup> CK) for phase\
    \ (iii). The delay introduced by F<sup>i</sup> on AW<sup>k</sup> is equal to the\
    \ delay introduced in Equation [1,](#page-3-2) d CDC <sup>W</sup> = d CDC <sup>R</sup>\
    \ .\n\n#### <span id=\"page-3-3\"></span>**4.2 AXI SRAM scratchpad memory (SPM)**\n\
    \nThe AXI SPM is a high-speed, low-latency memory component used for temporary\
    \ data storage – a block design representation is reported in Figure [5.](#page-4-0)\
    \ The SPM memory is a *peripheral*: we apply here the model presented in Section\
    \ [3.3.](#page-2-2)\n\n#### *4.2.1 RTL IP structure*\n\nThe first stage of the\
    \ SPM architecture is represented by a protocol converter (AXI-SRAM-Interface),\
    \ translating the read and write AXI channels into SRAM-compatible transactions.\
    \ Following the converter, an internal demux directs the SRAM transactions to\
    \ the desired SRAM bank, where the data is stored. Each SRAM bank provides two\
    \ independent SRAM ports, one for reads and one for writes, as from the specification\
    \ of industry-standard SRAM resources [\\[30\\]](#page-13-22).\n\n*The AXI-SRAM-Interface*\
    \ is structured in two submodules, independently managing read and write transactions.\
    \ The first stage of each submodule is a FIFO queue (of depth DSPM FIFO) buffering\
    \ the AXI AW or AR channel, respectively. Each submodule features the logic for\
    \ protocol translation, consisting of (i) saving transaction metadata (starting\
    \ address and length) and (ii) producing the output SRAM requests. For writes,\
    \ the incoming data on the W channel are directly propagated towards the banks.\
    \ The logic operating the protocol conversion generates the address for each W\
    \ beat. For reads, the data coming from the SRAM banks are directly driven on\
    \ the R channel. The logic keeps compliance with the AXI standard, adding the\
    \ last signal or generating write responses when required. *The demux* is fully\
    \ combinatorial\n\n<span id=\"page-4-0\"></span>![](_page_4_Figure_1.jpeg)\n\n\
    Fig. 4: CDC FIFO block diagram.\n\nFig. 5: AXI SPM block diagram\n\n0 1 N-1 ...\n\
    \nDual-Port SRAM Interface\n\nBank 1\n\nAR FIFO AW FIFO ADDR & REQ CTRL ADDR GEN\
    \ & CTRL\n\nAR R AW B W\n\nSRAM Interface\n\n0 1 N-1 ... Bank 0\n\nDual-Port SRAM\
    \ Interface\n\nCONVERTER\n\nDEMUXING\n\nBANKS\n\n![](_page_4_Figure_4.jpeg)\n\n\
    Fig. 6: IO subsystem block diagram.\n\nand selects the target bank according to\
    \ the request's address. *The SRAM banks* are technology-specific macros instantiated\
    \ at design time. Each SRAM bank's port exports an enable signal, an address signal,\
    \ and a signal to determine if a transaction is a read or a write. The SRAM interface\
    \ expects simultaneous propagation of data and commands for writes; for reads,\
    \ the data are sent the cycle following the command.\n\n#### *4.2.2 Delays and\
    \ parallelism analysis*\n\n*AXI-SRAM-Interface:* the FIFOs in the converter are\
    \ only in charge of data buffering – each FIFO introduces a fixed delay of one\
    \ clock cycle (t SPM CK ). After the FIFOs, the control logic requires at most\
    \ one clock cycle (t SPM CK ) to set up the propagation of a burst transaction\
    \ – the direct connection over the W and R channels makes the data streaming in\
    \ a pipeline fashion, adding no further latency. At the end of a write transaction,\
    \ the converter takes two clock cycles (2t SPM CK ) to generate the write response:\
    \ one to acknowledge that the last W beat has been accepted and one to provide\
    \ the B response. The same applies to reads, to generate the AXI last signal.\
    \ Summing up the contributions, the control latency introduced by the AXI-SRAM-Interface\
    \ to each transaction is upper bound by 4t SPM CK for both reads and writes.\n\
    \n*Demux:* The demultiplexing is combinatorial: it connects the transaction to\
    \ the SRAM bank in one clock cycle (t SPM CK ).\n\n*Banks:* As by the definition\
    \ of the SRAM interface [\\[30\\]](#page-13-22), an SRAM bank serves one transaction\
    \ per clock cycle, which makes t SPM DATA,R/W = t SPM CK . For write transactions,\
    \ the protocol guarantees that the SRAM bank samples the data in parallel with\
    \ the request (in the same clock cycle). For read transactions, the data are served\
    \ the clock cycle after the bank samples the request. So, it contributes to t\
    \ SPM CTRL,R with one clock cycle (t SPM CK ). Summing up the contributions, the\
    \ service time of the SPM in isolation is upper bound by:\n\n$$t\\_{\\rm{CTRL},W}^{\\\
    rm{SPM}} = 5 \\cdot t\\_{\\rm{CK}}^{\\rm{SPM}}; t\\_{\\rm{CTRL},R}^{\\rm{SPM}}\
    \ = 6 \\cdot t\\_{\\rm{CK}}^{\\rm{SPM}}; t\\_{\\rm{DATA},R/W}^{\\rm{SPM}} = t\\\
    _{\\rm{CK}}^{\\rm{SPM}}; \\tag{2}$$\n\nConsider now the parallelism supported\
    \ by the SPM. The maximum number of accepted outstanding transactions at the SPM\
    \ χ SPM <sup>R</sup> is defined by the depth DSPM FIFO of the input buffers implemented\
    \ in the AXI-SRAM-Interface. Thus,\n\n$$\n\\chi\\_R^{\\text{SPM}} = \\chi\\_W^{\\\
    text{SPM}} = D\\_{\\text{FIFO}}^{\\text{SPM}} \\tag{3}\n$$\n\nThe *SPM* module\
    \ under analysis is aggressively pipelined, operations are executed in one clock\
    \ cycle, and no stall sources are present in the design. Also, as mentioned earlier,\
    \ read and write transactions do not interfere with each other. From the previous\
    \ considerations, ρ SPM = 1 and θ SPM = 1.\n\n#### <span id=\"page-4-1\"></span>**4.3\
    \ IO Subsystem**\n\nSRAM Interface\n\nBank B-1\n\n0 1 N-1 ...\n\nInterface\n\n\
    The IO subsystem is the *peripheral* in charge of writing/reading data to/from\
    \ the off-chip I/Os. We apply here the model presented in Section [3.3.](#page-2-2)\
    \ It is composed of a set of memory-mapped peripheral registers that are accessed\
    \ through a demux and that manage the datapaths issuing the transactions on the\
    \ I/O interfaces (e.g., SPI, I2C, etc.).\n\n#### *4.3.1 RTL IP structure*\n\n\
    Figure [6](#page-4-0) shows the block diagram of the IO subsystem. It is composed\
    \ of an AXI-REG-Interface, a demux, and a set of registers. The first stage of\
    \ the *AXI-REG-Interface* is composed of two FIFOs (of depth DIO FIFO), buffering\
    \ read and write transactions, respectively. After the FIFOs, a round-robin arbiter\
    \ manages read and write transactions, allowing only one at a time to pass to\
    \ the protocol conversion. Since the IO subsystem is meant for low-power reads\
    \ and writes, registers' transactions share the same set of signals for reads\
    \ and writes and are limited to single-word accesses. For such a reason, the IO\
    \ subsystem does not support burst transactions (requests having β<sup>i</sup>\
    \ > 1 are suppressed). *The demux* stage decodes the request and directs it to\
    \ the proper register destination, where it is finally served as a register read\
    \ or write.\n\n#### *4.3.2 Delays and parallelism analysis*\n\nThe IO subsystem\
    \ is a *peripheral*, thus, we apply the model proposed in Section [3.5.](#page-3-1)\
    \ Considering the maximum service delays, overall, the IO subsystem is composed\
    \ of four stages: (i) the FIFOs, (ii) the protocol conversion, (iii) demultiplexing,\
    \ and (iv) target register access. The first three stages, contributing to the\
    \ control overhead, introduce a fixed delay of one clock cycle (t IO CK) each\
    \ for a total of 3 · t IO CK clock cycles. Consider now stage (iv). In the case\
    \ of a write, the request and the corresponding data are propagated in parallel\
    \ in one clock cycle. In the case of a read, the register provides the data in\
    \ the clock cycle following the request – t IO CTRL requires one extra clock cycle.\
    \ Summing all the contributions, the service time of the I/O subsystem is upper\
    \ bounded by:\n\n$$t\\_{\\rm{CTRL,W}}^{IO} = 3 \\cdot t\\_{\\rm{CK}}^{IO}; \\\
    quad t\\_{\\rm{CTRL,R}}^{IO} = 4 \\cdot t\\_{\\rm{CK}}^{IO}; \\quad t\\_{\\rm{DATA,W/R}}^{IO}\
    \ = t\\_{\\rm{CK}}^{IO} \\tag{4}$$\n\nConsider now the parallelism. Similarly\
    \ to the SPM module, the IO subsystem is capable of buffering up to DIO FIFO of\
    \ each type in its input FIFO queues. Thus, the maximum number of outstanding\
    \ transactions supported by the IO subsystem is equal to:\n\n$$\n\\chi\\_W^{\\\
    rm IO} = \\chi\\_R^{\\rm IO} = D\\_{\\rm FIFO}^{\\rm IO} \\tag{5}\n$$\n\nThe IO\
    \ subsystem serves read and write transactions one at a time, and no pipelining\
    \ is implemented among the different stages. This means that ρ IO = 0 and θ SPM\
    \ = 0.\n\n<span id=\"page-5-0\"></span>![](_page_5_Figure_0.jpeg)\n\nFig. 7: Block\
    \ diagrams of the components of the main memory subsystem. (a) LLC block diagram,\
    \ (b) Transaction control flow diagram, (c) Memory controller block diagram.\n\
    \n#### <span id=\"page-5-1\"></span>**4.4 The main memory subsystem**\n\nThe main\
    \ memory subsystem is a *peripheral*: we apply here the model presented in Section\
    \ [3.3.](#page-2-2) It is composed of three macro submodules: (i) the *AXI Last-level\
    \ Cache (LLC)*; (ii) the *HyperRAM memory controller (HMC)*; and (iii) the *HyperRAM\
    \ memory (HRAM)*. It is based on HyperRAM memories leveraging the HyperBUS protocol\
    \ [\\[25\\]](#page-13-17). HyperRAMs are optimized for low-overhead data storage\
    \ while offering up to 3.2Gbps bandwidth. HyperRAMs expose a low pin count, a\
    \ fully digital 8-bit double-data-rate (DDR) interface used for commands and data.\
    \ HyperRAMs serve transactions in order, one at a time, as required by the protocol\
    \ [\\[25\\]](#page-13-17). While a pure in-order strategy is simpler than those\
    \ deployed by high-end commercial memory controllers, it is important to note\
    \ that these controllers are typically complex closedsource IPs, making detailed\
    \ analysis extremely challenging. Notably, our analysis is the first to explore\
    \ this level of detail. Furthermore, the memory subsystem under analysis has shown\
    \ to be effective in tape-outs of Linux-capable chips [\\[16\\]](#page-13-7).\
    \ We model the service times of a single transaction in case of an LLC hit and\
    \ miss. By doing so, we provide upper bounds that can be leveraged by future studies\
    \ focusing on LLC interference between different *controllers* at the application\
    \ level. For example, advanced cache management studies for real-time applications\
    \ (e.g., cache coloring) could leverage the upper bounds provided here to bound\
    \ overall task execution times.\n\n#### *4.4.1 RTL IP structure*\n\n*The AXI Last-Level\
    \ Cache* is the interface of the memory subsystem with the platform. The LLC under\
    \ analysis has configurable cache line length, defined as LWLLC. Figure [7\\(](#page-5-0)a)\
    \ shows the LLC's block diagram, composed of 5 pipelined units: (i) burst splitter,\
    \ (ii) hit-miss detection, (iii) eviction/refill, (iv) data read/write, and (v)\
    \ data ways. Figure [7\\(](#page-5-0)b) shows how these units cooperate to serve\
    \ the requests. The burst splitter buffers and splits the incoming AXI requests\
    \ into multiple sub-requests that have the same length of the cache line, and\
    \ it calculates the tags of the sub-transactions. A βi-word AXI burst request\
    \ is split internally into ⌈ βi LWLLC ⌉ requests of length LWLLC. The tags are\
    \ the input to the hitmiss detection unit, which analyzes them to determine if\
    \ any sub-request will be a (a) hit or (b) miss. In case (a), the transaction\
    \ is directed to the read/write unit: if it is a (a.i) read, the read response\
    \ is generated and immediately sent through the AXI subordinate port, completing\
    \ the transaction. In the case of a (a.ii) write, the locally cached value is\
    \ updated, and a write response is generated and sent back to the\n\nAXI interface\
    \ to complete the transaction. In case (b), the transaction is submitted to the\
    \ eviction/refill unit. Refill is performed on every miss and consists of issuing\
    \ a read to the memory controller to fetch the missing data and update the data\
    \ way. Eviction is performed when a cache set is full to free the necessary spot\
    \ before a refill. A Least Recently Used (LRU) algorithm is used in the module\
    \ under analysis.\n\n*The HyperRAM memory controller* [\\[31\\]](#page-13-23)\
    \ is depicted in Figure [7\\(](#page-5-0)c). It consists of two tightly coupled\
    \ modules working in two separated frequency domains: (i) the AXI *front-end*\
    \ and (ii) the *back-end* PHY controller. The front-end handles and converts the\
    \ AXI transactions into data packets for the PHY controller; it runs at the same\
    \ clock as the LLC (t HMC CK ). The back-end features a Finite State Machine (FSM)\
    \ to send/receive the data packets and keep compliance with the HyperBUS protocol\
    \ timings and data flow; it runs at the same clock as the HyperRAMs (t HRAM CK\
    \ ). The back-end handles two off-chip HyperRAMs in parallel, configured with\
    \ interleaved addresses. As each HyperRAM arranges data as 16-bit words, the word\
    \ size of the back-end is DWHYPER = 32 bits.\n\nThe first stage of the front-end\
    \ is composed of two FIFOs buffering incoming AXI read and write requests. Then,\
    \ a serializer solves conflicts among reads and writes, allowing only one AW or\
    \ AR request at a time. Following, three modules translate between AXI and the\
    \ back-end protocol: (i) AXTOPHY, translating the AXI AW or AR requests into commands\
    \ for the back-end; (ii) PHYTOR converting the data words from the back-end into\
    \ AXI read beats for the AXI interface; and (iii) WTOPHY, converting AXI W data\
    \ beats into data words and generating write response at the end of the transaction.\
    \ Three CDC FIFOs are deployed between the AXTOPHY, WTOPHY, and PHYTOR and the\
    \ back-end. The back-end deploys an internal FSM arranging the requests coming\
    \ from the front-end into 48-bit vector requests, as required in the HyperBUS\
    \ protocol, and propagating the data packets to/from the two physical HyperRAM\
    \ memories through two *transceivers* (TRX).\n\n*The HyperRAM memory* is an off-chip\
    \ memory IP [\\[25\\]](#page-13-17). It is provided with a cycle-accurate model,\
    \ fundamental for our analysis purposes [\\[32\\]](#page-13-24). Each HyperRAM\
    \ is organized as an array of 16-bit words and supports one outstanding burst\
    \ transaction, up to 1kB long. As two HyperRAM are interleaved, the overall burst\
    \ can be up to 2kB long [\\[19\\]](#page-13-11).\n\n#### *4.4.2 Delays and parallelism\
    \ analysis*\n\nWe now bound the worst-case service time of the main memory subsystem,\
    \ analyzing its components one at a time. Starting with the LLC, we follow the\
    \ control flow diagram\n\nreported in Figure [7\\(](#page-5-0)b) to guide the\
    \ explanation. The LLC collects the requests incoming to the main memory. Three\
    \ scenarios can happen: (i) LLC cache hit, (ii) LLC cache miss with refill, and\
    \ (iii) LLC cache miss with eviction and refill.\n\nIn case (i), the LLC directly\
    \ manages the request, and no commands are submitted to the HMC. The request proceeds\
    \ through the LLC splitter, hit/miss unit, read/write unit, and data way stages.\
    \ By design, each stage of the LLC requires a fixed number of clock cycles. The\
    \ burst splitter executes in one clock cycle (t LLC CK ). The hit/miss detection\
    \ stage takes two clock cycles (2t LLC CK ): one for tag checking and one to propagate\
    \ the request to the read/write unit or the evict/refill unit. The read/write\
    \ unit requires one clock cycle (t LLC CK ) to route the transaction to the data\
    \ ways. The data ways accept the incoming request in one clock cycle (t LLC CK\
    \ ) to then access the internal SRAM macros (same as the SPM, Section [4.2\\)](#page-3-3).\
    \ The internal SRAM takes one clock cycle to provide the read data (t LLC CK ),\
    \ but no further latency is required on writes. Once it gets the response, the\
    \ read/write unit routes the read channel to the AX interface, whereas it takes\
    \ one clock cycle (t LLC CK ) to generate the write B response at the end. Thus,\
    \ read/write unit and data ways take together three clock cycles (3t LLC CK ).\
    \ Summing up the contributions, the service time in case of a hit is upper bound\
    \ by:\n\n$$t\\_{\\rm{CTRL},\\rm{R}/\\rm{W}}^{\\rm{MS-HIT}} = 6 \\cdot t\\_{\\\
    rm{CK}}^{\\rm{LLC}}; \\quad t\\_{\\rm{DATA},\\rm{R}/\\rm{W}}^{\\rm{MS-HIT}} =\
    \ t\\_{\\rm{CK}}^{\\rm{LLC}}; \\tag{6}$$\n\nConsider now cases (ii) and (iii):\
    \ the eviction and refill stage is also involved, and a read (for refill) and,\
    \ optionally, a write (for eviction) is issued to the main memory. Eviction and\
    \ refill are run in parallel. Each operation performs two steps, each taking one\
    \ clock cycle: (a) generating a transaction for the main memory and (b) generating\
    \ a transaction for the data way. Thus, summing the latency introduced by the\
    \ eviction and refill stage (2t LLC CK ) with the ones from the other stages,\
    \ the LLC's contribution to the overall control time in case of a miss is upper\
    \ bound by:\n\n$$t\\_{\\text{CTRL,R/W}}^{\\text{LLC-MISS}} = t\\_{\\text{CTRL,R/W}}^{\\\
    text{MS-HTT}} + 2t\\_{\\text{CK}}^{\\text{LLC}} \\tag{7}$$\n\nConsider now the\
    \ delay introduced by the HMC on a generic request. Later, we will use it to bound\
    \ the service time for the batch of transactions issued by the LLC. As described\
    \ earlier, the HMC is composed of (a) the frontend, (b) the CDC FIFOs, and (c)\
    \ the back-end. Consider (a): each one of the front-end's submodules takes one\
    \ clock cycle to sample and process the transaction, except for the serializer,\
    \ which takes two. As transactions pass through 4 modules (FIFOs, serializer,\
    \ AXITOPHY, and either WTOPHY or PHYTOR), the overall delay contribution of the\
    \ front-end is equal to 5t HMC CK . Consider now (b): these are the CDC FIFOs\
    \ composing the AXI CDC FIFOs introduced in Section [4.1.](#page-3-4) For writes,\
    \ the transmitter (TX) is the front-end, sending data to the back-end from the\
    \ AXTOPHY and the WTOPHY. As both transfers happen in parallel, the delay introduced\
    \ by the CDC on a write is upper bound by tCDC(t HMC CK , tHRAM CK ). For reads,\
    \ first, the front-end transmits (TX) the AXTOPHY request, and then the back-end\
    \ transmits the data beats: the delay introduced by the CDC on a read is upper\
    \ bound by tCDC(t HMC CK , tHRAM CK ) + tCDC(t HRAM CK , tHMC CK ). Consider now\
    \ (c): the back-end's FSM parses the incoming request into a HyperRAM command\
    \ in one cycle (t HRAM CK ). Following this, an extra cycle is required for the\
    \ data to cross the back-end. Summing up the contributions just described, the\
    \ control time of the HMC on a generic transaction is upper bound by:\n\n$$\\\
    begin{aligned} t\\_{\\rm{CIRL},\\rm{R}}^{\\rm{HMC}} &= 5 \\cdot t\\_{\\rm{CK}}^{\\\
    rm{HMC}} + t\\_{\\rm{CIRC}} (t\\_{\\rm{CK}}^{\\rm{HMC}}, t\\_{\\rm{CK}}^{\\rm{HRM}})\
    \ + t\\_{\\rm{CIRC}} (t\\_{\\rm{CK}}^{\\rm{HRM}}, t\\_{\\rm{CK}}^{\\rm{HMC}})\
    \ + 2 \\cdot t\\_{\\rm{CK}}^{\\rm{HRM}} \\\\ t\\_{\\rm{CIRL},\\rm{W}}^{\\rm{HMC}}\
    \ &= 5 \\cdot t\\_{\\rm{CK}}^{\\rm{HMC}} + t\\_{\\rm{CDC}} (t\\_{\\rm{CK}}^{\\\
    rm{HMC}}, t\\_{\\rm{CK}}^{\\rm{HRM}}) + 2 \\cdot t\\_{\\rm{CK}}^{\\rm{HRM}} \\\
    end{aligned} \\tag{8}$$\n\nConsider now the delays introduced by the HyperRAM\
    \ memories on a generic request. The control overhead time to access the HyperRAM\
    \ memory is defined by the HyperBUS protocol [\\[25\\]](#page-13-17). First, the\
    \ 48-bit HyperRAM command vector is sent over the two memories in 3 · t HRAM CK\
    \ clock cycles, as the HyperBUS command bus is 16 bits. Following, the HyperBUS\
    \ provides a fixed latency for the maximum time to access the first data word,\
    \ accounting for refresh effects and crossing row boundaries. The specifications\
    \ [\\[33\\]](#page-13-25) bound such a delay between 7 and 16 clock cycles. In\
    \ our case, this is set to 12 · t HRAM CK . Thus, the total control latency of\
    \ the HyperRAM memory is upper bound by:\n\n$$t\\_{\\text{CTRL,R/W}}^{\\text{HRAM}}\
    \ = 15 \\cdot t\\_{\\text{CK}}^{\\text{HRAM}} \\tag{9}$$\n\nAt this point, data\
    \ are ready to be propagated. As the AXI domain and the HyperRAM have different\
    \ data widths, the number of cycles to send/receive an AXI word is:\n\n$$t\\_{\\\
    text{DATA,R}/\\text{W}}^{\\text{HRAM}} = DW\\_{\\text{HYPER}} \\cdot \\lceil \\\
    frac{DW\\_{\\text{AXI}}}{DW\\_{\\text{HYPER}}} \\rceil \\cdot t\\_{\\text{CK}}^{\\\
    text{HRAM}} \\tag{10}$$\n\nWe now have all the elements to bound the overall service\
    \ time of the whole main memory subsystem in case of a miss (ii) with refill and\
    \ (iii) eviction and refill. First, we bound the service time to serve a refill\
    \ (read) request. A βi-long transaction is split by the LLC into ⌈βi/LWLLC⌉ subtransactions\
    \ to the memory, each LWLLC-long. Therefore, by multiplying the control time of\
    \ each sub-transaction (t HMC CTRL,R+ t HRAM CTRL,R) by the number of transactions\
    \ issued (⌈ βi LWLLC ⌉), we bound the control time introduced by the memory controller\
    \ and the off-chip memories. To this, we sum the control time of the LLC in case\
    \ of a miss (t MS-MISS CTRL,W/R) and obtain the whole control overhead. The same\
    \ reasoning applies to the data time: the total number of values requested by\
    \ the LLC to the memory will be equal to LWLLC · ⌈ <sup>β</sup><sup>i</sup> LWLLC\
    \ ⌉ and the overall time spent reading LWLLC · ⌈ <sup>β</sup><sup>i</sup> LWLLC\
    \ ⌉t HRAM DATA,R/W. It follows that the time to serve one word is LWLLC βi · ⌈\
    \ <sup>β</sup><sup>i</sup> LWLLC ⌉ · t HRAM DATA,R/W. Summing it with the data\
    \ time of the LLC (t MS-HIT DATA,R/W), we obtain the following upper bounds for\
    \ case (ii):\n\n<span id=\"page-6-0\"></span>\n$$\\begin{split} t\\_{\\text{CTRL,R/W}}^{\\\
    text{MS-MESS-REF}} &= t\\_{\\text{CTRL,R}}^{\\text{LLC-MESS}} + \\left[ \\frac{\\\
    beta\\_i}{LW\\_{\\text{LLC}}} \\right] \\cdot (t\\_{\\text{CTRL,R}}^{\\text{HDAC}}\
    \ + t\\_{\\text{CTRL,R}}^{\\text{HERAM}}); \\\\ t\\_{\\text{DATA,R/W}}^{\\text{MS-MESS-REF}}\
    \ &= t\\_{\\text{DATA,R/W}}^{\\text{MS-HIT}} + \\frac{LW\\_{\\text{LLC}}}{\\beta\\\
    _i} \\cdot \\left[ \\frac{\\beta\\_i}{LW\\_{\\text{LLC}}} \\right] \\cdot t\\\
    _{\\text{DATA,R}}^{\\text{HRAM}}; \\end{split} \\tag{11}$$\n\nIf the eviction\
    \ is also required, ⌈ βi LWLLC ⌉ extra write transactions of length β<sup>i</sup>\
    \ are performed to save the evicted data. Following the same reasoning as earlier,\
    \ this batch of transactions will introduce ⌈ βi LWLLC ⌉(t HMC CTRL,W + t HRAM\
    \ CTRL,W) clock cycles to the control time and LWLLC βi · ⌈ <sup>β</sup><sup>i</sup>\
    \ LWLLC ⌉ ·t HRAM DATA,W to the data time. We sum these numbers to eq. [11](#page-6-0)\
    \ to upper bound the overall control and data time as follows:\n\n$$t\\_{\\text{CTIL,W/R}}^{\\\
    text{MS-MIS-REF-EV}} = t\\_{\\text{CTIL,W/R}}^{\\text{MS-MIS-REF}} + \\left[\\\
    frac{\\beta\\_i}{LW\\_{\\text{LLC}}}\\right] (t\\_{\\text{CTIL,W}}^{\\text{HMCC}}\
    \ + t\\_{\\text{CTIL,W}}^{\\text{HBM}});$$\n\n$$t\\_{\\text{DATA,W/R}}^{\\text{MS-MIS-REF-EV}}\
    \ = t\\_{\\text{DATA,W/R}}^{\\text{MS-MIS-REF}} + \\frac{LW\\_{\\text{LLC}}}{\\\
    beta\\_i} \\cdot \\left[\\frac{\\beta\\_i}{LW\\_{\\text{LLC}}}\\right] \\cdot\
    \ t\\_{\\text{DATA,W}}^{\\text{HBM}};$$\n\nConsider now the parallelism of the\
    \ main memory subsystem. This is defined by the LLC, which acts as an interface\
    \ with the rest of the platform, buffering up to DLLC FIFO read and write transactions.\
    \ This means that the maximum number of supported outstanding transactions is\
    \ as follows:\n\n$$\n\\chi\\_R^{MS} = \\chi\\_W^{MS} = D\\_{\\rm FIFO}^{\\rm LLC}\
    \ \\tag{13}\n$$\n\nThe LLC is pipelined: in the case all the enqueued accesses\
    \ are hits, there is no stalling. However, the memory controller handles only\
    \ one transaction at a time, stalling the preceding ones, and only serves one\
    \ read or one write at a time. Hence, as soon as one access is a miss, ρ MS =\
    \ 0 and θ MS = 0.\n\n#### <span id=\"page-7-2\"></span>**4.5 AXI host crossbar**\n\
    \nThe AXI host crossbar under analysis is a consolidated AXI crossbar already\
    \ validated in multiple silicon tapeouts [\\[16\\]](#page-13-7), [\\[15\\]](#page-13-10),\
    \ [\\[24\\]](#page-13-16). We apply here the generic model for the *crossbar*\
    \ proposed in Section [3.4.](#page-2-3) The crossbar is referred as R0.\n\n####\
    \ *4.5.1 RTL IP structure*\n\nAs detailed in Figure [8,](#page-7-1) the crossbar\
    \ exports a set of input subordinate ports (S) and output manager ports (M). Each\
    \ S port is connected to a demultiplexer, which routes the incoming AW and AR\
    \ requests and W data to the proper destination. Each M port is connected to a\
    \ multiplexer, which (i) arbitrates AW and AR requests directed to the same *peripheral*,\
    \ (ii) connects the selected W channel from the *controller* to the *peripheral*,\
    \ and (iii) routes back the R read data and B write responses. The crossbar under\
    \ analysis can be configured for a fully combinatorial (i.e., decoding and routing\
    \ operations in one clock cycle) or pipelined structure with up to three pipeline\
    \ stages. In the platform under analysis, it is configured to be fully combinatorial.\
    \ ble of granting one AW and one AR request for each clock CUT\n\n#### *4.5.2\
    \ Delays and parallelism analysis*\n\nTo analyze the maximum propagation delays\
    \ introduced by the crossbar, we upper bound the overall latency on a transaction\
    \ by combining the delays introduced on each AXI channel. We provide two upper\
    \ bounds, one for transactions in isolation (i.e., t R<sup>0</sup> PROP,R/W as\
    \ defined in Section [3\\)](#page-2-0) and the other for transactions under contention\
    \ (i.e., t R<sup>0</sup> PROP,R/W+t R<sup>0</sup> CON,R/W as defined in Section\
    \ [3\\)](#page-2-0). We will use both of them in our architectural analysis reported\
    \ in Section [5.](#page-7-0)\n\n*Maximum delays in isolation:* Thanks to the combinatorial\
    \ structure, it is guaranteed by design that a request for a transaction, a data\
    \ word, or a write response crosses the crossbar in one clock cycle (t R<sup>0</sup>\
    \ CK). Consider a whole AXI transaction. For a read transaction, the crossbar\
    \ is crossed twice: on the AR and R AXI channels, respectively. For each AXI write\
    \ transaction, the crossbar is crossed two times: the first time is crossed by\
    \ the AW and W beats (propagated in parallel), and the second time by the B response.\
    \ Thus, the propagation delays in isolation are equal to:\n\n<span id=\"page-7-3\"\
    ></span>\n$$t\\_{\\text{PROP}\\text{R}/\\text{W}}^{R\\_0} = 2 \\cdot t\\_{\\text{CK}}^{R\\\
    _0};\\tag{14}$$\n\n*Maximum delays under contention:* Under contention, multiple\
    \ *controllers* connected to the crossbar can attempt to concurrently send requests\
    \ to the same *peripheral*, generating interference. The arbiters deploy a round-robin\
    \ scheme capa-\n\n<span id=\"page-7-1\"></span>![](_page_7_Figure_12.jpeg)\n\n\
    Fig. 8: AXI Crossbar block diagram\n\ncycle. In the worst-case scenario, the request\
    \ under analysis loses the round-robin and is served last, experiencing a delay\
    \ of MR<sup>0</sup> − 1 clock cycles (with MR<sup>0</sup> the number of *controller*\
    \ capable of interfering with the request under analysis). From the previous considerations,\
    \ the maximum propagation time introduced by the crossbar is upper bound by:\n\
    \n<span id=\"page-7-4\"></span>\n$$t\\_{\\text{CON,R}}^{R\\_0} = t\\_{\\text{CON,W}}^{R\\\
    _0} = M\\_{R\\_0} - 1 \\tag{15}$$\n\nConsider now the parallelism. Concerning\
    \ reads, the crossbar does not keep track of the inflight transactions. To route\
    \ the responses back, it appends information to the AXI ID. Doing so does not\
    \ limit the maximum number of outstanding transactions. The behavior is different\
    \ for writes: AXI enforces a strict in-order execution of write transactions (see\
    \ [\\[29\\]](#page-13-21) p. 98). This requires the crossbar to implement a table\
    \ to know the order of granted transactions. The maximum number of outstanding\
    \ write transactions per S port is limited by the depth of such tables, refereed\
    \ as D R<sup>0</sup> TAB. From the previous consideration: χ R<sup>0</sup> <sup>W</sup>\
    \ = D R<sup>0</sup> TAB. In the architecture under analysis, χ R<sup>0</sup> <sup>W</sup>\
    \ is set to be bigger than the parallelism supported by the *peripherals* so that\
    \ the crossbar does not limit the overall parallelism of the system.\n\n# <span\
    \ id=\"page-7-0\"></span>**5 SYSTEM-LEVEL WORST-CASE RESPONSE TIME ANALYSIS**\n\
    \nThis section introduces our system-level end-to-end analysis to upper bound\
    \ the overall response times of read and write transactions issued by a generic\
    \ *controller* and directed to a generic *peripheral*, considering the maximum\
    \ interference generated by the other *controllers* in the system. Our approach\
    \ is static [\\[34\\]](#page-13-26) and compositional [\\[35\\]](#page-13-27).\
    \ Specifically, we leverage the component-level static analysis introduced in\
    \ Section [4](#page-3-0) to then compose, step-by-step, the system-level worst-case\
    \ service time of transactions traversing the whole architecture.\n\nWe make an\
    \ assumption aligned with the SoA [\\[3\\]](#page-12-3), [\\[4\\]](#page-12-4),\
    \ [\\[8\\]](#page-13-0), [\\[11\\]](#page-13-2), [\\[12\\]](#page-13-3), [\\[36\\\
    ]](#page-13-28) to ensure independence among *peripherals* while not compromising\
    \ the generality of the analysis. It is assumed that multiple outstanding transactions\
    \ of the same type (either read or write) issued by the same *controller* target\
    \ the same *peripheral*: before issuing a transaction targeting a *peripheral*\
    \ P<sup>j</sup> , a *controller* completes the pending transactions of the same\
    \ type targeting a different *peripheral* Pz. Without such an assumption, due\
    \ to the strict ordering imposed by the AXI standard [\\[29\\]](#page-13-21) on\
    \ the W channel, and the structure of some *peripherals* generating interference\
    \ between reads and writes (i.e., ρ <sup>P</sup><sup>j</sup> = 0), transactions\
    \ issued by C<sup>k</sup> and directed to P<sup>j</sup> might interfere with transactions\
    \ issued by C<sup>i</sup> and directed to Pz, if C<sup>i</sup> also issues in\
    \ parallel transactions\n\nto P<sup>j</sup> , and vice-versa. This assumption\
    \ allows us to relax our analysis, removing such pathological cases. It is worth\
    \ noticing that it does not enforce any relationship between read and write transactions.\
    \ Such an assumption can either be enforced at the software level or at the hardware\
    \ level. The results of our analysis can be extended to such corner cases if required.\
    \ We leave this exploration for future works.\n\nThe first step of the analysis\
    \ is to bound the overall response time of a transaction in isolation (Lemma [1\\\
    )](#page-8-0). Secondly, we bound the maximum number of transactions that can\
    \ interfere with a transaction under analysis, either of the same type (e.g.,\
    \ reads interfering with a read, Lemma [2\\)](#page-8-1) or of a different type\
    \ (e.g., write interfering with a read, and vice versa, Lemma [3\\)](#page-8-2).\
    \ Lemma [4](#page-8-3) bounds the maximum temporal delay each interfering transaction\
    \ can delay a transaction under analysis. Finally, Theorem [1](#page-9-1) combines\
    \ the results of all the lemmas to upper bound the overall worstcase response\
    \ time of a transaction under analysis under interference. We report the lemmas\
    \ in a general form. AXi,j can represent either a read or write transaction issued\
    \ by the generic *controller* C<sup>i</sup> and directed to the generic *peripheral*\
    \ P<sup>j</sup> . The *crossbar* is referred to as R0. To make our analysis general,\
    \ we assume that Ψ<sup>j</sup> = [C0, ..., CM−1] is the generic set of interfering\
    \ *controllers* capable of interfering with C<sup>i</sup> issuing transactions\
    \ to P<sup>j</sup> and that that a generic set of *bridges* Θ<sup>i</sup> = {Q0,\
    \ ..., Qq−1} can be present between each *controller* C<sup>i</sup> and the crossbar\
    \ R0. The cardinality of Ψ<sup>j</sup> is referred to as \f Ψ<sup>j</sup> \f and\
    \ corresponds to the number of *controllers* interfering with AXi,j .\n\n<span\
    \ id=\"page-8-0\"></span>**Lemma 1.** *The response time in isolation of* AXi,j\
    \ *is upper bounded by:*\n\n<span id=\"page-8-4\"></span>\n$$d\\_{i,j}^{X} = d\\\
    _{\\mathbb{R}\\mathcal{W}}^{P\\_j} + \\sum\\_{Q\\_l \\in \\Theta\\_{i,j}} d\\\
    _{\\mathbb{R}\\mathcal{W}}^{Q\\_l} + d\\_{\\mathbb{R}\\mathcal{W}}^{R\\_0} \\\
    tag{16}$$\n\n*Proof.* Section [4](#page-3-0) upper bounds the worst-case delays\
    \ in isolation introduced by each component in the platform. According to their\
    \ definition, such delays account for all of the phases of the transaction. The\
    \ components in the platform are independent of each other. Thus, the delay introduced\
    \ by each traversed component is independent of the behavior of the other components.\
    \ It derives that the overall delay incurred in traversing the set of components\
    \ between C<sup>i</sup> and P<sup>j</sup> is upper bounded by the sum of the worst-case\
    \ delays introduced by all of the components in the set. Summing up the maximum\
    \ delay introduced by the target *peripheral* P<sup>j</sup> (d P<sup>j</sup> R/W),\
    \ by the set of traversed *bridges* Θ<sup>i</sup> , and by the *crossbar* R<sup>0</sup>\
    \ (d R<sup>0</sup> R/W), the lemma derives.\n\n<span id=\"page-8-1\"></span>**Lemma\
    \ 2.** *The maximum number of transactions of the same type that can interfere\
    \ with* AXi,j *is upper bounded by:*\n\n$$S\\_{i,j}^X = \\min\\left(\\sum\\_{C\\\
    _y \\in \\Psi\\_j} \\phi\\_X^{C\\_y}, \\chi\\_X^{P\\_j} + \\mid \\Psi\\_j \\mid\
    \ \\right) \\tag{17}$$\n\n*Proof.* The min in the formula has two components.\
    \ As from the AXI standard definition, an interfering *controller* C<sup>k</sup>\
    \ cannot have more than ϕ C<sup>k</sup> <sup>X</sup> pending outstanding transactions.\
    \ This means that summing up the maximum number of outstanding transactions for\
    \ each interfering *controller* in Ψ<sup>j</sup> provides an upper bound on the\
    \ number of transactions of the same type interfering with AXi,j – the left member\n\
    \nof the min derives. From our *peripheral* analysis reported in Section [4,](#page-3-0)\
    \ P<sup>j</sup> and R<sup>0</sup> can limit the maximum amount of transactions\
    \ accepted by the system: P<sup>j</sup> accepts overall at most χ P<sup>j</sup>\
    \ R/W transactions – when such a limit is reached, any further incoming transaction\
    \ directed to P<sup>j</sup> is stalled. After P<sup>j</sup> serves a transaction,\
    \ R<sup>0</sup> restarts forwarding transactions to the *peripheral* following\
    \ a round-robin scheme (see Section [4\\)](#page-3-0). In the worst-case scenario,\
    \ C<sup>i</sup> loses the round-robin arbitration against all of the \f Ψ<sup>j</sup>\
    \ interfering *controllers* in Ψ<sup>j</sup> , each ready to submit an interfering\
    \ request. Summing up the contributions, also χ P<sup>j</sup> <sup>R</sup> + \f\
    \ Ψ<sup>j</sup> \f upper bounds the maximum number of transactions interfering\
    \ with AXi,j – the right member of the min derives. Both of the bounds are valid\
    \ – the minimum between them is an upper bound providing the least pessimism –\
    \ Lemma [2](#page-8-1) derives.\n\n<span id=\"page-8-2\"></span>**Lemma 3.** *The\
    \ maximum number of transactions of a different type (represented here as Y, i.e.,\
    \ write transactions interfering with a read under analysis, and vice versa) interfering\
    \ with* AXi,j *is upper bounded by:*\n\n$$U\\_{i,j}^{Y} = (S\\_{i,j}^{X} + 1)\
    \ \\cdot (1 - \\theta^{P\\_j}) \\tag{18}$$\n\n*Proof.* According to Section [4.5,](#page-7-2)\
    \ R<sup>0</sup> manages transactions of different types independently – thus,\
    \ no interference of this type is generated at the R<sup>0</sup> level. From Section\
    \ [3,](#page-2-0) θ <sup>P</sup><sup>j</sup> = 1 represents the case in which\
    \ the *peripheral* is capable of serving read and write transactions in parallel\
    \ (e.g., the SPM *peripheral*, Section [4.2\\)](#page-3-3). Thus, no interference\
    \ is generated among them – the second equation derives. From Section [3,](#page-2-0)\
    \ θ <sup>P</sup><sup>j</sup> = 0 represents the case in which P<sup>j</sup> does\
    \ not feature parallelism in serving read and write transactions (i.e., also write\
    \ transactions interfere with reads, e.g., main memory subsystem, Section [4.4\\\
    )](#page-5-1). Considering lemma [2,](#page-8-1) at most S X i,j transactions\
    \ of the same type can interfere with AXi,j . With θ <sup>P</sup><sup>j</sup>\
    \ = 0, and assuming a round-robin scheme arbitrating between reads and writes\
    \ at the *peripheral* level, each one of the S X i,j interfering transaction of\
    \ the same type can be preceded by a transaction of the opposite type, which can,\
    \ therefore, create interference. The same applies to AXi,j , which can lose the\
    \ arbitration at the *peripheral* level as well. Summing up the contribution,\
    \ it follows that S X i,j + 1 can overall interfere with AXi,j – the first equation\
    \ derives.\n\n<span id=\"page-8-3\"></span>**Lemma 4.** *The maximum time delay\
    \ that a transaction of any kind* AXk,j *issued by the generic interfering* controller\
    \ C<sup>k</sup> *can cause on* AXi,j *is upper bounded by:*\n\n<span id=\"page-8-5\"\
    ></span>\n$$\n\\Delta\\_{k,j} = d\\_{\\text{R\\\\$}\\mathcal{W}}^{R\\_0} + (1\
    \ - \\rho^{P\\_j}) \\cdot t\\_{\\text{CTRL,R\\\\$}\\mathcal{W}}^{P\\_j} + t\\\
    _{\\text{DATT},\\text{R\\\\$}\\mathcal{W}}^{P\\_j} \\cdot \\beta\\_k \\tag{19}\n\
    $$\n\n*Proof.* In traversing the path between C<sup>k</sup> and P<sup>j</sup>\
    \ , AXk,j shares a portion of the path with AXi,j , i.e., the target *peripheral*\
    \ P<sup>j</sup> and the crossbar R<sup>0</sup> – no *bridges* from Θ<sup>k</sup>\
    \ belongs to the shared path, thus the delay propagation of AXk,j do not contribute\
    \ in delaying AXk,j . Considering the delay generated by AXk,j at R0, this is\
    \ upper bounded by d R<sup>0</sup> R/W in Section [3.4.](#page-2-3) As from Section\
    \ [3.3,](#page-2-2) t P<sup>j</sup> CTRL,R/W + t P<sup>j</sup> DATA,R/W · β<sup>k</sup>\
    \ is the maximum service time of P<sup>j</sup> for the transaction AXk,j and upper\
    \ bounds the maximum temporal delay that AXk,j can cause on AXi,j at P<sup>j</sup>\
    \ . As from the definition of an interfering transaction, AXk,j is served by P<sup>j</sup>\
    \ before AXi,j . As defined by the model in Section [3.3,](#page-2-2) when ρ <sup>P</sup><sup>j</sup>\
    \ = 1, the *peripheral* works in a pipeline fashion. This means that\n\nfor ρ\
    \ <sup>P</sup><sup>j</sup> = 1, the control time t P<sup>j</sup> CTRL,R/W of an\
    \ interfering transaction is pipelined and executed in parallel with the transaction\
    \ under analysis. Differently, when ρ <sup>P</sup><sup>j</sup> = 0, no pipeline\
    \ is implemented, and the control time of the interfering transaction can partially\
    \ or totally interfere with the transaction under analysis. From the previous\
    \ considerations, the contribution (1 − ρ <sup>P</sup><sup>j</sup> )·t P<sup>j</sup>\
    \ CTRL,R/W + t P<sup>j</sup> DATA,R/W · β<sup>k</sup> derives. Summing up the\
    \ contributions, the lemma follows.\n\n<span id=\"page-9-1\"></span>**Theorem\
    \ 1.** *The overall response time of* AXi,j *under the interference generated\
    \ by the other* controllers *in the system is upper bounded by:*\n\n<span id=\"\
    page-9-3\"></span>\n$$H\\_{i,j}^X = d\\_{i,j}^X + (S\\_{i,j}^X + U\\_{i,j}^Y)\
    \ \\cdot \\Delta\\_{k,j} \\tag{20}$$\n\n*Proof.* Summing up the contribution in\
    \ isolation for AXi,j (Lemma [1\\)](#page-8-0) with the sum of the maximum number\
    \ of interfering transactions of the same type (Lemma [2\\)](#page-8-1) and of\
    \ a different type (Lemma [3\\)](#page-8-2) multiplied by the maximum delay generated\
    \ by each interfering transaction (Lemma [4\\)](#page-8-3), Theorem [1](#page-9-1)\
    \ derives.\n\nThe results presented in this Section represent analytical upper\
    \ bounds derived through static code analysis and the formulation of mathematical\
    \ proofs. Section [6](#page-9-0) will validate them through a comprehensive set\
    \ of cycle-accurate experiments and measurements.\n\n# <span id=\"page-9-0\"></span>**6\
    \ EXPERIMENTAL VALIDATION**\n\nThis Section describes the experimental campaign\
    \ we conducted to validate the methodology and models. The aim of the experimental\
    \ campaign is to assess that the results presented in the previous Sections correctly\
    \ upper bound the maximum delays and response times at the component level and\
    \ the architectural level. We follow a hierarchical approach: at first, Section\
    \ [6.1](#page-9-2) aims to validate the results at the component level we proposed\
    \ in Section [4.](#page-3-0) Following, in Section [6.2,](#page-10-0) we experimentally\
    \ validate the system-level analysis we proposed in Section [5.](#page-7-0) The\
    \ experiments are conducted in a simulated environment (leveraging the Siemens\
    \ QuestaSIM simulator) and by deploying the design on an FPGA platform. In the\
    \ simulated experiments, we deploy custom AXI managers for *ad-hoc* traffic generation\
    \ and cycle-accurate performance monitors. The generic custom manager represents\
    \ a generic configurable *controller* C<sup>i</sup> issuing requests for transactions\
    \ – we will refer to that as GC<sup>i</sup> . In the FPGA, we leverage CVA6 and\
    \ the PULP cluster to generate the traffic with synthetic software benchmarks\
    \ and rely on their performance-monitoring registers to collect the measurements.\
    \ The experimental designs are deployed on the AMD-Xilinx VCU118, using the Vitis\
    \ 2022.1 toolchain. Similar State-of-the-Art works upper bounding the execution\
    \ time of a single transaction leverage synthetic benchmarks to measure the worst-case\
    \ access times since generic applications fail to do so [\\[8\\]](#page-13-0)–[\\\
    [10\\]](#page-13-1). For this reason, we concentrate on synthetic benchmarks at\
    \ the IP and the system level.\n\n#### <span id=\"page-9-2\"></span>**6.1 Component-level\
    \ hardware modules**\n\n#### *6.1.1 Delays analysis*\n\nThis subsection presents\
    \ the tests run to measure the worstcase access latency time in isolation for\
    \ the *peripherals* (d P<sup>j</sup> R/W ), 10\n\nthe *crossbar* (d R<sup>0</sup>\
    \ R/W ) and the *bridges* (d Q<sup>j</sup> R/W ) from Section [4.](#page-3-0)\
    \ We connect the generic controller CG<sup>i</sup> to the IP under analysis for\
    \ these experiments. We let CG<sup>i</sup> issue 100'000 transactions, one at\
    \ a time, with random burst length (βi). We monitor the service times and then\
    \ pick the longest ones for different β<sup>i</sup> .\n\nFigure [9](#page-10-1)\
    \ compares the maximum measured experimental delays with the upper bound proposed\
    \ in Section [4.](#page-3-0) Figure [9\\(](#page-10-1)a) reports the maximum service\
    \ time of the main memory subsystem in case of a miss as a function of the burst\
    \ length of the transaction under analysis, either when (i) only a refill is necessary\
    \ and (ii) both refill and eviction are necessary, compared with the bounds proposed\
    \ in Section [4.4.](#page-5-1) The measured service times are lower than the bounds.\
    \ The pessimism is between 3% and 10.1%; the larger β<sup>i</sup> , the higher\
    \ the pessimism. Higher pessimism on longer transactions is due to the internal\
    \ splitting at the LLC. As from our analysis, the memory subsystem is not fully\
    \ pipelined (ρMS = 0). However, in practice, the control and data phases of consecutive\
    \ sub-transactions might be partially served in parallel by the LLC and the memory\
    \ controller. This means that the longer the transaction, the higher the number\
    \ of sub-transactions and their overlap, and the lower the service time compared\
    \ to our model. Thus, the pessimism increases. Figure [9\\(](#page-10-1)b) reports\
    \ the measured results on the main memory subsystem but in case of a hit, compared\
    \ with the bounds proposed in Section [4.4.](#page-5-1) As we consider an LLC\
    \ hit, the access to the HyperRAM is not performed: this test analyzes the service\
    \ time of the LLC. Our bounds are always upper bounds for the maximum measured\
    \ results. The trend here is reversed w.r.t. Figure [9\\(](#page-10-1)a) – as\
    \ β<sup>i</sup> increases, the relative pessimism decreases from 7.7% down to\
    \ 0.4%. In this case, the source of the pessimism comes only from the control\
    \ time, which does not depend on β<sup>i</sup> , while there is no pessimism on\
    \ the data time. Hence, this pessimism gets amortized as the burst length and\
    \ the overall service time increase. We conduct the same experimental campaign\
    \ also on the AXI SPM – the measured results, compared with the bounds proposed\
    \ in Section [4.2,](#page-3-3) are reported in Figure [9\\(](#page-10-1)c). The\
    \ trends are similar to the ones reported in Figure [9\\(](#page-10-1)b) for LLC\
    \ hits – the pessimism of our analysis is limited to 1 and 2 clock cycles for\
    \ reads and writes on the control time, respectively. As in the case of the LLC\
    \ HITs, the upper bound on the control overhead gets amortized for longer transactions,\
    \ and the pessimism reduces from 8.8% to 0.5%.\n\nFigure [9\\(](#page-10-1)d)\
    \ reports the maximum measured latency to cross an AXI CDC FIFO as a function\
    \ of the manager clock period (the subordinate clock period is fixed to 30 ns)\
    \ and compared with the bounds proposed in Section [4.1.](#page-3-4) The results\
    \ are independent of the length of the transaction. To stimulate the highest variability,\
    \ the phases of the clocks are randomly selected on a uniform distribution. The\
    \ first bar reports the crossing delays from the manager to the subordinate side,\
    \ corresponding to the delays introduced on the AW, W, and AR AXI channels. The\
    \ second bar reports the crossing delays from the subordinate to the manager side,\
    \ corresponding to the overall delays on the AXI R and B channels. The third bar\
    \ shows the overall delay on a complete transaction, corresponding to the sum\
    \ of the two previously introduced contributions (see Section [4.1\\)](#page-3-4).\
    \ The pessimism of our bounds is, at most, one clock cycle of the slowest clock\
    \ between manager and subordinate.\n\n<span id=\"page-10-1\"></span>![](_page_10_Figure_0.jpeg)\n\
    \nFig. 9: Services time in isolation.\n\nFigure [9\\(](#page-10-1)e) reports the\
    \ measured propagation delays introduced by the crossbar over an entire write\
    \ and read transaction, compared with the bounds of Section [4.5,](#page-7-2)\
    \ varying the number of *controllers*. As explained in Section [4.5,](#page-7-2)\
    \ the propagation delay is the sum of the propagation latency without interference\
    \ (eq. [14\\)](#page-7-3) and the additional contention latency (eq. [15\\)](#page-7-4),\
    \ which depends on the number of *controllers*. Thanks to the simplicity of the\
    \ arbitration operated by the crossbar (pure round-robin), our proposed bounds\
    \ exactly match the measurements. We conducted the experimental campaign also\
    \ on the IO subsystem. We measured the maximum service time and compared it with\
    \ the upper bounds of Section [4.3,](#page-4-1) which we do not show for space\
    \ reasons: such IP supports only single-word transactions. Our upper bounds exceed\
    \ the maximum measured service time with pessimism of down to 2 clock cycles,\
    \ with service times of 4 (write) and 5 (read) clock cycles.\n\n#### *6.1.2 Parallelism*\n\
    \nWe also demonstrate our analysis of parallelism of the *peripherals* (χ P<sup>j</sup>\
    \ R/W ) and the *crossbar* (χ R<sup>0</sup> R/W ) analyzed in Section [4.](#page-3-0)\
    \ To do so, we configured CG<sup>i</sup> to issue unlimited outstanding transactions\
    \ to the *peripheral* under test. In parallel, we monitor the maximum number of\
    \ accepted outstanding transactions. Our measurements match our analysis: the\
    \ maximum number of outstanding transactions is defined by the maximum parallelism\
    \ accepted at the input stage of the peripherals and the crossbar.\n\n#### <span\
    \ id=\"page-10-0\"></span>**6.2 System-level experiments**\n\nWhile the previous\
    \ experiments focused on the evaluation at the IP level, this set of experiments\
    \ aims to evaluate the system-level bounds proposed in Section [5.](#page-7-0)\
    \ We first validate our analysis in simulation. We developed a System Verilog\
    \ testbench with two configurable AXI synthetic *controllers* CG<sup>i</sup> connected\
    \ to the target architecture (see Figure [2\\)](#page-1-2) stimulating overload\
    \ conditions to highlight worst-case scenarios. We also validate our results on\
    \ FPGA, generating traffic with CVA6 and the PULP cluster.\n\nAt first, we evaluate\
    \ the results in isolation *at the system level* as a function of the burst length,\
    \ leveraging the same strategy used for the previous experiments. Namely, these\
    \ tests are meant to validate Lemma [1](#page-8-0) (eq. [16\\)](#page-8-4). To\
    \ measure the service time at the system level in isolation, we let one GC<sup>i</sup>\
    \ issue 100'000 transactions, one at a time, with different β<sup>i</sup> , while\
    \ the other GC<sup>k</sup> is inactive. We monitor the service times and then\
    \ pick the longest ones for each\n\nβi . Figures [10](#page-11-1) (a) and (b)\
    \ report the maximum measured system-level response times in isolation for completing\
    \ a transaction issued by the generic *controller* GC<sup>i</sup> and directed\
    \ to (a) the main memory subsystem (case of cache miss, causing either refill\
    \ or both refill and eviction) and (b) to the SPM memory, compared with the bounds\
    \ proposed in Lemma [1.](#page-8-0) The measured service times are smaller than\
    \ the bounds in all the tested scenarios. The measure and the trends reported\
    \ in Figure [10\\(](#page-11-1)a) are aligned with the ones found at the IP level\
    \ and reported in Figure [9\\(](#page-10-1)a). This is because the overhead introduced\
    \ by the crossbar (in isolation) and the CDC FIFOs is negligible compared to the\
    \ memory subsystem's service time. Figure [10\\(](#page-11-1)b) shows a trend\
    \ aligned with the results at the IP-level reported in Figure [9\\(](#page-10-1)c):\
    \ the lower β<sup>i</sup> , the higher the pessimism. It is worth mentioning that\
    \ the analysis shows higher pessimism at the system level than at the IP level.\
    \ This is due to the extra pessimism from the crossbar and the CDC, which is nevertheless\
    \ amortized on longer transactions, down to 1.9%.\n\nWe now analyze the results\
    \ under maximum interference, to verify the results of Lemma [2](#page-8-1) and\
    \ [3](#page-8-2) and Theorem [1.](#page-9-1) For these tests, the execution of\
    \ GC<sup>i</sup> (100'000 transactions, one at a time) receives interference by\
    \ *controller* GCk. β<sup>k</sup> is fixed and equal to β<sup>i</sup> , while\
    \ we vary the amount of outstanding transactions GC<sup>k</sup> can issue (ϕ CG<sup>k</sup>\
    \ R/W ). Figures [10](#page-11-1) (c), (d), and (e) report the maximum measured\
    \ systemlevel response times for completing a transaction issued by the generic\
    \ *controller* GC<sup>i</sup> and directed to (c) the main memory with an LLC\
    \ miss considering β<sup>i</sup> = 16, (d) the SPM memory, considering β<sup>i</sup>\
    \ = 16, and (e) the SPM memory, considering β<sup>i</sup> = 256, and compare them\
    \ with the upper bounds proposed in equation [20.](#page-9-3) Figures [10](#page-11-1)\
    \ (c), (d), and (e) verify the results of Lemma [2](#page-8-1) and [3:](#page-8-2)\
    \ when ϕ CG<sup>k</sup> R/W > χMS R/W (two bars on the right), the total service\
    \ time is defined by the parallelism of the peripheral itself – as expected, after\
    \ saturating the number of interfering transactions accepted by the peripheral,\
    \ the measured results are the same regardless of the increase of ϕ CG<sup>k</sup>\
    \ R/W . Differently, when ϕ CG<sup>k</sup> R/W ≤ χMS R/W , a reduced value of\
    \ ϕ CG<sup>k</sup> R/W corresponds to lower interference and response times. Figure\
    \ [10\\(](#page-11-1)c) refers to the case of an LLC miss under interference when\
    \ β<sup>k</sup> = 16. The results confirm the safeness of our analysis, which\
    \ correctly upper bounds the overall response times with a pessimism around 15%,\
    \ which is slightly higher than the pessimism of a transaction in isolation at\
    \ the system level. As explained in the previous subsection, when multiple transactions\
    \ are enqueued, the memory subsystem can partially serve their data and control\n\
    \n12\n\n<span id=\"page-11-1\"></span>![](_page_11_Figure_1.jpeg)\n\nFig. 10:\
    \ Services times under interference.\n\nphases in parallel. However, our model\
    \ only allows ρMS = 1 or ρMS = 0, i.e., either the *peripheral* is fully pipelined\
    \ or not pipelined at all. Since ρMS = 0, the pessimism is slightly higher when\
    \ more transactions are enqueued (and partially served in parallel) as equation\
    \ [19](#page-8-5) counts the service time of a transaction fully when ρMS = 0.\
    \ Varying β<sup>k</sup> of GC<sup>k</sup> gives comparable results – we do not\
    \ report such results for briefness and lack of space. We provide two charts for\
    \ the SPM, in Figure [10\\(](#page-11-1)d) and Figure [10\\(](#page-11-1)e). The\
    \ comparison of the two charts highlights how the interfering transactions' length\
    \ impacts the analysis's pessimism, ranging between 19.7% for β = 16 to 1% for\
    \ β = 256. The trend here is aligned with the service time at the system level\
    \ in isolation: the pessimism comes from the control times of SPM and propagation\
    \ latency of the crossbar and the CDC, which are amortized as the data time increases\
    \ with βk.\n\n#### **6.3 Discussion**\n\nIn this Section, we validated the analysis\
    \ of Sections [4](#page-3-0) and [5](#page-7-0) through an extensive set of tests.\
    \ We demonstrated how the proposed approach enables detailed explanations of the\
    \ analysis's pessimism and facilitates iterative refinement. This allows us to\
    \ derive upper bounds that are safe yet not overly pessimistic, particularly when\
    \ compared to similar stateof-the-art works based on closed-source or loosely-timed\
    \ IPs. Nevertheless, while the methodology is promising, the resulting analysis\
    \ may seem limited in comparison to other works that model more sophisticated\
    \ closed-source IPs. Here, we discuss the limitations of our analysis, focusing\
    \ on its dependence on the underlying characteristics of the available open-source\
    \ hardware.\n\nIt is noteworthy how the analysis leverages the roundrobin policy\
    \ of the main interconnect and the in-order nature of *peripherals* in Lemmas\
    \ [2](#page-8-1) and [3.](#page-8-2) The absence of internal reordering allows\
    \ to derive the number of transactions preceding the one under interference directly\
    \ from the arbitration policy. As long as the *peripherals* serve the transactions\
    \ in order, extending the analysis to support other arbitration policies is expected\
    \ to require minimal effort. Instead, supporting *peripherals* with internal transaction\
    \ reordering can lead to *timing anomalies* [\\[7\\]](#page-12-2) and make the\
    \ proposed model unsafe, as previously demonstrated in [\\[5\\]](#page-12-1).\
    \ Our analysis focuses on the available *peripherals* within the target architecture,\
    \ as out-of-order *peripherals* are not available open-source to us. We envision\
    \ expanding the\n\nanalysis to match higher-performance platforms as opensource\
    \ hardware evolves.\n\nLastly, it is important to note that the analysis bounds\
    \ only a single transaction issued by C<sup>i</sup> – this limitation is not imposed\
    \ on the interfering controllers. Lemma [2](#page-8-1) does not consider C<sup>i</sup>\
    \ to have more pending transactions, except for the ones already accepted by P<sup>j</sup>\
    \ . In other words, Lemma [2](#page-8-1) assumes that there is not a queue of\
    \ transactions buffered in the *bridges* between C<sup>i</sup> and R0, which could\
    \ exist when P<sup>j</sup> is full. We could potentially extend the model to define\
    \ a batch of enqueued transactions and then modify Lemma [2](#page-8-1) to analyze\
    \ this scenario. Such an extension would further build upon the proposed model\
    \ and analysis, which is limited to bound the access time of a single transaction.\n\
    \n# <span id=\"page-11-0\"></span>**7 RELATED WORK**\n\nIn this Section, we provide\
    \ a thorough comparison with previous works focusing on enhancing the timing predictability\
    \ of digital circuits. Traditionally, the majority of these works leverage commercial\
    \ off-the-shelf devices [\\[34\\]](#page-13-26), [\\[38\\]](#page-13-29) or predictable\
    \ architectures modeled with a mix of cycleaccurate and behavioral simulators\
    \ [\\[39\\]](#page-13-30). Also, they focus on bounding the execution times for\
    \ predefined specific software tasks rather than the individual transaction service\
    \ times [\\[7\\]](#page-12-2), [\\[38\\]](#page-13-29)–[\\[40\\]](#page-13-31).\
    \ Furthermore, they build the models from dynamic experiments rather than from\
    \ static analysis, largely due to the dearth of detailed hardware specifications\
    \ [\\[35\\]](#page-13-27), limiting the generality of their approach. More recent\
    \ works advocate for static modeling and analysis of protocols [\\[8\\]](#page-13-0),\
    \ [\\[13\\]](#page-13-4), interconnect [\\[1\\]](#page-12-0), [\\[3\\]](#page-12-3),\
    \ [\\[9\\]](#page-13-5), and shared memory resources [\\[5\\]](#page-12-1), [\\\
    [10\\]](#page-13-1) to provide more generic and comprehensive models. While their\
    \ value is undeniable, due to the unavailability of the source RTL, each one focuses\
    \ on only one of these resources, resulting in a significant penalty to the pessimism\
    \ of the upper bounds [\\[5\\]](#page-12-1). Our work breaks from this convention,\
    \ presenting a holistic static model of an entire open-source architecture rigorously\
    \ validated through RTL cycle-accurate simulation and FPGA emulation. As Table\
    \ [1](#page-12-6) shows, this is the first work to analyze and model the open-source\
    \ siliconproven RTL of all the IPs composing a whole SoC to build the least pessimistic\
    \ upper bounds for data transfers within the architecture when compared to similar\
    \ SoA works.\n\nBiondi et al. [\\[13\\]](#page-13-4) developed a model of the\
    \ memory-access regulation mechanisms in the ARM MPAM and provided detailed instantiations\
    \ of such mechanisms, which they\n\n<span id=\"page-12-6\"></span>\n\n|  |  |\
    \  |  |  |  |  |  |  |  |  |  |  |  |  | TABLE 1: Comparison with State-of-the-Art\
    \ works for predictability. IC = Interconnect. DMR = Deadline miss ratio. |  |\
    \  |  |  |  |\n|--|--|--|--|--|--|--|--|--|--|--|--|--|--|--|-------------------------------------------------------------------------------------------------------------------|--|--|--|--|--|\n\
    |--|--|--|--|--|--|--|--|--|--|--|--|--|--|--|-------------------------------------------------------------------------------------------------------------------|--|--|--|--|--|\n\
    \n|                       | Target                             | Analysis on \
    \                   | Pessimism   | Technology  | Open RTL |\n|-----------------------|------------------------------------|--------------------------------|-------------|-------------|----------|\n\
    | Biondi et. al. [13]   | ARM MPAM Protocol                  | Protocol specs\
    \ (Model)         | No HW       | ✗           | ✗        |\n| Hassan et. al. [8]\
    \    | JEDEC DDR3 Protocol                | Protocol specs (Model)         | 0%\
    \ − 200%   | ✗           | ✗        |\n| Abdelhalim et.al. [5] | Whole mem. hier.\
    \                   | IPs & System (C++ Model)       | 16% − 50%   | ✗       \
    \    | ✗        |\n| BlueScale [3]         | Hier. mem. IC                   \
    \   | IC uArch (Black-box)           | DMR         | FPGA        | ✗        |\n\
    | AXI-RT-IC [1]         | AXI SoC IC                         | IC uArch (Black-box)\
    \           | DMR         | FPGA        | ✗        |\n| Restuccia et. al. [9]\
    \ | AXI Hier. mem. IC                  | IC uArch (Black-box)           | 50%\
    \ − 90%   | FPGA        | ✗        |\n| AXI-REALM [37]        | AXI traffic regulator\
    \              | No analysis                    | No model    | FPGA & ASIC |\
    \ ✓        |\n| Ditty [10]            | Cache coher. mechanism             | IP\
    \ (Fine-grained RTL)          | 100% − 200% | FPGA        | ✓        |\n| This\
    \ Work             | SoC IC, peripherals & system-level | IP & System (Fine-grained\
    \ RTL) | 1% − 28%    | FPGA & ASIC | ✓        |\n\nthen evaluated with IBM CPLEX,\
    \ a decision optimization software for solving complex optimization models. While\
    \ elegant, this approach is not validated on hardware and, therefore, is limited\
    \ in terms of applicability and precision. A more practical and adopted approach\
    \ is the one proposed by Hassan and Pellizzoni [\\[8\\]](#page-13-0). The authors\
    \ develop a finegrained model of the JEDEC DDR3 protocol, validated with MCsim\
    \ [\\[12\\]](#page-13-3), a cycle-accurate C++ memory controller simulator. Unfortunately,\
    \ not having access to the RTL prevents fine-grained modeling and analysis and\
    \ mandates over-provisioning, strongly impacting the overall pessimism of the\
    \ system, which can be as high as 200%. Abdelhalim et al. in [\\[5\\]](#page-12-1)\
    \ present a study bounding the access times of memory requests traversing the\
    \ entire memory hierarchy and propose µarchitectural modifications to the arbiters\
    \ in such hierarchy. Their modifications result in very low pessimism (down to\
    \ 16%) on synthetic and real-world benchmarks. However, the results are validated\
    \ on C++ models of the cores, interconnect, and memory controllers, not RTL code\
    \ targeting silicon implementation.\n\nMore recently, different researchers proposed\
    \ models of hardware IPs that they could validate through cycle-accurate experiments\
    \ [\\[1\\]](#page-12-0), [\\[4\\]](#page-12-4), [\\[9\\]](#page-13-5). In [\\\
    [9\\]](#page-13-5), Restuccia et al. focused on upper bounding the response times\
    \ of AXI bus transactions on FPGA SoCs through the modeling and analysis of generic\
    \ hierarchical interconnects arbitrating the accesses of multiple hardware accelerators\
    \ towards a shared DDR memory. In this work, the interconnect under analysis is\
    \ a proprietary Xilinx IP, which had to be treated as a black box. Also, due to\
    \ the unavailability of the RTL code, the authors did not model the other IPs\
    \ composing the target platform, limiting the precision of the proposed upper\
    \ bounds, which achieve a pessimism between 50% and 90%. Jiang et al. modeled,\
    \ analyzed, and developed AXI-ICRT [\\[1\\]](#page-12-0) and Bluescale [\\[3\\\
    ]](#page-12-3), two sophisticated interconnects providing predictability features\
    \ and coming with a comprehensive model. However, the model and analysis proposed\
    \ in AXI-ICRT [\\[1\\]](#page-12-0), and Bluescale [\\[3\\]](#page-12-3) are not\
    \ as fine-grained as ours: the authors do not provide upper bounds of the access\
    \ times but rather focus on the deadline miss ratio given a fixed workload for\
    \ the different controllers in the system. Moreover, the authors do not provide\
    \ the RTL of such solutions. AXI-REALM [\\[37\\]](#page-13-32) proposes completely\
    \ open-source IPs supporting predictable communications. However, it misses a\
    \ holistic model and analysis. In Ditty [\\[10\\]](#page-13-1), researchers propose\
    \ an open-source predictable directory-based cache coherence mechanism for multicore\
    \ safety-critical systems that guarantees a worst-case latency (WCL) on data accesses\
    \ with almost cycle-accurate precision. However, Ditty's model only covers the\
    \ coherency protocol latency and the core subsystem, overlooking systemlevel analysis\
    \ and achieving very pessimistic boundaries. In this landscape, it emerges clearly\
    \ that our work is the first one covering both modeling and analysis of the interconnects\
    \ and the shared memory resources, with an in-depth analysis of silicon-proven\
    \ open-source RTL IPs and achieving the lowest pessimism when compared to similar\
    \ SoA works.\n\n# <span id=\"page-12-5\"></span>**8 CONCLUSIONS**\n\nIn conclusion,\
    \ this is the first work to bridge the gap between open-source hardware and predictability\
    \ modeling and analysis. It presented (i) a fine-grained model and analysis for\
    \ the typical building blocks composing modern heterogeneous low-power SoCs directly\
    \ based on the source RTL, and (ii) a full mathematical analysis to upper bound\
    \ data transfer execution times. Namely, we demonstrated a methodology that successfully\
    \ exploits the availability of the source code to provide safe, but not overly\
    \ pessimistic, upper bounds for the execution times of data transfers when compared\
    \ to similar SoA works based on closed-source IPs.\n\nAs discussed in Section\
    \ [6,](#page-9-0) after this thorough evaluation, we envision extending our results\
    \ to other popular opensource IPs and different arbitration policies. To hopefully\
    \ stimulate novel research contributions, we open-source a guide to replicate\
    \ the results shown in Section [6](#page-9-0) at [https:](https://github.com/pulp-platform/soc_model_rt_analysis)\
    \ [//github.com/pulp-platform/soc](https://github.com/pulp-platform/soc_model_rt_analysis)\
    \ model rt analysis, comprehensive of the simulated environment and the software\
    \ benchmarks to run on a sophisticated Cheshire-based SoC targeting automotive\
    \ applications.\n\n# **REFERENCES**\n\n- <span id=\"page-12-0\"></span>[1] Z.\
    \ Jiang *et al.*, \"AXI-ICRT RT : Towards a Real-Time AXI-Interconnect for Highly\
    \ Integrated SoCs,\" *IEEE Transactions on Computers*, vol. 72, no. 3, pp. 786–799,\
    \ 2022.\n- [2] A. Biondi *et al.*, \"SPHERE: A multi-SoC architecture for nextgeneration\
    \ cyber-physical systems based on heterogeneous platforms,\" *IEEE Access*, vol.\
    \ 9, pp. 75 446–75 459, 2021.\n- <span id=\"page-12-3\"></span>[3] Z. Jiang *et\
    \ al.*, \"BlueScale: a scalable memory architecture for predictable real-time\
    \ computing on highly integrated SoCs,\" in *Proceedings of the 59th ACM/IEEE\
    \ Design Automation Conference*, 2022, pp. 1261–1266.\n- <span id=\"page-12-4\"\
    ></span>[4] F. Restuccia *et al.*, \"AXI HyperConnect: A Predictable, Hypervisorlevel\
    \ Interconnect for Hardware Accelerators in FPGA SoC,\" in *2020 57th ACM/IEEE\
    \ Design Automation Conference (DAC)*, 2020.\n- <span id=\"page-12-1\"></span>[5]\
    \ S. Abdelhalim *et al.*, \"A Tight Holistic Memory Latency Bound Through Coordinated\
    \ Management of Memory Resources,\" in *35th Euromicro Conference on Real-Time\
    \ Systems (ECRTS 2023)*, vol. 262, 2023, pp. 17:1–17:25.\n- [6] G. Fernandez *et\
    \ al.*, \"Contention in multicore hardware shared resources: Understanding of\
    \ the state of the art,\" in *Proceedings of the 14th International Workshop on\
    \ Worst-Case Execution Time Analysis (WCET 2014)*, 2014, pp. 31–42.\n- <span id=\"\
    page-12-2\"></span>[7] S. Hahn, M. Jacobs, and J. Reineke, \"Enabling Compositionality\
    \ for Multicore Timing Analysis,\" in *Proceedings of the 24th International Conference\
    \ on Real-Time Networks and Systems*. Association for Computing Machinery, 2016,\
    \ p. 299–308.\n- <span id=\"page-13-0\"></span>[8] M. Hassan and R. Pellizzoni,\
    \ \"Bounding DRAM Interference in COTS Heterogeneous MPSoCs for Mixed Criticality\
    \ Systems,\" *IEEE Transactions on Computer-Aided Design of Integrated Circuits\
    \ and Systems*, vol. 37, no. 11, pp. 2323–2336, 2018.\n- <span id=\"page-13-5\"\
    ></span>[9] F. Restuccia *et al.*, \"Bounding Memory Access Times in Multi-Accelerator\
    \ Architectures on FPGA SoCs,\" *IEEE Transactions on Computers*, vol. 72, no.\
    \ 1, pp. 154–167, 2022.\n- <span id=\"page-13-1\"></span>[10] Z. Wu, M. Bekmyrza,\
    \ N. Kapre, and H. Patel, \"Ditty: Directorybased Cache Coherence for Multicore\
    \ Safety-critical Systems,\" in *2023 Design, Automation & Test in Europe Conference\
    \ & Exhibition (DATE)*. IEEE, 2023, pp. 1–6.\n- <span id=\"page-13-2\"></span>[11]\
    \ M. Hassan, \"On the Off-Chip Memory Latency of Real-Time Systems: Is DDR DRAM\
    \ Really the Best Option?\" in *2018 IEEE Real-Time Systems Symposium (RTSS)*,\
    \ 2018, pp. 495–505.\n- <span id=\"page-13-3\"></span>[12] R. Mirosanlou, D. Guo,\
    \ M. Hassan, and R. Pellizzoni, \"Mcsim: An extensible dram memory controller\
    \ simulator,\" *IEEE Computer Architecture Letters*, vol. 19, no. 2, pp. 105–109,\
    \ 2020.\n- <span id=\"page-13-4\"></span>[13] M. Zini, D. Casini, and A. Biondi,\
    \ \"Analyzing Arm's MPAM From the Perspective of Time Predictability,\" *IEEE\
    \ Transactions on Computers*, vol. 72, no. 1, pp. 168–182, 2023.\n- <span id=\"\
    page-13-6\"></span>[14] A. Herrera, \"The Promises and Challenges of Open Source\
    \ Hardware,\" *Computer*, vol. 53, no. 10, pp. 101–104, 2020.\n- <span id=\"page-13-10\"\
    ></span>[15] A. Ottaviano, T. Benz, P. Scheffler, and L. Benini, \"Cheshire: A\
    \ Lightweight, Linux-Capable RISC-V Host Platform for Domain-Specific Accelerator\
    \ Plug-In,\" *IEEE Transactions on Circuits and Systems II: Express Briefs*, pp.\
    \ 1–1, 2023.\n- <span id=\"page-13-7\"></span>[16] L. Valente *et al.*, \"Shaheen:\
    \ An Open, Secure, and Scalable RV64 SoC for Autonomous Nano-UAVs,\" in *2023\
    \ IEEE Hot Chips 35 Symposium (HCS)*, 2023, pp. 1–12.\n- <span id=\"page-13-8\"\
    ></span>[17] M. B. Taylor, \"Your Agile Open Source HW Stinks (Because It Is Not\
    \ a System),\" in *2020 IEEE/ACM International Conference On Computer Aided Design\
    \ (ICCAD)*, 2020, pp. 1–6.\n- <span id=\"page-13-9\"></span>[18] PULP, \"PULP\
    \ Platform Github,\" [https://github.com/](https://github.com/pulp-platform) [pulp-platform,](https://github.com/pulp-platform)\
    \ 2022.\n- <span id=\"page-13-11\"></span>[19] L. Valente *et al.*, \"HULK-V:\
    \ a Heterogeneous Ultra-low-power Linux capable RISC-V SoC,\" in *2023 Design,\
    \ Automation & Test in Europe Conference & Exhibition (DATE)*, 2023, pp. 1–6.\n\
    - <span id=\"page-13-12\"></span>[20] OpenHW-Group, \"CVA6,\" [https://github.com/openhwgroup/](https://github.com/openhwgroup/cva6)\
    \ [cva6,](https://github.com/openhwgroup/cva6) 2022.\n- <span id=\"page-13-13\"\
    ></span>[21] M. Schneider *et al.*, \"Composite Enclaves: Towards Disaggregated\
    \ Trusted Execution,\" *IACR Transactions on Cryptographic Hardware and Embedded\
    \ Systems*, vol. 2022, no. 1, p. 630–656, Nov. 2021.\n- <span id=\"page-13-14\"\
    ></span>[22] P. Platform, \"PULP cluster,\" [https://github.com/pulp-platform/](https://github.com/pulp-platform/pulp_cluster)\
    \ pulp [cluster,](https://github.com/pulp-platform/pulp_cluster) 2022.\n- <span\
    \ id=\"page-13-15\"></span>[23] OpenHW-Group, \"CV32E40P,\" [https://github.com/](https://github.com/openhwgroup/cv32e40p)\
    \ [openhwgroup/cv32e40p,](https://github.com/openhwgroup/cv32e40p) 2023.\n- <span\
    \ id=\"page-13-16\"></span>[24] A. Kurth *et al.*, \"An Open-Source Platform for\
    \ High-Performance Non-Coherent On-Chip Communication,\" *IEEE Transactions on\
    \ Computers*, pp. 1–1, 2021.\n- <span id=\"page-13-17\"></span>[25] B. John, \"\
    HyperRAM as a low pin-count expansion memory for embedded systems,\" [https://www.infineon.com/dgdl/](https://www.infineon.com/dgdl/Infineon-HyperRAM_as_a_low_pin-count_expansion_memory_for_embedded_systems-Whitepaper-v01_00-EN.pdf?fileId=8ac78c8c7d0d8da4017d0fb28970272c&da=t)\
    \ [Infineon-HyperRAM](https://www.infineon.com/dgdl/Infineon-HyperRAM_as_a_low_pin-count_expansion_memory_for_embedded_systems-Whitepaper-v01_00-EN.pdf?fileId=8ac78c8c7d0d8da4017d0fb28970272c&da=t)\
    \ as a low pin-count expansion memory for embedded [systems-Whitepaper-v01](https://www.infineon.com/dgdl/Infineon-HyperRAM_as_a_low_pin-count_expansion_memory_for_embedded_systems-Whitepaper-v01_00-EN.pdf?fileId=8ac78c8c7d0d8da4017d0fb28970272c&da=t)\
    \ 00-EN.pdf?fileId= [8ac78c8c7d0d8da4017d0fb28970272c&da=t,](https://www.infineon.com/dgdl/Infineon-HyperRAM_as_a_low_pin-count_expansion_memory_for_embedded_systems-Whitepaper-v01_00-EN.pdf?fileId=8ac78c8c7d0d8da4017d0fb28970272c&da=t)\
    \ 2020.\n- <span id=\"page-13-18\"></span>[26] AMD, \"Zynq-7000 - Technical Reference\
    \ Manual, UG585,\" [https:](https://docs.xilinx.com/r/en-US/ug585-zynq-7000-SoC-TRM)\
    \ [//docs.xilinx.com/r/en-US/ug585-zynq-7000-SoC-TRM.](https://docs.xilinx.com/r/en-US/ug585-zynq-7000-SoC-TRM)\n\
    - <span id=\"page-13-19\"></span>[27] A. Noami, B. Pradeep Kumar, and P. Chandrasekhar,\
    \ \"High Performance AXI4 Interface Protocol for Multi-Core Memory Controller\
    \ on SoC,\" in *Data Engineering and Communication Technology*, K. A. Reddy, B.\
    \ R. Devi, B. George, and K. S. Raju, Eds. Singapore: Springer Singapore, 2021,\
    \ pp. 131–140.\n- <span id=\"page-13-20\"></span>[28] D. Rossi, I. Loi, G. Haugou,\
    \ and L. Benini, \"Ultra-low-latency lightweight dma for tightly coupled multi-core\
    \ clusters,\" in *Proceedings of the 11th ACM Conference on Computing Frontiers*,\
    \ ser. CF '14. New York, NY, USA: Association for Computing Machinery, 2014. [Online].\
    \ Available:<https://doi.org/10.1145/2597917.2597922>\n- <span id=\"page-13-21\"\
    ></span>[29] ARM, \"AMBA AXI Protocol Specification,\" [https://developer.arm.](https://developer.arm.com/documentation/ihi0022/j/?lang=en)\
    \ [com/documentation/ihi0022/j/?lang=en,](https://developer.arm.com/documentation/ihi0022/j/?lang=en)\
    \ 2022.\n- <span id=\"page-13-22\"></span>[30] Xilinx-AMD, \"Dual Port SRAM specifications,\"\
    \ [https://docs.xilinx.com/r/2022.1-English/](https://docs.xilinx.com/r/2022.1-English/ug1483-model-composer-sys-gen-user-guide/Dual-Port-RAM)\
    \ [ug1483-model-composer-sys-gen-user-guide/Dual-Port-RAM.](https://docs.xilinx.com/r/2022.1-English/ug1483-model-composer-sys-gen-user-guide/Dual-Port-RAM)\n\
    - <span id=\"page-13-23\"></span>[31] PULP, \"HyperRAM Controller RTL,\" [https://github.com/](https://github.com/pulp-platform/hyperbus)\
    \ [pulp-platform/hyperbus,](https://github.com/pulp-platform/hyperbus) 2022.\n\
    - <span id=\"page-13-24\"></span>[32] Infineon, \"HyperRAM RTL,\" [https://www.](https://www.infineon.com/dgdl/Infineon-S27KL0641_S27KS0641_VERILOG-SimulationModels-v05_00-EN.zip?fileId=8ac78c8c7d0d8da4017d0f6349a14f68)\
    \ [infineon.com/dgdl/Infineon-S27KL0641](https://www.infineon.com/dgdl/Infineon-S27KL0641_S27KS0641_VERILOG-SimulationModels-v05_00-EN.zip?fileId=8ac78c8c7d0d8da4017d0f6349a14f68)\
    \ S27KS0641 [VERILOG-SimulationModels-v05](https://www.infineon.com/dgdl/Infineon-S27KL0641_S27KS0641_VERILOG-SimulationModels-v05_00-EN.zip?fileId=8ac78c8c7d0d8da4017d0f6349a14f68)\
    \ 00-EN.zip?fileId= [8ac78c8c7d0d8da4017d0f6349a14f68,](https://www.infineon.com/dgdl/Infineon-S27KL0641_S27KS0641_VERILOG-SimulationModels-v05_00-EN.zip?fileId=8ac78c8c7d0d8da4017d0f6349a14f68)\
    \ 2022.\n- <span id=\"page-13-25\"></span>[33] Infineon, \"HyperBUS specifications,\"\
    \ [https://www.](https://www.infineon.com/dgdl/Infineon-HYPERBUS_SPECIFICATION_LOW_SIGNAL_COUNT_HIGH_PERFORMANCE_DDR_BUS-AdditionalTechnicalInformation-v09_00-EN.pdf?fileId=8ac78c8c7d0d8da4017d0ed619b05663)\
    \ [infineon.com/dgdl/Infineon-HYPERBUS](https://www.infineon.com/dgdl/Infineon-HYPERBUS_SPECIFICATION_LOW_SIGNAL_COUNT_HIGH_PERFORMANCE_DDR_BUS-AdditionalTechnicalInformation-v09_00-EN.pdf?fileId=8ac78c8c7d0d8da4017d0ed619b05663)\
    \ SPECIFICATION LOW SIGNAL COUNT HIGH [PERFORMANCE](https://www.infineon.com/dgdl/Infineon-HYPERBUS_SPECIFICATION_LOW_SIGNAL_COUNT_HIGH_PERFORMANCE_DDR_BUS-AdditionalTechnicalInformation-v09_00-EN.pdf?fileId=8ac78c8c7d0d8da4017d0ed619b05663)\
    \ DDR [BUS-AdditionalTechnicalInformation-v09](https://www.infineon.com/dgdl/Infineon-HYPERBUS_SPECIFICATION_LOW_SIGNAL_COUNT_HIGH_PERFORMANCE_DDR_BUS-AdditionalTechnicalInformation-v09_00-EN.pdf?fileId=8ac78c8c7d0d8da4017d0ed619b05663)\
    \ 00-EN.pdf?fileId= [8ac78c8c7d0d8da4017d0ed619b05663,](https://www.infineon.com/dgdl/Infineon-HYPERBUS_SPECIFICATION_LOW_SIGNAL_COUNT_HIGH_PERFORMANCE_DDR_BUS-AdditionalTechnicalInformation-v09_00-EN.pdf?fileId=8ac78c8c7d0d8da4017d0ed619b05663)\
    \ 2022.\n- <span id=\"page-13-26\"></span>[34] R. Wilhelm *et al.*, \"The worst-case\
    \ execution-time problem—overview of methods and survey of tools,\" *ACM Trans.\
    \ Embed. Comput. Syst.*, vol. 7, no. 3, may 2008. [Online]. Available: <https://doi.org/10.1145/1347375.1347389>\n\
    - <span id=\"page-13-27\"></span>[35] T. Mitra, J. Teich, and L. Thiele, \"Time-critical\
    \ systems design: A survey,\" *IEEE Design & Test*, vol. 35, no. 2, pp. 8–26,\
    \ 2018.\n- <span id=\"page-13-28\"></span>[36] F. Restuccia *et al.*, \"Modeling\
    \ and analysis of bus contention for hardware accelerators in FPGA SoCs,\" in\
    \ *32nd Euromicro Conference on Real-Time Systems (ECRTS 2020)*. Schloss Dagstuhl-Leibniz-Zentrum\
    \ fur Informatik, 2020. ¨\n- <span id=\"page-13-32\"></span>[37] B. Thomas *et\
    \ al.*, \"AXI-REALM: A Lightweight and Modular Interconnect Extension for Traffic\
    \ Regulation and Monitoring of Heterogeneous Real-Time SoCs,\" in *2024 Design,\
    \ Automation & Test in Europe Conference & Exhibition (DATE)*, 2024.\n- <span\
    \ id=\"page-13-29\"></span>[38] J. P. Cerrolaza *et al.*, \"Multi-Core Devices\
    \ for Safety-Critical Systems: A Survey,\" *ACM Comput. Surv.*, vol. 53, no. 4,\
    \ aug 2020. [Online]. Available:<https://doi.org/10.1145/3398665>\n- <span id=\"\
    page-13-30\"></span>[39] M. Schoeberl *et al.*, \"T-CREST: Time-predictable multi-core\
    \ architecture for embedded systems,\" *Journal of Systems Architecture*, vol.\
    \ 61, no. 9, pp. 449–471, 2015.\n- <span id=\"page-13-31\"></span>[40] G. Fernandez\
    \ *et al.*, \"Increasing confidence on measurement-based contention bounds for\
    \ real-time round-robin buses,\" in *Proceedings of the 52nd Annual Design Automation\
    \ Conference*, ser. DAC '15. New York, NY, USA: Association for Computing Machinery,\
    \ 2015.\n\n**Luca Valente** received the MSc degree in electronic engineering\
    \ from the Politecnico of Turin in 2020. He is currently a PhD student at the\
    \ University of Bologna in the Department of Electrical, Electronic, and Information\
    \ Technologies Engineering (DEI). His main research interests are hardware-software\
    \ co-design of heterogeneous SoCs.\n\n**Francesco Restuccia** received a PhD degree\
    \ in computer engineering (cum laude) from Scuola Superiore Sant'Anna Pisa, in\
    \ 2021. He is a postdoctoral researcher at the University of California, San Diego.\
    \ His main research interests include hardware security, on-chip communications,\
    \ timing analysis for heterogeneous platforms, cyber-physical systems, and time-predictable\
    \ hardware acceleration of deep neural networks on commercial FPGA SoC platforms.\n\
    \n**Davide Rossi** received the Ph.D. degree from the University of Bologna in\
    \ 2012. He has been a Post-Doctoral Researcher with the Department of Electrical,\
    \ Electronic and Information Engineering \"Guglielmo Marconi,\" University of\
    \ Bologna, since 2015, where he is currently an Associate Professor position.\
    \ His research interests focus on energy-efficient digital architectures. In this\
    \ field, he has published more than 100 papers in international peer-reviewed\
    \ conferences and journals.\n\n**Ryan Kastner** is a professor in the Department\
    \ of Computer Science and Engineering at UC San Diego. He received a PhD in Computer\
    \ Science at UCLA, a masters degree in engineering and bachelor degrees in Electrical\
    \ Engineering and Computer Engineering from Northwestern University. His current\
    \ research interests fall into three areas: hardware acceleration, hardware security,\
    \ and remote sensing.\n\n**Luca Benini** holds the chair of Digital Circuits and\
    \ Systems at ETHZ and is Full Professor at the Universita di Bologna. He received\
    \ a PhD from ` Stanford University. His research interests are in energy-efficient\
    \ parallel computing systems, smart sensing micro-systems and machine learning\
    \ hardware. He has published more than 1000 peer-reviewed papers and 5 books.\
    \ He is a Fellow of the ACM and a member of the Academia Europaea."
  decisions:
    evaluation_prompt: 'Qualified. Reason: All relevant sections passed.'
    related_work_prompt: 'Qualified. Reason: All relevant sections passed.'
    novelty_prompt: 'Qualified. Reason: All relevant sections passed.'
    review_only_prompt: 'Qualified. Reason: All relevant sections passed.'
  topics:
    main_topic: Computer Architecture
    main_topic_reasoning: The paper focuses on the modeling and analysis of open-source
      SoCs, addressing the architecture of hardware components which is a core aspect
      of computer architecture. It discusses the interplay of interconnects and shared
      memory resources in system design, which falls directly under the study of computer
      architecture.
    secondary_topic: Embedded & Real-time Systems
    secondary_topic_reasoning: The paper also deals with real-time systems and the
      predictability of system behavior, particularly in cyber-physical systems, making
      it relevant to the field of embedded and real-time systems.
    main_topic_sub: Architecture modeling and simulation methodologies
    secondary_topic_sub: Other
- title: "Green Adaptation of Real-Time Web Services for Industrial CPS within a\n\
    \  Cloud Environment"
  abstract: 'Managing energy efficiency under timing constraints is an interesting
    and big

    challenge. This work proposes an accurate power model in data centers for

    time-constrained servers in Cloud computing. This model, as opposed to previous

    approaches, does not only consider the workload assigned to the processing

    element, but also incorporates the need of considering the static power

    consumption and, even more interestingly, its dependency with temperature. The

    proposed model has been used in a multi-objective optimization environment in

    which the Dynamic Voltage and Frequency Scaling (DVFS) and workload assignment

    have been efficiently optimized.'
  url: http://arxiv.org/abs/2401.16387v1
  keywords: Adaptive Systems, Cyber Physical Systems, Cloud Computing, Real-Time Systems,
    Energy efficiency, Industrial-based Services, Multi-Objective Optimization, Parallel
    Computing.
  document: '#### I. INTRODUCTION


    B OTH *Cyber Physical Systems* (CPSs) and *Cyber Physical Society* [\[1\]](#page-7-0)
    combine computing and networking power with physical components, enabling innovation
    in a wide range of domains related to future-generation sensor networks (e.g.,
    robotics, avionics, transportation, manufacturing processes, energy, smart homes
    and vehicles, medical implants, healthcare, etc). The design and implementation
    of CPS involve the consideration of multiple aspects like energy and tight realtime
    constraints. Because of that, real-time scheduling for CPS brings new research
    issues in the scope of real-time systems [\[2\]](#page-7-1).


    Managing energy efficiency under timing constraints is a big challenge. Most modern
    micro-controllers already provide support for various energy saving modes (e.g.,
    Intel Xeon and AMD Opteron). A common way of reducing dynamic power is to use
    the technique called *Dynamic Voltage and Frequency Scaling* (DVFS), which changes
    the processor voltage and the clock frequency simultaneously, reducing the energy
    consumption. Decreasing the processor voltage and frequency will slow down the
    performance of the processor. If the execution performance is not a hard constraint,
    then, decreasing both processor voltage and frequency allows to reduce the dynamic
    power consumption of the processor.


    Copyright (c) 2009 IEEE. Personal use of this material is permitted. However,
    permission to use this material for any other purposes must be obtained from the
    IEEE by sending a request to pubs-permissions@ieee.org.


    Nowadays, new embedded devices are collaborating in distributed environments.
    In this new scenario, tasks and resources are widely distributed and then, real-time
    applications become more complex and more relevant. A cloud datacenter usually
    contains a large group of servers connected through the Internet, and a scheduler
    has to make an efficiently use of the resources of the cloud to execute jobs.
    Since many applications require *Quality of Service* (QoS), power consumption
    in data centers must be minimized, satisfying the *Service Level Agreement* (SLA)
    constraints. Consequently, novel approaches that base their optimizations on accurate
    power models must be devised, performing an optimized setting of the parameters
    of the server (frequency, voltage, workload allocation, etc) while accomplishing
    with time requeriments and a wide range of real-time constraints.


    DVFS-based solutions for distributed real-time environments identify two main
    dimensions of the problem: *(i)* taskto-*Central-Processing-Unit* (CPU) allocation
    and *(ii)* run-time voltage scaling on individual CPUs. In CPS, physical factors
    (e.g., the network topology of CPS may dynamically change due to physical environments)
    are not entirely predictable and may lead to problems such as missed task deadlines,
    that can impact dramatically on economic loss for individuals or for the industry.
    Moreover, a critical task deadline missed could trigger a disaster (e.g., humans
    life loss, natural disasters, or huge economic loss).


    In this paper, we propose a method for solving such CPS problems by introducing
    new adaptive real-time scheduling algorithms in distributed computing infrastructures
    that also consider energy efficiency. This scheme requires to know *a priori*
    the processing and timing constraints of the set of tasks, and must be supported
    by reservation-based real-time operating systems.


    The remainder of this paper is organized as follows: after a brief summary of
    the previous works in this field (Section [II\)](#page-0-0), a real-time scheduling
    algorithm for CPS is sketched (Section [III\)](#page-1-0). Following, the devised
    power model is presented (Section [IV\)](#page-3-0), and the optimization of the
    algorithm developed is profusely described (Section [V\)](#page-4-0). Experimental
    results can be found in Section [VI.](#page-5-0) Finally, some conclusions are
    drawn (Section [VII\)](#page-7-2).


    #### II. RELATED WORK


    <span id="page-0-0"></span>The energy-efficient scheduling problem in real-time
    systems consists in minimizing the energy consumption while ensuring that all
    the real-time tasks meet their deadlines. The work presented in [\[3\]](#page-7-3)
    is based on the observation that a


    M. Teresa Higuera-Toledano, Jose L. Risco-Mart ´ ´ın, and Jose L. Ayala are ´
    with the Department of Computer Architecture and Automation, Complutense University
    of Madrid, C/Prof. Jose Garc ´ ´ıa Santesmases 9, 28040 Madrid, Spain, email:
    mthiguer,jlrisco,jlayalar@ucm.es


    Patricia Arroba is with the Department of Electronic Engineering, Technical University
    of Madrid, Avda. Complutense 30, 28040, Madrid, Spain, email: parroba@die.upm.es


    significant percentage of time spent in idle mode is due to the accumulation of
    small gaps between tasks. Whether the gap between the activation of two periodic
    tasks is less than transition-time from idle to deep-sleep, the processor is not
    able to transition to the deep-sleep state even though there is no useful work
    to be done, and continues in the idle energy state all the time.


    There are extensive research works on energy-aware realtime scheduling by using
    DVFS (e.g., [\[4\]](#page-7-4)). Different works using this technique within a
    real-time context, considered an offline scheduling algorithm and a set of a periodic
    jobs on an ideal processor. Each job is characterized by its release time, deadline,
    and execution CPU cycles, and all jobs have the same power consumption function.


    Several papers have also proposed DVFS-based solutions for real-time multi-processor
    systems. As the complexity of CPS increases, *Chip Multicore Processors* (CMP)
    and parallel tasks scheduled in a real-time way are needed. The fact that the
    processing cores share a common voltage level makes the CMP energy-efficiency
    problem different from multi-processor platforms. The work presented in [\[5\]](#page-7-5)
    provides a simple, elegant and effective solution on energy-efficient real-time
    scheduling on CMP. This solution addresses fixed-priority scheduling of periodic
    real-time tasks having a deadline equal to their period. Note that this problem
    is NP-hard.


    The load balancing in CMP is particularly important because the main contributor
    to the overall energy consumption in the system is the core with the maximum load.
    This fact is given by the global voltage/frequency constraint. Considering a CMP
    system with a workload perfectly balanced across the processors, the *Earliest
    Deadline First* (EDF) scheduling minimizes the total energy consumption. This
    is not the case of *Rate Monotonic Scheduling* (RMS) where load-balancing does
    not always result in lowering energy consumption [\[5\]](#page-7-5).


    In mixed-criticality systems, varying degrees of assurance must be provided to
    functionalities of varying importances. As shown in [\[6\]](#page-7-6) there is
    a conflict between safety and energy minimization because critical tasks must
    meet their deadlines even whether exceeding their expected *Worts Case Execution
    Time* (WCET). This work integrates continuous DVFS with the EDF with *Virtual
    Deadlines* (EDF-VD) scheduling for mixedcriticality systems [\[7\]](#page-7-7)
    and shows that speeding up the system to handle overrun is beneficial for minimizing
    the expected energy consumption of the system.


    Generally, large-scale distributed applications require realtime responses to
    meet soft deadlines. Hence, the middleware coordinates resource allocation in
    order to provide services accomplishing with SLA requirements. In [\[8\]](#page-7-8),
    we can find a scheduling algorithm based on DVFS for clusters, which develops
    a green SLA-based mechanism to reduce energy consumption by increasing the scheduling
    makespans. In [\[9\]](#page-7-9), we can find an energy-aware resource allocation
    for Cloud computing with negotiated QoS. However, similarly to the solution presented
    in [\[8\]](#page-7-8), this method sacrifices system performance.


    The work presented in [\[10\]](#page-7-10) proposes a priority-based scheduler,
    which satisfies the minimum resource requirement of a job by selecting a *Virtual
    Machine* (VM) according to both the SLA level and the W<sup>i</sup> parameter
    that is described as W<sup>i</sup> = P<sup>i</sup> × R<sup>i</sup> , where P<sup>i</sup>
    is the unit power cost of V M<sup>i</sup> , and R<sup>i</sup> defines the resources
    used by the V M<sup>i</sup> .


    The location of nodes in CPS affects the effective release time and deadline of
    real-time tasks, which may be different depending on the node location and the
    migration delay time among the network nodes. Because of that, traditional realtime
    scheduling algorithms have to be modified to include the location node and the
    spatial factors. The work presented in [\[11\]](#page-7-11) proposes a CPS scheduling
    algorithm, where the servicing node (i.e., the CPU) needs to move to serviced
    (i.e., the executed Job) node for real-time services.


    The power modeling technique proposed in [\[12\]](#page-7-12) is most relevant
    for us. A correlation between the total system''s power consumption and the component
    utilization is observed, defining a four-dimensional linear weighted power model
    for the total power consumed (i.e., P = c0+c1PCP U +c2Pcache+ c3PDRAM + c4Pdisk).
    Our work follows a similar approach but also incorporates the contribution of
    the static power consumption, its dependency with temperature, and the effect
    of applying DVFS techniques.


    Static power consumption has a high impact on energy, due to the temperature-dependent
    leakage currents. In this manner, novel optimizations may be devised by quantitatively
    understanding the power-thermal trade-offs of a system, thus developing analytical
    models.


    Finally, Rafique et al. [\[13\]](#page-7-13) makes a description of the complexity
    of the power management and allocation challenge. Authors demonstrate that achieving
    an optimal allocation depends on many factors as the server''s maximum and minimum
    frequencies, the job''s arrival rate, and consequently, the relationship between
    power and frequency. They conduct a set of experiments that provides significant
    savings in terms of energy in both homogeneous and heterogeneous clusters. However,
    our work presented in this paper outperforms these savings by exploiting a multi-objective
    optimization strategy to help to minimize the servers'' power for time-constrained
    Cloud applications.


    #### <span id="page-1-0"></span>III. THE INDUSTRIAL SERVICES EXECUTION MODEL


    CPS comprise a large number of sensors and actuators, and computing units that
    exchange different types of data, some of these interactions have real-time constraints.
    Real-time system abstraction and hybrid system modeling and control are among
    the CPS research challenges. The hybrid system model of CPS requires the design
    and integration of both the physical and computational (i.e., cyber) elements.
    While physical elements behave in continuous real-time, computational elements
    change according to discrete logic. This fact requires to merge continuous-time
    based systems with event-triggered logical systems, and also we must address the
    dimensional scale (i.e., from on-chip level to the cloud). Moreover, the interaction
    with physical world introduces uncertainty in CPS because of randomness in the
    environment, errors in physical devices, and security attacks.


    Control and scheduling co-design is a well-known area in the embedded real-time
    systems'' community. However, since CPS are typically networked control systems,
    the tradeoff between the effects of the network must be included in the real-time
    schedulability, that results in a non-periodic control approach. In this work,
    we study how to guarantee the overall system stability with minimum computational
    resource and power usage. System properties and requirements (e.g., the control
    laws, real-time and power constraints) must be captured and supported by data
    abstractions encapsulated in components.


    #### *A. Task characterization*


    Typically CPS''s are composed of hard real-time tasks and feedback control tasks.
    Whereas real-time tasks present time constraints (i.e., deadlines) that must always
    be satisfied, feedback control tasks are characterized by their *Quality of Control*
    (QoC), which needs to be optimized. A typical approach to the above scheduling
    problem is to translate the QoC requirements into time constraints and then, to
    apply traditional real-time scheduling techniques [\[14\]](#page-7-14). Real-time
    systems are structured as a set of schedulable tasks, where parameters used for
    the scheduling (e.g., execution time, deadline, or period) are a priori known
    and clearly defined. However, this solution is very conservative and consequently
    it is not efficient for CPS.


    An alternative solution is the given in [\[15\]](#page-7-15), that deals with
    this problem using a *multi-layered* scheme based on mixed-critical real-time
    systems: *(i)* for real-time tasks it uses triggering patterns (i.e., uses arrival
    curves), which allow a more general characterization regarding the classical real-time
    task models (i.e., *periodic* or *sporadic*), and *(ii)* for control tasks, it
    is based on three QoC-oriented metrics. Mixedcritical real-time systems literature
    focuses on tasks with different criticality levels and certification issues[1](#page-2-0)
    , providing heterogeneous timing guarantees for tasks of varying criticality levels.


    As an example, in the *Unmanned Aerial Vehicles* (UAVs), functionalities can be
    categorized as safety-critical tasks (e.g., like flight control and trajectory
    computation) or missioncritical tasks (e.g., object tracking for surveillance
    purposes). Note that the system is still safe although mission-critical functionalities
    can be lost. This makes the design parameters for safety-critical tasks (e.g.,
    WCET) much more pessimistic than those for mission-critical tasks. However, in
    CPS, tasks are not characterized by criticality levels, but by their criticality
    types.


    There has been considerable research on schedule synthesis for control applications.
    However, these works are particularly centered on control/scheduling co-design
    for optimized QoC, and only deal with control tasks. On the other hand, CPS focus
    on mixed task sets comprising of feedback control tasks and hard real-time tasks,
    which requires a joint schedule synthesis.


    #### *B. The task model*


    In CPS, tasks may be classified according to their criticality types (e.g., deadline-critical
    real-time tasks and QoC-critical


    <span id="page-2-0"></span><sup>1</sup>When there are tasks with different safety
    requirements into the same real-time platform, it is called mixed-criticality
    system.


    feedback control tasks). While the system must satisfy always the deadlines of
    real-time tasks, particularly for those that are critical, only the QoC parameters
    for control tasks need to be optimized. In order to do that, we require stochastic
    hybrid systems to identify the interaction between continuous dynamical physical
    models and discrete state machines, and the CPS architecture must follow the new
    paradigm *"globally virtual, locally physical"*.


    We consider a set of independent tasks, (i.e., Σ) which are executed remotely
    in a set of physical servers m. We define our real-time problem as a pair P =
    (Σ, S) where S is a scheduling solution and Σ = τ1, ..., τ<sup>n</sup> is a set
    of n tasks with different timing characteristics (i.e., strict, flexible, and
    firm) as shows Figure [1.](#page-2-1)


    ![](_page_2_Figure_12.jpeg)


    <span id="page-2-1"></span>Fig. 1. An overrun in response time (i.e., a deadline
    miss) has a different value function depending on its possible consequences


    Each task τ<sup>i</sup> is a possibly infinite sequence of jobs (i.e., demands
    for processing time), each one with an associated deadline. Jobs of the same task
    must be executed sequentially and in First-In-First-Out (FIFO) order. If the timing
    characteristics of the task τ<sup>i</sup> are soft or firm, the jobs may be not
    identical.


    CPS requires jointly scheduling hard real-time, soft realtime or best-effort,
    and control-feedback tasks. Due to the stringent stability requirements, we classify
    control tasks as firm deadline. While a hard deadline cannot be missed, soft deadlines
    may be occasionally missed and it does not harm the system safety. Similarly,
    firm deadlines can be missed but there is an upper limit on the number of misses
    within a given time interval. However, as we aim to optimize the QoC, we must
    minimize the number of deadline misses to avoid QoC degradation. The characterization
    of each type of task is fundamentally different as follows.


    *1) Hard real-time tasks:* A real-time system is considered *hard* if an overrun
    in a task response time leads to potential loss of life and/or big financial damage.
    The system is considered to be safety critical or high integrity, and is often
    mission critical. We consider a real-time task as a tuple τ<sup>i</sup> = (R<sup>i</sup>
    , C<sup>i</sup> , T<sup>i</sup> , Di) where:


    - Ri is the first release time of the task (i.e. the *phase* of the task),

    - Ci is the WCET,

    - Ti is the activation period (i.e., minimum inter-release time), and


    D<sup>i</sup> is the relative deadline (r<sup>i</sup> ≤ D<sup>i</sup> ≤ Ti). The
    absolute deadline is the relative deadline plus the current arrival time.


    We compute the CPU utilization factor of τ<sup>i</sup> as U<sup>i</sup> = C<sup>i</sup>
    Ti .


    *2) Soft real-time tasks:* For *soft* real-time tasks, deadline overruns are tolerable
    but not desired (i.e., there are not catastrophic consequences of missing one
    or more deadlines). There is a cost function associated with these systems, which
    is often related to QoS. Hence, we consider a stochastic task model based on the
    one presented in [\[16\]](#page-7-16). Then, we represent each soft-real-time
    task using a tuple τ<sup>i</sup> = (r<sup>i</sup> , s<sup>i</sup> , a<sup>i</sup>
    , di) where:


    - ri is the release time of the task,

    - si is the service time, which follows an exponential distribution of average
    µ −1 (i.e., µ is the number of serviced jobs of τ<sup>i</sup> per unit time),

    - ai is the arrival time; tasks arrive according to a renewal process with exponential
    distribution of average λ −1 , and

    - di is the absolute deadline; the relative deadline is D<sup>i</sup> = d<sup>i</sup>
    − a<sup>i</sup> , D<sup>i</sup> distributed on [0, D].


    We compute the response time of τ<sup>i</sup> as ρ<sup>i</sup> = c<sup>i</sup>
    − a<sup>i</sup> , where c<sup>i</sup> is the completion time (i.e., c<sup>i</sup>
    = a<sup>i</sup> + si). The average CPU utilization factor is given by Υ<sup>i</sup>
    = µi λi .


    *3) Feedback control tasks:* For a *firm* real-time task the computation is obsolete
    whether the job is not finished on time. In this case, the cost function may be
    interpreted as loss of value associated to QoC. This is the case of the feedback
    control task in CPS. For this kind of task we can consider D<sup>i</sup> ≥ Ti)
    to guarantee that the controlled physical tasks are still stable in the worst
    case scenario. However, this sacrifices the system performance and also may result
    unstable under physical perturbations.


    In most cases, feedback control systems become unstable with too many missed control
    cycles. Therefore, a critical question is how to determine T<sup>i</sup> to ensure
    both schedulability and robustness of the physical system. Considering a simple
    proportional-gain feedback controller, which is fixed for each control task, in
    order to determine T<sup>i</sup> , we can find the minimum T<sup>i</sup> ∈ (T1,
    T2, . . . , Tn) under the following constraints:


    $$0 \le \quad \sum\_{i} \frac{C\_i}{T\_i} \quad \le p \tag{1}$$


    $$C\_i \le \quad \quad T\_i \qquad \le D\_i \tag{2}$$


    where p < 1 is a priori known. However, some controller parameters may need to
    be adjusted when the task period is changed. Alternatively, we can use a multiple-versions
    approach or a predictive model with a quadratic optimization computed iteratively
    for each job. However, very often, probabilistic guarantees are sufficient (e.g.,
    t out of k deadlines have to be met).


    Permitting skips in periodic tasks increases the system flexibility [\[17\]](#page-7-17).
    The maximum number of skipped jobs for each task can be controlled by a specific
    parameter S<sup>i</sup> associated with the task, which gives the minimum distance
    between two consecutive jobs skips (e.g., if (S<sup>i</sup> = 3) the task can
    skip one job every three). This parameter can be considered as a QoC metric (i.e.,
    the higher S, the better QoC).


    When S<sup>i</sup> = ∞ no skips are allowed, meaning that τ<sup>i</sup> is a real-time
    hard periodic task. We then consider a control task as a tuple τ<sup>i</sup> =
    (R<sup>i</sup> , C<sup>i</sup> , T<sup>i</sup> , D<sup>i</sup> , Si) where T<sup>i</sup>
    = D<sup>i</sup> .


    ## *C. The parallel scheduling*


    For each of the above described tasks τ<sup>i</sup> ∈ Σ, we consider a set of
    independent subtasks τ<sup>i</sup> = τi,1, ..., τi,q, where τi,j denotes the subtasks
    j of task τ<sup>i</sup> . Therefore, e<sup>i</sup> ≥ 0, is the energy consumption
    rate of the task τ<sup>i</sup> per time unit:


    $$e\_i \quad = \bigcup\_{j=1}^{|\tau\_i|} e\_{i,j} \tag{3}$$


    The scheduling allocates each τi,j subtask in a set of m physical servers, taken
    into account the critical timing characteristics of each task τ<sup>i</sup> and
    the minimal energy consumption of the task set Σ.


    The performance criteria generally used in systems when the model task does not
    have explicit deadlines, is to minimize the task delay (i.e., the response time
    of all tasks). However, when there are explicit deadlines, we must ensure that
    critical tasks fulfill their deadline and minimize the fraction of non-critical
    tasks that do not meet their timing requirements.


    We can consider lateness constraints of the form α(x) ≤ β, where α(x) is the fraction
    of jobs that miss their deadline by more than x time units. Here, missing a deadline
    by x time units is considered as a failure.


    - For firm deadlines, we require that α(0) ≤ β (i.e., the fraction of tasks missing
    their deadliness were limited to β). Note that this has a different meaning for
    the S parameter, which is the minimal distance between the consecutive misses
    of the task τ<sup>i</sup> . Hence, we consider a τ<sup>i</sup> missing whether
    one or more subtasks τi,j of a job miss the deadline.

    - For hard real-time tasks, we establish α(0) ≤ 0 (i.e., we do not tolerate any
    deadline missed), while for each control task τ<sup>i</sup> , α(0) ≤ Si−1 S<sup>i</sup>
    .

    - For soft real-time tasks, we generalize, α(xi) ≤ β<sup>i</sup> , for a set of
    time values x1, ..., x<sup>p</sup> and constraint specifications β1, ..., βp,
    where 1 ≤ i ≤ p, which allows to take into account the stochastic nature of task
    arrivals and service time of soft real-time tasks.


    #### IV. POWER AND ENERGY MODEL


    <span id="page-3-0"></span>Traditionally in electronic systems, dynamic consumption
    has been the major contributor to the power budget. In contrast, when scaling
    technology below 100nm, static consumption reaches the 30−59% of the total power,
    thus becoming much more significant [\[18\]](#page-7-18). Moreover, the exponential
    impact of temperature on leakage currents intensifies this effect. Thus, modeling
    leakage will allow the exploitation of the trade-offs between leakage and temperature
    at the server level when taking decisions on resource configuration and selection.


    Therefore, the impact of static consumption must be considered, taking into account
    its correlation with temperature. This section presents our leakage-aware static
    power model. We validate this model using real data gathered from real machines
    of our case study (e.g., Intel Xeon and AMD Opteron).


    ## *A. Leakage power*


    Equation [\(4\)](#page-4-1) shows the impact of leakage on the currents in a MOS
    device. Rabaey demonstrates in his work that, when VDS > 100mV , the second exponential
    may be considered negligible [\[19\]](#page-7-19). Consequently, the previous
    equation may be revised as in [\(5\)](#page-4-1), also regrouping technological
    parameters together obtaining the formula presented in equation [\(6\)](#page-4-1).


    <span id="page-4-1"></span>

    $$I\_{leak} = \left. I\_s \cdot e^{\frac{V\_{GS} - V\_{TH}}{nkT/q}} \cdot \left(1
    - e^{\frac{Vds}{kT/q}}\right) \right. \tag{4}$$


    $$I\_{leak} = \begin{array}{c c} I\_s \cdot e^{\frac{V\_{GS} - V\_{TH}}{nkT/q}}
    \\ \dots \end{array} \tag{5}$$


    $$I\_{leak} = -B \cdot T^2 \cdot e^{\frac{\cdot \cdot \omega\_s \cdot \cdot \mu
    \cdot H}{nkT/q}} \tag{6}$$


    The leakage power consumption for the physical machine m ∈ {1, . . . , M} presented
    in Equation [\(8\)](#page-4-2) can be inferred from the expression in [\(7\)](#page-4-2).
    Then, the expansion of the mathematical expression in its Taylor 3rd order series
    provides Equation [\(9\)](#page-4-2), where Bm, C<sup>m</sup> and D<sup>m</sup>
    represent the technological constants of the server.


    <span id="page-4-2"></span>

    $$P\_{leak,m} = \ \ I\_{leak,m} \cdot V\_{DD,m} \tag{7}$$


    $$\begin{array}{rcl}P\_{leak,m} & = & B\_m \cdot T\_m^2 \cdot e^{\frac{V\_{GS}
    - V\_{TH}}{nkT/q}} \\ P\_{leak,m} & = & B\_m \cdot T\_m^2 \cdot V\_{DD,m} \end{array}
    \tag{8}$$


    $$+\quad C\_m \cdot T\_h \stackrel{\cdots}{\cdot} V\_{DD,m}^2 + D\_m \cdot V\_{DD,m}^3
    \qquad (9)$$


    #### *B. Dealing with DVFS*


    The main contributors to energy consumption in nowadays servers are CPU and memory
    devices. Despite DVFS is easily found in CPUs, there are still few memories with
    these capabilities. However, memory consumption in some cases (memoryintensive
    applications) is very significant compared to the CPU consumption and, because
    of this, it was considered important enough to be studied independently.


    Equation [10](#page-4-3) provides the consumption of a physical server that has
    k ∈ {1 . . . K} DVFS modes, while memory remains at a constant voltage. This expression
    takes into account the impact of temperature on the static power contribution.
    We define E<sup>m</sup> as the contribution of other server resources operating
    at constant values of frequency and voltage.


    <span id="page-4-3"></span>

    $$\begin{aligned} P\_{leak,mk} &= -B\_m \cdot T\_{CPU,m}^2 \cdot V\_{DD,mk} \\
    &+ \quad C\_m \cdot T\_{CPU,m} \cdot V\_{DD,mk}^2 + D\_m \cdot V\_{DD,mk}^3 \\
    &+ \quad E\_m + G\_m \cdot T\_{MEM,m}^2 + H\_m \cdot T\_{MEM,m} \\ \end{aligned}$$


    In order to measure temperature-dependent leakage we must understand also the
    dynamic contribution of the server''s power consumption. To maintain constant
    conditions, we use *lookbusy* [2](#page-4-4) , which is a synthetic application
    that stresses the CPU during specifics periods of time. *Lookbusy* is able to
    stress, not only the cores but also the hardware threads of the CPU at a precise
    utilization, having no impact on memory or disk devices. Synthetic workloads help
    us to maintain the utilization rate constant (in terms of instructions per cycle),


    <span id="page-4-4"></span><sup>2</sup>http://www.devin.com/lookbusy/


    thus revealing the leakage contribution due to temperature variations. The formulation
    of the dynamic power consumption is shown in Equation [11.](#page-4-5)


    <span id="page-4-5"></span>

    $$P\_{CPU,dyn,imk} = \ = A\_m \cdot V\_{DD,mk}^2 \cdot f\_{mk} \cdot u\_{CPU,imk}
    \tag{11}$$


    where A<sup>m</sup> defines the technological constant of the physical machine
    m and fmk and VDD,mk are respectively the frequency and the supply voltage at
    the k DVFS mode of the CPU. uCP U,imk represents the CPU utilization and it is
    correlated with the number of CPU cycles.


    #### *C. Energy model*


    So far, the power model is derived as in [\(12\)](#page-4-6).


    <span id="page-4-6"></span>

    $$\begin{array}{rcl} P\_{tot,mk} & = & A\_m \cdot V\_{DD,mk}^2 \cdot f\_{mk} \cdot
    \sum\_i u\_{CPU,imk} \\ & & + & B\_m \cdot T\_{CPU,m}^2 \cdot V\_{DD,mk} \\ &
    & + & C\_m \cdot T\_{CPU,m} \cdot V\_{DD,mk}^2 \\ & & + & D\_m \cdot V\_{DD,mk}^3
    + E\_m \end{array}$$


    The corresponding energy model can be easily obtained taking into account that
    E = P × t, being P the power model in [\(12\)](#page-4-6) and t, the execution
    time. Thus, the total energy consumed per host is described as the summation of
    the following equations:


    <span id="page-4-7"></span>

    $$\begin{array}{rcl} E\_{CPU,dyn,mk} & = & A\_m \cdot V\_{DD,m}^2 \cdot CPI \\
    & & \cdot \sum\_i u\_{CPU,imk} \cdot n\_{CPU,imk} \\ E\_{leak,mk} & = & [ \\ &
    & B\_m \cdot T\_{CPU,m}^2 \cdot V\_{DD,m} \\ & & + & C\_m \cdot T\_{CPU,m} \cdot
    V\_{DD,m}^2 + D\_m \cdot V\_{DD,m}^3 \\ & + & E\_m + G\_m \cdot T\_{MEM,m}^2 +
    H\_m \cdot T\_{MEM,m} \\ & & & 1 \cdot CPI \cdot \sum \end{array}$$


    where


    • CP I is the number of cycles per instruction


    fmk · X i


    ] ·


    • nCP U,imk is the number of CPU instructions of each task i assigned to be executed
    in a specific server m and DVFS mode k.


    nCP U,imk (14)


    The summation of both the instructions to execute and the resources used by the
    workload hosted on the server are needed in order to get the execution time of
    all tasks executed in parallel considering the resources offered by each server,
    as seen in [\(14\)](#page-4-7).


    <span id="page-4-8"></span>

    $$E\_{tot} = \sum\_{mk} \left( E\_{CPU, dyn, mk} + E\_{leak, mk} \right) \qquad
    (15)$$


    #### V. MULTI-OBJECTIVE OPTIMIZATION ALGORITHM


    <span id="page-4-0"></span>In this work, we aim for a workload allocation in a
    cloud that allows to optimize energy consumption. In addition, the benefits offered
    by virtualization are exploited, allowing to allocate the tasks in a more versatile
    way. The proposed system is defined as a cluster of machines of a cloud facility.


    The proposed solution considers server heterogeneity, so the technological parameters
    will vary from one architecture to another, resulting in a different energy consumption.
    Since the resultant power model is non-linear and there exists a large set of
    constraints, the problem is tackled as a multi-objective optimization:


    <span id="page-5-1"></span>

    $$\begin{array}{rcl} \text{Minimize} \\ \mathbf{y} = \mathbf{f}(\mathbf{x}) &=&
    [\boldsymbol{\lambda}, (1 + \boldsymbol{\lambda}) \cdot E\_{tot}(\mathbf{x})]
    \\ \text{Subject to} \\ \mathbf{x} &=& (x\_1, x\_2, \dots, x\_n) \in \mathbf{X}
    \end{array} \tag{16}$$


    where x is the vector of n decision variables, f is the vector of 2 objectives
    function, λ is the number of constraints not satisfied, Etot is the total energy,
    and X is the feasible region in the decision space. Using λ as shown in Equation
    [16,](#page-5-1) unfeasible solutions are also allowed, but only when no other
    alternatives are found. In this particular case, Etot is measured using [\(15\)](#page-4-8),
    whereas λ is computed as a penalization over the control and soft tasks that are
    delivered after the deadline (see Figure [1\)](#page-2-1).


    Using this formulation, we are able to obtain optimal energy savings, realistic
    with the current technology. To provide an efficient assignment in data centers
    it is necessary to consider both the energy consumption and the resource needs
    of each task of the workload.


    A task τ<sup>i</sup> can be split in different subtasks τi,j in order to achieve
    energy savings. Therefore, a task τ<sup>i</sup> can be executed using a specific
    amount of resources of one or more servers defined by uCP U,imk. The utilization
    percentage of the resources assigned to a task determines its execution time (i.e.,
    C<sup>i</sup> or si). In summary, the proposed multi-objective formulation, once
    solved, decides the following aspects:


    - Operating server set, indicating which hosts are active according to the operating
    conditions of each physical machine.

    - Best assignment for the various tasks of the workload, distributing each CPU
    instruction and memory requirements according to the minimum energy consumption
    of the applications in the computing infrastructure. For control tasks, S = 2
    must be fulfilled. However, a penalty is added to λ when one control task is aborted,
    even when S is being satisfied.

    - Percentage of resources used by every task in each host where it is allocated,
    achieving best energy consumption.


    #### *A. The solver*


    Evolutionary algorithms have been used to run the proposed multi-objective formulation.
    In this work, we use the Nondominated Sorting Genetic Algorithm II (NSGA-II) [\[20\]](#page-7-20),
    which has become a standard approach to solve this kind of problems [\[21\]](#page-7-21).
    The chromosome encoding is shown in Figure [2.](#page-5-2)


    In this case, each gene represents a decision variable. Because many decision
    variables are integer, the chromosome uses


    <span id="page-5-2"></span>


    |  | DV F S1 | · · ·<br>DV F SM | nCP U,11 | · · · | nCP U,NM |

    |--|---------|------------------|----------|-------|----------|

    |--|---------|------------------|----------|-------|----------|


    Fig. 2. Chromosome encoding


    integer encoding. Decision variables like nCP U,imk are scaled to the integer
    interval 0 ≤ nCP U,imk ≤ 100, and transformed to its real value (i.e., multiplying
    the percentage by the total number of instructions in the multi-objective function
    for evaluation).


    NSGA-II is always executed with an initial random population of 100 chromosomes.
    After that, the algorithm evolves the population applying (1) the NSGA-II standard
    tournament operator, (2) a single point crossover operator with probability of
    0.9 as recommended in [\[20\]](#page-7-20), (3) a integer flip mutation operator
    (with probability of 1/number of decision variables as also recommended in [\[20\]](#page-7-20),
    and (4) the multi-objective evaluation. Steps (1) to (4) are applied for a variable
    number of iterations or generations, which depend on the time that the parameter
    λ becomes 0 (usually 25000 iterations have been enough).


    #### VI. RESULTS


    <span id="page-5-0"></span>Tests have been conducted gathering real data from
    a Fujitsu RX300 S6 server based on an Intel Xeon E5620 processor and a SunFire
    V20z Dual Core AMD Opteron 270, both operating at the set of frequencies fmi given
    in Table [I.](#page-5-3) Total power consumption and CPU temperature have been
    collected via the *Intelligent Platform Management Interface* (IPMI) during the
    execution of *lookbusy* at different utilization levels ranging from 0% to 100%,
    where a 65% of these levels were used to fit the energy model and the remaining
    35% for validation. We used MATLAB to fit our data, obtaining the constants and
    validation errors shown in Table [II.](#page-6-0)


    <span id="page-5-3"></span>TABLE I INTEL XEON E5620 AND SUNFIRE V20Z DUAL CORE
    AMD OPTERON 270 FREQUENCIES


    | Platform    | fm1  | fm2  | fm3  | fm4  | fm5  | fm6  |

    |-------------|------|------|------|------|------|------|

    | Intel (GHz) | 1.73 | 1.86 | 2.13 | 2.26 | 2.39 | 2.40 |

    | AMD (GHz)   | 1.0  | 1.8  | 2.0  |      |      |      |


    The efficiency of the power supplies affects the calculation of these constants
    for different temperatures. In consequence, negative constants appear due to the
    fact that only CPU and memory have been characterized in this work because of
    their dominant contribution. In order to adapt the problem to more generic Cloud
    computing environments, our model constants can be calculated for data obtained
    during the execution of the workload in virtual machines. In that experimental
    approach, both the power model and the multi-objective optimization formulations
    would still be valid.


    Once the model proposed in section [IV](#page-3-0) for both Intel Xeon and AMD
    Opteron servers have been validated, we have proceeded with the analysis of results.
    The considered performance parameters are the temperature of both CPU and memory,
    as well as the frequency and voltage of the DVFS modes available to the CPU in
    each physical machine. These


    <span id="page-6-0"></span>


    | Server | A       | B1     | B2      | C1      | C2     | D      | E       |
    F       | G1       | G2      | H1      | H2      | Error  | Temp. range |

    |--------|---------|--------|---------|---------|--------|--------|---------|---------|----------|---------|---------|---------|--------|-------------|

    | Intel  | 14.3505 | 0.1110 | -       | -0.0011 | -      | 0.3347 | -40700  |
    64.9494 | 275.702  | -       | -0.4644 | -       | 11.28% | 293-309K    |

    | AMD    | 11.2390 | 1.9857 | -6.1703 | -0.0002 | 0.0132 | 426.51 | -5.3506 |
    25.1461 | -444.480 | 464.076 | 0.6977  | -0.7636 | 9.12%  | 293-312K    |


    variables modify independently the dynamic and static consumption of servers in
    each architecture, so different behaviors for Intel and AMD have been found. Table
    [III](#page-6-1) shows the set of tasks used for the optimization.


    TABLE III PROFILE OF TASKS ALLOCATED


    <span id="page-6-1"></span>


    | Task Id | Type | # Ins     | Period | Deadline | # Jobs |

    |---------|------|-----------|--------|----------|--------|

    | 0       | REAL | 7740796   | 114.20 | 0.021    | 131    |

    | 1       | CTRL | 5594832   | 114.21 | 0.015    | 115    |

    | 2       | REAL | 4138643   | 137.12 | 0.011    | 112    |

    | 3       | CTRL | 98156923  | 124.66 | 0.267    | 95     |

    | 4       | REAL | 739437676 | 124.76 | 2.01     | 118    |

    | 5       | SOFT | 2591877   | 124.86 | 0.007    | 103    |

    | 6       | SOFT | 3093531   | 124.85 | 0.008    | 112    |

    | 7       | SOFT | 5447445   | 105.76 | 0.015    | 115    |

    | 8       | SOFT | 5722568   | 152.21 | 0.016    | 99     |


    These tasks have been adapted from the TUDelft workloads archive[3](#page-6-2)
    . The task set consists of a number of deadline-critical tasks τhrt = {τ0, τ2,
    τ4}, a number of QoC-critical control tasks τ<sup>c</sup> = {τ1, τ3}, and a number
    soft real-time tasks τsrt = {τ5, τ6, τ7, τ8}. We assume that all tasks are independent
    from each other. However, due to the interference from other tasks, each task
    τ<sup>i</sup> experiences a response time or delay R<sup>i</sup> . Periods and
    deadlines are given in seconds. Each real-time tasks τhrt is bounded to one single
    host. Only control τ<sup>c</sup> and soft tasks τsrt are allowed to loss their
    deadline, increasing the λ parameter in the multi-objective function. Control
    tasks are configured with S = 2.


    NSGA-II has been executed with the minimum frequency in all the CPUs (labeled
    in the results as DVFS-MIN), the maximum frequency (labeled as DVFS-MAX) and a
    range of 5 possible DVFS modes (from 1 to 5). This algorithm has been compared
    with a more traditional approach, the EDF-VD algorithm. The overall goal is to
    design a priority assignment technique with the following objectives:


    - All the real-time tasks τhrt meet their deadlines Dhrt in the WCET

    - The overall QoC of all the control tasks τ<sup>c</sup> and QoS of all the soft
    real-time tasks τsrt is maximized.

    - The overall energy is minimized.


    <span id="page-6-2"></span>Figure [3](#page-6-3) depicts the three obtained Pareto
    fronts for the Intel architecture. Both objectives have been normalized to the
    worst value in all the Intel and AMD optimizations (1 unit of energy = 95.6 KJ).
    As can be seen, the DVFS-MAX Intel framework is able to allocate all the tasks
    in Table [III](#page-6-1) without penalizations (labeled as full feasibility).
    Using DVFS-MIN, the algorithm was not able to allocate all the required tasks,
    having to break some soft timing constraints (labeled as partial feasibility).
    As can be seen, there is at least one DVFS-VAR


    ![](_page_6_Figure_13.jpeg)


    <span id="page-6-3"></span>Fig. 3. Pareto front obtained with NSGA-II after optimizing
    the allocation of tasks over the Intel architecture.


    ![](_page_6_Figure_15.jpeg)


    <span id="page-6-4"></span>Fig. 4. Pareto front obtained with NSGA-II after optimizing
    the allocation of tasks over the AMD architecture.


    configuration able to execute all the tasks without penalization and with less
    energy than DVFS-MAX and close to DVFS-MIN. Table [IV](#page-7-22) shows the DVFS
    modes selected by the DVFS-VAR solution with full feasibility.


    Similarly, Figure [4](#page-6-4) shows the three obtained non-dominated fronts
    for the AMD architecture. As with the Intel scenario, the algorithm was not able
    to execute all the REAL, CTRL and SOFT tasks without penalization using the minimum
    DVFS mode (DVFS-MIN), although all the REAL tasks were properly executed. However,
    we found a completely feasible solution in DVFS-VAR (feasibility=0), consuming
    less energy than DVFS-MAX. Table [IV](#page-7-22) shows the DVFS models selected
    by the multi-objective algorithm in the DVFS-VAR AMD optimization.


    EDF was able to schedule all the tasks in both cases, but using the maximum DVFS
    mode and thus consuming more energy than the proposed algorithm.


    <span id="page-7-22"></span>TABLE IV DVFS MODES OBTAINED BY NSGA-II PARETO FRONT
    IN THE DVFS-VAR OPTIMIZATION


    | Platform | CPU 1 | CPU 2 | CPU 3 | CPU 4 | CPU 5 | CPU 6 |

    |----------|-------|-------|-------|-------|-------|-------|

    | Intel    | 2     | 2     | 2     | 2     | 2     | 5     |

    | AMD      | 5     | 1     | 5     |       |       |       |


    As a result, the best DVFS configuration that can execute all the demanded services
    given in Table [III](#page-6-1) has been found without penalizations, obtaining
    a high diversity in terms of energy consumption.


    #### VII. CONCLUSIONS


    <span id="page-7-2"></span>CPS and Mobile Cloud Computing have collided with the
    lack of accurate power models for the energy-efficient provisioning of their devised
    infrastructures, and the real-time management of the computing facilities. In
    this paper, we have presented a reservation-based scheme aiming to jointly schedule
    deadline-critical, QoS non-critical, and QoC tasks. The work proposed in this
    paper has made substantial contributions in the area of power modeling of high-performance
    servers for Cloud computing services under timing constraints, which is an interesting
    and big challenge.


    We have proposed an accurate power model in data centers for time constrained
    servers in Cloud computing, which does not only consider the workload assigned
    to the processing element, but also incorporates the need of considering the static
    power consumption and its dependency with temperature.


    The proposed model has been used in a multi-objective optimization environment
    in which the DVFS and workload assignment have been efficiently optimized in a
    realistic scenario composed of Fujitsu RX300 S6 servers based on an Intel Xeon
    E5620 and SunFire V20z Dual Core AMD Opteron 270. Results show that the proposed
    multi-objective optimization framework is able to find the best DVFS configuration
    that can execute all the given demanded services without penalizations. In addition,
    the set of non-dominated solutions found presents a high diversity in terms of
    energy consumption.


    The obtained results open a motivating research line that could enable intensely
    sought Green computing paradigm with hard timing constraints. Future work envisages
    to extend the scheduling model to integrate the concept of criticality levels.


    #### ACKNOWLEDGMENT


    This work is supported by the Spanish Ministry of Economy and Competitivity under
    research grants TIN2013-40968-P and TIN2014-54806-R.


    #### REFERENCES


    - <span id="page-7-0"></span>[1] E. A. Lee, "CPS Foundations," in *Proceedings
    of the 47th Design Automation Conference*, ser. DAC ''10. New York, NY, USA: ACM,
    2010, pp. 737–742.

    - <span id="page-7-1"></span>[2] A. Banerjee, K. K. Venkatasubramanian, T. Mukherjee,
    and S. K. S. Gupta, "Ensuring safety, security, and sustainability of mission-critical
    cyber-physical systems." *Proceedings of the IEEE*, vol. 100, no. 1, pp. 283–299,
    2012. [Online]. Available: [http://dblp.uni-trier.de/db/journals/](http://dblp.uni-trier.de/db/journals/pieee/pieee100.html)
    [pieee/pieee100.html](http://dblp.uni-trier.de/db/journals/pieee/pieee100.html)

    - <span id="page-7-3"></span>[3] A. Rowe, K. Lakshmanan, H. Zhu, and R. Rajkumar,
    "Rate-harmonized scheduling for saving energy," in *Proceedings of the 2008 Real-Time
    Systems Symposium*, ser. RTSS ''08. Washington, DC, USA: IEEE Computer Society,
    2008, pp. 113–122. [Online]. Available: <http://dx.doi.org/10.1109/RTSS.2008.50>

    - <span id="page-7-4"></span>[4] J. Chen and C. Kuo, "Energy-efficient scheduling
    for real-time systems on dynamic voltage scaling (DVS) platforms," in *13th IEEE
    International Conference on Embedded and Real-Time Computing Systems and Applications
    (RTCSA 2007), 21-24 August 2007, Daegu, Korea*, 2007, pp. 28–38.

    - <span id="page-7-5"></span>[5] J. Kim, H. Kim, K. Lakshmanan, and R. R. Rajkumar,
    "Parallel scheduling for cyber-physical systems: Analysis and case study on a
    self-driving car," in *Proceedings of the ACM/IEEE 4th International Conference
    on Cyber-Physical Systems*, ser. ICCPS ''13. New York, NY, USA: ACM, 2013, pp.
    31–40. [Online]. Available: <http://doi.acm.org/10.1145/2502524.2502530>

    - <span id="page-7-6"></span>[6] P. Huang, P. Kumar, G. Giannopoulou, and L. Thiele,
    "Energy efficient dvfs scheduling for mixed-criticality systems," in *Proceedings
    of the 14th International Conference on Embedded Software*, ser. EMSOFT ''14.
    New York, NY, USA: ACM, 2014, pp. 11:1–11:10. [Online]. Available:<http://doi.acm.org/10.1145/2656045.2656057>

    - <span id="page-7-7"></span>[7] S. K. Baruah, V. Bonifaci, G. D''Angelo, H. Li,
    A. Marchetti-Spaccamela, S. van der Ster, and L. Stougie, "The preemptive uniprocessor
    scheduling of mixed-criticality implicit-deadline sporadic task systems." in *ECRTS*,
    R. Davis, Ed. IEEE Computer Society, 2012, pp. 145–154.

    - <span id="page-7-8"></span>[8] L. Wang, G. von Laszewski, J. Dayal, and F. Wang,
    "Towards energy aware scheduling for precedence constrained parallel tasks in
    a cluster with DVFS," in *Cluster, Cloud and Grid Computing (CCGrid), 2010 10th
    IEEE/ACM International Conference on*, May 2010, pp. 368–377.

    - <span id="page-7-9"></span>[9] A. Beloglazov, J. Abawajy, and R. Buyya, "Energy-aware
    resource allocation heuristics for efficient management of data centers for cloud
    computing," *Future Gener. Comput. Syst.*, vol. 28, no. 5, pp. 755–768, May 2012.
    [Online]. Available: [http://dx.doi.org/10.1016/j.future.2011.](http://dx.doi.org/10.1016/j.future.2011.04.017)
    [04.017](http://dx.doi.org/10.1016/j.future.2011.04.017)

    - <span id="page-7-10"></span>[10] C. Wu, R. Chang, and H. Chan, "A green energy-efficient
    scheduling algorithm using the DVFS technique for cloud datacenters," *Future
    Generation Comp. Syst.*, vol. 37, pp. 141–147, 2014.

    - <span id="page-7-11"></span>[11] S. Park, J. Kim, and G. Fox, "Effective real-time
    scheduling algorithm for cyber physical systems society," *Future Generation Comp.
    Syst.*, vol. 32, pp. 253–259, 2014.

    - <span id="page-7-12"></span>[12] C. Mobius, W. Dargie, and A. Schill, "Power
    consumption estimation models for processors, virtual machines, and servers,"
    *Parallel and Distributed Systems, IEEE Transactions on*, vol. 25, no. 6, pp.
    1600– 1614, June 2014.

    - <span id="page-7-13"></span>[13] M. M. Rafique and et al., "Power management
    for heterogeneous clusters: An experimental study," in *IGCC*, Washington, DC,
    USA, 2011, pp. 1–8.

    - <span id="page-7-14"></span>[14] R. I. Davis and A. Burns, "A survey of hard
    real-time scheduling for multiprocessor systems," *ACM Comput. Surv.*, vol. 43,
    no. 4, pp. 35:1–35:44, Oct. 2011. [Online]. Available: [http://doi.acm.org/10.1145/](http://doi.acm.org/10.1145/1978802.1978814)
    [1978802.1978814](http://doi.acm.org/10.1145/1978802.1978814)

    - <span id="page-7-15"></span>[15] R. Schneider, D. Goswami, A. Masrur, M. Becker,
    and S. Chakraborty, "Multi-layered scheduling of mixed-criticality cyber-physical
    systems," *Journal of Systems Architecture (JSA)*, vol. 59, no. 10-D, 2013.

    - <span id="page-7-16"></span>[16] L. Abeni, N. Manica, and L. Palopoli, "Efficient
    and robust probabilistic guarantees for real-time tasks," *J. Syst. Softw.*, vol.
    85, no. 5, pp. 1147–1156, May 2012. [Online]. Available: [http:](http://dx.doi.org/10.1016/j.jss.2011.12.042)
    [//dx.doi.org/10.1016/j.jss.2011.12.042](http://dx.doi.org/10.1016/j.jss.2011.12.042)

    - <span id="page-7-17"></span>[17] T. Chantem, X. Hu, and M. Lemmon, "Generalized
    elastic scheduling for real-time tasks," *Computers, IEEE Transactions on*, vol.
    58, no. 4, pp. 480–495, 2009.

    - <span id="page-7-18"></span>[18] S. Narendra and A. Chandrakasan, *Leakage in
    Nanometer CMOS Technologies*, ser. Integrated Circuits and Systems. Springer,
    2010.

    - <span id="page-7-19"></span>[19] J. Rabaey, *Low Power Design Essentials*, ser.
    Engineering (Springer-11647). Springer, 2009.

    - <span id="page-7-20"></span>[20] K. Deb, A. Pratap, S. Agarwal, and T. Meyarivan,
    "A Fast and Elitist Multiobjective Genetic Algorithm: NSGA-II," *IEEE Transactions
    on Evolutionary Computation*, vol. 6, no. 2, pp. 182–197, 2002.

    - <span id="page-7-21"></span>[21] A. Sayyad and H. Ammar, "Pareto-optimal search-based
    software engineering (posbse): A literature survey," in *Realizing Artificial
    Intelligence Synergies in Software Engineering (RAISE), 2013 2nd International
    Workshop on*, May 2013, pp. 21–27.'
  decisions:
    evaluation_prompt: 'Qualified. Reason: The paper includes an "Experimental results"
      section (Section VI) that discusses tests conducted on real data from servers,
      presents performance metrics, and provides tables and figures showing results.
      This indicates that the paper contains empirical evaluation.'
    related_work_prompt: 'Qualified. Reason: The paper meaningfully engages with prior
      research throughout its content. It includes a dedicated "Related Work" section
      that discusses various existing studies and methods related to energy-efficient
      scheduling, DVFS, and real-time systems. The paper also compares its proposed
      method to previous work, highlighting differences and improvements, which demonstrates
      a meaningful engagement with the academic literature.'
    novelty_prompt: 'Qualified. Reason: The paper proposes a new method for solving
      CPS problems by introducing new adaptive real-time scheduling algorithms in
      distributed computing infrastructures that consider energy efficiency. It also
      presents a novel power model for data centers that incorporates static power
      consumption and its dependency on temperature, which is used in a multi-objective
      optimization framework. These contributions demonstrate novelty in the proposed
      methods and their application to energy-efficient scheduling in cloud computing
      environments.'
    review_only_prompt: '- Qualified. Reason: The paper introduces new contributions
      by proposing a method for solving CPS problems with new adaptive real-time scheduling
      algorithms and presents an accurate power model for data centers, which are
      not merely summaries of existing work.'
  topics:
    main_topic: Other
    main_topic_reasoning: The paper discusses a power model in data centers and how
      it relates to Cloud computing, indicating a focus on optimizing cloud-based
      services for energy efficiency.
    secondary_topic: Embedded & Real-time Systems
    secondary_topic_reasoning: Since the paper also addresses real-time systems and
      timing constraints, it falls under the broader category of embedded and real-time
      systems, especially in the context of industrial applications.
    main_topic_sub: ''
    secondary_topic_sub: Other
- title: "Optimization of a Line Detection Algorithm for Autonomous Vehicles on a\n\
    \  RISC-V with Accelerator"
  abstract: 'In recent years, autonomous vehicles have attracted the attention of
    many

    research groups, both in academia and business, including researchers from

    leading companies such as Google, Uber and Tesla. This type of vehicles are

    equipped with systems that are subject to very strict requirements, essentially

    aimed at performing safe operations -- both for potential passengers and

    pedestrians -- as well as carrying out the processing needed for decision

    making in real time. In many instances, general-purpose processors alone cannot

    ensure that these safety, reliability and real-time requirements are met, so it

    is common to implement heterogeneous systems by including accelerators. This

    paper explores the acceleration of a line detection application in the

    autonomous car environment using a heterogeneous system consisting of a

    general-purpose RISC-V core and a domain-specific accelerator. In particular,

    the application is analyzed to identify the most computationally intensive

    parts of the code and it is adapted accordingly for more efficient processing.

    Furthermore, the code is executed on the aforementioned hardware platform to

    verify that the execution effectively meets the existing requirements in

    autonomous vehicles, experiencing a 3.7x speedup with respect to running

    without accelerator.'
  url: http://arxiv.org/abs/2402.00496v1
  keywords: ''
  document: '# Optimization of a Line Detection Algorithm for Autonomous Vehicles
    on a RISC-V with Accelerator


    Optimizacion de un Algoritmo de Detecci ´ on de L ´ ´ıneas para Veh´ıculos Autonomos
    en un ´ RISC-V con Acelerador


    Mar´ıa Jose Belda ´ 1 [,](https://orcid.org/0000-0002-2870-7679) Katzalin Olcoz<sup>1</sup>
    [,](https://orcid.org/0000-0002-1821-124X) Fernando Castro<sup>1</sup> [,](https://orcid.org/0000-0002-2773-3023)
    and Francisco Tirado<sup>1</sup>


    > <sup>1</sup>*Complutense University of Madrid, Madrid 28040, Espana˜* {mbelda,katzalin,fcastror,ptirado}@ucm.es


    # Abstract


    In recent years, autonomous vehicles have attracted the attention of many research
    groups, both in academia and business, including researchers from leading companies
    such as Google, Uber and Tesla. This type of vehicles are equipped with systems
    that are subject to very strict requirements, essentially aimed at performing
    safe operations –both for potential passengers and pedestrians– as well as carrying
    out the processing needed for decision making in real time. In many instances,
    general-purpose processors alone cannot ensure that these safety, reliability
    and real-time requirements are met, so it is common to implement heterogeneous
    systems by including accelerators. This paper explores the acceleration of a line
    detection application in the autonomous car environment using a heterogeneous
    system consisting of a general-purpose RISC-V core and a domain-specific accelerator.
    In particular, the application is analyzed to identify the most computationally
    intensive parts of the code and it is adapted accordingly for more efficient processing.
    Furthermore, the code is executed on the aforementioned hardware platform to verify
    that the execution effectively meets the existing requirements in autonomous vehicles,
    experiencing a 3.7x speedup with respect to running without accelerator.


    Keywords: Autonomous vehicles, Firesim, Image processing, Matrix accelerator,
    RISC-V


    # Resumen


    En los ultimos a ´ nos los veh ˜ ´ıculos autonomos est ´ an´ atrayendo la atencion
    de muchos grupos de investi- ´ gacion, tanto del ´ ambito acad ´ emico como del
    em- ´ presarial, entre los que se incluyen investigadores pertenecientes a empresas
    punteras como Google, Uber o Tesla. Los sistemas de los que estan dota- ´ dos
    este tipo de veh´ıculos estan sometidos a requisitos ´ muy estrictos relacionados
    esencialmente con la realizacion de operaciones seguras, tanto para los poten-
    ´ ciales pasajeros como para los peatones, as´ı como con


    que el procesamiento necesario para la toma de decisiones se realice en tiempo
    real. En muchas ocasiones, los procesadores de proposito general no pueden por
    ´ s´ı solos garantizar el cumplimiento de estos requisitos de seguridad, fiabilidad
    y tiempo real, por lo que es comun implementar sistemas heterog ´ eneos mediante
    ´ la inclusion de aceleradores. En este art ´ ´ıculo se explora la aceleracion
    de una aplicaci ´ on de detecci ´ on de ´ l´ıneas en el entorno de veh´ıculos
    autonomos utilizando ´ para ello un sistema heterogeneo formado por un core ´
    RISC-V de proposito general y un acelerador de do- ´ minio espec´ıfico. En particular,
    se analiza dicha aplicacion para identificar las partes del c ´ odigo m ´ as cos-
    ´ tosas computacionalmente y se adapta el codigo para ´ un procesamiento mas eficiente.
    Adem ´ as, se ejecuta ´ dicho codigo en la mencionada plataforma hardware ´ y
    se comprueba que su procesamiento efectivamente cumple con los requisitos presentes
    en los veh´ıculos autonomos, experimentando una reducci ´ on de 3.7x en ´ su tiempo
    de ejecucion con respecto a su ejecuci ´ on sin ´ acelerador.


    Palabras claves: Veh´ıculos autonomos, Firesim, ´ Procesamiento de imagenes, Acelerador
    de matrices, ´ RISC-V


    # 1 Introduction


    In the technological era in which we live, we every day strive to make all the
    usual tasks as automatic as possible in order to gain free time. In addition,
    we try to achieve scenarios that are impossible right now, such as smarter power
    grids, fully autonomous vehicles or smart cities. This is why the Internet of
    Things (IoT) arises, as we need new technologies to design these systems. Most
    of them are on-board systems, so they need to get a trade-off between power consumption
    and delivered performance. In particular, in this work we focus on autonomous
    vehicles.


    Autonomous driving systems aim to enable vehicles to drive on the road without
    human intervention [\[1,](#page-10-0) [2,](#page-10-1) [3\]](#page-10-2). Therefore,
    these systems must guarantee the safety and integrity of the vehicle, for which
    they must take a series of decisions in real time, including


    moving the steering wheel to ensure that the correct trajectory is followed, detecting
    obstacles in the path (pedestrians, animals, objects...), activating the braking
    mechanism when necessary and others. For this purpose, it is essential that the
    vehicle has a camera that records images of the route and processes them in real
    time to ensure the correct and safe operation of the vehicle. This image processing
    requires considerable computing power, but at the same time, when talking about
    on-board systems, it is essential to keep energy consumption at low levels so
    the vehicle does not loose autonomy [\[4\]](#page-10-3).


    For these reasons, autonomous vehicles require onboard automatic systems to process
    the recorded images that allow certain operations such as line and edge detection.
    Currently, the most widely used algorithms for this type of processing require
    high performance and their basic kernel is matrix and vector multiplication. It
    is therefore highly desirable that this type of algorithms could be executed in
    one of the many domain specific accelerators that have emerged in recent years.


    In this paper we propose to accelerate a line detection application employed in
    autonomous cars by using different heterogeneous systems made up of a general-purpose
    RISC-V core working at low frequency and a domain-specific accelerator. For this
    purpose, the application is deeply analyzed in order to identify the computationally
    intensive parts of the code and adapted consequently for a more efficient processing.
    As it will be explained in Section 3, the hardware platform used in this work
    includes, on the one hand, a general-purpose BOOM processor, which is an out-of-order
    RISC-V core [\[5\]](#page-10-4), and on the other hand, the Gemmini [\[6\]](#page-10-5)
    accelerator, specifically designed for matrix multiplication. This platform was
    chosen because the RISC-V architecture, in addition to being open source, allows
    the integration of accelerators and their potential adaptation in a very simple
    way. Furthermore, the RISC-V instruction set architecture (ISA) is highly modular,
    allowing to choose exactly the functionalities needed, which is especially useful
    in IoT environments.


    This paper leverages two image processing algorithms: 1) the Canny algorithm for
    edge detection of an image, and 2) the Hough transform, oriented to find imperfect
    instances of objects within a certain class of shapes by means of a voting procedure.
    In Section 4 we perform a detailed analysis of both algorithms codes, in order
    to identify the computational load of the different functions included in these
    programs, as well as the available parallelism. Moreover, we schedule some functions
    to run on the accelerator, while the rest of the algorithm is executed on the
    processor, aimed to optimize the total execution time and consequently to meet
    the strict requirements of performance, consumption and safety imposed by autonomous
    vehicles. The experimental evaluation carried out in


    Section 5 reports a speedup of 3.7x when executing these algorithms with respect
    to the baseline where no accelerator is employed. Finally, Section 6 concludes
    the paper.


    # 2 Basic notions and state of the art


    In this section we explain some basic notions related to autonomous vehicles.
    We also provide details on the RISC-V-based development environment that we employ,
    including the tools used that make it possible the evaluation of the proposal
    presented in this paper.


    ## 2.1 Autonomous vehicles


    Autonomous vehicles are equipped with several sensors, as shown in Fig. [1,](#page-2-0)
    including video cameras, which are responsible for obtaining the data that serve
    as input to the processing system. The purpose of this data processing is to recognize
    the environment which the vehicle is driving through, and as a result, to make
    the appropriate decisions at any time, so as to ensure that the vehicle can reach
    its destination efficiently and safely. In this aspect, autonomous vehicles have
    levels of driving automation from 0 (No automation) to 5 (Full automation), as
    explained in [\[7\]](#page-11-0). In the first levels, from 0 to 2, the vehicle
    has very little capacity to act (in level 2 it can only perform steering and acceleration)
    and all the responsibility lies on the driver. In contrast, the automatic system
    monitors the driving environment in levels 3 to 5, being this last one the ideal
    scenario in which the vehicle is completely autonomous, even not providing controls
    for the driver. So, there is a gap between levels 2 and 3. Between these levels
    there is also a technological gap, since generating hardware and software capable
    of monitoring the environment in real time becomes significantly difficult. However,
    this gap is progressively disappearing and this work aims to contribute to this.


    Notably, certain safety decisions are related to the correct recognition of the
    trajectory to be followed by the vehicle, based on the images recorded by the
    camera. In addition to allowing the car to follow the correct route, this functionality
    also involves restricting the likelihood of an accident. For this purpose, computer
    vision algorithms are commonly used in these processing systems [\[9,](#page-11-1)
    [10\]](#page-11-2) and, in particular, quite approaches use Canny algorithm to
    detect edges combined with the Hough transform to detect road lines [\[11,](#page-11-3)
    [12\]](#page-11-4). Therefore, in this paper we focus on improving the performance
    of these algorithms which are the basis of lane detection. The problem with these
    algorithms is their very high computational cost. In addition to this, there is
    a need for data processing to be performed in real time so that the vehicle could
    react with immediacy to changing situations that may occur during the journey.
    It is also highly desirable that the energy consumption associated with such processing


    ![](_page_2_Figure_0.jpeg)


    <span id="page-2-0"></span>Figure 1: Integrated sensors on an autonomous vehicle
    [\[8\]](#page-11-5).


    was as low as possible, so that the vehicle''s autonomy was not affected.


    Autonomous driving systems are essentially composed of three classes of sub-systems
    [\[1,](#page-10-0) [2\]](#page-10-1): *scene recognition*, *route planning* and
    *vehicle control*, consisting of a set of algorithms each. In particular, as shown
    in Fig. [2,](#page-2-1) *scene recognition*, the class in which this article falls,
    comprises three essential tasks, namely 1) *localization*, which precisely establishes
    the vehicle''s location, 2) *object detection*, which identifies objects of interest
    in the vehicle''s environment (such as other vehicles, pedestrians or road signs,
    with the aforementioned objective of avoiding accidents and also traffic violations),
    and 3) *object tracking*, which, since the object detection algorithm is carried
    out on each frame of the image, is responsible for relating its results to other
    frames in order to predict the trajectories of moving objects. These three tasks
    account for a very high percentage of the total computation time required [\[1\]](#page-10-0)
    and therefore constitute bottlenecks that significantly limit the ability of conventional
    processors to satisfy the existing restrictions in the design of this type of
    systems. For this reason, it is being proposed to incorporate some type of accelerator
    to the on-board processing systems that helps the processor to fulfill the strict
    time limits in which it must operate.


    ## 2.2 RISC-V-based development environment


    In order to carry out the implementation and evaluation of our proposal, which
    will be explained in the following section, a series of software tools have been
    used, as detailed next:


    ![](_page_2_Figure_7.jpeg)


    <span id="page-2-1"></span>Figure 2: Schematic of the subsystems of an autonomous
    vehicle.


    #### 2.2.1 Chipyard.


    Chipyard [\[13\]](#page-11-6) is an environment for the design and evaluation
    of hardware systems that consists of a set of tools and libraries designed to
    provide an integration path between open-source tools and commercial tools for
    the development of Systems on Chip (SoC). The environment provides a range of
    components for design construction as well as for compilation and simulation.
    Among these components there are several RISC-V cores and accelerators, including
    the BOOM core and Gemmini accelerator that make up the heterogeneous system chosen
    in this paper and that will be detailed in Section [3.](#page-3-0) The simulation
    of the complete system accelerated with FPGA is one of the types of simulation
    supported by Chipyard, using the FireSim tool described below.


    #### 2.2.2 FireSim.


    FireSim [\[14\]](#page-11-7) is a hardware simulation platform that runs on Amazon
    cloud services and automatically deploys the FPGA services in the cloud when needed.
    In particular, the user can generate the RTL of an own design and run it on these
    FPGAs, obtaining the same results as if the circuit was physically deployed.


    #### 2.2.3 Amazon Web Services.


    Amazon Web Services [\[15\]](#page-11-8) is a cloud services platform that offers
    from training courses in new technologies –such as artificial intelligence or
    IoT– to infrastructure services –such as storage or cloud computing. We focus
    on cloud computing because it offers a wide range of hardware platforms, including
    EC2 F1 instances that correspond to FPGAs, giving us the versatility we need to
    synthesize designs and to simulate the execution of applications on them.


    # <span id="page-3-0"></span>3 Platform design


    The platform employed in our experiments features a general-purpose processor
    equipped with an accelerator –implemented as a systolic array architecture– for
    matrix multiplication. Both components have been developed by the Computer Architecture
    group at Berkeley University [\[6\]](#page-10-5). The accelerator communicates
    with the processor through the RoCC (Rocket Co-Processor) interface, which allows
    the accelerator to receive the specific instructions that the processor sends,
    as shown in Fig. [3.](#page-3-1) In the following two sections we describe the
    processors and the accelerator used.


    ![](_page_3_Figure_6.jpeg)


    <span id="page-3-1"></span>Figure 3: Architecture of our heterogeneous platform
    [\[6\]](#page-10-5).


    ## 3.1 Processors


    As Fig. [3](#page-3-1) illustrates, our system features a core plus an accelerator.
    In our experiments we opted to employ either the Rocket or the BOOM (Berkeley
    Out-of-Order Machine) processor. Both of them are written in Chisel and implement
    the RV64GC instruction set. Also, they


    are easily parameterizable and can be synthesized. Notably, the cores are configured
    by using the Rocket Chip SoC generator [\[16\]](#page-11-9).


    The main differences between both cores lie in the pipeline characteristics: while
    the Rocket core features an in-order 5-stage pipeline, the BOOM core is equipped
    with a deeper out-of-order pipeline, which is inspired by those of MIPS R10000
    and Alpha 212645 [\[5\]](#page-10-4). Consequently, the BOOM core is expected
    to deliver higher performance when executing our line detection algorithm. However,
    this comes at the expense of higher energy consumption than that of the Rocket
    core. Therefore, we experiment with both processors in order to check if the speedup
    reported by the BOOM core is significant enough to cancel out the energy constraints.


    ## 3.2 The Gemmini Accelerator


    The Gemmini matrix multiplication accelerator relies on a 2D systolic array architecture,
    as shown in Fig. [3,](#page-3-1) to perform matrix multiplications in an efficient
    fashion. In addition to this systolic array, it also features a scratchpad memory
    with multiple banks and an accumulator, which has more bits than that of the systolic
    array. Besides, the implementation allows to choose, at compile time, between
    two specific calculation mechanisms: output-stationary or weight-stationary.


    Customized instructions –out of RISC-V standard– are available for the Gemmini
    accelerator, so that it is equipped with its own instruction queues that make
    it possible to execute concurrently with the processor. The Gemmini programming
    model can be broken down into three different levels. In the high-level we can
    run Open Neural Network Exchange (ONNX) models, being the accelerator itself in
    charge of mapping the ONNX kernel to the accelerator by means of dynamic dispatch.
    In the mid-level we use a handtuned library including C macros to perform data
    transfers between the main memory and the accelerator''s scratchpad memory, which
    should be explicitly defined, as well as to automate the calculation of the block
    size used to split a matrix and to perform the full multiplication in a transparent
    way for users. Among available functions we highlight the following: *tiled matmul*,
    to run a tiled matrix multiplication with hardcoded tiling factors; *tiled conv*,
    to apply a convolution with hardcoded tiling factors; *tiled matmul auto*, to
    run a tiled matrix multiplication with automatically calculated tiling factors;
    *gemmini mvin*, to move data from the main memory to the scratchpad and *gemmini
    mvout*, to move data from the scratchpad to the main memory. Finally, at the low-level,
    we can write our own mid-level kernels with low-level assembly instructions.


    # <span id="page-4-2"></span>4 Adapting image processing algorithms


    As stated previously, the aim of this work is to accelerate image processing algorithms
    employed to guide autonomous vehicles. Notably, we focus on those algorithms targeted
    to detect road lines from road images. In this section we first introduce the
    basic algorithms used (the Canny algorithm and the Hough transform). Then, we
    show the full algorithm that we have employed in this work as starting point for
    line detection and, finally, we propose some changes to this algorithm oriented
    to improve its efficiency and performance without impacting on accuracy.


    ## 4.1 Canny Algorithm


    Among the edge detection methods developed to date, the Canny algorithm is one
    of the methods more strictly defined that provides a satisfactory and reliable
    detection. Thus, it has become one of the most popular algorithms targeting edge
    detection.


    This algorithm relies on calculus of variations, which allows to find an analytical
    function to approximate the real curve (i.e., the road lines) as accurately as
    possible. The procedure followed by the Canny algorithm [\[17\]](#page-11-10)
    can be broken down into 5 stages as shown next:


    - 1. Noise reduction: applying the Gauss filter for image smoothing.

    - 2. To find the intensity gradient of the image.

    - 3. Magnitude threshold to the gradient: applying a threshold to the gradient
    for discarding edge false positives.

    - 4. Double threshold: applying again a threshold to the gradient for highlighting
    the potential edges.

    - 5. Hysteresis: removing weak or disconnected edges.


    Algorithm [1](#page-4-0) shows the pseudo-code we employed to apply the Canny
    algorithm, broken down into the 5 stages aforementioned. Essentially, it includes
    multiplications of consecutive matrices and conditions checking in order to detect
    edge points.


    ## 4.2 Hough Transform


    The Hough transform is a technique of features extraction which is employed in
    multiple fields involving image processing, as computer vision or image digital
    processing. The goal of the algorithm is to find imperfect objects among certain
    classes of objects by means of a voting procedure. This procedure lies in creating
    a space with the values assigned to each pixel, so that the resulting local maximums
    in the so called accumulator space are the possible detected objects.


    <span id="page-4-0"></span>Algorithm 1 Canny algorithm summarized pseudocode.


    - 1: float *NR* ← mask ∗ image ▷ Stage 1: Noise reduction

    - 2: float *G<sup>x</sup>* ← mask ∗NR ▷ Stage 2: Gradient intensity

    - 3: float *G<sup>y</sup>* ← mask ∗NR

    - 4: float *G* ← q *G*<sup>2</sup> *<sup>x</sup>* +*G*<sup>2</sup> *y*

    - 5: float φ ← arctan(|*Gy*|/|*Gx*|)

    - 6: if φ[∗] ≥ threshold<sup>φ</sup> then ▷ Stage 3: Gradient threshold

    - 7: float φ ∈ {0,45,90,135}

    - 8: end if

    - 9: if φ[∗] ≥ threshold<sup>φ</sup> && *G*[∗] ≥ threshold*<sup>G</sup>* then
    ▷ Stage 4: Double threshold

    - 10: int edge[∗] ← 1

    - 11: end if

    - 12: if *G*[∗] ≥ threshold*<sup>G</sup>* && edge[∗] == 1 then ▷ Stage 5: Hysteresis

    - 13: int image out[∗] ← 255


    14: end if


    Generally, the classical Hough transform was only applied to the detection of
    straight lines, but in recent years it has been modified and currently it is employed
    for the detection of arbitrary curves, as ellipses or circles.


    Algorithm [2](#page-4-1) illustrates the code we employed to apply the Hough transform
    [\[18\]](#page-11-11). In this code, for each edge point previously detected with
    the Canny algorithm, the Hough transform draws a set of straight lines going through
    that point, recording the amount of lines going through each image pixel. Hence,
    those points with more lines going through them will correspond to a line in the
    original image.


    <span id="page-4-1"></span>Algorithm 2 Hough transform summarized pseudocode.


    1: For each edge point (*i*, *j*) 2: if image[*i* ∗*width*+ *j*] ≥ 250 then 3:
    θ ← 0 4: while θ ≤ 180 do 5: float ρ ← *j* ∗ cosθ +*i* ∗ sinθ 6: accumulators[(ρ
    +*c*<sup>ρ</sup> ) ∗ 180+θ]++ 7: θ++ 8: end while 9: end if


    ## 4.3 Line Detection


    Once we have described the two previous algorithms, we now employ a combination
    of both as well as another specific code targeted to detect with higher accuracy
    the lines that demarcate lanes in conventional ways. For this purpose, given a
    certain input image, we first apply the Canny algorithm and then the Hough


    transform, so that we can apply a function (*Get lines coordinates*) to detect
    lines in the resulting image. In Algorithm [3](#page-5-0) we show the code of
    the mentioned function, which involves a search of local maximums in the preprocessed
    image and the generation of a straight line going through closest maximums.


    <span id="page-5-0"></span>Algorithm 3 Get lines coordinates algorithm summarized
    pseudo-code.


    - 1: For each image point (ρ,θ)

    - 2: if accumulators[∗] ≥ threshold then ▷ It is a local maximum

    - 3: *max* ← accumulators[\*]

    - 4: if accumulators[neighbourhood(\*)] ≥ *max* then ▷ We check its neighborhood

    - 5: *max* ← accumulators[neighbourhood(\*)]

    - 6: end if

    - 7: end if

    - 8: lines.add(*x*1, *y*1, *x*2, *y*2) ▷ We save the two points that demarcate
    the straight line


    ## 4.4 Delivering higher performance


    In the previous sections we have described the original code of the algorithms,
    which indeed deploys many floating point variables. Therefore, it is advisable
    to replace them by integer variables without any loss in accuracy. We effectively
    made these changes in the code and we verified that no accuracy loss occurs when
    detecting lines in an image. Fig. [4](#page-6-0) shows the original image with
    detected lines highlighted in red. The analytical results corresponding to the
    lines detected with the original algorithm and with the simplified one do match,
    and also the second algorithm has performed slightly faster. Details on these
    modifications can be found in [\[19\]](#page-11-12).


    Apart from this change, we also performed a profiling of the full code divided
    into three stages: 1) original image loading, 2) lines detection and 3) generation
    of an output image with the detected lines. Accordingly to the results obtained,
    we opted for not generating an output image (that is not needed by our system)
    due to the high cost associated, as shown in Table [1.](#page-5-1) In doing so,
    we are able to reduce the execution time by 4.2x as we can derive from data reported
    in Table [2.](#page-5-2) It is worth noting that the time values illustrated in
    the mentioned tables are approximate since the profiling was not performed on
    the target platform, but on an Intel i7 processor running Linux. However, in order
    to report time values as accurate as possible, the algorithms were run several
    times so that the tables show the average values obtained. According to numbers
    from Table [2,](#page-5-2) our attention is drawn to the line detection phase
    since it accounts for almost 70% of the execution time.


    In addition, we have performed another specific profiling of the stages of the
    line detection algorithm in


    <span id="page-5-1"></span>


    | Table 1: Phased profiling of the full code. |  |  |

    |---------------------------------------------|--|--|

    |                                             |  |  |


    |                  | Time(µs) | % over total |

    |------------------|----------|--------------|

    | Image load       | 43803    | 7,32%        |

    | Line detection   | 98171    | 16,42%       |

    | Image generation | 456030   | 76,26%       |

    | Total            | 598004   |              |


    <span id="page-5-2"></span>Table 2: Phased *profiling* of the full code excluding
    the generation of the output image.


    |                | Time(µs) | % over total |

    |----------------|----------|--------------|

    | Image load     | 43485    | 30,58%       |

    | Line detection | 98714    | 69,42%       |

    | Total          | 142199   |              |


    order to know in which parts of the processing the acceleration efforts should
    be focused. Table [3](#page-5-3) illustrates that the most time-consuming part
    is the application of the Canny algorithm, which accounts for more than 87% of
    the total execution time. Therefore, we will focus on accelerating this stage
    of image processing.


    <span id="page-5-3"></span>Table 3: Phased *profiling* of the line detection algorithm.


    |                 | Time(µs) | % over total |

    |-----------------|----------|--------------|

    | Canny algorithm | 90265    | 87,64%       |

    | Hough transform | 12275    | 11,92%       |

    | Get coordinates | 459      | 0,45%        |

    | Total           | 102999   |              |


    # 5 Experimental results


    In this section, we first describe the hardware platforms as well as the workloads
    employed in our experiments, and then we detail the results obtained.


    ## 5.1 Platforms generated


    All the components used in the designs generated are written in Scala, so it is
    easy to modify their main features such as number of registers or number of Re-Order
    Buffer (ROB) entries. Notably, we generate several designs: while all of them
    include one (or more) Rocket or BOOM cores, they may include or not the Gemmini
    accelerator.


    Apart from the cores, for the sake of fairness the remaining components in the
    different designs generated (such as memory, clock frequency or buses) are the
    same in all of them. Hence, all designs have an L2 –shared in multicore platforms–
    4MB size. In order to optimize the design to fit into smaller FPGAs, the option
    MCRams is enabled in the FireSim platform configuration for all designs. This
    option allows the FPGA simulation tool (Golden Gate [\[20\]](#page-11-13)) to
    simulate the RAM via serialized accesses with a decoupled model [\[14\]](#page-11-7).


    ![](_page_6_Picture_0.jpeg)


    Figure 4: Original image with detected lines highlighted in red.


    Platforms including the Gemmini accelerator can only be designed to work at 50MHz
    while the remaining ones can reach 80MHz. Thus, the later have been designed both
    at 50 and 80 MHz for a fair comparison against designs equipped with Gemmini.
    Notably, the platforms generated are:


    1. *Platform* 1: Rocket single core.


    This architecture includes a single *Big* Rocket core. There are four different
    sizes for the core, namely *Big, Medium, Small* and *Tiny*, with different features
    such as the size of L1-cache. The *Big* Rocket is the only one providing Floating
    Point Unit. It also has by default the parameters shown in Table [4.](#page-7-0)
    More information on the details of the configuration can be found in [\[19\]](#page-11-12).


    2. *Platform* 2: Rocket dual core.


    This is the same configuration as Platform 1 but it includes two *Big* Rocket
    cores. This dual configuration also has the option MTModels enabled in the FireSim
    platform configuration, so that each core is simulated with a separate thread
    of execution on a shared underlying physical implementation [\[14\]](#page-11-7).


    3. *Platform* 3: Heterogeneous Rocket single core + Gemmini Accelerator.


    This architecture is made up by a *Big* Rocket core and a Gemmini matrix multiplication
    accelerator, which has been designed with default options: 16x16 8-bit systolic
    array, both dataflows supported (output-stationary and weight-stationary), float
    data type supported, a set of accumulator registers with 64B of total capacity,
    a 256KB scratchpad with 4 banks, a small TLB with 4 entries and a bus width of
    128 bits.


    <span id="page-6-0"></span>4. *Platform* 4: BOOM Single core.


    This architecture includes a single *Large* BOOM core. There are different macros
    for defining BOOM cores of *Giga, Mega, Large, Medium* and *Small* sizes. The
    main differences between the one that we are using and the rest is the number
    of entries in the ROB and some L1-cache parameters. Thus, in the configuration
    *WithN-LargeBooms* the value of notable parameters are shown in Table [4.](#page-7-0)
    More information on the details of the configuration can be found in [\[19\]](#page-11-12).
    The Large size was chosen because it is just big enough to provide the required
    performance with minimum power consumption.


    5. *Platform* 5: BOOM dual core.


    This is the same configuration as Platform 4 but it includes two *Large* BOOM
    cores, with the MT-Models option enabled.


    6. *Platform* 6: Heterogeneous BOOM single core + Gemmini Accelerator.


    This architecture is made up by a *Large* BOOM core and a Gemmini matrix multiplication
    accelerator, which has been designed with the default options explained earlier.


    # 5.2 Workloads generated


    Different workloads were designed for running on the platforms described in the
    previous section. They are the following:


    1. *Workload* 1: Multithreaded application on top of Linux buildroot distribution.


    In this workload, a multithreaded application (with each thread computing the
    addition of 2


    |                  |             | Big Rocket | Large Boom |

    |------------------|-------------|------------|------------|

    | I&D Cache        | Size        | 16KB       | 32KB       |

    |                  | Sets        | 64         | 64         |

    |                  | Ways        | 4          | 8          |

    |                  | Prefetching | no         | disabled   |

    | TLB              | Sets        | 1          | 1          |

    |                  | Ways        | 32         | 512        |

    | BTB Entries      |             | 28         | 28         |

    | ROB Entries      |             | no         | 96         |

    | FPU              |             | yes        | yes        |

    | Branch predictor |             |            |            |

    | entries          |             | no         | 128        |


    <span id="page-7-0"></span>Table 4: Platform configuration options.


    long arrays, as explained in [\[19\]](#page-11-12)) is executed on top of Linux.
    It has been specifically designed to fully exploit the parallel features of the
    platforms, so that it can be used to evaluate the maximum performance obtainable
    in the different multicore designs. This value will serve as an upper bound when
    we evaluate the performance achieved by our target application.


    <span id="page-7-1"></span>2. *Workload* 2: Line detection algorithm on top of
    Linux buildroot distribution.


    In this workload, the modified version of the line detection application explained
    in Section [4](#page-4-2) is executed on top of Linux.


    <span id="page-7-2"></span>3. *Workload* 3: Line detection algorithm for baremetal
    platforms with Gemmini.


    In this workload, in addition to the modifications in Section 4, we have modified
    the line detection algorithm to add matrix multiplications. In the original version,
    this algorithm multiplies some mask values to a pixel neighborhood manually by
    writing the corresponding scalar multiplications. We have rewritten these multiplications
    in a matrix form, obtaining a 5x5 matrix for the mask and a 5x5 neighborhood matrix
    for each pixel. As for the platform, the differences with respect to the previous
    workload are that this platform includes a Gemmini accelerator for matrix multiplication
    and the fact that no operating system is available for this platform. Thus, matrix
    multiplications in the code have to be replaced by calls to a Gemmini multiplication.
    As previously explained, some C macros are provided with the designs that make
    it possible to easily programming the accelerator. First, data need to be moved
    from the main memory to the scratchpad memory in Gemmini, then the multiplication
    is performed in tiles and finally the results are transferred back to the main
    memory. We will use the *tiled matmul auto* function that receives the dimensions
    of both matrices as input parameters and automatically splits the multiplication
    in


    blocks of suitable size for the systolic array and memory, thus performing the
    whole multiplication. Finally, system calls not available outside Linux were removed
    from the code and their functionality was implemented in an equivalent way.


    # 5.3 Experiments


    In this section we show the results obtained from the execution of the workloads
    on the different platforms designed. The metrics measured are clock cycles and
    instructions retired provided by the performance counters of the target platforms.


    #### 5.3.1 Experiment 1: Execution of a multithreaded application on single core
    and dual core platforms both with Rocket and BOOM cores.


    The goal of this experiment is to verify the maximum performance attainable in
    the different platforms by using a massively parallel application. Therefore we
    employ *Workload 1*, configured with as many independent threads as the number
    of cores in the system, i.e., 1 or 2 depending on the specific platform.


    The target platforms in this case include both single and dual core processors
    (either Rocket or BOOM, running at 80MHz) that correspond to the *Platforms* 1,
    2, 4 and 5 previously described.


    The results of the experiment are shown in Table [5,](#page-8-0) both for a simulation
    in which the main loop is executed once (column labelled *N times = 1*) and 8
    times (column *N times = 8*). The number of clock cycles for the experiment with
    8 iterations is 8 times the one of the single iteration experiment. Besides, speedup
    of the dual core version with respect to the single core is very close to 2x for
    both Rocket and BOOM. Finally, comparing the performance of the different cores,
    BOOM achieves almost 2.2x higher performance than Rocket, so that a single BOOM
    core outperforms a dual core Rocket running at the same frequency for this highly
    parallel application.


    Thus, it has been verified that multithreaded applications are being correctly
    simulated in the multicore


    |                   | N times = 1 | N times=8 |  |

    |-------------------|-------------|-----------|--|

    |                   | Cycles      |           |  |

    | Rocket singlecore | 2.01×109    | 1.59×1010 |  |

    | BOOM singlecore   | 9.17×108    | 7.31×109  |  |

    | Rocket dualcore   | 9.97×108    | 7.99×109  |  |

    | BOOM dualcore     | 4.53×108    | 3.66×109  |  |

    | Speedup BOOM      |             |           |  |

    | vs Rocket         | 2.19x       | 2.18x     |  |

    | Speedup Rocket    |             |           |  |

    | dual vs single    | 2.02x       | 1.99x     |  |

    | Speedup BOOM      |             |           |  |

    | dual vs single    | 2.02x       | 1.99x     |  |


    <span id="page-8-0"></span>Table 5: Cycles when executing Workload 1 on Platforms
    1, 2, 4 and 5.


    platforms, achieving the expected speedup. Furthermore, the comparison between
    both types of cores has been established.


    #### 5.3.2 Experiment 2: Execution of the line detection application on Rocket
    and BOOM single cores.


    This second experiment involves simulating the execution of the line detection
    application (*workload* [2\)](#page-7-1) on the Rocket and BOOM single core platforms
    employed in the previous experiment (Platforms 1 and 4), also running at 80MHz.
    In Table [6](#page-9-0) we report the number of clock cycles and instructions
    retired corresponding to each of the different parts of the line detection algorithm,
    as well as the average cycles per instructions (CPI) value. In addition, we calculate
    the actual time from the cycles and clock frequency, resulting in times of around
    half second. In particular, for the Rocket core we obtain a total execution time
    of 0.648s and for the Boom core 0.327s. As shown, the CPI for the Hough transform
    is higher than 3 in both Rocket and BOOM platforms. Moreover, its execution on
    the BOOM processor almost matches the time reported on the Rocket platform, as
    the multiple data dependencies in the code make out-of-order capabilities useless.


    On the other hand, the Canny and the GetCoordinates algorithms exhibit lower CPI
    numbers in both platforms, achieving a speedup of 2x when executing on the Boom
    processor with respect to Rocket, due to the greater instruction level parallelism
    that can be extracted from both algorithms. Recall that the Canny algorithm is
    the most relevant part of the line detection application, consuming close to 90%
    of the total execution time (as shown in Table [3\)](#page-5-3). In conclusion,
    using the BOOM core for the execution of the workload is interesting in terms
    of the global speedup achieved.


    #### 5.3.3 Experiment 3: Execution of the line detection application on heterogeneous
    platforms with a Rocket or BOOM single core and a Gemmini matrix multiplication
    accelerator.


    This experiment consists on simulating the execution of the modified line detection
    application (*workload* [3\)](#page-7-2) on the heterogeneous single core platforms
    made up by a Rocket or BOOM processor plus a Gemmini matrix multiplication accelerator
    running at 50MHz.


    Table [7](#page-9-1) shows first the results obtained in the simulation of Workload
    3 (line detection application for bare metal) on a Rocket single core (used as
    baseline for computing speedups) and a BOOM single core, both running at 50MHz.
    As the first row shows, BOOM is 41% faster than Rocket. The execution results
    from the previous section, that is, those corresponding to Workload 2 (line detection
    application for Linux) on Rocket and BOOM single core at 80MHz are also compared
    to the baseline execution, achieving speedups of 2.09x and 3.76x respectively.
    It is worth noting that although the code of Workloads 2 and 3 does not exactly
    match, it performs the same functionality. Finally, the results from the simulation
    of Workload 3 on heterogeneous platforms in which matrix multiplications are performed
    using the Gemmini accelerator are also recap in Table [7.](#page-9-1) According
    to them, speedups of 2.36x and 3.7x are reported for Rocket and BOOM based platforms
    respectively, with respect to the baseline. Although these speedups can be considered
    as significant, they are far from the maximum values attainable by the accelerator.
    The reason is that the size of the matrices employed is smaller than that of the
    systolic array, which indeed is not fully utilized.


    Furthermore, in the graph shown in Fig. [5](#page-10-6) we can see the time corresponding
    to all the single core and heterogeneous experiments. The first thing we notice
    is that the out-of-order execution of the Boom core is beneficial for the Canny
    algorithm, leaving the Rocket core as the slowest by far at both 50 and 80MHz.
    Furthermore, we see how the combination of the cores with the Gemmini accelerator
    at 50MHz gives us a similar time to the same cores without accelerator at 80MHz,
    which gives us a great benefit in terms of consumption by running at a lower clock
    frequency which should be taken into account in the field of autonomous vehicles,
    as it would provide greater autonomy. In addition, we note that the shortest time
    is under half a second, in particular 300ms, and we achieve it with the combination
    of the Boom core and the Gemmini accelerator at a clock frequency of 50MHz. Thus,
    a vehicle travelling at 50km/h could run the algorithm every 4 metres approximately
    and if necessary, options such as mounting several systems in parallel or slightly
    increasing the clock frequency for faster processing could be explored.


    In conclusion, for this application with small matrices, both platforms based
    on the BOOM core deliver similar performance (speedup of around 3.7x with re-


    <span id="page-9-0"></span>


    |                        |             | Cycles   | Instructions | CPI   | Time(ms)
    |

    |------------------------|-------------|----------|--------------|-------|----------|

    | Rocket singlecore      | Canny       | 2.18×109 | 9.06×108     | 2.40  | 648,38   |

    |                        | Hough       | 3.32×108 | 9.35×107     | 3.55  | 98,86    |

    |                        | Coordinates | 6.49×106 | 3.47×106     | 1.87  | 1,93     |

    | Boom singlecore        | Canny       | 1.08×109 | 9.06×108     | 1.19  | 327,10   |

    |                        | Hough       | 3.16×108 | 9.35×107     | 3.38  | 96,07    |

    |                        | Coordinates | 3.2×106  | 3.47×106     | 0.92  | 0,97     |

    | Speedup Boom vs Rocket | Canny       | 2.02x    | 1.00x        | 2.02x | 1.98x    |

    |                        | Hough       | 1.05x    | 1.00x        | 1.05x | 1.03x    |

    |                        | Coordinates | 2.03x    | 1.00x        | 2.03x | 1.99x    |


    Table 6: Cycles, instructions retired and CPI when executing Workload 2 on Platforms
    1 and 4 at 80MHz.


    <span id="page-9-1"></span>Table 7: Speedup results when executing Workload 2
    on Platforms 1 and 4 at 80MHz, and Workload 3 on Platforms 3, 4, 6 at 50MHz, with
    respect to execution of Workload 3 on Platform 1 at 50 MHz.


    |                         | Speedup vs Rocket singlecore 50MHz |       |             |       |  |

    |-------------------------|------------------------------------|-------|-------------|-------|--|

    |                         | Canny                              | Hough | Coordinates
    | Total |  |

    | Boom singlecore 50MHz   | 1.44x                              | 1.04x | 1.85x       |
    1.41x |  |

    | Rocket singlecore 80MHz | 2.26x                              | 0.98x | 1.07x       |
    2.09x |  |

    | Boom singlecore 80MHz   | 4.57x                              | 1.03x | 2.18x       |
    3.76x |  |

    | Rocket + Gemmini 50MHz  | 2.54x                              | 1.16x | 1.03x       |
    2.36x |  |

    | Boom + Gemmini 50MHz    | 4.43x                              | 1.07x | 1.98x       |
    3.70x |  |


    spect to the Rocket baseline), being the BOOM single core at 80MHz slightly faster
    than the BOOM + Gemmini at 50MHz. Even in this non favourable scenario, the accelerator
    allows to report high performance working at a lower frequency, being more power
    efficient than the single core platform running at higher frequency.


    # 6 Conclusions and future work


    In this paper we have explored the acceleration of a line detection algorithm
    in the autonomous car environment using a heterogeneous system consisting of a
    general-purpose RISC-V core and a domain-specific accelerator. In particular,
    we analyzed the application to identify the most computationally intensive parts
    of the code and adapted it accordingly for more efficient processing.


    The first conclusion we extract from this work is that RISC-V architecture provides
    a hw-sw ecosystem that is well suited for IoT in general and autonomous vehicle
    systems in particular, due to its versatility and modularity, which allows to
    generate platforms adapted to different scenarios. In fact, in this work, we designed
    six different platforms covering a wide spectrum of alternatives: on one side
    single and dual core homogeneous systems, and on the other side heterogeneous
    platforms with a single core plus a matrix multiplication accelerator –all of
    them including high performance BOOM cores or more efficient Rocket cores.


    Also, a multithreaded application with high data parallelism has been designed
    to analyze the performance of the homogeneous platforms built. Thus, it has been


    verified that multithreaded applications are being correctly simulated in the
    multicore platforms, achieving the expected speedup. Furthermore, the comparison
    between both types of cores determined that a single BOOM core is up to 2.19 times
    faster than a Rocket one.


    Finally, the original application of line detection has been modified in order
    to decrease its execution time without losing accuracy, and it has also been adapted
    for bare metal and Gemmini execution. We simulated the application on all designed
    platforms. BOOM-based platforms reported the best performance numbers, achieving
    speedups of 3.7x with respect to the baseline (a single Rocket core running at
    50MHz), and being the single BOOM core running at 80MHz slightly faster than the
    BOOM + Gemmini platform at 50MHz. As previously stated, even working at a lower
    frequency the accelerator allows to report high performance, being more power
    efficient than the single core counterpart working at a higher frequency. It is
    worth noting that our goal in this work was to explore how an domain-specific
    accelerator was able to accelerate the baseline execution (just using a conventional
    single core) in applications belonging to autonomous vehicles environment.


    As future work, other applications which involve multiplication of big matrices
    can be adapted to heterogeneous platforms in order to implement more of the functionalities
    required for autonomous vehicles. Moreover, Gemmini is expected to achieve much
    higher speedups for inference using neural networks, as shown in [\[6\]](#page-10-5),
    so exploring this issue constitutes an interesting avenue for future work.


    ![](_page_10_Figure_0.jpeg)


    <span id="page-10-6"></span>Figure 5: Time results when executing Workload 2 on
    Platforms 1 and 4 at 50MHz and 80MHz, and Workload 3 on Platforms 3 and 6 at 50MHz.


    ## Competing interests


    The authors have declared that no competing interests exist.


    #### Funding


    The present work has been funded by the Comunidad de Madrid through project S2018/TCS-4423
    and by the Ministry of Science, Innovation and Universities through project RTI2018-093684-B-I00.


    #### Authors'' contribution


    MJB wrote the programs, conducted the experiments, analyzed the results and wrote
    the manuscript; KO and FC conceived the idea, analyzed the results and wrote the
    manuscript; FT revised the manuscript. All authors read and approved the final
    manuscript.


    # References


    <span id="page-10-0"></span>[1] S.-C. Lin *et al.*, "The architectural implications
    of autonomous driving: Constraints and acceleration," in *Proceedings of the Twenty-Third
    International Conference on Architectural Support for Programming*


    *Languages and Operating Systems*, ASPLOS ''18, p. 751–766, 2018.


    - <span id="page-10-1"></span>[2] S. Kato, E. Takeuchi, Y. Ishiguro, Y. Ninomiya,
    K. Takeda, and T. Hamada, "An open approach to autonomous vehicles," *IEEE Micro*,
    vol. 35, pp. 60–68, 11 2015.

    - <span id="page-10-2"></span>[3] P. Bose, A. J. Vega, S. V. Adve, V. S. Adve,
    and V. J. Reddi, "Secure and resilient socs for autonomous vehicles," in *Proceedings
    of the 3rd International Workshop on Domain Specific System Architecture (DOSSA)*,
    pp. 1–6, 2021.

    - <span id="page-10-3"></span>[4] B. Yu *et al.*, "Building the computing system
    for autonomous micromobility vehicles: Design constraints and architectural optimizations,"
    in *Proceedings of 53rd Annual IEEE/ACM International Symposium on Microarchitecture
    (MICRO)*, pp. 1067–1081, 2020.

    - <span id="page-10-4"></span>[5] J. Zhao, B. Korpan, A. Gonzalez, and K. Asanovic,
    "Sonicboom: The 3rd generation berkeley out-of-order machine," in *Proceedings
    of the 4th Workshop on Computer Architecture Research with RISC-V (CARRV)*, pp.
    1–7, 2020.

    - <span id="page-10-5"></span>[6] H. Genc *et al.*, "Gemmini: Enabling systematic
    deeplearning architecture evaluation via full-stack integration," in *Proceedings
    of the 58th Annual Design Automation Conference (DAC)*, pp. 769–774, 2021.

    - <span id="page-11-0"></span>[7] "The 6 levels of vehicle autonomy explained."
    Available at: [https://www.synopsys.com/](https://www.synopsys.com/automotive/autonomous-driving-levels.html)
    [automotive/autonomous-driving-levels.](https://www.synopsys.com/automotive/autonomous-driving-levels.html)
    [html](https://www.synopsys.com/automotive/autonomous-driving-levels.html). Accessed
    on 2022-09-07.

    - <span id="page-11-5"></span>[8] O. Vermesan *et al.*, *IoT technologies for
    connected and automated driving applications. Internet of Things - The Call of
    the Edge*, pp. 306–332. River Publishers, Oct. 2020.

    - <span id="page-11-1"></span>[9] R. Coppola and M. Morisio, "Connected car: technologies,
    issues, future trends," *ACM Computing Surveys (CSUR)*, vol. 49, no. 3, pp. 1–36,
    2016.

    - <span id="page-11-2"></span>[10] T. Rateke *et al.*, "Passive vision region-based
    road detection: A literature review," *ACM Computing Surveys (CSUR)*, vol. 52,
    no. 2, pp. 1–34, 2019.

    - <span id="page-11-3"></span>[11] F. Bounini, D. Gingras, V. Lapointe, and H.
    Pollart, "Autonomous vehicle and real time road lanes detection and tracking,"
    in *IEEE Vehicle Power and Propulsion Conference (VPPC)*, pp. 1–6, 2015.

    - <span id="page-11-4"></span>[12] G. Zhang, N. Zheng, C. Cui, Y. Yan, and Z.
    Yuan, "An efficient road detection method in noisy urban environment," in *Proceedings
    of IEEE Intelligent Vehicles Symposium*, pp. 556 – 561, 2009.

    - <span id="page-11-6"></span>[13] A. Amid *et al.*, "Chipyard: Integrated design,
    simulation, and implementation framework for custom socs," *IEEE Micro*, vol.
    40, no. 4, pp. 10–21, 2020.

    - <span id="page-11-7"></span>[14] S. Karandikar *et al.*, "Firesim: Fpga-accelerated
    cycleexact scale-out system simulation in the public cloud,"


    in *Proceedings of ACM/IEEE 45th Annual International Symposium on Computer Architecture
    (ISCA)*, pp. 29–42, 2018.


    - <span id="page-11-8"></span>[15] "Amazon web services (aws)." Available at:
    [https:](https://aws.amazon.com/es) [//aws.amazon.com/es](https://aws.amazon.com/es),
    2021. Accessed on 2022-09- 07.

    - <span id="page-11-9"></span>[16] K. Asanovic *et al.*, "The rocket chip generator.
    eecs department," *University of California, Berkeley, Tech. Rep. UCB/EECS-2016-17*,
    vol. 4, 2016.

    - <span id="page-11-10"></span>[17] J. F. Canny, "Finding edges and lines in images,"
    *Theory of Computing Systems - Mathematical Systems Theory*, p. 16, 1983.

    - <span id="page-11-11"></span>[18] R. O. Duda and P. E. Hart, "Use of the hough
    transformation to detect lines and curves in pictures," *Communications of the
    ACM*, vol. 15, no. 1, p. 11–15, 1972.

    - <span id="page-11-12"></span>[19] M. J. Belda, "Image processing in autonomous
    vehicles on a risc-v with accelerator," *Master Thesis, UCM*, 2022.

    - <span id="page-11-13"></span>[20] A. Magyar *et al.*, "Golden gate: Bridging
    the resourceefficiency gap between asics and fpga prototypes," in *Proceedings
    of IEEE/ACM International Conference on Computer-Aided Design (ICCAD)*, pp. 1–8,
    2019.


    Citation: M.J. Belda, K. Olcoz, F. Castro and F. Tirado. *Optimization of a line
    detection algorithm for autonomous vehicles on a RISC-V with accelerator*. Journal
    of Computer Science & Technology, vol. xx, no. x, pp. x–x, 202x.


    ✔


    ✕


    DOI: 10.24215/16666038.18.e01


    ✗


    ✖


    Received: August x, 2022 Accepted: xxx. Copyright: This article is distributed
    under the terms of the Creative Commons License CC-BY-NC.'
  decisions:
    evaluation_prompt: 'Qualified. Reason: The paper includes an "Experimental results"
      section (Section 5) that details the evaluation of the proposed line detection
      algorithm on various hardware platforms. It discusses performance metrics such
      as clock cycles, instructions retired, and speedup comparisons, and provides
      tables and figures illustrating the results.'
    related_work_prompt: '- Qualified. Reason: The paper meaningfully engages with
      prior research throughout its text. It includes a "Basic notions and state of
      the art" section that discusses the current state of autonomous vehicles and
      related technologies, citing relevant literature. Additionally, the paper references
      previous work on the Canny algorithm and the Hough transform, and it compares
      the performance of different platforms and algorithms, citing multiple sources
      to support its analysis and conclusions.'
    novelty_prompt: 'Qualified. Reason: The paper proposes a novel approach by accelerating
      a line detection algorithm for autonomous vehicles using a heterogeneous system
      with a RISC-V core and a domain-specific accelerator. It claims contributions
      such as analyzing the application to identify computationally intensive parts
      and adapting it for more efficient processing, achieving a 3.7x speedup. These
      elements demonstrate novelty in method and application.'
    review_only_prompt: '- Qualified. Reason: The paper introduces new contributions
      by exploring the acceleration of a line detection algorithm for autonomous vehicles
      using a RISC-V core and a domain-specific accelerator. It provides detailed
      analysis, experimental results, and modifications to existing algorithms, indicating
      original research rather than a summary of existing work.'
  topics:
    main_topic: Computer Architecture
    main_topic_reasoning: The paper discusses the optimization of a line detection
      algorithm focusing on the hardware implementation using a RISC-V core and an
      accelerator, which are central topics in computer architecture.
    secondary_topic: Embedded & Real-time Systems
    secondary_topic_reasoning: The application of the line detection algorithm in
      autonomous vehicles relates to embedded and real-time systems, as these systems
      require timely and efficient processing for decision-making in critical scenarios.
    main_topic_sub: Accelerator-based, application-specific, and reconfigurable architectures
    secondary_topic_sub: Other
