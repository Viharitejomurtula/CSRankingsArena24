papers:
- title: 'Reuse Detector: Improving the Management of STT-RAM SLLCs'
  abstract: 'Various constraints of Static Random Access Memory (SRAM) are leading
    to

    consider new memory technologies as candidates for building on-chip shared

    last-level caches (SLLCs). Spin-Transfer Torque RAM (STT-RAM) is currently

    postulated as the prime contender due to its better energy efficiency, smaller

    die footprint and higher scalability. However, STT-RAM also exhibits some

    drawbacks, like slow and energy-hungry write operations, that need to be

    mitigated. In this work we address these shortcomings by leveraging a new

    management mechanism for STT-RAM SLLCs. This approach is based on the previous

    observation that the stream of references arriving at the SLLC of a Chip

    MultiProcessor (CMP) exhibits reuse locality, i.e., those blocks referenced

    several times manifest high probability of forthcoming reuse. In this paper, we

    employ a cache management mechanism that selects the contents of the SLLC aimed

    to exploit reuse locality instead of temporal locality. Specifically, our

    proposal consists in the inclusion of a Reuse Detector between private cache

    levels and the STT-RAM SLLC to detect blocks that do not exhibit reuse, in

    order to avoid their insertion in the SLLC, hence reducing the number of write

    operations and the energy consumption in the STT-RAM. Our evaluation reveals

    that our scheme reports on average, energy reductions in the SLLC in the range

    of 37-30\%, additional energy savings in the main memory in the range of 6-8\%

    and performance improvements of 3\% up to 14\% (16-core) compared to an STT-RAM

    SLLC baseline where no reuse detector is employed. More importantly, our

    approach outperforms DASCA, the state-of-the-art STT-RAM SLLC management,

    reporting SLLC energy savings in the range of 4-11\% higher than those of

    DASCA, delivering higher performance in the range of 1.5-14\%, and additional

    improvements in DRAM energy consumption in the range of 2-9\% higher than

    DASCA.'
  url: http://arxiv.org/abs/2402.00533v1
  keywords: ''
  document: '# 1. INTRODUCTION


    In the last years chip multiprocessors have become majority on many off-the-shelf
    systems, such as high performance servers, desktop systems, mobile devices and
    embedded systems. In all of them the designers usually include a multilevel memory
    hierarchy, where the shared last-level cache (SLLC) plays an important role in
    terms of cost, performance and energy consumption. As for the cost, the SLLC generally
    occupies a chip area similar or even bigger than that of cores. Regarding performance
    and energy consumption, the SLLC is the last resource before accessing the main
    memory, which delivers higher energy consumption and lower performance as it is
    located outside


    the chip.


    The technologies currently employed in building SLLCs are mainly SRAM or embedded
    DRAM. However, they both reveal as power-hungry, especially for the large sizes
    required as the number of cores increases. One way of mitigating this problem
    is to employ emerging non-volatile memory technologies. Among them, Spin-Transfer
    Torque RAM (STT-RAM) is clearly the prime contender. STT-RAM removes almost all
    the static power consumption and, compared to SRAM, provides higher density and
    therefore much higher capacity within the same budget. Moreover, it delivers higher
    read efficiency in terms of latency and energy. Nevertheless, some obstacles restrict
    the adoptio n of STT-RAM as last-level cache for the next generation


    2 RODRIGUEZ-RODRIGUEZ ET AL


    of CMPs: its write operation is slower and requires more energy than an SRAM cache.
    These constraints may lead to a performance drop and even to almost cancel the
    energy savings derived from the minimal static power consumption of STT-RAM.


    In addition, previous research states that conventional SLLC designs are inefficient
    since they waste most storage space [\[1,](#page-20-0) [2\]](#page-20-1). This
    is due to the fact that SLLC management policies often lead to store *dead blocks*,
    i.e., blocks that will not be referenced again before eviction. Indeed, it is
    frequent that blocks were dead the very first time they enter into the SLLC. This
    is mainly because the cache levels closer to the processor exploit most of the
    *temporal locality*, which therefore becomes largely filtered before accessing
    the SLLC. With the goal of avoiding this effect and hence increasing the hit rate,
    various mechanisms that modify the SLLC insertion and replacement policies have
    been proposed recently.


    This work addresses the shortcomings aforementioned by focusing on improving the
    efficiency, in terms of both performance and energy, of a non-inclusive and nonexclusive
    STT-RAM SLLC in a chip multiprocessor system. Notably, we present a new mechanism
    of content selection for last-level caches that benefits from the *reuse locality*
    that SLLC references exhibit [\[3,](#page-20-2) [4\]](#page-20-3). This locality
    lies in the following principle: when a block is referenced twice in the last
    level cache (i.e. it is reused), this block has a good chance of being referenced
    again in the near future. Our approach pursues to insert in the SLLC only those
    blocks that exhibit reuse at that level. For this purpose, we propose to include
    a new hardware resource between the SLLC and the private cache levels –referred
    to as *Reuse Detector*– which determines for each block evicted from the private
    cache levels if it has been reused or not at the SLLC. If the block is determined
    to having been reused, it is inserted (or it updates) in the SLLC. Otherwise,
    the block *bypasses* the SLLC and is sent directly to main memory.


    Our proposal is evaluated in quad, eight and 16-core systems running multiprogrammed
    workloads, and our experimental results reveal that the reuse detector avoids
    the insertion of low-utility blocks in the STT-RAM SLLC, making it easier to retain
    most of reused blocks. This enables us to reduce the amount of the slow and energyhungry
    writes to the STT-RAM SLLC, which translates into energy consumption reduction
    and system performance improvement, outperforming other recent approaches.


    The rest of the paper is organized as follows: Section 2 motivates our work and
    explains some necessary background. Section 3 presents our proposal to improve
    the STT-RAM LLC management. Sections 4 and 5 detail the experimental framework
    used and the obtained results, respectively. Section 6 recaps some related work
    and finally, Section 7 concludes the paper.


    # 2. BACKGROUND AND MOTIVATION


    In this section we first motivate the need of a new SLLC management scheme by
    describing the main limitations of


    <span id="page-1-1"></span><span id="page-1-0"></span>![](_page_1_Figure_8.jpeg)


    FIGURE 1. (a) STT-RAM memory cell structure. (b) STT-RAM equivalent circuit.


    SRAM technology and conventional management. Then we briefly describe the DASCA
    scheme, which is the closest approach to our work and the state-of-the-art STT-RAM
    SLLC management scheme [\[1\]](#page-20-0).


    # 2.1. SRAM and STT-RAM technologies


    As stated above, various emerging technologies are currently considered to replace
    SRAM as the building-block for SLLCs, being STT-RAM the best placed to overcome
    SRAM constraints, such as energy consumption and read operation latency.


    The main difference between STT-RAM and SRAM is that the information carrier of
    the former is a Magnetic Tunnel Junction (MTJ) instead of electric charges. An
    MTJ contains two ferromagnetic layers (denoted as free and fixed layers) and one
    tunnel barrier layer, see Figure [1\(a\).](#page-1-0) The fixed layer has a fixed
    magnetic direction while the free layer can change its magnetic direction by passing
    a current through the MTJ. If the two ferromagnetic layers have different directions,
    the MTJ resistance is high, indicating a "1" state; if the two layers have the
    same direction, the MTJ resistance is low, indicating a "0" state. A read operation
    to an MTJ is performed by applying a small voltage difference between two electrodes
    of the MTJ and sensing the current flow (see Figure [1\(b\),](#page-1-1) where
    the STT-RAM cell is represented as a variable resistor). A write operation is
    performed by applying a large voltage difference between two electrodes for a
    given duration called write pulse width.


    Table [1](#page-2-0) shows the key features of an 1-bank 1MB LLC implemented with
    SRAM and STT-RAM 22 nm technology, modeled with CACTI 6.5 [\[5\]](#page-21-0)
    and NVSim [\[6\]](#page-21-1), respectively. As shown, an STT-RAM cache exhibits
    smaller die footprint and better efficiency in read operations than an SRAM cache.
    More importantly, an STT-RAM cache consumes almost two orders of magnitude less
    static power compared to SRAM. Conversely, the STT-RAM cache exhibits a significant
    drawback that needs to be mitigated: the poor write performance both in terms
    of latency and energy consumption.


    # 2.2. SLLC management techniques


    Regardless of implementation technology, last-level caches usually suffer from
    the same problem: they keep data


    | Parameter          | SRAM   | STT-RAM | Ratio SRAM/STT-RAM |

    |--------------------|--------|---------|--------------------|

    | Area (mm2<br>)     | 0.94   | 0.35    | 2.68               |

    | Read Latency (ns)  | 8.75   | 5.61    | 1.56               |

    | Write Latency (ns) | 8.75   | 16.5    | 0.53               |

    | Read Energy (nJ)   | 0.56   | 0.32    | 1.75               |

    | Write Energy (nJ)  | 0.56   | 1.31    | 0.43               |

    | Leakage Power (mW) | 190.58 | 3.09    | 61.67              |


    <span id="page-2-0"></span>TABLE 1. Area, latency and energy consumption for 22
    nm SRAM and STT-RAM 1MB caches.


    assuming that recently referenced lines are likely to appear in the near future
    (*temporal locality*). Nevertheless, various recent studies point out that the
    reference stream entered in the SLLC does not usually exhibit temporal locality.
    Notably, in [\[3\]](#page-20-2) the authors observe that this reference stream
    exhibits *reuse locality* instead of temporal locality. Essentially, that term
    describes the property that the second reference to a line is a good indicator
    of forthcoming reuse and also that recently reused lines are more valuable than
    other lines reused longer.


    The studies carried out in [\[3,](#page-20-2) [4,](#page-20-3) [7\]](#page-21-2)
    demonstrate, considering a large amount of multiprogrammed workloads and different
    multiprocessor configurations, two important aspects: first, most lines in the
    SLLC are dead (they will not receive any further hits during their lifetime in
    the SLLC) and second, most SLLC hits come from a small subset of lines. We have
    performed our own analysis about the behavior of the blocks evicted from the SLLC,
    in terms of the amount of accesses they receive before eviction, in the scenario
    with the configuration detailed in Table [2,](#page-7-0) but, as a starting point,
    employing just one core. Figure [2](#page-2-1) illustrates this behavior, grouping
    the blocks into three different categories: no reuse, just one reuse or more than
    one reuse (multiple reuse).


    ![](_page_2_Figure_5.jpeg)


    <span id="page-2-1"></span>FIGURE 2. Breakdown of blocks replaced from the LLC
    according to the number of accesses they receive before eviction.


    As shown, our experimental results confirm that most lines in the LLC are dead.
    Notably, around 70% of the blocks do not receive any further access since the
    moment they enter the LLC. Only around 5% of the blocks just receives one further
    hit (i.e. one reuse) and around 25% exhibit more than one reuse.


    Consequently, getting blocks with just one use (the block fill, so no reuse) to
    bypass the LLC when they are evicted from the previous level caches, and just
    storing blocks with reuse (at least two LLC accesses), should allow to hold the
    small fraction of blocks with multiple reuses, increasing the LLC hit rate and
    improving system performance.


    Furthermore, Figure [3](#page-2-2) shows that most LLC hits are to blocks having
    multiple reuses, which together with the aforementioned fact that most blocks
    inserted in the LLC do not experience any reuse, highly justify the idea of a
    content selector based on reuse detection between private caches and LLC.


    ![](_page_2_Figure_11.jpeg)


    <span id="page-2-2"></span>FIGURE 3. Breakdown of block hits at the LLC according
    to the number of accesses they have received before read.


    # 2.3. DASCA scheme


    In [\[1\]](#page-20-0), the authors propose *Dead Write Prediction Assisted STT-RAM
    Cache Architecture* (DASCA) to predict and bypass dead writes (writes to data
    in last level caches not referenced again during the lifetime of corresponding
    cache blocks) for write energy reduction. In this work dead writes are classified
    into three categories: dead-on-arrival fills, dead-value fills and closing writes,
    as a theoretical model for redundant write elimination. On top of that they also
    present a dead write predictor based on a state-of-the-art dead block predictor
    [\[2\]](#page-20-1). Thus, DASCA bypasses a write operation to the SLLC only if
    it is predicted not to incur extra cache misses.


    Notably, DASCA adds a specific field to each line at the private levels to store
    the PC (program counter) of the instructions writing a block, being this PC only
    updated upon write operations. Also, a PC-signature table (prediction table) is
    included in the design in order to make the prediction about dead writes (this
    table is updated according to the mechanism shown in the Table 2 of the paper
    itself [\[1\]](#page-20-0)). Specifically, the mechanism samples a few cache sets
    and keeps track of PC information only for those sets. Predictions are made via
    the predictor table, made up of saturating counters similar to those used in a
    bimodal branch predictor, being the counters indexed by the signatures stored
    in the sampler entries. Thus, this PC-based predictor correlates dead blocks with
    addresses of memory instructions (signatures), so that different signatures are
    used depending on the kind of dead write predicted.


    # 3. DESIGN


    In this section we first describe the baseline system we start from. Then we describe
    in detail the proposed design built on top of that.


    # 3.1. Baseline system


    The memory hierarchy used in the baseline multi-core system includes two private
    levels (L1 and L2) and a lastlevel cache shared among all the cores (SLLC). All
    caches are write-back, write-allocate and LRU. L1 and L2 are inclusive while the
    SLLC is non inclusive.


    The baseline management of this memory hierarchy is as follows: When a block is
    requested to Main Memory (MM), it is copied to the private cache levels of the
    requester core, but not to the SLLC. During its lifetime at the private levels
    of the core, the block can be requested by other cores, in which case a copy will
    be sent from a private L2 cache to the L1-L2 caches of the requesting core, as
    dictated by the directory-based coherency mechanism (please refer to Section [4](#page-6-0)
    for more details on the coherency mechanism).


    When a block is evicted from an L2 cache, the SLLC is checked: In case the block
    is not present there (either because it has not been inserted yet or because it
    has already been inserted and evicted by the SLLC replacement mechanism), it is
    inserted in the SLLC, otherwise if the block is already in the SLLC, the block
    is updated or just discarded, depending respectively on whether the block is dirty
    or clean. Thus, in our hierarchy, SLLC insertions never come from MM but from
    an L2, in a similar way to an exclusive policy. Note however that our mechanism
    differs from an exclusive policy in that, as a result of a hit in the SLLC, the
    block is copied to the private cache levels of the requester core, but maintained
    in the SLLC.


    # 3.2. The Reuse Detector


    As explained earlier, several works have demonstrated that a notable percentage
    of the blocks inserted/updated in the SLLC are in fact useless, as they are dead-blocks
    (i.e. blocks which will not be referenced any more during their lifetime in the
    SLLC) [\[1,](#page-20-0) [2\]](#page-20-1). These useless blocks are harmful,
    as they evict other blocks which could potentially be useful in the future, and
    moreover, they increase the amount of writes to the SLLC, which in the context
    of NVMs (Non-Volatile Memories) is far from convenient, as explained in previous
    sections.


    In this paper we leverage the technique for reducing the amount of dead-blocks
    inserted/updated in the SLLC [\[7\]](#page-21-2) to improve the efficiency of
    a STT-RAM SLLC. In [\[7\]](#page-21-2), the authors present a proposal that, in
    an exclusive memory hierarchy, reduces the amount of blocks inserted in a conventional
    SLLC by around 90%. We apply this technique to a different context, i.e., to a
    non-inclusive STT-RAM SLLC design within a memory hierarchy where L1-L2 are inclusive.
    The exclusion policy employed in [\[7\]](#page-21-2) implies that, upon a SLLC
    hit, the block is copied to the private cache levels and removed from the SLLC.
    In our case, the block is inserted in the SLLC at the end of its usage in the
    private caches and remains in the SLLC until eviction. For our purpose, we include
    an intermediate element between each private L2 cache and the SLLC (Figure [4\)](#page-3-0).
    A block evicted from the private L2 caches is targeted to the corresponding element,
    which we denote as Reuse Detector (RD), instead


    of accessing directly to the SLLC as it would do in the baseline system. The RD
    decides whether to send the block to the SLLC or not (i.e. to bypass the shared
    last-level cache), by means of a prediction about the future usefulness of the
    block. We must highlight that, being the RD out of the block request path to the
    SLLC, it does not impact the SLLC hit or miss latencies.


    ![](_page_3_Figure_9.jpeg)


    <span id="page-3-0"></span>FIGURE 4. Placement of a Reuse Detector between each
    private L2 level and STT-RAM SLLC.


    For accomplishing the RD prediction, we apply Albericio''s concept of reuse locality
    [\[3,](#page-20-2) [4\]](#page-20-3). As such, if the evicted block from the L2
    has never been requested to the SLLC since the time it entered the cache hierarchy
    (i.e. it has never been reused at the SLLC), the block is predicted as a deadblock
    and thus it bypasses the SLLC, directly updating MM (if the block is dirty) or
    being just discarded (if it is clean). Otherwise, if the block has been reused
    (i.e. it has been requested to the SLLC at least once since the time it entered
    the cache hierarchy) and thus it is predicted as a non-deadblock, it is inserted/updated
    in the SLLC.


    The RD consists of a FIFO buffer and some management logic. The buffer stores
    the tag addresses of the blocks evicted by the private levels in order to maintain
    their reuse state. Moreover, an extra bit, called *reuse bit*, is added to each
    cache line in the private levels. This bit distinguishes if the block was inserted
    at the private cache levels from the SLLC or from another private cache level
    (*reuse* bit is 1), or main memory (*reuse* bit is 0). In the following sections,
    we analyze in detail the Reuse Detector operation and implementation.


    # *3.2.1. Reuse Detector operation*


    As we said in a previous section, our proposal aims to reduce the amount of writes
    to the STT-RAM SLLC and to improve the management of SLLC blocks, which translate
    into system performance improvement and energy consumption reduction on the system.


    Figure [5](#page-4-0) shows a flow diagram of a block request from a


    core to its associated private caches. If the request hits in L1 or L2 the reuse
    bit is untouched, and the block is copied in L1 if it was not there (inclusive
    policy). Otherwise, the request is forwarded to the SLLC. If the access hits in
    the SLLC, the block is provided to the core and copied in the private levels with
    the reuse bit set. If the access misses in the SLLC but the coherency mechanism
    informs that the block is present in another private cache, the block is provided
    by that cache. In this case, the access is recognized as a reuse, so the reuse
    bits are also set. Finally, if no copy of the block is present in the cache hierarchy,
    it is requested to MM and copied in L1-L2 with the reuse bit unset.


    ![](_page_4_Figure_2.jpeg)


    <span id="page-4-0"></span>FIGURE 5. Block request and reuse bit management.


    Figure [6](#page-4-1) shows a flow diagram of a block eviction from an L2 cache
    (if required, the corresponding L1 cache is invalidated). When a block is evicted
    from a last private level cache, its reuse bit is checked. If the reuse bit is
    set, it means that the block was inserted into the private caches either from
    the SLLC or from another private cache after checking the SLLC and the coherency
    mechanism. In any case, the block is considered as having been reused, and it
    should be inserted in the SLLC (if not present yet) or just updated (if the block
    is dirty but it is already present in the SLLC). Note that if the block is clean
    and already present in the SLLC, it can just be discarded. If the reuse bit is
    unset (i.e. the block was brought into the private caches directly from main memory)
    but the block''s tag is found in the RD buffer, the block is also considered as
    having been reused, and thus it is handled as in the previous situation. Finally,
    if the reuse bit is unset and its tag is not present in the RD buffer, it means
    that the block is considered as not having been reused yet. Based again on Albericio''s
    observations [\[3,](#page-20-2) [4\]](#page-20-3), the block should bypass the
    SLLC, as it is predicted as a dead-block, and it should be sent to MM (if the
    block is dirty) or just discarded (if it is clean). Note that in all cases the
    coherency mechanism must be updated.


    ![](_page_4_Figure_5.jpeg)


    <span id="page-4-1"></span>FIGURE 6. Block eviction from a private cache and SLLC
    insertion.


    # *3.2.2. Example*


    For the sake of clarifying the Reuse Detector operation, in this subsection we
    provide a straightforward example illustrating the flow of five memory blocks
    (A, B, C, D and E) through the different cache levels under a given access pattern.
    In this example, we consider a dual-core system (*Core*<sup>0</sup> and *Core*1)
    with private first level caches (*L*1<sup>0</sup> and *L*11), a shared second
    level cache (SLLC), and the corresponding Reuse Detectors between both cache levels.
    In the example we assume a simplified configuration where: 1) direct-mapped L1s,
    2-way set associative RDs and 4-way set associative SLLC are considered; 2) all
    memory blocks map to the same L1 frame and to the same RD and SLLC set; and 3)
    initially, all caches and RDs are empty. Next, we detail the access sequence of
    our example and show the contents of the memory hierarchy after each access in
    Figure [7.](#page-5-0) Note that we specify as a subindex the dirty bit followed
    by the reuse bit (*Xd*,*<sup>r</sup>* ) for each block *X* in the private cache
    levels, and only the dirty bit (*Xd*) for each block *X* in the SLLC.


    1. *Core*<sup>0</sup> requests a word within block A for reading: The access misses
    in *L*10, it is forwarded to the SLLC, and given that the access to SLLC also
    misses and the block is not present in any other private cache, it is forwarded
    to MM. According to Figure [5,](#page-4-0) block A is copied to *L*1<sup>0</sup>
    with its reuse bit unset, and the requested word is provided to *Core*0.


    ![](_page_5_Figure_2.jpeg)


    <span id="page-5-0"></span>FIGURE 7. Example of the Reuse Detector operation.


    - 2. *Core*<sup>1</sup> requests a word within block A for reading: The access
    misses in *L*1<sup>1</sup> and SLLC. However, the coherency mechanism informs
    that the block is at *L*10, so the request is forwarded to that cache. According
    to Figure [5,](#page-4-0) the block is copied to *L*1<sup>1</sup> and both reuse
    bits are set, as we recognize this access as an SLLC reuse.

    - 3. *Core*<sup>1</sup> requests a word within block B for reading: The access
    misses in *L*1<sup>1</sup> and SLLC, and the block is not present in any other
    private cache, so the request is forwarded to MM. According to Figure [5,](#page-4-0)
    block B is copied to *L*1<sup>1</sup> (replacing block A) with its reuse bit unset,
    and the requested word is provided to *Core*1. According to Figure [6,](#page-4-1)
    given that block A had its reuse bit set, it is inserted into the SLLC.

    - 4. *Core*<sup>1</sup> requests a word within block C for reading: Block C is
    inserted in *L*1<sup>1</sup> and block B is replaced. As the reuse bit of block
    B was unset and its tag was not in *RD*1, according to Figure [6](#page-4-1) the
    tag is stored in *RD*<sup>1</sup> and, given that the block is clean, it is not
    inserted in the SLLC but just discarded.

    - 5. *Core*<sup>1</sup> requests a word within block B for reading: This


    access is handled analogously to the previous access.


    - 6. *Core*<sup>1</sup> requests a word within block D for reading: Block D is
    inserted in *L*1<sup>1</sup> and block B is replaced. As the reuse bit of block
    B was unset but its tag was present in *RD*1, according to Figure [6](#page-4-1)
    block B is inserted in the SLLC.

    - 7. *Core*<sup>0</sup> writes to a word within block A: The access hits in *L*10.
    The dirty bit for the block is set.

    - 8. *Core*<sup>0</sup> requests a word within block E for reading: Block E is
    inserted in *L*1<sup>0</sup> and block A is replaced. As the dirty bit of block
    A is set and A is already present in the SLLC, the block is updated at this level.


    # <span id="page-5-1"></span>*3.2.3. Implementation details*


    Although a typical set-associative design could be used for the RD implementation,
    where a whole block tag, a validity bit and some information related with the
    replacement policy is included for each line, as in [\[7\]](#page-21-2) we use
    two techniques aimed at reducing the required space: sectoring and compression.
    A sector is a set of consecutive memory blocks aligned to the sector size. Storing
    sector tags in


    the RD allows to merge in a single line of the RD the information related with
    several blocks. Note that for each entry it is necessary to store a presence bit.
    For example, with a sector comprising 4-blocks, each line is made up of a tag
    derived from the sector address, a validity bit, some bits storing the replacement
    state and 4 presence bits.


    The compression of the tags is achieved based on the following process: let *t*
    be the amount of bits of the full tag and *c* the amount of bits of the compressed
    tag, being t>c. We first divide the full tag into several pieces, each of size
    *c* (the last piece is filled with *0s* if necessary). Then, we *xor* all the
    pieces, obtaining the compressed tag. Note that each compressed tag is shared
    among various sectors, thus false positives are possible where non-reused blocks
    are delivered to the SLLC. This situation does not cause a functional problem,
    but it may degrade system performance, so the value of *c* must be chosen carefully.


    As for the storage overhead of the RD implementation, i.e., the total amount of
    extra bits required compared to the baseline, we need the following hardware:
    The RD has 1024 sets and 16 ways (our simulations reveal that this value provides
    similar performance to that of higher associativity values), and a sector size
    of 2 blocks. Each RD entry requires 14 bits (10 for the compressed tag, 2 for
    the block presence, 1 for the replacement policy and 1 validity bit) as Figure
    [8](#page-6-1) illustrates. Given that the amount of entries in the RD is 8K,
    the total extra storage required per core is 14 KB, which represents a negligible
    1.3% of an 1MB last level cache.


    | CompressedTag | S1 S1   SpL   V |                                                     |  |

    |---------------|-----------------|-----------------------------------------------------|--|

    | Bits 13-4     |                 | Bit 3 Bit 2 Bit 2 Bit 1 - Bit 1 - Bit 1 -
    Bit 0 - / |  |


    <span id="page-6-1"></span>FIGURE 8. Reuse Detector entry.


    Finally, as RD replacement policy we use a 1-bit FIFO. Again based on our simulations,
    this scheme delivers a similar performance as other policies that would require
    more storage. In a FIFO policy, age information is updated only when a new address
    is inserted, and not during subsequent hits. This approach is fully consistent
    with the main RD buffer goal of detecting the first reuse of a block.


    # 3.3. Reuse Detector vs DASCA


    In this section we briefly discuss the main differences between the RD approach
    and the DASCA scheme, which will be further extended and analyzed in the evaluation
    section. As for the operation of both approaches, note that the DASCA mechanism
    tries to predict dead writes based on a PC-based predictor. For this purpose,
    the PC signature of each block that accesses the SLLC must be recorded. Conversely,
    the RD scheme tries to predict dead-blocks based on their reuse state. Our prediction
    is based on the addresses of the accessed data instead of the instruction addresses
    used in DASCA. Also, in our approach we directly store the mentioned addresses
    while in the DASCA scheme the authors employ a PC-signatures table which is trained
    by an auxiliary cache that works in parallel with the conventional cache.


    Focusing on the specific implementation of the DASCA scheme for the evaluation
    of this proposal, it is worthy to note that our approach employs a memory hierarchy
    where L1 and L2 are inclusive while the SLLC (L3) is non inclusive, whereas the
    original DASCA scheme is evaluated in [\[1\]](#page-20-0) employing a memory hierarchy
    with just two cache levels and assuming non-inclusive caches by default, although
    the authors also propose a bypassing scheme that supports both inclusive and non-inclusive
    caches. Therefore, and looking for a fair comparison between RD and DASCA, we
    implement DASCA using exactly the same three-level non-inclusive non-exclusive
    hierarchy employed in the RD approach. As a result of that, the only high-level
    change with respect to the original version of DASCA lies in that one of the three
    possible cases they use to classify the dead writes ("dead-value fills", blocks
    that receive a writeback request from lower-level caches right after the block
    is filled, but before any read operation, i.e., the filled block data are overwritten
    before being read), can not exist since they are removed by the inclusion mechanism
    we employ in our approach. Notably, this is due to the fact that, in our configuration,
    all the insertions in the SLLC are motivated by an L2 block eviction, and, as
    L1 and L2 are inclusive, these evicted blocks are only located in the SLLC after
    the eviction from L2. If after that the processor generates a write request on
    one of these blocks, a write hit occurs, and consequently the block is copied
    to the private cache levels and therefore this block in the SLLC can not be written
    again in the SLLC before being read. Hence, we consider this evaluation as fair,
    since this way we are evaluating DASCA under the same conditions as our approach,
    so that we are not giving an advantage to our RD by the fact that it directly
    avoids the "dead-value fills" with the inclusiveness management.


    # <span id="page-6-0"></span>4. EXPERIMENTAL FRAMEWORK


    For our experiments we use the *gem5* simulator [\[8\]](#page-21-3) and we employ
    the *ruby* memory model, specifically the MOESI\_CMP-directory coherence policy
    provided by the simulator. It is worth noting that we focus on a MOESI policy
    since protocols with owned state (e.g. MOESI and MOSI) are able to reduce the
    number of writes to the LLC, as demonstrated by Chang et.al. [\[9\]](#page-21-4).
    We modify the coherence protocol, encoding the proposed reuse detector. We simulate
    both a single and a multi-core scenario. For the sake of a better accuracy in
    both execution modes, an O3 processor type (detailed mode of simulation) was used.


    The main features of both the processor and the memory hierarchy are shown in
    Table [2.](#page-7-0) The network used is a crossbar modeled with Garnet [\[10\]](#page-21-5),
    a detailed interconnection network model inside gem5. As explained above, for
    the evaluation of our proposed RDs we implement them in the cache hierarchy modifying
    the coherence protocol. For modeling the DRAM main memory we use DRAMSIM2 [\[11\]](#page-21-6).
    We adapt the LLC read and 8 RODRIGUEZ-RODRIGUEZ ET AL


    | Architecture         | x86                                                                                         |

    |----------------------|---------------------------------------------------------------------------------------------|

    | CPUS                 | 1/4/8, 2GHz                                                                                 |

    | Pipeline             | 8 Fetch, 8 Decode, 8 Rename, 8 Issue/Execute/Writeback,
    8 Commit                            |

    | Registers            | Integer (256), Floating Point (256)                                                         |

    | Buffers              | Reorder Buffer (192), Instruction Queue (64)                                                |

    | Branch Predictor     | TournamentBP                                                                                |

    | Functional Units     | IntALU=6, IntMulDiv=2, FPALU=4, FPMultDiv=2, SIMD-Unit=4,
    RdWrPort=4, IprPort=1             |

    | Private Cache L1 D/I | 32 KB, 8 ways, LRU replacement, Block Size 64B, Access
    Latency 2 cycles, SRAM               |

    | Private Cache L2 D/I | 256 KB, 16 ways, LRU replacement, Block Size 64B, Access
    Latency 5 cycles, SRAM             |

    | Interconnection      | Crossbar network, modeled using Garnet, latency 3 cycles                                    |

    | Shared Cache L3      | 1 bank/1MB/core, 16 ways, LRU replacement, Block Size
    64B, R/W Latency 6/17 cycles, STT-RAM |

    | DRAM                 | 2 Ranks, 8 Banks, 4kB Page Size, DDR3 1066MHz                                               |

    | DRAM Bus             | 2 channels with a 8 bus of 8 bits                                                           |


    <span id="page-7-0"></span>TABLE 2. CPU and Memory Hierarchy specification.


    | High<br>Medium                    |                                    | Low                                |  |

    |-----------------------------------|------------------------------------|------------------------------------|--|

    | lbm, mcf, libquantum, bwaves,     | bzip2, soplex, gcc, wrf, astar,    | gromacs,
    calculix, h264ref, tonto, |  |

    | milc, cactusADM, zeusmp, leslie3d | hmmer, xalancbmk, gobmk, perlbench | omnetpp,
    namd, sphinx3, GemsFDTD   |  |


    <span id="page-7-1"></span>TABLE 3. Benchmark characterization according to the
    number of LLC writes per Kinstruction (WPKI).


    write latencies according to the STT-RAM target. Both latencies and energy consumption
    values are obtained from NVSim [\[6\]](#page-21-1) for a 1MB (1 bank) cache and
    are illustrated in Table [1.](#page-2-0) For scaling the LLC to larger sizes,
    we multiply the leakage power by the number of cores.


    Our experiments make use of the SPEC CPU2006 benchmark suite [\[12\]](#page-21-7).
    When we evaluate our proposal in a single core scenario (LLC 1MB size) we employ
    *reference* inputs and simulate 1 billion instructions from the checkpoint determined
    using PinPoints [\[13\]](#page-21-8). Note that results from 4 out of 29 benchmarks
    are not considered in the evaluation section due to experimental framework constraints.
    We also report results of 28 multiprogrammed mixes employing SPEC CPU2006 programs
    in 4, 8 and 16- CMP systems with 4, 8 and 16MB SLLC sizes, respectively. In all
    the cases, we fast forward 100M instructions, warm up caches for 200M instructions
    and then report results for at least 500M instructions per core.


    For selecting the aforementioned multiprogrammed mixes, we employ the following
    methodology: we execute each benchmark alone, using an LLC of 1MB and without
    any reuse detector, and we measure the amount of LLC writes that it generates.
    We then obtain for each benchmark the *number of writes to LLC per 1000 instructions*ratio


    (WPKI). Based on these values, we include each benchmark into the *high*, *medium*
    or *low* category. Specifically, the *high* category includes benchmarks with
    a WPKI higher than 8, the *medium* one those with a WPKI satisfying 1 <*WPKI*
    < 8 and finally, in the *low* category we include the programs with a WPKI lower
    than 1. Table [3](#page-7-1) shows this classification. Based on this classification,
    and as detailed in Section [5,](#page-9-0) we build some mixes made up of programs
    with high WPKI, some with medium WPKI, some with low WPKI, and some combining
    applications from different WPKI categories trying to fill most of the combinations
    high-medium, high-low, medium-low and high-medium-low. Tables [4,](#page-7-2)
    [5](#page-8-0) and [6](#page-8-1) show the built mixes for the 4-core, 8-core
    and 16-core CMP systems, respectively, where for each mix the applications are
    sorted first by decreasing WPKI categories (from the high category to the low
    one) and then within each of the categories they are also sorted alphabetically.
    We employ the symbol ";" to separate applications from different categories and
    also the parenthesis to indicate how many instances of an application (when employ
    more than one) we are using in each mix.


    Energy model: The DRAM energy is obtained directly from the simulator. For computing
    the LLC energy we employ a model that includes both dynamic and static


    | Mixes    | Applications                         | Mixes    | Applications                           |

    |----------|--------------------------------------|----------|----------------------------------------|

    | mix.H0   | cactusADM, leslie3d, libquantum, mcf | mix.H1   | bwaves, cactusADM,
    leslie3d, milc      |

    | mix.H2   | libquantum, mcf, milc, zeusmp        | mix.H3   | bwaves, cactusADM,
    lbm, leslie3d       |

    | mix.M0   | bzip2, gobmk, soplex, xalancbmk      | mix.M1   | gcc, perlbench,
    wrf, xalancbmk         |

    | mix.M2   | gcc, gobmk, hmmer, soplex            | mix.M3   | astar, gobmk, perlbench,
    wrf           |

    | mix.L0   | calculix, GemsFDTD, namd, sphinx3    | mix.L1   | gromacs, h264ref,
    omnetpp, tonto       |

    | mix.L2   | calculix, GemsFDTD, omnetpp, sphinx3 | mix.L3   | gromacs, h264ref,
    namd, tonto          |

    | mix.HM0  | milc, zeusmp; astar, gcc             | mix.HM1  | lbm, leslie3d,
    libquantum; gobmk       |

    | mix.HM2  | milc, zeusmp; gcc, gobmk             | mix.HM3  | bwaves, mcf; soplex,
    xalancbmk         |

    | mix.HL0  | bwaves, cactusADM, leslie3d; omnetpp | mix.HL1  | bwaves, lbm, libquantum;
    omnetpp       |

    | mix.HL2  | lbm, leslie3d; gromacs, namd         | mix.HL3  | leslie3d, milc;
    GemsFDTD, omnetpp      |

    | mix.ML0  | perlbench; gromacs, sphinx3, tonto   | mix.ML1  | hmmer, wrf; gromacs,h264ref            |

    | mix.ML2  | perlbench, wrf; GemsFDTD, namd       | mix.ML3  | soplex, xalancbmk;
    sphinx3, tonto      |

    | mix.HML0 | mcf; hmmer; h264ref, omnetpp         | mix.HML1 | milc; hmmer; GemsFDTD,
    h264ref         |

    | mix.HML2 | milc; bzip2, wrf; GemsFDTD           | mix.HML3 | bwaves, leslie3d;
    xalancbmk; GemsFDTD, |


    <span id="page-7-2"></span>TABLE 4. SPEC 2006 multiprogrammed mixes for the 4-core
    CMP.


    | Mixes    | Applications                                                             |

    |----------|--------------------------------------------------------------------------|

    | mix.H0   | bwaves, cactusADM, lbm, leslie3d, libquantum, mcf, milc, zeusmp          |

    | mix.H1   | bwaves, cactusADM(2), leslie3d(2), libquantum, mcf, milc                 |

    | mix.H2   | bwaves, cactusADM, leslie3d, libquantum, mcf, milc(2), zeusmp            |

    | mix.H3   | cactusADM, lbm, leslie3d, libquantum(2), milc, zeusmp(2)                 |

    | mix.M0   | astar, gcc, gobmk, hmmer, perlbench, soplex, wrf, xalancbmk              |

    | mix.M1   | bzip2, gcc, perlbench(2), soplex, wrf, xalancbmk(2),                     |

    | mix.M2   | gcc, gobmk(2), hmmer, perlbench, soplex(2), xalancbmk                    |

    | mix.M3   | astar, gobmk(2), hmmer, perlbench, soplex, wrf(2)                        |

    | mix.L0   | calculix, GemsFDTD, gromacs, h264ref, namd, omnetpp, sphinx3, tonto      |

    | mix.L1   | GemsFDTD, gromacs, h264ref(2), namd, omnetpp(2), tonto                   |

    | mix.L2   | gromacs(2), h264ref(2), namd, omnetpp, tonto(2)                          |

    | mix.L3   | calculix, GemsFDTD(2), gromacs, namd, omnetpp, tonto(2)                  |

    | mix.HM0  | cactusADM, leslie3d, libquantum, mcf; bzip2, gobmk, soplex, xalancbmk    |

    | mix.HM1  | lbm, mcf, milc, zeusmp; gcc, hmmer, perlbench, wrf                       |

    | mix.HM2  | bwaves, cactusADM, libquantum, milc; astar, gobmk, perlbench, soplex     |

    | mix.HM3  | cactusADM, lbm, leslie3d, zeusmp; gcc, gobmk, soplex, wrf                |

    | mix.HL0  | cactusADM, leslie3d, libquantum, mcf; calculix, GemsFDTD, namd, sphinx3  |

    | mix.HL1  | lbm, mcf, milc, zeusmp; gromacs, h264ref, omnetpp, tonto                 |

    | mix.HL2  | bwaves, lbm, libquantum, milc; GemsFDTD, h264ref, sphinx3, tonto         |

    | mix.HL3  | cactusADM, lbm, leslie3d, zeusmp; calculix, gromacs, namd, omnetpp       |

    | mix.ML0  | bzip2, gobmk, soplex, xalancbmk; calculix, GemsFDTD, namd, sphinx3       |

    | mix.ML1  | astar, hmmer, perlbench, wrf; h264ref, gromacs, omnetpp, tonto           |

    | mix.ML2  | gcc, gobmk, perlbench, xalancbmk; GemsFDTD, gromacs, omnetpp, sphinx3    |

    | mix.ML3  | bzip2, hmmer, soplex, wrf; calculix, h264ref, namd, tonto                |

    | mix.HML0 | bwaves, cactusADM, leslie3d; gobmk, soplex, xalancbmk; GemsFDTD,
    sphinx3 |

    | mix.HML1 | lbm, libquantum, mcf; astar, perlbench; calculix, h264ref, namd          |

    | mix.HML2 | cactusADM, milc, zeusmp; hmmer, wrf, xalancbmk; h264ref, tonto           |

    | mix.HML3 | leslie3d, mcf, milc; perlbench, soplex; gromacs, namd, omnetpp           |


    <span id="page-8-0"></span>TABLE 5. SPEC 2006 multiprogrammed mixes for the 8-core
    CMP.


    | Mixes    | Applications                                                                                                                              |

    |----------|-------------------------------------------------------------------------------------------------------------------------------------------|

    | mix.H0   | bwaves(2), cactusADM(3), lbm, leslie3d(3), libquantum(2), mcf(2),
    milc(2), zeusmp                                                         |

    | mix.H1   | bwaves(2), cactusADM, lbm(2), libquantum(2), mcf(3), milc(4), zeusmp(2)                                                                   |

    | mix.H2   | bwaves, cactusADM, lbm(3), leslie3d(4), libquantum(3), mcf(2), milc,
    zeusmp                                                               |

    | mix.H3   | bwaves(3), cactusADM(2), lbm(2), leslie3d(2), libquantum, mcf, milc(2),
    zeusmp(3)                                                         |

    | mix.HM0  | bwaves, cactusADM, lbm, leslie3d, libquantum, mcf, milc, zeusmp;
    astar, gcc, gobmk, hmmer, perlbench, soplex, wrf, xalancbmk              |

    | mix.HM1  | bwaves, cactusADM(2), leslie3d(2), libquantum, mcf, milc; bzip2,
    gcc, perlbench(2) soplex, wrf, xalancbmk(2)                              |

    | mix.HM2  | bwaves, cactusADM, libquantum, leslie3d, mcf, milc(2), zeusmp; gcc,
    gobmk(2), hmmer, perlbench, soplex(2), xalancbmk                      |

    | mix.HM3  | bwaves, cactusADM, lbm, leslie3d, libquantum, mcf, milc, zeusmp;
    astar, gobmk(2), hmmer, perlbench, soplex, wrf(2)                        |

    | mix.M0   | astar(2), bzip2(2), gcc, gobmk(3), hmmer(2), perlbench(2), soplex,
    wrf(2), xalancbmk                                                      |

    | mix.M1   | astar, bzip2(2), gcc, gobmk, hmmer, perlbench, soplex(2), wrf(3),
    xalancbmk(4)                                                            |

    | mix.M2   | astar, bzip2, gcc, gobmk(3), hmmer(3), perlbench(2), soplex(4), xalancbmk                                                                 |

    | mix.M3   | astar(2), bzip2, gcc(3), perlbench, soplex(2), wrf(4), xalancbmk(3)                                                                       |

    | mix.L0   | calculix(2), GemsFDTD(2), gromacs(2), h264ref(2), namd(2), omnetpp(2),
    sphinx3(2), tonto(2)                                               |

    | mix.L1   | calculix, GemsFDTD(3), gromacs(3), h264ref, namd(2), omnetpp(2),
    sphinx3(3), tonto                                                        |

    | mix.L2   | calculix(2), GemsFDTD(3), gromacs(3), h264ref(3), namd(3), tonto(2)                                                                       |

    | mix.L3   | calculix(4), gromacs(2), h264ref(2), omnetpp(4), sphinx3(2), tonto(2)                                                                     |

    | mix.HL0  | cactusADM, lbm, leslie3d, libquantum, mcf(2), milc, zeusmp; calculix,
    GemsFDTD, gromacs, h264ref, namd, omnetpp, sphinx3, tonto           |

    | mix.HL1  | bwaves, lbm(2), libquantum, mcf, milc(2), zeusmp; GemsFDTD, gromacs,
    h264ref(2), omnetpp, sphinx3, tonto(2)                               |

    | mix.HL2  | cactusADM, lbm(2), leslie3d, mcf, milc, zeusmp(2); calculix, gromacs(2),
    h264ref, namd, omnetpp(2), tonto                                 |

    | mix.HL3  | bwaves, cactusADM, lbm, leslie3d, libquantum(2), mcf, milc; calculix,
    GemsFDTD(2), h264ref, namd, sphinx3(2), tonto                       |

    | mix.ML0  | astar, bzip2, gobmk, hmmer, perlbench, soplex, wrf, xalancbmk; calculix,
    GemsFDTD, gromacs, h264ref, namd, omnetpp, sphinx3, tonto        |

    | mix.ML1  | astar(2), gcc, perlbench(2), soplex(2), xalancbmk(2); GemsFDTD(3),
    h264ref(2), tonto(2)                                                   |

    | mix.ML2  | astar, bzip2, hmmer(2), perlbench, soplex, wrf(2); calculix, gromacs,
    h264ref(2), namd, omnetpp, tonto(2)                                 |

    | mix.ML3  | bzip2, gcc(2), gobmk(2), perlbench, soplex, xalancbmk(2); calculix,
    GemsFDTD, gromacs, namd, omnetpp, sphinx3(2)                          |

    | mix.HML0 | bwaves, cactusADM, lbm, leslie3d, libquantum, mcf; astar, gobmk,
    perlbench, soplex, xalancbmk; calculix, GemsFDTD, h264ref, namd, sphinx3 |

    | mix.HML1 | cactusADM, leslie3d, mcf, milc(2), zeusmp; hmmer, perlbench, soplex,
    wrf, xalancbmk; gromacs, h264ref, namd, omnetpp, tonto               |

    | mix.HML2 | lbm, leslie3d, libquantum, mcf(2), milc; astar, perlbench(2), soplex;
    calculix, gromacs, h264ref, namd(2), omnetpp                        |

    | mix.HML3 | bwaves, cactusADM(2), leslie3d, milc, zeusmp; gobmk, hmmer, soplex,
    wrf, xalancbmk(2); GemsFDTD, h264ref, sphinx3, tonto                  |


    <span id="page-8-1"></span>TABLE 6. SPEC 2006 multiprogrammed mixes for the 16-core
    CMP.


    contributions. The static component is calculated using NVSim [\[6\]](#page-21-1),
    which reports the leakage number for 1MB LLC. Thus, we multiply that number by
    the execution time and the number of cores to obtain the total static energy.
    In the case of the dynamic component, we again use NVSim for determining the dynamic
    energy consumption per access to the LLC. Then, we compute the dynamic energy
    consumption as follows:


    $$\begin{aligned} \text{Dynamic Energy} &= H\_{LLC} \ast H E\_{LLC} + W\_{LLC}
    \ast W E\_{LLC} + \\ &M\_{LLC} \ast M E\_{LLC} \end{aligned} \tag{1}$$


    where H*LLC*, W*LLC* and M*LLC* denote the number of hits, writes and misses in
    the LLC respectively, and HE*LLC*, WE*LLC* and ME*LLC* correspond to the energy
    consumption of a hit, a write and a miss in the LLC respectively.


    # <span id="page-9-0"></span>5. EVALUATION


    This section compares how well RD and DASCA behave when managing an STT-RAM LLC,
    both in terms of performance and energy consumption of LLC and main memory. Single,
    four and eight-core systems are discussed in Sections [5.1,](#page-9-1) [5.2,](#page-11-0)
    and [5.3,](#page-15-0) respectively. Finally, in Section [5.4,](#page-17-0) we
    show the results derived from the evaluation in a 16-core CMP and we analyze the
    performance of both approaches as the number of cores augments.


    # <span id="page-9-1"></span>5.1. Evaluation in a single-core scenario


    First, we show the number of writes to the LLC that each evaluated proposal involves
    as well as the performance delivered. Then, we focus on the involved energy consumption
    in both the STT-RAM and the main memory according to the model detailed in Section
    [4.](#page-6-0) Finally, we discuss the obtained results. All the graphs shown
    in this section report individual data for each benchmark, adding at the right
    end the arithmetic mean considering all data (labeled as *AVG*) or the geometric
    mean (labeled as *GMEAN*) in the case of the performance metric, and also the
    arithmetic mean of the eight most write-intensive benchmarks according to Table
    [3](#page-7-1) or the corresponding geometric mean for the performance metric
    (labeled as *HIGH* in both cases).


    # *5.1.1. Write filtering*


    Figure [9](#page-10-0) illustrates the number of writes to the STT-RAM LLC generated
    by the DASCA scheme and our proposal (using a RD of 8K entries) normalized to
    a baseline system without any write reduction/filtering scheme.


    As shown, our proposal significantly outperforms DASCA. Notably, in 20 out of
    25 benchmarks evaluated the Reuse Detector exhibits higher ability in cutting
    the write traffic to the STT-RAM LLC. Overall, the block bypassing decisions commanded
    by RD reduce the number of LLC writes in the baseline system around 65% whereas
    DASCA just achieves a 52% reduction. In addition, if we zoom just in the 8 programs
    with highest WPKI numbers (those labeled as *high* in Table [3\)](#page-7-1),
    RD reduces the number of LLC writes by around 80% with respect to the baseline,
    while DASCA cuts the write traffic by 66%.


    # *5.1.2. Performance*


    Apart from the goal of decreasing the STT-RAM LLC energy consumption (quantified
    later in this section), it is clear that energy efficiency should not come at
    the expense of a performance drop. Thus, to further evaluate the benefits of RD,
    Figure [10](#page-10-1) shows the performance (IPC) delivered.


    Overall our scheme performs moderately better than DASCA: RD delivers 1.9% performance
    improvement compared to the baseline while DASCA just improves IPC by 0.3%. If
    we focus on the write-intensive applications RD clearly outperforms DASCA, achieving
    performance improvements of 5% and 1.4%, respectively. This reveals, as we will
    confirm later in the multi-core environment, that


    our approach works especially well for those applications for which the number
    of writes performed to the LLC is high.


    # *5.1.3. Energy savings*


    Figure [11](#page-10-2) shows the total energy savings (adding both the dynamic
    and the static components) in the LLC. Overall, our proposal reports 34.5% energy
    reduction compared to the baseline while DASCA reports 29.5%. Considering only
    the write-intensive programs, the numbers are 60% and 49%, respectively. If we
    split the total energy savings with respect to the baseline into the dynamic and
    static parts, our proposal achieves 50% of reduction in the dynamic part considering
    all the applications (68% for the *high* programs), while DASCA obtains 42% (57%
    for the *high* benchmarks). As for the static part RD is able to obtain 2% energy
    savings (around 5% for the *high programs*) while DASCA just achieves 0.3% (1.4%
    for the write-intensive applications). Note that avoiding LLC writes reduces dynamic
    energy, whereas increasing performance translates into static energy savings.
    It is also worth noting that, as Figure [12](#page-11-1) illustrates, the dynamic
    energy consumption in the LLC of the baseline system is, for most of the applications
    evaluated, significantly higher than the static contribution.


    Finally, we have also explored the impact on the energy consumption in the DRAM
    main memory. For the sake of simplicity, we do not show the results for all the
    applications. However, as expected, the DRAM energy reduction follows the trend
    of performance improvement. Overall, our proposal manages to reduce the DRAM energy
    consumption by 2% (4.7% for the write-intensive programs) with respect to the
    baseline while DASCA just improves the memory energy consumption by 0.2% (1.1%
    for the *high* applications).


    # *5.1.4. Discussion*


    If we zoom into specific benchmarks, there are some special cases that deserve
    further detail to get a deeper insight. Note that globally, the relative trend
    shown in the amount of writes to the LLC between our approach and DASCA for each
    benchmark, is mainly held in the energy consumption differences, although modulated
    with the relative performance numbers. However, there are some few exceptions
    such as *namd*, *GemsFDTD* or *omnetpp*, where RD is able to reduce the amount
    of LLC writes significantly more than DASCA but the energy savings and also the
    performance improvements obtained by both techniques are almost the same (and
    quite low compared to the baseline) in all these three cases. The reason is that
    these programs are three of the four benchmarks that exhibit the lowest values
    of WPKI, so although the write reductions that RD achieves in relative terms compared
    to DASCA is significant for these applications, the corresponding reduction in
    absolute values are very modest, and therefore the impact on the energy is almost
    negligible.


    Also, in other applications such as *mcf*, *cactusADM* or *hmmer*, our approach
    is able to report IPC numbers significantly higher than in DASCA, while both techniques


    ![](_page_10_Figure_0.jpeg)


    FIGURE 9. Number of writes to the STT-RAM LLC normalized to the baseline: SPEC
    CPU2006 suite.


    <span id="page-10-0"></span>![](_page_10_Figure_2.jpeg)


    <span id="page-10-1"></span>FIGURE 10. Performance (Instructions per Cycle) normalized
    to the baseline: SPEC CPU2006 suite.


    exhibit quite similar write reduction capabilities. In order to explain that,
    first note that there are many different aspects involved in the system performance
    delivered. Among others, one key aspect is that reducing the amount of writes
    to the LLC is not sufficient in itself to guarantee performance improvements:
    although the main goals when bypassing blocks from the LLC to main memory are
    both to save energy and improve performance by increasing the hit rate in the
    LLC, obviously the bypassing may fail in the sense that a bypassed block could
    be referenced again soon, leading to a LLC miss and even a performance drop with
    respect to the case where bypassing is not carried out. Thus, for all these


    three benchmarks, the experimental data reveal that with our proposal the amount
    of hits in the LLC clearly outperforms both the baseline and the DASCA mechanism.
    Notably, the amount of LLC hits experienced in the *cactusADM* and *mcf* programs
    are 7.23x and 2x the values obtained in the baseline, while DASCA obtains 1.89x
    and 0.89x, respectively. Also, the amount of misses in the LLC is lower than that
    of the baseline and DASCA, with values ranging between 0.77-0.87x those obtained
    in the baseline. Considering all the evaluated benchmarks, RD is able to improve
    the amount of hits around 31% with respect to the baseline (106% if we only consider
    the write-intensive


    ![](_page_10_Figure_6.jpeg)


    <span id="page-10-2"></span>FIGURE 11. Energy consumption in the STT-RAM LLC normalized
    to the baseline: SPEC CPU2006 suite.


    ![](_page_11_Figure_1.jpeg)


    <span id="page-11-1"></span>FIGURE 12. Breakdown of energy consumption in the
    LLC into the static and dynamic contributions for the baseline in the singlecore
    system.


    applications) while DASCA experiments only 5% increment when considering all the
    benchmarks and 31% for the *high* applications.


    At a first glance, the behavior of the *libquantum* application may seem somehow
    strange: Neither RD nor DASCA are able to significantly reduce the amount of writes
    to the LLC, but however this benchmark running under RD reports a performance
    improvement of 7% with respect to the baseline while the performance remains largely
    unchanged under DASCA. In addition, and as one would expect since the number of
    bypasses is low, the number of hits in the LLC is practically the same in the
    three cases. The reason to explain the performance improvement lies in the LLC
    bank contention due to the write activity: this application is by far the most
    stalled one due to write contention. Thus, although the write reduction is very
    limited with our scheme, it is enough to reduce stalls with respect to the baseline
    by around 8%, which in absolute numbers implies various millions of these kind
    of situations avoided, which leads to the performance improvement obtained.


    Conversely, although other benchmarks such as *gromacs*, *calculix* or *wrf* exhibit
    moderate LLC writes reduction with RD and DASCA, they all perform worse than in
    the baseline. For these three programs the amount of hits experienced in the LLC
    is, in RD and DASCA, lower than in the baseline, which suggests that the bypassing
    performed is not efficient for these benchmarks. Recall that the energy savings
    achieved in the LLC as a consequence of the reduction in the number of writes
    performed in this cache level may be partially offset with the performance drop
    derived from the increment in the amount of LLC misses, as in these three programs
    occurs. Note also that, although the write operations are outside the critical
    path, the performance improvement derived from avoiding the long write operations
    may be mitigated if bank contention exists between the writes effectively performed.


    # <span id="page-11-0"></span>5.2. Evaluation in a 4-core CMP system


    In this section we extend the previous single-core analysis to a more up-to-date
    environment: a multi-core scenario where the LLC is shared among different cores.
    For this purpose, we measure again the number of writes to the SLLC, the performance
    and the energy consumption in both the STT-RAM SLLC and the DRAM main memory for
    RD and DASCA and report results normalized to the baseline. However, due to the
    inherent non-determinism that all simulators exhibit (especially in multi-core
    environments, where the number of instructions executed across different schemes
    are not stable owing to the random interleaving among memory accesses of different
    programs) and for the sake of higher accuracy, we employ in this scenario, as
    well as in the 8 and 16-core CMP systems, the arithmetic mean of the number of
    writes and energy consumption (per application) but *divided by the total number
    of instructions executed*. Note that, conversely, in the single-core scenario
    both kind of metrics match, since all the benchmarks execute the same amount of
    instructions (1B) in all the runs.


    We employ 28 mixes made up of applications from the SPEC CPU2006 suite chosen
    accordingly to the WPKI categories illustrated in Table [3.](#page-7-1) First,
    we randomly compose three groups of 4 mixes made up of applications belonging
    to just one WPKI category (mixes referred to as H*i*, M*i* and L*i* for high,
    medium and low WPKI respectively). Then, we build other 16 mixes merging applications
    with WPKI corresponding to different categories and trying to construct them in
    a balanced and homogeneous fashion. Again, the workload name encodes the WPKI
    categories of the applications. For example, HL2 is the third mix we build consisting
    of applications with high WPKI (2 in this case) and applications with low WPKI
    (other two). The detailed mixes are illustrated in Table [4.](#page-7-2) Most
    graphs in this section report individual results for each mix, the arithmetic
    mean (AVG) considering all the mixes, just the 4 H*i* mixes (HIGH), the 4 H*i*
    and the 4 HM*i* mixes together (H+HM), the 4 H*i*, the 4 HM*i* and the 4 HML*i*
    mixes together (H+HM+HML) and all the mixes including at least a high program
    (SomeH). Again, in the case of the performance metric we employ the geometric
    mean instead of the arithmetic one.


    # *5.2.1. Write filtering*


    Figure [13](#page-12-0) illustrates the number of writes to the STT-RAM SLLC generated
    by using DASCA and an 8K-entry RD per core normalized to a baseline STT-RAM without
    any write reduction mechanism.


    The experimental results reveal that RD exhibits a significantly greater ability
    to decrease the amount of writes to the SLLC than DASCA. Notably, in 25 out of
    the 28 mixes evaluated RD outperforms DASCA. Overall, the number of writes in
    the baseline system gets reduced to 37% by using RD, in contrast with DASCA which
    only achieves a 48%. As for the write-intensive mixes the RD and DASCA makes around
    40% and 46% of the writes the baseline performs, respectively.


    ![](_page_12_Figure_0.jpeg)


    <span id="page-12-0"></span>FIGURE 13. Number of writes to the STT-RAM SLLC normalized
    to the baseline in the 4-core CMP system.


    # <span id="page-12-1"></span>*5.2.2. Performance*


    In order to evaluate performance when executing multiprogrammed workloads, we
    analyze the *Instruction Throughput* (IT) and the *Weighted Speedup* (WS) metrics.
    The IT metric is defined as the sum of all instructions committed per cycle in
    the entire chip (∑ *n i*=1 *IPC<sup>i</sup>* , being *n* the number of threads),
    while the WS is defined as the slowdown experienced by each application in a mix,
    compared to its run under the same configuration when no other application is
    running on other cores (∑ *n i*=1 (*IPCshared i* /*IPCalone i* )). For the sake
    of simplicity and since in our context the WS does not constitute a metric as
    significant as IT, we do not show the WS results obtained. Anyway, these results
    follow an analogous trend to those obtained when we evaluate the instruction throughput.
    Figure [14](#page-13-0) illustrates the IT that each evaluated policy delivers
    normalized to the baseline.


    As shown, RD moderately outperforms DASCA. This is a key contribution of RD, since
    our approach, managing to reduce the amount of writes to the SLLC to a greater
    extent than DASCA, is also able to deliver higher performance (which also allows
    to report higher energy savings in both the SLLC and the main memory as shown
    later). The data reveal that, overall, RD improves performance by around 3% compared
    to the baseline, while DASCA just improves it by around 1.2%. Moreover, we can
    observe that, in almost all of the 28 mixes evaluated (except mainly those mixes
    made up of benchmarks with a reduced WPKI, those labeled as *low*, where the performance
    of both techniques essentially matches that of the baseline), our technique performs
    better. Zooming into particular mixes, the results reveal that RD performs especially
    better than DASCA in those mixes made up of write-intensive applications. Thus,
    our approach reports a performance improvement of more than 7% when considering
    just the H*<sup>i</sup>* mixes while DASCA just reports 1.7% IT improvement with
    respect to the baseline. Also, RD delivers significantly higher performance than
    DASCA and the baseline for those mixes which contain any application with high
    WPKI.


    # *5.2.3. Energy savings*


    Figure [15](#page-13-1) illustrates the energy savings in the SLLC. As in the
    single-core scenario, the graph follows a similar relative trend between our approach
    and DASCA to that observed in the write reduction numbers (Figure [13\)](#page-12-0),
    just slightly modulated with the performance numbers since, as shown in Figure
    [16,](#page-13-2) the dynamic contribution to the energy consumption in the SLLC
    is higher than the static part (except in the mixes made up of applications with
    low WPKI only), so that the ability to reduce the amount of writes to the SLLC
    (dynamic contribution) impacts the total energy consumption more than the ability
    to improve performance, which mainly affects the static contribution. Overall,
    our proposal reports around 37% energy reduction in the STT-RAM SLLC compared
    to the baseline while DASCA reduces it by around 31%. If we zoom into the write-intensive
    mixes, both RD and DASCA are able to save around 45% and 39% of SLLC energy consumption,
    respectively. If we break the SLLC energy numbers down into the static and dynamic
    contributions, our results reveal that, overall, RD is able to reduce –considering
    all mixes– the static energy part by around 2.7% with respect to the baseline
    (around 6% for the write-intensive mixes) while DASCA reduces the static contribution
    by 1.2% (1.7% for the *high* mixes). In addition, our approach reports dynamic
    energy savings of around 50% (51% for the *high* mixes) while DASCA numbers are
    42% (46% for the *high* mixes).


    Also, we explore the energy savings obtained in the DRAM main memory, where the
    leakage contribution has far greater significance than in the STT-RAM SLLC, so
    that the trends obtained essentially follow those of the IT graph, but inverted
    (higher performance translates into lower DRAM energy consumption). Figure [17](#page-14-0)
    illustrates that RD manages to additionally reduce the energy consumption of the
    main memory by around 6.2% on average compared to the baseline (8.3% for the write-intensive
    mixes), while DASCA barely reaches a 3.6% energy reduction (around 2% for the
    *high* mixes), mainly due to the higher performance improvement that our proposal
    exhibits.


    # *5.2.4. Discussion*


    For the sake of clarity, we next explain where the performance improvements of
    our technique come from. First, as Figure [13](#page-12-0) illustrated earlier,
    the write reductions to the SLLC that RD achieves are greater than those


    ![](_page_13_Figure_1.jpeg)


    <span id="page-13-0"></span>FIGURE 14. Instruction throughput normalized to the
    baseline in the 4-core CMP system.


    ![](_page_13_Figure_3.jpeg)


    <span id="page-13-1"></span>FIGURE 15. Energy consumption in the STT-RAM SLLC
    normalized to the baseline in the 4-core CMP system.


    ![](_page_13_Figure_5.jpeg)


    <span id="page-13-2"></span>FIGURE 16. Breakdown of energy consumption in the
    SLLC into the static and dynamic contributions for the baseline in the 4-core
    CMP system.


    of DASCA. Second, and more importantly, as Figure [14](#page-13-0) reveals, the
    bypasses dictated by RD translate into more performance than that of DASCA. As
    in the single-core scenario, the rationale behind that is related with the hit
    rate experimented in the SLLC with both schemes. Figure [18](#page-14-1) illustrates
    the number of hits in the SLLC per kilo instruction that each mix experiments
    normalized to the baseline.


    The results reveal that in most of the mixes evaluated the amount of hits in the
    SLLC is higher under our approach than using DASCA. Again, this is especially
    evident for the case of the mixes including write-intensive applications such
    as H1, H3 and HL2 where the number of hits is 2.87x, 2.45x and 1.37x those of
    the baseline, respectively. This is the key to explain our performance improvements:
    the efficient management of the SLLC contents by exploiting the reuse locality.
    In addition, there are other factors that also contribute to the throughput gain
    such as less write operations to the SLLC, less main memory accesses, and increased
    row buffer hit rates. In order to perform a deeper comparison between RD and DASCA,
    Table [7](#page-14-2) recaps the average values of different metrics involved
    in the performance delivered by RD and DASCA, normalized to those of the baseline.
    As shown, our scheme improves DASCA and the baseline (especially in the data from
    writeintensive mixes) in all the metrics considered.


    As in the single-core scenario, next we zoom into particular mixes that need further
    detail to get a better understanding. First, in some mixes such as H0, HM3 or
    HML0 we can observe that the DASCA scheme is able to reduce the amount of writes
    to the SLLC and also the energy consumption in the STT-RAM more than our scheme
    does (Figures [13](#page-12-0) and [15\)](#page-13-1). Conversely, the RD manages
    to deliver more throughput than DASCA (Figure [14\)](#page-13-0). However, these
    performance improvements our approach achieves are not enough to offset the higher
    energy savings in the SLLC that the DASCA scheme reports for these mixes as a
    consequence of the lower number of writes to the STT-


    ![](_page_14_Figure_0.jpeg)


    FIGURE 17. Energy consumption in the DRAM normalized to the baseline in the 4-core
    CMP system.


    <span id="page-14-0"></span>![](_page_14_Figure_2.jpeg)


    <span id="page-14-1"></span>FIGURE 18. Number of STT-RAM SLLC hits per kilo instruction
    normalized to the baseline in the 4-core CMP system.


    # RAM.


    Second, data for mix L2 reveal that the RD is able to reduce the amount of writes
    to the SLLC much more than DASCA with respect to the baseline (81% vs. 48%). However,
    this great difference translates into just 22% of energy savings in RD vs. 13%
    of DASCA. As shown, the difference between both policies has been significantly
    reduced due to the low contribution of the dynamic energy to the total energy
    consumption in the SLLC that this mix exhibits, as Figure [16](#page-13-2) illustrates.


    # *5.2.5. Sensitivity to Reuse Detector size*


    The RD size is a key design aspect of our proposal. In order to evaluate its impact
    we show in Figure [19](#page-15-1) the amount of writes to the SLLC, the Instruction
    Throughput, and the energy consumption in both the SLLC and the main memory for
    different RD sizes per core, namely 8K, 16K, 32K and 64K entries.


    As shown, the major impact is observed on the capability to reduce the number
    of writes in the SLLC, ranging from an average reduction of 63% with respect to
    the baseline when an 8K-entry RD per core is employed (60% for the write-intensive
    mixes) to a reduction of around 51% for a 64K-entry RD per core (50% for the *high
    mixes*). Note that maybe these data might appear contradictory at first sight.
    However, they are not: As the size of RD increases, it also augments the probability
    that a block finds its tag in the RD, so the probability of bypassing decreases,
    leading to minor reduction of writes to the SLLC. We can also observe a moderate
    impact on the average energy consumed in the SLLC, with values in the range 63-69%
    as the size of RD gets increased: again, note that these numbers follow a similar
    trend to that exhibited by the amount of writes. Finally, the impact over the
    performance and the energy consumption of the main memory is much reduced, falling
    the average IT variations into a small range of 1% (4% for the write-intensive
    mixes) and the average DRAM energy variations into a range of 1.5% (5% for the
    write-intensive mixes).


    | ❳<br>❳❳❳❳<br>Metrics | SLLC      | SLLC      | Row buffer Read | DRAM      |
    DRAM      | Bank contention in |

    |----------------------|-----------|-----------|-----------------|-----------|-----------|--------------------|

    | ❳❳❳❳❳<br>Policies    | Misses    | Hits      | Hit Rate        | reads     |
    Writes    | SLLC               |

    | DASCA (All/High)     | 1.01/1.05 | 1.04/1.10 | 1.03/1.00       | 1.01/1.05 |
    1.04/1.06 | 0.45/0.16          |

    | RD (All/High)        | 0.94/0.95 | 1.20/1.83 | 1.05/1.01       | 0.98/0.98 |
    0.96/0.94 | 0.29/0.08          |


    <span id="page-14-2"></span>TABLE 7. Average values of different metrics normalized
    to the baseline in the 4-core CMP system.


    # 16 RODRIGUEZ-RODRIGUEZ ET AL


    ![](_page_15_Figure_2.jpeg)


    <span id="page-15-1"></span>FIGURE 19. Writes to SLLC, IT and energy consumption
    in both SLLC and main memory normalized to the baseline for different RD sizes
    per core in the 4-core CMP system.


    # *5.2.6. Overhead analysis*


    In Section [3.2.3](#page-5-1) we outlined that an 8K-entry RD for a 1MB LLC requires
    an extra storage of 14KB, which represents a 1.37% overhead with respect to the
    LLC size. In this section we previously noted that for the 4-CMP system under
    evaluation (4MB SLLC) we employ an 8K-entry RD per core. The reason is that we
    are maintaining for each evaluated system the 1.37% overhead with respect the
    SLLC size. Therefore, in the 8-CMP evaluated later, we also employ an 8K-entry
    RD per core. Hence, the total extra storage (overhead) of RD is 56KB and 112KB
    for the 4- CMP and 8-CMP systems respectively, representing in all the cases a
    1.37% overhead with respect to the SLLC size.


    # *5.2.7. RD in a two-level cache hierarchy*


    We have evaluated the operation of our proposal in a threelevel cache hierarchy
    since most current processors employ this configuration. Furthermore, two private
    levels are more likely to filter better the temporal locality than using just
    one private level. However, for a fair comparison, we have also evaluated our
    proposal and the DASCA scheme in a configuration with just two cache levels. Notably,
    we reproduce the same configuration (4-CMP) used by the authors in [\[1\]](#page-20-0)
    when presenting the DASCA technique (32 KB IL1 and DL1 as private caches and a
    1MB per core shared L2 cache). Table [8](#page-16-0) illustrates the main results.


    As shown, RD maintains higher capability than DASCA (around 11% higher) in reducing
    the amount of writes to the SLLC. However, as expected, the amount of writes avoided
    (and also the hits experienced in the SLLC) is significantly lower than that exhibited
    in an scenario with 3 cache levels. Recall that this is due to the fact that with
    two cache levels only, most temporal locality has not been filtered, so that the
    reuse locality can not be fully exploitable. Also, as a consequence of this lower
    capability in cutting the write traffic to the SLLC, the energy savings achieved
    in the shared L2 are significantly lower than those obtained with three cache
    levels, although RD still reports better numbers than DASCA. Finally, RD again
    improves the Instruction Throughput to a greater extent than DASCA, and consequently
    also delivers higher energy savings in the main memory. Note that we have also
    evaluated 28 mixes in this configuration following the same criteria explained
    earlier, but they are not exactly the same as in the threelevel cache hierarchy
    experiments since the WPKI values that the benchmarks exhibit do not match those
    of the threelevel configuration and therefore some programs changed the category
    (high, medium or low) in which they were classified.


    # <span id="page-15-0"></span>5.3. Evaluation in an 8-core CMP system


    In this section we illustrate and analyze the main results obtained when using
    RD and DASCA in an 8-core CMP system with an 8MB SLLC. Like in the previous section,
    in this scenario we create 28 mixes following the same criteria as in a 4-CMP
    system. The mixes evaluated are shown in Table [5.](#page-8-0) Given that a detailed
    analysis of the 8-core system would show similar results as the 4-core scenario,
    in this section we will not zoom into details but will only describe the average
    results and main trends.


    # *5.3.1. Write filtering*


    Figure [20](#page-16-1) illustrates the number of writes to the STT-RAM SLLC generated
    with DASCA and with RD (assuming an 8K-entry RD per core). Both schemes are normalized
    to a baseline STT-RAM without any content selection mechanism.


    Similarly to the results for the 4-core scenario, the experimental results reveal
    that RD just performs 46% of the writes made in the baseline scheme, whereas DASCA
    produces 56% of the writes that the baseline did. For the write-intensive mixes,
    RD and DASCA reduce the amount of writes compared to the baseline in 44% and 35%
    respectively.


    # *5.3.2. Performance*


    As we did in Section [5.2.2,](#page-12-1) we employ the *Instruction Throughput*
    (IT) to evaluate the performance when executing multiprogrammed workloads. Figure
    [21](#page-16-2) illustrates the IT that each evaluated policy delivers normalized
    to the baseline.


    Similarly to the results obtained for a 4-core CMP system, RD outperforms DASCA
    in the 8-core scenario. Moreover, in the 8-core scenario, higher performance improvements
    are achieved in both schemes over the baseline. The


    | ❳<br>❳❳❳❳<br>Metrics  | Writes    | Instr.     | Energy consumpt. | Energy consumpt.
    | SLLC      |

    |-----------------------|-----------|------------|------------------|------------------|-----------|

    | ❳❳❳❳<br>Policies<br>❳ | SLLC      | Throughput | SLLC             | DRAM             |
    Hits      |

    | DASCA (All/High)      | 0.70/0.79 | 1.01/1.03  | 0.81/0.86        | 0.97/0.96        |
    0.98/1.00 |

    | RD (All/High)         | 0.59/0.76 | 1.03/1.04  | 0.75/0.83        | 0.95/0.95        |
    0.98/1.00 |


    TABLE 8. Average values of different metrics normalized to the baseline in a 4-core
    CMP system with two cache levels.


    <span id="page-16-0"></span>![](_page_16_Figure_3.jpeg)


    <span id="page-16-1"></span>FIGURE 20. Number of writes to the STT-RAM SLLC normalized
    to the baseline in the 8-core CMP system.


    results reveal that RD improves performance by around 7% compared to the baseline,
    while DASCA improves it by around 4%. As for write-intensive mixes, RD improves
    the baseline by 20% and DASCA by 11%. As shown in Figure [21,](#page-16-2) RD
    significantly overcomes DASCA and the baseline scheme in those mixes which contain
    any application with high WPKI.


    # *5.3.3. Energy savings*


    Figure [22](#page-17-1) illustrates the energy savings in the shared LLC. In general,
    the results in the 8-core scenario follow the trend observed for the 4-core environment.
    Specifically, RD reports around 32.5% energy reduction in the STT-RAM SLLC compared
    to the baseline while DASCA reduces energy by around 27%. In the case of write-intensive
    mixes, both RD and DASCA reduce the SLLC energy consumption by 34% and 27.5%,
    respectively. Analyzing the static and dynamic contributions on the SLLC energy
    consumption, overall, RD is able to reduce –for all mixes– the static energy part
    by around 6% with respect to the baseline (around 15% for the write-intensive
    mixes) while DASCA reduces the static contribution by 3.6% (9.5% for the *high*
    mixes). In addition, our approach reports dynamic energy savings of around 43%
    (36% for the *high* mixes) while DASCA numbers are 36% (30% for the *high* mixes).
    Note that mixes made up of applications with low WPKI exhibit the lowest energy
    savings across the board. This is consistent with the modest write reduction they
    report and especially with the high contribution of the static part to the total
    SLLC energy consumption that they exhibit, as Figure [23](#page-17-2) shows.


    Figure [24](#page-18-0) illustrates the energy savings obtained in the DRAM main
    memory, where it is shown that RD reduces the energy consumption of the main memory
    by around 6% on average compared to the baseline (3% for the write-intensive mixes),
    while DASCA reaches a 2.8% energy reduction and actually wastes more energy, around
    6%, for the *high* mixes.


    ![](_page_16_Figure_10.jpeg)


    <span id="page-16-2"></span>FIGURE 21. Instruction throughput normalized to the
    baseline in the 8-core CMP system.


    # 18 RODRIGUEZ-RODRIGUEZ ET AL


    ![](_page_17_Figure_2.jpeg)


    <span id="page-17-1"></span>FIGURE 22. Energy consumption in the STT-RAM SLLC
    normalized to the baseline in the 8-core CMP system.


    ![](_page_17_Figure_4.jpeg)


    <span id="page-17-2"></span>FIGURE 23. Breakdown of energy consumption in the
    SLLC into the static and dynamic contributions for the baseline in the 8-core
    CMP system.


    This energy waste may look surprising, given that DASCA is able to reduce the
    number of writes with respect to the baseline by 35% and to deliver a performance
    improvement higher than 10%. However, this can be explained by the fact that DASCA
    suffers a very significant increase in the amount of SLLC misses, which translates
    into high values of DRAM accesses (as shown in Table [9\)](#page-18-1).


    # *5.3.4. Discussion*


    As in the 4-core configuration, in this section we explain the reasons for the
    higher performance improvement achieved in our technique (RD) against DASCA in
    the 8-core scenario.


    As we already reasoned in the previous section, the better performance of RD is
    due to several factors, being the most important one the high efficiency achieved
    from the reuse locality exploitation. For demonstrating that fact, Figure [25](#page-18-2)
    shows the number of hits in the SLLC per kilo instruction that each mix experiments
    normalized to the baseline. As the figure shows, our approach achieves in most
    mixes a higher or much higher number of hits than DASCA, which confirms that RD
    uses a more efficient policy than DASCA.


    In addition to the hit rate improvement, there are other metrics that also justify
    achieving a better performance, such as SLLC misses, DRAM reads and writes, row
    buffer read hit rate and bank contention in the SLLC. All these metrics are shown
    in Table [9,](#page-18-1) for both RD and DASCA and also for both all and write-intensive
    mixes. Note that the RD beats DASCA in all the metrics considered.


    # *5.3.5. Sensitivity to Reuse Detector size*


    Given that the RD size is a determining factor in our proposal, and as done in
    the 4-CMP system, in Figure [26](#page-18-3) we show the amount of writes to the
    SLLC, the Instruction Throughput, and the energy consumption in both the SLLC
    and the main memory for different RD sizes per core, namely 8K, 16K, 32K and 64K
    entries.


    The trends are very similar to those observed in the 4 core scenario. Notably,
    a significant impact is observed on the capability to reduce the number of writes
    in the SLLC, especially for the All mixes, whereas a moderate (or even negligible
    in some cases) impact is seen on the average energy consumed in that cache level
    or main memory and performance of the overall system.


    # <span id="page-17-0"></span>5.4. RD performance in multi-core scenarios


    In this section we briefly inspect the main hints about the performance of RD
    when we augment the number of cores. So far, we have evaluated the RD in systems
    with one, four and eight cores. In order to further explore the RD behavior we
    have also evaluated our proposal in a system with 16 cores, employing the mixes
    shown in Table [6.](#page-8-1)


    In Table [10](#page-19-0) we recap the main numbers derived from RD and DASCA
    evaluation across the different configurations (for the sake of simplicity, we
    show just the numbers considering all applications or mixes, not those corresponding
    to the write-intensive ones). Notably, we illustrate the average LLC write reduction
    capability, energy savings in the LLC, the performance delivered and the energy
    savings in the DRAM.


    As shown, the write reduction capability in percentage terms gets reduced with
    the number of cores for both RD and DASCA. However, despite this decrease in the
    write filtering numbers, the most important consequence derived from this aspect,
    i.e. the net energy savings in the LLC, essentially remains in the range of 30-37%
    for RD, slightly decreasing


    ![](_page_18_Figure_0.jpeg)


    FIGURE 24. Energy consumption in the DRAM normalized to the baseline in the 8-core
    CMP system.


    <span id="page-18-0"></span>![](_page_18_Figure_2.jpeg)


    ![](_page_18_Figure_3.jpeg)


    <span id="page-18-2"></span>![](_page_18_Figure_4.jpeg)


    <span id="page-18-3"></span>FIGURE 26. Writes to SLLC, IT and energy consumption
    in both SLLC and main memory normalized to the baseline for different RD sizes
    per core in the 8-core CMP system.


    with the number of cores as a consequence of the increment in the number of writes,
    but keeping the benefit with respect to DASCA largely unchanged. Moreover, the
    performance improvement increases as the number of cores augments for both RD
    and DASCA, reporting significant numbers especially for the 8-core and 16-core
    system. Indeed, the


    | ❳<br>❳❳❳❳<br>Metrics | SLLC      | SLLC      | Row buffer Read | DRAM      |
    DRAM      | Bank contention in |

    |----------------------|-----------|-----------|-----------------|-----------|-----------|--------------------|

    | ❳❳❳❳❳<br>Policies    | Misses    | Hits      | Hit Rate        | reads     |
    Writes    | SLLC               |

    | DASCA (All/High)     | 1.08/1.30 | 0.92/0.84 | 1.00/0.99       | 1.08/1.30 |
    1.09/1.21 | 0.40/0.13          |

    | RD (All/High)        | 0.98/1.00 | 1.05/1.01 | 1.02/1.04       | 1.00/1.06 |
    1.02/1.05 | 0.24/0.07          |


    REUSE DETECTOR: IMPROVING THE MANAGEMENT OF STT-RAM SLLCS 19


    <span id="page-18-1"></span>TABLE 9. Average values of different metrics normalized
    to the baseline in the 8-core CMP system.


    20 RODRIGUEZ-RODRIGUEZ ET AL


    | ❵<br>❵❵❵❵<br>Metrics<br>❵❵❵<br>Scenario | LLC Write<br>reduction (%)<br>(DASCA/RD)
    | LLC Energy<br>Savings (%)<br>(DASCA/RD) | Performance<br>improvement (%)<br>(DASCA/RD)
    | DRAM Energy<br>Savings (%)<br>(DASCA/RD) |

    |-----------------------------------------|------------------------------------------|-----------------------------------------|----------------------------------------------|------------------------------------------|

    | Single core                             | 51.8 / 65.2                              |
    29.5 / 34.5                             | 0.3 / 1.9                                    |
    0.2 / 2.0                                |

    | 4-core CMP                              | 51.6 / 62.7                              |
    30.8 / 37.0                             | 1.2 / 2.9                                    |
    3.6 / 6.2                                |

    | 8-core CMP                              | 43.8 / 54.0                              |
    27.3 / 32.5                             | 3.7 / 6.7                                    |
    2.8 / 6.0                                |

    | 16-core CMP                             | 38.6 / 46.7                              |
    26.0 / 30.5                             | 10.2 / 14.5                                  |
    4.0 / 7.9                                |


    <span id="page-19-0"></span>TABLE 10. Average values of different metrics normalized
    to the baseline across different configurations.


    performance improvement that RD reports with respect to the DASCA scheme also
    increases with the number of cores (which is especially significant in the case
    of write-intensive mixes for the 16-CMP system, where RD is able to increase the
    performance of the baseline by around 38%, whereas DASCA only achieves 24% of
    improvement). This is due to the fact that the difference in LLC misses reported
    by both approaches increases as the number of cores augments (notably, these numbers
    are 1%, 6%, 10% and 13% for 1, 4, 8 and 16-core systems respectively, always reporting
    RD the best value) and also the difference in other factors as the number of hits
    in the LLC, the number of DRAM reads and writes or the row buffer hit rate also
    increase, benefiting RD more with the number of cores. Finally, and derived from
    the higher performance delivered, the same trend is observed in the energy savings
    experienced in the main memory (where it is worthy to note that, in the 16-core
    scenario, RD manages to reduce the energy consumption of the baseline by more
    than 15% for the write-intensive mixes, while DASCA hardly achieves 8%). Overall
    we can conclude that, as the number of cores increases, RD is able to significantly
    increase the performance delivered (and also to increase the energy savings achieved
    in the main memory) at the expense of a slight decrease in the LLC energy savings
    obtained.


    # 6. RELATED WORK


    To address the problems of energy consumption and performance of STT-RAM SLLCs,
    in the last years different researchers have proposed solutions aiming to reduce
    either the amount of writes or the per-write energy.


    A great body of work mainly tries to cut the write traffic to the STT-RAM: In
    [\[14\]](#page-21-9) the authors propose an obstructionaware cache management
    policy called OAP. OAP monitors the cache to periodically detect LLC-obstruction
    processes, and manage the cache accesses from different processes, so that when
    an LLC-obstruction is detected the data is forwarded to the next cache level or
    Main Memory as appropriate. In [\[15\]](#page-21-10) two techniques are proposed
    to reduce the number of writes to a last level (L2) STT-RAM cache and also save
    energy. The first one adds a small cache between L1 and L2 –called write-cache
    (WC)– which is mutually exclusive with L2 and stores only the dirty lines evicted
    from L1. On a cache access, both L2 and WC are accessed in parallel. The write
    misses are allocated in WC and the load misses are allocated in L2. WC reduces
    the number of L2 writes by absorbing most of the L1 writebacks. Other authors
    propose a coding scheme for STT-RAM last level cache based on the concept of value
    locality. They reduce switching probability in cache by swapping common patterns
    with limited weight codes to make writes less often as well as more uniform [\[16\]](#page-21-11).
    Other techniques [\[17\]](#page-21-12) rely on the observation that on average,
    a large fraction of bytes and words written to the L2 cache are only zero-valued
    data. Based on this, this technique adds additional "all-zero-data" flags in the
    tag arrays at the granularity of a single byte and a single word. Before any cache
    write, the data value is checked. If the all-zero bytes or words are detected,
    the corresponding flags are set and only the non-zero bytes or words are written.
    During a cache read operation, only the non-zero bytes or words are read and then
    the actual data are constructed by combining the information from the allzero
    flags. Another proposal [\[18\]](#page-21-13) logically divides the STT-RAM cache
    line into multiple partial lines. In L1 cache, a history bit is kept for each
    partial line to track which partial lines have changed. Using this information,
    when a dirty L1 block is written to last level cache, only those partial lines
    which have been changed are written. Other authors propose techniques for mitigating
    the write pressure caused due to prefetching in STT-RAM based LLC [\[19\]](#page-21-14).
    One of these techniques prioritizes different types of LLC requests such as load,
    store, prefetch, or write back, etc. based on their criticality. The critical
    requests are assigned a high priority and hence, they are served earlier. In multicore
    systems, the excessive requests generated from a cache-intensive program may block
    those generated from a cache-unintensive program which may lead to its starvation.
    To address this, they propose another technique which prioritizes the requests
    from a cache-unintensive program, so that they are served promptly. Also, authors
    in [\[9\]](#page-21-4) analyze the cache coherence protocols impact in the number
    of write to a LLC based on STT-RAM, showing that the protocols with a owned state
    (MOESI and MOSI) reduce the number of writes to LLC.


    Another body of work mainly deals with performance of STT-RAM caches: In [\[20\]](#page-21-15)
    a cache revive technique to calculate retention time is proposed. Some cache blocks
    retain data even after completion of retention time. The retention time is chosen
    so that it will minimize the number of unrefreshed cache blocks. Other authors
    propose the use of STT-RAM to design combinational logic, register files and on-chip
    storage (I/D L1 caches, TLBs and L2 cache) [\[21\]](#page-21-16). Also, to hide
    the write latency of STT-RAM, they propose subbank buffering which allows the
    writes to complete locally within each sub-bank, while the reads from


    other locations within the array can complete unobstructed. They show that by
    carefully designing the pipeline, the STT-RAM based design can significantly reduce
    the leakage power, while also maintaining the performance level close to the CMOS
    design. Also, an STT-RAM cache design for lower level caches where different cache
    ways are designed with different retention periods is proposed in [\[22\]](#page-21-17).
    For example, in a 16-way cache, way 0 is designed with a fast STT-RAM design with
    low retention period and the remaining 15 ways are designed with a slow STT-RAM
    design which has higher retention period. Their technique uses hardware to detect
    whether a block is read or write intensive. The write intensive blocks are primarily
    allocated to way 0, while the read intensive blocks are allocated to the other
    ways. Also, to avoid refreshing dying blocks in way 0, their technique uses data
    migration to move such blocks to banks with higher retention period. Finally,
    a write-buffer design to address the long write latency of last level (L2) STT-RAM
    cache is proposed in [\[23\]](#page-21-18). The L2 may receive a request from
    both L1 and the write buffer. Since read latency of STT-RAM is smaller than the
    write latency and also reads are performance-critical, the buffer uses a readpreemptive
    management policy, which ensures that a read request receives higher priority
    than a write request. The authors also propose a hybrid SRAM and STT-RAM cache
    design which aims to move the most write-intensive blocks to SRAM.


    It is worth mentioning that we compare our RD scheme just against DASCA due to
    three reasons: 1) DASCA, which is a more recent proposal than all the aforementioned
    works, is the closest work to ours in the sense that both schemes try to reduce
    the energy consumption of an STT-RAM LLC by bypassing write operations predicted
    to be dead (or not showing reuse), 2) some of the mentioned approaches are already
    evaluated in the DASCA paper, such as [\[14\]](#page-21-9), being clearly outperformed
    by the DASCA mechanism, and 3) some other schemes, such as [\[16,](#page-21-11)
    [17,](#page-21-12) [18,](#page-21-13) [19,](#page-21-14) [20,](#page-21-15) [22,](#page-21-17)
    [23\]](#page-21-18), although addressing the same problem, are completely orthogonal
    to our approach. Thus, the RD could be built on top of them, making a direct comparison
    meaningless. As an example, in [\[16\]](#page-21-11) the authors address the STT-RAM
    write energy problem at a circuit level, trying to reduce the number of writes
    and also to balance the wear among the cells. The proposal relies in identifying
    the most frequent values stored in the LLC (value locality) and encoding these
    patterns to reduce the number of writes. Thus, unlike our proposal, this approach
    operates at a bit-level.


    # 7. CONCLUSIONS


    In this paper we have addressed the main constraints of conventional SRAM last-level
    caches: power-hungry operation and inefficient management. In order to overcome
    these drawbacks we propose to employ a STT-RAM SLLC where its contents are selected
    according to a *Reuse Detector* which exploits the reuse locality of the stream
    of references arriving at the SLLC. The Reuse Detector is a hardware component
    that tracks block reuse and determines,


    according to its predicted future utility, if they must be inserted in the SLLC
    or bypassed to the main memory. The Reuse Detector succeeds in managing the STT-RAM
    SLLC contents in two complementary ways. First, it is able to bypass to main memory
    a significant fraction of the blocks coming to the SLLC, thus decreasing the amount
    of the energy-hungry writes to be performed. Second, it increases significantly
    the SLLC hit rate, which leads to moderate performance improvements. In addition,
    the energy consumption in the main memory is also reduced. This way, our approach
    is able to outperform other strategies also oriented to decrease the energy consumption
    in STT-RAM SLLCs, such as the DASCA scheme. Although DASCA exhibits slightly lower
    ability to cut the write operations to the SLLCs, this technique, which predicts
    if a block will not be reused again instead of predicting if a block is going
    to be reused as ours, achieves lower accuracy in the prediction, hence also significantly
    lower hit rates at this cache level and therefore much lower performance improvements.
    Overall RD reports on average energy reductions in the SLLC in the range of 37-30%,
    additional energy savings in the main memory in the range of 6-8% and performance
    improvements of 3% (quadcore), 7% (eight-core) and 14% (16-core) compared to an
    STT-RAM SLLC baseline where no reuse detector is employed. More importantly, our
    approach outperforms DASCA, the state-of-the-art STT-RAM SLLC management, reporting
    –depending on the specific scenario and the kind of applications used– SLLC energy
    savings in the range of 4-11% higher than those of DASCA, delivering higher performance
    in the range of 1.5-14%, and an additional improvement in DRAM energy consumption
    in the range of 2-9% higher than DASCA.


    # ACKNOWLEDGEMENTS


    This work has been supported in part by the Spanish government through the research
    contracts TIN2012-32180, TIN2015-65277-R, TIN2015-65316-P and by the HIPEAC-4
    European Network of Excellence. It has been also supported by a grant scholarship
    from the University of Costa Rica and the Costa Rican Ministry of Science and
    Technology MICIT and CONICIT.


    # <span id="page-20-0"></span>REFERENCES


    - [1] Ahn, J., Yoo, S., and Choi, K. (2014) Dasca: Dead write prediction assisted
    stt-ram cache architecture. *IEEE 20th International Symposium on High Performance
    Computer Architecture (HPCA), 2014*, pp. 25–36. IEEE.

    - <span id="page-20-1"></span>[2] Khan, S. M., Tian, Y., and Jimenez, D. A. (2010)
    Sampling dead block prediction for last-level caches. *Proceedings of the 2010
    43rd Annual IEEE/ACM International Symposium on Microarchitecture*, Washington,
    DC, USA MICRO ''43, pp. 175–186. IEEE Computer Society.

    - <span id="page-20-2"></span>[3] Albericio, J., Ibáñez, P., Viñals, V., and Llabería,
    J. M. (2013) Exploiting reuse locality on inclusive shared last-level caches.
    *ACM Trans. Archit. Code Optim.*, 9, 38:1–38:19.

    - <span id="page-20-3"></span>[4] Albericio, J., Ibáñez, P., Viñals, V., and Llabería,
    J. M. (2013) The reuse cache: Downsizing the shared last-level cache.


    *Proceedings of the 46th Annual IEEE/ACM International Symposium on Microarchitecture*,
    New York, NY, USA MICRO-46, pp. 310–321. ACM.


    - <span id="page-21-1"></span><span id="page-21-0"></span>[5] (2013). http://www.hpl.hp.com/research/cacti/.

    - [6] Dong, X. et al. (2012) NVSim: A circuit-level performance, energy, and area
    model for emerging nonvolatile memory. *IEEE Transaction on Compter-Aided Design
    of Integrated Circuits and Systems*, 31, 994–1007.

    - <span id="page-21-2"></span>[7] Díaz, J., Monreal, T., Viñals, V., Ibáñez, P.,
    and Llabería, J. M. (2015) Selección de contenidos basada en reuso para caches
    compartidas en exclusión. *Proceedings of the XXVI Jornadas de Paralelismo* JP-2015,
    pp. 433–442.

    - <span id="page-21-4"></span><span id="page-21-3"></span>[8] Binkert, N. et al.
    (2011) The gem5 simulator. *ACM SIGARCH Computer Architecture News*, 39, 1.

    - [9] Chang, M.-T., Lu, S.-L., and Jacob, B. (2014) Impact of cache coherence
    protocols on the power consumption of stt-rambased llc. *The Memory Forum Workshop*.

    - <span id="page-21-5"></span>[10] Agarwal, N., Krishna, T., Peh, L.-S., and Jha,
    N. K. (2009) Garnet: A detailed on-chip network model inside a fullsystem simulator.
    *Performance Analysis of Systems and Software, 2009. ISPASS 2009. IEEE International
    Symposium on*, pp. 33–42. IEEE.

    - <span id="page-21-6"></span>[11] Rosenfeld, P., Cooper-Balis, E., and Jacob,
    B. (2011) Dramsim2: A cycle accurate memory system simulator. *Computer Architecture
    Letters*, 10, 16 –19.

    - <span id="page-21-8"></span><span id="page-21-7"></span>[12] (2013). http://www.spec.org/cpu2006/.

    - [13] Patil, H., Cohn, R. S., Charney, M., Kapoor, R., Sun, A., and Karunanidhi,
    A. (2004) Pinpointing representative portions of large intel® itanium® programs
    with dynamic instrumentation. *MICRO*, pp. 81–92. IEEE Computer Society.

    - <span id="page-21-9"></span>[14] Wang, J., Dong, X., and Xie, Y. (2013) Oap:
    an obstructionaware cache management policy for stt-ram last-level caches. *Proceedings
    of the Conference on Design, Automation and Test in Europe*, pp. 847–852. EDA
    Consortium.

    - <span id="page-21-10"></span>[15] Rasquinha, M. (2011) An energy efficient cache
    design using spin torque transfer (STT) RAM. Master of science in the school of
    electrical and computer engineering Georgia Institute of Technology.

    - <span id="page-21-11"></span>[16] Yazdanshenas, S., Ranjbar Pirbast, M., Fazeli,
    M., and Patooghy, A. (2014) Coding last level stt-ram cache for high endurance
    and low power. *IEEE Computer Architecture Letters*, 13, 73–76.

    - <span id="page-21-12"></span>[17] Jung, J., Nakata, Y., Yoshimoto, M., and Kawaguchi,
    H. (2013) Energy-efficient spin-transfer torque ram cache exploiting additional
    all-zero-data flags. *Quality Electronic Design (ISQED), 2013 14th International
    Symposium on*, pp. 216–222. IEEE.

    - <span id="page-21-13"></span>[18] Park, S. P., Gupta, S., Mojumder, N., Raghunathan,
    A., and Roy, K. (2012) Future cache design using stt mrams for improved energy
    efficiency: devices, circuits and architecture. *Proceedings of the 49th Annual
    Design Automation Conference*, pp. 492–497. ACM.

    - <span id="page-21-14"></span>[19] Mao, M., Li, H. H., Jones, A. K., and Chen,
    Y. (2013) Coordinating prefetching and stt-ram based last-level cache management
    for multicore systems. *Proceedings of the 23rd ACM international conference on
    Great lakes symposium on VLSI*, pp. 55–60. ACM.

    - <span id="page-21-15"></span>[20] Jog, A., Mishra, A. K., Xu, C., Xie, Y., Narayanan,
    V., Iyer, R., and Das, C. R. (2012) Cache revive: architecting volatile stt-ram
    caches for enhanced performance in cmps. *Proceedings of the 49th Annual Design
    Automation Conference*, pp. 243–252. ACM.

    - <span id="page-21-16"></span>[21] Guo, X., Ipek, E., and Soyata, T. (2010) Resistive
    computation: avoiding the power wall with low-leakage, stt-mram based computing.
    *ACM SIGARCH Computer Architecture News*, pp. 371–382. ACM.

    - <span id="page-21-17"></span>[22] Sun, Z., Bi, X., Li, H. H., Wong, W.-F., Ong,
    Z.-L., Zhu, X., and Wu, W. (2011) Multi retention level stt-ram cache designs
    with a dynamic refresh scheme. *Proceedings of the 44th Annual IEEE/ACM International
    Symposium on Microarchitecture*, pp. 329–338. ACM.

    - <span id="page-21-18"></span>[23] Sun, G., Dong, X., Xie, Y., Li, J., and Chen,
    Y. (2009) A novel architecture of the 3d stacked mram l2 cache for cmps. *High
    Performance Computer Architecture, 2009. HPCA 2009. IEEE 15th International Symposium
    on*, pp. 239–249. IEEE.


    This figure "CeldaSTTRAM.jpeg" is available in "jpeg" format from:


    This figure "CeldaSTTRAM.png" is available in "png" format from:


    This figure "STTRAMCircuit.jpeg" is available in "jpeg" format from:


    This figure "STTRAMCircuit.png" is available in "png" format from:'
  decisions:
    evaluation_prompt: 'Qualified. Reason: All relevant sections passed.'
    related_work_prompt: 'Qualified. Reason: All relevant sections passed.'
    novelty_prompt: 'Qualified. Reason: All relevant sections passed.'
    review_only_prompt: 'Qualified. Reason: All relevant sections passed.'
  topics:
    main_topic: Computer Architecture
    main_topic_reasoning: The paper discusses a new management mechanism for STT-RAM
      based shared last-level caches (SLLCs), which directly relates to the design
      and performance of computer architecture, particularly in the context of memory
      technologies.
    secondary_topic: ''
    secondary_topic_reasoning: ''
    main_topic_sub: Memory Hierarchy
    secondary_topic_sub: ''
- title: "Cocco: Hardware-Mapping Co-Exploration towards Memory\n  Capacity-Communication\
    \ Optimization"
  abstract: "Memory is a critical design consideration in current data-intensive DNN\n\
    accelerators, as it profoundly determines energy consumption, bandwidth\nrequirements,\
    \ and area costs. As DNN structures become more complex, a larger\non-chip memory\
    \ capacity is required to reduce data movement overhead, but at\nthe expense of\
    \ silicon costs. Some previous works have proposed memory-oriented\noptimizations,\
    \ such as different data reuse and layer fusion schemes. However,\nthese methods\
    \ are not general and potent enough to cope with various graph\nstructures.\n\
    \  In this paper, we explore the intrinsic connection between network structures\n\
    and memory features to optimize both hardware and mapping. First, we introduce\n\
    a graph-level execution scheme with a corresponding dataflow and memory\nmanagement\
    \ method. This scheme enables the execution of arbitrary graph\npatterns with\
    \ high data reuse and low hardware overhead. Subsequently, we\npropose Cocco,\
    \ a hardware-mapping co-exploration framework leveraging\ngraph-level features\
    \ of networks. It aims to minimize communication overhead,\nsuch as energy consumption\
    \ and bandwidth requirements, with a smaller memory\ncapacity. We formulate the\
    \ graph-partition scheduling and memory configuration\nsearch as an optimization\
    \ problem and employ a genetic-based method to achieve\nefficient co-exploration\
    \ for large and irregular networks. Experiments\ndemonstrate that Cocco obtains\
    \ lower external memory access, lower bandwidth\nrequirements, and more stable\
    \ optimization for graph partition compared to the\ngreedy algorithm and dynamic\
    \ programming introduced in prior works. Cocco also\nreduces the costs by 1.89%\
    \ to 50.33% using co-exploration compared to other\ntypical methods."
  url: http://arxiv.org/abs/2402.00629v1
  keywords: ''
  document: '# Cocco: Hardware-Mapping Co-Exploration towards Memory Capacity-Communication
    Optimization


    Zhanhong Tan tanzh19@mails.tsinghua.edu.cn IIIS, Tsinghua University Beijing,
    China


    Zijian Zhu zhuzj23@mails.tsinghua.edu.cn IIIS, Tsinghua University Beijing, China


    Kaisheng Ma<sup>∗</sup> kaisheng@mail.tsinghua.edu.cn IIIS, Tsinghua University
    Beijing, China


    # Abstract


    Memory is a critical design consideration in current dataintensive DNN accelerators,
    as it profoundly determines energy consumption, bandwidth requirements, and area
    costs. As DNN structures become more complex, a larger on-chip memory capacity
    is required to reduce data movement overhead, but at the expense of silicon costs.
    Some previous works have proposed memory-oriented optimizations, such as different
    data reuse and layer fusion schemes. However, these methods are not general and
    potent enough to cope with various graph structures.


    In this paper, we explore the intrinsic connection between network structures
    and memory features to optimize both hardware and mapping. First, we introduce
    a graph-level execution scheme with a corresponding dataflow and memory management
    method. This scheme enables the execution of arbitrary graph patterns with high
    data reuse and low hardware overhead. Subsequently, we propose Cocco, a hardware-mapping
    co-exploration framework leveraging graph-level features of networks. It aims
    to minimize communication overhead, such as energy consumption and bandwidth requirements,
    with a smaller memory capacity. We formulate the graph-partition scheduling and
    memory configuration search as an optimization problem and employ a genetic-based
    method to achieve efficient co-exploration for large and irregular networks. Experiments
    demonstrate that Cocco obtains lower external memory access, lower bandwidth requirements,
    and more stable optimization for graph partition compared to the greedy algorithm
    and dynamic programming introduced in prior works. Cocco also reduces the costs
    by 1.89% to 50.33% using co-exploration compared to other typical methods.


    ASPLOS ''24, April 27-May 1, 2024, La Jolla, CA, USA © 2024 Copyright held by
    the owner/author(s). ACM ISBN 979-8-4007-0372-0/24/04. <https://doi.org/10.1145/3617232.3624865>


    CCS Concepts: • Hardware → Design reuse and communication-based design; On-chip
    resource management; • Computer systems organization → Parallel architectures;
    • Software and its engineering → Compilers.


    Keywords: Design space exploration, Memory, Graph analysis, Subgraph, Genetic
    algorithm, Deep learning accelerator


    #### ACM Reference Format:


    Zhanhong Tan, Zijian Zhu, and Kaisheng Ma. 2024. Cocco: Hardware-Mapping Co-Exploration
    towards Memory Capacity-Communication Optimization. In 29th ACM International
    Conference on Architectural Support for Programming Languages and Operating Systems,
    Volume 1 (ASPLOS ''24), April 27-May 1, 2024, La Jolla, CA, USA. ACM, New York,
    NY, USA, [16](#page-15-0) pages. <https://doi.org/10.1145/3617232.3624865>


    ## <span id="page-0-0"></span>1 Introduction


    The evolution of neural network topology has driven the remarkable progress of
    artificial intelligence from the early single-layer perceptron (SLP) [\[45,](#page-14-0)
    [54\]](#page-14-1) and multi-layer perceptron (MLP) [\[17,](#page-13-0) [22,](#page-13-1)
    [39\]](#page-13-2) to modern DNNs with plain [\[36,](#page-13-3) [57\]](#page-14-2)/inception
    [\[59\]](#page-14-3)/residual [\[20,](#page-13-4) [55\]](#page-14-4) structures
    based on manual design, and even irregular structures using neural architecture
    search (NAS) [\[53,](#page-14-5) [75\]](#page-15-1) or random network generation
    [\[68\]](#page-14-6). These technological innovations have resulted in increasingly
    complex computation graphs, which pose challenges for efficient memory design
    and deployment.


    Memory design is crucial in the accelerator system, as it performs data preparation
    at the start of each processing stage according to the scheduling scheme, determining
    energy consumption, bandwidth requirements, and area costs. Figure [1](#page-1-0)
    shows the trade-off between the on-chip memory size and the external memory access
    in DNN accelerators. A smaller on-chip buffer (left side) saves area but requires
    more data reloading. A larger buffer (right side) can reduce external memory access
    and save energy and bandwidth but at the cost of increasing the memory overhead.
    An excessively large SRAM may not be feasible due to the high silicon area cost,
    typically ranging from 1 to 2 mm<sup>2</sup> /MB in 12nm, and the high energy
    overhead, dozens of times that of a MAC operation for a large SRAM.


    Therefore, the key problem is: between the two extremes in Figure [1,](#page-1-0)
    how to find an appropriate memory configuration with efficient workload mapping
    and data management, especially under the growing complexity of neural network
    architectures.


    <sup>∗</sup>Corresponding author.


    Permission to make digital or hard copies of part or all of this work for personal
    or classroom use is granted without fee provided that copies are not made or distributed
    for profit or commercial advantage and that copies bear this notice and the full
    citation on the first page. Copyrights for thirdparty components of this work
    must be honored. For all other uses, contact the owner/author(s).


    ASPLOS ''24, April 27-May 1, 2024, La Jolla, CA, USA Zhanhong Tan, Zijian Zhu,
    and Kaisheng Ma


    <span id="page-1-0"></span>![](_page_1_Figure_1.jpeg)


    Figure 1. The effect of different memory capacities for a computation graph. Intermediate
    results can be buffered in the on-chip memory if it is large enough. The on-chip
    memory of small capacity can only buffer two nodes (marked in the red dotted box),
    and the larger memory can cover a larger subgraph (right side).


    The critical status of memory design has attracted extensive research. Most previous
    studies focus on simple layer-level optimization (the left one of Figure [1\)](#page-1-0)
    by applying loop transformation techniques such as tiling and reordering to fit
    the memory size and reuse the on-chip data [\[23,](#page-13-5) [43,](#page-14-7)
    [44,](#page-14-8) [61,](#page-14-9) [70\]](#page-15-2). In addition, several works
    also guide the memory capacity and hierarchy design using designspace exploration
    [\[12,](#page-12-0) [32,](#page-13-6) [37,](#page-13-7) [66,](#page-14-10) [67\]](#page-14-11).
    However, these layerlevel optimizations are confined to the limited intra-layer
    reuse, which is insufficient for memory-intensive networks. A subgraph-level scheme
    (e.g., the middle one and the right one of Figure [1\)](#page-1-0) provides a
    larger optimization space via inter-layer reuse [\[3,](#page-12-1) [4,](#page-12-2)
    [38,](#page-13-8) [73\]](#page-15-3) to reduce the I/O overhead. Therefore, this
    paper aims to leverage the subgraph-level computing flow to optimize the memory
    capacity and external communication for networks with any topology.


    However, there are three primary challenges to fully exploit the subgraph-level
    optimization.


    First, we need a general execution flow for any sub-graph. Due to the various
    kernel sizes and strides, a parent node in a subgraph may have unbalanced data
    requirements from its consumers, which makes it difficult to determine the tensor
    tiling scheme and the memory allocation for each node (layer). In the traditional
    single-layer execution, we usually divide a large tensor into loop tiles, which
    are processed through a series of regular computing steps. Similarly, we want
    the sub-graph execution to be a series of elementary computing steps with a simple
    control flow.


    Second, we require a suitable memory management method for the subgraph execution.
    Due to complicated dependency among nodes in a subgraph, careful management is
    needed to reuse overlapping and inter-layer intermediate data.


    Solving these two challenges contributes to a basic hardware execution model compatible
    with subgraph-level optimization. However, we also encounter the third challenge:
    how to partition a model into subgraphs and how much memory to allocate. The optimization
    space is huge, so we need to devise a search method with high sampling efficiency
    to find a proper subgraph partition and memory configuration result.


    In this paper, we first introduce a complete graph-level scheme for memory. In
    particular, it contains a consumptioncentric flow that enables the execution of
    arbitrary subgraphs with low memory footprints (for challenge 1). Accordingly,
    we provide an explicit memory dataflow and the corresponding memory management
    scheme for effective data reuse (for challenge 2). Building on the graph-level
    memory scheme, we propose Cocco, a hardware-mapping co-exploration framework,
    to establish a connection between model features and the memory configuration
    (for challenge 3).


    Cocco aims to find a combination of on-chip buffers and the corresponding graph-level
    scheduling for lower memory and communication overhead. In particular, we develop
    a genetic-based algorithm to efficiently explore the search space of graph partitions
    and the associated memory configuration for a series of neural networks.


    In summary, this work makes the following contributions:


    - Subgraph execution scheme. We first introduce a consumption-centric flow to
    determine a low-cost execution sequence by throttling and aligning the dataflow.

    - Efficient dataflow and memory management for subgraph data reuse. We propose
    a memory management scheme featuring multiple reconfigurable regions and the corresponding
    dataflow to support arbitrary subgraph execution with full data reuse.

    - Hardware-mapping co-exploration framework. Based on the subgraph execution scheme
    and memory dataflow, we propose Cocco, a genetic-based framework combining the
    graph-level partition and memory design-space exploration together. Cocco achieves
    1.89% to 50.33% lower costs (lower communication with a smaller size) using co-exploration
    in contrast to other methods.


    ## 2 Background and Motivation


    #### 2.1 Design of Neural Network Accelerators


    The DNN accelerator unit is the most basic execution unit in a computing system,
    on top of which, we can scale it out to many-core, many-socket, and many-drawer
    systems [\[24,](#page-13-9) [40,](#page-13-10) [48,](#page-14-12) [60\]](#page-14-13).
    An accelerator unit usually employs a processing element (PE) array on a sophisticated
    interconnection network to enable efficient tensor-level computation. Each PE
    typically contains local scratchpads and ALUs to process basic data packets. The
    global buffer and the weight buffer store activations and weights, and they are
    generally


    <span id="page-2-0"></span>![](_page_2_Figure_1.jpeg)


    *\* Those designs only support INT8 precision for tensor, we scale to FP16 performance
    by a factor of 0.5. \*\* For most designs fabricated under 12nm (or close to)
    process, we align all areas to 12nm. The SRAM area is estimated as 1.2mm<sup>2</sup>/MB.*


    Figure 2. Left: performance v.s. memory capacity of several industrial NPUs. Right:
    a summary of SRAM area ratio in these accelerators.


    located next to the PE array to serve as the data interface and manage data between
    the PE array and the external memory (e.g., DRAM or other cores). Due to the limited
    capacity of the global buffer, the compiler has to partition the network execution
    into a series of elementary workloads that are scheduled along the parallel spatial
    resources and the temporal dimension [\[18,](#page-13-11) [61,](#page-14-9) [72\]](#page-15-4).
    The capacity of the global buffer usually dominates the external memory access
    and bandwidth requirements, significantly impacting system performance. If the
    global memory is larger, it is more likely to buffer more intermediate data and
    avoid data being evicted to DRAM. As shown in Figure [1,](#page-1-0) a larger
    buffer expands the scope of elementary workloads from a single layer to a larger
    subgraph, reducing the communication overhead.


    However, choosing an appropriate memory specification is always a challenge. In
    Figure [2,](#page-2-0) we surveyed 16 popular industrial neural network processors
    with various memory/performance/area characteristics, where nine of them target
    the training domain [\[6,](#page-12-3) [11,](#page-12-4) [24,](#page-13-9) [34,](#page-13-12)
    [35,](#page-13-13) [40,](#page-13-10) [41,](#page-13-14) [48,](#page-14-12) [60,](#page-14-13)
    [63,](#page-14-14) [69\]](#page-14-15) and seven target model inference [\[1,](#page-12-5)
    [7,](#page-12-6) [8,](#page-12-7) [26–](#page-13-15)[28,](#page-13-16) [49,](#page-14-16)
    [65\]](#page-14-17). According to the survey, we can observe several trends as
    follows:


    - 1. Memory occupies a significant portion of the silicon footprint on an NPU
    chip, ranging from 4% to 79% of the area, with capacities from 2.5MB to 896MB.

    - 2. Figure [2](#page-2-0) Left shows a trend of diminishing marginal benefit
    of memory capacity. This is because there is a critical capacity to meet the data
    reuse and bandwidth requirement at the beginning, and the increments become negligible
    with higher memory capacity.

    - 3. We can infer that there is a saturated capacity equivalent to the ideal unlimited
    memory, especially for the inference design. For example, Hanguang [\[26\]](#page-13-15)
    is a special SRAM-only inference system without DDR, and the 394MB buffers are
    large enough to hold the intermediate data in their scenarios.


    <span id="page-2-1"></span>![](_page_2_Figure_10.jpeg)


    Figure 3. Evaluations on subgraphs fusing different number of layers (denoted
    as L=1,3,5). Y-axis is in the log domain. The 2TOPS NPU accelerator is configured
    with a 1MB global buffer and a 1.125MB weight buffer. The bandwidth requirement
    of weights is from the prefetch of the next subgraph, while that of activations
    is from the inputs and outputs of each subgraph.


    This survey implies a design trade-off between memory capacity and performance
    based on workloads and commercial considerations. Motivated by the observations
    above, this paper aims to provide several memory design considerations and study
    the connection between workload features and memory capacity in an NPU accelerator.


    #### 2.2 Workload Deployment


    A neural network is usually executed in a DNN accelerator with layer or graph
    granularities based on the buffer capacity and dataflow.


    2.2.1 Layer-level Assignment. This manner assigns tasks layer by layer. Most previous
    studies employ a tiling-based layer-wise execution manner [\[10,](#page-12-8)
    [21,](#page-13-17) [30,](#page-13-18) [37,](#page-13-7) [50,](#page-14-18) [61\]](#page-14-9),
    which elaborates the tiling sizes of tensors to fit in the accelerator buffers
    and maintain performance. A proper tiling scheme should overlap the data loading
    latency with the computing time of each tile and try to reduce the repeated access
    of local weight buffers. Tiles of data are transferred between the external memory
    and the global buffer, and PEs subsequently fetch data from the global to their
    local buffers. Given the larger bit-width of partial sums (e.g., 24bit partial
    sums v.s. 8bit inputs in Simba), the output-centric tiling scheme is more commonly
    used to calculate the final results before writing back to the global buffer [\[61\]](#page-14-9).


    2.2.2 Graph-level Assignment. Unlike the layer-level assignment that restrains
    from leveraging inter-layer reuse, a graph-level assignment processes several
    layers of a neural network as a whole. To demonstrate the effectiveness of the
    layer-level assignment, we evaluate four networks on a 2TOPS accelerator model,
    as shown in Figure [3.](#page-2-1) The results show that fusing layers into subgraphs
    significantly reduces external memory access by 42.3% ∼ 74.7% and average bandwidth
    requirements by 26.8% ∼ 67.8%. However, the improvements of larger subgraphs are
    marginal, indicating that there is an optimal trade-off between inter-layer


    reuse and subgraph size, which determines the memory requirement. For example,
    executing three-layer subgraphs reduces external memory access by 53.7% in ResNet50,
    while executing five-layer subgraphs only further reduces it by 13.6%.


    Several works have studied inter-layer reuse and graph partition. However, they
    have several limitations in terms of performance and flexibility. LCP [\[42\]](#page-14-19)
    groups similar layers into a cluster and executes them as a whole, which makes
    it challenging to generalize into an arbitrary graph. Fused-CNN [\[4\]](#page-12-2)
    and SR-CNN [\[38\]](#page-13-8) fuse large contiguous layers for plain networks
    using manually-designed strategies. Irregular-NN [\[73\]](#page-15-3) attempts
    to execute a complex subgraph using a DP-based algorithm, but the constrained
    search space limits the exploration.


    To overcome these challenges, we propose an end-to-end framework that automatically
    optimizes the graph partition and memory configuration for any neural network.
    Our framework consists of two main components: a graph-level dataflow and a hardware-mapping
    co-exploration algorithm. We first introduce the graph-level dataflow and its
    hardware implementation. Then, we present Cocco, an efficient algorithm that explores
    the trade-offs among memory configurations and graph partition schemes based on
    workload features.


    ## <span id="page-3-1"></span>3 The Proposed Graph-Level Scheme


    To execute layers on an NPU core in a graph-level manner, we need an effective
    approach to reuse intermediate data and decide the memory allocation. This section
    presents our comprehensive scheme for subgraph execution, which addresses the
    first two challenges mentioned in Section [1.](#page-0-0) First, we describe a
    multi-layer execution flow that minimizes the memory footprint by a friendly tiling
    approach (for challenge 1). Second, we explain how to implement this flow on a
    real NPU using an efficient data reuse pattern (for challenge 2). The consistent
    target is to reduce the memory footprint and be friendly to implementation.


    #### 3.1 Subgraph execution scheme


    It is common practice for the layer-level scheduling to partition the output tensor
    into several tiles as layer-level elementary operations [\[56,](#page-14-20) [61,](#page-14-9)
    [72,](#page-15-4) [74\]](#page-15-5), simplifying the scheduling and instruction
    generation. Likewise, our high-level idea is also to generate a series of explicit
    subgraph-level elementary operations. However, we need to address the challenges
    of various kernel sizes and strides in different paths to prevent unbalanced data
    production and unnecessary memory.


    A model''s subgraph consists of multiple layers (nodes) with dependencies. Section
    [4](#page-5-0) provides detailed information on subgraph partition. In Figure
    [4\(](#page-3-0)a), we present a straightforward production-centric scheme for
    executing a subgraph


    with different kernel sizes in two branches, deriving tile sizes of the subsequent
    layers based on the predetermined input tile sizes. For example, we can produce
    a 1 × 1 tile of Node(0) and a 2 × 2 tile of Node(2) with a given 5 × 5 feature
    map of input Node(-1). In this case, these intermediate results only reduce to
    1 × 1 in Node(3), limited by the smallest input of Node(0), so the remaining results
    of Node(2) can not be consumed immediately. As shown in Figure [4,](#page-3-0)
    three extra data of Node(2) along with sixteen extra source data of Node(1) take
    up extra memory space. There are more redundant cached data when the subgraph
    becomes larger and more complicated. Disadvantages of this manner are attributed
    to the production-centric idea that consumes all related activations from the
    producers at once.


    To avoid the memory overhead of storing unused data, we propose a consumption-centric
    scheme in Figure [4\(](#page-3-0)b), where results of each node are produced on
    demand based on consumer(s) (i.e., output node(s)). For example, given a 1 × 1
    tile of Node(3), we derive the 1 × 1 tile size for Node(2), which subsequently
    decides a 3 × 3 tile for Node(1).


    The backward-derivation for each producer node is nontrivial because of diverse
    kernel sizes and strides in different paths. Therefore, we propose a three-stage
    flow to determine the behavior of each node, as illustrated in Figure [5.](#page-4-0)
    The highlevel idea is to let output nodes drive the whole execution and match
    the data consumption and production in each subgraph-level elementary operation.


    The stage-1 is similar to the traditional single-layer scheduling, where the tile
    size is optimized for higher computation utilization. In order to hold a larger
    subgraph, the tile size


    <span id="page-3-0"></span>![](_page_3_Figure_14.jpeg)


    Figure 4. A conceptual comparison between two manners to process a subgraph. The
    node marked with a negative number represents the input node. The corresponding
    subgraph is shown in the upper right, where × / refers to the convolution kernel
    size ( ) and stride ().


    Cocco: Hardware-Mapping Co-Exploration towards Memory ... ASPLOS ''24, April 27-May
    1, 2024, La Jolla, CA, USA


    <span id="page-4-0"></span>![](_page_4_Figure_1.jpeg)


    Figure 5. The flow to determine the execution scheme of a subgraph (i.e., the
    computed tile size of each node, the tile offset, and the processing sequence
    of nodes). For simplicity, we discuss the 1D-CONV in this example and it is similar
    in the 2D-CONV case.


    tends to be smaller. In the 1D-CONV example, we set the tile size to be 2 for
    output nodes.


    The stage-2 aims to determine the data update offset Δ and the memory allocation
    size for each node based on the consumer(s), processing in the reverse topological
    order. We use the least common multiply (LCM) operation to determine Δ () of producers
    for aligning different input offset requirements (Δ () () ) from consumers. Hence,
    one producer update may correspond to multiple updates of a consumer. For example,
    Δ (−2) = lcm{Δ (0) (0) , Δ (1) (1) } = 4 = 2Δ (1) (1) , one update of Node(-2)
    corresponds to two updates of Node(1). As for the tile size deduction, (Δ () /
    () ) is to derive the required input tile size (,) for output node [1](#page-4-1)
    , where Δ () / () is the consumer offset (updated data) per producer update. The
    maximum result (,) of all outputs is the tile size () of input node . In this
    example, (−2) = max{<sup>0</sup> (2), <sup>1</sup> (4)} = 6 and (−1) = max{<sup>1</sup>
    (2), <sup>2</sup> (2)} = 4.


    As mentioned above, since we use LCM to align production and consumption, one
    producer update may correspond to multiple updates of a consumer. In the stage-3,
    we use \_ to represent the number of memory update per subgraph elementary operation.
    The generated result of the example in Figure [5](#page-4-0) is shown in Figure
    [6.](#page-4-2) \_ of Node(-1), Node(1), and Node(2) are two, where the second
    updates are highlighted in red boxes. Note that the {\_(−2) , . . . , \_(2) }
    solution is not unique, but the unique co-prime one {1, 2, 1, 2, 2} corresponds
    to the minimal elementary operation.


    <span id="page-4-2"></span>![](_page_4_Figure_8.jpeg)


    Figure 6. The memory snapshot during two subgraph elementary operations based
    on the execution scheme of Figure [5](#page-4-0) example. The allocated memory
    size and update offset correspond to and Δ, respectively (the [:] notation denotes
    data ranging from index to ). The arrows denote the data dependency according
    to the node relation in the subgraph.


    The proposed flow is based on a general directed acyclic computation graph and
    is not limited to specific layer features. In this way, we can determine the execution
    scheme for any complex irregular network like NasNet [\[75\]](#page-15-1) and
    RandWire [\[68\]](#page-14-6).


    #### 3.2 Memory Management for the subgraph execution


    Up to now, we have inferred the execution scheme for subgraphs, and the remaining
    challenge is how to implement it on hardware efficiently. Figure [7](#page-5-1)
    shows the memory allocation and update scheme for the subgraph execution. Before
    computing a subgraph, the compiler determines logical blocks for input, intermediate,
    and output nodes, where the block sizes depend on the tile sizes derived from
    the execution flow.


    For convenient management, we introduce two types of memory regions: MAIN and
    SIDE. The MAIN region stores the source data for PE (i.e., the tile of <sup>0</sup>
    × <sup>0</sup> × in Figure [7\)](#page-5-1). The SIDE region reserves the horizontally
    overlapping data[2](#page-4-3) . Considering no reuse requirement for some output
    nodes, we only need a MAIN region to buffer the results of the current tile. Except
    for the input nodes (negative numbers) loading data from DRAM, the other nodes
    update data locally based on the computed results of the input node(s).


    In detail, the update scheme leverages the collaboration between the MAIN region
    and the SIDE region to achieve full reuse across sliding tiles (we consider kernel
    size > stride). As shown in Figure [7,](#page-5-1) when the convolution windows
    slide across the feature maps, the vertical overlap data (e.g., column = 5) are
    reused locally in the MAIN region. In contrast, the horizontally overlapping data
    (e.g., the first row of = 6 ∼ 8) are loaded from the SIDE region (path ①). Only
    a subset of data is replaced by the newly calculated results


    <span id="page-4-1"></span><sup>1</sup>For example, assume node is a convolution
    layer with kernel size () and stride () , then ( ) = () + ( − 1) × () .


    <span id="page-4-3"></span><sup>2</sup>We assume the column is the inner loop
    while the row is the outer loop.


    (marked in green). Besides, the bottom horizontal slices write new data to the
    SIDE region for the next row loop (path ②).


    The extra hardware overhead for the proposed memory scheme is slight. Figure [8](#page-5-2)
    presents our 12nm NPU core for the subgraph processing, with a buffer region manager
    to logically partition the global buffer to support contiguous layer processing.
    The buffer region manager is a 2-depth register file, where determines the maximum
    subgraph size, and each entry pair indicates the start and the end address for
    each region. The area overhead is quite small, and in our test chip, the area
    ratio is only 0.18% with = 64 and 272-byte size (17-bit address for the 1MB 64bit-width
    global buffer).


    In summary, our high-level idea is to divide the buffer into logical blocks for
    different layers and try to reuse data for sliding convolution windows. The memory
    management approach can be compatible with an accelerator as long as it supports
    the data movement inside the on-chip memory and flexible data assignment for computing.
    Coupled with our subgraph execution scheme introduced before, intermediate outputs
    in the subgraph can avoid being recomputed. Only those layers required by other
    subgraphs are written back to DRAM for further reuse.


    # <span id="page-5-0"></span>4 Memory Communication-Capacity Co-Exploration


    <span id="page-5-1"></span>**A subgraph example**


    The aforementioned hardware model enables arbitrary subgraph execution, but there
    is always limited buffer capacity


    **Data update diagram**


    ![](_page_5_Figure_6.jpeg)


    Figure 7. Memory allocation and data update scheme in the global buffer for full
    data reuse. The data layout used in our implementation is NWHC8c (aligned to 8
    channels), which can be changed in another design. <sup>0</sup> and <sup>0</sup>
    are the height and width of an input tile; is the input channel size; is the global
    width-dimension index of the input tensor; and <sup>0</sup> is the width-dimension
    index of an input tile.


    <span id="page-5-2"></span>![](_page_5_Figure_9.jpeg)


    Figure 8. Hardware implementation with the buffer region manager in our 12nm NPU
    as a demonstration. The layout is an NPU core extracted from part of our in-house
    chip.


    in hardware. Therefore, we need to partition the whole computation graph into
    a series of subgraphs that fit the memory. Below, we move up to the optimization
    for graph partition and memory design-space exploration for challenge 3.


    #### 4.1 Problem Formulation


    4.1.1 Graph-Level Partition. Formally, a DNN model can be represented as a computation
    graph = ( , ), where is the vertex set consisting of all the layers in a DNN model,
    and is the edge set that defines the structure of DNN. In particular, an edge
    (, ) ∈ represents that the output of layer is an input of layer .


    We aim to find a partition scheme : → N that assigns each layer to a subgraph,
    where layer ∈ is computed in the ()-th subgraph. A valid partition scheme should
    satisfy that any layer is computed before use. Therefore, for any (, ) ∈ , we
    have () ≤ (). Moreover, any subgraph should be connected in , otherwise meaningless.


    We cast the partition exploration as an optimization problem. The objective is
    to find a valid partition scheme that minimizes the total cost:


    <span id="page-5-4"></span>

    $$\sum\_{i} Cost\_{\mathcal{M}}(\{v \in V \mid P(v) = i\}),\tag{1}$$


    where is a cost function of a given subgraph based on a target metric (e.g., external
    memory access (EMA) and energy). For each subgraph, the EMA cost contains the
    loading of weights and input activations and the storage of output activations[3](#page-5-3)
    . The energy cost includes the overhead of EMA, on-chip buffers, and computation
    units.


    4.1.2 Design-Space Exploration (DSE). Our work further extends the optimization
    to combine with the memory design-space exploration. In this paper, we focus on
    the global buffer and the weight buffer, given that they dominate


    <span id="page-5-3"></span><sup>3</sup>The nodes that are required to write-back
    to DRAM can be the model output layer or the layers required by the future subgraph.


    the overhead of energy and area in an NPU core. As illustrated in Figure [1,](#page-1-0)
    a larger buffer capacity can take in more layers inside a subgraph, reducing communication
    costs but compromising the silicon area. To co-explore the hardware configuration
    and mapping, we construct an objective function by a linear combination of the
    hardware and mapping costs:


    <span id="page-6-0"></span>

    $$\text{BUF\\_SIZE} + \alpha \cdot \sum\_{i} Cost\_{\mathcal{M}}(\{v \in V \mid
    P(v) = i\}),\qquad \text{(2)}$$


    where is a preference hyper-parameter to adjust the proportion between two costs.


    #### 4.2 Baseline Methods


    Several optimization methods that exist today can perform graph-level partition.
    However, most of them fail to directly co-explore hardware and partition. Below,
    we list four typical methods as our baselines and sketch their features.


    4.2.1 Enumeration-based Algorithm. Fused-CNN [\[4\]](#page-12-2) applies a straightforward
    way to enumerate all possible partition schemes and return the best one. Jangda
    et al. [\[25\]](#page-13-19) proposed state compression dynamic programming to
    speed up the enumeration-based algorithm. We migrate their methods as our baseline
    and further improve them by only recording one subgraph in the state to reduce
    the time complexity.


    Nonetheless, there are still exponential states in the improved implementation.
    Let be the number of nodes in a graph, and the enumeration-based method may explore
    up to (2 <sup>2</sup> ) states for irregular networks. Consequently, the search
    is hard to complete within a reasonable search time for large-scale networks,
    not to mention the co-exploration with DSE.


    4.2.2 Greedy Algorithm. Halide [\[47\]](#page-14-21) employs a greedy algorithm
    to perform function grouping, which can be applied to the graph-level partition.
    Specifically, it first assigns each layer into a single-layer subgraph. Then it
    iteratively fuses a pair of subgraphs contributing the greatest benefit until
    all benefits are negative.


    Therefore, this algorithm tends to be trapped at the local minimum. Moreover,
    since the fusion decision rules are based on a given hardware, the greedy method
    cannot co-explore with DSE.


    4.2.3 Dynamic Programming (DP)-based Algorithm. For the irregular network scheduling
    problem, Zheng et al. [\[73\]](#page-15-3) proposed a DP-based algorithm. They
    arrange the layers based on their depth and perform DP in a sequential manner.


    This method is restricted to assigning layers that are contiguous in the depth
    order into a subgraph, hence the exploration is confined to constrained search
    space. It is unlikely to find the global optimum, especially for non-plain network


    structures. In addition, since the state transition of DP depends on the predefined
    buffer size, it is also tough to carry out co-exploration.


    4.2.4 Simulated Annealing (SA). SA [\[33\]](#page-13-20) is a popular optimization
    algorithm that samples a point and updates it iteratively to improve. It adopts
    the new sample points with a probability affected by the performance difference
    and a hyper-parameter named temperature. We employ the customized mutation operations
    (described in Section [4.4.3\)](#page-7-0) to update the sample points and implement
    an SA-based algorithm as a baseline.


    SA is an alternative optimization method for our framework with compatible operators,
    but it is not stable as the genetic algorithm in a range of benchmarks, which
    will be shown in later experiments.


    #### 4.3 Genetic Algorithm


    Previous research shows competitive performance of the Genetic Algorithm (GA)
    in several scheduling optimization problems [\[30,](#page-13-18) [31\]](#page-13-21).
    We summarize several benefits of GA for our hardware-mapping co-exploration problem:


    - 1. White-box property: We can track and tune its optimization process conveniently.
    Therefore, it is easy and intuitive to understand.

    - 2. Complete search space: It has the potential to explore the complete search
    space by customized mutation and crossover operations.

    - 3. Avoid local optima: In contrast to the greedy algorithm, GA can naturally
    jump out of the local minimum benefiting from the diversity of the population.

    - 4. Flexible initialization: We can use the results of other optimization algorithms
    to initialize GA and use GA to finetune the result.

    - 5. Co-exploration: Through the proposed GA operations and genome encoding, it
    can further support partition-DSE co-exploration.


    We encode each candidate solution (partition scheme and the corresponding memory
    configuration for our problem) as a genome, and the population contains a set
    of genomes. The GA goes through a series of generations to obtain a lower cost.
    It performs the crossover and mutation operations on the population in each generation.
    Specifically, a crossover operation blends two genomes selected from the population
    to generate one offspring while a mutation operation modifies a genome randomly.
    At the end of each generation, the evaluation environment evaluates the fitness
    of each genome, and the population in the new generation is selected based on
    the fitness results.


    #### 4.4 Cocco Optimization Framework


    Cocco is a GA-based optimization framework that enables the co-exploration of
    memory configuration and graph-level partition, as shown in Figure [10.](#page-7-1)
    The core of Cocco is a


    <span id="page-7-2"></span>![](_page_7_Figure_2.jpeg)


    Figure 9. Illustration of crossover and mutation operations in Cocco.


    series of operations that explore a complete search space. We build a genetic
    algorithm based on these customized operations. Fed with the neural network structure
    and DSE requirements, Cocco goes through several steps to get the optimization
    results. The execution model described in Section [3](#page-3-1) is embedded in
    the evaluation environment. In the following, we introduce the five stages of
    Cocco.


    4.4.1 Initialization. The first step in Cocco is to generate the initial population,
    where each genome contains a partition scheme of the computation graph and a memory
    configuration for DSE. For the DSE part, every genome selects a capacity value
    in a given range following a uniform distribution. There are two options in Cocco
    to initialize the partition scheme of each genome. The first option is random
    initialization. Precisely, we determine the () for each layer ∈ in topological
    order, and each () is selected randomly within the valid range. The other option
    is to initialize the partition scheme from other optimization algorithms.


    4.4.2 Crossover. We designed a customized crossover operation to inherit and blend
    the features of two parents selected from the population. Specifically, each hardware
    configuration (i.e., memory capacity) in the offspring is the average of its parents
    and then rounds to the nearest candidate value. For the partition scheme, we assign
    layers to subgraphs in topological order. Each undecided layer chooses one parent
    randomly to reproduce the corresponding subgraph. If the reproduced subgraph contains
    layers that have been decided, we split out a new one excluding those layers,
    or merge it with one of the subgraphs to which the decided layers belong.


    As shown in Figure [9\(](#page-7-2)b), layer 1 and layer 3 select Dad as the parent
    to reproduce the subgraphs {1, 2} and {3, 4}, respectively. Next, layer 5 selects
    Mom as its parent, so it


    <span id="page-7-1"></span>![](_page_7_Figure_8.jpeg)


    Figure 10. Cocco framework overview.


    intends to reproduce subgraph {4, 5, 6}. However, since we have already decided
    on layer 4 in subgraph {3, 4}, there are two alternatives: creating a new subgraph
    {5, 6} (Child-1) or merging with subgraph {3, 4} to obtain {3, 4, 5, 6} (Child-2).


    <span id="page-7-0"></span>4.4.3 Mutation. Four mutation operations are customized
    for the optimization flow to explore the search space extensively. We guarantee
    the validity of genomes after each mutation in the implementation. At the bottom
    of Figure [9,](#page-7-2) we show a node-level operation (modify-node) and two
    subgraph-level ones (split-subgraph and merge-subgraph):


    - modify-node (Figure [9\(](#page-7-2)c)): Modify the assignment of a randomly
    selected node : from → () to → ′ (), where ′ () can be an existed subgraph or
    a new one.

    - split-subgraph (Figure [9\(](#page-7-2)d)): Split a randomly selected subgraph
    into two or more subgraphs.

    - merge-subgraph (Figure [9\(](#page-7-2)e)): Merge two randomly selected subgraphs
    into one subgraph.

    - mutation-DSE (not shown): Modify the memory configuration to a random one within
    the range. The new values are sampled based on a normal distribution, where the
    average is the original value, and the variance is a hyper-parameter.


    4.4.4 Evaluation. Since GA tries to maximize the fitness of the genomes, we set
    fitness to be the opposite of the cost (e.g., Formula [1](#page-5-4) and [2\)](#page-6-0).
    To evaluate the fitness of each genome in the population, we use our simulator
    (introduced in the next section) to extract the execution costs of subgraphs (e.g.,
    EMA and energy).


    During the evaluation, the simulator decodes the subgraph and hardware configuration
    of each genome and calculates the fitness by aggregating the cost of each subgraph.
    Particularly, when a large subgraph exceeds the buffer capacity, we perform the
    split-subgraph operation to ensure genome validity. This kind of in-situ tuning
    can increase the number of valid samples during the optimization operations and
    thus, improve the sample efficiency.


    4.4.5 Selection. At the end of each generation, Cocco performs the tournament
    selection. Specifically, it holds multiple tournaments among a few randomly selected
    genomes, and the winners (the genome with the best fitness) of these tournaments
    form the population of a new generation. This operation facilitates superior fitness
    in the new generation. The number of genomes in each tournament is decided by
    a hyper-parameter. The new generation subsequently starts from the crossover step
    again.


    ## 5 Experiments


    In the evaluations, we first present the superiority of Cocco for the graph partition;
    and then demonstrate its outstanding stability and sample efficiency of the co-exploration
    for the hardware optimization, followed by additional discussions about the results
    under different configurations.


    #### 5.1 Methodology


    5.1.1 Evaluated Models. In the following evaluations, we consider three types
    of model structures: plain (VGG16 [\[57\]](#page-14-2)), multi-branch (ResNet50,
    ResNet152 [\[20\]](#page-13-4), GoogleNet [\[59\]](#page-14-3), Transformer [\[64\]](#page-14-22),
    and GPT [\[52\]](#page-14-23)), and irregular structure (RandWire-A/B [\[68\]](#page-14-6)
    and NasNet [\[75\]](#page-15-1)). RandWire-A/B are


    generated based on the small and regular regime configurations introduced in the
    paper [\[68\]](#page-14-6). FC layers are transformed to 1×1 CONV while pooling
    and element-wise layers are analyzed as depth-wise CONV without weights. The scalar
    operations (e.g., activation function) are hidden in the pipeline (e.g., the post-process
    module following PE in Simba [\[56\]](#page-14-20)) and their overhead can be
    ignored.


    5.1.2 Accelerator Platform. As shown at the top of Figure [10,](#page-7-1) we
    consider a SIMBA-like hierarchical accelerator with a global buffer, a weight
    buffer, and a 4×4 PE array in each core used in several previous works [\[56,](#page-14-20)
    [61,](#page-14-9) [71\]](#page-15-6). Each PE contains an 8×8 MAC array to process
    a sub-tile from the global buffer. In particular, we model the execution flow
    based on the scheme described in Section [3.](#page-3-1) The parallelism of two
    dimensions of the PE array can be dynamically configured by the mapper results
    to ensure high utilization. We schedule subgraphs in topological order and prefetch
    weights of the next subgraph during the current computing. We also extend our
    platform to support fundamental multi-core studies by interconnecting cores with
    a crossbar. They share weights to release the burden of each core.


    The arithmetic and memory overhead is extracted in a 12nm library based on the
    synthesized RTL implementations (SRAM based on the ARM memory compiler) with 1GHz.
    The DRAM energy is set as 12.5pJ/bit [\[70\]](#page-15-2). The extra footprint
    of the plug-in design is mainly a 272-Byte register file to store the head and
    end logical region addresses of maximal 64 nodes, which is negligible. Based on
    off-the-shelf evaluators Timeloop [\[50\]](#page-14-18) and MAESTRO [\[37\]](#page-13-7)
    for spatial accelerators, we developed a modified simulator that supports the
    evaluation of latency and energy. It employs the consumption-centric scheme to
    determine the tile size of each layer, and the memory access in the model is free
    from padding data. The latency per subgraph depends on the maximum of the calculation
    and external communication cycles. We allocate 16GB/s DRAM bandwidth per accelerator
    core for loading weights and input activations and writing back data for subsequent
    subgraphs. The off-chip communication consists of weight loading of each layer
    and the inputs and outputs of each subgraph. As described in Section [3,](#page-3-1)
    our subgraph execution scheme avoids recomputing of intermediate outputs.


    5.1.3 Baselines. Three optimization baselines for graph partition are the greedy
    algorithm used in Halide [\[47\]](#page-14-21), dynamic programming (DP) used
    in Irregular-NN [\[73\]](#page-15-3) , and the enumeration-based method as a reference.


    For the DSE studies, we compare Cocco with simulated annealing (SA) [\[33\]](#page-13-20)
    to demonstrate the better stability of GA. These two methods are both the co-optimization
    scheme that optimizes partition and hardware settings at the same time. In contrast
    to co-optimization, the two-step scheme is another method for design-space exploration.
    Specifically, we


    <span id="page-9-0"></span>![](_page_9_Figure_2.jpeg)


    Figure 11. The evaluation results for graph partition using the EMA-opt configuration
    (EMA as the optimization metric). The enumeration-based method is deterministic,
    which figures out the optimal solution as a reference in the first four models.
    It cannot complete for large-scale models (Transformer, GPT, RandWire-A, and RandWire-B)
    in a reasonable time because of the exponential search space.


    use random search (RS) or grid search (GS) to sample memory capacity candidates
    and then explore the corresponding partition schemes. During the search, we evaluate
    5,000 samples for each capacity candidate and keep the best candidate as the output.
    As for the sampling method, RS randomly samples memory capacity candidates while
    GS uses a coarser granularity to enumerate the candidates.


    #### 5.2 Graph Partition Evaluations


    We start by presenting the partition performance on the single-core hardware with
    a 1MB global buffer and a 1.125MB weight buffer. The number of samples in Cocco
    is set to be 400,000. We evaluate the external memory access (EMA) and bandwidth
    requirements of eight models shown in Figure [11,](#page-9-0) where the results
    are normalized to the Halide baseline. This experiment aims to validate the effectiveness
    of our Cocco framework in graph partition. For networks with simpler structures,
    Cocco can find out the optimal solutions same as the enumeration-based results.
    For large-scale irregular networks (Transformer, GPT, RandWire-A, and RandWire-B),
    the enumeration-based method cannot complete in a reasonable time, while Cocco
    provides better solutions than Halide and DP. A better subgraph partition strategy
    helps to ease the communication burden, thus reducing the EMA cost and bandwidth
    requirements.


    #### 5.3 Hardware-Mapping Co-Exploration


    After learning the superiority of Cocco for the graph partition, we further co-explore
    the memory configuration and graph partition mapping as the core study of this
    work. Three categories of exploration methods are used, including the fixed hardware
    scheme, the two-step scheme as baselines, and the proposed co-optimization scheme.
    We set three fixed memory configurations with Small capacity, Medium capacity,
    and Large capacity, followed by a partition-only procedure. The two-step scheme
    is implemented with decoupled steps for capacity search (RS or GS) and partition
    (GA). The cooptimization methods include the proposed Cocco and an SA-based one
    as the comparison. All methods sample up to


    <span id="page-9-2"></span>Table 1. Hardware-mapping co-exploration for separate
    buffer. In this table, A refers to the global buffer, and W refers to the weight
    buffer. We evaluate the cost using Formula [2](#page-6-0) (the lower cost, the
    better), where the metric is energy. We use RandWire-A as RandWire in the following
    experiments.


    | Optimization |        | ResNet50      |                   |        | GoogleNet
    |                   |        |  |

    |--------------|--------|---------------|-------------------|--------|-----------|-------------------|--------|--|

    |              |        |               | Size (A) Size (W) | Cost   |           |
    Size (A) Size (W) | Cost   |  |

    |              | Buf(S) | 512KB         | 576KB             | 1.04E7 | 512KB     |
    576KB             | 4.07E6 |  |

    | Fixed<br>HW  |        | Buf(M) 1024KB | 1152KB            | 1.07E7 | 1024KB    |
    1152KB            | 5.06E6 |  |

    |              | Buf(L) | 2048KB        | 2304KB            | 1.24E7 | 2048KB    |
    2304KB            | 7.18E6 |  |

    |              | RS+GA  | 448KB         | 864KB             | 1.04E7 | 384KB     |
    432KB             | 3.88E6 |  |

    | Two-Step     | GS+GA  | 128KB         | 864KB             | 1.07E7 | 128KB     |
    144KB             | 3.80E6 |  |

    |              | SA     | 256KB         | 360KB             | 1.06E7 | 192KB     |
    144KB             | 3.78E6 |  |

    | Co-Opt       | Cocco  | 704KB         | 864KB             | 1.04E7 | 192KB     |
    432KB             | 3.75E6 |  |

    |              |        |               |                   |        |           |                   |        |  |

    |              |        |               | RandWire          |        |           |
    NasNet            |        |  |

    | Optimization |        |               | Size (A) Size (W) | Cost   |           |
    Size (A) Size (W) | Cost   |  |

    |              | Buf(S) | 512KB         | 576KB             | 3.23E6 | 512KB     |
    576KB             | 6.14E7 |  |

    | Fixed        |        | Buf(M) 1024KB | 1152KB            | 3.92E6 | 1024KB    |
    1152KB            | 5.83E7 |  |

    | HW           | Buf(L) | 2048KB        | 2304KB            | 6.00E6 | 2048KB    |
    2304KB            | 5.66E7 |  |

    |              | RS+GA  | 448KB         | 792KB             | 3.31E6 | 1152KB    |
    2016KB            | 5.60E7 |  |

    | Two-Step     | GS+GA  | 128KB         | 144KB             | 3.02E6 | 2048KB    |
    2304KB            | 5.66E7 |  |

    | Co-Opt       | SA     | 192KB         | 144KB             | 3.00E6 | 2048KB    |
    1872KB            | 5.61E7 |  |


    50,000 points. The energy-capacity co-optimization is used in the following evaluations.


    5.3.1 DSE analysis using separate and shared buffer. We first perform the hardware-mapping
    co-exploration to determine the suitable memory configuration (except for the
    fixed-HW scheme) with = 0.002[4](#page-9-1) and then solely execute the partition-only
    Cocco to obtain the final cost. In particular, we also compared the results using
    two memory designs: separate buffer and shared buffer. For the separate buffer
    design, activations and weights are stored in different buffers while they share
    the same space in the shared buffer design. The memory capacity candidates for
    the global buffer


    <span id="page-9-1"></span><sup>4</sup>The energy and the capacity units are pJ
    and Byte, respectively.


    Cocco: Hardware-Mapping Co-Exploration towards Memory ... ASPLOS ''24, April 27-May
    1, 2024, La Jolla, CA, USA


    <span id="page-10-0"></span>Table 2. Hardware-mapping co-exploration for shared
    buffer. We evaluate the cost using Formula [2](#page-6-0) (the lower cost, the
    better), where the metric is energy.


    | Optimization |        |        | ResNet50 | GoogleNet |        |  |

    |--------------|--------|--------|----------|-----------|--------|--|

    |              |        | Size   | Cost     | Size      | Cost   |  |

    | Fixed<br>HW  | Buf(S) | 576KB  | 1.01E7   | 576KB     | 3.66E6 |  |

    |              | Buf(M) | 1152KB | 1.00E7   | 1152KB    | 4.04E6 |  |

    |              | Buf(L) | 2304KB | 1.04E7   | 2304KB    | 5.09E6 |  |

    |              | RS+GA  | 1280KB | 0.98E7   | 640KB     | 3.65E6 |  |

    | Two-Step     | GS+GA  | 1344KB | 0.98E7   | 512KB     | 3.65E6 |  |

    |              | SA     | 896KB  | 1.00E7   | 192KB     | 3.75E6 |  |

    | Co-Opt       | Cocco  | 1344KB | 0.98E7   | 384KB     | 3.60E6 |  |

    |              |        |        |          |           |        |  |

    |              |        |        | RandWire | NasNet    |        |  |

    | Optimization |        | Size   | Cost     | Size      | Cost   |  |

    |              | Buf(S) | 576KB  | 2.83E6   | 576KB     | 6.36E7 |  |

    | Fixed        | Buf(M) | 1152KB | 3.03E6   | 1152KB    | 5.73E7 |  |

    | HW           | Buf(L) | 2304KB | 3.90E6   | 2304KB    | 5.51E7 |  |

    |              | RS+GA  | 320KB  | 2.85E6   | 2560KB    | 5.47E7 |  |

    | Two-Step     | GS+GA  | 832KB  | 2.86E6   | 3072KB    | 5.42E7 |  |

    | Co-Opt       | SA     | 256KB  | 2.92E6   | 1728KB    | 5.56E7 |  |


    (for activations) range from 128KB to 2048KB with a 64KB interval, while that
    for the weight buffer range from 144KB to 2304KB with a 72KB interval. The exploration
    range of the shared buffer is from 128KB to 3072KB with an interval of 64KB.


    The evaluation using separate buffers is shown in Table [1,](#page-9-2) where
    Cocco achieves better optimization with up to 1.89% (compared to SA in ResNet50)
    to 50.33% (compared to Fixed-HW(L) in RandWire) lower cost compared to various
    baselines across four models. The two-step scheme fails to combine the information
    between different sizes, so it is generally worse than the co-optimization method.


    The capacity results also reflect the inherent capacity preference of different
    models. The data amount in GoogleNet and RandWire is relatively smaller, and thus
    their capacity requirements are lower. In contrast, the data amount in NasNet
    is larger, so a high capacity is preferred.


    As shown in Table [2,](#page-10-0) the evaluation of the shared buffer setting
    shows a similar trend. Furthermore, we can observe that most of the cost results
    of the shared buffer are lower than those using the separate configuration. Although
    the shared buffer design requires additional control flows, it indeed improves
    efficiency than the separate buffer design.


    5.3.2 Sample efficiency analysis. We next study the sample efficiency of the two-step
    and the co-optimization scheme in Figure [12.](#page-10-1) We record the cost
    trends of the first 50,000 samples on ResNet50, GoogleNet, and RandWire during
    the exploration. Overall, Cocco shows a consistent convergence trend on these
    three networks. And it converges faster and


    <span id="page-10-1"></span>![](_page_10_Figure_9.jpeg)


    Figure 12. The convergence curve of Cocco compared with other baselines in the
    hardware-mapping co-explorations. The optimization method requiring fewer samples
    in (d) has higher sample efficiency.


    <span id="page-10-2"></span>![](_page_10_Figure_11.jpeg)


    Figure 13. The visualization of sample points distribution during optimization.
    The slope of the red dashed line denotes the preference between energy and capacity
    cost. The point on the line with a lower intercept has a smaller cost.


    achieves lower costs compared to other baselines, exhibiting a higher sample efficiency.
    The two-step methods perform graph-partition separately under different capacities,
    so they fail to utilize the partition information between capacities. Particularly,
    the GS method uses a deterministic search direction (search from large to small
    capacity in this experiment), so the convergence time depends on the optimal capacity.
    Since GoogleNet and RandWire require relatively small buffers, GS takes a considerable
    number of samples to converge.


    5.3.3 Optimization procedure analysis. We next study how the distribution of sample
    points changes during the optimization procedure of Cocco. While searching for
    20 generations with 500 genomes each, we divided them into ten groups with different
    colors in Figure [13.](#page-10-2) The results show that the distribution moves
    towards a lower intercept


    <span id="page-11-0"></span>![](_page_11_Figure_1.jpeg)


    Figure 14. The trade-off between energy and memory capacity. The optimization
    target is to minimize the cost defined in Formula [2,](#page-6-0) where the metric
    is energy. Energy results of each model are normalized to the first (= 0.0005)
    results.


    <span id="page-11-1"></span>Table 3. Multi-core and batch evaluation using the
    energycapacity co-opt configuration. Size denotes the shared buffer size in each
    core.


    | Core# Batch |             |            | ResNet50     |            | GoogleNet           |                 |              |  |

    |-------------|-------------|------------|--------------|------------|---------------------|-----------------|--------------|--|

    |             |             | Energy(mJ) | Lat.(ms)     |            | Size(KB)
    Energy(mJ) | Lat.(ms)        | Size(KB)     |  |

    | 1           | 1           | 4.21       | 4.59         | 1344       | 1.61                |
    2.05            | 384          |  |

    |             | 2           | 6.32       | 8.98         | 1728       | 2.18                |
    3.91            | 896          |  |

    |             | 8           | 11.88      | 35.93        | 2880       | 5.64                |
    15.53           | 1472         |  |

    |             | 1           | 4.38       | 2.48         | 768        | 1.66                |
    1.04            | 192          |  |

    | 2           | 2           | 6.46       | 4.78         | 1088       | 2.34                |
    1.99            | 384          |  |

    |             | 8           | 13.01      | 19.12        | 1664       | 5.84                |
    7.97            | 960          |  |

    |             | 1           | 4.29       | 1.39         | 448        | 1.34                |
    0.54            | 192          |  |

    | 4           | 2           | 6.58       | 2.68         | 640        | 2.20                |
    1.07            | 192          |  |

    |             | 8           | 11.50      | 10.71        | 1664       | 6.24                |
    4.30            | 448          |  |

    |             |             |            |              |            |                     |                 |              |  |

    |             |             |            | RandWire     |            |                     |
    NasNet          |              |  |

    |             | Core# Batch | Energy(mJ) | Lat.(ms)     |            | Size(KB)
    Energy(mJ) | Lat.(ms)        | Size(KB)     |  |

    |             | 1           | 1.26       | 1.47         | 384        | 28.57               |
    49.92           | 2624         |  |

    | 1           | 2           | 2.25       | 2.74         | 704        | 47.68               |
    99.87           | 3072         |  |

    |             | 8           | 8.66       | 10.85        | 1664       | 133.03              |
    396.90          | 3072         |  |

    |             | 1           | 1.41       | 0.95         | 192        | 29.18               |
    24.93           | 1728         |  |

    | 2           | 2           | 2.37       | 1.80         | 384        | 48.80               |
    49.73           | 2624         |  |

    |             | 8           | 8.39       | 7.16         | 1280       | 153.25              |
    227.19          | 3072         |  |

    |             | 1           | 1.39       | 0.71         | 192        | 28.00               |
    14.56           | 960          |  |

    | 4           | 2           | 2.91       | 1.40<br>5.55 | 192<br>960 | 45.03               |
    28.58<br>133.38 | 1664<br>2816 |  |


    of the -slope line and gets more centralized in the later generations during the
    optimization process of Cocco.


    #### 5.4 Sensitivity Study about Cocco framework


    5.4.1 Study of in the cost function. The results shown in Figure [14](#page-11-0)
    demonstrate the effectiveness of in adjusting the preference between the memory
    capacity and the given metric (energy is used here). The optimization trades the
    memory capacity for lower energy cost with the increase of . In addition, a larger
    memory capacity indeed contributes to lower energy, but the yields show differences
    because of their various model-inherent graph and layer patterns. For example,
    NasNet is more memory-intensive and more structure-complex than the other three
    models, so it requires a larger memory capacity for less energy consumption.


    5.4.2 Study of performance v.s. memory capacity. Figure [2](#page-2-0) shows that
    the increase of capacity is sub-linear with


    performance. To study this observation, we scale our model to the multi-core version
    and share weights of a subgraph across cores. Different cores only buffer a subset
    of weights and transfer the data between cores, similar to BSD in Tangram [\[18\]](#page-13-11)
    or data-rotation in NN-Baton [\[61\]](#page-14-9). The overhead of the interconnection
    crossbar is extracted from the implemented Arteries IP [\[5\]](#page-12-9).


    An accelerator with more cores can cover a larger subgraph but bring more core-to-core
    overhead. As shown in Table [3,](#page-11-1) in most cases, energy increases from
    the single-core to dual-core configuration because of the communication overhead.
    Moreover, profiting from the data-sharing mechanism, the required memory of each
    core drops with the increase of core number.


    5.4.3 Batch size study. For the batch size evaluation shown in Table [3,](#page-11-1)
    the latency with a larger batch size principally presents a sub-linear increase,
    which benefits from the lower bandwidth requirement of weights via the inter-sample
    data reuse. In addition, such data reuse amortizes the energy burden per batch
    processing. And owing to the better weight reuse in multi-batch processing, a
    larger batch size does not require a proportional capacity.


    ## 6 Related Works


    #### 6.1 Intra-layer Optimization


    Prior works focus on the data reuse for intra-layer assignments, like output-stationary
    in ShiDianNao [\[14\]](#page-13-22) and Envision [\[46\]](#page-14-24), weight-stationary
    in NeuFlow [\[15\]](#page-13-23) and Nvdla [\[49\]](#page-14-16), input-stationary
    in SCNN [\[51\]](#page-14-25), and row-stationary in Eyeriss [\[13\]](#page-12-10).
    Based on these primitive dataflow patterns, extensive studies explored the optimal
    tiling and reordering schemes via brute-force, feedback-based, and constraint
    optimization approaches [\[23,](#page-13-5) [30,](#page-13-18) [50\]](#page-14-18).
    These works focus on layer-level optimization, missing the graph information at
    a higher level. The efficiency of tile updates depends on the memory architecture.
    Simba [\[56,](#page-14-20) [74\]](#page-15-5) and NN-Baton [\[61\]](#page-14-9)
    view each tile as an independent workload so that the tile size has a prominent
    impact on memory access due to halo regions. Motivated by traditional vision processors,
    Ascend [\[40\]](#page-13-10) and DRQ [\[58\]](#page-14-26) employ line buffers
    to achieve data reuse in the row direction, but the line buffer cannot well support
    the 2D-tiling reuse in both row and column directions.


    #### 6.2 Inter-layer Optimization


    Intra-layer scheduling is sub-optimal, which is limited by the data reuse within
    a layer. Therefore, Fused-CNN [\[4\]](#page-12-2), SR-CNN [\[38\]](#page-13-8),
    and LCP [\[42\]](#page-14-19) introduce layer fusion method that cache intermediate
    data on-chip to reduce data transfer overhead using handcrafted or heuristic methods
    for fusion partition. Although Irregular-NN [\[73\]](#page-15-3) suggests a customized-DP
    algorithm, the exploration space is constrained because the layers in an assignment
    need to be successive in a specific


    order. A recent work named DNNFuser [\[29\]](#page-13-24) employs an RLbased method,
    but their formulation towards 1D layer-fusion is hard to handle complex irregular
    networks. Tangram [\[18\]](#page-13-11) and Atomic [\[72\]](#page-15-4) schedule
    DNN workloads on a multi-core (scalable) accelerator, but they focus on executing
    a single layer on each core at a time rather than processing multiple layers with
    local data reuse. Also, some previous works [\[2,](#page-12-11) [19,](#page-13-25)
    [62\]](#page-14-27) tackle the workload placement problem for multiple devides
    without discussing the downstream execution on each device.


    Cocco proposes an automatic framework for inter-layer scheduling with a comprehensive
    memory scheme. It focuses on the fundamental core-level temporal execution that
    can be potentially scaled up to the multi-core or multi-device scenario with a
    spatial parallelism mechanism.


    #### 6.3 Design-Space Exploration for Memory


    Memory design exploration methods lie primarily on two sides: analysis-driven
    and search-driven. For the analysisdriven method, Chen et al. [\[12\]](#page-12-0)
    leverage red-blue pebble models to derive the proper memory capacity representations.
    Subsequently, Cai et al. [\[9\]](#page-12-12) propose Olympus, which generalizes
    a framework to a batch of successive layers and also fills up with more scheduling
    and data reuse techniques. However, they are difficult to represent a subgraph
    with complex inter-layer connections. As for the search-driven method, Xiao et
    al. [\[67\]](#page-14-11), Kwon et al. [\[37\]](#page-13-7), and Feng et al. [\[16\]](#page-13-26)
    explore the memory configuration for the layer-level assignment using the brute-force
    search, while Kao et al. [\[32\]](#page-13-6) employ a genetic algorithm to improve
    the efficiency. These works principally focus on the layer-level information,
    while in comparison, Cocco exploits graph-level features for the better optimization.


    ## 7 Conclusion


    While layer-level scheduling is widely studied to improve memory efficiency, graph-level
    optimization remains relatively unexplored. This paper proposed a graph-level
    dataflow with the corresponding memory management scheme that enables flexible
    graph partitions with high memory utilization. On top of it, we propose Cocco,
    a framework to provide a recommended memory configuration with graph-level scheduling
    strategies. Cocco shows outstanding graph partition ability compared to the greedy
    algorithm and DP employed in previous works and enables efficient graph-level
    hardware-mapping co-exploration. This paper helps to provide an implementation
    philosophy for the accelerator memory and better deployment for it.


    ## Acknowledgments


    This research was partially supported by National Key R&D Program of China (2022YFB2804103),
    Tsinghua University Dushi Program, and Tsinghua University Talent Program. We
    would like to appreciate all the anonymous reviewers for their valuable feedback.


    ## References


    - <span id="page-12-5"></span>[1] Dennis Abts, Jonathan Ross, Jonathan Sparling,
    Mark Wong-VanHaren, Max Baker, Tom Hawkins, Andrew Bell, John Thompson, Temesghen
    Kahsai, Garrin Kimmell, Jennifer Hwang, Rebekah Leslie-Hurd, Michael Bye, E. R.
    Creswick, Matthew Boyd, Mahitha Venigalla, Evan Laforge, Jon Purdy, Purushotham
    Kamath, Dinesh Maheshwari, Michael Beidler, Geert Rosseel, Omar Ahmad, Gleb Gagarin,
    Richard Czekalski, Ashay Rane, Sahil Parmar, Jeff Werner, Jim Sproch, Adrian Macias,
    and Brian Kurtz. 2020. Think Fast: A Tensor Streaming Processor (TSP) for Accelerating
    Deep Learning Workloads. In Proceedings of the 47th ACM/IEEE Annual International
    Symposium on Computer Architecture (ISCA). IEEE, Valencia, Spain, 145–158.

    - <span id="page-12-11"></span>[2] Ravichandra Addanki, Shaileshh Bojja Venkatakrishnan,
    Shreyan Gupta, Hongzi Mao, and Mohammad Alizadeh. 2019. Learning Generalizable
    Device Placement Algorithms for Distributed Machine Learning. In Advances in Neural
    Information Processing Systems (NeurIPS), Hanna M. Wallach, Hugo Larochelle, Alina
    Beygelzimer, Florence d''Alché-Buc, Emily B. Fox, and Roman Garnett (Eds.). OpenReview.net,
    Vancouver, BC, Canada, 3983–3993.

    - <span id="page-12-1"></span>[3] Byung Hoon Ahn, Jinwon Lee, Jamie Menjay Lin,
    Hsin-Pai Cheng, Jilei Hou, and Hadi Esmaeilzadeh. 2020. Ordering Chaos: Memory-Aware
    Scheduling of Irregularly Wired Neural Networks for Edge Devices. In Proceedings
    of Machine Learning and Systems (MLSys), Inderjit S. Dhillon, Dimitris S. Papailiopoulos,
    and Vivienne Sze (Eds.). mlsys.org, Austin, TX, USA, 1–14.

    - <span id="page-12-2"></span>[4] Manoj Alwani, Han Chen, Michael Ferdman, and
    Peter A. Milder. 2016. Fused-layer CNN accelerators. In Proceedings of the 49th
    IEEE/ACM International Symposium on Microarchitecture (MICRO). IEEE Computer Society,
    Taipei, Taiwan, 22:1–22:12.

    - <span id="page-12-9"></span>[5] Arteries. 2022. Arteries IP Homepage. <https://www.arteris.com>.

    - <span id="page-12-3"></span>[6] Ljubisa Bajic and Jasmina Vasiljevic. 2020.
    Compute substrate for Software 2.0. In Proceedings of the IEEE Hot Chips 32 Symposium
    (HCS). IEEE, Palo Alto, CA, USA, 1–31.

    - <span id="page-12-6"></span>[7] Pete Bannon, Ganesh Venkataramanan, Debjit Das
    Sarma, and Emil Talpes. 2019. Computer and Redundancy Solution for the Full Self-Driving
    Computer. In Proceedings of the IEEE Hot Chips 31 Symposium (HCS). IEEE, Cupertino,
    CA, USA, 1–22.

    - <span id="page-12-7"></span>[8] John Burgess. 2019. RTX ON - The NVIDIA TURING
    GPU. In Proceedings of the IEEE Hot Chips 31 Symposium (HCS). IEEE, Cupertino,
    CA, USA, 1–27.

    - <span id="page-12-12"></span>[9] Xuyi Cai, Ying Wang, Kaijie Tu, Chengsi Gao,
    and Lei Zhang. 2022. Olympus: Reaching Memory-Optimality on DNN Processors. IEEE
    Transactions on Computers (TC) 71, 8 (2022), 1939–1951.

    - <span id="page-12-8"></span>[10] Prasanth Chatarasi, Hyoukjun Kwon, Angshuman
    Parashar, Michael Pellauer, Tushar Krishna, and Vivek Sarkar. 2022. Marvel: A
    Data-Centric Approach for Mapping Deep Learning Operators on Spatial Accelerators.
    ACM Transactions on Architecture and Code Optimization 19, 1 (2022), 6:1–6:26.

    - <span id="page-12-4"></span>[11] Karam Chatha. 2021. Qualcomm® Cloud Al-100:
    12TOPS/W Scalable, High Performance and Low Latency Deep Learning Inference Accelerator.
    In Proceedings of the IEEE Hot Chips 33 Symposium (HCS). IEEE, Palo Alto, CA,
    USA, 1–19.

    - <span id="page-12-0"></span>[12] Xiaoming Chen, Yinhe Han, and Yu Wang. 2020.
    Communication Lower Bound in Convolution Accelerators. In Proceedings of the IEEE
    International Symposium on High Performance Computer Architecture (HPCA). IEEE,
    San Diego, CA, USA, 529–541.

    - <span id="page-12-10"></span>[13] Yu-Hsin Chen, Joel S. Emer, and Vivienne Sze.
    2016. Eyeriss: A Spatial Architecture for Energy-Efficient Dataflow for Convolutional
    Neural Networks. In Proceedings of the ACM/IEEE Annual International Symposium
    on Computer Architecture (ISCA). IEEE Computer Society, Seoul,


    South Korea, 367–379.


    - <span id="page-13-22"></span>[14] Zidong Du, Robert Fasthuber, Tianshi Chen,
    Paolo Ienne, Ling Li, Tao Luo, Xiaobing Feng, Yunji Chen, and Olivier Temam. 2015.
    ShiDianNao: shifting vision processing closer to the sensor. In Proceedings of
    the ACM/IEEE Annual International Symposium on Computer Architecture (ISCA). ACM,
    Portland, OR, USA, 92–104.

    - <span id="page-13-23"></span>[15] Clément Farabet, Berin Martini, B. Corda,
    Polina Akselrod, Eugenio Culurciello, and Yann LeCun. 2011. NeuFlow: A runtime
    reconfigurable dataflow processor for vision. In Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition (CVPR) Workshops. IEEE Computer Society,
    Colorado Springs, CO, USA, 109–116.

    - <span id="page-13-26"></span>[16] Kaijie Feng, Xiaoya Fan, Jianfeng An, Xiping
    Wang, Kaiyue Di, Jiangfei Li, Minghao Lu, and Chuxi Li. 2021. ERDSE: efficient
    reinforcement learning based design space exploration method for CNN accelerator
    on resource limited platform. Graphics and Visual Computing 4 (2021), 1–11.

    - <span id="page-13-0"></span>[17] Ken-ichi Funahashi. 1989. On the approximate
    realization of continuous mappings by neural networks. Neural Networks 2, 3 (1989),
    183–192.

    - <span id="page-13-11"></span>[18] Mingyu Gao, Xuan Yang, Jing Pu, Mark Horowitz,
    and Christos Kozyrakis. 2019. TANGRAM: Optimized Coarse-Grained Dataflow for Scalable
    NN Accelerators. In Proceedings of the International Conference on Architectural
    Support for Programming Languages and Operating Systems (ASPLOS). ACM, Providence,
    RI, USA, 807–820.

    - <span id="page-13-25"></span>[19] Yuanxiang Gao, Li Chen, and Baochun Li. 2018.
    Spotlight: Optimizing Device Placement for Training Deep Neural Networks. In Proceedings
    of the 35th International Conference on Machine Learning (ICML) (Proceedings of
    Machine Learning Research, Vol. 80), Jennifer G. Dy and Andreas Krause (Eds.).
    PMLR, Stockholm, Sweden, 1662–1670.

    - <span id="page-13-4"></span>[20] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and
    Sun Jian. 2016. Deep Residual Learning for Image Recognition. In Proceedings of
    the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE Computer
    Society, Las Vegas, NV, USA, 770–778.

    - <span id="page-13-17"></span>[21] Kartik Hegde, Po-An Tsai, Sitao Huang, Vikas
    Chandra, Angshuman Parashar, and Christopher W. Fletcher. 2021. Mind mappings:
    enabling efficient algorithm-accelerator mapping space search. In Proceedings
    of the 26th ACM International Conference on Architectural Support for Programming
    Languages and Operating Systems (ASPLOS), Tim Sherwood, Emery D. Berger, and Christos
    Kozyrakis (Eds.). ACM, Virtual Event, USA, 943–958.

    - <span id="page-13-1"></span>[22] Kurt Hornik, Maxwell B. Stinchcombe, and Halbert
    White. 1989. Multilayer feedforward networks are universal approximators. Neural
    Networks 2, 5 (1989), 359–366.

    - <span id="page-13-5"></span>[23] Qijing Huang, Aravind Kalaiah, Minwoo Kang,
    James Demmel, Grace Dinh, John Wawrzynek, Thomas Norell, and Yakun Sophia Shao.
    2021. CoSA: Scheduling by Constrained Optimization for Spatial Accelerators. In
    Proceedings of the ACM/IEEE Annual International Symposium on Computer Architecture
    (ISCA). IEEE, Valencia, Spain, 554–566.

    - <span id="page-13-9"></span>[24] Drago Ignjatovic, Daniel W. Bailey, and Ljubisa
    Bajic. 2022. The Wormhole AI Training Processor. In Proceedings of the IEEE International
    Solid-State Circuits Conference (ISSCC). IEEE, San Francisco, CA, USA, 356–358.

    - <span id="page-13-19"></span>[25] Abhinav Jangda and Uday Bondhugula. 2018.
    An effective fusion and tile size model for optimizing image processing pipelines.
    In Proceedings of the 23rd ACM SIGPLAN Symposium on Principles and Practice of
    Parallel Programming (PPoPP), Andreas Krall and Thomas R. Gross (Eds.). ACM, Vienna,
    Austria, 261–275.

    - <span id="page-13-15"></span>[26] Yang Jiao, Liang Han, Rong Jin, Yi-Jung Su,
    Chiente Ho, Li Yin, Yun Li, Long Chen, Zhen Chen, Lu Liu, Zhuyu He, Yu Yan, Jun
    He, Jun Mao, Xiaotao Zai, Xuejun Wu, Yongquan Zhou, Mingqiu Gu, Guocai Zhu, Rong
    Zhong, Wenyuan Lee, Ping Chen, Yiping Chen, Weiliang Li, Deyu Xiao, Qing Yan,
    Mingyuan Zhuang, Jiejun Chen, Yun Tian, Yingzi Lin, Wei Wu, Hao Li, and Zesheng
    Dou. 2020. A 12nm Programmable Convolution-Efficient Neural-Processing-Unit Chip


    Achieving 825TOPS. In Proceedings of the IEEE International Solid-State Circuits
    Conference (ISSCC). IEEE, San Francisco, CA, USA, 136–140.


    - [27] Yang Jiao, Liang Han, and Xin Long. 2020. Hanguang 800 NPU The Ultimate
    AI Inference Solution for Data Centers. In Proceedings of the IEEE Hot Chips 32
    Symposium (HCS). IEEE, Palo Alto, CA, USA, 1–29.

    - <span id="page-13-16"></span>[28] Norman P. Jouppi, Doe Hyun Yoon, Matthew Ashcraft,
    Mark Gottscho, Thomas B. Jablin, George Kurian, James Laudon, Sheng Li, Peter
    C. Ma, Xiaoyu Ma, Thomas Norrie, Nishant Patil, Sushma Prasad, Cliff Young, Zongwei
    Zhou, and David A. Patterson. 2021. Ten Lessons From Three Generations Shaped
    Google''s TPUv4i : Industrial Product. In Proceedings of the 48th ACM/IEEE Annual
    International Symposium on Computer Architecture (ISCA). IEEE, Valencia, Spain,
    1–14.

    - <span id="page-13-24"></span>[29] Sheng-Chun Kao, Xiaoyu Huang, and Tushar Krishna.
    2022. DNNFuser: Generative Pre-Trained Transformer as a Generalized Mapper for
    Layer Fusion in DNN Accelerators. arXiv preprint arXiv:2201.11218 abs/2201.11218
    (2022), 1–8.

    - <span id="page-13-18"></span>[30] Sheng-Chun Kao and Tushar Krishna. 2020. GAMMA:
    Automating the HW Mapping of DNN Models on Accelerators via Genetic Algorithm.
    In Proceedings of the IEEE/ACM International Conference On Computer Aided Design
    (ICCAD). IEEE, San Diego, CA, USA, 44:1–44:9.

    - <span id="page-13-21"></span>[31] Sheng-Chun Kao and Tushar Krishna. 2022. MAGMA:
    An Optimization Framework for Mapping Multiple DNNs on Multiple Accelerator Cores.
    In IEEE International Symposium on High-Performance Computer Architecture, (HPCA).
    IEEE, Seoul, South Korea, 814–830.

    - <span id="page-13-6"></span>[32] Sheng-Chun Kao, Michael Pellauer, Angshuman
    Parashar, and Tushar Krishna. 2022. DiGamma: Domain-aware Genetic Algorithm for
    HW-Mapping Co-optimization for DNN Accelerators. In Proceedings of the Design,
    Automation & Test in Europe Conference & Exhibition (DATE), Cristiana Bolchini,
    Ingrid Verbauwhede, and Ioana Vatajelu (Eds.). IEEE, Antwerp, Belgium, 232–237.

    - <span id="page-13-20"></span>[33] Scott Kirkpatrick, D. Gelatt Jr., and Mario
    P. Vecchi. 1983. Optimization by Simmulated Annealing. Sci. 220, 4598 (1983),
    671–680.

    - <span id="page-13-12"></span>[34] Simon Knowles. 2017. Scalable Silicon Compute.
    In Workshop on Deep Learning At Supercomputer Scale, NIPS. OpenReview.net, Long
    Beach, CA, USA, 1–22.

    - <span id="page-13-13"></span>[35] Simon Knowles. 2021. Graphcore. In Proceedings
    of the IEEE Hot Chips 33 Symposium (HCS). IEEE, Palo Alto, CA, USA, 1–25.

    - <span id="page-13-3"></span>[36] Alex Krizhevsky, Ilya Sutskever, and Geoffrey
    E. Hinton. 2012. ImageNet Classification with Deep Convolutional Neural Networks.
    In Proceedings of the 26th Annual Conference on Neural Information Processing
    Systems (NIPS). Curran Associates, Inc., Lake Tahoe, Nevada, United States, 1106–1114.

    - <span id="page-13-7"></span>[37] Hyoukjun Kwon, Prasanth Chatarasi, Michael
    Pellauer, Angshuman Parashar, Vivek Sarkar, and Tushar Krishna. 2019. Understanding
    Reuse, Performance, and Hardware Cost of DNN Dataflow: A Data-Centric Approach.
    In Proceedings of the IEEE/ACM International Symposium on Microarchitecture (MICRO).
    ACM, Columbus, OH, USA, 754–768.

    - <span id="page-13-8"></span>[38] Juhyoung Lee, Dongjoo Shin, Jinsu Lee, Jinmook
    Lee, Sanghoon Kang, and Hoi-Jun Yoo. 2019. A Full HD 60 fps CNN Super Resolution
    Processor with Selective Caching based Layer Fusion for Mobile Devices. In Proceedings
    of the Symposium on VLSI Circuits. IEEE, Kyoto, Japan, 302–303.

    - <span id="page-13-2"></span>[39] Grzegorz Lewicki and Giuseppe Marino. 2004.
    Approximation of functions of finite variation by superpositions of a Sigmoidal
    function. Appl. Math. Lett. 17, 10 (2004), 1147–1152.

    - <span id="page-13-10"></span>[40] Heng Liao, Jiajin Tu, Jing Xia, Hu Liu, Xiping
    Zhou, Honghui Yuan, and Yuxing Hu. 2021. Ascend: a Scalable and Unified Architecture
    for Ubiquitous Deep Neural Network Computing : Industry Track Paper. In Proceedings
    of the IEEE International Symposium on High-Performance Computer Architecture,
    HPCA. IEEE, Seoul, South Korea, 789–801.

    - <span id="page-13-14"></span>[41] Heng Liao, Jiajin Tu, Jing Xia, and Xiping
    Zhou. 2019. DaVinci: A Scalable Architecture for Neural Network Computing. In
    Proceedings


    Cocco: Hardware-Mapping Co-Exploration towards Memory ... ASPLOS ''24, April 27-May
    1, 2024, La Jolla, CA, USA


    of the IEEE Hot Chips 31 Symposium (HCS). IEEE, Cupertino, CA, USA, 1–44.


    - <span id="page-14-19"></span>[42] Xinhan Lin, Shouyi Yin, Fengbin Tu, Leibo
    Liu, Xiangyu Li, and Shaojun Wei. 2018. LCP: a layer clusters paralleling mapping
    method for accelerating inception and residual networks on FPGA. In Proceedings
    of the 55th Annual Design Automation Conference (DAC). ACM, San Francisco, CA,
    USA, 16:1–16:6.

    - <span id="page-14-7"></span>[43] Wenyan Lu, Guihai Yan, Jiajun Li, Shijun Gong,
    Yinhe Han, and Xiaowei Li. 2017. FlexFlow: A Flexible Dataflow Accelerator Architecture
    for Convolutional Neural Networks. In Proceedings of the IEEE International Symposium
    on High Performance Computer Architecture (HPCA). IEEE Computer Society, Austin,
    TX, USA, 553–564.

    - <span id="page-14-8"></span>[44] Yufei Ma, Yu Cao, Sarma B. K. Vrudhula, and
    Jae-sun Seo. 2017. Optimizing Loop Operation and Dataflow in FPGA Acceleration
    of Deep Convolutional Neural Networks. In Proceedings of the ACM/SIGDA International
    Symposium on Field-Programmable Gate Arrays (FPGA). ACM, Monterey, CA, USA, 45–54.

    - <span id="page-14-0"></span>[45] Marvin Minsky and Seymour Papert. 1987. Perceptrons
    - an introduction to computational geometry. MIT Press, .

    - <span id="page-14-24"></span>[46] Bert Moons, Roel Uytterhoeven, Wim Dehaene,
    and Marian Verhelst. 2017. Envision: A 0.26-to-10TOPS/W subword-parallel dynamicvoltage-accuracy-frequency-scalable
    Convolutional Neural Network processor in 28nm FDSOI. In Proceedings of the IEEE
    International Solid-State Circuits Conference (ISSCC). IEEE, San Francisco, CA,
    USA, 246–247.

    - <span id="page-14-21"></span>[47] Ravi Teja Mullapudi, Andrew Adams, Dillon
    Sharlet, Jonathan Ragan-Kelley, and Kayvon Fatahalian. 2016. Automatically scheduling
    halide image processing pipelines. ACM Trans. Graph. 35, 4 (2016), 83:1– 83:11.

    - <span id="page-14-12"></span>[48] Thomas Norrie, Nishant Patil, Doe Hyun Yoon,
    George Kurian, Sheng Li, James Laudon, Cliff Young, Norman P. Jouppi, and David
    A. Patterson. 2020. Google''s Training Chips Revealed: TPUv2 and TPUv3. In Proceedings
    of the IEEE Hot Chips 32 Symposium (HCS). IEEE, Palo Alto, CA, USA, 1–70.

    - <span id="page-14-16"></span>[49] NVIDIA. 2018. THE NVIDIA DEEP LEARNING ACCELERATOR.
    In Proceedings of the IEEE Hot Chips 30 Symposium (HCS). IEEE, Cupertino, CA,
    USA, 1–18.

    - <span id="page-14-18"></span>[50] Angshuman Parashar, Priyanka Raina, Yakun
    Sophia Shao, Yu-Hsin Chen, Victor A. Ying, Anurag Mukkara, Rangharajan Venkatesan,
    Brucek Khailany, Stephen W. Keckler, and Joel S. Emer. 2019. Timeloop: A Systematic
    Approach to DNN Accelerator Evaluation. In Proceedings of the IEEE International
    Symposium on Performance Analysis of Systems and Software (ISPASS). IEEE, Madison,
    WI, USA, 304–315.

    - <span id="page-14-25"></span>[51] Angshuman Parashar, Minsoo Rhu, Anurag Mukkara,
    Antonio Puglielli, Rangharajan Venkatesan, Brucek Khailany, Joel S. Emer, Stephen
    W. Keckler, and William J. Dally. 2017. SCNN: An Accelerator for Compressed-sparse
    Convolutional Neural Networks. In Proceedings of the 44th Annual International
    Symposium on Computer Architecture (ISCA). ACM, Toronto, ON, Canada, 27–40.

    - <span id="page-14-23"></span>[52] Alec Radford and Karthik Narasimhan. 2018.
    Improving Language Understanding by Generative Pre-Training. In Preprint. OpenAI,
    , 1– 12.

    - <span id="page-14-5"></span>[53] Esteban Real, Alok Aggarwal, Yanping Huang,
    and Quoc V. Le. 2019. Regularized Evolution for Image Classifier Architecture
    Search. In Proceedings of the 33rd Conference on Artificial Intelligence (AAAI).
    AAAI Press, Honolulu, Hawaii, USA, 4780–4789.

    - <span id="page-14-1"></span>[54] Frank Rosenblatt. 1957. The perceptron, a perceiving
    and recognizing automaton Project Para. Cornell Aeronautical Laboratory, .

    - <span id="page-14-4"></span>[55] Mark Sandler, Andrew G. Howard, Menglong Zhu,
    Andrey Zhmoginov, and Liang-Chieh Chen. 2018. MobileNetV2: Inverted Residuals
    and Linear Bottlenecks. In Proceedings of the IEEE Conference on Computer Vision
    and Pattern Recognition (CVPR). Computer Vision Foundation / IEEE Computer Society,
    Salt Lake City, UT, USA, 4510–4520.

    - <span id="page-14-20"></span>[56] Yakun Sophia Shao, Jason Clemons, Rangharajan
    Venkatesan, Brian Zimmer, Matthew Fojtik, Nan Jiang, Ben Keller, Alicia Klinefelter,
    Nathaniel Pinckney, Priyanka Raina, Stephen G. Tell, Yanqing Zhang, William J.
    Dally, Joel Emer, C. Thomas Gray, Brucek Khailany, and Stephen W. Keckler. 2019.
    Simba: Scaling Deep-Learning Inference with Multi-Chip-Module-Based Architecture.
    In Proceedings of the IEEE/ACM International Symposium on Microarchitecture (MICRO).
    ACM, Columbus, OH, USA, 14–27.

    - <span id="page-14-2"></span>[57] Karen Simonyan and Andrew Zisserman. 2015.
    Very Deep Convolutional Networks for Large-Scale Image Recognition. In Proceedings
    of the International Conference on Learning Representations (ICLR). Computational
    and Biological Learning Society, San Diego, CA, USA, 1–14.

    - <span id="page-14-26"></span>[58] Zhuoran Song, Bangqi Fu, Feiyang Wu, Zhaoming
    Jiang, Li Jiang, Naifeng Jing, and Xiaoyao Liang. 2020. DRQ: Dynamic Region-based
    Quantization for Deep Neural Network Acceleration. In Proceedings of the 47th
    ACM/IEEE Annual International Symposium on Computer Architecture (ISCA). IEEE,
    Valencia, Spain, 1010–1021.

    - <span id="page-14-3"></span>[59] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre
    Sermanet, Scott E. Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke,
    and Andrew Rabinovich. 2015. Going deeper with convolutions. In Proceedings of
    the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE Computer
    Society, Boston, MA, USA, 1–9.

    - <span id="page-14-13"></span>[60] Emil Talpes, Douglas Williams, and Debjit
    Das Sarma. 2022. DOJO: The Microarchitecture of Tesla''s Exa-Scale Computer. In
    Proceedings of the IEEE Hot Chips 34 Symposium (HCS). IEEE, Cupertino, CA, USA,
    1–28.

    - <span id="page-14-9"></span>[61] Zhanhong Tan, Hongyu Cai, Runpei Dong, and
    Kaisheng Ma. 2021. NN-Baton: DNN Workload Orchestration and Chiplet Granularity
    Exploration for Multichip Accelerators. In Proceedings of the IEEE Annual International
    Symposium on Computer Architecture (ISCA). IEEE, Valencia, Spain, 1013–1026.

    - <span id="page-14-27"></span>[62] Jakub Tarnawski, Amar Phanishayee, Nikhil
    R. Devanur, Divya Mahajan, and Fanny Nina Paravecino. 2020. Efficient Algorithms
    for Device Placement of DNN Graph Operators. In Advances in Neural Information
    Processing Systems (NeurIPS), Hugo Larochelle, Marc''Aurelio Ranzato, Raia Hadsell,
    Maria-Florina Balcan, and Hsuan-Tien Lin (Eds.). Open-Review.net, Virtual, 1–13.

    - <span id="page-14-14"></span>[63] Tenstorrent. 2021. Grayskull. <https://tenstorrent.com/grayskull/>.

    - <span id="page-14-22"></span>[64] Ashish Vaswani, Noam Shazeer, Niki Parmar,
    Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin.
    2017. Attention is All you Need. In Advances in Neural Information Processing
    Systems (NIPS), Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach,
    Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (Eds.). OpenReview.net, Long
    Beach, CA, USA, 5998–6008.

    - <span id="page-14-17"></span>[65] Ofri Wechsler, Michael Behar, and Bharat Daga.
    2019. Spring Hill (NNP-I 1000) Intel''s Data Center Inference Chip. In Proceedings
    of the IEEE Hot Chips 31 Symposium (HCS). IEEE, Cupertino, CA, USA, 1–12.

    - <span id="page-14-10"></span>[66] Jian Weng, Sihao Liu, Vidushi Dadu, Zhengrong
    Wang, Preyas Shah, and Tony Nowatzki. 2020. DSAGEN: Synthesizing Programmable
    Spatial Accelerators. In Proceedings of the 47th ACM/IEEE Annual International
    Symposium on Computer Architecture (ISCA). IEEE, Valencia, Spain, 268–281.

    - <span id="page-14-11"></span>[67] Qingcheng Xiao, Size Zheng, Bingzhe Wu, Pengcheng
    Xu, Xuehai Qian, and Yun Liang. 2021. HASCO: Towards Agile HArdware and Software
    CO-design for Tensor Computation. In Proceedings of the 48th ACM/IEEE Annual International
    Symposium on Computer Architecture (ISCA). IEEE, Valencia, Spain, 1055–1068.

    - <span id="page-14-6"></span>[68] Saining Xie, Alexander Kirillov, Ross B. Girshick,
    and Kaiming He. 2019. Exploring Randomly Wired Neural Networks for Image Recognition.
    In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV).
    IEEE, Seoul, South Korea, 1284–1293.

    - <span id="page-14-15"></span>[69] Andrew Yang. 2019. Deep Learning Training
    At Scale Spring Crest Deep Learning Accelerator (Intel® Nervana™ NNP-T). In Proceedings
    of the IEEE Hot Chips 31 Symposium (HCS). IEEE, Cupertino, CA, USA,


    <span id="page-15-0"></span>1–20.


    - <span id="page-15-2"></span>[70] Xuan Yang, Mingyu Gao, Qiaoyi Liu, Jeff Setter,
    Jing Pu, Ankita Nayak, Steven Bell, Kaidi Cao, Heonjae Ha, Priyanka Raina, Christos
    Kozyrakis, and Mark Horowitz. 2020. Interstellar: Using Halide''s Scheduling Language
    to Analyze DNN Accelerators. In Proceedings of the International Conference on
    Architectural Support for Programming Languages and Operating Systems (ASPLOS).
    ACM, Lausanne, Switzerland, 369–383.

    - <span id="page-15-6"></span>[71] Size Zheng, Renze Chen, Anjiang Wei, Yicheng
    Jin, Qin Han, Liqiang Lu, Bingyang Wu, Xiuhong Li, Shengen Yan, and Yun Liang.
    2022. AMOS: enabling automatic mapping for tensor computations on spatial accelerators
    with hardware abstraction. In Proceedings of the 49th Annual International Symposium
    on Computer Architecture (ISCA). ACM, New York, New York, USA, 874–887.

    - <span id="page-15-4"></span>[72] Shixuan Zheng, Xianjue Zhang, Leibo Liu, Shaojun
    Wei, and Shouyi Yin. 2022. Atomic Dataflow based Graph-Level Workload Orchestration
    for Scalable DNN Accelerators. In Proceedings of the IEEE International Symposium
    on High-Performance Computer Architecture (HPCA). IEEE, Seoul, South Korea, 475–489.

    - <span id="page-15-3"></span>[73] Shixuan Zheng, Xianjue Zhang, Daoli Ou, Shibin
    Tang, Leibo Liu, Shaojun Wei, and Shouyi Yin. 2020. Efficient Scheduling of Irregular
    Network Structures on CNN Accelerators. IEEE Transactions on Computer-Aided Design
    of Integrated Circuits and Systems (TCAD) 39, 11 (2020), 3408–3419.

    - <span id="page-15-5"></span>[74] Brian Zimmer, Rangharajan Venkatesan, Yakun
    Sophia Shao, Jason Clemons, Matthew Fojtik, Nan Jiang, Ben Keller, Alicia Klinefelter,
    Nathaniel Ross Pinckney, Priyanka Raina, Stephen G. Tell, Yanqing Zhang, William
    J. Dally, Joel S. Emer, C. Thomas Gray, Stephen W. Keckler, and Brucek Khailany.
    2019. A 0.11 pJ/Op, 0.32-128 TOPS, Scalable Multi-Chip-Module-based Deep Neural
    Network Accelerator with Ground-Reference Signaling in 16nm. In Proceedings of
    the IEEE Symposium on VLSI Circuits (VLSI). IEEE, Kyoto, Japan, 300.

    - <span id="page-15-1"></span>[75] Barret Zoph, Vijay Vasudevan, Jonathon Shlens,
    and Quoc V. Le. 2018. Learning Transferable Architectures for Scalable Image Recognition.
    In IEEE Conference on Computer Vision and Pattern Recognition, (CVPR). Computer
    Vision Foundation / IEEE Computer Society, Salt Lake City, UT, USA, 8697–8710.'
  decisions:
    evaluation_prompt: '- Qualified. Reason: The paper contains multiple references
      to empirical evaluation, including sections discussing experiments, results,
      and comparisons with other methods. It mentions experiments demonstrating the
      performance of the proposed framework, Cocco, and provides quantitative results
      such as cost reductions and memory usage. Additionally, there are figures and
      tables presenting evaluation data, such as Figure 3 and Table 1, which further
      support the presence of empirical evaluation.'
    related_work_prompt: '- Qualified. Reason: The paper meaningfully engages with
      prior research throughout its content, including a detailed discussion of related
      works, comparisons to previous methods, and numerous academic citations that
      provide context and support for the proposed methods.'
    novelty_prompt: 'Qualified. Reason: The paper introduces Cocco, a novel hardware-mapping
      co-exploration framework that leverages graph-level features of networks to
      optimize memory capacity and communication. It proposes a new subgraph execution
      scheme, an efficient dataflow and memory management method, and a genetic-based
      algorithm for co-exploration, which are claimed to be novel contributions in
      the context of optimizing DNN accelerators. The paper makes clear claims of
      contribution and demonstrates novelty in its approach to memory and communication
      optimization.'
    review_only_prompt: '- Qualified. Reason: The paper introduces a new framework
      called Cocco for hardware-mapping co-exploration, which optimizes memory capacity
      and communication in DNN accelerators. It presents novel contributions, including
      a graph-level execution scheme, a genetic-based optimization framework, and
      experimental results demonstrating the effectiveness of the proposed methods.
      The paper does not primarily summarize existing work but instead focuses on
      new contributions and methodologies.'
  topics:
    main_topic: Computer Architecture
    main_topic_reasoning: The paper discusses memory capacity, data movement, and
      energy consumption in the context of hardware design for DNN accelerators, which
      are critical components of computer architecture.
    secondary_topic: Machine Learning
    secondary_topic_reasoning: The paper is centered around deep neural networks (DNNs),
      which are a significant aspect of machine learning, indicating that the topic
      is also relevant to this area.
    main_topic_sub: Memory Hierarchy
    secondary_topic_sub: Deep Learning
